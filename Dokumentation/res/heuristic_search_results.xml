<?xml version="1.0" ?>
<articles>
  <article>
    <title>Ranking of Predictor Variables Based on Effect Size Criterion Provides an Accurate Means of Automatically Classifying Opinion Column Articles</title>
    <count>1537</count>
    <raw_string>Physica A 390  2011  110119 Contents lists available at ScienceDirect Physica A journal homepage  wwwelseviercomlocatephysa Ranking of predictor variables based on effect size criterion provides an accurate means of automatically classifying opinion column articles Erika Fille Legara  Christopher Monterola   Cheryl Abundo National Institute of Physics  University of the Philippines  Diliman  Quezon City  1101  Philippines a r t i c l e i n f o Article history  Received 15 August 2009 Received in revised form 8 March 2010 Available online 17 March 2010 Keywords  Author identification analysis Effect size criterion Linear discriminant analysis a b s t r a c t We demonstrate an accurate procedure based on linear discriminant analysis that allows automatic authorship classification of opinion column articles  First  we extract the following stylometric features of 157 column articles from four authors  statistics on high frequency words  number of words per sentence  and number of sentences per paragraph  Then  by systematically ranking these features based on an effect size criterion  we show thatwe can achieve an average classification accuracy of 93  for the test set  In comparison  frequency size based ranking has an average accuracy of 80   The highest possible average classification accuracy of our data merely relying on chance is 31   By carrying out sensitivity analysis  we show that the effect size criterion is superior than frequency ranking because there exist low frequency words that significantly contribute to successful author discrimination  Consistent results are seen when the procedure is applied in classifying the undisputed Federalist papers of Alexander Hamilton and James Madison  To the best of our knowledge  the work is the first attempt in </raw_string>
  </article>
  <article>
    <title>Ranking of Predictor Variables Based on Effect Size Criterion Provides an Accurate Means of Automatically Classifying Opinion Column Articles</title>
    <count>1537</count>
    <raw_string>Physica A 390  2011  110119 Contents lists available at ScienceDirect Physica A journal homepage  wwwelseviercomlocatephysa Ranking of predictor variables based on effect size criterion provides an accurate means of automatically classifying opinion column articles Erika Fille Legara  Christopher Monterola   Cheryl Abundo National Institute of Physics  University of the Philippines  Diliman  Quezon City  1101  Philippines a r t i c l e i n f o Article history  Received 15 August 2009 Received in revised form 8 March 2010 Available online 17 March 2010 Keywords  Author identification analysis Effect size criterion Linear discriminant analysis a b s t r a c t We demonstrate an accurate procedure based on linear discriminant analysis that allows automatic authorship classification of opinion column articles  First  we extract the following stylometric features of 157 column articles from four authors  statistics on high frequency words  number of words per sentence  and number of sentences per paragraph  Then  by systematically ranking these features based on an effect size criterion  we show thatwe can achieve an average classification accuracy of 93  for the test set  In comparison  frequency size based ranking has an average accuracy of 80   The highest possible average classification accuracy of our data merely relying on chance is 31   By carrying out sensitivity analysis  we show that the effect size criterion is superior than frequency ranking because there exist low frequency words that significantly contribute to successful author discrimination  Consistent results are seen when the procedure is applied in classifying the undisputed Federalist papers of Alexander Hamilton and James Madison  To the best of our knowledge  the work is the first attempt in </raw_string>
  </article>
  <article>
    <title>Using Classifier Features for Studying the Effect of Native Language on the Choice of Written Second Language Words</title>
    <count>1463</count>
    <raw_string>Proceedings of the Workshop on Cognitive Aspects of Computational Language Acquisition  pages 916  Prague  Czech Republic  June 2007 c2007 Association for Computational Linguistics Using Classifier Features for Studying the Effect of Native Language on the Choice of Written Second Language Words Oren Tsur Institute of Computer Science The Hebrew University Jerusalem  Israel orencs huji acil Ari Rappoport Institute of Computer Science The Hebrew University Jerusalem  Israel wwwcs huji acilarir Abstract We apply machine learning techniques to study language transfer  a major topic in the theory of Second Language Acquisition  SLA   Using an SVM for the problem of native language classification  we show that a careful analysis of the effects of various features can lead to scientific insights  In particular  we demonstrate that character bi grams alone allow classification levels of about 66  for a 5class task  even when con tent and function word differences are ac  counted for  This may show that native lan guage has a strong effect on the word choice of people writing in a second language  1 Introduction While advances in NLP achieve improved results for NLP applications such as machine translation  ques  tion answering and document summarization  there are other fields of research that can benefit from the methods used by the NLP community  Second Lan guage Acquisition  SLA   a major area in Applied Linguistics and Cognitive Science  is one such field  In this paper we demonstrate how modern machine learning tools can contribute to SLA theory  In par ticular  we address the major SLA topic of language transfer  the effect of native language on second lan guage learners  Using an SVM for the </raw_string>
  </article>
  <article>
    <title>Experiments to Investigate the Utility of Nearest Neighbour Metrics Based on Linguistically Informed Features for Detecting Textual Plagiarism</title>
    <count>1451</count>
    <raw_string>Experiments to investigate the utility of nearest neighbour metrics based on linguistically informed features for detecting textual plagiarism Per Almquist and Jussi Karlgren Swedish Institute of Computer Science  SICS  Stockholm Royal Institute of Technology KTH  Stockholm Abstract Plagiarism detection is a challenge for linguistic models  most current im  plemented models use simple occurrence statistics for linguistic items  In this paper we report two experiments related to pla  giarism detection where we use a model for distributional semantics and of sen tence stylistics to compare sentence by sentence the likelihood of a text being partly plagiarised  The result of the com parison are displayed for visual inspection by a plagiarism assessor  1 Plagiarism detection Plagiarism is the act of copying or including another authors ideas  language  or writing  without proper acknowledgment of the original source  Plagiarism analysis is a collective term for computerbased methods to identify plagiarism   Stein et al   2007a  Plagiarism analysis can be performed intrinsically  a text is examined for internal consistency  to detect suspicious passages that appear to diverge from the surrounding text  or externally  a text is inspected with respect to some known corpus to find passages with suspi ciously similar content to other text  In external plagiarism detection  it is assumed that the source document dsrc for a given plagia  rized document dplg can be found in a target docu ment collection D Typically  plagiarism detection then proceeds in three stages  1  candidate selection through retrieval of a set of candidate source documents Dsrc is re  trieved from Dplg  2 candidates dsrc from Dsrc is compared pas  sage by passage with the suspicious docu ment </raw_string>
  </article>
  <article>
    <title>Reconnaissance de Texte dans les Image and les Videos en Utilisant des Reseaux de Neurones a Convolutions</title>
    <count>1418</count>
    <raw_string>These presentee et soutenue publiquement le 16 decembre 2008 pour lobtention du grade de Docteur TELECOM PARISTECH specialite  Signal et Image Reconnaissance de texte dans les image and les videos en utilisant des reseaux de neurones a convolutions Autheur  Zohra Saidane JURY President Alain Maruani Professeur  Telecom ParisTech Rapporteur Faouzi Ghorbel Professeur  ENSI Tunis Rapporteur Olivier Lezoray MC HDR  GREYC UMR Codirecteur Christophe Garcia Expert emerite  Orange Labs Codirecteur Jean Luc Dugelay Professeur  EURECOM December 2008 Acknowledgements This thesis owes its existence to the help and support of many people  First  I would like to express my deepest appreciation and gratitude to my supervisor Christophe Garcia for his advice and permanent guidance throughout this research  I enjoyed working with him and learning from him  I am also indebted to my academic supervisor Jean Luc Dugelay  I was fortunate to have him as my Professor during my undergraduate studies and have his support during all these years  I would like to thank the members of my jury  in particular Faouzi Ghor bel and Olivier Lezoray  who took the time and energy to point out my errors and omissions  My thanks also go to Alain Maruani for honoring me by presiding my jury  To all members of the ICM group1 at Orange Labs  I am very grateful for the cooperative spirit and the excellent working atmosphere  I really spent three very pleasant years  Finally  I want to say thank you to my family for their continuing support in every respect and especially to my Ayoub  and my little Cheima  1ICM group stands for Multimedia Content Indexing group ii Resume Grace a des moyens de stockage de plus en plus </raw_string>
  </article>
  <article>
    <title>An Approach to a Comprehensive Test Framework for Analysis and Evaluation of Text Line Segmentation Algorithms</title>
    <count>1303</count>
    <raw_string>Sensors 2011  11  87828812  doi103390s110908782 sensors ISSN 14248220 wwwmdpicomjournalsensors Article An Approach to a Comprehensive Test Framework for Analysis and Evaluation of Text Line Segmentation Algorithms Darko Brodic 1   Dragan R Milivojevic 2 and Zoran N Milivojevic 3 1 Technical Faculty Bor  VJ 12  University of Belgrade  19210 Bor  Serbia 2 Department of Informatics  Zeleni Bulevar 35  Mining and Metallurgy Institute  19210 Bor  Serbia  EMail  draganmilivojevicirmborcors 3 Technical College Ni  Aleksandra Medvedeva 20  18000 Ni  Serbia  EMail  zoranmilivojevicvtsnis edurs  Author to whom correspondence should be addressed  EMail  dbrodictfboracrs  Tel   38130424555  Fax  38130421078  Received  1 August 2011  in revised form  2 September 2011  Accepted  5 September 2011  Published  13 September 2011 Abstract  The paper introduces a testing framework for the evaluation and validation of text line segmentation algorithms  Text line segmentation represents the key action for correct optical character recognition  Many of the tests for the evaluation of text line segmentation algorithms deal with text databases as reference templates  Because of the mismatch  the reliable testing framework is required  Hence  a new approach to a comprehensive experimental framework for the evaluation of text line segmentation algorithms is proposed  It consists of synthetic multilike text samples and real handwritten text as well  Although the tests are mutually independent  the results are crosslinked  The proposed method can be used for different types of scripts and languages  Furthermore  two different procedures for the evaluation of algorithm efficiency based on the obtained error type classification are proposed  The first is based on the segmentation line </raw_string>
  </article>
  <article>
    <title>A Comparison of Stylometric and Lexical Features for Web Genre Classification and Emotion Classification in Blogs</title>
    <count>1302</count>
    <raw_string>A Comparison of Stylometric and Lexical Features for Web Genre Classification and Emotion Classification in Blogs Elisabeth Lex KnowCenter GmbH Graz  Austria elexknowcenterat Andreas Juffinger The European Library co the Koninklijke Bibliotheek 2509 LK The Hague andreas juffingerkbnl Michael Granitzer KnowCenter GmbH University of Technology Graz Graz  Austria mgraniknowcenterat Abstract In the blogosphere  the amount of digital content is expanding and for search engines  new challenges have been imposed  Due to the changing information need  automatic methods are needed to support blog search users to filter infor mation by different facets  In our work  we aim to support blog search with genre and facet information  Since we focus on the news genre  our approach is to classify blogs into news versus rest  Also  we assess the emotionality facet in news related blogs to enable users to identify peoples feelings towards specific events  Our approach is to evaluate the performance of text classifiers with lexical and stylometric features to determine the best performing combination for our tasks  Our experiments on a subset of the TREC Blogs08 dataset reveal that classifiers trained on lexical features perform consistently better than classifiers trained on the best stylometric features  KeywordsDocument Classification  Data Mining  Features I INTRODUCTION On the Web  a huge amount of information is available and navigating through it is not an easy task  Especially since the advent of Web 20 the amount of content is expanding and for search engines  new challenges have arisen to cope with the changing information need  As mentioned in 1  the problem of answering questions is transformed from finding a needle in a haystack to a process of being presented with a variety of needles and </raw_string>
  </article>
  <article>
    <title>Mathematical Modelling of the Pattern of Occurrence of Words in Different Corpora of the Hindi Language</title>
    <count>1301</count>
    <raw_string>This article was downloaded by   University of Aegean  On  09 February 2013  At  0241 Publisher  Routledge Informa Ltd Registered in England and Wales Registered Number  1072954 Registered office  Mortimer House  3741 Mortimer Street  London W1T 3JH  UK Journal of Quantitative Linguistics Publication details  including instructions for authors and subscription information  http wwwtandfonline comloinjql20 Mathematical Modelling of the Pattern of Occurrence of Words in Different Corpora of the Hindi Language  Hemlata Pande a  HS Dhami a a Department of Mathematics  Kumaun University  S S J Campus Almora  Uttarakhand  India Version of record first published  04 Feb 2013  To cite this article  Hemlata Pande  HS Dhami  2013   Mathematical Modelling of the Pattern of Occurrence of Words in Different Corpora of the Hindi Language   Journal of Quantitative Linguistics  201  112 To link to this article  http dx doi org101080092961742012754596 PLEASE SCROLL DOWN FOR ARTICLE Full terms and conditions of use  http wwwtandfonline compageterms  andconditions This article may be used for research  teaching  and private study purposes  Any substantial or systematic reproduction  redistribution  reselling  loan  sub  licensing  systematic supply  or distribution in any form to anyone is expressly forbidden  The publisher does not give any warranty express or implied or make any representation that the contents will be complete or accurate or up to date  The accuracy of any instructions  formulae  and drug doses should be independently verified with primary sources  The publisher shall not be liable for any loss  actions  claims  proceedings  demand  or costs or damages whatsoever or </raw_string>
  </article>
  <article>
    <title>Computational Stylometic Approach Based on Frequent Word and Frequent Pair in the Text Mining Authorship Attribution</title>
    <count>1301</count>
    <raw_string>IJCSNS International Journal of Computer Science and Network Security  VOL9 No3  March 2009 262 Manuscript received March 5  2009 Manuscript revised March 20  2009 Computational Stylometic Approach Based on Frequent Word and Frequent Pair in the Text Mining Authorship Attribution Tareef Kamil Mustafa1  Norwati Mustapha2  Masrah Azrifah Azmi Murad2 and Md  Nasir Sulaiman2 Faculty of Computer Science and Information Technology University Putra Malaysia Serdang  Selangor  Malaysia Summary Stylometric Authorship attribution is one of the new approaches in the text mining field that has been showing recently because of its delicateness  This approach is concerned about analyzing texts  eg  Novels and plays that famous authors wrote  trying to measure the author style  by choosing some attributes that shows the author style of writing  assuming that these writers have a special way of writing  that no other writer has  To achieve that  this paper discusses several algorithms which are used frequently and skipping the one time  adhoc adventures in this field  This paper is also opens the way for future works to merge and improve these techniques by showing experimentally the accuracy level of using both frequent words and frequent word pair depending on the computational approach  Key words  Authorship attribution  computational stylometric  text mining  1  Introduction Text mining is a diverted subject from the wellknown field Data mining  As for authorship investigation that using the writing style of the author is a sub field of text mining called Authorship attribution  or Stylometric Text mining  All these subjects need to be defined to get the picture well clarified  A Text Mining is the discovery by computer of new  previously unknown information </raw_string>
  </article>
  <article>
    <title>Analysis of the Role of Entropies and Assignment of Ranks to the Features in Genre Discrimination</title>
    <count>1295</count>
    <raw_string>Sutra  International Journal of Mathematical Science Education  Technomathematics Research Foundation Vol  4 No 1  pp  33  48  2011 33 ANALYSIS OF THE ROLE OF ENTROPIES AND ASSIGNMENT OF RANKS TO THE FEATURES IN GENRE DISCRIMINATION HEMLATA PANDEa AND H S DHAMIb Abstract Genre or style  an important property of text  and automatic text genre discrimination is becoming important for classification and retrieval purposes as well as for many natural language processing tasks  Various methods with feature cue vectors have been used for genre discrimination  which utilize different statistical measures corresponding to a range of linguistic features  Since all features of a text are not equally useful so for better results and to reduce the dimensionality of used spaces  it becomes necessary to select those features which have the capacity to discriminate between categories in a better manner  In order to achieve this aim  a technique known as feature selection is commonly applied in text classification  These techniques attempt to select the subset of features in text categorization that are most useful for the categorization task  We propose to use a ranking technique for feature selection with the help of a completely randomized design method for features and square deviations of property weights with their applications to familiar measures such as syntactic features  vocabulary features and most frequent words  It has been established that besides part of speech probabilities  their entropies play an important role in the discrimination process  Keywords  Categorization  Entropy  Features  Rank  Completely randomized design  Square deviations  1  Introduction Text classification research has been mostly focused on subject or prepositional content of text  As textbased applications become more varied </raw_string>
  </article>
  <article>
    <title>Una Versione Iterativa della Distanza Intertestuale Applicata a un Corpus di Opere della Letteratura Italiana Contemporanea</title>
    <count>1290</count>
    <raw_string>Una versione iterativa della distanza intertestuale applicata a un corpus di opere della letteratura italiana contemporanea Michele A Cortelazzo1  Paolo Nadalutti2  Arjuna Tuzzi3 1Universit di Padova  cortmicunipd it 2Universit di Padova  paolonadaluttiunipd it 3Universit di Padova  arjunatuzziunipd it Abstract The main aim of this study is testing the performance of Labbes intertextual distance in the case of a corpus of novels of the Italian literature  moreover an iterative revised version of intertextual distance is introduced  The revised procedure is based on repeated measures of the intertextual distance between pairs of textchunks of equal size  It is intended to counterbalance the size factor and assess the similarity of texts of very different sizes  In order to test if and to what extent intertextual distance is suitable to assess authorship attribution in the case of Italian novels  we took into account the rankings obtained with reference to each novel ordering the others by means of the values of intertextual distances  Opportunities and limits of the available procedures are discussed  Riassunto Lobiettivo principale del contributo  valutare il funzionamento della distanza intertestuale di Labb nel caso di un corpus di opere della letteratura italiana e presentarne una nuova versione iterativa  La nuova procedura si basa su misure ripetute della distanza intertestuale per coppie di porzioni di testo della stessa dimensione  mira a compensare leffetto delle dimensioni e a determinare la similarit tra testi di dimensioni molto diverse  Per valutare se  e in che misura  la distanza intertestuale pu essere impiegata per lattribuzione dautore nel caso di opere della narrativa italiana  si considerano le graduatorie che  data unopera  si ottengono ordinando le altre in base al valore della distanza intertestuale  Vengono  infine </raw_string>
  </article>
  <article>
    <title>Extracting Shallow Paraphrasing Schemata from Modern Greek Text Using Statistical Significance Testing and Supervised Learning</title>
    <count>1282</count>
    <raw_string>JM Sempere and P Garca  Eds    ICGI 2010  LNAI 6339  pp  297300  2010   SpringerVerlag Berlin Heidelberg 2010 Extracting Shallow Paraphrasing Schemata from Modern Greek Text Using Statistical Significance Testing and Supervised Learning Katia Lida Kermanidis Department of Informatics  Ionian University 7 Pl  Tsirigoti  49100 Corfu  Greece kermanioniogr Abstract  Paraphrasing normally involves sophisticated linguistic resources for preprocessing  In the present work Modern Greek paraphrases are automati cally generated using statistical significance testing in a novel manner for the extraction of applicable reordering schemata of syntactic constituents  Next  su pervised filtering helps remove erroneously generated paraphrases  taking into account the context surrounding the reordering position  The proposed process is knowledgepoor  and thus portable to languages with similar syntax  robust and domainindependent  The intended use of the extracted paraphrases is hid  ing secret information underneath a cover text  Keywords  paraphrasing  statistical significance testing  supervised learning  1 Introduction Paraphrasing is expressing the meaning of a sentence using a different set of words andor a different syntactic structure  Paraphrasing is useful in language learning  authoring support  text summarization  question answering  machine translation  tex  tual entailment  and natural language generation  Significant research effort has been put into paraphrase identification 9  6  1  and generation 7  2  The present work describes the automatic inference of syntactic patterns from Modern Greek  MG text for generating shallow paraphrases  The proposed method  ology is a combination of a statistical significance testing process for generating swapable  phrase  chunk  pairs based on their co occurrence statistics  followed by a supervised filtering phase </raw_string>
  </article>
  <article>
    <title>A Study on Plagiarism Detection and Plagiarism Direction Identification Using Natural Language Processing Techniques</title>
    <count>1221</count>
    <raw_string>A Study on Plagiarism Detection and Plagiarism Direction Identification Using Natural Language Processing Techniques Man Yan Miranda Chong A thesis submitted in partial fulfilment of the requirements of the University of Wolverhampton for the degree of Doctor of Philosophy 2013 This work or any part thereof has not previously been presented in any form to the University or to any other body whether for the purposes of as sessment  publication or for any other purpose unless otherwise indicated   Save for any express acknowledgements  references andor bibliographies cited in the work  I confirm that the intellectual content of the work is the result of my own efforts and of no other person  The right of Man Yan Miranda Chong to be identified as author of this work is asserted in accordance with ss 77 and 78 of the Copyright  Designs and Patents Act 1988  At this date copyright is owned by the author  Signature                                   Date                                      ii This thesis is dedicated to the memory of my grandfather  Yuk Ming Chong  iii iv Iron is full of impurities that weaken it  through the forging fire  it becomes steel and is transformed into a razorsharp sword  Human beings develop in the same fashion   Morihei Ueshiba v vi Abstract Ever since we entered the digital communication era  </raw_string>
  </article>
  <article>
    <title>Using Comparable Corpora to Track Diachronic and Synchronic Changes in Lexical Density and Lexical Richness</title>
    <count>1217</count>
    <raw_string>Using Comparable Corpora to Track Diachronic and Synchronic Changes in Lexical Density and Lexical Richness Sanja Stajner and Ruslan Mitkov Research Group in Computational Linguistics  RIILP University of Wolverhampton  UK SStajnerwlvacuk  RMitkovwlvacuk Abstract This study from the area of language variation and change is based on exploitation of the comparable diachronic and synchronic corpora of 20th century British and American English language  the Brown family  of corpora   We investigate recent changes of lexical density and lexical richness in two consecutive thirtyyear time gaps in British English  19311961 and 19611991  and in 19611992 in American English  Furthermore  we compare the diachronic changes between these two language varieties and discuss the results of the synchronic comparison of these two features between British and American parts of the corpora  in 1961 and in 19912   Additionally  we explore the possibilities of these comparable corpora by using two different approaches to their exploitation  using the fifteen finegrained text genres  and using only the four main text categories  Finally  we discuss the impact of the chosen approaches in making hypotheses about the way language changes  Keywords  corpus analysis  language change  lexical richness 1  Introduction Kroch  2008  defines language change as a failure in the transmission across time of linguistic features  and states that over historical time languages change at every level of the language structure  vocabulary  phonology  mor phology and syntax  He states that in principle  language change can occur within groups of adult native speakers of language as the result of the substitution of one feature with another as in the case of the substitution of old words with new ones  though </raw_string>
  </article>
  <article>
    <title>A Text Mining Approach to Assist the General Public in the Retrieval of Legal Documents</title>
    <count>1215</count>
    <raw_string>A Text Mining Approach to Assist the General Public in the Retrieval of Legal Documents YenLiang Chen and YiHung Liu Department of Information Management  National Central University  ChungLi  No 300  Jhongda Road  Jhongli City  Taoyuan County 32001  Taiwan  ROC   Email  ylchenmgt ncuedutw  lyh0315gmailcom WuLiang Ho Department of Legal Service  Straits Exchange Foundation  No 536  Beian Road  Zhongshan District  Taipei City  10465  Taiwan  ROC   Email  alainsouchan2005yahoocomtw Applying text mining techniques to legal issues has been an emerging research topic in recent years  Although some previous studies focused on assisting professionals in the retrieval of related legal documents  they did not take into account the general public and their difficulty in describing legal problems in profes  sional legal terms  Because this problem has not been addressed by previous research  this study aims to design a textminingbased method that allows the general public to use everyday vocabulary to search for and retrieve criminal judgments  The experimental results indicate that our method can help the general public  who are not familiar with professional legal terms  to acquire relevant criminal judgments more accurately and effectively  Introduction The 21st century is a century of laws  and so  to protect ones rights  one should be acquainted with the law  The law and our lives are intimately connected  numerous daily events involve legal issues and have become a necessity to modern life  The individual relationships within a society may be full of conflict and friction  which can endanger peoples lives and property  Therefore  to maintain social harmony  a country must develop laws and require its citi </raw_string>
  </article>
  <article>
    <title>Using the Complexity of the Distribution of Lexical Elements as a Feature in Authorship Attribution</title>
    <count>1215</count>
    <raw_string>Using the Complexity of the Distribution of Lexical Elements as a Feature in Authorship Attribution LM Spracklin  DZ Inkpen  A Nayak University of Ottawa Ottawa  Canada lspra072uottawaca  dianasite uottawaca  anayaksite uottawaca Abstract Traditional Authorship Attribution models extract normalized counts of lexical elements such as nouns  common words and punctuation and use these normalized counts or ratios as features for author fingerprinting  The text is viewed as a bagofwords  and the order of words and their position relative to other words is largely ignored  We propose a new method of feature extraction which quantifies the distribution of lexical elements within the text using Kolmogorov complexity estimates  Testing carried out on blog corpora indicates that such measures outperform ratios when used as features in an SVM authorship attribution model  Moreover  by adding complexity estimates to a model using ratios  we were able to increase the Fmeasure by 52118  1  Introduction Determining the author of a text is an important problem in computational linguistics  It has applications to plagiarism  copyright infringement and the analysis of anonymously written texts  Normally a machine learning system for authorship attribution extracts features which represent the counts of a variety of lexical elements which are normalized over the length of the text  For example the number of nouns  verbs  common words or punctuation characters may be counted  This is done to fingerprint an author  A model is created based on these features and texts of unknown origin are classified using the model  The problem with extracting counts of elements is that information is lost  The text is viewed as a bag ofwords  and the distribution of elements within the text is ignored </raw_string>
  </article>
  <article>
    <title>Using Wikipedia Concepts and Frequency in Language to Extract Key Terms from Support Documents</title>
    <count>1135</count>
    <raw_string>Expert Systems with Applications 39  2012  1348013491Contents lists available at SciVerse ScienceDirect Expert Systems with Applications journal homepage  wwwelsevier comlocate eswaUsing Wikipedia concepts and frequency in language to extract key terms from support documents M Romero   A Moreo  JL Castro  JM Zurita Dep of Computer Science and Artificial Intelligence  University of Granada  Spain a r t i c l e i n f o a b s t r a c tKeywords  Automatic Keyword Extraction Support documents FAQ Wikipedia Word sense disambiguation Natural Language09574174  see front matter 2012 Elsevier Ltd A http dx doi org101016jeswa201207011  Corresponding author  Address  CPeriodista Da 18071 Granada  Spain  Tel   34 958244019  fax  34 Email addresses  manudbdecsai ugres  M Ro  A  Moreo   castrodecsai ugres  JL Castro   zuritad In this paper  we present a new key term extraction system able to handle with the particularities of support documents   Our system takes advantages of frequencybased and thesaurusbased approaches to recognize two different classes of key terms  On the one hand  it identifies multidomain key terms of the collection using Wikipedia as knowledge resource  On the other hand  the system extracts specific key terms highly related with the context of a support document  We use the frequency in language as a cri terion to detect and rank such terms  To prove the validity of our system we have designed a set of exper iment using a Frequently Asked Questions  FAQ  collection of documents  Since our approach is generic  minor modifications should be undertaken to adapt the system to other kind of support documents  The empirical results </raw_string>
  </article>
  <article>
    <title>A Language Classifier that Automatically Divides Medical Documents for Experts and Health Care Consumers</title>
    <count>1134</count>
    <raw_string>A Language Classifier that Automatically Divides Medical Documents for Experts and Health Care Consumers Michael POPRAT a  Kornl MARK b  Udo HAHNa a Jena University Language  Information Engineering  JULIE  Lab  Jena  Germany b Medical Informatics Department  Freiburg University Hospital  Germany Abstract  We propose a pipelined system for the automatic classification of medical documents according to their language  English  Spanish and German  and their target user group  medical experts vs health care consumers   We use a simple ngram based categorization model and present experimental results for both classification tasks  We also demonstrate how this methodology can be integrated into a health care document retrieval system  1  Introduction The user population of medical document retrieval systems and their search strategies are really diverse  Not only physicians but also nurses  medical insurance companies and patients are increasingly getting access to these textbased resources  Especially health care consumers  henceforth  HCC  spend more and more time on getting information from Web resources to gather a better understanding about their personal health status  sufferings   inadequate treatments  medications and other interventions  Although many patients already exhibit a considerable level of expertise concerning their diseases  not every piece of relevant information from the Web is particularly suited and understandable for nonexperts  To help HCCs in finding appropriate information  as well as medical experts in suppressing layman information  we realized a text analysis pipeline which consists of a simple but highperforming classification algorithm  It distinguishes the kind of natural language in which a document is written and  subsequently  divides this document space into easily understandable texts  and  into those documents </raw_string>
  </article>
  <article>
    <title>Multilayered Feedforward Neural Networks as a Tool for Distinction of the Authors of Texts</title>
    <count>1134</count>
    <raw_string>Multilayered Feedforward Neural Networks as a Tool for Distinction of the Authors of Texts Suvad Selman Faculty of Engineering and Natural Sciences International University of Sarajevo Bosnia and Herzegovina Email  sselmanius eduba Alma HusagicSelman Faculty of Engineering and Natural Sciences International University of Sarajevo Bosnia and Herzegovina Email  asselmanius eduba AbstractThis paper proposes a means of using a multilayered feedforward neural network to identify the author of a text  The network has to be trained where multilayer feedforward neural network as a powerful scheme for learning complex inputoutput mapping have been used in learning of the textual descriptors in a paragraphs of an author  The resulting training information we get will be used to identify the texts written by authors  The computational complexity is solved by dividing it into a number of computationally simple tasks where the input space is divided into a set of subspaces and then combining the solutions to those tasks  By this  we have been able to successfully distinguish the books authored by Leo Tolstoy  from the ones authored by Fyodor Dostoyevsky  Keywords  Machine learning  author identification  artificial neural networks I INTRODUCTION Author identification is the task of identifying the author of a given text  It can be considered as a typical classification problem  where a set of documents with known authors are used for training and the aim is to automatically determine the corresponding author of an anonymous text  In contrast to other classification tasks  it is not clear which features of a text should be used to classify an author  Consequently  the main concern of computerbased author identification is to define an appropriate characterization of documents that captures the writing style 6  7 of authors </raw_string>
  </article>
  <article>
    <title>Authorship Attribution in Arabic Using a Hybrid of Evolutionary Search and Linear Discriminant Analysis</title>
    <count>1133</count>
    <raw_string>Authorship Attribution in Arabic using a Hybrid of Evolutionary Search and Linear Discriminant Analysis Kareem Shaker and David Corne School of Mathematical and Computer Sciences  HeriotWatt University Edinburgh  EH14 4AS  UK ks113hwacuk  dwcornehwacuk Abstract Authorship Attribution is the problem of determining the authorship of one or more texts  Applications include disputed authorship  or deciding which of a collection of pieces of text were by the same author  A popular and successful approach is to characterize a specific author in terms of the usage pattern of function words  These are common words that are unrelated to subject matter  and tend to be used in specific ways by different authors  In English  a wellknown collection of 70 function words is often used for this purpose  Previously  using a hybrid of evolutionary search and lineardiscriminant analysis  LDA   we have shown excellent performance in authorship attribution in English based on a function word approach  Here  for the first time  we propose and test a set of Arabic function words for use in Arabic authorship attribution  Tests indicate that the chosen collection forms an effective basis for authorship attribution in Arabic  1  Introduction The Authorship Attribution problem is the task of determining the authorship of a given piece of text  In cases of disputed authorship  two  or maybe more  distinct individuals may claim authorship  and there are several historical examples of such conflicting authorship claims  For example  two wellknown cases of disputed texts in English include the disputed Federalist papers 1  and the 15th Book of Oz 2  A wide variety of methods have been researched for authorship attribution  e g  see </raw_string>
  </article>
  <article>
    <title>A Comparative Survey of Image Binarisation Algorithms for Optical Recognition on Degraded Musical Sources</title>
    <count>1125</count>
    <raw_string>A COMPARATIVE SURVEY OF IMAGE BINARISATION ALGORITHMS FOR OPTICAL RECOGNITION ON DEGRADED MUSICAL SOURCES John Ashley Burgoyne Laurent Pugin Greg Eustace Ichiro Fujinaga Centre for Interdisciplinary Research in Music and Media Technology Schulich School of Music of McGill University Montreal  Quebec  Canada H3A 1E3 ashley laurent greg ich musicmcgillca ABSTRACT Binarisation of greyscale images is a critical step in optical music recognition  OMR  preprocessing  Binarising mu sic documents is particularly challenging because of the nature of music notation  even more so when the sources are degraded  eg  with ink bleedthrough from the other side of the page  This paper presents a comparative eval uation of 25 binarisation algorithms tested on a set of 100 music pages  A realworld OMR infrastructure for early music  Aruspix  was used to perform an objective  goal  directed evaluation of the algorithms  performance  Our results differ significantly from the ones obtained in stud ies on nonmusic documents  which highlights the impor tance of developing tools specific to our community  1 INTRODUCTION Binarising music documents  that is separating the fore ground from the background in order to prepare for other tasks such as optical music recognition  OMR  is much more challenging than binarising text documents  In text documents  the letters are all of approximately the same size and are regularly and uniformly distributed through  out the page  Music symbols  on the other hand  exhibit a wide range of sizes and markedly uneven distribution  they are clustered around musical staves  Large black ar eas  such as note heads  are conducive to ink accumula  tion during printing  which often results in strong bleed  through elements </raw_string>
  </article>
  <article>
    <title>Statistical Texture Features based Handwritten and Printed Text Classification in South Indian Documents</title>
    <count>1059</count>
    <raw_string>215 Statistical Texture Features based Handwritten and Printed Text Classification in South Indian Documents Mallikarjun Hangargea   KC Santoshb  Srikanth Doddamania  Rajmohan Pardeshia aDepartment of Computer Science Karnatak Arts  Science and Commerce College  Bidar  Karnataka  India bINRIA NancyGrand Est  LORIA BP  239  54506 VanduvrelsNancy Cedex  France Abstract In this paper  we use statistical texture features for handwritten and printed text classification  We primarily aim for word level classification in south Indian scripts  Words are first extracted from the scanned document  For each extracted word  statistical texture features are computed such as mean  standard deviation  smoothness  moment  uniformity  entropy and local range including local entropy  These feature vectors are then used to classify words via kNN classifier  We have validated the approach over several different datasets  Scripts like Kannada  Telugu  Malayalam and Hindi i e   Devanagari are primarily employed where an average classification rate of 9926  is achieved  In addition  to provide an extensibility of the approach  we address Roman script by using publicly available dataset and interesting results are reported  Keywords  Statistical texture features  handwritten and printed text  south Indian scripts  1  Introduction Advance development in digital technologies demands a robust Optical Character Recognition  OCR  system to reach the dream of paperless office  An overall performance of OCR system depends on the preprocessing technique such as segmentation  for instance  Among many  one of the prominent techniques is to separate handwritten and printed text  from a document image  a challenging problem in document image analysis domain  commercially speaking  In the context of Indian </raw_string>
  </article>
  <article>
    <title>Using Rhetorical Figures and Shallow Attributes as a Metric of Intent in Text</title>
    <count>1058</count>
    <raw_string>Using Rhetorical Figures and Shallow Attributes as a Metric of Intent in Text by Claus W Strommer A thesis presented to the University of Waterloo in fulfilment of the thesis requirement for the degree of Doctor of Philosophy in Computer Science Waterloo  Ontario  Canada  2011 c Claus W Strommer 2011 AUTHORS DECLARATION I hereby declare that I am the sole author of this thesis  This is a true copy of the thesis  including any required final revisions  as accepted by my examiners  I understand that my thesis may be made electronically available to the public  ii ABSTRACT In this thesis we propose a novel metric of document intent evaluation based on the detection and classification of rhetorical figure  In doing so we dispel the notion that rhetoric lacks the structure and consistency necessary to be relevant to compu tational linguistics  We show how the combination of document attributes available through shallow parsing and rules extracted from the definitions of rhetorical figures produce a metric which can be used to reliably classify the intent of texts  This metric works equally well on entire documents as on portions of a document  iii ACKNOWLEDGEMENTS It is difficult to overstate my gratitude to my PhD supervisor  Chrysanne DiMarco  Through enthusiasm  inspiration  and encouragement she provided me with guidance without which I would have felt lost in my work  I would like to thank the people who directly guided my thesis  especially Randy Harris for introducing me to the academic study of rhetoric  Charlie Clarke for guid  ing me through the pragmatics of applied corpus research  and Robin Cohen for her unerring eye for detail in thesis writing  I would further like to </raw_string>
  </article>
  <article>
    <title>Experiments to Investigate the Utility of Linguistically Informed Features for Detecting Textual Plagiarism</title>
    <count>1058</count>
    <raw_string>Experiments to Investigate the Utility of Linguistically Informed Features for Detecting Textual Plagiarism P E R A L M Q U I S T Master of Science Thesis Stockholm  Sweden 2011 Experiments to Investigate the Utility of Linguistically Informed Features for Detecting Textual Plagiarism P E R A L M Q U I S T Masters Thesis in Computer Science  30 ECTS credits  at the School of Computer Science and Engineering Royal Institute of Technology year 2011 Supervisor at CSC was Serafim Dahl Examiner was Stefan Arnborg TRITACSCE 2011109 ISRNKTHCSCE11109SE ISSN16535715 Royal Institute of Technology School of Computer Science and Communication KTH CSC SE100 44 Stockholm  Sweden URL  wwwkth secsc Abstract We perform experiments that shows whether or not two linguistic features are good indicators to be used when au  tomatically detecting plagiarism in digital texts  Two experiments are performed  In the first experiment a linguistic feature based on a semantic wordspace model is evaluated  and in the second experiment a linguistic feature based on stylometry is evaluated  Both experiments are evaluated using a nearest neighbor metric since the features are multidimensional vectors  We find that the first feature is a good indicator for detecting plagiarism that is an exact copy of its source  We find that the second feature performs equally good inde pendent of text obfuscation  Referat Experiment rrande anvndning av lingvistiska srdrag i plagiatkontroll Vi utfr experiment som visar huruvida tv lingvistiska sr drag r bra indikatorer att anvnda fr att automatiskt upptcka plagiat i digitala texter  Tv experiment utfrs  I det frsta experimentet utvr deras ett lingvistiskt srdrag som baseras p en semantisk ordrums modell och i det andra experimentet utvrderas ett lingvistiskt srdrag som baseras p stilometeri  eng  </raw_string>
  </article>
  <article>
    <title>Riding the Rough Waves of Genre on the Web Concepts and Research Questions</title>
    <count>1055</count>
    <raw_string>Chapter 1 Riding the Rough Waves of Genre on the Web Concepts and Research Questions Marina Santini  Alexander Mehler  and Serge Sharoff 11 Why Is Genre Important  Genre  in the most generic definition  takes the meaning kind  sort  style   OED   A more specialised definition of genre in OED reads  A particular style or category of works of art  esp a type of literary work characterised by a particular form  style  or purpose   Similar definitions are found in other dictionaries  for instance  OALD reads a particular type or style of literature  art  film or music that you can recognise because of its special features  Broadly speaking  then  generalising from lexicographic definitions  genre can be seen as a classificatory principle based on a number of characterising attributes  Traditionally  it was Aristotle  in his attempt to classify existing knowledge  who started genre analysis and defined some attributes for genre classification  Aristotle sorted literary production into different genre classes by focussing on the attributes of purpose and conventions 1 After him  through the centuries  numberless definitions and attributes of the genre of written documents have been provided in differing fields  including literary criticism  linguistics and library and information science  With the advent of digital media  especially in the last 15 years  the potential of genre for practical appli cations in language technology and information technology has been vigorously emphasised by scholars  researchers and practitioners  M Santini  B  KYH  Stockholm  Sweden email  marinasantinimsgmailcom 1 More precisely  in the Poetics  Aristotle writes  the medium being the same  and </raw_string>
  </article>
  <article>
    <title>Genre and Author Detection in Turkish Texts using Artificial Immune Recognition Systems</title>
    <count>1054</count>
    <raw_string>Yapay Baklk Sistemleri ile Trke Metinlerde Tr ve Yazar Tanma Genre and Author Detection in Turkish Texts Using Artificial Immune Recognition Systems Zafer Kaban Banu Diri Bilgisayar Mhendislii Blm Yldz Teknik niversitesi  stanbul zaferkabanargelacomtr  banuyildizedutr zete Bu alma 1de bahsedilmi olan bir dokmann farkl ekilde ifade edilmesi yntemi kullanlarak  Yapay Baklk Sistemlerinin bir metnin trn ve yazarn tanmadaki baarsn aratrmak amacyla yaplmtr  Gnmzde yaplan metin snflandrma sistemlerinin bir ou  kelime kk ya da gvdelerini zellik olarak alan Bag of Words  Kelime Torbas  modeli ile yaplmaktadr  Bu durum hem zellik saysn hem de snflandrma sresini arttrmaktadr  Bu almada  Yldz ve arkadalar tarafndan gelitirilen ve YTU yntemi adn verdiimiz metot kullanlarak  metinlerde geen kelime gvdelerine bir arlklandrma algoritmas uygulanarak zelliklerin says snf saysna indirilmekte ve bu sayede hem snflandrma sresi hem de baars arttrlm olmaktadr  Bu yntem ile test ettiimiz Yapay Baklk Sistemi algoritmalar olan YBS1  YBS2  YBS2Parallel tr ve yazar tanmada alnan baar sonularnn ykselmesini salamtr  Makalenin deneysel sonular blmnde tr ve yazar tanma almalarnda ok sk kullanlan Naive Bayes  NB  Destek Vektr Makinesi  DVM  Rasgele Orman  RO  ve K En Yakn Komuluk KEK  snflandrclarndan alnan sonular  Yapay Baklk Sistemi algoritmalarndan elde edilen sonular ile karlatrmal olarak verilmitir  zellikle tr tanmada YBS2Parallel snflandrcs 996lk baar oran ile KEn Yakn Komuluk ve Rastgele Ormanla birlikte en yksek baar orann vermektedir  Bu da bu yntem kullanlarak Yapay Baklk Sistemi algoritmalarnn tr tanmada kullanlabileceini gstermektedir  Abstract This study is made for investigating the performance of Artificial Immune Recognition Systems on genre and author detection by using method referenced as 1  based on representation of a document in a different scheme  Most of the studies done nowadays </raw_string>
  </article>
  <article>
    <title>Analyzing the Characteristics of Academic Paper Categories by Using an Index of Representativeness</title>
    <count>1054</count>
    <raw_string>Analyzing the characteristics of academic paper categories by using an index of representativeness  Takafumi Suzuki a  Kiyoko Uchiyama b  Ryota Tomisaka c  and Akiko Aizawa b a Faculity of Sociology  Toyo University  52820  Hakusan  Bunkyoku  Tokyo 1128606  Japan takafumi stoyojp b Digital Content and Media Sciences Research Division  National Institute of Informatics 212  Hitotsubashi  Chiyodaku  Tokyo 1018430  Japan kiyoko  aizawanii acjp c Graduate School of Information Science and Technology  The University of Tokyo 731  Hongo  Bunkyoku  Tokyo 1120023  Japan ryotatomisakagmailcom Abstract  This study proposes an index of representativeness for analyzing the characteris  tics of academic paper categories  Many textual indices have been proposed in the field of computational stylistics  but all of the previous indices are limited in that they  a  focus only on the styles of the texts   b  return an absolute value for every text  and  c  are based on the number of tokens  In this study  we propose an index of representativeness that does not have the weaknesses of the previous indices  Our index is based on the hindex that was originally proposed in the field of scientometrics  We redefine it here for textual data  We show the effectiveness of our index for analyzing the characteristics that differ between four genres and three subfields in Japanese academic papers  Keywords  academic papers  hindex  lexical indicators 1 Introduction Many textual indices have been proposed in the field of computational stylistics  The number of tokens  i e   the frequency of words in a document  is the simplest  Some indices use information related </raw_string>
  </article>
  <article>
    <title>Learning to Build a Semantic Thesaurus from Free Text Corpora without External Help</title>
    <count>1053</count>
    <raw_string>7 Learning to Build a Semantic Thesaurus from Free Text Corpora without External Help Katia Lida Kermanidis Ionian University Greece 1  Introduction The automatic extraction and representation of domain knowledge has been attracting the interest of researchers significantly during the last years  The plethora of available information  the need for intelligent information retrieval  as well as the rise of the semantic web  have motivated information scientists to develop numerous approaches to building thesauri  like dictionaries and Ontologies that are specific to a given domain  Ontologies are hierarchical structures of domain concepts that are enriched with semantic relations linking the concepts together  as well as concept properties  Domain terms populate the ontology  as they are assigned to belong to one or more concepts  and enable the communication and information exchange between domain experts  Furthermore  domain Ontologies enable information retrieval  data mining  intelligent search  automatic translation  question answering within the domain  Building Ontologies automatically to the largest extent possible  i e  keeping manual intervention to a minimum  has first the advantage of an easily updateable extracted ontology  and second of largely avoiding the subjective  i e  biased  impact of domain experts  which is inevitable in manuallybased approaches  This chapter describes the knowledgepoor process of extracting ontological information in the economic domain mostly automatically from Modern Greek text using statistical filters and machine learning techniques  Fig  1 shows the various stages of the process  In a first stage  the text corpora are being preprocessed  Preprocessing includes tokenization  basic morphological tagging and recognition of named and other semantic entities  that are Corpus Preprocessing Term Extraction Semantic Relations Learning Semantic Entity </raw_string>
  </article>
  <article>
    <title>A Comparison of Fraud Cues and Classification Methods for Fake Escrow Website Detection</title>
    <count>1049</count>
    <raw_string>A comparison of fraud cues and classification methods for fake escrow website detection Ahmed Abbasi  Hsinchun Chen Published online  21 July 2009 Springer ScienceBusiness Media  LLC 2009 Abstract The ability to automatically detect fraudulent escrow websites is important in order to alleviate online auction fraud  Despite research on related topics  such as web spam and spoof site detection  fake escrow website categorization has received little attention  The authentic appearance of fake escrow websites makes it difficult for Internet users to differentiate legitimate sites from phonies  making systems for detecting such websites an important endeavor  In this study we evaluated the effectiveness of various features and techniques for detecting fake escrow websites  Our analysis included a rich set of fraud cues extracted from web page text  image  and link information  We also compared several machine learning algorithms  including support vector machines  neural networks  deci sion trees  nave bayes  and principal component analysis  Experiments were conducted to assess the proposed fraud cues and techniques on a test bed encompassing nearly 90000 web pages derived from 410 legitimate and fake escrow websites  The combination of an extended feature set and a support vector machines ensemble classifier enabled accuracies over 90 and 96  for page and site level classification  respectively  when differentiating fake pages from real ones  Deeper analysis revealed that an extended set of fraud cues is necessary due to the broad spectrum of tactics employed by fraudsters  The study confirms the feasibility of using automated methods for detecting fake escrow websites  The results may also be useful for informing existing online escrow fraud resources and communities of practice about the plethora of fraud cues pervasive in </raw_string>
  </article>
  <article>
    <title>The Use of Sequences of Linguistic Categories in Forensic Written Text Comparison Revisited</title>
    <count>1047</count>
    <raw_string>192  The use of sequences of linguistic categories in forensic written text comparison revisited Nria Bel 1  Sheila Queralt Estevez 2  Maria S Spassova 3 and M Teresa Turell 4 1 IULA  Universitat Pompeu Fabra   Spain 234 ForensicLab  IULA  Universitat Pompeu Fabra   Spain 3 New Bulgarian University 1 nuriabelupf edu 2 sheilaqueraltupf edu 3ma spassovaabvbg 4teresaturellupf edu Abstract In recent years  the possibility of studying syntax use through computeraided queries of annotated corpora has led researchers working in the field of forensic written text comparison to explore a new possible marker of authorship  namely  tag sequences as representation of combinations of linguistic categories  A series of studies carried out during the first research stage at ForensicLab using Spanish language data have shown that tag sequences exhibit a significant discriminatory capacity and can be applied to authorship attribution tasks more effectively  In the second research stage reported in this paper  the analysis aims to identify specific traits of each linguistic category implemented in those tags within the exploited tag set which play a major role in the correct classification of texts and those which do not  without losing sight of the fact that either their exclusion or inclusion in tag composition can help to improve this forensic linguistic comparison method  This paper reports on the findings from the statistical testing of several variants of the Institut Universitari de Lingstica Aplicadas  IULA  tag set system and their evaluation in the context of authorship analysis  For testing purposes  a corpus of two types of written texts  novel fragments and newspaper articles   from six contemporary Spanish speaking novelists  was compiled  Furthermore  a subcorpus was used of </raw_string>
  </article>
  <article>
    <title>Comparison of the Performance of Genre Classifiers Trained by Different Machine Learning Algorithms</title>
    <count>1044</count>
    <raw_string>COMPARISON OF THE PERFORMANCE OF GENRE CLASSIFIERS TRAINED BY DIFFERENT MACHINE LEARNING ALGORITHMS Vedrana Vidulin  Mitja Lutrek  Matja Gams Department of Intelligent Systems Joef Stefan Institute Jamova 39  1000 Ljubljana  Slovenia Tel  386 1 477 3147  fax  386 1 425 1038 email  vedrana vidulinijs si ABSTRACT Modern search engines aim at classifying web pages not only according to topics  but also according to genres  This paper presents the results of an attempt to train a genre classifier  We present features extracted from a 20genre corpus used for training the genre classifiers and the results of using different machine learning  ML algorithms in the process of learning  Success of the genre classifiers was measured by accuracy  precision  recall and Fmeasure  Accuracy did not turn out to be a good indicator of classifier success  In the case of other measures the results show that different algorithms should be used for training purposes depending on whether the user wishes to obtain high precision or high recall  1  INTRODUCTION A good question to start with is why we want to classify a web page according to genre  For example  if we are interested in elephants and search for the keyword elephant  we can get as a result links to pages that scientifically describe the life of elephants  but we can also get links to web pages that describe movie with the title Elephant  or a newspaper article about saving the elephants in Africa  However  if we can define that we want to search only for journalistic materials about elephants  than we can get more specific results in accordance with our interest  Classification of web pages according </raw_string>
  </article>
  <article>
    <title>Hiding Secret Information by Automatically Paraphrasing Modern Greek Text with Minimal Resources</title>
    <count>1034</count>
    <raw_string>Hiding Secret Information By Automatically Paraphrasing Modern Greek Text With Minimal Resources Katia Lida Kermanidis Department of Informatics Ionian University Corfu  Greece kermanioniogr Abstract Paraphrasing normally involves sophisticated external resources and semantic thesauri  This paper describes the automatic generation of Modern Greek paraphrases using statistical significance testing for extracting applicable syntactic reordering schemata  Next  supervised filtering helps remove erroneous schemata minding their context  As the paraphrases will be used in steganographic communication  they need not be sophisticated alterations  but significant in number  The proposed process is therefore knowledgepoor  portable to other languages with similar syntax   robust and domainindependent  paraphrasing  statistical significance testing  supervised learning  Modern Greek  linguistic steganography I INTRODUCTION Linguistic steganography is a new field aiming to create systems for hiding information unremarkably underneath a cover text 2  6  A sentence can be expressed in many ways  This redundancy allows the insertion of bits within a sentence by transforming it syntactically 8  andor semantically 3  This work describes the automatic generation of Modern Greek  MG shallow paraphrases to be used in steganographic communication  Unlike most previous approaches to paraphrasing 1  3  10 that focus on generating a few intricate alterations of a sentence and require sophisticated resources  the goals of this approach are different  The first is to produce as many correct paraphrases as possible  as security depends largely on the grammaticality and the number of extracted paraphrases  The second goal is to employ as limited resources as possible  This will allow the portability of the methodology to other languages that share certain syntactic properties with MG Also  it will ensure robustness and domain independence  II  </raw_string>
  </article>
  <article>
    <title>Quantitative Authorship Attribution of Users of Mexican Drug Dealing Related Online Forums</title>
    <count>1030</count>
    <raw_string>QUANTITATIVE AUTHORSHIP ATTRIBUTION OF USERS OF MEXICAN DRUG DEALING RELATED ONLINE FORUMS A Dissertation submitted to the Faculty of the Graduate School of Arts and Sciences of Georgetown University in partial fulfillment of the requirements for the degree of Doctor of Philosophy in Linguistics By Antonio Rico Sulayes  MA Washington  DC November 9  2012 ii Copyright 2012 by Antonio Rico Sulayes All Rights Reserved iii QUANTITATIVE AUTHORSHIP ATTRIBUTION OF USERS OF MEXICAN DRUG DEALING RELATED ONLINE FORUMS Antonio Rico Sulayes  MA Thesis Advisor  Natalie Schilling  PhD ABSTRACT Abstract As the violence in the Mexican drug war escalates  a proliferation of social media sites about drug trafficking in Mexico was followed by the murder of some of their users  and the eventual disappearance of many of those sites  Despite these events  there still exist a number of drug dealing related social media outlets in this country with a large number of contributions  In this dissertation  I show that quantitative authorship attribution techniques  including state of the art machine learning algorithms  can be successfully applied to match posts of unknown authorship with their authors  Employing data from randomly selected prolific users of a drugdealing related online forum  in this research project I test a number of quantitative classification techniques in over a thousand authorship attribution tasks  These tasks attempt to recognize the author of texts  which are chosen to represent anonymous texts  within a closed set of known authors  In the best results rendered in all these experiments  which include corpora with up to 40 potential authors  the accuracy obtained is higher than for previous research using data from drug dealing related online forums and employing discriminant analysis  DA </raw_string>
  </article>
  <article>
    <title>Automated Classification of the Narrative of Medical Reports using Natural Language Processing</title>
    <count>1019</count>
    <raw_string>AUTOMATED CLASSIFICATION OF THE NARRATIVE OF MEDICAL REPORTS USING NATURAL LANGUAGE PROCESSING by Ira Goldstein A Dissertation Submitted to the University at Albany  State University of New York in Partial Fulfillment of the Requirements for the Degree of Doctor of Philosophy College of Computing and Information Department of Informatics 2011 Automated Classification of the Narrative of Medical Reports using Natural Language Processing by Ira Goldstein COPYRIGHT 2011 iii Abstract In this dissertation we present three topics critical to the document level classification of the narrative in medical reports  the use of preferred terminology in light of the presence of synonymous terms  the less than optimal performance of classification systems when presented with a nonuniform distribution of classes  and the problems associated with scarcity of labeled data when presented with an imbalance of classes in the data sets  The literature is replete with instances of conflicting reports regarding the value of applying preferred terminology to improve system performance when presented with synonymous terms  Our study shows that the addition of preferred terms to the text of the medical reports helps to improve true positives for a handcrafted rulebased system and that the addition did not consistently improve performance for the two machine learning systems  We show that the differences in the data  task  and approach can account for the variations in these results as well as the conflicting reports in the literature  The imbalance of classes in data sets can cause suboptimal classification performance by systems based on an exploration of statistics for representing attributes of data  To address this problem  we developed specializing  a panel of oneversusall classifiers  which have been activated in a strict order  and apply it to an imbalanced data set  </raw_string>
  </article>
  <article>
    <title>A Computational Approach to the Analysis and Generation of Emotion in Text</title>
    <count>1019</count>
    <raw_string>A Computational Approach to the Analysis and Generation of Emotion in Text by Fazel Keshtkar Thesis Submitted to the Faculty of Graduate and Postdoctoral Studies in Partial Fulfillment of the Requirements For the Degree of PhD of Computer Science  PhD  OttawaCarleton Institute for Computer Science School of Electrical Engineering and Computer Science University of Ottawa  Fazel Keshtkar  Ottawa  Canada  2011 Abstract Sentiment analysis is a field of computational linguistics involving identification  ex  traction  and classification of opinions  sentiments  and emotions expressed in natural language  Sentiment classification algorithms aim to identify whether the author of a text has a positive or a negative opinion about a topic  One of the main indicators which help to detect the opinion are the words used in the texts  Needless to say  the senti ments expressed in the texts also depend on the syntactic structure and the discourse context  Supervised machine learning approaches to sentiment classification were shown to achieve good results  Classifying texts by emotions requires finergrained analysis than sentiment classification  In this thesis  we explore the task of emotion and mood classification for blog postings  We propose a novel approach that uses the hierarchy of possible moods to achieve better results than a standard flat classification approach  We also show that using sentiment orientation features improves the performance of classification  We used the LiveJournal blog corpus as a dataset to train and evaluate our method  Another contribution of this work is extracting paraphrases for emotion terms based on the six basics emotions proposed by Ekman  happiness  anger  sadness  disgust  sur prise  fear   Paraphrases are different ways to express the same information  </raw_string>
  </article>
  <article>
    <title>A unified data mining solution for authorship analysis in anonymous textual communications</title>
    <count>983</count>
    <raw_string>Information Sciences 231  2013  98112Contents lists available at ScienceDirect Information Sciences journal homepage  wwwelsevier comlocate  insA unified data mining solution for authorship analysis in anonymous textual communications Farkhund Iqbal  Hamad Binsalleeh  Benjamin CM Fung   Mourad Debbabi Concordia Institute for Information Systems Engineering  Concordia University  Montreal  Quebec  Canada H3G 1M8a r t i c l e i n f o Article history  Available online 1 April 2011 Keywords  Authorship identification Authorship characterization Stylometric features Writeprint Frequent patterns Cyber forensics00200255  see front matter 2011 Elsevier Inc doi101016jins 201103006  Corresponding author  Tel   1 514 8482424x59 Email addresses  iqbalfciise concordiaca  F I seconcordiaca  M Debbabi a b s t r a c t The cyber world provides an anonymous environment for criminals to conduct malicious activities such as spamming  sending ransom emails  and spreading botnet malware  Often  these activities involve textual communication between a criminal and a victim  or between criminals themselves  The forensic analysis of online textual documents for addressing the anonymity problem called authorship analysis is the focus of most cybercri me investigations  Authorship analysis is the statistical study of linguistic and computa  tional characteristics of the written documents of individuals  This paper is the first work that presents a unified data mining solution to address authorship analysis problems based on the concept of frequent patternbased writeprint  Extensive experiments on real life data suggest that our proposed solution can precisely capture the writing styles of indi viduals  Furthermore  the writeprint is effective to identify the author of an anonymous text from a group of suspects and to infer sociolinguistic characteristics of the author  2011 Elsevier Inc All </raw_string>
  </article>
  <article>
    <title>A Comparative Study of Different Approaches of Noise Removal for Document Images</title>
    <count>981</count>
    <raw_string>K Deep et al   Eds    Proceedings of the International Conference on SocProS 2011  AISC 130  pp  847854  springerlink com  Springer India 2012 A Comparative Study of Different Approaches of Noise Removal for Document Images Brijmohan Singh1  Mridula3  Vivek Chand1  Ankush Mittal2  and D Ghosh1 1 Research Cell  College of Engineering Roorkee  Roorkee bmsingh1981 vknight078gmailcom  ghoshfeciitrernet in 2 Graphic Era University  Dehradun248002 drankushmittalgmailcom 3 IIT Roorkee  Roorkee mridulaiitrgmailcom Abstract  There has been intensive research carried out in the field of OCR  Opti cal Character Recognition   Lots of work has been done and articles have been published  Noise is one of the important factors which have to be handled at the stage of preprocessing before applying other steps of OCR systems  Noise is un desirable signal because it obscures the subject of the image  This paper presents the comparative study of the five noise removal approaches  Weiner  Median  Wavelet  Contourlet  and Curvelet for document images  The different approaches of noise removal were compared visually and by employing Peak Signal to Noise Ratio  PSNR   Fmeasure and NRM evaluation measures  Keywords  OCR  Noise  Curvelet  Wavelet  Contourlet  Weiner filter  Median filter  1 Introduction Noise removal is necessary step during preprocessing of the document images before applying OCR to it  The output of this step is clean or smoothed document images which can be used efficently as the input to OCR systems  Noise removal is one of the ongoing resarch area in the field of the textual image processing and OCR The era of OCR research is four decades old </raw_string>
  </article>
  <article>
    <title>Preprocessing and Feature Extraction Techniques for Multimodal Interactive Transcription of Text Images</title>
    <count>978</count>
    <raw_string>Preprocessing and Feature Extraction Techniques for Multimodal Interactive Transcription of Text Images Alejandro H Toselli  Veronica Romero  Moises Pastor and Enrique Vidal April 17  2009 Abstract To date  automatic handwriting recognition systems are far from being perfect and heavy human intervention is often required to check and correct the results of such sys  tems  This postediting  process is both inefficient and uncomfortable to the user  An example is the transcription of historic documents  Stateoftheart handwritten text recognition technology is not suitable to perform this task automatically and expensive paleography expert work is needed to achive correct transcriptions  As an alternative to postediting  a multimodal interactive approach is proposed here  where user feedback is provided by means of touchscreen pen strokes andor more traditional keyboard and mouse operation  Users feedback directly allows to improve system accuracy  while multimodality increases system ergonomy and user acceptability  Multimodal inter action is approached in such a way that both the main and the feedback data streams help eachother to optimize overall performance and usability  Empirical tests on three cursive handwritten tasks suggest that  using this approach  significant amounts of user effort can be saved with respect to the conventional  noninteractive  postediting pro cess  01 Introduction Lately  the paradigm for Pattern Recognition  PR  systems design has been shifting from the concept of fullautomaton  i e  systems where no human intervention is as sumed  to systems where the decision process is affected by human feedback  One remarkable PR example where this feedback can be successfully used is handwritten document transcription  This task is becoming an important research topic  specially because of the increasing number of online digital </raw_string>
  </article>
  <article>
    <title>Identification of Versions of the Same Musical Composition by Processing Audio Descriptions</title>
    <count>977</count>
    <raw_string>Identification of Versions of the Same Musical Composition by Processing Audio Descriptions Joan Serr Juli TESI DOCTORAL UPF  2011 Director de la tesi  Dr Xavier Serra i Casals Dept  of Information and Communication Technologies Universitat Pompeu Fabra  Barcelona  Spain Copyright c Joan Serr Juli  2011  Dissertation submitted to the Deptartment of Information and Communica  tion Technologies of Universitat Pompeu Fabra in partial fulfillment of the requirements for the degree of DOCTOR PER LA UNIVERSITAT POMPEU FABRA  with the mention of European Doctor  Music Technology Group  http mtg upf edu  Dept  of Information and Communica  tion Technologies  http www upf edudtic   Universitat Pompeu Fabra  http www  upf edu  Barcelona  Spain  Als meus avis  Acknowledgements I remember I was quite shocked when  one of the very first times I went to the MTG  Perfecto Herrera suggested that I work on the automatic identification of versions of musical pieces  I had played versions  both amateur and pro fessionally  since I was 13 but  although being familiar with many MIR tasks  I had never thought of version identification before  Furthermore  how could they  the MTG people  know that I played song versions  I dont think I had told them anything about this aspect  Before that meeting with Perfe  I had discussed a few research topics with Xavier Serra and  after he gave me feedback on a number of research proposals I had  I decided to submit one related to the exploitation of the temporal information of music descriptors for music similarity  Therefore  when Perfe suggested the topic of version identification I initially thought </raw_string>
  </article>
  <article>
    <title>Detecting Fake Escrow Websites using Rich Fraud Cues and Kernel Based Methods</title>
    <count>977</count>
    <raw_string>Seventeenth Annual Workshop on Information Technologies and Systems  2007 Detecting Fake Escrow Websites using Rich Fraud Cues and Kernel Based Methods Ahmed Abbasi and Hsinchun Chen Artificial Intelligence Lab  Department of Management Information Systems The University of Arizona  Tucson  Arizona 85721  USA aabbasiemail arizona edu  hchenellerarizona edu Abstract  The ability to automatically detect fraudulent escrow websites is important in order to alleviate online auction fraud  Despite research on related topics  fake escrow website categorization has received little attention  In this study we evaluated the effectiveness of various features and techniques for detecting fake escrow websites  Our analysis included a rich set of features extracted from web page text  image  and link information  We also proposed a composite kernel tailored to represent the properties of fake websites  including content duplication and structural attributes  Experiments were conducted to assess the proposed features  techniques  and kernels on a test bed encompassing nearly 90000 web pages derived from 410 legitimate and fake escrow sites  The combination of an extended feature set and the composite kernel attained over 98  accuracy when differentiating fake sites from real ones  using the support vector machines algorithm  The results suggest that automated webbased information systems for detecting fake escrow sites could be feasible and may be utilized as authentication mechanisms  Keywords  Online escrow services  internet fraud  website classification  kernelbased methods 1  Introduction Electronic markets have seen unprecedented growth in recent years  The lack of physical contact and prior interaction makes such places susceptible to opportunistic member behavior 12  Consequently  fraud and deception are highly prevalent in electronic markets  particularly online auctions which account for 50  of </raw_string>
  </article>
  <article>
    <title>Identifying Historical Period and Ethnic Origin of Documents Using Stylistic Feature Sets</title>
    <count>977</count>
    <raw_string>N Lavra  L Todorovski  and KP Jantke  Eds    DS 2006  LNAI 4265  pp  102  113  2006   SpringerVerlag Berlin Heidelberg 2006 Identifying Historical Period and Ethnic Origin of Documents Using Stylistic Feature Sets Yaakov HaCohenKerner1  Hananya Beck1  Elchai Yehudai1  and Dror Mughaz12 1 Department of Computer Science  Jerusalem College of Technology  Machon Lev  21 Havaad Haleumi St  POB 16031  91160 Jerusalem  Israel kerner  hananya  yehuday jctacil  myghazcs biuacil 2 Department of Computer Science  BarIlan University  52900 RamatGan  Israel Abstract  Text classification is an important and challenging research domain  In this paper  identifying historical period and ethnic origin of documents using stylistic feature sets is investigated  The application domain is Jewish Law arti cles written in HebrewAramaic  Such documents present various interesting problems for stylistic classification  Firstly  these documents include words from both languages  Secondly  Hebrew and Aramaic are richer than English in their morphology forms  The classification is done using six different sets of stylistic features  quantitative features  orthographic features  topographic fea  tures  lexical features and vocabulary richness  Each set of features includes various baseline features  some of them formalized by us  SVM has been cho sen as the applied machine learning method since it has been very successful in text classification  The quantitative set was found as very successful and supe rior to all other sets  Its features are domainindependent and language  independent  It will be interesting to apply these feature sets in general and the quantitative set in particular into other domains as well as into other  1 </raw_string>
  </article>
  <article>
    <title>On the Use of Homogenous Sets of Subjects in Deceptive Language Analysis</title>
    <count>977</count>
    <raw_string>Proceedings of the EACL 2012 Workshop on Computational Approaches to Deception Detection  pages 3947  Avignon  France  April 23  27 2012  c2012 Association for Computational Linguistics On the Use of Homogenous Sets of Subjects in Deceptive Language Analysis Tommaso Fornaciari Center for MindBrain Sciences University of Trento tommasofornaciariunitn it Massimo Poesio Language and Computation Group University of Essex Center for MindBrain Sciences University of Trento massimopoesiounitn it Abstract Recent studies on deceptive language sug gest that machine learning algorithms can be employed with good results for classi fication of texts as truthful or untruthful  However  the models presented so far do not attempt to take advantage of the dif  ferences between subjects  In this paper  models have been trained in order to clas  sify statements issued in Court as false or notfalse  not only taking into considera  tion the whole corpus  but also by identify ing more homogenous subsets of producers of deceptive language  The results suggest that the models are effective in recogniz  ing false statements  and their performance can be improved if subsets of homogeneous data are provided  1 Introduction Detecting deceptive communication is a challeng ing task  but one that could have a number of use  ful applications  A wide variety of approaches to the discovery of deceptive statements have been attempted  ranging from using physiological sen sors such as lie detectors to using neuroscience methods  Davatzikos et al   2005  Ganis et al   2003   More recently  a number of techniques have been developed for recognizing deception on the basis of the communicative behavior of subjects  Given the difficulty of the task  many such methods </raw_string>
  </article>
  <article>
    <title>Uma Abordagem Baseada em Genero para Coleta Tematica de Paginas da Web</title>
    <count>977</count>
    <raw_string>Universidade Federal de Minas Gerais Instituto de Ciencias Exatas Departamento de Ciencia da Computacao Uma Abordagem Baseada em Genero para Coleta Tematica de Paginas da Web Guilherme Tavares de Assis Orientador  Alberto Henrique Frade Laender Coorientador  Marcos Andre  Goncalves Belo Horizonte  Marco 2008 Guilherme Tavares de Assis Uma Abordagem Baseada em Genero para Coleta Tematica de Paginas da Web Orientador  Alberto Henrique Frade Laender Coorientador  Marcos Andre  Goncalves Tese apresentada ao Programa de Pos  Graduacao em Ciencia da Computacao da Uni versidade Federal de Minas Gerais  como re  quisito parcial para a obtencao do grau de Doutor em Ciencia da Computacao  Belo Horizonte 27 de Marco de 2008 Guilherme Tavares de Assis A GenreAware Approach to Focused Crawling of Web Pages Advisor  Alberto Henrique Frade Laender Coadvisor  Marcos Andre  Goncalves Thesis presented to the Graduate Program in Computer Science of the Federal University of Minas Gerais in partial fulfillment of the re  quirements for the degree of Doctor in Com  puter Science  Belo Horizonte March 27  2008 A minha mae  Cleonice Tavares de Assis  exemplo de amor  afeto e forca  A memoria de meu pai  Joaquim de Assis Filho  exemplo de saber e vitoria  Agradecimentos Toda trajetoria exige preparacao  esforco e dedicacao  A chegada ao ponto desejado deve ser comemorada com muita alegria e reconhecimento  E difcil  neste momento final  encontrar as palavras certas para expressar o tanto que necessito agradecer  O espaco tornase pequeno demais mas  mesmo assim  quero registrar os meus agradecimentos por tao esperada conquista  Ao meu orientador  Alberto Henrique Frade Laender  por ter acreditado em mim desde o incio da </raw_string>
  </article>
  <article>
    <title>Exploring Discrepancies in Findings Obtained with the KDD Cup 99 Data Set</title>
    <count>973</count>
    <raw_string>Intelligent Data Analysis 15  2011  251276 251 DOI 103233IDA20100466 IOS Press Exploring discrepancies in findings obtained with the KDD Cup 99 data set Vegard Engen  Jonathan Vincent and Keith Phalp Software Systems Research Centre  Bournemouth University  Fern Barrow  Talbot Campus  Poole  UK Abstract  The KDD Cup 99 data set has been widely used to evaluate intrusion detection prototypes  most based on machine learning techniques  for nearly a decade  The data set served well in the KDD Cup 99 competition to demonstrate that machine learning can be useful in intrusion detection systems  However  there are discrepancies in the findings reported in the literature  Further  some researchers have published criticisms of the data  and the DARPA data from which the KDD Cup 99 data has been derived   questioning the validity of results obtained with this data  Despite the criticisms  researchers continue to use the data due to a lack of better publicly available alternatives  Hence  it is important to identify the value of the data set and the findings from the extensive body of research based on it  which has largely been ignored by the existing critiques  This paper reports on an empirical investigation  demonstrating the impact of several methodological differences in the publicly available subsets  which uncovers several underlying causes of the discrepancy in the results reported in the literature  These findings allow us to better interpret the current body of research  and inform recommendations for future use of the data set  Keywords  Machine learning  intrusion detection  KDD Cup 99 data set  methodology 1  Introduction Despite the existence of security mechanisms such as cryptography and the </raw_string>
  </article>
  <article>
    <title>Probabilistic Neural Network with Homogeneity Testing in Recognition of Discrete Patterns Set</title>
    <count>973</count>
    <raw_string>Neural Networks 46  2013  227241Contents lists available at SciVerse ScienceDirect Neural Networks journal homepage  wwwelseviercomlocateneunet Probabilistic neural network with homogeneity testing in recognition of discrete patterns set AV Savchenko  National Research University Higher School of Economics  2512 Bolshaja Pecherskaja Ulitsa  Nizhny Novgorod 603155  Russia a r t i c l e i n f o Article history  Received 21 October 2012 Revised and accepted 6 June 2013 Keywords  Statistical pattern recognition Discrete patterns set Probabilistic neural network Homogeneity testing Face recognition Authorship attribution a b s t r a c t The article is devoted to pattern recognition task with the database containing small number of samples per class  By mapping of local continuous feature vectors to a discrete range  this problem is reduced to statistical classification of a set of discrete finite patterns  It is demonstrated that the Bayesian decision under the assumption that probability distributions can be estimated using the Parzen kernel and the Gaussian window with a fixed variance for all the classes  implemented in the PNN  is not optimal in the classification of a set of patterns  We presented here the novel modification of the PNN with homogeneity testing which gives an optimal solution of the latter task under the same assumption about probability densities  By exploiting the discrete nature of patterns our modification prevents the well  known drawbacks of the memorybased approach implemented in both the PNN and the PNN with homogeneity testing  namely  low classification speed and high requirements to the memory usage  Our modification only requires the storage and processing of the histograms of input and training samples  We present the results of an experimental study in two practically important tasks  </raw_string>
  </article>
  <article>
    <title>Adjectives and Adverbs as Indicators of Affective Language for Automatic Genre Detection</title>
    <count>972</count>
    <raw_string>Adjectives and Adverbs as Indicators of Affective Language for Automatic Genre Detection Robert Rittman and Nina Wacholder School of Communication  Information and Library Studies Rutgers  The State University of New Jersey New Brunswick  New Jersey Abstract  We report the results of a systematic study of the feasibility of automatically classifying documents by genre using adjectives and adverbs as indicators of affective language  In addition to the class of adjectives and adverbs  we focus on two specific subsets of adjectives and adverbs   1  trait adjectives  used by psychologists to assess human personality traits  and  2  speakeroriented adverbs  studied by linguists as markers of narrator attitude  We report the results of our machine learning experiments using Accuracy Gain  a measure more rigorous than the standard measure of Accuracy  We find that it is possible to classify documents automatically by genre using only these subsets of adjectives and adverbs as discriminating features  In many cases results are superior to using the count of  a  nouns  verbs  or punctuation  or  b  adjectives and adverbs in general  In addition  we find that relatively few speakeroriented adverbs are needed in the discriminant models  We conclude that at least in these two cases  the psychological and linguistic literature leads to identification of features that are quite useful for genre detection and for other applications in which identification of style and other nontopical characteristics of documents is important  1 INTRODUCTION This paper reports on the use of adjectives and adverbs to discriminate text genres characterized by affective expressions  e g  fiction  from genres in which affective expressions are typically inappropriate  e g  academic </raw_string>
  </article>
  <article>
    <title>Cross Lingual Text Reuse Detection Based on Keyphrase Extraction and Similarity Measures</title>
    <count>972</count>
    <raw_string>Cross Lingual Text Reuse Detection Based on Keyphrase Extraction and Similarity Measures Rambhoopal Kothwal and Vasudeva Varma International Institute of Information Technology  Hyderabad bhupal iiitresearch iiit acin  vviiit acin Abstract  Information on web in various languages is growing fast  but large amount of content still exists in English  There are several cases of English text reuse  cross language plagiarism  observed in nonEnglish languages  Detecting text reuse in nonEnglish languages is a challeng ing task due to complexity of the language used  Complexity further increases for less resource languages like Arabic and Indian languages  In this paper  we address the problem proposed in FIRE1 CLTR 20112 task of detecting plagiarized documents in Hindi language which was reused from English language source documents  We proposed three ap  proaches using classification and keyphrase retrieval techniques  Our winning approach attained 0792 Fmeasure  1 Introduction Growing information on the web in different languages provide many options for people searching for content  Sometimes similarity between content is observed due to myriad reasons  But reuse or plagiarism of text can be biggest concern for the original publishers  Lot of content on web in the form of text is sometimes rewritten in to different languages from source English documents without any acknowledgment  Text reuse can be at various granularities  It can be a direct copying of phrases  paragraphs or complete document  Challenge here is to identify granularity at which document was plagiarized  Imagining the size of web  it is a tedious task to identify the plagiarized text man ually  Thus  systems which can detect text reuse automatically come to rescue  We submitted three runs in FIRE CLTR 2011 task to solve </raw_string>
  </article>
  <article>
    <title>Natural Language Processing Based Information Retrieval for the Purpose of Author Identification</title>
    <count>972</count>
    <raw_string>International Journal of Information Technology  Management Information System  IJITMIS  ISSN 0976  6605 Print   ISSN 0976  6413 Online  Volume 1  Number 1  May  June  2010    IAEME 45 NATURAL LANGUAGE PROCESSING BASED INFORMATION RETRIEVAL FOR THE PURPOSE OF AUTHOR IDENTIFICATION Mousmi Chaurasia Research Scholar MATS University  Raipur EMail  mousmiashrediffmailcom Dr Sushil Kumar Department of Electrical  Electronics Bhilai Institute of Technology  Bhilai EMail  sk1bitrediffmailcom ABSTRACT With the increasing widespread use of computers and the internet large amount of informations are becoming available on the web Automatic information processing and retrieval are therefore an urgent need In this paper a new approach to automatic authorship identification dealing with realworld text  or unrestricted  is presented  A different approach to identify the author using initial character N gram is proposed  Keywords  Natural Language Processing  author identification  character ngram  dis  similarity measure  INTRODUCTION The rapid expansion of the World Wide Web  WWW in recent years has resulted in the creation of large volumes of text in electronic form  Many Natural Language Processing  NLP  tasks rely on bagofwords and ngrams as an important source of features  NLP applications such as information retrieval and information extraction have been developed to treat this information automatically  Furthermore  in the past few years  character ngrams have successfully been used for namedentity recognition 1  predicting authorship 2  web page genre identification 3  and sentencelevel subjectivity classification 4  International Journal of Information Technology  Management Information System  IJITMIS  ISSN 0976  6605 Print   ISSN 0976  6413 Online  Volume 1  Number 1  May  June </raw_string>
  </article>
  <article>
    <title>Towards the Exploitation of Statistical Language Models for Plagiarism Detection with Reference</title>
    <count>972</count>
    <raw_string>Towards the Exploitation of Statistical Language Models for Plagiarism Detection with Reference Alberto BarronCedeno and Paolo Rosso1 Abstract  To plagiarise is to robe credit of another persons work  Particularly  plagiarism in text means including text fragments  and even an entire document  from an author without giving him the cor respondent credit  In this work we describe our first attempt to detect plagiarised segments in a text employing statistical Language Mod  els  LMs and perplexity  The preliminary experiments  carried out on two specialised and literary corpora  including original  partofspeech and stemmed ver sions   show that perplexity of a text segment  given a Language Model calculated over an author text  is a relevant feature in pla  giarism detection  1 INTRODUCTION The Automatic Plagiarism Detection  a close related problem to the Automatic Authorship Attribution  has became a relevant task in In formation Retrieval  scholar environments and even scientific circles  There are some applications which try  for example  to detect whether a student report is plagiarised or not 2 However  inside of specialised circles  there are cases when a person takes text fragments from other authors without making the corresponding citation and  in extreme cases  different authors claim for the authorship of a text and even an idea  Language Models  commonly used in Speech Recognition 7  and Information Retrieval 11  5  have been exploited in Automatic Au thorship Attribution of text 10  2 and even of source code 4  In the first case  character level ngrams and perplexity are considered to determine the authorship of the analysed document  In the second case  frequencies of byte level </raw_string>
  </article>
  <article>
    <title>The Effect of Author Set Size and Data Size in Authorship Attribution</title>
    <count>966</count>
    <raw_string>The effect of author set size and data size in authorship attribution Kim Luyckx and Walter Daelemans CLiPS Computational Linguistics GroupUniversity of AntwerpBelgium Abstract Applications of authorship attribution in the wildKoppelMSchlerJand ArgamonS2010Authorship attribution in the wildLanguage Resources and EvaluationAdvanced Access published January 122010101007s1057900991112for instance in social networkswill likely involve large sets of candidate authors and only limited data per authorIn this articlewe present the results of a systematic study of two important parameters in supervised machine learning that significantly affect performance in computational authorship attribution 1the number of candidate authors  i ethe number of classes to be learned and  2the amount of training data available per candidate author  i ethe size of the training data  We also investigate the robustness of different types of lexical and linguistic features to the effects of author set size and data sizeThe approach we take is an operationalization of the standard text categorization modelusing memorybased learning for discriminating between the candidate authorsWe performed authorship attribution experiments on a set of three benchmark corpora in which the influence of topic could be controlledThe short text fragments of email length present the approach with a true challengeResults show thatas expectedauthorship attribution accuracy deteriorates as the number of candidate authors increases and size of training data decreasesalthough the machine learning approach continues performing significantly above chanceSome feature types  most notably character ngramsare robust to changes in author set size and data sizebut no robust individual features emerge 1 Introduction Authorship attribution has benefited from increased attention over the past decadein both computational linguistics and digital humanitiesThe dominating approach in computational linguistics consists of a combination of text analysis for extracting document features that are predictive of the authorand text categorization using Machine Learning  MLtechniques Howeveronly limited attention has been paid to </raw_string>
  </article>
  <article>
    <title>Quantitative Evidence for a Hypothesis Regarding the Attribution of Early Buddhist Translations</title>
    <count>966</count>
    <raw_string>Quantitative evidence for a hypothesis regarding the attribution of early Buddhist translations  JenJou Hung  Marcus Bingenheimer and Simon Wiles Library and Information Center  Dharma Drum Buddhist College  Taiwan  ROC  Abstract This article provides quantitative evidence for a hypothesis concerning fourth  century translations of Indian Buddhist texts from Prakrit and Sanskrit into Chinese  Using a Variable Length nGram Feature Extraction Algorithm  princi pal component analysis and average linkage clustering we are able to show that 24 sutras  attributed by the tradition to different translators  were in fact trans  lated by the same translator or group of translators  Since part of our method is based on assigning weight to ngrams  the analysis is capable of yielding distinc  tive features  i e  strings of Chinese characters  that are characteristic of the trans  lator s   This is the first time that these techniques have successfully been applied to medieval Chinese texts  The results of this study open up a number of new directions for the lexicographic and syntactic study of early Chinese translations of Buddhist texts   1  Introduction For several decades now the problem of computa  tional authorship attribution for works written in European languages has led to the emergence of a variety of methods and to some useful results  However  the absence of word separation in both modern and classical Chinese and the grammatical and semantic polyvalence of Chinese characters are just two ways in which authorship attribution poses special problems when it comes to Chinese texts  There have been number of studies focused on developing effective authorship identification approaches for modern Chinese texts  Peng et al   2003  Liu et </raw_string>
  </article>
  <article>
    <title>On Defining Design Patterns to Generalize and Leverage Automated Constraint Solving</title>
    <count>933</count>
    <raw_string>On Defining Design Patterns to Generalize and Leverage Automated Constraint Solving Thiago Serra PETROBRAS Petroleo Brasileiro SAAvenida Paulista90101311100Sao Paulo SPBrazil thiago serrapetrobrascombr AbstractThis position paper reflects on the generalization of adaptive methods for Constraint Programming  CPsolving mechanismsand suggests the use of applicationoriented descriptions as a means to broaden CP adoption in the industryWe regard as an adaptive method any procedure that modifies the behavior of the solving process according to previous experience gathered from similar casesDespite its design being much of a creativity mattermany patterns emerge from the comparison of existing methodsAs a starting pointwe propose a framework with design patterns for learning timesearch modificationand case discriminationThose patterns provide a glimpse of the circumstances in which certain design choices are better suited than othersand thus define a language to handle and address application domains with diverse needsKeywordsAlgorithm SelectionHardness ModelsAutonomous Search 1 Introduction The reliability of a solver is related to its robustness to deal with varied types of problemsConstraint Programming  CPis regarded as a valuable technique due to its expressive modeling capabilitiesHoweverit is crucial that its solving performance matches properly the expectations of such expressivenessOn the one handit has been observed that problems with different characteristics are better suited for different CP search algorithms 13On the other handsearch automation has been seen by some authors as an important step towards ease of useand thus broadening CPs audience 4923Considered togetherthose factors not only foster the development of blackbox solvers in which search definition is abstracted from the modeler 23but also praise for a design that makes a better use of benchmark sets of problems and solver feedbackNamelyrely on them to learn how to select the best algorithm in a real situation instead of electing an anticipated overall winnerAs observed by Selman et al53a misleading use of benchmarks may produce </raw_string>
  </article>
  <article>
    <title>Modality Specific Meta Features for Authorship Attribution in Web Forum Posts</title>
    <count>902</count>
    <raw_string>Proceedings of the 5th International Joint Conference on Natural Language Processing  pages 156164  Chiang Mai  Thailand  November 8  13  2011  c2011 AFNLP Modality Specific Meta Features for Authorship Attribution in Web Forum Posts Thamar Solorio and Sangita Pillay The University of Alabama at Birmingham 1300 University Boulevard Birmingham  AL 35294  USA soloriorsangitacis uabedu Sindhu Raghavan The University of Texas at Austin 1 University Station C0500 Austin  TX 78712  USA sindhucis uabedu Manuel Montes y Gomez The University of Alabama at Birmingham 1300 University Boulevard Birmingham  AL 35294  USA National Institute of Astrophysics  Optics  and Electronics Luis Enrique Erro No 1 Tonantzintla  Puebla  Mexico mmontesgcccinaoepmx Abstract This paper presents a new method for Au thorship Attribution  AA  on online fo rum posts  The idea behind the method is to generate meta features that cap  ture modality specific similarity relations among texts from different authors  Each modality represents a particular linguistic dimension  syntactic  lexical  stylistic   To evaluate this approach we measure predic  tion accuracy on data from an online fo rum with up to 100 candidate authors  We also compare our results with a state of the art approach that has shown to per form well across different genres  We have found the meta features to be especially helpful in the online forum domain  where the documents are very short  showing this to be a very promising direction for AA on a realistic web forum scenario  1 Introduction Authorship attribution  AA  refers to the task of analyzing a document to identify the potential au  thor who wrote the text  Earlier work on this problem involved </raw_string>
  </article>
  <article>
    <title>Enhanced Text Extraction from Arabic Degraded Document Images using EM Algorithm</title>
    <count>901</count>
    <raw_string>Enhanced Text Extraction from Arabic Degraded Document Images using EM Algorithm Wafa Boussellaa1  Aymen Bougacha1  Abderrazak Zahour2  Haikal EL Abed3  Adel Alimi 1 1University of Sfax  REGIM  ENIS  Route Soukra  BPW  3038  Sfax  Tunisia 2IUT  Universit du Havre  Place Robert Schuman  76610 Le Havre  France 3Technical University Braunschweig  Institute for Communication Technology  IfN  Germany Email wafaboussellaa adel alimi elabed ieee org  AymenBougachagmailcom  abderrazak zahourunivlehavre fr Abstract This paper presents a new enhanced text extraction algorithm from degraded document images on the basis of the probabilistic models  The observed document image is considered as a mixture of Gaussian densities which represents the foreground and background document image components  The EM algorithm is introduced in order to estimate and improve the parameters of the mixtures of densities recursively  The initial parameters of the EM algorithm are estimated by the kmeans clustering method  After the parameter estimation  the document image is partitioned into text and background classes by the means of ML approach  The performance of the proposed approach is evaluated on a variety of degraded documents comes from the collections of the National library of Tunisia  1  Introduction The automatic processing of degraded historical documents is a challenge in document image analysis field which is confronted with many difficulties due to the storage condition and the complexity of their content  For historical degraded and poor quality documents  enhancement is not an easy task  The main interest of an enhancement step of historical documents is to remove information coming from the background  Background artifacts can derive from many kinds of degradation  such as scan optical blur and noise  spots </raw_string>
  </article>
  <article>
    <title>Handwritten and Typewritten Word and Character Separation in Unconstrained Document Images</title>
    <count>901</count>
    <raw_string>Handwritten and Typewritten Word and Character Separation in Unconstrained Document Images Jia Tse  Dean Curtis  John Bunch  Christopher Jones  EA Yfantis  and Aaron Thomas Department Computer Science University of Nevada Las Vegas Las Vegas  NV  USA Abstract Separating handwritten and typewritten text within unconstrained paper documents can provide more accurate and efficient OCR results  This paper presents a technique developed that can isolate both the typewrit ten and handwritten portions of a document image  The classification between handwritten and typewritten text occurs at both the character and the word level  Char acters are grouped into words using a word separation technique with an island  grouping method  Struc  tural features of handwritten and typewritten characters are examined  The method developed correctly identi fied with probability close to 100  of the total number of typewritten words in a set of 30 handwritten docu ments  Keywords  Document Image Analysis  OCR  word sep aration 1 Introduction In performing OCR in unconstrained document im  ages  it is important to make a distinction between handwritten and typewritten text because both re  quire its own OCR engines  Techniques for type written and handwritten word classification can oc  cur at three basic levels  the line level  the word level  and the character level  More literature was found on separation that occurs at the line level  Kavallieratou et al analyzed the horizontal profile of a text line 1  They observed that typewritten characters on a line have a relatively stable height whereas the height of each character may vary in handwritten text lines  Classification was based on discriminant analysis  An average accuracy of 982  was achieved </raw_string>
  </article>
  <article>
    <title>Distinction of The Authors of Texts Using Multilayered Feedforward Neural Networks</title>
    <count>900</count>
    <raw_string>Southeast Europe Journal of Soft Computing Volume 1  Number 1 March 2012 128 Distinction of The Authors of Texts Using Multilayered Feedforward Neural Networks Suvad Selman1 Kemal Turan2 Ali Osman Kuaki3 International University of Sarajevo Faculty of Engineering and Natural Sciences Hrasnicka Cesta 15  71000 Sarajevo Bosnia and Herzegovina Abstract This paper proposes a means of using a multilayered feedforward neural network to identify the author of a text  The network has to be trained where multilayer feedforward neural network as a powerful scheme for learning complex inputoutput mapping have been used in learning of the average number of words and average characters of words in a paragraphs of an author  The resulting training information we get will be used to identify the texts written by authors  The computational complexity is solved by dividing it into a number of computationally simple tasks where the input space is divided into a set of subspaces and then combining the solutions to those tasks  By this  we have been able to successfully distinguish the books authored by Leo Tolstoy  from the ones authored by George Orwell and Boris Pasternak  KeywordsMachine learning  author identification  artificial neural networks 1 International University of Sarajevo  Faculty of Engineering and Natural Sciences  Electrical and Electronics Program 2 International University of Sarajevo  Faculty of Engineering and Natural Sciences  Mechanical Engineering Program 3 International University of Sarajevo  Faculty of Engineering and Natural Sciences  Industrial Engineering Program Southeast Europe Journal of Soft Computing Volume 1  Number 1 March 2012 129 INTRODUCTION Individuals have distinctive ways of speaking and writing  and there exists a long history of linguistic and stylistic investigation into author identification  In recent years  practical applications for author </raw_string>
  </article>
  <article>
    <title>Unsupervised Stylistic Segmentation of Poetry with Change Curves and Extrinsic Features</title>
    <count>897</count>
    <raw_string>Unsupervised Stylistic Segmentation of Poetry with Change Curves and Extrinsic Features Julian Brooke Dept of Computer Science University of Toronto jbrookecs torontoedu Adam Hammond Dept of English University of Toronto adam hammondutorontoca Graeme Hirst Dept of Computer Science University of Toronto ghcs torontoedu Abstract The identification of stylistic inconsistency is a challenging task relevant to a number of gen res  including literature  In this work  we carry out stylistic segmentation of a wellknown poem  The Waste Land by TS Eliot  which is traditionally analyzed in terms of numerous voices which appear throughout the text  Our method  adapted from work in topic segmen tation and plagiarism detection  predicts breaks based on a curve of stylistic change which com bines information from a diverse set of features  most notably cooccurrence in larger corpora via reduceddimensionality vectors  We show that this extrinsic information is more useful than  withintext  distributional features  We achieve well above baseline performance on both artifi cial mixedstyle texts and The Waste Land itself  1 Introduction Most work in automated stylistic analysis operates at the level of a text  assuming that a text is stylis  tically homogeneous  However  there are a number of instances where that assumption is unwarranted  One example is documents collaboratively created by multiple authors  in which contributors may  ei ther inadvertently or deliberately  e g  Wikipedia vandalism   create text which fails to form a stylis  tically coherent whole  Similarly  stylistic incon  sistency might also arise when one of the contrib  utors  is actually not one of the purported authors of the work at all  that is  in cases of plagia  rism  </raw_string>
  </article>
  <article>
    <title>Discrimination of Handwritten and Machine Printed Text in Scanned Document Images</title>
    <count>896</count>
    <raw_string>Chapter 46 Discrimination of Handwritten and Machine Printed Text in Scanned Document Images Surabhi Narayan and Sahana D Gowda Abstract Discrimination of handwritten and machine printed text in a scanned document image is an important process as the Optical Character Recognizers  OCRs  available are domain specific  In this paper  a novel approach has been proposed to discriminate handwritten and machine printed word components based on the structure  In the binarized form of the word component  due to the infor mative foreground overlay on the null background  transitions from 01 and 10 occur at the contour of the component structure  The count and occurrence of these transitions are used to discriminate handwritten and machine printed word com ponents  The proposed method is robust and simple  Extensive experimentation has been conducted over a wide range of data samples  English words   Keywords Document image Machine printed word Handwritten word Transition Component structure Critical lines 461 Introduction Despite the fact that offices are moving into paperless  document existence is still necessary in many places such as reservation forms  bank cheques  application forms  tax returns  and legal documents  These document images contain hand  written and machine printed word components  Information extraction from these document images is cumbersome due to the presence of handwritten and machine printed components together  Discrimination of handwritten and machine printed S Narayan    S D Gowda Department of CSE  BNM Institute of Technology  Bangalore  India email  surabhi narayangmailcom S D Gowda email  sahanagowdarediffmailcom V Chakravarthi et al  eds    Proceedings of International Conference on VLSI  Communication  Advanced Devices  Signals  Systems and Networking VCASAN2013   </raw_string>
  </article>
  <article>
    <title>Pattern Recognition Approach for Music Style Identification Using Shallow Statistical Descriptors</title>
    <count>896</count>
    <raw_string>248 IEEE TRANSACTIONS ON SYSTEMS  MAN  AND CYBERNETICSPART C  APPLICATIONS AND REVIEWS  VOL 37  NO 2  MARCH 2007 Pattern Recognition Approach for Music Style Identification Using Shallow Statistical Descriptors Pedro J Ponce de Leon and Jose  M Inesta Abstract In the field of computer music  pattern recognition algorithms are very relevant for music information retrieval appli cations  One challenging task in this area is the automatic recogni tion of musical style  having a number of applications like index  ing and selecting musical databases  From melodies symbolically represented as digital scores  standard musical instrument digital interface files   a number of melodic  harmonic  and rhythmic sta  tistical descriptors are computed and their classification capability assessed in order to build effective description models  A frame  work for experimenting in this problem is presented  covering the feature extraction  feature selection  and classification stages  in such a way that new features and new musical styles can be eas  ily incorporated and tested  Different classification methods  like Bayesian classifier  nearest neighbors  and selforganizing maps  are applied  The performance of such algorithms against different description models and parameters is analyzed for two particular musical styles  jazz and classical  used as an initial benchmark for our system  Index TermsBayesian classifier  music style classification  nearest neighbors  selforganizing maps  SOMs  I INTRODUCTION COMPUTER music research is an emerging area for pattern recognition and machine learning techniques to be applied  The contentbased organization  indexing  and explo ration of digital music databases  digital music libraries   where digitized  MP3   sequenced musical instrument digital inter face </raw_string>
  </article>
  <article>
    <title>Word Level Handwritten and Printed Text Separation Based on Shape Features</title>
    <count>896</count>
    <raw_string>International Journal of Emerging Technology and Advanced Engineering Website  wwwijetae com  ISSN 22502459  Volume 2  Issue 4  April 2012  590 Word Level Handwritten and Printed Text Separation Based on Shape Features Upasana Patil 1  Masarath Begum 2 12 Department of Computer Science  GND Engineering College  Bidar  India 1 upasanapatilgmailcom 2 masarath456gmailcom Abstract In this paper  we present a method for discriminating handwritten and printed text from document images based on shape features  The separation of handwritten and printed text from document image is essential to optimize the OCR accuracy and to activate an appropriate OCR engine  It leads to reduce the search space of the OCR and it also facilitates the retrieval of Handwritten and Printed text from document images  We have used IAM dataset 30 and with morphological transformations segmented 74 pages and obtained 10768 words and 2000 were used for experimentation and achieved average accuracy of 9857  with only seven features  The proposed method is simple  have promising discrimination accuracy and less time complexity as compared to 10  KeywordsDocument Image Analysis  OCR  Shape Features Handwritten and Printed Text  I INTRODUCTION Integration of new technologies and inventions leads us towards the achievement of paperless office and paperless society  Document image analysis is one of the import steps in automating the offices  Every activity of the office involves papers  which are in the form of petitions  application forms  reports  letters and accounts  In most of the situations we come across with numerous documents presenting a mixture of handwritten and printed text  For example  railway reservation forms  bank cheques  memorandums etc  Often we notice that interlacing of handwritten </raw_string>
  </article>
  <article>
    <title>The Authorship of the Disputed Federalist Papers with an Annotated Corpus</title>
    <count>896</count>
    <raw_string>This article was downloaded by   University of Aegean  On  25 May 2012  At  05 17 Publisher  Routledge Informa Ltd Registered in England and Wales Registered Number  1072954 Registered office  Mortimer House  3741 Mortimer Street  London W1T 3JH  UK English Studies Publication details  including instructions for authors and subscription information  http wwwtandfonline comloinest20 The Authorship of the Disputed Federalist Papers with an Annotated Corpus Antonio MirandaGarca a  Javier CalleMartn a a University of Mlaga  Spain Available online  22 May 2012 To cite this article  Antonio MirandaGarca  Javier CalleMartn  2012   The Authorship of the Disputed Federalist Papers with an Annotated Corpus  English Studies  933  371390 To link to this article  http dx doi org1010800013838X2012668795 PLEASE SCROLL DOWN FOR ARTICLE Full terms and conditions of use  http wwwtandfonline compagetermsand  conditions This article may be used for research  teaching  and private study purposes  Any substantial or systematic reproduction  redistribution  reselling  loan  sublicensing  systematic supply  or distribution in any form to anyone is expressly forbidden  The publisher does not give any warranty express or implied or make any representation that the contents will be complete or accurate or up to date  The accuracy of any instructions  formulae  and drug doses should be independently verified with primary sources  The publisher shall not be liable for any loss  actions  claims  proceedings  demand  or costs or damages whatsoever or howsoever caused arising directly or indirectly in connection with or arising out of the use of this material  The Authorship of the Disputed Federalist Papers with an Annotated Corpus Antonio MirandaGarca </raw_string>
  </article>
  <article>
    <title>Distinguishing Editorial and Customer Critiques of Cultural Objects Using Text Mining</title>
    <count>895</count>
    <raw_string>Distinguishing Editorial and Customer Critiques of Cultural Objects Using Text Mining Xiao Hu  xiaohuuiucedu  University of Illinois at UrbanaChampaign J Stephen Downie jdownieuiucedu  University of Illinois at UrbanaChampaign Andreas Ehmann  aehmannuiucedu  University of Illinois at UrbanaChampaign 1  Introduction  T here exists a large number of critical writings regardinghumanities objects such as reviews  forum posts  mailing lists and blogs  In many cases  readers do not necessarily know the authenticity and credibility of such writings  It is desirable to have a tool that is able to distinguish professional criticisms from lay comments  and furthermore  tomeasure the authenticity of criticisms on humanities objects  Such tools can have many applications ranging from mass mail filtering to customized criticism recommendation and summarization  In a preceding study we demonstrated that a simple machine learning model can be used to automatically differentiate editorial critiques  i e   those written by professional critics  from customer critiques  i e   those written by interestedmembers of the general public   Hu et al  2006a   In this poster  we extend and build upon our earlier work to include a new class of cultural objects  i e   United States Literature  and to uncover the set of influential features that contribute to making editorial and customer  reviews distinct  For the sake of comparison  we use the same dataset as in  Hu et al 2006a   namely reviews from amazoncom  the largest online retailer of various humanities materials including books andmusic  On amazoncom  many book andmusic objects have both editorial reviews and customer reviews  The former are written by editors in amazoncom  who can </raw_string>
  </article>
  <article>
    <title>Designing an Interface for Multilingual and Multiscript Database of Medieval Manuscripts</title>
    <count>894</count>
    <raw_string>Designing an Interface Shafi  Nadim DESIGNING AN INTERFACE FOR MULTILINGUAL AND MULTISCRIPT DATABASE OF MEDIEVAL MANUSCRIPTS S M Shak Nadim Akhtar Khan  ABSTRACT The paper discusses the design of an interface for developing a  database of medieval manuscripts  for accommodating data elements in multilingual and multiscript medium especially in Arabic  Urdie  and Persian using  Unicode character sets  with Visual Basic in front end and Ms Access in back end  KEY WORDS Unicode  Multilingual Interface  Medieval Manuscripts  INTRODUCTION Museums  archivesand libraries collect information sources in a variety of rare and fragile media and many institutions and libraries have undertaken initiatives to exploit the potential of digital technology for archiving  retrieving and displaying such unique and fragile materials  Medieval  Professor  DLIS  The University of Kashmir  SrinagarJK 190006 email shatism2000yahooeoin  Faculty  DLIS  The University of Kashmir  SrinagarJK 190006 TRIM V2  2  JulyDecember  2006 117 Designing an Interface Shafi  Nadim manuscripts  rich sources of culture and history  are available in different formats like parchment  vellum  palm leaves  paper  birch bark  papyrus  wood  stones etc  languages and scripts in various collection sizes and conditions  The advancement in information technology have made it possible to preserve  organize and disseminate them in more economic and effective way  With fast developments in software and database management systems to design a database in various languagesscripts has become more convenient and possible  It is with this background that present paper reports designing of an interface for a multilingual database of medieval manuscripts  PROBLEM a  Most interfaces are usually in English language  Therefore  exploring possibility of </raw_string>
  </article>
  <article>
    <title>Combining Integrated Sampling with SVM Ensembles for Learning from Imbalanced Datasets</title>
    <count>893</count>
    <raw_string>Combining integrated sampling with SVM ensembles for learning from imbalanced datasets Yang Liu a  Xiaohui Yu ab  Jimmy Xiangji Huang b   Aijun An c a School of Computer Science and Technology  Shandong University  Jinan  China b School of Information Technology  York University  Toronto  Canada cDepartment of Computer Science and Engineering  York University  Toronto  Canada a r t i c l e i n f o Article history  Received 16 July 2009 Received in revised form 25 September 2010 Accepted 12 November 2010 Available online 17 December 2010 Keywords  Data sampling Classification Imbalanced data mining a b s t r a c t Learning from imbalanced datasets is difficult  The insufficient information that is associ ated with the minority class impedes making a clear understanding of the inherent struc  ture of the dataset  Most existing classification methods tend not to perform well on minority class examples when the dataset is extremely imbalanced  because they aim to optimize the overall accuracy without considering the relative distribution of each class  In this paper  we study the performance of SVMs which have gained great success in many real applications  in the imbalanced data context  Through empirical analysis  we show that SVMs may suffer from biased decision boundaries  and that their prediction performance drops dramatically when the data is highly skewed  We propose to combine an integrated sampling technique  which incorporates both oversampling and undersampling  with an ensemble of SVMs to improve the prediction performance  Extensive experiments show that our method outperforms individual SVMs as well as several other stateoftheart classifiers  Crown Copyright 2010 Published by Elsevier Ltd All rights reserved  1  Introduction </raw_string>
  </article>
  <article>
    <title>Comparison of Classifier Fusion Methods for Classification in Pattern Recognition Tasks</title>
    <count>893</count>
    <raw_string>Comparison of Classifier Fusion Methods for Classification in Pattern Recognition Tasks Francisco MorenoSeco  Jose  M Inesta  Pedro J Ponce de Leon  and Luisa Mico Department of Software and Computing Systems University of Alicante PO box 99  E03080 Alicante  Spain paco  inesta  pierre  micodlsi uaes http grfiadlsi uaes Abstract  This work presents a comparison of current research in the use of voting ensembles of classifiers in order to improve the accuracy of single classifiers and make the performance more robust against the difficulties that each individual classifier may have  Also  a number of combination rules are proposed  Different voting schemes are discussed and compared in order to study the performance of the ensemble in each task  The ensembles have been trained on real data available for bench  marking and also applied to a case study related to statistical description models of melodies for music genre recognition  1 Introduction Combining classifiers is one of the most widely explored methods in pattern recognition in the recent years  These techniques have been shown to reduce the error rate in classification tasks in opossite to single classifiers  Also  the com bination of different techniques to make a final decision makes the performance of the system more robust against the difficulties that each individual classifier may have on each particular data set  Different reasons have been argued for this behaviour  amongst others  statistical  computational or representational reasons 1  Several different approaches have been used to obtain classifier ensembles  As stated in a recent work by Duin 2  base classifiers should be different  but they should be comparable as well  Also  works on this subject point out the </raw_string>
  </article>
  <article>
    <title>Employing Thematic Variables for Enhancing Classification Accuracy Within Author Discrimination Experiments</title>
    <count>891</count>
    <raw_string>Employing Thematic Variables for Enhancing Classification Accuracy Within Author Discrimination Experiments  George Tambouratzis and Marina Vassiliou Institute for Language and Speech Processing  Paradissos Amaroussiou  151 25 Athens  Greece  Abstract This article reports on experiments performed with a large corpus  aiming at separating texts according to the author style  The study initially focusses on whether the classification accuracy regarding the author identity may be improved  if the text topic is known in advance  The experimental results indicate that this kind of information contributes to more accurate author recognition  Furthermore  as the diversity of a topic set increases  the classification accuracy is reduced  In general  the experimental results indicate that taking into account knowledge regarding the text topic can lead to the construction of specialized models for each author with higher classification accuracy  For example  by focussing on a specific topic  the accuracy with which the author identity is determined increases  the exact amount depending on the specific topic  This also applies when the topic of the text is more broadly determined  as a set of topic categories  In an associated task  the most salient parameters within an 85parameter vector are studied  for a number of subsets of the corpus  where each subset contains speeches from a single topic  These studies indicate that the salient parameters are the same for the different subsets  Two fixed data vectors have been defined  using 16 and 25 parameters  respectively  The classification accuracy obtained  even with the smallest data vector  is only 5  less than with the complete vector  This indicates that the parameters retained in the reduced vectors bear a large amount </raw_string>
  </article>
  <article>
    <title>An Application of Neural Networks to Sequence Analysis and Genre Identification</title>
    <count>891</count>
    <raw_string>An Application of Neural Networks to Sequence Analysis and Genre Identification David Bisant  PhD The Laboratory for Physical Sciences 8050 Greenmead Drive College Park  MD 20740 bisantumbcedu Abstract This study borrowed sequence analysis tech  niques from the genetic sciences and applied them to a similar problem in email filtering and web searching  Genre identification is the process of determining the type or family of a given document  For example  is the document a letter  a news story  a horoscope  a joke  or an advertisement  Genre identification allows a computer user to further filter email and web sites in a way that is totally different than topicbased meth  ods  This study presents original research in an appli cation of neural networks to the genre identification problem  The data for the study came from a database constructed by the author and his colleagues  The data consisted of descriptive features and the genre classifi cation  as judged by a human  from over 5000 different documents  Ten different genres were represented  The descriptive features consisted of 89 different mea  surements of each document such as average word length  the number of numeric terms  the proportion of present tense verbs  etc  The data was divided into 2 sets  with 75  for training and 25  for testing  The first neural network applied was a very basic single layer network that achieved 79  correct classifications on the testing data  This performance was equivalent to the previous best method on the problem  decision trees  When more complex neural networks were applied to the problem  performance increased significantly  The best performance of 86  correct </raw_string>
  </article>
  <article>
    <title>On the Relevance of Search Space Reduction in Automatic Plagiarism Detection</title>
    <count>891</count>
    <raw_string>On the Relevance of Search Space Reduction in Automatic Plagiarism Detection  Sobre la importancia de la reduccion del espacio de busqueda en la deteccion automatica de plagio Alberto BarronCedeno and Paolo Rosso Natural Language Engineering Lab   ELiRF Dpto  Sistemas Informaticos y Computacion Universidad Politecnica de Valencia DSIC  edificio 1F Campus de Vera Camino de Vera sn  46022 Valencia  Spain lbarron  prossodsicupves Resumen  En la deteccion automatica de plagio con referencia  los fragmentos de texto de un documento sospechoso son buscados de manera exhaustiva en un conjunto de documentos originales  de referencia  con el objetivo de determinar si han sido plagiados o no  Uno de los factores mas importantes para el exito de este tipo de aplicaciones es el tamano del corpus de referencia el cual  al mismo tiempo  puede representar un problema al considerar el desempeno y la precision  En este artculo  abordamos la deteccion automatica de plagio con referencia analizando el impacto de una etapa previa de reduccion del espacio de busqueda  conformado por los documentos originales en el corpus de referencia   Nuestros experimentos sobre el corpus METER muestran una mejora en la Precision y Cobertura de los resultados obtenidos cuando la reduccion del espacio de busqueda es realizada al principio del proceso de deteccion de plagio  Palabras clave  deteccion de plagio  reduccion del espacio de busqueda  similitud de texto Abstract  In automatic plagiarism detection with reference  the text fragments in a suspicious document are exhaustively searched in a set of original  reference  documents in order to determine whether they have been plagiarised or not  One of the most important factors for the success of this kind of applications is </raw_string>
  </article>
  <article>
    <title>A Machine Learning Approach for Analyzing Musical Expressions of Piano Performance</title>
    <count>891</count>
    <raw_string>A Machine Learning Approach for Analyzing Musical Expressions of Piano Performance KuoLiang Ou  PaoTe Tsai  Wernhuar Tarng Graduate Instute of Computer Science  National HsinChu Uinversity of Education kloutwgmailcom Abstract This paper proposed a machine learning approach for analyzing teachers  expert knowledge of classifying students  piano performance into approximate expression categories  Students are usually confused when learning the expressive performance because of teachers  subjective intention difference on the same performance  In this paper  teacher models will be built by analyzing teachers  classification rules  By replaying their performances and read teachers  suggestions in graphical and textual modes which are generated automatically by teacher model  students could understand the nuance of performance features on each expression  Three teachers and ten students joined this experiment  Sixty piano performances were recorded for constructing the teacher models  The average accuracy of teacher models for classifying performance expression is 708   Questionnaires reflect both teachers and students are satisfied with the user interface  generated suggestions  and classification rules  1  Introduction The expressive marks on music staves support the communications between composers and the performers about the performance expression with the limitations of the scores 1  For example  a music scores with dolce expressive mark represents the composers  implication for a sweet and tender music  Therefore  a dolce music usually trend to be performed with lower volume and graceful rhythm  on the other hand  a music scores with agitato expressive make represents the implication for a agitate music  Consequently  an agitato music often trend to be performed with higher volume and sharp rhythm  In fact  there is neither formal definition nor detail description of musical expressive </raw_string>
  </article>
  <article>
    <title>An Experimental Study of Pruning Techniques in Handwritten Text Recognition Systems</title>
    <count>891</count>
    <raw_string>An Experimental Study of Pruning Techniques in Handwritten Text Recognition Systems Daniel MartnAlbo  Veronica Romero  and Enrique Vidal Departamento de Sistemas Informaticos y Computacion Universitat Politecnica de Valencia  Spain dmartinalbo vromero evidal  dsicupves http prhlt iti upves  Abstract  Handwritten Text Recognition is a problem that has gained attention in the last years mainly due to the interest in the transcription of historical documents  However  the automatic transcription of hand  written documents is not error free and human intervention is typically needed to correct the results of such systems  This interactive scenario demands realtime response  In this paper  we present a study compar ing how different pruning techniques affect the performance of two freely available decoding systems  HTK and iATROS These two systems are based on Hidden Markov Models and ngram language models  However  while HTK only considers 2gram language models  iATROS works with ngrams of any order  In this paper  we also carried out a study about how the use of ngrams of size greater than two can enhance results over 2grams  Experiments are reported with the publicly available ESPOS ALLES database  Keywords  Handwritten Text Recognition  pruning techniques  language models  ngrams  1 Introduction Lately  the paradigm for Pattern Recognition  PR  has been shifting from fully automatic systems to systems where the user interacts with the system to obtain the final result 11  One remarkable pattern recognition example where this in teraction can be successfully used is in handwritten document transcription 10  This task is becoming an important research topic  specially because of the in creasing number of digital libraries publishing large quantities of digitized legacy documents  Given that </raw_string>
  </article>
  <article>
    <title>A Generic Architecture for a Social Network Monitoring and Analysis System</title>
    <count>890</count>
    <raw_string>A generic Architecture for a Social Network Monitoring and Analysis System Alexander Semenov  Jari Veijalainen Dept  of CSIS University of Jyvskyl 40014 Univ of Jyvskyl  Finland alexander semenov  jari veijalainen jyufi Work was done when the author was the doctoral student of two universities  University ITMO and University of Jyvskyl Alexander Boukhanovsky eScience Research Institute  The National Research University of Information Technologies  Mechanics and Optics  University ITMO  197101  Russia  Saint Petersburg  Kronverkskiy pr  49 avbmailmail ru This paper describes the architecture and a partial implementation of a system designed for the monitoring and analysis of communities at social media sites  The main contribution of the paper is a novel system architecture that facilitates longterm monitoring of diverse social networks existing and emerging at various social media sites  It consists of three main modules  the crawler  the repository and the analyzer  The first module can be adapted to crawl different sites based on ontology describing the structure of the site  The repository stores the crawled and analyzed persistent data using efficient data structures  It can be implemented using special purpose graph databases andor objectrelational database  The analyzer hosts modules that can be used for various graph and multimedia contents analysis tasks  The results can be again stored to the repository  and so on  All modules can be run concurrently  Softwares architecture  web crawling  Social network monitoring  social network evolution  dynamic social network analysis I INTRODUCTION One of the most rapidly developing phenomena of modern age is the Internet and the simultaneous digitally encoded information accumulation on it  All digitally stored or produced data can be divided to public  semipublic and </raw_string>
  </article>
  <article>
    <title>Advanced Text Authorship Detection Methods and their Application to Biblical Texts</title>
    <count>889</count>
    <raw_string>Advanced text authorship detection methods and their application to biblical texts Tlis J Putnia  Domenic J Signorielloa  Samant Jaina  Matthew J Berrymana and Derek Abbotta aCentre for Biomedical Engineering and School of Electrical and Electronic Engineering  The University of Adelaide  SA 5005  Australia  ABSTRACT Authorship attribution has a range of applications in a growing number of fields such as forensic evidence  plagiarism detection  email filtering  and web information management  In this study  three attribution techniques are extended  tested on a corpus of English texts  and applied to a book in the New Testament of disputed authorship  The word recurrence interval method compares standard deviations of the number of words between successive occurrences of a keyword both graphically and with chisquared tests  The trigram Markov method compares the probabilities of the occurrence of words conditional on the preceding two words to determine the similarity between texts  The third method extracts stylometric measures such as the frequency of occurrence of function words and from these constructs text classification models using multiple discriminant analysis  The effectiveness of these techniques is compared  The accuracy of the results obtained by some of these extended methods is higher than many of the current state of the art approaches  Statistical evidence is presented about the authorship of the selected book from the New Testament  Keywords  Authorship attribution  stylometry  word recurrence interval  trigram Markov model  multiple discriminant analysis  1  INTRODUCTION Authorship attribution is the problem of identifying the author of an anonymous text  or text whose authorship is in doubt1  The techniques of authorship attribution have a range of applications in a growing number of fields such as </raw_string>
  </article>
  <article>
    <title>Modeling Musical Attributes to Characterize Ensemble Recordings Using Rhythmic Audio Features</title>
    <count>889</count>
    <raw_string>MODELING MUSICAL ATTRIBUTES TO CHARACTERIZE ENSEMBLE RECORDINGS USING RHYTHMIC AUDIO FEATURES Jakob AbeerOlivier LartillotChristian DittmarTuomas EerolaGerald SchullerFraunhofer IDMTIlmenauGermany Finnish Centre of Excellence in Interdisciplinary Music ResearchUniversity of JyvaskylaFinland ABSTRACT In this paperwe present the results of a prestudy on music performance analysis of ensemble musicOur aim is to implement a music classification system for the description of live recordingsfor instance to help musicologist and musicians to analyze improvised ensemble performancesThe main problem we deal with is the extraction of a suitable set of audio features from the recorded instrument tracksOur approach is to extract rhythmrelated audio features and to apply them for regressionbased modeling of eight more general musical attributesThe model based on Partial LeastSquares Regression without preceding Principal Component Analysis performed best for all of the eight attributesIndex TermsMusic performance analysisimprovisationmicrotimingregression analysisonset detection 1INTRODUCTION Improvisation and ensemble play are common techniques in modern music genres like swingbluesor funkbut have rarely been studied in the Music Information Retrieval literatureIn this paperwe investigate trio ensembles including bass guitarelectric guitarand drumsProfessional multitrack recordings allow the separate analysis of all instrument tracks in order to omit the errorprone step of source separationThis paper is organized as followsWe outline the goals of this publication in Sect2 and give a brief overview over related work in Sect3In Sect4we explain the music recording and data annotation processFurthermorewe illustrate the extracted audio features and the regression analysis configurations that we evaluatedFinallywe discuss the results in Sect5 and give a conclusion in Sect62GOALS In this workwe aim to develop a estimation model that allows to automatically characterize music ensemble performances The first author performed parts of the work between May and August 2010 during a research stay at the Department of MusicUniversity of JyvaskylaAll correspondence shall be addressed to abridmtfraunhoferde in terms of different attributesIn order to facilitate </raw_string>
  </article>
  <article>
    <title>On Building an Automatic Text Classification Model with Minimal Computational Costs</title>
    <count>882</count>
    <raw_string>743 ON BUILDING AN AUTOMATIC TEXT CLASSIFICATION MODEL WITH MINIMAL COMPUTATIONAL COSTS NEREA MARTNEZ AROCA Universidad de Murcia ABSTRACT The creation of large volumes of texts and text databases in electronic form has been the result of the recent and rapid expansion of the WWW Nevertheless  the major problem keeps on being the difficulty of accessing relevant information on a particular topic  Automated text categorisation or text classification has raised a great interest in the last decades for its applicability to Internet as well as to other fields such as document organisation and word sense disambiguation  Sebastiani 1999  4  It is the aim of the current paper to try and design a model of automatic text classification which allows text category discrimination as a prior step to new case assignment to previously established text categories on the basis of a series of linguistic and easily computable parameters and thus  reduced computational costs  For the purposes of the present pilot study we searched for those linguistic features which have been found to be reliable style markers  and may thus discriminate well among the categories analysed in our corpus Cooking recipes  Ecology  Music  Oncology  Physics and Religion  and which can also be computed from unnanotated text  KEYWORDS  automated text classification  discriminant analysis  classification functions  RESUMEN La creacin de grandes volmenes de textos y bases de datos en formato electrnico es resultado de la reciente y rpida expansin de Internet  El mayor problema  no obstante  reside todava en las dificultades de acceso a informacin relevante en cualquier mbito  La clasificacin automtica de textos nace de la necesidad imperante de organizar documentos  agruparlos  para su posterior eficaz recuperacin  El </raw_string>
  </article>
  <article>
    <title>A Methodology for Developing User Interfaces to Workflow Information Systems</title>
    <count>877</count>
    <raw_string>A Methodology for Developing User Interfaces to Workflow Information Systems By Josefina Guerrero Garcia A dissertation submitted in fulfillment of the requirements for the degree of Doctor of Philosophy in Economics and Management Sciences Option Information Systems of the Universit catholique de Louvain Committee in charge  Prof Jean Vanderdonckt  Universit catholique de Louvain  Advisor Prof Manuel Kolp  Universit catholique de Louvain  Examiner Prof Michal Petit  Facults Univ Notre Dame de la Paix  Examiner Prof Stphane Faulkner  Facults Univ Notre Dame de la Paix  Examiner Prof Philippe Palanque  Universit Paul Sabatier  Reader Prof Marco Winckler  Universit Paul Sabatier  Reader Prof Philippe Chevalier  Universit catholique de Louvain  President of Jury June 2010 Abstract Supporting business processes through the help of workflow systems is a necessary prerequisite for many companies to stay competitive  An important task is the specification of workflow  i e  the parts of a business process that can be supported by a computerbased system  This thesis introduces a methodology for developing user interfaces for a workflow information system in a systematic way  The methodology involves a set of models that capture the various aspects required for this purpose  a user interface description language to specify the corresponding user interface  a method to structure the usage of these models  and software support  The methodology is delineated by a set of requirements that are elicited and motivated by the state of the art and relying on a framework to model workflow  The validation of the proposed methodology is achieved by applying it over different realworld case studies belonging to different domains of human activity  The methodology provides designers with methodological guidance on how to derive user </raw_string>
  </article>
  <article>
    <title>A Reassessment of the Authorship of the Cheap Repository Tracts</title>
    <count>872</count>
    <raw_string>ABSTRACT A Reassessment of the Authorship of the Cheap Repository Tracts Anna Maree Blanch  MA Mentor  James E Barcus  PhD This thesis contends that the extent of Hannah Mores contribution to the Cheap Repository has never been definitively established and that there is a need for clarification of the authorship of individual tracts  This thesis details the current state of bibliographical scholarship for the Cheap Repository Tracts and identifies which tracts can be attributed conclusively  which can only be made tentatively  and those for which attribution must be reserved pending a nontraditional attribution study  This thesis will also explore the context in which the Cheap Repository Tracts were published and claims that ascertaining the authorship of individual tracts contributes productively to future criticism of the tracts within their sociohistorical  theological and political context  Page bearing signatures is kept on file in the Graduate School  A Reassessment of the Authorship of the Cheap Repository Tracts by Anna Maree Blanch  BA Hons   LLB A Thesis Approved by the Department of English                                   Dianna Vitanza  Ph D  Chairperson Submitted to the Graduate Faculty of Baylor University in Partial Fulfillment of the Requirements for the Degree of Master of Arts Approved by the Thesis Committee                                   James E Barcus  Ph D  Chairperson          </raw_string>
  </article>
  <article>
    <title>A Framework for Figurative Language Detection Based on Sense Differentiation</title>
    <count>857</count>
    <raw_string>Proceedings of the ACL 2010 Student Research Workshop  pages 6772  Uppsala  Sweden  13 July 2010  c2010 Association for Computational Linguistics A Framework for Figurative Language Detection Based on Sense Differentiation Daria Bogdanova University of Saint Petersburg Saint Petersburg dashabogdanovagmailcom Abstract Various text mining algorithms require the process of feature selection  Highlevel se mantically rich features  such as figurative language uses  speech errors etc  are very promising for such problems as eg  writ ing style detection  but automatic extrac  tion of such features is a big challenge  In this paper  we propose a framework for figurative language use detection  This framework is based on the idea of sense differentiation  We describe two algo  rithms illustrating the mentioned idea  We show then how these algorithms work by applying them to Russian language data  1 Introduction Various text mining algorithms require the pro cess of feature selection  For example  author ship attribution algorithms need to determine fea  tures to quantify the writing style  Previous work on authorship attribution among computer scien tists is mostly based on lowlevel features such as word frequencies  sentence length counts  ngrams etc  A significant advantage of such features is that they can be easily extracted from any corpus  But the study by Batov and Sorokin  1975  shows that such features do not always provide accurate measures for authorship attribution  The linguistic approach to the problem involves such highlevel characteristics as the use of figurative language  irony  sound devices and so on  Such character  istics are very promising for the mentioned above tasks  but the extraction of these features is ex  tremely hard </raw_string>
  </article>
  <article>
    <title>Word Prediction Techniques for User Adaptation and Sparse Data Mitigation</title>
    <count>857</count>
    <raw_string>Word Prediction Techniques for User Adaptation and Sparse Data Mitigation PhD Thesis Proposal Keith Trnka trnkacis udel edu Department of Computer and Information Sciences University of Delaware Newark  DE March 19  2008 CONTENTS CONTENTS Contents 1 Introduction 1 11 Background                                               3 12 Guide to remaining sections                                       5 2 Motivation for Adaptation  Corpus Studies 5 21 Keystroke savings                                            6 22 Corpus issues                                              10 221 List of corpora                                          11 222 Example domainvaried evaluation  Trigram baseline across domains         </raw_string>
  </article>
  <article>
    <title>Statistical Learning Techniques for Text Categorization with Sparse Labeled Data</title>
    <count>857</count>
    <raw_string>Thesis for obtaining the title of Doctor of Engineering of the Faculties of Natural Sciences and Technology of Saarland University Statistical Learning Techniques for Text Categorization with Sparse Labeled Data Georgiana Ifrim Supervisor  Prof DrIng  Gerhard Weikum MaxPlanck Institute for Informatics Saarbrucken  Germany 2009 ii Dean  Prof Dr Joachim Weickert Faculty of Mathematics and Computer Science Saarland University Saarbrucken  Germany Colloquium  27 February 2009 MaxPlanck Institute for Informatics Saarbrucken  Germany Examination Board Supervisor and Prof DrIng  Gerhard Weikum First Reviewer  Databases and Information Systems Group MaxPlanck Institute for Informatics Saarbrucken  Germany Second Reviewer  Prof DrIng  Thomas Hofmann Director of Engineering Google Inc Zurich  Switzerland Third Reviewer  Prof DrIng  Tobias Scheffer Department of Computer Science University of Potsdam Potsdam  Germany Chairman  Prof DrIng  Andreas Zeller Department of Computer Science Saarland University Saarbrucken  Germany Research Assistant  Dr Martin Theobald Databases and Information Systems Group MaxPlanck Institute for Informatics Saarbrucken  Germany iii Eidesstattliche Versicherung Hiermit versichere ich an Eides statt  dass ich die vorliegende Arbeit selbstandig und ohne Be nutzung anderer als der angegebenen Hilfsmittel angefertigt habe  Die aus anderen Quellen oder indirekt ubernommenen Daten und Konzepte sind unter Angabe der Quelle gekennzeichnet  Die Arbeit wurde bisher weder im In noch im Ausland in gleicher oder ahnlicher Form in einem Verfahren zur Erlangung eines akademischen Grades vorgelegt  Saarbrucken  den 03  Marz 2009  Unterschrift  Acknowledgements The work environment at the MaxPlanck Institute for Informatics is truly special  One is sur rounded by very smart and interesting people who are genuinely curious about difficult scientific problems  as well as difficult problems facing our current world  First and foremost I would like to thank </raw_string>
  </article>
  <article>
    <title>Using Style Markers for Detecting Plagiarism in Natural Language Documents</title>
    <count>857</count>
    <raw_string>Using Style Markers for Detecting Plagiarism in Natural Language Documents HSIDAMD03004 Marco Kimler Submitted by Marco Kimler to the University of Skovde as a dissertation towards the degree of MSc by examination and dissertation in the Department of Computer Science  August 2003 I certify that all material in this dissertation which is not my own work has been identified and that no material is included for which a degree has already been conferred to me  Marco Kimler i Abstract Most of the existing plagiarism detection systems compare a text to a database of other texts  These external approaches  however  are vulnerable because texts not contained in the database cannot be detected as source texts  This paper examines an internal plagiarism detection method that uses style markers from authorship attribution studies in order to find stylistic changes in a text  These changes might pinpoint plagiarized passages  Additionally  a new style marker called specific words  is introduced  A prestudy tests if the style markers can fingerprint  an authors style and if they are constant with sample size  It is shown that vocabulary richness measures do not fulfil these prerequisites  The other style markers  simple ratio measures  readability scores  frequency lists  and entropy measures  have these characteristics and are  together with the new specific words measure  used in a main study with an unsupervised approach for detecting stylistic changes in plagiarized texts at sentence and paragraph levels  It is shown that at these small levels the style markers generally cannot detect plagiarized sections because of intraauthorial stylistic variations  i e  noise   and that at bigger levels the results are strongly affected by the sliding window approach  </raw_string>
  </article>
  <article>
    <title>Corpus Linguistics as a Method for the Decipherment of Rongorongo</title>
    <count>855</count>
    <raw_string>1st June 2010 School of Social Sciences  History and Philosophy Department of Applied Linguistics and Communication Birkbeck University Corpus linguistics as a method for the decipherment of rongorongo Martyn Harris martynharrisymailcom Word count  28439 Dissertation submitted in partial fulfilment of the requirement for the MRes in Applied Linguistics 1 Acknowledgements This dissertation benefited from the help of many kind individuals  all of whom I would like to thank  in no particular order   Paul Horley and Tomas Melka for openly discussing their views on rongorongo  in addition to articles and hard to find sources  Fr  Jean Louis Schuester  and Maria Centofanti of the Congregation of the Sacred Hearts  SSCC  Rome   for allowing access to the Tahua  A   Arukukurenga  B  and Mamari tablets  C  and for granting permission to take photographs  see appendix   Jill Hasell of the British Museum  for granting access to the London tablet  K  and the Reimiro  L  and  J  Marco Baroni and Stefan Gries for comments and advice on lexicostatistic  and corpus linguistic methods  2 Table of Contents Abstract7 Chapter 1 Introduction 8 Chapter 2 Writing systems and the rongorongo script12 21 Empirical issues relating to writing systems research and rongorongo12 22 The principle of the autonomy of the graphic system 13 23 The principle of interpretation 15 24 The principle of historicity 18 25 Literary genres of the rongorongo inscriptions 20 Chapter 3 Data24 31 Data24 32 Preprocessing 31 Chapter 4 Methodology 34 41 Corpus linguistics 34 411 Ngrams  unigrams  bigrams  and trigrams 40 412 Key Word In Context concordances  KWIC 41 42 Authorshipattribution methods 41 421 Principal components analysis </raw_string>
  </article>
  <article>
    <title>A New Text Statistical Measure and its Application to Stylometry</title>
    <count>847</count>
    <raw_string>A new text statistical measure and its application to stylometry Felix Golcher Institut fr deutsche Sprache und Linguistik HumboldtUniversitt zu Berlin felixgolcherhuberlinde Abstract We dene a simple  purely surface frequency based measure ST  t  which quanties the similarity of a training text T with a test text t  S can be decomposed into three factors  one depending on the training text  one depending on the test text  and one nearly constant residual factor  The slight variations of this near constant allow us to measure stylistic dierences between T and t with high accuracy  The dened quantity S is unique among other stylometric measures in that it uses the full frequency information of all substrings in both texts  Its applicability for stylometric classications was tested in a variety of experiments  1 Introduction Stylometry aims at quantifying linguistic style  Style can be understood as the subtle but regular dierences between texts which ideally share language  genre and topic  but dier with respect to authorship  the gender of the author or similar parameters  The present paper develops a new text statistical measure S and uses it successfully for stylometric classication on a variety of data sets and in the framework of four dierent languages  S quanties how much of a training text is repeated in a test text  It can be factorised into three independent parts  two of which describe the dependency on test and training text re  spectively  and one which is nearly constant  Subtle variations of this near constant can be used for precisely measuring the stylistic similarity between test and training text  S has intriguing theoretical properties and using it for stylometry consti tutes a completely new </raw_string>
  </article>
  <article>
    <title>A Novel Probabilistic Feature Selection Method for Text Classification</title>
    <count>832</count>
    <raw_string>KnowledgeBased Systems 36  2012226235Contents lists available at SciVerse ScienceDirect KnowledgeBased Systems journal homepagewwwelsevier comlocate knosys A novel probabilistic feature selection method for text classification Alper Kursat Uysal Serkan Gunal Department of Computer EngineeringAnadolu UniversityEskisehirTurkiyea r t i c l e i n f o Article historyReceived 28 December 2011 Received in revised form 14 June 2012 Accepted 14 June 2012 Available online 9 July 2012 KeywordsFeature selection Filter Pattern recognition Text classification Dimension reduction09507051see front matter 2012 Elsevier BVA http dx doi org101016jknosys 201206005 Corresponding authorEmail addressesakuysalanadoluedutr  AKU edutr SGunal  a b s t r a c t High dimensionality of the feature space is one of the most important concerns in text classification problems due to processing time and accuracy considerationsSelection of distinctive features is therefore essential for text classificationThis study proposes a novel filter based probabilistic feature selection methodnamely distinguishing feature selector  DFSfor text classificationThe proposed method is compared with wellknown filter approaches including chi squareinformation gainGini index and deviation from Poisson distributionThe comparison is carried out for different datasetsclassification algorithmsand success measuresExperimental results explicitly indicate that DFS offers a competitive performance with respect to the abovementioned approaches in terms of classification accuracydimension reduction rate and processing time2012 Elsevier BVAll rights reserved 1Introduction With rapid advance of internet technologiesthe amount of electronic documents has drastically increased worldwideAs a consequencetext classificationwhich is also known as text categorizationhas gained importance in hierarchical organization of these documentsThe fundamental goal of the text classification is to classify texts of interest into appropriate classes 14Typical text classification framework consists of a feature extraction mechanism that extracts numerical information from raw text documentsand a classifier that carries out the classification process using a prior knowledge of labeled dataText classification has been successfully deployed to various domains such as topic detection </raw_string>
  </article>
  <article>
    <title>Feature Extraction and Author Profile Creation from Documents in Malayalam</title>
    <count>821</count>
    <raw_string>IJART International Journal of Advanced Research In Technology  Vol  1  Issue 1  September 2011 ISSN  Online   66023127 wwwijart org 77 wwwijart org Feature Extraction and Author Profile Creation from Documents in Malayalam Bindu Baby Thomas  Sindhu L Dr Sumam M Idicula Dept of Computer ScienceCUSATCochin bbtucegmailcom  Sindhulcepyahoocoin  sumamgmailcom Abstract Automated authorship attribution is the problem of identifying the author of an anonymous text or text whoseauthorship is in doubt from a given set of authors  The works by different authors are strongly distinguished by quantifiable features of the text  The first step in author identification hence is the extraction of such features from the available corpus  Profiles of the set of authors under consideration thus formed can be compared with the anonymous text  to suggest the most likely author  For this  various probabilistic  similarity based  vector space  machine learning algorithms can be used  We have done a detailed study on the various stylometric features that can be used to form an authors profile and have found that the frequencies of word collocations can be used to clearly distinguish an author in a highly inflectious language such as Malayalam  Keywords stylometrics  feature extraction  feature set  lexical features character features  collocations  classification  ngrams  IINTRODUCTION Authorship identification is the problem of identifying the author of an anonymous text or text whose authorship is in doubt  Scientific investigation regarding the authorship of texts has been done since the late nineteenth century  This mainly centered on distribution of sentence and word lengths in works of literature 1  and the gospels of the New Testament  These studies were done under the notion that works </raw_string>
  </article>
  <article>
    <title>Feature Selection for Online Writeprint Identification Using Hybrid Genetic Algorithm</title>
    <count>820</count>
    <raw_string>Feature Selection for Online Writeprint Identification Using Hybrid Genetic Algorithm Jianwen Sun  Zongkai Yang  Pei Wang  Lin Liu  Sanya Liu National Engineering Research Center for Elearning Huazhong Normal University Wuhan 430079  China email  lsy5918mailccnueducn AbstractOne major task of online writeprint identification is to select the key features for representing the writeprint and facilitating the classifier built by using only the selected feature subset  In this study  we develop a hybrid genetic algorithm  RelieF Fed Genetic Algorithm  RFGA  which incorporates feature weight information produced by using RelieF as the heuristic to identity the key features and improve the identification performance  Experiments are conducted on a test bed encompassing hundreds of reviews posted by 20 Amazon customers to examine the method  The experimental results using RFGA show the proposed approach is effective  obtaining a significant improvement in performance  with satisfactory classification accuracy of 9667   and having a heavy reduction in feature dimensionality that is only 3  of the no feature selection baseline  Keywordsonline writeprint identification  feature selection  hybrid genetic algorithm I INTRODUCTION The Internets numerous benefits have always been coupled with drawbacks attributable to the abuses of online anonymity  It is necessary to counter anonymity abuses and strengthen the social accountability in cyberspace  Writeprint identification is a technique to identify individuals based on textual identity cues people leave behind textbased media  Researchers have begun to use online writeprint analysis techniques as a forensic identification tool  with recent application to email and online forum  These techniques can also be used to analyze on student s class papers for plagiarism detection  Similar to human fingerprint  writeprint is composed of multiple features such as frequency of word </raw_string>
  </article>
  <article>
    <title>A Corpus of Online Discussions for Research into Linguistic Memes</title>
    <count>820</count>
    <raw_string>A Corpus of Online Discussions for Research into Linguistic Memes Dayne Freitag SRI International freitagai sricom Ed Chow SRI International edchowai sricom Paul Kalmar SRI International kalmarai sricom Tulay Muezzinoglu SRI International tulayai sricom John Niekrasz SRI International niekraszai sricom ABSTRACT We describe a 460million word corpus of online discussions  The data are collected from public news websites and communityof  interest Internet forums  and are designed to support research on the propagation of socially relevant ideas  ak a  memes   A struc  tural and statistical description of the corpus is given  and the em  ployed methods of website monitoring  collection  and extraction are described  We also present preliminary linguistic research on the corpus  We show that the corpus represents language from a wide variety of social and psychological communities  that discus  sion structure and popularity can be predicted in large part from lexical analysis  and that standard epidemiological models provide good fit for diachronic patterns of populationlevel lexical adop  tion  Categories and Subject Descriptors H24 Database Management Systems   Textual databases  H31  Information Storage and Retrieval  Content Analysis and In dexingLinguistic processing General Terms Corpus analytics  Memetics  Information diffusion 1  INTRODUCTION Over the relatively short period since its inception  the Web has as sumed an increasingly central role in the dissemination of informa  tion and the spread of ideas  The widespread adoption of social me dia  an even more recent phenomenon  has dramatically decreased the friction with which both trivial and momentous ideas spread  In the past  these socially relevant ideas  these memes  might have gained most of their force through official promulgation  In the </raw_string>
  </article>
  <article>
    <title>Affect Analysis of Web Forums and Blogs Using Correlation Ensembles</title>
    <count>818</count>
    <raw_string>Affect Analysis of Web Forums and Blogs Using Correlation Ensembles Ahmed Abbasi  Member  IEEE  Hsinchun Chen  Fellow  IEEE  Sven Thoms  and Tianjun Fu AbstractAnalysis of affective intensities in computermediated communication is important in order to allow a better understanding of online users  emotions and preferences  Despite considerable research on textual affect classification  it is unclear which features and techniques are most effective  In this study  we compared several feature representations for affect analysis  including learned ngrams and various automatically and manually crafted affect lexicons  We also proposed the support vector regression correlation ensemble  SVRCE  method for enhanced classification of affect intensities  SVRCE uses an ensemble of classifiers each trained using a feature subset tailored toward classifying a single affect class  The ensemble is combined with affect correlation information to enable better prediction of emotive intensities  Experiments were conducted on four test beds encompassing web forums  blogs  and online stories  The results revealed that learned ngrams were more effective than lexiconbased affect representations  The findings also indicated that SVRCE outperformed comparison techniques  including Pace regression  semantic orientation  and WordNet models  Ablation testing showed that the improved performance of SVRCE was attributable to its use of feature ensembles as well as affect correlation information  A brief case study was conducted to illustrate the utility of the features and techniques for affect analysis of large archives of online discourse  Index TermsAffective computing  discourse  emotion recognition  linguistic processing  machine learning  text mining   1 INTRODUCTION THE need for enhanced information retrieval and knowledge discovery from computermediated communica  tion archives has been articulated by many in recent years  </raw_string>
  </article>
  <article>
    <title>Genre Identification of Chinese Finance Text Using Machine Learning Method</title>
    <count>817</count>
    <raw_string>Genre Identification of Chinese Finance Text using Machine Learning Method Jun Xu  Yuxin Ding  Xiaolong Wang  Yonghui Wu Department of Computer Science and Technology Harbin Institute of Technology Shenzhen Graduate School Shenzhen  China hit xujungmailcom yxdinghitsz educn AbstractDocument genre information is one of the most dis  tinguishing features in information retrieval  which brings order to the search results  What the genre classification concerned is not the topic but the genre of document  In this paper  we examine the effectiveness of using machine learning techniques to solve genre classification of Chinese text with the same topic  viz  finance  Based on the likelihood ratio test  we present a new method for selecting feature terms  which can improve the performance clearly and perform better than others with up to 80  terms removal  In empirical results with SVMs classifier on the real world corpora  we find that this method can gain a better selecting effect and likelihood ratio is a reliable measure for selecting informative features  Index TermsGenre Classification  Likelihood Ratio Test  Support Vector Machines I INTRODUCTION As the developing of the WWW  the available information is becoming abundant than before  However  even with the help of search engine  it s still hard to find the most suitable information due to the huge amount of abundant information  In order to help user to acquire relevant and useful information  classification and cluster tools are imported to classify search engine results  All these techniques are topiccentered  succeed in topic classifying  and neglect the importance of identifying the genres in results ranking  takes little account of the individual users needs and preferences  Our research focus on </raw_string>
  </article>
  <article>
    <title>Evaluating the Effects of Textual Features on Authorship Attribution Accuracy</title>
    <count>816</count>
    <raw_string>Evaluating the Effects of Textual Features on Authorship Attribution Accuracy Reza Ramezani Department of Computer Engineering Ferdowsi University of Mashhad  Iran DDEmS Lab reza ramezanistuum acir Navid Sheydaei Electrical  Computer Engineering Isfahan University of Technology  Iran DM Lab n sheydaeieciut acir Mohsen Kahani Department of Computer Engineering Ferdowsi University of Mashhad  Iran WT Lab kahanium acir Abstract Authorship attribution  AA  or author identification refers to the problem of identifying the author of an unseen text  From the machine learning point of view  AA can be viewed as a multiclass  singlelabel textcategorization task  This task is based on this assumption that the author of an unseen text can be discriminated by comparing some textual features extracted from that unseen text with those of texts with known authors  In this paper the effects of 29 different textual features on the accuracy of author identification on Persian corpora in 30 different scenarios are evaluated  Several classification algorithms have been used on corpora with 2  5  10  20 and 40 different authors and a comparison is performed  The evaluation results show that the information about the used words and verbs are the most reliable criteria for AA tasks and also NLP based features are more reliable than BOW based features  Keywords  Authorship Attribution  Author Identification  Textual Features  Persian Corpus  Data Mining  Classification  I INTRODUCTION In the problem of authorship attribution  AA   a text with unknown author is assigned to one of the candidate authors  Each candidate author possesses a set of quantitative textual features that denotes hisher unconscious writing styles  From the machine learning point of view  AA is a multiclass  single  </raw_string>
  </article>
  <article>
    <title>Authorship Attribution and Verification with Many Authors and Limited Data</title>
    <count>815</count>
    <raw_string>Authorship Attribution and Verification with Many Authors and Limited Data Kim Luyckx and Walter Daelemans CNTS Language Technology Group University of Antwerp Prinsstraat 13  2000 Antwerp  Belgium kim luyckx walterdaelemans uaacbe Abstract Most studies in statistical or machine learning based authorship attribution focus on two or a few authors  This leads to an overestimation of the importance of the features extracted from the training data and found to be discriminating for these small sets of authors  Most studies also use sizes of training data that are unreal istic for situations in which stylometry is applied  e g  forensics   and thereby over estimate the accuracy of their approach in these situations  A more realistic interpre  tation of the task is as an authorship ver ification problem that we approximate by pooling data from many different authors as negative examples  In this paper  we show  on the basis of a new corpus with 145 authors  what the effect is of many authors on feature selection and learning  and show robustness of a memorybased learning approach in doing authorship at tribution and verification with many au  thors and limited training data when com pared to eager learning methods such as SVMs and maximum entropy learning  1 Introduction In traditional studies on authorship attribution  the focus is on small sets of authors  Trying to classify an unseen text as being written by one of two or of a few authors is a relatively simple task  which cKim Luyckx  Walter Daelemans  2008  Li censed under the Creative Commons Attribution  NoncommercialShare Alike 30 Unported license  http creativecommons orglicensesbyncsa30   Some rights reserved  in most cases can be solved with </raw_string>
  </article>
  <article>
    <title>Comparative study of Authorship Identification Techniques for Cyber Forensics Analysis</title>
    <count>815</count>
    <raw_string> IJACSA  International Journal of Advanced Computer Science and Applications  Vol  4  No5  2013 32  P a g e wwwijacsathesai org Comparative study of Authorship Identification Techniques for Cyber Forensics Analysis Smita Nirkhi Department of Computer Science  Engg GHRaisoni College of Engineering Nagpur  India DrRVDharaskar Director MPGI Nanded  India AbstractAuthorship Identification techniques are used to identify the most appropriate author from group of potential suspects of online messages and find evidences to support the conclusion  Cybercriminals make misuse of online communication for sending blackmail or a spam email and then attempt to hide their true identities to void detectionAuthorship Identification of online messages is the contemporary research issue for identity tracing in cyber forensics  This is highly interdisciplinary area as it takes advantage of machine learning  information retrieval  and natural language processing  In this paper  a study of recent techniques and automated approaches to attributing authorship of online messages is presented  The focus of this review study is to summarize all existing authorship identification techniques used in literature to identify authors of online messages  Also it discusses evaluation criteria and parameters for authorship attribution studies and list open questions that will attract future work in this area  Keywordscyber crime  Author Identification  SVM I INTRODUCTION Cyber crime is also known as computer crime  the use of a computer to further illegal ends  such as committing fraud  trafficking in child pornography and intellectual property  stealing identities  or violating privacy  Cybercrime  especially through the Internet  has grown in importance as the computer has become central to commerce  entertainment  and government  Senders can hide their identities by forging senders address  Routed through </raw_string>
  </article>
  <article>
    <title>Segmentation of Characters from old Typewritten Documents using Radon Transform</title>
    <count>815</count>
    <raw_string>International Journal of Computer Applications  0975  8887  Volume 37 No9  January 2012 10 Segmentation of Characters from old Typewritten Documents using Radon Transform Apurva A Desai Department of Computer Science Veer Narmad South Gujarat University  Surat  Gujarat  India ABSTRACT Optical character recognition is a very challenging area  Many works have been done and still being done for many languages across the world  For many Indian languages too good amount of work has been done  However  Gujarati is a language for which hardly any work can be found  Gujarati has a rich literary heritage  and therefore it is important to preserve it for the next generation  In this paper an attempt has be done to segmenting out the words and characters from old typewritten Gujarati documents  Here an algorithm is presented which makes use of global threshold for converting scan RGB documents to blank and white documents  Noise removal has also been applied  Here Radon transform is utilized for skew detection  The novel concept of using Radon transform is presented here in this work  Here Radon transform is used for segmenting documents into lines and then vertical profiles has been used for further segmentation of lines in characters  At last this segmentation algorithm is also tested for the documents typewritten in Hindi  The algorithm presented here gives very good results  Keywords  Segmentation  Radon transform  skew correction  digitization  noise removal 1  INTRODUCTION Gujarati is a language with very rich literary  cultural  and historical heritage  One can find very old historical documents  literature  books and manuscripts written in Gujarati language  These documents carry lots of knowledge within them  </raw_string>
  </article>
  <article>
    <title>On the Use of Supervised Learning Method for Authorship Attribution</title>
    <count>815</count>
    <raw_string>Eng   Tech  Journal  Vol30  No 2  2012 282 On the Use of Supervised Learning Method for Authorship Attribution Dr Walaa M Khalaf Engineering College  University of Almustansiriya Baghdad Email walaakhalaf yahoocom Received on  44 2011  Accepted on  3 11 2011 ABSTRACT In this paper we investigate the use of a supervised learning method for the authorship attribution that is for the identification of the author of a text  We suggest a new  simple and efficient method  which is merely based on counting the number of repetitions of each alphabetic letter in the text  instead of using the traditional classification properties  such as the contents of the text and style of the author  which falls into four feature categories  lexical  syntactic  structural  and contentspecific  Furthermore  we apply a spherical classification method  We apply the proposed technique to the work of two Italian writers  Dante Alighieri and Brunetto Latini  With almost high reliability  the spherical classifier proved its ability to discriminate between the selected authors  Finally the results are compared with those obtained by means of a standard Support Vector Machine classifier  Keywords  Authorship Attribution  Spherical Classification  Support Vector Machine                                                                                  </raw_string>
  </article>
  <article>
    <title>Text Extraction in Complex Color Document Images for Enhanced Readability</title>
    <count>815</count>
    <raw_string>Intelligent Information Management  2010  2  120133 doi104236iim201022015 Published Online February 2010  http www scirporgjournaliim  Copyright  2010 SciRes IIM Text Extraction in Complex Color Document Images for Enhanced Readability P Nagabhushan  S Nirmala Department of Studies in Computer Science  University of Mysore  Mysore  India Email  pnagabhushancompsciunimysore acin  nirshiv2002yahoocoin Abstract Often we encounter documents with text printed on complex color background  Readability of textual con tents in such documents is very poor due to complexity of the background and mix up of color s  of fore ground text with colors of background  Automatic segmentation of foreground text in such document images is very much essential for smooth reading of the document contents either by human or by machine  In this paper we propose a novel approach to extract the foreground text in color document images having complex background  The proposed approach is a hybrid approach which combines connected component and texture feature analysis of potential text regions  The proposed approach utilizes Canny edge detector to detect all possible text edge pixels  Connected component analysis is performed on these edge pixels to identify can didate text regions  Because of background complexity it is also possible that a nontext region may be iden tified as a text region  This problem is overcome by analyzing the texture features of potential text region corresponding to each connected component  An unsupervised local thresholding is devised to perform fore ground segmentation in detected text regions  Finally the text regions which are noisy are identified and re  processed to further enhance the quality of retrieved foreground  The proposed approach can handle docu ment images with varying background of multiple colors and texture  and </raw_string>
  </article>
  <article>
    <title>Text Reuse Detection Using a Composition of Text Similarity Measures</title>
    <count>814</count>
    <raw_string>Text Reuse Detection Using a Composition of Text Similarity Measures Daniel Br1 Torsten Zesch12 Iryna Gurevych12  1  Ubiquitous Knowledge Processing Lab  UKPTUDA  Department of Computer Science  Technische Universitt Darmstadt  2  Ubiquitous Knowledge Processing Lab  UKPDIPF  German Institute for Educational Research and Educational Information www ukptudarmstadtde ABSTRACT Detecting text reuse is a fundamental requirement for a variety of tasks and applications  ranging from journalistic text reuse to plagiarism detection  Text reuse is traditionally detected by computing similarity between a source text and a possibly reused text  However  existing text similarity measures exhibit a major limitation  They compute similarity only on features which can be derived from the content of the given texts  thereby inherently implying that any other text characteristics are negligible  In this paper  we overcome this traditional limitation and compute similarity along three characteristic dimensions inherent to texts  content  structure  and style  We explore and discuss possible combinations of measures along these dimensions  and our results demonstrate that the composition consistently outperforms previous approaches on three standard evaluation datasets  and that text reuse detection greatly benefits from incorporating a diverse feature set that reflects a wide variety of text characteristics  TITLE AND ABSTRACT IN GERMAN Erkennung von Textwiederverwendung durch Komposition von Texthnlichkeitsmaen Die Frage  ob und in welcher Weise Texte in abgewandelter Form wiederverwendet werden  ist ein zentraler Aspekt bei einer Reihe von Problemstellungen  etwa im Rahmen journalisti scher Ttigkeit oder als Mittel zur Plagiatserkennung  Textwiederverwendung wird traditionell ermittelt durch Berechnen von Texthnlichkeit zwischen einem Ursprungstext und einem po tentiell wiederverwendeten Text  Bestehende Texthnlichkeitsmae haben jedoch die starke Einschrnkung  dass sie hnlichkeit nur anhand von Eigenschaften berechnen  die </raw_string>
  </article>
  <article>
    <title>Efficiently Finding Near Duplicate Figures in Archives of Historical Documents</title>
    <count>814</count>
    <raw_string>Efficiently Finding Near Duplicate Figures in Archives of Historical Documents Thanawin Rakthanmanon Qiang Zhu Eamonn J Keogh Department of Computer Science and Engineering University of California  Riverside  CA  USA Email  rakthant  qzhu  eamonncs ucredu AbstractThe increasing interest in archiving all of humankinds cultural artifacts has resulted in the digitization of millions of books  and soon a significant fraction of the worlds books will be online  Most of the data in historical manuscripts is text  but there is also a significant fraction devoted to images  This fact has driven much of the recent increase in interest in querybycontent systems for images  While queryingindexing systems can undoubtedly be useful  we believe that the historical manuscript domain is finally ripe for true unsupervised discovery of patterns and regularities  To this end  we introduce an efficient and scalable system that can detect approximately repeated occurrences of shape patterns both within and between historical texts  We show that this ability to find repeated shapes allows automatic annotation of manuscripts  and allows users to trace the evolution of ideas  We demonstrate our ideas on datasets of scientific and cultural manuscripts dating back to the fourteenth century  Keywordscomponent  cultural artifacts  duplication detection  repeated patterns I INTRODUCTION The worlds books and manuscripts are being digitized at an increasing rate  and within a few years  the majority of the worlds books will be online  Much of the data will be text  most of which is more or less amiable to optical character recognition  However  in addition  there will be perhaps hundreds of millions of pages that contain one or more images  It is clear that these images will be very </raw_string>
  </article>
  <article>
    <title>Evaluation of Authorship Attribution Software on a Chat Bot Corpus</title>
    <count>814</count>
    <raw_string>Evaluation of Authorship Attribution Software on a Chat Bot Corpus Nawaf Ali Computer Engineering and Computer Science J B Speed School of Engineering University of Louisville Louisville  KY  USA ntali001louisville edu Musa Hindi Computer Engineering and Computer Science J B Speed School of Engineering University of Louisville Louisville  KY  USA mmhind01louisville edu Roman V Yampolskiy Computer Engineering and Computer Science J B Speed School of Engineering University of Louisville Louisville  KY  USA romanyampolskiylouisville edu Abstract Authorship recognition is a technique used to identify the author of an unclaimed document  or in case when more than one author claims a document  Authorship recognition has great potential for applications in Computer forensics  The intended goal of this research is to identify a Chat bot by analyzing conversation log files  This is a novel area of investigation  as artificially intelligent authors have not been profiled based on their linguistic behavior  The collected data comes from chat logs between different Chat Bots and between Chat Bots and Human users  The initial experiments utilizing collected data demonstrate the feasibility of our approach  Keywords  Authorship attribution  Authorship recognition  Chat bot  JGAAP  Stylometry  I INTRODUCTION A significant amount of research has been done in the area of authorship attribution 1  2  Stylometry is the study of differentiating authors by their styles  a survey of the field has been presented by Stamatatos 3  Four main methods of authorship identification are  lexical  syntactic  semantic  and content specific  In lexical methods  the word counts and distributions in the text to grasp more knowledge about the different kinds of statistical properties in the document  Syntactic methods focus on extracting specific </raw_string>
  </article>
  <article>
    <title>Training the Genre Classifier for Automatic Classification of Web Pages</title>
    <count>812</count>
    <raw_string>Training the Genre Classifier for Automatic Classification of Web Pages Vedrana Vidulin  Mitja Lutrek  Matja Gams Joef Stefan Institute  Jamova 39  SI1000 Ljubljana vedrana vidulinijs si  mitjalustrekijs si  matjazgamsijs si Abstract  This paper presents experiments on classifying web pages by genre  Firstly  a corpus of 1539 manually labeled web pages was prepared  Secondly  502 genre features were selected based on the literature and the observation of the corpus  Thirdly  these features were extracted from the corpus to obtain a data set  Finally  two machine learning algorithms  one for induction of decision trees  J48  and one ensemble algorithm  bagging   were trained and tested on the data set  The ensemble algorithm achieved on average 17  better precision and 16  better accuracy  but slightly worse recall  Fmeasure did not vary significantly  The results indicate that classification by genre could be a useful addition to search engines  Keywords  genre classification  web page  genre features  ensemble algorithm 1  Introduction A good question to start with is why we want to classify a web page by genre  For example  if we are interested in elephants and search for the keyword elephant  a search engine will return web pages that describe the life of elephants  but it will also return web pages with elephant picture gallery  newspaper articles about saving the elephants in Africa etc   see Figure 1   However  if we were able to specify that we want to search only for journalistic materials about elephants  we would get more specific results in accordance with our interest  Classification of web pages by </raw_string>
  </article>
  <article>
    <title>Bigrams of Syntactic Labels for Authorship Discrimination of Short Texts</title>
    <count>810</count>
    <raw_string>Bigrams of Syntactic Labels for Authorship Discrimination of Short Texts  Graeme Hirst and Olga Feiguina1 Department of Computer Science  University of Toronto  Toronto  Ontario  Canada  Abstract We present a method for authorship discrimination that is based on the frequency of bigrams of syntactic labels that arise from partial parsing of the text  We show that this method  alone or combined with other classification features  achieves a high accuracy on discrimination of the work of Anne and Charlotte Bronte  which is very difficult to do by traditional methods  Moreover  high accuracies are achieved even on fragments of text little more than 200 words long   1 Introduction Methods for identifying or discriminating the authorship of a text typically rely on both the questioned text and the body of attested work of the putative author being relatively large  In the canonical case  a novel or play of uncertain or disputed authorship is compared against attested corpora that are several times as large or more  The smaller the text or the comparison corpus  the less certain the results are  Thus  methods that could perform authorship tests with greater reliability on smaller texts would be welcome both in literary studies and in forensic analysis  In this article  we show the potential for the use of syntactic information in achieving this goal  and  in particular  we show that the use of bigrams of labels from a partial parser provides a good compromise between partofspeech tagging and a complete parse  11 Why short texts  We are  of course  not the first to think about attribution of authorship of small texts  For example  Burrows  2002  </raw_string>
  </article>
  <article>
    <title>Word Segmentation Approach of Discrete and Mixed Latin Handwritten Styles</title>
    <count>810</count>
    <raw_string>Word Segmentation Approach of Discrete and Mixed Latin Handwritten Styles                                                                                                                                              90                Abstract  this paper presents a new efficient approach for word segmentation into individual isolated characters for Handwriting Recognition Systems  The proposed segmentation method is for spaced discrete and mixed handwritten Latin characters styles  It is based on slantskew correction and thinning that represent its cornerstone beside of some heuristics features and using the projection profiles analysis  The method composed of three successive stages  The first one is a presegmentation stage that provides an estimation of the word main body boundaries  The second is a primary segmentation stage of the likely connected characters  ccs  which consisting the word  while the third one is a selection stage of the best processing path according to the aspect ratio of the extracted connected characters  The approach has </raw_string>
  </article>
  <article>
    <title>A Dynamic Programming Algorithm for the Segmentation of Greek Texts</title>
    <count>810</count>
    <raw_string>A Dynamic Programming Algorithm for the Segmentation of Greek Texts Pavlina Fragkou In this paper we introduce a dynamic programming algorithm to perform linear text segmentation by global minimization of a segmentation cost function which consists of   a  withinsegment word similarity and  b  prior information about segment length  The evaluation of the segmentation accuracy of the algorithm on a text collection consisting of Greek texts showed that the algorithm achieves high segmentation accuracy and appears to be very innovating and promissing  Keywords  Text Segmentation  Document Retrieval  Information Retrieval  Ma  chine Learning  1  Introduction Text segmentation is an important problem in information retrieval  Its goal is the division of a text into homogeneous  lexically coherent  segments  i e  segments exhibiting the following properties   a  each segment deals with a particular sub  ject and  b  contiguous segments deal with different subjects  Those segments can be retrieved from a large database of unformatted  or loosely formatted  text as being relevant to a query  This paper presents a dynamic programming algorithm which performs linear segmentation 1 by global minimization of a segmentation cost  The segmentation cost is defined by a function consisting of two factors   a  withinsegment word similarity and  b  prior information about segment length  Our algorithm has the advantage that it can be applied to either large texts  to segment them into their constituent parts  e g  to segment an article into sections   or to a stream of independent  concatenated texts  e g  to segment a transcript of news into separate stories   1As opposed to hierarchical </raw_string>
  </article>
  <article>
    <title>An Objective Evaluation Methodology for Handwritten Image Document Binarization Techniques</title>
    <count>810</count>
    <raw_string>An Objective Evaluation Methodology for Handwritten Image Document Binarization Techniques K Ntirogiannis  B Gatos and I Pratikakis Computational Intelligence Laboratory  Institute of Informatics and Telecommunications  National Center for Scientific Research Demokritos  GR153 10 Agia Paraskevi  Athens  Greece http wwwiitdemokritosgrcil  kntir bgat ipratika iitdemokritosgr Abstract This paper presents an objective evaluation methodology for handwritten document image binarization techniques that aims to reduce the human involvement in the ground truth construction and consecutive testing  A detailed description of the methodology along with a benchmarking of the stateoftheart binarization algorithms based on the proposed methodology is presented  Keywords  binarization  handwritten  objective evaluation  1  Introduction Document image binarization is an important step in the document image analysis and recognition pipeline  The performance of a binarization technique directly affects the required analysis or recognition outcome  Therefore  it is imperative to have an objective evaluation which will account for the performance of the binarization  Several efforts have been presented that strive towards evaluating the performance of binarization techniques  These efforts can be divided in three categories  In the first category evaluation is performed by a human evaluator 1  2  3  while in the second category  the binary result is subject to OCR and the corresponding result is evaluated with respect to accuracy 4  5  6  The third category uses a combination of humanoriented evaluation and OCR results accuracy 7  Evaluation performed by a human expert is not only subjective but also time consuming  Furthermore  it lacks robustness since it has been observed that in fuzzy situations  the same observer may make different selections for the same dataset in different sessions  The use of OCR </raw_string>
  </article>
  <article>
    <title>Version History Based Source Code Plagiarism Detection in Proprietary Systems</title>
    <count>810</count>
    <raw_string>Version History Based Source Code Plagiarism Detection in Proprietary Systems Girish Maskeri  Deepthi Karnam  Sree Aurovindh Viswanathan and Srinivas Padmanabhuni Infosys Labs  Infosys Limited Bangalore  India  girish rama  deepthi karnam  sreeaurovindh v  and srinivas pinfosyscom Abstract While the advent of open source code search tools have made the source code of thousands of open source software  OSS readily accessible  thereby increasing legitimate reuse  it has also opened up the possibility of unconscientious employees plagiarizing code from OSS repositories  Plagiarism in proprietary software would not only lead to costly lawsuits  but also undermine the credibility of the organization  Hence detecting plagiarism in proprietary software is an urgent need  Though there exist a number of techniques for detecting plagiarism in student project assignments  they do not scale well in the case of large proprietary software  Especially when code snippets are plagiarized from the large number of available open source software  In this paper we propose a novel approach that applies Mining Software Repositories  MSR  based techniques to the problem of plagiarism detection  We create a programming style profile for each maintenance engineer by mining the version history and use that to detect source code commits that are likely to be plagiarized  Such suspected code fragments can be analyzed using any of the existing plagiarism detection techniques to confirm the plagiarism and ascertain the original code  KeywordsPlagiarismAuthor Information Version History  I INTRODUCTION Hard work has a future payoff  Laziness pays off now  This attitude of human race is also prevalent in industrial software development which leads to a serious problem  Code Plagiarism  While we have been aware of unscrupu lous students plagiarizing open source code </raw_string>
  </article>
  <article>
    <title>Alignment between Text Images and their Transcripts for Handwritten Documents</title>
    <count>810</count>
    <raw_string>Alignment between Text Images and their Transcripts for Handwritten Documents Alejandro H Toselli  Vernica Romero and Enrique Vidal Abstract An alignment method based on the Viterbi algorithm is proposed to find mappings between word images of a given handwritten document and their respective  ASCII  words in its transcription  The approach takes advantage of the underlying segmentation made by Viterbi decoding in handwritten text recognition based on Hidden Markov Models  HMMs  Two levels of alignments are consid  ered  the traditional one at word level and the one at textline level where pages are transcribed without line break synchronization  According to various metrics used to measure the quality of the alignments  satisfactory results are obtained  Furthermore  the presented alignment approach is tested on two different HMMs modelling schemes  one using 78 HMMs  one HMM per character class  and other using two HMMs  for blank space and noblank characters respectively   Key words  handwriting image and transcription alignments  forced recognition  digital libraries  handwritten text recognition  Viterbi algorithm Dr Alejandro H Toselli Instituto Tecnolgico de Informtica  Universidad Politcnica de Valencia Camino de Vera sn  46022 Valencia  Spain  email  ahectoriti upves Dr Vernica Romero Instituto Tecnolgico de Informtica  Universidad Politcnica de Valencia Camino de Vera sn  46022 Valencia  Spain  email  vromeroiti upves Dr Enrique Vidal Instituto Tecnolgico de Informtica  Universidad Politcnica de Valencia Camino de Vera sn  46022 Valencia  Spain  email  evidaliti upves C Sporleder et al  eds    Language Technology for Cultural Heritage  Selected Papers from the LaTeCH Workshop Series  Theory and Applications of Natural Language Processing  DOI 10100797836422022782   </raw_string>
  </article>
  <article>
    <title>Application of Information Retrieval Techniques for Source Code Authorship Attribution</title>
    <count>810</count>
    <raw_string>Application of Information Retrieval Techniques for Source Code Authorship Attribution Steven Burrows  Alexandra L Uitdenbogerd  and Andrew Turpin School of Computer Science and Information Technology RMIT University GPO Box 2476V  Melbourne 3001  Australia stevenburrows alexandrauitdenbogerd andrewturpinrmit eduau Abstract  Authorship attribution assigns works of contentious authorship to their rightful owners solving cases of theft  plagiarism and authorship disputes in academia and industry  In this paper we inves  tigate the application of information retrieval techniques to attribution of authorship of C source code  In particular  we explore novel meth  ods for converting C code into documents suitable for retrieval systems  experimenting with 1597 student programming assignments  We investi gate several possible program derivations  partition attribution results by original program length to measure effectiveness of modest and lengthy programs separately  and evaluate three different methods for interpret ing document rankings as authorship attribution  The best of our meth  ods achieves an average of 7678  classification accuracy for a oneinten classification problem which is competitive against six existing baselines  The techniques that we present can be the basis of practical software to support source code authorship investigations  Keywords  Adversarial information retrieval  authorship attribution  source code  1 Introduction Automatically detecting the author of a document is useful in plagiarism detec tion  copyright infringement  computer crime and authorship disputes  To assign authorship  a profile of each author is constructed from known sources  and then documents of unknown or uncertain authorship can be matched against these profiles manually  or with computerbased statistical analysis  machine learning or similarity calculation methods 1  Authorship attribution techniques can be used to solve realworld natural lan guage  plain text </raw_string>
  </article>
  <article>
    <title>Degraded Document Image Enhancement Using Hybrid Thresholding and Mathematical Morphology</title>
    <count>809</count>
    <raw_string>Degraded Document Image Enhancement using Hybrid Thresholding and Mathematical Morphology Nija Babu Department of Computer Science and Engineering PES Institute of Technology Bangalore  India nijababuyahoocoin Preethi NG Department of Computer Science and Engineering PES Institute of Technology Bangalore  India ngpreetigmailcom Shylaja S S Department of Information Science and Engineering PES Institute of Technology Bangalore  India shylaja sharathpes edu Abstract The paper presents a hybrid thresholding approach for binarization and enhancement of degraded documents  Historical documents contain information of great cultural and scientific value  But such documents are frequently degraded over time  Digitized degraded documents require specialized processing to remove different kinds of noise and to improve readability  The approach for enhancing degraded documents uses a combination of two thresholding algorithms  First  iterative global thresholding is applied to the smoothed degraded image until the stopping criteria is reached  Then a threshold selection method from gray level histogram is used to binarize the image  The next step is detecting areas where noise still remains and applying iterative thresholding locally  A method to improve the quality of textual information in the document is also done as a post processing stage  thus making the approach efficient and better suited for character recognition applications  1  Introduction Binarization or thresholding refers to the conversion of gray level image into a black and white image  An ideal binarization technique must be able to perfectly separate text from background  thus removing any kind of noise and improve readability  Frequently  binarization is used as a pre processing stage before OCR Binarization plays a major role in document image processing since its performance affects further processing of the image such as segmentation and character recognition  Historical documents suffer from various </raw_string>
  </article>
  <article>
    <title>Authorship Discovery in Blogs Using Bayesian Classification with Corrective Scaling</title>
    <count>807</count>
    <raw_string>NAVAL POSTGRADUATE SCHOOL MONTEREY  CALIFORNIA THESIS AUTHORSHIP DISCOVERY IN BLOGS USING BAYESIAN CLASSIFICATION WITH CORRECTIVE SCALING by Grant T Gehrke June 2008 Thesis Advisor  Craig H Martell  PhD Second Reader  Kevin M Squire  PhD Approved for public release  distribution is unlimited THIS PAGE INTENTIONALLY LEFT BLANK REPORT DOCUMENTATION PAGE Form ApprovedOMB No 07040188 The public reporting burden for this collection of information is estimated to average 1 hour per response  including the time for reviewing instructions  searching existing data sources  gathering and maintaining the data needed  and completing and reviewing the collection of information  Send comments regarding this burden estimate or any other aspect of this collection of information  including suggestions for reducing this burden to Department of Defense  Washington Headquarters Services  Directorate for Information Operations and Reports  07040188   1215 Jefferson Davis Highway  Suite 1204  Arlington  VA 222024302  Respondents should be aware that notwithstanding any other provision of law  no person shall be subject to any penalty for failing to comply with a collection of information if it does not display a currently valid OMB control number  PLEASE DO NOT RETURN YOUR FORM TO THE ABOVE ADDRESS 1  REPORT DATE  DDMMYYYY  2 REPORT TYPE 3 DATES COVERED  From  To  4  TITLE AND SUBTITLE 5a  CONTRACT NUMBER 5b  GRANT NUMBER 5c  PROGRAM ELEMENT NUMBER 5d  PROJECT NUMBER 5e  TASK NUMBER 5f  WORK UNIT NUMBER 6  AUTHOR S  7 PERFORMING ORGANIZATION NAME S  AND ADDRESSES  8 PERFORMING ORGANIZATION REPORT NUMBER 9  SPONSORING  MONITORING AGENCY NAME S  AND ADDRESSES  10  SPONSORMONITORS ACRONYMS  11  SPONSORMONITORS REPORT NUMBER </raw_string>
  </article>
  <article>
    <title>EPSMS and the Document Occurrence Representation for Authorship Identification</title>
    <count>796</count>
    <raw_string>EPSMS and the Document Occurrence Representation for Authorship Identification  Notebook for PAN at CLEF 2011 Hugo Jair Escalante Graduate Program in Systems Engineering  Universidad Autnoma de Nuevo Len  San Nicols de los Garza  NL 66450  Mxico hugo jairgmailcom http hugojairorg Abstract This paper describes the participation of the PISIS team in the author ship identification track of PAN11  We adopted two different strategies for the tasks of authorship attribution and authorship verification  For authorship attri bution we performed experiments with a document occurrence representation using a standard classificationbased approach  Results obtained with this ap  proach were mixed  in the small data sets distributional representations resulted very helpful  although in the large data sets a simple bagofwords approach out performed the document occurrence approach  For authorship verification we adopted a classificationbased approach and proposed a modification to Ensem  ble Particle Swarm Model Selection  EPSMS  for selecting classification models for each task  This approach obtained acceptable performance in two out of the three data sets  1 Introduction Authorship attribution  AA  and authorship verification  AV  are two closely related problems that aim at uncovering the writing style of authors 27  Applications of AA and AV include spam filtering 30  fraud detection 14  computer forensics 18  cyber bullying 23 and plagiarism detection 25  Because of its wide applicability  mainly in security aspects  the development of automated AA techniques has received much attention recently 162527  AA is defined as the task of identifying whom  from a set of candidates  is the author of a given document 27  While AV is the task of deciding whether given text docu ments were or were not </raw_string>
  </article>
  <article>
    <title>Deteccion de Plagio Translingue Utilizando una Red Semantica Multilingue</title>
    <count>790</count>
    <raw_string>Deteccion de plagio translingue utilizando una red semantica multilingue Marc Franco Salvador DEPARTAMENTO DE SISTEMAS INFORMATICOS Y COMPUTACION Dirigido por  Paolo Rosso Trabajo Final de Master desarrollado dentro del Master en Inteligencia Artificial  Reconocimiento de Formas e Imagen Digital Valencia  Febrero 2013 Cuanto mas cercana a la verdad  mejor sera la mentira  y la misma verdad  cuando puede utilizarse  es la mejor mentira  Isaac Asimov Resumen El plagio es definido como el uso no autorizado del contenido ori ginal de la obra de otros autores  Es un fenomeno difcil de detectar cuyo problema se ha agravado en los ultimos anos a causa de Inter net  una inmensa fuente de informacion que permite a los usuarios copiar y apropiarse  de forma muy sencilla  del contenido original de otros autores  Aunque el plagio se puede detectar de forma manual  dada la gran cantidad de contenidos que se publican  es una tarea practicamente imposible de llevar a cabo  aun mas si las fuentes de plagio vienen de documentos en otros idiomas  Actualmente existe un gran interes  dentro de la literatura y la ciencia  por investigar y desarrollar sistemas de deteccion de simili tud a nivel monolingue y translingue que sean capaces de detectar de forma automatica las secciones de plagio entre documentos  La co munidad academica tambien se ve beneficiada por dichos sistemas  ya que permite la deteccion y disuasion por parte de los profesores hacia su alumnado  de las practicas habituales de copiar y pegar  sin referencia alguna a la fuente de procedencia  de contenidos originales obtenidos de la Web En la presente tesis describimos el estado del arte en materia de deteccion de plagio textual a nivel </raw_string>
  </article>
  <article>
    <title>Visualization and Detection of Multiple Aliases in Social Media</title>
    <count>776</count>
    <raw_string>IT 13 050 Examensarbete 30 hp Juli 2013 Visualization and Detection of Multiple Aliases in Social Media Amendra Shrestha Institutionen fr informationsteknologi Department of Information Technology Teknisk  naturvetenskaplig fakultet UTHenheten Besksadress  ngstrmlaboratoriet Lgerhyddsvgen 1 Hus 4  Plan 0 Postadress  Box 536 751 21 Uppsala Telefon  018  471 30 03 Telefax  018  471 30 00 Hemsida  http wwwteknat uu sestudent Abstract Visualization and Detection of Multiple Aliases in Social Media Amendra Shrestha Monitoring and analysis of web forums are becoming important for intelligence analysts around the globe since terrorists and extremists are using forums for spreading propaganda and communicating with each other  Various tools for analyzing the content of forum postings and identifying aliases that need further inspection by analysts have been proposed throughout literature  But a problem related to web forums is that individuals can make use of several aliases and conceal their identity  In this thesis work we propose a number of matching techniques for detecting forum users who make use of multiple aliases  By combining different techniques such as time profiling and stylometric analysis of messages the accuracy of recognizing users with multiple aliases increases  as shown in experiments conducted on the  ICWSM International Conference on Weblogs and Social Media Dataset Boards ie Dataset  Tryckt av  Reprocentralen ITC IT 13 050 Examinator  Ivan Christoff mnesgranskare  Mohamed Faouzi Atig Handledare  Lisa Kaati This thesis work is dedicated to those innocent men  women and children who losses their life in any kind of terror attacks  Acknowledgements I would like to express my sincere gratitude to my thesis supervisor  Mrs Lisa Kaati for her expert guidance and valuable feedback throughout the research  Her guidance helped me in </raw_string>
  </article>
  <article>
    <title>Concept based Tree Structure Representation for Paraphrased Plagiarism Detection</title>
    <count>776</count>
    <raw_string>i Masters Computing Minor Thesis Concept based Tree Structure Representation for Paraphrased Plagiarism Detection By Kiet Nim nimhy003mymail unisaeduau A thesis submitted for the degree of Master of Science  Computer and Information Science  School of Computer and Information Science University of South Australia November 2012 Supervisor Dr Jixue Liu Associate Supervisor Dr Jiuyong Li ii Declaration I declare that the thesis presents the original works conducted by myself and does not incorporate without reference to any material submitted previously for a degree in any university  To the best of my knowledge  the thesis does not contain any material previously published or written except where due acknowledgement is made in the content  Kiet Nim November 2012 iii Acknowledgements I would like to express my sincere gratitude to my supervisors  Dr Jixue Liu and Dr Jiuyong Li  professors and researchers at University of South Australia  for their dedicated support  professional advice  feedback and encouragement throughout the period of conducting the study  In addition  I would like to thank all of my course coordinators for their dedicated and indepth teaching  Finally  I would like to thank my family for always encouraging and providing me their full support throughout the study in Australia  iv Abstract In the era of World Wide Web  searching for information can be performed easily by the support of several search engines and online databases  However  this also makes the task of protecting intellectual property from information abuses become more difficult  Plagiarism is one of those dishonest behaviors  Most existing systems can efficiently detect literal plagiarism where exact copy or only minor changes are made  In cases where plagiarists use intelligent methods to hide their intentions  these PD </raw_string>
  </article>
  <article>
    <title>Identifying Sources of Global Contention in Constraint Satisfaction Search</title>
    <count>776</count>
    <raw_string>Title Identifying sources of global contention in constraint satisfaction search Author s  Grimes  Diarmuid Publication date 201207 Original citation Grimes  D  2012  Identifying sources of global contention in constraint satisfaction search  PhD Thesis  University College Cork  Type of publication Doctoral thesis Rights  2012  Diarmuid Grimes http creativecommons orglicensesbyncnd30 Item downloaded from http hdl handle net10468646 Downloaded on 20120805T07 5942Z Identifying Sources of Global Contention in Constraint Satisfaction Search Diarmuid Grimes A Thesis Submitted to the National University of Ireland in Fulfillment of the Requirements for the Degree of Doctor of Philosophy  July  2012 Research Supervisor  Dr Richard J Wallace Research Supervisor  Prof Eugene C Freuder Head of Department  Prof James Bowen Department of Computer Science  National University of Ireland  Cork  Contents Abstract xv Declaration xvii 1 Introduction 1 11 Background                              1 12 Motivation                               4 13 Overview of Dissertation                       6 14 Summary of Contributions                      8 2 Background 11 21 Constraint Satisfaction Problems                   11 211 Complexity Theory        </raw_string>
  </article>
  <article>
    <title>Some Issues in Automatic Genre Classification of Web Pages</title>
    <count>776</count>
    <raw_string>JADT 2006  8 es Journes Internationales dAnalyse statistique des Donnes Textuelles Some Issues in Automatic Genre Classification of Web Pages Marina Santini University of Brighton  Lewes Rd  Brighton  UK Abstract In this paper  two experiments in automatic genre classification of web pages are presented  These two experiments are designed to highlight three important issues related to genre classification  corpus composition and genre palettes  feature representativeness  and exportability of classification models  Results show the influence of corpus composition and genre palette on classification rates  They also show how well and to what extent feature sets represent genres in a palette  and give an idea of the limitations of the classification models when exported and used for predictive tasks  Rsum Dans cet article nous prsentons deux expriences dapprentissage pour le classement automatique des pages web en fonction de diffrents genres textuels  Ces deux expriences ont t conues pour mettre en lumire trois aspects importants qui peuvent influer sur le rsultat du classement  la composition du corpus et les genres utiliss  la reprsentativit des traits linguistiques et nonlinguistiques utiliss dans les modles et  enfin  lexportation des modles de classement  La premire exprience montre que les rsultats sont clairement influencs par la composition du corpus et par les genres utiliss  La seconde exprience montre les limites de la reprsentativit des traits et donne aussi une ides des limites des modles de classement quand on les exporte sur un autre corpus pour des fonctions prdictives  Keywords  genre classification  web pages  machine learning  genre prediction 1  Introduction In this paper  we present two experiments that use machine learning for automatically classifying web pages according to genre  These two </raw_string>
  </article>
  <article>
    <title>Personal Sense in Subjective Language Research in the Blogosphere</title>
    <count>772</count>
    <raw_string>Personal sense in subjective language research in the blogosphere Polina Panicheva Master of Science  research  in Information Technology Institute of Technology Tallaght Dublin  Ireland 2011 Personal sense in subjective language research in the blogosphere Polina Panicheva being a thesis presented for the award of Master of Science  research  degree in Information Technology Supervisors Dr John Cardiff Department of Computing Institute of Technology Tallaght  Dublin  Ireland Prof Paolo Rosso Universidad Politecnica de Valencia  Spain Submitted to the Higher Education and Training Awards Council  HETAC  May 2011 Abstract Blogs are a very important part of the digital world  indeed they can be viewed as a digital representation of the whole world  People share pictures and videos  describe their daily life  ask questions and  of course  give opinions  The blogosphere presents a unique opportunity to obtain huge statistics about what people like  feel  need  about their private states  The vast and evergrowing volumes of bloggers  and thus  information  demand an automated way of analyzing blog texts  This gives rise to a new research direction combining computing  linguistics and psychology  sentiment analysis  the computational treatment of  in alphabetical order  opinion  sentiment  and subjectivity in text  Objective characteristics of the writers based on their texts can be analyzed  their age  gender  social affiliation  character  subjective characteristics as moods  negative or positive opinions  polarity  emotions towards an object  can also be investigated  In the thesis we argue that individual component of word meaning is an essential phenomenon in subjectivity  and elaborate the notion of Personal Sense  described in  Leontev  </raw_string>
  </article>
  <article>
    <title>Extraction and Classification of Handwritten Annotations for Pedagogical Use</title>
    <count>771</count>
    <raw_string>EDIC RESEARCH PROPOSAL 1 Extraction and Classification of Handwritten Annotations for Pedagogical Use Andrea Mazzei CRAFT  IC  EPFL AbstractThis research proposal first describes a method for extracting and classifying handwritten annotations on printed documents using a simple camera integrated in a lamp or a mobile phone  The ambition of such a research is to offer a seamless integration of notes taken on printed paper in our daily interactions with digital documents  Existing studies propose a classification of annotations based on their form and function  We demonstrate a method for automating such a classification and report experimental results showing the classification accuracy  We also present the design of a preliminary user study to investigate the effects of annotation types on the cognitive processes involved in rereading documents  Index Termsmachineprinted and handwritten text separa  tion  document processing  annotation classification I INTRODUCTION DESPITE the numerous digital reincarnations of the traditional printed book  paper remains the preferred medium for reading  A consistent body of literature comparing reading activities on paper and online documents has been summarized by OHara et al  1  Paper documents offer better legibility and better orientation and location  Physical tangibility facil itates handwriting and concurrent reading on multiple pages  Proposal submitted to committee  August 18th  2010  Can didacy exam date  August 26th  2010  Candidacy exam com mittee  Exam president  thesis director  coexaminer  This research plan has been approved  Date             Doctoral candidate              name and signature  Thesis director             </raw_string>
  </article>
  <article>
    <title>Text Surface Features for Genre Request in Information Retrieval</title>
    <count>771</count>
    <raw_string>Text Surface Features for Genre Request in Information Retrieval Wenxin Xiong National Research Center for Foreign Language Education Beijing Foreign Studies University Beijing  100089  China email  xiongwenxinbfsueducn AbstractTraditional information retrieval focuses on topic relevance by computing the similarity between query and texts using contentbased Bag of Words  BOW strategy  This approach can not handle the terms expressing genre request in users query  which maynot occur in target texts with the same form or other derivative forms  We propose a post text classification on the results returned by search engines to meet the request expressed by genre vocabulary in queries  Some statistics of text surface features for genre detection  i e  average sentence length  number of specific part of speech and punctuations are examined  An experiment on identifying narrative texts and commentaries about a news event by using the abovementioned variables has been conducted  and yielded an encouraging result  Keywords  text surface features  genre  query  information retrieval I INTRODUCTION The different functions of words in queries submitted by users are overlooked by most researchers  In the state  ofart vector space model  each segmentation component in a query is treated uniformly or weighted by some strategies as a vector in the space 1  2  3  4  After filtering stop word  the remaining words in user s queries and words of target texts are utilized for computing the topicrelated similarity by counting their occurrences  Actually even in these remaining contentbearing query words  there are different implementations in target texts 5  Some words are related to the topic  others are concerned with genre of target texts or their expression styles  Consider the following </raw_string>
  </article>
  <article>
    <title>An Evaluation of Text Classification Methods for Literary Study</title>
    <count>770</count>
    <raw_string>c 2006 by Bei Yu  All rights reserved  AN EVALUATION OF TEXT CLASSIFICATION METHODS FOR LITERARY STUDY BY BEI YU BE  University of Science and Technology of China  1996 ME  Chinese Academy of Sciences  1999 DISSERTATION Submitted in partial fulfillment of the requirements for the degree of Doctor of Philosophy in Library and Information Science in the Graduate College of the University of Illinois at UrbanaChampaign  2006 Urbana  Illinois Abstract Text classification methods have been evaluated on topic classification tasks  This thesis extends the empirical evaluation to emotion classification tasks in the literary domain  This study selects two literary text classification problems  the eroticism classification in Dickin sons poems and the sentimentalism classification in early American novels  as two cases for this evaluation  Both problems focus on identifying certain kinds of emotion  a document property other than topic  This study chooses two popular text classification algorithms  naive Bayes and Support Vector Machines  SVM  and three feature engineering options  stemming  stopword removal and statistical feature selection  Odds Ratio and SVM   as the subjects of evaluation  This study aims to examine the effects of the chosen classifiers and feature engineering options on the two emotion classification problems  and the interaction between the classifiers and the feature engineering options  This thesis seeks empirical answers to the following research questions  1  is SVM a better classifier than naive Bayes regarding classification accuracy  new literary knowledge discovery and potential for examplebased retrieval  2  is SVM a better feature selection method than Odds Ratio regarding feature reduction rate and classification accuracy improvement  3  does stop word removal affect the classification performance  4 </raw_string>
  </article>
  <article>
    <title>Combining Classifiers for Flexible Genre Categorization of Web Pages</title>
    <count>766</count>
    <raw_string>Combining classifiers for flexible genre categorization of web pages Jebari Chaker  King Saud University College of Computer and Information Sciences Computer Science Department P O Box 51178  Riyadh 11543  KSU jebarichakeryahoofr Abstract With the increase of the number of web pages  it is very difficult to find wanted information easily and quickly out of thousands of web pages retrieved by a search engine  To solve this problem  many researches propose to classify documents according to their genre  which is another criteria to classify documents different from the topic  Most of these works assign a document to only one genre  In this paper we propose a new flexible approach for document genre categorization  Flexibility means that our approach assigns a document to all predefined genres with different weights  The proposed approach is based on the combination of two homogenous classifiers  contextual and structural classifiers  The contextual classifier uses the URL  while the structural classifier uses the document structure  Both contextual and structural classifiers are centroidbased classifiers  Experimentations provide a microaveraged breakeven point  BEP  more than 85   which is better than those obtained by other categorization approaches  Keywords Genre  structure  centroid  URL  categorization  combination  flexible  1  Introduction In front of the explosive growth of the number of web pages  users cannot quickly find desired information among the huge list of web pages returned by a search engine  To deal with this problem  many categorization approaches have been proposed to classify the result of search engines  Most of them have been interested by topic categorization 32  Even  if the documents are classified successfully by their topics  they </raw_string>
  </article>
  <article>
    <title>Analyse de la Performance Musicale et Synthese Sonore Rapide</title>
    <count>766</count>
    <raw_string>No dordre  3327 THESE PRESENTEE A LUNIVERSITE BORDEAUX I ECOLE DOCTORALE DE MATHEMATIQUES ET DINFORMATIQUE Par Matthias Robine POUR OBTENIR LE GRADE DE DOCTEUR SPECIALITE  INFORMATIQUE ANALYSE DE LA PERFORMANCE MUSICALE ET SYNTHESE SONORE RAPIDE Soutenue le  13 Decembre 2006 Apres avis des rapporteurs  Daniel Arfib                Directeur de Recherche Caroline Traube            Professeure adjointe Devant la commission dexamen composee de  Daniel Arfib                Directeur de Recherche Rapporteur Philippe Depalle            Professeur             Examinateur Myriam DesainteCatherine Professeur             Examinateur Sylvain Marchand          Matre de Conferences Examinateur Robert Strandh            Professeur             Examinateur Caroline Traube            Professeure adjointe    Rapporteur 2006 i Analyse de la performance musicale et synthese sonore rapide Resume  Cette these dinformatique musicale explore dune part la performance instrumentale et propose dautre part des algorithmes de synthese additive rapide  Un etat de lart sur lanalyse du jeu instrumental est dabord realise  explorant les differentes composantes de linterpretation musicale  Une etude sur limportance du doigte au piano est alors presentee  La performance pianistique est ainsi analysee pour mettre en evidence linfluence du doigte sur la performance  Le </raw_string>
  </article>
  <article>
    <title>Authorship Identification for Chinese Texts Based on Dependency Grammar</title>
    <count>742</count>
    <raw_string>Authorship Identification for Chinese Texts Based on Dependency Grammar Wan Jing  Liu Yuling  Sun Xingming  Sun Decai Journal of Convergence Information Technology  Volume6  Number 6  June 2011 Authorship Identification for Chinese Texts Based on Dependency Grammar 1Wan Jing  1 Liu Yuling  21Sun Xingming  1Sun Decai 1 College of Information Science and Engineering  Hunan University  No 252  Lushan South Road  Changsha  410082  China  wj319126com  yulingliu126com  sunnudt163com  sdecai163com 2College of Computer and Software  Nanjing University of Information Science  Technology  Nanjing  210044  China Corresponding Author  yulingliu126com doi  104156jcit vol6issue633 Abstract A crucial issue of authorship identification is to find a set of features which can represent the writing style of a particular author  Due to Chinese special characteristics  the features used for identifying Chinese authorship are relatively few  In this paper  we propose a method of Chinese authorship identification based on Dependency Grammar  Vector Space Model is employed to represent the document information  and the value of dimensions in a vector space is based on the frequencies of each feature occurring in each document  First  taking advantage of the dependency parsing technique in Natural Language Processing field  we extract 24 dependencies as the syntactic feature for identification  Next  we extract another three features  empty word  punctuation  and Part of Speech  together with dependency to comprise a large feature set  In order to get higher accuracies  we use Principal Component Analysis PCA  to optimize the feature set  We conduct experiments on Chinese literature to examine the effectiveness of our method  Together with Support Vector Machine  the experimental </raw_string>
  </article>
  <article>
    <title>Overview of the 3rd International Competition on Plagiarism Detection</title>
    <count>742</count>
    <raw_string>Overview of the 3rd International Competition on Plagiarism Detection Martin Potthast1  Andreas Eiselt1  Alberto BarrnCedeo2  Benno Stein1  and Paolo Rosso2 1Web Technology  Information Systems BauhausUniversit Weimar  Germany 2Natural Language Engineering Lab  ELiRF Universidad Politcnica de Valencia  Spain panwebisde http panwebisde Abstract This paper overviews eleven plagiarism detectors that have been de veloped and evaluated within PAN11  We survey the detection approaches de veloped for the two subtasks external plagiarism detection  and intrinsic pla  giarism detection   and we report on their detailed evaluation based on the third revised edition of the PAN plagiarism corpus PANPC11  1 Introduction Copying another authors text and claiming its authorship is called plagiarism  While research on automatic plagiarism detection has been conducted for decades  the stan dardized evaluation of plagiarism detection algorithms has a short history 13  In this regard we have organized three competitions on plagiarism detection  the latest one held in conjunction with the 2011 CLEF conference  This paper overviews the submitted de tectors and evaluates their performances  11 Plagiarism Detection Let s  splg  dplg  ssrc  dsrc denote a plagiarism case where splg is a passage of doc  ument dplg and a plagiarized version of some source passage ssrc in dsrc  Given dplg  the task of a plagiarism detector is to detect s by reporting a corresponding plagia  rism detection r  rplg  dplg  rsrc  dsrc  We say that r detects s iff splg  rplg 6   ssrc  rsrc 6   and dsrc  dsrc  This task can be tackled with external plagiarism detection as well as with intrinsic plagiarism detection  Algorithms for external plagiarism detection </raw_string>
  </article>
  <article>
    <title>Diverse Queries and Feature Type Selection for Plagiarism Discovery</title>
    <count>738</count>
    <raw_string>Diverse Queries and Feature Type Selection for Plagiarism Discovery Notebook for PAN at CLEF 2013 imon Suchomel  Jan Kasprzak  and Michal Brandejs Faculty of Informatics  Masaryk University suchomelkas brandejsfimunicz Abstract This paper describes approaches used for the Plagiarism Detection task in PAN 2013 international competition on uncovering plagiarism  authorship  and social software misuse  We present modified threeway search methodology for Source Retrieval subtask and analyse snippet similarity performance  The results show  that presented approach is adaptable in realworld plagiarism situations  For the Detailed Comparison task  we discuss feature type selection and global postprocessing  Resulting performance is significantly better with the described modifications  and further improvement is still possible  1 Introduction In PAN 20131 competition on plagiarism detection we participated in both the Source Retrieval and the Text Alignment subtasks  In both tasks we adapted methodology used in PAN 20122 11  Section 2 describes querying approach for source retrieval  where we used three different types of queries  We present a new type of query based on text paragraphs  The query execution was controlled by its type and by preliminary sim  ilarities discovered during the searches  Section 3 describes our approach for the text alignment  pairwise comparison  subtask  We briefly introduce our system  and then we discuss the feature types  which are usable for pairwise comparison  including the eval uation of their feasibility for this purpose  We then describe the global  corpuswide  optimizations used  and finally we discuss the results achieved and further development  2 Source Retrieval The source retrieval is a subtask in a plagiarism detection process during which only a relatively small subset of documents are retrieved from the </raw_string>
  </article>
  <article>
    <title>Applying Stylometric Analysis Techniques to Counter Anonymity in Cyberspace</title>
    <count>737</count>
    <raw_string>Applying Stylometric Analysis Techniques to Counter Anonymity in Cyberspace Jianwen Sun  Zongkai Yang  Sanya Liu National Engineering Research Center for Elearning  Central China Normal University  Wuhan  China Email  sunjwworkgmailcom  zkyangmailccnueducn  lsy5918gmailcom Pei Wang School of Information Management  Wuhan University  Wuhan  China Email  wangpeiworkgmailcom AbstractDue to the ubiquitous nature and anonymity abuses in cyberspace  it s difficult to make criminal identity tracing in cybercrime investigation  Writeprint identification offers a valuable tool to counter anonymity by applying stylometric analysis technique to help identify individuals based on textual traces  In this study  a framework for online writeprint identification is proposed  Variable length character ngram is used to represent the authors writing style  The technique of IG seeded GA based feature selection for Ensemble  IGAE  is also developed to build an identification model based on individual author level features  Several specific components for dealing with the individual feature set are integrated to improve the performance  The proposed feature and technique are evaluated on a real world data set encompassing reviews posted by 50 Amazon customers  The experimental results show the effectiveness of the proposed framework  with accuracy over 94  for 20 authors and over 80  for 50 ones  Compared with the baseline technique  Support Vector Machine   a higher performance is achieved by using IGAE  resulting in a 2  and 8  improvement over SVM for 20 and 50 authors respectively  Moreover  it has been shown that IGAE is more scalable in terms of the number of authors  than author group level based methods  Index Termsstylometric analysis  writeprint identification  character ngram  ensemble learning  genetic algorithm </raw_string>
  </article>
  <article>
    <title>A Novel Random Subspace Method for Online Writeprint Identification</title>
    <count>737</count>
    <raw_string>A Novel Random Subspace Method for Online Writeprint Identification Zhi Liu  Zongkai Yang  Sanya Liu National Engineering Research Center for ELearning  Central China Normal University  Wuhan  P R China Email  liuzhi8673gmailcom  lsy5918gmailcom Wenting Meng Department of Computer Science  Central China Normal University  Wuhan  P R China AbstractWith the widespread application of computer network technology  diverse anonymous cyber crimes begin to appear in the online community  The anonymous nature of onlineinformation distribution makes writeprint identification a critical forensic problem  But the difficulty of the task is the huge number of features in even a moderatesized available text corpus  which causes the problem of overtraining  In this paper  we proposed a novel random subspace method by constructing a set of stable classifiers to take advantage of nearly all the discriminative information in the high dimensional feature space  In the construction of base classifiers  an optimized synergetic neural network is employed to provide probabilistic information for each class  Performance results on the subset of Reuters Corpus Volume 1 RCV1  show that the proposed random subspace method achieves the better identification performance than a single classifier and conventional random subspace methods  Index Termsonline writeprint  random subspace method  synergetic neural network  principal component analysis  linear discriminant analysis I INTRODUCTION With the rapid advancement of new media technology  some network users wish to share some attractive information via email  forum  online chat room  blog  microblog  etc  Indeed  the openness and anonymity of online community makes people freely express their opinions  But cyber criminals often take this opportunity to deliver some illegal information by some anonymous ways  such as sending offensive  </raw_string>
  </article>
  <article>
    <title>An Examination of Genre Attributes for Web Page Classification</title>
    <count>737</count>
    <raw_string>An Examination of Genre Attributes for Web Page Classification Lei Dong  Carolyn Watters  Jack Duffy  Michael Shepherd Faculty of Computer Science Dalhousie University  Halifax  Nova Scotia  Canada ldong  watters  shepherdcs dalca  jack duffydalca  Abstract In this paper  we describe a set of experiments to examine the effect of various attributes of web genre on the automatic identification of the genre of web pages  Four different genres are used in the data set  namely  FAQ  News  EShopping and Personal Home Pages  The effects of the number of features used to represent the web pages 5  20  or 100  as well as the types of attributes  content  form  functionality  singly and in various combinations are examined  The results indicate that fewer features produce better precision but more features produce better recall  and that attributes in combinations will always perform better than single attributes  1  Introduction A genre is a  classifying statement   15  and is characterized by having similar content and form where content refers to themes and topics and form refers to   observable physical and linguistic features    25  It allows us to recognize items that are similar even in the midst of great diversity  For instance  the detective novel is a particular genre and we are able to recognize novels as members of that genre as opposed to being of some other genre  even though the novels themselves may be very different  Once recognized as being of the same genre  we can then more easily compare the individual novels  As the World Wide Web continues to grow exponentially </raw_string>
  </article>
  <article>
    <title>A Novel Random Subspace Method for Online Writeprint Identification</title>
    <count>737</count>
    <raw_string>A Novel Random Subspace Method for Online Writeprint Identification Zhi Liu  Zongkai Yang  Sanya Liu National Engineering Research Center for ELearning  Central China Normal University  Wuhan  P R China Email  liuzhi8673gmailcom  lsy5918gmailcom Wenting Meng Department of Computer Science  Central China Normal University  Wuhan  P R China AbstractWith the widespread application of computer network technology  diverse anonymous cyber crimes begin to appear in the online community  The anonymous nature of onlineinformation distribution makes writeprint identification a critical forensic problem  But the difficulty of the task is the huge number of features in even a moderatesized available text corpus  which causes the problem of overtraining  In this paper  we proposed a novel random subspace method by constructing a set of stable classifiers to take advantage of nearly all the discriminative information in the high dimensional feature space  In the construction of base classifiers  an optimized synergetic neural network is employed to provide probabilistic information for each class  Performance results on the subset of Reuters Corpus Volume 1 RCV1  show that the proposed random subspace method achieves the better identification performance than a single classifier and conventional random subspace methods  Index Termsonline writeprint  random subspace method  synergetic neural network  principal component analysis  linear discriminant analysis I INTRODUCTION With the rapid advancement of new media technology  some network users wish to share some attractive information via email  forum  online chat room  blog  microblog  etc  Indeed  the openness and anonymity of online community makes people freely express their opinions  But cyber criminals often take this opportunity to deliver some illegal information by some anonymous ways  such as sending offensive  </raw_string>
  </article>
  <article>
    <title>A State of the Art on Computational Music Performance</title>
    <count>736</count>
    <raw_string>Expert Systems with Applications 38  2011  155160Contents lists available at ScienceDirect Expert Systems with Applications journal homepage  wwwelsevier comlocate eswaA state of the art on computational music performance Miguel Delgado  Waldo Fajardo  Miguel MolinaSolana  Department of Computer Science and Artificial Intelligence  Universidad de Granada  Daniel Saucedo Aranda sn  18071 Granada  Spain a r t i c l e i n f oKeywords  Computational music Expressive performance Machine learning09574174  see front matter 2010 Elsevier Ltd A doi101016jeswa201006033  Corresponding author  Email addresses  mdelgadougres  M Delgado   miguelmolinaugres  M MolinaSolana  a b s t r a c t Musical expressivity can be defined as the deviation from a musical standard when a score is per formed by a musician  This deviation is made in terms of intrinsic note attributes like pitch  timbre  timing and dynamics  The advances in computational power capabilities and digital sound synthesis have allowed realtime control of synthesized sounds  Expressive control becomes then an area of great interest in the sound and music computing field  Musical expressivity can be approached from different perspectives  One approach is the musicological analysis of music and the study of the dif  ferent stylistic schools  This approach provides a valuable understanding about musical expressivity  Another perspective is the computational modelling of music performance by means of automatic analysis of recordings  It is known that music performance is a complex activity that involves com plementary aspects from other disciplines such as psychology and acoustics  It requires creativity and eventually  some manual abilities  being a hard task even for humans  Therefore  using machines appears as a very interesting and fascinating issue  </raw_string>
  </article>
  <article>
    <title>The Effect of Personality Type on Deceptive Communication Style</title>
    <count>735</count>
    <raw_string>The Effect of Personality Type on Deceptive Communication Style Tommaso Fornaciari CIMeC  University of Trento Corso Bettini 31  Rovereto  Italy Email  tommasofornaciarigmailcom Fabio Celli CIMeC  University of Trento Corso Bettini 31  Rovereto  Italy Email  fabiocelliunitn it Massimo Poesio University of Essex Wivenhoe Park  Colchester CO4 3SQ  UK Email  massimopoesioessex acuk AbstractIt has long been hypothesized that the ability to deceive depends on personality  some personality types are better  at deceiving in that their deception is harder to recognize  In this work  we evaluate how the pattern of personality of a speaker affects the effectiveness of machine learning models for deception detection in transcripts of oral speech  We trained models to classify as deceptive or not deceptive statements issued in Court by Italian speakers  We then used a system for automatic personality recognition to generate hypotheses about the personality of these speakers  and we clustered the subjects on the basis of their personality traits  It turned out that deception detection models perform differently depending on the patterns of personality traits which characterize the speakers  This suggests that speakers who show certain types of personality also have a communication style in which deception can be detected more  or less  easily  I INTRODUCTION Personality recognition and deception detection have been widely explored and studied using a number of different approaches 42  30  Nonetheless  it is only recently that these tasks have been approached using stylometric techniques  that is  making use of computational methods based on the stylistic features of written or spoken speech samples 32  27  In this perspective  while deception detection is the task of recognizing truth and deception in discourse </raw_string>
  </article>
  <article>
    <title>Integrating Features from Different Sources for Music Information Retrieval</title>
    <count>735</count>
    <raw_string>Integrating Features from Different Sources for Music Information Retrieval Tao Li School of Computer Science Florida International University Miami  FL 33199 taolics fiuedu Mitsunori Ogihara Department of Computer Science University of Rochester Rochester  NY 14620 ogiharacs rochester edu Shenghuo Zhu NEC Laboratories America Cupertino  CA 95014 zshsvneclabscom Abstract Efficient and intelligent music information retrieval is a very important topic of the 21st century  With the ulti mate goal of building personal music information retrieval systems  this paper studies the problem of identifying sim  ilar  artists using both lyrics and acoustic data  In this pa per  we present a clustering algorithm that integrates fea  tures from both sources to perform bimodal learning  The algorithm is tested on a data set consisting of 570 songs from 53 albums of 41 artists using artist similarity pro vided by All Music Guide  Experimental results show that the accuracy of artist similarity classifiers can be signifi cantly improved and that artist similarity can be efficiently identified  1 Introduction In multimedia information retrieval the data are natu rally multimodal  in the sense that they are represented by multiple sets of features  For example  the representation of a movie has three modes   i  the personnel  the producer  the director  the editor  the scenario writer  the music com poser  the cast  etc    ii  the visual features  which sum  marize the scenarios and the actions   and  iii  the acoustic features  which summarize the voice and the background audio   The representation of popular music is also tri modal in some sense  where the second feature set is re  placed </raw_string>
  </article>
  <article>
    <title>The Complex Networks Approach for Authorship Attribution of Books</title>
    <count>734</count>
    <raw_string>Physica A 391  2012  24292437 Contents lists available at SciVerse ScienceDirect Physica A journal homepage  wwwelseviercomlocatephysa The complex networks approach for authorship attribution of books Ali Mehri   Amir H Darooneh  Ashrafalsadat Shariati Department of Physics  Zanjan University  POBox 45196313  Zanjan  Iran a r t i c l e i n f o Article history  Received 1 July 2011 Received in revised form 12 November 2011 Available online 11 December 2011 Keywords  Complex systems Computational linguistics Nonextensive statistical mechanics Authorship attribution a b s t r a c t Authorship analysis by means of textual features is an important task in linguistic studies  We employ complex networks theory to tackle this disputed problem  In this work  we focus on some measurable quantities of word cooccurrence network of each book for authorship characterization  Based on the network features  attribution probability is defined for authorship identification  Furthermore  two scaling exponents  qparameter and exponent  are combined to classify personal writing style with acceptable high resolution power  The qparameter  generally known as the nonextensivity measure  is calculated for degree distribution and theexponent comes fromapower law relationship between number of links and number of nodes in the cooccurrence network constructed for different books written by each author  The applicability of the presented method is evaluated in an experiment with thirty six books of five Persian litterateurs  Our results show high accuracy rate in authorship attribution   2011 Elsevier BV All rights reserved  1  Introduction Recently the complex networks theory appears as a suitable framework for studying social and natural systems 12  In the language of networks  the systems entities are regarded as vertices of a </raw_string>
  </article>
  <article>
    <title>Authorship Identification Using a Reduced Set of Linguistic Features</title>
    <count>734</count>
    <raw_string>Authorship Identification Using a Reduced Set of Linguistic Features Notebook for PAN at CLEF 2012 Stefan Ruseti  Traian Rebedea Department of Computer Science and Engineering University Politehnica of Bucharest  Romania stefanrusetiyahoocom  traian rebedeacs pubro Abstract  The proposed solution for authorship attribution combines a couple of the most important features identified in previous research in this domain with classification algorithms in order to detect the correct author  We consider that the most relevant aspect of our work is the small number of linguistic features and the use of the same framework to solve both the open and the closed class authorship problem  by only changing the classification algorithm  This approach obtained an overall 77  accuracy with regard to the total number of correctly classified documents  1 Introduction The problem of authorship identification or attribution of text documents has been widely studied in the last decades  especially in the last 20 years  but the solutions are not mature enough to consider the problem solved  Nowadays  the Web offers very large amounts of texts to be used as corpora for authorship identification  but it also provides many different types of discourse that may be analyzed  from narratives and emails to online conversations and social network updates  It is obvious that each type of discourse should be treated independently  nevertheless even the problem of identifying the author of large narrative texts is far from being closed  Authorship attribution may be divided into two different subtasks  determining the most descriptive features of the texts under consideration  then applying a classification algorithm in order to detect the most probable author 1  The methods for conducting the classification stage range from principal component analysis and cluster </raw_string>
  </article>
  <article>
    <title>Analysis of Stylometric Variables in Long and Short Texts</title>
    <count>734</count>
    <raw_string>Procedia  Social and Behavioral Sciences 95  2013  604  611 18770428  2013 The Authors  Published by Elsevier Ltd Selection and peerreview under responsibility of CILC2013  doi  101016j sbspro201310688 ScienceDirect 5th International Conference on Corpus Linguistics  CILC2013  Analysis of Stylometric Variables in Long and Short Texts Fernanda LpezEscobedo  CarlosFrancisco MndezCruz  Gerardo Sierra  Julin SolrzanoSoto Instituto de IngenieraUniversidad Nacional Autnoma de MxicoCircuito Escolar sn Ciudad Universitaria  Mxico DF 04510  Mxico Abstract This paper presents some experiments in the task of authorship attribution  We achieve this task by a stylometric analysis of some stylistic markers tested in two Spanish corpora  The first corpus is composed of long texts written by professional authors  while the second corpus is formed by short texts written by students  In both corpora  different text genres are included  Thus  the objective of this study is to analyze several stylometric variables to test its capacity as markers for authorship attribution when the corpora vary in size and text genre  We represent the texts as high dimensional vectors and we visualize the similarities between them using multidimensional scaling  We conclude that the length of texts is a factor that affects the discriminatory capacity of the stylometric variables  We also found that there are certain variables that are better than others to identify specific authors and specific text genres   2013 The Authors  Published by Elsevier Ltd Selection and peerreview under responsibility of CILC2013  Keywords  authorship attribution  stylometry  stylometric variables  multidimensional scaling 1  Introduction One of the main approaches to authorship attribution is corpusbased stylometric analysis  Corpora used in this task are commonly formed by a set of documents </raw_string>
  </article>
  <article>
    <title>Using Syntactic Features to Predict Author Personality from Text</title>
    <count>734</count>
    <raw_string>Psychological profiling through textual analysis  John Noecker Jr  Michael Ryan and Patrick Juola Duquesne University  Abstract In this article  we examine the application of computational stylometry to psy chological profiling  We adapt several techniques  which have proven useful for author identification to the problem of identifying an individual authors MyersBriggs personality type indicator from the statistical features of the text  The MyersBriggs type indicator assigns four binary classifications to define per sonality type  Extrovert Introvert  IntuitiveSensing  ThinkingFeeling  and JudgingPerceiving  For this study  we use the Personae corpus  which consists of 145 Dutchlanguage texts all pertaining to a specific topic  each labeled with the MyersBriggs personality profile of the author  Luyckx K and Daelemans W Personae  A Corpus for Author and Personality Prediction from Text  In Proceedings of the 6th Language Resources and Evaluation Conference  Marrakech  Morocco  International Conference on Language Resources and Evaluation  2008   Our system builds upon earlier work by Luyckx and Daelemans  Using Syntactic Features to Predict Author Personality from Text  In Proceedings of Digital Humanities 2008  Oulu  Finland  Digital Humanities  pp  1469  to provide a set of best practices for personality profiling  We propose a more sophisticated modeling technique  combined with more advanced feature selection and stateoftheart analysis methods from author identification to achieve a signifi cant improvement over previous systems   1 Introduction An authors writing style has been hypothesized to contain clues to many aspects of the writers life  Studies have shown that it is possible to identify the author  Binongo  2003   gender  Koppel et al   2002   education level  </raw_string>
  </article>
  <article>
    <title>Using the Sociological Imagination to Teach about Academic Integrity</title>
    <count>734</count>
    <raw_string>http tso sagepubcom Teaching Sociology http tso sagepubcomcontent414377 The online version of this article can be found at  DOI  1011770092055X13490750 2013 41  377 originally published online 23 May 2013Teaching Sociology Mary Nell Trautner and Elizabeth Borland Using the Sociological Imagination to Teach about Academic Integrity Published by  http www sagepublicationscom On behalf of  American Sociological Association can be found atTeaching SociologyAdditional services and information for http tso sagepubcomcgialertsEmail Alerts  http tso sagepubcomsubscriptionsSubscriptions  http www sagepubcomjournals Reprints navReprints  http www sagepubcomjournalsPermissions navPermissions  What is This   May 23  2013OnlineFirst Version of Record  Oct 1  2013Version of Record   at INST POLITECNICO NACIONAL on January 30  2014tso sagepubcomDownloaded from Teaching Sociology 41 4  377 388  American Sociological Association 2013 DOI  1011770092055X13490750 ts sagepubcom Many students do not understand what kinds of behaviors are dishonest  and  perhaps even more troubling  many do not think of actions that faculty would consider to be academic dishonesty as prob  lematic  Howard and Davies 2009  Jones 2011  Power 2009   Despite widespread institutional  departmental  and courselevel policies promoting academic integrity  1 both classic and contemporary studies find academic dishonesty to be prevalent among college students  For example  LaBeff et al   1990  found that more than half of students had engaged in dishonest behavior within a sixmonth period  fewer than 2 percent had been caught  More recently  Stephens  Young  and Calabrese  2007  and McCabe  Butterfield  and Trevino  2012  found that more than twothirds of college students report some form of cheating behavior  Instructors find such behaviors to be very trou blesome  </raw_string>
  </article>
  <article>
    <title>Authorship Attribution for Twitter in 140 Characters or Less</title>
    <count>733</count>
    <raw_string>Authorship Attribution for Twitter in 140 characters or less Robert Layton Internet Commerce Security Laboratory University of Ballarat Email  rlaytonicslballarateduau Paul Watters Internet Commerce Security Laboratory University of Ballarat Email  pwattersicslballarateduau Richard Dazeley Data Mining and Informatics Research Group University of Ballarat Email  rdazeleyballarateduau AbstractAuthorship attribution is a growing field  moving from beginnings in linguistics to recent advances in text mining  Through this change came an increase in the capability of authorship attribution methods both in their accuracy and the ability to consider more difficult problems  Research into authorship attribution in the 19 century considered it difficult to determine the authorship of a document of fewer than 1000 words  By the 1990s this values had decreased to less than 500 words and in the early 21 century it was considered possible to determine the authorship of a document in 250 words  The need for this ever decreasing limit is exemplified by the trend towards many shorter communications rather than fewer longer communications  such as the move from traditional multipage handwritten letters to shorter  more focused emails  This trend has also been shown in online crime  where many attacks such as phishing or bullying are performed using very concise language  Cybercrime messages have long been hosted on Internet Relay Chats  IRCs  which have allowed members to hide behind screen names and connect anonymously  More recently  Twitter and other short message based web services have been used as a hosting ground for online crimes  This paper presents some evaluations of current techniques and identifies some new preprocessing methods that can be used to enable authorship to be determined at rates significantly better than chance for documents of 140 characters or less  a format </raw_string>
  </article>
  <article>
    <title>Viterbi Based Alignment Between Text Images and their Transcripts</title>
    <count>733</count>
    <raw_string>Proceedings of the Workshop on Language Technology for Cultural Heritage Data  LaTeCH 2007   pages 916  Prague  28 June 2007  c2007 Association for Computational Linguistics Viterbi Based Alignment between Text Images and their Transcripts  Alejandro H Toselli  Veronica Romero and Enrique Vidal Institut Tecnologic d Informatica Universitat Politecnica de Valencia Cam  de Vera sn 46071  Valencia  Spain ahector vromero evidal iti upves Abstract An alignment method based on the Viterbi algorithm is proposed to find mappings be tween word images of a given handwrit ten document and their respective  ASCII  words on its transcription  The approach takes advantage of the underlying segmen tation made by Viterbi decoding in hand  written text recognition based on Hidden Markov Models  HMMs  Two HMMs modelling schemes are evaluated  one using 78HMMs  one HMM per character class  and other using a unique HMM to model all the characters and another to model blank spaces  According to various metrics used to measure the quality of the alignments  en couraging results are obtained  1 Introduction Recently  many online digital libraries have been publishing large quantities of digitized ancient hand  written documents  which allows the general pub  lic to access this kind of cultural heritage resources  This is a new  comfortable way of consulting and querying this material  The Biblioteca Valenciana Digital  BiValDi1 is an example of one such digital library  which provides an interesting collection of handwritten documents  This work has been supported by the EC  FEDER   the Spanish MEC under grant TIN200615694C0201  and by the Consellera dEmpresa  Universitat i Ciencia  Generalitat Va  lenciana under contract GV06252  </raw_string>
  </article>
  <article>
    <title>Toward Personality Insights from Language Exploration in Social Media</title>
    <count>733</count>
    <raw_string>Toward Personality Insights from Language Exploration in Social Media H Andrew Schwartz  Johannes C Eichstaedt  Lukasz Dziurzynski  Margaret L Kern  Martin E P Seligman and Lyle H Ungar University of Pennsylvania Eduardo Blanco Lymba Corporation Michal Kosinski and David Stillwell University of Cambridge Abstract Language in social media reveals a lot about peoples personality and mood as they discuss the activities and relationships that constitute their everyday lives  Al though social media are widely studied  researchers in computational linguistics have mostly focused on pre  diction tasks such as sentiment analysis and authorship attribution  In this paper  we show how social media can also be used to gain psychological insights  We demon  strate an exploration of language use as a function of age  gender  and personality from a dataset of Facebook posts from 75000 people who have also taken person  ality tests  and we suggest how more sophisticated tools could be brought to bear on such data  Introduction With the growth of social media such as Twitter and Face  book  researchers are being presented with an unprecedented resource of personal discourse  Computational linguists have taken advantage of these data  mostly addressing pre  diction tasks such as sentiment analysis  authorship attri bution  emotion detection  and stylometrics  A few works have also been devoted to predicting personality  i e  stable unique individual differences   Prediction tasks have many useful applications ranging from tracking opinions about products to identifying messages by terrorists  However  for social sciences such as psychology  gaining insight is at least as important as making accurate predictions  In this paper  we explore the use of various language fea </raw_string>
  </article>
  <article>
    <title>Evaluation of Different Approaches to Training a Genre Classifier</title>
    <count>731</count>
    <raw_string>Evaluation of Different Approaches to Training a Genre Classifier Vedrana Vidulin  Mitja Lutrek  Matja Gams Joef Stefan Institute  Jamova 39  SI1000 Ljubljana  Slovenia vedrana vidulinijs si  mitjalustrekijs si  matjazgamsijs si Abstract This paper presents experiments on classifying web pages by genre  Firstly  a corpus of 1539 manually labeled web pages was prepared  Secondly  502 genre features were selected based on the literature and the observation of the corpus  Thirdly  these features were extracted from the corpus to obtain a data set  Finally  three machine learning algorithms  one for induction of decision trees  J48  and two ensemble algorithms  bagging and boosting   were trained and tested on the data set  Additionally  impact of feature selection on ensemble algorithms was tested  The best performed genre classifiers in terms of precision were selected to obtain the best of set of classifiers  On average the best of set achieved 9  better precision  but slightly worse recall  Accuracy and Fmeasure did not vary significantly  The results indicate that classification by genre could be a useful addition to search engines  1  Introduction A good question to start with is why we want to classify a web page by genre  For example  if we are interested in cats and search for the keyword cat  a search engine will return web pages that describe the life of cats  but it will also return web pages with cat picture gallery  newspaper articles about controlling the growth of cats population etc   see Figure 1   However  if we were able to specify that we want to search only for content delivery </raw_string>
  </article>
  <article>
    <title>A Novel Split and Merge Technique for Hypertext Classification</title>
    <count>731</count>
    <raw_string>A Novel Split and Merge Technique for Hypertext Classification Suman Saha  CA Murthy  and Sankar K Pal Center for Soft Computing Research  Indian Statistical Institute ssahar murthy sankar isical acin Abstract  As web grows at an increasing speed  hypertext classifica  tion is becoming a necessity  While the literature on text categorization is quite mature  the issue of utilizing hypertext structure and hyperlinks has been relatively unexplored  In this paper  we introduce a novel split and merge technique for classification of hypertext documents  The split ting process is performed at the feature level by representing the hyper  text features in a tensor space model  We exploit the localstructure and neighborhood recommendation encapsulated in the this representation model  The merging process is performed on multiple classifications ob  tained from split representation  A meta level decision system is formed by obtaining predictions of base level classifiers trained on different com ponents of the tensor and actual category of the hypertext document  These individual predictions for each component of the tensor are sub  sequently combined to a final prediction using rough set based ensemble classifiers  Experimental results of classification obtained by using our method is marginally better than other existing hypertext classification techniques  Keywords  Hypertext classification  tensor space model  rough ensem  ble classifier  1 Introduction As the web is expanding  where most web pages are connected with hyperlinks  the role of automatic categorization of hypertext is becoming more and more important  The challange of retrieval engine is  it need to search and retrieve toolarge number of web pages  By categorizing documents a priori  the search space can be reduced dramatically and the quality </raw_string>
  </article>
  <article>
    <title>A State of the Art on Computational Music Performance</title>
    <count>731</count>
    <raw_string>1 2 A state of the art on computational music performance 3 Miguel Delgado  Waldo Fajardo  Miguel MolinaSolana  4 Department of Computer Science and Artificial Intelligence  Universidad de Granada  Daniel Saucedo Aranda sn  18071 Granada  Spain 5 7 a r t i c l e i n f o 89 Keywords  10 Computational music 11 Expressive performance 12 Machine learning 13 1 4 a b s t r a c t 15Musical expressivity can be defined as the deviation from a musical standard when a score is performed 16by a musician  This deviation is made in terms of intrinsic note attributes like pitch  timbre  timing and 17dynamics  The advances in computational power capabilities and digital sound synthesis have allowed 18realtime control of synthesized sounds  Expressive control becomes then an area of great interest in 19the sound and music computing field  Musical expressivity can be approached from different perspec 20tives  One approach is the musicological analysis of music and the study of the different stylistic schools  21This approach provides a valuable understanding about musical expressivity  Another perspective is the 22computational modelling of music performance by means of automatic analysis of recordings  It is known 23that music performance is a complex activity that involves complementary aspects from other disciplines 24such as psychology and acoustics  It requires creativity and eventually  some manual abilities  being a 25hard task even for humans  Therefore  using machines appears as a very interesting and fascinating issue  26In this paper  we present an overall view of the works many researchers have done so far in the field of 27expressive music performance  with special attention to the computational approach  28 2010 Published by </raw_string>
  </article>
  <article>
    <title>A Review of Machine Learning Approaches to Spam Filtering</title>
    <count>730</count>
    <raw_string>Expert Systems with Applications 36  2009  1020610222Contents lists available at ScienceDirect Expert Systems with Applications journal homepage  wwwelsevier comlocate eswaReview A review of machine learning approaches to Spam filtering Thiago S Guzella   Walmir M Caminhas Department of Electrical Engineering  Federal University of Minas Gerais  Ave  Antonio Carlos  6627  Belo Horizonte  MG 31270910  Brazila r t i c l e i n f o Keywords  Spam filtering Online learning Bagofwords  BoW  Naive Bayes Image Spam09574174  see front matter 2009 Elsevier Ltd A doi101016jeswa200902037  Corresponding author  Present address  Instituto G Quinta Grande 6  Oeiras 2780156  Portugal  Tel   35 Email addresses  tguzellaigcgulbenkianpt cpdee ufmgbr  WM Caminhas  a b s t r a c t In this paper  we present a comprehensive review of recent developments in the application of machine learning algorithms to Spam filtering  focusing on both textual and imagebased approaches  Instead of considering Spam filtering as a standard classification problem  we highlight the importance of consider ing specific characteristics of the problem  especially concept drift  in designing new filters  Two partic  ularly important aspects not widely recognized in the literature are discussed  the difficulties in updating a classifier based on the bagofwords representation and a major difference between two early naive Bayes models  Overall  we conclude that while important advancements have been made in the last years  several aspects remain to be explored  especially under more realistic evaluation settings  2009 Elsevier Ltd All rights reserved 1  Introduction In recent years  the increasing use of email has led to the emer gence and further escalation of problems caused by </raw_string>
  </article>
  <article>
    <title>Automatic Discrimination between Printed and Handwritten Text in Documents</title>
    <count>730</count>
    <raw_string>Automatic discrimination between printed and handwritten text in documents Lincoln Faria da Silva  Aura Conci Instituto de Computacao Universidade Federal Fluminense  UFF Niteroi  Brasil lsilva  aconciicuffbr Angel Sanchez Departamento de Ciencias de la Computacion Universidad Rey Juan Carlos  Madrid  Spain angel sanchezurjces AbstractRecognition techniques for printed and handwritten text in scanned documents are significantly different  In this paper we address the problem of identifying each type  We can list at least four steps  digitalization  preprocessing  feature extraction and decision or classification  A new aspect of our approach is the use of data mining techniques on the decision step  A new set of features extracted of each word is proposed as well  Classification rules are mining and used to discern printed text from handwritten  The proposed system was tested in two public image databases  All possible measures of efficiency were computed achieving on every occasion quantities above 80   KeywordsData Mining  document analysis  text identification  optical characters recognition  Machine Vision I INTRODUCTION Great number of applications use documents presenting printed text and handwriting  Old documents  petitions  re  quests  applications for college admission  letters  require ments  memorandums  envelopes and bank checks are some examples  A considerable obstacle to optical character recog nition  OCR  systems is the mixture of printed and handwritten text in the same image  Each text type should be processed using different methods in order to optimize the recognition accuracy  Previous works addressed the problem of identifying each type by various classification techniques  These works utilize neural networks 17  employ linear polynomial for discrim  ination function 8  Fisher 912 and tree </raw_string>
  </article>
  <article>
    <title>Towards Text Copyright Detection Using Metadata in Web Applications</title>
    <count>730</count>
    <raw_string>Towards text copyright detection using metadata in web applications Marios Poulos Information Technology Laboratory  Department of Archives and Library Sciences  Ionian University  Corfu  Greece Nikolaos Korfiatis Institute of Informatics and Mathematics  Goethe University Frankfurt  Frankfurt  Germany  and George Bokos Information Technology Laboratory  Department of Archives and Library Sciences  Ionian University  Corfu  Greece Abstract Purpose  This paper aims to present the semantic content identifier SCI  a permanent identifier  computed through a lineartime onionpeeling algorithm that enables the extraction of semantic features from a text  and the integration of this information within the permanent identifier  Designmethodologyapproach  The authors employ SCI to propose a mechanism for simultaneously checking the authenticity and degrees of similarity between different information objects  and present an empirical investigation of the method  A management scenario for the control of the authentication process and the detection of the degree of violation of documents is proposed  Findings  Such a mechanism could be adopted as a component of libraries  strategy for the protection of the copyrights for documents published on the web Practical implications  The use of the proposed numeric code can be utilised efficiently as a constituent part of the digital object identifier  DOI  system  making its computation more efficient and meaningful  Originalityvalue  The identifier proposed in the paper can result in a more efficient index for identifying and retrieving objects in a digital library  as well as online repositories and commercial applications that can handle information retrieval requests more effectively  Keywords Text identification  Information retrieval  Semantics  Persistent identifiers  Data handling  Copyright  Research work Paper type Research paper 1  Introduction Information is the </raw_string>
  </article>
  <article>
    <title>Automated Genre Classification in the Management of Digital Documents</title>
    <count>730</count>
    <raw_string>2008 Annual Conference of CIDOC Athens  September 15  18  2008 Yunhyong Kim and Seamus Ross 1 AUTOMATED GENRE CLASSIFICATION IN THE MANAGEMENT OF DIGITAL DOCUMENTS Yunhyong Kim and Seamus Ross Digital Curation Centre DCC   Humanities Advanced Technology Information Institute  HATII  University of Glasgow 11 University Gardens Glasgow UK email  ykim  s ross hatii arts glaacuk URL  http www hatii arts glaacuk Abstract This paper examines automated genre classification of text documents and its role in enabling the effective management of digital documents by digital libraries and other repositories  Genre classification  which narrows down the possible structure of a document  is a valuable step in realising the general automatic extraction of semantic metadata essential to the efficient management and use of digital objects  The characterisation of digital objects in terms of genre also associates the object to the objectives that led to its creation  which indicates its relevance to new objectives in information search  In the present report  we present an analysis of word frequencies in different genre classes in an effort to understand the distinction between independent classification tasks  In particular  we examine automated experiments on thirtyone genre classes to determine the relationship between the word frequency metrics and the degree of its significance in carrying out classification in varying environments  INTRODUCTION The volume of digital resources inundating our everyday lives is growing at an enormously rapid pace  This information is emerging from unpredictable sources  in different formats and channels  sometimes involving little regulation and control  The storage  management  dissemination and use of this information has consequently become increasingly complex during recent years  Metadata  embodying the technical requirements  administrative function  </raw_string>
  </article>
  <article>
    <title>Simple and Efficient Classification Scheme Based on Specific Vocabulary</title>
    <count>729</count>
    <raw_string>Comput Manag Sci  2012  9401415 DOI 101007s102870120149z ORIGINAL PAPER Simple and efficient classification scheme based on specific vocabulary Jacques Savoy  Olena Zubaryeva Received  27 June 2011  Accepted  18 June 2012  Published online  5 July 2012  SpringerVerlag 2012 Abstract Assuming a binomial distribution for word occurrence  we propose computing a standardized Z score to define the specific vocabulary of a subset com pared to that of the entire corpus  This approach is applied to weight terms  character ngram  word  stem  lemma or sequence of them  which characterize a document  We then show how these Z score values can be used to derive a simple and efficient categorization scheme  To evaluate this proposition and demonstrate its effectiveness  we develop two experiments  First  the system must categorize speeches given by B Obama as being either electoral or presidential speech  In a second experiment  sentences are extracted from these speeches and then categorized under the head  ings electoral or presidential  Based on these evaluations  the proposed classification scheme tends to perform better than a support vector machine model for both exper iments  on the one hand  and on the other  shows a better performance level than a Nave Bayes classifier on the first test and a slightly lower performance on the second  10fold cross validation   Keywords Statistics in lexical analysis  Corpus linguistics  Text categorization  Machine learning  Natural language processing  NLP  1 Introduction During the last decade  various text categorization models and applications have been proposed  Weiss et al  2010   As a first example related to this study  we find the </raw_string>
  </article>
  <article>
    <title>A New Method of Authors Identification for Copyright Protection</title>
    <count>729</count>
    <raw_string>A New Method of Authors Identification for Copyright Protection Qiaoyan Kuang1  Xiaoming Xu2 1 College of Computer  Hunan International Economics University  410205Changsha  China 2 College of Continuing Education  Hunan International Economics University  410205Changsha  China  qiaoyankuang126com  ceoxoxo163com  Abstract To solve the copyright dispute  we provide a new method of authors identification for copyright protection in this paper With the feature extraction and text classification  we classify the original text  then enter the features extracted from the disputed text in the classifier to get the author and the level of the disputed text  The experiment shows the proposed algorithm could classify a disputed text and identify the author  So we believe this method could help to solve the copyright dispute of a disputed text  especially the famous authors  Keywords  Feature extraction  Data mining  Authors identification  Copyright protection 1 Introduction As the rapid development of computer and network technology  more and more information springs up to our world carried by text  images  video and cartoon  And we can find that text is one of the carriers been used the most widely  It carries almost the most of the information on the internet  Different author has different style on writing  The different text written by the same author is the same on these features  writing style  sentence structure  vocabulary etc  The steps of this method we used are these three points follow  First  extract and add up the features of large amounts of text written by different authors and enter them into the classifier  We called it train the classifier  Then  by using the effectively feature extraction </raw_string>
  </article>
  <article>
    <title>A Performance Evaluation Methodology for Historical Document Image Binarization</title>
    <count>729</count>
    <raw_string>Copyright  c  2011 IEEE Personal use is permitted  For any other purposes  permission must be obtained from the IEEE by emailing pubspermissionsieee org  This article has been accepted for publication in a future issue of this journal  but has not been fully edited  Content may change prior to final publication  1 A Performance Evaluation Methodology for Historical Document Image Binarization Konstantinos Ntirogiannis  Basilis Gatos and Ioannis Pratikakis  Member  IEEE AbstractDocument image binarization is of great importance in the document image analysis and recognition pipeline since it affects further stages of the recognition process  The evaluation of a binarization method aids in studying its algorithmic behaviour and verifying its effectiveness by providing qualitative and quantitative indication of its performance  This work concerns a pixelbased binarization evaluation methodology for historical handwrittenmachineprinted document images  In the proposed evaluation scheme  the Recall and Precision evaluation measures are properly modified using a weighting scheme that diminishes any potential evaluation bias  Additional performance metrics of the proposed evaluation scheme consist of the percentage rates of broken and missed text  false alarms  background noise  character enlargement and merging  Several experiments con ducted in comparison with other pixelbased evaluation measures  demonstrate the validity of the proposed evaluation scheme  Index Termsdocument image binarization  performance eval uation  ground truth  I INTRODUCTION H ISTORICAL documents suffer from various degradations due to ageing  extended use  several attempts of acquisition and environmental conditions 14  The main artefacts encountered in historical documents are shadows  nonuniform illumination  smear  strain  bleedthrough and faint characters  Fig  1   Those artefacts are problematic for document image analysis methods which assume smooth background and </raw_string>
  </article>
  <article>
    <title>Online Conversation Mining for Author Characterization and Topic Identification</title>
    <count>729</count>
    <raw_string>Online Conversation Mining for Author Characterization and Topic Identification Giacomo Inches University of Lugano Lugano  Switzerland giacomoinchesusich Fabio Crestani University of Lugano Lugano  Switzerland fabiocrestaniusich ABSTRACT The increasing popularity of onlinebased services  Twit ter  Facebook  IRC  Myspace  blogs  just to mention few of them  results in a production of a huge amount of novel documents  These documents present properties that can not be found in standard edited documents  In particular the messages generated during the use of Instant Messages Services  IM  like chatrooms or Twitter are short  user generated and noisy  We are investigating two different but related aspect of the content of these colloquial mes  sages  the topic identification and the author identification tasks  In the first case we would like to answer the question what is the conversation about  while in the second case the question is who are the people involved in the conversation   The combination of these two tasks into a unique model is a novel and interesting research problem that is the main topic of our research  Categories and Subject Descriptors H31  Information Systems   Information Storage And RetrievalContent Analysis and Indexing  I7 Computing Methodologies  Document and Text Processing General Terms Experimentation  Measurement 1  MOTIVATION AND OBJECTIVES Communication is a primary need of human being and the advent of the Internet amplified the possibilities of communi cation of individuals and the masses 13  As a result  many people every day use chat and instant messaging programs to get in touch with friends or family  or rely on online ser vices such as blog or social networks to share their emotions and thoughts with the </raw_string>
  </article>
  <article>
    <title>A Replicated Comparative Study of Source Code Authorship Attribution</title>
    <count>729</count>
    <raw_string>A Replicated Comparative Study of Source Code Authorship Attribution Matthew F Tennyson Department of Computer Science  Information Systems Bradley University Peoria  IL  USA mtennysonbradley edu Abstract Source code authorship attribution is  simply  the task of deciding who wrote a piece of software given its source code  Applications include software forensics  plagiarism detection  and determining software ownership  Several methods of source code authorship attribution have been proposed in the past  Based on the only known controlled  comprehensive comparative study of these methods  the two most effective methods are the Burrows method and the SCAP method  This paper presents a partial replication of that comparative study  Specifically  it only compares the two most effective methods  Burrows and SCAP   This paper also includes a slight extension of that study  the original comparative study only considered anonymized data  while the replicated study considers both anonymized and nonanonymized data  The original comparative study indicated that the Burrows method outperformed all other methods  including the SCAP method  by a considerable margin  However  the results of the replicated study indicate that the SCAP method outperforms the Burrows method by a small margin when using anonymized data and by a large margin when using nonanonymized data  Keywordsauthorship attribution  information retrieval  software forensics  plagiarism detection I INTRODUCTION The term authorship attribution  refers simply to  the task of deciding who wrote a document  1  Authorship attribution of source code  then  is the task of deciding who wrote a source code document  Source code authorship attribution is a tenet of software forensics  which is the process of analyzing software to identify characteristics of its </raw_string>
  </article>
  <article>
    <title>A Comparison of String Similarity Measures for Toponym Matching</title>
    <count>729</count>
    <raw_string>A Comparison of String Similarity Measures for Toponym Matching Gabriel Recchia Institute for Intelligent Systems University of Memphis 365 Innovation Drive Memphis  TN 38152 19705324287 grecchiamemphis edu Max Louwerse Institute for Intelligent Systems University of Memphis 365 Innovation Drive Memphis  TN 38152 19016783803 mlouwersememphis edu ABSTRACT The diversity of ways in which toponyms are specified often results in mismatches between queries and the place names contained in gazetteers  Search terms that include unofficial variants of official place names  unanticipated transliterations  and typos are frequently similar but not identical to the place names contained in the gazetteer  String similarity measures can mitigate this problem  but given their taskdependent performance  the optimal choice of measure is unclear  We constructed a task in which place names had to be matched to variants of those names listed in the GEOnet Names Server  comparing 21 different measures on datasets containing romanized toponyms from 11 different countries  Bestperforming measures varied widely across datasets  but were highly consistent withincountry and withinlanguage  We discuss which measures worked best for particular languages and provide recommendations for selecting appropriate string similarity measures  Categories and Subject Descriptors H28 Database Management  Database Applications  spatial databases and GIS H31  Information Storage and Retrieval  Content Analysis and Indexing  linguistic processing General Terms Algorithms  Performance  Experimentation Keywords toponyms  string similarity  edit distance  gazetteers  duplicate detection  data integration  geographic information retrieval 1  INTRODUCTION At least two topics relevant to computational models of place  point of interest conflation and placebased data integrationare closely tied to the expansion  search  and conflation of digital gazetteers  Some approaches to these tasks  particularly conflation  involve toponym matching </raw_string>
  </article>
  <article>
    <title>Evaluating Authorship Distance Methods Using the Positive Silhouette Coefficient</title>
    <count>729</count>
    <raw_string>Natural Language Engineering http journalscambridge orgNLE Additional services for Natural Language Engineering  Email alerts  Click here Subscriptions  Click here Commercial reprints  Click here Terms of use  Click here Evaluating authorship distance methods using the positive Silhouette coefcient ROBERT LAYTON  PAUL WATTERS and RICHARD DAZELEY Natural Language Engineering  Volume 19  Issue 04  October 2013  pp 517  535 DOI  101017S1351324912000241  Published online  28 September 2012 Link to this article  http journalscambridge orgabstractS1351324912000241 How to cite this article  ROBERT LAYTON  PAUL WATTERS and RICHARD DAZELEY  2013   Evaluating authorship distance methods using the positive Silhouette coefcient  Natural Language Engineering  19  pp 517535 doi101017S1351324912000241 Request Permissions  Click here Downloaded from http journalscambridge orgNLE  IP address  19525116689 on 04 Oct 2013 Natural Language Engineering 19  4   517535  c Cambridge University Press 2012 doi101017S1351324912000241 517 Evaluating authorship distance methods using the positive Silhouette coefficient R O B E R T L A Y T O N1  P A U L W A T T E R S1 and R I C H A R D D A Z E L E Y2 1Internet Commerce Security Laboratory University of Ballarat  Ballarat VIC  Australia email  rlaytonicslcom au 2Data Mining and Informatics Research Group University of Ballarat  Ballarat VIC  Australia emails  pwatters rdazeley ballarateduau  Received 7 November 2011  revised 20 August 2012  accepted 23 August 2012  first published online 28 September 2012  Abstract Unsupervised Authorship Analysis  UAA  aims to cluster documents by authorship without knowing the authorship of any documents  An important factor in UAA is the method for calculating the distance between documents  </raw_string>
  </article>
  <article>
    <title>A Statistical Pattern Recognition Approach to Symbolic Music Classification</title>
    <count>728</count>
    <raw_string>PhD Thesis A statistical pattern recognition approach to symbolic music classification Pedro J Ponce de Leon Amador Supervisor  Jose  M Inesta Quereda September 2011 Para Alvaro y Merche  por su paciencia y carino  Agradecimientos Para llegar hasta esta pagina he recorrido un largusimo camino  que empezo probablemente cuando mis padres me regalaron mi primer instrumento musical  y mi hermano mi primera calculadora  Ellos no se imaginaban como iba a terminar la cosa  claro  Es lo que tiene hacer regalos sin pensar en las consecuencias  Por ello  y por todo lo demas  les quiero expresar mi eterno agradecimiento y todo mi amor  La persona que me ha llevado de la mano en el  tambien largo  tramo final de este camino es el director de esta tesis  Jose  Manuel Inesta  Sus contribuciones al desarrollo de este trabajo van mucho mas alla de lo acostumbrado  Quiero agradecerle sus numerosas aportaciones y consejos tanto en lo academico y lo profesional como en lo personal  as como las facilidades que siempre me ha brindado para desarrollar mi trabajo de la mejor forma posible  Los consejos y el apoyo de los miembros del Grupo de Reconocimiento de Formas e Inteligencia Artificial del Departamento de Lenguajes y Sistemas Informaticos de la Universidad de Alicante han sido tambien de vital importancia en muchos aspectos de esta investigacion  Vaya para todos ellos mi agradecimiento  y en especial a los miembros del Laboratorio de Informatica Musical  liderado por Jose Manuel Inesta  David Rizo  cuya capacidad de trabajo no deja de asombrarme da tras da  Antonio Pertusa  Carlos PerezSancho  Tomas Perez  Jose  F Bernabeu  Placido R Illescas  Mara </raw_string>
  </article>
  <article>
    <title>Reutilizacion de Codigo Fuente entre Lenguajes de Programacion</title>
    <count>709</count>
    <raw_string>REUTILIZACION DE CODIGO FUENTE ENTRE LENGUAJES DE PROGRAMACION Enrique Flores Saez DEPARTAMENTO DE SISTEMAS INFORMATICOS Y COMPUTACION Dirigido por  Paolo Rosso y Lidia Moreno Trabajo Final de Master desarrollado dentro del Master en Inteligencia Artificial  Reconocimiento de Formas e Imagen Digital Valencia  Febrero 2012 La mentira mas comun es aquella con la que un hombre se engana a s mismo  Enganar a los demas es un defecto relativamente vano  Friedrich Nietzsche Resumen El uso de material ajeno sin reconocimiento al autor se considera plagio  Cuando se cita la fuente del material usado o simplemente este proviene de una fuente de distribucion libre se considera que se esta realizando un proceso de reutilizacion del mismo  En la ac  tualidad  dentro de la era digital  do nde casi cualquier contenido esta disponible en Internet existe una gran tentacion por reutilizar  El material reutilizado puede ser una pista musical  una imagen  un texto e incluso un codigo fuente  Esta facilidad de acceso a los recur sos impone la creacion de herramientas de deteccion de reutilizacion para preservar la propiedad intelectual de los mismo  En el mundo de la industria de desarrollo de software existe gran interes por mantener la autora de codigos fuente  y preservar el uso legtimo que se establece a traves de licencias o contratos de uso  En el ambito academico  tambien es interesante la deteccion de reuti lizacion con el fin de disuadir al alumnado de la creciente practica de copiar y pegar  o presentar el trabajo de un companero como propio  Reutilizando material escolar  no se cumple con la obligacion de demostrar sus conocimientos frente a la figura del profesor  En este trabajo presentamos una descripcion del </raw_string>
  </article>
  <article>
    <title>Genre distinctions for Discourse in the Penn TreeBank</title>
    <count>695</count>
    <raw_string>Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP  pages 674682  Suntec  Singapore  27 August 2009  c2009 ACL and AFNLP Genre distinctions for Discourse in the Penn TreeBank Bonnie Webber School of Informatics University of Edinburgh Edinburgh EH8 9LW  UK bonnie webbered acuk Abstract Articles in the Penn TreeBank were iden tified as being reviews  summaries  let ters to the editor  news reportage  correc tions  wit and short verse  or quarterly profit reports  All but the latter three were then characterised in terms of fea  tures manually annotated in the Penn Dis  course TreeBank  discourse connectives and their senses  Summaries turned out to display very different discourse features than the other three genres  Letters also appeared to have some different features  The two main findings involve  1  differ ences between genres in the senses asso ciated with intrasentential discourse con nectives  intersentential discourse con nectives and intersentential discourse re  lations that are not lexically marked  and  2  differences within all four genres be tween the senses of discourse relations not lexically marked and those that are marked  The first finding means that genre should be made a factor in automated sense labelling of nonlexically marked discourse relations  The second means that lexically marked relations provide a poor model for automated sense labelling of relations that are not lexically marked  1 Introduction It is wellknown that texts differ from each other in a variety of ways  including their topic  the read  ing level of their intended audience  and their in tended purpose eg  to instruct  to inform  to </raw_string>
  </article>
  <article>
    <title>An Overview of the Traditional Authorship Attribution Subtask</title>
    <count>695</count>
    <raw_string>An Overview of the Traditional Authorship Attribution Subtask Notebook for PAN at CLEF 2012 Patrick Juola Evaluating Variation in Language Laboratory Juola  Associates Duquesne University 276 W Schwab Ave  Pittsburgh  PA 15282 Munhall  PA 15210 juolamathcs duqedu pjuolajuolaassoccom Abstract This paper describes the Traditional Authorship Attribution subtask of the PANCLEF 2012 workshop  As a followup to our subtask at PANCLEF 2011  Amsterdam   we established a new corpus for analysis for 2012  Rome   The new corpus differed in several ways from the previous subtask   Both the number and size of documents were decreased  The documents were taken from a different genre  fiction  represented by the Feedbooks com site   The documents were no longer marked up extensively  A new subsubtask was added  Authorship clustering  In this new problem  related to intrinsic plagiarism  participants were given a text of mixed authorship and asked to determine which paragraphs came from which au  thors  The resulting corpus consisted of eight problems  including three closedclass authorship attribution problems  three openclass  the set of correct answers in cluded none of the above   and two clustering problems  Twentyfive teams participated in this subtask from many different parts of the world  Detailed re  sults are available on the Web at panwebisde and will be discussed in detail at the PANCLEF 2012 meeting in September  1 Background Although traditionally authorship studies are done on the basis of close reading for stylistic detail  nontraditional or statistical authorship attribution has been around long enough 61457 to have developed into a traditional research problem of its own  especially in comparison to new tasks such as sexual predator </raw_string>
  </article>
  <article>
    <title>Evaluation of the Audio Beat Tracking System BeatRoot</title>
    <count>695</count>
    <raw_string>This article was downloaded by HEAL  Link Consortium  On  21 November 2007 Access Details  subscription number 772810582  Publisher  Routledge Informa Ltd Registered in England and Wales Registered Number  1072954 Registered office  Mortimer House  3741 Mortimer Street  London W1T 3JH  UK Journal of New Music Research Publication details  including instructions for authors and subscription information  http wwwinformaworldcomsmpptitlecontentt713817838 Evaluation of the Audio Beat Tracking System BeatRoot Simon Dixon a a Queen Mary  University of London  UK Online Publication Date  01 March 2007 To cite this Article  Dixon  Simon  2007  Evaluation of the Audio Beat Tracking System BeatRoot  Journal of New Music Research  361  39  50 To link to this article  DOI  10108009298210701653310 URL  http dx doi org10108009298210701653310 PLEASE SCROLL DOWN FOR ARTICLE Full terms and conditions of use  http wwwinformaworldcomtermsandconditionsofaccesspdf This article maybe used for research  teaching and private study purposes  Any substantial or systematic reproduction  redistribution  reselling  loan or sublicensing  systematic supply or distribution in any form to anyone is expressly forbidden  The publisher does not give any warranty express or implied or make any representation that the contents will be complete or accurate or up to date  The accuracy of any instructions  formulae and drug doses should be independently verified with primary sources  The publisher shall not be liable for any loss  actions  claims  proceedings  demand or costs or damages whatsoever or howsoever caused arising directly or indirectly in connection with or arising out of the use of this material  D ow nl oa de d B y   H E A L L in k </raw_string>
  </article>
  <article>
    <title>Toward Summarization of Communicative Activities in Spoken Conversation</title>
    <count>695</count>
    <raw_string>Toward Summarization of Communicative Activities in Spoken Conversation John Niekrasz T H E U N I V E R S I T Y O F E D I N B U R G H Doctor of Philosophy University of Edinburgh 2012 1 Abstract This thesis is an inquiry into the nature and structure of facetoface conversation  with a special focus on group meetings in the workplace  I argue that conversations are composed of episodes  each of which corresponds to an identifiable communicative activity such as giving instructions or telling a story  These activities are important because they are part of participants  commonsense understanding of what happens in a conversation  They appear in natural summaries of conversations such as meeting minutes  and participants talk about them within the conversation itself  Episodic communicative activities therefore represent an essential component of practical  commonsense descriptions of conversations  The thesis objective is to provide a deeper understanding of how such activities may be recognized and differentiated from one another  and to develop a computational method for doing so automatically  The experiments are thus intended as initial steps toward future applications that will require analysis of such activities  such as an automatic minutetaker for workplace meetings  a browser for broadcast news archives  or an automatic decision mapper for planning interactions  My main theoretical contribution is to propose a novel analytical framework called par ticipant relational analysis  The proposal argues that communicative activities are princi pally indicated through participantrelational features  i e   expressions of relationships be tween participants and the dialogue  Participantrelational features  such as subjective lan guage  verbal reference to the participants  and the distribution of speech activity amongst the participants  </raw_string>
  </article>
  <article>
    <title>Authorship and Plagiarism Detection Using Binary BOW Features</title>
    <count>695</count>
    <raw_string>Authorship and Plagiarism Detection Using Binary BOW Features Notebook for PAN at CLEF 2012 Navot Akiva The Computer Science Department Bar Ilan University  Israel Affiliation navot akivagmailcom Abstract  Identifying writing style shifts and variations are fundamental capabilities when addressing authorship related tasks  In this work we examine a simplified approach for unsupervised authorship and plagiarism detection which is based on binary bag of words representation  We evaluate our approach using PAN2012 Authorship Attribution challenge data  which includes both openclosed class authorship identification and intrinsic plagiarism tasks  Our approach proved to be successful achieving overall average accuracy of 84  over and a 2nd place rank in the competition  Keywords  Intrinsic plagiarism  authorship attribution  outlier detection  1 Introduction Authorship and stylistic variation identification over documents has a broad range of applications from identifying specific author s writing style  author verification  plagiarism detection etc  Vast research efforts invested in approaching these areas has been conducted over recent years  by applying various feature representation and algorithmic approaches 1 2  In this work we evaluate the extent to which a straightforward feature representation method could be successfully utilized for authorship and plagiarism identification  We consider the problems of authorship identification either openclosed class and intrinsic plagiarism detection  both included in PAN2012 Authorship Identification competition  For each problem type  we first represent each document as a binary vector that encodes the presence or absence of common words in the text  2 Our Approach For all the tasks in this competition we use a single vector representation that captures the presenceabsence of common words in a text  We have previously demonstrated the power of this representation elsewhere 3 4   In both tasks </raw_string>
  </article>
  <article>
    <title>Authorship Attribution Based on a Probabilistic Topic Model</title>
    <count>691</count>
    <raw_string>Information Processing and Management 49  2013  341354Contents lists available at SciVerse ScienceDirect Information Processing and Management journal homepage  wwwelsevier com locate infopromanAuthorship attribution based on a probabilistic topic model Jacques Savoy Computer Science Department  University of Neuchatel  Rue Emile Argand 11  2000 Neuchtel  Switzerlanda r t i c l e i n f o Article history  Received 3 February 2012 Received in revised form 4 May 2012 Accepted 25 June 2012 Available online 31 July 2012 Keywords  Authorship attribution Text categorization Machine learning Lexical statistics03064573  see front matter 2012 Elsevier Ltd http dx doi org101016jipm201206003 Email address  Jacques Savoyunine cha b s t r a c t This paper describes  evaluates and compares the use of Latent Dirichlet allocation  LDA  as an approach to authorship attribution  Based on this generative probabilistic topic model  we can model each document as a mixture of topic distributions with each topic specifying a distribution over words  Based on author profiles  aggregation of all texts written by the same writer  we suggest computing the distance with a disputed text to determine its pos sible writer  This distance is based on the difference between the two topic distributions  To evaluate different attribution schemes  we carried out an experiment based on 5408 newspaper articles  Glasgow Herald  written by 20 distinct authors  To complement this experiment  we used 4326 articles extracted from the Italian newspaper La Stampa and written by 20 journalists  This research demonstrates that the LDAbased classification scheme tends to outperform the Delta rule  and the v2 distance  two classical approaches in authorship attribution based on a restricted number of terms  Compared to the Kull  </raw_string>
  </article>
  <article>
    <title>Automatic Detection of Authorship Changes within Single Documents</title>
    <count>690</count>
    <raw_string>Automatic Detection of Authorship Changes within Single Documents by Neil Graham A thesis submitted in conformity with the requirements for the degree of Master of Science Graduate Department of Computer Science University of Toronto Copyright c by Neil Graham Abstract Automatic Detection of Authorship Changes within Single Documents Neil Graham Master of Science Graduate Department of Computer Science University of Toronto One of the most dicult tasks facing anyone who must compile or maintain any large collaborativelywritten document is to foster a consistent style throughout In this thesis we explore whether it is possible to identify stylistic inconsistencies within documents even in principle given our understanding of how style can be captured statistically We carry out this investigation by computing stylistic statistics on very small samples of text comprising a set of synthetic collaborativelywritten documents and using these statistics to train and test a series of neural networks We are able to show that this method does allow us to recover the boundaries of authors contributions We nd that timedelay neural networks hitherto ignored in this eld are especially eective in this regard Along the way we observe that statistics characterizing the syntactic style of a passage appear to hold much more information for small text samples than those concerned with lexical choice or complexity ii Acknowledgements Although it almost seems a cliche it is unquestionably true that this document would not have been possible without the help of numerous people Since I came to this eld from the rather distant realm of mathematics my task of understanding the background material was not inconsiderable The fact that blindness makes independent review of printed technical material quite dicult would have caused this task to be even more arduous had it not been for the eorts of so many researchers in this </raw_string>
  </article>
  <article>
    <title>Authorship Identification Using Stylometry Analysis in Bengali Literature</title>
    <count>690</count>
    <raw_string>ar X iv 1 20 8 62 68 v2  cs C L  3 1 A ug 2 01 2 Authorship Identification Using Stylometry Analysis in Bengali Literature Tanmoy Chakraborty AbstractStylometry is the study of the unique linguistic styles and writing behaviors of individuals  It belongs to the core task of text categorization like authorship identification  plagiarism detection etc  Though reasonable amount of works have been studied in English for a long time  no major work has been done so far in Bengali  In this work  we present a strategy for authorship identification of the documents written in Bengali  It takes into account a writer  independent model and builds a robust system which reduces the patternrecognition problem  We adopt a set of finegrained stylistic features for the analysis of the text and use them to develop two different models  statistical similarity model consisting of three measures and their combination  and machine learning model with Decision Tree  Neural Network and SVM Experimental results show that SVM outperforms others with average 833  of accuracy after 10fold cross validations using same set of features  We also validate the relative importance of each stylistic feature to show some of them remain consistently significant in every model used in this experiment  KeywordsStylometry  Authorship Identification  Vocabulary Richness  Machine Learning  I INTRODUCTION RECENTLY  the rapid growth of text in electronic formin blogs  social media  forums etc creates anonymously or under unverified names  In the framework of forensic appli cations  it is needed to group texts written by the same author or track texts written under different names but belonging to the same person  Authorship identification supported by computational analysis of texts </raw_string>
  </article>
  <article>
    <title>Unsupervised Decomposition of a Document into Authorial Components</title>
    <count>661</count>
    <raw_string>Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics  pages 13561364  Portland  Oregon  June 1924  2011  c2011 Association for Computational Linguistics Unsupervised Decomposition of a Document into Authorial Components Moshe Koppel Navot Akiva Idan Dershowitz Nachum Dershowitz Dept  of Computer Science Dept  of Bible School of Computer Science BarIlan University Hebrew University Tel Aviv University Ramat Gan  Israel Jerusalem  Israel Ramat Aviv  Israel moishk navot akivagmailcom dershowitzgmailcom nachumdtauacil Abstract We propose a novel unsupervised method for separating out distinct authorial compo nents of a document  In particular  we show that  given a book artificially munged  from two thematically similar biblical books  we can separate out the two consti tuent books almost perfectly  This allows us to automatically recapitulate many con clusions reached by Bible scholars over centuries of research  One of the key ele  ments of our method is exploitation of dif  ferences in synonym choice by different authors  1 Introduction We propose a novel unsupervised method for separating out distinct authorial components of a document  There are many instances in which one is faced with a multiauthor document and wishes to deli neate the contributions of each author  Perhaps the most salient example is that of documents of his torical significance that appear to be composites of multiple earlier texts  The challenge for literary scholars is to tease apart the documents various components  More contemporary examples include analysis of collaborative online works in which one might wish to identify the contribution of a particular author for commercial or forensic pur poses  We treat two versions of the problem  In the first  easier  version  the document to </raw_string>
  </article>
  <article>
    <title>Analyzing Large Collections of Electronic Text Using OLAP</title>
    <count>659</count>
    <raw_string>APICS 29th Annual Conference Acadia University  Wolfville  NS  October 2123  2005 Computer Science Proceedings Analyzing Large Collections of Electronic Text Using OLAP Steven Keith1 Owen Kaser1 Daniel Lemire2 1 University of New Brunswick  Saint John  Canada 2 University of Quebec at Montreal  Montreal  Canada Abstract Computerassisted reading and analysis of text has applications in the humanities and social sciences  Everlarger electronic text archives have the advantage of allowing a more complete analysis but the disadvantage of forcing longer waits for results  OnLine Analytical Processing  OLAP  allows quick analysis of multidimensional data  By storing textanalysis information in an OLAP system  queries may be solved in seconds instead of minutes or hours  This analysis is userdriven  allowing users the freedom to pursue their own directions of research  1 Introduction Electronic text collections have existed for over half a century  In this time these archives have increased in both size and accuracy  Many tools have been created for searching  classifying  and retrieving information from these collections  Ex  amples include Signature 13  Word Cruncher 1  Word Smith Tools 19  and Intext 12  Such tools tend not to be interactive  Also  analyzing a multigigabyte corpus tends to be slow  We propose the creation of userdriven tools to interface with a  Data  Ware  house of Words  WoW   see Fig  1   A WoW is built by an Extraction  Transfor mation  and Loading  ETL  procedure  which processes the text and aggregates data from different sources  A WoW stores its data in data cubes 9  A data cube can be abstracted as a </raw_string>
  </article>
  <article>
    <title>Intrinsic Plagiarism Detection Using Character Trigram Distance Scores</title>
    <count>659</count>
    <raw_string>Intrinsic Plagiarism Detection Using Character Trigram Distance Scores Notebook for PAN at CLEF 2011 Mike Kestemont  Kim Luyckx  and Walter Daelemans CLiPS Computational Linguistics Group University of Antwerp  Belgium mikekestemont  kim luyckx  walterdaelemans uaacbe Abstract In this paper  we describe a novel approach to intrinsic plagiarism de tection  Each suspicious document is divided into a series of consecutive  po tentially overlapping windows  of equal size  These are represented by vectors containing the relative frequencies of a predetermined set of highfrequency char acter trigrams  Subsequently  a distance matrix is set up in which each of the documents windows is compared to each other window  The distance measure used is a symmetric adaptation of the normalized distance  nd1  proposed by Stamatatos 17  Finally  an algorithm for outlier detection in multivariate data  based on Principal Components Analysis  is applied to the distance matrix in or der to detect plagiarized sections  In the PANPC2011 competition  this system  second place  achieved a competitive recall  4279  but only reached a plagdet of 1679 due to a disappointing precision  1075   Keywords  intrinsic plagiarism detection  character ngrams  distance scores  out lier detection  stylometry 1 Intrinsic plagiarism detection Plagiarism  generally refers to the illegitimate use of someone elses information  text  ideas  etc  without proper reference to the original source of these borrowings 9  A pla  giarizing author implicitly claims the original authorship of these borrowings  which in many legal systems is considered a violation of the original authors intellectual prop  erty rights  The automatic detection of plagiarized sections in natural language text documents has become a </raw_string>
  </article>
  <article>
    <title>Duplicate Detection for Identifying Social Spam in Microblogs</title>
    <count>656</count>
    <raw_string>Duplicate Detection for Identifying Social Spam in Microblogs Qunyan Zhang  Haixin Ma  Weining Qian  Aoying Zhou Center for Cloud Computing and Big Data  Software Engineering Institute  East China Normal University  Shanghai  China Email  5112150004351111500010ecnucn  wnqian ayzhousei ecnueducn AbstractAs an important kind of social media  microblog has become an important source of opinion mining and collective behavior study  However  social spams may affect the analytical results greatly  This paper focuses on the problem of identifying potential social spammers who copy pieces of in formation from others  An improved localitysensitive hashing based method is used for detecting duplicated tweets  Intensive empirical study over a reallife microblog dataset crawled from Sina Weibo  one of the most popular microblogging services  is conducted  The characteristics of potential spammers and their behaviors are analyzed  Keywordsduplicate detection  social spam  microblog  localitysensitive hash  MapReduce I INTRODUCTION Social media has become one of the major sources for people to consume information  It has been shown that social media can be used in sensing earthquake32  predicting financial index3  and political election37  and sentiment analysis29  The increasing importance of social media also attracts spammers to post pieces of irrelevant  false  or even harmful information  Since such kind of spammings usually take advantage of social network to spread the spams  they are often called social spams11  28  22  A major difference between social spam and traditional spams  such as email spam10 and link spam14  is that  in social media  spam  mers need to attract common users to follow spammers  so that spams are subscribed  A social spammer often pretends to </raw_string>
  </article>
  <article>
    <title>Author Attribution of Turkish Texts by Feature Mining</title>
    <count>655</count>
    <raw_string>DS Huang  L Heutte  and M Loog  Eds    ICIC 2007  LNCS 4681  pp  10861093  2007   SpringerVerlag Berlin Heidelberg 2007 Author Attribution of Turkish Texts by Feature Mining Filiz Trkolu  Banu Diri  and M Fatih Amasyal Yldz Technical University  Computer Engineering  34349 stanbul  Turkey filizturkoglubanu mfatih ce yildizedutr Abstract  The aim of this study is to identify the author of an unauthorized document  Ten different feature vectors are obtained from authorship attributes  ngrams and various combinations of these feature vectors that are extracted from documents  which the authors are intended to be identified  Comparative performance of every feature vector is analyzed by applying Nave Bayes  SVM  kNN  RF and MLP classification methods  The most successful classifiers are MLP and SVM In document classification process  it is observed that ngrams give higher accuracy rates than authorship attributes  Nevertheless  using ngram and authorship attributes together  gives better results than when each is used alone  Keywords  Author attribution  ngrams  Text classification  Feature extraction  Turkish documents  1 Introduction The goal of text categorization is the classification of documents into a fixed number of predefined categories  One of the problems in text categorization is the authorship attribution  which is used to determine the author of a text when it is not clear who wrote it  It can be used in occasions where two people claim to be the author of same manuscript or on the contrary where no one is willing to accept the authorship of a document  It is not difficult for anyone to take somebody else s work and to publish it under </raw_string>
  </article>
  <article>
    <title>Detection and Correction of Deformed Historical Arabic Manuscripts</title>
    <count>655</count>
    <raw_string>International Conference on Computer and Communication Engineering  ICCCE 2010   1113 May 2010  Kuala Lumpur  Malaysia 978142446235310 2600 2010 IEEE Detection and Correction of Deformed Historical Arabic Manuscripts Sherif Said Eletriby Faculty of Computers and Information Menoufia University Shebin El kom  32511  Egypt Sherifalicimenofia edueg Khalid Mohammad Amin Faculty of Computers and Information Menoufia University Shebin El kom  32511  Egypt khaled abdelmoneam cimenofia edueg Abstract Historical manuscripts are considered one of the most imperative human riches and a source of intellectual production  Unfortunately  due to aging effects  multiple noises and deviations are found in the document image  Moreover  challenges for several images of ancient documents show defects of inclinations and curvatures of text lines  These defects arise due to bad storage conditions  or during the digitization process  In order to improve the readability and the automatic recognition of historical Arabic manuscripts  preprocessing steps are imperative  This paper presents a novel method that consists of two major phases  The first refer to binarization and enhancement of the scanned document image  In the second phase  correction of skew angle in the text line passes by the detection of curvatureinclination of the baseline  Then  calculating the skewed angle of this line  and finally  correcting the line with a rotation relative to its centre  The proposed method was implemented on different scanned Arabic documents  The proposed methodology overcomes the defects of global binarization method  also  save the high computation effort of adaptive binarization techniques  Moreover  it works well with both Arabic handwritten words and printed text  Keywords  preprocessing  Arabic Baseline  Ancient documents  correction of inclination  correction of </raw_string>
  </article>
  <article>
    <title>Tensor Framework and Combined Symmetry for Hypertext Mining</title>
    <count>655</count>
    <raw_string>Fundamenta Informaticae 97  2009  215234 215 DOI 103233FI2009198 IOS Press Tensor Framework and Combined Symmetry for Hypertext Mining Suman Saha  CA Murthy and Sankar K Pal  Center for Soft Computing Research Indian Statistical Institute  India ssaha r  murthy  sankar isical acin Abstract  We have made a case here for utilizing tensor framework for hypertext mining  Tensor is a generalization of vector and tensor framework discussed here is a generalization of vector space model which is widely used in the information retrieval and web mining literature  Most hypertext documents have an inherent internal tag structure and external link structure that render the desirable use of multidimensional representations such as those offered by tensor objects  We have focused on the advantages of Tensor Space Model  in which documents are represented using sixthorder tensors  We have exploited the localstructure and neighborhood recommendation encapsulated by the proposed representation  We have defined a similarity measure for tensor objects corresponding to hypertext documents  and evaluated the proposed measure for mining tasks  The superior perfor mance of the proposed methodology for clustering and classification tasks of hypertext documents have been demonstrated here  The experiment using different types of similarity measure in the dif  ferent components of hypertext documents provides the main advantage of the proposed model  It has been shown theoretically that  the computational complexity of an algorithm performing on ten sor framework using tensor similarity measure as distance is at most the computational complexity of the same algorithm performing on vector space model using vector similarity measure as distance  Keywords  tensor space  hypertext  internal structure  similarity measure  1  Introduction Most of the hypertext categorization systems use simple models of </raw_string>
  </article>
  <article>
    <title>Tracing Potential School Shooters in the Digital Sphere</title>
    <count>655</count>
    <raw_string>SK Bandyopadhyay et al   Eds    ISA 2010  CCIS 76  pp  163178  2010   SpringerVerlag Berlin Heidelberg 2010 Tracing Potential School Shooters in the Digital Sphere Jari Veijalainen  Alexander Semenov  and Jorma Kypp Univ of Jyvskyl  Dept  of Computer Science and Information Systems  PO Box 35 FIN40014 Univ of Jyvskyl  JariVeijalainenAlexanderSemenovJormakyppojyufi Abstract  There are over 300 known school shooting cases in the world and over ten known cases where the perpetrator s  have been prohibited to perform the attack at the last moment or earlier  Interesting from our point of view is that in many cases the perpetrators have expressed their views in social media or on their web page well in advance  and often also left suicide messages in blogs and other forums before their attack  along the planned date and place  This has become more common towards the end of this decennium  In some cases this has made it possible to prevent the attack  In this paper we will look at the possibilities to find commonalities of the perpetrators  beyond the fact that they are all males from eleven to roughly 25 years old  and possibilities to follow their traces in the digital sphere in order to cut the dangerous development towards an attack  Should this not be possible  then an attack should be averted before it happens  We are especially interested in the multimedia data mining methods and social network mining and analysis that can be used to detect the possible perpetrators in time  We also present in this paper a probabilistic model that can be used to evaluate the successfailure rate of the detection of the possible </raw_string>
  </article>
  <article>
    <title>Automatic Performer Identification in Commercial Monophonic Jazz Performances</title>
    <count>655</count>
    <raw_string>Automatic performer identification in commercial monophonic Jazz performances Rafael Ramirez   Esteban Maestre  Xavier Serra Information Systems and Telecommunications Department  Pompeu Fabra University  Tanger 122  080018 Barcelona  Spain a r t i c l e i n f o Article history  Available online 4 January 2010 Keywords  Expressive performance Information retrieval Signal processing Music Classification techniques a b s t r a c t We present a pattern recognition approach to the task of identifying performers from their interpretative styles  We investigate how professional musicians express their view of the musical content of musical pieces and how to use this information in order to automatically identify performers  We apply sound analysis techniques based on spectral models for extracting deviation patterns of parameters such as pitch  timing  amplitude and timbre characterising both the internal structure of notes and the musical context in which they appear  We describe successful performer identification case studies involving monophonic audio recordings of both scoreguided and commercial improvised performances  2010 Elsevier BV All rights reserved  1  Introduction Performers manipulate sound properties such as pitch  timing  amplitude and timbre  These manipulations are clearly distin guishable by the listeners and often are reflected in concert atten dance and recording sales  Expressive music performance research  for an overview see Gabrielsson  1999  2003  investigates the manipulation of these sound properties in an attempt to under stand and recreate expression in performances  While in most forms of western classical music performers strictly follow a score specification of a piece  in other music genres such as Jazz  musicians are often encouraged to deviate signifi cantly from the score  if the score exists at all  </raw_string>
  </article>
  <article>
    <title>Identifying Multiple Userids of the Same Author</title>
    <count>654</count>
    <raw_string>Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing  pages 11241135  Seattle  Washington  USA  1821 October 2013  c2013 Association for Computational Linguistics Identifying Multiple Userids of the Same Author Tieyun Qian Bing Liu State Key Laboratory of Software Eng  Wuhan University Department of Computer Science University of Illinois at Chicago 16 Luojiashan Road 851 South Morgan St  Chicago Wuhan  Hubei 430072  China IL  USA  60607 qtywhueducn liubcs uicedu Abstract This paper studies the problem of identifying users who use multiple userids to post in so  cial media  Since multiple userids may belong to the same author  it is hard to directly apply supervised learning to solve the problem  This paper proposes a new method  which still uses supervised learning but does not require train ing documents from the involved userids  In stead  it uses documents from other userids for classifier building  The classifier can be applied to documents of the involved userids  This is possible because we transform the document space to a similarity space and learning is performed in this new space  Our evaluation is done in the online review do  main  The experimental results using a large number of userids and their reviews show that the proposed method is highly effective  1 Introduction It is common knowledge that some users in social media register multiple accountsuserids to post articles  blogs  reviews  etc  There are many rea  sons for doing this  For example  due to past post ings  a user may become despised by others  Heshe then registers another userid in order to regain hisher status  A user may also use </raw_string>
  </article>
  <article>
    <title>Study on Feature Selection in Finance Text Categorization</title>
    <count>654</count>
    <raw_string>Study on Feature Selection in Finance Text Categorization Changqiu Sun  Xiaolong Wang  Jun Xu Department of Computer Science and Technology Shenzhen Graduate School  Harbin Institute of Technology Shenzhen  China sunchangqiugmailcom xlwanginsun hit educn hit xujungmailcom AbstractDocument genre information is one of the most dis  tinguishing features in information retrieval  which brings order to the search results  What the genre classification concerned is not the topic but the genre of document  In this paper  two different feature sets were employed  bag of words which are derived by feature selection method and structural features which are selected manually and subjectively  And a comparative study on feature selection in genre classification of Chinese finance text is presented  In empirical results with classifiers on the real world corpora  we find that that manual labeled features can improve the performance clearly  Index TermsText Categorization  Feature Selection  Genre Classification I INTRODUCTION With the developing of the WWW  the available information is becoming abundant than before  However  even with the help of search engine  it s still hard to find the most suitable information due to huge amount of abundant information  In order to help user to acquire relevant and useful information  classification and cluster tools are imported to classify search engine results  All these techniques succeed in topic classifying are topiccentered  which neglect the importance of genre in results ranking  and takes little account of the individual users needs and preferences  Our research focus on professional financial search  In finance domain  different genres of information have different authority which is important  Positive or negative news will influence the price of a stock  Depending on circumstances  </raw_string>
  </article>
  <article>
    <title>Genre Identification for Office Document Search and Browsing</title>
    <count>654</count>
    <raw_string>IJDAR  2012  15 167182 DOI 101007s1003201101637 ORIGINAL PAPER Genre identification for office document search and browsing Francine Chen  Andreas Girgensohn  Matthew Cooper  Yijuan Lu  Gerry Filby Received  20 October 2010  Revised  9 February 2011  Accepted  22 March 2011  Published online  26 April 2011  SpringerVerlag 2011 Abstract When searching or browsing documents  the genre of a document is an important consideration that com plements topical characterization  We examine design con siderations for automatic tagging of office document pages with genre membership  These include selecting features that characterize genrerelated information in office docu ments  examining the utility of textbased features and image  based features  and proposing a simple ensemble method to improve the performance of genre identification  Experi ments were conducted on the openset identification of four coarse office document genres  technical paper  photo  slide  and table  Our experiments show that when combined with imagebased features  textbased features do not significantly influence performance  These results provide support for a topicindependent approach to identification of coarse office document genres  Experiments also show that our simple ensemble method significantly improves performance rela  tive to using a support vector machine  SVM  classifier alone  We demonstrate the utility of our approach by integrating our automatic genre tags in a faceted search and browsing appli cation for office document collections  F Chen  B   A Girgensohn  M Cooper  G Filby FX Palo Alto Laboratory  Inc  3400 Hillview Ave  Bldg  4  Palo Alto  CA 94304  USA email  chenfxpalcom A Girgensohn email  girgensohnfxpalcom M Cooper email  cooperfxpalcom G Filby email  </raw_string>
  </article>
  <article>
    <title>Corpus and Evaluation Measures for Automatic Plagiarism Detection</title>
    <count>654</count>
    <raw_string>Corpus and Evaluation Measures for Automatic Plagiarism Detection Alberto BarrnCedeo1  Martin Potthast2  Paolo Rosso1  Benno Stein2 1Natural Language Engineering Lab  ELiRF Department of Information Systems and Computation Universidad Politcnica de Valencia  Spain http users dsicupvesgruposnle lbarron  prossodsicupves 2 Web Technology and Information Systems Faculty of Media  Media Systems BauhausUniversitt Weimar  Germany http wwwwebisde martinpotthast  benno steinuniweimarde Abstract The simple access to texts on digital libraries and the WWW has led to an increased number of plagiarism cases in recent years  which renders manual plagiarism detection infeasible at large  Various methods for automatic plagiarism detection have been developed whose objective is to assist human experts to analyze documents for plagiarism  Unlike other tasks in natural language processing and information retrieval  it is not possible to publish a collection of real plagiarism cases for evaluation purposes since they cannot be properly anonymized  Therefore  current evaluations found in the literature are incomparable and often not even reproducible  Our contribution in this respect is a newly developed largescale corpus of artificial plagiarism and new detection performance measures tailored to the evaluation of plagiarism detection algorithms  1  Introduction Plagiarism is the use of text written by a third party in ones own writing without permission or acknowledg ment  Clough  2000   The goal of automatic plagiarism de tection is to identify the plagiarized sections in a suspicious document dq  Two approaches exist to tackle this task  in trinsic plagiarism detection and external plagiarism detec tion  In intrinsic plagiarism detection  features that indicate writ ing style are used to detect style irregularities caused by the insertion of text from a different author into dq  The writing style of a text </raw_string>
  </article>
  <article>
    <title>Corpus and Evaluation Measures for Automatic Plagiarism Detection</title>
    <count>654</count>
    <raw_string>Corpus and Evaluation Measures for Automatic Plagiarism Detection Alberto BarrnCedeo1  Martin Potthast2  Paolo Rosso1  Benno Stein2  Andreas Eiselt2 1Natural Language Engineering Lab  ELiRF Department of Information Systems and Computation Universidad Politcnica de Valencia  Spain http users dsicupvesgruposnle lbarron  prossodsicupves 2 Web Technology and Information Systems Faculty of Media  Media Systems BauhausUniversitt Weimar  Germany http wwwwebisde martinpotthast  benno steinuniweimarde Abstract The simple access to texts on digital libraries and the WWW has led to an increased number of plagiarism cases in recent years  which renders manual plagiarism detection infeasible at large  Various methods for automatic plagiarism detection have been developed whose objective is to assist human experts to analyze documents for plagiarism  Unlike other tasks in natural language processing and information retrieval  it is not possible to publish a collection of real plagiarism cases for evaluation purposes since they cannot be properly anonymized  Therefore  current evaluations found in the literature are incomparable and often not even reproducible  Our contribution in this respect is a newly developed largescale corpus of artificial plagiarism and new detection performance measures tailored to the evaluation of plagiarism detection algorithms  1  Introduction Plagiarism is the use of text written by a third party in ones own writing without permission or acknowledg ment  Clough  2000   The goal of automatic plagiarism de tection is to identify the plagiarized sections in a suspicious document dq  Two approaches exist to tackle this task  in trinsic plagiarism detection and external plagiarism detec tion  In intrinsic plagiarism detection  features that indicate writ ing style are used to detect style irregularities caused by the insertion of text from a different author into dq  The writing style </raw_string>
  </article>
  <article>
    <title>Deep Neural Networks for Source Code Author Identification</title>
    <count>653</count>
    <raw_string>M Lee et al   Eds    ICONIP 2013  Part II  LNCS 8227  pp  368375  2013   SpringerVerlag Berlin Heidelberg 2013 Deep Neural Networks for Source Code Author Identification Upul Bandara and Gamini Wijayarathna Department of Industrial Management  University of Kelaniya  Sri Lanka upulbandaragmailcom  gaminikln aclk Abstract  Plagiarism and copyright infringement are major problems in aca  demic and corporate environments  Importance of source code authorship attri bution arises as it is the starting point of detection for plagiarism  copyright infringement and law suit prosecution etc  There have been many research re  gard to this topic  Majority of these researches are based on various algorithms which compute similarity amongst source code files  However  for this Paper we have proposed Deep Neural Network  DNN based technique to be used for source code authorship attribution  Results proved that DNN based author iden tification brings promising results once compared the accuracy against pre  viously published research  Keywords  Restricted Boltzmann Machine  Deep Neural Networks  Source Code Authorship Attribution  1 Introduction Source code authorship attribution has many applications in different fields such as source code plagiarism detection  digital forensics  and intellectual property infringe  ment 1  Since magnitude of source code repositories are growing very rapidly  it is impractical to use manual techniques for source code authorship attribution  There  fore  automatic techniques could be ideal solutions for identifying authors of source codes  In this paper we investigate deep neural networks for source code authorship at tribution task  Training deep neural networks are known to be hard  It is empirical ly shown that the standard backpropagation algorithm could </raw_string>
  </article>
  <article>
    <title>A Composite Model for Computing Similarity Between Texts</title>
    <count>653</count>
    <raw_string>A Composite Model for Computing Similarity Between Texts Vom Fachbereich Informatik der Technischen Universitt Darmstadt genehmigte Dissertation zur Erlangung des akademischen Grades DrIng  vorgelegt von DiplInform  Daniel Br geboren in Wrzburg Tag der Einreichung  20  August 2013 Tag der Disputation  11  Oktober 2013 Referenten  Prof Dr Iryna Gurevych Prof Ido Dagan  PhD Dr Torsten Zesch Darmstadt 2013 D17 i Ehrenwrtliche Erklrung1 Hiermit erklre ich  die vorgelegte Arbeit zur Erlangung des akademischen Grades DrIng  mit dem Titel A Composite Model for Computing Similarity Between Texts  selbstndig und ausschlielich unter Verwendung der angegebenen Hilfsmittel erstellt zu haben  Ich habe bisher noch keinen Promotionsversuch unternommen  Darmstadt  den 20  August 2013 DiplInform  Daniel Br 1Gem 9 Abs  1 der Promotionsordnung der TU Darmstadt ii iii Wissenschaftlicher Werdegang des Verfassers2 1002  1108 Studium der Informatik mit Nebenfach Linguistik an der JuliusMaximiliansUniversitt Wrzburg 0608  1108 Diplomarbeit am Lehrstuhl fr Knstliche Intelligenz und Angewandte Informatik der Universitt Wrzburg in Kooperation mit SAP AG  SAP Research   Karlsruhe  mit dem Titel Usercentered Annotation Concepts for the Semantic Web 0109  0509 Wissenschaftlicher Mitarbeiter am FraunhoferInstitut fr Graphische Datenverarbeitung  Darmstadt 0609  0813 Wissenschaftlicher Mitarbeiter am Fachgebiet Ubiquitous Knowledge Processing der Technischen Universitt Darmstadt 2Gem 20 Abs  3 der Promotionsordnung der TU Darmstadt iv v Abstract Computing text similarity is a foundational technique for a wide range of tasks in natural language processing such as duplicate detection  question answering  or au  tomatic essay grading  Just recently  text similarity received widespread attention in the research community by the establishment of the Semantic Textual Similarity STS  Task at the Semantic Evaluation  SemEval  workshop in 2012a fact that stresses the </raw_string>
  </article>
  <article>
    <title>A Case Study of Sockpuppet Detection in Wikipedia</title>
    <count>653</count>
    <raw_string>Proceedings of the Workshop on Language in Social Media  LASM 2013   pages 5968  Atlanta  Georgia  June 13 2013  c2013 Association for Computational Linguistics A Case Study of Sockpuppet Detection in Wikipedia Thamar Solorio and Ragib Hasan and Mainul Mizan The University of Alabama at Birmingham 1300 University Blvd  Birmingham  AL 35294  USA solorioragib mainulcis uabedu Abstract This paper presents preliminary results of using authorship attribution methods for the detec tion of sockpuppeteering in Wikipedia  Sock  puppets are fake accounts created by malicious users to bypass Wikipedias regulations  Our dataset is composed of the comments made by the editors on the talk pages  To overcome the limitations of the short lengths of these comments  we use an voting scheme to com bine predictions made on individual user en tries  We show that this approach is promising and that it can be a viable alternative to the current human process that Wikipedia uses to resolve suspected sockpuppet cases  1 Introduction Collaborative projects in social media have become very popular in recent years  A very successful ex  ample of this is Wikipedia  which has emerged as the worlds largest crowdsourced encyclopaedia  This type of decentralized collaborative processes are ex  tremely vulnerable to vandalism and malicious be havior  Anyone can edit articles in Wikipedia andor make comments in article discussion pages  Reg istration is not mandatory  but anyone can register an account in Wikipedia by providing only little in formation about themselves  This ease of creating an identity has led malicious users to create mul tiple identities and use them for various purposes  ranging from block evasion  false majority opinion claims  and vote stacking  </raw_string>
  </article>
  <article>
    <title>Semantic Analysis Based Forms Information Retrieval and Classification</title>
    <count>653</count>
    <raw_string>3D Res  04  03 20134 1010073DRes 03 20134 3DR REVIEW w Semantic Analysis Based Forms Information Retrieval and Classification Tanzila Saba  Fatimah Ayidh Alqahtani Received  04 March 2013  Revised  08 May 2013  Accepted  15 July 2013  3D Research Center  Kwangwoon University and Springer 2013 Abstract Data entry forms are employed in all types of enterprises to collect hundreds of customers information on daily basis  The information is filled manually by the customers  Hence  it is laborious and time consuming to use human operator to transfer these customers information into computers manually  Additionally  it is expensive and human errors might cause serious flaws  The automatic interpretation of scanned forms has facilitated many real applications from speed and accuracy point of view such as keywords spotting  sorting of postal addresses  script matching and writer identification  This research deals with different strategies to extract customers information from these scanned forms  interpretation and classification  Accordingly  extracted information is segmented into characters for their classification and finally stored in the forms of records in databases for their further processing  This paper presents a detailed discussion of these semantic based analysis strategies for forms processing  Finally  new directions are also recommended for future research  Keywords Form processing  Data interpretations  Pre  processing  Validation strategies  Semantic analysis  Post processing  1  Introduction Despite the wide use of electronic communication  paper document such as data entry forms  postal envelopes  and checks have central importance in our daily lives  As paper documents are cheap  reliable  secure for future reference  easily available and flexible in filling  Consequently  paper Tanzila </raw_string>
  </article>
  <article>
    <title>Improving the Reliability of the Plagiarism Detection System</title>
    <count>653</count>
    <raw_string>Improving the Reliability of the Plagiarism Detection System Lab Report for PAN at CLEF 2010 Jan Kasprzak and Michal Brandejs Faculty of Informatics  Masaryk University kas brandejsfimunicz Abstract In this paper we describe our approach at the PAN 2010 plagiarism detection competition  We refer to the system we have used in PAN09  We then present the improvements we have tried since the PAN09 competition  and their impact on the results on the development corpus  We describe our experiments with intrinsic plagiarism detection and evaluate them  We then discuss the com putational cost of each step of our implementation  including the performance data from two different computers  1 Introduction Discovering plagiarism in text files by means of software is a widelystudied area  Yet it is often not clear how the proposed approaches would perform in realworld scenarios  The PAN competition1 attempts to provide a testbed for comparing different approaches on the same data  We discuss the performance of our plagiarism detection system on two document corpora  used in PAN 2010 competition  The development corpus PANPC09 11 is based on the PAN09 competition data  It contains 14429 source documents  14428 suspicious documents for the external plagiarism detection  and 6183 suspicious docu ments for the intrinsic plagiarism detection  The competition corpus contains both the intrinsic and external plagiarism cases in one body of suspicious documents  There are 11147 source documents in the corpus  and 15925 suspicious documents  The next section of this paper contains a brief recapitulation of our approach used in PAN09  We then evaluate incremental modifications to this approach and their impact on the overall score on the development corpus  The two main areas of the research are covered </raw_string>
  </article>
  <article>
    <title>Authorship Attribution of Short Messages Using Multimodal Features</title>
    <count>653</count>
    <raw_string>NAVAL POSTGRADUATE SCHOOL MONTEREY  CALIFORNIA THESIS Approved for public release  distribution is unlimited AUTHORSHIP ATTRIBUTION OF SHORT MESSAGES USING MULTIMODAL FEATURES by Sarah R Boutwell March 2011 Thesis CoAdvisors  Robert Beverly Craig H Martell THIS PAGE INTENTIONALLY LEFT BLANK i REPORT DOCUMENTATION PAGE Form Approved OMB No 07040188 Public reporting burden for this collection of information is estimated to average 1 hour per response  including the time for reviewing instruction  searching existing data sources  gathering and maintaining the data needed  and completing and reviewing the collection of information  Send comments regarding this burden estimate or any other aspect of this collection of information  including suggestions for reducing this burden  to Washington headquarters Services  Directorate for Information Operations and Reports  1215 Jefferson Davis Highway  Suite 1204  Arlington  VA 222024302  and to the Office of Management and Budget  Paperwork Reduction Project  07040188  Washington DC 20503  1  AGENCY USE ONLY  Leave blank  2 REPORT DATE March 2011 3 REPORT TYPE AND DATES COVERED Masters Thesis 4  TITLE AND SUBTITLE Authorship Attribution of Short Messages Using Multimodal Features 6  AUTHOR S  Sarah R Boutwell 5  FUNDING NUMBERS 7 PERFORMING ORGANIZATION NAME S  AND ADDRESSES  Naval Postgraduate School Monterey  CA 939435000 8 PERFORMING ORGANIZATION REPORT NUMBER 9  SPONSORING MONITORING AGENCY NAME S  AND ADDRESSES  NA 10  SPONSORINGMONITORING AGENCY REPORT NUMBER 11  SUPPLEMENTARY NOTES The views expressed in this thesis are those of the author and do not reflect the official policy or position of the Department of Defense or the US Government  IRB Protocol number  na             </raw_string>
  </article>
  <article>
    <title>Toward Automated Stylistic Transformation of Natural Language Text</title>
    <count>653</count>
    <raw_string>DIGITAL HUMANITIES 2009 Page 177 lingual Machine Translation for Paraphrase Generation  Proceedings of the Conference on Empirical Methods in Natural Language Processing 142149  Barcelona  Spain  Stamatatos  E  N Fakotakis and G Kokkinakis  2000  A practical chunker for unrestricted text  Proceedings of the Conference on Natural Language Processing  139 150  Patras  Greece  Topkara  M  C M Taskiran  and E Delp  2005  Natural Language Watermarking  Proceedings of the SPIE In ternational Conference on Security  Steganography  and Watermarking of Multimedia Contents  San Jose  Toward Automated Stylistic Transformation of Natural Language Text Foaad Khosmood University of California Santa Cruz foaaducscedu Robert Levinson University of California Santa Cruz levinsoncse ucscedu Introduction Style is an integral part of natural language in written  spoken or machine generated forms  Humans have been dealing with style in language since the beginnings of language itself  Almost everyone who is capable of reading and writing  or even just hearing and speaking  routinely identifies and employs different variations of language in daily life  We easily recognize distinct styles of language and can produce our own in multiple varia  tions depending on the context  Computers  however  are not yet capable of anything that sophisticated  Automatic processing of text styles poses two interrelat ed challenges  classification and transformation  There have been some recent advances in corpus classifica  tion  automatic clustering and authorship attribution of text using a variety of features and techniques 1919 20212434  Integral to each approach is a feature set to be extracted  such as ngrams or vocabulary set   and a learning algorithm  such as neural nets or Bayes </raw_string>
  </article>
  <article>
    <title>Efficient Storage Methods for a Literary Data Warehouse</title>
    <count>653</count>
    <raw_string>Efficient Storage Methods for a Literary Data Warehouse by Steven WKeith BScCS University of New Brunswick Saint John A THESIS SUBMITTED IN PARTIAL FULFILLMENT OF THE REQUIREMENTS FOR THE DEGREE OF Master of Computer Science In the Graduate Academic Unit of Computer Science SupervisorDrOwen KaserPhDComputer Science CoSupervisorDrDaniel LemirePhDComputer ScienceUQAM Examining BoardDrLawrence GareyPhDComputer ScienceChair DrWeichang DuPhDComputer Science DrGheorghe StoicaPhDMathematics This thesis is accepted Dean of Graduate Studies THE UNIVERSITY OF NEW BRUNSWICK May 2006 cSteven WKeith2006 Dedication To my parents ii Abstract Computerassisted reading and analysis of text has applications in the humanities and social sciencesEverlarger electronic text archives have the advantage of allowing a more complete analysis but the disadvantage of forcing longer waits for resultsThis thesis addresses the issue of efficiently storing data in a literary data warehouseThe method in which the data is stored directly influences the ability to extract usefulanalytical results from the data warehouse in a timely fashionA variety of storage methods including mapped filestreeshashingand databases are evaluated to determine the most efficient method of storing data points in cubes in the data warehouseEach storage methods ability to insert and retrieve data points as well as slicediceand rollup a cube is evaluatedThe amount of disk space required to store the cubes is also consideredFive test cubes of various sizes are used to determine which method being evaluated is most efficientThe results lead to various storage methods being efficientdepending on properties of the cube and the requirements of the useriii Table of Contents Dedication ii Abstract iii Table of Contents vi List of Tables viii List of Figures x List of Algorithms xi 1 Introduction 1 11 Motivation 1 12 Practical Applications 3 13 Contributions 4 14 Organization 5 2 Background 6 21 Data Warehouses 6 211 Description of Data 7 22 Data Cubes 9 221 Operations </raw_string>
  </article>
  <article>
    <title>Paraphrase Learning in Two Phases For Steganographic Communication</title>
    <count>653</count>
    <raw_string>DIGITAL HUMANITIES 2009 Page 173 Paraphrase Learning in Two Phases For Steganographic Communication Katia Lida Kermanidis Ionian University kermanioniogr Introduction Given an original sentence  that conveys a specific mean ing  paraphrasing means expressing the same meaning using a different set of words or a different syntactic structure  Paraphrasing has been used extensively for educational purposes in language learning  as well as in several NLP tasks like text summarization  Brockett and Dolan  2005   question answering  Duclaye et al   2003  and natural language generation  Recently it has found yet another use in steganography  Regarding paraphrase identification and generation  pre  vious approaches have utilized supervised  Kozareva and Montoyo  2006  or unsupervised  Barzilay and Lee  2003  machine learning techniques  finite state automa  ta  Pang et al   2003   syntactic dependency rules  Meral et al   2007   statistical machine translation techniques  Quirk et al   2004   Figure 1  Paraphrase learning in two phases  In the present proposal  paraphrases of Modern Greek free text are learned in two phases  Henceforth  the term paraphrasing  will stand for shallow syntactic transfor mations  i e  swaps of consecutive phrasal chunks  Mod  ern Greek is quite suitable for shallow paraphrasing  due to the permissible freedom in the ordering of the phrases in a sentence  The paraphrase learning process is based on resource economy  the desire to utilize as minimal linguistic re  sources as possible  enabling thereby the methodology to be easily applicable to other morphologically rich lan guages like Modern Greek  The paraphrased text may then be used </raw_string>
  </article>
  <article>
    <title>Bootstrapped Named Entity Recognition for Product Attribute Extraction</title>
    <count>653</count>
    <raw_string>Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing  pages 15571567  Edinburgh  Scotland  UK July 2731  2011  c2011 Association for Computational Linguistics Bootstrapped Named Entity Recognition for Product Attribute Extraction Duangmanee  Pew  Putthividhya eBay Inc 2065 Hamilton Ave San Jose  CA 95125 dputthividhyaebay com Junling Hu eBay Inc 2065 Hamilton Ave San Jose  CA 95125 juhuebay com Abstract We present a named entity recognition  NER  system for extracting product attributes and values from listing titles  Information extrac  tion from short listing titles present a unique challenge  with the lack of informative con text and grammatical structure  In this work  we combine supervised NER with bootstrap  ping to expand the seed list  and output nor malized results  Focusing on listings from eBays clothing and shoes categories  our bootstrapped NER system is able to identify new brands corresponding to spelling variants and typographical errors of the known brands  as well as identifying novel brands  Among the top 300 new brands predicted  our system achieves 9033  precision  To output normal  ized attribute values  we explore several string comparison algorithms and found ngram sub  string matching to work well in practice  1 Introduction Traditional named entity recognition  NER  task has expanded beyond identifying people  location  and organization to book titles  email addresses  phone numbers  and protein names  Nadeau and Sekine 2007   Recently there has been a surge of interest in extracting product attributes from online data due to the rapid growth of ECommerce  Current work in this domain focuses on mining product reviews and descriptions from retailer websites  Such </raw_string>
  </article>
  <article>
    <title>Comparing Techniques for Authorship Attribution of Source Code</title>
    <count>652</count>
    <raw_string>SOFTWARE  PRACTICE AND EXPERIENCE Softw  Pract  Exper   2012  Published online in Wiley Online Library  wileyonlinelibrary com   DOI  101002spe 2146 Comparing techniques for authorship attribution of source code Steven Burrows   Alexandra L Uitdenbogerd and Andrew Turpin School of Computer Science and Information Technology  RMIT University  GPO Box 2476  Melbourne 3001  Australia SUMMARY Attributing authorship of documents with unknown creators has been studied extensively for natural language text such as essays and literature  but less so for nonnatural languages such as computer source code  Previous attempts at attributing authorship of source code can be categorised by two attributes  the software features used for the classification  either strings of n tokensbytes ngrams  or software metrics  and the classification technique that exploits those features  either information retrieval rank  ing or machine learning  The results of existing studies  however  are not directly comparable as all use different test beds and evaluation methodologies  making it difficult to assess which approach is superior  This paper summarises all previous techniques to source code authorship attribution  implements feature sets that are motivated by the literature  and applies information retrieval ranking methods or machine classifiers for each approach  Importantly  all approaches are tested on identical collections from varying programming languages and author types  Our conclusions are as follows   i  ranking and machine classifier approaches are around 90  and 85  accurate  respectively  for a onein10 classification problem   ii  the bytelevel ngram approach is best used with different parameters to those previously published   iii  neural networks and support vector machines were found to be the </raw_string>
  </article>
  <article>
    <title>Spam Control by Source Throttling Using Integer Factorization</title>
    <count>650</count>
    <raw_string>Spam Control by Source Throttling Using Integer Factorization Rochak Gupta  Vinay Kumar K  and Radhesh Mohandas Department of Computer Science and Engineering National Institute of Technology Karnataka  Surathkal  India guptarochakgmailcom Abstract  Existing solutions for spam control that are limited to spam filtering at the receiver side underestimate the fact that the network bandwidth and processing time of the recipient email servers are wasted  To cut down these costs spam should be controlled before it reaches the receiving email server  In this paper  we propose a solution to control spam at the senders email server by throttling the clients CPU using integer factorization problem  Integer factorization is used to generate stamps as a proof of CPU cycles expended by the senders system for each email recipient  Cost of generating stamps is negligible when the client is sending emails to only a few recipients  However  as the number of recipients increases  the cost of generating stamps also increases which adversely affects the processing speed of the client  The server requires minimal processing time to verify stamps generated by the client  Keywords  Integer Factorization  Source throttling  Spam  Spam con trol  Stamps  1 Introduction Techniques that control spam at the client side decrease only the costs associated with recipients  These techniques do not reduce the costs associated with network bandwidth to carry heavy load of spam and email servers to process spam emails  Cost based spam control can be a solution to reduce the volume of spam by making the senders to pay for each email being sent  Nevertheless  forcing legitimate sender to pay money is not a good solution to achieve spam control  However  computational proof </raw_string>
  </article>
  <article>
    <title>Towards Genre Classification for IR in the Workplace</title>
    <count>650</count>
    <raw_string>30 Towards Genre Classification for IR in the Workplace Luanne Freund Faculty of Information Studies  University of Toronto  Canada luanne freundutorontoca Charles LA Clarke School of Computer Science  University of Waterloo  Canada claclarkplg2math uwaterlooca Elaine G Toms Faculty of Management  Dalhousie University  Canada etomsdalca ABSTRACT Use of document genre in information retrieval systems has the potential to improve the taskappropriateness of results  However  genre classification remains a challenging problem  We describe a case study of genre classification in a software engineering workplace domain  which includes the development of a genre taxonomy and experiments in automatic genre classification using supervised machine learning  We present results based on evaluation using reallife enterprise data from this work domain  Categories and Subject Descriptors H33 Information Search and Retrieval General Terms Measurement  Experimentation  Human Factors Keywords contextual information retrieval  genredependent applications  genre classification  enterprise search 1  INTRODUCTION Searching for information in the workplace can be daunting  in some cases more daunting than the task that prompted the search in the first place  The rapid increase in digital resources  corporate portals and intranets  together with the growth of the World Wide Web  means that employees working in large companies now have vast libraries of digital resources readily available to support their work activities  These libraries  tend to be ad hoc and distributed collections of heterogeneous resources  lacking in unifying standards and quality controls 1  Unlike the Web  where information is often pushed to the appropriate community  internal corporate information is more passively  or even begrudgingly shared 2  Finding something useful under these conditions  requires that employees  a  know where to start searching </raw_string>
  </article>
  <article>
    <title>Code Analyzer for an Online Course Management System</title>
    <count>650</count>
    <raw_string>The Journal of Systems and Software 83  2010  24782486 Contents lists available at ScienceDirect The Journal of Systems and Software journa l homepage  wwwe lsev ier com locate  j ss Code analyzer for an online course management system Jong Yih Kuo   Fu Chu Huang Department of Computer Science and Information Engineering  National Taipei University of Technology  Taipei 106  Taiwan a r t i c l e i n f o Article history  Received 14 December 2009 Received in revised form 12 May 2010 Accepted 15 July 2010 Available online 24 July 2010 Keywords  Intelligent agent Program similarity Program plagiarism a b s t r a c t The online course management system  OCMS  assists online instruction in various aspects  including testing  course discussion  assignment submission  and assignment grading  This paper proposes a plagia  rism detection system whose design is integrated with an OCMS Online assignment submission is prone to easy plagiarism  which can seriously influence the quality of learning  In the past  plagiarism was detected manually  making it very timeconsuming  This research thus focuses on developing a system involving code standardization  textual analysis  structural analysis  and variable analysis for evaluating and comparing programming codes  An agent system serves as a daemon to analyze the program codes for OCMS For textual analysis  the Fingerprinting Algorithm was used for text comparison  Structurally  a formal algebraic expression and a dynamic control structure tree DCS Tree  were utilized to rebuild and evaluate the program structure  For variables  not only the relevant information for each variable was recorded  but also the programming structure was analyzed where the variables are positioned </raw_string>
  </article>
  <article>
    <title>Automated Authorship Attribution Using Advanced Signal Classification Techniques</title>
    <count>650</count>
    <raw_string>Automated Authorship Attribution Using Advanced Signal Classification Techniques Maryam Ebrahimpour1  Talis J Putnins123  Matthew J Berryman14  Andrew Allison1  Brian WH Ng1  Derek Abbott1 1 School of Electrical and Electronic Engineering  The University of Adelaide  Adelaide  South Australia  Australia  2 Stockholm School of Economics in Riga  Riga  Latvia  3 University of Technology Sydney  Sydney  New South Wales  Australia  4 SMART Infrastructure Facility  University of Wollongong  Wollongong  New South Wales  Australia Abstract In this paper  we develop two automated authorship attribution schemes  one based on Multiple Discriminant Analysis  MDA  and the other based on a Support Vector Machine  SVM  The classification features we exploit are based on word frequencies in the text  We adopt an approach of preprocessing each text by stripping it of all characters except az and space  This is in order to increase the portability of the software to different types of texts  We test the methodology on a corpus of undisputed English texts  and use leaveoneout cross validation to demonstrate classification accuracies in excess of 90   We further test our methods on the Federalist Papers  which have a partly disputed authorship and a fair degree of scholarly consensus  And finally  we apply our methodology to the question of the authorship of the Letter to the Hebrews by comparing it against a number of original Greek texts of known authorship  These tests identify where some of the limitations lie  motivating a number of open questions for future work  An open source implementation of our methodology is freely available for use at https githubcommatthewberrymanauthordetection  Citation  Ebrahimpour M  Putnins </raw_string>
  </article>
  <article>
    <title>Enhance Web Pages Genre Identification Using Neighboring Pages</title>
    <count>650</count>
    <raw_string>Enhance Web Pages Genre Identification Using Neighboring Pages Jia Zhu1  Xiaofang Zhou2  and Gabriel Fung3 1 School of ITEE  The University of Queensland  Australia jiazhuitee uqeduau 2 School of ITEE  The University of Queensland  Australia zxfitee uqeduau 3 IGNGAB Lab  Hong Kong gabrieligngabcom Abstract  Recently web pages genre identification attracts more atten tions because of its importance in web searching  Most of existing works used the features extracted from web pages and applied machine learn  ing approaches like SVM as classifier to identify the genre of web pages  However  in the case where web pages do not contain enough information  such an approach may not work well  In this paper  we consider to tackle genre identification in such situations  We propose a linkbased graph model that taking into account neighboring pages but greatly reducing the noisy information by selecting an appropriate subset of neighboring pages  We evaluated this neighboring pages based classifier with other classifiers  The experiments conducted on two known corpora  and the favorable results indicated that our proposed approach is feasible  1 Introduction As the World Wide Web continues to grow exponentially  web pages classification becomes extremely important in web searching  Web pages genre identification  as a sub category of web pages classification  focuses on functional purposes and discovering groups of texts compared to topic identification  For example  a website like wwwnewscom au  has many topics in the pages  eg  sport news and finance news  but the genre of these pages in this website is news  The most important component to identify web genre is to select an appropri ate feature set  As the word genre </raw_string>
  </article>
  <article>
    <title>Source Code Author Identification with Unsupervised Feature Learning</title>
    <count>649</count>
    <raw_string>Pattern Recognition Letters 34  2013  330334Contents lists available at SciVerse ScienceDirect Pattern Recognition Letters journal homepage  wwwelsevier comlocate patrecSource code author identification with unsupervised feature learning Upul Bandara   Gamini Wijayarathna Department of Industrial Management  University of Kelaniya  Sri Lanka a r t i c l e i n f o a b s t r a c tArticle history  Received 26 April 2012 Available online 16 November 2012 Communicated by S Sarkar Keywords  Source code author identification Unsupervised feature learning Code metrics Autoencoder Logistic regression01678655  see front matter 2012 Elsevier BV A http dx doi org101016jpatrec201210027  Corresponding author  Tel fax  94 715 468 345  Email address  upulbandaragmailcom  U BandAutomatic identification of source code authors has many applications in different fields such as source code plagiarism detection  and law suit prosecution  This paper presents a new source code author iden tification system based on an unsupervised feature learning technique  As a method of extracting features from high dimensional data  unsupervised feature learning has obtained a great success in many fields such as character recognition and image classification  However  according to our knowledge it has not been applied for source code author identification systems  Therefore  we investigated an unsupervised feature learning technique called sparse autoencoder as a method of extracting features from source code files  Our system was evaluated with several datasets and results have shown that performance is very close to the state of art techniques in the source code identification field  2012 Elsevier BV All rights reserved 1  Introduction Automatic identification of source code authors is an important activity in many fields in computing  Among those fields  source code plagiarism detection </raw_string>
  </article>
  <article>
    <title>Toward Unification of Source Attribution Processes and Techniques</title>
    <count>649</count>
    <raw_string>Proceedings of the Fifth International Conference on Machine Learning and Cybernetics  Dalian  1316 August 2006 142440060006 2000 2006 IEEE 4551 TOWARD UNIFICATION OF SOURCE ATTRIBUTION PROCESSES AND TECHNIQUES FOAAD KHOSMOOD  ROBERT LEVINSON Department of Computer Science University of California at Santa Cruz EMAIL  foaadsoe ucscedu  levinsonsoe ucscedu Abstract  Automatic Source Attribution refers to the ability for an autonomous process to determine the source of a previously unexamined piece of writing  Statistical methods for source attribution have been the subject of scholarly research for well over a century  The field  however  is still missing a definitive currency of established or agreedupon classes of features  methods  techniques and nomenclature  This paper represents continuation of research into the basic attribution problem  as well as work towards an eventual source attribution standard  We augment previous work which utilized incommon  non  trivial word frequencies with neural networks on a more standardized data set  We also use two other techniques  Phrasebased feature sets evaluated with nave Bayesians and bigram feature sets evaluated with the nearest neighbor algorithm  We compare the three and explore methods of combining the techniques in order to achieve better results  Keywords  Source attribution  neural networks  nave Bayesian  authorship attribution  ngrams  Meta predictors 1  Introduction Natural languages are ambiguous  imprecise  subject to interpretations and logical contradictions  These aspects make natural language processing one of the oldest and most difficult unsolved problems in artificial intelligence  It is precisely these same attributes  however  that allow humans to develop distinct styles of literary expression  Every conscious and unconscious choice allowed within the confines of the written communication contributes to the uniqueness </raw_string>
  </article>
  <article>
    <title>Genre Identification for Office Document Search and Browsing</title>
    <count>649</count>
    <raw_string>IJDAR DOI 101007s1003201101637 ORIGINAL PAPER Genre identification for office document search and browsing Francine Chen  Andreas Girgensohn  Matthew Cooper  Yijuan Lu  Gerry Filby Received  20 October 2010  Revised  9 February 2011  Accepted  22 March 2011  SpringerVerlag 2011 Abstract When searching or browsing documents  the genre of a document is an important consideration that com plements topical characterization  We examine design con siderations for automatic tagging of office document pages with genre membership  These include selecting features that characterize genrerelated information in office docu ments  examining the utility of textbased features and image  based features  and proposing a simple ensemble method to improve the performance of genre identification  Experi ments were conducted on the openset identification of four coarse office document genres  technical paper  photo  slide  and table  Our experiments show that when combined with imagebased features  textbased features do not significantly influence performance  These results provide support for a topicindependent approach to identification of coarse office document genres  Experiments also show that our simple ensemble method significantly improves performance rela  tive to using a support vector machine  SVM  classifier alone  We demonstrate the utility of our approach by integrating our automatic genre tags in a faceted search and browsing appli cation for office document collections  F Chen  B   A Girgensohn  M Cooper  G Filby FX Palo Alto Laboratory  Inc  3400 Hillview Ave  Bldg  4  Palo Alto  CA 94304  USA email  chenfxpalcom A Girgensohn email  girgensohnfxpalcom M Cooper email  cooperfxpalcom G Filby email  filbyfxpalcom Y  Lu Texas State University  San Marcos  TX </raw_string>
  </article>
  <article>
    <title>A Machine Learning Approach to Reading Level Assessment</title>
    <count>648</count>
    <raw_string>Available online at www sciencedirectcom COMPUTERComputer Speech and Language 23  2009  89106 wwwelseviercomlocatecsl SPEECH AND LANGUAGEA machine learning approach to reading level assessment Sarah E Petersen a   Mari Ostendorf b a Department of Computer Science and Engineering  University of Washington  Seattle  WA 98195  United States b Department of Electrical Engineering  University of Washington  Seattle  WA 98195  United States Received 20 December 2006  received in revised form 22 April 2008  accepted 23 April 2008 Available online 7 May 2008Abstract Reading proficiency is a fundamental component of language competency  However  finding topical texts at an appro priate reading level for foreign and second language learners is a challenge for teachers  Existing measures of reading level are not well suited to this task  where students may know some difficult topicrelated vocabulary items but not have the same level of sophistication in understanding complex sentence constructions  Recent work in this area has shown the ben efit of using statistical language processing techniques  In this paper  we use support vector machines to combine features from ngram language models  parses  and traditional reading level measures to produce a better method of assessing read  ing level  We explore the use of negative training data to handle the problem of rejecting data from classes not seen in training  and compare the use of detection vs regression models on this task  As in many language processing problems  we find substantial variability in human annotation of reading level  and explore ways that multiple human annotations can be used in comparative assessments of system performance  2008 Elsevier Ltd All rights reserved  Keywords  Reading level assessment  SVMs1  Introduction </raw_string>
  </article>
  <article>
    <title>Automated Unsupervised Authorship Analysis Using Evidence Accumulation Clustering</title>
    <count>648</count>
    <raw_string>Natural Language Engineering http journalscambridge orgNLE Additional services for Natural Language Engineering  Email alerts  Click here Subscriptions  Click here Commercial reprints  Click here Terms of use  Click here Automated unsupervised authorship analysis using evidence accumulation clustering ROBERT LAYTON  PAUL WATTERS and RICHARD DAZELEY Natural Language Engineering  Volume 19  Issue 01  January 2013  pp 95  120 DOI  101017S1351324911000313  Published online  21 November 2011 Link to this article  http journalscambridge orgabstractS1351324911000313 How to cite this article  ROBERT LAYTON  PAUL WATTERS and RICHARD DAZELEY  2013   Automated unsupervised authorship analysis using evidence accumulation clustering  Natural Language Engineering  19  pp 95120 doi101017S1351324911000313 Request Permissions  Click here Downloaded from http journalscambridge orgNLE  IP address  195251141162 on 14 Jul 2015 Natural Language Engineering 19  1   95120  c Cambridge University Press 2011 doi101017S1351324911000313 95 Automated unsupervised authorship analysis using evidence accumulation clustering ROBERT LAYTON1  PAUL WATTERS1 and RICHARD DAZELEY2 1Internet Commerce Security Laboratory  University of Ballarat  Australia emails  rlaytonicslcom au  pwattersballarateduau 2Data Mining and Informatics Research Group  University of Ballarat  Australia email  rdazeleyballarateduau  Received 9 May 2011  revised 18 October 2011  accepted 21 October 2011  first published online 21 November 2011  Abstract Authorship Analysis aims to extract information about the authorship of documents from features within those documents  Typically  this is performed as a classification task with the aim of identifying the author of a document  given a set of documents of known authorship  Alternatively  unsupervised methods have been developed primarily as visualisation tools to assist the manual discovery of clusters of authorship within a corpus by analysts  However </raw_string>
  </article>
  <article>
    <title>Forensic Authorship Attribution Using Compression Distances to Prototypes</title>
    <count>648</count>
    <raw_string>Forensic Authorship Attribution Using Compression Distances to Prototypes Maarten Lambers2 and Cor J Veenman12 1 Intelligent Systems Lab  University of Amsterdam  Amsterdam  The Netherlands 2 Digital Technology  Biometrics Department  Netherlands Forensic Institute  The Hague  The Netherlands Abstract  In several situations authors prefer to hide their identity  In forensic applications  one can think of extortion and threats in emails and forum messages  These types of messages can easily be adjusted  such that meta data referring to names and addresses is at least unreliable  In this paper  we propose a method to identify authors of short informal messages solely based on the text content  The method uses compression distances between texts as features  Using these features a supervised classifier is learned on a training set of known authors  For the experiments  we prepared a dataset from Dutch newsgroup texts  We com pared several stateoftheart methods to our proposed method for the identifica  tion of messages from up to 50 authors  Our method clearly outperformed the other methods  In 65  of the cases the author could be correctly identified  while in 88  of the cases the true author was in the top 5 of the produced ranked list  1 Introduction Besides the many legitimate uses of the internet  it also has become a communication medium for illegal activities  Examples include child pornography distribution  threat ening letters and terroristic communications  Finding the illegal distributor or the writer of illegal content is an important aspect of the efforts of the forensic experts to reduce criminal activity on the internet  In this research  we focus on the analysis of short messages such as email </raw_string>
  </article>
  <article>
    <title>Latin Etymologies as Features on BNC Text Categorization</title>
    <count>648</count>
    <raw_string>Latin Etymologies as Features on BNC Text Categorization  Alex Chengyu Fang a  Wanyin Li a  and Nancy Ide b a Department of Chinese  Translation  and Linguistics  City University of Hong Kong  Tat Chee Avenue  Kowloon  Hong Kong acfang  clairelicityueduhk b Department of Computer Science  Vassar College  124 Raymond Avenue  Poughkeepsie  NY 12604 idecs vassaredu Abstract  This paper presents an early experimental work on BNC Text Categorization TC  with Latin etymologies as features  emphasis on spoken and written texts  Two aims achieved in this study   1  to explore discriminative new linguistic features rather than lots of noisebringing bagofwords   BoW    2  to build up a base step to represent texts in distinct types of linguistic features with different weighting scheme rather than a plain feature vectors of BoW The experiments disclose a notable distinct distribution pattern of Latin etymologies in spoken and written BNC texts  The performance of a homemade classifier based on the probability distribution ranges of Latin etymologies reaches a precision of 7231  and recall of 7322  on BNC spoken texts and precision of 7331  and recall of 6998  on BNC written texts  Keywords  Text Categorization  Latin Etymologies  Discriminative Features   This work is supported by the project A Descriptive Multitiered Model for Text Type Classification Based on Syntactic Complexities  Project No 7002190   Investigating the Etymological Composition of Lexical Use in Contemporary English  A study Based on the British Notional Corpus  Project No 7200120   Text Genre Classification Based on LexicoGrammatical and Syntactic Cues  Project No 7002387   Department of Chinese  Translation and Linguistics </raw_string>
  </article>
  <article>
    <title>Analyzing Writing Styles of Bloggers with Different Opinions</title>
    <count>648</count>
    <raw_string>ANALYZING WRITING STYLES OF BLOGGERS WITH DIFFERENT OPINIONS Thomas H Park  Jiexun Li  Haozhen Zhao College of Information Science and Technology Drexel University thomaspark  jiexunli  haozhenzhao ischooldrexel edu Michael Chau School of Business The University of Hong Kong mchaubusiness hkuhk Abstract Understanding customers is crucial to companies  decisionmaking  With the advent of Web 20 more and mo re p eople ch oose to exp ress th eir feelin gs a nd a rticulate th eir a ttitudes th rough online social communities such as blogs and web forums  These new sources of information offer the potential to obtain large quantities of customer feedback using automated analysis techniques  In th is pap er  we stu dy ho w p eople with d ifferent o pinions to ward a commerci al p roduct write differently in their blogs  We define and extract four types of stylometric features  namely lexical  syntactic  structural  and sentimental features  t o represent a blogger s writing style  Based on multivariate analyses on a data set of iPodrelat ed blogs  we found vari ous writing style patterns of bloggers  Our analysis shows that a b loggers writing style is ma rginally related to his or her opinion toward a product  Keywords  Blog Analysis  Stylometry  Principal Components Analysis  MANOVA 1  Introduction Understanding customers is crucial to companies  decisionmaking  Traditionally  companies have relied on expensive  timeconsuming methods to collect customer feedback  including focus groups  surveys  and professional evaluations  With the advent of Web 20 a growing number of people are sharing their opinions about commercial products and services through online social communities such as blogs and web forums </raw_string>
  </article>
  <article>
    <title>A New Document Author Representation for Authorship Attribution</title>
    <count>648</count>
    <raw_string>A New Document Author Representation for Authorship Attribution Adrian Pastor LopezMonroy  Manuel MontesyGomez  Luis VillasenorPineda  Jesus Ariel CarrascoOchoa  and Jose  Fco  MartnezTrinidad National Institute for Astrophysics  Optics and Electronics  Computer Science Department  Luis Enrique Erro  1  Tonantzintla  Puebla  Mexico pastor mmontesg villasen ariel fmartine cccinaoepmx Abstract  This paper proposes a novel representation for Authorship Attribution  AA   based on Concise Semantic Analysis  CSA   which has been successfully used in Text Categorization TC   Our approach for AA  called Document Author Representation  DAR   builds document vectors in a space of authors  calculating the relationship between textual features and authors  In order to evaluate our approach  we compare the proposed representation with conventional approaches and previous works using the c50 corpus  We found that DAR can be very useful in AA tasks  because it provides good performance on imbalanced data  getting comparable or better accuracy results  Keywords  Authorship Attribution  Author Identification  Document Represen tation  Semantic Analysis  Text Classification  1 Introduction Authorship Attribution  AA  consists in learning the writing style of one or more au  thors  in order to identify them automatically in future texts 2  Today  the amount of information available on Internet is overwhelming  and much of it is plain text emails  online forums  blogs  source code   In this context  several issues and applications re  lated to AA have emerged  for example  ciberbullying  plagiarism detection  spam fil tering  computer forensics and fraud detection 2  The AA task can be stated as a singlelabeled multiclass </raw_string>
  </article>
  <article>
    <title>Examining Variations of Prominent Features in Genre Classification</title>
    <count>648</count>
    <raw_string>Examining Variations of Prominent Features in Genre Classification Yunhyong Kim and Seamus Ross Digital Curation Centre DCC  Humanities Advanced Technology and Information Institute  HATII  University of Glasgow  UK ykim sross hatii arts glaacuk Abstract This paper investigates the correlation between features of three types visual  stylistic and topical types  and genre classes  The majority of previous studies in automated genre classification have created models based on an amalgamated representation of a document using a combination of features  In these models  the inseparable roles of different features make it difficult to determine a means of improving the classifier when it exhibits poor performance in detecting selected genres  In this paper we use classifiers independently modeled on three groups of features to examine six genre classes to show that the strongest features for making one classification is not necessarily the best features for carrying out another classification  1  Introduction The research described in this paper examines genre classes of text documents and the role of different types of features in distinguishing these classes automatically  Automated genre classification  e g  classification into scientific research articles  news report  or email   which identifies the function and structure of the document  supports metadata extraction  12  which is essential for efficient and effective information management in repositories  It also helps information seekers focus their search on specific genres  thereby enabling the retrieval of documents exhibiting different levels of detail  and  aids knowledge mining by performing a firstlevel classification of documents into documents of similar physical and conceptual structure  The features which characterise a text often fall into welldefined groups  For example  some features capture the position of text </raw_string>
  </article>
  <article>
    <title>The Effect of Borderline Examples on Language Learning</title>
    <count>648</count>
    <raw_string>PLEASE SCROLL DOWN FOR ARTICLE This article was downloaded by  HEALLink Consortium  On  3 March 2009 Access details  Access Details  subscription number 786636557 Publisher Taylor  Francis Informa Ltd Registered in England and Wales Registered Number  1072954 Registered office  Mortimer House  3741 Mortimer Street  London W1T 3JH  UK Journal of Experimental  Theoretical Artificial Intelligence Publication details  including instructions for authors and subscription information  http wwwinformaworldcomsmpptitlecontentt713723652 The effect of borderline examples on language learning Katia Lida Kermanidis a a Department of Informatics  Ionian University  Corfu  Greece Online Publication Date  01 March 2009 To cite this Article Kermanidis  Katia Lida 2009  The effect of borderline examples on language learning Journal of Experimental  Theoretical Artificial Intelligence 21119  42 To link to this Article  DOI  10108009528130802113406 URL  http dx doi org10108009528130802113406 Full terms and conditions of use  http wwwinformaworldcomtermsandconditionsofaccesspdf This article may be used for research  teaching and private study purposes  Any substantial or systematic reproduction  redistribution  reselling  loan or sublicensing  systematic supply or distribution in any form to anyone is expressly forbidden  The publisher does not give any warranty express or implied or make any representation that the contents will be complete or accurate or up to date  The accuracy of any instructions  formulae and drug doses should be independently verified with primary sources  The publisher shall not be liable for any loss  actions  claims  proceedings  demand or costs or damages whatsoever or howsoever caused arising directly or indirectly in connection with or arising out of the use of this material  Journal of Experimental  Theoretical Artificial Intelligence Vol  21  No </raw_string>
  </article>
  <article>
    <title>Formulating Representative Features with Respect to Genre Classification</title>
    <count>648</count>
    <raw_string>Chapter 6 Formulating Representative Features with Respect to Genre Classification Yunhyong Kim and Seamus Ross 61 Introduction Document classification is one of the most fundamental steps in enabling the search  selection  and ranking of digital material according to its relevance in answering a predefined search  As such it is a valuable means of knowledge discovery and an essential part of the effective and efficient management of digital documents in a repository  library  or archive  Document classification has previously been dom  inated by the classification of documents according to topic  Recently  however  there has been a growing interest in the classification of documents with respect to factors other than topic  e g  classification into forms of dissemination such as scientific papers  emails  blogs  and news reports   This type of classification has been labelled in many different ways  including the phrase genre classification  The vast number of different contexts in which genres have emerged across classification attempts illustrate that genre is a highlevel  contextdependent concept  cf  litera  ture review 24   Genre has been referred to as aspects of the text described by level of information or degree of elaboration  persuasion and abstraction  cf  5   as well as  to common document forms such as FAQ  Job Description  Editorial or Reportage  e g  9  14  16   In some cases  genre has been used to describe the classification of a document according to whether or not it is a narrative and the target level of audience  e g  16   and whether it is fact or opinion  and  in the </raw_string>
  </article>
  <article>
    <title>Author Detection by Using Different Term Weighting Schemes</title>
    <count>648</count>
    <raw_string>Farkl Terim Arlklandrma Yntemleri Kullanarak Yazar Tanma Author Detection by Using Different Term Weighting Schemes Pnar Tfekci  Erdin Uzun Bilgisayar Mhendislii Blm Namk Kemal niversitesi  ptufekci  erdincuzun   nkuedutr zeteBu almada terim arlklandrmasnn metin snflandrma trlerinden yazar tanma zerindeki etkisi incelenmitir  Metinleri temsil etmede kullanlan zellik vektr  zellik olarak kelime kklerinden ve bunlara ait 14 farkl terim arlklandrma ynteminin uygulanmas ile elde edilmi saysal arlk deerlerinden olumaktadr  Bu zellik vektrlerinin yazar tanmadaki performanslar  3 farkl veri kmesi iin  Naive Bayes Multinominal  NBM   Destek Vektr Makinesi  SVM  Karar Aac  C45   ve Rastgele Orman  RF  snflandrma yntemleri ile denenmi ve birbirleriyle kyaslanmtr  Bunun sonucunda  bir ke yazsnn yazarn en yksek doruluk baar oran ile tahmin edebilen  en baarl snflandrc 9875lik ortalama doruluk baar oran ile SVM snflandrcs  en baarl terim arlklandrma yntemi ise  9154lk genel ortalama doruluk baar oran ile ACTFIDFICF1  arlklandrmas olarak bulunmutur  Anahtar Kelimeler  yazar tanma  terim arlklandrma yntemleri  metin snflandrma  Abstract In this study  the impact of term weighting on author detection as a type of text classification is investigated  The feature vector being used to represent texts  consists of stem words as features and their weight values  which are obtained by applying 14 different term weighting schemes  The performances of these feature vectors for 3 different datasets in the author detection are tested with some classification methods such as Nave Bayes Multinominal  NBM   and Support Vector Machine  SVM  Decision Tree  C45   and Random Forrest  RF  and are compared with each other  As a result of that  the most successful classifier  </raw_string>
  </article>
  <article>
    <title>Using Latent Communication Styles to Predict Individual Characteristics</title>
    <count>648</count>
    <raw_string>Using Latent Communication Styles to Predict Individual Characteristics Jordan Bates  Jennifer Neville  and Jim Tyler Computer Science Department  Psychological Sciences Department Purdue University West Lafayette  IN USA jtbates  neville cs purdue edu ABSTRACT Data in online social network and social media systems pro vides a significant source of information about individual attitudes  preferences  and relationships  While there is a large body of work using statistical and machine learning techniques to predict the characteristics of text and users from documents  these efforts typically do not try to exploit the text to infer personality traits and understand interper sonal communications  However  some recent work in social psychology has focused on the aspects of writing and speech that are suggestive of personal characteristics  These ef  forts differ from much of the current work in text mining in that they focus on style  rather than contentby modeling usage patterns involving the most frequent words  i e   func  tion words such as the  is  a   In this work  we identify stylistic patterns in communication data through the use of latent semantic analysis on function words  We use the discovered topics in logistic regression models and show that style is more predictive than content for several classification tasks focusing on personal traits such as gender  political party affiliation  verbal aggressiveness  and sentiment  1  INTRODUCTION Over the last ten years  online social networks  OSNs  and social media have become an integral aspect of the so  cial fabric that today has farreaching influence on nearly all aspects of our lives  The data in these online systems pro vides a significant source of information about </raw_string>
  </article>
  <article>
    <title>Authorship Attribution and Gender Identification in Greek Blogs</title>
    <count>648</count>
    <raw_string>Authorship Attribution and Gender Identification in Greek Blogs George K Mikros Department of Italian Language and Literature  School of Philosophy  National and Kapodistrian University of Athens  Athens  Greece gmikrosisll uoagr Abstract  The aim of this study is to obtain authorship attribution and authors gender identification in a corpus of blogs written in Modern Greek language  More specifically  the corpus used contains 20 bloggers equally divided by gender  10 males  10 females  with 50 blog posts from each author  1000 posts in total and an overall size of 406460 words   From this corpus we calculated a number of standard stylometric variables  e g  word length statistics and various vocabulary richness  indices  and 300 most frequent word and character ngrams  character and word uni grams  bigrams  trigrams   Support Vector Machines  SVM  were trained on this data  and the authors gender prediction accuracy in 10fold crossvalidation experiment reached 826  accuracy  a result that is comparable to current stateoftheart author profiling systems  Authorship attribution accuracy reached 854   an equally satisfy ing result given the large number of candidate authors  n20   Keywords  Authorship Attribution  Author profiling  Blogs  Machine Learn  ing  Support Vector Machines  Gender Identification  Stylometry 1 Introduction Over the last two decades Automatic Authorship Identification  AAI  has been evolved in a highly dynamic research strand exploiting recent advances in a number of fields like Artificial Intelligence  Linguistics and Computing  Furthermore  AAI research now is concerned not only with problems of authorship in the broad field of the Humanities  Literature  History  Theology   but also </raw_string>
  </article>
  <article>
    <title>Effects of Web Document Evolution on Genre Classification</title>
    <count>648</count>
    <raw_string>Effects of Web Document Evolution on Genre Classification Elizabeth Sugar Boese and Adele E Howe Computer Science Dept  Colorado State University Fort Collins  CO 80523 USA http wwwcs colostate edu  boeseResearch sugaracm org ABSTRACT The World Wide Web is a massive corpus that constantly evolves  Classification experiments usually grab a snapshot temporally and spatially  of the Web for a corpus  In this paper  we examine the effects of page evolution on genre classification of Web pages  Web genre refers to the type of the page characterized by features such as style  form or presentation layout  and metacontent  Web genre can be used to tune spider crawling revisits and inform relevance judgments for search engines  We found that pages in some genres change rarely if at all and can be used in presentday research experiments without requiring an updated version  We show that an old corpus can be used for training when testing on new Web pages  with only a marginal drop in accuracy rates on genre classification  We also show that features found to be useful in one corpus do not transfer well to other corpora with different genres  Categories and Subject Descriptors H33  Information Search and Retrieval  Retrieval models General Terms Experimentation Keywords Genre  text classification  corpora 1  INTRODUCTION Web page genre is a relatively untapped source of search information  Standard topic queries can be enhanced by specifying page genre  Instructors may look for pages with good questions or tutorials for their students  Academics may want scholarly articles on some topic  Shoppers may want reviews about specific products  Permission to make digital or hard copies of all or part of this work for </raw_string>
  </article>
  <article>
    <title>Stopword Graphs and Authorship Attribution in Text Corpora</title>
    <count>648</count>
    <raw_string>Stopword Graphs and Authorship Attribution in Text Corpora R Arun  V Suresh  C E Veni Madhavan Department of Computer Science and Automation Indian Institute of Science Bangalore  INDIA Email  arun r  vsuresh  cevm csaiiscernet in Abstract In this work we identify interactions of stopwords noisewords  in text corpora as a fundamental feature to effect author classification  It is convenient to view such interactions as graphs wherein nodes are stopwords and the interaction between a pair of stopwords are represented as edgeweights  We define the interaction in terms of the distances between pairs of stopwords in text documents  Given a list of authors  graphs for each author is computed based on their undisputed writings  Authorship of a test document is attributed based on the closeness of the graph derived from it to the above graphs  Towards this  we define a closeness measure to compare such graphs based on the KullbackLeibler divergence  We illustrate the accuracy of our approach by applying it on examples drawn from the Gutenberg archives  Our results show that the proposed approach is effective not only in binary author classification but also performs multiclass author classification for as many as 10 authors at a time and compares favourably with the stateoftheart in author identification  Keywordsstylometry  writer invariant  authorship attribu tion  KL divergence I INTRODUCTION Authorship classification received public notice with the classic work by Mosteller and Wallace 14 that resolved the disputed Federalist Papers  http enwikipedia orgwikiFederalist Papers  for the first time through scientific means  In general  a situation for authorship attribution or resolution arises when there is uncertainty about a documents authorship and there is list of potential authors among whom one is </raw_string>
  </article>
  <article>
    <title>An MDA Approach to Implement Personal IR Tools</title>
    <count>648</count>
    <raw_string>An MDA Approach to Implement Personal IR Tools Sven Meyer zu Eissen Benno Stein Faculty of Media  Media Systems Bauhaus University Weimar  Germany  svenmeyerzueissen  benno steinmedien uniweimarde Abstract We introduce TIRA1  a software architecture for the rapid prototyping of tailored information retrieval  IR  tools  TIRA allows to compose personal IR tools from atomic IR services  following the model driven architecture  MDA  paradigm  In a first step  an IR process is defined indepen dently from platforms  by means of a UML activity diagram  In a second step  the activity diagram is transformed to a platform specific model  which is executed in a distributed environment  Major driving force behind our research is the question of personalization  We see a large gap between informa  tion retrieval theory and algorithms on the one hand and their implementation and deployment to satisfy a personal information need on the other  This gap can be closed with adequate software engineering means  and TIRA shall con tribute in this respect  1  Introduction Information retrieval  IR  is considered as key technol ogy to address the problem of information overload  which is caused by global information accessibility and the in creasing number of information creators  Note that infor mation retrieval is not a universal answer to a generic infor mation need problem but a collective term for myriad solu tions to individual information need problems  To become an effective means  retrieval technology must be adapted to personal information needs  which pertains among others to the following points  1  Personal Data  Document sources on which retrieval tasks are carried out include local hard drives  the Web </raw_string>
  </article>
  <article>
    <title>Character Level Authorship Attribution for Turkish Text Documents</title>
    <count>648</count>
    <raw_string>Character Level Authorship Attribution for Turkish Text Documents 1Hidayet Tak  2Ekin Ekinci 1Mhendislik Fakltesi  Bilgisayar Mhendislii  Cumhuriyet niversitesi  SivasTurkey 2Mhendislik Fakltesi  Bilgisayar Mhendislii  Kocaeli niversitesi  KocaeliTurkey htakcigmailcom  ekin ekincikocaeli edutr Abstract  Individuals have their own style of speaking and writing  Style of a text can be used as a distinctive feature to recognize its author  In recent years  practical applications for authorship attribution have grown in areas such as criminal law  civil law and computer security  Recent research has used techniques from machine learning  information retrieval and natural language processing in authorship attribution  In this paper  Statistical Language Modeling is utilized in Authorship Attribution  Each author is represented with feature statistics  Letters  punctuations and special characters which build up the feature set are utilized to calculate the profiles of the authors  Key words  Authorship Attribution  Character Level Method  Centroid Values  Centroid Vector  Document Vector Introduction The topic of the article is authorship attribution and this study aims to recognize authors of Turkish texts automatically  In addition  it can be utilized in different areas such as spam filtering  determining plagiarism cases  identifying author of program code and in forensic analysis  The output of this study will be classification of texts based on the authors  determining the authors with similar styles in writing and classification of authors depending on their styles  Similarity in the authors  styles is related to their cultural or geographical backgrounds  This situation makes us able to reach interesting information about the authors  In authorship attribution studies researches have experienced different features such as function words  content words  character n gram </raw_string>
  </article>
  <article>
    <title>A Framework for Performer Identification in Audio Recordings</title>
    <count>648</count>
    <raw_string>A Framework for Performer Identification in Audio Recordings Rafael Ramirez and Esteban Maestre Music Technology Group Pompeu Fabra University Tanger 38  Barcelona  Spain Tel 34 935421365  Fax 34 935422202 rafael ramirezupf edu  estebanmaestreupf edu Abstract  We present a general framework for the task of identifying performers from their playing styles  We investigate how musicians ex  press and communicate their view of the musical content in pieces and how to use this information in order to automatically identify perform  ers  We study notelevel deviations of parameters such as timing and amplitude  Our approach to performer identification consists of inducing an expressive performance model for each of the interpreters essentially establishing a performer dependent mapping of internote features to a timing and amplitude expressive transformations   We outline two suc  cessful performer identification case studies  1 Introduction Music performance plays a central role in our musical culture today  Concert attendance and recording sales often reflect peoples preferences for particular performers  The manipulation of sound properties such as pitch  timing  ampli tude and timbre by different performers is clearly distinguishable by the listeners  Expressive music performance studies the manipulation of these sound properties in an attempt to understand expression in performances  There has been much speculation as to why performances contain expression  Hypothesis include that musical expression communicates emotions and that it clarifies musical struc  ture  i e  the performer shapes the music according to her own intentions  In this paper we describe a general framework for the task of identifying performers from their playing style using highlevel descriptors extracted from singleinstrument audio recordings  The identification of performers by using the expressive content in their performances raises particularly </raw_string>
  </article>
  <article>
    <title>Automatically Determining Phishing Campaigns Using the USCAP Methodology</title>
    <count>647</count>
    <raw_string>Automatically Determining Phishing Campaigns using the USCAP Methodology Robert Layton Internet Commerce Security Laboratory University of Ballarat Email  rlaytonicslballarateduau Paul Watters Internet Commerce Security Laboratory University of Ballarat Email  pwattersicslballarateduau Richard Dazeley Data Mining and Informatics Research Group University of Ballarat Email  rdazeleyballarateduau AbstractPhishing fraudsters attempt to create an environ  ment which looks and feels like a legitimate institution  while at the same time attempting to bypass filters and suspicions of their targets  This is a difficult compromise for the phishers and presents a weakness in the process of conducting this fraud  In this research  a methodology is presented that looks at the differ ences that occur between phishing websites from an authorship analysis perspective and is able to determine different phishing campaigns undertaken by phishing groups  The methodology is named USCAP  for Unsupervised SCAP  which builds on the SCAP methodology from supervised authorship and extends it for unsupervised learning problems  The phishing website source code is examined to generate a model that gives the size and scope of each of the recognized phishing campaigns  The USCAP methodology introduces the first time that phishing websites have been clustered by campaign in an automatic and reliable way  compared to previous methods which relied on costly expert analysis of phishing websites  Evaluation of these clusters indicates that each cluster is strongly consistent with a high stability and reliability when analyzed using new information about the attacks  such as the dates that the attack occurred on  The clusters found are indicative of different phishing campaigns  presenting a step towards an automated phishing authorship analysis methodology  I INTRODUCTION Phishing presents a major problem for all involved in this scam  An attacker  the phisher </raw_string>
  </article>
  <article>
    <title>Modeling Expressive Musical Performance with Hidden Markov Models</title>
    <count>646</count>
    <raw_string>UNIVERSITY OF CALIFORNIA SANTA CRUZ MODELING EXPRESSIVE MUSICAL PERFORMANCE WITH HIDDEN MARKOV MODELS A dissertation submitted in partial satisfaction of the requirements for the degree of Master of Science in COMPUTER SCIENCE by Graham Charles Grindlay March 2005 The Dissertation of Graham Charles Grindlay is approved  Professor David Helmbold  Chair Professor David Cope Professor Charles McDowell Robert C Miller Vice Chancellor for Research and Dean of Graduate Studies c 2005 Graham Charles Grindlay Contents List of Figures iv Abstract v 1 Introduction 1 2 Related Work 4 3 The ESP System 7 31 Musical Features                                     8 311 Melodic Features                                 9 312 Accompaniment Features                            11 32 Representing the Data                                  14 321 Discretevalued Features                             15 322 Continuousvalued Features and the Kernel Technique            16 33 Training         </raw_string>
  </article>
  <article>
    <title>Reutilizacion de Codigo Fuente entre Lenguajes de Programacion</title>
    <count>644</count>
    <raw_string>Detecting source code reuse across programming languages Deteccion de reutilizacion de codigo fuente entre lenguajes de programacion Enrique Flores  Alberto BarronCedeno  Paolo Rosso and Lidia Moreno Depto  Sistemas Informaticos y Computacion Universidad Politecnica de Valencia eflores  lbarron  prosso  lmorenodsicupves Resumen  Con el crecimiento de la Web muchas comunidades de programadores ponen a disposicion publica codigos fuente bajo licencias que protegen la propiedad de sus creaciones  Es una gran tentacion reutilizar un codigo disponible en la Web que funciona y ya esta testeado  en el ambito academico se observa mas esta situacion  donde un grupo de alumnos tienen asignada la misma tarea  Un progra  mador puede obtener un codigo en un lenguaje de programacion en el que no esta trabajando y traducirlo a otro lenguaje  En este trabajo se proponen dos modelos basados en ngramas de caracteres para detectar la similitud y posible reutilizacion de codigo fuente  incluso tratandose entre codigos escritos en distintos lenguajes de programacion  Ambos modelos abordan el problema de reutilizacion  uno trabaja a nivel de documento  y el segundo trabaja comparando fragmentos de codigo con el fin de detectar solo partes del codigo  el segundo representa mejor las situaciones reales de reutilizacion  Palabras clave  Reutilizacion de codigo  analisis de codigo fuente entre lenguajes  deteccion de plagio Abstract  With the growth of the Web many programmer communities make pub  licly available source codes under licences that protect their property  For a program  mer the temptation of reusing a working source code  available on the Web already tested  could be great  As well this kind of temptation exists in the academic envi ronment where a group of students is assigned </raw_string>
  </article>
  <article>
    <title>Interactive Degraded Document Enhancement and Ground Truth Generation</title>
    <count>644</count>
    <raw_string>Interactive degraded document enhancement and ground truth generation G Bala  G Agama  O Friedera  G Friederb aIllinois Institute of Technology  Chicago  IL 60616 bThe George Washington University  Washington  DC 20052 ABSTRACT Degraded documents are frequently obtained in various situations  Examples of degraded document collections include historical document depositories  document obtained in legal and security investigations  and legal and medical archives  Degraded document images are hard to to read and are hard to analyze using computerized techniques  There is hence a need for systems that are capable of enhancing such images  We describe a language  independent semiautomated system for enhancing degraded document images that is capable of exploiting inter and intradocument coherence  The system is capable of processing document images with high levels of degradations and can be used for ground truthing of degraded document images  Ground truthing of degraded document images is extremely important in several aspects  it enables quantitative performance measurements of enhancement systems and facilitates model estimation that can be used to improve performance  Performance evaluation is provided using the historical Frieder diaries collection 1 Keywords  degraded documents  image enhancement  historical documents  document image analysis  docu ment degradation models  image analysis 1  INTRODUCTION Degraded documents are archived and preserved in large quantities worldwide  Electronic scanning is a common approach in handling such documents in a manner which facilitates public access to them  Such document images are often hard to read  have low contrast  and are corrupted by various artifacts  Thus  given an image of a faded  washed out  damaged  crumpled or otherwise difficult to read document  one with mixed handwriting  typed or </raw_string>
  </article>
  <article>
    <title>Sincere and Deceptive Statements in Italian Criminal Proceedings</title>
    <count>644</count>
    <raw_string>126 Sincere and deceptive statements in Italian criminal proceedings Tommsso Fornaciari 1 and Massimo Poesio 2 University of Trento  Italy 1 tommasofornaciariunitn it 2 massimopoesiounitn it Abstract Identifying false or deceptive statements in testimonies is a difficult challenge in criminal proceedings because it is not a task humans find easy  Text classification techniques have shown promise at this taskbut so far  they have mainly been tested with laboratory produced data rather than authentic  real life data  We collected what is the first Italian corpus of hearings from criminal proceedings in which the defendant was found guilty of false testimony  In such cases  the transcriptions of each hearing report the words exactly as told by the subjects  and the judgment points out the statements found by the Court to be false  This characteristic makes it possible to annotate sincerity and deception of statements in such data on the basis of unusually solid objective information  We used these data to train models to classify statements as sincere or deceptive  showing that in spite of the difficulty humans have at this classification task  it is possible to obtain a performance well above chance level from automatic classifiers using very simple surface linguistic features  Keywords  FORENSIC LINGUISTICS  DECEPTION DETECTION  TESTIMONY IN COURT  TEXT CLASSIFICATION 1  Introduction 11  Detecting deception Identifying deceptive statements in testimonies could provide very useful support to investigative work  particularly when other kinds of evidence are scarce or absent  In spite of this  modern studies demonstrate that human performance in recognizing deception is not much better than chance  Bond and De Paulo  2006   Furthermore  in some studies human skills seem to be not particularly </raw_string>
  </article>
  <article>
    <title>Stochastic Language Models for Music Information Retrieval</title>
    <count>634</count>
    <raw_string>PhD thesis Stochastic Language Models for Music Information Retrieval Carlos Perez Sancho Supervised by Jose  M Inesta Jorge Calera Rubio June 2009 A Vanessa y Javier Agradecimientos Quiero agradecer a todos los que han contribuido  directa o indirecta  mente  a que esta tesis haya sido posible  En primer lugar  quiero dar las gracias a mis dos directores  A Jose  Manuel Inesta  por haberme brindado la oportunidad de investigar en un area apasionante  la fusion de la informatica y la musica  y por orientarme en todo momento  y a Jorge Calera  por resolver todas mis dudas y sacarme de mas de un atasco  A los dos  en general  por todo el apoyo  el esfuerzo y el tiempo que me han dedicado  Quiero agradecer tambien a todo el Departamento de Lenguajes y Sistemas Informaticos de la Universidad de Alicante por haberme propor cionado un ambiente de trabajo envidiable  En especial a mis companeros del Grupo de Reconocimiento de Formas e Inteligencia Artificial  que han estado siempre dispuestos a ayudarme  y a Mikel L Forcada por haber confiado en m  desde el principo  Gracias a Antonio  Pierre  David y Placido  que han colaborado activamente en este trabajo y de los que he aprendido mucho en todo este tiempo  y a Rafael Ramrez y Stefan Kersten del Music Technology Group de la Universitat Pompeu Fabra  por su colaboracion con el sistema de transcripcion de acordes  Gracias tambien a mis companeros David y Felipe  que han compartido este camino conmigo  siempre con un gran sentido del humor  Felicidades David por haber emprendido ese otro camino  Quiero dar las gracias  por ultimo </raw_string>
  </article>
  <article>
    <title>Effective Authorship Attribution in Large Document Collections</title>
    <count>614</count>
    <raw_string>Effective Authorship Attribution in Large Document Collections A thesis submitted for the degree of Doctor of Philosophy Ying Zhao  BCompSci School of Computer Science and Information Technology  Science  Engineering  and Technology Portfolio  RMIT University  Melbourne  Victoria  Australia  December 20  2007 iii Declaration I certify that except where due acknowledgement has been made  the work is that of the author alone  the work has not been submitted previously  in whole or in part  to qualify for any other academic award  the content of the thesis is the result of work which has been carried out since the official commencement date of the approved research program  and  any editorial work  paid or unpaid  carried out by a third party is acknowledged  Ying Zhao School of Computer Science and Information Technology RMIT University December 20  2007 iv Acknowledgments I would like to thank many people who gave me support to complete this thesis  Without their support this thesis would not appear in its present form  Any words would be too plain to express my deep and sincere gratitude to my amazing primary supervisor  Prof Justin Zobel  He taught me how to think critically  how to enjoy research  and how to be a good researcher  Without his enthusiasm  his inspiration  his encouragement  and his great efforts to guide me throughout my study  I would never have finished  He taught me to be positive and brave in a hard life  without his support and encouragement  I would have quit my studies when my family crisis happened  He is a great supervisor  Besides  I would like to thank </raw_string>
  </article>
  <article>
    <title>The Genre Structure of Bulletin Board Messages</title>
    <count>614</count>
    <raw_string>Number 2  2004 TEXT Technology 55 The Genre Structure of Bulletin Board Messages Maite Taboada Department of Linguistics  Simon Fraser University mtaboadasfuca Abstract Messages posted on bulletin boards on the Internet represent a new form of communication  They draw both from casual conversation and from other written genres in their structure and characteristics  In this paper I explore the genre structure of bulletin board posts  Genre is defined a purposeful  goaloriented activity  which evolves in clearly defined stages  Messages posted on bulletin boards correspond to a variety of genres  argumentation for an opinion  personal attacks on other posters  pros and cons of a certain product  demands for advice  etc  Because the goals of each genre are different  the genres will also be different in structure  This paper provides definitions of four of these genres and an exploration of their structural characteristics  The descriptions are useful in computa  tional applications such as topic classification and information retrieval  KEYWORDS  Bulletin boards  linguistics  genre  systemic functional lin guistics  rhetorical structure theory  text classification 1  Introduction Messages posted on bulletin boards can be classified into different types  or genres  This classification can be done on an intuitive basis  by examin ing the overall purpose of the message  and certain expectations of what it is one achieves by posting a message  providing information  asking for information  arguing for a certain point of view  etc   Ideally  this intuitive classification should be automated for search and retrieval purposes  The problem is how to formalize the classification of individual messages based on their characteristics  Research in computational lin guistics </raw_string>
  </article>
  <article>
    <title>Prosodic Boundary Prediction for Greek Speech Synthesis</title>
    <count>614</count>
    <raw_string>Journal of Computer Sciences and Applications  2013  Vol  1  No 4  6174 Available online at http pubs sciepubcomjcsa142  Science and Education Publishing DOI1012691jcsa142 Prosodic Boundary Prediction for Greek Speech Synthesis Panagiotis Zervas  Department of Music Technology  Acoustics  Technological Educational Institute of Crete  Rethymnon Branch  Greece Corresponding author  pzervasstaffteicrete gr Received December 30  2012  Revised May 18  2013  Accepted May 19  2013 Abstract In this article  we evaluate features and algorithms for the task of prosodic boundary prediction for Greek  For this purpose a prosodic corpus composed of generic domain text was constructed  Feature contribution was evaluated and ranked with the application of information gain ranking and correlation based feature selection filtering methods  Resulted datasets were applied to C45 decision tree  oneneighbour instance based learner and Bayesian learning methods  Models performance exploitation led as to the construction of a practically optimal feature set whose prediction effectiveness was evaluated with two prosodic databases  In terms of total accuracy and Fmeasure  evaluation results established the decision tree effectiveness in learning rules for prosodic boundary prediction  Keywords  prosody  phrase breaks  ToBI  C45  IB1  bayesian learning 1  Introduction A text tospeech  TtS  system is considered as a framework able to perform the conversion of text to synthetic speech  In this undertaking  several steps are carried out between the input informat ion text  and the output  synthetic speech   Macroscopically a TtS is composed of two major parts  the frontend and the back  end  Frontend accepts raw text as input and generates a symbolic representation of prosody that will be utilized for the </raw_string>
  </article>
  <article>
    <title>Technologies for Reusing Text from the Web</title>
    <count>614</count>
    <raw_string>Technologies for Reusing Text from the Web From the Faculty of Media of the BauhausUniversitt Weimar Germany The Accepted Dissertation of Martin Potthast To Obtain the Academic Degree of Dr rer  nat  Advisor  Prof Dr Benno M Stein Reviewer  Paul D Clough  PhD Oral Exam  December 16  2011 Fr meine Vter Contents Preface v 1 Introduction 1 11 Related Research and Technologies            2 12 Contributions of this Thesis                8 13 A Brief Introduction to Information Retrieval      14 I Text Reuse 24 2 Detecting Nearduplicate Text Reuse 25 21 Nearduplicate Detection Based on Fingerprinting   26 22 Fingerprint Construction                  28 23 Evaluating Fingerprint Algorithms            36 3 Detecting CrossLanguage Text Reuse 43 31 Differences to Monolingual Text Reuse Detection    44 32 Measuring CrossLanguage Text Similarity       47 33 Evaluating CrossLanguage Text Similarity Models  54 4 Evaluating Plagiarism Detectors 68 41 Detection Performance Measures             71 42 An Evaluation Corpus for Plagiarism Detectors     75 43 Three Evaluation Competitions              87 iii II Language Reuse 105 5 Web Comments for Multimedia Retrieval 106 51 A Survey of Commentrelated Research         107 52 Filtering  Ranking  and Summarizing Comments    117 53 </raw_string>
  </article>
  <article>
    <title>Automatic Identification of Genre in Web Pages</title>
    <count>614</count>
    <raw_string>Automatic Identification of Genre in Web Pages Marina Santini A thesis submitted in partial fulfilment of the requirements of the University of Brighton for the Degree of Doctor of Philosophy January 2007 University of Brighton  UK ii Abstract The aim of this thesis is to understand how genre is instantiated on the web  and thereby to develop automatic methods for genre identification in web pages  The main challenges arise from the interaction of three factors   1  the complexity of web pages   2  the fluidity and the fastpaced evolution of the web  and  3  the limitation of automaticallyextractable features for genre detection  First  genres on the web are instantiated in web pages  which  from a physical  linguistic and textual point of view  can be considered documents of a new type  much more unpredictable and individualised than documents on paper  Second  the web is unstable and fluid  undergoing a fastpaced evolution  so genre identification is influenced by phenomena such as the formation of novel genres  genre hybridism  individualisation  and intra  genre and intergenre variation  Finally  automaticallyextractable features represent a poor surrogate for potentially useful genrerevealing features  These three factors strongly affect the automatic identification of genre in web pages  Previous work has disregarded them for the sake of practicality  and built on the oversimplifying assumption that a web page is to be assigned to only one genre  relying as little as possible on the linguistic features returned by NLP tools  By contrast  this thesis argues for the necessity of a more flexible genre classification scheme  capable of assigning zero  one or multiple genre labels  </raw_string>
  </article>
  <article>
    <title>Topic Learning in Text and Conversational Speech</title>
    <count>614</count>
    <raw_string>Topic Learning in Text and Conversational Speech Constantinos Boulis A dissertation submitted in partial fulfillment of the requirements for the degree of Doctor of Philosophy University of Washington 2005 Program Authorized to Offer Degree  Electrical Engineering University of Washington Graduate School This is to certify that I have examined this copy of a doctoral dissertation by Constantinos Boulis and have found that it is complete and satisfactory in all respects  and that any and all revisions required by the final examining committee have been made  Chair of Supervisory Committee  Mari Ostendorf Reading Committee  Mari Ostendorf Eve A Riskin Joshua Goodman Date  In presenting this dissertation in partial fulfillment of the requirements for the doctoral degree at the University of Washington  I agree that the Library shall make its copies freely available for inspection  I further agree that extensive copying of this dissertation is allowable only for scholarly purposes  consistent with fair use  as prescribed in the US Copyright Law  Requests for copying or reproduction of this dissertation may be referred to Proquest Information and Learning  300 North Zeeb Road  Ann Arbor  MI 481061346  to whom the author has granted the right to reproduce and sell  a  copies of the manuscript in microform andor  b  printed copies of the manuscript made from microform   Signature Date University of Washington Abstract Topic Learning in Text and Conversational Speech Constantinos Boulis Chair of Supervisory Committee  Professor Mari Ostendorf Electrical Engineering Extracting topics from large collections of data is a crucial step to enhance information access  There has been an abundance of work on supervised topic learning methods on text  yet there are a number of directions in topic learning  </raw_string>
  </article>
  <article>
    <title>The Expert Linguist Meets the Adversarial System</title>
    <count>612</count>
    <raw_string>Electronic copy available at  http ssrncomabstract1522196 Brooklyn Law School Legal Studies Research Papers Accepted Paper Series Research Paper No 178 December 2009 The Expert Linguist Meets the Adversarial System Lawrence M Solan This paper can be downloaded without charge from the Social Science Research Network Electronic Paper Collection  http ssrncomabstract1522196 Electronic copy available at  http ssrncomabstract1522196 1 To appear in The Routledge Handbook of Forensic Linguistics  Malcolm Coulthard and Alison Johnson  eds   forthcoming 2010  The Expert Linguist Meets the Adversarial System Lawrence M Solan SCIENCE IN THE ADVERSARIAL SYSTEM The academic world and the world of litigation produce an awkward mix  Lawyers are in the business of winning their cases  Academics are in the business of engaging in disinterested research in an effort to uncover truths  Academics  including those who work in the hard sciences   are accustomed to such tasks as evaluating competing theories  each of which has its own strengths and weaknesses  Criteria of evaluation generally include both descriptive and explanatory adequacy and sometimes such things as Occams Razor and other measures of parsimony and elegance  In linguistic theory  for example  competing syntactic accounts are frequently judged on the breadth of the phenomena they are able to explain without resort to ad hoc solutions  The more elegant solution that covers more ground wins  In this realm  uncertainty is the norm  Those engaged in scientific inquiry do not close up shop once they have achieved some progress  Rather  they continue their explorations  often revising  and sometimes even discarding  earlier hypotheses as new data and new explanations come to light  The legal system also is designed to uncover truths  But  </raw_string>
  </article>
  <article>
    <title>Degraded Character Recognition by Image Quality Evaluation</title>
    <count>609</count>
    <raw_string>Degraded Character Recognition by Image Quality Evaluation Chunmei Liu Department of Computer Science and Technology  Tongji University Shanghai  China Email  chunmei liutongji educn AbstractThe character image quality plays an important role in degraded character recognition which could tell the recognition difficulty  This paper proposed a novel approach to degraded character recognition by three kinds of independent degradation sources  It is composed of two stems  character image quality evaluation  character recognition  Firstly  it presents the dualevaluation to evaluate the image quality of the input character  Secondly  according to the input evaluation result  the character recognition subsystems adaptively act on  These subsystems are trained by character sets whose image qualities are similar to the inputs quality  and have special features and special classifiers respectively  Experiment results demonstrate the proposed approach highly improved the performance of degraded character recognition system  Keywords  character recognition  degraded character recognition  image quality evaluation I INTRODUCTION With rapid progress of digital imaging technology and variety of image acquisition conditions  character recognition becomes increasingly important  The character recognition for lowquality becomes a bottleneck in OCR field  Images having luminance variations  noise  and random degradation of text are difficult to read by OCR systems because traditional methods can not provide satisfactory recognition performance  Most previous degraded character recognition methods mainly concentrated on image enhancement  For scanned images of old manuscripts  AGupta et al  7  presented an algorithm based on matched wavelets and MRF model to automatically identify and extract the low contrast text regions from scanned manuscript images and enhance them using a histograms matching technique  EKavallieratou et al  8  proposed a hybrid binarization approach for improving the </raw_string>
  </article>
  <article>
    <title>Exploiting Global Constraints for Search and Propagation</title>
    <count>609</count>
    <raw_string>UNIVERSITE DE MONTREAL EXPLOITING GLOBAL CONSTRAINTS FOR SEARCH AND PROPAGATION ALESSANDRO ZANARINI DEPARTEMENT DE GENIE INFORMATIQUE ET GENIE LOGICIEL ECOLE POLYTECHNIQUE DE MONTREAL THESE PRESENTEE EN VUE DE LOBTENTION DU DIPLOME DE PHILOSOPHI DOCTOR  GENIE INFORMATIQUE AVRIL 2010 c Alessandro Zanarini  2010  UNIVERSITE DE MONTREAL ECOLE POLYTECHNIQUE DE MONTREAL Cette these intitulee  EXPLOITING GLOBAL CONSTRAINTS FOR SEARCH AND PROPAGATION presentee par  ZANARINI Alessandro en vue de lobtention du diplome de  Philosophi Doctor a ete dument acceptee par le jury constitue  de  M GALINIER  Philippe  Doct  president  M PESANT  Gilles  Ph D  membre et directeur de recherche  M ROUSSEAU  LouisMartin  Ph D  membre  M VAN BEEK  Peter  Ph D  membre externe  iii To my family iv Abstract This thesis focuses on Constraint Programming  CP  that is an emergent paradigm to solve complex combinatorial optimization problems  The main contributions revolve around constraint filtering and search that are two main components of CP On one side  constraint filtering allows to reduce the size of the search space  on the other  search defines how this space will be explored  Advances on these topics are crucial to broaden the applicability of CP to reallife problems  For what concerns constraint filtering  the contribution is twofold  we firstly propose an improvement on an existing algorithm of the relaxed version of a constraint that frequently appears in assignment problems  soft gcc   The algorithm proposed outperforms the previ ously known in terms of timecomplexity both for the consistency check and for the filtering and in term of ease of implementiation  Secondly  we introduce a new constraint  both hard and </raw_string>
  </article>
  <article>
    <title>A Deep Graphical Model for Spelling Correction</title>
    <count>609</count>
    <raw_string>A Deep Graphical Model for Spelling Correction Stephan Raaijmakers a a TEXAR Data Science  Vijzelgracht 53C 1017 HP Amsterdam Abstract We propose a deep graphical model for the correction of isolated spelling errors in text  The model is a deep autoencoder consisting of a stack of Restricted Boltzmann Machines  and learns to associate error  free strings represented as bags of character ngrams with bit strings  These bit strings can be used to find nearest neighbor matches of spelling errors with correct words  This is a novel application of a deep learning semantic hashing technique originally proposed for document retrieval  We demonstrate the effectiveness of our approach for two corpora of spelling errors  and propose a scalable correction procedure based on small sublexicons  1 Introduction In this paper  we investigate the use of deep graphical models for the purpose of contextfree spelling correc tion in text  spelling correction in isolated words  Spelling correction is a wellstudied subject that has been on the agenda of the NLP community for decades  3   Nowadays  new applications arise in the contexts of big data  search engines and user profiling  For instance  in product data cleansing  4  19   where large quantities of manually entered product data need to be standardized  accurate detection and handling of mis  spellings is important  Search engines like Google proactively perform spelling correction on user queries  5  15  17  25   Stylometric user profiling in forensic situations may benefit from accurate estimates of the amount and type of spelling errors a person makes  eg  for authorship verification  e g  13  23   The approach we </raw_string>
  </article>
  <article>
    <title>The Expert Linguist Meets the Adversarial System</title>
    <count>602</count>
    <raw_string>26 The forensic linguist The expert linguist meets the adversarial system Lawrence M Solan Science in the adversarial system The academic world and the world of litigation produce an awkward mix  Lawyers are in the business of winning their cases  Academics are in the business of engaging in dis  interested research in an effort to uncover truths  Academics  including those who work in the hard sciences   are accustomed to such tasks as evaluating competing theories  each of which has its own strengths and weaknesses  Criteria of evaluation generally include both descriptive and explanatory adequacy and sometimes such things as Occams Razor and other measures of parsimony and elegance  In linguistic theory  for example  competing syntactic accounts are frequently judged on the breadth of the phenomena they are able to explain without resort to ad hoc solutions  The more elegant solution that covers more ground wins  In this realm  uncertainty is the norm  Those engaged in scientific inquiry do not close up shop once they have achieved some progress  Rather  they continue their explorations  often revising  and sometimes even discarding  earlier hypotheses as new data and new explanations come to light  The legal system is also designed to uncover truths  But  in places that employ an adversarial system  it does not do so by conducting disinterested research  but rather through the vigorous presentation of evidence slanted toward different positions  The assumptionmore a matter of faithis that the better sets of facts  arguments and the ories presented in the court room will rise to the top  and that thereby the quest for truth will be served  see Landsman 1984   For </raw_string>
  </article>
  <article>
    <title>Extracting Social Power Relationships from Natural Language</title>
    <count>578</count>
    <raw_string>Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics  pages 773782  Portland  Oregon  June 1924  2011  c2011 Association for Computational Linguistics Extracting Social Power Relationships from Natural Language Philip Bramsen Louisville  KY bramsenalummit edu Ami Patel Massachusetts Institute of Technology Cambridge  MA ampatelmit edu Martha EscobarMolano San Diego  CA mescobarasgardcom Rafael Alonso SET Corporation Arlington  VA ralonsosetcorpcom Abstract Sociolinguists have long argued that social context influences language use in all manner of ways  resulting in lects 1  This paper ex  plores a text classification problem we will call lect modeling  an example of what has been termed computational sociolinguistics  In particular  we use machine learning techniques to identify social power relationships between members of a social network  based purely on the content of their interpersonal communica  tion  We rely on statistical methods  as op  posed to languagespecific engineering  to extract features which represent vocabulary and grammar usage indicative of social power lect  We then apply support vector machines to model the social power lects representing su periorsubordinate communication in the En ron email corpus  Our results validate the treatment of lect modeling as a text classifica  tion problem  albeit a hard one  and consti tute a case for future research in computational sociolinguistics  1 Introduction Linguists in sociolinguistics  pragmatics and re  lated fields have analyzed the influence of social context on language and have catalogued countless phenomena that are influenced by it  confirming many with qualitative and quantitative studies  In  This work was done while these authors were at SET Corpo ration  an SAIC Company  1 Fields that deal with society and </raw_string>
  </article>
  <article>
    <title>Natural Speaking and How to Assess it</title>
    <count>578</count>
    <raw_string>TRAMES  2010  146459   2  120140 NATURAL SPEAKING AND HOW TO ASSESS IT Hille Pajupuu1  Krista Kerge2  Lya Meister3  Eva Liina Asu4  and Pilvi Alp5 1Institute of the Estonian Language  Tallinn  2Tallinn University  3Institute of Cybernetics at Tallinn University of Technology  4University of Tartu  and 5National Examinations and Qualifications Centre  Tallinn Abstract  One of the problems in testing the proficiency of Estonian as a first or second language is that highstake exams are assessed against the standards of the written language  Given this  we set out to describe the features of the actual use of educated language in different types of text  The goal was to develop L1 and L2 teaching and testing through models of educated language use which a language learner can approach step by step  To achieve this goal we compared the following features of educated use of Estonian as L1 and L2 in different situations   1  lexical richness and vocabulary range   2  con textuality and formality of the text   3  syntactic complicacy   4  temporal characteristics of the dialogue   5  strength and disruptiveness of the foreign accent   6  sentence intonation  The results show that educated language use is mainly genredependent  This moves the focus of language learning onto texts of specific genres and confirms the suitability of an actionbased approach centred on genres in L1 and L2 teaching and testing  and the need for regular assessor training  Keywords  L1  L2  language teaching and learning  language testing  natural language use  genres  vocabulary  formality  accent  intonation  syntactic </raw_string>
  </article>
  <article>
    <title>Authorship Identification with Modality Specific Meta Features</title>
    <count>576</count>
    <raw_string>Authorship Identification with Modality Specific Meta Features Notebook for PAN at CLEF 2011 Thamar Solorio  Sangita Pillay  and Manuel MontesyGmez  University of Alabama at Birmingham  USA   National Institute of Astrophysics  Optics and Electronics  Mexico soloriorsangita mmontesg cis uabedu Abstract This paper presents the approach used in the PAN 11 authorship iden tification competition  Our method extracts meta features from several indepen dently generated clustering solutions from the training set  Each clustering so  lution uses a disjoint set of features that represent a specific linguistic modality  The different clustering solutions encode similarities in writing styles of authors across specific dimensions  The final classifier is trained with a combination of the meta features with first level features  Our approach has outperformed a more syntactic oriented stateoftheart method on web forum data  We achieved mod  erately successful results on this PAN competition  with better results on the test set with a smaller number of authors  However  considering that our system was not fine tuned for the PAN evaluation data we found our results very encouraging  1 Introduction Authorship Identification  AI  assumes the existence of individualized and identifiable writing styles  The success of previous work empirically supports this assumption  at least on the collections that have been used to test these approaches  However  when looking at writing samples from different authors it is clear that similarities exist among them  For instance  when analyzing web forum data we can see that several authors share emoticon patterns  even the absence of emoticons is in itself a pattern shared by many authors  Similarly  other authors use punctuation marks in similar ways  some like to use </raw_string>
  </article>
  <article>
    <title>Binary Cybergenre Classification Using Theoretic Feature Measures</title>
    <count>575</count>
    <raw_string>Binary Cybergenre Classification Using Theoretic Feature Measures Lei Dong  Carolyn Watters  Jack Duffy  Michael Shepherd Dalhousie University  Halifax  Nova Scotia  Canada ldong  watters  shepherdcs dalca  jack duffydalca  Abstract In this study  we conducted an investigation on automatic genre classification for three common types of web pages addressing the effect of three theoretic feature selection measures  a range of feature set size  and three machine classifiers on the accuracy of the web page classification in the context of a set of controlled experiments  Our results are encouraging and we conclude that for binary classification tasks  at least for these web page genres  it is possible to reach satisfying results with small contentbased feature sets generated with a sound feature selection measure and furthermore there is no evidence of interaction between these feature selection measures and the machine classifiers used  1  Introduction The issue of relevance has become more critical as search engines retrieve too many documents and most users are now only check out the top 20 retrieved pages  Furthermore  the traditional ranking of search results is not particularly useful to users of the Web The use of classification within the retrieved results of search engine is one approach to this problem  Genre is a variation of classification that is based on shared understandings of the communicative purpose that has been used for a long time to classify discourse and written communication  Genre may be used to help the user by identifying types of pages within the retrieved documents  which reflect to some extent the intend of the author  In a recent study Meyer and Stein 10 found that 64  of the students in the study </raw_string>
  </article>
  <article>
    <title>Note Onset Deviations as Musical Piece Signatures</title>
    <count>574</count>
    <raw_string>Note Onset Deviations as Musical Piece Signatures Joan Serra  Tan Hakan Ozaslan  Josep Lluis Arcos IIIACSIC  Artificial Intelligence Research Institute  Spanish National Research Council  Bellaterra  Barcelona  Spain Abstract A competent interpretation of a musical composition presents several nonexplicit departures from the written score  Timing variations are perhaps the most important ones  they are fundamental for expressive performance and a key ingredient for conferring a humanlike quality to machinebased music renditions  However  the nature of such variations is still an open research question  with diverse theories that indicate a multidimensional phenomenon  In the present study  we consider eventshift timing variations and show that sequences of note onset deviations are robust and reliable predictors of the musical piece being played  irrespective of the performer  In fact  our results suggest that only a few consecutive onset deviations are already enough to identify a musical composition with statistically significant accuracy  We consider a midsize collection of commercial recordings of classical guitar pieces and follow a quantitative approach based on the combination of standard statistical tools and machine learning techniques with the semiautomatic estimation of onset deviations  Besides the reported results  we believe that the considered materials and the methodology followed widen the testing ground for studying musical timing and could open new perspectives in related research fields  Citation  Serra J  Ozaslan TH  Arcos JL  2013  Note Onset Deviations as Musical Piece Signatures  PLoS ONE 87   e69268  doi101371journal pone 0069268 Editor  Derek Abbott  University of Adelaide  Australia Received March 19  2013  Accepted June 6  2013  Published July 31  2013 Copyright  2013 Serra et al  This </raw_string>
  </article>
  <article>
    <title>Authorship Attribution and Optical Character Recognition Errors</title>
    <count>574</count>
    <raw_string>Authorship Attribution and Optical Character Recognition Errors Patrick JuolaJohn INoecker JrMichael VRyanEvaluating Variations in Language Laboratory Duquesne UniversityPittsburghPennsylvania USA Juola AssociatesPittsburghPennsylvania USA juolamathcs duqedupjuolajuolaassociatescom jnoeckerjuolaassociatescommryanjuolaassociatescom ABSTRACTStylometric authorship attribution is a fundamental problemThe basic idea behind the research is that one can determine the authorship of a document on the basis of cognitive and linguistic quirks that uniquely identify a personIn many caseshowevernoise in the original documents can make this analysis more difficult and less reliableWe investigate the errors introduced by a typical optical character recognition  OCRprocessUsing simulated  randomerrors in a standard benchmark corpuswe test to see how sensitive the authorship attribution process is to character misrecognitionOur results indicate thatwhile accuracy decreases measurably with noisethe decrease is not substantialRESUMELe probleme de lattribution stylometrique dauteur est un probleme fondamentalLidee fondamentale derriere cette recherche est que lon peut determiner la paternitedun document sur la base dun ensemble de trait cognitifs et linguistiques qui permettent didentifier de maniere unique le style decriture dune personneDans de nombreux cascependantle bruit present dans les documents originaux peut rendre cette analyse plus difficile et moins fiableNous etudions les erreurs introduites par un processus typique de reconnaissance optique de caracteres  OCREn utilisant des erreurs simulees  aleatoirementdans un corpus de reference standardnous evaluons la sensibiliteau bruit du processus dattribution dauteurNos resultats indiquent quebien que la precision diminue avec un niveau de bruitcette baisse nest pas substantielleKEYWORDSauthorshipstylometryoptical character recognitionOCR errors MOTSCLES paternitedes documentsstylometriereconnaissante optique de caractedeserreurs dOCR TALVolume 53 no 32012pages 101 a127 102 TALVolume 53 no 32012 1Introduction Authorship attributionthe problem of inferring authorship from the writing style of a documentis an important problem not just in computer sciencebut also in educationjournalismhistoryand lawGiven the stakes involved in some legal cases  in one case discussed hereseveral hundred million dollars it is critical that the technology be </raw_string>
  </article>
  <article>
    <title>Segmentation of Greek Text by Dynamic Programming</title>
    <count>573</count>
    <raw_string>Segmentation of Greek Text by Dynamic Programming Pavlina Fragkou  Vassilios Petridis Department of Electrical and Computer Engineering  Faculty of Engineering  Aristotle University of Thessaloniki 54124  Greece fragouegnatia ee authgr  petridiseng authgr Athanassios Kehagias Department of Math   Phys   and Computer Sciences  Faculty of Engineering  Aristotle University of Thessaloniki 54124  Greece kehagiat authgr Abstract We introduce a dynamic programming algorithm to perform linear segmentation of concatenated texts by global minimization of a segmentation cost function which consists of   a  withinsegment word similarity  expressed in terms of the generalized density of the text dotplot  and  b  prior information regarding segment length  Our algorithm is evaluated on two Greek text collections and proves that it outperforms several other algorithms because it performs global optimization of a global cost function  1  Introduction The text segmentation problem of concatenated texts can be stated as follows  given a text which consists of several parts  each part dealing with a different subject  it is required to find the boundaries between the parts  1  3  4  10  11  13   A starting point to this is the calculation of the within segment similarity based on the assumption that parts of a text having similar vocabulary are likely to belong to a coherent topic segment  While some authors have used fairly sophisticated word cooccurrence statistics  3  4  5  13  some evaluate the similarity between all parts of a text  3  4  10  11  14   while others only between adjacent sentences  5  6   To penalize deviations from the expected segment length several </raw_string>
  </article>
  <article>
    <title>Particle Swarm Model Selection for Authorship Verification</title>
    <count>573</count>
    <raw_string>Particle Swarm Model Selection for Authorship Verification Hugo Jair Escalante  Manuel Montes  and Luis Villasenor Laboratorio de Tecnologas del Lenguaje INAOE  Luis Enrique Erro No 1  72840  Puebla  Mexico hugojair mmontesg villasen inaoepmx Abstract  Authorship verification is the task of determining whether documents were or were not written by a certain author  The problem has been faced by using binary classifiers  one per author  that make in dividual yesno decisions about the authorship condition of documents  Traditionally  the same learning algorithm is used when building the clas  sifiers of the considered authors  However  the individual problems that such classifiers face are different for distinct authors  thus using a single algorithm may lead to unsatisfactory results  This paper describes the application of particle swarm model selection  PSMS  to the problem of authorship verification  PSMS selects an adhoc classifier for each author in a fully automatic way  additionally  PSMS also chooses preprocessing and feature selection methods  Experimental results on two collections give evidence that classifiers selected with PSMS are advantageous over selecting the same classifier for all of the authors involved  1 Introduction Author verification  AV  is the task of deciding whether given text documents were or were not written by a certain author 13  There is a wide field of applica  tion for this sort of methods  including spam filtering  fraud detection  computer forensics and plagiarism detection  In all of these domains  the goal is to confirm or reject the authorship condition for documents with respect to a set of candi date authors  given sample documents written by the considered authors  In the past decade this </raw_string>
  </article>
  <article>
    <title>Stylistic Analysis on Reviews of Humanities Objects</title>
    <count>572</count>
    <raw_string>Stylistic Analysis on Reviews of Humanities Objects Xiao HU  J Stephen DOWNIE  Jin Ha LEE University of Illinois at UrbanaChampaign 1  Introduction  Reviews on humanities materials provide important complementary information for humanities scholars  Our previous work has shown that text mining techniques can help analyze a large amount of humanities reviews written by customers among the general public  Hu et al 2006   Customer reviews record how the materials are propagated among and consumed by the public  thus serve as a rich resource for obtaining usergenerated metadata  In addition to customer reviews  there are also professional reviews on humanities objects which are written by experts including humanities scholars  These reviews bear professional criticism on the humanities materials  and thus can provide additional evidence for humanities scholars in analyzing the materials  Both kinds of reviews offer an opportunity to investigate the context in which humanities objects fit  Studying these reviews will help enable more robust contextsensitive search against humanities materials  This study is to examine the differences between the two kinds of reviews and to answer two research questions  1  Are there significant differences between expert reviews and customer reviews  2 If there are  what are the unique patterns in each of the two categories  We conducted preliminary experiments on reviews from amazoncom  the largest online store of various humanities materials including books and music  On amazoncom  many book and music objects have both Editorial reviews and Customer  reviews  The former are written by editors in amazoncom who can be seen as experts while the latter are written by arbitrary users among the general public  Since we are particularly interested in humanities objects  we </raw_string>
  </article>
  <article>
    <title>Multimodal Interactive Transcription of Handwritten Text Images</title>
    <count>572</count>
    <raw_string>UNIVERSIDAD POLITCNICA DE VALENCIA DEPARTAMENTO DE SISTEMAS INFORMTICOS Y COMPUTACIN Multimodal Interactive Transcription of Handwritten Text Images Thesis presented by Vernica Romero Gmez supervised by Prof Enrique Vidal Ruiz and Dr Alejandro Hctor Toselli Rossi September 3  2010 Multimodal Interactive Transcription of Handwritten Text Images Vernica Romero Gmez Thesis performed under the supervision of doctors Enrique Vidal Ruiz and Alejandro Hctor Toselli Rossi and presented at the Universidad Politcnica de Valencia in partial fulfilment of the requirements for the degree Doctor en Informtica Valencia  September 3  2010 Work supported by the Spanish Government  MICINN and Plan E  under the MITTRAL  TIN200914633C0301  research project and under the research programme Consolider Ingenio 2010  MIPRCV  CSD200700018   and by the Universidad Politcnica de Valencia  FPI fellowship 200604   Acknowledgements Desde el momento que decid dedicarme a la investigacin y llevar a cabo un doctorado  muchas son las personas que de una manera u otra han influido en la realizacin del presente trabajo  Personas que me han ayudado y a las que me gustara darles las gracias por todo el esfuerzo  el tiempo y la dedicacin invertida  En primer lugar me gustara darles las gracias a mis directores de tesis Enrique Vidal y Alejandro Hctor Toselli  quienes con su esfuerzo  enseanzas y consejos han conseguido que esta tesis sea una realidad  Gracias por todas las horas dedicadas y por su predisposicin permanente e incondicional a aclarar mis dudas y prestarme su ayuda  Esta tesis es tan suya como ma  Tambin quiero darle las gracias a Moiss  quien tanto me ayudo cuando empec y me ense todos los entresijos del reconocimiento de texto manuscrito y quien siempre ha estado dispuesto a echarme una </raw_string>
  </article>
  <article>
    <title>Towards Music Performer Recognition Using Timbre Features</title>
    <count>572</count>
    <raw_string>Proceedings of the 3rd International Conference of Students of Systematic Musicology  Cambridge  UK September 1315  2010 Towards Music Performer Recognition Using Timbre Features Magdalena Chudy Centre for Digital Music  School of Electronic Engineering and Computer Science Queen Mary University of London Mile End Road  E1 4NS London  UK Tel  44  0 20 7882 7480 magdalenachudyelecqmul acuk Simon Dixon Centre for Digital Music  School of Electronic Engineering and Computer Science Queen Mary University of London Mile End Road  E1 4NS London  UK Tel  44  0 20 7882 7681 simondixonelecqmul acuk ABSTRACT In this study  we investigate whether timbre descriptors commonly used for instrument recognition can serve as discriminators between different players performing on the same instrument  To address the problem we compare timbre features extracted from monophonic recordings of six cellists playing an excerpt from Bach s 1st Cello Suite on two different cellos  We test each descriptor s ability to reflect timbre differences between players and evaluate its adequacy for classification using standard analysis of variance  Keywords Timbre dissimilarities  performer discrimination  timbre descriptors 1  INTRODUCTION A classical music performer interprets a musical piece using parameters such as dynamics  tempo  articulation  timing and timbre  which are essential to enliven and shape an objective musical score into an emotionally expressive performance  They form an individual playing style that is heard in any musical interpretation  To trace and capture performer stylistic features by measuring performance aspects in musical recordings is still an open problem 17  20  11 Recognising performers from their playing style Several previous works have demonstrated the possibility of distinguishing music performers by measuring variations in tempo  dynamics  articulation and timing </raw_string>
  </article>
  <article>
    <title>On Improving Authorship Attribution of Source Code</title>
    <count>572</count>
    <raw_string>M Rogers and KC SeigfriedSpellar  Eds    ICDF2C 2012  LNICST 114  pp  5865  2013   Institute for Computer Sciences  Social Informatics and Telecommunications Engineering 2013 On Improving Authorship Attribution of Source Code Matthew F Tennyson Bradley University  Department of Computer Science  Information Systems  Peoria  IL  USA mtennysonbradley edu Abstract  Authorship attribution of source code is the task of deciding who wrote a program  given its source code  Applications include software forensics  plagiarism detection  and determining software ownership  A number of me thods for the authorship attribution of source code have been proposed  This paper presents an overview and critique of the state of the art in the field  An independent comparative study is presented using an unprecedented experimen tal design and data set  as well as proposals for improvements and future work  Keywords  authorship attribution  software forensics  plagiarism detection  1 Introduction In 1993  the term  software forensics  was coined to refer to the process of analyzing software  usually malicious remnants left after an attack  to identify the authors of the software in question or to at least identify characteristics of the authors 1  The basic premise behind software forensics is that programmers generally apply a unique style to the code they write  As a result  programmers often leave fingerprints  by embedding idiosyncratic features in their software  By identifying such features and associating them with a particular programmer  the original author of software whose author is otherwise unknown can be discovered  The term authorship attribution  refers simply to  the task of deciding who wrote a document  2  </raw_string>
  </article>
  <article>
    <title>Domain Independent Authorship Attribution without Domain Adaptation</title>
    <count>572</count>
    <raw_string>Proceedings of Recent Advances in Natural Language Processing  pages 309315  Hissar  Bulgaria  1214 September 2011  Domain Independent Authorship Attribution without Domain Adaptation Rohith K Menon Department of Computer Science Stony Brook University rkmenoncs stonybrook edu Yejin Choi Department of Computer Science Stony Brook University ychoics stonybrook edu Abstract Automatic authorship attribution  by its nature  is much more advantageous if it is domain  i e   topic andor genre  indepen dent  That is  many real world problems that require authorship attribution may not have indomain training data readily avail able  However  most previous work based on machine learning techniques focused only on indomain text for authorship at tribution  In this paper  we present com prehensive evaluation of various stylomet ric techniques for crossdomain authorship attribution  From the experiments based on the Project Gutenberg book archive  we discover that extremely simple techniques based on stopwords are surprisingly robust against domain change  essentially ridding the need for domain adaptation when sup  plied with a large amount of data  1 Introduction Many real world problems that require authorship attribution  such as forensics  e g  Luyckx and Daelemans  2008   or authorship dispute for old literature  e g  Mosteller and Wallace  1984  may not have indomain training data readily available  However  most previous work to date has focused on authorship attribution only for indomain text  e g  Stamatatos et al   1999   Luyckx and Daele  mans  2008   Raghavan et al   2010    On lim  ited occasions researchers include heterogeneous  crossdomain  dataset in their experiments  but they </raw_string>
  </article>
  <article>
    <title>Detection of Fraudulent Emails by Authorship Extraction</title>
    <count>572</count>
    <raw_string>International Journal of Computer Applications  0975  8887  Volume 41 No7  March 2012 7 Detection of Fraudulent Emails by Authorship Extraction A Pandian Department of MCA SRM University  Chennai  India ABSTRACT Fraudulent emails can be detected by extraction of authorship information from the contents of emails  This paper presents information extraction based on unique words from the emails  These unique words will be used as representative features to train Radial Basis function  RBF   Final weights are obtained and subsequently used for testing  The percentage of identification of email authorship depends upon number of RBF centers and the type of functional words used for training RBF One hundred and fifty authors with over one hundred files from the sent folder of Enron email dataset are considered  A total of 300 unique words of number of characters in each word ranging from three to seven are considered  Training and testing of RBF are done by taking different lengths of words  Our simulation shows the effectiveness of the proposed RBF network for email authorship identification  The accuracy of authorship identification ranges from 95  to 97   Keywords  email authorship identification  spam  word frequency  radial basis function 1  INTRODUCTION As the volumes of emails on the net increases  spam and hoax mails have to be detected  The principal objective of author identification is to classify Koppel et al  2002  the emails belonging to an author  This approach is used in forensic for author identification in malicious emails  Certain commercial software such as AntConc  Copy Catch Gold  Lexico3  Signature Stylometric System  Tlab  Yoshikoder  and WordSmith 2Department of Information Technology Tools use </raw_string>
  </article>
  <article>
    <title>Character Type Classification via Probabilistic Topic Model</title>
    <count>572</count>
    <raw_string>International Journal of Signal Processing  Image Processing and Pattern Recognition Vol  5  No 2  June  2012 123 Character Type Classification via Probabilistic Topic Model Takuma Yamaguchi and Minoru Maruyama Department of Information Engineering Shinshu University  Nagano  3808553  Japan s07t213gmailcom  maruyamacs shinshuuacjp Abstract In this paper  we propose a method for character type classification based on a probabilistic topic model  The topic model is originally developed for topic discovery in text analysis using bagofwords representation  Recent studies have shown the model is also useful for image analysis  We adopt the probabilistic topic model for character type classification  In our method  character type classification is carried out by classifying image patches based on their topic proportions  Since the performance of the method depends on a visual vocabulary generated by image feature extraction  we compare several feature extraction and description methods  and examine the relations to classification performance  In addition  by extending the method  we propose a coarsetofine approach to achieve stable character type classification for a small image patch  For that purpose  firstly  we partition an image into several patches which contain enough information to estimate the model parameters via EM algorithm  Then  each patch is subdivided into smaller patches  Estimation on the small patch is carried out by MAPtechnique with a prior reflecting topic proportion of its parent patch  Through the experiments  we show accurate character type classification is made possible by the probabilistic topic model  Keywords  Character type classification  Probabilistic topic model  pLSA model  Bagof  words 1  Introduction For document image analysis  it is often necessary to recognize parts   that constitute </raw_string>
  </article>
  <article>
    <title>Automatic Natural Language Style Classification and Transformation</title>
    <count>572</count>
    <raw_string>BCS Corpus Profiling Workshop 2008 Automatic Natural Language Style Classification and Transformation Foaad Khosmood and Robert A Levinson University of California Santa Cruz Department of Computer Science  1156 High St  Santa Cruz  CA 95064  USA foaadsoe ucscedu  levinsonsoe ucscedu Style is an integral part of natural language in written  spoken or machine generated forms  Humans have been dealing with style in language since the beginnings of language itself  but computers and machine processes have only recently begun to process natural language styles  Automatic processing of styles poses two interrelated challenges  classification and transformation  There have been recent advances in corpus classification  automatic clustering and authorship attribution along many dimensions but little work directly related to writing styles directly and even less in transformation  In this paper we examine relevant literature to define and operationalize a notion of style  which we employ to designate style markers usable in classification machines  A measurable reading of these markers also helps guide style transformation algorithms  We demonstrate the concept by showing a detectable stylistic shift in a sample piece of text relative to a target corpus  We present ongoing work in building a comprehensive style recognition and transformation system and discuss our results  style  natural language processing  artificial intelligence  corpus classification  style recognition  style transformation 1  INTRODUCTION For millennia  human scholars have been studying  debating style  authors have been writing and changing written styles and critics have been commenting on those styles  The concept of style  as well as detection of a particular instance of a style  and even rewriting of the same text in a different style seems like a trivial task to </raw_string>
  </article>
  <article>
    <title>Computer Assisted Transcription for Ancient Text Images</title>
    <count>571</count>
    <raw_string>Computer Assisted Transcription for Ancient Text Images Veronica Romero1  Alejandro H Toselli1  Luis Rodrguez2  and Enrique Vidal1 1 Instituto Tecnologico de Informatica Universidad Politecnica de Valencia Cam  de Vera sn  46022 Valencia  Spain  vromero ahector evidal iti upves 2 Departamento de Sistemas Informaticos Universidad de Castilla La Mancha  Spain luisrinfoabuclm es Abstract  Paleography experts spend many hours transcribing ancient docu ments and stateoftheart handwritten text recognition systems are not suitable for performing this task automatically  We propose here a new interactive  on  line framework which  rather than full automation  aims at assisting the experts in the proper recognitiontranscription process  that is  facilitate and speed up the transcription of old documents  This framework combines the efficiency of automatic handwriting recognition systems with the accuracy of the experts  leading to a costeffective perfect transcription of ancient manuscripts  Keywords  Automatic Transcription of Ancient Documents  Computer Assisted transcription  handwritten text image recognition  1 Introduction The increasing number of online digital libraries publishing a large quantity of digi tized legacy documents makes also necessary to transcribe these document images  in order to provide historians and other researchers new ways of indexing  consulting and querying these documents  These transcriptions are usually carried out by experts in paleography  who are spe cialized in reading ancient scripts  characterized  among other things  by different hand  writtenprinted styles from diverse places and time periods  How long experts takes to make a transcription of one of these documents depends on their skills and experience  For example  to transcribe many of the pages of the document used in the experiments reported in sections 41  they would spend </raw_string>
  </article>
  <article>
    <title>Structural Parse Tree Features for Text Representation</title>
    <count>571</count>
    <raw_string>Structural Parse Tree Features for Text Representation Sean Massung ChengXiang Zhai Department of Computer Science  College of Engineering University of Illinois at UrbanaChampaign massung1czhai juliahmr illinois edu Julia Hockenmaier Abstract We propose and study novel text representation features created from parse tree structures  Unlike the traditional parse tree features which include all the attached syntactic categories to capture linguistic properties of text  the new features are solely or primarily defined based on the tree structure  and thus better reflect the pure structural properties of parse trees  We hypothesize that these new complex structural features capture an orthogonal perspective of text even compared to advanced syntactic ones  Evaluation based on three different text categorization tasks  i e   nationality detection  essay scoring  and sentiment analysis  shows that the proposed new tree structure features complement the existing ones to enrich text representa  tion  Experiment results further show that a combination of the proposed new structure features with word ngrams can improve F1 score and classification accuracy  I INTRODUCTION Text representation is a fundamental issue in many text processing applications such as information retrieval  text clustering  and text categorization  Text representation is also critical for generating useful features to be used in many machine learning algorithms to support natural language pro cessing applications  In general  we can represent text by a set of extracted features  the simplest features can be just the words in the text  The issue of text representation is complicated because different tasks tend to require a somewhat different perspective of representationthus a different feature set  For example  while functional words are generally not useful for topic categorization  they may be useful for the author attribution </raw_string>
  </article>
  <article>
    <title>On Identifying Academic Homepages for Digital Libraries</title>
    <count>571</count>
    <raw_string>On Identifying Academic Homepages for Digital Libraries Sujatha Das Computer Science and Engineering The Pennsylvania State University University Park  PA 16802 gsdascse psuedu C Lee Giles  Prasenjit Mitra  Cornelia Caragea Information Science and Technology The Pennsylvania State University University Park  PA 16802 giles  pmitra  ccarageaistpsuedu ABSTRACT Academic homepages are rich sources of information on sci entific research and researchers  Most researchers provide in formation about themselves and links to their research pub  lications on their homepages  In this study  we address the following questions related to academic homepages   1  How many academic homepages are there on the web   2  Can we accurately discriminate between academic homepages and other webpages  and  3  What information can be extracted about researchers from their homepages  For addressing the first question  we use markrecapture techniques commonly employed in biometrics to estimate animal population sizes  Our results indicate that academic homepages comprise a small fraction of the Web making automatic methods for discriminating them crucial  We study the performance of contentbased features for classifying webpages  We propose the use of topic models for identifying contentbased features for classification and show that a small set of LDAbased fea  tures outperform term features selected using traditional techniques such as aggregate term frequencies or mutual in formation  Finally  we deal with the extraction of name and research interests information from an academic home  page  Termtopic associations obtained from topic models are used to design a novel  unsupervised technique to iden tify short segments corresponding to research interests of the researchers specified in academic homepages  We show the efficacy of our proposed methods on all the three tasks by </raw_string>
  </article>
  <article>
    <title>Highly Discriminative Statistical Features for email Classification</title>
    <count>570</count>
    <raw_string>Knowl Inf Syst  2012  31 2353 DOI 101007s1011501104037 REGULAR PAPER Highly discriminative statistical features for email classification Juan Carlos Gomez  Erik Boiy  MarieFrancine Moens Received  1 February 2010  Revised  26 January 2011  Accepted  24 February 2011  Published online  18 May 2011  SpringerVerlag London Limited 2011 Abstract This paper reports on email classification and filtering  more specifically on spam versus ham and phishing versus spam classification  based on content features  We test the validity of several novel statistical feature extraction methods  The methods rely on dimensionality reduction in order to retain the most informative and discriminative features  We successfully test our methods under two schemas  The first one is a classic classifica  tion scenario using a 10fold crossvalidation technique for several corpora  including four ground truth standard corpora  LingSpam  SpamAssassin  PU1  and a subset of the TREC 2007 spam corpus  and one proprietary corpus  In the second schema  we test the anticipatory properties of our extracted features and classification models with two proprietary datasets  formed by phishing and spam emails sorted by date  and with the public TREC 2007 spam corpus  The contributions of our work are an exhaustive comparison of several feature selec tion and extraction methods in the frame of email classification on different benchmarking corpora  and the evidence that especially the technique of biased discriminant analysis offers better discriminative features for the classification  gives stable classification results notwith  standing the amount of features chosen  and robustly retains their discriminative value over time and data setups  These findings are especially useful in a commercial setting  where short profile rules are built based on </raw_string>
  </article>
  <article>
    <title>Deteccion Automatica de Plagio en Texto</title>
    <count>570</count>
    <raw_string>Deteccion automatica de plagio en texto Luis Alberto Barron Cedeno Departamento de Sistemas Informaticos y Computacion Director  Paolo Rosso Tesis desarrollada dentro del Master en Inteligencia artificial  reconocimiento de formas e imagen digital Valencia  noviembre de 2008 A quien me dejo ir y a quien me acompana Que la aritmetica es la mas baja de todas las actividades mentales se demuestra por el hecho de que es la unica que puede realizarse por medio de una maquina  Arthur Schopenhauer Resumen Plagiar es robar el credito por el trabajo realizado por otra persona  En el caso de la lengua escrita  significa incluir en un documento fragmentos de texto escritos por alguna otra persona sin darle el credito correspondiente  La deteccion automatica de plagio se basa en diversas tecnicas de recuperacion y extraccion de informacion as como de reconocimiento de formas y teora de la informacion  Esta tarea ha comenzado a generar gran interes debido a la posibilidad de crear mecanismos que puedan detectar casos de plagio de manera eficiente  Esto puede provocar  como un efecto secundario  el desaliento por caer en esta falta  En este trabajo presentamos una descripcion del estado del arte en materia de deteccion de plagio  Ademas  describimos un conjunto de tecnicas  algunas de ellas ya existentes y otras disenadas por nosotros mismos  y presentamos distintas evaluaciones sobre ellas  Los resultados que hemos obtenido hasta ahora con diversas tecnicas basadas en conceptos tan variados como los modelos de lenguaje  distintas tecnicas de comparacion de texto y metodos estadsticos para la reduccion de espacios de busqueda  han generado resulta  dos prometedores  Como continuacion de los experimentos realizados  planteamos las pautas que consideramos conveniente seguir dentro de esta </raw_string>
  </article>
  <article>
    <title>Identifying Violin Performers by their Expressive Trends</title>
    <count>569</count>
    <raw_string>Intelligent Data Analysis 14  2010  555571 555 DOI 103233IDA20100439 IOS Press Identifying violin performers by their expressive trends Miguel MolinaSolanaa   Josep Llus Arcosb and Emilia Gomezc aDepartment of Computer Science and Artificial Intelligence  Universidad de Granada  Granada  Spain bArtificial Intelligence Research Institute  IIIA   Spanish National Research Council  CSIC   Bellaterra  Spain cMusic Technology Group  Universitat Pompeu Fabra  Barcelona  Spain Abstract  Understanding the way performers use expressive resources of a given instrument to communicate with the audience is a challenging problem in the sound and music computing field  Working directly with commercial recordings is a good opportunity for tackling this implicit knowledge and studying wellknown performers  The huge amount of information to be analyzed suggests the use of automatic techniques  which have to deal with imprecise analysis and manage the information in a broader perspective  This work presents a new approach  Trendbased modeling  for identifying professional performers in commercial recordings  Concretely  starting from automatically extracted descriptors provided by stateoftheart tools  our approach performs a qualitative analysis of the detected trends for a given set of melodic patterns  The feasibility of our approach is shown for a dataset of monophonic violin recordings from 23 wellknown performers  1  Introduction The advances in digital sound synthesis and computational power capabilities have allowed to provide realtime control of synthesized sounds  Expressive control becomes then a relevant area of research and a key challenge in the sound and music computing field 26  According to Serra et al  23  music performance is a complex activity that involves complementary facets from different areas such as acoustics  psychology and creativity  In this sense  </raw_string>
  </article>
  <article>
    <title>Automatic Deception Detection in Italian Court Cases</title>
    <count>569</count>
    <raw_string>Automatic deception detection in Italian court cases Tommaso Fornaciari  Massimo Poesio Springer ScienceBusiness Media Dordrecht 2013 Abstract Effective methods for evaluating the reliability of statements issued by witnesses and defendants in hearings would be an extremely valuable support to decisionmaking in court and other legal settings  In recent years  methods relying on stylometric techniques have proven most successful for this task  but few such methods have been tested with language collected in reallife situations of high  stakes deception  and therefore their usefulness outside lab conditions still has to be properly assessed  In this study we report the results obtained by using stylometric techniques to identify deceptive statements in a corpus of hearings collected in Italian courts  The defendants at these hearings were condemned for calumny or false testimony  so the falsity of  some of  their statements is fairly certain  In our experiments we replicated the methods used in previous studies but never before applied to highstakes data  and tested new methods  We also considered the effect of a number of variables including in particular the homogeneity of the dataset  Our results suggest that accuracy at deception detection clearly above chance level can be obtained with reallife data as well  Keywords Deception detection Stylometry Criminal proceedings 1 Introduction Effective methods for tagging potential deception on the basis of verbal or non  verbal cues  by hand or automatically  would have a number of applications in court T Fornaciari    Center for MindBrain Sciences  University of Trento  Trento  Italy email  tommasofornaciariunitn it M Poesio School for Computer Science and Electronic Engineering  University of Essex  Colchester  UK email  poesioessex acuk 123 Artif Intell Law DOI </raw_string>
  </article>
  <article>
    <title>Improving Gender Classification of Blog Authors</title>
    <count>568</count>
    <raw_string>Improving Gender Classification of Blog Authors Arjun Mukherjee Bing Liu Department of Computer Science University of Illinois at Chicago 851 South Morgan Street Chicago  IL 60607  USA amukherjcs uicedu Department of Computer Science University of Illinois at Chicago 851 South Morgan Street Chicago  IL 60607  USA liubcs uicedu Abstract The problem of automatically classifying the gender of a blog author has important appli cations in many commercial domains  Exist ing systems mainly use features such as words  word classes  and POS  partof  speech  ngrams  for classification learning  In this paper  we propose two new techniques to improve the current result  The first tech  nique introduces a new class of features which are variable length POS sequence pat terns mined from the training data using a se quence pattern mining algorithm  The second technique is a new feature selection method which is based on an ensemble of several fea  ture selection criteria and approaches  Empir ical evaluation using a reallife blog data set shows that these two techniques improve the classification accuracy of the current stateof  theart methods significantly  1 Introduction Weblogs  commonly known as blogs  refer to on  line personal diaries which generally contain in formal writings  With the rapid growth of blogs  their value as an important source of information is increasing  A large amount of research work has been devoted to blogs in the natural language processing  NLP  and other communities  There are also many commercial companies that exploit information in blogs to provide valueadded ser vices  eg  blog search  blog topic tracking  and sentiment analysis of peoples opinions on prod  ucts and services </raw_string>
  </article>
  <article>
    <title>Genre Document Classification Using Flexible Length Phrases</title>
    <count>567</count>
    <raw_string>Genre Document Classification Using Flexible Length Phrases Danijel Radoevi  Jasminka Doba University of Zagreb  Faculty of Organization and Informatics Pavlinska 2  42 000 Varadin  Croatia danijel radosevic  jasminkadobsafoi hr Dunja Mladeni Joef Stefan Institute Jamova 39  1000 Ljubljana  Slovenia DunjaMladenicijs si Zlatko Stapi  Miroslav Novak University of Zagreb  Faculty of Organization and Informatics Pavlinska 2  42 000 Varadin  Croatia zlatko stapic  miroslavnovak foi hr Abstract  In this paper we investigate possibility of using phrases of flexible length in genre classification of textual documents as an extension to classic bag of words document representation where documents are represented using single words as features  The investigation is conducted on collection of articles from document database collected from three different sources representing different genres  newspaper reports  abstracts of scientific articles and legal documents  The investigation includes comparison between classification results obtained by using classic bag of words representation and results obtained by using bag of words extended by flexible length phrases  Keywords  Flexible length phrases  bag of words representation  genre classification 1  Introduction The goal of text categorization is classification of text documents into a fixed number of predefined categories  Document classification is used in many different problem areas involving text documents such as classifying news articles based on their content  or suggesting interesting documents to the web user  The common way of representing textual documents is by vector space model or  so called bag ofwords representation 13  Generally  index term can be any word present in the text of document  but not all the words in the documents have equal importance in representation of document semantic  That is why various schemes in </raw_string>
  </article>
  <article>
    <title>New Methods for Attribution of Rabbinic Literature</title>
    <count>567</count>
    <raw_string>New Methods for Attribution of Rabbinic Literature Moshe Koppel Dror Mughaz Navot Akiva koppel myghaz navot cs biuacil Dept  of Computer Science BarIlan University Introduction In this paper  we will demonstrate how recent developments in the nascent field of automated text categorization can be applied to Hebrew and HebrewAramaic texts  In particular  we illustrate the use of new computational methods to address a number of scholarly problems concerning the classification of rabbinic manuscripts  These problems include ascertaining answers to the following questions 1  Which of a set of known authors is the most likely author of a given document of unknown provenance  2 Were two given corpora writtenedited by the same author or not  3 Which of a set of documents preceded which and did some influence others  4  From which version  manuscript  of a document is a given fragment taken  We will apply our techniques to a number of representative problems involving corpora of rabbinic texts  Text Categorization Text categorization is one of the major problems of the field of machine learning  Sebastiani 2002   The idea is that we are given two or more classes of documents and we need to find some formula usually called a model  that reflects statistical differences between the classes and that can then be used to classify a new document  For example  we might wish to classify a document as being about one of a number of possible topics  as having been written by a man or a woman  as having been written by one of a given set of candidate authors and so forth  Figure 1  Architecture of a text categorization system  In Figure 1 we show </raw_string>
  </article>
  <article>
    <title>Using Biased Discriminant Analysis for Email Filtering</title>
    <count>567</count>
    <raw_string>Using Biased Discriminant Analysis for Email Filtering Juan Carlos Gomez1 and MarieFrancine Moens2 1 ITESM  Eugenio Garza Sada 2501  Monterrey NL 64849  Mexico juancarlosgomezinvitados itesmmx 2 Katholieke Universiteit Leuven  Celestijnenlaan 200A  B3001 Heverlee  Belgium sienmoenscs kuleuvenbe Abstract  This paper reports on email filtering based on content fea  tures  We test the validity of a novel statistical feature extraction method  which relies on dimensionality reduction to retain the most informative and discriminative features from messages  The approach  named Biased Discriminant Analysis  BDA   aims at finding a feature space transfor mation that closely clusters positive examples while pushing away the negative ones  This method is an extension of Linear Discriminant Anal ysis  LDA   but introduces a different transformation to improve the separation between classes and it has up till now not been applied for text mining tasks  We successfully test BDA under two schemas  The first one is a tra  ditional classification scenario using a 10fold cross validation for four ground truth standard corpora  LingSpam  SpamAssassin  Phishing cor pus and a subset of the TREC 2007 spam corpus  In the second schema we test the anticipatory properties of the statistical features with the TREC 2007 spam corpus  The contributions of this work is the evidence that BDA offers better discriminative features for email filtering  gives stable classification re  sults notwithstanding the amount of features chosen  and robustly retains their discriminative value over time  1 Introduction  In data mining the goal is to find previously unknown patterns and relations in large databases 12  and a common task is automatic classification  Here  given a set of instances with known </raw_string>
  </article>
  <article>
    <title>Stylistics in Customer Reviews of Cultural Objects</title>
    <count>567</count>
    <raw_string>Stylistics in Customer Reviews of Cultural Objects Xiao Hu Graduate School of Library and Information Science University of Ilinois at UrbanaChampaign xiaohuuiucedu J Stephen Downie Graduate School of Library and Information Science University of Ilinois at UrbanaChampaign jdownieuiucedu 1  INTRODUCTION Online customer reviews of cultural objects  e g  books  movies  music  etc  are written by users themselves  These usergenerated reviews thus provide an important resource for researchers who are interested in creating better access mechanisms for users as they seek out and consume cultural materials  Furthermore  customer reviews at retail websites  e g  amazoncom  and community review websites  e g  epinionscom  are often well organized and thus provide researchers with valuable ground truth  data with regard to such things as genre labels  artist names  quality ratings  etc  A deep level understanding of how users express themselves with regard to the use of cultural objects can contribute enormously to the construction of more useful and powerful automated retrieval and recommendation tools  This overview paper describes a series of text mining and stylistics experiments conducted over the last year  The interested reader is guided toward to 256 where we have previously published various subsections of these experiments  Our work has focused upon one particularly datarich collection of user generated reviews of music  books and movies  namely epinionscom  At epinionscom  each review is associated with both a genre label and a numerical quality rating expressed as a number of stars  from 1 to 5  with higher ratings indicating more positive opinions  Also in each music review  there is a field called Great Music to Play While   where the reviewer </raw_string>
  </article>
  <article>
    <title>Using Expressive Trends For Identifying Violin Performers</title>
    <count>567</count>
    <raw_string>ISMIR 2008  Session 4b  Musical Expression and Meaning USING EXPRESSIVE TRENDS FOR IDENTIFYING VIOLIN PERFORMERS Miguel MolinaSolana Comp  Science and AI Dep University of Granada 18071 Granada  Spain miguelmolinaugres Josep Llus Arcos IIIA  AI Research Institute CSIC  Spanish National Res  Council 08193 Bellaterra  Spain arcosiiiacsices Emilia Gomez Music Technology Group Universitat Pompeu Fabra 08003 Barcelona  Spain egomeziua upf edu ABSTRACT This paper presents a new approach for identifying profes  sional performers in commercial recordings  We propose a Trendbased model that  analyzing the way Narmours Im  plicationRealization patterns are played  is able to charac  terize performers  Concretely  starting from automatically extracted descriptors provided by stateoftheart extraction tools  the system performs a mapping to a set of qualita  tive behavior shapes and constructs a collection of frequency distributions for each descriptor  Experiments were con ducted in a dataset of violin recordings from 23 different performers  Reported results show that our approach is able to achieve high identification rates  1 INTRODUCTION Expressive performance analysis and representation is cur rently a key challenge in the sound and music computing area  Previous research has addressed expressive music per formance using machine learning techniques  For example  Juslin et al  6  studied how expressivity could be com putationally modeled  Ramirez et al  12 have proposed an approach for identifying saxophone performers by their playing styles  Lopez de Mantaras et al  9  proposed a case based reasoning approach to deal with expressiveness  Hong 7  investigated expressive timing and dynamics in recorded cello  Dovey 2  analyzed Rachmaninoffs piano performances using inductive logic programming  Work on automatic piano performer identification has been done </raw_string>
  </article>
  <article>
    <title>Multimodal Interactive Transcription of Ancient Text Images</title>
    <count>567</count>
    <raw_string>Multimodal Interactive Transcription of Ancient Text Images Veronica Romero  Joan Andreu Sanchez  Alejandro H Toselli  and Enrique Vidal Instituto Tecnologico de Informatica  ITI  Universidad Politecnica de Valencia  Spain vromero jandreu ahector evidal iti upves Abstract  The amount of digitized legacy documents has been rising dramatically over the last years due mainly to the increasing number of online digital libraries publishing this kind of documents  On one hand  the vast majority of these documents remain waiting to be transcribed into a textual electronic format  such as ASCII or PDF that would provide historians and other researchers new ways of indexing  consult ing and querying these documents  On the other hand  in some cases  adequate transcriptions of the handwritten text images are already avail able  This drives an increasing need to align images and transcriptions in order to make it more comfortable the consulting of these documents  In this work two systems are presented to deal with these issues  The first one aims at transcribing these documents using a interactivepredictive approach  which integrates user correctivefeedback actions in the proper recognition process  The second one presents an alignment method based on the Viterbi algorithm to find mappings between word images of a given handwritten document and their respective  ASCII  words on its given transcription  Keywords  Handwritten text recognition  Multimodal interactive framework  Viterbi alignment  1 Introduction The task of transcribing old handwritten documents is becoming an important research topic  specially because of the increasing number of online digital li braries publishing large quantities of digitized legacy documents  The vast ma  jority of these documents  hundreds of terabytes worth of digital image data  remain </raw_string>
  </article>
  <article>
    <title>Web Content Mining Focused on Named Objects</title>
    <count>567</count>
    <raw_string>Web Content Mining Focused on Named Objects Vclav Snel and Milos Kudelka VSB  Technical University of Ostrava Faculty of Electrical Engineering and Computer Science 708 33 OstravaPoruba  Czech Republic vaclavsnaselvsbcz  miloskudelkainflexcz Abstract  In our chapter we are working within the field of Web content mining  In relation to the user s description of a Web page  we define a new term  Named object  Named objects are used for a new classification of selected methods dealing with mining information from Web pages  This classification has been made on the basis of a survey of published methods  Our approach is based on the perception of a Web page through an intention  This intention is important both for the users and authors of a Web page  Named object is near to Web design patterns  which became a basis for our own mining method  Pattrio  The Pattrio method is introduced in this work together with a few experiments  1 Introduction A Web page is like a family house  Each of its parts has its purpose  determined by a function which it serves  Every part can be named so that all users envision approximately the same thing under that name such as living room  bathroom  lobby  bedroom  kitchen and balcony  In order for the inhabitants to orientate well in the house  certain rules are kept  From the point of view of these rules  all houses are similar  That is why it is usually not a problem for first time visitors to orientate in the house  We can describe the house quite precisely thanks to names  If we add information about a more detailed location such </raw_string>
  </article>
  <article>
    <title>A Methodological Contribution to Music Sequences Analysis</title>
    <count>567</count>
    <raw_string>A Methodological Contribution to Music Sequences Analysis Daniele P Radicioni and Marco Botta Universita di Torino  Dipartimento di Informatica Cso Svizzera 185  10149  Turin  Italy radicion  bottadi unitoit Abstract  In this paper we present a stepwise method for the analysis of musical sequences  The starting point is either a MIDI file or the score of a piece of music  The result is a set of likely themes and motifs  The method relies on a pitch intervals representation of music and an event discovery system that extracts significant and repeated patterns from sequences  We report and discuss the results of a preliminary experimentation  and outline future enhancements  1 Introduction In the last few years many efforts have been spent on music issues within the AI community  Two main tasks have been addressed  requiring intelligent  and sophisticated strategies  music analysis and music performance  The first line of research investigated topics such as performer recognition 1  harmonic analysis 2  segmentation 3  whereas the second one aims at reproducing expressive music performance by means of artificial systems 45  Music analysis is a relevant task  in that it deeply affects our comprehension of music  as regards of composition  performance and listening  Music analysis is a challenging task  consider  eg  that a significant part of professional music instruction is concerned with assisting the learner in understanding  music for the different purposes of composition and performance  In this paper we point out the problem of discovering repeated patterns  as a major issue for building systems for music analysis  It is commonly ac  knowledged that in Western Tonal Music repetition plays a fundamental role  </raw_string>
  </article>
  <article>
    <title>Effects of Age and Gender on Blogging</title>
    <count>567</count>
    <raw_string>Effects of Age and Gender on Blogging Jonathan Schler 1 Moshe Koppel 1 Shlomo Argamon 2 James Pennebaker 3 1 Dept  of Computer Science  BarIlan University  Ramat Gan 52900Israel 2 Linguistic Cognition Lab  Dept  of Computer Science Illinois Institute of Technology  Chicago  IL 60616 3 Dept  of Psychology  The University of Texas  Austin  TX 78712 schlerjcs biuacil  koppelcs biuacil  argamoniit edu  pennebakermail utexas edu Abstract Analysis of a corpus of tens of thousands of blogs  incorporating close to 300 million words  indicates significant differences in writing style and content between male and female bloggers as well as among authors of different ages  Such differences can be exploited to determine an unknown authors age and gender on the basis of a blogs vocabulary  Introduction The increasing popularity of publicly accessible blogs offers an unprecedented opportunity to harvest information from texts authored by hundreds of thousands of different authors  Conveniently  many of these blogs include formatted demographic information provided by the authors   While much of this information is no doubt spurious  it is reasonable to assume that most is not   Moreover  the blog genre imposes no restrictions on choice of topic  Thus  we can exploit this rich data set to begin to answer the following question  How do content and writing style vary between male and female bloggers and among bloggers of different ages  How much information can we learn about somebody simply by reading a text that they have authored  These are very basic questions that are both of fundamental theoretical interest and of great practical consequence in forensic and commercial domains  In the following sections  we will </raw_string>
  </article>
  <article>
    <title>Extraction of Handwriting in Tabular Document Images</title>
    <count>567</count>
    <raw_string>Extraction of Handwriting in Tabular Document Images Robert T Clawson Brigham Young University Department of Computer Science Provo  Utah  USA rtclawsonbyuedu William A Barrett Brigham Young University Department of Computer Science Provo  Utah  USA barrettcs byuedu ABSTRACT We propose a method for detecting handwriting in sets of tabular document images that share a common form  This is accomplished leveraging previous work on aligning struc  tured documents  These aligned documents are processed to  gether as an image stack  First  the blank form common to the documents is generated using image averaging  Then  each image is compared individually to the blank form  and regions with a large difference are marked as handwriting  Results are pursued under the assumption that a good hand  writing detection algorithm will have as few false positives as possible while maximizing recall  Proof of concept efforts are convincing  though a more indepth analysis remains to be done  Work to filter out false positives will be pursued  1  INTRODUCTION Images within a batch of census records often share in com mon the same form  but differ in handwritten content  We seek to extract the handwriting from the form and back  ground  Our approach differs from previous efforts to sep arate handwriting from machine printed text  which com monly begin with connected component analysis  because we also must separate the handwriting from the form  Rather than using a bottom up approach with connected compo nents  we leverage the redundancy between images to filter out the form  leaving only handwriting behind  The ultimate goal in census indexing is to be able to con vert both table headers and fields into text </raw_string>
  </article>
  <article>
    <title>Automatic Unsupervised Parameter Selection for Character Segmentation</title>
    <count>567</count>
    <raw_string>Automatic Unsupervised Parameter Selection for Character Segmentation GVamvakas  N Stamatopoulos  B Gatos and SJPerantonis Computational Intelligence Laboratory  Institute of Informatics and Telecommunications  National Center for Scientific Research Demokritos  GR153 10 Agia Paraskevi  Athens  Greece gbam  nstam  bgat  sper iitdemokritosgr ABSTRACT A major difficulty for designing a document image segmentation methodology is the proper value selection for all involved parameters  This is usually done after experimentations or after involving a training supervised phase which is a tedious process since the corresponding segmentation ground truth has to be created  In this paper  we propose a novel automatic unsupervised parameter selection methodology that can be applied to the character segmentation problem  It is based on clustering of the entities obtained as a result of the segmentation for different values of the parameters involved in the segmentation method  The clustering is performed using features extracted from the segmented entities based on zones and from the area that is formed from the projections of the upperlower and leftright profiles  Optimization of an appropriate intraclass distance measure yields the optimal parameter vector  The method is evaluated on two segmentation algorithms  namely a recently proposed character segmentation technique based on skeleton segmentation paths  as well as the well known RLSA technique  The proposed parameter selection method is capable of finding the segmentation parameters that correspond to the optimal or near optimal segmentation result  as this is determined by counting the number of matches between the entities detected by the segmentation algorithm and the entities in the ground truth  Categories and Subject Descriptors I53 PATTERN RECOGNITION  Clustering  algorithms  similarity measures  I46  IMAGE PROCESSING AND COMPUTING VISION  Segmentation  I75  </raw_string>
  </article>
  <article>
    <title>Stylistic Text Classification using Functional Lexical Features</title>
    <count>566</count>
    <raw_string>Stylistic Text Classification Using Functional Lexical Features Shlomo Argamon1 Casey Whitelaw2 Paul Chase1 Sushant Dhawle1 Sobhan Raj Hota1 Navendu Garg1 Shlomo Levitan1 1Linguistic Cognition Lab  Department of Computer Science Illinois Institute of Technology 10 W 31st Street  Chicago  IL 60616  USA 2School of Information Technologies  University of Sydney Sydney  NSW 00000  Australia July 20  2005 Abstract Most text analysis and retrieval work to date has focused on determining the topic of a text  what it is about  However  a text also contains much useful information in its style  or how it is written  This includes information about its author  its purpose  feelings it is meant to evoke  and more  This paper addresses the problem of classifying texts by style  along several different dimensions   developing a new type of lexical feature based on taxonomies of various semantic functions of different lexical items  words or phrases   We show the usefulness of such features for text classification by author  author personality  gender of literary characters  sentiment  positivenegative feeling   and scientific rhetorical styles  We further show how the use of such functional features aids in gaining insight about stylistic differences between texts  Casey Whitelaw was a visiting scholar at the IIT Linguistic Cognition Laboratory during November 2004  1 1 Introduction A common goal in automated text analysis is to gain an understanding or summary of the topic  or topics  covered in the text  This may involve information extraction into framebased seman tic representations  Hammer  GarciaMolina  Cho  Crespo   Aranha  1997   text clustering and categorization  Sebastiani  2002  Kehagias  </raw_string>
  </article>
  <article>
    <title>Lexicalizing Computational Stylistics for Language Learner Feedback</title>
    <count>566</count>
    <raw_string>1 LEXICALIZING COMPUTATIONAL STYLISTICS For Language Learner Feedback Julian BROOKE and Graeme HIRST University of Toronto  Department of Computer Science  10 Kings College Road  Toronto  Ontario  Canada M5S 3G4 jbrookegh cs torontoedu 1 Background 11 Computational Stylistics Computational stylistics refers informally to a collection of tasks within computational linguis  tics that deal with the styleas opposed to the semantic contentof natural language  The most famous of these tasks is perhaps authorship attribution  Stamatatos et al   2001   which uses sta  tistical variations in word choice to select the most likely from a fixed set of potential authors  Though applicable to a number of different applications  this framework has been applied spe cifically to literary analysis  grouping authors by their style  Luyckx et al   2006   A broader definition of style brings it closer to the definition of register or genre  Biber and Conrad  2009   which has also received some attention in the context of text classification  Kessler et al   1997   In educational and literacy contexts  the readability of a text is an important stylistic feature  and the automatic detection of grade level  for instance  has been addressed in recent work  Petersen and Osendorf  2009   The field of text generation was among the first in computational linguis  tics to implement sensitivity to style  Hovy  1990   which has continued into the modern  data  driven era  Paiva and Evans  2005   Though the tasks mentioned above are quite diverse  there are some common themes across computational work in style  First  there is a overwhelming </raw_string>
  </article>
  <article>
    <title>Highly Discriminative Statistical Features for email Classification</title>
    <count>565</count>
    <raw_string>Knowl Inf Syst DOI 101007s1011501104037 REGULAR PAPER Highly discriminative statistical features for email classification Juan Carlos Gomez  Erik Boiy  MarieFrancine Moens Received  1 February 2010  Revised  26 January 2011  Accepted  24 February 2011  SpringerVerlag London Limited 2011 Abstract This paper reports on email classification and filtering  more specifically on spam versus ham and phishing versus spam classification  based on content features  We test the validity of several novel statistical feature extraction methods  The methods rely on dimensionality reduction in order to retain the most informative and discriminative features  We successfully test our methods under two schemas  The first one is a classic classifica  tion scenario using a 10fold crossvalidation technique for several corpora  including four ground truth standard corpora  LingSpam  SpamAssassin  PU1  and a subset of the TREC 2007 spam corpus  and one proprietary corpus  In the second schema  we test the anticipatory properties of our extracted features and classification models with two proprietary datasets  formed by phishing and spam emails sorted by date  and with the public TREC 2007 spam corpus  The contributions of our work are an exhaustive comparison of several feature selec tion and extraction methods in the frame of email classification on different benchmarking corpora  and the evidence that especially the technique of biased discriminant analysis offers better discriminative features for the classification  gives stable classification results notwith  standing the amount of features chosen  and robustly retains their discriminative value over time and data setups  These findings are especially useful in a commercial setting  where short profile rules are built based on a limited number of features for filtering emails  Keywords Data mining </raw_string>
  </article>
  <article>
    <title>Ensemble LUT Classification for Degraded Document Enhancement</title>
    <count>564</count>
    <raw_string>Ensemble LUT classification for degraded document enhancement Tayo ObafemiAjayi  Gady Agam  Ophir Frieder Department of Computer Science  Illinois Institute of Technology  Chicago  IL 60616 ABSTRACT The fast evolution of scanning and computing technologies have led to the creation of large collections of scanned paper documents  Examples of such collections include historical collections  legal depositories  medical archives  and business archives  Moreover  in many situations such as legal litigation and security investigations scanned collections are being used to facilitate systematic exploration of the data  It is almost always the case that scanned documents suffer from some form of degradation  Large degradations make documents hard to read and substantially deteriorate the performance of automated document processing systems  Enhancement of degraded document images is normally performed assuming global degradation models  When the degradation is large  global degradation models do not perform well  In contrast  we propose to estimate local degradation models and use them in enhancing degraded document images  Using a semiautomated enhancement system we have labeled a subset of the Frieder diaries collection 1 This labeled subset was then used to train an ensemble classifier  The component classifiers are based on lookup tables  LUT in conjunction with the approximated nearest neighbor algorithm  The resulting algorithm is highly efficient  Experimental evaluation results are provided using the Frieder diaries collection 1 Keywords  image enhancement  historical documents  document degradation models  ensemble classification  doc  ument image analysis 1  INTRODUCTION The enhancement of old typewritten historical documents is very essential and needful for preservation and contin uation of information  They currently exist electronically as scanned document images  Not only is the quality of the typewritten text </raw_string>
  </article>
  <article>
    <title>Cognitive and Social Effects of Handwritten Annotations</title>
    <count>564</count>
    <raw_string>Cognitive and social effects of handwritten annotations Andrea Mazzei CRAFT  EPFL Rolex Learning Center CH1015 Lausanne andreamazzeiepflch Frederic Kaplan CRAFT  EPFL Rolex Learning Center CH1015 Lausanne frederickaplanepflch Pierre Dillenbourg CRAFT  EPFL Rolex Learning Center CH1015 Lausanne pierre dillenbourgepflch Abstract This article first describes a method for extracting and classifying handwritten annotations on printed doc  uments using a simple camera integrated in a lamp  The ambition of such a research is to offer a seamless integration of notes taken on printed paper in our daily interactions with digital documents  Existing studies pro pose a classification of annotations based on their form and function  We demonstrate a method for automating such a classification and report experimental results showing the classification accuracy  In the second part of the article we provide a road map for conducting usercentered studies using eyetracking systems aiming to investi gate the cognitive roles and social effects of annotations  Based on our understanding of some research questions arising from this experiment we describe in the last part of the article a social learning environment that facilitates knowledge sharing across a class of students or a group of colleagues through shared annotations  1 Introduction Annotations played an important role in the history of the book  Already in the early middle ages  the annotations  at that time known as glosses  started to appear on the manuscripts  They were born with a social vocation and on a scholarly need for elucidation and reinterpretation of the obscure terminology and passages of the medieval manuscripts  Therefore the glosses became widely considered as precious reading support  For example  one thinks to the adoption of the Justinian Codes in many law schools  The Infortiatum 1 a  </raw_string>
  </article>
  <article>
    <title>A New Approach for Flexible Document Categorization</title>
    <count>563</count>
    <raw_string>Abstract In this paper we propose a new approach for flexible document categorization according to the document type or genre instead of topic  Our approach implements two homogenous classifiers  contextual classifier and logical classifier  The contextual classifier is based on the document URL  whereas  the logical classifier use the logical structure of the document to perform the categorization  The final categorization is obtained by combining contextual and logical categorizations  In our approach  each document is assigned to all predefined categories with different membership degrees  Our experiments demonstrate that our approach is best than other genre categorization approaches  KeywordsCategorization  combination  flexible  logical structure  genre  category  URL I INTRODUCTION ITH the increase of web documents  it is very difficult to retrieve desired information quickly out of the documents retrieved by a search engine  To improve the search quality  many works propose to classify documents according to their topics 20  Even if the documents are classified successfully by their subjects  they stayed heterogeneous  For example  the documents grouped by the topic cinema can be an actor homepage  a newspaper about a film or an actor  a collection of films posters and so on  So  a user looking for newspapers about cinema should consult all other document types or genres  Therefore  the document genre or type is considered as another categorization criterion  Many definitions of document genre have been proposed in the past 1  9  15  12  6  Common to all is that document genre is another document view orthogonal to document topic  say  documents having the same subject can be of different genres  Studies on </raw_string>
  </article>
  <article>
    <title>Automatic Text Block Separation in Document Images</title>
    <count>563</count>
    <raw_string>Automatic text block separation in document images ArvindKR  Peeta Basa Pati  AGRamakrishnan MILE Laboratory  Electrical Engineering  Indian Institute of Science  Bangalore560012  India  arvind  ragashri  ee  iisc  ernet  in  pati ee  iisc  ernet  in  ramkiag ee  iisc  emret  in Abstract Separation of printed text blocks from the nontext areas  containing signatures  handwritten text  logos and other such symbols  is a necessary first step for an OCR involving printed text recognition  In the present work  we compare the efficacy of some featureclassifier combinations to carry out this sep aration task  We have selected lengthnormalized horizontal projection profile  HPP  as the starting point of such a sepa ration task  This is with the assumption that the printed text blocks contain lines of text which generate HPP s with some regularity  Such an assumption is demonstrated to be valid  Our features are the HPP and its two transformed versions  namely  eigen and Fisher profiles  Four well known classifiers  namely  Nearest neighbor  Linear discriminantfunction  SVM s and artificial neural networks have been considered and efficiency of the combination of these classifiers with the above features is compared  A sequential floating feature selection technique has been adopted to enhance the efficiency of this separation task  The results give an average accuracy of about 96   Keywords  Eigen profiles  Fisher profiles  horizontal projec tion profile  1  INTRODUCTION Analysis of a document image involves some of the following tasks   i  to convert a it to editable text for its reusability   ii  extract important information from </raw_string>
  </article>
  <article>
    <title>Filtering Web Text to Match Target Genres</title>
    <count>562</count>
    <raw_string>FILTERING WEB TEXT TO MATCH TARGET GENRES M A Marin  S Feldman  M Ostendorf and M Gupta University of Washington Department of Electrical Engineering  Seattle  WA ABSTRACT In language modeling for speech recognition  both the amount of training data and the match to the target task impact the goodness of the model  with the tradeoff usually favoring more data  For con versational speech  having some genrematched text is particularly important  but also hard to obtain  This paper proposes a new ap  proach for genre detection and compares different alternatives for filtering web text for genre to improve language models for use in automatic transcription of broadcast conversations  talk shows   Index Terms  genre  web text filtering  language modeling 1  INTRODUCTION Research on language modeling has repeatedly shown that larger training sets lead to better performance  Matching the training data to the task  in topic  time epoch  and genre  also benefits perfor mance  Mismatched training and test conditions  e g  newswire and conversational speech  can lead to an order of magnitude increase in perplexity over matched conditions  and adding such data can hurt performance if the data is not properly weighted 1  While size cannot compensate for domain mismatch  a large amount of text that is only somewhat matched to the target is typically more use  ful than a small amount of well matched data  However  finding text that is somewhat matched to conversational speech tasks can be difficult  since written text sources include almost no instances of the sort of disfluencies and filled pauses that are frequently ob  served in spontaneous speech  Useful transcribed speech </raw_string>
  </article>
  <article>
    <title>Authorship Attribution Using Function Words Adjacency Networks</title>
    <count>561</count>
    <raw_string>AUTHORSHIP ATTRIBUTION USING FUNCTION WORDS ADJACENCY NETWORKS Santiago Segarra  Mark Eisen  and Alejandro Ribeiro Department of Electrical and Systems Engineering  University of Pennsylvania ABSTRACT We present an authorship attribution method based on relational data between function words  These are content independent words that help define grammatical relationships  As relational structures we use normalized word adjacency networks  We interpret these networks as Markov chains and compare them using entropy measures  We illustrate the accuracy of the method developed through a series of numerical experiments including comparisons with frequency based methods  We show that accuracy increases when combining relational and frequency based data  indicating that both sources of information encode different aspects of authorial styles  Index Terms  Authorship attribution  word adjacency network  Markov chain  relative entropy 1  INTRODUCTION The goal of authorship attribution is to match a text of unknown or dis  puted authorship to one of a group of potential candidates  More gen erally  it can be seen as the search for a compact representation of an authors writing style  or stylometric fingerprint  Applications of this study range from forensics to questions of plagiarism in the works of both published authors as well as students  With recent developments in computational efficiency and information processing  authorship at tribution studies are of both increasing interest and accuracy 1  2  The study of authorship attribution  sometimes called stylometry  has its be ginnings in works published over a century ago 3  which proposed dis  tinguishing authors by looking at word lengths  This was later improved upon by 4  to consider average sentence length as a determinant  These two rudimentary ideas have improved since  </raw_string>
  </article>
  <article>
    <title>Function Words for Chinese Authorship Attribution</title>
    <count>533</count>
    <raw_string>Workshop on Computational Linguistics for Literature  pages 4553  Montreal  Canada  June 8  2012  c2012 Association for Computational Linguistics Function Words for Chinese Authorship Attribution Bei Yu School of Information Studies Syracuse University byusyredu Abstract This study explores the use of function words for authorship attribution in modern Chinese CFWAA   This study consists of three tasks   1  examine the CFWAA effectiveness in three genres  novel  essay  and blog   2  compare the strength of function words as both genre and authorship indicators  and explore the genre interference on CFWAA   3  examine whether CFWAA is sensitive to the time periods when the texts were written  1 Introduction Function words are an important feature set for Authorship Attribution  hereafter AA  because they are considered topicindependent or context free  and that they are largely used in an unconscious manner  Holmes  1994  Stamatatos  2009  Koppel et al   2009   The Federalist Papers  Mostellar and Wallace  1964  may be the most famous example of AA in English  Mostellar and Wallace  1964  conducted a detailed study of searching and testing function words to distinguish Hamilton and Madison as the authors of the disputed Federalist Papers  Although Function Word based Authorship Attribution  hereafter FWAA  has been successful in many studies  Stamatatos  2009   Juola  2008  argued that FWAA are mainly applied in English texts  and it may not be appropriate for other highly inflected languages  like Finnish and Turkish  This may not be the case in that it is the content words  not the function words  that </raw_string>
  </article>
  <article>
    <title>Measuring Feature Distributions in Sentiment Classification</title>
    <count>528</count>
    <raw_string>Measuring Feature Distributions in Sentiment Classification Diego Uribe Instituto Tecnologico de la Laguna Division de Posgrado e Investigacion Blvd  Revolucion y Calz  Cuauhtemoc  Torreon  Coah  MX diegoitlalaguna edumx Abstract  We address in this paper the adaptation problem in senti ment classification  As we know  available labeled data required by sen timent classifiers does not always exist  Given a set of labeled data from different domains and a collection of unlabeled data of the target domain  it would be interesting to determine which subset of those domains has a feature distribution similar to the target domain  In this way  in the absence of labeled data for a particular target domain  it would be plau  sible to make use of the labeled data corresponding to the most similar domains  1 Introduction A huge volume of opinionated text is nowadays a common scenario in the Web It is precisely this enormous information that represents a great sandbox for devel oping practical applications for the industry and the government  For example  before buying a product or service  people look for someone elses experiences about the product on the web  i e  opinions  The same occurs with services provided by government or industry  In this way  persons and organizations are interested in the use of opinionated text for decision optimization  persons taking care of their money  and organizations getting significant feedbacks for improving their products or services 2  This increasing interest in the use of opinionated text demands automated opinion discovery and classification systems  Sentiment classification is basically a text classification task in which  instead of assigning one topic to the text  the attitude of the </raw_string>
  </article>
  <article>
    <title>Clustering voices in The Waste Land</title>
    <count>497</count>
    <raw_string>Proceedings of the Second Workshop on Computational Linguistics for Literature  pages 4146  Atlanta  Georgia  June 14  2013  c2013 Association for Computational Linguistics Clustering voices in The Waste Land Julian Brooke Dept of Computer Science University of Toronto jbrookecs torontoedu Graeme Hirst Dept of Computer Science University of Toronto ghcs torontoedu Adam Hammond Dept of English University of Toronto adam hammondutorontoca Abstract TS Eliots modernist poem The Waste Land is often interpreted as collection of voices which appear multiple times throughout the text  Here  we investigate whether we can automatically cluster existing segmentations of the text into coherent  expertidentified characters  We show that clustering The Waste Land is a fairly dif  ficult task  though we can do much better than random baselines  particularly if we begin with a good initial segmentation  1 Introduction Although literary texts are typically written by a sin gle author  the style of a work of literature is not nec essarily uniform  When a certain character speaks  for instance  an author may shift styles to give the character a distinct voice  Typically  voice switches in literature are explicitly marked  either by the use of quotation marks with or without a said quota  tive  or  in cases of narrator switches  by a major textual boundary  e g  the novel Ulysses by James Joyce   However  implicit marking is the norm in some modernist literature  a wellknown example is the poem The Waste Land by TS Eliot  which is usually analyzed in terms of voices that each appear multiple times throughout the text  Our interest is distinguishing these voices automatically  One of the poems most distinctive </raw_string>
  </article>
  <article>
    <title>Multimodal Subjectivity Analysis of Multiparty Conversation</title>
    <count>497</count>
    <raw_string>Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing  pages 466474  Honolulu  October 2008  c2008 Association for Computational Linguistics Multimodal Subjectivity Analysis of Multiparty Conversation Stephan Raaijmakers TNO Information and Communication Technology Delft  The Netherlands stephan raaijmakerstnonl Khiet Truong TNO Defense  Security and Safety Soesterberg  The Netherlands khiettruongtnonl Theresa Wilson School of Informatics University of Edinburgh Edingburgh  UK twilsoninf ed acuk Abstract We investigate the combination of several sources of information for the purpose of sub  jectivity recognition and polarity classification in meetings  We focus on features from two modalities  transcribed words and acoustics  and we compare the performance of three dif  ferent textual representations  words  charac  ters  and phonemes  Our experiments show that characterlevel features outperform word  level features for these tasks  and that a care  ful fusion of all features yields the best perfor mance  1 1 Introduction Opinions  sentiments and other types of subjective content are an important part of any meeting  Meet ing participants express pros and cons about ideas  they support or oppose decisions  and they make suggestions that may or may not be adopted  When recorded and archived  meetings become a part of the organizational knowledge  but their value is lim  ited by the ability of tools to search and summa  rize meeting content  including subjective content  While progress has been made on recognizing pri marily objective meeting content  for example  in formation about the topics that are discussed  Hsueh and Moore  2006  and who is assigned to work on given tasks  Purver et al   2006   there has </raw_string>
  </article>
  <article>
    <title>Handwritten Text Recognition for Historical Documents</title>
    <count>495</count>
    <raw_string>Proceedings of Language Technologies for Digital Humanities and Cultural Heritage Workshop  pages 9096  Hissar  Bulgaria  16 September 2011  Handwritten Text Recognition for Historical Documents Veronica Romero  Nicolas Serrano  Alejandro H Toselli  Joan Andreu Sanchez and Enrique Vidal ITI  Universitat Politecnica de Valencia  Spain vromero nserrano jandreu atoselli evidal iti upves Abstract The amount of digitized legacy documents has been rising dramatically over the last years due mainly to the increasing num  ber of online digital libraries publishing this kind of documents  The vast majority of them remain waiting to be transcribed into a textual electronic format  such as ASCII or PDF that would provide histori ans and other researchers new ways of in dexing  consulting and querying them  In this work  the stateoftheart Handwritten Text Recognition techniques are applied for the automatic transcription of these historical documents  We report results for several ancient documents  1 Introduction In the last years  huge amount of handwritten his torical documents residing in libraries  museums and archives have been digitalized and have been made available to the general public through spe cialized web portals  The vast majority of these documents  hundreds of terabytes worth of digital image data  remain waiting to be transcribed into a textual electronic format that would provide histo  rians and other researchers new ways of indexing  consulting and querying them  The automatic transcription of these ancient handwritten documents is still an incipient re  search field that has been started to be explored in recent years  For some time in the past decades  the interest in Offline Handwritten Text Recog nition  HTR  was diminishing  under the assump  tion </raw_string>
  </article>
  <article>
    <title>Tagging Sentence Boundaries in Biomedical Literature</title>
    <count>493</count>
    <raw_string>A Gelbukh  Ed    CICLing 2007  LNCS 4394  pp  186  195  2007   SpringerVerlag Berlin Heidelberg 2007 Tagging Sentence Boundaries in Biomedical Literature Weijian Xuan  Stanley J Watson  and Fan Meng Molecular and Behavioral Neuroscience Institute and Department of Psychiatry University of Michigan  Ann Arbor  Michigan 48109 mengfumich edu Abstract  Identifying sentence boundaries is an indispensable task for most natural language processing  NLP  systems  While extensive efforts have been devoted to mine biomedical text using NLP techniques  few attempts are specifically targeted at disambiguating sentence boundaries in biomedical literature  which has a number of unique features that can reduce the accuracy of algorithms designed for general English genre significantly  In order to increase the accuracy of sentence boundary identification for biomedical literature  we developed a method using a combination of heuristic and statistical strategies  Our approach does not require partofspeech taggers or training procedures  Experiments with biomedical test corpora show our system significantly outperforms existing sentence boundary determination algorithms  particularly for full text biomedical literature  Our system is very fast and it should also be easily adaptable for sentence boundary determination in scientific literature from nonbiomedical fields  1 Introduction High throughput experiment approaches  such as genomewide or organismwide expression profiling studies  significantly enrich biomedical literature and at the same time  make computerbased literature mining almost a necessity in biomedical research  Since most of the experiment results are still summarized and presented in free text format  automated methods for extracting relevant information in Medline as well as full length text literature can be very helpful for the understanding of biological significance of high throughput results  The Medline database 1 </raw_string>
  </article>
  <article>
    <title>Handwritten Text Recognition for Ancient Documents</title>
    <count>493</count>
    <raw_string>JMLR  Workshop and Conference Proceedings 11  2010  5865 Workshop on Applications of Pattern Analysis Handwritten Text Recognition for Ancient Documents Alfons Juan ajuaniti upves Veronica Romero vromeroiti upves Joan Andreu Sanchez jandreuiti upves Nicolas Serrano nserranoiti upves Alejandro H Toselli ahectoriti upves Enrique Vidal evidaliti upves Institut Tecnologic d Informatica Universitat Politecnica de Valencia Cam  de Vera sn  46022 Valencia  Spain Editors  Tom Diethe  Nello Cristianini  John ShaweTaylor Abstract Huge amounts of legacy documents are being published by online digital libraries world wide  However  for these raw digital images to be really useful  they need to be tran scribed into a textual electronic format that would allow unrestricted indexing  browsing and querying  In some cases  adequate transcriptions of the handwritten text images are already available  In this work three systems are presented to deal with this sort of docu ments  The first two address two different approaches for semiautomatic transcription of document images  The third system implements an alignment method to find mappings between word images of a handwritten document and their respective words in its given transcription  1  Introduction Huge historical document collections residing in libraries  museums and archives are cur rently being digitalized for preservation purposes and to make them available worldwide through large  online digital libraries  However  efforts should also focus on technologies aimed at reducing the human effort required for the annotation of the raw images with informative content  In the case of text images  which are among the most numerous and interesting  the most informative annotation level is their  paleographic  transcription into an adequate textual electronic format that would provide new ways of indexing  consulting </raw_string>
  </article>
  <article>
    <title>Performer Identification in Celtic Violin Recordings</title>
    <count>493</count>
    <raw_string>ISMIR 2008  Session 4b  Musical Expression and Meaning PERFORMER IDENTIFICATION IN CELTIC VIOLIN RECORDINGS Rafael Ramirez  Alfonso Perez and Stefan Kersten Music Technology Group Universitat Pompeu Fabra Ocata 1  08003 Barcelona  Spain Tel 34 935422864  Fax 34 935422202 rafael  aperez  skersten iua upf edu ABSTRACT We present an approach to the task of identifying perform  ers from their playing styles  We investigate how violinists express and communicate their view of the musical content of Celtic popular pieces and how to use this information in order to automatically identify performers  We study note  level deviations of parameters such as timing and amplitude  Our approach to performer identification consists of induc  ing an expressive performance model for each of the inter preters essentially establishing a performer dependent map  ping of internote features to a timing and amplitude expres  sive transformations   We present a successful performer identification case study  1 INTRODUCTION Music performance plays a central role in our musical cul ture today  Concert attendance and recording sales often reflect peoples preferences for particular performers  The manipulation of sound properties such as pitch  timing  am  plitude and timbre by different performers is clearly distin guishable by the listeners  Expressive music performance studies the manipulation of these sound properties in an at tempt to understand expression in performances  There has been much speculation as to why performances contain ex  pression  Hypothesis include that musical expression com municates emotions and that it clarifies musical structure  i e  the performer shapes the music according to her own intentions In this paper we focus on the task of identifying violin performers from their playing style using highlevel </raw_string>
  </article>
  <article>
    <title>Model Selection Strategies for Author Disambiguation</title>
    <count>492</count>
    <raw_string>Model Selection Strategies for Author Disambiguation Roman Kern Institute of Knowledge Management Graz University of Technology Graz  Austria rkerntugrazat Mario Zechner KnowCenter GmbH Graz  Austria mzechnerknowcenterat Michael Granitzer KnowCenter GmbH  Graz University of Technology Graz  Austria mgranitzertugrazat AbstractAuthor disambiguation is a prerequisite for uti lizing bibliographic metadata in citation analysis  Automatic disambiguation algorithms mostly rely on clusterbased disam  biguation strategies for identifying unique authors given their names and publications  However  most approaches rely on knowing the correct number of unique authors apriori  which is rarely the case in real world settings  In this publication we analyse clusterbased disambiguation strategies and develop a model selection method to estimate the number of distinct authors based on coauthorship networks  We show that  given clean textual features  the developed model selection method provides accurate guesses of the number of unique authors  Keywordsauthor disambiguation  model selection I INTRODUCTION Author disambiguation  i e  identifying unique authors given their names and publications  remains a core challenge for utilizing bibliographic metadata and citation management 1  Most disambiguation techniques tackling this problem rely on clustering methods 2  3  However  an often neglected problem in clustering  which is especially crucial for author disambiguation  is model selection  i e  detecting the number of unique authorscluster  In this publication we address the topic of clustering based author identification and disambiguation and present the following contributions   We developed a new model selection strategy based on authorship cooccurrence  We compare different clusterbased disambiguation strategies and feature combinations on real world data sets  We utilize web search engines to extend the features available for disambiguation As a result  our model selection strategy yields </raw_string>
  </article>
  <article>
    <title>Characterizing Stylistic Elements in Syntactic Structure</title>
    <count>492</count>
    <raw_string>Characterizing Stylistic Elements in Syntactic Structure Song Feng Ritwik Banerjee Yejin Choi Department of Computer Science Stony Brook University NY 11794  USA songfeng  rbanerjee  ychoics stonybrook edu Abstract Much of the writing styles recognized in rhetorical and composition theories involve deep syntactic elements  However  most previous research for computational sty lometric analysis has relied on shallow lexicosyntactic patterns  Some very re  cent work has shown that PCFG models can detect distributional difference in syn tactic styles  but without offering much in sights into exactly what constitute salient stylistic elements in sentence structure characterizing each authorship  In this paper  we present a comprehensive ex  ploration of syntactic elements in writing styles  with particular emphasis on inter pretable characterization of stylistic ele  ments  We present analytic insights with respect to the authorship attribution task in two different domains  1 Introduction Much of the writing styles recognized in rhetor ical and composition theories involve deep syn tactic elements in style  e g  Bain  1887   Kem  per  1987  Strunk and White  2008    However  previous research for automatic authorship at tribution and computational stylometric analy sis have relied mostly on shallow lexicosyntactic patterns  e g  Mendenhall  1887   Mosteller and Wallace  1984   Stamatatos et al   2001   Baayen et al   2002   Koppel and Schler  2003   Zhao and Zobel  2007   Luyckx and Daelemans  2008    Some very recent works have shown that PCFG models can detect distributional differ ence in sentence structure in gender attribution  Sarawgi et al   2011   authorship </raw_string>
  </article>
  <article>
    <title>Extraction and Classification of Handwritten Annotations</title>
    <count>492</count>
    <raw_string>Extraction and Classification of Handwritten Annotations Andrea Mazzei CRAFT  EPFL Rolex Learning Center Station 20 CH1015 Lausanne andreamazzeiepflch Frederic Kaplan CRAFT  EPFL Rolex Learning Center Station 20 CH1015 Lausanne frederickaplanepflch Pierre Dillenbourg CRAFT  EPFL Rolex Learning Center Station 20 CH1015 Lausanne pierre dillenbourgepflch ABSTRACT This article describes a method for extracting and classify ing handwritten annotations on printed documents using a simple camera integrated in a lamp or a mobile phone  The ambition of such a research is to offer a seamless integra  tion of notes taken on printed paper in our daily interactions with digital documents  Existing studies propose a classifi cation of annotations based on their form and function  We demonstrate a method for automating such a classification and report experimental results showing the classification accuracy  Author Keywords machineprinted and handwritten text separation  Document processing  annotation classification ACM Classification Keywords H52 Information interfaces and presentation  e g  HCI  Miscellaneous  General Terms Algorithms  Experimentation  Human Factors  Languages  Measurement  Performance  Reliability  Theory INTRODUCTION Annotating and taking notes on paper is a very common practice in our daily routines  Readers write comments in the margins of papers  underline important passages and use other various marking strategies  These practices help them to understand better what they read and  at a later stage  find back easier relevant passages  It plays also an important role for associative thinking and linking the content with other ideas and documents  Despite the efforts to transfer anno  tating practices to digital documents  annotating on paper has many advantages compared to any electronic equivalent  Kawase et al  6   This article describes a method for extracting </raw_string>
  </article>
  <article>
    <title>Using Relative Entropy for Authorship Attribution</title>
    <count>492</count>
    <raw_string>Using Relative Entropy for Authorship Attribution Ying Zhao  Justin Zobel  and Phil Vines School of Computer Science and Information Technology  RMIT University GPO Box 2476V  Melbourne  Australia yizhao  jz  philcs rmit eduau Abstract  Authorship attribution is the task of deciding who wrote a particular document  Several attribution approaches have been pro posed in recent research  but none of these approaches is particularly satisfactory  some of them are ad hoc and most have defects in terms of scalability  effectiveness  and efficiency  In this paper  we propose a prin cipled approach motivated from information theory to identify authors based on elements of writing style  We make use of the KullbackLeibler divergence  a measure of how different two distributions are  and explore several different approaches to tokenizing documents to extract style markers  We use several data collections to examine the performance of our approach  We have found that our proposed approach is as effec tive as the best existing attribution methods for two class attribution  and is superior for multiclass attribution  It has lower computational cost and is cheaper to train  Finally  our results suggest this approach is a promising alternative for other categorization problems  1 Introduction Authorship attribution  AA  is the problem of identifying who wrote a particular document  AA techniques  which are a form of document classification  rely on collections of documents of known authorship for training  and consist of three stages  preprocessing of documents  extraction of style markers  and classification based on the style markers  Applications of AA include plagiarism detection  doc  ument tracking  and forensic and literary investigations  Researchers have </raw_string>
  </article>
  <article>
    <title>Metareasoning About Propagators for Constraint Satisfaction</title>
    <count>491</count>
    <raw_string>Metareasoning About Propagators for Constraint Satisfaction A Thesis Submitted to the College of Graduate Studies and Research in Partial Fulfillment of the Requirements for the degree of Master of Science in the Department of Computer Science University of Saskatchewan Saskatoon By Craig DS Thompson Craig DS Thompson  May 2011  All rights reserved  Permission to Use In presenting this thesis in partial fulfilment of the requirements for a Postgraduate degree from the University of Saskatchewan  I agree that the Libraries of this University may make it freely available for inspection  I further agree that permission for copying of this thesis in any manner  in whole or in part  for scholarly purposes may be granted by the professor or professors who supervised my thesis work or  in their absence  by the Head of the Department or the Dean of the College in which my thesis work was done  It is understood that any copying or publication or use of this thesis or parts thereof for financial gain shall not be allowed without my written permission  It is also understood that due recognition shall be given to me and to the University of Saskatchewan in any scholarly use which may be made of any material in my thesis  Requests for permission to copy or to make other use of material in this thesis in whole or part should be addressed to  Head of the Department of Computer Science 176 Thorvaldson Building 110 Science Place University of Saskatchewan Saskatoon  Saskatchewan Canada S7N 5C9 i Abstract Given the breadth of constraint satisfaction problems  CSPs  and the wide variety of CSP solvers  it is often very difficult to determine a priori which solving method is best suited to a </raw_string>
  </article>
  <article>
    <title>Authorship Attribution Based on Specific Vocabulary</title>
    <count>491</count>
    <raw_string>12 Authorship Attribution Based on Specific Vocabulary JACQUES SAVOY  University of Neuchatel In this article we propose a technique for computing a standardized Z score capable of defining the specific vocabulary found in a text  or part thereof  compared to that of an entire corpus  Assuming that the term occurrence follows a binomial distribution  this method is then applied to weight terms  words and punc  tuation symbols in the current study   representing the lexical specificity of the underlying text  In a final stage  to define an author profile we suggest averaging these text representations and then applying them along with a distance measure to derive a simple and efficient authorship attribution scheme  To evaluate this algorithm and demonstrate its effectiveness  we develop two experiments  the first based on 5408 newspaper articles  Glasgow Herald  written in English by 20 distinct authors and the second on 4326 newspaper articles  La Stampa  written in Italian by 20 distinct authors  These experiments demonstrate that the suggested classification scheme tends to perform better than the Delta rule method based on the most frequent words  better than the chisquare distance based on word profiles and punctuation marks  better than the KLD scheme based on a predefined set of words  and better than the nave Bayes approach  Categories and Subject Descriptors  I27 Natural Language Processing  Text Analysis  H31 Content Analysis and Indexing  Linguistic Processing  H37 Digital Libraries  General Terms  Performance  Experimentation Additional Key Words and Phrases  Authorship attribution  text classifiaction  lexical statistics ACM Reference Format  Savoy  J 2012  Authorship attribution based on specific vocabulary  ACM Trans  Inf  </raw_string>
  </article>
  <article>
    <title>On Information Need and Categorizing Search</title>
    <count>491</count>
    <raw_string>On Information Need and Categorizing Search Faculty of Electrical EngineeringComputer Scienceand Mathematics Department of Computer Science of the University of PaderbornGermany The accepted dissertation of Sven Meyer zu Eien in partial fulfillment of the requirements to obtain the academic degree of Drrernatii ReviewerProfDrBenno Stein ProfDrNorbert Fuhr ProfDrHans Kleine Buning Date of Oral ExamFebruary 232007 Acknowledgements I wish to thank ProfDrHans Kleine Buning for supporting me and my workHe provided a stimulating environment and chaired the reading committeeI want to express my gratitude to ProfDrBenno Stein for supervising this thesisand for numerous fruitfulseriousand enthusiastic discussions that initiated or improved many of the presented ideas and resultsI am indepted to his guidancepatienceand mentorshipFurthermoreI wish to thank ProfDrNorbert Fuhr who evaluated this thesis and provided useful comments and suggestions for improvementFor many valuable discussions and enjoyable breaks I want to thank all of my  formercolleaguesin particular DrAndreas GobelsOliver KramerDrTheo LettmannDrOliver Niggemannand Martin PotthastFinally I would like to thank all of my friends and my whole familyespecially my wife LisaSven Meyer zu Eien iiiTable of Contents Notation vii 1 Introduction 1 11 Thesis Contributions 4 12 Thesis Overview 7 2 Document Representation and Retrieval 9 21 The Information Retrieval Process 10 22 Document Representation 11 221 Term Vector Models 12 222 Document Similarity 16 223 Index Term Set Construction Methods 20 23 Techniques for Information Need Satisfaction 23 3 Information Need and Categorizing Search 27 31 Supervised vsUnsupervised Categorization 29 32 The Cluster Hypothesis 30 33 Clustering Documents 31 331 Challenges 32 332 Clustering Algorithms 34 34 On the Validity of Document Clusterings 37 341 External Cluster Validity Measures 38 342 Internal Cluster Validity Measures 40 343 Statistical Hypothesis Testing 46 344 Experimental Evaluation 48 345 Concluding Remarks 53 35 The Suffix Tree Document Representation 54 351 Suffix Trees 55 v vi TABLE </raw_string>
  </article>
  <article>
    <title>Adaptive Enhancement of Historical Document Images</title>
    <count>491</count>
    <raw_string>2007 IEEE International Symposium on Signal Processing and Information Technology Adaptive Enhancement of Historical Document Images N Krishna Kishore Priti P Rege kishoren2gmaiiLcom pprextccoeporgJn Department of Electronics and Telecommunications College of Engineering  Pune  India Abstract  In this paper we present a simple and 2 Literature review on document image computationally efficient method for document binarization techniques image enhancement  We use Unsharp masking to enhance the edge detail information in the degraded Previous image enhancement algorithms for historical document  This image is then used to adjust the local documents have been designed primarily for threshold for each pixel  Proposed method is segmentation of textual content from the background experimentally compared with Laplacian sign method of the images  Binarization of gray scale image has and the Otsu method  It is shown that the method been the most important subject of intense research improves sharpness of the image with nearly half interest during last few years  Most of the commonly computational time  observed techniques focus on one aspect of choosing threshold either globally or locally  The global 1  Introduction threshold selection methods assume the graylevel histogram to be bimodal and then choose a single There is an increasing need to digitally preserve and threshold at valley point to label pixels into provide access to historical document collections foreground or background classes  residing  and possibly decaying  in libraries  museums and archives  These documents are like ancient Otsu s method is an early  but still popular histogram manuscripts through early printed books to type based technique  Because of its inefficient formulation written administrative documents of the twentieth of the betweenclass variance  the method is slow and century  They contain important information and also the </raw_string>
  </article>
  <article>
    <title>An Extremely Simple Authorship Attribution System</title>
    <count>491</count>
    <raw_string>Proceedings of the Second European IAFL Conference on Forensic Linguistics  Language and the Law  Barcelona 2006  An Extremely Simple Authorship Attribution System Rogelio Nazar rogelionazarupf edu Marta Snchez Pol marta sanchezupf edu Institut Universitari de Lingstica Aplicada Universitat Pompeu Fabra Pl  de la Merc 1012 08002 Barcelona Abstract  In this paper we present a very simple yet effective algorithm for authorship attribution  By this term we mean the act of telling whether a certain text was or was not written by a certain author  We shall not discuss the advantages or applications of this activity  but we propose a method for doing it in an automatic and instantaneous way  neither considering the language of the texts nor undertaking any kind of text preprocessing like tokenization or part of speech tagging  We have conducted an experiment that shows how authorship attribution can be seen as a text categorization problem  That is to say  each author represents a category and the documents are the elements to be classified  Text categorization has became a very popular issue in computational linguistics and it has developed to great complexity  motivating a huge amount of literature  However  in this article we show a basic method applied to authorship attribution  This program is language independent because it uses purely mathematical knowledge  an ngram model of texts  It works in a very simple way and is therefore easy to modify  In spite of its simplicity  this program is capable of classifying documents by author obtaining more than 90  of accuracy  As an example  we present an experiment carried out with a rather homogeneous corpus composed of 100 short newspaper articles around 400 words each </raw_string>
  </article>
  <article>
    <title>Assessment on Stylometry for Multilingual Manuscript</title>
    <count>491</count>
    <raw_string>IOSR Journal of Engineering  IOSRJEN  eISSN  22503021  pISSN  22788719  wwwiosrjen org Volume 2  Issue 9  September 2012   PP 0106 wwwiosrjen org 1  P a g e Assessment on Stylometry for Multilingual Manuscript Sushil Kumar 1  Mousmi A Chaurasia 2 1Department of Electrical  Electronics  BIT Durg  CG  INDIA  2Research Scholar  INDIA  Member ACM ABSTRACT Linguistics and stylistics have been studied for author identification  verification for few years but recently  we have testified a remarkable development in the quantity with which lawyers  courts  applicable in cyber crime  detective agencies etc  etc  have called upon the expertise of linguists in cases of disputed authorship  This inspires researchers to look to the problem of author identification  verification from a different outlook  This paper pact shows text author verification problem using character ngram information  final ngram  initial ngram  for both English  Arabic Text  Experiments demonstrate that author profiles generated with initial bigram and initial trigram are effective in verifying texts authors  A doorsill value has been set using dissimilarity measure that separate dissimilarity of same author texts from texts written by different authors  KEYWORDS Author Verification  Character Ngram  Dissimilarity measure I INTRODUCTION In the typical authorship attribution problem  a text of unknown authorship is assigned to one author  given a set of authors for whom text samples of undisputed authorship are available  The authorship attribution is a process of an anonymous text authorship recognition based on text samples written by a group of already known authors  It relies on a set of features deduced from text documents  and attempts to establish whether </raw_string>
  </article>
  <article>
    <title>Practical Attacks Against Authorship Recognition Techniques</title>
    <count>490</count>
    <raw_string>Practical Attacks Against Authorship Recognition Techniques Michael Brennan  Rachel Greenstadt Dept  of Computer Science  Drexel University mb553greeniecs drexel edu AbstractThe use of statistical AI techniques in authorship recognition  or sty lometry  has contributed to literary and historical breakthroughs  These successes have led to the use of these techniques in criminal investigations and prosecutions  However  few have studied adversarial attacks  motivated by a desire to protect anonymity and privacy in a variety of scenarios  and their devastating effect on the robustness of existing classification methods  This paper presents a framework for adversarial attacks including obfuscation attacks  where a subject attempts to hide their identity and imitation attacks  where a subject attempts to frame another subject by imitating their writing style  The major contribution of this research is that it demonstrates that both attacks work very well  The obfuscation attack reduces the effectiveness of the techniques to the level of random guess  ing and the imitation attack succeeds with 6891  probability depending on the stylometric technique used  These results are made more significant by the fact that the experimental subjects were unfamiliar with stylometric techniques  with  out specialized knowledge in linguistics  and spent little time on the attacks  This paper also provides another significant contribution to the field in using human subjects to empirically validate the claim of high accuracy for current techniques  without attacks  by reproducing results for three representative stylometric meth  ods  Current work based on these results that looks deeper into implications of stylometry on privacy and anonymity on the Internet is also discussed  1 Introduction  Disclaimer  this work has been accepted for publication in Innovative Appli cations of Artificial </raw_string>
  </article>
  <article>
    <title>PCA Document Reconstruction for Email Classification</title>
    <count>490</count>
    <raw_string>Computational Statistics and Data Analysis 56  2012  741751 Contents lists available at SciVerse ScienceDirect Computational Statistics and Data Analysis journal homepage  wwwelseviercomlocatecsda PCA document reconstruction for email classification Juan Carlos Gomez   MarieFrancine Moens KULEUVEN  Computer Science Department  Celestijnenlaan 200A  B3001 Heverlee  Belgium a r t i c l e i n f o Article history  Received 9 February 2011 Received in revised form 18 July 2011 Accepted 20 September 2011 Available online 1 October 2011 Keywords  Class representation PCA Email filtering Feature extraction a b s t r a c t This paper presents a document classifier based on text content features and its application to email classification  We test the validity of a classifier which uses Principal Component Analysis Document Reconstruction PCADR  where the idea is that principal component analysis PCA  can compress optimally only the kind of documents  in our experiments email classes  that are used to compute the principal components PCs   and that for other kinds of documents the compression will not perform well using only a few components  Thus  the classifier computes separately the PCA for each document class  and when a new instance arrives to be classified  this new example is projected in each set of computed PCs corresponding to each class  and then is reconstructed using the same PCs  The reconstruction error is computed and the classifier assigns the instance to the classwith the smallest error or divergence from the class representation  We test this approach in email filtering by distinguishing between two message classes  e g  spam from ham  or phishing from ham   The experiments show that PCADR is able to obtain very </raw_string>
  </article>
  <article>
    <title>Practical Attacks Against Authorship Recognition Techniques</title>
    <count>490</count>
    <raw_string>Practical Attacks Against Authorship Recognition Techniques Michael Brennan and Rachel Greenstadt Dept  of Computer Science Drexel University 3175 JFK Blvd Room 140 Philadelphia  PA 19104 tel 2158952920 fax 2158950545  mb553drexel edu  greeniecs drexel edu  Emerging Applications Track Application Domain  Authorship recognition in adversarial settings  AI Techniques  Statistical machine learning  neural networks  stylometry Issues addressed  Applicability of stylometric techniques to situations where they might come under adversarial attack  for example  forensic analysis  Application Status  Feasibility analysis of deployed applications  and some research prototypes  for authorship recognition tools for adversarial settings Abstract The use of statistical AI techniques in authorship recognition  or stylometry  has contributed to literary and historical breakthroughs  These successes have led to the use of these techniques in criminal investigations and prosecutions  However  few have studied adversar ial attacks and their devastating effect on the robustness of existing classification methods  This paper presents three key contributions to address this shortcoming  First  it uses human subjects to empirically validate the claim of high accuracy for current techniques  without attacks  by reproducing results for three representative stylometric methods  Secondly  it presents a framework for adversarial attacks including obfuscation attacks  where a subject attempts to hide their identity and imitation attacks  where a subject attempts to frame another subject by imitating their writing style  Finally  it demonstrates that both attacks work well  The obfuscation attack reduces the effectiveness of the techniques to the level of random guessing and the imitation attack succeeds with 6891  probability depending on the stylometric technique used  These results are made more significant by the fact that the experimental subjects were unfamiliar </raw_string>
  </article>
  <article>
    <title>Practical Attacks Against Authorship Recognition Techniques</title>
    <count>490</count>
    <raw_string>Practical Attacks Against Authorship Recognition Techniques Michael Brennan  Rachel Greenstadt Dept  of Computer Science  Drexel University mb553greeniecs drexel edu AbstractThe use of statistical AI techniques in authorship recognition  or sty lometry  has contributed to literary and historical breakthroughs  These successes have led to the use of these techniques in criminal investigations and prosecutions  However  few have studied adversarial attacks  motivated by a desire to protect anonymity and privacy in a variety of scenarios  and their devastating effect on the robustness of existing classification methods  This paper presents a framework for adversarial attacks including obfuscation attacks  where a subject attempts to hide their identity and imitation attacks  where a subject attempts to frame another subject by imitating their writing style  The major contribution of this research is that it demonstrates that both attacks work very well  The obfuscation attack reduces the effectiveness of the techniques to the level of random guess  ing and the imitation attack succeeds with 6891  probability depending on the stylometric technique used  These results are made more significant by the fact that the experimental subjects were unfamiliar with stylometric techniques  with  out specialized knowledge in linguistics  and spent little time on the attacks  This paper also provides another significant contribution to the field in using human subjects to empirically validate the claim of high accuracy for current techniques  without attacks  by reproducing results for three representative stylometric meth  ods  Current work based on these results that looks deeper into implications of stylometry on privacy and anonymity on the Internet is also discussed  1 Introduction  Disclaimer  this work has been accepted for publication in Innovative Appli cations of Artificial </raw_string>
  </article>
  <article>
    <title>Comparing Compression Models for Authorship Attribution</title>
    <count>489</count>
    <raw_string>Forensic Science International 228  2013  100104Comparing compression models for authorship attribution W Oliveira Jra  E Justino a  LS Oliveira b  a Pontifical Catholic University of Parana  PUCPR   R Imaculada Conceicao  1155 Curitiba  PR  Brazil b Federal University of Parana  UFPR   R Rua Cel  Francisco H dos Santos  100 Curitiba  PR 81531990  Brazil A R T I C L E I N F O Article history  Received 16 April 2012 Received in revised form 12 February 2013 Accepted 13 February 2013 Available online Keywords  Compression models Authorship attribution Questioned documents A B S T R A C T In this paper we compare different compression models for authorship attribution  To this end  three different types of compressors  LempelZiv type  GZip   block sorting type  BZip  and statistical type  PPM  along with two different similarity measures were considered in our experiments  Besides  two different attribution methods are analyzed in this paper  Through a series of experiments performed on two different databases  we were able to show that all the compressors behave similarly  but the similarity measures can vary considerably depending on the strategy used for authorship attribution  Our results corroborate with the literature in the sense that compression models are a good alternative for authorship attribution surpassing traditional pattern recognition systems based on classifiers and feature extraction  2013 Elsevier Ireland Ltd All rights reserved  Contents lists available at SciVerse ScienceDirect Forensic Science International jou r nal h o mep age  w wwels evier  co mlo c ate  fo r sc i in t1  Introduction Authorship attribution can be defined as the </raw_string>
  </article>
  <article>
    <title>Historical Document Enhancement Using LUT Classification</title>
    <count>489</count>
    <raw_string>IJDAR  2010  13 117 DOI 101007s1003200900993 ORIGINAL PAPER Historical document enhancement using LUT classification Tayo ObafemiAjayi  Gady Agam  Ophir Frieder Received  21 May 2008  Revised  16 July 2009  Accepted  9 October 2009  Published online  17 November 2009  SpringerVerlag 2009 Abstract The fast evolution of scanning and computing technologies in recent years has led to the creation of large collections of scanned historical documents  It is almost always the case that these scanned documents suffer from some form of degradation  Large degradations make documents hard to read and substantially deteriorate the performance of automated document processing systems  Enhancement of degraded document images is normally performed assuming global degradation models  When the degradation is large  global degradation models do not per form well  In contrast  we propose to learn local degrada  tion models and use them in enhancing degraded document images  Using a semiautomated enhancement system  we have labeled a subset of the Frieder diaries collection  The diaries of Rabbi Dr Avraham Abba Frieder  http iriit edu collections   This labeled subset was then used to train clas  sifiers based on lookup tables in conjunction with the approx  imated nearest neighbor algorithm  The resulting algorithm is highly efficient and effective  Experimental evaluation results are provided using the Frieder diaries collection  The dia  ries of Rabbi Dr Avraham Abba Frieder  http iriit edu collections   1 Introduction Historical document collections are often very poor in qual ity and suffer from some form of degradation  Many of these documents have deteriorated due to the age of paper and ink used  They currently exist electronically as scanned T ObafemiAjayi  </raw_string>
  </article>
  <article>
    <title>Classifying Foreground Pixels in Document Images</title>
    <count>489</count>
    <raw_string>Classifying foreground pixels in document images Prateek Sarkar  Eric Saund  Jing Lin Perceptual Document Analysis Palo Alto Research Center  Palo Alto  CA  USA psarkar saund jlinparccom Abstract We present a system that classifies pixels in a document image according to marking type such as machine print  handwriting  and noise  A segmenter module first splits an input image into fragments  sometimes breaking connected components  Each fragment is then classified by an auto matically trained multistage classifier that is fast and con siders features of the fragment  as well as its neighborhood  Features relevant for discrimination are picked out auto matically from among hundreds of measurements  Our sys  tem is trainable from example images in which each fore ground pixel has a groundtruth  label  The main distinc  tion of our system is the level of accuracy achieved in clas  sifying fragments at subconnected component level  rather than larger aggregate groups such as words or textlines  We have trained this system to detect handwriting  machine print text  machine print graphics  and noise  1 Introduction Markings in document images may come from a vari ety of sources  Machine print text  line graphics  block graphics   hand writing text  line graphics  shading   stamp marks  shadows  page folds  page curl  punch holes  pa per texture  are some of the different sources of marks on a page  Once a page has been scanned  these markings are flattened to an image of pixels  To human perception the various kinds of markings still stand out as separate entities  This helps us read documents even in the presence </raw_string>
  </article>
  <article>
    <title>Automatic Classification of Folk Narrative Genres</title>
    <count>488</count>
    <raw_string>Automatic classification of folk narrative genres Dong Nguyen1  Dolf Trieschnigg1  Theo Meder2  Mariet Theune1 1University of Twente  Enschede  The Netherlands 2Meertens Institute  Amsterdam  The Netherlands d nguyen dtrieschnigg utwente nl theomedermeertensknawnl mtheuneutwente nl Abstract Folk narratives are a valuable resource for humanities and social science researchers  This paper focuses on automatically recog nizing folk narrative genres  such as urban legends  fairy tales  jokes and riddles  We explore the effectiveness of lexical  struc  tural  stylistic and domain specific features  We find that it is possible to obtain a good performance using only shallow features  As dataset for our experiments we used the Dutch Folktale database  containing narra  tives from the 16th century until now  1 Introduction Folk narratives are an integral part of cultural her itage and a valuable resource for historical and contemporary comparative folk narrative studies  They reflect moral values and beliefs  and identi ties of groups and individuals over time  Meder  2010   In addition  folk narratives can be studied to understand variability in transmission of narra  tives over time  Recently  much interest has arisen to increase the digitalization of folk narratives  e g  Meder  2010   La Barre and Tilley  2012   Abello et al   2012    In addition  natural language process  ing methods have been applied to folk narrative data  For example  fairy tales are an interest ing resource for sentiment analysis  e g  Moham  mad  2011   Alm et al   2005   and methods have been explored to identify similar fairy tales  Lobo </raw_string>
  </article>
  <article>
    <title>Tools for Analysis of Musical Expression</title>
    <count>487</count>
    <raw_string>19th INTERNATIONAL CONGRESS ON ACOUSTICS MADRID  27 SEPTEMBER 2007 TOOLS FOR ANALYSIS OF MUSICAL EXPRESSION PACS  4375St Dixon  Simon1 1 Queen Mary  Univ of London  Mile End Rd  London  E1 4NS  UK  simondixonelecqmul acuk ABSTRACT Studies of expressive music performance require precise measurements of the parameters  such as timing  dynamics and articulation  of individual notes and chords  Particularly in the case of the great performers  the only data usually available to researchers are audio recordings and the score  and digital signal processing techniques are employed to estimate the higher level control parameters from the audio signal  In this paper  two systems for extraction of timing information from audio recordings are described  The first system is an interactive beat tracking and anno  tation system called BeatRoot  which estimates the times of the beats in the music by finding regularities in the timing of note onsets using a multiagent architecture  The second system  MATCH  performs alignment  or synchronisation  of different versions of the same piece of music  computing an index of corresponding locations in the different recordings using an efficient time warping algorithm  MATCH can be used to automatically transfer contentbased metadata from one recording to another  or to follow a live performance  for example in order to automatically turn pages for a musician  Both systems are equipped with an intuitive graphical user interface and are being used by musicologists in largescale studies of performance  INTRODUCTION A musical composition  expressed as a score  is only an approximation of what is performed on stage  It is like a sketch  where the fine details are left to the performer </raw_string>
  </article>
  <article>
    <title>PCA Document Reconstruction for Email Classification</title>
    <count>486</count>
    <raw_string>Accepted Manuscript PCA document reconstruction for email classification JC Gomez  MF Moens PII  S01679473 11003549 DOI  101016jcsda201109023 Reference  COMSTA 5119 To appear in  Computational Statistics and Data Analysis Received date  9 February 2011 Revised date  18 July 2011 Accepted date  20 September 2011 Please cite this article as  Gomez  JC  Moens  MF  PCA document reconstruction for email classification  Computational Statistics and Data Analysis  2011   doi101016jcsda201109023 This is a PDF file of an unedited manuscript that has been accepted for publication  As a service to our customers we are providing this early version of the manuscript  The manuscript will undergo copyediting  typesetting  and review of the resulting proof before it is published in its final form  Please note that during the production process errors may be discovered which could affect the content  and all legal disclaimers that apply to the journal pertain  PCA Document Reconstruction for Email Classification JC Gomez  MF Moens  KULEUVEN  Computer Science Department  Celestijnenlaan 200A  B3001 Heverlee  Belgium Abstract This paper presents a document classifier based on text content features and its application to email classification  We test the validity of a classifier which uses Principal Component Analysis Document Reconstruction PCADR  where the idea is that principal component analysis PCA  can compress optimally only the kind of documents  in our experiments email classes  that are used to compute the principal components PCs   and that for other kinds of documents the compression will not perform well using only a few components  Thus  the classifier computes separately the PCA for each document class  and when a new instance arrives to </raw_string>
  </article>
  <article>
    <title>Musician Identification with Tempo Performance Data</title>
    <count>486</count>
    <raw_string>Musician Identification with Tempo Performance Data John Bass and David McAllister  PhD Department of Computer Science North Carolina State University Raleigh  NC 27695  USA ABSTRACT Musician identification is explored by analyzing MIDI note onset data using a human timing pro duction pattern discovered by Collyer  Broadbent  and Church in 1992  Calculating the coefficient of determination after removing common traits is found to be an effective method to determine if a musician is a member of a set of known musicians and if so  identify the musician  Keywords  Musician Identification  Performance Data  Multimedia Analysis  Music Retrieval 1 INTRODUCTION Research described here explores musician identifica  tion by analyzing MIDI note onset data using a human timing production pattern discovered by Collyer  Broad bent  and Church in 1992 7  Based on this pattern  the experiment described by Collyer  et al  is modified and the resulting data is used to test an identification method using standard statistical methods  Calculating the coef  ficient of determination after removing common traits is found to be an effective method to determine if a musician is a member of a set of known musicians and if so  identify the musician  Section 2 describes the pattern discovered by Collyer et al  Section 3 describes an experiment based on the experiment developed by Collyer et al  This section also describes and tests a method for identifying a subject from their tempo performance data as well as test the effective  ness to determine if the subject is a member of a known set of subjects  Section 4 summarizes the findings and explores some followon research opportunities  The ability to algorithmically identify a musician using </raw_string>
  </article>
  <article>
    <title>A Shallow Approach to Subjectivity Classification</title>
    <count>486</count>
    <raw_string>A Shallow Approach to Subjectivity Classification Stephan Raaijmakers  Wessel Kraaij TNO Information and Communication Technology Delft The Netherlands stephan raaijmakers wesselkraaij tnonl Abstract We present a shallow linguistic approach to subjectivity clas  sification  Using multinomial kernel machines  we demon  strate that a data representation based on counting character ngrams is able to improve on results previously attained on the MPQA corpus using wordbased ngrams and syntactic information  We compare two types of stringbased repre  sentations  key substring groups and character ngrams  We find that wordspanning character ngrams significantly re  duce the bias of a classifier  and boost its accuracy 1 Introduction Subjectivity classification involves the discrimination be tween subjective and objective utterances  like sentences  or even phrases  Subjective utterances reflect a private point of view  emotion or belief  Recognition of subjectivity is im  portant from several points of view  Pang and Lee  2004  have shown that removing objective sentences from text prior to applying sentiment classification yields higher clas  sification accuracy  Subjectivity classification is important for product review mining  Kim and Hovy  2006   Both summarization and information extraction  Stoyanov and Cardie  2006   Riloff et al   2005   benefit from an ade quate discrimination between subjective and objective con tent  Stamatos  2006  has shown that shallow linguistic representations capture important linguistic aspects of ut terances  The LingPipe suite2 uses character ngrams for sentiment classification with good results  We investigate character ngrams for subjectivity classification  Character ngrams For the sentence This car really rocks  subword character bigrams and trigrams  subgrams  are th  hi  is  ca  ar </raw_string>
  </article>
  <article>
    <title>Classifying Business Marketing Messages on Facebook</title>
    <count>486</count>
    <raw_string>Classifying Business Marketing Messages on Facebook Bei Yu School of Information Studies Syracuse University 13154433614 byusyredu Linchi Kwok David B Falk College of Sport and Human Dynamics Syracuse University 13154432612 lkwoksyredu ABSTRACT Companies are increasingly using social media for marketing purposes  In this study we first demonstrate that although the majority of company posts on Facebook are aimed for direct sales and promotions  it is their communication messages that received the most attention from customers  We then trained an SVM classifier to automatically separate these two kinds of messages  hoping to use this tool to analyze messages from many companies and consequently monitor the evolution of their social media use over time  We found that the classifier trained with tfidf weighted partofspeech features performed best  It is better than classifiers trained with word features  Combining feature sets did not improve the performance  Feature ranking results show that this bestperformed classifier captured the genre characteristics of direct marketing and communication messages  Categories and Subject Descriptors H31  Information Storage and Retrieval  Content Analysis and Indexing  linguistic processing  General Terms Algorithms  Measurement  Design  Experimentation  Keywords Social media  advertisement  marketing  communication  machine learning  feature selection  text classification  text categorization  1  INTRODUCTION Social media can have significant impacts on consumer behaviors  and companies are increasingly using social media for marketing purposes 12  Facebook has become one of the most dominant media for B2C  businesstocusumer  and C2C  consumerto  consumer  communications 1416  Today  many websites embed Facebooks Like button  Popularity of a company s Facebook messages could be important in indicating the effectiveness of the company s social media strategies  </raw_string>
  </article>
  <article>
    <title>Applying Biometric Principles to Avatar Recognition</title>
    <count>486</count>
    <raw_string>Applying Biometric Principles to Avatar Recognition Marina L Gavrilova Department of Computer Science University of Calgary  CANADA marinacpscucalgary ca Roman V Yampolskiy Computer Engineering and Computer Science University of Louisville  USA romanyampolskiylouisville edu AbstractDomestic and industrial robots  intelligent software agents  virtual world avatars and other artificial entities are quickly becoming a part of our everyday life  Just like it is necessary to accurately authenticate identity of human beings  it is becoming essential to be able to determine identities of nonbiological agents  In this paper  we present the current state of the art in virtual reality security  focusing specifically on emerging methodologies for avatar authentication  We also outline future directions and potential applications for this high impact research field  Keywordsbiometric  avatar  recognition  robot  artimetrics I INTRODUCTION Domestic and industrial robots  intelligent software agents  virtual world avatars and other artificial entities are quickly becoming a part of our everyday life  Just like it is necessary to be able to accurately authenticate identity of human beings  it is becoming essential to be able to determine identity of the nonbiological entities rapidly infiltrating all aspects of modern society  Military soldier robots 27  robots museum guides 6  software office assistants 7  humanlike biped robots 35  office robots 2  bots 44  robots with humanlike faces 31  virtual world avatars 57 and thousands of other manmade entities all have something in common  a pressing need for a decentralized  affordable  automatic  fast  secure  reliable  and accurate means of identity authentication  To address these concerns  we proposed 62  65  64 the concept of Artimetrics  a field of study that will </raw_string>
  </article>
  <article>
    <title>Using Genres to Improve Search Engines</title>
    <count>486</count>
    <raw_string>Using Genres to Improve Search Engines Vedrana Vidulin Joef Stefan Institute Jamova 39  1000 Ljubljana Slovenia vedrana vidulinijs si Mitja Lutrek Joef Stefan Institute Jamova 39  1000 Ljubljana Slovenia mitjalustrekijs si Matja Gams Joef Stefan Institute Jamova 39  1000 Ljubljana Slovenia matjazgamsijs si Abstract Modern search engines are typically queried with keywords  which foremostly convey the topic of the sought web page  Consequently the resulting top hits are often topically relevant  but nonetheless not what the user wants  The premise of this paper is that the relevance of the hits can be improved when also searching by genre  classification criterion orthogonal to topic  To this end a genre classifier was built using machine learning methods  It was used in web page retrieval to filter out the hits not belonging to the desired genre  This approach considerably improved the relevance of the top ten hits  which indicates that genre classifier can be a useful addition to search engines  Keywords genre classifier  search engine  multilabeled classification  web page retrieval 1  INTRODUCTION Modern search engines rely on queries composed of keywords and ranking algorithms to retrieve web pages and to rank them by relevance 21  The problem with keyword search is that it often cannot precisely capture the users intent  For example  searching for the keyword elephant  will result in a list of web pages describing the life of elephants in various levels of detail  safari picture galleries  newspaper articles about saving the elephants in Africa etc  If the user is interested only in scientific papers on the life of elephants  specifying the genre as scientific would give more precise results  A web page is used to </raw_string>
  </article>
  <article>
    <title>Author Identification in Bengali Literary Works</title>
    <count>486</count>
    <raw_string>Author Identification in Bengali Literary Works Suprabhat Das and Pabitra Mitra Department of Computer Science and Engineering Indian Institute of Technology Kharagpur West Bengal  Pin  721302  India suprabhat pabitracse iitkgpernet in Abstract  In this paper  we study the problem of authorship identifi cation in Bengali literary works  We considered three authors namely Rabindranath Tagore  Bankim Chandra Chattopadhyay and Sukanta Bhattacharyay  It was observed that simple unigram and bigram fea  tures along with vocabulary richness were rich enough to discriminate amongst these authors  Although results degraded slightly when train ing set size was considerably small  For larger training set  a classification accuracy of above 90  for unigram feature and almost 100  for bigram feature was achieved  Results could be improved further by using more sophisticated features  Keywords  Stylometry  authorship attribution  Bengali literary works  unigram  bigram  1 Introduction Stylometry is the study of the unique linguistic styles and writing behaviors of individuals  Author identification is one of the important problems in stylomet rics and it can be seen as a singlelabel multiclass text categorization problem  It has many academic and literary applications  like author verification  plagia  rism detection  genre classification etc  It has legal applications too  like forensic linguistics  detection of genuine confessions  In the last few years it has success  fully been applied to broader areas  ranging from blogs  forums  wikis  email  chat and other forms of digital content to music and fineart paintings  Stylometry has been studied on English for long time  It was started by Mendenhall 1  in the 19th century  with his work on the plays </raw_string>
  </article>
  <article>
    <title>Flexible Length Phrases in Document Classification</title>
    <count>486</count>
    <raw_string>Flexible Length Phrases in Document Classification Danijel Radoevi  Jasminka Doba University of Zagreb Faculty of organization and informatics Pavlinska 2  Varadin  Croatia danijel radosevic jasminkadobsafoi hr Dunja Mladeni JStefan Institute  Jamova 39  1000 Ljubljana  Slovenia DunjaMladenicijs si Abstract  In this paper we investigate possibility of using phrases of flexible length in classification of textual documents as an extension to classic bag of words document representation where documents are represented using single words as index terms  The investigation is conducted on collection of articles from Ve ernji list  It is shown that usage of flexible length phrases improves precision of automatic document classification and there are indications that such approach could be used for genre classification  Keywords  documents classification  bag of words representation  flexible length phrases 1  Introduction The goal of text categorization is classification of text documents into a fixed number of predefined categories  Document classification is used in many different problem areas involving text documents such as classifying news articles based on their content  or suggesting interesting documents to the web user  The common way of representing textual documents is by vector space model or  so called bagofwords representation 11  Generally  index term can be any word present in the text of document  but not all the words in the documents have equal importance in representation of document semantic  That is why various schemes in bagofwords representation give greater weight to words which appear in smaller number of documents  and have greater discrimination power in document classification and smaller weight to words which are present in lots of documents  Common preprocessing step in document indexing is elimination of  so called  stop words  </raw_string>
  </article>
  <article>
    <title>A Dataset for Active Linguistic Authentication</title>
    <count>486</count>
    <raw_string>A Dataset for Active Linguistic Authentication Patrick Juola Juola  Associates pjuolajuolaassoccom John I Noecker Jr Juola  Associates jnoeckerjuolaassoccom Ariel Stolerman PSAL  Drexel University ams573cs drexel edu Michael V Ryan Juola  Associates mryanjuolaassoccom Patrick Brennan Juola  Associates pbrennanjuolaassoccom Rachel Greenstadt PSAL  Drexel University greeniecs drexel edu Abstract Biometric technologies provide the possibility of a new and more effective way of security computers against unauthorized access  linguis  tic technologies  and in particular  authorship attribution technologies  provide the possibility of a means to this end  We report on a novel corpus developed to test this possibility  Us ing temporary workers in a simulated office en vironment  we collected a weeks workproduct for 19 subjects and demonstrate that techniques culled from the field of authorship attribution can identify workers with more than 90  accu racy  1 Introduction Standard passwordbased identification systems are wellknown to have flaws  Passwords can be forgot ten  written down  stolen  and guessed  If any of these events happen  an intruder has the keys to the kingdom  Recognizing this  biometricbased identifi cation systems have been proposed that avoid or mit igate some of these issues   Its hard to forget your own thumbprint   But developing and testing these systems can be a challenge precisely because of the need for a wide variety of humans  especially when the biometric task is challenging or timeconsuming  One possibility for biometric validation is the indi vidual use of language  Prior work has shown that the authorship of documents as small as a few hundred words can be correctly identified  In a typical office environment  a worker will type many more words and thus </raw_string>
  </article>
  <article>
    <title>Sentence Boundary Verification in Polish Text</title>
    <count>486</count>
    <raw_string>Neurofuzzy Approach to Sentence Boundary Verification in Polish Text Krzysztof Simiski  Silesian University of Technology Abstract The sentence boundary verification is the crucial task in the natural language processing  The text analysis is very often based on sentences  The origin of the problem originates from the haplography of the full stop  Many approaches have been proposed  as direct  character  feature and phrase analyses   This paper presents the attempt at applying the neurofuzzy approach to the feature analysis of the lexems in the neighbourhood of the full stop in question  The results of the experiments reveal that the neurofuzzy systems achieve similar or better results with two or three rules whereas the crisp system generated rule bases with more than 20 rules  1 Introduction The sentence boundary disambiguation compri ses two tasks  The first one is the determination of sentence boundary in text with no punctuation marks  sometimes also with no case discrimination   The latter is the verification of the sentence boundaries in texts with punctuation marks and preserved letter cases  This paper deals with the verification of the sentence boundaries  The problem of sentence boundary verification is important in natural language processing  The text analysis is often based on sentences  as in Polsyn parser for Polish language 16   The certain kind of circular relation can be noticed  The text analysis is based on sentences and the sentence boundary dis  ambiguation needs the syntactical analysis of the text  The Polish language is an inflecting language so some stages of text analysis can be conducted basing on the morphological analysis of the lexems without detailed syntactical one  To cut this circulum vitiosum the sentence boundary verification is </raw_string>
  </article>
  <article>
    <title>Author recognition by Abstract Feature Extraction</title>
    <count>486</count>
    <raw_string>SOYUT ZELLK IKARIMI LE YAZAR TANIMA AUTHOR RECOGNITION BY ABSTRACT FEATURE EXTRACTION Murat Yasdi Yldz Teknik niversitesi Bilgisayar Mhendislii Blm muratyasdigmailcom Banu Diri Yldz Teknik niversitesi Bilgisayar Mhendislii Blm banuce yildizedutr ZETE Bu almann amac Soyut zellik karm ynteminin ok boyutlu zellik vektrlerine sahip olan almalarda baarl sonular verdiini gstermektir  Uygulama alan olarak yazar tanma almas alnm ve zellik vektrleri olarak da kelime kkleri ve 2 gramlar seilmitir  Soyut zellik karm ynteminin baars PCA  CFS  kikare gibi zellik karm yntemleri ile kyaslanarak snflandrmadaki baars hem Trke hem de ngilizce veri setleri zerinde gsterilmitir  ABSTRACT The purpose of this study is to show the success of Abstract Feature Extraction Method in multi dimensional feature vectors studies  Author recognition study is taken as an application area and word root and 2 gram s are chosen as feature vectors  The sucess of the Abstract Feature Extraction method in classification is shown on both Turkish and English data sets by comparing with feature extraction methods such as PCA  CFS  chisquare  1  GR Dokman snflandrma  bir dokmann sahip olduu zelliklere baklarak nceden belirlenmi belli saydaki kategorilerden hangisine dahil olacan belirlemektir  Dokman snflandrmadaki problemlerden biri  kime ait olduu bilinmeyen veya yazarnn kimliinden phelenilen dokmanlarn yazarnn tahmin edilmesi  bir dier problem de dokmann trnn veya yazarnn cinsiyetinin belirlenmesidir  Dokman snflandrma zerine yaplan ilk almalar yetmili yllarda otomatik dokman indeksleme olarak karmza kmtr  Belirli konular iin zel szlkler oluturulmu ve szlk ierisindeki kelimeler de birer kategori olarak kabul edilerek dokmanlar snflandrlmtr  Yazar tanma zerine yaplan almalardan Mosteller 1  yazarlk zelliklerini karm ve Bayesian analizi ile tanma yapmlardr  Burrows 2  ise almasnda en fazla sklkta kullanlan kelimeleri  Brinegar 3  kelimelerin uzunluunu  Morton 4  cmlelerin </raw_string>
  </article>
  <article>
    <title>Sentence Boundary Verification in Polish Text</title>
    <count>486</count>
    <raw_string>Sentence Boundary Verification in Polish Text Krzysztof Simiski Institute of Informatics  Silesian University of Technolog KrzysztofSiminskipolsl pl Summary  In this paper the heuristic metod based on phrase analysis is proposed for sentence boundary verification in Polish texts  The decision rules  maximum en tropy and neural network as reference methods are compared with the phrase analysis  The results elaborated by the proposed method are more acurate than the reference methods  1 Introduction The sentence boundary problem splits into two tasks  The first one is the sentence boundary detection in texts with no punctuation marks and no upper and lower case discrimination  The latter one is the sentence boundary verification in texts with punctuation marks and preserved letter cases  This paper deals with the verification of sentence boundaries in Polish texts  The problem of proper sentence boundary verification is crucial in natural language processing  The analysis is mostly based on the sentences  The danger of the vicious circle can appear  for proper analysis the text should be split into sentences  but the analysis should be carried out to split the text into sentences  Polish is an inflecting language and thanks to this many steps of analysis can be based on the morphological forms of words  The sentence is not easy to define  There are more than two hundred defini tions of sentence  The most popular belong to one of three groups  1  The formal definition  Sentence is a text fragment that start with capitalised word and ends with the full stop  or other punctuation mark that denotes the end of sentence   2 The quantum of information  The sentence is a text fragment containing the separate autonomous quantum </raw_string>
  </article>
  <article>
    <title>Authorship Attribution with Latent Dirichlet Allocation</title>
    <count>486</count>
    <raw_string>Authorship Attribution with Latent Dirichlet Allocation Yanir Seroussi Ingrid Zukerman Clayton School of Information Technology Faculty of Information Technology  Monash University Clayton  Victoria 3800  Australia firstname lastnamemonash edu Fabian Bohnert Abstract The problem of authorship attribution  attributing texts to their original authors  has been an active research area since the end of the 19th century  attracting increased interest in the last decade  Most of the work on authorship attribution focuses on scenarios with only a few candidate authors  but recently considered cases with tens to thousands of candidate authors were found to be much more challenging  In this report  we propose ways of employing Latent Dirichlet Allocation in authorship attribution  We show that our approach yields stateoftheart performance for both a few and many candidate authors  in cases where these authors wrote enough texts to be modelled effectively  1 Introduction The problem of authorship attribution  attributing texts to their original authors  has received considerable attention in the last decade 6  18  Most of the work in this field focuses on cases where texts must be attributed to one of a few candidate authors  eg  11  4  Recently  researchers have turned their attention to scenarios with tens to thousands of candidate authors 7  In this report  we study authorship attribution with few to many candidate authors  and introduce a new method that achieves stateoftheart performance in the latter case  Our approach to authorship attribution consists of building models of authors and their documents using Latent Dirichlet Allocation  LDA  3  We compare these models to models built from unseen texts to find the most likely authors of these texts  Section 32  </raw_string>
  </article>
  <article>
    <title>Identifying Topics by Using Word Distribution</title>
    <count>485</count>
    <raw_string>Identifying Topics by using Word Distribution Motoi NAKAYAMA Takao MIURA Deptof Elect  and Elect  Engineering  Hosei University Kajinocho 372  Koganei  Tokyo  Japan Abstract In this work  we examine and verify a topic word model which says each topic can be identified by means of word distribution under same author  and by using Random Projection  one of the dimension reduction techniques  we show we can obtain efficient and effec tive processing to the model  We examine Shakespeare works and show we can identify scenes correctly to their dramas  1 Introduction Authorship problem is one of the hot controversies over a period of one century  This issue concerns how we can identify authors by examining textual and some other features  Typical applications of the discussion are whether Shakespeare was really alive or not  and the analysis of the threatening letters on GlicoMorinaga case in Japan13  There has been proposed several techniques based on stylometry so far such as exam  ining word length  sentence length and the number of functional words  i e   while  on  to examine and an alyze authorship  but it is wellknown that there exist unremarkable variation within an identical author6  Another interesting problem comes from a topic identification  A topic means a subject of interests or discussion  and by topic identification  we means an issue what topic a text document concerns about  This technique allows us to automate document clas  sification as well as summarization  and to estimate contextsensitive information for efficient retrieval  Several investigations reveal a fact that it is eas  ier to extract useful information from text putting stress on relationship with topic  </raw_string>
  </article>
  <article>
    <title>Recentred Local Profiles for Authorship Attribution</title>
    <count>483</count>
    <raw_string>Natural Language Engineering 18  3   293312  c Cambridge University Press 2011 doi101017S1351324911000180 293 Recentred local profiles for authorship attribution R O B E R T L A Y T O N1  P A U L W A T T E R S1 and R I C H A R D D A Z E L E Y2 1Internet Commerce Security Laboratory  University of Ballarat  Australia email  rlaytonicslcom au  pwattersballarateduau 2Data Mining and Informatics Research Group  University of Ballarat  Australia email  rdazeleyballarateduau  Received 20 December 2010  revised 18 March 2011  accepted 24 April 2011  first published online 9 June 2011  Abstract Authorship attribution methods aim to determine the author of a document  by using information gathered from a set of documents with known authors  One method of performing this task is to create profiles containing distinctive features known to be used by each author  In this paper  a new method of creating an author or document profile is presented that detects features considered distinctive  compared to normal language usage  This recentreing approach creates more accurate profiles than previous methods  as demonstrated empirically using a known corpus of authorship problems  This method  named recentred local profiles  determines authorship accurately using a simple best matching author approach to classification  compared to other methods in the literature  The proposed method is shown to be more stable than related methods as parameter values change  Using a weighted voting scheme  recentred local profiles is shown to outperform other methods in authorship attribution  with an overall accuracy of 699  on the adhoc authorship attribution competition corpus  representing a significant improvement over related methods </raw_string>
  </article>
  <article>
    <title>Empirical Text Mining for Genre Detection</title>
    <count>482</count>
    <raw_string>EMPIRICAL TEXT MINING FOR GENRE DETECTION Vasiliki Simaki  Sofia Stamou  Nikos Kirtsis Computer Engineering and Informatics Department  Patras University  Greece  Department of Archives and Library Science Ionian University  Greece simaki  stamou  kirtsis ceid upatrasgr Keywords  Genre detection  annotation  human study Abstract  In this paper  we report on a preliminary study we carried out for identifying patterns that characterize the genre type of Greek texts  In the course of our study  we address four distinct genre types  we record their observable stylistic elements and we indicate their exploitation for automatic genrebased document classification  The findings of our study demon  strate that texts contain lexical features with discriminative power as far as genre is concerned  however modeling those features so that they can be explored by computerbased applications is still in early stages  1 INTRODUCTION The genre is the totality of characteristics we ob  serve in a text that gives a unique print  It is an het erogeneous categorical principle in that it provides clues for classifying texts into specific styles  Genre clues are closer to the field of semantics and unlike text type that conveys information about the texts  structure  they give out information about the style of a text  Genre clues are employed for characteriz  ing a text as subjectiveobjective  positivenegative about the subject it elaborates  opinionated  factual  etc  They may also be employed for unravelling stylistic features reflected within a text s content  with these ranging from literary content to proce  dural  descriptive and so forth  In this paper  we report on an observatory study we carried out in which we try </raw_string>
  </article>
  <article>
    <title>Demographic Information Classification Exploiting Spoken Language</title>
    <count>480</count>
    <raw_string>KONUMA DL KULLANILARAK DEMOGRAFK BLGLERN SINIFLANDIRILMASI DEMOGRAPHIC INFORMATION CLASSIFICATION EXPLOITING SPOKEN LANGUAGE H rem Trkmen  Banu Diri  Gksel Biricik  Reit Doan Bilgisayar Mhendislii Blm Yldz Teknik niversitesi irem banugoksel   ce yildizedutr  resityildizedutr ZETE Kiilerin yz ve ses zelliklerinden faydalanlarak ya  cinsiyet  rk gibi demografik bilgilerine ulaabilmek iin yaplan almalar son yllarda hz kazanmtr  Bu almada  doal dil ile yazlm dokmanlardan demografik bilgileri tahmin eden bir sistem gelitirilmi  bylece grnt ve ses zelliklerinin yan sra konuma dili zelliklerinin de kiilerin demografik profilleri ile ilikili olduu gsterilmitir  lk olarak  konuma dilinde yazlm dokmanlardan farkl sayda zellik vektrleri karlm  daha sonra gelitirmi olduumuz zellik azaltma yntemi ve Korelasyon Tabanl zellik Seici algoritmalar kullanlarak vektrlerin boyutlar azaltlmtr  Son olarak da Nave Bayes  Destek Vektr Makineleri ve KEn Yakn Komuluk algoritmalar ile snflandrma baarlar deerlendirilmitir  ABSTRACT Recently  extracting the demographic information like age  gender and race by using speech and face attributes takes much attention in the literature  In this research  we have focused on the implementation of a demographic information classification system and proved the relationship between spoken language and demographic profile of people  In the first step  the feature vectors of spoken language were extracted then dimensions of the feature vectors were reduced by our feature reduction method and Correlation Based Feature Selection method  Finally  the success of Nave Bayes  Support Vector Machine and KNearest Neigbour classification algorithms was evaluated  1  GR Doal dil ilemenin bir alt dal olan dokman snflandrma  web sayfalarnn hiyerarik olarak dzenlenmesi  bir dokmann trnn bulunmas  dokmann yazarnn tahmin edilmesi  yazarn cinsiyetinin tespit edilmesi gibi birok uygulama alanna sahiptir  Snflandrmadaki ama  farkl yaklamlar kullanarak bir </raw_string>
  </article>
  <article>
    <title>Indexing Methods for Web Archives</title>
    <count>452</count>
    <raw_string>Indexing Methods for Web Archives Dissertation zur Erlangung des Grades Doktor der Ingenieurwissenschaften  DrIng   der NaturwissenschaftlichTechnischen Fakultat I der Universitat des Saarlandes Avishek Anand MaxPlanckInstitut fur Informatik Saarbrucken 2013 ii Dekan der NaturwissenschaftlichTechnischen Fakultat I Prof Mark Groves Berichterstatter DrIng  Klaus Berberich Berichterstatter Prof DrIng  Gerhard Weikum Berichterstatter Prof DrIng  Kjetil Nrvag iv Eidesstattliche Versicherung Hiermit versichere ich an Eides statt  dass ich die vorliegende Arbeit selbststandig und ohne Benutzung anderer als der angegebenen Hilfsmittel angefertigt habe  Die aus anderen Quellen oder indirekt ubernommenen Daten und Konzepte sind unter Angabe der Quelle gekennzeichnet  Die Arbeit wurde bisher weder im In noch im Ausland in gleicher oder ahnlicher Form in einem Verfahren zur Erlangung eines akademischen Grades vorgelegt  Saarbrucken  den 02072013  Avishek Anand  vi Acknowledgements First and foremost  I would like to thank Prof Gerhard Weikum for giving me the opportunity to pursue this thesis under him and his timely and valuable inputs  I would like to express my sincere gratitude for his support and guidance  A special note of thanks to Dr Klaus Berberich and Prof Srikanta Bedathur  Their contribution over the past years has been invaluable and I have learned a lot from them  I thank them for their persistent support and patience through many discussions we had through the course of the thesis  I am very thankful to my parents and friends for the continued emotional support which in the past few years has often proven to be the deciding factor for my successes  Last but not the least I would like to thank my wife  Megha  for always being there and putting up with me during good times and bad  Her being there has </raw_string>
  </article>
  <article>
    <title>Unsupervised Detection of Anomalous Text</title>
    <count>452</count>
    <raw_string>Unsupervised Detection of Anomalous Text by David Guthrie Submitted in partial fulfillment of the requirements for the degree of Doctor of Philosophy Department of Computer Science University of Sheffield July 2008 c July 2008  David Guthrie All rights reserved  Thesis advisor Author Dr Robert Gaizauskas David Guthrie Unsupervised Detection of Anomalous Text Abstract This thesis describes work on the detection of anomalous material in text without the use of training data  We use the term anomalous to refer to text that is irregular  or deviates significantly from its surrounding context  In this thesis we show that identifying such abnormalities in text can be viewed as a type of outlier detection because these anomalies will differ significantly from the writing style in the majority of the data  We consider segments of text which are anomalous with respect to topic  i e  about a different subject   author  written by a different person   or genre  writ ten for a different audience or from a different source  and experiment with whether it is possible to identify these anomalous segments automatically  Five different innova  tive approaches to this problem are introduced and assessed using many experiments over large document collections  created to contain randomly inserted anomalous seg ments  In order to identify anomalies in text successfully  we investigate and evaluate 166 stylistic and linguistic features used to characterize writing  some of which are wellestablished stylistic determiners  but many of which are original  Using these features with each of our methods  we examine the effect of segment size on our ability to detect anomaly  allowing segments of size 100 words  500 words and 1000 words  We show substantial improvements </raw_string>
  </article>
  <article>
    <title>Native Language Identification with PPM</title>
    <count>452</count>
    <raw_string>Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications  pages 180187  Atlanta  Georgia  June 13 2013  c2013 Association for Computational Linguistics Native Language Identification with PPM Victoria Bobicev Technical University of Moldova 168  Stefan Cel Mare bvd  Chisinu  MD2004 Republic of Moldova victoriabobicevrolmd Abstract This paper reports on our work in the NLI shared task 2013 on Native Language Identi fication  The task is to automatically detect the native language of the TOEFL essays au  thors in a set of given test documents in Eng lish  The task was solved by a system that used the PPM compression algorithm based on an ngram statistical model  We submitted four runs  wordbased PPMC algorithm with normalization and without  characterbased PPMC algorithm with normalization and without  The worst result was obtained on training and testing data during the evaluation procedure using the characterbased PPM method and normalization  accuracy  319   the best one was macroaverage Fmeasure  0708 with the wordbased PPMC algorithm without normalization  1 Introduction With the emergence of usergenerated web con tent  text author profiling is being increasingly studied by the NLP community  Various works describe experiments aiming to automatically dis  cover hidden attributes of text which reveal au  thors gender  age  personality and others  While English remains one of the main global languages used for communication  interchange of infor mation and ideas  English texts written by different language speakers differ considerably  This is yet another characteristic of the author that can be learned from a text  While a great number of works have presented investigations in this area there was no common ground to </raw_string>
  </article>
  <article>
    <title>Using Probabilistic Tree Substitution Grammars</title>
    <count>452</count>
    <raw_string>Using Probabilistic Tree Substitution Grammars Ben Swanson January 24  2011 1 Abstract Probabilistic Tree Substitution Grammars model the syntactic structure of natural language  and can be learned in an unsupervised manner from anno  tated sentences such as the Penn Treebank  In this work we use this gram  mar in a novel approach to Form Function Tagging  using a specialized data structure to efficiently implement the sumproduct algorithm  We also ex  tend the existing induction algorithm to a hierarchical form  which we use to simultaneously induce grammars from a corpus split into document classes  We evaluate this model on the task of document dating and show that it outperforms other models commonly employed in previous work  Further more  we use the data structure for efficient sumproduct calculation in an implementation of a block sampler for grammar induction  and investigate the effects of grammar markovization on experimental performance  2 Introduction Due to the requirements of efficient parsing algorithms  most parsing re  search has focused on Probabilistic Context Free Grammars PCFG  How ever  techniques developed for PCFG models can be leveraged on more ex  pressive grammatical formalisms  We apply one such technique  hierarchical modeling  to Probabilistic Tree Substitution Grammars  PTSG  and eval uate its effect on the task of document classification  Document classification includes such concrete tasks as authorship at tribution  author verification  spam filtering  and sentiment analysis  The general formulation of the problem requires a corpus where each unit of text is associated with one or more classes  With this type of data  statistical machine learning techniques can be used to determine the most likely class of a piece of novel </raw_string>
  </article>
  <article>
    <title>Scalability Issues in Authorship Attribution</title>
    <count>452</count>
    <raw_string>Review  Scalability Issues in Authorship Attribution  Kim Luyckx  Brussels  University Press Antwerp  2010  xvi 180 pp  ISBN 97890  54878230  E 2995  paperback   The field of automated  nontraditional  authorship attribution is  not to put a fine point on it  a mess  While many significant accomplishments have been achieved  the field is highly fragmented  with little to no general theory or deep understandings of the strengths and weaknesses of different methods  Indeed  there is very little agreement  if any  on standard evaluation methods  so it is nearly impos sible to really measure progress in the field as a whole  A large part of the reason for this state of affairs is the highly multidisciplinary history of the field  as well as the dearth of large  well curated  publicly available corpora for testing and evalu ation   There have been few attempts at largescale systematic experimentation comparing different attribution techniques on different sorts of prob  lems  with the goal of deriving more general understandings than particular studies can give  Scalability Issues in Authorship Attribution  by Kim Luyckx  represents an important  though necessarily incomplete  step towards the goal of a general empirical theory of  automated non  traditional  authorship attribution  The books aim is to systematically examine questions of scalability in authorship attribution  i e  what effect variations in topic distribution  the number of features  the number of candidate authors  and the sizes of the texts have on attribution reliability  It is a focused monograph  and does not claim to provide a general survey of the field  </raw_string>
  </article>
  <article>
    <title>Genre Identification on the Web</title>
    <count>452</count>
    <raw_string>Genre Identification on the Web Mikael Gunnarsson mikaelgunnarssonhb se http wwwadm hb semg Swedish School of Library and Information Science Swedish National Graduate School of Language Technology September 29  2006 Abstract  One of the problems with the web and its tools for browsing and searching its contents is that its genre heterogeneity is not sufficently accounted for  Recent years have demonstrated some successful efforts in algorithmic genre identification  but the area is still in its infancy  This paper presents the general conception of genre and how document structure may be used as an additional discriminative feature in algorithmic genre identification  1 Introduction The web is nowadays an almost selfevident source in any information seeking task  It is sometimes described as a huge digital library  a statement which is refuted by others who object that it is not a library since it is not carefully organized  The description  indexing and classification of its contents that characterizes an ordinary library does not characterizes the web However  it cannot be denied that the web is a huge repository of documents and in that respect resembles an ordinary library  Therefore  methods and techniques that apply to library collections may be  and is  adopted for the web One of the main properties of the web that distinguishes it from a physical library is its heterogeneity with respect to genre  Genre as a reflection of typifications in documentary and communicative practices is generally not taken care of in all the tools available for browsing and searching the web Most efforts in improving information seeking tools focus on topical retrieval and seem to consider genre adherence as a secondary matter  Genre is generally not fully considered in libraries either  </raw_string>
  </article>
  <article>
    <title>Classifying XML Documents by Genre</title>
    <count>452</count>
    <raw_string>MSc Project Report Classifying XML Documents by Genre Volume 1 Malcolm Clark 2005 A report submitted as part of the requirements for the degree of MSc in Computing  Software Technology at The Robert Gordon University  Aberdeen  Scotland Malcolm Clark Page 2 ACKNOWLEDGEMENTS Huge thanks go to my wife Sam  Mum Annie and sons Owen  Callum and Nathan for their love  support and help through some tricky moments  Additional thanks go to the supervisor Dr Watt for his motivation  enthusiasm and also the opportunity to investigate a truly exciting subject area  Malcolm Clark Page 3 ABSTRACT The difficulties inherent with Information Retrieval are many fold  The main problem is how to find and display the users  needs  This volume briefly explores IR background  typical components and how it has traditionally been performed in the past on a multitude of works  Retrieval is being utilised on collections by using genre  which identifies the type of document by conceptual features instead of using traditional topical information  this volume investigates research in this field  Some authors have claimed that a document can be identified by form alone  Subsequently  a proposal is made with regard to investigating the genre of a document by using ten structural features  This would be carried out by conducting retrieval on the INEX XML collection which is to be indexed and searched using the Lucene library and classified by using two data mining algorithms  Malcolm Clark Page 4 TABLE OF CONTENTS ACKNOWLEDGEMENTS 2 ABSTRACT 3 TABLE OF FIGURES 5 CHAPTER ONE 6 1  INTRODUCTION 6 11 Project  6 12 Information Retrieval Background  6 13 Brief History  6 14 Information Retrieval Organisations  6 15 Information </raw_string>
  </article>
  <article>
    <title>Scalability Issues in Authorship Attribution</title>
    <count>452</count>
    <raw_string>Faculteit Letteren en Wijsbegeerte Departement Taalkunde Scalability Issues in Authorship Attribution Schaalbaarheid bij Auteursherkenning Proefschrift voorgelegd tot het behalen van de graad van doctor in de Taalkunde aan de Universiteit Antwerpen te verdedigen door Kim LUYCKX Promotor  Prof dr  Walter Daelemans Antwerpen  2010 Cover design  Tom De Smedt Print  Silhouet  Maldegem c 2010 Kim Luyckx c 2010 Uitgeverij UPA University Press Antwerp UPA is an imprint of ASP nv  Academic and Scientific Publishers nv  Ravensteingalerij 28 B1000 Brussels Tel   32  0 2 289 26 50 Fax  32  0 2 289 26 59 Email  infoaspeditions be www upaeditions be ISBN 978 90 5487 823 0 NUR 616  984 Legal deposit D201011161146 All rights reserved  No parts of this book may be reproduced or transmitted in any form or by any means  electronic  mechanical  photocopying  recording  or otherwise  without the prior written permission of the author  Abstract This dissertation is about authorship attribution  the task that aims to identify the author of a text  given a model of authorial style based on texts of known authorship  In computational authorship attribution  we do not rely on indepth reading  but rather automate the process  We take a text categorization approach that combines computational analysis of writing style using Natural Language Processing with a Machine Learning algorithm to build a model of authorial style and attribute authorship to a previously unseen text  In traditional applications of authorship attribution  for instance the investigation of disputed authorship or the analysis of literary style  we often find large sets of textual data of the same genre and small sets of candidate authors  Most approaches are </raw_string>
  </article>
  <article>
    <title>Report on Multimodal Content Abstraction</title>
    <count>451</count>
    <raw_string>AMIDA Augmented Multiparty Interaction with Distance Access http wwwamidaproject org Integrated Project IST033812 Funded under 6th FWP  Sixth Framework Programme  Action Line  IST2005257 Multimodal interfaces Deliverable D52  Report on multimodal content abstraction Due date  30112007 Submission date  07122007 Project start date  1102006 Duration  36 months Lead Contractor  Tilman Becker Revision  1 Project cofunded by the European Commission in the 6th Framework Programme  20022006  Dissemination Level PU Public X PP Restricted to other programme participants  including the Commission Services  RE Restricted to a group specified by the consortium  including the Commission Services  CO Confidential  only for members of the consortium  including the Commission Services  AMIDA D52  page 1 of 264 D52  Report on multimodal content abstraction Abstract  WP5s main objectives in the AMIDA project are  i  to provide WP6 and WP7 with components for inclusion in tools for remote meeting assistance and meeting browsing by  ii  extending and developing additional models and algorithms for multimodal structure and content analysis  and to provide a quantitative understanding of meeting structure while  iii  ensuring measurable quality by extending component evaluation schemes to AMIDA This deliverable describes results from the first 12 months of AMIDA in a range of fields  some of which are new while others extend work from AMI to the remote meeting sce  nario andor to realtime and online algorithms  The list includes  decision detection  subjectivity recognition  dialog act recognition  summarization  topic segmentation  ad dressee classification  argumentation  dominance modeling  speech indexing and retrieval and automatic video editing  AMIDA D52  page 2 of 264 D52 Multimodal content abstraction Contents 1 </raw_string>
  </article>
  <article>
    <title>Assessing Approaches to Genre Classification</title>
    <count>450</count>
    <raw_string>Assessing approaches to genre classification Philipp Petrenz Master of Science School of Informatics University of Edinburgh 2009 Abstract Four formerly suggested approaches to automated genre classification are assessed and compared on a unified data basis  Evaluation is done in terms of prediction accuracy as well as recall and precision values  The focus is on how well the algorithms cope when tested on texts with different writing styles and topics  Two US based newspaper corpora are used for training and testing  The results suggest that different approaches are suitable for different tasks and none can be seen as generally superior genre classifier  Acknowledgements First and foremost  I would like to thank my supervisor  Bonnie Webber  for her outstanding support throughout all stages of this project  I am also grateful to Victor Lavrenko  who provided input in preceding discussions  Furthermore  helpful information was received from Geoffrey Nunberg  Brett Kessler and Hinrich Schtze and was much appreciated  Table of Contents 1  Introduction  1 11  What are genres   1 12 How is genre different from style and topic   2 13  Genre classification  4 14  Report Structure  4 2 Previous work  5 21  Text classification  5 22  Genre classification  6 3 Project description  9 31  Motivation  9 32 Aims  10 33  Methodology  10 34  Software and tools  15 4  Material and Methods  16 41  The New York Times corpus  16 42  The Penn Treebank Wall Street Journal corpus  17 43  Data analysis and visualization  17 431  Metadata  17 432  Baseline genres  18 433  Genres </raw_string>
  </article>
  <article>
    <title>Automatic Analysis of Document Sentiment</title>
    <count>449</count>
    <raw_string>AUTOMATIC ANALYSIS OF DOCUMENT SENTIMENT A Dissertation Presented to the Faculty of the Graduate School of Cornell University in Partial Fulfillment of the Requirements for the Degree of Doctor of Philosophy by Bo Pang August 2006 c 2006 Bo Pang ALL RIGHTS RESERVED AUTOMATIC ANALYSIS OF DOCUMENT SENTIMENT Bo Pang  PhD Cornell University 2006 Sentiment analysis  which deals with the computational treatment of opinion  sen timent  and subjectivity in text  has attracted a great deal of attention  Potential applications include questionanswering systems that address opinions as opposed to facts and business intelligence systems that analyze user feedback  The research issues raised by such applications are often quite challenging compared to fact  based analysis  This thesis presents several sentiment analysis tasks to illustrate the new challenges and opportunities  In particular  we describe how we modeled different types of relations in approaching several sentiment analysis problems  our models can have implications outside this area as well  One task is polarity classification  where we classify a movie review as thumbs up  or thumbs down  from textual information alone  We consider a number of approaches  including one that applies text categorization techniques to just the subjective portions of the document  Extracting these portions can be a hard problem in itself  we describe an approach based on efficient techniques for find  ing minimum cuts in graphs that incorporate sentencelevel relations  The second task  which can be viewed as a nonstandard multiclass classification task  is the ratinginference problem  where one must determine the reviewers evaluation with respect to a multipoint scale  e g  one to five stars   We apply a metaalgorithm  based on a metriclabeling formulation </raw_string>
  </article>
  <article>
    <title>Klassifikation von Texten nach Genre</title>
    <count>447</count>
    <raw_string>Andrea Stubbe Klassifikation von Texten nach Genre Magisterarbeit im Fach Computerlinguistik LMU Mnchen 3 Inhaltsverzeichnis Inhaltsverzeichnis 1 Einleitung 7 2 Genre 9 21 Bisherige GenreSysteme 10 211 Vorhandene Korpora  Brown und WallstreetJournal 10 212 Neue Klassifikationen 11 213 WebGenres 12 22 Eine neue GenreHierarchie 13 221 Entstehung 14 222 Ergebnis 15 3 Methode 25 31 Korpus 25 32 Features und Klassifikatoren 26 4 Features 31 41 Bisherige Arbeiten 31 42 Bestimmung der relevanten Features 33 421 Arten von Features 34 422 Entscheidende Features je Genre 36 423 Probleme 49 5  Klassifikation 53 51 KlassifizierungsAlgorithmen 53 52 Eigene Klassifikation 56 6 Evaluierung 59 61 Eigene Klassifikatoren 60 611 Gesamtergebnis 60 612 Einzelklassifikatoren 62 613 Filterreglen 63 614 Klassifikation in Hauptgruppen 64 62 Automatische Klassifikatoren 65 63 Vergleich mit anderen Ergebnissen 66 7 Fazit 69 71 Verbesserungspotenzial 69 72 Ausblick 69 8 Literaturverzeichnis 71 9 Anhang 75 91 Genres 76 92 Aufgabenbeschreibung fr die Textklassifikation 77 93 Features 78 94 FilterRegeln 83 95 Recall und Precision 84 96 Accuracy und Classification Error 88 Klassifikation von Texten nach Genre 4 97 Konfusionsmatrix fr Mehrfachklassifikation 92 98 Vergleich der automatischen Klassifikatoren 93 Eidesstattliche Erklrung 95 Lebenslauf 96 5 Inhaltsverzeichnis Tabellenverzeichnis Tabelle 21  Genres im BrownKorpus KAC  10 Tabelle 22  Genres im Wall Street Journal STA 11 Tabelle 23  Dewes GenreEinteilung DEWE 11 Tabelle 24  Genres von Roussinov ROU 12 Tabelle 25  Das ursprngliche GenreSystem 14 Tabelle 26  Die verbesserte GenreHierarchie 15 Tabelle 27  Nicht in die OriginalKlasse eingeordnete Texte 22 Tabelle 41  Mittelwerte der Vorkommen von Orts  und Zeitdeixis 33 Tabelle 51  Regeln pro Hauptgruppe 56 Tabelle 61  Anzahl der erkannten Dateien und Einordnungen je Genre 60 Tabelle 62  Mittelwerte fr Accuracy und Classification Error 61 Tabelle 63  </raw_string>
  </article>
  <article>
    <title>Alias Detection in Malicious Environments</title>
    <count>447</count>
    <raw_string>Alias Detection in Malicious Environments Patrick Pantel Information Sciences Institute University of Southern California 4676 Admiralty Way Marina del Rey  CA 90292 pantelisi edu Abstract Alias detection is a challenging task critical in several areas such as the intelligence community  social network analysis  databases  biology  and marketing  Problem domains can be as simple as datasets containing accidentally replicated data  or as complex as populations containing criminals or terrorists wielding multiple identities  Teasing out aliases or near aliases in the later case is a serious and challenging problem  We propose an unsupervised information theoretic approach for automatically detecting aliases in malicious environments by observing the behaviors of the entities  Our model discovers the most informative observations  e g  emails  phone calls  relational data  between entities and then compares them to identify entities exhibiting similar behaviors  We test our model by applying it to the task of discovering aliases in a standard synthetic world of interrelated individuals  Given our systems top20 guesses  we extracted with 80  accuracy the true aliases of a given entity  Introduction Entity consolidation is the problem of uncovering duplicate or nearduplicate entities in a dataset  Problem domains can be as simple as datasets containing accidentally replicated data  or as complex as populations containing criminals or terrorists wielding multiple identities  Teasing out duplicate or near duplicate entities in the later case is a serious and challenging problem  Alias problems are commonly encountered in the intelligence community when performing background checks or  in general  when tracking individuals from a broad population  Often  simple orthographic cues indicate an alias  as in Osama bin Laden and Usama bin Laden  Other times  </raw_string>
  </article>
  <article>
    <title>Detecting Plagiarism in Text Documents</title>
    <count>420</count>
    <raw_string>VV Das et al   Eds    BAIP 2010  CCIS 70  pp  497500  2010   SpringerVerlag Berlin Heidelberg 2010 Detecting Plagiarism in Text Documents Shanmugasundaram Hariharan1  Sirajudeen Kamal2  Abdul Vadud Mohamed Faisal2  Sheik Mohamed Azharudheen2  and Bhaskaran Raman2 1 Assistant Professor  Department of Information Technology BS Abdur Rahman University  Chennai48  Tamilnadu  India 2 Student  Department of Information Technology BS Abdur Rahman University  Chennai48  Tamilnadu  India mailtos hariharankamal66077 ramancre gmailcom  azharcres brucefaisal yahoocom Abstract  Plagiarism aims at identifying the amount of information that is copied or reproduced in modified representation of original documents  This is quiet common among students  researchers and academicians that leads to a kind of unrecognizing  Though there exits some commercial tools to detect plagiarism  still plagiarism is tricky and quiet challenging task due to abundant information available online  Commercially existing softwares adopt methods like paraphrasing  sentence matching or keyword matching  This paper focuses its attention on identifying some key parameters that would help to identify plagiarism in a better manner and to report plagiarism in an effective way  The result seems to be promising and have further scope in detecting the plagiarism  Keywords  Plagiarism  similarity metric  paraphrasing  sentence matching  1 Introduction Plagiarism defined as the use or close imitation of language and thoughts of another author and the representing them as their one s work  Plagiarism affects the education quality of the students and there by reduce the economic status of the country which is done by techniques like paraphrasing or similarities between keywords and verbatim overlaps  change of sentences from one form to another form 7  Internet </raw_string>
  </article>
  <article>
    <title>Authorship Attribution in Health Forums</title>
    <count>418</count>
    <raw_string>Proceedings of Recent Advances in Natural Language Processing  pages 7482  Hissar  Bulgaria  713 September 2013  Authorship Attribution in Health Forums Victoria Bobicev Technical University of Moldova Chisinau  Moldova vikarolmd Marina Sokolova CHEO Research Institute  Ottawa University of Ottawa  Ontario  Canada sokolovauottawaca Khaled El Emam CHEO Research Institute  Ottawa University of Ottawa  Ontario  Canada kelemamuottawaca Stan Matwin Dalhousie University  Halifax  Canada Polish Academy of Sciences  Warsaw  Poland stancs dalca Abstract The emergence of social media  networks  blogs  web forums  has given people numer ous opportunities to share their personal sto  ries  including details of their health  Although users mostly post under assumed nicknames  stateoftheart text analysis techniques can combine texts from different media and use that linkage to identify private details of an in dividuals health  In this study we aim to em  pirically examine the accuracy of identifying authors of online posts on a medical forum  1 Our results show a high accuracy of the au  thorship attribution  especially when text is represented by the orthographical features  1 Introduction Emergence of social media  networks  blogs  web forums  has given people numerous oppor tunities to share their personal stories  including details of their health  e g  disease diagnosis  symptoms  treatment   Velden and Emam  2012  Bobicev et at  2012    The transfer went well  my RE did it himself which was comforting  2 embies  grade 1 but slow in development  so I am not holding my breath for a positive   I ve had 7 IUI and one ivf all cancelled </raw_string>
  </article>
  <article>
    <title>User Identification for Instant Messages</title>
    <count>418</count>
    <raw_string>BL Lu  L Zhang  and J Kwok  Eds    ICONIP 2011  Part III  LNCS 7064  pp  113120  2011   SpringerVerlag Berlin Heidelberg 2011 User Identification for Instant Messages Yuxin Ding  Xuejun Meng  Guangren Chai  and Yan Tang Harbin Institute of Technology Shenzhen Graduate School  Shenzhen 518055  China yxdinghitsz educn  xjmeng grchaiytang hotmailcom Abstract  In this paper we study on recognizing users identity based on instant messages  Considering the special characteristics of chatting text  we mainly focus on three problems  one is how to extract the features of chatting text  the second is how the users model is affected by the size of training data  and the third is which classification model is fit for this problem  The chatting corpus used in this paper is collected from a Chinese IM tool and different feature selection methods and classification models are evaluate on it  Keywords  Authorship  identification  classification  instant message  1 Introduction The goal of this paper is to determine whether an instant message is sent by the real sender indicated by the senders ID It belongs to the field of authorship verification  Authorship verification is used to determine whether an author  for whom we have of a corpus of writing samples  is also the author of a given anonymous text  Usually it can be viewed as a multiclass  singlelabel text categorization from the point of the machine learning  With the wide application of instant messaging tools  instant messaging  IM  has become the most popular way of communication through internet  Unfortunately  this is also the reason why IM becomes a popular </raw_string>
  </article>
  <article>
    <title>Meta Analysis within Authorship Verification</title>
    <count>416</count>
    <raw_string>Meta Analysis within Authorship Verification Benno Stein Nedim Lipka Sven Meyer zu Eissen Faculty of Media  Media Systems Bauhaus University Weimar  Germany  first lastmedien uniweimarde Abstract In an authorship verification problem one is given writ ing examples from an author A  and one is asked to de termine whether or not each text in fact was written by A In a more general form of the authorship verification prob  lem one is given a single document d only  and the question is whether or not d contains sections from other authors  The heart of authorship verification is the quantization of an authors writing style along with an outlier analysis to identify anomalies  Human readers are wellversed in de tecting such spurious sections since they combine a highly developed sense for wording with contextdependent meta knowledge in their analysis  The intention of this paper is to compile an overview of the algorithmic building blocks for authorship verifica  tion  In particular  we introduce authorship verification problems as decision problems  discuss possibilities for the use of meta knowledge  and apply meta analysis to post process unreliable style analysis results  Our meta analysis combines a confidencebased majority decision with the un masking approach of Koppel and Schler  With this strategy we can improve the analysis quality in our experiments by 33  in terms of the F measure  Keywords Authorship Verification  Plagiarism Analy sis  Meta Learning Tjoa  Wagner  Eds    Proceedings of the Dexa 08  Turin 19th International Conference on Database and Expert Systems Applications ISBN 9780769532998  pp  3439  cIEEE 2008  1  Introduction Authorship verification is a oneclass classification prob  lem  In a </raw_string>
  </article>
  <article>
    <title>Computational Methods in Authorship Attribution</title>
    <count>416</count>
    <raw_string>317 AUTHORSHIP ATTRIBUTION  WHATS EASY AND WHATS HARD  Moshe Koppel   Jonathan Schler   and Shlomo Argamon INTRODUCTION The simplest kind of authorship attribution problemand the one that has received the most attentionis the one in which we are given a small  closed set of candidate authors and are asked to attribute an anonymous text to one of them  Usually  it is assumed that we have copious quantities of text by each candidate author and that the anonymous text is reasonably long  A number of recent survey papers1 amply cover the variety of methods used for solving this problem  Unfortunately  the kinds of authorship attribution problems we typically encounter in forensic contexts are more difficult than this simple version in a number of ways  First  the number of suspected writers might be very large  possibly numbering in the many thousands  Second  there is often no guarantee that the true author of an anonymous text is among the known suspects  Finally  the amount of writing we have by each candidate might be very limited and the anonymous text itself might be short   Department of Computer Science  BarIlan University  RamatGan  Israel  moishkgmailcom  Corresponding Author    Department of Computer Science  BarIlan University  RamatGan  Israel  schlergmailcom   Department of Computer Science  Illinois Institute of Technology  argamoniit edu  1 Patrick Juola  Authorship Attribution  1 FOUND  TRENDS IN INFO RETRIEVAL 233  23839  2006   Moshe Koppel et al   Computational Methods in Authorship Attribution  60 J AM SOCY FOR INFO SCI  TECH 9  9  2009   Efstathios Stamatatos  A </raw_string>
  </article>
  <article>
    <title>Document Recommendation Using Data Compression</title>
    <count>415</count>
    <raw_string>Procedia  Social and Behavioral Sciences 27  2011  150  159 18770428  2011 Published by Elsevier Ltd Selection andor peerreview under responsibility of PACLING Organizing Committee  doi  101016j sbspro201110593 Pacific Association For Computational Linguistics  PACLING 2011  Document recommendation using data compression Takafumi Suzuki a   Shin Hasegawa b  Takayuki Hamamoto c  Akiko Aizawa d aTokyo University Faculty of Sociology 52820  Hakusan Bunkyoku  Tokyo  Japan bSony 171  Konan  Minatoku  Tokyo  Japan cTokyo University of Science  Faculty of Engineering 1146  Kudankita  Chiyodaku  Tokyo  Japan dNational Institute of Informatics Digital Content and Media Sciences Research Division 212  Hitotsubashi  Chiyodaku  Tokyo  Japan Abstract We propose a new method of contentbased document recommendation using data compression  Though previous studies mainly used bagsofwords to calculate the similarity between the profile and target documents  users in fact focus on larger unit than words  when searching information from documents  In order to take this point into consideration  we propose a method of document recommendation using data compression  Experimental results using Japanese newspaper corpora showed that  a  data compression performed better than the bagofwords method  especially when the number of topics was large   b  our new method outperformed the previous data compression method   c  a combination of data compression and bagofwords can also improve performance  We conclude that our method better captures users  profiles and thus contributes to making a better document recommendation system   2011 Published by Elsevier Ltd Selection andor peerreview under responsibility of PACLING 2011 Keywords  Data compression  document recommendation  LZ78  PRDC  document classification  a Corresponding </raw_string>
  </article>
  <article>
    <title>Low Resources Prepositional Phrase Attachment</title>
    <count>413</count>
    <raw_string>Low Resources Prepositional Phrase Attachment Pavlos Nalmpantis  Romanos Kalamatianos  Konstantinos Kordas and Katia Kermanidis Department of Informatics  Ionian University Corfu  Greece cs200664  cs200611  cs200539  kerman ioniogr AbstractPrepositional phrase attachment is a major disambiguation problem when it s about parsing natural language  for many languages  In this paper a low resources policy is proposed using supervised machine learning algorithms in order to resolve the disambiguation problem of Prepositional phrase attachment in Modern Greek  It is a first attempt to resolve Prepositional phrase attachment in Modern Greek  without using sophisticated syntactic annotation and semantic resources  Also there are no restrictions regarding the prepositions addressed  as is common in previous approaches  Decision Trees  Modern Greek  PP attachment  Supervised learning I INTRODUCTION The correct attachment of prepositional phrases  PPs  to another constituent in a sentence is a significant disambiguation problem for parsing natural languages  For example  take the following two sentences  1 She eats soup with a spoon  2 She eats soup with tomatoes  In sentence 1  the PP  with a spoon  is attached to the verb phrase VP  eats  denoting the instrument utilized for the eating action  thus making it the anchor phrase of the PP Sentence 2 seems to differ only minimally from the first sentence  but  as can be seen from their syntax trees in Fig  1 and Fig  2 respectively  their syntactic structure is quite different  The PP  with tomatoes  does not attach to the verb but to the noun phrase  NP   soup  denoting the type of soup  Figure 1  Syntax tree of sentence 1 Figure </raw_string>
  </article>
  <article>
    <title>Social Event Detection on Twitter</title>
    <count>411</count>
    <raw_string>Social Event Detection on Twitter Elena Ilina1  Claudia Hauff1  Ilknur Celik2  Fabian Abel1  and GeertJan Houben1 1 Web Information Systems  Delft University of Technology e ailinachauff fabel gjpmhouben tudelft nl 2 Middle East Technical University Northern Cyprus Campus cilknurmetuedutr Abstract  Various applications are developed today on top of microblog ging services like Twitter  In order to engineer Web applications which operate on microblogging data  there is a need for appropriate filtering techniques to identify messages  In this paper  we focus on detecting Twit ter messages  tweets  that report on social events  We introduce a filtering pipeline that exploits textual features and ngrams to classify messages into event related and nonevent related tweets  We analyze the impact of preprocessing techniques  achieving accuracies higher than 80   Fur ther  we present a strategy to automate labeling of training data  since our proposed filtering pipeline requires training data  When testing on our dataset  this semiautomated method achieves an accuracy of 79  and re  sults comparable to the manual labeling approach  Keywords  microblogging  Twitter  event detection  classification  semiautomatic training  1 Introduction Twitter is a popularmicrobloggingweb application servingmillions of usersTwit ter users chat and share information on news  workrelated issues and community matters 1  Despite the noise in Twitter blogs 2  Web applications can exploit the blogs  content as a source of information to identify natural disasters 3  news 2  or social events 4  Since the existing Twitter search is cumbersome in finding eventrelated information 5  a targeted search aiming at finding tweets specifi cally related to reallife events might be useful  The capability of searching reallife </raw_string>
  </article>
  <article>
    <title>Synthetic Review Spamming and Defense</title>
    <count>411</count>
    <raw_string>Synthetic Review Spamming and Defense Huan Sun  Alex Morales  and Xifeng Yan Department of Computer Science University of California  Santa Barbara huansun  alexmorales  xyan cs ucsbedu ABSTRACT Online reviews have been popularly adopted in many appli cations  Since they can either promote or harm the reputa  tion of a product or a service  buying and selling fake reviews becomes a profitable business and a big threat  In this paper  we introduce a very simple  but powerful review spamming technique that could fail the existing featurebased detection algorithms easily  It uses one truthful review as a template  and replaces its sentences with those from other reviews in a repository  Fake reviews generated by this mechanism are extremely hard to detect  Both the stateoftheart com putational approaches and human readers acquire an error rate of 35 48   just slightly better than a random guess  While it is challenging to detect such fake reviews  we have made solid progress in suppressing them  A novel defense method that leverages the difference of semantic flows be tween synthetic and truthful reviews is developed  which is able to reduce the detection error rate to approximate ly 22   a significant improvement over the performance of existing approaches  Nevertheless  it is still a challenging research task to further decrease the error rate  Synthetic Review Spamming Demo  wwwcs ucsbedualexmoralesreviewspam Categories and Subject Descriptors I27 Artificial Intelligence  Natural Language Process  ingtext analysis  K41 Computers and Society  Pub  lic Policy Issuesabuse and crime involving computers Keywords Review Spam  Spam Detection  Classification 1  INTRODUCTION Online reviews are widely adopted in many websites such as Amazon  Yelp </raw_string>
  </article>
  <article>
    <title>Style Variation in Cooking Recipes</title>
    <count>411</count>
    <raw_string>Style Variation in Cooking Recipes Jing Lin1 and Chris Mellish2 and Ehud Reiter3 Abstract  Human text by specific authors has varied characteris  tics  Even when different authors write about identical topics within a fixed genre under a predefined domain  their text will still be differ ent  For example  in food recipes there are style variations between authors even with describing the same cooking actions and foods  This paper summarises style variations in food recipes written by different people  1 Introduction Nowadays it is common to use corpora of humanauthored texts to acquire knowledge in natural language generation  NLG  such as STOP 10 and SumTime 12  and so on  To make sure the corpora are big enough and broad enough  the corpora generally contain texts from several authors  When corpora are analysed  the differences be tween authors are normally ignored  Reiter and Sripada 9  8 have suggested that There are substantial variations between individual writers  which reduces the effectiveness of corpusbased learning  Our ultimate goal is to build an NLG system which translates recipes written by one author into the style of another author  This requires a good model of what aspects of recipes differ between authors  the corpus analysis presented here is a step towards such a model  In the NLG area  some researchers have investigated producing text with varied styles  Hovy 4  5 discussed rhetorical goals that are the existence of a level of organization mediating between commu nicative goals and generator decisions  Furthermore  Stamatatos etc  13 can produce outputs based on different user requirements  such as written style  tone  and so on  None of these researches </raw_string>
  </article>
  <article>
    <title>Computational Methods in Authorship Attribution</title>
    <count>411</count>
    <raw_string>1 Computational Methods in Authorship Attribution Moshe Koppel  Corresponding Author  Dept  of Computer Science BarIlan University RamatGan  Israel moishkgmailcom Jonathan Schler Dept  of Computer Science BarIlan University Shlomo Argamon Dept  of Computer Science Illinois Institute of Technology 2 Computational Methods in Authorship Attribution Abstract Statistical authorship attribution has a long history  culminating in the use of modern machine learning classification methods  Nevertheless  most of this work suffers from the limitation of assuming a small closed set of candidate authors and essentially unlimited training text for each  Reallife authorship attribution problems  however  typically fall short of this ideal  Thus  following detailed discussion of previous work  three scenarios are considered here for which solutions to the basic attribution problem are inadequate  In the first variant  the profiling problem  there is no candidate set at all  in this case  the challenge is to provide as much demographic or psychological information as possible about the author  In the second variant  the needleinahaystack problem  there are many thousands of candidates for each of whom we might have a very limited writing sample  In the third variant  the verification problem  there is no closed candidate set but there is one suspect  in this case  the challenge is to determine if the suspect is or is not the author  For each variant  it is shown how machine learning methods can be adapted to handle the special challenges of that variant  1  Introduction The task of determining or verifying the authorship of an anonymous text based solely on internal evidence is a very old one  dating back at least to the medieval scholastics  for </raw_string>
  </article>
  <article>
    <title>Plagiarism Detection without Reference Collections</title>
    <count>411</count>
    <raw_string>Plagiarism Detection Without Reference Collections Sven Meyer zu Eissen  Benno Stein and Marion Kulig Faculty of Media  Media Systems  Bauhaus University Weimar  99421 Weimar  Germany  svenmeyerzueissen  benno steinmedien uniweimarde Abstract  Current research in the field of automatic plagiarism detection for text documents focuses on the development of algorithms that compare suspi cious documents against potential original documents  Although recent approaches perform well in identifying copied or even modified passages  Brin et al   1995   Stein  2005    they assume a closed world where a reference collection must be given  Finkel  2002    Recall that a human reader can identify suspicious passages within a document without having a library of potential original documents in mind  This raises the question whether plagiarized passages within a document can be detected automatically if no reference is given  e  g  if the plagiarized passages stem from a book that is not available in digital form  This paper contributes right here  it proposes a method to identify potentially plagiarized passages by analyzing a single document with respect to changes in writing style  Such passages then can be used as a starting point for an Internet search for potential sources  As well as that  such passages can be preselected for inspection by a human referee  Among others  we will present new style features that can be computed efficiently and which provide highly discriminative information  Our experiments  which base on a test corpus that will be published  show encouraging results  1 Introduction A recent largescale study on 18000 students by McCabe  2005  reveals that about 50  of the students admit </raw_string>
  </article>
  <article>
    <title>Vocabulary Richness Measure in Genres</title>
    <count>410</count>
    <raw_string>This article was downloaded by   University of Aegean  On  18 November 2013  At  07 00 Publisher  Routledge Informa Ltd Registered in England and Wales Registered Number  1072954 Registered office  Mortimer House  3741 Mortimer Street  London W1T 3JH  UK Journal of Quantitative Linguistics Publication details  including instructions for authors and subscription information  http wwwtandfonline comloinjql20 Vocabulary Richness Measure in Genres Miroslav Kubt a  Ji Milika a a Department of General Linguistics  Palack University  Olomouc  Czech Republic Published online  12 Nov 2013  To cite this article  Miroslav Kubt  Ji Milika  2013  Vocabulary Richness Measure in Genres  Journal of Quantitative Linguistics  204  339349  DOI  101080092961742013830552 To link to this article  http dx doi org101080092961742013830552 PLEASE SCROLL DOWN FOR ARTICLE Taylor  Francis makes every effort to ensure the accuracy of all the information  the Content  contained in the publications on our platform  However  Taylor  Francis  our agents  and our licensors make no representations or warranties whatsoever as to the accuracy  completeness  or suitability for any purpose of the Content  Any opinions and views expressed in this publication are the opinions and views of the authors  and are not the views of or endorsed by Taylor  Francis  The accuracy of the Content should not be relied upon and should be independently verified with primary sources of information  Taylor and Francis shall not be liable for any losses  actions  claims  proceedings  demands  costs  expenses  damages  and other liabilities whatsoever or howsoever caused arising directly or indirectly in connection with  in relation to </raw_string>
  </article>
  <article>
    <title>Toward Intelligent Music Information Retrieval</title>
    <count>410</count>
    <raw_string>564 IEEE TRANSACTIONS ON MULTIMEDIA  VOL 8  NO 3  JUNE 2006 Toward Intelligent Music Information Retrieval Tao Li and Mitsunori Ogihara AbstractEfficient and intelligent music information retrieval is a very important topic of the 21st century  With the ultimate goal of building personal music information retrieval systems  this paper studies the problem of intelligent music information re  trieval  Huron 10 points out that since the preeminent functions of music are social and psychological  the most useful characteri zation would be based on four types of information  genre  emotion  style  and similarity  This paper introduces Daubechies Wavelet Coefficient His  tograms  DWCH for music feature extraction for music informa  tion retrieval  The histograms are computed from the coefficients of the db8 Daubechies wavelet filter applied to 3 s of music  A comparative study of sound features and classification algorithms on a dataset compiled by Tzanetakis shows that combining DWCH with timbral features  MFCC and FFT  with the use of multiclass extensions of support vector machine  achieves approximately 80  of accuracy  which is a significant improvement over the previously known result on this dataset  On another dataset the combination achieves 75  of accuracy  The paper also studies the issue of detecting emotion in music  Rating of two subjects in the three bipolar adjective pairs are used  The accuracy of around 70  was achieved in predicting emotional labeling in these adjective pairs  The paper also studies the problem of identifying groups of artists based on their lyrics and sound using a semisupervised classification algorithm  Identification of artist groups based on the Similar Artist lists at All Music Guide is attempted  The semisupervised </raw_string>
  </article>
  <article>
    <title>Overview of Automatic Genre Identification</title>
    <count>410</count>
    <raw_string>Overview of Automatic Genre Identification1 Mitja Lutrek Joef Stefan Institute  Department of Intelligent Systems Jamova 39  1000 Ljubljana  Slovenia http dis ijs simitjal  mitjalustrekijs si Technical report  May 2006 1  Introduction Genre is a category of artistic  musical  or literary composition characterized by a particular style  form  or content  according to MerriamWebster Online Dictionary 2   among two other  less relevant definitions   It can be debated whether a genre can be defined by style  and form  alone  or must content  topic  also be taken into account  For example  can science fiction be distinguished from all other literary genres only by style or must one also consider that it talks about spaceships and time travel  But for the purpose of information retrieval  topic component of genre can and should be disregarded  because topic is usually treated separately and is characterized by keywords  and other methods   so genre should be orthogonal to topic  This report is written with a general corpus such as WWW in mind  Genres that cannot easily be distinguished by style are probably too finegrained anyway for a corpus like that  it would be nice to be able to search for science fiction  but at this granularity  there are simply too many genres  It is probably best to limit oneself to genres such as homepage  FAQ  scientific paper  etc  Another common way to describe the genre of a document is through the purpose of the document  but looking for a way to automatically detect the purpose of a document leads back to style  Genres can also be viewed through </raw_string>
  </article>
  <article>
    <title>Author Identification for Turkish Texts</title>
    <count>410</count>
    <raw_string>151 ankaya niversitesi FenEdebiyat Fakltesi  Journal of Arts and Sciences Say  7  Mays 2007 The main concern of author identification is to define an appropriate characterization of documents that captures the writing style of authors  The most important approaches to computerbased author identification are exclusively based on lexical measures  In this paper we presented a fully automated approach to the identification of the authorship of unrestricted text by adapting a set of style markers to the analysis of the text  In this study  35 style markers were applied to each author  By using our method  the author of a text can be identified by using the style markers that characterize a group of authors  The author group consists of 20 different writers  Author features including style markers were derived together with different machine learning algorithms  By using our method we have obtained a success rate of 80  in avarege  Introduction There exist very different types of documents on all over the Internet  Published texts on the Internet provide the ability to process them by using some special software  The motivation behind this software is the need for rapid retrieval of the required data  search for specific information and some language specific techniques such as Natural Language Processing  NLP  12  Natural Language Processing is a research area that is used for many different purposes and it becomes more popular continuously  Speech syntheses  speech recognition  machine translation  spelling correction are some of the application of NLP Author Identification for Turkish Texts Tufan TA1  Abdul Kadir GRR2 1 Computer Engineering Department  ankaya Univeristy  Anakara email  tufantasgmailcom 2 Computer Engineering Department  ankaya Univeristy  Anakara </raw_string>
  </article>
  <article>
    <title>Opinion Mining and Sentiment Analysis</title>
    <count>410</count>
    <raw_string>Foundations and Trends R in Information Retrieval Vol  2  Nos  12  2008  1135 c 2008 B Pang and L Lee DOI  1015611500000001 Opinion Mining and Sentiment Analysis Bo Pang1 and Lillian Lee2 1 Yahoo Research  701 First Avenue  Sunnyvale  CA 94089  USA  bopangyahooinccom 2 Computer Science Department  Cornell University  Ithaca  NY 14853  USA  lleecs cornell edu Abstract An important part of our informationgathering behavior has always been to find out what other people think  With the growing availability and popularity of opinionrich resources such as online review sites and personal blogs  new opportunities and challenges arise as people now can  and do  actively use information technologies to seek out and understand the opinions of others  The sudden eruption of activity in the area of opinion mining and sentiment analysis  which deals with the computational treatment of opinion  sentiment  and subjectivity in text  has thus occurred at least in part as a direct response to the surge of interest in new systems that deal directly with opinions as a firstclass object  This survey covers techniques and approaches that promise to directly enable opinionoriented informationseeking systems  Our focus is on methods that seek to address the new challenges raised by sentimentaware applications  as compared to those that are already present in more traditional factbased analysis  We include material on summarization of evaluative text and on broader issues regarding privacy  manipulation  and economic impact that the development of opinionoriented informationaccess services gives rise to  To facilitate future work  a discussion of available resources  benchmark datasets  and evaluation campaigns is also provided  1 Introduction Romance should never begin </raw_string>
  </article>
  <article>
    <title>Authorship Attribution in the Wild</title>
    <count>408</count>
    <raw_string>Authorship attribution in the wild Moshe Koppel  Jonathan Schler  Shlomo Argamon Published online  13 January 2010 Springer ScienceBusiness Media BV 2010 Abstract Most previous work on authorship attribution has focused on the case in which we need to attribute an anonymous document to one of a small set of candidate authors  In this paper  we consider authorship attribution as found in the wild  the set of known candidates is extremely large  possibly many thousands  and might not even include the actual author  Moreover  the known texts and the anonymous texts might be of limited length  We show that even in these difficult cases  we can use similaritybased methods along with multiple randomized feature sets to achieve high precision  Moreover  we show the precise relationship between attribution precision and four parameters  the size of the candidate set  the quantity of knowntext by the candidates  the length of the anonymous text and a certain robustness score associated with a attribution  Keywords Authorship attribution Open candidate set Randomized feature set 1 Introduction Authorship attribution has been much studied in recent years and several recent articles  Juola 2008  Koppel et al  2008  Stamatatos 2009  survey the plethora of methods that have been applied to the problem  A significant fact that examination of the literature reveals is that nearly all research in the field only considers the M Koppel    J Schler BarIlan University  RamatGan  Israel email  moishkgmailcom J Schler email  schlergmailcom S Argamon Illinois Institute of Technology  Chicago  IL  USA email  argamoniit edu 123 Lang Resources  Evaluation  2011  45 8394 DOI 101007s1057900991112 simplest version of the problem </raw_string>
  </article>
  <article>
    <title>Authorship Attribution using Word Sequences</title>
    <count>407</count>
    <raw_string>Authorship Attribution using Word Sequences Rosa Mara CoyotlMorales1  Luis VillaseorPineda1  Manuel MontesyGmez1 and Paolo Rosso2 Laboratorio de Tecnologas del Lenguaje  Instituto Nacional de Astrofsica  ptica y Electrnica  Mxico  mcoyotl  villasen  mmontesg inaoepmx Departamento de Sistemas Informticos y Computacin  Universidad Politcnica de Valencia  Espaa  prossodsicupves Abstract  Authorship attribution is the task of identifying the author of a given text  The main concern of this task is to define an appropriate characterization of documents that captures the writing style of authors  This paper proposes a new method for authorship attribution supported on the idea that a proper iden tification of authors must consider both stylistic and topic features of texts  This method characterizes documents by a set of word sequences that combine functional and content words  The experimental results on poem classification demonstrated that this method outperforms most current stateoftheart ap  proaches  and that it is appropriate to handle the attribution of short docu ments  1 Introduction Authorship attribution is the task of identifying the author of a given text  It can be considered as a typical classification problem  where a set of documents with known authorship are used for training and the aim is to automatically determine the corre  sponding author of an anonymous text  In contrast to other classification tasks  it is not clear which features of a text should be used to classify an author  Consequently  the main concern of computerassisted authorship attribution is to define an appro priate characterization of documents that captures the writing style of authors  There are several methods for authorship attribution  ranging from those using stylistic nontopic features such as the vocabulary richness of the author </raw_string>
  </article>
  <article>
    <title>Segmenting Documents by Stylistic Character</title>
    <count>407</count>
    <raw_string>Natural Language Engineering 11  4   397415  c 2005 Cambridge University Press doi101017S1351324905003694 Printed in the United Kingdom 397 Segmenting documents by stylistic character  NEIL GRAHAM  GRAEME HIRST and BHASKARA MARTHI Department of Computer Science  University of Toronto  Toronto  Ontario  Canada M5S 3G4 email  ghcs torontoedu  Received 26 August 2003  revised 7 March 2004  Abstract As part of a larger project to develop an aid for writers that would help to eliminate stylistic inconsistencies within a document  we experimented with neural networks to find the points in a text at which its stylistic character changes  Our best results  well above baseline  were achieved with timedelay networks that used features related to the authors syntactic preferences  whereas lowlevel and vocabularybased features were not found to be useful  An alternative approach with character bigrams was not successful  1 Introduction There are many different ways that authors can collaborate in the writing of a text  Posner and Baecker 1992   Variables include whether the granularity of the indi vidual authors  unit of contribution is sentences  paragraphs  or sections  whether or not one of the authors acts as editor  revising the writing of all the others  and  if not  whether each author revises only their own work or also that of some or all of the others  Thus a collaboratively written text might be nothing more than a concatenation of segments by each of the participating authors  or it could be essentially the work of a single person heavily influenced  by the other participants  or it could be something between these extremes 1 Regardless of the method by which the </raw_string>
  </article>
  <article>
    <title>Reclaiming Individuality of Mysterious Passage</title>
    <count>405</count>
    <raw_string>Reclaiming Individuality of Mysterious Passage M Chaurasia Member  ACM Dr Sushil Kumar Bhilai Institute of Technology  Durg  INDIA Abstract Authorship attribution  the science of inferring characteristics of author from characteristics of documents written by that author become an urgent need to find the original author of anonymous text  In this paper  a novel approach is proposed that attempts to measure the style variation of author using character ngram profiles  This proposed method is a different approach to identify the author using initial character ngram whereas prior research has shown the identification on total character n gram  This approach will prove to be quite stable  With the help of small experiment  we attempt to prove it  The results acquired from the mention technique are quite accurate and it hikes to 100  in identifying the author from an anonymous text  Using Ngram frequency profiles  it provides a simple and reliable way to categorize documents in a wide range of classification tasks  Keywords  Author identification  Character ngram  Dissimilarity measure  Natural Language Processing  I LITERATURE RIVIEW Organizations  business  academia  government  etc  are facing risks resulting from their ever  increasing reliance on the information infrastructure  Decision and policy makers managing these risks are challenged by a lack of information intelligence concerning the risks and consequences of cyber events They need to understand the implications of cyber security risks and solutions related to their information infrastructure and business  The combination of increased vulnerability  increased stakes and increased threats make cyber security and information intelligence  CSII  one of the most important emerging challenges in the evolution of modern cyberspace mechanism  In some criminal  civil </raw_string>
  </article>
  <article>
    <title>Feature Bagging for Author Attribution</title>
    <count>405</count>
    <raw_string>Feature Bagging for Author Attribution FranoisMarie Giraud and Thierry Artires LIP6  Universit Pierre et Marie Curie  UPMC  Paris  France giraudfpoleia lip6fr  thierry artiereslip6fr Abstract The authorship attribution literature demonstrates the difficulty to de sign classifiers overcoming simple strategies such as linear classifiers operating on a number  most frequent  of lexical features such as character trigrams  We claim this comes  at least partially  from the difficulty to efficiently learn the con tribution of all features  which leads to either undertraining or overtraining of classifiers  To overcome this difficulty we propose to use bagging techniques that rely on learning classifiers on different random subset of features  then to com bine their decision by making them vote  1 Introduction A key issue in author attribution and verification lies in feature definition and selection  which motivated many studies 14  8  One conclusion is that despite many efforts to build smart features 6  very simple ones such as counts  or tfidf like features  of words andor of character ngrams are commonly used  Moreover feature selection is performed using simple criterion such as choosing the most frequent words and charac  ter ngrams  Finally  simple classifiers such as linear SVM have been shown to perform well with above features and such simple systems appear to be difficult to outperform 8  Our work is an attempt to outperform such a simple  and efficient  strategy  It is inspired from two key observations that have been made in the past  First  it has been observed that learning rich models on few training data may yield a form of undertraining 15  where some relevant features are not fully taken into </raw_string>
  </article>
  <article>
    <title>An Authorship Attribution for Serbian</title>
    <count>405</count>
    <raw_string>An Authorship Attribution for Serbian Andelka Zecevic Faculty of Mathematics Studentski trg 16 Belgrade  Serbia andjelkazmatfbg acrs Milo Utvic Faculty of Philology Studentski trg 3 Belgrade  Serbia miskomatfbg acrs ABSTRACT An authorship attribution is a problem of identifying the author of an anonymous or disputed text if there is a closed set of candidate authors  Due to the richness of natural languages and numerous ways of expressing individuality in a writing process  this task employs all the sources of lan guage knowledge  lexis  syntax  semantics  orthography  etc  The impressive results of ngram based algorithms have been presented in many papers for many languages so far  The goal of our research was to test if this group of algorithms works equally well on Serbian and if it is a case  to cal culate the optimal values for the parameters appearing in the algorithms  Also  we wanted to test if a syllable based word decomposition  which represents a more human like word decomposition in comparison to ngrams  can be use  ful in an authorship attribution  Our results confirm good performance of an ngram based approach  accuracy up to 96   and show the potential usefulness of a syllable based approach  accuracy from 81  to 89    Keywords Authorship attribution  classification  ngrams  syllables 1  INTRODUCTION By definition  an authorship attribution is a problem re  lated to identifying the author of an anonymous or dis  puted text if there is a closed set of candidate authors  One of the first studies concerning this topic was published in 1787 by Edmond Malone 12 who argued that Shakespeare did not write some parts of </raw_string>
  </article>
  <article>
    <title>Text Mining the Biomedical Literature</title>
    <count>405</count>
    <raw_string>1 TEXT MINING THE BIOMEDICAL LITERATURE By Dr Ronald N Kostoff Office of Naval Research 875 N Randolph St  Arlington  VA 22217 Phone  7036964198 Internet  kostofronrnavy mil  The views in this report are solely those of the author  and do not represent the views of the Department of the Navy or any of its components  KEYWORDS Text Mining  Information Retrieval  Metrics  Bibliometrics  Computational Linguistics  Biomedical  Information Technology  Asymmetry Detection  Citation Analysis  LiteratureBased Discovery  LiteratureAssisted Discovery  ABSTRACT Text mining of the biomedical literature provides patterns of relationships among concepts  people  and institutions  offering enhanced medical technical intelligence unobtainable by other means  This report describes myriad text mining capabilities  It starts with a description of the larger context of knowledge management  then  addresses components of text mining in particular  Section 1 covers biomedical knowledge management  the role of text mining in knowledge management  and describes the cultural changes and global agreements required to allow the full power and capabilities of text mining to be utilized  The next two sections address information retrieval issues  Section 2 describes the extraction of useful information from the published biomedical literature  Section 3 describes the information content in different record fields  in a major medical database  Report Documentation Page Form ApprovedOMB No 07040188 Public reporting burden for the collection of information is estimated to average 1 hour per response  including the time for reviewing instructions  searching existing data sources  gathering and maintaining the data needed  and completing and reviewing the collection of information  Send comments regarding this burden estimate or any other aspect of this collection of information  </raw_string>
  </article>
  <article>
    <title>Web Content Mining Using MicroGenres</title>
    <count>405</count>
    <raw_string>Chapter 4 Web Content Mining Using MicroGenres Vaclav Snasel  Milos Kudelka  and Zdenek Horak Abstract  The size and growth of the current Web is still creating new challenges to researchers  For example  one of these challenges is the improvement of user familarity to a large number of Web pages  Todays search engines provide tools that allow users to refine their queries  One way is the refinement of a query based on the analysis of web content  Possible outcomes are not only recommended collocations  but also recommended page genres  e g  discussion forums  etc   It is proving to be very useful to provide the details of page content when viewing the page  Not only text snippets  but also parts of the page menu  for certain pages how many posts are present in the discussion  what day the review was created  or what the price is of a product sold on the page  Obtaining this information from unstructured or semistructured content is not straightforward  In this chapter the development of methods capable of detecting and extracting information from Web pages will be addressed  The concept of objects  called MicroGenre will be presented  Finally we also present experiments with our own Pattrio method  which provides a way to detect objects placed on Web pages  41 Introduction The Web is like a big city  It has its center  periphery  buildings and other facilities with various purposes  and communications that connect one another  Also people live in the city  and people move city over time  In order to study the city  one must tackle many tasks  It would be a </raw_string>
  </article>
  <article>
    <title>Exploring Linkability of User Reviews</title>
    <count>405</count>
    <raw_string>Exploring Linkability of User Reviews Mishari Almishari and Gene Tsudik Computer Science Department  University of California  Irvine malmishagts ics uciedu Abstract Large numbers of people all over the world read and contribute to various review sites  Many contrib  utors are understandably concerned about privacy in general and  specifically  about linkability of their reviews  and accounts  across multiple review sites  In this paper  we study linkability of community based reviewing and try to answer the question  to what extent are anonymous  reviews linkable  i e   highly likely authored by the same contributor  Based on a very large set of reviews from one very popular site  Yelp   we show that a high percentage of ostensibly anonymous reviews can be accurately linked to their authors  This is despite the fact that we use very simple models and equally simple features set  Our study suggests that contributors reliably expose their identities in reviews  This has important implications for crossreferencing accounts between different review sites  Also  techniques used in our study could be adopted by review sites to give contributors feedback about linkability of their reviews  1 Introduction In recent years  popularity of various types of review and communityknowledge sites has substantially in creased  Prominent examples include Yelp  Tripadvisor  Epinions  Wikipedia  Expedia and Netflix  They attract multitudes of readers and contributors  While the former usually greatly outnumber the latter  contrib  utors can still number in hundreds of thousands for large sites  such as Yelp or Wikipedia  For example  Yelp had more than 39 million visitors and reached 15 million reviews in late 2010 1  To motivate </raw_string>
  </article>
  <article>
    <title>Detecting Plagiarism in Text Documents</title>
    <count>405</count>
    <raw_string>241 Detecting Plagiarism in Text Documents through GrammarAnalysis of Authors Michael TschuggnallGunther Specht Institute of Computer Science Databases and Information Systems Technikerstrae 21a 6020 Innsbruck michaeltschuggnalluibk acat guenther spechtuibk acat AbstractThe task of intrinsic plagiarism detection is to find plagiarized sections within text documents without using a reference corpusIn this paperthe intrinsic detection approach PlagInn is presented which is based on the assumption that authors use a recognizable and distinguishable grammar to construct sentencesThe main idea is to analyze the grammar of text documents and to find irregularities within the syntax of sentencesregardless of the usage of concrete wordsIf suspicious sentences are found by computing the pqgram distance of grammar trees and by utilizing a Gaussian normal distributionthe algorithm tries to select and combine those sentences into potentially plagiarized sectionsThe parameters and thresholds needed by the algorithm are optimized by using genetic algorithmsFinallythe approach is evaluated against a large test corpus consisting of English documentsshowing promising results1 Introduction 11 Plagiarism Detection Today more and more text documents are made publicly available through large text collections or literary databasesAs recent events showthe detection of plagiarism in such systems becomes considerably more important as it is very easy for a plagiarist to find an appropriate text fragment that can be copiedwhere on the other side it becomes increasingly harder to correctly identify plagiarized sections due to the huge amount of possible sourcesIn this paper we present the PlagInn algorithma novel approach to detect plagiarism in text documents that circumvents large data comparisons by performing intrinsic data anaylsisThe two main approaches for identifying plagiarism in text documents are known as external and intrinsic algorithms PEBC11where external algorithms compare a suspicious document against a givenunrestricted set of source documents like the world wide weband intrinsic methods inspect the suspicious document onlyOften applied techniques 242 </raw_string>
  </article>
  <article>
    <title>Web Genre Benchmark Under Construction</title>
    <count>405</count>
    <raw_string>Marina Santini  Serge Sharoff Web Genre Benchmark Under Construction The project presented in this article focuses on the creation of web genre benchmarks  ak a  web genre reference corpora or web genre test collections   i e  newly conceived test collections against which it will be possible to judge the performance of future genreenabled web applications  The creation of web genre benchmarks is of key importance for the next generation of web applications because  at present  it is impossible to evaluate existing and inprogress genreenabled prototypes  We suggest focusing on the following key points    propose a characterisation of genre suitable for digital environments and empirical approaches shared by a number of genre experts working in automatic genre identification    define the criteria for the construction of web genre benchmarks and draw up annotation guidelines    create web genre benchmarks in several languages    validate the methodology and evaluate the results  We describe work in progress and our plans for future development  Since it is sometimes difficult to anticipate the difficulties that will arise when developing a large resource  we present our ideas  our current views on genre issues and our first results with the aim of stimulating a proactive discussion  so that the stakeholders  i e  researchers who will ultimately benefit the resource  can contribute to its design  1 The Concept of Genre The concept of genre is hard to agree upon  Many interpretations have been proposed since Aristotles Poetics without reaching any definite conclusions about the inventory or even principles for classifying documents into genres  Some studies put the number of genres to      Grlach  </raw_string>
  </article>
  <article>
    <title>Distinguishing Venues by Writing Styles</title>
    <count>405</count>
    <raw_string>Distinguishing Venues by Writing Styles Zaihan Yang Brian D Davison Department of Computer Science and Engineering Lehigh University Bethlehem  PA  18015 USA zay206davison cse lehigh edu ABSTRACT A principal goal for most research scientists is to publish  There are different kinds of publications covering different topics and requiring different writing formats  While authors tend to have unique personal writing styles  no work has been carried out to find out whether publication venues are distinguishable by their writing styles  Our work takes the first step into exploring this problem  Using the traditional classification approach and carrying out ex  periments on real data from the CiteSeer digital library  we demon  strate that venues are also distinguished by their writing styles  Categories and Subject Descriptors  H33  Information Storage and Retrieval  Information Search and Retrieval General Terms  Algorithms  Performance Keywords  classification  features 1  INTRODUCTION For research scientists  a fundamental task is to publish their work  There are many different kinds of publications requiring different writing formats  In this paper  we regard the publishing venues of all kinds of publications as venues  We have differ ent venues for different research domains  for example  the SI GIR conference for Information Retrieval  IR  research  and the VLDB conference for database research  Moreover  even in one research domain  we also have multiple venues  To take the IR research domain as an example  we have journal publications such as JASIST  as well as conferences  such as SIGIR  JCDL  WWW  CIKM  WSDM  etc  With so many different kinds of venues pro vided  a straightforward question may arise </raw_string>
  </article>
  <article>
    <title>Style Analysis of Academic Writing</title>
    <count>405</count>
    <raw_string>Style Analysis of Academic Writing Thomas Scholz and Stefan Conrad HeinrichHeineUniversity Dusseldorf  Institute of Computer Science  Universitatsstr  1  D40225 Dusseldorf  Germany scholzconrad cs uniduesseldorfde http dbscs uniduesseldorfde Abstract  This paper presents an approach which performs a Style Analysis of Academic Writing in terms of formal voice  readability and scientific language  Our intention is an analysis of academic writing style as a feedback for the authors and editors  The extracted features of a document collection are used to create SelfOrganizing Maps which are the interim results to generate reports in our Full Automatic Paper Anal ysis System  Fapas   To evaluate this method  the system has to solve different tasks to verify the informative value of the generated maps and reports  Keywords  Text Mining  Linguistic Style Analysis  User Interfaces and Visualization  1 Motivation The aim of this approach is a method which estimates style of writing  Today it is very important to share your ideas and developments with other people  Besides verbal presentations many new ideas are transferred by written texts  Therefore improving writing skills is very interesting for people who write publications or documentations  Furthermore  an automatic editorial department can be helpful for education institutes or companies who create or handle a great amount of written texts like publishers or press offices  Also a qualitative style analysis can give hints for authorship identification and verification tasks and as well can be used to categorize texts of web pages  However  a major problem with this kind of application is the question  What is style in a text and how can it be evaluated  We design an approach which handles this problem in the specific </raw_string>
  </article>
  <article>
    <title>Unsupervised Multilingual Sentence Boundary Detection</title>
    <count>405</count>
    <raw_string>Unsupervised Multilingual Sentence Boundary Detection Tibor Kiss  Jan Strunk  RuhrUniversitat Bochum RuhrUniversitat Bochum In this article  we present a languageindependent  unsupervised approach to sentence boundary detection  It is based on the assumption that a large number of ambiguities in the determination of sentence boundaries can be eliminated once abbreviations have been identified  Instead of rely ing on orthographic clues  as is usually done  the proposed system is able to detect abbreviations with high accuracy using three criteria that only require information about the candidate type itself and are independent of context  Abbreviations can be defined as a very tight collocation consisting of a truncated word and a final period  abbreviations are usually short  and abbrevi ations sometimes contain internal periods  We also show the potential of collocational evidence for two other important subtasks of sentence boundary disambiguation  namely the detection of initials and ordinal numbers  The proposed system has been tested extensively on eleven different languages and on different text genres  It achieves good results without any further amendments or languagespecific resources  We evaluate its performance against three different baselines and compare it to other systems for sentence boundary detection proposed in the literature  1 Introduction The sentence is a fundamental and relatively wellunderstood unit in theoretical and computational linguistics  Syntactic processing is generally performed on a sentence  bysentence basis  and many processes are constrained by this abstract concept in that they may be active inside a sentence but not between consecutive sentences  Among these  we find collocations  idioms  anaphora  as well as variable binding and quan tification  Given that these processes are crucially constrained by sentence boundaries  the successful determination of these </raw_string>
  </article>
  <article>
    <title>Stable Classification of Text Genres</title>
    <count>405</count>
    <raw_string>Squibs Stable Classification of Text Genres Philipp Petrenz  University of Edinburgh Bonnie Webber  University of Edinburgh Every text has at least one topic and at least one genre  Evidence for a texts topic and genre comes  in part  from its lexical and syntactic featuresfeatures used in both Automatic Topic Classification and Automatic Genre Classification  AGC  Because an ideal AGC system should be stable in the face of changes in topic distribution  we assess five previously published AGC methods with respect to both performance on the same topicgenre distribution on which they were trained and stability of that performance across changes in topicgenre distribution  Our experiments lead us to conclude that  1  stability in the face of changing topical distributions should be added to the evaluation critera for new approaches to AGC  and  2  partofspeech features should be considered individually when developing a highperforming  stable AGC system for a particular  possibly changing corpus  1  Introduction This article concerns Automated Genre Classification  AGC  Genre has a range of definitions  but for Language Technology  a good one is a class of documents that share a communicative purpose  e g  Kessler  Nunberg  and Schutze 1997   Although com municative purpose may be difficult to recognize without document understanding  researchers have found lowlevel features of texts to correlate with genre  making it a useful proxy  AGC can directly benefit Information Retrieval  Freund  Clarke  and Toms 2006   where users may want documents that serve a particular communicative purpose  instructions  reviews  user guides  etc   AGC can also benefit Language Technology indirectly  where differences in the </raw_string>
  </article>
  <article>
    <title>Author Attribution on Streaming Data</title>
    <count>405</count>
    <raw_string>Author Attribution on Streaming Data Sadi Evren SEKER  Khaled AlNAAMI  Latifur KHAN Computer Science Dept  The University of Texas at Dallas Dallas  USA sadi seker  kma041000  lkhanutdallas edu Abstract The concept of novel authors occurring in streaming data source  such as evolving social media  is an unaddressed problem up until now  Existing author attribution techniques deals with the datasets  where the total number of authors do not change in the training or the testing time of the classifiers  This study focuses on the question  what happens if new authors are added into the system by time   Moreover in this study we are also dealing with the problems that some of the authors may not stay and may disappear by time or may re  appear after a while  In this study stream mining approaches are proposed to solve the problem  The test scenarios are created over the existing IMDB62 data set  which is widely used by author attribution algorithms already  We used our own shuffling algorithms to create the effect of novel authors  Also before the stream mining  POS tagging approaches and the TF IDF methods are applied for the feature extraction  And we have applied bitag approach where two consecutive tags are considered as a new feature in our approach  By the help of novel techniques  first time proposed in this paper  the success rate has been increased from 35  to 61  for the authorship attribution on streaming text data  Keywordsdata mining  big data  text mining  natural language processing  POS Tagging  authorship attribution  author recognition  I INTRODUCTION The impact of social networks brings an increasing </raw_string>
  </article>
  <article>
    <title>Genre Categorization of Web Pages</title>
    <count>405</count>
    <raw_string>Chaker Jebari A New Centroidbased Approach for Genre Categorization of Web Pages In this paper we propose a new centroidbased approach for genre cate gorization of web pages  Our approach constructs genre centroids using a set of genrelabeled web pages  called training web pages  The obtained centroids will be used to classify new web pages  The aim of our approach is to provide a flexible  incremental  refined and combined categorization  which is more suitable for automatic web genre identificationOur approach is flexible because it assigns a web page to all predefined genres with a confidence score  it is incremental it classifies web pages one by one  it is refined because at each new web page  our approach either refines centroids or discards the page because it is considered as a noisy page  finally  our approach combines three different categorizations  which are based on the URL address  the logical structure and the hypertext structure  The exper iments conducted on two known corpora show that our approach is very fast and outperform other approaches  1 Introduction As the World Wide Web continues to grow exponentially  web page categorization becomes increasingly important in web searching  Web page categorization  called also web page classification assigns a web page to one or more predefined categories  According to the type of category  categorization can be divided into subproblems  topic categorization  sentiment categorization  genre categorization  and so on  Recently  more attention is given to automatic genre identification of web page be cause it can be used to improve the quality of web search results  see  for example  all the articles in this journal and Mehler et al </raw_string>
  </article>
  <article>
    <title>Stylometric Analysis of Scientific Articles</title>
    <count>405</count>
    <raw_string>Stylometric Analysis of Scientific Articles Shane Bergsma  Matt Post  David Yarowsky Department of Computer Science and Human Language Technology Center of Excellence Johns Hopkins University Baltimore  MD 21218  USA sbergsmajhuedu  postcs jhuedu  yarowskycs jhuedu Abstract We present an approach to automatically re  cover hidden attributes of scientific articles  such as whether the author is a native English speaker  whether the author is a male or a fe male  and whether the paper was published in a conference or workshop proceedings  We train classifiers to predict these attributes in computational linguistics papers  The classi fiers perform well in this challenging domain  identifying nonnative writing with 95  accu racy  over a baseline of 67    We show the benefits of using syntactic features in stylom  etry  syntax leads to significant improvements over bagofwords models on all three tasks  achieving 10  to 25  relative error reduction  We give a detailed analysis of which words and syntax most predict a particular attribute  and we show a strong correlation between our predictions and a papers number of citations  1 Introduction Stylometry aims to recover useful attributes of doc  uments from the style of the writing  In some do  mains  statistical techniques have successfully de duced author identity  Mosteller and Wallace  1984   gender  Koppel et al   2003   native language  Kop  pel et al   2005   and even whether an author has de mentia  Le et al   2011   Stylometric analysis is im  portant to marketers  analysts and social scientists because it provides demographic data directly from raw </raw_string>
  </article>
  <article>
    <title>Authorship Identification for Online Text</title>
    <count>405</count>
    <raw_string>Authorship Identification for Online Text Richmond Hong Rui Tan and Flora S Tsai School of Electrical  Electronic Engineering Nanyang Technological University Singapore ta0002ndntuedu sg and fst1columbia edu AbstractAuthorship identification for online text such as blogs and ebooks is a challenging problem as these documents do not have a considerable amount of content  Therefore  identification is much harder than other documents such as books and reports  The paper investigates the choice of features and classifier accuracy which are suitable for such texts  Syntactic features are found to be good for large data sets  whereas lexical features are good for small data sets  The results can be used to customize and further improve authorship detection techniques according to the characteristics of the writing samples  Keywordsauthorship identification  authorship detection  au  thorship attribution  data mining  classification  blog I INTRODUCTION Authorship identification  detection or attribution  is a process of identifying the most likely author of a disputed or anonymous document  based on a collection of known documents  As the usage of the Web has increased over the past few decades  more and more activities can be done in cyberspace  Dissemination of information can be performed using various methods with tremendous speed  For example  websites and RSS feeds are updated regularly where users can get a wealth of information with minimal effort  Social networking platforms like blogs and online discussion forums facilitate the ease of networking and information sharing with little or no cost 11  Not only can information posted on the Web reach out to millions of people in a very short time  users can also provide information anonymously  Hence  those with ill in tentions can utilize these </raw_string>
  </article>
  <article>
    <title>Classifier Ensembles for Genre Recognition</title>
    <count>404</count>
    <raw_string>Classifier ensembles for genre recognition  Pedro J Ponce de Leon  Jose  M Inesta  Carlos PerezSancho Universidad de Alicante  Departamento de Lenguajes y Sistemas Informaticos PO box 99  E03080 Alicante  Spain pierre inestacperez dlsi uaes http grfiadlsi uaes Abstract Previous work done in genre recognition and characterization from sym  bolic sources  monophonic melodies extracted from MIDI files  have pointed our research to the use of classifier ensembles to better accomplish the task  This work presents current research in the use of voting ensembles of classifiers trained on statistical description models of melodies  in order to improve both the accuracy and robustness of single classifier systems in the genre recognition task  Different voting schemes are discussed and compared  and results for a corpus of Jazz and Classical music pieces are presented and assesed  Keywords  Statistical pattern recognition  Classifier ensembles  Music infor mation retrieval  Musical genre recognition 1 Introduction Some recent works explore the capabilities of machine learning or pattern recognition methods to recognise music genre  either using audio 1  2  3  or symbolic 3  4  5 sources  or even metadata 6  After a period of time doing research on the use of statistical models and classification paradigms for music genre  or style  character  ization from symbolic data 7  8  we reached a point where the combination of the different learning systems we developed showed up as the logical next step in our research  The many ways of building classifier ensembles  i e   combining different classifiers  to improve both the accuracy and robustness of single classifiers is a hot topic in the areas of machine learning </raw_string>
  </article>
  <article>
    <title>Genre Categorization of Web Pages</title>
    <count>404</count>
    <raw_string>Refined and Incremental Centroidbased approach for Genre Categorization of Web pages Chaker Jebari King Saud University College of Computer and Information Sciences Computer Science Department P O Box 51178  Riyadh 11543 Kingdom of Saudi Arabia jebarichakeryahoofr ABSTRACT In this paper  I propose a refined and incremental centroidbased approach for genre categorization of web pages  My approach is based on the construction of genre centroids using a set of training web pages  These centroids will be used to classify new web pages  The originality of my approach is the implementation of two new aspects  which are refining and incrementing  My approach is based on the combination of three information sources  which are the URL address  the logical structure and the hypertext structure  Conducted experiments show that the proposed approach is very fast and provide microaverage accuracy more than 95   which is better than those obtained by other works on genre categorization of web pages and other machine learning techniques  Categories and Subject Descriptors I2 Artificial Intelligence  Natural Language Processing  Learning  H3  Information Storage and Retrieval  Information Storage  General Terms  Algorithms  Experimentation  Measurement  Performance  Keywords  Categorization  genre  centroid  refining  incrementing  combination  1  INTRODUCTION In front of the explosive growth of the number of web pages  users cannot quickly find desired information among the huge list of web pages returned by a search engine  To deal with this problem  many recent works have interested with genre categorization of web pages 17  Generally  the genre reflects the style of the document  Many definitions of genre have been proposed  which concerns non  digital documents  </raw_string>
  </article>
  <article>
    <title>Genre Categorization of Web Pages</title>
    <count>402</count>
    <raw_string>Genre categorization of web pages Jebari Chaker1 and Ounelli Habib2 1King Saud University  College of Computer and Information Sciences  Computer Science Department  P O Box 51178  Riyadh 11543  Kingdom of Saudi Arabia Jebarichakeryahoofr 2Universit de Tunis ElManar  Facult des Sciences de Tunis  Dpartement d Informatique  Campus Universitaire 1060 Tunis  Tunisia habibounellifst rnutn Abstract With the increase of the number of web pages  it is very difficult to find wanted information easily and quickly out of thousands of web pages retrieved by a search engine  To solve this problem  many researches propose to classify documents according to their genre  which is another criteria to classify documents different from the topic  Most of these works assign a document to only one genre  In this paper we propose a new flexible approach for document genre categorization  Flexibility means that our approach assigns a document to all predefined genres with different weights  The proposed approach is based on the combination of two homogenous classifiers  contextual and structural classifiers  The contextual classifier uses the URL  while the structural classifier uses the document structure  Both contextual and structural classifiers are centroidbased classifiers  Experimentations provide a microaveraged break  even point  BEP  more than 85   which is better than those obtained by other categorization approaches  1  Introduction In front of the explosive growth of the number of web pages  users cannot quickly find desired information among the huge list of web pages returned by a search engine  To deal with this problem  many categorization approaches have been proposed to classify the result of search engines  Most of them have been interested by topic categorization 32 </raw_string>
  </article>
  <article>
    <title>Source Code Authorship Attribution</title>
    <count>386</count>
    <raw_string>Source Code Authorship Attribution A thesis submitted for the degree of Doctor of Philosophy Steven David Burrows BAppSc  Hons    School of Computer Science and Information Technology  College of Science  Engineering and Health  RMIT University  Melbourne  Victoria  Australia  4th November  2010 Declaration I certify that except where due acknowledgement has been made  the work is that of the author alone  the work has not been submitted previously  in whole or in part  to qualify for any other academic award  the content of the thesis is the result of work which has been carried out since the official commencement date of the approved research program  and  any editorial work  paid or unpaid  carried out by a third party is acknowledged  Steven David Burrows School of Computer Science and Information Technology RMIT University 4th November  2010 i ii Acknowledgements I first and foremost thank my PhD supervisors Dr Alexandra Uitdenbogerd and Assoc Prof Andrew Turpin for their ongoing encouragement  guidance  and feedback  Words alone cannot articulate my gratitude  I am indebted to the support they have given me  for which I am truly thankful  I also thank my previous PhD supervisors Prof Justin Zobel and Dr Saied Tahaghoghi for their earlier supervision before moving on to other positions  I received an exceptional introduction to the world of research from them during my honours degree  I thank Upali Wickramasinghe for being my thesis reading buddy for exchanging thesis chapters for feedback  The discussions we had were very valuable for improving the thesis  I also thank Andrew Atkinson and David Burrows for taking the time to proof read a full draft of </raw_string>
  </article>
  <article>
    <title>Expressive Music Performance Modelling</title>
    <count>371</count>
    <raw_string>Expressive Music Performance Modelling Andreas Neocleous MASTER THESIS UPF  2010 Master in Sound and Music Computing Master thesis supervisor  Rafael Ramirez Department of Information and Communication Technologies Universitat Pompeu Fabra  Barcelona ii Acknowledgements I would like to thank my advisor Prof Rafael Ramirez for his consistent and valuable support during the process of research and preparation of this thesis  I would also like to thank Prof Xavier Serra for his support and the opportunity he gave me to be part of the music technology group  I am also grateful to Esteban Maestre  Alfonso Perez and Panos Papiotis for their help  valuable comments and suggestions  Finally  I would like to thank my family for their endless support  iii Abstract Machine learning approaches to modelling emotions in music performances were investigated and presented in this thesis  In particular  we investigated how professional musicians encode emotions  such as happiness  sadness  anger  fear and sweetness  in violin and saxophone audio performances  Suitable melodic description features were extracted from audio recordings  Following that  we applied various machine learning techniques for training expressive performance models  A model was trained for each emotion considered  Finally  new expressive performances were synthesized from inexpressive melody descriptions  i e  music scores  using the induced models and the result was perceptually evaluated by asking a number of people to listen  compare and evaluate to the computer generated performances  Several machine learning techniques for inducing the expressive models were systematically explored and we present the results  iv v Index Page Abstract iv List of Figures  vii List of Tables  ix 1  Introduction  1 11  Motivation  1 12 </raw_string>
  </article>
  <article>
    <title>Translation and Scientific Terminology</title>
    <count>366</count>
    <raw_string>Translation and Scientific Terminology A CorpusBased Multilingual Study Meng Ji University of Tokyo  Japan Abstract It is well known that the establishment of modern scientific language in the late nineteenth century was instrumental in the making of China s and Japans early modern scientific identity  The current study aims to offer an original empirical investigation of early modern Chinese scientific terminology from a cognitive and functional perspective  which has been rarely explored before  Through a detailed corpusbased linguistic investigation  the present study probes the complex historical process of crosscultural and crosslinguistic scientific exchange between the West and China and Japan in the late nineteenth century  The new insights brought about by the novel use of statistical methods point to potentially prolific directions for future research in contrastive historical linguistics and science history  Keywords  corpus linguistics  translation studies  multilingual studies  Chinese  Dutch  English  French  German  Japanese   scientific terminology  corpus statistics  1  Introduction The main purpose of this paper is to explore the complex historical process of crosscultural and crosslinguistic interaction between China  Japan and the West in the late nineteenth century  Instead of following a prescriptive approach to the subject matter which characterizes many past studies  the present paper attempts to formulate and test theoretical hypotheses through the generation and exploration of quantitative textual data retrieved from newly created largescale online multilingual databases of historical texts  It represents an important effort in the development and innovation of research methodologies and analytical techniques in historical linguistics and related fields such as science history and crosscultural studies  The establishment of a working model for the translation of scientific terminology played a central role in the introduction and </raw_string>
  </article>
  <article>
    <title>On combining multiple clusterings</title>
    <count>330</count>
    <raw_string>Appl Intell  2010  33  207219 DOI 101007s1048900901604 On combining multiple clusterings  an overview and a new perspective Tao Li  Mitsunori Ogihara  Sheng Ma Published online  12 February 2009  Springer ScienceBusiness Media  LLC 2009 Abstract Many problems can be reduced to the problem of combining multiple clusterings  In this paper  we first sum  marize different application scenarios of combining multi ple clusterings and provide a new perspective of viewing the problem as a categorical clustering problem  We then show the connections between various consensus and clustering criteria and discuss the complexity results of the problem  Finally we propose a newmethod to determine the final clus  tering  Experiments on kinship terms and clustering popular music from heterogeneous feature sets show the effective  ness of combining multiple clusterings  Keywords Multiple clusterings  Combining  Categorical 1 Introduction 11 Problem overview Generally the problem of combining multiple clusterings is  given multiple clusterings of the dataset  find a combined clustering which would provide better cluster results  Many problems can be reduced to the problem of combining mul tiple clusterings  T Li   School of Computer Science  Florida International University  11200 SW 8th Street  Miami  FL 33199  USA email  taolics fiuedu M Ogihara Department of Computer Science  University of Miami  1365 Memorial Drive  Coral Gables  FL 33146  USA S Ma Machine Learning for Systems  IBM TJ Watson Research Center  17 Skyline Drive  Hawthorne  NY 10532  USA  Ensemble clustering  Clustering is an inherently illposed problem due to the lack of label information  Different clustering algorithms and evenmultiple replications of the same algorithm result in different </raw_string>
  </article>
  <article>
    <title>Search in Constraint Programming</title>
    <count>329</count>
    <raw_string>Search in Constraint Programming Gilles Pesant  Polytechnique Montreal   Meinolf Sellmann  Brown University  November 29  2009  December 4  2009 1 Overview of the Field Constraint Programming  CP  is a powerful technique to solve combinatorial problems  It applies sophisti cated inference to reduce the search space and a combination of variable  and valueselection heuristics to guide the exploration of that search space  Like Integer Programming  one states a model of the problem at hand in mathematical language and also builds a search tree through problem decomposition  But there are mostly important differences  among them  CP works directly on discrete variables instead of relying mostly on a continuous relaxation of the model  the modeling language offers many highlevel primitives representing common combinatorial substructures of a problem  often a few constraints are sufficient to express complex problems  each of these primitives  called constraints  may have its own specific algorithm to help solve the problem  one does not branch on fractional variables but instead on indeterminate variables  which currently can take several possible values variables are not necessarily fixed to a particular value at a node of the search tree   even though CP can solve optimization problems  it is primarily designed to handle feasibility problems  We believe that a more principled and autonomous approach for search efficiency has to be started in Constraint Programming  Our main global objective is to advance in a significant way research on search automatization in CP so that combinatorial problems can be solved in a much more robust way  By furthering the automation of search for combinatorial problem solving  this workshop should have a direct impact on the range </raw_string>
  </article>
  <article>
    <title>Automatically Identifying Pseudepigraphic Texts</title>
    <count>329</count>
    <raw_string>Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing  pages 14491454  Seattle  Washington  USA  1821 October 2013  c2013 Association for Computational Linguistics Automatically Identifying Pseudepigraphic Texts Moshe Koppel Bar Ilan University RamatGan  52900  Israel moishkgmailcom Shachar Seidman Bar Ilan University RamatGan  52900  Israel shachar9gmailcom Abstract The identification of pseudepigraphic texts  texts not written by the authors to which they are attributed  has important historical  fo rensic and commercial applications  We in troduce an unsupervised technique for identi fying pseudepigrapha  The idea is to identify textual outliers in a corpus based on the pair wise similarities of all documents in the cor pus  The crucial point is that document simi larity not be measured in any of the standard ways but rather be based on the output of a re  cently introduced algorithm for authorship ve rification  The proposed method strongly outperforms existing techniques in systematic experiments on a blog corpus  1 Introduction The Shakespeare attribution problem is centuries old and shows no signs of abating  Some scholars argue that some  or even all  of Shakespeares works were not actually written by him  The most mainstream theory  and the one that interests us here  is that most of the works were written by Shakespeare  but that several of them were not  Could modern methods of computational author ship attribution be used to detect which  if any  of the works attributed to Shakespeare were not writ ten by him  More generally  this paper deals with the unsu pervised problem of detecting pseudepigrapha  documents in a supposedly singleauthor corpus that were not actually written by the corpuss pre </raw_string>
  </article>
  <article>
    <title>From Fingerprint to Writeprint</title>
    <count>327</count>
    <raw_string>76 April 2006Vol  49  No 4 COMMUNICATIONS OF THE ACM FROM FINGERPRINT TO WRITEPRINT Fingerprintbased identification has been the oldest biometric technique suc  cessfully used in conventional crime investigation  The unique  immutable patterns of a fingerprintthe pattern of ridges and furrows as well as the minutiae pointscan help a crime investigator infer the identities of suspects  ILLUSTRATION BY PAUL WILEY By JIEXUN LI  RONG ZHENG  and HSINCHUN CHEN Identifying the key features to help identify and trace online authorship  However  circumstances have changed since the emergence and rapid proliferation of cybercrime  Generally  cybercrime includes Internet fraud  computer hack  ingnetwork intrusion  cyber piracy  spreading of malicious code  and so on  Cyber criminals post online messages over various Webbased channels to distribute illegal materials  including pirated software  child pornography  and stolen property  Moreover  international criminals and ter rorist organizations such as Osama bin Laden and Al Qaeda use online messages as one of their major communication media  Since people are not usually required to provide their real identity in cyberspace  the anonymity makes identity tracing a critical problem in cybercrime investigation  This problem is further complicated by the sheer amount of cyber users and activities  Unlike conventional crimes  there are no fingerprints to be found in cybercrime  For tunately  there is another type of print  which we call writeprint   hidden in peo  ples writings  Similar to a fingerprint  a writeprint is composed of multiple features  such as vocabulary richness  length of sen tence  use of function words  layout of paragraphs  and keywords  These writeprint features can represent an authors writing </raw_string>
  </article>
  <article>
    <title>Text Classification Method Review</title>
    <count>325</count>
    <raw_string>Decision Engineering Report Series Edited by Rajkumar Roy and David Baxter TEXT CLASSIFICATION METHOD REVIEW By Aigars Mahinovs and Ashutosh Tiwari April 2007 Cranfield University Cranfield Bedfordshire MK43 OAL United Kingdom http wwwcranfield acuk  Cranfield University 2005  All rights reserved  No part of this publication may be reproduced without the written permission of the copyright owner  ISBN 9781861941282 Decision Engineering  is an emerging discipline that focuses on developing tools and techniques for informed operational and business decisionmaking within industry by utilising data and information available at the time  facts  and distributed organisational knowledge  The Decision Engineering Report Series  from Cranfield University publishes the research results from the Decision Engineering Centre  The research centre aims to establish itself as the leader in applied decision engineering research  The client base of the centre includes  Airbus  BAE SYSTEMS  BOC Edwards  BT Exact  Corus  EDS  Electronic Data Systems   Ford Motor Company  GKN Aerospace  Ministry of Defence  UK MOD   Nissan Technology Centre Europe  Johnson Controls  PRICE Systems  RollsRoyce  Society of Motor Manufacturers and Traders  SMMT  and XR Associates  The intention of the report series is to disseminate the centres findings faster and with greater detail than regular publications  The reports are produced on the core research interests within the centre   Cost Engineering  Product Engineering  Applied soft computing Edited by  Professor Rajkumar Roy David Baxter rroycranfield acuk dbaxtercranfield acuk Decision Engineering Centre Cranfield University Cranfield Bedfordshire MK43 OAL United Kingdom http wwwcranfield acuk Series librarian  John Harrington jharringtoncranfield acuk Kings Norton Library Cranfield University Publisher  Cranfield University Abstract With the explosion of information fuelled by the </raw_string>
  </article>
  <article>
    <title>Email Author Attribution</title>
    <count>325</count>
    <raw_string>Email Author Attribution Shrutiranjan Satapathy Sumit Bhagwani Computer Science and Engineering Computer Science and Engineering IIT Kanpur IIT Kanpur Kanpur  Uttar Pradesh 208016 Kanpur  Uttar Pradesh 208016 sranjanscse iitk acin sumitbcse iitk acin ABSTRACT This work aims at exploring various linguis  tic style markers used in distinguishing email authors  We focus on features based only on body of the emails  A selection criterion is im  posed on the markers to select the most impor tant style markers as features  These features are then used in classifying anonymous email body texts into a predefined set of authors as part of a HardClassification Supervised Learn  ing scheme using Support Vector Machines  1 INTRODUCTION Email is used in many different situations as  for ex  ample  in the exchange and broadcasting of messages  documents and for conducting electronic commerce  Unfortunately it can be misused for the distribution of unsolicited andor inappropriate messages and doc  uments  Examples include unauthorized conveyancing of sensitive information  mailing of offensive or threat ening material  etc  In such a scenario  identification of the sender of these mails becomes very important  Other application areas include identifying messages known to be from major enemy players  intercepting documents discussing terrorist attack plans  etc  Authorship identification has been studied a lot as part of Stylometry4  Linguistic stylometry attempts to distinguish authors based on certain linguistic char acteristics unique to each author  Many of such charac  teristics are easy to understand  such as  average word length  average sentence length  number of punctua  tions  net abbreviations  etc  We attempt to pool in many such stylometric features  and perform analyses to </raw_string>
  </article>
  <article>
    <title>Novel Topic Authorship Attribution</title>
    <count>325</count>
    <raw_string>NAVAL POSTGRADUATE SCHOOL MONTEREY  CALIFORNIA THESIS NOVEL TOPIC AUTHORSHIP ATTRIBUTION by Randale J Honaker March 2011 Thesis CoAdvisors  Craig Martell Ralucca Gera Approved for public release  distribution is unlimited THIS PAGE INTENTIONALLY LEFT BLANK REPORT DOCUMENTATION PAGE Form ApprovedOMB No 07040188 The public reporting burden for this collection of information is estimated to average 1 hour per response  including the time for reviewing instructions  searching existing data sources  gathering and maintaining the data needed  and completing and reviewing the collection of information  Send comments regarding this burden estimate or any other aspect of this collection of information  including suggestions for reducing this burden to Department of Defense  Washington Headquarters Services  Directorate for Information Operations and Reports  07040188   1215 Jefferson Davis Highway  Suite 1204  Arlington  VA 222024302  Respondents should be aware that notwithstanding any other provision of law  no person shall be subject to any penalty for failing to comply with a collection of information if it does not display a currently valid OMB control number  PLEASE DO NOT RETURN YOUR FORM TO THE ABOVE ADDRESS 1  REPORT DATE  DDMMYYYY  2 REPORT TYPE 3 DATES COVERED  From  To  4  TITLE AND SUBTITLE 5a  CONTRACT NUMBER 5b  GRANT NUMBER 5c  PROGRAM ELEMENT NUMBER 5d  PROJECT NUMBER 5e  TASK NUMBER 5f  WORK UNIT NUMBER 6  AUTHOR S  7 PERFORMING ORGANIZATION NAME S  AND ADDRESSES  8 PERFORMING ORGANIZATION REPORT NUMBER 9  SPONSORING  MONITORING AGENCY NAME S  AND ADDRESSES  10  SPONSORMONITORS ACRONYMS  11  SPONSORMONITORS REPORT NUMBER S  12  DISTRIBUTION  AVAILABILITY STATEMENT 13  SUPPLEMENTARY NOTES 14  ABSTRACT </raw_string>
  </article>
  <article>
    <title>Event Representation across Genre</title>
    <count>324</count>
    <raw_string>Event representation across genre Lidia Pivovarova  Silja Huttunen and Roman Yangarber University of Helsinki Finland Abstract This paper describes an approach for investi gating the representation of events and their distribution in a corpus  We collect and analyze statistics about subjectverbobject triplets and their content  which helps us com pare corpora belonging to the same domain but to different genretext type  We argue that event structure is strongly related to the genre of the corpus  and propose statistical proper ties that are able to capture these genre differ ences  The results obtained can be used for the improvement of Information Extraction  1 Introduction The focus of this paper is collecting data about certain characteristics of events found in text  in order to improve the performance of an Infor mation Extraction  IE  system  IE is a tech  nology used for locating and extracting specific pieces of informationor facts from unstruc  tured naturallanguage text  by transforming the facts into abstract  structured objects  called events  In IE we assume that events represent realworld facts and the main objective is to extract them from plain text  the nature of the events themselves rarely receives indepth attention in current research  Events may have various relationships to real world facts  and different sources may have contra  dictory views on the facts   Saur  and Pustejovsky  2012   Similarly to many other linguistic units  an event is a combination of meaning and form  the structure and content of an event is influenced by both the structure of the corresponding realworld fact and by the properties of the surrounding text  We use the notion of scenario to denote a set </raw_string>
  </article>
  <article>
    <title>Degraded Document Image Enhancement</title>
    <count>324</count>
    <raw_string>Degraded Document Image Enhancement G Agama  G Bala  G Friederb  O Friedera aIllinois Institute of Technology  Chicago  IL 60616 bThe George Washington University  Washington  DC 20052 ABSTRACT Poor quality documents are obtained in various situations such as historical document collections  legal archives  security investigations  and documents found in clandestine locations  Such documents are often scanned for automated analysis  further processing  and archiving  Due to the nature of such documents  degraded doc  ument images are often hard to read  have low contrast  and are corrupted by various artifacts  We describe a novel approach for the enhancement of such documents based on probabilistic models which increases the contrast  and thus  readability of such documents under various degradations  The enhancement produced by the proposed approach can be viewed under different viewing conditions if desired  The proposed approach was evaluated qualitatively and compared to standard enhancement techniques on a subset of historical documents obtained from the Yad Vashem Holocaust museum  In addition  quantitative performance was evaluated based on synthetically generated data corrupted under various degradation models  Preliminary results demonstrate the effectiveness of the proposed approach  Keywords  degraded document image enhancement  historical document image enhancement  document image analysis  document degradation models  image enhancement  image analysis 1  INTRODUCTION Degraded documents are archived and preserved in large quantities worldwide  Electronic scanning is a common approach in handling such documents in a manner which facilitates public access to them  Such document images are often hard to read  have low contrast  and are corrupted by various artifacts  Thus  given an image of a faded  washed out  damaged </raw_string>
  </article>
  <article>
    <title>Source Code Authorship Attribution</title>
    <count>324</count>
    <raw_string>Poster  Source Code Authorship Attribution Aylin Caliskan Islam  PhD Student  Advisor  Dr Rachel Greenstadt Drexel University  Department of Computer Science  Philadelphia  PA 19104 Webpage  https wwwcs drexel edu ac993 Email  ac993drexel edu I PROBLEM AND MOTIVATION As information becomes widely available and easily ac  cessible through the Internet and other sources  the trend of plagiarism has been increasing  Plagiarism and copyright infringement are issues that come up in both academic and corporate environments  We need author classification tech  niques to inhibit such unethical violations  Source code is also intellectual property and reflects individual style  It is impor tant to be able to identify the author of source code  Building a tool to detect the author of a program in an automated way aids in resolving copyleft  copyright and plagiarism issues in the programming fields  Making authorship attribution tools available to the public will also raise consciousness and decrease any possible tendency to plagiarize  Code stylometry methods can also attribute authors to malware or malicious code  Deanonymization techniques could be used to identify where the malware originates and link collaborating authors of malicious code  Obtaining this information will enable us to analyze the interaction graphs of malware authors  We could infer which author holds certain tools and how malware spreads and evolves  In this project  I perform authorship attribution on source code that is written in C I investigate how well we can classify authors within and across projects  How much source code  which machine learning classifier and which features are required for an accurate analysis of this specific problem  Answering these questions could provide proof of authorship in court or automate the </raw_string>
  </article>
  <article>
    <title>Web Page Genre Classification</title>
    <count>324</count>
    <raw_string>A Pure URLbased Genre Classification of Web Pages Chaker Jebari Information Technology Department Ibri College of Applied Sciences Sultanate of Oman jebarichakeryahoofr Abstract In this paper  we propose a new approach for multilabel genre classification of web pages that exploits character ngrams extracted from the URL of the web page rather than its content  Using only the URL reduces the time needed for feature extraction since it does not need to download the content of the web page  Our approach deals with the complexity of web pages because it uses a multilabel classification where each web page can be assigned to more than one genre  Moreover  our approach implements a new weighting technique that exploits the structure of the URL Experiments conducted on a known multilabel dataset show that our approach achieves encouraging results  Keywordsweb page  multilabel classification  genre  URL structure I INTRODUCTION As the World Wide Web continues to grow exponentially  the classification of web pages becomes increasingly important in web searching  Web page classification  assigns a web page to one or more predefined classes  According to the type of the class  the classification can be divided into subproblems  topic classification  sentiment classification  genre classification  and so on  Currently  search engines use keywords to classify web pages  Returned web pages are ranked and displayed to the user  who is often not satisfied with the result  For example  searching for the keyword Java will provide a list of web pages containing the word Java and belonging to different genres such as tutorial  exam  Call for papers  etc  Therefore  web page genre classification could be used to improve the retrieval quality of </raw_string>
  </article>
  <article>
    <title>Distinguishing Topic from Genre</title>
    <count>324</count>
    <raw_string>Tochtermann  Maurer  Eds    Proceedings of the IKNOW 06  Graz 6th International Conference on Knowledge Management Journal of Universal Computer Science  pp  449456  ISSN 0948695x Distinguishing Topic from Genre Benno Stein and Sven Meyer zu Eissen  Faculty of Media  Media Systems Bauhaus University Weimar  Germany benno steinmedien uniweimarde  Abstract  This paper contributes to a facet from the area of Web Information Retrieval that has recently received much attention  The satisfaction of a users personal information need with respect to text type  presentation type  or information quality  We imply that such properties can be quantified for all kinds of Web documents  and we subsume them under the term Web genre  or genre  Recent surveys show that there is  to a certain degree  a common understanding of Web genre  However  the strictness by which genre and nongenre aspects of a document are experienced is an individual matter  To get a better understanding of the challenges of Web genre identification and its possible limits we investigate in this paper a very interesting question  which has not been posed by now  Given a categorization C of documents  or bookmarks  links  document identifiers   can we provide a reliable assessment whether C is governed by topic or by genre considerations  We present instruments to answer this question as well as to make a distinct statement about the homogeneity of a categorization  Key Words  Genre analysis  Personal IR  Knowledge discovery  Unsupervised learning Category  H33 Information Search and Retrieval  Retrieval models  Information filtering 1 Introduction and Background Nearly all retrieval processes are topiccentered  We type in </raw_string>
  </article>
  <article>
    <title>Taxonomy of Behavioural Biometrics</title>
    <count>324</count>
    <raw_string>1 Copyright  2010  IGI Global  Copying or distributing in print or electronic forms without written permission of IGI Global is prohibited  Chapter 1 Taxonomy of Behavioural Biometrics Roman V Yampolskiy University of Louisville  USA Venu Govindaraju University at Buffalo  USA IntroductIon to BehavIoral BIometrIcs With the proliferation of computers in our every day lives need for reliable computer security steadily increases  Biometric technologies provide user friendly and reliable control methodology for ac  cess to computer systems  networks and workplaces  Angle  Bhagtani   Chheda  2005  Dugelay  et al   2002  Lee  Park  2003   The majority of research is aimed at studying well established physical bio metrics such as fingerprint  Cappelli  Maio  Maltoni  Wayman   Jain  2006  or iris scans  Jain  Ross   Prabhakar  2004d   Behavioural biometrics systems are usually less established  and only those which are in large part based on muscle control such as keystrokes  gait or signature are well analyzed  Bolle  Connell  Pankanti  Ratha   Senior  2003  Delac  Grgic  2004  Jain  Pankanti  Prabhakar  Hong   Ross  2004c  Ruggles  2007  Solayap  pan  Latifi  2006  Uludag  Pankanti  Prabhakar   Jain  2004   Behavioural biometrics provide a number of advantages over traditional biometric technologies  They can be collected nonobtrusively or even without the knowledge of the user  Collection of aBstract This chapter presents a taxonomy of the latest behavioural biometrics  including some future oriented approaches  Current research in the field is examined and analyzed along </raw_string>
  </article>
  <article>
    <title>Web Page Genre Classification</title>
    <count>324</count>
    <raw_string>Web Page Genre Classification Guangyu Chen  Ben Choi Computer Science Louisiana Tech University  LA 71272  USA proBenChoi org Abstract  In this paper we present an automatic genrebased Web page classification system  Unlike subject or topic based classifications  genrebased classifications focus on functional purposes and classify web pages into categories such as online shopping  technical paper  or discussion forum  Until now  the genre classifications are not well developed due to the subjectivities and difficulties to define the genre  the features  and even the categories  In this paper  we define five toplevel genre categories  each of which has several subcategories  and develop new methods to extract 31 features from Web pages to identify the categories  We analyze not only the contents of the Web pages  but also the URLs  HTML tags  Java scripts  and VB scripts  We developed a genre classification system that achieved average accuracy of 93   In addition  we combined this genre classification with our subjectbased classification to produce a comprehensive Web page classification system  Categories and Subject Descriptors  H33  Information Search and Retrieval  Information filtering  Relevance feedback  General Terms  Algorithms  Design  Experimentation  Keywords  Web Ontology  Semantic Web  Knowledge Classification  Web Mining  Information Retrieval  1  Introduction There are two main types of Web page classifications  In subjectbased classification  also known as topicbased classification   Web pages are classified according to their contents or subjects 23  Alternatively  some classifications are genre 14689 or style based focusing on genre related factors  such as document structure or format  purpose of the page  and </raw_string>
  </article>
  <article>
    <title>Web Page Genre Classification</title>
    <count>324</count>
    <raw_string>International Journal of Computer Applications  0975  8887  Volume 88  No13  February 2014 13 Web Page Genre Classification  Impact of nGram Lengths K Pranitha Kumari Assistant Professor Department of Computer Science and Engineering Osmania University Hyderabad AVenugopal Reddy  PhD Professor Department of Computer Science and Engineering Osmania University Hyderabad SSameen Fatima  PhD Professor Department of Computer Science and Engineering Osmania University Hyderabad ABSTRACT Web pages are discriminated based on their topic and genre  Web page genres are capable to improve the modern search engines to focus on the users information need  In this paper  web pages are represented using character ngrams  Character ngram representation is language independent and allows automatic extraction of features from a web page  Character ngram representation of a web page can be used efficiently to classify a web page by genre  Support Vector Machine  SVM  classification model is used for classification and experiments were carried out on 7Genre corpus by varying the length of ngrams  It is observed that the performance in terms of F measure improves as ngram lengths are varied from 3 to 5 and it is also observed that performance degrades as the n gram length is further increased  General Terms Genre classification  corpus Keywords Character ngram feature extraction  ngram length  web page representation  SVM classifier and term frequency 1  INTRODUCTION Web page genre classification is helpful in filtering the results of online searches  In order to classify Web pages by genre  it is necessary to extract features that effectively separate each web page and genre  In web page genre classification  web pages are represented through different ways such as keywords  ngrams  POS tags  </raw_string>
  </article>
  <article>
    <title>Age Detection in Chat</title>
    <count>324</count>
    <raw_string>Age Detection in Chat Jenny Tam and Craig H Martell Department of Computer Science Naval Postgraduate School 1411 Cunningham Road  Monterey  California jktam  cmartell nps edu Abstract This paper presents the results of using statistical analysis and automatic text categorization to identify an authors age group based on the author s online chat posts  A Naive Bayesian Classifier and Support Vector Machine  SVM  model were used  The SVM model experiments generated an fscore measurement of 0996 on test data distinguishing teens from adults  We also introduce an alternative method for generating stop words  that chooses ngrams based on their relative distribution across the classes  Keywordsonline chat  age classification  Support Vector Machine  Nave Bayesian Classifier  stop words I INTRODUCTION The second Youth Internet Safety Survey conducted in 2005 by the Crimes Against Children Research Center found that the percentage of youths receiving unwanted sexual solicitations has declined  The percentage of aggressive solicitations seeking offline contact  however  has not  Education and law enforcement may have deterred casual solicitors  but not the more determined or compulsive solicitors 8  To catch online predators  law enforcement officers or volunteers pose as youths in online chat rooms  Given the limited number of law enforcement officers and volunteers  being able to detect adults soliciting youths using an automated system will help law enforcement officials  Internet providers could also add such a system as a feature for parental control 10  Though it is a crime for adults to sexually exploit a child  depending on state law  it is not always a crime for teens to solicit other teens  When analyzing suspicious chat behavior  it is important that law enforcement </raw_string>
  </article>
  <article>
    <title>Web Page Genre Classification</title>
    <count>324</count>
    <raw_string>An ngram Based Approach to Multilabeled Web Page Genre Classification Jane E Mason Michael Shepherd Jack Duffy Vlado Keelj Carolyn Watters Dalhousie University Dalhousie University Dalhousie University Dalhousie University Dalhousie University jmasoncs dalca shepherdcs dalca jack duffydalca vladocs dalca watterscs dalca Abstract The extraordinary growth in both the size and popularity of the World Wide Web has created a growing interest not only in identifying Web page genres  but also in using these genres to classify Web pages  The hypothesis of this research is that an n gram representation of a Web page can be used effectively to automatically classify that Web page by genre  even when the Web page belongs to more than one genre  Experiments are run on a multilabeled data set using both an SVM classifier and a distance function classification model  These ngram based methods had very high precision results but somewhat lower recall results  indicating that the genre labels assigned by the classifiers are quite accurate  but that these machine learning classifiers are not assigning as many labels as did the human classifiers  The classification results compare favorably with those of other researchers on the same data set  1  Introduction Web page genre classification is a potentially powerful tool in filtering the results of online searches  As Shepherd and Watters 21 note  the use of genre for classification allows the recognition of items that are similar  even in the midst of great diversity  Categorizing Web pages by genre can allow a user to specify the genre that is of interest with regard to a particular query  For example  depending on the query  the user may be interested in retrieving only FAQ pages or only personal home pages  </raw_string>
  </article>
  <article>
    <title>Degraded Document Image Enhancement</title>
    <count>323</count>
    <raw_string>International Journal For Technological Research In Engineering Volume 1  Issue 11  July2014 ISSN  Online   2347  4718 wwwijtre com Copyright 2014All rights reserved  1407 DEGRADED DOCUMENT IMAGE ENHANCEMENT USING GLOBAL THRESHOLDING Niti Kamboj 1  Vinay Kumar 2 1 department of Computer Science and Engineering Ssgi  Yamunanagar  Kurukshetra University  Haryana  India 2 Deputy Engineer  Department Naval System Bel  Bengaluru  Karnataka  India Abstract  Recent years have witnessed the rapid growth of degraded images due to the increasing power of computing and the fast development of Internet  Because of this tremendous increase of quality of degraded images  there is an urgent need of image content description to facilitate automatic retrieval  Image is described by several low level image features  such as color  texture  shape or the combination of these features Shape is an important low level image feature  In this thesis  enhancement of quality of degraded image using Global Thresholding  This thesis is primarily concerned with the extracting numerical features from binary images  Image processing is very vast field  one of the most important part of image processing is thresholding  Thresholding  which is an important pre  processing steps for the degraded image to enhance their quality  has been studies in relation to various images  There are different algorithms that have been used  studied for various factors of image analysis  The value of thresholding is based on which segmentation has been performed  Keywords  Binarization  Noisy Documents  Threshold I INTRODUCTION Degraded documents are archived and preserved in large quantities worldwide  Electronic scanning is a common approach in handling such documents in a manner which facilitates public </raw_string>
  </article>
  <article>
    <title>Source Code Authorship Attribution</title>
    <count>322</count>
    <raw_string>Deanonymizing Programmers via Code Stylometry Aylin CaliskanIslam Drexel University Richard Harang US Army Research Laboratory Andrew Liu University of Maryland Arvind Narayanan Princeton University Clare Voss US Army Research Laboratory Fabian Yamaguchi University of Goettingen Rachel Greenstadt Drexel University Abstract Source code authorship attribution is a significant pri vacy threat to anonymous code contributors  However  it may also enable attribution of successful attacks from code left behind on an infected system  or aid in resolv  ing copyright  copyleft  and plagiarism issues in the pro gramming fields  In this work  we investigate machine learning methods to deanonymize source code authors of CC using coding style  Our Code Stylometry Fea  ture Set is a novel representation of coding style found in source code that reflects coding style from properties derived from abstract syntax trees  Our random forest and abstract syntax treebased ap  proach attributes more authors  1600 and 250  with sig nificantly higher accuracy  94  and 98   on a larger data set  Google Code Jam  than has been previously achieved  Furthermore  these novel features are robust  difficult to obfuscate  and can be used in other program  ming languages  such as Python  We also find that  i  the code resulting from difficult programming tasks is easier to attribute than easier tasks and  ii  skilled programmers  who can complete the more difficult tasks  are easier to attribute than less skilled programmers  1 Introduction Do programmers leave fingerprints in their source code  That is  does each programmer have a distinctive cod  ing style  Perhaps a programmer has a preference for spaces over tabs  or while loops </raw_string>
  </article>
  <article>
    <title>Source Code Authorship Attribution</title>
    <count>322</count>
    <raw_string>Deanonymizing Programmers via Code Stylometry Aylin CaliskanIslam  Richard Harang  Andrew Liu  Arvind Narayanan  Clare Voss  Fabian Yamaguchi  and Rachel Greenstadt  Drexel University  ac993drexel edu  greeniecs drexel edu ARL  richard e harangcivmail mil  clare rvosscivmail mil UMD  aliu1umd edu Princeton University  arvind ncs princeton edu  University of Goettingen  fabsgoesecde AbstractSource code authorship attribution could provide proof of authorship in court  automate the process of finding a cyber criminal from the source code left in an infected system  or aid in resolving copyright  copyleft and plagiarism issues in the programming fields  In this work  we investigate methods to de anonymize source code authors of C using coding style  We cast source code authorship attribution as a machine learning problem using natural language processing techniques to extract the necessary features  The Code Stylometry Feature Set is a novel representation of coding style found in source code that reflects coding style from properties derived from abstract syntax trees  Such a unique representation of coding style has not been used before in code attribution  Our random forest and abstract syntax treebased approach attributes more authors  250  with significantly higher accuracy  95   on a larger data set  Google Code Jam  than has been previously attempted  Furthermore these novel features are more robust than previous approaches  and are still able to attribute authors even when code is run through commercial obfuscation with no significant change in accuracy  This analysis also pro duces interesting insights relevant to software engineering  We find that  i  the code resulting from difficult programming tasks is easier to attribute than easier tasks and  ii  </raw_string>
  </article>
  <article>
    <title>Web Page Genre Classification</title>
    <count>321</count>
    <raw_string>CLASSIFYING WEB PAGES BY GENRE A Distance Function Approach Jane E Mason  Michael Shepherd and Jack Duffy Faculty of Computer Science  Dalhousie University  Halifax  NS  Canada jmasoncs dalca  shepherdcs dalca  jack duffydalca Keywords  Information retrieval  Web genre classification  Web page genres  Web page representation  ngram analysis  Abstract  The research reported in this paper is part of a larger project on the automatic classification of Web pages by their genres  using a distance function classification model  In this paper  we investigate the effect of several commonly used data preprocessing steps  explore the use of byte and word ngrams  and test our classification model on three Web page data sets  Our approach is to represent each Web page by a profile that is composed of fixedlength ngrams and their normalized frequencies within the document  Similarly  each of the genres in a data set is represented by a profile that is constructed by combining the ngram profiles for each exemplar Web page of that genre  forming a centroid profile for each Web page genre  We use a distance function approach to determine the similarity between two profiles  assigning each Web page the label of the genre profile to which its profile is most similar  Our results compare very favorably to those of other researchers  1 INTRODUCTION The extraordinary growth in both the size and popu larity of the World Wide Web has generated a growing interest in the identification of Web page genres  and in the use of these genres to classify Web pages  Web page genre classification is a potentially powerful tool for filtering the results of online searches  The research reported </raw_string>
  </article>
  <article>
    <title>Source Code Authorship Attribution</title>
    <count>321</count>
    <raw_string>When Coding Style Survives Compilation  Deanonymizing Programmers from Executable Binaries Aylin CaliskanIslam  Fabian Yamaguchi   Edwin Dauber  Richard Harang  Konrad Rieck   Rachel Greenstadt   and Arvind Narayanan Princeton University  aylincprinceton edu  arvind ncs princeton edu  University of Goettingen  fabianyamaguchics unigoettingende  konrad rieckcs unigoettingende Drexel University  egd34drexel edu  greeniecs drexel edu  US Army Research Laboratory  richard e harangcivmail mil AbstractThe ability to identify authors of computer pro grams based on their coding style is a direct threat to the privacy and anonymity of programmers  Previous work has examined at tribution of authors from both source code and compiled binaries  and found that while source code can be attributed with very high accuracy  the attribution of executable binary appears to be much more difficult  Many potentially distinguishing features present in source code  eg  variable names  are removed in the compilation process  and compiler optimization may alter the structure of a program  further obscuring features that are known to be useful in determining authorship  We examine executable binary authorship attribution from the standpoint of machine learning  using a novel set of features that include ones obtained by decompiling the executable binary to source code  We show that many syntactical features present in source code do in fact survive compilation and can be recovered from decompiled executable binary  This allows us to add a powerful set of techniques from the domain of source code authorship attribution to the existing ones used for binaries  resulting in significant improvements to accuracy and scalability  We demonstrate this improvement on data from the Google Code Jam  obtaining attribution accuracy of up to 96  with </raw_string>
  </article>
  <article>
    <title>Web Page Genre Classification</title>
    <count>320</count>
    <raw_string>27 TERM WEIGHTING BASED ON INDEX OF GENRE FOR WEB PAGE GENRE CLASSIFICATION Sugiyanto 1  Nanang Fakhrur Rozi 1  Tesa Eranti Putri 2  Agus Zainal Arifin 2 1 Informatics Department  Institut Teknologi Adhi Tama Surabaya 2 Informatics Department  Institut Teknologi Sepuluh Nopember Surabaya Email  sugiantoitats acid 1  nanangitats acid 1  tesa12mhs ifits acid 2  aguszacs its acid 2 ABSTRACT Automating the identification of the genre of web pages becomes an important area in web pages classification  as it can be used to improve the quality of the web search result and to reduce search time  To index the terms used in classification  generally the selected type of weighting is the documentbased TFIDF However  this method does not consider genre  whereas web page documents have a type of categorization called genre  With the existence of genre  the term appearing often in a genre should be more significant in document indexing compared to the term appearing frequently in many genres despites its high TFIDF value  We proposed a new weighting method for web page documents indexing called inverse genre frequency  IGF   This method is based on genre  a manual categorization done semantically from previous research  Experimental results show that the term weighting based on index of genre  TFIGF  performed better compared to term weighting based on index of document  TFIDF  with the highest value of accuracy  precision  recall  and Fmeasure in case of excluding the genrespecific keywords were 78   802   78   and 774  respectively  and in case of including the genrespecific keywords were 789   787   789   and 781  </raw_string>
  </article>
  <article>
    <title>Genre Oriented Summarization</title>
    <count>310</count>
    <raw_string> i  Genre Oriented Summarization Jade Goldstein Stewart December 2008 CMULTI09001 Language Technologies Institute School of Computer Science Carnegie Mellon University 5000 Forbes Ave   Pittsburgh  PA 15213 http wwwltics cmuedu Thesis Committee  Jaime Carbonell  Chair Jamie Callan Vibhu Mittal  Google John Conroy  IDACenter for Computing Sciences Submitted in partial fulfillment of the requirements for the degree Doctor of Philosophy Copyright  2009 Jade Goldstein Stewart  i  To Charles ii iii Acknowledgements There are a great many people who have my sincere gratitude for the assistance that they gave me during the course of my work on my dissertation  I would especially like to thank my advisor Jaime Carbonell for his support and encouragement over the years I am also grateful to the other members of my committee  Jamie Callan  Vibhu Mittal and John Conroy for their insightful comments  discussions and advice  Many thanks also go to the late Barbara Lazarus  who provided much encouragement during this effort  which was always deeply appreciated  My thanks also goes to my colleagues  friends and family members  who have provided much support and assistance during the process  especially  but not limited to  Roberta Sabin  Maria AlvarezRyan  Barb Wheatley  Mark Kantrowitz  Tina Kohler  Kathy Baker  Lisa Harper  Jon Nedel and Chad Langley  Thanks also to the numerous people  not listed here  who assisted with the annotation of the multiple data sets and judgments of summary quality  I owe a great deal to my husband Charles  who has supported and assisted greatly in this endeavor  Thanks also to my parents who fostered an atmosphere of experimentation and exploration in my </raw_string>
  </article>
  <article>
    <title>Detecting Stylistic Deception</title>
    <count>290</count>
    <raw_string>Proceedings of the EACL 2012 Workshop on Computational Approaches to Deception Detection  pages 9196  Avignon  France  April 23  27 2012  c2012 Association for Computational Linguistics Detecting Stylistic Deception Patrick Juola Evaluating Variation in Language Laboratory Duquesne University Pittsburgh  PA 15282 USA juolamathcs duqedu Abstract Whistleblowers and activists need the abil ity to communicate without disclosing their identity  as of course do kidnappers and ter rorists  Recent advances in the technol ogy of stylometry  the study of authorial style  or authorship attribution  have made it possible to identify the author with high reliability in a nonconfrontational setting  In a confrontational setting  where the au  thor is deliberately masking their identity  i e  attempting to deceive   the results are much less promising  In this paper  we show that although the specific author may not be identifiable  the intent to deceive and to hide his identity can be  We show this by a reanalysis of the Brennan and Green stadt  2009  deception corpus and discuss some of the implications of this surprising finding  1 Introduction Deception can occur in many different ways  it is possible to deceive not only about the content of a message  but about its background or origin  For example  a friendly invitation can become sexual harassment when sent from the wrong person  and very few ransom notes are signed by their authors  Recent research into stylometry has shown that it is practical to identify authors based on their writing style  but it is equally practical  at present technology  for authors to use a deliberately de ceptive style  either obfuscating their own style or </raw_string>
  </article>
  <article>
    <title>Statistics in Musicology</title>
    <count>288</count>
    <raw_string>CHAPMAN  HALLCRC A CRC Press Company Boca Raton London New York Washington  DC I n t e r d i s c i p l i n a r y S t a t i s t i c s STATISTICS in MUSICOLOGY Jan Beran 2004 CRC Press LLC This book contains information obtained from authentic and highly regarded sources  Reprinted material is quoted with permission  and sources are indicated  A wide variety of references are listed  Reasonable efforts have been made to publish reliable data and information  but the author and the publisher cannot assume responsibility for the validity of all materials or for the consequences of their use  Neither this book nor any part may be reproduced or transmitted in any form or by any means  electronic or mechanical  including photocopying  microfilming  and recording  or by any information storage or retrieval system  without prior permission in writing from the publisher  The consent of CRC Press LLC does not extend to copying for general distribution  for promotion  for creating new works  or for resale  Specific permission must be obtained in writing from CRC Press LLC for such copying  Direct all inquiries to CRC Press LLC  2000 NW Corporate Blvd  Boca Raton  Florida 33431  Trademark Notice  Product or corporate names may be trademarks or registered trademarks  and are used only for identification and explanation  without intent to infringe  Visit the CRC Press Web site at wwwcrcpresscom  2004 by Chapman  HallCRC No claim to original US Government works International Standard Book Number 1584882190 Library of Congress Card Number 2003048488 Printed in the United States of America 1 2 3 4 5 </raw_string>
  </article>
  <article>
    <title>Intrinsic Plagiarism Analysis</title>
    <count>253</count>
    <raw_string>Intrinsic plagiarism analysis Benno Stein  Nedim Lipka  Peter Prettenhofer Published online  20 January 2010 Springer ScienceBusiness Media BV 2010 Abstract Research in automatic text plagiarism detection focuses on algorithms that compare suspicious documents against a collection of reference documents  Recent approaches perform well in identifying copied or modified foreign sections  but they assume a closed world where a reference collection is given  This article investigates the question whether plagiarism can be detected by a computer program if no reference can be provided  eg  if the foreign sections stem from a book that is not available in digital form  We call this problem class intrinsic plagiarism analysis  it is closely related to the problem of authorship verification  Our con tributions are threefold   1  We organize the algorithmic building blocks for intrinsic plagiarism analysis and authorship verification and survey the state of the art   2  We show how the meta learning approach of Koppel and Schler  termed unmasking   can be employed to postprocess unreliable stylometric analysis results   3  We operationalize and evaluate an analysis chain that combines docu ment chunking  style model computation  oneclass classification  and meta learning  Keywords Plagiarism detection Authorship verification Stylometry Oneclass classification B Stein    N Lipka P Prettenhofer Faculty of Media  Media Systems  BauhausUniversitat Weimar  99421 Weimar  Germany email  benno steinuniweimarde N Lipka email  nedim lipkauniweimarde P Prettenhofer email  peterprettenhoferuniweimarde 123 Lang Resources  Evaluation  2011  45 6382 DOI 101007s105790109115y 1 Problem statement In the following  the term plagiarism refers to text plagiarism  i e   the use of another authors information  language  or </raw_string>
  </article>
  <article>
    <title>Information Flow Investigations</title>
    <count>252</count>
    <raw_string>Information Flow Investigations Michael Carl Tschantza Anupam Datta Jeannette M Wingb June 26  2013 CMUCS13118 aNow at University of California  Berkeley bNow at Microsoft Research School of Computer Science Carnegie Mellon University Pittsburgh  PA 15213 This research was supported by the US Army Research Office grants DAAD190210389 and W911NF091 0273 to CyLab  by the National Science Foundation  NSF  grants CCF0424422 and CNS1064688  and by the US Department of Health and Human Services grant HHS 90TR000301  The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies  either expressed or implied  of any sponsoring institution  the US government or any other entity  Keywords  Privacy  Formal Methods  Black Box Analysis Abstract Informationflow analysis has largely ignored the setting where the analyst has neither control over nor a complete model of the analyzed system  We formalize such limited information flow investigations and study it in three domains   1  the determination of what thirdparty web trackers do with the information they collect   2  the identification of copyright violators  and  3  the detection of insiders leaking data  We use a common framework based on information flow analysis to relate these three problems while pushing beyond traditional information flow analysis  Viewing these seemingly disparate problems in a common framework allows us to identify the assumptions underlying analyses used for these problems and to show where each area could benefit from the other  Our systematic study leads to practical advice for improving work on tracking web trackers  a previously unformalized area  1 Introduction Tracking Web Trackers  Concerns about privacy have led to much interest </raw_string>
  </article>
  <article>
    <title>Multimodal Musician Recognition</title>
    <count>248</count>
    <raw_string>MULTIMODAL MUSICIAN RECOGNITION Jordan Hochenbaum1 New Zealand School of Music1 PO Box 2332 Wellington  New Zealand 64  4  463 5369 hochenjordmyvuwacnz Ajay Kapur12 California Institute of the Arts2 24700 McBean Parkway Valencia  California  91355 1  661  952 3191 akapurcalarts edu Matthew Wright Univ California  Santa Barbara CREATEMAT  2213 Elings Hall Santa Barbara  California  93106 1  805  893 8352 mattcreate ucsbedu ABSTRACT This research is an initial effort in showing how a multimodal approach can improve systems for gaining insight into a musicians practice and technique  Embedding a variety of sensors inside musical instruments and synchronously recording the sensors  data along with audio  we gather a database of gestural information from multiple performers  then use machinelearning techniques to recognize which musician is performing  Our multimodal approach using both audio and sensor data  yields promising performer classification results  which we see as a first step in a larger effort to gain insight into musicians  practice and technique  Keywords  Performer Recognition  Multimodal  HCI  Machine Learning  Hyperinstrument  eSitar 1  INTRODUCTION We imagine a new multimodal language between musician and machine in which the computer receives multiple channels of information from the performer and interprets these data to derive information allowing for more effective communication back to the musician  It is important for the computer first to understand who is the performer  in order to tailor a specific and meaningful interaction  Moreover  we suggest that our musician recognition framework establishes a foundational multimodal language that can be the basis for future novel interactive and educational experiences between musicians and computers  Two main approaches to performer recognition currently exist  The first </raw_string>
  </article>
  <article>
    <title>Clasificacion Mediante Conjuntos</title>
    <count>245</count>
    <raw_string>Universidad Autonoma de Madrid Escuela Politecnica Superior Departamento de Ingeniera Informatica CLASIFICACION MEDIANTE CONJUNTOS TESIS DOCTORAL FEBRERO 2006 GONZALO MARTINEZ MUNOZ DIRECTOR  ALBERTO SUAREZ GONZALEZ A Lucia  Pietro y Nora Agradecimientos Agradezco muy sinceramente a mi Director de Tesis  D Alberto Suarez Gonzalez por su disponibilidad y apoyo durante todo el desarrollo de esta tesis  Sus sugerencias para orientar el trabajo de investigacion y su lectura rigurosa de esta memoria han sido muy valiosas  Agradezco a Pilar Rodrguez por sus consejos y por animarme a hacer la tesis en el Departamento  Muchas gracias a Eduardo Perez  lector designado por el Departamento  por su lectura minuciosa que ha contribuido a mejorar este documento  Quiero agradecer a Francisco Rodrguez por su disponibilidad y por permitirme utilizar tiempo de CPU para realizar parte de los experimentos contenidos en esta tesis  Muchas gracias a Alejandro Sierra por facilitarme codigo fuente que he utilizado en algunos expe rimentos  Gracias tambien a Luis Fernando Lago que me ayudo  tecnica y moralmente con el arranque de esta memoria de tesis  Agradezco a Jordi  mi companero de despacho  por su buena compana  por ponerme al da sobre los clasicos de la informatica  as  como por su apoyo con LATEX en la recta final de la tesis  Un agradecimiento a Raul  con quien he compartido tantos anos en la Autonoma  en los barracones del colegio Prncipe de Asturias antes  luego en la Facultad de Fsicas y ahora como profesores de esta universidad  Gracias a Antonio  que tambien ha compartido conmigo los anos de la Facultad y mu chos mas  Mis agradecimientos a la gente del office  y en particular a Ana  Estrella </raw_string>
  </article>
  <article>
    <title>Web Page Classification</title>
    <count>243</count>
    <raw_string>5 Web Page Classification Ben Choi and Zhongmei Yao Computer Science  College of Engineering and Science  Louisiana Tech University  Ruston  LA 71272  USA proBenChoi org  zya001latech edu ABSTRACT This chapter describes systems that automatically classify web pages into meaningful categories  It first defines two types of web page classification  subject based and genre based classifications  It then describes the state of the art techniques and subsystems used to build automatic web page classification systems  including web page representations  dimensionality reductions  web page classifiers  and evaluation of web page classifiers  Such systems are essential tools for Web Mining and for the future of Semantic Web 51 INTRODUCTION 511 Motivation Over the past decade we have witnessed an explosive growth on the Internet  with millions of web pages on every topic easily accessible through the Web The Internet is a powerful medium for communication between computers and for accessing online documents all over the world but it is not a tool for locating or organizing the mass of information  Tools like search engines assist users in locating information on the Internet  They perform excellently in locating but provide limited ability in organizing the web pages  Internet users are now confronted with thousands of web pages returned by a search engine using simple keyword search  Searching through those web pages is in itself becoming  This research was supported in part by a grant from the Center for Entrepreneurship and Information Technology  CEnIT   Louisiana Tech University  2 Ben Choi and Zhongmei Yao impossible for users  Thus it has been of more interest in tools that can help make a relevant and quick selection of information that we are seeking </raw_string>
  </article>
  <article>
    <title>Web Page Classification</title>
    <count>242</count>
    <raw_string>Classifying Web Pages by Genre  An ngram Based Approach Jane E Mason  Michael Shepherd  and Jack Duffy Faculty of Computer Science Dalhousie University Halifax  NS  Canada jmasoncs dalca  shepherdcs dalca  jack duffydalca Abstract The research reported in this paper is part of a larger project on the classification of Web pages by genre  Such classification is a potentially powerful tool in filtering the results of online searches  In this paper  we describe two sets of experiments investigating the automatic classification of Web pages by their genres  In these experiments  our approach is to represent the Web pages by profiles that are composed of fixedlength byte ngrams  The first set of experiments in this study examines the effect of three feature selection measures on the accuracy of Web page classification  The second set of experiments in this study compares the classification accuracy of three clas  sification methods  each using ngram representations of the Web pages  The classification methods which are compared are a distance function approach  the knearest neighbors method  and the support vector machine approach  We also examine a range of ngram lengths and a range of Web page profile sizes to determine what combination s  of ngram length and profile size give the best classification accuracy  Each set of experiments is run on two wellknown data sets  7Genre and KI04  for which published results are available  1  Introduction The research reported in this paper focuses on the au  tomatic classification of Web pages by their genres  using ngram representations of the Web pages  Categorizing Web pages by genre can allow a user to specify the genre that is of interest </raw_string>
  </article>
  <article>
    <title>Web Page Classification</title>
    <count>242</count>
    <raw_string>IJIRST International Journal for Innovative Research in Science TechnologyVolume 2 Issue 04 September 2015ISSN online 23496010All rights reserved by wwwijirst org 42Visual Webpage Content Segmentation andRetrieval based on nGrams Kumud Jaglan DrKulvinder SinghUIETKurukshetra UniversityKurukshetraIndia UIETKurukshetra UniversityKurukshetraIndiaVipul JaglanNIFTEMKundliAbstractWeb documents are often viewed as complicated objects which frequently contain multiple entities every of which may representa separate unitThoughmost processing requests applications for the web and web content because of the smallest indivisiblecomponents and knowledge Extraction from Web Pages has continually trusted comprehensive human involvement within thesort of hand crafted extraction algorithms or scripts using usual expressionsPreceding works usually flout the underlyingcontent segments that are composed of unimportant knowledge like net ads and knowledge moot to the usersThis paper resolvethese subjectswe tend to endorsed ngram established website segmentation algorithmic program that used the density forsegmenting the webpage lacking hoping on the DOM tree for the segmentation method KeywordsWeb page classificationsegmentationVision based page segmentationWeb information extraction                                                                                                     IINTRODUCTIONWeb documents can be viewed as complex objects which often contain multiple entities each of which can represent astandalone unitHowevermost information processing applications developed for the webconsider web pages as the smallestundividable unitsThis fact is best illustrated by web information retrieval engines whose results are presented in the </raw_string>
  </article>
  <article>
    <title>Detecting Stylistic Deception</title>
    <count>242</count>
    <raw_string>Detecting Hoaxes  Frauds  and Deception in Writing Style Online Sadia Afroz  Michael Brennan and Rachel Greenstadt  Department of Computer Science Drexel University  Philadelphia  PA 19104 Emails  sadia afrozdrexel edu  mb553drexel edu and greeniecs drexel edu Abstract In digital forensics  questions often arise about the authors of documents  their identity  de mographic background  and whether they can be linked to other documents  The field of stylometry uses linguistic features and machine learning tech  niques to answer these questions  While stylometry techniques can identify authors with high accuracy in nonadversarial scenarios  their accuracy is reduced to random guessing when faced with authors who intentionally obfuscate their writing style or attempt to imitate that of another author  While these results are good for privacy  they raise concerns about fraud  We argue that some linguistic features change when people hide their writing style and by identifying those features  stylistic deception can be recognized  The major contribution of this work is a method for detecting stylistic deception in written documents  We show that using a large feature set  it is possible to distinguish regular documents from deceptive doc  uments with 966  accuracy Fmeasure   We also present an analysis of linguistic features that can be modified to hide writing style  Keywordsstylometry  deception  machine learn  ing  privacy  I INTRODUCTION When an American male blogger Thomas Mac  Master posed as a Syrian homosexual woman Amina Arraf in the blog A Gay Girl in Damas  cus  and wrote about Syrian political and social issues  several news media including The Guardian and CNN thought the blog was brutally honest   and published </raw_string>
  </article>
  <article>
    <title>Web Page Classification</title>
    <count>242</count>
    <raw_string>Syllable ngram approach for Identification and Classification of genres in Telugu language K Pranitha Kumari Department of CSE  OUCE  Osmania University Hyderabad  India  pranithakundarapugmailcom Prof AVenugopal Reddy Department of CSE  OUCE  Osmania University Hyderabad  India  avgreddy55gmailcom AbstractThe use of internet in India is increasing day by day and availability of information in Indian languages on the web is also increasing  So there is a need to classify the web data to improve the search results  Research is going on topicbased text classification but  the genre  nontopical  based web page classification for Telugu web pages is so far not considered  This work attempts to identify the web genres in Telugu language  In this paper  three web genres were identified from the Telugu language web pages based on the social acceptance and communicative purpose i e  discourse functionality  Syllable extraction algorithm to extract character n gram features is proposed  The classification was performed using SVM  Naive Bayes and Random forest classifiers  The classification results obtained show that the proposed algorithm gave better performance in terms of Fmeasure and accuracy  Keywords  Telugu web genres  genre identification  genre classification  syllable extraction  character n gram features  I INTRODUCTION As a result of fast growth of World Wide Web  web data is exponentially increasing  Nowadays  most of the search engines are topic based search engines  Lot of research has been done and techniques have been proposed to improve the quality of search results for topic based search engines  Eg  Retrieval query about a certain topic such as Microsoft  gets many documents  These documents belong to different genres such as company </raw_string>
  </article>
  <article>
    <title>Web Page Classification</title>
    <count>242</count>
    <raw_string>wwwijecs in International Journal Of Engineering And Computer Science ISSN23197242 Volume 4 Issue 6 June 2015  Page No 1297012973 Kumud Jaglan1 IJECS Volume 4 Issue 6 June  2015 Page No1297012973 Page 12970 Vision Assisted Web structure and Data Extraction Methods Kumud Jaglan1  Dr Kulvinder Singh2 12University Institute of Engineering  Technology  Kurukshetra University  Kurukshetra  India 1kumudjaglanyahoocom  2kshandarediffmailcom Abstract  Web page segmentation has been done to address the problems in different fields including mobile web  archiving  phishing  etc  In this paper  different algorithms are summarized that web page segmentation addresses in different fields Web page segmentation has myriad applications like information retrieval  page type classification etc   This paper presents a survey of web page segmentation algorithms including DOM Tree  VIPS and SD Tree algorithms  VIPS approach is independent of underlying HTML representation and works well even when layout structure is different from the HTML structure  As there is difficulty in finding the meaningful blocks existing approaches presented can extract informative parts from web pages by creating meaningful blocks and segmenting noisy WebPages  Keywords  Web page classification  Web page segmentation  Web Page  Web information extraction 1  Introduction Web documents can be viewed as complex objects which often contain multiple entities each of which can represent a standalone unit  However  most information processing applications developed for the web  consider web pages as the smallest undividable units  This fact is best illustrated by web information retrieval engines whose results are presented in the form of links to web documents rather than to the exact regions within these documents that directly match a user s query  A better retrieval performance can be achieved by considering </raw_string>
  </article>
  <article>
    <title>Web Page Classification</title>
    <count>241</count>
    <raw_string>Ryan Levering  Michal Cutler CostSensitive Feature Extraction and Selection in Genre Classifica  tion Automatic genre classification of Web pages is currently young compared to other Web classification tasks  Corpora are just starting to be collected and organized in a systematic way  feature extraction techniques are incon  sistent and not well detailed  genres are constantly in dispute  and novel applications have not been implemented  This paper attempts to review and make progress in the area of feature extraction  an area that we believe can benefit all Web page classification  and genre classification in particular  We first present a framework for the extraction of various Webspecific feature groups from distinct data models based on a tree of potentials models and the transformations that create them  Then we introduce the concept of costsensitivity to this tree and provide an algorithm for per forming wrapperbased feature selection on this tree  Finally  we apply the costsensitive feature selection algorithm on two genre corpora and analyze the performance of the classification results  1 Introduction A classification task cannot achieve high performance without being provided a good set of features  regardless of the algorithm that is being used to train the computer  For instance  if you were attempting to train a computer to recognize dogs from other animals and the only measurement you took was the number of legs the animal had  it would be an impossible task  This paper is primarily concerned with the goal of obtaining thorough measurements  known as features  from a particular automatic classification domain  Web pages  The features that we are discussing would most likely be useful for any number of classification tasks within the domain of Web </raw_string>
  </article>
  <article>
    <title>Web Page Classification</title>
    <count>241</count>
    <raw_string>Abstract In this paper we present an automatic genre  based web page classification system for determining whether a web page contains an advertisement or not  Due to the difficulties and subjectivities in defining a genre  its features and their categorization  genre based classification is still rudimentary  In this research  we identified key features and used those features to define the advertisement category  We then developed a genrebased classification system to automatically classify a web page into advertisement category  which is important for commerce and for web users who prefer to either view or skip advertisements  We implemented and tested the proposed system  which achieved an average accuracy of 82   Furthermore  we incorporated this system with other genre and subject based system to create a comprehensive web page classification system  Index Terms Information retrieval  knowledge classification  semantic web  web mining  web ontology  I INTRODUCTION Over the past decades  there has been a tremendous growth in the internet and the World Wide Web The web users have increased exponentially resulting in the extreme commercialization of the web Presently  online advertising has been a major source of revenue for businesses  With the growth of advertisements on the web  came the stage of it s over exploitation causing various problems to the users  Thus  classification of web pages into advertisement plays an important role  both  for users who are interested to view them  as well as for users who want to skip them  In this paper  we focus on classifying web pages into advertisement category  Thus  we created a genrebased classification system to automatically classify web pages into advertisement category  In </raw_string>
  </article>
  <article>
    <title>Web Page Classification</title>
    <count>240</count>
    <raw_string>International Journal of Computer Science and Telecommunications Volume 6  Issue 7  July 2015  8 Journal Homepage  wwwijcst org Souksan Vilavong 1 and Khanh Phan Huy 2 1 Champasak University  Ban Chat Sanh  Pakse  Laos 2 Danang University of Technology  The University of Danang  Danang  Vietnam 1 ssucheduyahoocom  2 khanhph29gmailcom AbstractText categorization is one of the most important role in many applications in natural language processing  NLP  The task of text classification is assignment of free text document to one or more predefined categories based on their content  Whereas a wide range of methods have been applied to English text classification  relatively very few studies have been done on Lao text  In this paper  we present methodology for Lao document presentation and two of the best machine learning techniques  which have namely Radial Basis Function  RBF  network and support vector machines  SVM  to classify the documents  Experimental results revealed that these approaches could achieve an average about 82  accuracy  Additionally  we also analyze the advantages and disadvantages of each approach to find out the best method in specific circumstances  Index Terms  Machine Learning  Comparison  Natural Language Processing  NLP  and Lao Text Categorization I INTRODUCTION EXT categorization has been one of the most popular problem in natural language processing  It is based module in many applications and most of categorization systems classify documents into one or more given predefined categories  It has been utilized in many application areas such as  spam filtering 25  language identification 18  genre classification 27  customer relationship management 7  web page classification 23  text sentinel classification 31 and </raw_string>
  </article>
  <article>
    <title>Authorship Analysis</title>
    <count>204</count>
    <raw_string>1 Evaluating Text Visualization for Authorship Analysis Victor Benjamin1  Wingyan Chung2  Ahmed Abbasi3  Joshua Chuang4  Catherine A Larson1  Hsinchun Chen1 vabenjiemail arizona edu  contact author   wchungstetson edu  abbasicommvirginia edu  joshuachuanggmailcom  calellerarizona edu  hchenellerarizona edu 1MIS Dept  University of Arizona  Tucson  AZ USA  2Dept  of Decision and Info  Sciences  Stetson University  DeLand  FL USA  3McIntire School of Commerce  University of Virginia  Charlottesville  VA USA  4HC Analytics  Tucson  AZ USA Abstract Methods and tools to conduct authorship analysis of web contents is of growing interest to researchers and practitioners in various securityfocused disciplines  including cybersecurity  counterterrorism  and other fields in which authorship of text may at times be uncertain or obfuscated  Here we demonstrate an automated approach for authorship analysis of web contents  Analysis is conducted through the use of machine learning methodologies  an expansive stylometric feature set  and a series of visualizations intended to help facilitate authorship analysis at the author  message  and feature levels  To operationalize this  we utilize a testbed containing 506554 forum messages in English and Arabic  source from 14901 authors that participated in an online web forum  A prototype portal system providing authorship comparisons and visualizations was then designed and constructed in order to support feasibility analysis and real world value of the automated authorship analysis approach  A preliminary user evaluation was performed to assess the efficacy of visualizations  with evaluation results demonstrating task performance accuracy and efficiency was improved through use of the portal  Keywordsterrorism  text visualization  online forum  authorship analysis  cybercrime  INTRODUCTION Authorship analysis is useful </raw_string>
  </article>
  <article>
    <title>Internet Genres</title>
    <count>203</count>
    <raw_string>Internet genres 1 INTERNET GENRES Kevin Crowston Syracuse University School of Information Studies 348 Hinds Hall Syracuse  NY 13244 crowstonsyredu Tel  1  315  4431676 Fax  1 866  2657407 This research was partially supported by NSF IIS Grant 0414482  Internet genres 2 INTERNET GENRES ABSTRACT Rhetoricians since Aristotle have attempted to classify communications or documents into categories or genres  with similar form  topic or purpose  This article surveys research on genre as it relates to Internet documents  The article briefly presents the concept of genre in general  and then reviews the evolution and emergence of genres on the Internet  It concludes with an examination of the possible use of genre for improving information access on the Internet  with specific discussion of the issues in developing taxonomies of genre and automatically recognizing document genre  Keywords  Genre  Worldwide web  Internet  Digital documents  Information access Word count  6908 words plus abstract  tables and references Internet genres 3 INTERNET GENRES INTRODUCTION Rhetoricians since Aristotle have attempted to classify documents into categories or genres  with similar form  topic or purpose   In this article  we adopt a broad definition of document as signifying objects  1   meaning something that serves as evidence  regardless of the particular medium or form   Numerous definitions of genre have been debated in the applied linguistics community  e g  2  37   while other groups have struggled with similar notions  such as discourse or document types  eg  in SGML  8   This article  for example  is an instance of the encyclopedia article genre  commonly used to communicate the state </raw_string>
  </article>
  <article>
    <title>Authorship Analysis</title>
    <count>202</count>
    <raw_string>CC Yang et al   Eds    PAISI 2007  LNCS 4430  pp  1  20  2007   SpringerVerlag Berlin Heidelberg 2007 Exploring Extremism and Terrorism on the Web  The Dark Web Project Hsinchun Chen McClelland Professor of Management Information Systems Director  Artificial Intelligence Lab Director  NSF COPLINK Center Management Information Systems Department Eller College of Management  University of Arizona  USA Abstract  In this paper we discuss technical issues regarding intelligence and security informatics  ISI research to accomplish the critical missions of international security and counterterrorism  We propose a research framework addressing the technical challenges facing counterterrorism and crimefighting applications with a primary focus on the knowledge discovery from databases  KDD  perspective  We also present several Dark Web related case studies for opensource terrorism information collection  analysis  and visualization  Using a web spidering approach  we have developed a largescale  longitudinal collection of extremistgenerated Internetbased multimedia and multilingual contents  We have also developed selected computational link analysis  content analysis  and authorship analysis techniques to analyze the Dark Web collection  Keywords  intelligence and security informatics  terrorism informatics  dark web 1 Introduction The tragic events of September 11 and the following anthrax contamination of letters caused drastic effects on many aspects of society  Terrorism has become the most significant threat to national security because of its potential to bring massive damage to our infrastructure  economy  and people  In response to this challenge federal authorities are actively implementing comprehensive strategies and measures in order to achieve the three objectives identified in the National Strategy for Homeland Security  report  Office of Homeland Security  2002    1 </raw_string>
  </article>
</articles>
