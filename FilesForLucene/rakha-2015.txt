Empir Software Eng
DOI 10.1007/s10664-015-9404-6
Studying the needed effort for identifying duplicate issues
Mohamed Sami Rakha1 ·Weiyi Shang1 ·
Ahmed E. Hassan1
© Springer Science+Business Media New York 2015
Abstract Many recent software engineering papers have examined duplicate issue reports.
Thus far, duplicate reports have been considered a hindrance to developers and a drain
on their resources. As a result, prior research in this area focuses on proposing automated
approaches to accurately identify duplicate reports. However, there exists no studies that
attempt to quantify the actual effort that is spent on identifying duplicate issue reports.
In this paper, we empirically examine the effort that is needed for manually identify-
ing duplicate reports in four open source projects, i.e., Firefox, SeaMonkey, Bugzilla and
Eclipse-Platform. Our results show that: (i) More than 50 % of the duplicate reports are
identified within half a day. Most of the duplicate reports are identified without any discus-
sion and with the involvement of very few people; (ii) A classification model built using a
set of factors that are extracted from duplicate issue reports classifies duplicates according
to the effort that is needed to identify them with a precision of 0.60 to 0.77, a recall of 0.23
to 0.96, and an ROC area of 0.68 to 0.80; and (iii) Factors that capture the developer aware-
ness of the duplicate issue’s peers (i.e., other duplicates of that issue) and textual similarity
of a new report to prior reports are the most influential factors in our models. Our findings
highlight the need for effort-aware evaluation of approaches that identify duplicate issue
reports, since the identification of a considerable amount of duplicate reports (over 50 %)
appear to be a relatively trivial task for developers. To better assist developers, research on
Communicated by: Emerson Murphy-Hill
 Mohamed Sami Rakha
rakha@cs.queensu.ca
Weiyi Shang
swy@cs.queensu.ca
Ahmed E. Hassan
ahmed@cs.queensu.ca
1 Software Analysis and Intelligence Lab (SAIL), School of Computing,
Queen’s University, Kingston, Ontario, Canada
Empir Software Eng
identifying duplicate issue reports should put greater emphasis on assisting developers in
identifying effort-consuming duplicate issues.
Keywords Mining software repositories · Automated detection of duplicate issues ·
Software issue reports · Effort based analysis · Duplicate bug reports
1 Introduction
Issue tracking systems are widely used in practice to track the requests and concerns of
users. Such systems provide a communication platform to help developers get information
from users for the rapid resolution of issues (Bertram et al. 2010). These systems also allow
users to track the status of their submitted issue reports.
Newly submitted issue reports usually go through a filtration process before they are
assigned to developers. One of the common filtering steps is to identify whether a newly
reported issue is a duplicate of an existing issue. Two issue reports are considered duplicates
of each other, if they refer to a similar software problem or propose a similar feature. Due
to the limited resources of software projects, duplicate issue reports are often considered a
waste of developers’ effort. Studies show that up to 20–30 % of the issue reports are dupli-
cates (Bettenburg et al. 2008b; Jalbert and Weimer 2008). Such large amounts of duplicate
issue reports have motivated extensive studies on the automated identification of duplicate
issue reports (Anvik et al. 2005; Runeson et al. 2007; Wang et al. 2008; Jalbert and Weimer
2008; Nagwani and Singh 2009; Sun et al. 2011; Alipour et al. 2013). Various advanced
approaches, such as Natural Language Processing (NLP), are commonly used to accurately
identify whether a new issue report is a duplicate of previously reported issues (Runeson
et al. 2007).
While prior research aims to reduce the needed efforts for identifying duplicate issue
reports, there exist no studies that examine the actual effort that is spent on manually iden-
tifying the duplicate issues in practice. In fact, the needed effort for identifying duplicate
issue reports may vary considerably. For example, a Mozilla developer needed less than
15 minutes to identify that issue #3127821 is a duplicate of a previously reported issue,
while the duplicate identification of issue #653052 required around 44 days, and involved
20 comments from 11 people.
To better understand the effort that is involved in identifying duplicate issue reports in
practice, this paper studies duplicate reports from four open source projects (Firefox, Sea-
Monkey, Bugzilla and Eclipse-Platform). In particular, we explore the following research
questions:
RQ1: How much effort is needed to identify a duplicate issue report?
Half of the duplicate reports are identified in less than half a day. 50 - 60 % of the
duplicates are identified without any discussion.
RQ2: How well can we model the needed effort for identifying duplicate issue
reports?
Random forest classifiers trained using factors derived from duplicate issue reports
achieve a precision of 0.60 to 0.77, and a recall of 0.23 to 0.96, along with an ROC
area of 0.68 to 0.80.
1https://bugzilla.mozilla.org/show bug.cgi?id=312782
2https://bugzilla.mozilla.org/show bug.cgi?id=65305
Empir Software Eng
RQ3: What are the most influential factors on the effort that is needed for identify-
ing duplicate issue reports?
The textual similarity between a new report and previously reported issues plays
an important role in explaining the effort that is needed for identifying duplicate
reports. Another influential factor is the team’s awareness of previously reported
issues. Such awareness is measured using: a) The experience of a developer in
identifying prior duplicates, b) The recency of the duplicate report relative to the
previously filed reports of that issue, and c) The number of people that are involved
in the previously filed reports of that issue.
Our findings show that developers are able to identify a large portion (over 50 %) of dupli-
cate reports with minimal effort. On the other hand, there exist duplicate reports that are
considerably more difficult to identify. These findings highlight the need for effort-aware
evaluation of automated approaches for duplicate identification. To better assist develop-
ers, research on identifying duplicate issue reports should put greater emphasis on assisting
developers in identifying effort-consuming duplicate issues.
The rest of the paper is organized as follows: Section 2 presents related work on the
management of duplicate issue reports. Section 3 describes the studied projects. Section 4
provides an overview of our experimental setup. Section 5 motivates our research questions,
discusses the approaches that we used to answer our questions and presents the results of
our questions. Finally, Section 6 outlines some of the threats to the validity of our study and
Section 7 concludes the paper.
2 Background and Related Work
In this section we present prior research that relates to our study.
2.1 Empirical Studies on Duplicate Issue Reports
Duplicate issue reports represent a large portion of issue reports. Anvik et al. (2005)
report that 20–30 % of the issue reports in Eclipse and Firefox respectively are duplicates.
Cavalcanti et al. (2013) found that duplicate reports represent 32 %, 43 %, and 8 % of all
the reports in Epiphany, Evolution and Tomcat, respectively.
Duplicate reports are not always harmful. Bettenburg et al. (2008b) found that merging the
information across duplicate reports produces additional useful information over using the infor-
mation from a single report. In particular, Bettenburg et al. found that such additional informa-
tion improves the accuracy of automated approaches for issue triaging3 by up to 65 %.
Several prior studies have explored the effort associated with duplicate reports. Davidson
et al. (2011) and Cavalcanti et al. (2010), Cavalcanti et al. (2013) examined the effort that
is associated with closing a duplicate report: Davidson et al. looked at the time needed for
closing a duplicate report after it was triaged, and Cavalcanti et al. examined the total time
that is needed to close a duplicate report. Cavalcanti et al. also investigated the time spent
by report submitters to check whether they are about to file an issue that is a duplicate of
previously-reported issues. In our case, we consider the needed effort for identifying the
3Issue triaging is the task of determining if an issue report describes a meaningful new problem or
enhancement, so it can be assigned to an appropriate developer for further handling (Anvik et al. 2006).
Empir Software Eng
duplicate report not the time to close it (since up to 50 % of the studied duplicate issue
reports are not closed).
2.2 Automated Identification of Duplicate Issue Reports
Duplicate issue report identification is the task of matching a newly reported issue with
one or more previously reported ones. Prior research has proposed various duplicate identi-
fication approaches that leverage textual and non-textual information that are derived from
issue reports.
Leveraging Textual Information Anvik et al. (2005) proposed an approach that uses
cosine similarity to classify newly reported issues as either new reports or duplicate ones.
Their approach can correctly identify 28 % of the duplicate issue reports in the Firefox 1.0
project. Runeson et al. (2007) proposed an approach to identify duplicate reports based
on Natural Language Processing (NLP) and evaluated the approach on issue reports from
Sony Ericsson. The prototype tool correctly classified 40 % of the duplicate issue reports.
Nagwani and Singh (2009) used string similarity measures, such as TD-IDF similarity,
and text semantics measures, such as edge counting methods, to identify duplicate issue
reports. Prifti et al. (2011) proposed an approach based on information retrieval, while lim-
iting the search for duplicates to the most recently filed reports. Prifti et al.’s approach is
able to identify around 53 % of the duplicate issue reports in their case study. Sun et al.
(2010) built a discriminative model to determine if two issue reports are duplicates. The
output of the model is a probability score which is used to determine the likelihood that an
issue is a duplicate. Sun et al.’s approach outperforms the prior state of the art approaches
(Runeson et al. 2007; Jalbert and Weimer 2008; Wang et al. 2008) by 17–31 %, 22–26 %,
and 35–43 % on OpenOffice, Firefox, and Eclipse datasets, respectively. Sureka and Jalote
(2010) used a character n-grams approach (Kanaris et al. 2007) to measure the text simi-
larity between the titles and descriptions of issue reports. Sureka et al.’s approach achieved
around 62 % recall for 2,270 randomly selected reports from the Eclipse project.
Leveraging both Textual and Non-textual Information Other information in issue
reports, such as the component, version, Bug Id, and platform, are found useful in improv-
ing the accuracy of approaches for duplicate identification. Jalbert and Weimer (2008)
proposed an approach that combines textual similarity with non-textual features that are
derived from issue reports. Their approach was able to filter out 8 % of the duplicate issue
reports while allowing at least one report for each unique issue to reach developers. Jalbert
et al.’s approach achieved a recall of 51 %. Kaushik and Tahvildari (2012) compared the
performance of word-based and topic-based information retrieval models using non-textual
features (such as Component) and textual features. For the word based models, Kaushik et
al. applied different term weighting approaches including the ones proposed by Runeson
et al. (2007) and Jalbert and Weimer (2008). While for the topic based models, Kaushik et
al. applied Latent Semantic Indexing (LSI) (Deerwester et al. 1990), Latent Dirichlet Allo-
cation (LDA) (Blei et al. 2003), and Random Indexing (Kanerva et al. 2000) approaches.
The comparison is applied on Mozilla and Eclipse datasets. The results show that the word
based approaches outperform the topic based approaches.
Sun et al. (2011) leveraged a BM25F model (Robertson et al. 2004) to combine both tex-
tual and non-textual information for identifying duplicate issue reports. Sun et al. achieved
an improvement of 10-27 % in recall in comparison to Sureka et al.’s approach (Sureka and
Jalote 2010). Alipour et al. (2013) proposed an approach which uses software dictionaries
and word lists to extract the implicit context of each issue report. Alipour et al. showed that
Empir Software Eng
Table 1 An overview of the issue reports in the studied projects
Project # Issues # Duplicates Period
Firefox 90,128 27,154 (30.1%) Jun.1999-Aug.2010
SeaMonkey 88,049 35,827 (40.7%) Nov.1995-Aug.2010
Bugzilla 15,632 3,251 (20.0%) Sep.1994-Aug.2010
Eclipse-Platform 85,382 15,044 (17.6%) Oct.2001-Jun.2010
the use of contextual features improves the accuracy of duplicate identification by 11.55 %
over (Sun et al. 2011)’s approach on a large dataset of issues from several Android projects.
Lazar et al. (2014) proposed a duplicate identification approach based on new textual
features that capture WordNet augmented word-overlap and normalized sentences-length
differences. Lazar et al. also used non-textual features such as component and open date.
The new set of textual features achieves an accuracy improvement between 3.25 % and
6.32 % over the Alipour et al.’s approach (Alipour et al. 2013).
Wang et al. (2008) combined textual information and execution traces to identify dupli-
cate issue reports. For two issue reports, Wang et al. calculates two separate similarity
measures: one measure for textual similarity and another measure to capture the similarity
of the execution traces. By combining both similarity measures, a list of possible duplicate
issue reports is suggested for each newly reported issue. Wang et al.’s approach achieved an
accuracy of 67 % to 93 %. Lerch and Mezini (2013) proposed an automatic duplicate iden-
tification approach based on the stack traces that are attached to issue reports. Lerch et al.
used the TD-IDF similarity of stack traces for identifying duplicate reports (around 10 % of
reports contain stack traces).
Aggarwal et al. (2015) proposed a duplicate identification approach based on the
contextual information that is extracted from software-engineering textbooks and project
documentation. Aggarwal et al.’s approach achieved lower accuracy than the approach by
Alipour et al. (2013). However, Aggarwal et al.’s approach is simpler with up to six times
less computation time on the same Android dataset.
Although prior research sought to accurately identify duplicate issue reports with advanced
approaches, there exists no research which examines whether identifying duplicate issue reports
is indeed consuming a large amount of developer effort. Hence in this paper, we do not aim
to propose yet another duplicate identification approach. Instead, we study the needed effort
for identifying duplicate issue reports in order to understand how and whether developers
can make good use of automated approaches for duplicate identification in practice.
3 Case Study Setup
In this section, we present the studied projects and the process of preparing the data that is
used for our case studies. We share our data and scripts as an online replication package.4
3.1 Studied Projects
We used the issue reports that are stored in the Bugzilla issue tracking system from
four open-source projects: Firefox, SeaMonkey, Bugzilla, and Eclipse-Platform. All four
4Replication package: http://sailhome.cs.queensu.ca/replication/EMSE2015 DuplicateReports/
Empir Software Eng
New Assigned Resolved
VerifiedClosed
Issue Report Status
FIXED
INVALID 
WONTFIX
DUPICATE
WORKSFORME
Issue Report Resolution
Fig. 1 The life cycle of an issue report in Bugzilla
projects are mature projects with years of development history. Table 1 shows an overview
of the studied projects. Firefox, SeaMonkey and Bugzilla are open-source projects from
the Mozilla foundation. These projects have been frequently studied by prior research
for evaluating the automated approaches for duplicates identification (Anvik et al. 2005).
Eclipse-Platform is a sub-project of Eclipse, which is one of the most popular Java Inte-
grated Development Environments (IDEs). The duplicate issue reports in the Eclipse project
have also been studied by prior research. For example, (Bettenburg et al. 2008a) used the
Eclipse dataset to demonstrate the value of merging information across duplicate issue
reports. We studied the projects up to the end of 2010 because Bugzilla 4.0 (released in
February 2011) introduced an automated duplicate identification feature when filing an
issue.5 Such an automated identification feature may bias our measurement of the needed
effort for identifying duplicate reports. For example, many trivial duplicate reports might
not be filled since Bugzilla would warn about them at the filing time of the report.
3.2 Linking of Duplicate Issue Reports
In practice, the identification of duplicate reports is done manually. Whenever devel-
opers identify a duplicate issue report, they change the resolution status of a report to
DUPLICATE, and add a reference (i.e., Id-Number) to the previously reported issue. An
automatically-generated comment is attached to the discussion of the issue report in order
to indicate that this issue report is a duplicate of another one. We use such automatically
generated comments to identify duplicate issue reports. For example, if we find a message
like ***This issue has been marked as a duplicate of B *** in the discussions of issue report
A, we consider issue report A and B as duplicates of each other. We consider the latest Id-
Number in case it was changed. We group duplicate issue reports. For example, if we find
that issue report A and B are duplicates of each other and issue report B and C are dupli-
cates of each other, we consider issue reports A, B and C as duplicates of each other. We
repeat this grouping process, until we cannot add additional duplicate reports to a group.
3.3 Pre-processing of Duplicate Issue Reports
Figure 1 shows the life cycle of an issue report in Bugzilla Koponen (2006). First an issue
is reported with status NEW. Next, a triaging process determines if an issue report should
be assigned to a developer to fix. If the issue is assigned to a developer the status of the
issue is changed to ASSIGNED. After the developer fixes the issue, the report status is
changed to RESOLVED-FIXED. In some cases, an issue is not fixed because it is invalid
5Release notes for Bugzilla 4.0: https://www.bugzilla.org/releases/4.0/release-notes.html
Empir Software Eng
Triaged
Marked as a  
Duplicate
Un-duplicated Reported
Identification Delay
Identification Delay (without Triage)
Fig. 2 A typical timeline for managing duplicate issue reports
(i.e., RESOLVED-INVALID), not reproducible (i.e., RESOLVED-WORKSFORME) or dupli-
cate (i.e., RESOLVED-DUPLICATE). Finally, the issue is verified by another developer or
by a tester (i.e., VERIFIED-FIXED or CLOSED-FIXED).
Figure 2 presents the typical process for managing a duplicate report. First, a dupli-
cate report is triaged. Then, it gets identified as a duplicate. The un-duplication happens
when the DUPLICATE resolution is removed. If a developer finds that a duplicate report
is actually not a duplicate, the DUPLICATE resolution is removed. On average 2.5 %
of the duplicates in the studied projects are un-duplicated. We remove the un-duplicated
issue reports from our analysis of duplicate reports. The removal is performed by ensuring
that the last resolution status is one of the following statuses: RESOLVED-DUPLICATE,
VERIFIED-DUPLICATE and CLOSED-DUPLICATE.
4 Results
We now present the results of our research questions. The presentation of each research
question is composed of three parts: the motivation of the research question, the approach
that we used to address the research question, and our experimental results.
4.1 RQ1: How Much Effort is Needed to Identify a Duplicate Issue Report?
Motivation There exists a considerable amount of duplicate reports. Prior research shows that
around 20–30 % of the issue reports in Mozilla and Eclipse, respectively, are duplicates
(Bettenburg et al. 2007). Duplicate issue reports are typically considered as a waste of develop-
ers’ time and resources. Thus, many researchers have proposed automated identification app-
roaches for duplicates. However, the effort that is needed for manually identifying duplicate
reports has never been investigated. Therefore, we would like to examine whether identifying
duplicate reports consumes a considerable amount of developers’ effort in practice.
Approach To measure the effort of identifying duplicate issue reports, we calculate the
following three measures:
– Identification Delay. We calculate the identification delay for each duplicate report (see
Fig. 2). This measure provides us a rough estimate of the needed effort. The more time
it takes to identify a duplicate report, the more developers’ effort is needed. For reports
that are triaged, we measure the time in days between the triaging of the report and
when the report is marked as a duplicate. For reports that are never triaged, we count
the number of days between the reporting of the report and when the report is marked
as a duplicate.
– Identification Discussions. We count the number of comments posted on an issue report
before it is identified as a duplicate report. The more discussions about a particular
Empir Software Eng
Table 2 Mean and Five number summary of the three effort measures
Project Metric Mean Min 1st.Qu Median 3rd Qu. Max
Firefox Identification Delay (Days) 34.74 0.00 0.02 0.20 2.84 2,042
Identification Discussion (Count) 1.20 0.00 0.00 0.00 1.00 230
People Involved (Count) 1.59 0.00 1.00 1.00 2.00 37
SeaMonkey Identification Delay (Days) 35.59 0.00 0.03 0.18 3.51 3,245
Identification Discussion (Count) 1.52 0.00 0.00 0.00 2.00 111
People Involved (Count) 1.75 0.00 1.00 1.00 2.00 75
Bugzilla Identification Delay (Days) 92.84 0.00 0.02 0.32 27.41 3,132
Identification Discussion (Count) 1.82 0.00 0.00 0.00 2.00 135
People Involved (Count) 1.62 0.00 1.00 1.00 2.00 43
Eclipse-Platform Identification Delay (Days) 57.49 0.00 0.06 0.83 13.83 2,931
Identification Discussion (Count) 1.60 0.00 0.00 0.00 2.00 58
People Involved (Count) 1.80 0.00 1.00 1.00 2.00 46
report, the more complex and effort consuming it is to identify a duplicate. We ignore
the comments that are not representing the issue report discussions. For example, all
auto-generated comments, such as Created an attachment, are filtered. Then, in the case
of duplicates with one remaining comment, we filter that comment if it is posted by the
same reporter of the issue in order to add missing information.
– People Involved. We count the number of unique people that are involved in discussing
each issue other than the reporter before the issue is marked as a duplicate. More people
discussing a duplicate report indicates that more team time and resources are spent on
understanding that particular duplicate report.
Results. Most of the Duplicate Reports are Identified Within a Short Time We find
that the median identification delay for the studied projects is between 0.18 to 0.83 days
(see Table 2). Such results show that more than 50 % of the duplicate reports are identified
in less than one day. We do note that our identification delay metric is an over estimate of
the actual time that is spent on identifying a duplicate report. Developers are not likely to
start examining a report as soon as it is reported. Instead, there are many reasons for such
activity to be delayed (e.g., reports are filed while developers are away from their desks, or
developers are busy with other activities).
Most of the Duplicate Reports are Identified Without any Discussion We find that
over 50 % of the issue reports are marked as duplicates with no discussions, and over 75 %
of the issue reports are marked as duplicates with just one or two comments (see Table 2).
Figure 3 shows the cumulative density function (CDF) plot of the discussions count for
the four projects. The curve shows that at least 50 % of the duplicates are identified without
any discussions. We only show one CDF plot (the one for identification discussions) since
the other CDF plots follow the same pattern.
Very few People are Involved in Identifying Duplicates Table 2 shows that for over half
of the duplicate reports, the identification effort involves one person without counting the
reporter of the issue.
Empir Software Eng
0 10 20 30 40 50 60
0.
0
0.
2
0.
4
0.
6
0.
8
1.
0
Number of Comments
C
um
ul
at
iv
e 
D
en
si
ty
FireFox
Eclipse−Platform
SeaMonkey
Bugzilla
Fig. 3 Cumulative Density Function (CDF) of identification discussions for the four studied projects. The
x-axis shows the identification discussions, and y-axis shows the cumulative density
Discussion The results in Table 2 show that it takes little effort (i.e., time, discussions,
and people) to identify the majority of the duplicate reports. However, it is also inter-
esting to note that there are issue reports taking thousands of days to identify, with up
to 230 comments, involving up to 75 people. Such issue reports consume a considerable
amount of developers’ efforts until developers realize that the issue reports are dupli-
cates of previously-reported issues. Although various advanced approaches are proposed by
prior research to automatically identify duplicate reports, those approaches do not differ-
entiate between the easily-identifiable duplicate issue reports with the ones that are more
effort-consuming to identify. Automated duplicate identification approaches should focus
on helping developers identify duplicate reports that consume more effort to identify.
4.2 RQ2: How Well Can We Model the Needed Effort for Identifying Duplicate
Issue Reports?
Motivation The results of RQ1 show that while the majority of the duplicated reports are
identified with little effort, the duplicate identification of some reports requires considerable
Empir Software Eng
amount of efforts. Understanding the effort that is needed to identify duplicate reports is
important for managing project resources. To the best of our knowledge, no study has ever
examined such effort and whether it is predictable (i.e., whether one can determine if a
particular issue will require more effort over another issue). Therefore, in this RQ, we would
like to build classifiers to determine the needed efforts to identify duplicate reports.
Approach In order to build a classifier for the effort that is needed of identifying dupli-
cates, we compute a set of factors from the duplicate reports. The factors are from five
dimensions: Peers, Workload, Issue Report, Work Habits, and Identifier Experience. We
describe each dimension below:
– Peers: Peers of a duplicate report are the issue reports that are duplicates of that report.
In our work, we only include the peers that are reported before the reporting of an
issue report, since only such peers can provide information to help identify that an issue
report is a duplicate of previously-reported issues. The more information available from
the peers of an issue report, the easier it is for developers to identify that an issue report
is a duplicate.
– Workload: Workload refers to the number of open issues at the time of reporting or
triaging of a duplicate report. If there is a high workload at the time of reporting an
issue, the duplicate identification of a newly reported issue may be delayed.
– Issue Report: Issue report refers to the information that can be derived from an issue
report at the time of its reporting. Better information in the issue report should help
reduce the effort that is needed to identify that the issue report has duplicates.
– Work Habits: Work habit refers to the time when an issue is reported. If an issue is
reported during the weekends or at night, identifying that the report is a duplicate may
be delayed.
– Identifier Experience: Refers to the possible information that a duplicate report
identifier has. Duplicate reports identified by experienced persons might require less
effort.
Table 3 presents the list of our studied factors along with their descriptions and our rationale
for studying such factors. To calculate the factors Title Similarity and Description Similarity
from the Peers Dimension, we first apply simple pre-processing steps, such as remov-
ing special characters, long spaces and converting all text to lowercase. Then, we apply
a trigram algorithm (Lazar et al. 2014) to calculate the similarity between texts. For the
Reproduce Steps factor, we use a similar approach as Bettenburg et al. (2008a) by searching
the description text for certain keywords in, such as steps to reproduce, steps, reproducible
and reproduce. We build individual models for every project because each project may have
different ways for handling duplicate issue reports. For example, The triaging processes for
the Mozilla and Eclipse projects are different. For Mozilla, triaged issue reports are not ini-
tially triaged to any developer. On the other hand, Eclipse issue reports are triaged at filing
to a sub-group of developers based on the project component. Only the developers respon-
sible to a certain component receive the notifications about any newly reported issue report.
Based on this triaging process, Eclipse’s developers may have a better chance to see a newly
reported issue earlier than the developers of Mozilla (though the Eclipse developers might
be overwhelmed with many irrelevant reports).
Correlation Analysis We use the varclus package (Chavent et al. 2015) in R to display
Spearman rank correlations (ρ) between our factors given the large number of factors in
our study. Varclus does a hierarchical cluster analysis on the factors using Spearman cor-
relations. If the correlation between two factors is equal to or greater than 0.7, we ignore
Empir Software Eng
Ta
bl
e
3
T
he
fa
ct
or
s
us
ed
to
m
od
el
th
e
ef
fo
rt
s
ne
ed
ed
fo
r
id
en
tif
yi
ng
du
pl
ic
at
e
re
po
rt
s
D
im
en
si
on
Fa
ct
or
s
V
al
ue
D
ef
in
iti
on
(d
)
–
R
at
io
na
le
(r
)
Pe
er
s
Pe
er
s
C
ou
nt
N
um
er
ic
d:
T
he
to
ta
ln
um
be
r
of
pe
er
s
th
at
an
is
su
e
re
po
rt
ha
s.
W
e
on
ly
co
ns
id
er
th
e
pe
er
s
th
at
ex
is
te
d
be
fo
re
th
e
re
po
rt
in
g
of
th
e
ne
w
is
su
e.
r:
A
n
is
su
e
re
po
rt
w
ith
m
an
y
ex
is
tin
g
pe
er
s
m
ig
ht
be
ea
si
er
to
id
en
tif
y
as
a
du
pl
ic
at
e.
T
itl
e
Si
m
ila
ri
ty
N
um
er
ic
d:
T
he
m
ax
im
um
si
m
ila
ri
ty
be
tw
ee
n
th
e
tit
le
te
xt
of
an
is
su
e
re
po
rt
an
d
tit
le
te
xt
of
its
pe
er
s.
(0
-1
)
T
he
si
m
ila
ri
ty
is
m
ea
su
re
d
us
in
g
tr
ig
ra
m
m
at
ch
in
g.
r:
Si
m
ila
r
tit
le
te
xt
as
a
pr
ev
io
us
ly
re
po
rt
ed
is
su
e
m
ay
as
si
st
in
id
en
tif
yi
ng
th
at
an
is
su
e
re
po
rt
is
a
du
pl
ic
at
e
of
an
ex
is
tin
g
on
e.
D
es
cr
ip
tio
n
Si
m
ila
ri
ty
N
um
er
ic
d:
T
he
m
ax
im
um
si
m
ila
ri
ty
be
tw
ee
n
th
e
de
sc
ri
pt
io
n
te
xt
of
an
is
su
e
re
po
rt
an
d
th
e
de
sc
ri
pt
io
n
(0
-1
)
te
xt
of
its
pe
er
s.
T
he
si
m
ila
ri
ty
is
m
ea
su
re
d
us
in
g
tr
ig
ra
m
m
at
ch
in
g.
r:
Si
m
ila
r
de
sc
ri
pt
io
n
te
xt
as
a
pr
ev
io
us
ly
re
po
rt
ed
is
su
e
m
ay
as
si
st
in
id
en
tif
yi
ng
th
at
an
is
su
e
re
po
rt
is
a
du
pl
ic
at
e
of
an
ex
is
tin
g
on
e.
Pe
er
s
C
C
L
is
t
N
um
er
ic
d:
T
he
su
m
of
th
e
un
iq
ue
pe
rs
on
s
in
th
e
C
C
lis
ts
of
an
is
su
e
re
po
rt
’s
pe
er
s.
r:
A
la
rg
er
pe
er
s’
C
C
lis
ts
co
ul
d
be
a
si
gn
of
be
tte
r
aw
ar
en
es
s
of
an
is
su
e
ac
ro
ss
th
e
de
ve
lo
p-
m
en
tt
ea
m
.
Pe
er
s
C
om
m
en
ts
N
um
er
ic
d:
T
he
to
ta
ln
um
be
r
of
co
m
m
en
ts
th
at
be
lo
ng
to
th
e
pe
er
s
of
an
is
su
e
re
po
rt
.W
e
on
ly
co
ns
id
er
th
e
co
m
m
en
ts
th
at
ex
is
te
d
be
fo
re
th
e
re
po
rt
in
g
of
th
e
is
su
e
re
po
rt
.
r:
If
an
is
su
e
re
po
rt
ha
s
w
el
ld
is
cu
ss
ed
pe
er
s
(w
ith
m
or
e
co
m
m
en
ts
),
th
e
is
su
e
re
po
rt
m
ay
be
ea
si
er
to
id
en
tif
y
as
a
du
pl
ic
at
e.
R
ep
or
tin
g
R
ec
en
cy
of
Pe
er
s
N
um
er
ic
d:
T
he
sm
al
le
st
di
ff
er
en
ce
in
da
ys
be
tw
ee
n
th
e
tr
ia
ge
or
re
po
rt
da
te
of
an
is
su
e
re
po
rt
to
th
e
da
te
w
he
n
on
e
of
its
pe
er
s
ar
e
re
po
rt
ed
.
r:
If
a
de
ve
lo
pe
r
ha
s
se
en
a
si
m
ila
r
is
su
e
th
at
is
re
po
rt
ed
re
ce
nt
ly
,i
tc
ou
ld
sp
ee
d
up
th
e
du
pl
ic
at
e
id
en
tif
ic
at
io
n
pr
oc
es
s.
R
ec
en
cy
of
Pe
er
Id
en
tif
ic
at
io
n
N
um
er
ic
d:
T
he
sm
al
le
st
di
ff
er
en
ce
in
da
ys
be
tw
ee
n
th
e
tr
ia
ge
or
re
po
rt
da
te
of
an
is
su
e
re
po
rt
to
th
e
da
te
w
he
n
on
e
of
its
pe
er
s
is
id
en
tif
ie
d
as
a
du
pl
ic
at
e.
Empir Software Eng
Ta
bl
e
3
T
he
fa
ct
or
s
us
ed
to
m
od
el
th
e
ef
fo
rt
s
ne
ed
ed
fo
r
id
en
tif
yi
ng
du
pl
ic
at
e
re
po
rt
s
(c
on
tin
ue
d)
D
im
en
si
on
Fa
ct
or
s
V
al
ue
D
ef
in
iti
on
(d
)
–
R
at
io
na
le
(r
)
r:
If
a
de
ve
lo
pe
r
ha
s
se
en
a
si
m
ila
r
is
su
e
th
at
is
id
en
tif
ie
d
re
ce
nt
ly
,i
tc
ou
ld
sp
ee
d
up
th
e
du
pl
ic
at
e
id
en
tif
ic
at
io
n
pr
oc
es
s.
R
ec
en
cy
of
C
lo
se
d
Is
su
e
N
um
er
ic
d:
T
he
sm
al
le
st
di
ff
er
en
ce
in
da
ys
be
tw
ee
n
th
e
tr
ia
ge
or
re
po
rt
da
te
of
a
du
pl
ic
at
e
re
po
rt
to
th
e
da
te
w
he
n
an
y
is
su
e
re
po
rt
cl
os
ed
w
ith
in
th
e
sa
m
e
pr
oj
ec
ta
nd
co
m
po
ne
nt
is
cl
os
ed
.
r:
If
a
de
ve
lo
pe
r
ha
s
re
ce
nt
ly
fi
xe
d
an
is
su
e
w
ith
in
th
e
sa
m
e
pr
oj
ec
ta
nd
co
m
po
ne
nt
,i
tc
ou
ld
in
di
ca
te
th
e
de
ve
lo
pe
r’
s
ac
tiv
ity
an
d
it
co
ul
d
sp
ee
d
up
th
e
du
pl
ic
at
e
id
en
tif
ic
at
io
n
pr
oc
es
s
w
ith
in
th
e
sa
m
e
pr
oj
ec
ta
nd
co
m
po
ne
nt
.
W
or
kl
oa
d
W
ai
tin
g
L
oa
d
N
um
er
ic
d:
T
he
to
ta
ln
um
be
r
of
ot
he
r
is
su
es
re
po
rt
s
th
at
ar
e
op
en
at
th
e
tim
e
w
he
n
th
at
is
su
e
is
re
po
rt
ed
w
ith
in
th
e
sa
m
e
co
m
po
ne
nt
an
d
pr
oj
ec
t.
r:
A
la
rg
e
nu
m
be
r
of
op
en
is
su
es
co
ul
d
m
ak
e
de
ve
lo
pe
rs
to
o
bu
sy
to
re
co
gn
iz
e
w
he
th
er
an
is
su
e
is
a
du
pl
ic
at
e.
Is
su
e
R
ep
or
t
Se
ve
ri
ty
N
um
er
ic
d:
A
nu
m
be
r
re
pr
es
en
tin
g
th
e
se
ve
ri
ty
of
an
is
su
e
re
po
rt
.
r:
M
or
e
se
ve
re
is
su
e
re
po
rt
s
ar
e
lik
el
y
to
at
tr
ac
ta
tte
nt
io
n
fr
om
de
ve
lo
pe
rs
,t
hu
s
su
ch
hi
gh
se
ve
ri
ty
is
su
es
ar
e
lik
el
y
to
be
id
en
tif
ie
d
as
a
du
pl
ic
at
e
fa
st
er
.
Pr
io
ri
ty
N
um
er
ic
d:
A
nu
m
be
r
re
pr
es
en
tin
g
th
e
pr
io
ri
ty
of
an
is
su
e
re
po
rt
.
r:
Is
su
e
re
po
rt
w
ith
hi
gh
er
pr
io
ri
ty
ar
e
lik
el
y
to
at
tr
ac
ta
tte
nt
io
n
fr
om
de
ve
lo
pe
rs
,t
hu
s
su
ch
hi
gh
se
ve
ri
ty
is
su
es
ar
e
lik
el
y
to
be
id
en
tif
ie
d
as
a
du
pl
ic
at
e
fa
st
er
.
C
om
po
ne
nt
N
om
in
al
d:
T
he
co
m
po
ne
nt
sp
ec
if
ie
d
in
an
is
su
e
re
po
rt
.
r:
Pa
rt
ic
ul
ar
co
m
po
ne
nt
s
m
ay
be
m
or
e
im
po
rt
an
tt
ha
n
ot
he
rs
.H
en
ce
is
su
es
as
so
ci
at
ed
w
ith
th
em
ar
e
m
or
e
lik
el
y
to
be
id
en
tif
ie
d
as
du
pl
ic
at
es
fa
st
er
.
T
itl
e
Si
ze
N
um
er
ic
d:
T
he
nu
m
be
r
of
w
or
ds
in
th
e
tit
le
te
xt
.
r:
Is
su
es
w
ith
lo
ng
er
tit
le
m
ig
ht
pr
ov
id
e
m
or
e
us
ef
ul
in
fo
rm
at
io
n
w
hi
ch
m
ig
ht
ea
se
th
e
du
pl
ic
at
e
id
en
tif
ic
at
io
n.
D
es
cr
ip
tio
n
Si
ze
N
um
er
ic
d:
T
he
nu
m
be
r
of
w
or
ds
in
th
e
de
sc
ri
pt
io
n
of
an
is
su
e
re
po
rt
.
r:
Is
su
es
w
ith
lo
ng
er
de
sc
ri
pt
io
n
co
ul
d
be
ea
si
er
to
un
de
rs
ta
nd
an
d
m
ig
ht
ea
se
th
e
du
pl
ic
at
e
id
en
tif
ic
at
io
n.
Empir Software Eng
Ta
bl
e
3
T
he
fa
ct
or
s
us
ed
to
m
od
el
th
e
ef
fo
rt
s
ne
ed
ed
fo
r
id
en
tif
yi
ng
du
pl
ic
at
e
re
po
rt
s
(c
on
tin
ue
d)
D
im
en
si
on
Fa
ct
or
s
V
al
ue
D
ef
in
iti
on
(d
)
–
R
at
io
na
le
(r
)
is
B
lo
ck
in
g
B
oo
le
an
d:
A
bo
ol
ea
n
va
lu
e
in
di
ca
tin
g
w
he
th
er
an
is
su
e
re
po
rt
is
bl
oc
ki
ng
ot
he
r
is
su
es
.
r:
A
bl
oc
ki
ng
is
su
e
co
ul
d
be
m
or
e
im
po
rt
an
tt
o
ad
dr
es
s
an
d
th
is
m
ig
ht
le
ad
to
fa
st
er
id
en
ti-
fi
ca
tio
n
of
du
pl
ic
at
es
.
is
D
ep
en
da
bl
e
B
oo
le
an
d:
A
bo
ol
ea
n
va
lu
e
in
di
ca
tin
g
w
he
th
er
an
is
su
e
re
po
rt
de
pe
nd
s
on
ot
he
r
is
su
es
.
r:
A
n
is
su
e
de
pe
nd
in
g
on
ot
he
r
is
su
es
m
ay
ha
ve
m
or
e
at
te
nt
io
n
fr
om
de
ve
lo
pe
rs
an
d
ge
t
id
en
tif
ie
d
as
a
du
pl
ic
at
e
ea
si
er
.
R
ep
ro
du
ce
St
ep
s
B
oo
le
an
d:
A
bo
ol
ea
n
va
lu
e
in
di
ca
tin
g
w
he
th
er
th
e
de
sc
ri
pt
io
n
of
an
is
su
e
re
po
rt
co
nt
ai
ns
st
ep
s
fo
r
re
pr
od
uc
in
g
th
e
is
su
e.
r:
Is
su
es
w
ith
re
pr
od
uc
e
st
ep
s
sh
ou
ld
be
ea
si
er
to
un
de
rs
ta
nd
an
d
m
ig
ht
ea
se
th
e
id
en
tif
ic
at
io
n
of
du
pl
ic
at
es
.
W
or
k
H
ab
its
is
W
ee
ke
nd
B
oo
le
an
d:
A
bo
ol
ea
n
va
lu
e
re
pr
es
en
tin
g
w
he
th
er
an
is
su
e
is
re
po
rt
ed
on
a
w
ee
ke
nd
da
y
(i
.e
.,
Sa
tu
rd
ay
an
d
Su
nd
ay
)
or
no
t.
r:
T
he
du
pl
ic
at
e
re
po
rt
s
su
bm
itt
ed
on
w
ee
ke
nd
s
m
ig
ht
ta
ke
lo
ng
er
tim
e
to
be
id
en
tif
ie
d
th
an
th
e
on
es
on
w
ee
k
da
ys
.
Id
en
tif
ie
r
To
ta
lD
up
lic
at
es
Id
en
tif
ie
d
N
um
er
ic
d:
T
he
to
ta
ln
um
be
r
of
du
pl
ic
at
e
re
po
rt
s
th
at
ar
e
pr
ev
io
us
ly
id
en
tif
ie
d
by
th
e
sa
m
e
id
en
tif
ie
r
E
xp
er
ie
nc
e
pe
r
ea
ch
du
pl
ic
at
e
re
po
rt
.
r:
A
n
id
en
tif
ie
r
w
ith
m
or
e
ex
pe
ri
en
ce
in
fi
nd
in
g
du
pl
ic
at
e
re
po
rt
s
m
ig
ht
af
fe
ct
th
e
ef
fo
rt
ne
ed
ed
fo
r
id
en
tif
yi
ng
a
ne
w
du
pl
ic
at
e
re
po
rt
.
D
up
lic
at
e
Pe
er
s
Id
en
tif
ie
d
N
um
er
ic
d:
T
he
to
ta
ln
um
be
r
of
pe
er
s
du
pl
ic
at
e
re
po
rt
s
th
at
ar
e
pr
ev
io
us
ly
id
en
tif
ie
d
by
th
e
sa
m
e
id
en
tif
ie
r.
r:
A
n
id
en
tif
ie
r
th
at
ha
s
se
en
se
ve
ra
lp
ee
r
du
pl
ic
at
es
m
ig
ht
id
en
tif
y
du
pl
ic
at
es
fa
st
er
.
Fi
xe
d
Is
su
es
pe
r
Id
en
tif
ie
r
N
um
er
ic
d:
T
he
to
ta
ln
um
be
r
of
is
su
e
re
po
rt
s
th
at
ar
e
pr
ev
io
us
ly
m
ar
ke
d
as
FI
X
E
D
by
th
e
sa
m
e
id
en
tif
ie
r.
r:
A
n
id
en
tif
ie
r
w
ith
m
or
e
ex
pe
ri
en
ce
in
fi
xi
ng
is
su
es
m
ig
ht
id
en
tif
y
du
pl
ic
at
es
fa
st
er
.
Empir Software Eng
Fig. 4 Spearman hierarchical cluster analysis for the Firefox dataset
one of them since we consider a correlation of 0.7 as a high correlation (McIntosh et al.
2015; Mockus and Weiss 2000). Figure 4 shows the output of varclus where the red dotted
line marks the 0.7 Spearman correlation. Looking at Fig. 4, we notice that the factors Peers
Counts, Peers CC List, and Peers Comments are highly correlated with a correlation range
of 0.85 − 0.91. Hence, we need to pick just one of these three factors and discard the other
two. Based on the results of Varclus (see Fig. 4), we remove Peers Comments, Peers Counts,
and Fixed Issues per Identifier factors due to the high correlation especially in cases such
as between Peers CC List and Peers Comments. After this step, 18 of 21 factors remain.
Modeling Technique We build two classifiers to understand the effort that is needed for
identifying duplicate reports. In RQ1, we have three measures of the effort that is needed
for identifying duplicate reports. However, the People Involved and the Identification Dis-
cussions factors are highly correlated. Table 4 presents the Spearman correlations between
the three effort factors for the studied projects. Hence, we only build two classifiers, one
classifier for Identification Delay and another one for Identification Discussions.
We divide the issue reports into two classes (Slow and Fast) based on the Identification
Delay. All studied projects are open-source ones, where developers may not be able to
examine an issue as soon as it is reported. In RQ1, we find that over half of the duplicate
Table 4 Spearman correlations between the three effort measures in the four studied projects
Correlation Firefox SeaMonkey Bugzilla Eclipse-Platform
Identification Delay vs Identification Discussions 0.54 0.55 0.56 0.45
Identification Delay vs People Involved 0.54 0.52 0.53 0.36
Identification Discussions vs People Involved 0.78 0.81 0.81 0.68
Empir Software Eng
Table 5 Criteria for each class based on Identification Delay and Identification Discussions
Identification Delay Classes
Slow Includes all duplicate reports with identification
delay that is more than one day.
Fast Includes all duplicate reports with identification
Delay that is less than or equal to one day.
Identification Discussions Classes
Not-Discussed Includes all duplicate reports that did not have
any comments.
Discussed Includes all duplicate reports with one or more
comments.
reports are identified within one day. Therefore, we choose one day to be the threshold to
determine whether a duplicate report is identified Slow or Fast.
Similarly, we divide the issue reports into two classes (Not-Discussed and Discussed)
based on the number of Identification Discussions. We consider that the issue reports that
are identified as a duplicate without discussion as the ones requiring minimal effort.
Table 5 summarizes the criteria for each class based on the Identification Delay (Slow,
Fast) and Identification Discussions (Not-Discussed, Discussed). Table 6 presents the
distribution of duplicate reports for each class per each project.
We build our classifiers using a random forest (Breiman 2001) classifier. Random forest
classifiers are known to be robust to data noise and often achieve a high accuracy in soft-
ware engineering research (Robnik-Ṡikonja 2004; Jiang et al. 2008; Lessmann et al. 2008;
Kamei et al. 2010; Ghotra et al. 2015). We use the random forest algorithm provided by the
randomForest R-package (Liaw and Wiener 2014). In this study, we train the random forest
classifier using 100 decision trees.
An important benefit of a random forest classifier is that it has a mechanism to exam-
ine the relative influence of the underlying factors in contrast to other machine learning
approaches (e.g., Neural Networks). Thus, we would like to point out that although random
forest classifier has been used to build accurate classifier for prediction, our purpose of using
a random forest classifier in this paper is not for predicting the needed effort for identifying
duplicate issues. Our purpose is to study the important factors that impact the needed effort
for identifying duplicate issues. Nevertheless, we first need to determine whether the prob-
lem at hand (i.e., understanding duplicate identification effort) is non-random in nature (and
hence easy to model), before we can start examining the important (i.e., influential) factors
in RQ3. To minimize the risks of over-fitting, we use the 10-fold cross validation technique.
A k-fold cross validation divides the data into k equal parts. For each fold, k-1 parts are used
as training data to build a classifier and the remaining part is used as testing data.
Evaluation Metrics To measure the performance of our classifiers, we use precision,
recall, F-score and area under ROC curve to evaluate our classifier. We describe each
measure below:
Precision(P) is the percentage of correctly classified duplicate reports per effort class
(e.g., Slow). Precision ranges between 0 and 1. A correct prediction is counted if the clas-
sified effort class of a duplicate report matches its actual class. The number of correctly
Empir Software Eng
Table 6 The number of duplicate reports from each project that belongs to each class
Project Not-Discussed Discussed Total
Firefox 16,686 (64.7 %) 9,088 (35.3 %) 25,774
SeaMonkey 19,535 (57.9 %) 14,214 (42.1%) 33,749
Bugzilla 1,909 (61.6 %) 1,190 (38.4 %) 3,099
Eclipse-Platform 8,057 (55.6 %) 6,421 (44.4 %) 14,478
Fast Slow Total
Firefox 17,859 (69.3 %) 7,915 (30.7 %) 25,774
SeaMonkey 22,980 (68.1 %) 10,769 (31.9 %) 33,749
Bugzilla 1,875 (60.5 %) 1,224 (39.5 %) 3,099
Eclipse-Platform 7,647 (52.8 %) 6,831 (47.2 %) 14,478
labeled reports is called true positives (TP). The number of incorrectly labeled duplicate
reports is called false positives (FP). Precision equals to (T P )/((T P ) + (FP )).
Recall(R) is the percentage of correctly classified duplicate reports for an effort class over
all the reports belong to that class. Recall ranges between 0 and 1. Recall is equal to one for
a certain effort class if all the duplicate reports belonging to that class are addressed. The
number of duplicate reports that were not labeled to a certain effort class even though they
should be labeled to it is called false negatives (FN). Recall equals to (T P )/((T P )+(FN)).
F-score is the harmonic mean that considers both precision and recall to calculate an
accuracy score. F-score equals to (2 ∗ P ∗ R)/(P + R). Where P is the precision and R is
the recall. F-score reaches the best value at 1 and worst at zero.
ROC area value measures the discriminative ability of a classifier in selecting instances
of a certain class from a dataset. The ROC metric ranges between 0 and 1. ROC curve
shows the trade-off between the true positive rate (also called Sensitivity) and false negative
rate (also called Specificity). The closer the ROC value to 1, the less trade-off exists. The
calculation of the ROC area is based on the probabilities of a test dataset for each class. A
classifier that randomly guesses the effort class without any training has an ROC area of
0.5. We use the pROC R package (Xavier et al. 2015) to calculate the ROC area.
Results. Our Classifiers for Identification Discussions Achieve an Average Preci-
sion Between 0.60 to 0.72 and an Average Recall Between 0.23 to 0.93 Table 7 shows
the precision, recall, F-score and ROC area for each class per project. Note that the highest
precisions for Bugzilla and Firefox are for the Not-Discussed class, whereas the Discussed
class has the highest precision in SeaMonkey. The recall value is highest in SeaMonkey for
the class Not-Discussed, whereas in Eclipse-Platform the highest precision and recall are
for the class Discussed. A possible reason for the high recall for the Eclipse-platform is that
the dataset is more balanced (see Table 6 which shows for example that the number of Not-
Discussed duplicates is twice as many as the number of Discussed duplicates in the Firefox
dataset). The F-score for the Non-Discussed class ranges between 0.69 to 0.78 where as
for the Discussed class it ranges from 0.34 to 0.57. The ROC areas are between 0.68 to
0.72. Such results indicate that the Identification Discussions classifier is better than random
guessing (ROC of 0.5) and indicate that our factors can be used to model and understand
the discussion effort that is needed for identifying duplicate reports.
Empir Software Eng
Table 7 Evaluation results for all the studied projects
Project Class Precision Recall F-Score ROC area
Firefox Not-Discussed 0.71 0.88 0.78 0.68
Discussed 0.60 0.34 0.43
SeaMonkey Not-Discussed 0.62 0.93 0.74 0.69
Discussed 0.69 0.23 0.34
Bugzilla Not-Discussed 0.72 0.84 0.78 0.72
Discussed 0.65 0.47 0.55
Eclipse-Platform Not-Discussed 0.66 0.72 0.69 0.69
Discussed 0.61 0.54 0.57
Firefox Fast 0.76 0.90 0.83 0.74
Slow 0.63 0.37 0.46
SeaMonkey Fast 0.73 0.96 0.83 0.77
Slow 0.74 0.23 0.35
Bugzilla Fast 0.77 0.86 0.81 0.80
Slow 0.74 0.60 0.66
Eclipse-Platform Fast 0.68 0.69 0.69 0.74
Slow 0.65 0.64 0.65
Our Classifier for Identification DelayAchieves an Average Precision Between 0.63
to 0.77 and Average Recall Between 0.23 to 0.96 From Table 7, the highest precision
and recall for the Fast class are for Firefox and SeaMonkey, while both projects have low
recall values (0.37 for Firefox and 0.23 for SeaMonkey) for the Slow class. The F-score for
the Fast class ranges between 0.70 to 0.83, whereas for the Slow class it ranges from 0.35
to 0.66. Our Identification Delay classifier has ROC areas between 0.74 to 0.80 which are
better than random guessing (ROC of 0.5). These results provide a sound starting point for
modeling the effort that is needed for identifying duplicate reports, and for examining the
important (i.e., influential) factors in the models.
4.3 RQ3: What Are the Most Influential Factors on the Effort That Is Needed
for Identifying Duplicate Issue Reports?
Motivation We wish to understand the most influential factors to the efforts that are
needed for identifying duplicate reports. By knowing the important factors, we can shed
some light on whether current state of the art automated approaches are identifying duplicate
issue reports that require little effort.
Approach To find the most influential factors to the effort that is needed for identifying
duplicate reports, we compute the variable importance for each of the factors that are used
in the RandomForest classifiers that are built in RQ2. In order to calculate the variable
Empir Software Eng
importance, we use the same randomForest R package. A Random Forest (Breiman 2001)
classifier consists of a number of decision trees. Each tree is constructed using a different
sample from the original data. Around a third of the data is left out from the construction
sample per tree. At the end of each iteration the left out data is used to test the classifica-
tion and estimate what is called the out-of-bag (oob) error (Mitchell 2011). Random Forest
follows a fitting process where the target is to minimize the out-of-bag error. To measure
the importance of each factor after building the random forest, the values of each factor are
permuted one at a time within the training set and the out-of-bag error is calculated again.
The importance of a factor is computed by calculating the average difference in out-of-bag
error. We compute the factor importance values based on the change in the out-of-bag error.
We take the average of the 10-folds classifiers for all the variable importance values for
each factor. The factor with the highest rank is identified to be the most influential factor
for modeling the identification effort.
To compute a statistically stable ranking of the factors, we use the Scott-Knott test (Scott
and Knott 1974; Tantithamthavorn et al. 2015) (with p < 0.05). The Scott-Knott test is a sta-
tistical multi-comparison cluster analysis technique. The Scott-Knott test clusters the factors
based on the reported importance values across the ten folds. The Scott-Knott test divides the
factors into two different clusters, if the difference between the two clusters is statistically
significant. The Scott-Knott test runs recursively until no more clusters can be created.
Results. Factors from the Identifier Experience and Peers Dimensions Have the
Largest Influence on Modeling the Needed Effort for Identifying Duplicates Table 8
shows the results from Scott-Knott test for the four projects. The test groups the 18 factors
into 11 significant groups on average. The clusters show that the factors from the Identifier
Experience and Peers dimensions are in the top groups of influencing factors for the needed
effort for identifying duplicate reports across all the studied projects. For Bugzilla, we find
that the Priority factor has a large influence in both effort classifiers.
Identifier Experience Dimension Has the Most Influential Factors in the Identifica-
tion Discussions Classifier The results show that the factor Total Duplicates Identified in
the Identifier Experience dimension has the most influential effect. Peers dimension factors
such as the Description Similarity, Reporting Recency of Peers/Recency of Peer Identifica-
tion, and Peers CCList play a high influential role in determining the effort that is needed
for identifying duplicates in Firefox, SeaMonkey and Eclipse-Platform. The role of identi-
fier experience in identifying duplicates highlights the importance of dedicating experienced
developers for triaging new issue reports.
Peers Dimension Has the Most Influential Factors in the Identification Delay Classi-
fier The results show that the factors in the Peers dimension are the most influential. Factors
such as the Reporting Recency of Peers/Recency of Peer Identification, Peers CCList and
Description Similarity, are most influential in determining the identification delay of dupli-
cates in Firefox, SeaMonkey and Eclipse-Platform. These results show that a new duplicate
issue may get identified faster if it has a recent peer (i.e., fresh peer). In Bugzilla, the
Total Duplicates Identified and Priority factors are still the most influential factors in the
identification discussions classifier.
Discussion. On the Impact of the Priority of an Issue Our results show that the Pri-
ority of a report is an important factor only for the Bugzilla project in contrast to the other
projects. By looking at the data, we found that 98.6 % of the duplicate reports for Firefox
Empir Software Eng
Ta
bl
e
8
Sc
ot
t-
K
no
tt
te
st
re
su
lts
w
he
n
co
m
pa
ri
ng
th
e
im
po
rt
an
ce
pe
r
fa
ct
or
fo
r
th
e
fo
ur
st
ud
ie
d
pr
oj
ec
ts
.
Fa
ct
or
s
ar
e
di
vi
de
d
in
to
gr
ou
ps
th
at
ha
ve
a
st
at
is
tic
al
ly
si
gn
if
ic
an
t
di
ff
er
en
ce
in
th
e
im
po
rt
an
ce
m
ea
n
(p
<
0.
05
)
Id
en
tif
ic
at
io
n
D
is
cu
ss
io
ns
C
la
ss
if
ie
r:
Sc
ot
t-
K
no
tt
te
st
re
su
lts
Fi
re
fo
x
Se
aM
on
ke
y
B
ug
zi
lla
E
cl
ip
se
-P
la
tf
or
m
G
ro
up
Fa
ct
or
M
ea
n
G
ro
up
Fa
ct
or
M
ea
n
G
ro
up
Fa
ct
or
M
ea
n
G
ro
up
Fa
ct
or
M
ea
n
1
To
ta
ld
up
lic
at
es
0.
01
63
1
To
ta
ld
up
lic
at
es
0.
01
15
1
To
ta
ld
up
lic
at
es
0.
01
81
1
R
ep
or
tin
g
re
ce
nc
y
0.
01
52
id
en
tif
ie
d
id
en
tif
ie
d
id
en
tif
ie
d
of
pe
er
s
2
D
es
cr
ip
tio
n
0.
01
12
2
D
es
cr
ip
tio
n
0.
01
01
2
Pr
io
ri
ty
0.
01
59
2
D
es
cr
ip
tio
n
0.
01
33
si
m
ila
ri
ty
si
m
ila
ri
ty
si
m
ila
ri
ty
R
ep
or
tin
g
re
ce
nc
y
3
R
ep
or
tin
g
re
ce
nc
y
0.
00
95
3
is
D
ep
en
da
bl
e
0.
01
14
3
C
om
po
ne
nt
0.
01
25
of
pe
er
s
of
pe
er
s
3
R
ec
en
cy
of
pe
er
0.
01
00
4
R
ec
en
cy
of
pe
er
0.
00
81
T
itl
e
si
m
ila
ri
ty
T
itl
e
si
m
ila
ri
ty
id
en
tif
ic
at
io
n
id
en
tif
ic
at
io
n
4
Pe
er
s
C
C
lis
t
0.
00
91
T
itl
e
si
m
ila
ri
ty
4
D
es
cr
ip
tio
n
0.
00
83
To
ta
ld
up
lic
at
es
si
m
ila
ri
ty
id
en
tif
ie
d
5
Se
ve
ri
ty
0.
00
80
5
Se
ve
ri
ty
0.
00
73
5
R
ec
en
cy
of
pe
er
0.
00
64
Pr
io
ri
ty
id
en
tif
ic
at
io
n
6
T
itl
e
si
m
ila
ri
ty
0.
00
66
Pe
er
s
C
C
lis
t
is
B
lo
ck
in
g
4
W
ai
tin
g
lo
ad
0.
00
74
7
C
om
po
ne
nt
0.
00
56
6
C
om
po
ne
nt
0.
00
56
R
ep
or
tin
g
re
ce
nc
y
5
R
ec
en
cy
of
pe
er
0.
00
58
of
pe
er
s
id
en
tif
ic
at
io
n
W
ai
tin
g
lo
ad
7
D
up
lic
at
e
pe
er
s
0.
00
41
6
Pe
er
s
C
C
lis
t
0.
00
51
D
up
lic
at
e
pe
er
s
id
en
tif
ie
d
id
en
tif
ie
d
8
is
B
lo
ck
in
g
0.
00
46
8
is
D
ep
en
da
bl
e
0.
00
31
7
C
om
po
ne
nt
0.
00
37
D
es
cr
ip
tio
n
si
ze
9
is
D
ep
en
da
bl
e
0.
00
38
is
B
lo
ck
in
g
W
ai
tin
g
lo
ad
6
Pe
er
s
C
C
lis
t
0.
00
28
R
ec
en
cy
of
cl
os
ed
9
W
ai
tin
g
lo
ad
0.
00
24
D
es
cr
ip
tio
n
si
ze
Se
ve
ri
ty
is
su
e
Empir Software Eng
Ta
bl
e
8
(c
on
tin
ue
d)
Id
en
tif
ic
at
io
n
D
is
cu
ss
io
ns
C
la
ss
if
ie
r:
Sc
ot
t-
K
no
tt
te
st
re
su
lts
Fi
re
fo
x
Se
aM
on
ke
y
B
ug
zi
lla
E
cl
ip
se
-P
la
tf
or
m
G
ro
up
Fa
ct
or
M
ea
n
G
ro
up
Fa
ct
or
M
ea
n
G
ro
up
Fa
ct
or
M
ea
n
G
ro
up
Fa
ct
or
M
ea
n
10
D
up
lic
at
e
pe
er
s
0.
00
28
Pr
io
ri
ty
8
R
ec
en
cy
of
cl
os
ed
0.
00
20
7
R
ec
en
cy
of
cl
os
ed
0.
00
21
id
en
tif
ie
d
is
su
e
is
su
e
11
D
es
cr
ip
tio
n
si
ze
0.
00
18
D
es
cr
ip
tio
n
si
ze
Se
ve
ri
ty
is
B
lo
ck
in
g
12
Pr
io
ri
ty
10
R
ec
en
cy
of
cl
os
ed
0.
00
14
D
up
lic
at
e
pe
er
s
is
D
ep
en
da
bl
e
0.
00
12
is
su
e
id
en
tif
ie
d
12
T
itl
e
si
ze
0.
00
08
11
R
ep
ro
du
ce
st
ep
s
0.
00
03
T
itl
e
si
ze
8
T
itl
e
si
ze
R
ep
ro
du
ce
st
ep
s
T
itl
e
si
ze
9
is
W
ee
ke
nd
0.
00
14
is
W
ee
ke
nd
13
is
W
ee
ke
nd
0.
00
02
12
is
W
ee
ke
nd
0.
00
01
R
ep
ro
du
ce
st
ep
s
9
R
ep
ro
du
ce
st
ep
s
0.
00
00
Id
en
tif
ic
at
io
n
D
el
ay
C
la
ss
if
ie
r:
Sc
ot
t-
K
no
tt
te
st
re
su
lts
Fi
re
fo
x
Se
aM
on
ke
y
B
ug
zi
lla
E
cl
ip
se
-P
la
tf
or
m
G
ro
up
Fa
ct
or
M
ea
n
G
ro
up
Fa
ct
or
M
ea
n
G
ro
up
Fa
ct
or
M
ea
n
G
ro
up
Fa
ct
or
M
ea
n
1
R
ep
or
tin
g
re
ce
nc
y
0.
03
16
1
R
ep
or
tin
g
re
ce
nc
y
0.
03
09
1
To
ta
ld
up
lic
at
es
0.
04
06
1
R
ep
or
tin
g
re
ce
nc
y
0.
03
09
of
pe
er
s
of
pe
er
s
id
en
tif
ie
d
of
pe
er
s
2
R
ec
en
cy
of
pe
er
0.
02
74
2
To
ta
ld
up
lic
at
es
0.
02
47
2
Pr
io
ri
ty
0.
03
12
2
R
ec
en
cy
of
pe
er
0.
02
03
id
en
tif
ic
at
io
n
id
en
tif
ie
d
id
en
tif
ic
at
io
n
3
To
ta
ld
up
lic
at
es
0.
02
45
3
R
ec
en
cy
of
pe
er
0.
02
36
3
R
ec
en
cy
of
pe
er
0.
01
60
3
C
om
po
ne
nt
0.
01
37
id
en
tif
ie
d
id
en
tif
ic
at
io
n
id
en
tif
ic
at
io
n
4
Pe
er
s
C
C
lis
t
0.
01
19
4
Pe
er
s
C
C
lis
t
0.
01
18
4
R
ep
or
tin
g
re
ce
nc
y
0.
01
41
4
To
ta
lD
up
lic
at
es
0.
01
28
of
pe
er
s
Id
en
tif
ie
d
Empir Software Eng
Ta
bl
e
8
(c
on
tin
ue
d)
Id
en
tif
ic
at
io
n
D
el
ay
C
la
ss
if
ie
r:
Sc
ot
t-
K
no
tt
te
st
re
su
lts
Fi
re
fo
x
Se
aM
on
ke
y
B
ug
zi
lla
E
cl
ip
se
-P
la
tf
or
m
G
ro
up
Fa
ct
or
M
ea
n
G
ro
up
Fa
ct
or
M
ea
n
G
ro
up
Fa
ct
or
M
ea
n
G
ro
up
Fa
ct
or
M
ea
n
5
D
es
cr
ip
tio
n
D
es
cr
ip
tio
n
5
D
es
cr
ip
tio
n
0.
01
05
5
D
es
cr
ip
tio
n
0.
01
06
si
m
ila
ri
ty
si
m
ila
ri
ty
si
m
ila
ri
ty
Si
m
ila
ri
ty
W
ai
tin
g
lo
ad
is
D
ep
en
da
bl
e
C
om
po
ne
nt
6
W
ai
tin
g
L
oa
d
0.
00
80
6
C
om
po
ne
nt
D
up
lic
at
e
pe
er
s
6
Pe
er
s
C
C
lis
t
0.
00
88
7
R
ec
en
cy
of
C
lo
se
d
0.
00
63
id
en
tif
ie
d
Is
su
e
T
itl
e
si
m
ila
ri
ty
0.
00
79
5
W
ai
tin
g
L
oa
d
0.
00
59
T
itl
e
si
m
ila
ri
ty
Se
ve
ri
ty
D
up
lic
at
e
pe
er
s
T
itl
e
si
m
ila
ri
ty
is
D
ep
en
da
bl
e
8
Pe
er
s
C
C
lis
t
0.
00
54
id
en
tif
ie
d
7
is
B
lo
ck
in
g
0.
00
55
6
C
om
po
ne
nt
0.
00
47
7
is
B
lo
ck
in
g
0.
00
66
Pr
io
ri
ty
is
D
ep
en
da
bl
e
0.
00
35
7
is
B
lo
ck
in
g
0.
00
32
W
ai
tin
g
lo
ad
T
itl
e
si
m
ila
ri
ty
8
R
ec
en
cy
of
0.
00
33
8
R
ec
en
cy
of
0.
00
28
8
R
ec
en
cy
of
0.
00
35
9
D
up
lic
at
e
pe
er
s
0.
00
29
cl
os
ed
is
su
e
cl
os
ed
is
su
e
cl
os
ed
is
su
e
id
en
tif
ie
d
Se
ve
ri
ty
Se
ve
ri
ty
9
D
es
cr
ip
tio
n
si
ze
0.
00
21
T
itl
e
si
ze
9
D
es
cr
ip
tio
n
si
ze
D
es
cr
ip
tio
n
si
ze
0.
00
22
Se
ve
ri
ty
is
W
ee
ke
nd
Pr
io
ri
ty
0.
00
19
9
Pr
io
ri
ty
D
up
lic
at
e
pe
er
s
10
D
es
cr
ip
tio
n
si
ze
0.
00
23
id
en
tif
ie
d
10
T
itl
e
si
ze
0.
00
09
10
T
itl
e
si
ze
0.
00
13
10
T
itl
e
si
ze
0.
00
06
11
is
B
lo
ck
in
g
0.
00
17
R
ep
ro
du
ce
st
ep
s
is
W
ee
ke
nd
is
W
ee
ke
nd
12
is
D
ep
en
da
bl
e
0.
00
07
11
is
W
ee
ke
nd
0.
00
01
11
R
ep
ro
du
ce
st
ep
s
0.
00
03
R
ep
ro
du
ce
st
ep
s
13
R
ep
ro
du
ce
st
ep
s
0.
00
01
Empir Software Eng
have an empty priority value. The high percentages of empty values imply that the Priority
field is rarely used in Firefox. Even though SeaMonkey and Bugzilla have less empty pri-
ority values (83.7 % and 85 %). SeaMonkey’s priority values are barely used, since around
88 % of the non-empty priority values at SeaMonkey are set to P 3 (the default priority).
On the other hand, only 54 % of the non-empty priority values in Bugzilla are P 3. Eclipse-
Platform dataset does not support empty priority values. But around 91 % of the filled
priority values at Eclipse-Platform are set to P 3. These results give us a good reason behind
the unique role of the Priority factor in the Bugzilla classifiers.
5 Discussion
5.1 On the Textual Similarity of Duplicate Issue Reports
Description Similarity is one of the most important factors to influence the effort that is
needed for identifying a duplicate report. The majority of the automated approaches for
duplicate issue reports identification (Anvik et al. 2005; Runeson et al. 2007; Jalbert and
Weimer 2008; Nagwani and Singh 2009; Sureka and Jalote 2010; Prifti et al. 2011; Sun
et al. 2011; Alipour et al. 2013; Lazar et al. 2014) are based on textual similarity (e.g.,
description similarity). Our results imply that the Description Similarity is one of the most
important factors to influence the needed effort for identifying a duplicate report. However,
our results imply that the more similar the description, the easier developers can identify the
duplicate issue report. Thus, the identified duplicate reports by automated approaches may
be the ones that require the least effort to identify. Figure 5 shows a Beanplot (Kampstra
et al. 2008) of the description similarity distributions for the 10 % hardest and 10 % easiest
duplicate reports for the identification delay metric. The similarity score ranges between 0
(very dissimilar) to 1 (very similar). The closer the score to 1 the more similar the issue
reports. Figure 5 shows that the hardest 10 % duplicate reports have a lower text similarity
median with a peak at the lowest similarity (i.e., no textual similarity), while the easiest
10 % have a peak at the highest similarity (i.e., identical text). The same observation holds
for identification discussions.
The results in Fig. 5 show that duplicates that require more effort to identify tend to be
less similar. Such a finding implies that current state of the art automated approaches are
likely to miss effort-consuming duplicates.
5.2 On the Difficulty of Duplicate Issue Reports
We manually examined the top 20 reports for both effort metrics (i.e, identification delay
and identification discussions) across the four studied projects (160 issues in total). We
observed that these reports correspond to difficult issues. In particular, many of the reports
Empir Software Eng
Hardest Vs Easiest Based On Identification Delay
Hardest Vs Easiest 10%
D
es
cr
ip
tio
n 
S
im
ila
irt
y
FireFox SeaMonkey Bugzilla Eclipse−Platform
0.
0
0.
2
0.
4
0.
6
0.
8
1.
0
Easiest 10% 
Hardest 10%
Fig. 5 Description Similarity Beanplot for the Hardest and Easiest duplicate reports in all studied projects
correspond to issues that take a long time to resolve and around 50 % of the corresponding
issues are blocker issues. Figure 6 shows the difference between blocker and non-blocker
duplicate reports based on identification discussions. To define the blocker issues, we
use the Blocks field that is provided by Bugzilla. Blocker duplicate reports have a larger
identification discussions median in all the studied projects (p < 0.001 for Student T-test).
To understand the relationship between duplicate report identification delay and the com-
plexity of an issue (i.e., overall resolution time of an issue), we built a Quantile regression
model (Angrist and Pischke 2008). The model examines the relation between a group of
Blocker Vs Non−Blocker for Identification Dicussions
Blocker Vs Non−Blocker Duplicate Reports
Id
en
tif
ic
at
io
n 
D
ic
us
si
on
s
FireFox SeaMonkey Bugzilla Eclipse−Platform
0
10
20
30
40
50 Non−Blocker 
Blocker
Fig. 6 The Identification Discussions Beanplot of Blocker vs Non-Blocker duplicate reports
Empir Software Eng
0.2 0.4 0.6 0.8
0
1
2
3
4
5
0.2 0.4 0.6 0.8
0e
+
00
4e
−
04
8e
−
04
(Intercept) resolution Time
(a) (b)
Fig. 7 Quantile regression Intercept and Coefficient Change for the Resolution Time when modeling the
Identification Delay in Firefox
predictor variables and specific percentiles of the response variable. Quantile regression
allows us to understand how some percentiles of the response variable are affected differ-
ently by different percentiles of the predictor variables. Such regression is appropriate given
our desire to understand the impact of the full range of our metrics. In our case, the response
variable is the Identification Delay of a report, and our predictor variable is the overall Res-
olution Time of the issue. Figure 7 shows the Quantile regression results for the Firefox
project. The red horizontal lines are the Linear Regression model (Neter et al. 1996) coef-
ficients with a red dotted confidence interval. The black curves are the Quantile regression
coefficients with a gray confidence interval. Figure 7 shows that the resolution time has
higher coefficients at higher quantiles, which shows that the complexity of an issue plays
a role in the identification delay of duplicate reports. The same pattern holds for the other
studied projects.
5.3 On the Generality of Our Observations
To explore the generality of our observations, we built two global models instead of building
project specific models as done in Section 4. We built one model for identification delay
and another model for identification discussion. To build a global model, we put data from
all the studied projects into one dataset.
The goal of the global model is to uncover the generalization of our observations. How-
ever, due to the varying sizes of the studied projects, we took random samples of equal size
from each project. The size of the random sample from each project is equal to the size
of smallest dataset (3,099 issues for Bugzilla). Such an approach to create our dataset for
the global model ensures that no data from one specific project can over take the global
trends across the studied projects. The accuracies of the two global models are close to the
project-specific models (see Table 9).
Table 10 presents the results of the Scott-Knott test for the studied factors of the
global models. Similar to the project-specific models, the Total Duplicates Identified and
Description Similarity factors rank as the most impacting factors. However, the Priority and
Reporting Recency of Peers factors that are important in the individual models of Bugzilla
and Eclipse-Platform (see Section 4) do not rank as one of the top important factors in the
global model. If we only built a global model, we would not observe the importance of
the Priority and Reporting Recency of Peers. Such results highlight the value of building
project specific models in order to uncover the different ways for handling duplicate issue
reports among the studied projects. Our global model highlights the general trends across
the projects.
Empir Software Eng
Table 9 Evaluation results for the global models
Global Model Class Precision Recall F-Score ROC area
Identification Discussion Not-Discussed 0.67 0.80 0.73 0.69
Discussed 0.60 0.43 0.50
Identification Delay Fast 0.73 0.85 0.79 0.75
Slow 0.67 0.49 0.57
6 Threats to Validity
In this section, we discuss the threats to the validity of our conclusions.
External Validity Our study is conducted on four large open source projects. While three
of the projects (i.e., Firefox, SeaMonkey, Bugzilla) are incubated by the Mozilla founda-
tions, these projects share only 9 % of the top developers (the ones who identify over 80 %
of duplicate reports in total). Our earlier findings (see Section 4 ) highlight as well that each
one of these projects follows a different process for managing their issue reports. As usual
additional case studies are always desirable. However for the purpose of this paper we do
believe that highlighting our raised observations even in a single project is sufficient to raise
concerns about future research for duplicate identification.
Table 10 Scott-Knott test results when comparing the importance per factor for the global models. Factors
are divided into groups that have a statistically significant difference in the importance mean (p < 0.05)
Identification discussion model Identification delay model
Group Factors Mean Group Factors Mean
1 Total duplicates identified 0.0142 1 Reporting recency of peers 0.0301
2 Description similarity 0.0134 2 Recency of peer identification 0.0267
Priority 3 Total duplicates identified 0.0253
Reporting recency of peers 4 Priority 0.0118
3 Recency of peer identification 0.0092 Peers CC list
Title similarity 5 Description similarity 0.0112
4 Peers CC list 0.0066 Waiting load
is Dependable Title similarity 0.0061
5 Severity 0.0048 6 Fixed issues per identifier
is Blocking 7 is Dependable 0.0053
Waiting load is Blocking
Fixed issues per identifier 8 Duplicate peers identified 0.0040
6 Description size 0.0031 Description size
Duplicate peers identified 9 Severity 0.0023
Reproduce steps 10 Reproduce steps 0.0016
Title size 0.0003 Title size
7 is Weekend 11 is Weekend 0.0006
Empir Software Eng
In particular, the purpose of our study is not to establish a general truth about all dupli-
cate reports across all projects of the world. Instead the ultimate goal of our study is to raise
awareness that the amount of the needed effort spent on identifying duplicates varies con-
siderably and that (at least for our studied projects) the duplicate identification for a large
proportion of reports appears to be a trivial task.
In this study, we examined projects that used the Bugzilla issue tracking system. There
exist several other issue tracking systems. Each issue tracking system may introduce addi-
tional confounding factors that would impact our observations. To avoid the bias from such
confounding factors, we choose to use the same issue tracking system. Replicating our study
using projects that use other issue tracking system may complement our results. Future stud-
ies might wish to explore whether our observations would generalize to other open source
and commercial software projects. Nevertheless, our observations highlight the need for
future duplicate identification research to factor in the identification effort when proposing
new approaches and when designing evaluation measures, otherwise evaluations are likely
not to reflect the true value of newly proposed approaches and the practical impact of such
approaches will remain unexplored.
Internal Validity The internal threat considers drawing conclusions from the used factors.
Our study of modeling the needed effort for identifying duplicate issue reports cannot claim
causal relations, as we are investigating correlations, rather than conducting impact studies.
The important factors for the needed effort for identifying duplicate issue reports do not
indicate that the factors lead to low/high efforts. Instead, our study indicates the possibility
of a relation that should be studied in depth through user studies.
One of the other possible ways to measure discussion effort is to consider the amount
of time associated with each comment instead of the number of comments. However, such
estimation is not a straightforward one. Hence, we opted for a simple measure of effort.
Future studies should explore other measures of effort instead of our current basic measure
of comment counting.
We choose one day as a threshold to determine rapidly identified reports, since we want
to ensure globally distributed developers all have a chance to see the issue report. Using
more granular time periods (e.g., minutes) to measure the identification delay may introduce
more noise into the data because there might be cases that a developer is too busy with other
activities to immediately look into a newly filed issue report.
We used the time to mark a duplicate report as the identification delay. However, this
time is just an upper bound approximation for the time that the developer spent on each
duplicate report. Hence, the identification delay might be trivial for an even larger number of
issues than what we report. To achieve a better estimation of the needed time for identifying
duplicate reports, we consider the time of issue triaging as the start time for looking at
each duplicate report. The different processes of triaging may impact our measurement of
identification effort. However, we address this threat by selecting the latest triage date just
before identifying a duplicate (i.e., some issues are triaged multiple times). We measure
the identification effort by counting the number of discussion comments on an issue report.
However, some discussions may be from non-developers. Unfortunately, in our study we do
not have the data to determine whether a commenter is developer or not.
Construct Validity We measure effort using three metrics: Identification Delay, Identi-
fication Discussions and People Involved. However, these metrics may not fully measure
Empir Software Eng
the effort that is needed for identifying duplicate reports. In addition, there might be other
ways to measure the needed efforts for identifying duplicate report. We plan to explore more
metrics to measure such effort in future case studies.
7 Conclusion
A large portion of the issue reports are duplicates. Various automated approaches are pro-
posed for identifying such duplicate reports in order to save developers’ efforts. However,
there exists no research that studies the effort that is actually needed for identifying dupli-
cate issue reports. In this paper, we examined such effort using issue reports from four open
source projects, i.e., Firefox, SeaMonkey, Bugzilla, and Eclipse-Platform. We find that:
1. More than half of the duplicate reports are identified within one day, with no discussion
and without the involvement of any people (other than the reporter); nevertheless there
exists a small number of reports that are identified as duplicates after a long time, with
many discussions and with the involvement of as much as 80 developers.
2. The developer experience and the knowledge about duplicate peers of an issue report,
such as description similarity, play an important role in the effort that is needed for
identifying duplicates
Our study results show that in many cases, developers require minimal effort to identify
that an issue report is a duplicate of an existing one; while there exists some cases where
such identification is a difficult and effort-consuming task. However, nowadays automated
approaches for identifying duplicate reports treat all issues the same.
Moreover, our results show that the strong reliance on textual similarity by state of
the art duplicate identification approaches will likely lead to the identification of dupli-
cates that developers are already able to identify with minimal effort. While, the duplicate
reports that need minimal effort are still worthy to be handled by current automatic iden-
tification approaches; our results highlight the need for duplicate identification approaches
to put additional emphasis on the issue reports that are more difficult to identify as a
duplicate.
References
Aggarwal K, Rutgers T, Timbers F, Hindle A, Greiner R, Stroulia E (2015) Detecting duplicate bug reports
with software engineering domain knowledge. In: SANER 2015: International conference on software
analysis, evolution and reengineering. IEEE, pp 211–220
Alipour A, Hindle A, Stroulia E (2013) A contextual approach towards more accurate duplicate bug report
detection. In: MSR 2013: Proceedings of the 10th working conference on mining software repositories,
pp 183–192
Angrist JD, Pischke JS (2008) Mostly harmless econometrics: An empiricist’s companion. Princeton
university press, Princeton
Anvik J, Hiew L, Murphy GC (2005) Coping with an open bug repository. In: Eclipse 2005: Proceedings of
the 2005 OOPSLA Workshop on Eclipse Technology eXchange. ACM, pp 35–39
Anvik J, Hiew L, Murphy GC (2006) Who should fix this bug? In: ICSE 2006: Proceedings of the 28th
international conference on software engineering. ACM, pp 361–370
Bertram D, Voida A, Greenberg S, Walker R (2010) Communication, collaboration, and bugs: The social
nature of issue tracking in small, collocated teams. In: CSCW 2010: Proceedings of the ACM conference
on computer supported cooperative work. ACM, pp 291–300
Empir Software Eng
Bettenburg N, Just S, Schröter A, Weiss C, Premraj R, Zimmermann T (2007) Quality of bug reports
in eclipse. In: Eclipse 2007: Proceedings of the 2007 OOPSLA Workshop on Eclipse Technology
eXchange. ACM, New York, pp 21–25
Bettenburg N, Just S, Schröter A, Weiss C, Premraj R, Zimmermann T (2008a) What makes a good bug
report? In: SIGSOFT ’08/FSE-16: Proceedings of the 16th ACM SIGSOFT international symposium on
foundations of software engineering. ACM, New York, pp 308–318
Bettenburg N, Premraj R, Zimmermann T, Kim S (2008b) Duplicate bug reports considered harmful really?
In: ICSM 2008: Proceedings of the IEEE international conference on software maintenance. IEEE,
pp 337–345
Blei DM, Ng AY, Jordan MI (2003) Latent dirichlet allocation. J Mach Learn Res 3:993–1022
Breiman L (2001) Random forests. Mach Learn 45(1):5–32
Cavalcanti YC, Da Mota Silveira Neto PA, de Almeida ES, Lucrédio D, da Cunha CEA, de Lemos Meira
SR (2010) One step more to understand the bug report duplication problem. In: SBES 2010: Brazilian
symposium on software engineering. IEEE, pp 148–157
Cavalcanti YC, Neto PAdMS, Lucrédio D, Vale T, de Almeida ES, de Lemos Meira SR (2013) The bug
report duplication problem: an exploratory study. Softw Qual J 21(1):39–66
Chavent M, Kuentz V, Liquet B, Saracco J (2015) Variable Clustering. http://svitsrv25.epfl.ch/R-doc/library/
Hmisc/html/varclus.html
Davidson JL, Mohan N, Jensen C (2011) Coping with duplicate bug reports in free/open source software
projects. In: VL/HCC 2011: IEEE symposium on visual languages and Human-Centric computing.
IEEE, pp 101–108
Deerwester SC, Dumais ST, Landauer TK, Furnas GW, Harshman RA (1990) Indexing by latent semantic
analysis. JAsIs 41(6):391–407
Ghotra B, McIntosh S, Hassan AE (2015) Revisiting the impact of classification techniques on the perfor-
mance of defect prediction models. In: ICSE 2015: Proceedings of the 37th international conference on
software engineering
Jalbert N, Weimer W (2008) Automated duplicate detection for bug tracking systems. In: DSN 2008: Pro-
ceedings of the IEEE international conference on dependable systems and networks with FTCS and
DCC. IEEE, pp 52–61
Jiang Y, Cukic B, Menzies T (2008) Can data transformation help in the detection of fault-prone modules?
In: Proceedings of the 2008 workshop on Defects in large software systems. ACM, pp 16–20
Kamei Y, Matsumoto S, Monden A, Matsumoto Ki, Adams B, Hassan AE (2010) Revisiting common
bug prediction findings using effort-aware models. In: ICSM 2010: IEEE international conference on
software maintenance. IEEE, pp 1–10
Kampstra P et al. (2008) Beanplot: A boxplot alternative for visual comparison of distributions
Kanaris I, Kanaris K, Houvardas I, Stamatatos E (2007) Words versus character n-grams for anti-spam
filtering. Int J Artif Intell Tools 16(06):1047–1067
Kanerva P, Kristofersson J, Holst A (2000) Random indexing of text samples for latent semantic anal-
ysis. In: Proceedings of the 22nd annual conference of the cognitive science society, vol 1036.
Citeseer
Kaushik N, Tahvildari L (2012) A comparative study of the performance of ir models on duplicate bug
detection. In: CSMR 2012: Proceedings of the 16th European conference on software maintenance and
reengineering. IEEE Computer Society, pp 159–168
Koponen T (2006) Life cycle of defects in open source software projects. In: Open Source Systems. Springer,
pp 195–200
Lazar A, Ritchey S, Sharif B (2014) Improving the accuracy of duplicate bug report detection using textual
similarity measures. In: MSR 2014: Proceedings of the 11th working conference on mining software
repositories. ACM, pp 308–311
Lerch J, Mezini M (2013) Finding duplicates of your yet unwritten bug report. In: CSMR 2013: 17th
European conference on software maintenance and reengineering. IEEE, pp 69–78
Lessmann S, Baesens B, Mues C, Pietsch S (2008) Benchmarking classification models for software
defect prediction: A proposed framework and novel findings. IEEE Trans Softw Eng 34(4):485–
496
Liaw A, Wiener M (2014) Random Forest R package. http://cran.r-project.org/web/packages/randomForest/
randomForest.pdf
McIntosh S, Kamei Y, Adams B, Hassan AE (2015) An empirical study of the impact of modern code review
practices on software quality. Empirical Software Engineering 1–44
Mitchell MW (2011) Bias of the random forest out-of-bag (oob) error for certain input parameters. Open J
Stat 1(03):205
Mockus A, Weiss DM (2000) Predicting risk of software changes. Bell Labs Tech J 5(2):169–180
Empir Software Eng
Nagwani NK, Singh P (2009) Weight similarity measurement model based, object oriented approach for bug
databases mining to detect similar and duplicate bugs. In: ICAC 2009: Proceedings of the international
conference on advances in computing, communication and control. ACM, pp 202–207
Neter J, Kutner MH, Nachtsheim CJ, Wasserman W (1996) Applied linear statistical models, vol. 4. Irwin
Chicago
Prifti T, Banerjee S, Cukic B (2011) Detecting bug duplicate reports through local references. In: PROMISE
2011: Proceedings of the 7th international conference on predictive models in software engineering.
ACM, pp 8:1–8:9
Robertson S, Zaragoza H, Taylor M (2004) Simple bm25 extension to multiple weighted fields. In: CIKM
2004: Proceedings of the Thirteenth ACM international conference on information and knowledge
management. ACM, pp 42–49
Robnik-Ṡikonja M (2004) Improving random forests. In: Machine Learning: ECML 2004. Springer, pp 359–
370
Runeson P, Alexandersson M, Nyholm O (2007) Detection of duplicate defect reports using natural language
processing. In: ICSE 2007: Proceedings of the 29th international conference on software engineering.
IEEE Computer Society, pp 499–510
Scott A, Knott M (1974) A cluster analysis method for grouping means in the analysis of variance. Biometrics
507–512
Sun C, Lo D, Khoo SC, Jiang J (2011) Towards more accurate retrieval of duplicate bug reports. In: ASE
2011: Proceedings of the 26th IEEE/ACM international conference on automated software engineering.
IEEE, pp 253–262
Sun C, Lo D, Wang X, Jiang J, Khoo SC (2010) A discriminative model approach for accurate duplicate
bug report retrieval. In: ICSE 2010: Proceedings of the 32Nd ACM/IEEE international conference on
software engineering. ACM, pp 45–54
Sureka A, Jalote P (2010) Detecting duplicate bug report using character n-gram-based features. In: APSEC
2010: Proceedings of the Asia Pacific software engineering conference. IEEE Computer Society,
pp 366–374
Tantithamthavorn C, McIntosh S, Hassan AE, Ihara A, Matsumoto Ki, Ghotra B, Kamei Y, Adams B,
Morales R, Khomh F, et al. (2015) The impact of mislabelling on the performance and interpretation of
defect prediction models. In: ICSE 2015: Proceedings of the 37th international conference on software
engineering
Wang X, Zhang L, Xie T, Anvik J, Sun J (2008) An approach to detecting duplicate bug reports using natural
language and execution information. In: ICSE 2008: Proceedings of the 30th international conference
on software engineering. ACM, pp 461–470
Xavier R, Turck N, Hainard A, Tiberti N, Lisacek F, Sanchez J-C, Müller M (2015) pROC R package.
http://cran.r-project.org/web/packages/pROC/pROC.pdf
Mohamed Sami Rakha received his B.Sc. and M.Sc. degrees in computer science from Cairo University
(Egypt). He previously worked as a Senior Enterprise Java Developer at Fawry Corporation, Egypt. Currently,
he is a PhD student at the School of Computing, Queens University, Canada, since September 2013. His
research interests include Mining Software Repositories, Machine Learning and Image Processing. More
information at: http://mohamedsamirakha.info/.
Empir Software Eng
Weiyi Shang is an Assistant Professor at Concordia University (Canada). He received his Ph.D. and M.Sc.
degrees from Queens University (Canada) and he obtained a B.Eng. from Harbin Institute of Technology. His
research interests include big data software engineering, software engineering for ultra-large-scale systems,
software log mining, empirical software engineering, and software performance engineering. His work has
been published at premier venues such as ICSE, ASE, ICSME, MSR and WCRE, as well as in major journals
such as EMSE, JSS, JSEP and SCP. His industrial experience includes helping improve the quality and
performance of ultra-large-scale systems at BlackBerry. Early tools and techniques developed by him are
already integrated into products impacting millions of users worldwide. More information at: http://users.
encs.concordia.ca/∼shang/.
Ahmed E. Hassan is a Canada Research Chair in Software Analytics and the NSERC/Blackberry Industrial
Research Chair at the School of Computing in Queen’s University. Dr. Hassan serves on the editorial board
of the IEEE Transactions on Software Engineering, the Journal of Empirical Software Engineering, and
PeerJ Computer Science. He spearheaded the organization and creation of the Mining Software Repositories
(MSR) conference and its research community.
Early tools and techniques developed by Dr. Hassan’s team are already integrated into products used by
millions of users worldwide. Dr. Hassan’s industrial experience includes helping architect the Blackberry
wireless platform, and working for IBM Research at the Almaden Research Lab and the Computer Research
Lab at Nortel Networks. Dr. Hassan is the named inventor of patents at several jurisdictions around the world
including the United States, Europe, India, Canada, and Japan. More information at: http://sail.cs.queensu.ca/.
