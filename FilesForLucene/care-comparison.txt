 
 
Authorship Attribution: a Comparison of Three Methods 
 
 
 
 
 
 
Matthew Care 
MSc in Software Systems Technology 
 
 
 
 
Supervised by Dr Mirella Lapata 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
This report is submitted in partial fulfilment of the requirement for the 
degree of MSc in Software Systems Technology 
 
 
 
26.8.03
 
Declaration 
 
All sentences or passages quoted in this dissertation from other people's work have 
been specifically acknowledged by clear cross-referencing to author, work and page(s). 
Any illustrations which are not the work of the author of this dissertation have been 
used with the explicit permission of the originator and are specifically acknowledged. I 
understand that failure to do this amounts to plagiarism and will be considered grounds 
for failure in this dissertation and the degree examination as a whole.  
 
Name:  
Signature:  
Date:  
 
 
 
 2
Abstract 
 
Authorship attribution attempts to assign the correct author to a given text.  There is a 
large range of techniques available for authorship attribution but all share certain 
characteristics.  They must all undergo a process of training using texts by all of the 
possible authors of the test document.  The training process is used to create a list of 
characteristics from which the author can hopefully be separated.  These characteristics 
(e.g. word frequencies, sentence length etc) are used subconsciously by the author and 
are assumed to be a consistent marker for that authors work.  There must also be 
enough differences in these characteristics to separate a group of authors.    
 
This project compared three authorship attribution methods; cusum, n-gram and 
zipping.  These methods were chosen as they vary greatly in their underlying 
principals.  The cusum technique works at the word/sentence level, the n-gram at the 
character level and the zipping at a more abstract level. This allowed a cross-section of 
the existing authorship attribution techniques to be compared side by side. 
 
Programs were written for the n-gram and cusum techniques, while for the zipping 
technique two existing compression programs were tested. 
 
The methods were analysed using the same corpus as from the n-gram study carried 
out by Peng et al (2003).  The highest overall accuracy’s obtained by the cusum, n-
gram and zipping techniques were 59%, 91% and 100% respectively.     
 
 
 
   
  
 
 
 
 3
Acknowledgements 
 
I would like to thank Dr Mirella Lapata for help throughout the project. 
 
I would also like to thank my parents for providing support and putting up with a 
slightly stressed person. 
 
 
 
 
 
 
 
 4
Contents 
 
1 INTRODUCTION ................................................................................................................................ 6 
2 LITERATURE SURVEY .................................................................................................................... 8 
2.1 OVERVIEW OF AUTHORSHIP ATTRIBUTION ...................................................................................... 8 
2.2 LEXICALLY/SYNTACTICALLY BASED TECHNIQUES .......................................................................... 8 
2.2.1 Principal Component Analysis ................................................................................................ 8 
2.2.2 Cusum Analysis ..................................................................................................................... 10 
2.3 NON-LEXICAL/SYNTACTICAL TECHNIQUES ................................................................................... 12 
2.3.1 N-grams.................................................................................................................................. 12 
2.3.2 Zipping Technique................................................................................................................. 17 
3 REQUIREMENTS ............................................................................................................................. 21 
3.1 PROJECT AIM.................................................................................................................................. 21 
3.2 CORPUS PROCESSING ..................................................................................................................... 21 
3.3 EXPERIMENTS................................................................................................................................. 23 
4 IMPLEMENTATION AND TESTING............................................................................................ 24 
4.1 ZIPPING TECHNIQUE CODING ......................................................................................................... 24 
4.2 N-GRAM CODING............................................................................................................................ 24 
4.3 CUSUM CODING ............................................................................................................................. 25 
4.4 CODE TESTING ............................................................................................................................... 26 
5 EXPERIMENTS AND RESULTS .................................................................................................... 27 
5.1 NGRAM EXPERIMENTS ................................................................................................................... 27 
5.1.2 Authorship Attribution .......................................................................................................... 27 
5.1.3 Influence of Training Size..................................................................................................... 28 
5.1.4 Influence of Context Length ................................................................................................. 30 
5.1.5 Influence of Vocabulary ........................................................................................................ 32 
5.2 CUSUM EXPERIMENTS .................................................................................................................... 33 
5.2.1 Decision Trees and The J48 Algorithm ................................................................................ 34 
5.2.2 Weka Experiments ................................................................................................................. 34 
5.3 ZIPPING EXPERIMENTS ................................................................................................................... 37 
5.4 DISCUSSION OF RESULTS................................................................................................................ 40 
6 CONCLUSIONS................................................................................................................................. 42 
REFERENCES ...................................................................................................................................... 44 
 
 
 
 
 
 
 
 
 
 
 
 
 5
 
 
1 Introduction 
 
Authorship attribution deals with the process of assigning an author to a particular 
piece of text.  The methods used for authorship attribution have to be able to 
differentiate texts from a group of authors.  This differentiation requires the detection 
of certain characteristics that are author dependent.  Ideal characteristics must be used 
subconsciously by the author and be a persistent marker for that author across all of 
their work.  Clearly authors can write in many styles and finding markers that remain 
reliable across this range is the key to authorship attribution.  These characteristics 
range from the frequencies of words, habits such as 2 or 3 length words down to the 
frequency of n-grams.   
 
At the current time there are many techniques that aim to solve authorship disputes but 
none of these are universally applicable.  This research field also has more than its fair 
share of controversy, with strong disagreements about the validity of some techniques.  
A lack of consensus has lead to a research field for which there are no fixed standards 
by which to compare new studies. This has caused authorship attribution to be 
perceived as being too fallible for use in court proceedings (Rudman, 1998). 
 
 
The aim of this project is to compare and contrast three authorship attribution methods  
 
• Cusum 
• N-gram 
• Zipping 
 
The cusum technique looks at the frequencies of a range of possible habits.  This 
normally involves counting the sentence lengths for a given document.  The 
cumulative sum (running total) of the each sentences deviation from the mean is 
plotted on a graph.  Then for each sentence certain habits are counted, this involves a 
count of the 2 length, 3 length and initial vowel words (words starting with a vowel).  
These are added together to produce a total habit count, the cumulative sum of the total 
habit counts deviation from the mean is then plotted on a graph.  Traditionally the 
sentence length and total habit graphs were plotted onto overheads and then aligned to 
produce the best fit.  The resulting graph theoretically allows the visualisation of 
mixed authorship within a single document.  This is represented by a divergence 
between the two plotted lines, showing a change in style.  For the purposes of this 
report these graphs are deemed to be too subjective and therefore the habits produced 
by the cusum technique were used with a machine learning technique. 
 
The n-gram technique used in this report works by counting the occurrence of different 
levels of n-gram (1-6).  An example of a 2-gram, known as a bigram, would be ‘ab’.  
From the frequencies of all of the n-grams it is possible to calculate the maximum 
likelihood estimate (MLE) of each n-gram, an estimate of the probability.  At the 
bigram level the probability assigned to ‘ab’ would be interpreted as ‘the probability of 
getting ‘b’ given that we have encountered ‘a’.  Thus for a given author an n-gram 
model is produced, by counting all of the different bigrams, trigrams etc and then 
 6
calculating their probabilities.  These n-gram probabilities are then the profile for that 
particular author.  For each possible author of an unassigned text an n-gram model is 
produced.  The probability of the unassigned text is then calculated for each authors n-
gram model, it is then assigned to the author whose n-gram model produces the highest 
probability.   
 
The zipping technique uses a compression program to judge the similarity between 
pairs of data sequence.  The compression program used must implement the Lempel 
and Ziv compression algorithm.  This algorithm finds duplicate strings within a given 
text.  It finds the longest match possible from within a sliding window.  It then encodes 
the most frequent sequences with the fewest bits, encoding the rarer sequences with 
bytes.  Thus as it progresses through a document it becomes more efficient.  To assign 
an author to a particular text, the unassigned text is appended to a text by each possible 
author.  All of these files are compressed as well as the original files by each author.  
The difference between each compressed composite file and the compressed author file 
is calculated.  The author for whom the smallest difference is produced is deemed to 
have written the text.  This relies on the algorithm becoming efficient for the sample of 
authors’ work.  If a different authors work has been appended then the algorithm can 
not encode this authors work efficiently, therefore producing a larger file. 
 
All three authorship methods described were tested using a single corpus, which had 
previously been used by Peng et al (2003) for a series of n-gram experiments. 
 
 
Chapter 2 gives an overview of a variety of authorship attribution methods describing 
the current literature including the criticism of some of the techniques.  It also covers 
the methods mentioned here in more depth.  Chapter 3 describes the requirements that 
were needed for the project.  Chapter 4 covers the coding element of the project 
discussing the classes produced and their main functions.  It then discusses the testing 
procedure that was undertaken.  Chapter 5 shows the experimental results, and 
discusses their meaning.  Chapter 6 covers the conclusions of the project. 
     
 
 
 
 
 
 
 
 
 
 
 
 
 
 7
 
2 Literature Survey 
 
2.1 Overview of Authorship Attribution 
 
There are many techniques in the area of authorship attribution.  The traditional 
methods of deciding authorship were based on an understanding of the work in 
question.  Evidence of the authorship came either from the style of the studied work or 
external evidence such as correspondence or testimonials (Holmes et al, 2001). 
 
The advent of non-traditional authorship attribution techniques can be traced back to 
1887, when Mendenhall first created the idea of counting features such as word length.  
His work was followed by work from Yule (1938) and Morton(1965) with the use of 
sentence lengths to judge authorship (Holmes,1997).  These works were the beginning 
of an area known as stylometry, which aims to find some underlying ‘fingerprint’ for 
an authors style.  Stylometry relies upon the hypothesis that when an author writes 
they use certain words unconsciously, the frequencies of the usage of these words can 
be used to ascertain authorship.  Burrows (1987) formed the idea of counting the most 
commonly occurring function words (e.g. the, is, all etc.) as these were felt to be the 
most unconsciously used words.  This idea has been used for a variety of authorship 
attribution studies and is felt by many to be a robust technique (Holmes, 1997). 
 
With the advent of computing, stylometry became more fashionable as the slow 
process of hand-counting words could be automated.  This has allowed studies to count 
as many frequently occurring words as required.  Computing has also allowed the 
process of authorship attribution to become far more abstract, leading to techniques 
that are non-lexical in nature.  Thus non-traditional authorship has split into two broad 
categories, methods which use lexical/syntactical information and those that do not. 
 
2.2 Lexically/Syntactically Based Techniques  
 
These techniques judge authorship using some lexical or syntactically based measure 
and will be discussed below. 
 
 
2.2.1 Principal Component Analysis 
 
There are many techniques that fit under the category of PCA (principal component 
analysis).  They all descend from the idea of commonly occurring function words that 
was developed by Burrows (1987).  In order to ascertain authorship the frequecies of 
50 or more commonly occurring function words are calculated for a ‘training’ set of 
texts.  This set of texts includes work by all of the possible authors of the work in 
question.  It is now normal for the words to ‘self select’, meaning that the most 
commonly occurring function words are used in preference to a predetermined list. 
 8
The next step is to calculate the frequencies of the same words in a separate piece of 
work for each author and the work in question.  The frequencies of the tested words 
are used for cluster analysis, which should group works by author. 
 
PCA has been used ‘successfully’ for many studies with minor variations in the 
protocol.  Burrows studies of Jane Austin (Burrows, 1987,1989,1992; Burrows and 
Hassall 1998 and Burrows and Craig, 1994).  For the studies of the authorship of the 
Book of Mormon (Holmes, 1992), the Federalist Papers (Holmes and Forsyth, 1995), 
the authorship of De Doctrina Christiana (Holmes 1994, Baayen et al. 1996, Tweedie 
et al. 1998 and Craig. 1999)  [see Hoover, 2001,for details of references] and finally 
the work of Steven Crane (Holmes et al 2001). 
 
Although it has had relative success PCA is not without problems.  David L. Hoover 
analysed PCA and found that it rarely exceeds an accuracy rate above 90% (Hoover, 
2001).  His study was in two parts; the initial stage looked at 3000 word sections from 
50 contemporary novels.  The second stage, which examined the effects of larger texts, 
looked at 30,000 word sections from 46 modern novels.   
  For testing 3000 word sections for 27 authors he initially used the 50 most commonly 
occurring function words and gained a success rate of only ~25%.  When using the 100 
most commonly occurring function words the success rate dropped to ~22%.  Using 
the 200 most frequent function words increases the success rate to ~45%, but this is 
only gained by a ‘more liberal’ interpretation of results (where he has counted clusters 
within cluster as correct).  He then examined the usage of words other than commonly 
occurring function words.  With the 400 most commonly occurring content words he 
found that the success rate was less than that for the 200 most commonly occurring 
function words.  But when using the 500 most commonly occurring words of all types 
he gained a accuracy of ~%50, the highest so far.  Lowering the number of authors 
tested, and also the number of texts for which the frequencies are calculated, to 10 
resulted in a accuracy of between 60-70%. 
 
 The second stage of testing involved looking at 46 30,000 word novels from 31 
authors, 20 of which had only a single representative novel.  For the 50, 100 and 200 
most frequent function words the success rate was ~50%.  The 500 most frequent 
words of all kinds produced a success rate of ~70%. 
Hoover then separated the narrative and dialogue in 29 novels to see if this would 
effect the results.  He found that the highest accuracy for the narrative-only sections 
was 59%, lower than the mixed texts.  Testing only third-person narratives produced 
the best results so far with an accuracy of 87.5% (with the 400 most frequent words of 
all kinds). 
Hoover also tested the effect of disambiguation (parsing to remove word ambiguities, 
such as words that can be used as verbs or nouns) of the function words and expansion 
of contractions.  The highest accuracy gained was 87.5%, not increasing the best 
accuracy gained without the extra processing. 
Hoover finally carried out similar tests to those mentioned above on non-fictional 
works.  The only difference here was that the process of disambiguation improved the 
accuracy, but as Hoover points out ‘not to a degree that seems proportional to the 
tremendous amount of tedious labour required’.   
 
Thus Hoovers results show that PCA is rarely accurate enough to conclusively state 
authorship of a piece of text.  It is most accurate when the field of authors to be 
 9
compared is relatively small.  Furthermore there is no linear increase of accuracy when 
the sample of frequently occurring words is increased.  In fact the optimum number of 
frequently occurring word seems to change for every individual case.  The conditions 
have to be changed quite dramatically to get the optimum results for the know authors.  
So when the field includes work by an anonymous author it is not obvious how to 
produce the correct conditions.  Furthermore getting the known authors to cluster 
correctly may produce false positives.  This does not mean that PCA has no use but it 
is probably limited to fairly closed cases when the question of authorship lies between 
2 authors. 
 
Finally it should be said that the PCA technique is constantly being modified, with 
newer methods increasing the overall accuracy.  A new method by John Burrows 
(Burrows, 2002), which will be mentioned briefly here, shows promising initial results.  
The method called the Delta procedure aims to calculate the ‘pure difference’ between 
two texts.  To calculate the ‘pure difference’, or delta, the following procedure is 
followed.  As with the PCA described above, the frequencies of the most commonly 
occurring function words are calculated.  Both for a main-set of texts written by all of 
the authors in question and for the texts which are to be compared.  The standard 
deviations for all of the words are calculated for the main-set.  Then the z-score is 
calculated for each word for every piece of text.  The z-score shows how many 
standard deviations from the mean a value is and is calculated thus using Equation 2.1. 
 
σ
µ−
=
XZ  (Eq. 2.1) 
 
 Where ‘X’ is the frequency of a words occurrence for the tested text, µ is the mean 
frequency of that word in the main-set and σ is the standard deviation of that word in 
the main-set.  The difference between texts by every author and the work in question is 
calculated.  This is the difference between the z-scores for each word in the tested 
article and each authors work.  Absolute values for these differences are calculated and 
the mean for all the words is produced.  This value is the Delta score and is a measure 
of the difference of each text in comparison to that being tested. 
  The results of the Delta method seem quite promising with a high accuracy (~90%) 
even when comparing a number of authors. 
 
2.2.2 Cusum Analysis 
 
The cusum technique was first developed in 1988 by Andrew Q. Morton.  Like PCA it 
is based on the principals of stylometry but it differs in many respects.  It measures 
several components of the language.  The first component studied is sentence length.  
The average sentence length for a whole document is calculated (counting names as a 
single unit irrespective of length).  Then each sentence’s deviation from the mean is 
calculated.  Finally the cumulative (running) total is calculated for each sentences 
deviation from the mean (thus will add to 0).  These values are then plotted on a graph 
to show how a sample deviates from the mean, an example is shown in Fig 2.1. 
 
 
 
 10
Cusum Graph
-15
-10
-5
0
5
10
1 2 3 4 5 6 7 8 9 10
Sentence Number
(A Graph of cumulative sum for sentence-length for a ten sentence sample)
C
um
ul
at
iv
e 
Su
m
Fig 2.1 
 
The graph of sentence deviation is of little use on its own, in order to gain any useful 
information about an author other stylistic features need to be studied.  There are nine 
other features that can be analysed in order to ascertain authorship.  The most 
commonly used are 2 and 3 letter words (23lw), initial vowel words (ivw) and a 
combination of both (23lw + ivw).  For each sentence the occurrence of these habits is 
calculated.  In order to combine the information, as in the case of 23lw + ivw, the sum 
of all of the habits is calculated for all of the sentences and an average is produced.  
Then for each sentence the deviation of the total habits from the mean is calculated.  
Finally a cumulative total is calculated per sentence for the habits, and this is plotted 
on a graph along with the sentence length deviation.   Normally one of the two lines is 
scaled so that the lines are closer together.  An illustration of the graph is shown 
below. 
Cusum Graph
-15
-10
-5
0
5
10
15
1 2 3 4 5 6 7 8 9 10
Sentence Number
(A graph of cumulative sum for sentence-length and 23lw habit )
C
um
ul
at
iv
e 
Su
m
Cusum Sentence
Cusum 23lw
Fig 2.2 
 11
 
If both of the lines deviate at the same speed (i.e. stay the same distance apart) then 
this is an indicator of a single author.  Thus in order to test authorship the text in 
question can be placed in texts by known authors.  Then simply finding a set of graphs 
for which there is little deviation between the two lines will suggest an author 
(Fraringdon).   
 
  The cusum technique has suffered from much criticism (Barr 1998,Hilton and 
Holmes 1993).  One major criticism of the cusum technique is the subjective nature of 
graph interpretation.  At what point are two lines separated sufficiently to suggest two 
authors?  This in its self is not necessarily a problem but when combined with the 
process of scaling can produce graphs that are hard to accurately interpret.  Farringdon 
suggests that graphs should have a height that is between a quarter and a third of the 
length of the graphs base.  The graphs are then vertically scaled using the cumulative 
sums of the two plotted variables.  This forces the two graphs into a rough fit, partially 
destroying any useful information that they may contain.  The scaling process can also 
change the apparent location of inserted text, shifting the separation of the two lines 
along the x-axis.  This is not a problem for judging authorship but it does produce 
confusing results when separations appear at non expected locations. 
  Furthermore concatenating works by a single author has been shown to sometimes 
produce results that would imply two authors (Barr, 1998).  This demonstrates that the 
habits used to judge authorship are not reliable for a single author yet alone comparing 
multiple authors.  Again this is not necessarily a failure of the method, but more a 
failure of the belief that an author has a unique cusum fingerprint.  The belief that an 
author’s style is consistent throughout their life and across their work appears flawed.  
The work by Holmes et al looking at the works of Stephen Crane (Holmes 2001) 
showed just how much a single author’s style can vary across genre. 
  
Another issue with the cusum technique is that location of an inserted passage alters 
the result produced.  The cumulative nature of the method means that habit change can 
only really be detected if it occurs within the first half of a document.  Again this is 
irrelevant for authorship attribution studies as tested passages can be placed near the 
beginning of a document. 
 
2.3 Non-Lexical/Syntactical Techniques 
 
 
2.3.1 N-grams 
 
(Jurafsky and Martin, 2000) 
 
These can operate at the word or character level, thus as a group could be divided into 
lexical and non-lexical in nature.  The concepts will be explained using word n-grams 
but the principals are the same for character level n-grams. 
 
n-grams are used in language modelling, they have been used in a variety of 
applications such as speech recognition, context sensitive spelling correction, optical 
character recognition etc.  n-grams show the probability of a word occurring given 
 12
what has gone before it.  This allows the probability of whole sentence to be 
calculated, which is useful for applications were context plays a role. 
 
 
Bi-grams/Markov Chain 
 
The bigram model shows the probability of a word given the single word before it. 
 
E.g.  is the probability of want given the existence of the word i.  )|( Iwantp
 
The bigram is also known as a first-order Markov model.  This is based on the Markov 
assumption that suggests that a word depends only on the previous word.  The bigram 
can be extended to a trigram, quadrigram etc all the way to n-gram (which looks N-1 
words into the past).  These are called second-order, third-order and N-1 order Markov 
models respectively. 
 
The general equation for an n-gram is 
 
    (Eq. 2.2) )|()|( 1 1
1
1
−
+−
− ≈ n Nnn
n
n wwpwwp
 
which shows that the probability of wn (word n) given all the previous words can be 
approximately calculated given only the previous N words. 
  
 
  To calculate the probabilities needed for the model, words need to be counted. In the 
case of bigrams, every occurrence of each bigram is counted.  The relative frequency 
for the bigrams is then calculated by dividing these counts by the sum of all the 
bigrams that have the same first word.  This will result in a list of all of the seen 
bigrams with probabilities associated with each. 
 
Thus for bigrams 
 
)1(
1 )(
1)|(
−
−=−
nw
nn
C
C
wwp wwnn   (Eq. 2.3) 
 
Where C is the count for a particular bigram. 
 
And for n-grams 
)(
)(1
1
1
1
1
1)|(
−
+−
−
+−=− +−
n
Nn
n
n
Nn
w
wwn
Nnn C
C
wwp  (Eq. 2.4) 
 
The resultant probabilities can then be used to calculate the probability of any 
sentence.  With author attribution studies the probability of a whole document can be 
calculated (normally the probabilities will be stored as logs to avoid numerical 
 13
underflow).  In the case of bigrams this is accomplished by multiplying all of the 
probabilities for a given sentence. 
 
Thus 
 
∏
=
−≈
n
k
kk
n wwpwp
1
11 )|()(    (Eq. 2.5)  
 
 
A problem arises when the sentence for which the probability is to be calculated 
contains a bigram that has a probability of zero (this will occur if a particular bigram 
was not observed within the training data).  This would result in some sentences for 
which the probability was calculated as zero.  Clearly just because a bigram was not 
seen in the training corpus does not mean that it will never occur.  This problem is 
magnified with trigrams, quadrigrams etc, where it is even more likely that all possible 
combinations of words will not have been observed in training.  This problem is solved 
via the process of smoothing. 
 
Smoothing 
 
At the most basic level the process of smoothing involves assigning non-zero values to 
all of the zero probability bigrams (or n-grams).  
 
Add-One Smoothing   
 
In add-one smoothing a value of one is added to all counts.  This addition of one to 
every bigram needs to be compensated for. 
 
Thus 
 
VC
C
wwp
n
nn
w
ww
nn +
+
=
−
−
−
)(
)(
1
1
1
1
)|(   (Eq. 2.6) 
 
 
 Where V is the number of word types (i.e. distinct words).  This is because the number 
of distinct unigrams is the same as V. 
 
Add-one smoothing is rarely ever used as it results in very sharp changes in the overall 
probabilities, with too much probability assigned to zero bigrams. 
 
Witten-Bell Discounting 
 
This relies upon the idea that probabilities of things that have never been seen can be 
calculated from those seen once.  In the case of n-grams those “seen for the first time” 
are all observed n-grams.  Thus the total probability mass for all of the zero bigrams 
can be calculated with the following equation. 
 
 14
 
∑
= +
=
0)(: )()(
)()|(
ix xx
x
wwci ww
w
xi TN
T
wwP   (Eq. 2.7) 
 
Where  is the number of bigram types beginning with word w)( xwT x, and is 
the count of bigram tokens beginning with word w
)( xw
N
x. 
 
This probability needs to be shared amongst all of the unseen bigrams. 
 
Thus 
 
)(
)|(
)()1()(
)(
1
11
1
−−
−
+
=
−
−
iii
i
www
w
ii TNZ
T
wwp
 (Eq. 2.8) 
 
 
Where )()( 11 −− −= ii ww TVZ  and V is the number of distinct bigrams. 
 
 
The non-zero bigrams must also be discounted as such 
 
)()(
)(
1
11
1)|(
−−
−
+
=−
ii
ii
ww
ww
ii TC
C
wwp   (Eq. 2.9) 
 
Witten-Bell smoothing produces much better overall results than Add-One smoothing 
without overcompensating for the zero values. 
 
 
Good-Turing Discounting 
 
This method works out how much probability to assign to the zero/low count n-grams 
by looking at those with higher counts.  To calculate the smoothed count of the zero 
occurring n-grams, the number of bigrams that occurred once is divided by the number 
of bigrams that never occurred.  This process is applied to all of the counts using the 
following equation 
C
C
N
N
CC 1)1( ++=    (Eq. 2.10) 
 
Where  is the number of bigrams of count C. n-grams with low counts (i.e. 1) are 
normally treated as having a count of 0. 
CN
 
 
 15
Backoff 
 
Another means of smoothing which is well suited to larger n-grams is the process of 
backoff.  The basic idea is that if no examples of a particular trigram exist, then to 
calculate another trigram use the information about bigrams.  If these bigrams do not 
exist then backoff to another level and use unigrams. 
 
When using trigrams the equation is 
 
⎪
⎪
⎭
⎪
⎪
⎬
⎫
⎪
⎪
⎩
⎪
⎪
⎨
⎧
>
=
>
=
−
−−−
−−−−
−−
),(
,0)(
0)(),|(
0)(),|(
)|(ˆ
2
1
1211
1212
12
i
ii
iiiii
iiiiii
iii
wp
OtherwisewwCand
wwwCifwwp
wwwCifwwwp
wwwp
α
α
 (Eq. 
2.11)   
 
The α is needed otherwise the probabilities will add to more than one.  Thus in order to 
be able to have α all values need to be discounted; either Witten-Bell or Good-Turing 
can be used.  When using backoff the normal practice is to treat all 1 count n-grams as 
if they never existed. 
 
The equation for calculating α is given in Eq 2.12.  Where p~  is the discounted 
probability. 
 
 
∑
∑
>
−
+−
>
−
+−−
+−
+−
+−
−
−
=
0)(:
1
2
0)(:
1
11
1
1
1
)|(~1
)|(~1
)(
n
Nnn
n
Nnn
wcw
n
Nnn
wcw
n
Nnnn
Nn wwp
wwp
wα   (Eq 2.12) 
 
 
Usage of N-grams 
 
In authorship attribution studies n-grams which work at the character level have shown 
promising results.  One advantage of character level n-grams is that they capture some 
of the morphological feature present in a text without the need for complicated parsing.  
Another advantage is that the vocabulary used (26 characters and possibly 
punctuation) is vastly smaller than that used in word level n-grams.  This allows the 
use of larger n values and at the same time reduces the sparse data problems of word 
level n-grams.  Character level n-grams can also be applied to different languages 
without any modifications to the protocol.  This is especially suited to languages such 
as Chinese where word boundaries are not obvious. 
 
Peng et al (2003) used character level n-grams to study texts in Greek, Chinese and 
English.  The Greek data consisted of two sets each containing 200 documents by 10 
authors, with 20 document per author.  They used 10 of the documents by each author 
as training data and the other 10 as experimental data.  The two sets used varied in 
genre with one set written by journalists and the other written by scholars.  For the set 
written by journalists they obtained an accuracy of 74% while the highest gained for 
 16
the other set was 90%.  In both cases the best results were gained using 3-gram 
models.  Thus they gained better results for the set of text that are more homogenous.  
This they suggest maybe because journalists may write in many different styles. 
 
The English data consisted of work by 8 authors.  They converted the corpus of work 
to lowercase characters, reducing their vocabulary to 30 characters.  The best accuracy 
obtained was 98% when using 6-gram models.  This is extremely high but as they 
suggest this may be due to the ‘distinct idiosyncratic writing styles of these famous 
authors’. 
 
The Chinese data consisted of work by 8 popular novelists.  They used 1 or 2 novels 
by each author for training and 20 as a test set.  Due to the high number of words in the 
Chinese language they selected only the most commonly used characters, resulting in a 
vocabulary of 2500 characters.  The best accuracy obtain was 94% using 3-grams 
models. 
 
This paper shows how powerful the n-gram technique can be when using relatively 
short n’s (3-6). 
   
An independent study using character level bigrams suggests that even shorter n’s can 
yield highly accurate results.  Khmelev and Tweedie (2001) studied 387 texts from 45 
authors.  They chose one text per author to keep out of the training set.  These random 
texts were then compared with the 45 authors producing an accuracy of 73.3%.  This is 
extremely high considering the size of field (85 authors) and the fact that they are 
using bigrams. 
 
 
2.3.2 Zipping Technique 
 
This method uses a compression program to judge the similarity between pairs of data 
sequence.  Any compression program that implements the Lempel and Ziv 
compression algorithm can be used.  This algorithm finds duplicate strings within the 
input data, finding the longest matching sequences using a sliding window system.  
The program encodes the most frequent sequences with the least bits, saving bytes for 
rarer sequences.  Thus the algorithm becomes more efficient the larger the data file is. 
 
Thus in order to judge the author of an unknown text, x , this text is appended onto 
larger sections by known authors (A).  The resultant texts are then zipped and the 
relative size calculated using the following formula 
 
 
)()( ASizeAxSizeAx −=∆   (Eq. 2.12)    
 
Where Size(Ax) is the size of the combined zipped file, and Size(A) is the size of the 
zipped work by the known Author.  Thus the file with the smallest value of ∆ is the 
closest match to the anonymous author (x). 
 
 17
This works as the algorithm becomes more efficient as it compresses a long piece of 
text by one author.  If a new author’s work is appended to the bottom of the file, then 
when the algorithm reaches this section it is optimised for the original author.  It tries 
to encode the appended text using the system that is optimised for another author, and 
thus is less efficient. 
 
This system has been used by Benedetto et al (2002) for language recognition, 
authorship attribution and language classification.  These all work on the same 
premise; described above.  The authorship attribution study used a corpus of 90 
different texts from 11 authors.  They selected one text out of the 90 appended it to all 
other texts, and then calculated the author (the one with the smallest ∆).  This was 
carried out for all possible combinations of texts.  This procedure resulted in an 
accuracy of 93.3%. 
 
 
Like the cusum technique Benedetteo et al’s work has suffered criticism.  J Goodman 
of Microsoft compared the zipping technique to a Naïve Bayes approach (Goodman, 
2002).  He criticises the technique for being 17 times slower and producing 3 times 
more errors than the simple Naïve Bayes technique.  He compared the two technique 
ability to attribute a text to a particular topic, using the 18828 corpus of newsgroup 
texts (available at http://www.ai.mit.edu/~jrennie/20Newsgroups/).  This corpus contains 
~20,000 newsgroup documents which are sorted into 20 different topics. 
For both the zipping and Naïve Bayes approaches Goodman used perl script.  In the 
case of the Naïve Bayes approach the texts were segregated at the word boundaries 
using the perl ‘split’ command.  For each topic the frequency of each word was 
calculated and smoothed using add-one smoothing and a probability was calculated for 
each word occurring in a particular topic (Equation 2.13). 
 
 
topicanyincounttokenwordtopicoflengthtotal
otherwise
topicanyinoccurswordiftopicinwordcount
topicwordP
+
⎭
⎬
⎫
⎩
⎨
⎧
+
=
0
1)(
)|(   (Eq 2.13) 
 
 
The resulting probabilities were then converted to log, to avoid numerical underflow.  
To calculate which topic a document belongs to the log probability of each word for a 
given topic was summed to produce a total probability for that document.  The 
document was deemed to belong to the topic group that produced the largest negative 
log (i.e. the largest probability). 
 
For the zipping technique the perl code was used to run the Gzip compression program 
with each combination of texts (see 2.3.2 for details of protocol). 
 
Using 99% of the texts for training and 1% for testing (192 documents), Goodman 
gained an overall accuracy of 86% for the Naïve Bayes and a 53% accuracy for the 
zipping technique.  The Naïve Bayes algorithm taking 5 minutes to run while the 
zipping algorithm took 85 minutes. 
 
 18
Benedetto et al (Benedetto, 2002 B) responded to Goodmans criticism by repeating his 
experiment using 3 different compression programs.  They mention that Goodmans 
choice of experiment is odd, as they did not cover the area of subject classification in 
their original paper (Benedetto, 2002 A).  They also point out that Goodman suggests 
that he used the zipping algorithm from their paper, but since that paper did not cover 
subject classification it was not clear what Goodman had actually done. 
 
Benedetto et al’s technique for authorship attribution is to attach each unknown text to 
every other text that is considered to be a reference.  All of the composite files, as well 
as the original reference files, are then compressed.  Then the difference (∆) between 
each compressed composite file and its corresponding compressed reference file is 
calculated.  The smallest ∆ suggests that the file belongs to the reference group of the 
file it is joined to. 
Benedetto et al, using the same 18828 corpus as Goodman, randomly selected 200 
documents for testing using their algorithm.  This results in a vast number of text 
combinations to be compressed and ordered (~3.7 million).  They carried out the 
experiment with three different compression programs (gzip, blc and mz), producing 
overall accuracy’s of 86.5%, 86.5% and 87% respectively. 
These results are on par with those produced by Goodman using Naïve Bayes.  It 
seems likely that the discrepancy between the two authors results for the zipping 
protocol lie in the exact technique used.  Goodman does not clearly state the procedure 
he undertook to produce an accuracy of 53%.  It is possible that he attached each test 
document to a training document for each newsgroup.  With the training documents 
consisting of all of the documents in the training corpus that are from the same 
newsgroup.  This system would produce a much smaller number of combination texts 
(3840).  It has been shown that this approach produces less accurate results in 
comparison to the protocol adopted by Benedetto et al (Benedetto, 2003). 
 
 Another paper criticising the zipping technique compares the method to an n-gram 
language model approach.  Khmelev and Teahan (2003) criticise various aspects of 
Benedetto et al’s paper.  They suggest that Benedetto et al’s discussion of language 
trees is flawed, that the application of compression programs to DNA analysis is 
doubtful, that their definition of entropy is incorrect, that other entropy based methods 
produce better accuracys and that the compression method is significantly slower. 
 
To backup their criticism they quote results from a paper that compared the zipping 
and n-gram approach (Kukushkina et al 2001).  Kukushkina et al used a corpus of 
work from 82 authors, with one text from each author used for testing.  The results, 
comparing 16 compression programs with a Markov chain approach, are shown in 
table 2.1. 
 
 
 
 
 
 
 
 
 
 
 19
Classification Technique Overall accuracy 
7zip 47.6% 
Arj 56.1% 
Bsa 53.7% 
Bzip2 46.3% 
Compress 14.6% 
Dmc 43.9% 
Gzip 61% 
Ha 57.3% 
Huff1 12.2% 
Lzari 20.7% 
lzss 16.1% 
Ppm 26.8% 
Ppmd5 56.1% 
Rar 70.7% 
Rarw 86.6% 
rk 63.4% 
Markov Chain 84.1% 
Table 2.1 
 
 
Khmelev and Teahan then say: 
 
 “Clearly, gzip is significantly outperformed by other compression algorithms and the 
first order Markov chain model.  Notice also that in practical implementations, the 
gzip-based approach is significantly slower than the first order Markov chain method.” 
 
They go on to suggest that Markov models are more appropriate than an LZ approach.  
They do not mention that the rarw compression program produced a slightly higher 
accuracy than the Markov chain model (86.6% and 84.1% respectively). 
Khmelev also has a more optimistic view on the compression method in the original 
paper (Kukushkina et al 2001) where he says: 
 
“It follows from the data of Table 5 that data compression algorithms assign correct 
author to control text quite often.  Therefore they are undoubtfully useful.  Notice that 
application of the program rarw yields even better results than results obtained with 
help of Markov chains in [1].  Although such a superiority could be related to some 
statistical mistake, it is the best result achieved recently.” 
 
In Benedetto et al’s reply (Benedetto 2003) to Khmelev and Teahan’s criticism they 
point out that the rarw program outperformed the Markov chains method.  They then 
point out that Kukushkina et al performed the zipping technique in a different way to 
the protocol in their original paper (Benedetto 2002).  Kukushkina et al carried out the 
classification by using a large training text for each author (all of that authors work 
appended together), and appending all combinations of test and training documents.  
Benedetto et al suggest that since gzip only has a sliding window size of 32Kb, in 
effect only the last 32Kb of each training file will be used.  As discussed earlier 
Benedetto et al’s protocol is to attach each test file to all separate training files.  They 
show results from two experiment to contrast the two methods.  The first experiment 
reported is the one carried out in response to Goodmans critisism, classifying texts into 
newsgroup categories.  When using Kukushkina et al’s approach they get an overall 
accuracy of 60%, while using their own method they get 85%. 
 20
The second experiment uses the authorship attribution experiment from their original 
paper (Benedetto et al 2002), with Kukushkina et al’s approach they get an accuracy of 
77% while using their own method yields an accuracy of 93%. 
 
Thus despite the criticism of the zipping technique it is clearly of some value.  While 
the protocol described by Benedetto et al is much slower than other techniques it can 
produce high levels of accuracy.  If their protocol was applied to compression 
programs that have been shown to yield good results elsewhere (e.g. rarw) then the 
accuracy levels may be increased further. 
 
 
3 Requirements 
 
3.1 Project Aim 
 
The aim of this project is to compare three different methods of authorship attribution.  
The chosen methods to compare are 
 
• Cusum 
• N-Gram 
• Zipping 
 
These methods have been selected because of the large difference within their 
protocols.  The Cusum technique works at the sentence level, the N-gram at the 
character level and the Zipping on a more abstract level.   
 
 
3.2 Corpus Processing 
 
To allow a baseline to be set the English corpus used and the authors selected are the 
same as those used by Peng et al (2003) and are from the Alex Catalogue.  The authors 
selected are the 8 most prolific writers in the collection and are shown in Table 3.1. 
 
Author Name Number of Works Total size Characters 
Charles Dickens 20 10,781,483 
Edgar Allan Poe 123 2,354,442 
John Keats 34 240,809 
John Milton 30 709,710 
Oscar Wilde 17 507,957 
Ralph W. Emerson 16 2,143,382 
Robert L. Stevenson 21 5,420,030 
William Shakespeare 18 1,869,709 
Table 3.1 
 
 21
Using the same corpus as Peng et al allows the direct comparison of results obtained 
by the n-gram techniques.  This allows the cusum and zipping methods to be gauged 
with more insight.   
 
The corpus was split so that ~80% was set aside for training and ~20% was kept for 
testing.  It would have been ideal to carry this out exactly as Peng et al had done to 
gain a more accurate comparison.  Sadly their paper only details the training size used 
for the eight authors but not how the text were split into training and test sets.  Thus 
the papers were selected randomly; the training sizes are shown in Table 2. 
 
Author Name Training Size (Characters) Training Size (Percentage 
of Authors Work) 
Charles Dickens 9,069,797 84% 
Edgar Allan Poe 2,019,281 85% 
John Keats 199,565 83% 
John Milton 622,528 88% 
Oscar Wilde 463,385 91% 
Ralph W. Emerson 1,765,570 82% 
Robert L. Stevenson 4,724,807 87% 
William Shakespeare 1,441,149 77% 
Table 3.2 
 
The next step was to process the texts by removing the various different types of 
header that were attached to the text files (e.g. Project Gutenberg).  Once this was 
completed the training texts for each author were combined to produce a single 
training document per author. 
 
The documents in the test corpus were of various sizes, for some authors (e.g. Poe) the 
size would have been insufficient to test.  Thus for these authors test documents were 
combined to produce a set of test documents for that author.  For other authors that had 
a small number of large documents, these were split.  This relies upon the assumption 
that the two resulting documents can be regarded as independent.  This is not always 
the case as a document that is easily identified when split could give rise to two easily 
identified documents.  But of course the opposite is true.  While this is not ideal it was 
needed to produce a large enough number of test documents, and since the same 
corpus is used for all three methods it should not bias the results.  The number of test 
documents per author is shown in Table 3.3. 
 
Author Name Number of Test Documents 
Charles Dickens 8 
Edgar Allan Poe 8 
John Keats 1 
John Milton 3 
Oscar Wilde 2 
Ralph W. Emerson 8 
Robert L. Stevenson 9 
William Shakespeare 8 
Table 3.3 
 22
The corpus was POS-tagged by Mirella Lapata using the TnT tagger (Thorsten, 2000), 
this was divided in exactly the same way, producing the same 47 test documents, and 
the same training documents.  
 
3.3 Experiments 
 
There are certain things that needed to be considered in order to compare the three 
methods successfully.  The zipping and n-gram techniques produce a numerical set of 
results.  If the cusum technique were used as described by Fraringdon, it would result 
in a set of graphs.  These graphs would be too subjective in comparison to the other 
two methods leaving interpretation down to the user.  Thus to compare the cusum 
technique a machine learning technique was used (see section 5.2).  This allowed the 
habits analysed by the cusum technique to be used in a decision tree algorithm. 
 
The experiments with the zipping technique were carried out using a single training set 
for each author.  This is the same as the technique used by Kukushkina et al (2001), 
and while it has been shown to be less efficient than Benedetto et al’s technique 
(Benedetto 2003) it is far faster (requiring 376 text combinations in comparison with 
13113).  This smaller number of texts was a requirement as cabinet manager, one of 
the two compression programs used, was very slow to run. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 23
4 Implementation and Testing 
 
The coding requirement for this project was for the n-gram and cusum techniques, the 
classes produced for these techniques are discussed below. 
 
 
4.1 Zipping Technique Coding 
 
  The Zipping technique required no actual implementation as existing compression 
programs were used.  In order to compare results with those gained by Benedetto et al 
[2002] one of the compression programs chosen was gzip. 
 
4.2 N-gram Coding 
 
The coding for the N-gram method was broken down into small classes.  This allowed 
each class to only carry out a limited number of functions, which made coding, and 
testing a simpler task.  An overview of the n-gram classes is shown in Fig 4.1, a brief 
description is given for each class. 
 
 
class java.lang.Object 
 class CalculateAlpha 
 class CalculateTextProbability 
 class ControllerNGrams 
 class ConvertToCharacters 
 class ConvertToNGrams 
 class NGram (implements java.lang.Comparable) 
 class WittenBellSmoothing 
 
Fig 4.1 
 
 
The ConvertToCharacters class reads in the text to be converted to n-grams, removes 
all characters that are not within the set vocabulary, converts the characters to 
lowercase, and creates a document with a character per line.  While placing a character 
per line is not the most elegant solution it is very easy to program and proves to work 
for even large document with good speed. 
 
The ConvertToNGrams class reads the file produced by the ConvertToCharacters 
class to construct all of the different levels of n-gram (1n-6n).  Each different level of 
n-gram is stored in a different TreeSet, and all of the TreeSets are stored in an array of 
TreeSets.  As TreeSet implements the Set interface only one of each type of n-gram 
object (dicussed below) will be stored.  The TreeSet was chosen even though it is 
marginally slower than the HashSet.  Firstly this is because it allows results to be 
printed out in alphabetical order, which makes program-testing simple.  Secondly this 
alphabetical order allows for certain methods, such as smoothing and calculating α, to 
be greatly optimised. 
 
 24
The Ngram class contains all of the variables for each n-gram.  It implements the 
Comparable interface which allows the objects to be arranged in the TreeSet in a 
sorted order.  The CompareTo method (required by the Comparable interface) dictates 
how the Ngram objects are to be sorted.  In this instance the n-grams are sorted 
entirely by their character sequence.  The CompareTo method has also been used to 
increment the counter for the Ngram.  Thus if a TreeSet already contains a particular n-
gram when the CompareTo method is called it will increment the counter of the n-
gram contained in the TreeSet.  This allows for a quick way of counting all of the n-
grams in a document and producing an ordered set. 
 
The WittenBellSmoothing class uses Equation 2.7 to calculate the smoothed count 
and probability for each n-gram.  The smoothed count is mainly used for visual 
purposes when testing the program. 
 
The CalculateAlpha class uses Equation 2.12 to calculate the α factor required for 
backoff. 
 
The ControllerNgrams is the main class for the n-gram method.  This class is used to 
set all of the possible variables used (e.g. n-gram level, Number of characters read etc).  
Firstly the ControllerNgrams class calls the ConvertToNgrams class passing the name 
of the training text to be read as a variable.  The ConverToNgrams in turn calls the 
ConvertToCharacters class to process the text and then reads the outputted characters 
to construct the n-grams.  Once all of the different levels of n-gram are contructed the 
ConvertToNgrams class calls the WittenBellSmoothing and CalculateAlpha classes.  
After the language model for the author is constructed the ControllerNgrams class calls 
the CalculateTextProbability class for each different text to be compared to the training 
text. 
 
The CalculateTextProbability class reads a text file using the ConvertToCharacters 
class to process the text.  It constructs n-grams from the text and for each n-gram in the 
text it gets the probability of that n-gram from the TreeSet for the tested author.  The 
log probabilities for each n-gram in the text to be compared are added to produce an 
overall probability for that text (N.B. Log probabilities are used to stop numerical 
underflow).  If and N-gram is missing from the data set then the process of backoff is 
used to estimate a probability.  This reduces the data sparsity problems encountered at 
higher N-levels. 
 
 
4.3 Cusum Coding 
 
In order to implement the cusum technique the corpus was POS-tagged by Mirella 
Lapata using the TnT tagger (Thorsten, 2000).  This uses the Penn Treebank Tagset 
(Marcus et al, 1994) to assign a part-of-speech tag to each word in the corpus.  This 
allows the text to be easily broken down into sentences and for the various cusum 
habits to be calculated.  An overview of the implemented cusum classes is shown in 
Fig 4.2 followed by a brief description of each class. 
 
 
 
 25
 
class java.lang.Object  
 class ControllerCusum  
 class ConvertToSentences  
 class Sentence  
 
Fig 4.2 
 
 
The Sentence class contains all of the information for each sentence, a list of the 
words and all of the Cusum habits.  An ArrayList is used to store the words contained 
within a sentence, and each word is stored as a Word object along with its tag.  This 
allows for names with multiple words to be replaced with just one word to stop long 
names from biasing the sentence length. 
 
The ConvertToSentences class reads a tagged text file and converts it into Sentence 
objects.  This was accomplished by using the “.” tags to denote sentence boundaries.  
To deal with abbreviations like “e.g.” the “LS” (list marker) tag was used to correct for 
false sentence boundaries.  
 
The ControllerCusum class is a very simple class from which the other classes are 
controlled. 
 
 
4.4 Code Testing 
 
The code testing involved testing each class as it was developed, this was 
accomplished using a combination of screen prints and file outputs. 
 
For the n-gram coding a simple version of the ControllerNgrams class was written to 
run all of the other classes.  The next class written was the ConvertToCharacters class.  
Testing this involved checking that the outputted file only included the correct 
characters (i.e. not those excluded) and that they were all in lowercase.  Then the 
ConvertToNgrams and Ngrams classes were written at the same time.  These were 
tested using various very short texts to make sure that the correct n-grams were 
generated along with the correct counts.  To aid this process the toString method of the 
Ngram class was used to print out the characters and count of each n-gram.  Then the 
TreeSet for each level of n-gram was outputted to a file.  This resulted in an 
alphabetical list of the n-grams for every n level with counts for each, making testing a 
very simple process. 
For the WittenBellSmoothing and CalculateAlpha classes various texts were processed 
and converted into n-grams.  The smoothed count, probability and alpha values 
generated by these two classes were added to the toString method of the Ngram class 
again allowing a file output for each n-gram level showing each n-grams details.  For a 
variety of texts the results produced by the classes were compared to hand calculated 
answers to verify the accuracy.  Some effort was put into producing the most 
optimised code for both the WittenBellSmoothing and CalculateAlpha classes, taking 
advantage of the alphabetical order of the TreeSets to speed cycling through the 
TreeSets. 
 26
Testing the CalculateTextProbability class involved creating a language model for a 
training text and then using the produced n-gram information to calculate the 
probability of a very small text.  The probability of the same text was then calculated 
by hand using the outputted information from the n-grams, this included checking the 
backoff process. 
Finally the ControllerNgrams class was finished so that for each language model 
trained the probability of a batch of test documents could be produced. 
 
The cusum coding and testing followed a very similar path to that for the n-gram.  The 
first class written was the ControllerCusum.  The ConvertToSentences and Sentence 
classes were then written simultaneously.  The toString method of the Sentence class 
was used to output information about each sentence.  The sentence length and habit 
counts generated by these classes were then compared with manual counts for a variety 
of text by each author. 
 
 
5 Experiments and Results 
 
 
5.1 Ngram Experiments 
 
Peng et al [2003] compared Absolute, Good-Turing, Linear and Witten-Bell smoothing 
techniques.  They found that there is a slight difference in the results obtained between 
these methods but suggest that any of the methods could be successfully used, as the 
ranking order was almost always the same.  For this report the chosen method was 
Witten-Bell which Peng et al showed to perform consistently.  As is the norm when 
implementing a backoff system n-grams with 1 counts were discarded.  Thus after a 
the whole training text was processed and converted into n-grams all of the 1 count n-
grams were removed. 
 
Unless otherwise stated the vocabulary used for these experiments consisted of 29 
characters (the 26 letters of the alphabet and `,` `;` `.` ).  The effect of changing the 
vocabulary size and constituents is discussed later (see section 5.5). 
 
 
5.1.2 Authorship Attribution 
 
Classifying a piece of work to belong to an author was carried out by producing a 
language model for each author, using their training corpus, and then subjecting the 
text in question to every language model.  The author whose language model produced 
the largest probability (i.e. The smallest absolute log) is judged to have written the 
text.  
 27
 
5.1.3 Influence of Training Size 
 
In order to gauge the effect of training size on overall accuracy the test corpus was 
randomly divided into 4 separate groups.  This was carried out for the 6 most prolific 
authors thus allowing a maximum training size of 550,000 characters.  These groups 
would in effect be separate experiments thus allowing an estimate of the standard 
deviation for each training size.  For this experiment a 6-gram model was used with a 
test size of 10,000 characters.  An example of the data produced is shown in Table 5.1, 
the highlights represent the highest probability.  The results are shown in Fig 5.1 with 
a line of best fit and standard deviations. 
 
 
  6N  Training Size 300,000/TestSize 10000 
 Dickens Emerson Milton Poe Shakespeare Stevenson Correct 
    
dickensComp7 -13668.7971 -14451.74689 -15290.3767 -14276.3619 -15824.8811 -14178.842  
dickensComp8 -13661.5888 -14338.86738 -15117.0528 -14348.4508 -15312.3316 -14189.123  
emersonComp6 -14497.6467 -13231.96052 -14842.118 -14627.1802 -15567.155 -14193.131  
emersonComp8 -14961.4287 -13047.61979 -14871.0237 -14280.4868 -16245.486 -14579.635  
miltonComp1 -15711.2756 -15139.7307 -14082.3131 -15271.5947 -15358.7358 -15646.09  
poeComp7 -16924.4863 -16585.92536 -16466.7902 -16036.672 -16044.2897 -16429.145  
shakespeareComp2 -16928.6026 -16589.68924 -16470.5924 -16040.213 -16046.541 -16432.905  
shakespeareComp3 -16792.3208 -16320.01055 -17108.6578 -16510.1578 -15129.7319 -16451.854  
shakespeareComp7 -16168.2684 -16198.64814 -15663.755 -15957.5117 -15256.3547 -16067.573  
shakespeareComp8 -16165.0339 -16379.30143 -15928.7959 -16216.7967 -15206.7086 -15870.896  
stevensonComp3 -14828.9333 -14577.13381 -15308.76 -14563.9694 -15926.8247 -13884.157 10/11 
        
dickensComp2 -14531.2925 -14887.73712 -15076.4679 -14455.3911 -15680.1261 -14764.513  
dickensComp3 -13984.3113 -13942.52577 -14853.9298 -13726.7014 -15396.9915 -13369.587  
miltonComp2 -16138.2114 -15561.00656 -14725.371 -15628.4315 -15725.5226 -15981.24  
poeComp1 -14246.8783 -14379.64729 -15305.6733 -13769.3795 -15843.0465 -14238.497  
poeComp2 -14320.7853 -14056.7464 -15054.1611 -13343.5486 -15648.5478 -14004.112  
poeComp4 -15112.6567 -15227.87958 -15731.5101 -14827.8186 -16763.3839 -15345.499  
poeComp6 -16887.5276 -16054.45182 -17320.6946 -16026.7409 -17702.4267 -15943.809  
poeComp8 -13788.7321 -14081.6383 -14711.399 -12879.4603 -15648.5644 -14333.748  
shakespeareComp4 -16369.4173 -16269.99277 -16496.7616 -16251.6612 -14406.0642 -15618.665  
stevensonComp1 -14359.3505 -13273.2669 -14889.7268 -14069.5809 -15490.4874 -14204.047  
stevensonComp6 -14485.6669 -14515.19481 -15164.0671 -14751.8408 -15855.509 -14017.621 7/11 
        
dickensComp1 -13824.8345 -14919.64439 -15169.6648 -14545.8923 -15492.487 -14548.386  
dickensComp5 -14505.3625 -15088.92176 -15761.4913 -14779.3927 -16171.7865 -15001.821  
emersonComp1 -14697.0538 -13242.91362 -14708.6587 -14336.4403 -15264.7948 -14785.484  
emersonComp2 -14256.821 -12634.96301 -14814.8184 -13973.1006 -15438.9884 -13909.759  
emersonComp7 -14689.5201 -13706.02869 -15271.4024 -14739.5177 -16030.6368 -14690.16  
miltonComp3 -15960.7891 -14567.65774 -13897.5621 -14935.1387 -15409.5352 -15658.547  
poeComp3 -15228.0801 -14529.99432 -15353.4033 -14371.4913 -15879.9153 -14742.802  
poeComp5 -15272.7712 -14845.16583 -14910.4899 -13930.7632 -15493.5769 -15038.723  
shakespeareComp1 -16229.1152 -16048.80678 -15712.5941 -15822.997 -14339.4094 -15829.208  
stevensonComp5 -14460.3462 -14975.42116 -15393.1974 -14738.9349 -15956.617 -14352.917  
stevensonComp7 -13894.0208 -14162.4883 -15207.0377 -14101.9924 -15278.3537 -13234.861 11/11 
 28
        
dickensComp4 -14687.285 -15654.98766 -15618.8114 -15072.4218 -16250.8281 -15048.255  
dickensComp6 -13659.4364 -13409.51586 -14361.0328 -13639.8896 -14751.2318 -13296.273  
emersonComp3 -14479.8123 -13413.60788 -15052.7862 -14172.6574 -15811.7362 -14369.673  
emersonComp4 -14650.2856 -14086.04365 -15069.7469 -14831.2992 -15739.8834 -14753.833  
emersonComp5 -14495.2849 -13691.98016 -15046.3223 -14476.4234 -15563.8059 -14330.381  
shakespeareComp5 -16485.398 -16105.49019 -15723.0261 -15834.9417 -14912.3429 -15764.319  
shakespeareComp6 -16784.5632 -16625.68226 -16032.9354 -16438.1214 -15216.3091 -16262.877  
stevensonComp2 -14485.7501 -13299.25504 -14425.3442 -14129.8494 -15050.5084 -13955.892  
stevensonComp4 -14672.6821 -15264.03237 -15892.717 -14999.2963 -16248.8682 -14328.734  
stevensonComp8 -14164.4713 -14553.81193 -15056.8408 -14227.5443 -15331.0542 -13541.379  
stevensonComp9 -14614.467 -15051.79993 -15436.6464 -14771.8787 -16100.4708 -14424.944 9/11 
        
        
Mean  0.84090909       
Standard Dev 0.15525683       
Error 0.07762841       
Table 5.1 (Showing 1 of 6 data sets for Fig 5.1) 
 
Influence of Training Size
0.4
0.5
0.6
0.7
0.8
0.9
1
0 100000 200000 300000 400000 500000 600000
Training Size (Characters)
O
ve
ra
ll 
A
cc
ur
ac
y
 
Fig 5.1 
 
The training size clearly has an effect on the overall accuracy, and not surprisingly the 
accuracy increases with larger training sizes.  Even at 12500 characters (~2700 words) 
the overall accuracy is 59%, by 50,000 characters (~11,000 words) the accuracy has 
reached 70% and yet the standard deviation across this range remains at ~0.24.  When 
a training size of 175,000 characters (~38,000 words) is reached the overall probability 
is 86% and the standard deviation has dropped to 0.117.  After this point the increase 
in training size produces no further gain in accuracy.  
 
 29
5.1.4 Influence of Context Length 
 
The context length of the N-gram (e.g. bigram, trigram etc) plays an important role in 
deciding the level of information that the language model contains.  A longer context 
length will provide more information with which to model an author.  The upper level 
of context length is decided by two factors.  Firstly higher context lengths start to 
display more data sparsity and will therefore have to backoff (i.e. drop to lower gram 
levels) more often.  Thus the advantage gained by having more information is lost by 
having to estimate more probabilities.  The second reason large contexts lengths are  
not used is purely one of computing power, with the possible character combinations 
increasing exponentially (e.g. for 30 characters: 30,900,27000,810000 etc for 
1N,2N,3N,4N respectively) resulting in very large data sets which are impossible to 
process. 
 
To test the effects of context length on accuracy a training size of 150,000 characters 
was used for all 8 authors.  This training size limit was imposed by the size of the 
training corpus for Keats.  The test size was again kept at 10,000 characters.  The same 
experiment was then repeated but this time the test size was not fixed, thus the 
maximum amount of characters were used for each text to be classified (varying from 
~ 18K – 400K). The results are displayed in Fig 5.2 
 
 
Influence of Context Length
(Training Size 150K)
0.5
0.55
0.6
0.65
0.7
0.75
0.8
0.85
1 2 3 4 5 6
Context Length
O
ve
ra
ll 
A
cc
ur
ac
y
Test 10K
Test Maximum
Fig 5.2 
 
 
The graph shown in Fig 5.2 shows that accuracy increases with larger context lengths, 
the highest accuracy reached is ~80%.  These results are of interest because even 
though the training size is only 150K the accuracy gained at the 1-gram level is 65%.  
This is a large difference from the 23% gained by Peng et al (2003) when they used the 
full test and training data.  Where this difference originates is not clear although Peng 
et al did gain higher accuracy’s for their Chinese and Greek experiments (~ 71% and 
 30
58% respectively).  In the case of the Chinese experiments the richness of the 
vocabulary at the character level would account for the high accuracy at the 1-gram 
level.  For the Greek experiments Peng et al do not state the size of the vocabulary they 
have used.  The Greek alphabet contains 24 characters and thus the overall size was 
probably not dissimilar to that used for the English experiments.  Thus the discrepancy 
between the results discussed here and those gained by Peng et al may lie in the way 
the corpus has been divided, with different training and test documents (See section 
3.2 for information). 
 
The fact that the experiment using the smaller test size produces a higher accuracy is 
also of interest.  This may be because the training size of 150K does not produce a 
language model that covers all of each authors styles, thus the larger training 
documents are more likely to cover a larger range of styles causing a drop in accuracy.  
 
The experiments were repeated using all of the training texts available for each author.  
The only limit imposed here was for Dickens were the amount of data available (~9 
million character) was too great to process efficiently at the 6-gram level.  The training 
sizes used for the 8 authors are shown in Table 5.2, and the results shown in Fig 5.3. 
 
Author Approximate Training Size (Characters) 
Charles Dickens 6,000,000 
Edgar Allan Poe 1,800,000 
John Keats 150,000 
John Milton 550,000 
Oscar Wilde 400,000 
Ralph W. Emerson 1,500,000 
Robert L. Stevenson 4,200,000 
William Shakespeare 1,000,000 
Table 5.2 
 
 
Influence of Context Length
(Training Size Maximum)
0.5
0.55
0.6
0.65
0.7
0.75
0.8
0.85
0.9
0.95
1 2 3 4 5 6
Context Length
O
ve
ra
ll 
A
cc
ur
ac
y
Test 10K
Test Maximum
Fig 5.3 
 31
With an increase in the training size the situation is reversed with the maximum test 
size producing the highest accuracy, 91% using a 4-gram model.  The extra training 
information also results in the graphs shifting towards the left, with lower context 
lengths providing the highest accuracy.  For both the 10K and maximum test sizes the 
6-gram model produces an overall accuracy which is lower than the 1-gram.  This 
maybe due to the fact that with the 150K training size the 6-gram model drops to a 5-
gram model more frequently than the maximum training size, were it backs-off  ~1/2 
as often.  Thus the larger training size produces a language model which is over trained 
to each authors style within the training set.  This again is in contrast to the results 
produced by Peng et al (2003) with their maximum accuracy for the English 
experiments gained at the 6-gram level.  
 
5.1.5 Influence of Vocabulary 
 
For all of the experiments discussed so far the vocabulary used has consisted of 29 
characters (the 26 letters of the alphabet and `,` `;` `.` ).  This is acceptable for testing 
the effects of context length and training size but may not produce the highest overall 
accuracy of authorship classification.  Peng et al (2003) used the 30 most frequently 
occurring characters contained in the corpus.  Thus the frequency of all of the 
characters in the corpus was calculated, the top 40 of which are shown in table 5.3. 
Two experiments were carried out using the maximum training and test sizes, and a 
vocabulary of the 20/30 most frequently occurring characters.  Fig 5.4 shows the 
results alongside a graph for the original 29-character vocabulary. 
 
 Character Count   Character Count 
1 e 2383594  21 b 298060 
2 t 1737867  22 . 196852 
3 a 1553890  23 v 184951 
4 o 1475305  24 k 153396 
5 i 1349196  25 ' 139428 
6 n 1347324  26 - 68525 
7 s 1220936  27 ; 55638 
8 h 1201291  28 " 37714 
9 r 1141358  29 x 28734 
10 d 848111  30 ! 23135 
11 l 785466  31 j 21674 
12 u 552471  32 ? 20170 
13 m 520519  33 q 19650 
14 c 469593  34 : 14898 
15 w 452587  35 z 7894 
16 f 437492  36 ) 5679 
17 , 419757  37 ( 5673 
18 y 388985  38 ] 3094 
19 g 383623  39 [ 3092 
20 p 337786  40 _ 1045 
Table 5.3 
 32
 
Influence Of Vocab
(Training Size Max/Test Size Max)
0.5
0.55
0.6
0.65
0.7
0.75
0.8
0.85
0.9
0.95
1 2 3 4 5 6
Context Length
O
ve
ra
ll 
A
cc
ur
ac
y
29 Original Vocab
30 MF Vocab
20 MF Vocab
Fig 5.4 
 
Fig 5.4 shows that using the 30 most frequently occurring characters has resulted in no 
gain in overall accuracy, with the highest accuracy the same as that using the original 
vocabulary.  The 20 most frequently occurring characters also produces the same 
maximum accuracy but results in a larger fluctuation over the context length range.  
This is to be expected as there is less information to differentiate the authors.  It is at 
the 1-gram level that there is the biggest difference between the three vocabularies.  
Interestingly the original vocabulary produces a higher overall accuracy than the 20 or 
30 most frequently occurring characters (85%, 53% and 74% respectively). 
At the 6-gram level the three vocabularies produce closer results, with an increase in 
vocabulary size yielding slightly higher accuracy’s.   
 
Overall the results generated by the n-gram language model are acceptable.  While 
they differ from those produced by Peng et al this could mainly be due to the 
differences in the separation of corpus into training and test data.  The results show 
internal consistencies, with a subset of test documents proving difficult to correctly 
classify across all experimental conditions.  
 
 
5.2 Cusum Experiments 
 
It was felt that the normal cusum graphs (see section 2.2.2) produce results that were 
too subjective to be compared to the n-gram and zipping techniques.  Thus to test the 
information produced by the cusum procedure a machine learning technique was 
utilised.  The machine learning system called Weka, which implements machine 
learning algorithms in Java was used.  In particular the J48 decision tree algorithm, 
which is now discussed. 
 
     
 33
5.2.1 Decision Trees and The J48 Algorithm 
 
(Blurock) 
 
A decision tree algorithm tries to create the best description of a classifier from a given 
data set.  The produced tree has internal nodes, which are tests, and after passing 
through these tests a leaf node is reached which produces a classification.  Fig 5.5 
shows a simple example of a decision tree, produced using the Weka ‘weather.arff’ 
file. 
 
 
Outlook 
 
            Overcast                                         Rain               
               Sunny      
      
                        Play                     Humidity             Windy 
 
 
                                                  <=75                >=75     True               False              
 
 
     Play      Don’t Play    Don’t Play   Play 
 
Fig 5.5 
 
 
The ID3 decision tree algorithm starts by trying to find the best descriptor for a data set 
(the best attribute for making a decision), once this is found the data is split into 
subsets.  The best descriptor becomes the first node in the decision tree and the 
remaining nodes are built by repeating the search for the best descriptor and splitting 
the data into smaller subsets.  At each split of the data the entropy gain is calculated.  
This is the difference between the original entropy and the entropy for each branch, 
allowing a measure of the value of each descriptor, the larger the gain the better the 
descriptor.  The idea is to reduce the overall entropy with each subset of the data.  If 
the entropy is reduced to 0 then the decision tree exactly describes the data. 
 
J48 is a Java implementation of the 8th version of the C4.5 decision tree algorithm, 
hence the name J48.  C4.5 was introduced by Quinlan(1993) as an extension to ID3 to 
deal with some of the issues of the ID3 algorithm.  Unlike ID3 the J48 algorithm can 
deal with missing data values, cope with continuos attribute values and replace 
subtrees that are likely to cause errors with leaf nodes (Reduced Error Pruning - REP). 
 
5.2.2 Weka Experiments 
 
To use the J48 algorithm the data produced by the cusum technique was arranged in a 
file structure recognised by the J48 classifier.  This requires a header declaring all of 
the attributes used and the possible classes (the 8 authors).  The data is arranged by 
separating the attributes by commas and displaying the class they belong to.  For these 
 34
experiments each line in the data file corresponds to a sentence, an example data set is 
shown in Fig 5.6. 
 
 
@relation Authors 
 
@attribute sentenceLength real 
@attribute TotalHabits real 
@attribute IVW real 
@attribute 2LW real 
@attribute 3LW real 
@attribute IVW+2LW real 
@attribute IVW+3LW real 
@attribute 2LW+3LW real 
@attribute Class{Dickens,Emerson,Keats,Milton,Poe,Shakespeare,Stevenson,Wilde} 
@data 
21,13,6,4,3,10,9,7,Dickens 
20,23,13,6,4,19,17,10,Dickens 
25,21,11,5,5,16,16,10,Dickens 
45,30,16,8,6,24,22,14,Dickens 
9,4,1,2,1,3,2,3,Dickens 
25,19,12,4,3,16,15,7,Dickens 
12,5,2,3,0,5,2,3,Dickens 
13,8,5,2,1,7,6,3,Dickens 
25,16,8,4,4,12,12,8,Dickens 
10,7,5,1,1,6,6,2,Dickens 
9,7,4,3,0,7,4,3,Dickens 
16,13,6,2,5,8,11,7,Dickens 
28,17,8,2,7,10,15,9,Dickens 
40,31,17,9,5,26,22,14,Dickens 
 
Fig 5.6 
 
The J48 classifier was trained by supplying a data file containing information from all 
8 authors.  Classification of texts was carried out by submitting a test file at the same 
time as the training data.  The test file contained sentence information from one piece 
of work for each author (N.B like the training file these had to include the authors 
name).  The J48 produces a confusion matrix, example shown in Table 5.4, showing 
how many sentences it assigned to each author for the 8 texts in the test file.  The 
author of the text was deemed to be that with the highest number of assigned 
sentences, if there was a draw then the text was not successfully assigned.  Thus to 
classify the 47 test documents 9 test files were submitted (See section 3.2 for 
information on the test documents) for every different training file. 
 
Four experiments were carried out using training sizes of 500,1000,5000 and 10000 
sentences, each experiment was then repeated using reduced error pruning.  All of the 
other J48 settings were left as default.  The results of the experiments are shown in 
table 5.5 and the data set with the highest overall accuracy is shown in table 5.6, with 
the highest number of sentences highlighted.  
 
Dickens Emerson Keats Milton Poe Shakespeare Stevenson Wilde  
106 196 102 78 118 384 47 79 Dickens 
54 115 32 29 39 50 42 23 Emerson 
36 39 47 38 36 67 18 36 Keats 
16 15 22 29 8 21 12 23 Milton 
67 85 35 55 50 68 45 40 Poe 
47 112 72 51 89 362 47 47 Shakespeare 
47 58 21 45 44 23 34 18 Stevenson 
31 3 12 13 1 4 4 20 Wilde 
Table 5.4 
 35
 
 
Training Size (Sentences) Overall Accuracy (No 
REP) 
Overall Accuracy (REP) 
500 0.510638298 0.510638 
1000 0.595744681 0.553191 
5000 0.510638298 0.553191 
10000 0.340425532 0.361702 
Table 5.5 
 
 
 Training Size 1000 Sentences/ All Habits Used 
 Dickens Emerson Keats Milton Poe Shakespeare Stevenson Wilde Correct
dickensComp1 109 175 81 83 143 335 88 96  
dickensComp2 91 123 88 54 140 361 76 105  
dickensComp3 56 90 43 30 88 117 55 59  
dickensComp4 182 236 166 121 252 596 161 198  
dickensComp5 198 290 173 135 275 489 178 193  
dickensComp6 410 259 180 364 229 101 262 123  
dickensComp7 198 254 187 177 247 451 211 206  
dickensComp8 194 280 188 161 254 435 211 208 1 
          
emersonComp1 51 91 35 23 43 45 57 39  
emersonComp2 51 63 23 15 37 10 49 23  
emersonComp3 99 119 66 44 68 75 112 56  
emersonComp4 41 65 39 24 35 33 53 26  
emersonComp5 50 72 38 26 38 27 44 34  
emersonComp6 62 105 48 43 47 44 90 42  
emersonComp7 86 92 43 25 53 60 60 41  
emersonComp8 89 140 53 54 82 55 101 39 11 
          
keatsComp1 42 30 34 42 49 54 30 36 0 
          
miltonComp1 19 17 18 31 10 11 19 21  
miltonComp2 34 25 39 42 25 32 38 24  
miltonComp3 18 16 16 21 4 13 10 17 3 
          
poeComp1 59 81 36 47 59 61 54 48  
poeComp2 98 100 80 57 97 118 85 60  
poeComp3 63 76 29 37 63 63 56 37  
poeComp4 52 58 28 28 44 53 23 34  
poeComp5 53 46 36 26 38 78 35 22  
poeComp6 52 60 52 26 61 93 49 52  
poeComp7 67 63 54 26 114 210 38 52  
poeComp8 54 54 39 27 36 29 51 19 0 
          
shakespeareComp1 48 95 76 57 115 329 47 60  
shakespeareComp2 74 123 109 91 132 456 84 112  
shakespeareComp3 58 112 84 50 112 429 69 71  
shakespeareComp4 68 102 100 40 132 439 61 87  
shakespeareComp5 24 68 64 32 84 276 35 49  
shakespeareComp6 43 88 69 42 83 273 32 56  
 36
shakespeareComp7 52 67 60 74 84 263 47 57  
shakespeareComp8 72 104 83 70 90 284 78 73 11 
          
stevensonComp1 65 65 23 34 24 16 40 24  
stevensonComp2 62 75 34 49 49 51 59 36  
stevensonComp3 106 126 57 74 78 58 103 49  
stevensonComp4 106 128 81 74 88 82 101 65  
stevensonComp5 211 199 127 118 119 135 174 114  
stevensonComp6 129 149 78 72 82 87 114 76  
stevensonComp7 73 93 45 40 58 107 60 57  
stevensonComp8 102 102 63 73 71 112 74 47  
stevensonComp9 112 146 75 75 68 80 122 68 0 
          
wildeComp1 10 6 11 23 2 1 8 27  
wildeComp2 15 13 14 21 13 19 10 27 2 
          
mean 0.59574         
Table 5.6 
 
 
The results produced using the cusum habits range from 34%-59% with the highest 
accuracy found at the 1000 sentence length.  With increasing training size the accuracy 
of a decision tree algorithm tends to increase, after a certain point no further gain in 
accuracy is made and sometimes the increasing size leads to a decrease in accuracy.  
This is because the decision tree has become over-fitted to the training data and is so 
specific it does not cover all situations that could occur (or slightly different styles of 
an author).  This is a possible cause of the decrease in accuracy of the experiments 
using training sizes above 5000 sentences.  The use of reduced error pruning results in 
minor fluctuations in the accuracy but does not give any overall advantage. 
 
As with the n-gram language model there are certain authors that are consistently 
easily identified (Shakespeare, Emerson and Milton).  Of all of the authors these tend 
to write in a very consistent style, and furthermore they are quite different from the 
other 5 authors.  As far as cusum habits are concerned Shakespeare and Emerson are 
quite distinct from the rest of the authors, with Shakespeare tending to have small habit 
counts while Emerson tends to have much larger habit counts.   
 
 
 
5.3 Zipping Experiments 
 
When using the zipping technique a text is classified by appending it to the training 
texts for all 8 authors.  These composite texts are then compressed and the difference 
between the size of the compressed composite texts and the compressed training text is 
calculated (The ∆).  The author for whom the smallest ∆ is generated is deemed to be 
the author of the text.  Thus the 47 test documents have to be combined with all 8 
training documents producing 376 different combinations. 
 
The experiment was carried out using two compression programs that utilise the LZ77 
compression algorithm.  To allow a comparison with results produced by Benedetto et 
 37
al (2002) one of the programs selected was gzip (ver 1.2.4).  The other program 
selected was cabinet manager 2000 (ver 2.5) to provide an internal comparison of 
LZ77 programs.  The programs were both run at their highest level of compression, the 
results are shown in tables 5.7 and 5.8. 
 
 
 
 
Gzip Compression Lvl 9   
∆ Dickens ∆ Emerson ∆ Keats ∆ Milton ∆ Poe ∆ Shakespear
e 
∆ Stevens
on 
∆ Wilde
dickensComp1 37,728 37,800 38,109 38,020 37,951 38,106 37,807 38,181  
dickensComp2 30,268 30,239 30,591 30,452 30,441 30,557 30,331 30,701  
dickensComp3 18,109 18,325 18,721 18,589 18,487 18,610 18,310 18,819  
dickensComp4 61,999 62,093 62,377 62,307 62,245 62,329 62,130 62,453  
dickensComp5 209,237 209,409 209,769 209,574 209,616 209,721 209,321 209,842  
dickensComp6 169,005 169,092 169,595 169,450 169,309 169,508 169,087 169,707  
dickensComp7 127,538 127,616 127,864 127,779 127,699 127,866 127,631 127,927  
dickensComp8 165,142 165,215 165,506 165,423 165,346 165,413 165,177 165,534 7 
         
emersonComp1 17,464 17,296 17,991 17,698 17,712 17,951 17,484 18,117  
emersonComp2 16,120 15,972 16,676 16,384 16,372 16,619 16,161 16,778  
emersonComp3 34,926 34,983 35,506 35,328 35,300 35,423 35,048 35,593  
emersonComp4 16,249 16,213 16,833 16,555 16,560 16,745 16,347 16,878  
emersonComp5 17,528 17,446 18,116 17,795 17,808 18,033 17,590 18,203  
emersonComp6 25,939 25,830 26,538 26,249 26,258 26,400 25,982 26,580  
emersonComp7 25,535 25,505 26,152 25,926 25,898 26,048 25,665 26,233  
emersonComp8 35,599 35,520 36,234 36,012 35,907 36,147 35,731 36,334 7 
         
keatsComp1 22,594 22,626 22,271 22,627 22,552 22,696 22,631 22,638 1 
         
miltonComp1 15,060 15,048 15,177 14,833 15,161 15,242 15,091 15,339  
miltonComp2 22,469 22,511 22,561 22,329 22,564 22,652 22,465 22,697  
miltonComp3 10,441 10,450 10,607 10,282 10,583 10,657 10,408 10,785 3 
         
poeComp1 24,141 24,263 24,666 24,520 24,327 24,569 24,269 24,724  
poeComp2 32,496 32,573 32,993 32,905 32,632 32,963 32,608 33,183  
poeComp3 21,474 21,616 21,875 21,837 21,577 21,892 21,564 22,006  
poeComp4 15,159 15,204 15,499 15,409 15,242 15,521 15,274 15,660  
poeComp5 15,638 15,617 15,807 15,786 15,605 15,882 15,701 15,962  
poeComp6 18,912 19,061 19,359 19,286 19,020 19,256 19,027 19,426  
poeComp7 18,978 19,077 19,061 19,122 18,993 19,066 18,993 19,205  
poeComp8 19,479 19,457 19,780 19,707 19,502 19,820 19,533 19,921 1 
         
shakespeareComp1 33,025 33,120 33,281 33,124 33,284 32,835 33,110 33,414  
shakespeareComp2 33,656 33,695 33,861 33,729 33,829 33,416 33,657 34,023  
shakespeareComp3 23,433 23,552 23,672 23,625 23,595 23,337 23,541 23,809  
shakespeareComp4 26,326 26,410 26,574 26,423 26,445 26,137 26,340 26,656  
shakespeareComp5 18,079 18,158 18,273 18,120 18,183 17,897 18,123 18,412  
shakespeareComp6 22,687 22,758 22,800 22,722 22,795 22,488 22,652 22,994  
shakespeareComp7 26,271 26,326 26,513 26,385 26,506 26,173 26,316 26,697  
shakespeareComp8 32,298 32,398 32,505 32,367 32,510 32,124 32,342 32,636 8 
 38
         
stevensonComp1 21,488 21,631 22,116 21,798 21,762 22,024 21,461 22,230  
stevensonComp2 27,481 27,607 28,184 27,852 27,864 28,046 27,428 28,224  
stevensonComp3 40,833 41,125 41,450 41,219 41,274 41,451 40,945 41,533  
stevensonComp4 43,054 43,335 43,681 43,477 43,539 43,641 43,104 43,773  
stevensonComp5 66,984 67,210 67,538 67,322 67,379 67,491 67,039 67,596  
stevensonComp6 40,787 41,076 41,462 41,206 41,300 41,394 40,874 41,584  
stevensonComp7 22,635 22,798 23,167 23,053 22,967 23,100 22,754 23,268  
stevensonComp8 32,915 33,033 33,403 33,306 33,238 33,325 33,031 33,469  
stevensonComp9 42,373 42,550 42,905 42,725 42,789 42,866 42,408 42,988 2 
         
wildeComp1 13,444 13,474 13,409 13,494 13,448 13,604 13,444 13,438  
wildeComp2 11,294 11,267 11,188 11,269 11,182 11,398 11,285 11,080 1 
         
Mean 0.6382979         
Table 5.7 
 
 
 Cabinet Manager   
∆ Dickens ∆ Emerson ∆ Keats ∆ Milton ∆ Poe ∆ Shakespear
e 
∆ Stevens
on 
∆ Wilde
dickensComp1 29891 31515 33887 33,099 31039 32827 30943 32135  
dickensComp2 24081 25617 27269 26659 25209 26429 25061 25909  
dickensComp3 14165 15247 16883 16229 14879 15895 14605 15633  
dickensComp4 48825 50949 54531 53239 50429 52743 50173 51997  
dickensComp5 161245 166947 174903 171933 166391 171097 162887 170983  
dickensComp6 129533 130915 141021 137891 131355 137283 131421 137573  
dickensComp7 99829 102547 109057 106829 101143 105843 101323 104761  
dickensComp8 128791 131791 138951 136525 130231 135439 130571 134295 8 
         
emersonComp1 14395 13257 16237 15217 14307 15411 14383 15477  
emersonComp2 12973 11997 15137 14117 13101 14229 13093 14263  
emersonComp3 28111 27079 32061 30653 28441 30839 28381 30387  
emersonComp4 13311 12773 15349 14495 13519 14513 13369 14509  
emersonComp5 14119 13581 16453 15523 14411 15463 14261 15429  
emersonComp6 21041 20119 24081 22883 21337 22887 21259 22727  
emersonComp7 20807 19675 23747 22561 20983 22651 21039 22495  
emersonComp8 28685 27295 32557 30973 28775 31345 28859 30855 8 
         
keatsComp1 19565 19733 18945 19487 19571 19749 19657 19635 1 
         
miltonComp1 12,923 13149 13557 12133 13119 13035 13009 13429  
miltonComp2 19363 19503 20247 18377 19705 19495 19439 20037  
miltonComp3 8969 8931 9511 8393 9131 9029 9039 9435 3 
         
poeComp1 19487 20015 22273 21499 18439 21321 19611 20849  
poeComp2 26383 26881 29387 28513 25037 28611 26609 27759  
poeComp3 17487 17861 19955 19155 16443 19147 17571 18785  
poeComp4 12239 12707 13915 13387 11605 13349 12349 13007  
poeComp5 12779 13033 14209 13765 12279 13701 12891 13491  
poeComp6 15865 16609 17327 17145 15217 16985 16035 16591  
poeComp7 15637 16197 17013 16555 15119 16273 15873 16375  
poeComp8 15803 16253 17659 17085 14889 17301 15883 16823 8 
 39
         
shakespeareComp1 28371 28785 29737 28925 28947 25787 28639 29255  
shakespeareComp2 28981 29421 30085 29313 29527 26381 29229 29793  
shakespeareComp3 20429 20985 21597 21263 20865 19173 20625 21125  
shakespeareComp4 22693 23277 23927 23523 23207 21151 22823 23405  
shakespeareComp5 15671 16161 16425 16017 15999 14329 15853 16251  
shakespeareComp6 19709 20039 20445 20047 20025 17895 19827 20301  
shakespeareComp7 22723 23145 23795 23227 23227 20613 22883 23517  
shakespeareComp8 27655 28219 29067 28319 28295 25553 27893 28641 8
        
stevensonComp1 17389 17685 19955 18883 17663 19067 16907 19135 
stevensonComp2 21989 22689 25465 24339 22835 24433 21211 24279 
stevensonComp3 32459 33777 36985 35597 33851 36031 31589 35693 
stevensonComp4 34053 35709 38891 37369 35621 37781 33109 37469 
stevensonComp5 53047 55103 59267 57421 55203 57823 51953 57691 
stevensonComp6 32299 33841 36871 35335 33869 35723 31483 35517 
stevensonComp7 18221 18979 20825 20121 18467 19797 17817 19345 
stevensonComp8 26509 27227 29889 28715 26399 28635 26015 28069 
stevensonComp9 33539 35257 38399 36911 35099 37197 32777 36823 9
        
wildeComp1 11825 12217 11755 11947 11913 12069 11851 11053 
wildeComp2 9823 9861 9725 9905 9789 9891 9847 9051 2
        
mean 1       
 
   
 
 
The results from the zipping technique are very interesting.  The gzip program 
produced an accuracy of 64% using the highest compression level.  The results 
produced share similarities with those produced by the J48 decision tree, again having 
difficulty classifying Poe and Stevenson although gzip successfully classifies Dickens. 
 
The 100% accuracy produced by the cabinet manager compression program is very 
impressive.  The ∆ values for the correct authors are quite often significantly smaller 
than those produced by the other authors. 
 
 
 
 
5.4 Discussion of Results 
 
 The results produced by the n-gram show that even with relatively small training sizes 
(11,000 characters) a reasonable level of accuracy can be achieved (70%).  Once the 
training size is increased to ~150,000 characters this accuracy reached 86% (for 6 
authors).  An accuracy of 91% was reached, for all 8 authors, using the maximum 
possible training and test size for each author.  The context length and vocabulary have 
been shown to have a large effect on the overall accuracy. 
 
Comparing the n-gram results gained here to those produced by Peng et al (2003) it 
becomes clear there is a large amount of variation possible even when the same corpus 
 40
is used.  Their highest overall accuracy (98%) was gained when using a 6-gram 
language model, while the highest gained here (91%) was produced using a 4-gram 
language model.  At the 1-gram level there is a larger discrepancy, Peng et al get an 
accuracy of  ~26% while here the accuracy is 85%.  It is hard to see that this accuracy 
difference could be entirely due to differing implementations of the n-gram technique, 
as at the 1-gram level there is very little scope for technique variation.  As mentioned 
earlier, this may in part be due to differences in the way the corpus was split into 
training and test sets.  What becomes apparent from this comparison is the level of 
testing that would be necessary before an n-gram model could be used for ‘real world’ 
authorship attribution.  It would be necessary to separate the corpus of work into three 
sections so that enough testing could be carried out to find the optimum n-gram level, 
the best vocabulary etc. 
 
There are a vast amounts of possible conditions that could effect the n-gram protocol, 
this gives rise to many different experiments that could be carried out.  This report has 
covered some of those aspects but there is much further work that could be done.  
Smaller training sizes could be used allowing for much larger context lengths.  A range 
of different vocabularies could be tested including those that cover less frequent 
characters.  A system similar to that used by Benedetto et al (2003) for their zipping 
protocol could be used.  This would involve using all of the texts in the training corpus 
separately rather than combining all texts together by each author.  This in turn would 
require the n-gram technique to work for relatively small training sizes.  Although the 
training texts could be combined into sections that are of the optimum size. 
 
 
The highest accuracy (59%) produced by the cusum technique is interesting.  It is hard 
to judge if this is an accurate interpretation of the value of the cusum habits.  The 
accuracy of the J48 decision tree seems to drop quite dramatically above the 5000 
sentences level.  This could be due to over-fitting of the decision tree to the available 
data.  This would results in a decision tree that models the training data very well but 
can not cope with even minor changes.  It is also possible that with an increase in data 
a diversity of habit frequencies are observed for some of the authors.  This would 
result in these habit measures producing no entropy gain, as they are effectively 
random. 
 
Again there are a wide variety of experiments that could be carried out using the 
cusum habits.  In order to gain a higher accuracy using the J48 decision tree a 
technique called boosting could be implemented.  This involves splitting the training 
data into smaller sections and using all of these to classify each text.  The different 
decision trees produced by these sections are then used to ‘vote’ for the most likely 
author of each test text (a process not dissimilar to Benedetto et al’s zipping method).  
This technique solves the problem of over-fitting by keeping all of the training sizes 
small. 
The cusum habits could also be analysed using a variety of other machine learning 
approaches, such as Naïve Bayes, neural networks etc. 
 
The zipping technique proved to be able to produce a high level of accuracy.  The gzip 
compression program produced an accuracy of 64%, which is quite a bit below the 
accuracy possible with the n-gram technique.  This correlates with results produced by 
Kukushkina et al (2001), where they get an accuracy of 61% using the gzip technique 
 41
while getting 84% using Markov Chains.  This could probably have been improved 
using the zipping technique of Benedetto et al (2002). 
The results produced by the cabinet manager program are very impressive, not only 
did it produce an accuracy of 100%, but in most cases there was also a large separation 
between the ∆ of the correct author and the other 7.  Quite why this program produces 
a higher accuracy than gzip is not apparent. Kukushkina et al showed that the rarw 
compression program could perform at the same level as Markov Chains, producing an 
accuracy of 87%.  cabinet manager may have a larger sliding window than gzip, 
allowing more of the training file to have an effect.  It certainly produces smaller 
compressed files than gzip, although this does not necessarily mean a higher level of 
accuracy.  There are programs that exist that can compress text files more efficiently 
than either of these programs, but would be no use for authorship attribution. 
 
Despite the accuracies possible with these compression programs there are a few 
problems.  The programs are slower to run than alternative methods, the cabinet 
manager program in particular, due to its GUI, required each file to be selected by 
hand.  This was acceptable for the 384 files that needed to be compressed for this 
project, but clearly is no use with Benedetto et al’s method, which would have required 
13121 compressions.  Of course this time requirement has to be compared with the 
initial coding time for the n-gram and cusum techniques.  Yet once this code has been 
generated it is much quicker to carry out a variety of experiments. 
The other problem with compression programs is that the code is not open source, 
making alterations and improvements impossible.  This means that at the moment the 
zipping technique is a black box approach.  This does not mean that it is of no use but 
it does not aid further progress.  
Benedetto et al suggest that as gzip has a sliding window size of 32Kb larger training 
files are of no use.  Thus an interesting experiment would be to find the optimum 
training size for a variety of compression programs. 
 
 
 
6 Conclusions 
 
The aim of this project was to compare three differing authorship attribution methods, 
this has been successfully carried out.  The highest overall accuracy’s obtained for the 
cusum, n-gram and zipping techniques were 59%, 91% and 100% respectively. 
 
What has become apparent during this study is that a set of stand-alone results can 
provide little insight into these methods.  While the 91% accuracy produced here for 
the n-gram technique is less than the 98% produced by Peng et al, if standard 
deviations are taken into account there is probably a 5% fluctuation in this result.  Thus 
it is not clear what size the real difference is between these results.  The same is true of 
the 100% produced by the zipping technique, if more experiments were carried out 
there will be a degree of fluctuation, thus is it really ‘better’ than the n-gram technique.  
In the authorship attribution area there are many papers testing different techniques.  
These papers test a corpus and show that a technique produced a certain overall 
accuracy.  Often no attempt is made to calculate the error in this accuracy and so it is 
of little value.  The corpus used in this report allowed for overall accuracy errors to be 
calculated for only a subset of authors.  It would have been ideal to provide errors with 
 42
all calculated accuracy’s, but this would have required a larger corpus.  Thus to 
properly compare the three methods discussed in this paper would have required a 
larger corpus and a range of experiments. 
 
The comparison of the n-gram results with those produced by Peng et al demonstrates 
how much variation is possible given a single corpus.  Thus is it good enough for two 
groups comparing techniques to just use the same corpus?  When Benedetto et al 
replied to Goodmans criticism they use the same 18828 corpus but select 200 texts 
randomly.  So even though the same corpus is used the two experiment are in effect 
quite different.  This again means that comparisons of methods carried out by separate 
researchers can not truly be compared.  What is needed is for a set of standardised 
corpora to be produced to allow comparison experiments.  This would involve a set of 
texts for which no processing is required, they have no headers to remove and are 
already split into training and test sets.  Until researchers start adopting certain 
standards no real comparisons can be made between techniques and protocols.  This is 
needed for the area to make progress, and more importantly to be able to gauge that 
progress.  Proper research is also needed if authorship attribution is ever going to be 
used in critical situations such as the court room, where the level of uncertainty for a 
forensic technique needs to be understood. 
 
 
 
 
 
 43
References  
 
 
Alex Catalogue http://www.infomotions.com/alex/
 
Barr, G.K.  “The Cusum Mechanism- A Review Of Analysing For Authorship By Jill 
M. Farringdon.”  Expert Evidence, 6:43-55, 1998. 
 
Benedetto, D., Caglioti, E. & Loreto, V.  “Language Trees and Zipping.”  Physical 
Review Letters.”  88(4): art. no. 048702, 2002. [Paper A] 
 
Benedetto, D., Caglioti, E. & Loreto, V.  “On J.Goodmans’s comment to Language 
Trees and Zipping”.  arXiv:cond-mat/0203275,1,2002. [Paper B] 
 
Benedetto, D., Caglioti, E. & Loreto, V.  “Benedetto, Caglioti, and Loreto Reply”.”  
Physical Review Letters.”  90(8): 089804-1, 2003. 
 
Blurock,E.S.“ID3Algorithm.”http://www.risc.unilinz.ac.at/people/blurock/ANALYSIS
/manual/document/node26.html
 
Brants, T.  “TnT - A Statistical Part-of-Speech Tagger.” In Proceedings of the Sixth 
Applied Natural Language Processing Conference ANLP-2000, Seattle, WA. 
 
Burrows, J.F.  “Word Patterns and Story Shapes: The Statistical Analysis of Narrative 
Style”, Literary and Linguistic Computing,2: 61-70, 1987. 
 
Burrows, J.  “ ‘Delta’: a Measure of Stylistic Difference and a Guide to Likely 
Authorship.”  Literary and Linguistic Computing, 17(3): 267-287, 2002. 
 
Fraringdon,  J.  “How to be a Literary Detective: Authorship Attribution.” 
http://members.aol.com/qsums/QsumIntroduction.html
 
Goodman, J.  “Extended Comment on Language Trees and Zipping”.  arXiv:cond-
mat/0202383. 1, 2002. 
 
Hilton, M.L. & Holmes, D.I.  “An Assessment of Cumulative Sum Charts for 
Authorship Attribution.” Literary and Linguistic Computing, 8(2):73-80, 1993 
 
Holmes, D.I.  “Stylometry: Its Origins, Development and Aspirations.”  
http://www.cs.queensu.ca/achallc97/papers/s004.html, 1997. 
 
Holmes, D.I., Robertson, M. & Paez, R.  “Stephen Crane and the New-York Tribune: 
A Case Study in Traditional and Non-Traditional Authorship Attribution.”  Computers 
and the Humanities, 35:315-331, 2001. 
 
Hoover, D.L.  “Statistical Stylistics and Authorship Attribution: an Empirical 
Investigation.”  Literary and Linguistic Computing, 16(4): 421-444, 2001. 
 
 
 44
Jurafsky, D. & Martin, J.H.  “Speech and Language Processing.”  Prentice Hall. 
ISBN 0-13-095069-6,  2000. 
 
Khmelev, D.V. & Tweedie, F.J.  “Using Markov Chains for Identification of Writers.”  
Literary and Linguistic Computing, 16(4):299-307, 2001. 
 
Khmelev, D.V. & Teahan, W.J.  “Comment on ‘Language Trees and Zipping’” 
Physical Review Letters, 90(8): 089803-1, 2003. 
 
Kukushkina, O.V., Polikarpov, A.A., & Khmelev, D.V.  “Using Literal and 
Grammatical Statistics for Authorship Attribution.  Problems of Information 
Transmission” 37(2): 172-184, 2001. 
 
Marcus, M.P., Santorini, B. & Marcinkiewicz, M.A.  “Building a Large Annotated 
Corpus of English: The Penn Treebank.” Computational Linguistics, 19(2): 313-330, 
1994. 
 
Peng, F., Schuurmans, D., Keselj, V. & Wang, S.  ``Language Independent Authorship 
Attribution Using Character Level Language Models'' 10th Conference of the 
European Chapter of the Association for Computational Linguistics, EACL2003 
 
Quinlan, R.  “C4.5: Programs for Machine Learning” Morgan Kaufmann Publishers, 
San Mateo, CA. 1993. 
 
Rudman, J.  “The State of Authorship Attribution Studies: Some Problems and 
Solutions.” Computers and the Humanities, 31: 351-356, 1998. 
 
  
 
 
 
 
 
 
 
 
 
 
 
  
 
 
 
 
 
 
 
 
 
 
 
 45
