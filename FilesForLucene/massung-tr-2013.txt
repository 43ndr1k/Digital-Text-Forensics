Reduction of High-Dimensional Feature Spaces with Supervised LDA
Sean Massung
CS 598jhm Project
Spring 2013
Abstract
We explore the use of supervised LDA
as a feature selection metric, and com-
pare it to two existing measures in the
text classification domain. Particularly, we
are interested in exploring feature selec-
tion in very high-dimensional space, as
current trends are exploring more com-
plicated structures. Grammatical parse
tree features are such structures, often-
times generating more features than even
n-grams of words. Combinations of mul-
tiple features have yet even higher dimen-
sionality, so this is our main focus. In this
context, we show that sLDA is a viable
candidate for feature selection, occasion-
ally even outperforming the two compari-
son methods.
1 Introduction
Feature extraction (also known as feature selection
or data dimensionality reduction) is a common
preprocessing step in many classification experi-
ments. Limiting the number of features has several
advantages: generalization (avoidance of overfit-
ting), lowered training time, and better model in-
terpretability.
In this paper, we explore a well-known super-
vised topic modeling algorithm to perform fea-
ture selection. We compare it against two standard
metrics: information gain and Chi square score.
We show that using supervised topic models to
perform feature selection is just as viable an op-
tion as existing methods.
We evaluate feature selection performance with
three text categorization tasks. Features used in
the text classification are both lexical and syntac-
tic; unigram and bigram words are used as the lex-
ical features and rewrite rules (Stamatatos (2009))
and annotated tree skeletons (Massung and Zhai
(2013)) are used as syntactic features extracted
from grammatical parse trees. Higher classifica-
tion accuracy is achieved when combining a sim-
ple lexical feature with a deep syntactic one (Kop-
pel et al. (2009)), so we focus on reducing the di-
mensionality of combined feature sets as well.
Topic models such as latent Dirichlet allocation
(LDA, Blei et al. (2003)) are considered dimen-
sionality reduction techniques. Imagine a fixed
collection of latent topics existing across docu-
ments; each document is composed of a unique
mixture of these global topics, where each topic is
a distribution over the corpus vocabulary. By re-
ducing each document to a mixture of topics, we
have transformed the large vocabulary space into
the much smaller topical one.
Supervised latent Dirichlet allocation (sLDA,
Blei and McAuliffe (2007)) adds a response vari-
able to each document. For example, the response
could be the nationality of an author, the grade on
an essay, or the sentiment of a movie review. Both
the topics and responses are modeled in sLDA in
order to predict the responses of unlabeled doc-
uments. sLDA can also be used to capture real-
valued responses for regression, but we only use
the discrete valued version in this work.
In the next section, we review some related
work exploring topic models as a dimensionality
reduction technique. Then, we explain the two
comparison methods, information gain and Chi
square. Next, we detail experiments and data sets,
followed by method evaluation. Lastly, we have
a brief discussion on the results, and list future
work.
2 Related Work
The original LDA paper explores feature reduc-
tion for two-class classification. LDA is run on the
corpus, and the top half-percent of features are se-
lected. Accuracy using this reduced feature set and
the entire feature set is compared using SVMs. It
is shown that there is minimal performance detri-
ment, and in some cases an increase in accuracy
when using the drastically smaller collection.
Another similar work to this paper is MedLDA
(Zhu et al. (2009)). While not an outright fea-
ture selection metric, MedLDA’s purpose is to
be used for classification. Whereas sLDA is
entirely likelihood-driven, MedLDA utilizes the
max-margin principle (used by SVMs) to separate
seen data in the most discriminative fashion. The
MedLDA paper compares MedLDA with sLDA
and LDA+SVM, with LDA+SVM meaning defin-
ing classes with LDA, and using SVM to classify
them. Again though, this is for the full classifica-
tion task, not considering feature selection.
From Lacoste-Julien et al. (2008), DiscLDA is
yet another LDA variant that is touted as useful for
classification. DiscLDA lies somewhere between
sLDA and MedLDA. It’s similar to sLDA in the
fact that its approach is mostly Bayesian, but sim-
ilar to MedLDA in that it tries to preserve more
predictive power for classification.
In reality, any of these methods (sLDA,
MedLDA, DiscLDA) could have been used in our
experiments, but we chose sLDA for its simplic-
ity, and because it was the first such topic model
to explore a response variable that could be useful
in classification.
Tasci and Gungor (2009) actually explores fea-
ture selection using LDA, and compares it to in-
formation gain and Chi square, just as we do here.
A main disadvantage, as noted in the paper, is that
the optimal number of topics to use for LDA is
unknown. With sLDA (and other supervised topic
models
3 Two Comparison Methods
Zheng et al. (2004) explains some common feature
selection metrics. Given the following variables
for a term t and a category ci we can define the
probabilities:
1. P (t, ci): presence of t, membership in ci
2. P (t, ci): presence of t, non-membership in ci
3. P (t, ci): absence of t, membership in ci
4. P (t, ci) absence of t, non-membership in ci,
Using these four probabilities, we can formulate
the two comparison methods.
3.1 Information Gain
Information is a commonly-used feature selection
metric in the machine learning and information
retrieval communities. It describes the difference
in entropy by knowing the presence or absence
of a specific term appearing in a class. It is
calculated as follows:
IG(t, ci) =
∑
c∈{ci,ci}
∑
t′∈{t,t′}
P (t′, c) log
P (t′, c)
P (t′)P (c)
3.2 Chi-Square
From statistics, Chi square measures the indepen-
dence of two random variables. In our case, the
random variables are a term appearing in a class.
χ2(t, ci) =
(P (t, ci)P (t, ci)− P (t, ci)P (t, ci))2
P (t)P (t)P (ci)P (ci)
4 Experiments
4.1 Data sets
We want to investigate whether sLDA can be con-
sidered as a viable feature selection method. In or-
der to do this, we compare it to two existing well-
known methods described in the previous section.
We run experiments using three different data sets:
The Ishikawa (2009) data set (“Nationality”)
consists of essays written in English by native Chi-
nese, Japanese, and English students. Essays were
classified by their writers’ native language. In at-
tempts to keep content uniform, each essay is a
response to one of two writing prompts: 1) It is
important for college students to have a part-time
job or 2) Smoking should be completely banned at
all restaurants in the country.
The Foundation (2012) data set (“Essay”) is
scored student essays on a range of 0 to 12. The
scores are an average of three human graders’
scores in an attempt to portray the most accurate
human judgement. For our task, we split the es-
says into three categories: good (scores 9 to 13),
OK (scores 5 to 8), and bad (scores 0 to 4).
The data set from Maas et al. (2011) consists
of 50,000 movie reviews from the International
Movie Database, classified as either positive or
negative. All movie reviews are scored out of 10,
but only clearly negative (score ≤ 4) or clearly
positive (score ≥ 7) are included in the data set
for data polarization. We used a subset of 10,000
reviews to speed up running times.
4.2 Features
We chose to explore three different feature sets for
evaluation.
1. Unigram and bigram words, with stop words
removed and stemming performed. The En-
glish Porter2 (Porter (2012)) stemmer was
used, and the stop words came from a list of
430.
2. Words + Rewrite rules from grammatical
parse trees (explained in more depth in Sta-
matatos (2009)). This can be thought of as a
collection of common subtrees per class.
3. Words + annotated structural skeletons from
grammatical parse trees (explained more in
depth in Massung and Zhai (2013)). In
short, annotated skeletons capture the struc-
tural properties of grammatical parse trees,
largely ignoring the syntactic category node
labels.
The word features are relevant due to their sim-
plicity and abundance in many text categorization
tasks. More deep, syntactic features are often cited
as complementary to more simple, lexical features
(Koppel et al. (2009)). Thus we’ve included two
interpretations of parse tree features to add to the
simple word model as is becoming increasingly
common.
Adding multiple features together greatly in-
creases the dimensionality of each corpus vocabu-
lary. Figure 1 shows the vocabulary sizes for each
feature set. We see that the Nationality data set is
rather small, with Essay significantly larger, and
Sentiment larger still, even though it has fewer
documents than Essay.
Feature Nationality Essay Sentiment
1,2-gram 42,939 296,734 653,400
1,2-gram + RR 46,558 310,866 672,476
1,2-gram + AS 99,169 850,173 1,233,119
Corpus Size 1,008 10,686 10,000
Figure 1: Number of documents and dimensions
for single and combined features on various data
sets. RR and AS stand for Rewrite Rules and An-
notated Skeletons respectively.
5 Evaluation and Discussion
We compare the three feature selection strategies
with an SVM baseline using all the features. SVM
is implemented with liblinear, as described in Fan
et al. (2008).
Then, running each feature selection algorithm
on the data, we sort the features based on their
judged effectiveness. We re-run the classification
using the top 1, 5, 10, 15, 20, and 25% selected
features. These experiments are carried out on all
three data sets using all three feature sets. Results
are displayed in Figure 2.
We see that for the Nationality and Essay data
sets, using sLDA for feature selection performs
equally with the two comparison methods. It
seems to perform consistently better than the other
two only for words + annotated skeletons on the
Essay data set.
On the other hand, sLDA performed consis-
tently worse on the Sentiment data set, always be-
low information gain and Chi square. The main
differences in this test are the large vocabulary size
and two-class (vs three-class) classification.
Except for using 1% of features, performance of
all three measures were within .5% to 1% accuracy
of each other on the first two data sets. On the
third, information gain and Chi square were within
.5% accuracy, with sLDA behind by an average of
1.5% to 2%.
It is also interesting to note that there was no
real performance benefit from feature selection on
the first two tasks; that is, the best feature selec-
tion resulted in accuracies within 1% of the base-
line. Conversely, information gain and Chi square
boosted performance by up to 2% for the senti-
ment analysis test, notably in words and words +
annotated skeletons features. This implies many
irrelevant features exist for this third category; an-
notated skeletons especially generate many fea-
tures when added on top of words, so it makes
sense that performance from feature selection was
the best when applied to this combination.
6 Conclusion and Future Work
We explored sLDA as a potential feature selection
metric, experimenting on a large amount of fea-
tures across three different data sets. Comparisons
were made with an SVM baseline and other fea-
ture selection measures information gain and Chi
square. On two data sets, sLDA performed equally
to the two comparison methods. On the third, it
Nationality Detection
Black: SVM baseline, green circles: information gain, red squares: Chi square, blue triangles: sLDA
Essay Grading
Black: SVM baseline, green circles: information gain, red squares: Chi square, blue triangles: sLDA
Sentiment Analysis
Black: SVM baseline, green circles: information gain, red squares: Chi square, blue triangles: sLDA
Figure 2: Classifier accuracy with different features.
performed worse, suggesting that while an inter-
esting proposal, it is probably better practice to use
the existing methods.
Although sLDA did underperform, reasons for
its performance can still be investigated. For ex-
ample, it is still unclear whether the vocabulary
size detrimented performance, or some other fac-
tor such as number of classes and features used.
Another possible extension would be to con-
sider performance gain using a classifier other than
SVM. Although SVM is considered to be state-of-
the-art, it is feasible that a different classifier may
be called for depending on the application.
References
David M. Blei and Jon D. McAuliffe. 2007. Super-
vised topic models. In NIPS’07.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet allocation. Journal of Ma-
chine Learning Research, pages 993–1022.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A li-
brary for large linear classification. J. Mach. Learn.
Res., 9:1871–1874, June.
The Hewlett Foundation. 2012. Asap: Automated stu-
dent essay prize, 5.
Shin Ishikawa. 2009. Vocabulary in interlanguage: A
study on corpus of english essays written by asian
university students (ceeaus). In Phraseology, cor-
pus linguistics and lexicography, Phraseology ’09,
pages 87–100.
Moshe Koppel, Jonathan Schler, and Shlomo Arga-
mon. 2009. Computational methods in authorship
attribution. J. Am. Soc. Inf. Sci. Technol., 60(1):9–
26, January.
Simon Lacoste-Julien, Fei Sha, and Michael I. Jor-
dan. 2008. Disclda: Discriminative learning for di-
mensionality reduction and classification. In NIPS,
pages 897–904.
Andrew L. Maas, Raymond E. Daly, Peter T. Pham,
Dan Huang, Andrew Y. Ng, and Christopher Potts.
2011. Learning word vectors for sentiment analy-
sis. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 142–150, Port-
land, Oregon, USA, June. Association for Computa-
tional Linguistics.
Sean Massung and Chengxiang Zhai. 2013. Structural
parse tree features for text representation. In review,
May.
Martin Porter. 2012. The english porter2 stemming
algorithm, 10.
Efstathios Stamatatos. 2009. A survey of modern au-
thorship attribution methods. J. Am. Soc. Inf. Sci.
Technol., 60(3):538–556, March.
S. Tasci and T. Gungor. 2009. Lda-based keyword
selection in text categorization. In Computer and
Information Sciences, 2009. ISCIS 2009. 24th Inter-
national Symposium on, pages 230–235.
Zhaohui Zheng, Xiaoyun Wu, and Rohini Srihari.
2004. Feature selection for text categorization on
imbalanced data. SIGKDD Explor. Newsl., 6(1):80–
89, June.
Jun Zhu, Amr Ahmed, and Eric P. Xing. 2009.
Medlda: maximum margin supervised topic mod-
els for regression and classification. In Proceedings
of the 26th Annual International Conference on Ma-
chine Learning, ICML ’09, pages 1257–1264, New
York, NY, USA. ACM.
