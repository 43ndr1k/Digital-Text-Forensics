Authorship Attribution: Statistical and Computational Methods 611Authorship Attribution: Statistical and Computational Methods
J Rudman, Carnegie Mellon University, Pittsburgh,
PA, USA
 2006 Elsevier Ltd. All rights reserved.
Introduction
Traditional authorship attribution studies attempt to
identify the author of an anonymous text by external
methods (e.g., a publisher’s payment records, ads in
contemporary newspapers) or a subset of available
internal methods (e.g., an author mentioning other
books that he or she has written, an author mention-
ing a spouse, or even a practitioner invoking impres-
sionistic stylistic evidence). If a rigorous traditional
study of an anonymous work is inconclusive, it
should be determined whether a valid nontraditional
study could be employed. Nontraditional authorship
attribution studies (a.k.a., ‘stylometry’ or ‘stylo-
metrics’) use the computer, statistics, and stylistics to
arrive at a statement of probability that a suspected
author wrote the anonymous work. The nontradi-
tional approach is based on the hypothesis that
every writer has a unique and verifiable style. It
is important to realize that every nontraditional au-
thorship study is different and demands a unique
experimental setup.Background
When – a Short History
Much of the following brief history is based on
the Delcourt (1994) article and other items in the
bibliography – specific references can be had there.
The quantitative description of texts goes at least as
far back as about 300 B.C. Saunaka, a Sanskrit gram-
marian, counted the number of syllables, words, and
stanzas in the Rig-Veda. This was done to aid in the
accurate preservation of the text. In about 180 B.C.,
Aristarchus of Samothrace listed ‘seldom used expres-
sions’ and ‘once used expressions’ in the Iliad, Odyssey,
and other Greek works. Aaron Ben-Asher, in about 950
A.D., counted the number of letters, words, verses, and
other, even more arcane style markers in the Hebrew
Bible.
Many similar studies were done down through
the ages. Then, in 1851, Augustus de Morgan stated
that word length might prove to be a unique marker
of an author’s style. This is generally accepted as the
beginning of nontraditional authorship attribution
studies. T. C. Mendenhall, in 1887, using word-length
distributions, conducted the first nontraditional
experiment to determine whether it was Bacon orShakespeare who wrote some disputed plays. There
was then a modest spate of activity. Lucius Sherman
(1888) worked on sentence length and punctuation as
authorship discriminators. Also in 1888, William
B. Smith (writing under the name of Conrad Mascol)
worked on various style markers in attribution stud-
ies. G. W. Gerwig (1894) worked on sentence-length
markers in authorship studies. Wincenty Lutoslawski
(1897) wrote about a ‘‘science of style’’ that would not
only help with chronology problems, but also with
‘‘questions of authenticity.’’ Robert E. Moritz (1903)
worked on various style markers in attribution stud-
ies. In Britain and Germany, classicists also began the
quantitative analysis of texts in the 19th century.
The early 20th century saw Andrej A. Markov
(1913) studying grapheme sequences, and Godfrey
H. Thomson and J. Ridley Thompson (1915) analyz-
ing vocabulary. In 1932, George K. Zipf published
the first of his important works. But it was not until
the late 1940s that the first large burst of activity
started and continued to the 1970s. To name a few
of these practitioners, there was George Yule, Gustav
Herdan, and C. B. Williams in the United Kingdom;
Pierre Guiraud in France; Louis T. Milic, Frederick
Mosteller, and David L. Wallace in the United
States; Wilhelm Fucks in Germany; Alvar Ellegård in
Sweden; and Herman Somers in Belgium. Then in the
1970s, with the increased availability of computers,
the number of studies took a quantum leap. Some of
these studies appear in the bibliography – but keep in
mind that over 1000 other publications do not.What and Why
Nontraditional authorship studies are interdisciplin-
ary both in their own domain (the computer, statis-
tics, and stylistics) and in ‘subject’ matter (the data).
The subject matter is drawn from many disciplines
and can be divided into two groups: group one, where
the results are important but not really critical – e.g.,
literature (in the broadest sense), political science,
and history; and group two, where the results are not
only important but potentially critical – e.g., religion,
the law (both law enforcement and the courts), and
the intelligence community.
The reason for doing an authorship attribution
study and the ramifications of the subsequent answer
are quite different for the two groups. The usual
reason for doing an attribution study on group one
is to determine an author’s canon, so that studies of
the author’s ideas, stylistic traits, literary techniques,
and ‘place’ in the discipline can be carried out. Each
definitive attribution study allows scholars to be
more precise and give valid, more meaningful insights
612 Authorship Attribution: Statistical and Computational Methodsinto the history of ideas. Group two, on the other
hand, presents more serious implications. What rami-
fications are presented to Christians in general if the
Pauline Epistles were not all written by St Paul? What
ramifications are presented to the Church of Jesus
Christ of Latter-day Saints if Joseph Smith is shown
to be the author of the Book of Mormon? Attribution
in the court room can be a matter of a guilty or not-
guilty verdict, prison or freedom, even life or death.
The FBI uses nontraditional authorship techniques
to aid in their investigations – e.g., the writings in
the Unabomber case and the note in the Jon Benet
Ramsey case. The Rand Corporation is using these
techniques to aid the intelligence community in their
work on counterterrorism.
Who
Nontraditional attribution studies demand much of
the practitioners. Ideally, they must be scholars in the
field of the anonymous text – not just in the language,
genre, and historical period, but also in the techniques
of the editors and publishers. And they must be able
to carry out a valid traditional attribution study. They
must be competent bibliographers, more than compe-
tent statisticians, and competent computer users with
a knowledge of programming. They must be ‘scien-
tists,’ versed in the scientific method, able to construct
and implement a valid experimental design.
In reality, very few practitioners meet these criteria.
Published practitioners range from undergraduates to
scholars of religion, history, literature, philosophy,
economics, linguistics, mathematics, statistics, and
computer science, and even to physicists, biologists,
librarians, and businessmen.
Many studies are done in collaboration. Yet, even
in collaboration, not only should all of the areas of
expertise be covered, but also every member of the
collaboration should have a good enough working
knowledge of every other area, so that meaningful
checks and balances can be employed. Enough time
and resources must be made available to do a valid
study. Shortcuts demanded by unrealistic time con-
straints and money constraints have relegated many
published studies to the all-too-large stack of ‘highly
questionable’ works. Too many reviewers are ineffec-
tual gatekeepers, foisting invalid nontraditional stud-
ies on an unsuspecting audience and casting doubt on
the nontraditional enterprise as a whole.
Where
It is vitally important that attribution researchers be
aware of the studies that have gone before. Too many
studies cover the same ground, make the same mis-
takes, and render themselves meaningless because
they are unaware of the problems that have been solvedand the pitfalls to be avoided. Doing the required bib-
liographic search is a daunting task, more so than in
any other discipline. There are hundreds of different
journals in a multitude of disciplines that have pub-
lished articles on nontraditional authorship attribu-
tion. In addition, there are even more journals in
more disciplines that have published pertinent per-
ipheral articles on style, stylistics, rhetoric, linguistics,
statistics, computational methods, and bibliographic
studies – and this is not an exhaustive list. And, al-
though this entry is emphasizing works that were pub-
lished in English, the practitioner must be aware
of and knowledgeable about the wealth of excellent
studies written in French, German, Italian, Russian,
Japanese, and many other languages.
Dissertations (doctoral, masters, and even under-
graduate) offer a rich source of background material
and innovative techniques. It is not enough to re-
search only the books about nontraditional studies,
but the practitioner should become aware of the
dozens of imbedded chapters and sections in books
whose titles give no indication of any nontraditional
attribution content. And, as in most disciplines, con-
ference papers give the most up-to-date information
and allow for valuable give and take.Methodology
Since there are separate articles in this encyclopedia
for corpus linguistics and forensic linguistics (see
Corpus Linguistics; Applied Forensic Linguistics)
this article concentrates on literary attribution that
compares an unknown writing to a suspect author
and to all of the other valid potential authors. The
use of corpus linguistics in authorship attribution
usually compares the unknown writing with a suspect
author by using the linguistic norms of a given popu-
lation. Forensic linguistics usually does likewise,
looking especially for linguistic anomalies that are
then shown to differ from not only linguistic norms
but from the usage by the other suspects in the case.
However, any practitioner working in any of these
areas must be aware of the methodology used by the
others – there is substantial and valuable overlap.
The computer has become so ubiquitous that its use
in all of the stages of nontraditional attribution, from
bibliographic research and editing, formatting, and
analysis of the study texts to formatting and publish-
ing of the final report, is a ‘given’ and not reported on
here. Be forewarned, however, that the technological
advances associated with the computer are so rapid
and far reaching that the practitioner must continually
upgrade his or her skills.
Before any nontraditional attribution study is
undertaken, an exhaustive traditional study must be
Authorship Attribution: Statistical and Computational Methods 613done. The nontraditional methodology is only a tool
for the traditional scholar – and surely not the most
important one.
There are four basic categories of nontraditional
authorship attribution studies:
1. An anonymous text with no idea of who the au-
thor is;
2. An anonymous text with two, three, or some other
small number of suspected authors;
3. An anonymous text that is the result of a collabo-
ration;
4. An anonymous text with one suspected author.
In addition to this fractionating, there are almost as
many methodological variations as there are studies.
Not all four types of study are treated here. The
following is a simplified outline of an ideal experi-
mental design for a category-four study that embraces
many techniques. It gives a general idea of what
should take place in nontraditional attribution stud-
ies. There is much more involved in a valid attribution
study. Not all of these steps are necessary for every
study – remember, every attribution study is different.
Because the field is so complicated, the following
concentrates on literary prose. More specifically, an
actual authorship problem: Did Daniel Defoe write
the anonymous 1705 political tract, A letter from
Scotland to a friend in London (Letter)?Get the Texts
All necessary texts must be identified and obtained.
As a rule, the printed edition closest to the final
holograph is to be used; every subsequent edition
has the potential to compound the systematic error
by allowing editorial and typographical interpola-
tions and errors. The following two are the most
important of several criteria to be used in selecting
the texts:
1. All of the texts must be the same genre as the
anonymous text. In our example, all of the texts
must be political tracts. It has been shown empiri-
cally that an author uses a different style for differ-
ent genres. Studies can be done, for example, to see
if an author’s style in political tracts is statistically
the same in closely allied genres, such as religious
or historical tracts. This has the potential to in-
crease the size of the data, thereby improving the
statistical probabilities.
2. All of the texts must have been originally pub-
lished in the same constricted time frame, e.g.,
plus or minus five years. It has been shown empir-
ically that an author’s style changes over time.
A few of the other criteria are length, gender, andsame ‘native’ language – in our example British
English.
These selected texts are divided into three
categories:
1. The anonymous text – the Letter;
2. The suspected author’s texts – every political tract
written by Defoe between 1700 and 1710.
A random sample of these texts are taken from
this sample and used as a control;
3. The control texts – (a) a subset of the suspected
author’s texts, i.e., the random sample just men-
tioned above, and (b) a significant random sample
of all of the other potential authors of political
tracts published between 1700 and 1710.
The ideal would be to include every text that passes
the criteria. It is a daunting task for even an expert in
18th century bibliography to identify, locate, and
obtain these texts. However, every text that is exclud-
ed, for any reason, increases the systematic error of
the study.
Each text is then prepared for analysis by unedit-
ing, deediting, and editing, e.g., deleting editorial
intervention, fixing obvious typesetting errors, delet-
ing quotes, deleting foreign languages, disambiguat-
ing (e.g., homonyms), ‘marking-up’ the text (i.e.,
tagging the text using a system such as that of the
Text Encoding Initiative), and in certain circum-
stances, lemmatization (see Rudman, 2005). This is
an especially vexing problem working with Defoe and
18th century literature – self-quoting, ‘imitation,’ and
plagiarism were the rule rather than the exception.
This is a tedious and time-consuming task; well over a
million words must be looked at with critical, schol-
arly care. All texts not already in a machine-readable
form are then scanned into the computer by an opti-
cal character reader, obtained from the Web, or typed
into the computer. Scanning the texts for our example
is impractical. The latest ‘error rate’ obtained in scan-
ning pre ~1750 texts is over 65%. Obtaining texts via
the Internet causes other problems, e.g., bad editions
and incomplete texts.
Style and Stylistics
Style and stylistics are separate entries in this encyclo-
pedia and it is necessary that the reader of this article
be familiar with their content (see Style; Stylistics).
A complete and accurate knowledge of style and sty-
listics is such a vital part of nontraditional attribution
studies that it is with some trepidation that I do no
more than point to these entries. As a general rule,
practitioners of nontraditional authorship attribution
use this operational definition of style: the combina-
tion of selected quantifiable stylemarkers. There are
614 Authorship Attribution: Statistical and Computational Methodstwo basic types of these stylemarkers: (1) those that
are taken from the author’s own words, e.g., word-
length distributions, type/token ratios, and percen-
tages of various function words; and (2) stylemarkers
that are not the author’s own words, e.g., quotations,
foreign phrases. Since the use of type-two stylemar-
kers entails a different kind of study, only type-one
markers are referenced in this entry.
Another hypothesis of nontraditional attribution
studies is that authors have a conscious control over
some stylemarkers while other markers are used un-
consciously. Most practitioners feel that the uncon-
scious stylemarkers (e.g., word-length correlation)
give more reliable results.
There are hundreds of thousands of quantifiable
type-one stylemarkers that have been identified. They
can be divided into four main categories: (1)
metaword – paragraph (e.g., length, first word of,
number of sentences per), sentence (e.g., length, word
order, types), phrases, clauses; (2) word – type/token
ratios, length distribution, length correlation, once-
used words, cooccurrences, function words, parts
of speech (e.g., distribution, ratio); (3) subword –
n-grams, syllables, prefixes, suffixes, phonemes,
morphemes, letters; and (4) other – punctuation, im-
agery, rhetorical devices (e.g., zeugma, chiasmus).
In order to show that a suspected author wrote the
anonymous text, it must be shown that the chosen
stylemarkers are used in a consistent way by the sus-
pected author, and not used in the same way by the
‘other suspected authors’ of the control group. Then
it also must be shown that the anonymous work uses
the same stylemarkers consistent with the suspect
author. The determination of which stylemarkers to
use is complex; in general, the more markers that pass
the selection criteria and are used, the ‘better’ (higher
probability) the answer. Putting this into the context
of our example of the Letter, we now want to identify
those stylemarkers that Defoe uses consistently in his
selected writings (minus the randomly selected subset
that is to be used as a control).Sampling
Sampling is used in two different ways in nontradi-
tional attribution studies. One has to do with the
intratext block size to be used in determining the
variation of a stylemarker within a text. This block
size is determined by the stylemarker that is being
used, which statistical test is being used, and the
length of the text. Given today’s technology, the entire
prepared text should be used. No block of text should
be excluded, even by random sampling. The second
way that sampling comes into play is in the selection
of the subset of the suspected author’s texts and in theselection of ‘all other possible authors’ control texts.
Sampling is not desirable in selecting these texts but
is, at times, necessary. It is very rare, if ever, that all of
the suspected author’s texts (in the same genre and
constricted time frame) cannot be used. And, al-
though a random sample of ‘all other authors’ texts
may be necessary, the ideal study would include every
text (same genre and constrained time frame) by
every author – the closer the sample comes to 100
percent of these authors, the ‘better’ the results.Statistics
Much of the following is based on the Holmes (1985)
article and other items in the bibliography. The statis-
tics employed in nontraditional studies run the gamut
from simple word counts and percentages, through
chi-square tests to multivariate analysis and the use of
‘learning’ programs. These include, for example, neu-
ral networks (that recognize the underlying organiza-
tion of data – a connectionist approach), genetic
algorithms (that ‘learn’ by generating a large number
of syntactically valid but semantically meaningless
rules), and support-vector machines (that use an opti-
mally defined surface that is typically nonlinear in the
input space, linear in a higher-dimensional space, and
used for classification, regression and data fitting,
and supervised and unsupervised learning). Every
practitioner who is not a statistician must possess a
level of statistical sophistication that would enable
him to at least carry on an intelligent two-way con-
versation with a statistician. Although the value of
the study does not go up with the sophistication of the
statistics, very simple statistics usually produce sim-
plistic answers. It is incumbent upon the statistician
to understand, employ, and communicate the assump-
tions behind each statistical test to be used. A statis-
tical technique that requires random input data
cannot use stylemarkers that were not produced ran-
domly. A test that requires multiple stylemarkers to
be independent cannot produce valid results if the
stylemarkers are not independent.
It is usually easier for a practitioner who is a statis-
tician to achieve an adequate level of expertise in
stylistics and bibliographical studies than for an ex-
pert in literature, history, or religion to achieve a com-
parable level in statistics. The Mosteller and Wallace
(1964) nontraditional authorship study of the Feder-
alist papers (arguably the best nontraditional study to
date) tried various classical methodologies but made
a case for the Bayesian approach. This technique had
the added advantage of being able to use the ‘best
guesses’ of traditional authorship practitioners as to
who is the author of an anonymous text as the prior-
odds input. Their statistically sophisticated study was
Authorship Attribution: Statistical and Computational Methods 615the most praised and most cited nontraditional study.
However, no practitioner has duplicated his or her
methodology on a different authorship problem. This
article cannot begin to explicate the mathematical
complexity of the statistics that can be used in non-
traditional authorship studies. There were many
books and articles like Brainerd’s (1974) that
attempted to do this.
The following is a simplified list of some of the
techniques that have been employed in the field: (1)
chi-square test, (2) frequency distributions (e.g., per-
centages, occurrences per block), (3) ratios (e.g., type/
token), (4) Simpson’s index (D), (5) Yule’s Character-
istic (K), (6) Waring-Herdan law, (7) Dolphin-Muller
law, (8) Zipf’s law, (9) Ule’s relative vocabulary over-
lap, (10) Morton’s Qsum, (11) Burrows’s Delta, (12)
z-test, (13) t-test (for independent samples), (14) t test
(for correlated samples), (15) f-test (e.g., analysis
of variance [anova]), (16) Markov chains, (17) phi
coefficient, (18) Spearman rank correlation coeffi-
cient, (19) Kendall rank correlation, (20) Pearson
product moment correlation, (21) entropy, (22)
Mann-Whitney U test, (23) Wilcoxon test, (24) factor
analysis, (25) discriminant analysis, and (26) cluster
analysis. Many variations of these tests also have been
employed.
Very few of these techniques have escaped criticism
from other statisticians. There are no universally
accepted statistical methodologies in nontraditional
authorship attribution. The use of multivariate meth-
odologies seems to show a consensus of acceptability
and preferability by present day practitioners.
The use of ‘packaged’ statistical programs poses
dangers to the unsuspecting nonstatisticians. It is not
always clear exactly what algorithms are used in the
‘black box’ or what assumptions are made about
the data. When all of the statistical manipulation is
finished, a complete and completely understandable
presentation of the results should be made. A picture
(or graph) is worth much more than 1000 tables
of numbers. A statement of probabilities has to
be accompanied by a meaning of the statistical
probabilities.
Returning to the Letter example with all of this in
mind, the practitioner now wants to identify those
stylemarkers that Defoe uses consistently in his pri-
mary sample of selected writings. This is done by
testing the thousands of potential stylemarkers on
the primary Defoe data. Say, for illustrative purposes,
that Defoe uses 500 stylemarkers in a consistent way
throughout his selected writings, e.g., every 1000
word block shows, within acceptable statistical
error, the same word-length distribution, the same
type/token ratio, the same various function-word dis-
tributions. These 500 markers are then tested againstthe ‘all other potential authors’ control sample. This
is done to weed out those markers that are zeitgeist-
driven or genre-driven. Again, for illustrative pur-
poses, say that 200 of these 500 stylemarkers are
genre- or zeitgeist-driven, i.e., the control group uses
these 200 markers in the same way (statistically) that
Defoe does. These 200 markers are discarded from
further study.
This is not to say that any of the remaining 300
stylemarkers are not used by any of the control group
authors in the same way as Defoe, but that none of
the other authors use all of these markers the same
way (statistically) as Defoe – an important distinc-
tion. The amount of this overlap correlates with the
probability of the answer. The next step is to test the
remaining 300 stylemarkers against the Letter. Does
the author of the Letter show the same statistical
stylemarker usage as Defoe? But before going to
an ‘answer,’ the Defoe subgroup control that was
randomly selected before analysis began is tested to
ensure that there was no ‘cherry picking’ of stylemar-
kers (even unconsciously) by the practitioner. If the
subsample control exhibits the same statistical results
as the main Defoe sample with all 500 stylemarkers,
the marker selection is validated. Now, only the final
probability (with experimental and systematic error
rates) remains to be calculated.
Keep in mind that this example of the Letter is the
most difficult type of authorship problem. If there are
only a few possible candidates for authorship, as in
the Federalist dispute, the ‘other potential author’
control group becomes more manageable and allows
for a ‘more reliable’ answer but to a different and
far easier problem: which of the limited number
of potential authors is the most likely to have written
the text.Conclusion
Nontraditional practitioners (and many others) look
upon their work as ‘science,’ with the concomitant
commitment to produce and follow a complete and
valid experimental design. However, there are many
skeptics who look at the field as pseudoscience or
simply ‘aspiration’ to become a science. The skepti-
cism (not entirely unjustified) is the result of many
factors, besides the ones mentioned earlier: (1) a lack
of consensus on methodologies, techniques, and
results; (2) many (if not most) practitioners allow
their work to be governed by expediency – it is easier
to take shortcuts than do a ‘complete’ experiment; (3)
a lack of expertise in all of the allied fields; and (4) a
lack of understanding of what is required and what
has been done in the field. In addition to these there
are the well-publicized ‘mistakes’ (e.g., the debunking
616 Authorship Attribution: Statistical and Computational Methodsof Morton’s Qsum methodology by England’s
Channel 4 program ‘Street-legal,’ and the much bal-
lyhooed Foster attribution of the Funerall elegye to
Shakespeare, with the subsequent proof that he was
in error – an error that Foster admitted in a retraction.
If the experimental design has any flaws, the entire
study is put in doubt. This is truly a case where the
chain is only as strong as its weakest link. Yet even
these flawed outcomes must be studied by practi-
tioners. And, if a practitioner publishes a flawed
study, it does not mean that all of his or her studies
are flawed; Erratum in unum does not mean erratum
in omnes. There are two important principles that the
practitioner must follow: (1) completeness – every
detail must be published or made available; and (2)
reproducibility – any other competent practitioner
must be able to reproduce the results.
Nontraditional attribution studies do not give a
‘yes’ or ‘no’ answer. Answers are expressed in statisti-
cal probabilities. This is not to denigrate the value of
these answers; the probabilities can approach certain-
ty the same way that DNA analysis does. The answer
can be presented to the ‘jury’ as being beyond a
reasonable doubt.
The future full acceptance of nontraditional
authorship studies as a legitimate discipline does not
lie with more-sophisticated statistical techniques; the
successful implementation of a neural net or support
vector machine cannot make up for faulty control
selection or faulty primary text preparation. Its future
lies with a rigorous adherence to valid experimental
designs.
Because of length restrictions, this article cannot
do the subject justice. I have tried to give the
curious reader and the serious researcher a good
idea of what nontraditional authorship attribution is
all about and how a fledgling practitioner might get
started.See also: Applied Forensic Linguistics; Computers in the
Linguistic Humanities: Overview; Corpus Linguistics;
Genre and Genre Analysis; Literature: Empirical Studies;
Style; Stylistics: Corpus Approaches; Stylistics.Bibliography
[Note: There are well over 100 different journals that pub-
lish articles pertaining to nontraditional authorship attri-
bution. The following two journals are worth noting for
the quantity and quality of these articles: Literary and
Linguistic Computing and Computers and the Human-
ities. The following 30 entries were selected from a bibli-
ography of well over 1000 items. They were selected for
their representativeness, importance, and/or historical
value to the field. This bibliography does not containthe seminal and other important works in style and sty-
listics by such giants as Yule, Milic, and Burrows or in
other component fields such as forensic linguistics by
practitioners like McMenamin and Chaski.
Binongo J N G (2000). ‘Stylometry and its implementation
by principal component analysis.’ Dissertation, Universi-
ty of Ulster.
Brainerd B (1974). Weighing evidence in language and
literature: a statistical approach. Toronto: University of
Toronto Press.
Burrows J (1995). ‘Computers and the idea of authorship.’
In Schreuder D (ed.) The humanities and a creative
nation. Canberra: The Highland Press. 89–108.
Burrows J (1999). ‘A computational approach to the
Rochester canon.’ In Love H (ed.) The works of John
Wilmot Earl of Rochester. New York: Oxford University
Press. 681–705.
Burrows J (2002). ‘Delta: a measure of stylistic difference
and a guide to likely authorship.’ Literary and Linguistic
Computing 17, 267–287.
Burrows J (2003). ‘Questions of authorship: attribution and
beyond, the Robert Busa award lecture.’ Computers and
the Humanities 37, 5–32.
Burrows J (2005). ‘Andrew Marvell and the ‘‘painter
satires’’: a computational approach to their author-
ship.’ Modern Language Review 100, 281–297.
Delcourt C (1994). ‘Stylometry.’ (Lecture notes of the tuto-
rial ‘Use and misuse of statistics in literary and linguistic
studies.’ Paris.) ALLC-ACH-École normale supérieure de
Lettres et Sciences humaines 4, 1–23.
Forsyth R S (1995). ‘A computational approach to text
classification.’ Dissertation, University of Nottingham.
Foster D (2000). Author unknown: on the trail of Anony-
mous. New York: Henry Holt and Company.
Hänlein H (1998). Studies in authorship recognition – a
corpus-based approach. Frankfurt am Main: Peter Lang.
Holmes D I (1985). ‘The analysis of literary style – a
review.’ Journal of the Royal Statistical Society. Series
A (General) 148(4), 328–341.
Holmes D I (1990). ‘Authorship attribution and the Book
of Mormon: a case study in stylometric techniques.’
Dissertation, University of London.
Holmes D I (1992). ‘A stylometric analysis of Mormon
scripture and related texts.’ Journal of the Royal Statisti-
cal Society. Series A (General) 155(1), 91–120.
Holmes D I (1998). ‘The evolution of stylometry in human-
ities scholarship.’ Literary and Linguistic Computing
13(3), 111–117.
Holmes D I, Gordon L J & Wilson C (2001). ‘A widow
and her soldier: stylometry and the American civil war.’
Literary and Linguistic Computing 16(4), 403–420.
Hoover D L (2001). ‘Statistical stylistics and authorship
attribution: an empirical investigation.’ Literary and
Linguistic Computing 16(4), 421–444.
Horton T B (1987). ‘The effectiveness of the stylometry of
function words in discriminating between Shakespeare
and Fletcher.’ Dissertation, University of Edinburgh.
Kenny A (1982). The computation of style: an introduction
for students of literature and humanities. Oxford:
Pergamon Press.
L
L
M
M
N
R
A
m
i
p
w
o
b
m
c
o
g
v
b
m
d
e
r
A
s
f
r
f
i
i
a
c
(
Autism and Asperger Syndrome: A Spectrum of Disability 617ebart Lu, Salem A & Berry L (1998). Exploring textual
data. Dordrecht: Kluwer Academic Publishers.
ove H (2002). Attributing authorship: an introduction.
Cambridge: Cambridge University Press.
orton A Q (1978). Literary detection: how to prove
authorship and fraud in literature and documents. New
York: Charles Scribner’s Sons.
osteller F & Wallace D L (1984). Applied Bayesian and
Classical inference: the case of the Federalist papers. New
York: Springer-Verlag (2nd Edition of Inference and dis-
puted authorship: the Federalist (1964)).
eumann K J (1990). The authenticity of the Pauline
epistles in the light of stylostatistical analysis. Atlanta,
Georgia: Scholars Press.
udman J (1998). ‘Non-traditional authorship attribution
studies in the Historia Augusta: some caveats.’ Literary
and Linguistic Computing 13(3), 151–157.Autism and Asperger Syndrome: A
H Cohen and S Rémillard, Université du Québec à
Montréal, Montréal, Canada
 2006 Elsevier Ltd. All rights reserved.
utism spectrum disorder (ASD) is a severe develop-
ental disorder, first labeled and described by Kanner
n 1943. The onset of ASD occurs before age 3 and
ersists throughout life. A male predisposition exists
ith a ratio of approximately 3 to 1. The prevalence
f this disorder is approximately 1 of every 1000
irths. According to the Diagnostic and statistical
anual, 4th edition (DSM-IV; American Psychologi-
al Association), and the International classification
f diseases, 10th edition (ICD-10; World Health Or-
anization), this disorder is part of the Pervasive De-
elopmental Disorders category and is characterized
y three main disabilities: (1) failure to develop nor-
al social interactions and relationships, (2) language
elay and communication disability, and (3) restrict-
d, repetitive, and stereotyped behaviors. Mental
etardation is frequently associated with autism.
pproximately 75% of individuals with autism have
evere mental retardation (also known as low-
unctioning autism; LFA), and the rest have mild
etardation or average intelligence (also called high-
unctioning autism; HFA). Approximately 10% of
ndividuals with ASD may have very low levels of
ntellectual abilities, but show excellent rote memory
nd possess isolated savant skills, such as calendar
alculations, exceptional musical or drawing abilities
Volkmar and Pauls, 2003).Rudman J (1998). ‘The state of authorship attribution stud-
ies: some problems and solutions.’ Computers and the
Humanities 31(4), 351–365.
Rudman J (2005). ‘Unediting, deediting, and editing in
nontraditional authorship attribution studies: with an
emphasis on the canon of Daniel Defoe.’ Papers of the
Bibliographic Society of America 99, 5–36.
Vickers B (2002). ‘Counterfeiting’ Shakespeare: evidence,
authorship, and John Ford’s Funerall elegye.’ Cambridge
University Press: Cambridge.
Wachal R S (1966). ‘Linguistic evidence, statistical infer-
ence, and disputed authorship.’ Dissertation, University
of Wisconsin.
Zimmer R A (1968). ‘The attribution of authorship: a
computerized method evaluated and compared to other
methods past and future.’ Dissertation, Michigan State
University.Spectrum of Disability
Individual differences in ASD are also observed in
language development. Some children with autism
never develop speech, whereas others become verbal-
ly fluent but still have problems with comprehension
and language use. As the manifestation of autism
varies in degree of severity and from one individual
to another, it is generally assumed that there is a
spectrum of autistic disabilities ranging from LFA to
HFA and Asperger’s syndrome (AS). The distinction
between LFA and HFA consists mainly of the fluency
and flexibility of expressive language skills (Lord and
Paul, 1997). LFA individuals may be mute or may
acquire only minimum speech characterized by echo-
lalia (the immediate repetition of what is said by
another person) without any apparent communica-
tion purpose. HFA individuals develop speech, but
tend to show idiosyncratic use of words and phrases
and have difficulty participating in conversation
(Bushwick, 2001). Asperger’s syndrome constitutes a
milder variant of the autism spectrum and, like the
autistic disorder, is characterized by social disability
and restricted stereotyped behaviors. However, AS
individuals do not show mental retardation and lan-
guage acquisition delay as in autism (Volkmar and
Klin, 2000). Consequently, AS is often not diagnosed
until late childhood or even adulthood. The preva-
lence of AS is higher than that of autism and is esti-
mated at 3 of 1000 births. Although the cause of ASD
is unknown, there is strong evidence for a genetic
contribution to the development of the disorder. The
concordance rate for monozygotic twins is very high
(i.e., over 90%; Volkmar and Pauls, 2003).
