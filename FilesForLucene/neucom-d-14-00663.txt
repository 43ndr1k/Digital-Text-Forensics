                                        Neurocomputing 
                                  Manuscript Draft 
 
Manuscript Number: NEUCOM-D-14-00663 
 
Title: Learning to classify short text from scientific documents using topic model and Support Vector 
Machine 
 
Article Type: Full Length Article (FLA) 
 
Keywords: Data sparseness, Information retrieval, Latent Dirichlet Allocation, Short text classification, 
Topic model 
 
Abstract: Classification of short text has been challenging because of data sparseness, which is a typical 
characteristic of short text. In this paper, we propose a method to enhance features using topic models, 
which make short text seem less sparse and more topic-oriented for classification. We have exploited 
topic model analysis based on Latent Dirichlet Allocation for enriched datasets, and then we presented 
a method to enhance features by combining external texts from topic models that make documents 
more effective for classification. In experiments, we have utilized the title contents of scientific articles 
as short text documents, and then enriched these documents using topic models from various types of 
universal datasets for classification in order to show that our approach performs efficiently. 
 
 
 
 
Learning to classify short text from scientific documents using topic model 
and Support Vector Machine 
 
Duc-Thuan Vo1 and Cheol-Young Ock2* 
1,2School of Electrical Engineering, University of Ulsan 
93 Daehak-ro, Nam-gu, Ulsan 680-749, Korea 
thuanvd@gmail.com, okcy@ulsan.ac.kr 
 
Abstract: Classification of short text has been challenging because of data sparseness, which is a typical 
characteristic of short text. In this paper, we propose a method to enhance features using topic models, which make 
short text seem less sparse and more topic-oriented for classification. We have exploited topic model analysis based 
on Latent Dirichlet Allocation for enriched datasets, and then we presented a method to enhance features by 
combining external texts from topic models that make documents more effective for classification. In experiments, 
we have utilized the title contents of scientific articles as short text documents, and then enriched these documents 
using topic models from various types of universal datasets for classification in order to show that our approach 
performs efficiently. 
Keywords: Data sparseness, Information retrieval, Latent Dirichlet Allocation, Short text classification, Topic 
model 
1. Introduction 
Text classification has been used as an important task of automatically sorting a set of documents into 
redefined categories where each document can be belonged to one or multiple classes. The task has 
several applications such as indexing documents, clustering web searches, filtering spams, and classifying 
products. With regard to long documents, a large amount of researches [5,14,19,21,26] based on bag-of-
words features combining with methods of cluttering, matching and ranking. Nigam et al. [21] applied 
Max-Entropy to measure similarities between documents, while Joachims [14] successfully demonstrated 
classifying documents by term frequency-inverse document frequency (tf-idf) using Support Vector 
Machine (SVM). These approaches focused on feature enrichment regarding to correlation of words 
occurring in documents. Recently, there has been an increase of short text documents which are different 
from long documents about their shortness and data sparseness, which hinders the application of 
conventional machine learning and text mining algorithms. In fact, short text documents are limited both 
words occurrences and context shares that lead to failures on classifying the sparse data accurately.  
With the rapid increase in the number of scientific documents such as proceedings papers and journals, 
classification of these documents is a constant task to assign them to one or more categories. In particular, 
titles of scientific documents are often brief as short texts. They contain succinct attributes about specialty 
* Corresponding author 
Manuscript
Click here to view linked References
in statistical reports, references, and citations. Short length and poor informative content lead to weak 
linkage to certain topics on these documents for classifying. Additionally, due to diversification of 
language, a same topic can be expressed in different ways; that reduces the possibility of a feature term 
occurrence in several different short texts. As a result, the accuracy of short text classification based on 
feature term co-occurrence is often reduced due to data sparseness. Classifying scientific documents 
based on their titles as short text is challenging because of data sparseness of occurrence of words. This 
discourages determining or classifying such documents.  
Recently, research on classification of short text has focused on two methods, combing external text 
into target documents and utilizing user-defined topics. First, combining external text into target short text 
for classifying [1,17,20,24] that enhance features in documents impacting on classification. The limitation 
of these researches is problematic in guaranteeing external text definition effectively. Second, the 
methods utilize user-defined topics to adapt into short text documents that make the relevant among 
sparse words. The performance of these researches achieved high accuracy in using universal dataset for 
user-defined topics. For instance, Phan et al. [22] and Chen et al. [6] proposed a successful topic model 
analysis [2] from large web corpus of Wikipedia in order to cluster short text from web searching and 
advertising.   
Using topic models from universal datasets, this paper proposes a method to classify scientific 
documents based on titles. We consider to enhance features in short text documents using topic models 
from various universal datasets, then to apply Support Vector Machine for classification. The main 
contributions of this work are as follows: 
 We propose a framework to classify short text documents; we focus on classifying scientific 
documents based on their titles, which seem as short texts.  
 We use three types of universal datasets as Computer Science Bibliography1 (DBLP), Lecture 
Notes in Computer Science2 book series (LNCS), and Wikipedia 3  to analyze topic models for 
enriching features in short text documents. 
 We propose two ways to enhance features in short text by (1) assigning topics from topic models as 
external features, and (2) combining external texts of adapted topics as external features. 
 We apply SVM, a robustness and power method in the training phrase for classification. We 
consider comparisons of the classification performance of short text documents using three types of 
universal datasets. 
                                                             
1 http://dblp.uni-trier.de/xml 
2 http://www.springer.com/computer/lncs 
3 http://www.wikipedia.org 
In experiments, we carefully use datasets with six types of sub-categories related to Computer Science 
domain from scientific documents. Those documents are enriched from three types of universal datasets 
based on topic models to gain the efficiency of performance on classification. The rest of the paper is 
organized as follows. Section 2 presents related work on text classification. Section 3 describes 
probabilistic topic model in which we focus on LDA. In Section 4, we present a general framework for 
short text classification. Section 5 discusses about topic model analysis using three types of universal 
datasets. Section 6 describes methods to enhance features in short text documents. In Section 7, 
experiments are applied to short text documents with 6 sub-categories related to Computer Science 
domain for classification. Last section ends with conclusions and future work. 
2. Related work 
Researches of text classification on long documents have commonly focused on feature extraction and 
feature reduction. Most traditional techniques concentrate on measuring the similarity of two documents 
based on the co-occurrences of words in them. Hunnisett et al. [13], Joachims [14] and Stamatatos et al. 
[28] proposed a method to extract features from texts to construct vector classifier. And, some others in 
reducing the feature vector are through the use of genetic computing [30]. These researches obtain 
relatively high accuracy in the case of long documents based on sharing common words, but it seems not 
for short text snippets because of its sparseness in feature extraction. Thus, researchers could possibly use 
new ways of incorporating linguistic knowledge into presentations of feature extraction, feature reduction 
and even classifying parts of a text classification engine. 
Classification of short text will always meet challenges if using same methods in long text to analyze 
data sparseness with respect to explore the feature correlation. Several methods of classifying, clustering 
and matching rely on short text snippets from search engines. Metzler et al. [20] evaluated a wide range of 
similarity measures for short queries from Web search logs. Yih et al. [29] explored Web-relevance 
similarity, and Sahami et al. [24] integrated external texts into target short texts to enhance features in the 
document. Long et al. [17] proposed a transfer classification method   to exploit the external data to tackle 
the data sparsity issue from short text that classifier will be learned in the original feature space. Bollegala 
et al. [3] used search engines to determine the semantic relatedness of words. Efron et al. [8] improved 
information retrieval for short text in Twitter based on aggressive document expansion, which helps in 
this content because short text yields little in the way of term frequency information. Meanwhile, 
Gabrilovich et al. [9] computed semantic relatedness using Wikipedia concepts for matching between 
short texts from the Web and search engines.  
Based on external topic-oriented, Cai [4], Letsche et al. [16] and Liu et al. [18] attempted to analyze 
topics using probability Latent Semantic Analysis, and they used both original data and generated topics 
in order to train and boost two different weak classifiers. The disadvantages of these researches are to 
extract topics only from the training and testing data. Sun et al. [27] selected the most representative and 
topical-indicative words from a given short text as query words, and then searched for a small set of 
labeled short texts best matching the query words. The predicted category label voted for the research 
results. While Phan et al. [22], Chen et al. [6] and Quan et al. [23] discovered hidden topics from external 
large-scale data collections which can make short text documents less sparse and more topic-oriented. 
They paid attention to process short texts and Web segments rather than normal text documents. 
Experiments showed that their methods using the discovered latent topics have achieved the state-of-the-
art performance.   
Some researchers have applied SVM for short text classification [6,15]. Chen et al. [6] applied 
datasets from [22] which are short text snippets from searching to leverage topics at multiple granularities. 
Le et al. [15] carried out short text classification of queries gathered from library searching. The 
Wikipedia was used as universal dataset for topic models in these researches. The proposed approaches in 
those researches are still limited such as solving noises after enhancing features from universal dataset, 
and lacking comparison between Wikipedia and other datasets. 
3. Probabilistic topic model 
In documents, words are related to each other in terms of synonymy, hypernymy, and hyponymy. 
Their co-occurrence in a document can be reasoned existence of certain relations definitely [25]. 
Deerwester et al. [7] presented Latent Semantic Indexing (LSI) as an indexing and retrieval method that 
uses a mathematical technique called singular value decomposition (SVD) to identify patterns in the 
relationships between the terms and concepts in an unstructured collection of text. Hofmann [12] 
introduced a new probabilistic topic approach extended from LSI, known as probabilistic Latent Semantic 
Indexing (pLSI). Although pLSI is a generative model of the documents, it is not clear how to assign the 
probability to a document outside the training set. Blei et al. [2] proposed Latent Dirichlet Allocation 
(LDA) based on pLSI model, the authors integrate into a Dirichlet prior, which has been pointed out to be 
more attractive than pLSI. We briefly discuss the principle of LDA in following section. 
3.1 Latent Dirichlet Allocation 
Blei et al. [2] proposed LDA as a generative probability model of a corpus that can be used to estimate 
multinomial observations using unsupervised learning. Given M documents = { , , … } with N 
unique words = { , , … }, we assume the collection contains Z hidden topics = { , , … }. 
For each word i in the document, we use ( )to present the corresponding word i, ( ) its document, and 
( ) its topic. In LDA, a document d is generated by picking a distribution of topics ( ) from a Dirichlet 
distribution Dir(α), which determines the topic assignment for words in that document. Then, the topic 
assignment for each word placeholder is performed by sampling a particular topic ( )  from the 
multinomial distribution(  ( ( )) ). Finally, a particular word ( ) is generated for placeholder by 
sampling from the multinomial distribution(  ( ⃗ ). This process is repeated until all K topics have 
been generated for the entire collection. The algorithm of LDA is as follows. 
1. Choose number of words n according to Poisson Distribution Poisson(ξ) 
2. Choose distribution over topics θ by  Dirichlet Distribution Dir(α) 
3. To characterize each of the n words ( ) 
a. Choose a topic ( ) ~ Multinomial(θ) 
b. Choose a word ( ) from ( ) ( ), , a multinomial probability conditioned on the 
topic ( ) 
Taking the product of the marginal probabilities of a document to obtain a new document d with n 
words is described as follows: 
 
( | , ) =  ( | ) ( ) ( ), ( )  
( )
 
where ( | ) is derived by the Dirichlet Distribution parameterized by α, and ( ) ( ),  is the 
probability of  ( )  under topic ( )  parameterized by β. The parameter α can be viewed as a prior 
observation counting on the number of times each topic is sampled in a document before we have seen 
any words from that document. The parameter β is a hyper parameter determining the number of times 
words are sampled from a topic before any words of the corpus is observed. Once the distribution is 
known, we know the probability of the whole corpus D by multiplying all of the above probabilities 
according to: 
 
( | , ) = ( )   
3.2 Gibbs Sampling for LDA estimation parameter 
The topic distribution ( ) ( ),  in Eq. (1) for estimation in LDA was explored by Gibbs 
Sampling [10,11,22]. Gibbs Sampling is a method to extract topics from the corpus, which is also used for 
distributing topics. Let ⃗ and ⃗ be the vectors of all words and their topic assignment of the whole data 
collection D. The topic assignment for a particular word depends on the current topic assignment of all 
(2) 
(1) 
other word positions. Specifically, the topic assignment of a particular word w is sampled from the 
following multinomial distribution as. 
 
( = |⃗ , ⃗) =  ,  
( ) +  
∑ + ( ) + − 1
 ,  
( ) + 
∑ + ( ) + − 1
 
where  ,  
( )is the number of times the word w is assigned to topic k except for the current assignment. 
∑ + ( ) + − 1 is the total number of words assigned to topic k except for the current assignment. 
,  
( )  is the number of words in set m assigned to topic k except for the current assignment. ∑ + ( ) +
− 1 is the total number of words in set m except the current word w. In normal cases, Dirichlet 
parameters ⃗ and ⃗ are symmetric. That mean all  (k = 1...K) are the same, and similar with  (v = 
1..V ). Two matrices are computed after finishing the Gibbs Sampling as follows: 
 
, =  
( ) +
∑ ( ) +
 
and  
 
, =  
( ) +
∑ ( ) +
 
4. General framework 
In this section, the entire of the proposed framework will be described as a classification system of 
short text documents. The framework is depicted in Figure 1 and consists of four major steps such as 
short text preprocessing, topic model analysis, feature enrichment and classifier. It is summarized as 
follows: 
(1) Short text preprocessing. Titles of scientific documents as short texts are performed to remove 
stop-words, function words and useless punctuation. We then extract features based on bag-of-words of 
such documents that will be served for feature enrichment in Step 3. 
 (2) Topic model analysis. We collect raw documents from DBLP, LNCS and Wikipedia then 
perform preprocessing for topic model analysis. The preprocessing steps also include removal of HTML 
tag, normalization, punctuation and stop word. We estimate topic model analysis based on LDA through 
Gibb Sampling on such knowledge datasets. This will be discussed more detail in Section 5. 
(3) 
(4) 
(5) 
(3) Feature enrichment. We exploit topic model analysis from Step 2 to enhance features in short 
text documents by (1) assigning topics from topic models as external features into documents, and (2) 
combining external texts of adapted topics as external features into documents.  The detail discussion for 
feature enrichment is in Section 6.  
(4) Classifier. Enriched data of short text is performed to construct vector classifier. We use Support 
Vector Machine for classification algorithm. The result of classification will be discussed in Section 7. 
 
Fig. 1 Proposed framework for short text classification 
5. Topic model analysis 
This section gives a detail description of topic model analysis from three types of universal datasets. 
We collect raw documents from DBLP, LNCS and Wikipedia then perform preprocessing as shown in 
Table 1. The preprocessing steps include removal of the HTML tag, normalization, punctuations and stop 
word. Three types of universal datasets are detail as follows: 
DBLP dataset. A computer bibliography lists more than two million of tracked articles on computer 
science in Dec 2012. Tracked articles are in most of journals and conference proceedings. We collected 
from DBLP XML dump then extracted title records. We filtered on records due to guarantee the length of 
documents be capable for LDA. 
LNCS dataset. LNCS is a series of computer science books that has been published by Springer 
Media  since 1973. LNCS reports research results in computer science, especially in the form of 
proceedings, post-proceedings and research monographs. We chose about 1200 International Standard 
Serial Number (ISSN) of LNCS and then collected abstract contents of 43,600 articles.  
Wikipedia dataset. Wikipedia, a collaborative Wiki-based encyclopedia, has become a huge 
phenomenon among Internet users. It covers a large number of concepts in a variety of fields and contains 
more than four million articles. We carry out to filter only in Wikipedia documents related to objective 
domain of document classification by using Wiki crawler with keywords searching. For Computer 
Science domain, we crawled about 50,000 Wikipedia documents, then gathered 42,000 articles for topic 
model analysis. 
We estimate topic models based on LDA through Gibb Sampling using GibbsLDA++4 on universal 
datasets. We optimize three important parameters α, β and number of topics T in the LDA. It is based on 
the topic number and the size of the vocabulary in the document collection. They are α = 50/T and β = 
0.01 respectively [11,22,23], with T is the number of topics in each model. Then we arrange topic models 
from 20, 30, ..., 100, 120 and 150. Analyzing topic models from three universal datasets took about 180 
hours. Several sample hidden topics of model 150 from three universal datasets shown in Figures 2a, 2b 
and 2c.  Universal datasets are very important for topic models analysis. We check number of some 
sample words existing in the domain "Natural Language Processing" in topic model 150 from three 
universal datasets in Figure 2d. The distributions of such words are quite different about quantity and 
topic position. For examples, the word "grammar" is in Topic66 of DBLP, in Topic52 of LNCS and in 
Topic93 Wikipedia. However the words "extraction" is in two topics Topic51 and Topic141 of DBLP, 
and one topic Topic139 of LNCS, but not in any topic of Wikipedia. Thus, exploiting such universal 
datasets for topic model analysis will cause differences of features improvement. We will discuss about 
the comparison of performance in experimentation section. 
 DBLP LNCS Wikipedia 
Raw 
documents 
2 Million records, 
Size: 932MB 
1200 book series with 43,600 
articles (abstract only),  
Size: ~ 250MB 
50,000 HTML documents,  
Size: ~ 920MB 
Final 
documents 
1,3 Million records, 
Size: 88MB 
43,600 articles, 
Size: 25MB 
42,000 articles, 
Size: 282MB 
Table 1 Universal knowledge datasets for topic model analysis 
                                                             
4 http://gibbslda.sourceforge.net 
 
 
 
Fig. 2 (a) Sample topics analyzed from topic model 150 of DBLP; (b) Sample topics analyzed from topic 
model 150 of LNCS; (c) Sample topics analyzed from topic model 150 of Wikipedia; and (d) Distribution 
of sample words in topic model 150 of three types universal datasets 
 
Fig. 3 Enhancing features in short text documents. (A) Matching topic models into short texts; (B) 
Selecting optimal number of the adapted topics; (C) Combining external text from adapted topics to short 
text documents 
6. Classification of short text document 
6.1 Enhancing features of short text documents 
To enhance features of short text documents, we match topics from topic models to all words of the 
documents. We consider two types of external features for enrichment by (1) selecting optimal number of 
adapted topics5 and (2) selecting optimal number of adapted topics then combine external words6 from 
adapted topics as depicted in Figure 3. 
                                                             
5 We call topics assigned into short text documents are adapted topics. 
6 A topic model contains list of topics, and each topic contains list of relationship words. 
Algorithm 1 Matching documents and topic models 
input: A document collection D={d1, d2, ...,dn} with di ={w1, w2, ...,wk} 
           A set of topic models T={t1,t2,...,tm} with tj={w1', w2', ..., wh'} 
output: updated D={d1, d2, ...,dn} 
 
for each di ∈ D do 
       for each wk ∈ di do 
               for each tj ∈ T do 
                       for each wh' ∈ tj do 
                              if Matching(wk,wh') then 
                                      add tj and value into wk   
                                      /* value is probability value of words wm' in topic tj 
                              end 
                        end 
                end 
       end 
end 
 
Algorithm 2 Selecting optimal adapted topics in word  
input: A document d ={w1, w2, ...,wn}with wi ={t1, t2, ...,tm}  
           number of optimal selected topic n 
           value β 
output: updated d ={w1, w2, ...,wn} 
 
for each wi ∈ d do 
       for each tj ∈ wi do 
               ( )=0; 
               sum = 0; 
                     for each wh ∈ d (wh ≠ wi) do 
                          for each tk ∈ wh do 
                                if matching(tj,tk) then     
                                       sum=sum+ βtk;      
                                end  
                           end  
                     end 
                ( ) = ( )+ sum;                             /*according to Eq. (6) */ 
       end 
       assign  ( ) into tj in wi 
end 
 
for each wi ∈ d do 
      select top n adapted topic tj in wi  based on P     /* according to Eq. (7) */ 
end 
 
We enrich features in short text documents using adapted topics. Let  ⃗ = { , … , }  and ⃗ =
{ , , … } be the vector of the topic model generated by LDA and the vector of the short text 
document. We first integrate the topic into the short text document, which is the process of merging topic 
(7) 
(6) 
models into short text documents described by ⃗  ∪  ⃗. The process of merging topic model and document 
is summarized in Algorithm 1. It is performed by matching between words in short text document and 
words in the topic model. A sample correspondence of topics from the topic model 150 assigned to short 
text documents with a certain probability value is shown in Figure 2A. Particularly, the word “estimation” 
of document d1 exists in topics 47, 48, 92 and 98 with probability values 0.034, 0.014, 0.133 and 0.055 
respectively. So these topics will be merged into the target word “estimation” in document d1. 
After assigning topics, the words in short texts adapted by topics can be presented as vector 
 ⃗( , , … ) with  as the adapted topic. However, the probabilities of different topics assigned in a 
target word are different, which causes increasing of spaces in short text documents. Hence, evaluating 
and selecting the quantity of adapted topics are necessary to enhance features and also to avoid diluted 
features. As in Figure 2A, the word “nonlinear” in document d1 is assigned by 5 topics as 105, 54, 142, 
32 and 20. Some adapted topics are quite homogeneous in their semantic relationship, but some others are 
different. Selecting the optimal number of assigned topics in the target word is based on the probability 
value of topics and how these topics are assigned to other words in the same document. Notice, the word 
“nonlinear” in document d1 is in Topic32 with distribution probability equal to 0.018, while in Topic54 
with probability value equal to 0.020. Furthermore, the word “nonlinear” is associated with the words 
“instantaneous” and “exponent” because they are in the same topic Topic32. Thus, assigning item 
“Topic_32” to the target word “nonlinear” in document d1 will be better for enhancing features than item 
“Topic_54”. To do this, in each target word we add the probability value of topics assigned to this word 
and the probability value of such topics assigned to other words in the same document according to the 
following formula. 
 
( ) =  ( ) + ( ) 
We continually determine the optimal number of topics for the target word.   
 
( )  
 
The distribution value ( ) of topic  assigned to each target word w is calculated by the probability 
value ( )  added to ∑ ( ) where, ( ) is probability value of the adapted topic  assigned to 
target word w directly and ∑ ( ) is the sum of probability values of this topic  assigned to other 
words in the same document. Topics assigned to the target word directly are more effective than such 
topics assigned to other words in the document. Thus, we set 0 ≤β≤ 1.0 for controlling the effect of 
adapted topics. ( ( )) is the order number of n topics ranked from the highest ( ) value to 
the lowest ( ) value. The process of selecting optimal adapted topics is summarized in Algorithm 2. 
Figure 2B shows the results of selecting the three best topics. For example, the word  “nonlinear” in 
document d1 is assigned by 5 topics as 105, 54, 142, 32 and 20, but the three best adapted topics are 105, 
32 and 142, which will be selected with n=3. In another example, the word “dynamic” in document d3 is 
assigned by 4 topics as 130, 46, 126 and 86, with n=3 the three best topics, 130, 126 and 46 will be 
selected. 
We enrich features in short text using external words from adapted topics. After selecting the number 
of best topics, we continue to combine external words from such adapted topics to short text. Notice each 
topic in the topic model is considered as a placeholder containing 20 topic-related words (see Figure 1). 
We will base word’s ranking of probability values and their existence in all short text documents. Figure 
2C displays three words belonging to the adapted topics with the highest probability values, which are 
combined into short text documents. For example, in document d1, the target word “estimation” will be 
combined with external words from adapted topics as Topic_92 with group words {motion, reduction, 
parameter},  Topic_98 with group words {frequency, phase,  signal}, and Topic_47 with group words 
{distribution, regression, probability}. 
6.2 Classification algorithm 
Many classification methods, such as k-NN, Decision Tree, Naive, and recent advanced models, like 
MaxEnt, can be used in our framework. Among of them we choose SVM because it is a powerful and 
robust method for text classification [14, 31]. It is employed to find a hypothesis to separate positive 
examples and negative examples with maximum margin. We also used SVM with a linear kernel and one-
versus-all-multi-class. In linear SVM, w * x – b = 0 represents the hyper-plane to define the distance from 
the hyper-plane to the nearest positive and negative examples with normal vector w and constant b. Thus, 
we convert documents into vector ⃗ = { , , … , } after enhancing features with  is weight of the ith 
feature in the vector ⃗ then calculate weight of tf-idf as.  
 
 
=  ×  log  
 
where M is the total number of documents in the collection, dfj is the document frequency, and tfij is 
simply the number of occurrences of feature vi in document dfj . Enriched short text documents of 
framework will be increasing of number for tf-idf presentations are necessary for classification. 
 
(8) 
7. Experimentation 
7.1 Experimental datasets 
We manually collected titles of scientific documents in journals and conference proceedings related to 
Computer Science domain via online publishing systems such as IEEEXplore7, SpringerLink8, ACM 
Digital Library9 and so on. We chose six sub-categories that cover most fields in Computer Science 
including Bioinformatics (Bio), Computer Architecture (CA), Database, Geographic Information System 
(GIS), Network, and Natural Language Processing (NLP) shown in Table 2. Those sub-categories do not 
overlap with each other. Numbers of documents are from 1300 to 1400 that guarantee the training/test 
datasets.  
Next, we annotated labels of classes by determining which classes belong to which categories for 
training phrase. Note that this task needs to be done by experts. In this study, we carefully annotate each 
document based on indexing of papers of journals or conference proceedings by publishers with an 
appropriate label. For example, in NLP category collected documents of proceedings at the NLP 
conference as "Association for Computational Linguistics 10" that indexed in the NLP field will be 
annotated positive label. While collected documents were indexed in other fields as Network in this 
category will be annotated negative label. 
We manually collected titles of scientific documents in journals and conference proceedings related to 
Computer Science domain via online publishers. We chose six sub-categories that cover most fields in 
Computer Science including Bioinformatics (Bio), Computer Architecture (CA), Database, Geographic 
Information System (GIS), Network, and Natural Language Processing (NLP) shown in Table 2. Those 
sub-categories do not overlap with each other. 
                                                                                          Table 2 Experiment datasets  
Categories Positive Negative Total 
Bioinformatics (Bio) 650 650 1300 
Computer Architecture (CA) 700 700 1400 
Database 700 700 1400 
Geographic Information System (GIS) 650 650 1300 
Network 650 650 1300 
Natural Language Processing (NLP) 700 700 1400 
Fig. 4 Evaluating optimal number of adapted topics                                                            
7.2 Experimental results 
We evaluate the impact of feature improvement in short text documents using topic models and 
                                                             
7 http://ieeexplore.ieee.org 
8 http://link.springer.com 
9 http://dl.acm.org 
10 http://www.aclweb.org 
without using topic models in experimental datasets. We enhance features in 6 sub-categories of short text 
documents using topic models from DBLP, LNCS, and Wikipedia. The distribution of words in topic 
models from three types of universal datasets impacts feature enrichment differently. Each category of 
short text documents is enriched with each topic model from DBLP, LNCS and Wikipedia then tf-idf 
weight is performed to serve for SVM classifier. Following Eq. (6) and Eq. (7) we evaluate number of 
adapted topics using topic models 50, 100, and 150 from DBLP in Bio category as shown in Figure 4. 
Thus, we set n=3 to define the number of optimal adapted topics for experiments.  
Experiments are applied to the use of two types of external features for enrichment as (1) external 
features are adapted topics, which are topics assigned into short text documents, and (2) external features 
are external words from adapted topics. The evaluation follows 5-fold cross validation schema. 
Experiments are applied in six cases as follows: 
 DBLP-1: using external features as adapted topics from topic models of DBLP. 
 LNCS-1: using external features as adapted topics from topic models of LNCS. 
 Wikipedia-1: using external feature as adapted topics from topic models of Wikipedia. 
 DBLP-2: using external features as external words in adapted topics from topic models of DBLP. 
 LNCS-2: using external features as external words in adapted topics from topic models of LNCS. 
 Wikipedia-2: using external features as external words in adapted topics from topic models of 
Wikipedia. 
Table 3 System accuracy (%) of short text classification in DBLP-1 
Categories Original Top20 Top30 Top40 Top50 Top60 Top70 Top80 Top90 Top100 Top120 Top150 
Bio 75 65.39 70 80.77 74.23 76.15 77.69 72.31 80 82.3 68.85 80.77 
CA 73.57 76.79 73.93 80.71 80 84.29 83.21 86.79 79.64 81.43 76.07 82.26 
Database 73.57 81.42 72.14 75.71 78.93 79.29 80.36 80.71 81.07 83.21 81.79 81.07 
GIS 68.85 78.46 63.43 68.85 74.23 67.31 68.85 73.46 72.08 70.38 74.23 80 
Network 61.54 63.85 63.46 65 63.85 62.3 63.45 64.6 62.69 63.85 61.92 63.08 
NLP 77.14 89.29 89.64 77.14 78.21 73.93 86.07 78.57 86.42 83.93 86.43 85.36 
Overall 71.61 75.87 72.10 74.70 74.91 73.88 76.61 76.07 76.98 77.52 74.88 78.76 
Table 4 System accuracy (%) of short text classification in LNCS-1  
Categories Original Top20 Top30 Top40 Top50 Top60 Top70 Top80 Top90 Top100 Top120 Top150 
Bio 75 83.76 78.08 77.69 83.08 78.08 80.38 80.77 76.54 80.38 80.02 81.39 
CA 73.57 71.43 70 77.86 73.58 81.07 81.42 83.57 80 81.07 86.07 85.36 
Database 73.57 72.86 71.43 74.26 81.07 79.26 81.07 81.07 83.21 72.5 78.2 80.7 
GIS 68.85 61.15 64.23 65 69.61 71.92 70.38 65.38 70.38 70.38 73.07 63.85 
Network 61.54 63.46 63.08 63.85 64.62 63.46 65 63.46 62.31 62.69 66.15 65.38 
NLP 77.14 88.57 87.5 89.29 91.07 81.07 86.07 83.21 84.64 78.93 73.36 85.71 
Overall 71.61 73.54 72.39 74.66 77.17 75.81 77.39 76.24 76.18 74.33 76.15 77.07 
 
For enhancing features using external feature as term of adapted topics, Table 3 shows performance 
results using DBLP-1. The column "Original" indicates the system accuracy without using topic model. 
The bold numbers show the best performance in each category. For instance, the Bio category had the 
highest accuracy of 82.3% using topic models 100. The CA category obtained accuracy of 86.79% 
referencing Topic 80. The best overall system accuracy achieved 78.76% in topic model 150. However, 
the overall system accuracy is only 71.61% without using topic model. Similarly, the system performance 
using LNCS-1 is described in Table 4. The best overall of system accuracy is 77.39% in topic model 70. 
Finally, Table 5 shows the performance result using Wikipedia-1 and the best overall system accuracy is 
76.56% in topic model 50. 
Table 5 System accuracy (%) of short text classification in Wikiepdia-1 
Categories Original Top20 Top30 Top40 Top50 Top60 Top70 Top80 Top90 Top100 Top120 Top150 
Bio 75 76.15 73.85 84.23 74.23 75.77 84.23 83.46 79.23 79.23 78.08 78.46 
CA 73.57 77.5 71.07 75.36 80.36 77.86 83.21 65.36 67.86 72.5 72.86 67.5 
Database 73.57 70.71 71.43 73.21 74.29 75.71 69.64 71.78 74.64 72.85 72.14 72.14 
GIS 68.85 69.61 78.46 71.53 78.85 68.46 68.08 84.23 81.53 72.69 81.15 84.61 
Network 61.54 61.92 62.3 62.3 63.08 62.69 61.15 62.3 63.08 63.46 61.15 63.46 
NLP 77.14 90.71 85.35 87.5 88.57 86.79 82.5 87.14 79.28 81.43 88.21 85.35 
Overall 71.61 74.43 73.74 75.69 76.56 74.55 74.80 75.71 74.27 73.69 75.60 75.25 
Table 6 System accuracy (%) of short text classification using topic models from DBLP-2 
Categories Original Top20 Top30 Top40 Top50 Top60 Top70 Top80 Top90 Top100 Top120 Top150 
Bio 75 62.3 75.77 76.54 81.54 71.54 82.31 78.45 77.31 77.69 79.62 79.62 
CA 73.57 72.14 70.71 82.86 76.43 85.36 84.29 83.21 83.21 86.79 84.64 85 
Database 73.57 75.71 71.79 76.07 76.79 81.07 80.71 74.64 78.57 81.79 81.79 79.29 
GIS 68.85 75.38 63.08 65.77 79.23 59.23 66.15 80 77.69 59.62 72.69 79.62 
Network 61.54 69.62 70 74.62 70.77 66.54 68.08 73.08 69.23 69.23 69.62 64.62 
NLP 77.14 58.57 82.14 82.86 78.93 78.56 80 78.57 73.57 80.36 83.21 86.07 
Overall 71.61 68.95 72.25 76.45 77.28 73.72 76.92 77.99 76.60 75.91 78.60 79.04 
Table 7 System accuracy (%) of short text classification in LNCS-2 
Categories Original Top20 Top30 Top40 Top50 Top60 Top70 Top80 Top90 Top100 Top120 Top150 
Bio 75 73.85 75 72.69 76.54 63.85 78.08 77.69 66.15 77.31 74.62 78.46 
CA 73.57 79.64 76.79 78.57 74.29 75.71 69.29 83.93 81.43 85.71 83.93 77.5 
Database 73.57 71.43 66.07 74.64 82.5 78.93 72.86 78.93 81.43 73.57 79.29 80 
GIS 68.85 58.85 58.85 63.46 72.3 66.15 66.15 70.38 73.85 65.39 70.77 65.38 
Network 61.54 65.77 63.08 63.85 66.54 65.77 65.38 65.77 64.62 64.62 70.38 66.15 
NLP 77.14 77.5 76.79 83.21 86.07 83.21 86.43 82.14 86.79 77.14 80 90.71 
Overall 71.61 71.17 69.43 72.74 76.37 72.27 73.03 76.47 75.71 73.96 76.50 76.37 
Table 8 System accuracy (%) of short text classification in Wikipedia-2 
Categories Original Top20 Top30 Top40 Top50 Top60 Top70 Top80 Top90 Top100 Top120 Top150 
Bio 75 81.15 81.54 70 86.92 77.3 69.23 82.3 76.92 82.69 82.69 83.45 
CA 73.57 81.79 76.79 79.64 84.64 82.86 83.57 62.86 68.93 67.86 65.71 69.28 
Database 73.57 65 68.57 73.57 71.43 76.79 69.64 73.21 71.43 70.71 74.29 72.14 
GIS 68.85 67.69 70.77 62.69 68.46 60 57.31 78.08 73.08 65 76.54 82.69 
Network 61.54 61.54 61.92 62.69 67.69 66.54 62.31 65.77 67.31 64.61 62.69 67.31 
NLP 77.14 79.64 76.43 85.71 81.43 82.86 76.79 78.21 72.86 82.14 84.64 85.36 
Overall 71.61 72.80 72.67 72.38 76.76 74.39 69.81 73.41 71.76 72.17 74.43 76.71 
 
For enhancing features using external features as external words in adapted topics, Table 6 reports 
performance results using DBLP-2. The bold numbers also show the best performance in each category. 
For instance, the Database category obtained the highest accuracy of 81.79% using topic model 100. The 
NLP category obtained accuracy of 86.07% referencing Topic 150. The best overall system accuracy is 
achieved at 79.04% in topic model 150. But, the overall system accuracy is only 71.61% without using 
topic model. Similarly, the system performance using LNCS-2 is described in Table 7. The best overall of 
system accuracy is 76.50% in topic model 120. Finally, Table 8 shows the performance result using 
Wikipedia-2 and the best overall system accuracy is 76.76% in topic model 50. 
7.3 Evaluation Comparison 
7.3.1 Category Comparison 
In this Section, we discuss on performance comparison of short text classification on each category. 
Figure 5 depicts the comparison using DBLP-1, LNCS-1, and Wikipedia-1 on each category. A 
comparison of performance on Bio category is shown in Figure 5a. The best system accuracy is 84.23% 
using topic model 70 from Wikipedia-1, 83.76% using topic model 20 from LNCS-1, and 82.3% using 
topic model 100 from DBLP-1. However, without using topic model, the system accuracy is only 75% 
that indicate better performance when using topic models for feature enrichment. Figure 5b shows the 
best system accuracy is 86.79% using topic model 80 from DBLP-1 in CA category. In others, the system 
accuracy is 86.07% using topic model 120 from LNCS-1 and 83.21% using topic model 50 from 
Wikipedia-1. But, without using topic model, the system accuracy is only 73.57%. Figure 5c shows the 
performance in Database category where the highest system accuracy obtained is 83.21% using both topic 
model 100 from DBLP-1 and topic model 50 from LNCS-1. And, the system accuracy has achieved 81.79% 
in both using topic models 100 and 120 from DBLP-1. Otherwise, the system accuracy is 75.71% using 
topic model 60 from Wikipedia-1. Figures 5d, 5e and 5f report comparisons on the remaining categories, 
GIS, Network and NLP. 
Figure 6 shows the performance comparison using DBLP-2, LNCS-2, and Wikipedia-2. For Bio 
category, a comparison of performance is presented in Figure 6a. The best system accuracy is 86.92% 
using topic model 50 from Wikipedia-2, 82.31% using topic model 70 from DBLP-2, and 78.46% using 
topic model 150 from LNCS-2. However, without topic model, the system accuracy is only 75% that 
indicates better performance when using topic models for feature enrichment. Figure 6b shows the best 
system accuracy is 86.79% using topic model 100 from DBLP-2 in CA category. In others, the system 
accuracy is 85.71% using topic model 100 from LNCS-2 and 84.64% using topic model 50 from 
Wikipedia-2. However, without using topic model, the system accuracy is only 73.57%. Figure 6c shows 
the performance in Database category where the highest system accuracy obtained is 82.5% using topic 
model 50 from LNCS-2. And, the system accuracy has achieved 81.79% in both using topic models 100 
and 120 from DBLP-2. Otherwise, the system accuracy is 76.79% using topic model 60 from Wikipedia-
2. The comparisons of the remaining categories, GIS, Network and NLP are shown in Figures 6d, 6e and 
6f.   
 
Fig. 5 Performance comparison between DBLP-1, LNCS-1, and Wikipedia-1. (a) Bio category; (b) CA 
category; (c) Database category; (d) GIS category; (e) Network category; (f) NLP category 
 
Fig. 6 Performance comparison between DBLP-2, LNCS-2, and Wikipedia-2. (a) Bio category; (b) CA 
category; (c) Databasecategory; (d) GIS category; (e) Network category; (f) NLP category 
7.3.2 Overall comparison 
In this Section, we discuss on overall performance of short text classification based on three types of 
universal datasets shown in Figure 7.  
Figure 7a presents the overall performance using DBLP-1, LNCS-1, and Wikipedia-1. The highest 
system accuracy is 78.76% using topic model 150 of DBLP-1. The system accuracies in the most cases of 
enriched short texts are better than original performance without using topic model (71.61%). The curve 
of DBLP-1 indicates the system accuracies have achieved the better performance using topic models 90, 
100 and 150 in overall comparison than others. While the curve of LNCS-1 shows the system accuracy in 
overall is better in using topic model 50, 60, 70 and 80. And using Wikipedia-1 the system accuracy only 
is achieved better overall performance in topic model 30 and 40. In highest cases, system accuracies are 
78.76% and 77.38% using DBLP-1 and LNCS-1, and 76.56% using Wikipedia-1.  
 
 
Fig. 7 Overall comparison. (a) Performance comparison between DBLP-1, LNCS-1, and Wikipedia-1; (b) 
Performance comparison between DBLP-2, LNCS-2, and Wikipedia-2 
In using DBLP-2, LNCS-2, and Wikipedia-2, the overall comparison is depicted in Figure 7b. The 
highest system accuracy has obtained 79.04% in using topic model 150 of DBLP-2. The curve of DBLP 
indicates the system accuracies have achieved better performance using topic models from 40 to 150 in 
overall comparison than others. While, the curve of LNCS-2 shows the system accuracy in overall is 
better than Wikipedia in using topic models from 70 to 120. In using Wikipedia-2, the system accuracy 
has just achieved better overall performance in topic models 20 and 30 than others. In sum, DBLP-2 is 
better than LNCS-2 and Wikipedia-2 in comparison of the highest cases. The highest system accuracies 
have achieved 79.04% and 76.76% in using DBLP-2 and Wikipedia-2, and 76.50% in using LNCS-2. 
Performance comparisons between two types of external texts for feature enrichment are shown in 
Figure 8. Figure 8a shows the overall comparison using DBLP-1 and DBLP-2. The best system accuracy 
is 79.04% using topic model 150 from DBLP-2. The curve of DBLP-2 is better than the curve of DBLP-1 
in topic models 40, 50, 70, 80, 120, and 150. Figure 8b shows a comparison of performance using LNCS-
1 and LNCS-2. The highest system accuracy achieved 77.17% using topics 50 from LNCS-1. The curve 
of LNCS-1 shows the results better than LNCS-2 in topic models 20, 30, 40, 60, 70, 90, 100 and 150. 
LNCS-2 only obtained better results in topic models 80 and 120. Most of cases of both LNCS-1 and 
LNCS-2 obtained better results than the original performance except topic model 20 and 30 of LNCS-2. 
For comparison through the use of Wikipedia-1 and Wikipeida-2 shown in Figure 8c, Wikipedia-1 is 
better than Wikipedia-2 in topic models 20, 30, 40, 60, 70, 80, 90, 100, and 120. However, Wikipedia-2 
obtained the highest results in using topic models 50 and 150. In comparison with original performance, 
most of cases in using Wikipedia-1 achieved higher results, whereas Wikipedia-2 obtained lower results 
on topic models 70 and 90. 
The experimental results are different from those using topic models from various universal datasets. 
This indicates that the number of adapted topics and their external words affect assessing short text 
documents using different topic models. However, the results show that the effect of number adapted 
topics from each topic model does not cause a significant change in the overall performance. Therefore, 
we conclude that the number of topics should be large enough to discriminate among differences in terms 
in short text documents. When the number of topics is large enough, the performance of the overall 
system is quite stable. 
7.4 Discussion 
In experiments, our approach has obtained a remarkable improvement in the accuracy of short text 
classification. The experiments show that enhancing features will intensify the similarity of short text 
documents in categories. Applying LDA is an efficient method to make short text documents being less 
sparse and more topic-oriented. The universal datasets affect analysis of topic models with respect to 
enrich features in short text documents. The differences of three types of universal datasets and word 
distributions in such datasets cause a slight difference in classification results. 
 
Fig. 8 Performance comparison between two types of external texts for feature enrichment. (a) DBLP-1 
and DBLP-2; (b) LNCS-1 and LNCS-2; (c) Wikipedia-1 and Wikipedia-2 
Our approach makes features of short text documents much more strength due to external words that 
make the correlation of word relationships affecting to classification. The comparison between using with 
and without topic model has illustrated that word correlations generated from combining external texts 
from topic models (e.g. kernel function) make the accuracy increase in classification. Next, by 
experiments we understand that the characteristics of word distribution in universal datasets of DBLP and 
LNCS are quite significant for tasks of text classification in domains related to scientific documents (e.g. 
Computer Science domain). Otherwise, Wikipedia is suitable in using for popular domains in searching 
and clustering [22]. 
In this study, we have examined the impact of the quantity of topics and added external text into such 
topics on each short text document through evaluating different topic models. The result shows that the 
number of topics does not cause a significant change in overall performance. For examples, in Figure 7b 
the performance of using topic models 50, 80, 120 and 150 yield better results than using topic model 60 
or 100 in DBLP. But if using topic models from middle range in Wikipedia will yield a better result than 
others e.g., using topic models 50 is better than topic model 80 and 120. Therefore, we conclude the 
reasonable number of topics of a topic model will create differences enough among external words that 
would be better for analyzing topics for short text documents. From here, the performance of the overall 
system is quite stable. And, since the number of topic is large enough to combine external texts, the 
performance of the overall system is quite stable. 
Finally, we exam the effect of topic models from various universal datasets, our approach deal with the 
problem of data sparseness in short text document for classification. The main advantages of our approach 
are follows: (1) to exploit various universal datasets as DBLP, LNCS and Wikipedia to enhance features 
in short texts is signification method for classification; (2) to pay attention to the consistency of merging 
between the universal datasets and short texts by ranking method to optimize features in short text 
documents; and (3) to be easy for implementation and not expensive due to the availability of knowledge 
resources as DBLP, LNCS and Wikipedia. In experiments, the results are obtained more impressive over 
the normal method without topic model that indicates the high quality contextual classification for short 
text especially in scientific documents. 
8. Conclusion and future work 
We have presented a method to classify short text from scientific documents based on title. The 
method is that enhancing features of short text documents with topic models from three types of universal 
datasets as DBLP, LNCS and Wikipedia then applied SVM for classification. We exploited the recent 
successful topic model Latent Dirichlet Allocation on enriched datasets, when included, can make 
documents less sparse and more topic-oriented. Furthermore, we present a method to combine external 
texts from adapted topics to improve features in short text documents which are, then, used as an enriched 
presentation of snippets to be classified. In experiments, we carefully conducted short text datasets from 
six types of sub-categories related to Computer Science domain. Those datasets were enriched by topic 
models to show our approach performed more impressive accuracy over than the original performance 
without topic model. Based on that, we assert that our approach is one of useful establishment approach to 
classify scientific documents focusing on short text. 
For future work, we will continue to pay attention on the method to estimate and adjust the number of 
adapted topics automatically. Moreover, we will also consider to extend the application using 
unsupervised methods for measuring the similarity between two short text documents after enhancing 
features. Finally, the work can be applied for queries classification in scientific documents searching. 
 
References 
1. S. Banerjee, K. Ramanathan, A. Gupta, Clustering Short Texts Using Wikipedia, in: Proceedings of 
ACM SIGIR 2007. 
2. D. Blei, Y. Ng Andrew, M. I. Jordan, Latent Dirichlet Allocation, Journal of Machine Learning 
Research, vol. 3, 2003, pp. 993-1022. 
3. B. Bollegala, Y. Matsuo, M. Ishizuka, Measuring Semantic Similarity between Words Using Web 
Search Engines, in: Proceeding of 16th International conference World Wide Web (WWW'07), 2007. 
4. L. Cai, T. Hofmann, Text Categorization by Boosting Automatically Extracted Concepts, in: 
Proceedings of ACM SIGIR 2003. 
5. N. Cancedda, E. Gaussier, C. Goutte, J. M. Renders, Word sequence kernels, Journal of Machine 
Learning Research, vol. 3, 2003, pp. 1059-1082. 
6. M. Chen, X. Jin, D. Shen, Short text classification improved by learning multi-granularity topics, in: 
Proceedings of the Twenty-Second international joint conference on Artificial Intelligence, p 1776-
1781, July 16-22, 2011, Barcelona, Catalonia, Spain. 
7. S. Deerwester, G. Furnas, T. Landauer, Indexing by Latent Semantic Analysis, Journal of the 
American Society For Information Science, vol. 41, no. 6, 1990. pp. 391-407. 
8. M. Efron, P. Organisciak, K. Fenlon, Improving retrieval of short texts through document expansion, 
in: Proceedings of SIGIR '12, 2012. 
9. E. Gabrilovich, S. Markovitch, Computing Semantic Relatedness Using Wikipedia-Based Explicit 
Semantic Analysis, in: Proceedings of 20th International Joint Conference Artificial Intelligence 
(IJCAI'07), 2007. 
10. T. Griffiths, M. Steyver, Finding Scientific Topics, in: Proceedings of National Academy of Sciences 
of the United States of America, vol. 101, 2004, pp. 5228-5235. 
11. G. Heinrich, Parameter Estimation for Text Analysis, Rapport technique, University of Leipzig, 2008. 
12. T. Hofmann, Probabilistic latent semantic indexing, in: Proceedings of the 22nd annual international 
ACM SIGIR conference on Research and development in information retrieval (SIGIR'99), 1999, pp. 
50-57. 
13. D. S. Hunnisett, W. J. Teahan, Context-based methods for text categorization, in: Proceedings of the 
27th annual international ACM SIGIR conference on Research and development in information 
retrieval, 2004, pp. 578-579. 
14. T. Joachims, Learning to Classify Text using Support Vector Machines, Dissertation, Kluwer, 2002. 
15. D. T. Le, R. Bernardi, E. Vald, Query Classification via Topic Models for An Art Image Archive, in: 
Proceedings of Recent Advances in Natural Language Processing, RANLP2011, 2011. Bulgaria. 
16. T. A. Letsche, M. W. Berry, Large-Scale Information Retrieval with Latent Semantic Indexing, 
Information Science, vol. 100, no. 1-4, 1997, pp. 105-137. 
17. G. Long, L. Chen, X. Zhu, C. Zhang, Tcsst: transfer classification of short & sparse text using 
external data, in: Proceedings of ACM CIKM'12, 2012. 
18. T. Liu, Z. Chen, B. Zhang, W. Ma, G. Wu, Improving Text Classification using Local Latent 
Semantic Indexing, in: Proceedings of IEEE International Conference on Data Mining (ICDM 2004), 
2004. 
19. L. Ma, J. Shepherd, A. Nguyen, Document classification via structure synopses,  in: Proceedings of 
the Fourteenth Australasian database conference on Database technologies 2003, vol .17, pp. 59-65, 
Adelaide, Australia. 
20. D. Metzler, S. Dumais, C. Meek, Similarity Measures for Short Segments of Text, in: Proceedings of 
29th European Conference IR Research (ECIR'07), 2007. 
21. K. Nigam, J. Lafferty, A. McCallum, Using Maximum Entropy for Text Classification, in: Workshop 
on Machine Learning for Information Filtering IJCAI-99, 1999, pp. 61-67. 
22. X. H. Phan, C. T. Nguyen, D. T. Le, L. M. Nguyen, S. Horiguchi, Q. T. Ha, A Hidden Topic-Based 
Framework toward Building Applications with Short Web Documents, IEEE Transactions on 
Knowledge and Data Engineering, vol. 23, 2011, pp. 961-976. 
23. X. Quan, G. Liu, Z. Lu, X. Ni, W. Liu, Short text similarity based on probabilistic topics, Knowledge 
and Information Systems, vol. 25, no. 3, 2010, pp. 473-491.  
24. M. Sahami, T. Heilman, A Web-based Kernel Function for Measuring the Similarity of Short text 
Snippets, in: Proceedings of the 15th international conference on World Wide Web (WWW2006). 
ACM Press, New York, 2006, pp. 377–386. 
25. G. Salton, A. Wong, C. S. Yang, A Vector Space Model for Automatic Indexing. Comm, ACM, vol. 
18, no. 11, 1975, pp. 613-620. 
26. F. Sebastiani, Machine learning in automated text categorization, ACM Computing Surveys, vol. 34, 
2002, pp. 1-47. 
27. A. Sun, Short text classification using very few words, in: Proceedings of ACM SIGIR '12, 2012. 
28. E. Stamatatos, G. Kokkinakis, N. Fakotakis,Automatic Text Categorization in Terms of Genre and 
Author. Computational Linguistics, vol. 26, 2000, pp. 471-495. 
29. W. Yih, C. Meek, Improving Similarity Measures for Short Segments of Text, in: Proceedings of 
22nd National Conference Artificial Intelligence (AAAI2007), 2007. 
30. B. Zhang, Y. Chen, W. Fan, E. A. Fox, M. Goncalves, M. Cristo, P Calado, Intelligent GP fusion 
from multiple sources for text classification, in: Proceedings of the 14th ACM international 
conference on Information and knowledge management, 2005, pp. 477-484. 
31. W. Zhang, T. Yoshida, X. Tang, Text Classification Based on Multi-word with Support Vector 
Machine, Knowledge-Based Systems, vol. 21, no. 8, 2008, pp. 879-886 
