Appl Intell
DOI 10.1007/s10489-014-0543-z
Learning and exploiting concept networks
with ConNeKTion
Fulvio Rotella · Fabio Leuzzi · Stefano Ferilli
© Springer Science+Business Media New York 2014
Abstract Studying, understanding and exploiting the con-
tent of a document collection require automatic techniques
that can effectively support the users in extracting use-
ful information from it and reason with this information.
Concept networks (e.g., taxonomies) may play a relevant
role in this perspective, but are seldom available, and can-
not be manually built and maintained cheaply and reliably.
On the other hand, automated learning of these resources
from text needs to be robust with respect to missing or
partial knowledge, because often only sparse fragments of
the target network can be extracted. This work presents
ConNeKTion, a tool that is able to learn concept networks
from plain text and to structure and enrich them by find-
ing concept generalizations. The proposed methodologies
are general and applicable to any language. It also provides
functionalities for the exploitation of the learned knowl-
edge, and a control panel that allows the user to comfortably
carry out these activities. Several experiments and applica-
tions are reported, showing the usefulness and flexibility of
ConNeKTion.
Keywords Concept network · Text mining · Reasoning ·
Generalization
F. Rotella · F. Leuzzi · S. Ferilli ()
Department of Computer Science, University of Bari,
via E. Orabona, 4, Bari, Italy
e-mail: stefano.ferilli@uniba.it
F. Rotella
e-mail: fulvio.rotella@uniba.it
F. Leuzzi
e-mail: fabio.leuzzi@uniba.it
1 Introduction
The spread of electronic technology has boosted the pro-
duction of documents in all fields of knowledge, and the
flourishing of document repositories aimed at supporting
scholars and non-specialists in satisfying their information
needs. Studying, understanding and exploiting the content
of, and extracting useful information from, these reposito-
ries are complex activities for which suitable support must
be provided to the users. However, automatic Full Text
Understanding is not trivial, due to the intrinsic ambiguity
of natural language and to the huge amount of knowledge
needed to switch from a purely syntactic representation to
the underlying semantics. Nevertheless, even small portions
of such a knowledge may significantly improve understand-
ing performance.
In particular, a relevant role can be played by concept net-
works, intended as structured organizations (often graphs)
of concepts (expressed in formalisms that range from simple
terms, as in lexical taxonomies, up to formal logic defi-
nitions, as in ontologies) interconnected by several kinds
of relationships. Indeed, they embed implicit knowledge
that may be required to correctly understand a text and
that can be provided to humans and automatic systems.
Unfortunately, several factors prevent the wide availability
of these resources: their manual building and maintenance
are costly and error-prone, they are language- and often
domain-dependent, and sometimes they are built for very
specific tasks and circulate in restricted groups. Proba-
bly the only exception for quality and diffusion, and the
most famous one, is WordNet [8, 37] and its derivatives: it
includes terms, concepts (as sets of synonymic terms called
synsets), and several kinds of lexical and semantic relation-
ships between terms, concepts or both. Along with many
merits, it also has several shortcomings. It is general, so it
F. Rotella et al.
does not cover satisfactorily specific branches of knowledge
or the topics of specific collections. It is maintained manu-
ally, which introduces errors and causes new versions not to
be backward compatible. The original WordNet is available
only for English; extensions and similar resources for some
other languages have been developed (e.g., ItalWordNet
for Italian, MultiWordNet for cross-language applications,
WordNet Domains for associating words to categories, Sen-
tiWordNet for the sentiment domain, etc.), but often do
not have the same quality, and for some languages a sim-
ilar resource is not available at all (at least, not for free).
This encourages the research on techniques to automatically
build concept networks by mining documents in natural
language in order to extract the concepts and relationships
expressed by words in the text.
This work aims at introducing and describing the func-
tionality, underlying techniques and possible uses of Con-
NeKTion (acronym for ‘CONcept NEtwork for Knowledge
representaTION’), a tool for concept network learning, con-
sultation and exploitation. As for the learning task, working
on plain text it is able to identify the concepts and rela-
tionships underlying a document collection, and to further
structure and enrich the learned graphs, also in the pres-
ence of missing or partial knowledge (a typical problem in
small collections). Concerning consultation, its Graphical
User Interface (GUI) allows to comfortably visualize, fil-
ter according to various criteria and use the learned graph
to provide information on the document collection and its
content. Finally, as regards exploitation, it aims at partially
simulating some human abilities in text understanding and
concept formation, such as [13, 28, 29, 44, 45]: extracting
the concepts expressed in given texts and assessing their rel-
evance; obtaining formal and human-readable descriptions
of the concepts underlying the terms; generalizing concepts;
applying some kind of reasoning on the learned concepts;
identifying relevant keywords in a text; helping the user in
retrieving useful information from a document corpus; rec-
ognizing the author of a document based on writing style
and structure of the sentences. ConNeKTion integrates and
brings to cooperation a mix of existing and novel tools and
techniques in order to reach its objectives. It adopts both
propositional and relational concept descriptions, that allow
to handle different levels of complexity and expressiveness
in concept representation. While the implemented proto-
type works on English, the underlying methodologies are
completely general and applicable to any language.
This work is organized as follows. After presenting the
learning core of ConNeKTion in the next section, along
with experiments showing its effectiveness, Section 3 pro-
poses several different successful applications of the sys-
tem. Then, Section 4 describes the system’s GUI, and finally
Section 5 draws some conclusions and outlines future work
issues.
2 ConNeKTion and its concept network learning core
The main feature of ConNeKTion consists in processing
a corpus of texts in natural language to build a concept
network. Its basic assumption is that nouns express con-
cepts, and that a concept can be defined by (a) the set of
other concepts that interact with it in the world described
by the corpus, and (b) the set of its associated proper-
ties and attributes, expressed by verbs and adjectives. This
knowledge can be formalized as a graph, where nodes
are the concepts/nouns appearing in the corpus, and edges
represent the relationships expressed by the grammatical
structure of the text (the direction of edges denoting the
role of the nodes in the relationship). The resulting net-
work can be considered as an intensional representation
of the corpus. Translating it into a suitable First-Order
Logic (FOL) formalism enables the subsequent exploita-
tion of logic inference engines in applications that use that
knowledge.
2.1 Basic graph construction
To extract the concept network underlying a set of texts,
some preliminary Natural Language Processing (NLP)
activities are needed:
1. Anaphora Resolution, performed by JavaRAP [40],
replaces pronouns by the explicit nouns they stand
for [29]. This provides more material from which
extracting information, since the relationships involv-
ing pronouns must be discarded because they cannot
be associated to specific concepts. As a side effect,
sentences that were previously unrelated due to the pro-
nouns, may now share the same subjects and/or objects,
which improves the quality of the resulting concept
network.
2. Syntactic Analysis yields a relational representation of
the structure of sentences. It is performed by the Stan-
ford Parser and Stanford Dependencies tools [26, 34],
that provide the parse tree of each sentence along with
the graph of the involved grammatical relations. These
typed dependencies are expressed as binary relations
between pairs of words, where the former element rep-
resents the governor of the grammatical relation, and
the latter its dependent.
3. Normalization, that turns all words in the input text
into a standard form (lemmatization is used instead
of stemming, because this allows to distinguish their
grammatical role and is more comfortable to read by
humans).
Once this pre-processing is carried out, additional infor-
mation is extracted using a purposely developed linguistic
Learning and exploiting concept networks with ConNeKTion
expert system, that produces the following relationships
between concepts:
attribute(C, A) attribute A describes concept C;
can(C, A) concept C may perform action A;
be(C, A) concept C may receive (be a passive subject of)
action A;
is a(C1, C2) concept C1 is a subclass of concept C2;
relationship(C1, C2) relationship holds between con-
cepts C1 and C2.
where actions are expressed by verbs, and sentences involv-
ing verb ‘to be’ are used to obtain an initial sub-class
structure for the taxonomy: e.g., “penguins are birds” yields
is a(penguin,bird). The expert system is a peculiarity of
ConNeKTion, that provides additional high-level features
about the textual content with respect to other approaches
available in the literature.
A representational trick is adopted to treat indirect com-
plements as direct ones, by embedding the corresponding
preposition into the verb: e.g., “The cat jumped on the
table” becomes jump on(cat,table). Each arc in the graph is
labeled with the frequency with which the associated rela-
tionship was found in the training corpus. This improves
robustness, allowing to filter out all elements that do not
pass a given level of reliability/frequency, and lays the
basis for applying statistical reasoning on the graph. More
precisely, the presence of negation modifiers for verbs in
the syntactic tree of a sentence is used to track separately
positive and negative forms: depending on the balance of
frequency for these two forms, the system can infer whether
a given relationship is occasional, typical, mandatory, pro-
hibited, etc. for some concepts.
This yields the basic graph representation on which Con-
NeKTion works. Figure 1 shows the graph corresponding
to the sentence “Bell, based in Chicago, makes and dis-
tributes electronic, computer and building products”. Note
that this part is completely incremental: any time new
texts are available, they can be processed separately and
integrated into the existing graph, updating the frequen-
cies of existing nodes/edges and possibly adding new nodes
and/or edges. It should also be noted that the learned graph,
or portions thereof, can be directly mapped onto a First-
Order Logic (FOL) description, that enables an application
of several kinds of reasoning strategies on it. In particular,
the neighboring subgraph of a node can be interpreted as
a formal definition of the concept associated to that node:
the wider the selected neighborhood, the more detailed the
concept definition.
2.2 Taxonomy building
The taxonomic part of a concept network includes the
class-subclass relationships according to which the set of
available concepts is structured. The basic relation under-
lying taxonomies is generalization, for which we adopt a
simple and operational definition:
Definition 1 (Generalization) [29] A concept G general-
izes another concept C if anything that can be labeled as C
can also be labeled as G, but not vice-versa.
Thus, generalization is fundamental, and provides many
opportunities of enrichment and/or manipulation of the
basic graph:
1. building taxonomic structures by (repeatedly) applying
a generalization operator to identify existing, or create
new, generalized concepts;
2. shifting the representation, by removing the generalized
nodes from the graph and leaving just their gener-
alization (that inherits all their relationships to other
concepts);
3. extending the number of relationships between con-
cepts belonging to the same connected component of
the graph, or adding connections between disjoint com-
ponents that open new (previously impossible) paths.
Fig. 1 Graphical representation
of the output
bell
can distribute
can make
can base in
chicago product
is electronic
is building
be make
be distribute
property
property
property
make
distribute
base in
attibute
attibute
property
property
F. Rotella et al.
In particular, as regards the addition of new connections
between disjoint components, the following definition can
be given:
Definition 2 (Bridge) Given a concept graph K , a bridge
in K is a set of new nodes and/or edges that, added to the
graph, connects otherwise disjoint subgraphs, allowing to
reach any node in one of them from any node in another. A
bridge connecting n disjoint sub-graphs is called an n-way
bridge.
Introducing bridges in the basic graph may allow to
connect very distant and unrelated concepts, and helps
in tackling the data poorness and concept sparsity prob-
lems. Given two disjoint sub-graphs to be bridged, the
proposed approach tends to bridge their central concepts,
which as a consequence evenly improves the bridging (in
terms of average distance of paths between any two nodes
in the two sub-graphs) of their peripheral neighborhoods as
well.
While a few fragments of the taxonomy can be extracted
directly from the text (as in the case of explicit sentences
such as “penguins are birds”), most of this structure must
be indirectly inferred by applying suitable techniques to the
non-taxonomic information embedded in the learned graph.
These techniques must be robust with respect to missing
or partial knowledge and flexible with respect to noise.
ConNeKTion works in two steps:
1. Concept Grouping: the set of all concepts in the graph
is grossly partitioned into separate groups;
2. Group Generalization: each group obtained in the pre-
vious step undergoes generalization, providing new
generalized concepts (or identifying existing concepts
that are generalizations of the group).
For the grouping step, ConNeKTion adopts pairwise
agglomerative clustering (see Algorithm 1): initially, each
concept becomes a singleton cluster; then, clusters are pro-
gressively merged until a stop criterion is met. Specifically,
ConNeKTion adopts a complete link clustering strategy [6].
Depending on the generalization objectives, complete link
is applied differently: for the purpose of enriching relation-
ships within already connected components, it is applied
in the standard way (clusters are merged if the distance
between their farthest items is less than a given threshold);
when the aim is to build bridges, clusters are merged if
the distance of their closest items is greater than a given
threshold. If several pairs meet the condition, for each such
pair (C′, C′′) the average distance of all pairs of concepts
(c′, c′′) ∈ C′ × C′′ is computed, and the pair having small-
est (respectively, greatest) average is merged. To improve
efficiency, the average score between all pairs of items in
the two clusters is saved. We define more formally the
clustering of dissimilar objects as follows.
Definition 3 (Inverse clustering) Given a set of objects
and a distance measure, inverse clustering is obtained by
iteratively grouping most dissimilar objects while their dis-
tance is above a given distance threshold.
Summing up, each cluster contains similar (resp., dis-
similar) concepts that can be generalized in order to cre-
ate new relationships (resp., to connect disjoint pieces
of knowledge) for enrichment (resp., bridging) purposes.
If a resulting generalization matches an existing concept
in the graph, then that concept is promoted as super-
concept of the generalized concepts, introducing corre-
sponding is a relationships; otherwise, the generated con-
cept remains without a human-understandable name until
a new concept that matches it will be formed. Depend-
ing on the representation formalism adopted for con-
cepts, this general strategy must be properly adapted, and
suitable techniques must be applied. ConNeKTion cur-
rently adopts two formalisms, a propositional and a rela-
tional one, which we are going to describe in the next
subsections.
2.2.1 Propositional approach
In the propositional approach, ConNeKTion describes con-
cepts by their direct neighbor concepts plus the verbs used
to connect them (distinguishing their positive and negated
forms). More formally, consider the sets C = {c1, . . . , cn}
of all concepts, and A = {a1, . . . , am} of all verbs (in both
their positive and negated form), in the graph. Then, each
concept c ∈ C is described by a binary feature vector of
size n+m, whose first n positions are associated one-to-one
to the concepts, and whose last m positions are associated
one-to-one to the verbs. Thus, the whole set of concepts is
Learning and exploiting concept networks with ConNeKTion
described by a matrix C[n×(n+m)] where for the i-th concept
ci ∈ C, i = 1, . . . , n:
– for j = 1, . . . , n : Ci,j = 1 if there is at least one
relationship between concepts i and j , or Ci,j = 0
otherwise;
– for j = n + 1, . . . , n + m : Ci,j = 1 if there is at
least one relationship between concept i and verb j , or
Ci,j = 0 otherwise.
Based on this representation, the Hamming distance [17]
can be applied to compare pairs of concepts, and used
to carry out clustering. Note that some concepts may be
described by all-0 vectors, in which case they can be ignored
by the procedure. Also note that, in the bridging perspective,
pairs of descriptions undergoing generalization cannot have
any 1 in the same positions, because this would indicate the
existence of a path between them. Moreover, the more 1’s
overall in the two disjoint descriptions, the larger their dis-
tance, in which case a high-quality bridge, merging two hub
(i.e., highly connected) nodes, would be obtained.
2.2.2 Relational approach
The relational representation adopted by ConNeKTion
describes each concept using the FOL translation of a neigh-
borhood of the concept in the learned graph within a radius
of k edges (the larger k, the more refined and detailed the
description). Technically speaking, the sub-graph induced1
by the set of concepts that are at most k hops away from the
‘root concept’ is extracted. So, concepts in this sub-graph
are connected to the root concept by at least one path of
length at most k, and all edges between these concepts are
also included in the sub-graph. To improve efficiency, the
weak components2 of the graph are preliminarily extracted
using the JUNG Java library [39], and each sub-graph is
processed separately.
Edges are translated using binary predicates (including
in the name the positive or negative valence of the action
they represent), and nodes as their arguments (the first argu-
ment representing the subject of the relation, and the second
argument representing the object). Expressiveness is signif-
icantly improved over the propositional solution, since not
only the attributes and relationships directly associated to
the root concept are considered (relation-centric descrip-
tion), but also those among its neighbors at various levels
k of distance (concept-centric description). For instance, in
a hypothetical sub-graph translation for a root concept c
1A vertex-induced sub-graph is a subset of the vertexes of a graph
together with all edges in the graph whose endpoints are both in the
subset.
2A weak component is defined as a maximal sub-graph in which there
exists a path between all pairs of vertexes (considering undirected
edges).
expressed as rel1(c, c′), rel2(c′′, c), rel3(c′, c′′′), c′ and c′′
are level-1 neighbors of c, c′′′ is a level-2 neighbor, and the
binary predicates reli are (possibly negated) verbs.
Concerning the clustering step, the relational similarity
function presented in [12] can be applied to these concept
representations. It takes values in ]0, 4[ and is computed by
repeated applications of the basic similarity formula (1) in
the next section to different parameters extracted from the
relational descriptions.
In the generalization step, ConNeKTion exploits the least
general generalization operator (lggOI ) proposed in [51]
to generalize the concept descriptions in each cluster, and
obtain a FOL description of their generalization, which is
the subsumer of the cluster. The use of this operator is not
limited to the insertion of new taxonomic relations or to the
bridging of disjoint portions of the graph, but can also sup-
port more advanced tasks, including retrieval of documents
of interest and representation shift (abstraction).
2.2.3 Use of external taxonomies
Concepts occurring seldom in the corpus might suffer from
underspecification (i.e., having few connections, their asso-
ciated descriptions are very poor), which would affect their
cluster assignment. While bridging is less sensitive to this
event, because it tends to group dissimilar concepts, and
poor descriptions are usually very different from all other
descriptions, this might be a problem for the taxonomic
enrichment task.
External taxonomic resources can be used to improve the
quality of the result. Indeed, the existing taxonomy can be
used to filter the concepts in a given cluster, by discard-
ing all pairs of concepts whose similarity is below a given
threshold. Unfortunately, external taxonomic resources are
not always available, for instance concerning languages dif-
ferent than English or specific domains. For this reason,
ConNeKTion was designed to learn the concept network
without the need for external resources, but provides an
F. Rotella et al.
option for using an existing taxonomy, when available, to
refine and improve the result. The general procedure can be
summarized in two steps:
1. Word Sense Disambiguation (WSD): associates each
term/concept in the graph to a single element of the
external taxonomy;
2. Computation of taxonomic similarity: the external tax-
onomy is exploited to further filter the groups found
by clustering, and to choose an appropriate subsumer
(Algorithm 2).
Due to many words being polysemic, carrying out step 1
is not trivial. Considering WordNet, for instance, concepts
must be mapped onto corresponding synsets. The current
prototype uses the one domain per discourse assump-
tion [14] (“the meanings of close words in a text tend to refer
to the same domain, which is probably the dominant one in
that portion of text”), as described in [13].3
As to step 2, the similarity between a pair of concepts
P = {c′, c′′} is computed by blending two measures that
mutually smooth each other, as:
sim(P ) = simFa(P ) · simWP (P )
The former [10] is based on the following function:
simFa(P ) = sf(n, l, m)
= l + 1
2
(
1
l + n + 2 +
1
l + m + 2 ) ∈]0, 1[ (1)
and adopts a global perspective based on the whole taxon-
omy, such that:
l = number of common super-concepts of c′ and c′′;
n = number of super-concepts of c′ that are not also
super-concepts of c′′;
m = number of super-concepts of c′′ that are not also
super-concepts of c′;
while the latter [57] considers a single path only:
simWP (P ) = 2 ∗ depth(lcs({c
′, c′′}))
depth(c′) + depth(c′′) ∈ [0, 1]
where, looking at the external taxonomy:
depth(c) is the depth of concept c;
lcs(C) is the Least Common Subsumer (LCS) (infor-
mally, the closest common superconcept) of the concepts
in C.
3First the synsets of each word are extracted from WordNet, then,
for each synset, all the associated domains in WordNet Domains are
selected, and finally each domain is weighted according to the den-
sity function presented in [1], depending on the number of domains to
which each synset belongs, on the number of synsets associated to each
word, and on the number of words that make up the sentence. Each
synset of a word is weighted based on the weights of the associated
domains, and the one with highest weight is selected.
At this point, a set of similar pairs is selected, and the star
of the best pairs is computed, defined as follows:
Definition 4 (Star) Given a set S of unordered pairs of
items and a P ∈ S, we define
star(P ) = {{x, y} ∈ S | (x ∈ P ∨ y ∈ P) ∧ {x, y} = P }
(i.e., the set of pairs including an element from P , but not
both).
Then, using again function lcs(C), the generalization set
is obtained:
Definition 5 (Generalization set) Given a set of concepts
C, and a pair
P = arg max
P ′∈C×C
sim(P ′)>T
sim(P ′)
the generalization set of P in C with similarity threshold T
is
gs(P, C, T ) = {i | i ∈ P ∧ P ∈ S}
where S = {P ′′ ∈ star(P ) | lcs(P ) = lcs(P ′′)}.
Finally, the generalization set is generalized using the
identified least common subsumer. In this way, a gener-
alization over a single star per cluster can be obtained.
Then, the subset of pairs used for the generalization can be
removed, and the procedure can be iterated obtaining many
generalizations per cluster (if any).
2.3 Related work
Although some researchers claim that fully automatic ontol-
ogy acquisition from text is not very realistic [31, 32], many
approaches have been attempted to build taxonomies and
ontologies by mining large amounts of text. They can be
arranged into two main groups: (a) those based only on what
is expressed in the text, and (b) those that exploit external
resources to fill the gap between the purely syntactic level
and the semantic one. In both cases, there are no standard
formal techniques to validate the quality of the learned con-
cept network; it can be assessed by the social consensus of
domain experts or by its usability for given business objec-
tives [18]. The improvements of ConNeKTion over previous
approaches can be summarized in its ability to learn the con-
cept network with no need for external resources nor for
human intervention, and in its exploitation of all the avail-
able relationships, not just taxonomic or domain-specific
ones.
Approaches of type (a) are needed when no machine-
processable external knowledge is available. [3] builds con-
cept hierarchies using Formal Concept Analysis to group
nouns/objects based on verbs/attributes, determined from
Learning and exploiting concept networks with ConNeKTion
the syntactic dependencies automatically extracted from
the text corpus using a linguistic parser. The result is a
concept lattice that is converted into a hierarchy. Con-
versely, ConNeKTion builds a concept network exploiting
the whole set of available concepts and relationships, not
only the shared attributes used for the taxonomic organi-
zation of concepts. Another approach, proposed in [38],
defines a language to build formal ontologies by deduc-
tive discovery as in logic programming. In particular, both
a specific language for manipulating Web pages, and a
logic program to discover the concept lattice, are defined.
Differently from [38], we do not limit the nature/kind of
relationships to a predefined set, since new relationships are
created as soon as a new (i.e., never encountered before)
relationship between concepts is found. Finally, [52] auto-
matically expands a small manually-built ontology kernel
(including primitive concepts, relations and operators) using
a combination of logic, linguistic and semantic analysis
approaches.
Among the approaches of type (b), TEXT-TO-
ONTO [31, 32] semi-automatically builds a taxonomy
exploiting constraints at various language levels (from
morphology to pragmatics and background knowledge) to
discover new concepts and establish relationships between
them. The process of knowledge acquisition is performed
by means of external resources and user contributions,
and identifies taxonomic relations only. Conversely, Con-
NeKTion is fully automatic and labels also non-taxonomic
relationships denoted by verbs. OntoLearn [56] builds a
taxonomy considering only concepts that are present in a
domain but do not appear in others, using WordNet as an
external resource. This prevents its application for domain-
specific applications, while ConNeKTion can exploit all
recognized concepts independently of their being generic
or domain-specific. The system in [19] uses a combination
of VerbNet [25] and WordNet to shift the representation
to the semantic level. The semantic roles identified in a
sentence are used, together with semi-automatically com-
piled domain-specific knowledge, to construct the concept
network. As said, ConNeKTion completely avoids human
intervention.
2.4 Evaluation
The different concept network building and enrichment
techniques embedded in ConNeKTion were evaluated using
ad-hoc tests, with the objective of obtaining qualitative out-
comes that may point out its strengths and weaknesses. A
corpus of documents concerning social networks on socio-
political and economic topics was purposely collected,
involving 695 nouns (subjects and/or objects) and 727 verbs.
The size of the dataset was deliberately kept small in
order to have poor knowledge. Experimental results, and
corresponding discussions, reported below show that the
approach is satisfactory.
Propositional approach To compensate for poor knowledge
in the dataset and for the little expressiveness of proposi-
tional representations, the support of WordNet as an external
taxonomy was used in these experiments. Both the ‘enrich-
ment’ and the ‘bridging’ perspectives of generalization were
tested (whose outcomes are summarized in Tables 1 and 2,
respectively). In both cases, the maximum threshold for the
Hamming distance was set to 0.001, while the minimum
threshold for taxonomic similarity was fixed at 0.45. Both
conceptual similarity measures returned very high values for
all selected pairs of concepts, leading to final scores that
neatly pass the 0.45 threshold.
Table 1 shows that, as expected, no bridges are built for
the ‘enrichment’ perspective. Very interestingly, simWP is
always greater than simFa . Since simWP is associated to a
specific path, and hence to the goodness of the chosen sub-
sumer, this confirms that the chosen subsumer was always
close to the generalized concepts, and hence that the clus-
ters included very similar concepts. This regularity is not
present in Table 2, where inverse clustering was used (and
indeed several bridges were built), which supports the need
for the simFa component that can capture similar concepts
when their subsumer is not so close as to be identified
using a single subsuming path. Three more aspects can be
emphasized: the effectiveness of the search for bridges in
case 5 of Table 2, in which a three-way bridge was built;
the overall quality of the generalizations obtained; and the
opportunity to perform not only representation shifts, but
also alignments as in case 1 of Table 1. We can conclude
that the propositional approach in ConNeKTion can suc-
cessfully deal with missing (or partial) knowledge and noisy
data, and its generalization operator can effectively perform
representation shifts.
Relational approach We ran several experiments, using 1-
level neighborhood descriptions and varying the threshold
in [2.0, 2.3] (remember that the similarity function ranges
in ]0, 4[) with a 0.05 step. Table 3 shows the results for
threshold 2.0, that summarizes the largest set of clusters.
Outcomes for the other thresholds are qualitatively analo-
gous, although not quantitatively. We specifically discuss
some clusters in Table 3 that may sound strange, in order to
have an insight on how they were obtained and to show that
each of them is special in some way.
Let us examine case 35. Applying the lggOI we obtain:
gen 35 (X) :- impact(Y,X), signal(Y,X), signal as(Y,X),
do with(Y,X), consider(Y,X), offer(Y,X),
offer to(Y,X), average(Y,X),
average about(Y,X), experience(Y,X),
flee in(Y,X), be(Y,X).
F. Rotella et al.
Table 1 Generalizations for pairwise clustering of similar concepts, and corresponding conceptual similarity scores (bottom)
# Bridge Subsumer Subs. Domain Concepts Conc. Domain
1 No variable [105857459] mathematics variable [105857459] mathematics
factor [105858317] mathematics
2 No person [100007846] biology, type [109909060] person
person
collegian [109937056] factotum
3 No person [100007846] biology, type [109909060] person
person
model [110324851] person
4 No integer [113728499] mathematics nineteen [113747989] number
forty [113749527] number
5 No person [100007846] biology, scholar [110251779] school
person
job [110222949] person
6 name [110344443] person
# Pairs Fa score WP score Score
1 variable, factor 0.7 0.857 0.6
2 type, collegian 0.659 0.737 0.486
3 type, model 0.692 0.778 0.538
4 nineteen, forty 0.75 0.75 0.562
5 scholar, job 0.711 0.823 0.585
scholar, name 0.678 0.778 0.528
where gen 35 is a fictitious name for the general-
ized concept X, whose description is reported in the
body of the Horn clause. It is obtained by generaliz-
ing cluster {visible, textual} with substitutions {visible/X,
internet/Y } and {textual/X, internet/Y }, respectively. This
reveals that both concepts in the cluster, within the social
network domain, have the same relationships to concept
internet. The description does not correspond to that of
either generalized concept, so neither of them can be pro-
moted to subsumer, and the subsumer must be left unla-
beled.
Now, considering case 38, the generalized concept
is:
gen 38(X) :- lead(Y,X), lead into(Y,X), come for(Y,X),
come to(Y,X), likely for(Y,X), be(Y,X),
transform into(Y,X).
obtained from cluster {future, gain} by substitutions
{future/X, youth/Y } and {gain/X, youth/Y }, respectively.
Again, this shows and describes their common relations to
concept youth (and nothing else, at least in this domain).
Indeed, the portion of future’s description discarded by the
lggOI :
take into(plan,future), be(future,digital),
see in(negotiation,future), take(plan,future),
be(negotiation,future), see(negotiation,future).
is not applicable to youth. This is an interesting case in
which the generalization fully matches the description of
concept gain, that can be promoted to be the subsumer of
the cluster, as its best representative.
Learning and exploiting concept networks with ConNeKTion
Table 2 Generalizations for pairwise Inverse clustering, and corresponding conceptual similarity scores (bottom)
# Bridge Subsumer Subs. Domain Concepts Conc. Domain
1 Yes teaching [100887081] pedagogy talk [100893243] pedagogy
lecture [100892861] pedagogy
2 Yes person [100007846] biology, scientist [110560637] person
person
youth [110804406] person
3 Yes music [107020895] music introduction [106396930] literature, music
end [106398401] literature, music
4 Yes figure [113741022] number two [113743269] number
pair [113743605] number
5 Yes person [100007846] biology, viewer [110633450] person
person
model [110324851] person
volunteer [110759151] person
6 No theory [105888929] factotum basis [105793554] factotum
theory [105888929] factotum
7 No municipality [108626283] administration, hometown [108671644] geography, literature, administration,
geography,
town planning city [108524735] geography, town planning
8 No territorial administration, state [108544813] geography
dominion [108552138] town planning department [108548733] geography
9 No structure [104341686] factotum wall [104546855] buildings
level [103365991] buildings
10 No representation factotum [104076846] scene [106614729] photography, racing, sociology
photo [103925226] telecommunication photography
# Pairs Fa score WP score Score
1 talk , lecture 0.739 0.869 0.643
2 scientist, youth 0.724 0.727 0.526
3 introduction, end 0.75 0.714 0.536
4 two, pair 0.72 0.823 0.593
5 viewer, model 0.71 0.666 0.474
viewer, volunteer 0.71 0.666 0.474
F. Rotella et al.
Table 2 (continued)
# Pairs Fa score WP score Score
6 basis, theory 0.694 0.842 0.584
7 hometown, city 0.738 0.842 0.621
8 state, department 0.75 0.75 0.562
9 wall, level 0.733 0.8 0.586
10 scene, photo 0.735 0.823 0.605
Finally, let us analyze the generalization of cluster 20
{kid, guru, limit}:
gen 20(X) :- protect(Y,X), protect by(Y,X), becomeY,X),
use(Y,X), have(Y,X), have to(Y,X),
have in(Y,X), have on(Y,X), find(Y,X),
go(Y,X), look(Y,X), begin(Y,X),
begin with(Y,X), begin about(Y,X),
suspect in(Y,X), suspect for(Y,X).
obtained through substitutions {kid/X, parent/Y }, {guru/X,
parent/Y } and {limit/X, parent/Y }. It does not match any
description in the original cluster, and hence the gener-
alizing concept cannot be labeled, as for case 35. The
parts of these descriptions discarded by the generalization
are:
teach(kid,school), launch about(foundation,kid),
teach(kid,contrast), come from(kid,contrast),
launch(foundation,kid), finish in(kid,school),
invite from(school,parent), possess to(school,parent),
invite(school,parent), finish in(kid,side),
come from(kid,school), find in(school,parent),
produce(school,parent), come from(kid,side),
find from(school,parent), finish in(kid,contrast),
invite about(school,parent),
come before(school,parent), release(foundation,kid),
invite to(school,parent), teach(kid,side),
release from(foundation,kid).
for kid,
become(teenager,guru).
for guru and
be(ability,limit), limit(ability,limit).
for limit. Overall, the results show that the procedure can
reliably recognize similar concepts based on their structural
position in the graph.
3 Exploitation
There are several possible exploitations of the learned
graph. A selection of applications, in which ConNeKTion
provided interesting results, follows.
3.1 Reasoning by association
We intend ‘reasoning by association’ in a given concept
network as the task of finding a path of pairwise related
concepts that establishes an indirect interaction between
two concepts [29]. Of course, the quality of the reasoning
results strongly depends on that of the graph, in turn depend-
ing on the processed texts. Indeed, if two nodes belong
to disjoint regions of the graph, reasoning cannot succeed.
Thus, the bridges built in taxonomy generation are precious
to improve reasoning performance. ConNeKTion proposes
two reasoning strategies: one works in breadth and aims
at obtaining the minimal path between concepts together
with all involved relations, the other works in depth and
supports probabilistic queries on the concept network. The
latter, in particular, cannot be applied to the graphs obtained
by other techniques in the literature, where the relationships
are sharp (i.e., non-probabilistic). The former cannot be
applied to techniques that consider taxonomic relationships
only.
3.1.1 Non-probabilistic reasoning
Non-probabilistic reasoning works by looking for a min-
imal path between two given concepts c′ and c′′ using a
Breadth-First Search (BFS) strategy, applied to both con-
cepts under consideration as shown in Fig. 2. The expansion
steps of the two processes are interleaved, checking at each
step whether the new set of concepts just introduced has a
non-empty intersection with the set of concepts of the other
process. When this happens, each concept in such an inter-
section identifies one or more shortest paths connecting c′
and c′′, that can be retrieved by tracing back the parent nodes
at each level in both directions up to the roots c′ and c′′. To
obtain more sensible ‘reasoning’, path descriptions are not
Learning and exploiting concept networks with ConNeKTion
Table 3 Clusters obtained processing concepts described using one
level of their neighborhood with a similarity threshold equal to 2.0
# Cluster
1 peer, preference, picture, thing, able, close, life adopter, music
2 content, internet
3 way, computer
4 matter, online
5 television, school, facebook
6 domain, supervision
7 screen, broadband
8 adult, transition,
9 tool, point
10 hour, time
11 paramount, today
12 percent, household
13 interest, challenge
14 role, student
15 property, executive, trouble, york, europe, fortune
16 hand, america, segment
17 relationship, capital, technology
18 question, game
19 income, female, newspaper, age
20 kid, guru, limit
21 theme, staff
22 classmate, note, connection, pass
23 magnet, comparison, relic
24 evolve, elite
25 day, view, clip, modicum, community, station, opportunity
26 environment, activity
27 average, bulk, space, creator
28 conversation, educator
29 power, payoff, classroom, media,
30 value, human
31 creation, july, announce
32 myspace, destination
33 phone, visitor, letter, class, touch
34 trivial, vetere, sincere, genuine
35 visible, textual
36 shift, collection
37 desire, sphere
38 future, gain
made up of concepts only, but also include the relationships
that connect adjacent concepts in the chain, represented
by the edge labels (i.e., verbs). For each relationship, the
(absolute and relative) frequency of positive/negative
instances are also provided,4 denoting different gradations
4Note that this is different than the spreading activation algorithm [4],
in that (1) graph traversal is not affected by weights on edges nor
thresholds, (2) we focus on paths rather than nodes, and specifically we
(such as permitted, prohibited, typical, rare, etc.) of actions
between two concepts. While this value does not affect the
reasoning strategy, it allows to distinguish which reasoning
path is more suitable for a given task.
3.1.2 Probabilistic reasoning
Since real world data (especially text, as in our case), and
as a consequence the knowledge learned from it, is typically
noisy and uncertain, there may be the need for strate-
gies that soften the classical rigid logical reasoning and
express a preference for some paths over others. ConNeK-
Tion applies probabilistic reasoning on the learned graph
using ProbLog [41]. A ProbLog program T = {p1 ::
c1, ..., pn :: cn}, is made up of clauses ci (as in a stan-
dard ProLog program) with associated probabilities pi .
T determines a probability distribution over all its possi-
ble non-probabilistic subprograms [50]. The semantics of
ProbLog is defined by the success probability of a query,
i.e. the probability that the query succeeds in a randomly
sampled program. To answer a query q, ProbLog exploits
SLD-resolution to compute the set E(q) of all different
proofs or explanations for it. Each successful proof of q (if
any) in the resulting SLD-tree uses a subset of facts in T ,
whose associated probabilities can be used to compute the
maximum probability of the query as follows5 [24]:
Px(q|T ) = max
e∈E(q)
P (e|T ) = max
e∈E(q)
∏
ci∈e
pi
In ConNeKTion, facts are of the form link(subject, verb,
object), and their probability is the ratio between their num-
ber of occurrences in the text and the sum of all possible
links between subject and object. The frequency weights
associated to the edges/relationships among concepts are
used for this computation. The rules consist of the recursive
definition of a path between two nodes based on the link/3
relationship, and are not probabilistic. Asking the proba-
bilistic reasoner for a path between two concepts c′ and c′′,
ProbLog is run on the query ?−path(c′, c′′). Note that using
this strategy a longer path might turn out to be more likely
than a shorter one.
are interested in the path(s) between two particular nodes rather than
in the whole graph activation, hence (3) in our approach setting the ini-
tial activation weight of start nodes makes no sense, and (4) this allows
to exploit a bi-directional partial search rather than a mono-directional
complete graph traversal.
5Again, this is not a spreading activation, even if weights on edges are
exploited.
F. Rotella et al.
Fig. 2 Example of
non-probabilistic reasoning
3.1.3 Evaluation
Both kinds of reasoning were tested on the dataset presented
in Section 2.4.
Table 4 shows sample outcomes of the minimum path
search strategy. Each row reports an edge/relationship, also
specifying the occurrence frequency of the associated verb,
which may be seen as an indication of the strength of that
reasoning step. For instance, case 1 explains the relationship
between young and schoolwork saying that “young look
television, that talks about and criticizes facebook, which
typically does not help in, rather distracts from, school-
work”. Verb look occurs only in positive sentences, and
hence has probability 1.0; this means that the available
knowledge consistently indicates that young ‘always’ look
television. Conversely, as regards the relationships between
facebook and schoolwork, the verb help appears with prob-
ability 0.25 in positive sentences and 0.75 in negative ones
(which can be interpreted as “typically does not help”).
Note that the outcome of this reasoning strategy shows all
possible verbs between each pair of adjacent nodes in the
path.
Table 5 shows the results of the same queries as in the
previous experiment, but using probabilistic reasoning. For
each query, it reports the exact success probability, the most
likely explanation probability and an approximate proba-
bility according to the MonteCarlo method implemented
in [23] keeping a 95 % confidence interval. The approximate
computation has been provided because exact inference can
be intractable even for small networks. For instance, in case
1 with most likely explanation, the proof entails the set
{(young, look, television), (television, talk about, facebook),
(facebook, not help, schoolwork)}, with probabilities {1.0,
0.75, 0.5}, whose product is 0.375.
3.2 Keyword extraction
Keyword Extraction (KE) aims at identifying words that are
present in a corpus and that are representative of its topics
and content. The flourishing literature in this field proposed
different ways to pre-process the text and to assign a weight
to each word. However, the state-of-the-art [15] is dom-
inated by term-frequency-based solutions, which require
large collections in order to draw significant statistics. Con-
versely, ConNeKTion adopts a mixture of techniques based
on different considerations and approaches, some of which
borrowed from the literature, which makes it less depen-
dent on statistics, and allows to successfully analyze even
Table 4 Examples of smooth reasoning ‘by association’ through BFS (start and target nodes in emphasis)
# Subject Verb Complement
1
young look [Pos: 3/3] television
television talk about [Pos: 3/3], critic [Pos: 1/1] facebook
facebook help [Pos: 1/4, Neg: 3/4], distract [Pos: 1/1] schoolwork
2 people be in [Pos: 1/1] group
group invite [Pos: 2/2] facebook
facebook help [Pos: 1/1] individual
3 everyone want [Pos: 5/5] occupation
occupation maintain [Pos: 1/1] lifestyle
lifestyle see in [Pos: 1/1], change in [Pos: 1/1], media
Learning and exploiting concept networks with ConNeKTion
Table 5 Examples of
probabilistic reasoning ‘by
association’ through ProbLog
# Query Probability
problog exact(path(young - schoolwork)) 0.530
1 problog max(path(young - schoolwork)) 0.375
problog approx(path(young - schoolwork)) 0.555
problog exact(path(people - individual)) 0.167
2 problog max(path(people - individual)) 0.167
problog approx(path(people - individual)) 0.162
problog exact(path(occupation - media)) 0.750
3 problog max(path(occupation - media)) 0.500
problog approx(path(occupation - media)) 0.744
small collections. This solution was adopted since the KE
task in ConNeKTion can be cast as the selection of rele-
vant nodes (i.e., concepts) in the graph, and identifying key
concepts in a text is more complex than just identifying
keywords.
Approach A first component pre-processes the graph to
obtain a more comfortable representation of its content.
The whole set of relations is represented in a Con-
cepts×Attributes matrix V that recalls the classical Vector
Space Model (VSM) [49] (a task known as Text Transfor-
mation in the Knowledge Discovery from Text community).
The matrix is filled according to the following scheme:
Vi,j = fi,j∑
k fk,j
· log |A||{j : ci ∈ aj }|
where:
– fi,j is the frequency of the i-th concept co-occurring
with the j -th attribute;
–
∑
k fk,j is the sum of the co-occurrences of all concepts
with the j -th attribute;
– A is the entire set of attributes;
– |{j : ci ∈ aj }| is the number of attributes with which
the concept ci co-occurs (i.e., for which fi,j = 0).
Its values represent the term frequency tf, as an indicator
of the relevance of the term in the text at hand (no idf
component is considered, to allow the incremental addi-
tion of new texts without the need of recomputing this
statistic). Based on this representation, clustering can be
applied to the rows of V to identify groups of elements
having similar features (i.e., involved in the same verbal
relationships). The idea is that concepts belonging to the
same cluster should share some semantics. For instance,
if concepts dog, John, bear, meal, cow all share attributes
eat, sleep, drink, run, they might be sufficiently close to
each other to fall in the same cluster, indicating a pos-
sible underlying super-concept (indeed, they are all ani-
mals). Since the target number of clusters is not known
in advance, (the Weka [16] implementation of) the EM
clustering approach based on the Euclidean distance is
exploited.
Another component explicitly performs KE, applying
various techniques based on different (and complementary)
aspects, perspectives and theoretical principles, to iden-
tify relevant concepts. Specifically, a quantitative approach
based on co-occurrences kc [35], a qualitative one based
on WordNet kw [11] and a psychological one based on
word positions kp are used. The psychological approach
is novel, and is based on the consideration that humans
tend to place relevant terms/concepts toward the start and
end of sentences and discourses, where the attention of
the reader/listener is higher. Thus, it assigns the chance of
a term being a keyword simply based on its position in
the sentence/discourse, according to a mixture model deter-
mined by mixing two Gaussian curves whose peaks are
placed around the extremes of the portion of text to be exam-
ined. The outcomes of these techniques on the given text can
be mixed, suitably weighted, to provide a compound weight
for a concept c:
k(c) = ζkc(c) + ηkw(c) + θkp(c)
where ζ , η and θ are weights ranging in [0, 1] and summing
up to 1.
Finally, inspired by the approach in [27],6 ConNeKTion
computes a Relevance Weight W(·) expressing the impor-
tance of each node/concept in the graph, by combining all
6A technique to semi-automatically extract a domain-specific ontology
from free text without using external resources but focusing on Hub
Words. After building the ontology, the ‘Hub Weight’ of a word t is
computed as:
W(t) = αw0 + βn + γ
n∑
i=1
w(ti)
where w0 is a given initial weight, n is the number of relationships in
which t is involved, w(ti ) is the tf ∗idf weight of the i-th word related
to t , and α + β + γ = 1. These elements, with some modifications,
appear in the first three terms of our formula.
F. Rotella et al.
the above components. Given a node/concept c,
W(c) = α w(c)
maxc w(c)
+ β e(c)
maxc e(c)
+ γ
∑
(c,c) w(c)
e(c)
+ δ dM − d(c)
dM
+ 	 k(c)
maxc k(c)
where α, β, γ , δ, 	 are weights summing up to 1, and:
– w(c) is an initial weight assigned to node c;
– e(c) is the number of edges of any kind involving node
c;
– (c, c) denotes an edge involving node c;
– dM is the largest distance between any two nodes in the
whole vector space;
– d(c) is the distance of node c from the center of the
corresponding cluster;
– k(c) is the keyword weight associated to node c.
The first term represents the initial weight provided by V ,
normalized by the maximum initial weight among all nodes.
The second term considers the number of connections
(edges) of any category (verbal or taxonomic relationships)
in which c is involved, normalized by the maximum num-
ber of connections of any node in the network. The third
term (Neighborhood Weight Summary) considers the aver-
age initial weight of all neighbors of c (a simple summation
of the weights would yield a value that is proportional to the
number of neighbors, which was already considered in the
previous term). The fourth term represents the Closeness to
Center of the cluster, i.e. the distance of c from the center
of its cluster, normalized by the maximum distance between
any two instances in the whole vector space. The last term
takes into account the outcome of the compound KE tech-
nique. These terms were designed to be independent of each
other. A partial interaction is present only between the sec-
ond and the third ones, but is significantly smoothed due to
the applied normalizations.
Ranking the nodes by decreasing Relevance Weight, a
suitable cutpoint in the ranking can be determined to distin-
guish relevant concepts from irrelevant ones. In ConNeK-
Tion, the list is cut at the first item ck in the ranking whose
difference in relevance weight from the next item is greater
or equal than the maximum difference between all pairs
of adjacent items, smoothed by a user-defined parameter
p ∈ [0, 1], i.e.:
W(ck) − W(ck+1) ≥ p · max
i=0,...,n−1
(W(ci) − W(ci+1))
Evaluation The KE functionality of ConNeKTion was
tested on the dataset presented in Section 2.4. Table 6 shows
on the top the weight settings and the cutpoint value for
selecting relevant concepts used in three different runs. In
all cases, the remaining parameter values were: ζ = 0.45,
η = 0.45, θ = 0.1 (to reduce the impact of the psychologi-
cal perspective, that is more naive compared to the others).
The corresponding outcomes are reported at the bottom,
where the Relevance Weight components are denoted, for
short, as A, B, C, D, E in the same order as they appear in
the formula. The parameter configuration 1 yields 3 relevant
concepts, having very close weights. Component D deter-
mines the inclusion of the very infrequent concepts (as can
be noted looking at column A) access and subset as relevant
ones. They benefit from the large initial weight of network,
to which they are connected. Using the second set of param-
eter values, the predominance of component A in the overall
computation, and the cutpoint threshold lowered to 70 %,
cause the frequency-based approach associated to the ini-
tial weight to give neat predominance to the first concept
in the ranking. Using the third set of parameter values, the
threshold is again 100 % and the other weights are such that
the frequency-based approach expressed by component A is
balanced by the number of links affecting the node and by
the weight of its neighbors. Thus, both nodes with highest
frequency and nodes that are central in the network are con-
sidered relevant. Overall, concept network is always present,
while the other concepts significantly vary depending on the
parameter values.
3.3 Information retrieval
Information Retrieval (IR) aims at providing the users with
techniques for retrieving and ranking interesting documents
in a repository, based on some kind of query. In the over-
whelming majority of cases, documents are in textual form,
and user queries are typically expressed in the form of nat-
ural language sentences, or sets of terms. This is clearly
a tricky setting, due to the inherent ambiguity of natural
language. Numerical/statistical manipulation of (key)words
has been widely explored in the literature, but in its several
variants it seems unable to fully solve the problem. Achiev-
ing better retrieval performance requires to go beyond
simple lexical interpretation of the user queries, toward an
understanding of their semantic content and aims.
Related work Many works aimed at building IR systems
exist in the literature. Most of them are based on the
Vector Space Model (VSM) [49], that represents a cor-
pus of documents in a ‘Terms × Documents’ matrix, in
which the value in the (i, j)-th cell represents the impor-
tance of the i-th term in the j -th document based on
different weighting schemes [43, 46, 53]. This allows to
compute the similarity between documents simply using a
geometrical distance measure on that space [20, 47, 48].
More recently, techniques have been explored that map the
documents into a space whose dimensions correspond to
concepts, which is more closely related to ConNeKTion.
Learning and exploiting concept networks with ConNeKTion
Table 6 Three parameter choices and corresponding outcome of relevant concepts
Test # α β γ δ 	 p
1 0.10 0.10 0.30 0.25 0.25 1.0
2 0.20 0.15 0.15 0.25 0.25 0.7
3 0.15 0.25 0.30 0.15 0.15 1.0
Test # Concept # A B C D E W
1 network 0.100 0.100 0.021 0.178 0.250 0.649
access 0.001 0.001 0.154 0.239 0.250 0.646
subset 6.32E-4 0.001 0.150 0.239 0.250 0.641
2 network 0.200 0.150 0.0105 0.178 0.250 0.789
3 network 0.150 0.25 0.021 0.146 0.150 0.717
user 0.127 0.195 0.022 0.146 0.150 0.641
number 0.113 0.187 0.022 0.146 0.150 0.619
individual 0.103 0.174 0.020 0.146 0.150 0.594
Latent Semantic Indexing (LSI) [5] starts from a classical
VSM approach, and applies Singular Value Decomposition
to identify latent concepts underlying the collection. Con-
cept Indexing (CI) [22] uses Concept Decomposition [7],
clustering the documents and considering each cluster as
representing a concept in the collection. The dimensionality
reductions computed by CI and LSI [21] achieve compara-
ble retrieval performance, but the former requires an order
of magnitude less time. A problem of both approaches, as
well as of most numeric approaches exploited in IR, is the
need of a large amount of text to effectively reduce noise,
redundancy, and ambiguity. Conversely, ConNeKTion per-
forms well also working with few short documents, since
it shifts the whole computation on a semantic layer which
does not require much textual data both at pre-processing
and at query time.
Approach In order to go beyond the syntactic level, we
switch from the terms in the collection to their meaning by
choosing a semantic surrogate for each word, exploiting an
external resource. Although the proposed technique applies
to any taxonomy, at the moment we use WordNet, and
its extension WordNet Domains [33], as readily available
general-purpose resources.
A preliminary off-line phase preprocesses the document
collection in order to partition it according to different
sub-domains. A classical VSM weighted according to the
TF*IDF scheme is built (currently, only nouns —i.e., nodes
in the graph— are used). The techniques embedded in the
DOMINUS framework [9] are used to select the keywords
in each document, then the keywords in each document
are ranked by decreasing TF*IDF weight. To filter out
noise and irrelevant information, only the top (in our case
15) keywords in each document are selected, and mapped
to corresponding synsets (i.e., their semantic representa-
tives) in WordNet, using the same WSD approach as in
Section 2.2.3. In this way the document representation is
shifted from terms to concepts. All the synsets are parti-
tioned using pairwise clustering with the complete link strat-
egy (see Section 2.2), based on an extension of the similarity
function (1) that leverages the additional (non-taxonomic)
relationships available in WordNet (e.g., meronymy, etc.):
simIR(c
′, c′′)=simFa(c′, c′′)+sf(n′, l′, m′)+sf(n′′, l′′, m′′)
where the second and third components work in breadth,
considering the number of common (l′; l′′) and differ-
ent (n′, m′; n′′, m′′) synsets to which c′ and c′′ are
directly connected by any outcoming (n′, l′, m′) or incom-
ing (n′′, l′′, m′′) relationship in WordNet. To give an idea
of these components, consider the following hypothetical
situation:
rel1(c
′, c3) rel2(c′, c4) rel3(c′, c5)
rel4(c
′′,c5) rel5(c′′,c6)
for the outcoming relationships, yielding l′ = |{c5}| = 1,
n′ = |{c3, c4}| = 2, m′ = |{c6}| = 1, and
rel1(c7, c
′) rel2(c8, c′) rel3(c9, c′)
rel4(c9, c
′′) rel5(c3, c′′) rel2(c8, c′′)
for the incoming ones, yielding l′′ = |{c8, c9}| = 2, n′′ =
|{c7}| = 1, m′′ = |{c3}| = 1.
Now, each document is considered in turn, and each of its
keywords votes for the cluster to which the associated synset
has been assigned. The contribution of each vote is equal
to the TF*IDF value determined in the keyword extraction
phase, normalized by the sum of the weights of the chosen
keywords. Then the document is associated to the three most
voted clusters (selecting only the top-voted cluster would
F. Rotella et al.
be too restrictive). This closes the off-line preprocessing
phase. Each cluster can be interpreted as a representative
of a specific domain, and thus can be exploited to retrieve
the associated sub-collection. Note that some clusters might
be empty (when they are not in the top 3 clusters of any
document in the collection).
The second phase actually carries out IR, working on-
line. Since queries are usually very short, keyword extrac-
tion is not performed, and all terms (actually, nouns for
consistency with the previous phase) are considered. For
the same reason, WSD might hardly identify a significant
domain. So, given an n-term query, where the i-th term
is associated to ni synsets,
∏n
i=1 ni semantic queries are
obtained, each of which selects one candidate synset for
each word, and represents a candidate disambiguation of
the query. Then, each semantic query is compared to each
non-empty cluster as follows: given the set of synsets in
a query, Q, and the set of synsets associated to a clus-
ter, C, the similarity between each possible pair of synsets
(q, c) ∈ Q×C is evaluated using simIR , and the maximum
similarity among all such pairs is returned. As a side-effect,
the arg max of these comparisons identifies the best word
sense disambiguation for the query. At this point, the best
combination of each cluster is used to rank the clusters by
descending relevance, and the documents associated to the
top-ranked clusters are returned as an answer to the user’s
query. The ranked list is exploited, instead of taking just
the best cluster, to allow navigation among the results by
decreasing relevance.
Evaluation To understand the contribution of each step
in the overall result, a collection made up of 200 docu-
ments was created by randomly drawing 50 documents
from 4 Wikipedia top-categories (general science, music,
politics, religion). A structured version of the Wikipedia
dump was obtained exploiting the Java Wikipedia
Library [58].
A selection of queries, with a corresponding performance
evaluation, is summarized in Table 7. For each query, the
ranked list of most similar clusters was considered, and the
top 10 documents were exploited for evaluating two per-
formance measures: classical Precision P , expressing how
many retrieved documents belong to the intended category
of the query, and a more loose version thereof P ′, consid-
ering as good outcomes also documents in categories that
are compatible with the query, even if that was not in the
user’s intentions. The decision to take several clusters (not
just the top-ranked one) improved the True Positives for all
queries. Outcomes pre-ceded by ‘+’ denote the number of
documents, immediately following the best 10 documents
used for computing P and P’, that were nevertheless relevant
for the query, showing that good performance is not lim-
ited to top items only. Also a qualitative evaluation of some
cases reveals interesting aspects. For instance all results for
query # 1 can be accepted as good, taking into account
that a scientific perspective might correctly satisfy the user’s
search about the creation of mankind, as well. Also for
query # 2, it is quite agreeable that both traditions and
folks are strictly related to religion as well as popular
Table 7 Performance evaluation
# Query Outcomes P P ′
1 creation of the mankind [1 to 5] religion 0.5 1.0
[6 to 10] science
[+3] science
2 traditions and folks [1 to 8] music 0.8 1.0
[9 to 10] religion
[+3] religion
3 ornaments and melodies [1 to 8] music 0.8 0.9
[9] science
[10] religion
4 capitalism vs communism [1 to 2] religion 0.8 0.8
[3 to 10] politics
[+4] politics
5 markets and new economy [1 to 10] politics 1.0 1.0
[+1] politics
6 gene structure and function [1 to 2] science 0.8 0.8
[3] religion
[4] politics
[5 to 10] science
[+2] science
Learning and exploiting concept networks with ConNeKTion
music. This motivated further analysis of some specific
queries. Two such queries are reported in the following, to
better describe the behavior of the technique (for the sake
of readability, in the following concepts will be described
using the synset code, the set of associated terms, and the
corresponding gloss).
The former is ornaments and melodies, for which only 2
combinations of synsets were found, among which the best
one was:
– 103169390 = {decoration, ornament and ornamenta-
tion} (‘something used to beautify’);
– 107028373 = {air, line, melodic line, melodic phrase,
melody, strain and tune} (‘a succession of notes forming
a distinctive sequence’).
recognized by the technique to be most similar to the
following cluster:
– 107044760 = {symphonic music, symphony} (‘a long
and complex sonata for symphony orchestra’);
– 107033753 = {mass} (‘a musical setting for a Mass’);
– 107026352 = {opera} (‘a drama set to music, con-
sists of singing with orchestral accompaniment and an
orchestral overture and interludes’);
– 107071942 = {genre, music genre, musical genre and
musical style} (‘an expressive style of music’);
– 107064715 = {rock, rock ’n’ roll, rock and roll,
rock music, rock’n’roll and rock-and-roll} (‘a genre
of popular music originating in the 1950s, a blend
of black rhythm-and-blues with white country-and-
western’);
– 107043275 = {concerto} (‘a composition for orchestra
and a soloist’).
It’s easy to note that this cluster contains elements that are
consistent with each other, possibly due to the use of a
complete link strategy, which is more restrictive in group-
ing items. This cluster corresponds to 8 documents, all
talking about music, returned as first (i.e., more relevant)
outcomes. Note that this result includes relevant docu-
ments that do not contain the terms that are present in the
query. This proves that the technique is able to go beyond
simple lexical interpretation of the queries, even allowing
queries whose terms are not present at all in the entire
collection.
The other query is market and new economy. The 2 nouns
yield a total of 20 synset combinations to be evaluated, the
best of which was:
– 108424951 = {market} (‘the customers for a particular
product or service’);
– 100192613 = {economy, saving} (‘an act of economiz-
ing; reduction in cost’).
The most similar cluster corresponded to the following
synsets:
– 108166552 = {country, land, nation} (‘the people who
live in a nation or country’);
– 108179689 = {populace, public, world} (‘people in
general considered as a whole’);
– 107965937 = {domain, world} (‘people in general,
especially a distinctive group of people with some
shared interest’).
and was associated to 8 main results talking about politics.
Again, both the benefits of returning as a result the ranked
list of clusters instead of just the best one, and the consis-
tency of the cluster elements can be appreciated. Also, it
should be noted that, although very simple, the one-domain-
per-discourse based WSD technique was able to select a
strongly consistent solution.
3.4 Author identification
Author Identification is a primary need in professional
forensic linguistics. Many works have investigated this task
by exploiting frequency-based approaches, numeric tech-
niques and writing style analysis. This was the motivation to
face the following task in [30]: given a small set of ‘known’
documents (no more than 10, possibly just one) written by
a single person (called the base), and a ‘questioned’ doc-
ument, determine whether the person who wrote the latter
(called the target) is the same as the former. Differently
from all previous techniques in the literature, ConNeK-
Tion aims at preserving the informative richness of textual
data by extracting and exploiting complex patterns from
them. It does not need external resources, nor requires the
user to set parameters or to embed deep knowledge in the
procedure.
Related work Research focused on different properties of
texts, the so-called style markers, to capture the writing
style under different labels and criteria. At the character
level, [59] uses n-grams as features, where the choice of n is
critical for the effectiveness and efficiency of the approach.
[2] defines new lexical features and taxonomies, but the def-
inition criteria are arbitrary and require language-dependent
expertise. Using syntactic features requires robust and accu-
rate NLP tools to perform syntactic analysis of texts,
and requires a huge amount of extracted features (e.g.,
about 900k features in [55]). Semantic approaches use
the dependencies obtained by external taxonomic or onto-
logical resources (e.g., [36] exploits WordNet), which are
not always available, especially for very specific domains.
Finally, other approaches define application-specific mea-
sures (e.g. based on the use of particular words or spacing
F. Rotella et al.
style) to better represent the style in a given text domain
(see [54] for an overview). While all the above approaches
adopt a flat (vectorial) representation of the documents, [42]
presents a different approach that preserves the phrase struc-
ture, where a probabilistic context-free grammar (PCFG) is
built for each author, but needs many documents per author
to learn the correct probabilities, and sometimes exploiting
parse trees only is not enough to characterize the author’s
style.
Approach ConNeKTion builds both base and target mod-
els, that describe the ways in which an author composes
sentences. Then, the classification results from the compar-
ison of these two models: if the writing habits of the target
match the base model, one may conclude that the author is
the same.
After obtaining a relational description for each sen-
tence as described in Section 2.2.2, the similarity mea-
sure described in [12] is applied to pairs of sentences.
For each training-test pair a triangular similarity matrix
between all pairs of sentences is built. Part of this
matrix contains the similarity scores between each pair
of sentences of known documents (base); another part
includes the similarities between pairs of sentences belong-
ing to the unknown document (target), which we will
call the gray zone in the following; and still another part
reports the similarity scores across known and unknown
documents.
ConNeKTion applies the usual agglomerative clustering
based on a complete link strategy to both base and target
submatrices according to Algorithm 1. Since more than one
pair might fulfill the complete link requirement, the pairs
are ranked, and in each iteration only the pair with the high-
est average similarity is merged. So, the ordering of the
pairs affects the final model. Technically, a model is a pos-
sible grouping of similar descriptions, as obtained by run-
ning the clustering algorithm with a threshold determined
empirically as follows. Clustering is repeatedly applied for
increasing values of the threshold in the range ]0, 4[ (the
same as the similarity measure) by step 0.005, which yields
a sequence of models < m1, ..., mn >; then, the desired
threshold is determined as:
t = arg max
0<i<n
c(mi+1)
c(mi)
where c(mi) counts the number of clusters in the i-th model.
I.e., t is the threshold associated to the model mi yielding
the greatest distance from model mi+1. It is easy to note
that a given diference value obtained with many clusters is
less significant than one obtained with a smaller number of
clusters.
After determining the proper thresholds and defining
base and target models (each having its own threshold), clas-
sification can be performed. As shown in Algorithm 3, this
phase works on clusters including more than one item. For
each cluster in the target model, if it can be merged with at
least one cluster in the base model (under the complete link
assumption), the author is the same, otherwise it is not. Such
a check exploits the similarities in the submatrix relating
known and unknown documents and the maximum thresh-
old between base and target model, which makes harder a
full alignment between such models. This choice encour-
ages accurate classifications. Problems in which models
consist of just one, two or three clusters can be considered
too poor to be reliable. Thus for each of them, if the num-
ber of instances of a model is less than 20 % of the other,
Algorithm 3 does not try a classification.
Evaluation The procedure was evaluated using the English
section of the benchmark dataset provided in the ‘9th eval-
uation lab on uncovering plagiarism, authorship, and social
software misuse’ (PAN) held as part of the CLEF 2013
conference. It involves 59 items: 10 for Training, 20 for
Test 1 and 29 for Test 2. Since ConNeKTion does not
require a training phase, the training set could be con-
sidered as part of the dataset. Table 8 reports statistics
on the English dataset, that are useful to understand the
amount of information to be dealt with when facing the
task. Specifically, the number of documents, the total num-
ber of clauses built from such documents and their aver-
age length, for both known and unknown documents, are
reported.
Table 9 reports the performance of the approach with
(smoothed evaluation) and without (boolean evaluation)
using the gray zone. Both for each sub-dataset and for the
entire dataset, there is a gain in misclassifications with the
Learning and exploiting concept networks with ConNeKTion
Table 8 Dataset details
Set Known docs
#docs #clauses avglength
min avg max min avg max min avg max
Training 2 3.20 5 51 178.87 274 121.06 166.82 216.37
Test 1 3 4.45 9 29 146.79 329 107.35 218.24 322.82
Test 2 2 4.27 14 29 145.59 367 100.81 209.55 319.59
Total 2 3.96 14 29 157.08 367 100.81 198.2 322.82
Set Unknown doc
#clauses avglength
min avg max min avg max
Training 24 56.10 96 134.65 194.22 322.09
Test 1 9 117.31 238 117.01 228.11 358.41
Test 2 9 56.00 301 110.29 213.85 351.18
Total 9 76.47 301 110.29 212.06 358.41
Table 9 Outcomes overview that sums up true positives (T.P.), true negatives (T.N.), false positives (F.P.), false negatives (F.N.) and not classified
(N.C.)
Type Set T.P. + T.N. F.P. F.N. N.C.
Training 0.7 0.3 0.0 0.0
boolean Test 1 0.7 0.15 0.15 0.0
evaluation Test 2 0.45 0.31 0.24 0.0
Total 0.58 0.25 0.17 0.0
Training 0.7 0.1 0.0 0.2
smoothed Test 1 0.65 0.1 0.05 0.2
evaluation Test 2 0.41 0.14 0.14 0.31
Total 0.55 0.12 0.08 0.25
Table 10 Evaluation of the grey zone application
Type Set Precision Recall F-measure
Training 0.7 0.7 0.7
boolean Test 1 0.7 0.7 0.7
evaluation Test 2 0.45 0.45 0.45
Total 0.58 0.58 0.58
Training 0.87 0.7 0.77
smoothed Test 1 0.81 0.65 0.72
evaluation Test 2 0.6 0.41 0.49
Total 0.73 0.55 0.62
F. Rotella et al.
Fig. 3 The main control panel
use of the gray zone, but a loss in correct classifications.
However, the gain is much more than the loss.
Table 10 reports the performance in terms of Precision,
Recall and F-measure. Let us consider each sub-dataset.
Precision with the use of the gray zone reports a gain,
whereas Recall reports a loss. Unlike the previous evalu-
ation, here both gain and loss are referred to the correct
classifications. In particular, the gain represents the decrease
in misclassifications with respect to the cases in which our
approach gives a response, whereas the loss represents the
correct classifications over the entire dataset. Among the
three sub-datasets, the gain is much more than the loss.
F-measure, that combines Precision and Recall, confirms
the predominance of the good performance given by the use
of the gray zone.
4 Exploitation tool
ConNeKTion provides its functionalities to the users
through a graphical tool that displays the concept network
in a large area (on the left part of the window), and includes
(on the right-hand-side of the window) a set of controls
allowing to explore, analyze and use it. Figure 3 shows two
Learning and exploiting concept networks with ConNeKTion
screenshots of the main interface, representing two different
perspectives on the same graph (a complete overview and a
selection thereof, respectively).
Among the general features, graphs can be learned
(incrementally, to avoid long times of unavailability of the
system), exported to a suitably organized XML file (includ-
ing concepts, relationships, weights and labels), and loaded
from files in the same format. The graph includes nodes for
concepts, nodes for relationships, edges that connect con-
cept nodes to relationship nodes, and edges that directly
connect two concepts between which at least a relationship
exists. When drawing the graph, nodes are automatically
organized in the best possible way to help readability. Dif-
ferent colors are used for concept and relationship nodes.
Also the relations are filled with a different color depend-
ing on the positive or negative valence of the corresponding
phrase. After loading a concept network, it can be explored
in an intuitive way, using classical mouse-based controls.
Since the compound view of the whole graph is typically
cluttered and very dense of (often overlapping) nodes and
edges (but still useful to grasp the overall shape of the net),
scroll, pan and zoom in/out controls allow to focus on spe-
cific parts thereof and to have a better insight on them.
Single nodes can be dragged as well, and the entire net is
automatically rearranged accordingly to best fit the avail-
able space. All the applications described in the previous
section can be started from the Tools menu. Conversely,
the controls for graph consultation and filtering, along with
their settings and results, are readily available in a panel
standing to the right of the graph visualization area. Such
a panel is in turn divided into several sub-parts (shown
in Fig. 4), which will be described in more details in the
following.
Figure 4a shows the part of the panel including the con-
trols for exploring the graph. Distance hops, placed in
the top part of the panel, contains a text field in which,
after selecting a specific node in the graph, the user can
enter a maximum neighborhood level up to which other
nodes are to be shown, starting from the selected one.
This allows to set a neighborhood limit such that all the
nodes whose shortest path to the selected node are out-
side the selected level are filtered out. Relevance filtering
Fig. 4 Panels for parameters
and results
F. Rotella et al.
contains two sliding bars: the former is Distance, aimed
at providing the same functionality as the Distance hops
area, but bound in [0, maxHops(net)], where maxHops(·)
is a function that returns the diameter of a given graph;
the latter is Relevance, that allows to set a threshold such
that only nodes whose relevance weight passes the thresh-
old are highlighted. Highlight nodes is a radio button that
allows to switch between the visualization that highlights
relevant nodes and the classical one. Choosing the relevant
nodes perspective enables access to the advanced func-
tionality of relevant node recognition, useful for a deeper
analysis of the collection. Using the Hibernation checkbox,
a human reader may more comfortably study the graph.
Indeed, when this checkbox is not selected, the search for a
balanced spatial placement of nodes within the interface is
always ‘alive’, so that the nodes automatically and continu-
ously rearrange their position in the screen, especially after
the perturbations introduced by the user when he moves
some elements to study them more comfortably. When the
checkbox is selected, manual rearrangement of single nodes
is still possible, but does not cause an adjustment in the
whole graph shape. Network embeds four options affect-
ing the number and type of elements that make up the
graph, and specifically: Show taxonomy includes/excludes
taxonomic relations; Show object-verb adds/removes verb
nodes, and associated edges towards their subjects and
objects; Show object-complement shows/hides direct edges
between subjects and objects involved in the same sen-
tences (regardless of the verbs connecting them); Show
tagged object-complement enables/disables the tagging of
the edges between subjects and objects with verbs and asso-
ciated (positive or negative) valence (so, the visual outcome
is the same as for Show object-complement, but the tagged
relations can be used by other functionalities).
The Reasoning section (shown in Fig. 4c), placed in
the middle of the panel, is devoted to the reasoning opera-
tors. It contains two text fields, in each of which a concept
(i.e., the label of a node in the graph) can be entered, so
that pressing the Search path button starts the Reasoning
by Association functionality in order to obtain a plausible
complex relation between the specified concepts. This sub-
area also contains a Search generalizations button that starts
the search for generalizations; its behavior can be modi-
fied by acting on two checkboxes, Get clusters from XML
(that avoids computing the clusters if they have already been
computed and stored in suitable XML files), and Run anti-
clustering (that starts the technique to build bridges between
different components of the net).
The Results section (shown in Fig. 4d), placed at the
bottom of the panel, is dedicated to textual results, such
as the paths obtained by Reasoning by Association, where
each row reports in square brackets (the labels of) the rela-
tions that exist between two adjacent nodes in the path. In
particular, each relation (verb) is associated to the number
and percentage of positive and negative instances in which
it occurred, expressing its valence. For instance, the screen-
shot in Fig. 4d shows the resulting path between nodes
‘symbol’ and ‘literature’:
sentence(symbol, humanity, [relate P: 1/1 (100.0 %), N: 0/1 (0.0 %)])
sentence(god, humanity, [reveal to P: 1/1 (100.0 %), N: 0/1 (0.0 %)])
sentence(teaching, god, [lead to P: 2/2 (100.0 %), N: 0/2 (0.0 %)])
sentence(teaching, literature, [include including P: 4/4 (100.0 %), N: 0/4 (0.0 %)])
which can be interpreted as: “Humanity can relate by means
of symbols. God reveals to humanity. Teaching (or edu-
cation) leads to God, and includes the use of literature.”.
Here only one relation per row is present, and there are no
sentences with negative valence.
Finally, an additional functionality concerns the possibil-
ity of querying the ProLog knowledge base expressing the
content of the net, which allows more complex kinds of rea-
soning than simple reasoning by association on the graph. It
can be accessed from menu Tools in the main window, using
the PROLOG user interface item. A window like that in
Fig. 4b is opened, that allows to enter a ProLog query to be
answered using the knowledge base (e.g., “what does a dog
eat?” might be asked in the form eat(dog,X) ). The ProLog
representation of the net can be obtained and saved from the
same window, by choosing the Create new K.B. item in the
Options menu.
Let us now describe a typical interaction scenario of a
user using ConNeKTion. Suppose the user has processed
a large collection of documents, involving a huge num-
ber of concepts. Since the learned network is so wide and
cluttered that surfing among the concepts would be very
hard, he selects the radio button “Show relevant concepts”
(Fig. 4a), so that nodes associated to relevant concepts
become red. Since the relevant concepts might still be many
more than needed, the user acts on the sliding button “Rel-
evance” available in the section “Relevance filtering” to
keep only the top k ones in the relevance ranked list. In
the filtered network, two concepts seem to be conceptu-
ally far, which raises the question about what do they have
to do with the collection contents. So, the user checks the
“Show object-verb” option in the “Network” control section
and clicks on the “Repaint whole net” button. Since the
resulting sub-graph includes many more concepts and rela-
tions than before, he de-selects “Show object-complement”
and clicks on “Repaint whole net” again, removing the
direct connections between objects and keeping the rela-
tions between objects and verbs only. Since the graph is
too complex to visually identify the indirect connection
among those concepts, the user exploits the functionality for
complex relationship discovery. In the “Reasoning” section
Learning and exploiting concept networks with ConNeKTion
(Fig. 4c) he types the names of the involved nodes in the
two textboxes and clicks on “Search path”, starting the
reasoning by association procedure. After a few seconds
one of the minimal paths connecting the selected pair of
concepts is identified by CoNeKTion and reported in the
“Results area” according to the formalism explained above.
Of course, the visual approach would make available many
possible paths of different complexity and length (limited
only by the borders of the displayed sub-graph), but the rea-
soning by association option provides an automatic, easy
to understand solution (which cognitively is consistent with
the principle of Occam’s razor).
5 Conclusions
The wide availability of digital documents calls for tools
that can support the user in retrieving, understanding and
exploiting them. In turn, these tools must exploit back-
ground knowledge in the form of linguistic resources that
are often not available. An important kind of resources are
concept networks.
This paper presented ConNeKTion, a tool that includes
several functionalities for learning concept networks from
plain text, structuring and enriching them by finding con-
cept generalizations, consulting and exploiting the learned
knowledge in several applications. These functionalities are
general and applicable to any language for which stan-
dard NLP pre-processing tools are available. After pre-
senting the basic techniques embedded in ConNeKTion to
learn the concept network, several applications are reported,
along with how ConNeKTion deals with them, a quick
overview of the state-of-the-art, and experimental results
showing that good performance is reached. Finally, the con-
trol panel of ConNeKTion, that allows to comfortably carry
out these activities, is described. The promising experi-
mental results encourage further extension of ConNeKTion,
both as regards the embedded techniques and as regards the
application fields.
Acknowledgments This work was partially funded by the Italian
PON 2007-2013 project PON02 00563 3489339 “Puglia@Service”.
References
1. Angioni M, Demontis R, Tuveri F (2008) A semantic approach for
resource cataloguing and query resolution. Commun SIWN Spec
Issue Distrib Agent-based Retr Tools 5:62–66
2. Argamon S, Whitelaw C, Chase P, Hota SR, Garg N, Levitan,
S (2007) Stylistic text classification using functional lexical:
research articles. J Am Soc Inf Sci Technol 58(6):802–822
3. Cimiano P, Hotho A, Staab S (2005) Learning concept hierarchies
from text corpora using formal concept analysis. J Artif Int Res
24(1):305–339
4. Crestani F (1997) Application of spreading activation techniques
in information retrieval. Artif Intell Rev 11:453–482
5. Deerwester S (1988) Improving information retrieval with latent
semantic indexing. In: Borgman CL, Pai EYH (eds) Proceedings
of the 51st ASIS annual meeting (ASIS 88), vol 25. American
Society for Information Science, Atlanta
6. Defays D (1977) An efficient algorithm for a complete link
method. Comput J 20(4):364–366
7. Dhillon IS, Modha DS (2001) Concept decompositions for large
sparse text data using clustering. In: Machine learning, pp 143–
175
8. Fellbaum C (ed) (1998) An electronic lexical database. MIT Press,
Cambridge
9. Ferilli S (2011) Automatic digital document processing and man-
agement, problems, algorithms and techniques, 1st edn. Springer
Publishing Company, Incorporated
10. Ferilli S, Basile TMA, Di Mauro N, Esposito F (2011) Plugging
numeric similarity in first-order logic horn clauses comparison.
In: Pirrone R, Sorbello F (eds) 7th international conference of the
Italian association for artificial intelligence, vol 6934. Springer,
LNCS, pp 33–44
11. Ferilli S, Biba M, Basile TMA, Esposito F (2009) Combin-
ing qualitative and quantitative keyword extraction methods with
document layout analysis. In: Post-proceedings of the 5th Italian
research conference on digital libraries - IRCDL 2009, Padova
Italy, 29–30 January 2009, pp 22–33
12. Ferilli S, Biba M, Di Mauro N, Basile TMA, Esposito F
(2009) Plugging taxonomic similarity in first-order logic horn
clauses comparison. In: Emergent perspectives in artificial intel-
ligence, lecture notes in artificial intelligence. Springer, pp 131–
140
13. Ferilli S, Leuzzi F, Rotella F (2011) Cooperating techniques for
extracting conceptual taxonomies from text. In: Proceedings of the
workshop on mining complex patterns at AI*IA 7th conference
14. Gale W. A., Church K. W., Yarowsky D. (1992) One sense per
discourse. In: DARPA speech and natural language workshop
15. Gupta V, Lehal G (2009) A survey of text mining techniques and
applications. J Emerg Tech Web Intell 1(1):60–76
16. Hall M, Frank E, Holmes G, Pfahringer B, Reutemann P, Witten
IH (2009) The weka data mining software: An update. SIGKDD
Explor Newsl 11(1):10–18
17. Hamming RW (1950) Error detecting and error correcting codes.
Bell Syst Tech J 26(2):147–160
18. Hasegawa R, Kitamura M, Kaiya H, Saeki M (2009) Extracting
conceptual graphs from Japanese documents for software require-
ments modeling. In: Proceedings of the 6th APCCM, APCCM 09,
vol 96. Australian Computer Society, Inc., Darlinghurs, Australia,
pp 87–96
19. Hensman S (2004) Construction of conceptual graph represen-
tation of texts. In: Proceedings of the student research workshop
at HLT-NAACL 2004, HLT-SRWS 04. Association for Computa-
tional Linguistics Stroudsburg, pp 49–54
20. Jones WP, Furnas GW (1987) Pictures of relevance: a geometric
analysis of similarity measures. J Amer Soc Inf Sci 38(6):420–442
21. Karypis G, Han E-H (2000) Concept indexing: a fast dimension-
ality reduction algorithm with applications to document retrieval
and categorization. Technical report tr-00-0016, University of
Minnesota
22. Karypis G, (Sam) Han E-H (2000) Concept indexing: a fast
dimensionality reduction algorithm with applications to document
retrieval and categorization. Technical report, IN CIKM00
23. Kimmig A, Costa VS, Rocha R, Demoen B, De Raedt L (2008) On
the efficient execution of problog programs. In: Garcia de la Banda
M, Pontelli E (eds) ICLP, Lecture notes in computer Science, vol
5366. Springer, pp 175–189
F. Rotella et al.
24. Kimmig A, De Raedt L, Toivonen H (2007) Probabilistic explana-
tion based learning. In: ECML, pp 176–187
25. Kipper K, Dang HT, Palmer M (2000) Class-based construction of
a verb lexicon. In: Proceedings of the 17th NCAI and 12th IAAI
conference. AAAI Press, pp 691–696
26. Klein D, Manning CD (2003) Fast exact inference with a fac-
tored model for natural language parsing. In: Advances in neural
information processing systems, vol 15. MIT Press
27. Koo S-O, Lim S-Y, Lee S-J (2003) Constructing an ontology
based on hub words. In: ISMIS03, pp 93–97
28. Leuzzi F, Ferilli S, Rotella F (2013) ConNeKTion: a tool for
handling conceptual graphs automatically extracted from text.
In: Catarci T, Ferro N, Poggi A (eds) Bridging between cul-
tural Heritage Institutions Proceedings of the 9th Italian research
conference on digital libraries (IRCDL 2013), CCIS, vol 385.
Springer
29. Leuzzi F, Ferilli S, Rotella F (2013) Improving robustness and
flexibility of concept taxonomy learning from text. In: Appice A,
Ceci M, Loglisci C, Manco G, Masciari E, Ras ZW (eds) New
frontiers in mining complex patterns - first International Work-
shop, NFMCP 2012, Held in Conjunction with ECML/PKDD
2012, Bristol, UK, September 24, 2012 Revised Selected Papers,
CCIS, vol 7765. Springer, pp 232–244
30. Leuzzi F, Ferilli S, Taranto C, Rotella F (2013) A relational
unsupervised approach to author identification. In: Workshop new
frontiers in mining complex patterns 2013 held at ECML-PKDD
2013
31. Maedche A, Staab S (2000) Mining ontologies from tex. In:
EKAW, pp 189–202
32. Maedche A, Staab S (2000) The text-to-onto ontology learning
environment. In: ICCS-2000 — 8th international conference on
conceptual structures, software demonstration
33. Magnini B, Cavaglià G (2000) Integrating subject field codes into
wordnet, pp 1413–1418
34. De Marneffe M-C, Maccartney B, Manning CD (2006) Gener-
ating typed dependency parses from phrase structure parses. In:
Proceedings international conference on language resources and
evaluation (LREC), pp 449–454
35. Matsuo Y, Ishizuka M (2004) Keyword extraction from a single
document using word co-occurrence statistical information. Int J
Artif Intell Tools 13:2003
36. Mccarthy PM, Lewis GA, Dufty DF, Mcnamara DS (2006) Ana-
lyzing writing styles with coh-metrix. In: Sutcliffe G, Goebel R
(eds) Proceedings of the Florida artificial intelligence research
society international conference (FLAIRS). AAAI Press, pp 764–
769
37. Miller GA (1995) Wordnet: A lexical database for English. Com-
mun ACM 38(11):39–41
38. Ogata N (2001) A formal ontology discovery from web doc-
uments. In: Web intelligence: research and development, 1st
Asia-Pacific conference (WI 2001), lecture notes on artificial
intelligence, no 2198. Springer, pp 514–519
39. O’Madadhain J, Fisher D, White S, Boey Y (2003) The JUNG
(Java Universal Network/Graph) framework. Technical report,
UCI-ICS
40. Qiu L, Kan M-Y, Chua T-S A public reference implementation
of the RAP anaphora resolution algorithm. In: Proceedings of the
4th international conference on language resources and evaluation,
LREC 2004, May 26–28, 2004. European Language Resources
Association, Lisbon, pp 291–294
41. De Raedt L, Kimmig A, Toivonen H (2007) Problog: a probabilis-
tic prolog and its application in link discovery. In: Proceedings of
20th IJCAI. AAAI Press, pp 2468–2473
42. Raghavan S, Kovashka A, Mooney R (2010) Authorship attribu-
tion using probabilistic context-free grammars. In: Proceedings of
the ACL 2010 conference short papers, ACLShort 10. Association
for Computational Linguistics, Stroudsburg, pp 38–42
43. Robertson SE, Walker S, Jones S, Hancock-Beaulieu MM,
Gatford M (1996) Okapi at trec-3, pp 109–126
44. Rotella F, Ferilli S, Leuzzi F (2013) An approach to automated
learning of conceptual graphs from text. In: Ali M, Bosse T,
Hindriks KV, Hoogendoorn M, Jonker CM, Treur J (eds) Recent
trends in applied artificial intelligence, 26th international confer-
ence on industrial, engineering and other applications of applied
intelligent systems, IEA/AIE 2013, Amsterdam, The Netherlands,
17-21 June 2013, Proceedings of lecture notes in computer sci-
ence, vol 7906. Springer, pp 341–350
45. Rotella F, Ferilli S, Leuzzi F (2013) A domain based approach to
information retrieval in digital libraries. In: Agosti M, Esposito F,
Ferilli S, Ferro N (eds) Digital Libraries and archives - 8th Ital-
ian research conference, IRCDL 2012, Bari, Italy, 9-10 Feb 2012.
Revised selected papers, CCIS, vol 354. Springer-Verlag, Berlin
Heidelberg, pp 129–140
46. Salton G (1971) The SMART retrieval system experiments in
automatic document processing. Prentice-Hall, Upper Saddle
River
47. Salton G (1980) Automatic term class construction using
relevance–a summary of work in automatic pseudoclassification.
Inf Process Manage 16(1):1–15
48. Salton G., McGill M. (1984) Introduction to modern information
retrieval. McGraw-Hill Book Company
49. Salton G, Wong A, Yang CS (1975) A vector space model for
automatic indexing. Commun ACM 18:613–620
50. Sato T (1995) A statistical learning method for logic programs
with distribution semantics. In: Proceedings of the 12th ICLP
1995. MIT Press, pp 715–729
51. Semeraro G, Esposito F, Malerba D, Fanizzi N, Ferilli S (1997) A
logic framework for the incremental inductive synthesis of datalog
theories. In: Fuchs, NE (ed)LOPSTR, Lecture notes in computer
science, vol 1463. Springer, pp 300–321
52. Shamsfard M, Barforoush AA (2004) Learning ontologies from
natural language texts. Int J Hum-Comput Stud 60(1):17–63
53. Singhal A, Buckley C, Mitra M, Mitra A (1996) Pivoted document
length normalization. ACM Press, pp 21–29
54. Stamatatos E (2009) A survey of modern authorship attribution
methods. J Am Soc Inf Sci Technol 60(3):538–556
55. van Halteren H (2004) Linguistic profiling for author recognition
and verification. In: Proceedings of the 42nd annual meeting on
association for computational linguistics, ACL 04. Association or
Computational Linguistics, Stroudsburg
56. Velardi P, Navigli R, Cucchiarelli A, Neri F (2006) Evaluation
of OntoLearn, a methodology for automatic population of domain
ontologies. In: Ontology learning from text: methods, applications
and evaluation. IOS Press
57. Wu Z, Palmer M (1994) Verbs semantics and lexical selection. In:
Proceedings of the 32nd annual meeting on association for com-
putational linguistics. Association for Computational Linguistics,
Morristown, pp 133–138
58. Zesch T, Müller C, Gurevych I (2008) Extracting lexical seman-
tic knowledge from wikipedia and wiktionary. In: Proceedings
of the 6th International Conference on Language Resources and
Evaluation, Marrakech, Morocco, Electronic proceedings
59. Zheng R, Li J, Chen H, Huang Z (2006) A framework for author-
ship identification of online messages: writing-style features and
classification techniques. J Am Soc Inf Sci Technol 57(3):378–
393
Learning and exploiting concept networks with ConNeKTion
Fulvio Rotella received his Master Degree with full marks and honors
in Computer Science from the University of Bari in 2011 discussing
a thesis on the “Extraction and Filtering of Concepts and Relation-
ships from Natural Language Texts”. He started a Ph.D. Course at the
University of Bari, focusing on methods for learning and reasoning
in complex domains involving textual, social and biological data by
means of joint approaches between logic and probability (Probabilis-
tic Inductive/Abductive Logic Programming and Statistical Relational
Learning). His work also concerned the definition of a Probabilistic
Abductive logical operator.
Fabio Leuzzi received his master’s degree with full marks and honors
in Computer Science in July, 2011 at the University of Bari Aldo Moro,
where he is a PhD student since February, 2012. He is the author of sev-
eral publications regarding text mining, knowledge representation and
reasoning. In addition to his work on ConNeKTion, his research inter-
ests focus on Automated Reasoning by Analogy and Argumentation,
with the aim to exploit these operators in more complex strategies of
Multi-Strategy Reasoning and Learning. Pursuing these objectives, he
is also interested in issues concerning the Knowledge Representation
field.
Stefano Ferilli born 1972, Laurea degree in Computer Science (1996),
Ph.D. in Computer Science (2001), Specialistic Laurea degree in Com-
puter Science (2003), qualifying examination as an Engineer in 2005.
Since 2002 Assistant Professor, and since 2006 Associate Professor in
Computer Science at the University of Bari, where he serves as the
Head of the Inter-departmental Center for research on Logic and its
Applications. His scientific interests are centered on Expert Systems
and on Knowledge Representation and Acquisition, with particular ref-
erence to Incremental and Multi-strategy Learning using First-Order
Logic formalisms. He is the author of a book on Document Processing
published by Springer, 2 textbooks and more than 150 scientific papers
published on National and International journals, books and confer-
ences/workshops proceedings. He served in the Program Committee
of National and International events, and as a reviewer of scientific
papers submitted to national and international journals, conferences
and workshops.
