Τσιµπουκάκης Νικόλαος   
Υποψήφιος ∆ιδάκτορας ΕΜΠ 
ntsimb@ilsp.gr 
Ταµπουρατζής Γεώργιος 
Ερευνητής Α’ ΙΕΛ                           
giorg_t@ilsp.gr 
Καραγιάννης Γεώργιος  
Καθηγητής ΕΜΠ 
gcara@ilsp.gr
 
 
Περίληψη –  Αντικείµενο της εργασίας αυτής είναι η 
κατηγοριοποίηση κειµένων µε βάση το συγγραφέα από τον 
οποίον προήλθαν µε χρήση τεχνητών νευρωνικών δικτύων. H 
κατηγοριοποίηση των κειµένων βασίστηκε στην υφολογική 
ανάλυσή τους, µε χρήση γλωσσικών χαρακτηριστικών τα οποία 
µετρώνται αυτόµατα. Κάθε κείµενο αναπαρίσταται στο χώρο 
των προτύπων µε ένα διάνυσµα που αποτελείται από τις τιµές 
των επιλεχθέντων χαρακτηριστικών. Έπειτα τα διανύσµατα 
των χαρακτηριστικών χρησιµοποιούνται ως είσοδοι σε έναν 
κατηγοριοποιητή που στην περίπτωσή µας είναι ένα νευρωνικό 
δίκτυο πολυστρωµατικού τύπου (multi-layer perceptron - 
MLP). Ο αλγόριθµος εκπαίδευσης του δικτύου, ο οποίος 
επιλέχθηκε, αποσκοπεί πέρα από τη µείωση του σφάλµατος 
κατηγοριοποίησης και στην απλοποίηση του µοντέλου. Με τον 
τρόπο αυτό είναι δυνατόν να αξιολογηθούν τα χαρακτηριστικά 
που χρησιµοποιήθηκαν ανάλογα µε τη συµβολή τους στο τελικό 
αποτέλεσµα. Τέλος, παρουσιάζονται και συγκρίνονται η 
ακρίβεια κατηγοριοποίησης για νευρωνικά δίκτυα 
διαφορετικών µεγεθών. 
 
Λέξεις Κλειδιά: νευρωνικά δίκτυα, µηχανική µάθηση, 
κατηγοριοποίηση κειµένων, υφολογική ανάλυση, επιλογή 
χαρακτηριστικών. 
I. ΕΙΣΑΓΩΓΗ 
Ο όγκος της πληροφορίας που είναι διαθέσιµη σε 
ηλεκτρονική µορφή και ειδικότερα σε µορφή απλού 
κειµένου, εξαιτίας της διάδοσης της χρήσης του διαδικτύου, 
αυξάνεται συνεχώς. Η αύξηση αυτή είναι δυνατόν να 
οδηγήσει σε αχρήστευση πολύ σηµαντικού µέρους των 
πληροφοριών εφόσον δεν οργανωθούν κατάλληλα, καθώς 
είναι δύσκολο να εντοπισθεί και να χρησιµοποιηθεί 
αποτελεσµατικά από τον απλό χρήστη. Είναι πολύ 
σηµαντικό να γίνεται µε αποδοτικό τρόπο αναζήτηση της 
υπάρχουσας πληροφορίας µέσα σε ένα µεγάλο αριθµό 
κειµένων. Η ταξινόµηση των κειµένων είναι µία πολύ 
χρήσιµη διαδικασία η οποία µας επιτρέπει την οργάνωση 
ενός σώµατος κειµένων µε βάση κάποια κοινά 
χαρακτηριστικά, διευκολύνοντας την αναζήτηση στο σώµα 
αυτό. Από την άλλη µεριά η διαδικασία αυτή πρέπει να 
σχεδιαστεί µε πολύ προσοχή ώστε τα αποτελέσµατά της να 
συµβαδίζουν µε τον τρόπο σκέψης του ανθρώπου, αφού το 
σύστηµα θα χρησιµοποιηθεί από ανθρώπους. Για το λόγο 
αυτό οι κατηγορίες πρέπει να είναι σε αρµονία µε την 
ανθρώπινη αντίληψη. Για παράδειγµα, είναι πιο εύκολο για 
τον άνθρωπο να αναζητά κείµενα που έχουν οργανωθεί µε 
βάση το συγγραφέα ή το περιεχόµενό τους παρά µε βάση το 
µέγεθός τους. Η αποτελεσµατικότητα των αλγόριθµων 
αναζήτησης καθορίζεται από δύο πολύ σηµαντικές µετρικές, 
(ι) την ακρίβεια (precision) και (ιι) την ανάκληση (recall) 
[1]. Ως ακρίβεια ορίζεται το ποσοστό των κειµένων που 
επιστρέφει ο αλγόριθµος αναζήτησης και τα οποία είναι 
σχετικά µε το θέµα. Αντίστοιχα η ανάκληση πληροφορίας 
ορίζεται ως το ποσοστό των κειµένων που ανευρέθηκαν σαν 
σχετικά µε το θέµα αναζήτησης, σε σχέση µε το σύνολο των 
κειµένων. Είναι επιθυµητό σε έναν αλγόριθµο αναζήτησης, 
και οι δύο αυτές µετρικές να έχουν αρκετά υψηλές τιµές αν 
και κάτι τέτοιο δεν είναι πάντα εφικτό.  
Κάθε κείµενο είναι γραµµένο µε το δικό του ύφος, το οποίο 
µαζί µε το περιεχόµενό του, το ξεχωρίζει αλλά και το 
συσχετίζει µε άλλα κείµενα [2]. Ως υφολογική ανάλυση ενός 
κειµένου ορίζεται η στατιστική ανάλυση – καταγραφή ενός 
συνόλου από µετρήσιµες ιδιότητες [3]. Είναι δυνατόν ένα 
σώµα  κειµένων να διαχωριστεί ως προς το είδος ή το 
συγγραφέα µε βάση τον τρόπο γραφής, όπως εκφράζεται 
από την υφολογική ανάλυση. Τέλος, επιλέγοντας κάποιες 
από τις ιδιότητες ενός κειµένου είναι δυνατόν να 
διαχωρίσθουν τα κείµενα µε βάση το ύφος του συγγραφέα. 
Η επιλογή των κατάλληλων γλωσσικών χαρακτηριστικών 
µπορεί να φανερώσει τον ιδιαίτερο τρόπο µε τον οποίο ένας 
συγγραφέας χρησιµοποιεί τη γλώσσα για να εκφραστεί [4].  
Το σύστηµα που παρουσιάζεται αποτελείται συνήθως από 
δύο µέρη. Το πρώτο µέρος αναλαµβάνει την εξαγωγή – 
µέτρηση από κάθε κείµενο των προεπιλεγµένων 
χαρακτηριστικών µε αυτοµατοποιηµένο τρόπο. Το δεύτερο 
µέρος, χρησιµοποιώντας ως είσοδο το διάνυσµα που 
παράγεται από το πρώτο µέρος, διαχωρίζει µε τη χρήση ενός 
πολυστρωµατικού νευρωνικού δικτύου (MLP) τα κείµενα σε 
προεπιλεγµένες κατηγορίες. Γενικότερα, στο στάδιο αυτό, 
θα µπορούσε να χρησιµοποιηθεί οποιαδήποτε µέθοδος 
κατηγοριοποίησης (classifier). Ενδεικτικά αναφέρεται η 
διακριτική ανάλυση (discriminant analysis), η οποία και έχει 
χρησιµοποιηθεί σε αρκετές εφαρµογές στο παρελθόν [5].  
 
II. ΑΡΧΙΤΕΚΤΟΝΙΚΗ ΚΑΙ ΛΕΙΤΟΥΡΓΙΑ ΠΟΛΥΣΤΡΩΜΑΤΙΚΩΝ 
∆ΙΚΤΥΩΝ 
 
A. Αρχιτεκτονική 
 
Η δοµή ενός πολυστρωµατικού δικτύου (MLP – multilayer 
perceptron) έχει ως εξής [6]:  
Αναγνώριση συγγραφέων από κείµενα µε χρήση Τεχνητών 
Νευρωνικών ∆ικτύων MLP 
Περιοχή έρευνας: Εφαρµογές νευρωνικών δικτύων 
(ι) το επίπεδο εισόδου όπου και εφαρµόζονται οι 
είσοδοι στο σύστηµα.  
(ιι) στη συνέχεια τα ενδιάµεσα κρυµµένα επίπεδα  
(ιιι). Και τέλος το επίπεδο εξόδου.  
Οι νευρώνες κάθε επιπέδου συνδέονται µε τους νευρώνες 
των επόµενων επιπέδων µε τα βάρη wmij , όπου w
m
ij είναι το 
βάρος που συνδέει τον i νευρώνα του m επιπέδου µε το j 
νευρώνα του m+1 επιπέδου. Στα δίκτυα αυτά, κάθε 
νευρώνας συνδέεται µόνο µε νευρώνες επόµενων επιπέδων, 
γι’ αυτό και ανήκουν στην κατηγορία των δικτύων 
προσοτροφοδότησης (feed forward networks). Γενικά 
συνηθίζεται κάθε νευρώνας να συνδέεται µε όλους (ή 
κάποιους από) τους νευρώνες µόνο του αµέσως επόµενου 
επιπέδου, οπότε και το νευρωνικό δίκτυο ονοµάζεται 
δοµηµένο και έχει τη µορφή του παρακάτω σχήµατος 
(σχήµα 4.1).  
 
 
Σχ. 1. Αρχιτεκτονική δικτύου MLP. 
 
Η έξοδος κάθε νευρώνα είναι ίση µε το άθροισµα του 
γινοµένου των εισόδων του επί τα αντίστοιχα βάρη τους 
αυξηµένο κατά µια σταθερά, το κατώφλι (bias) και 
περασµένο µέσα από µια συνάρτηση ενεργοποίησης f.  Η 
έξοδος ενός νευρώνα εφαρµόζεται ως είσοδος στους 
νευρώνες των επόµενων επιπέδων µε τους οποίους 
συνδέεται, µέχρι το επίπεδο εξόδου. Πιο αναλυτικά το βάρος 
που συνδέει το j νευρώνα του m  επιπέδου µε το i νευρώνα 
του m+1, συµβολίζεται µε wji
m, µε bi
m το κατώφλι του 
νευρώνα και  µε yi
m η έξοδός του. Τότε η αντίστοιχη έξοδος 
είναι ίση µε: 





=
≥+⋅
=
∑ −−
1
211
mx
m)b)ywf((
y
i
m
i
j
mm
m
i
jji
  (1) 
όπου f η συνάρτηση ενεργοποίησης και xi οι είσοδοι. 
Παρατηρούµε ότι για το πρώτο επίπεδο, δηλαδή για 1=m , 
η έξοδος  του κάθε νευρώνα είναι ίδια µε την είσοδο. 
Γενικά η συνάρτηση ενεργοποίησης f είναι επιθυµητό να 
είναι συνεχής και παραγωγίσιµη, κάτι που απλοποιεί σε 
µεγάλο βαθµό τους αλγορίθµους εκπαίδευσης. Επίσης, η 
συνάρτηση αυτή τις περισσότερες φορές προσεγγίζει τη 
διπολική διακριτική συνάρτηση, χωρίς φυσικά να 
αποκλείονται και άλλες µορφές ανάλογα µε τις ανάγκες του 
προβλήµατος. Οι πιο συχνά χρησιµοποιούµενες συναρτήσεις 
είναι: 
• Η λογιστική σιγµοειδής συνάρτηση που έχει τύπο:  
axe
sig(x)log
−+
=
1
1
 (2) 
όπου α είναι µια παράµετρος που ορίζει τη µορφή και πιο 
συγκεκριµένα την κλίση της καµπύλης της συνάρτησης (όσο 
πιο µεγάλη είναι η τιµή του α τόσο πιο απότοµη είναι η 
καµπύλη).  
• Η υπερβολική εφαπτοµένη:  
ax
ax
e
e
sig(x)tan
−
−
+
−
=
1
1
 (3) 
όπου α παράµετρος που καθορίζει τη µορφή της καµπύλης.  
• Μια απλή γραµµική συνάρτηση:  
axf(x) =  (4) 
η οποία είναι για οποιοδήποτε εύρος του x και 
χρησιµοποιείται κυρίως  για προβλήµατα προσέγγισης 
συνάρτησης (regression). Στα περισσότερα προβλήµατα 
αναγνώρισης προτύπων χρειαζόµαστε δύο καταστάσεις για 
τις εξόδους (π.χ. κάτι ανήκει ή δεν ανήκει σε µια κατηγορία) 
οπότε χρησιµοποιούµε διπολικές συναρτήσεις, σιγµοειδούς 
συνήθως µορφής, που περιορίζουν το εύρος των τιµών στο 
[-1 1] ή στο [0 1]. Όταν όµως επιδιώκεται η προσέγγιση µιας 
συνάρτησης της οποίας οι τιµές δεν περιορίζονται σε κάποιο 
εύρος, τότε δεν είναι αποδοτική η χρήση διπολικών 
συναρτήσεων και προτιµώνται απλές γραµµικές 
συναρτήσεις στην έξοδο, καθώς δεν έχουν περιορισµένο 
πεδίο τιµών. 
Συνηθίζεται στα περισσότερα πολυστρωµατικά νευρωνικά 
δίκτυα (MLP) να χρησιµοποιείται µόνο ένα κρυφό επίπεδο. 
Αυτό συµβαίνει γιατί έχει αποδειχθεί ότι  ένα δίκτυο µε ένα 
µόνο ένα κρυφό επίπεδο έχει τις ίδιες δυνατότητες, σε ό,τι 
αφορά την απεικόνιση των δεδοµένων µε ένα δίκτυο µε 
περισσότερα επίπεδα, εφόσον έχει επαρκή αριθµό από 
νευρώνες στο κρυφό επίπεδο. Είναι δηλαδή δυνατόν 
αυξάνοντας τον αριθµό των νευρώνων στο µοναδικό κρυφό 
επίπεδο να επιτυγχάνεται η ίδια απόδοση µε ένα δίκτυο µε 
περισσότερα επίπεδα.  
Πιο αναλυτικά το θεώρηµα Kolmogorov αναφέρει τα εξής:  
«Για κάθε ε>0 και κάθε συνεχή συνάρτηση F υπάρχει ένα 
πολυστρωµατικό νευρωνικό δίκτυο µε ένα µοναδικό 
κρυµµένο στρώµα τέτοιο ώστε να ισχύει για την έξοδο του 
δικτύου y: ε<− )x(y)x(F για κάθε είσοδο x». Το 
θεώρηµα αυτό µας εξασφαλίζει ότι ένα κρυµµένο επίπεδο 
είναι αρκετό για την προσέγγιση οποιασδήποτε συνάρτησης 
από το δίκτυό µας [7]. Στα περισσότερα πολυστρωµατικά 
νευρωνικά δίκτυα (MLP) όλοι οι νευρώνες έχουν την ίδια 
συνάρτηση ενεργοποίησης f.   
 
 
B. Εκπαίδευση 
 
Μια πολύ σηµαντική ικανότητα των νευρωνικών δικτύων 
είναι η δυνατότητα να «µαθαίνουν», δηλαδή να 
προσαρµόζουν τα βάρη τους (αλλάζοντας τις τιµές τους ) µε 
τέτοιο τρόπο ώστε να παράγουν περισσότερο «επιθυµητές» 
εξόδους. Καθώς σε ένα δίκτυο MLP δίνεται εκτός από την 
είσοδο και η επιθυµητή έξοδος, ο τρόπος αυτός εκπαίδευσης 
ονοµάζεται εκπαίδευση µε επίβλεψη (supervised). Για τα 
πολυστρωµατικά νευρωνικά δίκτυα ο πιο διαδεδοµένος 
αλγόριθµος εκπαίδευσης είναι ο αλγόριθµος της 
ανάστροφης διάδοσης (Back Propagation) που σκοπεύει στη 
µείωση της τιµής της συνάρτησης σφάλµατος [6] [7].  
 
Σχ. 2. Εφαρµογή αλγορίθµου ανάστροφης διάδοσης σε 
MLP µε ένα κρυµµένο επίπεδο. 
 
Ο αλγόριθµος ανάστροφης διάδοσης λειτουργεί ως εξής: 
Αρχικά παρουσιάζεται κάποιο παράδειγµα στην είσοδο του 
δικτύου και υπολογίζεται η έξοδος του δικτύου, 
υπολογίζοντας πρώτα τις ενδιάµεσες αποκρίσεις του 
δικτύου, µε φορά από την είσοδο προς την έξοδο µε τον 
τύπο (1) για m=1,2,3.  
Από την έξοδό του δικτύου υπολογίζεται η τιµή του 
σφάλµατος. Η συνάρτηση σφάλµατος που χρησιµοποιείται 
συνήθως είναι το τετραγωνικό σφάλµα:  
∑=
i
ip ))k((e)k(E
2
2
1
 (5) 
µε )k(y)k(d)k(e iii
3−=  (6) 
όπου di(k) είναι η επιθυµητή έξοδος για την έξοδο i του 
επιπέδου 3 για το διάνυσµα εισόδου k (οι έξοδοι του τρίτου 
επιπέδου είναι και οι έξοδοι του δικτύου αφού το νευρωνικό 
δίκτυο έχει µόνο ένα κρυµµένο επίπεδο). 
Στη συνέχεια υπολογίζονται µε φορά από την έξοδο προς 
την είσοδο (δηλ. µε ανάστροφη φορά) τα σήµατα 
σφάλµατος δ. Πιο αναλυτικά, µε βάση τον κανόνα της 
κατάβασης στην επιφάνεια σφάλµατος (Steepest Gradient 
Descent) προκύπτει ο  παρακάτω τύπος (7) που µας δίνει 
έναν τρόπο για την αλλαγή των βαρών ώστε να µειωθεί το 
σφάλµα του δικτύου: 
m
ji
pm
ji
w
)k(dE
γ∆w −=  (7) 
όπου γ είναι µια θετική παράµετρος µικρότερη της µονάδας 
και καθορίζει το ρυθµό µάθησης.  
Συµβολίζοντας τώρα µε  hi
m το άθροισµα των σηµάτων 
διέγερσης της συνάρτηση ενεργοποίησης f προκύπτει ότι:  
∑ +⋅= −−
j
m
i
m
ji
m
j
m
i bwyh
11
 (8) 
Η παράγωγος της επιφάνειας του σφάλµατος ως προς τα 
βάρη, µε χρήση του κανόνα της αλυσίδας είναι: 
m
ji
m
i
m
i
m
i
m
i
p
m
ji
p
dw
dh
dh
dy
dy
)k(dE
dw
)k(dE 1
1
1
1
+
+
+
+
=  (9) 
Θέτοντας: 
1
1
1 +
+
+
−=
m
i
m
i
m
i
pm
i
dh
dy
dy
)k(dE
δ  (10) 
Προκύπτει ότι: 
m
j
m
im
ji
p
y
dw
)k(dE
δ−=  (11) 
Συνδυάζοντας τις σχέσεις (7), (10) και (11) προκύπτει: 
m
j
m
i
m
ji yw γδ=∆ (12). 
Από τον τύπο (12) φαίνεται ότι ο αλγόριθµος µάθησης 
ανάστροφης διάδοσης υπακούει στον κανόνα του Hebb, 
καθώς η αλλαγή στα βάρη εξαρτάται τόσο από την είσοδο 
που δέχεται ο νευρώνας (yj
m) όσο και από τα σφάλµατα δi
m 
που του αποδίδονται.  
Στη συνέχεια αν χρησιµοποιείται η σιγµοειδής συνάρτηση 
ενεργοποίησης (2) µε a=1 η  παράγωγος είναι:   
))x(siglog)(x(siglog...
dx
)x(siglogd
−== 1 (13) 
Ενώ αν χρησιµοποιείται η υπερβολική εφαπτοµένη (3): 
21 ))x(sig(tan...
dx
)x(sigtand
−== (14) 
Έπειτα θα υπολογισθούν υπολογίζονται αναλυτικά τα 
σήµατα σφάλµατος για κάθε ένα από τα τρία επίπεδα ενός 
τυπικού δικτύου MLP. Με τον τρόπο βρίσκουµε τις 
διορθώσεις που πρέπει να γίνουν στα βάρη του δικτύου για 
κάθε επίπεδο. Πιο συγκεκριµένα: 
Για το επίπεδο εξόδου τα σήµατα σφάλµατος είναι: 
3
3
3
3
3
2
i
i
i
i
i
i
p
i
dh
)h(df
e...
dh
dy
dy
)k(dE
⋅==−=δ  (15) 
Τα βάρη τότε ενηµερώνονται µε βάση την εξίσωση (12). 
Αντίστοιχα για τα κατώφλια bi
m µπορούµε να θεωρήσουµε 
ότι είναι και αυτά βάρη µε  σταθερή είσοδο ίση µε 1. 
Προφανώς για το κρυµµένο επίπεδο δεν έχουµε τη 
δυνατότητα να  γνωρίζουµε την επιθυµητή έξοδο, οπότε 
αυτή πρέπει να εκτιµηθεί από τα σήµατα σφάλµατος δ των 
νευρώνων εξόδου. ∆ηλαδή θα εκτιµηθεί από το άθροισµα 
των σφαλµάτων όλων των νευρώνων µε τα οποία συνδέεται 
ανάλογα µε την τιµή του βάρους µέσω του οποίου 
συνδέεται. Πιο αναλυτικά, εφαρµόζοντας και πάλι τον 
κανόνα της αλυσίδας για τα βάρη εισόδου του κρυµµένου 
επιπέδου έχουµε (k=1):  
⇒⋅−=
==−=−=
∑
j i
j
j
j
j
p
i
i
i
i
i
p
i
i
i
p
i
dy
dh
dh
dy
dy
)k(dE
dh
)h(df
...
dh
)h(df
dy
)k(dE
dh
dy
dy
)k(dE
2
3
3
3
32
2
2
2
22
2
2
1δ
 
∑⋅−=
j
jij
i
i
i w
dh
)h(df 22
2
2
1 δδ  (16) 
Η εκπαίδευση ενός πολυστρωµατικού νευρωνικού δικτύου 
τύπου MLP είναι µια επαναληπτική διαδικασία (iterative) 
κατά την οποία παρουσιάζεται στο δίκτυο ένα σύνολο από 
δεδοµένα, τα δεδοµένα εκπαίδευσης, και για κάθε δεδοµένο 
επιβάλλονται στο δίκτυο οι επιθυµητές έξοδοι. Κάθε 
παρουσίαση των δεδοµένων εκπαίδευσης στο δίκτυο 
ονοµάζεται εποχή (epoch). Πολλοί αλγόριθµοι εκπαίδευσης 
αντί να αλλάζουν τα βάρη µε κάθε παράδειγµα που 
παρουσιάζεται κάνουν συνολική αλλαγή των βαρών µετά το 
πέρας όλου του συνόλου εκπαίδευσης, δηλαδή στο τέλος 
κάθε εποχής. Ο αλγόριθµος εκπαίδευσης δεν εγγυάται τη 
σύγκλιση όταν τα βάρη ανανεώνονται µε το πέρασµα κάθε 
παραδείγµατος του συνόλου εκπαίδευσης. Αυτό συµβαίνει 
γιατί η κλίση που υπολογίζεται, για την ενηµέρωση των 
βαρών υπολογίζεται µόνο για ένα παράδειγµα και όχι για 
όλο το σύνολο εκπαίδευσης. Για να εξασφαλισθεί η 
σύγκλιση, ο αλγόριθµος πρέπει να βασίζεται σε όλο το 
σύνολο εκπαίδευσης. Ο βελτιωµένος αυτός αλγόριθµος 
ανανεώνει τα βάρη µια φορά οµαδικά στο τέλος κάθε 
εποχής (batch training) µε βάση τη µέση τιµή των σηµάτων 
δ για κάθε δεδοµένο εκπαίδευσης [6]. Η αλλαγή αυτή στον 
αλγόριθµο είναι ισοδύναµη µε την τροποποίηση της 
συνάρτησης σφάλµατος (5) ως εξής: 
∑∑∑
==
==
N
k i
i
N
k
pb ))k((e)k(EE
1
2
1
2
1
 (17) 
 Ο αλγόριθµος αυτός αν και βελτιωµένος ως προς την 
σύγκλιση, έχει πολύ µεγαλύτερες απαιτήσεις σε µνήµη. 
Πολύ σηµαντική για την εκπαίδευση του δικτύου είναι η 
σωστή αρχικοποίησή του, επειδή οι αρχικές τιµές των 
βαρών διαδραµατίζουν πολύ σηµαντικό ρόλο στην πορεία 
της εκπαίδευσης. Συνήθως αρχικοποιούνται τα βάρη σε 
µικρές αρχικές τιµές οµοιόµορφα κατανεµηµένες σε ένα 
διάστηµα. Η µέθοδος που χρησιµοποιήθηκε για την 
αρχικοποίηση των βαρών είναι η µέθοδος Nguyen-Widrow 
[8].  Τα κυριότερα πλεονεκτήµατα της µεθόδου αυτής είναι 
ότι δεν αχρηστεύονται νευρώνες µιας και τα βάρη 
αρχικοποιούνται στον ενεργό χώρο εισόδου των σιγµοειδών 
συναρτήσεων, µε αποτέλεσµα τα γινόµενα των εισόδων µε 
τα βάρη να βρίσκονται περίπου στο ίδιο εύρος τιµών και η 
εκπαίδευση να προχωράει πιο γρήγορα. Με τον τρόπο αυτό 
αποφεύγεται η ενεργοποίηση των συναρτήσεων στην 
περιοχή κορεσµού. Όλα τα βάρη αρχικοποιούνται µε βάση 
τον τύπο: 
).,.(U
R
m.w p 5050
1
70
1
−⋅=   (18)  
όπου m o αριθµός των νευρώνων στο, R το εύρος των τιµών 
των εισόδων και p ο αριθµός των βαρών σε κάθε νευρώνα 
(χωρίς τα κατώφλια) και U(-0.5,0.5) είναι η οµοιόµορφη 
κατανοµή στο διάστηµα [-0.5,0.5]. Σηµειώνεται ότι αρχικά 
το δίκτυο θεωρείται πλήρως διασυνδεδεµένο, οπότε όλοι οι 
νευρώνες κάθε επιπέδου έχουν τον ίδιο αριθµό βαρών. Στη 
συνέχεια για τα κατώφλια ας θεωρήσουµε m τιµές βι 
ισοκατανεµηµένες στο διάστηµα [0 0.7m1/p], οπότε: 
iii )w(signb β⋅= 1  (19) 
όπου w1i το βάρος που συνδέει την 1
η είσοδό του µε το 
νευρώνα i. 
 
 
C. Ανάστροφη ∆ιάδοση Levenberg-Marquardt  
 
Στην προηγούµενη ενότητα παρουσιάσθηκε ο τρόπος 
ενηµέρωσης των παραµέτρων ενός νευρωνικού δικτύου µε 
σκοπό την «προσέγγιση» σε µία επιθυµητή λειτουργία ένα 
σύνολο εκπαίδευσης. Ο τρόπος ενηµέρωσης των 
παραµέτρων βασιζόταν στον απλό κανόνα του Gradient 
Descent. Εφαρµόζοντας διαφορετικούς κανόνες κίνησης 
στην επιφάνεια σφάλµατος, από το χώρο της αριθµητικής 
ανάλυσης και βελτιστοποίησης κυρίως, προκύπτουν 
παραλλαγές του Back-propagation.  
Ο αλγόριθµος Levenberg-Marquardt [9] γενικότερα 
ελαχιστοποιεί συναρτήσεις τετραγωνικής µορφής, όπως 
είναι και η συνάρτηση σφάλµατος (5) που παρουσιάσθηκε 
στην προηγούµενη ενότητα.  
Η ενηµέρωση των βαρών γίνεται µε τον παρακάτω τύπο:  
eJ]IJJ[WW TT
1−+−= µ  (20) 
Με W συµβολίζεται ο πίνακας-στήλη που περιέχει όλα τα 
βάρη και κατώφλια του δικτύου µε κάποια διάταξη και έχει 
διάσταση Sx1, όπου S είναι ο αριθµός των παραµέτρων του 
δικτύου. Με J συµβολίζεται ο πίνακας που περιλαµβάνει 
τις πρώτες παραγώγους των όρων ie (που εκφράζονται από 
την εξίσωση (6)) ως προς τα βάρη και κατώφλια του 
δικτύου. Σηµειώνεται ότι οι παράγωγοι αυτές (
j
i
dw
de
) 
υπολογίζονται µε back propagation, όπως περιγράφτηκε 
στην προηγούµενη ενότητα. Ο πίνακας J  ονοµάζεται 
Ιακωβιανός και έχει διάσταση QxS, όπου S ορίζεται µε βάση 
τον αριθµό εξόδων. Επίσης I  είναι ο µοναδιαίος πίνακας 
διάστασης S και τέλος το διάνυσµα e

 αποτελείται από τους 
S όρους ie όπως ορίσθηκαν στην σχέση (6). Το µέγεθος S 
είναι ίσο µε το πλήθος των εξόδων του δικτύου αν 
χρησιµοποιείται εκπαίδευση ανά παράδειγµα, ενώ αν 
χρησιµοποιείται η συνολική εκπαίδευση (batch training) 
είναι ίσο µε το πλήθος των εξόδων επί τον αριθµό των 
παραδειγµάτων Ν (καθώς θα περιλαµβάνει Ν φορές το 
σφάλµα ie κάθε εξόδου, σε κυκλική διάταξη). Τέλος η 
παράµετρος µ  καθορίζει την εξέλιξη του αλγορίθµου. Ο 
αλγόριθµος Levenberg-Marquardt αποτελεί ένα συνδυασµό 
της µεθόδου Gauss - Newton (για την επίλυση προβληµάτων 
βελτιστοποίησης) [6][10] και της µεθόδου Gradient Descent. 
Όταν η παράµετρος µ αυξάνει τότε η µέθοδος προσεγγίζει 
την µέθοδο της βαθµωτής κατάβασης (gradient descent), η 
οποία όµως είναι λιγότερο αποτελεσµατική κοντά σε 
περιοχές της συνάρτησης σφάλµατος που έχουν τοπικά 
ελάχιστα. Αντίθετα η µέθοδος Newton είναι πιο γρήγορη 
στις περιοχές αυτές. Για το λόγο αυτό η παράµετρος µ 
αυξάνεται όταν χειροτερεύει η απόδοση του δικτύου, ενώ 
αντίστοιχα µειώνεται όταν η απόδοση καλυτερεύει ώστε να 
αναζητηθεί  γρηγορότερα το ελάχιστο. 
Πρέπει να σηµειωθεί ότι ο αλγόριθµος εκπαίδευσης δεν 
εξασφαλίζει ότι το σύστηµα θα σταθεροποιηθεί στην 
κατάσταση που ελαχιστοποιεί απόλυτα την συνάρτηση 
σφάλµατος. Πιο συχνά το δίκτυο που εκπαιδεύεται µε τον 
αλγόριθµο της ανάστροφης διάδοσης εγκλωβίζεται σε 
κάποιο τοπικό ελάχιστο. 
 
 
D. Προστασία γενίκευσης 
 
Η σηµαντικότερη ιδιότητα των νευρωνικών δικτύων (MLP) 
είναι ότι µπορούν να παράγουν αποτελέσµατα «λογικά» και 
για δεδοµένα για τα οποία  δεν έχουν εκπαιδευτεί αλλά είναι 
σχετικά µε αυτά που έχουν εκπαιδευτεί. Με βάση δηλαδή το 
σύνολο εκπαίδευσης που έχουν «µάθει», καταλήγουν σε νέα 
συµπεράσµατα. Η ιδιότητά τους αυτή είναι και η πλέον 
σηµαντική και ονοµάζεται γενίκευση. 
Αν όµως ένα δεδοµένο δίκτυο είναι αρκετά πιο περίπλοκο 
από ότι απαιτείται για το δεδοµένο πρόβληµα, τότε είναι 
πολύ πιθανό να παρατηρηθεί το φαινόµενο της 
υπερεκπαίδευσης (overtraining)[6] του δικτύου, το οποίο και 
µειώνει δραστικά την ικανότητά γενίκευσης. Το δίκτυο 
µαθαίνει πάρα πολύ καλά µόνο τα δεδοµένα εκπαίδευσης 
(overfitting), δίνοντας πολύ χαµηλές τιµές στη συνάρτηση 
σφάλµατος (5),(17), αλλά δεν αποδίδει καθόλου καλά σε 
δεδοµένα που δεν έχει εκπαιδευτεί. 
Για να αποφευχθεί το αρνητικό αυτό φαινόµενο 
ακολουθούνται δύο κυρίως τεχνικές: 
1. Έγκαιρο σταµάτηµα της εκπαίδευσης (early 
stopping). Για το λόγο αυτό, ένα µέρος των 
διαθέσιµων δεδοµένων, το οποίο ονοµάζεται 
σύνολο επαλήθευσης (validation set), δεν 
παρουσιάζεται στο δίκτυο κατά τη φάση της 
εκπαίδευσης, αλλά χρησιµοποιείται για να 
εκτιµηθεί η απόδοση του δικτύου σε άγνωστα 
δεδοµένα µετά το τέλος κάθε εποχής. Όταν αρχίσει 
να µειώνεται η απόδοση του δικτύου στο σύνολο 
επαλήθευσης, τότε η εκπαίδευση τερµατίζεται. 
2. Απλοποίηση του µοντέλου. Η µείωση του αριθµού 
των νευρώνων στο κρυµµένο επίπεδο αποτελεί την 
πλέον ενδεδειγµένη λύση αφού ο αριθµός των 
νευρώνων στην είσοδο και την έξοδο εξαρτάται 
συνήθως από τις παραµέτρους του προβλήµατος. 
Πιο αναλυτικά, το πλήθος των εισόδων εξαρτάται 
άµεσα από τα διαθέσιµα χαρακτηριστικά ενώ το 
πλήθος των εξόδων από τις δυνατές αποφάσεις που 
χρειάζεται να λαµβάνει το σύστηµα. Η απλοποίηση 
του µοντέλου µπορεί να λάβει χώρα επιβάλλοντας 
στην εκπαίδευση όχι µόνο να ελαχιστοποιεί το 
σφάλµα αλλά και να µειώνει την τιµή των βαρών 
που χρησιµοποιούνται. Αυτό γίνεται εύκολα αν 
αλλάξουµε το κριτήριο σφάλµατος (17) ως εξής: 
 wbb EEaÊ ⋅+⋅= β  (21) 
Όπου bÊ η νέα συνάρτηση εκτίµησης σφάλµατος 
και ο όρος wE  εξαρτάται από τα βάρη και 
συνήθως είναι: 
∑=
i
iw )w(E
2
2
1
 (22) 
Όπως εύκολα µπορούµε να παρατηρήσουµε, η bÊ  
«τιµωρεί» δίκτυα µε πολλά βάρη και µε µεγάλες τιµές των 
βαρών. Χρησιµοποιώντας τη συνάρτηση σφάλµατος (21) 
αντί της (17) ή της (5), η σχέση (20) στον αλγόριθµο 
Levenberg-Marquardt τροποποιείται ως εξής: 
)WeJ(]I)(JaJ[WW TT ββµ +++−= −
1
(23) 
Όπου ο πίνακας J  είναι ίδιος µε την προηγούµενη 
περίπτωση, όπου δεν χρησιµοποιείται η νέα bÊ  (21).   Η 
προσθήκη των νέων όρων οφείλεται στο γεγονός ότι κατά 
τον υπολογισµό των παραγώγων της συνάρτησης 
σφάλµατος ως προς τα βάρη προστίθεται ένας όρος λόγω 
του δεύτερου όρου της (21). Για παράδειγµα: 
i
i
b
i
b w
dw
dE
a
dw
Êd
⋅+⋅= β  (24) 
Με χρήση Bayesian συλλογιστικής [11] οι παράµετροι 
a και β εκτιµώνται και ανανεώνονται µε το πέρας κάθε 
εποχής µε βάση τον τύπο: 
[ ]
b
T
'
E
])IJaJ[(trLS
:a
2
1−+⋅−−
=
ββ
 (25) 
w
T
'
E
])IJaJ[(trL
:
2
1−+⋅−
=
ββ
β  (26) 
 
Όπου 
'a και 'β είναι οι νέες τιµές των a και β , S είναι ο 
συνολικός αριθµός εξόδων του δικτύου για όλα τα δεδοµένα 
εκπαίδευσης, δηλαδή είναι ίσος µε τις  εξόδους του δικτύου 
επί τον αριθµό των δεδοµένων εκπαίδευσης και L είναι ο 
συνολικός αριθµός παραµέτρων (βάρη και κατώφλια) του 
δικτύου. Η ποσότητα:  
])IJaJ[(trLF T 1−+⋅−= ββ  (27) 
εκφράζει τον αριθµό των χρήσιµων µεταβλητών του δικτύου 
ή των βαθµών ελευθερίας ή αλλιώς τον αριθµό των ενεργών 
παραµέτρων. Είναι χαρακτηριστικό ότι αν το δίκτυο είναι το 
ελάχιστο για τα δεδοµένα εκπαίδευσης τότε 0=⇒= βLF  
και η ελαχιστοποίηση λαµβάνει χώρα µόνο µε βάση το 
σφάλµα στα δεδοµένα. 
 
III. ∆Ε∆ΟΜΕΝΑ 
 
Η µέθοδος που περιγράφηκε στην προηγούµενη ενότητα 
χρησιµοποιήθηκε για την κατηγοριοποίηση µιας συλλογής 
κειµένων µε βάσητο ύφος του συγγραφέα. Για τα πειράµατα 
αυτά, έχει επιλεγεί ένα σύνολο από 1005 κείµενα που 
αντιστοιχούν σε οµιλίες που έλαβαν χώρα στο ελληνικό 
κοινοβούλιο. Τα  1005 κείµενα αποτελούν το σύνολο των 
οµιλιών κατά την περίοδο Ιανουάριος 1996 – Μάρτιος 2000 
από πέντε συγκεκριµένα µέλη της βουλής, καθένα από τα 
οποία εκπροσωπεί διαφορετικό πολιτικό χώρο. Τα κείµενα 
προέρχονται από τα πρακτικά της βουλής και έχουν υποστεί 
προεπεξεργασία προκειµένου να αφαιρεθούν σύντοµοι 
διάλογοι και ολιγόλογες παρεµβάσεις. Επιπλέον στα 
πρακτικά έχουν διορθωθεί τυχόν ορθογραφικά λάθη. Πιο 
αναλυτικά, το σώµα των κειµένων έχει την ακόλουθη δοµή: 
 
Οµιλητές 1997 1998 1999 2000 Οµιλίες 
Οµιλητής 
Α 161 107 125 25 418 
Οµιλητής 
Β 36 28 18 3 85 
Οµιλητής 
Γ 81 71 67 26 245 
Οµιλητής 
∆ 72 49 30 3 154 
Οµιλητής 
Ε 16 42 38 7 103 
Σύνολο 366 297 278 64 1005 
 
Πίνακας. 1. Οµιλίες που χρησιµοποιήθηκαν στα 
πειράµατα. 
 
Στη συνέχεια τα κείµενα τα οποία υπόκεινται σε 
επεξεργασία από τον Tagger – Lemmatizer του Ινστιτούτου 
Επεξεργασίας του Λόγου (ΙΕΛ - ILSP), ο οποίος έχει πολύ 
υψηλή ακρίβεια [12]. Η επεξεργασία  αυτή έγκειται στο 
γραµµατικό χαρκατηρισµό των λέξεων και στην εύρεση του 
αντίστοιχου λήµµατος για κάθε λέξη. Η αντιστοίχηση κάθε 
λέξης µε ένα λήµµα είναι πολύ σηµαντική διαδικασία 
εξαιτίας της µεγάλη πολυµορφίας που παρουσιάζει η 
Ελληνική γλώσσα. Η διαδικασία αυτή βοηθά, στη συνέχεια 
στην πραγµατοποίηση µετρήσεων για γλωσσικών 
χαρακτηριστικών που είναι αντιπροσωπευτικά του ύφους. 
Η επιλογή των χαρακτηριστικών που θα µετρηθούν πρέπει  
να αντικατοπτρίζει το ιδιαίτερο ύφος µε το οποίο 
εκφράζεται ο οµιλητής ενός κειµένου. Είναι επιθυµητό τα 
χαρακτηριστικά που θα επιλεγούν να είναι αποτελεσµατικά 
για τον διαχωρισµό των κειµένων µε βάση τον συγγραφέα. 
Τα χαρακτηριστικά που µετριούνται σε κάθε κείµενο έχουν 
οµαδοποιηθεί στις εξής πέντε κατηγορίες:  
• Συχνότητες ληµµάτων. Στην κατηγορία αυτή 
ανήκουν χαρακτηριστικά που αναφέρονται στη 
συχνότητα εµφανίσεων όλων των ληµµάτων που 
έχουν χρησιµοποιηθεί σε κάθε κείµενο. Ο αριθµός 
των εµφανίσεων είναι κανονικοποιηµένος ως προς 
τον συνολικό αριθµό των λέξεων του κειµένου. Η 
κανονικοποίηση συµβάλλει στο να γίνουν άµεσα 
συγκρίσιµες οι µετρήσεις µεταξύ κειµένων 
διαφορετικού µεγέθους. Από την κατηγορία αυτή 
επιλέχθηκε αλγοριθµικά η µέτρηση της συχνότητας 
εµφάνισης 17 συγκεκριµένων ληµµάτων. 
• Γραµµατικά Χαρακτηριστικά. Στην κατηγορία 
αυτή υπάγονται χαρακτηριστικά που µετρούν τον 
αριθµό των εµφανίσεων µερών του λόγου καθώς 
και τη µορφή µε την οποία εµφανίζονται στο 
κείµενο. Για παράδειγµα, µετριέται τόσο το σύνολο 
των άρθρων στο κείµενο όσο και, λόγου χάρη, ο 
αριθµός των άρθρων, ο αριθµός των ουσιαστικών 
σε γενική κτλ. Οι µετρήσεις έχουν και εδώ 
κανονικοποιηθεί ως προς τον αριθµό των λέξεων 
του κειµένου. Τέλος µετρήθηκε η συχνότητα 
εµφάνισης λέξεων που είχαν αντιστοιχηθεί σε 19 
γραµµατικούς χαρακτηρισµούς.  
• ∆οµικά Χαρακτηριστικά. Στην κατηγορία αυτή 
υπάγονται διάφορα δοµικά χαρακτηριστικά του 
λόγου, όπως για παράδειγµα ο αριθµός των λέξεων 
µε συγκεκριµένο µήκος (από 1 έως 30 γράµµατα) 
κανονικοποιηµένος ως προς τον συνολικό αριθµό 
λέξεων του κειµένου, ο αριθµός των προτάσεων µε 
συγκεκριµένο µήκος (από 1 έως 160 λέξεις) 
κανονικοποιηµένος ως προς τον συνολικό αριθµό 
των προτάσεων του κειµένου και τέλος ο αριθµός 
ορισµένων σηµείων στίξης (τελείες, ερωτηµατικά, 
αποσιωπητικά, κόµµατα, παύλες, παρενθέσεις), ο 
αριθµός των χρονολογιών και των συντοµογραφιών 
και όλα αυτά πάλι κανονικοποιηµένα ως προς τον 
αριθµό λέξεων. Από αυτήν την κατηγορία 
επιλέχθηκαν 27 χαρακτηριστικά. 
• Καταλήξεις δηµοτικής – καθαρεύουσας (Endings 
Logia/Demotiki Frequency). Επειδή είναι δυνατόν 
από ορισµένους οµιλητές να προτιµάται η χρήση 
ορισµένων καταλήξεων της καθαρεύουσας σε 
σχέση µε αντίστοιχες της δηµοτικής ή και το 
αντίστροφο, είναι ενδιαφέρον να εξετασθεί πόσο 
συχνά εµφανίζονται ορισµένες καταλήξεις 
ρηµάτων [13]. Για το σκοπό αυτό µετράµε την 
εµφάνιση συγκεκριµένων καταλήξεων ρηµάτων 
κανονικοποιηµένων ως προς το συνολικό αριθµό 
των λέξεων του κειµένου. Από την κατηγορία αυτή 
µετρήθηκε η συνολική συχνότητα εµφάνισης ανά 
αριθµό και πρόσωπο για όλα τα ρήµατα που η 
κατάληξή τους ήταν χαρακτηριστική της δηµοτικής 
ή καθαρεύουσας, καθώς επίσης και συνολικές 
µετρήσεις για τις καταλήξεις δηµοτικής και 
καθαρεύουσας. Από την κατηγορία αυτή προήλθαν 
14 µετρήσεις (2 αριθµοί, 3 πρόσωπα και 2 
συνολικές µετρήσεις δηµοτικής-καθαρεύουσας). 
• Χρήση αρνητικών λέξεων. Στην κατηγορία αυτή 
µετρήθηκε η συχνότητα εµφάνισης 8 λέξεων που 
δηλώνουν άρνηση. 
Ο συνολικός αριθµός επιλεγµένων χαρακτηριστικών είναι 
85. Πιο αναλυτικές επεξηγήσεις για τα χαρακτηριστικά 
βρίσκονται στο [5].  
 
IV. ΑΠΟΤΕΛΕΣΜΑΤΑ 
 
Το δίκτυο MLP που χρησιµοποιήθηκε για τη λύση του 
προβλήµατός της αναγνώρισης οµιλητών εφαρµόζεται 
έχοντας ως είσοδο το διάνυσµα των 85 µεταβλητών 
χαρακτηριστικών για κάθε κείµενο. Οι µεταβλητές αυτές 
κανονικοποιούνται ως προς το εύρος τους, ώστε οι τιµές 
τους να είναι µέσα στο διάστηµα [0 1]. Η κανονικοποίηση 
στο ίδιο εύρος για όλες τις εισόδους είναι πολύ σηµαντική 
για τη σωστή εκπαίδευση και λειτουργία του δικτύου. Αν 
και η κανονικοποίηση δεν είναι απαραίτητη για τη 
λειτουργία του αλγόριθµου εκπαίδευσης, ωστόσο 
εξασφαλίζει ότι όλες οι µεταβλητές θα επηρεάζουν εξίσου 
την απόφαση του δικτύου και η πληροφορία που παρέχουν 
τουλάχιστον αρχικά θα λαµβάνεται εξίσου από το δίκτυο, 
αφού δεν θα υπάρχει κάποιο χαρακτηριστικό που να δίνει 
πολύ µεγαλύτερο ερέθισµα από τα υπόλοιπα. Το δίκτυο 
αποτελείται από 5 εξόδους, µία για κάθε οµιλητή των 
οποίων οι τιµές κυµαίνονται στο διάστηµα [-1 1] λόγω της 
επιλογής της υπερβολικής εφαπτοµένης ως συνάρτηση 
ενεργοποίησης. Είναι γνωστό ότι για τη µοντελοποίηση 5 
διαφορετικών καταστάσεων επαρκούσαν µόλις 
  352 =log εξόδοι. Κάτι τα τέτοιο όµως εισάγει επιπλέον 
«θόρυβο» στη διαδικασία και αποφεύγεται µιας και η 
απόσταση (Ευκλείδεια ή Hamming) από τις 5 τάξεις δεν θα 
ήταν ίδια για όλες. Κάθε διάνυσµα εισόδου είναι επιθυµητό 
να ενεργοποιεί µία µόνο από τις 5 εξόδους (δηλαδή µία 
έξοδος να γίνεται 1 και όλες οι άλλες -1), που ισοδυναµει µε 
την απόδοση του λόγου αποδίδεται στον αντίστοιχο 
οµιλητή. Σε περίπτωση που περισσότερες από µία εξόδους 
είναι ενεργοποιηµένες (έχουν δηλαδή τιµή µεγαλύτερη από 
το -1), εφαρµόζεται ένα κριτήριο για να ληφθεί απόφαση. 
Συγκεκριµένα, επιλέγεται η µέγιστη έξοδος από τις 5 και το 
κείµενο αποδίδεται στον συγκεκριµένο οµιλητή. Με το 
κριτήριο αυτό υπάρχει πάντα µία έξοδος µε τη µέγιστη τιµή. 
Αυτό συνεπάγεται ότι το σύστηµα βγάζει ακόµα και σε 
οριακές περιπτώσεις απόφαση, κάτι που σε ορισµένες 
περιπτώσεις ενδέχεται να µην είναι επιθυµητό. 
 
 
 
Σχ. 3. Συνολικό διάγραµµα εκτέλεσης των πειραµάτων. 
 
Στη συνέχεια το σύνολο των κειµένων, µε κυκλικό τρόπο 
(modulo-10) χωρίσθηκε σε 10 περίπου ισοµεγέθη σύνολα. 
Από τα δέκα σύνολα αυτά χρησιµοποιήθηκαν τα 8 για 
εκπαίδευση, το ένα για επαλήθευση (validation set) και το 
ένα για να εκτιµήσουµε την απόδοση του συστήµατος (test 
set). Το σύνολο επαλήθευσης χρησιµοποιείται για να 
προστατεύσει τη γενίκευση του δικτύου, όπως αναφέρθηκε 
στην ενότητα ΙΙ-D. Με αυτό τον τρόπο έγιναν 90 δοκιµές, 
δηλαδή όλοι οι δυνατοί συνδυασµοί µε διαφορετικά σύνολα 
δοκιµής-επαλήθευσης. Επιπλέον δοκιµάσθηκαν διάφορα 
µεγέθη δικτύου µε 5 έως 8 νευρώνες στο κρυµµένο επίπεδο. 
Στο παρακάτω σχήµα παρουσιάζεται η µέση τιµή της 
ακρίβειας κατηγοριοποίησης του συνόλου δοκιµής για το 
σύνολο των δοκιµών (90), ενώ η διασπορά της απόδοσης 
αναπαρίσταται µε το εύρος των κάθετων γραµµών : 
 
Ακρίβεια Κατηγοριοποίησης
88,2%
88,3%
88,4%
88,5%
88,6%
88,7%
88,8%
88,9%
89,0%
89,1%
5 6 7 8
Αριθµός Νευρώνων Στο Κρυµµένο Επίπεδο
 
 
Σχ. 4. Ακρίβεια κατηγοριοποίησης για MLP µε 
διαφορετικούς νευρώνες στο κρυµµένο επίπεδο. 
 
Ο συνολικός αριθµός των παραµέτρων κάθε δικτύου σε 
συνάρτηση µε τον αριθµό των νευρώνων στο κρυµµένο 
επίπεδο δίνεται από τον τύπο:  
59151185 +⋅=⋅++⋅+= l)l(l)()l(P  (28) 
Όπου l ο αριθµός των νευρώνων στο κρυµµένο επίπεδο. 
Είναι προφανές από τον τύπο (28) ότι αυξάνοντας το 
µέγεθος του κρυµµένου επιπέδου κατά 1 προκύπτει δίκτυο 
µε 91 επιπρόσθετα βάρη. Για το λόγο αυτό προτιµητέο 
κρίνεται το δίκτυο µε 6 κρυµµένους νευρώνες µιας και είναι 
µόλις 0,3% χειρότερο (88,7%) από αυτό µε 8 (89%) ενώ 
είναι σαφώς πιο απλό (182 λιγότερα βάρη). Επιπλέον, το 
δίκτυο µε 6 κρυµµένους νευρώνες παρουσιάζει τη 
µικρότερη διασπορά (0,06%). Στο σχήµα 5 παρουσιάζεται ο 
αριθµός των ενεργών παραµέτρων δικτύου όπως 
υπολογίζεται από τη σχέση (27), ενώ σηµειώνεται και ο 
πραγµατικός αριθµός των παραµέτρων.  
 
300
350
400
450
500
550
600
650
700
750
5 6 7 8
Αριθµός Νευρώνων στο Κρυµµένο Επίπεδο
Αριθµός Παραµέτρων
Αριθµός Ενεργών Παραµέτρων
 
 
Σχ. 5. Αριθµός ενεργών και πραγµατικών παραµέτρων για 
δίκτυα µε διαφορετικούς νευρώνες στο κρυµµένο επίπεδο. 
 
Όπως παρατηρείται για δίκτυα µε περισσότερους από 6 
νευρώνες, παρά τη γραµµική αύξηση των πραγµατικών 
παραµέτρων, ο αριθµός των ενεργών παραµέτρων αυξάνεται  
ελάχιστα. 
Λαµβάνοντας υπόψη ότι η συνάρτηση σφάλµατος που 
χρησιµοποιήθηκε ωθεί το δίκτυο να µειώνει τις τιµές των 
βαρών και ότι οι τιµές των χαρακτηριστικών είναι 
κανονικοποιηµένες, µπορεί να εκτιµηθεί η συµβολή του 
κάθε χαρακτηριστικού στη λήψη απόφασης και κατ’ 
επέκταση στην ακρίβεια κατηγοριοποίησης. Πιο αναλυτικά, 
υπολογίζοντας για όλους τους συνδυασµούς συνόλων το 
άθροισµα των παραµέτρων µε τις οποίες κάθε 
χαρακτηριστικό συνδέεται µε τους νευρώνες του κρυµµένου 
επιπέδου µπορεί να γίνει µία ιεράρχηση της σηµασίας των 
χαρακτηριστικών, όπως φαίνεται στη σχέση (29): 








= ∑
=
l
k
ikseti wEfT
1
)(  (29) 
Όπου )f(T i  είναι µία τιµή αντιπροσωπευτική της αξίας 
του if χαρακτηριστικού, {}Eset είναι η µέση τιµή για όλα 
τα σύνολα που γίνεται η εκτίµηση της απόδοσης και l ο 
αριθµός των νευρώνων στο κρυµµένο επίπεδο. Με βάση τις 
τιµές )f(T i  υπολογίζεται η αξία κάθε µίας από τις πέντε 
οµάδες χαρακτηριστικών, όπως παρουσιάζεται στο σχήµα 6. 
Οι τιµές που εµφανίζονται στον άξονα Y είναι 
κανονικοποιηµένες, ώστε το άθροισµα )f(T i  όλων των 
χαρακτηριστικών να αθροίζει στη µονάδα και επιπλέον είναι 
διαιρεµένο µε το πλήθος των χαρακτηριστικών σε κάθε 
οµάδα. Με αυτόν τον τρόπο, η συµβολή της κάθε οµάδας 
µετριέται ανάλογα µε το πλήθος των χαρακτηριστικών που 
ανήκουν σε αυτήν.  
 
0,000
0,002
0,004
0,006
0,008
0,010
0,012
0,014
0,016
0,018
0,020
5 6 7 8
Αριθµός Νευρώνων στο Κρυµµένο Επίπεδο
Λήµµατα
Αρνητικές Λέξεις
Μέρη του Λόγου
∆οµικά
∆ηµοτική/Καθαρεύσα
 
 
Σχ. 6. Συµβολή κάθε οµάδας χαρακτηριστικών στην 
κατηγοριοποίηση για δίκτυα µε διαφορετικούς νευρώνες στο 
κρυµµένο επίπεδο. 
 
Από το σχήµα 6 φαίνεται ότι η πιο σηµαντική οµάδα 
χαρακτηριστικών είναι αυτή µε τα µέρη του λόγου, 
ακολουθούµενη από αυτές των ληµµάτων και των δοµικών 
χαρακτηριστικών. Η οµάδα των αρνητικών λέξεων είχε τη 
µικρότερη συµβολή στο τελικό αποτέλεσµα. Τέλος είναι 
πολύ σηµαντικό να σηµειωθεί ότι η συµβολή των 
χαρακτηριστικών είναι πρακτικά αµετάβλητη για όλα τα 
µεγέθη νευρωνικών δικτύων. 
V. ΣΥΜΠΕΡΑΣΜΑΤΑ 
 
Στην παρούσα εργασία παρουσιάσθηκε ένα σύστηµα 
αναγνώρισης οµιλητή µε βάση τη µέτρηση γλωσσικών 
χαρακτηριστικών. Τα χαρακτηριστικά που επιλέχθηκαν 
µπορούν να µετρηθούν µε αυτόµατο τρόπο και να 
αποκαλύψουν τον ιδιαίτερο τρόπο µε τον οποίο ένας 
οµιλητής χρησιµοποιεί το λόγο. Ιδιαίτερη σηµασία δόθηκε 
στον αλγόριθµο εκπαίδευσης του δικτύου και στην 
προστασία της γενίκευσής του, η οποία και αποτελεί τον 
ακρογωνιαίο λίθο του συστήµατος. Η απόδοση που 
µετρήθηκε σε άγνωστα δεδοµένα  είναι της τάξης του 88,8% 
για ένα νευρωνικό δίκτυο µε 6 κρυµµένους νευρώνες και 
κρίνεται ικανοποιητική, ενώ και το µέγεθος του 
χρησιµοποιούµενου δικτύου καθιστά την εφαρµογή της 
µεθόδου αυτής στην πράξη εφικτή. Τέλος 
πραγµατοποιήθηκε αξιολόγηση των χαρακτηριστικών που 
χρησιµοποιήθηκαν ανά οµάδα, ώστε να βρεθούν τα πιο 
αποτελεσµατικά και χρήσιµα για το πρόβληµα της 
περιγραφής ύφους. 
 
 
VI. ΑΝΑΦΟΡΕΣ 
 
[1] Yiming Yang. “An Evaluation of Statistical Approaches 
to Text Categorization”. In Journals of Information 
Retrieval, Vol. 1, No. 1-2, pp. 67 – 88, 1999. 
[2] Stamatatos E., Fakotakis N. and Kokkinakis G. 
“Automatic Text Categorization in Terms of Genre and 
Author”. Association for Computational Linguistics, 
Vol. 26, No. 4, pp. 471 – 494, 2001. 
[3] Tweedie F., Singh S. and Holmes D. “An Introduction 
to Neural Networks in Stylometry”. Research in 
Humanities Computing, Vol. 5, pp. 249 – 263, 1996. 
[4] Matthews R. and Merriam T. “Neural Computation in 
Stylometry I: An Application to the Works of 
Shakespeare and Fletcher”. Literary and Linguistic 
Computing, Vol. 8, No. 4, pp. 203 – 209, 1993. 
[5] Tambouratzis G., Markantonatou S., Hairetakis N. , 
Vassiliou M., Tambouratzis D. and Carayannis G. 
“Discriminating The Registers And Styles In The 
Modern Greek Language”. ACL 2000, Proceedings of 
the workshop on Comparing Corpora, pp. 34 – 43, 2000. 
[6] Haykin S. “NEURAL NETWORKS: A Comprehensive 
foundation” Second Edition, Prentice Hall, 1999. 
[7] Τζαφέστας Σ. “Υπολογιστική νοηµοσύνη Τόµος Α: 
ΜΕΘΟ∆ΟΛΟΓΙΕΣ”. ΕΜΠ, Αθήνα, 2002. 
[8] Nguyen D. and Widrow B. “Improving the learning 
speed of 2-layer neural networks by choosing initial 
values of the adaptive weights.” Proceedings of the 
International Joint Conference on Neural Networks, 
Vol. 3, pp. 21-26, 1990. 
[9] Hagan, M.T and Menhaj, M.B. “Training feedforward 
networks with the Marquardt algorithm”, IEEE 
Transactions on Neural Networks, Vol. 5, No. 6, pp 
989-993, 1994. 
[10] Foresee F. and Hagan, T. “Gauss Newton 
Approximation To Bayesian Learning” Proceedings of 
the 1997 International Joint Conference on Neural 
Networks, Vol. 3, pp. 1930-1935, 1997. 
[11] MacKay, D. ”A practical Bayesian framework for 
backpropagation networks.” Neural Computation, Vol. 
4, No. 3, pp. 448–472, 1992. 
[12] Papageorgiou H., Prokopidis P., Giouli V. and Piperidis 
S. “A Unified PoS Tagging Architecture and its 
Application to Greek”. Second International Conference 
on Language Resources and Evaluation Proceedings, 
Athens, Greece, Vol. 3, pp. 1455 – 1462, 2000.   
[13] Micros G. and Carayannis G. “Modern Greek Corpus 
Taxonomy”. Second International Conference on 
Language Resources and Evaluation Proceedings, 
Athens, Greece, Vol. 1, pp. 129 – 134, 2000.   
