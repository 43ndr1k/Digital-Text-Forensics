Email Author Attribution
Shrutiranjan Satapathy Sumit Bhagwani
Computer Science and Engineering Computer Science and Engineering
IIT Kanpur IIT Kanpur
Kanpur, Uttar Pradesh 208016 Kanpur, Uttar Pradesh 208016
sranjans@cse.iitk.ac.in sumitb@cse.iitk.ac.in
ABSTRACT
This work aims at exploring various linguis-
tic style markers used in distinguishing email
authors. We focus on features based only on
body of the emails. A selection criterion is im-
posed on the markers to select the most impor-
tant style markers as features. These features
are then used in classifying anonymous email
body texts into a predefined set of authors as
part of a Hard-Classification Supervised Learn-
ing scheme using Support Vector Machines.
1 INTRODUCTION
Email is used in many different situations as, for ex-
ample, in the exchange and broadcasting of messages,
documents and for conducting electronic commerce.
Unfortunately it can be misused for the distribution
of unsolicited and/or inappropriate messages and doc-
uments. Examples include unauthorized conveyancing
of sensitive information, mailing of offensive or threat-
ening material, etc. In such a scenario, identification
of the sender of these mails becomes very important.
Other application areas include identifying messages
known to be from major enemy players, intercepting
documents discussing terrorist attack plans, etc.
Authorship identification has been studied a lot as
part of Stylometry[4]. Linguistic stylometry attempts
to distinguish authors based on certain linguistic char-
acteristics unique to each author. Many of such charac-
teristics are easy to understand, such as, average word
length, average sentence length, number of punctua-
tions, net abbreviations, etc. We attempt to pool in
many such stylometric features, and perform analyses
to identify what linguistic traits are most helpful in
distinguishing authors. Finally, we will be performing
classification using supervised learning techniques such
as Support Vector Machines.
Section 2 discusses the previous work done in this
area. Section 3 introduces the dataset used and the
preprocessing performed. Section 4 describes the style
markers in detail. Section 5 explains the feature ex-
traction scheme. Section 6 describes the classification
strategy. Section 7 explains the experimental setup and
the results obtained. Finally, section 8 concludes the
project and offers suggestions for future work.
2 PRIOR WORK
Computational Stylistics has been an active area of
research. Most of the approaches try to come up with
new discriminative linguistic features which can distin-
guish authors. A lot of style markers have been studied
in the literature [3] [4] [5]
Olivier et al.[1] uses basic structural features of the
email document and proposes the use of Support Vec-
tor Machines for the multi-class classification.
Levinson et al.[6] proposed the usage of standard sta-
tistical measures instead of just average of attributes
namely, maximum, minimum, median and variance, for
all array based markers. For single value markers (such
as xLegomena, readability indices) etc. they only use
the raw estimate calculated. Their aim was to create a
taxonomy of markers for computational stylistics and
propose a method on evaluation of these markers.
Anderson et al.[5] introduced usage of lexical richness
measures like Yule’s K, Herdan’s C etc. They also use
structural features like number of attachments, pres-
ence of reply text, presence of greeting etc. Gender
Discriminating features were also proposed by them.
They normalized all attribute values to [0,1] so that all
attributes are treated equally in classification.
Argamon et al.[2] examined multiclass variants of the
Exponentiated Gradient (EG) algorithm for the clas-
1
sification using some standard stylistic markers. They
also focused on use of Ultraconservative algorithms in
this area.
Iqbal et al.[3] developed an e-mail analysis frame-
work to extract different writing styles from a collec-
tion of anonymous emails. They use clustering based
on stylometric features, for email forensics. Clustering
techniques experimented were Expectation Maximiza-
tion (EM), k-means, and bisecting k-means.
3 DATASET & PREPROCESSING
3.1 Dataset
We use the ENRON Email Dataset[11] for the project.
This dataset was collected and prepared by the CALO
Project (A Cognitive Assistant that Learns and Orga-
nizes). It contains data from about 150 users, mostly
senior management of Enron, organized into folders.
The corpus contains a total of about 0.5M messages.
This data was originally made public, and posted to
the web, by the Federal Energy Regulatory Commis-
sion during its investigation. 5 authors were chosen
at random from the dataset, and their sent mails col-
lected, to form 5 classes. The problem then reduces to
a 5-class classification problem.
3.2 Data Preprocessing
The emails of the authors were stripped of their
headers, salutations and signatures to retain only
the bodies. One-liner emails and forwarded mails
were filtered. The final dataset is summarized in the
following table:
4 STYLE MARKERS
There are a lot of linguistic style markers which have
been proposed in literature[2][3][4][5]. The major style
markers can be broadly classified into 4 categories
4.1 Character Based Features
As the title suggests, these features are based on the
characters of the email text. The features are listed
below :
• character count
• alphabet count to character count ratio
• uppercase count to character count ratio
• lowercase count to character count ratio
• digit count to character count ratio
• space count to character count ratio
• tab count to character count ratio
• presence of special characters : < > % | { } [ ] /
\ @ # ∼ + − ∗ $ˆ&
• frequency distribution of alphabets, normalized
with character count
• presence of ellipsis 1 : ”...”
• Smileys/emoticons count
4.2 Word Based Features
In this section, we’ll enumerate the word based feature
used:
• token count
• average sentence length in terms of characters
• average token length in terms of characters
• ratio of number of characters in words to character
count
• presence of abbrevations, from a predefined set of
abbrevations
• non-english token count
• count of lower case words
• count of Upper case words
• count of MiXeD case words
• count of ALLUPPER case words
• word length frequency distribution
• short words(length < 3) to token count ratio
1Wikipedia : Ellipsis
2
• Vocabulary Richness Measures : Vocabulary is de-
fined as stemmed set of tokens. Measures are
expressed in terms of simple transformations of
V(N), the vocabulary size, and N, the text length.
All these measures can be described as arising from
attempts to fit simple mathematical functions to
the curve of the vocabulary size V(N) as a func-
tion of N. Here V(i,N) represents count of words
with frequency i.
– Guiraud’s R : V (N)√
N
– Herdan’s C : log(V (N))log(N)
– Rubet’s K : log(V (N))log(log(N))
– Mass’s A : log(N)−log(V (N))log2(N)
– Dugast’s U : log
2(N)
log(N)−log(V (N))
– Brunet’s W : NV (N)
0.172
– Honore’s H : 100 log(N)
1−V (1,N)
V (N)
– Michea’s M : V (N)V (2,N)
– Hapax Legomena Norm : V (1,N)V (N)
– Hapax Dislegomena Norm : V (2,N)V (N)
– Lukjanenkov & Nesitoj’s constant :
1−V (N)2
V (N)2log(N)
– Yule’s K : 104[−1N +
∑
i V (i,N)(
i
N )
2]
• Automated Readability Index : Readability test. It
produces an approximate representation of the US
grade level needed to comprehend the text.2
Given by :
4.71
charactercount
tokencount
+ 0.5
tokencount
sentencecount
− 21.43
• Parts of Speech based Features
– nouns to words ratio
– verbs to words ratio
– adjectives to words ratio
– adjectives to words ratio
– others to words ratio
– proper nouns to nouns ratio
– plurals to nouns ratio
– past tense verbs to verbs ratio
• Syllable based Features
– Number of tokens with < 3 syllables
– Number of tokens with >= 3 syllables
2Wikipedia : Automated Readability Index
4.3 Syntactic Features
• Count of Punctuations , . ? ! : ; ’ ”
• Presence of Glue/Function Words(308) 3 : These
are structural words which have no meaning on
their own but are used in sentence construction.
They are shown in literature to be effective as style
markers.
• Gender Preferential Features : ratio of words
ending with ’able’, ’al’, ’ful’, ’ible’, ’ic’, ’ive’, ’less’,
’ly’ and ’ous’ with token count. Also ratio of words
with ’apolog’ as stemmed form to token count is
used. These are shown to be gender discriminative
features in literature[5].
4.4 Structural Features
• line count
• sentence count
• paragraph count
• usage of tab as paragraph separator
• usage of newline as paragraph separator
• average paragraph length in terms of characters
• average paragraph length in terms of words
• average paragraph length in terms of sentences
5 FEATURE EXTRACTION
The aforementioned style markers are all implemented
to obtain 503 features in total. Since the values taken
by attributes vary widely, we use Z-Score normaliza-
tion. Many of these features are proposed in a literary
domain and their performance in the email domain is
not known. In order to filter out the unimportant fea-
tures, all the attrubutes are ranked as per a ranking
scheme :
• Info-Gain Ranking Scheme : Each feature is given
an entropy score based on its scatter across all the
classes.
• Gain-Ratio Ranking Scheme : Since info-Gain
scores tend to favour multi-class splits, Gain-Ratio
score imposes a normalization on the Info-Gain
scores.
3contains a list of 308 function words
3
The features are sorted in the descending order of their
scores. For different values of k, the highest ranked
k attributes are chosen as features for classification
using multi-class SVMs, and the accuracy obtained
is plotted against k. The step size for k is 10. The
following graphs represent the performance of the
features :
The peaks of the above graphs give the best threshold
for the number of features to be chosen. Both the
ranking schemes agree upon a threshold of 330 features
and an accuracy of 75.42%.
An alternative is to use the threshold corresponding
to the sharpest decrease in rank-score, as described in
the following plots. But as can be observed from the
above graphs, the accuracy obtained is much lesser
in comparision to the peak accuracy, where the slope
change label corresponds the sharpest decrease in
rank-score of the attributes.
6 CLASSIFICATION SCHEME
The problem boils down to a hard-classification prob-
lem of 5 classes. SVM is used for multicategory classifi-
cation, using a one vs rest model. 80% of the emails of
each author are retained for training and the remain-
ing 20% of the emails are put together for testing to
perform stratified cross validation.
7 EXPERIMENTAL SETUP AND
RESULTS
7.1 Implementation of Style Markers
Many of the style markers needed extensive use of the
WordNet corpus[10] (for stemming) and a natural lan-
guage toolkit. Such markers include :
• Yule’s K Measure (measures the vocabulary com-
plexity of the email) and other vocabulary mea-
sures such as Dugast’s U, Herdan’s C, etc.
• Features based on part of speech
• Measuring syllable complexity of words in an email
4
Python NLTK[8] is used as a natural language toolkit.
The WordNet corpus of NLTK is used for stemming
purposes.
SVM is used for the 5-class classification, using a
one vs rest model. SVM-Light[9] is used as the
implementation of SVMs with default parameters
(linear kernel).
7.2 Results
7.2.1 Best Results
• All the style markers are ranked based on the Info-
Gain ranking scheme, and the best rank threshold
is chosen based on accuracy
• Ended up selecting the top 330 features from a
total of 503 features as they give the best accuracy
• Overall Accuracy : 75.42%
7.2.2 Comparision
• Results of different feature sets differ across differ-
ent datasets
• Keeping this in mind, and the fact that other
datasets weren’t publically available, Olivier et
al’s[1] feature set and algorithm was implemented
and run on our dataset for comparision and the
results are as follows:
• Overall Accuracy : 57.14%
8 CONCLUSIONS
This work aims at exploring various linguistic style
markers in literature. A significant portion of the
project was given to the exploration and implemen-
tation of the style markers. Then, an attempt is made
to find the feature set best suitable to the email do-
main by ranking the attributes based on an Info-Gain
scoring system. SVMs are used for multi-class clas-
sification, giving very encouraging results. Compari-
sion with other works in the same area requires their
datasets, which weren’t publically available. For com-
parision puposes, Olivier et al’s [1] feature set and al-
gorithm was implemented and run on our dataset. The
results in the above section show that our feature set
performs much better than Olivier et al’s feature set.
8.1 Future Work
Future work can include expanding the feature set to
include sophisticated style markers which break up sen-
tences into phrases and clauses and look at the phrase
depth or clause depth of each sentence. Also, the num-
ber of authors considered in our dataset is quite small.
The performance of the approach must be measured
on a larger number of authors for the feasibility of the
authorship attribution task in the real world. Finally,
in order to apply the task in an unsupervised setting,
where neither the suspects list nor training examples
are available to the investigator, an attempt must be
made to identify major groups of writing styles through
cluster analysis. [3] tries to address this issue, but the
area is still nascent.
5
References
[1] Olivier de Vel. 2000. Mining E-Mail Authorship.
KDD-2000 Workshop on Text Mining
[2] Argamom, Saric and Stein. 2003. Style Mining of
Electronic Messages for Multiple Authorship Dis-
crimination: First Results. SIGKDD’03.
[3] Iqbal Farkhund et al. 2010. Mining writeprints from
anonymous e-mails for forensic investigation. Digi-
tal Investigation (2010).
[4] Efstathios Stamatatos. 2009. A Survey of Mod-
ern Authorship Attribution Methods. Journal of the
American Society for Information Science and Tech-
nology.
[5] Olivier de Vel , Malcolm Corney , Alison Anderson ,
and George Mohay. 2002. Language and Gender Au-
thor Cohort Analysis of E-mail for Computer Foren-
sics.In Proc. digital forensic research workshop 2002
[6] Foaad Khosmood and Robert Levinson. 2010 Tax-
onomy and Evaluation of Markers for Computa-
tional Stylistics. WORLDCOMP’10.
[7] WEKA, http://www.cs.waikato.ac.nz/ml/weka/
[8] Natural Language Toolkit (NLTK),
http://www.nltk.org/
[9] SVM-Light Support Vector Machine,
http://svmlight.joachims.org/
[10] WordNet Affect, http://wndomains.fbk.eu/wnaffect.html
[11] Enron Dataset, http://www.cs.cmu.edu/ enron/
6
