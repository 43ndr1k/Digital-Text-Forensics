Neurocomputing 108 (2013) 93–102Contents lists available at SciVerse ScienceDirectNeurocomputing0925-23
http://d
n Corr
E-mjournal homepage: www.elsevier.com/locate/neucomSemi-random subspace method for writeprint identificationZhi Liu n, Zongkai Yang, Sanya Liu n, Yinghui Shi
National Engineering Research Center for E-Learning, 152 Luoyu Road, Wuhan, Hubei 430079, PR Chinaa r t i c l e i n f o
Article history:
Received 6 March 2012
Received in revised form
9 July 2012
Accepted 30 November 2012
propose a novel method, called semi-random subspace (Semi-RS), to simultaneously address the twoAvailable online 28 December 2012
Keywords:
Writeprint
Individual-author feature set (IAFS)
Random subspace method (RSM)
Principal component analysis (PCA)
Diversity12/$ - see front matter & 2012 Elsevier B.V. A
x.doi.org/10.1016/j.neucom.2012.11.015
esponding authors. Tel.: þ15827442670.
ail addresses: liuzhi8673@gmail.com (Z. Liu),a b s t r a c t
The anonymous nature of online messages distribution causes a series of moral and legal issues. By
analyzing identity cues people leave behind their texts, i.e., writeprint, potential authors can be
identified individually. But writeprint identification is a difficult learning task, because of the high
redundancy in stylistic feature set and high similarity of some authors’ writing-style. In this paper, we
problems. Different from the conventional random subspace method (RSM) which samples features
from the whole feature set in a completely random way, the proposed Semi-RS randomly samples
features on each individual-author feature set (IAFS) partitioned from the whole feature set. More
specifically, we first divide the whole feature set into several IAFSs in a deterministic way, then
construct a set of base classifiers on different randomly sampled feature sets from each IAFS, and finally
combine all base classifiers for the final decision. Experimental results on the benchmark dataset
demonstrate the effectiveness of the proposed method which improves previously reported results. In
addition, we analyze the diversity of algorithm, reveals that Semi-RS constructs more diverse base
classifiers than conventional RSMs.
& 2012 Elsevier B.V. All rights reserved.1. Introduction
The ongoing development of computer network technology
has promoted the diversity of communication ways in the online
community, many people prefer to communicate with others
through the Web-based channels such as e-mail, Internet news-
groups, Internet chat room, etc. But at the same time, cyber
criminals often take this opportunity to spread some illegal
information by some anonymous ways, such as delivering offen-
sive, threatening emails, and so on. Therefore, efficient automated
methods for authorship identification of texts are becoming
imperative. In recent years, some researchers have paid increas-
ing attention to authorship identification of electronic texts, such
as recognizing the authorship of emails [1–4], forum messages [5]
and personal blogs [6,7].
Writeprint [8] identification is the technique of predicting the
most likely authorship of anonymous texts by using stylistic
information hidden in texts; this study can be seen as a single-
label multi-class text categorization problem [9] where the candi-
date authors play the role of the classes in the high-dimensional and
sparse feature space. One major subtask of the writeprint identifica-
tion is to extract the most appropriate features for the writing-style,
the so-called Stylometry [10]. In many stylometric techniques,ll rights reserved.
lsy5918@gmail.com (S. Liu).character n-grams (i.e., character sequences of length n) have been
proved to be effective for representing the style of authors. Keselj
et al. [11] used the fixed-length n-grams in various test collections of
English, Greek and Chinese texts, achieved the good identifica-
tion results and improved previously reported accuracy. In [12],
Houvardas and Stamatatos conducted the authorship identification
experiment on the subset of Reuters corpus volume 1 (RCV1) by
using variable-length n-grams (n¼2, 3, 4), and achieved a higher
identification accuracy than using word-based features. Although
these stylistic features provide a good discriminative basis for
identity tracing, this approach considerably increases the dimen-
sionality of the problem, and the high-dimensional feature space
contains too much noise, which greatly degrades the generalization
ability of the identification system. On the other hand, most
previous research [4,5,7,8,10–12] used author-group feature set
(AGFS) where one set of features is applied across all authors. In
fact, many authors use the same cyber terms and writing patterns,
which make it difficult to discriminate those authors with similar
style in a single feature set. Moreover, many inner stylistic features
cannot be exploited using AGFS. Based on the reason mentioned
above, Abbasi and Chen [6] designed an ensemble method with
support vector machine (Ens–SVM) by selecting a feature set for
each author. However, the generalization ability of the ensemble
method is limited since the number of individual classifiers is fixed
by the number of authors. Therefore, how to make full use of the
individual discriminative information among different authors is the
critical issue in writeprint identification.
Z. Liu et al. / Neurocomputing 108 (2013) 93–10294In recent years, random subspace method (RSM) [13] has been
developed and widely studied in the field of pattern recognition
[14–16]; the ensemble method can effectively overcome the high
redundancy in high-dimensional feature space. In the previous
studies of writeprint identification, Stamatatos applied the exhaus-
tive disjoint subspace method (EDS) [17] to construct multiple linear
discriminative classifiers to identify the authorship of the anon-
ymous texts. In this method, a large feature set is divided into
equally-sized disjoint feature subsets drawn at random. Each
particular attribute is used exactly once by using the random
sampling without replacement. Each resulting feature subset is used
to train a base classifier (BC) using a learning algorithm able to
provide posterior possibilities. In this way, the diversity among the
predictions of different BCs is improved. However, the discrimina-
tive information will become more dispersed as the result of the
increasing size of feature space; and thus the stability of the BCs
constructed in the different random subspaces will be weakened. In
the conventional RSM, several low-dimensional subspaces are
generated by randomly sampling from the original high-
dimensional feature space. Then, multiple classifiers constructed in
these random subspaces are combined to make a final decision.
Although the method contributes to improve the diversity among
predictions of BCs, it cannot ensure the stability of the BCs. To
reduce the feature-to-instance ratio, some researchers applied
principal component analysis (PCA) to compress the original feature
space. Chen and Zhu [18] have proposed SpPCA (sub-pattern PCA)
which extracts local features seperately from each sub-pattern set
consisting of all the sub-patterns with the common positions or
attributes, this method has been proved to be effective in face
recognition [14]. Wang and Tang [19] do random sampling in
reduced principal component analysis (PCA) subspace instead and
random subspace is not completely random by fixing the first
several dimensions of each subspace as those largest eigenvectors.
This improved method is not a completely random selection
method; it can improve the stability (accuracy) of BCs in comparison
to the completely random subspace method. However, there are
several duplicate features in each subspace, it degrades the diversity
among the predictions of BCs, the final result is not necessarily
better than the mean accuracy of the single classifiers.
In this paper, we follow the language-independent stylometric
method [20] using variable-length n-grams as feature set. Mil-
lions of the most frequent n-grams are used to represent the style
of a text. On this basis, a semi-random subspace (Semi-RS)
method is proposed to overcome the high redundancy of feature
set and non-robustness for writeprint identification. By Semi-RS
here, we mean that random sampling in individual-author feature
set (IAFS) partitioned from the whole AGFS rather than random
global sampling in RSM is performed. Inspired by the similar
studies on face recognition [14,19], we first apply PCA to com-
press the original feature space, and divide the PCA subspace into
several IAFSs by computing divergence between different authors’
training sets. Then, a set of BCs are constructed on different
feature subsets which are randomly sampled from each IAFS, and
finally all BCs are combined for the final decision using a
combination rule. Generally, our method and experimental study
address the following research questions:
Q1: Can the proposed ensemble method obtain a better
performance than RSMs and the single classification models?
Q2: Compared with conventional RSMs, can the Semi-RS
construct more diverse BCs?
Q3: How scalable is the Semi-RS in terms of number of authors
and the number of texts per author?
The organization of the paper is as follows. Section 2 describes
the framework of writeprint identification based on the proposedmethod. Section 3 gives the theoretical analysis. Experimental
settings, results and discussions are reported in Section 4. Section
5 concludes this paper.2. Semi-random subspace method
In the current study, each text is represented as a vector of
character n-gram frequencies of occurrence. Let Gd ¼ g1,g2,:::,gd be
the ordered set (by decreasing frequency of occurrence) of the
most frequent n-grams (character sequences of variable-length
n¼1–5) in training set. Consider f ij as the normalized value of the
jth n-gram of Gd in the ith text. Then a text is represented as the
vector ti ¼ f i1,f i1,:::,f id
 
ATm, where Tm represents the whole
sample space.
The variable-length n-grams feature vector contains significant
complex writeprint information. Unfortunately, the feature set con-
tains only a small proportion of the discriminative features in a high-
dimensional space. Therefore, the conventional single classification
model fails to deliver good performance. In other words, it is difficult
to find a single optimal classifier that can be robust enough to achieve
the good identification performance in a high-dimensional space.
Generally the high-dimensional feature vector is projected to a low-
dimensional feature space using PCA to avoid high feature-to-
instance ratio. In order to construct multiple stable classifiers, the
eigenvectors with small eigenvalues are usually removed in PCA
subspace. But eigenvalue does not necessarily indicate the discrimin-
ability of a feature. Their removal may cause a loss of useful
discriminative information. To solve these problems, we first select
an IAFS for each author to represent individual writeprint, here we
assume that each author has his/her own unique writing stylistic
features, i.e., IAFS, which are optimal to distinguish a given author
from other authors. Then we randomly sample several small feature
subsets from each IAFS to construct multiple informative BCs. We
finally combine these BCs to construct a more powerful classifier that
cover most of discriminative features. As support vector machine
(SVM) [21] is considered as state-of-the-art in text categorization and
hence is used as the type of the underlying BC in the ensemble. Fig. 1
describes the whole framework of writeprint identification, in which
the Semi-RS algorithm is briefly illustrated as follows.
At the training stage,1. Normalize every component of the feature vector Gd to the
range [0, 1] as Tm ¼ f 1,f 2,:::,f d
 
.
2. Apply PCA to the normalized feature vectors and calculate
their eigenvectors uk and eigenvalues lk k¼ 1,2,:::,dð Þ. Respec-
tively, the eigenvectors with larger eigenvalues that preserve
98% of contribution rate are preserved as ut ¼ u1,u2,:::,ud0 . The
rest dd0 eigenvectors containing noise are removed.3. For each training text, project the normalized vectors f k to the
respective eigenvectors using vk ¼ uTk  f kmk
 
, where mk is
the mean of all values in the kth feature vector.
4. Calculate IAFSi
L
i ¼ 1 for each author by computing the best
features to discriminate the author ai i¼ 1,2,:::,Lð Þ from all
the other authors.5. Generate K random subspaces Si,j
K
j ¼ 1 from corresponding
IAFSi
L
i ¼ 1 for each author and construct a SVM classifier
Ci,j ¼ SVM Si,j
 
on each subspace.
6. Construct K SVM classifiers Ci,j
K
j ¼ 1 from the corresponding K
random subspaces.
At the identification stage,1. For each anonymous text m, normalize every component of
the feature vector to the range of [0, 1] and project them to
the respective eigenvectors ut .
Author 1
Feature
extractor
Variable-length
n-grams
n=1~5
Ut={U1,U2,...,UM} Normalization
Construction of individual-author-level feature sets
… …… … Author i Author L
Feature transformation using PCA
Feature generation
RS RS RS
Training
texts
Combination scheme
Identification modelAnonymous
texts
Identification result
Semi-random subspace method
split dataset
Fig. 1. Writeprint identification system based on Semi-RS.
Z. Liu et al. / Neurocomputing 108 (2013) 93–102 952. Project the transformed feature vector to each of the K  L
random subspaces and feed them to the K  L corresponding
BCs in parallel.3. Run the K  L BCs and feed the all the classification results to
the ensemble as follows.
votes a1,a2,:::,aL9m
 
¼ ensemble Ci,j m,Si,j
 
,1r irL, 1r jrK :
ð1Þ4. According to the voting results, make a decision and output
the final identification result.Different from the conventional RSMs that completely ran-
domly samples from the original feature set, the Semi-RS makes
full use of local stylistic information by mining IAFS. Hence the
sampled features from these regions of IAFSs are not greatly
affected by varying pose. By contrast, under the conventional
RSMs, the sampled features will vary considerably from those
training subsets by completely randomly dividing method, and
the constructed BCs may become unstable. On the other hand,
IAFSs consist of a feature set for each author (e.g., 50 authors¼50
feature sets), each IAFS represents a unique writeprint of an
author. Similar to fingerprints, and the writeprint of a given
author is different with other authors, so the feature sets sampled
from different IAFSs can generate more diverse BCs than the
conventional RSMs. In Semi-RS algorithm, further details about
the calculation of IAFS and combination scheme are illustrated as
following sections.2.1. Calculation of IAFS
The rationale behind IAFS selection is that it is usually quite
difficult to find the best features (AGFS) for all of the authors.
Fortunately, the idea of individual writeprint provides a critical
clue to find the best features to represent an author’s unique
writing style. The designed method makes possible to find the
specific feature sets that, although not well suited to discriminate
all of the authors, are optimal to distinguish style patterns that
belonging to a given author from the ones belonging to the otherauthors. To find the IAFS for author ai, let A be the whole author
set, we calculate the best features to discriminate this author ai
(i.e., training texts written by ai) from all the other authors’ set
ai ai \ ai ¼ |,ai [ ai ¼ A
 
. Here we introduce a separability mea-
sure rule [22] which sorts the features that maximize the
divergence between ai and ai, and then features with larger
divergence value are selected as IAFS of ai. Specifically, the
divergence dki of each feature xk for ai and ai is calculated
according to the following measure.
Let pðxk9aiÞ and p xk9ai
 
be the probability density function of
feature variable xk for ai and ai, the mutual divergence of xk for ai
and ai can be expressed as
Dk ai,aið Þ ¼
Z þ1
1
p xk9ai
 
ln
p xk9ai
 
p xk9ai
  dxk
Dk ai,aið Þ ¼
Z þ1
1
p xk9ai
 
ln
p xk9ai
 
p xk9ai
  dxk ð2Þ
where Dk ai,aið Þ is different from Dk ai,aið Þ. To obtain a measurable
result, the average divergence of xk for ai and ai is calculated using
the following formula:
dki ¼D
k ai,aið ÞþDk ai,aið Þ
¼
Z þ1
1
p xk9ai
 
 ln
p xk9ai
 
p xk9ai
  þp xk9ai  ln p xk9ai
 
p xk9ai
 
 !
dxk
¼
Z þ1
1
p xk9ai
 
 ln
p xk9ai
 
p xk9ai
  þp xk9ai  ln p xk9ai
 
p xk9ai
 
" #10@
1
Adxk
¼
Z þ1
1
p xk9ai
 
p xk9ai
 
Þln
p xk9ai
 
p xk9ai
  dxk
 
ð3Þ
In this study, we assume that the probability density functions
pðxk9aiÞ and p xk9ai
 
are Gaussians N mi,Si
 
and N m0i,S
0
i
 
which
are widely used in high-dimensional data statistics. Respectively,
the calculation of the divergence of each feature xk is simplified as
following:
dki ¼
1
2
s0i2
s2i
þ
s2i
s0i2
2
 !
þ 1
2
mim0i
 2 1
s2i
þ 1s0i2
 !
ð4Þ
As we can see in formula (4), the divergence cannot depend
only on the difference of the mean values; it must also be variance
dependent. Therefore, both mean values and variance of feature
xk in the training sets of ai and ai determine its discriminability.
2.2. Combination scheme
After multiple BCs are generated, we need to combine
these BCs to construct an ensemble for the final decision.
The hierarchical and parallel structures are utilized to combine
these BCs. Respectively, ensemble structure of parallel Semi-RS
(PSemi-RS) and the hierarchical Semi-RS (HSemi-RS) are shown in
Fig. 2.
For the PSemi-RS, when an anonymous text vector m is given,
we first divide it into several subsets IAFSi
L
i ¼ 1 and identify the ith
IAFSi using K classifiers Ci,j j¼ 1,2,:::,Kð Þ, which are constructed on
the feature subsets by randomly sampling from IAFSi. Thus, LK
BCs are generated for all IAFSi
L
i ¼ 1. And then, a combination rule is
used to construct the destination classifier. In this paper, the
relative majority voting method is used as combination rule. Let
the BC be Ci,jA0,1, the value of ac Cij
 
equals 1 if the output of the
BC Cij is the author ac otherwise 0. The decision function of
PSemi-RS is described as follows:
author

mÞ ¼ argmax
1r cr L
XL
i ¼ 1
XK
j ¼ 1
ac Cij
 8<:
9=
; ð5Þ
IAFS 1 ………… IAFS i IAFS L
S1 Sj Sk S1 Sj Sk S1 Sj Sk... ... ... ... ... ...
C1 Cj Ck C1 Cj Ck C1 Cj Ck... ... ... ... ... ...
Fusion rule
RS RS RS
IAFS 1 ………… IAFS i IAFS L
S1 Sj Sk S1 Sj Sk... ... ... ...
C1 Cj Ck C1 Cj Ck... ... ... ...
Fusion rule
RS RS RS
RS SVM1 RS SVMi RS SVML
S1 Sj Sk... ...
C1 Cj Ck... ...
Fig. 2. The ensemble structure of identification system (a) the parallel Semi-RS and (b) the hierarchical Semi-RS.
Fig. 3. The improved relative majority voting method.
Z. Liu et al. / Neurocomputing 108 (2013) 93–10296For the HSemi-RS, a two-step ensemble is utilized. First
HSemi-RS performs a classifier combination within each IAFSi to
classify corresponding subset IAFSi
L
i ¼ 1 from the anonymous text
vector, L classification results are obtained; and then combine the
L classification results using the majority voting rule. Accordingly,
RSSVMi represents the SVM classifier formed by K random
subspaces partitioned from IAFSi, the decision function of
HSemi-RS is described as follows:
author

mÞ ¼ argmax
1r crL
XL
i ¼ 1
argmax
1r cr L
XK
j ¼ 1
ac Cij
 24
3
5
8<
:
9=
; ð6Þ
However, common majority voting method may encounter the
problem of the same highest votes; hence when there are several
same votes appear in decision results, we apply a feedback
mechanism of computing similarity between the anonymous text
and the training texts of potential authors who have the same
votes. Finally, the author who has the highest similarity score will
be identified as the authorship of the anonymous text. The
improved algorithm is illustrated in Fig. 3.
The improved voting method computes the correlation coeffi-
cient sim_score m,Tji
 
between m and each text Tji of potential
author Si using an iterative accumulation method (Lines 2–7 in
the algorithm). During each iteration, such a similarity score is
added to the total score highest_score Sið Þ belong to Si. In this way,
similarity score for each potential author can be obtained to
match the anonymous text, finally the author who has the highest
score is identified as the author of the anonymous text m.
3. Theoretical analysis of ensemble method
We assume that each sample x,yð Þ in the training set is
independently drawn from the underlying probability distributionP, where y represents the class label of sample x. The correspond-
ing feature space is denoted by F 0. f ðx,FÞ is an individual predictor
constructed in the feature subspace F, where F is generated by
randomly sampling on the full feature space F 0 using Semi-RS. Then
the classifier ensemble is
f E x,Fð Þ ¼ EF f x,Fð Þ ð7Þ
where EFf x,Fð Þ represents the expectation of f ðx,FÞ over F.
Let X,Yð Þ be the random variables denoting a testing sample
drawn from the identical probability distribution P, and indepen-
dent of the training set, where Y is the voting indicator of sample
X. For example, Y ¼ 6,3,:::,2ð Þ indicates that x belongs to author a1
as the votes corresponding to a1 is the maximum value of all
indicators. The mean-squared error of predictions by the BC f ðx,FÞ
is deduced as following:
Errðf Þ ¼ EFEY ,X Yf X,Fð ÞÞ2

¼ EFEY ,X Y2þ f 2 X,Fð Þ2Y  f X,Fð ÞÞ

¼ EFY2þEFEXf 2 X,Fð Þ2 EY Yð Þ  EXf E X,Fð ÞÞ

¼ EFY2þEXEFf 2 X,Fð Þ2 EY Yð Þ  EXf E X,Fð ÞÞ

ð8Þ
And the mean-squared error of predictions by the classifier
ensemble is deduced as following:
Err f E
 
¼ EY ,X Yf E X,Fð ÞÞ2

¼ EY ,X Y2þ f 2E X,Fð Þ2Y  f E X,Fð ÞÞ

¼ EY Y2þEXf 2E X,Fð Þ2 EY Yð Þ  EXf E X,Fð ÞÞ

¼ EY Y2þEX½EFf X,Fð Þ22 EY Yð Þ  EXf E X,Fð ÞÞ

ð9Þ
According to the definition of the variance, we have
DF f X,Fð ÞÞZ0ð ð10Þ
Thus,
Errðf ÞErr f E
 
¼ EXEFf 2 X,Fð ÞEX½EFf X,Fð Þ2
¼ EX½EFf 2 X,Fð Þ EFf X,Fð ÞÞ2

¼ EXDFf X,Fð ÞZ0 ð11Þ
From the inequality (11), we can deduce that the mean-
squared error of the classifier ensemble is smaller than the
average mean-squared error of the individual BC. The reason is
that the ensemble of multiple stable classifiers can reduce the
classification error [23], as multiple experts in different fields
make a more powerful decision. How much improvement the
ensemble can achieve relies on the difference between EFf
2ðX,FÞ
and EFf X,Fð ÞÞ2

. The more diverse the output of individual BC
f ðX,FÞ is, the better performance the ensemble system can achieve.
The individual BC f ðX,FÞ have larger error diversity by using
proposed RSM, as the random subspaces capture discriminative
Z. Liu et al. / Neurocomputing 108 (2013) 93–102 97information from different portions of the reduced PCA subspace.
Our method randomly constructs multiple BCs on IAFSs of
different authors; different discriminative information in these
IAFSs can make more diverse BCs. In order to utilize the diversity
more sufficiently, we prefer to combine all generated BCs in
parallel way as shown in Fig. 2(a).4. Experiment studies
4.1. Dataset
Reuter corpus volume 1 (RCV1) is a large corpus for the English
language; the texts in RCV1 are short (approximately 2–8 kbytes).
The top 50 authors (with respect to total size of texts) in the RCV1
have been selected as benchmark dataset for writeprint identifi-
cation [12,24]. In this study, after removing all duplicate texts
found the R-measure [25], the top 50 authors of texts labeled with
at least one subtopic of the class CCAT (corporate/industrial) were
selected. The training set consists of 2500 texts (50 per author)
and the testing set includes other 2500 texts (50 per author) non-
overlapping with the training set. Some brief information and
reported results about the dataset is summarized in Table 1. In
addition, characters and words usage information of different
authors in the training set are described as Fig. 4, it can be
observed that the usage frequencies of words among different
authors are similar, and mostly concentrated in the range of [450,
550]. But the usage frequencies of characters are very different
ranging from 2000 to 3700 across the 50 authors, so characters
contain richer discriminative information, which is suitable for
writeprint identification.Table 1
The information description of dataset in this study and reported accuracy results.
Training set Testing set
Authors 50 50
Texts per author 50 50
Words per text 502.33 509.56
Characters per text 3078.82 3127.11
Reported results (accuracy %)
Houvardas and Stamatatos [12] 74.04
Stamatatos [24] 73.06
Fig. 4. Characters and words usage information of 50 authors.4.2. Experimental settings
In the preprocessing, no linguistic processing of the corpus is
required for extracting the feature set for current study. We first
extract 10,000 most frequent character n-grams (ordered by
decreasing frequency of occurrence) from training set. There are
26 1-g, 3127 2-g, 3476 3-g, 2896 4-g and 475 5-g included in the
variable length n-grams feature set. Here, the granularity selec-
tion of n-grams is inspired by the work [17]; It is suitable to keep
the granularity nr5, and too big selection granularity will
increase the sparsity of feature space. Second, we normalize every
component of the feature vector to the same scale and apply PCA
to transform the normalized feature vector. Respectively, 2100
larger eigenvectors v¼ ðv1,v2,:::,v2100Þ in PCA subspace are
selected to preserve nearly 98% of information rate of original
feature set.
In order to show the effectiveness of Semi-RS, we compare
Semi-RS (PSemi-RS, HSemi-RS) with Ens–SVM, EDS–SVM, RSM in
PCA subspace (RS–PCA), Wang’RSM [19] and SVM, SVM–PCA (the
single classifier) in the following experiments. The improved
voting method is adopted as the combination rule of these
ensembles. SVM with linear kernel function is selected as under-
lying classifier in all identification methods. The settings of these
methods are as follows:(1)Tabl
Para
Alg
PS
HS
En
ED
RS
W
SV
SVFor Semi-RS, there are three important parameters need to be
set: the size of IAFS, random sampling rate and the number of
random subspaces sampled from each IAFS. Table 2 lists their
settings and a further discussion of their influence on identi-
fication result is given in Section 4.3.2.(2) For Ens–SVM, each BC is constructed by an IAFS. This method
is designed according to the ensemble mechanism presented
in [6], in which the number of authors equals the number
of BCs.(3) Among the conventional RSMs, EDS–SVM uses the method of
random sampling without replacement. According to the
idea in [17], the lower the size of random subspace, the better
the performance of the ensemble system. Thus, we set
4 dimensions as the size of each subspace in EDS–SVM, and
select the most discriminative 500 subspaces from the origi-
nal 525 subspaces in terms of the information rate. In
Wang’RSM, we do random sampling in the reduced PCA
subspace. Each random subspace is spanned by n0þn1
dimensions. The first n0 dimensions are fixed as those n0
largest eigenvectors in ut and the other n1 dimensions are
randomly sampled from remaining eigenvectors. The rate of
n0/n1 is set as 3/2 to maintain the stability and diversity of
different BCs.(4) For the single classifier, SVM–PCA uses the transformed 2100
feature set by PCA; SVM uses the original feature set including
10,000 normalized n-grams features.e 2
meters settings for algorithms.
orithm Random
sampling rate
Size of
subspace
Size of
IAFS
Number of
BCs
Feature set
type
emi-RS 0.5 525 1050 500 IAFS
emi-RS 0.5 525 1050 500 IAFS
s–SVM – – 1050 50 IAFS
S–SVM 0.002 4 – 500 AGFS
–PCA 0.5 1,050 – 500 AGFS
ang’RSM 0.5 1,050 – 500 AGFS
M–PCA – 2,100 – – AGFS
M – 10,000 – – AGFS
Fig. 6. The average accuracy of combining 500 BCs constructed in the random
subspaces using majority voting. Features in each subspace are selected by
randomly sampling with replacement.
Z. Liu et al. / Neurocomputing 108 (2013) 93–10298For all RSM based methods (PSemi-RS, HSemi-RS, EDS–SVM,
RS–PCA, Wang’RSM), we repeat independently each experiment
10 times and report their averaged accuracies. We mainly use the
following criterion to evaluate the identification result.
Accuracy¼ number of correctly identif ied testing texts
total number of testing texts
ð12Þ
4.3. Results
4.3.1. Comparative results
First, we use EDS with SVM to construct a classifier ensemble,
for each subspace, we select 4 dimensions from the feature vector
v¼ ðv1,v2,:::,v2100Þ by randomly sampling without replacement.
An individual BC is then trained on the selected 4 features. Fig. 5
shows the identification result of combining the best 500 BCs
using majority voting. We can observe that by sampling disjoint
feature subsets, the accuracy of each BC is very low, ranging from
4% to 8%. Although these weak BCs are greatly enforced using
combination rule, only 54.4% accuracy is achieved. The main
reason is that the granularity of each subspace is too small to
form multiple stable BCs.
Second, the RS–PCA is used to conduct the experiment. The
ensemble construction method is very simple that 1050 dimen-
sions are completely randomly sampled from the preserved
features v¼ ðv1,v2,:::,v2100Þ. Then a BC is trained on each sampled
subspace. Fig. 6 demonstrates the identification result using
RS–PCA. We can see that by using the completely random
method, the accuracy of each BC is lower than 70%, ranging from
56% to 70%. But constructed BCs are more diverse than EDS–SVM,
hence these BCs are greatly enforced with the improved majority
voting, and 74.68% accuracy is achieved. The result shows that the
BCs constructed in different random subspaces are complemen-
tary of one another.
Third, Wang’RSM is adopted to construct an improved ensem-
ble. We fix the first 500 dimensions of each random subspace as
the largest 500 eigenvectors, and randomly select the other 725
dimensions from remaining eigenvectors. As described in Fig. 7,
the BCs constructed in the random subspaces are improved
significantly as much discriminative information is contained in
the first 500 largest eigenvectors. But the same part in each
subspace leads to a low diversity among the predictions ofFig. 5. The average accuracy of combining 500 BCs constructed in the random
subspaces using majority voting. Features in each subspace are selected by
randomly sampling without replacement.
Fig. 7. The average accuracy of combining 500 BCs using Wang’RSM, the first 500
dimensions of each random subspace are fixed as the 500 largest eigenvectors, and
the other 725 dimensions are randomly selected from remaining eigenvectors.different BCs. The ensemble method obtains the accuracy rate of
75.08%, which is higher than RS–PCA. The result shows that the
first 500 largest eigenvectors contribute to the stability of each
BC, but other 725 dimensions randomly selected from the
remaining 1600 eigenvectors are not enough to improve the high
diversities among different BCs. The main reason is that the
distributions of classification errors of these BCs are similar with
one another in the whole sample space.
Finally, we use proposed Semi-RS to improve the overall
performance of the ensemble system. In this method, we first
select an IAFS for each author, then 10 feature sets are randomly
sampled from each IAFS, Fig. 8 shows that by PSemi-RS, the
ensemble method achieves the accuracy rate of 77.04%, which is
higher than the reported results and the conventional RSMs
above. HSemi-RS also achieves a high accuracy of 76.48%. More-
over, the accuracy of each BC ranges from 45% to 68%; the
fluctuation range of accuracy is wider than that of conventional
RSMs. The main reason is that each subspace contains a part of
individual writeprint of an author; the discrepancy of the indivi-
dual writeprint ensures a high diversity among different BCs.
Z. Liu et al. / Neurocomputing 108 (2013) 93–102 99The result demonstrates that the introduction of IAFS is effective for
capturing potential discriminative features, which provide heuristic
information of individual writeprint for each random subspace. It
needs to be noted that the result is not always consistent for every
run. We find the decision results of several samples are different for
some times. A possible reason is that the randomness of sampling
leads to the deviation of some BCs’ outputs. However, the ensemble
identification results are still stable for most of identified texts. For
the further comparison, we list identification results of different
algorithms in Table 3, including average value, standard deviation of
BCs’ accuracies and so on. Here, we introduce the entropy measure
to compute the diversity of BCs. The measure method is described as
formula (13).
entropy¼ 1
T
XT
i ¼ 1
XL
j ¼ 1

Nij
K
logL
Nij
K
 !
ð13Þ
where K is the number of BCs, T is the total number of test texts, Nij
is the number of BC that assign text i to class j and L is the number of
classes. The effectiveness of a classifier ensemble is indirectly
indicated by the diversity of the BCs. The higher the entropy of an
ensemble, the more diverse the predictions of the different BCs.
Table 3 shows all the evaluation indices of different algo-
rithms. In the aspect of single classifier, both SVM and SVM–PCA
obtain below 75% accuracy, even though the result is the best
accuracy that a SVM single classifier can obtain. The result
explains the limitation of the single classifier using the whole
author-group feature set. In the aspect of classifiers ensemble, theFig. 8. The average accuracy of combining 500 BCs using Semi-RS (PSemi-RS,
HSemi-RS). The 525 dimensions of each random subspace are randomly sampled
form the IAFS for each author.
Table 3
Comparison results of different algorithms (%) (the best performance is bolded).
Algorithm Classifiers ensemble
EDS–SVM RS–PCA Wang’
Accuracy (using improved voting method) 52.48 74.68 75.08
Accuracy (using common voting method) 52.48 73.24 74.48
Mean accuracy 5.22 65.45 74.32
Standard deviation 0.39 2.26 0.35
Diversity 4.24 26.61 4.32
Recall 52.48 74.68 75.08
Precision 53.15 75.16 75.32Semi-RS exhibits their effectiveness and superiority to other
compared methods in terms of diversity. It is worth to note that
BCs constructed by PSemi-RS are the same as HSemi-RS; they
only adopt different combination methods. Obviously, PSemi-RS
outperforms HSemi-RS, the main reason is that PSemi-RS can
utilize diversity more sufficiently than HSemi-RS does, similar
conclusion can be drawn that the parallel structure outperforms
the hierarchical structure in [26]. Among the conventional RSMs,
EDS–SVM shows the worst performance both in the stability and
the diversity among BCs, RS–PCA constructs more diverse BCs, but
the completely random sampling method cannot effectively
capture discriminative features in the whole feature space.
Wang’RSM achieves only the standard deviation of 0.35%, which
is lower than other algorithms. Accordingly, the BCs constructed
in Wang’RSM have less diversity of 4.32%. Like Semi-RS, the
selection mechanism of IAFS is also utilized in Ens–SVM. But
the ensemble method does not achieve an ideal result since the
number of authors determines the number of BCs, which is far
less than that of other ensembles.
On the other hand, recall and precision are also important
indices for measuring false positives and false negatives. The
results show that recall is equal to accuracy for all algorithms
since each author class owns the same number of samples in
testing set. PSemi-RS achieves a higher precision than other
algorithms, which means that PSemi-RS has a lower false alarm
rate in identifying texts of a potential author. Besides, Ens–SVM
still obtains good average precision although it generates far less
BCs. The possible reason is that each BC is trained on an IAFS,
which contains important discriminative information between
different individuals. But the selection granularity of feature
subset is too big to effectively utilize these individual features
in Ens–SVM. For PSemi-RS, a low omission rate of 22.96%
indicates that the proposed method can be more sensitive to
detect texts written by a potential author with less loss.
To compare the performances of classification algorithms on
the accuracy of writeprint identification, 5 individual pairwise t
tests are conducted. Table 4 shows the statistic value and p-value
of the t tests for classifier comparison. Alpha¼0.05 is chosen as
the threshold for each individual t test. We can observe that for
each individual t test, PSemi-RS achieves significantly higher
accuracy than other algorithms. The performance differences
between these algorithms are considered statistically significant
if the p-value is less 0.05. Therefore, the experimental result
strongly indicates that PSemi-RS is more robust than other
algorithms in the high-dimensional feature space.
These comparative results demonstrate the view that the
diversity among BCs depends on the difference between
EFf
2ðX,FÞ and EFf X,Fð Þ2. In addition, the ensemble algorithms
using improved voting method achieves better performance than
those using common voting method, which demonstrates the
confusion results may appear in the classifiers ensemble. The
same voting results greatly affect the final decision. With theSingle classifier
RSM PSemi-RS HSemi-RS Ens–SVM SVM SVM–PCA
77.04 76.48 75.04 74.28 74.84
76.28 75.16 74.86 – –
56.86 56.86 72.15 – –
3.86 3.86 1.67 – –
38.90 38.90 18.24 – –
77.04 76.48 75.04 74.28 74.84
77.41 77.24 75.33 74.64 75.12
Table 4
Pairwise t test on accuracy for different algorithms (a¼0.05, df¼9).
PSemi-RS vs. HSemi-RS RS–PCA Wang’RSM Ens–SVM SVM SVM–PCA
Test statistic value 2.925 11.545 10.395 10.984 12.633 11.266
p-value 0.0169 0.000103 0.000194 0.000143 0.0000587 0.000122
Fig. 9. Accuracy vs. number of authors and identification algorithms.
Fig. 10. Accuracy vs. number of texts and identification algorithms.
Z. Liu et al. / Neurocomputing 108 (2013) 93–102100improved voting method, the dilemma of voting decision can be
effectively solved.
4.3.2. Scalability of Semi-RS
To evaluate the scalability of the Semi-RS, two experiments are
conducted to examine the impact of parameters of writeprint
identification, including number of authors and number of texts
per author. The results are described in Figs. 9 and 10.
We first compare the identification accuracy of different
algorithms (PSemi-RS, RS–PCA, Wang’RSM, Ens–SVM and SVM–
PCA) when the number of authors varies from 5 to 50. As is shown
in Fig. 9, the PSemi-RS appears to be more scalable as the number
of authors increases. The proposed algorithm obtains nearly 100%
accuracy in identifying five authors and keeps a high robustness
ranging from 20 to 50 authors. In contrast, the accuracy of other
algorithms decreases more sharply as the number of authors goes
from 5 to 50. Specially, the SVM single classifier can obtain the
high accuracy of above 90% when the number of authors is
between 5 and 10. But SVM presents a sharper decreasing trend
than other algorithms as the number of authors goes from 20 to
50. Futhermore, the gap in accuracy appears to widen as the
number of authors increases. It appears that IAFSs effectively
differentiate authorship across larger number of authors.
Given 20 authors, the five algorithms are compared when the
number of training texts per author varies from 10 to 50. From
Fig. 10, we can observe that the accuracy of classification
increases as the number of texts per author increases. The results
are consistent among different algorithms. Additionally, SVM–
PCA as the best single classification model achieves 64.38% to
71.62% accuracy when the number of texts varies from 10 to 20,
which is similar to the performance of these ensembles. But, there
is still a slight advantage for PSemi-RS as compared to other
algorithms when the training set is small. Indeed, it is difficult to
improve performance of these classifiers when fewer learning
resources are provided for training models. However, the gap
between the PSemi-RS and the SVM model appears to widen as
the number of training texts increases, since more informative
features can be exploited. These results above indicate that
PSemi-RS has a better scalability than traditional ensemble
methods and SVM.
4.3.3. Selection of parameters
To illustrate the influence of parameters (size of IAFS, random
sampling rate) on the performance of Semi-RS, we conduct
experiments by employing the parallel ensemble structure
(PSemi-RS). The experimental results are described in Fig. 11.
We find that the smaller the random sampling rate, the more
efficient the algorithm performs, but at the same time, many
discriminative features will be lost. One extreme case is that only
one dimension is selected as sampled feature from each IAFS. So
the random sampling rate should not be too small. On the
contrary, the random sampling rate should also not be too big.
The bigger the random sampling rate, the less the diversity
between BCs is. Another extreme case is that the PSemi-RS will
reduce to the ensemble of all IAFSs, when the random sampling
rate is 100%. We set the size of each IAFS to 1250, and report the
identification accuracies by varying random rate from 0.1 to1.0 by interval 0.1 as in Fig. 11(a), from which we can observe
that PSemi-RS (1) achieves high accuracies of about 77% on the
range of [0.5, 0.7]; (2) obtains the highest diversity when the
random rate is 0.3, and with the random rate turns bigger, the
diversity will become lower, finally it can keep relatively stable
when the random rate is between 0.7 and 1.0.
Another parameter needs to be selected in PSemi-RS is the size
of IAFS. Similar to the analysis above, we can find that the smaller
the size of IAFS, the less selected discriminative features is. When
the size of IAFS is one dimension, random sampling will become
infeasible. On the contrary, the bigger the size of IAFS, the less
individual features the PSemi-RS extracts. When the size of IAFS is
Fig. 11. Influence of parameters on performance. (a) Influence of random sample
rate. (b) Influence of size of IAFS.
Z. Liu et al. / Neurocomputing 108 (2013) 93–102 101equal to that of PCA subspace, the PSemi-RS reduces to the
completely random sampling method as RS-PCA and little indivi-
dual stylistic information can be extracted. To examine the
influence of the IAFS size on the performance of PSemi-RS, we
fix a random sampling rate to 0.5. As shown in Fig. 11(b), PSemi-
RS (1) can obtain the highest accuracy when the IAFS size is set to
1500, and gain a relatively stable performance when the IAFS size
is between 1750 and 2500; (2) can gain the highest diversity
when the IAFS size is set as 750. With the increasing of the IAFS
size, the diversity will become low which is similar to the
selection of random rate.
The results above confirm our intuitive theoretical analysis;
the IAFS size should be neither too small nor too big, but it needs
to consider the complexity of system to choose a proper size of
IAFS. Too big random rate will decrease the running efficiency of
identification system, and when the size of the random rate turn
bigger, the diversity will obviously become lower. Moreover, it is
worth to note that the number of BCs is also an optional
parameter. Since the RSM tends to converge toward a stable
point when hundreds of BCs are generated in our experiment, the
iteration number is set as 10 in each IAFS. Like the random
sampling rate, too big number of BCs will increase the burden of
system. Actually, the trade-off between diversity and accuracy ofBC has been a problem needs to be solved in classifier ensemble,
but generally, both diversity and accuracy are inversely related. In
this study, by introducing the selection of IAFS, the accuracy–
diversity dilemma is effectively solved; hence Semi-RS achieves a
better performance with heuristic information of IAFSs.5. Conclusions
In this paper, we have developed a Semi-RS algorithm espe-
cially for writeprint identification. Unlike conventional random
subspace method which completely randomly selects features
from the whole feature space (or PCA subspace), Semi-RS takes
into account the distribution of individual-author writeprint
hidden in feature space as well as make full use of the discrepancy
among individual writeprint. The algorithm randomly samples
features on each individual-author feature set partitioned from
PCA subspace, which improves the diversity among base classi-
fiers and robustness of identification. Experimental results on the
benchmark dataset show that Semi-RS outperforms conventional
random subspace methods with a high accuracy rate acceptable
for identification system, and Semi-RS (1) fuses the advantage of
RSM and individual discriminative features for each author; (2) is
more effective than the single classifier using author-group
feature set; (3) is more scalable than conventional random sub-
space methods. Furthermore, we also discuss the diversities of
different algorithms using entropy measure, and the results show
that Semi-RS achieves a higher diversity than EDS–SVM, RS–PCA,
Wang’RSM and Ens–SVM.Acknowledgements
This work was supported by the National Key Technology R&D
Program in the 12th Five-Year Plan (Grant no. 2011BAK08B03)
and self-determined research funds of CCNU from the colleges’
basic research and operation of MOE (No.CCNU09A02006). We
would like to thank Prof. Stamatatos at University of the Aegean
who provided the experimental dataset for this study, and
reviewers for their comments which helped improve this paper.References
[1] F. Iqbal, R. Hadjidj, B.C.M. Fung, M. Debbabi, A novel approach of mining
write-prints for authorship, Digital Invest. 5 (2008) 42–51.
[2] F. Iqbal, H. Binsalleeh, B.C.M. Fung, M. Debbabi, Mining writeprints from
anonymous e-mails for forensic investigation, Digital Invest. 7 (2010) 56–64.
[3] F. Iqbal, H. Binsalleeh, B.C.M. Fung, M. Debbabi, A Unified Data Mining
Solution for Authorship Analysis in Anonymous Textual Communications,
Information Sciences, DOI: 10.1016/j.ins.2011.03.006 (2011).
[4] O. de Vel, A. Anderson, M. Corney, G. Mohay, Mining e-mail content for
author identification forensics, SIGMOD Rec. 30 (4) (2001) 55–64.
[5] A. Abbasi, H. Chen, Applying authorship analysis to extremist-group web
forum messages, IEEE Intell. Syst. 20 (5) (2005) 67–75.
[6] A. Abbasi, H. Chen, Writeprints: a stylometric approach to identity-level
identification and similarity detection in cyberspace, ACM Trans. Inf. Syst. 26
(2) (2008) 1–29.
[7] A. Narayanan, H.S. Paskov, N.Z. Gong, J. Bethencourt, E.C.R. Shin, E. Stefanov,
D. Song, On the feasibility of internet-scale author identification, in: Proceed-
ings of the IEEE Symposium on Security & Privacy (IEEE S&P), California, USA,
2012, in press.
[8] J. Li, R. Zheng, H. Chen, From fingerprint to writeprint, Commun. ACM 49 (4)
(2006) 76–82.
[9] F. Sebastiani, Machine learning in automated text categorization, ACM
Comput. Surv. 34 (1) (2002) 1–47.
[10] R. Zheng, J. Li, H. Chen, Z. Huang, A framework for authorship identification of
online messages: writing-style features and classification techniques, J. Am.
Soc. Inf. Sci. Technol. 57 (3) (2006) 378–393.
[11] V. Keselj, F. Peng, N. Cercone, C. Thomas, n-gram based author profiles for
authorship attribution, in: Proceedings of the Conference Pacific Association
for Computational Linguistics, 2003, pp. 255–264.
Z. Liu et al. / Neurocomputing 108 (2013) 93–102102[12] J. Houvardas, E. Stamatatos, n-gram feature selection for authorship identi-
fication, in: Proceedings of the 12th International Conference on Artificial
Intelligence: Methodology, Systems, Applications, 2006, pp. 77–86.
[13] T.K. Ho, The random subspace method for constructing decision forests, IEEE
Trans. PAMI 20 (8) (1998) 832–844.
[14] X. Wang, X. Tang, Random sampling LDA for face recognition, in: Proceedings
of the 2004 IEEE Conference on Computer Vision and Pattern Recognition,
2004, pp. 259–265.
[15] Y. Yaslan, Z. Cataltepe, Co-training with relevant random subspaces, Neuro-
computing 73 (2010) 1652–1661.
[16] L.I. Kuncheva, J.J. Rodriguez, C.O. Plumpton, D.E.J. Linden, S.J. Johnston,
Random subspace ensembles for fMRI classification, IEEE Trans. Med.
Imaging 29 (2) (2010) 531–542.
[17] E. Stamatatos, Ensemble-based author identification using character
n-grams, in: Proceedings of TIR’06, 2006, pp. 41–46.
[18] S.C. Chen, Y.L. Zhu, Subpattern-based principle base analysis, Pattern Recog-
nit. 37 (1) (2004) 1081–1083.
[19] X. Wang, X. Tang, Random sampling for subspace face recognition, Int.
J. Comput. Vision 70 (1) (2006) 91–104.
[20] J.F. Burrows, Word patterns and story shapes: the statistical analysis of
narrative style, Lit. Linguist. Comput. 2 (1987) 61–70.
[21] C. Silva, B. Ribeiro, RVM ensemble for text categorization, Int. J. Comput.
Intell. Res. 3 (1) (2007) 31–35.
[22] S. Theodoridis, K. Koutroumbas, Pattern Recognition, fourth ed., Elsevier
Science Publishers, 2009.
[23] Z.H. Zhou, Y. Yang, Ensembling local learners through multimodal perturba-
tion, IEEE Trans. Syst. Man Cybern.—Part B 35 (4) (2005) 725–735.
[24] E. Stamatatos, Author identification using imbalanced and limited training
texts, in: Proceedings of the Fourth International Workshop on Text-based
Information Retrieval, 2007, pp. 237–241.
[25] D.V. Khmelev, W.J. Teahan, A repetition based measure for verification of text
collections and for text categorization, in: Proceedings of the 26th ACM SIGIR
Conference on Research and Development in Informaion Retrieval, 2003,
pp. 104–110.
[26] D.C. Tao, X.O. Tang, Asymmetric bagging and random subspace for support
vector machines-based relevance feedback in image retrieval, IEEE Trans.
Pattern Anal. Mach. Intell. 28 (7) (2006) 1088–1098.Zhi Liu is a Ph.D. Candidate at National Engineering
Research Center for E-learning (NERCEL), Central China
Normal University (CCNU), Wuhan, China. His research
interests are in machine learning, data mining, intelli-
gent system and knowledge service.Zongkai Yang received the B.E. and M.E. degrees from
Huazhong University of Science and Technology
(HUST), Wuhan, China, in 1985 and 1988, respectively,
and the Ph.D. degree from Xi’an Jiaotong University,
Xi’an, China, in 1991. From 1991 to 1993, he devoted
himself to his postdoctoral research in HUST. He is
currently a professor in NERCEL, CCNU. His research
interests include signal processing, network commu-
nication, and information technology.Sanya Liu received the B.E. and M.E. degrees in 1996
and 1999, and received the Ph.D. degree in 2003 from
HUST. He devoted himself to his postdoctoral research
in Xiamen University from 2003 to 2005. Currently,
he is a professor in NERCEL, CCNU. His research
interests include artificial intelligence, and computer
application.Yinghui Shi is a Ph.D. Candidate at National Engineer-
ing Research Center for E-learning (NERCEL), Central
China Normal University (CCNU), Wuhan, China. His
research interests are in educational information
technology.
