Who Wrote This? Textual Modeling with Authorship
Attribution in Big Data
Naruemon Pratanwanich
Computer Laboratory
University of Cambridge
William Gates Building
JJ Thomson Avenue, CB3 0FD
Email: np394@cam.ac.uk
Pietro Lio’
Computer Laboratory
University of Cambridge
William Gates Building
JJ Thomson Avenue, CB3 0FD
Email: pl219@cam.ac.uk
Abstract—By representing large corpora with concise and
meaningful elements, topic-based generative models aim to re-
duce the dimension and understand the content of documents.
Those techniques originally analyze on words in the documents,
but their extensions currently accommodate meta-data such as
authorship information, which has been proved useful for textual
modeling. The importance of learning authorship is to extract
author interests and assign authors to anonymous texts. Author-
Topic (AT) model, an unsupervised learning technique, success-
fully exploits authorship information to model both documents
and author interests using topic representations. However, the
AT model simplifies that each author has equal contribution
on multiple-author documents. To overcome this limitation, we
assumes that authors give different degrees of contributions on
a document by using a Dirichlet distribution. This automatically
transforms the unsupervised AT model to Supervised Author-
Topic (SAT) model, which brings a novelty of authorship
prediction on anonymous texts. The SAT model outperforms
the AT model for identifying authors of documents written by
either single authors or multiple authors with a better Receiver
Operating Characteristic (ROC) curve and a significantly higher
Area Under Curve (AUC). The SAT model not only achieves com-
petitive performance to state-of-the-art techniques e.g. random
forests but also maintains the characteristics of the unsupervised
models for information discovery i.e. word distributions of topics,
author interests, and author contributions.
I. INTRODUCTION
Characterizing the content of documents is generally con-
ducted in order to search, organize, and classify a large
collection of documents effectively. Some documents often
come with a variety of side information such as authors,
keywords, and publishers, while such information is missing
in others and needs to be predicted. Authorship attribution
involves assigning authors to anonymous texts, which plays
an important role in areas such as criminal investigation,
social science, text analysis, cognitive systems, to name but a
few. Since different authors have different interests in writing,
learning their interests based on textual data brings many ad-
vantages such as matching authors and reviewers in publication
systems, recommending favorite media to users, and promoting
personalized advertisement. However, the complication in this
issue is the increasing dimensionality in the number of words
and authors used in the analysis.
Typically, text data can be represented as a count bag-
of-words vector. As the size of vocabulary in the corpus
is increasing, the task is to learn the low frequency count
data in the high-dimensional setting for information discovery
and prediction. Conventional machine learning approaches to
authorship prediction have largely relied on discriminative
modeling techniques that depend crucially on a variety of
features such as word functions, word length distributions, and
word contents [1]. The main drawback is that they generate a
“black box” that makes it hard to understand why they give
high performance on prediction.
To discover the meaningful structures underlying doc-
uments, probabilistic generative models which employ the
abstract definition of topics as a fundamental concept to
generate words have gained popularity for document analysis
as unsupervised learning techniques. In topic-based generative
models, a document is described by a particular topic propor-
tion, where a topic is defined as a distribution over words.
After latent Dirichlet allocation (LDA), a mixed-membership
topic model, was introduced [2], many studies have proposed a
great number of model variations [3]–[7]. The primary goal of
such extensions is to incorporate side information or meta-data
together with words in the texts for better characterization of
the content of documents in unsupervised learning settings. In
contrast, the author model proposed by McCallum [8] uses
authorship information of each document to model author
interests with word distributions instead of the latent topics.
Accordingly, the model assumes that a word in a document is
written by an author of the document. Combining both models,
the author-topic (AT) model represents documents and authors
by topic distributions [9]. Thus, characterizing author interests
and modeling documents can be achieved simultaneously with
concise, meaningful representations.
Regarding authorship attribution, the original AT model
focuses on the unsupervised learning environment where words
are assigned into topics and author interests better than those
being learned by the LDA model and the author model
[9]. Nonetheless, recent studies showed that the topic-based
generative models have been effectively applied for authorship
prediction [10], [11]. The AT model is found superior to
the LDA model to predict authors for anonymous texts [11].
In addition, the performance of authorship prediction can be
improved if the model distinguish words generated from a topic
that belongs to either documents or author interests, resulting
in the disjoint author-document topic (DADT) model [11].
TABLE I: Notations used in the SAT model.
Notation Description
V, T,A,D Number of distinct words, topics, authors, and documents.
Nd, Ad Number of words and authors in a document d respectively.
wd,n, zd,n, xd,n A word and its topic and author assignment of a document d
at position nth.
φt An V -dimensional probability vector of word distribution
under a topic t, per-topic word distribution.
θa A T -dimensional probability vector of topic distribution under
an author a, per-author topic distribution.
ψd An A-dimensional probability vector of author distribution
under a document d, per-document author distribution.
ad An A-dimensional binary vector of authors under a document
d, where ad,a = 1 when a is an author of the document d,
and 0 otherwise.
CTV A T × V count matrix of words for each topic.
CAT A A × T count matrix of topic assignments for each author.
CDA A D × A count matrix of author assignments for each
document.
α, β, γ Concentration parameters of Dirichlet distributions.
However, the DADT performance strongly depends on
the specification of many parameters regarding to the ratio
between documents and author words, which are different from
corpus to corpus. It is also noted that only single-author texts
have been studied. Moreover, even though the LDA model
extensions can accommodate various types of the response
variable for prediction, they rely on the assumption that the
response variable arises from the same underlying topics that
generate words. This hypothesis is not suitable for modeling
authorship where authors can have their own topics of interests
differ from documents they have written.
In this study, we have developed a new generative method,
Supervised Author-Topic (SAT) model, with the aim of iden-
tifying authorship of anonymous documents and concurrently
extracting author interests based on topic-word distributions.
Particularly, we introduce a new definition of author contri-
butions modeled by using an additional Dirichlet distribution
layer in addition to the two Dirichlet priors that are used to
model topics and author interests. With the novelty of putting
the Dirichlet prior on author contributions in a document level,
the SAT model can automatically assign authors to anonymous
documents written by one or multiple authors.
II. SUPERVISED AUTHOR-TOPIC MODEL
Even though the AT model takes advantage of authorship
information to model documents and author interests, it does
not provide an explicit framework for author prediction of
anonymous texts. Moreover, the AT model assumes the equity
of author contributions for documents written by many authors
using a uniform distribution. To overcome these two limita-
tions, the SAT model uses an additional Dirichlet distribution
to model author contributions. Therefore, a document is repre-
sented by a particular proportion of author contributions, where
each author is described by a topic distribution and each topic
is defined as a distribution over vocabulary. With this imple-
mentation, the SAT model can also perform a straightforward
prediction of authors on new anonymous documents. In the
following subsections, we illustrate a mathematical description,
an inference method, and an approach for author prediction of
our proposed model, discussing its structure in comparison to
the AT model. The notations are summarized in Table I.
Fig. 1: Graphical representation of the Supervised Author-
Topic (SAT) model, where shaded nodes are observed data,
transparent nodes are model parameters and latent variables,
the others are hyperparameters. All notations are described in
Table I.
A. Model description
Dirichlet distribution is a distribution over distributions.
Thus, we posit three Dirichlet priors in order to model the
author contribution per document, the topic interest of each
author, and the distribution of words per topic. The generative
process of our SAT model for learning a corpus is fully
described as follows:
1) For each topic t = 1, . . . , T :
a) φt ∼ Dirichlet(α);
2) For each author a = 1, . . . , A :
a) θa ∼ Dirichlet(β);
3) For each document d = 1, . . . , D :
a) ψd ∼ Dirichlet(γad);
4) For each word n = 1, . . . , Nd and d = 1, . . . , D :
a) xd,n ∼ Multinomial(ψd);
b) zd,n ∼ Multinomial(θxd,n );
c) wd,n ∼ Multinomial(φzd,n ).
For each position n in each document d, the process begins
with choosing an author xd,n based on their contributions
ψd modeled by an asymmetric Dirichlet distribution with a
parameter γad. The sparsity pattern of this parameter is the
same as that of the observed author vectors ad where ad,a = 1
when a is an author of the document d, and 0 otherwise. Then,
a topic zd,n is selected from the interests of the author θxd,n
modeled by a distribution drawn from a symmetric Dirichlet
with a parameter β. Finally, a word is picked up from the
distribution over words of that topic φzd,n .
The main differences from the AT model are the assump-
tion of author contributions modeled by a Dirichlet distribution
in Step 2a and the consequence of author assignment in
Step 4a. This generalization on the author contributions also
brings the extra ability to predict authorship of anonymous
documents. Figure 1 exhibits the graphical representations the
SAT model.
B. Inference of model parameters and latent variables
Given a training set of documents where words and authors
of each document are observed, we perform an inverse of the
generative process in order to make inference on the latent vari-
ables and the model parameters (xd,n, zd,n,φt,θa,ψd). Since
the exact joint posterior distribution of the model parameters
is intractable, approximate algorithms such as variational EM
(deterministic) and Gibbs sampling (stochastic) have achieved
the parameter estimation. We apply a collapsed Gibbs sampling
to find only the joint posterior distribution of xd,n and zd,n
while φt,θa,ψd are integrated out (Equation (1)). Due to
the conjugacy of the Dirichlet and multinomial models, the
collapsed Gibbs sampling can be implemented through the
updating of count matrices CTV ,CAT , and CDA which
denote the number of words for each topic, the number of
topics for each author, and the number of authors for each
document respectively. The steps of posterior derivation can
be analogous to those described in the LDA model [12]. In
each iteration of the collapsed Gibbs sampler, the probability
to assign an author a and a topic t to a word wd,n is updated
by Equation (1). Note that the subscript \(d, n) indicates that
all positions excluding the current position n in the current
document d are regarded.
P (xd,n = a, zd,n = t|
wd,n = w,w\(d,n),x\(d,n), z\(d,n), α, β, γ, ad)
∝
CTVt,v,\(d,n) + α∑
v′ C
TV
t,v′,\(d,n) + V α
×
CATa,t,\(d,n) + β∑
t′ C
AT
a,t′,\(d,n) + Tβ
×
CDAd,a,\(d,n) + γ
ad,a
∑
a′(C
DA
d,a′,\(d,n) + γ
ad,a′ )
(1)
Intuitively, the probability of assigning a word w to a
topic t written by an author a depends on three probabilities
- how likely the word w belongs to the topic t, how likely
the topic t is written by the author a, and how likely the
author a contributes to the document d. All count matrices
(CTV ,CAT ,CDA) in Equation (1) are used in the Gibbs
sampler to represent the aforementioned probabilities. Ac-
cordingly, the contribution of an author in a document is
proportional to the number of author assignments to words
in the document with respect to all topics.
Owing to a sufficient statistic, the point estimates of the
model parameter values (φt,θa, and ψd) are finally calculated
as in Equation (2)-(4):
Φ =
CTVt,v + α∑
v′ C
TV
t,v′ + V α
(2)
Θ =
CATa,t + β∑
t′ C
AT
a,t′ + Tβ
(3)
Ψ =
CDAda + γ
ad,a
∑
a′(C
DA
d,a′ + γ
ad,a′ )
(4)
Our R implementation of the pathway-based Gibbs sam-
pling isavailable upon request to the corresponding author.
C. Authorship attribution
Given an unseen anonymous document and the learned
model, we aim to predict a set of authors who wrote the doc-
ument. By folding the initial parameters of the new document
into the learned model, the SAT model identifies authors and
their contributions using the aforementioned Gibbs sampler.
At the initialization step, we randomly generate the count
matrices CTV , CAT , and CDA of the new document and
integrate with those from the latest iteration of the Gibbs
sampler obtained in the training stage. Since the authors of
the new document are not observed, every entry in the binary
vector ad is set to one, that is, every author has equal possibility
to write the document. The sampling process is similar to the
aforementioned inference algorithm, but only samples for the
new document are drawn. Hence, the chain would converge
in tens of iterations. We finally identify the potential authors
of the new document from the probability distribution ψ̃d
estimated by the Dirichlet distribution.
Likewise, we apply the predictive Gibbs sampler which
relies on the probabilistic structure of the AT model. However,
there is no parameter of the AT model that we can interpret
directly for author identification. Based on the latent author
assignment, we therefore count the number of author assign-
ments in the entire document and normalize so as to indicate
the possibility of each author belonging to the document
(Equation (5)).
ψ̃d = P (a ∈ ad|w̃d,M) =
Nd∑
n=1
δ(x̃d,n = a)
Nd
(5)
With a straightforward interpretation on the inferred con-
tributions (ψ̃d), we can identify a subset of authors given an
anonymous text. We remark that other methods that use the
inferred variables from the AT model for author prediction may
yield better results, but we use the results that are confined to
its assumption of the data generation for the model comparison
purpose.
III. RESULTS AND DISCUSSION
We collected 2484 articles of the NIPS conferences from
1987 to 2003. The corpus was made up of 2865 unique authors,
14036 distinct words and 3280697 word tokens in total [13].
Figure 2 depicts the histograms of the number of authors (a)
and words (b) of the NIPS data. Of those, 90% were used as
a training set and the rest as a test set, where every author in
the test set appeared at least once in the training set.
            	    
                 
 ! " #$##%##&##'##
(###
(a)

        )   *    
 ! " + , + + + - + + + . + + + / + + +#0#(##(0#
$##$0#
(b)
Fig. 2: Histograms exhibiting the number of authors (a) and
the number words (b) of the 2484 documents from the NIPS
conferences in 1987-2003.
For the Gibbs sampler settings in this paper, the smoothing
hyper-parameters of both models are conventionally fixed to
the same values at 0.01, 50/T , and 0.01 for α, β, γ respec-
tively. However, we initialize the count matrices of a Markov
chain with random initial assignments. To ensure that the Gibbs
sampler converged to a stationary state, 2000 iterations were
run along with convergence analysis.
A. Convergence analysis and model fitness
To fit a probabilistic model, we aim to achieve the highest
likelihood on the training data set. We used the perplexity
[2], which is a monotonically decreasing function of the data
likelihood (Equation (6)), as a metric of fitness for comparison
between the two models.
perplexity = exp(−
∑D
d=1 log p(wd|α, β, γ, ad)∑D
d=1 Nd
) (6)
Specifically, we computed the perplexity at each iteration
of the Gibbs sampler obtained from the SAT model and the
AT model. The 100-topic models were selected to display as
a representative in Figure 3 (a). Varying number of topics, we
calculated the perplexity at the stationary state (Figure 3 (b)).
A lower perplexity value suggests a relatively better model
fitness. Notably, both models reach the stationary state quickly
after 1000 iterations. However, the SAT model achieves the
invariant state at the lower perplexity, confirming that the con-
cept of contributions can improve the fitness for the research
article data. Even though increasing the number of topics can
enhance the model fitness, it should be traded off with the
computational cost, especially in the author prediction. As a
larger number of topics causes the Gibbs sampler to run slower
per iteration, we explored the performance of both models
based on 100 topics for further evaluation on author interests
and authorship attribution.
In order to investigate the model convergence in the case
of multiple-author documents, we excluded those documents
that did not meet the condition and trained the models again.
Figure 3 shows that the perplexity at the stationary state of
the AT model seems unchanged, while the SAT yields a better
Fig. 3: Perplexity results as a function of iterations from the
Gibbs sampler given T = 100, demonstrated in a refined
perplexity scale from 1400 to 1700 (a). The inset shows all
perplexity values. The solid lines were resulted from running
the models based on the entire corpus, while the dashed lines
relied on only documents written by multiple authors. The
percentage exhibits the difference of the perplexity that the
SAT model improves from the AT model. Perplexity results
varied by the number of topics (b). The dashed line represents
the improvement percentage between the perplexity values
between the two models.
fitness. This could be because the AT model executes single-
author documents in the same way as learning multiple-author
ones, while the SAT model treats these two types of texts
differently by implementing author contributions. Despite the
modest difference in the perplexity of model fitness, the SAT
model has a distinguished performance over the AT model in
the authorship prediction.
B. Topics, author interests, and author contributions
In this section, we display examples of the inferred φt,θa,
and ψd that indicate the word distribution in a topic t, the
topics in which an author a is interested, and the degrees of all
authors contributing in a document d. The selected probability
distributions of these parameters were obtained from a single
chain at the last iteration (2000th) of the Gibbs sampler. Table
II shows the most interesting topics of selected five authors
TABLE II: Representatives of author interests obtained from the SAT model. Each author is shown with the top interesting topic
IDs that have probabilities more than the threshold of 0.01 (1/T ). The topic IDs also correspond to those shown in Table III.
Author: Cole R Author: Agin P Author: Ghahramani Z Author: MacKay D Author: Bishop C
Topics Probability Topics Probability Topics Probability Topics Probability Topics Probability
64 0.895 58 0.842 22 0.551 53 0.335 53 0.379
11 0.070 78 0.152 63 0.180 78 0.20 78 0.334
52 0.098 22 0.183 22 0.189
78 0.071 10 0.093 86 0.067
87 0.036 63 0.027
TABLE III: Representatives of 12 topics obtained from the 100-topic SAT model. The most frequent 10 words are shown in
each topic.
Topic: 10 - Kernel learning Topic: 11 - Classification Topic: 22-Maximum likelihood Topic : 48 - Face recognition
Words Probability Words Probability Words Probability Words Probability
rbf 0.048 classification 0.048 model 0.040 face 0.046
experts 0.028 classifier 0.045 data 0.025 images 0.024
basis 0.024 class 0.039 models 0.021 faces 0.022
expert 0.021 training 0.031 probability 0.019 recognition 0.020
gating 0.019 classifiers 0.026 likelihood 0.015 facial 0.017
network 0.018 classes 0.016 mixture 0.014 image 0.017
radial 0.017 feature 0.015 distribution 0.013 human 0.009
networks 0.017 pattern 0.014 parameters 0.013 based 0.009
mixture 0.016 decision 0.012 em 0.012 view 0.008
gaussian 0.013 nearest 0.011 density 0.011 system 0.008
Topic: 52 - Gradient algorithm Topic: 53 - Bayesian/Monte Carlo Topic: 58 - Protein structure Topic: 63 - Network
Words Probability Words Probability Words Probability Words Probability
function 0.024 bayesian 0.028 protein 0.023 network 0.050
algorithm 0.019 gaussian 0.025 chain 0.021 units 0.031
learning 0.017 prior 0.022 region 0.016 input 0.030
gradient 0.013 posterior 0.019 structure 0.015 learning 0.025
vector 0.012 distribution 0.016 mouse 0.014 output 0.023
convergence 0.010 evidence 0.014 proteins 0.014 training 0.023
problem 0.009 monte 0.012 human 0.013 hidden 0.023
linear 0.009 carlo 0.012 sequences 0.012 networks 0.022
case 0.008 mackay 0.009 prediction 0.010 unit 0.019
algorithms 0.008 noise 0.009 sequence 0.010 layer 0.019
Topic: 64 - Speech recognition Topic: 78 Topic: 86 - PCA Topic: 87 - Statistical mechanics
Words Probability Words Probability Words Probability Words Probability
speech 0.039 data 0.022 matrix 0.038 energy 0.036
recognition 0.029 set 0.019 pca 0.028 boltzmann 0.028
word 0.023 number 0.013 linear 0.026 temperature 0.020
system 0.018 figure 0.012 principal 0.025 annealing 0.017
training 0.015 results 0.012 analysis 0.017 units 0.013
hmm 0.013 model 0.012 component 0.017 state 0.012
speaker 0.012 neural 0.011 components 0.015 field 0.011
context 0.011 learning 0.010 covariance 0.013 machine 0.011
network 0.009 function 0.009 eigenvectors 0.012 probability 0.008
neural 0.008 training 0.009 subspace 0.011 signature 0.008
using the probability threshold is 0.01 (1/T ). The probabilities
of each topic conditioned on a given author can suggest the
ranking of author interests, which can be used as a similarity
measure between authors. Conditioned on these topics, the
words that were most frequently generated are illustrated in
Table III.
Table II and Table III illustrate examples of author interests
and topic distributions discovered by the model on the NIPS
data. To begin with, Topic 58 is related to Bioinformatics while
Topic 64 is pertain to speech recognition, which characterize
Cole R from Agin P. From these results, we can conclude
that Cole R is interested in applying classification techniques
(Topic 11) for speech recognition, and Agin P focuses also on
machine learning (Topic 78) but applies to Bioinformatics. We
can also see that the other three authors, whose interests lie on
the core of machine learning techniques, have many sub areas
of interest in common, each of which has a set of different
dominant words that can be manually annotated (Table III).
Similar to the AT model [9], there were approximately 30%
of all topics that are generic such as Topic 78. This could
be dominated by the nature of the NIPS corpus. These topics
may be used as the representative topics to compare across
different types of corpora in order to gain more insights into
the characteristics of each database.
It is problematic to make a direct evaluation of the inferred
contributions ψd based on the order of authorship appearance
in documents. In other words, it is not necessary that the
preceding authors appearing in the document contributes on
writing more than the authors coming later in the order. For
example, the first author sometimes conducts the experiments
but the one who wrote the paper is the last author.
In order to validate our assumption of the author con-
tributions we therefore created a pseudo document by com-
bining six single-author documents - one document from
Abu-Mostafa Y (0.17%), two documents from Anastasio T
1 2 1 1 3 1 1 1 3 2 1 1 4 1 1 1 4 2 1 15 655 675 68
5 695 6:
; < = > ? @ A BC DEF GHI JFH D
EKGDLI LIHMH F N ; O < P Q ? R = S T S U V; W S R = S R X ? U Y
Z X W R [ \ @ U ]
(a)
1 2 1 1 3 1 1 1 3 2 1 1 4 1 1 1 4 2 1 15 6555 65̂5 675
5 67̂5 6855 68̂
; < = > ? @ A BC DEF GHI JFH D
EKGDLI LIHMH F N _ ? ` a U b Q c ? ` R W \ R R U d
] S W e S @ S c S W U ;
(b)
1 2 1 1 3 1 1 1 3 2 1 1 4 1 1 1 4 2 1 15 65555 65575 65
585 65595 655:
; < = > ? @ A BC DEF GHI JFH D
EKGDLI LIHMH F N _ ? ` a U b Q c ? ` R W \ R R U d] S W e S @ S c S W U ;
(b)
Fig. 4: Example of a predictive distribution resulted from running the learned SAT model given the pseudo document that
contains six single-author documents - one document from Abu-Mostafa Y (0.17%), two documents from Anastasio T (0.33%),
and three documents from Linsker R (0.50%) for validating author contributions (a). Examples of document-specific distributions
of author contributions in the authorship prediction task resulted from the SAT model, which uses a Dirichlet distribution (b)
and the AT model, which uses a uniform distribution (c), given an anonymous article in NIPS 1993. Red dots indicate the true
authors of the document.
(0.33%), and three documents from Linsker R (0.50%), and
tested how much the SAT model can describe the contributions
of those authors. Figure 4 (a) shows that our model not only
predicts those writers from 2865 candidates correctly, but also
ranks the contributions precisely.
More importantly, the inferred contribution probabilities
modeled by a Dirichlet distribution prove their benefit to
the authorship prediction. Figure 4 (b-c) exemplifies that the
characteristic of the uniform assumption in the AT model
fails to perform the authorship prediction based on its author
assignment in Equation (5) using the fold-in Gibbs sampler. In
contrast, the Dirichlet distribution modeled by the SAT model
can distinguish the true authors from 2865 candidate authors
given an anonymous documents.
C. Evaluation for authorship attribution
In order to evaluate the predictive power of authorship
attribution, we performed a random forest technique, a discrim-
inative classifier [14], as a baseline approach. Authorship pre-
diction is a multi-class and multi-label classification problem,
where the number of classes is equal to the number of authors
in the analysis and a document can have multiple authors [6].
In the random forest framework, the vector of word counts are
regarded as a feature vector directly. The final class assignment
is resulted from the average of all trees in the forest. We do
not need to wrap up the feature selection module during the
learning stage since it is a built-in unit in the algorithm to
generate randomized trees.
We applied both of the learned models including the ran-
dom forests with a single randomized tree and five trees to the
test dataset. Given an anonymous document, a set of candidate
authors, and a learned model, the authorship attribution is to
identify a subset of the authors who are the most likely to write
the document. We divided the experiments into two scenarios:
authorship prediction on documents written by single authors
and by multiple authors. Having obtained a Markov chain from
the predictive Gibbs sampler of the SAT model and the AT
model and the predicted probabilities from the random forest
classifiers, we ranked the author contributions ψ̃d according
to their probabilities in a descending order for each document
d̃ in the test set. The higher contributions an author has, the
more likely the author wrote the document. We calculated true
positive rate (TPR) and false positive rate (FPR) at each rank
as shown in Equation (7) and Equation (8) respectively:
TPRk =
TPk
A
(7)
FPRk =
D × k − TPk
D ×A
(8)
where TPk is the number of true positives in the top-k ranks
and other notations are described in Table I.
As a result, the receiver operating characteristic (ROC)
curves of each method for both experiment scenarios as shown
in Figure 5 (top) as well as the area under the ROC curve
(AUC) in Table IV. Furthermore, for the single-author test
documents, the SAT model shows the best performance, while
the random forest with five trees comes for the first place
TABLE IV: Area under the receiver operator curve (AUC) for
authorship prediction.
No. of authors SAT AT RF: single tree RF: 5 trees
Single 0.76 0.71 0.57 0.71
Multiple 0.67 0.62 0.55 0.71
f g f f g h f g i f g j f g k l g fm nmm no
m npm nqm nr
s nm
t u v w x y z w { | { } x ~ u | x
                                       f g f f g h f g i f g j f g k l g fm nmm no
m npm nqm nr
s nm
t u v w x y z w { | { } x ~ u | x
                                      
mom
pmqm
 u  
    ¡ l l f f ¢ f f l f f f £ ¤ ¢ ¥ ¦                                 
(a) Single-author documents
mom
pmqm
 u  
    ¡ l l f f ¢ f f l f f f £ ¤ ¢ ¥ ¦                                 
(b) Multiple-author documents
Fig. 5: ROC curves of authorship prediction (top) and the percentage of true positives as a function of top-k ranks (bottom) in
single-author documents (a), multiple-author documents (b).
for those documents written by multiple authors. Even though
the predictive power of random forests is high, even higher
when growing more trees, the main disadvantage is that they
cannot provide information discovery i.e. author interests and
underlying topics. Noticeably, of all test documents, AUC
scores of the SAT model were significantly greater than the
AT model for both scenarios (p-value < 0.05).
Indeed, we are interested in the inferred contributions at
the top-K ranks for author identification, regardless the order
at the lower ends of the probability vector. In this case, we
therefore emphasize the percentage of true positives at the
top 35% of the rank as shown in Figure 5 (bottom). The
SAT model is superior to the AT model for both scenarios.
Importantly, the SAT model identified true authors at the first
rank approximately 25%, whereas the AT model achieved with
very tiny percentage (6%). This difference also occurs in the
multiple-author case.
IV. CONCLUSION
We have presented the Supervised Author-Topic (SAT)
model in order to infer topic-specific word distributions, author
interests and author contributions. We also derived the poste-
rior of all model parameters and implemented the collapsed
Gibbs sampling for inference on the model parameters and for
authorship attribution to anonymous documents.
The SAT model can be viewed as a generalized model of
the AT model, a successful unsupervised learning technique
for analyzing documents and author interests based on topic
representations. Since the AT model uses a uniform distribution
to assign an author to a word, this implies that the model
holds the assumption of equal contributions among the authors
on a document. As a result, it has limited performance on
authorship prediction in a supervised learning setting. We relax
this restriction by imposing an additional Dirichlet distribution
as a prior on author contributions. The underlying assumption
is that different authors contribute to a document with different
degrees. Furthermore, the AT model can be considered a
special case when the parameter of the Dirichlet prior on
contributions in the SAT model is set to one.
With regard to author prediction, we define the contribution
of an author as the number of words in the given document
assigned to the author. Given an anonymous document, the
SAT allocates each author with the equal contribution. Having
observed more words with the learned SAT model, the Gibbs
sampler updates the contribution probability according to
word-author assignments. Such probability distribution across
all authors at the invariant stage can indicate the likeliness of
those who wrote the document. Consequently, the SAT model
provides an explicit framework for authorship attribution to
anonymous documents with knowledge discovery of topics and
author interests.
Integrated with a scoring system, the inferred contributions
may be used for measuring the impact of publication articles
of scholars. Moreover, executing the SAT model on single
sections e.g. introduction, methodology, and conclusion sep-
arately may reveal the dynamic of writing within the research
articles. Performing the SAT model with respect to the level
of the authors, e.g. full professor, assistant professor, and PhD
student at the time of submission would also provide insights
into the relationships between the academic hierarchy and the
writing contribution. In addition, this research could be in
theory used with sentiment analysis in the message content
in the facebook/twitter of a person with suicide intention [15],
[16]. This would introduce the ability to detect the degrees
contributions of the suspects.
The SAT model can also be applied on any other
kind of dyadic data that have the same relationship as
authors and words. For example, analyzing clinical data
annotated by physicians and nurses such as prescriptions
and diagnostic texts for individual patients using the SAT
model can establish the system of “patient like me”
(http://www.patientslikeme.com). This helps patients compare
treatments, symptoms and experiences with people like them
and take control of their health. In addition, this system allows
tracking their health by characterizing them into the groups
like them over time and also contributes to research that can
advance medicine for all.
ACKNOWLEDGMENT
We thank the anonymous reviewers for their useful sug-
gestions for improving the paper. NP acknowledges the Royal
Thai Government Scholarship.
REFERENCES
[1] M. Koppel, J. Schler, and S. Argamon, “Computational methods in
authorship attribution,” Journal of the American Society for Information
Science and Technology, vol. 60, no. 1, pp. 9–26, 2009.
[2] D. M. Blei, A. Y. Ng, and M. I. Jordan, “Latent dirichlet allocation,”
Journal of Machine Learning Research, vol. 3, pp. 993–1022, 2003.
[3] C. Wang and D. M. Blei, “Collaborative topic modeling for recom-
mending scientific articles,” in Proceedings of the 17th ACM SIGKDD
international conference on Knowledge discovery and data mining.
ACM, 2011, pp. 448–456.
[4] D. Agarwal and B.-C. Chen, “flda: matrix factorization through latent
dirichlet allocation,” in Proceedings of the third ACM international
conference on Web search and data mining. ACM, 2010, pp. 91–100.
[5] P. Hennig, D. Stern, R. Herbrich, and T. Graepel, “Kernel topic models,”
arXiv preprint arXiv:1110.4713, 2011.
[6] T. N. Rubin, A. Chambers, P. Smyth, and M. Steyvers, “Statistical topic
models for multi-label document classification,” Machine Learning,
vol. 88, no. 1-2, pp. 157–208, 2012.
[7] J. D. Mcauliffe and D. M. Blei, “Supervised topic models,” in Advances
in neural information processing systems, 2008, pp. 121–128.
[8] A. McCallum, “Multi-label text classification with a mixture model
trained by em,” in AAAI99 Workshop on Text Learning, 1999, pp. 1–7.
[9] M. Rosen-Zvi, T. Griffiths, M. Steyvers, and P. Smyth, “The author-
topic model for authors and documents,” in Proceedings of the 20th
Conference on Uncertainty in Artificial Intelligence. AUAI Press, 2004,
pp. 487–494.
[10] Y. Seroussi, I. Zukerman, and F. Bohnert, “Authorship attribution with
latent dirichlet allocation,” in Proceedings of the Fifteenth Conference
on Computational Natural Language Learning. Association for
Computational Linguistics, 2011, pp. 181–189.
[11] Y. Seroussi, F. Bohnert, and I. Zukerman, “Authorship attribution with
author-aware topic models,” in Proceedings of the 50th Annual Meeting
of the Association for Computational Linguistics. Association for
Computational Linguistics, 2012, pp. 264–269.
[12] T. L. Griffiths and M. Steyvers, “Finding scientific topics,” Proceedings
of the National academy of Sciences of the United States of America,
vol. 101, no. Suppl 1, pp. 5228–5235, 2004.
[13] A. Globerson, G. Chechik, F. Pereira, and N. Tishby, “Euclidean
Embedding of Co-occurrence Data,” The Journal of Machine Learning
Research, vol. 8, pp. 2265–2295, 2007.
[14] L. Breiman, “Random forests,” Machine learning, vol. 45, no. 1, pp.
5–32, 2001.
[15] B. Pang and L. Lee, “Opinion mining and sentiment analysis,” Foun-
dations and trends in information retrieval, vol. 2, no. 1-2, pp. 1–135,
2008.
[16] C. Lin and Y. He, “Joint sentiment/topic model for sentiment analysis,”
in Proceedings of the 18th ACM conference on Information and
knowledge management. ACM, 2009, pp. 375–384.
