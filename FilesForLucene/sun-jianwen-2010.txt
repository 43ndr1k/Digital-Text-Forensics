Feature Selection for Online Writeprint Identification Using Hybrid Genetic 
Algorithm 
 
Jianwen Sun, Zongkai Yang, Pei Wang, Lin Liu, Sanya Liu* 
National Engineering Research Center for E-learning 
Huazhong Normal University 
Wuhan 430079, China 
e-mail: lsy5918@mail.ccnu.edu.cn
 
 
Abstract—One major task of online writeprint identification is 
to select the key features for representing the writeprint and 
facilitating the classifier built by using only the selected feature 
subset. In this study, we develop a hybrid genetic algorithm: 
RelieF Fed Genetic Algorithm (RFGA) which incorporates 
feature weight information produced by using RelieF as the 
heuristic to identity the key features and improve the 
identification performance. Experiments are conducted on a 
test bed encompassing hundreds of reviews posted by 20 
Amazon customers to examine the method. The experimental 
results using RFGA show the proposed approach is effective, 
obtaining a significant improvement in performance, with 
satisfactory classification accuracy of 96.67%, and having a 
heavy reduction in feature dimensionality that is only 3% of 
the no feature selection baseline. 
Keywords-online writeprint identification; feature selection; 
hybrid genetic algorithm 
I. INTRODUCTION 
The Internet’s numerous benefits have always been 
coupled with drawbacks attributable to the abuses of online 
anonymity. It is necessary to counter anonymity abuses and 
strengthen the social accountability in cyberspace. Writeprint 
identification is a technique to identify individuals based on 
textual identity cues people leave behind text-based media. 
Researchers have begun to use online writeprint analysis 
techniques as a forensic identification tool, with recent 
application to email and online forum. These techniques can 
also be used to analyze on student's class papers for 
plagiarism detection. 
Similar to human fingerprint, writeprint is composed of 
multiple features such as frequency of word, vocabulary 
richness, use of punctuation, length of sentence, structural 
information, etc. These features can represent an author’s 
unique, immutable writing style and further become the basis 
of writeprint analysis. Previous studies proposed taxonomies 
of features under different labels and criteria [1] [2] [3]. The 
feature set used in writeprint analysis studies often includes 
multiple feature types. In addition, some feature types, such 
as character n-grams, can considerably increase the 
dimensionality of the feature set. In such cases, feature 
selection techniques need to be applied to reduce the 
dimensionality of the representation. Then they can 
potentially improve classification accuracy and also provide 
greater insight into important writeprint features.  
In this paper we focus on the problem of feature selection 
for online writeprint identification. Character n-gram is 
utilized as the feature type here, which has been proven to be 
quite useful to quantify the writing style and can capture 
higher level nuances according to previous research [4] [5]. 
Based on character n-gram feature type, we developed a 
hybrid genetic algorithm for feature selection to improve 
classification accuracy and narrow in on a key feature subset 
of writeprint discriminators. Experiments were conducted to 
evaluate the effectiveness of the proposed method in 
comparison with several other classical feature selection 
techniques. 
The rest of the paper is organized as follows. Section 2 
presents a general review of feature selection technology and 
its application to online writeprint identification. Section 3 
describes the proposed RFGA algorithm. Section 4 presents 
experiments and results discussion. The conclusion and 
future work are given in section 5. 
II. RALATED WORK 
Feature selection has found success in many applications 
such as text categorization. Filter and wrapper are the two 
major feature selection techniques. The wrapper is 
computationally expensive for data with a large number of 
features. To balance the tradeoff between result optimality 
and computational efficiency, a wide range of heuristic or 
stochastic search strategies have been employed to generate 
candidate feature subsets for evaluation. As one of the most 
popular search techniques, genetic algorithm (GA) was often 
adopted to search the optimal feature subset [6] [7].  
Frequency is the most important criterion for selecting 
features in the area of writeprint analysis. Houvardas and 
Stamatatos proposed an approach for extracting character n-
grams of variable length using frequency information only 
[5]. The result of comparison with information gain showed 
that the frequency based feature set was more accurate for 
feature sets comprising up to 4,000 features. Instability is 
another important criterion for feature selection in writeprint 
identification [8]. Given a certain number of variations of the 
same text, all with the same meaning, the features that 
remain practically unchanged in all texts are considered 
stable. Experiments showed the results were much better 
2010 International Symposium on Computational Intelligence and Design
978-0-7695-4198-3/10 $26.00 © 2010 IEEE
DOI 10.1109/ISCID.2010.28
867
when the frequency and the instability criteria were 
combined. 
Previous work has mostly focused on the individual 
evaluation for writeprint identification which can not remove 
redundant features efficiently. Moreover, certain features that 
seem irrelevant when examined independently may be useful 
while jointly with others. However, there has been limited 
emphasis on feature subset evaluation for writeprint 
identification that is more capable of avoiding these 
problems. Li, et al. used GA to reduce an initial set of 270 
features to an optimal subset for the specific training corpus 
comprising 134 features and improved the classification 
performance [9]. But the feature space introduced in this 
study is huge and most existing heuristic search strategies 
like simple GA for feature subset selection can not 
efficiently handle high dimensional feature spaces. Hence, 
we intend to develop a hybrid GA by incorporating valuable 
heuristics to address the aforementioned issues which often 
occur in feature selection of writeprint identification. More 
details are described in the following section. 
III. HYBRID GA FOR FEATURE SELECTION OF 
ONLINE WRITEPRINT IDENTIFICATION: RFGA 
Inspired by most previous hybrid GA, the hybridizations 
of RFGA include initial population seeding, crossover, and 
mutation modification. As shown in Fig.1, the candidate 
features are first ranked by using RelieF, and then the feature 
weight information is incorporated as the heuristic to feed 
the GA’s initial population, crossover, and mutation 
operators. A more detailed description of major steps of 
RFGA is presented below. 
A. Feature Ranking 
RelieF (RF) is employed in this step to produce the 
heuristic for RFGA by ranking the features in the full set. RF 
 
is a feature weighting algorithm which attempts to 
approximate the following difference of probabilities for 
the weight xW of a feature X  [10]. 
 
class) same of instancenearest  |X of t valueP(differen-       
 class)different  of instancenearest  |X of t valueP(differen=xW  (1) 
B. Chromosome Decoding 
The chromosome is decoded using a binary string with 
the length equal to the number of a candidate feature subset. 
Each gene in a chromosome may have value “0” or “1” 
which indicates whether a feature is selected or not.  
C. Initial Population 
The feature weight information derived from the first 
step is used to feed the generation of initial population here. 
The chromosomes in RFGA are randomly generated by 
using only the top 500 genes according to the ranked feature 
list produced by RF to form the initial population. It means 
that all the genes of the chromosomes in the initial 
population always have value “0” except the first 500 ones. 
D. Fitness Evaluation 
The fitness function for evaluation of a chromosome 
combines two criteria: the number of selected features and 
the accuracy of the classification. The 2-criteria fitness 
formula is given below. 
 )
)1(
)(exp()1()||exp()(
0
0
A
AxA
n
xxFitness
β
βαα
−
−×−+−×=  (2) 
Here, )(xFitness  is the fitness of the feature subset 
denoted by x . || x  indicates the size of x  and n  is the size 
of all the candidate features. )(xA  represents the 
classification accuracy by using the subset x , and 0A  
represents the accuracy by using the full set. α  and β  are 
two constants specified according to the problem. α  is used 
to control the contributes to the fitness function from the two 
parts. β  is used to set the threshold of the classification 
accuracy by using x  (i.e. 0Aβ ). When 0)( AxA β<  the 
exponential function penalizes heavily, which means if the 
classification accuracy by using x  is not compatible with β  
of the accuracy by using the whole set, the value of the 
second part will drop sharply and result in x  not selected. 
E. Genetic Operators 
1) Selection 
To maintain solution diversity and prevent premature 
convergence, the generational replacement strategy is used 
here that the whole population is replaced in every 
generation. Parents are selected probabilistically with better 
ones having a higher selection chance. Stochastic universal 
sampling is employed to perform selection in this study. 
2) Crossover 
Figure 1.  Illustration of the RFGA process 
877
For simplicity, single point crossover is adopted in 
RFGA. The crossover point is not randomly determined as a 
traditional GA but determined by incorporating the feature 
weight information derived from the first step to maximize 
the difference in cumulative weight information across the 
two swapped substrings. This method is designed to keep the 
population diversity by ensuring the heavier concentrations 
of features with higher weight and those with lower weight. 
Solutions with higher weight consist of features that have 
more discriminatory potential while those with lower weight 
help maintain the diversity balance that is important to avoid 
local maxima. 
3) Mutation 
The mutation operator in RFGA is hybridized to favor 
features with higher weight in order to decrease the chance 
of selecting features with lower weight. This is done by 
factor the feature weight information produced by using RF 
into the mutation probability. In detail, the probability of a 
gene to mutate from “0” to “1” based on the feature’s weight 
while the probability to mutate from “1” to “0” is set to the 
value one minus the feature’s weight. 
By applying these genetic operators in the successive 
generations iteratively, the output of RFGA will achieve the 
steady fitness value. The feature subset with the highest 
classification accuracy among all the generations is regarded 
as the optimum. 
IV. EXPERIMENTAL EVALUATION 
A. Test bed 
The top 20 customer’s online reviews from Amazon's 
Top Customer Reviewers were collected as our test bed, of 
which the main characteristics are listed in TABLE I. 
B. Experimental design 
Two major steps involved in this experiment were as 
follows: extracting an initial set of writeprint features (i.e. 
character n-grams) and performing the proposed feature 
selection method. 
The purpose of the first step was to pre-select an initial set 
only including character n-grams that were considered to be 
important. Intuitively, all the character n-grams extracted 
from the test bed with possible length from 1 to n-1 should 
be included in the search space as the candidate initial set. 
But this set was usually extremely huge so that some shallow 
selection criterions need to be incorporated to reduce the 
feature space for character n-grams. According to previous 
research, frequency was a common criterion to select the 
valuable writeprint features. Therefore, we used frequency as 
the measure to rank all the n-grams and then filtered the 
trivial ones. In addition, we used only up to 5-grams as these 
higher level n-grams tended to be redundant. In this way  
TABLE I.  CHARACTERISTICS OF THE TEST BED 
No. of 
authors 
Average no. of 
messages per 
author 
Average length of 
message Time span 
20 30 1,383 characters 1 year 
18,167 character n-grams (including all the 91 1-gram, 3,076 
2-grams, the 5,000 most frequent 3-grams, the 5,000 most 
frequent 4-grams, and the 5,000 most frequent 5-grams) 
were extracted to compose the initial set for performing the 
feature selection in the next step. 
In the second step we applied RFGA on the initial feature 
set extracted in the previous step and compared it with no 
feature selection (as Baseline) and other three typical feature 
selection methods including simple genetic algorithm (SGA), 
RF, and Information Gain (IG). We used the standard 10-
fold cross-validation to validate these techniques. For each 
fold of 10, feature selection was performed on the 540 
training samples, while the remaining 60 were used to 
evaluate the classification accuracy for that particular fold. 
For HGA and SGA, they were run using SVM with 5-fold 
cross-validation on the training samples for each fold. For IG 
and RF, we applied them to training samples for feature 
ranking and selection in each fold. The features selected by 
all techniques were then used on testing samples to evaluate 
the accuracy for that fold. The overall accuracy and number 
of selected features were computed as the average value 
across all 10 folds. 
The classifier we employed in this study was realized by 
Support Vector Machine (SVM) which outperformed other 
classifiers according to previous studies [1] [2]. We used 
LIBLINEAR algorithm [11] included in WEKA [12]. In all 
experiments, linear kernels were used with C=1.  
GA was run for 1000 iterations, with a population size of 
50 for each generation, using a crossover probability of 0.7 
and a mutation probability of 0.03. The α  and β  in fitness 
function were set to 0.2 and 0.9 respectively. For RF, the 
number of nearest neighbors was used with k=10. 
C. Result discussion 
Fig.2 and Fig.3 demonstrate the change of accuracy and 
the number of selected features across 1000 generations 
using RFGA compared with SGA. As shown in Fig.2, RFGA 
outperformed SGA from start to end along the 1000 
generations. This could be attributable to the RF feeding 
operation in the initial population of RFGA. Moreover, 
RFGA obtained a more concise set of key features than SGA 
as Fig.3 illustrated. 
TABLE II shows the best performance of the baseline 
and all the four feature selection methods adopted in this 
study. All of the four techniques significantly outperformed 
the baseline. RFGA obtained the best performance, resulting 
in a 6% improvement over the no feature selection baseline, 
4% over IG, 2% over RF, and 3% over SGA. And the most 
important point was that RFGA heavily reduced the feature 
dimensionality, identifying an extremely concise set of key 
features that was 6%-11% of RF and IG, and just 3% of the 
baseline. 
TABLE III illustrates the performance of other methods 
by using the same number of features as RFGA. For IG and 
RF, the top 575 features were selected according to their 
ranking information. For baseline, the 575 most frequent 
features were used to compare with RFGA. As shown in 
TABLE III, RFGA obtained a great performance 
improvement, resulting in a 22% improvement over the no 
887
feature selection baseline and a 12% improvement over RF 
and IG. This indicated the feature set selected by RFGA will 
be richer in different character n-grams corresponding to 
different kind of stylometric information when the same 
amount of character n-grams was selected. 
V. CONCLUSIONS AND FUTURE WORK 
In this paper we proposed a hybrid genetic algorithm: 
RelieF Fed Genetic Algorithm (RFGA) for feature selection 
of online writeprint identification which incorporated the 
feature weight information produced by using RelieF. 
 
 
 
TABLE II.  COMPARISON OF THE BEST PERFORMANCE 
Methods 
Performance 
Mean Accuracy Std.Dev No. of Features 
Base 90.17% 0.024 18,167 
IG 92.96% 0.029 5,000 
RF 94.50% 0.015 9,000 
SGA 93.67% 0.016 645 
RFGA 96.67% 0.015 575 
TABLE III.  COMPARISON OF THE PERFORMANCE BY USING THE SAME 
NUMBER OF FEATURES AS RFGA 
Methods 
Performance 
Mean Accuracy Std.Dev No. of Features 
Base 74.67% 0.057 575 
IG 84.99% 0.035 575 
RF 84.83% 0.029 575 
RFGA 96.67% 0.015 575 
Improving performance and identifying a key feature subset 
were the two major purposes of RFGA. The experimental 
results on the test bed using RFGA indicated high 
performance levels, with accuracy to 96.67%, and had a 
heavy reduction in feature dimensionality which was only 
3% of the no feature selection baseline. 
In the future we would continue to improve the RFGA 
based feature selection technology for dealing with larger 
solution spaces (a common scenario for this kind of 
problem). We believe this technology could improve the 
performance and scalability of online writeprint 
identification. We also intend to explore the effectiveness of 
other forms of GA hybridization for feature selection. 
ACKNOWLEDGMENT 
This work was supported by National High-tech R&D 
Program of China (863 Program) (No.2008AA01Z131) and 
self-determined research funds of CCNU from the 
colleges’basic research and operation of MOE 
(No.CCNU09A02006). We are also grateful for the research 
assistance provided by Zhi Liu and Yuying Wei. 
REFERENCES 
[1] Zheng, R., Li, J., Chen, H., and Huang, Z., “A framework for 
authorship identification of online messages: Writing style features 
and classification techniques,” Journal of the American Society of 
Information Science and Technology, 57(3), 378-393, 2006. 
[2] Abbasi, A., and Chen, H., "Applying authorship analysis to extremist-
group web forum messages," IEEE Intelligent Systems, 20(5), pp. 67-
75, 2005. 
[3] Stamatatos, E., "A survey of modern authorship attribution methods," 
Journal of the American Society of Information Science and 
Technology, 60(3), 538–556, 2009. 
[4] Keselj, V., Peng, F., Cercone, N., and Thomas, C., "N-gram-based 
author profiles for authorship attribution," In Proceedings of the 
Pacific Association for Computational Linguistics, pp. 255-264, 
2003. 
[5] Houvardas, J., and Stamatatos E., “N-gram feature selection for 
authorship identification,” In Proceedings of the 12th International 
Conference on Artificial Intelligence: Methodology, Systems, 
Applications, pp. 77-86, Springer, 2006. 
[6] Yang, J.H., and Honavar, V., “Feature subset selection using a 
genetic algorithm,” IEEE Intell. Systems 13 (2), 44–49, 1998. 
[7] IS Oh, JS Lee, and BR Moon, "Hybrid genetic algorithm for feature 
selection," IEEE Trans. Pattern Anal. Mach.Intell., vol. 26, no. 11, pp. 
1424–1437, Nov. 2004. 
[8] Koppel, M., Akiva, N., and Dagan, I, "Feature instability as a 
criterion for selecting potential style markers," Journal of the 
American Society for Information Science and Technology, 
57(11),1519–1525, 2006. 
[9] Li, J., Zheng, R., and Chen, H., “From fingerprint to writeprint,” 
Communications of the ACM, 49(4), 76–82, 2006. 
[10] I. Kononenko, "Estimating attributes: Analysis and extensions of 
relief," In Proceedings of the European Conference on Machine 
Learning, 1994. 
[11] R.E. Fan, K.W. Chang, C.J. Hsieh, X.R. Wang, and C.J. Lin, 
"LIBLINEAR: A library for large linear classication," The Journal of 
Machine Learning Research, 9:1871–1874, 2008. 
[12] Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer, Peter 
Reutemann, and Ian H. Witten, “The WEKA data mining software: 
An update,” SIGKDD Explorations, 11(1), 2009. 
Figure 2.  The change of accuracy using RFGA compared with SGA 
Figure 3.  The change of number of selected features using RFGA 
compared with SGA 
897
