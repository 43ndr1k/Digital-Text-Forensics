Application of Cascading Rough Set-Based Classifiers on Authorship Attribution 
 
Oguz Aslantürk, Ebru A. Sezer, Hayri Sever 
Department of Computer Engineering 
Hacettepe University 
Ankara, TURKEY 
{oguz.aslanturk, esezer, sever}@cs.hacettepe.edu.tr 
Vijay Raghavan 
Center for Advanced Computer Studies 
University of Louisiana 
Lafayette, LA, USA 
vijayraghavan@gmail.com
 
 
Abstract—Author attribution aims to detect the author of given 
text by using various features coming from texts and authors. 
In this study, rough set-based classifiers are employed in a 
cascading style for achieving author attribution. The aim of 
cascading is to produce homogeneous text subsets from a 
heterogeneous corpus and to apply author attribution onto the 
smaller sets. Experiments to discover the effects of cascading 
on author attribution are carried out and promising results are 
obtained. 
Keywords—authorship attribution; rough sets; cascading 
I.  INTRODUCTION  
Text categorization deals with assigning a document to 
one or more categories based on its contents. Stylometry is 
one of the problems in text categorization and can be defined 
as the statistical analysis of literary style [1]. Three specific 
and distinct problems of stylometry are authorship attribution 
(or author identification), similarity detection, and author 
characterization. 
Authorship attribution is the task of determining the 
author of a given text basing on the features coming from 
both of the text and the author. It is useful when more than 
one people claims to be the author of the same text or no one 
is willing to accept or able to identify the authorship of the 
text. Similarity detection is used to assess the degree of 
similarity between two or more texts, without necessarily 
determining the authors. Most studies in this category are 
related to plagiarism detection. Finally, author 
characterization aims at determining the sociolinguistic 
profile of the author of a given text.  Koppel, Argamon, and 
Shimoni state that author characteristics such as age, gender, 
educational cultural background, etc., constitute this profile 
[2]. 
In this study, the benefits of application of classical 
Rough Set-based classifiers to authorship attribution of 
Turkish texts is studied, and classification performance of 
the proposed cascaded rough set-based classifier, with two 
levels, is compared  to that of a stand-alone, rough set-based 
classifier.  
The rest of the paper is organized as follows: in section 
II, brief background information about authorship attribution 
is given, in section III, the tool used for the experiments is 
introduced after briefly mentioning the advantages of rough 
set-based classification. Explanations about the proposed 
structure, corpus and the experiments are given in section IV 
and finally, conclusions are drawn in section V. 
II. AUTHORSHIP ATTRIBUTION 
The idea of quantifying the writing style of a text goes 
back to 19th century. After Augustus de Morgan suggested 
that the lengths of the words in a text might be a 
distinguishing characteristic of an author in 1851, 
Mendenhall published results of his study on plays of 
Shakespeare in 1887 [3]. Zipf in 1932 [4], Yule in 1938 [5] 
and 1944 [6] followed Mendenhall by applying statistical 
methods to authorship attribution. Mosteller and Wallace 
initiated non-traditional authorship attribution, employing 
computers instead of traditionally employing human experts, 
with their study [7]. Non-traditional authorship attribution 
studies continued using statistical methods including 
Principal Component Analysis, Factor Analysis, 
Discriminant Analysis, Cluster Analysis and Cumulative 
Sum (CUSUM or QSUM). A detailed history of statistical 
methods and the large set of different stylometric features 
used on authorship attribution can be seen in various studies 
[6, 8, 9, 10]. 
Machine learning techniques were also employed in 
authorship attribution.  Radial Basis Function (RBF) 
networks [11], Genetic Algorithms (GA) [12], Multi Layer 
Perceptron (MLP) [13], Naïve Bayesian (NB) models [14], 
Decision Trees (DT) [15], k-Nearest Neighbour (kNN) 
classification [16],  Support Vector Machines (SVM) [17], 
Markov chains [18], Self-Organizing Maps (SOM) [19] and 
hybrid methods blending machine learning techniques [20] 
are all used for authorship attribution. 
Since Zdzislaw Pawlak has developed Rough Set theory 
in the early 1980s [21], it has been widely used in both 
feature selection and classification. It is no surprise to find 
applications of rough set-based methods on authorship 
attribution because of the two basic steps of the problem: 
feature selection and classification. Staczyk and Cyran 
employed elements of rough set theory to stylometric 
analysis of texts in 2007 [22]. Stanczyk studied optimised 
rough set-based classifier and dominance-based rough set 
approach on authorship attribution in 2008 [23] and 2009 
[24], respectively. Stanczyk also used rough set approach to 
select features for an Artificial Neural Network (ANN) 
classifier in 2009 [25]. Another hybrid algorithm using rough 
sets was proposed by Miao, Duan, Zhang, and Jiao in 2009 
[26].  
Features used in authorship attribution studies are usually 
classified into five categories: lexical, structural, syntactic, 
2010 IEEE International Conference on Granular Computing
978-0-7695-4161-7/10 $26.00 © 2010 IEEE
DOI 10.1109/GrC.2010.110
656
content-specific and idiosyncratic. Lexical features are 
composed of word or character-based statistical measures 
such as total number of letters, words, sentences, average 
number of words per sentence, average number of characters 
per word, etc... Structural features are about the general 
layout of the text like the number of paragraphs or headers, 
type, size, or color of fonts, number of hyperlinks, or images 
in the text, etc... Syntactic features include function words, 
punctuation, n-grams, types of words, etc.. Content-specific 
features are comprised of important keywords and phrases 
with higher importance or with specific relevance to some 
domain. Idiosyncratic features include misspellings, 
grammatical mistakes, and other usage anomalies. A detailed 
history of features used in authorship attribution is given in 
Holmes [8], Juola [9] and Stamatatos [10]. 
III. ROUGH SETS 
Rough Set Theory, proposed by Pawlak in 1982 [21], is a 
powerful mathematical approach to handle the vagueness 
and uncertainty [27]. Detailed information about Rough Set 
Theory can be found in the studies [21, 27, 28, 29]. 
Rough set deals with classification of quantitative and 
qualitative data in a supervised learning environment. It 
provides efficient algorithms for finding hidden patterns in 
data and these patterns are usually represented as a set of 
decision rules [27]. It needs no preliminary or additional 
information to generate the decision rules from data, but the 
data itself. As stated by Pawlak [27], “the main advantage of 
rough set theory is that it does not need any preliminary or 
additional information about data -i.e., like probability in 
statistics, grade of membership, or the value of possibility in 
fuzzy set theory”. 
Classification process using rough sets consists of mainly 
four phases: discretization of training and test sets, reduction 
of training set, rule generation using reducts and 
classification on test set. 
In this study Rosetta is used for rough set based 
classification. In fact, Rosetta is a toolkit for data analysis 
designed and implemented by Aleksander Øhrn for his 
doctoral dissertation [30] and it is downloadable from 
http://www.lcb.uu.se/tools/rosetta/. It is a tool designed to 
analyze tabular data using rough sets and supports 
importing/exporting data from/to different sources including 
databases and Excel files, browsing and preprocessing the 
data, finding reducts, generating rules, filtering the data 
and/or reducts, and classification of the data. 
IV. CASCADING ROUGH SET-BASED CLASSIFIERS AND 
EXPERIMENTS 
In this section, the usage of rough set-based classifiers in 
cascading style is explained, experiments investigating the 
effect of cascading are designed and their results are 
discussed. 
A. Corpus 
Some specific features of the natural laguage used by an 
author may be important for authorship attribution. As a 
result, although there are some common text collections as 
“Federalist Papers” which were studied by different 
researchers [7, 13], it is not uncommon to prepare a new 
corpus for a language other than English. For example, 
Greek newspapers were used in [32, 33] while Polish novels 
were used in [23, 24]. 
In this study, an authorship attribution system for Turkish 
authors was targeted, so the corpus was prepared using 
articles of Turkish daily newspapers’ authors.  
The corpus of the experiments consists of texts 
downloaded from web sites of 4 different daily Turkish 
newspapers. Total 513 texts were collected and all of them 
were published in the second half of year 2008. Totally, 9 
different authors and 2 different genres, such as politics and 
life, were used for corpus production. 5 of the authors write 
about politics, while the other 4 author write about life. 57 
texts of each author are separated into two distinct sets of 45 
and 12 texts, for training and testing, respectively. 
B. Proposed structure of the classifier 
The purpose of the proposed structure for this study is to 
test an example implementation of the idea that classification 
may perform better on small homogenous sets than larger 
heterogenous sets. Although different models and 
applications of this model may be thought, genre was 
selected as a basic feature to determine small homogenous 
sets by classification of a large heterogenous set according to 
genre. 
The proposed structure of the classifier is shown in Fig.1. 
It is a cascaded rough set-based classifier that has two levels. 
In the first level, the goal is to classify texts based on their 
genres and, in the second level classification, authorship 
attribution is performed for the result sets that are outputs of 
the first level.  The purpose of this type filtering is to divide 
one big heterogeneous text set into small homogenous text 
sets and to try author attribution in these sets. 
 
 
 
Figure 1. Proposed structure of the classifier. 
 
 
657
As can be seen in Fig. 1, the classification implemented 
in the first level (genre classification) has three possible 
outputs: “texts about politics", "texts about life" and "texts 
with unknown genre".  The second step is to classify each 
result set with the specific classifier trained for authors 
writing about that genre. Any text with unknown genre -in 
other words, texts which cannot be classified in the first 
level-  should be classified by the classifier trained without 
genre information.  
 For this purpose, four different classifiers were trained 
through experiments 1 to 4. These classifiers were trained for 
classification of genre, authors writing about politics, authors 
writing about life, and authors without the genre information, 
respectively. Details of the experiments are given in section 
D. 
C. Features 
In total of 34 features including numbers of letters, 
capitals, words, sentences, punctuation marks, word types, 
paragraphs, abbreviations, distinct words, alien words, slangs 
and Ottoman Turkish-based words are used in this work. In 
order to identify the features, Zemberek (is currently 
available at Internet address 
http://code.google.com/p/zemberek/), a Turkish NLP library, 
is used within the program we developed with Java 
programming language. A list of features selected for this 
study is given in Table I. 
Because the values of features were continuous, they had 
to be discretized in order to be applied to rough set 
methodology. Though discretization is an important part of 
machine learning applications, it is not focused in the 
proposed structure. This is the reason for the usage of simple 
discretization method: median of counts for each feature was 
determined independently from other features and each 
median value was used as a threshold for the selected 
feature. As a result, smaller and greater values than the 
threshold were considered 0 and 1, respectively. Threshold 
values were calculated on training sets and applied on 
discretization of both train and test sets. 
 
TABLE I.  SELECTED FEATURES 
 
Elements of text Punctuation marks Types of words 
# of letters # of periods # of nouns 
# of capitals # of commas # of adjectives 
# of words # of question marks # of verbs 
# of sentences # of exclamation 
marks # of adverbs  
# of passive 
sentences # of ellipsis # of counts 
# of inverted 
sentences 
# of slashes # of reflection words 
# of hyphens # of question words 
# of paragraps # of brackets # of time words 
 # of apostrophes # of infinitives 
 # of quotes # of proper nouns 
 # of semicolons # of distinct words 
 # of colons # of foreign words  
  # of Ottoman Turkish-
based words 
  # of slangs 
  # of abbreviations 
D. Experiments 
In the present study five experiments were performed and 
each experiment has the following steps: reducing data, 
generating classification rules and classifying test data by 
using the generated rules. These steps are shown in Fig. 2. 
Rosetta v1.4.41 was employed to execute all of these steps. 
Although it includes implementations of different reduction 
algorithms, only one of them was selected and used with 
default options. This algorithm is named 
SAVGeneticReducer and it is an implementation of a genetic 
algorithm described by Vinterbo and Øhrn [31]. After 
generating if – then rules based on reducts, Standard voting 
algorithm with its default options was employed for 
classification. 
1) Experiment 1 – author classifier without genre 
information: Experiment 1 was classification of the whole 
corpus without using genre information. 513 texts of 9 
authors including texts about both politics and life divided 
into two distinct sets for training and testing. Training set 
including 45 texts for each author (totally, 9 x 45 = 405 
texts) was used for reduction and rule generation after 
discretization. Test set including 12 texts for each author 
(totally, 9 x 12 = 108 texts) was classified using the rules 
generated with the training set. The results of experiment-1 
are given in Table II. The classifier trained in this 
experiment was the third classifier of the second level of the 
proposed structure. 
2) Experiment 2 – author classifier (politics): 
Experiment 2 was training a classifier for authors writing 
about politics. The training set was formed with 45 texts per 
5 different authors writing about politics, and the test set 
was formed with 12 texts from each of these authors. The 
results of the experiment are listed in Table III. 
 
 
 
Figure 2. Steps of experiments. 
 
658
TABLE II.  RESULTS OF EXPERIMENT 1 
 
Result # of texts (108 in total) % 
Correct 60 55,56 
Incorrect 7 6,48 
Unclassified 41 37,96 
 
TABLE III.  RESULTS OF EXPERIMENT 2 
 
Result # of texts (60 in total) % 
Correct 50 83,33 
Incorrect 7 11,67 
Unclassified 3 5 
 
3) Experiment 3 – author classifier (life): Experiment 3 
was training a classifier for authors writing about life. The 
training set was formed with 45 texts per 4 different authors 
writing about life, and the test set was formed with 12 texts 
from each of these authors. The results of the experiment 
can be seen in Table IV. 
4) Experiment 4 – genre classifier: Experiment 4 was 
training a classifier for genre classification. The corpus was 
separated into a training set that consisted 45 texts per 9 
different authors, and a test set that consisted 12 texts for 
each author. The results of the experiment are given in 
Table V. 
5) Experiment 5 – cascaded classifier: Experiment 5 
was the main point of this study. Previous experiments had 
two purposes: to clarify and compare the success ratios of 
classifiers trained with little variations of data, and to train 
the necessary classifiers for the last experiment. In figure-1, 
it is clear that four different classifiers were needed; a genre 
classifier (trained in Experiment 4) at level 1, and three 
author classifiers for politics (Experiment 2), life 
(Experiment 3) and without genre information (Experiment 
1) at level 2. 
Since the results of Experiment 5 were subject to a 
comparison with Experiment 1’s, the same test set in 
Experiment 1, 9 different authors and 12 texts per each (108 
texts in total), were used in this experiment. 
Experiment 5 was performed in two steps. First step was 
classifying texts according to their genres and the results are 
in Table VI. 
The second step of the experiment includes three 
classifications. The test sets of these classifiers were 
prepared using the results of level-1 classification. Results of 
level-2 classification can be seen in Table VII. 
 
TABLE IV.  RESULTS OF EXPERIMENT 3 
 
Result # of texts (48 in total) % 
Correct 39 81,25 
Incorrect 5 10,42 
Unclassified 4 8,33 
 
TABLE V.  RESULTS OF EXPERIMENT 4 
 
Result # of texts (108 in total) % 
Correct 80 74,07 
Incorrect 9 8,33 
Unclassified 19 17,60 
TABLE VI.  RESULTS OF EXPERIMENT 5, LEVEL 1 
 
Result # of texts (108 in total) 
Politics 45 
Life 44 
Unclassified 19 
 
TABLE VII.  RESULTS OF EXPERIMENT 5, LEVEL 2 
 
Results Politics (45 texts) 
Life 
(44 texts) 
Unclassified (19 
texts) 
Correct 39 31 6 
Incorrect 6 8 0 
Unclassified 0 5 13 
 
E. Discussion 
By comparing results of Experiment 1 with results of 
Experiment 2 and Experiment 3, it is obvious that 
classification of authors writing on the same genre performed 
better than classification which mixes authors writing on 
different genres. This comparison is given in Table VIII. 
In addition, comparing results of Experiment 1 with 
Experiment 4, it is seen that genre classification performed 
better than author classification with same data sets. This 
comparison is given Table IX. 
Considering these observations, it is excepted to achieve 
higher success ratios with a cascaded classifier. A 
comparison of results of Experiment 1 and Experiment 5 is 
given in Table X. 
In Table X, it is clear that cascading classifiers increases 
the classification success by about 15%. There are still texts 
which cannot be correctly specified. This is most probably 
due to insufficient precision of selected features. 
 
TABLE VIII.  COMPARISON OF EXPERIMENTS 1, 2, AND 3 
 
Results Exp 1 Exp 2 Exp 3 
Correct 55,56 83,33 81,25 
Incorrect 6,48 11,67 10,42 
Unclassified 37,96 5 8,33 
 
 
TABLE IX.  COMPARISON OF EXPERIMENTS 1 AND 4 
 
Results Exp1 (author cla.) Exp4 (genre cla.) 
Correct 55,56 74,07 
Incorrect 6,48 8,33 
Unclassified 37,96 17,60 
 
 
TABLE X.  COMPARISON OF EXPERIMENTS 1 AND 5 
 
Results 
Experiment 1 Experiment 5 
# of texts 
(Tot.:108) % 
# of texts 
(Tot.: 108) % 
Correct 60 55,56 76 70,37 
Incorrect 7 6,48 14 12,96 
Unclassified 41 37,96 18 16,67 
 
659
V. CONCLUSION 
In this study, author attribution is focused and cascaded 
rough sets are employed to increase ratio of success rates. As 
can be seen from the results of the experiments, the proposed 
cascading approach improves classification success by about 
15%. These results are encouraging for future works on the 
selection of features and the usage of hybrid methods for 
author attribution.  
REFERENCES 
[1] D.I. Holmes, “The Evolution of stylometry in humanities 
scholarship”, Literary and Linguistic Computing, Association for 
Literary & linguistic Computing, 1998, pp. 111-117. 
[2] M. Koppel, S. Argamon, and A.R. Shimoni, “Automatically 
categorizing written texts by author gender”, Literary and Linguistic 
Computing, Association for Literary & linguistic Computing, 2002, 
401-412. 
[3] T. C. Mendenhall, “The characteristic curves of composition”, 
Science, 1887, pp. 237-49. 
[4] G.K. Zipf, Selected studies of the principle of relative frequency in 
language, Harvard University Press, Cambridge, MA., USA, 1932. 
[5] G.U. Yule, “On sentence-length as a statistical characteristic of style 
in prose, with application to two cases of disputed authorship”, 
Biometrika, 1938, pp. 363-390. 
[6] G.U. Yule, The statistical study of literary vocabulary, Cambridge 
University Press, 1944. 
[7] F. Mosteller, D.L. Wallace, Inference and disputed authorship: The 
Federalist, Addison-Wesley, MA., USA, 1964. 
[8] D.I. Holmes, “Authorship attribution”, Computers and the 
Humanities, Springer, Netherlands, 1994, pp. 87-106. 
[9] P. Juola, “Authorship attribution”, Foundations and Trends in 
Information Retrieval, Now Publishers Inc., MA., USA, 2006, pp. 
233-334. 
[10] E. Stamatatos, “A survey of modern authorship attribution methods”, 
Journal of American Society for Information Science & Technology, 
John Wiley & Sons, NY, USA, 2009, pp. 538-556. 
[11] D. Lowe and R. Matthews, “Shakespeare vs. Fletcher: A stylo metric 
analysis by radial basis functions”, Computers and the Humanities, 
Springer, 1995, pp. 449–461. 
[12] D.I. Holmes, and R.S. Forsyth, “The Federalist revisited: New 
directions in authorship attribution”, Literary and Linguistic 
Computing, Association for Literary & linguistic Computing, 1995, 
pp. 111-127. 
[13] F.J. Tweedie, S. Singh, and D.I. Holmes, “Neural network 
applications in stylometry: The Federalist papers”, Computers and the 
Humanities, Springer, 1996, pp. 1-10. 
[14] A. McCallum and K. Nigam, “A comparison of event models for 
naive Bayes text classification”, in Proceedings of the AAAI-98 
Workshop on Learning for Text Categorization, Citeseer, 1998, pp. 
41-48. 
[15] C. Apte, F. Damereau, and S.Weiss, “Text mining with decision rules 
and decision trees”, in Proceedings of the Conference on Automated 
Learning and Discovery, Workshop 6: Learning from Text and the 
Web, 1998. 
[16] W. Lam and C. Ho, “Using a generalized instance set for automatic 
text categorization”, in Proceedings of the 21st annual 
international ACM SIGIR conference on Research and 
development in information retrieval  (SIGIR ’98), ACM, NY, 
USA, 1998, pp. 81–89. 
[17] J. Diederich, J. Kindermann, E. Leopold, and G. Paass, “Authorship 
attribution with Support Vector Machines”,  Applied Intelligence, 
Kluwer Academic Publishers, Netherlands, 2000, 109-123. 
[18] D.V. Khmelev and F.J. Tweedie, “Using Markov chains for 
identification of writers”, Literary and Linguistic Computing, 
Association for Literary & linguistic Computing, 2001, 299-307. 
[19] G. Tambouratzis, N. Hairetakis, S. Markantonatou, and G. 
Carayannis, “Applying the SOM model to text classification 
according to register and stylistic content”, International Journal of 
Neural Systems, 2003, pp. 1-11,  
[20] P. Sallis and S. Shanmuganathan, “A blended text mining method for 
authorship authentication analysis”, In Proceedings of the Second 
Asia International Conference on Modelling & Simulation (AMS), 
IEEE Computer Society, DC, USA, 2008, pp. 451-456. 
[21] Z. Pawlak, “Rough sets”, International Journal of Computer and 
Information Science, Springer, 1982, pp. 341-356. 
[22] U. Stanczyk and K.A. Cyran, “On employing elements of rough set 
theory to stylometric analysis of literary texts”, International Journal 
on Applied Mathematics and Informatics, 2007, pp. 159-166. 
[23] U. Stanczyk, “On construction of optimised rough set-based 
classifier”, International Journal of Mathematical Models and 
Methods in Applied Sciences, 2008, pp. 533-542. 
[24] U. Stanczyk, “Dominance-based rough set approach employed in 
search of authorial invariants”, Computer Recognition Systems 3, 
AISC 57, Springer-Verlag, Berlin, Germany, 2009, pp. 293-301. 
[25] U. Stanczyk, “Relative reduct-based selection of features for ANN 
classifier”, Man-Machine Interactions, AISC 59, Springer-Verlag, 
Berlin, Germany, 2009, pp. 335-344. 
[26] D. Miao, Q. Duan, H. Zhang, N. Jiao, “Rough set based hybrid 
algorithm for text classification”, Expert Systems with Applications, 
Elsevier, 2009, pp. 9168-9174. 
[27] Z. Pawlak, “Rough set theory and its applications to data analysis”, 
Cybernetics and Systems, Taylor and Francis Ltd, 1998, pp. 661-688 
[28] J. Komorowski, Z. Pawlak, L. Polkowski, A. Skowron, “Rough sets: 
a tutorial”, in Pal S.K., Skowron A. (eds.) Rough Fuzzy Hybridization. 
A new trend in decision making, Springer-Verlag, Singapore, 1999, 
pp. 3-98. 
[29] Z. Pawlak and A. Skowron, “Rudiments of rough sets”, Information 
Sciences, Elsevier, 2007, pp. 3-27. 
[30] A. Øhrn,  “Discernibility and rough sets in medicine: tools and 
applications”, PhD thesis, Norwegian University of Science and 
Technology, 1999. 
[31] S. Vinterbo and A. Øhrn, “Minimal approximate hitting sets and rule 
templates”, International Journal of Approximate Reasoning, 2000, 
pp. 123-143. 
[32] E. Stamatatos, N. Fakotakis, G. Kokkinakis, “Automatic text 
categorization in terms of genre and author”, Computational 
Linguistics, 2000, pp. 471-495. 
[33] E. Stamatatos, N. Fakotakis, G. Kokkinakis, “Computer-based 
authorship attribution without lexical measures”, Computers and the 
Humanities, 2001, pp. 193-214. 
 
 
 
660
