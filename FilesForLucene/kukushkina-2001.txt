Problems of Information Transmission, Vol. 37, No. 2, 2001, pp. 172–184. Translated from Problemy Peredachi Informatsii, No. 2, 2001, pp. 96–109.
Original Russian Text Copyright c© 2001 by Kukushkina, Polikarpov, Khmelev.
SOURCE CODING
Using Literal and Grammatical Statistics
for Authorship Attribution
O. V. Kukushkina, A. A. Polikarpov, and D. V. Khmelev
Received August 8, 2000; in final form, January 11, 2001
Abstract—Markov chains are used as a formal mathematical model for sequences of elements
of a text. This model is applied for authorship attribution of texts. As elements of a text, we
consider sequences of letters or sequences of grammatical classes of words. It turns out that
the frequencies of occurrences of letter pairs and pairs of grammatical classes in a Russian text
are rather stable characteristics of an author and, apparently, they could be used in disputed
authorship attribution. A comparison of results for various modifications of the method using
both letters and grammatical classes is given. Experimental research involves 385 texts of 82
writers. In the Appendix, the research of D.V. Khmelev is described, where data compression
algorithms are applied to authorship attribution.
1. INTRODUCTION
In this paper, the problem of identification of the author of a text is stated as follows. Let large
fragments of prose works by a number of authors be given. These texts are in Russian or in another
phonological (nonhieroglyphic) language1. An anonymous text known to belong to one of these
authors is disputed between all of them. One has to determine the actual author. Based on results
of testing the technique suggested by D.V. Khmelev in [1], we state that this can be done with
high enough probability. This technique is based on statistics of occurrences of pairs of successive
elements in the text (letters, morphemes, etc).
To the best of our knowledge, first attempts in the search for a technique for authorship attribu-
tion were made in [2]. Markov [3] almost immediately replied to [2], which shows that the founder
of the theory of Markov chains was quite interested in this field. Note also that the first application
of “events linked to a chain” was described by Markov in [4], where he studied the distribution of
vowels and consonants among initial 20 000 letters of “Evgenii Onegin.”
Modern methods of authorship attribution in Russia are reviewed in [5, Chapter 1] and a nice
review of foreign works is given in [6]. Despite the enormous variety of methods described, none of
them has ever been applied to a large number of texts. The reason is that usually these methods
cannot be automated and require human interposal, which makes computational analysis of a large
number of large texts practically impossible. Hence, the question of generality of each of these
methods arises: Can any of them be used outside the situation it was devised for?
Until recently, the only exception was [7], where the chosen technique was applied to a large
enough number of texts. In this paper, the rate of function words used by an author was examined.
It was found that this rate is stable for each author among a large number of Russian writers of the
eighteenth–twentieth centuries. This technique was applied in [7] to the problem of determining
plagiarism.
1 We consider phonological writing only because hieroglyphic writing reduces possibilities for the analysis
of pair associations since phonological information (which distinguishes morphemes and words) is hidden
by conventional hieroglyphic representation of these units.
0032-9460/01/3702-0172 $25.00 c© 2001 MAIK “Nauka/Interperiodica”
USING LITERAL AND GRAMMATICAL STATISTICS 173
A new method of authorship attribution for natural-language texts (independent of a particular
language considered) was first suggested by Khmelev in [1].
The new technique is based on a formal Markov model for sequences of letters (or any other
elements) of a text, i.e., a sequence of letters is considered as a Markov chain where each letter
(element) depends on the preceding letter (element) only. Matrices of transition frequencies of pairs
of elements (letters, grammatical codes, etc.) are computed over all texts by each of the authors.
Therefore we (approximately) know the probabilities of transitions from one letter to another for
each author. The actual author of an anonymous text is computed using the principle of maximal
likelihood; i.e., for each matrix, we compute the probability of the anonymous text and choose the
author with the maximal probability, who is assumed to be the actual author.
This method is surprisingly precise as is shown in [1], where this method was applied to a large
number of various texts. The more surprising is that nice results were obtained by taking into
account only frequencies of letter pairs.
Markov models of various orders were used in a large number of works (see [8]) in 1950s–
1960s for the estimation of entropies of various kinds of texts. But none of these works put the
question of application of Markov chains in the framework of authorship attribution. Moreover,
the accepted viewpoint was that characteristics of any piece of literature measured via frequencies
of letters and letter pairs are close to the average characteristics for the language and therefore
are practically indistinguishable from each other (see [8, p. 181] and [3, 9]). The principle of
maximal likelihood has either never been applied in authorship attribution because of the following
reasons. Firstly, computations with a large amount of data were difficult before computers, together
with a great number of electronic texts, appeared. Secondly, a kind of a psychological barrier
existed in the absence of an underlying theory since a first-order Markov chain is the first and
very rough approximation for a natural-language text. This fact is pointed out in many papers
concerning the estimation of text entropy via high-order Markov chains [8, p. 187]. Finally, in the
problem of authorship attribution it was believed that there should exist a set of stable quantitative
characteristics for grammatical information, which are useful in distinguishing the writers. To the
best of our knowledge, the only significant result in this direction for the Russian language was
recently obtained in [7].
In the present paper, we further develop a validation procedure for the method from [1] and
apply this method to different units of the analysis:
(a) Letter pairs in their natural sequences in a text, i.e., in words (as they appeared in the text)
and spaces between them;
(b) Letter pairs in sequences of letters in the vocabulary forms of words. For example, the
previous sentence is reduced as follows: “letter pair in sequence of letter in vocabulary form of
word.” For Russian texts, this reduction is much more significant since Russian words have a large
number of various forms;
(c) Pairs of most general (“incomplete”) grammatical classes of words in their sequences in
sentences of the text. In Russian, 14 parts of speech are traditionally discerned: nouns, verbs,
adjectives, etc. We also introduce 4 conventional categories: “end of sentence,” “abbreviation,”
“unclear class,” and “dash symbol.” The category “unclear class” was introduced since gram-
matical classes were assigned automatically (covering more than 99% of words) but some words
(for example, misprinted) were not construed automatically and thus their grammatical classes
remained unclear;
(d) Pairs of less general (“complete”) grammatical classes of words, for example, animate noun,
inanimate noun, qualitative adjective, relative adjective, possessive adjective, etc.
Full cross-validation of the method was carried out. It involved 385 texts of 82 writers. Results
are presented in Tables 1 and 2. The list of authors with sizes of texts and the number of texts for
PROBLEMS OF INFORMATION TRANSMISSION Vol. 37 No. 2 2001
174 KUKUSHKINA et al.
each author is given in Table 3.
As a measure of precision of the method, one can consider the percentage of correctly classified
texts. The best results were obtained in cases (a) and (b) (73% and 62% respectively). In case (c),
61% of texts were correctly classified. Results for case (d) are much worse (4%).
In Section 2, principles and results of preprocessing are described. In Section 3, the procedure
of cross-validation is described. A detailed description of results is given in Section 4. Conclusions
are made in Section 5. In the Appendix written by D.V. Khmelev, another approach to authorship
attribution is presented. This approach uses data compression algorithms.
2. PREPROCESSING
After preprocessing, the source corpus of texts was presented in all forms, (a)–(d).
In case (a), all words of unclear grammatical class (i.e., those unrecognized automatically and
hence with unknown vocabulary form) were ignored (this is one of the differences between the
present research and [1]). This was done in order to be able to compare the results obtained
in case (a) with those in case (b). The text was converted into a sequence of words and space
characters, all punctuation was removed, and, finally, words beginning with capitals were also
omitted (including the first words of sentences). As was shown in [1], this trick significantly increases
the precision of authorship attribution. Apparently, this improvement is due to ignoring personal
names, which are usually not correlated with the author’s style. The letter “ë” was identified with
“e” and hence we had 33 characters, including the space symbol. Each character was encoded with
its number: 1 corresponded to letter “a” and 32 corresponded to “ya.” The space character was
encoded by 0.
The total number of letters in all texts was 96 209 964. The total number of different pairs of
letters found in the corpus of texts was 1011 (out of possible 33 × 33 = 1089). Clearly, 1011 is a
bit larger than the actual number of different letter pairs that can be found in Russian texts. This
is a result of misprints in electronic versions of books and it can lead to errors in computations.
In order to obtain an estimate for this noise, 121 letter pairs were selected whose occurrence in a
Russian text is unlikely. In total, they were used 38 495 times, or 0.04% of the total size of the
corpus, which is quite negligible.
Preprocessing of the source corpus of texts in cases (b), (c), and (d) was carried out with the
help of an automatic classifier developed by O.V. Kukushkina at the Laboratory for General and
Computational Lexicology and Lexicography of the Philological faculty at Moscow State Univer-
sity. The classifier is based on the grammar vocabulary of Zaliznyak [10] and academic grammars
[11,12].
Case (c) is based on information obtained from the most general grammatical class of a word, i.e.,
information concerning the part of speech (particle, preposition, interjection, copulative and adver-
sative conjunction, other conjunctions, verb, pronoun, adverb, adjective, noun, numeral, predicate
noun, comparative, and modal adverbs).
Case (d) is based on information obtained from lexical and grammatical category of a given part
of speech (animate noun, inanimate noun, etc.).
Let us present some statistics for cases (b)–(d).
In case (b), the total number of letters is 110 704 464. In vocabulary forms, there were 1029
different pairs of letters out of 33 × 33 = 1089. As one can see, this is greater than in case (a).
This effect is due to conversion of oblique forms to vocabulary ones.
In cases (c) and (d), the number of elements was 20 262 449. In case (c), the total number of
different pairs of elements was 302 out of possible 18×18 = 324. In case (d), the number of different
pairs of elements was 8124 out of possible 112× 112 = 12 544.
PROBLEMS OF INFORMATION TRANSMISSION Vol. 37 No. 2 2001
USING LITERAL AND GRAMMATICAL STATISTICS 175
3. TECHNIQUE AND ITS CROSS-VALIDATION
The analysis of texts in each case was carried out using software devised by Khmelev. For
each case, we present the results of cross-validation of the method of [1]. The cross-validation was
performed as follows.
Recall that elements of the texts are encoded with numbers from 0 to 32 (in cases (a) and (b)),
17, or 111 (in cases (c) and (d) respectively). Code 0 always corresponds to a delimiter between
large blocks: in cases (a) and (b), it corresponds to the space between words; in cases (c) and (d),
to the delimiter between sentences (“end of sentence”).
Given W writers, each of whom has Nw texts, where w = 0, . . . , W −1, we compute Qwnij , which
is the number of transitions from letter i to letter j in text n (n = 0, . . . , Nw − 1) by writer w
(w = 0, . . . , W − 1). In order to find the predicted author for text n̂ (by a known author, ŵ) using
information about authorship of the other texts for all authors including ŵ, we compute
Qkij =
Nw−1∑
n=0
Qknij , Q
k
i =
∑
j
Qkij
for authors k 6= ŵ; for author ŵ, we exclude text n̂ from the training set:
Qŵij =
∑
n 6=n̂
Qŵnij , Q
ŵ
i =
∑
j
Qŵij .
Then we compute
Λk(ŵ, n̂) = −
∑
i:Qki>0
∑
j:Qkij>0
Qŵn̂ij ln
Qkij
Qki
and
Λŵ(ŵ, n̂) = −
∑
i:Qŵi >0
∑
j:Qŵij>0
Qŵn̂ij ln
Qŵij
Qŵi
.
Ignoring degenerate cases Qkij = 0 and Q
k
i = 0, one can see that each Λk(ŵ, n̂) is minus the
logarithm of the probability that text n̂ is written by writer ŵ provided that it is generated as a
Markov chain with transition probabilities P kij = Q
k
ij/Q
k
i . We may ignore the degenerate summands
according to results on the optimal maximal likelihood estimate presented in [13, p. 224].
We also define the rank Rk(ŵ, n̂) to be the rank of Λk(ŵ, n̂) in {Λk(ŵ, n̂), k = 0, . . . ,W − 1},
where the smallest rank is 0, i.e., Rk(ŵ, n̂) ∈ {0, . . . ,W − 1}, and the smallest number has the
smallest rank. If the text is assigned to the correct author, then Rŵ(ŵ, n̂) = 0.
If the text is assigned to another author and the actual author is the second among other
pretenders, then Rŵ(ŵ, n̂) = 1, etc.
As a result of cross-validation, we get a set of ranks
{Rŵ(ŵ, n̂)}ŵ∈{0,...,W−1}, n̂∈{0,...,N
ŵ
−1}.
The precision of the technique for authorship attribution is measured by this set of ranks. The
rate of correct predictions coincides with the fraction of zero ranks. Results close to correct are
PROBLEMS OF INFORMATION TRANSMISSION Vol. 37 No. 2 2001
176 KUKUSHKINA et al.
characterized by the fraction of small ranks. Another measure of the precision of prediction is given
by the average rank
M =
1
W−1∑
ŵ=0
Nŵ
W−1∑
ŵ=0
N
ŵ
−1∑
n̂=0
Rŵ(ŵ, n̂). (1)
We shall also present the results of cross-validation in the analysis by comparing frequencies of
individual letters (in cases (a) and (b)) and individual grammatical classes (in cases (c) and (d)).
All computations are the same but we additionally compute
Qk =
∑
i
Qki and Q
ŵ =
∑
i
Qŵi
and, instead of Λk(ŵ, n̂), use the following quantities:
Γk(ŵ, n̂) = −
∑
i:Qki>0
(∑
j
Qŵn̂ij
)
ln
Qki
Qk
and
Γŵ(ŵ, n̂) = −
∑
i:Qŵi >0
(∑
j
Qŵn̂ij
)
ln
Qŵi
Qŵ
.
4. DESCRIPTION OF THE RESULTS
The results of the research are presented in Tables 1 and 2. The column R corresponds to ranks
0, . . . , 4 and ranks not less than 5. The row with rank 0 contains the rate of correctly assigned
texts. The row with rank 1 contains the rate of texts such that the actual author was the second
among the other pretenders, etc. Finally, the row ≥ 5 contains the rate of texts such that the
actual author was at least the sixth.
The row M contains the average rank determined by (1).
Straight away we can note that the frequencies of individual letters and individual grammatical
classes give a low (but essentially nonrandom) level of correct authorship attribution (except for
the case of individual “complete” grammatical classes; reasons for this exception are to be studied).
All computations with pairs of elements (letters or grammatical classes) give better results as
compared to computations with individual letters or grammatical classes.
Let us now consider the difference between results of the analysis with the use of information
about frequencies of letter pairs in words as they appeared in the text and letter pairs in vocabulary
forms of words (see Table 1). Comparison shows that the rate of successful authorship attribution
is better in the case of natural sequences of letters in the text (there are 73% correct authorship
predictions against 62% and average rank 3.38 against 4.77). Perhaps, “equalization” of different
forms of a word reduced to the vocabulary form eliminates some useful information related to
endings of the words, which leads to a comedown in the success rate in case (b) as compared to
case (a).
Comparison of the results of authorship attribution using information about pairs of letters and
pairs of generalized (incomplete) grammatical classes (cases (a) and (c)) shows that the success
rate in both cases is quite high, namely, 73% and 61% correctly classified texts respectively. The
average rank of correct authorship attribution grows by more than 2 as we pass from letter pairs
PROBLEMS OF INFORMATION TRANSMISSION Vol. 37 No. 2 2001
USING LITERAL AND GRAMMATICAL STATISTICS 177
Table 1. Full cross-validation of authorship attribution based on the sequence of letters
Case (a) Case (b)
Words as appeared in the text Words in the vocabulary forms
R Letters pairs Individual R Letter pairs Individual
0 282/385 27/385 0 240/385 12/385
1 21/385 55/385 1 29/385 40/385
2 9/385 25/385 2 17/385 19/385
3 5/385 24/385 3 9/385 16/385
4 5/385 17/385 4 6/385 16/385
≥ 5 63/385 237/385 ≥ 5 84/385 282/385
M 3.38 12.69 M 4.77 17.88
Table 2. Full cross-validation of authorship attribution based on the sequence of grammatical classes
Case (c) Case (d)
Generalized grammatical classes “Complete” grammatical classes
R Pairs Individual R Pairs Individual
0 235/385 128/385 0 15/385 6/385
1 31/385 43/385 1 21/385 12/385
2 16/385 29/385 2 9/385 6/385
3 8/385 15/385 3 12/385 6/385
4 11/385 17/385 4 19/385 8/385
≥ 5 84/385 153/385 ≥ 5 309/385 347/385
M 5.43 10.13 M 17.76 31.93
to generalized grammatical classes. Perhaps, a relatively lower efficiency of pairs of grammatical
classes (in comparison with letter pairs for words as they appeared in the text) on the same corpus
is related to the smaller sample size (recall that the same corpus of texts contains 96 million letters
in case (a) against 20 million grammatical classes in case (c)).
Also, it requires attention that the use of individual grammatical classes in case (c) (see Table 2)
is much more effective than the use of individual letters: 33% correct authorship attribution in
case (c) against 7% in case (a). The average rank in case (c) is almost two and a half less than in
case (a). Perhaps, this is related to more specific information (which distinguishes stable structural
characteristics of texts for each author more precisely) given by individual grammatical codes in
contrast to individual letters.
“Complete” grammatical classes were the least effective in authorship attribution both in the
case of pair frequencies and the case of individual frequencies. Reasons for this are also to be
studied.
Table 3 contains information about the authors of texts analyzed, the number of texts for each
author, the dispersion of text sizes together with the minimal, average (in brackets), and maximal
sizes, results of authorship attribution in all four cases, (a)–(d), together with the minimal, average
(in brackets), and maximal rank of the text with respect to its actual author.
The corpus also contains translated texts (S. Lem, J. Chmielewska, B. Rainov). It is inter-
esting that the quality of attribution of these texts is not worse than of texts written by authors
PROBLEMS OF INFORMATION TRANSMISSION Vol. 37 No. 2 2001
178 KUKUSHKINA et al.
Table 3. The number and size of texts in the corpus, by the author
w Writer Nw
Size of texts
(thousands letters) (a) (b) (c) (d)
1 O. Avramenko 7 223.7(279.5)395.1 0(0.0)0 0(0.0)0 0(0.0)0 10(13.9)22
2 A. Bol’nykh 7 0.8(185.0)298.8 0(4.1)24 0(5.9)37 1(12.1)79 4(15.3)75
3 K. Bulychev 16 3.3(129.5)458.9 0(6.8)59 0(6.8)53 0(9.0)65 3(16.3)69
4 A. Volkov 9 5.2(186.8)610.5 0(20.4)50 0(26.3)51 0(12.3)57 5(23.1)65
5 G. Glazov 6 184.5(263.7)326.1 0(0.0)0 0(0.0)0 0(0.0)0 9(13.2)19
6 O. Grinevskii 2 96.1(127.4)158.6 0(0.0)0 0(0.0)0 0(0.0)0 0(0.5)1
7 N.V. Gogol’ 4 97.7(213.3)334.0 0(1.0)4 0(1.5)6 0(8.0)32 14(22.8)42
8 N. Gumilev 2 70.1(70.6)71.0 0(0.0)0 0(0.0)0 0(0.0)0 0(0.0)0
9 F.M. Dostoevskii 4 88.6(175.5)268.9 0(0.0)0 1(2.0)3 0(0.3)1 10(13.3)18
10 M. and S. Dyachenko 6 23.3(325.1)553.2 0(0.0)0 0(0.2)1 0(0.0)0 7(11.5)17
11 S. Esenin 2 44.6(131.5)218.4 0(0.0)0 0(0.0)0 0(0.0)0 0(0.5)1
12 A. Etoev 6 2.7(57.9)114.8 0(1.0)4 0(4.3)19 0(2.2)13 2(3.0)5
13 I. Efremov 2 256.5(396.5)536.5 0(0.0)0 0(0.0)0 0(0.0)0 1(2.0)3
14 A. Zhitinskii 3 253.6(793.2)1207.6 0(0.3)1 0(0.0)0 0(9.0)26 18(26.3)42
14 A. Kabakov 5 69.0(225.5)418.4 0(0.0)0 0(0.2)1 0(2.6)11 15(19.2)26
16 S. Kazmenko 5 132.8(400.5)1148.3 0(1.2)5 0(0.8)4 0(0.2)1 16(21.8)28
17 V. Kaplan 7 19.3(91.9)305.2 0(4.1)25 0(5.6)24 0(5.0)23 9(23.0)40
18 A. Kats 2 81.7(461.4)841.0 0(0.0)0 0(0.0)0 0(0.0)0 1(2.5)4
19 V. Klimov 4 58.5(107.5)179.9 0(7.0)15 0(7.0)20 0(1.5)6 3(6.8)9
20 E. Kozlovskii 2 848.6(868.4)888.2 0(0.0)0 0(2.0)4 15(42.0)69 40(56.5)73
21 I. Krashevskii 3 380.6(555.2)803.1 0(0.0)0 0(0.0)0 0(0.0)0 6(8.3)10
22 I. Kublitskaya 2 170.2(226.2)282.3 0(0.0)0 0(0.0)0 0(0.0)0 1(2.0)3
23 L. Kudryavtsev 4 108.3(190.5)348.2 0(0.3)1 0(1.5)5 0(0.0)0 8(13.5)24
24 V. Kunin 4 296.3(407.9)610.3 0(0.0)0 0(2.5)7 0(3.5)5 10(15.5)23
25 A. Kurkov 7 17.5(121.0)276.9 0(3.1)10 0(11.6)28 0(1.9)3 4(11.9)19
26 A. Lazarevich 4 11.3(101.3)274.7 0(14.3)47 5(20.3)54 2(11.0)18 4(9.3)15
27 A. Lazarchuk 6 141.4(434.2)786.9 0(0.0)0 0(0.8)2 0(0.0)0 19(25.5)34
28 Yu. Latynina 3 116.8(970.7)2511.8 0(3.3)10 0(13.0)36 0(0.0)0 4(15.3)23
29 S. Lem 8 11.6(238.6)535.2 0(0.9)5 0(1.1)8 0(5.1)27 11(26.5)44
30 N. Leonov 3 273.1(282.7)295.7 0(0.0)0 0(0.0)0 0(0.0)0 3(4.0)5
31 S. Loginov 14 1.3(153.4)916.2 0(15.9)36 4(18.1)37 0(18.9)49 14(35.6)59
32 E. Lukin 5 26.9(144.6)367.9 0(3.2)15 0(0.0)0 0(4.4)19 8(16.0)39
33 L. and E. Lukin 4 105.2(239.9)564.7 0(0.3)1 2(3.8)6 0(0.5)2 4(12.8)20
34 S. Luk’yanenko 15 6.0(277.6)542.9 0(3.0)22 0(6.9)76 0(9.1)58 9(25.4)73
35 N. Markina 2 93.6(179.8)266.0 0(0.0)0 0(0.0)0 1(2.5)4 0(1.5)3
36 A. Melikhov 2 457.6(536.4)615.2 0(0.0)0 0(2.5)5 0(0.0)0 17(17.5)18
37 V. Mikhailov 2 84.2(169.3)254.5 0(0.0)0 0(0.0)0 0(0.0)0 1(2.5)4
38 A. Molchanov 2 206.5(302.4)398.3 0(0.0)0 0(0.0)0 0(0.5)1 4(5.5)7
39 V. Nabokov 6 102.0(310.6)599.8 0(2.0)11 0(0.8)3 0(3.0)15 5(12.7)18
40 M. Naumova 4 5.2(161.1)337.8 0(7.8)31 0(11.8)47 0(17.8)69 4(10.0)17
41 Yu. Nesterenko 2 71.1(212.0)352.8 0(1.0)2 1(3.5)6 0(0.0)0 1(3.5)6
42 Yu. Nikitin 3 656.9(681.4)702.2 0(11.3)34 0(17.0)51 0(0.7)1 5(6.7)8
43 S. Pavlov 2 375.6(414.5)453.4 0(0.0)0 0(0.5)1 0(0.0)0 6(6.5)7
44 A.S. Pushkin 2 57.1(113.7)170.3 0(0.0)0 0(0.0)0 0(0.0)0 0(0.0)0
45 B. Rainov 5 267.7(363.6)420.3 0(0.0)0 0(0.0)0 0(0.6)3 12(13.8)15
46 L. Reznik 2 79.6(97.8)115.9 0(0.0)0 0(0.0)0 0(0.0)0 1(1.0)1
47 N. Rerikh 4 84.5(305.6)608.7 0(5.5)22 0(3.5)14 0(2.3)9 0(2.5)9
48 N. Romanetskii 7 5.5(203.2)530.6 0(5.4)21 0(7.7)20 0(1.0)4 15(27.7)45
49 A. Romashov 2 87.7(88.1)88.4 0(0.0)0 0(0.0)0 2(3.0)4 0(0.0)0
50 V. Rybakov 7 9.7(119.5)366.1 0(9.0)24 0(9.0)21 0(16.4)36 15(25.6)41
PROBLEMS OF INFORMATION TRANSMISSION Vol. 37 No. 2 2001
USING LITERAL AND GRAMMATICAL STATISTICS 179
Table 3. (Contd.)
w Writer Nw
Size of texts
(thousands letters) (a) (b) (c) (d)
51 M.E. Saltykov- 2 101.6(170.4)239.1 0(0.0)0 0(0.0)0 0(0.0)0 1(2.5)4
Shchedrin
52 R. Svetlov 3 29.2(241.0)425.4 0(0.0)0 3(13.3)20 0(0.7)2 3(8.7)17
53 A. Sviridov 4 13.4(224.0)601.5 10(27.5)65 0(11.8)41 0(11.0)44 6(26.5)56
54 V. Segal’ 3 60.5(132.0)259.7 0(0.0)0 0(0.0)0 0(0.3)1 1(4.7)8
55 K. Serafimov 2 75.3(130.8)186.4 0(0.0)0 0(0.0)0 0(0.0)0 1(1.0)1
56 I. Sergievskaya 2 50.7(79.8)108.9 0(0.0)0 0(1.0)2 0(0.0)0 0(0.5)1
57 K. Sitnikov 8 13.0(66.1)274.3 0(0.0)0 0(2.3)18 0(0.5)4 3(8.3)22
58 S. Snegov 3 385.8(411.1)438.4 0(0.0)0 0(0.0)0 0(0.0)0 5(5.0)5
59 S.M. Solov’ev 2 159.9(1251.6)2343.3 0(0.0)0 0(0.0)0 0(0.0)0 0(2.5)5
60 A. Stepanov 6 83.7(219.6)390.3 0(0.0)0 0(0.0)0 0(0.0)0 4(5.0)8
61 A. Stolyarov 2 137.2(241.9)346.7 0(5.0)10 9(11.0)13 0(7.5)15 1(2.5)4
62 A. and B. 30 37.1(230.4)579.5 0(1.9)24 0(2.5)23 0(5.9)54 15(36.4)79
Strugatskii
63 A. Strugatskii 2 51.9(101.6)151.3 0(0.0)0 0(0.0)0 0(0.5)1 1(1.0)1
64 B. Strugatskii 2 260.7(279.6)298.4 0(0.0)0 0(0.0)0 0(0.0)0 7(8.0)9
65 E. Til’man 3 307.8(390.0)464.7 0(0.0)0 0(0.0)0 0(0.0)0 11(11.3)12
66 A. Tolstoi 2 97.9(113.8)129.7 0(0.0)0 0(0.0)0 0(0.0)0 0(0.0)0
67 L.N. Tolstoi 2 199.9(712.5)1225.1 0(0.0)0 0(0.0)0 0(0.0)0 1(1.5)2
68 D. Truskinovskaya 9 82.6(235.9)478.6 0(0.8)3 0(2.7)12 0(3.8)28 13(23.8)53
69 A. Tyurin 19 1.3(222.0)832.7 0(2.3)20 0(1.2)13 0(2.6)25 24(34.0)55
70 E. Fedorov 2 221.3(667.2)1113.1 0(0.0)0 0(0.0)0 0(0.0)0 1(1.5)2
71 E. Khaetskaya 3 204.1(309.0)414.3 1(10.3)22 12(31.3)42 54(57.0)62 28(39.3)54
72 D. Kharms 3 13.9(104.1)185.5 0(0.0)0 0(0.0)0 0(12.0)29 5(16.7)30
73 V. Khlumov 4 183.3(242.9)395.5 0(3.8)15 0(11.3)38 6(15.3)38 26(34.0)46
74 J. Chmielewska 5 203.7(345.7)459.1 0(0.0)0 0(0.0)0 0(0.4)2 9(12.8)18
75 V. Chernyak 3 201.6(373.7)501.0 0(0.0)0 0(2.7)8 0(11.7)35 2(11.0)25
76 A.P. Chekhov 3 247.9(335.3)414.5 0(0.0)0 0(0.0)0 4(13.3)20 18(18.7)20
77 V. Shinkarev 3 56.2(78.9)100.1 0(11.7)29 6(13.3)22 4(23.7)61 5(5.7)6
78 V. Shukshin 2 66.7(187.7)308.8 0(0.0)0 0(0.0)0 0(0.0)0 1(2.5)4
79 S. Shcheglov 3 55.2(103.0)146.1 0(4.0)12 0(13.7)41 0(3.0)9 2(2.3)3
80 A. Shchegolev 3 105.6(318.0)561.7 0(0.0)0 0(0.0)0 0(0.0)0 12(18.7)32
81 V. Yugov 6 66.7(149.2)304.3 0(0.5)2 0(0.7)2 0(1.8)10 8(10.7)18
82 V. Yan 2 507.3(553.9)600.4 0(0.0)0 0(0.0)0 0(0.0)0 2(2.0)2
whose native language is Russian (although the corpus contains Lem’s translations by different
interpreters).
5. CONCLUSION
The main result of the research is that the use of grammatical information in authorship attri-
bution is not only reasonable but rather efficient and even comparable with the use of information
about frequencies of letter pairs (efficiency of the last method was earlier shown in [1]).
At the same time, it is still amazing that the use of such a, seemingly, simple unit as a pair
of successive letters in the text gives more precise results than the use of such categories as indi-
vidual grammatical classes and their pairs. Perhaps, letter pairs contain a kind of (converted and
incomplete) information about the morphemes of words as they appear in a text. Therefore, a
lot of information about word changing and formation is contained in the statistics of occurrences
of letter pairs, which leads to a rather high efficiency of this statistics when used for authorship
attribution.
PROBLEMS OF INFORMATION TRANSMISSION Vol. 37 No. 2 2001
180 KUKUSHKINA et al.
In other words, the frequencies of letter pairs take into account the vocabulary used by the
author and, by implication, information about preferred grammatical structures. Although the
differences in frequencies of particular pairs of letters are, most likely, insignificant since they
converge to average frequencies for the language (as was noticed by Markov [3] long ago), the
maximal “likelihood,” which takes into account the “total” effect of changing of frequencies of
pairs of letters, nevertheless provides a high precision in determining the actual author of a text.
This was earlier shown in [1] and is approved in this research by full cross-validation.
However, further experiments with grammatical classes of words with a more precise grammatical
analysis, probably, may lead to a higher success rate than achieved in this research. Perhaps,
usefulness of grammatical information is also shown in the observation that the use of information
on individual generalized grammatical classes is much more effective than the use of information
on individual letters.
Since results obtained with the use of different units (letters and generalized grammatical classes)
are consistent with each other, one may presume that elaborate methods for authorship attribution
to come will employ different representations of texts with the use of these units for mutual cross-
validation.
APPENDIX
APPLICATION OF DATA COMPRESSION ALGORITHMS
IN AUTHORSHIP ATTRIBUTION
In the Appendix, we show how one can use data compression algorithms for authorship attri-
bution. Results of cross-validation for this new method are presented. Here we analyze the same
corpus of texts as in [1] and in the main body of the paper.
The corpus in [1] contains texts of 82 authors. One randomly chosen text by each author was
held out as a test set. The other texts were used as a training sample. After this, all control
texts were classified and the correct author was determined in 69 cases. To estimate how good this
result is in terms of the probability of correct authorship attribution, consider the following mental
experiment. Assume that we have a black box such that given two texts it produces 1 if the texts
belong to the same author and 0 otherwise. Assume that the level of alpha and beta errors for
each trial is 0 < p < 1. Consider the application of the black box to the problem of authorship
attribution. Given 82 alternatives and one anonymous text, the black box should produce 81 zeros,
which correspond to the comparison of the control text to training samples of wrong authors, and
only one 1 corresponding to the comparison of the control text to the actual author. Naturally,
any other outcome is considered as misclassification. Assuming that the results of these pairwise
comparisons are independent of each other, we get that the probability of correct classification is
(1 − p)82. For the confidence level p = 0.05, we have (1− 0.05)82 ≈ 0.015, p = 0.01 corresponds to
(1 − 0.01)82 ≈ 0.439, and for p = 0.005 we obtain (1 − 0.005)82 ≈ 0.663. Note that 69/82 ≈ 0.84;
if we want to surpass the method of [1] in the quality of recognition, we should require that, say,
p = 0.001 and hence (1 − 0.001)82 ≈ 0.921. Therefore, 69 correct classifications among 82 writers
should be considered as an extremely good result. With certain modifications, this argument is as
well applicable to the results of the main text of the paper.
Here, by a text we understand a sequence of symbols of some alphabet A. Denote by |B| the
length of a text B. Let us call a concatenation of texts B and A the sequence S of length |B|+ |A|
such that the first |B| letters of S coincide with B and the last |A| letters of S coincide with A.
We shall write S = B · A.
PROBLEMS OF INFORMATION TRANSMISSION Vol. 37 No. 2 2001
USING LITERAL AND GRAMMATICAL STATISTICS 181
Now let us give an “ideal” definition of the relative complexity following the definition of the
Kolmogorov complexity (see [14, 15]): the relative complexity K(A |B) of a text A with respect
to a text B is the length of the shortest program in a binary alphabet which translates B into A.
Unfortunately, K(A |B) is not computable and it is not clear how one can use it in practice.
A rough approximation to K(A |B) (however, quite sufficient for authorship attribution as we
shall see below) can be obtained with the use of data compression algorithms. Let us define the
relative complexity C(A |B) of a text A with respect to a text B as follows. Compress B into a
text B′ and compress S = B · A to a text S′. Now put C(A |B) = |S′| − |B′|. This definition is
ambiguous since we have not fixed the data compression algorithm. In this research, we use several
algorithms which are described below.
We shall apply C(A |B) for authorship attribution. Given texts by n authors, form a test set
by holding out one control text U1, . . . , Un, by each author. All the other texts of each author are
concatenated into the texts T1, . . . , Tn.
The authorship for Ui is determined as follows. First, find the rank Ri of the number C(Ui |Ti)
in the set {C(Ui |T1), . . . , C(Ui |Tn)}. The ranks take values from 0 to n − 1. If Ri equals 0, then
the control text Ui is correctly assigned. Similarly to [1], one can introduce different measures of
precision for disputed authorship resolution. For example,
1. The simplest measure is the number of zero ranks Ri;
2. A more general measure is given by the average rank
M =
1
n
n∑
i=1
Ri.
Cross-validation of different data compression algorithms is carried out with the corpus of texts
used in [1] and in the main body of this paper. Recall that the corpus consists of 385 texts by 82
writers. The total size of the texts is about 128 million letters. The texts were preprocessed as
follows. First, all hyphenated (carried over) words were restored. Secondly, all words beginning
with capitals were deleted. The remaining words were kept in the same order as they appeared
in the text. They were delimited by the line-feed character. For each of the n = 82 authors,
a control text Ui was selected. The other texts of each author were concatenated into the texts
Ti, i = 1, . . . , 82. The size of each control text was at least 50 000–100 000 letters.
Let us consider possible lossless data compression algorithms. Presently, the following algorithms
became most popular: the Huffman coding, arithmetic coding, Burrows–Wheeler technique [16],
and many variations of the Lempel–Ziv coding [17]. Some algorithms are specially aimed on text
compression: these are PPM [18] (this algorithm uses small-order Markov models) and DMC [19]
(dynamical Markov coding). Each algorithm has a huge number of variations and parameters
(for example, there exists the so-called dynamic Huffman coding, the size of the vocabulary may
vary, etc.). Moreover, there exists a lot of “mixed” algorithms where a text obtained by the PPM
compression is additionally compressed by the Huffman coding.
All these algorithm are implemented in a large variety (over 150) of data compression programs.
Each of these programs implements some modification of some data compression algorithm. Extra
variety appears since many data compression programs have multiple versions which use different
data compression algorithms. All programs selected for this research are given in Table 4.
Most of these programs with their descriptions can be obtained from the Archiver Index2 sup-
ported by Jeff Gilchrist3. The compress program was taken from the SunOS 5.6 operation system.
The dmc program is available by ftp4. Note that the dmc program has an option of maximal memory
2 http://web.act.by.net/~act/act-index.html
3 jeffg@cips.ca
4 http://plg.uwaterloo.ca/~ftp/dmc/
PROBLEMS OF INFORMATION TRANSMISSION Vol. 37 No. 2 2001
182 KUKUSHKINA et al.
Table 4. Data compression programs
Program Author Algorithm used
1. 7zip version 2.11 Igor’ Pavlov Arithm. coding, LZ + arithm.
coding, PPM
2. arj version 2.60 RAO Inc. LZSS + Huffman
3. bsa version 1.9.3 Sergei Babichev LZ
4. bzip2 Julian Seward Burrows–Wheeler + Huffman
5. compress Sun Inc. LZW
6. dmc Gordon V. Cormack DMC
7. gzip Jean-Loup Gailly Shannon–Fano, Huffman
8. ha version 0.999c Harri Hirvola Sliding window dictionary +
Arithm. coding, Finite contex
model + arithm. coding
9. huff1 William Demas (LDS 1.1) static Huffman
10. lzari Haruhiko Okumura (LDS 1.1) LZSS + arithm. coding
11. lzss Haruhiko Okumura (LDS 1.1) LZSS
12. ppm William Teahan PPM
13. ppmd5 version F Dmitrii Shkarin PPM
14. rarw version 2.00 Eugene Roshal LZ77 variant + Huffman
15. rar version 2.70 Eugene Roshal LZ77 variant + Huffman
16. rk version 1.03α Malcolm Taylor Reduced-offset LZ, PPMZ
to use. In this research, this option was set to 100 000 000 bytes. The LDS 1.1 source kit is also
available by ftp5. The ppm program is available from the personal page of its author6.
The results of application of these programs are given in Table 5. The last row of Table 5
contains the results of application of Markov chains [1] to the same corpus of texts. Computations
for data shown in Table 5 were performed under different operation systems on different types of
computers and took about three weeks of nonstop computing.
It follows from Table 5 that data compression algorithms assign the correct author to a control
text quite often. Therefore, they are undoubtedly useful. Note that application of the rarw program
yields even better results than those obtained with the help of Markov chains in [1]. Although such
a superiority could be related to some statistical error, it is the best result achieved at present.
Perhaps an explanation for these splendid results is as follows. After the preprocessing of training
texts of an actual author, data compression algorithms in fact adapt to the control text better than
after preprocessing of texts of another author. The disadvantage of this method, as compared to
the method of Markov chains [1], is that data compression algorithms are not so “transparent” and,
in commercial software, are even not available for study. Nevertheless, many programs among those
presented in Table 4 have an open source code and well-described open algorithms. Further study
of these programs may explain the efficiency of this relative-complexity approach to authorship
attribution.
Note that a great advantage of the method of authorship attribution presented here is that it
does not require any special programs and is available on almost every computer when one chooses
among a small number of authors since most of the compression programs mentioned here are
widely spread and some of them (like gzip or rar) are implemented in all types of computers and
all operation systems.
5 ftp://garbo.uwasa.fi/pc/programming/lds 11.zip
6 http://www.cs.waikato.ac.nz/~wjt/
PROBLEMS OF INFORMATION TRANSMISSION Vol. 37 No. 2 2001
USING LITERAL AND GRAMMATICAL STATISTICS 183
Table 5. The quality of authorship attribution for data compression algorithms
Program Rank
0 1 2 3 4 ≥ 5 M
7zip 39 9 3 2 3 26 6.43
arj 46 5 2 7 2 20 5.16
bsa 44 9 3 1 1 24 5.30
bzip2 38 5 5 1 33 13.68
compress 12 1 1 3 2 63 24.37
dmc 36 4 3 4 4 31 9.81
gzip 50 4 1 2 1 24 4.55
ha 47 8 1 3 3 20 5.60
huff1 10 11 4 4 2 51 15.37
lzari 17 5 4 2 6 48 14.99
lzss 14 3 1 1 3 60 20.05
ppm 22 14 2 1 3 40 10.39
ppmd5 46 6 6 2 22 5.96
rar 58 1 1 1 21 7.22
rarw 71 3 2 1 5 1.44
rk 52 9 3 1 17 4.20
Markov chains (see [1]) 69 3 2 1 7 2.35
REFERENCES
1. Khmelev, D.V., Using Markov Chains for Authorship Attribution, Vestn. MGU, Ser. 9, Philolog., 2000,
no. 2, pp. 115–126.
2. Morozov, N.A., Linguistic Spectra: Methods for Telling Plagiary from True Creation of a Known Author.
Stylometric Study, Izv. Otd. Rus. Yazyka i Slovesnosti Imp. Akad. Nauk, 1915, vol. 20, no. 4.
3. Markov, A.A., On One Application of the Statistical Method, Izv. Imp. Akad. Nauk, Ser. 6, 1916, no. 4,
pp. 239–242.
4. Markov, A.A., An Example of Statistical Analysis of the Text of “Evgenii Onegin” Illustrating the
Linking of Events into a Chain, Izv. Imp. Akad. Nauk, Ser. 6, 1913, no. 3, pp. 153–162.
5. Ot Nestora do Fonvizina. Novye metody opredeleniya avtorstva (From Nestor to Fonvizin. New Methods
for Authorship Attribution), Moscow: Progress, 1994.
6. Holmes, D.I., The Evolution of Stylometry in Humanities Scholarship, Literary Linguist. Comput., 1997,
vol. 13, no. 3, pp. 111–117.
7. Fomenko, V.P. and Fomenko, T.G., Author Invariant for Russian Literary Texts, in Metody kolichestven-
nogo analiza tekstov narrativnykh istochnikov (Methods of Quantitative Analysis of Texts from Narrative
Sources), Moscow: Inst. Istorii SSSR, 1983, pp. 86–109.
8. Yaglom, A.M. and Yaglom, I.M., Veroyatnost’ i informatsiya, Moscow: Nauka, 1960. Translated under
the title Probability and Information, Boston: Reidel, 1983.
9. Dobrushin, R.L., Mathematical Methods in Linguistics, Matematicheskoe prosveshchenie, 1959, issue 6.
10. Zaliznyak, A.A., Grammaticheskii slovar’ russkogo yazyka (Grammatical Dictionary of the Russian Lan-
guage), Moscow: Rus. Yaz., 1977.
11. Grammatika sovremennogo russkogo literaturnogo yazyka (The Grammar of Modern Russian Literary
Language), Moscow: Nauka, 1970.
12. Russkaya grammatika (Russian Grammar), 2 vols., Moscow: Nauka, 1980.
PROBLEMS OF INFORMATION TRANSMISSION Vol. 37 No. 2 2001
184 KUKUSHKINA et al.
13. Ivchenko, G.I. and Medvedev, Yu.I., Matematicheskaya statistika (Mathematical Statistics), Moscow:
Vyssh. Shkola, 1992.
14. Li, M. and Vitányi, P., An Introduction to Kolmogorov Complexity and Its Applications, New York:
Springer, 1997.
15. Kolmogorov, A.N., Three Approaches to the Definition of the Notion of “Information Amount,” Probl.
Peredachi Inf., 1965, vol. 1, no. 1, pp. 3–11.
16. Burrows, M. and Wheeler, D.J., A Block-Sorting Lossless Data Compression Algorithm, Digital SRC
Research Report, 1994, no. 124. Available from
ftp://ftp.digital.com/pub/DEC/SRC/research-reports/SRC-124.ps.gz
17. Lempel, A. and Ziv, J., On the Complexity of Finite Sequences, IEEE Trans. Inf. Theory., 1976, vol. 22,
no. 1, pp. 75–81.
18. Cleary, J.G. and Witten, I.H., Data Compression Using Adaptive Coding and Partial String Matching,
IEEE Trans. Commun., 1984, vol. 32, no. 4, pp. 396–402.
19. Cormack, G.V. and Horspool, R.N., Data Compression Using Dynamic Markov Modelling, Comput. J.,
1987, vol. 30, no. 6, pp. 541–550.
PROBLEMS OF INFORMATION TRANSMISSION Vol. 37 No. 2 2001
