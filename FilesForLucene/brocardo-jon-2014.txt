Verifying Online User Identity using Stylometric
Analysis for Short Messages
Marcelo Luiz Brocardoa, Issa Traorea, Sherif Saada, Isaac Woungangb
a Department of Electrical and Computer Engineering, University of Victoria, Victoria, British Columbia, Canada
Email:{marcelo.brocardo, itraore, shsaad}@ece.uvic.ca
b Department of Computer Science, Ryerson University, Toronto, Ontario, Canada
Email: iwoungan@scs.ryerson.ca
Abstract—Stylometry consists of the analysis of linguis-
tic styles and writing characteristics of the authors for
identification, characterization, or verification purposes. In
this paper, we investigate authorship verification for the
purpose of user authentication process. In this setting,
authentication consists of comparing sample writing of an
individual against the model or profile associated with the
identity claimed by that individual at login time (i.e. 1-to-1
identity matching). In addition, the authentication process
must be done in a short period of time, which means
analyzing short messages. Although a significant amount
of literature has been produced showing high accuracy
rates for long documents, it is still challenging to identify
accurately authors of short unstructured documents, in
particular when dealing with large authors populations. In
this paper, we pose some steps toward achieving that goal
by proposing a supervised learning technique combined with
n-grams analysis for authorship verification for short texts.
We introduce a new n-gram metric and study several sizes of
n-grams using a relatively large dataset. The experimental
evaluation shows increased effectiveness of our approach
compared to the existing approaches published in the lite-
rature.
Index Terms—Authentication and access control; biomet-
rics systems; authorship verification; stylometry; n-gram
features; short message verification.
I. INTRODUCTION
THE writing style is an unconscious habit, whichvaries from one author to another in the way he/she
uses words and grammar to express an idea. The patterns
of vocabulary and grammar could be a reliable indicator
of the authorship. The linguistic characteristics used to
identify the author of a text is referred to as stylometry
[1], [2]. Although the writing style may change a bit with
time [3], each author has a unique stylistic tendency.
Forensic authorship analysis consists of inferring the
authorship of a document by extracting and analyzing the
writing styles or stylometric features from the document
content. Authorship analysis of physical and electronic
documents has generated a significant amount of interest
over the years and led to a rich body of research literature
[4]–[7]. Authorship analysis can be carried from three
different perspectives including authorship attribution or
identification, authorship verification, and authorship pro-
filing or characterization. Authorship attribution consists
of determining the most likely author of a target document
among a list of known individuals. Authorship verification
consists of checking whether a target document was writ-
ten or not by a specific individual. Authorship profiling or
characterization consists of determining the characteristics
(e.g. gender, age, and race) of the author of an anonymous
document.
According to Koppel et al., “verification is significantly
more difficult than basic attribution and virtually no work
has been done on it, outside the framework of plagia-
rism detection” [6]. Most previous works on authorship
verification focus on general text documents. However,
authorship verification for online documents can play a
critical role in various criminal cases such as blackmailing
and terrorist activities, to name a few. To our knowledge,
only a handful of studies have been done on authorship
verification for online documents. Authorship verification
of online documents is difficult because of their relatively
short lengths and also because these documents are quite
poorly structured or written (as opposed to literary works).
We address the above challenge by proposing a new
supervised learning technique combined with a new n-
gram analysis approach to check the identity of the author
of a short online document. We evaluate experimentally
our approach using the Enron emails dataset and compute
the following performance metrics:
• False Acceptance Rate (FAR): measures the likeli-
hood that the system may falsely recognize someone
as the genuine author of a document while they are
not;
• False Rejection Rate (FRR): measures the likelihood
that the system will fail to recognize the genuine
author of a document;
• Equal Error Rate (ERR): corresponds to the opera-
ting point where FAR and FRR have the same value.
Our evaluation yields an EER of 14.35%, which is very
encouraging considering the existing works on authorship
verification using stylometry.
The rest of the paper is structured as follows. Section
II summarizes and discusses related works. Section III
introduces our proposed approach. Section IV presents
our experimental evaluation by describing the underlying
methodology and discussing the obtained results. Section
V discusses the strengths and shortcomings of our ap-
proach and outlines the ground for future works. Section
JOURNAL OF NETWORKS, VOL. 9, NO. 12, DECEMBER 2014 3347
© 2014 ACADEMY PUBLISHER
doi:10.4304/jnw.9.12.3347-3355
VI makes some concluding remarks.
II. RELATED WORK
A large number of studies have used stylometric tech-
niques not only for authorship identification, but also for
authorship verification and authorship characterization.
Some of the previous studies on authorship identifica-
tion investigated ways to identify patterns of terrorist
communications [8], the author of a particular e-mail
for computer forensic purposes [9]–[11], as well as how
to collect digital evidence for investigations [12] or to
solve a disputed literary, historical [13], or musical au-
thorship [14]–[16]. Work on authorship characterization
has targeted primarily gender attribution [17]–[19] and
the classification of the author education level [20]. In
this section, we present related works on stylometry for
authorship attribution, characterization, and verification.
A. Authorship Attribution or Identification
Despite significant progress achieved on the identifica-
tion of an author within a small group of individuals, it
is still challenging to identify an author when the number
of candidates increases or when the sample text is short
as in the case of e-mails or online messages.
For instance, Chaski (2005) reported 95.70% accuracy
in their work on authorship identification, the evaluation
sample consisted of only 10 authors [12]. Similarly,
Iqbal et al. (2010) achieved when using k-means for
author identification, classification accuracy of 90% with
3 authors; the rate decreased to 80% when the number
of authors increased to 10 [21]. Iqbal et al. (2008)
also proposed another approach named AuthorMiner [9],
which consists of an algorithm that captures frequent
lexical, syntactical, structural and content-specific pat-
terns. The experimental evaluation used a subset of the
Enron dataset, varying from 6 to 10 authors, with 10 to
20 text samples per author. The authorship identification
accuracy decreased from 80.5% to 77% when the authors
population size increased from 6 to 10.
Hadjidj et al. (2009) used the C4.5 and SVM clas-
sifiers to determine authorship [22], and evaluated the
proposed approach using a subset of three authors from
the Enron dataset. They obtained as correct classification
rates 77% and 71% for sender identification, 73% and
69% for sender-recipient identification, and 83% and
83% for sender-cluster identification, for C4.5 and SVM,
respectively.
B. Authorship Characterization
Works on authorship characterization have targeted the
determination of various traits or characteristics of an
author such as gender and education level.
Cheng et al. (2011) investigated the author gender
identification from text by using Adaboost and SVM clas-
sifiers to analyze 29 lexical character-based features, 101
lexical word-based features, 10 syntactic, 13 structural,
and 392 functional words. Evaluation of the proposed
approach involving 108 authors from the Enron dataset
yielded classification accuracies of 73% and 82.23%, for
Adaboost and SVM, respectively [19].
Abbasi and Chen (2005) analyzed the individual cha-
racteristics of participants in an extremist group web
forum using decision tree and SVM classifiers. Experi-
mental evaluation yielded 90.1% and 97% success rates
in identifying the correct author among 5 possible indi-
viduals for decision tree and SVM, respectively [8].
Kucukyilmaz et al. (2008) used k-NN classifier to
identify the gender, age, and educational environment of
a user. Experimental evaluation involving 100 participants
grouped in gender (2 groups), age (4 groups), and edu-
cational environment (10 groups), yielded accuracies of
82.2%, 75.4% and 68.8%, respectively [23].
C. Authorship Verification
Among the few studies available on authorship verifi-
cation are works by Koppel et al. [6], Iqbal et al. [21],
Chen and Hao’s [24], and Canales et al. [5] .
Koppel et al. proposed an authorship verification
method named “unmasking” where an attempt is made
to quantify the dissimilarity between the sample docu-
ment produced by the suspect and that of other users
(i.e. imposters) [6]. The experimental evaluation of the
approach yields 95.70% of correct verification, but shows
that the proposed approach can provide trustable results
only for documents of at least 500 words long, which is
not realistic in the case of online verification.
Iqbal et al. studied email authorship verification by ex-
tracting 292 different features and analyzing these features
using different classification and regression algorithms
[21]. Experimental evaluation of the proposed approach
using the Enron e-mail corpus yielded EER ranging from
17.1% to 22.4%.
Chen and Hao’s (2011) extracted 150 stylistic features
from e-mail messages for authorship verification [24]. Ex-
perimental evaluation involving 40 authors from the Enron
dataset yielded varying classification accuracy rates based
on the number of e-mails analyzed. More specifically,
84% and 89% classification accuracy rates were obtained
for 10 and 15 short e-mails, respectively.
Canales et al. extracted keystroke dynamics and stylis-
tic features from sample exam documents for the purpose
of authenticating online test takers [5]. The extracted
features consisting of keystroke timing features and 82
stylistic features were analyzed using a K-Nearest Neigh-
bor (KNN) classifier. Experimental evaluation involving
40 students with sample document size between 1,710
to 70,300 characters yielded (FRR=20.25%, FAR=4.18%)
and (FRR= 93.46%, FRR=4.84%) when using separately
keystroke and stylometry, respectively. The combination
of both types of features yielded an EER of 30%.
In an earlier version of the current paper, presented
at the 2013 Conference on Computer, Information and
Telecommunication Systems (CITS 2013), we investi-
gated only the presence or not of a specific n-gram [25].
3348 JOURNAL OF NETWORKS, VOL. 9, NO. 12, DECEMBER 2014
© 2014 ACADEMY PUBLISHER
The current paper extends our previous work by consid-
ering all unique and non-unique n-grams, and also all n-
grams with frequency equal or higher than some number
f . We validate our approach by performing experiments
using different sets of configurations and varying the size
of n-grams from 3 to 5 characters.
III. AUTHORSHIP VERIFICATION APPROACH
In this section, we present our approach by discussing
feature selection and describing in detail our classification
model.
A. Feature Selection
Over a thousand stylistic features have already been
identified and used in the literature along with a wide
variety of analysis methods. The stylistic features can be
categorized as lexical, syntactic, semantic, and application
specific.
Lexical features are related to the words or vocabulary
of a language. Lexical analysis consists of breaking a
text into a single atomic unit of language called token.
A token can be a word or a character [26]. While earlier
studies used a set of 100 frequent words to determine the
author of a document [27], recent studies have used more
than 1000 frequently used words to represent the style of
an author [28]. However, lexical features encompass not
only the frequency of characters or words found in a text
but also vocabulary richness, sentence/line length, word
length distribution, n-grams and lexical errors [5], [29].
Some lexical features measure the frequency of charac-
ters, which include letters (upper-case and lower-case),
digits, and special characters (e.g. ’@’, ’#’, ’$’, ’%’,
’(’, ’)’, ’{’, ’}’, etc.). Other lexical features are obtained
by extracting n-grams from a text. N-grams are tokens
formed by a contiguous sequence of n items. The most
frequent n-grams constitute the most important feature for
stylistic purposes. Importantly, n-grams are noise tolerant
since their representation is not affected dramatically by
factors such as misspelling [29].
Vocabulary richness measures the diversity of vocabu-
lary in a text by quantifying the total number of unique
vocabulary, the number of hapax legomenon (i.e., a word
which occurs only once in a text) and the number of hapax
dis legomenon (e.g., dis legomenon or tris legomenon,
referring to double or triple occurrences). This metric
is computed by dividing the total number of unique
vocabulary (hapax legomenon or dis legomenon) by the
total number of tokens (each token is a word).
Syntactic features can be divided into average of punc-
tuation and part-of-speech (POS). Syntactic pattern is an
unconscious characteristic and it is considered to be more
reliable than lexical information [30]. Punctuation is an
important rule to define boundaries and identify meaning
(quotation, exclamation, etc.) by splitting a paragraph into
sentences and each sentence into various tokens. However,
it is not sufficient to analyze only the punctuation of a
document, as certain words such as ’Ph.D.’ or ’uvic.ca’
include punctuation characters too. Therefore, it is nec-
essary to format the text before analyzing it. The part-
of-speech tagging (POS tag or POST) is to categorize
the tokens according to their function in the context.
Basic POS tags include the functional words that express
a grammatical relationship (i.e. articles, auxiliary verbs,
personal pronouns, possessive adjectives) [16], [19], [20],
[22].
Semantic features are related to the meaning of lan-
guage and involve factors such as the meaning of words,
grammatical construction, semantic relationships, and
content-specific features [31]. Content-specific features
are derived by measuring the use of certain vocabulary in
the text. These features can be useful when they identify
the gender, age, or a specific group the author may be
part of. For example, within the same group, authors tend
to use identical taxonomy in their communication and
each generation has its own unique vocabulary [22], [23],
[32]. In addition, some approaches measure the use of
words indicative of the individual’s race, nationality, and
even tendency towards certain types of violence [8], as
well as the number of gender-specific words [18], and
psycho-linguistic cues [19]. However, these features are
more useful when the context of the text being analyzed
does not vary, avoiding the confounding factor of cross-
topic texts [33].
Application specific features can easily be extracted
from documents by analyzing structural and content-
specific characteristics [22], [24], [32], [34]. Structural
characteristics are related to the organization and format
of a text and are usually more flexible in online documents
such as e-mail. These features can be categorized at
the message-level, paragraph-level or according to the
technical structure of the document [4].
As a matter of fact, analyzing a large number of fea-
tures does not necessarily provide the best results, as some
features provide very little or no predictive information.
Previous studies yielded encouraging results with lexical
features, specially n-grams [5], [28]. In particular, since
n-gram features are noise tolerant and effective, and e-
mails are non-structured documents, we will focus in this
paper only on these types of features.
Although n-gram features have been shown to be
effective, classification based on such feature is complex
while the data processing is time consuming. While the
approach used so far in the literature has consisted of
computing n-gram frequency in given sample document,
we propose an innovative approach that analyzes n-grams
and their relationship with the training dataset. This
allows us to reduce the number of n-grams features to
one, and address the above mentioned challenges.
B. Basic Classification Model
Our model consists of a collection of profiles generated
separately for individual users. The model involves two
modes of operations, namely, training and verification,
where the users profiles are built and then checked, re-
spectively. The training phase involves two steps. During
JOURNAL OF NETWORKS, VOL. 9, NO. 12, DECEMBER 2014 3349
© 2014 ACADEMY PUBLISHER
the first step, the user profile is derived by extracting n-
grams from sample documents. During the second step, a
user specific threshold is computed and used later in the
verification phase.
Given a user U , we divide randomly her training data
into two subsets, denoted TU1 and TU2 , corresponding to
2/3 and 1/3 of the training data, respectively. Let N(TU1 )
denote the set of all unique n-grams occurring in TU1 .
We divide TU2 into p blocks of characters of equal size:
b
U
1 , ..., b
U
p .
Given a block bUi , let N(bUi ) denote the set of all unique
n-grams occurring in bUi .
Given two users U and I , let rU (bIi ) denote the
percentage of unique n-grams shared by block bIi (of user
I) and (training set) TU1 , giving:
rU (b
I
i ) =
|N(bIi ) \N(TU1 )|
|N(bIi )|
(1)
where |X| denotes the cardinality of set X .
Given a user U , our model approximates the actual (but
unknown) distribution of the ratios (rU (bU1 ), ..., rU (bUp ))
(extracted from TU2 ) by computing the sample mean de-
noted µU and the sample variance  2U during the training.
A block b is said to be a genuine sample of user U if
and only if |rU (b)|   (✏U +  ), where ✏U is a specific
threshold for user U , and   is a predefined constant.
(
genuine or 1 if |rU (b)|   (✏U +  )
0, otherwise
(2)
We derive the value of ✏U for user U using a supervised
learning technique outlined by Algorithm 1 when given
when given training samples from other users I1, ..., Ik
(Ii 6= U ). Let up and down be local variables (in the
algorithm) used to verify whether the difference between
FRR and FAR is increasing or decreasing, and   be a
local variable that denote the increment/decrement for the
value of ✏U . The threshold is initialized (i.e. ✏U = µU  
( U/2)), and then varied incrementally by minimizing the
difference between FRR and FAR values for the user, the
goal being to obtain an operating point that is as close as
possible to the EER (i.e. FRR = FAR) for   = 0.
In each iteration, the FRR and FAR for user U
denoted FRRU and FARU , respectively, are calculated
for the current values of ✏U and  . Let   be a local variable
(in the algorithm) that denote the increment/decrement for
the ✏U value. If (FRRU   FARU ) > 0, a true value
is assigned to the variable down and the threshold is
decreased by  . If (FRRU  FARU ) < 0, a true value is
assigned to the variable up and the threshold is increased
by  . Finally, we test if up and down are true, which
means that a local optimum was found. In this case, the
values of up and down are reset to false and   is divided
by 10. This process is repeated until   is lower than
0.0001.
Algorithm 2 returns the FAR and FRR for a user U
given some training data, a user-specific threshold value,
and some constant value assigned to  .
/
*
U a user for whom the threshold
is being calculated
*
/
/
*
I1, ..., Ik: a set of other users
(Ii 6= U)
*
/
/
*
✏U: threshold computed for user U
*
/
Input: Training data for U, I1, ..., Ik
Output: ✏U
1 begin
2 up false;
3 down false;
4    1;
5 ✏U µU   ( U/2);
6    0;
7 while   > 0.0001 do
/
*
Calculating FAR and FRR for
user U
*
/
8 FRRU , FARU =
calculate(U, I1, ..., Ik, ✏U ,  );
/
*
Minimizing the difference
between FAR and FRR
*
/
9 if (FRRU   FARU ) > 0 then
10 down true;
11 ✏U ✏U    ;
12 else if (FRRU   FARU ) < 0 then
13 up true;
14 ✏U  ✏U +  ;
15 else
16 return ✏U ;
17 end
18 if (up & down) then
19 up false;
20 down false;
21     /10;
22 end
23 end
24 return ✏U ;
25 end
Algorithm 1: Threshold calculation for a given user.
C. Extended Classification Model
We extend the above basic classification model by
introducing two new variables, named frequency and
mode, in order to capture repeated n-grams in the training
dataset (frequency) and in the testing (mode), respectively.
Given a user U , we divide her training data into two
subsets, denoted T (f)U1 and TU2 . Let N(T (f)U1 ) denote
the set of all unique n-grams occurring in T (f)U1 with
frequency f .
Let m denote a binary variable (i.e., m 2 {1, 0}) that
represents the mode of calculation of the n-grams.
Given a block b, let Nm(b) denote the following:
(
- the set of all unique n-grams occurring in b, if m = 0
- the set of all n-grams occurring in b, otherwise.
(3)
Therefore, the basic classification model, expressed in
3350 JOURNAL OF NETWORKS, VOL. 9, NO. 12, DECEMBER 2014
© 2014 ACADEMY PUBLISHER
Input: ✏U ,  , Training data for U, I1, ..., Ik
Output: (FARU , FRRU )
1 begin
/
*
Calculating FRR for user U
*
/
2 for i! 1 to p do
3 FR 0;
4 if rU (bUi )) < (✏U +  ) then
5 FR FR+ 1;
6 end
7 end
8 FRRU  FRp ;
/
*
Calculating FAR for user U
*
/
9 for i! 1 to k do
10 for j ! 1 to n do
11 FA 0;
12 if rU (bIij )   (✏U +  ) then
13 FA FA+ 1;
14 end
15 end
16 end
17 FARU  FAp⇥k ; return (FARU , FRRU );
18 end
Algorithm 2: FAR and FRR calculation for a given user
the equation 1, has the same output when f and m are
set to 0 in the following equation:
rU (b) =
|Nm(b) \N(T (f)U1 )|
|Nm(b)|
(4)
IV. EXPERIMENTAL EVALUATION
In this section, we present in this section the experi-
mental evaluation of our proposed approach by describing
our dataset and data preprocessing technique, and then
outlining our evaluation method and results.
A. Dataset and Data Preprocessing
In order to validate our system, we performed exper-
iments on a real-life dataset from Enron e-mail corpus1.
Enron was an energy company (located in Houston,
Texas) that was bankrupt in 2001 due to white collar
fraud The e-mails of Enron’s employees were made public
by the Federal Energy Regulatory Commission during
the fraud investigation. The e-mail dataset contains more
than 200 thousands messages from about 150 users. The
average number of words per e-mail is 200. The e-
mails are plain texts and cover various topics ranging
from business communications to technical reports and
personal chats.
While traditional documents are very well structured
and large in size providing several stylometric features,
an e-mail typically consists of a few paragraphs, wrote
quickly and often with syntactic and grammatical errors.
In our approach, we grouped all the sample e-mails used
1available at http://www.cs.cmu.edu/⇠enron/
to build a given author profile into a single document that
could be subsequently divided into small blocks.
In order to obtain the same structural data and improve
classification accuracy, we performed several preprocess-
ing steps to the data as follows:
• E-mails from the folders “sent” and “sent items”
within each user’s folder were selected, with all
duplicate e-mails removed;
• JavaMail API was used to parse each e-mail and
extract the body of the message;
• Since different texts must be logically equivalent,
(i.e., must have the same canonical form), the fol-
lowing filters were applied:
– Strip e-mail replay;
– Replace e-mail address by double @ character
(i.e. @@);
– Replace http address by a meta tag http;
– Replace currency by $XX;
– Replace percentage by XX%;
– Replace numbers by the digit 0;
– Normalize the document to printable ASCII;
– Convert the document to lower-case characters;
– Strip white space;
– Strip any punctuation from the document.
• All messages, per author, were grouped creating a
long text or stream of characters that was divided
into blocks.
B. Evaluation Method
After the preprocessing phase, the dataset was reduced
from 150 authors to sets of 107, 92 and 87 authors to
ensure that only streams of text with 12,500, 18,750 and
25,000 characters were used in our analysis, respectively.
We assess experimentally the effectiveness of our ap-
proach through a 10-fold cross-validation test. We ran-
domly sorted the dataset, and allocated in each (valida-
tion) round 90% of the dataset for training and the re-
maining 10% for testing. The 90% training data allocated
to a given user U was further divided as follows: 2/3
of the training data allocated to subset TU1 and 1/3 of
the data for subset TU2 , respectively. The 10% test data
for user U was divided in p blocks of equal size s. We
tested two different block sizes, s = 250 and s = 500
characters, respectively. The number of blocks per user
p varied from 25 to 100. In addition, we investigated
separately n-grams of sizes (n=) 3, 4, and 5, for each of
these analyses yielding in total 18 different experiments.
Our experiments cover three different values for the
frequency f (i.e. f= {0, 1, 2}) and two different values
for the mode of calculation of the n-grams (i.e. m= {0,
1}). Table I shows the configuration of our experiments.
For each user U , we computed a corresponding profile
by using their training data and training data from other
users considered as impostors. This allows computing the
acceptance threshold ✏U for user U as explained before. A
given block b is considered to belong to an hypothesized
genuine user U when the ratio |rU (b)| is greater than
JOURNAL OF NETWORKS, VOL. 9, NO. 12, DECEMBER 2014 3351
© 2014 ACADEMY PUBLISHER
TABLE I
CONFIGURATION OF EXPERIMENTS
Experiment con-
figuration #
Number of Users
(k)
Number of
blocks per
author (p)
Block
size (s)
1 107 50
2502 92 75
3 87 100
4 107 25
5005 92 37
6 87 50
✏U +  , where   is a predefined constant and ✏U is the
user specific threshold.
We compute the FRR for user U by comparing each of
the blocks from her test data against her profile. A false
rejection (FR) is counted when the system rejects one of
these blocks. The FAR is computed by comparing each
of the test blocks from the other users (i.e. the impostors)
against the profile of user U . A false acceptance (FA)
occurs when the system categorizes any of these blocks
as belonging to user U . By repeating the above process
for each of the users, we compute the overall FAR and
FRR by averaging the individual measures.
C. Evaluation Results
Table II shows the overall FRR and FAR for the 18
experiments, where the constants   = 0, f = 0, and
m = 0. It can be noted that the accuracy decreases not
only when the number of authors increases, but also when
the number of blocks per user p and the block size s
decreases.
Experiments using 5-grams achieve better results than
those using 3 and 4-grams for large number of blocks per
user and block size. Experiments using 4-grams yield bet-
ter results when the number of blocks per user decreases.
Overall, the best result is achieved in experiment 6, with
87 authors, 50 blocks per user, and a block size of 500
characters (FRR=14.71%, FAR=13.93%).
TABLE II
PERFORMANCE RESULTS FOR THE DIFFERENT EXPERIMENTS
(  = 0, f = 0, m = 0)
No. 3-gram 4-gram 5-gramFRR FAR FRR FAR FRR FAR
1 24.85 28.61 22.05 24.09 24.11 20.50
2 26.76 26.82 23.64 21.68 25.13 19.39
3 24.82 28.15 23.56 21.15 17.24 20.39
4 26.47 22.70 23.67 17.81 23.98 16.29
5 23.36 21.81 18.75 18.01 18.20 15.40
6 22.29 22.21 19.77 16.11 14.71 13.93
Based on configuration 6 (which yields the best results),
we assess the impact of the frequency f and mode m on
the system performance, by varying the frequency from
0 to 2 and the mode between 0 and 1. Table III lists the
obtained results.
Figure 1 depicts the receiver operating characteristic
(ROC) curve for experiment configuration # 6 (from Table
I) using 5-gram, f = 0, and m = 0. The curve illustrates
TABLE III
PERFORMANCE RESULTS VARYING f AND m FOR EXPERIMENT
NUMBER 6 (  = 0)
f m
3-gram 4-gram 5-gram
FRR FAR FRR FAR FRR FAR
0 0 22.29 22.21 19.77 16.11 14.71 13.931 21.83 22.16 19.08 16.02 15.40 13.90
1 0 21.60 22.93 21.83 17.09 19.54 15.451 21.60 22.87 22.75 17.30 20.68 15.46
2 0 23.21 22.20 21.83 18.11 21.37 16.381 22.75 22.78 21.60 18.47 22.98 16.46
the relationship between the FRR and FAR for different
values of  . The equal error rate (ERR) was estimated as
14.35% and achieved when   =  0.25.
Fig. 1. Receiver Operating Characteristic curve for experiment config-
uration #6 using 5-gram and sample performance values for different
values of  .
V. DISCUSSIONS
The Enron dataset has previously been used not only in
authorship verification [24], but also in authorship identi-
fication [4], [9], [11], [21], [22] and authorship character-
ization [11], [18], [19]. These previous experiments used
a number of users ranging from 3 to 114, and achieved
in the best cases EER varying from 17% to 30%. In the
present study, the best configuration was achieved with
block size of 500 characters, achieving EER below 15%
which is better compared to the accuracy obtained using
similar techniques in the literature. Table IV summarizes
the performances, block sizes, and population size of
previous stylometry studies.
Despite our encouraging results, more works must be
done to improve the accuracy to an acceptable level
for authorship verification in forensics investigation. We
believe that our proposed scheme is a good step toward
achieving that goal. It is important to notice that these
results were obtained using only one type of features
out of hundreds of potential stylometric features. We
believe that we can reduce significantly our error rates by
incorporating other types of features in our framework.
We investigated in this work block sizes of 250 and
500 characters, respectively, which represent significantly
3352 JOURNAL OF NETWORKS, VOL. 9, NO. 12, DECEMBER 2014
© 2014 ACADEMY PUBLISHER
shorter messages compared to the messages used so far in
the literature for identity verification. To our knowledge,
one of the few works that have investigated compara-
ble message sizes includes the work by Sanderson and
Guenter, who split a long text in chunks of 500 characters
[7].
They achieved similar results using block size of 500
characters, although with a relatively smaller dataset (i.e.
50 users) [7]. Furthermore, it is important to mention that
their dataset consisted of newspapers’ articles, which are
known to be well structured compared to e-mail messages.
Our experiments varying f and m showed a slight
increase of FRR and FAR across different size of n-grams.
However, we note that a block can be classified cor-
rectly by one configuration and misclassified by another,
suggesting that a combination of different configurations
submitted to a machine learning classifier (e.g. SVM,
Logistic Regression) could improve the general results.
We still need to investigate even shorter messages (e.g.
10 to 50 characters) to be able to cover (beyond emails)
a broader range of online messages such as twitter feeds
and text messages. However, attempting to reduce at the
same time the block size and verification error rates is a
difficult task in the sense that these attributes are loosely
related to each other. A smaller verification block may
lead to increased verification error rates and vice-versa.
We intend to tackle such challenge in the future.
Another important limitation of many previous stylom-
etry studies is that the performance metrics computed
during their evaluations cover only one side of the story,
and this is clearly emphasized by Table IV. Accuracy is
traditionally measured using the following two different
types of errors:
1) Type I error, which corresponds to the FRR, also
referred to as False Non-Match Rate (FNMR) or
False Positive Rate (FPR);
2) Type II error, which corresponds to the FAR, also
referred to as False Match Rate (FMR) or False
Negative Rate.
However, most previous studies calculate the so-called
(classification) accuracy (see Table IV) which actually
corresponds to the true match rate and allows deriving
only one type of error, namely, Type II error: FAR =
1   Accuracy. Nothing is said about Type I error in
these studies, which makes it difficult to judge their real
strength in terms of accuracy. As shown by Table IV, only
few studies have provided both types of errors, among
which our work can be considered as one of the most
strongest in terms of sample population size, block size,
and accuracy.
VI. CONCLUSION
In this paper, we have introduced a new approach
and investigated the effectiveness of using stylometry for
authorship verification for short online messages. Our
proposed model combines supervised learning and n-gram
analysis. We have validated our system by performing
experiments on a real-life dataset from Enron, where the
e-mails were combined to produce a single long message
per individual, and then divided into smaller blocks used
for authorship verification. Our experimental evaluation
yields an EER 14.35% for 87 users for relatively small
block sizes. While the obtained results are promising, it
is clear that more work must be done for the proposed
scheme to be usable in real-world authentication of online
users. We discussed the limitations of our approach and
plan to address them in our future work.
In particular, we will improve verification accuracy by
creating new n-grams features based on different values
for the frequency f (i.e. f = 1 and f = 2) and for the mode
of calculation of the n-grams (i.e. m = 0 and m = 1), and
by expanding our feature set beyond n-grams. We will
also improve the robustness of the scheme in handling
shorter and shorter message structures.
ACKNOWLEDGEMENT
This research has been enabled by the use of computing
resources provided by WestGrid and Compute/Calcul Ca-
nada. The research is funded by a Vanier scholarship from
the Natural Sciences and Engineering Research Council
of Canada (NSERC) and CNPq scholarship (Brazil).
REFERENCES
[1] J. Li, R. Zheng, and H. Chen, “From fingerprint to writeprint,”
Commun. ACM, vol. 49, pp. 76–82, April 2006.
[2] J. L. Hilton, On verifying wordprint studies: Book of Mormon
authorship, ser. Reprint (Foundation for Ancient Research and
Mormon Studies). F.A.R.M.S., 1991.
[3] F. Can and J. M. Patton, Change of Writing Style With Time.
Kluwer Academic Publishers, 2004, vol. 38.
[4] A. Abbasi and H. Chen, “Writeprints: A stylometric approach to
identity-level identification and similarity detection in cyberspace,”
ACM Trans. Inf. Syst., vol. 26, pp. 1–29, April 2008.
[5] O. Canales, V. Monaco, T. Murphy, E. Zych, J. Stewart, C. T. A.
Castro, O. Sotoye, L. Torres, and G. Truley, “A stylometry system
for authenticating students taking online tests,” P. of Student-
Faculty Research Day, Ed., CSIS. Pace University, May 6 2011.
[6] M. Koppel and J. Schler, “Authorship verification as a one-class
classification problem,” in Proceedings of the 21st international
conference on Machine learning, ser. ICML ’04. Banff, Alberta,
Canada: ACM, 2004, pp. 62–69.
[7] C. Sanderson and S. Guenter, “Short text authorship attribution
via sequence kernels, markov chains and author unmasking: an
investigation,” in Proceedings of the 2006 Conference on Empir-
ical Methods in Natural Language Processing, ser. EMNLP ’06.
Stroudsburg, PA, USA: Association for Computational Linguistics,
2006, pp. 482–491.
[8] A. Abbasi and H. Chen, “Applying authorship analysis to
extremist-group web forum messages,” IEEE Intelligent Systems,
vol. 20, pp. 67–75, September 2005.
[9] F. Iqbal, R. Hadjidj, B. C. Fung, and M. Debbabi, “A novel
approach of mining write-prints for authorship attribution in e-
mail forensics,” Digital Investigation, vol. 5, pp. S42–S51, 2008.
[10] F. Iqbal, L. A. Khan, B. C. M. Fung, and M. Debbabi, “E-mail
authorship verification for forensic investigation,” in Proceedings
of the 2010 ACM Symposium on Applied Computing, ser. SAC
’10. New York, NY, USA: ACM, 2010, pp. 1591–1598.
[11] F. Iqbal, H. Binsalleeh, B. C. Fung, and M. Debbabi, “A unified
data mining solution for authorship analysis in anonymous textual
communications,” Information Sciences, vol. 231, pp. 98–112,
2013.
[12] C. E. Chaski, “Who’s at the keyboard: Authorship attribution in
digital evidence investigations,” International Journal of Digital
Evidence, vol. 4, no. 1, pp. 1–13, Spring 2005.
[13] F. Mosteller and D. L. Wallace, Inference and Disputed Author-
ship: The Federalist. Addison-Wesley, 1964.
JOURNAL OF NETWORKS, VOL. 9, NO. 12, DECEMBER 2014 3353
© 2014 ACADEMY PUBLISHER
TABLE IV
COMPARATIVE PERFORMANCES, BLOCK SIZES AND, POPULATION SIZES FOR STYLOMETRY STUDIES.
Category Reference Sample Population Size Block Size Accuracy* (%) EER (%)
A
ttr
ib
ut
io
n
[4] 100 ** 277 words 83.10 - -
[12] 10 200 words 95.70 - -
[35] 2 - 4 60,000 words 93.8 - 97.8 - -
[22] 3 ** 200 words 69 -83 - -
[28] 87 287 words 50 - 60 - -
[21] 3 - 10 ** 200 words 80 - 90 - -
[11] 4 - 20 ** 300 words 69.75 - 88.37 - -
[9] 6 - 10 ** 200 words 77 - 80.5 - -
[36] 1000 500 words 42.2 - 93.2 - -
[1] 20 169 words 99.01 - -
[37] 20 600 words 84.30 - -
[7] 50 500 characters - - 8.08 - 30.88
[36] 10,000 500 words 46 - -
[38] 100,000 335 words 20 - -
C
ha
ra
ct
er
iz
at
io
n
[8] 5 76 words 90.1 - 97 - -
[19] 108 ** 50 - 200 words 73 - 82.23 - -
[18] 114 ** 50 - 200 words 80.08 - 82.20 - -
[39] 325 50 - 200 words 70.20 - -
[11] 4 - 20 ** 300 words 39.13% - 60.44% - -
[23] 100 300 words 39.0 - 99.70 - -
[17] 10 - 40 450 words 68.3 - 91.5 - -
Ve
rifi
ca
tio
n
[5] 40 1710 - 70300 characters - - 30
[24] 25 - 40 ** 30 - 50 words 83.90 - 88.31
[40] 8 628 - 1342 words - - 3
[6] 10 500 words 95.70 - -
[41] 29 2400 words - - 22
Proposed Ap-
proach
87 500 character - - 14.35%
* The accuracy is measured by the percentage of correctly matched authors in the testing set.
** Used Enron dataset for testing.
[14] J. Burrows, “Delta: a measure of stylistic difference and a guide
to likely authorship,” Literary and Linguistic Computing, vol. 17,
no. 3, pp. 267–287, 2002.
[15] E. Backer and P. van Kranenburg, “On musical stylometry pattern
recognition approach,” Pattern Recognition Letters, vol. 26, no. 3,
pp. 299–309, 2005.
[16] Y. Zhao and J. Zobel, “Searching with style: authorship attribution
in classic literature,” in Proceedings of the thirtieth Australasian
conference on Computer science - Volume 62, ser. ACSC ’07.
Darlinghurst, Australia, Australia: Australian Computer Society,
Inc., 2007, pp. 59–68.
[17] K. G. Ruchita Sarawgi and Y. Choi, “Gender attribution: tracing
stylometric evidence beyond topic and genre,” in Proceedings of
the 15th Conference on Computational Natural Language Learn-
ing, ser. CoNLL ’11. Stroudsburg, PA, USA: Association for
Computational Linguistics, 2011, pp. 78–86.
[18] N. Cheng, X. Chen, R. Chandramouli, and K. Subbalakshmi,
“Gender identification from e-mails,” in Computational Intelli-
gence and Data Mining, 2009. CIDM ’09. IEEE Symposium on,
30 2009-april 2 2009, pp. 154–158.
[19] N. Cheng, R. Chandramouli, and K. Subbalakshmi, “Author gender
identification from text,” Digital Investigation, vol. 8, no. 1, pp.
78–88, 2011.
[20] P. Juola and R. H. Baayen, “A controlled-corpus experiment in
authorship identification by cross-entropy,” Literary and Linguistic
Computing, vol. 20, no. Suppl, pp. 59–67, 2005.
[21] F. Iqbal, H. Binsalleeh, B. C. Fung, and M. Debbabi, “Mining
writeprints from anonymous e-mails for forensic investigation,”
Digital Investigation, vol. 7, no. 1-2, pp. 56–64, 2010.
[22] R. Hadjidj, M. Debbabi, H. Lounis, F. Iqbal, A. Szporer, and
D. Benredjem, “Towards an integrated e-mail forensic analysis
framework,” Digital Investigation, vol. 5, no. 3-4, pp. 124–137,
2009.
[23] T. Kucukyilmaz, B. B. Cambazoglu, C. Aykanat, and F. Can,
“Chat mining: Predicting user and message attributes in computer-
mediated communication,” Information Processing Management,
vol. 44, no. 4, pp. 1448–1466, 2008.
[24] X. Chen, P. Hao, R. Chandramouli, and K. P. Subbalakshmi, “Au-
thorship similarity detection from email messages,” in Proceedings
of the 7th international conference on Machine learning and data
mining in pattern recognition, ser. MLDM’11. Berlin, Heidelberg:
Springer-Verlag, 2011, pp. 375–386.
[25] M. L. Brocardo, I. Traore, S. Saad, and I. Woungang, “Authorship
verification for short messages using stylometry,” in In Proceed-
ings of the International Conference on Computer, Information
and Telecommunication Systems (CITS). Piraeus-Athens, Greece,
May 2013, pp. 1–6.
[26] S. M. Alzahrani, N. Salim, and A. Abraham, “Understanding
plagiarism linguistic patterns, textual features, and detection meth-
ods,” Systems, Man, and Cybernetics, Part C: Applications and
Reviews, IEEE Transactions on, vol. 42, no. 2, pp. 133–149, March
2012.
[27] J. F. Burrows, “Word patterns and story shapes: The statistical
analysis of narrative style,” Literary and Linguistic Computing,
vol. 2, no. 1, pp. 61–70, 1987.
[28] N. Homem and J. Carvalho, “Authorship identification and au-
thor fuzzy fingerprints,” in Fuzzy Information Processing Society
(NAFIPS), 2011 Annual Meeting of the North American, march
2011, pp. 1–6.
[29] E. Stamatatos, “A survey of modern authorship attribution meth-
ods,” J. Am. Soc. Inf. Sci. Technol., vol. 60, pp. 538–556, March
2009.
[30] H. Baayen, H. van Halteren, and F. Tweedie, “Outside the cave
of shadows: using syntactic annotation to enhance authorship
attribution,” Literary and Linguistic Computing, vol. 11, no. 3,
pp. 121–132, 1996.
[31] D. Jurafsky and J. H. Martin, Speech and Language Processing:
An Introduction to Natural Language Processing, Computational
Linguistics and Speech Recognition, 2nd ed. Prentice Hall, Feb.
2008.
[32] R. Zheng, J. Li, H. Chen, and Z. Huang, “A framework for
authorship identification of online messages: Writing-style features
and classification techniques,” J. Am. Soc. Inf. Sci. Technol.,
vol. 57, pp. 378–393, February 2006.
[33] E. Stamatatos, “Ensemble-based author identification using char-
acter n-grams,” in 3rd International Workshop on Text-based
Information Retrieval, 2006, pp. 41–46.
[34] O. de Vel, A. Anderson, M. Corney, and G. Mohay, “Mining e-
3354 JOURNAL OF NETWORKS, VOL. 9, NO. 12, DECEMBER 2014
© 2014 ACADEMY PUBLISHER
mail content for author identification forensics,” Sigmod Record,
vol. 30, no. 4, pp. 55–64, 2001.
[35] J. H. Clark and C. J. Hannon, “A classifier system for author
recognition using synonym-based features,” in Proceedings of the
6th Mexican international conference on Advances in artificial
intelligence, ser. MICAI’07. Berlin, Heidelberg: Springer-Verlag,
2007, pp. 839–849.
[36] M. Koppel, J. Schler, and S. Argamon, “Authorship attribution in
the wild,” Lang. Resour. Eval., vol. 45, pp. 83–94, March 2010.
[37] D. Pavelec, L. Oliveira, E. Justino, F. Neto, and L. Batista, “Author
identification using compression models,” in Document Analysis
and Recognition, 2009. ICDAR ’09. 10th International Conference
on, july 2009, pp. 936–940.
[38] A. Narayanan, H. Paskov, N. Z. Gong, J. Bethencourt, E. Stefanov,
E. C. R. Shin, and D. Song, “On the feasibility of internet-scale
author identification,” in Proceedings of the 2012 IEEE Symposium
on Security and Privacy, ser. SP ’12. Washington, DC, USA:
IEEE Computer Society, 2012, pp. 300–314.
[39] M. Corney, O. de Vel, A. Anderson, and G. Mohay, “Gender-
preferential text mining of e-mail discourse,” in Proceedings of the
18th Annual Computer Security Applications Conference, 2002,
pp. 282–289.
[40] H. V. Halteren, “Author verification by linguistic profiling: An
exploration of the parameter space,” ACM Trans. Speech Lang.
Process., vol. 4, pp. 1–17, February 2007.
[41] I. Krsul and E. H. Spafford, “Authorship analysis: identifying the
author of a program,” Computers and Security, vol. 16, no. 3, pp.
233–257, 1997.
M.Sc Marcelo Luiz Brocardo received his B.Sc. in Computer
Science from Regional University of Blumenau, Brazil (1995),
M.Sc in Computer Science from Federal University of Santa
Catarina, Brazil (2001). In 2011 started his PhD in the Depart-
ment of Electrical and Computer Engineering of the University
of Victoria. His primary research interests are in continuous
authentication using stylometry.
Dr. Issa Traore is the CEO and co-founder of Plurilock Security
Solutions Inc. (www.plurilock.com) He has been with the faculty
of the Electrical and Computer Engineering Department of
the University of Victoria since 1999, where he is currently
a Professor. Dr. Traore is also the founder and Director of
the Information Security and Object Technology (ISOT) Lab
(www.isot.ece.uvic.ca). He obtained in 1998 a PhD in Soft-
ware Engineering from the Institute Nationale Polytechnique
of Toulouse, France. His main research interests are biometrics
technologies, intrusion detection systems, and software security.
M.Sc Sherif Saad received his B.Sc. in Computer Science from
Helwan University, Egypt (2003), M.Sc in Computer Science
from Arab Academy for Science, Technology and Maritime
Transport, Egypt (2007). In 2008 he received the University of
Victoria Fellowship and started his PhD in the Department of
Electrical and Computer Engineering of the University of Vic-
toria. His primary research interests are in advancing machine-
learning methods and their application to computer and network
security. Since 2009, he is working as an information security
engineer for Plurilock Security Solutions (www.plurilock.com/).
Dr. Isaac Woungang received his M.S. & Ph. D degrees,
in Mathematics, from the Universite de la Mediterranee- Aix
Marseille II, France, and Universite du Sud, Toulon & Var,
France, in 1990 and 1994 respectively. In 1999, he received a
M.S degree from the INRS-Materials and Telecommunications,
University of Quebec, Montreal, Canada. From 1999 to 2002,
he worked as a software engineer at Nortel Networks. . Since
2002, he has been with Ryerson University, where he is now
an Associate Professor of Computer Science and Coordinator
of the Distributed Applications and Broadband (DABNEL) Lab
(http//www.scs.ryerson.ca/iwoungan).
JOURNAL OF NETWORKS, VOL. 9, NO. 12, DECEMBER 2014 3355
© 2014 ACADEMY PUBLISHER
