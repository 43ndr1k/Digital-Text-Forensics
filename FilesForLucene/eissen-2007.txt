Plagiarism Detection Without Reference
Collections
Sven Meyer zu Eissen, Benno Stein and Marion Kulig
Faculty of Media, Media Systems, Bauhaus University Weimar, 99421 Weimar,
Germany; {sven.meyer-zu-eissen, benno.stein}@medien.uni-weimar.de
Abstract. Current research in the field of automatic plagiarism detection for
text documents focuses on the development of algorithms that compare suspi-
cious documents against potential original documents. Although recent approaches
perform well in identifying copied or even modified passages (Brin et al. (1995),
Stein (2005)), they assume a closed world where a reference collection must be
given (Finkel (2002)). Recall that a human reader can identify suspicious passages
within a document without having a library of potential original documents in mind.
This raises the question whether plagiarized passages within a document can
be detected automatically if no reference is given, e. g. if the plagiarized passages
stem from a book that is not available in digital form. This paper contributes right
here; it proposes a method to identify potentially plagiarized passages by analyzing
a single document with respect to changes in writing style. Such passages then can
be used as a starting point for an Internet search for potential sources. As well as
that, such passages can be preselected for inspection by a human referee. Among
others, we will present new style features that can be computed efficiently and which
provide highly discriminative information: Our experiments, which base on a test
corpus that will be published, show encouraging results.
1 Introduction
A recent large-scale study on 18,000 students by McCabe (2005) reveals that
about 50% of the students admit to plagiarize from extraneous documents.
Plagiarism in text documents happens in several forms: one-to-one copies,
passages that are modified to a greater or lesser extent, or even translated
passages. Figure 1 shows a taxonomy of plagiarism delicts along with possible
detection methods.
1.1 Some background on plagiarism detection
The success of current approaches in plagiarism detection varies according to
the underlying plagiarism delict. The approaches stated in Brin et al. (1995)
360 Sven Meyer zu Eissen, Benno Stein and Marion Kulig
Plagiarism delict
Detection method
Accurate copy
Identity analysis
Modified
copy
Small part of document
Local identity analysis
Large part of document
Global identity analysis: Document model comparison (suffix-tree)
Language translation
Structure analysis
Transformation
Similarity analysis
w/o reference corpus:
Style analysis
with reference corpus:
Chunk identity (MD5-Hash)
Small part of document
Local similarity analysis
Large part of document
Global analysis: Document model comparison (vsm)
w/o reference corpus:
Style analysis
with reference corpus:
Fuzzy-Fingerprint
Fig. 1. A taxonomy of plagiarism delicts and analysis methods according to Stein
and Meyer zu Eissen (2006). The encircled parts indicate our contributions: the
detection of a plagiarism delict without having a reference corpus at hand.
and Hoad and Zobel (2003) employ cryptographic hash functions to gener-
ate digital fingerprints of so-called text chunks, which are compared against
a database of original text passage fingerprints. Since cryptographic finger-
prints identify a text chunk exactly, the quality of these approaches depends
on offsets and sizes of chunks within both plagiarized and original texts. An
approach introduced in Stein (2005) overcomes these limitations: unlike cryp-
tographic fingerprints, the proposed method generates fingerprints that are
robust against modifications to some extent.
However, the mentioned approaches have one constraint in common: they
require a reference collection with original documents. Observe that human
readers may identify suspicious passages within a document without having a
library of reference documents in mind: changes between brilliant and baffling
passages, or the change of person narrative give hints to plagiarism. Situa-
tions where such an intrinsic plagiarism detection can be applied are shown
encircled in Figure 1.
1.2 Contributions of the paper
Basically, the power of a plagiarism approach depends on the quality of the
quantified linguistic features. We introduce features which measure—simply
put—the customariness of word usage, and which are able to capture a sig-
nificant part of style information. To analyze the phenomenon of intrinsic
plagiarism detection we have constructed a base corpus from which various
application corpora can be compiled, each of which modeling plagiarism delicts
of different severity. Section 3 reports on experiments that we have conducted
with this corpus.
Plagiarism Detection Without Reference Collections 361
2 Quantification of writing style
Intrinsic plagiarism detection can be operationalized by dividing a document
into “natural” parts, such as sentences, paragraphs, or sections, and analyzing
the variance of certain style features. Note in this connection that within the
experiments presented in Section 3 the size of a part is chosen rather small
(40-200 words), which is ambitious from the analysis standpoint, but which
corresponds to realistic situations.
2.1 Stylometric features
Each author develops an individual writing style; i. e. he or she employs con-
sciously or subconsciously patterns to construct sentences and uses an indi-
vidual vocabulary. Stylometric features quantify style aspects, and some of
them have been used successfully in the past to discriminate between texts
with respect to authorship (Koppel and Schler (2004), Sorensen (2005)). Most
stylometric features are based on the following semiotic features:
1. Text statistics, which operate at the character level.
Examples: number of commas, question marks, word lengths.
2. Syntactic features, which measure writing style at the sentence level.
Examples: sentence lengths, use of function words
3. Part-of-speech features to quantify the use of word classes.
Examples: number of adjectives or pronouns
4. Closed-class word sets to count special words.
Examples: number of stopwords, foreign words, “difficult” words
5. Structural features, which reflect text organization.
Examples: paragraph lengths, chapter lengths
Based on these features, formulas can be constructed that quantify the
characteristic trait of an author’s writing style. Almost all of the developed
formulas aim at a quantification of the educational background, i. e., they
quantify an author’s vocabulary richness or style complexity, or a reader’s
grading level that is required to understand a text. Figure 2 classifies style-
quantifying formulas according to their intention.
Widely employed grading measures include the Flesch Kincaid Grade Level
(Flesch (1948), Kincaid et al. (1975)) and the Dale-Chall formula (Dale and
Chall (1948)). The former, which is used among others by the US Government
Department of Defense, combines the average number of syllables per word,
ASW , with average sentence length, ASL, as follows: FK = 0.39 ·ASL+11.8 ·
ASW − 15.59. The resulting number shall be an estimate for the number of
years a reader has to spend in school before being able to understand the text.
The Dale-Chall formula employs a closed-class word list containing 3000
familiar words usually known by 4th grade children. The formula combines the
percentage of difficult words that do not appear in the list with the average
sentence length and defines a monotonic function that maps onto a grading
level.
362 Sven Meyer zu Eissen, Benno Stein and Marion Kulig
Style-
quantifying
formulas
Writer-specific
Reader-specific
Flesch Reading Ease
Miyazaki Readability Index
Passive Sentences Readability Score
Complexity,
understandability
The Army’s Automated Readability Index (ARI)
Fry Readability Formula
Dale Chall Formula
Powers Sumner Kearl Grade
Bormuth Grade Level 
Flesch Kincaid Grade
Gunning Fog Index
Coleman Liau Grade Level
Honoré’s R
Yule’s K
Sichel’s S
Grading level
Vocabulary richness
Fig. 2. A classification of the most well-known style-quantifying formulas with
respect to their application range and underlying concept.
Methods to measure an author’s vocabulary richness are often based on
the ratio between the number of different words and the total number of
words within a document; well-known examples include Yule’s K (Yule (1948))
and Honore’s R (Honore (1979)). However, it was reported by Tweedie and
Baayen (1997) and Stamatatos et al. (2001) that these measures depend signif-
icantly on document length or passage length. As a consequence, they are not
suited to compare passages of varying lengths and deliver unreliable results
for short passages, which is a disqualifying criterion for plagiarism analysis.
We now introduce a new vocabulary richness statistic, the averaged word
frequency class, which turned out to be the most powerful and stable concept
with respect to intrinsic plagiarism detection that we have encountered so far.
2.2 Averaged word frequency class
The frequency class of a word is directly connected to Zipf’s law and can be
used as an indicator of a word’s customariness. Let C be a text corpus, and
let |C| be the number of words in C. Moreover, let f(w) denote the frequency
of a word w ∈ C, and let r(w) denote the rank of w in a word list of C, which
is sorted by decreasing frequency.
In accordance with (University of Leipzig (1995)) we define the word fre-
quency class c(w) of a word w ∈ C as &log2(f(w∗)/f(w))', where w∗ denotes
the most frequently used word in C. In the Sydney Morning Herald Corpus,
w∗ denotes the word “the”, which corresponds to the word frequency class 0;
the most uncommonly used words within this corpus have a word frequency
class of 19. A document’s averaged word frequency class tells us something
about style complexity and the size of an author’s vocabulary—both of which
are highly individual characteristics (Meyer zu Eissen and Stein (2004)).
Note that, based on a lookup-table, the averaged word frequency class of a
text passage can be computed in linear time in the number of words. Another
salient property is its small variance with respect to text length, which renders
it ideal for our purposes.
Plagiarism Detection Without Reference Collections 363
3 Experimental analysis
This section reports on experiments related to a plagiarism analysis without
reference collections; it addresses the following questions:
1. Which vocabulary richness measure is suited best?—which leads us to the
question: How stable is a measure with respect to text length?
2. To which extent is the detection of plagiarized text portions possible?
The first question can be answered by analyzing the characteristic of the
vocabulary richness measures concerning single author (= non plagiarized)
documents. The second question can be reformulated as a document classi-
fication task, given a reference corpus with plagiarized and non plagiarized
documents.
3.1 Evaluation of vocabulary richness measures
As pointed out above, changes in vocabulary richness across paragraphs are
a good indicator for plagiarism. Confer in this connection the left plot in
Figure 3, which contrasts the averaged word frequency class of four different
authors.
Plagiarism analysis requires a measure that works reliably at the para-
graph level. Put another way, when analyzing a portion of text from a single-
author document the ideal vocabulary richness measure should behave fairly
constant—regardless of the portion’s position and size. An according compar-
ison of Honore’s R, Yule’s K, and the average word frequency class is shown
in the right plot of Figure 3; here, the analyzed text portion varies between
10% and 100% of the entire document. Observe that the average word fre-
quency class is stable even for small paragraphs, which qualifies the measure
as a powerful instrument for intrinsic plagiarism analysis.
 0
 5
 10
 15
4 different authors
 10  20  30  40  50  60  70  80  90  100
Analyzed text portion (in % of document)
A
vg
. w
or
d 
fr
eq
ue
nc
y 
cl
as
s
 6
 8
 10
 12
 14
 16
 18
 20
 10  20  30  40  50  60  70  80  90  100
V
oc
ab
ul
ar
y 
ric
hn
es
s
Analyzed text portion (in % of document)
avg. word frequency class
Honore’s R
Yule’s K
Fig. 3. Average word frequency class of four different authors (left plot). The right
plot shows the development of Honore’s R, Yule’s K, and the average word frequency
class of a single-author document for different text portions. For a better readability
the values of Honore’s R and Yule’s K are divided by 100 and 10 respectively.
364 Sven Meyer zu Eissen, Benno Stein and Marion Kulig
3.2 Corpus construction
Since no reference collection is available for classification experiments, we
have compiled a new corpus, which will be made available for all interested
researchers. Its construction is oriented at the following corpus-linguistic cri-
teria described in Garside et al. (1997):
1. authenticity and homogeneity
2. possibility to include many types of plagiarism
3. easy processable for both human and machine
4. clear separation of text and annotations
We chose genuine computer science articles from the ACM digital library,
which were “plagiarized” by hand with both copied as well as reformulated
passages from other ACM computer science articles, contributing to crite-
rion 1. To separate annotations from text and to allow both maintenance for
human editors and standardized processing for machines, all documents in the
corpus are represented in XML-syntax (cf. criteria 2-4). They validate against
the following DTD, which declares a mixed content model and provides ele-
ment types for plagiarism delict and plagiarism source among others.
<!ELEMENT document (#PCDATA|plagiarized)*>
<!ATTLIST document source CDATA #REQUIRED>
<!ELEMENT plagiarized (#PCDATA)>
<!ATTLIST plagiarized type (copied|mod|trans) source CDATA #REQUIRED>
An XML document with k plagiarized passages defines a template from
which 2k instance documents can be generated, depending on which of the k
plagiarized parts are actually included. Instance documents contain no XML
tags in order to ensure that they can be processed by standard algorithms.
Instead, a meta information file is generated for each, specifying the exact
position of plagiarized passages.
3.3 Classification experiments
For the results presented here more than 450 instance documents were gener-
ated each of which containing between 3 and 6 plagiarized passages of different
lengths. During the plagiarism analysis each instance document was decom-
posed into 50 - 100 passages, and for each passage a paragraph-specific feature
vector fp was computed. The feature set includes average sentence length,
18 part-of-speech features, average stopword number, the Gunning Fog index,
Flesch-Kincaid Grade Level, the Dale-Chall formula, Honore’s R, Yule’s K,
and the averaged word frequency class.
Since we are interested in the detection of writing style variations, a
document-specific feature vector, fd, was computed and compared to each
of the fp. The rationale is that the relative differences between fd and the fea-
ture vectors of the plagiarized passages reflect possible writing style changes.
Plagiarism Detection Without Reference Collections 365
 0
 0.6
 0.8
 1
P
re
ci
si
on
 / 
R
ec
al
l
Proportion of
plagiarized text
450 documents
recall
precision
 0.02  0.04  0.06  0.08  0.1  0.12  0.14  0.16  0.18
Fig. 4. Detection performance versus severity of plagiarism delict: The plot shows
the averaged values for precision and recall of a series of experiments, where the
sizes of the plagiarized passages are successively increased. The values are averaged
using a ten-fold cross-validation.
The vector of these relative differences along with the class information (pla-
giarized or not) formed the input for different machine learning approaches.
Figure 4 summarizes the results: We obtained good detection rates for pla-
giarism delicts in terms of precision and recall, irrespective of the plagiarism
severity. These results were achieved using a classical discriminant analysis;
however, an SVM classification showed similar results. Table 1 quantifies the
discrimination power of the best features.
Table 1. Significance scores for the three best-discriminating features. Lower
Lambda-values and higher F-ratios indicate a better performance.
Wilks Lambda F-Ratio significant
av. word frequency class 0.723 152.6 yes
av. preposition number 0.866 61.4 yes
av. sentence length 0.880 54.0 yes
4 Summary
This paper presented an approach to detect plagiarized passages within a doc-
ument if no reference collection is given against which the suspicious document
can be matched. This problem, which we call “intrinsic plagiarism detection”,
is related to the identification of an author’s writing style, for which various
measures have been developed in the past. We presented new style features
and showed their usability with respect to plagiarism detection: Classification
experiments on a manually constructed corpus delivered promising precision
and recall values, even for small plagiarized paragraphs.
Another result of our research shall be emphasized: A vocabulary richness
measure qualifies for intrinsic plagiarism detection only, if is has a small vari-
ance subject to the analyzed text portion’s size. Our experiments revealed that
366 Sven Meyer zu Eissen, Benno Stein and Marion Kulig
the introduced averaged word frequency class outperforms other well-known
measures in this respect.
References
BRIN, S., DAVIS, J. and GARCIA-MOLINA, H. (1995): Copy Detection Mecha-
nisms for Digital Documents. In: Proceedings of SIGMOD ’95.
DALE, E. and CHALL, J.S. (1948): A Formula for Predicting Readability. Educ.
Res. Bull., 27.
FLESCH, R. (1948): A New Readability Yardstick. Journal of Applied Psychology,
32, 221–233.
GARSIDE, R., LEECH, G. and MCENERY, A. (1997): Corpus Annotation: Lin-
guistic Information from Computer Text Corpora. Longman.
HOAD, T.C. and ZOBEL, J. (2003): Methods for Identifying Versioned and Plagia-
rised Documents. JASIST, 54, 3, 203–215.
HONORE, A. (1979): Some Simple Measures of Richness of Vocabulary. Association
for Literary and Linguistic Computing Bulletin, 7, 2, 172–177.
KINCAID, J., FISHBURNE, R.P., ROGERS, R.L. and CHISSOM, B.S. (1975):
Derivation of New Readability Formulas for Navy Enlisted Personnel. Research
Branch Report 85, US Naval Air Station.
KOPPEL, M. and SCHLER, J. (2004): Authorship Verification as a One-class Clas-
sification Problem. In Proceedings of ICML 04, Banff, Canada. ACM Press.
MCCABE, D. (2005): Research Report of the Center for Academic Integrity.
http://www.academicintegrity.org.
MEYER ZU EISSEN, S. and STEIN, B. (2004): Genre Classification of Web Pages:
User Study and Feasibility Analysis. In: KI 2004, LNAI. Springer.
SORENSEN, J. (2005): A Competitive Analysis of Automated Authorship Attribu-
tion Techniques. http://hbar.net/thesis.pdf.
STAMATATOS, E., FAKOTAKIS, N. and KOKKINSKIS, G. (2001): Computer-
based Authorship Attribution without Lexical Measures. Computers and the
Humanities, 35, 193–214.
STEIN, B. (2005): Fuzzy-Fingerprints for Text-Based Information Retrieval. In the
Proceedings I-KNOW 05, Graz, J.UCS, 572–579. Know-Center.
STEIN, B. and MEYER ZU EISSEN, S. (2006): Near Similarity Search and Plagia-
rism Analysis. In Proc. 29th Annual Conference of the GfKl, Springer, Berlin.
TWEEDIE, F.J. and BAAYEN, R.H. (1997): Lexical “Constants” in Stylometry
and Authorship Studies. In Proceedings of ACH-ALLC ’97.
UNIVERSITY OF LEIPZIG (1995): Wortschatz. http://wortschatz.uni-
leipzig.de.
YULE, G. (1944): The Statistical Study of Literary Vocabulary. Cambridge Univer-
sity Press.
