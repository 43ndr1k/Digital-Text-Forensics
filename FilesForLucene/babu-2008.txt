Degraded Document Image Enhancement using Hybrid Thresholding and 
Mathematical Morphology 
 
 
Nija Babu 
Department of Computer 
Science and Engineering  
PES Institute of Technology  
Bangalore, India 
nijababu@yahoo.co.in 
Preethi N.G 
Department of Computer 
Science and Engineering 
PES Institute of Technology 
Bangalore, India 
ng.preeti@gmail.com 
Shylaja S. S 
Department of Information 
Science and Engineering 
PES Institute of Technology 
Bangalore, India 
shylaja.sharath@pes.edu 
 
     
 
 
Abstract 
 
The paper presents a hybrid thresholding approach 
for binarization and enhancement of degraded 
documents. Historical documents contain information 
of great cultural and scientific value. But such 
documents are frequently degraded over time. 
Digitized degraded documents require specialized 
processing to remove different kinds of noise and to 
improve readability. The approach for enhancing 
degraded documents uses a combination of two 
thresholding algorithms. First, iterative global 
thresholding is applied to the smoothed degraded 
image until the stopping criteria is reached. Then a 
threshold selection method from gray level histogram 
is used to binarize the image. The next step is detecting 
areas where noise still remains and applying iterative 
thresholding locally.  A method to improve the quality 
of textual information in the document is also done as 
a post processing stage, thus making the approach 
efficient and better suited for character recognition 
applications. 
 
1. Introduction 
 
Binarization or thresholding refers to the conversion 
of gray level image into a black and white image. An 
ideal binarization technique must be able to perfectly 
separate text from background, thus removing any kind 
of noise and improve readability. Frequently, 
binarization is used as a pre processing stage before 
OCR. Binarization plays a major role in document 
image processing since its performance affects further 
processing of the image such as segmentation and 
character recognition.  
     Historical documents suffer from various 
degradation problems which may occur due to poor 
lighting or environmental conditions. Some examples 
of degradations which appear frequently in such 
documents include non uniform intensity, poor contrast 
due to humidity, paper deterioration, presence of 
smear, ink etc. Therefore, such document images 
require specialized thresholding technique to remove 
noise while restoring essential textual information. 
In this paper, we present a hybrid thresholding 
approach by combining the advantages of two 
thresholding techniques making it better suited for 
historical archive documents. The paper is organized as 
follows. The next section gives a brief introduction to 
binarization methods. Section 3 gives an overview of 
proposed methodology while in section 4 the various 
steps of the proposed method are discussed in detail. In 
section 5 experimental results are shown. Finally, 
conclusions drawn and future enhancements are 
summarized in section 6. 
 
2. Document Image Binarization Methods 
 
 Image binarization methods can be broadly 
classified as global thresholding and local thresholding 
methods.  Global thresholding is applied to the entire 
image, and the threshold computed is used for 
separating text from background. But global 
thresholding may not be suited for images where the 
noise is confined to a small area of the image like 
smear, ink etc. Local thresholding is more adaptive by 
selecting a different threshold for each area in image 
based on the image characteristics.   
     Image binarization has been an area of high 
research interest for the past several years and various 
thresholding techniques have been proposed [1-4].  
One of the oldest and most widely used method is 
Otsu’s [1] based on variance of pixel intensity.  
Sauvola [2] uses two algorithms to compute local 
threshold for each pixel.  Gatos [3] proposes a method 
which uses filters and thresholding to remove noise. 
Sixth Indian Conference on Computer Vision, Graphics & Image Processing
978-0-7695-3476-3/08 $25.00 © 2008 IEEE
DOI 10.1109/ICVGIP.2008.55
701
Authorized licensed use limited to: Agean University. Downloaded on March 20, 2009 at 11:06 from IEEE Xplore.  Restrictions apply.
Kavallieratou [6] presents an efficient and cost 
effective approach for image binarization.        
 
3. Overview of Proposed Binarization 
Technique  
 
The proposed methodology combines iterative 
global thresholding (IGT) [5] and Otsu threshold 
selection method from gray level histogram [1] to 
binarize the image. Iterative thresholding method 
combines the advantages of global and local 
thresholding, thus making it more adaptable to 
different kinds of noise in the image. Otsu’s threshold 
selection method has proved to be efficient in selecting 
optimal threshold for image binarization. 
The input of the algorithm is assumed to be gray 
level documents which can be represented by the 
equation, 
I(x, y) = g                                  (1) 
  
where g is the gray level value of the pixel which can 
be in the range [0..255], x and y are the horizontal and 
vertical coordinates of the image respectively. The 
value 0 for g represents black color and value 255 
represents white. Our aim is to convert the intermediate 
gray level values into either black or white. The 
complete flow of the proposed thresholding algorithm 
is given in Figure 1. 
The first stage of the proposed approach is 
smoothing the image using weighted median filter. The 
next step is application of iterative global thresholding 
to the smoothed image. The main steps in the process 
are computing average pixel value of image, 
subtraction of average from each pixel in the image 
and histogram equalization. This process is continued 
until a stopping criteria, based on the threshold 
calculated for each iteration, is satisfied. 
The next stage is computing an optimal threshold 
for binarizing the image using Otsu’s algorithm. 
Detecting areas in the image where noise still remains 
is the next step. Iterative thresholding is reapplied to 
each of the detected area separately. 
The final stage is applying suitable structuring 
element for enhancing textual information in the 
image. In some cases, when the number of iterations in 
the thresholding stage is more, the text maybe affected 
which can lead to broken characters in the image. To 
solve this problem morphological dilation operator is 
used on the binarized result to fill gaps in broken 
characters and make it more legible.  
 
 
Figure 1: Hybrid Thresholding Flow 
 
4. Hybrid Thresholding Methodology 
 
4.1 Smooth Image 
     The first step of our approach is smoothing the 
input degraded image using a weighted median filter 
shown in Figure 2. Weighted median filter is used 
because it smoothes the image with less blurring. 
  
1 2 1 
2 3 2 
1 2 1 
                         Figure 2: 3x3 median filter 
 
 
Input image 
Apply IGT on image 
(Global thresholding) 
Binarize image using Otsu 
thresholding algorithm 
Detect areas where noise still 
remains 
Apply IGT to each 
detected area 
(Local thresholding) 
 
Apply morphological dilation 
operator 
Iterate 
until 
stopping 
criteria 
satisfied 
Iterate 
until 
stopping 
criteria 
satisfied 
Smooth image using 
weighted median filter 
702
Authorized licensed use limited to: Agean University. Downloaded on March 20, 2009 at 11:06 from IEEE Xplore.  Restrictions apply.
4.2 Iterative Global Thresholding 
      
 After smoothing next step is application of IGT to 
the smoothed image represented by equation (1). 
Figure 3 shows a part of image where IGT is applied. 
The steps in the process can be summarized as follows: 
 
(i) Compute average pixel value 
(ii) Average pixel value is subtracted from 
each pixel in the image 
(iii) Histogram Equalization 
 
During the i-th iteration, the image Ii(x, y) can be 
represented as, 
 
where Ii-1(x, y) is the image from previous iteration, 
Ti is the threshold calculated (average pixel value) in 
the i-th iteration and Ei is the minimum pixel value in 
the i-th repetition. The procedure is repeated until the 
following criterion is satisfied: 
                        | Ti – Ti-1 | < 1                                (3)    
 
                      
           (a)   (b) 
Figure 3: (a) Degraded portion and (b) Output after 8th 
iteration of IGT on degraded portion of image 
 
4.3 Otsu Threshold Selection and Binarization 
 
Otsu threshold selection algorithm is used to select 
an optimal threshold to binarize the output image from 
the previous thresholding step.   
 
4.4 Detection of Areas with Remaining Noise 
and Local Thresholding 
 
From binarized image got from second step, the 
areas that still contain noise are detected by dividing 
the image into fixed size segments and computing the 
black pixel frequency of each of the segments [6]. The 
detected segments are those which satisfy the 
following criterion: 
                       F(S) > m + ks                             (4) 
 
where F(S) is the black pixel frequency of segment, 
S and m and s are the mean and standard deviation of 
black pixel frequency considering all segments, 
respectively. Figure 4 shows detected areas in red box. 
 
 
Figure 4: Detected areas with remaining noise 
 
Iterative global thresholding is reapplied to the 
detected areas separately until the stopping criterion 
mentioned in equation (3) is satisfied. Figure 5 shows 
the output of local thresholding.  
 
 
   Figure 5: After local thresholding to detected areas 
 
4.5 Post Processing 
 
The results from IGT algorithm depends on the 
number of iterations and image characteristics. In some 
cases when the number of iterations is more, textual 
information may be affected which can lead to broken 
characters. Morphological dilation operator is applied 
to the binarized image to solve this problem and make 
it more legible.  A 3x3 or 4x4 filter can be used 
depending on the stroke width of text in the image. 
Figure 6 shows the structuring elements and Figure 7 
shows dilation output. 
 
 
 
 
 
 
 
 
                  (a)                     (b) 
Figure 6: (a) 3x3 and (b) 4x4 structuring element for 
dilation 
 
                               
                   (a)                                         (b) 
Figure 7: (a) Before dilation and (b) After dilation  
 
 
 
            255 * (Ti(x, y) - Ii-1(x, y)) 
Ii(x, y) = 255 –                                                   (2) 
                  255 - Ei 
0  1 0 
1 1 1 
0 1 0 
0 1 1 0
1 1 1 1
1 1 1 1
0 1 1 0
703
Authorized licensed use limited to: Agean University. Downloaded on March 20, 2009 at 11:06 from IEEE Xplore.  Restrictions apply.
               
                (a)            (b)    (c)        (d) 
 
                                  
  (e)            (f)              (g)                               
 
 
Figure 8: (a) Degraded input  image,  (b) After applying weighted median filter,  (c) Output after 5th iteration of IGT , (d) 
Binarized output using Otsu algorithm, (e) Detected areas with noise, (f) Local thresholding to noise detected areas, (g) 
Dilation using 3x3 structuring element 
 
 
5. Experimental Results 
 
The proposed approach has been tested on more 
than 50 degraded document images. The images 
considered for testing suffered from degradation 
problems like non uniform illumination, smear, ink 
fading, scanning noise etc.  
Figure 8(a) shows a degraded document image 
where the document is affected by paper deterioration 
and ink bleed through effect. The results of various 
stages of the approach, smoothing, application of IGT 
globally, Otsu’s binarization, detection of noisy areas, 
local thresholding and dilation to enhance text, are 
shown in Figure 8 (b-g). Figure 9 and 10 shows more 
examples which demonstrate the efficiency of the 
proposed hybrid thresholding methodology. 
 
The observations made after examining the results 
can be summarized as follows:- 
(i) The approach gave promising results 
when the noise was confined to a small 
area and when the noise was spread 
across the whole image. 
(ii) Background noise is removed while 
preserving the textual information 
(iii) Gaps between broken characters are filled 
making text more legible. 
(iv) Smoothing the image initially resulted in 
reduced number of iterations to reach 
stopping criterion, thus reducing 
computational cost. 
(v) When the gray level tones of background 
and foreground are similar, the textual 
information is getting affected 
704
Authorized licensed use limited to: Agean University. Downloaded on March 20, 2009 at 11:06 from IEEE Xplore.  Restrictions apply.
 
(a)                        
                      
 
(b) 
Figure 9: (a) Degraded input image (b) Noise removed 
text enhanced output image 
 
 
 
(a) 
 
 
(b) 
Figure 10: (a) Input image degraded from scanning noise 
(b) Enhanced output image  
 
6. Conclusion 
 
Document image binarization is an important 
preprocessing stage in most document imaging 
systems. The quality of the thresholding results affects 
further processing of the document image like 
segmentation and recognition.  In this paper we have 
presented a hybrid binarization approach by combining 
iterative global thresholding algorithm and Otsu’s 
threshold selection method and a post processing stage 
to improve the legibility of the document image. From 
the results, it is evident that the proposed hybrid 
approach is producing binarized image with 
background noise removed and enhanced textual 
information. We plan to do template matching and 
replacing of degraded characters as future work. 
 
References 
 
[1] Otsu, N. “A threshold selection method from gray-level 
histograms”. IEEE Trans. Systems Man, Cybernet. pp. 62-66, 
9 (1), 1979. 
[2] Sauvola, J., Pietikainen, M., “Adaptive Document Image 
Binarization”, Pattern Recognition, pp. 225-236, 33 (2000). 
 
[3] Gatos B., Pratikakis I. and Perantonis S.J. “An adaptive 
binarisation technique for low quality historical documents”. 
IAPR Workshop on  Document Analysis systems 
(DAS’2004), Lecture Notes in Computer Science (3163), 
Florence, Italy, pp. 102-113. 
 
[4] Leedham, G., S. Varma, A. Patankar, V. Govindaraju 
“Separating Text and Background in Degraded Document 
Images” Proceedings Eighth International Workshop on 
Frontiers of Handwriting Recognition, pp. 244-249, 
September, 2002. 
 
[5] E. Kavallieratou, E. Stamatatos, “Improving the Quality 
of Degraded Document Images”, Second International 
Conference on Document Image Analysis for Libraries 
(DIAL ’06), pp. 340-349, 2006 
 
[6] E. Kavallieratou, S. Stathis, “Adaptive Binarization of 
Historical Document Images”, 18th International  Conference 
on Pattern Recognition, ICPR 2006,  
Volume 3, 2006, pp. 742 – 745 
 
 
705
Authorized licensed use limited to: Agean University. Downloaded on March 20, 2009 at 11:06 from IEEE Xplore.  Restrictions apply.
