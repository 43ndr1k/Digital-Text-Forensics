 
 
 
PROGRAMA DE FOMENTO  
A LA INVESTIGACIÓN EDUCATIVA 
 
CONVOCATORIA SEP/SEBYN-CONACYT 2003 
FONDOS SECTORIALES SEP-SEBYN 
 
 
 
 
 
Nombre del proyecto:  Recopilación y Estructuración Automática de Contenidos 
Educativos Digitales a partir de la Web 
Clave:  SEPSEByN-2003-C01-40 
 
 
Título de la ponencia:     Reporte final 
 
 
Autores: 
Luis Villaseñor Pineda, Instituto Nacional de Astrofísica, Óptica y Electrónica. 
Liliana Morales Zanatta, Universidad Autónoma de Tlaxcala. 
Manuel Montes y Gómez, Instituto Nacional de Astrofísica, Óptica y Electrónica. 
Paolo Rosso, Universidad Politécnica de Valencia. 
Alberto Téllez Valero, Instituto Nacional de Astrofísica, Óptica y Electrónica. 
Rosa María Coyotl, Instituto Nacional de Astrofísica, Óptica y Electrónica. 
Manuel Alberto Pérez Coutiño, Instituto Nacional de Astrofísica, Óptica y Electrónica. 
 
Responsable: 
Luis Villaseñor Pineda 
Laboratorio de Tecnologías del Lenguaje 
Instituto Nacional de Astrofísica, Óptica y Electrónica 
Luis Enrique Erro No. 1, C.P. 72840, Tonantzintla, Puebla, México. 
Teléfono: [+52] (222) 266-3100 ext. 8306 
Fax: [+52] (222) 266-3152 
Email: villasen@inaoep.mx 
 
 
 
 
Recopilación y Estructuración Automática de Contenidos Educativos Digitales a partir de la Web 
 2 
Recopilación y Estructuración Automática de Contenidos Educativos Digitales a partir de la Web 
 3 
Recopilación y Estructuración Automática de Contenidos Educativos 
Digitales a partir de la Web 
 
Reporte técnico final (junio 2006) 
Luis Villaseñor Pineda1, Liliana Morales Zanatta2,3, Rosa María Coyotl1,  
Manuel Montes y Gómez1, Alberto Téllez Valero1,  
Manuel Alberto Pérez Coutiño1, Paolo Rosso4 
3 Instituto Nacional de Astrofísica, Óptica y Electrónica (INAOE), México. 
{villasen, mcoyotl, mmontesg, albertotellezv, mapco}@inaoep.mx 
1 Secretaría de Educación Pública del Estado de Puebla, México. 
2 Universidad Autónoma de Tlaxcala, México. 
lmzanatta@hotmail.com 
4 Universidad Politécnica de Valencia, España. 
prosso@dsic.upv.es 
 
 
Resumen 
El presente trabajo presenta los resultados alcanzados de un proyecto que aborda la 
necesidad de métodos innovadores para la elaboración de contenidos educativos en formato 
digital, usando la información disponible en Internet acorde a los temas y nivel de 
educación básica. Para ello fue necesario (i) realizar un estudio del alcance y tipo de 
contenidos disponibles en la Web relacionados con la curricula en educación básica; 
(ii) proponer y desarrollar nuevos métodos para buscar y recuperar páginas Web con 
contenido y lenguajes apropiados a la enseñanza de un tema en particular; (iii) definir 
mecanismos de organización y agrupamiento automáticos del conjunto de documentos 
recuperado, utilizando para ello las facilidades del hipertexto en un ambiente 
computacional. 
 
1. Introducción 
La Web es un conjunto de recursos enorme al cual podemos acceder de manera muy 
sencilla y que podemos aprovechar de muy diferentes maneras, por ejemplo, como lugar de 
encuentro virtual, como medio de comunicación, como fuente de información, etc. De 
particular interés es la posibilidad de participar directamente en la Web al poder compartir 
nuevos contenidos, nuestros propios contenidos. Sin embargo, uno de los grandes 
inconvenientes que tiene la Web como recurso pedagógico se deriva precisamente de la 
Recopilación y Estructuración Automática de Contenidos Educativos Digitales a partir de la Web 
 4 
dificultad para encontrar la información deseada. Es tal el cúmulo de información a nuestra 
disposición, que encontrar aquello que realmente nos interesa puede ser, en muchos casos, 
una tarea llena de sinsabores y sorpresas desagradables. Tras una búsqueda realizada hoy 
podemos encontrar una enorme cantidad de información; pero puede ser que mañana la 
misma búsqueda nos dé un resultado total o parcialmente distinto, debido entre otras cosas, 
a la tremenda volatilidad de la información que circula por la Red. Otro inconveniente, más 
grave que el anterior, es la dificultad para distinguir la “buena” de la “mala” información, 
es decir, cómo distinguir o discriminar entre la información de calidad de aquella poco 
seria. En la Web circula una gran cantidad de información, en muchos casos de autoría 
desconocida, lo que nos hace dudar de su calidad [5]. Así, un alumno se encontrará en una 
difícil situación, sin criterios claros que le sirvan para filtrar la pertinencia de la 
información encontrada.  
También es importante notar que deseamos no sólo encontrar documentos que 
respondan a nuestra pregunta, también deseamos que ese documento sea lo suficientemente 
claro para facilitar la asimilación de dicha información. En el caso particular de los 
alumnos de educación básica, el lenguaje usado debe ser sencillo y claro, por ejemplo es 
distinto el lenguaje que usa un estudiante de educación básica a un estudiante universitario. 
Por supuesto, en la Web, como gran biblioteca digital, no hace ninguna distinción sobre los 
niveles de un posible lector.  
Así llegamos a la problemática de la elaboración de materiales y contenidos propios, 
orientados a satisfacer necesidades de información precisas para individuos específicos. En 
esta problemática los primeros involucrados son los docentes, quienes conocen qué 
materiales y contenidos son los apropiados para determinado nivel y tema. Sin embargo, 
esta labor es extremadamente demandante. La idea detrás de este proyecto es la búsqueda 
de un punto intermedio, donde la elaboración de contenidos educativos digitales propios se 
apoye parcialmente en la información ya existente en la Web y en métodos 
computacionales, pero brindando al docente mecanismos pertinentes para la creación de 
una colección a partir de criterios específicos. 
En particular, este proyecto brinda al docente con nuevos elementos para la 
elaboración de contenidos educativos complementarios y actualizados a los materiales 
tradicionales. Tres problemas centrales se desean resolver con este proyecto. El primero es 
Recopilación y Estructuración Automática de Contenidos Educativos Digitales a partir de la Web 
 5 
establecer las características determinantes para identificar un documento como apropiado a 
un tema específico. Para ello se realizará un estudio sobre los contenidos disponibles en la 
Web orientados a la educación básica bajo dicha temática. El segundo problema es la 
definición de métodos de búsqueda y recuperación de contenidos pertinentes a partir de la 
Web. Un documento será considerado apropiado no sólo por corresponder al tema de 
interés sino también por el nivel de lenguaje usado, ya que dicho documento será leído por 
un alumnado de educación básica. El tercer problema es la estructuración y generación 
automática de hipertexto a partir de dichos contenidos. Es decir, se desean brindar 
mecanismos automáticos que simplifiquen el proceso de creación y actualización de 
contenidos digitales. 
2. Primera Etapa 
2.1. Estudio del alcance y tipo de contenidos disponibles en la Web  
En la primera etapa del proyecto se estudió en forma empírica el alcance y tipo de 
contenidos disponibles en la Web relacionados con la curricula en educación básica. Para 
ello se propuso una metodología para estudiar el alcance y el tipo de contenidos disponibles 
en la Web relacionados con un currículo determinado. Esta metodología se aplicó para la 
evaluación de los contenidos educativos disponibles en la Web relacionados con algunos 
temas del currículo de educación básica de México. Los resultados obtenidos indican que 
existen suficientes contenidos web sobre el currículo de nivel básico, pero que la gran 
mayoría de éstos no son mexicanos y mucho menos institucionales. A continuación se 
presenta la metodología usada y los resultados obtenidos. 
2.1.1. Antecedentes de la Web 
La educación es sin lugar a dudas un factor de primera importancia en toda sociedad 
moderna. Así se ha reconocido en México desde sus inicios como país independiente, y así 
ha quedado manifestado en el más reciente Plan Nacional de Desarrollo, donde se expresa 
la convicción del actual gobierno por hacer de la educación el gran proyecto nacional. Los 
retos de la educación en México pueden expresarse mediante tres principios fundamentales: 
educación para todos, educación de calidad y educación de vanguardia [1]. Sin duda alguna 
el uso y aplicación de las nuevas tecnologías en la educación juega un papel importante en 
Recopilación y Estructuración Automática de Contenidos Educativos Digitales a partir de la Web 
 6 
la consolidación de estos retos. En particular el uso de Internet como herramienta educativa 
facilitará la educación a distancia y la implantación de los programas de superación 
continua de los profesores, ayudará en las tareas de administración educativa, y por 
supuesto, “consolidará una educación de calidad y vanguardia con acceso al mayor 
repositorio de información jamás construido por el hombre” [2, 3, 4]. 
Actualmente existe la creencia, casi generalizada, de que la Web (servicio de 
Internet que consiste de un conjunto páginas conectadas con enlaces de hipertexto) es una 
gran biblioteca, y que sus contenidos pueden satisfacer prácticamente cualquier necesidad 
de información por más específica o rara que ésta sea. Sin embargo factores como la 
diversidad de contenidos e idiomas, su gran tamaño, y la carencia de herramientas 
adecuadas para buscar información complican su aplicación. 
Además debemos recordar que la Web surgió como un proyecto científico-
académico, pero con el tiempo se ha transformado en un medio masivo de información 
claramente comercial y de entretenimiento. Esta situación implica grandes inconvenientes 
para el uso de la Web como recurso pedagógico. Por una parte de ello se deriva la dificultad 
para encontrar la información deseada, puesto que los contenidos educativos se encuentran 
generalmente inmersos en un mar de información irrelevante y muchas veces incluso 
inapropiada para los estudiantes. Por otra parte, y más grave aún, muchos de estos 
contenidos son de autoría desconocida y por ende de dudosa calidad [5]. 
A estos problemas se suma la poca representatividad de nuestra lengua en la Web. 
Estadísticas recientes señalan que solamente un 3% de las páginas web existentes están en 
Español [6]. Así pues, aún persiste la duda de si la Web es o no es una gran biblioteca –en 
Español– con fines educativos. 
Este artículo confronta esta inquietud. Básicamente se enfoca en el diseño de una 
metodología para la evaluación de los contenidos educativos disponibles en la Web 
relacionados con un currículo particular. A partir de dicha evaluación se pretende contestar 
las preguntas como: ¿existen suficientes contenidos educativos disponibles en la Web 
relacionados con el currículo predefinido? ¿estos contenidos están distribuidos 
uniformemente sobre todos sus temas? ¿son ellos veraces y de calidad? 
Asimismo, el artículo presenta los resultados obtenidos al aplicar la citada 
metodología en la evaluación de los documentos web relacionados con los temas propios 
Recopilación y Estructuración Automática de Contenidos Educativos Digitales a partir de la Web 
 7 
del sexto año de la educación básica de México. La elección de este dominio de estudio 
estuvo motivada por nuestra creencia en que los temas del nivel básico son los menos 
cubiertos en la Web, así como por la necesidad de considerar el uso de un lenguaje 
apropiado para niños en su evaluación. 
A continuación, en la sección 2.1.2, se describe la metodología propuesta para 
evaluar el contenido educativo disponible en la Web. En la sección 2.1.3, se presentan los 
resultados preliminares de la evaluación de los contenidos web relacionados con el 
currículo del sexto año de primaria de México. Finalmente, en la sección 2.1.4, se discuten 
los resultados obtenidos y se establecen nuestras primeras conclusiones. 
2.1.2. Metodología de evaluación 
Es evidente que Internet está adquiriendo día a día una mayor relevancia y presencia 
en el sector educativo. Su uso como complemento informativo se sostiene en la creencia de 
que la Web es una gran biblioteca. Sin embargo diversos factores están complicado su 
adecuada aplicación, y con ello la duda sobre si es realmente una gran biblioteca está 
emergiendo. 
La metodología que se describe a continuación pretende facilitar el estudio del 
alcance y tipo de contenidos disponibles en la Web relacionados con un currículo 
determinado. El objetivo de dicho estudio es evaluar la disponibilidad de contenidos, así 
como su calidad y riqueza informativa. 
Es claro que una evaluación completa y precisa del contenido educativo disponible 
en la Web sólo se lograría si se pudieran visitar todas sus páginas. Dado que esto es 
imposible, pues su tamaño es de varios miles de millones de páginas, es necesario plantear 
una metodología inversa para esta evaluación. 
La metodología que proponemos parte de un conjunto de peticiones de temática 
educativa, y se realiza sobre los documentos recuperados por los buscadores para dichas 
peticiones. Además, esta metodología tiene un sentido pragmático puesto que los conjuntos 
de peticiones, las peticiones en sí mismas, y los documentos revisados son pequeños, pero 
ajustados a las costumbres de uso de la Web. 
La metodología de evaluación propuesta consiste de los siguientes pasos: 
 
Recopilación y Estructuración Automática de Contenidos Educativos Digitales a partir de la Web 
 8 
1. Seleccionar la máquina de búsqueda con la que se realizará el estudio. 
Esta selección se basará en un estudio comparativo de los principales buscadores 
disponibles en la Web. Algunas de las características sugeridas para dicha comparación 
son: su cobertura (i.e., tamaño del índice), su popularidad, y grado de frescura, es decir, la 
frecuencia de actualización de su índice. 
En los casos que se desee usar mas de una máquina de búsqueda para el estudio 
también deberá considerarse su nivel de traslape, es decir, el grado de intersección de sus 
índices, intentando seleccionar el conjunto de buscadores cuya unión produzca una mayor 
cobertura de la web. 
 
2. Definir un conjunto muestra de peticiones de temática educativa. 
Esta definición deberá considerar entre otros los siguientes criterios. Primero, las 
peticiones tendrán que obtenerse de forma directa de los libros de texto. Segundo, deberán 
cubrir adecuadamente toda la curricula de los grados académicos seleccionados. Tercero, 
deberán reflejar distintos tipos de necesidades de información, entre otras: 
• Cronológicas: cuya respuesta es información biográfica sobre algún personaje o 
cronológica sobre un evento. 
• Definitorias: cuya respuesta debe incluir la definición, las características y algunos 
ejemplos del concepto en cuestión. 
• Procedurales: cuya respuesta debe incluir la descripción de las etapas o fases del 
tema en cuestión, así como sus partes o participantes, causas y consecuencias. 
Por otra parte, las peticiones deberán ser preferentemente cortas, pues las 
costumbres de uso de la Web señalan que los buscadores reciben en promedio peticiones de 
dos palabras [7]. 
 
3. Definir los criterios para la clasificación y evaluación de los documentos. 
Para cada página web recuperada nos interesa determinar por lo menos: (i) su 
relevancia, es decir, si tiene contenido educativo; (ii) su adecuación, es decir, si el 
contenido que presenta es del nivel deseado, por ejemplo de nivel básico para nuestro caso 
Recopilación y Estructuración Automática de Contenidos Educativos Digitales a partir de la Web 
 9 
de estudio; y (iii) si es complementaria a los contenidos oficiales, es decir, si expone algo 
adicional a lo contenido en los libros de texto. 
Además nos interesa calificar la calidad del contenido educativo de cada página 
recuperada. Una manera indirecta pero sencilla de hacerlo es determinado si dicho 
contenido fue creado por una institución mexicana de carácter gubernamental o docente. 
 
4. La consulta manual de la Web y el análisis estadístico de los datos. 
Esta consulta se realizará a partir del buscador y conjunto de peticiones definidos en 
los pasos anteriores. Para cada petición se revisarán los primeros 20 resultados regresados 
por la máquina de búsqueda. Este número tiene un sentido pragmático y está sustentado en 
las costumbres de uso de la Web, que señalan que los internautas revisan en promedio 
solamente entre las 10 y 15 primeras páginas retornadas por los buscadores [7]. 
El contenido de cada página recuperada deberá analizarse y caracterizarse según los 
criterios definidos en el paso 3. Los datos obtenidos deberán contabilizarse y analizarse 
estadísticamente de tal forma que pueda cuantificarse la disponibilidad de contenidos 
educativos en la Web, así como su distribución por temas y calidad. 
2.1.3. Resultados del estudio 
La metodología propuesta en la sección anterior se aplicó para la evaluación de los 
contenidos educativos disponibles en la Web relacionados con los temas propios del 
currículo de sexto año de primaria de México. A continuación se describen los principales 
elementos de dicha evaluación y se presentan los resultados obtenidos. 
En primer lugar se seleccionó el buscador Google1 para conducir el experimento. 
Esta decisión se basó en los siguientes datos [8, 9]: (i) Google es actualmente el buscador 
con el mayor índice, aproximadamente 4 mil millones de páginas web; (ii) es el buscador 
más usado a nivel mundial, recibe diariamente alrededor de 250 millones de consultas, y 
(iii) es uno de los buscadores que actualiza su índice más frecuentemente, en promedio una 
vez por mes. 
En segundo lugar se decidió analizar los contenidos relacionados con las materias de 
matemáticas, historia de México, ciencias naturales y geografía, quedando pendiente la 
                                                 
1 www.google.com.mx 
Recopilación y Estructuración Automática de Contenidos Educativos Digitales a partir de la Web 
 10 
asignatura de español. Para cada materia se definió un conjunto de peticiones siguiendo las 
sugerencias establecidas en el paso 2 de nuestra metodología (referirse a la sección 2). Por 
ejemplo, para el caso de historia de México se definieron peticiones como: “Miguel 
Hidalgo”, Dictadura, e “Imperio de Maximiliano”. En promedio se realizaron 44.5 
consultas por materia. 
Finalmente se realizó la consulta manual a la Web, analizando las respuestas de 
Google de acuerdo con lo establecido en el paso 3 de la metodología. Las siguientes figuras 
resumen los resultados obtenidos. 
La figura 1 indica el porcentaje de páginas relevantes por materia encontradas entre 
las primeras 20 respuestas de Google. Esta gráfica señala que los contenidos educativos 
están distribuidos mas o menos uniformemente entre las distintas materias, siendo historia 
de México la materia más cubierta y geografía la menos. 
20%
38%
30%
15%
0%
5%
10%
15%
20%
25%
30%
35%
40%
Matemáticas Historia de
México
Ciencias
Naturales
Geografía
 
Figura 1. Porcentaje de páginas relevantes por materia 
La figura 2 por su parte muestra la distribución de los contenidos disponibles en la 
web por tipo de necesidad de información. Aquí puede observarse que en la Web existe 
más información de tipo cronológico, que definitoria o descriptiva (por lo menos para el 
nivel de educación básica). En parte esta situación explica los resultados mostrados en la 
figura 1, pues la materia de historia de México contiene más peticiones de tipo cronológico 
que el resto, y a su vez, la asignatura de geografía estuvo modelada solamente por 
preguntas de tipo definición. 
Recopilación y Estructuración Automática de Contenidos Educativos Digitales a partir de la Web 
 11 
46%
20%
31%
0%
5%
10%
15%
20%
25%
30%
35%
40%
45%
50%
Cronológias Definitorias Procedurales  
Figura 2. Porcentaje de páginas relevantes por tipo de consulta 
En la figura 3 se presentan algunas características de los contenidos marcados como 
relevantes. Curiosamente para las materias de matemáticas y geografía existen menos 
contenidos disponibles, pero estos son en su gran mayoría apropiados para niños y 
complementarios al libro de texto, es decir, son contenidos que usan un lenguaje sencillo y 
que además agregan información sobre los temas en cuestión. Esta situación no es tan 
evidente para las otras dos materias. Otro aspecto importante a resaltar es que muy pocos 
contenidos fueron claramente reconocidos como mexicanos e institucionales. En este caso, 
tal como la lógica lo indicaba, se encontraron más páginas mexicanas sobre los temas de 
historia de México que sobre los tópicos de otras materias de interés internacional. 
2.1.4. Conclusiones del estudio  
Hasta ahora todos los comentarios y opiniones relacionados con el uso de la Web 
como complemento informativo en la educación se han sustentado en creencias y 
observaciones aisladas mas que en datos concretos. El gran aporte de este trabajo radica en 
la propuesta de una metodología simple y fácilmente reproducible para la evaluación de los 
contenidos educativos disponibles en la Web relacionados con un currículo o tema 
determinado. La característica más importante de esta nueva metodología es, en nuestra 
opinión, su sentido pragmático ajustado a las costumbres de uso de la Web. 
Recopilación y Estructuración Automática de Contenidos Educativos Digitales a partir de la Web 
 12 
La aplicación de dicha metodología en la evaluación de los contenidos educativos 
relacionados con los temas propios del sexto año de educación básica de México arrojó 
resultados interesantes: 
En primer lugar podemos concluir que “la Web contiene suficientes contenidos 
educativos relacionados con el currículo de nivel básico”. Aproximadamente un 25% de las 
primeras páginas retornadas por Google son relevantes a los temas de la educación básica, 
es decir, 5 páginas por petición. Mientras que solamente para un 9% (16 de 178) de los 
temas analizados no se obtuvo ninguna información relevante. 
 
 
 
 
 
 
 
 
Figura 3. Características de las páginas relevantes 
En contraposición con este resultado favorable observamos que “el problema de 
selección de información sigue presente”. La información apropiada se encuentra inmersa 
dentro de una considerable cantidad de información no relevante (en una razón de 1 a 4), e 
incluso se corre el riesgo de que aparezcan algunos contenidos inapropiados. A manera de 
ejemplo podemos citar que para la petición “lluvia”, del área de geografía, se encontraron 3 
páginas con contenido pornográfico dentro de las 10 primeras respuestas de Google. 
Además, hecho que resulta realmente preocupante, “la gran mayoría de las páginas 
con información relevante no son mexicanas y mucho menos institucionales”. Esta 
observación implica que el contenido de dichos documentos no está necesariamente 
validado y podría ser incorrecto, además de que muy probablemente no está adaptado al 
entorno del estudiante mexicano. También demuestra el terrible rezago de nuestro país en el 
92%
77%
71%
89%
100%
95%
97%97%
16%
57%
10%
21%
8%
25%
7%
14%
0%
10%
20%
30%
40%
50%
60%
70%
80%
90%
100%
Relevantes
Apropiadas
Reelevantes
Complementarias
Relevantes
Mexicanas
Relevantes
Institucionales
Matemáticas Historia de México Ciencias Naturales Geografía
Recopilación y Estructuración Automática de Contenidos Educativos Digitales a partir de la Web 
 13 
uso y aplicación de las nuevas tecnologías en la educación, realidad que de no cambiar a 
corto plazo podría causar que los contenidos educativos en Español, relacionados con 
nuestra historia, cultura y entorno geográfico desaparezcan para las nuevas generaciones. 
 
Referencias (primera etapa) 
 
1. Secretaria de Educación Pública de México (2001). Programa Nacional de Educación 2001-
2006. Sept 2001. 
2. Adell, J. (1996). Internet en educación: una gran oportunidad. Revista Net Conexión, nº11, 
Sept 1996. 
3. Gómez Galán, J. (2002). La World Wide Web (WWW) en la Educación. Educación Social y 
Nuevas Tecnologías. (pp. 120- 145). Sevilla: Kronos, 2002. 
4. Morales Zanatta, L. (2002). La tecnología aplicada a la educación del Siglo XXI. Revista 
Identidad Magisterial, Febrero 2003. 
5. Gómez Galán, J. (2001). Internet: ¿Realmente Una Herramienta Educativa?. I Congreso 
internacional de Educared, 2001. 
6. O'Neill, E. T., Lavoie, B. F., y Bennett, R. (2003). Trends in the Evolution of the Public Web 
1998-2002. D-Lib Magazine, Volume 9, Number 4, April 2003. 
7. Jansen, B. J. Spink, A., y Saracevic, T. (2000). Real Life, real users, and real needs: a study 
and analysis of user queries on the web. Information Processing and Management, 36(2), 
2000. 
8. Sullivan, D. Search Engine Watch. http://ww.searchenginewatch.com/ 
9. Notess, G. Search Engine Showdown. http://searchengineshowdown.com/ 
 
Recopilación y Estructuración Automática de Contenidos Educativos Digitales a partir de la Web 
 14 
3. Segunda etapa 
3.1. Búsqueda y Filtrado de documentos pertinentes 
Durante la segunda etapa del proyecto se abordó el problema de recuperación de 
documentos pertinentes de Internet. Esta etapa considera dos retos a resolver: (i) la 
identificación de documentos por su temática; y, (ii) la identificación de documentos por el 
lenguaje empleado. En las siguientes secciones se presentan las soluciones propuestas. El 
método empleado aprovecha los avances actuales en técnicas de clasificación automática de 
textos para filtrar la información relevante a una temática, además de ubicar la información 
de acuerdo a un tipo de contenido y nivel del lector.  
3.1.1. Antecedentes 
Afrontando el problema que implica encontrar la información deseada en Internet 
desde el punto de vista didáctico, en este trabajo nos enfocamos en el uso de técnicas de 
clasificación automática de textos para filtrar páginas Web con información relevante a un 
cierto contenido educativo. En otras palabras, lo que se quiere es determinar de forma 
automática si cierta información debe formar parte del contenido, lo cual es un problema 
donde la clasificación automática de textos parece ser la solución adecuada, tomando en 
cuenta que dichas técnicas se encuentra actualmente en una etapa madura, y algunas de sus 
aplicaciones logran resultados comparables al desempeño humano [9].  
Tradicionalmente la clasificación automática busca determinar a que categoría 
pertenece un documento dado un conjunto de clases previamente establecido. La 
clasificación se basa en el supuesto que cada categoría puede ser identificada por los 
términos que emplea y la frecuencia de ocurrencia de éstos. Tal suposición es oportuna si 
las categorías están claramente diferenciadas; por ejemplo, si deseamos distinguir entre 
documentos técnicos sobre aeronáutica y documentos descriptivos de biología. Este 
enfoque es el utilizado en la clasificación automática de los 5 temas relevantes a la 
educación básica usados como ejemplo en este proyecto. Sin embargo, este enfoque no 
resultó ser del todo apropiado para clasificar documentos por el lenguaje usado; de ahí que 
se desarrollará un segundo método propio para este tipo de clasificación.  
Recopilación y Estructuración Automática de Contenidos Educativos Digitales a partir de la Web 
 15 
Las secciones subsecuentes describen, en primer lugar, los detalles del método de la 
clasificación temática y presentan los resultados al aplicar este método a diferentes 
conjuntos de documentos. Posteriormente, se presenta los detalles del método de 
clasificación por lenguaje usado, o clasificación por estilo, así como los resultados 
alcanzados. 
3.1.2. Clasificación Temática de Textos 
La tarea de clasificar un texto consiste en ubicar adecuadamente un documento 
escrito en lenguaje natural dentro de un conjunto de categorías previamente definido. La 
automatización de esta tarea comienza a tener fines prácticos a partir de los años 90’s, esto 
se debe principalmente a la convergencia de tecnologías como son: la recuperación de 
información, el aprendizaje automático y el procesamiento del lenguaje natural desde un 
punto de vista estadístico. Dentro de este enfoque, un proceso inductivo (el aprendiz) es 
provisto con un conjunto de documentos preclasificados de acuerdo a las categorías de 
interés. Entonces, por medio de observar las características de los documentos de 
entrenamiento, el aprendiz puede generar un modelo (el clasificador) de las condiciones 
que deben ser satisfechas por los documentos para pertenecer a las categorías consideradas. 
Finalmente, el clasificador puede ser aplicado a nuevos documentos de los cuales se 
desconoce su categoría (para más detalles referirse a [9]). 
Basados en la técnica descrita anteriormente, en este trabajo nos enfocamos en 
desarrollar un clasificador que permitiera ubicar páginas Web en categorías temáticas 
relacionadas a los contenidos educativos del nivel básico, como son: ciencias naturales, 
español, geografía, historia y matemáticas. Además, siguiendo el mismo proceso también 
se desarrollaron clasificadores que permitieran conocer el tipo de contenido (definiciones o 
biografías), así como el nivel del lenguaje utilizado, esto para decidir si un documento es o 
no adecuado para el nivel básico. La colección de documentos de entrenamiento utilizada 
para crear los clasificadores consistió de 1,020 documentos obtenidos desde Internet (ver 
tabla 1), de los cuales 613 contienen definiciones y el resto biografías (en [6] se describe el 
proceso de recuperación). 
 
 
Recopilación y Estructuración Automática de Contenidos Educativos Digitales a partir de la Web 
 16 
Tabla 1. Corpus de entrenamiento 
Nivel del lenguaje 
Adecuado Temas 
definiciones bibliografías 
No adecuado 
Ciencias 
naturales 179 8 78 
Español 111 4 8 
Geografía 110 4 16 
Historia 244 14 76 
Matemáticas 156 0 12 
 
3.1.3. Representación de los Documentos 
El aprendiz, discutido en la sección anterior, no puede operar directamente sobre los 
documentos, es decir, requiere de una representación interna que le proporcione algún 
sentido de los mismos. Esta misma representación debe ser proporcionada al clasificador 
una vez que ha sido generado por el aprendiz. La representación más comúnmente usada, y 
que también utilizamos en este trabajo, es el llamado modelo vectorial [9]. En este modelo 
los documentos son representados por vectores de palabras en un espacio de 
dimensionalidad n, siendo n el número de términos diferentes que ocurren en el corpus de 
entrenamiento (el vocabulario). Generalmente, en tareas de clasificación de textos del tipo 
temático se busca que el vocabulario coincida con aquellas palabras que tienen relación con 
los temas analizados. Por lo tanto, en este proyecto a cada uno de los documentos en la 
colección de entrenamiento se les aplico el siguiente pre-procesamiento: 
• Eliminación de etiquetas HTML 
• Eliminación de términos no alfabéticos (e.g., símbolos de puntuación, números, entre 
otros) 
• Eliminación de palabras vacías (e.g., pronombres, preposiciones, conjunciones, etc.). El 
resultado fue un vocabulario de 54,226 palabras. 
Una vez definido el vocabulario, el siguiente paso es establecer los valores en el 
vector, es decir, reflejar la importancia que cada una de las palabras del vocabulario tiene 
para determinar la semántica de cierto documento. Usualmente estos valores son calculados 
Recopilación y Estructuración Automática de Contenidos Educativos Digitales a partir de la Web 
 17 
por funciones de pesado del tipo estadístico, por ejemplo calcular la frecuencia de un 
término en el documento (en [9] se presenta un resumen de las diferentes funciones de 
pesado). En nuestro caso la función de pesado utilizada es una de las más simples pero 
también de las más ampliamente usadas, llamada ponderado booleano, y consiste en asignar 
uno si la palabra analizada aparece en el documento y cero en caso contrario. 
Para terminar esta sección cabe discutir que a pesar de que las técnicas empleadas 
en la representación de los documentos pueden parecer extremadamente primitivas (desde 
el punto de vista del análisis lingüístico), en intentos por utilizar un análisis sofisticado 
(e.g., análisis sintáctico [7], extracción de colocaciones [3] o desambiguación del sentido de 
las palabras [2]) no han presentado beneficios substanciales con respecto a representaciones 
básicas como la utilizada en este trabajo. 
 
Tabla 2. Términos con mayor GI en el corpus 
 
Tema Palabras GI 
Ciencias 
naturales 
Salud 
Medicina 
Síndrome 
0.118 
0.110 
0.101 
Español Palabra 
Verbo 
Adjetivo 
0.197 
0.164 
0.144 
Geografía Zona 
Continental 
Energía 
0.098 
0.086 
0.068 
Historia Independencia 
Gobierno 
Ejército 
0.284 
0.265 
0.245 
Matemáticas Decimal 
Número 
Suma 
0.124 
0.111 
0.095 
 
3.1.4. Reducción de Dimensionalidad 
Un problema central en la clasificación de textos es la alta dimensionalidad del 
espacio de características (i.e., el tamaño de vocabulario), lo que provoca un procesamiento 
extremadamente costoso en términos computacionales. Además, los resultados en 
ocasiones llegan a ser poco confiables debido a las deficiencias en los datos de 
Recopilación y Estructuración Automática de Contenidos Educativos Digitales a partir de la Web 
 18 
entrenamiento al tomar en cuenta palabras con poca información del dominio, por ejemplo 
palabras con una frecuencia igual a uno en toda la colección. Por tal motivo, existe la 
necesidad de reducir el espacio de características originales, es decir, hacer una reducción 
de dimensionalidad. 
Existen varias técnicas para reducir la dimensionalidad en la clasificación de textos 
(ver [9] para un resumen). En este trabajo empleamos una de las técnicas más efectivas 
según los estudios presentados en [10], la cual forma parte del conjunto de métodos que 
reducen la dimensionalidad por medio de seleccionar desde el conjunto original de 
características (i.e., las palabras del vocabulario) aquellas que mejor apoyan la tarea de 
clasificación. La técnica utilizada es conocida como Ganancia en la Información (GI), y su 
trabajo es medir los bits de información obtenidos para predecir la clase por medio de la 
presencia o ausencia de ciertas palabras en el documento. Por lo tanto, la forma en que se 
reduce el espacio de características con este método consiste en calcular la GI para cada 
una de las palabras en el vocabulario, y finalmente aplicar un umbral en los valores para 
tomar sólo aquellos términos con una mayor ganancia. En la tabla 2 se exponen los tres 
términos con mayor GI en el vocabulario para cada uno de los temas en la colección, donde 
se puede apreciar que la GI encuentra términos con sentido para la clasificación. 
El uso de GI implica realizar un conjunto de pruebas con respecto a construir 
diferentes clasificadores para medir su efectividad y así determinar cuál umbral es el 
adecuado, en la tabla 3 se presenta el resultado de las pruebas, donde se puede ver que la 
reducción fue en promedio aproximadamente del 94% del vocabulario. 
Tabla 3. Reducción de dimensionalidad 
 
Clasificador de 
contenido 
Dimensión 
Original 
Dimensión 
Reducida 
Temático 54,226 5,766  (GI>0) 
Tipo 51,918 3,037  (GI>0.08) 
Nivel de lenguaje 54,226 813     (GI>0.15) 
 
 
3.1.5. Resultados de la clasificación temática 
Como se describió en párrafo anteriores, el enfoque utilizado en este trabajo para la 
construcción de los clasificadores de texto requiere que se defina el aprendiz para generar el 
Recopilación y Estructuración Automática de Contenidos Educativos Digitales a partir de la Web 
 19 
clasificador. En el estado de la tecnología muchos esquemas basados en aprendizaje 
supervisado han sido utilizados (ver [9] para un resumen). Sin embargo, en años recientes 
los esquemas conocidos como: máquinas de vectores de soporte (SVM) [1], boosting 
(AdaBoost) [8] y Bayes (naiveBayes) [4], son los que han dominado en la clasificación de 
textos. Por tal motivo, en este trabajo enfocamos nuestros experimentos en estos tres 
métodos. Cabe destacar que para comparar los diferentes esquemas de aprendizaje se 
utilizó la medida F, que representa una combinación lineal de la precisión y la cobertura, 
medidas tradicionales en la clasificación de textos [9]. Además, en la evaluación se utilizó 
el método de validación cruzada con 10 pliegues (10 Fold Cross Validation, en inglés). Esto 
debido a que es considerada una de las técnicas más realistas y adecuadas cuando no se 
cuenta con un conjunto de prueba [5]. 
La figura 1 muestra una tabla comparativa con los resultados obtenidos en la 
clasificación de documentos por temas. Como puede observarse los resultados en general 
de la clasificación alcanzan los mejores resultados con el algoritmo SVM (91% en 
promedio); mientras que naiveBayes es el peor de los tres, excepto en la categoría de 
historia, donde los tres esquemas reportan resultados similares. 
0.5
0.55
0.6
0.65
0.7
0.75
0.8
0.85
0.9
0.95
1
Ciencias
naturales
Español Geografia Historia Matematicas
M
ed
id
a
 F
SVM AdaBoost naiveBayes
Figura 1. Clasificación temática 
 
 
 
Recopilación y Estructuración Automática de Contenidos Educativos Digitales a partir de la Web 
 20 
0.95
0.96
0.97
0.98
0.99
1
Definiciones Biografías
M
ed
id
a 
F
SVM AdaBoost naiveBayes
Figura 2. Clasificación de tipo 
 
 
0.7
0.75
0.8
0.85
0.9
0.95
1
Adecuado No adecuado
M
ed
id
a 
F
SVM AdaBoost naiveBayes
Figura 3. Clasificación de nivel de lenguaje 
 
Con respecto a la tarea de verificar si el contenido expresa definiciones o biografías, 
los resultados comprueban que SVM es el más adecuado (casi el 100% en promedio, ver 
figura 2). En este caso palabras como nació y murió son las que mayor GI obtienen. 
Finalmente, se utilizó el mismo método de clasificación para identificar los 
documentos por el tipo de lenguaje empleado. Con ello se desea distinguir entre los 
Recopilación y Estructuración Automática de Contenidos Educativos Digitales a partir de la Web 
 21 
documentos con un lenguaje sencillo y adecuado al nivel básico. En la figura 3 se exponen 
los resultados alcanzados. Como puede observarse es el caso con resultados más bajos. En 
este caso, el enfoque tradicional no es del todo apropiado. Como se ve en la figura 3 en una 
de las dos clases se tiene un comportamiento menor al 75%. Es por eso que se planteó un 
nuevo método orientado a distinguir los documentos por su estilo de redacción. En las 
secciones subsecuentes se presenta este segundo método. 
3.1.6. Clasificación de textos por estilo 
El caso de clasificación de textos por el lenguaje empleado puede ser visto como un 
caso particular de un problema más general: la clasificación por estilo. Antes de detallar el 
método de clasificación por estilo es necesario definir que entendemos por estilo dada las 
muchas acepciones del término. Por estilo nos estamos refiriendo a la elección que debe 
hacer un autor entre cierto número de disponibilidades contenidas en la lengua. De esta 
manera, un autor plasma en un texto una idea escogiendo un conjunto de términos con los 
cuales transmite dicha idea. En nuestro caso, se desea identificar textos cuyos autores han 
expresado sus ideas usando aspectos verbales y léxicos que facilitan su entendimiento. Es 
decir, textos más fáciles de leer y por ende de entender. Estos textos más legibles, más 
fáciles requieren menor tiempo, atención y esfuerzo por parte del lector. 
La clasificación por estilo se orienta a identificar textos por su estructura no por su 
contenido. En este tipo de clasificación no nos interesa el tema sobre el que versa el 
documento, nos interesan los términos, y sus combinaciones, para intentar determinar su 
estructura.  
Para proponer un método para este tipo de clasificación fue necesario cambiar las 
condiciones para su evaluación. En específico se evalúo el método en un caso más exigente 
de clasificación por estilo: la atribución de autoría.  
La atribución de autoría es un problema de clasificación, donde un conjunto de 
documentos con autores conocidos son utilizados para entrenamiento de modelos para, 
posteriormente, determinar automáticamente el autor de un texto anónimo. En contraste con 
otras tareas de clasificación textual, en este caso no es sencillo determinar el conjunto de 
características que deben ser utilizadas para identificar un autor. Así, el desafío principal de 
Recopilación y Estructuración Automática de Contenidos Educativos Digitales a partir de la Web 
 22 
esta tarea es la definición de una caracterización apropiada de los documentos que capture 
el estilo de escritura de los autores. 
Existen varios métodos de atribución automática de autoría, desde aquellos que usan 
características estilísticas tales como la riqueza del vocabulario del autor o la frecuencia de 
ocurrencia de algunas palabras funcionales, hasta aquellos que utilizan la representación 
tradicional de bolsa de palabras e intentan clasificar los documentos basándose en las 
palabras de contenido [15, 18]. En las secciones subsecuentes se presenta un nuevo método 
para la atribución de autoría. Este método descansa en la hipótesis de que una apropiada 
identificación de los autores debe considerar tanto el estilo de redacción como el tema 
abordado en los textos. Por tanto, una adecuada caracterización de los documentos debe 
combinar eficazmente las palabras funcionales y de contenido. Nuestra propuesta es 
construir dicha caracterización a través de secuencias de palabras. 
Es importante mencionar que las secuencias de palabras (especialmente, los n-
gramas de longitud fija) se han aplicado sin mucho éxito en la clasificación temática de 
textos [13]. Sin embargo, no hay suficientes estudios de su aplicación en la clasificación 
por estilo, y en particular en la tarea de la atribución de autoría [20]. 
Otra condición también poco estudiada es el impacto del tamaño de los documentos 
en la clasificación por autor. Se sabe que algunos enfoques para la tarea de atribución de 
autoría son muy sensibles a la longitud de los documentos. Especialmente, los métodos 
basados en características estilométricas tienden a fallar cuando confrontan documentos 
cortos [21]. Este comportamiento nos motiva a aplicar nuestro método en la clasificación de 
poemas por autor. Dado que los poemas son generalmente textos muy cortos, nuestros 
experimentos no sólo contribuyen a evaluar la utilidad de las secuencias de palabras como 
características para la atribución de autoría, sino también permiten analizar su adecuación 
para enfrentar escenarios complicados de clasificación. 
La siguiente sección presenta algunos trabajos previos relacionados a la tarea de 
clasificación de textos por autor. Posteriormente se introduce el método propuesto y se 
describen los experimentos realizados. Finalmente, se concluye con algunos comentarios 
sobre el método.  
 
 
Recopilación y Estructuración Automática de Contenidos Educativos Digitales a partir de la Web 
 23 
3.1.7. Trabajo Relacionado 
El análisis de estilo para la atribución de autoría se sostiene principalmente en la 
hipótesis de que cada autor tiene sus propios hábitos de redacción (es decir, sus propias 
prácticas en el manejo de las palabras) que marcan de manera única su escritura. Sin 
embargo, esta suposición no es completamente verdadera, pues el estilo de un autor puede 
variar dependiendo de la audiencia a quien dirige su escrito, o puede cambiar por el tema o 
género literario tratado. Por esta razón, es difícil determinar un conjunto adecuado de 
características invariantes que permita distinguir entre diferentes autores. 
Existen varios métodos para clasificación automática de textos por autor. Estos 
métodos pueden agruparse en los siguientes tres enfoques principales: 
Medidas estilométricas. Este enfoque considera características tales como la longitud de las 
palabras y oraciones, así como la riqueza del vocabulario [17, 19]. Los resultados 
alcanzados con este esquema de caracterización no son del todo concluyentes. Sin embargo, 
son extremadamente sensibles a la longitud de los textos; perdiendo su validez para el 
tratamiento de textos cortos. 
Marcadores sintácticos. Bajo este enfoque la identificación del autor se realiza a través de 
ciertos marcadores de estilo. Estos marcadores describen la estructura del lenguaje usado 
por un autor. Para determinar el conjunto de marcadores más apropiados se realiza un 
análisis sintáctico profundo de los documentos [14, 15, 21]. Básicamente, los textos son 
caracterizados por la presencia y frecuencia de ciertas estructuras sintácticas. Esta 
caracterización es muy detallada y relevante, desafortunadamente es computacionalmente 
cara y en ciertos casos imposible de realizar dada la ausencia de recursos lingüísticos 
apropiados para el lenguaje en cuestión. 
Caracterización léxica. Este enfoque incluye por lo menos tres métodos diferentes. En el 
primero, la caracterización se realiza usando exclusivamente un conjunto de palabras 
funcionales, ignorando las palabras de  contenido [12, 22]. Este método trabaja 
apropiadamente pero es muy sensible al tamaño de los documentos. En este caso, la 
longitud de los documentos no sólo influye en la frecuencia de ocurrencia de las palabras 
funcionales sino también en su posible presencia. El segundo método usa la representación 
tradicional de bolsa de palabras, en este caso, considerando únicamente las palabras de 
contenido [15, 18]. Este método es muy robusto y produce excelentes resultados siempre y 
Recopilación y Estructuración Automática de Contenidos Educativos Digitales a partir de la Web 
 24 
cuando exista una fuerte correlación entre los temas y los autores. Finalmente, el tercer 
método considera n-gramas, es decir, secuencias de n palabras consecutivas. Este método 
intenta capturar la estructura del lenguaje de los textos por medio de simples secuencias de 
palabras en contraste de las complejas estructuras sintácticas [20]. De esta manera, el 
propósito es obtener una adecuada caracterización de los textos sin llevar a cabo un costoso 
análisis sintáctico. Desafortunadamente este tipo de caracterización nos lleva a una 
explosión combinatoria, por lo que comúnmente sólo se llega al cálculo de trigramas (3-
gramas).  
En general, nuestro método es similar al enfoque basado en n-gramas. En ambos 
casos, los documentos son caracterizados por una combinación de palabras funcionales y de 
contenido. Sin embargo, nosotros consideramos un tipo especial de secuencias de palabras 
(llamadas, secuencias frecuentes maximales) las cuales son determinadas por su frecuencia 
de ocurrencia y no por su longitud. Usando esta simple estrategia, se seleccionan las 
secuencias más relevantes, e indirectamente se afronta el problema de la explosión de 
características. En la siguiente sección se describe a detalle el método propuesto.  
3.1.8. Un nuevo método para clasificación de textos por estilo 
Como se menciona en párrafos anteriores, este artículo presenta un nuevo método 
para la clasificación automática de textos por autor. Este método caracteriza los 
documentos a través de un conjunto de secuencias relevantes que combinan palabras 
funcionales y de contenido. La idea es usar estas secuencias para clasificar los documentos 
dado que éstas expresan las colocaciones léxicas más significativas utilizadas por un autor. 
Tradicionalmente, las secuencias se extraen aplicando el cálculo general de n-gramas. En 
contraste, nosotros proponemos descubrirlas por medio de un proceso de minería de 
secuencias frecuentes maximales.  
A continuación se define el concepto de secuencia frecuente maximal, se describe el 
proceso para la extracción de dichas secuencias, y se presenta un algoritmo de clasificación 
que considera la caracterización de los documentos mediante un conjunto de secuencias de 
palabras. 
Recopilación y Estructuración Automática de Contenidos Educativos Digitales a partir de la Web 
 25 
 Secuencias Frecuentes Maximales 
Asumiendo que D es un conjunto de textos (por texto nos referimos a un documento 
completo o incluso a una sola oración), y donde cada texto consiste de una secuencia de 
palabras, entonces tenemos la siguientes definiciones [11]. 
Definición 1. Una secuencia p = a1…ak es una subsecuencia de una secuencia q si todos los 
elementos ai, 1 = i = k, ocurren en q y además ocurren en el mismo orden que en p. Si una 
secuencia p es una subsecuencia de una secuencia q, entonces se dice que p ocurre en q.  
Definición 2. Una secuencia p es frecuente en D si p es una subsecuencia de al menos s  
textos de D, donde s  es un umbral de frecuencia predefinido. 
Definición 3. Una secuencia p es una secuencia frecuente maximal en D si no existe alguna 
otra secuencia p´ en D tal que p es una subsecuencia de p´ y p´ es frecuente en D. 
Una vez introducidas las secuencias frecuentes maximales, el problema de minería 
puede declararse formalmente como sigue: Dada una colección de textos D y un valor 
entero arbitrario s  tal que 1 = s  = |D|, enumerar todas las secuencias frecuentes maximales 
en D. 
Es importante mencionar que la implementación de un método para minería de 
secuencias frecuentes maximales no es una tarea trivial dada su complejidad 
computacional. El algoritmo usado en nuestros experimentos está descrito en [16]. 
3.1.9. Algoritmo de Clasificación 
La atribución de autoría es un problema de clasificación, en donde un conjunto de 
documentos con autores conocidos son usados para entrenar un clasificador, el cual es 
utilizado posteriormente para determinar el autor de un texto anónimo. En la tabla 1 se 
muestra el algoritmo de clasificación propuesto, el cual se basa en el uso de secuencias 
frecuentes maximales como características de los documentos. 
El algoritmo propuesto plantea construir un conjunto de características que combine 
las secuencias frecuentes maximales extraídas con diferentes valores de s . La idea es 
construir el conjunto de características de manera iterativa, incrementando el valor de s  en 
cada paso. Este proceso comienza con la inclusión de las secuencias correspondientes a s  = 
2, y termina cuando no hay más colocaciones léxicas (secuencias mayores de dos palabras) 
por agregar al conjunto de características. 
Recopilación y Estructuración Automática de Contenidos Educativos Digitales a partir de la Web 
 26 
Es importante señalar que el proceso de minería de secuencias descrito en la sección 
3.1 depende en gran manera de la definición del umbral de frecuencia s . Se espera que 
distintos valores s  generen diferentes conjuntos de secuencias de palabras, y por 
consiguiente produzcan diferentes niveles de clasificación. Por ejemplo, con valores bajos 
de s  se extraen secuencias grandes y se favorece la tasa de precisión, mientras que con 
valores altos se tiende a generar secuencias cortas que soportan el porcentaje de recuerdo. 
Desafortunadamente, el valor de s  más adecuado es dependiente del tamaño de la colección 
de documentos dada, y por lo tanto debe ser determinado empíricamente para cada 
situación particular. En cierta manera la construcción iterativa del conjunto de 
características confronta este problema reduciendo la dependencia de la clasificación 
respecto al umbral de frecuencia seleccionado. Básicamente, el algoritmo de clasificación 
propuesto intenta tomar ventaja de los distintos tipos de secuencias que pueden extraerse, 
tanto de las que favorecen la precisión como de las que benefician el recuerdo. 
 
Tabla 1. Algoritmo de clasificación 
 
Sea DT el conjunto de documentos etiquetados que se usará para el entrenamiento 
Sea d un documento anónimo  
 
ENTRENAMIENTO 
1. Establecer el valor del umbral de frecuencia s = 2 
2. Establecer el conjunto de características F1 = {∅} 
3. HACER 
a. Enumerar todas las secuencias frecuentes maximales en DT 
correspondientes al umbral de frecuencia s . Nombrar el conjunto de 
secuencias Ss 
b. Integrar las nuevas secuencias al conjunto de características, esto es, 
Fs = Fs -1 ∪ Ss 
c. Incrementar el umbral de frecuencia, s = s + 1 
     MIENTRAS (Ss -1 contenga al menos una secuencia de dos o mas palabras no 
incluida en Fs -2) 
4. Construir las instancias de entrenamiento usando las secuencias descubiertas 
como características booleanos. 
5. Dar las instancias de entrenamiento al algoritmo de aprendizaje y realizar el 
entrenamiento. 
 
CLASIFICACIÓN 
1. Construir la representación de d de acuerdo con el conjunto de características de 
entrenamiento 
2. Dejar que el clasificador, previamente entrenado, clasifique la nueva instancia 
 
 
Recopilación y Estructuración Automática de Contenidos Educativos Digitales a partir de la Web 
 27 
3.1.10. Experimentos 
3.1.10.1. Corpus de prueba 
Desafortunadamente, no existe un conjunto de datos estándar para evaluar los 
métodos orientados a la clasificación de textos por autor. Por ello se tuvo que construir un 
corpus de prueba propio. Este corpus fue recopilado a partir de la Web, y consiste de 353 
poemas escritos por cinco autores diferentes. La tabla 2 resume algunas estadísticas del 
corpus. Es importante mencionar que los poemas recolectados son documentos muy cortos 
(176 palabras en promedio), y además todos ellos corresponden a poetas mexicanos 
contemporáneos, evitando así la identificación de algún autor por el uso de términos 
anacrónicos. 
3.1.10.2. Valores de referencia (baseline) 
Debido a la dificultad que existe para comparar nuestro enfoque con otros trabajos 
previos –principalmente por la ausencia de un corpus estándar de evaluación– se realizaron 
varios experimentos para establecer algunos valores de referencia. Estos experimentos 
consideraron cuatro conjuntos diferentes de características: (i) palabras funcionales, (ii) 
palabras de contenido, (iii) la combinación de palabras funcionales y de contenido, y (iv) n-
gramas. En la tabla 3 se muestran los resultados correspondientes de cada uno de estos 
enfoques.  
Tabla 2. Corpus de Prueba 
Poetas Número de  documentos 
Tamaño del 
Vocabulario 
Número 
de Frases 
Promedio de  
Palabras por 
documento 
Promedio de 
frases por 
documento 
Efraín Huerta 48 3831 510 236.5 22.3 
Jaime Sabines 80 3955 717 155.8 17.4 
Octavio Paz 75 3335 448 162.6 27.2 
Rosario Castellanos 80 4355 727 149.3 16.4 
Rubén Bonifaz 70 4769 720 178.3 17.3 
 
Es importante mencionar que, dado que nuestro interés principal es determinar una 
caracterización apropiada de los documentos para la identificación del autor, en todos los 
casos se usó el mismo algoritmo de clasificación (Naïve Bayes), se aplicó la misma técnica 
para reducción de dimensionalidad (ganancia de información) y se utilizó el mismo 
esquema de evaluación (validación cruzada con 10 pliegues).  
Recopilación y Estructuración Automática de Contenidos Educativos Digitales a partir de la Web 
 28 
Tabla 3. Resultados de referencia 
Características Exactitud Precisión  promedio  
Recuerdo 
promedio  
Palabras Funcionales 41.0% 0.42 0.39 
Palabras de contenido 73.0% 0.78 0.73 
Todo tipo de palabras 73.0% 0.78 0.74 
n-gramas(unigramas más bigramas) 78.8% 0.84 0.79 
n-gramas (de unigramas a trigramas) 76.8% 0.84 0.77 
 
Los resultados que se muestran en la tabla 3 son interesentes pues ellos confirman 
algunas de nuestras principales hipótesis. Primero, las palabras funcionales por sí mismas 
no ayudan a capturar el estilo de redacción de documentos cortos. Segundo, las palabras de 
contenido aportan información relevante para distinguir entre los diferentes autores, incluso 
cuando todos los documentos corresponden al mismo género y discuten temas similares. 
Tercero, las colocaciones léxicas, capturadas por secuencias de palabras (n-gramas), son 
útiles para la tarea de atribución de autoría. Cuarto, debido a la explosión de características 
y al pequeño tamaño del corpus, el uso de n-gramas de mayor orden no necesariamente 
mejora los resultados de la clasificación. 
3.1.10.3. Resultados experimentales 
En esta sección se reportan los resultados obtenidos usando secuencias frecuentes 
maximales como características para el problema de atribución de autoría. Es importante 
señalar que en este experimento, al igual que en los experimentos realizados para calcular 
los valores de referencia, se consideraron todas las palabras –incluidos los signos de 
puntuación–, y se usó el clasificador Naïve Bayes, la técnica de ganancia de información 
para reducir la dimensionalidad, y el esquema de validación cruzada con 10 pliegues para la 
evaluación. 
El algoritmo de clasificación propuesto (sección 3.2) construye un conjunto de 
características combinando secuencias frecuentes maximales que corresponden a diferentes 
valores de s . De esta manera, se intenta disminuir la dependencia del resultado de la 
clasificación respecto al umbral de frecuencia. Este proceso empieza con la inclusión de 
secuencias largas (con mayor capacidad de discriminación) y termina con la inserción de 
secuencias cortas (aquellas con mayor cobertura). En total se ensambló un conjunto de 425 
características. La tabla 4 muestra algunos datos relacionados con la construcción de dicho 
conjunto. 
Recopilación y Estructuración Automática de Contenidos Educativos Digitales a partir de la Web 
 29 
Tabla 4. Construcción del conjunto de características 
s  Secuencias  extraídas 
Secuencias  
agregadas 
Longitud promedio 
 de las secuencias 
agregadas 
Número de  
características 
2 141 141 2.58 141 
3 203 100 1.71 241 
4 225 80 1.76 321 
5 195 53 1.74 374 
6 156 23 1.35 397 
7 129 13 1.46 410 
8 124 12 1.25 422 
9 105 3 1 425 
 
Por su parte, la tabla 5 muestra los resultados alcanzados con el algoritmo 
propuesto. De estos resultados, es claro que este algoritmo supera todas las configuraciones 
de referencia. Además, dado que el conjunto de características resultante es comparable en 
tamaño al conjunto de n-gramas, los resultados obtenidos validan nuestra hipótesis de 
determinar las secuencias de palabras por su frecuencia de ocurrencia y no por su longitud. 
Básicamente, el método propuesto permite seleccionar las secuencias de palabras más 
relevantes y evitar el problema de la explosión de características. 
Tabla 5. Resultados del algoritmo propuesto 
Poetas Precisión Recuerdo 
Efraín Huerta 1.00 0.75 
Jaime Sabines 0.83 0.83 
Octavio Paz 0.95 0.75 
Rosario Castellanos 0.65 0.91 
Rubén Bonifaz 0.94 0.87 
Promedio 0.87 0.82 
Exactitud Total 83% 
 
3.1.11. Comentarios 
En las secciones anteriores se describen dos métodos para la clasificación de textos. 
El primero fue usado para la clasificación temática mientras el segundo para la clasificación 
de textos por estilo. Este último método se apoya en la idea de que una caracterización 
apropiada debe considerar tanto el estilo de redacción como el tema de los documentos. En 
particular, el método propuesto caracteriza los documentos a través de secuencias de 
palabras, las cuales combinan tanto palabras funcionales como de contenido.  
Recopilación y Estructuración Automática de Contenidos Educativos Digitales a partir de la Web 
 30 
Los resultados experimentales demuestran la pertinencia del método obteniendo 
mejores resultados que con el uso de n-gramas. Asimismo, los resultados indican que este 
método permite capturar las colocaciones léxicas más significativas usadas por un autor. 
Es también importante mencionar que el método no utiliza ningún análisis 
lingüístico sofisticado sobre los textos, y además, que a diferencia de los enfoques actuales, 
no es sensible al tamaño de los documentos. Esta última conclusión fue manifiesta por el 
comportamiento obtenido con el corpus de poemas. 
 
Referencias (segunda etapa) 
 
1. T. Joachims, “Text Categorization with Support Vector Machines: Learning with Many 
Relevant Features”, Proceedings of ECML-98, 10th European Conference on Machine 
Learning, Springer Verlag, Heidelberg, 1998, pp. 137-142. 
2. A. Kehagias, V.G. Kaburlasos, and P. Fragkou, “A Comparison of Word and Sense-Based Text 
Categorization Using Several Classification Algorithms”, Journal of Intelligent Information 
Systems, 2003, 21(3), pp. 227-247. 
3. C.H. Koster, and M. Seutter, “Taming Wild Phrases“, Proceedings of ECIR-03, 25th European 
Conference on Information Retrieval. Springer Verlag, Pisa, 2003, pp. 161-176. 
4. D. Lewis, “Naive (Bayes) at Forty: The Independence Assumption in Information Retrieval”, 
Proceedings of 10th European Conference on Machine Learning, Springer Verlag, 1998, pp. 4-
15. 
5. Mitchell, T., Machine Learning, McGraw-Hill, 1997.  
6. L. Morales, M. Montes, P. Rosso, L. Villaseñor, y .J.G. Acevedo, “Evaluación de los 
Contenidos Educativos de Nivel Básico en Internet”, XX Simposio Internacional de 
Computación en Educación SOMECE 2004, Puebla, México, 2004. 
7. A. Moschitti, and R. Basili, “Complex Linguistic Features for Text Classification: a 
Comprehensive Study”, Proceedings of ECIR-04, 26th European Conference on Information 
Retrieval. Springer Verlag, Heidelberg, 2004, pp. 181-196. 
8. R.E. Schapire, and Y. Singer, “BoosTexter: A Boosting Based System for Text Categorization”, 
Machine Learning, 2000, 39(2/3), pp. 135-168. 
9. F. Sebastiani, “Machine Learning in Automated Text Categorization”, ACM Computing Survey, 
2002, 34(1), pp. 1-47. 
10. Y. Yang, and J. Pedersen,. “A comparative study on feature selection in text categorization”, 
Proceedings of the 14th International Conference on Machine Learning, 1997, pp. 412-420. 
11. Ahonen-Myka H. (2002). Discovery of Frequent Word Sequences in Text Source. Proceedings 
of the ESF Exploratory Workshop on Pattern Detection and Discovery. London, UK, 2002. 
12. Argamon, S. & Levitan, S. (2005). Measuring the Usefulness of Function Words for Authorship 
Attribution. Association for Literary and Linguistic Computing/ Association Computer 
Humanities, University Of Victoria, Canada, 2005. 
13. Bekkerman, R. & Allan, J. Using (2004). Bigrams in Text Categorization. CIIR Technical 
Report IR-408 Center for Intelligent Information Retrieval, University of University of 
Massachusetts Amherst, 2004. 
Recopilación y Estructuración Automática de Contenidos Educativos Digitales a partir de la Web 
 31 
14. Chaski, C. (2005). Who's at the Keyword? Authorship Attribution in Digital Evidence 
Investigations. International Journal of Digital Evidence. Volume 4, Issue 1, 2005. 
15. Diederich, J., Kindermann, J., Leopold, E. & Paas, G. (2003). Authorship Attribution with 
Support Vector Machines. Applied Intelligence, 19(1):109-123, 2003.  
16. García-Hernández, R., Martínez-Trinidad F., and Carrasco-Ochoa A. (2006). A New Algorithm 
for Fast Discovery of Maximal Sequential Patterns in a Document Collection. International 
Conference on Computational Linguistics and text Processing, CICLing-2006. Mexico City, 
Mexico, 2006. 
17. Holmes, D. (1995). Authorship Attribution. Computers and the Humanities, 28:87-106. Kluwer 
Academic Publishers. 1995. 
18. Kaster, A., Siersdorfer, S., & Weikum, G. (2005). Combining Text and Linguistic Document 
Representations for Authorship Attribution. Workshop Stylistic Analysis of Text for 
Information Access, 28th Int. SIGIR 1. MPI, Saarbrücken 2005. 
19. Malyutov, M.B. (2004). Authorship Attribution of Texts: a Review. Proceedings of the program 
"Information transfer" held in ZIF. University of Bielefeld, Germany. 2004. 
20. Peng, F., Schuurmans, D., Keselj, V. & Wang, S. (2004). Augmenting Naïve Bayes Classifiers 
with Statistical Languages Models. Information Retrieval, vol. 7, 317-345. Kluwer Academic 
Publishers. 2004. 
21. Stamatatos, E., Fakotakis, N. & Kokkinakis,G.  Computer-Based Authorship Attribution 
Without Lexical Measures. Computers and the Humanities 35: 193-214, 2001. Kluwer 
Academic Publishers, 2001. 
22. Zhao, Y. & Zobel, J. (2005). Effective and Scalable Authorship Attribution Using Function 
Words. Lecture Notes in Computer Science, vol. 3689, 174-189. Springer Verlag. 2005. 
 
4. Tercera etapa 
Dos actividades se llevaron a cabo bajo esta etapa: la estructuración automática de 
documentos y el desarrollo de un prototipo que integre y demuestre la utilidad de las ideas 
planteadas en este proyecto.  
4.1. Estructuración de documentos  
En esta última etapa se desea resolver el problema de organizar y estructurar la 
información recopilada por el docente en la Web. La organización de los documentos 
recopilados se realizará a través de su similitud. Es decir, dos documentos se relacionan su 
existe un mínimo de similitud entre ellos. Si la relación existe se crea una liga en cada uno 
de ellos que los liga. Para realizar esta organización de manera automática es necesario 
medir la similitud de los documentos. A continuación se describen las ideas detrás de los 
sistemas de hipertexto y se presenta el método utilizado para relacionar dos documentos. 
Recopilación y Estructuración Automática de Contenidos Educativos Digitales a partir de la Web 
 32 
4.1.1. Sistemas de Hipertexto 
Los sistemas de hipertexto dominan los ambientes computacionales a partir del éxito de la 
web. Un sistema de hipertexto consiste de una serie de páginas y ligas que las conectan 
entre sí. Las ligas pueden conectar cualquier página a cualquier otra página y las páginas 
pueden tener ligas dentro de ellas. Los sistemas de Hipertexto no tienen que ser lineales 
(fig. 4.1) y los diferentes usuarios del sistema no tienen que recorrerlo por el mismo 
camino. Esto significa que la información se puede examinar en cualquier orden deseado, 
seleccionando el tópico que se desea ver a continuación. En vez de tener que especificar 
palabras clave o cadenas de búsqueda, simplemente se recorre (navega) a través de una 
base de datos2, saltando de liga en liga. 
 
 
 
Figura 4.1 Vista simplificada de una estructura de hipertexto con 6 nodos y 9 ligas 
 
La idea de hipertexto fue sugerida en principio por Vannevar Bush en 1945 en su 
artículo “As we may think” publicado en la revista Atlantic Monthly. En éste, Bush 
describía un sistema completo (Memex) de almacenamiento y manejo de información y 
consideró entonces el uso de relaciones (ligas) entre diferentes documentos, así como los 
conceptos de: anotaciones, compartir segmentos de hipertexto, visualización de múltiples 
documentos, compra-venta de volúmenes de hipertexto  (que sustituirían a los libros, 
fotografías, revistas, periódicos, etc). El término hipertexto fue popularizado por Ted 
Nelson. El primer sistema de hipertexto en ser implementado fue NLS durante los años 
60’s. Tras un lento desarrollo durante los 70’s, los sistemas de hipertexto finalmente 
comenzaron a tener aplicaciones reales en los 80’s, a partir de los 90’s y gracias a la 
                                                 
2 En este contexto nos referimos a una colección de documentos de texto con información por contenido. 
Recopilación y Estructuración Automática de Contenidos Educativos Digitales a partir de la Web 
 33 
especificación de la WWW (CERN), así como del desarrollo de Mosaic (NCSA), con lo 
cuál el crecimiento de internet y del hipertexto se acelera. 
El significado más común de hipertexto es una base de datos que tiene referencias 
cruzadas activas y permite al lector “saltar” a otras partes de la base de datos a su voluntad.  
Una base de datos se puede conceptualizar como una red de nodos y ligas, donde los 
documentos son los nodos y las ligas las referencias cruzadas, es decir, son las etiquetas 
que conectan un nodo con otro. Cuando una liga se activa, se hace un salto al documento 
apuntado por la liga. Un nodo es una unidad de información. Un sólo documento en una 
base de datos de hipertexto es llamado nodo. Cada nodo en un sistema de hipertexto 
corresponde a una o más pantallas de presentación. Aún cuando no hay una guía aceptada 
para el tamaño de un documento, usualmente describe un concepto sencillo o tópico. La 
continuidad entre los documentos la proveen las ligas. 
4.1.2. Identificación de relaciones entre documentos. 
El método usado para descubrir relaciones entre documentos se hace a través de sus 
palabras representando las palabras del documento en un espacio vectorial. De esta manera 
cada documento es representado como un vector. La aproximación vectorial permite una 
representación matemática y física usando un modelo de espacio vectorial. Considere un 
espacio D de documentos consistiendo de N documentos di, cada uno identificado por uno o 
más de los n términos del índice  T. Cada término ti del índice puede ser considerado otra 
dimensión en un espacio de representación para un documento. En este caso, cada 
documento di está representado por un vector t-dimensional. 
 
( )
)..1|(
..1|
njtT
NidD
j
i
==
==
 
( ) ( ) ( )( )njjjii twtwtwdd ,,, 21 K
r
=→  
wj es el peso del término j-ésimo. 
 
La figura 4.1 muestra tres documentos representados en el espacio vectorial. 
 
Recopilación y Estructuración Automática de Contenidos Educativos Digitales a partir de la Web 
 34 
 
Figura 4.1. Representación vectorial del espacio de documentos.  
 
 
Dados los vectores del índice de dos documentos, es posible calcular un coeficiente 
de similitud entre ellos, S(di,dj), el cual refleja el grado de similitud en los términos 
correspondientes y en los pesos de los términos. Una métrica de similitud puede ser el 
producto de los dos vectores, o alternativamente, una función inversa del ángulo entre los 
pares correspondientes de vector; cuando el término asignado para dos vectores es idéntico, 
el ángulo será cero, produciendo una métrica de similitud máxima. La métrica usada en este 
proyecto es la llamada coeficiente de Dice la siguiente ecuación define esta métrica. 
 
 
 
Al utilizar documentos normalizados la expresión anterior se simplifica, dando lugar 
a la siguiente ecuación:  
 
 
Donde w(tk) denota el peso normalizado del k-ésimo término en el documento d. 
Utilizando esta métrica fue posible identificar documentos relacionados y por ende 
construir las ligas para la creación de un hipertexto final. 
∑∑
∑
==
=
×
+
=
n
k kj
n
k ki
n
k kjki
ji
dd
ddddS
1
2
,1
2
,
1 ,,
)()(
)(2
),(
∑ = +=
n
k kjkiji
twtwddS
1
))()((
2
1
),(
Recopilación y Estructuración Automática de Contenidos Educativos Digitales a partir de la Web 
 35 
4.2. Prototipo 
En esta sección se muestran algunas de las pantallas que confirman el prototipo 
resultante de este proyecto y se ejemplifican los pasos del proceso de construcción de 
contenidos digitales educativos. 
El primer paso consiste en la consulta y recuperación de documentos relevantes. 
Para ello el docente escribe una consulta y el sistema recupera de la Web documentos 
asociados. (Véase la figura 4.2). Posteriormente el docente escoge alguno de los 
documentos presentado. Al momento de recuperar el documento la herramienta muestra la 
categoría temática del documento y lo califica respecto a su estilo de redacción. (Véase las 
figuras 4.3 y 4.4). Si el docente lo considera conveniente aparta el documento. Al terminar 
la selección de documentos el docente aplica un último proceso que crea las ligas para 
confirmar el hipertexto final. Este hipertexto puede finalmente ser visualizado con un 
navegador cualquiera. 
 
  
 
Figura 4.2 Documentos recuperados dada una consulta 
 
 
 
consulta 
 
documentos 
recuperados 
Recopilación y Estructuración Automática de Contenidos Educativos Digitales a partir de la Web 
 36 
 
 
Figura 4.3 Documento con sus indicadores de relevancia  
categoría Historia; lenguaje complejo 
 
 
 
Figura 4.4 Documento con sus indicadores de relevancia  
categoría Historia; lenguaje simple
 
Medidas de relevancia:  
- categoría Historia  
- lenguaje complejo 
 
Medidas de relevancia:  
- categoría Historia  
- lenguaje simple 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
ANEXO I 
 
 
Evaluación de los Contenidos Educativos de Nivel Básico en Internet 
Liliana Morales Zanatta, Manuel Montes y Gómez, Paolo Rosso,  
Luis Villaseñor Pineda y Gerardo Acevedo Ponce de León.  
Trabajo presentado y publicado en el  
XX Simposio Internacional de Computación en Educación SOMECE 2004.  
Puebla, México, Octubre 2004. 
Evaluación de los Contenidos Educativos de Nivel Básico en Internet 
 
Liliana Morales Zanatta1,2, Manuel Montes y Gómez3,4, Paolo Rosso4, 
Luis Villaseñor Pineda3 y José Gerardo Acevedo y Ponce de León2 
1 Secretaría de Educación Pública del Estado de Puebla, México. 
2 Universidad Autónoma de Tlaxcala, México. 
{lmzanatta, jgera_4908}@hotmail.com 
3 Instituto Nacional de Astrofísica, Óptica y Electrónica (INAOE), México. 
{mmontesg, villasen}@inaoep.mx 
4 Universidad Politécnica de Valencia, España. 
{mmontes, prosso}@dsic.upv.es 
Resumen. El uso de Internet como complemento informativo se sostiene en la creencia de 
que la Web es una gran biblioteca. Sin embargo factores como la diversidad de contenidos e 
idiomas, su gran tamaño, y la carencia de herramientas adecuadas para buscar información 
complican su aplicación, y han puesto en duda dicha condición. Este artículo confronta esta 
inquietud, en él se propone una metodología que pretende facilitar el estudio del alcance y 
tipo de contenidos disponibles en la Web relacionados con un currículo determinado. Esta 
metodología se aplicó para la evaluación de los contenidos educativos disponibles en la Web 
relacionados con algunos temas del currículo de educación básica de México. Los resultados 
obtenidos indican que existen suficientes contenidos web sobre el currículo de nivel básico, 
pero que la gran mayoría de éstos no son mexicanos y mucho menos institucionales. 
Palabras Clave: Contenidos educativos, Internet, educación básica. 
1. Introducción 
La educación es sin lugar a dudas un factor de primera importancia en toda sociedad moderna. Así se ha 
reconocido en México desde sus inicios como país independiente, y así ha quedado manifestado en el más 
reciente Plan Nacional de Desarrollo, donde se expresa la convicción del actual gobierno por hacer de la 
educación el gran proyecto nacional. Los retos de la educación en México pueden expresarse mediante 
tres principios fundamentales: educación para todos, educación de calidad y educación de vanguardia [1]. 
Sin duda alguna el uso y aplicación de las nuevas tecnologías en la educación juega un papel importante 
en la consolidación de estos retos. En particular el uso de Internet como herramienta educativa facilitará 
la educación a distancia y la implantación de los programas de superación continua de los profesores, 
ayudará en las tareas de administración educativa, y por supuesto, “consolidará una educación de calidad 
y vanguardia con acceso al mayor repositorio de información jamás construido por el hombre” [2, 3, 4]. 
Actualmente existe la creencia, casi generalizada, de que la Web (servicio de Internet que consiste de 
un conjunto páginas conectadas con enlaces de hipertexto) es una gran biblioteca, y que sus contenidos 
pueden satisfacer prácticamente cualquier necesidad de información por más específica o rara que ésta 
sea. Sin embargo factores como la diversidad de contenidos e idiomas, su gran tamaño, y la carencia de 
herramientas adecuadas para buscar información complican su aplicación. 
Además debemos recordar que la Web surgió como un proyecto científico-académico, pero con el 
tiempo se ha transformado en un medio masivo de información claramente comercial y de entretenimien-
to. Esta situación implica grandes inconvenientes para el uso de la Web como recurso pedagógico. Por 
una parte de ello se deriva la dificultad para encontrar la información deseada, puesto que los contenidos 
educativos se encuentran generalmente inmersos en un mar de información irrelevante y muchas veces 
incluso inapropiada para los estudiantes. Por otra parte, y más grave aún, muchos de estos contenidos son 
de autoría desconocida y por ende de dudosa calidad [5]. 
A estos problemas se suma la poca representatividad de nuestra lengua en la Web. Estadísticas recien-
tes señalan que solamente un 3% de las páginas web existentes están en Español [6]. Así pues, aún persis-
te la duda de si la Web es o no es una gran biblioteca –en Español– con fines educativos. 
Este artículo confronta esta inquietud. Básicamente se enfoca en el diseño de una metodología para la 
evaluación de los contenidos educativos disponibles en la Web relacionados con un currículo particular. 
A partir de dicha evaluación se pretende contestar las preguntas como: ¿existen suficientes contenidos 
educativos disponibles en la Web relacionados con el currículo predefinido? ¿estos contenidos están dis-
tribuidos uniformemente sobre todos sus temas? ¿son ellos veraces y de calidad? 
Asimismo, el artículo presenta los resultados obtenidos al aplicar la citada metodología en la evalua-
ción de los documentos web relacionados con los temas propios del sexto año de la educación básica de 
México. La elección de este dominio de estudio estuvo motivada por nuestra creencia en que los temas 
del nivel básico son los menos cubiertos en la Web, así como por la necesidad de considerar el uso de un 
lenguaje apropiado para niños en su evaluación. 
A continuación, en la sección 2, se describe la metodología propuesta para evaluar el contenido educa-
tivo disponible en la Web. En la sección 3, se presentan los resultados preliminares de la evaluación de 
los contenidos web relacionados con el currículo del sexto año de primaria de México. Finalmente, en la 
sección 4, se discuten los resultados obtenidos y se establecen nuestras primeras conclusiones. 
2. Metodología de evaluación 
Es evidente que Internet está adquiriendo día a día una mayor relevancia y presencia en el sector educati-
vo. Su uso como complemento informativo se sostiene en la creencia de que la Web es una gran bibliote-
ca. Sin embargo diversos factores están complicado su adecuada aplicación, y con ello la duda sobre si es 
realmente una gran biblioteca está emergiendo. 
La metodología que se describe a continuación pretende facilitar el estudio del alcance y tipo de conte-
nidos disponibles en la Web relacionados con un currículo determinado. El objetivo de dicho estudio es 
evaluar la disponibilidad de contenidos, así como su calidad y riqueza informativa. 
Es claro que una evaluación completa y precisa del contenido educativo disponible en la Web sólo se 
lograría si se pudieran visitar todas sus páginas. Dado que esto es imposible, pues su tamaño es de varios 
miles de millones de páginas, es necesario plantear una metodología inversa para esta evaluación. 
La metodología que proponemos parte de un conjunto de peticiones de temática educativa, y se realiza 
sobre los documentos recuperados por los buscadores para dichas peticiones. Además, esta metodología 
tiene un sentido pragmático puesto que los conjuntos de peticiones, las peticiones en sí mismas, y los 
documentos revisados son pequeños, pero ajustados a las costumbres de uso de la Web. 
La metodología de evaluación propuesta consiste de los siguientes pasos: 
1. Seleccionar la máquina de búsqueda con la que se realizará el estudio. 
Esta selección se basará en un estudio comparativo de los principales buscadores disponibles en la 
Web. Algunas de las características sugeridas para dicha comparación son: su cobertura (i.e., tamaño del 
índice), su popularidad, y grado de frescura, es decir, la frecuencia de actualización de su índice. 
En los casos que se desee usar mas de una máquina de búsqueda para el estudio también deberá consi-
derarse su nivel de traslape, es decir, el grado de intersección de sus índices, intentando seleccionar el 
conjunto de buscadores cuya unión produzca una mayor cobertura de la web. 
2. Definir un conjunto muestra de peticiones de temática educativa. 
Esta definición deberá considerar entre otros los siguientes criterios. Primero, las peticiones tendrán 
que obtenerse de forma directa de los libros de texto. Segundo, deberán cubrir adecuadamente toda la 
curricula de los grados académicos seleccionados. Tercero, deberán reflejar distintos tipos de necesidades 
de información, entre otras: 
–Cronológicas: cuya respuesta es información biográfica sobre algún personaje o cronológica sobre un 
evento. 
–Definitorias: cuya respuesta debe incluir la definición, las características y algunos ejemplos del con-
cepto en cuestión. 
–Procedurales: cuya respuesta debe incluir la descripción de las etapas o fases del tema en cuestión, así 
como sus partes o participantes, causas y consecuencias. 
Por otra parte, las peticiones deberán ser preferentemente cortas, pues las costumbres de uso de la Web 
señalan que los buscadores reciben en promedio peticiones de dos palabras [7]. 
3. Definir los criterios para la clasificación y evaluación de los documentos. 
Para cada página web recuperada nos interesa determinar por lo menos: (i) su relevancia, es decir, si 
tiene contenido educativo; (ii) su adecuación, es decir, si el contenido que presenta es del nivel deseado, 
por ejemplo de nivel básico para nuestro caso de estudio; y (iii) si es complementaria a los contenidos 
oficiales, es decir, si expone algo adicional a lo contenido en los libros de texto. 
Además nos interesa calificar la calidad del contenido educativo de cada página recuperada. Una mane-
ra indirecta pero sencilla de hacerlo es determinado si dicho contenido fue creado por una institución 
mexicana de carácter gubernamental o docente. 
4. La consulta manual de la Web y el análisis estadístico de los datos. 
Esta consulta se realizará a partir del buscador y conjunto de peticiones definidos en los pasos anterio-
res. Para cada petición se revisarán los primeros 20 resultados regresados por la máquina de búsqueda. 
Este número tiene un sentido pragmático y está sustentado en las costumbres de uso de la Web, que seña-
lan que los internautas revisan en promedio solamente entre las 10 y 15 primeras páginas retornadas por 
los buscadores [7]. 
El contenido de cada página recuperada deberá analizarse y caracterizarse según los criterios definidos 
en el paso 3. Los datos obtenidos deberán contabilizarse y analizarse estadísticamente de tal forma que 
pueda cuantificarse la disponibilidad de contenidos educativos en la Web, así como su distribución por 
temas y calidad. 
3. Resultados preliminares 
La metodología propuesta en la sección anterior se aplicó para la evaluación de los contenidos educativos 
disponibles en la Web relacionados con los temas propios del currículo de sexto año de primaria de 
México. A continuación se describen los principales elementos de dicha evaluación y se presentan los 
resultados obtenidos. 
En primer lugar se seleccionó el buscador Google1 para conducir el experimento. Esta decisión se basó 
en los siguientes datos [8, 9]: (i) Google es actualmente el buscador con el mayor índice, aproximadamen-
te 4 mil millones de páginas web; (ii) es el buscador más usado a nivel mundial, recibe diariamente alre-
dedor de 250 millones de consultas, y (iii) es uno de los buscadores que actualiza su índice más frecuen-
temente, en promedio una vez por mes. 
En segundo lugar se decidió analizar los contenidos relacionados con las materias de matemáticas, his-
toria de México, ciencias naturales y geografía, quedando pendiente la asignatura de español. Para cada 
materia se definió un conjunto de peticiones siguiendo las sugerencias establecidas en el paso 2 de nues-
tra metodología (referirse a la sección 2). Por ejemplo, para el caso de historia de México se definieron 
peticiones como: “Miguel Hidalgo”, Dictadura, e “Imperio de Maximiliano”. En promedio se realizaron 
44.5 consultas por materia. 
Finalmente se realizó la consulta manual a la Web, analizando las respuestas de Google de acuerdo con 
lo establecido en el paso 3 de la metodología. Las siguientes figuras resumen los resultados obtenidos. 
                                                          
1 www.google.com.mx 
La figura 1 indica el porcentaje de páginas relevantes por materia encontradas entre las primeras 20 
respuestas de Google. Esta gráfica señala que los contenidos educativos están distribuidos mas o menos 
uniformemente entre las distintas materias, siendo historia de México la materia más cubierta y geografía 
la menos. 
La figura 2 por su parte muestra la distribución de los contenidos disponibles en la web por tipo de ne-
cesidad de información. Aquí puede observarse que en la Web existe más información de tipo cronológi-
co, que definitoria o descriptiva (por lo menos para el nivel de educación básica). En parte esta situación 
explica los resultados mostrados en la figura 1, pues la materia de historia de México contiene más peti-
ciones de tipo cronológico que el resto, y a su vez, la asignatura de geografía estuvo modelada solamente 
por preguntas de tipo definición. 
En la figura 3 se presentan algunas características de los contenidos marcados como relevantes. Curio-
samente para las materias de matemáticas y geografía existen menos contenidos disponibles, pero estos 
son en su gran mayoría apropiados para niños y complementarios al libro de texto, es decir, son conteni-
dos que usan un lenguaje sencillo y que además agregan información sobre los temas en cuestión. Esta 
situación no es tan evidente para las otras dos materias. Otro aspecto importante a resaltar es que muy 
pocos contenidos fueron claramente reconocidos como mexicanos e institucionales. En este caso, tal co-
20%
38%
30%
15%
0%
5%
10%
15%
20%
25%
30%
35%
40%
Matemáticas Historia de
México
Ciencias
Naturales
Geografía
 
Figura 1. Porcentaje de páginas relevantes por materia 
46%
20%
31%
0%
5%
10%
15%
20%
25%
30%
35%
40%
45%
50%
Cronológias Definitorias Procedurales  
Figura 2. Porcentaje de páginas relevantes por tipo de consulta 
mo la lógica lo indicaba, se encontraron más páginas mexicanas sobre los temas de historia de México 
que sobre los tópicos de otras materias de interés internacional. 
4. Conclusiones 
Hasta ahora todos los comentarios y opiniones relacionados con el uso de la Web como complemento 
informativo en la educación se han sustentado en creencias y observaciones aisladas mas que en datos 
concretos. El gran aporte de este trabajo radica en la propuesta de una metodología simple y fácilmente 
reproducible para la evaluación de los contenidos educativos disponibles en la Web relacionados con un 
currículo o tema determinado. La característica más importante de esta nueva metodología es, en nuestra 
opinión, su sentido pragmático ajustado a las costumbres de uso de la Web. 
La aplicación de dicha metodología en la evaluación de los contenidos educativos relacionados con los 
temas propios del sexto año de educación básica de México arrojó resultados interesantes: 
En primer lugar podemos concluir que “la Web contiene suficientes contenidos educativos relaciona-
dos con el currículo de nivel básico”. Aproximadamente un 25% de las primeras páginas retornadas por 
Google son relevantes a los temas de la educación básica, es decir, 5 páginas por petición. Mientras que 
solamente para un 9% (16 de 178) de los temas analizados no se obtuvo ninguna información relevante. 
En contraposición con este resultado favorable observamos que “el problema de selección de informa-
ción sigue presente”. La información apropiada se encuentra inmersa dentro de una considerable cantidad 
de información no relevante (en una razón de 1 a 4), e incluso se corre el riesgo de que aparezcan algunos 
contenidos inapropiados. A manera de ejemplo podemos citar que para la petición “lluvia”, del área de 
geografía, se encontraron 3 páginas con contenido pornográfico dentro de las 10 primeras respuestas de 
Google. 
Además, hecho que resulta realmente preocupante, “la gran mayoría de las páginas con información re-
levante no son mexicanas y mucho menos institucionales”. Esta observación implica que el contenido de 
dichos documentos no está necesariamente validado y podría ser incorrecto, además de que muy proba-
blemente no está adaptado al entorno del estudiante mexicano. También demuestra el terrible rezago de 
nuestro país en el uso y aplicación de las nuevas tecnologías en la educación, realidad que de no cambiar 
a corto plazo podría causar que los contenidos educativos en Español, relacionados con nuestra historia, 
cultura y entorno geográfico desaparezcan para las nuevas generaciones. 
92%
77%
71%
89%
100%
95%
97%97%
16%
57%
10%
21%
8%
25%
7%
14%
0%
10%
20%
30%
40%
50%
60%
70%
80%
90%
100%
Relevantes
Apropiadas
Reelevantes
Complementarias
Relevantes
Mexicanas
Relevantes
Institucionales
Matemáticas Historia de México Ciencias Naturales Geografía  
Figura 3. Características de las páginas relevantes 
Agradecimientos 
Los autores agradecen a la Secretaria de Educación Pública del Estado de Puebla por la beca comisión 
otorgada a la primera autora para la realización de sus estudios de maestría. Asimismo agradecen al 
CONACYT de México y a la Secretaría de Estado de Educación y Universidades de España por el sopor-
te económico brindado. 
Referencias 
1. Secretaria de Educación Pública de México (2001). Programa Nacional de Educación 2001-2006. Sept 
2001. 
2. Adell, J. (1996). Internet en educación: una gran oportunidad. Revista Net Conexión, nº11, Sept 1996. 
3. Gómez Galán, J. (2002). La World Wide Web (WWW) en la Educación. Educación Social y Nuevas 
Tecnologías. (pp. 120- 145). Sevilla: Kronos, 2002. 
4. Morales Zanatta, L. (2002). La tecnología aplicada a la educación del Siglo XXI. Revista Identidad 
Magisterial, Febrero 2003. 
5. Gómez Galán, J. (2001). Internet: ¿Realmente Una Herramienta Educativa?. I Congreso internacional 
de Educared, 2001. 
6. O'Neill, E. T., Lavoie, B. F., y Bennett, R. (2003). Trends in the Evolution of the Public Web 1998-
2002. D-Lib Magazine, Volume 9, Number 4, April 2003. 
7. Jansen, B. J. Spink, A., y Saracevic, T. (2000). Real Life, real users, and real needs: a study and analy-
sis of user queries on the web. Information Processing and Management, 36(2), 2000. 
8. Sullivan, D. Search Engine Watch. http://ww.searchenginewatch.com/ 
9. Notess, G. Search Engine Showdown. http://searchengineshowdown.com/ 
Datos de la ponencia: 
 
Título: 
Evaluación de los Contenidos Educativos de Nivel Básico en Internet 
 
Autores: 
Liliana Morales Zanatta, Universidad Autónoma de Tlaxcala. 
Manuel Montes y Gómez, Instituto Nacional de Astrofísica, Óptica y Electrónica. 
Paolo Rosso, Universidad Politécnica de Valencia. 
Luis Villaseñor Pineda, Instituto Nacional de Astrofísica, Óptica y Electrónica. 
José Gerardo Acevedo y Ponce de León, Universidad Autónoma de Tlaxcala. 
 
Presentador: 
Luis Villaseñor Pineda 
Instituto Nacional de Astrofísica, Óptica y Electrónica 
Luis Enrique Erro No. 1, C.P. 72840, Tonantzintla, Puebla, México. 
Teléfono: [+52] (222) 266-3100 ext. 8306 
Fax: [+52] (222) 266-3152 
Email: villasen@inaoep.mx 
 
Información de los autores: 
Liliana Morales es Licenciada en Administración de Empresas. Desde 1999 trabaja en la Secretaria 
de Educación Publica del Estado de Puebla. Actualmente es estudiante del Posgrado en Admi-
nistración Educativa de la Universidad Autónoma de Tlaxcala, y se encuentra realizando una es-
tancia de investigación en la Universidad Politécnica de Valencia. 
Manuel Montes es Doctor en Ciencias de la Computación. Miembro del Sistema Nacional de Inves-
tigadores (SNI). Investigador del Laboratorio de Tecnologías del Lenguaje del INAOE, y profe-
sor invitado en la Universidad Politécnica de Valencia. Autor de varias decenas de artículos in-
ternacionales en el área de Procesamiento Automático de Texto. 
Paolo Rosso es Doctor en Ciencias de la Computación. Actualmente es profesor del Departamento 
de Sistemas Informáticos y Computación de la Universidad Politécnica de Valencia. Su investi-
gación es en Recuperación de Información y Desambiguación del Sentido de las Palabras. 
Luis Villaseñor (presentador) es Doctor en Ciencias de la Computación. Miembro del Sistema Na-
cional de Investigadores (SNI). Investigador del Laboratorio de Tecnologías del Lenguaje del 
INAOE. Actualmente, líder del proyecto “Recopilación y Estructuración Automática de Conte-
nidos Educativos Digitales a partir de la Web”, realizado por el INAOE en colaboración con la 
Politécnica de Valencia. 
José Gerardo Acevedo es Licenciado en Administración de Empresas y Maestro en Consultaría Or-
ganizacional. Desde 1976 ha impartido cursos y seminarios en la Universidad Popular Autóno-
ma del Estado de Puebla, en la Universidad de las Américas de Puebla, y actualmente en la Uni-
versidad Autónoma de Tlaxcala. 
 
Grupo de trabajo: 
2, Contenido digital 
 
Requerimientos de la presentación: 
Una computadora, con sistema operativo windows, powerpoint instalado, y un proyector. 
 
 
 
 
 
 
 
 
 
 
 
 
ANEXO II 
 
 
Authorship Attribution using Word Sequences 
Rosa María Coyotl-Morales, Luis Villaseñor-Pineda, 
Manuel Montes-y-Gómez y Paolo Rosso 
Trabajo presentado y publicado en el  
11th Iberoamerican Congress on Pattern Recognition (CIARP 2006)  
Cancún, México, 2006 
Authorship Attribution using Word Sequences 
Rosa María Coyotl-Morales1, Luis Villaseñor-Pineda1, 
Manuel Montes-y-Gómez1 and Paolo Rosso2 
Laboratorio de Tecnologías del Lenguaje, 
Instituto Nacional de Astrofísica, Óptica y Electrónica, México. 
{mcoyotl, villasen, mmontesg}@inaoep.mx 
Departamento de Sistemas Informáticos y Computación, 
Universidad Politécnica de Valencia, España. 
prosso@dsic.upv.es 
Abstract. Authorship attribution is the task of identifying the author of a given text. 
The main concern of this task is to define an appropriate characterization of docu-
ments that captures the writing style of authors. This paper proposes a new method 
for authorship attribution supported on the idea that a proper identification of au-
thors must consider both stylistic and topic features of texts. This method character-
izes documents by a set of word sequences that combine functional and content 
words. The experimental results on poem classification demonstrated that this 
method outperforms most current state-of-the-art approaches, and that it is appro-
priate to handle the attribution of short documents. 
1 Introduction 
Authorship attribution is the task of identifying the author of a given text. It can be con-
sidered as a typical classification problem, where a set of documents with known author-
ship are used for training and the aim is to automatically determine the corresponding 
author of an anonymous text. In contrast to other classification tasks, it is not clear which 
features of a text should be used to classify an author. Consequently, the main concern of 
computer-assisted authorship attribution is to define an appropriate characterization of 
documents that captures the writing style of authors. 
There are several methods for authorship attribution, ranging from those using stylistic 
non-topic features such as the vocabulary richness of the author and the frequency of 
occurrence of some functional words1 [12], to those based on the traditional bag-of-words 
representation that consider all content words of documents [5, 8]. In this paper, we pro-
pose a new method for authorship attribution. This method relies on the hypothesis that a 
proper identification of authors must consider both stylistic and topic features of texts. 
                                                          
1 Words having little semantic content of their own, such as prepositions, conjunctions, and arti-
cles. In information retrieval, they are also known as stopwords. 
Therefore, an adequate characterization of documents must effectively combine func-
tional and content words. Our proposal is to construct this characterization by means of 
word sequences. 
It is important to mention that word sequences (specially, fixed-length word n-grams) 
have been applied without much success in topic-based text classification [3]. Neverthe-
less, there are not enough studies on their application to non-topic-based classification, 
and in particular to the task of authorship attribution [10]. 
On the other hand, other less studied difficulty is the impact of the document size on 
the classification accuracy. It is known that some approaches for authorship attribution 
are very sensible to the length of documents. Specially, the methods based on stylistic 
features tend to fail when confront short documents [11]. This behavior motivates us to 
apply our method on the classification of poems by authors. Given that poems are very 
short documents, our experiments not only contribute to evaluate the usefulness of word 
sequence features for authorship attribution, but also allow analyzing their appropriate-
ness to handle difficult classification scenarios. 
The rest of the paper is organized as follows. Section 2 discusses some previous works 
related to the task of authorship attribution. Section 3 introduces the proposed method. 
Section 4 describes the experimental setup. Section 5 presents some experimental results 
on the use of word sequences features. Finally, section 6 depicts our conclusions and 
future work. 
2 Related Work 
The analysis of style for authorship attribution is mainly based on the assumption that 
each author has habits in wording (i.e., in the use of words) that make their writing 
unique. However, this assumption is not completely true, since the style of an author may 
be variable depending on the target audience, or may change because of differences in 
topics or genre. For this reason, it is difficult to determine a set of features stable to these 
variations but adequate to distinguish between writings of different authors. 
There are several methods for authorship attribution. These methods may be cluster in 
the following three main approaches: 
Stylistic measures as document features. This approach considers features such as the 
length of words and sentences as well as the richness of the vocabulary [7, 9]. It results 
are not conclusive, but have demonstrated that these features are not sufficient for the 
task. It seems that they vary depending on the genre of the text, and that they lost most of 
their meaning when dealing with short texts. 
Syntactic cues as document features. This approach uses a set of style markers. These 
markers go beyond the stylistic measures by integrating information related to structure of 
the language, which is obtained by an in depth syntactic analysis of documents [4, 5, 11]. 
Basically, texts are characterized by the presence and frequency of certain syntactic struc-
tures. This characterization is very detailed and relevant; unfortunately, it is computation-
ally expensive and even impossible to build for languages lacking of text-processing 
resources. Besides, it is also clearly influenced by the length of documents. 
Word-based document features. This approach includes at least three different kinds 
of methods. The first one characterizes documents using a set of functional words, ignor-
ing the content words since they tend to be highly correlated with the document topics [2, 
12]. This method works properly, but it is also affected by the size of documents. In this 
case, the document length not only influences the frequency of occurrence of the func-
tional words but also their sole presence. The second method applies the traditional bag-
of-words representation and uses single content-words as document features [5, 8]. It is 
very robust and produces excellent results when there is a noticeable relation between 
authors and topics. Finally, a third method considers word n-gram features, i.e., features 
consisting of sequences of n consecutive words. This method attempts to capture the 
language structure of texts by simple word sequences instead of by complex syntactic 
structures [10]. Somehow, it purpose is to obtain a rich characterization of texts without 
performing an expensive syntactic analysis. Nevertheless, due to the feature explosion, it 
tends to use only n-grams up to three words. 
In general, our method is very similar to the n-gram based approach. In both cases, 
documents are characterized by a combination of function and content words. However, 
ours considers a special kind of word sequences (namely, maximal frequent word se-
quences), which are determined by their frequency of occurrence instead of by their 
length. Using this strategy, it selects the most relevant word sequences, and indirectly 
tackles the problem of feature explosion. The following section describes in detailed the 
proposed method. 
3 Our Method 
As we previously mentioned, this paper presents a new method for authorship attribution. 
This method characterizes documents by a set of relevant sequences that combine func-
tional and content words. The idea is to use these sequences to classify the documents in 
view that they express the more significant lexical collocations2 used by an author. Tradi-
tionally, these sequences are extracted by applying a general n-gram calculus. In contrast, 
we propose to discover them by means of a process for mining maximal frequent word 
sequences. 
The following subsections define the maximal frequent word sequences, the process 
for their extraction, as well as a classification algorithm using them as document features. 
                                                          
2 A collocation is defined as a sequence of words or terms that co-occur more often than would be 
expected by chance. 
3.1 Mining Maximal Frequent Word Sequences 
Assume that D is a set of texts (a text may represent a complete document or even just a 
single sentence), and each text consists of a sequence of words. Then, we have the follow-
ing definitions [1]. 
Definition 1. A sequence p = a1…ak is a subsequence of a sequence q if all the items ai, 1 
= i = k, occur in q and they occur in the same order as in p. If a sequence p is a subse-
quence of a sequence q, we also say that p occurs in q.  
Definition 2. A sequence p is frequent in D if p is a subsequence of at least s  texts of D, 
where s  is a given frequency threshold. 
Definition 3. A sequence p is a maximal frequent sequence in D if there does not exist 
any sequence p´ in D such that p is a subsequence of p´ and p´ is frequent in D. 
Once introduced the maximal frequent word sequences, the problem of mining them 
can formally state as follows: Given a text collection D and an arbitrary integer value s  
such that 1 = s  = |D|, enumerate all maximal frequent word sequences in D.3 
It is important to mention that the implementation of a method for sequence mining is 
not a trivial task because of its computational complexity. The algorithm used in our 
experiments is described in [6]. 
3.2 Classification algorithms 
Authorship attribution is a classification problem, where a set of documents with known 
authorship are used for training and the aim is to automatically determine the correspond-
ing author of an anonymous text. Table 1 shows a direct classification algorithm based on 
the use of maximal frequent word sequences as document features. 
Table 1. Direct algorithm 
Let DT be the set of labeled documents that will be used for training 
Let d be an anonymous document 
TRAINING 
1. Set the value of the frequency threshold s  
2. Enumerate all maximal frequent word sequences in DT corresponding to 
the given frequency threshold 
3. Build the training instances using the discovered word sequences as Boo-
lean features 
4. Give the learning algorithm the training instances and perform training 
CLASSIFICATION 
1. Build the representation of d in accordance to the training feature space 
2. Let the trained classifier label the new instance 
 
                                                          
3 It is important to notice that a maximal frequent sequence may consist of only one single word. 
The proposed direct algorithm is conceptually simple and appropriate. However, it 
greatly depends on the adequate definition of the frequency threshold s . It is expected that 
different values of s  generate different sets of word sequences, and consequently produce 
different performance rates. For instance, low s -values allow extracting large sequences 
and favor the precision rate, while high s -values tend to generate many short sequences 
that support the recall percentage. Unfortunately, the most adequate s -value is influenced 
by the size of the given document collection, and therefore it need to be empirically de-
termined for each particular situation. 
In order to reduce the dependency of the classification performance to the used fre-
quency threshold, we propose to construct the feature set by combining the maximal fre-
quent sequences extracted by different s -values. The idea is to construct the feature set by 
an iterative process, incrementing the s -value at each step. This process starts with the 
inclusion of sequences corresponding to the frequency threshold s  = 2, and ends when 
there are not more lexical collocations (sequences of at least two words) to aggregate to 
the feature set. Table 2 describes the enhanced algorithm. 
Table 2. Enhanced algorithm 
Let DT be the set of labeled documents that will be used for training 
Let d be an anonymous document 
TRAINING 
1. Set the value of the frequency threshold s  = 2 
2. Set the feature set F1 = {∅} 
3. DO 
a. Enumerate all maximal frequent word sequences in DT correspond-
ing to the frequency threshold s . Name the set of sequences Ss 
b. Integrate new sequences to the feature set, i.e., Fs = Fs -1 ∪ Ss 
c. Increment the frequency threshold; i.e., s  = s  + 1 
     WHILE (Ss -1 contain at least one sequence of two or more words not in-
cluded in Fs -2) 
4. Build the training instances using the discovered Boolean features 
5. Give the learning algorithm the training instances and perform training 
CLASSIFICATION 
1. Build the representation of d in accordance to the training feature space 
2. Let the trained classifier label the new instance 
 
4 Experimental Setup 
4.1 Corpus 
Unfortunately, there is not a standard data set for evaluating authorship attribution meth-
ods. Therefore, we had to assemble our own corpus. This corpus was gathered from the 
Web. It consists of 353 poems writing by five different authors. Table 3 resumes some 
statistics about this corpus. It is important to notice that, on the one hand, the collected 
poems are very short documents (176 words in average), and on the other hand, that all of 
them correspond to contemporary Mexican poets. In particular, we were very careful on 
selecting modern writers in order to avoid the identification of authors by the use of 
anachronisms. 
Table 3. Corpus Statistics 
Poets Number of documents 
Size of 
Vocabulary 
Number of 
Phrases 
Average 
Words by 
Documents 
Average 
Phrases by 
Documents 
Efraín Huerta 48 3831 510 236.5 22.3 
Jaime Sabines 80 3955 717 155.8 17.4 
Octavio Paz 75 3335 448 162.6 27.2 
Rosario Castellanos 80 4355 727 149.3 16.4 
Rubén Bonifaz 70 4769 720 178.3 17.3 
 
4.2 Classifier 
The Naïve Bayes classifier has proved to be quite competitive for most text processing 
tasks including text classification. This fact supported our decision to use it as main clas-
sifier for our experiments. It basically computes the probability of a document d to belong 
to a category ci given the set of features F = {f1, f2,…, f|F|}.4 This probability can be ex-
pressed using Bayes’ rule as follows: 
( ) ( ) ( )( )dP
cPcdP
dcP iii
|
| =  
Simplifying and assuming statistical independence of the features: 
( ) ( ) ( )∏ ==
||
1
||
F
j ijii
cfPcPdcP  
These probabilities can be estimated directly from the training set as follows: 
( ) ( )
∑ =+
+
== ||
1
1
|, F
k ki
ji
ij
i
i
NF
N
cfP
N
N
cP  
where N is the number of documents in the whole collection, Ni the number of docu-
ments of category ci, and Nji the number of documents from category ci having the feature 
fj. Finally, |F| indicates the number of features. 
                                                          
4 Text classification is the problem of assigning a document d to one of a set of |C| predefined 
categories C = {c1, c2,…, c|C|}. 
4.3 Baseline Configurations 
Because of the difficulty of comparing our approach with other previous works –mainly 
caused by the absence of a standard evaluation corpus–, we performed several experi-
ments in order to establish a baseline. These experiments consider the use of four differ-
ent kinds of word-based features: (i) functional words, (ii) content words, (iii) the combi-
nation of functional and content words, and (iv) word n-grams. Table 4 shows the results 
corresponding to each one of these approaches. 
Table 4. Baseline configurations 
Features Accuracy 
Average 
Precision 
Average 
Recall 
Functional words 41.0% 0.42 0.39 
Content words 73.0% 0.78 0.73 
All kind of words 73.0% 0.78 0.74 
n-grams (unigrams plus bigrams) 78.8% 0.84 0.79 
n-grams (from unigrams to trigrams) 76.8% 0.84 0.77 
 
It is important to mention that because our main interest was to determine an appropri-
ate document characterization for authorship attribution, we used in all cases the same 
classification algorithm, namely, the naïve Bayes classifier. As well, we applied the same 
technique for dimensionality reduction (information gain) and the same evaluation 
schema (a 10-cross-fold validation).  
The results shown in table 4 are very interesting since they confirm some of our major 
assumptions. First, functional words by themselves do not help to capture the writing style 
from short documents. Second, content words contain some relevant information to dis-
tinguish between authors, even when all documents correspond to the same genre and 
discuss similar topics. Third, the lexical collocations, captured by word n-gram se-
quences, are useful for the task of authorship attribution. Fourth, due to the feature explo-
sion and the small size of the corpus, the use of higher n-gram sequences not necessarily 
improves the classification performance. 
5 Experimental Results 
In this paper, we have proposed the use of maximal frequent word sequences as document 
features for authorship attribution. This section presents the results of two basic experi-
ments. The first one evaluates the classification performance of the direct algorithm using 
different frequency thresholds (s ). The second experiment applies the enhanced algo-
rithm. It goal is to evaluate the impact of using a feature set that combines maximal se-
quences extracted by different s -values.  
In these experiments, as in the baseline generation, we used sequences considering not 
only content words, but also function words as well as punctuation marks. In the same 
way, we used the naïve Bayes classifier, the information gain technique for dimensional-
ity reduction5, and a 10-cross-fold validation schema. 
5.1 Experiments with the Direct Algorithm 
Table 5 shows the results obtained using different frequency threshold values. It can be 
noticed that for all s -values our results were worst than those obtained using the n-gram 
features (combining unigrams and bigrams). However, it is interesting to point out that 
number of sequences –for the best case– was much less than the number of n-grams, 4276 
and 45245 respectively. Moreover, after the dimensionality reduction, the number of 
sequences was less than the number of n-grams, 203 and 455 respectively. This condition 
indicates that even when our method did not outperform the n-gram based approach, it 
could obtain a reduced set of features with better discrimination capacity. 
Table 5. Results of the direct algorithm 
s  Number of sequences 
Average 
words per 
sequence 
Accuracy Average Precision 
Average 
Recall 
2 141 2.59 68.60% 0.76 0.69 
3 203 2.32 77.30% 0.82 0.77 
4 225 2.26 77.30% 0.82 0.77 
5 195 1.67 77.10% 0.81 0.77 
6 156 1.59 75.40% 0.79 0.75 
7 129 1.57 74.80% 0.78 0.74 
8 124 1.50 74.20% 0.76 0.74 
9 105 1.46 71.40% 0.73 0.71 
10 94 1.45 70.50% 0.72 0.70 
In addition, the results of table 5 demonstrate the great influence of the frequency 
threshold on the classification process. It is clear that the s -value determines the number 
and kind of discovered sequences, and therefore, it has a direct effect on the overall clas-
sification performance. In particular, it is noticeable that the accuracy decreases while 
increasing the frequency threshold. This is because high s -values tend to fragment se-
quences, losing several relevant lexical collocations. 
5.2 Experiment using the enhanced algorithm 
The enhanced algorithm (refer to section 3.2) constructs the feature set by combining 
maximal frequent sequences corresponding to different s -values. In this way, it attempts 
diminishing the dependency of the classification performance on the used frequency 
threshold. Table 6 gives some data on the construction of the feature set. This process 
started with the inclusion of large sequences (those having more discriminatory capacity) 
and ended with the insertion of short sequences (those having more coverage). In total, 
we assembled a set of 425 features. 
                                                          
5 In particular, we selected all attributes with information gain greater than 1. 
Tabla 6. Construction of the enhanced feature set 
s  Extracted Sequences 
Added 
Sequences 
Average 
Length of  
Added 
Sequences 
Number 
of Features 
2 141 141 2.58 141 
3 203 100 1.71 241 
4 225 80 1.76 321 
5 195 53 1.74 374 
6 156 23 1.35 397 
7 129 13 1.46 410 
8 124 12 1.25 422 
9 105 3 1 425 
 
Table 7 shows the results related to the enhanced algorithm. From these results, it is 
clear that the enhanced algorithm not only goes one better than the direct algorithm, but 
also that it outperforms all baseline configurations. Furthermore, given that the resultant 
feature set is comparable in size to the n-gram set, the obtained results validate our hy-
pothesis that determining the word sequences by their frequency of occurrence instead of 
by their length is a good strategy, which allows to select the most relevant word se-
quences and to tackle the problem of feature explosion. 
Table 7. Results of the enhanced algorithm 
Poets Precision Recall 
Efraín Huerta 1.00 0.75 
Jaime Sabines 0.83 0.83 
Octavio Paz 0.95 0.75 
Rosario Castellanos 0.65 0.91 
Ruben Bonifaz 0.94 0.87 
Average Rates 0.87 0.82 
Overall Accuracy 83% 
 
6 Conclusions 
In this paper, we proposed a new method for authorship attribution. This method is sup-
ported on the idea that a proper identification of author must consider both stylistic and 
topic features of documents. In particular, it characterizes the documents by a set of word 
sequences that combine functional and content words.  
Other previous approaches for authorship attribution also characterized documents by 
word sequences. Specifically, they used word n-gram features, that is, word sequences of 
a fixed predefined size. In contrast to these approaches, our method considers a special 
kind of word sequences (namely, maximal frequent word sequences), which are deter-
mined by their frequency of occurrence instead of by their length. The experimental re-
sults demonstrated that this kind of sequences are superior to the n-grams, since they 
allow capturing the more significant lexical collocations used by an author. 
It is also important to mention that our method, without using any sophisticated lin-
guistic analysis of texts, could outperform most of the state-of-the-art approaches for 
authorship attribution. Furthermore, our method, contrary to other current approaches, is 
not very sensible to the size of documents and the document collection. Somehow, this 
behavior was demonstrated by the experiments on the poem corpus. 
As future work, we plan to apply the proposed method (document characterization) to 
other problems of text classification. In particular, we want to investigate the contribution 
of function words to topic-based text classification. 
Acknowledgements 
This work was done under partial support of CONACYT (project grants 43990 and 
U39957-Y, SEPSEBYN-C01-40). We also thank SNI-Mexico and INAOE for their assis-
tance. 
References 
1. Ahonen-Myka H. (2002). Discovery of Frequent Word Sequences in Text Source. Proceedings 
of the ESF Exploratory Workshop on Pattern Detection and Discovery. London, UK, 2002. 
2. Argamon, S. & Levitan, S. (2005). Measuring the Usefulness of Function Words for Author-
ship Attribution. Association for Literary and Linguistic Computing/ Association Computer 
Humanities, University Of Victoria, Canada.  
3. Bekkerman, R. & Allan, J. Using (2004). Bigrams in Text Categorization. CIIR Technical 
Report IR-408 Center for Intelligent Information Retrieval, University of University of Mas-
sachusetts Amherst. 
4. Chaski, C. (2005). Who's at the Keyword? Authorship Attribution in Digital Evidence Inves-
tigations. International Journal of Digital Evidence. Volume 4, Issue 1.  
5. Diederich, J., Kindermann, J., Leopold, E. & Paas, G. (2003). Authorship Attribution with 
Support Vector Machines. Applied Intelligence, 19(1):109-123, 2003.  
6. García-Hernández, R., Martínez-Trinidad F., and Carrasco-Ochoa A. (2006). A New Algo-
rithm for Fast Discovery of Maximal Sequential Patterns in a Document Collection. Interna-
tional Conference on Computational Linguistics and text Processing, CICLing-2006. Mexico 
City, Mexico, 2006. 
7. Holmes, D. (1995). Authorship Attribution. Computers and the Humanities, 28:87-106. Klu-
wer Academic Publishers. 1995. 
8. Kaster, A., Siersdorfer, S., & Weikum, G. (2005). Combining Text and Linguistic Document 
Representations for Authorship Attribution. Workshop Stylistic Analysis of Text for Informa-
tion Access, 28th Int. SIGIR 1. MPI, Saarbrücken 2005, 27-35. 
9. Malyutov, M.B. (2004). Authorship Attribution of Texts: a Review. Proceedings of the 
program "Information transfer" held in ZIF. University of Bielefeld, Germany. 2004. 17 
pages. 
10. Peng, F., Schuurmans, D., Keselj, V. & Wang, S. (2004). Augmenting Naïve Bayes Classifiers 
with Statistical Languages Models. Information Retrieval, vol. 7, 317-345. Kluwer Academic 
Publishers. 2004. 
11. Stamatatos, E., Fakotakis, N. & Kokkinakis,G.  Computer-Based Authorship Attribution 
Without Lexical Measures. Computers and the Humanities 35: 193-214, 2001. Kluwer Aca-
demic Publishers. 2001 
12. Zhao, Y. & Zobel, J. (2005). Effective and Scalable Authorship Attribution Using Function 
Words. Lecture Notes in Computer Science, vol. 3689, 174-189. Springer Verlag. 2005. 
 
