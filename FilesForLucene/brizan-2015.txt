Author's Accepted Manuscript
Utilizing linguistically-enhanced keystroke
dynamics to predict typist cognition and
demographics
David Guy Brizan, Adam Goodkind, Patrick
Koch, Kiran Balagani, Vir V. Phoha, Andrew
Rosenberg
PII: S1071-5819(15)00097-X
DOI: http://dx.doi.org/10.1016/j.ijhcs.2015.04.005
Reference: YIJHC1959
To appear in: Int. J. Human-Computer Studies
Received date: 11 August 2014
Revised date: 23 April 2015
Accepted date: 27 April 2015
Cite this article as: David Guy Brizan, Adam Goodkind, Patrick Koch, Kiran
Balagani, Vir V. Phoha, Andrew Rosenberg, Utilizing linguistically-enhanced
keystroke dynamics to predict typist cognition and demographics, Int. J.
Human-Computer Studies, http://dx.doi.org/10.1016/j.ijhcs.2015.04.005
This is a PDF file of an unedited manuscript that has been accepted for
publication. As a service to our customers we are providing this early version of
the manuscript. The manuscript will undergo copyediting, typesetting, and
review of the resulting galley proof before it is published in its final citable form.
Please note that during the production process errors may be discovered which
could affect the content, and all legal disclaimers that apply to the journal
pertain.
www.elsevier.com/locate/ijhcs
Utilizing Linguistically-Enhanced Keystroke Dynamics
to Predict Typist Cognition and Demographics
David Guy Brizana, Adam Goodkinda, Patrick Kochb, Kiran Balaganib, Vir
V. Phohac, Andrew Rosenberga,d,∗
aCUNY Graduate Center, New York, NY, USA 10016
bNew York Institute of Technology, Old Westbury, NY, USA 11568
cLouisiana Tech University, Ruston, LA, USA 71272
dCUNY Queens College, Queens, NY, USA 11367
Abstract
Entering information on a computer keyboard is a ubiquitous mode of ex-
pression and communication. We investigate whether typing behavior is
connected to two factors: the cognitive demands of a given task and the
demographic features of the typist. We utilize features based on keystroke
dynamics, stylometry, and “language production”, which are novel hybrid
features that capture the dynamics of a typists linguistic choices. Our study
takes advantage of a large dataset (∼350 subjects) made up of relatively short
samples (∼450 characters) of free text. Experiments show that these features
can recognize the cognitive demands of task that an unseen typist is engaged
in, and can classify his or her demographics with better than chance accu-
racy. We correctly distinguish High vs. Low cognitively demanding tasks
with accuracy up to 72.39%. Detection of non-Native speakers of English is
achieved with F1=0.462 over a baseline of 0.166, while detection of female
typists reaches F1=0.524 over a baseline of 0.442. Recognition of left-handed
typists achieves F1=0.223 over a baseline of 0.100. Further analyses reveal
∗Corresponding Author Contact Information: Phone: +1 718 997 3562. Permanent
Address: Computer Science Department. Queens College. 65-30 Kissena Blvd. Queens,
NY, 11367
Email addresses: dbrizan@gradcenter.cuny.edu (David Guy Brizan),
agoodkind@gradcenter.cuny.com (Adam Goodkind), pkoch03@nyit.edu (Patrick
Koch), kbalagan@nyit.edu (Kiran Balagani), phoha@latech.edu (Vir V. Phoha),
andrew@cs.qc.cuny.edu (Andrew Rosenberg)
URL: http://eniac.cs.qc.cuny.edu/andrew/ (Andrew Rosenberg)
Preprint submitted to International Journal of Human-Computer Studies May 18, 2015
that novel relationships exist between language production as manifested
through typing behavior, and both cognitive and demographic factors.
Keywords: Keystroke Dynamics, Stylometry, Cognitive Load Recognition,
Demography Recognition, Typing Production
1. Introduction
As early as World War II, U.S. military intelligence began to identify1
individuals by their rhythms in tapping out Morse code messages. These2
rhythms, called “the Fist of the Sender,” supported the tracking of Morse3
code operators and therefore the troops and vehicles moving with the opera-4
tors. Since then, keystroke dynamics or typing dynamics have had a number5
of practical applications, including the determination of the cognitive de-6
mands of individuals [1]. In much the same way individual operators may be7
identified by their tapping rhythms and individual speakers may be identi-8
fied through spectral and prosodic features of their speech [2, 3], individual9
typists exhibit unique but self-consistent typing patterns [4, 5].10
A related field, stylometry, is also concerned with author identification11
[6]. Stylometry describes the measurement of linguistic “style” and has been12
effectively used in authorship attribution [7, 8], in dating a single piece of13
writing [9] and in establishing genre shifts within the work of a single au-14
thor [10]. However, whereas keystroke dynamics has been used to verify the15
identity of one of hundreds of typists, stylometric applications typically dis-16
tinguish between many fewer individuals. The metrics developed within this17
field typically rely on the user’s spelling of specific words, choice of words in18
a sentence and choices with respect to grammar. Stylometric analyses are19
applied to prepared, static text.20
In this paper, we describe two applications of combining keystroke dy-21
namics, stylometry and a new set of language production features: to identify22
the type of cognitive task a typist is performing and to identify three demo-23
graphic cohort identifications. We collect data from subjects’ typed responses24
to prompts which require them to engage in one type of task or another. This25
analysis is non-interruptive and non-intrusive. While our experiments oper-26
ate on data collected from local computer users, our work could easily be27
extended to remote users via keystroke logging software.28
In the case of predicting the type of cognitive task, we aim to determine29
whether the user is performing a cognitively simple task, such as recalling30
2
known information, or performing a more cognitively taxing task such as31
analyzing an argument or creating a new idea. The research presented in this32
paper rests on two assumptions. First, we assume that performing differing33
tasks will have differing associated cognitive demands. This assumption is34
based on the Bloom Taxonomy [11], widely used in education to categorize35
the cognitive demands of instructional activities. Second, we assume that36
the cognitive activity of a typist, particularly when performing a language37
production task, is reflected in his or her typing behavior. This assumption38
is supported by the findings of Vizer et al. [1], which observed that a user’s39
typing patterns vary based on the cognitive demands of a task. Specifically,40
we hypothesize that the cognitive demands of performing a task will have41
an observable impact on a typist’s behavior that can be measured through42
features related to keystroke dynamics, stylometry, and language production.43
In the first set of experiments presented in this paper, subjects respond to44
prompts which are drawn from different types of cognitive tasks with varying45
complexity. We then predict the type of task a subject is performing given46
the typing patterns and final response.47
For demographic prediction, we divide our subjects along three broad48
demographic dimensions: gender, dominant hand and primary language (na-49
tive vs. non-native speakers of English). Each of these demographic divisions50
may be viewed as a cohort with a different set of keystroke dynamics when51
compared to its counterpart. We aim to be able to place a user in a cohort52
based on the user’s typing patterns and language use, such as “left-handed,53
female, native English-speaker”. In the context of user identification and54
verification, this can be used as a filter to eliminate some candidates from55
further consideration enabling more focused downstream analysis.56
This work employs a number of novel features for keystroke dynamics57
and stylometry. In addition to measuring hold and interval times of each58
key individually, we explore aggregations of keys based on their keyboard59
position, which distinguishes, for example, keys typed by the left and right60
hand. By performing stylometric analysis on streams of typed data, we are61
able to develop features measuring revision behavior in addition to the final,62
static text. Moreover we develop a number of language production features63
which extend traditional stylometric measures with information about their64
timing.65
The most important contributions of our study are:66
• We demonstrate how the type of task a typist is performing – based67
3
on the expected cognitive demand – affects typing output. Previous68
studies have centered around a homogeneous task type, whereas we69
can show the effects of varying cognitive demands.70
• We propose and implement a new class of features, keystroke language71
production. These features take advantage of both keystroke dynam-72
ics and stylometry, to capture the dynamics, or prosody, of a typist’s73
language production.74
• The text being analyzed in this work is entered freely, with minimal75
constraints as to length or content. Moreover, predictions are made76
using much less data per answer than comparable studies, and with77
significantly more subjects.78
• Typical studies of this kind (cf. Section 2) attempt to model the behav-79
ior of a typist and compare subsequent samples of the same person’s80
typing to this model. In this work, we demonstrate the value of typing81
behavior to generalize to unknown typists, i.e. those not seen during82
training.83
Our paper is structured as follows: Section 2 discusses related work and84
applications. Section 3 describes the methods that are in common between85
the two sets of experiments including details of the data collection (Section86
3.1) and a description of the features we analyze (Section 3.2). Sections 4.187
and 4.2 describe experiments in predicting cognitive task and demography88
from an unknown typist, respectively. We conclude and discuss future work89
in Section 5.90
2. Related Work91
Many researchers have employed keystroke dynamics for a variety of pur-92
poses including individual and cohort identification, identification of the typ-93
ist’s stress level or emotional state and a measurement on cognitive perfor-94
mance [12, 13, 14]. Because keystroke dynamics uses available hardware95
common to most computer systems, it is especially effective as a “soft bio-96
metric” in authentication systems [15, 16, 17]. A “soft biometric” predicts97
demographic classification but not specific identity.98
Specifically with respect to the cognitive complexity of a task, [18] found99
a negative correlation between programming performance and typing speed.100
4
While programming is a cognitively demanding task, this experiment was101
designed for a specific population group (student programmers) performing102
a specific task and the specific task is essentially making the experiment a103
speed-accuracy tradeoff in a limited domain.104
In work that serves as support for our assumption that cognitive demands105
have an impact on typing behavior, [1] used features drawn from keystroke106
dynamics (as well as stylometry) to infer levels of cognitive stress with 75%107
accuracy. This work discusses unobtrusive monitoring of physical and mental108
health, for example in the aging population. The experiments in [1] require109
each subject register with the system and establish performance means and110
ranges under different stress conditions. These distributions are later used111
to determine the stress of the subjects by comparing to a recorded set of112
distributions.113
For gender prediction, both [19] and [20] found traction in predicting gen-114
der with the use of features from keystroke dynamics; in fact, they further115
employed this prediction as a soft biometric in user identification on the GR-116
EYC data set [21]. [19] reports approximately 80% accuracy in determining117
gender with a single classifier and [20] reports 91.63% accuracy, exceeding118
the 73% baseline. However, their experiments require the user to type pre-119
defined text (specifically a password string) on which the determination of120
gender is performed.121
Detecting a typist’s dominant hand (handedness) is an intuitive applica-122
tion of keystroke dynamics. Some of the earliest mentions of using keystroke123
dynamics for authentication [22] note this. Experiments in the prediction of124
handedness were undertaken in [23], though they were tested on a fixed set125
of passwords, with the same users employed for training and testing.126
Stylometric features have been used in predicting gender differences in127
writing styles with success [24, 25, 26]. [26] report 77.39% accuracy (on a128
49.4% baseline with a minimum of 500 words in the training set), 70.2%129
F1 (on a minimum of 150 training words) and an accuracy range of “ap-130
proximately 80%.” Similar to work presented here, [25] and [26] extract an131
extensive feature set without assumptions about what stylometric features132
would be most useful.133
Our experimental design is closest to [27], in which the topics and genres134
have been carefully balanced to avoid bias. Similar to our results, they find135
character-level features are best at differentiating between males and females,136
yielding 71.3% accuracy in doing so with a training set composed of 430 -137
470 words per user. [28] also use stylometric features to predict gender (at138
5
48.2% F1 score) as well as native language (91.6% F1 score) and publication139
venue in scientific articles. In both experiments, the corpora are limited to140
two particular domains: blogs and scientific papers.141
The experimental data used in this work has also been utilized in two re-142
lated papers, [29] and [30]. These studies both investigate user verification,143
where the subject’s keystrokes are used to train a template, and then tested144
against a subject pool to uniquely identify future typing from the same user.145
In addition, while both studies take advantage of some of the language pro-146
duction features, they do not use the full set reported below. In this work, we147
are investigating the generalization of these keystroke-derived features from148
one group of typists to another, either with respect to the type of task that149
is being performed, or with respect to demographic cohorts.150
3. Methods151
In this section we describe the experimental methods shared by the exper-152
iments in predicting cognitive task (cf. Section 4.1), and recognizing demog-153
raphy (cf. Section 4.2). We first describe the data collection in Section 3.1. In154
Section 3.2, we describe the features used for the analysis and classification155
in both subsequent experiments.156
3.1. Data Collection157
The typing data was collected from 1,013 Louisiana Tech University stu-158
dents (hereinafter referred to as “subjects”) in two sessions; 838 attended a159
first session (April-May 2012), 491 attended a second (October 2012) with160
486 attending both sessions. Subjects received unique IDs identifying them161
across sessions, and we collected a number of self-reported demographics162
for each subject including age range, gender, native language, handedness163
and typing style. All of the demographic indicators are based on these self-164
reports. The subjects are 41.3% female, 56.4% male; 79.7% native English165
speakers, 17.0% non-native English speakers, and 88.3% right-handed and166
9.1% left-handed. Note that these do not sum to 100%; on each question167
some percentage of subjects chose not to respond to one or more of the168
demographic questions.169
In addition, we asked subjects whether they look at their hands when170
typing (visual typing) or look at the screen (touch typing); 64.7% of subjects171
use touch typing, while 31.3% use practice visual typing. We note that this172
was self-reported, and not objectively verified. In other words, a subject173
6
might want to believe that he does not look at his hands when typing, but174
in fact, he usually does.175
All experiments below were performed on the entire population, both176
touch- and visual-typists, whereas some previous studies (e.g. [31]) report177
results from only touch typists. We believe this makes the reported results178
more robust, as we did not restrict our subject set to only a subset of the179
population based on typing style.1180
In each session, the subjects were seated at a Dell desktop with a QWERTY181
keyboard and presented with a series of prompts in Standard American En-182
glish selected from those listed in Appendix A. The subject was required183
to type at least 300 characters in response to each prompt, at which time184
the subject was presented with a button allowing him or her to proceed to185
the next prompt. Each subject responded to 10 - 12 prompts. The prompts186
in session 1 were completely distinct from those in session 2, though they187
were drawn from the same categorization of cognitive tasks. Prompts were188
presented in random orders, though there was an equal distribution of each189
task type. (However, in the second session, one of the level 3 questions was190
omitted and replaced by a level 5 due to an error in the stimuli preparation.)191
The average response contains 921 keystrokes; the final response contained192
an average of 448 characters and 87 words. A keylogger with 15.625 millisec-193
onds clock resolution was used to record text and keystroke event timestamps194
[29]. This collection protocol was reviewed and approved by the Louisiana195
Tech University IRB.196
Within each valid response, we do not perform any outlier removal. How-197
ever, the data was vetted to ensure that all responses are, in fact, responses to198
the prompt. This editing was restricted to those instances where a subject199
was unquestionably unresponsive – this included cases where the response200
included a seemingly random sequence of characters, or a word or phrase201
repeated multiple times, until the minimum character count was reached.202
However, as distinguishing between an outlier and an idiosyncrasy can be203
difficult and subjective, this editing was quite conservative. Only in cases204
where a research assistant was completely certain that the subject was non-205
responsive was the response removed from the data set.206
1In unreported results, we also repeated all experiments on only touch-typists. We
found that the results were only marginally different, and not consistently better nor
worse.
7
Each prompt was drawn from one of six tasks: Remember, Under-207
stand, Apply, Analyze, Evaluate, or Create. This task type was208
determined by the experimenters, who assessed the cognitive demands of209
each question as they related to Bloom’s Taxonomy [11]. Bloom’s Taxonomy210
assigns a level from 1-6 to these cognitive tasks. We use this as an ordering211
of tasks from low to high cognitive demand. Table 1 contains a list of the212
types of tasks, the cognitive activity required to respond to each prompt213
and a sample question associated with each task type. Appendix A contains
Table 1: Cognitive Load Definitions and Example Prompts
Task and Level Required Activity Example Prompts
Remember - 1 Retrieve knowledge from
long-term memory or ex-
plain
List the recent movies you’ve seen
or books you’ve read. When did
you see or read them? What were
they about?
Understand - 2 Explain, Summarize or
Interpret
Where is a place that you par-
ticularly enjoy visiting? Describe
what makes you happy about be-
ing at this place.
Apply - 3 Apply, execute or imple-
ment
What would you do if you and a
friend are on vacation alone and
your friend’s leg gets cut? De-
scribe what procedure you would
use for first aid or for finding help.
Analyze - 4 Organize or break ma-
terial into constituent
parts
Explain what you think the dif-
ference is between “communicat-
ing with” someone and “talking
to” someone. How are these two
terms often confused?
Evaluate - 5 Critique or make judg-
ments based on criteria
Do you think it’s a good idea to
raise tuition for students in order
to have money to make improve-
ments to the University? Why or
why not?
Create - 6 Generate, plan or put el-
ements together
Pretend a Hollywood executive
offered to pay you to write and
act in a movie. Create a movie
plot with a character in it for
yourself and remember that you
will only be paid for creating an
original plot to a movie.
214
8
all of the questions posed to the subjects and the task associated with each215
question.216
This data was collected with the intention of it being used to evaluate user217
verification. In order to measure the impact of the type of behavior a user is218
engaged in on the consistency of his or her typing, the subjects were asked219
to perform a wide range of different tasks. In this work, we are investigating220
whether we can recognize what type of behavior an unseen typist is engaged221
in, by observing other typists performing similar tasks (though responding222
to different prompts).223
We note that the cognitive level measure of a prompt is most accurately224
interpreted as the expected cognitive demands as hypothesized by Bloom’s225
Taxonomy. It is, of course, possible or even likely that some subjects may ex-226
perience different cognitive loads than would be expected by a given prompt.227
For example, a subject may choose to create new knowledge (e.g., making up228
his or her favorite movie), rather than retrieve a memory in responding to a229
Remember prompt. Additionally, a subject may have experiences that are230
relevant to a Create prompt (e.g., having written a film script); this may231
lead to a response which is based more on recall than creative thought. In232
addition to these confounds, there are environmental effects that may lead to233
a user experiencing a higher than expected cognitive load. For example, a cell234
phone might ring, or they may be distracted by other thoughts. Due to these235
effects, we note that there may be a significant amount of noise between the236
true cognitive demand which a subject is experiencing while responding to237
a prompt and the expected cognitive demands dictated by the prompt itself.238
Measuring the discrepancy between these would provide valuable informa-239
tion for this research, but is outside the scope of this work, and the protocol240
under which the data was collected.241
We also note that while Bloom’s Taxonomy provides a valuable way to242
delineate a set of tasks (e.g. rote knowledge vs. knowledge creation), the tax-243
onomic, hierarchical nature of Bloom’s system has been called into question244
[32] and heavily revised for pedagogical purposes [11]. Further, it is often dif-245
ficult to achieve consensus as to which label to assign, even among a group of246
subject-matter experts [33]. Our findings reflect the fact that Bloom’s tasks247
are not hierarchical or continuous in nature, but rather, reflect different,248
discrete tasks.249
9
3.2. Features250
The features used to analyze each subject’s answers fall under 3 broad251
categories: keystroke dynamics, stylometry, and language production. There252
were a total of 2,381 features extracted. Table 2 describes the broad classes253
of features used in this work.254
Due to the short length of subject responses, many features are not255
present in a given response. If a certain feature was not present in the256
test set, we replaced missing features with the mean feature value calculated257
over the training data. This guarantees minimal impact on classification per-258
formance due to unseen features. However, the ‘count’ of a particular event259
is never a ‘missing’ feature, a count is simply 0. An example of a missing260
feature would be the mean key interval between keys ‘Q’ and ‘Z’. If a user261
has never typed these two characters in sequence, there is no mean interval262
that can be calculated. No outlier removal or modification was applied to263
the observed features. While this may improve performance, we are hesitant264
to modify any observed data, as what may appear to be an outlier, may be265
an important signal to some population or uniquely indicative of a task.266
Table 2: Feature List
Feature Group (Count of Features) Feature Type Example Feature
Keystroke Dynamics (2098)
Key Hold Mean Shift Key Hold, Mean H Key Hold
Preceding Pause Variance of Pause Before Spacebar, Mean Pause Before T Key
Hand-based Mean Left Hand Key Hold
Finger-based Variance of Index Finger Key Holds
Keyboard Row Mean Pause Preceding Home Row Keys
Common/Rare Consonant/Vowel Mean Pause Preceding Common Consonants
Common to Rare Character Ratio Ratio of Mean Consonant to Vowel Key Hold
Stylometry (89)
Sentence Metrics Mean Sentence Length, Total Sentence Count
Word Metrics Median Word Length
Character-Type Metrics Alphabetic to Numeric Character Ratio
Capitalization Metrics Capital to Lowercase Character Ratio
Type-Token Ratio Lexical Diversity, Lexical Density
Language Production (194)
Part-of-Speech Timing Variance of Pause Preceding Noun
Punctuation Timing Mean Pause Preceding Comma
Misspelling Metrics Ratio of Misspelled to Correctly Spelled Words
Revision Metrics Mean Time Spent Revising Text
Lexical Units Within Burst Mean Words Produced in Typing Burst
Keystroke Dynamics Features267
Keystroke dynamics looks at the speed at which a user’s hands move across268
a keyboard [34] and the timing between keystrokes. The features analyzed269
in the present study capture rate and rhythm qualities including the overall270
user typing speed, durations and frequencies of pauses in typing, and pauses271
before specific keys.272
As noted in Table 2, we created a large amount of keystroke dynamics273
features. This is to be expected, as the size of a feature set capturing every274
10
key combination would be equal to the number of keys squared. This leads275
to many empty features. Thus the number of features reported in the above276
table represents an upper bound on the effective dimensionality of the feature277
vectors. We note that all empty features are ignored by all classifiers.278
With greater than 90% accuracy, a typist can be identified by the rate279
and rhythm of their typing (see [35, 34, 6, 4, 36, 37]). The rate of keystroke280
production may be indicative of familiarity with the typed material. Taking281
the latter notion one step further, familiarity with the typed material may282
have a multitude of underlying causes, from the physical presence of the283
typed text or availability in memory (affecting cognitive demand) to the284
native language of the typist. Here we investigate whether these measures285
are consistent across a population by virtue of their demographics, or by the286
task which they are engaged in.287
Key Intervals and Key Holds: One of the staple metrics of keystroke288
dynamics, digraph rates measure the latency between any two keystrokes289
and the duration that each key is depressed [17]. For the present study, we290
utilize the mean latency between any two keystrokes, including punctuation,291
symbols and numbers, and the mean “hold”, the duration of depression, for292
each key.293
Consonant timing: If a user is more familiar with English, and the distri-294
bution of letters in the English language, it stands to reason that he or she295
is also more adept at quickly depressing the more common letter keys. On296
the other hand, if a user is newer to the English language, he or she may be297
equally adept at striking any keyboard key [38]. By looking at the timing298
surrounding common and rare characters, we hoped to determine language299
familiarity.300
• Common consonant timing: Mean time between pressing any key and301
then pressing a common consonant. Common consonants are defined302
as elements of the set (h, n, r, s, t) [39].303
• Common consonant timing ratio: Ratio of pause between any two304
keystrokes and pause between a keystroke of any key to a keystroke305
of a common consonant key.306
• Rare consonant timing: Mean time between pressing any key and then307
pressing a rare consonant key. Rare consonants are defined as elements308
of the set (j, k, q, v, x, z) [39].309
11
• Rare consonant timing ratio: Ratio of pause between any two keystrokes310
and pause between a keystroke of any key to a keystroke of a rare con-311
sonant key.312
• Common to rare consonant timing ratio: Ratio of common consonant313
timing to rare consonant timing.314
Hand- and Finger-specific timing: Traditional keystroke dynamics per-315
forms measurements based on each key individually. In this set of features316
we group keys by their canonical hand and finger that would be used to type317
them based on “touch-typing” norms. For example, the ‘A’ key is indicated318
by Left Hand, Little Finger. This categorization enables us to measure319
the key holds intervals grouped by hand (Left, Right) and finger Index,320
Middle, Ring, and Little Finger). We do not include the thumb in these321
measurements. The space bar is the only canonical key for the thumb, and322
our data is not able to distinguish which thumb is used to hit the spacebar.323
Based on these broad classes, we extract key holds and key intervals based324
on 1) hand, 2) finger, and 3) finger and hand.325
Keyboard row production rates: Similar to the features mentioned above326
we group keys by canonical row. For example, every key in the Home row,327
i.e. Caps Lock though Enter, is grouped under one category. Given these328
categories, we extract key hold and interval measures based on the key’s row.329
Stylometric Features330
Stylometry incorporates syntactic, lexical and semantic analyses of a given331
text. Every aspect from average sentence length to part-of-speech frequency332
falls under the purview of stylometric analysis. Stylometry analyzes static333
text rather than the dynamic features of text production (cf. [7]).334
Stylometry represents measurements of linguistic information to quantify335
the individual “style” of a writer. The development of authorship attribution336
has been markedly improved by the development of more sophisticated Nat-337
ural Language Processing techniques [40]. We hypothesize that the writing338
style of a typist is impacted by the cognitive demands of the given task, and339
moreover, that stylometric features reveal key demographic information.340
Linguistic unit lengths: The most basic set of stylometric features counts341
the number and length of linguistic units, words and sentences.342
• Sentence count: Utilizing Apache OpenNLP’s Sentence Detector [41],343
rather than rely on common sentence-terminating punctuation, we counted344
the number of sentences per response.345
12
• Mean sentence length: Utilizing the same resources as sentence count,346
the mean number of word tokens per sentence was determined. As a347
language user gains better command of a language, their mean sen-348
tence length also becomes longer [40]. This has repercussions to native349
language, and cognitive load classification.350
• Word token count: The number of word tokens in a response. The351
tokenizer divides words such as “that’s” into the two words, “that”352
and “is”.353
• Mean word token length: A measure of the mean word token length,354
in number of characters.355
Character type: Use of, or disuse of, specific character types can aid356
in the identification of typist [42]. Certain users may prefer to write out357
numerals, e.g. “four”, versus Arabic numerals, e.g. “4.” Further, users tend358
to exhibit patterns in capitalization, e.g. “Soccer Champion” versus “soccer359
champion” [43]. We contrast these linguistic types to the keyboard position360
types described previously.361
• Alphabetical character ratio: Ratio of alphabetical characters (a-z,362
non-numeric) to total number of keystrokes363
• Numeric character ratio: Ratio of numeric characters (0-9) to total364
number of keystrokes365
• Uppercase character ratio: Ratio of uppercase characters to total al-366
phabetical characters367
• Spacebar ratio: Ratio of total depressions of space bar to total keystrokes368
• Vowel ratio: Ratio of vowels to total number of alphabetical characters369
Consonant frequency We hypothesize that the use of common or rare370
consonants (cf. [39]) are indicative of lexical complexity. If a subject uses a371
high ratio of rare consonants, this could be a marker of more sophisticated372
word use, and/or a more advanced vocabulary.373
• Common consonant ratio: Ratio of h, n, r, s, t to total alphabetical374
characters375
13
• Rare consonant ratio: Ratio of j, k, q, v, x, z to total alphabetical376
characters377
Lexical Diversity: Lexical diversity measures the ratio of unique words378
to total words. As a metric of writing style, lexical diversity is one of the379
oldest methods for authorship attribution, preceding many more complicated380
analyses [44].381
• Type-token ratio: This is the most basic measurement of lexical diver-382
sity, in which the number of unique word tokens is divided by the total383
number of words. This generally reflects the size of a typist’s vocab-384
ulary, as a larger vocabulary results in the use of a greater number of385
different words.386
• Moving-average type-token ratio (MATTR): The primary shortcom-387
ing of the type-token ratio is that it does not control for length, i.e.388
longer texts will usually have a lower ratio [45]. MATTR, on the other389
hand, only considers a fixed number of words at a time, and increments390
through the text, e.g. words 1-50, 2-51, 3-52, etc. This produces more391
informative results when comparing texts of different lengths [45].392
Lexical Density: Lexical density measures the number of unique parts of393
speech divided by the total number of word tokens. Higher lexical density is394
used as a measure of language complexity. [46]395
Language Production Features396
Production features are a hybrid of the above two categories, incorporating397
elements from both linguistic analyses and keystroke rate and timing. While398
stylometric and keystroke dynamic features are measured independently of399
one another, language production features use elements of both to create400
unique categories of features. For example, while stylometric features may401
look only at the frequency of verbs to nouns, and keystroke dynamics may402
look only at average keystroke typing speed, language production features403
may measure the average typing speed of verbs versus nouns.404
Our investigation is informed by linguistic meta-information. Given that405
languages exhibit certain predictable linguistic patterns, we hypothesize that406
these patterns are also borne out during the typing process [28]. By exploiting407
this fact, we hope to gain a more nuanced understanding of typing patterns,408
based on lexical and syntactic patterns.409
The features we created, based on a hybridization of keystroke dynamics410
and stylometry, follow.411
14
Part-of-speech pauses: We measure the mean length of a pause before and412
after each word as represented by its part of speech. By combining pause413
data with this syntactic data, we hope to deduce underlying features of a414
typist’s habits. The parts of speech used in this study were: a) Nouns: sin-415
gular, plural, proper, gerund; b) Verbs: verbs of all tenses and persons, past416
participles, modals; c) Modifiers: numbers, determiners, adjectives, adverbs,417
wh-determiners, wh-adverbs.418
Punctuation pauses: This metric measured the pause time before and419
after punctuation marks that break up phrases or units of thought. These420
include sentence ending punctuation (periods, exclamation points, question421
marks), commas, and semicolons [1]. We hypothesize that a user who pauses422
for a greater length of time around a phrase terminating punctuation mark423
is engaging in planning behavior indicating greater cognitive load.424
Misspelling pauses: This metric used data from Jazzy Spell Checker [47]425
to identify the correct spelling and common misspellings of individual words.426
From this we calculate whether a user pauses longer or more often before427
and after misspelled words.428
Revision Features: We hypothesize that a typist’s behavior when revising429
previously typed text is influenced by cognitive load and language familiarity.430
We define “in revision” as any delete or backspace keystroke and any time at431
which the typist is not at the leading edge of the buffer, but rather has gone432
back and made a revision. This allows us to characterize a user’s typing as433
being “in revision” or not. Using this distinction, we extract a number of434
features to capture a typist’s revision behavior.435
• Mean character length of revisions: A typist’s behavior or demogra-436
phy may be characterized by whether he or she makes long or short437
revisions.438
• Mean length of time in revisions: We hypothesize that a typist may439
pause and think more during a revision if he or she is performing a task440
with greater cognitive demands, while typists executing less demanding441
tasks will make brief, immediate corrections.442
• Revision ratio: This feature measures the length of time in revision to443
the time of the overall typing session. This can be loosely considered444
as how “confidently” a typist is behaving, i.e. whether he or she is445
constantly backtracking and making corrections, or spending more time446
in the production of novel text.447
15
Typing Burst Features: In addition to segmenting lexical events by traditional448
grammatical units, e.g. sentences, we also divide typing sessions according449
to when a user paused during his or her typing. We defined a pause as a450
cessation in typing greater than 250 milliseconds. This number is based on451
findings in [35], which found similar timing to be indicative of a suspension452
in typing activity, as opposed to a keystroke interval. After breaking down453
typing sessions in this manner, we analyze the events that took place between454
pauses.455
• Mean word count between pauses: This feature provided a baseline456
measurement of the number of complete words that occurred between457
each pause. If a typist is producing in a more “stream of consciousness”458
mode, we predict that this is indicative of cognitive task, as well as459
greater language familiarity.460
• Mean word count between sentence start and pause: We measured the461
number of words that were produced between the beginning of a new462
sentence and the typist’s first pause.463
• Mean character count between sentence start and pause: Similar to464
the previous feature, this feature counts the number of characters.465
By counting characters as opposed to words, we can control for word466
length, and detect features such as typist fatigue, which may be medi-467
ated by character count rather than word count.468
4. Experiments469
In this section, we present two sets of experiments: 1) predicting the470
cognitive task (Section 4.1), and 2) predicting the demographic indicators:471
gender, handedness and native language (Section 4.2). Both experiments use472
the data and features described in Section 3. In the following subsections473
we describe any methods that are specific to only one of the experiments,474
present and discuss experimental results.475
4.1. Experiment 1: Prediction of Cognitive Task476
We conduct four experiments to predict the type of cognitive task a typist477
is engaged in as described in Section 3.478
16
4.1.1. Experiment 1: Methods479
We divide subjects between one of two distinct sets, training and test,480
each with 352 distinct typists. These subsets are constructed such that that481
no user in the training set was included in the test set, and vice versa.482
Moreover, we use responses to prompts from session 1 for training, and483
prompts from session 2 for testing. Thus no specific prompt was used for484
both training and testing. This allows us to recognize the type of task rather485
than recognizing specifically the prompt to which a subject was responding.486
Training and testing sets contained approximately equal numbers of each487
type of cognitive task. We used Weka [48] to predict which type of task was488
being performed, using four classifiers: Naive Bayes, AdaBoost with single489
split decision trees, SVM with an RBF Kernel, and SVM with a Linear490
Kernel. For each experiment, available parameters are tuned using ten-fold491
cross validation on the training data. All of the classifiers, save Naive Bayes,492
include protections against overfitting in their training procedure (either via493
a sensitive objective function, or explicitly training criteria). They are well494
motivated for use in situations where the number of available features is large495
with respect to the number of data points.496
Our choice of classifiers centered around the types of features each could497
best work with. For example, SVM was used because it is defined by a498
convex optimization problem (no local minima). AdaBoost was selected for499
its ability to ensemble multiple (weak) classifiers. For all classifiers such as500
SVM, Logistic and Naive Bayes, missing features were imputed as the feature501
mean. These represent a set of classifiers that have shown to be effective on502
a variety of other classification tasks.503
Moreover, with the exception of Naive Bayes, all are well motivated in504
classification contexts where the dimensionality of the feature vector is large,505
relative to the number of training data points; they each include protection506
against overfitting via regularization, effectively eliminating irrelevant fea-507
tures from the model. By exploring a range of classification routines we are508
able to measure the difficulty of the classification task, rather than making509
any assumptions about which specific classifier will be most effective a priori.510
We explore a number of different ways to represent cognitive task. First,511
we attempt to classify each task individually. However, since assigning cog-512
nitive labels can be subjective, we also attempted broader classifications. We513
divided the tasks in groups of 2 and groups of 3, and also looked at the polar514
extremes, comparing only the most demanding tasks to the simplest tasks515
17
under Bloom’s Taxonomy, to remove any noise from moderately demanding516
tasks. Finally, we attempt a regression analysis, assigning each task a num-517
ber from 1 to 6, to determine if a linear relationship between the tasks is518
helpful (cf. the ordering of Bloom’s original taxonomy).519
4.1.2. Experiment 1: Results520
The results of experiments predicting cognitive task are reported in Table521
3. Baseline accuracy values appear in parentheses below the class granularity522
identifier along with the total size of the experiment. For the sake of concise-523
ness, we used the numeric labels (1-6) for each task (cf. Table 1). However,524
for classification experiments, each task was considered a discrete class. The525
best classifier for each experiment is bolded. P-values based on a one-tailed526
binomial proportion tests are included for all results.527
Table 3: Results of cognitive demand identification experiments
Class granularity Naive Bayes AdaBoost SVM-RBF SVM-Linear
6-way
Acc. 17.71 % 22.14% 33.14% 31.61%
(16.67%)
N=4236 p=0.036 p< 10−19 p< 10−150 p< 10−125
3-way
Acc. 35.17% 49.66% 48.11% 47.26%
(33.33%)
N=4236 p=0.0059 p< 10−105 p< 10−87 p< 10−77
2-way (1,2,3 vs. 4,5,6)
Acc. 47.07% 58.55% 59.56% 59.70%
(50.00%)
N=4236 p=0.99 p< 10−28 p< 10−35 p< 10−36
2-way (1,2 vs. 5,6)
Acc. 49.22% 66.66% 69.42% 69.68%
(50.00%)
N=3179 p=0.81 p< 10−79 p< 10−108 p< 10−111
2-way (1 vs. 6)
Acc. 53.95% 64.97% 70.55% 72.39%
(50.00%)
N=1416 p=0.0016 p< 10−29 p< 10−54 p< 10−65
We also reran the cognitive task recognition experiment using only the528
subset of subjects who were native English speakers. By running the ex-529
periment with only this subset, we aimed to elucidate whether non-native530
speakers were a source of noise, as their cognitive demands would include531
not only responding to the prompt but responding in a non-native language.532
Surprisingly, the results were only marginally improved. Correctly classified533
instances increased by 1.3%, and mean F-score only increased from 0.286 to534
0.307. Perhaps this speaks to the robustness of our feature-set, in that it535
18
cuts through any noise not related to cognitive demand. However, before536
drawing any strong conclusions, a more thorough analysis is required.537
Moreover, Table 4 lists how many answers were misclassified by 1, 2, 3,538
4 and 5 levels. These results closely parallel the above classification results.539
Table 4: Margin of Error
Margin of Error Accuracy Random Baseline
0 16.48% 16.67%
1 50.19% 44.44%
2 80.71% 61.11%
3 96.46% 77.78%
4 99.48% 94.44%
5 100% 100%
4.1.3. Experiment 1: Discussion540
For each of the experiments and for each classifier, we find that we are541
able to predict which type of task is being completed at or above baseline542
regardless of how we group the tasks. The independence of both subject and543
prompt across training and test sets demonstrates that these features, based544
on keystroke dynamics, stylometry and language production are able, to545
some degree, to capture differences in the cognitive demands of an unknown546
prompt being undertaken by an unknown subject.547
We observe that while there is not a single best classifier to predict cog-548
nitive demands, the SVM variants perform consistently and reliably above549
chance. The choice of kernel–RBF or Linear–never leads to significant differ-550
ences in accuracy at the 0.01 level. Differences between these two classifiers551
are irrelevant.552
Through inspection of the results of the three binary classification ex-553
periments, distinguishing High (Create and Evaluate tasks) and Low554
(Remember and Understand) cognitive demand tasks we can draw some555
conclusions about the differences between these categories. We find that as556
we restrict data points to instances of more extreme examples of cognitive557
demand, the overall accuracy increases. This is to be expected.558
We also observe that the ability to differentiate High from Low cognitive559
demands when the intermediate tasks are included, achieves a performance560
not exceeding 60%. By omitting these data points – the Remember, Un-561
derstand vs. Create,Evaluate – we see a ∼10% absolute improvement562
to accuracy on both the SVM and AdaBoost classifiers. This suggests that563
19
the inclusion of the Apply and Analyze tasks is a source of noise. Despite564
having fewer data points, we are seeing greater differentiation in the smaller565
data set. In contrast, the difference from omitting the level 2 and 5 data566
points is much more modest; the absolute change to accuracy is at most567
2.71%.568
When examining the results of finer distinctions of cognitive demand, we569
still achieve performance that exceeds the chance baselines. In the 6-way570
classification, SVM classification with RBF Kernels can predict the type of571
task with 33.14% accuracy over a baseline of 16.67%. The performance on572
3-way classification, distinguishing High (Create and Evaluate), Mid573
(Apply and Analyze) and Low (Remember and Understand) achieves574
49.66% accuracy over a 33.33% chance baseline (p< 10−105).575
These results suggest that it is possible to recognize what sort of task576
an unknown person is performing based on inspection of a relatively short577
observation (roughly 450 characters) of typing behavior. It is not yet deter-578
mined what should be considered the upper bound for performance on this579
task. Some of the errors are due to individual differences between typists,580
other errors are due to the discrepancy between different questions labeled581
as the same type of task. Another source of error is the use of a label for582
cognitive demand based on how we expect a typist to respond cognitively to583
a type of prompt instead of using a more empirical measure of the cognitive584
demand the subjects are, in fact, experiencing.585
A number of features proved to be especially useful in these experiments.586
Up to 12 of the most useful feature names are listed in Table 5 by the exper-587
iment performed along with each feature’s Information Gain Ratio, and its588
relationship to Bloom’s Taxonomic cognitive level of the task. For the rela-589
tionship to cognitive cemand, we determine one of five relationships based on590
the relationship between the feature value and the cognitive level associated591
with a task:592
• Ascending (↗): Increasing with higher cognitive level593
• Descending (↘): Decreasing with higher cognitive level594
• Bimodal (∨): Strictly lower with middle values of cognitive level595
• Unimodal (∧): Strictly higher with middle values of cognitive level596
• Multimodal (∼): Any relationship other than the above597
20
We find that for the 6-way classification, the relevant features do not show598
consistent directions in their relationship with task.599
The three-way classification of cognitive demand includes many groupings600
of keys by keyboard region, as opposed to specific bigrams. It is unclear for601
each of these which specific words lead to increased or decreased observances602
of these features, but it suggests that there is benefit in looking at broad603
classes of letters, rather than individual key intervals, when there is enough604
data to generalize from.605
The binary classification results show some consistent differences between606
high and low cognitive demand tasks. Responses to prompts that are likely607
to require higher cognitive demand tend to include more modal verbs (e.g.608
“could”, “may”). Responses to prompts requiring greater cognitive demand609
also contain a decreased upper case ratio – possibly due to longer sentences as610
well as fewer proper names, fewer space characters – indicating fewer words611
– increased lexical density – a measure of the complexity of the sentence –612
and a significant amount of differentiation between groups of keys both by613
keyboard position and the distinction of rare and common consonants.614
4.2. Experiment 2: Prediction of Demography615
In this section we describe a number of experiments to predict three616
demographic indicators: gender (male vs. female), handedness (left vs. right)617
and primary language (English vs. non-English).618
4.2.1. Experiment 2: Methods619
The demographic labels were extracted according to how the subjects620
identified themselves (cf. Section 3). Although we recognize there may be621
differences between the way subjects identify themselves and their actual622
demographics, we nonetheless identify the subjects based on these labels in623
these experiments. As in the cognitive load prediction, demographic predic-624
tion experiments are all performed on training and test data which contain625
both different subjects and different prompts. As some subjects did not re-626
spond to demographic questions, the number of distinct subjects in the train627
and test partitions for these experiments were 329, 344, and 348 for hand-628
edness, gender, and primary language, respectively. For consistency we kept629
the size of train and test sets equal in all experiments.630
Again, we use Weka [48] to perform classification experiments. Unlike631
the cognitive load classification experiments, the distribution of males and632
females and English and non-English speaking subjects are not even. Only633
21
Table 5: Cognitive Demand Feature Relevance Measured by Information Gain
Class Granularity Info Gain Feature
6-way 0.3724 ∼ Modal Counts (Mean, Median)
0.2200 ∼ Lexical Density (Mean, Median)
0.2054 ∼ “D” unigram Count
0.1819 ∼ Right Index to Right Index (Count)
0.1746 ∼ Right Ring to Left Middle (Count)
0.1674 ∼ Characters Per Word (Median)
0.1631 ∼ Right Ring to Right Index (Count)
0.1568 ∼ Middle Finger to Thumb/Space (Count)
0.1563 ∼ Characters Per Word (Mean)
3-way 0.1306 ↗ Modal Counts (Mean, Median)
0.1120 ∨ Right Ring to Right Index (Count)
0.1101 ↘ Right Index to Right Index (Count)
0.0920 ∧ “U” unigram Count
0.0873 ∨ Characters Per Word (Median)
0.0873 ∨ Top Row Right to Bottom Row Right (Count)
0.0821 ↗ Right Index to Left Little Finger (Count)
0.0795 ∨ Characters Per Word (Mean)
0.0765 ∧ Bottom Row Left to Top Row Left (Count)
0.0753 ∨ Uppercase Ratio (Mean, Median)
2-way 0.0864 ↘ Characters Per Word (Median)
{1,2,3 vs. 4,5,6} 0.0791 ↘ Characters Per Word (Mean)
0.0572 ↗ Middle to Ring (Count)
0.0530 ↗ Right Index to Left Ring (Count)
0.0514 ↘ Top Row to Thumb (Count)
0.0509 ↘ Top Row to Space (Count)
0.0497 ↘ Function Word to Vowel (Count)
0.0493 ↗ Right Index to Home Row (Count)
0.0490 ↘ None to Left (Count)
0.0487 ↗ Top Row to Left Ring (Count)
0.0421 ↗ Right Index to Left Little Finger (Count)
0.0418 ↘ Speed Thumb
2-way 0.1468 ↗ Modal Counts (Mean, Median)
{1,2 vs. 5,6} 0.0855 ↗ Top Row to Ring (Count)
0.0730 ↗ Right Index to Home Row (Count)
0.0667 ↗ Right Ring to Left Middle (Count)
0.0601 ↗ Right Index to Left Little Finger (Count)
0.0535 ↗ Top Row to Right Ring (Count)
0.0506 ↗ Ring to Middle (Count)
0.0489 ↗ Right to Left Home Row (Count)
0.0454 ↗ “OU” bigram Count
0.0452 ↗ Top Row Right to Home Row Right (Count)
2-way 0.43652 ↗ Modal Counts (Mean, Median)
{1 vs. 6} 0.20148 ↗ U Count
0.19811 ↗ “OU” bigram Count
0.1859 ↗ Right Ring to Left Middle (Count)
0.18245 ↘ Lexical Density (Mean, Median)
0.17017 ↗ “D” unigram Count
0.15441 ↗ Right Index to Home Row (Count)
0.15031 ↗ Top Row Left to Top Row Right (Count)
0.14758 ↗ Left Middle to Space Row (Count)
41.3% of subjects are female, 9.1% are left-handed and 17.0% have a native634
language that is not English.635
Thus, we treat these tasks as detection tasks, where the challenge is iden-636
tifying the minority class, either Female, Left-handed or “non-English”. As637
detection tasks we evaluate performance using Fβ-measure with β = 1 in638
detecting the minority class and ROC area of the demographic classification639
experiments using the features described in the previous section. We con-640
duct our experiments with four classifiers: LogitBoost, Naive Bayes, SMO641
(RBFKernel) and SimpleLogistic, tuning hyper-parameters through ten-fold642
cross validation on the training data. These classifiers were selected for their643
22
effective detection of minority classes when the training data has a skewed644
class distribution.645
4.2.2. Experiment 2: Results646
One pair of experiments uses all training data (“Unbalanced”), wherein647
the majority of demographic labels belongs to one class. Another pair (“Bal-648
anced”) is conducted on a downsampled set in which the number of labels649
of the majority set matches the number in the minority set for the train-650
ing material only. Downsampling is a standard technique used to address651
imbalanced data sets [49]. A summary of the results appears in Table 6.652
Baseline F-measure values appear in parentheses below the task. This base-653
line is based on random class assignment based on the unmodified training654
distribution.655
The F1 measure, or the harmonic mean of precision and recall, is a way to656
measure accuracy. Precision is the number of true positives (i.e. the number657
of items correctly labeled as belonging to the positive class) divided by the658
total number of instances labeled as belonging to the positive class (i.e. the659
sum of true positives and false positives, which are items incorrectly labeled660
as belonging to the class). Recall is defined as the number of true positives661
divided by the total number of instances that actually belong to the positive662
class (i.e. the sum of true positives and false negatives, which are items663
which were not labeled as belonging to the positive class but should have664
been) [50]. Higher F-measure indicates better performance. F-scores are665
used heavily in the Information Retrieval community, but has been applied666
to other detection tasks.667
The F1-measure does not lend itself to parametric distribution for statis-668
tical significance testing. To generate a p-value for these results, we use a669
randomization approach. We assign each data point a random class based670
on the training class distribution, and measure the F1 score of this random671
assignment. We repeat this random process 50,000 times, and calculate the672
rate at which the random process yields a higher F1 score than the classifier.673
This results in a measure of p-value for the null hypothesis.674
4.2.3. Experiment 2: Discussion675
We have varying levels of success in recognizing the demography of an un-676
known typist, although we do consistently classify with greater than chance677
performance. We are encouraged that we are able to predict with perfor-678
mance greater than chance using as little as 300 characters of data, however,679
23
Table 6: Results of prediction of demographic recognition experiments
Demographic Training LogitBoost Naive Bayes SVM-RBF Logistic
Gender Unbalanced F1 0.518 0.473 0.485 0.524
(0.447) p-value 0.0000 0.0022 0.0000 0.0000
Gender Balanced F1 0.516 0.468 0.462 0.513
(0.447) p-value 0.0000 0.01046 0.0484 0.0000
Handedness Unbalanced F1 0.010 0.183 0.043 0.113
(0.100) p-value 1.0 0.0000 1.0 0.1917
Handedness Balanced F1 0.050 0.223 0.009 0.097
(0.100) p-value 0.9999 0.0000 1.0 0.5871
Primary Language Unbalanced F1 0.170 0.355 0.000 0.254
(0.166) p-value 0.3818 0.0000 1.0 0.0000
Primary Language Balanced F1 0.037 0.462 0.455 0.387
(0.166) p-value 1.0 0.0000 0.0000 0.0000
there is still room for improvement here.680
In predicting primary language, we see a clear benefit from balancing681
the distribution of the training data; performance increases on all classifiers.682
We do not see the same benefit in predicting gender nor handedness. For683
gender we find performance to decrease, and handedness has mixed results.684
This is due to the fact that there is greater imbalance in the language and685
handedness labels compared to gender. The gender classification suffers from686
the reduced size of the training data while not benefiting sufficiently from687
class balance. For gender classification we do not find a consistently best688
performing classifier, though Simple Logistic yields the best overall results.689
For primary language and handedness classification, we find the Naive Bayes690
classifier to generate the best results. This is somewhat remarkable, as Naive691
Bayes includes no protection against overfitting when dealing with large (and692
potentially sparse) feature vectors.693
We also divided demographic prediction results by cognitive task to see694
if any task was particularly better or worse for predicting a certain demo-695
graphic. We found the results to be consistent across tasks, with the accuracy696
varying by less than 1%.697
In addition, we compiled prediction results for each answer. For 55% of698
the answers, we correctly predicted all 3 demographics. For 95% of the an-699
swers, we correctly predicted at least 2 of the 3 demographics. Being able to700
correctly predict demographics is of great interest to user verification appli-701
cation by providing a mechanism to pare down or prune a pool of subjects.702
We find a number of features are especially useful in predicting these703
demographic labels. For gender classification, three features are especially704
useful:705
24
1. the timing between a punctuation symbol and the spacebar is faster706
for male subjects than for female subjects,707
2. the timing before and after function keys is faster for males,708
3. the timing before and after common digraphs (such as “ou” and “er”)709
is faster for females.710
We hesitate to speculate as to the source of these differences. We do, how-711
ever, find it interesting that the differences are in language production and712
keystroke dynamics, but not in traditional stylometric indicators. This novel713
result is in contrast with previous studies which have found gender to be reli-714
ably predicted based on stylometry [24, 25, 26]. One source of this disparity715
is the length of the analyzed text; we are using a few sentences, while these716
studies have used full blog posts, emails, and full length texts averaging over717
30,000 words, respectively.718
For primary language, we find non-native English subjects are typically719
slower typists than those who identify as native speakers. Three features are720
especially telling:721
1. the timing before and after function keys including shift and backspace722
keys,723
2. the timing before and after the “.” key,724
3. the timing before and after common digraphs (“ou” and “er” for ex-725
ample).726
On each of these, non-native English typists are slower than native English727
speaking typists. While we expected to see faster typing in the native En-728
glish subjects, the differences are not so dramatic as to make it trivial to729
distinguish these groups. It is possible that our subjects, all students in730
an American university with experience typing, are more familiar with the731
QWERTY keyboard and with the language than an average person whose732
primary language is not English. Each of the three features mentioned pre-733
viously contribute more to effective classification of a typist as a native or734
non-native English speaker than overall typing rate. The increased pauses735
around function keys and the period (“.”) key may be evidence of increased736
sentence planning time, suggesting that non-native English speakers type737
most words at a similar rate as native English speakers, but take extra time738
planning a sentence or making a revision (longer pauses around backspace739
and delete keystrokes). These are novel observations which warrant addi-740
tional exploration to understand the impact of a speaker’s native tongue on741
their typing behavior.742
25
5. Conclusions and Future Work743
This paper describes two experiments: predicting the cognitive demands744
of a task being completed by an unknown typist, and predicting the gender,745
handedness and native language based on the same, short, unconstrained746
input. We have applied a number of novel features to these analyses, finding747
evidence that each of these typing-derived broad feature sets contribute to748
the prediction of cognitive effort and demography. While we have demon-749
strated performance that is consistently better than chance, there is still750
significant room for improvement, particularly in the recognition of demo-751
graphic cohorts.752
This work is distinct from prior work on recognizing cognitive load and753
in keystroke dynamics in a number of ways. The two most similar investi-754
gations into the relationship between typing and cognitive load are [18] and755
[1]. [18] investigates the typing speed of student programmers as a function756
of accuracy and cognitive stress. The methods and results are, however, too757
different to be compared directly to ours. Our work is different from [1]in758
that we infer a baseline for users’ cognitive demands from our training set759
(not derived from the same subject), rather than taking multiple days of data760
captured from the same individual at an earlier time. We also do not require761
the individual to perform any self-assessment of cognitive state. As such,762
our approach requires no customization to the user. In addition, although763
our experiments were conducted on consenting subjects, we do not assume a764
cooperative or self-assessing individual.765
Our experiments distinguish themselves by being typist-independent, and766
prompt-independent. In all of the presented experiments, the typist’s used for767
testing are never used for training. Therefore we avoid ever learning patterns768
that are unique to a particular person, but rather test the generalization of769
these behaviors across the type of cognitive task, or demographic cohort.770
Our experiments on recognizing the gender of a typist use orders of mag-771
nitude less data than [25] and [26]. Our experiments also stand in contrast to772
[51] in which gender is predicted at an average accuracy of 82.2% from email773
messages of at least 50 words; we perform a prediction based on our training774
set and 300 characters from a training set which has no material from our775
test subjects.776
In the future, we will continue to explore the value of these feature sets.777
While we have explored a wide range of features, we have not exhausted this778
space. For example, more sophisticated syntactic and discourse analysis can779
26
be performed. Refinement of revision based features may provide more in-780
sight into the typing behavior of a subject. We are exploring other keystroke781
dynamic features extending hold and interval measures to capture a longer782
range of typing rhythm information. We will explore additional classification783
algorithms and techniques for dealing with class imbalance.784
Finally, we will integrate this work with user authentication applications,785
as keystroke dynamics can be used as a biometric modality for user ver-786
ification. We will investigate whether using predictions of cognitive load787
can condition the traditional keystroke dynamics modeling to be sensitive to788
different language production modes. Additionally, we will use demographic789
predictions as a filter to identify cohorts of candidates during authentication.790
6. Acknowledgements791
This work was supported in part by DARPA Active Authentication grants792
FA8750-12-2-0201 and FA8750-13-2-0274. The views, findings, recommenda-793
tions, and conclusions contained herein are those of the authors and should794
not be interpreted as necessarily representing the official policies or endorse-795
ments, either expressed or implied, of the sponsoring agencies or the U.S.796
Government.797
27
Appendix A. Prompts Posed to Subjects in Data Collection
In this appendix, we include a full listing of the prompts used in the
collection of typing data. The collection was divided into two sessions, con-
taining distinct prompts with similar expected cognitive loads. Table A.7
contains the cognitive load, session number and prompt text for all prompts.
Table A.7: List of all Prompts along with their expected cognitive load and session number
Cog. Load Session Prompt
1 1 List the recent movies you’ve seen or books you’ve read. When did you see or read
them? What were they about? Please use complete sentences.
1 1 Which sport(s) do you like to watch/play?
1 1 What made you decide to join Louisiana Tech University?
1 2 What are some things that you like about Ruston?
1 2 What are your favorite things about winter?
1 2 What is the best thing you ever ate at a restaurant? Describe it.
2 1 Where is a place that you particularly enjoy visiting? Describe what makes you
happy about being at this place.
2 1 What is your favorite place to go out for a meal? What do you like about this place?
2 2 What would you say has been the best college class you have taken and what did you
enjoy about that class?
2 2 What is something that you dread talking to your family about? Why do you not
like to talk to them about this?
2 2 Can you describe the process of applying to college?
3 1 What would you do if you and a friend are on vacation alone and your friend’s leg
gets cut? Describe what the procedure you would use for first aid or for finding help.
3 1 What would you do if you were home alone and a fire started?
3 2 Suppose you were in NYC and had a very important presentation to give at 8AM the
next morning at Louisiana Tech. You get to the airport in New York to discover that
your flight has been delayed and will likely cause you to miss your layover in Atlanta.
What steps would you take to ensure that you are at Louisiana Tech in time for your
presentation?
3 2 What would you do if you woke up and realized your car would not start?
4 1 Explain what you think the difference is between “communicating with” someone
and “talking to” someone. How are these two terms often confused?
4 1 Compare and contrast two genres of music.
4 2 Compare and contrast two sources you use for news and current events.
4 2 Give step-by-step driving directions to your favorite place in or around Ruston.
4 2 Explain what the saying “Not all that glitters is gold” means.
5 1 What email provider do you think is the best?
5 1 What social networking web-sites do you use?
5 1 Do you think it’s a good idea to raise tuition for students in order to have money to
make improvements to the University? Why or why not?
5 2 Do you think that capital punishment should be legal? Why or why not?
5 2 Do you think people should be required to have car insurance? Defend your decision.
6 1 Pretend a Hollywood executive offered to pay you to write and act in a movie. Create
a movie plot with a character in it for yourself and remember that you will only be
paid for creating an original plot to a movie.
6 1 If you were to create a picture of any type of landscape you wanted what objects
would you include in it? How would you go about creating the landscape?
6 1 How would you design your class if you were the teacher? What subject would you
teach? How would you structure your tests?
6 2 Decide on a party or event that you want to have and write details as to how you
would plan this event. Write only about the planning you would do before the day
of the event.
References
[1] L. M. Vizer, L. Zhou, A. Sears, Automated stress detection using
keystroke and linguistic features: An exploratory study, International
Journal of Human-Computer Studies 67 (10) (2009) 870–886.
28
[2] D. A. Reynolds, T. F. Quatieri, R. B. Dunn, Speaker verifica-
tion using adapted gaussian mixture models, Digital Signal Processing
10 (1Ä̀ı3) (2000) 19 – 41. doi:http://dx.doi.org/10.1006/dspr.1999.0361.
URL http://www.sciencedirect.com/science/article/pii/S1051200499903615
[3] E. Shriberg, L. Ferrer, S. Kajarekar, Svm modeling of snerf-grams for
speaker recognition, in: Proc. ICSLP, South Korea, 2004.
[4] D. Gunetti, C. Picardi, Keystroke analysis of free text, ACM Trans. Inf.
Syst. Secur. 8 (3) (2005) 312–347. doi:10.1145/1085126.1085129.
URL http://doi.acm.org/10.1145/1085126.1085129
[5] Y. Sheng, V. Phoha, S. Rovnyak, A parallel decision tree-based method
for user authentication based on keystroke patterns, Systems, Man, and
Cybernetics, Part B: Cybernetics, IEEE Transactions on 35 (4) (2005)
826–833. doi:10.1109/TSMCB.2005.846648.
[6] O. Canales, V. Monaco, T. Murphy, E. Zych, J. Stewart, C. Tappert,
A. Castro, O. Sotoye, L. Torres, G. Truley, A stylometry system for
authenticating students taking online tests, in: Proceedings of Student-
Faculty Research Day, CSIS, Pace University, 2011.
[7] P. Juola, Authorship attribution, Foundations and Trends in informa-
tion Retrieval 1 (3) (2006) 233–334.
[8] E. Stamatatos, A survey of modern authorship attribution methods,
Journal of the American Society for information Science and Technology
60 (3) (2009) 538–556.
[9] F. Can, J. M. Patton, Change of writing style with time, Computers
and the Humanities (2004) 61–81.
[10] B. Kessler, N. Geoffrey, H. Schutze, Automatic detection of text genre,
in: Proceedings of the 35th Annual Meeting of the Association for Com-
putational Linguistics and 8th Conference of the European Chapter of
the Association of Computational Linguistics, 1997, pp. 32–38.
[11] L. W. Anderson, D. R. Krathwohl, B. S. Bloom, A taxonomy for learn-
ing, teaching, and assessing: A revision of Bloom’s taxonomy of educa-
tional objectives, Allyn & Bacon, 2001.
29
[12] C. Epp, M. Lippold, R. Mandryk, Identifying emotional states using
keystroke dynamics, in: Proceedings of the 2011 Annual Conference
on Human Factors in Computing Systems (CHI 2011), Vancouver, BC,
Canada, 2011, pp. 715–724.
[13] F. Monrose, A. D. Rubin, Keystroke dynamics as a biometric for authen-
tication, Future Generation Computer Systems 16 (4) (2000) 351–359.
[14] F. Bergadano, D. Gunetti, C. Picardi, Identity verification through dy-
namic keystroke analysis, Intelligent Data Analysis 7 (5) (2003) 469–496.
[15] N. Bartlow, B. Cukic, Evaluating the reliability of credential harden-
ing through keystroke dynamics, in: 17th International Symposium on
Software Reliability Engineering (ISSRE ’06), 2006.
[16] M. Villani, C. Tappert, G. Ngo, J. Simone, H. Fort, S. H. Cha,
Keystroke biometric recognition studies on long-text input under ideal
and application-oriented conditions, in: Computer Vision and Pattern
Recognition Workshop, 2006, pp. 17–22.
[17] R. Joyce, G. Gupta, Identity authentication based on keystroke laten-
cies, Communications of the ACM 33 (2) (1990) 168–176.
[18] R. C. Thomas, A. Karahasanovic, G. E. Kennedy, An investigation into
keystroke latency metrics as an indicator of programming performance,
in: Proceedings of the 7th Australasian Conference on Computing Ed-
ucation, Vol. 42, 2005, pp. 127–134.
[19] M. Fairhurst, M. D. Costa-Abreu, Using keystroke dynamics for gender
identification in social network environment, in: 4th International Con-
ference on Imaging for Crime Detection and Prevention (ICDP 2011),
2011.
[20] R. Giot, C. Rosenberger, A new soft biometric approach for keystroke
dynamics based on gender recognition, International Journal of Infor-
mation Technology and Management (IJITM) 11 (1/2) (2012) 35–49.
[21] R. Giot, M. El-Abed, C. Rosenberger, Greyc keystroke: A benchmark for
keystroke dynamics biometric systems, in: IEEE 3rd International Con-
ference on Biometrics: Theory, Applications, and Systems (BTAS’09),
2009.
30
[22] F. Monrose, A. Rubin, Authentication via keystroke dynamics, in: Pro-
ceedings of the 4th ACM conference on Computer and communications
security, ACM, 1997, pp. 48–56.
[23] S. Z. S. Idrus, E. Cherrier, C. Rosenberger, P. Bours, Soft biometrics
for keystroke dynamics: Profiling individuals while typing passwords,
Computers & Security 45 (2014) 147–155.
[24] S. Goswami, S. S. adn M. Rustagi, Stylometric analysis of bloggers’ age
and gender, in: Proceedings of the Third International AAAI Conference
on Weblogs and Social Media (ICWSM 2009), 2009.
[25] O. de Vel, M. W. Corney, A. M. Anderson, G. M. Mohay, Language
and gender author cohort analysis of e-mail for computer forensics, in:
Proceedings Digital Forensics Research Workshop, Syracuse, NY, USA,
2002.
[26] M. Koppel, S. Argamon, A. R. Shimoni, Automatically categorizing
written texts by author gender, Literary and Linguistic Computing
17 (4) (2002) 401–412.
[27] R. Sarawgi, K. Gajulapalli, Y. Choi, Gender attribution: Tracing stylo-
metric evidence beyond topic and genre, in: Proceedings of the Fifteenth
Conference on Computational Natural Language Learning (CoNLL ‘11),
2011, pp. 78–86.
[28] S. Bergsma, M. Post, D. Yarowsky, Stylometric analysis of scientific
articles, in: Proceedings of the 2012 Conference of the North Ameri-
can Chapter of the Association for Computational Linguistics: Human
Language Technologies, 2012, pp. 327–337.
[29] H. Locklear, S. Govindarajan, Z. Sitova, A. Goodkind, D. G. Brizan,
A. Rosenberg, V. V. Phoha, P. Gasti, K. S. Balagani, Continuous au-
thentication with cognition-centric text production and revision fea-
tures, in: Biometrics (IJCB), 2014 IEEE International Joint Conference
on, IEEE, 2014, pp. 1–8.
[30] K. S. Balagani, Investigating cognitive rhythms as a new modality for
continuous authentication, Tech. rep., DTIC Document (2013).
31
[31] D. X. Song, D. Wagner, X. Tian, Timing analysis of keystrokes and
timing attacks on ssh., in: USENIX Security Symposium, Vol. 2001,
2001.
[32] R. W. Paul, A. Binker, Critical thinking: What every person needs to
survive in a rapidly changing world., ERIC, 1990.
[33] M. J. van Hoeij, J. Haarhuls, R. F. Wierstra, P. van Beukelen, De-
veloping a classification tool based on bloom’s taxonomy to assess the
cognitive level of short essay questions, Journal of veterinary medical
education 31 (2004) 261–267.
[34] F. Bergadano, D. Gunetti, C. Picardi, User authentication through
keystroke dynamics, ACM Transactions on Information and System Se-
curity (TISSEC) 5 (4) (2002) 367–397.
[35] V. M. Baaijen, D. Galbraith, K. de Glopper, Keystroke analysis re-
flections on procedures and measures, Written Communication 29 (3)
(2012) 246–277.
[36] K. Killourhy, R. Maxion, Why did my detector do that?!: Predicting
keystroke-dynamics error rates, in: Proceedings of the 13th Interna-
tional Conference on Recent Advances in Intrusion Detection, RAID’10,
Springer-Verlag, Berlin, Heidelberg, 2010, pp. 256–276.
URL http://dl.acm.org/citation.cfm?id=1894166.1894184
[37] K. S. Killourhy, R. A. Maxion, Comparing anomaly-detection algorithms
for keystroke dynamics., in: DSN, IEEE, 2009, pp. 125–134.
[38] D. Gunetti, C. Picardi, G. Ruffo, Keystroke analysis of different lan-
guages: A case study, in: Advances in Intelligent Data Analysis VI,
Springer Berlin / Heidelberg, 2005, pp. 133–144.
[39] J. C. Stewart, J. V. Monaco, S.-H. Cha, C. C. Tappert, An investigation
of keystroke and stylometry traits for authenticating online test takers,
in: Biometrics (IJCB), 2011 International Joint Conference on, IEEE,
2011, pp. 1–7.
[40] E. Stamatatos, N. Fakotakis, G. Kokkinakis, Computer-based author-
ship attribution without lexical measures, Computers and the Humani-
ties 35 (2) (2001) 193–214.
32
[41] J. Baldridge, The opennlp project (2005).
[42] J. Grieve, Quantitative authorship attribution: An evaluation of tech-
niques, Literary and linguistic computing 22 (3) (2007) 251–270.
[43] O. de Vel, Mining e-mail authorship, in: Proc. Workshop on Text Min-
ing, ACM International Conference on Knowledge Discovery and Data
Mining (KDD’2000), 2000.
[44] D. I. Holmes, The analysis of literary style–a review, Journal of the
Royal Statistical Society. Series A (General) (1985) 328–341.
[45] M. A. Covington, J. D. McFall, Cutting the gordian knot: The moving-
average type–token ratio (mattr), Journal of Quantatiative Linguistics
17 (2) (2010) 94–100.
[46] J. Ure, Lexical density and register differentiation, Applications of lin-
guistics (1971) 443–452.
[47] M. Idzelis, Jazzy: The java open source spell checker (2013).
URL http://jazzy.sourceforge.net
[48] M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reutmann, I. H. Wit-
ten, The weka data mining software: An update, SIGKDD Explorations
11 (1).
[49] N. Japkowicz, S. Stephen, The class imbalance problem: A systematic
study, Intelligent Data Analysis 6 (5) (2002) 429–450.
[50] Wikipedia, Precision and recall — wikipedia, the free encyclopedia,
[Online; accessed 10-February-2015] (2014).
URL http://en.wikipedia.org/w/index.php?title=Precision and recall&oldid=63585
[51] N. Cheng, X. Chen, R. Chandramouli, K. P. Subbalakshmi, Gender
identification from e-mails, in: IEEE Symposium on Computational In-
telligence and Data Mining (CIDM ’09), 2009.
33
