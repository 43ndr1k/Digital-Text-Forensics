An Open-Set Size-Adjusted Bayesian Classifier for
Authorship Attribution
G. Bruce Schaalje
Department of Statistics, 230 TMCB, Brigham Young University, Provo, UT, 84602. E-mail:schaalje@byu.edu
Natalie J. Blades
Department of Statistics, 230 TMCB, Brigham Young University, Provo, Utah, 84602.
E-mail: blades@stat.byu.edu
Tomohiko Funai
Department of Pediatrics, Williams Building, P.O. Box 581289, Salt Lake City, Utah 84158. E-mail:
tomohiko.funai@hsc.utah.edu
Recent studies of authorship attribution have used
machine-learning methods including regularized multi-
nomial logistic regression, neural nets, support vector
machines, and the nearest shrunken centroid classifier
to identify likely authors of disputed texts. These
methods are all limited by an inability to perform open-
set classification and account for text and corpus size.
We propose a customized Bayesian logit-normal-beta-
binomial classification model for supervised authorship
attribution. The model is based on the beta-binomial
distribution with an explicit inverse relationship between
extra-binomial variation and text size. The model inter-
nally estimates the relationship of extra-binomial varia-
tion to text size, and uses Markov Chain Monte Carlo
(MCMC) to produce distributions of posterior authorship
probabilities instead of point estimates. We illustrate the
method by training the machine-learning methods as
well as the open-set Bayesian classifier on undisputed
papers of The Federalist, and testing the method on
documents historically attributed to Alexander Hamil-
ton, John Jay, and James Madison. The Bayesian clas-
sifier was the best classifier of these texts.
Introduction
Supervised authorship attribution involves measuring fre-
quencies of a large number of words or n-grams on each of
several texts of undisputed (training texts) and also unknown
(test texts) authorship. A classification rule is developed or
“trained” using the training texts, and then applied to the test
texts in order to attribute authorship of each test text to
one—or possibly none—of the authors of the training texts.A
great deal has been written over the last 2 decades about the
use of various machine-learning methods, including regular-
ized multinomial logistic regression (RMLR), support vector
machines (SVM), neural nets (NN), and nearest shrunken
centroid classification (NSC), in supervised authorship attri-
bution problems (Forstall & Scheirer, 2010; Jockers, Witten,
& Criddle, 2008; Jockers & Witten, 2010; Koppel, Schler, &
Argamon, 2009; Schaalje, Fields, Roper, & Snow, 2011;
Schaalje & Fields, 2011; Stamatatos, 2009).
RMLR (Park & Hastie, 2007) is a statistically motivated
tool that treats word frequencies as predictor variables and
authorship as the categorical response variable. Authorship of
a text is viewed as a realization of a series of binomial random
variables whose expected values are logistically linked to
linear functions of relative word frequencies. Combining the
results of a series of binary regressions is justified under the
independence of irrelevant alternatives assumption (Paramesh,
1973). Authorship probabilities are modeled and directly
obtained from RMLR. Regularized (i.e., penalized) maximum
likelihood is usually used to train the classifier in high dimen-
sional problems. Regularization allows for estimation of
regression parameters even when separation (Heinze, 2006)
and dimensionality prohibit ordinary maximum likelihood
estimation. Both of these issues are common in authorship
attribution. The RMLR procedure automatically adjusts for
corpus size, and weighting can be used to adjust for varying
text sizes. Weighting due to intrinsic variability of the predic-
tors (relative word frequencies) is ignored, however. Also, the
extension of RMLR to open-set attribution is difficult in that
RMLR does not make distributional assumptions about the
predictor variables (word frequencies).
SVM, a very general classification methodology, constructs
a set of hyperplanes (or “support vectors”) that separate the
Received June 5, 2012; revised October 25, 2012; accepted October 27,
2012
© 2013 ASIS&T • Published online 28 June 2013 in Wiley Online Library
(wileyonlinelibrary.com). DOI: 10.1002/asi.22877
JOURNAL OF THE AMERICAN SOCIETY FOR INFORMATION SCIENCE AND TECHNOLOGY, 64(9):1815–1825, 2013
high-dimensional relative-word-frequency space into non-
overlapping partitions that in a geometric sense achieve
maximum separation of training texts associated with distinct
authors (Cortes & Vapnik, 1995). The basic SVM is a binary
classifier, but it is applied to multiauthor problems by employ-
ing multiple binary classifications. Strictly speaking, SVM
does not produce probabilistic output; however, distances of
training and test texts from support vectors can be converted to
authorship probabilities by assuming multivariate normality,
binning, or nonparametric isotonic regression (Platt, 2000;
Zadrozny & Elkan, 2002). Issues of text size and corpus size
are usually ignored when applying SVM to authorship attribu-
tion (Luyckx & Daelemans, 2011). The extension of SVM
from closed-set classification to open-set classification is a
difficult and unsolved problem that is just now receiving atten-
tion (Scheirer, Rocha, Sapkota, & Boult, 2011). Overfitting is
often a problem with SVM classification.
NN is also a general purpose learning algorithm. In
authorship attribution, an NN classifier is trained by sequen-
tially adjusting a series of binary classification rules
(perceptrons) (Freund & Schapire, 1999; Hoorn, Frank,
Kowalczyk, & van der Ham, 1999; Tweedie, Singh, &
Holmes, 1996), each based on the relative frequency of a
specific word. The procedure is iterated until maximal
correct authorship attribution for the training data are
attained according to some stopping rule. As with SVM,
authorship probabilities are obtained by calibration of text
distances from the classification boundary, and text size and
open-set issues are unresolved. Overfitting and bias (in favor
of the author with the largest number of training texts) are
often troubling issues with NN classification.
NSC (Tibshirani, Hastie, Narasimham, & Chu, 2002,
2003) is also a statistically motivated classifier. Unlike
RMLR, NSC views word frequencies as response variables
and authorship as the categorical predictor variable. NSC is a
shortcut Bayesian classifier originally designed for high-
dimensional genomics classification problems in which tens
of thousands of gene expression intensities are measured for
training and test individuals. Assuming homogeneous multi-
variate normality of sets of gene expression intensities, test
individuals are classified to the training class with the closest
centroid. Equivalently, test individuals are assigned to the
training class with the highest posterior probability of mem-
bership. An innovative feature of NSC is the use of a shrink-
age parameter (D) to shrink the estimated group centroids
toward the overall centroid, and thus reduce overfitting. The
shrinkage parameter also facilitates model selection in that if
all group centroid components for a particular gene shrink to
the overall centroid, that gene is eliminated from consider-
ation. Even though word frequencies do not follow homog-
enous multivariate normal distributions, NSC seems to
perform reasonably well in closed-set attribution problems
with training and test texts of large and approximately equal
size (Jockers & Witten, 2010). Because many, if not most,
attribution problems are “open problems” (Burrows, 2002,
2003; Juola, 2006; Stamatatos, 2009) in which it is not known
if the candidate author set includes the true author, Schaalje
et al. (2011) and Schaalje and Fields (2011) extended NSC to
open-set classification problems. They also extended the
NSC method to handle test texts of varying size. Even with
these extensions, however, problems remain with the NSC
method: the training texts must be approximately the same
size, one must rely on external data to generate the variance-
text size relationship for test texts, one must carry out
computer-intensive cross-validation calculations to choose
D, corpus size for each of the candidate authors is implicitly
assumed to be the same, and NSC provides no measures of
uncertainty for its authorship attribution probabilities. Also,
as mentioned previously, the authorship probabilities of NSC
depend on the false assumption of multivariate normality of
relative frequencies of specified words or n-grams.
The purpose of this paper is to propose and apply a
Bayesian authorship attribution model specifically designed
for open-set authorship attribution based on word-frequency
data. As such, it deals naturally with the vexing issues of size
(text size, corpus size, etc.) as well as appropriate weight-
ings of specific word frequencies (Koppel et al., 2009, 2011;
Luyckx & Daelemans, 2011; Maciej, 2010). The proposed
procedure is computationally intensive, but not much more
so than RMLR, SVM, NN, or NSC (when combined with
cross-validation to choose various tuning constants associ-
ated with the methods). Among other things, the procedure
provides additional information about authorship attribu-
tion, including posterior distributions of authorship prob-
abilities that indicate uncertainty associated with
attributions. Details of applying the Bayesian procedure are
somewhat technical, but the current availability of general
software for Bayesian calculations makes the procedure
accessible to authorship attribution practitioners.
We illustrate the new method by applying it to the well-
known Federalist papers problem (Mosteller & Wallace,
1964). As training texts we use Federalist papers of undis-
puted authorship. As test texts we use the 12 Federalist
papers of disputed authorship and the three papers of joint
authorship, papers for which any acceptable attribution
method should work well. As additional test texts, we use
several non-Federalist writings historically attributed to
Hamilton, Madison, and Jay in order to expose the classifiers
to difficult problems that are likely to reveal differences
among the methods. These latter texts, which have not pre-
viously been used in authorship attribution work, were also
chosen to illustrate the use of the Bayesian method for texts
of greatly varying sizes.
We assess the Bayesian method by goodness-of-fit tests
for training texts, classification rates for the training texts
themselves, and by comparison of authorship attributions of
test texts by RMLR, SVM, NN, and NSC. Open-set classi-
fication is used mainly to illustrate the procedure. However,
open-set classification is appropriate for the historical non-
Federalist test texts for several reasons: The texts were
written in different time periods and often in different genres
than the Federalist papers, some of the texts were at some
point attributed to different authors, and the provenance of
some of the texts is not fully known.
1816 JOURNAL OF THE AMERICAN SOCIETY FOR INFORMATION SCIENCE AND TECHNOLOGY—September 2013
DOI: 10.1002/asi
Bayesian Authorship Attribution Model
Let i denote a marker word (i = 1, . . . , w), j denote a
candidate author (j = 1, . . . , m), and k denote a training text.
The observed frequencies of the words are denoted by yi,j,k
and the text sizes are denoted by nj,k. The probability of
occurrence of word i in a text written by candidate author j
is denoted as pi,j.
We propose a Bayesian hierarchical model for the train-
ing text data. It would be natural to model word frequencies
using the binomial distribution, but empirically the frequen-
cies exhibit extra-binomial variation (Schaalje et al., 2011).
Furthermore, the degree of extra-binomial variability is
greatest for small text sizes. Hence, at the lowest level of the
hierarchy, the word frequency yi,j,k is assumed to follow a
beta-binomial distribution (Johnson, Kemp, & Kotz, 2005)
with mean ni,j,k, variance ti,j,k, and sample-size dependent
extra-binomial variation parameter rj,k. That is,
y BETABIN v nijk i j k i j k j k j k∼ , , , , , ,, , |τ ρ( ) (1)
where
ν π
τ π π ρ
ρ
i j k j k i j
i j k j k i j i j j k j k
j k
n
n n
, , , ,
, , , , , , ,
,
=
= −( ) + −( )[ ]1 1 1
= + −( )a n aj kb, .1
(2)
Noninformative prior distributions for a and b are specified
as
a UNIF
b UNIF
∼
∼
1 10
0 3
,
,
( )
( ) (3)
where UNIF denotes the uniform distribution. The beta-
binomial distribution is traditionally parameterized by
two parameters, a and b, and thus Equation (1) would be
traditionally specified as
y BETABIN nijk i j k i j k j k∼ α β, , , , ,, |( ) (4)
where, in this case,
α ν τ τ ν ν νi j k j k i j k i j k j k i j k i j k i j k i jn n, , , , , , , , , , , , , , ,= −( ) − −( ) +1 ,
, , , , , , , , , ,
, , , ,
k
i j k j k i j k j k i j k i j k
j k i j k i j
n n
n
[ ]
= −( ) −( )β ν τ ν
τ ν , , , , , .k i j k i j k− −( ) +[ ]ν ν1
(5)
At the next level of the hierarchy, the word occurrence
probability pi,j from Equation (2) is linked to the mean for
word i plus a deviation from the mean for author j and word
i using the logit function:
π μ δi j i i j, ,exp= + − +( )[ ]{ }1 1 (6)
with prior distributions
μ
δ ψ
i
i j
N
N
∼
∼
0 10000000
0
, ,
,,
( )
( )
and
(7)
and noninformative hyper-prior
ψ ∼ IG 3 0 001, .( ) (8)
where N denotes the normal distribution and IG denotes the
inverse gamma distribution.
The model can be described as a logit-normal-beta-
binomial model. Because of the model for rj,k (Equation
[2]), the beta-binomial distribution of the word frequencies
is almost identical to the binomial distribution when nj,k is
large. The parameters a and b are used to model the inverse
relationship between extra-binomial variation and text size.
The beta-binomial distribution for the various word frequen-
cies ensures that text sizes influence estimation of the
parameters both through binomial variation and the extra-
binomial variation component of the beta-binomial distribu-
tion. The normally distributed word effects and author
effects are related to the word probabilities in a specific text
via the logit link (McCullagh & Nelder, 1989), and the
parameter y turns out to be a shrinkage parameter.
Markov Chain Monte Carlo (MCMC) methods (Gilks,
Richardson, & Spiegelhalter, 1996) are used to fit the model
to the data. MCMC is computer-intensive, but not much
more so than machine-learning methods combined with the
cross-validation necessary to choose tuning constants. Baye-
sian estimates of the parameters can be taken as medians of
the posterior distributions of the parameters, but more
importantly posterior distributions of the parameters can be
used to characterize posterior distributions of functions of
the parameters. The posterior probability of authorship of a
new text is such a function of the model parameters.
General software such as WinBugs or the MCMC procedure
of SAS 9.2.3 (SAS Institute, 2010) can be used to carry out the
MCMC calculations. No cross-validation need be carried out to
estimate the shrinkage parameter because it is estimated as part
of the model-fitting process. Similarly, adjustments for training
text size are an inherent part of the model.
Using the Bayes formula, the posterior probability of
authorship of test text k* by author j0 is calculated as
ω
θ α β
θ φ
j k
j i k k i j k i j k
i
w
i k
i
w
f y n
0
0 0 0
1
0
1
, *
, * * , , * , , *
, *
, , ,
=
( )
+
=
=
∏
∏ θ α βj
j
m
i k k i j k i j k
i
w
f y n
= =
∑ ∏ ( )
1 1
, * * , , * , , *, , ,
(9)
where yi,k* is the observed frequency of word i in test text k*,
nk* is the test text size, the qj are prior probabilities of
authorship for each author, and f(.) is the probability mass
function for the beta-binomial distribution. q0 is the prior
probability of authorship by an author not included in the
candidate set. If closed set attribution is appropriate, q0 = 0
and the fi,k* need not be calculated. If open set classification
is appropriate,
φ α βi k j i k k i j k i j k i if y n g g, * , * * , , * , , * , ,max min , , , , min ,= ( )[ ] [ ]1 2{ }
(10)
JOURNAL OF THE AMERICAN SOCIETY FOR INFORMATION SCIENCE AND TECHNOLOGY—September 2013 1817
DOI: 10.1002/asi
where
g f y ni i k k i i, , * * , ,, , , ,1 = ( )+ +α β with
υi i k ky n= +( )[ ]logit , * *.5
ρ* *= + −[ ]a n akb 1
γ ρ υ ρ υi k i k k i kn n n n= + −( )⎡⎣ ⎤⎦ + + −( )⎡⎣ ⎤⎦1 1 1 1* * * ** *
π υ γi i ic, exp+ = + − +( )[ ]{ }1 1
ν πi k in, * ,+ +=
τ ν π ρi i i kn, , , * *+ + += −( ) + −( )⎡⎣ ⎤⎦1 1 1
α ν τ τ ν ν νi k i i k i i i in n, * , , * , , , ,+ + + + + + += −( ) − −( ) +[ ]1
β ν τ ν
τ ν ν ν
i k i k i i
k i i i i
n n
n
, * , * , ,
* , , , ,
+ + + +
+ + + +
= −( ) −( )
− −( ) +[ ]1
and
g f y ni i k k i i i i, , * * , ,, , , , , *, ,2 = ( )− −α β υ ρ γwith and as above and
π υ γi i ic, exp− = + − −( )[ ]{ }1 1
ν πi k in, * ,− −=
τ ν π ρi i i kn, , , * *− − −= −( ) + −( )⎡⎣ ⎤⎦1 1 1
α ν τ τ ν ν νi k i i k i i i in n, * , , * , , , ,− − − − − + += −( ) − −( ) +[ ]1
β ν τ ν
τ ν ν ν
i k i k i i
k i i i i
n n
n
, * , * , ,
* , , , , .
− − − −
− − − −
= −( ) −( )
− −( ) +[ ]1
In the expressions for g1 and g2, c is a tuning constant for the
desired power of the procedure for attributing authorship to
someone outside of the explicit candidate set. Normally c
should be greater than or equal to 2, with c = 2 giving great-
est sensitivity but least specificity.
Using the posterior distributions of the parameters for the
training data, posterior distributions of these posterior
authorship probabilities are obtained. Authorship attribution
is based on the maximum of the means or specified quantiles
(10th, 25th, or 50th) of the posterior distributions of author-
ship probabilities, wj,k*, across candidate authors.
Texts and Words
To illustrate and compare the Bayesian and other classi-
fiers, we used the 70 undisputed Federalist papers, written
in 1787–1788 by Alexander Hamilton, James Madison, and
John Jay, as training texts. As test texts we used the 12
disputed Federalist papers, the three joint Hamilton–
Madison Federalist papers, 15 letters or articles historically
attributed to Hamilton (1904), 15 letters or articles histori-
cally attributed to Madison (1900), and 10 letters or articles
historically attributed to Jay (2010). The texts were chosen
in order to apply the classifiers to difficult attribution prob-
lems involving texts of greatly varying sizes.
Using a PERL script, we computed frequencies of 70
noncontextual function words which occurred with a relative
frequency of at least 0.005 in the undisputed corpus of
The Federalist:
a against all always among an and any are as at because been
between both but by can could even every for from had has have
if in into it its little may might more must no not of on only or out
over should so some such than that the there therefore these
this those though to upon very well what when where whether
which who with without would
We used these noncontextual words as features in order to
attribute authorship of documents that differed from the
training texts in genre and topic. The use of these noncon-
textual words avoids attribution solely on the basis of arti-
facts inherent in topics or sentiments (Koppel et al., 2009);
instead, authorship is attributed based on unconscious
writing habits of the authors. In other linguistic applications,
the use of context-rich words or n-grams as features might
be appropriate.
Methods
Preliminary Analysis
As an aid in visualizing relationships among the training
and test texts, we produced a principal components plot of
the training texts—the undisputed Federalist papers. To
visually simulate the process of classifying the test texts
using the training texts, we then applied the coefficients
derived from this analysis to compute principal component
scores for all of the test texts. The principal components
procedure does not take training or test text size into account
and thus is a crude visual counterpart of at least the Bayesian
authorship attribution procedure.
Classification by SVM, NN, RMLR, and NCS
We used the plr() function of the stepplr package (Park &
Hastie, 2007) of R to apply RMLR. Multinomial logistic
regression requires combining a series of binary logistic
regressions, each with a common reference level. The choice
of reference level has little effect on the results for ordinary
multinomial logistic regression. However, RMLR results are
affected by choice of reference level. We chose the reference
level (Madison) that resulted in the highest correct classifi-
cation rate for the training texts. Cross-validation for each
binary regression was used to select the regularization
constant l. Since the selected constants were different
but similar for the two regressions, we used the average
(l = 1e-6).
1818 JOURNAL OF THE AMERICAN SOCIETY FOR INFORMATION SCIENCE AND TECHNOLOGY—September 2013
DOI: 10.1002/asi
SVM was applied to the authorship data using the svm()
function of the e1071 package (Dimitriadou, Hornik,
Leisch, Meyer, & Weingessel, 2005) of R. A Gaussian radial
kernel was used with hyperparameters for precision and cost
of misclassification of training points. A grid search deter-
mined that the precision and cost hyperparameters for the
best model are g = 0.01 and C = 1.
NN was applied via the nnet() function of the nnet
package (Venables & Ripley, 2002) of R. A neural net with
a single hidden layer with 13 nodes and a weight decay
parameter of .001 was selected using a grid search. Given
the large number of observed covariates and hidden nodes,
convergence was somewhat slow (1,800 steps were required
before the process converged).
We calculated open-set NSC authorship probabilities in
two ways: assuming that all test texts were the same size and
adjusting for test text size. In these calculations, we used
cross-validation to set D to 1.5, and we set the tuning con-
stant l (Schaalje & Fields, 2011) to 5. We evaluated the fit of
the NSC model to the training data by multiplying expected
relative frequencies of specific words in the training texts by
their sizes to obtain expected frequencies, and then com-
pared the distribution of these expected relative frequencies
to the distribution of observed frequencies. The code for
NSC was written using SAS/IML (SAS Institute, 2010).
Classification by the Bayesian Model
We fit the Bayesian classification model to the training
data using the MCMC procedure of SAS 9.2.3 (SAS Institute,
2010). We used 1,000 burn-in iterations, and then sampled the
joint posterior distribution of the model parameters 50,000
times. For each test text, we also sampled the posterior author-
ship probability (Equation [10]) with all prior authorship
probabilities (qj’s) including the probability of an unknown
author set to 1/4. We set c = 2 in all of these calculations. We
also generated 100 posterior predictions for each of the
observed values and compared the distribution of the pre-
dicted values to those of the observed values as a goodness-
of-fit test (Gelman, Carlin, Stern, & Rubin, 2004). These
calculations were done using SAS/IML (SAS Institute, 2010).
Results
The Bayesian model fit the training data well (Figure 1).
The expected word count frequencies were calculated using
posterior predictions (word counts greater than or equal to
50 were aggregated). These were compared to the observed
word count frequencies, and the agreement was strong. For
example, the expected frequency of 0’s was 467.07 and the
observed frequency was 470. The chi-square statistic was
56.64 for the 51 sets of counts, indicating excellent fit of the
model to the data.
The NSC model, the only other procedure that specifi-
cally models frequencies of marker words, fit the training
data less well. The distribution of expected frequencies of
marker words in the texts did not match the observed fre-
quencies very well (Figure 2). There were more expected
frequencies of 0 or 1 than observed, and at the other extreme,
fewer expected frequencies greater than or equal to 50 than
observed. The chi-square statistic was 154.37 for the 51 sets
of counts, indicating poor fit of the model to the data. At
FIG. 1. Expected and observed frequencies of word counts for the Bayesian classification model fitted to the training data. The response variable for the
model was the count of specific words in the texts. The last bars represent expected and observed frequencies of counts greater than or equal to 50.
JOURNAL OF THE AMERICAN SOCIETY FOR INFORMATION SCIENCE AND TECHNOLOGY—September 2013 1819
DOI: 10.1002/asi
least some of the lack-of-fit is due to not accounting for text
size in the model, and some is due to that fact that sample
proportions near zero follow right skewed distributions as
opposed to normal distributions assumed by the model.
Lack-of-fit due to the normality assumption would not
greatly affect some inferences associated with this model,
but the posterior authorship probability calculations depend
heavily on the assumption of normality.
Other indications of better fit of the Bayesian model than
the NSC model are found in the use of the models to classify
the training texts (Table 1). The Bayesian procedure classi-
fied the texts perfectly, while the unadjusted NSC method
misclassified one of the Madison texts. The size-adjusted
NSC method misclassified two texts, one Hamilton text and
one Madison text. The closed set classifiers, SVM, NN, and
RMLS, perfectly classified the training texts.
FIG. 2. Expected and observed frequencies of word counts for the NSC classification model fitted to the training data. The response variable for the NSC
model, relative frequency of specific words in the texts, was multiplied by text sizes to obtain the expected counts. The last bars represent expected and
observed frequencies of counts greater than or equal to 50.
TABLE 1. Training text classifications.
Assigned author
Method Actual author Hamilton Jay Madison Other
RMLR Hamilton 51 0 0 –
Jay 0 5 0 –
Madison 0 0 14 –
SVM Hamilton 51 0 0 –
Jay 0 5 0 –
Madison 0 0 14 –
NN Hamilton 51 0 0 –
Jay 0 5 0 –
Madison 0 0 14 –
NSC unadjusted Hamilton 51 0 0 0
Jay 0 5 0 0
Madison 1 0 13 0
NSC adjusted Hamilton 50 0 0 1
Jay 0 5 0 0
Madison 1 0 13 0
Bayesian Classifier Hamilto 51 0 0 0
Jay 0 5 0 0
Madison 0 0 14 0
1820 JOURNAL OF THE AMERICAN SOCIETY FOR INFORMATION SCIENCE AND TECHNOLOGY—September 2013
DOI: 10.1002/asi
The principal components plots (Figure 3) demonstrate
that while the training texts are quite distinct, the test texts
are, as expected, more scattered. The disputed and joint
Federalist papers are well within the Madison cloud, but the
non-Federalist texts only roughly conform to purported
authorship as characterized by styles of the Federalist
papers. Much of the scatter in these plots is due to the size of
the test texts. For example, the letters associated with Jay are
all small, ranging in size from 400 to 1,100 words. However,
the scatter is due to other things as well. The Hamilton text
which is furthest from the cloud for the Hamilton Federalist
papers is the longest text of all, having more than 32,000
words. Madison’s non-Federalist papers and letters appear
to scatter well into the Hamilton and even Jay clouds.
Authorship attributions for the Bayesian classifier could
have been based on any summary statistic of the posterior
authorship probability distributions. We used the 10th, 25th,
and 50th percentiles of the distributions in classifying the
test texts, and found that authorship attributions were the
same using any of these percentiles as the classification
criterion. Hence, in the remainder of this paper we report the
results of Bayesian classifications based on the 50th percen-
tile (BC50). It is worth noting, however, that some posterior
distributions were more compactly distributed around the
median attribution probability than others, and the spread of
the distributions was not solely a function of text size.
Figure 4A displays the authorship probabilities for the 12
disputed papers and three joint papers. For each paper, the
probability that each method assigned the paper to Madison
is displayed in gray. The Bayesian classifier is the only
method to attribute all disputed and joint papers to Madison,
a result that agrees with many previous analyses and with
Madison’s own account (Mosteller & Wallace, 1964;
Holmes & Forsyth, 1995; Jockers & Witten, 2010). All six
methods assign the correct author to the first 8 of 12 disputed
papers arrayed clockwise around Figure 4A. SVM and NSC
misclassify at least one of the next three disputed papers.
Disputed Paper 55, the text assigned to Hamilton by RMLR,
NN, and NSC, was assigned to Madison by the Bayesian
classifier and by SVM, but the median of the posterior prob-
ability distribution for Madison’s authorship probabilities
was relatively low (.708) compared to those for the other
disputed papers and the SVM attribution probability for
Madison was only slightly higher than that for Hamilton.
Together with the Bayesian method, both RMLR and NN
attributed all three of the joint papers to Madison with high
probability. The NSC methods assigned two of the three
joint papers to Madison, but assigned Paper 18 to a
FIG. 3. Principal components plots of the undisputed Federalist data based on 70 noncontextual words. The coefficients of the principal components were
then applied to the disputed and joint Federalist papers, the historical Hamilton papers, the historical Jay papers, and the historical Madison papers.
JOURNAL OF THE AMERICAN SOCIETY FOR INFORMATION SCIENCE AND TECHNOLOGY—September 2013 1821
DOI: 10.1002/asi
non-included author (the probability of assignment to a non-
included author is indicated in black in Figure 4A).
Although it is not clear from Figure 1 why this would
be so, the 3rd principal component score for Paper 18 is
different from scores for the papers assigned to Madison.
SVM assigned Paper 18 to Jay, but with moderately low
probability.
Even though the principal components in Figure 1
seemed to indicate that the 15 historical Hamilton texts
would be fairly easy to classify correctly, they were not. The
Bayesian classifier assigned seven papers to Hamilton, but it
assigned “Farmer” and “Good Countrymen,” two large texts
written by Hamilton at the age of 17 (Hamilton, 1904) to a
non-included author (Figure 4B). These two texts are the
earliest samples of his writing, and his style may have
changed as he matured and gained education. The
unadjusted and size-adjusted NSC methods assigned eight
and nine papers, respectively, to Hamilton while RMLR and
S
V
M
N
N
R
M
LR
N
S
C
N
S
C
a
B
C
50
S
V
M
N
N
R
M
LR
N
S
C
N
S
C
a
BC
50
SV
M
NN
RM
LR
NS
C
NS
Ca
BC
50
SV
M
NN
RM
LR
NSC
NSC
a
BC50
SVM
NN
RMLR
NSC
NSCa
BC50
SVM
NN
RMLRNSCNSCaBC50
SVMNNRMLR
NSCNSCa
BC50SVM
N
NRM
LR
N
S
C
N
S
C
a
B
C
50
S
V
M
N
N
R
M
LR
N
S
C
N
S
C
a
B
C
50S
V
MN
N
R
M
LRN
S
C
N
S
C
a
B
C
50S
VM
N
N
RM
LRNS
C
NS
CaBC
50
SV
MN
NRM
LRN
SCN
SC
aBC
50
SVM
NN
RMLR
NSC
NSCa
BC50
SVM
NN
RMLR
NSC
NSCa
BC50
SVM
NN
RMLR
NSC
NSCa
BC50
50%
90%
Disputed 58 Disputed 57
Disputed 49
D
isputed 63
D
isputed 51
D
isputed 52
Disputed 53
Disputed 54
Disputed 62Dis
pute
d 56
Di
sp
ute
d 5
0
D
is
pu
te
d 
55
Jo
in
t 1
9
Jo
in
t 2
0
Jo
in
t 1
8
2080
2206
1646
3040
1914
1844
2216
1996
23871
565
11
01
22
04
20
21
15
18
20
87
S
V
M
N
N
R
M
LR
N
S
C
N
S
C
a
B
C
50
S
V
M
N
N
R
M
LR
N
S
C
N
S
C
a
BC
50
SV
M
NN
RM
LR
NS
C
NS
Ca
BC
50
SV
M
NN
RM
LR
NSC
NSC
a
BC50
SVM
NN
RMLR
NSC
NSCa
BC50
SVM
NN
RMLRNSCNSCaBC50
SVMNNRMLR
NSCNSCa
BC50SVM
N
NRM
LR
N
S
C
N
S
C
a
B
C
50
S
V
M
N
N
R
M
LR
N
S
C
N
S
C
a
B
C
50S
V
MN
N
R
M
LRN
S
C
N
S
C
a
B
C
50S
VM
N
N
RM
LRNS
C
NS
CaBC
50
SV
MN
NRM
LRN
SCN
SC
aBC
50
SVM
NN
RMLR
NSC
NSCa
BC50
SVM
NN
RMLR
NSC
NSCa
BC50
SVM
NN
RMLR
NSC
NSCa
BC50
50%
90%
Continentalist 3
Publius 3
Vindication
P
ublius 1
C
onstitution
C
ontinentalist 6
Continentalist 1
Continentalist 5Good Countrymen
Publ
ius 2
Co
nti
ne
nta
lis
t 2
C
on
tin
en
ta
lis
t 4
Fa
rm
er
Q
ue
be
c 
2
Q
ue
be
c 
1
1717
992
1212
395
7064
2614
1189
2632
5622
1129
13
85
16
38
31
93
6
26
72
14
54
S
V
M
N
N
R
M
LR
N
S
C
N
S
C
a
B
C
50
S
V
M
N
N
RM
LR
NS
C
NS
Ca
BC
50
SV
M
NN
RML
R
NSC
NSCa
BC50
SVM
NN
RMLR
NSC
NSCaBC50
SVMNNRM
LRNSCNSC
a
BC
50SV
M
N
NRM
LR
N
S
C
N
S
C
a
B
C
50S
V
MN
N
R
M
LRN
S
C
N
S
C
a
BC
50
SV
M
NNR
ML
RN
SC
NS
Ca
BC5
0
SVM
NN
RMLR
NSC
NSCa
BC50
SVM
NN
RMLR
NSC
NSCa
BC50
50%
90%
War 2
Goodloe
M
orris 2
W
ar 1
Lovell
Adams
Mor
ris 1
Je
ffe
rs
on
H
ob
ar
t
W
as
hi
ng
to
n
4347
441
831
462
476
627
535
38
1
32
0
63
2
S
V
M
N
N
R
M
LR
N
S
C
N
S
C
a
B
C
50
S
V
M
N
N
R
M
LR
N
S
C
N
S
C
a
BC
50
SV
M
NN
RM
LR
NS
C
NS
Ca
BC
50
SV
M
NN
RM
LR
NSC
NSC
a
BC50
SVM
NN
RMLR
NSC
NSCa
BC50
SVM
NN
RMLRNSCNSCaBC50
SVMNNRMLR
NSCNSCa
BC50SVM
N
NRM
LR
N
S
C
N
S
C
a
B
C
50
S
V
M
N
N
R
M
LR
N
S
C
N
S
C
a
B
C
50S
V
MN
N
R
M
LRN
S
C
N
S
C
a
B
C
50S
VM
N
N
RM
LRNS
C
NS
CaBC
50
SV
MN
NRM
LRN
SCN
SC
aBC
50
SVM
NN
RMLR
NSC
NSCa
BC50
SVM
NN
RMLR
NSC
NSCa
BC50
SVM
NN
RMLR
NSC
NSCa
BC50
50%
90%
Reflections Peace Treaty
LaFayette
E
m
igration
nil
kn
ar
F
R
eligious
Jefferson
Washington
InauguralBill 
of R
ights
Pa
rtie
s
Ti
tle
s
B
ra
df
or
d 
2
B
ra
df
or
d 
3
Vi
rg
in
ia
2925
817
1956
2498
1132
2710
1366
953
1178
979
88
7
67
8
89
7
91
2
15
45
A: 12 Disputed and 3 Joint Federalist Papers B: Alexander Hamilton
C: John Jay D: James Madison
FIG. 4. Authorship probabilities assigned by five authorship attribution methods for (A) 12 disputed and three joint Federalist papers, (B) 15 letters and
papers historically attributed to Alexander Hamilton, (C) 10 letters and papers historically attributed to John Jay, and (D) 15 letters and papers historically
attributed to James Madison. The probabilities assigned to the correct author, incorrect authors, and an author not considered are displayed as gray, white
and black, respectively. Text size is indicated for each document.
1822 JOURNAL OF THE AMERICAN SOCIETY FOR INFORMATION SCIENCE AND TECHNOLOGY—September 2013
DOI: 10.1002/asi
NN assigned only five and seven papers correctly. SVM
performed best; 11 of Hamilton’s texts were correctly clas-
sified. The six methods were in complete agreement on only
three of the papers.
The Bayesian classifier and SVM both correctly classi-
fied all 10 of the Jay texts even though all but one are short
letters (320 to 831 words) (Figure 4C). RMLR and NN
classified the same eight texts correctly to Jay while the two
NSC methods attributed only five and seven of the Jay texts
to Jay. Interestingly, no classifier attributed a Jay text to
Madison even though the principal components plot placed
one Jay text in the middle of the Madison cluster. Higher
components show, however, that this text is distinct from the
Madison writings. The posterior authorship probabilities of
the Bayesian classifier reflected both the size of the texts and
the position relative to the Jay centroid. For example, the
Hobart letter was attributed by the Bayesian classifier to Jay
with a median posterior probability of .624. A 95% interval
for the posterior probability of authorship by Jay was (.328,
.874) and Figure 5 is a density plot of the posterior attribu-
tion probabilities for the Hobart letter to Jay. In contrast,
NSC attributed the Hobart letter to a non-included author
with posterior probability .999. Authorship attributions were
in complete agreement among methods on 5 of the 10 Jay
texts.
The principal components plot suggested that the his-
torical Madison texts would be hard to classify, and indeed
they were. The Bayesian classifier assigned only eight
texts to Madison while RMLR was the most accurate, cor-
rectly assigning 10 texts to Madison (Figure 4D). NN
assigned nine texts to Madison, and SVM only assigned
three texts to Madison. The unadjusted and size-adjusted
NSC methods correctly assigned five and seven Madison
texts, respectively. The size-adjusted NSC classifier
attributed two texts to a non-included author while the
Bayesian classifier did not attribute any texts to a non-
included author. Perfect agreement among classification
methods was achieved on only 3 of the 15 Madison texts.
Furthermore, no pair of methods was in complete agree-
ment for all 15 texts.
Overall, the Bayesian classifier had the highest correct
attribution classification rate: 25 (62.5%) of the historical
texts were correctly classified. RMLR correctly attributed 23
(57.5%) of the historical texts, SVM correctly attributed 24
(60%), NN correctly attributed 24(60%), nonadjusted NSC
correctly attributed 18 (45%), and size-adjusted NSC cor-
rectly attributed 23 (57.5%). Complete agreement among all
classification methods for the historical texts was 11/40
(27.5%). Agreement of each of the methods with the
Bayesian classifier was 62.5%, 62.5%, 75%, 42.5%, and
55% for RMLR, SVM, NN, unadjusted NSC, and adjusted
NSC, respectively.
Clustering of the attribution methods was instructive.
Based on average disagreement in test text attributions
(Figure 6), NN and RMLR formed a tight cluster, NSC and
the Bayesian method formed a second, less tight cluster, and
SVM formed a separate cluster. This makes sense because
NN and RMLR are both regression methods that view
authorship as a response to be optimally predicted by
explanatory variables. NSC and the Bayesian method are
multivariate methods that model joint distributions of the
features for each author, and then use Bayes rule to classify
new individuals based on the most likely distribution. NSC
and the Bayes method of this paper differ substantially,
however, in the nature of the distributional models for
marker word frequencies. SVM is unique among the classi-
fication tools in that it is a nonstatistical method, based
solely on the high dimensional geometry of the data.
FIG. 5. Kernel density estimate of the posterior distribution of posterior
probability of authorship of the Hobart letter by John Jay. Part of the reason
for uncertainty is that this letter was only 320 words long. Such density
plots could be constructed for all of the Bayesian authorship attributions.
FIG. 6. Dendrogram for cluster analysis of five authorship attribution
methods based on attributions of letters and papers historically attributed to
Alexander Hamilton, John Jay, and James Madison. Papers that were
attributed to a non-included author by NSC or the Bayesian classifier were
excluded from this analysis.
JOURNAL OF THE AMERICAN SOCIETY FOR INFORMATION SCIENCE AND TECHNOLOGY—September 2013 1823
DOI: 10.1002/asi
Discussion and Conclusions
The Bayesian model fits the training data well—
definitely better than the NSC model, the only other model
for which goodness-of-fit to the word frequencies could be
internally assessed. There was considerable attribution dis-
agreement among the methods as they struggled with issues
of size and genre. The Bayesian method was slightly more
accurate than any of the other attribution methods for the test
texts, and it clustered more closely with the other multivari-
ate method—the NSC method—than any other method. All
of this suggests that the Bayesian method is working as it
was designed to work, and one would therefore expect
authorship attributions based on the Bayesian model to be
more reliable.
The Bayesian classifier provides at least three additional
advantages over other methods. First, it internally adjusts for
text size, and the adjustment appears to work well for texts
as small as about 300 words. Thus, training texts as well as
test texts can be of varying sizes, and have no need to be
composited into artificial larger texts. Second, the Bayesian
classifier calculates posterior distributions of attribution
probabilities, thus providing secondary information on the
uncertainty associated with authorship attributions. The
method is therefore more informative about the strength of
evidence for authorship of a specific text by a specific
author. Third, because the method is a statistical method
based on a model for the joint distribution of marker-word
frequencies, its extension to open-set classification problems
is relatively straightforward (Real & Baumann, 2000;
Schaalje & Fields, 2011).
The RMLR, NN, SVM, and NSC classifiers are general
methods, not customized to the specific characteristics of
authorship attribution based on word frequencies. Hence,
there is a danger that in specific cases these classifiers can be
misleading. The strength of the Bayesian classifier is empiri-
cally clear from a higher rate of classifying the disputed and
joint texts to Madison, a higher correct classification rate of
the historical (non-Federalist) texts, and a higher correct
classification rate of the training texts. There is little, if any,
downside to using the more realistic, powerful, and infor-
mative Bayesian classifier in preference to the other
methods. While the correct classification rate of 62.5% for
the historical texts used in this paper is not overly impres-
sive, it is better than that of the other classifiers. This analy-
sis focused on a set of historical texts, and the method should
now be applied to other types of texts. In specific applica-
tions, classification accuracy can be improved by the use of
a larger number of marker words and the use of a carefully
selected set of training texts.
Current work in authorship attribution is aimed at
improving methodology by leveraging advances in natural
language processing, including automated detection of
complex syntactic and lexical features (Koppel et al., 2009).
This paper demonstrates that advances in customized statis-
tical modeling and Bayesian technology can also contribute
to improvements in authorship attribution. Given this
approach, analogous gains could be pursued by extending
the Bayesian model to n-grams and other semantic features,
incorporating the Bayesian classifier in a multiple-classifier
technique (Rokach, 2010), and developing unsupervised
Bayesian clustering methods to explore internal authorship
structure within series of texts.
Acknowledgments
The Neal A. Maxwell Institute for Religious Scholarship,
the Department of Statistics, and an Office of Research and
Creative Activities (ORCA) mentoring grant from Graduate
Studies, all at Brigham Young University, provided funding
for this project.
References
Burrows, J.F. (2002). “Delta”: a measure of stylistic difference and a guide
to likely authorship. Literary and Linguistic Computing, 17(3), 267–287.
Burrows, J.F. (2003). Questions of authorship and beyond. Computers and
the Humanities, 37, 5–32.
Cortes, C., & Vapnik, V.N. (1995). Support-vector networks. Machine
Learning, 20, 273–297.
Dimitriadou, E., Hornik, K., Leisch, F., Meyer, D., & Weingessel, A.
(2005). e1071: Misc functions of the Department of Statistics. TU Wien,
Version 1.5-11. Available at: http://CRAN.R-project.org/
Forstall, C., & Scheirer, W. (2010). Features from frequency: authorship
and stylistic analysis using repetitive sound. Journal of the Chicago
Colloquium on Digital Humanities and Computer Science, 1(2). Avail-
able at: https://letterpress.uchicago.edu/index.php/jdhcs/issue/view/6
Freund, Y., & Schapire, R.E. (1999). Large margin classification using the
perceptron algorithm. Machine Learning, 37(3), 277–296.
Gelman, A., Carlin, J.B., Stern, H.S. & Rubin, D.B. (2004). Bayesian data
analysis (2nd ed.). New York: Chapman & Hall/CRC.
Gilks, W.R., Richardson, S., & Spiegelhalter, D.J. (1996). Markov Chain
Monte Carlo in practice. New York: Chapman & Hall/CRC.
Hamilton, A. (1904). The works of Alexander Hamilton, Vol. 1 (Federal
Edition), Henry Cabot Lodge ed. New York: G.P. Putnam’s Sons.
Available at: http://oll.libertyfund.org/index.php?option=com_staticxt&
staticfile=show.php%3Ftitle=1378&Itemid=28
Heinze, G. (2006). A comparative investigation of methods for logistic
regression with separated or nearly separated data. Statistics in Medicine,
25, 4216–4226.
Holmes, D.I., & Forsyth, R.S. (1995). The Federalist revisited: New direc-
tions in authorship attribution. Literary and Linguistic Computing, 10(2),
111–127.
Hoorn, J.F., Frank, S.L., Kowalczyk, W., & van der Ham, F. (1999). Neural
network identification of poets using letter sequences. Literary and
Linguistic Computing, 14(3), 311–338.
Jay, J. (2010). John Jay letters for the years 1776 thru 1827. Available at:
http://www.familytales.org/results.php?tla=joj
Jockers, M.L., & Witten, D.M. (2010). A comparative study of machine
learning methods for authorship attribution. Literary and Linguistic
Computing, 25(2), 215–223.
Jockers, M.L., Witten, D.M., & Criddle, C.S. (2008). Reassessing author-
ship of the Book of Mormon using delta and nearest shrunken centroid
classification. Literary and Linguistic Computing, 23(4), 465–491.
Johnson, N.L., Kemp, A.W., & Kotz, S. (2005). Univariate discrete distri-
butions (3rd ed.). New York: Wiley.
Juola, P. (2006). Authorship attribution. Foundations and Trends in Infor-
mation Retrieval, 1(3), 233–334.
Koppel, M., Schler, J., & Argamon, S. (2009). Computational methods in
authorship attribution. Journal of the American Society for Information
Science and Technology, 60(1), 9–26.
1824 JOURNAL OF THE AMERICAN SOCIETY FOR INFORMATION SCIENCE AND TECHNOLOGY—September 2013
DOI: 10.1002/asi
Koppel, M., Schler, J., & Argamon, S. (2011). Authorship attribution in the
wild. Language Resources and Evaluation, 45(1), 83–94.
Luyckx, K., & Daelemans, W. (2011). The effect of author set size and data
size on authorship attribution. Literary and Linguistic Computing, 26(1),
35–55.
Maciej, E. (2010). Does size matter? Authorship attribution, small samples,
big problem. London: Digital Humanities. Available at: http://dh2010.
cch.kcl.ac.uk/academic-programme/abstracts/papers/pdf/ab-744.pdf
Madison, J. (1900). The writings of James Madison, comprising his public
papers and his private correspondence, including his numerous letters
and documents now for the first time printed. G. Hunt (Ed.). New York:
G.P. Putnam’s Sons. Available at: http://oll.libertyfund.org/index.php?
option=com_staticxt&staticfile=show.php%3Ftitle=1932&layout=html
McCullagh, P., & Nelder, J.A. (1989). Generalized linear models (2nd ed.).
New York: Chapman & Hall/CRC.
Mosteller, F., & Wallace, D.L. (1964). Applied Bayesian and classical
inference: The case of the Federalist Papers. New York: Springer.
Paramesh R. (1973). Independence of irrelevant alternatives, Econometrica,
41(5), 987–991.
Park, M.Y., & Hastie, T. (2007). Penalized logistic regression for detecting
gene interactions. Biostatistics, 9(1), 30–50.
Platt, J.C. (2000). Probabilistic outputs for support vector machines and
comparisons to regularized likelihood methods. In P.J. Bartlett, B.
Schölkopf, D. Schuurmans, & A.J. Smola (Eds.), Advances in large
margin classifiers. Cambridge, MA: MIT Press. pp. 61–74.
Real, E.C., & Baumann, A.H. (2000). Open set classification using
tolerance intervals. Conference Record of the Thirty-Fourth Asilomar
Conference on Signals, Systems and Computers, 2:1217–1221.
Rokach, L. (2010). Ensemble-based classifiers. Artificial Intelligence
Review, 33(1-2), 1–39.
SAS Institute Inc. (2010). SAS/STAT 9.2.3 User’s Guide. Cary, NC: SAS
Institute Inc.
SAS Institute Inc. (2010). SAS/IML 9.2.3 User’s Guide. Cary, NC: SAS
Institute Inc.
Schaalje, G.B., & Fields, P.J. (2011). Open-set nearest shrunken centroid
classification. Communications in Statistics, 41(4), 638–652.
Schaalje, G.B., Fields, P.J., Roper, M. & Snow, G.L. (2011). Extended
nearest shrunken centroid classification: a new method for open-set
authorship attribution of texts of varying sizes. Literary and Linguistic
Computing, 26(1), 71–88.
Scheirer, W.J., Rocha, A., Sapkota, A., & Boult, T.E. (2011). Meta-
recognition: Tools for improving recognition systems. Open set.
Available at: http://www.metarecognition.com/openset-2/
Stamatatos, E. (2009). A survey of modern authorship attribution methods.
Journal of the American Society for Information Science and Technol-
ogy, 60(3), 538–556.
Tibshirani, R., Hastie, T., Narasimham, B., & Chu, G. (2002). Diagnosis
of multiple cancer types by shrunken centroids of gene expression.
Proceedings of the National Academy of Science, 99(10), 6567–
6572.
Tibshirani, R., Hastie, T., Narasimham, B., & Chu, G. (2003). Class
prediction by nearest shrunken centroids, with application to DNA
microarrays. Statistical Science, 18(1), 104–117.
Tweedie, F.J., Singh, S., & Holmes, D.I. (1996). Neural network applica-
tions in stylometry: The Federalist Papers. Computers and the Humani-
ties, 30, 1–10.
Venables, W.N., & Ripley, B.D. (2002). Modern applied statistics with S
(4th ed). New York: Springer.
Zadrozny, B., & Elkan, C. (2002). Transforming classifier scores into
accurate multiclass probability estimates. In Proceedings of the
International Conference on Knowledge Discovery and Data Mining,
Edmonton, Canada, 2002. pp. 694–699.
JOURNAL OF THE AMERICAN SOCIETY FOR INFORMATION SCIENCE AND TECHNOLOGY—September 2013 1825
DOI: 10.1002/asi
