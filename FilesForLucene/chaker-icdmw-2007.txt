Genre categorization of web pages 
 
 
Jebari Chaker1 and Ounelli Habib2 
1King Saud University, College of Computer and Information Sciences, Computer Science 
Department,  P. O. Box 51178, Riyadh 11543, Kingdom of Saudi Arabia  
Jebarichaker@yahoo.fr 
2Université de Tunis El’Manar, Faculté des Sciences de Tunis, Département d’Informatique, 
Campus Universitaire 1060 Tunis, Tunisia 
habib.ounelli@fst.rnu.tn 
 
 
Abstract 
 
With the increase of the number of web pages, it is 
very difficult to find wanted information easily and 
quickly out of thousands of web pages retrieved by a 
search engine. To solve this problem, many researches 
propose to classify documents according to their 
genre, which is another criteria to classify documents 
different from the topic. Most of these works assign a 
document to only one genre. In this paper we propose 
a new flexible approach for document genre 
categorization. Flexibility means that our approach 
assigns a document to all predefined genres with 
different weights. The proposed approach is based on 
the combination of two homogenous classifiers: 
contextual and structural classifiers. The contextual 
classifier uses the URL, while the structural classifier 
uses the document structure. Both contextual and 
structural classifiers are centroid-based classifiers. 
Experimentations provide a micro-averaged break-
even point (BEP) more than 85%, which is better than 
those obtained by other categorization approaches. 
 
1. Introduction 
 
In front of the explosive growth of the number of web 
pages, users cannot quickly find desired information 
among the huge list of web pages returned by a search 
engine. To deal with this problem, many categorization 
approaches have been proposed to classify the result of 
search engines. Most of them have been interested by 
topic categorization [32]. Even, if the documents are 
classified successfully by their topics, they stayed 
heterogeneous. For example, the documents grouped 
by the topic “cinema” can be an actor homepage, a 
newspaper about a film or an actor, a collection of 
films posters and so on. So, a user looking for 
newspapers about an actor should consult all other 
document types or genres. Therefore, the document 
genre is another view for document categorization 
different from the topic. Recently, many works have 
been focused on genre categorization (for more 
references you can see [30]).   
In this section, we firstly defines genre. Next, we 
reviews previously works on genre categorization of 
web documents. 
 
1.1. Genre definition 
 
Most of English dictionaries defines genre as a 
category of artistic, musical or literary composition 
characterized by a particular style, content and form, 
where the style describes the structural features of the 
writing, the content is the topic of the document and 
the form refers to the layout in which the content is 
presented. 
With the emergence of the web genres, the 
functionality has been added as another attribute to 
identify the genre of web pages (also called cyber 
genre) [33]. The functionality is the purpose of the 
document and describes how a person interacts with a 
web page. This attribute becomes important when 
dealing with interactive web pages such as search 
engine Homepage, game Homepage, etc.       
Many authors define the web genre through facets 
such as complexity of language, subjectivity, number 
of graphics, etc. [21][28][11][30].  
Genres can be defined using these facets. For 
example a scientific paper is a genre with relatively 
complex language, low subjectivity and moderate 
number of graphics.  
Kessler et al. [21], defines the genre as “any 
widely recognized class of texts defined by some 
common communicative purpose or other functional 
 
Seventh IEEE International Conference on Data Mining - Workshops
0-7695-3019-2/07 $25.00 © 2007 IEEE
DOI 10.1109/ICDMW.2007.120
455
traits, provided the function is connected to some 
formal cues or commonalities and that the class is 
extensible”.  
The advantage of facets is that they can tell 
something about documents with unknown genre. The 
disadvantage is that common genres are something 
users are more familiar with than facets.      
In the next paragraph we review some related 
works. We present respectively the different features 
and machine learning techniques used to classify 
documents by genres.  
 
1. 2. Related works          
 
The selection of features and the choice of 
categorization technique are the core of any genre 
categorization system. These tasks are relatively 
independent, so they will be presented separately.  
 
1.2.1. Features. Many types of features have been 
proposed for automatic genre categorization. These 
features can be grouped on four groups. The first 
group refers to surface features, such as function 
words, genre specific words, punctuation marks, 
document length, etc. The second group concerns 
structural features, such as Parts Of Speech (POS), 
Tense of verbs, etc. The third group is presentation 
features, which mainly describe the layout of 
document. Most of these features concerns HTML 
documents and cannot be extracted from plain text 
documents. Among these features we quote the 
number of specific HTML tags and links. The last 
group of features is often extracted from metadata 
elements (URL, description, keywords, etc.) and 
concerns only structured documents.  
Most of these features have been used in many 
works to identify the genre of web pages.  
Kessler et al. [21] Have used four types of features 
to classify the Brown corpus1 by genre. The first types 
are structural features, which includes counts of 
functional words, sentences, etc. The second types are 
lexical features, which includes the existence of 
specific words or symbols. The third kinds of features 
are character level features, such as punctuation marks. 
The last kind concerns derivative features, which are 
derived from character level and lexical features. 
These four features sets can be grouped on two sets, 
structural features and surface features. 
Karlgren [20] have used twenty features: count of 
functional words, POS count, textual count (e.g. the 
count of characters, the count of words, number of 
words per sentence, etc.) and count of images and 
links. 
                                                                                                                    
1 http://en.wikipedia.org/wiki/Brown_Corpus 
Stamatatos et al. [33] Identified genre based on the 
most English common words. They have used the fifty 
most frequent words on the BNC corpus2 and the eight 
frequent punctuation marks (period, comma, colon, 
semicolon, quotes, parenthesis, question mark and 
hyphen).      
Dewdney et al. [13] Have adopted two features 
sets: BOW (Bag Of Words) and presentation features. 
They used a total of 89 features including layout 
features, linguistic features, verb tenses, etc. 
Finn and Kushmerick [14] used a total of 152 
features to differentiate between subjective vs. 
objective news articles and positive vs. negative movie 
reviews. Most of these features were the frequency of 
genre-specific words. 
More recently, Santini [30] and Lim et al. [22] 
have tried to exploit all previously used features, 
where Lim and al. have used the document URL as 
new feature. 
 
1.2.2. Machine learning techniques. Once a set of 
features has been obtained it is necessary to choose a 
categorization algorithm. Generally, genre 
categorization algorithms are often based on machine 
learning techniques [25]. Among these techniques, we 
briefly explain naïve Bayes, k-nearest neighbor, 
decision trees and neural networks techniques because 
they are widely used in literature.    
Naïve Bayes (NB) is a simple probability 
algorithm that determines the probability of a 
document belonging to a particular genre. NB is a very 
fast learning algorithm, which is robust to irrelevant 
features. It need less storage space and can handle 
missing values. However, because the weights are the 
same for all features, performance can be degraded by 
having more irrelevant features. This technique has 
been implemented by Argamon et al., 1998 [2], 
Dewdney et al. [13] and Santini [30][31]. 
The K-Nearest Neighbour (KNN) algorithm 
groups documents within a vector space. TFIDF (Term 
Frequency Inverse Document Frequency) is usually 
used to represent documents and Euclidean or cosine 
measure are often used to compute the similarity 
between documents. New documents are classified to 
the same genre as the nearest neighbor. The K 
represents how many neighbours should be analysed. 
KNN have only been used by Lim et al. [22]. 
Decision trees (Tree) are a popular technique used 
by Argamon et al. [2], Bretan et al. [4], Karlgren  [20], 
Dewdney et al. [13] and Finn [15]. Karlgren calculated 
2 http://www.natcorp.ox.ac.uk/ 
 
456
textual features for each document and categorized 
into a hierarchy of clusters based on C4.5 if-then 
categorization rules. The labels for genres were 
accomplished using nearest neighbour assignments and 
cluster centroids.  
Support Vector Machine (SVM) is a powerful 
learning method introduced by Vapnik [36]. Is has 
been successfully applied to text categorization [18]. 
SVM is based on Structural Risk Maximization theory, 
which aims to minimize the generalization error 
instead of the empirical error on training data alone. 
The SVM technique has been used in genre 
categorization by many authors (e.g. Dewdney et al. 
[13], Argamon and Dodick [1]).    
Neural networks (NN) have been used by Kessler 
et al. [21] to classify genres of the Brown corpus. 
While Rauber and Muller-Kogler [28] used a vector of 
terms appearing in each document. The document 
vectors are the input of a Self-Organizing Maps 
(SOM). 
The reminder of this paper is structured as 
follows. In section 2 we emphasize the importance of 
flexibility in genre categorization. In section 3 we 
briefly present some approaches for web page 
categorization. In section 4 we explains our approach. 
Section 5 presents our experimental results using two 
datasets of web pages. Finally we present some 
conclusions as well as our future work.           
 
2. Flexible genre categorization 
 
Most of the authors working in automatic genre 
categorization assume that genres are mutually 
exclusive discrete classes; each document is assigned 
to one genre. For this reason, most of genre 
categorization approaches are based on discrete single-
genre categorization. In this context, many researchers 
involved in genre categorization states that a classifier 
should be able to assign multiple genres into a 
document [9] [22]. Crowston and Kawsnik [9] have 
stated that documents on the Web are sometimes 
composed of multiple Web pages. Although many 
documents contain integrated genres, some documents 
contain various genres embedded in distinct locations 
of a document, easily identifiable by some predefined 
markers or section headings. For example a course 
document is a one main page that contains information 
on the course, projects, instructors and a list of 
hypertext links to related information. Another 
example concerning an email, where it can be sended 
to communicate a call for papers or a FAQ. Three 
methodologies have been used to handle multi-genre 
categorization. The first classify documents to multiple 
genres. The second consist in deciding on a dominant 
genre and the third consist in separating documents 
into various segments and index on each individual 
segment. In the literature only three studies have 
studied categorization flexibility, namely Kessler et al. 
[21], Rauber and Muller-Kogler [28] and Santini 
[30][31]. 
Kessler et al. [21] Explicitly address the problem 
of genre heterogeneity by proposing a multi-facetted 
approach. In this approach, genres are expressed in 
terms of bundles of generic facets that correlate to 
surface cues. According to the authors, a facet is “a 
property which distinguishes a class of texts that 
answers to certain practical interests, and which is 
associated with a characteristic set of computable 
structural or linguistic properties, the ‘generic cues’”. 
In their experiments, the authors use only three facets: 
brow, narrative and genre.         
Rauber and Muller-Kogler [28] experiment is 
interesting. However, in this experiment, the 
assignment of documents to mixed or fuzzy genres is 
more incidental than deliberate. In the resulting 
visualization documents belonging to different genres 
are depicted as books with different colours.  
We notice that these two works have not addresses 
satisfactory the problem of flexible genre 
categorization. Recently, Santini [30][31] have 
proposed a flexible categorization scheme that 
encompass zero, one or multi-genre assignment, as 
required by the actual state of genres on the web.   
In this paper we study the flexibility of genre 
categorization by proposing a new flexible approach 
that assign a document to all genres with different 
member ship degrees. 
 
3. Approaches of web page categorization 
 
Most works on categorization are interested with raw 
texts and there is limited works dealing with structured 
and semi-structured documents, especially web pages. 
All approaches for web page classification use the 
semantic of HTML tags.  
Quek [27] for example combines three classifiers 
based respectively on the textual content of a page, 
section titles and hyperlinks.  
Cline [7] represents a structured document by a vector 
where each element (title, links, text, etc.) is encoded 
in a specific part of the vector using tfidf formula.  
Chakrabarti, Dom and Indyk [6] exploit the hyperlink 
structure of web pages and combine this with the 
prediction based on the local text.  
Furnkranz [16] show an improvement in accuracy 
when classifying hypertext documents by using the 
 
457
textual context of links to a web page as features for 
classification.  
All these approaches use document structure to 
determine the linked documents and to extract the 
relevant portions of text from linked documents.  
Yi and Sundaresan [39] and Denoyer and Gallinari 
[12] propose a tree structure for modeling HTML 
documents where nodes represent document elements 
(links, headings, title, etc.).  
The approach by Yang, Slattery and Ghani [38] 
combine three classifiers operating respectively on 
linked pages textual information, HTML tags and 
metadata. 
In email categorization, Brutlag and Meek [5] 
show that using only the features of sender and subject 
fields can ameliorate categorization accuracy.  
As the number of web pages increases infinitely, 
automatic categorization becomes time consuming. To 
deal with this problem, Kan [19] studies how a web 
page can be classified without retrieving its content. 
He bases his work on the URL.  
The aim of these approaches is to identify the 
document topic. In this paper we extract internal and 
external structure to identify the web page structure 
and then perform structural categorization. The URL is 
used for contextual categorization. Both structural and 
contextual categorizations are combined to identify the 
genre of a given web page. 
 
4. Proposed approach  
 
Our approach is based on the combination of two 
homogenous classifiers. The first is a contextual 
classifier, which uses the web page URL. However, 
the second is a structural classifier, which aim is to 
exploit the web page structure to identify the genre of a 
web page. The web page structure is extracted from 
title, headings and anchors. Both contextual and 
structural classifiers are based on category centroid, 
which is generated from the training set. The 
originality of our approach is the use of new features, 
which are used in previous works but in different ways 
and the flexibility, which is not enough, studied and 
not yet implemented.  
In this section, we firstly explain the principal of 
our approach. Next we explain contextual, structural 
and combined classifiers.  
 
4.1. Approach principal  
 
Our approach is based on category centroid, where 
documents are represented using the vector-space 
model [29]. In this model, each document d is 
represented by a tfidf vector dtfidf = (tf1 log(N/df1), …, 
tfn log(N/dfn)), where tfi is the frequency of the ith 
term in the document, dfi is the number of documents 
that contain the ith term and N is the number of 
training documents. For a category c, the centroid is 
represented by the average for all vectors for the 
positive examples for this category: 
 
C = ∑
∈
⋅
cc
1
d
d  
 
The idea of our approach is to compute the 
centroid vectors of all categories. So, if you have k 
categories, this leads to k centroid vectors {C1, C2, …, 
Ck}, where Ci is the centroid for the ith category. For a 
new document x, our approach compute the 
similarities between x and all k centroids using the 
cosine measure as follow: 
 
( )
2i2
i
i Cx
Cx
C,xcos
×
⋅
=  
 
Our approach combines two classifiers (contextual 
and structural classifiers). These classifiers are 
homogenous because they are based on the same 
principal presented above. In the next paragraph we 
explains contextual, structural and combined 
classifiers. 
 
4.2. Contextual classifier 
 
The URL address defines the location of a document 
on the web. Is composed of three parts: host name 
(domain), directory path and file name. The URL is not 
expensive to obtain and one of the more informative 
sources about the document genre. For example, if the 
file extension is PDF, PS or DOC, then the document 
is long and may be a paper, a book, a thesis, a manual, 
etc. An other example, if the file name contain some 
genre specific words like faq, cv, how, thesis, etc., we 
can easily recognize the document genre. URLs are 
often meant to be easily recalled by humans, and web 
sites that follow good design techniques will encode 
useful words that describe their resources in the web 
site’s host name (domain). Web sites that present a 
huge amount of information often break their contents 
into web pages. This information structuring is also 
accompanied with URLs structuring. 
As URL is a rich resource of information about 
document genre, is used by the contextual classifier to 
perform genre categorization. 
 
458
In our approach we have used the words contained 
in the URL after preprocessing which is based on three 
steps. The first step removes specific stop words 
(www, ftp, http, etc.), special characters (/, -, _, +, :, #, 
?, etc.) and digits. The second step consists in 
stemming the obtained words using the famous Porter 
stemmer [26]. Finally, the third step is the dimension 
reduction, which is based on both word frequency and 
document frequency [37]. We only words that appears 
more than 3 times in the document and appears in 
more than 3 documents. The remaining words will be 
processed using the principal presented in section 4.1.  
For a new document d, the contextual classifier 
provide a contextual categorization CC(d) given as 
follow: 
 
CC(d) = {(c1, α1), …, (ci, αi), …, (cn, αn)} 
 
Where, ci is a predefined category and αi is the 
pertinence degree of the category ci for the given 
document d. αi is the similarity between the category ci 
and the document d, which is calculated using the 
cosine similarity as explained in the section 2.1.1.   
 
4.3. Structural classifier 
 
Identification of document genres based on 
document structure has been investigated by Toms et 
al. [35] and [10]. In their research, Toms et al. [35] 
found for specific document genres, such as letters and 
journal articles, there is a strong link between 
identification of genres and the structure of the 
document. Although their researches has not been 
extended into the development of automatic methods 
for document genre classification, the strong 
correlation between document genres and document 
structure suggests that development of machine-based 
methods are plausible. Crowston and Williams [10] 
have indicated how linking structure of a web page can 
help identify the form of the web page and can be used 
to help identify its genre. In our approach we have 
used the linking structure in different way than used by 
previous researches. In our work we have used the 
terms contained in hyperlinks contrary to many other 
researches that use the number of internal and external 
links, number of images, etc. [10][22][3]. 
In this paper, we suggest to implement a classifier 
that is based on a web page structure of the web page. 
The web page structure is very useful in genre 
categorization [17]. To extract the web page structure 
we have used title, headings and anchors. Title and 
headings tags represent the internal structure, however, 
anchors denote the external or hypertext structure. In 
our approach the web structure is the combination of 
internal and hypertext structures.  
Our structural classifier exploits the terms 
contained in title, headings and anchors tags. This 
classifier is based on three steps. The first step consists 
in removing only special characters (:, ., -, _, etc.) and 
digits. The second step stem remained words using the 
Porter stemmer [26]. In the third step, our approach 
reduce vocabulary dimension using both word 
frequency and document frequency, which consists in 
pruning words that appeared less than 3 times in a 
document and appeared in less than 3 documents. The 
remaining words will be processed using the principal 
presented in section 4.1.  
For a new document d, the structural classifier 
provide a structural categorization SC(d) given as 
follow: 
 
SC(d) = {(c1, β1), …, (ci, βi), …, (cn, βn)} 
 
Where, ci is a predefined category and αi is the 
pertinence degree of the category ci for the given 
document d. αi is the similarity between the category ci 
and the document d, which is calculated using the 
cosine similarity as explained in the section 2.1.1. 
 
4.4. Combined classifier 
 
In our approach both contextual and structural 
classifiers are combined. In the literature many 
methods have been proposed to combine classifiers, 
which depend on the type of classifiers [40]. 
Generally, a classifier can be a class classifier, a rank 
classifier or a measurement classifier. A class classifier 
provides the correct class of a given document. The 
result of a rank classifier is a vector of ranks. For a 
given class, the rank represents his importance. The 
measurement classifier provides a vector of values, 
where each value represents the class membership.  
As described previously, contextual and structural 
classifiers are measurement classifiers because they 
provide a set of pairs (c, w), where c is a category and 
w is the confidence degree in the membership of a 
given document in the category c. Since our classifiers 
are independent, we have used the following 
combination rules: minimum, maximum, product, sum 
and average. For example, using the minimum rule, for 
a document d, the combined classifier provides the 
following categorization: 
 
C(d) = min(CC(d), SC(d))={(c1, min(α1, β1)), …, 
(ci, min(αi, βi)), …, (cn, min(αn, βn))} 
 
 
459
5. Experimentation 
 
In this section we presents a series of experiments 
conducted to evaluate our approach and compare it 
against other categorization techniques. This section is 
organized on five paragraphs. In the first paragraph we 
describe the experimentation setup. Next, in the second 
paragraph we present the datasets used to perform 
experimentations. In the third paragraph, we describe 
the performance measure used. In the fourth paragraph 
we present and discuss the obtained results. Finally, we 
compare our approach against some other 
categorization techniques. 
 
5.1. Experimentation setup 
 
Our experimentation methodology is to experiment 
contextual, structural and combined separately. Next, 
the combined classifier is compared against some 
classification techniques implemented in the rainbow 
program3 [23].  
In this paper all experimentations was done using 
10-fold cross validation. To do this, we split data 
randomly into ten equal parts. On each cross validation 
fold, 10% of the data was used for testing and the 
remaining 90% of the data was used for training. The 
system is then trained and tested for ten iterations, and 
in each iteration nine subsets of data are used as 
training data and the remaining set as testing data. In 
rotation, each subset of data serves as the testing set in 
one iteration. The micro-averaged BEP of the system is 
the average of BEP over the ten iterations. 
 
5.2. Datasets 
 
Experimentations should be conducted using datasets 
of HTML documents grouped by genres. To evaluate 
contextual classifier, we should know the URL address 
of the document. According to these conditions, we 
can use only two datasets, which are KI-044 [24] and 
WebKB5 [8] datasets.  
The KI-04 Dataset was built following a palette of 
eight genres suggested by a user study on genre 
usefulness. It includes 1295 web pages, but only 800 
web pages (100 per genre) were used in the experiment 
described in Meyer zu Eissen and Stein [24]. In the 
experiments described in this paper, I have used only 
1205 web pages because we have excluded empty web 
pages and error messages (see Table 1). 
                                                          
3 http://www.cs.cmu.edu/~mccallum/bow/rainbow/ 
4 http://www.itri.brighton.ac.uk/~Marina.Santini/ 
5http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-
20/www/data/ 
 
Table 1. Composition of KI-04 dataset 
Category # Of samples 
Article 127 
Download 151 
Link collection 205 
Private portrayal 126 
Non private portrayal 163 
Discussion 127 
Help 139 
Shop 167 
Total 1205 
 
The WebKB dataset has become a reference corpus in 
the machine learning community for topic 
categorization of web pages, but is not used for genre 
categorization. It is composed of 8282 web page from 
computer science department web sites of four 
American universities (Cornell, Texas, Washington 
and Wisconsin). These web pages are issued from 
seven categories, but we have used only six categories 
(course, department, faculty, project, staff and student) 
as usually done. So, after discarding the category other 
and empty web pages, we have obtained only 4249 
web page (see Table 2). 
 
Table 2. Composition of WebKB dataset 
Category # Of samples  
Student 1541 
Faculty 1063 
Staff 126 
Department 170 
Project 474 
Course 875 
Total 4249 
 
Experimentations presented in this paper are conducted 
using different vocabulary’ sizes according to the 
dataset and the feature used. These vocabularies are 
obtained after dimension reduction, which is based on 
word frequency and document frequency as mentioned 
in previous sections (see Table 3). 
 
Table 3. Number of selected terms for each feature and for 
both KI-04 and WebKB datasets 
Feature KI-04 WebKB 
URL 230 457 
Title 300 780 
Hn 425 1200 
Anchor 670 2180 
Title+Hn 725 1980 
Title+Anchor 970 2960 
Hn+Anchor 1085 3292 
Title+Hn+Anchor 1270 4072 
 
 
460
5.3. Performance measure 
 
As explained in section 4.1 our approach performs a 
multi-class multi-label categorization. For this reason it 
is suitable to use micro or macro averaged BEP 
measure. The macro-averaged BEP is obtained by 
averaging BEP for every category. However, micro-
averaged BEP is obtained by weighting the average by 
the relative size of each category. In our datasets some 
categories are very large and some are very small. 
These categories are not disjoint. For this reason, we 
have decided to use the micro-average BEP, which is 
defined in terms of standard measures of precision and 
recall. Precision p is the proportion of true document-
category assignments among all assignments predicted 
by the classifier. Recall r is the proportion of true 
document-category assignments that were also 
predicted by the classifier. Formally, the BEP measure 
finds the point where precision and recall are equal. 
Since it is very difficult to find the exact point, a 
common approach is to use the mean of recall and 
precision as an approximation. Thus, BEP equals 
(p+r)/2. The micro-averaged BEP is computed by first 
summing the elements of all binary contingency tables 
(one for each category). Precision, recall and BEP are 
then computed from these accumulated statistics.   
 
5.4. Results and discussion 
 
In this section we present the results obtained using 
each of the three classifiers described in previous 
sections. We also discuss the usefulness of URL and 
web page structure in genre categorization [17].   
 
5.4.1. Contextual categorization. For the contextual 
classifier, we have obtained 78.70% as micro-averaged 
BEP for WebKB dataset and 74.25% for KI-04 
dataset. The micro-averaged BEP for KI-04 corpus is 
greater than the micro-averaged BEP for WebKB 
dataset because web pages of the KI-04 collection have 
been downloaded form different sources, unlike 
WebKB collection, which have been downloaded from 
only computer science departments of four American 
universities (Cornell, Texas, Washington and 
Wisconsin). 
 
5.4.2. Structural categorization. The experimentation 
of the structural classifier is performed by different 
combinations of the content of title, hn and anchor 
tags. Different results have been obtained, which are 
summarized in the following table. For both KI-04 and 
WebKB datasets, the best micro-averaged BEP is 
obtained for the combination of title, Hn and anchor 
tags. 
Table 4. Micro-averaged BEP of structural classifier for 
different combinations of title, hn and anchors tags 
Tags KI-04 WebKB 
Title 79.13 84.15 
Hn 78.50 82.10 
Anchor 81.22 83.45 
Title+Hn 77.05 86.13 
Hn+Anchor 80.76 76.85 
Title+Hn+Anchor 86.35 88.65 
 
5.4.3. Combined categorization. In this 
experimentation we measure the effect of each 
combination rule on the final micro-averaged BEP of 
categorization. For the structural classifier we have 
used title, Hn and anchor tags because they provide the 
best micro-averaged BEP as shown in the table above. 
The results are presented in the Table 5. These results 
show that the minimum rule provide the best result 
(88.15% for KI-04 corpus and 90.23% for WebKB 
corpus). 
 
Table 5. Micro-averaged BEP of combined classifier for 
different combination rules and for KI-04 and WebKB 
datasets 
Combination rule KI-04 WebKB 
Minimum 88.15 90.23 
Maximum 87.27 84.35 
Product 76.05 77.10 
Average 86.11 77.17 
Sum 80.34 78.72 
 
5.5. Comparison with other approaches 
To compare our approach with other classification 
methods, we have used the famous rainbow program. 
As first step, we have extract for each document his 
URL and the content of title, Hn and anchor tags in 
separate files, which are used by the rainbow program 
to generate models. We notice that rainbow provides a 
number of data preparation and classification options. 
In our experimentations we have used TFIDF/Rocchio, 
Naïve Bayes, K-Nearest Neighbor (K=30), SVM 
(Fisher kernel) and tree nodes (Tree) as classification 
methods. For removing stop words we have used a 
specific words other than those included in Rainbow 
program because the stop words used by default in 
Rainbow are often used to perform topic 
categorization. The results are presented in the 
following table (Table 6 for WebKB dataset and Table 
7 for KI-04 dataset). From these tables we notice that 
TFIDF/Rocchio method provide best results for all 
features. But these results are less than those obtained 
by our approach. 
 
 
461
Table 6. Micro-averaged BEP for different 
classification methods and for different features for 
KI-04 dataset 
 TFIDF NB KNN SVM Tree 
URL 70.10 65.75 55.45 70.00 63.12 
Title 81.19 72.12 63.45 76.78 68.95 
Anchor 71.20 63.23 51.20 74.45 56.75 
Hn 70.30 66.19 60.23 72.12 68.25 
Title+Anchor 75.61 65.12 62.35 63.45 73.00 
Title+Hn 80.32 77.34 57.29 80.15 72.85 
Hn+Anchor 74.70 75.12 60.15 72.10 73.34 
Title+Hn+Anchor 84.40 78.17 66.50 77.89 70.17 
 
Table 7. Micro-averaged BEP for different 
classification methods and for different features for 
WebKB dataset 
 TFIDF NB KNN SVM Tree 
URL 73.57 69.63 44.55 72.23 70.15 
Title 72.75 61.25 55.25 70.15 68.54 
Amchor 73.60 71.12 48.48 74.12 68.47 
Hn 76.25 59.86 40.47 75.34 57.58 
Title+Anchor 72.35 70.78 55.46 69.45 70.17 
Title+Hn 77.33 80.13 45.23 78.80 79.28 
Hn+Anchor 77.45 74.78 59.15 72.15 75.46 
Title+Hn+Anchor 84.34 77.74 67.29 80.16 80.19 
 
For both KI-04 and WebKB datasets, we notice that 
the combination of the content of title, Hn and anchor 
tags outperform all other categorizations which uses 
each tag separately. It also better than contextual 
categorization which based on the content of the URL. 
These results confirm the results obtained by our 
approach. Our aim in this section is to show that our 
approach outperforms all other classifiers for different 
combinations of features. From Table 8 and 9 we 
conclude that TFIDF/Rocchio is the best classifier for 
all combinations of features. But these results cannot 
outperform the results obtained by the combined 
classifier proposed in this paper. 
        
6. Conclusion and Future Work 
 
In this paper we have proposed a new approach for 
flexible document genre categorization. The originality 
of our approach is the combination of two 
homogenous classifiers and the use of the web page 
structure. The proposed approach is flexible because it 
assigns a web page to all categories. Each category is 
associated with a weight representing the similarity 
between the document and the given category centroid 
generated from the training set. The Experimentations 
show that our approach provides results well than 
those obtained by rainbow classifiers. In the future we 
hope to integrate our approach in a web search engine. 
7. References 
 
[1] S. Argamon, and J. Dodick, “Conjunction and Modal 
Assessment in Genre Classification: A Corpus-Based Study 
of Historical and Experimental Science Writing”, AAAI 
Spring Symposium on Attitude and Affect in Text, 2004. 
 
[2] S. Argamon, M. Koppel, and G. Avneri, “Routing 
Documents According to Style”, First International 
Workshop on Innovative Information Systems, 1998. 
 
[3] E. Boese, and A. Howe, “Genre Classification of Web 
Documents”, Proceedings of the 20th National Conference on 
Artificial Intelligence (AAAI-05), Poster paper, Pittsburgh, 
Pennsylvania (USA), 2005. 
 
[4] I. Bretan, J. Dewe, A. Hallberg, N. Wolkert and J. 
Karlgren, “Web-Specific Genre Visualization”, WebNet, 
Orlando, USA, 1998. 
 
[5] J. D. Brutlag, and C. Meek, “Challenges of the email 
domain for text classification”, In proceedings of 
ICML’2000, 17th International Conference on Machine 
learning, Pages 103-110. Morgan Kaufmann Publishers Inc, 
2000. 
[6] S. Chakrabarti, B. E. Dom, and P. Indyk, “Enhanced 
hypertext categorization using hyperlinks”, In Proceedings 
of SIGMOD-98, ACM International Conference on 
Management of Data, Pages 307-318, ACM Press, New 
York, USA, 1998. 
 
[7] M. Cline, “Utilizing HTML Structure and Linked Pages 
to Improve Learning for Text Categorization”, Department 
of Computer Sciences, University of Texas, Undergraduate 
Honors Thesis, May 1999. 
 
[8] M. Craven, D. DiPasque, D. Freitag, A. McCallum, T. 
Mitchell, K. Nigam, and S. Slattery, “Learning to extract 
symbolic knowledge from the word wide web”, In 
proceeding of the 15th  national/10th conference on artificial 
intelligence/innovative applications of artificial intelligence, 
Madison, W, 1998. 
 
[9] K. Crowston, and M. Williams, “Can document-genre 
metadata improve information access to large digital 
collections?, Library Trends, Fall 2003. 
 
[10] K. Crowston, and M. Williams, “Reproduced and 
Emergent Genres of Communication on the World-Wide 
Web”. In Proceedings of the 30th Hawaii International 
Conference on System Sciences (HICSS-30). Hawaii (USA), 
1997. 
 
[11] K. Crowston, and B. Kwasnik, “A Framework for 
Creating a Facetted Classification for Genres: Addressing 
Issues of Multidimensionality”. In Proceedings of the 37th 
Hawaii International Conference on System Science (HICSS-
37, Hawaii (USA), 2004. 
 
 
462
[12] L. Denoyer, and P. Gallinari, “Bayesian network model 
for semi-structured document classification” Information 
processing and management 40(5), 807-827, 2004.  
 
[13] N. Dewdney, C. Vaness-Dikema, and R. Macmillan, 
“The form is the Substance: Classification of Genres in 
Text”. In Proceedings of the 39th Annual Meeting of the 
Association for Computational Linguistics and 10th 
Conference of the European Chapter of the Association for 
Computational Linguistics, Toulouse, France, 2001. 
 
[14] A. Finn and N. Kushmerick, “Learning to classify 
documents according to genre”. In Proceedings of the 
Workshop “DOING IT WITH STYLE: Computational 
Approaches to Style Analysis and Synthesis”, held in 
conjunction with IJCAI 2003), Acapulco (Mexico), 2003. 
 
[15] A. Finn, “Machine learning for genre classification”. 
MSc thesis, University College of Dublin, 2002. 
 
[16] J. Furnkranz, “Exploiting structural information for text 
classification on the WWW”. In Proceedings of AIDA-99, 3rd 
International Symposium on Advances in Intelligent Data 
Analysis, pages 487-498, 1999. 
 
[17] C. Jebari, and H. Ounalli, “The Usefulness of Logical 
Structure in Flexible Document Categorization”. In 
Proceeding of the International Conference on 
Computational Intelligence, Istanbul, Turkey. International 
Journal of Information Technology. 2004. 
 
[18] T. Joachims, “Text categorization with support vector 
machines: learning with many relevant features”, In 
Proceedings of ECML-98, 10th European Conference on 
Machine Learning, 1998. 
 
[19] M. Y. Kan, “Web page classification without the web 
page”, In WWW Alt. ’04: Proceedings of the 13th 
international World Wide Web conference on Alternate track 
papers & posters, pages 262–263, ACM Press, New York, 
NY, USA, 2004. 
 
[20] J. Karlgren, “Stylistic experiments in information 
retrieval”, In Natural Language Information Retrieval (ed. T. 
Strzalkowski), pages 147-166, 1999 
 
[21] B. Kessler, G. Numberg, and H. Shutze, “Automatic 
Detection of Text Genre”, In Proceedings of the 35th Annual 
Meeting of the Association for Computational Linguistics 
and 8th Conference of the European Chapter of the 
Association for Computational Linguistics, Madrid, Spain, 
1997. 
 
[22] C. S. Lim, K. J. Lee, and G. C. Kim, “Multiple sets of 
features for automatic genre classification of web 
documents”, Information processing and management 41(5), 
pages 1263-1276, 2005. 
 
[23] A. McCallum, “A toolkit for statistical language 
modeling, text retrieval, classification and clustering”, 1996. 
(http://www.cs.cmu.edu/~mccallum/bow). 
 
[24] S. Meyer zu Eissen, and B. Stein, “Genre Classification 
of Web Pages: User Study and Feasibility Analysis”. In 
Biundo S., Fruhwirth T. and Palm G. (eds.). KI2004: 
Advances in Artificial Intelligence, Springer. Berlin-
Heidelberg-New York, Pages. 256-269, 2004. 
 
[25] T. Mitchell, “Machine Learning”, McGraw-Hill, 
Boston, USA, 1997. 
 
[26] M. Porter, “An algorithm for suffix stripping”. Program, 
14(3), 1980. 
 
[27] C. Y. Quek, “Classification of World Wide Web 
Documents”, School of Computer Science, Carnegie Mellon 
University, Senior Honors Thesis, 1997. 
 
[28] A. Rauber, and A. Muller-Kogler, “Integrating 
Automatic Genre Analysis into Digital Libraries”, In 
Proceedings of the ACM/IEEE Joint Conference on Digital 
Libraries (JCDL 2001), Roanoke, Virginia (USA), 2001. 
 
[29] G. Salton, “Automatic Text Processing: The 
transformation, analysis and retrieval of information by 
computer”, Addison-Wesley, 1989.  
 
[30] M. Santini, “Automatic identification of genre in web 
pages”, Ph.D Thesis, University of Brighton, UK, 2007. 
 
[31] M. Santini, “A Shallow Approach To Syntactic Feature 
Extraction For Genre Classification”, In Proceedings of the 
7th Annual Colloquium for the UK Special Interest Group 
for Computational Linguistics (CLUK 2004), Birmingham 
(UK), 2004. 
 
[32] F. Sebastiani, “Machine learning in automated text 
categorization”, ACM Computing Surveys, 34(1), Pages 1-47, 
2002. 
 
[33] Shepherd, M., Watters, C. The Functionality Attribute 
of Cybergenres. In Proceedings of the 32nd Hawaii 
International Conference on System Sciences (HICSS-32). 
1999. Hawaii. USA. 
 
[34] E. Stamatatos, N. Fakotakis, and G. Kokkinakis, “Text 
Genre Detection Using Common Word Frequencies”, In 
Proceedings of the 18th  International Conference on 
Computational Linguistics (COLING 2000), Saarbrücken 
(Germany), 2000. 
 
[35] E. G. Toms, D. G. Campbell, and R. Blades, “Does 
genre define the shape of information? the role of form and 
function in user interaction with digital documents”, In 
Proceedings of the 62nd Annual Meeting of the American 
Society for Information Science, pages 693–704, Information 
Today, Inc, 1999.  
 
463
 
[36] V. Vapnik, “The Nature of Statistical Learning”, , 
Springer-Verlag, 1995. 
 
[37] Y. Yang, and J. O. Pedersen, “A comparative study on 
feature selection in text categorization”, In Proceedings of 
International conference on machine learning, pages 412-
420, 1997. 
 
[38] Y. Yang, S. Slattery and R. Ghani, “A Study of 
Approaches to Hypertext Categorization”, Journal of 
Intelligent Information systems –18(2-3), Pages 219-241, 
2002. 
 
[39] J. Yi, and N. Sundaresan, “A classifier for semi-
structured documents”, In proceedings 6th ACM SIGKDD 
International Conference on Knowledge Discovery and Data 
Mining, pages 340-344, ACM Press, 2000.  
 
[40] H. Zouari, L. Heutte, L. Lecourtier and A. Alimi, “Un 
panorama des méthodes de combinaison de classifieurs en 
reconnaissance de formes”, In 13ème Congrès Francophone 
AFRIF-AFIA de Reconnaissance des Formes et 
d'Intelligence Artificielle RFIA'02, Angers, France, vol. 2, 
Pages 499-508, 2002. 
 
464
