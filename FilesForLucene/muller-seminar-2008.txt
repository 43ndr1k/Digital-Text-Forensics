Seminar
Music Information Retrieval
Meinard Müller, Verena Konz, Peter Grosche
Saarland University, Max-Planck-Institut für Informatik
Campus E1 4, 66123 Saarbrücken, Germany
{meinard,vkonz,pgrosche}@mpi-inf.mpg.de
        
       
  
       
1 Organization
• Winter term 2008/2009, Thu. 16-18, Room 023, Campus E1-4
• First preparatory meeting: Thu. 24.07.2008, 16-18
• Second preparatory meeting: Thu. 23.10.2008, 16-18
• First seminar talk: Thu. 30.10.2008
• http://www.mpi-inf.mpg.de/departments/d4/teaching/ws2008-2009/mir_mm/index.html
• Contact:
– Meinard Müller, meinard@mpi-inf.mpg.de
– Verena Konz, vkonz@mpi-inf.mpg.de
– Peter Grorsche, pgrosche@mpi-inf.mpg.de
2 Content
In this seminar, we discuss a number of current research problems in the field of music information
retrieval (MIR).
3 Course Prerequisite
The seminar particularly (but not exclusively) addresses the participants of the course Information
Retrieval for Music and Motion1. Requirements are a solid mathematical background, a good
understanding of fundamentals in digital signal processing, as well as a general background and
personal interest in music. The seminar is accompanied by readings from textbooks or the research
literature. Furthermore, the students are required to experiment with MATLAB.
1http://www.mpi-inf.mpg.de/departments/d4/teaching/sose2008/ir_mm/index.html
1
4 Topics
4.1 Performance Analysis
Expressive music performance is the art of shaping a musical piece by continuously varying tempo,
dynamics, and articulation. Musicians give the piece of music a personal touch while, e. g., speeding
up at some places, slowing down at others, or stressing certain notes instead of playing mechanically.
In this regard Performance Analysis deals with questions like: Are there commonalities between
different performers which lead to fundamental principles of music performance and allow us to
find general rules? On the other hand, can one characterize formally what is special about the
style of a particular pianist – can we find, e. g., a “Horowitz factor”? In order to build interpretable
quantitative models of certain aspects of performance, methods from the areas of machine learning,
data mining and pattern recognition are applied.
Literature: [7, 8, 10, 11, 14, 15, 16, 17, 18, 19]
4.2 Music Segmentation
Content-based processing, classification, retrieval, and indexing of music is a difficult undertaking
because of the complexity and abundance of information given for a single piece of music. The
goal of music segmentation is to cut the whole piece into smaller sections, like intro-chorus-verse-
outro or similar segments, each with some homogeneous properties with regard to the musical
content. Segment boundaries are defined by some variation of sound types that can be modelled
using timbral and rhythmic features of the signal adapted to the human perception of music (“The
way it sounds” [1]). Besides methods from digital signal processing, we will encounter various
machine learning and pattern recognition techniques including clustering methods and Hidden
Markov Models used for segment classification and boundary detection.
Literature: [1, 4, 6, 9]
4.3 Music Structure Analysis
One subtask of music segmentation is referred to as music structure analysis. Here, the goal is to
automatically extract the repetitive structure or, more generally, the musical form of the underlying
piece of music. Automated structure analysis becomes a challenging task when repeating parts
reveal significant variations in dynamics, timbre, articulation, instrumentation and tempo. In this
seminar block, we discuss various approaches to music structure analysis based on self-similarity
matrices. Furthermore, we investigate how these approaches may be combined with segmentation
strategies.
Literature: [2, 3, 5, 12, 13]
5 Course Requirement
• Reading assignments
• MATLAB experiments
• Meetings with tutors
• Seminar Talk (maximal 45 minutes, using PowerPoint template)
• Summary (2 pages, using LATEX template)
• Participation in seminar
2
6 Evaluation Criteria
• Content of presentation
• Style of presentation
• Quality of summary
• Degree of participation in seminar
• Impression by tutor and fellow students
References
[1] J.-J. Aucouturier, F. Pachet, and M. Sandler. The way it sounds : Timbre models for analysis and
retrieval of polyphonic music signals. IEEE Transactions of Multimedia, 7(6):1028–1035, December
2005.
[2] M. A. Bartsch and G. H. Wakefield. Audio thumbnailing of popular music using chroma-based
representations. IEEE Trans. on Multimedia, 7(1):96–104, Feb. 2005.
[3] M. Cooper and J. Foote. Automatic music summarization via similarity analysis. In Proc. ISMIR,
Paris, France, 2002.
[4] M. Goodwin and J. Laroche. Audio segmentation by feature-space clustering using linear discriminant
analysis and dynamic programming. Applications of Signal Processing to Audio and Acoustics, 2003
IEEE Workshop on., pages 131–134, Oct. 2003.
[5] M. Goto. A chorus section detection method for musical audio signals and its application to a music
listening station. IEEE Transactions on Audio, Speech & Language Processing, 14(5):1783–1794, 2006.
[6] K. Jensen. Multiple scale music segmentation using rhythm, timbre, and harmony. EURASIP J.
Appl. Signal Process., 2007(1):159–159, 2007.
[7] P. Knees, E. Pampalk, and G. Widmer. Automatic classification of musical artists based on web-data.
ÖGAI Journal, 24(1):16–25, 2005.
[8] J. Langner and W. Goebl. Visualizing expressive performance in tempo-loudness space. Computer
Music Journal, 27(4):69–83, 2003.
[9] M. Levy and M. Sandler. Structural segmentation of musical audio by constrained clustering. Audio,
Speech, and Language Processing, IEEE Transactions on, 16(2):318–326, 2008.
[10] S. T. Madsen and G. Widmer. Exploring pianist performance styles with evolutionary string matching.
International Journal on Artificial Intelligence Tools, 15(4):495–514, 2006.
[11] L. Mion and G. D. Poli. Score-independent audio features for description of music expression. IEEE
Transactions on Audio, Speech & Language Processing, 16(2):458–466, 2008.
[12] M. Müller. Information Retrieval for Music and Motion. Springer, 2007.
[13] J. Paulus and A. Klapuri. Music structure analysis by finding repeated parts. In AMCMM ’06:
Proceedings of the 1st ACM workshop on Audio and music computing multimedia, pages 59–68, New
York, NY, USA, 2006. ACM.
[14] C. Saunders, D. R. Hardoon, J. Shawe-Taylor, and G. Widmer. Using string kernels to identify famous
performers from their playing style. In ECML, pages 384–395, 2004.
[15] E. Stamatatos and G. Widmer. Automatic identification of music performers with learning ensembles.
Artif. Intell., 165(1):37–56, 2005.
[16] G. Widmer. Studying a creative act with computers: Music performance studies with automated
discovery methods. Musicae Scientiae, IX(1):11–30, 2005.
[17] G. Widmer, S. Dixon, W. Goebl, E. Pampalk, and A. Tobudic. In search of the horowitz factor. AI
Magazine, 24(3):111–130, 2003.
[18] G. Widmer and W. Goebl. Computational models of expressive music performance: The state of the
art. Journal of New Music Research, 33(3):203–216, 2004.
[19] Y.-H. Yang, Y.-C. Lin, Y.-F. Su, and H. H. Chen. A regression approach to music emotion recognition.
IEEE Transactions on Audio, Speech & Language Processing, 16(2):448–457, 2008.
3
