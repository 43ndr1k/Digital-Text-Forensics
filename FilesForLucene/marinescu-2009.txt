A timing-based classification method for human
voice in opera recordings
Maria-Cristina Marinescu
Department of Computer Science
Universidad Carlos III de Madrid
Leganes, Spain
Email: mcristina@arcos.inf.uc3m.es
Rafael Ramirez
Music Technology Group
Universitat Pompeu Fabra
Barcelona, Spain
Email: rramirez@iua.upf.edu
Abstract—The goal of this work is to identify famous tenors
from commercial recordings. Our approach is based on training
expressive singer-specific models and using them to classify new
musical fragments interpreted by singers that perform arias from
the training set. In this paper we focus on expressive timing
variations and build the models by applying machine learning
techniques to a body of data consisting of high-level descriptors
extracted from audio recordings. The experimental results show
evidence that performers can be automatically identified at a rate
significantly better than random choice.
I. INTRODUCTION
Music performance plays an important role in our musical
culture. Even within the same genre people have specific
preferences for particular performers; this has a great impact
on concert attendance and recording sales. The manipulation
of sound properties such as pitch, timing, amplitude and
timbre by different performers is clearly distinguishable by
the listeners. Expressive music performance research studies
the manipulation of these sound properties in an attempt to
understand how different singers express emotion in perfor-
mances.
The goal of this work is to identify operatic singers from
commercial audio recordings. We approach this problem by
training expressive singer-specific models and testing them
on new arias that do not form part of the training set. By
comparing the results of testing across singers we predict who
is the performer for each of the test arias. For this method to
correctly identify a target singer it must be the case that one of
the models has been trained on other arias by the same singer.
As a result of this work it would be possible to understand
what makes a singer unique with regard to other singers and
what constitutes their personal signature, be it mostly the
timbre or typical expressive alterations to the score. This could
be a useful tool in learning different expressive ways to reach
the same effect, as well as a search and classification tool.
In this paper we focus on timing expressive performance
models. We characterize the arias in our training set us-
ing high-level descriptors extracted from a cappella audio
recordings. Based on the values taken by the descriptors
we automatically learn expressive patterns typical of specific
performers. Given a new aria performed by a yet unidentified
singer we calculate the degree to which it fits the patterns
of each of the performers in the training set. As result the
target singer is classified as being the one whose performance
model most closely predicts the actual performance. Currently
our dataset consists of pieces sung by only two performers but
our method works independently of this number.
The rest of the paper is organized as follows. Section II
describes the background and the related work. Section III
describes our approach for classifying target singers. Section
IV describes our test suite and discusses our results. Section
V comments on a few future steps and concludes.
II. BACKGROUND
Most research in expressive music performance in con-
cerned with analyzing and predicting the emotional intention
of the player (e.g. [12], [8]), generating expressive perfor-
mances, and learning expressivity rules. To our knowledge
nevertheless, there doesn’t seem to exist an extensive body
of work concerned with identifying music performers. Un-
derstanding and formalizing expressive music performance is
an extremely challenging problem which in the past has been
studied from several different perspectives (e.g. [19], [13], [7]).
The main approaches to empirically studying expressive per-
formance have been based on statistical analysis (e.g. [18]),
mathematical modeling (e.g. [22]), and analysis-by-synthesis
(e.g. [9]). In all these approaches, it is a person who is
responsible for devising a theory or mathematical model which
captures different aspects of musical expressive performance.
The theory or model is later tested on real performance data
in order to determine its accuracy.
A different approach to capture expressive music perfor-
mance is to build the model based on the data, often by
applying machine learning techniques. Widmer [23] reported
on the task of discovering general rules of expressive classical
piano performance from real performance data via inductive
machine learning. Bresin [5] developed a system that com-
bines symbolic decision rules with neural networks to simulate
the style of real, specific, piano performers.
Ramirez et al. [16], [17] explore and compare different
machine learning techniques for inducing both, an inter-
pretable expressive performance model (characterized by a
set of rules) and a generative expressive performance model.
Based on this, they describe a performance system capable
2009 International Conference on Machine Learning and Applications
978-0-7695-3926-3/09 $26.00 © 2009 IEEE
DOI 10.1109/ICMLA.2009.128
577
Authorized licensed use limited to: Aegean University. Downloaded on July 27,2010 at 15:02:54 UTC from IEEE Xplore.  Restrictions apply. 
of generating expressive monophonic Jazz performances and
providing ’explanations’ of the expressive transformations it
performs. Lopez de Mantaras et al. [14], [4] report on SaxEx,
a performance system capable of generating expressive solo
performances in jazz. Their system is based on case-based
reasoning, a type of analogical reasoning where problems are
solved by reusing the solutions of similar, previously solved
problems. In order to generate expressive solo performances,
the case-based reasoning system retrieves, from a memory
containing expressive interpretations, those notes that are simi-
lar to the inexpressive notes of the input. The KTH group from
Stockholm [6], [11], [9], [10] developed a system which incor-
porates rules inferred both from theoretical musical knowledge
as well as experimentally from training—particularly using the
so-called analysis-by-synthesis approach.
A bit surprisingly, the use of expressive models to identify
musicians has not been extensively used. This may be due to
one or both of two factors: (1) the high complexity of the
feature extraction process required to characterize expressive
performances, and (2) the question of what is the best way to
use the information provided by the expressive model for the
recognition task. Most of the work in automatic performer
identification comes, to the best of our knowledge, from
the group led by Gerhard Widmer. Zanon and Widmer [25]
apply machine learning to performance data extracted from
recordings of world-class pianists with the purpose of recog-
nizing them based on their playing style. Each of the learning
instances has an associated context of a fixed size. The goal of
the learning task is the n-way classification of all the players
at once. Stamatos and Widmer [26] approach the problem of
distinguishing between skilled pianists by proposing a set of
very simple features for representing stylistic characteristics
of a performer that relate to a kind of “average” performance.
They propose an ensemble of classifiers derived by sub-
sampling the training set and the input features. The pianists
are playing the same piece on a computer-monitored piano
which can record precise information about each performance.
Sauders et al. [27] apply string kernels to the problem of rec-
ognizing famous pianists. From deviations in tempo and beat
general performance alphabets can be derived, and pianists’
performances can then be represented as strings.
Ramirez et al. [28] investigate how violinists perform pop-
ular Celtic music and how to use this information to identify
performers. Unlike most research on expressive performance
this work moves away from classical performance in which a
score is available and it is clear how to measure and evaluate
variations between performers.
Most of the research in expressive music performance
is concerned with instrumental music, particularly jazz and
classical, and focuses on specific instruments (e.g. piano,
saxophone). However, singing voice expressive performance
has been much less explored. Alonso [3] describes the design
of an expressive performance model focused on emotions for
a singing voice synthesizer, but does not tackle the problem
of singer identification.
The speaker recognition field has long been concerned with
Fig. 1. Prototypical Narmour structures
the problem of speaker identification both in text-dependent
and text-independent contexts (e.g. [29], [30], [32]). Speaker
identity is correlated with physiological and behavioral char-
acteristics of the speaker which exist both in the spectral
envelope of the signal as well as in the supra-segmental
features. In the training phase reference templates for each
speaker are generated for each phonetic category; similarity
over a computed threshold for these phonetic categories will
positively identify the speaker. Feature extraction is crucial
for the front-end process of speaker identification; most of
the approaches use coefficients such as MFCC and LPCC as
feature vectors, but do not take into account context informa-
tion which can give clues about specific ways that a speaker
tends to utter a sentence. Some of the more recent work [1]
addresses various temporal aspects and relationships between
prosodic features to capture speaker-specific phenomena such
as intonation and stress.
III. OUR APPROACH
We begin this section by introducing the set of parameters
that we choose to characterize a musical piece. We specify
a musical piece based on (1) its score, (2) the actual perfor-
mance, and (3) the actual singing tempo. In this work the
performance is used to compute a duration ratio parameter;
this is associated with every note and stands for the ratio by
which the performed note duration differs from the specified
note duration in the score. The second half of this section
presents the analysis step, which consists of three parts: (1)
pre-process the arias into smaller units, (2) apply the learning
algorithms to obtain a set of singer-specific models, and (3)
evaluate the singer-specific models against each corresponding
unit to predict the target singer.
A. Melody characterization
We characterize each note by a set of features representing
both properties of the note itself and aspects of the musical
context in which the note appears. Information about the
note includes note pitch, note duration and metrical strength.
Information about its melodic context includes the relative
pitch of the neighboring notes, the relative duration of the
neighboring notes, the Narmour structures to which the note
belongs, and the actual tempo of the piece.
In order to provide an abstract structure to our performance
data, we are using Narmours theory [15] to analyze the
performances. The Implication/Realization model proposed by
Narmour is a theory of perception and cognition of melodies.
The theory states that a melodic musical line continuously
causes listeners to generate expectations of how the melody
should continue. According to Narmour, any two consecutively
perceived notes constitute a melodic interval, and if this
interval is not conceived as complete, it is an implicative
578
Authorized licensed use limited to: Aegean University. Downloaded on July 27,2010 at 15:02:54 UTC from IEEE Xplore.  Restrictions apply. 
interval, i.e. an interval that implies a subsequent interval with
certain characteristics. That is to say, some notes are more
likely than others to follow the implicative interval. Based on
this, melodic patterns or groups can be identified that either
satisfy or violate the implication as predicted by the intervals.
Fig. 1 shows prototypical Narmour structures. We parse each
aria in the training data in order to automatically generate an
implication/realization analysis of the fragments.
We use sound analysis techniques based on spectral mod-
els [20] for extracting the high-level symbolic features related
to note onset and duration. The tempo is calculated manually
at the granularity of the whole aria and is the actual singing
tempo of the performance calculated based on the score and
the duration of the performance. Most scores are annotated
with a suggested tempo; invariably for our benchmarks the
real tempo is slower than the suggested tempo, up to 100%.
The metrical strength associated with a note corresponds with
the position that the note takes within a bar. This depends on
the time signature of a music piece. For instance in 4/4 time
signature the first beat of a measure receives the strongest
stress, followed by the third beat. Following in decreasing
stress level are beats two and four, followed by off-beats,
and lastly by notes which begin at any other time during the
bar. The metrical strength of each note in a unit is computed
automatically based on the note duration and the starting
position of the first note in the unit. The duration and pitch of
the notes are determined by their values in the written score.
B. The analysis step
We use the Waikato Environment for Knowledge Analysis
(WEKA) [24] to obtain the singer-specific expressive models.
Our benchmarks are a cappella fragments from arias per-
formed by Josep Carreras and Placido Domingo. We mini-
mally process these fragments in two different alternative man-
ners: (1) semantic by manually splitting them into prosodic
units, and (2) syntactic by splitting them into larger units of
(almost) equal number of notes—in our case 13 or 12. A
prosodic unit is a semantic unit with length which can vary
from a single word to a full statement. Even though it isn’t
necessary that the prosodic units and those phrases that hold
well together musically overlap, in practice this is often the
case. For instance the score of our benchmark fragment from
“La pia materna mano” contains 11 rests. All of them signal
the end of prosodic units; there are only 4 additional prosodic
units whose end does not coincide with a rest. For “Oh! Fede
negar potessi!” there is a rest in the score at the end of each of
the 22 prosodic units, and no other additional rest anywhere
else in the score.
While not all prosodic units are equally meaningful, each
of them is a standalone unit to a greater or lesser degree.
There exists evidence of the correlation between the structure
of a musical fragment or a piece of poetry and the kinds
of expressive variations which skilled musicians or actors
apply to the “plain” score or poem. In the case of a vocal
musical piece the structural information which the singer
tries to convey via expressive alterations has to do both
with the structure of the score as well as with that of the
libretto. For that reason we expect most of the expressive
patterns to be confined within such semantic units. Some of
them will nevertheless cross inter-prosodic boundaries when
needed to express higher-level semantic relationships. Given
this hypothesis, separating the arias into prosodic units will not
break most expressive patterns; equally important, it will not
interfere with the process of estimating the prediction accuracy
of a model when testing it over a prosodic unit which has been
extracted from an aria.
Once we have obtained the prosodic and the syntactic
units we experimented with various machine learning
algorithms to build our singer-specific models: SMOreg,
LinearRegression, AdditiveRegression, and
DecisionTable. SMOreg is a sequential minimal
optimization algorithm for training a support vector
regression model [21]. The specific regression problem
we are solving is to predict a duration ratio for each note
in the training set which is characteristic of the expressive
style of the singer it models. The LinearRegression
classifier uses the Akaike criterion for model selection [2].
AdditiveRegression is a meta classifier which improves
on a regression base classifier by iteratively fitting a model
to the residuals left by the classifier on the previous
iteration [31]. DecisionTable implements a simple
decision table majority classifier [33]. As reported in the
experimental section, it is not always the same learning
algorithm which works best for all singers.
All of the models are built using the leave-one-out evalu-
ation strategy. This method runs a cross-validation with the
number of folds equal to the number of input instances—in
our case the number of notes in the training set. Specifically,
for each singer we generate a number of models equal to the
number of musical units (prosodic or syntactic) —one model
for each set containing all of the musical units performed by
that singer except one. As we will see when we describe the
classification process, the important fact about this methodol-
ogy is that the test data (each of the units in turn) will not
have been used in training the model under evaluation.
In this work we are mainly interested in the quantitative
aspect of the singer-specific models. This aspect reflects the
accuracy of correctly classifying a new aria as being performed
by a specific singer, as opposed to the qualitative aspect which
reflects how well the expressive patterns of a specific singer
can be modeled. It is an important distinction to make because
of two reasons: (1) even a poor qualitative model may be able
to distinguish very efficiently between singers if it catches on
the aspects that differentiate them, and (2) it may be the case
that a good qualitative model is bad at distinguishing singers
because it has not learned those specific patterns. That is not
to say that the quality of the models is of no consequence, but
rather that there is no causality between the qualitative aspect
and the reliability of the quantitative results.
To perform the classification step we use “test set” eval-
uation of the two singer specific models for each of the
musical units (prosodic and syntactic). These models are
579
Authorized licensed use limited to: Aegean University. Downloaded on July 27,2010 at 15:02:54 UTC from IEEE Xplore.  Restrictions apply. 
obtained by training on the benchmark set from which the
unit under test was previously eliminated. As result we obtain
a correlation coefficient and a mean absolute error which
measure the degree to which the models predict the actual
performance of the musical unit. The performer of a musical
unit is correctly recognized if the correlation coefficient is
greater when testing against that singer’s model then against
the others’. By aggregating the correlation coefficients for all
of the musical unit/model pairs we can compute for each singer
the probability that he is going to be correctly recognized as
the actual performer.
IV. EXPERIMENTS
This section describes the recordings we use as benchmarks
and how we pre-process them for the learning step. It then
presents our classification results and comments on the cer-
tainty of classifying a musical fragment as being sung by a
specific singer.
A. The performance data
For our experiments we selected two different interpreta-
tions of the a cappella sections of the arias “La pia materna
mano” from La battaglia di Legnano and “Oh! Fede negar
potessi!” from Luisa Miller. The performers are Josep Carreras
and Placido Domingo. The first aria has 99 notes and the
second one 111; we are therefore analyzing a total of 420
notes. The manual splitting of the arias was done as follows.
“La pia materna mano” was split in 15 prosodic units; the
shortest unit contains 3 notes, the longest one 13, and the
average length is 6.6 notes. “Oh! Fede negar potessi!” was
split in 22 prosodic units; the shortest unit also contains 3
notes, the longest one 8, and the average length is 5.04 notes.
The alternative syntactic split was done straightforwardly by
forming units of 12-13 consecutive notes.
The recordings are evidently from commercial CDs, and
are done during life performances. Therefore there is no
way of communicating to the singer whether he should sing
bearing in mind a specific emotional intention, nor it is
possible to record the emotional content that he intended to
communicate. This may not seem like an issue given that our
goal is classifying singers by their expressive patterns and not
identifying specific emotional states. We nevertheless expect
to notice a connection between affective states and expressive
singing patterns, maybe more so in the context of classical
music where most of the musical structure is otherwise rigidly
predefined. If this hypothesis holds it would be better to only
compare interpretations which exhibit similar affective states.
The intuition is that training on the set of musical units which
form part of the same short fragment will capture patterns
that are specific to its affective state. These may, or may not,
be specific of the particular singer style per se; they will
nevertheless make it easier to identify the singer by criteria
which may not hold for other arias sung by him with a different
emotional intent. A more straightforward way to control over
these factors is to include in the benchmark set arias which
have a similar style, but a different affective feel. We are
currently extending our benchmark set to more effectively
control this factor in the future.
B. Results
Table I summarizes the classification results for each of
the four classifiers and for each of the two singers for the
case of the semantic split. The dataset contains 37 prosodic
units, 15 from “La pia materna mano” and 22 from “Oh! Fede
negar potessi!”. The experiment was done by (1) obtaining the
correlation coefficients when using each prosodic unit PUi to
tests the model obtained for each singer after eliminating PUi
from the training set, then (2) comparing them to decide which
of the two singers was recognized with greater probability. A
match between the actual and predicted singers is counted as
good (g), while a mismatch is a bad result (b) in the table.
We consider that negative or insignificantly small correlation
coefficients for both models tested on a prosodic unit render
the classification irrelevant; we therefore do not count these
results as either g nor b.
Classifier Carreras Domingo
SMOreg g: 21 b: 9 g: 20 b: 12
LinearRegression g: 17 b: 11 g: 17 b: 13
AdditiveRegression g: 17 b: 12 g: 19 b: 12
DecisionTable g: 20 b: 12 g: 20 b: 8
TABLE I
CLASSIFICATION FOR SEMANTIC SPLIT
Notice that allthough we tested 37 units the g and b numbers
never add up to 37; the remaining results were irrelevant. In
the case of using the DecisionTable classifier for testing
the model for Carreras against the prosodic units in “La
pia materna mano” the correlation coefficients were all zero.
SMOreg turns out to be the best overall learning method both
for Carreras and Domingo. Out of 30 valid (i.e. not irrelevant)
classification instances for Carreras and 32 for Domingo,
Carreras was correctly predicted 21 times, while Domingo
20. This gives a 66.12% percentage of correct predictions per
Carreras-Domingo pair. Carreras is correctly predicted 70% of
the time, a value significantly bigger than the baseline accuracy
(i.e. random choice) of 50%, and given that the models only
capture expressive alterations with regard to timing. Domingo
is correctly predicted 62.5% of the time. If we choose the
best classifier for each singer separately then Domingo is best
predicted by DecisionTable. In this case the percentage of
correct predictions per Carreras-Domingo pair is 70.68% and
Domingo is correctly predicted 71.42% of the time. Fig. 2
shows the curves for the note duration ratios in case of the
actual performances by Carreras and Domingo, as well as the
predictions of the two singer-specific models for the longest
prosodic unit.
Table II summarizes the classification results for each of the
four classifiers and for each of the two singers for the syntactic
split. The dataset consists of 17 units each containing 13 or 12
notes. The experiment and result counting was done similarly
to the case of the semantic split. As in the pevious case, when
580
Authorized licensed use limited to: Aegean University. Downloaded on July 27,2010 at 15:02:54 UTC from IEEE Xplore.  Restrictions apply. 
0
1
2
3
4
5
6
1 2 3 4 5 6 7 8 9 10 11 12 13
notes
st
re
tc
h
 f
ac
to
r
performed_D
predicted_by_D
predicted_by_C
performed_C
Fig. 2. Performed/Predicted note duration ratios for the longest prosodic unit
Classifier Carreras Domingo
SMOreg g: 7 b: 7 g: 8 b: 5
LinearRegression g: 6 b: 9 g: 10 b: 4
AdditiveRegression g: 9 b: 5 g: 9 b: 5
DecisionTable g: 9 b: 6 g: 9 b: 2
TABLE II
CLASSIFICATION FOR SYNTACTIC SPLIT
using the DecisionTable classifier for testing the model
for Carreras against the units in “La pia materna mano” the
correlation coefficients were all zero. Notice that in this case
there isn’t an single learning method which works best for both
Carreras and Domingo; AdditiveRegression works best
for Carreras. In the case of Domingo it is DecisionTable
which gives the best results. It is not apparent at this point
why testing the model for Carreras with this classifier gives
correlation coefficients of zero for “La pia materna mano”.
Playing on the cautious side we report prediction numbers for
the two first best methods for Domingo, DecisionTable
and LinearRegression. Out of 14 valid classification
instances for Carreras and 14/11 for Domingo, Carreras was
correctly predicted 9 times, while Domingo 10/9. When using
LinearRegression for Domingo we obtain a 67.85%
percentage of correct predictions per Carreras-Domingo pair. If
we consider DecisionTable instead, the percentage of cor-
rect predictions goes up to 72% per pair. Carreras is correctly
predicted 64.28% of the time while Domingo is correctly pre-
dicted 71.42% of the time when using LinearRegression
and 81.81% when using DecisionTable. Note that the
overall results for the two splitting policies are roughly the
same, slightly better for the syntactic approach. Nevertheless
Carreras has a higher correct prediction rate for the semantic
approach, while for Domingo the opposite holds. We suspect
this to be a result of their specific approaches of expressively
interpreting a musical piece and we are investigating this
hypothesis.
C. Precision and certainty of classification
For a given singer A the precision of classification can
be computed via dividing the number of correct predic-
tions for A by the total number of times when an aria
was classified as A. This includes the instances when the
prediction is A but the actual performance is by B. For
the syntactic split, units are falsely classified as Carreras 4
times when using LinearRegression and 2 times when
using DecisionTable. This leads to a precision of 69.23%
/ 81.81%. Domingo is misclassified 5 times and correctly
classified 10/9 times; the precision is therefore 66.66% with
LinearRegression and 64.28% with DecisionTable.
For the case of semantic split the precision of classification
for Carreras is 63.63% when using SMOreg for both singers.
When using DecisionTable for Domingo the precision
of classifying Carreras raises to 72.41%. The classification
precision for Domingo is 68.96%. Just like for the prediction
rates the classification precisions are comparable for the two
splitting policies, slightly better for the syntactic approach.
Note that the precision and the prediction rate for each singer
are inversely correlated with each other.
An important factor which we have not taken into account in
the classification process is the relative certainty with which
a model predicts that a test song is performed by singer A
vs. singer B. For instance, a model which predicts with 0.7
accuracy that the singer is A and with 0.6 that it is B is less
reliable than another which predicts 0.6 for B and 0.4 for A.
A high prediction rate is desirable but it must be correlated
with an acceptable rate of certainty that it reflects differences
in singers’ performing expressivity patterns.
One simplistic way to incorporate this factor is to compute,
for each model, the ratio between the number of correct and
incorrect predictions and choose the one with the larger value
to be the most-reliable. We can then simply use this model
to make all the predictions. A more realistic approach is to
compute the distance between the prediction coefficients for
the two interpretations of each musical unit (semantic/prosodic
and syntactic) and pick the most-reliable model for each of
the units. For lack of space we are not showing the most-
reliable models that correspond to each musical unit. Table III
summarizes the number of correct and incorrect predictions
(relative to the actual performers) for the classifiers that we
found most successful in the previous section. Their names
are abbreviated in the obvious way; the second column shows
the classifiers used for modeling Carreras. Similarly the third
column shows the classifiers for modeling Domingo. The
last two columns show the number of correct and incorrect
predictions made by the most-reliable models. The sum of
these two numbers for each line is the number of valid
predictions; for example the configuration on the third line
only predicts valid values for 16 out of the 17 syntactic units.
Remember that the musical units which we used to test the
singer-specific models are extracted from the two arias per-
formed by Carreras and Domingo. Under these circumstances
we know for a fact that if PUix was classified as C then the
581
Authorized licensed use limited to: Aegean University. Downloaded on July 27,2010 at 15:02:54 UTC from IEEE Xplore.  Restrictions apply. 
Fragm. type Classif-C Classif-D Corr. Incorr.
Prosodic Unit SMOreg SMOreg 18 13
Prosodic Unit SMOreg DecTbl 18 10
Synt. Unit AddReg LinReg 9 7
Synt. Unit AddReg DecTbl 9 6
TABLE III
CLASSIFICATION RESULTS WHEN USING THE most-reliable MODELS
corresponding unit PUiy will be classified as D, and vice-
versa. Given this observation, for the configuration on the first
line in Table III, 36 instances are correctly classified out of the
62 in total. The percentage of correct predictions is therefore
58.06%. Both Carreras and Domingo were correctly predicted
18 times out of 31, a 58.06% percent correct classification. For
the following three configurations in Table III the percentages
of correct predictions are 64.28%, 56.25%, and 60%.
V. CONCLUSIONS
This paper presents preliminary work on identifying famous
tenors via an approach which uses machine learning to build
singer-specific expressive models. The models are trained
on data consisting of high-level descriptors extracted from
commercial audio recordings and focus on expressive timing
variations. Our results show evidence that performers can
be automatically identified at a rate significantly better than
random choice. This is a promising result given that we have
exclusively focused on timing variations and the patterns that
make a singer clearly distinguishable may not gravitate equally
around timing across singers.
There are several directions for future work, some of them
already in progress. A larger set of arias is necessary to have
strong confidence in our results; this implies a scaling both in
the number of performers as well as in the number of arias per
performer. As we mentioned above, some singers may have a
distinguishable personal style due to other sound parameters
such as pitch or energy variations. We are therefore working
on extending the timing model with an energy model. We are
also exploring variations of strategies for splitting the arias
such as small vs. large number of notes for both semantic and
syntactic splits.
REFERENCES
[1] Adami, A.G.: “Modeling prosodic differences for speaker recog-
nition”, Speech Communication, 49:4, 2007.
[2] Akaike, H.: “A new look at the statistical model identification”,
IEEE Trans. on Automatic Control, 19:6, 1974.
[3] Alonso, M.: “Expressive performance model for a singing voice
synthesizer”, 2005.
[4] Arcos, J.L., Lopez de Mataras, R.: “An interactive case-based
reasoning approach for generating expressive music”, Applied
Intelligence, 14:1, 2001.
[5] Bresin, R.: “Artificial neural networks based models for auto-
matic performance of musical scores”, Journal of new music
research, 27:3, 1998.
[6] Bresin, R.: “Articulation rules for automatic music performance”,
ICMC, 2001.
[7] Bresin, R.: “Virtual Visrtuosity: Studies in Automatic Music
Performance”, PhD Thesis, KTH, 2000.
[8] Canazza, S.; Poli, G. De; Roda, A.; Vidolin, A.: “Analysis by
synthesis of the expressive intentions in musical performance ”,
ICMC, 1997.
[9] Friberg, A., Sundberg, J., Frydn L.: “Music from motion: Sound
level envelopes of tones expressing human locomotion”, Journal
of New Music Research, 29:3, 2000.
[10] Friberg, A., Bresin, R., Frydn, L., Sundberg, J.: “ Musical
punctuation on the microlevel: Automatic identification and
performance of small melodic units ”, Journal of New Music
Research, 27:3, 1998.
[11] Friberg, A.: “A Quantitative Rule System for Musical Perfor-
mance”, Doctoral Disertation, KTH, 1995.
[12] Friberg, A. ,Schoonderwaldt, E., Juslin, P., Bresin, R.: “Au-
tomatic Real-Time Extraction of Musical Expression”, ICMC,
2002.
[13] Gabrielsson, A.: “The performance of Music”, In D.Deutsch
(Ed.), The Psychology of Music (2nd ed.), Academic Press, 1999.
[14] Lopez de Mantaras, R., Arcos, J.L.: “AI and music, from
composition to expressive performance”, AI Magazine, 23:3,
2002.
[15] Narmour, E.: “The Analysis and Cognition of Basic Melodic
Structures: The Implication / Realization Model”, Chicago: Uni-
versity of Chicago Press, 1990.
[16] Ramirez, R., Hazan, A., Gómez, E., Maestre, E.: “Under-
standing Expressive Transformations in Saxophone Jazz Perfor-
mances”, Journal of New Music Research, 34:4, 2005.
[17] Ramirez, R., Hazan, A., Maestre, E., Serra, X.: “A Data Mining
Approach to Expressive Music Performance Modeling, in Multi-
media Data mining and Knowledge Discovery”, Springer.
[18] Repp, B.H.: “Diversity and Commonality in Music Perfor-
mance: an Analysis of Timing Microstructure in Schumann’s
‘Traumerei’ “, Journal of Acoust. Soc. of America, 104, 1992.
[19] Seashore, C.E. (ed.): “Objective Analysis of Music Perfor-
mance”, University of Iowa Press, 1936.
[20] Serra, X., Smith, S.: “Spectral Modeling Synthesis: A Sound
Analysis/Synthesis System Based on a Deterministic plus
Stochastic Decomposition”, Computer Music Journal, 14:4, 1990.
[21] Smola, A.J., Schoelkopf, B.: “A tutorial on support vector
regression”, NeuroCOLT2 TR Series, 1998.
[22] Todd, N.: “The Dynamics of Dynamics: a Model of Musical
Expression”, Journal of Acoust. Soc. of America, 91, 1992.
[23] Widmer, G.: “Machine Discoveries: A Few Simple, Robust
Local Expression Principles”, Journal of New Music Research,
31:1, 2002.
[24] Witten, I.H., Frank, E.: ”Data Mining: Practical machine learn-
ing tools and techniques”, 2nd Edition, Morgan Kaufmann, San
Francisco, 2005.
[25] Zanon, P., Widmer, G.: “Recognition of famous pianists using
machine learning algorithms: first experimental results”, OEFAI-
TR-2003-01, 2003.
[26] Stamatos, E., Widmer, G.: “Automatic identification of music
performers with learning ensemble”, Artif. Intell., 165:1, 2005.
[27] Saunders, C., Hardoon, D., Shawe-Taylor, J., Widmer, G.:
“Using string kernels to identify famous performers from their
playing style”, ECML, 2004.
[28] Ramirez, R., Perez, A., Kersten, S.: “Performer identification in
Celtic violin recordings”, ISMIR, 2008.
[29] Doddington, G.: “Speaker recognition identifying people by
their voices”, Proceedings of IEEE, 73:11, 1985.
[30] Reynolds, D.A., Rose, R.C.: “Robust text-independent speaker
identification using Gaussian mixture speaker models”, IEEE
Trans. Speech and Audio Proc., 3:1, 1995.
[31] Friedman, J.H.: “Stochastic Gradient Boosting”, Tech. Report
Stanford University, 1999.
[32] Gish, H., Schmidt, M., Mielke, A.: “A robust, segmental method
for text-independent speaker identification”, Proc. ICASSP, 1994.
[33] Kohavi, R.: “The power of decision tables”, Proc. ECML, 1995.
582
Authorized licensed use limited to: Aegean University. Downloaded on July 27,2010 at 15:02:54 UTC from IEEE Xplore.  Restrictions apply. 
