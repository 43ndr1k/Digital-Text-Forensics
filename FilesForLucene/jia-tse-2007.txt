Handwritten and Typewritten Word and Character
Separation in Unconstrained Document Images
Jia Tse, Dean Curtis, John Bunch, Christopher Jones,
E.A Yfantis, and Aaron Thomas
Department Computer Science
University of Nevada Las Vegas
Las Vegas, NV, USA
Abstract Separating handwritten and typewritten text
within unconstrained paper documents can provide more
accurate and efficient OCR results. This paper presents
a technique developed that can isolate both the typewrit-
ten and handwritten portions of a document image. The
classification between handwritten and typewritten text
occurs at both the character and the word level. Char-
acters are grouped into words using a word separation
technique with an ‚Äúisland‚Äù grouping method. Struc-
tural features of handwritten and typewritten characters
are examined. The method developed correctly identi-
fied with probability close to 100% of the total number
of typewritten words in a set of 30 handwritten docu-
ments.
Keywords: Document Image Analysis, OCR, word sep-
aration
1 Introduction
In performing OCR in unconstrained document im-
ages, it is important to make a distinction between
handwritten and typewritten text because both re-
quire its own OCR engines. Techniques for type-
written and handwritten word classification can oc-
cur at three basic levels: the line level, the word
level, and the character level. More literature was
found on separation that occurs at the line level.
Kavallieratou et al analyzed the horizontal profile
of a text line [1]. They observed that typewritten
characters on a line have a relatively stable height
whereas the height of each character may vary in
handwritten text lines. Classification was based
on discriminant analysis. An average accuracy of
98.2 % was achieved. Fan et al focused on using
a word block layout to identify typewritten text
lines [2]. They observed that typewritten text lines
are written straight, such that a line connecting the
center low point of each word is fairly straight. An
accuracy of 86 % was achieved.
Separation has also been performed on the word
level [3]. A total of 31 features were extracted,
and trained Fisher classifiers were used to identify
typewritten and handwritten text. A Markov Ran-
dom Field approach was used to rectify misclas-
sified words. An accuracy of 98 % was achieved.
It was noted that the mean and variance of the
width of each character is consistent in typewrit-
ten words. Also characters of typewritten words
are less likely to overlap than handwritten words.
Another method analyzed the vertical projection
profile of the word. Since handwritten characters
may be touching each other, peaks and valleys of
the connected components will not be easily identi-
fied. A Hidden Markov Model was used to classify
typewritten and handwritten words based on ver-
tical projection profiles [4]. This method achieved
a precision of 92.86 %.
Analysis on the character level is relatively more
difficult since less information is available. One
method analyzed physical features such as the
straightness of lines, and character symmetry [5].
Individual character classification reached an accu-
racy of 78.5 %. The method we developed is im-
plemented at the character level similar to [5]. In
unconstrained document images, words may appear
in arbitrary locations. Separation at the character
level allows for classification to occur regardless of
word location or arrangement. This is our motiva-
tion for analysis on the character level.
Our method assumes that all invariant features
have been removed from the document image. This
research is performed for document recognition and
indexing applications for the Department of En-
ergy. Our method is implemented after the doc-
ument feature extraction has been performed [6].
The algorithm is implemented on an image that
contains only handwritten and typewritten text.
The text is then grouped into islands and each char-
acter is analyzed.
Our method was tested on a corpus of 30 doc-
ument images selected from the medical record re-
search project for the Department of Energy. Of all
the typewritten words in these documents, our sys-
tem correctly identified close to 100% of the type-
written words.
This paper is organized as follows. Section 2 will
be the algorithm used in island grouping. Then we
will present our algorithm in section 3 and describe
the three features we used. Finally, we will conclude
in section 4.
2 Island Grouping
After all features have been removed as discussed
in [6], the only objects contained in the image are
words. The first step is to isolate lines of words
that satisfies the following constraints:
a) The line consists of letters of the same font and
size
b) The letters are linearly aligned so that all let-
ters are on the same horizontal line
To do this, we first find the connected components
of the image. Then the letter components are
sorted based on the left most coordinate of each
component. Finally, the characters are grouped
based on two conditions:
a) Linearly centered proximity to account for ver-
tical orientation
b) Left and right threshold for spans of empty
space between segments
Each character that passes this test is merged into
the corresponding section to which it belongs, and
these groups ultimately become the islands. After
the completion of this algorithm, the result is a list
of islands which will then be used for word separa-
tion.
3 Typewritten word isolation
We begin classification on the character level, using
the word islands obtained in section 2. Three main
features of typed characters are extracted from the
connected component: 1)horizontal and vertical
lines 2)the existence of a loop and loop symmetry
and 3)character symmetry. The existence of one of
the three above properties is sufficient to classify
all the typed uppercase and lowercase characters of
the English alphabet.
Poor quality images may alter the effectiveness
of feature extraction. To counter this potential er-
roneous effect, we further apply classification on the
word level based on a weighted probability. Each
character of the word is classified as either hand-
written or typewritten. Based on the strict nature
of the rules in identifying typewritten characters,
a ratio 3:5 of typewritten to handwritten charac-
ters is sufficient to classify the word as typewritten.
The weights are based on experiments that show a
67% confidence in typewritten classification. So,
given a count of the number of typewritten com-
ponents found in an island (T ) versus the count of
the number of handwritten components found in an
island(H), the island‚Äôs classification C is
C =
{
t .63(T ) ‚â• .37(H)
h .63(T ) < .37(H)
(1)
where t is the classification for typewritten words
and h is the classification for handwritten words.
3.1 Horizontal and Vertical Line De-
tection
We define vertical and horizontal lines as lines
that stretch across the entire connected compo-
nent (fig 1). By observation, it is difficult and un-
likely to hand write vertical/horizontal lines that
extend the entire height/width of the component.
By requiring lines to extend the entire length of
the component, we are strengthening our classifi-
cation method. To find the lines, we find (1) the
thickness of the vertical/horizontal segments of the
character, and (2) the vertical/horizontal projec-
tion profiles of the character. Since the methods
for determining vertical and horizontal features are
similar, from this point forward, we will only focus
on the vertical features.
3.1.1 Line Thickness
Vertical line thickness and horizontal line thickness
may be different in typed characters, and must be
dealt with separately, but using the same approach.
To determine the vertical line thickness in pixels,
the character image is thinned using the method
described in [7]. Every pixel at coordinate P (x,y)
of the thinned image is scanned for the vertical line
property:
‚àÉq1q2Np|(q1y = Py ‚àí 1 ‚àß q1x = Px) ‚àß
(q2y = Py + 1 ‚àß q2x = Px) (2)
where q1 and q2 are black pixels of the thinned
image and Np is the neighborhood set of pixels of P .
If P satisfies the vertical line property, horizontal
scans of the original image will originate from point
P and travel outwards until a white pixel is found.
The thickness is the sum in pixels of the left and
right scans.
The quality of the thinner may cause arbitrary
pixels to satisfy the vertical line property. However,
this is both arbitrary and inconsistent, so thickness
values associated with false vertical lines will de-
viate. True vertical lines are uniform in thickness,
and stretches the entire height of the component.
Every pixel of the line will return a consistent thick-
ness value. Therefore the mode of all vertical thick-
ness values calculated represents the correct thick-
ness of the vertical line.
3.1.2 Projection Profile
A vertical projection profile combined with the ver-
tical line thicknesses is used to identify the exis-
tence of vertical lines in the character (fig 1). For a
line to be considered a vertical line, it must satisfy
the following two properties:
a) All pixels of the line must stretch the height of
the character.
b) The thickness of the line must be equal to the
thickness found in 3.A.1.
Figure 1: Horizontal and vertical line detection.
3.2 Loop Detection and Loop Sym-
metry Analysis
Observations indicate typewritten characters con-
sisting of loops have loops that are highly symmet-
rical. Our method takes into consideration loops
(a) Case I (b) Case II
Figure 2: Vertical and Horizontal loop symmetry
Figure 3: Traversing pattern for loop detection.
that have point symmetry as well as line symmetry,
such as the ‚ÄòD‚Äô and the ‚ÄòB‚Äô. This eliminates hand-
written characters that may possess solely of point
symmetry(fig 2). This aspect strengthens our clas-
sification method. If an upper or lower case char-
acter (except lower case ‚Äòa‚Äô) possess one or more
loops, they are either horizontally symmetrical or
vertically symmetrical, or both. Our approach is
sufficient in classifying these characters. We calcu-
late loop symmetry by finding (1) the existence of
a loop and (2) vertical and horizontal line symme-
try levels with respect to a vertical/horizontal line
drawn at its center of mass.
3.2.1 Loop Detection
To detect the existence of a loop, we first thin the
character image. The image is scanned from left to
right until it twice crosses from a white to a black
pixel. The two crossing points represent potential
loop boundaries. To verify the existence of the loop,
we pick one of the points and trace its path around
the loop until it reaches itself. If a path cannot be
found that begins and ends at the same starting
point, the loop does not exist.
The tracing algorithm works as follows (fig 3):
Assume P is the previous pixel of the trace and C
is the current pixel. N represents possible next
pixels along the tracing path. Starting from P ,
we traverse C‚Äôs neighbors in a counter-clockwise
manner until it reaches an N . N becomes the new
C , C becomes P and the process repeats.
3.2.2 Loop Symmetry levels
Using the loop detected in section 3.2.1, we find
its center of mass. Next we find the vertical loop
(a) Case I (b) Case II
Figure 4: Vertical and Horizontal character sym-
metry
symmetry level with respect to a vertical line drawn
at the center of mass. The same is later done for
horizontal symmetry. Let P (Xc, Yc) be the center
of mass of loop L, and p be a point in L.
‚àÄpL : p(x, y) =
{
1 if(2XC ‚àí x, y) is black
0 otherwise (3)
The vertical symmetry level Vsymis defined as:
Vsym =
1
N
N‚àë
i=0
pi(x, y) (4)
where N is the total number of black pixels in L.
3.3 Character Symmetry Analysis
Character symmetry analysis was implemented in
[5]. However, like loop analysis [5] extracted point
symmetries as features. We noted that only the
characters ‚ÄòH‚Äô, ‚ÄòI‚Äô, ‚ÄòO‚Äô, ‚ÄòS‚Äô, and ‚ÄòX‚Äô have point sym-
metry. We combine the point symmetry approach
with line symmetry to identify additional charac-
ters such as ‚ÄòM‚Äô and ‚ÄòD‚Äô. Handwritten characters
are unlikely to be written perfect enough on a con-
sistent basis to impact the identification of type-
written words.
For vertical line symmetry levels, we reflect the
character image over a vertical line drawn at its
center of mass. For point symmetry, we follow [5]‚Äôs
method of reflecting each pixel with respect to its
center of mass. Fig 4 shows some characters with
vertical and horizontal symmetry.
4 Conclusion
We perform handwritten and typewritten word sep-
aration on a total of 30 document images. An aver-
age of 93.2% of the typewritten words of the image
was classified correctly. It falsely identified 17.5% of
the handwritten words as typewritten words. Each
form has an average of a total of 123 typewritten
words and 49 handwritten words.
Our classification techniques are especially vul-
nerable to italicized and moderately smeared
words. This is precisely the drawback of this al-
gorithm. The advantage of our approach is its sim-
plicity. We were able to achieve competitive results
by using the combination of a word and character
level approach.
Our future work will be to improve handwritten
and typewritten text separation by incorporating
the use of grey scale document images. Grey scale
images contain more data than binary images and
we plan on using the extra knowledge to our ad-
vantage.
References
[1] E. Kavallieratou and S. Stamatatos, ‚ÄúDiscrim-
ination of Machine-Printed from Handwrit-
ten Text Using Simple Structural Characteris-
tics,‚Äù 17th International Conference on Pattern
Recognition - Volume 1, 2004.
[2] K.C. Fan, L.S. Wang, and Y.T. Tu, ‚ÄúClassi-
fication of Machine- Printed and Handwritten
Texts Using Character Block Layout Variance,‚Äù
Pattern Recognition, vol. 31, no. 9, 1998.
[3] Y. Zheng, H. Li, and D. Doermann. ‚ÄúMa-
chine printed text and handwriting identifica-
tion in noisy document images.‚Äù Technical re-
port, LAMP Lab, UMD, College Park, 2002.
[4] J.K. Guo and M.Y. Ma, ‚ÄúSeparating Handwrit-
ten Material from Machine Printed Text Using
Hidden Markov Models,‚Äù Proc. Intl Conf. Doc-
ument Analysis and Recognition, 2001.
[5] K. Kuhnke, L. Simoncini, and Z.M. KovaÃÅcs-
V, ‚ÄúA System for Machine-Written and Hand-
Written Character Distinction,‚Äù Proc. Int‚Äôl
Conf. Document Analysis and Recognition,
1995.
[6] J. Adams, E.A. Yfantis, D. Curtis and T. Pack,
‚ÄúFeature Extraction Methods for Form Recog-
nition Applications,‚Äù WSEAS trans. on Infor-
mation Science and Applications, Issue 3, Vol-
ume 3 March 2006 Pages 666-671.
[7] M. Ahmed and R. Ward, ‚ÄúA Rotation Invari-
ant Rule-Based Thinning Algorithm for Char-
acter Recognition,‚Äù IEEE Transactions on Pat-
tern Analysis and Machine Intelligence, v.24
n.12, p.1672-1678, 2002.
