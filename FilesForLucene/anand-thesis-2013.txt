Indexing Methods
for
Web Archives
Dissertation
zur Erlangung des Grades
Doktor der Ingenieurwissenschaften (Dr.-Ing.)
der Naturwissenschaftlich-Technischen Fakultät I
der Universität des Saarlandes
Avishek Anand
Max-Planck-Institut für Informatik
Saarbrücken
2013
ii
Dekan der
Naturwissenschaftlich-Technischen
Fakultät I Prof. Mark Groves
Berichterstatter Dr.-Ing. Klaus Berberich
Berichterstatter Prof. Dr.-Ing. Gerhard Weikum
Berichterstatter Prof. Dr.-Ing. Kjetil Nørvåg
iv
Eidesstattliche Versicherung
Hiermit versichere ich an Eides statt, dass ich die vorliegende Arbeit selbstständig und
ohne Benutzung anderer als der angegebenen Hilfsmittel angefertigt habe.
Die aus anderen Quellen oder indirekt übernommenen Daten und Konzepte sind unter
Angabe der Quelle gekennzeichnet.
Die Arbeit wurde bisher weder im In- noch im Ausland in gleicher oder ähnlicher Form
in einem Verfahren zur Erlangung eines akademischen Grades vorgelegt.
Saarbrücken, den 02.07.2013
(Avishek Anand)
vi
Acknowledgements
First and foremost, I would like to thank Prof. Gerhard Weikum for giving me the
opportunity to pursue this thesis under him and his timely and valuable inputs. I would
like to express my sincere gratitude for his support and guidance.
A special note of thanks to Dr. Klaus Berberich and Prof. Srikanta Bedathur. Their
contribution over the past years has been invaluable and I have learned a lot from them.
I thank them for their persistent support and patience through many discussions we
had through the course of the thesis.
I am very thankful to my parents and friends for the continued emotional support
which in the past few years has often proven to be the deciding factor for my successes.
Last but not the least I would like to thank my wife, Megha, for always being there and
putting up with me during good times and bad. Her being there has made this a journey
to cherish.
vii
viii
Abstract
There have been numerous efforts recently to digitize previously published content and
preserving born-digital content leading to the widespread growth of large text reposi-
tories. Web archives are such continuously growing text collections which contain ver-
sions of documents spanning over long time periods. Web archives present many op-
portunities for historical, cultural and political analyses. Consequently there is a grow-
ing need for tools which can efficiently access and search them.
In this work, we are interested in indexing methods for supporting text-search work-
loads over web archives like time-travel queries and phrase queries. To this end we make
the following contributions:
• Time-travel queries are keyword queries with a temporal predicate, e.g., “mpii
saarland” @ [06/2009], which return versions of documents in the past. We in-
troduce a novel index organization strategy, called index sharding, for efficiently
supporting time-travel queries without incurring additional index-size blowup.
We also propose index-maintenance approaches which scale to such continuously
growing collections.
• We develop query-optimization techniques for time-travel queries called partition
selection which maximizes recall at any given query-execution stage.
• We propose indexing methods to support phrase queries, e.g., “to be or not to be
that is the question”. We index multi-word sequences and devise novel query-
optimization methods over the indexed sequences to efficiently answer phrase
queries.
We demonstrate the superior performance of our approaches over existing methods
by extensive experimentation on real-world web archives.
ix
Abstract
x
Kurzfassung
In der jüngsten Vergangenheit gab es zahlreiche Bemühungen zuvor verffentlichte In-
halte zu digitalisieren und elektronisch erstellte Inhalte zu erhalten. Dies führte zu
einem weit verbreitenden Anstieg groer Textdatenbestände. Webarchive sind eine solche
Art konstant ansteigender Textdatensammlung. Sie enthalten mehrere Versionen von
Dokumenten, welche sich über längere Zeiträume erstrecken. Darüber hinaus bieten sie
viele Möglichkeiten für historische, kulturelle und politische Analysen. Infolgedessen
gibt es einen wachsenden Bedarf an Werkzeugen, die eine effiziente Suche in Webarchi-
ven und einen effizienten Zugriff auf die Daten erlauben.
Der Fokus dieser Arbeit liegt auf Indexierungsverfahren, um die Arbeitslast von Text-
suche auf Webarchiven zu unterstützen, wie zum Beispiel time-travel queries oder phrase
queries. Zu diesem Zweck leisten wir folgende Beiträge:
• Time-travel queries sind Suchwortanfragen mit einem temporalen Prädikat. Zum
Beispiel liefert die Anfrage “mpii saarland” @ [06/2009] Versionen des Doku-
ments aus der Vergangenheit als Ergebnis. Zur effizienten Unterstützung solcher
Anfragen ohne die Indexgröe aufzublasen, stellen wir eine neue Strategie zur Or-
ganisation von Indizes dar, so genanntes index sharding. Des Weiteren schlagen wir
Wartungsverfahren für Indizes vor, die für solch konstant wachsende Datensätze
skalieren.
• Wir entwickeln Techniken zur Anfrageoptimierung von time-travel queries, nach-
stehend partition selection genannt. Diese maximieren den Recall in jeder Phase der
Anfrageverarbeitung.
• Wir stellen Indexierungsmethoden vor, die phrase queries unterstützen, z. B.
“Sein oder Nichtsein, das ist hier die Frage”. Wir indexieren Sequenzen beste-
hend aus mehreren Wörtern und entwerfen neue Optimierungsverfahren für die
indexierten Sequenzen, um phrase queries effizient zu beantworten.
xi
Kurzfassung
Die Performanz dieser Verfahren wird anhand von ausführlichen Experimenten auf
realen Webarchiven demonstriert.
xii
Contents
Abstract ix
Kurzfassung xi
1. Introduction 1
1.1. Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1
1.2. Research Challenges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2
1.3. Contributions and Publications . . . . . . . . . . . . . . . . . . . . . . . . . 5
1.4. Outline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
2. Foundations and Technical Background 7
2.1. Web Archiving . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
2.2. Data Management . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
2.2.1. Temporal Databases . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
2.2.2. Synopsis Structures . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
2.3. Information Retrieval . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
2.3.1. Retrieval Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12
2.3.2. Indexing Text . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
2.3.3. Indexing Archives . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
3. Efficient Indexing and Maintenance for Time-Travel Text Search 27
3.1. Motivation and Problem Statement . . . . . . . . . . . . . . . . . . . . . . . 27
3.1.1. Approach . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
3.1.2. Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
3.1.3. Organization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30
3.2. Model and Index Organization . . . . . . . . . . . . . . . . . . . . . . . . . 30
xiii
Contents
3.3. Sharding Posting Lists . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32
3.3.1. Query Processing over Sharded Index . . . . . . . . . . . . . . . . . 33
3.4. Idealized Index Sharding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34
3.4.1. Optimal Algorithm for Idealized Sharding . . . . . . . . . . . . . . 36
3.4.2. Proof of Optimality . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37
3.5. Cost-Aware Merging of Shards . . . . . . . . . . . . . . . . . . . . . . . . . 39
3.5.1. Model for Shard Merging . . . . . . . . . . . . . . . . . . . . . . . . 39
3.5.2. Algorithm for Shard Merging . . . . . . . . . . . . . . . . . . . . . . 42
3.6. Index Maintenance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43
3.7. Incremental Sharding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45
3.7.1. Incremental Sharding Algorithm . . . . . . . . . . . . . . . . . . . . 47
3.7.2. Approximation Guarantee for Incremental Sharding . . . . . . . . 49
3.8. System Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51
3.9. Experimental Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52
3.9.1. Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53
3.9.2. Datasets Used . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53
3.9.3. Index Management . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54
3.9.4. Query Workloads and Execution . . . . . . . . . . . . . . . . . . . . 55
3.10. Experimental Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55
3.10.1. Sharding vs Vertical Partitioning . . . . . . . . . . . . . . . . . . . . 55
3.10.2. Effect of Coalescing . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58
3.10.3. Comparing Sharding Approaches . . . . . . . . . . . . . . . . . . . 59
3.10.4. Index Sizes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62
3.10.5. Index Maintenance Performance . . . . . . . . . . . . . . . . . . . . 63
3.11. Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66
3.12. Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67
4. Query Optimization for Approximate Processing of Time-Travel Queries 69
4.1. Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69
4.1.1. Motivation and Problem Statement . . . . . . . . . . . . . . . . . . 69
4.1.2. Approach . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70
4.1.3. Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72
4.1.4. Organization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72
4.2. Index Organization and Query Processing . . . . . . . . . . . . . . . . . . . 72
4.3. Single-Term Partition Selection . . . . . . . . . . . . . . . . . . . . . . . . . 75
4.3.1. Optimal Algorithm for Single-Term Partition Selection . . . . . . . 76
4.3.2. Approximation Algorithm . . . . . . . . . . . . . . . . . . . . . . . . 79
4.4. Multi-Term Partition Selection . . . . . . . . . . . . . . . . . . . . . . . . . . 81
xiv
Contents
4.4.1. GREEDYSELECT for Multi-term Selection . . . . . . . . . . . . . . . 82
4.5. System Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 87
4.6. Practical Issues . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 88
4.6.1. Dealing with Partition and Query Boundary Alignment . . . . . . 88
4.6.2. I/O Budget Underflow . . . . . . . . . . . . . . . . . . . . . . . . . . 88
4.7. Experimental Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 89
4.7.1. Evaluation Framework . . . . . . . . . . . . . . . . . . . . . . . . . . 89
4.7.2. Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 89
4.7.3. Datasets Used . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 89
4.7.4. Query Workload . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 90
4.7.5. Index Management . . . . . . . . . . . . . . . . . . . . . . . . . . . . 90
4.7.6. Evaluation Methodology . . . . . . . . . . . . . . . . . . . . . . . . 92
4.7.7. Performance of Partition Selection . . . . . . . . . . . . . . . . . . . 92
4.7.8. Query-Processing Performance . . . . . . . . . . . . . . . . . . . . . 96
4.7.9. Impact of using Synopses . . . . . . . . . . . . . . . . . . . . . . . . 99
4.7.10. Impact of Partition Granularity . . . . . . . . . . . . . . . . . . . . . 99
4.8. Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103
4.9. Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103
5. Phrase Indexing and Querying 105
5.1. Motivation and Problem Statement . . . . . . . . . . . . . . . . . . . . . . . 105
5.1.1. Approach . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 106
5.1.2. Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107
5.1.3. Organization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107
5.2. Model and Index Organization . . . . . . . . . . . . . . . . . . . . . . . . . 108
5.3. Indexing Framework . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 109
5.4. Query Optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 110
5.4.1. Optimal Solution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 113
5.4.2. Approximation Guarantee . . . . . . . . . . . . . . . . . . . . . . . . 118
5.5. Phrase Selection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 119
5.5.1. Query-Optimizer-Based Phrase Selection . . . . . . . . . . . . . . . 120
5.5.2. Coverage-Based Phrase Selection . . . . . . . . . . . . . . . . . . . . 122
5.5.3. Optimizations for Practical Indexing . . . . . . . . . . . . . . . . . . 124
5.6. System Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 126
5.7. Experimental Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 127
5.7.1. Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 127
5.7.2. Datasets Used . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 128
5.7.3. Query Workload . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 128
xv
Contents
5.7.4. Index Management and Competitors . . . . . . . . . . . . . . . . . 128
5.7.5. Performance of Query Optimization . . . . . . . . . . . . . . . . . . 131
5.7.6. Effect of Phrase Selection . . . . . . . . . . . . . . . . . . . . . . . . 132
5.8. Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 136
5.9. Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 138
6. Conclusions 139
6.1. Summary of Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 139
6.2. Outlook on Future Directions . . . . . . . . . . . . . . . . . . . . . . . . . . 140
Bibliography 143
A. Appendix 155
List of Figures 159
List of Tables 161
List of Algorithms 163
xvi
1
Introduction
1.1. Motivation
With the popularity and ubiquity of the Internet our digital presence and footprint has
been growing at a rapid pace. This has resulted in numerous digital curation efforts
which are believed to lead to cultural preservation [Con10]. Curation in the form of
preservation of digital information deals with archiving born-digital content like Web
archives and news archives. Web archives such as Internet Archive [http://www.arch-
ive.org] and Internet Memory [http://www.internetmemory.org] have been involved
in periodically archiving websites for over 17 years with collection sizes amounting
to several terabytes of data. Similarly, many news companies such as The New York
Times [NYT13], Wall Street Journal [WSJ13], The Times [TIM13] archive both their pub-
lished digital content as well as digitize non-digital articles. Digitization for preserva-
tion [KUL], apart from newspapers, also targets other content generated before the Web
era in form of digitized books [Coy06], a prominent example being the Million Books
Project [MBP13]. All these efforts have given rise to huge repositories of text data which
span considerable time periods. In this work we collectively refer to these collections as
web archives.
Cultural preservation is the first step, but the true potential of these collections can be
realized by enabling efficient searching and mining tasks. Web archives present many
opportunities for various kinds of historical analyses [SSU08, HER13], cultural analy-
ses [MSA+10], and analytics for computational journalism [CHT11, CLYY11]. A case in
point is data-driven journalism where the user resorts to various search tasks, visual-
ization and longitudinal analyses on large amounts of data for obtaining useful insights
in order to construct stories. As an example, The Weyeser Explorer [http://www.data-
art.net/weyeser explorer] provides visualizations of word clusters from news articles
from The Guardian [http://www.guardian.co.uk/]. In this work we are interested in
1
Chapter 1. Introduction
indexing methods for supporting various text-search workloads over web archives.
With the popularity of search engines, full-text search has evolved into a powerful
and popular way of searching text collections. However, search is no longer limited to
humans typing queries. It is increasingly being used as a “primitive” operation in pre-
processing steps in a longer pipeline for various extraction, mining and analytics tasks.
Entity extraction systems like [ACCG08, KN10] commonly employ techniques based on
keyword search as the first step. Similarly, many text mining and analytics applications
like [MTB+10, SBBW10] use text search as a filtering step to focus on a smaller and hence
more usable document collection. To realize these benefits of search over archives it
would be important to support keyword search functionality with additional temporal
predicates.
What has also evolved with the wide usage of search engines is the manner in which
users interact with it. The user behaviour relies on multiple query reformulations and
expansions to refine query intentions. A quick approximation of the query results often
serves as feedback for further reformulations. Thus, to enhance the current day user
behaviour it is important to provide functionality by which a large subset of the query
results can be determined quickly.
Along with keyword queries, phrase queries are another important query type in text
search. Although a small fraction of the queries issued to search engines are phrase
queries, a fairly large number are implicitly invoked, for instance, by means of query-
segmentation methods [HPBS12, LHZW11]. Beyond their usage in search engines, phra-
se queries increasingly serve as a building block for other applications such as (a) pla-
giarism detection [Sta11] (e.g., to identify documents that contain a highly discriminative
fragment from the suspicious document), or (b) culturomics [MSA+10](e.g., to identify
documents that contain a specific n-gram and compute a frequency time-series from
their timestamps).
1.2. Research Challenges
In this section, motivated from the scenarios presented above, we introduce the three
different workloads we address in this thesis : (A) time-travel queries, (B) approximate
queries and (C) phrase queries. We illustrate these by providing potential usecases for
each of the workloads and consider the corresponding research challenges in designing
efficient indexing and query processing methods.
(A) In spite of the progresses in preservation, search capabilities over archives have
been limited. A Naı̈ve adaptation of the indexing infrastructure used for text re-
trieval is expensive and does not capture the temporal dimension inherent to such
2
1.2. Research Challenges
archives. With this in mind, time-travel queries, which combine temporal predi-
cates with keyword queries, e.g., “fifa world cup” @ [06/2006 - 07/2006], were
proposed in [BBNW07] as an effective way of searching collections.
This combination of keyword queries with a temporal context could be an attrac-
tive construct in various analytical and comparative longitudinal analyses. Con-
sider a journalist interested in views about the recent ponzi scheme in 2008. A
keyword query “ponzi scheme” without the temporal predicates over an archive
might result in articles about various ponzi schemes spanning the entire time-line
of the archive. With the proper temporal constraints she would be able to restrict
the search to time-intervals of her interest.
To answer time-travel queries [BBNW07] propose indexes that incur an index-
size blowup, by replicating parts of the index, for better query performance. This
motivates :
RC I : How do we build index structures which eliminate wasteful replication so as to have
smaller index sizes and support time-travel queries efficiently?
Current indexing techniques are agnostic to index-access costs which can make a
considerable difference to retrieval efficiency.
RC II : Can we devise index-tuning methods which take into account index-access costs
rather than abstract cost measures ?
Web archives are usually in a state of change where content is continuously added.
The index needs to be in a fresh state and consistently reflect these changes. The
current state-of-art indexing techniques are limited to static collections and do not
consider updates. We need index-maintenance strategies which do not compro-
mise query efficiency.
RC III : Can we design indexes which can be efficiently maintained ?
(B) Web archives are often associated with redundant information due to periodic re-
crawls. Also in many application domains like news articles, the same information
is available from different documents, so missing a few of them could be accept-
able. Similarly, a subset of the true results is usually good enough for quickly
checking if content or temporal predicates of the query need to be adapted. Con-
sider a historian interested in the former US president “George Bush” in 2005.
He might go through a series of reformulations to clarify his intent from “bush”
@ [2005] to “george bush” @ [2005] to “george bush senior” @ [2005]. After
the first query, he might have results relating to the infamous bush fires in 2005
prompting him a second round of reformulation. The second round might be
3
Chapter 1. Introduction
unsuccessful due to mentions of the son of the actual entity he is interested in,
thus necessitating a third reformulation. A quick review of the results (partially
computed thus far) in each of these rounds would allow him for a productive re-
formulation experience.
Due to the sheer size of archives, processing the temporal queries is expensive.
This hampers the user experience when a user does not necessarily require the
exact query result, but often would be satisfied with a good approximation that is
determined quickly. Hence the need for query-optimization techniques to provide
support for quick approximate results.
RC IV : Can we support query optimizations given current indexes for temporal queries
for quick approximate results ?
(C) Consider a company that is interested in the product reviews for a camera model
“canon eos 1100d” which was released in 2008 and is interested in all its men-
tions for planning for the next product release. This specific model is referred to
as “canon rebel xs”, “eos rebel t3” and “eos kiss x50” in different contexts. In or-
der to determine the documents which mention this specific model, these surface
forms can be used as phrase queries along with the year of its release. The out-
put documents can then be fed into a more elaborate extraction system for further
analysis.
Traditional indexing methods for processing phrase queries use inverted indexes
with positional information. Phrases are processed by intersecting posting lists
corresponding to the query terms and additionally checked for positional proxim-
ity. Phrase-query processing in such a setting becomes expensive for large collec-
tions because (i) one cannot employ standard retrieval techniques like stop-word
elimination common to standard keyword retrieval and (ii) additional checks for
positional adjacency.
The above problem can be addressed by indexing phrases or multi-word sequences,
but explicitly indexing all possible sequences is prohibitive. However, not all
phrases are frequently queried and not all combinations of words make valid
multi-word sequences. Promising sequences can be mined from the document
collection and from workloads. By considering their selectivities in the document
collections and frequency in the query workload, practical indexing and efficient
phrase querying solutions are possible.
RC V : How can we index multi-word sequences to improve phrase-query efficiency ?
RC VI : How can query processing efficiently answer phrase queries using indexed multi-
word sequences ?
4
1.3. Contributions and Publications
1.3. Contributions and Publications
We now present a synopsis of our contributions made in this work in addressing the
research questions posed above. We also mention the key publications in which these
contributions appear.
(I) Scalable and Efficient Support for Time-travel Queries : We present a novel
index organization scheme, called index sharding, that results in an almost zero
increase in index size. This practically efficient index organization method recon-
ciles the costs of random and sequential accesses, hence minimizing the cost of
reading index entries during query processing.
We also describe index maintenance strategies based on which the proposed in-
dex can be updated incrementally as new document versions are added to the
web archive. Our solution bounds the number of wasted read index entries per
posting-list and can be maintained using small in-memory buffers and append-
only operations. This work is published in:
• [ABBS11]: Avishek Anand, Srikanta Bedathur, Klaus Berberich, Ralf Schenkel.
Temporal Index Sharding for Space-Time Efficiency in Archive Search in ACM Con-
ference on Research and Development in Information Retrieval, SIGIR 2011.
• [ABBS12]: Avishek Anand, Srikanta Bedathur, Klaus Berberich, Ralf Schenkel.
Index Maintenance for Time-Travel Text Search in ACM Conference on Research
and Development in Information Retrieval, SIGIR 2012.
(II) Supporting Approximate Time-travel Queries : Building on the state-of-art in-
dex partitioning scheme proposed in [BBNW07] we present a framework for ef-
ficient approximate processing of time-travel queries and present practical algo-
rithms for the query-optimization problem. We derive a query plan for each time-
travel query ensuring that the number of results obtained at each stage of the exe-
cution is maximized. Our experiments with three diverse, large-scale text archives
reveal that our proposed approach can provide close to 80% recall even when only
about half the index is allowed to be read. This work is published in:
• [ABBS10]: Avishek Anand, Srikanta Bedathur, Klaus Berberich, Ralf Schenkel.
Efficient Temporal Keyword Queries over Versioned Text in ACM International
Conference on Information and Knowledge Management, CIKM 2010.
(III) Efficient Indexing and Phrase-Query Processing : We propose a phrase index-
ing solution and query-optimization techniques to improve the performance of
5
Chapter 1. Introduction
phrase queries. Our solution augments the existing word level index by a phrase-
level index which is tunable by index size and is optimized for phrase-query per-
formance. We study how arbitrary phrase queries can be processed efficiently
on such an augmented inverted index. Moreover, we develop methods to select
multi-word sequences to be indexed so as to optimize query-processing cost tak-
ing into account characteristics of both the workload and the document collection.
Experiments on two real-world document collections demonstrate the efficiency
and effectiveness of our methods. This work is under peer review:
• Avishek Anand, Ida Mele, Srikanta Bedathur, Klaus Berberich. Efficient Phrase
Indexing and Querying, under review.
1.4. Outline
This thesis is organized in three parts, each focused on a different workload type. Before
we describe our contributions in detail, Chapter 2 introduces the required background
and necessary technical foundations on which we build.
Chapter 3 explores a novel space-time efficient index partitioning technique called
index sharding. It addresses issues surrounding index organization, tuning and mainte-
nance along with extensive experimental evaluation. Chapter 4 introduces the problem
of supporting approximate queries by proposing query-optimization techniques over
the state-of-art vertically-partitioned index. It discusses the different approaches to this
problem along with detailed experimental results. Chapter 5 deals with the last work-
load type, namely phrase queries. It introduces novel indexing and query processing
techniques for efficient phrase-query processing. We finally conclude in Chapter 6 by
revisiting the research challenges sketched before and discussing our contributions. In
addition, we give an outlook on future research directions.
6
2
Foundations and Technical Background
In this chapter, we introduce the technical background necessary to understanding the
contributions presented in this thesis. We start with a brief description of web archives
and recent efforts relating to their acquisition and preservation. Next, we give an overvi-
ew of information retrieval with focus on text retrieval. In particular, we focus on the
efficiency aspects of text retrieval and describe issues concerning indexing and query
processing over large text collections. Finally, we conclude with the state-of-art tech-
niques relating to indexing archives.
2.1. Web Archiving
The Web is in a continuous state of change [ATDE09, NCO04, FMNW03]. Existing pages
are modified, new content is added, and old pages are removed resulting in a change
of state of the Web. Web archives prevent this loss by attempting to capture and pre-
serve this knowledge before it disappears. Efforts to preserve web contents have been
undertaken both by governments [WAR13, ARK13, BNF13, PAN13], non-profit orga-
nizations [IA13, IM13], and companies [HAN13]. Undoubtedly, the most popular and
large-scale effort in archiving the web has been the Internet Archive [IA13] which has an
estimated size of 500 Terabytes and is growing at 100 Terabytes a year.
Crawling in Web Archives Crawling is the most common method of acquiring con-
tent for web archives. A web crawler systematically requests and stores content from
web pages. It starts visiting pages from a seed set of pages and traverses the website
in a breadth-first or depth-first manner. The key difference from crawling strategies in
standard web search is that all files require to be fetched as opposed to only the ones
which would be later indexed. The challenges in crawling for archiving is usually in
capturing a consistent snapshot of a website at a given instance. Usually, due to po-
7
Chapter 2. Foundations and Technical Background
Figure 2.1.: Website preserved by Internet Archive
liteness requirements, it takes a long time to crawl an entire website. In the meanwhile
many portions of the website might undergo changes and modifications which are not
captured. Figure 2.1 shows how often and when yahoo.com has been crawled and
preserved by the Internet Archive.
Denev et al. [DMSW09] propose a model for assessing the quality of the crawled data
for web archives. They define blur as a stochastic measure of the quality of capture. The
longer it takes to capture an entire website the more blur the gathered data is said to
have. Further, coherence is a deterministic quality measure which determines the accu-
racy of a snapshot, i.e., number of pages which did not change during the crawl. They
further propose effective crawling techniques which optimize for blur and coherence.
The Internet Preservation Consortium (IIPC) [IIP13] and other preservation organi-
zation are involved in establishing standards necessary for effective web preservation.
IIPC is also involved in the development of open source software, most prominently
Heritrix [HER13], for crawling for web archives. For other issues and best practices in
web archives please refer to [Mas06].
Although there has been progress in content acquisition for web archives; access
methods and search support over them have been limited. Most of the archives either
do not have an explicit search interface or provide limited search functionality based on
open-source tools like Nutch [NUT13] or Solr [SOL13].
8
2.2. Data Management
2.2. Data Management
In this section we look at various data management principles and techniques which
are either related or used in our work. We briefly discuss the work done in temporal
databases which also deals with managing temporal data. Next, we give a detailed
account of distinct value estimators and KMV synopsis which we require in Chapter 4
in our work on query optimization for time-travel text search.
2.2.1. Temporal Databases
Temporal databases deal with data management issues pertaining to data associated
with temporal aspects. Specifically these temporal aspects include the notion of transac-
tion and valid times. Transaction time refers to the time when a certain fact was stored in
the database. Valid time, on the other hand, refers to the time when the fact existed or
was active in the real world. As an example if we enter a fact about the great depression
of 1930’s into the database, the valid time would refer to a time-interval [1930 - 1940],
while the transaction time would be when the entry was made, for example March 2013.
Transaction times evolve linearly and are immutable to change in the past unlike valid
times where changes in the past are allowed. Research in temporal databases has pro-
posed models, query languages and indexing techniques over arbitrary data with such
temporal aspects.
We now discuss in brief about notable index structures and indexing methods for
transaction time data. Early approaches, like the Time-Split B-trees (TSBT) [LS93] and
Multi-Version B-trees (MVBT) [BGO+96], adapt B+-trees for time-evolving data. The
Time-Split B-Tree was aimed at reducing storage costs and improving query perfor-
mance over current records but did not provide good worst case guarantees. MVBT
overcame this problem and also provided the same asymptotic space and time com-
plexity as the B-tree when a single version per record is managed.
In the late nineties, the log-structured history data access method (LHAM) was proposed
by Muth et al. [MOPW00] for data with high update rates. LHAM stores its data in suc-
cessive components C0, . . . , Cm of varying sizes and access costs. The core idea is that
the most recent data is stored in the component with the least access cost C0 thereby im-
proving update performance. The older records are gradually merged into components
with high access costs Ci+1 whenever a component Ci is full. The partitioning of records
into components entails a partitioning of the entire time duration into non-overlapping
time intervals. Consequently, each componentCi is associated with a time interval. Typ-
ically the component sizes follow a geometric progression and whenever a Ci is full it
potentially triggers a series of merges via a rolling merge operation. A detailed survey of
9
Chapter 2. Foundations and Technical Background
access methods for temporal databases can be found in [ST99].
2.2.2. Synopsis Structures
Determining number of distinct values in large collections, termed as distinct-value es-
timation or DV estimation, is an important task in query optimization, data streams and
network monitoring. While the exact count of distinct values is desired, determining
them is both computationally expensive and does not scale well. To this extent, approx-
imate methods have been proposed in the literature which efficiently estimate the result
like Bloom filters [Blo70], Hash Sketches [FNM85] and KMV Synopsis [BHR+07]. These
approaches are based on concise data-structures constructed over the input called syn-
opsis or sketches. In this section we give brief background information on the synopsis
structures, and specifically KMV synopsis, that are utilized by our algorithms.
Bloom Filters
It is a space-efficient probabilistic data structure used for answering set membership, set
containment, or set-intersection queries with a high confidence. Bloom filters are con-
cerned with sets and offer less than 10 bits per element for a 1% false positive probability.
However, they cannot be employed for arbitrary multiset operations.
Hash Sketches
These are distinct-value estimation technique was proposed in [FNM85]. They rely on
a pseudo-uniform hash function over the input to derive a sketch. These sketches allow
for various multiset operations by counting distinct elements after allowing appropri-
ately combining the sketches. Multiple intersections introduce high relative errors and
has a high computational complexity.
KMV Synopsis
Beyer et al. [BHR+07] introduced KMV synopses as effective sketches for sets that sup-
port arbitrary multiset operations including union, intersection, and differences. They
differ from hash sketches in having lower computational costs and more accurate DV
estimation. In this section we first explain the principle of DV estimation and introduce
the KMV synopsis data structure. We then explain how multiset operations like union,
intersection and set difference can be applied to them and estimated.
Consider an input multiset S with D distinct values Θ(S) = {v1, v2, . . . , vD} where
Θ(S) represents the domain. Each of these values is mapped to a random location on an
unit interval. Assuming that the points are distributed uniformly, the expected distance
10
2.3. Information Retrieval
between any two neighboring points is 1/(D + 1) ≈ 1/D. The expected position of the
k-th point from the origin or the k-th smallest point, Uk, , is k/D, i.e, E[Uk] ≈ k/D.
The simplest estimator of E[Uk] is simply Uk and yields the basic estimator as introduced
in [BYJK+02]:
D̂BEk = k/Uk.
Consider a hash function h : Θ(S) 7→ 0, 1, . . . ,M which maps the input in the range
[0 − M] where M >> D. The hash function h is chosen such that mapped values
h(v1), h(v2), . . . , h(vD) resemble an independent and identically distributed sample from
the discrete uniform distribution on 0, 1, . . . ,M. Normalizing the range to a unit range,
as above, the values Ui can be expressed as Ui = h(vi)/M, vi ∈ Θ(S). The value M
regulates the number of collisions. The higher the value of M the lower the probability
of collisions.
Building on this notion of the basic estimator, KMV synopsis is defined in [BHR+07]
for a multiset S as follows. Given a hash function h, the k-smallest values of the mapping
h(vi) where vi ∈ Θ(S) constitute the KMV (k-minimum values) synopsis of S. Typically
M = O(D2) is chosen to avoid collisions and hence each of the synopsis values can be
encoded in O(logD) bits. Consequently, the number of bits required to store the KMV
synopsis is kO(logD). The KMV synopsis can be created by a single scan over S and
only storing the k smallest hashed values implemented efficiently by a priority queue.
The basic estimator is further extended to an unbiased estimator D̂k.
D̂k = (k− 1)/Uk
The cardinality estimation of the multiset operations involving two or more synopses,
like unions and intersections, can be carried out efficiently based on the above estima-
tors.
2.3. Information Retrieval
Information retrieval (IR) deals with finding information from large collections of un-
structured data to satisfy user information needs. Historically, the focus of IR has been
on investigating concepts, models and computational methods for searching large text
collections. However, the field has expanded its horizons to multimedia content [Cho10],
facts and semi-structured content [Lal12, CMS10]. In this work we focus on text collec-
tions and deal with the core problem in IR, i.e.,
We are given a document collection D associated with a vocabulary of terms V . For a given
query q ∈ V our aim is to find the documents which satisfy the user information need expressed
11
Chapter 2. Foundations and Technical Background
by q.
2.3.1. Retrieval Models
The question above gives rise to two key aspects which account for the effectiveness of IR
systems – (1) How are documents and queries modelled ? (2) How is the relevance of a
document to a given query modelled ?
Boolean Retrieval Many retrieval models have been proposed to model documents
and queries and the similarity between them. We provide the key ideas of some of the
fundamental retrieval models. The Boolean retrieval model is the earliest and simplest
retrieval model. Queries in this retrieval model are Boolean expressions comprising
of terms v ∈ V and connected by Boolean operators (OR, AND, NOT). The notion of
relevance of a document d to a query q is binary, i.e., either d is relevant to q or not
depending on the evaluation result of the Boolean expression.
Ranked Retrieval Salton introduced the vector space model [SWY75] to rank documents
according to their perceived relevance to the query by modelling documents and the
query as a vector of features. The features are typically terms in this case and the sim-
ilarity value between a query and a document encodes the relevance of a document
given a query. The similarity between q and d is given by the cosine similarity between
the document and query vector cast in the feature space of terms.
Query Semantics There are two widely accepted query semantics – conjunctive and
disjunctive query semantics. Conjunctive query semantics require all terms to be present
in a document to qualify as a relevant result whereas disjunctive semantics allow any
occurrence of the query terms in the document. For example, under conjunctive query
semantics, the relevant documents for the keyword query “german soccer team” using
boolean retrieval should contain all the constituent terms “german”, “soccer” and “team”
to constitute as results. Although simple to implement, Boolean-retrieval models are
limiting when a large number of documents qualify as results. It is hard for the user to
evaluate all results and a notion of ranking is hence desirable.
We now come to the question of assigning feature values for vectors. To address
this there are two aspects which are considered. The more a query term occurs in a
document the more important it is. This is captured by the term frequency or tf(d, v)
of a term v for a document d, which is defined as the number of times v is present in d.
Note that in this model, documents are considered as bags of terms. However, certain
terms frequently occur throughout the corpus, for instance articles, prepositions and
12
2.3. Information Retrieval
stop words. These terms, although highly frequent, might not be discriminative from
other query terms when it comes to capturing the true user intent. For example in
the query “house of fraser”, “fraser” is more discriminative than the other terms which
appear frequently in the collection. To account for this, the second aspect or document
frequency df(v) of a term v is taken into consideration. Using the value df, the inverse
document frequency is defined as
idf(v) = log
|D|
df(v)
is construed as being directly correlated with the discriminativeness of a term.
Combining both the measures we arrive at the tf-idf which has become one of the
most central weighting scheme for the vector space model. Most of the other weight-
ing schemes are often variations and refinements of tf-idf. A detailed account of these
weighting schemes can be found in Manning et al. [MRS08]. The most popular refine-
ment of the tf-idf value is the Okapi BM25 model by Robertson et al. which is known
to provide high retrieval effectiveness. Okapi BM25 takes into account the length of the
document and also the average length of documents in the entire collection to normalize
the term frequencies. Formally,
Definition 2.1 (Okapi BM25) For a document d and a query q, the relevance of d to q is
defined by Okapi BM25 as
r(q, d) =
∑
v∈q
wtf(v, d).widf(v). (2.1)
The tf-score wtf(v, d) is defined as
wtf(q, d) =
(k1 + 1) · tf(q, d)
k1 · ((1− b) + b · dl(d)avdl ) + tf(q, d)
, (2.2)
where 0 ≤ b ≤ 1 and k1 ≥ 1 are tunable parameters. The length of d is denoted as |d| and avdl
represents the average document length in the document collection.
The idf-score is defined as
widf(v) = log
N− df(v) + 0.5
df(v) + 0.5
, (2.3)
where N is the collection size and df(v) has the aforementioned semantics.
The tf component in Okapi BM25 is scaled by adjusting the parameter k1 while b
controls the effect of length normalization. Typical values for the two parameters which
have been seen to work well are k1 = 1.2 and b = 0.75.
13
Chapter 2. Foundations and Technical Background
information
(d1 , 2, <2, 15>)
(d4 , 1, <21>)
(d34 , 3, <2, 15, 23>)
(d64 , 5, <2, 15, 23, 56, 155>)
.....
retrieval
(d1 , 1, <5>)
(d14 , 3, <21, 24, 78>)
(d64 , 4, <16, 24, 67>)
(d78 , 2, <27, 56>)
.....
Lexicon
posting 
lists
Thursday, April 25, 13
Figure 2.2.: Inverted index
The success of a retrieval system not only depends on the effectiveness of the retrieval
models but also on how fast it can respond to queries. With the exponential growth of
collection sizes the challenges for scalable and efficient search have accounted for a rich
body of research over the past decade. The efficiency issues can be broadly classified
into two major areas in the retrieval phases – (i) indexing the document collection and
(ii) processing queries over the index. In the following, we introduce the common tech-
niques employed for indexing text.
2.3.2. Indexing Text
Indexing methods are required in order to efficiently perform the retrieval process. The
most popular choice to index text collections is the inverted index. It is the cornerstone
for most commercial search engines and real-world information retrieval systems. The
inverted index comprises of two components - (i) the lexicon and the (ii) collection of
posting lists comprising the inverted file. Each of them can be implemented using different
data structures and are materialized into separate files.
Lexicon The lexicon or the term dictionary contains the indexed terms and their cor-
responding statistics. These terms are typically words which are extracted and stemmed
from individual documents. Each entry in the lexicon represents a term v and it must
contain, but is not limited to, the following – (i) the string of the term v, (ii) document-
14
2.3. Information Retrieval
frequency information df(v) and, (iii) pointer to the posting list of v in the postings file.
During query processing the lexicon is consulted to check for the containment of the
query terms in the collection. The look-up process results in providing further data,
if needed, for the rest of the retrieval process. There are many strategies to improve
lexicon look-up speeds. One straightforward approach, which is now common with
the growth of memory capacities, is to load the entire lexicon into memory as a hash
table. Other examples of dictionary layout include search trees and dictionary-as-a-string
proposed by Witten et. al [WMB99]. For a detailed discussion we point the reader to
Manning et al. [MRS08].
Postings File The second data structure is the postings file which contains informa-
tion about occurrence of terms in documents. Since terms are the indexed units here,
each term v is associated with a list of postings Lv, called the posting list, and each post-
ing represents a document where v is present. The postings file is a collection of posting
lists, one for each term. A posting belonging to the posting list Lv, contains the docu-
ment identifier di where v is present and information about the v’s occurrences. The
occurrence information can be binary, if v is present in the document or not, or more
expressive, like the frequency of occurrences or even positions where v was present in
the document, i.e.,
〈di, tf(v, di), < pos1, . . . , postf(v,di) >〉.
For Boolean retrieval storing only document identifiers is sufficient. In case of ranked
retrieval, which is indeed the more common scenario, a score s is stored which is usually
the tf(v, di) or tf(v, di).idf(v). If phrase or proximity queries need to be supported the
posting also stores a list of positions where the term occurs in the document (see Fig-
ure 2.2). These positions are sorted for best compression and query processing benefits
as we will see later. Posting lists are highly compressed and stored contiguously on disk
for best space and cache effectiveness. However, for dynamic collections the contigu-
ous placement might not always be desirable and we discuss later how such collections
can be indexed. Additionally these lists can be augmented with skip pointers for faster
list traversal during query processing [MZ96]. For the scope of this work we do not
consider such optimizations, since they are orthogonal to the methods proposed.
The posting list can be document-ordered or score-ordered. In document-ordering the
postings are ordered by increasing document identifiers. The benefit of such an organi-
zation is that it achieves high compression ratios. Additionally, document-ordered lists
are easier to maintain than their score ordered counterparts. On the other hand score-
ordered lists are ordered according to their scores. Score-ordered lists facilitate dynamic
pruning of lists during query processing and do not require the entire list to be read into
15
Chapter 2. Foundations and Technical Background
memory. Several top-k algorithms have been proposed which allow for early termina-
tion by leveraging the score order during query processing [ADGK07, FLN01, BMS+06,
TWS04, YSL+12]. However, the compression factor achieved by such an ordering is not
as high as document-ordered lists and index maintainance is also expensive.
Compression
The posting list accounts for the majority of the index size. Thus posting-list compres-
sion is a key problem in indexing text and has been subject to active research over the
past two decades. The obvious advantage of list compression is their savings in overall
index size. Since inverted indexes are typically stored on disk, compression can reduce
the space consumption by 75% [MRS08]. Also, due to the use of caching for query pro-
cessing, some of the frequently accessed index lists are stored in memory to increase
query performance. Since, memory is a more expensive resource than disks, a smaller
memory footprint is desirable. Finally, CPU performance has improved relatively more
than hard-disk performance in recent years. Thus the aggregate benefit of transferring
a highly compressed posting list to memory and decompressing it outperforms read-
ing it in its uncompressed form. We outline the most widely used techniques for index
compression.
Integer Compression We first talk about integer-compression techniques useful in
compressing document identifiers and positional information. Storing integers in fixed-
size bit sequences or arrays wastes space for small integers. Variable byte encoding uses
integral number of bytes to encode an integer. An example is the 7-bit encoding in which
the most-significant bit of a byte is the continuation bit and the remaining seven bits
represent the payload. The 7-bit encoding can be decoded by reading a sequence of
bytes until the continuation bit is zero. The sequence read concatenates the payloads of
each byte, disregarding the continuous bit values of each byte, to decode the integer. As
an example, n = 129 is encoded as 〈 00000001 11111111 〉 and n = 30 is 〈 00011110 〉.
Unlike variable byte encoding, which is byte aligned, we can consider directly work-
ing with bits. The simplest bit-level encoding technique unary encoding stores n− 1 one
bits followed by a zero as a delimiter to store the positive integer n. Although, it is ben-
eficial for small n it soon becomes infeasible for higher values of n. For more efficient
representations of integers Elias-γ encoding is used which splits the positive integer n
into two parts. The first part encodes 1 + blognc in unary encoding. The second part
encodes n− 2blognc using binary encoding. As an example n = 10would be encoded as
〈 1110 10 〉.
Elias-δ encoding is slightly different from Elias-γ encoding in which the first part, 1 +
16
2.3. Information Retrieval
blognc, is encoded using Elias-γ encoding instead of unary encoding. Thus, n = 10
would now be encoded as 〈 110 00 10 〉.
Compressing d-gaps Let us for simplicity view a posting list as an array of document
identifiers. The obvious choice in compressing a posting list is to use one of the above
techniques to compress each document identifier did. In case of document-ordered
lists, we can exploit the ascending order of dids by encoding the difference between
consecutive did values or so called d-gaps. In this scheme, the original integer value
of only the first or minimum integer is stored. The remaining integers are represented
by their d-gap value. All integers can be reconstructed in the course of the linear scan
from begin of the list from the d-gaps. For example 〈d1, d2, d2, d4 〉 can be represented
as 〈d1 , d2 − d1 , d3 − d2 , d4 − d3 〉.
The primary advantage of storing d-gaps is that they tend to be small integers and can
be compactly represented using the techniques discussed above. The gains are signifi-
cant especially for long posting lists, corresponding to frequent terms such as articles or
preposition, where the difference between dids is very small. Apart from using d-gaps
for dids it can also be employed for positional information. A more comprehensive
survey of other compression methods for d-gaps can be found in [WMB99].
Query Processing
After having discussed retrieval models and indexing we now turn to query processing:
Given a query, q, and an inverted index how can we process queries efficiently ? Depending
on the index organization there are two major query processing techniques. We discuss
query processing techniques for document-ordered lists - Term-at-a-Time and Document-
at-a-Time.
In Term-at-a-time (TAAT), the posting list Lv for each query term v ∈ q is processed one
after the other. A set of accumulators, one for each document, is maintained which store
partial scores for the results determined thus far. In each iteration, the partial scores in
the accumulators are updated. Finally, after all the terms have been processed the scores
in the accumulators are sorted in descending order.
In order to reduce the number of operations and accumulators, numerous methods
and heuristics have been proposed. For conjunctive query semantics, the retrieval of
the posting list is scheduled in increasing order of their lengths. Since the result set is
always a subset of every posting list such a scheduling facilitates reduction in number
of accumulators initialized. Dynamic pruning strategies have also been proposed for
memory-limited conditions, a detailed account of which is presented in [BYGJ+08].
Document-at-a-Time (DAAT) processing accesses all the posting lists in parallel. In
17
Chapter 2. Foundations and Technical Background
this strategy the final scores of result documents are not incrementally computed, as
in TAAT using accumulators, but determined on the fly. The posting lists are merged
efficiently using a priority queue of postings. A cursor is kept for each posting list Lv
corresponding to each term in the query, v ∈ q, and a min-heap of document identifiers
is maintained. In each iteration, the cursors are advanced to the minimum document
identifier, say dmin, in the heap. Next, the final score for dmin is computed and the
priority queue is populated with the minimum document identifier from the current
cursor pointers. For processing phrase queries, an additional check is done utilizing
the positional information in the postings if the occurrence of the v in dmin is a part of
the query phrase. For example, consider a query “information retrieval” on an inverted
index represented in Figure 2.2. Although document d1 contains both query terms,
“information” at positions 2, 15 and “retrieval” at position 5, but they are not present next
to each other. Hence d1 does not qualify as a result to the phrase query. On the other
hand d64 contains them one after another starting at positions 23.
Depending on the query semantics, further optimizations can be performed. For in-
stance when conjunctive query semantics are required, a max-heap instead of a min-
heap is maintained to avoid scoring of documents not containing all query terms.
Index Maintenance
The indexing methods discussed so far concern static document collections. However,
document collections change over time. New documents are inserted and old docu-
ments are either deleted or modified. These changes should appropriately be reflected
in the index to keep it in sync with the document collection. In this section, we discuss
index maintenance strategies which have been proposed to address this problem. The
index can be updated either in a batched manner or incrementally.
Batched Index Updates In batched updates, the index maintenance is carried out pe-
riodically while accumulating the changes to the document collections in the meantime.
When the index is finally updated, these changes are incorporated into building a fresh
index which is consistent with the collection. A straightforward strategy to accomplish
this is to re-build the entire index from scratch on the current state of the document col-
lection. Although easier to implement, such a strategy works well only if more than
60% of the changes involve deletions and updates [BCC10]. When insertions dominate,
which is the case in most situations, re-building the index from scratch often becomes
expensive. The other, more practical alternative is to build a partial index on the accu-
mulated updates and merge it with the existing index. This update strategy is referred
to as re-merging. Re-merging avoids the inversion of the older documents into postings
18
2.3. Information Retrieval
and creates posting list by merging the posting lists, for each term, of the partial and ex-
isting index. Deletions are managed by keeping a list of documents which are no longer
present in the collection. This list is used as a filter during query processing to remove
non-existing documents from the result set.
Incremental Index Updates In many applications it is critical that the index always
reflects the current state of the collection. Batched updating of indexes is undesirable
for such requirements. Most of the literature which deals with incremental updates as-
sumes a strict incremental nature of the updates, i.e., only new documents can be added
and existing documents can never be modified or removed.
The underlying principle is to create multiple in-memory partial indexes, similar to
the partial indexes in re-merging, and allow query processing on them. The partial
indexes are of fixed sizes depending on memory limitations. Once a partial index ex-
hausts the space budget, it is materialized to disk and a new index is started. Over a
period of time and numerous updates we have multiple small disk-resident indexes.
When queries are issued, they are routed to all the partial indexes. For a query q, the
posting-list fragments for each term v ∈ q are fetched from saym partial indexes. Allm
fragments, including the in-memory index, are concatenated to form the term’s posting
list. Following this, the corresponding query processing method is invoked for comput-
ing the final result set.
Partitioning the entire index into such smaller partial indexes might be desirable for
indexing but is clearly expensive for query performance. In this scenario, the number of
random seeks for fetching the fragments are |q|.m. The query performance can be im-
proved by selectively merging these partial indexes into a smaller set of partial indexes.
Taken to the extreme, the performance is at its best when there is one consolidated index
thereby eliminating fragmented lists. In such a strategy, called immediate merge, only a
single disk-resident index is maintained apart from the in-memory partial index. Once
the in-memory index reaches the size threshold it is merged into the main index.
Many factors determine the efficiency of the merge operation like – assignment of doc-
ument identifiers, choice of compression algorithm and allowing for in-place merging.
Typically, in a dynamic collection, document identifiers are assigned incrementally, i.e.,
every new document is assigned an identifier greater than previously assigned identi-
fiers. Consequently the partial indexes always have document identifiers greater than
those indexed in the primary index. Thus the merging operation of the two posting
lists reduces to an append operation. Indexes constructed with a different document-
identifier assignment, which does not exhibit the above property, can be difficult to
maintain due to expensive de-serialization and sorting operations.
The second factor which affects merge efficiency is the choice of the compression
19
Chapter 2. Foundations and Technical Background
method. As discussed in the section earlier d-gaps are computed and compressed us-
ing integer-compression methods. If we employ compression algorithms which are in-
dependent of global parameters, like collection-wide statistics, one does not have to
de-compress the already compact posting list from the primary index. For example, the
merging of posting list of term v using 7-bit encoding of d-gaps would require us to read
Lv from the primary index and determine the last document identifier in the list. The
last document identifier, say dlast, can be identified on the fly or can be explicitly stored.
The posting list from the partial index is compressed and appended to Lv by computing
d-gaps from dlast.
In-Place Merging After the updated posting list, say L ′v is determined, the new copy
has to be written to a new location. However, in-place merge techniques have been
proposed which allow for over-allocation of space after each posting list. This allows
Lv to be updated in-place and avoids expensive relocations. There are two kinds of in-
place merge strategies. One requires the entire posting list to be contiguous, as proposed
by Lester et. al [LZW06], while the other does not [TGMS94]. Contiguity of posting
lists improves query performance but sometimes involves expensive relocations. On
the other hand, allowing for non-contiguous lists improves update performance at the
expense of query performance. To reconcile these two approaches hybrid methods have
also been proposed in [BCL06a] which take into account the posting-list length and
update characteristics for over-allocation.
We have discussed two techniques which are on the extremes of the trade-off between
index maintenance and query performance, i.e., the no-merge strategy which has no
maintenance overhead and immediate merging which has the best query performance.
There are other approaches which explore this trade-off to selectively merge partial in-
dexes to balance maintenance and query performance namely logarithmic merge, geomet-
ric merge, etc. A detailed account of these can be found in [BCC10].
2.3.3. Indexing Archives
In this section we look at text search over versioned text collection with focus on web
archives. Before we discuss the details of the index organization we outline the docu-
ment, collection and query model and formally define the time-travel retrieval task.
Document and Collection Model We adopt a discrete notion of time and assume that
a time-stamp t is a positive integer and is computed periodically, with a fixed granular-
ity, from a reference point in the past where t = 0. For instance, a widely accepted time
of origin in computer systems is the Unix Epoch – 00:00:00 UTC on 1 January 1970. The
20
2.3. Information Retrieval
granularity of measurement could be milliseconds, hours, days, weeks or years.
We define the overlap of time-intervals as follows.
Definition 2.2 (Overlapping intervals) An interval I1 = [b1, e1) is said to overlap with
another interval I2 = [b2, e2), denoted as I1 I2, if the following relation holds:
I1 I2 ⇐⇒ b1 < e2 ∧ b2 < e1
and I1 does not overlap with I2, denoted as I1 I2, if the following holds:
I1 I2 ⇐⇒ b1 ≥ e2 ∨ b2 ≥ e1.
We operate on a versioned document collection D. Each document di ∈ D is as-
sociated with a sequence of its versions 〈d1i , d2i , . . .〉. Each version dki is drawn from a
vocabulary of terms V , i.e. dki ⊆ V . Furthermore, each document version dki has an
associated valid-time interval valid(dki ) = [begin(d
k
i ) , end(d
k
i )) when d
k
i existed in the
real world. We also make a natural assumption that versions of the same document do
not have overlapping valid-time intervals, i.e.,
∀dji, d
k
i : valid(d
j
i) valid(d
k
i ).
A document version is active, or has not yet been superseded by a new version if
end(dji) = ∞. Note that this is similar to the notion of transaction time in temporal
databases as introduced earlier.
Query Model Berberich et al. in 2007 [BBNW07] introduced the time-travel queries
as an important query type to search archives and other time-stamped corpora. For-
mally, a time-travel query Q, as considered in this work, consists of a set of terms
keywords(Q) = {q1, . . . , qm} and a time interval denoted by interval(Q). The begin
and end of the query time-interval are represented by begin(Q) and end(Q), i.e.,
interval(Q) = [begin(Q) , end(Q)].
As an example, for a time-travel query Q = “bhopal gas tragedy” @ [11/1984 -
01/1985] – keywords(Q) = {bhopal, gas, tragedy}, begin(Q) = 11/1984 and end(Q) =
01/1985. With the model in place we can now formally define the time-travel retrieval
task.
Definition 2.3 (Time-travel Retrieval Task) Given a document collection D with versioned
documents (each document version associated with a time-interval), and a time-travel query Q,
the objective is to find all documents versions R(Q) whose valid-time interval overlaps with the
query time-interval and which contains all the query keywords.
21
Chapter 2. Foundations and Technical Background
R(Q ) ={
dki ∈ D | ∀q ∈ keywords(Q) : q ∈ dki ∧ valid(dki ) interval(Q)
}
.
Queries for which begin(Q) = end(Q) holds, so that the query time-interval col-
lapses into a single time point, will be referred to as time-point queries.
Time-Travel Index
Berberich et al. proposed the time-travel inverted index as an adaptation of the inverted
index for efficient processing of time-travel queries. It transparently extends the stan-
dard inverted index and proposes novel compression and index partitioning schemes.
We start by looking at the extensions with respect to the postings and posting-list or-
ganization. Firstly, the posting representing a version dki is extended by addition of a
valid-time interval valid(dki ) = [begin(d
k
i ) , end(d
k
i )), i.e.,
〈dki , [begin(dki ), end(dki )), s〉.
Secondly, it was proposed that each posting list Lv may be be partitioned along the
time dimension into a set of partitions. Each partition has an associated time-interval
span(φv,j) = [begin(φv,j) , end(φv,j)). It contains postings which are present in the
unpartitioned list Lv, φv,j ⊆ Lv, and those which represent document versions whose
valid-time interval overlaps with span(φv,j), i.e.,
φv,j =
{
dki ∈ D | valid(dki ) span(φv,j)
}
.
Further, we assume that partition-spans for a given term v are disjoint, i.e.,
∀i ∀j : span(φv,i) span(φv,j) .
For processing a time-travel query, we must first retrieve for each query term, the
set of postings which overlap with the query-time interval. Once retrieved, standard
query processing techniques discussed before can be applied on this subset of postings.
To reduce the retrieval cost, only those partitions which overlap with the query-time
interval are fetched, i.e., for a retrieved partition φv,j with respect to a time-travel query
Q the following holds
interval(Q) span(φv,j).
Following that, all partitions for the same term are merged and irrelevant postings,
whose valid-time interval do not overlap with the query-time interval, are filtered out
22
2.3. Information Retrieval
time
t1 t2 t3 t4 t5 t6 t7 t8
d1
d2
d3
d11 d12 d13 d14
d21 d22 d23
d31 d32 d33
Thursday, April 25, 13
Figure 2.3.: Partitioning a posting list
in the process. Note that identifying if a postings is irrelevant in a partition is only pos-
sible once the partition is retrieved. If most of the postings in a partition are irrelevant
then an entire partition must be wastefully read to determine a few relevant postings.
An approach to reduce the cost of filtering out such postings, thus improving query
performance, the granularity of partitioning can be reduced. As one extreme, called the
performance-optimal approach or Popt, partition boundaries are placed at time-points that
occur as boundaries of valid-time intervals as shown in the Figure 2.3. Such a partition-
ing eliminates irrelevant postings in a partition.
On the other hand, since each partition is associated with a time span, postings whose
valid-time intervals overlap with multiple partitions are replicated in all of them. For
instance, in Figure 2.3, the version d13 is replicated multiple times for the performance-
optimal approach. Replication of postings in multiple partitions results in an index
blowup thus making the performance-optimal approach infeasible in practice. The
other extreme case, referred to as space-optimal partitioning or Sopt, ensures that there
is no index blowup by disallowing replication of postings.
Partitioning Strategies There is a natural trade-off between query performance and
index-size blow and the above two approaches represent the two extremes of this. In
practical situations the desired partitioning strategy lies between these two strategies. To
explore this middle ground two approaches of partitioning strategies are proposed - the
performance guarantee approach and the space bound approach.
In the performance guarantee approach a bounded loss of performance over Popt is
tolerated. The reduction of performance, by the performance-guarantee approach, for a
given query is measured by the excess of postings processed over Popt. The performance
reduction is bounded by a user specified parameter γ and a partitioning is desired which
adheres to this bound for all possible queries. The objective, under such a performance
guarantee, is to minimize the overall index-size blowup.
23
Chapter 2. Foundations and Technical Background
time
sc
or
e
non-coalesced
coalesced
Thursday, April 18, 13
Figure 2.4.: Temporal coalescing
The space-bound approach is proposed for situations when storage space is at a pre-
mium and a bound is desired on the overall index size. Given a user specified index-size
bound, κ, the objective here is to minimize the reduction of performance relative to Popt.
The size bound on the entire index can be translated to a bound on the size of each post-
ing list. The size bound for partitioning posting list for each term v is given by κ.|Lv|.
The space-bound approach also allows for a query distribution determining the proba-
bility of a term given a time-point P(t). An optimization problem is formulated which
intends to partition a posting list Lv to minimize the expected query processing cost,
given κ and P(t), while adhering to a space bound κ.|Lv|. We refer to these partitioning
strategies as vertical partitioning in the rest of the thesis.
Temporal Coalescing Indexing all the versions is expensive in terms of storage costs.
However, archives are characterized by a high degree of redundancy. Often times,
changes to documents are minor resulting in a high overlap of text content between
consecutive documents versions. Temporal coalescing techniques exploits this redun-
dancy to substantially decrease the overall index size.
A naı̈ve implementation of the posting list Lv would create a posting for each docu-
ment version term pair. To exploit the redundancy between consecutive versions tempo-
ral coalescing in conjugation with the time-travel index. Temporal coalescing methods
aim to coalesce postings belonging to consecutive versions of the same documents when
the information contained between them is not significantly different. The techniques
are applied on a per-term basis. They also have different semantics for different types
of payloads, i.e., boolean, scalar or positional.
For boolean payloads of the form 〈dki , valid(dki )〉, multiple postings for consecutively
occurring versions can be replaced by a single posting. The single posting represents
that the term was present in a document for a contiguous period of time encoded in the
posting.
For scalar payloads, scores of consecutive versions with low variance can be captured
24
2.3. Information Retrieval
and coalesced into a single posting. Figure 2.4 illustrates that coalescing postings with
small variation in scores results in reducing the number of postings, in this example,
from 9 to 5. The degree of relaxation allowed is specified by the user and captured
by the parameter η. This is formulated as an optimization problem. Given an input
sequence of postings of the same document, ordered by their begin times, the objective
is to generate a minimal number of coalesced postings adhering to the user specified
relaxation.
25
Chapter 2. Foundations and Technical Background
26
3
Efficient Indexing and Maintenance for Time-Travel Text
Search
3.1. Motivation and Problem Statement
In this chapter we address two major indexing challenges. Firstly, given a versioned
document collection we address how to efficiently identify documents which are solu-
tions to the time-travel retrieval task. To this end we propose novel index organization
and tuning approaches. Secondly, given a dynamically growing versioned collection we
propose strategies for efficient index maintenance.
Archives are collected over a period of time and hence have an implicit temporal
dimension. Consequently, time-travel text search is important in searching archives as
it limits the search to a specified time of interest in the past. We motivate this by two
use cases. A political analyst is interested in statements made by presidential candidates
during the recent financial crisis (in 2008). Constraining the search to document versions
that existed in the year 2008 filters out versions during previous instances of such events
thus preventing dilution of content.
Consider a business analyst looking for web pages predicting and analyzing upcom-
ing releases of tablet computers. If only keyword queries are used to retrieve pages
from a web archive, irrelevant information about earlier releases of tablet computers
may corrupt the analytics results. By including a temporal constraint on the publication
or discovery time of web pages, such undesirable results can be eliminated.
Archives are dynamically increasing collections with content being added over time.
Almeida et al. [AMC07] report how Wikipedia has grown exponentially since its incep-
tion. News archives grow continuously and are progressively becoming accessible on-
line. Research and improvements in archive crawling technology [Den12, HER13] also
suggest that more content will be amassed in the future. To keep up with the dynam-
ically growing archives it is essential that our indexes have to be efficiently maintain-
27
Chapter 3. Efficient Indexing and Maintenance for Time-Travel Text Search
able. Naı̈ve approaches like periodically rebuilding indexes are prohibitively expensive.
Hence there is a need for strategies for index organizations which can be updated easily
while still being competitive in query performance.
3.1.1. Approach
time
doc.
id
time
doc.
id
d21
d12
d31
d41
d51
d21
d12
d31
d41
d51
Vertical
Partitioning
Index
Sharding
Friday, May 17, 13 Figure 3.1.: Vertical partitioning vs. sharding of a posting list
The state-of-art approaches to process time-travel queries, as discussed in Chapter 2,
transparently extend the inverted index with time-enriched postings. To optimize for
time-travel query processing each posting list is partitioned into a number of smaller
posting lists along the time dimension. We refer to these methods as vertical partitioning.
Partitioning in this way reduces access to postings which are irrelevant to the query.
Specifically, those postings which do not overlap with the query time-interval. In-
creasing the degree of vertical partitioning improves the query-processing performance.
However, such an index organization suffers from an index-size blowup, incurred due
to the replication of postings, due to partitioning. A careful choice of the partitioning
boundaries can help to reduce the index-size blowup [BBNW07], but in order to achieve
acceptable levels of efficiency 2 to 3 times index size increase is necessary.
28
3.1. Motivation and Problem Statement
We look at a novel way of partitioning posting lists called index sharding in which we
propose to shard – or horizontally partition – each posting list along document identifiers,
instead of time (see Figure 3.1). An immediate benefit of this alternative partitioning
is almost no increase in the overall index-size (as we show later the only overhead is
to maintain a small set of location pointers in each partition). We develop a single-
pass greedy algorithm that optimally shards the posting list, minimizing the number of
postings read during query processing.
The idea is to achieve superior query performance by organizing the postings into
shards, exploiting the geometry of the associated intervals, which helps in avoiding ac-
cess to postings which are irrelevant to the query time-interval. Query processing over
a sharded index proceeds by accessing all the shards in parallel but reading only a small
portion from each of them. As a random access is at least as expensive as a sequential
read (as in disk-based and network-based index storage), breaking the posting list into
too many shards actually degrades performance, even if we optimize access to only rel-
evant postings. Thus, the practical efficiency of the index organization is achieved only
if it is sensitive to the cost ratio of random accesses to sequential accesses. We formu-
late optimization problems for tuning the parameter for query performance that takes
into account the I/O cost ratio of the storage infrastructure and propose a heuristic to
combine shards to gain practical runtime efficiency.
Based on the index sharding principle, we propose a framework which efficiently
handles additions of new document versions to the archive without sacrificing query
efficiency. We propose an algorithm which is incremental in nature thus avoiding the
expensive recomputation of shards. Further, it reconciles the relative cost of random
and sequential access for better query performance.
3.1.2. Contributions
In summary, the key contributions made in this chapter are the following.
1. A novel sharded index organization for a time-enriched inverted index that over-
comes the issue of index-size blowup.
2. An optimal greedy algorithm to shard the posting list, so that no time-travel query
reads more than the required postings, thus achieving ideal query-processing per-
formance.
3. A framework that achieves practical runtime efficiency by tuning the number of
shards that each posting list is split into, taking into account the I/O cost ratio of
the storage infrastructure. Note that the cost of such a partitioning is independent
29
Chapter 3. Efficient Indexing and Maintenance for Time-Travel Text Search
of the dynamics in the query workload, as it depends only on the storage system
parameters.
4. A framework to support updates and perform efficient index maintenance based
on an incremental sharding algorithm.
5. An extensive empirical evaluation on large-scale versioned document collections
using real-world keyword queries with temporal constraints at varying granular-
ities.
3.1.3. Organization
The remainder of this chapter is organized as follows: In Section 3.2, we present the data
model and index organization that we use in this chapter. Next, in Section 3.3, we de-
scribe in detail the idea of sharding the inverted index and how queries can be processed
over the sharded index. In Section 3.4 and in Section 3.5 we present the details of our
idealized sharding and a shard-merging strategy. Section 3.7 describes our incremental
sharding method for index maintenance. Section 3.8 presents the overall architecture
of our update-aware retrieval system. Details of our experimental setup and its results
are presented in Section 3.10. Finally, we discuss previous related work in Section 3.11,
before summarizing in Section 3.12.
3.2. Model and Index Organization
We adopt the document, collection, and query models from Chapter 2 and briefly recap
the notation in Table 3.1.
We call a document version dki an active version if it is the most current version of
document di and consequently has end(dki ) = ∞ where the current time or “now” is
represented as∞.
Our proposed index for time-travel queries is based on the established inverted index
as described in Chapter 2. We extend the inverted index to support time-travel queries
by modifications to the postings structure, posting-list organization, and the lexicon.
We extend the contents of each posting by additionally storing the valid-time interval,
valid(dki ) = [begin(d
k
i ), end(d
k
i )), of the document version d
k
i along with the identifier
di, and a payload s, i.e.,
〈dki , [begin(dki ), end(dki )], s〉.
The index organization supports different retrieval models by allowing the payloads
to be empty (for Boolean retrieval), containing a scalar value (tf in the document ver-
sion) or containing positional information (phrase-based retrieval). When no confusion
30
3.2. Model and Index Organization
Notation Description
D collection of documents
V vocabulary, a set of words
d
j
i ∈ D a document version, j
th version of document di
valid(dji) valid-time interval
begin(dji) begin time of d
j
i
end(dji) end time of d
j
i
Q time-travel query
keywords(Q) set of keywords in Q
interval(Q) query time-interval of Q
I1 I2 Overlapping intervals I1 and I2
I1 I2 Non-overlapping intervals I1 and I2
Table 3.1.: Notation
arises, we simply use begin(p) and end(p) of a posting p to refer to the valid-time in-
terval boundaries of the corresponding document version.
We partition each posting list into disjoint partitions referred to as shards. The post-
ings in a shard are ordered according to their begin times. We assign the identifiers to
documents and versions in the order of their begin times. This ensures that the post-
ings are also ordered by their identifiers in a shard. Subsequently we can use standard
compression techniques, commonly used for posting-list compression, for compressing
shards.
As a result of index sharding, each term in the vocabulary may be associated with
multiple lists. These mappings have to be appropriately reflected in the lexicon. To this
end, we extend the lexicon to store pointers to all the shards for each term. Note that
since our partitions are not local to a given time interval we do not need to maintain time
intervals corresponding to each partition, unlike the lexicon of the vertically-partitioned
index.
Impact Lists For each shard we maintain an additional access structure for efficiently
determining the postings whose time intervals overlap with a query time-interval. These
are called impact lists. An impact list is an associative data structure which maintains,
for every possible begin time of a query time-interval, the position in the shard of the
earliest posting whose valid time overlaps with the query begin time. In other words,
the impact list stores pairs of query begin times (key) and offsets (values) from the shard
beginning. The overall size of each impact list can be reduced by storing only the dis-
tinct offset values rather than offsets for all possible query begin times. An example of
31
Chapter 3. Efficient Indexing and Maintenance for Time-Travel Text Search
[ t0 - t7)
[ t7 - t9)
intervals in
shard
Impact List
query 
begin-time
shard
offset
d21
d12
d51t1 t6
t7
t8 t9
t0
< d12 , [ t0 - t7) >
< d21 , [ t1 - t6) >
< d31 , [ t2 - t4) >
d31
d41
t2
t3
t4
t5
< d41 , [ t3 - t5) >
< d51 , [ t8 - t9) >
Posting list for 
shard
Wednesday, May 15, 13
Figure 3.2.: Impact list
an impact list of a shard of five postings is shown in Figure 3.2. The possible begin times
are represented as intervals and they map to the offset in the posting list for the shard
from where the postings are read sequentially. Thus, for a query interval in [t8 − t11]
we start accessing the list sequentially from the fifth posting. Although represented as
intervals, in practice it is sufficient to store the begin or end of the interval. Thus keys
admit a non-decreasing integer sequence and a straightforward binary search over the
keys efficiently gives the correct offset location. For practical granularities of query be-
gin times such as days, the impact lists for the complete index (i.e., for all shards of all
terms) can be easily kept in memory. We discuss how query processing is performed
using impact lists in the next section.
3.3. Sharding Posting Lists
Index sharding refers to partitioning or sharding a posting list Lv into disjoint partitions
or shards such that no two shards share common postings. We now formally define the
notion of a shard and posting-list sharding.
Definition 3.1 (Shard) A shard σ is a sequence of postings, σ = 〈pi〉, ordered by their begin
32
3.3. Sharding Posting Lists
times, i.e.,
begin(pi) ≤ begin(pi+1).
Definition 3.2 (Posting-List Sharding) Given a posting list Lv for a term v, posting-list
sharding partitions Lv into a set of shards Sv = {σ1, . . . , σm}, σi ⊆ Lv where
(∀i, ∀j i 6= j =⇒ σi ∩ σj = ∅) ∧
(⋃
i
σi = Lv
)
.
In what follows a sharded index refers to an index which employs posting-list shard-
ing. A sharded index thus maintains multiple shards per term and the disjoint parti-
tioning avoids replication completely (see Figure 3.1).
3.3.1. Query Processing over Sharded Index
For query processing over a sharded index, we employ the established term-at-a-time
query-processing model (described in more detail in Chapter 2). In short term-at-a-time
processing posting lists are read one after the other and scores of a document version
from different lists are merged in memory. For our sharded index, each shard of every
query term is processed in a sequence of the following open-skip-scan operations.
1. Open – Open a shard for a query term. This involves a lookup from the lexicon
for the location of beginning of the shard.
2. Skip – Given the query begin time, lookup offset position from the impact list of
the shard and perform a seek to that position. In practice we can combine the open
and skip steps into an open-skip operation to avoid an extra I/O operation.
3. Scan – Perform sequential reads from this position and terminate when it is certain
that the rest of the postings do not overlap with the query time-interval. As we
read the postings sequentially in begin-time order, we can safely terminate when
the begin time of the next posting exceeds the query end-time.
An illustration of the query-processing operation is shown in Figure 3.3. We are given
a sharding 〈d21, d12, d15 〉 and 〈d13, d14 〉. Each of the shards is associated with an impact list,
as illustrated in the figure. For simplicity we denote the offsets from the shard beginning
by the posting which is accessed first.
Consider a time-travel query with time interval [tb, te], shaded in gray, where we
have t3 < tb < t4. Firstly, by the use of impact lists we try to quickly skip to the first
overlapping posting in the shard. For instance, in the second shard, by looking up the
impact list, we start accessing the shard from the second posting onwards. This avoids
33
Chapter 3. Efficient Indexing and Maintenance for Time-Travel Text Search
d21
d12
d31
d41
d51t1
t2
t3
t4
t5
t6
t7
t8 t9
t0
Shard 1
Shard 2
 t4 d31
t5 d41
Impact List for Shard 2
query time interval
 t7 d12
t9 d51
Impact List for Shard 1
open-skip
open-skip
tb te
Friday, May 17, 13
Figure 3.3.: Open-skip and scan operations for query processing
processing of postings at the head of the shard which do not overlap with the query
time-interval. Secondly, the postings are accessed sequentially, from the seek position,
until a posting p is encountered such that begin(p) > te. In the example, we terminate
reading Shard 1 after accessing the first two postings and thus avoid reading d15.
In essence, we only access a relatively smaller number of postings of a shard.
For each query, all the participating shards can be processed in parallel. Since the con-
tents postings per shard are disjoint, merging lists is relatively inexpensive. However,
if the number of shards per term are large in number, query-processing performance
might degrade due to a large number of random seeks (open for each open-seek oper-
ation). We now at look at the different posting-list sharding strategies for optimizing
query performance.
3.4. Idealized Index Sharding
Although the begin-time order in shards avoids wasteful reading of postings, it does
not guarantee the elimination of all wasteful reads. A posting is said to be a wasted
34
3.4. Idealized Index Sharding
d21
d12
d31
d41
d51
t0
t1
t2
t3
t4
t5
t6
t7
t8 t9
Monday, May 20, 13
Figure 3.4.: Subsumption of postings
read, if it is accessed during query processing although its valid-time interval does not
overlap with the query time-interval. We introduce the concept of subsumption in shards
to explain why this occurs.
Definition 3.3 (Subsumption of Postings) For a pair of postings p and q (from the same
posting list), p subsumes q (for short p A q) if
p A q⇔ (begin(p) ≤ begin(q)) ∧ (end(p) > end(q)) .
A shard is said to exhibit subsumption if it has a pair of postings where one sub-
sumes another. Consider a shard in Figure 3.4 with five postings where d21 A d
1
2. Now,
the queries with end(d12) ≤ begin(Q) ≤ end(d21) (highlighted region in the figure) will
wastefully read d12, i.e., d
1
2 is processed but does not contribute to the result. Such a
scenario, where there are postings like d21 which span long intervals and subsume many
postings, can arbitrarily degrade performance. Building on this example, if we further
introduce n postings which are subsumed by d12, they are in turn automatically sub-
sumed by d21. Consequently, the number of wasteful reads when the query has a begin
time end(d12) ≤ begin(Q) ≤ end(d21) will be n+ 1.
We can avoid any wasteful reads of postings if we can avoid shards with subsump-
tions of postings. In other words, we require that postings in a shard satisfy the staircase
property, defined as follows:
Definition 3.4 (Staircase Property) A shard σ is said to have the staircase property if σ has
exactly one posting or
∀p, q ∈ σ, begin(p) ≤ begin(q) ⇒ end(p) ≤ end(q) .
We let the Boolean predicate staircase(σ) denote whether σ has the staircase property.
35
Chapter 3. Efficient Indexing and Maintenance for Time-Travel Text Search
d21
d12
d31
d41
d51
t0
t1
t2
t3
t4
t5
t6
t7
t8 t9Shard 1
Shard 2
Shard 3
Wednesday, May 15, 13
Figure 3.5.: Idealized sharding with staircase property
Clearly, it may be possible to shard a given posting list in many different ways so
that the t ircase property is satisfied. Since query processing proceeds by open-skip
operations for all shards of a term, it is desirable to minimize the number of idealized
shards. This can be cast into an optimization problem with the input being a list of
postings Lv. The objective is to partition Lv into a feasible index sharding S so as to
minimize the overall number of shards where each shard exhibits the staircase property.
Formally, the idealized-sharding problem is defined as follows:
Definition 3.5 (Idealized-Sharding Problem)
argmin
S
|S | s.t. ∀σ ∈ S : staircase(σ) .
where S is a feasible index sharding of Lv.
3.4.1. Optimal Algorithm for Idealized Sharding
We solve the idealized-sharding problem optimally using the greedy algorithm de-
scribed in Algorithm 1. We let the last posting added to σ be denoted by last(σ). We
additionally let each shard σ be associated with an end time σ.end representing the end
time of last(σ), i.e., σ.end = end(last(σ)).
We process all postings of a list Lv in increasing begin-time order. In each iteration, we
try to add a posting Lv[i] to an existing shard if the end time of Lv[i] is greater than the
end time of the shard, i.e., end(Lv[i]) ≥ σ.end. If there are multiple shards to which Lv[i]
can be assigned, we add it to the shard with the minimum gap, i.e., end(Lv[i]) − σ.end.
If there are currently no shards to which Lv[i] can be added, we start a new shard with
Lv[i] in it.
36
3.4. Idealized Index Sharding
Algorithm 1: Idealized sharding algorithm
1: Input: Lv sorted in increasing order of begin times
2: Iv = ∅ // Idealized sharding
3:
4: for i = 1 .. |Lv| do
5: // Iterate over all postings in the posting list for v
6: if ¬∃σ ∈ Iv : σ.end ≤ end(Lv[i]) then
7: create new shard σnew
8: σnew.end = 0
9: Iv = Iv ∪ {σnew}
10: end if
11: σt = argmin
σ∈Iv
(end(Lv[i]) − σ.end)
12: σt.end = end(Lv[i]) // Update the end time of the shard
13: σt = σt ∪ {Lv[i]} // Include the current posting into the shard
14:
15: end for
16:
17: Output: Iv is the idealized sharding.
3.4.2. Proof of Optimality
We develop the proof of optimality of Algorithm 1 by first proving three lemmas about
key properties of the generated shards. Let the shards created by Algorithm 1 for a list
Lv be numbered by their order of creation starting with σ1, i.e., σ1 was created before σ2
and so on.
The first lemma states that the algorithm produces only shards that have the staircase
property.
Lemma 3.1 (Staircase Property) When Algorithm 1 terminates, every shard created by the
algorithm has the staircase property.
Proof: We show this by contradiction. Assume that there is a shard σ that does not have the
staircase property. This means that there is a pair of postings p and q in this shard such that
begin(p) ≤ begin(q), and end(p) > end(q) or q @ p. Since begin(p) ≤ begin(q), p was
added to the shard before q. But when p was added, the end time of the shard was set to end(p)
or σ.end = end(p). Thus q could not have been be added to the same shard, which contradicts
our assumption. 
Lemma 3.2 (Descending-End Times) If Algorithm 1 created a shard σi before σj, i.e. i > j,
then σi.end > σj.end.
37
Chapter 3. Efficient Indexing and Maintenance for Time-Travel Text Search
Proof: We prove this property by induction over increasing number of postings added.
i = 1: For the first posting e1 the property holds since there are no earlier shards.
i→ i + 1: Let there be n existing shards G = {σ1 · · ·σn}. Depending on the end time of the
i+ 1th posting, i.e., end(ei+1) we consider two cases:
If end(ei+1) is less than all the existing shard ends (end(ei+1) < σk.end , ∀σk ∈ G), then
ei+1 forms a new shard and the end of the shard is less than all the existing ends. This proves the
claim.
Due to the induction hypothesis, the end times of shards are sorted in the descending order
i.e., σ1.end > σ2.end > · · · > σn.end. If end(ei+1) is greater than any of the shards, then
by definition (line 6), it will have to be added to the shard which minimizes the difference of their
end times. It is easy to see that this shard is the earliest shard which can accommodate ei+1 since
any other shard will either violate the staircase property or have a greater difference. This proves
the claim. 
Lemma 3.3 (Temporal Subsumption of Postings) For every posting in a shard σi (i > 1)
there exists a posting in σi−1 which completely subsumes it.
Proof: When a posting p is added to shard σi it is the last added posting, i.e., last(σi) = p.
Since we process all the postings in begin-time order, all postings which have been placed into
shards before have a begin time less than begin(p). And, from the property of descending-end
times end(last(σi−1)) > end(last(σi)). Thus, at this current execution state of the algorithm
the posting last(σi−1) completely subsumes p, i.e., last(σi−1) A last(σi). 
We now introduce the notion of a stalactite set of time intervals which is essential for
the rest of the proof.
Definition 3.6 (Stalactite Set) A stalactite set Υ consists of time intervals such that,
∀p, q ∈ Υ, begin(p) ≤ begin(q)⇒ end(p) > end(q).
There may be many such stalactite sets that can be formed using postings from a given
posting list, Lv. Let us denote the stalactite set of maximum cardinality as Υmax(Lv).
Lemma 3.4 (Stalactite property) The number of shards created by Algorithm 1 for a list Lv is
equal to |Υmax(Lv)|.
Proof: We prove the lemma by contradiction. Assume that the new posting to be added enew
is not a part of |Υmax(Lv)|. We also assume that its addition creates a new shard σ|Υmax(Lv)|+1
which is more than the claimed |Υmax(Lv)| shards. Since the postings arrive in begin-time or-
der, begin(enew) is greater than any of the previously processed postings. This means that
end(enew) < end(σ|Υmax|) for it to start a new shard σ|Υmax|+1. Now Lemma 3.3 says that
there exists a posting in σ|Υmax| which subsumes e, making e a part of a larger stalactite set of
38
3.5. Cost-Aware Merging of Shards
cardinality |Υmax|+1which is contrary to initial assumption thatΥmax is the maximal stalactite
set. 
We can now prove the optimality of our algorithm for idealized sharding.
Theorem 3.1 Algorithm 1 creates an optimal sharding.
Proof: Lemma 3.1 establishes that there are no subsumptions in a shard. Since Υmax(Lv) is the
maximum size stalactite set, from Lemma 3.1, the overall number of shards are lower bounded by
|Υmax(Lv)|. However, Lemma 3.4 proves that we exactly obtain |Υmax(Lv)| shards by idealized
sharding. This proves that the optimal solution to the idealized sharding of Lv has |Υmax(Lv)|
shards thus proving the optimality of Algorithm 1. 
Further, we show that the algorithm can be implemented efficiently, by making use of
the descending-end times property of the sharding at any stage during the algorithm.
Due to this ordering of shard ends the addition step of postings to shards can be effi-
ciently implemented via a binary search over the shard ends.
3.5. Cost-Aware Merging of Shards
Depending on the distribution of valid-time intervals, the idealized sharding introduced
in the previous section might generate a large number of shards. Each shard requiring
one open-seek operation, involving a random seek. If the cost of such a random seek is
high and if the distribution of time intervals gives rise to many idealized shards, query-
processing performance can degrade. In such cases, it might actually be beneficial to
reduce the number of shards arising from idealized sharding at the cost of allowing
some wasted reads.
In this section, we present an I/O cost-aware technique to selectively merge idealized
shards allowing for a controlled amount of wasted reads while reducing the number of
random seeks. We introduce a model for merging idealized shards which limits sequen-
tial wasted reads due to merging of a set of idealized shards by taking into consideration
costs of random seeks and sequential accesses of the underlying index.
3.5.1. Model for Shard Merging
Multiple shards can be merged into a merged shard which contains all postings of the
input shards and have a begin-time order on the intervals associated with the postings
(see Figure 3.6). We denote a shard created from merging an input set of shards S as
µ(S). In our model we take as input an idealized sharding Iv of a posting list Lv. Our
intention is to reduce the overall number of shards per posting list by identifying ideal-
ized shards which can be merged according to our cost model. To this end we propose
39
Chapter 3. Efficient Indexing and Maintenance for Time-Travel Text Search
d21
d12
d31
d41
d52
d21
d12
d31
d41
d52
Idealized Shards Merged Shards
Merging
Friday, May 17, 13
Figure 3.6.: Cost-aware shard merging
to disjointly partition the input set Iv into sets or partitions of shardsMv. Merged shards
µ(S) are then created using shards from each partition S ∈Mv. The partitioning thereby
ensures that no two merged shards have postings from the same idealized shard. The
resulting merged shards, µ(S) , S ∈Mv, thus formed also are a feasible index sharding
of Lv, i.e.,
⋃
S∈Mv
µ(S) = Lv.
Merging of shards might result in subsumptions of postings which further leads to
wasted reads during query processing. However, not all query time-points lead to
wasted reads. For example, in Figure 3.4 only queries which have a begin time inter-
val [t6, t7] result in wasted reads. Secondly, an open-seek operation to a merged shard
accompanied by a few sequential wasted reads may be cheaper than two open-seek
operations to idealized shards without any wasted reads. These are two factors which
should be taken into considerations in choosing which shards to merge. An accurate
estimate of the performance of a shard can be modelled by considering the performance
over all query time-points.
Cost Model We let the cost of a random seek be Cr, and that of a sequential read be
Cs. We allow for a penalty function Ψ(S), over a set of shards S and require it to be
bounded by the parameter η. To reconcile the costs of random and sequential accesses
the parameter η is set to Cr/Cs − 1. We refer to this bounding of the penalty function as
the threshold criterion.
40
3.5. Cost-Aware Merging of Shards
Ψ(S) ≤ η
An example of such a penalty function is expected wasted reads, which is defined as the
number of wasted reads incurred during query processing, averaged over all possible
query time points. We definew(σ, t) as the set of wasted postings read for a query with
a time-point t over a shard σ. We consider a discrete notion of time and denote the time
granularity as δ. If all possible query times lie in the interval [t0, tn] the number of valid
query time-points is tn−t0δ . Formally,
Definition 3.7 (Expected Wasted Reads) Given a merged shard µ(S) and its wasted read
distribution w(µ(S), t), the expected wasted reads incurred per query time-point is given by
Ψ(S) =
∑
t∈[t0,tn] |w(µ(S), t)|
(tn − t0)/δ
.
Under this penalty function, a set of shards can be merged when the expected wasted
sequential reads in the merged shard is less than the overhead incurred in an open-seek
operation. If costs of sequential and random accesses are the same, Cr = Cs, we obtain
η = 0 which means we resort to idealized sharding. Understandably, with Cr = Cs
the cost of accessing a new shard is the same as reading a wasted posting. Thus it is
preferable to avoid wasted reads completely in this situation and employing idealized
sharding. However, in more practical scenarios Cr > Cs. As an example, if η = 100,
then the wasted reading of less than 100 postings, that do not qualify by the temporal
constraint, on an average would be more beneficial than performing a random seek to
an additional shard. One can, in principle, use other notions of aggregation measures for
w(σ, t) and then define the penalty function accordingly. In this work we use expected
wasted reads as the penalty function.
The partitioning of idealized shards can now be formulated as an optimization prob-
lem where we have as input a set of idealized shards, the parameter η, and the penalty
function. Like the idealized-sharding problem we would want to minimize the overall
number of shards for minimizing the expensive open-skip operations. The cost-aware
shard merging problem intends to find a partitioning Mv of idealized shards Iv so as to
have minimum number of partitions subject to the threshold criterion on each partition.
Formally,
Definition 3.8 (Cost-aware Shard Merging Problem)
argmin |Mv| s.t. Ψ(S) ≤ η : ∀S ∈Mv.
41
Chapter 3. Efficient Indexing and Maintenance for Time-Travel Text Search
To solve this problem we first look at how we can efficiently determine the penalty
of merging a partition or set of idealized shards Iv. A partition containing at least two
idealized shards will have a non-zero penalty. To compute the overall penalty value
of a partition we need to aggregate the wasted read distribution for the combination
of shards in the partition over the entire time-period. The combinatorial nature of the
problem makes it prohibtive to pre-compute penalty values for all combinations. What
we do instead is compute penalty values of pairs of shards and show that the penalty of
any partition (≥ 2) can be computed efficiently from these pairwise penalties.
Given a partition S of m idealized shards the wasted reads at a query time-point t
is w(µ(S), t). We retain the order in which idealized shards were created, i.e., shards
created early have a lower index. We further let first(S) denote the shard which was
created first in S . For shards σi, σj, σk ∈ S and i < j < k, it holds
w(µ(i, k), t) = w(µ(j, k), t).
Thus w(µ(S), t) =
∑
σ∈S Ψ({first(S), σ}). The penalty Ψ(S) follows from this observa-
tion and Definition 3.8:
Ψ(S) =
∑
σ∈S
Ψ({first(S), σ}).
As an example, the penalty incurred due to merging idealized shards {7,10,3,12}
would be Ψ(3, 7) + Ψ(3, 10) + Ψ(3, 12). Computing wasted reads at each time point
can be efficiently implemented by interleaving computation of pairwise wasted reads
within Algorithm 2.
3.5.2. Algorithm for Shard Merging
We present a heuristic algorithm which is shown to perform well in practice in our
experimental evaluation. As inputs we expect the set of the idealized shards and η. We
retain the order in which idealized shards were created, i.e., earlier created shards have
a lower index.
The pseudo code for merging the idealized shards is presented in Algorithm 2. Every
iteration employs a two stage greedy process. The first stage is an ascending choice phase
in which it chooses all the unmerged/available idealized shards in ascending order of
their index until the threshold constraint is violated (lines 11 to 20).
The second stage is a greedy phase (lines 23 to 27) where the remaining capacity is
greedily chosen with smallest unmerged shard first (as in the standard greedy approach
to the knapsack problem).
42
3.6. Index Maintenance
Algorithm 2: Cost-Aware Shard Merging
1: Input: Iv and Ψ(σi, σj)
2: Mv = ∅ // Merged shards
3:
4: for i = 1 .. |Iv| do
5: Let σi ∈ Iv \Mv be next shard in order
6: create new shard ri
7: ri = ri ∪ {σi}
8: capacity = η
9:
10: // Ascending choice phase
11: for j = i+ 1 .. |Iv| do
12: if (Ψ(σi, σj) ≤ capacity) ∧ (σj /∈Mv) then
13: capacity = capacity− Ψ(σi, σj)
14: ri = ri ∪ {σj}
15: else
16: if (σj /∈Mv) ∧ (σj /∈ ri) then
17: break
18: end if
19: end if
20: end for
21:
22: // Smallest size first
23: while capacity > 0 do
24: σmin = argmin
g∈Iv\Mv
{Ψ(σi, g)}
25: capacity = capacity− Ψ(σi, σmin)
26: ri = ri ∪ {σmin}
27: end while
28: Mv =Mv ∪ ri
29: end for
30:
31: Output: Mv is the set of merged shards.
3.6. Index Maintenance
So far, we have assumed a static document collection. These indexing techniques are
fine for collections that change infrequently or never. In such cases, occasionally re-
43
Chapter 3. Efficient Indexing and Maintenance for Time-Travel Text Search
building indexes is a viable solution in practice. However, for web archives, which
grow frequently and dynamically, the index needs to be updated frequently. New terms
are added to the index and shards for existing terms are modified. In this section we
describe index-maintenance strategies to deal with dynamic updates. We first discuss
our assumptions about the nature of the updates in an archive setting and the factors
relevant for indexing.
Updates in web archives are a result of periodic crawls. An update might result in
either (i) new unseen document, or (ii) report modifications to an already existing doc-
ument. Recollect that in Section 3.2 we established the document model where each
document is a sequence of versions. Modifications to an existing document, either ad-
dition or deletion of content, results in the creation of a new version in the document
version sequence. If a new document is reported, a new sequence is started for the doc-
ument with the reported version being the first. Importantly, existing versions are never
removed. This means that the index steadily grows over time with the older collection
indexed by the archive index being a subset of the current collection. We assume that
deletions in the past are rare and in our current system arbitrary deletions in the past
are not supported.
Each version is associated with a valid-time interval. Every update results in a change
in the valid-time interval of the current version of each document. Consider the state of
the archive at time tnow when it was updated. If the update reports no change in the
state of a document version d4i (document d at its fourth version) the end of the valid-
time of d4i is updated to tnow. On the contrary if a new version for d would have been
reported, the end time of d4i would have been finalized to tnow (never to be modified
again). Additionally, the fifth version d5i would have been started with a begin time as
tnow. An accurate estimate of the valid-time intervals of the versions are determined
when they are finalized, and hence they are sent to the indexing pipeline in the order of
their end times.
With these assumptions in mind we distinguish between active versions, as the most
recent and still current document versions, and archive versions, as the document ver-
sions already superseded by a more recent version of the same document. The active
version of a document turns into an archive version when the document is re-crawled
and either a new active version is found or the document was removed from the Web.
In both cases, the end time of the old version is set to the current time.
During indexing we organize postings for active versions into an active index and
the archived versions are indexed in an archive index. The active index is implemented
as an incrementally updatable inverted index [BC08, LMZ08]. Depending on the frac-
tion of time-travel queries among all queries posed to the system, posting lists in the
active index can be organized to efficiently support queries on active versions or time-
44
3.7. Incremental Sharding
travel queries. For the former, postings may be ordered by their document identifier
to allow for more efficient query processing, possibly together with additional struc-
tures [BCH+03, DS11, MZ96, SC07]. For the latter, postings may be ordered by the begin
boundary of their valid-time interval to support efficient filtering of recent document
versions.
The archive index on the other hand is implemented as a sharded index and is our
primary focus. Finally, from the indexing point of view it is preferable to have an up-
dating scheme which appends newly created postings at the end of the posting list. The
major benefit of an append operation is that the indexes built for the older indexes can
be used as partial solutions to build an up-to-date archive index efficiently thus avoid-
ing recomputation. Recomputation of shards with a non-append based technique (say
cost-aware shard merging) is expensive because it involves processing the entire input
(existing data along with the new updates) thus limiting its applicability to an update
aware indexing system.
To this end, we develop incremental sharding which
• is competitive in query processing by trading-off, like shard merging, wasted
reads for random-accesses,
• takes into account the end-time order of arrival of input, and
• can be maintained easily because it can be incrementally computed and results in
append-only operations to shards.
3.7. Incremental Sharding
Incremental sharding takes into account the relative costs of random and sequential ac-
cesses by a more restrictive bound on the number of subsumptions for a given shard.
It bounds the absolute number of subsumptions for a given shards. This is opposed to
shard merging where the number of subsumptions per shard is on an average limited
to a system-wide parameter. A bound on the number of subsumptions per shard trans-
lates to bounding the number of sequential wasted reads. We term this restriction as
the bounded subsumption property. A shard is said to exhibit a bounded subsumption
property if a posting in that shard does not subsume more than η postings, where η is a
system-wide constant (same as in sharding merging). Formally,
Definition 3.9 (Bounded Subsumption) A shard σ is said to satisfy the bounded subsump-
tion property with threshold η if each posting pi ∈ σ does not subsume more than η postings:
∀pi ∈ σ : | {pj ∈ σ | pj 6= pi ∧ pi A pj} | ≤ η .
45
Chapter 3. Efficient Indexing and Maintenance for Time-Travel Text Search
term “v” tnow
d11
d23
d39
d42 d43
d42 sent to the Archive Indexing system d43 in Live Index
crawl 1 crawl 2
time
Friday, May 17, 13
Figure 3.7.: End time order of finalizing versions
We let the Boolean predicate bounded(σ, η) denote whether the shard σ has the bounded-
subsumption property with a threhold η.
The problem of minimizing the number of random accesses with a limited number of
wasted sequential accesses can be redefined in terms of minimizing the overall number
of shards such that each shard exhibits the bounded subsumption. Formally,
Definition 3.10 (Incremental Sharding Problem) Given a set of postings Lv for a term v,
partition Lv into a feasible index sharding S = {σ1, . . . , σm},
argmin
S
|S | s.t. ∀σ ∈ S : bounded(σ, η) .
Before attempting to solve the above problem let us revisit the properties in an archive
indexing setting. Firstly, the archive setting is very specific in terms of arrival of the in-
put sequence, i.e., in the order of arrival of new versions to the archive index. Whenever
the end time of an existing version is determined, it is sent to the archive indexing sys-
tem (see Figure 3.7). Since versions are generated in end-time order, the input intervals
also follow the same order.
Secondly, we do not deal with deletions of versions since a deletion of a document
results in a posting in the archive index for that document, and existing versions are
never removed.
Finally, we would want to avoid recomputation of shards by only allowing appends
to the materialized shards. Apart from avoiding recomputation ensuring an append-
only operation also avoids expensive decompression and compression cycles. While
46
3.7. Incremental Sharding
Shard buffers
Inserted 
incoming 
interval
Appended 
popped 
interval
Archive Index Shards
buf(si)
si
Monday, February 13, 2012
Figure 3.8.: Incremental sharding
merging two posting lists by employing recomputation, entire posting lists are decom-
pressed and re-ordered (according to posting-begin times) to form usable input for the
sharding algorithm. After recomputation, the new set of shards are compressed back
again for storage. The most popular compression algorithms used in posting list com-
pression are based on gap-encoding schemes which are local schemes and hence append
friendly. Hence, rather than optimally solving the problem, we look for an approach
which is based on an append-only operation to the existing shards and exploits the
end-time order of input arrival. In the following section, we introduce the incremen-
tal sharding algorithm which apart from being incremental also has an approximation
guarantee.
3.7.1. Incremental Sharding Algorithm
We present the incremental sharding algorithm which is an update aware incremental al-
gorithm with a factor (2− 2η+2) approximation guarantee (see Algorithm 3). Apart from
having the natural benefit of being an append-only algorithm, it also exploits the end-
time arrival order of the postings by processing the postings in that order thus avoiding
expensive sort operations on the input.
The algorithm processes postings in the increasing order of end times (see line 1) and
creates or updates shards incrementally. It follows a scheme of immediate assignment
but deferred append of a posting to a shard. For each shard, we maintain a shard buffer of
size η+1 and a shard-begin time. The assignment of the posting to a shard is based on the
begin time of the shard and the shard buffer defers the actual writing or appending of
the posting of the shard to satisfy the bounded subsumption property. In other words,
the shard buffer maintains the posting until it deems it right to be appended to the end
47
Chapter 3. Efficient Indexing and Maintenance for Time-Travel Text Search
Algorithm 3: Incremental sharding algorithm
1: Input: (i)η, (ii)Lv sorted in increasing order of end times
2: S = ∅ // Incremental sharding
3:
4: for i = 1 .. |Lv| do
5: //Creates new shard
6: if ¬∃S ∈ S : S.begin ≤ begin(Lv[i]) then
7: create new shard σnew and buf(σnew)
8: σnew.begin = 0
9: add Lv[i] to buf(σnew)
10: S = S ∪ {σnew}
11: end if
12:
13: //Find the best candidate shard for assignment
14: σcand = {σi | σi ∈ S ∧ σi.begin ≤ begin(Lv[i])}
15: σt = argmin
g∈σcand
(begin(Lv[i]) − g.begin)
16:
17: //Update buffers and begin times of shards
18: if σt 6= σnew then
19: insert Lv[i] into buf(σt) in begin-time order
20: end if
21: if |buf(σt)| = η+ 1 then
22: //first element in the buffer finalized
23: σt = σt ∪ removefirst(buf(σt))
24: σt.begin = begin(first(buf(σt)))
25: end if
26: end for
27:
28: //Finally append the buffer postings to the shards
29: for S ∈ S do
30: S = S ∪ buf(S)
31: end for
32:
33: Output: S is the incremental sharding.
of the shard.
When a posting Lv[i] is processed, it is either assigned to an existing shard based on
48
3.7. Incremental Sharding
the posting and shard-begin times (see line 15), or it results in the creation of a new
shard (lines 7-10). The creation of a new shard involves setting the begin time of the
shard to zero and placing the chosen posting in its shard buffer. Until the buffer reaches
its capacity of η the begin time of the shard remains zero. The shard-begin time is first
updated when a posting is popped out of it after the buffer reaches its capacity η + 1.
When the assignment for the posting is decided, say σt, it is placed in the respective
shard buffer buf(σt) (see line 19). The incremental sharding chooses the shard whose
begin time has the least difference with the begin time of the incoming posting (see line
15).
The shard buffers determine the relative position of the postings in the shard where
it will be finally stored. The insertions into the buffer are made to preserve the begin-
time order (see line 19) which in turn ensures a begin-time order when postings are
removed from it. This is shown in Figure 3.8. Only the first posting or the posting with
the minimum begin time is removed from the buffer to limit the buffer size to η + 1
(line 23) and it is appended to the end of its corresponding shard σt. The shard buffers
also ensure that no posting in a shard subsumes more than η postings. This is done by
setting the begin time of a shard σt.begin to the first posting begin(first(buf(σt)))(or
the posting with the least begin time) of the shard buffer as in line 24. The posting with
the minimum begin time in the shard can subsume the maximum number of postings
and any posting with a begin time lesser than it is disallowed.
Note that the use ∪ in lines 23 and 30 indicates the append operation on the shard that
is logically organized as a list of postings in their begin-time order.
3.7.2. Approximation Guarantee for Incremental Sharding
Theorem 3.2 Incremental sharding is a (2− 2η+2) approximation.
We use the following lemmas to prove the theorem. Assuming that incremental
sharding produces m shards, we first construct a worst case scenario. For notational
convenience let us assume that shards are numbered according to their creation times
in incremental sharding, i.e., σ1 was created before σ2 and so on.
Lemma 3.5 (Descending Begin Times) If incremental sharding created a shard σi+1 after
σi, then σi.begin > σi+1.begin.
Proof: We prove this property by induction over increasing number of postings pi ∈ Lv which
are added in in end-time order, i.e, end(pi+1) > end(pi) .
i = 1 : For the first posting p1 the property holds since there are no earlier shards.
i→ i + 1: Let there be n existing shards S = {σ1 · · ·σn}. Depending on the begin time of the
(i+ 1)-th posting begin(pi+1) we consider the following two cases:
49
Chapter 3. Efficient Indexing and Maintenance for Time-Travel Text Search
Case 1: If begin(pi+1) < σk.begin , 1 ≤ k ≤ n, then pi+1 forms a new shard σnew and
the begin of the shard σnew.begin is less than all the existing shard-begin times. This proves the
claim.
Case 2: Assuming ∃σk : begin(pi+1) ≥ σk.begin and on addition of pi+1 to σk there is a
violation of the descending begin-time order of shards, i.e., σk.begin > σk−1.begin. This means
that σk−1.begin < begin(pi+1) and pi+1 should have been assigned to σk−1 due to a smaller
difference according to the induction hypothesis of σk+1.begin > σk.begin , ∀1 ≤ k < n. 
Lemma 3.6 (Incremental Subsumption) A posting added to σi subsumes at least (i−1)(η+
1) postings.
Lemma 3.7 (Incremental Subsumption) The number of postings subsumed a posting added
to σi is at least (i− 1)(η+ 1).
Proof: Note that σi.begin refers to the time of the first posting in the buffer of shard σi or the
earliest begin time in the shard buffer buf(S).
By Lemma 3.5 we know that there is an ordering of the shard-begin times. Hence for a posting
p assigned to σi the following holds – begin(p) < σi−1.begin < · · · < σ1.begin. Since we
assume end-time arrival order of postings, it subsumes all postings in i−1 shards buffers, which
are σ1, ..., σi−1. Further the fixed buffer size of η + 1 results in making the subsumptions lower
bounded by (i− 1)(η+ 1). 
We now introduce the notion of stalactite groups, building on the notion of the Stalactite
set Υ according to the definition 3.6. Formally,
Definition 3.11 (Stalactite Groups) A shard σ is said to be exhibit stalactite grouping if we
can partition the shard into sets of postings called groups G such that for groups si, sj ∈ σ and
i < j the following holds:
q A p, ∀p ∈ si, ∀q ∈ sj.
In other words, stalactite groups (see Figure 3.9) in a shard is the organization of post-
ings into groups such that choosing one posting from each group results in a stalactite
set.
Lemma 3.8 (Stalactite Grouping) Stalactite grouping with a staircase property in each shard
is the worst case for incremental sharding.
Proof: From the previous lemma, any input resulting in m shards from incremental shard-
ing will have at least postings with η + 1, 2(η + 1), · · · , (m − 1)(η + 1) subsumptions in
σ2, σ3, · · · , σm respectively. Let us suppose that the set of subsumed postings when the first
posting added to σi be represented as sub(σi). To reduce the overall number of shards we should
strive for a configuration with a minimum number of subsumptions. This is possible when
50
3.8. System Architecture
...
Sm
Sm-1
S1⌘ + 1
Wednesday, June 6, 12
Figure 3.9.: Stalactite groups
sub(σ1) ⊆ sub(σ2) ⊆ · · · ⊆ sub(σm) and |σi| = η + 1, ∀i = 1, . . . ,m. This arrangement
forms a stalactite group (see Figure 3.9) with each of the groups having a cardinality of η+ 1.
Additionally, each of these stalactite groups should have a staircase arrangement to allow
for the maximum capacity - η - out-of-place insertions. Any additional posting or misaligned
posting either increases subsumption or reduces capacity for out of place insertions. Any removal
of postings on the other hand result in contradiction to the original assumption that there arem
shards from incremental sharding. 
Now we can complete the proof for Theorem 1.
Proof: From the Lemma 3.8 we know that there arem stalactite groups with each group residing
in the shards formed from incremental sharding. It is easy to see that none of the optimal shards
will have more than 2η + 1 postings. Thus we choose an assignment where we try to minimize
the number of shards, i.e., choose as many shards with 2η + 1 postings as possible. One such
assignment is when we assign η of the η+ 1 postings of σi to σm+1−i, ∀i ≤ m2 . The remaining
m
2 postings (a posting from each σi) can then be placed in
m
2(η+1) shards. Hence for m shards
created by incremental sharding we get a minimum of m2 +
m
2(η+1) shards. Notice that we can
have other arrangements which give the same number of minimum shards. The ratio
|S|
OPT
=
m
m
2 +
m
2(η+1)
= 2
(
1−
1
η+ 2
)
proves that incremental sharding is a factor (2− 2η+2) approximation algorithm. 
3.8. System Architecture
Figure 3.10 shows a high-level overview of the architecture of a search engine using our
incremental sharding method. It consists of
• the active index for all active versions of documents, consisting of an in-memory
inverted list for each term that keeps the active versions of documents,
• the archive index for all archive versions of documents, consisting of an inverted
list for each term that is organized in shards. The archive index consists of an
in-memory index IMAI and an on-disk index EMAI, both organized in shards.
51
Chapter 3. Efficient Indexing and Maintenance for Time-Travel Text Search
Active Index
Archive Index
IMAI EMAI
Crawls
Monday, February 13, 2012
Figure 3.10.: System architecture
• A crawler that continuously crawls the target Web sites, for example, a predefined
set of domains or the complete Web.
When the crawler encounters a new document that has been unknown so far, it adds
it to the active index; this is an inexpensive operation since the active index is in main
memory. When a document is found again, it is checked for changes (using, for example,
a fingerprinting technique such as [BGMZ97, Hen06]). If changes are detected, the active
version of that document turns into an archive version (with end time equal to the crawl
time) and is sent to the archive index, and postings for the new active version are added
to the active index.
The archived version is then added to the in-memory archive index by first creat-
ing the corresponding postings for each term, which are then added to the in-memory
archive index using the incremental technique from Section 3.7. Figure 3.7 shows an
example for this, where in a crawl at time tnow a new version d34 for document d4 is
detected. This results in firstly adding a new version d34 with a begin time tnow to the
affected terms in the active index. Secondly, the end time of d24 is finalized and the post-
ing 〈d24, [t1, tnow], score 〉 with the complete posting information is sent to the archive
indexing system. In the archive indexing system this posting is processed by placing it
in the shard buffer of term “v” and updating the IMAI from the popped posting in the
buffer as shown in Figure 3.8. As soon as the in-memory index IMAI is full, postings are
merged into the disk-based archive index EMAI, merging corresponding shards; this
essentially corresponds to incremental maintenance of standard inverted lists.
3.9. Experimental Evaluation
In this section, we describe our experimental evaluation of index sharding. We first
present our experimental setup, datasets, and workloads used. We then examine in
detail the impact of all indexing methods considered on query processing, index sizes
and index maintenance.
52
3.9. Experimental Evaluation
Dataset Coverage Size (in GB) N V µ/σ
WIKI 2001 to 2005 ∼700 1,517,524 15,079,829 9.94 / 46.08
UKGOV 2004 to 2005 ∼400 685,678 17,297,548 25.23 / 28.38
Table 3.2.: Characteristics of datasets used
3.9.1. Setup
All experiments were conducted on Dell PowerEdge M610 servers with 2 Intel Xeon
E5530 CPUs, 48 GB of main memory, a large iSCSI-attached disk array, and Debian
GNU/Linux (SMP Kernel 2.6.29.3.1) as operating system. Experiments were conducted
using the Java Hotspot 64-Bit Server VM (build 11.2-b01).
3.9.2. Datasets Used
For our experiments we use the following two real-world datasets WIKI and UKGOV.
The characteristics of these datasets are detailed below and summarized in Table 3.2.
WIKI The English Wikipedia Revision History [WIK13], whose uncompressed raw data
amounts to 0.7 TBytes, contains the full editing history of the English Wikipedia
from January 2001 to December 2005. We indexed all versions of encyclopaedia
articles excluding versions that were marked as the result of a minor edit (e.g., the
correction of spelling errors etc.). This yielded a total of 1,517,524 documents with
15,079,829 versions having a mean (µ) of 9.94 versions per document at standard
deviation (σ) of 46.08.
UKGOV This is a subset of the European Archive [EA13], containing weekly crawls of
eleven governmental websites from the U.K. We filtered out documents not be-
longing to MIME-types text/plain and text/html to obtain a dataset that to-
tals 0.4 TBytes. This dataset includes 685,678 documents with 17,297,548 versions
(µ = 25.23 and σ = 28.38).
These two datasets represent realistic classes of time-evolving document collections.
WIKI is an explicitly versioned document collection, for which all its versions are known.
UKGOV is an archive of the evolving Web, for which, due to crawling, we have only in-
complete knowledge about its versions. The incomplete knowledge can be attributed
to inability to capture some versions in between crawls and inability to determine with
certainty the version valid-times. For ease of experimentation, we rounded timestamps
(in both datasets) to day granularity.
53
Chapter 3. Efficient Indexing and Maintenance for Time-Travel Text Search
3.9.3. Index Management
We use the following types of indexes in our experiments:
1. Sharded Index We consider idealized sharding (IS) and three variants of cost-
aware shard merging (CAS) as introduced in Section 3.5. The parameter η reflects
the I/O cost ratio. Since the Cr/Cs values of disks usually vary in the order of
100 and 1000 thus we choose the parameter for shard merging accordingly, i.e.,
η ∈ {10, 100, 1000}. The corresponding indexes are denoted as CAS-10, CAS-100
and CAS-1000. The penalty function used for shard merging is based on expected
wasted reads.
We also consider indexes created with incremental sharding (INC) as described in
Section 3.7, with the same parameter values for CAS, i.e., η ∈ {10, 100, 1000}.
2. Vertically-Partitioned Index As the first competitor, we consider the vertically-
partitioned index, referred to as VERT from now on, that are partitioned follow-
ing the space-bound approach [BBNW07]. The parameter κ denotes the space re-
striction that models the maximum blowup in the index size relative to a non-
partitioned index. For our experiments we consider four variants of the space-
bound approaches i.e., parameter values for κ ∈ {1.5, 2.0, 2.5, 3.0}. These variants
are denoted subsequently in the text as VERT-1.5, VERT-2.0, VERT-2.5 and VERT-
3.0.
3. Naı̈ve Unpartitioned Index As a second competitor, we build an unpartitioned
index with provision for impact lists over ordered begin times referred to as CAS-
inf. This serves as a proof that our techniques are effective not only because of the
impact list construction and a global begin-time order.
We also evaluate the effect of temporal coalescing [BBNW07] on index size and query
processing. To this effect we build sharded and vertically-partitioned indexes with ap-
plication of temporal coalescing using a parameter  = 0.01. Other than that, we use the
same choice of parameters as in the experiments without temporal coalescing.
Both the vertically-partitioned indexes and the sharded indexes are stored on disk us-
ing flat files containing both the lexicon as well as the partitioned posting lists. We do
not filter out stop words, nor do we apply stemming/lemmatization when indexing the
above datasets. We assigned document identifiers in the order of the begin time of the
document versions. For compression, we employ 7-bit encoding on d-gaps and apply
temporal coalescing whenever necessary. Note that variable-byte encoding is comple-
mentary to temporal coalescing. We use scalar payloads and store the tf-scores as floating
point numbers using eight bytes.
54
3.10. Experimental Results
At runtime, the lexicon and impact lists are read completely into main memory, and
for a given query the appropriate partitions or shards are retrieved from the index flat
file on disk.
3.9.4. Query Workloads and Execution
We compiled two dataset-specific query workloads by extracting frequent queries from
the AOL query logs, which were temporarily made available during 2006. For the WIKI
dataset we extracted 300 most frequent queries which had a result click on the domain
en.wikipedia.org and similarly for UKGOV we compiled 50 queries which had re-
sult click on .gov.uk domains. Both the vertically-partitioned and sharded index struc-
tures are built for terms specific to the query workload. Using these keyword queries,
we generated a time-travel query workload with five instances each for the following
four different temporal predicate granularities: day, month, year and queries spanning
the full lifetime of the respective document collection. Hence, the workload comprised
of 6000 time-travel queries for WIKI and 1000 queries for UKGOV.
For query processing, we employed conjunctive query semantics, i.e., query results
contain documents that include all the query terms. We use wall-clock times (in millisec-
onds) to measure the query-processing performance on warm caches using only a single
core. Specifically, each query was executed five times in succession and the average of
the last four runs was taken for a more stable and accurate runtime measurement.
3.10. Experimental Results
3.10.1. Sharding vs Vertical Partitioning
In the first set of experiments, we compare the performance of CAS and VERT on dif-
ferent query granularities. Query granularity of a time-travel text query refers to the
time interval component of the query. Figures 3.11 through to 3.14 presents the query
response times, in milliseconds, for the different variants of CAS and VERT. Each chart
corresponds to the performance of CAS and VERT on a given query granularity – day
(Figure 3.11), month (Figure 3.12), year (Figure 3.13) or the full lifetime of the respective
collection (Figure 3.14). While comparing CAS against VERT, we choose CAS-1000 as a
reasonable representative for sharding since the wall-clock times for both CAS-100 and
CAS-1000 show low variance throughout all experiments.
VERT-3.0 is optimized for query performance, because of a higher degree of par-
titioning, and as expected it has the best performance among its other counterparts
(κ = {1.5, 2.0, 2.5}). In case of WIKI, we see a low difference in query processing times
between VERT-3.0 (10.63 ms) and CAS-1000 (11.86 ms) for day-granularity queries (see
55
Chapter 3. Efficient Indexing and Maintenance for Time-Travel Text Search
 0
 25
 50
 75
 100
IS C
AS
-1
0
C
AS
-1
00
C
AS
-1
00
0
C
AS
-in
f
VE
R
T-
1.
5
VE
R
T-
2.
0
VE
R
T-
2.
5
VE
R
T-
3.
0
W
al
l-C
lo
ck
 T
im
e 
(m
ilis
ec
on
ds
) Non-Coalesced
Coalesced
(a) Wikipedia
 0
 250
 500
 750
 1000
IS C
AS
-1
0
C
AS
-1
00
C
AS
-1
00
0
C
AS
-in
f
VE
R
T-
1.
5
VE
R
T-
2.
0
VE
R
T-
2.
5
VE
R
T-
3.
0
W
al
l-C
lo
ck
 T
im
e 
(m
ilis
ec
on
ds
) Non-Coalesced
Coalesced
(b) UKGOV
Figure 3.11.: Wall-clock times for day-granularity queries
Figures 3.11(a)). The difference is notable for month-granularity queries with CAS-1000
exhibiting a 19.5% improvement over VERT-3.0 (see Figure 3.12(a)). In UKGOV, CAS-
1000 takes 69.88 ms to process day-granularity queries which is almost a 40% improve-
ment over VERT-3.0 which takes 117.9 ms (see Figure 3.11(b)). This shows that although
VERT is optimized for short time interval queries, and only one or very few partitions
have to be accessed for processing, the number of wasted reads accessed in VERT is
substantially more those accessed by CAS.
Next, we compare performances for longer time-granularity queries, i.e., year and
full-lifetime queries. In case of WIKI, comparing VERT-1.5, which has the best runtimes
for year queries and lifetime queries, with CAS-1000 shows that the latter consistently
outperforms the former in both cases. Query-processing times improve by 22.2% (see
Figure 3.13(a)) for year-granularity queries and by 19% for full lifetime queries (see Fig-
56
3.10. Experimental Results
 0
 25
 50
 75
 100
IS C
AS
-1
0
C
AS
-1
00
C
AS
-1
00
0
C
AS
-in
f
VE
R
T-
1.
5
VE
R
T-
2.
0
VE
R
T-
2.
5
VE
R
T-
3.
0
W
al
l-C
lo
ck
 T
im
e 
(m
ilis
ec
on
ds
) Non-Coalesced
Coalesced
(a) Wikipedia
 0
 250
 500
 750
 1000
IS C
AS
-1
0
C
AS
-1
00
C
AS
-1
00
0
C
AS
-in
f
VE
R
T-
1.
5
VE
R
T-
2.
0
VE
R
T-
2.
5
VE
R
T-
3.
0
W
al
l-C
lo
ck
 T
im
e 
(m
ilis
ec
on
ds
) Non-Coalesced
Coalesced
(b) UKGOV
Figure 3.12.: Wall-clock times for month-granularity queries
ure 3.14(a)). Note that although VERT-3.0 is optimized to access fewer postings than
VERT-1.5, it performs slightly worse. This is possibly because VERT-3.0 has to access
more number of partitions than VERT-1.5 for longer time-granularity queries due to
its high degree of partitioning. In UKGOV, there is an improvement of almost 29.9%
or 279.26 ms (CAS-1000 vs VERT-1.5, see Figure 3.13(b)) for year-granularity queries.
The full-lifetime queries are faster by 931 ms or 21% (CAS-1000 vs VERT-1.5, see Fig-
ure 3.14(a) and 3.14(b)) which in absolute terms is a considerable difference. Thus, the
first insight which we draw is that sharded posting list avoid wasted reads substantially
as compared to VERT resulting in superior performance.
57
Chapter 3. Efficient Indexing and Maintenance for Time-Travel Text Search
 0
 100
 200
 300
 400
IS C
AS
-1
0
C
AS
-1
00
C
AS
-1
00
0
C
AS
-in
f
VE
R
T-
1.
5
VE
R
T-
2.
0
VE
R
T-
2.
5
VE
R
T-
3.
0
W
al
l-C
lo
ck
 T
im
e 
(m
ilis
ec
on
ds
) Non-Coalesced
Coalesced
(a) Wikipedia
 0
 250
 500
 750
 1000
 1250
 1500
IS C
AS
-1
0
C
AS
-1
00
C
AS
-1
00
0
C
AS
-in
f
VE
R
T-
1.
5
VE
R
T-
2.
0
VE
R
T-
2.
5
VE
R
T-
3.
0
W
al
l-C
lo
ck
 T
im
e 
(m
ilis
ec
on
ds
) Non-Coalesced
Coalesced
(b) UKGOV
Figure 3.13.: Wall-clock times for year-granularity queries
3.10.2. Effect of Coalescing
Our experimental results with temporal coalescing of postings lead to similar results
as our experiments on the original uncoalesced indexes. Independent of the partition-
ing/sharding used, the index sizes thus obtained are much smaller than with uncoa-
lesced indexes—up to an order of magnitude for UKGOV and up to a factor of 2 for
Wikipedia. Also the indexes created with sharding are always smaller than VERT (see
Figure 3.15). The wall-clock times of CAS and VERT methods with temporal coalescing
are depicted in Figures 3.11(a) through 3.14(b). It is evident that query performance im-
proves with temporal coalescing, with longer time interval queries gaining more than
those with smaller time ranges as more postings need to be read.
58
3.10. Experimental Results
 0
 250
 500
 750
 1000
 1250
 1500
 1750
 2000
IS C
AS
-1
0
C
AS
-1
00
C
AS
-1
00
0
C
AS
-in
f
VE
R
T-
1.
5
VE
R
T-
2.
0
VE
R
T-
2.
5
VE
R
T-
3.
0
W
al
l-C
lo
ck
 T
im
e 
(m
ilis
ec
on
ds
) Non-Coalesced
Coalesced
(a) Wikipedia
 0
 1000
 2000
 3000
 4000
 5000
 6000
 7000
IS C
AS
-1
0
C
AS
-1
00
C
AS
-1
00
0
C
AS
-in
f
VE
R
T-
1.
5
VE
R
T-
2.
0
VE
R
T-
2.
5
VE
R
T-
3.
0
W
al
l-C
lo
ck
 T
im
e 
(m
ilis
ec
on
ds
) Non-Coalesced
Coalesced
(b) UKGOV
Figure 3.14.: Wall-clock times for full-lifetime queries
3.10.3. Comparing Sharding Approaches
In the next set of results, we first present the improvements, in query-processing wall-
clock times, due to reductions in the number of shards by carefully allowing for wasted
reads regulated by the parameter η. Secondly, we compare the query performance of
both sharding approaches CAS and INC.
To explain the query performance of the sharding methods, we first compare the av-
erage number of shards generated by both the algorithms for the terms present in the
query workload. These results are summarized in Table 3.3. Interestingly, INC has lesser
number of shards than CAS in UKGOV in-spite of being more restrictive. This means
that the INC, with an approximation guarantee, seems to make better choices than the
heuristic approach employed by CAS. On the contrary for WIKI the number of shards
59
Chapter 3. Efficient Indexing and Maintenance for Time-Travel Text Search
Sharding
Dataset η INC CAS
WIKI
IS 79.75 79.75
10 33.37 32.59
100 14.42 11.78
1000 6.83 4.71
UKGOV
IS 16.75 16.75
10 11.95 14.12
100 8.05 11.67
1000 5.49 5.84
Table 3.3.: Average number of shards per term
per term created by INC is more than that of CAS. However, in both these cases the
difference between CAS and INC in the number of shards is not significant.
Effect of the Parameter η Depending on the value of the parameter η there are two
conceptual extremes in sharding. The scenario η = 0 represents idealized sharding,
denoted by IS, when wasted reads are completely avoided. Note that both CAS and
INC would give rise to IS when η = 0. The other extreme is CAS-inf which results in no
sharding of the posting list. cop
Idealized sharding is a restrictive form of sharding and for certain distributions pro-
duces a fairly high number of shards (see Table 3.3). Although query processing on IS
results in reading only the postings intersecting with the query time-interval, they suf-
fer from inefficiencies due to a large number of random seeks in accessing each shard
individually. Especially for disk with Cr >> Cs, the open-seek operation on idealized
shards might result in considerable overheads.
To put this into perspective with the actual query performance, we present the wall-
clock times in Table 3.4. In WIKI, we see a consistent improvement from IS to CAS-1000.
This is because idealized sharding admits a fairly large number of shards in this case
and thus the I/O costs are dominated by initial random seeks to access these idealized
shards. Improvements result from the reduction in the number of shards due to careful
merging of idealized shards, as presented before in Section 3.5. Although these reduc-
tions might not be significant for queries with longer time intervals, but they reduce
query processing time by a sizable fraction for small-time granularity queries – day-
granularity queries improve by 62% (see Figure 3.11(a)) and month-granularity queries
60
3.10. Experimental Results
η = 10 η = 100 η = 1000
CAS INC CAS INC CAS INC
WIKI
Day 17.64 15.81 11.57 10.65 11.86 8.70
Month 29.01 28.88 25.03 24.22 24.75 22.70
Year 131.06 132.08 127.96 125.32 127.57 123.76
Full 851.43 817.76 834.74 806.87 815.20 809.90
UKGOV
Day 55.78 50.22 53.34 44.97 49.25 43.27
Month 158.89 155.72 156.28 145.77 153.89 143.60
Year 711.63 724.56 707.84 702.69 699.81 696.87
Full 2,875.35 2,839.80 2,794.46 2,845.31 2,940.5 2,999.32
Table 3.4.: Comparison of wall-clock times between CAS and INC – in milliseconds
by 35% (see Figure 3.12(a)). A similar trend is seen in the case of INC (see Table 3.4)
where there is a 47% improvement in day-granularity queries in INC-1000 from INC-10
for WIKI. Unlike WIKI, the difference in performance between CAS and INC in UKGOV
is not considerable, which is due to the already low number of initial idealized shards.
This indicates that sharding can be applied as a self-organizing approach depending on
the distribution of initial shards.
CAS-1000, and eventually INC-1000, outperforms CAS-inf by a fairly large margin in
all query granularities except one scenario. This shows that the improvement in query
performance is not only due to begin-time order of postings in the shards but a result of
careful sharding of posting lists to avoid wasted reads. The only scenario when CAS-inf
outperforms others is when we consider queries spanning the full-lifetime of the collec-
tion. This behavior is to be expected because all postings in CAS-inf become relevant
for such kind of queries and have to be subsequently read.
Comparing CAS and INC As one can observe from the in Table 3.4, the sharded in-
dex generated using INC compares quite favourably with the CAS. This behaviour is
consistent across all the granularities of temporal predicates for all values of η. In a
small number of cases, the performance of INC is slightly better than that of CAS, and
is never worse. Although we see a less shards in some scenarios, it is counteracted more
number of wasted reads per shard for a given query. Hence, the small differences in the
number of shards between CAS and INC do not to make a considerable difference in
61
Chapter 3. Efficient Indexing and Maintenance for Time-Travel Text Search
 0
 20
 40
 60
 80
 100
 120
 140
 160
 180
 200
 220
 240
 260
 280
 300
IS C
AS
-1
0
C
AS
-1
00
C
AS
-1
00
0
C
AS
-in
f
VE
R
T-
1.
5
VE
R
T-
2.
0
VE
R
T-
2.
5
VE
R
T-
3.
0
In
de
x 
Si
ze
 (G
By
te
s)
Non-Coalesced
Coalesced
(a) Wikipedia
 0
 20
 40
 60
 80
 100
 120
 140
 160
 180
 200
 220
 240
 260
 280
 300
IS C
AS
-1
0
C
AS
-1
00
C
AS
-1
00
0
C
AS
-in
f
VE
R
T-
1.
5
VE
R
T-
2.
0
VE
R
T-
2.
5
VE
R
T-
3.
0
In
de
x 
Si
ze
 (G
By
te
s)
Non-Coalesced
Coalesced
(b) UKGOV
Figure 3.15.: Index sizes
η = 10 η = 100 η = 1000
CAS INC CAS INC CAS INC
WIKI 76.31 78.50 76.29 77.60 76.18 77.20
UKGOV 68.31 68.50 68.28 68.47 68.18 68.46
Table 3.5.: Comparison between index sizes of CAS and INC - in gigabytes
the wall-clock times. Hence the arguments presented above comparing CAS and VERT
also apply when comparing INC and VERT.
3.10.4. Index Sizes
As expected, the size of the index files of the sharded indexes is the same as that of the
unpartitioned index. This is due to the fact that sharding partitions the postings of the
unpartitioned lists in a disjoint manner. On the contrary the postings in VERT are subject
to replication across the vertical partitions. The index sizes of the different variants of
VERT show a direct correlation with the input parameter κ as shown in Figure 3.15(a).
As discussed before, κ regulates the upper bound to the index-size blowup. The higher
the κ, the more efficient is the performance of time-travel queries at the expense of a
larger index size. Thus the vertically-partitioned index has to be carefully tuned trading
off index size and query efficiency. This is not the case with the sharded index where
the tradeoff is between number of random seeks and sequential reads, which are local
tunable parameters depending on physical characteristics of disks (where the index is
stored) irrespective of the query workload.
The difference in overall index sizes is due to the impact lists. Since each shard is as-
62
3.10. Experimental Results
sociated with an impact list, the number of shards in an index is directly correlated with
size of the impact-list file. Thus, the file which store the impact lists decrease with the
increasing η. Each impact list is stored as list of pairs of integers without compression.
From our experiments we see that the impact-list file is typically 1%-7% of the entire in-
dex. There is, however, scope for compactly representing the impact lists using integer
compression over d-gaps when the key and values are stored in integer array separately.
From our experiments, we observe that the time taken to build a sharded index is
roughly twice the time taken for the standard unpartitioned inverted index. Since
the sharded index building process can be easily parallelized, one can efficiently build
sharded indexes using a distributed processing platform (e.g., Hadoop).
3.10.5. Index Maintenance Performance
As we introduced in Section 3.8, the archive index is maintained by first collecting the
updates to a partial-archive index which is then periodically merged into the primary
archive index. A partial index is responsible for all versions which end in the time
interval between two consecutive merges. To simulate index management for archive
indexes as follows: we first created partial indexes for each month containing postings
of only those versions that have end time within that month. The index is incrementally
maintained, starting from an empty index, by merging partial indexes of each month in
sequence. We employed immediate-merging [BCC10] to create one consolidated index at
the end of every monthly merge operation, and each sharded posting list in the index
is incrementally maintained using our incremental sharding algorithm. We compared
this with CAS, which recomputes the entire sharding for the combined index every
time from scratch. In other words, all shards of a posting list in the currently merged
index are read and decompressed, the corresponding list from the partial index for the
next month is also read and decompressed, these two are combined, and finally, a new
sharding is generated using the CAS algorithm for the merged index, which is finally
written in compressed form to disk.
Figures 3.16 and 3.17 show results of our experiments on index maintenance with
WIKI and UKGOV datasets respectively. Note the log-scale used on the y-axis, which
represents the time-taken for the consolidated sharded index to be built in milliseconds.
It is evident from these charts that INC outperforms CAS by a large margin. In UK-
GOV, the improvements are nearly a factor of 4 (see Figure 3.17) while the improve-
ments in WIKI are around a factor of 10 (see Figure 3.16). This efficiency comes from
two advantages that INC enjoys over CAS: first, recomputing the sharding by CA on
the merged index takes much of the time and grows as the index size grows. Since
INC does not recompute the sharding, its performance improves significantly. Second,
63
Chapter 3. Efficient Indexing and Maintenance for Time-Travel Text Search
 100
 1000
 10000
 100000
 1e+06
 1e+07
 1e+08
-5  0  5  1
0
 1
5
 2
0
 2
5
 3
0
 3
5
 4
0
 4
5
 5
0
W
al
l-C
lo
ck
 T
im
e 
(m
ilis
ec
on
ds
)
Monthly Index Partitions
INC
CAS
(a) η = 10
 100
 1000
 10000
 100000
 1e+06
 1e+07
 1e+08
-5  0  5  1
0
 1
5
 2
0
 2
5
 3
0
 3
5
 4
0
 4
5
 5
0
W
al
l-C
lo
ck
 T
im
e 
(m
ilis
ec
on
ds
)
Monthly Index Partitions
INC
CAS
(b) η = 100
 100
 1000
 10000
 100000
 1e+06
 1e+07
 1e+08
-5  0  5  1
0
 1
5
 2
0
 2
5
 3
0
 3
5
 4
0
 4
5
 5
0
W
al
l-C
lo
ck
 T
im
e 
(m
ilis
ec
on
ds
)
Monthly Index Partitions
INC
CAS
(c) η = 1000
Figure 3.16.: Performance of index maintenance - WIKI
64
3.10. Experimental Results
 100000
 1e+06
 1e+07
 1e+08
-5  0  5  1
0
 1
5
 2
0
 2
5
W
al
l-C
lo
ck
 T
im
e 
(m
ilis
ec
on
ds
)
Monthly Index Partitions
INC
CAS
(a) η = 10
 100000
 1e+06
 1e+07
 1e+08
-5  0  5  1
0
 1
5
 2
0
 2
5
W
al
l-C
lo
ck
 T
im
e 
(m
ilis
ec
on
ds
)
Monthly Index Partitions
INC
CAS
(b) η = 100
 100000
 1e+06
 1e+07
 1e+08
-5  0  5  1
0
 1
5
 2
0
 2
5
W
al
l-C
lo
ck
 T
im
e 
(m
ilis
ec
on
ds
)
Monthly Index Partitions
INC
CAS
(c) η = 1000
Figure 3.17.: Performance of index maintenance – UKGOV
65
Chapter 3. Efficient Indexing and Maintenance for Time-Travel Text Search
it does not have to decompress, merge and shard the entire list before writing do the
disk. Instead, it has to just read two posting lists in parallel and append correspond-
ing shards (without decompressing), and write to the disk. From our experiments, we
observe that recomputation of shards accounts for an average of 55%-60% of the entire
maintenance time. Compression and decompression take upto 15% of the overall time
but since we use 7-bit encoding for compression we expect that a more involved com-
pression technique would only increase the maintenance time. It should be noted that in
our simulation we do not perform an append using in-place merge techniques. Instead
we resort to creating a new index file in each step incurring additional overheads. Thus,
the performance of INC can be further improved by carefully implementing advanced
index merging methods.
3.11. Related Work
Temporal information associated with documents has recently seen increasing attention
in information retrieval. One of the earliest known efforts in this direction is by An-
ick and Flynn [AF92] who developed a framework for versioning the complete index
for historical queries. Recently, Alonso et al. [AGBY07] give an overview of relevant
research directions. The work by Herscovici et al. [HLY07] focuses on exploiting the
redundancy commonly seen in versioned documents to compress the inverted index.
Similarly, He et al. [HYS09, HZS10] consider the problem of efficiently storing inverted
indexes on disk using compression; these are orthogonal to our work and could be com-
bined with our sharding techniques.
The closest to our work and the most relevant related work is the work on vertical
partitioning of posting lists by Berberich et. al. [BBNW07]. They consider posting-list
partitioning strategies which trade-off index size and query-processing performance.
Two approaches employed by them either bound the index size, called the space-bound
approach, or bound the worst performance called the performance-guarantee approach. Fur-
ther, they also introduced index compression techniques called temporal coalescing
aimed at supporting different query types while keeping the index compact. These
compression techniques are also relevant in our setting as shown in our experiments in
Section 3.10.2.
Research in temporal databases has taken a broader perspective beyond text docu-
ments and targeted general class of time-annotated data. Index structures tailored to
such data like the Multi-Version B-Tree [BGO+96] or LHAM [MOPW00] are related to
our work, since they also, implicitly or explicitly, rely on a temporal partitioning and
replication of data. It is therefore conceivable to apply our proposed techniques in con-
66
3.12. Summary
junction with one of these index structures.
Work on index maintenance can be categorized into (a) work on maintaining inverted
indexes when faced with changes in the document collection and (b) approaches that
make search aware of temporal information associated with documents. No work, to
the best of our knowledge, has been done at the intersection of the two categories.
Given that the construction of inverted indexes is well understood and can easily be
parallelized, one existing maintenance strategy has been to rebuild the index periodi-
cally. Returning stale query results, most of the time, is an obvious disadvantage of this
approach. For a long time, though, this has been the approach taken by major search en-
gines on the Web. Only lately, Peng et al. [PD10] have addressed the issue of handling
updates in web-scale indexes. Instead of rebuilding the index, Lester et al. [LZW06]
suggest to collect updates in an in-memory index that is then merged, from time to
time to amortize costs, with a disk-resident inverted index. This merge can either be
performed in-situ, thus modifying posting lists at their current location, or by storing
entirely new posting lists. A hybrid approach that chooses between these alternatives
is described by Büttcher et al. [BC08]. Guarajada and Kumar [GK09] can be seen as an-
other extension that leverages query logs to determine terms whose posting lists man-
date eager maintenance. In a spirit similar to log-structured methods [OCGO96], Lester
et al. [LMZ08] propose to keep multiple indexes of geometrically increasing size and
merge them, when they overflow, in a rolling manner. Query results reflecting the cur-
rent state of the document collection can be obtained in these approaches by executing
queries both on in-memory and disk-resident indexes. For more detailed discussions of
inverted index maintenance we refer the reader to Chapter 2.
3.12. Summary
This chapter presents a novel method of index organization based on sharding for pro-
cessing time-travel queries efficiently. Previous approaches traded-off index size and
query performance resulting in an index-size blowup. We take an alternative approach
of index partitioning by exploiting the valid-intervals and taking into account index-
access costs. The resulting index has a small space overhead and we show by experi-
ments that the sharded indexes consistently outperform the vertically-partitioned index
in query performance. We also introduce index-maintenance strategies, based on in-
cremental sharding, which avoid expensive shard recomputations when dealing with
dynamic collections. We show through experiments that, by employing incremental
sharding, we outperform sharding based on recomputation by at least four times.
67
Chapter 3. Efficient Indexing and Maintenance for Time-Travel Text Search
68
4
Query Optimization for Approximate Processing of
Time-Travel Queries
4.1. Introduction
4.1.1. Motivation and Problem Statement
Text search is an expensive operation on web archives due to their large scale. How-
ever, many of the documents in the archive contain redundant information. In certain
scenarios, users are often satisfied with a subset of the true results that are determined
quickly. That is, missing a few results during search might not lead to substantial in-
formation loss. Particularly in search tasks which involve multiple interactive steps of
query reformulation, expansion, and refinement. Consider the following two use cases
in the context of time-travel text search.
• A sports journalist is in interested the game between the popular cricket teams,
from Mumbai and Rajasthan, and issues a query “indians vs royals” @ [6/2008].
However content from the popular baseball teams from Cleveland and Kansas,
also having the same titles, which also were in the news in the same period might
dilute the results. For an interactive and constructive search experience the analyst
should be able to quickly identify the ambiguity of the keywords used and alter
the query to “mumbai indians vs rajasthan royals” @ [6/2008]. Here a subset of the
exact results is good enough for the user to adapt “indians vs royals” to “mumbai
indians vs rajasthan royals”.
• Partial results can also help reformulating for the correct temporal predicates of
the query. Consider the utility of time-travel text search in patent retrieval. The
time-interval of a query “retina display patent” @ [1/2010 - 12/2012] can quickly
be reformulated to “retina display patent” @ [6/2012 - 6/2013] when the user
observes that the initial results cluster around the second half of the year 2012.
69
Chapter 4. Query Optimization for Approximate Processing of Time-Travel Queries
In such scenarios, a subset of the results is often representative of the information
contained in the entire result set. Increasing the number of elements in this subset, or
the recall, improves the information contained therein. Also, it is imperative that the
user needs to have the ability to control the performance of the time-travel retrieval
task. To this extent, we develop query processing techniques to maximize recall, for a
time-travel query, given a user-specified bound on the performance.
4.1.2. Approach
In the previous chapters we discussed different index-organization strategies for web
archives with the focus on exact solutions to time-travel queries. We showed that hori-
zontal partitioning is an efficient index-organization strategy for exact time-travel queries.
In this chapter, we argue that, for approximate processing of time-travel queries vertical
partitioning is more suitable. This is due to the fact that vertical partitions are clustered
in the time dimension.
By identifying the time periods which have high result contribution, and in turn the
partitions which temporally overlap with it, the recall can be increased by processing
such partitions early on. In contrast, a lack of such clustering in horizontal partitioning
or sharding prevents easy identification of shards which have high result contribution.
Consequently, we operate on an index with vertically-partitioned posting lists and pro-
pose query-optimization methods to determine partial results efficiently.
A natural side effect of vertical partitioning is that postings with long valid-time in-
tervals are replicated across several temporally-adjacent partitions. This is not an issue
for time-point queries where only one partition per term is accessed during query pro-
cessing. For the more practical and general class of time-travel (text) queries where the
temporal predicate is a time interval, the straightforward query processing of [BBNW07]
quickly becomes inefficient as a consequence of this replication, as repeatedly reading
replicated postings from several partitions within the query time-interval wastes I/O
operations.
We introduce an approach called partition selection which exploits the following ob-
servation: if we can determine that a partition largely consists of postings that are repli-
cated in already processed partition(s), we can avoid processing this partition without
significantly compromising the final result quality. We aim at selecting a set of partitions
which can be processed incurring no more than a given maximal processing cost and
that yield high recall. We consider abstract cost measures for processing a query, namely
the number of partitions or the number of postings accessed during execution. A user
would specify bounds on the execution time that can be transformed into bounds on
the abstract cost by the system. Alternatively, the user can also stop the processing at
70
4.1. Introduction
d21
d12
d31
d21
d12
d31
A1
“A”
time
A2 A3
B1 B2 B3
“B”
[tb , te]
“A B” @ [tb , te]
Sunday, May 26, 13
Figure 4.1.: Processing a time-travel query “A B” @ [tb , te] using partition selection
any time when she determines that the results are already satisfying (or the query needs
to be refined); our methods support this by selecting partitions first that are likely to
contain many unseen answers.
We use the example in Figure 4.1 to illustrate the general idea of our approach. This
figure shows posting lists for two terms A and B built over documents with valid-time
intervals. On the left side of the figure, posting lists are shown, each spanning the entire
time interval. The region, shaded in gray, represents a temporal predicate that spans a
small time range over these lists. In the absence of temporal partitioning, query pro-
cessing needs to entirely scan both lists and filter out postings that do not satisfy the
temporal predicate. When the index is partitioned, however, the processing can be sped
up by reading only the relevant partitions that overlap with the temporal predicate,
represented on the right. Thus, in our example, a total of 6 partitions – A1, A2, A3 and
B1, B2, B3 – have to be processed to determine the result set of {d1, d2, d3} – marked as
red line-segments.
However, a closer inspection of Figure 4.1 reveals that the same result set can be ob-
tained by processing only 2 partitions, A2 and B2, since the replicas of postings for doc-
uments in the result set are fully available within these two partitions.
How can we make use of this observation in practice ? In order to do so, we need to
answer the following questions: (i) does a partition contribute non-redundantly towards
the final result set when it is processed ?, (ii) how much does a partition contribute to the
final result set ? (iii) is there an alternative set of partitions which can contribute these
answers at a lower access cost ? Based on answers to these questions, we can generate a
partition-access plan so that for a specified cost budget only those partitions are chosen
71
Chapter 4. Query Optimization for Approximate Processing of Time-Travel Queries
for processing which maximize the number of results.
4.1.3. Contributions
We formally model these partition selection problems as optimization problems. Making
use of KMV synopses for cardinality estimation under set operations [BHR+07], we de-
velop algorithms for efficiently solving such partition-selection problems. In particular,
the contributions made in this chapter are:
1. An optimal dynamic programming based partition selection algorithm for single-
keyword queries;
2. An efficient greedy alternative for partition selection that can be applied for both
single keyword as well as multi-keyword queries;
3. A detailed experimental evaluation on three large-scale real-world text archives:
the revision history of the English Wikipedia, a Web archive, and the Annotated
New York Times archive spanning 20 years.
4.1.4. Organization
The remainder of this chapter is organized as follows: in Section 4.2, we present index
organization and query processing in our setup. In Section 4.3, we detail an optimal
algorithm and its greedy approximation for selecting the set of partitions to process
for the case of single-keyword queries. Extensions for multi-term query setting are de-
scribed in Section 4.4. Our experimental setup and results are detailed in Section 4.7
before summarizing in Section 4.9.
4.2. Index Organization and Query Processing
We adopt the document, collection and query models from the previous chapters (c.f.
Section 3.2 and Section 2.3.3). We briefly recap the notation in Table 4.1.
Index Organization We use a temporally-partitioned index as described in Chapter 2
with the following posting structure
〈dki , [begin(dki ), end(dki ))〉.
dki refers to the version identifier d
k
i , [begin(d
k
i ), end(d
k
i )) is its valid-time interval.
The temporally-partitioned index consists of posting lists vertically partitioned into
a set of partitions. We let partitions(v) denote the set of partitions of the posting list
72
4.2. Index Organization and Query Processing
Notation Description
D collection of documents
V vocabulary as a set of words
d
j
i ∈ D the j
th version of document di
valid(dji) valid-time interval
begin(dji) begin-time of d
j
i
end(dji) end-time of d
j
i
Q time-travel query
keywords(Q) set of keywords in Q
interval(Q) query time-interval of Q
I1 I2 Overlapping intervals I1 and I2
I1 I2 Non-overlapping intervals I1 and I2
Table 4.1.: Notation.
Lv for term v ∈ V . Each partition φv,j ∈ partitions(v) has an associated time-interval
span(φv,j) = [begin(φv,j) , end(φv,j)) and stores postings representing document ver-
sions whose valid-time intervals overlap with span(φv,j), i.e.,
φv,j =
{
dki ∈ D | v ∈ dki ∧ valid(dki ) span(φv,j)
}
.
Further, we assume that partition spans for a given term v are disjoint, i.e.,
∀i ∀j : span(φv,i) span(φv,j) .
A lexicon L stores this partitioning information as a mapping from term to the parti-
tion statistics (partition span, partition size and location).
For a pair of partitions φv,i and φv,i+k, or simply φi and φi+k, φi ∩φi+k represents the
set of postings common to both the partitions. The overlap of valid-time intervals and
partition spans gives rise to the time-continuity property as described below.
Lemma 4.1 (Continuity Property in Vertical Partitions) For a set of partitions belonging
to a term v, the overlaps of contents of partition φi with φi+k, ∀k ≥ 0 and 0 ≤ j ≤ k have the
following property:
φi ∩φi+k ⊆ φi ∩φi+j
Proof: Every posting p ∈ φi ∩ φi+k represents a version dtk for which begin(dtk) < end(φi)
and end(dtk) > begin(φi+k). This implies that
valid(dtk) span(φi+k)
and hence the claim holds. 
73
Chapter 4. Query Optimization for Approximate Processing of Time-Travel Queries
Query Processing In this work we use conjunctive query semantics, i.e., we identify
as results documents versions that contain all query terms. We employ the Term-at-a-
Time (TAAT) approach of posting-list intersection during query processing. First, the
partitions overlapping with the query time-interval are determined by consulting the
lexicon. We refer to these partitions as affected partitions.
Definition 4.1 (Affected Partitions) For a time-travel query Q, the set of partitions of the
term qi ∈ keywords(Q) which overlap with the query-time interval interval(Q) are
a(qi, Q) =
{
φi,j | interval(Q) span(φi,j)
}
.
When it is clear from the context we use ai for a(qi, Q). The overall set of affected
partitions is A(Q) =
⋃
qi∈keywords(Q) ai. The determined ais are merged to determine a
candidate set of postings which overlap with interval(Q). Following that, these can-
didate sets are intersected employing TAAT posting-list intersection, for the final set of
results.
For approximate processing of time-travel queries we additionally go through a query-
optimization phase prior to query processing - referred to as partition selection. Partition
selection determines a subset S ⊆ A(Q) of all the affected partitions for query process-
ing. Query processing over S yields partial results. We formalize the notion of a result
set R(S, Q) given a time-travel query Q over a subset of affected partitions S ⊆ A(Q).
R(S, Q)=

{ ⋃
φj∈S φj
}
: |Q| = 1{ ⋂
1≤i≤|Q|
⋃
φi,j∈S φi,j
}
: |Q| > 1.
(4.1)
Using the notation above, the exact set of results is captured hence by R(A(Q), Q). The
case when |Q| = 1 refers to the scenario when the query contains a single keyword. In
such a case all postings accessed are relevant and are captured by the union operation
over all affected partitions. However, when |Q| > 1, only those postings which are
common to all the terms are relevant. This is captured by the intersection over unions.
As an example, we refer to Figure 4.1. For a query “A B” @ [tb, te] the document
versions, colored in red, are retrieved as results. However, for a single-term query, say
“A” @ [tb, te], all document versions index in “A” get qualify as results.
To quantify how much of the exact results are retrieved using S we use relative recall.
The relative recall for S ⊆ A(Q), RR(S, Q), is defined as the ratio of the number of
retrieved results to that of the exact results, i.e., fraction of results retrieved. Formally,
RR(S, Q)= |R(S, Q)|/|R(A(Q), Q)|. (4.2)
The intention of partition selection is to choose S in order to maximize RR(S, Q).
74
4.3. Single-Term Partition Selection
Use of Partition Synopsis Partition-selection algorithms, as we discuss in detail later,
use cardinality values of set operations (unions and intersections) on partitions as prim-
itive operations. However, to determine the exact cardinalities the partitions have to be
accessed which is exactly what we want to avoid. Instead, we depend on high qual-
ity cardinality estimates under union and intersection of large sets of document iden-
tifiers. For this purpose, we utilize the recently proposed KMV synopses [BHR+07].
In a pre-computation step, we build and store the synopsis for each partition on disk
of the temporally-partitioned index, which we use during our partition-selection pro-
cess. Pointers to each partition-synopsis can be either stored in the existing lexicon or
an altogether separate lexicon can be constructed explicitly for this task.
During partition selection, the lexicon storing pointers to partition synopses is con-
sulted followed by retrieval of the synopsis of each affected partition. These highly com-
pact partition synopses are used by our selection algorithms to determine cardinalities
of set operations over partitions. In the next section, we first detail partition-selection
methods for queries with only single terms. We then generalize these approaches to
multi-term queries.
4.3. Single-Term Partition Selection
Let us consider the special case where the time-travel keyword queryQ consists of only
a single query term, i.e., keywords(Q) = {q }. Since we deal with single-term queries,
we denote φq,j as φj for ease of notation from now on.
Our objective, when selecting partitions to process the time-travel keyword query, is
to retrieve as many of the original query results as possible, while not violating a user-
specified I/O bound. Our optimization objective, to put it differently, is to maximize the
relative recall as the fraction of original query results retrieved. The user-specified I/O
bound, which constrains the space of valid solutions, can be of two types – size-based
partition selection or equi-cost partition selection.
In both selection problems we model the performance to be proportional to the in-
dex accesses. Hence a bound on the accesses simulates a bound on the response time
for a time-travel query. In case of size-based partition selection we bound the number of
postings accessed. A similar analysis is undertaken for most query-processing methods
over posting lists.
However, we also take a more coarse-grained approach in modelling the constraint
on accesses in equi-cost partition selection. Accesses to each partition results in a random
access and random I/O are substantially more expensive than sequential I/O. Also,
partitions tend to be smaller than entire posting lists. Hence the bottleneck in answer-
75
Chapter 4. Query Optimization for Approximate Processing of Time-Travel Queries
ing a time-travel query in such scenarios is the number of distinct partitions that are
accessed. To this extent, in equi-cost partition selection, we model the allowable budget
to be accessed as a fixed number of affected partitions that can be accessed rather than
number of postings.
Size-Based Partition Selection The input to this optimization problem is the set of
affected partitions A(Q), and a user-specified I/O bound β where 0 < β ≤ 1. Here, β
denotes the fraction of postings of all affected partitions that we are allowed to read. The
cardinality of each partition |φj| represents the number of postings accessed on selecting
the |φj| for processing. Note that maximizing result size
∣∣⋃
φj∈S φj
∣∣ is equivalent to
maximizing relative recall since the original result size
∣∣⋃
φj∈A(Q)φj
∣∣ is constant. Thus,
we intend to determine S ⊆ A(Q) so as to maximize the result size while retaining our
I/O budget. Formally,
Definition 4.2 (Size-Based Selection for Single-Term Queries)
argmax
S⊆φ
∣∣∣∣∣∣∣
⋃
φj∈S
φj
∣∣∣∣∣∣∣ s.t.
∑
φj∈S
|φj| ≤ β ·
 ∑
φi∈A(Q)
|φi|
 .
Equi-Cost Partition Selection The inputs to this problem is the same as before – φj,
and bound β. However, we assume that all partitions are equally expensive to process
irrespective of their sizes. The constraint is now bound to a fixed number of partitions
that can be accessed β · |φ|. The objective function remains the same as before. Formally,
Definition 4.3 (Equi-Cost Selection for Single-Term Queries)
argmax
S⊆φ
∣∣∣∣∣∣∣
⋃
φj∈S
φj
∣∣∣∣∣∣∣ s.t.
|S | ≤ β · |A(Q)| .
4.3.1. Optimal Algorithm for Single-Term Partition Selection
The above problems can be solved using dynamic programming over an increasing
number of affected partitions. We first present a solution to the more general size-based
76
4.3. Single-Term Partition Selection
partition selection. Our solution to equi-cost partition selection follows from this, as we
show later.
We process the affected partitions A(Q) in the order of their begin times, i.e.,
begin(φv,j) < begin(φv,j+1).
Consider a prefix sub problem which considers affected partitions {φ1, · · · , φk}, and ca-
pacity c = β · (
∑
j |φj|). Let OPT(c, k) denote the optimal set of partitions for the prefix
sub-problem with capacity c, the optimal result-set size hence is R(OPT(c, k)). The opti-
mal solution is computed by the following recurrence on the constituent sub-problems.
R ( OPT(c, k) )= max
 R ( OPT(c, k− 1) )max
0<k ′<k
R ( OPT (c− |φk| , k ′ )
⋃
{φk} )
We now state and prove the optimality of the recurrence in Theorem 4.1.
Theorem 4.1 Given a query Q and access budget of capacity c = β · (
∑
φj∈A(Q) |φj|) the
optimal solution to the size-based selection problem is given by OPT(c, |A(Q)|).
Proof: Assume that we have optimal solutions for all sub-problems by the given recurrence,
OPT(c ′, k ′), with capacities c ′ such that 0 < c ′ < c, for the set of partitions {φi} where
0 < k ′ < k.
Now we consider computing the optimal selection set by the recurrence OPT(c, k) using OPT(c ′, k ′)’s.
Let us assume that there is a better solution OPT(c, k) such that
R(OPT(c, k)) > R(OPT(c, k)).
Case 1 – φk /∈ OPT(c, k) : From the recurrence this means
R(OPT(c, k)) > R(OPT(c, k− 1))
since R(OPT(c, k)) = R(OPT(c, k − 1)) when φk /∈ OPT(c, k). As a consequence, we have
a new optimal solution for the sub-problem for capacity c and the set of partitions {φi} where
0 < i < k, i.e, R(OPT(c, k − 1)) > R(OPT(c, k − 1)). But, this is contrary to our initial
assumption since we assumed R(OPT(c, k − 1)) is optimal for {φi} where 0 < i < k. Thus, by
contradiction our claim holds.
Case 2 – φk ∈ OPT(c, k) : The second relation in the recurrence is applicable now. Our
assumption leads to the condition R(OPT(c, k)) > max
0<k ′<k
R ( OPT (c− |φk| , k ′ )
⋃
{φk} ).
77
Chapter 4. Query Optimization for Approximate Processing of Time-Travel Queries
Let us denote the index of the partition selected just before φk for OPT(c, k) to be k ′, i.e., for
k ′ = argmax OPT(c, k)\{φk}. According to our assumption we have
R(OPT(c, k)) −
∣∣φk ∩φk ′∣∣ > ( max
0<k ′<k
R(OPT(c, k))
)
−
∣∣φk ∩φk ′∣∣
=⇒ R(OPT(c− |φk|, k ′)) > R(OPT(c, k ′)) − ∣∣φk ∩φk ′∣∣
=⇒ R(OPT(c− |φk|, k ′)) > R(OPT(c− |φk|, k ′))
This is a contradiction since OPT(c− |φk|, k ′) is optimal for all 0 < c ′ < c. Hence our claim
in the theorem holds true. 
Algorithm 4: Partition Selection - dynamic programming solution
1: cmax = bβ · (
∑
j |φj|c
2: // Dynamic programming table, n is number of affected partitions
3: DP [ 0 .. cmax ][ 0 .. n ]
4:
5: for i = 0 .. cmax do
6: DP [ i ][ 0 ] = ∅
7: end for
8:
9: for k = 1 .. n do
10: for i = 0 .. |φk|− 1 do
11: DP [ i ][ k ] = ∅ // No partitioning possible
12: end for
13: for i = |φk| .. cmax do
14: for k ′ = 0 .. k− 1 do
15: // Update if recall is better than current value
16: rk ′ = DPr [ i− |φk| ][ k
′ ] + (|φk|− (φk ∩DPlp [ i− |φk| ][ k ′ ]))
17: end for
18: k ′ = argmax rk ′
19: // Update the DP table with the best partitioning
20: DPr [ i ][ t ] = max{DPr[cj][ki − 1], rk ′}
21: DPlp [ i ][ t ] = argmax DPr [ i ][ t ]
22: end for
23: end for
24:
25: return DP[ cmax ][n ]
Algorithm 4 efficiently implements the recurrence relation presented above. Each
78
4.3. Single-Term Partition Selection
of the DP table cell contains a pair of values – (i) the last partition selected, DPlp, for
the corresponding sub-problem (i.e., the selected partition with the maximum begin-
time), and (ii) the optimal recall value DPr. Since the choice of partitions cannot be
made independently, the computation of recall for a newly selected partition takes into
account only the postings that are not already included in previously selected partitions.
Using Lemma 4.1, we can efficiently compute the optimal recall for each sub-problem
since all overlaps with the preceding partitions to lp are already covered in lp.
The DP-based algorithm, outlined in Algorithm 4, has a time complexityO(n2·(
∑
j |φj|)),
where n is the number of affected partitions, and a space complexity of O(n · (
∑
j |φj|)).
The optimal partitioning can be easily computed by backtracking from the best solution
seen at DPlp[ cmax ][n ]. Observe that the complexities depends on the cardinalities of
the partitions, i.e.,
∑
j |φj|). Thus, Algorithm 4 is a pseudo-polynomial algorithm, for
size-based partition selection, which is polynomial in the value of the size bound.
Equi-cost partition selection is a special case of size-based selection and employs a
uniform cost per partition. The recurrence relation for equi-cost selection is
R ( OPT(c, k) )= max
 R ( OPT(c, k− 1) )max
0<k ′<k
R ( OPT (c− 1 , k ′ )
⋃
{φk} )
This results in improvements in both the space and time complexity of the algorithm,
as the optimal value is independent of the sum of the sizes of the partitions read. The
time complexity of the algorithm reduces to be O(n3) and a space complexity of O(n).
Note that unlike the algorithm for size-based selection, the algorithm for equi-cost selec-
tion is a polynomial algorithm which is polynomial in the number of partitions affected.
4.3.2. Approximation Algorithm
While the selection algorithms outlined above allow for polynomial run times, they
might not be efficient enough to be applied during query processing, e.g., when the
partitions contain a large number of postings. Alternatively, we propose the use of
(1 − 1e)-approximation algorithm called GreedySelect, developed in [KMN99] for solv-
ing budgeted maximum coverage (BMC) problem. We first show the equivalence of our
partition selection problem and the BMC problem.
Definition 4.4 (Budgeted Maximum Coverage) A collection of sets S = {S1,S2, . . . ,Sm}
with associated costs {ci} is defined over a domain of elements X = {x1, x2, . . . , xn} with associ-
ated weights {wi}. The goal is to find a collection of sets S ′ ⊆ S, such that the total cost of the
elements in S ′ does not exceed a given budget L, and the total weight of every element covered
by S ′ is maximized.
79
Chapter 4. Query Optimization for Approximate Processing of Time-Travel Queries
Lemma 4.2 Partition selection is an instance of budgeted maximum coverage (BMC).
Proof: The selection problem for single terms can be cast into an instance of the BMC prob-
lem [KMN99] in the following way: The affected partitions, φjs, are the analogous to the sets
in the BMC problem with the postings in the partitions being the elements of the respective set.
For size-based partition selection, the cost for each set is its cardinality; for equi-cost selection,
the cost for each set is unity. The cost budget is exactly the I/O bound cmax. With this reduc-
tion we can use the approximation algorithm proposed by Khuller et al. [KMN99] which has a
constant factor approximation guarantee of (1− 1e). 
Algorithm 5: GREEDYSELECT for single-term partition selection
1: input: cmax , φ
2: S = ∅
3: A = φ
4: C = 0
5:
6: repeat
7: Select φi ∈ A that maximizes Bici
8: if C + ci ≤ cmax then
9: S = S ∪ φi
10: C = C + ci
11: end if
12: A = A\φq,i
13: until A = ∅
14:
15: Select a partition φt that maximizes Bt over S
16: if B(S) ≥ Bt then
17: output S
18: else
19: output {φt}
20: end if
Algorithm The greedy approximate algorithm, GREEDYSELECT is shown in Algorithm 5.
The input to GREEDYSELECT is the bound on the allowable accesses denoted as cmax and
the set of partitions φ. The solution to the selection problem is denoted as S and we re-
fer to it as the selection set. We associate every partition φi with a cost ci and a benefit Bi.
Here, ci is the cost of processing an unselected partition φj /∈ S. For size-based selection
ci is the number of postings in the partition, and for equi-cost selection ci is one. Its
80
4.4. Multi-Term Partition Selection
benefit, Bi, is the number of unprocessed postings in φj, i.e., |φj\ ∪s∈S s|. A maintains
the available partitions for selection and is updated after each iteration.
Each iteration of GREEDYSELECT consists of a selection step and an update step. In
the selection step, the most promising partition based on the benefit-cost ratio Bici is
chosen from A (cf. line 7). We do not select a partition if it leads to exceeding the cost
budget (cf. line 8). The update step updates A and the benefits of all the partitions in A
which are future candidates for selection (cf. line 12).
Finally, to determine the solution set we compare the overall benefit of the selection
set S, i.e. B(S) = |
⋃
φj∈S φj|, with the partition with the best overall benefit (c.f. line
16-20).
4.4. Multi-Term Partition Selection
In the case of partition selection for single-term queries, every posting read from a par-
tition qualifies as an answer, given that the time span of the partition overlaps with
the query time-interval. Unlike this simpler setting, for multi-term queries there is an
additional constraint imposed by the conjunctive semantics of query evaluation which
requires that every result document also contain all the query keywords. Mimicking the
conventional query processing (over standard posting lists), multi-term queries can be
evaluated by intersecting partitions of individual query terms. Now, the partition se-
lection aims to increase the coverage of postings that belong to this intersection space of
partitions. Formally, both the selection variants, size-based and equi-cost, for a queryQ
wherem = |keywords(Q)|, are defined as follows.
Definition 4.5 (Size-Based Selection for Multi-Term Queries)
argmax
S⊆φ
∣∣∣∣∣∣∣
⋂
1≤i≤m
⋃
φi,j∈S
φi,j
∣∣∣∣∣∣∣ s.t.
∑
φi,j∈S
|φi,j| ≤ β ·
 ∑
φi,j∈A(Q)
|φi,j|
 .
Definition 4.6 (Equi-Cost Selection for Multi-Term Queries)
argmax
S⊆φ
∣∣∣∣∣∣∣
⋂
1≤i≤m
⋃
φi,j∈S
φi,j
∣∣∣∣∣∣∣ s.t.
|S | ≤ β · |A(Q)|.
81
Chapter 4. Query Optimization for Approximate Processing of Time-Travel Queries
Similar to the argumentation for single-term selection, the result size is the objective
function. Observe that the constraints for multi-term selection are preserved from the
previous problem formulations. Let us consider the objective function of both the multi-
term selection problems. The intersection space, in the objective function above, is the
intersection of the unions of the affected partitions. Using the distributive property of
the set intersection operator we can also represent the above into unions of intersections
of partitions φi,j’s.
⋂
1≤i≤m
⋃
φi,j∈S
φi,j =
⋃
φi,j∈S
⋂
1≤i≤m
φi,j
4.4.1. GREEDYSELECT for Multi-term Selection
Let each of these resulting smaller intersections, consisting of one partition each from
every term, be represented as a tuple x. These tuples x come from the Cartesian product
among affected partitions for each term a(qi, Q), i.e., for a m-term query the Cartesian-
product set X = a(q1, Q) × . . . × a(qM, Q). We formally define x, an element of this
Cartesian-product set X , as :
x = { (x1, . . . , xm) | xi ∈ a(qi, Q)}
Although this is a m-ary tuple, we treat this as a set whenever necessary. Having
turned the objective function into a disjunctive formulation, analogous to the single-
term setting, the problem formulation now intends to maximize the coverage of the
results in the intersection space. We can now use GREEDYSELECT over X , where each
element x is equivalent to a partition in single-term selection scenario.
The benefit of x, B(xi), is defined as the cardinality of the documents in the intersection
of the partitions in x which are not in the selection set S .
Bx = | R ( x \ S ) |
In other words, the benefit or contribution of x represents the number of new docu-
ments which are present in every element partition of x. The cost definition of x depends
on the sizes of the element partitions in x. As earlier, the cost of x, cx, in size-based
selection is the sum of the sizes of the partitions in x \ S, i.e.,
cx =
∑
φi,j∈x\S
|φi,j|.
Equi-cost selection on the other hand defines the cost of x as the number of participat-
ing partitions not in S.
cx = |x \ S | .
82
4.4. Multi-Term Partition Selection
Algorithm 6: GREEDYSELECT for multi-term partition selection
1: input: cmax , X
2: S = ∅
3: A = X
4: C = 0
5:
6: repeat
7: Select x ∈ A that maximizes Bxcx
8: if C + cx ≤ cmax then
9: S = S ∪ {φi,j|φi,j ∈ x}
10: C = C + cx
11: end if
12: A = A\S
13: Update Bx ′ and cx ′ for x ′ ∈ A
14: until A = ∅
15:
16: Select y ∈ X that maximizes R({y})
17: if |R(S)| ≥ R({y}) then
18: output S
19: else
20: output {φt|φt ∈ y}
21: end if
The modified inputs to GREEDYSELECT is the set X , with benefit Bx and cost cx for
each of its elements x. GREEDYSELECT now proceeds conventionally by greedily choos-
ing the x with the best benefit by cost ratio Bxcx . Observe that the choices of elements from
X are not independent. A pair of tuples can share the same partition. Thus, selection
of a tuple x might result in reducing the cost (which is not the case in single-term se-
lection) of others which have at least one of the constituent partitions common with x.
Hence in the update step apart from updating the benefit of x, we also update its cost
cx. Owing to such a cost dependence among the tuples, the approximation guarantee of
GREEDYSELECT does not apply to multi-term selection.
Exploiting temporal overlap among partitions For a time-travel query withm terms
with p affected partitions per term, the input size is exponential in the number of terms,
i.e., |X | = pm. In case of queries with large m or p, computations in GREEDYSELECT
can become prohibitive. This is because of the update steps in each iteration where the
83
Chapter 4. Query Optimization for Approximate Processing of Time-Travel Queries
d21
d12
d31
d21
d12
d31
d21
d12
d31
q1
q2
time
- set⌧
Sunday, May 26, 13
Figure 4.2.: τ for the affected partitions time-travel query “q1 q2”
benefits and costs of the remaining partitions A are recomputed.
To alleviate this, we operate on a constrained set, τ⊆ X , which has a cardinality linear
in the number of participating partitions as opposed to high number of combinations
in X . This constrained set is obtained by defining a τ-join operation over the term-
partition sets φi’s. Each element t of the resulting tuple c ∈ X has the property that
there is a non-zero time-overlap between all of the constituent partitions.
c = { (c1, . . . , cm) | ∀ci ∈ a(qi, Q), cj ∈ a(qj, Q), span(ci) span(cj)}
For example, in Figure 4.2, the queries q1 and q2 have 3 partitions each. The |X | = 9
and the resulting τ has a cardinality 5 after the τ-join operation.
For a time-travel query with m terms and p partitions per term, the number of ele-
ments in τ is linear in the number of affected partitions, i.e., p ≤ |τ| ≤ m.p. The parti-
tions in τ exploits the temporal overlap across partitions of different query terms and has
a reduced input size as compared to X . The temporal partitioning induces a temporal
clustering of the postings. Hence, the results contained in the intersection of partitions
inX are already captured in the τ ⊂ X . We further show for equi-cost based partition se-
lection, GREEDYSELECT chooses elements only from τ. In other words, GREEDYSELECT
over the Cartesian set is equivalent to GREEDYSELECT over τ.
Theorem 4.2 GREEDYSELECT for equi-cost based selection on X always chooses elements
which belong to τ.
We prove this theorem by contradiction, by first choosing an element from X\τ and
showing that we can replace this element with a better candidate from τ. For the formal
84
4.4. Multi-Term Partition Selection
proof, we introduce the notion of selected-partition space. Let the selection set S be the
set of already selected partitions and thus the result set R(S, Q) denote the actual set
of result documents covered by the partitions in S. A time interval [tb, te] is said to be
selected if there is a partition from each term qi in S which covers it, i.e.,
∃φi,j ∈ ai, begin(φi,j) < tb ∧ te < end(φi,j).
In other words, an unselected-space refers to a range where all of the partitions have
been selected at the current state of the algorithm.
We first prove that for any x ∈ X there exists a t ∈ τwhich has at least the same result
size.
Lemma 4.3 For any x ∈ X there exists a t ∈ τ such that the following holds∣∣∣∣∣∣
⋂
φx∈x
φx
∣∣∣∣∣∣ ≤
∣∣∣∣∣∣
⋂
φt∈t
φt
∣∣∣∣∣∣ .
Proof: Let us consider the candidate x ∈ X \ τ. Let MAXBP = argmax
φ∈x
begin(φ) denote the
partition in x that has the maximum begin-time. Also consider a t ∈ τ such that
∀φi,t ∈ t, begin(φi,t) ≤ begin(MAXBP) < end(φi,t).
Since x ∈ X\τ there exists a φi,x ∈ x such that span(φi,x) span(MAXBP). Consider a
version dki which belongs to the result set R(x, Q) (assuming R(x, Q) 6= ∅). This means that the
following holds
begin(dki ) < end(φi,x) ∧ end(d
k
i ) > begin(MAXBP).
We further note that (φi,x ∩ MAXBP) ⊆ (φi,t ∩ MAXBP) since for all versions dki in
φi,x ∩ MAXBP,
valid(dki ) span(φi,t).
We can thus replace φi,x with φi,t for a larger result set. We can carry the same replacement
for all terms qi ∈ keywords(Q) where span(φi,x) MAXBP for a better overall result set∣∣∣∩φ∈tφ∣∣∣. This proves our claim.

We now proceed with the proof of Theorem 4.2.
Proof: We prove this by induction on the number of iterations i of the GREEDYSELECT algo-
rithm.
i = 1: For the first iteration S = ∅. We argue that, given that there is enough budget for
selection the candidate selected is always from τ. We prove this by contradiction. We assume
85
Chapter 4. Query Optimization for Approximate Processing of Time-Travel Queries
that the candidate x ∈ X\τ has the best Bxcx . According to Lemma 4.3 there exists a t ∈ τ with
Bt ≥ x. S = ∅ means that the costs are the same for all candidates ct = cx, hence, Bxcx ≥
Bt
ct
.
i → i + 1: Choosing from τ for the first i iterations induces multiple selected regions in
the intersection space. Because of the nature of the τ-join certain time intervals are completely
covered.
Now choosing a candidate x ′ ∈ X \ τ could have a cost (where 0 ≤ cx ′ ≤ m) depending
on the number of constituent partitions already in the selection set. To prove that the choice of
the candidate is still made from τ we argue as in the proof of Lemma 4.3. Assume that there is
a better candidate x ∈ X \ τ (best benefit/cost ratio), and a non-zero cost cx. We can always
replace the partitions xi ∈ x \ S by another partition of the same term in the following ways:
Case 1 – xi /∈ S ∀xi ∈ x : In the case of x having no partitions from the selection set S, i.e.,
∀xi ∈ x : xi /∈ S
we use Lemma 4.3 to choose a better candidate from τ since there are only non-selected regions
from where a choice can be made.
Since the selected regions provide no benefit we operate only within unselected regions. We
denote the minimum time boundary in the region as left region boundary and the maximum
time boundary as the right region boundary. For cases 2 and 3, we consider candidates x with
non-zero benefit, and non-zero cost less thanm, i.e.,
∃xi, xi ∈ x ∩ S.
Case 2 – Suppose that the partition x ′i ∈ x ′ | x ′i ∈ S, only belong to the right region boundary.
We can always choose a replacement partition rj for x ′j ∈ x ′ | x ′j /∈ S, where rj and x ′j belong to
the same term, such that the new replacement candidate r ∈ τ has a better benefit than x ′. More
specifically, the replacement candidate r ∈ τ has the following selected and unselected partitions
• selected partitions: Selected partitions x ′i such that x ′i ∈ x ′ | x ′i ∈ S.
• unselected partitions: Unselected replacement partitions rj which contain the minimum
begin time of the selected partitions, tmbt = min{bx ′i |x
′
i ∈ x ′ ∧ x ′i ∈ S} , i.e.,
rj | brj ≤ tmbt < erj
The replacement candidate r has the same cost as its counterpart x ′, cost(r) = cost(x ′), and a
benefit-cost ratio Brcr ≥
Bx ′
cx ′
. Since such a replaced candidate belongs to τ, this is contrary to our
assumption and our claim holds.
Case 3 – Similar to Case 2, if x ′ has partitions belonging to the left boundary of the region,
we can replace the unselected partitions of each term by a replacement partition which con-
tains/overlaps with the maximum end time among the partitions which belong to the selection
set in x, i.e., tmet = max{ex ′i | x
′
i ∈ x ′ ∧ x ′i ∈ S}. The new replacement candidate r belongs to
τ and has a better or equal benefit than x contrary to our assumption. 
86
4.5. System Architecture
Versioned Document Collection
Query Processor
Query Interface
Vertically-
Partitioned Index
Synopsis 
Index
query plan
Indexing System
query
resultsquery
Wednesday, June 26, 13 Figure 4.3.: System architecture
4.5. System Architecture
Figure 4.3 shows a hihg-level overview of the indexing system where our selection
methods can be employed. The indexing system consists of vertically-partitioned index
and the synopsis index.
• Vertically-partitioned index : The entire document collection is indexed employ-
ing the partitioning schemes described in [BBNW07].
• Synopsis Index : During index building we materialize a KMV synopsis for ev-
ery partition into a synopsis index. When a query is issued, synopses (i.e., multiple
synopsis) corresponding to the affected partitions are retrieved. In the partition
selection algorithm that follows, GREEDYSELECT for single and multi-terms, car-
dinality estimates are determined for benefit values Bi and Bx.
The query interface can be a user of a system which generates time-travel queries.
During processing queries, query optimization is performed on the synopsis index by
87
Chapter 4. Query Optimization for Approximate Processing of Time-Travel Queries
invoking the selection algorithms and a plan is generated. Based on the query plan,
accesses are scheduled on the vertically-partitioned index and the results are reported.
Because of the anytime nature of the selection algorithm, the user can terminate the
search, when satisfied, giving her the maximum recall computed thus far.
4.6. Practical Issues
While the previous two sections presented the theoretical underpinnings for the partition-
selection problem, in this section, we discuss a few issues relevant to their implementa-
tion that we faced in practice and present our solutions.
4.6.1. Dealing with Partition and Query Boundary Alignment
In our descriptions of the algorithms, we assumed that if a partition overlaps with the
query time-interval, then its contribution to the final answer set is from all the postings
in the partition. In other words, we ignored the fact that even within a partition, pos-
sibly a large number of postings may not satisfy the temporal predicate if the temporal
boundaries of the partition are not completely contained within the range specified by
the temporal predicate. Note that this affects the estimates of the benefit values of the
partitions in the boundaries of the query time – thus the benefits of at most 2 partitions
per term are in error.
This error can be significantly improved if we adjust the value of benefit of a partition
to account for incomplete overlap along the time axis. A straightforward approach for
this, which we employ in our implementation, is to scale the benefits by the fraction of
temporal overlap between the query and the partition. In practice, we observed that this
simple scaling (which is similar to making an uniformity assumption during cardinality
estimates) works very well.
4.6.2. I/O Budget Underflow
Another issue that comes up when we are using only estimates of benefit provided by
partition(s) towards the final answer set is that during partition selection, we may en-
counter a situation where none of the partitions show any non-zero benefit, although in
reality they may contain some results. When faced with such a situation, the partition
selection algorithms described in Sections 4.4 and 4.3 simply terminate – even if the
specified I/O budget allows for more partitions to be read.
To avoid this undesirable behaviour, the partition-selection algorithm can be modified
to ignore the estimates of benefits when all the unselected partitions have zero estimated
88
4.7. Experimental Evaluation
benefits. At this stage, partitions are selected in decreasing order of their size as long as
the I/O budget is not violated.
4.7. Experimental Evaluation
4.7.1. Evaluation Framework
In this section, we present and discuss the results of a detailed experimental evaluation
of our algorithms in terms of their effectiveness in achieving high relative recall with a
specified budget of index accesses.
4.7.2. Setup
All our algorithms, including the underlying time-travel inverted index framework,
were implemented using Java 1.6. All experiments were conducted on Dell PowerEdge
M610 servers with 2 Intel Xeon E5530 CPUs, 48 GB of main memory, a large iSCSI-
attached disk array, and Debian GNU/Linux (SMP Kernel 2.6.29.3.1) as operating sys-
tem. Experiments were conducted using the Java Hotspot 64-Bit Server VM (build 11.2-
b01).
4.7.3. Datasets Used
For our experiments we used three different datasets, all derived from real-world data
sources.
WIKI The English Wikipedia revision history [WIK13], whose uncompressed raw data
amounts to 0.7 TBytes, contains the full editing history of the English Wikipedia
from January 2001 to December 2005. We indexed all versions of encyclopedia ar-
ticles excluding versions that were marked as the result of a minor edit (e.g., the
correction of spelling errors etc.). This yielded a total of 1,517,524 documents with
15,079,829 versions having a mean (µ) of 9.94 versions per document at standard
deviation (σ) of 46.08.
UKGOV This is a subset of the European Archive [EA13], containing weekly crawls of
eleven governmental websites from the U.K. We filtered out documents not be-
longing to MIME-types text/plain and text/html to obtain a dataset that to-
tals 0.4 TBytes. This dataset includes 685,678 documents with 17,297,548 versions
(µ = 25.23 and σ = 28.38).
NYT The New York Times Annotated corpus [NYT13] comprises more than 1.8 million
articles from the New York Times published between 1987 and 2007. Every article
89
Chapter 4. Query Optimization for Approximate Processing of Time-Travel Queries
has an associated time-stamp which was taken as the begin time for that article.
The end time for each article was chosen to be 90 days after the begin time, giving
every document a validity time of 90 days. This is done to reflect the real world
setting where the news articles are publicly available only for a limited period
from their publication.
Note that each of these datasets represents a realistic class of time-varying text collection
typically used in temporal text analytics. Specifically, WIKI corresponds to an explic-
itly version controlled text collection, UKGOV is an archive of the evolving Web, and
NYT is an instance of archive of continually generated newspaper content. For the ease
of experimentation, we rounded the time-stamps of versions to the nearest day for all
datasets.
4.7.4. Query Workload
We compiled three dataset-specific query workloads by extracting frequent queries from
the AOL query logs, which were temporarily made available during 2006. For the
WIKI dataset we extracted 300 most frequent queries which had a result click on the
domain en.wikipedia.org and similarly for NYT and UKGOV we compiled 300
queries which had a result hit on nytimes.com and 50 queries which had result hit
on .gov.uk domains (cf. Appendix). Using these keyword queries, we generated a
time-travel query workload with 3 instances each for the following 2 different temporal
predicate granularities: 30 days and 1 year.
4.7.5. Index Management
Index UKGOV NYT WIKI
Fixed-7 11GB 13GB 13GB
Synopsis Index - 5% sample 146MB 134MB 146MB
Synopsis Index - 10% sample 291MB 258MB 290MB
Fixed-30 4.4GB 3.5GB 6.3GB
Synopsis Index - 5% sample 61MB 39MB 75MB
Synopsis Index - 10% sample 122MB 74MB 149MB
Table 4.2.: Synopsis index
Since our selection techniques operate on temporally-partitioned posting lists, we
chose the following partitioning schemes :
90
4.7. Experimental Evaluation
• Fixed-time partitioning A simple partitioning scheme in which a partition bound-
ary is placed after a fixed time window. We present results for two time window
sizes: (i) 1 week (referred to as Fixed-7 partitioning), and (ii) 1 month (referred to
as Fixed-30 partitioning). Unless otherwise mentioned, all the results presented in
this chapter are from Fixed-7 partitioning.
• Vertical Partitioning These are index structures built using partitioning strategies
discussed in [BBNW07]). More specifically, we build index structures using the
space-bound approach with the parameters κ = 1.5, 3.0 as two representatives of
lower and higher degree of partitioning. These are represented as VERT-1.5 and
VERT-3.0 respectively.
Each of the above time-travel inverted indexes is stored on disk using flat files con-
taining both the lexicon as well as posting lists. At run time, the lexicon is read com-
pletely into memory, and for a given query the appropriate partition is retrieved from
the index flat file on disk. These posting lists are stored using variable-byte compres-
sion.
Synopsis structures The estimates from the KMV synopses [BHR+07] that we chose
to implement are naturally dependent on their size in relation to the raw data size. We
experimented with two sizes of synopses: 5% and 10% of the partition size (with mini-
mum size set to 100). Unless otherwise mentioned, we report results for 10% size of the
KMV synopsis. A synopsis index was generated during index construction time and
stored as flat files on disk. Instead of storing the list of hashed double values of the KMV
synopsis, the corresponding document identifiers (32-bit integers) were stored for bet-
ter compression (Table 4.2). The document identifiers were translated to their respective
doubles during query time for the necessary KMV intersection estimation. An addi-
tional entry in the lexicon was stored the offset in the synopsis index file corresponding
to the synopsis for each partition.
Finally, we employed a practically infeasible oracle for partition selection, which com-
putes the accurate values of set operations (intersection and union) between partitions.
Oracle computes these values by simply evaluating the query completely, without any
partition selection, and then uses them in partition selection to overcome the errors due
to estimates from the KMV synopses. In our experiments, we consider the oracle as
a competitor, where exact cardinalities are known, to compare against our synopsis-
based-selection approaches.
91
Chapter 4. Query Optimization for Approximate Processing of Time-Travel Queries
4.7.6. Evaluation Methodology
We evaluate the impact of partition selection by measuring the recall obtained at differ-
ent values of the parameter β. In each experiment we measure the average recall value
obtained per query, for a certain selection method, over increasing values of β from 0 to
1 with a step size of 0.1. During averaging we exclude the measurements for two types
of queries. First, we exclude queries with term/s not in the lexicon as they contribute
to false-positives for partition selection. Since we employ conjunctive query semantics
we need to select at least one partition per-term. Thus, secondly we also ignore queries
which result in exactly one affected partition for each term.
To compare the I/O performance of different techniques, we measure the number of
postings read after applying partition selection – denoted as RWS, and the number of
postings read without applying partition selection – denoted as RWOS. The ratio RWSRWOS ,
called Ratio-of-index read, is denoted as RIR. The ratio-of-index read captures the amount
of index accessed relative to the overall index-access cost for processing the entire query.
We also measure the wall-clock times during query processing, reported in milliseconds.
4.7.7. Performance of Partition Selection
In the first set of experiments, we examine the impact of both the selection methods
described in this chapter - size-based selection and equi-cost selection. To this end, we
execute the different query granularities on the Fixed-7 index for the given datasets, and
measure the recall values at various stages of query execution. The cost incurred at a
given stage of query execution is measured, as introduced above, by RIR. The results
presenting the recall levels achieved at different RIR values are shown in Figure 4.4
(size-based selection) and Figure 4.5 (equi-cost partition selection).
We observe that both selection algorithms achieve perfect recall already when access-
ing about 50% of the index. Since both the algorithms are incremental in nature, recall
always increases with an increase in allowable I/O budget β. Both the selection meth-
ods are able to achieve a recall of 80% by accessing less than 30% of the affected postings
for NYT. In UKGOV, a 80% recall is achieved by accessing 40% of the index. In WIKI,
the results are not as temporally clustered as in NYT and UKGOV but we still reach 80%
of recall by accessing around 60% of the affected postings.
Next, we observe that, with the exception of UKGOV, the partition-selection methods
respond better to month-granularity queries than the year-granularity queries. Month-
granularity queries have fewer affected partitions than year-granularity queries. This
suggests that there is (i) a high degree of temporal clustering of results and (ii) high
replication of postings, which the selection methods exploit. Selecting the partitions
with a high concentration of results gives the observed boost to the recall levels. The
92
4.7. Experimental Evaluation
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  0.2  0.4  0.6  0.8  1
R
ec
al
l
RIR
Year-gran. 
Month-gran. 
(a) Wikipedia
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  0.2  0.4  0.6  0.8  1
R
ec
al
l
RIR
Year-gran.
Month-gran. 
(b) UKGOV
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  0.2  0.4  0.6  0.8  1
R
ec
al
l
RIR
Year-gran.
Month-gran.
(c) New York Times
Figure 4.4.: Performance of size-based partition selection on Fixed-7 index
93
Chapter 4. Query Optimization for Approximate Processing of Time-Travel Queries
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  0.2  0.4  0.6  0.8  1
R
ec
al
l
RIR
Year-gran 
Month-gran. 
(a) Wikipedia
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  0.2  0.4  0.6  0.8  1
R
ec
al
l
RIR
Year-gran. 
Month-gran. 
(b) UKGOV
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  0.2  0.4  0.6  0.8  1
R
ec
al
l
RIR
Year-gran. 
Month-gran.
(c) New York Times
Figure 4.5.: Performance of equi-cost partition selection on Fixed-7 index
94
4.7. Experimental Evaluation
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  0.2  0.4  0.6  0.8  1
R
ec
al
l
RIR
Year gran.
Month gran.
(a) Wikipedia
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  0.2  0.4  0.6  0.8  1
R
ec
al
l
RIR
Year gran.
Month gran.
(b) UKGOV
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  0.2  0.4  0.6  0.8  1
R
ec
al
l
RIR
Year gran.
Month gran.
(c) New York Times
Figure 4.6.: Performance of size-based partition selection on VERT-3.0 index
95
Chapter 4. Query Optimization for Approximate Processing of Time-Travel Queries
subsequent choice of partitions understandably adds lower improvement than the ini-
tial choices exhibiting the property of diminishing returns. When we conducted the
same experiment on the vertically-partitioned index VERT-3.0, we did not observe gains
as significant as in Fixed-7. This is because the replication of the postings is controlled
and bounded.
The final observation which we make is that both the selection methods perform
almost at par. There is no considerable difference in performance and the choices of
partitions selected, in our experiments, is almost the same. This prompts the use equi-
cost partition selection which is easier to implement and has a proven approximation
guarantee. Thus from now on, the charts and tables contain results which employed
equi-cost partition selection.
4.7.8. Query-Processing Performance
In the next set of experiments,s we examine the performance of query processing, guided
by partitioning selection, by measuring wall-clock times during query execution. Since
partition selection is typically useful when there are no caching effects, the focus was on
measuring run-times in a cold-cache setup. We start with cold caches and flush them af-
ter each query execution step. Each time-travel query from the workload was evaluated
for different β values (0.1 through to 1.0) and the average time taken (in milliseconds)
for each of these bounds are presented in Tables 4.3 and 4.4. The first column represents
the tunable parameter β, followed by the recall attained, and finally the average wall-
clock per query. We compare the results of selection based retrieval by introducing two
competitors :
• the standard unpartitioned posting list, NOPARTITION, and
• partitioned lists not supporting partition selection, NOSELECTION.
Notice that the wall-clock times reported for β = 1.0 are the times taken for NOSE-
LECTION. The reported wall-clock times include the time taken by the synopsis-based
partition selection along with the time taken for the actual query processing. The time
taken for partition selection, however, is negligible and the major fraction of the overall
reported time is spent on query processing. The wall-clock times further corroborate the
observations presented before. We observe that in case of executing time-travel queries,
NOPARTITION takes almost 3 secs for WIKI, 1 sec for NYT and as long as 12 secs for
UKGOV (see Table 4.3) irrespective of the query-time granularity. Firstly, employing
a partitioned index results in superior performance as indicated by the NOSELECTION
values in the tables. Secondly, a partitioned index allows for partition selection fur-
ther reducing wall-clock times to give recall values of almost 0.8 in only 50%-60% of
96
4.7. Experimental Evaluation
Year Granularity Month Granularity
BOUND Recall Wall-clock times (ms) Recall Wall-clock times (ms)
WIKI
0.1 0.27 207.6 0.01 5.7
0.2 0.42 367.7 0.49 88.5
0.3 0.53 436.8 0.53 94.3
0.4 0.63 521.0 0.67 134.3
0.5 0.71 594.8 0.73 135.9
0.6 0.78 646.7 0.80 165.2
0.7 0.85 736.3 0.87 165.9
0.8 0.91 798.5 0.91 186.4
0.9 0.97 877.4 0.95 192.0
NOSELECTION 1 1.00 1,020.8 1.00 212.0
NOPARTITION 1 1.00 3,217.0 1.00 3,217.0
UKGOV
0.1 0.42 1,615.9 0.00 0.0
0.2 0.61 2,788.8 0.48 352.6
0.3 0.76 3,705.2 0.51 370.4
0.4 0.88 4,592.1 0.73 590.3
0.5 0.94 5,183.2 0.80 644.3
0.6 0.97 5,772.6 0.89 751.2
0.7 0.98 6,427.9 0.91 852.7
0.8 0.98 7,025.2 0.96 927.4
0.9 0.99 7,635.8 0.96 1,026.6
NOSELECTION 1 1.00 8,490.1 1.00 1,225.9
NOPARTITION 1 1.00 12,598.0 1.00 12,598.0
NYT
0.1 0.66 200.9 0.00 0.0
0.2 0.81 254.5 0.82 103.5
0.3 0.86 301.4 0.89 106.0
0.4 0.89 308.2 0.95 117.5
0.5 0.94 328.1 0.95 122.0
0.6 0.96 358.7 0.97 125.0
0.7 0.97 384.8 0.97 126.0
0.8 0.99 428.6 0.99 128.5
0.9 0.99 468.2 0.98 141.5
NOSELECTION 1 1.00 525.7 1.00 146.0
NOPARTITION 1 1.00 1,014.0 1.00 1,014.0
Table 4.3.: Wall-clock times for selection over Fixed-30
97
Chapter 4. Query Optimization for Approximate Processing of Time-Travel Queries
Year Granularity Month Granularity
BOUND Recall Wall-clock times (ms) Recall Wall-clock times (ms)
WIKI
0.1 0.28 105.8 0.26 12.8
0.2 0.43 154.4 0.42 33.4
0.3 0.55 212.2 0.55 44.0
0.4 0.65 268.1 0.62 55.5
0.5 0.73 320.5 0.70 64.6
0.6 0.81 375.6 0.77 75.6
0.7 0.87 429.4 0.84 87.4
0.8 0.92 477.6 0.89 97.2
0.9 0.96 539.0 0.93 104.3
NOSELECTION 1.0 1.00 596.4 1.00 129.0
NOPARTITION 1.0 1.00 3,217 1.00 3,217.0
UKGOV
0.1 0.25 295.1 0.27 2.3
0.2 0.42 601.6 0.45 41.8
0.3 0.56 915.6 0.65 124.9
0.4 0.68 1260.7 0.73 172.3
0.5 0.78 1544.4 0.83 239.7
0.6 0.82 1803.1 0.87 271.1
0.7 0.87 2098.3 0.90 325.0
0.8 0.91 2265.2 0.94 358.7
0.9 0.94 2436.2 0.95 395.7
NOSELECTION 1 1.00 2720.2 1.00 572.7
NOPARTITION 1 1.00 12,598 1.00 12,598.0
NYT
0.1 0.28 26.7 0.00 0.0
0.2 0.50 53.0 0.18 1.0
0.3 0.62 50.87 0.32 3.0
0.4 0.76 54.9 0.47 11.8
0.5 0.83 60.6 0.61 21.4
0.6 0.87 65.3 0.71 19.9
0.7 0.92 72.4 0.81 24.1
0.8 0.95 81.9 0.88 26.1
0.9 0.98 83.6 0.95 27.2
NOSELECTION 1.0 1.00 90.1 1.00 44.7
NOPARTITION 1.0 1.00 1,014 1.00 1,014.0
Table 4.4.: Wall-clock times for selection over VERT-(κ = 3.0)
98
4.7. Experimental Evaluation
time taken by NOSELECTION. Although, in smaller collections, like NYT, the absolute
improvements in wall-clock times might not be large but in larger corpora, which is
typically what web archives represent, the gains are substantial.
Additionally, the anytime nature of the selection algorithm also means that the user
can terminate the query processing at any instant she wishes and can still get the max-
imum recall at that stage of the computation. A quick preview at the results after 3/4
of a second can prove beneficial with almost 90% recall (UKGOV monthly-granularity
queries) or 85% recall (WIKI year-granularity queries). The results for space-bound in-
dexing follow a similar trend (as reported in Table 4.4).
4.7.9. Impact of using Synopses
The next set of experiments is aimed at quantifying the impact of using KMV synopses
for the estimation of benefits and the effect of different synopses size. For each dataset,
we measure the average recall obtained for each granularity of time-travel queries, us-
ing 5% and 10% synopses, and compare them with those of oracle outlined earlier. The
results of this experiments over indexes with Fixed-7 partitioning, are shown in Fig-
ure 4.7 for query-granularity of one year.
We can make the following observations from these plots: (i) The gap between a 5%
KMV synopsis and 10% synopsis is negligible, prompting our choice of using 5% KMV
synopsis. (ii) Although oracle-based estimates are, as expected, better overall, improve-
ments over using KMV synopsis estimates are not significantly large.
KMV synopsis are stored as arrays of doubles and much smaller than individual post-
ings and can also be compressed and kept in memory. We also see from our experiments
that query optimization using partition selection is a small fraction of the overall query-
processing time. Thus, with a small memory footprint, and a quick estimation capa-
bility, we perceive that the use of KMV synopsis is a reasonable choice for partition
selection. We used KMV-5% in all our experiments unless explicitly mentioned.
4.7.10. Impact of Partition Granularity
In our final experiment, we wanted to examine the effect of partition selection over
varying partitioning granularities. We experimented with two different granularities of
fixed partitioning – 7-day and 30-day time-intervals, resulting in Fixed-7 and Fixed-30
index configurations. Fixed-7 has a higher number of partitions, thus can be seen as
having smaller partition sizes in comparison to Fixed-30. Clearly, this allows efficient
processing of time-point or short duration queries. However, it deteriorates for larger
time-interval queries if partition selection is not employed. On the other hand, perfor-
mance with partition selection shown in Figure 4.8 for year-granularity queries, shows
99
Chapter 4. Query Optimization for Approximate Processing of Time-Travel Queries
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  0.2  0.4  0.6  0.8  1
R
ec
al
l
RIR
Oracle
kmv - 5%
kmv - 10%
(a) Wikipedia
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  0.2  0.4  0.6  0.8  1
R
ec
al
l
RIR
Oracle
kmv - 5%
kmv - 10%
(b) UKGOV
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  0.2  0.4  0.6  0.8  1
R
ec
al
l
RIR
Oracle
kmv - 5%
kmv - 10%
(c) New York Times
Figure 4.7.: Impact of using synopses
100
4.7. Experimental Evaluation
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  0.2  0.4  0.6  0.8  1
R
ec
al
l
RIR
Fixed-7
Fixed-30
(a) Wikipedia
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  0.2  0.4  0.6  0.8  1
R
ec
al
l
RIR
Fixed-7
Fixed-30
(b) UKGOV
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  0.2  0.4  0.6  0.8  1
R
ec
al
l
RIR
Fixed-7
Fixed-30
(c) New York Times
Figure 4.8.: Effect of varying partition granularity on FIXED indexes
101
Chapter 4. Query Optimization for Approximate Processing of Time-Travel Queries
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  0.2  0.4  0.6  0.8  1
R
ec
al
l
RIR
VERT-3.0
VERT-1.5
(a) Wikipedia
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  0.2  0.4  0.6  0.8  1
R
ec
al
l
RIR
VERT-3.0
VERT-1.5
(b) UKGOV
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  0.2  0.4  0.6  0.8  1
R
ec
al
l
RIR
VERT-3.0
VERT-1.5
(c) New York Times
Figure 4.9.: Effect of varying partitioning granularities on VERT indexes
102
4.8. Related Work
that even for smaller partitions sizes this issue can be effectively alleviated.
We also considered the partitions created by vertical partitioning of granularities —
VERT-1.5 and VERT-3.0. We see a similar effect as in the case of fixed partitioning,
i.e., selection is much more effective for partitioning schemes with high replication of
postings. In this case, selection over VERT-3.0 provides higher benefit consistently over
VERT-1.5 (see Figure 4.9).
4.8. Related Work
Closest to the ideas presented here is the work on time-travel text search [BBNW07] that
allows users to search only the part of a document collection that existed at a given time
point. To support this functionality efficiently, posting lists from an inverted index are
temporally partitioned either according to a given space bound or required performance
guarantee. Postings whose valid-time interval overlaps with multiple of the determined
temporal partitions are judiciously replicated and put into multiple posting lists, thus
increasing the overall size of the index.
Join-processing techniques for temporal databases [GJSS05] are a second class of re-
lated work whose focus, to the best of our knowledge, has been on producing accurate
query results opposed to the approximate results that our techniques deliver.
As data volumes grow, many queries are increasingly expensive to evaluate accu-
rately. However, an approximate but almost accurate answer that is delivered quickly
is often good enough. Approximate query processing techniques [AGP99, AGPR99]
developed by the database community aim at quickly determining an approximate an-
swer and, to this end, typically leverage data statistics (often approximated using his-
tograms), sampling, and other data synopses. In contrast to our scenario, approximate
query processing techniques target scenarios with a well-designed relational schema
that implies certain reasonable queries (e.g., based on foreign keys). When cast into
a relational schema, our scenario gives rise to millions of relations (corresponding to
terms and their corresponding partitions).
4.9. Summary
In this chapter, we present a framework for efficient approximate processing of keyword
queries over a temporally-partitioned inverted index. By using a small synopsis for
each partition we identify partitions that maximize the number of final non-redundant
results and schedule them for processing early on. Our approach aims to maximize the
recall at each stage of query execution given budget on the index-access cost.
103
Chapter 4. Query Optimization for Approximate Processing of Time-Travel Queries
Our experimental evaluation shows that our proposed methods can compute more
than 80% of final results even when the I/O budget is set as low as 50% of the total size
of the partitions that satisfy the temporal predicate. We derive the following insights
from our experimental results. Firstly, the choice of the selection methods does not
seem to affect the results. Both selection methods are able to deliver a relatively high
recall by accessing a small fraction of the index. Secondly, our model for expected query
processing time depending on the access costs is vindicated as the wall-clock times seem
to be correlated with the fraction of the index accessed. Finally, selection methods are
more effective for fine-granular indexes.
104
5
Phrase Indexing and Querying
5.1. Motivation and Problem Statement
Phrases are sequences of multiple words. Phrase queries are such multi-word sequences,
typically expressed with quotation marks (e.g., “the republic of india”). In this chapter
we deal with the problem of document retrieval for phrase queries– Given a document
collection D and a phrase query p, we intend to find all documents d ∈ D that literally
contain p. Our focus in this work is on supporting phrase queries more efficiently.
Phrase queries are supported by all modern search engines and are one of their most
popular among their advanced features. Phrases tend to be unambiguous concept mark-
ers [Sal89, CTL91, LC89] and are known to increase precision in search [DLP99]. Treat-
ing a query as a phrase yields documents which are closer to the intended concept
like “six pack”, “times square”, “hurt locker”. Even when unknown to the user, phrase
queries can still be implicitly invoked, for instance, by means of query-segmentation
methods [HPBS12, LHZW11]. Query segmentation refers to pre-retrieval algorithms
that automatically introduce phrase queries in the user’s input.
Beyond their usage in search engines, phrase queries increasingly serve as a build-
ing block for other applications such as (a) entity-oriented search and analytics [ACCG08]
(e.g., to identify documents that refer to a specific entity using one of its known labels),
(b) plagiarism detection [Sta11] (e.g., to identify documents that contain a highly discrim-
inative fragment from the suspicious document), (c) culturomics [MSA+10] (e.g., to iden-
tify documents that contain a specific n-gram and compute a frequency time-series from
their timestamps).
These applications are also relevant in the context of web archives making phrase
queries an important workload type. Search over archives can be extended to use phrase
queries by replacing the keyword component in time-travel queries with phrases. Inter-
esting applications which capture entity evolution could potentially use phrase queries
105
Chapter 5. Phrase Indexing and Querying
to aid entity extraction and tracking over periods of time.
5.1.1. Approach
Traditional approaches to phrase-query processing, as outlined in Chapter 2, uses in-
verted indexes with postings containing positional information. This posting payload
captures where words occur in documents has to be maintained to support phrase
queries, which leads to indexes that are larger. Büttcher et. al. [BCC10] report a fac-
tor of about 4× for the inverted index than those required for keyword queries. Phrase
query processing, unlike keyword queries where stop words can be ignored, considers
all words in the query. It enforces conjunctive query semantics and also devotes extra
processing cycles and memory to ensure that the terms occur in the same order as in the
query. Consequently, phrase queries are substantially more expensive to process.
The problem of substring matching, which is at the core of phrase queries, has been
studied extensively by the string-processing community. However, the solutions devel-
oped (e.g., suffix arrays [MM93] and permuterm indexes [FV10]) are designed for main
memory and cannot cope with large-scale document collections such as web archives.
Solutions developed by the information-retrieval community [TS09, WZB04] build on
the inverted index, extending it to index selected multi-word sequences, so-called phrases,
in addition to single words. The intuition behind indexing phrases into such an aug-
mented index exploit the fact that word sequences are more selective than words re-
sulting in improved response times. However the selection of which phrases to index
has been addressed in a limited manner. Typically phrases having a fixed length are
selected based on heuristics (e.g., whether they contain a stopword [CP08, WZB04]) or
taking into account characteristics of either the document collection [TS09] or the work-
load [WZB04]. Indexing additional phrases leads to an increase in index size which
brings us to the topic of the natural trade-off between inde size and query performance.
All the existing approaches barring [TS09] are agnostic to the index size blowup due to
indexing extra phrases and hence do not provide size-based index tuning.
Once we construct an augmented index as described before we now turn to how
queries are processed using such an enriched vocabulary. With the addition of more
terms to the lexicon there are multiple choices as to how a phrase query can be pro-
cessed. As an example, for a query “we are the champions” can be processed by {“we
are”, “we are the”, “champions”} or {“we are”, “the champions”}. A non-optimal choice can
sometimes lead to a considerable performance degradation compared to the best choice.
In the literature this problem of phrase-query optimization, that is, selecting a set of terms
to process a given phrase query has been addressed only using greedy heuristics.
We follow the general approach of augmenting the inverted index by selected phrases,
106
5.1. Motivation and Problem Statement
but our approach differs in several important aspects. Firstly, it allows for variable-length
phrases to be indexed while keeping the total index size under a user-specified size bud-
get. We tackle the problem of phrase selection, that is, deciding which phrases should
be indexed, by taking into account both the document collection and the workload. The
workload indicates how frequent a particular phrase appears in queries. The document
collection on the other hand establishes how expensive it is to index a given phrase. We
balance both the benefit of usage and storage cost of a phrase to chose a set of phrases
which maximize the expected query performance. Secondly, we take a more principled
approach to solving the query optimization problem by proposing algorithms which
produce an optimal or close-to-optimal set of terms. Note that the “time-travel” aspect
is orthogonal to the indexing methods proposed in this chapter. In principle one could
partition each posting list corresponding to a multi-word sequence for efficiently pro-
cessing time-travel queries where the keyword component is a phrase query.
5.1.2. Contributions
We make the following contributions in this chapter.
1. We introduce the augmented inverted index as a generalization of existing approaches.
2. We study the problem of phrase-query optimization, establish its NP-hardness, and
describe an exact exponential algorithm as well as an O(logn)-approximation al-
gorithm to its solution.
3. We propose two novel phrase-selection methods tunable by a user-specified space
budget that consider characteristics of both the document collection and the work-
load.
4. We carry out an extensive experimental evaluation on ClueWeb09 and a corpus from
The New York Times, as two real-world document collections, and entity labels
from the YAGO2 knowledge base, as a workload, comparing our approach against
state-of-the-art competitors and establishing its efficiency and effectiveness.
With as little as 5% additional space, our approach improves phrase-query processing
performance by a factor of more than 3× over a standard positional inverted index,
thereby considerably outperforming its competitors.
5.1.3. Organization
The rest of this chapter unfolds as follows. Section 5.2 introduces our formal model. The
augmented inverted index is described in Section 5.3. Section 5.4 deals with optimizing
107
Chapter 5. Phrase Indexing and Querying
phrase queries. Selecting phrases to be indexed is the subject of Section 5.5. Section 5.7
describes our experimental evaluation. We relate our work to existing prior work in
Section 5.8 and conclude in Section 5.9.
5.2. Model and Index Organization
We introduce our formal model and the notation used throughout the rest of the chapter.
For easy reference we also recap the notation in Table 5.1.
We let V denote the vocabulary of all words. The set of all non-empty sequences of words
from this vocabulary is denoted V+. Given a word sequence s = 〈 s1, . . . , sn 〉 ∈ V+, we
let | s | = n denote its length. We use s[i] to refer to the word si at the i-th position of s,
and s[i..j] (i ≤ j) to refer to the word subsequence 〈 si, . . . , sj 〉.
Definition 5.1 (Position within a sequence) Given two word sequences r and s, we let pos(r, s)
denote the set of positions at which r occurs in s, formally
pos(r, s) = { 1 ≤ i ≤ |s| | ∀ 1 ≤ j ≤ |r| : s[i+ j− 1] = r[j] } .
For r = 〈ab 〉 and s = 〈 cabcab 〉, as a concrete example, we have pos(r, s) = { 2, 5 }.
We say that s contains r if pos(r, s) 6= ∅. To ease notation, we treat single words from V
also as word sequences when convenient. This allows us, for instance, to write pos(w, s)
to refer to the positions at which w occurs in s.
Until now we operated on versions, however the approaches discussed in this chapter
are general enough to be applied to all text collections. Thus for ease of notation we
consider a document collection D of documents d ∈ D. Each document d ∈ D is a word
sequence from V+. Since we allow for duplicate documents,D is a bag of word sequences.
We letW denote our workload. Each query q ∈ W is a word sequence from V+. Since
we allow for repeated queries,W is also a bag of word sequences.
Using our notation, we now define the standard notions of document frequency and col-
lection frequency, as common in Information Retrieval. Let S be a bag of word sequences
(e.g., the document collection or the workload), we define the document frequency of
the word sequence r, as the total number of word sequences from S that contain it, as
df(r,S) = | { s ∈ S | pos(r, s) 6= ∅ } | .
Analogously, its collection frequency, as the total number of occurrences, is defined as
cf(r,S) =
∑
s∈S
|pos(r, s) | .
108
5.3. Indexing Framework
Notation Description
W workload, a bag of queries
q ∈ W a query
D collection of documents
d ∈ D a document
V vocabulary, a set of words
w ∈ V a word
V+ set of non-empty word sequences
s ∈ V+ a word sequence
L ⊆ V+ lexicon, the set of indexed word sequences
t ∈ D an indexed word sequence
S(D) size of the index for the lexicon D
Table 5.1.: Notation
5.3. Indexing Framework
Having introduced our formal model, we now describe the indexing framework within
which we operate.
We build on the inverted index as the most widely-used index structure in Information
Retrieval that forms the backbone of many real-world systems. The inverted index con-
sists of two components, namely, a lexicon L of terms and the corresponding posting lists
that record for each term information about its occurrences in the document collection.
For a detailed discussion of the inverted index and its efficient implementation we refer
to Chapter 2.
To support arbitrary phrase queries, an inverted index has to contain all words from
the vocabulary in its lexicon (i.e., V ⊆ L) and record positional information in its posting
lists. Thus, the posting (d13, 〈 3, 7 〉 ) found in the posting list for word w conveys that
the word occurs at positions 3 and 7 in document d13. More formally, using our nota-
tion, a posting ( id(d), pos(w,d) ) for wordw and document d contains the document’s
unique identifier id(d) and the positions pos(w,d) at which the word occurs.
Query-processing performance for phrase queries on such a positional inverted index
tends to be limited, in particular for phrase queries that contain frequent words (e.g.,
stopwords). Posting lists for frequent terms are long, containing many postings each of
which with many positions therein, rendering them expensive to read, decompress, and
process.
Several authors [CP08, TS09, WZB04] have proposed, as a remedy, to augment the
inverted index by adding multi-word sequences, so-called phrases, to the set of terms.
109
Chapter 5. Phrase Indexing and Querying
The lexicon L of such an augmented inverted index thus consists of individual words along-
side phrases (i.e., L ⊆ V+) as terms. Phrase selection can be done, for example, taking
into account their selectivity [TS09], whether they contain a stopword [CP08, WZB04],
or based on part-of-speech tags [MRS08]. Our approaches to select phrases, which take
into account both the document collection and the workload and keep index size within
a user-specified space budget, are detailed in Section 5.5.
To process a given phrase query q, a set of terms is selected from the lexicon L, and
the corresponding posting lists are intersected to identify documents that contain the
phrase. Intersecting of posting lists can be done using term-at-a-time (TAAT) or document-
at-a-time query processing (DAAT). For the former, posting lists are read one after each
other, and bookkeeping is done to keep track of positions at which the phrase can still
occur in candidate documents. For the latter, posting lists are read in parallel and a
document, when seen in all posting lists at once, is examined for whether it contains the
phrase sought. In both cases, the cost of processing a phrase query depends on the sizes
of posting lists read and thus the set of terms selected to process the query.
5.4. Query Optimization
In this section, we describe how a phrase query q can be processed using a given aug-
mented inverted index with a concrete lexicon L. Our objective is thus to determine, at
query-processing time, a subset P ⊆ L of terms, further referred to as query plan, that can
be used to process q.
To formulate the problem, we first need to capture when a query plan P can be used
to process a phrase query q. Intuitively, each word must be covered by at least one term
from P .
Definition 5.2 (Cover of a query)
covers(P,q) = ∀ 1 ≤ i ≤ |q | : ∃ t ∈ P : ∃ j ∈ pos(q[i], t) :
∀ 1 ≤ k ≤ | t | : q[i− j+ k] = t[k]
Consider a lexicon
L = { 〈a 〉, 〈b 〉, 〈 c 〉, 〈d 〉, 〈ab 〉, 〈bc 〉, 〈 cd 〉 } .
The phrase query q = 〈abc 〉, for instance, can be processed using { 〈ab 〉, 〈bc 〉 } but not
{ 〈ab 〉, 〈 cd 〉 }.
Without the augmented index, each query term was covered by exactly one term.
With the augmented index there are more choices to process each query term. The
110
5.4. Query Optimization
choices for covering b are 〈b 〉, 〈ab 〉 and 〈bc 〉. Multiple choices for processing each
position in the query leads to combinatorial choices for covering the query. In order to
chose the best query plan we would need to quantify the cost of each query cover.
As detailed above, in Section 5.3, different ways of processing a phrase query (i.e., TAAT
vs. DAAT) exist. In the worst case, regardless of which query-processing method is em-
ployed, all posting lists have to be read in their entirety. We model the cost of a query
plan P as the total number of postings that has to be read
c(P) =
∑
t∈P
df(t, C) .
While posting sizes are not uniform (e.g., due to compression and varying numbers
of contained positions), which may suggest collection frequency as a more accurate cost
measure, we found little difference in practice and thus, for simplicity, stick to document
frequency in this work. This is in line with [MTO12], who found that aggregate posting-
list lengths is the single feature most correlated with response time for full query evalu-
ation, as required for phrase queries, which also do not permit dynamic pruning.
Prior work in [TS09, WZB04] use cost-based heuristics to determine a valid query
plan. A candidate set of sequences ρ is first determined which are both present in the
query as well as the lexicon, i.e.,
ρ = { r | r ∈ L ∧ pos(r, q) 6= φ } .
Candidates are then greedily chosen from R in the order of ascending df values until
the entire query is covered. In every round the best candidate which overlaps with the
uncovered regions of the query is chosen. However, such a heuristic algorithm might
not yield an optimal plan. We illustrate this with an example. Consider a lexicon Lwith
the following df values
df(〈ab 〉) = 10
L = { 〈a 〉, 〈b 〉, 〈 c 〉, 〈d 〉, 〈ab 〉, 〈bc 〉, 〈 cd 〉 } df(〈ab 〉) = 10
df(〈 cd 〉) = 60
A phrase query q = 〈abcd 〉 evaluated according to the greedy heuristic above yield the
following plan – GRD = { 〈ab 〉, 〈bc 〉, 〈 cd 〉 } – with an cost c(GRD) = 120. It is easy
to see that the optimal cover OPT of this query is OPT = { 〈ab 〉, 〈 cd 〉 } with an cost
c(OPT) = 70.
Theoretically, one can construct a worst case scenario which can lead to arbitrary
degradation of performance. As an example let there be a query q = 〈q1 · · ·qm 〉. As-
suming that only bi-grams are indexed and the distribution of df values are such that
111
Chapter 5. Phrase Indexing and Querying
df(〈qiqi+1 〉) < df(〈qi+1qi+2 〉). The cost of the query plan using the greedy heuristic is∑
1≤i<m df(〈qiqi+1 〉).
We take a more principled approach and model the query-optimization as an op-
timization problem. Assembling the above definitions of coverage and cost, we now
formally define the problem of finding a cost-minimal query plan P for a phrase query
q and lexicon L as the following NP-hard optimization problem
Definition 5.3 PHRASE-QUERY OPTIMIZATION
argmin
P⊆L
c(P) s.t. covers(P,q) .
Theorem 5.1 PHRASE-QUERY OPTIMIZATION is NP-hard.
Proof: Our proof closely follows Neraud [Nér90], who establishes that deciding whether a
given set of strings is elementary is NP-complete. We show through a reduction from VERTEX
COVER that the decision variant of PHRASE-QUERY OPTIMIZATION (i.e., whether a P with
c(P) ≤ τ exists) is NP-complete, from which our claim follows.
Let G(V, E) be an undirected graph with vertices V and edges E. We assume that there are no
isolated vertices, i.e., each vertex has at least one incident edge. VERTEX COVER asks whether
there exists a subset of vertices VC ⊆ V having cardinality |VC | ≤ k, so that
∀ (u, v) ∈ E : u ∈ VC∨ v ∈ VC,
that is, for each edge one of its connected vertices is in the vertex cover.
We obtain an instance of PHRASE-QUERY OPTIMIZATION from G(V, E) as follows:
• V = V ∪ E – we introduce a word to the vocabulary for each vertex (v) and edge (uv) in
the graph;
• q =
⊎
(u,v)∈E
u uv v – we obtain the query q as a concatenation of all edge words uv brack-
eted by the words of their source (u) and target (v);
• D = {q } – the document collection contains only a single document that equals our query;
• L = V ∪
⋃
(u,v)∈E
{ 〈u uv 〉 } ∪
⋃
(u,v)∈E
{ 〈uv v 〉 } – each vertex word (v) and edge word
(uv) is indexed as well as each combination of edge and source (uuv) and edge and target
(uv v).
This can be done in polynomial time and space. Note that, by construction, ∀ t ∈ L : df(t,D) =
1 holds. We now show that G(V, E) has a vertex cover with cardinality |VC | ≤ k iff there is a
query plan P with c(P) ≤ k+ |E |.
112
5.4. Query Optimization
(⇒) Observe that VC contains at least one of u or v for each u uv v from the query, incurring
a cost of |VC | ≤ k. We can complement this to obtain a query plan P by adding exactly one
term (〈uv 〉, 〈uuv 〉, or 〈uv v 〉) for each u uv v from the query, incurring a cost of |E |. Thus,
c(P) ≤ k+ |E | holds.
(⇐) Observe thatP must cover eachu uv v from the query in one of four ways: (i) 〈u 〉〈uv 〉〈 v 〉,
(ii) 〈u 〉〈uv v 〉, (iii) 〈uuv 〉〈 v 〉 (iv) 〈uuv 〉〈uv v 〉. Whenever a u uv v from the query is
covered as 〈uuv 〉〈uv v 〉, we replace 〈uuv 〉 by 〈u 〉, thus reducing case (iv) to case (ii).
We refer to the query plan thus obtained as P ′. Note that c(P ′) ≤ c(P), since all terms
have the same cost. P ′ contains exactly one term (〈uv 〉, 〈uuv 〉, or 〈uv v 〉) for each u uv v
from the query, incurring a cost of |E |. Let VC be the set of vertices whose words are con-
tained in P ′. We can thus write c(P ′) = |VC | + |E |. VC is a vertex cover, since after
eliminating case (iv), each u uv v from the query is covered using either 〈u 〉 or 〈 v 〉. Thus,
c(P ′) = |VC |+ |E | ≤ c(P) ≤ |E |+ k⇒ |VC | ≤ k. 
5.4.1. Optimal Solution
If an optimal query plan P∗ exists, so that every term therein occurs exactly once in the
query, we can determine an optimal query plan using dynamic programming based on
the recurrence
OPT(i)=

df(q[1..i],D) : q[1..i] ∈ L
min
j<i
OPT(j) +min
k≤j+1∧
q[k..i]∈L
df(q[k..i],D)
 : otherwise
in time O(n2) and space O(n) where |q | = n. OPT(i) denotes the cost of an optimal
solution to the prefix subproblem q[1..i] – once the dynamic-programming table has
been populated, an optimal query plan can be constructed by means of backtracking. In
the first case, the prefix subproblem can be covered using a single term. In the second
case, the optimal solution combines an optimal solution to a smaller prefix subproblem,
which is the optimal substructure inherent to dynamic programming, with a single term
that covers the remaining suffix.
Theorem 5.2 If there is an optimal query plan P∗ for a phrase query q such that
∀ t ∈ P∗ : |pos(t,q) | = 1 ,
then c(P∗) = OPT(|q |), that is, an optimal solution can be determined using the recurrence
OPT.
Proof: Observe that Theorem 5.2 is a special case of Theorem 5.4 for F = ∅. We therefore only
prove the more general Theorem 5.4. 
113
Chapter 5. Phrase Indexing and Querying
It entails that we can efficiently determine optimal query plans for phrase queries that
do not contain any repeated words.
Corollary 5.3 We can compute an optimal query plan for a phrase query q in polynomial time
and space, if
∀ 1 ≤ i ≤ |q | : |pos(q[i],q) | = 1 .
In practice this special case is important: We found that about 99% of phrase queries in
our workload do not contain any repeated words, as detailed in Section 5.7.
Algorithm We present the algorithm which efficiently implements the recurrence re-
lation. The input to the algorithm are the query q and the lexicon L. A set of candidate
word sequencesR is constructed such thatCand = { s ∈ L : |pos(s,q) | > 1 }. Let us de-
note the begin and of a sequence s ∈ R as begin(s) and end of the sequence as end(s).
For example, begin(q) = 0 and end(q) = |s|− 1. The cost of each sequence s is denoted
by c(s) = df(s).
We note that the sequences have a anti-monotonocity property on their costs as stated
below which we utilize to prune the candidate set of sequences.
Lemma 5.1 (Cost-anti monotonicity of sequences) For a pair of sequences r and s, it holds
|pos(r, s)| > 1 =⇒ df(r) ≥ df(s).
Proof: Every occurence of a sequence of s in the collection is also an occurence of r. 
We exploit the cost-anti monotonicity property of sequences to eliminate sequences
inRwhich are substrings of other candidate sequences. If the entire query q is indexed,
i.e, q ∈ L, then all the remaining candidates are pruned out and we can terminate
immediately.
After the pruning step it is easy to see that each there can be only one sequence which
begins from or ends at any given position 0 ≤ i < |q| in the query. We organize the
remaining sequences into a list I ordered by their increasing end positions end(s), i.e,
I = 〈 si 〉 : si ∈ Cand ∧ @sj ∈ I, pos(si, sj) > 0 ∧ end(si) ≤ end(si+1).
Dynamic programming proceeds by computing optimal solutions OPT(i) to the pre-
fix subproblems q[1...i], i.e, optimal cost of covering the first i positions of (q). Since
I is ordered ends of sequences, in every iteration we consider a new sequence s which
increases the problem size to q(1...end(s)). We compute OPT(end(s)) by trying to mini-
mize OPT(j)+c(s) for 1 < j < end(s). We also ensure that the new solution OPT(end(s))
also a cover for q(1...end(s)). Thus s) is the last sequence in the optimal solution for the
q(1...end(s)). We store these last sequences for optimal subproblems in a queueQ. Thus
114
5.4. Query Optimization
each sequence t in a queue encodes the solution to the subproblem q(1...end(t)), i.e.,
OPT(end(t)). Since each element of Q is the last sequence in their respective subprob-
lems, the future sequences considered only have to check overlaps with the sequences
in the queue to ensure that the subproblems that they represent are covered. We let
last(Q) denote the sequence at the end of the queue or the last sequence added.
We establish an invariance in plan cost, i.e., OPT(i + 1) > OPT(i) over the sequences
inQ. Thus if an incoming sequence computes with end i has OPT(i) greater than earlier
values of OPT in the queue, then those sequences are removed until the invariance is
established. After determining the optimal solution for which s is the last sequence,
and maintaining the invariance by pruning out sequences in Q, s is appended to the
end of Q.
After we have exhausted all input values from I, as a final step, we backtrack from
the last sequence representing OPT(|q|) to construct the plan with the minimum cost (c.f.
lines 18-21). Then invariance improves the efficiency of the algorithm by decreasing the
number of comparisons (i) while determining a feasible cover (line 7) and (ii) while
backtracking for the determination of the final plan.
Queries with Repetition Otherwise, when there is no optimal query plan P∗ accord-
ing to Theorem 5.2, dynamic programming can not be directly applied, since there is
no optimal substructure. Consider, as a concrete problem instance, the phrase query
q = 〈abxayb 〉 with lexicon L = { 〈a 〉, 〈b 〉, 〈 x 〉, 〈y 〉, 〈ab 〉 } and assume df(t,D) > 1
for t ∈ { 〈a 〉, 〈b 〉, 〈 x 〉, 〈y 〉 } and df(〈ab 〉,D) = 1. For this problem instance, the op-
timal solution P∗ = { 〈a 〉, 〈b 〉, 〈 x 〉, 〈y 〉 } does not contain an optimal solution to any
prefix subproblem q[1..i] (1 < i < |q |), which all contain the term 〈ab 〉.
However, as we describe next, an optimal query plan can be computed, in the general
case, using a combination of exhaustive search over sets of repeated terms and a variant
of our above recurrence.
For a phrase query q let the set of repeated terms be formally defined as
R = { t ∈ L | |pos(t,q) | > 1 } .
Let further F ⊆ R denote a subset of repeated terms. We now define a modified docu-
ment frequency that is zero for terms from F , formally
df ′(t,D) =
{
0 : t ∈ F
df(t,D) : otherwise
and denote by OPT ′ the variant of our above recurrence that uses this modified docu-
ment frequency.
115
Chapter 5. Phrase Indexing and Querying
Algorithm 7: Dynamic programming solution for optimal plan for queries with
non-repeated tokens
Input: Phrase query q, lexicon L
Output: Cost optCost of optimal query plan
begin1
Cand = { s ∈ L : |pos(s,q) | > 0 };2
I = 〈 si 〉 : si ∈ Cand ∧ @sj ∈ I, pos(si, sj) > 0 ∧ end(si) ≤ end(si+1);3
Q←− 〈i1〉;4
//Q is the queue of processed sequences.5
for s ∈ I do6
imin = argmin
ik∈Q ∧ begin(ik)≤begin(s)≤begin(ik)
(end(ik) − begin(s));
7
// imin is the sequence with the minimum intersection with ij. if imin = ∅8
then
OPT(imin) = 0 ;9
OPT(end(s)) = OPT(imin) + c(s);10
// Preserving increasing score order in Q11
while OPT(end(Q.last)) > OPT(end(s)) do12
removeLastSequence(Q);13
AppendInterval(s, Q) ;14
optCost = OPT(end(Q.last));15
// Backtracking for plan construction16
inext ← Q.last ;17
while inext 6= Q.first do18
P ← P ∪ inext ;19
inext = argmin
ik∈V ∧ (inext∩ik 6=∅)
(end(ik) − begin(inext))
20
end21
Algorithm 8 considers all subsets of repeated terms and, for each of them, extends it
into a query plan for q by means of the recurrence OPT ′. The algorithm keeps track of
the best solution seen and eventually returns it. Its correctness directly follows from the
following theorem.
116
5.4. Query Optimization
Algorithm 8: Phrase-query optimization
Input: Phrase query q, lexicon L
Output: Cost optCost of optimal query plan
1 R = { t ∈ L : |pos(t,q) | > 1 }
2 optCost =∞
3 for F ∈ 2R do
4 cost = c(F) + OPT ′(|q|)
5 if cost < optCost then
6 optCost = cost
7 return optCost
Theorem 5.4 Let P∗ denote an optimal query plan for the phrase query q and let
F = { t ∈ P∗ | |pos(t,q) | > 1 }
be the set of repeated terms therein, then
c(F) + OPT ′(|q |) ≤ c(P∗) .
Proof: Let P∗ denote an optimal query plan for the phrase query q and
F = { t ∈ P∗ | |pos(t,q) | > 1 }
be the set of repeated terms and F̄ = P∗ \ F be the set of non-repeated terms therein. Without
loss of generality, we assume that q ends in a non-repeated terminal term # having df(#,D) = 0
– this can always be achieved by “patching” the query. We order non-repeated terms t ∈ F̄ by
their single item in pos(t,q) to obtain the sequence 〈 t1, . . . , tm 〉 with m =
∣∣ F̄ ∣∣. We refer to
the first position covered by ti, corresponding to the single item in pos(t,q), as bi and to the last
position as ei = (bi + | ti |− 1).
We now show by induction that
OPT ′(ei) ≤
i∑
j=1
df(tj,D) = c(P∗) − c(F) .
(i = 1) We have to distinguish two cases: (i) b1 = 1, that is, q[1..e1] is covered using a single
non-repeated term – OPT’ selects this term according to its first case. (ii) b1 > 1, that is, there
is a set of repeated terms from F that covers q[1..k] for some b1 − 1 ≤ k < e1 – OPT’ can select
the same repeated terms at zero cost and combine it with t1 that covers q[b1..e1]. Thus, in both
cases, OPT ′(e1) ≤ df(t1,D).
117
Chapter 5. Phrase Indexing and Querying
(i→ i+ 1) We assume OPT ′(ei) ≤∑ij=1 df(tj,D). Again, we have to distinguish two cases:
(i) ei ≥ bi+1 − 1, that is, the term before ti+1 is also a non-repeated term. Thus, our recurrence
considers OPT ′(ei) + df(ti+1,D) as one possible solution. (ii) e1 < bi+1 − 1, that is, there is a
gap covered by repeated terms between ti and ti+1 – OPT’ can select the same repeated terms at
zero cost and thus considers OPT ′(ei) + 0 + df(ti+1,D) as one solution. Thus, in both cases,
OPT ′(ei+1) ≤
∑i+1
j=1 df(tj,D). 
The cost of Algorithm 8 depends on the number of repeated terms |R |, which is small
in practice and can be bounded in terms of the number of positions in q occupied by a
repeated word
r = | { 0 ≤ i ≤ |q | | |pos(q[i],q) | > 1 } | .
For our above example phrase query q = 〈abxayb 〉 we obtain r = 4. Note that the
following holds |R | ≤ r·(r+1)2 . Algorithm 8 thus has time complexity O(2
r·(r+1)
2 n2) and
space complexity O(n2) where |q | = n.
5.4.2. Approximation Guarantee
Computing an optimal query plan can be computationally expensive in the worst case,
as just shown. We observe that our query-optimization problem can be seen as an in-
stance of SET COVER [Vaz01]. This means that we can re-use well known approximation
algorithms for SET COVER which are known to be efficient. We state the SET COVER
problem and show how our query-optimization problem is its instance.
Definition 5.4 (SET COVER problem) Given a universe U of n elements, a collection of sub-
sets U , S = {S1, . . . , Sn}, and a cost function c : S =⇒ Q+, finds a minimum cost sub-
collection of S that covers all elements of U .
To this end, we convert an instance of our problem, consisting of a phrase query q
and a lexicon L with associated costs, into a SET COVER instance as follows: Let the
universe of items U = { 1, . . . , |q | } correspond to positions in the phrase query. For each
term t ∈ L, we define a subset S t ⊆ U of covered positions as
S t = { 1 ≤ i ≤ |q | | ∃ j ∈ pos(t,q) : j ≤ i < j+ | t | } .
The collection of subsets of U is then defined as
S = {S t | t ∈ L }
and we define cost(S t) = df(t,D) as a cost function.
For our concrete problem instance q = 〈abxayb 〉 andL = { 〈a 〉, 〈b 〉, 〈 x 〉, 〈y 〉, 〈ab 〉 }
from above, we obtain U = { 1, . . . , 6 } and S = { { 1, 4 } , { 2, 6 } , { 3 } , { 5 } , { 1 } } as the cor-
responding SET COVER instance.
118
5.5. Phrase Selection
We can now use the greedy algorithm which is known to be aO(logn)-approximation
algorithm [Vaz01]. The greedy algorithm iteratively the most cost effective set from S
until it covers all elements. The cost-effectiveness is defined as a benefit-cost ratio where
benefit is the number of yet-uncovered elements and the cost is cost(St). This can be
implemented in O(n2) time and O(n2) space where |q | = n.
Note that, as a key difference to the greedy algorithm described in [WZB04], which
to the best of our knowledge does not give an approximation guarantee, our greedy
algorithm (APX) selects subsets (corresponding to terms from the lexicon) taking into
account the number of additional items covered and the coverage already achieved by
selected subsets. This is unlike the approach in [WZB04], referred to as GRD, where
items are also chosen greedily based solely on their costs. The number of uncovered
items is not factored in and hence they do not make a distinction if one element is un-
covered or many. As an example consider a q = 〈abcd 〉 and a lexicon L with the
following costs:
cost(〈ab 〉) = 10
L = { 〈a 〉, 〈b 〉, 〈 c 〉, 〈d 〉, 〈ab 〉, 〈bc 〉, 〈 cd 〉 } cost(〈bc 〉) = 20
cost(〈 cd 〉) = 30
GRD would in the first choose 〈ab 〉, followed by 〈bc 〉 and finally 〈 cd 〉. However,
APX after choosing 〈ab 〉 in the first round would choose 〈 cd 〉 over 〈bc 〉 which has a
better benefit-cost ratio because of larger number of uncovered positions.
5.5. Phrase Selection
Having described how phrase queries can be efficiently processed on a given aug-
mented inverted index, we now turn to the complementary problem of phrase-selection.
We identify two key ingredients on which we build upon. Firstly, we analyze the work-
load to determine the most frequently used word sequences. The frequency of usage
of a sequence is an indicator that it is an important sequence and a potential compo-
nent in many queries in the workload. But a word sequence might be frequent, in the
workload or collection or both, but might be expensive in terms of query processing
and storage. Consider the phrase “of the”, which is a frequent sequence in the work-
load but also frequent in the document collection. A high df value of the phrase means
that the corresponding posting list would be big leading high storage overhead. Since
we model our query processing cost as the sum of df values of the participating terms,
it also means that using “of the” has performance overheads as well. However, note
119
Chapter 5. Phrase Indexing and Querying
that the performance using “of the” is still better than using the single terms “of” and
“the”. This brings us to the second ingredient which is the cost of using a certain word
sequence. We determine the cost of usage of the word sequence from the occurrence in
the document collection. In our methods we balance these two ingredients to select a
set of sequences to be indexed which are perceived to improve query performance.
In what follows, we introduce two phrase-selection methods that determine, at index-
build time, which phrases should be included in the lexicon, taking into account both
characteristics of the document collection and the workload.
5.5.1. Query-Optimizer-Based Phrase Selection
Our first method, coined query-optimizer-based phrase-selection (QOBS), builds on
Section 5.4 and considers how phrase queries from the workload would actually be
processed if a specific lexicon was available.
Let c(q,L) denote the cost of processing the phrase query q with lexicon L, in terms
of the number of postings that have to be read, determined using one of the query-
optimization methods from Section 5.4. We define the expected cost of processing a
phrase query from the workloadW with lexicon L as
c(W,L) = 1
|W |
∑
q∈W
c(q,L) .
Recall that W is a bag of word sequences, so that repeated phrase queries are taken
into account. The benefit of having the lexicon L instead of only single words from V ,
which serves as our baseline as described in Section 5.3, as the improvement in expected
processing cost is defined as
b(L) = c(W,V) − c(W,L) .
The space consumed by the augmented inverted index with lexicon L is captured as
s(L) =
∑
t∈L
df(t,D) ,
corresponding to the total number of postings in the index.
Our objective is to compile a lexicon that minimizes the expected processing cost for
phrase queries from the workload and results in an index whose size is within a user-
specified space budget. We model the user-specified space budget as a percentage over-
head α (0 ≤ α) over the size of our baseline augmented inverted index having lexicon
L = V . We obtain the optimization problem
Definition 5.5 QUERY-OPTIMIZER-BASED PHRASE SELECTION
argmax
V ⊆L⊆V+
b(W,L) s.t. s(L) ≤ (1+ α) · s(V) .
120
5.5. Phrase Selection
Observe that our formulation implicitly encodes the trade-off between how frequent
a phrase is and how expensive it is to process. We obtain the first information from the
workload and the second from the document collection. A phrase s having low df(s,L)
and high df(s,W) is more likely to be selected. In addition, our formulation takes into
account the cost of the terms that a phrase substitutes. For two candidate phrases s and
r with df(s,L) = df(r,L), the one that substitutes more expensive terms is preferred.
Unfortunately, our optimization problem is NP-hard as we prove below.
Theorem 5.5 QUERY-OPTIMIZER-BASED PHRASE SELECTION is NP-hard.
Proof: We show this through reduction from the 0-1 or binary KNAPSACK PROBLEM which
is proven to be NP complete [LSS88]. In the binary knapsack problem, we have a set of n
items {ei} associated with integral weights {wi} and profits {pi}. We want to select items to put
in the knapsack such that the sum of the profits is maximized and the sum of weights is less than
L - which is the size of the knapsack.
We can map every instance of the binary problem into our problem by the following construc-
tion: (i) We construct a query workload with n two word queries, each query 〈aibi 〉 correspond-
ing to an item ei, where both words are different and also requiring that no two queries share a
word. (ii) We create a document collection of n documents, where each document contains the
phrase 〈aibi 〉 exactly wi times. This ensures that df(〈aibi 〉,D) = wi.
We additionally populate the rest of the document with single words ai and bi such that
df(〈ai 〉,D) + df(〈bi 〉,D) = pi +wi. This ensures that the benefit of materializing a lexicon
L = { 〈aibi 〉 } is pi. We want to maximize the benefit of materializing a lexicon L subject to the
constraint that the sum of costs is not greater than a given space budget, which maps to the same
objective function as the binary knapsack problem. 
Our objective function is non-decreasing, so that L ⊆ L ′ ⇒ b(L) ≤ b(L ′) – the overall
benefit can only improve when a phrase is added to the lexicon. However, somewhat
surprisingly, it is not submodular and thus does not have the property of diminishing
returns, which has unfortunate ramifications detailed below.
To show this, we present a counter example. Let the marginal benefit of adding the
phrase s to lexicon L be defined as
m(s,W,L) = b(W,L ∪ { s }) − b(W,L)
= c(W,L) − c(W,L ∪ { s }) .
Now consider q = 〈abcd 〉 as the only phrase query inW , two dictionaries L1 and L2
L1 = { 〈a 〉, 〈b 〉, 〈 c 〉, 〈d 〉, 〈bc 〉 } ,
L2 = { 〈a 〉, 〈b 〉, 〈 c 〉, 〈d 〉, 〈ab 〉, 〈bc 〉 } .
121
Chapter 5. Phrase Indexing and Querying
Further, assume that df(t,D) = 2 for single words t ∈ { 〈a 〉, 〈b 〉, 〈 c 〉, 〈d 〉 } and
df(t,D) = 1 for phrases t ∈ { 〈ab 〉, 〈bc 〉, 〈 cd 〉 }. When using OPT from Section 5.4,
we obtain c(q,L1) = 5 and c(q,L2) = 4. When we add 〈 cd 〉 to L1 and L2, respec-
tively, we obtain c(q,L1 ∪ { 〈 cd 〉 }) = 4 (from the query plan { 〈a 〉, 〈bc 〉, 〈 cd 〉 }) and
c(q,L2 ∪ { 〈 cd 〉 }) = 2 (from the query plan { 〈ab 〉, 〈 cd 〉 }) – expressed in terms of
marginal benefits m(〈 cd 〉,W,L1) = 1 and m(〈 cd 〉,W,L2) = 2. Although L1 ⊂ L2,
the marginal benefit of adding the phrase 〈 cd 〉 to L2 is larger.
Since our objective function is not submodular, we can not leverage the result from
Nemhauser et al. [NWF78], which shows that the greedy algorithm that selects items in
descending order of their benefit-cost ratio has a (1− 1e)-approximation guarantee.
Nevertheless, even without an approximation guarantee, we rely on this greedy al-
gorithm to compile the lexicon L. The algorithm considers as candidates all phrases
that are contained in both the workload and the document collection – no other phrase
can impact our objective function. It iteratively extends the lexicon L until the space
budget is exhausted. In every iteration, the algorithm selects the phrase that has the
largest benefit-cost ratio m(s,W,L)/df(s,D). After adding a phrase to the lexicon L,
the benefit-cost ratios of the remaining candidates have to be updated. To this end,
for each of the candidates, the algorithm performs a what-if analysis, computing query
plans for each phrase query from the workload under the assumption that the candidate
has been added to the lexicon. This is clearly prohibitive when implemented naı̈vely. It
can be made feasible in practice by keeping track of which candidates can potentially
be used to process which phrase query. One can then selectively update the benefit-cost
ratios of only those candidates that can potentially be used to process any of the phrase
query whose query plan has changed after the latest addition to the lexicon.
5.5.2. Coverage-Based Phrase Selection
While QOBS presented above is aware of the query-optimization method used and ac-
tively invokes it, our second phrase-selection method, coined coverage-based phrase-
selection (CBS), is agnostic to phrase-query optimization. It is based on a simpler prob-
lem formulation that considers how many distinct positions in phrase queries from
the workload can be covered using phrases from the lexicon and also keeps index size
within a user-specified space budget.
We first formally define the notion of coverage of a position. A word sequence s =
〈w1w2 . . . wn 〉 is a sequence of n words where the index n indicates the nth position or
nth word of the sequence. A position k is said to be covered by a phrase p, |p| > 1, if the
following holds ∃i ∈ pos(s, p)|i ≤ k ≤ i+ |p|.
Using this notion of a covered position we extend it to capture the concept of positions
122
5.5. Phrase Selection
covered by a word sequence p in s denoted by poscov(p, s).
poscov(p, s) = {k ∈ Z+ | i ∈ pos(s, p), i ≤ k ≤ i+ |p| }.
As an example positions covered by 〈ab 〉 in phrase 〈abxyab 〉 is
poscov(〈abxyab 〉,V ∪ { 〈ab 〉 }) = {1, 2, 4, 5}.
To measure how much of a phrase query q can be covered using phrases from the
lexicon L we introduce the measure coverage(q,L). The value of coverage(q,L) value
ranges in [0, |q |] and conveys how many distinct positions from the phrase query can
be covered. Formally,
Definition 5.6 (Coverage)
coverage(q,L) =
∣∣∣∣∣∣
 ⋃
p∈L
poscov(p,q) | ∀pos(p,q) > 0

∣∣∣∣∣∣ .
In other words, coverage encodes the distinct positions covered by word sequences
(not single words) from the lexicon in a query. Some examples of coverage are given
below.
coverage(〈abxy 〉,V ∪ { 〈abc 〉, 〈 xy 〉 }) = 2
coverage(〈abaxaba 〉,V ∪ { 〈aba 〉, 〈ab 〉 }) = 6
coverage(〈ababac 〉,V ∪ { 〈aba 〉, 〈 xy 〉 }) = 5 .
We extend our definition of coverage to the workload as
coverage(W,L) =
∑
q∈W
coverage(q,L) .
Again,W is a bag of word sequences, so that the coverage of repeated phrase queries is
reflected.
Our objective is to compile a lexiconL that maximizes the coverage(W,L) and results
in an index whose size is within a user-specified space budget. To measure index size
and model the user-specified space budget, we use the same formalism as in QOBS. We
obtain the optimization problem
Definition 5.7 COVERAGE-BASED PHRASE SELECTION
argmax
V ⊆L⊆V+
coverage(W,L) s.t. s(L) ≤ (1+ α) · s(V) .
123
Chapter 5. Phrase Indexing and Querying
This problem formulation considers the same ingredients as the problem formulation
behind QOBS. It implicitly takes into account the frequency df(s,W) of a phrase s in the
workload, using the coverage measure, and also df(s,D) reflecting how expensive the
phrase is to select. Unlike QOBS, this formulation does not take into account the costs
of the terms which are replaced by a phrase. Consider a case where two phrases 〈 xy 〉
and 〈ab 〉 have df(〈 xy 〉,W) = df(〈ab 〉,W) but
df(〈a 〉,D) + df(〈b 〉,D) > df(〈 x 〉,D) + df(〈y 〉,D).
CBS is agnostic of the fact that 〈ab 〉 is more beneficial to select. Every instance of
coverage-based selection can be mapped to an instance of the budgeted-maximum cov-
erage (BMC) problem [KMN99]. BMC takes as input a collection of sets S = { s1, . . . , sm }
with associated costs { c1, . . . , cm } defined over a domain of items { x1, . . . , xn } that have
associated weights {w1, . . . , wn }. The goal is to find a collection of sets S ′ ⊆ S that max-
imizes the total weight of items covered and whose total cost does not exceed a given
budget L. The transformation is straightforward: (i) candidate phrases are sets si with
costs df(si,D), (ii) items are distinct positions in phrase queries from the workload each
with unit weight, (iii) the budget is set as L = (1+ α) · s(V).
The greedy algorithm that selects items in descending order of their benefit-cost ratio
gives a (1 − 1e)-approximation guarantee for BMC. The cost of a phrase s is df(s,D); its
benefit is defined as the number of yet-uncovered distinct positions that it covers
coverage(W,L ∪ { s }) − coverage(W,L) .
Since the objective function captures coverage of items in a set, it is submodular which
implies that the benefits of candidates are non-increasing. This offers opportunities for
optimization in practice which we discuss in the next section.
5.5.3. Optimizations for Practical Indexing
Both the phrase selection methods outlined before use a greedy algorithm to select se-
quences which are to be indexed. The greedy algorithm, in every iteration, chooses the
most promising candidate and adds it to the lexicon. Candidates are maintained in a
priority queue and the best candidate is chosen based on the benefit-cost ratio. After
a choice has been made, the benefit values of all the remaining candidate sequences
are updated. The bottleneck of the greedy algorithm is usually the update of candidate
benefits after a phrase has been added to the lexicon. We discuss optimizations for both
selection algorithms to make them feasible in practice.
124
5.5. Phrase Selection
Maintaining Dependent Queries
For QOBS, the benefit of each candidate s, given the current state of the lexicon L, is
given by c(W,L) − c(W,L ∪ { s }). Implementing this in a naı̈ve manner involves re-
computing query plans for all queries in the workload with any addition of s to the
lexicon (L ∪ { s }). This is clearly an expensive operation. However, a candidate is
not present in all queries. Only a subset of queries W ′ ⊆ W where s is present in,
{q ∈ W ′ | pos(s,q) 6= φ }, are affected. Based on this we pre-compute and store such de-
pendencies between candidates and queries. This has a couple of benefits. Firstly, as
discussed above, they avoid unnecessary recomputation of query plans. Secondly, once
the query plans forW ′ have to be recomputed, we use the dependencies to update the
benefits of only those candidates affected byW ′, i.e,{
s ∈ L \ V+ | pos(s,q) 6= φ, q ∈ W ′
}
.
This optimization is also applicable for CBS where instead of computing query plans
we compute coverage coverage(W,L) at every update step. As an additional optimiza-
tion one can dynamically prune out queries which have been completely covered.
Lazy Updates
We discussed in the previous section that CBS admits sub-modularity due to which
the benefits of candidates are non-increasing over the increasing execution states of the
algorithm. We say that a pair of candidates s and s ′ are independent if they do not have
a query in common, or,{
q | pos(s, q) 6= ∅∧ pos(s ′, q) 6= ∅
}
= ∅.
Hence the choice of s does not affect the benefit value of s ′ for the subsequent itera-
tion. Utilizing this observation we can defer updating the benefit values of candidates
until it is deemed required. Specifically, we can avoid the update step if the next best
candidate is independent to all the previously selected candidates C ⊆ L from the last
update step. Once this condition is violated all the dependent queries of candidates in C
are updated. This allows us to lazily update them, resulting in runtime improvements
by almost 50%.
Candidate Pruning
Finally for CBS, when the query plans are indeed updated, the partial contributions for
all candidate phrases associated with each query have to be determined. This is imple-
mented by maintaining associatively the query to candidate phrase mappingM(q).
125
Chapter 5. Phrase Indexing and Querying
Whenever a query q is updated with the new lexicon L, the modified benefit of each
candidate s dependent on q needs to be computed. However, if sequences from L al-
ready cover the regions covered by s, choosing s would not provide no further benefit
to q. This allows to prune away candidate phrases fromM(q) when
poscov(s, q) ⊆ {poscov(s ′, q)|s ′ ∈ L}.
This allows to update fewer number of candidates in the subsequent stages of the algo-
rithm potentially leading to further improvements in efficiency.
5.6. System Architecture
Document Collection
Phrase-Query Processor
Query Interface
Term Index
 P
hr
as
e 
In
de
xi
ng
 S
ys
te
m
resultsquery
Phrase Index
termphrase
phrase 
selection
determine
candidate
terms
determine
candidate
terms
index access 
with
optimized 
plan
Query
Workload
Friday, June 28, 13
Figure 5.1.: System architecture
Figure 5.1 shows a high-level overview of the architecture of our phrase-indexing
system. It consists of two indexes – the term index and the multi-word index.
• Term Index It is the standard word-level inverted index consisting of the lexi-
con and the inverted files built over the document collection. It indexes all single
126
5.7. Experimental Evaluation
words with their positional information.
• Phrase Index It is constructed over the document collection for a set of phrases
selected employing the phrase-selection algorithms detailed in Section 5.5.
In practice, we build the term index first and its associated lexicon. The statistics re-
quired for the phrase-selection algorithm are computed from the lexicon and the query
workload. The selection algorithms are then executed and a selection set of word-sequences
are determined which have to be indexed in the phrase index. Typically, the selected set
is small in size and fits in memory. Hence, indexing infrastructure for indexing words
can be reused employing the selection set of word sequences to filter out sequences
which do not need to be indexed.
While processing queries, both the lexicons, for term and the phrase index, are con-
sulted to determine the candidate words or word sequences presented in the query.
Query optimization is performed over the candidate terms to determine the best plan.
Finally, the respective indexes are accessed, for the terms in the optimized plan, for
fetching and intersecting the posting lists to compute results.
5.7. Experimental Evaluation
In this section, we describe our experimental evaluation. We begin with details about
our experimental setup including employed datasets, before describing our comparison
of the query-optimization methods from Section 5.4, followed by an evaluation of our
phrase-selection methods from Section 5.5 against state-of-the-art competitors.
5.7.1. Setup
All indexes were built on a local Hadoop cluster consisting of ten Dell R410 server-
class computers, each equipped with 64 GB of main memory, two Intel Xeon X5650
6-core CPUs, and four internal 2 TB SAS 7,200 rpm hard disks configured as a bunch-
of-disks. The machines are connected by 10 Gbit Ethernet, run Cloudera CDH3u0 as
a distribution of Hadoop 0.20.2, and use Oracle Java 1.6.0 26. Query optimization and
phrase-selection experiments were performed on a Dell PowerEdge M610 server with
2 Intel Xeon E5530 CPUs, 48 GB of main memory, a large iSCSI-attached disk array,
Debian GNU/Linux (SMP Kernel 2.6.29.3.1) and running Oracle Java 1.6.0 34. Wall-
clock time measurements were performed with the Java Hotspot 64-Bit Server VM using
the CMS garbage collector.
127
Chapter 5. Phrase Indexing and Querying
5.7.2. Datasets Used
We use two real-world document collections for our experiments:
• ClueWeb09-B [CWC13] (CW) – ClueWeb09-B is a subset of the ClueWeb09 corpus
consisting of more than 50 million web documents in English language crawled in
2009;
• The New York Times Annotated Corpus [NYT13] (NYT) – The New York Times An-
notated Corpus, as introduced in the previous chapter, contains more than 1.8
million newspaper articles published by The New York Times between 1987 and
2007.
Both document collections were processed using Stanford CoreNLP [COR13] for tok-
enization. To make CW more handleable, we use boilerplate detection as described
in [KFN10] and available in the DefaultExtractor of boilerpipe [BOI] .
5.7.3. Query Workload
As a workload we use entity labels from the YAGO2 knowledge base [HSBW13]. In its
rdfs:label (formerly means) relation, YAGO2 collects strings that may refer to a spe-
cific entity, which are mined from anchor texts in Wikipedia. For the entity Bob Dylan,
as a concrete example, it includes among others the entity labels “bob dylan”, “bob allen
zimmerman”, and “robert allen zimmerman”. In total, the workload that we obtain con-
tains 13.4 million entity labels having an average length of 2.41 words. Interestingly,
almost 99% of them do not contain any repeated word; we observe at most eight re-
peated words for the phrase queries in our workload. We only consider those entity
labels for which all constituent words occur in the document collection at hand, leaving
us with 10.7million and 8.0million phrase queries for CW and NYT, respectively.
For our experiments on the query-optimizers effectiveness, we additionally consider
a subset of our workload which refer to artist names, albums and song titles. Some
examples of phrases in this workload are “american national anthem”, “and the green
grass grew all around” etc. The workload that we obtain has 107, 245 entity labels having
an average length of 3.4words.
5.7.4. Index Management and Competitors
We implemented our indexing framework using Hadoop. The lexicon, containing for
each term its term identifier, document frequency, and collection frequency, is stored in a
flat file and loaded into main memory at runtime. Posting lists are kept in an indexed file
(implemented using Hadoop’s MapFile) and are stored using variable-byte encoding
128
5.7. Experimental Evaluation
in combination with d-gaps for document identifiers of consecutive postings and offsets
within each posting. We use TAAT to process phrase queries.
We compare against the following methods in our experiments – some from the liter-
ature and others conceivable baselines:
• Uni-Gram Index (UNI) indexes unigrams and does not select any phrases;
• Oracle Index (ORA) indexes unigrams and selects all phrase queries from the
workload as phrases;
• Next-Word Index [WZB04] (NEXT) indexes unigrams and selects all bigrams from
the workload that contain a stopword as phrases;
• Combined Index [WZB04] (COMB) combines NEXT and ORA. In the original
paper, the authors considered, instead of ORA, a phrase index that contains pre-
computed results of popular phrase queries. ORA thus selects a superset of the
phrases that the original approach considered. We further strengthen this com-
petitor, in comparison to its original description, by using phrases from ORA also
to process other longer phrase queries. Thus, if “united states” has been selected
by ORA, our query optimizer may use it to processing the phrase query “president
of the united states”.
• Bi-Gram Index (BI) indexes unigrams and selects all bigrams from the workload
as phrases;
• Tri-Gram Index (TRI) indexes unigrams and selects all bigrams and trigrams from
the workload as phrases;
• Out-of-Box Index [TS09] (OOBI) indexes unigrams and selects all bigrams whose
cost is above a user-specified threshold. Unlike the other competitors, OOBI is
thus also tunable. To make it comparable to our phrase-selection methods, we
adapt it, so that it ranks bigrams in descending order of their document frequency
and selects phrases from the obtained list until the user-specified space budget has
been exhausted.
We compare these approaches against our query-optimizer-based selection (QOBS)
and coverage-based selection (CBS). We use document frequency as a cost measure for
all our experiments. As mentioned earlier, one could use collection frequency instead.
In practice, though, the two measures are highly correlated and we did not observe
big differences. Also, as a one-time pre-processing performed using Hadoop and made
available to all methods, we computed document frequencies in the workload and the
document collection for all n-grams from the entire workload.
129
Chapter 5. Phrase Indexing and Querying
GRD APX
l = 2 l = 4 l = 6 l = 2 l = 4 l = 6
%
NYT [0] 7,586,656 7,839,328 7,877,367 7,662,566 7,868,885 7,900,573
(0 − 20) 403,399 200,017 174,744 382,428 192,417 166,658
[20 − 40) 70,852 31,399 22,281 35,157 18, 750 12,712
[40 − 60) 19,256 9,368 5,706 12 111 220
[60 − 80) - 51 65 - - -
%
CW [0] 10,080,400 10,488,737 10,589,685 10,176,979 10,523,862 10,607,792
(0 − 20) 547,289 204,716 135,579 525,781 27,894 132,417
[20 − 40) 98,391 45,313 21,526 49,995 27,894 12,496
[40 − 60) 26,700 13,740 5,764 25 136 75
[60 − 80) - 274 226 - - -
Table 5.2.: Percentage improvement in query-processing cost by OPT over GRD and
APX
GRD APX
l = 2 l = 4 l = 6 l = 2 l = 4 l = 6
%
NYT [0] 77,130 82,058 83,708 79,459 82,703 84,031
(0 − 20) 7,705 3,686 2,747 6,656 3,654 2,667
[20 − 40) 1,735 950 379 839 594 254
[40 − 60) 387 261 117 3 6 5
[60 − 80) - 2 6 - - -
%
CW [0] 84,108 91,039 94,839 86,281 92,034 95,218
(0 − 20) 9,888 4,129 1,849 9,772 4,259 1,832
[20 − 40) 2,710 1,753 505 1,342 1,105 347
[40 − 60) 704 480 197 15 12 13
[60 − 80) - 9 20 - - -
Table 5.3.: Percentage improvement in query-processing cost by OPT over GRD and
APX – on song titles
130
5.7. Experimental Evaluation
5.7.5. Performance of Query Optimization
Our first experiment examines the effect that the choice of query-optimization method
can have on query-processing performance. We consider three query-optimization meth-
ods for this experiment: the greedy algorithm (GRD) from [WZB04], our greedy algo-
rithm (APX) that gives an approximation guarantee, and our exponential exact algo-
rithm (OPT). GRD considers terms in increasing order of their document frequency,
thus based on their selectivity, and chooses a term if it covers any yet-uncovered por-
tion of the phrase query. Originally designed to deal with bigrams only, we extend GRD
to break ties based on term length, and thus favor the longer term, if two terms have the
same document frequency.
To compare the three query-optimization methods, we built augmented inverted in-
dexes whose dictionaries include all phrases up to a specific maximum length l ∈
{ 2, 4, 6 }. Thus, for l = 4, all phrases of length four or less are indexed. This allows us to
study the behavior of the methods as more terms to choose from become available.
First, we examine the different methods in terms of their runtime in practice. We ob-
serve an average runtime of 0.01ms for each of them, showing that there is no difference
in practice. The maximum runtime observed for OPT for any of the phrase queries from
our workload is 2.00 ms, indicating that its exponential nature rarely affects its run-
time in practice. Thus, for all further experiments we use OPT as a query-optimization
method.
Second, we compare the different methods in term of the costs of their generated
query plans. To this end, we determine for each phrase query from the workload the
percentage improvement over GRD and APX, respectively that one can achieve by us-
ing OPT. Let COPT , CGRD, and CAPX denote the cost of the query plan (in terms of to-
tal number of postings read) determined by the respective query-optimization method.
The percentage improvement is given by the values (CGRD−COPT )/(CGRD) and (CAPX−
COPT )/(CAPX) that range in [0, 1). Table 5.2 gives bucketed percentage improvements for
our two datasets and three augmented inverted indexes. Each cell reports the number
of phrase queries from the workload for which a percentage improvement in the given
range was observed.
From Table 5.2, we observe that, for the majority of phrase queries, there is no substan-
tial improvement (i.e., less than 20%) when using OPT instead one of the non-optimal
methods. Further, we see that the number of phrase queries for which an improve-
ment is achieved is generally lower for APX than for GRD, which is expected given
the former’s approximation guarantee. As can also be seen from the table, there is a
non-negligible number of phrase queries for which OPT improves by 40% or more over
GRD. For the query “we are the champions” with l = 2, as a concrete example, GRD
131
Chapter 5. Phrase Indexing and Querying
picks {we are, are the, the champions } as a query plan, which is more than twice as
expensive as the query plan {we are, the champions } determined by OPT. When com-
paring percentage improvements across different augmented inverted indexes, we see
less improvement for larger values of l, which makes sense since those include longer,
more selective phrases favored by the non-optimal methods.
To examine the effect of the query optimizer of longer query lengths, we now look at
the results on experiments on song titles summarized in Table 5.3. We observe that, like
in the previous table, for majority of the queries, OPT shows no improvement over GRD
and APX. However, OPT shows non-zero improvement for around 11% of the queries in
Table 5.3, as compared to the entire workload where the improvements are for less than
6% of the queries. This means that OPT improves over the other optimizers for longer
query lengths. Consistent with the previous observation, we see that APX performs bet-
ter than GRD in majority of the scenarios. It is also interesting to note that, although the
improvements for larger values of l is lesser, the magnitude of improvement is larger as
compared to l = 2, 4. As an example, for l = 6 in CW, we see an 60%-80% improvements
over GRD when using OPT in some queries.
In summary, neither GRD nor APX falls far behind OPT in terms of the cost of its
generated query plans. APX is robust, comes with an approximation guarantee, and
is easy to implement. However, as stated above, we did not see any phrase query for
which OPT was too expensive to run, making it a viable choice in practice.
5.7.6. Effect of Phrase Selection
Our second experiment compares our phrase-selection methods against their tunable
and non-tunable competitors in terms of query-processing performance.
We use five-fold cross validation throughout this experiment. Our workload is split
into five folds, yielding five training-test configurations. Phrase selection is then per-
formed using the four training folds; query-performance measurements are performed
using the test fold. We report averages over the five training-test configurations.
For all methods, we assume that single words are present in the lexicon – UNI thus
serves as a baseline that all methods under comparison build upon. On CW the standard
positional inverted index obtained by UNI amounts to 71 GB and contains a total of
90.17 billion postings; on NYT the corresponding index amounts to 3 GB and contains a
total of 4.91 billion postings. Index sizes, in the following, are indicated in terms of their
percentage overhead over UNI – an index size of 10% thus means that the corresponding
is 1.1× larger than the baseline index.
Query-processing performance is measured both in abstract and concrete terms. As
an abstract cost measure, we use the average number of postings that is read to process a
132
5.7. Experimental Evaluation
 0
 50000
 100000
 150000
 200000
 250000
 300000
 350000
 400000
 450000
 500000
0%
UNI
6%
ORA
59%
NEXT
65%
COMB
73%
BI
80%
TRI
P
e
rf
o
rm
a
n
c
e
 (
p
o
s
ti
n
g
s
 r
e
a
d
/q
u
e
ry
)
Non-tunable-Competitor
OOBI
QOBS
CBS
(a) NYT
 0
 1e+06
 2e+06
 3e+06
 4e+06
 5e+06
 6e+06
 7e+06
0%
UNI
5%
ORA
55%
NEXT
60%
COMB
67%
BI
80%
TRI
P
e
rf
o
rm
a
n
c
e
 (
p
o
s
ti
n
g
s
 r
e
a
d
/q
u
e
ry
)
Non-tunable-Competitor
OOBI
QOBS
CBS
(b) CW
Figure 5.2.: Performance of tunable phrase-selection methods relative to non-tunable
competitors in terms of abstract cost measures
phrase query from the test folds. We use wall-clock times (in milliseconds) as a concrete
cost measure. These were obtained based on a sample of 25, 000 phrase queries (5, 000
per test fold), using a single core, and pre-fetching all required posting lists into main
133
Chapter 5. Phrase Indexing and Querying
 0
 50
 100
 150
 200
 250
 300
 350
0%
UNI
6%
ORA
59%
NEXT
65%
COMB
73%
BI
80%
TRI
P
e
rf
o
rm
a
n
c
e
 (
M
ill
is
e
c
o
n
d
s
)
Non-tunable-Competitor
OOBI
QOBS
CBS
(a) NYT
 0
 1000
 2000
 3000
 4000
 5000
 6000
 7000
 8000
 9000
0%
UNI
5%
ORA
55%
NEXT
60%
COMB
67%
BI
80%
TRI
P
e
rf
o
rm
a
n
c
e
 (
M
ill
is
e
c
o
n
d
s
)
Non-tunable-Competitor
OOBI
QOBS
CBS
(b) CW
Figure 5.3.: Performance of tunable phrase-selection methods relative to non-tunable
competitors in terms of wall-clock times
memory.
Figure 5.3 compares the tunable phrase-selection methods (QOBS, CBS, OOBI) against
their non-tunable competitors. As a first step, we built an index for each of the non-
134
5.7. Experimental Evaluation
 0
 50000
 100000
 150000
 200000
 250000
 300000
 350000
 400000
 450000
 500000
0% 5% 10% 20% 30% 40% 50% 60% 70% 80%
P
e
rf
o
rm
a
n
c
e
 (
p
o
s
ti
n
g
s
 r
e
a
d
/q
u
e
ry
)
Space Budget (α)
UNI
OOBI
QOBS
CBS
(a) NYT
 0
 1e+06
 2e+06
 3e+06
 4e+06
 5e+06
 6e+06
 7e+06
0% 5% 10% 20% 30% 40% 50% 60% 70% 80%
P
e
rf
o
rm
a
n
c
e
 (
p
o
s
ti
n
g
s
 r
e
a
d
/q
u
e
ry
)
Space Budget (α)
UNI
OOBI
QOBS
CBS
(b) CW
Figure 5.4.: Performance of tunable phrase-selection methods
tunable competitors. The sizes of these indexes determine the values of α, which we
feed into the tunable phrase-selection methods to obtain indexes of correspond sizes, in
a second step. For the sake of comparison, we include UNI in Figure 5.3, corresponding
to α = 0, that is, no phrases are selected.
135
Chapter 5. Phrase Indexing and Querying
We observe that ORA results in the smallest index, since it materializes only phrases
that occur in exactly that form as phrase queries in the workload. However, doing so,
it overfits to the training folds, resulting in query-processing performance that is only
slightly better than our baseline UNI. When given the same amount of additional space
(6% for NYT and 5% for CW), our tunable phrase-selection methods QOBS and CBS
achieve considerably better query-processing performance – an improvement by at least
a factor 3× over ORA and the baseline UNI in terms of both abstract and concrete mea-
sures. NEXT and COMB, materializing all bigrams from the training folds that contain
a stopword, result in indexes that consume 55% − 60% additional space. While they
achieve good improvements in query-processing performance over the baseline, they
are consistently outperformed by QOBS and CBS. BI and TRI, which result in indexes
that require 67% − 80% of additional space, improve query-processing performance by
at least a factor 7× over the baseline. From the tunable phrase-selection methods OOBI
and CBS perform at par, when given this much additional space. While this also holds
for QOBS on CW, it performs slightly worse than its competition on NYT. Moreover,
we see that the trends observed in abstract and concrete measures of query-processing
performance are consistent. In the rest of this section, we thus only consider abstract
cost measures.
Figure 5.4 compares our tunable phrase-selection methods QOBS and CBS against
OOBI as the only tunable competitor. For all three methods, we constructed indexes
considering values of α ranging from 5% to 80%. UNI is again included at α = 0%
for the sake of comparison. With as little as 5% additional space, QOBS and CBS im-
prove query-processing performance by at least a factor 3×. When comparing our two
methods, we observe that CBS performs slightly better than QOBS on NYT, whereas
their performance is comparable on CW. QOBS and CBS perform consistently better
than their sole competitor OOBI. When given 5% additional space, as a concrete figure,
OOBI improves over UNI by at most a factor 1.2×. What is also apparent from the fig-
ure are the diminishing returns of additional space, which are more pronounced for our
methods that already make highly effective use of the initial 5% of additional space. Fi-
nally, when given ample additional space, all tunable phrase-selection methods perform
at par.
5.8. Related Work
We now discuss the connection between our work and existing prior work, which we
categorize as follows:
Phrase Queries. Williams et al. [WZB04] put forward the combined index to support
136
5.8. Related Work
phrase queries efficiently. It assembles three levels of indexing: (i) a first-word index as
a positional inverted index, (ii) a next-word index that indexes all bigrams containing a
stopword, and (iii) a phrase index with popular phrases from a query log. Its in-memory
lexicon is kept compact by exploiting common first words between bigrams. Query
processing escalates through these indexes – first it consults the phrase index and, if
the phrase query is not found therein, processes it using bigrams and unigrams from
the other indexes. Transier and Sanders [TS09] select bigrams to index based only on
characteristics of the document collection. Selecting bigrams makes sense in settings
where phrase queries are issued by human users and tend to be short – as observed
for web search by Spink et al. [SWJS01]. We also target application-generated queries
(e.g., quotations and titles of movies or songs) and thus select variable-length phrases.
Those have previously been considered by Chang and Poon [CP08] in their common
phrase index, which builds on [WZB04], but indexes variable-length phrases common in
the workload. Our methods, in contrast, consider both the document collection and the
workload.
Proximity scoring, such as the model by Büttcher et al. [BCL06b], is similar in spirit to
phrase queries but targets ranked retrieval. Proximity of query words is an important
signal in modern web search engines. Several authors have looked into making the
computation of proximity scores more efficient. Yan et al. [YSZ+10] propose a word-pair
index and develop query-processing methods that support early termination. Broschart
and Schenkel [BS12] describe a tunable word-pair index that relies on index pruning
to keep its size manageable. Fontoura et al. [FGJV11] describe an alternative method
of indexing word pairs, which maintains them as bitmaps along with posting lists for
single words.
Caching is often used to speed up query processing and reduce the overall system
load. It can be applied at different granularities including query results, posting lists of
single words, and posting-list intersections. The first two are considered by Saraiva et
al. [SSdMZ+01] as well as Baeza Yates et al. [BYGJ+08]. Long and Suel [LS06] propose
a three-level cache that also includes posting-list intersections. Going beyond that, Oz-
can et al. [OAC+12] describe a five-level cache that additionally includes result snippets
and documents. Policies for admitting/evicting items to/from the cache have been de-
scribed, among others, by Baeza-Yates et al. [BYJPW07] as well as Gan and Suel [GS09].
Fagni et al. [FPSO06] distinguish between a static and a dynamic cache, where the for-
mer is periodically bootstrapped from query logs, and the latter is managed using a
replacement policy such as LRU. While none of the aforementioned works has specif-
ically addressed phrase queries, it is conceivable to add a layer that caches phrases as
intersections of multiple posting lists. Our phrase-selection methods could then be used
to bootstrap a (static) cache.
137
Chapter 5. Phrase Indexing and Querying
5.9. Summary
In this chapter we developed efficient solution to processing phrase queries. We
studied how arbitrary phrase queries can be efficiently processed over an augmented-
inverted index of word sequences. We developed methods to select multi-word se-
quences to be indexed to optimize query-processing cost while keeping the index size
within a user-specified budget. We also proposed novel query-optimization techniques
to efficiently process phrase queries over such an augmented index.
With regard to phrase-query optimization, a first insight from our experiments is
that the non-optimal methods perform close to the optimum for a majority of phrase
queries. As a second insight, we observed that our tunable phrase-selection methods
make highly effective use of additional space, in particular when there is only little of it,
and considerably improve the processing performance of phrase queries.
138
6
Conclusions
6.1. Summary of Results
Supporting workloads which combine text (keywords and phrases) and time are useful
in many interesting search, mining, and exploration tasks over web archives. In this
work, we have addressed three problems in indexing text to support such workloads in
web archives.
We presented a novel index-organization scheme called index sharding to process time-
travel text queries that partitions each posting list with almost zero increase in index size.
Our approach is based on avoiding access to irrelevant postings by exploiting the ge-
ometry of the valid-time intervals associated with the document versions. We proposed
an optimal algorithm to completely avoid access to irrelevant postings. We further fine
tuned the index, taking into account the index-access costs, by allowing for a few wasted
sequential accesses while gaining significantly by reducing the number of random ac-
cesses. Finally, we proposed an incremental index sharding approach that supports
efficient index maintenance for dynamic updates to the index without compromising
the query performance. We empirically established the effectiveness of our sharding
scheme with experiments over the revision history of the English Wikipedia, and an
archive of U.K. governmental web sites. Our results demonstrate the feasibility of faster
time-travel query processing with no space overhead. Moreover, we showed that main-
taining our index structure incrementally has large benefits over indexes which are re-
computed periodically.
Next, we looked at the problem of query optimization in time-travel text search. We
presented approaches for efficient approximate processing of time-travel queries over a
vertically-partitioned inverted index. By using a small synopsis for each partition we
identified partitions that maximize the result size, and schedule them for processing
early on. Our approach aims to balance the estimated gains in the result recall against
139
Chapter 6. Conclusions
required index-access cost. Our experiments with three diverse, large-scale text archives
– the English Wikipedia revision history, the New York Times collection and, the UK-
GOV dataset – show that our proposed approach can provide close to 80% result recall
even when only about half the index is allowed to be read.
Finally, we proposed indexing and query-optimization approaches to efficiently an-
swer phrase queries. We considered an augmented inverted index that indexes selected
variable-length multi-word sequences in addition to single words. We studied how arbi-
trary phrase queries can be processed efficiently on such an augmented inverted index.
Moreover, we developed methods to select multi-word sequences to be indexed so as to
optimize query-processing cost while keeping index size within a user-specified space
budget, taking into account characteristics of both the workload and the document col-
lection. We demonstrated experimentally the efficiency and effectiveness of our meth-
ods on two real-world corpora, i.e, the New York Times collection and the ClueWeb09
dataset.
6.2. Outlook on Future Directions
The problems addressed in this work are some of the many efficiency issues which arise
in the context of text search and mining for web archives. Hence, there are many oppor-
tunities for future research.
Exploiting Redundancy for Better Retrieval Web archives are characterized by a
lot of redundant content. Content is continuously added to such collections, but the
addition of new content does not necessarily contribute novel content. Much of the
content is either copied, enriched or recompiled from existing documents. Redundancy
in text collections, apart from wasting storage space, degrades search results. Initial
attempts have been made to remove redundancy in web archives by giving user the
flexibility to define her notion of redundancy [PAB13]. However, there are challenges in
identifying and removing redundancy from search results. Many versions of the same
document are likely to match the query because (i) either they are near duplicates or
(ii) they share the context of the query terms. Designing retrieval models and indexing
methods to counter the effect of such redundancy in search results, specifically for web
archives, is an interesting direction for future research.
Phrase Indexing for Batched Query Processing In our work, we indexed com-
monly occurring word sequences to improve query processing efficiency of a given
phrase query. However, there are scenarios when multiple phrase queries are issued
in a batch. Consider a retrieval task of finding all documents which contain mentions
140
6.2. Outlook on Future Directions
of the entity Barack Obama. Assume that we already know the different textual repre-
sentations or labels used to denote this entity, i.e., “president of U.S.A”, “president of the
united states”, “leader of the united states” and so forth. Treating each label as a phrase,
each entity is associated with a batch of phrases as above. The research challenge is to
come up with novel query processing methods, based on our current work on phrase
indexing, to efficiently process such batches of phrase queries.
Mining and Exploration of Web Archives Exploratory tasks over web archives re-
quire multiple rounds of searching and aggregation over both the text and time axes. A
promising research direction would be to identify how users interact with such collec-
tions and what features are they interested in. As an example, consider a user interested
in all entities present in the documents which are results of the time-travel query “google
io” @ [03/2013 - 06/2013]. The challenges are in improving retrieval effectiveness
and evaluation of such systems.
141
Chapter 6. Conclusions
142
Bibliography
[ABBS10] Avishek Anand, Srikanta Bedathur, Klaus Berberich, and Ralf Schenkel.
Efficient temporal keyword search over versioned text. In Proceedings of
the 19th ACM Conference on Information and Knowledge Management, pages
699–708, 2010.
[ABBS11] Avishek Anand, Srikanta Bedathur, Klaus Berberich, and Ralf Schenkel.
Temporal index sharding for space-time efficiency in archive search. In
Proceedings of the 34th International ACM SIGIR Conference on Research and
Development in Information Retrieval, pages 545–554, 2011.
[ABBS12] Avishek Anand, Srikanta Bedathur, Klaus Berberich, and Ralf Schenkel.
Index maintenance for time-travel text search. In Proceedings of the 35th
International ACM SIGIR Conference on Research and Development in Infor-
mation Retrieval, pages 235–244, 2012.
[ACCG08] Sanjay Agrawal, Kaushik Chakrabarti, Surajit Chaudhuri, and Venkatesh
Ganti. Scalable ad-hoc entity extraction from text collections. Proceedings
of the International Conference on Very Large Data Bases, pages 945–957, 2008.
[ADGK07] Benjamin Arai, Gautam Das, Dimitrios Gunopulos, and Nick Koudas.
Anytime measures for top-k algorithms. In Proceedings of the International
Conference on Very Large Data Bases, pages 914–925, 2007.
[AF92] Peter Anick and Rex Flynn. Versioning a full-text information retrieval
system. In Proceedings of the 15th International ACM SIGIR Conference on
Research and Development in Information Retrieval, pages 98–111, 1992.
[AGBY07] Omar Alonso, Michael Gertz, and Ricardo Baeza-Yates. On the value of
143
Bibliography
temporal information in information retrieval. SIGIR Forum, pages 35–41,
2007.
[AGP99] Swarup Acharya, Phillip B. Gibbons, and Viswanath Poosala. Aqua: A
Fast Decision Support Systems Using Approximate Query Answers. In
Proceedings of the International Conference on Very Large Data Bases, pages
754–757, 1999.
[AGPR99] Swarup Acharya, Phillip B. Gibbons, Viswanath Poosala, and Sridhar Ra-
maswamy. Join synopses for approximate query answering. In Proceedings
of ACM SIGMOD International Conference on Management of Data, pages
275–286, 1999.
[AMC07] Rodrigo Almeida, Barzan Mozafari, and Junghoo Cho. On the Evolution
of Wikipedia. In International Conference on Weblogs and Social Media, 2007.
[ARK13] http://www.netarkivet.dk, (accessed on 28-06-2013).
[ATDE09] Eytan Adar, Jaime Teevan, Susan T. Dumais, and Jonathan L. Elsas. The
web changes everything: understanding the dynamics of web content. In
Proceedings of the 2nd ACM International Conference on Web Search and Data
Mining, pages 282–291, 2009.
[BBNW07] Klaus Berberich, Srikanta Bedathur, Thomas Neumann, and Gerhard
Weikum. A Time Machine for Text Search. In Proceedings of the 30th Inter-
national ACM SIGIR Conference on Research and Development in Information
Retrieval, pages 519–526, 2007.
[BC08] Stefan Büttcher and Charles Clarke. Hybrid index maintenance for con-
tiguous inverted lists. Information Retrieval, pages 175–207, 2008.
[BCC10] Stefan Büttcher, Charles Clarke, and Gordon Cormack. Information Re-
trieval - Implementing and Evaluating Search Engines. The MIT Press, 2010.
[BCH+03] Andrei Broder, David Carmel, Michael Herscovici, Aya Soffer, and Jason
Zien. Efficient query evaluation using a two-level retrieval process. In
Proceedings of the 13th ACM Conference on Information and Knowledge Man-
agement, pages 426–434, 2003.
[BCL06a] Stefan Büttcher, Charles Clarke, and Brad Lushman. Hybrid index main-
tenance for growing text collections. In Proceedings of the 29th International
ACM SIGIR Conference on Research and Development in Information Retrieval,
pages 356–363, 2006.
144
Bibliography
[BCL06b] Stefan Büttcher, Charles Clarke, and Brad Lushman. Term proximity scor-
ing for ad-hoc retrieval on very large text collections. In Proceedings of
the 29th International ACM SIGIR Conference on Research and Development in
Information Retrieval, pages 621–622, 2006.
[BGMZ97] Andrei Broder, Steven Glassman, Mark Manasse, and Geoffrey Zweig.
Syntactic clustering of the web. Computer Networks, pages 1157–1166, 1997.
[BGO+96] Bruno Becker, Stephan Gschwind, Thomas Ohler, Bernhard Seeger, and
Peter Widmayer. An asymptotically optimal multiversion b-tree. The
VLDB JournalThe International Journal on Very Large Data Bases, pages 264–
275, 1996.
[BHR+07] Kevin Beyer, Peter J Haas, Berthold Reinwald, Yannis Sismanis, and
Rainer Gemulla. On synopses for distinct-value estimation under mul-
tiset operations. In Proceedings of ACM SIGMOD International Conference
on Management of Data, pages 199–210, 2007.
[Blo70] Burton H Bloom. Space/time trade-offs in hash coding with allowable
errors. Communications of the ACM, pages 422–426, 1970.
[BMS+06] Holger Bast, Debapriyo Majumdar, Ralf Schenkel, Martin Theobald, and
Gerhard Weikum. Io-top-k: Index-access optimized top-k query process-
ing. In Proceedings of the International Conference on Very Large Data Bases,
pages 475–486, 2006.
[BNF13] National library of france. http://www.bnf.fr, (accessed on 28-06-2013).
[BOI] Boilerpipe. http://code.google.com/p/boilerpipe/.
[BS12] Andreas Broschart and Ralf Schenkel. High-performance processing of
text queries with tunable pruned term and term pair indexes. ACM Trans-
actions for Information Systems, page 5, 2012.
[BYGJ+08] Ricardo Baeza-Yates, Aristides Gionis, Flavio P Junqueira, Vanessa Mur-
dock, Vassilis Plachouras, and Fabrizio Silvestri. Design trade-offs for
search engine caching. ACM Transactions on the Web (TWEB), page 20, 2008.
[BYJK+02] Ziv Bar-Yossef, T. S. Jayram, Ravi Kumar, D Sivakumar, and Luca Tre-
visan. Counting distinct elements in a data stream. In Randomization and
Approximation Techniques in Computer Science, pages 1–10. 2002.
145
Bibliography
[BYJPW07] Ricardo Baeza-Yate, Flavio Junqueira, Vassilis Plachouras, and Hans-
Friedrich Witschel. Admission policies for caches of search engine results.
In String Processing and Information Retrieval, pages 74–85. 2007.
[Cho10] Gobinda Chowdhury. Introduction to modern information retrieval. 2010.
[CHT11] Sarah Cohen, James Hamilton, and Fred Turner. Computational journal-
ism. Communations of ACM, pages 66–71, 2011.
[CLYY11] Sarah Cohen, Chengkai Li, Jun Yang, and Cong Yu. Computational jour-
nalism: A call to arms to database researchers. In Proceedings of the 5th Bi-
ennial Conference on Innovative Data Systems Research, pages 148–151, 2011.
[CMS10] W Bruce Croft, Donald Metzler, and Trevor Strohman. Search engines: In-
formation retrieval in practice. 2010.
[Con10] Paul Conway. Preservation in the age of google: Digitization, digital
preservation, and dilemmas. 2010.
[COR13] Stanford corenlp. http://nlp.stanford.edu/software/corenlp.shtml, (ac-
cessed on 21-04-2013).
[Coy06] Karen Coyle. Mass digitization of books. The Journal of Academic Librarian-
ship, pages 641 – 645, 2006.
[CP08] Matthew Chang and Chung Keung Poon. Efficient phrase querying with
common phrase index. Information Processing and Management, pages 756–
769, 2008.
[CTL91] Bruce Croft, Howard Turtle, and David Lewis. The use of phrases and
structured queries in information retrieval. In Proceedings of the 14th Inter-
national ACM SIGIR Conference on Research and Development in Information
Retrieval, pages 32–45, 1991.
[CWC13] ClueWeb09. http://lemurproject.org/clueweb09/, (accessed on 28-06-
2013).
[Den12] Dimitar Denev. Models and methods for web archive crawling. PhD thesis,
Saarland University, 2012.
[DLP99] Erika De Lima and Jan Pedersen. Phrase recognition and expansion for
short, precision-biased queries based on a query log. In Proceedings of the
22nd International ACM SIGIR Conference on Research and Development in
Information Retrieval, pages 145–152, 1999.
146
Bibliography
[DMSW09] Dimitar Denev, Arturas Mazeika, Marc Spaniol, and Gerhard Weikum.
Sharc: framework for quality-conscious web archiving. Proceedings of the
International Conference on Very Large Data Bases, pages 586–597, 2009.
[DS11] Shuai Ding and Torsten Suel. Faster top-k document retrieval using block-
max indexes. In Proceedings of the 34th International ACM SIGIR Conference
on Research and Development in Information Retrieval, pages 993–1002, 2011.
[EA13] European archive. http://www.europarchive.org, (accessed on 28-06-
2013).
[FGJV11] Marcus Fontoura, Maxim Gurevich, Vanja Josifovski, and Sergei Vassil-
vitskii. Efficiently encoding term co-occurrences in inverted indexes. In
Proceedings of the 20th ACM Conference on Information and Knowledge Man-
agement, pages 307–316, 2011.
[FLN01] Ronald Fagin, Amnon Lotem, and Moni Naor. Optimal aggregation al-
gorithms for middleware. In Proceedings of ACM SIGMOD International
Conference on Management of Data, pages 102–113, 2001.
[FMNW03] Dennis Fetterly, Mark Manasse, Marc Najork, and Janet Wiener. A large-
scale study of the evolution of web pages. In Proceedings of the 12th Inter-
national Conference on World Wide Web, pages 669–678, 2003.
[FNM85] Philippe Flajolet and G Nigel Martin. Probabilistic counting algorithms
for data base applications. Journal of computer and system sciences, pages
182–209, 1985.
[FPSO06] Tiziano Fagni, Raffaele Perego, Fabrizio Silvestri, and Salvatore Orlando.
Boosting the performance of web search engines: Caching and prefetching
query results by exploiting historical usage data. ACM Transactions on
Information Systems, pages 51–78, 2006.
[FV10] Paolo Ferragina and Rossano Venturini. The compressed permuterm in-
dex. ACM Transactions on Algorithms, page 10, 2010.
[GJSS05] Dengfeng Gao, Christian S. Jensen, Richard T. Snodgrass, and Michael D.
Soo. Join operations in temporal databases. The VLDB JournalThe Interna-
tional Journal on Very Large Data Bases, pages 2–29, 2005.
[GK09] Sairam Gurajada and P. Sreenivasa Kumar. On-line index maintenance
using horizontal partitioning. In Proceedings of the 18th ACM Conference on
Information and Knowledge Management, pages 435–444, 2009.
147
Bibliography
[GS09] Qingqing Gan and Torsten Suel. Improved techniques for result caching
in web search engines. In Proceedings of the 18th International Conference on
World Wide Web, pages 431–440, 2009.
[HAN13] Hanzo archives. http://www.hanzoarchives.com/, (accessed on 28-06-
2013).
[Hen06] Monika Rauch Henzinger. Finding near-duplicate web pages: a large-
scale evaluation of algorithms. In Proceedings of the 29th International ACM
SIGIR Conference on Research and Development in Information Retrieval, pages
284–291, 2006.
[HER13] Heritrix Archival Crawler. http://www.digitalhistory.uh.edu/, (accessed
on 28-06-2013).
[HLY07] Michael Herscovici, Ronny Lempel, and Sivan Yogev. Efficient indexing of
versioned document sequences. In Advances in Information Retrieval, 29th
European Conference on IR Research, pages 76–87, 2007.
[HPBS12] Matthias Hagen, Martin Potthast, Anna Beyer, and Benno Stein. Towards
optimum query segmentation: in doubt without. In Proceedings of the 21st
ACM Conference on Information and Knowledge Management, pages 1015–
1024, 2012.
[HSBW13] Johannes Hoffart, Fabian M. Suchanek, Klaus Berberich, and Gerhard
Weikum. Yago2: A spatially and temporally enhanced knowledge base
from wikipedia. Artificial Intelligence, pages 28–61, 2013.
[HYS09] Jinru He, Hao Yan, and Torsten Suel. Compact full-text indexing of ver-
sioned document collections. In Proceedings of the 18th ACM Conference on
Information and Knowledge Management, pages 415–424, 2009.
[HZS10] Jinru He, Junyuan Zeng, and Torsten Suel. Improved index compression
techniques for versioned document collections. In Proceedings of the 19th
ACM Conference on Information and Knowledge Management, pages 1239–
1248, 2010.
[IA13] Internet archive. http://archive.org, (accessed on 28-06-2013).
[IIP13] International internet preservation consortium. http://netpreserve.org,
(accessed on 28-06-2013).
[IM13] Internet memory. http://www.internetmemory.org, (accessed on 28-06-
2013).
148
Bibliography
[KFN10] Christian Kohlschütter, Peter Fankhauser, and Wolfgang Nejdl. Boiler-
plate detection using shallow text features. In Proceedings of the 3rd ACM
International Conference on Web Search and Data Mining, pages 441–450,
2010.
[KMN99] Samir Khuller, Anna Moss, and Joseph (Seffi) Naor. The budgeted maxi-
mum coverage problem. Information Processing Letters, pages 39–45, 1999.
[KN10] Nattiya Kanhabua and Kjetil Nørvåg. Quest: query expansion using
synonyms over time. In Machine Learning and Knowledge Discovery in
Databases, pages 595–598, 2010.
[KUL] Swedish royal library: Kulturarw3 – long-term preservation of electronic
documents. http://www.kb.se/kw3/ENG/.
[Lal12] Mounia Lalmas. Xml information retrieval. Understanding Information Re-
trieval Systems: Management, Types, and Standards, page 345, 2012.
[LC89] David Lewis and Bruce Croft. Term clustering of syntactic phrases. In
Proceedings of the 13th International ACM SIGIR Conference on Research and
Development in Information Retrieval, pages 385–404, 1989.
[LHZW11] Yanen Li, Bo-June Paul Hsu, ChengXiang Zhai, and Kuansan Wang. Unsu-
pervised query segmentation using clickthrough for information retrieval.
In Proceedings of the 34th International ACM SIGIR Conference on Research and
Development in Information Retrieval, pages 285–294, 2011.
[LMZ08] Nicholas Lester, Alistair Moffat, and Justin Zobel. Efficient online index
construction for text databases. ACM Transactions on Database Systems,
2008.
[LS93] David Lomet and Betty Salzberg. Exploiting a history database for
backup. In Proceedings of the International Conference on Very Large Data
Bases, pages 380–380, 1993.
[LS06] Xiaohui Long and Torsten Suel. Three-level caching for efficient query
processing in large web search engines. Proceedings of the 15th International
Conference on World Wide Web, 9(4):369–395, 2006.
[LSS88] Jong Lee, Eugene Shragowitz, and Sartaj Sahni. A hypercube algorithm
for the 0/1 knapsack problem. Journal of Parallel and Distributed Computing,
pages 438 – 456, 1988.
149
Bibliography
[LZW06] Nicholas Lester, Justin Zobel, and Hugh Williams. Efficient online index
maintenance for contiguous inverted lists. Information Processing & Man-
agement, pages 916–933, 2006.
[Mas06] Julien Masannes. Web archiving. Springer, 2006.
[MBP13] The million books project. http://archive.org/details/millionbooks, (ac-
cessed on 28-06-2013).
[MM93] Udi Manber and Eugene W. Myers. Suffix arrays: A new method for on-
line string searches. SIAM Journal on Computing, pages 935–948, 1993.
[MOPW00] Peter Muth, Patrick E. O’Neil, Achim Pick, and Gerhard Weikum. The
LHAM Log-Structured History Data Access Method. The VLDB JournalThe
International Journal on Very Large Data Bases, pages 199–221, 2000.
[MRS08] Christopher D. Manning, Prabhakar Raghavan, and Hinrich Schütze. In-
troduction to Information Retrieval. Cambridge University Press, 2008.
[MSA+10] Jean-Baptiste Michel, Yuan Kui Shen, Aviva Presser Aiden, Adrian Veres,
Matthew K Gray, Joseph P Pickett, Dale Hoiberg, Dan Clancy, Peter
Norvig, and Jon Orwant. Quantitative Analysis of Culture Using Millions
of Digitized Books. science, 2010.
[MTB+10] Michael Matthews, Pancho Tolchinsky, Roi Blanco, Jordi Atserias, Peter
Mika, and Hugo Zaragoza. HCIR Challenge 2010. In International Con-
ference on Architectural Support for Programming Languages and Operating
Systems, 2010.
[MTO12] Craig Macdonald, Nicola Tonellotto, and Iadh Ounis. Learning to predict
response times for online query scheduling. In Proceedings of the 35th Inter-
national ACM SIGIR Conference on Research and Development in Information
Retrieval, pages 621–630, 2012.
[MZ96] Alistair Moffat and Justin Zobel. Self-indexing inverted files for fast text
retrieval. ACM Transactions on Information Systems, pages 349–379, 1996.
[NCO04] Alexandros Ntoulas, Junghoo Cho, and Christopher Olston. What’s new
on the web?: the evolution of the web from a search engine perspective.
In Proceedings of the 13th International Conference on World Wide Web, pages
1–12, 2004.
[Nér90] Jean Néraud. Elementariness of a finite set of words is co-np-complete.
Informatique théorique et applications, pages 459–470, 1990.
150
Bibliography
[NUT13] Nutchwax. http://archive-access.sourceforge.net/projects/nutch/index.html,
(accessed on 28-06-2013).
[NWF78] George Nemhauser, Laurence Wolsey, and Marshall Fisher. An analysis of
approximations for maximizing submodular set functions. Mathematical
Programming, pages 265–294, 1978.
[NYT13] New york times annotated corpus. http://corpus.nytimes.com, (accessed
on 28-06-2013).
[OAC+12] Rifat Ozcan, Sengor Altingovde, Barla Cambazoglu, Flavio Junqueira, and
Oezguer Ulusoy. A five-level static cache architecture for web search en-
gines. Information Processing and Management, pages 828 – 840, 2012.
[OCGO96] Patrick O’Neil, Edward Cheng, Dieter Gawlick, and Elizabeth O’Neil. The
log-structured merge-tree (lsm-tree). Acta Informatica, pages 351–385, 1996.
[PAB13] Bibek Paudel, Avishek Anand, and Klaus Berberich. User-Defined Re-
dundancy in Web Archives. In Proceeding of the 2013 ACM Workshop on
Large-Scale Distributed Systems for Information Retrieval, 2013.
[PAN13] Pandora archive - national library of australia. http://pandora.nla.
gov.au, (accessed on 28-06-2013).
[PD10] Daniel Peng and Frank Dabek. Large-scale incremental processing using
distributed transactions and notifications. In Proceedings of the 9th USENIX
conference on Operating systems design and implementation, pages 1–15, 2010.
[Sal89] Gerard Salton. Automatic text processing: the transformation, analysis, and
retrieval of information by computer. Addison-Wesley Longman Publishing
Co., Inc., 1989.
[SBBW10] Vinay Setty, Srikanta Bedathur, Klaus Berberich, and Gerhard Weikum.
Inzeit: efficiently identifying insightful time points. Proceedings of the 34rd
International Conference on Very Large Data Bases, pages 1605–1608, 2010.
[SC07] Trevor Strohman and W Bruce Croft. Efficient document retrieval in main
memory. In Proceedings of the 30th International ACM SIGIR Conference on
Research and Development in Information Retrieval, pages 175–182, 2007.
[SOL13] Solr. http://lucene.apache.org/solr/, (accessed on 28-06-2013).
151
Bibliography
[SSdMZ+01] Paricia Correia Saraiva, Edleno Silva de Moura, Novio Ziviani, Wagner
Meira, Rodrigo Fonseca, and Berthier Riberio-Neto. Rank-preserving two-
level caching for scalable search engines. In Proceedings of the 21st Inter-
national ACM SIGIR Conference on Research and Development in Information
Retrieval, pages 51–58, 2001.
[SSU08] Susan Schreibman, Ray Siemens, and John Unsworth. A Companion to
Digital Humanities. 2008.
[ST99] Betty Salzberg and Vassillis Tsotras. Comparison of access methods for
time-evolving data. ACM Computing Surveys, pages 158–221, 1999.
[Sta11] Efstathios Stamatatos. Plagiarism detection based on structural informa-
tion. In Proceedings of the 20th ACM Conference on Information and Knowledge
Management, pages 1221–1230, 2011.
[SWJS01] Amanda Spink, Dietmar Wolfram, Major B. J. Jansen, and Tefko Saracevic.
Searching the web: The public and their queries. Journal of the American
Society for Information Science and Technology, pages 226–234, 2001.
[SWY75] Gerard Salton, Anita Wong, and Chung-Shu Yang. A vector space model
for automatic indexing. Communications of the ACM, pages 613–620, 1975.
[TGMS94] Anthony Tomasic, Hector Garcia-Molina, and Kurt Shoens. Incremental
updates of inverted lists for text document retrieval. ACM, 1994.
[TIM13] The times. http://www.thetimes.co.uk, (accessed on 28-06-2013).
[TS09] Frederik Transier and Peter Sanders. Out of the box phrase indexing. In
String Processing and Information Retrieval, pages 200–211, 2009.
[TWS04] Martin Theobald, Gerhard Weikum, and Ralf Schenkel. Top-k query eval-
uation with probabilistic guarantees. In Proceedings of the International Con-
ference on Very Large Data Bases, pages 648–659, 2004.
[Vaz01] Vijay Vazirani. Approximation algorithms. Springer, 2001.
[WAR13] Uk web archive. http://www.webarchive.org.uk, (accessed on 28-06-
2013).
[WIK13] Wikipedia. http://en.wikipedia.org/, (accessed on 28-06-2013).
[WMB99] Ian Witten, Alistair Moffat, and Timothy Bell. Managing Gigabytes: Com-
pressing and Indexing Documents and Images, Second Edition. Morgan Kauf-
mann, 1999.
152
Bibliography
[WSJ13] The Wall Street Journal. http://www.wsj.com, (accessed on 28-06-2013).
[WZB04] Hugh Williams, Justin Zobel, and Dirk Bahle. Fast phrase querying with
combined indexes. ACM Transactions on Information Systems, pages 573–
594, 2004.
[YSL+12] Jing Yuan, Guangzhong Sun, Tao Luo, Defu Lian, and Guoliang Chen.
Efficient processing of top-k queries: selective nra algorithms. Journal of
Intelligent Information Systems, pages 687–710, 2012.
[YSZ+10] Hao Yan, Shuming Shi, Fan Zhang, Torsten Suel, and Ji-Rong Wen. Ef-
ficient term proximity search with term-pair indexes. In Proceedings of
the 19th ACM Conference on Information and Knowledge Management, pages
1229–1238, 2010.
153
Bibliography
154
A
Appendix
10 commandments, hurricane season, abortion, abraham lincoln, acre, adenocarcinoma,
adolf hitler, africa, agnostic, alexander great, allegory, american idol, anal, anderson
cooper, andrea lowell, andy milonakis, anus, aorta, appendix, argentina, ash wednes-
day, ask jeeves, audie murphy, beastiality, bees, beethoven, bill gates, blood tests, brazil,
buddha, buddhism, cameltoe, candy samples, chamber horrors match, characters yu gi
oh gx, charles darwin, charlie rose, charmed, cher, chris daughtry, chris penn, chris-
tianity, chuck norris jokes, cinco de mayo, cleveland steamer, clitoris, cocaine, cold
war, columbine, communism, concentration camps, crystal meth, cuba, da vinci code,
dana reeve, danzig, darfur, david blaine, deal or no deal, deaths 2006, debra lafave,
deposition, dixie chicks, dna, dominican republic, domino harvey, donnie mcclurkin,
dopamine, doxycycline, drudge report, dubai, easter, ebay, eleanor roosevelt, elmo’s
world, emancipation proclamation, emily rose, emo, encarta, england, erection, estro-
gen, existentialism, facebook, fascism, del castro, ngerprints, av , orence nightingale,
french revolution, genocide, georg fuerst, george rr martin, george washington, ger-
many, gloria vanderbilt, goggle, good friday, gospel judas
Figure A.1.: Queries used for WIKI dataset
155
Appendix A. Appendix
guns n roses, gwen stefani, hades, haiku, hanso, hanso foundation, harlem renaissance,
hawaii, he-man toys, helga sven, henry ford, hermaphrodite, hesiod, high school musi-
cal 2, hitler, hotmail, hurricane katrina, hurricane rita, football season , hurricane wilma,
hustler, hydrocodone, imperialism, incest, industrial revolution, israel, italy, jack dun-
phy, japan, joan arc, john adams, john lennon, johnny cash, julian beever, julius caesar,
june carter, june carter cash, kama sutra, karma, kelly clarkson, kkk, knights templar,
korean war, ku klux klan, lafave, lecithin, led zeppelin, lenin, lent, liger, limewire, liver,
lost, louisiana, purchase, lymph nodes, manifest destiny, marcheline bertrand, mari-
juana, martin luther, marvel scream, maslow’s hierarchy needs, matisyahu, may day,
maya angelou, mayo clinic, mccarthyism, memorial day, metaphor, mexico, mime, mis-
sissippi river, missouri compromise, monroe doctrine, moors, morphine, mortal kombat
characters, moses, mozart, mrsa, msnbc, mussolini, naruto, neil armstrong, neuropathy,
newgrounds, n, niacin, norepinephrine, nudity, opium, opus dei, oxycodone, palm sun-
day, pamela rogers, panama canal, pancreas, pandemic, penis, penthouse, peru, pete
wentz, peter tomarken, ph, phentermine, plato, playboy, playgirl, poland, polygamy,
potassium prednisone, priory sion, prohibition, protein, pus, randy jackson, randy or-
ton, rape, renaissance, roe v. wade, roman numerals, romanticism, rome, ronald reagan,
rosa parks, satire, schizophrenia, scientific method, scientologist, scientology, segrega-
tion, serotonin, sesame street, shakira, shane macgowan, silent hill, simon cowell, skull
island, snopes, sociopath, sodomy, sonny moore, spain, spanish civil war, spanish in-
quisition, spanking, spiderman 3, spleen, sportsnet new york, stadium arcadium, stalin,
statue liberty, stephanie mcmahon, sudoku, superman, symbols, syntax, tachycardia,
ten commandments, tet offensive, american dream, beatles, cold war, da vinci code,
great depression, kennedy family, last supper, neocons, ten commandments, thomas
edison, thong, tiffany fallon, timothy treadwell, trees, truman capote, truman, doctrine,
tsunami, tuberculosis, united, vagina, vatican city, venezuela, vicodin, vietnam war,
vivian, liberto, vulva, watergate, whitney houston, wiccan, winmx, winston churchill,
world war, world war ii, wwe, x-men, xiaolin showdown, yalta conference
Figure A.2.: Queries used for WIKI dataset - cont.
156
action plan template, animals boarding, boys names, breast implants, british crown jew-
els, british government summary, british royal family, british royalty, citizenship cere-
mony dates, criminal record, diana princess wales, different types writing, disability,
edinburgh castle, food poisioning, free emergencey signs, international contributions
longbenton, kensington palace, kevin ramsay, king george, london metro, mergers ac-
quisitions business, mi5, migrant workers statistics, millenium development goals, min-
imum wage, national archives, parents rights, pensions, power ranger clip art, renewal
wedding vows, renne de chateau, residential home, residential home extensions, re-
tirement speeches, risk assessment, road surfaces, robert edwards fortune, royal family,
stuart kings, summary judgement, tax tables, uk immigration, uk passports, vehicle
recalls, william morris wallpaper, winchester museum, windsor castle, witness intimi-
dation, woodchurch, work-life balance surveys, world war two names, yin yang symbol
Figure A.3.: Queries used for UKGOV dataset
157
Appendix A. Appendix
158
List of Figures
2.1. Website preserved by Internet Archive . . . . . . . . . . . . . . . . . . . . . . 8
2.2. Inverted index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
2.3. Partitioning a posting list . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
2.4. Temporal coalescing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
3.1. Vertical partitioning vs. sharding of a posting list . . . . . . . . . . . . . . . 28
3.2. Impact list . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32
3.3. Open-skip and scan operations for query processing . . . . . . . . . . . . . 34
3.4. Subsumption of postings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35
3.5. Idealized sharding with staircase property . . . . . . . . . . . . . . . . . . 36
3.6. Cost-aware shard merging . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40
3.7. End time order of finalizing versions . . . . . . . . . . . . . . . . . . . . . . 46
3.8. Incremental sharding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47
3.9. Stalactite groups . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51
3.10. System architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52
3.11. Wall-clock times for day-granularity queries . . . . . . . . . . . . . . . . . 56
3.12. Wall-clock times for month-granularity queries . . . . . . . . . . . . . . . . 57
3.13. Wall-clock times for year-granularity queries . . . . . . . . . . . . . . . . . 58
3.14. Wall-clock times for full-lifetime queries . . . . . . . . . . . . . . . . . . . . 59
3.15. Index sizes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62
3.16. Performance of index maintenance - WIKI . . . . . . . . . . . . . . . . . . . 64
3.17. Performance of index maintenance – UKGOV . . . . . . . . . . . . . . . . . 65
4.1. Processing a time-travel query “A B” @ [tb , te] using partition selection . 71
4.2. τ for the affected partitions time-travel query “q1 q2” . . . . . . . . . . . . 84
159
List of Figures
4.3. System architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 87
4.4. Performance of size-based partition selection on Fixed-7 index . . . . . . . 93
4.5. Performance of equi-cost partition selection on Fixed-7 index . . . . . . . . 94
4.6. Performance of size-based partition selection on VERT-3.0 index . . . . . . 95
4.7. Impact of using synopses . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100
4.8. Effect of varying partition granularity on FIXED indexes . . . . . . . . . . 101
4.9. Effect of varying partitioning granularities on VERT indexes . . . . . . . . 102
5.1. System architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 126
5.2. Performance of tunable phrase-selection methods relative to non-tunable
competitors in terms of abstract cost measures . . . . . . . . . . . . . . . . 133
5.3. Performance of tunable phrase-selection methods relative to non-tunable
competitors in terms of wall-clock times . . . . . . . . . . . . . . . . . . . . 134
5.4. Performance of tunable phrase-selection methods . . . . . . . . . . . . . . 135
A.1. Queries used for WIKI dataset . . . . . . . . . . . . . . . . . . . . . . . . . . 155
A.2. Queries used for WIKI dataset - cont. . . . . . . . . . . . . . . . . . . . . . . 156
A.3. Queries used for UKGOV dataset . . . . . . . . . . . . . . . . . . . . . . . . 157
160
List of Tables
3.1. Notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31
3.2. Characteristics of datasets used . . . . . . . . . . . . . . . . . . . . . . . . . 53
3.3. Average number of shards per term . . . . . . . . . . . . . . . . . . . . . . 60
3.4. Comparison of wall-clock times between CAS and INC – in milliseconds . 61
3.5. Comparison between index sizes of CAS and INC - in gigabytes . . . . . . 62
4.1. Notation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73
4.2. Synopsis index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 90
4.3. Wall-clock times for selection over Fixed-30 . . . . . . . . . . . . . . . . . . 97
4.4. Wall-clock times for selection over VERT-(κ = 3.0) . . . . . . . . . . . . . . 98
5.1. Notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 109
5.2. Percentage improvement in query-processing cost by OPT over GRD and
APX . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 130
5.3. Percentage improvement in query-processing cost by OPT over GRD and
APX – on song titles . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 130
161
List of Tables
162
List of Algorithms
1. Idealized sharding algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . 37
2. Cost-Aware Shard Merging . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43
3. Incremental sharding algorithm . . . . . . . . . . . . . . . . . . . . . . . . . 48
4. Partition Selection - dynamic programming solution . . . . . . . . . . . . . 78
5. GREEDYSELECT for single-term partition selection . . . . . . . . . . . . . . . 80
6. GREEDYSELECT for multi-term partition selection . . . . . . . . . . . . . . . 83
7. Dynamic programming solution for optimal plan for queries with non-
repeated tokens . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 116
8. Phrase-query optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117
163
