Corpus-based Information Extraction and Opinion 
Mining for the Restaurant Recommendation System 
Ekaterina Pronoza, Elena Yagunova, and Svetlana Volskaya 
Saint-Petersburg State University 
7/9 Universitetskaya Nab., Saint-Petersburg, Russia 
{katpronoza, iagounova.elena, svetlana.volskaya}@gmail.com 
Abstract. In this paper corpus-based information extraction and opinion mining 
method is proposed. Our domain is restaurant reviews, and our information ex-
traction and opinion mining module is a part of a Russian knowledge-based rec-
ommendation system. 
Our method is based on thorough corpus analysis and automatic selection of ma-
chine learning models and feature sets. We also pay special attention to the veri-
fication of statistical significance. 
According to the results of the research, Naive Bayes models perform well at 
classifying sentiment with respect to a restaurant aspect, while Logistic Regres-
sion is good at deciding on the relevance of a user’s review. 
The approach proposed can be used in similar domains, for example, hotel re-
views, with data represented by colloquial non-structured texts (in contrast with 
the domain of technical products, books, etc.) and for other languages with rich 
morphology and free word order. 
Keywords. Information extraction, opinion mining, restaurant recommendation 
system, machine learning 
1 Introduction 
In this paper information extraction (IE) and opinion mining (OM) for the restaurant 
recommendation system are considered. Our goal is to introduce an effective corpus-
based restaurant IE and OM method for Russian using machine learning techniques. 
We try to combine thorough language analysis, adopted from corpus linguistics, with 
fully automatic machine learning techniques. 
The system is built in several steps. First, we define a set of restaurant aspects; the 
most frequently occurring ones are to be extracted using machine learning techniques. 
We conduct corpus analysis and construct dictionaries and sentiment lexicon (these 
procedures are described in our earlier paper [27]). Second, we compare several classi-
fication techniques in combination with various feature sets to determine the 
best classifier for each of the restaurant aspects defined earlier [27]. 
There are two types of restaurant aspects extracted from reviews: those which are 
evidently subjective and depend on the user’s tastes (staff amiability, food quality, etc.) 
and those which are more or less objective (crampedness, noise, etc.) – users usually 
 Corpus-based IE and OM for the Restaurant Recommendation System 259 
agree on them. Thus, our task is similar to IE or OM depending on a restaurant aspect. 
In this paper we do not distinguish these two tasks and use the term 
OM to refer to both of them when conducting our experiments. 
2 Related Work 
IE and OM system we implement is a part of the restaurant recommendation system. 
Recommendation systems are usually classified as content-based and collaborative fil-
tering ones [25] [26]. In text-based systems checking items similarity usually demands 
the application of text mining techniques. However, using linguistic methods or ideas 
is not common practice in this area: the overview of text mining methods gives an idea 
of the dominating bag-of-words and key words approaches [11] [16] [21]. Sometimes 
more advanced techniques are applied, but IE is completely integrated into items rank-
ing algorithm inside the recommendation system [17]. 
As far as IE and OM are concerned, the corresponding problems are solved using 
both rule-based and statistics-based methods. 
In the early days of IE, hand-crafted rules were used. They were later substituted 
with automatically extracted ones, and then machine learning-based approach devel-
oped [30]. Although traditionally IE is a domain dependent task, machine learning ap-
proaches to IE are both domain dependent and domain independent [12] [35]. In OM, 
rule-based approaches usually include the application of a semantic thesaurus [10], and 
machine learning ones do not always demand such linguistic resources. 
As part of our research, we analyze features and classifiers commonly used in OM 
(and/or sentiment analysis). They are listed in Tables 1 and 2. 
Table 1. Features in Sentiment Analysis 
Feature References 
Unigrams [2] [4] [7] [12] [15] [22] [29] [37] 
N-grams (bigrams, trigrams, etc.) [2] [4] [7] [22] [37] 
Unigrams of a certain POS (adjectives, adverbs, etc.) [2] [7] 
N-grams of certain POS [2] [3] 
Token positions [22] 
Emoticons [8] [19] [31] 
Substrings [7] 
Syntactic relations, syntactic n-grams [19] [23] [33] [34] 
Valence shifters [12] [15] 
Semantic classes [7] [29] 
Sentiment lexicon words [6] [15] [31] 
 
As far as unigrams are concerned, it is shown in [22] that occurrence-based unigram 
features generally perform better than frequency-based ones. Some researches consider 
higher order n-grams (non-contiguous ones) not useful [22] while others argue that they 
improve overall performance for some tasks [7] [37]. 
260 E. Pronoza, E. Yagunova, and S. Volskaya 
Sometimes position information (i.e., the position of a token in a paragraph) is also 
included in the feature set [22]. Lower order features, such as substrings, are experi-
mented with in [7]. 
When dealing with informal language (e.g., tweets or sms), some authors propose 
taking emoticons into account [8] [19] [31]. 
As for part-of-speech (POS) information, unigrams of certain POS as well as word 
combinations of certain POS [2] (e.g., adverbs and adjectives in [3]) are often used. 
Deeper linguistic-based features include dependency or constituent-based features 
[24]. Semantic classes of words are also employed. Thus, words referring to a particular 
object are replaced with their class labels (and extracted inside n-grams) [7] [29]. In 
[14] valence shifters (intensifiers, diminishers and negations) are used. As for negation, 
a common approach to its handling involves attaching “not” to the negated word [2] [6] 
[7] [19] [20]. 
Table 2. Supervised Models in Sentiment Analysis 
Model References 
Naive Bayes [2] [4] [6] [7] [20] [22] [29] [31] [37] 
Logistic Regression [31] 
Maximum Entropy [22] [29] 
Support Vector Machines [2] [4] [7] [12] [15] [22] [23] [31] 
Random Forest [31] 
Perceptron [1] [2] 
Neural Networks [32] [34] 
 
As far as classifiers are concerned, it can be seen from Table 2 that Naive Bayes 
classifier (NB) and Support Vector Machines (SVM) and the most popular ones in sen-
timent analysis. NB is commonly used as a baseline model, as SVM usually demon-
strates better performance. However, it is shown in [4] and [37] that NB can outperform 
SVM on short-form domain like microblogs. 
As stated earlier, our goal is to introduce an effective corpus-based restaurant IE and 
OM method using machine learning techniques. Since there is lack of linguistic re-
sources for Russian, we heavily rely on corpus analysis and our focus is on the appli-
cation of machine learning to the problem. Our objectives include the identification of 
feature sets and models to experiment with, and the automatic selection of the best 
model and feature set combination. Our tasks are to conduct corpus analysis and con-
struct dictionaries, to define feature sets and models for the experiments, to evaluate 
the classifiers and to propose the rules for automatic selection of the best classifier. 
As far as features are concerned, our choice is dictated by the realities of the Russian 
language. Since there are no available linguistic sentiment resources for Russian known 
to us, we employ various lexicons either learnt semi-automatically or constructed man-
ually from the corpus. Thus, we use combinations like “modifier + predicative-attribu-
tive word” as one of the features, and this idea is similar to that of “adverb + adjective” 
pairs and valence shifters. We also experiment with occurrence-based unigrams and 
contiguous bigrams, emoticons and exclamation marks.  As there is lack of parsing 
 Corpus-based IE and OM for the Restaurant Recommendation System 261 
tools for Russian, we consider non-contiguous bigrams an alternative to syntactic ones, 
taking into account free word order and the variety of sentiment expression in Russian. 
Negation is also to be covered by non-contiguous bigrams. 
As for machine learning models, in our research we experiment with NB, Logistic 
Regression (LogReg), linear SVM and Perceptron. NB appears to be the best one at 
classifying opinion in our domain for most restaurant aspects. It agrees with the results 
obtained in earlier papers [4] and [37] for English and with the notion that NB, as a 
simple generative model, is better at small amount of data. 
3 Data 
The data consists of 32525 reviews (4.2 millions of words) about restaurants in informal 
Russian language. The corpus is full of slang, misprints and prolonged vowels (as a 
means of expressing emotions). The reviews are mostly unstructured and vary from 1 
to 96 sentences. A part of the corpus, with 1025 reviews about 206 restaurants from the 
central part of Saint-Petersburg, is annotated. 
We outline a list of restaurant characteristics which are presumably mentioned in 
users’ reviews (see Table 3)1.  
Table 3. Restaurant Aspects 
Aspect Value domain Aspect Value domain Aspect Value do-
main 
Restaurant 
type 
String Noise level {-2; -1; 0; 1; 2} Dancefloor {yes; no} 
Cuisine 
type 
String(s)2 Cosiness {yes; no} Bar {yes; no} 
Food qual-
ity 
{-2; -1; 0; 1; 2} Romantic at-
mosphere 
{yes; no} Parking 
place 
{yes; no} 
Company {large; small} Crampedness {yes; no} VIP room {yes; no} 
Audience String(s) Price level {-2; -1; 0; 1; 2} Dancefloor {yes; no} 
Service 
quality 
{-2; -1; 0; 1; 2} Average 
cheque 
Integer or Inter-
val 
Railway 
station 
{yes; no} 
Service 
speed 
{-2; -1; 0; 1; 2} Smoking 
room 
{yes; no; area; 
room} 
Hotel {yes; no} 
Staff po-
liteness 
{-2; -1; 0; 1; 2} Children {yes; no} Shopping 
mall  
{yes; no} 
Staff amia-
bility 
{-2; -1; 0; 1; 2} Children’s 
room 
{yes; no}   
 
                                                          
1 Our recommendation system suggests a dialogue with a user based on a predefined list of res-
taurants aspects, and therefore we do not perform automatic topic clustering described, for 
example, in [18]. 
2 Multiple valued cuisine type and audience are split into several binary aspects. 
262 E. Pronoza, E. Yagunova, and S. Volskaya 
 Our task can be considered a classification problem. For each aspect the system 
should either label a review with one of the possible classes or reject it as irrelevant 
with respect to the given aspect. 
The aspects in italics are the most frequent ones in the annotated subcorpus. As most 
restaurants characteristics are never mentioned in the reviews, we define an empirical 
threshold frequency value of 10% and consider aspects mentioned in at least 10% of 
reviews frequent. We only train classifiers for the frequent aspects. In Table 4 the 11 
selected aspects are divided into groups according to their frequency in the reviews. 
Table 4. Restaurant Aspects Distribution in the Corpus 
Occurrence Percentage List of Aspects 
[85%; 100%] Food quality (86%) 
[55%; 85%) Service quality (55%) 
[25%; 55%) Staff politeness & amiability, service speed, price level, cosiness 
[10%; 25%] Noise level, crampedness, romantic atmosphere, company 
4 Methods 
The research described in this paper is conducted in several stages, including corpus 
analysis, features and classifiers identification and the automatic selection of the best 
classifiers and features. 
4.1 Corpus Analysis 
The first step is described in [27] in detail. It consists of corpus preprocessing (tokeni-
zation, lemmatization, spell checking and splitting into sentences) and dictionaries 
learning. The dictionaries learnt from the corpus include 
 trigger words dictionaries (for service and food frames only), 
 predicative-attributive dictionaries (for service and food frames only), 
 modifiers dictionary (for aspects taking one of the 5 values from “-2” to “2”), 
 key words and phrases dictionary and 
 sentiment lexicon (for aspects taking one of the 5 values from “-2” to “2”). 
Trigger words and predicative-attributive dictionaries are learnt semi-automatically 
from non-contiguous bigrams (gathered from the corpus) using bootstrapping proce-
dure. Here and further in the paper by trigger words we mean such nominations for 
service or food that if a trigger word occurs in a review, the review is likely to contain 
relevant information about the aspect in question. Predicative-attributive words are ad-
jectives and participles which occur in the context of trigger words. 
The modifiers are filtered from the adverbs list (collected from the corpus) and key 
words are written out and annotated manually. Sentiment lexicon consists of adjectives 
and participles and is also annotated manually. 
 Corpus-based IE and OM for the Restaurant Recommendation System 263 
4.2 Features identification 
We try to incorporate the dictionaries learnt at the corpus analysis stage into our feature 
sets. There are 11 different feature sets defined (see Table 5). 
Table 5. Feature Sets 
Feature set\Feature N-grams Non-contig-
uous N-
grams 
Emoticons 
and Excla-
mations 
Key 
Words 
Predicative-
Attributive 
Words and 
Modifiers 
Sentiment 
Lexicon 
Baseline +      
Extended Distant + +     
Extended Distant 
Emoticons + + +    
Extended Distant 
Emoticons Lex + + +   + 
Extended KWs +   +   
Extended KWs 
PredAttr Lex +   + +  
Extended PredAttr +    +  
Extended Lex +     + 
Extended KWs Lex +   +  + 
Extended Emoti-
cons KWs Lex +  + +  + 
Extended All + + + + + + 
 
All the features except for the emoticons and exclamations (taking frequency values) 
are occurrence-based and binary. Baseline features consist of unigrams and bigrams, 
and non-contiguous n-gram features are represented by non-contiguous bigrams (with 
at most two words between the components). 
There are predicative-attributive features for each word from the respective diction-
aries. They take “1” values when a word from predicative-attributive dictionary occurs 
within 3 words to the left from any of the trigger words. If a modifier occurs inside such 
left context, a corresponding feature is taken into account too. Sentiment lexicon fea-
tures also take “1” values when occurring in a trigger word context. They take form 
“LEX_label_LEFT” or “LEX_label_RIGHT” depending on their position with respect 
to the trigger word, and “label” stands for aspects class label e.g., “-2”, “-1”, etc.). 
Since the size of feature space appears to be quite large, we prune irrelevant features 
using Randomized Logistic Regression implemented in scikit-learn. 
4.3 Models 
The models we experiment with include NB (Bernoulli, with non-occurring features 
taken into account, and Multinomial, with non-occurring features ignored), LogReg, 
linear SVM and Perceptron (with shuffled samples) from scikit-learn3. 
                                                          
3 http://scikit-learn.org 
264 E. Pronoza, E. Yagunova, and S. Volskaya 
For each of the restaurants aspects there are two classifiers trained: first, to label a 
review as relevant or irrelevant with respect to the aspect, and then, if relevant, to pre-
dict its class. Given the size of our annotated corpus, it should be mentioned that while 
for the relevance/irrelevance task there are only two classes and the whole training data 
set available, while for further sentiment classification task only relevant reviews are 
considered. Therefore, when the latter task is concerned, we have a limited amount of 
data for some of the aspects, especially for the subjective ones. 
5 Evaluation. Classifiers Selection 
To choose the best combination of model and feature set for each of the restaurant as-
pects, we conduct a two-step procedure. First, 10-fold cross-validation is held (with 
models trained and tested on the same random data splits and test size equal to 10% of 
the corpus). Then statistical tests are employed to check whether the best combinations 
are significantly better than the other ones. 
As it was mentioned in section 4.3, we choose two classifiers for each of the aspects: 
the first one is to decide whether a review was relevant or not and the second one – to 
predict its class label. As far as the former is concerned, it is an intermediate task, and 
we only try the classifiers on baseline feature set according to the empirically derived 
conclusion that n-grams-based approach is sufficient to separate relevant reviews from 
the irrelevant ones. For the latter task we experiment with 11 different feature sets (or 
4, for the aspects not belonging to service and food frames). Thus, there are 5 and 55 
(or 20) different combinations respectively. 
To be able to compare the models, one has to choose some single score. During the 
cross-validation procedure we calculate average weighted F1 scores for our “classifier 
+ feature set” combinations. These F1 scores are average across all the classes with 
weights equal to their frequencies in the training data set. 
To test whether the best combinations are significantly better than the other ones, we 
follow the recommendations described in [9]. The tests are conducted in two stages. 
First, we apply a modified non-parametric Friedman test (proposed by Iman and Dav-
enport in [13]) to see whether there is any significant difference between our models 
performance scores. Then, in case the difference is significant, we proceed with a series 
of post-hoc Holm-Bonferroni tests (also described in [9]). 
Holm-Bonferroni test is quite powerful and can be used for comparing one classifier 
to the others even for dependent data sets. In our research we not only test whether the 
best classifier is significantly better than the other ones, but also divide them into groups 
according to their ranks. The ranks are calculated for each classifier as average ranks 
for each test set in the cross-validation. 
Thus, let us assume there are four types of classifiers: the best one (class 3)4, those 
which are significantly worse than the best one (class 0), those which are significantly 
                                                          
4 It should be noted that the classifier (“model + feature set” combination) with the highest rank 
does not necessarily demonstrate the highest average weighted F1 score. The classes 0, 1, 2 
and 3 assigned to the classifiers in this paper are based on their ranks (according to non-para-
metric Holm-Bonferroni test) and not F1 scores. 
 Corpus-based IE and OM for the Restaurant Recommendation System 265 
better than each classifier from class 0, except for the best one (class 2) and all the rest 
(class 1). As far as the latter (class 1) is concerned, one cannot tell whether there is any 
significant difference between their ranks compared to the best rank (class 3) or the 
worst ranks (class 0). 
Having divided our classifiers (i.e., “classifier + feature set” combinations) into the 
groups as described above (in case there is significant difference according to Friedman 
test), we choose the classifier for each of the restaurant aspects according to the follow-
ing rules: 
 if there is no statistically significant difference between the classifiers (e.g., null hy-
pothesis is not rejected in Friedman test or all groups belong to class 1), choose the 
simplest combination5 among the classifiers with scores within 2% from the maxi-
mal score (in case of ties the classifier with better scores should be chosen); 
 if there is statistical difference according to Friedman test, and the groups are class 
3 and class 0 only (or class 3 and classes 1 and 0 only), choose the only element of 
class 3; 
 if there is statistical difference according to Friedman test, and all the four groups 
(0, 1, 2 and 3) take place, choose the simplest combination among those class 3 + 
class 2 classifiers which are within 1% from the class 3 F1 score, either higher or 
lower (if none such classifiers in class 2, choose the only element of class 3; in case 
of ties choose the classifier with better scores). 
Such an approach seems reasonable because it guarantees that significantly worse 
classifiers (if any) are never chosen and provides a balance between high performance 
scores and computational effectiveness. 
Cross-validation results are shown in Table 6. It contains information only about the 
aspects for which Friedman test proves significant difference at the 0.05 level between 
classifiers performance. During the series of Holm-Bonferroni tests we also test the null 
hypotheses at the 0.05 significance level. For the aspects for which the null hypothesis 
in Friedman test is not rejected we adopt Multinomial NB classifier and baseline feature 
set by default. 
Table 6. Best Models and Feature Sets According to Holm-Bonferroni Tests Series 
Aspect Model:Feature Set 
Priority 
Class 
Accuracy, 
% 
Average 
F1, % 
Class Selection 
amiability MNB:extended_All 3 77,30 76,84 
cosy MNB:extended_Distant 3 96,00 95,74 
cramped MNB:baseline 2 87,86 87,52 
level MNB:baseline 2 65,00 61,82 
                                                          
5 Baseline features set is considered the simplest one, while Extended_All – the most complex 
one. MNB and NB models are considered the simplest models, Perceptron – a more complex 
one, and LogReg and linear SVM – the most complex ones (in fact, they are both similar to 
Perceptron but their training is more computationally expensive [5]). MNB and NB classifiers 
are considered similar in the degree of “simplicity” as well as LogReg and linear SVM. A 
simple model with complex features is considered simpler than a complex model with simple 
(e.g., baseline) features.  
266 E. Pronoza, E. Yagunova, and S. Volskaya 
noise MNB:extended_KWs 3 82,67 80,40 
politeness NB:extended_All 3 79,66 79,20 
service quality MNB:extended_Distant_Emoticons 3 72,71 71,96 
food quality MNB:extended_Distant 3 75,05 74,05 
speed MNB:extended_KWs_PredAttr_Lex 2 69,71 68,72 
Relevant vs. Irrelevant 
amiability NB:baseline 3 82,78 82,76 
company LogReg:baseline 2 93,24 92,66 
cosy LogReg:baseline 3 89,91 89,78 
cramped LogReg:baseline 3 92,22 91,73 
level LogReg:baseline 3 92,96 92,95 
noise LogReg:baseline 3 93,89 93,68 
politeness LogReg:baseline 3 87,59 87,52 
service quality LogReg:baseline 3 82,87 82,79 
romantic Prcp:baseline 2 93,98 93,91 
speed LogReg:baseline 2 88,33 88,30 
 
As far as relevance/irrelevance task is concerned, LogReg appears to be the best 
classifier. For crampedness, politeness and service quality (in bold) LogReg is signifi-
cantly better than all the rest classifiers at the 0.05 level. For most of the other aspects 
it performs better than Bernoulli and Multinomial NB and Perceptron. And indeed, 
LogReg is known to be better on large training sets, and for the relevance/irrelevance 
task there is more training data than for the task of classifying relevant reviews. 
Thus, we suggest that LogReg could be recommended for the classification of infor-
mal unstructured Russian texts into those which contain information or opinion about 
the specific aspect and those which do not. 
As for deciding on the sentiment or opinion class, NB classifiers are chosen for all 
the aspects. It can be partly explained by the nature of the classifier itself and the rules 
which direct our choice. Namely, NB, having high bias, usually behaves better when 
there is small amount of training data, and, according to the outlined rules, simple clas-
sifiers have higher priority. However, with the given rules, for 6 aspects out of 9, NB 
classifiers, combined with extended feature sets, still have the highest ranks. Therefore 
it might be suggested that NB is good at classifying sentiment in the informal texts with 
small training set. 
Holm-Bonferroni test series also reveals the following tendency: some of the “model 
+ feature set” combinations are never labeled with class 0 for any of the aspects con-
sidered. Such combinations include Naive Bayes (Bernoulli) with the following feature 
sets: extended_PredAttr, extended_KWs_PredAttr_Lex, extended_Distant, ex-
tended_Distant_Emotions and extended_Distant_Emotions_Lex. It suggests that even 
if we simply pick up one of these combinations for each of the aspects, the obtained 
scores will not be among the worst ones. 
Another observation that can be made is that including emoticons and exclamations 
into the extended_Distant feature set is not a good idea unless the aspect to be extracted 
is service quality. For the other restaurant aspects extended_Distant_Emoticons feature 
set does not improve F1 score or even worsens it. 
As for dictionaries, the corresponding features can improve the results for the service 
frame. However, food quality, one of the most important restaurant characteristics 
along with service quality, is best extracted using just non-contiguous bigrams which 
seem to cover a wide variety of the expressions of opinion. Thus, a more elaborate 
lexicon and dictionaries construction could be one of our future work directions. For 
 Corpus-based IE and OM for the Restaurant Recommendation System 267 
example, sentiment lexicon currently includes ambiguous words and thus demands 
elaborate sense differentiation. 
6 Conclusion. Further Work 
In this paper we propose a corpus-based method of information extraction and opinion 
mining for the restaurant recommendation system. It uses machine learning techniques 
and is based on elaborate corpus analysis and automatic classifier selection. 
We have experimented with a number of machine learning models and feature sets 
with respect to our tasks, and employed statistical tests to select the optimal classifier 
for each of the restaurant aspects. The features include dictionaries constructed during 
corpus analysis stage (the latter is described in our earlier paper [27]). Selection proce-
dure is based on a set of rules and classifiers priorities and enables us to choose the 
most computationally effective combination of a model and a feature set among those 
which perform best during cross-validation. 
As a result of the experiments, Bernoulli and Multinomial Naive Bayes classifiers 
appear to be the most appropriate ones for the opinion class labeling. For the task of 
deciding on the relevance of a user’s review Logistic Regression, outperforms other 
models. Thus, these classifiers could be recommended for the tasks similar to those 
described above where the data is represented by colloquial non-structured texts and its 
amount is limited. 
For service quality frame, the application of dictionaries improves models perfor-
mance, which confirms the idea of employing preliminary corpus analysis. 
Thus, the results of the research verify the effectiveness of corpus-based methods 
with respect to the problem of information extraction and opinion mining from collo-
quial non-structured texts (in domains similar to restaurants) in flective languages with 
rich morphology and relatively free word order (like Russian), especially under re-
sourced ones. 
Our further work directions include more sophisticated sentiment and modifiers lex-
icons construction and annotation with the help of several experts. Since sentiment de-
gree of a word may depend on the restaurant aspect it refers to, the words are to be 
annotated with respect to every aspect separately; verbs are also to be included in the 
lexicon. We also plan to extend annotated subcorpus and to conduct another series of 
experiments according to the method described in the paper to verify that Logistic Re-
gression and SVM which are normally more effective than Naive Bayes on larger data 
sets will outperform it. 
References 
1. Aston, N., Liddle, J., Hu, W.: Twitter Sentiment in Data Streams with Perceptron. Journal 
of Computer and Communications, vol. 2, pp. 11–16 (2014) 
2. Bakliwal, A., Patil, A., Arora, P., Varma, V.: Towards Enhanced Opinion Classification 
using NLP Techniques. In: Proceedings of the Workshop on Sentiment Analysis where AI 
meets Psychology (SAAIP), IJCNLP, pp. 101–107 (2011) 
268 E. Pronoza, E. Yagunova, and S. Volskaya 
3. Benamara, F., Cesarano, C., Picariello, A., Reforgiato, D., Subrahmanian, V. S.: Sentiment 
Analysis: Adjectives and Adverbs are Better than Adjectives Alone. In: Proceedings of the 
International Conference on Weblogs and Social Media (ICWSM) (2007) 
4. Bermingham, A., Smeaton, A.: Classifying Sentiment in Microblogs: Is Brevity an Ad-
vantage? In: Proceedings of the International Conference on Information and Knowledge 
Management (CIKM) (2010) 
5. Collobert, R., Bengio, S.: Links between Perceptrons, MLPs and SVMs. In: Proceedings of 
the 21th International Conference on Machine Learning (2004) 
6. Das, S. R., Chen, M. Y.: Yahoo! for Amazon: Sentiment Parsing from Small Talk on the 
Web. Management Science, vol. 53 (9), pp. 1375–1388 (2007) 
7. Dave, K., Lawrence, S., Pennock, D. M.: Mining the Peanut Gallery: Opinion Extraction 
and Semantic Classification of Product Reviews. In: Proceedings of the 12th International 
Conference on World Wide Web, pp. 519–528 (2003) 
8. Davidov, D., Tsur, O., Rappoport, A.: Enhanced Sentiment Learning Using Twitter 
Hashtags and Smileys. In: Proceedings of the 23rd International Conference on Computa-
tional Linguistics: Posters, pp. 241–249. Association for Computational Linguistics (2010) 
9. Demšar, J.: Statistical Comparisons of Classifiers over Multiple Data Sets. Journal of Ma-
chine Learning Research, vol. 7, pp. 1–30, (2006) 
10. Devitt, A., Ahmad, K. Is there a Language of Sentiment? An Analysis of Lexical Resources 
for Sentiment Analysis. Language Resources and Evaluation, vol. 47 (2), pp. 475–511 
(2013) 
11. Emadzadeh, E., Nikfarjam, A., Ghauth, K. I., Why, N. K.: Learning Materials Recommen-
dation Using a Hybrid Recommender System with Automated Keyword Extraction. World 
Applied Sciences Journal, vol. 9 (11), pp. 1260–1271 (2010) 
12. Gatterbauer, W., Bohunsky, P., Herzog, M., Krüpl, B., Pollak, B.: Towards Domain-Inde-
pendent Information Extraction from Web Tables. In: Proceedings of the 16th International 
Conference on World Wide Web, pp. 71–80 (2007) 
13. Iman, R. L., Davenport, J. M.: Approximations of the Critical Region of the Friedman Sta-
tistic. Communications in Statistics, pp. 571–595 (1980) 
14. Kennedy, A., Inkpen, D.: Sentiment Classification of Movie Reviews Using Contextual Va-
lence Shifters. Computational Intelligence (2006) 
15. Kotelnikov, M., Klekovkina, M.: The Automatic Sentiment Text Classification Method 
based on Emotional Vocabulary. RCDL’2012 (2012)  
16. Leksin V., Nikolenko S. I.: Semi-supervised Tag Extraction in a Web Recommender Sys-
tem. In: Proceedings of the 6th International Conference on Similarity Search and Applica-
tions (SISAP 2013), Lecture Notes in Computer Science, pp. 206–212 (2013) 
17. Li, Y., Nie, J., Zhang, Y., Wang, B., Yan, B., Weng, F.: Contextual Recommendation Based 
on Text Mining. In: Proceedings of the 23rd International Conference on Computational 
Linguistics (Coling 2010): Poster Volume, pp. 692–700 (2010) 
18. Liu, J., Seneff, S.: Review Sentiment Scoring via a Parse-and-Paraphrase Paradigm. In: Pro-
ceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, 
Singapore, pp. 161–169 (2009) 
19. Marchand, M., Ginsca, A. L., Besançon, R., Mesnard, O.: [LVIC-LIMSI]: Using Syntactic 
Features and Multi-polarity Words for Sentiment Analysis in Twitter. In: Proceedings of the 
Seventh International Workshop on Semantic Evaluation, pp. 418–424 (2013) 
20. Narayanan, V., Arora, I. and Bhatia, A.: Fast and Accurate Sentiment Classification Using 
an Enhanced Naive Bayes Model (2013) 
21. Naw, N., Hlaing, E. E.: Relevant Words Extraction Method for Recommendation System. 
International Journal of Emerging Technology and Advanced Engineering, vol. 3 (1), (2013) 
 Corpus-based IE and OM for the Restaurant Recommendation System 269 
22. Pang, B., Lee, L., Vaithyanathan, S.: Thumbs up? Sentiment classification using machine 
learning techniques. In: Proceedings of the Conference on Empirical Methods in Natural 
Language Processing (EMNLP), pp. 79–86 (2002) 
23. Pak A., Paroubek P.: Language Independent Approach to Sentiment Analysis. Komp’uter-
naya Lingvistika i Intellektualnie Tehnologii: po materialam ezhegodnoy mezhdunarodnoy 
konferencii “Dialog”, vol. 11 (18), RGHU, Moscow, pp. 37–50 (2012) 
24. Pang, B., Lee, L.: Opinion Mining and Sentiment Analysis. Foundations and Trends in In-
fomation Retrieval, vol. 2 (1–2), pp. 1–135 (2008) 
25. Park D. H., Kim H. K., Kim J. K.: A Literature Review and Classification of Recommender 
Systems Research. Social Science, vol. 5, pp. 290–294 (2011) 
26. Pazzani, M. J., Billsus, D.: Content-Based Recommendation Systems. In: Brusilovsky P., 
Kobsa A., Nejdl W. (eds.) The Adaptive Web, LNCS, vol. 4321, pp. 325–341, Springer-
Verlag, Heildelberg (2007) 
27. Pronoza, E., Yagunova, E., Lyashin, A.: Restaurant Information Extraction for the Recom-
mendation System. In: Proceedings of the 2nd Workshop on Social and Algorithmic Issues 
in Business Support: “Knowledge Hidden in Text”, LTC’2013, (2013) 
28. Ricci F., Rikach L., Shapira B., Kantor, P.: Recommender Systems Handbook. Springer 
(2011) 
29. Saif, H.: Sentiment Analysis of Microblogs. Mining the New World. Technical Report KMI-
12-2 (2012) 
30. Sarawagi, S.: Information Extraction. Foundations and Trends in Databases, vol. 1 (3), 
pp. 261–377 (2007) 
31. Shah, K., Munshi, N., Reddy, P.: Sentiment Analysis and Opinion Mining of Microblogs 
(2013) 
32. Sharma, A., Dey, S.: An Artificial Neural Network Based Approach for Sentiment Analysis 
of Opinionated Text. In: Proceedings of the 2012 ACM Research in Applied Computation 
Symposium, pp. 37–42 (2012) 
33. Sidorov, G., Velasquez, F., Stamatatos, E., Gelbukh, A., Chanona-Hernández, L.: Syntactic 
N-grams as Machine Learning Features for Natural Language Processing. Expert Systems 
with Applications, vol. 41 (3), 2014, pp. 853–860 (2014) 
34. Socher, R., Perelygin, A., Wy, J. Y., Chuang, J., Manning, Ch. D., Ng, A. Y., Potts, Ch.: 
Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank. In: Pro-
ceedings of the Conference on Empirical Methods in Natural Language Processing (2013) 
35. Turmo, J., Ageno, A., Català, N.: Adaptive Information Extraction (2006) 
36. Turney, P.: Thumbs up or Thumbs Down? Semantic Orientation Applied to Unsupervised 
Classification of Reviews. In: Proceedings of the 40th Annual Meeting of the Association 
for Computational Linguistics (ACL), pp. 417–424 (2002) 
37. Wang, S., Manning, Ch. D.: Baselines and Bigrams: Simple, Good Sentiment and Topic 
Classification. In: Proceedings of the 50th Annual Meeting of the Association for Compu-
tational Linguistics (ACL): Short Papers – vol. 2, pp. 90–94 (2012) 
