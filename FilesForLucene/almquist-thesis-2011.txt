 
Experiments to Investigate 
the Utility of Linguistically 
Informed Features for Detecting 
Textual Plagiarism 
 
 
 
 
   
  
  
 
 
 
 
 
 
 
 
 P E R  A L M Q U I S T    
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
  
  
 
 
 
 
 
 
 
 
 
 
 
 
 Master of Science Thesis 
 Stockholm, Sweden 2011 
 
 
 
Experiments to Investigate 
the Utility of Linguistically 
Informed Features for Detecting 
Textual Plagiarism  
 
 
 
 
   
  
  
 
 
 P E R  A L M Q U I S T  
 
 
 
 
 Master’s Thesis in Computer Science (30 ECTS credits) 
 at the School of Computer Science and Engineering 
 Royal Institute of Technology year 2011  
 Supervisor at CSC was Serafim Dahl 
 Examiner was Stefan Arnborg 
 
 TRITA-CSC-E 2011:109 
 ISRN-KTH/CSC/E--11/109--SE 
 ISSN-1653-5715 
 
 
 
 
 
 Royal Institute of Technology 
 School of Computer Science and Communication 
 
 KTH CSC 
 SE-100 44 Stockholm, Sweden 
 
 URL: www.kth.se/csc 
 
 
 
 
 
 
Abstract
We perform experiments that shows whether or not two
linguistic features are good indicators to be used when au-
tomatically detecting plagiarism in digital texts.
Two experiments are performed. In the first experiment
a linguistic feature based on a semantic word-space model is
evaluated, and in the second experiment a linguistic feature
based on stylometry is evaluated. Both experiments are
evaluated using a nearest neighbor metric since the features
are multidimensional vectors.
We find that the first feature is a good indicator for
detecting plagiarism that is an exact copy of its source. We
find that the second feature performs equally good inde-
pendent of text obfuscation.
Referat
Experiment rörande användning av
lingvistiska särdrag i plagiatkontroll
Vi utför experiment som visar huruvida två lingvistiska sär-
drag är bra indikatorer att använda för att automatiskt
upptäcka plagiat i digitala texter.
Två experiment utförs. I det första experimentet utvär-
deras ett lingvistiskt särdrag som baseras på en semantisk
ord-rums modell och i det andra experimentet utvärderas
ett lingvistiskt särdrag som baseras på stilometeri (eng.
stylometry). Båda experimenten utvärderas med hjälp av
ett närmaste granne (eng. nearest neighbor) mätvärde ef-
tersom särdragen är flerdimensionella vektorer.
Vi finner att det första särdraget är en bra indikator för
att upptäcka plagiat som är en exakt kopia av källan. Vi
finner att det andra särdraget fungerar lika bra oberoende
av text-förvirring (eng. obfuscation).
This work was performed at the Swedish Institute of Computer Science (SICS),
supported by the Swedish Research Council (Vetenskapsrådet) through the project
“Distributionally derived grammatical analysis models”.
I would like to start by thanking my supervisors at SICS, Jussi Karlgren and
Magnus Sahlgren, for educating me within the appropriate research areas and for
giving me many memorable discussions around the sofa group at the Userware
laboratory.
Thank you Serafim Dahl for keeping track of me and making sure that this thesis
progressed and for telling me many inspiring life stories.
Thank you Ann Bengtsson for making sure that this thesis complied with all
administrative aspects.
Thank you Linnea Lundberg for being a great opponent and giving me extensive
feedback on this report.
Thanks to everyone in the sxteam1 (Per Strand Eklund, Karl Trumstedt, Carl-
Fredrik Sundlöf, and Niklas Frisk) for supporting this thesis.
Thank you Stefan Arnborg for examining this thesis.
Finally I would like to thank my mother, Margareta Almquist, for proofreading
this theis.
 
Contents
1 Introduction 1
1.1 Definitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2
1.1.1 Plagiarism . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2
1.1.2 External analysis . . . . . . . . . . . . . . . . . . . . . . . . . 3
1.1.3 Intrinsic analysis . . . . . . . . . . . . . . . . . . . . . . . . . 3
1.1.4 Linguistic feature . . . . . . . . . . . . . . . . . . . . . . . . . 4
1.2 Problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
1.3 Aim . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
1.4 Objective . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
1.5 Delimitation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
2 Relevant theory 5
2.1 Word-space model . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5
2.2 Vector distance and similarity measures . . . . . . . . . . . . . . . . 6
2.3 Nearest neighbor search in vector space . . . . . . . . . . . . . . . . 7
2.4 Stylometry . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
2.4.1 Named Entity Recognition . . . . . . . . . . . . . . . . . . . 8
2.5 Machine learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
2.6 Quality measures in information retrieval . . . . . . . . . . . . . . . 9
2.6.1 Precision . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
2.6.2 Recall . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
2.6.3 F-measure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
2.6.4 Granularity . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
2.6.5 Overall . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
3 Related work 11
4 Experiments 19
4.1 Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
4.2 Process . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
4.3 Used software . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
4.3.1 Plagiarism detector . . . . . . . . . . . . . . . . . . . . . . . . 20
4.3.2 Natural Language Toolkit (NLTK) . . . . . . . . . . . . . . . 20
4.3.3 NumPy and SciPy . . . . . . . . . . . . . . . . . . . . . . . . 21
4.3.4 Apache Lucene . . . . . . . . . . . . . . . . . . . . . . . . . . 21
4.3.5 PyLucene . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
4.3.6 Sequence tagger . . . . . . . . . . . . . . . . . . . . . . . . . 22
4.3.7 Guile Sparse Distributed Memory . . . . . . . . . . . . . . . 22
4.4 Platform . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
4.5 Chosen features . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
5 Results 25
6 Obstacles 29
6.1 Deciding which text level to operate on . . . . . . . . . . . . . . . . 29
6.2 Dealing with large amounts of data . . . . . . . . . . . . . . . . . . . 29
6.2.1 Extracted values . . . . . . . . . . . . . . . . . . . . . . . . . 30
6.3 The blackbox machine learner . . . . . . . . . . . . . . . . . . . . . . 30
6.3.1 Tuning the parameters . . . . . . . . . . . . . . . . . . . . . . 30
6.3.2 Interpreting the output . . . . . . . . . . . . . . . . . . . . . 31
6.4 Determining the sliding window length . . . . . . . . . . . . . . . . . 31
6.5 Separation of features . . . . . . . . . . . . . . . . . . . . . . . . . . 31
6.6 Number of folds . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31
7 Conclusions 33
7.1 Experiment 1: the word-space model . . . . . . . . . . . . . . . . . . 33
7.2 Experiment 2: stylometry . . . . . . . . . . . . . . . . . . . . . . . . 33
7.3 Out of the blue conclusions . . . . . . . . . . . . . . . . . . . . . . . 33
8 Future work: where to go from here 35
Bibliography 37
Chapter 1
Introduction
plagiarizing: to steal and pass off (the ideas or words of another)
as one’s own : use (another’s production) without
crediting the source
In Merriam-Webster Online Dictionary. Retrieved April 22, 2010, from
http://www.merriam-webster.com/dictionary/plagiarizing
One may argue that as the amount of text increases so does the amount of
plagiarism. It is simply a mean of quantity. The more words that are put into
sentences to create meaning the bigger the risk that your specific idea has already
been put in print and that infer that if you put that same idea in print you are
plagiarizing.
There is however a difference between intentional and unintentional plagiarism.
One might argue that in order for an author to be intentionally plagiarizing, the
author needs to know that the work that he or she is performing has previously
been written by someone else.
Just because a plagiarism has occurred does not mean that anyone is aware of
it. Detecting plagiarism is like catching criminals, first the suspect needs to be
identified and accused and then the suspect’s guilt needs to be proven in some way.
This is not an easy and straightforward task.
As the number of digital texts available to the public increase, due to the digital
evolution and its main carrier the Internet, the easier it is for an author to become
1
CHAPTER 1. INTRODUCTION
aware of previously done work. So the claim that one unintentionally plagiarizes
someone else’s work should decrease, at least in theory.
The digitalization of texts brings means of possible solutions to the problem of
detecting plagiarism. If one was to automatically detect plagiarism, in theory it
would be as simple as analyzing every new text-input to the digital world, cross-
checking it against every previous text-input and decide whether or not this new
input is a duplicate of any previous text. In practice this kind of comparison is
seldom possible because the amount of previous texts, that one needs to compare a
new text against, is enormous.
Fortunately the problem of automatic plagiarism detection is a problem that
has already been studied to some extent. Solutions have been found, but no unified
solution that suffices for every occasion, and hence the research continues. This is
where we try to provide and commit to the community as we dig deeper into the
research field of plagiarism detection.
1.1 Definitions
Here follows a few central definitions of how we use words in this report.
1.1.1 Plagiarism
We define plagiarism as the process seen in figure 1.1.
A text has a source which is given by a time, a place, and an author.
A text might be transformed into another text through a set of text operations
as seen in figure 1.2. Some text operations are divided into levels of granularity
within the text they operate on. For example, "Word level" text operations can
operate on both words and phrases but "Phrase level" text operations can operate
phrases and levels of higher granularity but not on words.
When a text is transformed using the text operations described above the source
needs to be credited. If no credit is given to the source we consider the transformed
text to be plagiarism of the source.
Source
Text operations
AND
Uncredited source
Plagiarism
Figure 1.1. Definition of plagiarism.
2
1.1. DEFINITIONS
Figure 1.2. Possible operations on text that can lead to plagiarism.
We divide the problem of automatic plagiarism detection into two subproblems,
external and intrinsic analysis.
1.1.2 External analysis
Figure 1.3. Definition of external analysis.
As seen in figure 1.3, external analysis is defined given a set of original documents
O and a set of suspicious documents S. The problem is to find pairs of phrases
(pO, pS) such that pO ∈ O and pS ∈ S and where pS plagiarize pO (according to the
definition seen in figure 1.1).
1.1.3 Intrinsic analysis
Intrinsic analysis is defined given a set of suspicious documents S. The problem is
to find phrases p where p ∈ S and p is plagiarism.
3
CHAPTER 1. INTRODUCTION
Intrinsic analysis is similar to external analysis but the source is not cared for.
There is of course a source to the plagiarized phrases but by who, where, and
when the source text was created is not relevant. Intrinsic analysis, in practice,
is a method of classifying whether a phrase differs enough from the rest of that
particular text so that the phrase could be plagiarism.
1.1.4 Linguistic feature
A linguistic feature can be any measurable characteristic of a text. For example the
average length of the words used in a text or the number of times the word "yellow"
appears in a text.
1.2 Problem
Our problem is to investigate how well linguistic features perform in detecting pla-
giarism through either external or intrinsic analysis. We especially want to evaluate
features based on the word-space model and stylometry.
1.3 Aim
1.4 Objective
Our aim is to find linguistic features that can be used as indicators when detecting
plagiarism in textual documents. Our objective is to train a machine learner to be
able to classify text phrases as either plagiarism or not given these features.
1.5 Delimitation
We only look at a limited amount of linguistic features. We only evaluate our
method on one corpus, although it is fairly large and varies in both text style and
genre.
4
Chapter 2
Relevant theory
In this chapter we give you some theory about the techniques that our approach is
built upon.
2.1 Word-space model
"The word-space model is a computational model of word meaning
that utilizes the distributional patterns of words collected over large
text data to represent semantic similarity between words in terms
of spatial proximity." (Sahlgren, 2006).
The word-space model (WSM) models the meaning of words, in a linguistic
semantic way. And it does so depending on where in the text the words appear.
The word-space in the WSM is a high dimensional vector space where words are
represented by vectors. Two words are semantically similar if their respective vectors
are similar. For example the words "yellow" and "green" could be argued to have
similar semantic meaning. So the vectors for yellow and green should be similar as
seen in figure 2.1.
Figure 2.1. The WSM vectors for the words "yellow" and "green".
The WSM is, as its name implies, mainly used to model words. It can however
be used to model other linguistic entities such as sentences and documents using
workarounds. A sentence can be modeled using the WSM by taking the centroid of
the sentence’s individual word’s vectors. Therefore if the sentence "A yellow car."
was changed to "A green car." the centroid should not change that much since the
only change to the centroid would be one vector that in the first case represented
5
CHAPTER 2. RELEVANT THEORY
the word "yellow" and in the second case the word "green" and these vectors should
be fairly similar as seen in figure 2.2 and figure 2.3.
A yellow car. A green car.
Figure 2.2. A changed sentence.
Figure 2.3. A centroid of a changed sentence.
We use the WSM to model sentences in such a way that if a sentence were to
be obfuscated, by the text operations described in section 1.1 and figure 1.2, its
semantic similarity should be kept.
For more information about the WSM see Kanerva (1988); Sahlgren (2006).
2.2 Vector distance and similarity measures
The science of measuring similarity between vectors provide many different methods.
Most of them follow the relationship seen in equation 2.1 between the similarity
(sim) and the distance (dist) of two vectors (~x and ~y).
sim(~x, ~y) = 1
dist(~x, ~y) (2.1)
Perhaps the most famous distance measure is the Minkowski distance as seen
in equation 2.2 and its specializations the Eucleadian distance and the City-Block
distance as seen in equation 2.3.
distM (~x, ~y) =
(
n∑
i=1
|xi − yi|N
) 1
N
(2.2)
distCB(~x, ~y) =
n∑
i=1
|xi − yi| (2.3)
6
2.3. NEAREST NEIGHBOR SEARCH IN VECTOR SPACE
Another common measure is the cosine similarity as seen in the equation 2.4.
simCOS(~x, ~y) =
~x · ~y
|~x||~y|
=
∑n
i=1 xiyi√∑n
i=1 x
2
i
√∑n
i=1 y
2
i
(2.4)
We use the City-Block distance and the cosine similarity since they have been
shown (Sahlgren, 2006) to work well together with the word-space model.
2.3 Nearest neighbor search in vector space
The nearest neighbor search problem is about finding a given vector’s closest neigh-
bor (another vector that is the most similar or the closest to the given vector) under
some similarity or distance measure.
Weber, Schek, and Blott (1998) showed that the best approach to a nearest
neighbor search in a space of magnitude larger than 10 dimensions is the naïve
exhaustive search approach. Although one can partition the space, with respect
to either the input data or the actual space, that partitioning work often becomes
heavier than the actual similarity/distance measure.
In our case the City-Block distance and the cosine similarity is fairly easy to
calculate and since we work in dimensions of magnitude 3000 the naïve exhaustive
search approach is the way to go. We define the nearest neighbor for a vector ~u and
a set of vectors V as in equation 2.5.
max(sim(~u, ~x)) = min(dist(~u, ~x)) for all ~x ∈ V (2.5)
2.4 Stylometry
Stylometry is the study of stylistic metrics that can be used to determine what
style a text follow. The styles might be almost anything, for example: journalistic,
scientific, written by a specific author, or political and does not need to be disjoint.
We use stylometry with the idea that when an author writes a document the
document adheres to a specific style. So if there is a passage in a document whose
style diverse from the rest of the document’s style, the passage might be subject to
plagiarism.
The features that decide styles in stylometry might also be almost anything. Our
features are mainly concerned with statistics about the text (Biber, 1988; Karlgren,
2000). For example any of the following:
• the length of words or sentences,
• whether specific words occur in the text or not,
• on what form the phrases are written in.
7
CHAPTER 2. RELEVANT THEORY
Almost, barely, hardly, merely, mildly, nearly, only, partially,
partly, practically, scarcely, slightly, somewhat, at about, some-
thing like, more or less, almost, maybe, sort of, kind of, ab-
solutely, altogether, completely, enormously, entirely, extremely,
fully, greatly, highly, intensely, perfectly, strongly, thoroughly, to-
tally, utterly, very, for sure, a lot, such a, just, really, most, more,
anyway anyhow, anyways, can, may, might, could, ought, should,
must, will, would or shall.
Figure 2.4. Stylometry feature that describes an argumentative style.
These features are then analyzed and leads to a fingerprint that represents a
specific style.
In (Biber, 1988) Douglas Biber develops no less than 66 different linguistic sty-
lometry features. As an example, a text is declared as being argumentative if it
contain any of the words in figure 2.4.
2.4.1 Named Entity Recognition
Named entity recognition (NER) is the process of identifying, classifying, and ex-
tracting named entities from texts. Named entities are definite noun phrases that
refer to specific types of individuals, such as organizations, persons, dates, loca-
tions, and so on (Bird, Klein, and Loper, 2009, chapter 7 section 5). We use NER
as linguistic features.
2.5 Machine learning
Machine learning is a special kind of computer program that perfom some task and
gets better at performing its task the more data input it encounters and the longer
it runs.
We use the machine learning algorithm Max-margin which is a hybrid between
the famous support vector machine (SVM) and the Bayesian probabilistic graph
learning algorithm. Max-margin was first proposed in “Max-margin Markov net-
works” by Taskar, Guestrin, and Koller (2003).
We use Max-margin with the extension that the margin rescaling is done via
stochastic sub-gradient decent and beam search. This extension has been shown
to provide good results for labeling sequences as seen in the paper “Uncertainty
Detection as Approximate Max-Margin Sequence Labelling” by Täckström et al.
(2010).
8
2.6. QUALITY MEASURES IN INFORMATION RETRIEVAL
2.6 Quality measures in information retrieval
In information retrieval there are several metrics for determining the quality of a
query result. In order to measure how accurate our plagiarism detection is we will
use the metrics: precision, recall, F-measure, granularity, and overall; all described
below.
Notation In order to define the following quality measures we need to introduce
some notation (Croft, Metzler, and Strohman, 2009; Potthast et al., 2009). Let s
denote a plagiarized passage from the set S of all plagiarized passages. Let r denote
a detection from the set R of all detections and let SR be the subset of S for which
detections exist in R. Let |s|, |r| denote the char lengths of s, r and let |S|, |R|,
|SR| be the sizes of the respective sets.
2.6.1 Precision
Precision measures the proportion between the correctly detected plagiarized pas-
sages and the total amount of detected plagiarized passages (including the ones that
were detected as plagiarism but in fact were not plagiarism).
Precision = true positivestrue positives + false positives =
|R ∩ S|
|R|
= 1
|R|
|S|∑
i=1
# detected chars of ri
|ri|
2.6.2 Recall
Recall measures the proportion between the correctly detected plagiarized passages
and the total amount of plagiarized passages (including the ones that were not
detected).
Recall = true positivestrue positives + false negatives =
|R ∩ S|
|S|
= 1
|S|
|R|∑
i=1
# detected chars of si
|si|
2.6.3 F-measure
The F-measure is a composite measurement that tries to capture both precision and
recall, and is defined as the harmonic mean between them.
F −measure = 11
2(
1
P recision +
1
Recall )
= 2 · Precision ·Recall
Precision+Recall
2.6.4 Granularity
The granularity measures the number of times a part of the text is detected as
plagiarism and introduce a way to penalize overlapping plagiarism detections.
Granularity = 1
|SR|
SR∑
i=1
# of detections of si in R
9
CHAPTER 2. RELEVANT THEORY
2.6.5 Overall
The overall measurement is, as the F-measure, a way to combine the other mea-
surements, it combines precision, recall, and granularity.
Overall = F −measurelog2(1 +Granularity)
10
Chapter 3
Related work
In this chapter we give you an overview of the related work that has been done in
the field of plagiarism detection and its neighboring fields authorship identification
and near duplicate detection. Common techniques in these fields are:
• compression,
• fingerprints,
• longest common subsequence,
• inverted indices,
• n-grams,
• shingles,
• suffix trees,
• vector space models,
• WordNet.
We provide you an historical overview of the related work by the following summa-
rized publications in chronological order.
Uzuner, Katz, and Nahnsen (2005) In “Using syntactic information to iden-
tify plagiarism” a plagiarism detection system that uses syntactic and semantic word
classes is presented to be used to detect plagiarism without a reference corpus.
Eissen and Stein (2006) In “Intrinsic Plagiarism Detection” the problem of
detecting plagiarism without a reference corpus is further studied and the name
intrinsic plagiarism detection is coined. Stylometrics is used to detect intrinsic
plagiarism and the stylometric features are divided into five categories as seen in
figure 3.1.
11
CHAPTER 3. RELATED WORK
Text statistics
Syntatic
Part of speech (POS)
Closed-class words
Structural
Figure 3.1. Stylometric feature categories.
Stein, Eissen, and Potthast (2007) In “Strategies for retrieving plagiarized
documents” plagiarism detection with a large reference corpus is analyzed and a
strategy for retrieving plagiarized documents is proposed. The strategy is divided
into three mayor stages as seen in figure 3.2 and table 3.1. Stein, Eissen, and Pot-
thast also experiment with fuzzy fingerprinting as a means of comparing documents
with each other.
Heuristic Retrieval Detailed analysis Knowledge based post-processing
Figure 3.2. A three-stage retrieval strategy.
Name Description
Heuristic retrieval
(HR)
Retrieves candidate documents (Dc) from the ref-
erence corpus.
Detailed analysis
(DA)
Find passages in Dc that are plagiarized in the sus-
picious document.
Knowledge based
post-processing
(KBPP)
Are there any duplicate or overlapping plagiarized
passages.
Table 3.1. A three-stage retrieval strategy.
12
Eissen, Stein, and Kulig (2006) In “Plagiarism Detection Without Reference
Collections.” the five stylometric categories as seen in figure 3.1 are described in
more detail as can be seen in table 3.2.
Name Description
Text statistics Statistics that operate on the character level, i.e.
number of commas.
Syntactic Measures writing style on sentence level, i.e. sen-
tence lengths.
POS Quantifies the use of word classes, i.e. number of
pronouns.
Closed-class words Counts special words, i.e. foreign words.
Structural Reflects text organization, i.e. paragraph lengths.
Table 3.2. Further descriptions of stylometric categories.
Stein and Eissen (2007) In “Intrinsic Plagiarism Analysis with Meta Learning”
intrinsic plagiarism analysis is defined as a one-class classification problem. That
is, information about only one class is available, the so-called target class, and all
other objects belong to the outlier class. Stein and Eissen also propose 20 stylistic
features to be used when detecting intrinsic plagiarism. Meta learning is then used
to find the most suitable stylistic features.
Alberto Barrón-Cedeño and Paolo Rosso (2008) In “Towards the Exploita-
tion of Statistical Language Models for Plagiarism Detection with Reference” a
statistical language model, that tries to predict a word given previous words is,
proposed. The language model is then used to find the perplexity of text fragments
and this perplexity is found to be a relevant feature in detecting plagiarism.
Alberto Barrón-Cedeño and Paolo Rosso and David Pinto and Alfons
Juan (2008) In “On Cross-lingual Plagiarism Analysis Using a Statistical Model”
semantic similarity between cross-lingual texts is analyzed and a statistical model
is proposed to detect near duplicates between text phrases.
Gustafson, Pera, and Ng (2008) In “Nowhere to Hide: Finding Plagiarized
Documents Based on Sentence Similarity” a method called Similarity-based Plagia-
rism Detection (SimPAD) is proposed that detect plagiarism between documents
using sentence similarity. The sentence similarity is computed through pre-defined
word-correlation factors.
Grozea, Gehl, and Popescu (2009) In “ENCOPLOT: Pairwise Sequence Match-
ing in Linear Time Applied to Plagiarism Detection” the overall winning plagiarism
13
CHAPTER 3. RELATED WORK
detection system of the PAN’09 competition is presented (see table 3.3 for details
about the score). The system follows an approach similar to the one in figure 3.2.
First, sets of candidate documents (Dc) from the reference corpus is retrieved
for every suspicious document (dS) (this step is similar to the HR step in the three
stage retrieval strategy). This is done by comparing document similarity through a
matrix of string kernels .
Then a eN-gram COincidence PLOT (ENCOPLOT) is created for every dS and
its Dc (corresponding to the DA step). These ENCOPLOTs are scatter plots that
describe where pairs of documents have the same n-grams. The areas in the plots
that share the most n-grams are the ones of interest for plagiarism detection.
Once these areas has been located Monte Carlo simulations are performed to
provide supplementary filtering (corresponding to the KBPP step).
Place Overall F Precision Recall Granularity Participant
1 0.6957 0.6976 0.7418 0.6585 1.0038 Grozea, Gehl, and Popescu (2009)
2 0.6093 0.6192 0.5573 0.6967 1.0228 Kasprzak, Brandejs, and Kripac (2009)
3 0.6041 0.6491 0.6727 0.6272 1.1060 Basile et al. (2009)
4 0.3045 0.5286 0.6689 0.4370 2.3317 Palkovskii, Belov, and Muzika (2009)
5 0.1885 0.4603 0.6051 0.3714 4.4354 Muhr et al. (2009)
6 0.1422 0.6190 0.7473 0.5284 19.4327 Shcherbinin and Butakov (2009)
Table 3.3. PAN’09 External Analysis Partial Results
Kasprzak, Brandejs, and Kripac (2009) In “Finding Plagiarism by Evaluat-
ing Document Similarities” an approach to external analysis plagiarism detection is
proposed that is very similar to the strategy seen in figure 3.2, but the first (HR)
and the second (DA) stage is more intervened in this approach.
By tokenizing words, stripping of diacritics and ignoring short words, a more
normalized form of documents is achieved.
Then the tokens are joined into a chunk by using a hash function, and these
chunks are used to create an inverted index over the reference corpus. That index
also holds information about the offset and length of a chunk.
For every suspicious document a list of candidate documents are retrieved from
the inverted index but that is not all that is retrieved, a list of common chunks
(with offset and length) is also retrieved (this is why HR and DA is intervened in
this approach since not only the candidate documents are retrieved in the HR phase
but also information about the actual passages within the candidate documents).
After that the detailed analysis phase continues by further looking at the common
chunks and only so-called valid intervals (chunks that adhere to specific properties)
are kept.
In the end (KBPP) the overlapping valid intervals are taken care of in the way
that the largest intervals are kept and the smaller ones are thrown away.
This approach finished second in PAN’09 external analysis competition as seen
in table 3.3.
14
Basile et al. (2009) In “A plagiarism detection procedure in three steps: selec-
tion, matches and ’squares’” an approach, similar to figure 3.2, for detecting external
plagiarism is proposed.
The first step (HR), reduce the set of candidate documents for every suspicious
document to a size of ten candidate documents. This reduction is done by first
coding every document, using only the length of the words within the text, and
then building frequency vectors from 8-grams of the coded documents, and finally
calculating the top 8-gram distances to receive the ten candidate documents.
The second part (DA), first tries to recover some of the information that was lost
in the previous coding of the documents. This recovery is done by a T9-like coding
of the words. Then a longest common subsequence matching algorithm is run over
the suspicious documents and its candidate documents (all T9-coded) which results
in a two dimensional plot that represent the areas of commonality. These areas look
like squares where there might be plagiarism so any square is chosen.
The algorithm is then run once again over the squares (KBPP) to refine the
result further.
Basile et al. finished third in PAN’09 external analysis competition as seen in
table 3.3.
Shcherbinin and Butakov (2009) In “Using Microsoft SQL Server platform
for plagiarism detection” a two stage approach for detecting external plagiarism
using databases (DB) is proposed.
First, documents and corresponding document fingerprints are stored into the
DB. The fingerprints are calculated using the Winnowing fingerprint algorithm.
Candidate documents for every suspicious document are retrieved by using the
heuristic that the suspicious document and the candidate document should share at
least one fingerprint. The suspicious documents and its set of candidate documents
are stored into the DB.
A console application is run that compares each suspicious document with its
candidate documents in both an exact string match and a n-gram approach using
Levestein distance as a similarity measure. This result in a number of intervals
in the suspicious and candidate documents that are stored in the DB and these
intervals are the plagiarized passages.
Finally the intervals corresponding to the plagiarized passages are exported from
the DB.
Muhr et al. (2009) In “External and Intrinsic Plagiarism Detection Using Vec-
tor Space Models” two approaches, one for external and one for intrinsic detection
of plagiarism, are proposed that are built on some common concepts. Both ap-
proaches uses Vector Space Models (VSM) that operate on a sentence level, detect
similarity between sentences by the cosine similarity measure, and follows a three
stage process.
For the external plagiarism detection the three parts are as follows:
15
CHAPTER 3. RELATED WORK
1. Turn every sentence in the reference corpus into a vector. This vector is
called term vector ~r and is built from the VSM. Partition the term vectors
into clusters of term vectors Ci that are based upon their cosine similarity.
2. For every sentence in the suspicious documents the sentence is turned into
a term vector ~s. The ~s is then compared to all different Ci by using cosine
similarity. The two clusters that are most similar to the suspicious sentence is
consider for further analysis and a fixed number of term vectors (~r) from these
clusters are retrieved. These term vectors are then compared to the suspicious
sentence term vector and if any of them are similar enough (as decided by a
threshold) the suspicious sentence is considered as plagiarism of that sentence
(annotated as ~ur−>s).
3. For every suspicious sentence that was considered as plagiarism its directly
preceding and following sentence neighbors are also compared against ~ur−>s
to further increase the plagiarism detection. Merge all plagiarized sentences
that are neighbors until there are no neighbors left.
For the intrinsic plagiarism detection the three parts are as follows:
1. Turn every sentence in the suspicious documents into term vectors.
2. Determine if any outlier sentences exist based on the document’s mean vector
which in turn is based on the mean of the vectors cosine similarities and their
standard deviation.
3. Merge all plagiarized sentences that are neighbors until there are no neighbors
left.
Stamatatos (2009) In “Intrinsic Plagiarism Detection Using Character n-gram
Profiles” the winning approach of the PAN’09 sub-competition for intrinsic plagia-
rism detection is presented (see table 3.4 for details about the score).
The approach is based upon the so-called style change function, that detects
when the style changes in a text, and several criteria are developed around this
function and they are used when detecting plagiarism.
The approach assumes that documents contain less than 50% plagiarism. There-
fore every document should have a dominant style, the so-called document style,
and when this style is not followed plagiarism might have occurred.
The style change function (SC) is based upon sliding windows over the doc-
ument. For every window a profile (wi for window i) is created from character
n-grams and all together all window’s profiles creates the document profile (D).
These profiles are used to create a distance function, that decides how far from D
a window profile is, and it is this distance function that creates the SC.
If the standard deviation (std(SC)) of the style change function is below a
predefined threshold then the document style varies so little that the document is
considered free of plagiarism.
16
To detect plagiarized passages the mean of the style change function (mean(SC))
and the std(SC) is used. If SC(w,D) > mean(SC) + std(SC) then that window is
detected as plagiarism.
After that another style change function (SC ′) is built, in the same way as
above but without considering the already plagiarized passages. If SC ′(w,D′) >
mean(SC ′)+std(SC ′)∗a where a is a predefined threshold value, then that window
is also detected as plagiarism.
A number of techniques is used to improve the result of the approach. All
documents are transformed into lowercase letters only. If there exists a n-gram
within a w that only contains non-letter characters (space, punctuation, digits etc.)
then that n-gram is excluded from the profile. The sliding window length and step
size only considers letter characters.
Place Overall F Precision Recall Granularity Participant
1 0.2462 0.3086 0.2321 0.4607 1.3839 Stamatatos (2009)
2 0.1955 0.1956 0.1091 0.9437 1.0007 Hagbi and Koppel (2009)
Table 3.4. PAN’09 Intrinsic Analysis Partial Results
As seen by the previous list of related work the problem of automatic plagiarism
detection is an area that has been studied to some extent. There are actually several
systems in use at the time of writing. To name a few, the interested reader might
want to check out TurnItIn1 and Urkund2.
1http://www.turnitin.com
2http://www.urkund.se
17

Chapter 4
Experiments
In this chapter we describe the experiments in more detail.
We perform two experiments. In the first experiment we evaluate how well the
nearest neighbor (NN) metric of vectors in a vector space based on the word-space
model (WSM) manage to detect plagiarism. In the second experiment we evaluate
how well the NN metric of binary vectors based on 19 different stylometry features
manage to detect plagiarism.
4.1 Data
We use the PAN-PC-091 corpus as data for our experiments. We use the corpus
since it can be used free of charge for research and contain plagiarized passages which
has previously been marked and labeled as plagiarism, so that we know beforehand
which passages are plagiarism.
The documents in the PAN-PC-09 corpus vary in length, style, and language.
The documents stem from the Project Gutenberg2 but some of the documents has
been incorporated by plagiarized passages in an artificial way. Every document is
stored as raw text in UTF-83 encoding and has a corresponding extensible markup
language4 (XML) file that labels the plagiarized passages in the document. Every
label hold information about the start and length of the plagiarized passage, some
also hold information about the source.
For detailed information about the PAN-PC-09 corpus see (Potthast et al.,
2009).
1http://www.uni-weimar.de/cms/medien/webis/research/corpora/pan-pc-09.html
2http://www.gutenberg.org
3http://tools.ietf.org/html/rfc3629
4http://www.w3.org/TR/REC-xml/
19
CHAPTER 4. EXPERIMENTS
4.2 Process
As seen in figure 4.1, we start off with a corpus that contain plagiarized sections
and unplagiarized (source) sections. The corpus is then transformed into our two
spaces, the word space and the stylometry space, by processing each sentence in the
corpus texts and extracting different values (based on the features). For each of the
vectors (that represents a sentence) we find its nearest neighbor in the rest of vectors
and calculate their similarity. We then go back to the corpus and select interesting
sections where the sentences contain different types of plagiarism. Finally we create
a plot for our selected sections that show our nearest neighbor score together with
the plagiarism label for each sentence.
Figure 4.1. How we performed the experiments
4.3 Used software
In order to perform the process described in figure 4.1 we use several software. Some
software is written in-house at SICS and some are available for use through various
open source licenses. Here follows a description about the software that we use.
4.3.1 Plagiarism detector
Plagiarism detector (pdect) is one of the main contributions of this work. It is the
main glue that is used to run all of the other software used in our approach to
build a system to automatically detect plagiarism. It also preprocesses, parses, and
tokenizes the text documents that are to be used as either original or suspicious
documents (see figure 1.3. pdect was written in Python5 by Per Almquist during
the spring of 2010.
4.3.2 Natural Language Toolkit (NLTK)
NLTK6 is a natural language application programming interface (API) accessible
via Python. We use it in the following way:
5http://www.python.org/
6http://www.nltk.org/
20
4.3. USED SOFTWARE
• to keep track of the texts as a corpus,
• to perform name entity recognition (NER) (see section 2.4.1),
• as a means to parse raw text into elements representing sentences,
• as a means to parse raw text into elements representing words,
• as a tokenizer from raw text to word classes (i.e. nouns, adjectives etc.).
4.3.3 NumPy and SciPy
NumPy7 and SciPy8 are APIs accessible via Python to help with scientific (math
and engineering) computing. We use them for a number of math calculations like
the dot-product, the cosine similarity, and the kdtree.
4.3.4 Apache Lucene
Apache Lucene9 is an open source search engine implemented in Java10. We use it to
reduce the amount of similarity comparisons needed between sentences by filtering
out unwanted documents. Apache Lucene can be used to build indices of a set of
documents and then perform efficient search queries through term frequency-inverse
document frequency (tf-idf) techniques.
MoreLikeThis
MoreLikeThis11 is, for us, a particularly interesting method of using Apache Lucene
to search. Given a previously created search index it provides means to search for
documents that are similar to a given document. It was created by David Spencer,
Mark Harwood, and Bruce Ritchie.
It works in the following way (as described by Johnson, 2008, blog) with the pre-
requisite that a Apache Lucene index of the documents has already been created.
• The document, that the result should comply with, is inserted into Apache Lucene.
Either as a file, an input stream, a reader, an URL or as a reference if the document
is already in the Apache Lucene index.
• The MoreLikeThis class partitions the document into several disjoint parts that are
called fields.
• A vector is created, for each field, that describe the frequency of words in that par-
ticular field. This vector is called the term vector.
• The term vectors are merged into a map where the name of the term is used as key
and a score, calculated for each term, is used as the value. This score is the essence
of the MoreLikeThis class and is based on tf-idf.
7http://numpy.scipy.org/
8http://www.scipy.org/
9http://lucene.apache.org
10http://www.java.com/
11http://lucene.apache.org/java/3_0_2/api/all/org/apache/lucene/search/similar/
MoreLikeThis.html
21
CHAPTER 4. EXPERIMENTS
• The top 25 terms with highest score is used to create a Apache Lucene search query.
• The documents that comply the most with the search query is returned.
4.3.5 PyLucene
PyLucene12 is a wrapper that provides access to Apache Lucene via Python.
4.3.6 Sequence tagger
Sequence tagger (seqtag) is a machine learner built upon the Max-margin (described
in section 2.5 and in the paper Täckström et al., 2010) implemented in Java by Oscar
Täckström.
4.3.7 Guile Sparse Distributed Memory
Guile Sparse Distributed Memory (gsdm) is an implementation of the word-space
model (described in section 2.1 and in the PhD thesis Sahlgren, 2006) implemented
in C13 and Guile14 by Magnus Sahlgren.
4.4 Platform
We run our experiments on a server that has two 64-bit Quad-Core AMD Opteron
Processor 2376 running at 2300 MHz and 32 GiB of RAM. The server runs the
operating system Ubuntu 10.4 with kernel 2.6.32-21-server.
4.5 Chosen features
In the first experiment only one feature is chosen. But it is a quite informative
feature, namely a vector from the word-space model with a magnitude of 3000 real
dimensions.
For the second experiment 19 features are chosen (as seen in table 4.1). The
features are solely based on stylometrics. Most of the features are implemented
using regular expressions and the rest through name entity recognition (NER). All
the features return a boolean metric that is true if the feature conditions are true.
12http://lucene.apache.org/pylucene/
13http://www.open-std.org/jtc1/sc22/wg14/
14http://www.gnu.org/software/guile/guile.html
22
4.5. CHOSEN FEATURES
Table 4.1. Stylometric features
Name Description
arg Sentence is argumentative (merely, for sure, ... )
cog Sentence describes cognitive process (remember, think, ...)
com Sentence is complex (average word length > 6 characters or sentence
length > 25 words)
date Sentence contains one or more date references
fin Sentence contains a money symbol or a percentage sign
fpp Sentence contains first person pronouns
le Sentence refers to named entities such as a person or an organization
loc Sentence mentions a location
neg Sentence contains a grammatical negation
num Sentence contains numbers
pa Sentence contains place adverbials (inside, outdoors ... )
pun Sentence contains punctuation in addition to its ending punctuation
se Sentence contains split infinitives or stranded prepositions
spp Sentence contains second person pronouns
sub Sentence has subordinate clauses
ta Sentence contains time adverbials (early, presently, soon ... )
tim Sentence contains one or more time expression
tpp Sentence contains third person pronouns
uni Sentence contains symbols representing a unit of measurement
23

Chapter 5
Results
In this chapter we present the results from our two experiments based on the nearest
neighbor (NN) metric.
The NN metric has the fortunate characteristic that a value of 1 describes iden-
tical or very similar vectors. So if we were to find NN values of 1 those two sentences
would be very alike and therefore we would be able to assume that the newer sen-
tence plagiarizes the older sentence.
As an example of plagiarism inspection mechanism we plot the NN metric with
the sentences of a text along the x-axis against the score of the sentence. The
objective is to find a stretch of material where several sentences have high NN
scores. As a comparison we will also plot the plagiarism labeling of respective
sentence. Now we can just plot our NN scores and our modified labels against the
sentences in the corpus. To determine the effectiveness of each nearness measure, the
results (red rhomboids) are displayed together with an indication of which section
of the text is plagiarized (blue squares) noted with a score of 1 and a score of 0 for
the non-plagiarized sections.
We choose to perform the experiments on three different text sections contain-
ing plagiarism of different obfuscation levels (performed through the different text
operations described in figure 1.2). On the first level the plagiarized text sequence
has no obfuscation, it is a straightforward copy of its source, on the second level the
plagiarized text sequence is a little obfuscated, and on the third level it is highly
obfuscated.
In figure 5.1, 5.2, and 5.3 we show the NN metric between sentences modelled
with the WSM. In figure 5.4, 5.5, and 5.6 we show the NN metric between sentences
modelled with stylometry. The NN metric is calculated as described in equation
2.5.
25
CHAPTER 5. RESULTS
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71
0
0.2
0.4
0.6
0.8
1
1.2
label
nearest neighbor
Figure 5.1. NN metric based on WSM for sentences with no obfuscation.
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38
0
0.2
0.4
0.6
0.8
1
1.2
label
nearest neighbor
Figure 5.2. NN metric based on WSM for sentences with a low level of obfuscation.
26
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54
0
0.2
0.4
0.6
0.8
1
1.2
label
nearest neighbor
Figure 5.3. NN metric based on WSM for sentences with a high level of obfuscation.
1 2 3 4 5 6 7 8 9 1
0
1
1
1
2
1
3
1
4
1
5
1
6
1
7
1
8
1
9
2
0
2
1
2
2
2
3
2
4
2
5
2
6
2
7
2
8
2
9
3
0
3
1
3
2
3
3
3
4
3
5
3
6
3
7
3
8
3
9
4
0
4
1
4
2
4
3
4
4
4
5
4
6
4
7
4
8
4
9
5
0
5
1
5
2
5
3
5
4
5
5
5
6
5
7
5
8
5
9
6
0
6
1
6
2
6
3
6
4
6
5
6
6
6
7
6
8
6
9
7
0
7
1
7
2
7
3
7
4
7
5
7
6
7
7
7
8
7
9
8
0
8
1
8
2
8
3
8
4
8
5
8
6
8
7
8
8
0
0.2
0.4
0.6
0.8
1
1.2
label
nearest neighbor
Figure 5.4. NN metric based on stylometry for sentences with no obfuscation.
27
CHAPTER 5. RESULTS
1 2 3 4 5 6 7 8 9 1
0
1
1
1
2
1
3
1
4
1
5
1
6
1
7
1
8
1
9
2
0
2
1
2
2
2
3
2
4
2
5
2
6
2
7
2
8
2
9
3
0
3
1
3
2
3
3
3
4
3
5
3
6
3
7
3
8
3
9
4
0
4
1
4
2
4
3
4
4
4
5
4
6
4
7
4
8
4
9
5
0
5
1
5
2
5
3
5
4
5
5
5
6
5
7
5
8
5
9
6
0
6
1
6
2
6
3
6
4
6
5
6
6
6
7
6
8
6
9
7
0
7
1
7
2
7
3
7
4
7
5
7
6
7
7
7
8
7
9
8
0
8
1
8
2
8
3
8
4
8
5
8
6
8
7
8
8
8
9
9
0
9
1
9
2
9
3
9
4
9
5
9
6
0
0.2
0.4
0.6
0.8
1
1.2
label
nearest neighbors
Figure 5.5. NN metric based on stylometry for sentences with a low level of obfus-
cation.
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48
0
0.2
0.4
0.6
0.8
1
1.2
label
nearest neighbor
Figure 5.6. NN metric based on stylometry for sentences with a high level of
obfuscation.
28
Chapter 6
Obstacles
In this chapter we discuss why we ended up performing only one out of several
planned experiments and discuss the numerous obstacles that we faced.
6.1 Deciding which text level to operate on
One of the first things we had to make a decision about was on which level of
granularity we would handle text. We discussed four different levels; word, sentence,
paragraph, and document. There were pros and cons with all of the levels so
this was a tough decision to make. The word level kept the most information
but required very demanding calculations. The document level lost information
but the calculations would be less demanding. The sentence and paragraph levels
were somewhere in between where the sentence level would keep most information.
Therefore we chose the sentence level.
We based our decision on an overall score that; made the granularity good
enough, kept the sequence information in the texts, and kept the feasability of our
calculations at some manageable level.
6.2 Dealing with large amounts of data
Due to our decision to base our experiments on a sentence level the amount of data
to be processed grew from large to very large. We had to do calculations on a set of
22 million sentences. It led us to introduce the filtering mechanism made possible
by Apache Lucene’s MoreLikeThis search.
We also decided upon using Python’s libraries NumPy and SciPy because al-
tough they were accessible from Python their implementation were in highly opti-
mized C code.
We made sure that we could run our different programs in parallel to make use
of the multi core architechture in our plattform.
We introduced several ways to store our partially calculated results (the sen-
tence vectors and their respective nearest neighbor metrics) that were based on the
29
CHAPTER 6. OBSTACLES
Hierarchical Data Format 51 (HDF5).
Even with the optimizations described above we ended up with very time con-
suming runtimes.
6.2.1 Extracted values
Based on the chosen features (as described in section 4.5) we extracted three differ-
ent sets of values.
One set (set1) based on the 19 stylometry features and sentences from the
intrinsic part of the PAN-PC-09 data, and two sets using sentences from the external
part of the PAN-PC-09 data where one set (set2) were based on the feature simCOS
(equation 2.4) and the other set (set3) were based on the feature distCB (equation
2.3). The external analysis sets of values were separated due the nearest neighbor
search, taking an enormous time to complete.
set1 took 17 days to extract (running the extraction in parallel with one process
for each of the 8 available CPU cores) and produced 12.517.087 lines of values (each
line containing 19 values corresponding to the chosen features). set2 run for 17 days
(also in parallel) but was then stopped since it had only produced 509.774 lines of
values (or around 2% of the (22 million) available sentences in the data). set3 run
for nine days (also in parallel) before it was stopped and produced 180.069 lines of
values.
6.3 The blackbox machine learner
It turned out that the machine learner could only handle a small number of input
sequences at a time (due to the Java Runtime Environment and our server’s 32GB
RAM limit). Therefore the amount of extracted values were decreased and set1 was
cropped into a smaller set (set1.1) containing 1.032.759 lines of values.
6.3.1 Tuning the parameters
There was also the issue of tuning the machine learner’s set of parameters in order to
improve our results. This turned out to be a large optimization problem since there
were several parameters to tune. We experimented with the following parameters:
• learningrate η
• beam step
• weighting, normal or inversed
• the template language describing how many sets of values to look at at the same time
1http://www.hdfgroup.org/HDF5/
30
6.4. DETERMINING THE SLIDING WINDOW LENGTH
6.3.2 Interpreting the output
Once we had the parameters fairly tuned we understood that there were information,
relevant to our experiments, that were missing in the machine learner’s output. This
led us to abort the experiments based on the machine learner and settle for a less
sophisticated approach (the plots shown in section 5) that could not be evaluated
by the quality measures (described in 2.6).
6.4 Determining the sliding window length
We hypothesised that the sequence information between sentences would matter in
plagiarism detection and wanted to test our hypothesis by experimenting with the
sliding window length. We wanted to to train our machine learner not only with the
extracted values from a single sentence but with values from the previous and next
sentence or sentences too. However this experiment were aborted when we chose to
not use the machine learner (as discussed in section 6.3).
6.5 Separation of features
We wanted to measure performance of the features as individual entities, as a for-
ward selection of individual entities, and combined features in different combina-
tions. However these experiments were abandoned due to limited time.
6.6 Number of folds
The machine learner, which was quite sophisticated, supported cross validation. We
had plans to test different folds using the cross validation support but these exper-
iments were aborted when we chose to not use the machine learner (as discussed in
section 6.3).
31

Chapter 7
Conclusions
In this chapter we present our conclusions based on the performed experiments.
7.1 Experiment 1: the word-space model
We find that the word-space model:
• is a good detector for no obfuscation;
• does not hold up for obfuscated materials, neither for low or high obfuscation
since it is based on the presence of each word in the text;
• and consequentially needs tuning so that specifically topical terms are weighted
up compared to less topical terms. This should be done specifically for the
topic in the candidate document being examined, since presumably the topic
under consideration is the most likely topic to be plagiarized.
7.2 Experiment 2: stylometry
We find that the stylometric space :
• which is a dramatic dimensionality reduction, unsurprisingly gives a large
number of false positives for all levels of obfuscation;
• gives a comparatively high precision even for a high level of obfuscation.
7.3 Out of the blue conclusions
While working on our experiments we came to several "Aha!"-moments where we
gained a deeper understanding of the problem and conclusions came out of the blue.
We understand that although our corpus has labels that determine which parts
of the text are plagiarism and which are not there could be more plagiarism that
33
CHAPTER 7. CONCLUSIONS
are yet to be discovered and might not even have been introduced by the artificial
plagiarism mechanism. Project Gutenberg can contain plagiarism that is not yet
detected and widely accepted by the community.
We understand that our stylometry approach is a bit shaky. What if a text is
written by several authors? How can one decide upon one style then?
34
Chapter 8
Future work: where to go from here
As mentioned in chapter 6 we faced a lot of obstacles during our experiments which
led to us aborting several of the experiments that we intended to perform. Some of
these experiments would still be very interesting to perform.
The decision to operate on the text’s sentence level should really be evaluated
more and therefore experiments were a plagiarism detection system is run on dif-
ferent text levels would be interesting.
We still feel that the use of a machine learner could produce in great results,
we think that there are patterns in the texts that are yet to be discovered. And
discovering patterns is one of the machine learning technique’s main purposes.
The experiments with the sliding window length (keeping track of previous and
future sentences also and not only the current sentence). It would be good to get
an understanding of how important the surrounding sentences are in plagiarism
detection.
The stylometric features could be more thorougly evaluated by measuring their
information content as individual features. A forward selection to find the best and
worse features would also be interesting.
Perhaps the chosen stylometric features were not representative or perhaps we
missed some features that could be of great importance. So a fresh selection of
stylometric features could be a good idea.
It would be interesting to perform experiments with rare pairs (when two words
appear together in a combination that very seldom appear in text) and collocations
(sequences of words that appear together more often than would be expected by
chance).
If one were to deal with the problem of citation (i.e. detecting quotes within
texts) that would be interesting and probably very useful to real world applications
of plagiarism detection.
35

Bibliography
Alberto Barrón-Cedeño and Paolo Rosso (2008). “Towards the Exploitation of Sta-
tistical Language Models for Plagiarism Detection with Reference”. In: Proceed-
ings of the ECAI’08 PAN Workshop: Uncovering Plagiarism, Authorship and
Social Software Misuse.
Alberto Barrón-Cedeño and Paolo Rosso and David Pinto and Alfons Juan (2008).
“On Cross-lingual Plagiarism Analysis Using a Statistical Model”. In: Proceed-
ings of the ECAI’08 PAN Workshop: Uncovering Plagiarism, Authorship and
Social Software Misuse.
Basile, Chiara et al. (2009). “A plagiarism detection procedure in three steps: se-
lection, matches and ’squares’”. In: SEPLN 2009 Workshop on Uncovering Pla-
giarism, Authorship, and Social Software Misuse (PAN 09).
Biber, Douglas (1988). Variation across Speech and Writing. Cambridge University
Press.
Bird, Steven, Ewan Klein, and Edward Loper (2009). Natural Language Process-
ing with Python: Analyzing Text with the Natural Language Toolkit. Beijing:
O’Reilly. isbn: 978-0-596-51649-9. url: http://www.nltk.org/book.
Croft, C., D. Metzler, and T. Strohman (2009). Search Engines: Information Re-
trieval in Practice. 1st ed. Addison Wesley. isbn: 0136072240.
Eissen, Sven Meyer zu, Benno Stein, and Marion Kulig (2006). “Plagiarism De-
tection Without Reference Collections.” In: GfKl. Ed. by Reinhold Decker and
Hans-Joachim Lenz. Studies in Classification, Data Analysis, and Knowledge
Organization. Springer, pp. 359–366. isbn: 978-3-540-70980-0.
Eissen, Sven Meyer Zu and Benno Stein (2006). “Intrinsic Plagiarism Detection”.
In: Proceedings of the European Conference on Information Retrieval (ECIR-06.
Grozea, C., C. Gehl, and M. Popescu (2009). “ENCOPLOT: Pairwise Sequence
Matching in Linear Time Applied to Plagiarism Detection”. In: SEPLN 2009
Workshop on Uncovering Plagiarism, Authorship, and Social Software Misuse
(PAN 09).
Gustafson, Nathaniel, Maria Soledad Pera, and Yiu-Kai Ng (2008). “Nowhere to
Hide: Finding Plagiarized Documents Based on Sentence Similarity”. In: WI-
IAT ’08: Proceedings of the 2008 IEEE/WIC/ACM International Conference
on Web Intelligence and Intelligent Agent Technology. Washington, DC, USA:
IEEE Computer Society, pp. 690–696. isbn: 978-0-7695-3496-1.
37
BIBLIOGRAPHY
Johnson, Aaron (2008). How MoreLikeThis Works in Lucene. url: http://cephas.
net/blog/2008/03/30/how-morelikethis-works-in-lucene/.
Kanerva, Pentti (1988). Sparse Distributed Memory. Cambridge, MA, USA: MIT
Press. isbn: 0262111322.
Karlgren, Jussi (2000). “Stylistic Experiments In Information Retrieval”. PhD the-
sis. Department of Linguistics, Stockholm University.
Kasprzak, Jan, Michal Brandejs, and Miroslav Kripac (2009). “Finding Plagiarism
by Evaluating Document Similarities”. In: SEPLN 2009 Workshop on Uncover-
ing Plagiarism, Authorship, and Social Software Misuse (PAN 09), p. 24.
Muhr, Markus et al. (2009). “External and Intrinsic Plagiarism Detection Using
Vector Space Models”. In: SEPLN 2009 Workshop on Uncovering Plagiarism,
Authorship, and Social Software Misuse (PAN 09), p. 47.
Potthast, M. et al. (2009). “Overview of the 1st International Competition on Pla-
giarism Detection”. In: SEPLN 2009 Workshop on Uncovering Plagiarism, Au-
thorship, and Social Software Misuse (PAN 09), pp. 1–9.
Sahlgren, Magnus (2006). “The Word-Space Model: Using distributional analysis
to represent syntagmatic and paradigmatic relations between words in high-
dimensional vector spaces”. PhD thesis. Department of Linguistics, Stockholm
University.
Shcherbinin, Vladislav and Sergey Butakov (2009). “Using Microsoft SQL Server
platform for plagiarism detection”. In: SEPLN 2009 Workshop on Uncovering
Plagiarism, Authorship, and Social Software Misuse (PAN 09), p. 36.
Stamatatos, Efstathios (2009). “Intrinsic Plagiarism Detection Using Character n-
gram Profiles”. In: SEPLN 2009 Workshop on Uncovering Plagiarism, Author-
ship, and Social Software Misuse (PAN 09), p. 38.
Stein, Benno and Sven Meyer zu Eissen (2007). “Intrinsic Plagiarism Analysis with
Meta Learning”. In: PAN.
Stein, Benno, Sven Meyer zu Eissen, and Martin Potthast (2007). “Strategies for
retrieving plagiarized documents”. In: SIGIR ’07: Proceedings of the 30th annual
international ACM SIGIR conference on Research and development in informa-
tion retrieval. ACM, pp. 825–826. isbn: 978-1-59593-597-7.
Täckström, Oscar et al. (2010). “Uncertainty Detection as Approximate Max-
Margin Sequence Labelling”. In: CoNLL 2010: Proceedings of the Fourteenth
Conference on Computational Natural Language Learning, July 2010, Uppsala,
Sweden.
Taskar, Ben, Carlos Guestrin, and Daphne Koller (2003). “Max-margin Markov
networks”. In: MIT Press.
Uzuner, Özlem, Boris Katz, and Thade Nahnsen (2005). “Using syntactic infor-
mation to identify plagiarism”. In: EdAppsNLP 05: Proceedings of the second
workshop on Building Educational Applications Using NLP. Ann Arbor, Michi-
gan: Association for Computational Linguistics, pp. 37–44.
Weber, Roger, Hans-Jörg Schek, and Stephen Blott (1998). “A Quantitative Analy-
sis and Performance Study for Similarity-Search Methods in High-Dimensional
Spaces”. In: VLDB ’98: Proceedings of the 24rd International Conference on
38
BIBLIOGRAPHY
Very Large Data Bases. San Francisco, CA, USA: Morgan Kaufmann Publish-
ers Inc., pp. 194–205. isbn: 1-55860-566-5.
39
TRITA-CSC-E 2011:109 
ISRN-KTH/CSC/E--11/109-SE 
ISSN-1653-5715 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
www.kth.se 
