Hindawi Publishing Corporation 
Advances in Multimedia 
 
 
Research article 
Text extraction from historical document images 
by the combination of several thresholding 
techniques 
Abderrahmane KEFALI
1
, Toufik Sari
1
, Halima Bahi
1
 
1 
LabGED Laboratory, 
Badji Mokhtar ‚Äì Annaba University, Algeria 
{kefali,sari,bahi}@labged.net 
 
 
This article presents a new technique for the binarisation of historical document images characterized by deteriorations and 
damages making their automatic processing difficult at several levels. The proposed method is based on hybrid thresholding 
in order to combines the advantages of global and local thresholding methods, and on the mixture of several binarization 
techniques. Two stages have been included. In the first stage, global thresholding is applied on the entire image and two 
different thresholds are determined from which the most of pixels are classified into foreground or background. In the second 
stage, the remaining pixels are assigned to foreground or background classes based on local analysis. In this stage, several 
local thresholding methods are combined and the final binary value of each remaining pixel is chosen as the most probable 
one. The proposed technique has been tested on a large collection of standard and synthetic documents and compared with 
well known methods using standard measures and showed powerful.  
 
1. Introduction 
Binarization is an important step in all process of 
document analysis and recognition. It has as goal to 
segment the image into two classes (foreground and 
background in the case of document images). The 
resulting image is a binary image in black and white 
where the black represents the foreground and the white 
represents the background. In fact, document image 
binarization is critical in the sense that bad separation 
will cause the loss of pertinent information and/or add 
useless information (noise), generating wrong results. 
This difficulty increases for old documents which have 
various types of damages and degradations from the 
digitization process itself, aging effects, humidity, 
marks, fungus, dirt, etc. making the automatic 
processing of these materials difficult at several levels.  
A great number of techniques have been proposed in 
the literature for the binarization of gray-scale or 
colored documents images, but, no one between them is 
generic and efficient for all types of documents.  
The binarization techniques of grayscale images 
may be classified into two categories: global 
thresholding and local thresholding [1] [2]. Another 
category of hybrid methods can be added [3]. The 
global thresholding methods are widely used in many 
document image analysis applications for their 
simplicity and efficiency. However, these methods are 
powerful only when the original documents are of good 
quality, well contrasted, have a clear bimodal pattern 
that separates foreground text and background. For 
historical document images which are generally noisy 
and of poor quality, global thresholding methods 
become not suitable, because no single threshold is able 
to completely separates the foreground from the 
background of the image since there is no sufficient 
distinction between the gray range of background and 
foreground pixels. This kind of document requires a 
more detailed analysis, which may be guaranteed by 
local methods. Local methods calculate a different 
threshold for each pixel based on the information of its 
neighborhoods. These methods are more robust against 
uneven illumination, low contrast and varying colors 
than global ones but they are very time consuming since 
a separate threshold is computed for each pixel of the 
image considering its neighborhoods. This calculation 
becomes slower when increasing the size of 
neighborhood considered. Hybrid methods combine 
global and local information for segmenting the image.  
In this paper, we propose a new hybrid thresholding 
technique for binarizing images of historical documents. 
The proposed approach uses a mixture of thresholding 
methods and it combines the advantages of the two 
families of techniques: execution rapidity and 
efficiency. The remainder of this paper is organized as 
follows. In Section 2, we present some existing 
binarization methods. Then in Section 3 we describe the 
2  Advances in Multimedia 
 
proposed approach. The experiments performed and the 
results will be shown in Section 4, before concluding.  
2. State of the Art  
According to [27], existing methodologies for 
image binarization may be divided under two main 
strategies: grouping based and thresholding based. 
Thresholding based methods use global or local 
threshold(s) to separate text from background. In 
Grouping based methods we distinguish two categories: 
Region based grouping methods, and Clustering based 
grouping methods. Region based grouping methods are 
mainly based on spatial-domain region growing, or on 
splitting and merging. Also, clustering based grouping 
methods are based on classification of intensity or color 
values in function of a homogeneity criterion. However 
several techniques have been employed to reach this 
classification: K-means algorithm, Artificial neural 
networks, etc.  
Sezgin et al. [4] established a classification of 
binarization methods according to the information that 
they exploit in 6 categories:  
- Histogram-based methods:  the methods of this class 
perform a thresholding based on the form of the 
histogram. 
- Clustering-based methods: These methods assign the 
image pixels to one of the two clusters: object and 
background.  
- Entropy-based methods: These algorithms use the 
information theory to obtain the threshold.   
- Object attribute-based methods: Find a threshold 
value based on some similarity measurements 
between original and binary images.  
- Spatial binarization methods: find the optimal 
threshold value taking into account spatial measures.  
- Locally adaptive methods: These methods are 
designed to give a new threshold for every pixel. 
Several kinds of adaptive methods exist. We find 
methods based on local gray range, local variation, 
etc. 
We present in this section some binarization 
methods the most frequently cited in the literature, and 
we consider only thresholding based methods. 
  
2.1. Global methods  
Noting I the grayscale image of which the intensities 
vary from 0 (black) to 1 (white), and H its histogram of 
intensities. The number of pixels having a gray level i  
is noted H(i).  
2.1.1. Otsu‚Äôs method  
Otsu‚Äôs method [5] tries to find the threshold T which 
separates the gray-level histogram in an optimal way 
into two segments (which maximize the inter-segments 
variance or which minimize the intra-segments 
variance). The calculation of the inter-classes or intra-
classes variances is based on the normalized histogram 
Hn= [Hn(0)‚Ä¶Hn(255)] of the image where ‚àëHn(i)=1. 
The inter-classes variance for each gray level t is 
given by: 
  
ÔÅõ ÔÅù22121int )()()()( tttqtqV er ÔÅ≠ÔÅ≠ ÔÄ≠ÔÇ¥ÔÇ¥ÔÄΩ  (1) 
Such as: ÔÉ•
ÔÄ≠
ÔÄΩ
ÔÇ¥ÔÄΩ
1
1
1 )(
)(
1
)(
t
ai
n iiH
tq
tÔÅ≠   
ÔÉ•
ÔÄΩ
ÔÇ¥ÔÄΩ
b
ti
n iiH
tq
t )(
)(
1
)(
2
2ÔÅ≠   
ÔÉ•
ÔÄ≠
ÔÄΩ
ÔÄΩ
1
1 )()(
t
ai
n iHtq
   
and   ÔÉ•
ÔÄΩ
ÔÄΩ
b
ti
n iHtq )()(2  
 
2.1.2. ISODATA method  
Thresholding using ISIDATA [6] consists to find a 
threshold by separating iteratively the gray-level 
histogram into two classes, with the apriority knowledge 
of the values associate to each class. This method starts 
by dividing the interval of non-null values of the 
histogram into two equidistant parts, and next we take 
m1 and m2 as the arithmetic average of each class. 
Repeat until convergence, the calculation of the optimal 
threshold T as the closest integer to (m1 + m2) / 2 and 
update the two averages m1  and m2  . 
 
2.1.3. Kapur et al.‚Äôs method 
Kapur et al.‚Äôs method [7] is an entropy based method 
which takes into account the foreground likelihood 
distribution Pf and the background likelihood 
distribution (Pb = 1 - Pf) in the determination of the 
division entropy. The binarization threshold T is chosen 
for which the value:  E =  Ef + Eb be maximal, such as:  
 
Ef=- 
Pi
Pf
t
i=0  √ólog 
Pi
Pf
         (2) 
Eb=- 
Pi
1-P
f
255
i=t+1  √ólog 
Pi
1-P
f
    (3) 
 
Where Pi is the occurrence probability of the gray 
level i in the image, and ùëÉùëì =  ùëÉùëñ
ùë°
ùëñ=0  
2.1.4. Iterative global thresholding (IGT) 
The proposed method selects a global threshold to the 
entire image based on an iterative procedure [19]. At 
each iteration i, the following steps are performed: 
a. Calculating the average gray level (Ti) of the image, 
b. Subtracting Ti from all pixels of the image, 
c. Histogram equalization to extend the pixels over the 
whole gray levels interval. 
The algorithm stops when: |Ti - Ti-1| < 0.001. 
 
2.2. Local methods  
Local methods compute a local threshold for each pixel 
by sliding a square or rectangular window over the 
entire image.  
Advances in Multimedia         3 
 
2.2.1. Bernsen‚Äôs method 
It is an adaptive local method [10]. Thus for each pixel 
of coordinates (x, y), the threshold is given by: 
 
2
),(
highlow ZZ
yxT
ÔÄ´
ÔÄΩ
 
 (4) 
 Such as Zlow and Zhigh are the lowest and the highest 
gray-level respectively in a squared window w √ó w 
centered over the pixel (x, y). 
However, if the local contrast C(x, y) = (Zhight ‚Äì Zlow) 
is below a threshold l (l = 15), then the neighborhood 
consists of a single class: Foreground or Background. 
2.2.2. Niblack‚Äôs method 
The local threshold T(x, y) is calculated using the mean 
m and standard deviation œÉ of all pixels in the window 
(neighborhood of the pixel in question) [11]. Thus, the 
threshold T(x, y) is given by:  
 
T(x,y) = m + k √ó œÉ.   (5) 
 
Such as k is a parameter used for determining the 
number of edge pixels considered as object pixels, and 
takes a negative values (k is fixed -0.2 by authors) . 
 
2.2.3. Sauvola and Pietikainen‚Äôs method 
Sauvola et al.‚Äôs algorithm [3] is a modification of that of 
Niblack, in order to gives more performance in the 
documents with a background containing a light texture 
or too variation and uneven illumination. In the 
modification of Sauvola, the local binarization threshold 
is given by:  
ÔÉ∑ÔÉ∑
ÔÉ∏
ÔÉ∂
ÔÉßÔÉß
ÔÉ®
ÔÉ¶
ÔÉ∑
ÔÉ∏
ÔÉ∂
ÔÉß
ÔÉ®
ÔÉ¶
ÔÄ≠ÔÇ¥ÔÄ≠ÔÇ¥ÔÄΩ
R
kmyxT
ÔÅ≥
11),(
  
(6) 
 
Where R is the dynamic range of the standard 
deviation œÉ, and the parameter k takes positives values 
in the interval [0.2, 0.5].  
2.2.4. Nick method  
This method improves considerably the binarization of 
lighted images and low contrasted images, by 
downwards moving, the threshold of binarization [2]. 
The threshold calculation is done as follow: 
ÔÄ® ÔÄ©
NP
mp
kmyxT
iÔÉ• ÔÄ≠
ÔÄ´ÔÄΩ
22
),(
 
        (7) 
Such as: k is the Niblack factor and vary between -
0.1 and -0.2 according to the application need, m: the 
average gray-level, pi: the gray-level of pixel i and NP is 
the total number of pixels. In their tests, the authors 
used a window of size 19 √ó 19. 
2.2.5. Sari et al.‚Äôs method 
This method uses an artificial neuron network of type 
multilayer Perceptron (MLP) to classify the image 
pixels into two classes: Foreground and background 
[18]. The MLP have one hidden layer, 25 inputs, and 
one single output. To assign a new value (black or 
white) to a pixel, the MLP takes as input a vector of 25 
values corresponding to the intensities of the pixel in a 
5√ó5 window centered on the processed pixel. The MLP 
parameters (structure, the input statistics, etc.) have 
been chosen after several experiments.  
2.3. Hybrid methods  
2.3.1. Improved IGT method 
This method [20] is an improvement of IGT technique 
[19] and it consists of two passes. In the first pass, a 
global thresholding is applied to the entire image, and in 
the second pass a local thresholding to areas still 
containing noise. To do this, the binary image resulting 
from global thresholding is divided into several 
segments of size n √ó n, and for each segment the 
frequency f of black pixels is calculated. The Segments 
satisfying the following criteria are kept: f > Œº + k √ó œÉ 
such as Œº and œÉ denote the mean and the standard 
deviation of the black pixels frequency in the segment, 
and k is a constant (equal to 2 according to the authors ). 
For each detected area, the IGT method is applied to the 
corresponding area in the original image. Areas of size 
50 √ó 50 give good results according to the authors. 
2.3.2. Gangamma et al.‚Äôs method 
Gangamma et al. [8] proposed a method based on a 
simple and effective combination of spatial filters with 
gray scale morphological operations to remove the 
background and improve the quality of historical 
document image of palm scripts. The first step of this 
technique is to apply adaptive histogram equalization 
(AHE) to overcome the problem of uneven illumination 
in the document image. On the resulting image, the 
morphological opening operation is applied and the 
opened image is added later with the histogram 
equalized image. After, the morphological closing 
operation is applied to the image in order to smooth it. 
The histogram equalized image is subtracted from the 
smoothed image and the result is subtracted again from 
the previous addition image. A Gaussian filter is 
subsequently applied in order to remove the noise. A 
final improvement is obtained by adding the last image 
with the histogram equalized image. Finally, a global 
thresholding (Otsu‚Äôs algorithm) is required to separate 
the text from the background. 
2.3.3. Background Subtraction thresholding 
This technique has been proposed in [9] and it consists 
of three steps. The background is modeled by removing 
the handwriting by applying a closing to the original 
image with a small disk as a structuring element. After 
that, the background is subtracted from the original 
image which only leaves the foreground. Finally, the 
resulting image is segmented using Otsu‚Äôs algorithm 
multiplied by an empirical constant. 
2.3.4. Tabatabaie et al.‚Äôs method 
It is a nonparametric method proposed for the 
binarization of bad illuminated document images [12]. 
In this method, the morphological closing operation is 
4  Advances in Multimedia 
 
used to solve the background uneven illumination 
problem. Indeed, closing may produce a reasonable 
estimate of the background if we use the appropriate 
structuring element. Experiments show that a 
structuring element of size equal to twice the stroke size 
gives the best results. The appropriate structuring 
element size is estimated as follows. A global threshold 
is first applied to the original image. Then we look for 
the size of the largest black square for each pixel and we 
save these values in a matrix M. The biggest value of M 
in each connected set of pixels is calculated and 
assigned to the other elements of the set. After that, the 
S-histogram (f) is made starting from the matrix M. The 
value of f at the point i is equal to the number of 
elements of M having the value i. Finally, we determine 
xmax the greatest value of x, with x satisfies: 
 f i xi=1 >0.02√ó f(i)
‚àû
i=1  and   f i =0
2x
i=x ) (8) 
The structuring element size will be 2 xmax.  
3. Proposed Technique  
As we said earlier, the global thresholding techniques 
are generally simple and fast algorithms which tend to 
calculate a single threshold in order to eliminate all 
background pixels, with preserving all foreground 
pixels. Unfortunately, these techniques are applicable 
only when the original documents are of good quality, 
well contrasted, with a bimodal histogram. Fig. 1 shows 
an example.  
 
(a) Gray-level image, 
 
 
(b) Its histogram, 
 
 
(c) Thresholding result with Otsu‚Äôs method 
 
Fig.1. Global thresholding of a bimodal image 
When the documents are of poor quality, containing 
different types of damage (stains, transparency effects, 
etc.), with a textured background and uneven 
illumination, or when the gray levels of the foreground 
pixels and the gray levels of the background pixels are 
close, it is not possible to find a threshold that 
completely separates the foreground from the 
background of the image (Fig. 2).  
 
  
(a) Original image 
  
  
          (b) Otsu‚Äôs thresholding result 
 
 
       
 
(c) Corresponding Histogram 
 
Fig. 2. Global thresholding of degraded image 
 
In this case, a more detailed analysis is needed, and 
we have recourse to local methods. Local methods are 
more accurate and may be applied to variable 
backgrounds, quite dark or with low contrast, but they 
are very slow since the threshold calculation based on 
the local neighborhood information is done for each 
pixel of the image. This calculation becomes slower 
with a high size of sliding window.  
To solve this problem, we propose a hybrid 
thresholding approach that will be fast and at the same 
time effective as well as local methods, and that by 
combining the advantages of both families of 
binarization methods. The proposed technique uses two 
thresholds T1 and T2, and it runs in two passes. In the 
first pass, a global thresholding is performed in order to 
Number of 
pixels 
133 
|| Gray levels 
|| 
 124 
Number of 
pixels 
Gray levels 
Advances in Multimedia         5 
 
class the most of pixels of the image. All pixels having a 
gray-level higher than T2 are removed (becomes white) 
because they represent the background pixels. All pixels 
having a gray-level lower than T1 are considered as 
foreground pixels and therefore they are kept and 
colored in black. The remaining pixels are left to the 
second pass in which they are locally binarized by 
combining the results of several local thresholding 
methods to select the most probable value.  
We details in the following the processing steps.  
3.1. Estimation of two thresholds T1 and T2  
The first step in the binarization process is the 
calculation of the two thresholds T1 and T2. Since the 
thresholding purpose is to divide the image into two 
classes: foreground and background, and since a single 
threshold is not able to accomplish this task, the use of 
two thresholds seems be a solution.  
 These two thresholds are estimated from the gray-
levels histogram of the original image and represent the 
average intensity of the foreground and background 
respectively.  
To obtain these two thresholds, we first compute a 
global threshold T using a global thresholding algorithm 
which can be Otsu‚Äôs algorithm, Kapur, or any other 
global algorithm. In our approach, we chose Otsu‚Äôs 
algorithm because this technique has shown its 
efficiency and overcome other global methods in 
several comparative studies [13] [15]. T separates the 
gray-levels histogram of the image into two classes: 
foreground and background.  
T1 and T2 are estimated from T. Noting dmin the 
minimum distance between the average intensity of the 
foreground represented by the first half of the histogram 
and the average intensity of the background represented 
by the second half. 
2
min
1
d
TT ÔÄ≠ÔÄΩ   (9) 
2
min
2
d
TT ÔÄ´ÔÄΩ   (10) 
 
3.2. Global image thresholding using T1 and T2  
After the estimation of the two thresholds T1 and T2, all 
pixels having a gray-level higher than T2 are 
transformed into white which eliminates most of the 
image background, and those whose gray level is less 
than T1 are colored in black. Noting I the resulting 
image. These pixels are certainly foreground pixels. The 
resulting image still contains noise but all the 
foreground information are preserved.  
3.3. Local thresholding of the remaining pixels  
The pixels unprocessed in the previous step (those with 
a gray level between T1 and T2) may belong to the 
foreground, and they must be preserved, likewise they 
may be background or noise pixels and in these both 
cases they must be eliminated. The decision to assign 
the remaining pixels to one of two classes: foreground 
or background is performed using a local process by 
examining the neighborhood of these pixels. To ensure 
a more correct classification, we proposed to apply 
several local thresholding methods. In our experiments, 
we chose the following methods: Niblack, Sauvola and 
Nick, since these methods were ranked in first places in 
several previous comparative studies [2] [16] [17].  
For each pixel (x, y) of I not yet classified, we 
calculate locally its new binary values (0 for black and 1 
for white) obtained by applying Niblack‚Äôs, Sauvola‚Äôs, 
and Nick methods and we obtain thus three temporary 
images I1, I2, and I3 respectively. Each one of the three 
local methods computes the binary value of each 
remaining pixel (x, y) by: 
ÔÉÆ
ÔÉ≠
ÔÉ¨ ÔÄº
ÔÄΩ
             otherwise      255,
y)(x,LTy)I(x,  if         0,
yxI
i
i ),( ,  1 ‚â§ i ‚â§ 3    (11) 
With LT1, LT2, LT3 are the local threshold computed 
using Niblack‚Äôs, Sauvola‚Äôs, and Nick methods 
respectively. 
The final binary value Ib(x,y) of each remaining 
pixel is that resulted of at least two of the three methods.  
ÔÉØ
ÔÉÆ
ÔÉØ
ÔÉ≠
ÔÉ¨
ÔÄº
ÔÄΩ ÔÉ•
ÔÄΩ
        otherwise        
yxI   if    0,
yxI
i
i
b
,1
2),(
),(
3
1
 (12) 
 
4. Experiments and Results  
Experiments have been performed in order to estimate 
the performance of our approach. We applied the 
proposed technique over a test set and compared the 
obtained results with well known methods, including 
global, local, and hybrid methods. The comparison 
concerns both the binarization quality and the execution 
time.  
For the parameterized methods, a series of 
experiments have been performed first in order to set 
their optimal parameter values. Noting PS(m) the 
parameters set of a specific thresholding method m. For 
example PS(Niblack)={k, w},  PS(Sauvola)={k, w, R}, 
etc. We seek to find the optimal values of PS(m) with 
which the binarization results are closest to the ground 
truth images. A specific range of values [a, b] is first 
defined for each parameter. To improve the accuracy of 
the process, we used a wide initial range for every 
parameter. For Niblack method per example, the range 
of the parameter w is defined as [a, b]=[3, 299]. After 
that, we apply the binarization method m with the 
different values of PS(m) from the pre-determinate 
ranges on the test set described in section (4.1). We 
compare the binarization results with the ground truth 
images using the evaluation measures detailed in section 
(4.2). A ranking of the obtained results is then 
performed according to each measure separately. By 
calculating the sum of the ranks, we can infer the 
optimal set of parameter values, as the one with which 
the obtained results are ranked first. The optimal 
parameter values of the parameterized methods are 
summarized in Table 1. 
6  Advances in Multimedia 
 
Table 1. Optimal parameters values of the parameterized 
methods 
Binarization technique Optimal Parameter values 
Niblack k= -0.2 and w=35 
Sauvola and Pietikainen k= 0.2, R= 128 and w=27 
Nick k=-0.1 and w=19 
Improved IGT n=50 and k=3 
4.1. Test base 
Two test sets have been used for the evaluation of the 
proposed method. The first is a public set composed of 
document images from the four collections proposed 
within the context of the competitions DIBCO 2009
1
, 
H-DIBCO 2010
2
, DIBCO 2011
3
 and H-DIBCO 2012
4
. 
These four collections contain a total of 50 real 
documents images (37 handwritten and 13 printed) 
coming from the collections of several libraries, with 
the associated ground truth images. All the images 
contain representative degradations which appear 
frequently (e.g. variable background intensity, shadows, 
smear, smudge, low contrast, bleed-through).  Fig. 3 
shows some images from these collections. 
 The second set of images is a synthetic collection 
composed of 150 synthetic images of documents 
constructed by the fusion of 15 different backgrounds 
and 10 binary images (Fig. 4). The fusion is done by 
applying the image mosaicing by superimposing 
technique for blending [25]. The idea is as follow, we 
start with some images of documents in black & white, 
which represent the ground truth, and with some 
backgrounds extracted from old documents and we 
apply a fusion procedure to get as many different 
images of old documents. However, P. Stathis et al., in 
[26] proposed two different techniques for the blending: 
maximum intensity and image averaging. We adopt to 
use the image averaging technique in order to have a 
more natural result.   
 
             (a) 
 
          (b) 
 
               (c)               (d) 
Fig. 3. Images taken from the collections of: (a) DIBCO 2009, 
(b) H-DIBCO 2010, (c) DIBCO 2011, (d) H-DIBCO 2012 
                                                          
1 http://users.iit.demokritos.gr/Àúbgat/DIBCO2009/benchmark/ 
2 http://www.iit.demokritos.gr/Àúbgat/H-DIBCO2010/benchmark 
3 http://utopia.duth.gr/~ipratika/DIBCO2011/benchmark 
4 http://utopia.duth.gr/~ipratika/HDIBCO2012/benchmark 
 
(a) (b) 
 
(c) 
Fig. 4. Image of document obtained by fusing binary image 
and background. (a) background from old document, (b) 
ground truth binary image, (c) the resulting synthetic image 
 
4.2. Evaluation measures  
In addition to the execution time, the qualitative 
assessment is performed in terms of five standard 
evaluation measures used in DIBCO 2009, H-DIBCO 
2010, DIBCO 2011, and H-DIBCO 2012 which are: F-
measure, PSNR, NRM, MPM, and DRD.  
Noting TP, TN, FP, FN the True positive, True 
Negative, False positive and False negative values, 
respectively.  
 
a) F-Measure  
F-Measure was introduced first by Chinchor in [21]. 
PrecisionRecall
PrecisionRecall2
FMeasure
ÔÄ´
ÔÇ¥ÔÇ¥
ÔÄΩ      (13) 
 
Where: 
FNTP
TP
Recall
ÔÄ´
ÔÄΩ  and 
FPTP
TP
recisionP
ÔÄ´
ÔÄΩ  
 
b) PSNR  
PSNR is a similarity measure between two images. 
However, the higher the value of PSNR, the higher the 
similarity of the two images [22][23].  
ÔÉ∑ÔÉ∑
ÔÉ∏
ÔÉ∂
ÔÉßÔÉß
ÔÉ®
ÔÉ¶
ÔÄΩ
MSE
C
PSNR
2
log.10
 
 (14) 
Where : 
ÔÄ® ÔÄ©
MN
yxIyxI
MSE
M
x
N
y
ÔÉ•ÔÉ•
ÔÄΩ ÔÄΩ
ÔÄ≠
ÔÄΩ
1 1
2
2 ),(),(
 
I and I2 represents the two images matched. M and N 
are there height and width respectively. C is the 
difference between foreground and background. 
 
c) NRM (Negative Metric rate)  
NRM is based on the pixel-wise mismatches between 
the Ground Truth and the binarized image [24]. It 
combines the false negative rate NRFN and the false 
positive rate NRFP. It is denoted as follows: 
 
 
2
FPFN NRNRNRM
ÔÄ´
ÔÄΩ
 
 (15) 
Advances in Multimedia         7 
 
 
With : 
TPFN
FN
NRFN
ÔÄ´
ÔÄΩ   and  
TNFP
FP
NRFP
ÔÄ´
ÔÄΩ  
Contrary to Fmeasure and PSNR, The better 
binarization quality is obtained for lower NRM value. 
 
d) MPM (Misclassification Penalty Metric) 
The Misclassification penalty metric MPM evaluates the 
binarization result against the Ground Truth on an 
object-by-object basis [24]. 
 
2
FPFN MPMPMPM
ÔÄ´
ÔÄΩ   (16)
 
Where: 
D
d
MP
FN
i
i
FN
FN
ÔÉ•
ÔÄΩÔÄΩ 1    and  
D
d
MP
FP
j
j
FP
FP
ÔÉ•
ÔÄΩ
ÔÄΩ
1  
i
FNd and 
j
FPd denote the distance of the i
th
  false 
negative and the j
th
  false positive pixel from the contour 
of the Ground Truth segmentation. The normalization 
factor D is the sum over all the pixel-to-contour 
distances of the Ground Truth object. A low MPM score 
denotes that the algorithm is good at identifying an 
object‚Äôs boundary. 
 
e) DRD (Distance Reciprocal Distortion Metric) 
DRD is an objective distortion measure for binary 
document images, and it was proposed by Lu et al. in 
[23]. This measure properly correlates with the human 
visual perception and it measures the distortion for all 
the S flipped pixels as follows: 
NUBN
DRD
DRD
S
k
kÔÉ•
ÔÄΩÔÄΩ 1 . 
(17) 
NUBN is the number of the non-uniform 8√ó8 blocks in 
the GT image. 
DRDk is the distortion of the k
th
 flipped pixel of 
coordinate (x, y) and it is calculated using a 5√ó5 
normalized weight matrix WNm. This last is defined in 
[23] as follow: 
 
ÔÉ•ÔÉ•
ÔÄΩ ÔÄΩ
ÔÄΩ
m
i
m
j
m
m
Nm
jiW
jiW
jiW
1 1
),(
),(
),(  
Such as:  
ÔÉØ
ÔÉÆ
ÔÉØ
ÔÉ≠
ÔÉ¨
ÔÄ≠ÔÄ´ÔÄ≠
ÔÄΩÔÄΩ
ÔÄΩ            otherwise.  ,
)()(
1
 and for                                 ,0
),(
22
C
CC
C
m
jjii
jjii
jiW
With  m = 5, and iC = jC = (1 + m) / 2. 
 
DRDk is given as follow : 
 
 
4.3. Results and discussion  
The average results obtained over the test images are 
summarized in the following Table 2. The final ranking 
of the compared methods is shown in Table 3, which 
also summarizes the partial ranks of each method 
according to each evaluation measure and the sum of 
ranks. 
 
Table 2. Average results obtained on the test dataset 
 
 Execution 
time  (ms) 
F-mesure 
(%) 
PSNR NRM 
(√ó10-2) 
MPM  
(√ó10-3) 
DRD 
Global 
Otsu 458,75 
80,565 15,716 
8.83 
2.961 
26.208 
Kapur et al. 
520,828 81,921 15,692 
8.109 2.059 23.104 
IGT 309,543 
76,556 14,307 
10.11 2.778 28.73 
Local 
Niblack 78504.1 66.88 7.78 11.47 18.239 43.42 
Sauvola and Pietikainen 73914.2 85.68 18.62 7.95 1.254 6.13 
Nick 72480.9 82.22 19.047 9.31 
2.201 
5.27 
Hybrid 
Improved IGT 
487,943 78,039 14,728 12.05 3.72 22.78 
Gangamma et al. 
73999,157 69,187 13,469 21.71 15.43 38.81 
Background Subtraction 
thresholding 
1122,057 68,907 13,432 22.34 17.75 40.97 
Tabatabaie and al. 75875 85.82 26.69 8.2 1.183 3.439 
Proposed Approche  1417.86 87.44 27.97 6.74 1.025 3.161 
 
 
 
ÔÉ•ÔÉ•
ÔÄ≠ÔÄΩ ÔÄ≠ÔÄΩ
ÔÄ´ÔÄ´ÔÇ¥ÔÄ≠ÔÄ´ÔÄ´ÔÄΩ
2
2
2
2
)2,2(),(),(
i j
NmBGTk jiWyxIjyixIDRD
8  Advances in Multimedia 
 
Table 3. Final ranking of the compared methods on the test set 
Rank Method Execution 
time  (ms) 
F-mesure 
(%) 
PSNR NRM 
(√ó10-2) 
MPM 
(√ó10-3) 
DRD Sum of 
ranks 
1 Proposed Approche 6 1 1 1 1 1 11 
2 Tabatabaie and al. 10 2 2 4 2 2 22 
3 Sauvola and Pietikainen 8 3 4 2 3 4 24 
4 Kapur et al. 4 5 6 3 4 6 28 
5 Nick 7 4 3 6 5 3 28 
6 Otsu 2 6 5 5 7 7 32 
7 IGT 1 8 8 7 6 8 38 
8 Improved IGT 3 7 7 9 8 5 39 
9 Gangamma et al. 9 9 9 10 9 9 55 
10 Background Subtraction thresholding 5 10 10 11 10 10 56 
11 Niblack 11 11 11 8 11 11 63 
 
From the above tables, it is shown that our 
proposed method is ranked first in totally and it has the 
best performance according to all measures of 
binarization quality. It exceeded local methods such as 
the famous Sauvola and Pietikainen‚Äôs method, which 
ranked 3
ed
 in our experiments, and even other hybrid 
techniques. Indeed, the combination of three local 
thresholding techniques has enabled a more robust 
determination of the binary value of each pixel by 
choosing the most likely value.    
Regarding the execution time, our method is very 
fast compared to local methods (about 52 times than 
Sauvola and Pietikainen‚Äôs method), which enabled us to 
earn about than 98% of the execution time. This is 
logical because only a portion of pixels (having a gray 
level between the two thresholds T1 and T2) is processed 
locally. 
 
5. Conclusion  
In this paper we tackled the problem of foreground/ 
background separation from the images of historical 
documents. We proposed a hybrid approach of degraded 
document images binarization. The proposed approach 
runs on two passes. Firstly, a global thresholding using 
Otsu‚Äôs algorithm is applied on the entire image, and two 
different thresholds are determined. All pixels bellow 
the first threshold are preserved, and all pixels higher 
than the second threshold are eliminated as they 
represent background pixels. The remaining pixels are 
then processed locally based on their neighborhood 
information. In this step, three local thresholding 
methods are combined in order to obtain a more 
accurate decision. Since the number of pixels processed 
locally is very small compared to the total number of 
pixels, the time required for the binarization is reduced 
considerably without reducing the performance. To 
validate our approach, we compared it with methods in 
the literature and the results obtained on a standard and 
synthetic test collections are encouraging and confirm 
our proposal. 
References 
[1] N. Arica, and F. T. Yarman-Vural, An overview of 
character recognition focused on off-line 
handwriting, IEEE transactions on systems, man 
and cybernetics - part C : Applications and 
reviews, Vol. 31, No. 2, 2001, pp. 216 - 233. 
[2] K. Khurshid, I. Siddiqi, C. Faure, and N. Vincent, 
Comparison of Niblack inspired Binarization 
methods for ancient documents, Proceedings of 
16
th
 International conference on Document 
Recognition and Retrieval, USA, 2009. 
[3] J. Sauvola, and M. Pietikainen, Adaptive document 
image binarization, Pattern Recognition, Vol. 33, 
No. 2, 2000, pp. 225 ‚Äì 236. 
[4] M. Sezgin, and B. Sankur, Survey over image 
thresholding techniques and quantitative 
performance evaluation, Journal of Electronic 
Imaging, Vol. 13, No. 1, 2004, pp. 146 - 165. 
[5] N. Otsu, A Threshold Selection Method from 
Gray-Level Histograms, IEEE transactions on 
Systems, Man and Cybernetics, Vol. 9, No. 1, 
1979, pp. 62 -66. 
[6] F.R.D. Velasco, Thresholding using the ISODATA 
clustering algorithm, IEEE Transaction on system, 
Man and Cybernitics, Vol. 10, 1980, pp. 771 - 774. 
[7] J.N. Kapur, P.K. Sahoo, and A.K.C. Wong, A New 
method for gray-level picture threshold using the 
entropy of the histogram, Computer Vision, 
Graphics, and Image Processing, Vol. 29, 1985, 
pp. 273 - 285. 
[8] B. Gangamma, S.M. K, Enhancement of Degraded 
Historical Kannada Documents,  International 
Journal of Computer Applications, Vol. 29, No.11, 
2011, pp. 1 - 6.  
Advances in Multimedia         9 
 
[9] G. Leedham, C. Yan, K. Takru, J.H.N. Tan, L. 
Mian, Comparison of Some Thresholding 
Algorithms for Text/Background Segmentation in 
Difficult Document Images, Proceedings of the 
Seventh International Conference on Document 
Analysis and Recognition, 2003, pp. 859-864. 
[10] J. Bernsen, Dynamic thresholding of grey-level 
images, Proceedings of 8
th
 International 
Conference on Pattern Recognition, Paris 
(France), 1986, pp. 1251 - 1255.  
[11] W. Niblack, An Introduction to Digital Image 
Processing, Ed. Prentice Hall, Englewood Cliffs, 
1986. 
[12] S.A. Tabatabaei, M. Bohlool, Novel method for 
binarization of badly  illuminated document 
images, Proceedings of  IEEE 17
th
 International 
Conference on Image Processing, Hong Kong, 
2010, pp.3573-3576.  
[13] P. K.  Sahoo,  S.  Soltani,  and  A. K.  C. Wong, A  
Survey  of  Thresholding  Techniques, Computer  
Vision,  Graphics,  and  Image  Processing, Vol. 
41,1988, pp. 233 - 260. 
[14] J. He, Q. D. M. Do, A. C. Downton, and J. H. 
Kim, A comparison of binarization methods for 
historical archive documents, Proceedings of 
International Conference on Document Analysis 
and Recognition, 2005, pp. 538 - 542. 
[15] A. Marte, P. Ramirez and R. Rojas, Unsupervised 
Evaluation Methods Based on Local Gray-
Intensity Variances for Binarization of Historical 
Documents, Proceedings of 20
th
 International 
Conference on Pattern Recognition, IEEE 
Computer Society, Istambul (Turkey), 2010, pp. 
2029 ‚Äì 2032.  
[16] A.J.O. Trier, Goal-directed evaluation of 
binarization methods, IEEE Transactions on 
Pattern Analysis and Machine Intelligence, Vol. 
17, No. 12, 1995, pp. 1191 - 1201. 
[17] A. Kefali, T. Sari, and M. Sellami, Evaluation of 
several binarization techniques for old Arabic 
documents images, Proceedings of International 
symposium on modelling and implementation of 
complex systems, Constantine (Algeria), 2010. 
[18] T. Sari, A.Kefali, and H.Bahi, An MLP for 
binarizing images of old manuscripts, Proceedings 
of International Conference on Frontiers in 
Handwriting Recognition, Bari (Italy), 2012, pp. 
247 - 251. 
[19] E. Kavallieratou, A Binarization Algorithm 
Specialized on Document images and Photos, 
Proceedings of 8
th
 International Conference on 
Document Analysis and Recognition, 2005, pp. 
463 - 467. 
[20] E. Kavallieratou, S. Stathis, Adaptive Binarization 
of Historical Document Images, Proceedings of 
18
th
 Conference on pattern recognition, 2006, pp. 
742-745. 
[21] N. Chinchor, MUC-4 Evaluation Metrics, 
Proceedings of the Fourth Message Understanding 
Conference, 1992, pp. 22‚Äì29.  
[22] B. Gatos, K. Ntirogiannis, I. Pratikakis, DIBCO 
2009: document image binarization contest, 
International Journal of Document Analysis and 
Recognition, Vol.14, No. 1, 2009, pp. 35-44.  
[23] H. Lu, A.C. Kot, Y.Q. Shi, Distance-Reciprocal 
Distortion Measure for Binary Document Images, 
IEEE Signal Processing Letters, Vol. 11, No. 2, 
2004, pp. 228-231.  
[24] J. Aguilera, H. Wildenauer, M. Kampel, M. Borg, 
D. Thirde, J. Ferryman, Evaluation of motion 
segmentation quality for aircraft activity 
surveillance, Proceedings of the Joint IEEE 
International Workshop on Visual Surveillance 
and Performance Evaluation of Tracking and 
Surveillance VS-PETS, 2005, pp. 317-324. 
[25] L.G. Brown, A survey of Image Registration 
Techniques, ACM Computing Surveys, Vol. 24, 
No. 4, 1992, pp. 325-376. 
[26] P. Stathis, E. Kavallieratou, N. Papamarkos, An 
Evaluation Survey of Binarization Algorithms on 
Historical Documents, Proceedings of the 19th 
International Conference on Pattern Recognition, 
2008, pp. 742-745.  
[27] B. Fernando, S. Karaoglu, S, Extreme Value 
Theory Based Text Binarization in Documents and 
Natural Scenes, Proceedings of the 3rd   
International Conference on Machine Vision 2010, 
pp. 144-151. 
 
 
Disclosure Policy 
The authors declare that there is no conflict of interests 
regarding the publication of this article 
