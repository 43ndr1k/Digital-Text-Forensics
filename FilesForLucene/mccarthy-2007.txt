7
Textual Signatures: Identifying Text-Types
Using Latent Semantic Analysis to Measure
the Cohesion of Text Structures
Philip M. McCarthy, Stephen W. Briner, Vasile Rus, and
Danielle S. McNamara
7.1 Introduction
Just as a sentence is far more than a mere concatenation of words, a text is far
more than a mere concatenation of sentences. Texts contain pertinent information
that co-refers across sentences and paragraphs [30]; texts contain relations between
phrases, clauses, and sentences that are often causally linked [21, 51, 56]; and texts
that depend on relating a series of chronological events contain temporal features
that help the reader to build a coherent representation of the text [19, 55]. We re-
fer to textual features such as these as cohesive elements, and they occur within
paragraphs (locally), across paragraphs (globally), and in forms such as referential,
causal, temporal, and structural [18, 22, 36]. But cohesive elements, and by conse-
quence cohesion, does not simply feature in a text as dialogues tend to feature in
narratives, or as cartoons tend to feature in newspapers. That is, cohesion is not
present or absent in a binary or optional sense. Instead, cohesion in text exists on
a continuum of presence, which is sometimes indicative of the text-type in ques-
tion [12, 37, 41] and sometimes indicative of the audience for which the text was
written [44, 47]. In this chapter, we discuss the nature and importance of cohesion;
we demonstrate a computational tool that measures cohesion; and, most impor-
tantly, we demonstrate a novel approach to identifying text-types by incorporating
contrasting rates of cohesion.
7.2 Cohesion
Recent research in text processing has emphasized the importance of the cohesion of
a text in comprehension [5, 44, 43]. Cohesion is the degree to which ideas in the text
are explicitly related to each other and facilitate a unified situation model for the
reader. As McNamara and colleagues have shown, challenging text (such as science)
is particularly difficult for low-knowledge students. These students are cognitively
burdened when they are forced to make inferences across texts [22, 34, 35, 38, 44].
Adding cohesion to text alleviates this burden by filling conceptual and structural
gaps. Recent developments in computational linguistics and discourse processing
have now made it possible to measure this textual cohesion. These developments
108 Philip M. McCarthy et al.
have come together in a computational tool called Coh-Metrix [22] that approxi-
mates over 200 indices of textual cohesion and difficulty. Armed with this technology,
text-book writers and teachers have the opportunity to better assess the appropri-
ateness of a text for particular students [47], and researchers have the opportunity
to assess cohesion patterns in text-types so as to better understand what consti-
tutes a prototypical text from any given domain, genre, register, or even author
[37, 42, 12, 14].
7.3 Coh-Metrix
Coh-Metrix assesses characteristics of texts by using parts of speech classifiers
[4, 49, 7, 8, 9, 10, 11], and latent semantic analysis [32, 33]. The indices gener-
ated from Coh-Metrix offer an assessment of the cohesiveness and readability of any
given text . These indices have been used to indicate textual cohesion and difficulty
levels in a variety of studies. For example, Ozuru et al. [47] used Coh-Metrix to rate
high and low cohesion versions of biology texts, the study showing that participants
benefited most from the high cohesion versions. And Best, Floyd, and McNamara
[1] used Coh-Metrix to compare 61 third-graders’ reading comprehension for nar-
rative and expository texts, the study suggesting that children with low levels of
world knowledge were more inclined to have comprehension problems with expos-
itory texts. While research into assessing the benefits of cohesion continues apace,
the utility of the Coh-Metrix tool has also allowed the pursuit of other avenues of
textual investigation.
One of these alternative avenues is text identification. For example, Louwerse
et al. [37] used Coh-Metrix to investigate variations in cohesion across written and
spoken texts, finding evidence for a significant difference between these modes. Mc-
Carthy, Lewis, et al. [42] showed that Coh-Metrix indices were versatile enough to
distinguish between authors even within the same register. Crossley et al.[12] used a
wide variety of Coh-Metrix indices to show significant differences between authentic
English language texts and the simplified versions used in texts designed for En-
glish language learners. And McCarthy, Lightman et al. [41] used Coh-Metrix to
investigate variations in cohesion and difficulty across units of science and history
texts, finding evidence that while difficulty scores for textbooks reflect the grade to
which they are assigned, the cohesion rates differed significantly depending upon
domain. In this chapter, we build on these approaches to identification of textual
characteristics by demonstrating how cohesion indices produced by Coh-Metrix can
be used to form prototypical models of text-types that we call textual signatures.
7.4 Approaches to Analyzing Texts
Traditional approaches to categorizing discourse have tended to treat text as if it
were a homogeneous whole. These wholes, or bodies of text, are analyzed for various
textual features, which are used to classify the texts as belonging to one category or
another [2, 3, 26, 27, 37, 42]. To be sure, such approaches have yielded impressive
findings, generally managing to significantly discriminate texts into categories such
as dialect, domain, genre, or author. Such discrimination is made possible because
7 Identifying Text-Types Using Latent Semantic Analysis 109
different kinds of texts feature different quantities of features. For example, Biber
[2] identified if clauses and singular person pronoun use as key predictors in distin-
guishing British- from American-English. Louwerse et al. [37] used cohesion scores
generated from Coh-Metrix to distinguish both spoken from written texts and nar-
ratives from non-narratives. And Stamatatos, Fakotatos, and Kokkinakis [50] used a
number of style markers including punctuation features and frequencies of verb- and
noun-phrases to distinguish between the authors of a variety of newspaper columns.
Clearly, discriminating texts by treating them as homogenous wholes has a good
track record. However, texts tend to be heterogeneous, and treating them as such
may substantially increase the power of corpus analyses.
The parts of a text serve the textual whole either by function or by form. In
terms of function, Propp [48] identified that texts can be comprised of fundamental
components, fulfilled by various characters, performing set functions. In terms of
form, numerous theories of text structure have demonstrated how textual elements
are inter-related [24, 25, 31, 40]. Labov’s narrative theory, to take one example,
featured six key components: the abstract (a summary), the orientation (the cast of
characters, the scene, and the setting), the action (the problem, issue, or action), the
evaluation (the story’s significance), the resolution (what happens, the denouement),
and the coda (tying up lose ends, moving to the present time and situation).
Unfortunately for text researchers, the identification of the kinds of discourse
markers described above has proven problematic because the absence or ambiguity of
such textual markers tends to lead to limited success [39]. This is not to say that there
has been no success at all. Morris and Hirst [46], for example, developed an algorithm
that attempted to uncover a hierarchical structure of discourse based on lexical
chains. Although their algorithm was only manually tested, the evidence from their
study, suggesting that text is structurally identifiable through themes marked by
chains of similar words, supports the view that the elements of heterogeneous texts
are identifiable. Hearst [23] developed this idea further by attempting to segment
expository texts into topically related parts. Like Morris and Hirst [46], Hearst
used term repetition as an indicator of topically related parts. The output of his
method is a linear succession of topics, with topics able to extend over more than
one paragraph. Hearst’s algorithm is fully implementable and was also tested on
magazine articles and against human judgments with reported precision and recall
measures in the 60th percentile, meaning around 60% of topic boundaries identified
in the text are correct (precision) and 60% of the true boundaries are identified
(recall).
The limited success in identifying textual segments may be the result of searching
for a reliable fine grained analysis before a courser grain has first been established.
For example, a courser approach acknowledges that texts have easily identifiable
beginnings, middles, and ends, and these parts of a text , or at least a sample from
them, are not at all difficult to locate. Indeed, textual analysis using such parts
has proved quite productive. For example, Burrows [6] found that the introduction
section of texts rather than texts as a whole allowed certain authorship to be signifi-
cantly distinguished. And McCarthy, Lightman et al.[41] divided high-school science
and history textbook chapters into sections of beginnings, middles, and ends, finding
that reading difficulty scores rose with significant regularity across these sections as
a chapter progressed.
If we accept that texts are comprised of parts, and that the text (as a whole)
is dependent upon the presence of each part, then we can form the hypothesis that
110 Philip M. McCarthy et al.
the parts of the text are inter-dependent and, therefore, are likely to be structurally
inter-related. In addition, as we know that cohesion exists in texts at the clausal,
sentential, and paragraph level [22], it would be no surprise to find that cohesion
also existed across the parts of the text that constitute the whole of the text . If this
were not the case, parts of text would have to exist that bore no reference to the
text as a whole. Therefore, if we measure the cohesion that exists across identifiable
parts of the text, we can predict the degree to which the parts co-refer would be
indicative of the kind of text being analyzed. In Labov’s [31] narrative model, for
example, we might expect a high degree of coreference between the second section
(the orientation) and the sixth section (the coda): Although the two sections are
textually distant, they are semantically related in terms of the textual elements with
both sections likely to feature the characters, the motive of the story, and the scene
in which the story takes place. In contrast, we might expect less coreference between
the forth and fifth sections (evaluation and resolution): While the evaluation and
resolution are textually juxtaposed, the evaluation section is likely to offer a more
global, moral and/or abstracted perspective of the story. The resolution, however, is
almost bound to be local to the story and feature the characters, the scene, and the
outcome. Consequently, semantic relations between these two elements are likely to
be less marked.
By forming a picture of the degree to which textual parts inter-relate, we can
build a representation of the structure of the texts, a prototypical model that we
call the textual signature. Such a signature stands to serve students and researchers
alike. For students, their work can be analyzed to see the extent to which their paper
reflects a prototypical model. Specifically, a parts analysis may help students to see
that sections of their papers are under- or over-represented in terms of the global
cohesion. For researchers, a text-type signature should help significantly in mining
for appropriate texts. For example, the first ten web sites from a Google search for a
text about cohesion (featuring the combined keywords of comprehension, cohesion,
coherence, and referential) yielded papers from the field of composition theory, En-
glish as a foreign language, and cognitive science, not to mention a disparate array
of far less academic sources. While the specified keywords that were entered may
have occurred in each of the retrieved items, the organization of the parts of the
retrieved papers (and their inter-relatedness) would differ. Knowing the signatures
that distinguishes the text types would help researchers to locate more effectively
the kind of resources that they require. A further possible benefit of textual signa-
tures involves Question Answering (QA) systems [45, 52]. Given a question and a
large collection of texts (often in gigabytes), the task in QA is to draw a list of short
answers (the length of a sentence) to the question from the collection. The typical
architecture of a modern QA system includes three subsystems: question process-
ing, paragraph retrieval and answer processing. Textual signatures may be able to
reduce the search space in the paragraph retrieval stage by identifying more likely
candidates.
7.5 Latent Semantic Analysis
To assess the inter-relatedness of text sections we used latent semantic analysis
(hereafter, LSA ). An extensive review of the procedures and computations involved
in LSA is available in Landauer and Dumais [32] and Landauer et al. [33]. For this
7 Identifying Text-Types Using Latent Semantic Analysis 111
chapter, however, we offer only an overview of the theory of LSA, its method of
calculations, and a summary of some of the many studies that have incorporated its
approach.
LSA is a technique that uses a large corpus of texts together with singular value
decomposition to derive a representation of world knowledge [33]. LSA is based
on the idea that any word (or group of words) appears in some contexts but not
in others. Thus, words can be compared by the aggregate of their co-occurrences.
This aggregate serves to determine the degree of similarity between such words [13].
LSA’s practical advantage over shallow word overlap measures is that it goes beyond
lexical similarities such as chair/chairs or run/ran, and manages to rate the relative
semantic similarity between terms such as chair/table, table/wood, and wood/forest.
As such, LSA does not only tell us whether two items are the same, it tells us how
similar they are. Further, as Wolfe and Goldman [54] report, there is substantial
evidence to support the notion that the reliability of LSA is not significantly different
from human raters when asked to perform the same judgments.
As a measure of semantic relatedness, LSA has proven to be a useful tool in
a variety of studies. These include computing ratings of the quality of summaries
and essays [17, 29], tracing essay elements to their sources [15], optimizing texts-
to-reader matches based on reader knowledge and projected difficulty of unread
texts [53], and for predicting human interpretation of metaphor difficulty [28]. For
this study, however, we adapted the LSA cohesion measuring approach used by
Foltz, Kintsch & Landauer [16]. Foltz and colleagues formed a representation of
global cohesion by using LSA to analyze the relationship of ever distant textual
paragraphs. As the distances increased, so the LSA score of similarity decreased.
The results suggested that LSA was a useful and practical tool for measuring the
relative degrees of similarity between textual sections. In our study, however, we
replace Foltz and colleagues comparison of paragraphs with a comparison of journal
sections, and rather than assuming that cohesion would decrease relative to distance,
we made predictions based on the relative similarity between the sections of the
article.
7.6 Predictions
The abstract section was selected as the primary source of comparison as it is the
only section whose function is specifically to relate the key elements of each other
section of the paper. But the abstract does not relate to each other section of the
paper equally. Instead, the abstract outlines the theme of the study (introduction);
it can refer to the basic method used in the study (methods); it will briefly state a
prominent result from the study (results); and it will then discuss the relevance of the
studys findings (discussions). This definition allowed us to make predictions as to the
signature generated from such comparisons. Specifically, we predicted that abstracts
would feature far greater reference to the introduction (AI comparison type) and
discussion sections (AD comparison type), less reference to the results section (AR
comparison type), and less reference still to the methods section (AM comparison
type). The reason for such predictions is that abstracts would take more care to
set the scene of the paper (the introduction) and the significance of the findings
(discussions). The results section, although important, tends to see its key findings
restated in the discussion section, where it is subsumed into the significance of the
112 Philip M. McCarthy et al.
paper. We predicted that the abstract to methods comparison type (AM) would
form the weakest co-reference as experimental methods, although essential to state
clearly in the body of a paper, tend to follow well-established patterns and are of
little interest to the reader of an abstract who needs to understand quickly and
succinctly the gist of the paper.
7.7 Methods
Using freely available on-line psychology papers from five different journals (see
Appendix), we formed a corpus of 100 texts. For the purposes of simplification and
consistency, we extracted from this corpus only the texts that were comprised of five
author-identified sections: abstract, Introduction, methods, results, discussion. This
left 67 papers in the analysis. We then removed titles, tables, figures, and footnotes
before forming the paired sections outlined above (AI, AM, AR, AD). Each of the
pairs from the 67 papers was then processed through the Coh-Metrix version of LSA
.
7.8 Results of Experiment 1
To examine differences in relatedness of the abstract to each of the text sections, we
conducted a repeated measures Analysis of Variance (ANOVA) on the LSA cosines
including the within-text factors of AI (M=.743, SD=.110), AM (M=.545, SD=.151),
AR (M=.637, SD=.143), and AD (M=.742, SD=.101). As shown in Figure 7.1,
the results confirmed our predictions. There was a main effect of comparison type,
F(3,66)= 54.701, MSE=.011, p<.001. Pairwise contrasts (see Table 7.1) indicated
that all of the differences were reliable except for the difference between the AI and
AD comparisons. The pattern depicted in Figure 7.1 is what we will refer to as the
textual signature for scientific reports such as those we have analyzed in this study.
Table 7.1. Pairwise Comparisons of the Relatedness of Text Sections to the Ab-
stract
Method (AM) Results (AR) Discussion (AD)
Introduction (AI) Diff=.198 (.021)* Diff=.106 (.021)* Diff=.001(.010)
Method (AM) Diff=-.092 (.019)* Diff=-.197(.019)*
Results (AR) Diff=-.105 (.018)*
Notes: Diff denotes the average difference between the cosines; * p<.01
While the signature from Experiment 1 confirmed our prediction, one possi-
bility is that the differences may simply reflect the relative length of the textual
sections. To test this possibility, we examined differences in relatedness of the ab-
stract to each of the text sections by conducting a repeated measures ANOVA on
7 Identifying Text-Types Using Latent Semantic Analysis 113
Fig. 7.1. Textual signature formed from means of the abstract to other sections for
Experiments 1, 2 and 3.
the number of words in each text section including the within-text factors of Intro-
duction (M=1598.015, SD=871.247), Method (M=1295.791, SD=689.756), Results
(M=1408.627, SD=841.185), and Discussion (M=1361.284, SD=653.742). There was
a main effect of comparison type, F(3,66)= 2.955, MSE=382691.182, p=.034. Pair-
wise contrasts (see Table 7.2) indicated that the only significant differences were
between the AI/AM comparison types and the AI/AD comparison type. The re-
sults confirmed that the section length signature does not reflect the LSA signature
(see Figure 7.2). Removing words that LSA does not account for from this analysis
(such as numbers) made no significant difference to the results.
Table 7.2. Pairwise Comparisons of the Relatedness of Text Sections to the Ab-
stract for text length
Method (AM) Results (AR) Discussion (AD)
Introduction (AI) Diff=302.22 (113.13)* Diff=189.39 (107.91) Diff=236.73 (94.37)*
Method (AM) Diff=-112.84 (120.59) Diff=-65.49 (105.67)
Results (AR) Diff=47.34 (97.41)
Notes: Diff denotes the average difference between the lengths; * p<.01
114 Philip M. McCarthy et al.
Fig. 7.2. Average length in terms of number of words in each text section.
7.9 Experiment 2
If LSA compares relative similarities between sections, then it is reasonable to as-
sume that comparing similarly themed papers would produce signatures similar to
those observed in Experiment 1. Because less of a relationship is expected from
papers with overlapping themes as compared to those from the same paper, we pre-
dicted a relative decline in the LSA comparison scores. We also expected a reduced
relationship between the abstract and results section because they are from different
studies.
To test this prediction, we composed an entirely new corpus of 20 similar articles:
all themed as working memory and intelligence. We then extracted the abstracts of
these texts from the bodies and randomly reassigned the abstract to a different
articles parts. As in Experiment 1, we conducted a repeated measures ANOVA on
the LSA cosines including the within-text factors of AI (M=.439, SD=.136), AM
(M=.289, SD=.110), AR (M=.304, SD=.144), and AD (M=.454, SD=.162). There
was a main effect of comparison type, F(3,19)= 16.548, MSE=.009, p<.001. Pairwise
contrasts (see Table 7.3) indicated that all differences were reliable except between
AM and AD and between AM and AR.
As shown in Figure 7.1, the pattern of cosines is similar to that of Experiment
1, with reduced scores overall compared to Experiment 1, and a reduction in the
relationship between the abstracts and results section. These results allow us to
predict that LSA can produce prototypical signatures that are able to differentiate
between sections from the same articles, and those articles that are merely similar
in theme.
7 Identifying Text-Types Using Latent Semantic Analysis 115
Table 7.3. Pairwise comparison of abstract to similar-themed body
Method (AM) Results (AR) Discussion (AD)
Introduction (AI) Diff=.150 (.032)* Diff=.135 (.032)* Diff=-.015 (.020)
Method (AM) Diff=-.015 (.026) Diff=-.165 (.036)*
Results (AR) Diff=.150 (.032)*
Notes: Diff denotes the average difference between the cosines; * p<.01
7.10 Experiment 2a
One potential weakness of Experiment 2 was the relatively small size of the corpus
(i.e., 20 texts). To alleviate the concern that the results were a function of the size
of text corpora, we split the original 67-text corpus from Experiment 1 into three
random groups of 20 texts and re-analyzed the results. If 20 texts were a sufficiently
sized corpus , then the analysis should yield the same pattern as observed in Exper-
iment 1. This analysis produced three sets of scores for each of the four comparison
types (AI, AM, AR, and AD). As can be seen from Figure 7.3, the three new signa-
tures map almost perfectly to the original signature from Experiment 1. There were
also no significant differences within the corresponding section comparisons from the
three 20-text corpora.
Fig. 7.3. Comparison of Experiment 1 to three sets of 20 texts taken from the
original corpora.
116 Philip M. McCarthy et al.
7.11 Experiment 3
In Experiment 1, we showed that LSA may be able to provide a textual signature
based on the relationships between the abstract of the paper and the sections within
the paper. We will refer to this kind of signature as type same paper (SP). In Ex-
periment 2 we showed that LSA can also produce prototypical signatures indicative
of articles of a similar theme. We will refer to this kind of signature as type same
theme (ST). In Experiment 3, we show that LSA based signatures can also indicate
papers that are differently themed. We will refer to this kind of signature as type
different theme (DT).
Based on the findings from Experiment 2, we predicted that the DT signature
would more closely match that of Experiment 2. However, because the themes of
Experiment 3’s abstracts are different from the sections they were being compared
to, we predicted the differences of the LSA scores for the AI and AD comparison
types over the AM and AR comparison types would be less pronounced. To test
this prediction, we randomly replaced the abstracts from Experiment 1 with the
thematically consistent abstracts from Experiment 2.
To examine differences in relatedness of the abstract to each of the text sec-
tions for the DT corpus, we conducted a repeated measures ANOVA on the LSA
cosines including the within-text factors of AI (M=.289, SD=.138), AM (M=.231,
SD=.141), AR (M=.234, SD=.143), and AD (M=.298, SD=.150). There was a main
effect of comparison type, F(3,19)= 9.278, MSE=.002, p<.001. Pairwise contrasts
(see Table 7.4) indicated that the DT signature (like the ST signatures) resulted
in the AR and AM comparisons being significantly different from the AI and AD
comparison types. However, also like the ST signature, the AI comparisons did not
significantly differ from the AD comparisons. Thus, despite the appearance of Fig-
ure 7.1 producing a lower cosine signature, we could not be sure from these results
whether the DT corpus significantly differed from the ST corpus.
Table 7.4. Pairwise comparison of abstract to dissimilarly themed body
Method (AM) Results (AR) Discussion (AD)
Introduction (AI) Diff=.058 (.013)* Diff=.055 (.014)* Diff=-.002 (.013)
Method (AM) Diff=-.003 (.015) Diff=-.060 (.019)*
Results (AR) Diff=-.056 (.017)*
Notes: Diff denotes the average difference between the cosines; * p<.01
To examine whether the SP, ST, and DT corpora were significantly different from
one another, we ran mixed ANOVA, including the within-text factor of comparison
type and the between-text factor of corpora. Because the SP corpus contained 67
papers, and the other corpora were comprised of 20 papers, we randomly selected a
20-paper corpus from Experiment 2a to represent the SP corpus.
As shown in the previous studies, there was a main effect of comparison type,
F(3,171) = 41.855, MSE=.007, p<.001. There was also a main effect of corpus,
F(2,57) = 67.259, MSE=.013, p<.001. A post hoc Bonferroni test between the cor-
pora indicated that there was a significant difference between each of the corpora: SP
7 Identifying Text-Types Using Latent Semantic Analysis 117
to ST (p<.001); SP to DT (p<.001); and ST to DT (p<.05). Most importantly, the
interaction between corpus and section was significant, F(6,171) = 4.196, MSE=.007,
p<.001, indicating that the differences between the sections depends on the type of
corpora.
The results from this experiment indicate that the corpus using the same pa-
pers for the comparisons (SP) shows greater internal difference than do those with
either similar or different themes (i.e., ST, DT). This result is largely due to the
stronger AR comparison generated in the SP corpus. While the signatures generated
from the ST and DT corpora are internally similar, the results of this experiment
offer evidence that the degree of similarity between sections within the corpora is
significantly different.
These results allowed us to extend our signature assumption to predicting that
LSA can differentiate three text-types: the same paper, similarly themed papers,
and differently themed papers.
7.12 Discussion
The results of this study suggest that LSA comparisons of textual sections can
produce an identifiable textual signature. These signatures serve as a prototypical
model of the text-type and are distinguishable from those produced by texts which
are merely similar in theme (ST), or similar in field (DT).
Textual signatures of the type produced in this study have the potential to
be used for a number of purposes. For example, students could assess how closely
the signature of their papers reflected a prototypical signature. The discrepancies
between the two LSA cosines may indicate to the student where information is
lacking, redundant, or irrelevant. For researchers looking for supplemental material,
the signatures method could be useful for identifying texts from the same field,
texts of the same theme, and even the part of the text in which the researcher is
interested. Related to this issue is a key element in Question Answering systems:
as textual signatures stand to identify thematically related material, the retrieval
stage of QA systems may be better able to rank its candidate answers.
Future research will focus on developing a range of textual signatures beyond the
abstract comparisons outlined in this chapter. Specifically, comparisons of section
parts from the perspective of the introduction, methods, results, and discussions
sections need to be examined. This broader scope offers the possibility of greater
accuracy in textual identification. For example, papers that were only thematically
related would likely have higher overlaps generated from introduction sections than
from other sections. Introductions feature a review of the literature which would
likely be highly consistent across papers within the same theme, whereas the other
sections (especially the results section) would likely be significantly different from
paper to paper.
In addition to extending the perspectives of signatures, we also need to consider
how other indices may help us to better identify textual signatures. Coh-Metrix
generates a variety of alternative lexical similarity indices such as stem, lemma,
and word overlap. While these indices do not compare semantic similarities such as
table/chair or intelligence/creativity (as LSA does), they do compare lexical overlaps
such as produce/production, suggest/suggests and investigate/investigated. Indices
118 Philip M. McCarthy et al.
such as these, and the signatures they generate, may come to form a web of soft
constraints that could help us improve the confidence we have that a retrieved text
or textual unit matches a target set of constraints.
If future research offers continued efficacious signatures then an array of indices
can be imagined. Once achieved, a discriminant analysis between corpora such as
the SP, ST, and DT outlined in this study could be conducted. Such testing would
lend substantial support to a textual signatures approach to text identification.
Looking even further ahead, we would also like to extend our signatures research
beyond the type of texts presented in this study. For example, we need to consider
the signatures generated from articles with multiple experiments as well as articles,
essays, and reports from other fields. It is reasonable to expect that any identifiable
genre is composed of elements, and that those elements exposed to methods such
as those used in this study will produce identifiable and therefore distinguishable
signatures.
While a great deal of work remains to be done, we believe that LSA-based
textual signatures contributes to the field by offering a useful and novel approach
for computational research into text mining.
7.13 Acknowledgments
This research was supported by the Institute for Education Sciences (IES
R3056020018-02). Any opinions, findings, and conclusions or recommendations ex-
pressed in this material are those of the authors and do not necessarily reflect the
views of the IES. We would also like to thank David Dufty and Mike Rowe for their
contributions to this study.
References
1. Best, R.M., Floyd, R.G., & McNamra, D.S. (2004). Understanding the fourth-
grade slump: Comprehension difficulties as a function of reader aptitudes and
text genre. Paper presented at the 85th Annual Meeting of the American Ed-
ucational Research Association.
2. Biber, D. (1987). A textual comparison of British and American writing. Amer-
ican Speech, 62, 99-119.
3. Biber, D. (1988). Linguistic features: algorithms and functions in variation
across speech and writing. Cambridge: Cambridge University Press.
4. Brill, E. (1995). Unsupervised learning of disambiguation rules for part of
speech tagging. In Proceedings of the Third Workshop on Very Large Corpora,
Cambridge, MA.
5. Britton, B. K., & Gulgoz, S. (1991). Using Kintschs computational model to
improve instructional text: Effects of inference calls on recall and cognitive
structures. Journal of Educational Psychology, 83, 329-345
6. Burrows, J. (1987). Word-patterns and story-shapes: The statistical analysis of
narrative style. Literary and Linguistic Computing, 2, 6170.
7. Charniak, E. (1997) Statistical Parsing with a context-free grammar and word
statistics Proceedings of the Fourteenth National Conference on Artificial In-
telligence, Menlo Park: AAAI/MIT Press
7 Identifying Text-Types Using Latent Semantic Analysis 119
8. Charniak, E. (2000) A Maximum-Entropy-Inspired Parser. Proceedings of the
North-American Chapter of Association for Computational Linguistics, Seattle,
WA
9. Charniak, E. & Johnson, M. (2005) Coarse-to-fine n-best parsing and Max-
Ent discriminative reranking. Proceedings of the 43rd Annual Meeting of the
Association for Computational Linguistics (pp. 173-180). Ann Arbor, MI
10. Collins, M. (1996) A New Statistical Parser Based on Bigram Lexical Depen-
dencies. Proceedings of the 34th Annual Meeting of the ACL, Santa Cruz, CA
11. Collins, M. (1997) Three Generative, Lexicalised Models for Statistical Parsing
Proceedings of the 35th Annual Meeting of the Association for Computational
Linguistics, Madrid, Spain.
12. Crossley, S., Louwerse, M.M., McCarthy, P.M., & McNamara, D.S. (forthcom-
ing 2007). A linguistic analysis of simplified and authentic texts. Modern Lan-
guage Journal, 91, (2).
13. Dennis, S., Landauer, T., Kintsch, W. & Quesada, J. (2003). Introduction to
Latent Semantic Analysis. Slides from the tutorial given at the 25th Annual
Meeting of the Cognitive Science Society, Boston.
14. Duran, N., McCarthy, P.M., Graesser, A.C., McNamara, D.S., (2006). An em-
pirical study of temporal indices. Proceedings of the 28th annual conference of
the Cognitive Science Society, 2006.
15. Foltz, P. W., Britt, M. A., & Perfetti, C. A. (1996). Reasoning from multiple
texts: An automatic analysis of readers’ situation models. In G. W. Cottrell
(Ed.) Proceedings of the 18th Annual Cognitive Science Conference (pp. 110-
115). Lawrence Erlbaum, NJ.
16. Foltz, P. W., Kintsch, W., & Landauer, T. K. (1998). The measurement of
textual Coherence with Latent Semantic Analysis. Discourse Processes, 25, 285-
307.
17. Foltz, P. W., Gilliam, S., & Kendall, S. (2000). Supporting content-based feed-
back in on-line writing evaluation with LSA. Interactive Learning Environ-
ments, 8, 111-127.
18. Gernsbacher, M.A. (1990). Language comprehension as structure building.
Hillsdale, NJ: Erlbaum.
19. Givn, T. (1995). Coherence in the text and coherence in the mind. In Gerns-
bacher, M.A. & Givn, T., Coherence in spontaneous text. (pp. 59-115). Ams-
terdam/Philadelphia, John Benjamins.
20. Graesser, A.C. (1993). Inference generation during text comprehension. Dis-
course Processes, 16, 1-2.
21. Graesser, A.C., Singer, M., & Trabasso, T. (1994). Constructing inferences dur-
ing narrative text comprehension. Psychological Review, 101, 371-95.
22. Graesser, A.C., McNamara, D., Louwerse, M., & Cai, Z. (2004). Coh-Metrix:
CohMetrix: Analysis of text on cohesion and language. Behavioral Research
Methods, Instruments, and Computers, 36, 193-202.
23. Hearst, M.A. (1994) Multi-paragraph Segmentation of Expository Text. Pro-
ceedings of the Association of Computational Linguistics, Las Cruces, NM.
24. Hobbs, J.R. (1985). On the coherence and structure of discourse. CSLI Tech-
nical Report, 85-37. Stanford, CA.
25. Hovy, E. (1990). Parsimonious and profligate approaches to the question of dis-
course structure relations. Proceedings of the Fifth International Workshop on
Natural Language generation, East Stroudsburg, PA, Association for Compu-
tational Linguistics.
120 Philip M. McCarthy et al.
26. Karlsgren J. & Cutting, D. (1994). Recognizing text genres with simple met-
rics using discriminant analysis. International Conference on Computational
Linguistics Proceedings of the 15th conference on Computational linguistics -
Volume 2 (pp. 1071-1075). Kyoto, Japan.
27. Kessler, Nunberg, G., & Schutze, H. (1997). Automatic detection of text genre.
In Proceedings of 35th Annual Meeting of Association for Computational Lin-
guistics, and in 8th Conference of European Chapter of Association for Com-
putational Linguistics (pp. 32-38). Madrid, Spain.
28. Kintsch, W. & Bowles, A. (2002) Metaphor comprehension: What makes a
metaphor difficult to understand? Metaphor and Symbol, 2002, 17, 249-262.
29. Kintsch, E., Steinhart, D., Stahl, G., LSA Research Group, Matthews, C., &
Lamb, R. (2000). Developing summarization skills through the use of LSA-
based feedback. Interactive Learning Environments 8, 87-109.
30. Kintsch, W., & van Dijk, T.A. (1978). Toward a model of text comprehension
and production. Psychological Review, 85, 363-394.
31. Labov, W. (1972). The Transformation of Experience in Narrative Syntax, In
W. Labov (ed.), Language in the Inner City, 1972, University of Pennsylvania
Press, Philadelphia.
32. Landauer, T. K., & Dumais, S. T. (1997). A solution to Plato’s problem: The
Latent Semantic Analysis theory of the acquisition, induction, and representa-
tion of knowledge. Psychological Review, 104, 211-240.
33. Landauer, T. K., Foltz, P. W., & Laham, D. (1998). Introduction to Latent
Semantic Analysis. Discourse Processes, 25, 259-284.
34. Lehman, S., & Schraw, G. (2002). Effects of coherence and relevance on shallow
and deep text processing. Journal of Educational Psychology, 94, 738-750.
35. Linderholm, T., Everson, M.G., van den Broek, Mischinski, M., Crittenden, A.,
& Samuels, J. (2000). Effects of causal text revisions on more and less skilled
readers comprehension of easy and difficult text. Cognition and Instruction, 18,
525-556.
36. Louwerse, M.M. (2002). Computational retrieval of themes. In M.M. Louw-
erse & W. van Peer (Eds.), Thematics: Interdisciplinary Studies (pp. 189-212).
Amsterdam/Philadelphia: John Benjamins.
37. Louwerse, M. M., McCarthy, P. M., McNamara, D. S., & Graesser, A. C. (2004).
Variation in language and cohesion across written and spoken registers. In K.
Forbus, D. Gentner, & T. Regier (Eds.), Proceedings of the 26th Annual Meet-
ing of the Cognitive Science Society (pp. 843-848). Mahwah, NJ: Erlbaum.
38. Loxterman, J.A., Beck, I. L., & McKeown, M.G. (1994). The effects of thinking
aloud during reading on students’ comprehension of more or less coherent text.
Reading Research Quarterly, 29, 353-367.
39. Mani, I. & Pustejovsky, J. (2004). Temporal discourse markers for narrative
structures.ACL Workshop on Discourse Annotation, Barcelona, Spain. East
Stoudsburg, PA, Association for Computational Linguistics.
40. Mann, W. C. & Thompson, S. A. (1988). Rhetorical Structure Theory: Toward
a functional theory of text organization. Text, 8 (3). 243-281
41. McCarthy, P.M., Lightman, E.J., Dufty, D.F. & McNamara (in press). Us-
ing Coh-Metrix to assess distributions of cohesion and difficulty in high-school
textbooks. Proceedings of the 28th annual conference of the Cognitive Science
Society.
7 Identifying Text-Types Using Latent Semantic Analysis 121
42. McCarthy, P.M., Lewis, G.A., Dufty, D.F., & McNamara, D.S. (2006). Ana-
lyzing Writing Styles with Coh-Metrix. 19th International FLAIRS Conference
2006.
43. McNamara, D.S., Kintsch, E., Songer, N.B., & Kintsch, W. (1996). Are good
texts always better? Text coherence, background knowledge, and levels of un-
derstanding in learning from text. Cognition and Instruction, 14, 1-43.
44. McNamara, D. S. (2001). Reading both high and low coherence texts: Effects
of text sequence and prior knowledge. Canadian Journal of Experimental Psy-
chology, 55, 51-62.
45. Moldovan, D., Harabagiu, S., Pasca, M., Mihalcea, R., Girju, R., Goodrum, R.,
& Rus, V. (2000): The Structure and Performance of an Open-Domain Question
Answering System, in Proceedings of ACL 2000, Hong Kong, October
46. Morris, J., Hirst, G. (1991) Lexical cohesion computed by thesaural relations
as an indicator of the structure of text, Computational Linquistics, 17, 21-48.
47. Ozuru, Y., Dempsey, K., Sayroo, J., & McNamara, D. S. (2005). Effects of
text cohesion on comprehension of biology texts. Proceedings of the 27th An-
nual Meeting of the Cognitive Science Society (pp. 1696-1701). Hillsdale, NJ:
Erlbaum.
48. Propp, V. (1968). Morphology of the folk tale. Baltimore: Port City Press, pp
19-65.
49. Ratnaparkhi, A. (1996), A maximum entropy model for part-of-speech tag-
ging. Proceedings of Conference on Empirical Methods in Natural Language
Processing, University of Pennsylvania.
50. Stamatatos, E., Fakotatos, N., & Kokkinakis, G. (2001). Computer-based au-
thorship attribution without lexical measures. Computers and the Humanities,
35, 193-214.
51. Trabasso, T., & van den Broek, P. (1985). Causal thinking and the representa-
tion of narrative events. Journal of Memory and Language, 24, 612-630.
52. Voorhees, E. M. & Tice, D.M. (2000). Building a question answering test col-
lection. Proceedings of the Twenty-Third Annual International ACM SIGIR
Conference on Research and Development in Information Retrieval
53. Wolfe, M. B., Schreiner, M. E., Rehder, B., Laham, D., Foltz, P. W., Kintsch,
W., & Landauer, T. K. (1998). Learning from text: Matching readers and text
by Latent Semantic Analysis. Discourse Processes, 25, 309-336.
54. Wolfe, M. B.W., & Goldman S.R. (2003). Use of latent semantic analysis for
predicting psychological phenomena: Two issues and proposed solutions. Be-
havior Research Methods, Instruments, & Computers, 35, 22-31.
55. Zwaan, R.A.(1996). Processing narrative time shifts. Journal of Experimental
Psychology: Learning, Memory, and Cognition, 22, 1196-1207.
56. Zwaan, R.A. & Radvansky, G.A. (1998). Situation models in language compre-
hension and Memory. Psychological Bulletin, 123, 162-185.
122 Philip M. McCarthy et al.
Appendix (Journals Analyzed)
Table 7.5.
Journal Name Articles Publication Date Range
Acta Psychologica 11 2000-2004
Biological Psychology 14 2000-2004
Cognition 4 2000-2001
Intelligence 20 2000-2003
Journal of Applied Psychotherapy Research 17 2002-2003
