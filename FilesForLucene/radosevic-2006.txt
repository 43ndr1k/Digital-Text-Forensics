Flexible Length Phrases in Document Classification 
Danijel Radoševi , Jasminka Dobša 
University of Zagreb 
Faculty of organization and informatics 
Pavlinska 2, Varaždin, Croatia 
{danijel.radosevic,jasminka.dobsa}@foi.hr 
Dunja Mladeni
J.Stefan Institute, 
Jamova 39, 1000 Ljubljana, Slovenia 
Dunja.Mladenic@ijs.si 
Abstract. In this paper we investigate 
possibility of using phrases of flexible length 
in classification of textual documents as an 
extension to classic bag of words document 
representation where documents are 
represented using single words as index terms. 
The investigation is conducted on collection of 
articles from Ve ernji list. It is shown that 
usage of  flexible length phrases improves 
precision of automatic document classification 
and there are indications that such approach 
could be used for genre classification. 
Keywords: documents classification, bag of 
words representation, flexible length phrases 
1. Introduction 
The goal of text categorization is classification 
of text documents into a fixed number of 
predefined categories. Document classification 
is used in many different problem areas 
involving text documents such as classifying 
news articles based on their content, or 
suggesting interesting documents to the web 
user. 
The common way of representing textual 
documents is by vector space model or, so 
called bag-of-words representation [11]. 
Generally, index term can be any word present 
in the text of document, but not all the words 
in the documents have equal importance in 
representation of document semantic. That is 
why various schemes in bag-of-words 
representation give greater weight to words 
which appear in smaller number of documents, 
and have greater discrimination power in 
document classification and smaller weight to 
words which are present in lots of documents. 
Common preprocessing step in document 
indexing is elimination of, so called, stop
words, or words such as conjunctions, 
prepositions and similar, because these words 
have low discrimination  power.   
Some approaches extend bag-of-words 
representation by some additional index terms 
such as n-grams proposed in [10]. Those index 
terms consist of n words forming sequences. 
In this work, we suggest using statistical 
phrases of flexible length.  In the further text 
we will refer to statistical phrases of flexible 
length as phrases.  The main difference 
between n-grams and phrases, beside the fact 
that n- grams consist of exactly n words, 
opposite to phrases  of the flexible length, is 
that the phrases can contain punctuations. It is 
important to stress that phrases can also 
include stop words. The reason for inclusion 
of stop words is the intention to form phrases 
characteristic to writing style. So, beside the 
index terms consisting of just one word that 
are key words characteristic for some topic, 
for representation of document we use phrases 
which could reveal the style of writing. 
Nevertheless, some phrases are very common, 
and their classification power is low. That is 
why we introduce stop phrases as phrases 
which occur in all categories.  
Classification of documents according to 
style of writing or genre has already been 
recognized as useful procedure for 
457
heterogeneous collections of documents, 
especially for Web ([4], [6], [8],[9],[12]).     
Many algorithms are already developed for 
automatic categorization [5]. For the purpose 
of our experiments we used the algorithm of 
support vector machines (SVMs), which was 
introduced in 1992 by Vapnik and coworkers 
[2]. Since then, it was shown that algorithm of 
SVMs is very effective in large scale of 
applications, especially for the classification of 
text documents [7].  
The paper is organized as follows. In Section 2 
we describe the algorithm for generating 
flexible length phrases. Section 3 gives 
experimental design. Results of experiments 
are described in Section 4 and discussion with 
some directions for future work is given in 
Section 5. 
2. Generating statistical phrases of flexible 
length 
The algorithm of generating statistical phrases 
is based on the frequency of the phrases (Term 
Frequency - TF) over all the documents in 
each category. Frequency of the phrases is 
counted in each category separately. Only 
phrases occurring at least two times in at least 
one category are considered. 
2.1. Algorithm Description 
The algorithm starts by dividing all sentences 
from all the documents into sets of 
subsentences. Each subsentence starts from 
subsequent word position and ends with the 
sentence end. All punctuations are treated in 
the same way as words. After that, 
subsentences are sorted by category. Inside 
each category they are sorted alphabetically in 
ascending order. Extracting phrases starts by 
comparing beginnings of subsequent 
subsentences. The longest possible phrase 
(consist of maximum possible number of 
words) is saved, together with information 
about the corresponding document category. 
The algorithm for extracting phrases is 
summarized in a form of  pseudocode in 
Figure 1. 
Given: Set of documents (each document is a 
sequence of words) 
foreach Document 
  foreach Sentence 
     save Sentence as a set of subsentences,  
starting from each sentence word 
  end // Sentence 
end // Document   
foreach Document category 
  collect all subsentences from documents in  
    that category 
  sort subsentences alphabetically 
  foreach Subsentence 
    compare Subsentence to the next  
      subsentence by taking the maximum  
     number of overlapping words from the first  
     word (forming a phrase) 
     save Phrases consisting of minimum two  
       words 
    end // Subsentence 
  foreach Phrase 
   count number of occurrences in documents 
  end // Phrase 
end  // Document category 
Figure 1. Algorithm for generating 
flexible length phrases
After the application of this algorithm, we can 
see that some phrases occur in all the 
categories. We consider such phrases as “stop 
phrases”, i.e. phrases that are useless for the 
given classification task. Consequently, these 
phrases are excluded from phrases list. 
2.2. Illustration of the Algorithm
For illustration of the algorithm, let’s consider 
the following sentences in Croatian language, 
all belonging to different documents of the 
same category.  
. . . 
Prošle je godine u Afganistanu proizvedeno 90 
posto svjetske koli ine heroina, te znatne koli ine 
drugih droga. 
Masovno spaljivanje droge obavljeno je i u još 
nekoliko azijskih zemalja velikih proizvo a a
droge. 
Naime, na prošlogodišnjim op im izborima 
konzervativci su pobijedili reformiste i vratili 
kontrolu nad parlamentom. 
. . . 
458
Step 1: Making subsentences 
In the first  step all sentences in all documents 
are divided into sets of all subsentences, 
starting from each position in the sentence 
(punctuation signs are considered as words; 
last word is not particularly excluded because 
cannot form a phrase). 
Prošle je godine u Afganistanu proizvedeno 90 
posto svjetske koli ine heroina, te znatne koli ine 
drugih droga. 
. . . 
posto svjetske koli ine heroina, te znatne koli ine 
drugih droga. 
svjetske koli ine heroina, te znatne koli ine 
drugih droga. 
koli ine heroina, te znatne koli ine drugih droga. 
heroina, te znatne koli ine drugih droga. 
, te znatne koli ine drugih droga. 
te znatne koli ine drugih droga. 
znatne koli ine drugih droga. 
koli ine drugih droga. 
drugih droga. 
droga. 
Step 2: Collecting and sorting all 
subsentences for each category 
In the second step all subsentences belonging 
to same category are put together and sorted. 
... 
konzervativci su pobijedili reformiste i ... 
Masovno spaljivanje droge obavljeno je i ... 
na prošlogodišnjim op im izborima...  
nad parlamentom. 
Naime, na prošlogodišnjim op im izborima...  
nekoliko azijskih zemalja velikih proizvo a a... 
obavljeno je i u još nekoliko azijskih zemalja...  
op im izborima konzervativci su pobijedili...  
... 
Step 3: Comparing beginnings of 
neighboring sentences and excluding 
phrases 
In the third step each beginning of subsentence 
is compared to the beginning of next 
subsentence taking maximum number of 
overlapping words. Phrases consist of 
minimum two words (punctuations are 
considered in the same way as words).  
In the following text phrases excluded in the 
procedure are shadowed.  
. . . 
, te u budu nosti. 
, te znatne koli ine drugih droga. 
90 posto svjetske koli ine heroina, te znatne...  
. . . 
je godine u Afganistanu proizvedeno 90 posto...  
je i pobijedila. 
je i u još nekoliko azijskih zemalja velikih... 
jer su izgubili izbore. 
još nekoliko azijskih zemalja velikih ... 
 još nekoliko dana. 
koli ine drugih droga. 
. . . 
3. Experimental design 
3.1 Data description
The experiments are conducted on the 
collection of  articles from Croatian daily 
newspaper Ve ernji list (Evening Newspaper). 
The collection contains 3020 articles that were 
published since 16th of June 2005 until 8th of 
Jul 2005. We have divided the collection on 
training and test set in a following way: 
training set contains all articles published until 
2nd of Jul (inclusive) and test set contains all 
articles published after that date.  List of the 
phrases is formed by usage of documents from 
training set. 
Articles in Ve ernji list originally are 
organized in three hierarchical levels. On the 
top level there are four categories: Newsroom
(Aktualnosti), LifeStyle, Free Time  and Other 
Categories. Category of Other Categories
contains articles which are not included in any 
of the standard categories that newspaper 
regularly has and this category will not be 
concerned. Number of documents in the first 
three top categories and the ratio of training 
and test documents for that categories are 
represented on Figure 1. The category  
Newsroom is the largest category on the top 
level and it has eight categories on the second 
hierarchical level: News (Vijesti), Black 
Chronicle (Crna kronika), Sports (Sport), 
Scene (Scena), Culture (Kultura), Economics
(Gospodarstvo), Curiosities (Zanimljivosti) 
and Regions (Regije). Our assumption is that 
last two categories can hardly be recognizable 
on the base of the content of articles that they 
contain. 
459
We conducted two experiments: the first 
will include classification of articles into the 
first three categories on the top level 
(Newsroom, Lifestyle and Free Time) and the 
second experiment will include classification 
of articles into the first six (out of eight) 
categories on the second hierarchical level of 
the category Newsroom. Number of 
documents for that six categories and the ratio 
of train and test documents is represented on 
Figure 2.  The first experiment included 2823 
articles of the collection Ve ernji list, out of 
which 29.04% formed test set (according to 
division by date), while the second experiment 
included 1570 articles with 28.66% in test set. 
0
500
1000
1500
2000
2500
Newsroom LifeStyle Free Time
nu
m
be
r 
of
 d
oc
.
Train documents Test documents
Figure 2. Number of documents in the first 
three top categories of collection Ve ernji 
list and the ratio of   training and test 
documents for the categories.
0
100
200
300
400
500
600
700
800
N
ew
s
B
lack
C
hronicle
S
ports
S
cene
C
ulture
E
conom
ics
N
um
be
r o
f d
oc
.
Train documents Test documents
Figure 3. Number of documents for six 
subcategories of the top category 
Newsroom and the ratio of train and test 
documents for that categories.
4. Experimental results 
We used the Support vector machine ([3],[7]) 
in classification, implemented by SvmLight 
v.5.0 software by Joachims [7] with default 
parameters. Experimental results include the 
list of frequent phrases by categories and the 
list of stop phrases (occurring in each 
category), extracted by the proposed algorithm 
presented in Section 2. 
4.1. List of most frequent phrases 
Our algorithm found that in addition to stop 
phrases, which are common to all the 
categories, there are some phrases that are 
typical for only a subset of categories. In Table 
1 we list the five most frequent phrases that 
occur only in one category.
4.2. Stop phrases 
Phrases occurring in all categories are 
considered as stop phrases. In this 
investigation, all phrases were classified in 
two ways. The first, classification into three 
main categories: Newsroom, LifeStyle, Free 
Time having 294 stop phrases. The second, 
classification into six subcategories of 
category Newsroom having a total of 91 stop 
phrases that occur in all six subcategories. 
4.3. Classification results 
For each of the two experiments we performed 
the classification experiments to test 
contribution of the flexible length phrases to 
the classification performance. The list of 
index terms is formed by use of words and 
phrases occurred in at least 4 documents. The 
list of stop terms (stop words and stop phrases) 
is excluded from the list of index terms. Stop 
words for Croatian are formed manually as a 
list of functional words. List of stop words is 
same for both experiments while the list of 
stop phrases differs for two experiments. 
Notice that the list of index terms is the same 
for both experiments while the list of phrases 
differs.  
For evaluation of classification, we have used 
the standard recall, precision and F1. Precision 
p is a proportion of documents predicted 
positive that are actually positive. Recall r is 
defined as a proportion of positive documents 
that are predicted positive. The F1 measure is 
defined as ( )rpprF += 21 .
460
Table 1. The most frequent phrases for each of the categories.
BOW BOW+phrases
Category Recall Precision F1 Recall Precision F1
Newsroom 97,94 84,97 90,99 98,26 86,10 91,77 
LifeStyle 11,67 100,00 20,90 13,33 100,00 23,52 
FreeTime 52,56 91,11 66,66 52,56 91,11 66,66 
Table 2: Results of the first experiment for bag-of-words on single word and with phrases. 
Better results for BOW+phrases representation are bolded. 
 BOW BOW+phrases 
Category Recall Precision F1 Recall Precision F1
News 66,84 81,88 73,59 66,84 82,39 73,80 
Black 
Chronicle 38,30 75,00 50,70 31,91 83,33 46,14 
Sports 74,29 96,30 83,87 75,24 96,34 84,49 
Scene 76,09 100,00 86,42 10,87 100,00 19,60 
Culture 0,00 0,00 0,00 0,00 0,00 0,00 
Economy 3,23 50,00 6,06 3,23 100,00 6,25 
Table 3: Results of the second experiment for bag-of-words on single word and with 
phrases. Better results for BOW+phrases representation are bolded 
.
4.3.1. The first experiment 
The first experiment was performed using 
words only, and words together with phrases 
(Table 2). There were 13505 index terms 
(10367 words and 3138 phrases). Category 
Newsroom has the best results because of the 
largest number of documents, which is quite 
unbalanced compared to the other categories. 
We assume that absolute number of 
documents in the other two categories is too 
small for efficient learning characteristics of 
the classes. Generally, the results are slightly 
better when the fixed length phrases are 
included in addition to single words. 
4.3.2. The second experiment 
The second experiment was also performed 
using words only, and words together with 
phrases (Table 3). There were 12836 index 
terms (10367 words and 2469 phrases). We 
can see that for categories News and Sports we 
get the highest performance, because of the 
largest number of documents. Categories 
Culture, Scene and Economy are too small for 
efficient learning characteristics of classes. 
There is also possibility that some phrases that 
are really stop phrases (but not classified as 
such) had some negative impact. Similarly to 
the first experiment, the results are slightly 
better when the flexible length phrases are 
included in addition to single words.
Newsroom LifeStyle Free Time News Black chronicle
, po 
, je 
, pri 
, a o 
, koji s 
prijenosnih ra unala 
RTL TV 
da mi 
, a u to 
a ne 
, film 
u filmu 
za Oscara 
film je 
, Paul 
u Brezovici 
u Mostaru 
Ivo Sanader 
vanjskih poslova 
, iji 
, pri 
u zatvor 
sati na 
u no i
prešao na 
Sport Scene Culture Economy
, je 
da sam 
je iro 
m: 1 
na travi 
Alen Islamovi
Goran Bregovi
na scenu 
na stadionu 
Željko Bebek 
u Muzeju 
Vladimir Nazor 
Muzeju za umjetnost i 
Muzeju za umjetnost i obrt 
i obrt 
dolara po 
dolara po barelu 
po barelu 
centi na 
imovinske kartice 
461
5. Discussion and further work 
Our experiments show that using flexible 
length phrases slightly improves the 
classification performances. Adding phrases to 
the bag-of-words document representation has 
more effect on precision, than on recall. In the 
future, we would like to compare the 
performance of flexible length phases to n-
grams. We plan to use more heterogenic 
Croatian collections, obtained from different 
sources, by merging these sources. We assume 
that using phrases could give better results on 
more heterogenic collection of documents, and 
especially in genre classification. Also we plan 
to conduct experiments on English language 
collections. 
We consider some ideas of algorithm 
modification, in a sense of using patterns 
instead of phrases, which can be obtained from 
phrases by replacing key words by 
replacements tags.
6. Acknowledgements 
This work was supported by the Slovenian 
Research Agency and the IST Programme of 
the European Community under SEKT 
Semantically Enabled Knowledge 
Technologies (IST-1-506826-IP) and 
PASCAL Network of Excellence (IST-2002-
506778). This publication only reflects the 
authors' views. We thank prof.dr. Marko Tadi
for list of Croatian stop words. 
7. References 
[1] R. Baeza-Yates, B.Ribeiro-Neto. Modern 
Information Retrieval, Addison-Wesley, ACM 
Press, New York, 1999. 
[2] B.E. Bosner, I.M. Guyon, V.N. Vapnik. A 
training algorithm for optimal margin 
classifier, In Proceedings of the 5th Annual 
ACM Workshop on Computational Learning 
Theory, 1992., pp. 144-152. 
[3] N. Cristianini, J. Shave-Taylor. Support
Vector Machines and Other Kernel-based 
Learning Methods, Cambridge University 
Press, 2000. 
[4] A. Dillon, B. Gushrowski, Genres and the 
Web - is the home page the first digital genre?, 
Journal of the American Society for 
Information Science, Vol. 51, No.2, pp. 202-
205. 
[5] R.O. Duda, P.E. Hart, D.G. Stork. Pattern 
Classification, second edition, Willey, New 
York, 2001. 
[6] A. Finn, N. Kushmerick, Barry Smyth: 
Genre Classification and Domain Transfer for 
Information Filtering, In Advances in 
Information Retrieval, Proceedings of 24th 
BCS-IRSG European Colloquium on IR 
Research, 2002, pp. 353-362.  
[7] T. Joachims. Text categorization with 
support vector machines: Learning with many 
relevant features, In Proceedings of the 
European Conference on Machine Learning,
1998, Springer, pp. 137-142. 
[8] J. Karlgren, D. R. Cutting. Recognizing 
Text Genres With Simple Metrics Using 
Discriminant Analysis, In Proceedings of 15th 
International Conference on Computational 
Linguistics,  1994, Vol. 2, pp. 1071-1075. 
[9] B. Kessler, G. Numberg, N. Schutze. 
Automatic Detection of Text Genre, In 
Proceedings of 35th Annual Meeting of 
Association for Computational Linguistics and 
8th Conference of European Chapter of the 
Association for Computational Linguistics, 
1997, pp. 32-38. 
[10] D. Mladeni , M. Grobelnik. Word 
sequences as features in text-learning, In 
Proceedings of the 7th Electornical and 
Computer Science Conference, Ljubljana,
1998. 
[11] G. Salton, C. Buckley. Term-weighting 
approaches in automatic retrieval, Information 
Processing & Menagement, 1988, Vol. 24, No. 
5, pp. 513-523. 
 [12] E. Stamatatos, N. Fakotakis, G. 
Kokkinakis: Automatic Text Categorisation in 
Terms of Genre and Author , Computational 
Linguistics, 2001, Vol. 26, No. 4, pp. 471-495. 
462
