 
Accepted Manuscript
Analyzing Documents with Quantum Clustering: A Novel Pattern
Recognition Algorithm Based on Quantum Mechanics
Ding Liu, Minghu Jiang, Xiaofang Yang, Hui Li
PII: S0167-8655(16)00077-5
DOI: 10.1016/j.patrec.2016.03.008
Reference: PATREC 6474
To appear in: Pattern Recognition Letters
Received date: 14 July 2015
Accepted date: 8 March 2016
Please cite this article as: Ding Liu, Minghu Jiang, Xiaofang Yang, Hui Li, Analyzing Documents with
Quantum Clustering: A Novel Pattern Recognition Algorithm Based on Quantum Mechanics, Pattern
Recognition Letters (2016), doi: 10.1016/j.patrec.2016.03.008
This is a PDF file of an unedited manuscript that has been accepted for publication. As a service
to our customers we are providing this early version of the manuscript. The manuscript will undergo
copyediting, typesetting, and review of the resulting proof before it is published in its final form. Please
note that during the production process errors may be discovered which could affect the content, and
all legal disclaimers that apply to the journal pertain.
ACCEPTED MANUSCRIPT
AC
CE
PT
ED
 M
AN
US
CR
IP
T
1
Highlights
• We extended the Quantum Clustering to text analysis and
authorship identification.
• We performed the full-scale comparison between Quan-
tum Clustering and Parzen-window.
• The Quantum Clustering is better than Parzen-window on
achieving optimal clustering.
• Quantum Clustering depend much less sensitively upon
the choice of parameter.
• The Pattern Search could be an appropriate solution to the
parameter estimation.
ACCEPTED MANUSCRIPT
AC
CE
PT
ED
 M
AN
US
CR
IP
T
2
Pattern Recognition Letters
journal homepage: www.elsevier.com
Analyzing Documents with Quantum Clustering: A Novel Pattern Recognition Algorithm
Based on Quantum Mechanics
Ding Liua,b,, Minghu Jiangb,, Xiaofang Yangb, Hui Lic
aDepartment of Computer Science and Technology, School of Computer Science & Software Engineering,Tianjin Polytechnic University, No. 399 Binshui Road,
Xiqing District, Tianjin 300387, China
bLaboratory of Computational Linguistics, School of Humanities,Tsinghua University, Haidian District, Beijing 100084, China
cInstitute of Computer Science, Heidelberg University, Im Neuenheimer Feld 348, Heidelberg 69120, Germany
ABSTRACT
The article introduces Quantum Clustering, a novel pattern recognition algorithm inspired by quantum
mechanics and extend it to text analysis. This novel method improves upon nonparametric density
estimation (i.e. Parzen-window), and differentiates itself from it in a significant way, Quantum Clus-
tering constructs the potential function to determine the cluster center instead of the Gaussian kernel
function. Specifically, detailed comparative analysis shows that the potential function could clearly
reveal the underlying structure of the data that the Gaussian kernel could not handle. Moreover, the
problem of parameter estimation is solved successfully by the numerical optimization approach (i.e.
Pattern Search). Afterwards, the results of detailed comparative experiments on three benchmark
datasets confirms the advantage of Quantum Clustering over the Parzen-window, and the additional
trial on authorship identification illustrates the wide application scope of this novel method.
Keywords: Quantum Clustering,Text analysis,Text clustering
c© 2016 Elsevier Ltd. All rights reserved.
1. Introduction
Text clustering (document clustering) is a process of auto-
matic document organization which is also representative of
the pattern recognition problem. It plays a crucial role in text
mining and, to this point, has been subjected to intensive ex-
amination (Hotho et al., 2003; Zamir and Etzioni, 1998; Xu
et al., 2003). Various clustering algorithms have been devel-
oped based on different principles, such as the hierarchical
and partitional algorithms. However, researchers are still in-
terested in developing new methods to promote the research on
text clustering. Recently, a novel approach known as Quantum
Clustering (QC) has emerged which derives from principles of
quantum mechanics (Horn and Gottlieb, 2001b,a). This novel
method is developed based upon the conventional approach of
density estimation and is considered as a new approach to non-
parametric clustering methods (Nasios and Bors, 2007). As a
Corresponding author: Tel.: +86 15822510163; fax: +86 022-58685358;
Corresponding author: Tel.: +86 13520115507; fax: +86 010-62785736;
e-mail: dingliu_thu@126.com (Ding Liu),
jiang.mh@mail.tsinghua.edu.cn (Minghu Jiang)
result, the researchers will ask “Whether it could be applied
to text analysis? And what about its performance on real task
in comparison with the classical density estimation approach?”
This article focuses on introducing this novel method into text
analysis and giving a clear and detailed comparative analysis
between Quantum Clustering and the conventional density esti-
mator, in order to show the real advantage of Quantum Cluster-
ing and answer these questions.
The most important conventional density estimation is re-
ferred to as the Parzen-window estimator. It is considered the
main approach in the nonparametric family. In Ref.(Nasios and
Bors, 2007), the nonparametric methods approximate the prob-
ability density function without any underlying model assump-
tion, and usually associate a kernel function to each data sam-
ple. Typically, the commonly used Gaussian kernel is regarded
as the kernel function, and this kind of kernel function depends
on a single parameter (i.e. the width parameter). Based on
the Parzen-window estimator, the Quantum Clustering is devel-
oped.
The thinking of Quantum Clustering is inspired by the fun-
damental physics principles (i.e. quantum mechanics). Dif-
ferent from the Parzen-window estimator, Quantum Clustering
ACCEPTED MANUSCRIPT
AC
CE
PT
ED
 M
AN
US
CR
IP
T
3
Fig. 1. Electron cloud diagram of hydrogen atoms (available at
http://baike.haosou.com/doc/history/id/168383)
constructs the potential function to estimate the density distri-
bution of the data points instead of the Gaussian kernel, since
the potential function has the potential to reveal the underlying
structure of the data. Due to these advantages, some researchers
began to apply this method to image segmentation, signal pro-
cessing, etc.(Nasios and Bors, 2007, 2005). Furthermore, the
Dynamic Quantum Clustering (DQC) has been proposed (We-
instein and Horn, 2009; Weinstein et al., 2008; Weinstein, 2010)
and successfully employed in analyzing big, complex, real-
world datasets obtained from various fields, such as x-ray nano-
chemistry, condensed matter, seismology, biology, finance (We-
instein et al., 2013), and information retrieval (Di Buccio and
Di Nunzio, 2011a,b). And also, researcher have focus on speed-
ing it up by graphics processor(Wittek, 2013). Based on these
works, we extended QC to text analysis in Ref.(Liu et al., 2014).
Furthermore, in this paper, detailed and full-scaled compara-
tive analysis between QC and Parzen-window estimator were
conducted. Moreover, we also compared QC with the com-
mon used DBSCAN algorithm. And, the problem of parameter
estimation was solved successfully by the numerical optimiza-
tion approach (i.e. Pattern Search). All of these have not been
subjected to rigorous analysis in previous works. Experimental
results show the QC outperformed the Parzen-window signifi-
cantly.
2. Method
2.1. Principle of Quantum Clustering
The inspiration of Quantum Clustering derives from the anal-
ogy that exists between data points and particles in a cer-
tain state. According to the postulate of quantum mechan-
ics, a quantum system evolves in space and time following the
Schrödinger differential equation. The Schrödinger equation
describes the evolutionary process of a quantum mechanics sys-
tem and is specified by a wave function. The Schrödinger equa-
tion can be written as various formulations in different context,
and the time-independent version is given by Eq. (1) (Feynman,
1965)
Hψ (x) =
(
− ~
2
2m
∇2 + v(x)
)
ψ (x) = Eψ (x) (1)
where H is the Hamiltonian operator, E is the eigenvalue energy
level, ~ and m denote the Reduced Plank Constant and the mass
of a particle respectively. The function ψ(x) refers to the so-
called wave function and corresponds to the eigenstate of the
given quantum system. The function v(x) denotes the potential
function, and ∇2 is the Laplacian. Conventionally, the potential
v(x) is given and the equation is solved to find solutions ψ(x).
Such a function ψ(x), can be assimilated with the kernel-based
sum which depends on the given data points.
Different from quantum physics, where we want to estimate
the location of particles given their potential function v(x), in
Quantum Clustering we solve the inverse problem. By con-
sidering the wave function ψ(x) as an known condition of the
Schrödinger equation, we aim to determine the potential v(x) in
Quantum Clustering, which characterizes the data probability
density function(Nasios and Bors, 2007). Similar to particles in
quantum physics, data points that are located in close proxima-
tion have close potential values. In this case, we can observe the
analogy between clusters of data points and an electron cloud
of hydrogen atoms illustrated in Fig. 1. The atomic nucleus
corresponds to the cluster center, and each position where the
electron probably appears (white dot) corresponds to each data
point. According to the laws of quantum mechanics, the atomic
system is in the lowest energy level (ground state) when the
electron stays in the lowest orbit. Conversely, if the electron
transits to a higher orbit, the system will be activated to the
higher energy state (excited state). In a similar way, the values
of potential function v(x) calculated from the highest density
data points reach the bottom, and increase with the decline of
density of data points. Therefore, the cluster center could be
revealed by the minima of v(x). That is exactly the most signif-
icant characteristic of v(x) as required.
2.2. Algorithm
The essential part of the algorithm is to calculate the potential
function v(x) by Schrödinger equation. Given Gaussian kernel
as Eq. (2) for the wave function, and m = ~2/σ2, where σ
denotes the width parameter.
ψ (x) =
∑
i
e−(x−xi)
2/2σ2 (2)
Afterwards,we solve Eq. (1) for v(x) by next few steps. First,
the Eq. (1) is rewritten as:
Hψ (x) =
(
−σ
2
2
∇2 + v(x)
)
ψ (x) = Eψ (x) (3)
Then, the v(x) could be solved as:
v (x) = E +
σ2
2 ∇2ψ (x)
ψ (x)
(4)
Further, based on Eq. (1), the first-order derivative of ψ(x) is:
ψ(x)′ =
∑
i
(e
−(x−xi)2
2σ2 · − (x − xi)
σ2
) (5)
And the second-order derivative of of ψ(x) is:
ψ(x)′′ =
∑
i
(e
−(x−xi )2
2σ2 · (x − xi)
2
σ4
− e
−(x−xi )2
2σ2 · 1
σ2
) (6)
ACCEPTED MANUSCRIPT
AC
CE
PT
ED
 M
AN
US
CR
IP
T
4
Thus, the v(x) could be solved as:
v(x) = E +
∑
i(e
−(x−xi )2
2σ2 · (x−xi)22σ2 − e
−(x−xi )2
2σ2 · 12 )
∑
i e
−(x−xi )2
2σ2
= E − 1
2
+
1
2σ2ψ(x)
∑
i
(x − xi)2e
−(x−xi )2
2σ2
≈ 1
2σ2ψ(x)
∑
i
(x − xi)2e
−(x−xi)2
2σ2
(7)
where E is considered as constant, since they do not affect the
topological structure of v(x). Thus, the final version of v(x)
approximates to the last line in Eq. (7).
Generally, after we obtain the v(x), some classic optimiza-
tion approaches can be employed to deduce the clustering al-
location, which is intended to locate the minima according to
their topographic locations on the hypersurface of v(x). In
our study, we utilized the classic BFGS( Broyden-Fletcher-
Goldfarb-Shanno) algorithm which is a kind of Quasi-Newton
Methods (Broyden, 1970) to address the problem. The details
of the BFGS could be found in many academic literatures (e.g.,
(Lewis and Overton, 2009; Byrd et al., 1995)).
2.3. Potential function VS. Gaussian kernel
Like the Gaussian kernel function in Parzen-window estima-
tor, the potential function formulate a hypersurface from the
given data points and play the crucial rule in Quantum Cluster-
ing. Therefore, we compared the potential function v(x) with
Gaussian kernel function ψ(x) by two experiments in order to
reveal the difference between them.
In the first experiment, we utilized a simple artificial dataset
to demonstrate the advantage of potential function. In Fig. 2(a),
this plot corresponds to a smaller value of σ (σ = 2) and
it showed that the Gaussian kernel produce barely discernible
variations corresponding to the location of the first centroid (on
the left side). Nevertheless, in Fig. 2(b), the corresponding
potential function clearly revealed the underlying structure of
the data. Furthermore, we ploted for a 25% larger value of σ
(σ = 2.5), and now the variation of the Gaussian kernel is es-
sentially invisible on the left side in Fig. 2(c). However, the
corresponding potential function still clearly exhibited the lo-
cation of the centroid in Fig. 2(d).
We then extended the comparative experiment to a small sub-
set of a benchmark data set, Reuters-21578. The data set we
tested contained 200 documents belonging to “earnings” cate-
gory and 200 documents belonging to “others” categories. In
this test run, we extracted the features by calculating the χ2
score of each term from the “earnings” category (Manning and
Schütze, 1999). For the sake of compact presentation and vi-
sualization, we employed the Principle Component Analysis
(PCA) to perform dimensionality reduction, and selected the
top two principal components as new features to represent the
data. In panel Fig. 3(a), this plot corresponded to a smaller
value of σ (σ = 2.5) and it is obvious that the ψ(x) showed
barely discernible variations corresponding to the location of
the centroid on the right side, and further, the variation of ψ(x)
was essentially invisible at the corresponding location with a
20% larger value of σ (σ = 3) in Fig. 3(c). Nevertheless,
the potential function v(x) of Quantum Clustering displayed in
Fig. 3(b) and Fig. 3(d) presents the required two minima at the
corresponding locations where the ψ(x) failed to display local
maxima.
3. Comparative Experiment
In this section, to compare the performance of both Quantum
Clustering and Parzen-window estimator on real text clustering
task, both of these are applied to several benchmark datasets.
Furthermore, we also compared them with the common used
DBSCAN algorithm, which is also a density-based clustering
algorithm (Ester et al., 1996). In our task, the lexical unigram
was employed as the textual feature. Furthermore, in prepro-
cessing procedure, the Tf-idf (term frequency inverse document
frequency) technique was used as the weighting factor and the
PCA was also employed for dimensionality reduction. We se-
lected two principle components to map the original data points
to a 2D features spaces for reducing the cost of computational
consumption.
3.1. Evaluation methodology
The commonly used F1 measure that combines the precision
and recall is selected as the evaluation method, and it is calcu-
lated by several steps as follows (Zhou, 2005):
P(P j,Ci) =
∣∣∣P j ∩Ci
∣∣∣
|Ci| (8)
R(P j,Ci) =
∣∣∣P j ∩Ci
∣∣∣
∣∣∣P j
∣∣∣
(9)
F(P j,Ci) =
2P(P j,Ci) × R(P j,Ci)
P(P j,Ci) + R(P j,Ci)
(10)
where the P j represents the clusters annotated in advance, and
the Ci denotes each clusters generated by algorithm. The F1
measure of each P j is the maximum among all candidates as:
F(P j) = max
i=1,2...m
{
F(P j,Ci)
}
(11)
Finally, the overall F1 measure of the whole clustering result
was calculated as:
F =
∑(∣∣∣P j
∣∣∣ × F(P j)
)
∑ ∣∣∣P j
∣∣∣
(12)
3.2. Experiment dataset
The first dataset we employed is the Reuters50 50. It
is the subset of RCV1 which is a benchmark text dataset
obtained from UCI machine learning repository (http://
archive.ics.uci.edu/ml), and has already been used in text anal-
ysis. This dataset consists of 50 authors’ articles, and from this
dataset, we selected 150 documents belonging to three authors:
Aaron Pressman, Alan Crosby and Alexander Smith, each one
containing 50 documents.
ACCEPTED MANUSCRIPT
AC
CE
PT
ED
 M
AN
US
CR
IP
T
5
(a) (b) (c) (d)
Fig. 2. Comparison between potential function and Gaussian kernel on artificial datasets; (a) Gaussian kernel (σ = 2); (b) potential function (σ = 2); (c)
Gaussian kernel (σ = 2.5); (d) potential function (σ = 2.5)
(a) (b) (c) (d)
Fig. 3. Comparison between potential function and Gaussian kernel on subset of Reuters21578; (a) Gaussian kernel (σ = 2.5); (b) potential function
(σ = 2.5); (c) Gaussian kernel (σ = 3); (d) potential function (σ = 3)
Second, the benchmark dataset Reuters21578 was employed.
We selected documents from 8 topics: coffee, corn, cpi, crude,
gold, ship, trade and wheat, 2262 documents in total. This
dataset is also available from the UCI machine learning reposi-
tory.
Finally, we employed the Chinese oral conversation corpus
which was gathered by the laboratory of computational linguis-
tics of Tsinghua University. This dataset consists of 460 texts
which were collected from daily, real-life conversations, and
were categorized into 9 classes according to the locations where
the conversations took place, (e.g. hospital, restaurant, ticket
office, airport, book store, bank, shopping mall, travelling and
embassy).
3.3. Results
The experimental results are illustrated in Fig. 4. We evalu-
ated the performance with a range of σ which cover the whole
cycle in a clustering task in order to compare the two methods
from holistic view. It helped us understand the global perfor-
mance of Quantum Clustering.
As can be seen, Quantum Clustering is better compared
to Parzen-window in achieving optimal F1 in all cases. In
Reuters50 50 experiment, Quantum Clustering demonstrated
a significant advantage (optimal F1=0.76) over the one (op-
timal F1=0.58) Parzen-window reached. In the experiments
on Reuters21578 and Oral conversation corpus, the optimal F1
Quantum Clustering obtained also surpassed the one Parzen-
window obtained (0.57 vs 0.50, 0.77 vs 0.75). Both of these
methods have an upper limit of σ (maximum value). Since the
potential function/ Gaussian kernel function will form a single
valley/peak if the σ exceeds the upper limit, all of the data will
be clustered into one group. Both the optimal F1 and the upper
limit of σ are marked in each panel.
Furthermore, we performed the DBSCAN algorithm on these
three datasets and concluded the optimal F1 in Table 1. Clearly,
the Quantum Clustering also exhibit significant advantage over
it.
3.4. Parameter estimation
The shape of the potential function is determined by the
width parameter σ as in Eq. (7). Although we can adjust the
clustering results by turning up or turning down the σ, it is im-
portant to find an optimal σ in order to obtain a reliable result,
such as we observed form the Fig. 4. Some previous studies
proposed the statistical approach of KNN(k-nearest neighbors)
in estimating the σ (Nasios and Bors, 2007, 2005). Different
from that, we utilized another flexible and easy to understand
method named Pattern Search to tackle this problem.
Pattern Search (PS) is a widely used numerical optimization
method. It does not require the gradient of the objective func-
tion and it can be used on functions which are discontinuous
and non-differential. Hence, it could be an appropriate solution
for the σ estimation which is considered as discontinuous op-
timization. Some previous studies built the solid algorithmic
foundation for the Pattern Search(Audet and Dennis Jr, 2002;
Torczon, 1997). Based on these, we set up the framework for
the parameter estimation. Furthermore, as we know, the initial-
ization (choosing starting point) also has much effect on Pattern
Search method. Therefore, we first calculated the Euclidean
distance between each data point, and selected the average dis-
tances as the starting point. In our experiments, we conducted
10 trials for each experiment by adding a random small value
ACCEPTED MANUSCRIPT
AC
CE
PT
ED
 M
AN
US
CR
IP
T
6
(a) (b) (c)
Fig. 4. Comparison between Quantum Clustering and Parzen-window estimator; (a) Reuters50 50 datasets; (b) Reuters21578 datasets; (c) Oral conversa-
tion corpus. We marked the upper limit of σ on all panels. It means that the clustering results will be stabilized if the σ exceed the upper limits. In our
experiments, it is enough to cover the whole cycle of the Quantum Clustering with a range of σ from 0 to 3.
Table 1. Details of experiments on the three datasets
Dataset Quantum Parzen DB
Clustering Window SCAN
Reuters Optimal F1 0.76 0.58 0.52
50 50 upper limit of σ 1.6 1 /
σ optimized 1.29 / /
by PS
Reuters Optimal F1 0.57 0.50 0.21
21578 upper limit of σ 2.4 1 /
σ optimized 0.56 / /
by PS
Oral Optimal F1 0.77 0.75 0.28
corpus upper limit of σ 2.8 1.6 /
σ optimized 1.15 / /
by PS
on the average distance as the starting point. In Fig. 5(a), each
curve is ploted in an descending order, and the corresponding
absolute value of σ are presented in the Fig. 5(b). As we ob-
served, in general, it is enough to ensure that we could find
the optimal parameter σ. Moreover, another advantage of this
framework is that the objective function could be replaced by
another evaluation method, but not always F1 measure.
4. Application to Authorship Identification
Authorship identification is a process of identifying the most
likely author of a disputed or anonymous document. It is a use-
ful technique to identify the original resource of online mes-
sage, such as junk mail, offensive messages in order to pre-
dict and prevent cybercrime (Tan and Tsai, 2010; Abbasi and
Chen, 2006; Stamatatos, 2009; Zheng et al., 2006). In addition,
it is considered as a novel approach to analyze the authorship
of anonymous or disputed literature which continues to cause
much confusion especially in historical research. In our study,
we applied Quantum Clustering to conduct authorship identifi-
cation on the Dream of the Red Chamber, which is one of the
most famous classical works of fictions in China. Convention-
ally, authorship identification is based on the analysis of known
documents, that is, it can be regarded as a supervised, multi-
class, single-label text categorization task. However, the unsu-
pervised clustering approach is often used to automate the ex-
ploration of the hidden relationships among documents. Thus
it is considered as a potential approach to estimate the author-
ship of documents(Juola, 2006). The character N-grams are a
widely adopted approach to represent text for stylistic purposes
since it is capable of capturing nuances in lexical, syntactical,
and contextual level. The character N-grams of fixed length
has been used for authorship identification (Houvardas and Sta-
matatos, 2006). In our task, the Part-of-Speech (POS) tri-gram,
four-gram and five-gram were employed to represent the texts.
4.1. Dream of the Red Chamber
Dream of the Red Chamber, composed by Cao Xueqin (Tsao
Hsueeh-Chin) in the middle of the 18th century during the Qing
Dynasty, is a semi-autobiographic novel and belongs to one
of ”The Four Classical Novels”. With a broad scope, well-
balanced structure and delicate detail, this book is also called
the “Encyclopedia of Chinese Feudal Society”. After Cao Xue-
qin died, only the first 80 chapters were released to the public
and the remaining chapters were lost. Afterwards, many re-
searchers believed that Gao E, a writer who lived in the 18th
century, added 40 additional chapters to complete the novel.
However, other researchers who doubted this conclusion, and
they believe that the entire 120 chapters of Dream of the Red
Chamber were composed by only one author, Cao Xueqin.
Thus, people have continued trying to find new evidence to con-
clusively confirm authorship. In our study, we subjected this
work to text clustering analysis.
The electronic edition of this fiction was obtained from the
SINA ishare corpus. The Chinese word segmentation was per-
formed by ICTCLAS, which is the state of the art Chinese word
segmentation, tokenization, and part-of-speech tool (Institute
of Computing Technology, Chinese Lexical Analysis System,
http : //www.ictclas.nlpir.org/). Afterwards, each chapter of
the fiction was formalized as a feature vector.
ACCEPTED MANUSCRIPT
AC
CE
PT
ED
 M
AN
US
CR
IP
T
7
(a) (b)
Fig. 5. Parameter estimation by Pattern Search on 10 trials; (a) Optimized F1 measure ploted in an descending order; (b) Corresponding optimized
absolute value of σ. The numerical optimization method probably find the negative value of the σ, therefore, we presented the absolute value of σ
.
(a) (b) (c)
Fig. 6. Experiments on Dream of the Red Chamber; (a) POS tri-gram (variance = 88%, σ = 3.2); (b) POS four-gram (variance = 85%, σ = 3.5); (c) POS
five-gram (variance = 94%, σ = 2.7)
4.2. Results
We conducted a series of experiments. Similar to the last
experiment, we mapped the data points to the subspace by uti-
lizing PCA for the dimensional reduction. In our testing, we
selected leading 10 principle components which contain more
than 85% variance in a data set. Afterward, we performed the
Quantum Clustering algorithm and the clustering results were
plotted in Fig. 6. We observed that two main clusters emerged
from the clustering results and were marked by the ellipse line.
It means that the Quantum Clustering revealed the hidden rela-
tionship among the data points. These clustering results clearly
demonstrated the differences in the writing style between the
first 80 and the second 40 chapters. To some extent, it seems
to support the conventional wisdom that the entire novel was
not written by a single author, that is, the additional 40 chap-
ters were not writed by the same author who wrote the first 80
chapters. However, in spite of this evidential support, we cannot
fully deduce the real author of the second 40 chapters affirma-
tively in this study. Hopefully, we can find out if we obtained
more literature works of Gao E in future study.
5. Discussion
Based upon our analysis as shown in this paper, we conclude
that QC holds significant advantages as follows.
1. As an improvement upon the conventional Parzen window
estimator, Quantum Clustering performs better in achieving op-
timal clustering result than both Parzen window and DBSCAN
algorithm, and has distinct advantages in some cases. It benefits
from the potential function may have minima at points where
Gaussian kernel exhibits no corresponding maxima. That is,
the potential function is more sensitive to variations of the den-
sity of the data than the Gaussian kernel. Furthermore, as σ
decreased, more minima appeared in potential function, while
the previous minima become deeper.
2. In contrast to Parzen window, added benefit of Quantum
Clustering is that its clustering results are less sensitive to the
choice of parameter σ. In our comparative experiments, it was
clear that the tiny variation of σ has no effect on producing the
cluster centers by potential function, but led to the Gaussian
kernel losing the corresponding cluster centers.
3. Unlike some random clustering algorithms such as K-
means, Quantum Clustering generate determinate clustering re-
sults. That is, the clustering result generated by Quantum Clus-
tering is well-determined with the fixed parameter σ. More-
over, this novel method’s emphasis is on cluster centers, rather
than cluster boundaries. Since the equipotential of potential
function may take arbitrary shapes (Horn and Gottlieb, 2001a),
the clusters are not spherical as in the K-means algorithm. Such
as the case we presented in Fig. 3, the equipotential of potential
function depict the data points distributed in a long and narrow
area more effectively.
4. Quantum Clustering has only one free parameter σ, which
controls the width of potential function we search for. An ap-
propriate σ depends on the distance between each data point
in a cluster. We can utilize the numerical optimization Pattern
ACCEPTED MANUSCRIPT
AC
CE
PT
ED
 M
AN
US
CR
IP
T
8
Search to find an optimal σ. However, we could in some cases
explore a more subtle structure of the documents and pursue
different clustering results on various levels of hierarchy by ad-
justing the σ, rather than a rigid clustering result.
In addition to the superiority over the Parzen-window estima-
tor and the DBSCAN algorithm, another advantage is the rela-
tion between the unsupervised machine learning and the funda-
mental equations of quantum physics. It describes the “energy”
distribution of data by potential function, and the process of
clustering can be considered as convergence of “energy” of the
whole system. Thus, it offers us new perspectives of machine
learning technology. Potentially, fresh methodologies or tools
could be developed using this approach to text mining and other
areas of machine learning.
Acknowledgments
I would like to thank Dennis Hejna for improving the lan-
guage of the manuscript. This work was supported by the Na-
tional Natural Science key Fund (61433015), and National So-
cial Science Major Fund (14ZDB154) of China.
References
Abbasi, A., Chen, H., 2006. Visualizing authorship for identification, in: Intel-
ligence and Security Informatics. Springer, pp. 60–71.
Audet, C., Dennis Jr, J.E., 2002. Analysis of generalized pattern searches.
SIAM Journal on Optimization 13, 889–903.
Broyden, C.G., 1970. The convergence of a class of double-rank minimization
algorithms 1. general considerations. IMA Journal of Applied Mathematics
6, 76–90.
Byrd, R.H., Lu, P., Nocedal, J., Zhu, C., 1995. A limited memory algorithm for
bound constrained optimization. SIAM Journal on Scientific Computing 16,
1190–1208.
Di Buccio, E., Di Nunzio, G.M., 2011a. Distilling relevant documents by means
of dynamic quantum clustering, in: Advances in Information Retrieval The-
ory. Springer, pp. 360–363.
Di Buccio, E., Di Nunzio, G.M., 2011b. Envisioning dynamic quantum cluster-
ing in information retrieval, in: Quantum Interaction. Springer, pp. 211–216.
Ester, M., Kriegel, H.P., Sander, J., Xu, X., 1996. A density-based algorithm
for discovering clusters in large spatial databases with noise., in: Kdd, pp.
226–231.
Feynman, 1965. Lectures on Physics Vol 3, Exercises/1965. Addison Wesley
Publishing Company Incorporated.
Horn, D., Gottlieb, A., 2001a. Algorithm for data clustering in pattern recog-
nition problems based on quantum mechanics. Physical Review Letters 88,
018702.
Horn, D., Gottlieb, A., 2001b. The method of quantum clustering., in: NIPS,
pp. 769–776.
Hotho, A., Staab, S., Stumme, G., 2003. Ontologies improve text document
clustering, in: Data Mining, 2003. ICDM 2003. Third IEEE International
Conference on, IEEE. pp. 541–544.
Houvardas, J., Stamatatos, E., 2006. N-gram feature selection for authorship
identification, in: Artificial Intelligence: Methodology, Systems, and Appli-
cations. Springer, pp. 77–86.
Juola, P., 2006. Authorship attribution. Foundations and Trends in information
Retrieval 1, 233–334.
Lewis, A.S., Overton, M.L., 2009. Nonsmooth optimization via bfgs. Submit-
ted to SIAM J. Optimiz .
Liu, D., Jiang, M., Yang, X., 2014. Quantum clustering &# x2014; a novel
method for text analysis, in: Computational Intelligence and Data Mining
(CIDM), 2014 IEEE Symposium on, IEEE. pp. 17–23.
Manning, C.D., Schütze, H., 1999. Foundations of statistical natural language
processing. MIT press.
Nasios, N., Bors, A.G., 2005. Nonparametric clustering using quantum me-
chanics, in: Image Processing, 2005. ICIP 2005. IEEE International Confer-
ence on, IEEE. pp. III–820.
Nasios, N., Bors, A.G., 2007. Kernel-based classification using quantum me-
chanics. Pattern Recognition 40, 875–889.
Stamatatos, E., 2009. A survey of modern authorship attribution methods. Jour-
nal of the American Society for information Science and Technology 60,
538–556.
Tan, R.H.R., Tsai, F.S., 2010. Authorship identification for online text, in:
Cyberworlds (CW), 2010 International Conference on, IEEE. pp. 155–162.
Torczon, V., 1997. On the convergence of pattern search algorithms. SIAM
Journal on optimization 7, 1–25.
Weinstein, M., 2010. Strange bedfellows: Quantum mechanics and data min-
ing. Nuclear Physics B-Proceedings Supplements 199, 74–84.
Weinstein, M., Horn, D., 2009. Dynamic quantum clustering: A method for
visual exploration of structures in data. Physical Review E 80, 066117.
Weinstein, M., Horn, D., et al., 2008. Dynamic Quantum Clustering: A Tool for
Unsupervised Exploration of Structures in Data. Technical Report. Stanford
Linear Accelerator Center (SLAC).
Weinstein, M., Meirer, F., Hume, A., Sciau, P., Shaked, G., Hofstetter, R., Persi,
E., Mehta, A., Horn, D., 2013. Analyzing big data with dynamic quantum
clustering. arXiv preprint arXiv:1310.2700 .
Wittek, P., 2013. High-performance dynamic quantum clustering on graphics
processors. Journal of Computational Physics 233, 262–271.
Xu, W., Liu, X., Gong, Y., 2003. Document clustering based on non-negative
matrix factorization, in: Proceedings of the 26th annual international ACM
SIGIR conference on Research and development in informaion retrieval,
ACM. pp. 267–273.
Zamir, O., Etzioni, O., 1998. Web document clustering: A feasibility demon-
stration, in: Proceedings of the 21st annual international ACM SIGIR con-
ference on Research and development in information retrieval, ACM. pp.
46–54.
Zheng, R., Li, J., Chen, H., Huang, Z., 2006. A framework for authorship
identification of online messages: Writing-style features and classification
techniques. Journal of the American Society for Information Science and
Technology 57, 378–393.
Zhou, 2005. Quality Evaluation of Text Clustering Results and Investigation
on Text Representation. Master’s thesis. University of Chinese Academy of
Sciences. Beijing.
