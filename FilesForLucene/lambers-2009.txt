Forensic Authorship Attribution Using
Compression Distances to Prototypes
Maarten Lambers2 and Cor J. Veenman1,2
1 Intelligent Systems Lab,
University of Amsterdam, Amsterdam, The Netherlands
2 Digital Technology & Biometrics Department,
Netherlands Forensic Institute, The Hague, The Netherlands
Abstract. In several situations authors prefer to hide their identity. In forensic
applications, one can think of extortion and threats in emails and forum messages.
These types of messages can easily be adjusted, such that meta data referring to
names and addresses is at least unreliable. In this paper, we propose a method
to identify authors of short informal messages solely based on the text content.
The method uses compression distances between texts as features. Using these
features a supervised classifier is learned on a training set of known authors. For
the experiments, we prepared a dataset from Dutch newsgroup texts. We com-
pared several state-of-the-art methods to our proposed method for the identifica-
tion of messages from up to 50 authors. Our method clearly outperformed the
other methods. In 65% of the cases the author could be correctly identified, while
in 88% of the cases the true author was in the top 5 of the produced ranked list.
1 Introduction
Besides the many legitimate uses of the internet, it also has become a communication
medium for illegal activities. Examples include child pornography distribution, threat-
ening letters and terroristic communications. Finding the illegal distributor or the writer
of illegal content is an important aspect of the efforts of the forensic experts to reduce
criminal activity on the internet.
In this research, we focus on the analysis of short messages such as e-mail, news-
group and forum messages. To determine the source of such a communication item, the
most straightforward approach seems to use the addressee from the meta data of the
message. Unfortunately this information is very unreliable, because this type of data
can be changed easily at will. Moreover, even if the addressee was an email address of
the true sender, the address and identity can still be meaningless. Obtaining an arbitrary
email account name from a webmail provider can be done easily and anonymously.
Another option is to focus on network traffic, i.e. to find out how a message was
routed or to monitor the IP addresses e.g. of the users accessing a terrorist website
[1]. While this can provide important clues in headers and meta data, there are also
several ways to disguise the source of a packet, i.e. message. Another downside of
this approach is that only a link from a message to a computer can be established. For
forensic purposes it is more useful to attribute the authorship of a message, that is to
link the writings to the actual writer.
Z.J.M.H. Geradts, K.Y. Franke, and C.J. Veenman (Eds.): IWCF 2009, LNCS 5718, pp. 13–24, 2009.
c© Springer-Verlag Berlin Heidelberg 2009
14 M. Lambers and C.J. Veenman
Most of the early research in the field of authorship attribution deals with the dis-
puted authorship of papers or books [2], the best known example being the case of the
Federalist Papers (see for example [3] and [4]). In the forensic domain, where texts are
typically relatively short and the number of potential authors relatively high, different
methods are required [5], [6], [7], [8], [9]. Expectedly, the reported performances in
these works are dependent on the number of authors used in the experiments [10]. The
model from [9] has an accuracy of 75% on a 20-class problem, whereas an accuracy of
90% is reported in [5] on a 5-class problem. From [8] can be learned that an author is
more difficult to categorize compared to other authors when fewer texts are available of
that particular author. In [7] it is concluded that 20 messages containing approximately
100 words are sufficient for authorship attribution. However, these results are based on
experiments conducted on e-mail messages from only four different authors. Accord-
ingly, the researches mentioned are not yet useful for forensic applications, considering
the accuracies and the constraints; either the accuracy is too low (30% in [6]) or the
number of classes is too low (three to five in [5], [8] and [7]). An appropriate method
must be able to handle tens of different authors and must have a higher accuracy.
In this paper, we deal with a scenario that is more realistic from a forensic perspec-
tive. We propose a method for author attribution from a set of 50 known authors, which
is a lot more than in previously reported research. Such a scenario fits a situation were a
substantial though limited list of targets is followed for tactical investigations. As [11],
[12], [13], the proposed method is based on compression distances between texts. To
cope with the computational as well as modelling problems that typically arise with
these instance based approaches [10], we propose to use a set of texts as prototypes to
which the compression distances are computed. These distances serve as feature repre-
sentation which allows for the training of supervised classification models. We further
use ranked lists as output of the models to fit the investigation scenario and accommo-
date for some classification errors. That is, in case of classification error the target is
likely one of the following candidates on the ranked list.
The next section, Section 2, formally defines the problem, where the constraints
of this research are elaborated. This is followed by an overview of related work on
authorship attribution in Section 3. In Section 4, we describe our proposed method. The
experiments are described subsequently in Section 5, including data description, feature
extraction and the results. A discussion of the results with pointers for additional work
concludes this paper in Section 6.
2 Problem Statement
The goal of this research is to assign an author’s identity to a text message without the
use of any meta data. The list of potential authors is considered known. This is known
as a closed set recognition problem [14].
Further, we constrain the research to short and informal messages. E-mails as well
as newsgroup or forum messages are considered applicable data sources. These exam-
ples can be seen as similar data considering the informality and use of such messages.
However, the meta data and the text structure can be different. That is, features based
on attachments (as in [8]) are not applicable on forum messages and features based on
Forensic Authorship Attribution Using Compression Distances to Prototypes 15
fonts (the size or the colour of the font, as in [5]) might not be applicable to e-mails.
This constraint restricts the problem to basic text content (UTF-8).
For investigation purposes, typically the required output is a ranked list. Therefore,
also for the author attribution problem the output should be a ranked list. That is, ranked
in order of relevance, where the most probable author is ranked first.
3 Text Features
In order to learn a classification model for authorship attribution, we need a represen-
tation of the texts that enables to differentiate one author from the other. Traditionally,
the objects, here texts, are represented using a set of properly chosen features. Approx-
imately 1,000 features or style markers are known in the field of authorship attribution
that have proved to be successful in certain scenarios [15]. We describe them shortly and
focus on those features that we use in our experiments. We follow the feature categories
as proposed in [10]. Slightly different categorizations have been proposed elsewhere
[16], [17], [9].
3.1 Lexical Features
Lexical features describe a range of ratios, averages, lengths, counts and frequencies
of words, lines and characters. They are supposed to profile the author’s vocabulary
richness computed as the ratio between the number of different words and the total
number of words [18], statistics about the sentences as average length of sentences,
number of sentences [9], and relative frequencies of special characters (punctuation
characters, (tab) spaces, digits and upper- and lowercase characters) [19].
3.2 Syntactic Features
Syntactic features refer to the patterns used for the formation of sentences, using part-
of-speech tagging and function words. These features are considered beyond an author’s
conscious control, for instance short all-purpose words [12]. The author thus leaves a
writeprint [17] in his or her text by using certain words in certain frequencies. The
term function words is referred to by many papers in the field of text mining: [16], [6],
[20], [7], [8], [21], [22], [17], [4], [23], [24], [9]. This term describes words that do
not contain information about the document’s content such as prepositions, pronouns
or determiners [22] or can be defined as words that are context-independent and hence
unlikely to be biased towards specific topics [25]. Previous studies show the value of
features based on these function words. In [7], they are referred to as the best single
discriminating feature. The number of function words used in experiments reported
range from 50 words [20] to 365 words [24].
3.3 Semantic Features
Using Natural Language Technology (NLP) additional information about texts can be
obtained, such as part-of-speech tags. This can give insight in the use of different classes
16 M. Lambers and C.J. Veenman
of words (noun, verb, adjective, etc.). Having semantic knowledge about the text can
also help identifying synonyms, hypernyms or antonyms, thus gaining knowledge about
the context of the text on which features can be based.
3.4 Application Specific Features
Application specific features contain structural features and content-specific features.
Structural features are based on the structure and layout of a text. This includes infor-
mation about indentation, paragraph lengths and the use of a greeting statement [9], or
the font size or font colour, the use of links and the use of images [5]. Content-specific
features denote the use of certain key words belonging to the subject of the texts. This
can be of use when all the available texts discuss the same topic. Carefully selected
content-based information may reveal some authorial choices [10].
3.5 Character Features
The use of character n-grams in the field of authorship attribution is proposed in [7]
and [26]. Character n-grams are simply substrings of the original text. When n = 2, the
word ’text’ results in the character bi-grams ’te’, ’ex’ and ’xt’. The features generated
from these bi-grams are typically the relative frequency of occurrence within a text.
Some experiments have used only the 26 characters of the alphabet [26] while others
also use punctuation characters [7].
In the research described in [26], a study in authorship attribution is conducted where
only features based on character n-grams are used. The goal of the investigation was
to find the optimal number n. The characters that are used in [26] only consist of the
26 character of the alphabet. The experiments that are conducted let n run from 1 to
10. The other variable in these experiments is the profile size L, denoting the number
of features. With character 1-grams, the number of dimensions representing the object
is only 26. With every increase of variable n, the number of dimensions multiplies
with 26, making character 3-gram features already computationally very complex. The
profile size L limits the number of dimensions by using only the L most frequently used
n-grams. Generally, more dimensions results in a higher accuracy according to these
experiments. However, the complexity of the problem reduces the accuracy, when the
number of dimensions becomes too high. Experiments on English data suggest the use
of 5 to 7-grams, while experiments on Greek data slightly differ. When considering a
profile size of 500, which seems a reasonable amount of dimensions, 2 or 3-grams work
best [26], [27].
Compression-based Approaches. Another type of methods categorized as exploiting
character features are compression-based approaches [11], [28]. They can be consid-
ered character features, because compression models also describe text characteristics
based on repetition of character sequences. The basic idea is that texts that have similar
characteristics should be compressed with each others compression model (dictionary)
easily. That is, without the need for substantial extension of the compression model
(dictionary).
Forensic Authorship Attribution Using Compression Distances to Prototypes 17
4 Compression Distances to Prototypes (CDP)
Compression-based similarity methods are motivated from the Kolmogorov complex-
ity theory [29]. They have recently gained popularity in various domains, ranging from
the construction of language trees [30], the phylogenetic tree [31] and plagiarism de-
tection [32]. The compression-based approaches are practical implementations of the
information distances expressed in the non-computable Kolmogorov complexity. Sev-
eral compression distances have been defined [11], [33], [28], [13], [34], [35].
We follow the Compression Distance Metric (CDM) defined in [13]:
CDM(x,y) =
C(xy)
C(x) + C(y)
, (1)
where C(x) is the size of the compressed object x and xy is the concatenation of x
and y.
The CDM expresses the compression distance between two objects x and y as the
difference in compressed length of the concatenation of the two objects xy divided by
the sum of the length of the individually compressed objects, which can be computed
easily using various compressors. If objects x and y are similar, then CDM(x,y) < 1.
The more similar x and y are, the smaller the CDM value becomes. While it approaches
0.5, when x and y are equal. On the other hand the CDM will approach 1 for objects
that are completely unrelated [13]. Clearly, these properties of CDM depend on the
quality of the compressor.
Having a distance metric to find similar objects, a straightforward approach to rec-
ognize unseen objects is to attribute them to the author of the most similar object in
the training database. In other words, the nearest neighbour classification method can
be applied. This instance-based approach [10] is followed by [11]. There are, however,
some downsides to the use of the nearest neighbour rule. When using this approach, to
classify an unseen object x, the distances between x and all training objects need to be
calculated. This is a computationally expensive procedure. Another problem is that the
nearest neighbour approach runs the risk of overfitting [36]. Alternatively, all training
texts from the same category, here an author, can be merged. In such a profile-based
approach [10], the compression distance is computed between the unseen text and the
merged author corpus. Unseen texts are then attributed to the closest author corpus [12].
This can be seen as converting all text prototypes into one, similar to the nearest mean
classifier approach.
We propose another way to reduce the computational load and the overfitting. We
construct a feature vector from the distances to the training texts, a dissimilarity-based
classification approach [37]. As a result, the feature vector could become as long as
the number of objects in the training set. This poses, however, new computational and
learning problems. Still, the distances have to be computed from a test text to all training
database texts. Moreover, the resulting enormous feature vectors make the learning of
models costly. More importantly, the feature space will be too sparsely sampled leading
to dimensionality curse problems. While dimension reduction techniques and robust
classifiers could be considered, that is deemed outside the scope of this work.
Instead, in our approach we first select a subset of prototypes of the training set. Then,
for each training text, the CDM to all prototypes is computed. Accordingly, we obtain
18 M. Lambers and C.J. Veenman
a feature vector consisting of distances to the prototypes. The idea of using a subset as
prototypes is that such a subset of the data can describe the training set sufficiently [38].
It should be noted that for dissimilarity-based feature vectors, prototype selection is the
same as feature selection.
Once a feature vector is computed with distances to the prototypes, any feature based
classifier can be applied. Since our problem is author attribution for a closed set of
authors, classifiers that are applicable to multi-class problems are preferred. Though
also combining schemes can be considered, such as one-versus-all and one-versus-one.
5 Experiments
In order to test the relative performance of the proposed Compression Distance to Pro-
totypes (CDP) method, we need to compare it to other methods known in the literature.
The related methods that we consider in the comparison are based on feature sets as
described in Section 3 and further specified in this Section. Besides a feature represen-
tation, a classifier has to be chosen. Based on unreported experiments, we have selected
the Fisher discriminant classifier (e.g. [39]) for all methods and experiments. Its per-
formance is competitive, while it has also computational attractive properties. Below,
we first describe the experimental dataset and the required preprocessing. Then, we
elaborate on the compared feature sets. We also describe additional details that need
specification for practical application of the proposed method. We further describe the
evaluation protocol and the actual experiments.
5.1 Dataset and Preparation
The data we have used in the experiments originates from Dutch newsgroups. They are
mostly informally written and are varying in length. We collected the data from four
newsgroups, covering the subjects of politics, jurisdiction, motorbikes and cars. These
newsgroups are chosen because the contributors to these newsgroups are assumed to
discuss serious subjects and have no intention to fake their identities, which makes the
author labels reliable.
We selected messages of authors that have sent at least 100 messages. This resulted
in a dataset with 130 authors with in total 55,000 messages.
Fig. 1(a) shows the distribution of authors (the vertical axis) that have written a
certain number of messages. As can be seen in the figure, most authors have written
around 300 messages and only few persons have written more than 1000 messages. The
length of these messages is dispersed in a similar fashion: the bulk of messages are short
(88% of the messages have less than 100 words), see Fig. 1(b).
The newsgroup messages that have been used in the experiments have been prepared
in order to remove headers, quoted text, greetings and signatures. No text is allowed to
have a direct link to the author; the resulting text should only consist of the body of the
message. Signatures are replaced by the string ’signature’, any the name or nickname of
the author is replaced by respectively ’Firstname’, ’Surname’ or ’Nick’. This is chosen
over deleting this information in order to be able to distinguish between authors who do
use a signature/greeting and authors who do not.
Forensic Authorship Attribution Using Compression Distances to Prototypes 19
0 500 1000 1500 2000 2500 3000 3500 4000
0
5
10
15
20
25
30
35
40
45
Number of messages per author
N
um
be
r 
of
 a
ut
ho
rs
(a) Histogram messages per author
0 50 100 150 200 250
0
1000
2000
3000
4000
5000
6000
7000
8000
Number of words
N
um
be
r 
of
 m
es
sa
ge
s
(b) Histogram of message lengths
Fig. 1. Statistics of the dataset
Randomly sampling the prepared messages confirms the absence of headers etc., al-
though some other unwanted text is sometimes included in the resulting bodies. Quotes
from previous messages on the newsgroup are preceded by characters such as ’>’, but
when a user pastes a newspaper article or law book excerpt in their message, this text
is not automatically recognized. In roughly three to four messages in every 100, pasted
text is found. This makes the authorship attribution problem a bit more difficult, but
checking every message by hand is not feasible.
5.2 Evaluation Protocol
For the performance evaluation of the different methods, we have used 10-fold cross-
validation repeated 20 times. The reported performance is the fraction of messages for
which the author is correctly classified. That is, the messages for which the correct
author is ranked first in the yielded ranked list (the required output). We also use the
Cumulative Match Characteristic (CMC) Curve to report system performance [14]. This
curve plots the rank against the fraction of messages recognized below and including
that rank.
5.3 Compared Feature Sets
We created three feature sets to compare the proposed method to. These feature sets
are based on features reported in the literature and fit reasonably to the taxonomy as
described in Section 3.
Lexical Features. We based our lexical features on [19] and [9] that both used features
that measure the vocabulary richness from [18]. These features are based on the number
of different words and the total number of words in a text. Furthermore the lexical
feature set contains the frequency of punctuation characters, the word length histogram
ranging from 1 to 30 characters which delivers 30 features, and the average sentence
and word length.
20 M. Lambers and C.J. Veenman
Syntactic Features. The syntactic features that we used are function word features.
Some papers list a predefined set of function words [40], [9]. Others simply use the
most frequently used words as function words [4], [18]. The first approach requires a
validated word list per language. This makes the latter approach more generic. Although
it may seem that mainly context-dependent words are found as most frequent, the con-
trary has been found in [20]. Since this is also our experience, we used this generic
approach.
Character Features. We used character 2-grams as character features as in [7], [27],
and [26]. The 26 characters of the alphabet together with the space character generates
a total of 27 ·27 = 729 features. We did not include punctuation characters. The feature
value is the number of occurrences of the respective character combination divided by
the total number of characters in the message. There has been no selection of the most
discriminating bi-grams, as was exercised in [26].
5.4 Parameters and Tuning
For the syntactic features, the optimal number of function words can be tuned. For the
tuning we sampled datasets with the messages of 50 authors, 50 messages per author
and a minimum message length of 50 words. In Fig. 2(a), the cross-validated average
classification performance is displayed as a function of the number of function words.
For this problem setting the optimum number of function words is around 600.
For the CDP method, there are two parameters to settle in order to obtain a com-
pression-based set of distances to prototypes. First, a proper compressor has to be found.
The most suitable compressor is probably domain dependent. In some studies the RAR
scheme [41] has been found a good general purpose candidate [42], [34]. For practical
implementation reasons we applied the LZ76 compression method [43], [44]. The other
important parameter is the number prototypes and the way they are selected. Various
prototype selection schemes have been researched and compared [45], [46]. Because
random sampling is competitive in performance to advanced and computationally in-
volved methods [45], [46], we applied random prototype selection.
For the tuning of the number of prototypes, we used the same set-up as for the func-
tion words tuning, that is, samples of 50 authors, 50 messages per author and a minimum
message length of 50 words. Fig. 2(b) shows the cross-validated average classification
performance. With more than fifteen prototypes the performance is maximal. Conse-
quently, fifteen is the most suitable number of prototypes for this problem setting. In a
similar fashion the number of prototypes is determined when 50 authors all contribute
90 messages of at least 50 words in length.
5.5 Results
Together with the three feature sets as described above, the proposed CDP method is
put to the test. In a direct competition, our method shows a significantly better result
compared to the other feature sets, of which the lexical features rank second best, see
Fig. 3(a). The difference in performance is very noticeable: an authorship attribution
system using the lexical features correctly identifies the true author in 40% of the cases,
Forensic Authorship Attribution Using Compression Distances to Prototypes 21
0 200 400 600 800 1000 1200 1400 1600 1800 2000
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
Number of function words
E
st
im
at
ed
 p
er
fo
rm
an
ce
(a) Variable number of function words
0 5 10 15 20 25 30 35 40
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Number of prototypes
E
st
im
at
ed
 p
er
fo
rm
an
ce
(b) Variable number of prototypes
Fig. 2. Parameter tuning graphs with average performance and standard error bars
0 5 10 15 20 25 30 35 40 45 50
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Ranking
C
um
ul
at
iv
e 
M
at
ch
 C
ha
ra
ct
er
is
tic
Character
Lexical
Syntactic
CDP
(a) 50 messages per author
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
C
um
ul
at
iv
e 
M
at
ch
 C
ha
ra
ct
er
is
tic
 
Character
Lexical
Syntactic
CDP
0 5 10 15 20 25 30 35 40 45
Ranking
50
(b) 90 messages per author
Fig. 3. Systems performance displayed as Cumulative Match Characteristic (CMC) Curves. The
messages of 50 different authors, contributing messages of at least 50 words in length, are used
for these experiments. (The standard deviation is left out for readability purposes.)
whereas a similar system using compression distances to prototypes scores 60%. Bear
in mind that given the high number of authors, a random classifier would only correctly
identify 2%.
When the resulting top five is inspected, one can see that in 83% of the cases the true
author of a questioned document is found in the five most likely authors using the CDP
method. This in contrast to 70% using a system with the second best set of features.
The steepness of the CMC curve gradually decreases and a certainty nearing the 100%
is only found when the first 30 of the 50 ranked authors are inspected.
The same experiment is repeated with an increase of messages per author from 50 to
90. Fig. 3(b)) shows similar results; again the proposed CDP method outperforms the
known methods. As expected, the system scores better when more messages per author
are available for training.
22 M. Lambers and C.J. Veenman
6 Conclusion
This research shows that the established methods for authorship attribution perform
significantly worse than our proposed CDP feature set. Depending on the dataset, an
increase around 20-25% in accuracy can be measured (see Fig. 3(a) and 3(b)). The
performance gain makes practical forensic applications more feasible. Given a suitable
set of training data, a set of 50 suspected authors can be comprised to only five, with a
certainty near 90%. This heavily reduces the workload. It should be noted that a closed
set situation, as dealt with in this research, is not a completely realistic scenario for
tactical investigators. An interesting continuation of this research could be to shift the
focus of the investigation towards an open set problem. An open set problem does not
give the certainty that the classification model is trained on documents written by the
author of a questioned document, as this research has. An ideal model would then be
able to distinguish between the text of an unknown author and the texts from the dataset
it has trained on, thus identifying the new text as being written by a yet unknown author.
An extension towards an authorship verification model is then also a possibility. In
this case two texts are compared, of which one is of known authorship. The question
then becomes if the second text is written by the same author as the first. From a foren-
sic point-of-view, an authorship verification model would be more suitable for tactical
purposes.
The CDP method is among others proposed as alternative to instance-based ap-
proaches using compression distances. Like profile-based approaches, CDP features
make compression distances computationally feasible. A experimental comparison with
respect to performance between these three compression-based methods is also worth
investigating.
Considering the data used in the experiments as described in this paper, the fact that
messages from only four different newsgroups are used can have a positive influence on
the results. Although authors were found contributing to discussions in multiple groups,
certain authors tend to stick to certain newsgroups. This could work to the advantage
of the classification method, although it is likely that it would not favour any feature
set in particular. As a recommendation for future work, similar experiments could be
conducted on a more diverse set of messages and authors. Another recommendation
for future work is to investigate the proposed method on messages written in another
language (e.g. English).
References
1. Elovici, Y., Kandel, A., Last, M., Shapira, B., Zaafrany, O.: Using data mining techniques
for detecting terror-related activities on the web. Journal of Information Warfare 3(1), 17–29
(2004)
2. Stamatatos, E., Fakotakis, N., Kokkinakis, G.: Computer-based authorship attribution with-
out lexical measures. Computers and the Humanities 35(2), 193–214 (2001)
3. Holmes, D., Forsyth, R.: The federalist revisited: New directions in authorship attribution.
Literary and LInguistic Computing 10(2), 111–127 (1995)
4. Mosteller, F., Wallace, D.: Inference and disputed authorship: the Federalist. Addison-
Wesley, Reading (1964)
Forensic Authorship Attribution Using Compression Distances to Prototypes 23
5. Abbasi, A., Chen, H.: Applying authorship analysis to extremist-group web forum messages.
IEEE Intelligent Systems 20(5), 67–75 (2005)
6. Argamon, S., Sari, M., Stein, S.: Style mining of electronic messages for multiple authorship
discrimination: first results. In: Proceedings of the 9th ACM SIGKDD, pp. 475–480 (2003)
7. Corney, M., Anderson, A., Mohay, G., Vel, O.D.: Identifying the authors of suspect e-mail.
Technical report, Queensland University of technology (2001)
8. Vel, O.D., Anderson, A., Corney, M., Mohay, G.: Mining e-mail content for author identifi-
cation forensics. ACM SIGMOD Record 30(4), 55–64 (2001)
9. Zheng, R., Li, J., Chen, H., Huang, Z.: A framework of authorship identification for online
messages: Writing style features and classification techniques. Journal American Society for
Information Science and Technology 57(3), 378–393 (2006)
10. Stamatatos, E.: A survey of modern authorship attribution methods. Journal of the American
Society for Information Science and Technology 60(3), 538–556 (2009)
11. Benedetto, D., Caglioti, E., Loreto, V.: Language trees and zipping. Phys. Rev. Lett. 88(4),
048702 (2002)
12. Khmelev, D.V., Teahan, W.: A repetition based measure for verification of text collections
and for text categorization. In: 26th Annual International ACM SIGIR Conference on Re-
search and Development in Information Retrieval, Toronto, Canada, August 2003, pp. 104–
110 (2003)
13. Keogh, E., Lonardi, S., Ratanamahatana, C.: Towards parameter-free data mining. In: Pro-
ceedings of the 10th ACM SIGKDD International Conference on Knowledge Discovery and
Data Mining, pp. 206–215 (2004)
14. Bolle, R., Connell, J., Pankanti, S., Ratha, N., Senior, A.: Guide to Biometrics. Springer, New
York (2004)
15. Rudman, J.: The state of authorship attribution studies: Some problems and solutions. Com-
puters and the Humanities 31(4), 351–365 (1998)
16. Abbasi, A., Chen, H.: Visualizing authorship for identification. In: Mehrotra, S., Zeng, D.D.,
Chen, H., Thuraisingham, B., Wang, F.-Y. (eds.) ISI 2006. LNCS, vol. 3975, pp. 60–71.
Springer, Heidelberg (2006)
17. Li, J., Zeng, R., Chen, H.: From fingerprint to writeprint. Communications of the ACM 49(4),
76–82 (2006)
18. Tweedie, F., Baayen, R.: How variable may a constant be? measure of lexical richness in
perspectiv. Computers and the Humanities 32(5), 323–352 (1998)
19. Corney, M.: Analysing e-mail text authorship for forensic purposes. Master’s thesis, Queens-
land University of technology (2003)
20. Binongo, J.: Who wrote the 15th book of oz? an application of multivariate analysis to au-
thorship attribution. Chance 16(2), 9–17 (2003)
21. Gamon, M.: Linguistic correlates of style: authorship classification with deep linguistic anal-
ysis features. In: Proceedings of the 20th International Conference on Computational Lin-
guistics, pp. 611–617 (2004)
22. Kaster, A., Siersdorfer, S., Weikum, G.: Combining text and linguistic document representa-
tions for authorship attribution. In: In SIGIR Workshop: Stylistic Analysis of Text for Infor-
mation Access, pp. 27–35 (2005)
23. Uzuner, O., Katz, B.: A comparative study of language models for book and author recogni-
tion. In: Dale, R., Wong, K.-F., Su, J., Kwong, O.Y. (eds.) IJCNLP 2005. LNCS, vol. 3651,
pp. 969–980. Springer, Heidelberg (2005)
24. Zhao, Y., Zobel, J.: Effective and scalable authorship attribution using function words. In:
Lee, G.G., Yamada, A., Meng, H., Myaeng, S.-H. (eds.) AIRS 2005. LNCS, vol. 3689, pp.
174–189. Springer, Heidelberg (2005)
24 M. Lambers and C.J. Veenman
25. Koppel, M., Schler, J.: Exploiting stylistic idiosyncrasies for authorship attribution. In: Pro-
ceedings of IJCAI 2003 Workshop on Computational Approaches to Style Analysis and Syn-
thesis, pp. 69–72 (2003)
26. Kešelj, V., Peng, F., Cercone, N., Thomas, C.: N-gram-based author profiles for authorship
attribution. In: Proceedings of the Conference Pacific Association for Computational Lin-
guistics, pp. 255–264 (2003)
27. Kjell, B.: Authorship attribution of text samples using neural networks and bayesian classi-
fiers. IEEE International Conference on Systems, Man and Cybernetics 2, 1660–1664 (1994)
28. Li, M., Chen, X., Li, X., Ma, B., Vitányi, P.M.B.: The similarity metric. IEEE Transactions
on Information Theory 50(12), 3250–3264 (2004)
29. Li, M., Vitányi, P.M.B.: An Introduction to Kolmogorov Complexity and its Applications.
Springer, New York (1997)
30. Ball, P.: Algorithm makes tongue tree. Nature (January 2002)
31. Li, M., Badger, J.H., Chen, X., Kwong, S., Kearny, P., Zhang, H.: An information-based
sequence distance and its application to whole mitochondrial genome phylogeny. Bioinfor-
matics 17(2), 149–154 (2001)
32. Chen, X., Francia, B., Li, M., McKinnon, B., Seker, A.: Shared information and program
plagiarism detection. IEEE Transactions on Information Theory 50(7), 1545–1551 (2004)
33. Cilibrasi, R., Vitányi, P.M.B.: Clustering by compression. IEEE Transactions on Information
Theory 51(4), 1523–1545 (2005)
34. Kukushkina, O.V., Polikarpov, A.A., Khmelev, D.V.: Using literal and grammatical statistics
for authorship attribution. Problems of Information Transmission 37(2), 172–184 (2001)
35. Telles, G., Minghim, R., Paulovich, F.: Normalized compression distance for visual analysis
of document collections. Computers and Graphics 31(3), 327–337 (2007)
36. Mitchell, T.: Machine Learning. McGraw-Hill, New York (1997)
37. Pȩkalska, E., Skurichina, M., Duin, R.: Combining fisher linear discriminants for dissimilar-
ity representations. In: Kittler, J., Roli, F. (eds.) MCS 2000. LNCS, vol. 1857, pp. 117–126.
Springer, Heidelberg (2000)
38. Duin, R., Pȩkalska, E., Ridder, D.D.: Relational discriminant analysis. Pattern Recognition
Letters 20(11–13), 1175–1181 (1999)
39. Duda, R., Hart, P., Stork, D.: Pattern Classification. John Wiley and Sons, Inc, New York
(2001)
40. Craig, H.: Authorial attribution and computational stylistics: If you can tell authors apart,
have you learned anything about them? Literary and Linguistic Computing 14(1), 103–113
(1999)
41. Roshal, E.: RAR Compression Tool by RAR Labs, Inc (1993-2004),
http://www.rarlab.com
42. Marton, Y., Wu, N., Hellerstein, L.: On compression-based text classification. In: Advances
in Information Retrieval, pp. 300–314 (2005)
43. Lempel, A., Ziv, J.: On the complexity of finite sequences. IEEE Transaction on Information
Theory 22, 75–81 (1976)
44. Kaspar, F., Schuster, H.: Easily calculable measure for the complexity of spatiotemporal
patterns. Physical Review A 36(2), 842–848 (1987)
45. Kim, S.W., Oommen, B.J.: On using prototype reduction schemes to optimize dissimilarity-
based classification. Pattern Recognition 40(11), 2946–2957 (2007)
46. Pekalska, E., Duin, R., Paclik, P.: Prototype selection for dissimilarity-based classifiers. Pat-
tern Recognition 39, 189–208 (2006)
