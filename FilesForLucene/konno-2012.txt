Early-Vision-Inspired Method to Distinguish
between Handwritten and Machine-Printed
Character Images Using Hough Transform
Yuuya Konno1 and Akira Hirose2
1 Fuji Xerox Co., Ltd,
6–1 MinatoMirai, Nishi, Yokohama, Kanagawa 220-8668, Japan
2 Department of Electrical Engineering and Information Systems,
The University of Tokyo, 7–3–1 Hongo, Bunkyo, Tokyo 113–8656, Japan
sayaka.yamauchi@inf.kyushu-u.ac.jp
Abstract. This paper proposes a method to distinguish handwritten
character regions from machine-printed ones using Hough transform (HT).
The Gabor filtering in the human early vision realizes a type of Fourier
transform (FT). Previously we proposed a FT-based distinction method
successfully. However, we noticed simultaneously that the HT, instead
of FT, may extract more features when we deal with characters which
are regarded as piles of line segments. Experiments show that HT-based
method, in combination with real-space features, achieves higher accu-
racy than the FT-based method. At the same time, the total calculation
cost is found lower.
Keywords: Handwritten character, machine-printed character, Hough
transform, optical character reader (OCR).
1 Introduction
We have been pursuing methods to distinguish between handwritten and
machine-printed character regions. We have two aims. One is to contribute to
the technology of optical character recognition (OCR). It is true that some of
OCR engines does not require any distinction between handwritten and machine-
printed character regions. However, their performance is lower than those of
handwritten / machine-printed specialized engines that are switched according
to the distinction. In addition, by clarifying existing factors that leads to their
difference, we will be able to contribute to the improvement of the recognition
accuracy for the respective kinds of characters.
The other aim is applications to search and automatic classification of docu-
ments. Since electronic documents are widely available in recent years, document
users are interested in wiser ways to use those documents for search, classification
and analysis. For this purpose, it becomes more important to collect key strings
efficiently. Besides higher-level processing such as the morphological analysis,
we can also utilize the lower-level information such as whether characters are
T. Huang et al. (Eds.): ICONIP 2012, Part V, LNCS 7667, pp. 353–360, 2012.
c© Springer-Verlag Berlin Heidelberg 2012
354 Y. Konno and A. Hirose
handwritten or machine-printed. Characters written by hand often represent
personal information such as name, address and telephone number in, for ex-
ample, sales records. If we can determine handwritten character regions, we can
enable the search and automatic classification based on such personal informa-
tion.
This research realizes the distinction of handwritten and machine-printed
character regions by image feature extraction and classification. We are inspired
by the processing of early visual cortex. That is, we distinguish them by paying
attention to the fluctuation of line segment directions. The method of Koyama
et al. using Fourier transform is existing technology focused on the processing of
early visual cortex. We expect that we can realize higher accuracy by employing
the HT to extract more characteristics of the line segments.
In this paper, we propose HT-based method to distinguish between hand-
written and machine-printed character regions. We assume to deal with low-
quality office documents such as faxed sales records. Experiments demonstrate
that our method achieves a distinction rate higher than the conventional FT-
based method. In addition, the total calculation cost is lower because of the
acceleration techniques specific to HT.
2 Conventional Methods
We can categorize the conventional methods to distinguish handwritten and
machine-printed characters into the following three types based on the features
used.
1. Methods using single-character features
For example, method by comparing the horizontal and vertical run length of
the character image [1], method by using matching character-image template
[2].
2. Methods using relationship features among multiple characters
Method using variances of baseline, median point, gap, aspect ratio of the
bounding rectangles of characters [3].
3. Methods using the pixel features in arbitrary area
Method using the angle fluctuations of line segments in the frequency domain
by transforming the area image by the fast Fourier transform (FFT) [4] [5].
Methods 1 and 2 using real space (RS) features normally require hundreds of
characters. Method 3 needs a high calculation cost for the FFT.
3 Proposal of the Hough-Transform-Based Method
To solve these problems, we propose a new method to distinguish between hand-
written and machine-printed character regions using HT.
Early-Vision-Inspired Method 355
3.1 Feature Extraction Using Hough Transform
Koyama et al. have defined the fluctuation in handwritten characters as the
nonuniformity of character size, unevenness in line spacing and misalignment of
vertical or horizontal lines [4][5]. Information in spatial frequency area is used
to extract the fluctuation. A type of information similar to that obtained by
the method proposed by Koyama et al. is extracted using the line component
extraction based on using HT.
3.2 Effective Utilization of Hough Transform
HT is a popular method to extract lines or circles from image[6]. The feature of
HT is that it can detect
– Lines and circles from images with noise.
– Broken and/or crossing lines.
– Multiple lines at a time.
However, this procedure also has the following disadvantages:
– It uses large memory capacity for parameter space.
– It cannot be applied to all situations.
– It takes more time than real-space image processing.
– It does not include start and end point information of lines.
In the case of determining handwritten or machine-printed characters, this
method is advantageous since we can overcome all the disadvantages except
for the processing time.
3.3 Procedure to Extract the Features Using Hough Transform
The processing steps are shown in Fig.1 and explained as follows.
1. Apply thinning process to a character image.
2. Set the window to the specified size.
3. Apply HT and accumulate the Hough curves in the parameter space.
4. Remove the HT accumulation below threshold θA to cut noise to reduce the
calculation cost in the next step.
5. Search peaks above another threshold θB, and compile the peak number in
the angle domain.
6. Shift the window, and repeat steps 2 to 5. Continue the process until the
window scans over all the image to make an angle-domain histogram.
7. Distinguish between handwritten or machine-printed characters based on the
difference of the histogram.
The optimal parameters have been found as : Window size = 32 × 32 pixels,
Threshold θA = 3, Threshold θB = 9, Shift interval for window scanning = 16
pixels.
356 Y. Konno and A. Hirose
Fig. 1. Feature extraction procedure using Hough transform (HT)
Early-Vision-Inspired Method 357
Fig. 2. Peaks histogram in angle space for (a) handwritten and (b) machine-printed
characters
3.4 Feature Extraction Procedure Using Hough Transform
Fig. 2 shows a typical histogram obtained from handwritten and machine-printed
images using HT. Fig. 2 indicates that the distribution is high at 0, 45, 90,
135, 180 degrees for both images. The variation of handwritten images is larger
than that of machine-printed characters. This fact shows that it is possible to
distinguish the characters based on the profile difference in the histogram made
in the line-angle domain. We set observation regions at angles of 0, 45, 90, 135
and 180 degrees with ±7 degree width, in which we calculate local distribution
variances. We feed the variances to an adaptive classifier (e.g., Support Vector
Machine: SVM) to obtain the distinction decision.
4 Experiment
We implemented and compared the conventional methods and our HT-based
method.
4.1 Experimental Condition
In order to evaluate performance, we prepared character image samples. It is
desirable that a verification can cover various conditions. Then we defined the
following condition for making the samples.
– 100 strings are selected. They are composed of digit, Kanji, Hiragana,
Katakana, alphabet, mixture of number and Kanji, mixture of Kanji and
Hiragana.
– A sample contains a single character, some characters or a few words.
– We make patterns of character descriptions according to the experimental
design method, resulting in 18 patterns for machine-printed characters and
4 patterns for handwritten ones.
358 Y. Konno and A. Hirose
Table 1. Handwritten characters Level table
Factor No. Factor name Number of level Level 1 Level 2
L4-1 Text direction 2 Horizontal Vertical
L4-2 Writing space Height 2 10.5pt 22pt
L4-3 Writing space Width 2 Normal Wide
Inappropriate combinations such as vertical alphabet are excluded.
Table 2. Machine-printed characters Level table
Factor No. Factor name Number of level Level 1 Level 2 Level 3
L18-1 Type of font 2 Gothic Ming
L18-2 Font size 3 10.5pt 22pt 34pt
L18-3 Text decoration 3 Normal Bold Italic
L18-4 Proportional Font 3 Normal Proportional Normal
L18-5 Error 3 Error 1 Error 2 Error 3
L18-6 Text Direction 3 Horizontal Vertical Horizontal
L18-7 Character spacing 3 Narrow Standard Wide
L18-8 Error 3 Error 1 Error 2 Error 3
Inappropriate combinations such as vertical alphabet are excluded.
– Handwritten samples are written by 10 people so that the garphoanalytical
aspects are taken into consideration.
– In total, we prepare (100 strings) × {(4 handwritten pattens) × (10 people)
+ (18 machine-printed pattens)} samples.
– We scan the samples to generate binary low-resolution (200dpi) images by
assuming FAX quality.
Table 1 and 2 show the definition of the pattern descriptions . Inappropriate
combinations such as vertical alphabet are excluded. Character image samples
consist of 3,240 handwritten characters and 1,800 machine-printed characters.
Fig. 3 shows a part of that set.
We compare accuracy and processing speed for the conventional methods
and our HT-based method to distinguish the above character image samples
as follows. We divide the sample images into ten groups. We use 9 groups as
teachers for the learning, and test the performance for the last group. We change
the groups so that all the groups experience one test, respectively. We define that
the final result is the average of the correct distinction rates. We examine the
following three methods, that is, RS-based method, FFT-based method which
is the method proposed by Koyama et al. (conventional methods), HT-based
method, and their combinations. We used a SVM as an adaptive classifier. The
parameters of the SVM are optimized for respective methods.
4.2 Experimental Results
Table 3 shows the accuracy obtained in experiments. The accuracy of the HT-
based method is lower than that of the FFT-based method. However, when it
Early-Vision-Inspired Method 359
Fig. 3. (a) Handwritten and (b) machine-printed character test samples
Table 3. Comparison of accuracy and processing time
Used method Accuracy() Processing timeisecj
RS-based method 91.1 0.45
FFT-based method 93.9 7.72
HT-based method 90.4 0.52
FFTRS method 94.0 8.01
HTRS method 96.0 1.02
is combined with the RS-based method (HTRS method), the accuracy is higher
than the FFT-based method. In the FFT-based case, the combination with the
RS-based method (FFTRS method) does not change the accuracy.
Table 3 shows the processing time. The processing time of HTRS method is
7 times as fast as that of the FFT-based method.
5 Conclusions
In conclusion, the results suggest the following points.
– The accuracy of the HTRS method is higher than that of the FFT-based
method.
– The HTRS method is quicker than the FFT-based method.
– The accuracy of FFTRS method is almost the same as that of the FFT-based
method.
360 Y. Konno and A. Hirose
As indicated above, the accuracy of the HT-based method alone does not surpass
that of the conventional FFT-based method. However, the combination of the
HT-based method with the RS-based method showed the best accuracy with
reduced processing time. The remaining issues are the following points.
– Creating the better classifier.
This paper focus on the effects of features which are extracted using existing
methods and the proposed method. The study of classifier is not sufficient.
In order to find the better method to obtain the distinction decision, further
validation is required.
– Extracting text images from the document image.
The final goal of our research is to extract text images from the document
image and distinguish between handwritten and machine-printed character
images. This paper focus on only the latter method. It is necessary to con-
sider the former method.
References
1. Ding, X., Chen, L., Wu, T.: Character independent font recognition on a single
chinese character. In: IEEE Trans. Pattern Anal., pp. 195–204 (2007)
2. Kavallieratou, E., Stamatatos, S.: Discrimination of machine-printed from hand-
written text using simple structural characteristics. In: International Conference on
Pattern Recognition, pp. 437–440 (2004)
3. Wang, S.L., Fan, C.K., Tu, T.Y.: Classificationof machine-printed and handwritten
texts using character block layout variance. Pattern Recognition, 1275–1284, 437–
440 (1998)
4. Koyama, J., Kato, M., Hirose, A.: Distinction between handwritten and machine-
printed characters without extracting characters or text lines. In: Proc. of World
Congress on Computational Intelligence, International Joint Conference on Neural
Networks (IJCNN), Hong Kong, pp. 4143–4150 (2008)
5. Koyama, J., Kato, M., Hirose, A.: Local-spectrum-based distinction between hand-
written and machine-printed characters. In: Int’l Conf. on Image Processing, San
Diego, pp. 12–15 (2008)
6. Hough, P.V.C.: U.S. Patent 3069654: method and means for recognizing complex
patterns (1962)
