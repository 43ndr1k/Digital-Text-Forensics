Proceeding of the 3rd International Conference on Artificial Intelligence and Computer Science 
(AICS2015), 12 - 13 October 2015, Penang, MALAYSIA. (e-ISBN 978-967-0792-06-4).   
Organized by http://worldconferences.net  144  
A FRAMEWORK FOR SEMANTIC-BASED ANOMALY DETECTION IN TEXT 
Mohammed Ahmed Taiye1, Siti Sakira Kamaruddin2 and Farzana Kabir Ahmad 3   
123School of Computing Universiti Utara Malaysia 
tfeatslekan@gmail.com1, {sakira, farzana58}@uum.edu.my23 
     
 
ABSTRACT 
Anomaly detection is a vital task in text mining. It has a diverse application in domains like Law, 
Digital Library, Print media and Journal articles. More so, anomalous text information is detected 
from documents to communicate useful patterns of text data. These text data can be translated into 
core ideas by revealing its meaning to disturbing events like the misuse of information overload in 
unstructured text documents, bank fraud detection and socio-political threats to national 
security.  However, detecting core ideas in an unstructured anomalous text data requires more effort 
and often leads to enormous amount of inconsistency. Moreover, retrieving semantics in text 
document may yield important knowledge for optimized decision making and enhanced business 
excellence. A conceptual framework of mining anomalous text data in corpus is introduced in this 
article. The proposed framework includes text pre-processing and text representation of documents 
into a normalized canonical text format. Canonical form is a notion stating that related idea should 
have the same meaning representation. This representation enables the similar meanings of 
different words to be captured in a single semantic representation through word senses and 
synonyms to simplify the reasoning task. The framework also employs sequential exception 
technique to identify anomalous text data from the represented canonical form. This technique 
belongs to the deviation-based anomaly detection method where it do not depend on statistical 
analysis or distance measurement to detect anomalous data instead it sequentially identifies 
anomaly by analysing the primary features of the data in a set. The main idea behind this technique 
is to investigate the implicit redundancy of data.  This method is favourable compared to statistical 
and distance-based method due to its linear complexity.  In our framework, we propose to include all 
functions of the sequential exception technique namely; dissimilarity function, smoothing factor and 
cardinality function. The components presented in this framework will be leveraged to improve the 
efficiency of detecting semantics from different corpora to accuratly identify anomalous text data 
with consistent and meaningful level of information. 
Keywords: Text anomaly, Sequential exception technique, corpus, semantic representation, 
canonical form 
 
1. Introduction 
 
In the new information era, text information retrieval is an interesting block in Artificial Intelligence 
(AI). Text documents, whether books, articles or reports, are composed of sequence of text data 
with meaningful ideas (Jurafsky & Martin, 2000). The reliability and completeness of these text data 
in corpus is an interesting aspect of text mining. However, information retrieval has attracted 
researchers to investigate more in exploring different approaches to detect meaningful information 
in corpus. In addition, 80% of today‚Äôs data are text-based (Zhang & Zhou, 2007). Therefore, text 
Proceeding of the 3rd International Conference on Artificial Intelligence and Computer Science 
(AICS2015), 12 - 13 October 2015, Penang, MALAYSIA. (e-ISBN 978-967-0792-06-4).   
Organized by http://worldconferences.net  145  
information overflow needs to be leveraged by detecting useful knowledge in documents. These 
knowledge can be used for decision making and optimized business excellence (Jurafsky & Martin, 
2000). Extracting useful text information from a huge document is a big issue. Past researchers have 
shown that traditional information retrieval technique does not meet the demand of retrieving 
meaningful knowledge from the growing amount of text data that is produced (Hiemstra & Vries, 
2000). Judging from these literatures (Hiemstra & Vries, 2000; Jurafsky & Martin, 2000; Zhang & 
Zhou, 2007), it is evident that text pattern needs to map across multiple documents for the purpose 
of knowledge creation (Nair, 2013).  Identified form of unique text pattern can be performed by 
detecting anomalous text data in documents. Semantic analysis is one of the most effective ways of 
identifying anomalous text data from documents (Chandola, Banerjee, & Kumar, 2009). 
Anomaly detection identifies meaningful knowledge that is hidden in text. It does this by detecting 
interesting text patterns of objects which are uniquely distinguishable from other datasets. 
Abnormality in the text pattern are referred to as an outlier, deviation or an observation. However, 
text anomaly detection techniques aims at uncovering useful and interesting words, phrases, 
sentences, paragraphs in corpus documents. Text data are represented in high dimensional 
document format (Manevitz, 2001). These high dimensional patterns are identified by predicting 
values of data or target attributes in text data (Nair, 2013). Many researchers have detected text 
anomaly either by classification or by clustering approach. (Abouzakhar, Allison, & Guthrie, 2008; 
Chandola et al., 2009; Kamaruddin, Hamdan, Bakar, & Mat Nor, 2012; Kamruzzaman, Haider, & 
Hasan, 2010). Clustering is the act of grouping similar object that are similar physically or in 
abstraction. These objects are similar to objects within the same cluster and dissimilar to other 
objects outside their clusters (Kumar, 2012). While classification classifies documents into 
predefined categories based on document content. Classification retrieves text in response to user 
query and understanding that can be transformed into summaries, knowledge (answered questions) 
or extracted text data (Kamruzzaman et al., 2010). In (Manevitz, 2001) research, which emphasized 
on the use of classification approach on text document employed the (Support Vector Machine 
classifier). This classifier was constructed on lexical features in text data to retrieve positive 
information in documents. Another interesting research by (Kamaruddin, Hamdan, Bakar, & Mat 
Nor, 2009; 2012) used the deviation detection approach on extracting semantics in financial text 
information in text document using the Conceptual Graph and the Conceptual Graph Interchange 
Format. These literatures have successfully embedded their approaches in achieving good result in 
retrieving useful information from text data  (Kamaruddin et al., 2012; Manevitz, 2001). Many 
research based their techniques on distance, statistical and deviation detection approach (Tamberi, 
2007).   In coping with the issue of detection semantic in text data. This work focuses on identifying 
to represent text semantics by proposing canonical format and detecting anomaly using one of the 
deviation detection method known as the sequential exception technique. 
According to (Lewis, Schapire, Hill, & Callan, 1983), sequential exception technique provides high 
dimensional data in large dataset by acquiring feasible mapping of semantic knowledge that is 
categorized by text structures. In other words, it compares set of outliers in text. Sequential 
exception technique can be found in the work by (Abouzakhar, Allison, & Guthrie, 2008; Arning & 
Rakesh, 1996; Kamaruddin, Hamdan, Bakar, & Mat Nor, 2012). Unfortunately, these literatures 
emphasized on one of its techniques in tackling challenges of text information retrieval. (Arning & 
Rakesh, 1996) has perhaps given an excellent application of combining some measures in sequential 
exception techniques such as the dissimilarity functions, smoothing factors and cardinality 
functions. These techniques are employed to detect deviations of dataset in log files within a large 
Proceeding of the 3rd International Conference on Artificial Intelligence and Computer Science 
(AICS2015), 12 - 13 October 2015, Penang, MALAYSIA. (e-ISBN 978-967-0792-06-4).   
Organized by http://worldconferences.net  146  
database. It thus includes the problem of discovering frequent co-occurrence of attributes values in 
large dataset. Nevertheless, a more effective experimental evaluation result would have been 
acquired, if the nature of exception needed in the datasets are known. Arning & Rakesh, (1996) 
work is relevant and related to this research in the sense that, it techniques will be applied in this 
research. However, this research will not be focused on log file unlike (Arning & Rakesh, 1996) 
research, rather this research will concentrate on the extraction of semantic in corpus.  
 
2. The Conceptual framework  
 
The proposed framework comprises of different steps in detecting anomaly. The steps involved in 
this framework includes gathering of multiple documents for text preprocessing and text 
representation. Preprocessing of text data involves the tokenization, tagging, stemming and stop 
word elimination of words in a given context. These processes are crucial steps in overcoming issues 
like identification and retrieval of text from documents. While text representation stage involves the 
representation of text in a form that will be easily understood and interpreted by user for 
identification and retrieval of meaningful idea from documents. This phase includes word 
disambiguation and canonical formatting of words. 
2.1 Text data Pre-processing 
 
Preprocessing of text data from documents are crucial steps in overcoming text problems. In order 
to make extraction more efficient data pre-processing is applied on text data. It enhances the quality 
of text data and also reduces time of processing like word positioning, stop word elimination, 
frequency in documents, stemming process, tagging and tokenization process. In this research, we 
have collected some samples of text documents from the library to analyze the semantics in text 
corpuses. Figure 1 below illustrates the conceptual framework for semantic-based anomaly 
detection in text. 
 
 
Proceeding of the 3rd International Conference on Artificial Intelligence and Computer Science 
(AICS2015), 12 - 13 October 2015, Penang, MALAYSIA. (e-ISBN 978-967-0792-06-4).   
Organized by http://worldconferences.net  147  
 
Figure 1: The conceptual Framework
2.1.1 Tokenization 
 
Tokenization is the first step of morphological analysis which explores word.  It is a method used in 
segmenting file content into individual word. It also detects meaningful keywords and can cater for 
consistency in documents. Textual data at the initial stage is a block of characters, where the task of 
retrieving information requires the words in a datasets (Kumar, 2012). Tokenization still have some 
left-over issues like the removal of punctuation marks, acronyms and abbreviation needs to be 
transformed into standard form. Other characters like brackets, hyphens, requires processing as 
well.  Furthermore, it inconsistency can be the variation in time format and number.  To simplify this 
issue we introduce tagging.  
2.1.2 Tagging  
 
Tagging involves assigning syntactic tag for each words. Also tagging is done to resolve ambiguity, 
reduce the number of parse, search space and structures of sentence in text documents. The input 
to tagging technique are tag sets and stream of words that shows the results for each tags in a word. 
For example, the following are tag set for a sample of sentence about air travel. 
VB  DT NN . 
Book  that flight . 
VBZ DT NN  VB NN ? 
Does     that flight   serve  dinner ? 
Proceeding of the 3rd International Conference on Artificial Intelligence and Computer Science 
(AICS2015), 12 - 13 October 2015, Penang, MALAYSIA. (e-ISBN 978-967-0792-06-4).   
Organized by http://worldconferences.net  148  
In this example it is seen that book is ambiguous it has more than a possible usage in Part of 
speech.Resolving ambiguity is a major issue facing Part of speech (POS) tagging.  Consequently, 
many techniques have been developed to improve the efficiency of tagging and remove ambiguity 
in POS, techniques like; Stochastic Part of speech, which uses probability in tagging text (Oettinger, 
1965).  
2.1.3 Stemming  
 
Many English words can be reduced into it root word. A root word is a word that does not have a 
prefix or suffix. It is a primary lexical unit of words. Stemming known as lemmatization is a technique 
used in reducing words into their roots (Kamaruddin, Hamdan, & Bakar, 2007). Names are 
transformed into stems by reducing the ‚Äú‚Äôs‚Äù the variation of ‚ÄúAhmed‚Äôs‚Äù can be reduced to ‚ÄúAhmed‚Äù.
 Another example is illustrated in the table below 
 
Table 1: Stemming root word 
N0 Suffix Root Word PREFIX 
1 Disengage engage engagement 
 
Stemming is useful in transforming all sort of inflections in words to root words. However case 
sensitive systems may find it difficult to compare words cases either capital letters or low case 
letters.  To overcome the issue of case sensitivity, the standard porter stemming technique is 
employed to find root words in documents (On, Buntine, & Yee, 2006).  
2.1.4 Stop word elimination 
 
Pronouns, conjunctions, prepositions like ‚Äòthe‚Äô, ‚Äòa‚Äô, ‚Äòin‚Äô, ‚Äòwith‚Äô are stop words in text documents 
(Kamaruddin et al., 2007). Stop words are commonly used words not just in English language but 
any language. Every text document deal with these words for better understanding of sentences, 
phrases or paragraphs. However, they are not necessary for text mining application. In fact they 
slow down text mining process due to the fact that, they add more words in text documents. The 
removal of stops word in documents improves performance and reduces the amount of text to be 
processed in a document. If the common words are removed from a given context, more focus will 
be emphasized on the meaningful words. A good example is the search engine, in the context of 
using the search query to retrieve information of terms like (how to search for semantics) ‚Äúhow‚Äù, 
‚Äúto‚Äù, ‚Äúsearch‚Äù, ‚Äúfor‚Äù, ‚Äúsemantics‚Äù. The search engine will find more pages that contains ‚Äúhow‚Äù, ‚Äúto‚Äù 
than pages that contains search for semantics. This is because the word how and to are commonly 
used words in English language (On et al., 2006). 
2.2  Text representation 
 
We propose two text representation operations in this work which are word sense disambiguation 
and canonical format. They are both employed for better semantic representation of text and for 
overcoming issues like noise in text.   Additionally, in overcoming these issues, enhanced text 
identification performance solution often depends on text representation. Due to the 
Proceeding of the 3rd International Conference on Artificial Intelligence and Computer Science 
(AICS2015), 12 - 13 October 2015, Penang, MALAYSIA. (e-ISBN 978-967-0792-06-4).   
Organized by http://worldconferences.net  149  
heterogeneity, inconsistency and noisiness of text data, there is a possibility that retrieving 
semantics in text can be difficult 
2.2.1 Word Sense Disambiguation (WSD) 
 
Word sense disambiguation is an important task in many Natural Language Processing. It 
determines the sense of polysemic word in its context. However, the enormous amount of 
polysemic word in lexicon is overwhelming when trying to interpret word or make meaning in text 
document.  Word tokens are specifically examined to the exact word senses that are being used in a 
given context. It focuses on correct word senses and by automatically eliminating flawed 
representations that results from incorrect senses can be seen as WSD. it involves tasks like 
determining different meaning of words and tagging words in a text with its appropriate sense 
(Bakx, 2006). Effective word sense disambiguation can improve computer application performance 
in text summarization, natural language understanding, machine translation and information 
extraction from text documents. (Jurafsky & Martin, 2000).  Many researchers have made used of 
Word sense disambiguation in different application. Boyd-Graber, Blei, & Zhu, (2007), developed a 
Latent Dirichlet Allocation using WordNet (LDAWN) for simultaneously disambiguating corpus and 
learning the domains in which to consider each words. An improved and accurate Word Sense 
Disambiguation for topic model was constructed in this research. Another research by (Bakx, 2006), 
stated that word senses could be seen as the target labels of a classification problem. The 
Classification problem was handled by applying techniques of the Machine Learning field to enhance 
word sense disambiguation task. 
2.2.2 Canonical format 
 
Word can be converted into canonical form. Canonical form is a notion stating that related idea 
should have the same meaning representation. It is an approach that greatly simplifies task by 
dealing with a single meaning representation for a wide range of expression. An example is given as 
follows; 
a. Ahmed best console is Mortal Kombat 
b. Mortal Kombat is Ahmeds‚Äô best console 
Despite the difference in the placement of argument ‚Äòbest‚Äô in the above example, we assign Ahmed 
and Mortal Kombat to the same roles in both sentences. Both sentences can be seen as a passive 
and active sentence. Moreover, in the above context the syntactic parse is compatible with meaning 
representation. Meaning representation plays a similar role with syntactic and morphological 
representation in text. Text representation is performed in canonical form by capturing formal 
structures in text documents (Jurafsky & Martin, 2000). This work will use Logic representation in 
representing text format. Logic representation contains individual variables and quantifiers in 
analyzing natural language.  It is of two parts, which are; the syntax which is a group of legal 
construct or expression and the semantics which refers to the meaning of these expressions.  
Semantics in predicate logic checks the validity and satisfiability of statement using the truth table. 
Value is attached to variable in an appropriate context to a truth table through predicates P (t 1, t2, . 
. . , tn) for precise and consistent interpretation of words (Kobus, Yvon, & Damnati, 2008). 
 
Proceeding of the 3rd International Conference on Artificial Intelligence and Computer Science 
(AICS2015), 12 - 13 October 2015, Penang, MALAYSIA. (e-ISBN 978-967-0792-06-4).   
Organized by http://worldconferences.net  150  
2.3 Anomaly detection using sequential exception technique 
 
According to (Kamaruddin et al., 2007), ‚ÄúAnomaly detection is one of the text mining task that are 
performed by comparing text and detect the most dissimilar‚Äù. In order to detect anomaly in text in a 
canonical form. Canonical form needs to be properly evaluated and compared. For this purpose we 
employed one of the deviation detection method known as the sequential exception technique. In 
other words, it compares set of outliers in text.  Before we describe these problems, we will identify 
the measures in sequential exception techniques that will be used to enhance anomaly detection in 
text documents. As earlier stated, this research work concentrates on three measures in sequential 
exception techniques which are; dissimilarity function, smoothing factor and cardinality function. 
(Lewis et al., 1983) stated that, sequential exception technique provides high dimensional data in 
large dataset by acquiring feasible mapping of semantic knowledge that is categorized by text 
structures.   Optimal mapping and detection of text in document was performed using dissimilarity 
function as a technique to detect plagiarized text according to (Stamatatos, 2009). After patterns 
have been successfully mapped in corpus, words needs to be matched and compared for use. A 
novel approach by (S Jimenez, Becerra, & Gelbukh, 2012) made use of soft cardinality to proffer a 
flexible comparison between words in text. It allows similarity function suitable for directional 
judgement. Another research made use of soft count cardinality function to compare inter-element 
similarity functions with text semantics in documents (Henriksson, Moen, Skeppstedt, 
Daudaraviƒçius, & Duneld, 2014). However, in an event to count non-identical elements in text, 
Classical set cardinality function can be used(Sergio Jimenez, Gonzalez, & Gelbukh, 2010). Classical 
and soft cardinality works hand-in-hand for proper identification of elements either identical or non-
identical datasets in corpus. Furthermore, to investigate more on similarity of objects and mapping 
of words in text data, pattern needs to be understood using the smoothing factor (Arning & Rakesh, 
1996). It identifies polysemous and synonymous relations between terms. The smoothing functions 
approximately captures useful patterns in text data while leaving out noise.  More so, it adjust the 
maximum likelihood estimator for language model so that mapping of text pattern will be more 
accurate (Vilar et al., 2004). 
We begin by giving definitions of the anomaly detection techniques for exception problems.  
ÔÇ∑ Dissimilarity function d: ùí≥ √óùí≥ ‚Üí ‚Ñù is usually understood to measure some kind of 
distance between points. In particular dissimilarity functions increases the more apart 
two points gets. The term dissimilar is synonymous to distance. (Von Luxburg, 2004).  
For this work, we are going to use the Power set equation on the selected techniques to 
check for similarity in text.  
A set of items (and thus Power set P (I)); 
A dissimilarity function D: P (I) ‚Üí‚Ñù+0; 
The subset of an Item is an exception set. It contributes to the dissimilarity of an item 
set I with the least number of items. For D (I-Ix) ‚Äì the dissimilarity of a reduced set is 
important to finding the exception set Ix, the dissimilarity D (Ix) of the exception set itself 
is not considered. Nevertheless, it may be valuable to take into consideration D (Ix) in 
subsequent evaluation of exception set. For D, any function can be employed which will 
return a low value if the elements of I that are same with each other, and an increase in 
the value if the elements are dissimilar. However, functions does not require a metrical 
distance between items.  
Proceeding of the 3rd International Conference on Artificial Intelligence and Computer Science 
(AICS2015), 12 - 13 October 2015, Penang, MALAYSIA. (e-ISBN 978-967-0792-06-4).   
Organized by http://worldconferences.net  151  
 
ÔÇ∑ Cardinality function is used to compare sentences divided into words and in turn, words 
divided  into text characters (Sergio Jimenez, Becerra, & Gelbukh, 2012). 
A cardinality function C: P (I) ‚Üí‚Ñù+0; with I1 ‚äÇ I2‚Üí C (I1) < C (I2) for all I1, I2 ‚äÜ I  
 
ÔÇ∑ Smoothing function refers to the adjustment of Maximum Likelihood Estimator for 
language model that helps to remove noise from a target set. This is done by identifying 
target patterns in a set of text data (Arning & Rakesh, 1996). Exception must not be 
unique. It can be empty. In a case where all item in I are similar, smoothing factor is 
then said to be 0. The smoothing factor shows the degree to which dissimilarity reduces 
a subset IJ of elements in set I. it is observed that Smoothing factor may be negative for 
some subset IJ if the dissimilarity. In most cases function C (I ‚Äì Ij) may return |I ‚Äì Ij |, the 
number of items in the set I - IJ. This function may also be defined by formula C(I ‚Äì Ij) = 1 
√∑ (|Ij| + 1) (Arning & Rakesh, 1996). Therefore, SF (Ij) = C (I - Ij) * (D (I) ‚Äì D (I -Ij)). We say 
that Ix ‚äÇ I, Ix ‚â†I is an exception set of I with respect to D and C if   SF (Ix) ‚â• SF (Ij) for all Ij ‚äÇ 
I.  So therefore, we define for each Ij ‚äÇ I, the smoothing factor. 
4. Conclusion 
 
This proposed framework for the semantic-based text anomaly detection addresses key problem in 
representing text semantics and identifying anomalies in high dimensional unstructured text data.  
The proposed framework will be a good addition to current efforts in the area of text mining 
particularly in resolving the semantic representation issue and in text anomaly detection. The 
framework presented in this work can be seen as an initial step towards proposing an improved 
method of identifying text anomalies especially with the inclusion of semantic representation of text 
using canonical format. The next step in this work is to implement the framework by designing 
effective semantic representation and improving the proposed anomaly detection technique to 
increase the accuracy of text anomaly detection. 
 
References 
 
Abouzakhar, N., Allison, B., & Guthrie, L. (2008). Unsupervised Learning-based Anomalous Arabic 
Text Detection. Proceedings of the 6th International Conference on Language Resources and 
Evaluation (LREC‚Äô08), 291‚Äì296. Retrieved from http://www.lrec-
conf.org/proceedings/lrec2008/summaries/83.html 
Arning, A., & Rakesh, A. (1996). Method for Deviation in Large Databases. KDD-96 Proceedings. 
Bakx, G. E. (2006). Machine Learning Techniques for Word Sense Disambiguation. Machine Learning. 
Boyd-Graber, J., Blei, D. M., & Zhu, X. (2007). A Topic Model for Word Sense Disambiguation. 
Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language 
Processing and Computational Natural Language Learning (EMNLP-CoNLL‚Äô07), 1024‚Äì1033. 
Proceeding of the 3rd International Conference on Artificial Intelligence and Computer Science 
(AICS2015), 12 - 13 October 2015, Penang, MALAYSIA. (e-ISBN 978-967-0792-06-4).   
Organized by http://worldconferences.net  152  
Chandola, V., Banerjee, A., & Kumar, V. (2009). Anomaly detection: A survey. ACM Computing 
Surveys (CSUR), 41(September), 1‚Äì58. http://doi.org/10.1145/1541880.1541882 
Henriksson, A., Moen, H., Skeppstedt, M., Daudaraviƒçius, V., & Duneld, M. (2014). Synonym 
extraction and abbreviation expansion with ensembles of semantic spaces. Journal of 
Biomedical Semantics, 5(1), 6. http://doi.org/10.1186/2041-1480-5-6 
Hiemstra, D., & Vries, A. De. (2000). Relating the new language models of information retrieval to 
the traditional retrieval models, (May), 1‚Äì14. Retrieved from 
http://eprints.eemcs.utwente.nl/5950/01/00000022.pdf 
Jimenez, S., Becerra, C., & Gelbukh, a. (2012). Soft cardinality+ ML: learning adaptive similarity 
functions for cross-lingual textual entailment. ‚Ä¶  of the First Joint Conference on ‚Ä¶, 684‚Äì688. 
Retrieved from http://dl.acm.org/citation.cfm?id=2387752 
Jimenez, S., Becerra, C., & Gelbukh, A. (2012). Soft Cardinality: A Parameterized Similarity Function 
for Text Comparison. {*SEM 2012}: The First Joint Conference on Lexical and Computational 
Semantics -- Volume 1: Proceedings of the Main Conference and the Shared Task, and Volume 
2: Proceedings of the Sixth International Workshop on Semantic Evaluation {(SemEval 2012)}, 
449‚Äì453. Retrieved from http://www.aclweb.org/anthology/S12-1061 
Jimenez, S., Gonzalez, F., & Gelbukh, A. (2010). Text comparison using soft cardinality. Lecture Notes 
in Computer Science (including Subseries Lecture Notes in Artificial Intelligence and Lecture 
Notes in Bioinformatics), 6393 LNCS, 297‚Äì302. http://doi.org/10.1007/978-3-642-16321-0_31 
Jurafsky, D., & Martin, J. H. (2000). Speech and Language Processing: An Introduction to Natural 
Language Processing, Computational Linguistics, and Speech Recognition. Speech and 
Language Processing An Introduction to Natural Language Processing Computational 
Linguistics and Speech Recognition, 21, 0‚Äì934. http://doi.org/10.1162/089120100750105975 
Kamaruddin, S. S., Hamdan, A. R., & Bakar, A. A. (2007). Text Mining for Deviation Detection in 
Financial Statement, 446‚Äì449. 
Kamaruddin, S. S., Hamdan, A. R., Bakar, A. A., & Mat Nor, F. (2012). Deviation detection in text 
using conceptual graph interchange format and error tolerance dissimilarity function. 
Intelligent Data Analysis, 16(3), 487‚Äì511. http://doi.org/10.3233/IDA-2012-0535 
Kamruzzaman, S. M., Haider, F., & Hasan, A. R. (2010). Text Classification using Data Mining. Science, 
19. Retrieved from http://arxiv.org/abs/1009.4987 
Kobus, C., Yvon, F., & Damnati, G. (2008). Normalizing SMS: are two metaphors better than one? 
Proceedings of the 22nd International Conference on Computational Linguistics-Volume 1, 
(August), 441‚Äì448. Retrieved from http://dl.acm.org/citation.cfm?id=1599137 
Proceeding of the 3rd International Conference on Artificial Intelligence and Computer Science 
(AICS2015), 12 - 13 October 2015, Penang, MALAYSIA. (e-ISBN 978-967-0792-06-4).   
Organized by http://worldconferences.net  153  
Kumar, a A. (2012). Text Data Pre-processing and Dimensionality Reduction Techniques for 
Document Clustering Sri Sivani College of Engineering Sri Sivani College of Engineering, 1(5), 1‚Äì
6. 
Lewis, D. D., Schapire, R. E., Hill, M., & Callan, J. P. (1983). Training Algorithms for Linear Text Classi 
ers. Online. 
Manevitz, L. M. (2001). One-Class SVMs for Document Classification. Journal of Machine Learning 
Research, 2, 139‚Äì154. http://doi.org/10.1162/15324430260185574 
Nair, V. M. (2013). Visualization Based Sequential Pattern Text, 2(12), 254‚Äì259. 
Oettinger, A. G. (1965). Grammatical CocHiig of English, (6). 
On, S. W., Buntine, W., & Yee, W. G. (2006). Second Workshop On Open Source Information 
Retrieval In Conjunction with the 2006 ACM SIGIR Conference OSIR 2006 Organization 
Workshop chairs. Information Retrieval, 1‚Äì73. 
Stamatatos, E. (2009). Intrinsic plagiarism detection using character n-gram profiles. CEUR Workshop 
Proceedings, 502, 38‚Äì46. 
Tamberi, F. (2007). Anomaly Detection, (June), 1‚Äì33. Retrieved from 
http://www.cli.di.unipi.it/~tamberi/old/docs/tdm/anomaly-detection.pdf 
Vilar, D., Vilar, D., Ney, H., Ney, H., Juan, A., Juan, A., ‚Ä¶ Vidal, E. (2004). Effect of Feature Smoothing 
Methods in Text Classification Tasks. International Workshop on Pattern Recognition in 
Information Systems, 108‚Äì117. 
Von Luxburg, U. (2004). Statistical Learning with Similarity and Dissimilarity Functions. Pediatric 
Neurosurgery, 30(2), 86‚Äì92. Retrieved from http://opus.kobv.de/tuberlin/volltexte/2004/862/ 
Zhang, M. L., & Zhou, Z. H. (2007). ML-KNN: A lazy learning approach to multi-label learning. Pattern 
Recognition, 40(7), 2038‚Äì2048. http://doi.org/10.1016/j.patcog.2006.12.019 
 
