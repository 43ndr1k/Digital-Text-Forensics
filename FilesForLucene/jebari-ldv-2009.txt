Chaker Jebari
A New Centroid-based Approach for Genre Categorization of Web
Pages
In this paper we propose a new centroid-based approach for genre cate-
gorization of web pages. Our approach constructs genre centroids using a
set of genre-labeled web pages, called training web pages. The obtained
centroids will be used to classify new web pages. The aim of our approach
is to provide a flexible, incremental, refined and combined categorization,
which is more suitable for automatic web genre identification.Our approach
is flexible because it assigns a web page to all predefined genres with a
confidence score; it is incremental it classifies web pages one by one; it is
refined because at each new web page, our approach either refines centroids
or discards the page because it is considered as a noisy page; finally, our
approach combines three different categorizations, which are based on the
URL address, the logical structure and the hypertext structure. The exper-
iments conducted on two known corpora show that our approach is very
fast and outperform other approaches.
1 Introduction
As the World Wide Web continues to grow exponentially, web page categorization
becomes increasingly important in web searching. Web page categorization, called
also web page classification assigns a web page to one or more predefined categories.
According to the type of category, categorization can be divided into sub-problems:
topic categorization, sentiment categorization, genre categorization, and so on.
Recently, more attention is given to automatic genre identification of web page be-
cause it can be used to improve the quality of web search results — see, for example, all
the articles in this journal and Mehler et al. (). The term “genre” is very difficult
to define. According to Merriam-Webster Online Dictionary , a genre is a category of
artistic, musical, or literary composition characterized by a particular style, form, or
content.
Kessler et al. () defined a genre as a bundle of facets, i.e. different textual
properties such as brow, narrative and genre. According to Shepherd and Watters
(), while non-digital genre is defined by the tuple <content, form>, the genre of
web page (or “cybergenre”) characterized by the triple 〈content, form, functionality〉,
where the functionality attribute account for the interaction between the user and the
web page. Rauber and Müller-Kögler () defined the genre as a group of documents
that share the same stylistic properties. In their experiment with digital libraries,
http://www.m-w.com
JLCL 2009 – Volume 25 (1) – 69-92
Jebari
documents of the same genre are noted with the same color. According to Finn (),
genre is orthogonal to the topic, and relates to polarities such as subjectivity/objectivity
and positivity/negativity. For Boese () a genre is characterized by the same style,
form and content.
In this article, the word “genre” is loosely defined as a textual category that can be
more or less related to the topic or content of a web pages. For this reason, we use
two different collections. One created by genre researchers for whom the concept of
genre is independent from topic (the KI- corpus, see Section ); the other including
a number of academic categories (the WebKB collection, see Section ).
In order to comply with our view of genre, our approach is flexible, incremental,
refinable and combines different feature sets. We devised it to be fast, so that in the
future it can be applied to web search engines.
Currently, search engines use key words to classify web pages. Returned web pages
are ranked and displayed to the user that it often not satisfied by the result. For
example, searching for the keyword “machine learning” will provide a list of web pages
containing the words “machine” and “learning”. These pages are from different genres.
Web page genre categorization can be used to improve the retrieval quality of search
engines (Meyer Zu Eissen, ). For instance, a classifier could be trained on existing
web directories and be applied to new web pages. At query time the user could be
asked to specify one or more desired genres so that the search engine would returns a
list of genres under which the pages would fall.
However, a web page is a complex object that includes heterogeneous elements with
different communicative purposes. Generally, a web page is composed of different
sections organized in the form of headings and links. These sections belong to different
genres. Graphical elements (search buttons, images, menus, forms, etc.) and text
types, sizes and colors are used to mark sections in web pages. Our approach assigns
a web page to all predefined genres with different confidence scores, which represent
the similarity between the web page and the centroid of each genre (see Section ).
Web genre evolves over time because of the continuous modification of the content
of web pages. Web genre evolution consists in updating old genres and creating new
ones. In our approach we focus on the adjustment of old genres. Since automatic genre
identification of web pages requires continuous learning, because web pages are often
updated, we propose in this paper and incremental approach (see Section ). The Word
Wide Web is an open environment, where the user can add a new page, modify the
content of actual web page, delete a web page and so on. For this reason, the web is
instable and contains many noisy web pages. Considering such web pages decrease the
accuracy of genre classification of web pages (Shepherd et al., ). In our approach
we propose a refined genre classification of web pages to discard noisy web pages. A
web page is considered noisy where its similarities to all genre centroids are less than
a refining threshold.
As mentioned above, a web page is not only a text but contains many html tags.
The information delimited by these tags is very useful for genre categorization. These
information sources are heterogeneous because they have different representation struc-
70 JLCL
A New Centroid-based Approach for Genre Categorization of Web Pages
tures and should be combined to increase the performance of genre classification of web
pages. The aim of our approach is to provide a flexible, incremental, refined and com-
bined categorization, which is more suitable for automatic web genre identification.Our
approach is flexible because it assigns a web page to all predefined genres with a confi-
dence score; it is incremental it classifies web pages one by one; it is refined because at
each new web page, our approach either refines centroids or discards the page because
it is considered as a noisy page; finally, our approach combines three different catego-
rizations, which are based on the URL address, the logical structure and the hypertext
structure.
This article is organized as follows: in Section  we present an overview of works
on genre categorization of web pages; in section  we explains our approach; Section 
presents our experimental results using two data sets of web pages; Section  presents
a comparative study of our approach. Finally we present some conclusions as well as
our future work.
2 Related Work
Previous work on automatic genre identification is reviewed by focussing on features,
classification algorithms and genre corpora.
2.1 Features
Many types of features have been proposed for automatic genre categorization. These
features can be grouped on four groups. The first group refers to surface features, such
as function words, genre specific words, punctuation marks, document length, etc. The
second group concerns structural features, such as Parts Of Speech (POS), Tense of
verbs, etc. The third group is the presentation features, which mainly describe the
layout of document. Most of these features concerns HTML documents and cannot
be extracted from plain text documents. Among these features we quote the number
of specific HTML tags and links. The last group of features is often extracted from
meta-data elements (URL, description, keywords, etc.) and concerns only structured
documents.
Kessler et al. () used four types of features to classify part of the Brown corpus
by multiple facets (i.e. brow, narrative and genre). The first types are structural
features, which includes counts of functional words, sentences, etc. The second types
are lexical features, which includes the existence of specific words or symbols. The
third kinds of features are character level features, such as punctuation marks. The
last kind concerns derivative features, which are derived from character level and lexical
features. These four features sets can be grouped on two sets: structural features and
surface features.
Karlgren () used twenty features including count of functional words, POS count,
count of images and links and textual counts (e.g. count of characters, count of words,
http://en.wikipedia.org/wiki/Brown_Corpus
Volume 25 (1) – 2009 71
Jebari
number of words per sentence, etc.). Stamatatos et al. () Identified genre based
on the most English common words. They used the fifty most frequent words on
the BNC corpus and the eight frequent punctuation marks (period, comma, colon,
semicolon, quotes, parenthesis, question mark and hyphen). Dewdney et al. ()
adopted two features sets: BOW (Bag Of Words) and presentation features. They
used a total of  features including layout features, linguistic features, verb tenses,
etc. Finn and Kushmerick () used a total of  features to differentiate between
subjective vs. objective news articles and positive vs. negative movie reviews. Most
of these features were the frequency of genre-specific words. Meyer Zu Eissen and
Stein () used different kinds of features including presentation features (html tag
frequencies), classes of words (names, dates, . . .), frequencies of punctuation marks and
Part-Of-Speech tags. Kennedy and Shepherd () used a feature set that comprises
features about the content (e.g. common words, met tags), the form (e.g. number of
images) and the functionality (e.g. number of links, JavaScripts). Boese and Howe
() used different kind of features, which can be grouped in three classes including
stylistic features, form features and content features.
More recently, Santini () and Lim et al. () tried to exploit all previously
used features, where Lim and al. used the document URL as new feature. Mehler et al.
() studied the usefulness of logical document structure in text type classification.
They adopted two approaches, which are the Quantitative Structure Analysis (QSA)
and the Document Object Model Tree Kernel (DomTK). They conducted experiments
to stress the usefulness of structure in document type recognition and to compare the
QSA approach against the DomTK approach. Finally, Kanaris and Stamatatos ()
used character n-grams extracted from both text and structure.
2.2 Machine Learning Techniques
Once a set of features has been obtained it is necessary to choose a categorization algo-
rithm. Generally, genre categorization algorithms are often based on machine learning
techniques (Mitchell, ). Among these techniques, we briefly explain Naïve Bayes,
k-Nearest Neighbor, Decision trees and Support Vector Machine techniques because
they are widely used in literature.
Naïve Bayes is a simple probability algorithm that determines the probability of a
document belonging to a particular genre. Naïve Bayes is a very fast learning algorithm,
which is robust to irrelevant features. It need less storage space and can handle missing
values. However, because the weights are the same for all features, performance can be
degraded by having more irrelevant features. This technique has been implemented by
(Argamon et al., ; Dewdney et al., ; Santini, ).
The k-Nearest Neighbor (k-NN) algorithm groups documents within a vector space.
tfidf (Term Frequency Inverse Document Frequency) is usually used to represent doc-
uments and Euclidean or cosine measure are often used to compute the similarity be-
tween documents. New documents are classified to the same genre as the nearest neigh-
http://www.natcorp.ox.ac.uk
72 JLCL
A New Centroid-based Approach for Genre Categorization of Web Pages
bor. The K represents how many neighbors should be analyzed. K-Nearest Neighbor
is used only by Lim et al. ().
Decision trees are a popular technique used by Argamon et al. (); Bretan et al.
(); Dewdney et al. (); Karlgren (); Finn (). Karlgren calculated
textual features for each document and categorized into a hierarchy of clusters based
on C. if-then categorization rules. The labels for genres were accomplished using
Nearest Neighbor assignments and cluster centroids.
Support Vector Machine is a powerful learning method introduced by Vapnik ().
It has been successfully applied to text categorization Joachims (). SVM is based
on Structural Risk Maximization theory, which aims to minimize the generalization
error instead of the empirical error on training data alone. The Support Vector Machine
technique has been used in genre categorization by many authors (e.g. Kanaris and
Stamatatos ; Dewdney et al. ; Meyer Zu Eissen and Stein ; Santini ).
2.3 Corpora and Results
The performance of genre categorization systems depends on used corpora. These
corpora are different from a work to another. Kessler et al. () used a corpus of
 texts from Brown Corpus belonging to six diverse genres (reportage, scientific and
technical, fiction, etc). They report . and . accuracies for logistic regression and
neural network classifiers, respectively. Dewdney et al. () used a corpus of 
texts belonging to seven diverse genres (advertisements, bulletin, boards, radio news,
etc.). They achieved ., . and . accuracies for Naïve Bayes, C. and SVM,
respectively. Meyer Zu Eissen and Stein () compiled the KI- corpus. In their
first experiment, they used  web pages ( web pages for each of the eight genres
included in the corpus) and applied discriminant analysis. They achieved an accuracy
of .. Boese and Howe () used the WebKB corpus to study the effect of web
genre evolution. Based on logistic regression classifier, they report an accuracy of ..
Kanaris and Stamatatos () used the KI- corpus and the SVM classifier. They
obtained accuracy between . and .. Santini () used SVM to classify the
KI- corpus. She reports an accuracy of about .. Mehler et al. () used SVM
classifiers and a German newspaper corpus that contains , texts distributed over
 genres or types. Conducted experiments provide an F of . for QSA and an F
of . for DomTK.
3 Proposed Approach
The aim of our approach is to classify web pages by genre based on three different
features, which are the URL address, the logical structure and the hypertext structure.
The proposed approach is based on the construction of genre centroid using a set of
labeled web pages. Each new web page is assigned to all genres with different member-
ships, which represents the similarity between the web page and the centroid of each
genre. In this section we explains the feature extraction process. The representation
Volume 25 (1) – 2009 73
Jebari
of features, the construction of centroids, the categorization of new web page and the
combination of classifiers are described in sections ., ., . and ., respectively.
3.1 Feature Extraction
In our approach, we used three different types of features, which are the URL address,
the logical structure and the hypertext structure.
The URL is encoded as a text line, which contains genre-specific words. For example,
the presence of “FAQ” and “CV” in the file name is a reliable hint of the membership
of a web page to the FAQ and CV genres, respectively.
The logical and hypertext structures of a web page are encoded into the html tags
used in the web page. The logical structure is represented by the text between <
title > and < /title > tags and the text between < Hn > and < /Hn > tags
(n = 1, . . . , 6). While, the hypertext structure is represented by the text included in
the anchors (between < A . . . > and < /A . . . > tags).
To quantify the contextual and structural information’s, we use the bag-of-words
approach (e.g. see Dewdney et al. ()). This approach considers all words without
ordering.
3.2 Representation
Web page representation is performed through three principal steps, which are the
pre-processing, the term weighting and the normalization.
3.2.1 Pre-processing
As widely known, pre-processing is a basic step in document categorization. In our
approach, the aim of this step is summarized in the following points:
• Tokenize text into words.
• Remove numbers, non-letter characters and special characters.
• Remove stop words, which are automatically identified using the Luhn Law (Luhn,
).
• Use the information gain to reduce the number of obtained terms (Yang and
Pedersen, ).
• Stem selected terms using the Porter stemmer (Porter, ).
3.2.2 Term Weighting
In our work, web pages are represented using vector space model by three different
vectors representing the URL, the logical structure and the hypertext structure (Salton
74 JLCL
A New Centroid-based Approach for Genre Categorization of Web Pages
and Buckley, ). For each feature, a web page is represented by a vector pj of terms.
Each term ti is weighted using the tfidf weighting technique (Salton and Buckley, ).
In this technique, the wij of a term ti in a web page pj increases with the number of
times that the term ti occurs in the page pj and decreases with the number of times
the term ti occurs in the collection. This means that the importance of a term in a
page is proportional to the number of times that the term appears in the page, while
the importance of the term is inversely proportional to the number of times that the
term appears in the entire collection. Formally, is defined as follow:
wij =
tfij
max(tf1j)
× log( |D|
nti
) ()
Where tfij is the number of times that term ti appears in web page pj , |D| is the
total number of pages in the collection, and nti is the number of pages where term ti
appears.
3.2.3 Normalization
The tfidf technique favors big documents instead of small documents. To deal with this
problem, Lertnattee and Theeramunkong () proposed a normalization technique,
called TD, which based on term distribution within a particular class and within a
collection of documents.
The term distribution is based three different factors. These factors depend on the
average frequency of the term ti in all pages of genre gk. This average, denoted by tfik
is defined as follow:
tfik =
∑
pj∈gk
tfijk
|Dgk |
()
Where Dgk represents the set of web pages that belong to genre gk, tfijk the frequency
of term ti in page pj of genre gk.
The normalization factors are the inter-class standard deviation (icsd), the class
standard deviation (csd) and the standard deviation (sd).
The inter-class standard deviation promotes a term that exists in almost all genres
but its frequencies for those genres are quite different. For a term ti, this factor is
defined as follow:
icsdi =
√√√√∑k[tfik − ∑k tfik|G| ]2
|G| ()
The class standard deviation of a term ti in a genre gk depends on the different fre-
quencies of the term in the pages of that genre, and varies from genre to genre. This
factor is defined as follow:
csdik =
√∑
dj∈gk
[tfijk − tfik]2
|gk|
()
Volume 25 (1) – 2009 75
Jebari
The standard deviation of a term ti depends on the frequency of that term in the pages
in the collection and is independent of genres. Is defined as follow:
sdi =
√√√√√∑k∑dj∈gk [tfijk −
∑
k
∑
dj∈gk
tfijk∑
k
|gk|
]2∑
k
|gk|
()
Using the tfidf weighting technique and term distributions for normalization, the weight
of term ti for page pj in genre gk is defined as follow:
wtdijk = wij × sdαi × icsdβi × csd
γ
ik ()
Where α, β and γ are the normalization parameters, which used to adjust the relative
weight of each factor and to indicate whether it is used as a multiplier or as a divisor
for the term’s tfidf weight, wij . An experimental study is conducted in section  to
identify the appropriate values of these parameters.
3.2.4 Construction of Genre Centroids
The centroid of a particular genre gj is represented by a vector Gj . This centroid is
the combination of the vectors pj belonging (or not) to that genre. Several ways were
proposed to calculate this centroid. The most used one is the normalized sum, defined
as follow:
Gj =
1
‖gj‖
·
∑
pi∈gj
pi ()
We observed that web pages that are far away from its genre centroid tend to reduce
the performance of categorization. Our hypothesis is that these kinds of web pages
increase web search noise and not considered as useful training pages. For this reason,
they should be excluded during centroid computation.
Assume that you have obtained a set of genre centroids G = {G1, . . . , Gj , . . . , G|G|},
where |G| is the number of genres. In our approach, we discarded web pages that have
a similarity with the genre centroid less than a predefined threshold s0.
For each genre gj , we calculate a new set of training or labeled web pages sj as
follow:
sj = {pi ∈ gj\sim(pi, Gj) ≥ s0} ()
Where pi is a web page and sim is the cosine similarity between the page pi and the
genre centroid Gj defined as follow:
sim(pi, Gj) =
pi ·Gj
‖pi‖ · ‖Gj‖
()
76 JLCL
A New Centroid-based Approach for Genre Categorization of Web Pages
The sets of training or labeled pages obtained after refining, will be used to recalculate
the genre centroids using the normalized sum presented in equation  as follow:
Sj =
1
‖sj‖
·
∑
pi∈sj
pi ()
Finally, the refined centroids will be applied to classify new web pages. An experimental
study will be conducted in the next section to choose the appropriate refining threshold.
Note that the complexity of centroids construction is linear in the number of labeled
web pages m and the number of predefined genres |G|, hence, learning time is bounded
by O(m|G|).
3.3 Categorization of New Web Pages
In our approach, the categorization of new web pages is performed incrementally. For
each new web page p, we calculate its cosine similarity with all genre centroids. Then,
we refine the centroids, which have a similarity with the page p, greater or equal than
S0. The refining process is performed by the following attributions:
NSi = NSi + p, Si =
NSi
‖NSi‖
()
Where NSi is the non-normalized centroid of the genre gi and represent the norm of
the vector NSi. Is calculated as follow:
‖NSi‖ =
√ ∑
k∈NSi
wtd2ijk ()
The complexity of web page classification is linear in the number of genres |G| and the
number of unlabeled web pages. So, the running time for classification is bounded by
O(n|G|).
3.4 Combination
The basic idea behind the combination of different classifier methods is to create a more
accurate classifier via some combination of the outputs of the contributing classifiers. In
our approach, the idea is based on the intuition that, combining homogenous classifiers
using heterogeneous features might improve the final result.
3.4.1 OWA Operators
OWA (Ordered Weighting Average) operators were first introduced in Yager ().
Generally, a mapping F : [0, 1]n → [0, 1] is called an OWA operator of dimension n if
it is associated with a weighting vector W = [w1, . . . , wi, . . . , wn], such that wi ∈ [0, 1],∑
i
wi = 1 and F (a1, . . . , an) =
∑
i
wibi, Where bi is the ith largest element in the
Volume 25 (1) – 2009 77
Jebari
collection a1, . . . , an. Yager () suggested that exist two methods for identifying
weights wi’s. The first approach uses learning techniques. The second one gives some
semantics to the weights. Then, based on these semantics, we can provide the values
for weights.
In our approach, we used the second approach based on fuzzy linguistic quantifiers
for the weights. According to Zadeh (), there are two types of quantifiers: absolute
and relative. Here, we used relative quantifiers typified by terms such as “as most”, “as
least half”, etc. A relative quantifier Q is defined as a mapping Q : [0, 1] → [0, 1],
verifying Q(0) = 0, there exists r ∈ [0, 1] such that Q(r) = 1 and Q is a non decreasing
function. Herrera and Verdegay () defined a quantifier function as follows:
Q(r) =
 0, ifr < a;r−ab−a , ifr ∈ [a, b];1, ifr > b. ()
With a, b ∈ [0, 1] are two parameters. Yager () compute the weight wi(i =
1, . . . , n) as follow:
wi = Q(
1
n
)−Q( i− 1
n
) ()
Where n is set to 3 because we have three classifiers, named URL, logical and hypertext
classifiers. According to the values of parameters a and b, we have used the following
function operators:
• Minimum: Represented by the quantifier “For all” and the function:
Q(r) =
{
0, r 6= 1;
1, r = 1.
()
• Maximum: Represented by the quantifier “There exists” and the function:
Q(r) =
{
0, r < 1/3;
1, r ≥ 1. ()
• Median: Represented by the quantifier “At least one” and the function:
Q(r) =
{
0, r < 0;
r, 0 ≤ r ≤ 1;
1, r > 1.
()
• Vote: Represented by the quantifier “At least half” and the function:
Q(r) =
{
0, r < 0;
2r, 0 ≤ r ≤ 0.5;
1, r > 0.5.
()
78 JLCL
A New Centroid-based Approach for Genre Categorization of Web Pages
• Vote: Represented by the quantifier “As possible” and the function:
Q(r) =
{
0, r < 0.5;
2r − 1, 0.5 ≤ r ≤ 1;
1, r > 0.5.
()
3.4.2 Decision Templates
This technique is proposed by Kuncheva et al. (). Let E1, E2 and E3 are the
URL, the logical and the hypertext classifiers. Each of these classifiers produces the
output Ei(p) = [di1(p), . . . , di|G|(p)] where dij(p) is the membership degree given by
the classifier Ei that a web page p belong to the genre j. The outputs of all classifiers
can be represented by a decision profile DP matrix as follow:
DP (p) =
(
d11(p) . . . d1|G|(p)
d21(p) . . . d2|G|(p)
d31(p) . . . d3|G|(p)
)
()
Using the training set Z = Z1, . . . , ZN , we have compute the fuzzy template F of each
genre i, which represented by a 3 × |G| matrix Fi = fi(k, s). The element fi(k, s) is
calculated as follow:
fi(k, s) =
∑N
j=1
Ind(Zj , i) · dks(Zj)∑N
j=1
Ind(Zj , i)
()
Where Ind(Zj , i) is an indicator function with value  if Zj comes from genre i and
 otherwise. At this stage, the ranking of genres can be achieved by aggregating the
columns of DP using fixed rules (minimum, maximum, product, average, etc.). An-
other method calculates a soft class label vector with components expressing similarity
S between the decision template DP and the fuzzy template F . The final classification
CLV is defined as follows:
CLV (p) = [µ1(p), . . . , µi(p), . . . , µ|G|(p)] ()
Where µi(p) is the similarity S(Fi, DP (p)) between the fuzzy template Fi of the genre
i and the decision profile DP (p) of the web page p. This similarity is calculated using
the Euclidean measure as follow:
µi(p) = S(Fi, DP (p)) = 1−
1
3× |G| ·
3∑
k=1
|G|∑
s=1
(fi(k, s)− dks(p))2 ()
4 Evaluation
Our approach is implemented in the Flexible, Refined and Incremental Centroid-based
Classifier (FRICC) system. Here we conducted many experiments to evaluate the
FRICC system. The aims of the evaluation process can be summarized as follow:
Volume 25 (1) – 2009 79
Jebari
Table 1: Composition of the KI-04 corpus.
Genre ] of web pages
Article 
Download 
Link collection 
Private portrayal 
Non-private portrayal 
Discussion 
FAQ 
Shop 
• Identify the best proportions of labeled and unlabeled web pages for those we
can achieve the best performance.
• Identify the appropriate number of terms for those we can obtain the best perfor-
mance.
• Identify the appropriate values of normalization parameters.
• Identify the best refining thresholds.
• Identify the best combination technique.
4.1 Corpora
In our experiment, we use the KI- corpus and WebKB collection. These corpora
are composed of English web pages, each of which is associated with its URL address.
Each web page in these corpora is associate to a single genre class.
• KI- corpus was compiled by Meyer Zu Eissen and Stein (). It is composed
of  HTML web pages, which are divided into  genres (see Table ).
Both these corpora can be reached through the WebGenreWiki http://http://www.webgenrewiki.
org/index.php5/Genre_Collection_Repository/
80 JLCL
A New Centroid-based Approach for Genre Categorization of Web Pages
Table 2: Composition of the WebKB corpus.
Genre ] of web pages
Student 
Faculty 
Staff 
Department 
Project 
Course 
• WebKB corpus is gathered in Carnegie-Mellon University during the WebKB
project (Craven et al., ). This corpus contains  HTML web pages from
 different universities. The corpus comprises  genres (See Table ).
4.2 Performance Measure
For multi-class corpus, it is suitable to use the break-even-point (BEP ), which is de-
fined in terms of the standard measures of precision and recall (Joachims, ). Pre-
cision P is the proportion of true document-category assignments among all assign-
ments predicted by the classifier. Recall R is the proportion of true document-category
assignments that were also predicted by the classifier. Formally, the BEP statistic
finds the point where precision and recall are equal. Since this is hard to achieve in
practice, a common approach is to use the arithmetic mean of recall and precision
as an approximation, i.e. BEP = (P + R)/2. Since our corpora are unbalanced, we
used the micro-averaged BEP computed by first summing the elements of all binary
contingency tables (one for each genre). Then, the micro-averaged BEP is computed
from these accumulated statistics. Note that the noisy web pages are not considered
in evaluation process.
4.3 Experimental Setup
To measure the performance, we used the 10 × k cross-validation. For this end, we
randomly split each corpus into k equal parts. Then we use one part for testing
and the remaining parts for remaining. This process is performed  times and the
final performance is the average of the  performances. The number k is identified
experimentally according to the used feature and corpus.
Volume 25 (1) – 2009 81
Jebari
Table 3: Best proportions of training and test web pages (Test%-Train%).
URL Logical Structure Hypertext Structure
KI- %-% %-% %-%
WebKB %-% %-% %-%
4.4 Results
4.4.1 Effect of Incremental Aspect
In this experiment we varied the proportion of unlabeled web pages between % and
% by step of %. For each proportion we measured the micro-averaged BEP for
each feature and corpus. The results are illustrated in Figure .
Figure 1: Micro-averaged BEP for each feature and for both KI-04 (Left) and WebKB (Right) corpora when
the proportion of test pages is varied between 10% and 90%.
The curves presented in the Figure  shows that micro-averaged BEP depends on
the proportion of labeled and unlabeled web pages. According to these curves, we
noted that the logical structure classifier achieve the best performance for both KI-
and WebKB corpora.
The best proportions of unlabeled web pages for those we report the best performance
for the KI- corpus is more than those for WebKB corpus.
The proportions of unlabeled and labeled web pages for those we reported the best
performances are presented in the Table . These proportions are used in the next
experiments.
4.4.2 Effect of Vocabulary Size
The aim of this experiment is to identify the suitable values of number of terms, for
those we achieve the best performance. For this end, we calculate the micro-averaged
82 JLCL
A New Centroid-based Approach for Genre Categorization of Web Pages
Table 4: Best values of number of terms.
URL Logical Structure Hypertext Structure
KI-   
WebKB   
BEP by varying the number of terms between  and . The number of terms is fixed
according the information gain measure. Note that in this experiment, we used the
tfidf weighting technique without normalization. The obtained results are illustrated
in Figure .
Figure 2: Micro-averaged BEP for each feature and for both KI-04 (Left) and WebKB (Right) corpora when
the number of terms is varied between 5 and 3000.
The best values of number of terms for those we achieve the best performance are
summarized in Table . These values will be used in the next experiments.
4.4.3 Effect of Term Weighting
To evaluate the effect of each normalization factor alone (icsd, csd and sd), we con-
ducted a first experiment. The obtained results are showed in Table .
According to this table, we observed that the icsd factor is very suitable for the KI-
corpus because it contains heterogeneous genres. On the other hand, the sd factor
achieve the best performance for the WebKB corpus because it contains homogenous
genres.
To choose the appropriate value of normalization parameters, we varied the values
of α, β and γ between - and  by a step of .. The best results are presented in the
Table .
Volume 25 (1) – 2009 83
Jebari
Table 5: The effect of each normalization factor on genre categorization performance.
KI-
α β γ URL Logical Hypertext
   . . .
-   . . .
   . . .
 -  . . .
   . . .
  - . . .
WebKB
   . . .
-   . . .
   . . .
 -  . . .
   . . .
  - . . .
According to this table, we observed that the best performance is reported by setting
the normalization parameters α, β and γ to ., - and -. respectively. These values
will be used in the next experiment to choose the appropriate refining threshold.
4.4.4 Effect of Refining Aspect
To measure the effect of refining on genre categorization we varied the refining threshold
between  and  by step of .. Zero value means that is no refining. The obtained
results for all features and corpora are presented in the following figure. As illustrated
in the Figure , the value of refining threshold affects the micro-averaged BEP of genre
categorization.
Figure 3: Micro-averaged BEP for each feature and for both KI-04 (Left) and WebKB (Right) corpora when
the refining threshold is varied between 0 and 1.
84 JLCL
A New Centroid-based Approach for Genre Categorization of Web Pages
Table 6: The effect of normalization on genre categorization performance.
KI-
α β γ URL Logical Hypertext
.  . . . .
-. .  . . .
. -  . . .
. -. -. . . .
.  -. . . .
- . -. . . .
. - -. . . .
  -. . . .
- -.  . . .
WebKB
.  . . . .
-. .  . . .
. -  . . .
. -. -. . . .
.  -. . . .
- . -. . . .
. - -. . . .
  -. . . .
- -.  . . .
Table 7: Best number of noisy web pages and refining thresholds for each feature and corpus.
URL Logical Structure Hypertext Structure
KI- (.) (.) (.)
WebKB () (.) (.)
According to these results we notice that in the case of noisy web pages like those
contained in KI- corpus, the refining is very useful. On the other hand, for noiseless
corpus like WebKB corpus, the refining is useless. Therefore, we concluded that refining
work very well for noisy corpora. The best refining thresholds will be used in next
experiments.
The number of noisy web pages and the refining thresholds for those we reported the
best micro-averaged BEP are illustrated in the Table .
4.4.5 Effect of Combination
Here we conducted many experiments to choose the appropriate operator for combi-
nation. The obtained results are illustrated in the Table . These results show that
the decision templates technique provide the best micro-averaged BEP (. for KI-
corpus and . for WebKB corpus).
Volume 25 (1) – 2009 85
Jebari
Table 8: Micro-averaged BEP for each combination technique and for both KI-04 and WebKB corpora.
Combination technique KI- WebKB
Minimum . .
Maximum . .
Median . .
Vote . .
Vote . .
Decision templates . .
Table 9: Micro-averaged accuracy for both KI-04 and WebKB corpora.
Author KI- WebKB
Meyer Zu Eissen and Stein ( web pages) . -
Boese and Howe . .
Kanaris and Stamatatos ( web pages) . -
Santini ( web pages) . -
Our approach . .
5 Comparison
5.1 Genre Categorization Approaches
The majority of the previous studies do not provide a reliable comparison with other
approaches. The main reason for this is that, until recently, there were no publicly
available and standard corpora for this task. Another reason is that there is not agreed
sense of web page genres and each study focuses on a different set of genres (Meyer
Zu Eissen and Stein, ).
In this article, we propose a comparison with experiments by Meyer Zu Eissen and
Stein (), Kanaris and Stamatatos () and Santini (), who use the KI-
corpus. The WebKB corpus is used only by Boese and Howe ().
Notice that these works use accuracy as a performance measure. The micro-averaged
accuracy for both KI- and WebKB corpora and for each author is presented in Table
.
This table show that our approach outperforms other methods.
5.2 Machine Learning Techniques
Since, our approach is based on new learning aspects, we conducted experiments to
com-pare it against other known machine learning used in genre categorization. Among
these techniques, we used the Rocchio algorithm (Rocchio with α= and β= as
control parameters), k-Nearest Neighbor (k-NN with k=), Support Vector Machines
(SVM with Fisher Kernel), Naïve Bayes (NB) and decision trees (TreeNode). These
techniques are implemented in Rainbow toolkit . Micro-averaged BEP for each feature
and for both KI- and WebKB corpora are presented in Tables  and .
86 JLCL
A New Centroid-based Approach for Genre Categorization of Web Pages
Table 10: Micro-averaged BEP for the KI-04 corpus.
URL Logical structure Hypertext structure
FRICC . . .
SVM . . .
Rocchio . . .
NB . . .
k-NN . . .
TreeNode . . .
Table 11: Micro-averaged BEP for the WebKB corpus.
URL Logical structure Hypertext structure
FRICC . . .
SVM . . .
Rocchio . . .
NB . . .
k-NN . . .
TreeNode . . .
5.3 Statistical Significance
To determine the statistical significance of the results, we used 5 × 2 cross validation
t-test (Dietterich, ). The results are presented in Table . The symbols used in
this table are defined as follow:
• ≈ indicates no significant differences.
• < indicates that the row approach achieves a significantly lower measurement
then FRICC with . as a significance level.
• << indicates that the row approach achieves a significantly lower measurement
then FRICC with . as a significance level.
• <<< indicates that the row approach achieves a significantly lower measurement
then FRICC with . as a significance level.
This table shows that the FRICC approach outperforms all other machine learning
techniques in  case. Only SVM technique has similar performance. Note also that
decision trees performs disappointedly.
5.4 Train and Test Times
Besides the categorization performance that categorization techniques yields, another
important aspect to consider when comparing classification techniques is the time that
they require to execute. Time is a very important aspect for comparison, especially
when you wish to integrate our approach in a search engine. Figures ,  and  shows
Volume 25 (1) – 2009 87
Jebari
Table 12: Statistical Significance of our approach FRICC against other machine learning techniques.
KI-
URL Logical Structure Hypertext Structure
SVM ≈ << <<
Rocchio << < <<
NB << << <<<
k-NN << << <<<
TreeNode <<< <<< <<<
WebKB
SVM ≈ << ≈
Rocchio << < <<
NB << << <<<
k-NN <<< << <<<
TreeNode <<< <<< <<<
a comparison of the time that each classification technique needs to execute, in both
the training and classification phases for respectively KI- and WebKB corpora.
Figure 4: Train and test times for URL and for both KI-04 (left) and WebKB (right) corpora.
The results show that our approach is the fastest. Also Rocchio and SVM are very
fast. These results can be explained by the fact that the required time is proportional
to the number of categories instead of the number of web pages. Decision tree is the
slowest machine learning technique for all features and for both KI- and WebKB
corpora.
6 Conclusion and Future Work
In this article, we proposed a new approach for genre categorization of web pages. Our
approach implements four new aspects, which not explored in previous studies on genre
categorization. These aspects are flexibility, refining, incrementing and combination.
88 JLCL
A New Centroid-based Approach for Genre Categorization of Web Pages
Figure 5: Train and test times for logical structure and for both KI-04 (left) and WebKB (right) corpora.
Figure 6: Train and test times for hypertext structure and for both KI-04 (left) and WebKB (right) corpora.
Also we have conducted many experiments to measure the effectiveness and speed of
each aspect in genre categorization. The comparison with other approaches shows that
our approach is very fast and outperforms many known genre categorization approaches
and machine learning techniques. The work presented in this article represent the de-
scription of our PhD thesis research. In the future we hope to investigate the following
points:
• As the pdf format is a very useful format on the web, we propose to classify pdf
documents.
• In my PhD work, I have used only English web page, in the future we wish to
focus on Arabic web documents.
Volume 25 (1) – 2009 89
Jebari
• As our approach is very fast and outperforms many other machine learning tech-
niques, we hope to include it in a search engine (i.e. Google, Firefox), in a similar
way as the WEGA add-on (Stein et al., ).
Acknowledgement
I would like to thank the anonymous reviewers and the journal’s editors for their useful
comments on this articles.
References
Argamon, S., Koppel, M., and Avneri, G. (). Routing documents according to
style. In Proc. International Workshop on Innovative Internet Information Systems
(IIIS-), Pisa.
Boese, E. S. (). Stereotyping the Web: Genre Classification of Web Documents
(M.S. Thesis). Computer Science Department, Colorado State University, USA.
Boese, E. S. and Howe, A. E. (). Effect of web document evolution on genre clas-
sification. In Proceedings of the th ACM International conference on Information
and knowledge management.
Bretan, I., Dewe, J., Hallberg, A., Wolkert, N., and Karlgren, J. (). Web-specific
genre visualisation. In Proceedings of the rd World Conference on the WWW and
Internet.
Craven, M., DiPasquo, D., Freitag, D., McCallum, A., Mitchell, T., Nigam, K., and
Slattery, S. (). Learning to extract symbolic knowledge from the word wide web.
In Proceedings of the th conference on artificial Intelligence.
Dewdney, N., VanEss-Dykema, C., and MacMillan, R. (). The form is the sub-
stance: Classification of genres in text. In Proceedings of the th Annual Meeting of
the Association for Computational Linguistics and th Conference of the European
Chapter of the Association for Computational Linguistics.
Dietterich, T. G. (). Approximate statistical tests for comparing supervised classi-
fication learning algorithms. Neural Comput, ():–.
Finn, A. (). Machine learning for genre classification (M.S. Thesis). Computer
Science Department, University College of Dublin, UK.
Finn, A. and Kushmerick, N. (). Learning to classify documents according to
genre. In Proceedings of the Workshop "DOING IT WITH STYLE: Computational
Approaches to Style Analysis and Synthesis, Mexico.
90 JLCL
A New Centroid-based Approach for Genre Categorization of Web Pages
Herrera, F. and Verdegay, J. L. (). Genetic algorithms and soft computing. Physi-
caVerlag, Heidelberg, Germany.
Joachims, T. (). A probabilistic analysis of the rocchio algorithm with tfidf for
text categorization. In Proceedings of ICML-, pages –.
Joachims, T. (). Text categorization with support vector machines: learning with
many relevant features. In Proceedings of th European Conference on Machine
Learning.
Kanaris, I. and Stamatatos, E. (). Webpage genre identification using variable
length character n-grams. In Proceeding of the th IEEE International Conference
on Tools with Artificial Intelligence.
Karlgren, J. (). Stylistic experiments in information retrieval. In Natural Language
Information Retrieval.
Kennedy, A. and Shepherd, M. (). Automatic identification of home pages on the
web. In Proceedings of the th Annual Hawaii International Conference on System
Sciences.
Kessler, B., Nunberg, G., and Schütze, H. (). Automatic detection of text genre.
In Proceedings of the th ACL/th EACL, pages –.
Kuncheva, L. I., Bezdek, J. C., and Duin, R. P. (). Decision templates for multiple
classifier fusion. Pattern Recognition, ():–.
Lertnattee, V. and Theeramunkong, T. (). Effect of term distributions on cen-
troid-based text categorization. Journal of Information Sciences, ():–.
Lim, C. S., Lee, K. J., and Kim, G. C. (). Multiple sets of features for auto-
matic genre classification of web documents. Journal of Information processing and
management, ():–.
Luhn, H. P. (). The automatic creation of literature abstracts. IBM Journal of
Research and Development, ():–.
Mehler, A., Geibel, P., and Pustylnikov, O. (). Structural classifiers of text types:
Towards a novel model of text representation. LDV Forum, ():–.
Mehler, A., Sharoff, S., and Santini, M., editors (). Genres on the Web: Computa-
tional Models and Empirical Studies. Springer, Berlin/New York.
Meyer Zu Eissen, S. (). On Information Need and Categorizing Search (PhD.
Thesis). University of Paderborn.
Volume 25 (1) – 2009 91
Jebari
Meyer Zu Eissen, S. and Stein, B. (). Genre classification of web pages: User study
and feasibility analysis. In Proceedings KI : Advances in Artificial Intelligence,
pages –.
Mitchell, T. (). Machine learning. McGraw-Hill.
Porter, M. (). An algorithm for suffix stripping. Program, ():–.
Rauber, A. and Müller-Kögler, A. (). Integrating automatic genre analysis into
digital libraries. In JCDL ’: Proceedings of the st ACM/IEEE-CS joint conference
on Digital libraries, pages –.
Salton, G. and Buckley, C. (). Term weighting approaches in automatic text
retrieval. Information Processing and Management, ():–.
Santini, M. (). Automatic identification of genre in web pages (PhD. Thesis).
University of Brighton, UK.
Shepherd, M., Watters, C., and Kennedy, A. (). Cybergenre: automatic identifica-
tion of home pages on the web. Journal of Web Engineering, ():–.
Shepherd, M. A. and Watters, C. (). Evolution of cybergenre. In Proceedings of
the nd Hawaiian International Conference on System Sciences.
Stamatatos, E., Fokatakis, N., and Kokkinakis, G. (). Text genre detection using
common word frequencies. In Proceedings of the th International Conference on
Computational Linguistics.
Stein, B., Meyer zu Eissen, S., and Lipka, N. (). Web genre analysis: Use cases,
retrieval models, and implementation issues. In Mehler et al. ().
Vapnik, V. N. (). The Nature of Statistical Learning Theory. Springer, New York.
Yager, R. (). On ordered weighted averaging aggregation operators in multi-
-criteria decision making. IEEE Transactions on Systems, Man, and Cybernetics,
():–.
Yang, Y. and Pedersen, J. O. (). A comparative study on feature selection in text
categorization. In Proceedings of ICML-.
Zadeh, L. (). The role of fuzzy logic in the management of uncertainty in expert
systems. Fuzzy Sets and Systems, ():–.
92 JLCL
