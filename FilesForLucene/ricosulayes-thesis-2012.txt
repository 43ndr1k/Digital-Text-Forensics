 
 
 
 
QUANTITATIVE AUTHORSHIP ATTRIBUTION OF USERS OF MEXICAN DRUG DEALING RELATED 
ONLINE FORUMS 
  
 
 
 
 
 
A Dissertation 
submitted to the Faculty of the 
Graduate School of Arts and Sciences 
of Georgetown University 
in partial fulfillment of the requirements for the 
degree of 
Doctor of Philosophy 
in Linguistics 
 
 
 
 
By 
 
 
 
 
Antonio Rico Sulayes, M.A. 
 
 
 
 
 
Washington, DC 
November 9, 2012
 
ii 
 
 
 
 
 
 
 
 
 
 
 
 
Copyright 2012 by Antonio Rico Sulayes 
All Rights Reserved 
  
 
iii 
 
QUANTITATIVE AUTHORSHIP ATTRIBUTION OF USERS OF MEXICAN DRUG DEALING RELATED 
ONLINE FORUMS 
 
Antonio Rico Sulayes, M.A. 
 
Thesis Advisor:  Natalie Schilling, Ph.D.  
 
ABSTRACT 
 
Abstract 
 As the violence in the Mexican drug war escalates, a proliferation of social media sites about 
drug trafficking in Mexico was followed by the murder of some of their users, and the eventual 
disappearance of many of those sites. Despite these events, there still exist a number of drug-
dealing related social media outlets in this country with a large number of contributions. In this 
dissertation, I show that quantitative authorship attribution techniques, including state of the art 
machine learning algorithms, can be successfully applied to match posts of unknown authorship 
with their authors. Employing data from randomly selected prolific users of a drug-dealing 
related online forum, in this research project I test a number of quantitative classification 
techniques in over a thousand authorship attribution tasks. These tasks attempt to recognize the 
author of texts, which are chosen to represent anonymous texts, within a closed set of known 
authors. In the best results rendered in all these experiments, which include corpora with up to 40 
potential authors, the accuracy obtained is higher than for previous research using data from drug 
dealing related online forums and employing discriminant analysis (DA), the first method ever 
applied to this kind of data (Rico-Sulayes, 2011). These results are obtained with the statistically 
relevant contribution of a number of novel discriminating features. Examining the features used 
in the experiments with the best results, these features (tagged with a fully automated system) 
represent paralinguistic and grammatically shallow information, and yet they seem to capture 
 
iv 
 
stylistic decisions or habits that permit a quantitative approach to obtain a success rate greater 
than previous research with DA in authorship attribution. By both offering an analysis of the 
kind of information that renders the best results in the experiments conducted and improving the 
success rate of previous research, this dissertation should help further the application and 
acceptance of authorship attribution research in real-life contexts, such as criminal investigation 
and legal prosecution. 
  
 
v 
 
TABLE OF CONTENTS 
Chapter 1: Introduction ................................................................................................................... 1 
1.1 Problem statement ............................................................................................................ 2 
1.2 Background ...................................................................................................................... 3 
1.2.1 Need for the study ..................................................................................................... 3 
1.2.2 Rationale ................................................................................................................... 5 
1.2.3 Significance............................................................................................................... 7 
1.3 Research questions ........................................................................................................... 8 
1.4 Outline of the Dissertation ............................................................................................... 9 
Chapter 2: Defining Authorship Analysis and Authorship Attribution ........................................ 12 
2.1  Authorship Analysis Tasks............................................................................................. 12 
2.2  Authorship Attribution Task .......................................................................................... 29 
Chapter 3: Authorship Analysis Approaches ................................................................................ 64 
3.1  Feature Selection and Feature Reduction Techniques ................................................... 66 
3.1.1  Feature Selection in Authorship Attribution ........................................................... 73 
3.1.2  Feature Reduction Techniques in Authorship Attribution ...................................... 82 
3.1.3  Feature Selection in Author Profiling ..................................................................... 99 
3.1.4  Feature Reduction Techniques in Author Profiling .............................................. 108 
3.2  Class Attribution Methods in Authorship Analysis ..................................................... 111 
 
vi 
 
3.2.1  Quantitative Class Attribution Methods in Authorship Attribution ..................... 116 
3.2.2  Quantitative Class Attribution Methods in Author Profiling ................................ 120 
3.3 Authorship Analysis Approaches on Spanish Data .......................................................... 124 
Chapter 4: Quantitative Authorship Attribution of Online Forum Posts in Spanish .................. 136 
4.1 Data for Authorship Attribution Experiments .................................................................. 139 
4.1.1  The Drug-Dealing Related Social Media Phenomenon in Mexico ...................... 140 
4.1.2  Experimental Corpora ........................................................................................... 142 
4.3 Authorship Attribution Approach Components ................................................................ 150 
4.3.1  Set of Authorship Features.................................................................................... 151 
4.3.1a Feature Selection ................................................................................................... 155 
4.3.1b Feature Reduction Techniques .............................................................................. 168 
4.3.2  Class Attribution Methods .................................................................................... 179 
4.3.3  Authorship Attribution Approach Configurations ................................................ 196 
Chapter 5: Analysis of Results .................................................................................................... 201 
5.1  Measuring Results ........................................................................................................ 201 
5.1.1  The Success Rate in the Experiments: Accuracy .................................................. 202 
5.1.2  Results: The Effects of Classifier, Feature Reduction, and Normalization .......... 206 
5.1.3 Comparison of Authorship Attribution Configurations over All Corpora............ 212 
5.1.4 The Effects of Automated Tagging and Machine Learning Classification .......... 220 
 
vii 
 
5.1.5 Summary of Results .............................................................................................. 222 
5.2 Linguistic Components of Authorship Attribution Experiments ................................. 223 
Chapter 6: Conclusion................................................................................................................. 248 
6.1 Revisiting the Research Questions .................................................................................... 249 
6.2 Discussion of Contributions to Authorship Analysis ....................................................... 251 
6.3 Future Work ...................................................................................................................... 255 
Appendix A: Function Word N-Grams in Spanish ..................................................................... 258 
Appendix B: Frequent Keyboard-Based Emoticons in Spanish Online References .................. 262 
Appendix C: Features Selected Using IG ................................................................................... 263 
Appendix D: Features Selected Using CFS ................................................................................ 268 
References ................................................................................................................................... 270 
 
 
  
 
viii 
 
LIST OF FIGURES 
Figure 4.1. A linear classification problem with two authors as classes .....................................191 
Figure 4.2. A non-linear classification problem with two authors as classes  .............................192 
Figure 4.3. Support vectors attempting the maximum-margin decision  .....................................195 
Figure 5.1. MNB, non-normalized data, and all reduction techniques, over 39 corpora .............213 
Figure 5.2. NB, non-normalized data, and all reduction techniques, over 39 corpora  ...............215 
Figure 5.3. DA, non-normalized data, and all reduction techniques, over 39 corpora  ...............217 
Figure 5.4. Feature set reduced with frequency, non-normalized data, and all classifiers, over 39 
corpora  ........................................................................................................................................219 
 
 
  
 
ix 
 
LIST OF TABLES 
Table 2.1 Authorship analysis tasks and their corresponding terms in other studies  .................. 28 
Table 2.2 Data source collected and topic selection in previous authorship attribution studies 
....................................................................................................................................................... 41 
Table 2.3 Number of subjects, training data, and testing data in previous authorship attribution 
studies  .......................................................................................................................................... 49 
Table 2.4 Distribution of information between data sets and some corpus figures in previous 
authorship attribution studies  ....................................................................................................... 55 
Table 3.1 Number of features, feature categories, and selected features samples in previous 
authorship attribution studies  ....................................................................................................... 76 
Table 3.2 Authorship attribution studies testing feature subsets and their best results  ............... 81 
Table 3.3 Authorship attribution studies using feature reduction techniques and their best results  
....................................................................................................................................................... 96 
Table 3.4 Number of features, feature categories, and selected features sampled in previous 
author profiling studies  .............................................................................................................. 102 
Table 3.5 Profiling categories, feature subset testing, and best results for subsets in previous 
author profiling studies  .............................................................................................................. 106 
Table 3.6 Author profiling studies using feature reduction techniques and their best results  
..................................................................................................................................................... 110 
 
x 
 
Table 3.7 Class attribution methods used in previous authorship attribution studies  ................ 118 
Table 3.8 Class attribution methods used in previous author profiling studies  ......................... 122 
Table 3.9 Languages targeted in previous authorship attribution studies  .................................. 125 
Table 4.1 Number of words for largest and smallest testing sample and total words per author 
 .................................................................................................................................................... 148 
Table 4.2 List of all authorship features in their original order before reduction/classification 
..................................................................................................................................................... 167 
Table 4.3 Approach configurations run on each experimental corpus ........................................197 
Table 5.1 Confusion matrix, accuracy, and error rate for experiment with ten authors ............. 205 
Table 5.2 Results for all eight approach configurations with MNB as the classifier ................. 207 
Table 5.3 Accuracy results for all 40 authorship attribution approach configurations  .............. 210 
Table 5.4 Best results obtained with different classifiers in the corpus with 10 authors  ........... 221 
Table 5.5 Approach configurations matching or improving previous results with DA  ............. 222 
Table 5.6 Size of feature sets in experimental corpora  .............................................................. 227 
Table 5.7 Features selected by IG and CFS in experiments with 10 or more authors  ............... 229 
Table 5.8 Linguistic/paralinguistic information in features commonly selected by IG or CFS 
..................................................................................................................................................... 243 
 
 
 
1 
 
CHAPTER 1: INTRODUCTION 
This chapter will present, in general terms, the task that will be the center of this dissertation, 
authorship attribution, as well as the specific context in which it will be applied, drug-dealing 
related online forums in Spanish. One of the various tasks targeted in the more general field of 
authorship analysis, as will be contextualized in Chapter 2, authorship attribution is aimed at 
recognizing the author of an anonymous text within a closed set of potential authors or subjects. 
The experimental part of this dissertation will explore the performance of different authorship 
attribution approaches, which will result by combining various options in the two main 
components of any attribution approach: the features that are selected to represent the authorship 
of a text and the method that uses these features to make an attribution, i.e., that classifies some 
anonymous text as belonging to some author. Among the options that will be explored regarding 
these two components of any authorship attribution approach, there will be newly proposed 
features, methods that are new in the context of authorship attribution, and both features and 
methods applied for the first time with some types of data (including new types of text and data 
in a language never explored with those components -- in this project, all data will be in 
Spanish). Besides these methodological innovations and contributions, this project will aim at 
improving previous results obtained in the study of authorship attribution with approach 
components that have been widely used in the literature. By improving previous results, this 
dissertation will contribute not only to widening the range of techniques used in authorship 
attribution studies, including both those that target Spanish data and those that use data in other 
languages, but it will also contribute to improve the results that this increasingly important task 
has achieved. Beyond the intrinsic value of achieving the highest success rate in the attribution of 
anonymous texts under controlled circumstances, the experiments conducted here and their 
 
2 
 
successful results should help improve the acceptance of this type of work in real-life contexts, 
such as criminal investigation and legal prosecution. 
1.1 PROBLEM STATEMENT 
In this dissertation, I will apply the most commonly used quantitative class attribution methods 
in authorship analysis (a decision-tree classifier, C4.5; discriminant analysis, DA; multinominal 
naive Bayes, MNB; the Bernoulli model of naive Bayes, NB; and support vector machines 
SVMs, all of which are introduced in detail in Chapter 4) to the task of authorship attribution -- 
as mentioned above, the task that attempts to recognize the author of an anonymous text within a 
closed set of potential authors or subjects. These commonly used quantitative class attribution 
methods will be combined with a collection of lexical, syntactic, and structural features that are 
intended to characterize the authorship of the text in which they appear. This collection of 
features will include both features adapted from previous research and newly proposed features, 
based on my reading of the texts included in the various tasks. Through applying new features I 
have intended to reach and improve the results I have previously obtained with DA, which 
represent the best results that have been achieved with this classification method in the task of 
authorship attribution (Rico-Sulayes, 2011). In contrast with the just cited study, in this research 
project I will modify the feature selection in order to have a fully automated tagging of the texts 
used in the experiments. In this study, the performance of the various class attribution methods, 
in combination with a number of reduced subsets of the features selected, will be evaluated 
across a range of values for the number of subjects in the set of potential authors. With this range 
of values, the evaluation of the different combinations of classification methods and feature 
subsets will be aimed not only at testing the success of the attribution method in some given 
situation, but also at identifying its limitations as the number of subjects in the set of potential 
 
3 
 
authors increases or decreases. Given the data targeted in this study, drug-dealing related online 
forum posts in Spanish, this work attempts to contribute to the potential use of state-of-the-art 
authorship analysis techniques in relevant criminal contexts, such as the various forms of 
electronic communications that are related to Mexican drug dealing. 
1.2 BACKGROUND 
1.2.1 Need for the study 
The need for the most sophisticated authorship analysis tools by the authorities in charge of 
fighting the drug war in Mexico becomes apparent when we review its escalating death toll and 
the diverse use of written communications by drug dealers in this country. Rapidly growing over 
the last five years, the estimated number of deaths in the Mexican drug war has gone from 2,800 
casualties in 2007, to 9,500 in 2009, to 15,300 in 2010 (Córdoba and Luhnow, 2011), to nearly 
13,000 during the first nine months of 2011 (Agar, 2012; Planas, 2012). Unofficial figures report 
16,466 deaths for 2011 (WM Consulting, 2012). Even with the partial figures for 2011, based on 
the last official statistics released by the Mexican government after months of requests for this 
information, in the last 12 months for which there is official information (September 2010 – 
September 2011) the death toll has risen 11% compared to former years (Planas, 2012). These 
numbers translate into a rate of one murder every half an hour according to Planas. During this 
increasingly worsening war, drug dealers have constantly sent written messages to the 
government and the general population through a number of media. They have used so-called 
narco mantas ‘narco banners’ or ‘canvas signs’ and narcopintas ‘narco graffiti’ displayed on 
streets or trucks patrolling small towns, narcomensajes ‘narco messages’ in signs taped to 
dismembered body parts and to barrels of caustic soda with disposed bodies, and electronic 
 
4 
 
communications posting comments on blogs and online forums (Blog del Narco, 2012; Foros 
Blog del Narco, 2010). Given all this incriminating written evidence, law enforcement 
organizations fighting drug gangs in Mexico could use tools that, relying on state-of-the-art 
statistical techniques in authorship analysis, help them during both criminal investigation and 
legal prosecution. In the Unites States, for example, authorship analysis has been be used in 
criminal cases that offer a closed set of suspects. McMenamin (2002, pp. 207-216) gives a long 
list of legal cases where expert testimony regarding authorship analysis has been presented. 
Although to the best of my knowledge there is no central record of the use of expert testimony on 
authorship attribution in Mexico (not in the United States; e.g. Howald, 2008), in its Código 
Federal de Procedimientos Penales (CFPP) 239, Mexican law considers the possibility of using 
expert testimony to verify the authenticity of written documents by the examination of letras y 
firmas1 ‘letters and signatures’ (2010). Therefore, the CFPP opens the possibility of including 
handwriting and stylistic analysis for authorship attribution during legal prosecution. Besides, 
criminal investigation, which is usually more flexible in its procedures than legal prosecution, 
can benefit not only from authorship attribution but from author profiling when it makes “soft” 
decisions that guide investigation efforts in more reliable directions (Ortega-Garcia, Gonzalez-
Rodriguez, and Marrero-Aguilar, 2000). Author profiling can also be useful for law enforcement 
agencies during intelligence analysis of general patterns in criminal or risky activities (Juola, 
2008). Therefore, given the precedents of authorship attribution use in legal contexts and the 
                                                 
1
 In its third meaning for the headword letra, the Diccionario del español usual en México offers as its first three 
senses the following: “1 Escrito de cualquier clase, particularmente una carta o un mensaje” ‘1 Any kind of written 
text, particularly a letter or a message’, “2 Forma particular de escribir una persona o una escuela” ‘2 The 
particular way of writing of a person or a school’, “3 Letra manuscrita Escritura que se hace directamente con la 
mano, normalmente usando un lápiz, una pluma u otro utensilio manual” ‘3 Handwriting Writing that is made 
directly by hand, commonly using a pencil, pen, or another hand device.’ 
 
5 
 
explicit consideration of authorship analysis in Mexican law, the potential usefulness of 
authorship attribution and author profiling tools in this country’s drug war becomes apparent. 
 Having in mind the abundance of written communications in Mexico’s drug war and the 
consideration of authorship analysis in Mexican law (CFPP 239, 2010), this study will randomly 
select prolific users from a drug dealing related online forum and will randomly sample their 
known contributions to build various corpora with different numbers of authors. Then, a wide 
range of authorship attribution approaches, which attempt to recognize the author of an 
anonymous text within a closed set of subjects, will be tested in these various corpora. 
Anonymous texts will be represented by texts whose authorship is known but that remain 
anonymous during the experiments for testing purposes. By exploring the performance of 
various authorship attribution approaches on the multiple corpora, I will try to improve the 
results obtained previously in this kind of task and determine what approaches are the most 
successful, how the number of authors in the set of subjects affects the task, and what kind of 
information is the most useful in the experiments conducted. Although the authorship attribution 
experiments conducted here remain an academic exercise, answering these questions should be 
valuable not only for the potential application of authorship attribution methods to the Mexican 
drug war, but for the general study of authorship analysis. 
1.2.2 Rationale 
Before their potential deployment in the current Mexican drug war, authorship attribution tools 
face a number of challenges that will be addressed in the proposed research. First, authorship 
analysis methods in general have not achieved a consistent performance across different 
authorship analysis tasks and settings, so this study should contribute to the exploration of and 
improvement in the performance for the analysis methods studied here. Second, the application 
 
6 
 
of authorship analysis methods in general and authorship attribution in particular to Spanish data 
is rare. The performance of attribution methods applied to data in other languages may not be 
successful or features may not suitable for adaptation. Therefore, the application of previously 
used statistical and machine learning classification techniques (and the sets of features provided 
to them) has to be tested in and most likely adapted for this language. For example, among the 
five class attribution methods selected, the four machine learning classification methods (C4.5, 
MNB, NB, and SVMs) have never been applied to Spanish data in the context of authorship 
attribution. Third, there are elements of the authorship attribution approaches used here that have 
not been applied to the type of data collected for this study. For example, unlike the four 
machine learning classification methods just mentioned, the statistical classifier selected, DA, 
has not been applied to online forums for authorship attribution, except for previous research I 
conducted (Rico-Sulayes, 2011). Defining characteristics of the contributions by online forum 
users, such as the paucity and the shortness of contributions, can hinder the performance success 
of formerly tested classification methods. Hence, both the classification methods and the features 
previously used in attribution tasks have to be tested when using data collected from online 
forums and related social technologies (e.g., blogs and social networks) in Spanish. 
Another important component of this research proposal that should be mentioned is the 
identification of limits for the various attribution approaches evaluated as relevant experimental 
conditions vary. In this respect, this study will explore how variation in the number of subjects in 
the set of potential authors affects the performance of a given attribution approach. Various 
attribution approaches will be designed by combining a number of feature subsets and 
classification methods. For each of these approaches, the potential changes in their performance, 
as the number of subjects in the set of authors increases or decreases, and their patterns (if there 
 
7 
 
are any) will be identified and made explicit. This should be useful to potential users of similar 
attribution approaches in the future. 
1.2.3 Significance 
 This project will make contributions to both the literature that has studied authorship 
attribution in general and previous authorship analysis that has targeted Spanish data. As to the 
first general area, this project will expand the scarce comparison of statistically-based, often 
automated methods that reduce the number of features used to attribute a text to some particular 
author -- this reduction is performed with the goal of improving the attribution success rate. 
Discussed in detail in Chapter 3, feature reduction methods, which decrease the total number of 
features used in an attribution, are widely used in different tasks of authorship analysis (such as 
in authorship attribution, which attempts to recognize the author of an anonymous text within a 
closed set of potential authors or subjects, and author profiling, which is aimed at characterizing 
an anonymous authors across a number of social categories, such as gender and age). However, 
there is very little comparison among the diverse methods that are available for feature reduction. 
Actually, only one study has made this kind of comparison (Rico-Sulayes, 2011), and it did so 
with methods that are usually restricted to specific software programs. Unlike the just cited 
article, this study will apply methods that are external to other components of the attribution and 
independent of software implementations. Also in this respect, this study will evaluate a method 
to reduce the number of features whose application is entirely new in the attribution of an 
anonymous text. This application will be useful not only because it will evaluate a completely 
different resource, but also because the reduction of features obtained by this method will be 
compared to other previously used methods in an attempt to offer an interpretation of the kind of 
information that is the most useful in the experiments conducted in this project. 
 
8 
 
 As to the specific body of literature that targets Spanish data, this study will contribute to 
it in a number of ways. First, this study will select features of authorship that have not been used 
in this body of literature. These features will include newly proposed features in Spanish, types 
of feature lists that have never been used in authorship attribution studies that target data in 
Spanish, and new combinations of various features that have not been put together formerly in 
the study of authorship analysis in this language. A second contribution will be the application of 
various attribution methods that process the formerly mentioned features. Except for one study 
(Nazar and Sánchez Pol, 2007), all authorship analysis studies that target Spanish data have used 
the same method (DA) to make the attribution of an anonymous text to some potential author. 
This contrasts with studies that target data in languages other than Spanish, which have used 
many different attribution methods. This study will apply to Spanish data all the most commonly 
used methods in authorship analysis in general (C4.5, DA, MNB, NB, and SVMs), which include 
the methods reported as the most successful ones. The third contribution of this study will be to 
compare various forms of the techniques that reduce the number of features to improve the 
attribution. Finally, this study will widen noticeably the scope of former authorship attribution 
studies that use texts in Spanish by using data from social media communications, specifically 
from online forums. Only once has this kind of data been used in authorship analysis with 
Spanish texts, and the study that uses this kind of data is the origin of this research project (Rico-
Sulayes, 2011). 
1.3 RESEARCH QUESTIONS 
The following research questions are derived from the above proposed work: 
1) Is it possible to at least match and ideally improve the best results obtained with DA in 
authorship attribution, while using a fully automated tagging of features?  
 
9 
 
2) By testing a number of different classifiers and feature sets with the goal of affirmatively 
answering the former question, what combination of classifiers and feature sets achieve this 
goal?  
3) Regarding the best results obtained in the whole set of experimental corpora, are there any 
novel features proposed in this project that contribute to these results? Do these novel features 
contribute to achieving the goal of at least matching and ideally improving previous research 
with DA? 
4) Are the best results obtained consistent across a range of values for the number of subjects in 
the set of potential authors? 
5) What kind of linguistic or paralinguistic information is used to render the best results obtained 
in the authorship attribution task targeted in this project? 
1.4 OUTLINE OF THE DISSERTATION 
The rest of this dissertation will be aimed at answering the questions just listed. This will be done 
over the next five chapters. In Chapter 2, the theoretical and conceptual framework for this study 
will be presented. In this chapter, a categorization of different tasks targeted in the field of 
authorship analysis will be given. With this categorization, the specific task of authorship 
attribution that is the center of this study will be defined. Also, a number of design decisions that 
have been reported to influence the success rate of this task in the research literature will be 
surveyed. The design decisions that will be included in Chapter 2 involve general issues in 
authorship attribution rather than specific linguistic features and analysis techniques. 
 Chapter 3 will survey the authorship analysis research literature in a number of areas 
relevant to this study. First, this chapter will present the two essential components of the 
attribution approach itself: the selection of relevant authorship features and the classification 
 
10 
 
method that processes these features to attribute an author to an anonymous text. Then, Chapter 3 
will survey the literature in authorship attribution and present what researchers have reported as 
the most efficient decisions regarding these two components of the attribution approach. This 
chapter will also include a survey of the two components just mentioned for another authorship 
analysis task: author profiling. The survey for author profiling will not only make apparent the 
methodological similarities between this task and authorship attribution, but it will help support 
the experimental decisions made later in this project. Chapter 3 will also survey all authorship 
analysis research that has targeted data in Spanish. With this survey, a number of gaps in this 
specific body of research will be identified in order to argue for the importance of the current 
dissertation. 
 In the methodological chapter of this project, Chapter 4, I will present the data and 
methods that will be applied in a number of authorship attribution experiments. This chapter will 
briefly present the kind of data that will be collected in this project and a number of corpora that 
will be built with such data. The detailed introduction of the various methodological choices 
tested later in the experimental part of this project will occupy most of this chapter. The various 
approach configurations that will result from combining these methodological choices will be 
presented and listed at the end of this chapter. 
 The experimental section of this dissertation is represented by Chapter 5. This chapter 
will report the results obtained after applying 40 different approach configurations to 39 
experimental corpora, both of which are presented in Chapter 3. The outcome of these 1,560 
experiments will be presented by both discussing the success rates of the authorship attribution 
tasks and interpreting the kind of linguistic information that has been used in the most successful 
experiments. 
 
11 
 
 Finally, Chapter 6 will answer the research questions presented in the current chapter. 
Chapter 6 will also summarize a number of gaps and shortcomings identified in authorship 
analysis research that have been identified in the course of this study. This chapter will discuss 
how this project has filled these gaps and contributed to the general study of authorship analysis 
in a number of ways. At the end of this chapter, a pair of tasks which could not be covered in this 
project will be mentioned. These two tasks, an authorship analysis task and a text classification 
task, are tasks I hope to target in the near future by extending the work done in this dissertation. 
  
 
12 
 
CHAPTER 2: DEFINING AUTHORSHIP ANALYSIS AND AUTHORSHIP 
ATTRIBUTION 
In this research project, authorship analysis will be used as a broad term that covers a number of 
related forensic linguistics tasks aimed at making inferences about the author of an anonymous 
piece of text. Without the intention of being exhaustive, this section will begin by giving an 
overview of different categorizations of authorship analysis work recently presented by a number 
of researchers in the field. Following these categorizations and building upon an authorship 
analysis taxonomy I have formerly used (2011), I will make a division of the different tasks 
usually targeted in authorship analysis. Along with this division, a definition for each of those 
tasks will be given. This division of tasks and their definitions should help contextualize the 
authorship analysis tasks that will be undertaken in the experimental chapter of this project, 
authorship attribution. Also, this chapter will review a number of design decisions that have been 
reported to influence the success rate of this task in the research literature. A comprehensive 
survey of recent papers that target the task of authorship attribution will be presented, placing 
emphasis on the experimental choices made in these studies with respect to these influencing 
aspects of the design. The design decisions presented in this chapter for authorship attribution 
will be included here, as they involve general issues in authorship attribution rather than specific 
linguistic features and analysis techniques. 
2.1  AUTHORSHIP ANALYSIS TASKS 
There are a number of researchers that have offered a categorization of authorship analysis work 
based on the task or problem that prompts the analysis. Some recent and detailed categorizations 
of authorship analysis work in general are found in Abbasi and Chen (2008); Juola (2007; 2008); 
 
13 
 
Koppel, Schler, and Argamon (2009); Solan and Tiersma (2005); and Zheng, Li, Chen, and 
Huang (2006). In their categorization of authorship analysis, which they call stylometric 
analysis, Abbasi and Chen (2008) include a division based on the task targeted by authorship 
analysis. They identify two main tasks: identification and similarity detection. For these authors, 
an identification task consists of finding out the author of an anonymous text among a closed set 
of potential authors. Such identification is achieved by comparing the anonymous text to a 
collection of texts of known authorship that belong to the subjects in the closed set, in which the 
anonymous author is assumed to be included. In contrast, similarity detection for Abbasi and 
Chen consists of the comparison of an anonymous text to other anonymous texts. The objective 
of this other task is to cluster groups of anonymous texts under some potential common identity. 
According to the description of similarity detection in Abbasi and Chen, there cannot be previous 
class definitions (i.e. texts of known authorship) in this task. 
For Juola (2007) the general concept of authorship analysis is expressed by the term 
authorship attribution, which is used with a more restricted meaning in the present research 
project. In his article, Juola defines authorship attribution as “the task of inferring characteristics 
of a document’s author, including but not limited to identity, from the textual characteristics of 
the document itself” (p. 120). Using this broad definition, he subdivides this general task of 
“authorship attribution” (authorship analysis in this paper) according to the questions and topics 
it targets. Following his subdivision, authorship analysis for this researcher may try to answer a) 
if a given subject is the author of an anonymous text, b) who in a closed set of subjects is the 
author of an anonymous text, c) if a set of anonymous texts belong to one single author, d) when 
an anonymous document was written, e) what the gender of the author of an anonymous text is, 
and f) what the author’s mental capacity is. Juola does not give labels for these tasks, but these 
 
14 
 
questions seem to be his first attempt to categorize the tasks or problems that guide authorship 
analysis. 
In Juola’s full-length book study (2008), he uses again the same broad definition for 
authorship attribution, which corresponds to authorship analysis in the present research project. 
In his study, Juola labels three tasks or problems which are the focus of this field. First, a closed-
class problem comprises the identification of the author of an anonymous text within a closed set 
of subjects, in which the author of the text is included. Second, an open-class problem includes 
two possibilities. In one of the two options, an open-class problem involves the identification of 
the author of an anonymous text within a set of subjects, in which the author of the text may or 
may not be present. The other option for an open-class problem is given by the identification of 
the author of an anonymous text without any particular suspect. Finally, Juola (2008)’s third 
task, which according to him researchers prefer to call either profiling or stylometry, covers the 
determination of any possible properties of the author of an anonymous text (e.g., the author’s 
gender, dialect, and language proficiency). 
Koppel et al. (2009) use the label authorship attribution as an umbrella term for a number 
of possible tasks in authorship analysis (i.e., that is the same use of the term as in Juola (2007; 
2008). These authors identify and describe at least four attribution tasks or problems. A text 
categorization problem, in the first place, targets the identification of the author of an anonymous 
text in a closed set of subjects. A second task, profiling the author, consists of providing as much 
demographic or psychological information about the author as possible. In the third situation, a 
needle-in-a-haystack problem, the task is again identifying the author of anonymous text, but 
there are many thousands of candidates and little written evidence for each of them. Finally, in a 
verification task, for Koppel et al., there is no closed set of candidates in which the unknown 
 
15 
 
author of a piece of text is included, but there is a suspect. This task requires a Boolean 
determination of whether the suspect is or is not the author of the anonymous text, namely the 
goal is to state whether the only suspect given by the context is or is not the author of the text 
whose authorship is questioned. 
Although Solan and Tiersma (2005) do not make an explicit categorization of authorship 
analysis tasks, they discuss a number of legal contexts where different authorship analysis tasks 
have been targeted. First, the authors give the name of forensic author identification to the task of 
comparing stylistic similarities and differences between documents of known and unknown 
authorship and then concluding if the same subject authored the compared documents. They also 
present and discuss a few legal cases where this procedure has been used or attempted to be used. 
Later, just briefly labeled as authorship attribution, Solan and Tiersma describe a different task 
where there is a limited number of possible authors (a closed set of them) among which the 
author of an anonymous text or collection of texts has to be identified.  
The last categorization of authorship analysis tasks that will be reviewed in this research 
project appears in Zheng et al. (2006). Similar to the use of the term in this paper, these 
researchers use authorship analysis as an umbrella term for a number of tasks whose goal is to 
draw inferences about the authorship of a piece of text by examining its characteristics. After a 
first broad description of the task of author identification, they define this task as assigning an 
author to a new text by one of a number of subjects for which a collection of texts is given. 
Zheng et al. mention that this task is often called authorship attribution by linguists. A second 
task presented by these authors is what they call authorship characterization. For them, this task 
aims at summarizing the characteristics of the author of a text and generating an author profile 
based on the characteristics of the text. Among the possible characteristics of the author to 
 
16 
 
include in his or her profile, Zheng et al. include the author’s gender, educational and cultural 
background, and language proficiency. The third and last task these authors give as part of their 
taxonomy for authorship analysis is similarity detection. This task consists of the comparison of 
a number of texts in order to determine whether they belong to a single author or not. 
 Given the above reviewed categorizations of authorship analysis tasks, I will build upon a 
categorization I presented in my previous work (2011), in which I identified and defined three 
authorship analysis tasks. Drawing on the broad review of authorship analysis tasks just made, I 
will expand my former categorization and divide in four the tasks commonly targeted in 
authorship analysis: a) authorship attribution, b) author profiling, c) author identification, and d) 
clustering. 
The first task I will describe as part of authorship analysis work is authorship attribution. 
As it is understood in this project, authorship attribution comprises the identification of the 
author of an anonymous text among a closed set of subjects in which the unknown author is 
included. Therefore, the term of authorship attribution as it will be used in this research project 
corresponds to the same term as it is used by Solan and Tiersma (2005). This definition of the 
term also matches its use in a large number of studies in the field of forensic linguistics (e.g., 
Chaski, 2001; Chaski, 2005; Grant and Baker, 2001; and Nazar and Sánchez Pol, 2007), the field 
of stylistics (e.g., Baayen, van Halteren, Neijt, and Tweedie, 2002; Grieve, 2007; Hirst and 
Feiguina, 2007; Holmes, 1994; Jockers, Witten, and Criddle, 2008; Juola, 2006; Juola and 
Baayen, 2005; and Stamatatos, Fakotakis, & Kokkinakis, 2001), and the field of information 
retrieval (e.g., de Vel, Corney, Anderson, and Mohay, 2002; and Koppel, Schler, Argamon, and 
Messeri, 2006). In this research project, I have decided to use authorship attribution to refer to 
the task formerly described because that is the most common label for it in recent forensic 
 
17 
 
linguistics studies. This is pointed out by Grant (2007) and Zheng et al. (2006), although the 
authors of both studies actually opt for different terms to refer to this task.  
Following the review of task categorizations presented above, it should also be noted that 
authorship attribution as defined in this paper corresponds to what Abbasi and Chen (2008) and 
Zheng et al. (2006) call author identification. The use of this label for the concept of authorship 
attribution is probably more frequent in information retrieval studies (e.g, Abbasi and Chen, 
2008; and Chaski, 2007). The meaning of author identification in forensic linguistics studies is 
entirely different (Solan and Tiersma, 2005), and that will be the one assigned in this study to 
this label. Also, the formerly defined task of authorship attribution corresponds to Juola (2007)’s 
second question, about who in a closed set of subjects is the author of an anonymous text, which 
he later labels as a closed-class task (2008). Finally, this definition of authorship attribution is 
also equivalent to Koppel et al. (2009)’s categorization task. Other terms equivalent to the term 
of authorship attribution are sometimes used in the fields of forensic linguistics (e.g., Grant’s 
“authorship analysis” (2007, p. 3) and stylistics (e.g., Burrows’ “authorial attribution” (2002, p. 
267); Hirst and Feiguina’s “authorship discrimination” (2007, p. 405); and McMenamin’s 
“population model” of authorship analysis (2002, p.117). 
One last comment that should be made about the definition of authorship attribution to be 
used in this project is that it also comprises a number of conditions which some researchers 
classify as a separate task. Mentioned in the review of categorizations presented above, Koppel 
et al. (2009) label their third task as a needle-in-a-haystack problem. This task also consists of 
identifying the author of an anonymous text within a closed set of authors in which the unknown 
author is included. However, in a needle-in-a-haystack problem, the closed set is composed of 
many thousands of subjects and there are very limited written data for each subject in the set. 
 
18 
 
From that description, it becomes apparent that the problem is formally the same as in authorship 
attribution, but the conditions for the attribution of an anonymous text in a needle-in-a-haystack 
problem are likely to pose a bigger challenge. Regarding these conditions, Koppel et al. define 
vaguely the needle-in-a-haystack problem as using many thousands of authors in the closed set, 
but they do not give a threshold that divides authorship attribution and the new task they propose 
a different name for. They do not mention a threshold to define what very limited data means 
either. Beyond these missing pieces of information, even if the use of another label may be 
justified to further describe a problem with specific conditions, the task targeted in a needle-in-a-
haystack problem is essentially the same as in authorship attribution. 
Despite the formal equivalence mentioned above, the difference between a needle-in-a-
haystack problem and an authorship attribution task is emphasized in Koppel, Schler, and 
Messeri (2008). In this paper, the researchers make a further distinction between the needle-in-a-
haystack problem, and the task of authorship attribution as here understood, which they call a 
“vanilla authorship attribution” (p. 112). These researchers do not explicitly say why they qualify 
authorship attribution as “vanilla”. However, taking into account their description of this task, 
commented on in the rest of this paragraph, the label “vanilla” may refer to an anecdotal story 
from the seventies about an engineering problem (Mikkelson and Mikkelson, 2011). The story 
describes a problem where selecting the right variables is the most important element to describe 
and eventually solve such problem2. For Koppel et al., the “vanilla” authorship attribution task is 
                                                 
2
 Labeled as a “vanilla problem” in an e-business professionals forum  (WebProWorld, 2004), the problem example 
mentioned above is described in detail in Mikkelson and Mikkelson (2011). In this problem, a car owner insistently 
complaints to a car manufacturer because his new vehicle stalls every time he buys vanilla ice cream, but it does 
not when he buys other ice cream flavors. After confirming the facts described by the car owner, an engineer 
assigned to investigate the case decides to take careful notes on the variables related to the problem. Analyzing 
the situation, he realizes that buying vanilla ice cream takes less time than buying other ice cream flavors. Since 
vanilla is the most popular flavor in the store visited by the car owner, the store has arranged things to serve this 
flavor more quickly to their clients. After noticing this, the engineer uses time and not ice cream flavor as the 
 
19 
 
a “definitely solvable” task that requires mainly a good feature selection (2008, p. 114). Also in a 
“vanilla” authorship attribution, the choice of a classification method to process those features is 
marginally important for these researchers. Although they present a number of features 
commonly used in authorship analysis and support the use of some more than others (e.g., they 
prefer function words over content words), they never say if the problem of making a good 
feature selection has been actually solved. In contrast with this “vanilla” authorship attribution 
task, Koppel et al. decide to target the more challenging context of a needle-in-a-haystack 
problem. Given the complexity of this task, the researchers devise a statistical technique to 
identify text samples whose authors are especially difficult to distinguish from each other, i.e. 
authors whose use of the selected authorship features is very similar. Then, the researchers run 
their statistical technique in a very large closed set with 10,000 authors and use texts that have 
between 500 and 600 words by each author. After identifying and eliminating a portion of the 
most difficult cases (i.e., the authors that are especially hard to discriminate), the researchers’ 
success rate in the correct attribution of the author of testing samples increases. Applying 
progressively their algorithm for the elimination of difficult cases, Koppel et al. achieve a 90% 
of correct attributions for 30% of the subjects in the original database. They also run the same 
experiment on texts of 200 words and obtain an accuracy of 82% for the same 30% of all 
subjects in the original set. A similar result is obtained in Koppel et al. (2009), where the 
scholars describe with more detail their algorithm for the elimination of difficult cases. In this 
paper, the researchers achieve a success rate of 88.2% correct positive attributions (i.e., assigning 
anonymous texts to their true authors) for 31.3% of the subjects in the original set of authors.  
                                                                                                                                                             
variable to study. This leads the engineer to identify the problem to solve: the vapor lock does not dissipate in the 
engine when there is little time for it to cool down. 
 
20 
 
Koppel et al. (2008) argue that their algorithm for the elimination of difficult cases is 
equivalent to an “I don’t know” comment or expert opinion in a law enforcement scenario. So, 
the significance of their results is derived from the value of this comment in such a context. 
However, these researchers do not report any legal cases or consultation where their method has 
been used. Also, the results presented in both studies just discussed (Koppel et al. 2008; 2009) 
represent only a fraction of the entire 10,000 subject set. If we look at the entire set of subjects, 
the performance in the attribution of the correct author in this whole set is 27% for larger texts 
and 24.6% for shorter texts in Koppel et al. (2008) and 27.6% in Koppel et al. (2009). In other 
words, if hard to discriminate cases are not eliminated, a 90% out of 30% of the subjects in the 
whole set, for example, represent only 27% of the all subjects in this set. Regarding the 
interpretation of the results obtained by the kind of methods just presented, a useful observation 
has been made in the field of biometrics. Although authorship analysis has not been traditionally 
seen as a biometric test and is not usually included in field manuals (Petrovska-Delacretaz, 
Chollet, and Dorizzi, 2009), it is possible to contextualize authorship attribution as a biometric 
test -- since this task aims to identify the author of an anonymous text within a set of subjects by 
comparing their writing samples, it matches the definition of a biometric process, which includes 
the recognition of an individual based on measurable behavioral characteristics (Zvetco 
Biometrics, 2010). Considering authorship attribution as a biometric process, an observation 
made in the field of biometrics by Bolle, Connell, Pankanti, Ratha, and Senior (2004) about the 
elimination of difficult cases is relevant. These scholars note that there is a problem with the 
elimination of poor (or especially challenging) data. Although the elimination of poor data may 
be an acceptable quality control practice in the construction of biometric databases, it can make 
the evaluation of a system “look arbitrarily good” (p. 166). Also, there is a significant difference 
 
21 
 
between the elimination of biometric samples (texts of known authorship in our context) by 
means of substituting them with better samples (as when a fingerprint or picture is retaken), and 
the elimination of biometric subjects (or authors) from the database. Beyond the drawbacks of 
the results obtained so far in the needle-in-a-haystack problem, it remains an interesting exercise 
and an authorship attribution task with special predefined conditions. 
 The second task commonly defined in authorship analysis categorizations is what will be 
called author profiling. In this research project, author profiling consists of characterizing the 
author of an anonymous piece of text across a number of categories. This term and its definition 
are more regular in the literature than the former term of authorship attribution. Examples of the 
use of this term as above defined can be found in Gibbons (2003), Koppel, Schler, and Zigdon 
(2005), Olsson (2008), and Schler, Koppel, Argamon, and Pennebaker (2005). As to the 
authorship analysis categorizations formerly reviewed, author profiling captures Juola’s 
questions about when an anonymous document was written, what the gender of the author of an 
anonymous is, and what the author’s mental capacity is (2007). Those topics correspond to Juola 
(2007)’s fourth to sixth questions. In Juola (2008), this researcher explicitly labels as profiling or 
stylometry the task of determining any of the properties of a text’s author and gives as examples 
of those properties the author’s gender, dialect, and language nativeness. As mentioned before, 
Koppel et al. (2009) see author profiling as the task of providing as much demographic or 
psychological information about the author as possible. Therefore, the variation in the definitions 
of author profiling by different researchers has to do mostly with the profiling categories they list 
for this task, the exhaustiveness of the task (Koppel et al. mention the procurement of as much 
profiling characteristics as possible), and the further subdivision of the profiling categories into 
social and psychological. There is one more term, equivalent to author profiling as defined 
 
22 
 
above, that has been used in authorship analysis taxonomies. Zheng et al. (2006) call this task 
authorship characterization. Beyond the categorizations formerly summarized, there are other 
labels used to refer to this task in the literature. For example, de Vel et al. (2002) use the label 
author cohort attribution and Koppel, Argamon, and Shimoni, (2002) talk about a text 
classification by gender, or some other profiling category. 
 The third task to include in the categorization of authorship analysis in this project is 
author identification. Defined here as the task of identifying the author of an anonymous text 
among any possible authors, rather than among a closed set of them, author identification has 
been targeted in a number of ways that differ in how the open set of all possible authors is 
delimited. In the field of forensic linguistics, it has been a common practice to represent the open 
set with all possible authors as composed of only one subject, usually a suspect in some legal 
case. The task that is given by these conditions is also called author identification by Solan and 
Tiersma (2005). In order to perform this task, some forensic linguists compare one or more 
anonymous texts to documents authored by one subject, who has been linked to the anonymous 
texts in the context of a legal investigation. This comparison is aimed at establishing stylistic 
similarities and differences in order to conclude if the subject could have authored both the 
anonymous and the known documents. Solan and Tiersma discuss a number of legal cases where 
this kind of analysis has been made (2005, pp. 151-169). Other authors in the field of forensic 
linguistics who use the term authorship identification to describe this type of analysis and the 
type of cases in which it is used are Coulthard (2004), Finegan (1990), and Gibbons (2003), to 
mention just a few. With the set of all possible authors reduced to only one subject, this task 
corresponds to what Koppel et al. (2009) call a verification problem. In this task, as defined by 
these researchers, there is no closed set of candidates in which the unknown author is included, 
 
23 
 
but there is a suspect, and the authorship of that suspect with respect to the anonymous text has 
to be verified or rejected. This problem is also equivalent to Juola (2007)’s problem about 
answering if a given subject is the author of an anonymous text. Another label used for author 
identification applied to only one subject is McMenamin’s “resemblance model” of authorship 
analysis (2002, p. 117).  
A different delimitation for the open set of all possible authors in author identification 
includes more than one subject. That is what Juola (2008) describes as the first option of his 
open-class problem. That option of the open-class problem involves the identification of the 
author of an anonymous text within a set of subjects, in which the author of the text may or not 
be present. These conditions are given in several of the tasks in the Ad-Hoc Authorship 
Attribution Competition (AAAC) of the Association of Literary and Linguistic Computing and 
the Association for Computers and the Humanities, whose results are summarized in Juola 
(2006). This competition included a total of 13 authorship analysis problems. Four out of those 
13 (problems C, D, E, and F) had short lists of potential authors in which the true author of the 
anonymous text is not necessarily present (Juola, 2008). The first problem, C, contains 5 subjects 
in its lists of potential authors, and the rest of the problems (D, E, and F) have a list of three 
potential authors. It should be mentioned that it is hard to compare the results from all four 
problems results. A number of important factors, such as the amount of data used in the four 
problems or the number of testing samples used to evaluate the performance varies significantly. 
Problem C uses 100,000 character fragments from novels, problem D, whole play acts, problem 
E, all plays by each of the selected authors, and problem F, extracts from letters. While problems 
D and E have only four testing samples (i.e., there offer few possible accuracy results: 100%, 
75%, 50%, 25% and 0%), problems C and F have 9 and 10 testing samples respectively. Not 
 
24 
 
surprisingly, in the latter two problems there are more participants who obtained results above a 
89% accuracy.   
Finally, there is another delimitation of the open set of all possible authors that can be 
described, at least as a potential exercise. In this last configuration for the problem of authorship 
identification, an anonymous text is compared against a very large set of potential authors, in 
which the author may or may not be included, and whose size is large enough to represent at 
least everyone in the community of practice in which the author of the anonymous text is 
inserted. Depending on the situation, the set of potential authors could include the general 
population, i.e. all the speakers (or writers) of the language in which the anonymous text is 
written. The challenge presented in this task is similar to the needle-in-a-haystack problem 
because there are many subjects in the set of potential authors, but unlike this formerly described 
problem in this task there is no certainty that the author is included in the set. Another important 
characteristic of this task is that there is no suspect, unlike the context described for author 
identification in Solan and Tiersma (2005). This new task could be seen as the problem that lies 
behind Juola’s second option of his open-class problem, in which there is no suspect, but we still 
have to answer who the author is. It may seem obvious to discuss why this task is unattainable, 
but a few problems can be highlighted by drawing parallels to the same type of task in biometric 
studies. Bolle et al. (2004) point out three issues, among others, in the creation and maintenance 
of large biometric databases. First, it is impractical if not impossible to enroll everybody in a 
database of biometric samples. Second, the solution of modeling the world under some reduced, 
fictitious model of a subject that represents the general population is not clear yet. Third, there 
are moral and legal issues in obtaining and keeping both voluntary and involuntary biometric 
samples. These three issues can be easily translated to the context of authorship analysis. In first 
 
25 
 
place, even in contexts where writing is widely used in everyday life, it is difficult to have 
representative writing samples for all the individuals in a community, especially if a database is 
supposed to cover different genres. Although the productive use of corpora in linguistics has 
yielded successful applications in the specific field of forensic linguistics (e.g. Gales’ study on 
threatening language, 2010), the limitations on corpora coverage remain present throughout the 
general field of linguistics. To mention a couple of apparent corpora limitations regarding genre 
coverage, not all the members of a society are productive in all genres, such as academic writing, 
and some particular genres are not meant to be shared publicly, such as personal letters. In 
second place, assuming that it is possible to have writing samples from a considerable number of 
individuals in a society, it is not clear how to build a model from that reduced data set so that it 
represents the general population. The question, for example, about how big a database must be 
to represent the general population does not have a straightforward answer. Finally, the 
construction of a database with writing samples from a number of individuals always raises 
concerns about both the copyrights of the authors, if the samples have some alleged public value, 
and a potential privacy invasion, when the samples have a personal value and are not intended to 
be shared publicly. Beyond the three issues just mentioned, a special challenge posed by 
authorship analysis is that important factors that influence the style of a person, such as register, 
may vary in a text. This variation may make an author’s style less stable than many of the 
features used in some biometric systems (mostly in systems that use physiological biometric 
samples, such as fingerprints, iris images, or DNA samples). At the same time, it should be noted 
that not all biometric systems use relatively stable samples. There are also systems categorized as 
behavioral (Bolle et al,. 2004) whose biometric samples, such as gait and lip motion videos, may 
be more prone to being influenced by contextual factors. 
 
26 
 
Despite how obvious the problems just discussed might be, it is worthwhile mentioning 
them, as they should help put in perspective the implications of reducing a set of all possible 
authors to one or a few subjects. On the one hand, when using only one subject to represent the 
set of all possible authors, the task of author identification is presented as a mere problem of 
homogeneity, as Rudman (1998) notes. In this simplification of the task, the main goal is to point 
to similarities and differences between anonymous texts and texts of known authorship. The 
main problem with this approach is that it does not attempt to model the world. The final 
identification, therefore, depends solely on the intuitions of the researcher or, at best, the 
researcher’s interpretations of external sources such as corpora. The lack of a consistent method 
to model the world hinders the approach replicability and its capability of rendering both a 
success and an error rate. On the other hand, when reducing the set of all possible authors to a 
few subjects, the author identification approach should be able to justify the representativeness of 
its database, its coverage of the genres relevant to the task, and the method used to model the 
world with its set of subjects. These methodological justifications are not done in the problems 
described in Juola (2008), for example. 
The last task I will include in this categorization of authorship analysis work is what I call 
clustering. I have decided to use this general term because it can be used to cover two different 
contexts where the main goal is to group sets of anonymous texts. In its first form, the most 
commonly referred to in authorship analysis categorizations, clustering has as its goal to group 
anonymous texts under one or more author identities. This task is what Abbasi and Chen (2008) 
and Zheng et al. (2006) call similarity detection. This task also captures Juola’s inquiry about 
finding if a set of anonymous texts belong to one single author (2007). Beyond the taxonomies 
reviewed at the beginning of this section, McMenamin (2002) names this task as a “consistency 
 
27 
 
model” of authorship analysis (p. 117). A second way in which the task of clustering aims at 
grouping writing samples of questioned authorship is when various subjects are identified as the 
authors of one single text. This task is what Rudman (1998) calls collaboration. In this case, what 
are grouped together are not necessarily whole texts, but excerpts of them. In this type of 
clustering, texts excerpts that are originally identified as authored by a single subject are 
associated with several author identities. This task is particularly relevant in the context of 
plagiarism detection. 
This section has presented a categorization of authorship analysis work, in which four 
major tasks have been identified: authorship attribution, author profiling, author identification, 
and clustering. This section has also attempted to establish a relationship between this taxonomy 
and other recent efforts to classify authorship analysis work. Since the approaches that have been 
frequently used in authorship attribution and author profiling will be applied in the experiments 
of this investigation, the ultimate goal of this section has been to contextualize these two tasks in 
the study of authorship analysis. Regarding the definition of these two tasks, this section should 
be an important reference for both the review of experimental decisions that will be made in the 
coming sections of these chapter and the survey of methods presented in the next chapter. In 
order to accomplish this, the current section has tried to make explicit what the labels for the 
various authorship attribution tasks mentioned above mean in this project and what they do not. 
This is especially important because, as it has been shown, the use of terms in different fields and 
by researchers interested in authorship analysis frequently overlaps or even contradicts. Below, 
Table 2.1 attempts to summarize the various terminological differences that have just been 
discussed regarding the various tasks targeted in authorship analysis. 
  
 
28 
 
Table 2.1 Authorship analysis tasks and their corresponding terms in other studies 
Table  
Authorship Analysis Task Other Terms for Same 
Task 
Studies that Use the 
Other Terms 
a) authorship attribution author identification Abbasi and Chen (2008) 
      Chaski (2007) 
      Zheng et al. (2006) 
    closed-class task Juola (2008) 
    categorization task Koppel et al. (2009) 
    needle-in-a-haystack 
problem 
Koppel et al. (2008; 
2009) 
    vanilla authorship 
attribution Koppel et al. (2008) 
b) author profiling stylometry Juola (2008) 
    profiling Juola (2008) 
    profiling problem Koppel et al. (2009) 
    authorship 
characterization Zheng et al. (2006) 
    author cohort attribution de Vel et al. (2002) 
    text classification (by 
profiling category) Koppel et al. (2002) 
c) author identification verification problem Koppel et al. (2009) 
    resemblance model McMenamin (2002) 
    open-class problem Juola (2008) 
d) clustering similarity detection Abbasi and Chen (2008) 
      Zheng et al. (2006) 
    consistency model McMenamin (2002) 
    collaboration Rudman (1998) 
        
    
 
 
Table 2.1 is useful to easily visualize a number of issues regarding the terminology for 
the different tasks in authorship analysis. For example, there is an abundance of terms, between 
three and six for each of the four tasks described. This table also shows an overlap between two 
key labels: authorship attribution and author identification. In this regard, it should be mentioned 
that this overlap does not originate in this study. The terms chosen in this project have been used 
 
29 
 
by other researchers before, but references to their studies in this table have not been included for 
readability purposes. Table 2.1 also shows that multiple labels may refer to one same task, e.g., 
authorship attribution is covered by two terms, a “categorization problem” and a “needle-in-a-
haystack problem”, in Koppel (2009), and by a different pair of terms, a “vanilla authorship 
attribution” and a “needle-in-a-haystack problem”, in Koppel (2008). Beyond summarizing the 
issues above discussed, Table 2.1 is aimed at helping the reader who is familiar with other task 
labels find the corresponding terms in this study. In face of the terminological issues just 
described and presented in the table above, the whole categorization of authorship analysis tasks 
offered in this section is intended to both give credit to former taxonomies and lay out clearly 
what should be understood by the different names of authorship analysis tasks used in the 
remainder of this study. 
2.2  AUTHORSHIP ATTRIBUTION TASK 
In the former section, authorship attribution has been defined as the task of identifying the author 
of an anonymous text among a closed set of subjects, in which the unknown author is included. 
In this section, a number of current papers that target this task will be surveyed. This part will 
also review what some of the most recent studies have done in terms of selecting their sources of 
data, designing their set of subjects, and assembling their training data (texts of known 
authorship) and testing data (anonymous texts). Later sections will present and discuss 
attribution methods, text features chosen as tokens of authorship, and techniques for feature 
selection. The aspects of authorship attribution that will be commented on in this section have 
been included here because they are not part of the attribution approach, but they are rather 
experiment design decisions that have an influence in the success rate of this task, as discussed 
below. 
 
30 
 
As discussed above, having a set of potential authors is part of the defining characteristics 
of authorship attribution. This task also requires having a minimum amount of data or writing 
samples from each of the subjects in the set (Rico-Sulayes, 2011). Once a corpus with data for 
each subject is collected, an attribution experiment requires having two types of data at one’s 
disposal. First, it is necessary to have training data for all the subjects (i.e., texts whose authors 
are identified during the experiment). This data set allows one to build, for each author, a model 
of the use of a number of selected authorship features. Second, an authorship attribution task also 
requires having some testing data (i.e., texts that remain anonymous during the attribution). By 
creating a model of the use of authorship features for each of the subjects, using for that purpose 
the training data, it is possible to compare an anonymous text from the testing data set against all 
of these models. Through this comparison, the anonymous text can be attributed to one of the 
subjects in the set of potential authors. As far as the data remains the same, the model’s 
efficiency depends on the authorship features selected and the attribution method that processes 
those features. How computer-assisted or automated the construction of the model is does not 
have relevance for the task performance and is not a defining aspect of the task itself. As 
mentioned before, the selection of authorship features and the method that processes those 
features to make an attribution will be the topic of later sections, so we will review in this section 
the aspects of the data and the experiment design that, as modified, have a potential impact in the 
efficiency of the task. 
Regarding the data utilized for an authorship attribution task, there are a number of 
design decisions that are often considered important for the attribution success: the sources or 
kind of data to use, the number of subjects in the set of potential authors, and the amount of data 
in training and testing texts or text samples. Different sources of data, which may result in 
 
31 
 
various topics or genres, can have an impact on the success rate of the attribution. This makes 
Chaski (2005) consider texts’ topic a variable to control for. In this paper, Chaski only includes 
authors that “wrote on similar topics” (p. 4). This idea is modified in a later paper by this author 
(Chaksi, 2007) where she states that attribution “methods should accurately discriminate between 
authors even when the known and comparative texts are different texts types on different topics” 
(p. 137). Although she seems to be using the same set of randomly selected authors and the same 
writing sample database for both arguments, these statements reflect this researcher’s concern 
about the potential influence of the number of topics in the data for the attribution results.  
In authorship analysis literature, the study of the effect of topics in authorship attribution 
has rendered mixed results, as Mikros and Argiri (2007) point out. For example, Corney (2003) 
found out that topic does not affect authorship attribution. Using emails from four subjects on 
three topics (movies, food, and travel), this author argues that the attribution accuracy varies 
according to the type of features used in the classification, but that content/topic-independent 
features, such as function words, render good results across different topics. In contrast with 
these findings, the improvement of the success rate in the attribution of anonymous texts to their 
true authors by means of a reduction of topics was attested by Tambouratzis and Vassiliou 
(2007). In this paper, the researchers put together a collection of texts which were produced by 
five authors and were classified in 28 topics. Using this collection, the authors perform a number 
of attribution tasks selecting only training and testing data from single topics. Then, they 
compare the results of these individual topic attribution tasks to the results obtained in an 
attribution with data from all topics. When using the three topics with the most training data 
(around 230-450 texts), as opposed to the rest of the topics which had less than 80 texts 
individually, these authors report an increase in the rate of positive attributions (i.e., assignments 
 
32 
 
of anonymous texts to their true author) in single topic tasks. In a different experiment, the 
researchers also found that if all 28, fine-grained topic subdivisions in the corpus were used the 
improvement did not take place. Under these conditions, no improvement was accomplished 
even if the large number of topics was reduced by collapsing them in broader, common topic 
labels. Therefore, according to these authors’ results, either a difference of topic between training 
and testing data, or an imposed definition of comprehensive topics for both types of data does 
not improve the success rate of an authorship attribution task. Although these findings seem to 
contradict each other, the authors conclude that their reduction of fine-grained topics did not 
improve the success rate in the task because it did not avoid the data sparseness present in their 
highly subdivided corpus. Compared to the three topics with the most texts (230-450 texts), five 
topics had 80-50 texts, and the rest much less than that (e.g., eight topics had less than ten texts). 
Using a corpus with writings only from two authors on two different topics, Mikros and Argiri 
(2007) are somewhere in between the last two studies. Comparing different arrangements of 
features that are usually considered topic independent, they show that multi-feature sets can 
actually be used to obtain a high accuracy in both authorship attribution and topic classification 
(assigning testing texts to the topic that they belong to). However, they notice that some features 
that may be considered topic-independent (such as different word lengths and vocabulary 
richness measurements) actually discriminate topic rather than authorship. Therefore, the authors 
conclude that it is necessary to be careful about what features are being used when working with 
multi-topic corpora to avoid obtaining an attribution that actually depends on the correlation of 
authors and topics. In contrast with the several studies that explore the impact of the number of 
topics on the success of the attribution, the concept of genre has been devoted much less space in 
authorship analysis literature. This may be due to the fact that topics and genres are sometimes 
 
33 
 
hard to distinguish (Paltridge, 2006). Below, I will comment on the intertwining of these two 
defining aspects of the type of data sources, topics and genres, and will further discuss the 
scarcity of research that has studied the impact of genre on the success of the attribution in 
authorship analysis. 
Another aspect of the data that is relevant for the success rate of an authorship attribution 
task is the number of authors in the set of subjects. A few studies have reported that the results in 
the correct attribution of anonymous texts deteriorate as the number of subjects in the closed set 
increases. Argamon, Šarić, and Stein (2003) report a decrease in the success rate of their 
authorship attribution experiment when more subjects are included in the set of authors. In three 
different experiments, for which they use newsgroup posts from different topics or communities, 
these researchers’ results show a decrease in the positive accuracy as the number of authors 
included in the closed set increases. With data from posts in the communities of “abbreviated 
books”, “theory”, and the computer “language C”, the best results obtained by these researchers 
drop from 67% to 21%, from 73% to 18%, and from 99% to 43%, respectively. This drop takes 
place as the set of potential authors go from 2 to 20 subjects. This degradation in the 
performance of the attribution is shown to be gradual as the number of authors in the closed set 
increases in intervals. For example, the best results obtained with the posts harvested from the 
“abbreviated books” newsgroup deteriorate from 67%, to 43%, to 21%, as the number of authors 
in the set increases from 2, to 5, to 20. Similar patterns are shown in the experiments with the 
other newsgroups topics. Using the “theory” newsgroups, the positive accuracy rate drops from 
73%, to 42%, to 18%. With the “language C” newsgroups the decrease goes from 99%, to 68%, 
to 43%. In these two last cases, this drop also takes place as the number of authors in the closed 
set raises from 2, to 5, to 20. Probably the most thorough study as to the range in the number of 
 
34 
 
authors tested and reported is found in Grieve (2007). This author uses articles from 40 opinion 
columnists in a British newspaper. He reports the positive accuracy rendered by a large number 
of attribution features, which he first evaluates separately and then in a number of different 
combination arrangements. For the best results obtained by this researcher, in an experiment 
where he uses several features at the same time, there is a gradual decrease in the attribution 
accuracy from 97%, to 95%, to 93%, to 91%, to 85%, to 78%, to 69%. This drop takes place as 
the number of authors in the closed set increases from 2, to 3, to 4, to 5, to 10, to 20, and finally 
to 40. This same pattern is present in all 96 experiments reported by Grieve, among which 92 test 
individual authorship features and 4 evaluate different combinations of them. Zheng et al. (2006) 
also report a drop in the accuracy of the positive identification of anonymous texts’ authors as 
the number of subjects in the closed set increases. Using data from English and Chinese, the 
success rate in the positive attribution of authors for both languages gradually decreases as the 
closed set increases from 5, to 10, to 15, to 20 subjects. This tendency remains consistent across 
a number of experiments that utilize different machine learning algorithms as the attribution 
method, and different amounts of data in the authors’ writing samples. In the best results 
obtained by Zheng et al. (2006) the accuracy drops steadily from 97% to 83%, as the number of 
authors in the closed set increases from 5 to 20. It is worthwhile mentioning here that among the 
three papers just discussed, which test the effects of the number of subjects on the success rate of 
an authorship attribution task, two papers (Argamon et al., 2003; Zheng et al. 2006) do not test 
simultaneously the effects of the selection of feature subsets. Noted formerly in this paragraph, 
only Grieve (2007) does test the performance of individual features and feature subsets as he 
uses corpora with different numbers of authors. In all of his experiments, a tendency in the 
success rate to drop as the number of authors in the set increases is observed and no individual 
 
35 
 
feature or combination of them contradicts this tendency in any of Grieve’s 96 experiments with 
different features or feature subsets. 
As to the data in the training set, composed of the texts of known authorship, having a 
large amount of this data seems to be an advantage in the construction of the statistical model of 
authors in the corpus and the improvement of the final positive accuracy achieved (i.e., the 
correct assignment of anonymous texts to their true authors). The amount of data in the training 
set can be expressed in a number of different values: the number of texts by author, the number 
of words by author, and the number of words per text combined with the number of texts by 
author. Unfortunately not all researchers provide all of these values. Therefore, while reviewing 
the studies that address this topic, I will give as much information about the amount of training 
data as is provided by the researchers in their papers. In Burrows (2002), the author confirms the 
idea that the more the training data the better the positive accuracy obtained. That is the general 
tendency in Burrows’ authorship attribution experiments with English poems from the late 
seventeenth century. In his study he reports an improvement in the success rate of the attribution 
as the length of the poems, by 25 authors, grows from a maximum of 500 words to 1,000, to 
1,500, to 2,000, to more than 2,000 words. Following these different ranges in the amount of 
training data, the best results obtained by Burrows increase gradually from 27%, to 45%, to 65%, 
to 85%, to 95%, respectively. Peng, Schuurmans, Keselj, and Wang (2003) show a similar 
pattern using a corpus of newspaper articles in Modern Greek. Their attribution method performs 
better when using more training data, 20 texts by each of 10 authors, compared to 10 texts only. 
This improvement takes place consistently across a range of values for their testing data. For the 
best results obtained by these researchers, the success rate in the positive attribution of 
anonymous texts goes from 90% to 92%, as the number of training texts by author increases 
 
36 
 
from 10 to 20. In the original study that presented the Modern Greek corpus utilized by Peng et 
al. (2003), Stamatatos et al. (2001) also report an improvement in the accuracy rate derived from 
an increase in the number of texts by each of the authors in their closed set. For the approach 
with which these authors obtain their best results, the accuracy rate increases uniformly from 
over 60% to 87%, as the number of texts per author goes from 10 to 20. One more study that 
reports this trend in the effect of the training data size on attribution accuracy is Zheng et al. 
(2006). Formerly mentioned as a reference for the impact of the number of authors on attribution 
accuracy, this paper also reports an improvement in the success rate of positive attributions as the 
amount of training data increases. This tendency takes place consistently in experiments for both 
English and Chinese data sets, as well as in the application of three different machine learning 
algorithms as the attribution method. In all these different conditions, the attribution accuracy 
increases gradually as the amount of texts in the training data for each author increases. For the 
best results obtained by Zheng et al. (2006) the accuracy improves from 83% to 97%, as the 
number of messages per subject in the closed set goes from 10 to 30. 
A different pattern in the impact of the training data size on the success rate of the 
attribution is reported by Grant (2007). In this study, he shows a trend opposite to the one 
reported by the papers referred to above (Burrows, 2002; Peng et al., 2003; Stamatatos et al., 
2001; and Zheng et al., 2006). In his experiment with emails by three different authors, the 
positive accuracy in the attribution of anonymous texts decreases as the number of texts by these 
authors grows from 3 to 5, to 10, to 15, to 20. The drop in the accuracy within those ranges goes 
from 100%, to 93%, to 83%, to 82%, to 81%, respectively. Grant does not compare his results to 
any of the studies commented above or any other papers exploring the effect of the training data 
in the attribution success rate. He does not give any rationale either on why he obtains those 
 
37 
 
results. It should also be mentioned that Grant (2007) uses some “query” texts. Grant labels as 
“query” texts a number of testing writing samples that have been left out of the standard 
evaluation of a cross-validated corpus. In a classification task, for example, a cross validation 
consists of dividing a corpus into training and testing samples. The training samples are used to 
create statistical models of the different classes in the corpus, and the testing samples are 
classified based on the models created with the training data. Every time a classification is made, 
a testing sample cannot be included in the training data. The process is repeated until all the 
corpus is classified without ever using testing data in the training set (Jurafsky and Martin, 
2008). Grant (2007) reports the results of these non-cross-validated “query” texts as they are 
attributed to their true authors. As I have discussed elsewhere (2011), there is a problem with the 
use of the results obtained with these testing samples. Since they are not part of the cross-
validated design, it is impossible to generalize from them. In Grant (2007) these “query” texts 
are also affected by the amount of data in the training set. The inclusion of these “query” texts, 
however, should not influence the success rate of the cross-validated experiments, and the 
unexpected tendency reported by Grant (2007) remains an exception. 
 Finally, there is one more aspect to comment on in this section about the data used in the 
attribution task. Also relevant for the success of the attribution, the testing data set is the 
collection of texts that remain anonymous during the experiment in order to evaluate the 
performance of the class attribution method. Similar to what happens with training data, the 
common view about the influence of testing data on the attribution accuracy rate is that the more 
data available in the testing samples the more resources to compare and correctly attribute these 
samples to their true author. The complementary reasoning that the less the testing data the more 
difficult the attribution task has been tested and confirmed in Chaski (2007). In her paper, this 
 
38 
 
researcher presents two different experimental settings where she uses different amounts of 
testing data. In the first experiment she uses between 4 and 9 documents for each of 10 subjects 
in a closed set of authors. All of the documents for each author contain between 103 and 134 
sentences. Given these conditions, she attempts to attribute each document separately. In her 
second experiment, this author further divides the testing data beyond the document level and 
uses 5-sentence chunks as testing samples. Chaski (2007) does not use positive accuracy results, 
but negative ones (i.e., the correct discrimination of anonymous texts from authors other than 
their true ones) which render larger figures when compared to their positive counterparts (see 
Rico-Sulayes, 2011). In addition to its use of negative accuracy results, Chaski’s paper shows a 
very drastic reduction in the size of documents in the two sets of experiments. Despite these two 
issues, this researcher’s results support the idea that less testing data produces a lower accuracy. 
When she uses 5-sentence chunks for testing samples, instead of whole documents, the accuracy 
decreases to 93% (compared to the 97% achieved with full texts). Hirst and Feiguina (2007) 
experiment on the effects of testing samples size using both positive accuracy figures and a more 
gradual reduction of document size. With data from two nineteenth century novelists, the authors 
subdivide a corpus of 250,000 words for each author in blocks of 1,000, 500, and 200 words. As 
the researchers attempt to attribute progressively smaller testing samples, the best positive 
accuracy results achieved drop from 99%, to 97%, to 92%, respectively. Burrows (2002) also 
tests the effects of the size of testing samples in authorship attribution. In his experiments, this 
researcher uses English poems by 25 writers from the late seventeenth century. He shows results 
for a large number of experiments that combine the size of testing samples with two other 
variables, the number of features in his authorship attribution approach and the rank with which 
the author is selected as the most probable author -- this last variable, the rank of the true author 
 
39 
 
in an ordered list of the most probable matches given by the approach, is not commonly explored 
in authorship attribution, but it is not uncommon in biometric systems (Bolle et al., 2004). In all 
of Burrows’ experiments that test if the true author is the most probable author rendered by his 
approach (the design common to all authorship attribution studies surveyed in this research 
project), there is also a tendency in the accuracy to drop as the size of testing documents 
decreases. For his experiments, this researcher uses five document size ranges: 2,001 or more 
words, between 2,000 and 1,501 words, between 1,500 and 1,001 words, between 1,000 and 501 
words, and 500 words or less. Using testing documents from these five size ranges, the accuracy 
in Burrows’ best results (which are obtained when using his largest set of features) drops from 
95%, to 85%, to 65%, to 45%, to 27%, respectively. Given the three studies discussed in this 
paragraph (Burrows, 2002; Chaski, 2007; and Hirst and Feiguina, 2007), all of them show a 
consistent trend in the accuracy to deteriorate as the amount of information in the testing samples 
decreases. At the same time, it should be noted that there is not an answer to the question of 
whether there is an ideal minimum amount of information to use for the testing samples in 
authorship attribution. Between the two studies that show a more elaborate set of values for the 
size of testing samples, Burrows (2002) and Hirst and Feiguina (2007), there is an evident 
difference in their results. The latter study shows a much greater decline in the accuracy as the 
document size is reduced. This suggests that the answer to the questions just mentioned may 
greatly depend on the authorship attribution approach employed, as well as other relevant aspects 
of the experiment, such as the particularities of the data collected. 
With the above review of research findings about the data source selection, the design of 
the closed set of subjects, and the arrangement of training data and testing data in authorship 
attribution, I have tried to emphasize the importance of these aspects in the success rate of this 
 
40 
 
authorship analysis task. Given the relevance of these aspects, the rest of this section will 
summarize the decisions regarding the data topic and genre selection, the number of subjects in 
the set of potential authors, and the amount of training and testing data in a number of recent 
authorship attribution studies. Since research production in authorship attribution studies has 
proliferated over the last few years, this review aims to be representative rather than exhaustive. 
As formerly mentioned, other important aspects of authorship attribution research (such as 
attribution methods, authorship features, and techniques for feature selection) will be devoted 
separate individual sections in this research project. 
Table 2.2 summarizes the type of data source, the number of genres and the number of 
topics in a number of recent studies that include authorship attribution. It should be mentioned 
that a number of these papers also target other authorship analysis tasks; however, this table only 
presents data on authorship attribution experiments. Later in this subsection, Table 2.3 will 
present the number of subjects, as well as the training and the testing data sets for all of the 
studies surveyed in Table 2.2. Table 2.4 will present the amount of data between the two data 
sets and summarize all the figures provided by these studies regarding their corpora. 
  
 
41 
 
Table 2.2 Data source collected and topic selection in previous authorship attribution studies 
Table 
Authorship Attribution 
Studies   Type of data source   
No. of 
genres   No. of topics 
Abbasi and Chen (2005)   online forums   1   1 
Abbasi and Chen (2008)   
a) emails, b) purchase 
comments, c) code 
snippets, d) chat logs 
  4   NDP 
Argamon et al. (2003)   newsgroups   1   3 
Baayen et al. (2002)   a) fiction, b) essays, c) elicited descriptions   3   3 
Burrows (2002; 2007)   poems   1   NDP 
Caver (2009)   newspaper articles   1   4, 23 
Chaski (2001)   personal and business 
letters   NDP   4 
Chaski (2005)   personal and business 
letters   NDP   10 
Chaski (2007)   personal and business 
letters   NDP   10 
Corney (2003)   a) fiction, b) dissertations, 
c) emails   3   
a) NDP, b) 
NDP, c.1) NDP, 
c.2) 3 
Gamon (2004)   fiction   1   NDP 
Grant (2007)   emails   1   NDP 
Grieve (2007)   newspaper articles   1   8 
Hirst and Feiguina (2007)   fiction   1   NDP 
Jockers et al. (2008)   religious   1   NDP 
Juola and Baayen (2005)   a) fiction, b) essays, c) 
elicited descriptions   3   3 
Koppel et al. (2009)   a) emails, b) fiction, c) blog posts   3   NDP 
Mikros and Argiri (2007)   newswire articles   1   2 
Nazar and Sánchez Pol (2007)   newspaper articles   1   NDP 
Orebaugh and Allnutt (2009)   text messages   1   NDP 
Peng et al. (2003)   a) newspaper articles, b) fiction, c) novels   1   NDP 
 
 
  
 
42 
 
Table 2.2 (continued) 
 
Authorship Attribution 
Studies   Type of data source   
No. of 
genres   No. of topics 
Raghavan, Kovashka, and 
Mooney (2010)   
news articles (4 topics) and 
poems   2   
5 (poems as a 
topic set) 
Rico-Sulayes (2011)   online forums   1   NDP 
Spassova (2008)   narrative   1   NDP 
Spassova (2009)   a) novels, b) opinion 
articles   2   NDP 
Spassova and Turell (2007)   a) novels, b) newpaper 
articles   2   NDP 
Stamatatos et al. (2001)   newspaper articles   1   10 (different across authors) 
Tambouratzis and Vassiliou 
(2007)   
transcribed political 
speeches   1   4, 28 
Tearle, Taylor, and Demuth 
(2008)   
a) fiction, b) political 
essays   2   NDP 
Zheng et al. (2006)   newsgroups   1   NDP 
Zheng, Qin, Huang, and Chen 
(2003)   
a) email, b) newsgroups, c) 
online bulletin board   3   
a) 3, b) NDP, c) 
NDP 
              
Note. NDP = no details provided 
 
 
 
An important comment that should be made regarding the second and third columns of Table 2.2 
(the type of data source and the number of genres, respectively) is that so far the term genre has 
been used loosely in this project to refer to the type of data in the corpus, i.e. whether the data 
analyzed is a collection of newspaper articles, emails, online forum posts, novel excerpts, poems, 
or some other source. This general characterization of the concept of genre matches its broad 
definition as the “type of text” or “type of discourse”, common in introductory texts in the field 
of linguistics, specifically in discourse analysis course books (e.g. Georgakopoulou and Goutsos, 
2004; Paltridge, 2006). According to this broad definition, genres are associated with contexts 
where language is used (with a certain function and purpose) to address certain content or topics. 
 
43 
 
Actually, as Paltridge (2006) points out, certain genres are typically associated with specific 
topics. This intertwining of genres and topics is also frequent in authorship analysis studies, as 
commented on below. 
 Regarding the genres or types of texts targeted in the authorship attribution research 
papers surveyed in Table 2.2, it is possible to identify five genres that have been targeted in 
various research papers. First, perhaps the most common type of text studied in the origins of 
authorship analysis is represented by literary works. Of course, literary texts can be subdivided 
into more specific genres, such as poems (Burrows, 2002; 2007; Raghavan et al., 2010), novels 
(Peng et al., 2003; Spassova, 2008; Spassova, 2009; Spassova and Turell, 2007), opinion 
essays/articles (Baayen et al., 2002; Juola and Baayen, 2005; Spassova, 2009), or fiction in 
general (Baayen et al., 2002; Corney, 2003; Gamon; 2004; Hirst and Feiguina, 2007; Juola and 
Baayen, 2005; Koppel et al., 2009; Peng et al., 2003; Tearle et al., 2008). Second, among the 
increased use of electronic communications in authorship analysis research, there are a number 
of studies that use emails (Abbasi and Chen, 2008; Corney, 2003; Koppel et al., 2009; Grant, 
2007; Zheng et al., 2003). Third, given their increasing availability in electronic format, it is also 
common to find studies that use news articles, either from newspapers (Caver, 2009; Grieve, 
2007; Nazar and Sánchez Pol, 2007; Peng et al., 2003; Raghavan et al., 2010; Spassova and 
Turell, 2007; Stamatatos et al., 2001) or newswire (Mikros and Argiri, 2007). A fourth 
frequently used genre results from the possibility of using programs to automatically retrieve and 
copy information from the Internet, a process known as spidering the Web. Given this 
possibility, a number of studies use posts by online users. Among the various technologies used 
to create and display posts by online users, authorship analysis studies have collected online 
users’ posts from online forums (Abbasi and Chen, 2005; Rico-Sulayes, 2011), newsgroups 
 
44 
 
(Argamon et al., 2003; Zheng et al., 2006; Zheng et al., 2003), bulletin boards (Zheng et al., 
2003),  and blogs (Koppel et al., 2009). Finally, several studies by different researchers have 
used elicited descriptions of events, people or objects (Baayen et al., 2002; Chaski, 2001; 2005; 
2007; Juola and Baayen, 2005). Other less common, but separate genres are chat logs, computer 
code snippets, product reviews (Abbasi and Chen, 2008), doctoral dissertations (Corney, 2003), 
personal and business letters (Chaski, 2001; 2005; 2007), text messages (Orebaugh and Allnutt, 
2009), religious texts (Jockers et al., 2008), transcribed political speeches (Tambouratzis and 
Vassiliou, 2007), and political essays (Tearle et al., 2008). This division in genres has 
organizational purposes, and different divisions among the first five broad genres could be 
argued for. Novels, for example, could be seen as part of fiction in general. Likewise, emails and 
online users’ posts could be included in a broad genre for electronic communications, along with 
chat blogs. As theoreticians of the concept of genre acknowledge (Georgakopoulou and Goutsos, 
2004; Paltridge, 2006), genre systems are not static and some genres can include other genres 
and/or be part of other more comprehensive ones. Despite other possible genre subdivisions, the 
above presentation of the different genres targeted in authorship analysis research shows the 
wide spectrum of text types that have been used in this field. 
 As it was mentioned before, in contrast with the interest about the impact of topic in the 
authorship attribution success rate (e.g., Corney, 2003; Mikros and Argiri, 2007; Tambouratzis 
and Vassiliou, 2007), to the best of my knowledge there is only one study that explores the 
effects of different genres in the success rate of this task, Baayen et al. (2002). In this study, the 
researchers collect texts for 7 authors in three genres: fiction, argument and description. Baayen 
et al. (2002) propose a balance in the genres represented in the testing and training data. This 
balance is achieved by a modification of their test design. Every time a testing document is to be 
 
45 
 
attributed to some author, it is compared only to training data from the same genre. Through this 
balance in genres, the authors achieve an improvement in the positive accuracy of the task from 
79% to 81%, for their best results.  
With only one study devoted to this topic, the scarcity of research that explores the 
influence of genre in authorship attribution may be the result of a number of issues. First, the 
need to get access to spontaneously produced texts (as opposed to elicited texts), from clearly 
distinguishable genres, by a number of authors makes such a study strategically difficult. Second, 
there may be theoretical difficulties when trying to define as separate genres closely related text 
types, such as blogs posts and online forum posts. As mentioned before, genre systems are 
neither definite nor exclusive. Finally, the line that divides the concept of genre from other 
related concepts such as topic is not always clear-cut. Some genres are typically associated with 
a particular topic, as mentioned before (Paltridge, 2006), and this may result in an 
interchangeable use of both terms. For example, in Table 2.2 there is one study where these two 
concepts, genre and topic, are closely intertwined. Raghavan et al. (2010) collect news articles 
(which can be seen as a genre, or type of text) from 4 different topics: football, business, travel, 
and cricket. Then, they also collect poems (another type of text) and perform an authorship 
attribution task using as equivalent, or at least comparable, the five data sets (p. 39). Although 
the authors acknowledge that there is some topic variability inside their “poetry” data set, they 
still treat it as analogous to their other topic data sets and compare the results obtained for all five 
sets in one table (p. 41). Probably due to the above mentioned issues, and other potential ones 
that have not been listed, there is just one single study in authorship attribution that targets the 
influence of genres in the success rate of the task. Despite the scarcity of research on this topic, 
the concern about genres in authorship analysis has been present in this area for some time. As 
 
46 
 
shown in Table 2.2, there are several authorship attribution studies that have collected data from 
different genres to show the differences in the construction and the performance of their 
approach for their various data types (Abbasi and Chen, 2008; Corney, 2003; Koppel et al., 
2009; Peng et al., 2003; Spassova and Turell, 2007; Tearle et al., 2008; Zheng et al., 2003). 
However, all of the studies just listed conduct separate authorship attribution experiments with 
data belonging to just one genre, and do not include inter-genre experiments. 
 One final comment deserves to be made with respect to the variation of topics in 
authorship attribution tasks. Researchers are not always explicit about the number of topics in 
their data, as shown in Table 2.2. In some studies, it can be inferred that the topics are likely to 
be diverse, such as in the studies that use novels or poems (Burrows, 2002; 2007; Peng et al., 
2003; Raghavan et al., 2010; Spassova, 2008; Spassova, 2009; Spassova and Turell, 2007). 
Among studies that utilize online users’ posts, the amount of topics is not explicit either. 
Although there are researchers that collect data from topic organized sources (e.g., Abbasi and 
Chen 2005 use extremist groups’ forums, and Rico-Sulayes 2011, drug-trafficking related 
forums), other researchers harvest posts from sources that are more likely to have several topics. 
Abbasi and Chen (2005) collect information from purpose-oriented classified sources (purchase 
reviews/feedback), and Zheng et al. (2003) harvest general interest forums (popular Chinese 
bulletin boards). Also, it should be mentioned that even in the topic-organized forums there is no 
guarantee that the posts retrieved belong exclusively (or even mostly) to a single topic. 
 In contrast, as can be seen in Table 2.2, there are also authorship attribution studies that 
make explicit their number of topics (Argamon et al., 2003; Baayen et al., 2002; Caver, 2009; 
Chaski, 2001; 2005; 2007; Corney, 2003; Grieve, 2007; Juola and Baayen, 2005; Mikros and 
Argiri, 2007; Raghavan et al., 2010; Stamatatos et al., 2001; Tambouratzis and Vassiliou, 2007; 
 
47 
 
Zheng et al., 2003). Among these studies there are a number of them that report the fact that their 
data contains a variety of topics but do not perform any topic related experiment (e.g., Chaski, 
2001; 2005; 2007; Grieve, 2007; Stamatatos et al., 2001). Some of these researches report this 
topic diversity because they see it as a condition that should be handled by the classification 
method if it were to be applied in real-life scenarios (Chaski, 2007) or they consider this 
diversity a way to avoid a correlation between topic and authorship in their data set (Corney, 
2003; Grieve, 2007; Raghavan et al., 2010; Zheng et al., 2003). Other researchers, as has been 
mentioned before, actually exploit the multiple topics in their data and explore the relationship 
between the number of topics and other variables, such as the training data and the testing data 
(Baayen et al., 2002; Caver, 2009; Mikros and Argiri, 2007) or the set of features (Caver, 2009). 
 Another crucial experiment design decision, the number of subjects in the closed set of 
potential authors is always mentioned in authorship attribution tasks, as shown in Table 2.3. 
Besides the number of subjects in the set of potential authors, this table also shows the amount of 
testing and training data per author (in terms of texts or text samples), whenever this information 
has been provided by researchers. Table 2.3 surveys all of the studies previously reviewed in 
Table 2.2. It should be mentioned again that a few of these papers target other authorship 
analysis tasks, but this table only shows data for their authorship attribution experiments. In 
Table 2.3, there are a few studies that perform several experiments using various numbers of 
subjects in the closed set of potential authors. In these cases, a list with the number of subjects 
separated by commas has been presented for different experiments where the data type or topic 
remains the same. In several studies, however, there are experiments with different numbers of 
potential authors that employ data from different genres or topics. For this reason, the various 
genres and topics, formerly presented in Table 2.2, have been repeated in the second column of 
 
48 
 
Table 2.3. The different genres and topics have been listed again using lower case letters. These 
lower case letters have been used in the third, fourth and fifth columns to link the information 
provided in these columns, when they belong to the same experiment or set of experiments. As it 
can be seen in Table 2.3, the number of subjects in the closed set of potential authors goes from 2 
to 100 in the papers surveyed. Between these two extremes, only 4 papers out of the 33 in the 
survey include an experiment that has more than 20 authors (Abbasi and Chen, 2008; Burrows, 
2002; 2007; Grieve, 2007) -- Table 2.3 has only 32 rows because Burrows uses the same corpus 
in two different studies (2002; 2007). Ten studies include at least one experiment with 10 to a 
maximum of 20 authors in the closed set of subjects (Argamon et al., 2003; Caver, 2009; Chaski, 
2005; 2007; Koppel et al., 2009; Peng et al., 2003; Rico-Sulayes, 2011; Spassova, 2009; 
Stamatatos et al., 2001; Zheng et al., 2006). Finally, the rest of the studies, 19 out of the 33 
summarized in the table, have only experiments with fewer than 10 subjects. The number of 
authors in the closed set is very important in the interpretation of the positive accuracy obtained 
in an experiment. As it has been commented on before, the papers that have studied this variable 
have consistently found that the rate in the correct attribution of anonymous texts to their true 
authors deteriorates as the number of subjects in the closed set increases (Argamon et al., 2003; 
Grieve, 2007; Zheng et al., 2006).  
  
 
49 
 
Table 2.3 Number of subjects, training data, and testing data in previous authorship attribution 
studies 
Table  
Authorship Attribution 
Studies Genres/topics 
No. of 
authors 
Testing sz per 
author 
Training sz 
per author 
Abbasi and Chen (2005) online forums 
5 per 
language 
1 sample 29 samples 
Abbasi and Chen (2008) 
a) emails, b) purchase 
comments, c) code 
snippets, d) chat logs 
25, 50, 
100 
5 samples 5 samples 
Argamon et al. (2003) newsgroups 2, 5, 20 NDP NDP 
Baayen et al. (2002) 
a) fiction, b) essays, c) 
elicited descriptions 
8 1 text 7 texts 
Burrows (2002; 2007) poems 25 1 text all other texts 
Caver (2009) newspaper articles 2, 15 25% 75% 
Chaski (2001) personal/business letters 4 1 text all other texts 
Chaski (2005) personal/business letters 10 1 text 3-9 texts 
Chaski (2007) personal/business letters 10 a) 1 text, b) NDP 
a) 3-9 texts, b) 
NDP 
Corney (2003) 
a) fiction, b) 
dissertations, c) emails 
a) 5, b) 3, 
c.1) 4, 
c.2) 3  
a) and b) 100, 200, 
500, 1000 word 
chunks, c) NDP 
a) 10, 20, 30, 
40, 50, 60, 80, 
100 word 
chunks, b) 
NDP, c) NDP 
Gamon (2004) fiction 3 20% 80% 
Grant (2007) emails 3 
1 text, 1 non-cross-
validated 
19, 14, 9, 4, 2 
texts 
Grieve (2007) newspaper articles 
40, 20, 
10, 5, 4, 
3, 2 
1 text all other texts 
Hirst and Feiguina (2007) fiction 2 random 10% random 90% 
Jockers et al. (2008) religious 7, 5 random % random % 
Juola and Baayen (2005) 
a) fiction, b) essays, c) 
elicited descriptions 
8 1 text all other texts 
Koppel et al. (2009) 
a) emails, b) fiction, c) 
blog posts 
a) 2, b) 9, 
c) 20 
a) NDP, b) one 
book, c) 30 posts 
a) NDP, b) one 
book, c) all 
other posts 
Mikros and Argiri (2007) newswire articles 2 1 text 99 texts 
Nazar and Sánchez Pol 
(2007) 
newspaper articles 5 1 sample 
25, 30, 35, 40, 
45, 50, 55, 60, 
65, 70, 75% 
  
 
50 
 
Table 2.3 (continued) 
 
Authorship Attribution 
Studies Genres/topics 
No. of 
authors 
Testing sz per 
author 
Training sz 
per author 
Orebaugh and Allnutt (2009) text messages 4 NDP NDP 
Peng et al. (2003) 
a) newspaper articles, b) 
fiction, c) novels 
a) 10, b) 
8, c) 8 
a) 10 texts, b) NDP, 
c) NDP 
a) 10, 20 texts, 
b) NDP, c) all 
texts 
Raghavan et al. (2010) 
news articles (4 topics) 
and poems 
3, 6, 4, 4, 
6 per 
topic 
NDP NDP 
Rico-Sulayes (2011) online forums 10 1 sample 3 samples 
Spassova (2008) narrative 3 NDP NDP 
Spassova (2009) 
a) novels, b) opinion 
articles 
a) 17, b) 
4 
5 non-cross-
validated samples 
for 2 authors 
all other texts 
Spassova and Turell (2007) 
a) novels, b) newpaper 
articles 
3 NDP NDP 
Stamatatos et al. (2001) newspaper articles 10 10 texts 
10, 12, 14, 16, 
18, and 20 
texts 
Tambouratzis and Vassiliou 
(2007) 
transcribed political 
speeches 
5 NDP NDP 
Tearle et al. (2008) 
a) fiction, b) political 
essays 
a) 2, b) 3 
a) NDP; b) 12 
questioned texts 
a) NDP, b) 85 
texts 
Zheng et al. (2006) 
newsgroups (2 
languages) 
5, 10, 15, 
20 
NDP 
10, 15, 20, 25, 
30 and all tetxs 
Zheng et al. (2003) 
a) email, b) 
newsgroups, c) online 
bulletin board 
a) 3, b) 9, 
c) 3 
1/30 of texts all other texts 
          
Note. NDP = no details provided       
 
 
 
The amounts of testing data, the texts that remain anonymous during an attribution, and 
training data, the texts of known authorship, are shown in the fourth and fifth column of Table 
2.3, respectively. Despite the importance of these two design decisions, they are not always 
explained by researchers in their studies, unlike the number of subjects in the set of potential 
authors. It should also be noted that the amounts of testing and training data can be expressed not 
only in the number of texts included in each set, but in the number of words per text or the 
number of words in each set. However, this further piece of information, the number of words 
 
51 
 
per text or the number of words in each data set, is even less frequently reported -- this will be 
discussed below in the presentation of Table 2.4. As it has been commented on before, the 
amount of information used for the testing and training data sets has an impact on the success 
rate of the attribution. Among the studies that explore the effects of the training set size, which is 
indicated in the fifth column of Table 2.3, most of them have found out that the more the training 
information, the better the accuracy achieved in the task (Burrows, 2002; Peng et al., 2003; 
Stamatatos et al., 2001; and Zheng et al., 2006). Actually, only one study shows a different 
pattern (Grant, 2007), where there is a negative effect on the success rate as the training data 
increases. However, Grant does not compare his results to other studies targeting this 
experimental variable, and he does not offer any rationale for the trend in his experiments. The 
amount of testing data (fourth column of Table 2.3) has also been evaluated. With consistent 
results in different studies, researchers have shown that better accuracy results are obtained when 
more of this type of information is available in the experiment (Chaski, 2007; Hirst and Feiguina, 
2007). Regardless of these findings, there are several papers summarized in Table 2.3 that 
provide no details about the amount of information used for any of these two types of data sets 
(Argamon et al., 2003; Orebaugh and Allnutt, 2009; Raghavan et al., 2010; Spassova, 2008; 
Spassova and Turell, 2007; Tambouratzis and Vassiliou, 2007). Also, there are a few studies that 
have at least one experiment where no information is provided about the training or testing data 
size (Chaski, 2007; Corney, 2003; Koppel et al., 2009; Peng et al., 2003; Tearle et al., 2008). 
Although this lack of information does not necessarily invalidate these studies, it makes it 
difficult to compare the results obtained by one approach against the results reported in other 
studies. 
 
52 
 
 Beyond the effects of design decisions on experiments results, there is a difference in the 
use of training and testing data that has important implications in the resulting size of these two 
data sets. While some studies use whole texts and distribute them between the two data sets 
during the attribution, others use samples or chunks of these texts. The first option has the 
advantage of keeping the integrity of documents throughout the task, i.e., texts are not split 
between different texts samples to be used in experiments. This kind of data distribution, using 
entire documents, seems convenient because there is not any modification of the original text 
during the identification or tagging of authorship features. However, as I have pointed out 
elsewhere (Rico-Sulayes, 2011), it is not necessary to affect authorship features in the analysis of 
selected samples or chunks. As long as the text units considered as authorship features are not 
split between samples, there should not be problems in counting feature instances and adding 
them to the vector (or the list with the total of instances) for each sample or chunk. For example, 
sentence level features, such as the number of words per sentence, would require not splitting 
sentences between samples -- unless clear and consistent guidance about how to distribute the 
counts of split units are provided in the description of the method. In the studies summarized in 
Table 2.3, there are 16 studies that use whole documents (referred to as “texts” in the table) and 
11 papers where text samples were used. In Table 2.3, studies that use text samples include the 
number of “samples”, “chunks” (for sampling designs that use smaller units), or the percentage 
of samples utilized. The 27 papers that do present information about the amount of training and 
testing data used do it in at least one of the experiments conducted. As discussed above, a 
number of studies (a total of 6) do not offer any information about the training and testing data 
sets. 
 
53 
 
 In addition to not implying a disruption, at least not necessarily, in the process of 
identifying and counting instances of authorship features, using samples offers a few advantages. 
Unless all documents are exactly the same size, which is not the case most of the time (see Table 
2.4 and its presentation below), sampling texts permits the use of a more consistent ratio between 
the two data sets for each individual author, i.e. every time a testing text sample is considered as 
anonymous and then compared to the training set by its true author, the proportion of information 
between both sets remains similar. This is important because using documents with very 
different sizes may produce experiment conditions under which one of the data sets has very 
little information, which can be detrimental for the success rate of the task, as formerly 
discussed. Another advantage is that using text samples permits the researcher to have a similar 
corpus size across different authors. Given the influence of the size of the two data sets in the 
success rate of the attribution, this balance at the intra-author and inter-author level should be a 
desirable characteristic in experiment designs. The great variation in the size of documents (as 
opposed to samples) by each individual author and in the total corpus for each subject in the 
closed set of authors can be seen in Table 2.4. This table includes as much information as it has 
been provided by researchers about the corpora used in their authorship attribution experiments. 
Therefore, the details are not necessarily equivalent among studies across different rows. Once 
more, for studies that conduct experiments on data from different genres or topics, the various 
genres and topics have been shown in the second column of Table 2.4, and they have been listed 
using lower case letters. In a number of studies, researchers offer information about their corpora 
that is specific to the data collected on each genre or topic (Corney, 2003; Koppel et al., 2009; 
Peng et al., 2003; Spassova, 2009; Spassova and Turell, 2007; Tearle et al., 2008; Zheng et al., 
2003). For these studies, separate sets of details (about corpora on different genres) have been 
 
54 
 
summarized in the fourth column. Information details about different corpora have been referred 
to by their genre or topic using the same lower case letters as their corresponding genre in the 
second column. The content of the second column, with the different genres or topics targeted in 
the studies surveyed, is the same in Table 2.2 through Table 2.4. In the case of studies that 
assemble various corpora for different experiments, but use the data from the same genre or 
topic, the corpora are listed with numbers (see e.g., Caver, 2009; Chaski, 2007; and Zheng et al., 
2006). Given the amount of information shown in Table 2.4, horizontal lines between different 
studies have been added for readability. 
 
  
 
55 
 
Table 2.4 Distribution of information between data sets and some corpus figures in previous 
authorship attribution studies 
Table  
Authorship Attribution 
Studies Genres/topics 
Data sets 
division Corpus 
Abbasi and Chen (2005) online forums samples 
20 messages per author, 77 words 
avg per message, divided in 30 
samples 
Abbasi and Chen (2008) 
a) emails, b) 
purchase comments, 
c) code snippets, d) 
chat logs 
samples 
1422-27774 words per author, 
divided in 10 samples 
Argamon et al. (2003) newsgroups NDP 
10-300 posts equivalent to 3-4142 
words per post 
Baayen et al. (2002) 
a) fiction, b) essays, 
c) elicited 
descriptions 
documents 
72 texts, 908 words avg. 9 texts per 
author, 3 texts per genre 
Burrows (2002; 2007) poems documents 200 poems total, 540244 words 
Caver (2009) newspaper articles samples 
1) 3000 texts 84000 words, 2) 18862 
texts and 216000 words 
Chaski (2001) 
personal/business 
letters 
documents 
2-3 texts per author, 93-556 words 
per text, 345-998 words per author 
Chaski (2005) 
personal/business 
letters 
documents 
69 texts total, 90-608 words per text, 
1487-2706 words per author 
Chaski (2007) 
personal/business 
letters 
documents 
1) 69 texts total, 90-608 words per 
author, 1487-2706 words per author, 
2) 100 sentences per author in 5 
sentence chunks 
Corney (2003) 
a) fiction, b) 
dissertations, c) 
emails 
samples 
a) 1-2 books per author, 70161-
243992 words per author, b) 24880-
33984 words, c.1) 36-86 messages 
per author, c.2) 30-63 messages per 
author 
Gamon (2004) fiction samples 
1-2 books per author, 6410-13220 
sentences, in 1440 samples of 20 
sentences 
Grant (2007) emails documents 63 messages, 21 per author 
Grieve (2007) newspaper articles documents 
1600 texts, 1.5 million words, 
24750-53891 words per author 
Hirst and Feiguina (2007) fiction samples 
250,000 words per author in blocks 
of 1000, 500, and 200 words 
 
  
 
56 
 
Table 2.4 (continued) 
 
Authorship Attribution 
Studies Genres/topics 
Data sets 
division Corpus 
Jockers et al. (2008) religious samples 
217 known samples, 114-17797 
words; 239 questioned samples, 95-
3752 words 
Juola and Baayen (2005) 
a) fiction, b) essays, 
c) elicited 
descriptions 
documents 
72 texts, 908 words avg. 9 texts per 
author, 3 texts per genre 
Koppel et al. (2009) 
a) emails, b) fiction, 
c) blog posts 
samples 
a) 242-246 messages, b) 2 books in 
500 words chunks, c) 217-745 posts 
250 words avg 
Mikros and Argiri (2007) newswire articles documents 
100 texts per author, 59495-62668 
words per author 
Nazar and Sánchez Pol 
(2007) 
newspaper articles samples 
20 articles per author, 400 words per 
article 
Orebaugh and Allnutt (2009) 
text messages NDP 
140 messages, 35 per author, 2500 
words per message 
Peng et al. (2003) 
a) newspaper 
articles, b) fiction, 
c) novels 
documents 
a) 2 corpora, 10 authors per corpus, 
30 texts per author, 45766-77549 and 
36665-104065 words per author; b) 
49314-1614258 words per author; c) 
1-2 novels per author for training 
(536986-1860555 characters) and 20 
novels for testing 
Raghavan et al. (2010) 
news articles (4 
topics) and poems 
NDP 
5 corpora, 7261-23765 words per 
author (balanced), 14-28 document 
per author 
Rico-Sulayes (2011) online forums samples 
2001-2099 words per author divided 
in 4 samples, 486-595 words per 
sample 
Spassova (2008) narrative NDP 
5 texts per author, 3000 words per 
text 
Spassova (2009) 
a) novels, b) 
opinion articles 
samples 
# samples per author (600 words per 
sample): a.1) 25; a.2) 2-7; a.3) 25; 
a.4) 3-9; b.1) 5-9; b.2) 4-7. 6 more 
experiments with similar numbers 
Spassova and Turell (2007) 
a) novels, b) 
newspaper articles 
NDP 
a) and b) 5 texts per author, a) 3000 
and b) 1500 words per text 
Stamatatos et al. (2001) newspaper articles documents 
30 texts and 17166-50670 words per 
author 
 
  
 
57 
 
Table 2.4 (continued) 
 
Authorship Attribution 
Studies Genres/topics 
Data sets 
division Corpus 
Tambouratzis and Vassiliou 
(2007) 
transcribed political 
speeches 
NDP 
undetermined # of texts, 210-7770 
words per text 
Tearle et al. (2008) 
a) fiction, b) 
political essays 
documents 
a) undetermined # of samples, 1000 
words per sample; b) 97 texts 
Zheng et al. (2006) 
newsgroups (2 
languages) 
documents 
1) 48 texts per author avg, 169 words 
per text avg; 2) 37 text per author 
avg, 807 words per text avg 
Zheng et al. (2003) 
a) email, b) 
newsgroups, c) 
online bulletin 
board 
documents 
a) 20-28 texts per author, b) 8-30 
texts per author, c) 20-28 texts per 
author 
        
Note. NDP = no details provided     
 
 
Just mentioned above, two sources of variation in the arrangement of the corpora can be seen in 
Table 2.4: the difference in size for the various texts by each individual author and the diverse 
size of the whole collection of texts by each subject in the set of potential authors. First, as seen 
in Table 2.4, in a number of studies there is a great variation in the size of the different 
documents collected for the subjects in the set of potential authors. Chaski (2001) uses 
documents that can have between 93 and 556 words, with the largest document having 6 times as 
many words as the shortest one. Although the shortest and largest documents are not shared by 
the same author, a piece of information not included in Table 2.4, the author with the shortest 
document (93 words) has also a document with 239 words, with the latter document having over 
2.5 times as many words as the former one. Similarly, with a variation between 90 and 608 
words in the entire document collection for Chaski (2005) and the first experiment in Chaski 
(2007), in these two studies the single author with the largest difference in document size goes 
from 90 to 323 words per document, over 3.5 times as many words in the largest document as in 
 
58 
 
the shortest one. Taking into account what has been mentioned above about the influence of the 
testing set in the success rate of the attribution, a great variation in the size of the documents 
collected for each individual subject does not seem to be best option. With very different sizes in 
the documents collected for a particular subject, the testing text from a given subject in each 
experiment is sometimes large (which is convenient for its comparison to authors’ statistical 
models as it has more data) and is sometimes very short (which is less convenient). Besides the 
three studies that do mention the size of the texts collected for each of their subjects (Chaski, 
2001; 2003; 2007), there is one more paper in Table 2.4 that provides the average size of texts 
for the subjects in the set of potential authors (Stamatatos et al., 2001). However, that piece of 
information does not reveal the intra-author variation in document size, which has just been 
discussed. All other studies that utilize whole documents do not include information about the 
size of documents for individual authors. Therefore, it is not possible to comment on any 
potential intra-author variation regarding the amount information collected for individual 
subjects in the experiments conducted in these studies (see Baayen et al., 2002; Burrows 2002; 
2007; Grant, 2007; Grieve, 2007; Juola and Baayen, 2005; Mikros and Argiri, 2007; Peng et al., 
2003; Tearle et al., 2008; Zheng et al., 2006; Zheng et al., 2003).  
 Another source of variation in the corpora, the size of the whole collection of texts by 
each individual author is presented in a few of the studies surveyed in Table 2.4. With this piece 
of information, it is possible to see the inter-author variation across the set of subjects in these 
different research papers. Given the formerly mentioned influence of the training set in the 
success rate of the attribution, a great variation in the size of the data collected for each subject 
does not seem optimal. Subjects with large individual collections should offer a better chance to 
be distinguished than subjects with smaller sets of collected texts. A total of seven studies in 
 
59 
 
Table 2.4 show the size of the collection of texts by each subject in the closed set of potential 
authors. These studies reach differences in the size of the data collected for individual subjects 
that go from 345 to 998 words per author (Chaski, 2001), from 1,487 to 2,706 (Chaski, 2005; 
2007), from 24,750 to 53,891 (Grieve, 2007), from 59,495 to 62,668 (Mikros and Argiri, 2007), 
from 49,314 to 1,614,258 (Peng et al., 2003), and from 17,166 to 50,670 (Stamatatos et al., 
2001). With these differences, these 7 studies have, respectively, the following ratios of words in 
their shortest collection for an individual subject to words in their largest collection: 1 to 2.9, 1 to 
1.8 (two studies), 1 to 2.2, 1 to 1.1, 1 to 32.7, and 1 to 3. From these figures, two possible 
outcomes become apparent. First, using documents does not necessarily imply a wide inter-
author variation in the amount of information collected for each subject in the closed set of 
potential authors. At least one study has a pretty even distribution of information for the two 
authors in its closed set of subjects: 59,495 and 62,668 words for each of the two subjects 
(Mikros and Argiri, 2007). However, this resulting balance in the data collected for different 
subjects seems more difficult to achieve with larger sets of potential authors. Opposite to the 
balance in Mikros and Argiri (2007), all of the other studies have a considerable difference in the 
amount of information collected for their individual subjects. All but one study have around 2 to 
3 times as much information in the largest collection by an individual author as in the shortest 
one. The only study that is not within that range has a much larger difference of 32.7 times more 
information for its more documented author when compared to the author with the least amount 
of data, 49,314 words to 1,614,258, respectively. This shows that most of the studies that use 
whole texts instead of text samples have great variation in the amount of data collected for 
individual subjects in their set of potential authors. One more study in Table 2.4 presents the 
average size of the text collection per author Zheng et al. (2006). This piece of information, 
 
60 
 
unfortunately, does not permit knowledge of the inter-author variation regarding the size of the 
data collection by individual subjects. All other studies summarized in the table do not offer 
information about the size of the collection of texts by each author (Baayen et al., 2002; Burrows 
2002; 2007; Grant, 2007; Juola and Baayen, 2005; Tearle et al., 2008; Zheng et al., 2003).  
Given the influence of the amount of training and testing data on the success rate of the 
class attribution, the lack of information about both the size of the various texts and the whole 
text collection by each individual subject represents a shortage in the resulting interpretation of 
the studies that fail to provide these important characteristics of their corpora. Studies that use 
text samples instead of whole texts can avoid the imbalance of intra-author and inter-author 
distribution of information. Even when the original data collection for each individual subject 
results in broadly different collection sizes, researchers can further select the samples so there is 
a more even distribution of information in the experiments. This is done in a number of papers 
surveyed in Table 2.4 (see e.g., Abbasi and Chen, 2005; Caver, 2009; Gamon, 2004; Hirst and 
Feiguina, 2007; Koppel et al., 2009; Nazar and Sánchez Pol, 2007; Rico-Sulayes, 2011; 
Spassova, 2009).  
Another way of avoiding the effects of the imbalance of training and testing data sets and 
samples is the use of normalization. This process attempts to avoid the anomaly that results from 
having documents with diverse sizes in a corpus (Manning et al., 2008). When using documents 
with various sizes, long texts are likely to have features with a greater number of instances than 
short texts do. These larger numbers may be the mere consequence of the fact that long 
documents have more data. Normalization can reduce the high frequencies rendered by features 
in longer texts. This is done by scaling the absolute frequency for some feature in relation to the 
highest frequency obtained for this feature in all the corpus. This type of normalization, known 
 
61 
 
as maximum normalization, is just one of the several possible forms this process can have 
(Aksoy and Haralick, 2001; Rome, Miasnikov, and Haralick, 2003). Unfortunately, this process 
is either not mentioned or not used by all researchers. Among the 32 studies in Table 2.4, many 
of which do not use balanced corpora, only 7 report their use of some form of normalization 
(Abbasi and Chen, 2005; Argamon et al., 2003; Chaski, 2007; Corney, 2003; Nazar and Sánchez 
Pol, 2007; Raghavan et al., 2010; Stamatatos et al., 2001). Besides the fact that not all studies 
with imbalanced corpora use or acknowledge their use of normalization, this process also has a 
number of shortcomings. In information retrieval, at least three problems in the use of 
normalization have been identified (Manning et al., 2008). In the first place, when feature values 
are not represented by simple absolute frequencies (or total number of instances), these values 
may be derived from other features values, such as when the relative frequencies or rankings of 
features are used. In these cases, a minimum change in the list of features (the insertion or 
deletion of only one feature) may require a recalculation of all normalized feature values. For 
Manning et al., this can render the normalization process “unstable” or “hard to tune” (2008, p. 
117). Second, a document may contain a term that appears with an atypical high frequency. This 
non-representative frequency will affect, however, the values of this feature across the whole 
corpus after normalization. Finally, documents with frequent features that have similar absolute 
frequencies should have a different representation from documents where frequent features have 
a more skewed distribution. This kind of difference is not captured by vectors with normalized 
feature values, at least not when maximum normalization is used (such as in Abbasi and Chen, 
2005). Feature values can also be normalized by some variable, which is considered the 
influencing factor to be neutralized or controlled for. In our context this variable is the text 
length. This normalization is done in some of the authorship attribution studies formerly listed 
 
62 
 
(e.g., Chaski, 2007; Corney, 2003; Raghavan et al., 2010; Stamatatos et al., 2001). However, the 
problem with this normalization procedure is that there are several features that can be chosen to 
represent the variable to control for. For example, text length can be expressed in terms of the 
number of sentences in the document, the number of words, or the number of characters, to 
mention a few possibilities. Therefore, this normalization process depends on the feature selected 
for normalization, which is not always the same among different researchers; the researchers just 
listed use both the number of words (Chaski, 2007; Corney, 2003; Raghavan et al., 2010; 
Stamatatos et al., 2001) and the number of characters in the document (Corney, 2003). Also, the 
question of whether the feature selected to represent the variable to be controlled for is the best 
possible one usually remains unanswered. None of the authorship attribution studies that use this 
type of normalization address this question. 
Following the definition of authorship attribution offered at the beginning of this chapter, 
this section has further explained the experimental conditions necessary to perform this task. 
From these experimental conditions, a number of design decisions that have an influence on the 
success rate of authorship attribution have been reviewed. In this review, research evidence that 
offers a number of trends about the impact of these decisions has been referred to. Also, a 
comprehensive survey of recent papers that target authorship attribution has been presented. This 
survey has emphasized the discussed design decisions made in this collection of research papers, 
whenever this information has been provided by researchers. The final goal of this section has 
been to give enough background on the importance of experiment design decisions for the task 
that will occupy the experimental chapter of this project: authorship attribution. This background 
should help support, or at least inform about, the reasons behind a number of experiment 
characteristics that are not always devoted enough space in authorship attribution research. The 
 
63 
 
design decisions presented in this last section have been included in this chapter, as discussed 
previously, since they involve general issues that influence authorship attribution rather than 
specific linguistic features and analysis techniques. Therefore, these design decisions are rather 
part of the experiment conditions and they should be controlled for, or at least explicitly 
mentioned, so the results obtained in authorship analysis tasks can be contextualized and 
eventually compared. 
  
 
64 
 
CHAPTER 3: AUTHORSHIP ANALYSIS APPROACHES 
Attributing a class to an anonymous text requires a number of experimental conditions. As has 
been discussed before, it is necessary to have at one’s disposal both anonymous texts and texts of 
known authorship. In authorship analysis research, anonymous texts are either represented by 
texts whose information or classes are known but that remain anonymous during the class 
attribution or by actual questioned or disputed texts whose classes are unknown. The first type of 
anonymous text, the collection of which constitutes the testing data set, is used to evaluate the 
class attribution performance in an experimental setting. Using this kind of text allows 
researchers to obtain a success rate for the class attribution approach, as well as a corresponding 
error rate. Obtaining these figures can contribute to the acceptance of this type of work in legal 
courts in the United States which consider, under the Daubert standard, a known error rate as a 
criterion of admissibility for an expert opinion (Howald, 2008; Solan and Tiersma, 2004; 2005). 
All of the research papers surveyed in the former chapter, and summarized in Table 2.2 through 
Table 2.4, use this kind of anonymous text, whose class is known but remains anonymous during 
the class attribution in an experimental setting. The second type of text, a real-life questioned or 
disputed text, is commonly used to show the application of the class attribution approach on data 
relevant to legal cases (e.g., Coulthard, 2004; Grant, 2010; and McMenamin, 2002). Although I 
think it is possible to obtain error rates for the use of this kind of text (through a historical 
analysis of real-life cases, for example), to the best of my knowledge there is not a single study 
presenting any procedure to render error rates for real-life disputed texts. 
In authorship analysis work, texts of known authorship, which remain as such during the 
experiment, are also necessary for the attribution of some class to an anonymous text. These 
texts of known authorship constitute the training data set. Considered a general principle of 
 
65 
 
authorship analysis, actual questioned or disputed texts should not be used as part of the training 
data set (Smith, 1990; Holmes, 1994). Also, whether the class to be attributed is an identity 
within a set of potential authors (authorship attribution) or a category in a set of author’s 
characteristics (author profiling), it is necessary to have texts from all possible classes in the 
training data set. Having training data for all classes, the anonymous text can be compared to the 
texts in the different classes and the class attribution can eventually be made. 
Once a testing and a training data set have been assembled, there are two essential 
components of the attribution approach itself that make possible the comparison of texts from 
these two data sets. The first component is the selection of relevant authorship features. A 
number of linguistic variables are chosen, so the class attribution renders the best possible 
results. Secondly, authorship features have to be processed by some classification method that 
attributes a class to the anonymous texts. This class attribution is based on the identification and 
comparison of features in the anonymous text and the texts in the training data set. As long as the 
corpus and its arrangement of the testing and training data sets remain the same, the efficiency of 
the whole attribution approach is generally agreed to depend on these two core components, the 
selection of authorship features and the class attribution method (Zheng et al., 2006). The rest of 
this chapter will aim at defining and characterizing these two components of the attribution 
approach. As the former chapter, the following sections devoted to these two topics will survey 
the literature in authorship attribution and present what researchers have reported as the most 
efficient decisions regarding these two components of the attribution approach. Unlike Chapter 
2, this chapter will also include a survey, of the class attribution methods and the feature 
selection processes, in another authorship analysis task: author profiling. The survey of this task 
regarding these two topics will not only make apparent the methodological similarities between 
 
66 
 
this task and authorship attribution, but it will help support the experimental decisions made later 
in this project. As it will be discussed later in these chapter, these decisions will be inspired by 
the most successful methodological decisions that have been reported in both authorship 
attribution and author profiling literature.  
3.1  FEATURE SELECTION AND FEATURE REDUCTION TECHNIQUES 
In authorship analysis work there has been a constant proposal of new authorship features to 
improve the success rate of the class attribution results. Surveying over 300 authorship analysis 
publications in more than 30 years, Rudman (1998) found that approximately 1,000 authorship 
features had been proposed in the literature by the time he wrote his paper. Despite this 
abundance of features, Rudman also notes that, since no feature (or set of features) has been 
shown to render the best results across different contexts, there is a need to continue testing 
previously proposed and novel features in new experimental settings. After all, different contexts 
in authorship analysis (such as its various tasks) may involve different subsets of authorship 
features. However, even at the level of individual experiments, over the last few years a number 
of researchers in both linguistics and computer science research have suggested, following 
experimental findings, that the selection of features is more relevant to the improvement of the 
attribution approach results than the subtle tuning of the classification method (Juola, 2008; 
Koppel et al., 2008; Rico-Sulayes, 2011).  
With the constant proposal of authorship features, their resulting proliferation has 
prompted a few researchers to group them into a number of categories. In general, most studies 
that propose groupings of authorship features include three categories: lexical, syntactic, and 
structural features (Abbasi and Chen, 2005; Abbasi and Chen, 2008; Juola, 2007; 2008; Koppel 
et al., 2009; Koppel et al., 2008; McMenamin, 2002; Rico-Sulayes, 2011; Zheng et al., 2006).  
 
67 
 
Although there are discrepancies on which features are included under each of these three labels, 
some general tendencies are shared by researchers in the use of these three category names. 
Drawing on a feature taxonomy I previously proposed (2011), in this research paper these three 
categories of features will be defined as follows. First, lexical features are represented by words 
and their parts, including their characters. Lexical features, therefore, include any kind of 
characters (which may include punctuation), character sequences (character n-grams), and 
morphemes. Also, this category comprises words tagged with their semantic information, their 
frequency, or their special use. Second, syntactic features consist of punctuation, words, or 
groups of words that are tagged with their grammatical information. It should be noted here that 
in contrast with any punctuation characters, split from the words they usually attach to and 
counted as any other lexical item, syntactically classified punctuation is further characterized by 
the function it performs in the sentence, e.g. end-of-phrase or end-of-sentence punctuation 
(Chaski, 2005; 2007; Rico-Sulayes, 2011). The syntactic category can also include the syntactic 
tags by themselves such as parts of speech (POS) labels, POS n-grams (i.e., sequences of POS in 
the text such as the POS trigram “determiner-adjective-noun”), and other information of the 
syntactically tagged features, such as their frequency. Finally, structural features include any 
kind of paragraph or document level information, as well as media specific features. This 
category comprises all paragraph and document measurements or scores, such as words or 
sentences per paragraph, and words, sentences, or paragraphs per document. Media features 
consist of elements that characterize the medium of communication, for example, the use of 
various text formatting options, hyperlinks, or images in electronic messages or posts. 
As defined above, the three authorship feature categories proposed (lexical, syntactic, and 
structural) include a few other category labels that have been proposed by researchers. For 
 
68 
 
example, so-called content specific features (Abbasi and Chen, 2005; 2008; Zheng et al., 2006; 
Zheng et al., 2003) or content words (Koppel et al., 2009; Koppel et al., 2008) are represented by 
words tagged, according to their semantic information, because they have a lexical meaning. 
Examples of content words are “deal”, “sale”, “check”, and “offer” (taken from Zheng et al., 
2006, p. 385). Content words are distinguished from function words, which carry preponderantly 
grammatical information, such as “a”, “between”, “in”, and “nor” (also taken from Zheng et al., 
2006, p. 393) -- function words fall under the syntactic category, discussed below. It should be 
mentioned that this content specific category usually includes words that have been associated 
with certain topics in different text classification tasks, either automatically or tagged by humans. 
For example, the formerly cited content words from Zheng et al. (2006) are associated, in the 
authorship attribution task included in this paper, with online posts aimed at selling pirated 
software. In this project, content specific features are considered part of the lexical category of 
authorship features. Another feature category in the literature that can be included in the lexical 
category is composed of lexicographic features (Juola, 2007), orthographic-morphological 
properties (Juola, 2008), morphology (Koppel et al., 2008), or word formation elements 
(McMenamin, 2002). As defined above, the lexical category is composed of words and their 
parts, which include morphemes and word characters. Also, since this definition of the lexical 
category considers any characters and their sequences as belonging to this category, the category 
of character n-grams (i.e., sequences of characters) in Koppel et al. (2009) is also part of the 
lexical feature category as defined in this project. It should be noted that word character n-grams, 
or character sequences, may be equivalent to some morphemes but most of the time are different 
from them. For example, even in the most fine-grained morpheme division of the word 
 
69 
 
“characters”, its character bi-grams, or two character sequences, (“ch”, “ha”, “ar”, “ra”, “ac”, 
“ct”, “te”, “er”, and “rs”) are not morphemes. 
With respect to the syntactic feature category and its definition given above, this label 
includes a number of feature categories that have been proposed as separate by a number of 
researchers. As formerly mentioned, function words, which are also known as stop words in 
information retrieval (Manning, Raghavan, and Schütze, 2008), are seen as a separate category 
for some (Koppel et al., 2009; Koppel et al., 2008). Function words consist of words that have 
primarily a grammatical function and therefore, they are essentially words selected based on their 
POS. For this reason, they are part of the syntactic feature category as defined here. This is also 
the case of so-called functional lexical taxonomies (Koppel et al., 2009) or systemic functional 
linguistics trees (Koppel et al., 2008). These proposed categories are represented by words 
tagged with their POS plus other semantic or subcategorization information. Koppel et al. (2009) 
give as examples of their functional lexical taxonomies the use of pronouns (a POS) and the 
different types of pronouns (as the added information). Therefore, made-up here for the sake of 
exemplification, possible features in the functional lexical taxonomies of these authors could be 
compound labels like “pronoun-personal”, “pronoun-demonstrative”, and “pronoun-possessive”. 
Koppel et al. (2008) give concrete examples of the words that belong to their systemic functional 
linguistic trees and the tags associated with them. For instance, these researchers tag “if”, 
“because”, “since”, and “therefore” as conjunctions of the type “ConjCausalConditional”. Other 
conjunction tags used by these researchers include, for example, “ConjExtension”, 
“ConjElaboration”, and “ConjSpatiotemporal” (p. 113). These tags by themselves, along with 
other information such as their frequency, can also be used as authorship features. 
 
70 
 
Regarding the structural feature category, there is a category of features proposed by 
researchers that can be part of this more general category as it has been defined in this project. 
This is the case of so-called complexity measures (Koppel et al., 2009; Koppel et al., 2008) or 
vocabulary richness measurements (Grieve, 2007; Holmes, 1994; Hoover, 2003; Juola, 2008). 
These measures comprise a number of scores which use distributional or statistical information 
at the sentence and the document level. In the origins of authorship analysis, these kinds of 
measures were very popular and were largely explored in univariate approaches, in which single 
measures were tested as the unique feature bearing an authorial fingerprint (Juola, 2008; Koppel 
et al., 2009).  However by the end of the last century, a couple of critical studies (Holmes, 1994; 
Rudman, 1998), considered foundational by some (Juola, 2008), pointed out the need for testing 
multivariate approaches with different sets of features, as opposed to using univariate 
approaches. Over the last few years, the search for single-feature, authorial fingerprints has lost 
popularity and multivariate approaches, which employ more than one authorship feature, have 
dominated authorship analysis. This can be observed in all of the studies in authorship attribution 
and author profiling surveyed in the former chapter, which employ multivariate approaches. 
Despite this important change in the feature selection of authorship analysis approaches, the 
inclusion of complexity measures in authorship analysis experiments has not been invalidated. 
Actually, many researchers believe that it is necessary to continue exploring the potential 
contribution of complexity measures in authorship analysis and include them in their 
experiments. Some recent examples are Chaski (2001), Grant (2007), Grieve (2007), Juola 
(2008), Koppel et al. (2009), and Koppel et al. (2008). A few of the complexity measures used in 
the studies just listed are: the average word length and the word length distribution (the 
proportion of words with different lengths) both in syllables and in characters, and the average 
 
71 
 
number of words per sentence. Other more elaborate complexity or vocabulary richness 
measures are the type-token ratio (tokens are individual words instances and types, distinctive 
vocabulary word forms), Yule’s K-characteristic (1944), Simpson’s D-index (1949), Sichel’s S-
measure (1975), and Honore’s R-measure (1979). Briefly, it can be mentioned that Yule’s K-
characteristic attempts to represent the likelihood that two words chosen randomly in a text are 
the same (Everything Development, 2001). Simpson’s D-index aims at representing the 
probability of two tokens, randomly chosen from a text, belonging to the same type. Sichel’s S-
measure was developed to capture the distribution of words that appear only twice in a text 
(hapax dislegomena) and was aimed at characterizing the vocabulary richness of individual 
writers. As to Honore’s R-measure (Honore’s H in Grieve, 2007), this value attempts to measure 
how often a word in a text is a word previously used or a new word. It should be noted that the 
complexity measures just presented are mere examples, and there a number of other measures 
that have not been listed here. For an in-depth presentation and discussion of the complexity 
measures above mentioned, and others frequently used in authorship analysis, see Holmes (1994) 
and Grieve (2007). 
Finally, there a few categories of authorship features whose items may be included in 
more than one of the categories proposed in this research project. This is the case of the category 
labeled as idiosyncratic features (Abbasi and Chen, 2008), idiosyncrasies (Koppel et al., 2008), 
anomalies (Juola, 2008), or spelling (McMenamin, 2002). This category of features is 
represented by lists of non-normative uses of language. These lists are usually collected from 
prescriptive grammars, corpus studies, or spell checker reject lists. Since these lists may include 
lexical, syntactic, and media-specific items, such items could fall under any of the three 
categories formerly presented. This is also the case for the category of interference features 
 
72 
 
(McMenamin, 2002), which comprise language uses associated with non-native speakers or 
multilingual speakers. These features are usually collected from second language and 
bilingualism studies and corpora. Finally, punctuation is another category that has been 
considered as separate (McMenamin, 2002) and whose items may belong to any of the three 
categories formerly proposed. For example, punctuation instances have been tagged in a number 
of studies based on their syntactic information, such as end-of-clause or end-of-phrase 
punctuation (Chaski, 2001; 2005; 2007; Rico-Sulayes, 2011). In this case, punctuation can be 
considered part of the syntactic feature category. However, punctuation instances can also be 
considered either as single characters, splitting them from the words they are usually attached to 
(e.g. Orebaugh and Allnutt, 2009), or as portraying media specific uses, such as in the frequent 
reduplication of punctuation characters in electronic communications (Rico-Sulayes, 2011). 
Therefore, they can also be included in the lexical and the structural feature categories, 
respectively. 
Given the continuous proposal of authorship features, the three-category taxonomy 
presented should help group a number of commonly used features and refer to them with more 
general labels. This taxonomy is not intended to be the only or even the best possible 
classification of authorship features. As it has been mentioned, there are a number of other 
categories that can comprise features from any of the three categories in the taxonomy just 
presented. The main goal of the taxonomy proposed here is rather to give some organization to 
the survey of features that will be made later in this section. This survey will review the selection 
of features made in recent studies that have targeted authorship attribution and author profiling 
tasks. After presenting the decisions made by researchers regarding the selection of features in 
these two tasks, this section will finish with a discussion of feature reduction techniques. These 
 
73 
 
feature reduction techniques are aimed at simplifying the list of features by statistical means and 
improving the success rate of the class attribution. 
3.1.1  Feature Selection in Authorship Attribution 
In its second, third, and fourth columns, Table 3.1 presents the total number of features, the 
feature categories, and a number of selected features or feature subcategories used in the 
authorship attribution studies surveyed in the last chapter. It should be mentioned that the feature 
categories in the third column correspond to the categories in the taxonomy formerly presented, 
and not to the categories that some of the surveyed studies include. Since many of the 
categorizations in these studies are not equivalent to each other, and the number of categories can 
grow in a potentially indefinite way, the taxonomy formerly presented should help give 
consistency to the survey of features presented in this and the next section. As to the features 
presented in the fourth column, the inclusion of subcategories that comprise several individual 
features has been given preference. Examples of these subcategories are complexity measures 
(which may include a wide number of values such as the type-token ratio or Yule’s 
characteristic, among other measures), or any type of n-grams (which either for characters, 
words, or POS may include all two-item sequences (bigrams), three-item sequences (trigrams), 
four-item sequences (4-grams), and so forth). Further discussed below, the number of features 
employed in a study can easily reach the thousands, so it is not feasible to list them all in this 
table. This is true even in the cases where just a few dozens of features are used. 
 As seen in Table 3.1, there is a great variation in the number of features that are used in 
different studies. First, a number of studies employ a small number of features. The rationale 
behind short lists is usually that efficiently discriminative features should achieve a high success 
rate in the attribution task, without producing the noise that large lists of non-discriminative 
 
74 
 
features can bring when applied to new data (Manning et al., 2008).  In Table 3.1, eleven out of 
32 studies surveyed use features lists with less than 100 features (Baayen et al., 2002; Chaski, 
2001; 2005; Grant, 2007; Grieve, 2007; Mikros and Argiri, 2007; Orebaugh and Allnutt, 2009; 
Rico-Sulayes, 2011; Spassova, 2008; Stamatatos et al., 2001; Tearle et al., 2008). The noise issue 
that characterizes long lists of features has two well known possible solutions. Later mentioned 
in the discussion following Table 3.3, feature reduction techniques allow researchers to further 
select a subset of the features and perform the class attribution using this subset. These feature 
reduction techniques attempt to eliminate the noisy features and improve the success rate of the 
task. Another possible solution is the use of machine learning classifiers. Introduced with more 
detail in the next chapter, there are machine learning classifiers that are especially tolerant to 
noise in long lists of features (Abbasi and Chen, 2005; Zheng et al., 2006). In order to understand 
the noise challenge posited by long lists of features, we can describe a classification task that 
contains three classes A, B, and C and six features, 1, 2, 3, 4, 5, and 6. After tagging these six 
features in a training corpus for the three classes, the features defining each class could be found 
to be A {1, 2}, B {3, 4}, and C {5, 6}. If a new testing sample (a text, in our context) is 
characterized by the vector (or list) of features [1, 5, 6] is possible to decide that the text belongs 
to class C, with which it shares 100% of the defining features, instead of belonging to class A, 
with which it shares only 50% of the defining features. However, more features could be 
introduced as defining features of class C, resulting in the set {5, 6, 7, 8}, with the defining 
features for class A remaining the same. In this case, the same vector of features for the testing 
text [1, 5, 6] contains 50% of the defining features for both classes A and C. For the 
classification of the testing text just described, the new features have added only noise and have 
made the other defining features in class C lose their discriminatory power. As mentioned before, 
 
75 
 
a number of machine learning algorithms are especially good at handling feature noise (Manning 
et al., 2008). It should be noted that the use of feature reduction techniques and machine learning 
classifiers is not exclusively applicable to long lists of features. There are a number of studies 
that use short lists of features combined with feature reduction techniques (e.g., Chaski, 2005; 
Grant, 2007; and Rico-Sulayes, 2011). Also, there are studies in Table 3.1 that use less than 100 
features and employ machine learning classifiers (Orebaugh and Allnutt, 2009; Tearle et al., 
2008). 
 
  
 
76 
 
Table 3.1 Number of features, feature categories, and selected features sampled in previous 
authorship attribution studies 
Table 
Authorship Attribution 
Studies 
# of 
features 
Feature 
categories Feature samples 
Abbasi and Chen (2005) 301 
a) lexical, b) 
syntactic, c) 
structural 
a) content words, letter frequency, special 
characters; b) punctuation, function 
words; c) font color, font size, word 
length distribution 
Abbasi and Chen (2008) 27632+ 
a) lexical, b) 
syntactic, c) 
structural 
a) character n-grams, misspellings; b) 
POS tags, function words; c) vocabulary 
richness, word length distribution 
Argamon et al. (2003) 504 
a) lexical, b) 
syntactic, c) 
structural 
a) special use words (e.g., netabbrevs), 
caps; b) function words; c) word and 
sentence length 
Baayen et al. (2002) 58 a) lexical, b) syntactic 
a) 8 punctuation marks; b) 50 most 
frequent function words  
Burrows (2002) 40-150 lexical most frequent types 
Burrows (2007) NDP lexical frequency selected types 
Caver (2009) NDP lexical all types 
Chaski (2001) 10 
a) lexical, b) 
syntactic, c) 
structural 
a) spelling errors, word form errors; b) 
syntactically classified punctuation, verbal 
phrases; c) vocabulary richness,  
Chaski (2005) 6 a) syntactic, 
b) structural 
a) syntactically classified punctuation; b) 
average word length per document 
Chaski (2007) NDP a) syntactic, 
b) structural 
a) syntactically classified punctuation, 
syntactic structure; b) average word length 
per document 
Corney (2003) 857+ 
a) lexical, b) 
syntactic, c) 
structural 
a) character n-grams or sequences; b) 
function word-token ratios; c) average 
sentence length, type-token ratio  
Gamon (2004) 26117 a) syntactic, 
b) structural 
b) function word frequencies, POS 
trigrams or sequences of 3; c) average 
sentence and phrase length 
Grant (2007) 25 structural mean word length, mean sentence length, 
proportion of words with different lengths 
 
  
 
77 
 
Table 3.1 (continued) 
 
Authorship Attribution 
Studies 
# of 
features 
Feature 
categories Feature samples 
Grieve (2007) 92 
a) lexical, b) 
syntactic, c) 
structural 
a) word n-grams, character n-grams; b) 
punctuation frequency; c) word length 
distribution, vocabulary richness measures 
Hirst and Feiguina (2007) NDP a) syntactic, 
b) structural 
a) POS bigrams or sequences of 2; b) 
complexity measures applied to POS, 
complexity measures 
Jockers et al. (2008) 521 lexical unigrams/types shared by training and testing samples 
Juola and Baayen (2005) NDP a) lexical, b) 
syntactic 
a) 1024-character sequences; b) function 
words 
Koppel et al. (2009) 3922 a) lexical, b) syntactic 
a) content words, frequent words ; b) 
function words, POS n-grams or 
sequences 
Mikros and Argiri (2007) 68 a) syntactic, 
b) structural 
a) frequent function words; b) complexity 
measures 
Nazar and Sánchez Pol 
(2007) NDP 
a) lexical, b) 
syntactic 
a) words bigrams or sequences; b) word 
and punctuation bigrams or sequences 
Orebaugh and Allnutt (2009) 68+ a) lexical, b) 
structural 
a) emoticons, netabbrevs; b) average 
words per sentence 
Peng et al. (2003) NDP lexical character n-grams or sequences 
Raghavan et al. (2010) NDP a) lexical, b) syntactic 
a) non-function words, word n-grams or 
sequences; b) PCFG-obtained POS  
Rico-Sulayes (2011) 25 a) syntactic, 
b) structural 
a) syntactically classified punctuation; b) 
hyperlinks, font formatting 
Spassova (2008) 56 syntactic verbal phrases 
Spassova (2009) 146 syntactic POS bigrams or sequences of 2, POS 
trigrams or sequences of 3 
Spassova and Turell (2007) NDP syntactic POS bigrams or sequences of 2, POS 
trigrams or sequences of 3 
Stamatatos et al. (2001) 22 
a) lexical, b) 
syntactic, c) 
structural 
a) most frequent words; b) phrase types, 
words per phrase type; c) complexity 
measures 
  
 
78 
 
Table 3.1 (continued) 
 
Authorship Attribution 
Studies 
# of 
features 
Feature 
categories Feature samples 
Tambouratzis and Vassiliou 
(2007) NDP 
a) lexical, b) 
syntactic, c) 
structural 
a) frequency of lemmas (dictionary entry 
headwords), frequency of negative words; 
b) POS, punctuation frequency; c) word 
length, sentence length 
Tearle et al. (2008) 32 structural punctuation distribution, word distribution 
Zheng et al. (2006) 270 
a) lexical, b) 
syntactic, c) 
structural 
a) content words frequency, characters 
frequency; b) punctuation frequency, 
function word frequency; c) complexity 
measures 
Zheng et al. (2003) 207 a) lexical, b) structural 
a) content words; b) function words; c) 
complexity measures, media specific, e.g. 
email signature types 
        
Note. NDP = no details provided 
 
 
Given the possibility of dealing with the noise of long lists of features, eight out of 32 studies in 
Table 3.1 use hundreds of features, but less than a thousand (Abbasi and Chen, 2005; Argamon 
et al., 2003; Burrows, 2002; Corney, 2003; Jockers et al., 2008; Spassova, 2009; Zheng et al., 
2006; Zheng et al., 2003). Also, there are 3 studies that actually use thousands of features: 
Abbasi and Chen (2008), Gamon (2004), and Koppel et al. (2009). It should be noted that 10 out 
of 32 studies in Table 3.1 do not mention the number of features they use (Burrows, 2007; Caver, 
2009; Chaski, 2007; Hirst and Feiguina, 2007; Juola and Baayen, 2005; Nazar and Sánchez Pol, 
2007; Peng et al., 2003; Raghavan et al., 2010; Spassova and Turell, 2007; Tambouratzis and 
Vassiliou, 2007). Many of these studies provide only general descriptions of the features they 
employ, such as all types (word forms, as opposed to tokens or word instances), or all POS 
bigrams or trigrams (POS sequences such as “determiner-noun” for bigrams or “determiner-
adjective-noun” for trigrams). Some of these general descriptions can actually be a useful 
 
79 
 
summarizing comment. Providing a list of all types or n-grams may not be very helpful to the 
reader; especially if there has not been any feature reduction. However, at least the total number 
of features included in these general descriptions should be provided, as Spassova (2009) does, 
for example, for her bigrams and trigrams. Besides, in the case of studies that use very specific or 
novel features, the lack of a detailed list of them represents a serious shortage for the replicability 
of these studies (see for example Chaski, 2007). 
 As to the feature categories used in the studies surveyed, the categories mentioned in the 
third column of Table 3.1 correspond to the classification of features presented in this project. As 
mentioned before, these categories are not equivalent to the ones a number of these studies 
present. Despite this lack of correspondence, it can be mentioned briefly that in authorship 
attribution there have been studies that work with any possible subsets of the three categories in 
the taxonomy proposed in this project. Namely, there are studies that use only one of the three 
single categories (lexical, syntactic, or structural), one of their three possible pairings (lexical and 
syntactic, lexical and structural, or syntactic and structural), or the three of them together. 
The arrangement of features in different subsets is especially important when these 
subsets are used separately to perform the class attribution and their results are compared. This is 
a common task in authorship analysis research. Table 3.2 lists the studies that test the effects of 
using different subsets of features. Half of the studies in Table 3.1, sixteen out of 32, are 
included in this new table. In the second column of Table 3.2, the categories of features used in 
the studies are included again; however, it should be noted that the three categories proposed 
here and their different combinations do not match the feature subset testing in these studies. The 
majority of these studies test different feature subsets, as well as various combinations of them. 
In addition, as it has been pointed out several times, the distribution of individual features among 
 
80 
 
categories differs among researchers. Despite these discrepancies, Table 3.2 is useful because of 
the general conclusions that can be drawn from its third column, where the best results obtained 
in these studies are summarized. Examining this column it can be seen that 11 out of the 14 
studies that test subsets of features report that their best results are obtained when all the features 
are fed to the classification method (Abbasi and Chen, 2005; 2008; Corney, 2003; Gamon, 2004; 
Grieve, 2007; Hirst and Feiguina, 2007; Orebaugh and Allnutt, 2009; Raghavan et al., 2010; 
Rico-Sulayes, 2011; Zheng et al., 2003; 2006).  
  
  
 
81 
 
Table 3.2 Authorship attribution studies testing feature subsets and their best results 
Table 
Authorship Attribution Studies Feature 
categories Best results for feature subsets 
Abbasi and Chen (2005) all three all features combined 
Abbasi and Chen (2008) all three all features combined 
Corney (2003) all three all features combined 
Gamon (2004) a) syntactic, b) 
structural all features combined 
Grieve (2007) all three all features combined 
Hirst and Feiguina (2007) a) syntactic, b) 
structural 
all features and all syntactic 
features (with longer texts) 
Juola and Baayen (2005) a) lexical, b) 
syntactic 
function words (no test for all 
combined) 
Koppel et al. (2009) a) lexical, b) syntactic 
combined sets (no test for all 
combined) 
Orebaugh and Allnutt (2009) a) lexical, b) 
structural all features combined 
Raghavan et al. (2010) a) lexical, b) syntactic all features combined 
Rico-Sulayes (2011) a) syntactic, b) 
structural all features combined 
Stamatatos et al. (2001) all three 
syntactic and novel complexity 
measures (no test for all 
combined) 
Zheng et al. (2003) a) lexical, b) structural all features combined 
Zheng et al. (2006) all three all features combined 
      
 
 
Among the remaining 3 studies that test subsets of features, they do not include the combination 
of all features in their experiments (Juola and Baayen, 2005; Koppel et al., 2009; Stamatatos et 
al., 2001). Thus, it is not possible to know if the results in these other studies may have been 
improved by using all their features together. Such an improvement would be expected if the 
trend in the other studies is consistent. 
 
82 
 
 In conclusion, the experimentation with different subsets of features has not produced 
better results in authorship attribution. However, these experiments have been indirectly useful 
because they have shown that the best option is to feed all features to the classification method. 
The appropriateness of considering all features together is especially true when a system to 
further select the most discriminative features is used. This leads directly to the next subsection, 
where the use of feature reduction techniques in authorship attribution research will be discussed. 
3.1.2  Feature Reduction Techniques in Authorship Attribution 
With the large number of features that have been proposed in authorship analysis work, every 
single authorship attribution study can but make a selection of them. This first selection is 
entirely dependent on researchers’ intuitions, and is different from any statistical (or stochastic), 
automated feature selection method. Therefore, in this project a distinction will be made between 
these two types of processes. Although these two processes are usually aimed at collecting the 
most useful features for a classification, the researcher-driven inclusion of features will be called 
feature selection here. The term feature reduction will be reserved for the usually automated, 
statistics-based scoring of features and the subsequent grouping of them. This distinction is 
important because these two different processes tend to be referred to, rather loosely, with 
various labels in the literature, and at times these various labels overlap in a number of ways. For 
example, while Holmes (1994) talks about “selecting features” to describe the researcher-driven 
type of process, other authors understand the “selection of features” as the type of statistical 
method that has been defined as feature reduction above (e.g., Forman, 2003; Manning et al., 
2008; Witten, Frank, and Hall, 2011). Also, the researcher-driven inclusion of features, feature 
selection in this project, has been referred to by several authors as “identifying” features (e.g., 
McMenamin, 2002; Olson, 2008). However, the equivalent expression of “identifying markers” 
 
83 
 
is used by Grant and Baker (2001) to refer to both processes. This is especially interesting 
because these authors’ paper is actually a critique of the feature selection performed by Chaski 
(2001), which represents what I have called here a researcher-driven selection. Against the 
feature selection used by Chaski, Grant and Baker offer the alternative use of what has been 
defined here as a feature reduction technique. I should also mention that I have not coined the 
label “reduction” to refer to the kind of technique I have described above. Although they used it 
interchangeably with the term “selection”, a number of authors refer to the statistics-based 
selection of features as a “reduction” of variables (see e.g., Manning et al., 2008; and 
Tambouratzis and Vassiliou, 2007). 
In feature selection as understood in this project (the initial, researcher-driven inclusion 
of features), authors often pick up features that have been successful in previous studies, and 
propose new experimental conditions to test them. Among the new experimental conditions, 
researchers may include the proposal of novel authorship attributes. Sometimes, as has been 
shown in the previous subsection, researchers include in their experimental settings the testing of 
different feature subsets to explore if some subgroup of them obtains a better success rate than 
the others. However, if an experiment begins with a researcher-driven feature selection, what 
subgroup of the features initially chosen renders the best possible results will remain unknown, 
unless individual experiments testing all the possible combinations of features subsets are 
conducted. The number of experiments for all possible feature subsets grows extremely fast as 
the number of features increases. The number of all possible feature subsets for n features is 
equal to the sum of all possible subsets of size n, n-1, n-2, all the way down to all possible 
subsets of size 1. We have to exclude the subset with zero elements, as it would not be 
productive to attempt a class attribution using no features. Without a formal proof and discussion 
 
84 
 
of the growing pattern for the sum of all feature subsets, this sum is equal to the sum of all the 
numbers in the nth row in a Pascal’s Triangle, minus one to eliminate zero or the empty set (for 
details about combinations and their algebra see Epp, 2004). Also informally, it can be 
mentioned that this sum is equal to 2 to the nth power minus one. This means that an authorship 
analysis study that wants to find the best possible results rendered by any possible subset formed 
with a list of 5, 25, and 100 features will have to perform 31, over 33.5 million, and over 1.2 
nonillion (or 1.2 x 1030) experiments, respectively. In the face of the impracticality of testing all 
the possible combination of feature subsets, a number of procedures have been developed to 
further reduce the initial selection of features in a classification task. The application of this type 
of procedure to authorship attribution will be reviewed in the rest of this subsection. 
 In this project, feature reduction techniques will be defined as any statistics-based, 
usually automated, process of further selecting a subset of the features originally chosen by the 
researcher. Also a defining aspect of feature reduction, this process is aimed at improving the 
success rate obtained in an authorship analysis task. Two more important purposes of feature 
reduction techniques are the improvement in the efficiency of the class attribution (the 
widespread availability of powerful computers today make this just a marginal advantage in our 
context) and the elimination of noise features, which make the class attribution error rate 
increase on new data (Manning et al., 2008). 
In the software implementation of class attribution methods, feature reduction techniques 
can be built-in options that are selected and fed values. This is the case with SPSS 
implementation of discriminant analysis (DA), a well known statistical classifier that has been 
widely used in authorship analysis (Baayen et al., 2002; Chaski, 2005; 2007; Grant, 2007; 
Mikros and Argiri, 2007; Rico-Sulayes, 2011; Spassova, 2008; 2009; Spassova and Turell, 2007; 
 
85 
 
Stamatatos et al., 2001; Tambouratzis and Vassiliou, 2007; Thomson and Murachver, 2001). 
Feature reduction techniques in DA are called stepwise procedures (for an in depth discussion of 
stepwise feature selection see McLachlan, 2004: 398--412). Examples of stepwise procedures in 
DA are Mahalanobis and Wilks’ Lambda. These stepwise procedures allow the user to give an 
entry and a removal value for the inclusion or exclusion of each feature during the reduction of 
their list. The entry and removal values represent the maximum and minimum scores that should 
be obtained for the F value of each feature. An F value is a score that represents the statistical 
significance of a feature in the discrimination of different classes (StatSoft, 2011). Once F values 
are calculated for all features, the program chooses the features whose scores are within the 
range given by the entry and removal values. This way, these two values permit an increase or 
reduction of the number of features to include in the task. It should be noted, however, that the 
entry and removal values do not allow the researcher to decide the exact number of features to 
use for the final classification. For example, in the SPSS implementation of DA the default entry 
value for F is 3.84, and its default removal value is 2.71. By narrowing or extending the range of 
these two values for F, the number of features used in the classification decreases or increases, 
respectively. If there is no experimentation on the effects of using various entry and removal 
values, default values are commonly selected (e.g., Chaski, 2005; Rico-Sulayes, 2011; Spassova, 
2009), but unfortunately this piece of information is not always reported (e.g., Chaski, 2007; 
Stamatatos et al., 2001; Tambouratzis and Vassiliou, 2007). 
An alternative to built-in feature reduction procedures in text classification is the use of 
an external feature reduction process (for an introduction see Manning et al., 2008). External 
feature reduction processes, as they will be understood in this project, have to be applied 
previously to and independently of the class attribution method.  In an external feature reduction 
 
86 
 
process the subset of features to use in the task is usually chosen from the top features in a list 
where they have been ranked according to some value.  Manning et al. (2008) present a number 
of these values. In text classification in general, the value that is the easiest to determine among 
those presented by these authors is plain frequency: the selection of the most frequent features 
(i.e., with the most instances). Another common value used as an external feature reduction 
process is mutual information (MI). This is a value that measures how much information the 
presence or absence of a feature offers for the class attribution. In the context of information 
retrieval, MI can be equivalent to another commonly used value, information gain (IG) (Hall, 
1999; Manning et al., 2008). In order to obtain the IG (or its equivalent MI) score for some 
feature f in a class c given some collection of texts, it is necessary to have five pieces of 
information: the total number of texts in the collection (ts), the number of texts in the collection 
that contain feature f in class c (tswfinc), the number of texts in the collection that contain feature f 
but are not part of class c (tswfoutsidec), the number of texts in the collection that do not contain 
feature f but are part of class c (tsw/ofinc), and the number of texts in the collection that do not 
contain the feature f and are not part of class c (tsw/ofoutsidec). It should be remembered that in the 
context of authorship attribution, which consists of the identification of the author of an 
anonymous text among a closed set of subjects, features are often represented by linguistic items 
and classes are the subjects in the closed set of potential authors. If we want to calculate, for 
example, the IG value of the use of the keyboard-based smiling emoticon, :), for one of the 
subjects in a set of potential authors, we need the five pieces of information above mentioned (ts, 
tswfinc, ts
wfoutsidec, ts
w/ofinc, and ts
w/ofoutsidec) and the high school level calculation of a few 
logarithms (for an easy-to-read presentation of logarithms see Math Forum, 2012). To complete 
the example, let us suppose that: i) the subject, for which IG will be calculated, uses the 
 
87 
 
keyboard-based smiling emoticon in 2 texts (tswfinc = 2), ii) there are a total of 10 texts written by 
this subject in the corpus (namely there are 8 more texts by the subject without the use of the 
emoticon, tsw/ofinc = 8), iii) 10 texts in the corpus have the smiling emoticon and are authored by 
other subjects (tswfoutsidec = 10), iv) the whole corpus has 100 texts (ts = 100), and v) 
consequently there are 80 texts that are not authored by the subject in question and do not have 
the smiling emoticon, tsw/ofoutsidec = 80). With these hypothetical data, the calculation of IG for 
the keyboard-based smiling emoticon in the subject in question requires substituting the values in 
the following, very simple, but long formula for IG (adapted from Manning et al., 2008): 
 
(3.1) 
IG (f, c) = ((tswfinc / ts) (log2 (ts * ts
wfinc / (ts
wfinc + ts
w/ofinc) * (ts
wfinc + ts
wfoutsidec)))  
 + ((tswfoutsidec / ts) (log2 (ts * ts
wfoutsidec / (ts
wfoutsidec + ts
w/ofoutsidec) *  
  (tswfinc + ts
wfoutsidec)))  
 + ((tsw/ofinc / ts) (log2 (ts * ts
w/ofinc / (ts
wfinc + ts
w/ofinc) * (ts
w/ofinc + ts
w/ofoutsidec)))  
 + ((tsw/ofoutsidec / ts) (log2 (ts * ts
w/ofoutsidec / (ts
wfoutsidec + ts
w/ofoutsidec) *  
  (tsw/ofinc +  ts
w/ofoutsidec))) 
 
If we plug in the values given in the example above, we get: 
 
IG (f, c) = ((2 / 100) (log2 (100 * 2 / (2 + 8) * (2 + 10)))  
+ ((10 / 100) (log2 (100 * 10 / (10 + 80) * (2 + 10)))  
+ ((8 / 100) (log2 (100 * 8 / (2 + 8) * (8 + 80)))  
+ ((80 / 100) (log2 (100 * 80 / (10 + 80) * (8 + 80))) 
 
88 
 
 
If we solve all sums, multiplications and divisions we first obtain: 
 
IG (f, c) = (0.02) (log2 (1.667)) + (0.01) (log2 (0.926)) 
+ (0.08) (log2 (0.909)) + (0.8) (log2 (1.01)) 
 
After calculating the logarithms and solving the last few operations, we can get the IG value of 
the keyboard-based smiling emoticon for the subject in question: 
 
IG (f, c) = (0.02) (0.737) + (0.01) (-0.111) + (0.08) (-0.138) + (0.8) (0.014) ≈ 0.0138 
 
If the distribution of feature f in the texts by some subject in the set of potential authors (class c) 
is the same as the distribution of this feature in the entire collection of texts, the IG value for f in 
this author (or class c) is 0. On the contrary, if feature f is only present in texts belonging to the 
subject in question and feature f is present in all of the texts authored by this subject, IG renders 
its maximum value. After computing the IG values for all of the initially selected features, a 
certain number of them with the highest IG scores can be further selected. This reduction of the 
list of features has as its goal the improvement of the final accuracy in the attribution of a class in 
an authorship analysis task. 
Manning et al. (2008) also mention as an external feature reduction value the calculation 
of a chi square test. Chi square is used to test if the occurrence of some feature is independent of 
the occurrence of a given class. In order to test the hypothesis of independence for the 
occurrence of feature f and class c, chi square uses the same text counts formerly presented: the 
 
89 
 
total number of texts in the collection (ts), the number of texts that contain feature f in class c 
(tswfinc), the number of texts that contain feature f but are not in class c (ts
wfoutsidec), the number 
of texts without feature f but in class c (tsw/ofinc), and the number of texts without feature f and 
not in class c (tsw/ofoutsidec). Relying on these figures, chi square calculates the expected frequency 
of f and c occurring together (Efreq
wfinc), the expected frequency of f occurring in classes other 
than c (Efreq
wfoutsidec), the expected frequency of f not occurring in class c (Efreq
w/ofinc), and the 
expected frequency of f not occurring in classes other than c (Efreq
w/ofoutsidec). These expected 
frequencies are calculated by following formulae (3.2) through (3.5) listed below and are adapted 
from Manning et al: 
 
(3.2) 
Efreq
wfinc = (ts) ((ts
wfinc + ts
w/ofoutsidec) / ts) ((ts
wfinc + ts
w/ofinc) / ts) 
(3.3) 
Efreq
wfoutsidec = (ts) ((ts
wfinc + ts
w/ofoutsidec) / ts) 
(3.4) 
Efreq
w/ofinc = (ts) ((ts
wfinc + ts
w/ofinc) / ts) 
(3.5) 
Efreq
w/ofoutsidec = (ts) (1- (ts
wfinc + ts
w/ofoutsidec) / ts)) (1- (ts
wfinc + ts
w/ofinc) / ts)) 
 
For the calculation of these expected frequencies, chi square assumes that feature f and class c 
are independent events. Once all of the expected frequency values defined in (3.2) through (3.5) 
are obtained, the chi square score for feature f in class c given some collection of texts can be 
calculated. In order to obtain this chi square value, all possible combinations of the texts’ actual 
 
90 
 
counts for feature f and class c (tswfinc, ts
wfoutsidec, ts
w/ofinc, and ts
w/ofoutsidec) and the expected 
frequency values (Efreq
wfinc, Efreq
wfoutsidec, Efreq
w/ofinc, and Efreq
w/ofoutsidec) are used in a simple 
formula that subtracts the square of the second type of value, the expected frequencies, from the 
first type of value, the text counts, and divides the results by the expected frequency:  
 
(3.6) 
 ((tssfsc  - Efreq
sfsc
2 ) / Efreq
sfsc) 
 
In (3.6) s stands for any of the two possible options for f and c, with or w/o and in or outside, 
respectively. The sum of all the possible combinations for (3.6) represents the chi square score of 
feature f in class c. The elaborate matrix with all the possible combinations of this simple 
formula and the not very intuitive mathematical expression that represents that matrix can be 
arithmetically simplified in the following, much easier-to-read formula, which renders the chi 
square (X2) score of f in c, (adapted from Manning et al., 2008): 
 
(3.7) 
 X2 (f, c) = (tswfinc + ts
wfoutsidec + ts
w/ofinc + ts
w/ofoutsidec)  
* ((tswfinc * ts
w/ofoutsidec) – (ts
wfoutsidec * ts
w/ofinc))
2  
/ (tswfinc + ts
w/ofinc) * (ts
wfinc + ts
wfoutsidec)  
* (tswfoutsidec + ts
w/ofoutsidec) * (ts
w/ofinc + ts
w/ofoutsidec) 
 
One more advantage of (3.7), besides its legibility, is that it only uses the original text counts. 
Therefore, we can plug into this formula the same values given above for the example about the 
 
91 
 
hypothetical use of the keyboard-based smiling emoticon in a chosen subject among a set of 
potential authors. Then, we will suppose once more that in a given collection of texts some 
subject uses the keyboard-based smiling emoticon in 2 texts (tswfinc = 2), that this subject 
authored 8 more texts without the emoticon (tsw/ofinc = 8), that 10 more texts authored by other 
subjects have the keyboard-based smiling emoticon (tswfoutsidec = 10), and that the whole corpus 
has 80 more texts that are not authored by the subject in question and do not have the emoticon 
(tsw/ofoutsidec = 80). Substituting these values in (3.7) we get: 
 
 X2 (f, c) = (2 + 10 + 8 + 80) * ((2 * 80) – (10 * 8))2  
/ (2 + 8) * (2 + 10) * (10 + 80) * (8 + 80) 
 
Solving for this formula we obtain the chi square score of the keyboard-based smiling emoticon 
for the subject in our example: 
 
X
2 (f, c) = 640000 / 950400 ≈ 0.6734 
 
In the context of our example, a chi square value is interpreted as a test of the hypothesis of 
independence between the feature and the class, namely the use of the keyboard-based smiling 
emoticon and the subject in question. High values for a chi square score imply that the expected 
frequency values and the text counts are not similar and that the hypothesis of independence 
between the feature and the class can be rejected with a small chance of being wrong (namely, 
high chi square scores indirectly imply a dependence of the feature and the class with a greater 
degree of confidence). If the chance of being wrong, or the degree of confidence, is especially 
 
92 
 
important in the interpretation of the chi square test, high chi square values are necessary. For 
example, chi square scores of 7.88 or larger are necessary to have a 99.5% confidence in the 
rejection of the assumption of independence (see Manning et al., 2008, p. 256). In our example, 
the resulting chi square score for the use of the keyboard-based smiling emoticon in the subject 
in question does not render a very high value, only 0.6734. After all, the hypothetical author only 
uses the emoticon in 20% of the texts authored by him or her, and the emoticon is present in 10% 
of the whole corpus. However, in authorship analysis tasks, which posit a problem similar to 
other texts classification tasks in this respect, an interpretation of the importance of individual 
features is not necessary; neither is it a maximal reduction of the list of features (Manning et al., 
2008). In authorship analysis, where the goal of feature reduction is to obtain the best accuracy 
result in the class attribution, different lists of features with various lengths are usually tested 
(see e.g, Burrows, 2002; Gamon, 2004; Hirst and Feiguina, 2007; and Rico-Sulayes, 2010). This 
means that the keyboard-based smiling emoticon may still be included in the reduced list of 
features. This will happen if there are not many other features with a higher score, or if the 
lowest chi square value that will be considered comprises the score obtained by the emoticon. 
Besides the three feature reduction methods just mentioned (plain frequency, IG, and chi 
square), there are a number of variable evaluation methods that are commonly used in statistics 
and that can be employed as external feature reduction techniques or along with them. Some of 
these variable evaluation methods are principal component analysis (PCA), analysis of variance 
(ANOVA), two-way ANOVA, and analysis of covariance (ANCOVA). The individual 
presentation of each of these variable evaluation methods would take much more space, as these 
methods draw on other mathematical constructs which would require their own introduction. 
However, just like the feature reduction techniques above presented, these methods permit 
 
93 
 
researchers to reduce a list of variables, or features, keeping the most important ones according 
to some criteria.  
Among the four methods just mentioned, only PCA represents a method that attempts to 
directly reduce a list of variables. In PCA, a list of measured variables (a list with the counts of 
instances for each linguistic feature in authorship analysis) is reduced to a shorter list that 
attempts to capture all of the variance among the variables in the original list (for a detailed 
introduction to the basics of PCA, see Hatcher, 1994). This means that in some list of counted 
features, a number of them tend to be redundant and not add more variability among the classes 
they belong to. These features can be eliminated without losing the variance shown in the list 
with all the features. Based on the original list of feature counts, PCA calculates the variability 
among the classes and identifies the features that add no or very little variability, so they can be 
eliminated. 
ANOVA, two-way ANOVA and ANCOVA do not attempt to reduce a list of variables, 
but evaluate the groups or classes (i.e. the subjects in authorship attribution, and the profiling 
categories in author profiling). These statistical methods evaluate if classes differ or are similar 
according to some variable (a linguistic feature in authorship analysis). Therefore, these methods 
can be used in a number of ways to reduce the list of features to either improve the accuracy of 
the attribution, or the validity of the study. For a detailed presentation of the mathematical 
calculations used to obtain the scores for ANOVA, two-way ANOVA and ANCOVA see 
StatSoft (2011). 
ANOVA can be used to obtain a score that represents how much classes (subjects or 
profiling categories) differ according to some feature. If we calculate scores for all features, the 
scores obtained with ANOVA can be used to pick up the features that represent the greater 
 
94 
 
variation among the classes. In the context of authorship attribution, for example, ANOVA 
allows the researcher to select the features according to which the subjects in the set of potential 
authors differ the most. Trying to solve a different problem, if there are some parallel classes 
whose influence should not be carried over into the classification of other classes, they can be 
identified with a two-way ANOVA. For example, we do not want to obtain a method that 
correctly differentiates between authors because they write on different topics. Mikros and Argiri 
(2007) use a two-way ANOVA to eliminate features that interact between topics and authors. 
Similarly, when there are features whose influence should not be transferred to other features, 
they can be identified with ANCOVA. This is the case of text length when whole texts (instead 
of similar sized text samples) are used. We do not want to correctly differentiate authors because 
the texts that were collected from each of them happened to have consistently different lengths. 
In authorship analysis studies that use whole texts, text length is controlled for in a number of 
ways. Grant (2007) uses ANCOVA to control for the influence of text length in the class 
attribution. The last two examples on the use of two-way ANOVA and ANCOVA (to control for 
topic and text length, respectively) are rather focused on the improvement of the validity of the 
studies. Therefore, these methods are not feature reduction techniques, as these techniques have 
been defined in this project. However, these two methods have been mentioned here because 
they can be gradually applied with other feature reduction techniques and can be hard to separate 
from them. A number of studies that employ all of the feature reduction techniques mentioned 
above (both built-in and external) will be surveyed right below. It should be mentioned once 
more that beyond the existence of various feature reduction techniques, a combination of several 
of them in the same experiment is possible, as it happens in a number of the studies that will be 
surveyed. 
 
95 
 
  Table 3.3 shows all the studies, among the research papers presented in Table 3.1, which 
employ feature reduction techniques. As can be seen in Table 3.3, more than half the studies 
surveyed at the beginning of this section as authorship attribution studies use some form of 
feature reduction technique, i.e., 17 out 32 studies formerly surveyed. Five of the 17 studies 
included in Table 3.3 use some built-in feature reduction technique, or stepwise procedure, in 
DA (Chaski, 2005; 2007; Rico-Sulayes, 2011; Spassova, 2009; Tambouratzis and Vassiliou, 
2007). This does not mean that only built-in feature reduction techniques can be used when the 
class attribution method is DA. External feature reduction techniques can also be applied 
independently, and the resulting reduced list of features can then be fed to DA. This is actually 
done in a couple of studies in Table 3.3 (Grant, 2007; Mikros and Argiri, 2007).  
  
 
96 
 
Table 3.3 Authorship attribution studies using feature reduction techniques and their best results 
Table 
Authorship Attribution Studies Feature reduction 
techniques Best results 
Abbasi and Chen (2008) IG NA 
Burrows (2002) relative frequency with the most 
features 
Chaski (2005) Mahalanobis NA 
Chaski (2007) NDP NA 
Gamon (2004) plain frequency 75 instances in 
corpus 
Grant (2007) ANCOVA + ANOVA NA 
Hirst and Feiguina (2007) relative frequency largest set of 
features 
Jockers et al. (2008) mean relative freq = 0.1 % and above NA 
Koppel et al. (2009) IG among the most frequent NA 
Mikros and Argiri (2007) stepwise procedure (NDP) 
+ two-way ANOVA NA 
Orebaugh and Allnutt (2009) IG with chi-square NA 
Peng et al. (2003) plain frequency NA 
Raghavan et al. (2010) eliminating stop words NA 
Rico-Sulayes (2011) Wilks’ Lambda, 
Mahalanobis indistinct 
Spassova (2009) Wilks’ Lambda NA 
Tambouratzis and Vassiliou (2007) stepwise procedure, NDP with the most 
features 
Tearle et al. (2008) PCA NA 
      
Note. NDP = no details provided, NA = does not apply 
 
 
As to the score values mentioned in Manning et al. (2008), two of them are the most commonly 
among the studies surveyed in Table 3.3. Six studies use frequency. Two of these studies use 
plain frequency (Gamon, 2004; Peng et al., 2003), one study uses relative (or normalized) 
frequencies (Jockers et al., 2008), two studies use the n top elements in a list where these 
elements are ordered by their relative frequency (Burrows, 2002; Hirst and Feiguina, 2007); and 
 
97 
 
one more study combines frequency with another technique (Koppel et al., 2009, combine 
frequency with IG). Three studies use IG. One of them uses only IG (Abbasi and Chen, 2008), 
and the other two combine it with other methods. Formerly mentioned, Koppel et al. (2009) 
combine IG and frequency, and Orebaugh and Allnutt (2009) use IG followed by chi-square 
tests. Besides these two common techniques, one study uses a statistical feature evaluation 
method: Tearle et al. (2008) employ PCA. Two studies more studies, not mentioned before, 
explore combinations of different techniques, which include a statistical feature evaluation 
method. Grant (2007) uses a two step procedure, applying first ANCOVA and then ANOVA, 
and Mikros and Argiri (2007) use a combination of a stepwise procedure in DA (for which they 
do not give more details) with two-way ANOVA. Finally, Raghavan et al. (2010) use a reject list 
procedure, eliminating all stop words, another term often used for function words in computer 
science.  
 Although feature reduction techniques are common in authorship attribution studies, as 
can be seen in Table 3.3 which includes over half the studies surveyed in authorship attribution, 
not many of them test and compare different techniques or settings for those techniques. Only 5 
studies out of 17 in Table 3.3 do this (Burrows, 2002; Gamon, 2004; Hirst and Feiguina, 2007; 
Tambouratzis and Vassiliou, 2007; Rico-Sulayes, 2011). Four out of these five studies compare 
the results obtained when using criteria that allows a technique to produce feature sets with 
various sizes. Burrows (2002) uses the types with the highest relative frequency. Using the 
number of most frequent types to include in the experiment as a cut point value, he explores the 
results obtained for 6 different sizes of the feature set. Burrows tests feature sets that include the 
most frequent 150, 120, 100, 80, 60, and 40 types. This researcher obtains the best results with 
the largest list of most frequent types, namely 150. Gamon (2004) uses as cut points the number 
 
98 
 
of feature instances in the corpus, namely he uses features absolute frequency to select what 
features to include in the class attribution. Using this selection criterion, Gamon compares the 
results obtained with the features from different absolute frequencies (5, 10, 20, 50, 75, 100, 200, 
and 500 instances). In Gamon’s corpus, using features with an absolute frequency of 75 instances 
renders the best results. Hirst and Feiguina (2007) employ as their selection criterion the top n 
elements in a ranked list of the most frequent features. They test different values for n (50, 75, 
and 150) and obtain their best results with their largest set of features. Tambouratzis and 
Vassiliou (2007) use a stepwise procedure in DA, although they do not give details about which 
one. These researchers test a number of different options for the entry and removal values fed to 
the stepwise procedure. They find out that the best results are obtained when these values are the 
furthest apart, namely when the most features are used in DA. Finally, only one study (Rico-
Sulayes, 2011) compares the results obtained with different feature reduction techniques. This 
study tests two different feature reduction techniques in DA, Wilks’ Lambda and Mahalanobis, 
but does not get any difference in the success rate when comparing the best results obtained by 
both procedures. 
It should also be noted that it is not common to compare the results obtained with and 
without using feature reduction techniques. This happens because it is generally assumed that 
feature reduction techniques do improve the results. At least in DA, stepwise procedures do not 
eliminate any features (i.e. they do not reduce their list) unless there is an improvement in the 
success rate of the classification task. In the case of external feature reduction techniques, 
however, it is good to provide the results obtained before and after the application of the 
reduction procedure. This is actually done in at least one of the author profiling studies, in which 
a great improvement is obtained using IG as the feature reduction technique. The need for 
 
99 
 
showing the improvement obtained through feature reduction, if any, is especially true for plain 
frequency, where there is not an elaborate calculation of the statistical value or weight of both 
the features selected and the features eliminated from the classification task. 
3.1.3  Feature Selection in Author Profiling 
The two former subsections, devoted to authorship attribution, presented and discussed briefly 
the proliferation of authorship features, a number of proposed categorizations of them, the 
research-driven feature selection that takes place before the experiment, the possibility of 
grouping features in subsets, the testing of these subsets’ performance, and the various statistics-
based feature reduction techniques that have been used in previous studies. Since these 
comments apply in general to authorship analysis, the topics of feature selection and feature 
reduction in relation to author profiling does not require a separate presentation of these topics in 
this and the following subsections. As discussed at the beginning of this chapter, the survey of 
the feature selection processes in author profiling is aimed at supporting the experimental 
decisions made later in this project. 
 As broadly defined in Chapter 2, author profiling consists of characterizing the author of 
an anonymous piece of text across a number of categories. Although this term and its definition 
are more regular in the literature than the formerly discussed term of authorship attribution, the 
author profiling categories considered in authorship analysis taxonomies are not always the 
same. While most authors would probably agree on profiling categories such as the author’s 
gender, age, and dialect, some others include the historical time when the author wrote the 
document, the author’s mental capacity (Juola, 2007), the author’s educational and cultural 
background, the author’s language proficiency (Zheng et al., 2006), and the author’s language 
nativeness (whether the author is a native speaker of the language in which the document is 
 
100 
 
written or not) (Juola, 2008). If we consider the categories that have actually been explored in 
research papers targeting author profiling (see Table 3.4), this list grows. In this research project, 
however, I will leave aside the issue of delimiting the categories of author profiling, mainly 
because this project will not target an author profiling task. This chapter includes a survey of the 
experiment conditions that influence the success rate of author profiling because this task shares 
the same experimental conditions as authorship attribution, and a survey of the treatment that has 
been given to this in the literature should help support the decisions made in the experimental 
chapter of this project. 
 Table 3.4 shows the number of features, the categories they belong to (following the 
feature three-category taxonomy formerly presented in this project), and a selection of feature 
samples for each category used by a number of author profiling research papers. Once more, the 
features selected as examples of feature categories represent, whenever possible, subcategories 
that comprise several individual features. Although author profiling research has been less 
prolific than authorship analysis, the survey presented in Table 3.4 should be seen as 
representative rather than exhaustive. The main goal of this table is to draw a broad picture of the 
kind of feature selection that has performed in author profiling, as this task has been approached 
by researchers. It should also be noted that this table does not attempt to include descriptive work 
where language differences across common author profiling categories (such as gender and age) 
have been targeted, either qualitatively (e.g., Corson, 2001; Eckert and McConnell-Ginet, 2003; 
Tannen, 1994; 2005) or quantitatively (e.g., Eckert, 2000; Labov, 2001; Wolfram and Schilling-
Estes, 2006). In author profiling, as it has been defined in Chapter 2, the goal of characterizing 
the texts with different categories is to apply this characterization to the classification of 
 
101 
 
anonymous texts, assembled in testing data sets. Therefore, only studies that pursue this goal 
have been summarized in Table 3.4. 
 
 
  
 
102 
 
Table 3.4 Number of features, feature categories, and selected features sampled in previous 
author profiling studies 
Table  
Author Profiling Studies # of 
features 
Feature 
categories Feature samples 
Corney (2003) 
857+ 
and 
gender 
specific 
a) lexical, b) 
syntactic, c) 
structural 
a) character bigrams; b) function word 
frequencies; c) average sentence length, 
type-token ratio  
Corney, de Vel, Anderson, 
and Mohay (2002) 222 
a) lexical, b) 
syntactic, c) 
structural 
a) # words with suffixes, words with root 
'apolog', instances of 'sorry'; b) function 
word frequencies; c) media specific, 
complexity measures 
de Vel et al. (2002) 222 
a) lexical, b) 
syntactic, c) 
structural 
a) # words with suffixes, words with root 
'apolog', instances of 'sorry' (gender 
preferential?); b) function word 
frequencies; c) media specific, complexity 
measures 
Koppel et al. (2002) 1081 syntactic function words, most frequent POS 
unigrams, bigrams and trigrams 
Koppel et al. (2009) 1372 a) lexical, b) 
syntactic 
a) frequent content words; b) functional 
grammar labels (POS + semantic / 
subcategorization information) 
Koppel et al. (2008) NDP a) lexical, b) 
syntactic 
a) frequent content words; b) functional 
grammar labels (POS + semantic / 
subcategorization information) 
Koppel et al. (2005) 1035 a) lexical, b) 
syntactic 
a) character n-grams, frequent lexical 
idiosyncrasies/errors; b) function words, 
rare POS bigrams 
Schler et al. (2005) 1502 
a) lexical, b) 
syntactic, c) 
structural 
a) content words, Internet neologisms; b) 
selected POS, function words; c) media 
specific 
Spassova (2009) 
1) 133-
159, 2) 
152 
syntactic most frequent POS bigrams and trigrams 
Thomson and Murachver 
(2001) 13 
a) lexical, b) 
syntactic 
a) content words and phrases; b) modals, 
adverbs, adjectives 
 
  
 
103 
 
Table 3.4 (continued) 
 
Author Profiling Studies # of 
features 
Feature 
categories Feature samples 
Zhang, Dang, and Chen 
(2009) 10284 
a) lexical, b) 
syntactic, c) 
structural 
a) word unigrams and bigrams (single 
words and two-word sequences); b) NDP; 
c) NDP 
        
Note. NDP = no details provided 
 
 
As shown in Table 3.4, in author profiling there is also great variation in the number of features 
that have been used in different studies. However, in the case of the studies devoted to this 
authorship analysis task, there is a tendency to use longer lists of features. In these long lists of 
features, it is common to find long word lists, such as all types (or all word unigrams) in the 
corpus. As a consequence, only one study utilizes less than 100 features (Thomson and 
Murachver, 2001). In contrast, 4 out 11 studies in Table 3.4 use hundreds of features and less 
than a thousand (Corney, 2003; Corney et al., 2002; de Vel et al., 2002; Spassova, 2009). There 
are also 5 studies that employ over a thousand features (Koppel et al., 2002; Koppel et al., 2009; 
Koppel et al., 2005; Schler et al., 2005; Zhang et al., 2009). It should be noted that the author 
profiling studies surveyed are much more consistent in making explicit the number of features 
they use as compared to the authorship attribution papers reviewed in the former subsections. 
Only 1 out of 11 author profiling studies (compared to 10 out of 32 authorship attribution 
research papers) fails to mention the number of features it employs (Koppel et al., 2008). As 
discussed above, the feature categories shown in the third column of Table 3.4 correspond to the 
classification of features presented in this project. Two studies use features from one single 
category (the syntactic one) (Koppel et al., 2002; Spassova, 2009), four studies use features from 
one pair of categories (lexical and syntactic) (Koppel et al., 2009; Koppel et al., 2008; Koppel et 
 
104 
 
al., 2005; Thomson and Murachver, 2001), and the rest of the research papers, a total of 5, 
employ features from all three categories. 
It is relevant to mention that three studies, all of them related, report using “gender 
specific” or “gender preferential” features (Corney, 2003; Corney et al., 2002; de Vel et al., 
2002). In these three studies, the authors report no improvement after including these features in 
the task of profiling the gender category. Although Corney (2003) does not give details about 
what features he used, the other two studies mention the inclusion of 9 adjective suffixes (“-
able”, “-al”, “-ful”, “-ible”, “-ic”, “-ive”, “-less”, “-ly”, “-ous”), the word “sorry”, and the prefix 
“apolog-”. Corney et al. (2002) and de Vel et al. (2002) refer to an online list of gender and 
language studies, in which according to these researchers these kinds of language items are 
reported as symptomatic of language use differences between genders. However, no link 
between any individual studies and their list of features is established. They even use a quoted 
label for the concept of “rapport talk”, among other related concepts, but no reference to any of 
Tannen’s studies on the topic is included in their bibliography (e.g., 1990; 1994; 1995). In the 
three interrelated papers listed at the beginning of this paragraph (Corney, 2003; Corney et al., 
2002; de Vel et al., 2002), the final selection of category-specific features is not explicitly 
justified, and no arguments are given to support the exhaustiveness of the gender-specific 11-
item list. In addition to giving little argumentation to support their list, it is not particularly hard 
to see that the two tokens of apologetic language these scholars include may be failing to 
represent the “women’s language” that these authors characterize, following other studies, as 
“more punctuated with attenuated assertions, apologies, questions, personal orientation and 
support” (Corney et al., 2002, p. 283). It should also be mentioned that although a binary 
 
105 
 
biological categorization of gender seems to be used in the three studies reviewed in this 
paragraph, there is not an explicit definition of this concept in any of them. 
As it happens in authorship attribution studies, arranging features in a number of subsets 
and testing the performance of these subsets independently and in different combinations is a 
common task in author profiling. Table 3.5 includes all of the studies in Table 3.4 and shows in 
its third column whether a testing of feature subsets was performed or not. This table also 
reproduces the different profiling categories explored in these studies (originally shown in Table 
2.4), since each profiling category constitutes an independent experiment. The categories of 
features used in the studies are included once more, in the third column of Table 3.5. It should be 
noted that the three categories mentioned in this column do not necessarily match the subsets of 
features tested in the studies.  
  
 
106 
 
Table 3.5 Profiling categories, feature subset testing, and best results for subsets in previous 
author profiling studies 
Table  
Author Profiling Studies Profiling 
categories 
Feature 
categories 
Tests for 
feature 
subsets 
Best results for feature 
subsets 
Corney (2003) 
1) gender, 2) 
language 
nativeness 
a) lexical, b) 
syntactic, c) 
structural 
1) yes, 2) 
no all features combined 
Corney et al. (2002) gender 
a) lexical, b) 
syntactic, c) 
structural 
yes all features combined 
de Vel et al. (2002) 
1) gender, 2) 
language 
nativeness 
a) lexical, b) 
syntactic, c) 
structural 
1) yes, 2) 
no all features combined 
Koppel et al. (2002) gender syntactic yes all features combined 
Koppel et al. (2009) 
1) gender, 2) 
age, 3) native 
language, 4) 
neuroticism 
level 
a) lexical, b) 
syntactic yes 
1) and 2) all feature 
combined, 3) lexical, 
4) syntactic 
Koppel et al. (2008) 
1) gender, 2) 
age, 3) native 
language 
a) lexical, b) 
syntactic yes 
1) and 2) all feature 
combined, 3) lexical 
Koppel et al. (2005) native language a) lexical, b) 
syntactic yes all features combined 
Schler et al. (2005) 
1) gender, 2) 
age 
a) lexical, b) 
syntactic, c) 
structural 
yes all features combined 
Spassova (2009) 
1) writer's time 
span, 2) dialect syntactic yes 
1) unitary invariant 
approach (no test for 
all combined), 2) 
indistinct (no test for 
all combined) 
Thomson and Murachver 
(2001) 
gender a) lexical, b) 
syntactic yes all features combined 
Zhang et al. (2009) gender 
a) lexical, b) 
syntactic, c) 
structural 
no NA 
          
Note. NA = does not apply 
    
 
 
107 
 
 
As seen in Table 3.5, all but one of the studies surveyed in author profiling perform some feature 
subset testing. Only Zhang et al. (2009) do not include this kind of experiment. As to the 10 
studies that do test feature subsets, they include a total of 19 separate profiling category 
experiments. This larger number of experiments results from the fact that 6 studies explore more 
than one profiling category (Corney, 2003; de Vel et al., 2002; Koppel et al., 2009; Koppel et al., 
2008; Schler et al., 2005; Spassova, 2009). After examining the fifth column in Table 3.5, it can 
be seen that 14 out of the 19 profiling category experiments obtained their best results using the 
set of all features. Only 3 profiling category experiments conducted in two related studies 
(Koppel et al., 2009; Koppel et al., 2008) obtained their best results through the use of other 
subsets. In both studies the lexical feature subset rendered the highest success rate when profiling 
the category of native language. The functional grammar tags (Parts of speech plus semantic or 
subcategorization information), which represented the syntactic feature category, rendered the 
best results when neuroticism level was targeted as a profiling category (Koppel et al., 2009). As 
to how this profiling category was measured by the researchers, they briefly explain they used a 
psychological questionnaire, filled by the authors of a corpus of short essays. In the 
questionnaire, which is supposed to measure various personality dimensions including 
neuroticism level, the researchers define two classes “positive” and “negative”. Then, they 
choose some threshold in the neuroticism scores rendered by the questionnaire to classify the 
authors of the essays. Koppel et al. do not offer any rationale to support their definition of 
classes. Two more profiling category experiments, out of the 19 formerly mentioned, did not test 
the combination of all the features (both conducted in Spassova, 2009). So it is not possible to 
know if their results could have been improved by feeding the set of all their features to the class 
 
108 
 
attribution method. Similar to what happens in the subset experimentation in authorship 
attribution studies, subset testing has not produced a consistently higher success rate in author 
profiling. Nevertheless, these experiments have been indirectly useful because they have pointed 
out, as a better option, the use of all features and their potential reduction through different 
techniques. 
3.1.4  Feature Reduction Techniques in Author Profiling 
Table 3.6 surveys all the studies that use feature reduction techniques among the author profiling 
studies formerly presented in Table 3.4 and Table 3.5. Once more, this table includes the 
different profiling categories, as each profiling category represents an independent experiment. 
Table 3.6 includes 5 out of the 11 studies surveyed in the previous two tables. Among these 5 
studies, two use built-in feature reduction techniques. Koppel et al. (2002) implement their own 
classification method (further commented on in the next subsection) and they use it to generate 
weight scores for all features. As they apply their classification method to the training data, this 
method produces a score for each feature. This score changes monotonically as the classification 
method is applied repeatedly to the training data. This means that a low score (of a non-relevant 
feature) decreases and a high score (of a relevant feature) increases with a number of passes on 
the data. After applying the classification method a number of times, the final weight scores are 
used to select the features that will be fed to the classifier. Testing a range of weights that goes 
from 16 to 256, the authors report that their best results are obtained using values between 64 and 
128. Since this feature reduction technique is completely dependent on Koppel et al.’s classifier 
implementation, it is not used in other authorship analysis studies. The other study in Table 3.6 
that uses a built-in feature reduction technique is Spassova (2009). She uses a DA stepwise 
procedure to calculate Mahalanobis distance. Mahalanobis is one stepwise procedure, among 
 
109 
 
several possible ones in DA, which has also been used in a pair of authorship attribution studies 
(Chaski, 2005; Rico-Sulayes, 2011); this is shown above in Table 3.3. Finally, three studies in 
Table 3.6 use an external feature reduction technique. These three studies employ IG: Koppel et 
al. (2008), Schler et al. (2005), and Zhang et al. (2009). The last two studies, Schler et al. (2005) 
and Zhang et al. (2009), apply IG to content words only. Namely, the rest of the items in their 
initial feature selection are not scored or further selected using this feature reduction technique.  
 
  
 
110 
 
Table 3.6 Author profiling studies using feature reduction techniques and their best results 
Table  
Author Profiling Studies Profiling 
categories 
Feature 
reduction 
techniques 
Best results 
Koppel et al. (2002) gender 
weight from 
multiplicative 
update rules 
128-64 highest 
weight features 
(from 256-16 
range) 
Koppel et al. (2008) 
1) gender, 2) 
age, 3) native 
language 
1) and 2) 
none, 3) IG NA 
Schler et al. (2005) 1) gender, 2) age 
IG on content 
words NA 
Spassova (2009) 1) writer's time 
span, 2) dialect 
2) 
Mahalanobis NA 
Zhang et al. (2009) gender IG on content words 
using feature 
reduction 
        
Note. NA = does not apply 
   
 
 
One more important comment should be made in this subsection about the results 
presented in Zhang et al. (2009). In the subsection devoted to the use of feature reduction 
techniques in authorship attribution, it was mentioned that researchers do not usually show the 
results obtained without their feature reduction technique, when they include this variable in their 
studies. This is because it is assumed that feature reduction always improves the results. In 
Zhang et al. (2009), however, the authors show great improvement in their results after using a 
feature reduction technique. Applying IG to content words, the success rate in their experiments 
goes from 66% to 92% when a list of 10,284 features is reduced to 640. These authors probably 
decide to show their results before and after the application of a feature reduction technique 
because of the dramatic improvement in accuracy brought about by the inclusion of this 
procedure in their experiment. However, even in the cases where the improvement is less 
 
111 
 
remarkable, the presentation of both results should be helpful in showing the benefits of feature 
reduction to those who are not familiar with the use of these techniques. 
3.2  CLASS ATTRIBUTION METHODS IN AUTHORSHIP ANALYSIS 
As formerly explained, the first step in authorship analysis studies after collecting and arranging 
the data is feature selection, the research-driven inclusion of a number of all possible authorship 
features. This feature selection, which depends on the researchers’ intuition or experience, is a 
virtually inevitable step as the number of features keeps growing, and it is unknown if the list of 
all possible features will ever be complete or exhausted. After performing this first selection of 
features, the data has to be analyzed to identify all the instances of those features in the data. All 
feature instances are usually collected in either tables with examples or lists (also called vectors) 
with the total number of feature instances per document. Both tables and vectors may also 
include other representations of the presence of a given feature in some document, beyond the 
count of its instances (or absolute frequency). Other feature representations that can be used, 
instead of the absolute frequency, are the relative frequency of the feature (the count of its 
instances normalized over all features observed) or various possible ratios of the feature in 
question to other features. As an optional step, features can be further selected or reduced in an 
attempt to improve their efficiency in the task, as it has just been described in the previous 
section. Then, the results of the feature identification step have to be processed using some 
method in order to attribute a class to testing or disputed samples. In authorship attribution the 
class to be assigned corresponds to an author identity, and in author profiling it represents a 
characteristic of the author.  
Elsewhere, I have made a categorization of the methods that have been commonly used 
for the classification of text samples in authorship attribution (2011). Since this categorization 
 
112 
 
does not depend on the characteristics of the task, but on the characteristics of the classifying 
methods themselves, it applies to all of the class attribution methods that will be presented below 
for the different authorship analysis tasks. In this categorization, I followed a number of previous 
taxonomies and divided the methods by the techniques they use to perform the classification. I 
identified three different types of techniques: qualitative, statistical, and machine learning. In 
qualitative methods, where tables with examples of various features are commonly used, the 
analyst depends on his or her expertise to reach a conclusion and attribute a class, based on a few 
occurrences of some features. Even in the case that an analyst uses a feature reduction technique 
to further process the features selected, if the class attribution is based on the expertise of the 
analyst, the classification method is qualitative -- in a case like this the approach as a whole 
would be a hybrid, but the class attribution method would still be qualitative. On the contrary, 
both statistical and machine learning methods apply classification techniques to all instances of 
the features selected by the researcher, or to all instances of a reduced feature list, and make the 
class attribution based on some statistical model of the classes in the tasks. Another important 
characteristic of both statistical and machine learning methods is that they attempt to reach 
statistical significance in their evaluation of their classification. I formerly separated these two 
types of methods, statistical and machine learning, following a number of researchers that make 
a distinction between them. However, it should be mentioned that both types of methods use 
statistics-based classification approaches, and their main difference is that they have originated 
or have been widely adopted in different scientific communities.  
As Witten et al. (2011) point out, there is not a dividing line between statistical and 
machine learning methods, but a continuum. Expanding on what I have said about the different 
communities that employ these techniques, statistical methods, such as DA, have been proposed 
 
113 
 
by mathematicians and statisticians and have been used to solve problems in various different 
scientific fields. In contrast, machine learning techniques have mostly originated and been 
applied in the field of computer science and electronics. Therefore, the divergence between these 
two fields has to do mostly with the fact that their methods originate in different traditions, as 
Witten et al. concur. For these authors, if we are forced to single out a difference in the emphasis 
of these two fields, we may say “that statistics has been more concerned with testing hypotheses, 
whereas machine learning has been more concerned with formulating the process of 
generalization as a search through possible hypotheses” (p. 28). Namely, in statistical classifiers 
a hypothesis is usually assumed and stated from the beginning, while in machine learning 
classifiers a number of outcomes are tested as if the classification problem was a matter of 
searching and finding the correct hypothesis. However, Witten et al. acknowledge that this 
methodological difference is not a constant and this distinction oversimplifies the delimitation of 
the two fields; after all statistical classifiers go beyond mere hypotheses testing and there are 
machine learning classifiers that do not perform any search. The distinction between these two 
types of methods in authorship analysis is usually supported by researchers that view a milestone 
improvement in the use of machine learning classification methods, before statistical methods 
(see e.g. Koppel et al., 2009); however, it is not always the case that the former methods perform 
better than the latter (see e.g. Chaski, 2007). Besides, if we define machine learning as a field 
that “uses the theory of statistics in building mathematical models, because the core task is 
making inference from a sample” (Alpaydin, 2010), there is virtually no difference between this 
field and the field of statistics for the purposes of authorship analysis work, where the methods 
of these two fields are used to perform a classification of a text sample based on some corpus. 
For all this, in this project I will only distinguish between qualitative and quantitative class 
 
114 
 
attribution methods. The latter should include both statistical and machine learning techniques. It 
should also be mentioned that Grieve has used before the label of “quantitative authorship 
attribution” to describe the same type of methods this label will refer to in this project (2007). 
 Before surveying quantitative classification methods in the authorship attribution tasks 
relevant to this research, a few issues regarding the data fed to these methods should be 
commented upon. Given the statistical constructs upon which many of these methods are based, 
it should be noted that depending on the authorship analysis task, it may be necessary to have 
training data from all possible classes. Having training data for all classes is a requirement in 
authorship attribution and author profiling, the two authorship analysis tasks targeted in this 
project. This is not necessarily the case for the other two authorship analysis tasks defined in the 
taxonomy proposed at the beginning of last chapter, author identification and clustering. In 
author identification, for example, it is not necessary to have information for a potential class 
“other” or “none in the set”. This is the case in two experimental conditions described in the 
literature about this task: when the set of all possible authors has one subject (e.g., McMenamin, 
2002), or when the set of all possible authors has a few subjects among whom the author is not 
necessarily included (Juola, 2008). An exception to this lack of information for the class “other” 
in author identification is Coulthard’s corpus approach (as classified by Solan and Tiersma, 
2004). This approach does use information from corpora to comment on the features selected 
from the anonymous text, but does not attempt to build a model for the class “other” or “none in 
the set”. As mentioned before, researchers in the biometric community acknowledge that 
building this model is not a straightforward task (Bolle et al., 2004). The lack of information for 
one of the classes in author identification is closely related to the frequent use of disputed texts in 
this task, i.e., texts of unknown authorship to the researcher or investigator. Since the true author 
 
115 
 
of a disputed text may not be in the set of suspects, data for the true author may be completely 
absent in the training data. As implied above, this lack of a reference to compare the disputed 
text to has a possible solution. An option would be to do what Bolle et al. call modeling the 
world, i.e., to build a model for an identity labeled “other” which would try to capture the feature 
distributional patterns that are not present in the set. This solution is not simple because this label 
“other” may have to capture all possible distributional patterns absent in the set, which may be 
infinite, or at least all the relevant ones, which may be extremely hard to determine. As to the 
other task of authorship analysis defined in the proposed taxonomy, clustering poses a different 
problem where the total number of classes is not necessarily known. If a class is not identified by 
the clustering method, the features for such class are never labeled and processed as belonging to 
it. If disputed texts are used during clustering, there may be no more texts by the same author in 
the set of texts of known authorship. However, clustering has the possibility of isolating the 
disputed text from the rest of the data set. 
 It should also be noted that the potential lack of data for one or more classes and the use 
of disputed texts are closely related, but one experimental condition does not entail the other. On 
one side, for example, given the definition of authorship attribution in subsection 2.1 (the 
identification of the author of an anonymous text among a closed set of subjects in which the 
unknown author is included), this task does include information for all possible classes. If the 
experiment includes texts whose authors may not be part of the set of potential authors, the 
testing samples would be disputed and the task would be an instance of author identification, a 
task also defined in subsection 2.1. On the other side, author profiling can take disputed texts and 
yet have data from all possible classes. In general, it is relatively acceptable to model the world 
under a set of profiling classes and get training data for all those classes. Therefore, a disputed 
 
116 
 
text can be used and yet the experiment can have training data for the category to which the 
unknown author of that text belongs. Certainly, one class may be especially hard to collect 
information for, but as far as a minimally efficient model can be implemented and tested, the task 
should be attainable. Also, potentially dispreferred sets of classes should result in new sets with 
rearranged or extended data collections. For example, a set of classes for the category age which 
divides all testing samples in the two classes “under 40 years old” and “40 years old and above” 
may not be useful in some contexts. If that is the case, a different class set should be proposed 
and training data that corresponds to the new class division should be obtained.  
The discussion about the potential lack of information for one or more classes is 
important because this experimental condition is not suitable for a number of classification 
methods. In many classifiers, such as DA, all classes belonging to one category are mutually 
exclusive (no testing sample can belong to two classes in the same category) and collectively 
exhaustive (all testing samples belong to one of all the possible classes in a given category) 
(Burns and Burns, 2008). Of course, if various categories are used, one sample will belong to 
several classes, but only to one per category. Namely, an author can have an age and a gender 
class, but no author will belong to two age classes at the same time. These experimental 
conditions are assumed by many classification methods, and a failure to comply with these 
conditions can violate the statistical constructs upon which many of the methods that will be 
presented below rely. 
3.2.1  Quantitative Class Attribution Methods in Authorship Attribution 
A number of different class attribution methods have been applied to the task of authorship 
attribution. In Table 3.7, there are a total of 23 different quantitative methods that have been used 
in all the authorship attribution experiments surveyed in former sections of this project. At least 2 
 
117 
 
of those methods are common statistical methods implemented in popular software packages, 
such as SPSS and Stata, which are used across different scientific fields. These methods are chi 
square and DA. Three other methods (delta, iota and zeta) have been proposed in the field of 
forensic stylistics, specifically for their application in authorship attribution (Burrows, 2002; 
2007). The rest of the methods, a total of 18, are machine learning algorithms (or variations of 
them) that are commonly used in computer science. Many of the 23 class attribution methods 
included in Table 3.7 are based on highly elaborate mathematical calculations and a detailed 
description of all of them is beyond the scope of this research. However, an introduction to the 
most frequently used methods is widely available in statistical pattern recognition manuals (see 
e.g. McLachlan, 2004; and Timm, 2002) and information retrieval textbooks (see e.g. Alpaydin, 
2010; Manning et al., 2008; and Witten et al., 2011). Methods specifically developed for 
authorship analysis (e.g., Abbasi and Chen’s writeprints, 2008; Burrows’ delta, 2002; and 
Burrows’s iota and zeta, 2007) are presented in the research papers where they have been 
proposed, which are included in Table 3.7. This is also the case for methods that have been 
adapted for their application in this field (e.g., Juola and Baayen’s cross-entropy, 2005; and 
Tearle et al.’s artificial neural networks, 2008). The papers where these adaptations have been 
proposed include an introduction to their methods and their implementation. In the next chapter, 
after selecting below the classification methods that will be tested in the experiments conducted 
in this project, the selected methods will be introduced in greater detail. 
  
 
118 
 
Table 3.7 Class attribution methods used in previous authorship attribution studies 
Table  
Authorship Attribution Studies Classifiers Best results 
Abbasi and Chen (2005) C4.5, SVMs SVMs 
Abbasi and Chen (2008) Writeprints, SVMs varies 
Argamon et al. (2003) multiclass EG NA 
Baayen et al. (2002) DA NA 
Burrows (2002) Delta NA 
Burrows (2007) Zeta, Iota Zeta 
Caver (2009) MEGAM NA 
Chaski (2001) Chi-square NA 
Chaski (2005) DA NA 
Chaski (2007) DA, SVMs, DT 
forest DA 
Corney (2003) SVMs NA 
Gamon (2004) SVMs NA 
Grant (2007) DA NA 
Grieve (2007) Chi-square NA 
Hirst and Feiguina (2007) SVMs NA 
Jockers et al. (2008) Delta, NSC NSC 
Juola and Baayen (2005) Cross-entropy NA 
Koppel et al. (2009) NB, J4.8, RMW, BMR, SVMs SVMs, BMR 
Mikros and Argiri (2007) DA NA 
Nazar and Sánchez Pol (2007) MC NA 
Orebaugh and Allnutt (2009) J4.8, IBk, NB NB 
Peng et al. (2003) NB NA 
Raghavan et al. (2010) PCFG, MaxEnt, NB 
MaxEnt with 
PCFG 
Rico-Sulayes (2011) DA NA 
Spassova (2008) DA NA 
Spassova (2009) DA NA 
Spassova and Turell (2007) DA NA 
 
  
 
119 
 
Table 3.7 (continued) 
 
Authorship Attribution Studies Classifiers Best results 
Stamatatos et al. (2001) DA NA 
Tambouratzis and Vassiliou (2007) DA NA 
Tearle et al. (2008) ANN NA 
Zheng et al. (2006) C4.5, NN, SVMs SVMs 
Zheng et al. (2003) C4.5, NN, SVMs SVMs 
      
Note. NA = does not apply 
Classifiers: ANN = artificial neural networks, BMR = Bayesian multiclass 
regression, C4.5 = decision-tree-based classifier, DA =  discriminant analysis, EG 
= exponentiated gradient, IBk = known nearest neighbor implementation , J4.8 = 
decision-tree-based classifier, MaxEnt = maximum entropy, MC = matching 
coefficient for real values, MEGAM = maximum entropy GA model, NB = naive 
Bayes, NN = backpropagation neural networks, NSC = nearest shrunken centroids, 
PCA = principal component analysis, PCFG = likelihood of probabilistic context-
free grammar, RMW = Winnow multiclass implementation, SVMs = support 
vector machines, Writeprints = Karhunen-Loeve-transforms-based technique 
 
 
As seen in Table 3.7, ten out of the 32 studies surveyed use various class attribution methods and 
compare their performance when applied in authorship attribution (Abbasi and Chen, 2005; 
2008; Burrows, 2007; Chaski, 2007; Jockers et al., 2008; Koppel et al., 2009; Orebaugh and 
Allnutt, 2009; Raghavan et al., 2010; Zheng et al., 2006; Zheng et al., 2003). The most 
frequently used methods are DA, employed in 10 studies, support vector machines (SVMs), in 9, 
C4.5 (or its Weka clone J4.8), in 5, and naïve Bayes (NB), in 4. All other methods are used only 
in one or two of the research papers surveyed in Table 3.7. A number of studies which compare 
different methods also include one or more of the four frequently used methods listed above, 
DA, SVMs, C4.5, and NB. Among these studies, three frequently used methods have been found 
to render the best results in a number of their experiments. SVMs have been reported to perform 
better than other frequently used methods (C4.5 or NB) in Abbasi and Chen (2005), Koppel et al. 
 
120 
 
(2009), Zheng et al. (2006), and Zheng et al. (2003). DA has been reported to perform better than 
SVMs in Chaski (2007). Finally, NB obtained a higher success rate when compared to C4.5 in 
Orebaugh and Allnutt (2009). Since none of the methods surveyed in Table 3.7 has performed 
consistently better than all of the rest (or even than the majority of them), this research project 
will use and compare all of the most commonly used quantitative methods that have rendered the 
best results in authorship attribution, namely DA, SVMs, and NB. In the studies surved in Table 
3.7, C4.5 has not been reported to have an overall performance that surpass the other three 
classifiers. However, this fact makes this class attribution method a perfect option to use as a 
performance baseline for the different approach configurations that will be explored in the 
experimental chapter of this project. Therefore, the final list of four methods that will be tested 
later in this project does not only include the most frequent ones, but it also comprises the three 
among the most frequent that have been reported to render the best results, i.e., SVMs, DA, and 
NB. All experiments for the four different methods selected to test in authorship attribution will 
be conducted and reported in Chapter 5. 
3.2.2  Quantitative Class Attribution Methods in Author Profiling 
The third column in Table 3.8 shows the quantitative methods used in the author profiling 
experiments that have been surveyed in former sections of this chapter. This table also includes 
the different profiling categories that have been targeted in these studies, because each category 
represents a separate experiment. A total of 8 different quantitative class attribution methods are 
used in the studies surveyed in Table 3.8. Once again, 2 of these methods are common statistical 
methods implemented in SPSS and Stata, among other statistical software packages. These 
methods are DA and ANOVA. The other 6 methods are machine learning classification 
algorithms (or variations of them) commonly used in the field of computer science. Similar to 
 
121 
 
what was done in last subsection, this one will not include a detailed presentation of these 
quantitative methods, applied in this case to author profiling. Most of these methods, some of 
which are listed in the last subsection, are also introduced in the formerly referred statistical 
pattern recognition manuals (McLachlan, 2004; Timm, 2002) and information retrieval textbooks 
(Manning et al., 2008; Witten et al., 2011). Only 2 methods in Table 3.8 are not common in the 
literature. A method resulting from a hybrid implementation of two other algorithms, 
exponentiated gradient and balanced Winnow, is introduced by the authors in Koppel et al. 
(2002). Similarly, the rather rare algorithm used in Schler et al. (2005), multi-class real Winnow, 
is presented in the research paper by the authors. 
  
 
122 
 
Table 3.8 Class attribution methods used in previous author profiling studies 
Table  
Author Profiling Studies Profiling 
categories Classifiers Best results 
Corney (2003) 
1) gender, 2) 
language 
nativeness 
SVMs NA 
Corney et al. (2002) gender SVMs NA 
de Vel et al. (2002) 
1) gender, 2) 
language 
nativeness 
SVMs NA 
Koppel et al. (2002) gender 
EG-BW 
hybrid, NB, 
Ripper 
EG-BW 
hybrid 
Koppel et al. (2009) 
1) gender, 2) 
age, 3) native 
language, 4) 
neuroticism 
level 
BMR NA 
Koppel et al. (2008) 
1) gender, 2) 
age, 3) native 
language 
BMR NA 
Koppel et al. (2005) native language SVMs NA 
Schler et al. (2005) 1) gender, 2) age MCRW NA 
Spassova (2009) 1) writer's time 
span, 2) dialect 
1) ANOVA, 
2)  DA NA 
Thomson and Murachver 
(2001) gender DA NA 
Zhang et al. (2009) gender SVMs NA 
        
Note. NDP = no details provided, NA = does not apply 
Classifiers: BMR = Bayesian multiclass regression, BW = balanced Winnow, DA 
=  discriminant analysis, EG = exponentiated gradient,  MCRW = Multi-Class 
Real Winnow, NB = naive Bayes, Ripper = decision-tree learner, SVMs = support 
vector machines 
 
 
 
123 
 
In contrast to the studies surveyed in authorship attribution, among which 10 out of 32 used 
various class attribution methods, only one research paper in Table 3.8 utilizes and compares the 
performance of several methods applied to author profiling (Koppel et al., 2002). As to the most 
frequently used methods in the task of author profiling, SVMs are employed in 5 studies, DA, in 
2, and Bayesian multiclass regression (BMR), in 2. All the other methods mentioned in Table 3.8 
are used only once. Interestingly, none of these frequently used methods is included in the only 
paper that compares different methods in author profiling (Koppel et al., 2002). This paper, 
however, includes one of the methods frequently used in authorship attribution, NB. The best 
success rate obtained in this paper is achieved through the use of the hybrid algorithm proposed 
by the authors of this study. Unfortunately, this paper does not include other widely used and 
highly successful algorithms in both authorship attribution and author profiling tasks, such as 
SVMs or DA.  
 With this brief survey of the classification methods in author profiling, it possible to see 
that three out of four methods that are either frequently used or compared in this task are the 
same as three out of four frequently used methods in authorship attribution (SVMs, DA, and 
NB). Also, the three methods that are common to the two tasks are also the three classification 
methods that have rendered the best results in authorship attribution experiments comparing 
various methods. Therefore, the selection of these three methods for the various experimental 
approaches that will be tested in this project, along with the other frequently used method of 
C4.5, are also supported by the survey of methods employed in author profiling. The inclusion of 
this last classification method should be useful as a baseline. As mentioned in the subsection 
above, all the methods selected for the experiments that will be conducted later will be devoted a 
more detailed introduction in the next chapter. 
 
124 
 
3.3 AUTHORSHIP ANALYSIS APPROACHES ON SPANISH DATA 
As discussed previously, the next chapter will make a detailed introduction of the experimental 
decisions and the approach elements (the feature selection and the class attribution methods) that 
will be applied to the data collected in the experimental corpora of this project. Regarding the 
data set that will be used, one very important characteristic needs discussion: the language it is 
in. As mentioned in the introduction, authorship analysis studies that target Spanish language 
data are scarce. The current study is largely motivated by the assumption that particular 
authorship attribution approaches may not be equally successful when applied to data in different 
languages. Another challenge posited by the application of an authorship analysis approach to 
data in a different language is that features may not be suitable for adaptation in this language. 
Given the challenges of applying an approach to a new language, the introduction of the data that 
will be used for the authorship attribution experiments in this project should begin by 
contextualizing this project among previous studies conducting authorship analysis tasks on 
Spanish data. Since this body of research is rather scarce, this section will briefly describe all 
authorship analysis studies that have used Spanish data. 
 In Chapter 2, I presented broad surveys of various relevant authorship attribution 
experimental conditions, such as topics, genres, and size of training and testing data sets. In 
addition, in this chapter I have presented surveys of the two main components of authorship 
analysis tasks, the feature sets and the classification methods that process these feature sets. So 
far, however, nothing has been said about the language or languages of the data sets used in the 
various studies. Filling this gap, Table 3.9 shows what languages are targeted in all of the 
authorship attribution studies that have been surveyed. 
  
 
125 
 
Table 3.9 Languages targeted in previous authorship attribution studies 
Table 15 
Authorship Attribution Studies Language(s) 
Abbasi and Chen (2005) English, Arabic 
Abbasi and Chen (2008) English 
Argamon et al. (2003) English 
Baayen et al. (2002) Dutch 
Burrows (2002; 2007) English 
Caver (2009) English 
Chaski (2001) English 
Chaski (2005) English 
Chaski (2007) English 
Corney (2003) English 
Gamon (2004) English 
Grant (2007) English 
Grant (2010) English 
Grieve (2007) English 
Hirst and Feiguina (2007) English 
Jockers et al. (2008) English 
Juola and Baayen (2005) Dutch 
Koppel et al. (2009) English 
Mikros and Argiri (2007) Modern Greek 
Nazar and Sánchez Pol (2007) Spanish 
Orebaugh and Allnutt (2009) English 
Peng et al. (2003) Modern Greek, English, Chinese 
Raghavan et al. (2010) English 
Rico-Sulayes (2011) Spanish 
Spassova (2008) Spanish 
Spassova (2009) Spanish 
Spassova and Turell (2007) Spanish 
Stamatatos et al. (2001) Modern Greek 
Tambouratzis and Vassiliou (2007) Modern Greek 
Tearle et al. (2008) English 
Zheng et al. (2003) English, Chinese 
Zheng et al. (2006) English, Chinese 
    
  
 
 
 
126 
 
In Table 3.9, two thirds of the studies surveyed (22 out 33) include data in English in their 
experimental corpora. Five languages other than English are also targeted in the studies 
surveyed. Arabic data is used in one study (Abbasi and Chen, 2005), Chinese data in three (Peng 
et al., 2003; Zheng et al., 2003; 2006), Dutch data in two (Baayen et al., 2002; Juola and Baayen, 
2005), Modern Greek in four (Mikros and Argiri, 2007; Peng et al., 2003; Stamatatos et al., 
2001; Tambouratzis and Vassiliou, 2007), and Spanish data in five (Nazar and Sánchez Pol, 
2007; Rico-Sulayes, 2011; Spassova, 2008; 2009; Spassova and Turell, 2007). It should be noted 
that only a third of the studies (11 out of 33) target data exclusively in languages other than 
English. It should also be mentioned that the number of studies that use Spanish data in the table 
above is probably larger than the number of studies targeting languages other non-English 
languages because of a bias of this study. In Table 3.9, I have intended to include all of the 
quantitative authorship analysis studies that employ data in Spanish. This is not the case for the 
studies that target other languages in this survey. Regardless of this bias, the number of studies 
that employ Spanish data is noticeably smaller than the number of studies that include data in 
English. The implications of the paucity of studies that use Spanish data are actually qualitative 
rather than quantitative. As it will be argued below, having just a few studies devoted to this 
language means that this body of research lacks the broad exploration of types of data source, 
feature sets, feature reduction techniques, and class attribution methods that have been tested in 
the literature devoted to authorship analysis in general. 
 The gaps in authorship analysis research that uses data in Spanish become apparent with 
a summary of the approaches and experiments conducted in the five studies targeting this 
language in Table 3.9 (Nazar and Sánchez Pol, 2007; Rico-Sulayes, 2011; Spassova, 2008; 2009; 
Spassova and Turell, 2007).  For example, we find that few choices in the two core components 
 
127 
 
of authorship analysis approaches (feature sets and classifiers) have been explored and compared 
using data from this language. Before summarizing briefly the five studies just mentioned, it 
should be emphasized that the studies reviewed below represent the totality of authorship 
attribution research that applies quantitative methods to Spanish data. Beyond these five papers, 
which apply quantitative approaches in authorship analysis, I have been able to find only one 
study that uses a qualitative approach in an authorship attribution task with Spanish data 
(McMenamin, 2002). This study will also be summarized at the end of this section. 
 In the first quantitative study using Spanish data in Table 3.9, Nazar and Sánchez (2006) 
take as features all bigrams (two word sequences) from a set of 100 newspaper articles in 
Spanish by five authors. The researchers feed bigram counts to a machine-learning algorithm that 
they develop for the purposes of the study and evaluate the significance of their results using a 
standard statistical procedure, chi-square. The algorithm uses purely mathematical information, 
so the researchers argue that it is language-independent and can also be used as a general text 
classification technique. Despite this argument, the researchers do not provide any information 
whether their technique has been applied to other languages or classification tasks. The 
researchers report a maximum accuracy of 92% in the assignment of anonymous texts to their 
true authors. This accuracy is achieved, as mentioned above, using a set of potential authors with 
5 subjects. 
 The second study listed above (Rico-Sulayes, 2011) employs a 25-item feature set that 
includes syntactically-classified punctuation and structural features, mostly media-specific. This 
study processes these features with a statistical classifier, DA. The study also tests the 
performance of two feature reduction techniques, Wilks’ Lambda and Mahalanobis. These two 
reduction techniques are built into SPSS, the software implementation of this classifier used by 
 
128 
 
the researcher. The described authorship attribution approach is applied to 40 randomly 
aggregated text samples by 10 users of an online forum post. In order to have a balanced corpus 
design, each author in the set of subjects is represented with 4 text samples with roughly the 
same size, around 500 words. A number of different accuracy measures are explained and given 
at the end of the experiment. These several measures are offered in an attempt to make the 
performance statistics comparable to the results reported in previous authorship attribution 
studies that use DA as their class attribution method. The best accuracy rate achieved in this 
paper is 93%, with 10 subjects in the set of potential authors. 
 Spassova and Turell (2007) use as features bigrams, trigrams and four-grams (two, three, 
and four element sequences) of previously tagged parts of speech (POS). Explained with a few 
examples in Chapter 3, these POS n-grams are sequences such as the POS bigram “determiner-
noun” or the POS trigram “determiner-adjective-noun”. Spassova and Turell select three well-
known writers and collect 15 fragments from novels and 15 newspaper articles from each writer. 
These scholars process all features applying the same statistical classification method employed 
in Rico-Sulayes 2011 (and many other quantitative authorship attribution studies), DA. The 
researchers report a maximum accuracy of 100% in the assignment of anonymous texts to their 
true authors, using a set of potential authors with 3 subjects. 
 In the fourth study that applies a quantitative approach to an authorship analysis problem 
with Spanish data, Spassova (2008) uses as features fourteen categories of periphrastic verbal 
constructions in Spanish, which have one of three possible patterns: verb + preposition + 
infinitive (e.g, ir a hacer ‘to be going to do’), verb + participle (e.g., haber hecho ‘to have 
done’), and verb + gerund (e.g., continuar haciendo ‘to keep doing’). The feature instances are 
counted in a sub-collection of five texts by the same three authors used in Spassova and Turell 
 
129 
 
(2007). The included texts have an average length of 3,000 words. The results in this paper are 
the same as in Spassova and Turell (2007), a maximum accuracy of 100% is achieved in the 
correct assignment of anonymous texts having 3 subjects in the set of potential authors. 
 The last study from Table 3.9 that will be summarized in this subsection is Spassova’s 
doctoral dissertation (2009). In addition to a detailed review of her previous authorship analysis 
work, also summarized above, she presents in her study a number of new experiments. She uses 
data from 17 authors, eight female and nine male, from five Spanish speaking countries. The five 
countries selected for the Latin American corpus (Argentina, Colombia, Chile, Peru, and 
Uruguay) do not belong to a single subregional dialect of the Spanish-speaking world, and the 
researcher does not offer any rationale for her selection. Spassova uses this data to build three 
corpora, a corpus consisting of fragments from novels (17 authors), a second corpus of 
newspaper articles (10 authors), and another corpus that is a sample of the first one (4 authors). 
In a number of authorship attribution tasks, she processes all POS bigrams and trigrams using 
DA in the three corpora. Both tasks also use Wilks’ Lambda as a feature reduction technique. 
Spassova uses the same methods as Spassova and Turell (2007), but adds both new data and new 
authorship analysis tasks, which will be commented on below. Spassova (2009) makes an in-
depth analysis of the performance of her authorship attribution approach in the classification of 
testing samples that are not cross-validated3. At the end of her experiments, Spassova includes 
                                                 
3
 Formerly explained in this project, a cross validation is a standard procedure in a classification task where the 
corpus is divided into training and testing samples. Training samples are used to create statistical models of the 
classes in the corpus, and testing samples are classified based on these models. In a cross-validation the corpus is 
divided in n pieces, and every time that an nth part of the corpus is classified (a nth split from the corpus 
constitutes a testing set) no documents in that part of the corpus (namely no testing samples) are included in the 
training data (Jurafsky and Martin, 2008). After performing n classifications the whole corpus has been classified. 
This process gives statistical meaning to the final result, which is obtained by adding all individual classifications. In 
the non-cross validated design of Spassova (2009), the researcher leaves out an nth part of the whole corpus for 
testing, but it only performs one classification. The rest of the corpus is never classified. Then the researcher 
reports the results obtained in this single nth part. These results, of course, have no statistical meaning. Statistical 
programs such as SPSS allow the researcher to classify testing sets one by one, so the researcher can design 
 
130 
 
cross-validated results for her corpus with 10 subjects. The researcher reports an accuracy rate of 
83% using POS bigrams and 89% using POS trigrams. No cross-validated results are given for 
the larger corpus with 17 subjects. 
Spassova (2009) also presents data for two real-life cases that she approaches as an 
author identification tasks. In these cases, she attempts to identify the author of disputed texts by 
comparing the disputed texts to texts by two subjects. In order to build her set of potential 
authors, one of the two subjects is a suspect in the legal case scenario from which the disputed 
texts have been taken, and the other one is an individual taken from a completely different case. 
This researcher does not justify the selection of the latter subject to build her set of potential 
authors, neither does she argue its appropriateness in an author identification task in which the 
actual author may not be part of this set. In the first designed experiment, 42% of the disputed 
texts are assigned to the individual considered as a suspect in the legal case. In the second case 
there is no cross-validated design. 
In yet another authorship analysis task, Spassova (2009) performs an author profiling task 
using 14 authors from her corpus with excerpts from novels. She attempts to profile the dialect of 
these authors dividing the Spanish language in two dialects, European and Latin American. 
Using 25 novel fragments per author, the researcher decides to represent peninsular Spanish with 
seven authors and her other variety, “Latin American Spanish”, with the other seven. These last 
seven authors are from 5 out of 19 Spanish-speaking Latin American countries. As previously 
discussed, the five countries selected (Colombia, Peru, Argentina, Uruguay, and Chile) do not 
                                                                                                                                                             
manually what information gets in each nth part of the classification. However, stopping the classification process 
after splitting only one nth part destroys the statistical construct of the cross-validation. As I have argued 
elsewhere (2011), we cannot generalize from non cross-validated results, rendering the Spassova study less than 
ideal. At best, the non-cross validated nth part is selected at random (this is done by Grant, 2007). Spassova does 
not do this. If the nth part classified has millions of events (texts in the case of authorship attribution), some value 
can be given to the results. When it has only 12 events (Spassova, 2009) or 3 events (Grant, 2007), even if these 
events are selected randomly, the results have no statistical generalizability (Rico-Sulayes, 2011). 
 
131 
 
represent a single regional Spanish dialect. In the division of Latin American regional dialects in 
the Corpus of the Royal Academy of the Spanish Language (Real Academia Española, 2012), the 
first two countries belong to the Andean dialect, the following two to the Plata River dialect, and 
the last one to the Chilean dialect. These three dialect do not constitute some regional dialect of 
its own. Besides, no authors from Mexican, Central American, or Caribbean dialects are 
included. Spassova does not justify her selections of authors. The researcher processes separately 
POS bigrams and POS trigrams in two different experiments that use DA as a classifier and 
Mahalanobis as a feature reduction technique. In the two experiments, the best cross-validated 
accuracy rate reported is 67%. 
 Finally, McMenamin (2002) uses a qualitative rather than quantitative approach to target 
Spanish data in an authorship analysis problem. Therefore, this study is not included in Table 3.9 
or any of the former surveys presented in this research project. However, I have decided to 
summarize here his work on Spanish data to ensure that all the authorship analysis research on 
this language will have been reviewed. McMenamin presents two author identification analyses, 
in which he attempts to identify the author of disputed texts (incriminating letters) in two legal 
cases. The researcher compares the disputed texts to texts by suspects in the legal cases. In the 
first case, there is a suspect who claims that he did not authored the disputed texts. For this case, 
McMenamin selects 24 features, mostly lexical idiosyncrasies or spelling mistakes. In one table, 
he summarizes all these features, which show similarities and differences between the disputed 
texts and texts both elicited and previously produced by the suspect. The summarizing table 
contains 5 similarities and 19 differences between the two sets of texts. The table includes a 
couple of examples for 22 out of the 24 selected features. Based on this comparative table, the 
researcher states his attribution opinion about this case, California v. Armas (1997, as cited in 
 
132 
 
MacMenamin, 2002). He concludes that the disputed texts are not authored by the suspect, which 
is the only subject in his open set of potential authors. 
 In his second author identification task, McMenamin provides a table summarizing the 
presence or absence of 40 features in both disputed texts and texts of known authorship by one 
subject. These features are shared between documents in English and Spanish, which were 
produced in a bilingual context. Only two out of the 40 features correspond to Spanish data, and 
are lexical features. 25 more features address English data. The remaining 13 features, which 
include lexical and structural features, are presented with general descriptions and have no 
examples, so they could belong to any of the two languages. McMenamin does not offer an 
attribution opinion about this second case. No accuracy or error rates are given for any of the two 
author identification tasks just summarized. 
 With the brief presentation of approaches and experimental decisions in the six 
authorship analysis studies just presented, three important gaps can be noted in the core elements 
of the authorship analysis approaches used in this body of research. First, the above summarized 
studies have selected as features either exhaustive (usually long) lists of various n-grams, namely 
of various element sequences (Nazar and Sánchez Pol, 2007; Spassova, 2009; Spassova and 
Turell, 2007), or shorter lists of lexical, syntactical, and structural elements (MacMenamin, 
2002; Rico-Sulayes, 2011; Spassova, 2008). There is not one single study employing Spanish 
data that uses both of these types of features, as we find in several authorship attribution studies 
in languages other than Spanish (Abbasi and Chen, 2008; Gamon, 2004; Grieve, 2007; Koppel et 
al., 2009), and in many of the author profiling studies surveyed in this chapter (Corney, 2003; 
Koppel et al., 2002; 2005; 2008; 2009; Zhang et al., 2009). A second gap that can be noticed 
among the quantitative authorship analysis studies presented above is that few class attribution 
 
133 
 
methods have been tested using Spanish data. With the exception of one study, in which a 
classifier developed by the researchers is implemented (Nazar and Sánchez Pol, 2007), all other 
studies use DA as their classification method (Rico-Sulayes, 2011; Spassova, 2008; 2009; 
Spassova and Turell, 2007). The third gap in the studies just summarized has to do with the 
feature reduction techniques employed, and it is probably related to the lack of experimentation 
with various class attribution methods, although it is not a necessary consequence of it. Only 
built-in, DA related feature reduction techniques have been used among the studies that use 
Spanish data. Rico-Sulayes (2011) compares two techniques, Wilks’ Lambda and Mahalanobis. 
Spassova (2009) uses the former technique in her authorship attribution experiments and the 
latter for author profiling. These feature reduction techniques are not present in any of the studies 
that use class attribution methods other than DA. Besides these three gaps in the authorship 
analysis approaches used in the body of research just presented, there is one more gap in the kind 
of data source that has been used in authorship analysis studies that target Spanish data. Only one 
study uses data from social media, or electronic communications in general, and that study has 
been the origin of this research project (Rico-Sulayes, 2011). 
 Given the above review of all the authorship analysis studies that use Spanish data, it is 
possible to further situate this research project not only in the general forensic linguistics area of 
authorship analysis, but in the specific body of literature within this area that has targeted data in 
this language. Setting this research project against this very specific background highlights some 
of the shortcomings in the studies that have used Spanish data. The four identified gaps in these 
studies regarding their approaches and type of data source can be seen as arguments for the 
importance of this project. As it has been mentioned before, the authorship approach 
configurations in this research project will contain features from all categories, including both 
 
134 
 
exhaustive, long n-gram lists and shorter, category-specific lists. The approach configurations in 
this research project will also use the most frequent feature reduction techniques (which are 
external and non-related to a specific implementation of a classifier) and the most frequent class 
attribution methods (which include three machine learning classifiers). Finally, this study will 
continue the only work that has been done in authorship analysis with electronic communications 
(specifically social media data) in Spanish. With the range of options that will be combined in 
the various approach configurations and the type of data that will be explored in this chapter, the 
experiments conducted in this project should noticeably expand the scope of authorship analysis 
using Spanish data. 
 The current chapter has begun focusing on the two core components of the authorship 
analysis approach, the selection of authorship features and the application of a method to process 
these features so that a class is assigned to an anonymous text sample. Establishing a parallel 
with the last chapter, in which a number of important experimental decisions were examined, this 
chapter has made a comprehensive review of these two core components in the literature. This 
review consisted of a comprehensive survey of recent studies targeting two authorship analysis 
tasks which share many methodological elements: authorship attribution and author profiling. 
The main goal of this review is to offer a rationale for the methodological decisions that will 
guide the experiments reported in Chapter 5. In the presentation of the feature selection 
component, the commonly employed procedure of feature reduction has also been introduced, 
and a survey of its different techniques in both tasks has been offered. This chapter has also 
made a review of the class attribution methods utilized in all the research studies surveyed in this 
and the former chapter. In this review of the class attribution methods, an emphasis has been 
made on identifying both the most commonly employed methods and the ones among these that 
 
135 
 
have been reported to render the highest success rates. This identification of the most common 
and most successful quantitative class attribution methods has been aimed at supporting the 
selection of methods that will be used and compared in the experimental chapter of this project. 
Finally, this chapter has surveyed of all authorship analysis research that has targeted data in 
Spanish. The ultimate goal of this last survey has been to identify a number of gaps that 
characterize this specific body of research. Since the experiments conducted later in this project 
will fill these gaps, this last section has offered a number of arguments for the importance of the 
current study. 
  
 
136 
 
 
CHAPTER 4: QUANTITATIVE AUTHORSHIP ATTRIBUTION OF ONLINE 
FORUM POSTS IN SPANISH 
This chapter will present the data and methods that will be applied in a number of authorship 
attribution experiments reported in the results section of this project. Following the definition of 
authorship attribution in Chapter 2, these experiments will attempt to assign an anonymous piece 
of text to a subject within a closed set of potential authors. These experiments will be conducted 
on data obtained from an online forum in Spanish. The data will consist of a number of 
samplings from the posts created in the first five months of existence of a drug-dealing related 
forum. Since it is not possible to easily attest the identities of online forum users, all data 
samplings, including the selection of online forum users, will be performed randomly. This will 
be done in an attempt to minimize the impact of the potential existence of usernames that do not 
correspond to one single author. The various experiments, reported in the next chapter, will test 
and compare a number of choices in the two core elements of any authorship attribution 
approach: the selection (and optional reduction) of the feature set and the class attribution 
method that processes this feature set to assign some anonymous text to a given author. The 
introduction of the various methodological choices tested later in the experimental part of this 
project will occupy most of this chapter. At the end of this chapter, the various approach 
configurations that will result from combining these methodological choices will be presented. 
 Beyond the fundamental components of a full authorship attribution approach, the feature 
set and the classification method, there are a number of experimental decisions influencing the 
results of any given approach. These experimental decisions will be made explicit in this chapter 
before introducing the various options that will be tested for the core components of the 
 
137 
 
approach. The experimental decisions that will be presented and discussed are: the topics and the 
type of data source included in the experimental corpus, the number of subjects in the set of 
potential authors, the amount of training data used in the construction of these authors’ statistical 
models, and the amount of testing data in each anonymous text sample. As to the number of 
subjects in the set of potential authors, this experimental condition will be used to compare the 
performance of each approach beyond one single experiment. Evaluating each approach along a 
wide range of values for this variable, the discussion about the results will also examine the 
consistency of each approach performance after testing it multiple times. Testing the 
performance of given authorship attribution methods over a wide range of values for a number of 
potential authors should also point to the limitations of authorship analysis methods in real-life 
case scenarios, where the number of suspected authors is often limited and of course not open to 
experimental manipulation. 
 As discussed above, an authorship attribution approach has two core components, the 
feature set and the classification method. Regarding the first core component of the authorship 
attribution approach, a number of features and feature categories that have been used in previous 
studies were presented in Chapter 3. Drawing on this presentation, the experiments in the current 
chapter will select features formerly tested in the literature, from all different categories 
discussed (lexical, syntactic, and structural). Besides including features previously used in 
authorship attribution, this study will also propose a number of novel features. The main goal of 
this proposal of novel features will be to enable a fully automated tagging of features instances, 
without decreasing the accuracy previously achieved by the author of this project with features 
tagged by hand (Rico-Sulayes, 2011). This accuracy rate, 93% for a set of potential authors with 
10 authors, represents the highest figure that has been obtained in an authorship attribution task 
 
138 
 
using DA as the class attribution method. Regarding the use of a feature reduction technique, the 
experiments reported in this chapter will compare the results obtained with the two most 
common techniques previously used by researchers in authorship attribution, frequency and IG 
(information gain). In addition to these two reduction techniques, this study will also explore the 
results rendered by a feature reduction algorithm that has not been used in previous authorship 
attribution studies, correlation-based feature subset (CFS) evaluation (Hall, 1999). The results 
obtained with these three techniques will be compared as they are applied in combination with 
various class attribution methods.  
 Regarding the second component of authorship attribution, I will apply and compare 
those that have been most commonly used, as revealed in the survey of authorship attribution 
studies in the previous chapter: DA (discriminant analysis), SVMs (support vector machines), 
C4.5, and NB (naive Bayes). Given the restrictions of space and the focus of this project, which 
does not include the proposal or evaluation of a new classification method in authorship analysis, 
the four class attribution methods formerly mentioned will be briefly introduced in this chapter. 
References for further reading about the construction and behavior of these four classifiers will 
also be provided.  
 In summary, this chapter will first present all the experimental decisions summarized 
above (topic, genre, number of potential authors, amount of training and testing data), along with 
the various options for the two core components of the approach (feature sets and classifiers). 
The performance of a number of approach configurations, which combine a given feature set 
with a given classification method, will be analyzed and discussed in the next chapter. 
 
139 
 
4.1 DATA FOR AUTHORSHIP ATTRIBUTION EXPERIMENTS 
In the Introduction, some of the potential applications of the type of research conducted in this 
project were discussed regarding the drug war in Mexico. As was noted in Section 1.2.1 (“Need 
for the Study”), this war has been characterized by an escalating death toll, which has increased 
from 2,800 casualties in 2007, to 9,500 in 2009, to 15,300 in 2010 (Córdoba and Luhnow, 2011), 
to nearly 13,000 during the first nine months of 2011 (Agar, 2012; Planas, 2012). Unofficial 
figures report 16,466 deaths for 2011 (WM Consulting, 2012). Another characteristic of this war, 
besides its steadily increasing death toll, has been the constant written communications that drug 
dealers have sent to the government and the general population using various media, including 
displaying narco mantas ‘narco banners’ or ‘canvas signs’ and narcopintas ‘narco graffiti’ 
displayed on streets or trucks patrolling small towns, narcomensajes ‘narco messages’ in signs 
taped to dead bodies or dismembered body parts, and posting comments on blogs and online 
forums (Blog del Narco, 2012; Foros Blog del Narco, 2010). Given this scenario, the potential 
application of state of the art quantitative techniques in authorship analysis to this type of 
incriminating written evidence, specifically to electronic communications, has been one of the 
motivations of this study. 
 In Chapter 2, a number of authorship attribution studies targeting data from social media 
were surveyed among the studies devoted to this authorship analysis task (Abbasi and Chen, 
2005; 2008; Argamon et al., 2003; Koppel et al., 2009; Rico-Sulayes, 2011; Zheng et al., 2003: 
2006). However, the social media that has sprung up around the specific topic of drug-trafficking 
in Mexico is unexplored in these studies, at least beyond previous work by the author of this 
project (Rico-Sulayes, 2011). For this reason, the authorship attribution experiments conducted 
in this research project will target data gathered from a drug-trafficking related online forum in 
 
140 
 
Mexico. This project will test the performance that can be achieved in authorship attribution 
tasks with this kind of data when applying state of the art quantitative approaches and a fully 
automated feature tagging process, unlike my previous work with this data. 
4.1.1  The Drug-Dealing Related Social Media Phenomenon in Mexico 
As I have commented elsewhere (2011), drug-dealing related blogs became popular in Mexico in 
2010. For example, in September 2010, I found that many such blogs could be found by 
performing a simple Google search with the words blog, narco ‘drug dealer’ / ‘drug dealing’ or 
narcotráfico ‘drug dealing’, and México. Visiting these blogs at that time, it was possible to see 
that the majority of them had only a few dozen usernames registered and most of these 
usernames had not been used to post any comments. The proliferation of these blogs was 
probably prompted by the success of the arguably most popular drug-dealing related blog at the 
time: Blog del Narco, which was created on March 2, 2010 and promptly captured the attention 
of international media (e.g., CNN, August 18, 2010). Just a few months after its creation, Blog 
del Narco had contributions by alleged common citizens, relatives of the kidnapped and 
murdered, policemen, soldiers, and drug dealers. As I predicted in September 2010, many of the 
drug-trafficking related blogs I found back then have either vanished or have been abandoned by 
their creators and ephemeral users. 
 However, drug-dealing related social media phenomenon did not die out. New blogs, 
online forums, and chat sites on the topic have also become popular in the last two years. For 
example, on September 18, 2012, the blog NuevoLaredoEnVivo (Nuevo Laredo Live) reported 
having received 3,510,229 comments in its chat room during the 749 days it had been active. 
These figures apply to a blog that is devoted mostly to news in the northeastern region of 
Mexico, which has experienced a spike in drug-dealing related violence since last year (Cedillo, 
 
141 
 
2011). This blog is mostly intended for its users to share information on drug-dealing related 
events and it includes contact information in its home page to call the military police. 
 At the same time, other important social media on the topic have disappeared, although 
not necessarily because of their users’ lack of interest. This is the case of the online forum 
Frontera al Rojo Vivo (White-Hot Border) (2012), which was created and maintained by El 
Norte (The North), a newspaper belonging to the nationwide print media company Grupo 
Reforma (elnorte.com, 2012). El Norte decided to close their online forum and delete all the 
posts created in it after “abuses” and “personal attacks” on the forum users took place (Frontera 
al Rojo Vivo, 2012). The vague comments in what remains of the deleted forum are probably 
related to the killing of two social media users in the city of Nuevo Laredo. One of the victims, a 
woman whose body was hung from a pedestrian overpass, was tortured and killed because of her 
posts in NuevoLaredoEnVivo, as a placard left by the murderers made explicit (Borderland Beat, 
2011; Goodman, 2011; Stevenson, 2011). The other victim was a man, and his dead body was 
also hung from an overpass. He was killed “for snitching in Frontera al Rojo Vivo,”  as could be 
read in a note attached to his leg (Goodman, 2011). A banner left in the overpass next to the 
man’s body also threatened “all the Internet snitches” in Blog del Narco and Denuncia 
Ciudadana, a website sponsored by the Mexican Federal Police for citizens to make anonymous 
reports and give anonymous tips (Procuraduría General de la República, 2010). These events are 
probably what prompted the newspaper El Norte to close their online forum. The cited killings of 
the two online users, which are just part of a series of continuous threats against online media 
and anonymous tipsters by the drug cartels (Borderland Beat, 2011), have made explicit the risks 
and challenges of creating and maintaining a drug-dealing related social media site in Mexico. 
 
142 
 
  Despite these events, Blog del Narco (2012), the most important social media related to 
drug-dealing in Mexico for some (Borderland Beat, 2011), remains active. As I have mentioned 
elsewhere (2011), the popularity of Blog del Narco and its forum may be attributed to the content 
of their uncensored videos and photos and the nature of their contributions. Browsing some of 
the discussions in both the blog and its forum, Foros Blog del Narco (2010), it is possible to find 
contributions by alleged common citizens, relatives of the kidnapped and murdered, policemen, 
soldiers, and drug dealers. In one of the most dangerous countries in which to work for the news 
industry (Cevallos, 2007; International Freedom of Expression Exchange, 2011; Lauría and 
O’Connor, 2010), the blog has probably survived because the administrator, a young Mexican 
student of computer science (CNN, August 18, 2010), has managed to protect himself behind a 
firewall. In this chapter, a number of authorship attribution approaches will be tested on data 
obtained from the online forum associated with Blog del Narco, Foros Blog del Narco, which 
was created just one month after its parent site on April 7, 2010. 
4.1.2  Experimental Corpora 
Five months after becoming active, on September 12, 2010, Foros Blog del Narco reported 
having 41,751 posts in 4,205 topic threads. On that same date, I retrieved 37,649 posts from the 
forum, with the somewhat larger number probably being due to deleted contributions and deleted 
duplicate submissions not updated in the online forum counter. On October 31, the forum 
statistics had risen to 61,304 posts in 6,624 threads. All of the above mentioned statistics belong 
to the online forum only. The main blog site, Blog del Narco, has its own contributions, but 
individual statistics are not provided for this other site. Among the posts harvested from the 
online forum, 37,571 were created by signed authors, i.e. not by anonymous or guest forum 
users. These posts belonged to 1,026 different usernames and contained a total of 2,128,049 
 
143 
 
word tokens. It should be mentioned that the spidering of the forum, i.e., the automatic fetching 
of its Web pages, was performed using the free software package GNU Wget (GNU Operating 
System, 2012), a simple application that can retrieve a potentially infinite number of WebPages 
executing simple, one-line scripts.  
 In November 2010, the forum was migrated to a new domain name or URL 
(forosdemexico.com), and most of the original posts created before the migration were lost, as 
the forum administrator acknowledged in a post with the title “Información acerca de 
modificaciones en el Foro” (information about modifications to the Forum) on November 17, 
2010 (Foros de México). Later in 2010, after this domain migration, the name of the forum was 
also changed to Foros de México, but this forum continued to be associated as the online forum 
of Blog del Narco (2012). The oldest posts in the new domain dated back to November 14, 2010. 
This means that most of the posts created between September 12, 2010 and the later date were 
lost. Based on the statistics shown in the forum between September 12 and October 31, there are 
at least 19,553 new posts and 2,419 new topic threads that were not retrieved by the author of 
this project and were lost after the migration. Since the oldest posts in the new domain are from 
November 14, the amount of information lost during the migration is probably greater.  
 In order to monitor the forum’s continuous growth, the new domain name, Foros de 
México, was harvested in July 2011. As mentioned before, differences between reported 
statistics and posts retrieved can be attributed to deleted contributions and threads or to duplicate 
submissions. During the first two weeks of July 2011, 129,987 posts were retrieved from the 
forum. These posts included 119,691 posts created by online forums users and 10,296 empty 
posts (probably deleted submissions). This second retrieval of posts did not include the posts 
copied in the first spidering of the forum, which was performed in 2010. The number of available 
 
144 
 
individual posts retrieved in July 2011 (129,987) had more than doubled by February 2012 (close 
to 300,000), but the latter set of posts was not harvested. 
 Another important characteristic that should be mentioned about the online forum 
associated with Blog del Narco is that it functions intermittently. Namely, it goes inactive from 
time to time. In September 2012, Foros de México had been inactive for six months and it is 
unclear if it will ever be back online. After this long inactivity period, the users of other drug-
dealing related online forums have started to comment about the possibility that Foros de México 
has disappeared indefinitely (El Foro del Narco, 2012). Given the risks of making available a 
social media site like this, the sporadicity of Foros de México online forum is not completely 
unexpected. 
 The experiments conducted in this chapter will use the posts harvested from Foro Blog 
del Narco in September 2010. This data probably represents the beginning of the drug-dealing 
related social media phenomenon in Mexico, and it is no longer available online. Foro Blog del 
Narco, the first online forum associated with Blog del Narco, had a narrower scope topic-wise 
when compared to Foros de México (2012). This is especially important because, as was noted in 
Chapter 2, the inclusion of different topics in the data has been shown to have an impact in the 
performance of the authorship attribution approach (Corney, 2003; Mikros and Argiri, 2007; 
Tambouratzis and Vassiliou, 2007). Although there is no guarantee that all the posts in a single-
topic oriented forum belong exclusively (or even mostly) to the given topic, browsing both the 
threads and posts in Foro Blog del Narco it is possible to see that most of them are related to the 
topic of drug trafficking. On the contrary, Foros de México attempted to become a 
topic/community-organized forum that included several topic tags (or communities), such as 
“sports” and “dating”, among others (2012).  
 
145 
 
 As mentioned above, in the data first spidered from Foros Blog del Narco, 37,571 posts 
(with 2,128,049 word tokens) were created by 1,026 signed authors. Among these authors, I 
selected those that had at least both 40 posts and a minimum of 2,000 words of original text. 
These two selection criteria for the training data of each author render a list of 106 online forum 
users. The amount of data considered as the selection criterion for the training data from each 
author (namely, 2,000 words) is in the low end of what has been previously used in authorship 
attribution experiments. For example, other researchers have used 2,000, 8,000, 15,000, 33,000, 
40,000, and 55,000 words as training data for each author (Chaski, 2005; Baayen et al., 2002; 
Spassova, 2009; Stamatatos et al., 2001; Burrows, 2002; Koppel et al., 2009; respectively). The 
other  selection criterion for the inclusion of subjects in the set of potential authors (having at 
least 40 posts) was used in order to avoid selecting sporadic posters with a few long messages. 
For the experiments reported in the next chapter, I randomly selected subjects from the list of 
106 users. Then, I included them in the experiments after I read their also randomly selected 
posts. I did this to ensure that the selected posts contained at least 2,000 words of original text 
production. All non-original text, such as information copied from news sources, was discarded. 
Reading the data before tagging it with an automatic system was an invaluable process in this 
project for the proposal of novel features. As will be discussed later, the novel features I chose 
based on my reading were proven to have discriminatory potential, by being selected in lists of 
features produced with feature reduction techniques. 
 Selecting 40 users who had 2,000 words of original text, I built a number of corpora. In 
all, I created 39 corpora, with the number of authors increasing progressively from 2 authors in 
the smallest corpus to 40 subjects in the largest one. The corpus with the largest set of potential 
authors is close to the high end of what others have used in their set of subjects for authorship 
 
146 
 
attribution tasks. As was mentioned before, unlike the amount of information included in training 
sets and testing samples, the number of authors in authorship attribution is always mentioned by 
researchers. In a thorough description of what other researchers have used, the 32 authorship 
attribution studies surveyed in former chapters used a set of potential authors with the following 
maximum of subjects: two (Hirst and Feiguina, 2007; Mikros and Argiri, 2007), three (Grant, 
2007; Gamon, 2004, Spassova, 2008; Spassova and Turell, 2007; Tearle et al., 2003), four 
(Chaski, 2001; Orebaugh and Allnutt, 2009), five (Abbasi and Chen, 2005; Corney, 2003; Nazar 
and Sánchez Pol, 2007; Tambouratzis and Vassiliou, 2007), six (Raghavan et al., 2010), seven 
(Jockers et al., 2008), eight (Baayen et al., 2002; Juola and Baayen, 2005), nine (Zheng et al., 
2003), ten (Chaski, 2005; 2007; Peng et al., 2003; Stamatatos et al., 2001; Rico-Sulayes, 2011), 
15 (Caver, 2009), 17 (Spassova, 2009), 20 (Argamon et al., 2003; Koppel et al., 2009; Zheng et 
al., 2006), 25 (Burrows, 2002; 2007), 40 (Grieve, 2007), and 100 (Abbasi and Chen, 2008). In 
this list of the maximum number of subjects used in authorship attribution studies, this research 
project can be placed, along with Grieve’s, right before the study with the largest set of potential 
authors. 
 For all 39 corpora, 4 aggregated text samples of approximately 500 words have been 
selected randomly from all the posts created by each author. Each of these 500-word samples is 
used as a testing data unit. In the cross-validation4 that will be used here, the training data for a 
given testing sample should include the rest of the documents from the corresponding author, 
around 1,500 words in three text samples. The size range of testing samples, 478-541 words, was 
also at the low end of what has been used by others. For example, a number of studies have used 
                                                 
4
 As explained in Chapter 3, in a cross-validation a corpus is divided in n parts. Every time that an nth part of the 
corpus is classified, no documents in that part of the corpus are included in the training data (Jurafsky and Martin, 
2008). After performing n classifications the whole corpus has been classified without ever using testing data in the 
training data set. 
 
147 
 
99-608, 100-1000, 500-2000, 600, 628-1342, 7,500+ words in their testing text samples (Chaski, 
2005; Corney, 2003; Stamatatos et al., 2001; Burrows, 2002; Spassova, 2009; Baayen et al., 
2002). The variation of size in testing samples in this project depended on the word length of 
posts; i.e., no post was divided to distribute between two samples. The average testing sample 
size was 515 words. Since each author has 4 testing samples, every authorship attribution 
configuration tested performs between 8 attribution tasks (for 2 authors with 4 samples each in 
the smallest corpus) and 160 attribution tasks (for 40 authors with 4 samples each in the largest 
corpus). Table 4.1 below presents the number of words in the smallest and the largest testing 
sample for each of the 40 authors used in all 39 corpora. This table also shows the total number 
of words for each individual author. 
  
 
148 
 
Table 4.1 Number of words for largest and smallest testing sample and total words per author 
Table   
Author # Largest 
sample 
Smallest 
sample Total words 
1 488 536 2040 
2 495 535 2056 
3 508 524 2061 
4 499 514 2018 
5 490 521 2014 
6 523 541 2122 
7 499 533 2039 
8 495 528 2040 
9 493 524 2023 
10 480 533 2068 
11 510 517 2051 
12 502 536 2056 
13 500 526 2039 
14 478 533 2045 
15 501 539 2055 
16 511 531 2089 
17 495 534 2060 
18 509 530 2081 
19 506 529 2071 
20 518 534 2114 
21 499 515 2026 
22 496 513 2024 
23 497 530 2038 
24 494 535 2065 
25 494 520 2035 
26 486 511 1997 
27 502 524 2044 
28 502 519 2045 
29 498 519 2035 
30 496 526 2040 
31 500 533 2089 
32 513 527 2075 
33 501 535 2072 
34 515 534 2098 
35 507 528 2072 
36 514 532 2101 
37 501 533 2065 
38 493 523 2044 
39 503 511 2028 
40 520 532 2108 
        
 
 
 
 
149 
 
As can be seen in Table 4.1, the smallest text sample in terms of number of words belongs to 
author 14, with 478 words. Author number 6 has the sample with the largest number of words, a 
total of 541. Given these numbers, the variation in the number of words for individual text 
samples in all 39 corpora reaches a maximum ratio of 1 to 1.1.  As to the total number of words 
per author, the difference between any two authors renders a smaller ratio. The author with the 
most words is author 6, with 2122 words. The author with the least words is author 26, which has 
1997 words. The difference in the total number of words between these two authors renders a 
ratio of 1 to 1.06. 
 In this section, five experimental conditions have been described regarding the data that 
will be used in this chapter: its genre, its topic, the number of subjects in the set of potential 
authors for each corpus, and the amount of training and testing data for each author. As argued in 
Chapter 2, these experimental conditions are relevant to the success rate of an authorship 
attribution approach. Two of these conditions have been described in a general way: the genre 
that the data collected belongs to (a social media technology, specifically an online forum) and 
the topic that prompted the creation of this forum, along with the appearance of a number of 
other similar social media sites (drug dealing). The three other experimental conditions have 
been described with specific numeric details: the number of subjects in the set of potential 
authors (which resulted in the creation of 39 progressively larger corpora), the amount of training 
data used for each authors’ statistical model (when a text from a given author is selected as a 
testing sample, the rest of the authors’ production should be in the training data set: 1,500 words 
in three text samples), and the amount of data included in each testing sample (around 500 
words). As discussed before, these experimental conditions are influential in the results obtained 
by an authorship attribution approach. Therefore, the description of these conditions is important 
 
150 
 
for the interpretation of the results obtained and essential for the comparison of these results 
against the results obtained by researchers that conduct the same authorship analysis task. 
4.3 AUTHORSHIP ATTRIBUTION APPROACH COMPONENTS 
If the data set to which an authorship attribution approach is applied remains the same, the 
success rate in the attribution of anonymous text samples to their true authors depends on the two 
fundamental components of the approach: the feature set and the class attribution method. In 
order for the data set to remain the same, it is important that the experimental conditions 
discussed in the last section do not vary. The variation of some of these conditions (genre, topic, 
and number of subjects in the closed set of authors) results in an apparent modification of the 
corpus upon which the task will be performed. Especially with the first two conditions just listed, 
genre and topic, a change in one or both could not only produce an entirely different 
experimental corpus, but would require a different data collection process. For example, if text 
messages were to be introduced in the experiments, a different technique to collect this type of 
text would be necessary -- spidering software is not designed to fetch text messages. Also, 
changing the number of subjects in the set of potential authors should noticeably change the size 
of the corpus if the size of the training and testing sets for each author is the same. The other two 
experimental conditions discussed in the last chapter (the amount of information in the training 
data set and the size of the testing samples) may result in more subtle modifications of the 
corpus, which still have consequences in the accuracy of the approach. Because of their potential 
influence on the results of the task, these five data variables will remain constant in the 
experiments described below, with the exception of the number of subjects in the set of authors 
which is explicitly intended to produce several corpora, and individual experiments for each of 
them. The control of these five variables should also allow us to interpret the different accuracy 
 
151 
 
results reported in the next section as the consequence of using the various feature sets and class 
attribution methods that are described below. 
4.3.1  Set of Authorship Features 
In Chapter 3, a distinction was made between two stages in the production of a feature set, the 
feature selection and the feature reduction. In the feature selection process, the first of these two 
stages, researchers choose authorship features based on their intuitions, which may be guided by 
previous research and their observation of the data. Besides the possibility of including 
previously tested features, researchers may also make a proposal of novel features during the 
feature selection process. In contrast to feature selection, which is a necessary step, feature 
reduction is optional. As formerly defined in this project, feature reduction consists of any 
statistics-based, often automated, process that attempts to further select a subset of the features 
chosen by researchers in order to improve the success rate of an authorship analysis approach. 
 Regarding feature selection (the first, researcher-driven process), there is not a unique 
answer to the question about what the best method to select features for an authorship attribution 
task is. In Chapter 3, a number of surveys showed that there is a great variation in both the type 
of features (or feature categories) and the number of features included by researchers in their 
experiments. Regarding feature categories, the last chapter discussed how a number of studies 
have offered a number of such categories and have tested their efficiency when used separately 
to perform an authorship attribution task. Although the categories of features that have been 
tested in these experiments do not always include the same features, and may even assign some 
features to different categories, an important conclusion can be reached by examining the studies 
that test different features sets.  As discussed in Section 3.1.1, eleven out 14 studies that compare 
results obtained using feature categories separately report that they achieve their highest success 
 
152 
 
rate when they feed all their features to their class attribution method (Abbasi and Chen, 2005; 
2008; Corney, 2003; Gamon, 2004; Grieve, 2007; Hirst and Feiguina, 2007; Orebaugh and 
Allnutt, 2009; Raghavan et al., 2010; Rico-Sulayes, 2011; Zheng et al., 2003; 2006). The rest of 
the studies do not support directly these findings, but they do not contradict them either. In the 
remaining three studies the researchers do not use a combination of all their features in their 
experiments (Juola and Baayen, 2005; Koppel et al., 2009; Stamatatos et al., 2001). Therefore, it 
is not possible to know if these studies would have reached the same general conclusion found in 
the other studies, had they used all their features together. 
 With regard to the number of features selected by researchers, there is a great variation 
regarding how many features are used in different studies. Section 3.1.1 surveyed 32 authorship 
attribution studies and their feature selection. In that survey, eleven studies use less than 100 
features (Baayen et al., 2002; Chaski, 2001; 2005; Grant, 2007; Grieve, 2007; Mikros and Argiri, 
2007; Orebaugh and Allnutt, 2009; Rico-Sulayes, 2011; Spassova, 2008; Stamatatos et al., 2001; 
Tearle et al., 2008), eight studies use hundreds of features, but less than a thousand (Abbasi and 
Chen, 2005; Argamon et al., 2003; Burrows, 2002; Corney, 2003; Jockers et al., 2008; Spassova, 
2009; Zheng et al., 2006; Zheng et al., 2003), and three studies use thousands of features (Abbasi 
and Chen, 2008; Gamon, 2004; Koppel et al., 2009). Beyond this large variation in the size of the 
feature set used by researchers, ten studies do not even mention the number of features they use 
(Burrows, 2007; Caver, 2009; Chaski, 2007; Hirst and Feiguina, 2007; Juola and Baayen, 2005; 
Nazar and Sánchez Pol, 2007; Peng et al., 2003; Raghavan et al., 2010; Spassova and Turell, 
2007; Tambouratzis and Vassiliou, 2007). Although these numbers tell us very little about the 
origin in the size difference of the feature set, a closer look at the studies that use thousands of 
features allow us to see that they all use different n-gram lists. Abbasi and Chen (2008) include 
 
153 
 
in their feature set various character n-grams. Gamon (2004) employs POS trigrams, and Koppel 
et al. (2009) use POS n-grams with several lengths. In the studies with the two largest lists of 
features below one thousand elements (Corney, 2003; Jockers et al., 2008), the authors also 
include n-gram lists. Corney (2003) uses over 853 features and includes character n-grams. 
Jockers et al. (2008) employ 521 features, which consist of a list of word unigrams, i.e., single 
word forms, also called types. In contrast, the use of n-grams is rather absent in the studies that 
report shorter lists of features. Only Grieve 2007, who uses character n-grams, has a short list 
under 100 features. Spassova (2009) uses 146 features, which consist of POS bigrams and 
trigrams, but this number of features results from adding together two already reduced lists, 
which means that her original list of POS n-grams should be longer. Finally, it should also be 
noted that there are several studies surveyed that use n-grams but do not report the length of their 
feature lists (Hirst and Feiguina, 2007; Nazar and Sánchez Pol, 2007; Peng et al., 2003; 
Raghavan et al., 2010). 
 As discussed in Section 3.1, n-gram lists can include lists of different element sequences. 
These sequences may be made up of characters, words, and POS tags, among other elements. In 
an example formerly used, it was explained that the word “characters” contains the following 
character bi-grams, or two character sequences: “ch”, “ha”, “ar”, “ra”, “ac”, “ct”, “te”, “er”, and 
“rs”. N-gram lists can also contain unigrams or single elements. In the word “characters”, there 
are 7 character unigrams: “c”, “h”, “a”, “r”, “t”, “e”, and “s”. In the case of word unigrams, for 
example, their list is equivalent to all types, or distinctive word forms in a text. Since n-gram lists 
are exhaustive (they include direct representations of all the data in the text), they usually result 
in very long lists. In section 3.1.1, it was mentioned that long lists of features may include a great 
number of non-discriminatory or redundant features. It was also explained how this abundance of 
 
154 
 
features may make certain features lose their discriminatory power, introducing noise. However, 
the use of n-grams is not prohibitive in classification tasks because, as was mentioned in that 
section, noise can be dealt with in a number of ways, including feature reduction techniques. In 
addition to the use of these techniques, a few machine learning classifiers are especially good at 
handling the noise introduced by long lists of features (Manning et al., 2008). In addition to the 
fact that their challenges can be dealt with in various ways, n-gram lists have been shown to have 
useful properties in a wide range of computational linguistics tasks (Jurafsky and Martin, 2008). 
 Despite the differences in both the various features categories tested and the size of the 
feature sets used by researchers in authorship attribution, two general observations can be made 
after examining the surveys related to these two aspects of feature selection. First, regarding 
feature categories, the studies that use several categories and compare their performance 
separately have shown that the best results are obtained when using all feature categories 
together. Regarding the number of features, one of the main reasons for the wide variation in 
authorship attribution studies is the use or non-use of n-gram lists, since studies with long lists of 
features include n-gram lists, while those with short feature lists typically do not. Although the 
question of what is the best method for feature selection for an authorship attribution task does 
not have a unique answer, the two tendencies identified above regarding feature categories and 
number of features will guide the feature selection in this research project. Regarding feature 
categories, the feature selection in this chapter will include features from all categories that have 
been presented above and will combine them all. Previously discussed in Section 3.1.1, all 
studies that test the performance of feature subsets from predefined categories have not obtained 
any improvement by dividing the feature set in categories. As to the number of features that will 
be used, this chapter will include long, exhaustive list of n-grams (specifically word n-grams) as 
 
155 
 
well as other shorter, category-specific lists, such as media specific uses of punctuation. This last 
decision should fill the gap found in the survey of authorship analysis studies that have targeted 
Spanish data. As mentioned in Section 3.3, among all of the studies that use data in this 
language, none used a combination of exhaustive, long, n-gram lists with other shorter, category-
specific lists of features. All of the features selected in this project will be further evaluated with 
feature reduction techniques in an attempt to both handle noisy features and identify the features 
with the most discriminatory power. This last application of feature reduction techniques, 
identifying highly discriminatory features, will be aimed at supporting an analysis of the kind of 
linguistic information that is useful in the most successful experiments reported. 
4.3.1a Feature Selection 
This project will include features from all categories presented in Chapter 3: lexical, syntactic, 
and structural. As mentioned in Section 3.1, the use of these three category labels is intended to 
give some organization to the presentation of features made in this project. Section 3.1.1 has 
shown that all the studies that have tested the performance of separate, category-based feature 
subsets have always obtained their best results when combining all their features together. 
Therefore, in the experimental chapter of this project all features will be combined together and 
their categorization in this section has only presentational purposes.  
 Regarding the lexical category, a list of word unigrams will be extracted from each 
experimental corpus and their units will be used as features. As it was mentioned before, a list of 
word unigrams is equivalent to all types, or distinctive word forms in the corpus (e.g., some of 
the most common unigrams in the corpus with 40 authors are de ‘of’, que ‘that’, and y ‘and’). 
Since this list is data-driven and there are 39 different corpora, the size of the unigram list grows 
from 1,393 types in the smallest corpus, with only two authors, to 13,089 in the largest one, with 
 
156 
 
40 subjects. As to the elements that have been considered word unigrams, they are any 
distinctive alphanumeric sequences of characters separated by spaces. These sequences of 
characters separated by spaces went through some basic preprocessing or cleaning, to keep only 
the individual words or tokens. This process of cleaning the original text in order to keep only its 
tokens is also known as tokenization. A standard and simple tokenization has been used in this 
process (Manning et al., 2008), removing word boundary punctuation and changing upper case 
alphabetical characters to lower case -- it should be mentioned that punctuation was removed 
from word tokens, but it was not eliminated altogether. As discussed below, punctuation items 
were also used as separate features. 
 As to the use of word unigrams in authorship attribution, a number of studies have 
successfully employed lists of unigrams in this task (e.g., Burrows, 2002; 2007; Caver, 2009; 
Gamon, 2004; Grieve, 2007; Jockers et al., 2008; Koppel et al., 2009; Tambouratzis and 
Vassiliou, 2007). In some of these studies, researchers continue processing the list of unigrams 
obtained after tokenization and keep just part of it in the lexical category. For example, a number 
of researchers split their list of unigrams into function words and content words and classify only 
the latter as lexical features (e.g., Koppel et al., 2009). Since in this project all features are fed 
together to either a feature reduction technique or a class attribution method, I have not 
performed any division of the unigram list, such as the one just described. Other researchers also 
reduce the original list of unigrams, or distinctive word forms, by obtaining their lemmas, i.e., 
the dictionary-entry word forms (Tambouratzis and Vassiliou, 2007). No reduction of the list of 
unigrams, other than the lower casing of upper case characters described above, has been 
performed in this project. 
 
157 
 
 Regarding the employment of all unigrams (or any other word n-grams) in authorship 
attribution, it should be mentioned that the use of these features, especially content words, poses 
the risk of relying on topic-related information. The issue of using topic-related information in an 
authorship attribution task is that the classification could depend on the topic the authors write 
about, and not on the differences of the authors’ styles, i.e., their particular way of writing. This 
problem becomes apparent when the data collected includes several topics or genres. All of the 
formerly cited studies that use lists of unigrams (Burrows, 2002; 2007; Caver, 2009; Gamon, 
2004; Grieve, 2007; Jockers et al., 2008; Koppel et al., 2009; Tambouratzis and Vassiliou, 2007) 
do control for genre, but they all have multi-topic corpora. Among these researchers, Grieve 
(2007) acknowledges the problem of including multiple topics in an authorship attribution task, 
and he argues that during the five-year span from which he collected data, the opinion columnists 
of the newspaper he selected may have written about the same topics. He does not say, however, 
if anything was done to confirm that the columnists actually wrote about the same topics, and 
what these topics were. Koppel et al. (2009) notice a potential problem in the use of word 
unigrams, which include content words, in authorship attribution. These researchers recognize 
the problem of using content words but do not mention doing anything to counteract or for 
control it. Finally, Gamon (2007) acknowledges the problem of using content-dependent 
information among his n-grams and tries to implement some solution. In his corpus of five 
novels by the Brontë sisters, he attempts to make these texts less content-dependent by 
substituting proper names and singular personal pronouns for generic labels (the former are 
substituted for the label “NAME” and the latter for the label “Perspro”). Although it is unlikely 
that these two actions make the novels in his corpus completely content independent, this study 
is the only one that acknowledges the problem of using unigrams in authorship attribution and 
 
158 
 
tries to do something about it. Without having a perfect solution, in this project I have tried to 
control for both topic and genre by choosing a single-topic oriented online forum devoted to drug 
dealing. Formerly, I have acknowledged that there is no guarantee that all the posts in this kind 
of single-topic oriented online forums belong to only one topic. For this reason, I have also read 
all posts included in the corpora of this study. Reading all posts, I could confirm that they all 
refer to or were prompted by comments referring to the forum’s central topic, which is drug 
dealing. Since this was the case for all posts read, none of the posts were eliminated based on 
their topic orientation. 
 Also included in the lexical category, punctuation marks will be used as features in this 
project (some of the most common punctuation marks in the largest corpus are “,”, “.”, “!”, and 
“?”). Represented by words and their parts according to their definition in Chapter 3, lexical 
features comprise any word and punctuation characters. Regarding the category of punctuation, it 
should be mentioned that punctuation marks, as well as some other linguistic features, can be 
included in any feature category. Previously explained in Section 3.1, depending on what criteria 
are used to select punctuation marks, they can be considered lexical, syntactic, or structural 
features. Given this possibility, this research project will include punctuation features in two 
different categories, lexical and structural. As lexical features, all punctuation marks will be 
removed from unigrams during tokenization, but will be used as separate features. For example, 
for the four punctuation marks above listed (“,”, “.”, “!”, and “?”) there are four feature labels 
“comma”, “period”, “finalExcMark”, and “finalQuesMark”. A number of studies have used 
punctuation removed from unigrams (e.g., Baayen, 2002; Grieve, 2007; Nazar and Sánchez Pol, 
2007; Orebaugh and Allnutt, 2009; Tambouratzis and Vassiliou, 2007; Zheng et al., 2006). 
Chaski has shown that syntactically classified punctuation (2001), in contrast to simple 
 
159 
 
punctuation, as I have just described, has a slightly better performance in authorship attribution. 
Also, syntactically selected punctuation has rendered good results in a couple of related studies 
by the just mentioned researcher (Chaski, 2005; 2007), as well as in a study by the author of this 
project (Rico-Sulayes, 2011). In these studies, punctuation instances are counted under labels 
such as “end-of-phrase” or “end-of-clause,” which express the function of punctuation marks in 
the text. However, in all these studies using syntactically selected punctuation required a manual 
tagging of texts, and this kind of tagging, even if computer assisted (as in Chaski, 2005; 2007), is 
slower. In order to avoid any manual annotation of the corpora in this project, only lexical 
category punctuation will be used. In this project, this kind of punctuation consists of plain, 
single-character marks. Also, through the inclusion of simple punctuation marks as lexical 
features (i.e., single character punctuation marks split from the words they originally attached 
to), their contribution in the classification task can be compared to the contribution of novel, 
media-specific uses of punctuation, which are part of the structural category in this study. A 
number of punctuation instances will be categorized as media-specific because they characterize 
the type of media in which they are used, such as the frequent reduplication of punctuation 
characters in electronic communications. Following Rico-Sulayes, (2011), the paper that was the 
origin of this project, sequences of 2, 3, 4, 5, 6, and 7 or more punctuation marks will be tagged 
and considered media-specific feature instances. Since this study will not perform any category-
specific subset testing, the contribution of these two types of features, lexical and structural 
punctuation features, will be determined by looking at the selection performed by reduction 
techniques, which aim at identifying especially useful and discriminatory features. 
 Besides the punctuation features just described, which have been proposed in previous 
studies, I have also included a number of novel lexical punctuation features that attempt to 
 
160 
 
capture some of the stylistic variation that I noted while reading the text samples in the corpora. 
As mentioned above, I have formerly used syntactically classified punctuation, which included 
word internal marks (Rico-Sulayes, 2011). Although word internal marks in general are not 
considered punctuation marks by Real Academia Española (2005), but auxiliary spelling marks, 
in (Rico-Sulayes, 2011) I included two word internal marks, hyphens and apostrophes. I included 
these two marks following Chaski (2005), who uses these two marks to identify morphemic 
edges. However, these two word internal marks, which also delimit morphemic edges in Spanish, 
are not very frequent in this language. In the experiment presented in Rico-Sulayes (2011), there 
were only 5 instances of these two marks in the texts by ten authors, for which 20,382 words of 
annotated text were included. Other Spanish word internal marks, such as stress/accent marks 
and umlauts, were not included in this study because they do not delimit morphemic edges. 
However, in the current project, I have included seven features to represent word internal marks 
that do not represent morphemic edges. The features for these marks, which Real Academia 
Española labels as auxiliary spelling marks, include five individual features that count instances 
of stressed vowels (“á”, “é”, “í”, “ó”, and “ú”), one feature for the umlauted vowel “ü”, and a 
generic feature for any of the former, i.e., any vowel with a diacritic. Although the presence or 
absence of stress in Spanish has been suggested as a feature of authorship (McMenamin, 2002), 
to the best of my knowledge individual features for specific vowels with auxiliary spelling marks 
and a generic feature for any vowel with a diacritic have not been used for this purpose before. 
 Regarding the second category of features listed at the beginning of this section, syntactic 
features, a list of Spanish multi-unit function words has been used. This list is part of a Spanish 
lexicon I collected during the summer of 2011. With several syntactical and semantic categories, 
this lexicon was derived from a variety of online resources and was aimed at various 
 
161 
 
computational linguistic tasks. The list of multi-unit function words used here, which can be seen 
in Appendix A, contains words from two Spanish closed class categories, i.e., whose list of 
members is relatively fixed: prepositions (such as después de ‘after’ and lejos de ‘far from’) and 
conjunctions (such as después de que ‘after’ and mientras que ‘whereas’). The multi-word 
prepositions included in Appendix A show separate entries for prepositions which can take 
different forms when they are contracted with articles (for example de ‘of’ and del ‘of (the)’, as 
in después de ‘after’ and después del ‘after (the)’). This list has a total of 131 multi-word 
sequences, which work together as individual function words. The multi-word sequences in this 
list consist of 68 bigrams, 56 trigrams, and 7 four-grams (i.e., sequences of two, three, and four 
words, respectively). Since no other higher order n-grams (sequences of two or more words) 
were collected from the experimental corpora, Appendix A shows all the multi-word sequences 
used in this project. It should also be noted that a number of individual words included in this list 
of multi-word sequences have other grammatical categories when they stand on their own. This 
happens because in Spanish prepositions or conjunctions can combine with other words to 
produce new multi-word units with their same grammatical category (Real Academia Española, 
2005). Using one of the examples just cited above, in después de ‘after’, the first word, después 
‘after’, functions as an adverb on its own, but becomes part of a preposition when is before de 
‘of’. As it is attested in Appendix A, this process is a productive resource in this language. Only 
multi-word prepositions and conjunctions were included in this list because these two closed 
class categories are especially productive in their construction of new elements through their 
combination with other words; other closed class categories, such as pronouns, determiners, and 
complementizers, do not share this characteristic in Spanish (Real Academia Española). As 
mentioned above, this project will not compare the performance obtained by using features from 
 
162 
 
single categories in the attribution. Therefore, single-unit function words were not included in 
this list because single-unit function words existing in the corpus should be contained in the data-
driven unigram list formerly described. Function words were chosen in this project because a 
number of authorship attribution studies that have used them have reported good results with 
them (e.g., Abbasi and Chen, 2005; 2008; Argamon, 2003; Baayen et al., 2002; Gamon, 2004; 
Juola and Baayen, 2005; Mikros and Argiri, 2007; Zheng, 2003; 2006). Function words are 
preferred in authorship analysis in general because they have the advantage of being content 
independent. Since not all of the n-grams included in Appendix A had instances in the corpora, 
especially in the corpora with a small set of subjects, a generic feature that counted all instances 
of multi-unit function words was also included. Once again, to the best of my knowledge the use 
of a predetermined list of word sequences, to capture all higher order n-grams that are multi-unit 
function words, has not been used in authorship attribution. Authors that use grammatical 
information in word sequences usually parse (grammatically annotate) the text. After parsing the 
text, they often use the grammatical labels obtained with this process (see e.g., Hirst and 
Feiguina, 2007; Koppel et al., 2008; 2009; Spassova, 2009). As to the studies that use word 
sequences, they tend to perform an exhaustive identification of all higher order n-grams (e.g., 
Nazar and Sánchez Pol, 2007), which can produce extremely long lists of features. 
 Finally, a number of structural features will be added to the feature list. Although these 
structural features are presented here in the last place, they actually represent the beginning of 
the feature list. As it will be further discussed below, all shorter lists of category-specific features 
will precede the long list of word unigrams, which will be ordered by their frequency. Among 
the various structural features used in this study, one vocabulary complexity measure will be 
used. First introduced in Chapter 3, so-called complexity measures comprise a number of scores 
 
163 
 
which use distributional or statistical information at the sentence and the document level. 
Originally developed to capture an authorial fingerprint (Juola, 2008; Koppel et al., 2009), none 
of these measures succeeded in this goal. However, these kinds of features are still commonly 
included in multivariate approaches (e.g., Chaski, 2001; Corney, 2003; Grant, 2007; Grieve, 
2007; Hirst and Feiguina, 2007; Juola, 2008; Koppel et al., 2009; and Koppel et al., 2008; 
Mikros and Argiri, 2007; Tearle et al., 2008), like the one used here.  In order to have a 
comprehensive list of features, the type-token ratio, one of the complexity (or vocabulary 
richness) measures most commonly used in authorship attribution (Stamatatos et al., 2001), has 
been included in this project. Among the above cited studies, Chaski (2001), Corney (2003), 
Grieve (2007), Hirst and Feiguina (2007), Koppel et al. (2008), Mikros and Argiri (2007), and 
Tearle et al. (2008) include this feature. The type-token ratio consists of the division of all 
distinctive word forms (or types) by all individual word instances (or tokens) in a text sample. 
 The rest of the structural features presented in this paragraph, a total of 17, are media-
specific, i.e., elements that characterize the medium of communication. Formerly cited examples 
of this kind of features are the use of various text formatting options (such as underlining or bold 
face), hyperlinks, or images in electronic messages or posts. A number of studies that do 
authorship attribution and collect data from electronic communications have used this kind of 
media-specific information (Abbasi and Chen, 2005; 2008; Orebaugh and Allnutt, 2009). The 
media-specific features used here were: replies to others’ messages, use of bold font, italics, 
underlining, font colors, special font sizes, emoticon images, keyboard-based emoticons, active 
or clickable hyperlinks, non-clickable hyperlinks, inclusion of images, and reduplicated 
punctuation series. This last feature was further divided to count separately series of 2, 3, 4, 5, 6, 
and 7 or more punctuation marks repeated in a single string. This further division of reduplicated 
 
164 
 
punctuation was a novel proposal made in the paper that was the origin of this project (Rico-
Sulayes, 2011). In this list of 17 media-specific features, there is a new one that was not 
originally included in the paper just cited, non-clickable hyperlinks. The distinction between 
clickable and non-clickable hyperlinks, encoded in two different features is also a novel proposal 
in this project. Out of these 17 media-specific features, 13 can be considered as forms of 
emotext, the collection of textual conventions that are developed in electronic communications to 
convey emotional content  (de Vel et al., 2002). These forms of emotext are the six features for 
reduplicated punctuation, the five features related to font use, and the two features for emoticons. 
 With respect to what combination of characters were considered a keyboard-based 
emoticon, a list of frequently referenced emoticons in Spanish was collected. This list of 
keyboard-based emoticons commonly used in this language was derived from various resources, 
in order to improve a short list formerly used by the author of this project (Rico-Sulayes, 2011). 
This first list was based solely in one online reference (UNED, Instituto Universitario de 
Educación a Distancia (IUED), 2008), and their elements were chosen arbitrarily, following the 
author’s intuitions. In order to improve this list and its construction criteria, four online reference 
lists of emoticons, which contained 338 emoticons, were identified (aunm@s, Enciclopedia 
Latinoamericana, 2012; Facebook Español, 2001; English.com.mx, 2008; UNED, IUED, 2008).  
The emoticons that were present in at least three out of the four mentioned sources (a total of 20) 
were used to create a preliminary list of emoticons. Two more emoticons frequently observed in 
the data were also added to this list. The final list with 22 emoticons is shown in Appendix B. 
This list was used to identify the instances of the keyboard-based emoticon feature in this 
project. Former studies have used emoticons in authorship attribution of different electronic 
communications (e.g., Orebaugh and Allnutt, 2009; Rico-Sulayes, 2011). 
 
165 
 
 Finally, three more features have to be added the ones formerly presented. One of these 
features is related to the two former categories of punctuation marks (lexical and structural), but 
it is in between both of them. This is the feature of ellipsis. Although ellipsis could be considered 
another lexical punctuation mark, this type of punctuation marks was formerly described as 
containing single character marks. So ellipsis was left out in that description. Also, the series of 
three reduplicated punctuation marks in the structural category encompasses ellipsis when it 
searches for repeated periods. Since posters in the online forum seemed to use this punctuation 
mark quite often, ellipsis was identified separately in the corpus, and was not collapsed with the 
reduplicated series of three punctuation marks.  
 The two last features that will be presented are intrinsically related to each other, even 
though one is a structural feature and the other one is a lexical punctuation mark. Given the 
conditions taken into account for the identification of these features, they also imply a novel 
proposal of authorship features. The first feature, the carriage return in electronic 
communications (specifically in the posts of an online forum) is structural. Technically this 
carriage return is equivalent to a paragraph, but in electronic communications that is not always 
the case. Especially when the text is reflowable (namely when it adapts to the size of the screen 
in which it is displayed), technology users tend to utilize this carriage return in a peculiar way, 
sometimes as a new line and sometimes as a new paragraph. In order to further address this 
ambiguous use of the carriage return in reflowable text, one more feature related to this was 
included. This last feature counts the presence of punctuation before the carriage return. 
 Table 4.2 below summarizes all the features used in the experiments analyzed and 
discussed later in this chapter. This table presents the whole list of features in the order they were 
fed to the feature reduction techniques or the classifiers that will be introduced in the next 
 
166 
 
sections. This order is different from the one in the above presentation, which followed the 
categorization of features in Chapter 3. As can be seen in Table 4.2, the list of all features 
presents first 19 structural features. Next, syntactic features consist of 132 multi-unit sequences 
(or higher order n-grams) for function words. These function word n-grams include both multi-
word prepositions and multi-word conjunctions. Finally, the list of lexical features begins with 9 
features: the multi-character punctuation mark of ellipsis, the presence of punctuation before the 
carriage return in reflowable text, a generic feature for vowels with diacritics, and six individual 
features for stressed and umlauted vowels in Spanish. At the end of the set of lexical features, 
and the whole set of features in general, a long list of single-character punctuation marks and 
word unigrams is ordered by their absolute frequency in the experimental corpora. 
  
 
167 
 
Table 4.2 List of all authorship features in their original order before reduction/classification 
Table 1 
Feature category Feature(s) number of features 
in category 
  
    
Structural   19 
 
    
 
reply message 
 
 
bold font 
 
 
Italics 
 
 
Underlining 
 
 
font colors 
 
 special font sizes  
 emoticon images  
 keyboard-based emoticons  
 clickable hyperlinks  
 non-clickable hyperlinks  
 Images  
 
reduplicated punctuation 
(series of 2, 3, 4, 5, 6, 7+)  
 carriage return  
 
type-token ratio 
 
   
Syntactical   132 
   
 
multi-unit function word 
 
 
multi-word prepositions 
 
 
multi-word conjunctions 
 
   
Lexical   1402-13098 
   
Ellipsis 
 
punctuation before return 
 
vowels with diacritics 
 
stressed a, e, i, o, u 
 
umlauted u 
 
single-character punctuation  
 word unigrams  
        
 
 
168 
 
 
4.3.1b Feature Reduction Techniques 
As mentioned before, the experiments in this chapter will include the most commonly used 
feature reduction techniques in previous authorship attribution studies, frequency and 
information gain (IG), as well as a technique new to the area of authorship analysis, correlation-
based feature subset (CFS). Formerly discussed in Section 3.1.2, six out of 17 authorship 
attribution studies that use feature reduction techniques employ some form of frequency-based 
selection (Burrows, 2002; Gamon, 2004; Hirst and Feiguina, 2007; Jockers et al., 2008; Koppel 
et al., 2009; Peng et al., 2003). Three more studies use IG to reduce their list of features (Abbasi 
and Chen, 2008; Koppel et al., 2009; Orebaugh and Allnutt, 2009). It should be mentioned that 
the most commonly used feature reduction techniques have been selected here because there is 
very little experimentation comparing different feature reduction settings or techniques. As 
mentioned in Section 3.1.2, only four studies out of 17 that use feature reduction techniques do 
any testing with reduction techniques parameters that results in feature sets with different sizes 
(Burrows, 2002; Gamon, 2004; Hirst and Feiguina, 2007; Tambouratzis and Vassiliou, 2007). 
Also, three out of these four studies obtain their best results with the largest list of features they 
test (Burrows, 2002; Hirst and Feiguina, 2007; Tambouratzis and Vassiliou, 2007). Only 
Gamon’s results are different in this respect. Testing a wide range of absolute frequencies of 
features (5, 10, 20, 50, 75, 100, 200, and 500 instances as the minimum number of instances of a 
feature to be included), Gamon’s best results are obtained using features that have at least 75 
instances in the corpus. Although there may be a complex set of factors that produce these 
results, it seems apparent that his smallest value (i.e. a 5-instance minimum) does not perform a 
great reduction of noisy features and therefore, the longest list obtained with this frequency value 
 
169 
 
is not the most efficient in this study. Only one out of 17 studies that use feature reduction 
compares two different techniques (Rico-Sulayes, 2011). In addition, the two techniques 
compared in this study are built-in options in the SPSS software implementation of DA, Wilks’ 
Lambda and Mahalanobis. Thus, there is no one single comparison of several external feature 
reduction techniques in the survey of 32 authorship attribution studies. Because of this scarce 
experimentation in the use of feature reduction techniques, I have decided to explore and 
compare the results obtained with the most common, external (or classifier independent) 
techniques, frequency and IG. As to the other technique that will be tested in this project, I 
decided to use CFS for two main reasons. First, this is also an external technique, namely it is not 
tied to a classifier. Second, this technique has never been applied in the context of authorship 
analysis. Therefore, any results obtained with this technique should be useful as a reference for 
future work in this area. 
 As to the way of implementing the concept of frequency for feature reduction, in the 
survey of techniques made in Section 3.1.2 it was shown that frequency can take several forms 
when it is applied for this purpose. For example, some researchers use plain frequency, the 
number of instances of a given feature in the experimental corpus (Gamon, 2004; Peng et al., 
2003). In this case, a number that represents the minimum number of instances is usually 
employed as the threshold value to include a feature in the reduced list. For example, researchers 
may include features that have 5 or more instances, as Gamon does with the smallest value in the 
range of values he tests. Researchers can also use relative (or normalized) frequencies, i.e., the 
proportion or percentage that the absolute frequency of a feature represents with respect to the 
total of feature instances counted in the corpus. One study surveyed uses relative frequencies 
(Jockers et al., 2008). To exemplify the relationship of these two forms of frequencies, in a 
 
170 
 
corpus with a total of 1,000 instances for all different features a minimum of 5 instances would 
represent a relative frequency of 0.005. This minimum relative frequency can also be used as a 
threshold value. Jockers et al. include features whose relative frequency represents at least 1% 
(or 0.01) of the corpus feature instances. Another way to use frequency as a feature reduction 
criterion is to first order features by their frequency (absolute or relative), and then select the n 
most frequent elements (Burrows, 2002; Hirst and Feiguina, 2007). Finally, it is also possible to 
combine frequency with other criteria, applying for example another technique to a list of the n 
most frequent features (Koppel et al., 2009, combine frequency with IG in this fashion). The 
concept of frequency, however, is behind all of these forms of applying it, and they can all be 
translated to each other. Any plain frequency can be expressed as a relative frequency, and the 
bottom element in a list of n most frequent elements has some absolute or relative frequency. In 
this project, I will use plain frequency, with a minimum of 4 instances as the threshold value. I 
have selected this threshold value because, as will be further discussed in the next paragraph, this 
number of instances is equal to the number of testing samples per subject in the closed set of 
potential authors. 
 The minimum plain frequency value used as the threshold for the inclusion of features 
was chosen for three main reasons. First, if a feature can have as many instances as the number 
of text samples per author in the corpus (namely, as the number of events per class in statistical 
terms), it has the potential to reach the maximum IG value. This would happen if the instances 
were distributed among all samples by a given author and absent in the rest of the corpus. Thus, 
there is some statistical discriminatory potential in the features included with this absolute 
frequency. In addition, using this threshold value eliminates a great number of elements from the 
long n-gram lists, as it will be shown in the presentation of experiments at the end of this chapter. 
 
171 
 
Therefore, this simple criterion can help in the elimination of a great deal of potentially non-
discriminatory features. Finally, the third reason for the selection of this value is somehow the 
counterpart of the last one, in that many elements are certainly eliminated, but the final feature 
set is considerably larger than the one produced by IG or CFS in the experiments later reported. 
Longer lists are obtained with this absolute frequency because, on one side there is a 
discriminatory potential in having as many instances as the number of text samples per author in 
the corpus, but on the other side the demanding conditions necessary for a high IG score or the 
best CFS evaluation are not necessarily present. Therefore, the criterion is lax compared to the 
other two techniques. As it will be shown below, having more information can be a better option 
for classifiers that are particularly good at handling noise themselves. 
 IG was presented in detail in Section 3.1.2. There it was explained that, given a collection 
of texts by a number of authors, the IG value for some feature f (such as the use of a word, a 
punctuation mark, or an emoticon) in one class c (namely a specific author) depends on five 
pieces of information. These five information elements are the number of texts by all authors 
(ts), the number of texts with feature f in class c (tswfinc), the number of texts with feature f that 
are not in class c (tswfoutsidec), the number of texts without feature f but in class c (ts
w/ofinc), and 
the number of texts without feature f and not in class c (tsw/ofoutsidec). From these five pieces of 
information it is possible to derive the IG value for some feature in any class using formula (3.1), 
below repeated as (4.1).  
 
(4.1) 
IG (f, c) = ((tswfinc / ts) (log2 (ts * ts
wfinc / (ts
wfinc + ts
w/ofinc) * (ts
wfinc + ts
wfoutsidec)))  
 + ((tswfoutsidec / ts) (log2 (ts * ts
wfoutsidec / (ts
wfoutsidec + ts
w/ofoutsidec) *  
 
172 
 
  (tswfinc + ts
wfoutsidec)))  
 + ((tsw/ofinc / ts) (log2 (ts * ts
w/ofinc / (ts
wfinc + ts
w/ofinc) * (ts
w/ofinc + ts
w/ofoutsidec)))  
 + ((tsw/ofoutsidec / ts) (log2 (ts * ts
w/ofoutsidec / (ts
wfoutsidec + ts
w/ofoutsidec) *  
  (tsw/ofinc +  ts
w/ofoutsidec))) 
 
Besides first introducing formula (4.1), Section 3.1.2 also presented a detailed example of how to 
obtain the IG value for the use of the keyboard-based smiling emoticon for a hypothetical author, 
using hypothetical numbers for the five pieces of information used in the formula. In the 
experiments described later in this chapter, I will select the features that render an IG score 
greater than zero in the selected implementation of this reduction technique, which will be 
further described below. 
 Finally, the other feature reduction technique that will be combined with a number of 
classifiers in this chapter is CFS. This feature selection relies on the assumption that good 
features should correlate with classes (or authors in this project), but they are not correlated with 
each other (Hall, 1999). CFS attempts to identify relevant features (which correlate with class 
membership) and to eliminate both non-relevant features (non-correlated with classes) and 
redundant features (which correlate with other features). Therefore, the selection of a feature in 
CFS depends on how much it predicts the membership of class that is not predicted by other 
features. As it happens with other feature reduction techniques formerly mentioned in Section 
3.1.2 (such as PCA, ANOVA, two-way ANOVA, and ANCOVA), CFS depends on a number of 
mathematical constructs which require their own introduction. The main source of complexity in 
the calculation of CFS scores has to do mostly with the computation of correlation values, either 
between a class and a feature or among various features. Hall (1999) presents three different 
 
173 
 
methods to calculate correlation values, but after experimenting with all of them, he considers 
that the preferred or standard method for CFS should be symmetrical uncertainty (Hall, 1999). 
Once this method to calculate correlations is understood, the evaluation of a subset of features is 
obtained by a simple, straightforward formula.  
 Symmetrical uncertainty, the standard correlation calculation method in CFS, can be 
derived from the concept of entropy, which is a measurement of uncertainty or unpredictability 
of an event in some system. The entropy of a feature f, say the use of the keyboard-based smiling 
emoticon in some author, can be derived from formula (4.2) adapted from Hall: 
 
(4.2) 
  
     	  
 log 	

 
 
where H stands for entropy, F is set of all features selected, and P (f) is the probability of feature 
f occurring in the text collection. As was mentioned in Chapter 3, in information retrieval the 
probability of the occurrence of a feature is typically calculated as its relative frequency 
(Manning et al., 2008), and therefore equation (4.2) is equivalent to (4.3): 
 
(4.3) 
 
     s

s  
  log  
s
s 
 
 
 
174 
 
where tswf  is equal to the total texts that contain feature f and ts is the total number of texts in the 
collection.  
 In order to fully explain entropy and CFS, let us take again the example used in the 
presentation of IG and X2 in Chapter 3. Following this example, we will suppose once more that 
in a given collection of texts some subject uses the keyboard-based smiling emoticon in 2 texts 
(tswfinc = 2), that this subject authored 8 more texts without the emoticon (ts
w/ofinc = 8), that 10 
more texts authored by other subjects have the keyboard-based smiling emoticon (tswfoutsidec = 
10), and that the whole corpus has 80 more texts that are not authored by the subject in question 
and do not have the emoticon (tsw/ofoutsidec = 80). Given these conditions, the total number of 
texts in the corpus (ts) is 100, the sum of the four given values (2 + 8 + 10 + 80). Also, there are 
12 texts with the emoticon in the whole corpus (tswf = (ts
wfinc + ts
wfoutsidec) = (2 + 10) = 12). 
Inputting these values in (4.3) we get: 
 
    ∑   
 log  

   
 =   ∑  0.12 
  log 0.12  
 =   ∑  0.12 
  – 3.058 
 =   ∑   – 0.367 
  –12 * – 0.367 ≈ 4.404 
 
 Equation (4.3) can also be changed to calculate the entropy of class c, or of the 
hypothetical author in the above example. This can be done substituting this variable for f 
accordingly, and using the total of texts that contain class c (tswc) (namely, that are authored by 
this hypothetical author) instead of the total of texts that contain a given feature, tswf. These 
 
175 
 
substitutions give us formula (4.4), where C is the set of all classes, or the total number of 
subjects in our context. 
 
(4.4) 
 '    s
'
s  . log  
s'
s ()
 
 
Taking the example described above, the total of texts that contain class c (tswc), namely which 
are authored by subject c, is equal to the sum of the number of texts authored by this subject with 
the keyboard-based smiling emoticon (tswfinc = 2) and without it (ts
w/ofinc = 8). Namely, ts
wc = 
(tswfinc + ts
w/ofinc) = (2 + 8) = 10. Plugging in these values in (4.4) we now get: 
 
 '   ∑   
 log  

()   
 =   ∑  0.10 
  log 0.10()  
 =   ∑  0.10 
 () – 3.321 
 =   ∑  () – 0.332 
  –10 * – 0.332 ≈ 3.321 
 
Once the entropy is calculated for all features and classes in a corpus, the symmetrical 
uncertainty coefficient (UC), for some feature f and some class c, can be computed using formula 
(4.5), adapted from Hall (1999). This formula relies on the previous calculation of IG for these 
two elements: 
 
 
176 
 
(4.5) 
 UC (f, c) = 2.0 * [IG (f, c) / H (f) + H (c)] 
 
Formula (4.5) can also be used to calculate the correlation coefficients for inter-feature 
relationship, which is seen as a method to identify redundant features in Hall (1999). Since the 
IG value for the hypothetical use of the keyboard-based emoticon in some author was calculated 
in Section 3.1.2, we can just plug in the value rendered by our formula in that Section  (0.0138), 
along with the two entropy values just obtained, to find the UC of the given emoticon f in the 
subject c. 
 
 UC (f, c) = 2.0 * [0.0138 / 4.404 + 3.321] 
  = 2.0 * [0.0138 / 7.725] 
  = 2.0 * 0.0017 ≈ 0.0036 
 
In order to evaluate a subset of features, from the general list of features selected by the 
researcher (for example, a subset of the first 10 features in a longer list of 100 features), the UC 
correlation coefficients for all 10 features in the subset and some given class are calculated, as 
well as the inter-feature UC correlation coefficients among all 10 features in the subset. After 
computing all these coefficients, the mean for all feature-class correlations in the subset is 
obtained (MeanFC), and so is the average of all feature-feature correlations in the subset 
(AvgFF). With these two values the CFS value of a given feature subset S is computed with the 
following formula adapted from Hall (1999), where n is equal to the number of features in the 
subset evaluated: 
 
177 
 
 
(4.6) 
CFS -  n 
 /012345n 6 nn  1 
  78933 
 
If we wanted to evaluate a feature subset that consisted of only one feature, such as our broadly 
explored keyboard-based smiling emoticon, we could plug in the values that we have obtained in 
formula (4.6). Since there is only one feature and one class evaluated, the mean for all feature-
class correlations (MeanFC) is equal to the only correlation value we have obtained (0.0036). 
The correlation of any feature to itself is 1.0, as shown in (Hall, 1999, p.72), and the average of 
the feature-feature correlations (AvgFF) in one feature set is just the same value. Inputting the 
values just mentioned into formula (4.6) gives us the CFS score for this one feature subset: 
 
CFS -  1 
 0.003651 6 11  1 
  1.0 
 1 
 0.003651 6 0  
 1 
 0.00361  
  = 0.0036 
 
This is the final CFS score for a subset of features that includes only one feature, the smiling 
emoticon. Other possible subsets will input other values into formula (4.6) rendering various 
CFS scores. In order to select one feature subset among a number of different subsets with 
different features, CFS scores for all subsets are computed and the subset with the highest score 
 
178 
 
is chosen. Then, the only missing part of the feature reduction technique, which is explained 
below, is the process to choose various subsets to be compared. 
 Formerly mentioned in Chapter 3, the evaluation of all possible feature subsets is not 
feasible even for short feature lists, and Hall concurs with this (1999). This is because the 
number of all subsets that can be derived from some feature set grows extremely fast as the set 
gets bigger. The number of all subsets in a set with n features is equal to 2 to the nth power minus 
one. Therefore, as few as 25 features in the set produce over 33.5 million subsets and would 
require the same number of experiments to test which is the best set. Having just 100 features 
will require conducting over 1.2 nonillion (or 1.2 x 1030) experiments. Conducting this many 
experiments is virtually impossible for any researcher, regardless of the tools used. Just to put it 
into a human perspective, there are only over 3.15 billion (or 3.15 x 109) seconds in a 100-year 
life span. In order to deal with the impracticality of testing all feature subsets, CFS has two basic 
search methods: moving forward in the list, beginning with a set of only one feature and adding a 
new feature to each new subset, or moving backward in the list, eliminating features from the 
whole list one at a time for each new subset. These two search processes can be combined with 
another parameter that avoids exploring the whole list of subsets these processes can create. With 
this option, after a number of attempts are performed to improve the highest CFS score, if no 
improvement is achieved the search is finished. This parameter is called, not very intuitively, 
“best first” in CFS. The default search evaluation method in CFS, which is the one used in this 
project, is best first (with 5 tries) and forward. This means that the subsets evaluated were built 
gradually adding features from the beginning of the general list of features and stopping after 5 
attempts to improve the highest CFS score. 
 
179 
 
 The two rather complex feature selection techniques explored in this project, IG and CFS, 
have been applied using their implementation in the University of Waikato Environment for 
Knowledge Analysis (Weka) version 3.7.5. As mentioned before, features with IG scores greater 
than zero have been selected to reduce the feature list with the first technique, and default search 
method parameters (best first with five tries and forward) have been used to obtain a feature set 
with CFS. Regarding the use of frequency as a feature reduction technique, the absolute value of 
all features has been determined online during the tagging of the corpus. A minimum absolute 
frequency of 4 has been used as the threshold value to include features in the reduced set. An n-
gram list ordered by absolute frequency (from the most to the least common n-gram) and 
preceded by all other shorter, category-specific feature lists, has been fed to the Weka 
implementation of IG and CFS. 
4.3.2  Class Attribution Methods 
In contrast with the scarce research that compares different feature reduction techniques in 
authorship attribution (only one out of 32 studies surveyed does this), several studies compare 
the results rendered by different class attribution methods in this task. As shown in Section 3.2.1, 
ten out of the 32 studies compare different class attribution methods in authorship attribution 
(Abbasi and Chen, 2005; 2008; Burrows, 2007; Chaski, 2007; Jockers et al., 2008; Koppel et al., 
2009; Orebaugh and Allnutt, 2009; Raghavan et al., 2010; Zheng et al., 2006; Zheng et al., 
2003). Among all 32 studies surveyed, four class attribution methods were identified as the most 
commonly used: C4.5, discriminant analysis (DA), naive Bayes (NB), and support vector 
machines (SVMs). Out of 32 research papers, 22 studies use one of these four methods, with 
each method used in four to ten different studies. All other class attribution methods, a total of 
17, are used in one or two studies only. As to the best results obtained with the most commonly 
 
180 
 
used methods, three have rendered the best results in the various experiments where their 
performance has been compared: DA, NB, and SVMs. DA performed better than SVMs in 
Chaski (2007). NB obtained a higher success rate when compared to C4.5 in Orebaugh and 
Allnutt (2009). Finally, SVMs was reported to perform better than other frequently used methods 
(C4.5 and NB) in a number of studies (Abbasi and Chen, 2005; Koppel et al., 2009; Zheng et al., 
2006; Zheng et al., 2003). Even though C4.5 was not reported to outperform other classifiers in 
the studies surveyed in Section 3.2.1, it has been selected, for this very reason, to be used as a 
performance baseline. This baseline should be especially useful to compare the various approach 
configurations that have been assembled when combining the most successful classifiers and the 
most commonly used feature reduction techniques. 
 Before presenting the four class attribution methods that have been mentioned above, it 
will be helpful to describe briefly the kind of information that these methods are fed. These 
methods process individual representations of all documents, which in this project are aggregated 
text samples of about 500 words in length. Each document or text sample representation contains 
the instance counts for all features selected in the final set, reduced or not. For example, let us 
assume that a final set of features contains three features, say the use of the word “tú” in Spanish 
(you), the question mark, and any of the emoticons in a predefined list. Given this feature set, a 
document could be represented by a list of numbers which consist of the instance counts for 
these three features, d1 = {5, 2, 1}. These lists of numbers are usually known as vectors in 
information retrieval. In the tagging process of a corpus, feature instances are identified and 
counted in all the documents in the corpus, producing at the end as many vectors as there are 
documents. Once all vectors are obtained, a usually large proportion of them is used as the 
training set, and the rest of the vectors (used as testing samples) are classified individually by 
 
181 
 
comparing them to the first ones. Continuing with the example, a corpus could consist of three 
documents represented by the following vectors: d1 = {5, 1, 1}, d2 = {0, 4, 5}, and d3 = {4, 1, 1}. 
If the first two vectors (d1 and d2) are used as the training set, the last vector could be classified 
by comparing it to the first two. In this example, it is not particularly difficult to see that d1 is 
almost identical to d3 and we would expect a classifier to group together these two documents. If 
we know that document d1 was written by author A1 and d2 by A2 , the classifier would then 
assign d3 to the first author. In experiments with dozens of authors and thousands of features, as 
the ones that will be performed in this chapter, class attribution methods perform highly complex 
mathematical operations in an attempt to achieve the correct classification for all documents. Of 
course, class attribution methods are not the only approach component responsible for a good or 
bad outcome, but they depend greatly on a selection of adequate, discriminatory features. 
Actually, it has been argued in the literature that the selection of features is more important than 
the subtle tuning of class attribution methods in authorship analysis (Juola, 2008; Koppel et al., 
2008; Rico-Sulayes, 2011). Following this idea, this research project is focused on proposing 
novel, discriminatory features, rather than testing or implementing new classification methods. 
 One more observation that has to be made about the document representations that are 
fed to classifiers is the normalization of features. First mentioned in Chapter 2, normalization is a 
procedure that can be used for a number of purposes in the general field of statistics. One of the 
most important objectives of feature normalization in the context of information retrieval and 
text classification is to avoid the anomaly that results from having documents of diverse sizes in 
the corpus (Manning et al., 2008). When documents of different lengths are used, longer texts are 
likely to have features with higher absolute frequencies. In these long documents, these higher 
frequencies may be the mere consequence of the fact that those documents have more 
 
182 
 
information and they repeat features (such as words) much more times than shorter texts do. In 
the face of this issue, normalization can reduce the high frequencies rendered by features in 
longer texts by scaling the frequencies for some feature in relation to the highest frequency 
obtained in all the corpus. This is called maximum normalization. Normalization can take several 
forms and there are studies that explore the differences that various normalization procedures 
bring to classification tasks in information retrieval (Aksoy and Haralick, 2001). In one of the 
most common forms of normalization (Rome et al., 2003), linear scaling to unit range, all values 
in a vector are reduced to a range of values that goes from 0 to 1. In linear scaling to unit range, 
the normalized value of a feature (NF) can be obtained using the maximum (maxF) and lowest 
(lowF) value for the feature in the corpus. Using these two pieces of information, as well as the 
count of instances of the feature in a given document (f), the NF of this feature can be obtained 
using formula (4.7) (adapted from Aksoy and Haralick): 
 
(4.7) 
 NF = (f – lowF / maxF – lowF) 
 
If we take the first feature in the formerly presented vectors d1 = {5, 1, 1}, d2 = {0, 4, 5}, and d3 
= {4, 1, 1}, the maximum value, maxF, is 5 and the minimum, lowF, is 0.  Then, plugging in 
these values, along with the first feature value for d3, 4, in formula (4.7), we get: 
 
  NF = (4 – 0 / 5 – 0) = 4 / 5 = 0.8 
 
 
183 
 
Applying formula (4.7) above renders the normalized valued (0.8) for the first feature in d3. 
Since a vector represents one individual document or text sample, this normalization procedure 
helps avoid the effects of having documents of different sizes in both the training and testing data 
sets. After applying formula (4.7) to all the feature values in the three vectors above described, 
they become: nd1 = {1, 0, 0}, nd2 = {0, 1, 1}, and nd3 = {0.8, 0, 0}. As it was mentioned in 
Chapter 2, normalization is not perfect and there are studies in information retrieval that test and 
compare different normalization schemes (Aksoy and Haralick, 2001). Manning et al. (2008) 
identifies at least three problems in the use of normalization. First, if feature values are not 
represented by simple values, such as their total number of instances, these values may be 
derived from other features values, such as in relative frequencies or in the various complexity 
measures discussed before. In these cases, a minimum change in the list of features (the insertion 
or deletion of only one feature) may require a recalculation of all normalized values. This is what 
renders, for Manning et al., the normalization process “unstable” or “hard to tune” (2008, p. 
117). Second, a document may contain a term that appears with an atypically high frequency. 
This non-representative frequency will affect, however, the values of this feature across the 
whole corpus after normalization. Finally, documents with frequent features that have similar 
absolute frequencies should have a different representation from documents where frequent 
features have a more skewed distribution. This kind of difference is not captured by vectors with 
normalized feature values. Although this study has tried to control the size of documents, feature 
values will be normalized. Since a number of issues related to normalization have been 
identified, experiments with and without feature normalization will be conducted and their 
results will be analyzed and discussed. The comparison of results obtained with and without 
normalization will help emphasize the contexts in which the need to control document length is 
 
184 
 
especially important. Also, this comparison should identify what approaches can benefit more 
from the feature frequency information that is lost after normalization. 
 After describing the document representations that are fed to class attribution methods, 
the first of these methods that will be presented in this Section is C4.5. This class attribution 
method is a decision tree classifier (Alpaydin, 2010). This means every time this method takes 
one feature, a decision node takes the feature and splits the feature values in a number of 
branches. If the feature has discrete values (i.e., word representations or labels, such as the labels 
“hot”, “mild”, and “cold”, for a feature “weather”), which have defined previously in the data, 
C4.5 splits the feature in as many branches as there are labels for it. In an example that is closer 
to the ones formerly presented, the feature “emoticon” could produce four branches for the labels 
“smiling”, “sad”, “laughing”, and “crying”. In the description of document representations given 
above, we have seen that in this project these representations consist of vector with counts of 
instances. This means that the values are numerical. Features with numerical values are 
discretized in a number of branches by the classifier. If the classifier divides a feature in two 
branches, for example, there is a binary split. In the vectors described above as a training set (d1 
= {5, 1, 1}, d2 = {0, 4, 5}), the third feature (the use of emoticons) had the values {1, 5}. Taking 
this feature, a decision tree will create a rule that splits this values in two branches. For example, 
the following rule will divide the feature (f) input in a right (Rf) and a left branch (Lf) for the 
feature in question: Lf = {f | f  > 5} and Rf = {f | f ≤ 5}. As the classifier considers more features, 
new decision nodes are added at the bottom of higher branches (decision trees are drawn upside 
down, as linguistic trees, with the root on the top). As the classifier considers all features in the 
documents of the training set, the tree is constructed in a process called tree induction. The whole 
branch construction process is usually automatic, although researchers can also interact with the 
 
185 
 
classifier to take some control of it. Since many possible trees may represent accurately a given 
training set, a number of procedures are followed to find the smallest possible tree that describes 
correctly the training set. Another characteristic of the training process when using decisions 
tress is that tree learning algorithms are greedy, which means that they try to process the entire 
data set. Taking one feature at a time, they split all features in the set recursively until there is no 
need to split anymore to describe the training set. A classification tree judges the appropriateness 
of a split based on an impurity measure. This measure considers that a new branch is pure when 
after adding some split all the events (or documents) that reach the bottom of that branch belong 
to only one class (or author). In a classification tree, such as C4.5, the construction of a tree 
continues until all of its branches are pure. Alpaydin (2010) gives a detailed description of the 
most important processes in C4.5 (and other decision trees) including the calculation of impurity 
measures, the early stop of the tree construction (also called pruning), which is one of the ways 
in which the researcher can influence branch construction, and the various applications of the 
rules that make up a tree. The experiments described later in this chapter have used the Weka 
implementation of C4.5, which is called J48 in this software workbench, with all their default 
parameters. 
 The second class attribution method presented here is DA. This method has a long history 
in the general field of statistics. DA attempts to determine a reduced number of functions, 
associated with the features in the set, which permit the maximum separation among a given 
number of classes (or authors in this context) (Timm, 2002). The functions produced for a 
training set, called discriminant functions, are one less than the total number of classes (or 
authors) in the corpus (Burns and Burns, 2008). For the construction of each discriminant 
function each feature value by a given author is multiplied by some weight, the purpose of which 
 
186 
 
is to separate the values of the features across different authors. All weighted features by the 
author are added and this sum gives the discriminant score for the author in question. In the 
example formerly used, an author A1 is represented in the training set with the vector d1 = {5, 1, 
1}. A discriminant score for the representation of this document by A1 would use a discriminant 
function (DF) such as DF = (w1 * 5) + (w2 * 1) + (w3 * 1). In this function, the ws represent some 
weight that maximizes the separation of the values of the feature in turn across the different 
classes or authors. After obtaining DFs for the representation of the documents by one author, in 
which the weight can have any value since no separation is needed yet, the weights for the 
documents of a different author are calculated using mathematical constructs, such as 
Mahalanobis distance or Wilk’s Lambda (also used in feature reduction techniques), which 
attempt to separate DFs that belong to different authors. During the training stage, discriminant 
scores for all the documents by one author are produced. The mean value for the discriminant 
scores by one class or author represents the class centroid. Later, when classifying an event (or 
document) between two classes (or authors), a discriminant score for the testing document is 
produced. Then, the testing document is assigned to the author whose centroid is closer to its 
own discriminant score. Timm (2002) makes an in-depth presentation of the calculation of 
discriminant scores, the rules and the rule evaluation used to make a final class attribution, and 
the use of feature reduction techniques (which he calls stepwise selection procedures) linked to 
this classification method. In this research project, the implementation of SPSS version 20 for 
DA will be used.  
 In third place, two variations of NB classification will be presented. In order to assign a 
testing document to some class or author, NB classifiers estimate the probability that the testing 
document d belongs to a given class c, P(c|d), in the training data (Manning et al., 2008). The 
 
187 
 
probability of a testing document belonging to some author or class P(c|d) is calculated as the 
multiplication of the probability of any document being in class c, P(c), by the product of the 
probabilities of all the features in the testing document (f ϵ d) being in class c. Namely, P(c|d) = 
P(c) + ∏ f ϵ d (P(f |c)). Since the probabilities of features that are absent in a document are equal 
to zero, these zeros are eliminated from the final calculation. Once P(c|d), the probability that a 
testing document belongs to some class, is computed for all authors in the training data, the 
author that renders the highest P(c|d) value is assigned to the testing sample. As mentioned 
before, probabilities are often calculated as relative frequencies in information retrieval. 
Therefore, P(c) is equivalent to the total of texts that contain class c (tswc) divided by the total of 
texts in the training data (ts), P(c) = (tswc + ts). As to the probability of a feature in the testing 
document belonging to some class, P(f |c), there are two ways to compute this probability. In one 
form of NB classification P(f |c) is calculated as the number of instances of feature f in 
documents belonging to class c, f inc, divided by the number of instances of all features (F) in all 
the documents (D) in the training data, or the sum of f ind for all features in all documents. 
Namely, P(f |c) = (f inc / ∑ f ϵ F ∑ d ϵ D (f ind)). This calculation of P(f |c) corresponds to a 
multinomial NB (MNB) model. As Manning et al. point out, in this model of NB the documents 
can be represented as vectors with the instance count for each feature, which is precisely the 
document representation that has been used in this research project. 
 In a different model of NB classification, the probability of a feature in the testing 
document belonging to some class, P(f |c), can be calculated as the relative frequency of f in class 
c (Manning et al., 2008). Using formerly presented pieces of information, this relative frequency 
is equivalent to the total of documents with the feature in the class, tswfinc, divided by the number 
 
188 
 
of documents in the class, tswc. Namely, in this model P(f |c) = (tswfinc / ts
wc). This model is 
known as the multivariate Bernoulli model.  
 The main difference between the two models of NB classification, MNB and the 
Bernoulli model, is given by the calculation of the probability of a feature occurrence in a testing 
document for a given class, i.e., P(f |c). This difference can have important consequences in the 
classification task. As has been discussed in the description of MNB, this form of NB uses 
information about feature instances, i.e., how many times some feature is found in the documents 
of an author. On the contrary, the Bernoulli model uses the relative frequency of features in an 
author’s documents, i.e., the proportion of the documents by a given author that have some given 
feature. The information used in MNB is played down in normalization to avoid differences in 
text length. This does not happen in the Bernoulli model because it uses document level 
information. As has been argued, if text length is controlled externally, the feature instance 
information employed by MNB could be useful. Also, if text length has been controlled, the 
Bernoulli model of NB should not show a great impact after normalization, since it relies only on 
document- rather than instance-based probabilities. I will come back to these comments as the 
results obtained by these two variations of NB classification are compared. 
 Mannin et al. (2008) provide a thorough introduction of the two models of NB just 
discussed, with examples of how to compute all of the above-mentioned probabilities, and how 
to eliminate zero probabilities (a process known as smoothing). They present NB classification in 
two important contexts in information retrieval, the ranking of search engine results and general 
text classification. These authors also discuss other important modifications that can be made to 
NB, such as including the use of information regarding the position of the feature in the 
document, or relying on the opposite assumption of positional independence. The differences 
 
189 
 
between the two models of NB, MNB and the Bernoulli model, have been discussed in the above 
paragraph not only to justify the inclusion of these two models in the experiments conducted in 
the next chapter, but also to interpret the differences in their results. In the application of the two 
models of NB just presented, all default values for these two classifiers in Weka will be used. 
Following the implementation of these two models in Weka, from this point on the Bernoulli 
model will be called simply NB, and the multinomial model will continue to be referred to as 
MNB. 
 The three classifiers introduced so far represent different classification schemes. C4.5 
builds a tree by using a decision node for each feature. At each node, a rule splits the values of 
the feature in branches (Alpaydin, 2010). DA is a linear classification process, in which the 
representation of  events, or documents in our context, is a simple sum of the values of the 
features in the document (Witten et al., 2011). In order to achieve the maximum distance 
between events belonging to different classes, feature values are weighted individually. MNB 
estimates the probability that some testing document belongs to each of the authors in a corpus 
and chooses the author for which the testing document renders the highest probability. The 
probability of occurrence of some feature in some author, another crucial piece of information in 
MNB, is computed using information about feature instances (Manning et al., 2008). NB does 
the same as MNB, but calculates the last piece of information, i.e., the probability of occurrence 
of some feature in some author, using document level information. Compared to these three 
types of classifiers, support vector machines (SVMs) represent a different classification scheme, 
which is an extension of the linear models, such as DA.  
 In a linear model, such as DA, the feature values of a document are weighted and added 
using a function (such as DF = (w1 * f1) + (w2 * f2) + (w3 * f3)). The result of the function, DF, is 
 
190 
 
a score that represents a point in space. That point is expected to be closer to the scores obtained 
by other documents belonging to the same class. DA attempts to maximize the distance between 
the scores of events from different classes. In authorship attribution, for example, the documents 
by two authors, A1 and A2, are sometimes separated perfectly by this scheme. This constitutes a 
linear classification problem. In a linear classification problem, the numerical representations, or 
DFs, for all documents by some author are separated from the numerical representations for the 
documents by another author in such a way, that simple a line can divide in space the two sets of 
representations. An optimal linear separation is presented in Figure 4.1. 
 
  
 
 
Figure 4.1. A linear classification problem with two authors as classes
  
 
 
 
The kind of linear classification problem that is represented in Figure 4.1, where all numerical 
representations for the documents of different classes are separa
not always found in real-life data. In many cases, such as the one represented in Figure 4.2, the 
data offers a different kind of situation. In this figure, it can be seen that some of the events or 
documents belonging to one of the authors, 
A1. In these kinds of cases, there is not one line that could possibly divide all the events from 
both classes. This type of event distribution constitutes a non
  
191 
 
 
ted in space by a simple line, is 
A2, fall among the documents by the other author, 
-linear classification problem. 
 
 
Figure 4.2. A non-linear classification problem with two authors as classes
  
 
 
 
 There are a number of options to attempt to solve non
as the one represented in Figure 4.2. One of the preferred methods to target this kind of problem 
in the machine learning community is SVMs. It should be mentioned t
to all kinds of classification problems on their own, but have the added capability of dealing with 
non-linear problems quite efficiently. Considered to have state
widely used in information retrieva
linear problems, SVMs attempt to manipulate the dimensional space of the document 
representation. We can think of this process as similar to the industrial procedure in which 
different objects are separated by adding water to a separation tank in which the objects are 
placed. Before adding the water, all the objects are on the bottom of the tank. As the water is 
added, a two dimensional distribution of the objects acquires a third dimension, volume,
which gravity plays a roll. SVMs project the classification problem into higher dimensional 
192 
 
 
-linear classification problems, such 
hat SVMs are applicable 
-of-the-art performance, SVMs are 
l in general (Manning et al., 2008). In order to solve non-
 in 
 
193 
 
space to try to find a line or a plane (this is actually a hyperplane in a space with many 
dimensions) that divides accurately the events from different classes. The separation tank 
metaphor is represented in Figure 4.3 below. In this figure, we can see that the documents by 
author A2 which were “trapped” among documents by A1 can now be grouped with the other 
documents authored by the same subject.  
 The projection of the classification problem into higher dimensional space has both pros 
and cons (Witten et al., 2011). On the positive side, the trick to obtain a function that projects the 
problem to a new space can be quite simple. If we take the linear function DF in DA (DF = (w1 * 
f1) + (w2 * f2) + (w3 * f3)), and simplify it even more by removing one feature, we obtain the 
linear function F with two features, F = (w1 * f1) + (w2 * f2). Given the very simple function F, 
the individual multiplications of all possible three-element combinations of the two features ({ f1 
* f1 * f1}, { f1 * f1 * f2}, { f1 * f2 * f2}, { f2 * f2 * f2}) are used as input of a new function F
+1. 
Namely, a vector of n features is projected into higher dimensional space by simply creating a 
new vector with the products of all possible combinations of n+1 elements using possibly 
repeated n features (for more on combinations of n elements into sub-groups of m elements with 
repetition allowed see Epp, 2004). The simplification of the vector with the 4 possible 
combinations of 3 elements from 2 possibly repeated features gives formula (4.8), which has 
been adapted from Witten et al: 
 
(4.8) 
 F+1 = (w2 * f1
3) + (w2 * f1
2 * f2) + (w3 * f1 * f2
2) + (w4 * f2
3) 
 
 
194 
 
One of the issues that results from projecting a classification problem into higher dimensional 
space is shown in formula (4.8). The formula for F+1 requires to calculate four weights w and not 
only two as in the linear formula F described above. The kind of projection expressed in formula 
(4.8) is quite simple, but results in a proliferation in the number of feature coefficients. This 
happens because the space has been augmented and now there are many more parameters to 
handle in the classification task. In order to handle this problem, SVMs look for the maximum-
margin classification decision. The support vectors in Figure 4.3, which are represented by the 
dotted lines, identify the maximum margin in the separation between the two classes. Once these 
vectors are identified, the plane that is furthest away from any data point (the solid line in Figure 
4.3) is chosen as the classification criterion. 
 
  
 
 
Figure 4.3. Support vectors attempting
  
 
 
 
 As they do with NB, Manning et al. (2008) present 
ranking of search engine results and general text classification. These authors also discuss how 
SVMs can be used as a feature reducti
advanced topics on the various augmentations that can be made to SVMs to tune their 
performance. This project will use the Weka implementation of the sequential minimal 
optimization (SMO) algorithm for 
SMO version of SVMs in Weka will be used. The software implementation of this classification 
scheme will be referred from this point on simply as SVMs.
195 
 the maximum-margin decision 
 
SVMs in two relevant contexts, the 
on technique. Witten el al (2011) include a number of 
training a support vector classifier. All default values for the 
 
 
196 
 
4.3.3  Authorship Attribution Approach Configurations 
The five class attribution methods introduced in the last Section (C4.5, DA, MNB, NB, and 
SVMs) will be combined with the three feature reduction techniques presented in Section 4.1.1 
(frequency, IG, and CFS). These five classifiers will also be fed the whole feature list before any 
reduction. Although this is not common in authorship analysis studies (only one author profiling 
study does this, Zhang et al., 2009), presenting the results obtained without any feature reduction 
should make apparent the effects of using feature reduction techniques. As mentioned in Section 
3.1.2, the comparison between not using feature reduction and using feature reduction is 
especially relevant in the case of the use of frequency, where there is not an elaborate calculation 
of the statistical value or weight of both the features selected and the features eliminated from the 
classification task. Also, the comparison of results using reduced feature sets and the set of all 
features should show which classifiers benefit more from the use of feature reduction techniques 
and which ones can handle noisy features themselves in the context of the experiments 
conducted. The different options in the approach components mentioned at the beginning of this 
paragraph (five classifiers and four feature sets) render a total of 20 different configurations. 
Since results will be reported for experiments with and without normalization, there are 40 
experiments conducted on each of the corpora described in Section 4.1.2. Table 4.3 summarizes 
all these experiments. 
  
 
197 
 
Table 4.3 Approach configurations run on each experimental corpus 
Table 1 
                                  
Classifier C4.5 
Feature set all  
Freq 
 
IG 
 
CFS 
Normalized y n y n y n y n 
Config # 1   2   3   4   5   6   7   8 
Classifier DA 
Feature set all  
Freq 
 
IG 
 
CFS 
Normalized y n y n y n y n 
Config # 9   10   11   12   13   14   15   16 
Classifier MNB 
Feature set all  
Freq 
 
IG 
 
CFS 
Normalized y n y n y n y n 
Config # 17   18   19   20   21   22   23   24 
Classifier NB 
Feature set all  
Freq 
 
IG 
 
CFS 
Normalized y n y n y n y n 
Config # 25   26   27   28   29   30   31   32 
Classifier SVMs 
Feature set all  
Freq 
 
IG 
 
CFS 
Normalized y n y n y n y n 
Config # 33 34 35 36 37 38 39 40 
                                  
 
 
 
An important comment that should be made about the 40 experiments summarized in Table 4.3 
is that all of them will be cross-validated. First mentioned in Chapter 2, cross validation is a 
standard evaluation procedure in many areas of computational linguistics (Jurafsky and Martin, 
2008). Although there are various forms or cross validation, they all require dividing a corpus in 
training and testing samples. In a classification task, the training samples are used to create 
 
198 
 
statistical models of the different classes in the corpus, and the testing samples are classified 
based on the models created with the training data. When a testing sample is classified, it is not 
included in the training data. This process is repeated recursively until all the documents in the 
corpus are classified without ever using the testing units in the training set. Cross-validation is 
used to mitigate one common problem in the evaluation of predictive models, the limited 
availability of testing data (Witten et al., 2011). In the context of this project, one wants to know 
how well a specific approach, out of the 40 described in Table 4.3, will perform once it is applied 
on new data. If one sets apart a small proportion of the available data, and use it as the only test, 
there is the risk that this testing data is not representative. A small, non-representative testing 
data set with a particularly good or bad outcome will lead to misguided expectations regarding 
the performance of a given approach. On the contrary, if we use a significant portion of the data 
in our testing sample, for example 50%, we may not have enough information in the training set 
to build an efficient model. Using cross validation, all available data is tested at some point, and 
a large proportion of the data is always available to create the statistical model for each class. 
 In the 32 authorship analysis studies surveyed in former chapters, all experiments other 
than those in two studies (Grant, 2007; Spassova, 2009) have been cross-validated. As to the non 
cross-validated experiments in the two studies mentioned, I have argued elsewhere (Rico-
Sulayes, 2011) that it is impossible to generalize from them, because the authors do not attempt 
to predict if their non cross-validated results are actually representative of the whole corpus, 
something that in general cannot be done efficiently in this context (Witten et al., 2011). The 
solution to this problem is precisely a cross-validated design. 
 As was noted above, there are different forms of cross-validation. Two of the most 
common, which will be described here, are leave-one-out cross validation and n-fold cross 
 
199 
 
validation (Witten et al., 2011). In a leave-one-out cross validation, all documents but one are 
used as training data. The held out document is then classified comparing it to the rest of the 
documents. This general form of leave-one-out cross validation uses the most information 
available for training data. Leave-one-out is sometimes performed using a pairwise comparison 
of the classes. This means that in the classification of each testing sample, all the documents by 
two authors, but the one to be classified, are used as training data. The testing sample is then 
classified based on the models drawn from this data. The process is repeated until all documents 
by the two authors in the pair are classified without using any of them in its own classification 
(Burns and Burns, 2008). The other form of cross validation that will be used and described in 
this chapter is n-fold cross validation. In an n-fold cross validation, the entire corpus is divided 
into n number of parts. The documents in n-1 parts are used as the training data, and the 
documents in the remaining nth part are classified based on the models created with the former 
data set. The classification is repeated n times until the whole corpus is tested without having 
used any testing document during training (Jurafsky and Martin, 2008). Regarding the number of 
folds to use as the value for n, there has been an extensive experimentation with this value, and it 
has been shown that 10 is the best option to use for this number (Witten et al., 2011). Although 
using 10-fold cross-validation is not a must, it is in general considered the standard option in 
contexts with limited data, such as authorship attribution. 
 In this project, for all the experiments with DA I will use the default cross validation 
available with this classifier in its SPSS implementation, pairwise leave-one-out. For all other 
experiments, run in Weka, 10-fold cross validation will be selected. The only exception to this 
will take place when processing the corpus with only two authors, in which there are only eight 
documents in the entire data set. Since the number of folds cannot be larger than the number of 
 
200 
 
events in the corpus, an 8-fold cross validation will be used for the corpus with two authors. This 
8-fold cross validation for a corpus with 8 documents is also equivalent to a general leave-one-
out cross validation (Witten et al., 2011), because an eighth of the corpus in this case is equal to 
one document. In order to explain the selection of testing samples in n-fold cross validation, we 
can take as an example the corpus with 40 authors. Among the 160 texts in this corpus (in all 
corpora there are four texts per author), 16 documents are used as testing samples and 144 are 
used as training data in any given fold. For each fold, 16 different texts that have not been 
classified before are used for testing. As a general procedure, the testing documents are stratified 
per class (or per author in our context) (Jurafsky and Martin, 2008). This means that not more 
than one tenth (or 10%) of the documents by some author is selected during each fold. In the 
experiments conducted in this study, each author has only four documents, so each document 
represents 25% of the production of a given author. When single events are larger than a 
stratified nth-part of the corpus, they are used as the minimum unit. Therefore, in our example 
for the corpus with 40 authors, a total of 16 documents (one for each of 16 different authors) is 
used in each fold. 
  
 
201 
 
CHAPTER 5: ANALYSIS OF RESULTS 
The former chapter described the data used to build a total of 39 experimental corpora, which 
include an increasing number of authors from 2 up to 40. In Section 4.1, a number of options for 
the two main components of an authorship attribution approach (the feature set and the classifier) 
were also introduced. The combination of these options resulted in 40 different approach 
configurations. Applying these approach configurations to all the corpora results in the design of 
1,560 individual experiments. This chapter will present and discuss the outcome of these 
experiments in two parts. First, the figures obtained in the success rate of the experiments will be 
presented and a number of implications regarding these figures will be discussed. Second, the 
kind of linguistic information that has been used in the most successful configurations, namely 
the configurations that have achieved the highest success rates, will be discussed. This discussion 
will be aimed at interpreting the characteristics of the linguistic component of the most 
successful experiments conducted in this project. 
5.1  MEASURING RESULTS 
In order to analyze the results of the 1,560 experiments designed in Chapter 4, this section will 
begin discussing the meaning of the accuracy figures that will be given in the rest of this chapter. 
Second, the best results obtained by some configuration over all corpora will be presented, along 
with the best results for each of the 40 configurations designed. Third, the results obtained by a 
number of configurations over all corpora will be compared. In this comparison, the results by 
various sets of configurations will be plotted together in a number of graphs. This will permit the 
visualization of tens of dozens of experiments in each of these graphs. With these graphs a 
number of tendencies in the experiments, such as the superiority in performance by some 
 
202 
 
configuration, of the difficulty of some classifier to handle small data sets, should become 
apparent. Finally, the best results obtained in this project with the 10-author corpus will be 
compared to results I achieved previously (Rico-Sulayes, 2011) with essentially the same corpus. 
As will be further discussed in this section, the accuracy I obtained previously represents the best 
results that have been achieved using DA in an authorship attribution task. 
5.1.1  The Success Rate in the Experiments: Accuracy 
As to what results will be presented for each experiment, the proportion of true positives (TP), or 
the testing samples correctly classified as belonging to their true author, will be given. This 
proportion of TP is equivalent to the concept of accuracy. Accuracy is the most commonly used 
effectiveness measure in text classification (Manning et al., 2008). All but 4 authorship 
attribution studies surveyed in Chapter 3 use this measure as the central figure to evaluate their 
experiments. Elsewhere (Rico-Sulayes, 2011), I have discussed the problems derived from 
reporting various measures in authorship attribution. One of these problems derives from using 
the proportion of true negatives (TN), the correct classification of a testing sample as not 
belonging to an author other than its true author. This problem is also present when some other 
measure that relies on this figure is used. The issue with using this measure (or a derivative one) 
as the central result figure is that this measure tends to be too optimistic (Manning et al.). For any 
problem that has more than two classes (or authors), the number of TN is much larger, compared 
to the number of TP. For example, in the classification of some number of texts by ten authors, 
whenever the classification of a text renders one TP, it simultaneously gives nine TN. If the 
classification is wrong, there are zero TP, but still there are eight TN. This means that if a text is 
correctly assigned to its true author, it is tacitly not assigned to all the other subjects in the set of 
potential authors. If the assignment is wrong, the text is still correctly not assigned to all other 
 
203 
 
authors minus one. This is certainly right, but as the number of authors grows the measure of TN 
renders near perfect figures even when there are zero correct matches. In an experiment with 100 
authors, for instance, where no single text is correctly assigned to its true author, the proportion 
of TN is 0.9898 (i.e., 9,800 correct negative rejections divided by 9,900 possible ones). Two 
articles of the four formerly mentioned base the interpretation of their results on TN-related 
measures (Chaski, 2005; 2007). Another issue that I have discussed before in this research 
project and elsewhere (Rico-Sulayes, 2011) is the use of non cross-validated results. These types 
of results do not permit the researcher to generalize from them, because it is not feasible to 
determine if the samples left out of a cross-validated design are representative of the whole 
corpus (Witten et al., 2011). Two authorship attribution studies surveyed fail to understand or 
address this problem (Grant, 2007; Spassova, 2009). 
 In order to further exemplify the concept of accuracy as understood in this project, Table 
5.1 shows the results for an experiment with ten subjects and four testing samples per author. 
This kind of table is also known as a confusion matrix and it is part of the standard classification 
output in the graphical interfaces of both SPSS and Weka. Table 5.1 has been adapted from 
(Rico-Sulayes, 2011), and represents the best results obtained in this study using 27 structural 
and syntactic features processed with DA as the classifier. In this table, the proportion of TP 
obtained for each author are represented in the diagonal line formed by the cells with the same 
author in the x and y axes. Since all authors have the same number of testing samples, an average 
of these proportions is equal to the accuracy for the whole experiment. In the current research 
project, in which I have used a fully automated authorship attribution approach, one of my goals 
has been to at least match and ideally improve the accuracy of previous research using data from 
drug dealing related online forums and employing DA with a manually tagged feature set (Rico-
 
204 
 
Sulayes, 2011). Other studies that use manually tagged feature sets and DA are for example 
Chaski (2005; 2007). The best results previously obtained with data from drug dealing related 
online forums are shown in Table 5.1, adapted from Rico-Sulayes (2011): 
  
 
205 
 
Table 5.1 Confusion matrix, accuracy, and error rate for experiment with ten authors 
Table 15 
 Predicted author   
Actual 
author a b c d e f g h i j 
FP 
a .75    .25      .25 
b  .50       .50  .50 
c   1.0        .00 
d    1.0       .00 
e     1.0      .00 
f      1.0     .00 
g       1.0    .00 
h        1.0   .00 
i         1.0  .00 
j                   1.0 .00 
       accuracy   ER 
              .925   .075 
 
 
 
Table 5.1 also shows the proportion of false positives (FP), the testing samples incorrectly 
classified as belonging to an author other than their true author. The average proportion of FP per 
author is summarized in the right-most column of this table. Having an equal number of testing 
samples per class or author, an average of these proportions is equal to the error rate (ER). The 
error rate can also be calculated as one minus accuracy. Having an error rate is one of the 
desirable features of any tool aimed at assisting in the interpretation of evidence in a legal court 
in the United States (Solan and Tiersma, 2005; Howald, 2008). As mentioned in Chapter 3, 
under the Daubert standard, the courts in this country consider a known error rate as a criterion 
of admissibility for an expert opinion. As to the context of Mexican law, Chapter 1 mentioned 
that the Código Federal de Procedimientos Penales (CFPP) 239 considers the possibility of using 
expert testimony to verify the authenticity of written documents by the examination of ‘letters 
and signatures’ (2010). Although, the CFPP does not require a know error rate for this kind of 
 
206 
 
expert testimony, the scientific criteria established in the Daubert standard could help the 
acceptance of authorship attribution results in both criminal investigation and legal prosecution 
in Mexico. 
5.1.2  Results: The Effects of Classifier, Feature Reduction, and 
Normalization 
Table 5.2 shows the results obtained for the eight approach configurations that include MNB as 
the classifier. The table presents the results for these configurations on the 39 experimental 
corpora, with an increasing number of authors in each of them. The number of authors for each 
corpora is shown in the first column of the table. Table 5.2 shows all individual results for the 
configurations in the third row of configurations in Table 4.3, which include eight configurations 
from number 17 to number 24. The three parameters that define these eight configurations in 
Table 4.3 (classifier: MNB; feature set: all, frequency-selected, IG-selected, and CFS-selected; 
and normalized features: normalized and non-normalized) are reproduced on the top three rows 
of Table 5.2. In the bottom row of Table 5.2, an average for each approach configuration over all 
the experimental corpora is included. I have started presenting results with this table, because 
MNB has the approach configuration that averages the highest accuracy over all the corpora. 
This configuration uses MNB to processes a feature set reduced with IG (using a score greater 
than 0) and non-normalized values. The accuracy for this approach configuration over all corpora 
averages 0.947. The second and third best averaged results are also included in this table. Using 
MNB, a reduced set of features with an absolute frequency of 4, and non-normalized values, the 
second best averaged result in this project renders an accuracy of 0.942. With the same settings, 
but a reduced feature set obtained with CFS, the third best accuracy over all 39 corpora is 0.940. 
  
 
207 
 
Table 5.2 Results for all eight approach configurations with MNB as the classifier 
Table 15 
classifier MNB 
features all  
freq 
 
IG 
 
CFS 
normalized y n  
y 
 
n 
 y 
n 
 
y 
 
n 
2 1.000 
 
1.000 
 
1.000 
 
1.000 
 
1.000 
 
1.000 
 
1.000 
 
1.000 
3 1.000 
 
1.000 
 
1.000 
 
1.000 
 
1.000 
 
1.000 
 
1.000 
 
1.000 
4 0.938 
 
1.000 
 
1.000 
 
1.000 
 
1.000 
 
1.000 
 
0.750 
 
1.000 
5 0.900 
 
0.900 
 
1.000 
 
0.950 
 
1.000 
 
0.950 
 
0.950 
 
0.950 
6 0.958 
 
0.917 
 
1.000 
 
0.917 
 
1.000 
 
1.000 
 
1.000 
 
1.000 
7 0.857 
 
0.893 
 
0.964 
 
0.929 
 
1.000 
 
1.000 
 
1.000 
 
1.000 
8 0.875 
 
0.813 
 
0.969 
 
0.938 
 
0.969 
 
1.000 
 
0.938 
 
1.000 
9 0.861 
 
0.833 
 
0.972 
 
0.944 
 
0.972 
 
1.000 
 
0.833 
 
1.000 
10 0.825 
 
0.850 
 
0.950 
 
0.950 
 
0.900 
 
1.000 
 
0.775 
 
0.975 
11 0.864 
 
0.864 
 
0.955 
 
0.955 
 
0.909 
 
1.000 
 
0.909 
 
1.000 
12 0.896 
 
0.792 
 
0.979 
 
0.938 
 
0.833 
 
0.917 
 
0.854 
 
0.938 
13 0.885 
 
0.769 
 
0.923 
 
0.923 
 
0.808 
 
0.923 
 
0.692 
 
0.923 
14 0.893 
 
0.804 
 
0.929 
 
0.929 
 
0.804 
 
0.929 
 
0.750 
 
0.929 
15 0.900 
 
0.783 
 
0.950 
 
0.933 
 
0.833 
 
0.933 
 
0.667 
 
0.950 
16 0.859 
 
0.766 
 
0.953 
 
0.938 
 
0.719 
 
0.938 
 
0.563 
 
0.953 
17 0.838 
 
0.794 
 
0.941 
 
0.941 
 
0.721 
 
0.941 
 
0.691 
 
0.956 
18 0.875 
 
0.792 
 
0.903 
 
0.931 
 
0.736 
 
0.931 
 
0.708 
 
0.931 
19 0.842 
 
0.816 
 
0.961 
 
0.947 
 
0.816 
 
0.934 
 
0.789 
 
0.934 
20 0.838 
 
0.763 
 
0.900 
 
0.938 
 
0.825 
 
0.925 
 
0.800 
 
0.950 
21 0.833 
 
0.774 
 
0.905 
 
0.952 
 
0.786 
 
0.940 
 
0.762 
 
0.940 
22 0.875 
 
0.750 
 
0.943 
 
0.943 
 
0.761 
 
0.932 
 
0.693 
 
0.943 
23 0.804 
 
0.728 
 
0.902 
 
0.957 
 
0.761 
 
0.913 
 
0.652 
 
0.902 
24 0.823 
 
0.750 
 
0.906 
 
0.948 
 
0.677 
 
0.927 
 
0.563 
 
0.906 
25 0.770 
 
0.740 
 
0.870 
 
0.960 
 
0.790 
 
0.920 
 
0.650 
 
0.910 
26 0.798 
 
0.683 
 
0.856 
 
0.952 
 
0.683 
 
0.923 
 
0.587 
 
0.913 
27 0.787 
 
0.648 
 
0.852 
 
0.954 
 
0.657 
 
0.926 
 
0.546 
 
0.926 
28 0.795 
 
0.670 
 
0.884 
 
0.964 
 
0.571 
 
0.955 
 
0.420 
 
0.929 
29 0.802 
 
0.621 
 
0.914 
 
0.966 
 
0.578 
 
0.948 
 
0.457 
 
0.897 
30 0.817 
 
0.625 
 
0.892 
 
0.950 
 
0.467 
 
0.933 
 
0.325 
 
0.883 
31 0.815 
 
0.605 
 
0.887 
 
0.944 
 
0.460 
 
0.927 
 
0.339 
 
0.895 
32 0.797 
 
0.586 
 
0.875 
 
0.930 
 
0.492 
 
0.930 
 
0.359 
 
0.914 
33 0.811 
 
0.591 
 
0.894 
 
0.947 
 
0.515 
 
0.939 
 
0.409 
 
0.924 
34 0.794 
 
0.588 
 
0.882 
 
0.926 
 
0.507 
 
0.949 
 
0.382 
 
0.941 
35 0.750 
 
0.579 
 
0.879 
 
0.907 
 
0.414 
 
0.921 
 
0.343 
 
0.900 
36 0.764 
 
0.556 
 
0.861 
 
0.917 
 
0.347 
 
0.924 
 
0.299 
 
0.903 
37 0.784 
 
0.595 
 
0.872 
 
0.905 
 
0.426 
 
0.932 
 
0.365 
 
0.919 
38 0.776 
 
0.618 
 
0.855 
 
0.914 
 
0.428 
 
0.928 
 
0.375 
 
0.921 
39 0.782 
 
0.564 
 
0.872 
 
0.923 
 
0.391 
 
0.929 
 
0.295 
 
0.904 
40 0.781 
 
0.575 
 
0.838 
 
0.894 
 
0.438 
 
0.913 
 
0.338 
 
0.913 
                                
               
Average 0.843  
0.743 
 
0.920 
 
0.942 
 
0.718 
 
0.947 
 
0.637 
 
0.940 
                                
 
 
 
208 
 
The best results obtained with MNB are rendered using non-normalized values for the count of 
feature instances because MNB uses information about the skewedness in the frequency values 
of discriminatory features, which are usually frequent in the corpus. This information is not as 
important in other classifiers. For this reason, and given the fact that the corpora have been 
constructed controlling the size in both testing and training data, the impact of normalization in 
all other classifiers is minimal. Excluding MNB, the largest difference produced by 
normalization in the averaged accuracy for two configuration approaches that differ only in the 
inclusion or not of this procedure is 0.018. The average impact of normalization in all approach 
configuration pairs of this sort is 0.0009. Further examples about the effects of normalization in 
various approach configurations will be given below. 
 The averaged results over all corpora for the 40 approach configurations listed in Table 
5.1 are shown in Table 5.3. A number of observations can be made based on this table. First, this 
table shows that the classifier with the best results, MNB, renders accuracy figures that are 
noticeably above the ones obtained with the configurations that use C4.5, the proposed baseline 
classifier. In the configuration that renders the best results for each of these two classifiers, 
which is identical in all other options beyond the classifier itself (IG and non-normalized), the 
baseline classifier obtains a 0.671 accuracy, in contrast with the above mentioned 0.947 by 
MNB. Secondly, as was just noted, Table 5.3 shows that the effect of normalization is minimal in 
the classifiers other than MNB. In the baseline classifier, C4.5, the effect is zero for all its 
approach configurations. The largest impact of normalization is present in DA, and the range of 
this impact goes from zero to 0.018. NB presents a difference between zero and 0.004 in its 
accuracy, with and without normalization, in all of its approach configurations. As to SVMs, the 
difference in accuracy is between zero and 0.001. In the case of this last classifier, the minimal 
 
209 
 
impact of normalization is not surprising because the Weka implementation of this classifier in 
this project performs its own normalization process, as the Java code documentation of the 
classifier indicates. However, this is not the case for the former classifiers discussed. It should 
also be noted that normalization does not always imply an improvement or deterioration of the 
results. In the classifiers that reflect little impact on their accuracy after normalization, the 
direction of the change can have both outcomes. This is also the case with MNB, which 
improves its results with normalization when using all features, and decreases its accuracy when 
it combines this process with a feature reduction technique. 
  
 
210 
 
Table 5.3 Accuracy results for all 40 authorship attribution approach configurations  
Table 15 
                                
Classifier C4.5 
Feature set all  
Freq 
 
IG 
 
CFS 
Normalized y n y n y n y n 
Accuracy 0.456   0.456   0.489   0.489   0.671   0.671   0.660   0.660 
Classifier DA 
Feature set all  
Freq 
 
IG 
 
CFS 
Normalized y n y n y n y n 
Accuracy 0.296   0.313   0.460   0.466   0.699   0.700   0.826   0.811 
Classifier MNB 
Feature set all  
Freq 
 
IG 
 
CFS 
Normalized y n y n y n y n 
Accuracy 0.843   0.743   0.920   0.942   0.718   0.947   0.637   0.940 
Classifier NB 
Feature set all  
Freq 
 
IG 
 
CFS 
Normalized y n y n y n y n 
Accuracy 0.831   0.831   0.819   0.821   0.850   0.848   0.816   0.820 
Classifier SVMs 
Feature set all  
Freq 
 
IG 
 
CFS 
Normalized y n y n y n y n 
Accuracy 0.456 0.457 0.828 0.829 0.775 0.775 0.726 0.726 
                                
 
 
One more thing that can be seen in Table 5.3 is the feature set that renders the best results for 
each classifier. The best results rendered with C4.5, MNB, and NB are obtained when using the 
feature set selected with an IG score greater than zero. DA obtains its best results with CFS, and 
SVMs do with frequency. This agrees with the results shown in Zhang et al. (2009), the only 
study formerly surveyed that shows its results with and without a feature reduction technique. In 
this study, Zhang et al. report a great accuracy improvement after using a reduced feature set. In 
 
211 
 
this project, all classifiers also render their best results using a reduced feature set. After 
including some form of feature reduction, the five classifiers tested in this project (C4.5, DA, 
MNB, NB, and SVMs) obtain their best accuracy improving their best results with all features in 
21.5%, 51.2%, 10.4%, 1.9%, and 37.3%, respectively.  
 Finally, an important note should be made regarding the number of experiments that were 
run to obtain the figures reflected in Table 5.3. As mentioned before, individual experiments 
were designed applying the 40 approach configurations outlined in this table to 39 corpora, 
which results in 1,560 experiments. However, 58 experiments were not successfully completed. 
These are the experiments for DA, with and without normalization, using all features, and the 
corpora with 12 up to 40 authors. These experiments could not be completed because of the 
memory usage restrictions of SPSS version 20. Even after resetting the allocation of memory in 
SPSS to its maximum (2 GB), the program crashes after attempting to handle more than five 
thousand variables, with over 50,000 observations or cells. Although these numbers are not 
particularly large, this software may actually collapse due to its massive production of result 
details. Just for 10 experiments using between 2 and 11 authors and the whole set of features, 
SPSS produces 160 MB of results. Implemented in Java, Weka offers the possibility of 
interacting directly with and changing its code source. Given these options, this program has no 
restrictions on memory allocation or data manipulation other than what the hardware it is 
installed on can handle. Due to the problems with the software implementation of DA, the results 
shown for this classifier when combined with all features are the averaged accuracy of only ten 
successful experiments. As to the experiments with reduced feature sets, all of them could be 
handled by SPSS. 
 
 
212 
 
5.1.3 Comparison of Authorship Attribution Configurations over All 
Corpora 
In an attempt to help in the interpretation of results obtained with the various configuration 
approaches, the figures that will be presented below will plot accuracy against the number of 
authors in each experimental corpus. Following this arrangement, Figure 5.1 shows the 
performance of the MNB classifier, as different features sets are used on non-normalized data. 
This figure includes the three approach configurations (the ones with some feature reduction) 
that had the best performance in this project over a wide range of values for the number of 
subjects in the set of potential authors.  
  
 
213 
 
Figure 5.1 MNB, non-normalized data, and all reduction techniques, over 39 corpora 
  
 
0.000
0.100
0.200
0.300
0.400
0.500
0.600
0.700
0.800
0.900
1.000
2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40
A
cc
u
ra
cy
Number of authors in closed set of subjects
all Features
Absolute Freq = 4
IG > 0
CFS
 
 
 
There are at least three things that can be more easily appreciated in Figure 5.1, compared to the 
tables formerly discussed. First, this figure shows a clear improvement in the accuracy of MNB 
when using any feature reduction technique. Second, although there are differences in accuracy 
in the three configurations that use a feature reduction, especially in the corpora with fewer 
subjects, they all are quite consistent in their performance over all experimental corpora. In third 
place, the best averaged accuracy performance reported (the line that represents IG in Figure 5.1) 
remains consistently above 0.9 in the entire set of corpora. In contrast with MNB, which clearly 
benefits from the use of feature reduction techniques, plotting the performance of NB with all 
 
214 
 
feature sets shows how this classifier is consistent in its results regardless of what feature set it is 
given. The performance consistency of NB across all feature sets can be seen in Figure 5.2 
below. This has a two-fold interpretation in the context of the results reported. On one side, NB, 
like some other machine learning classifiers (Manning et al., 2008), seems to be able to handle 
noisy or redundant features quite efficiently. On the other side, these results also highlight one of 
the advantages of feature reduction, which is the possibility of containing all relevant 
information in a much smaller set. Containing far fewer features, reduced sets become more 
portable and less computationally expensive to process. These characteristics are useful when 
there are computational restrictions, such as the ones formerly mentioned for SPSS version 20. In 
this research project, applying reduction techniques made it possible to run experiments for all 
corpora using DA in SPSS. As mentioned before, this was not possible using the set with all 
features, at least not for corpora with 12 authors or more.  
  
 
215 
 
Figure 5.2 NB, non-normalized data, and all reduction techniques, over 39 corpora 
  
 
0.000
0.100
0.200
0.300
0.400
0.500
0.600
0.700
0.800
0.900
1.000
2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40
A
cc
u
ra
cy
Number of authors in closed set of subjects
all Features
Absolute Freq = 4
IG > 0
CFS
 
 
 
Another important pattern that becomes apparent in Figure 5.2 is the tendency in the accuracy of 
individual experiments to drop as the number of subjects in set of potential authors increases. 
This tendency was not evident in MNB when using feature reduction techniques, but it could be 
seen in the line that represents the combination of this classifier and the set of all features in 
Figure 5.1. The tendency in the accuracy of individual experiments to drop as the number of 
authors increases is also present in SVMs. The results for this classifier are not shown in an 
individual figure in order to save space.  
 
216 
 
 One last figure addressing the performance of a classifier across various reduction 
techniques is worth showing. Figure 5.3 plots the performance of DA on non-normalized feature 
sets. This figure shows results for 39 corpora with reduced sets, and a hard to distinguish line for 
10 experiments using all feature sets. What is interesting to see in the behavior of DA is that this 
classifier seems to have difficulty in handling very small amounts of data in the experiments 
conducted. For all three feature reduction techniques, in the experiments with less than 14 
authors there is a wide inconsistency in the performance of the classifier. After reaching some 
minimum number of authors, and most likely a certain amount of data, the performance of the 
classifier is much better and stable. At least in the experiments conducted in this project, 
depending on the feature reduction technique, DA reaches a more stable performance after it is 
given a minimum number of authors that is between 14 and 20, i.e., between 28,000 and 40,000 
words. About this observed behavior in DA, it is important to mention that this tendency in the 
experiments of this project is not necessarily a general characteristic of this classifier. In the 
experiments I have conducted elsewhere with DA (2011), this classifier exhibited a performance 
tendency like the one of NB or SVMs in this project. Although individual results for corpora 
with different numbers of subjects were not reported in the paper just cited, the accuracy attained 
by this classifier dropped from a perfect classification to 0.925 as the number of subjects grew 
from 2 to 10. In those experiments, however, two built-in feature reduction techniques for the 
DA implementation in SPSS were used. This highlights the fact that results in authorship 
attribution must be interpreted within the scope of the experiments reported, and generalizations 
should be considered with caution until more experimentation in the field is conducted. 
  
 
217 
 
Figure 5.3 DA, non-normalized data, and all reduction techniques, over 39 corpora 
  
 
0.000
0.100
0.200
0.300
0.400
0.500
0.600
0.700
0.800
0.900
1.000
2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40
T
ru
e
 P
o
si
ti
v
e
s 
R
a
te
Number of authors in closed set of subjects
all Features
Absolute Freq = 4
IG > 0
CFS
 
  
 
 Compared to the arrangement of information in the two previous figures, in Figure 5.4 
below there is a change in the focus of the information presented. Instead of plotting the results 
obtained by some classifier against all feature sets, this table shows the results of a feature 
reduction technique, frequency, when combined with all classifiers. It should be noted that 
Figure 5.4 shows the results for frequency as the feature reduction technique, and this reduction 
technique rendered the second best results obtained by MNB over all corpora. However, plotting 
the performance of all classifiers with frequency as the reduction technique (and non-normalized 
 
218 
 
data) shows the maximum separation achieved between the classifier with the best results, MNB, 
and the rest of them. Also, when using non-normalized data, the difference in accuracy over all 
corpora between MNB with IG (the configuration with highest accuracy rate) and MNB with 
frequency (the configuration with the highest accuracy in Figure 5.4) is just (0.5%). Taking all 
this into consideration, the fact that MNB outperforms all other classifiers is especially visible in 
Figure 5.4. 
  
 
219 
 
Figure 5.4 Feature set reduced with frequency, non-normalized data, and all classifiers, over 39 
corpora 
  
 
 
 
 
As it can be seen in Figure 5.4, in the experiments conducted with all 39 corpora, MNB performs 
better that any of the other classifiers. Something similar is observable if the other two feature 
reduction techniques are plotted against all classifiers with non-normalized data, although the 
superiority of MNB with other reduction techniques is slightly less noticeable. Actually, there 
are very few exceptions in which a configuration with another classifier and a feature reduction 
technique outperforms MNB. Only in 4 corpora (with 5, 12, 13, and 14 subjects), SVMs 
0.000
0.100
0.200
0.300
0.400
0.500
0.600
0.700
0.800
0.900
1.000
2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40
T
ru
e
 P
o
si
ti
v
e
s 
R
a
te
Number of authors in closed set of subjects
C4.5
DA
MNB
NB
SVMs
 
220 
 
combined with CFS perform better than the equivalent MNB configuration. In order to save 
space, the plotting of these other feature reduction techniques with all classifiers will not be 
included here. 
5.1.4 The Effects of Automated Tagging and Machine Learning Classification 
In order to finish with the analysis of results, the results of the fully automated tagging process 
implemented in this research project have to be compared to the results I previously obtained 
using manually tagged features (Rico-Sulayes, 2011). As was discussed above, the figures 
obtained in the just cited paper represent the best results that have been obtained using DA in an 
authorship attribution task. Therefore, those results were set at the very beginning of this 
research project as a reference point to match or improve when switching to an all automatic 
tagging process. With the accuracy I obtained previously (Rico-Sulayes, 2011) included in its top 
row, Table 5.4 shows the best accuracy obtained in the corpus with ten authors by the different 
classifiers tested in this project. For the just cited paper, the results shown in Table 5.4 were 
obtained using DA combined with either Mahalanobis or Wilk’s Lambda for feature reduction, 
and 27 syntactic and structural features. As to the corpus used in the current project, it should be 
mentioned that it includes the same ten authors used in the paper just cited. In the current 
research project, however, the data in a few of the testing samples have been rearranged in order 
to have a more balanced distribution in the number words for all testing samples. 
  
 
221 
 
Table 5.4 Best results obtained with different classifiers in the corpus with 10 authors 
Table 15 
Approach configuration Accuracy ER 
Rico-Sulayes (2011) 0.925 0.075 
C4.5, CFS, normalized / non- 0.800 0.200 
DA, IG, normalized / non- 0.875 0.125 
MNB, IG, non-normalized 1.000 0.000 
NB, all sets, non-normalized / IG 
and CFS, normalized 0.900 0.100 
SVMs, IG, normalized / non- 0.950 0.050 
      
 
 
As Table 5.4 shows, in this project, three configurations with two different classifiers have 
improved the results I previously obtained. These three configurations are MNB combined with 
IG and non-normalized features, and SVMs with IG and either normalized or non-normalized 
features. About the whole set of 40 different approaches, whose configurations are shown in 
Table 4.3, and whose best averaged results for all corpora were presented in Table 5.3, there are 
several of them that can match or improve previous results with ten authors. Table 5.4 above 
shows already three of these approach configurations. Below, Table 5.5 shows all approach 
configurations matching or improving the results in Rico-Sulayes (2011), in an experiment with 
ten subjects. 
  
 
222 
 
Table 5.5 Approach configurations matching or improving previous results with DA  
Table 15 
Approach configuration Accuracy ER 
Rico-Sulayes (2011) 0.925 0.075 
MNB, freq, normalized 0.950 0.050 
MNB, freq, non-normalized 0.950 0.050 
MNB, IG, non-normalized 1.000 0.000 
MNB, IG, normalized 0.975 0.025 
SVMs, IG, normalized 0.950 0.050 
SVMs, IG, non-normalized 0.950 0.050 
SVMs, CFS, normalized 0.925 0.075 
SVMs, CFS, non-normalized 0.925 0.075 
      
 
  
As can be seen in Table 5.5, there are a total of eight different approaches with equal or better 
results than what I previously obtained. Also, given the observed tendency in the accuracy to 
deteriorate as the number of classes or authors increases (this tendency was observed in all 16 
configurations with NB and SVMs), it is relevant to mention that there are a few configurations 
not shown in Table 5.5, which obtain an accuracy below 0.925 in the corpus with 10 authors, but 
improve this number in a few experiments with more than 10 authors. Regardless of this last 
comment, the eight approach configurations in the table above are enough to show that the goal 
of matching and improving previous results using a fully automated system, which relies 
exclusively on manually tagged features, has been achieved. 
5.1.5 Summary of Results 
In the first part of this chapter, Section 5.1.1 began with a brief introduction of the concepts that 
lie behind the result measures reported in this project. Then, Section 5.1.2 presented the best 
results obtained in this project. Since these results have been obtained with non-normalized data, 
 
223 
 
an explanation about why this kind of data is important for the classifier with the best 
performance has been offered. Also, the minimal impact of normalization in the rest of the 
approach configurations has been shown. This minimal impact has been highlighted to argue that 
the balanced design in testing and training samples has made document size have very little 
influence on the experiment results. This balanced design is attained by having as little variation 
as possible in the text length of testing samples. Section 5.1.3 has also identified and shown a 
tendency in the machine learning algorithms to decrease their accuracy as the number of authors 
increases. In contrast with this tendency, an unstable performance in the DA classifier when 
applied to corpora with fewer authors and an eventual improvement when exposed to more data 
has also been pointed out in this section. Since this tendency was not observed in previous work 
conducted with this classifier and other feature reduction techniques, a word of caution has been 
made about the interpretation of these results. As to the classifier with the best averaged 
accuracy, Section 5.1.3 has also provided data showing a consistent, superior performance by 
MNB in combination with any feature reduction technique. Finally, in Section 5.1.4, a number of 
the proposed approach configurations have been shown to match and improve the accuracy that 
was previously obtained with a feature set requiring manual tagging. Since these configurations 
are applied to a feature set with a fully automated tagging process, a considerable improvement 
in the efficiency of the authorship attribution approach has been achieved. 
5.2 LINGUISTIC COMPONENTS OF AUTHORSHIP ATTRIBUTION EXPERIMENTS 
In order to wrap up this chapter, this section will examine what features rendered the best results 
obtained in this project. This section will also discuss which of these features, if any, were 
proposed in this project as novel features. In an attempt to identify the features that contributed 
the most to obtaining the best results, a review of the features used across the multiple 
 
224 
 
experimental corpora will be made for the approaches than rendered these results. Beyond the 
formerly presented categorization of features (structural, syntactic, and lexical), this section will 
further characterize, from a linguistic point of view, the features that were the most useful in the 
various authorship attribution corpora. This section will not emphasize the three-category 
division of features just mentioned for two main reasons. First, as was shown in Section 3.1.1,  
all the studies that have tested the performance of separate, category-based feature subsets in the 
attribution have always obtained their best results when combining all their features. Therefore, 
the attribution configurations used throughout this project did not have any category-related 
information. Second, as mentioned in Section 4.3, the three-category division used in that section 
had only presentational purposes. In order to discuss what features contributed the most to 
obtaining the best results in this project, this section will both show numerical data regarding the 
features used in the most successful experiments and provide an interpretation of the kind of 
linguistic information that characterizes the features identified as the most useful. 
 As was mentioned in Section 5.1.2, the best averaged accuracy rendered by an approach 
configuration was obtained using MNB, IG, and non-normalized data. In addition, as discussed 
in this same section, the three best averaged accuracy figures produced in this project were 
obtained by this same classifier and the three reduced feature sets (frequency, IG, and CFS). 
Similarly, the best results obtained by all other classifiers (C4.5, DA, NB, and SVMs) were 
rendered by combining one of these classifiers and a feature reduction technique. C4.5, MNB, 
and NB had their best performance over all corpora using IG, DA did it with CFS, and SVMs 
with frequency.  In the comparison between the results for the five classifiers in this project and 
the accuracy previously achieved in a similar task with DA (Rico-Sulayes, 2011), the best results 
for each of the five classifiers when applied to the corpus with ten authors are also obtained with 
 
225 
 
a feature reduction technique. When applied to the corpus with ten authors, DA, MNB, and 
SVMs gave their best results with IG, C4.5 with CFS, and NB with both techniques. Similarly, 
the configurations that achieved a performance equal or better than my previous work in 
authorship attribution used some reduction technique. Four out of the eight approaches that 
achieved this goal used IG, two approaches employed CFS, and the other two used frequency. 
Examining the results in all the above mentioned experiments, it becomes apparent that the 
reduction technique that most commonly rendered the best results was IG, while CFS was the 
second most useful technique. Since the two techniques that were the most productive in terms of 
rendering the best results are also the ones that have relatively small features sets, the features 
selected by these techniques in all 39 corpora will be discussed in this section. 
 In the paragraph above, it was mentioned that IG and CFS use relatively small feature 
sets. In order to support this comment, Table 5.6 shows the size of all feature sets (which include 
either all features, or a set reduced with frequency, IG or CFS) in the various experiments 
conducted. Observing the size of the four feature sets for each of the 39 corpora, it is possible to 
observe in this table that there are two different patterns in the size of the different features sets 
used. On one side, the set of all features and the set reduced based on the features’ absolute 
frequency grows as the number of authors in the corpus increases. The set of all features grows 
from 1,450 features in the corpus with two authors to 13,156 features in the corpus with 40 
authors. With a very similar growth pattern, the set of features reduced using frequency increases 
from 229 features in the smallest corpus to 2,169 features in the largest one. On the other side, 
the feature sets produced by IG and CFS do not show an increase in their number of elements. 
On the contrary, in the case of IG there seems to be a tendency in the feature set produced with 
this technique to get smaller as the number of authors in the corpus increases. As it can be seen 
 
226 
 
in Table 5.6, the feature set rendered by IG reaches a maximum of 79 features in the corpus with 
three authors and contains only 24 features in the set with 40 authors. Finally, CFS is quite stable 
and for each individual corpus produces a set with a size that is close to the average size of the 
feature set in all corpora, which is 19 features. 
  
 
227 
 
Table 5.6 Size of feature sets in experimental corpora 
Table 15 
Corpus size, 
# authors all freq IG CFS 
2 1450 229 56 23 
3 1894 324 79 7 
4 2356 384 75 10 
5 2811 438 71 25 
6 3315 501 50 19 
7 3657 570 52 24 
8 4090 625 46 23 
9 4390 683 46 18 
10 4659 734 45 17 
11 4996 791 46 19 
12 5332 841 46 24 
13 5706 900 42 20 
14 6009 975 43 20 
15 6316 1017 36 18 
16 6579 1067 39 14 
17 6891 1118 30 20 
18 7130 1172 31 20 
19 7482 1235 31 23 
20 7776 1290 37 27 
21 8047 1334 33 23 
22 8307 1387 32 21 
23 8502 1422 30 19 
24 8765 1453 29 20 
25 9041 1495 33 24 
26 9200 1547 29 20 
27 9446 1589 30 21 
28 9703 1635 28 19 
29 9960 1683 26 17 
30 10240 1736 24 17 
31 10472 1786 26 17 
32 11167 1824 27 20 
33 11421 1858 26 20 
34 11704 1901 26 20 
35 11874 1943 24 18 
36 12063 1992 24 17 
37 12352 2047 24 20 
38 12604 2091 23 20 
39 12881 2128 23 17 
40 13156 2168 24 18 
          
 
 
 
228 
 
 Since IG and CFS have consistently smaller feature sets, when compared to the sets given 
by the other selection criteria, the accumulated list of features they use in all of the experimental 
corpora is not very long. Putting together the features produced by IG in the 39 corpora and 
collapsing the features that get used several times across all experiments, the accumulated list of 
all features selected by IG is shown in Appendix C. In this list, there are a total of 144 features 
(125 lexical, 2 syntactic, and 17 structural). Among these features, 43 are selected in 10 or more 
out of all 39 corpora. A list of accumulated features chosen by CFS is presented in Appendix D. 
Producing even smaller feature sets with all corpora, if compared to IG, CFS selects 65 features 
across all experiments. Among these features there are 46 lexical, 2 syntactic, and 17 structural. 
Out of all 65 features in Appendix D, 25 features get selected in 10 or more corpora. Further 
examining the similarities between the lists of accumulated features in Appendices C and D, it is 
possible to find out that all features chosen by CFS are also selected by IG. Table 5.7 below 
presents the 25 features selected by both IG and CFS for all experiments with 10 or more 
authors. 
  
 
229 
 
Table 5.7 Features selected by IG and CFS in experiments with 10 or more authors 
Table 15 
Features selected by IG and 
CFS in 10+ corpora 
Gloss Category 
bienvenido welcome lexical 
CommentBold (bold font) structural 
CommentFontColor structural 
Ellipsis lexical 
EOPpunctuation (punctuation before return) lexical 
FinalCommaMark (comma) lexical 
FinalPunct!Mark (exclamation point) lexical 
FinalPunct.Mark (period) lexical 
Image structural 
jaja hehe lexical 
ke that (netabbrev) lexical 
más more lexical 
porQue because of which, because [sic] syntactical 
que that lexical 
RedupliPunct2 
(series of n reduplicated 
punctuation marks) 
structural 
RedupliPunct3 structural 
RedupliPunct4 structural 
RedupliPunct6 structural 
RedupliPunct7more structural 
ReplyMessage structural 
Return structural 
saludos regards lexical 
EmoticonImage structural 
stressedO lexical 
zetas z's, (drug cartel) lexical 
      
 
 
Among the 25 features presented in Table 3.7, there are 13 lexical, 1 syntactic, and 11 structural.  
Table 3.7 is interesting because it shows the most common features shared by the two feature 
reduction techniques that rendered the best results in this project, IG and CFS. In the rest of this 
chapter, I will discuss and give examples about two more aspects of all the features selected by 
either IG or CFS, which are shown in Appendix C and D, respectively. First, I will mention 
 
230 
 
which of these features represent novel features that were proposed in this project. Second, I will 
offer an interpretation of the kind of linguistic information that characterizes the features selected 
by any of the two feature reduction techniques that rendered the best results here. Regarding this 
interpretation of the linguistic information in Appendix C and D, it should be mentioned that 
some of the features in these appendices are not linguistic, but paralinguistic. Following one of 
the pioneers of the concept of paralanguage (Crystal, 1975), these paralinguistic features lie 
somewhere between purely non-linguistic forms that communicate information and traditional 
forms of linguistic study. In Table 3.7, for example, the use of special font colors is a feature that 
combines the non-linguistic use of color (which is used to communicate information in fields 
such as graphic design) and the use of words (which are conventionally used to express linguistic 
meaning). Although in-depth studies of paralanguage are usually focused on the analysis of 
sound (e.g., Poyatos, 1993, which studies paralinguitics features such as laughter and sighing), 
there are characteristics of writing, such as layout and spacing, that are also considered 
paralinguistic (Crystal, 1975). 
 As to the novel features proposed in this project, examining Appedix C and D it is 
possible to find that a great number of these novel features get selected by the two reduction 
techniques that render the best results achieved. Among the novel lexical punctuation marks (or 
rather auxiliary spelling marks, according to Real Academia Española, 2005), two of the features 
proposed for stressed vowels (“á” and “ó”), along with the generic feature for any vowel with a 
diacritic, get selected by both techniques. Also, the two features suggested to address the 
ambiguity of the return carriage in reflowable text, the return carriage in html and the presence of 
any punctuation preceding the former feature were both selected. From the list of multi-unit 
function words proposed, two multi-word conjunctions were also chosen (por que ‘because of 
 
231 
 
which’ and para que ‘for which’). As to the series of reduplicated punctuation marks proposed in 
(Rico-Sulayes, 2011), the paper that was the origin of this project, all six features (for sequences 
of 2, 3, 4, 5, 6, and 7 or more marks) were picked up by both IG and CFS. Two more novel 
structural features, which introduced a distinction in hyperlinks (clickable and non-clickable) 
were also chosen. This means that novel features from all categories proposed in this project 
were useful not only in one, but in the two most productive types of features sets. If we further 
consider the list of accumulated features selected by IG across all experiments, even more novel 
features proposed in this project are also chosen. In the accumulated list of features for the 
reduction technique that renders the best results, all features for stressed vowels get selected 
(including “é”, “í”, and “ú”). 
 Summarizing the last paragraph, between Appendix C and D, all but one feature among 
the novel ones proposed in this project as lexical features, a total of seven, get selected. These 
include features for five stressed vowels, for any vowel with a diacritic, and for the presence of 
punctuation before the carriage return in reflowable text -- although the feature for the carriage 
return itself was considered structural, as mentioned in Section 4.3.1a, the feature for the 
presence of punctuation before this carriage return was classified as lexical because this 
punctuation character was split from the latter feature without a further analysis of its function. 
Also, two syntactical features of the novel multi-unit function word list proposed are chosen. 
Finally, all novel structural features proposed, a total of nine, get selected. These include six 
features for reduplicated punctuation, two for hyperlinks, and the carriage return in html. Given 
these numbers, it becomes apparent that the proposal of novel features has been a productive 
exercise and has contributed noticeably to achieve the best results in this project. With these 
 
232 
 
results, it has also been possible to match and improve the accuracy in my previous work with 
DA while using a fully automated authorship attribution approach. 
 As to the motivation behind the proposal of these novel features, I should emphasize that 
I came up with most of them after reading the contributions by each of the users included in the 
various tested corpora; something I mentioned at the beginning of this project in Section 1.1, and 
later again in Section 4.1.2. I proposed the majority of the features listed in the last two 
paragraphs because of their peculiar use among the online forum contributors and my own 
intuitions about the potential reasons for that peculiarity. The only novel features that I did not 
propose based on my readings of the documents in the corpora are the multi-unit function words, 
which were proportionally less successful (only 2 out 132 were selected by feature reduction 
techniques). Among the successful novel features I included, I proposed features for individual 
stressed vowels because I noticed that some contributors did not use stress marks at all, while 
others did employ stress marks but not in a completely normative way. In Spanish, there are just 
four simple rules for writing stressed vowels (Real Academia Española, 2005), but they get 
complicated because of their multiple exceptions and their implied knowledge about how to 
divide words in syllables. Because of these complications, I assumed that some users may write 
stressed vowels employing mnemonic shortcuts of the type “all words that end in ción or sión 
‘tion’ have a stressed ó”. If so, different users may stress vowels utilizing a combination of actual 
rules and these type of mnemonic shortcuts, depending on their educational level and personal 
ability/preference to use stress marks in an online forum. In the case of other novel features, such 
as the ones for reduplicated punctuation, I thought these may be influenced rather by habits. For 
example, different users may habitually use series of 2, 3, 4, 5, 6, or 7 or more exclamation 
marks. Regarding the features connected to the carriage return, I thought they may be linked to 
 
233 
 
personal preferences for the visualization of text. Similarly, I though that personal preference 
regarding the look of the text,  combined with the way that people copy and paste links, may be 
the reason for the different uses of hyperlinks. As I have acknowledged, these were only 
intuitions, and this research project is not aimed at proving any of them.The goal in the proposal 
of novel features in this project was to improve the performance of an authorship attribution 
approach through the use of potentially discriminative authorship variables. The accomplishment 
of this goal has already been explained in the last two paragraphs. At the same time, however, I 
think it is important to emphasize that these successful novel features have been proposed based 
on an consideration of their linguistic or paralinguistic functionality, and they are not randomly 
chosen variables that happened to be effective in the task. 
 In order to wrap up this chapter, I also want to offer an interpretation of the kind of 
linguistic or paralinguistic information that was used in the most successful experiments in this 
project. Examining carefully the features included in Appendix C and D, there are a number of 
patterns that are worth discussing. First, following the study of the conversational structure of 
email discussions by Harrison (1998), there are a number of features identified in this section 
that can be seen as representative of various constructs that have been developed in the general 
study of the structure of conversation. Among the concepts introduced by Stenström (1994) in 
her theoretical framework of spoken interaction, moves are the speaker’s actions that attempt to 
start, carry on and finish a conversational exchange. This author further divides her concept of 
moves in eight types: summons, focus, initiate, repair, response, re-open, follow-up, and 
backchannel signals. These eight types of actions are aimed at calling the attention of the 
listener, introducing an exchange, opening it, holding it up, continuing or terminating it, delaying 
its end, closing it, and signaling the listener’s attention. Stenström further divides these moves 
 
234 
 
into acts, which are the smallest units of conversational interaction. Representing the 
communicative intention of the speaker, in Stenström’s framework there are 28 types of acts. 
These acts are made up by elements that express agreement/disagreement, confirm the reception 
of information, close a conversation, respond to a statement, greet or bid farewell to somebody, 
among other possible communicative intentions. Although these concepts were developed in and 
applied to the study of the structure of conversation, it is not surprising that online forum posts 
present linguistic items that exemplify them. As was mentioned before, a first application of 
Stenström’s theory to electronic communications was done by Harrison (1998) in the context of 
email discussions. It was for this reason that I selected Stenström’s linguistic framework for 
conversational analysis and applied it to another form of electronic communications, which is 
online forums in this project. It is worth mentioning, however, that there are other influential 
theories that analyze conversation in the field of linguistics. For example, “speech act theory 
(Austin, 1962; Searle, 1969; 1975), is a well established field of linguistic inquiry (Vanderveken 
and Kubo, 2001) that studies how utterances and sequences of utterances and are used to perform 
social actions; various other examples of linguistic approaches to the study of conversational 
structure (as well as how people shape discourse to make personal and social meanings) can be 
found in the comprehensive volume by Schiffrin, Tannen, and Hamilton (2001). 
 Considering online forums as a form of conversational text is not a not a new idea either. 
Taking into account Owyang (2008)’s characterization of online forums, this form of social 
media has as one of its main goals the construction of a multy-party conversation. For Owyang 
online forums are a form of communication among many users with equal status. In contrast with 
blogs, in which the “speaker” or blogger guides the interaction between him or her and some 
audience (Miller and Shepherd, 2004; Owyang, 2008), online forums and their threads are 
 
235 
 
intended to promote a conversation among many users. They are “social mixers” in Owyang’s 
words. For this reason, this form of social technology is especially prone to have features that 
represent different parts of the structure of conversation, such as moves and acts. The 
contribution of this study in this respect will not be to show that these examples of 
conversational moves and acts are present in online forums, but that they can be represented by 
items that are also useful features in an authorship attribution task.  
 In the lists of accumulated features selected by IG and CFS, there are nine features that 
represent various moves or acts. About these features, it should be mentioned that several of 
them are words that have several possible meanings or senses. In one or more of these senses, 
they can exemplify some kind of move or act. Before deciding to consider these features as items 
that represent a given type of move or act, I looked at the text samples of users in which these 
features were always present and were also frequent. Namely, I looked at the users’ posts where 
these features render high IG scores. After I found with what sense these words were actually 
used, I included them in the discussion below. An authorship feature that represents some form 
of move (a response) is the feature reply messages, messages that respond to other user’s 
comment. Reply messages usually include the message commented on in the reply, and they are 
a common function in online forums and other forms of electronic communication, such as 
email. The feature for reply messages was selected in 35 experiments in both IG and CFS. 
Representing two specific forms of conversational acts, showing agreement and disagreement, 
the unigram, or lexical feature, for cierto ‘true’ often represents both acts. In the following two 
examples taken from online forum users where this form renders a high IG score, these 
comments are directed to a specific user’s comment: “pero es cierto me meto al chat and...”, ‘it is 
true, when I join the chat...’; “No es cierto, el blog es para informar”, ‘It isn’t true, [the purpose 
 
236 
 
of] the blog is to inform’. Two separate unigrams, buena ‘good’ and información ‘information’ 
also get selected as they are often used together (buena información ‘good information’) to both 
signal the reception of information  posted by others and positively evaluate their contributions. 
Both communicative intentions represent forms of conversational acts for Stenström (1994). Two 
forms of salutations, both lexical features, are commonly used, and they represent forms of the 
conversational act of greeting. One of them, bienvenido ‘welcome’ is commonly used to 
acknowledge the self-introduction of new users, and the other one saludos ‘regards’ is used to 
either close a message or direct some message to a specific user, so as to continue a 
conversation. In this last sense, this comment can be translated as “hey X”, but it is used at the 
end of the message. At least two more lexical features selected by IG represent the first type of 
move listed by Stenström, summons. For this author, summons are aimed at calling the attention 
of the listener. Two usernames used as vocatives direct the message to a specific online forum 
user. These usernames are also selected as relevant features by the two reduction techniques that 
render the best results. The original usernames have been substituted by “username1” and 
“username2” in Appendix C. Finally, one lexical feature is used as a conversational act, a 
confirmation question, one of three variations of the question act (which comprises 
identification, polarity, and confirmation questions). A form of confirmation question that asks 
for the reader’s agreement with the author’s comment is represented by the unigram verdad 
‘right’. This feature exemplifies this conversational move at the same time that renders a high IG 
score (“fue un accidente verdad”, ‘it was an accident, right?’; “cada quien sus .inches costumbres 
verdad”, ‘everyone has their own f*ing habits, right?’). 
 Another linguistic pattern in the features selected by IG and CFS is present in the diverse 
forms of emotext. Formerly defined as any of the textual conventions developed in electronic 
 
237 
 
communications to convey emotional content (de Vel et al., 2002), emotext is present in various 
features that are relevant for the classification. First, there are a number of lexical features that 
match the definition of emotext. Among these features there are eight lexical, ja and jaja (both 
meaning ‘hehe’) are selected by IG and CFS. IG also selects jeje, ha, jajajajajaja, hahahah, 
hahahaha (all with the same meaning as the former two), and wow (a borrowing from English). 
In second place, the repetition of a specific punctuation mark in a single string can also represent 
forms of emotext, as it was mentioned in Subsection 4.2.1a. Encoding six individual media-
specific features (for sequences of 2, 3, 4, 5, 6, and 7+ repeated punctuation marks in a row), all 
these emotext features were selected by both IG and CFS. These features were often represented 
in the text samples by series of reduplicated exclamation points, question marks and periods, 
which can be used for emphasis. Although these six features were the only ones that directly 
exploited the use of repetition to convey an extra emotional meaning, a resource that has also 
been explored in the study of conversation (Tannen, 2007), it is worth mentioning that repetition 
also plays a role in the unigrams used to show laughter. With various series of repeated syllables 
(two for “ja” and “je”, three and four for “ha”, and six for “ja”), these various forms of 
reduplicated linguistic elements seem to carry not only emotional content but also information 
regarding personal choice as to how the author represents laughter and its various levels of 
intensity. As I mentioned elsewhere (2011), I originally proposed the features for the various 
series of reduplicated punctuation marks because they seemed to capture stylistic decisions or 
habits that I considered could be relevant. As the results show, these features have proven useful 
for the classification. I did not select the various forms of repeated syllables that represent 
laughter for this reason, they were actually selected automatically by feature reduction 
techniques, but they also seem to carry the stylistic decisions or habits that prompted me to select 
 
238 
 
reduplicated punctuation. Two more media-specific features that represent forms of emotext are 
the use of emoticon images (selected in 38 corpora by IG and 35 by CFS) and keyboard based 
emoticons (selected in 32 experiments by IG and in 5 by CFS). Finally, there are a number of 
features that do not represent special conventions that have been developed in electronic 
communications, but they are also used to convey emotional content. These other features used 
to mostly convey emotions are represented by 6 lexical items: triste ‘sad’, humilde ‘poor’, 
pobresito [sic] ‘poor thing’, hay ‘ouch’ [sic], feo ‘ugly’, and impresionante ‘shocking’. Adding 
all the features presented in this paragraph, there are a total of 22 features that can be seen as 
forms used to convey emotional content. 
 Given the sensitivity of the topic in the selected online forum, in which there are 
contributions by both supporters and opponents of the drug war in Mexico (including soldiers 
and drug dealers), it is not uncommon to find verbal disputes among the users of the forum. 
Along with these disputes, and probably influenced by them, there are a number of users that 
tend to have an irreverent sometimes hostile tone. Reading the posts, it is possible to see that this 
attitude is usually reflected in the use of both swear words and euphemisms. Regardless of the 
motivation or the intention in the use of these two types of linguistic elements, a good number of 
them get selected among the list of unigrams (i.e., lexical features) by the two feature reduction 
techniques that render the best results in this project. For the swear words listed in this 
paragraph, I have provided approximate glosses for the sense that I consider was the most 
common one in the various corpora. There are four swear words selected by IG and CFS : 
cabron [sic] ‘fucker’, puta ‘bitch’, puto ‘faggot’, and wey [sic] ‘dude’. IG further selects three 
swear words (culeros ‘fuckers’, pinches ‘fucking’, putos ‘faggots’, and we [sic] ‘dude’), three 
euphemisms (ingada ‘fucked’, pex ‘fart’, and uta ‘bitch’), and two words (madre ‘mother’ and 
 
239 
 
hijos ‘sons’) that appeared in multi-word expressions with profanity. Since these two last words 
listed have various meanings, I included them in this list after verifying that they were used in 
collocations (commonly occurring word sequences) with profanity. I did this by looking at the 
text samples of users in which these features could render high IG scores. It should also be 
mentioned that the unigrams wey and we represent non-normal spelling varieties of the form 
güey, which does follow Spanish conventions for phonological representation. The Diccionario 
del español usual en México (DEUM) (Lara, 1996) considers this form “offensive” in various 
contexts, but this word can also be used with a softer connotation in colloquial speech, such as in 
the gloss ‘dude’ -- for an interesting study on the translation pair güey-dude, see Bucholtz 
(2009). With all the lexical features discussed in this paragraph, a total of 13 swear words or 
euphemisms get selected by both IG and CFS. 
 Also related to the informal tone of the online forum posts, there are a number of 
netabbrevs chosen from the list of unigrams, which were all considered as lexical features. 
Defined by Argamon et al. (2003) as lexical items that are developed in communities that use 
electronic communications, netabbrevs include acronyms for common expressions (such as 
“BTW” for “by the way”) and abbreviations, which may include character substitutions (“F2F” 
for “face to face”). Argamon et al. use a predetermined list of 190 netabbrevs in their authorship 
attribution experiments. In the experiments conducted in the current study, IG and CFS selected 
three netabbrevs (ke for que ‘that’, x for por ‘for’, and ps for pues ‘then’). IG also selects two 
more (k for que ‘that’ and pq for porque ‘because’). It should be noted that all five netabbrevs 
selected by the two feature reduction techniques stand for function words in Spanish. However, 
they were selected among the general list of unigrams, classified as lexical since none of its 
elements was selected based on their syntactic information. 
 
240 
 
 Finally, many other non-topic related unigrams, or lexical features, get selected by the 
two reduction techniques, IG and CFS. Between these two techniques, 17 punctuation related 
features (in addition to the six included as emotext) are selected. These features include six 
marks that, following Spanish conventions (Real Academia Española, 2005), occur at the end of 
a word: the period, the ellipsis (which was not counted under the reduplicated punctuation marks, 
as explained in Section 4.3.1a), the comma, the exclamation point, and the question mark. Two 
marks used in a non-standard way, at the beginning of a word, were chosen as relevant features 
too, the comma and the colon. Used in both positions, before and after words, a feature for any 
quotation marks was selected by both IG and CFS. Among the novel features proposed, the 
carriage return in html, any punctuation before the carriage return, and all the auxiliary spelling 
marks (the generic feature for any vowel with a diacritic, and the five features for all stressed 
vowels) were also selected. Only present in one of the authors and not a convention as far as I 
know, the idiomatic use of a sequence of three carats was also chosen.  
 Besides these 17 punctuation-related features, 23 function words get selected between IG 
and CFS. These function words do not include the five netabbrevs formerly presented, which are 
also function words. Once more, these words were selected from the list of unigrams, which was 
presented as lexical because none of its elements was selected based on their syntactic 
information. The 23 function words selected in addition to the five netabbrevs are: que, por que, 
me, por, el, los, a, mi, tal, ustedes, yo, alguien, asta [sic], nadie, se, para que, de, esto, le, nos, 
para, pues, and unos. These words include five prepositions (por ‘by/for’, a ‘to’, asta [sic] 
‘until’, de ‘of’, and para ‘for’), five determiners (el ‘the’, los ‘the’, mi ‘my’, tal ‘such’, and unos 
‘some’), four conjunctions (que ‘that/which’, por que ‘because of which’, pues ‘then’, and para 
que ‘for which’), and nine pronouns (me ‘to me’, ustedes ‘you’, yo ‘I’, alguien ‘somebody’, 
 
241 
 
nadie ‘nobody’, se ‘to him/her’, le ‘for him/her’, esto ‘this’, and nos ‘to us’). In addition to these 
23 features that belong to closed class word categories, whose list of members is relatively fixed 
in Spanish (Real Academia Española, 2005), there are also 11 adverbs selected. Adverbs are not 
a closed class word category in this language, but they are also fairly topic independent. The 
lexical features for these adverbs include: más ‘more’, aquí and aqui [sic] ‘here’, muy ‘very’, 
alla [sic] ‘there’, así and asi [sic] ‘so’, mejor ‘better’, mucho ‘much’, seguramente ‘sure/surely’, 
and siquiera ‘even’. One more form of topic-neutral features is given by the rest of structural 
features that have not been commented on earlier in this section. These structural features, a total 
of seven, are selected in a great number of the experimental corpora. These seven features are: 
the use of a bold font, a font with color, a font with a special size, images, clickable links, non-
clickable links, and the type-token ratio. The first six features also represent media-specific 
features, which characterize the media they are produced in, as was explained in Section 4.3.1a. 
Also, mentioned in that section, the feature for the type-token ratio has been considered 
structural because it represents a complexity measure. 
 This section has explored a number of the patterns of the kind of linguistic and 
paralinguistic information that is shared by the 144 features selected by both IG and CFS. In the 
examination of these features, 9 features were shown to represent conventions in the structure of 
conversation. Besides these conversational elements, 22 features are forms that convey 
emotional content, 13 lexical items represent forms of swearing or euphemisms, 5 are netabbrevs 
(and function words), 17 are punctuation-related marks, 23 are function words, 11 are adverbs, 
and 7 are structural features. It should be noted that in the discussion above, the 6 features for 
reduplicated punctuation were included among the forms that convey emotional content, so they 
 
242 
 
were not counted as part of the 17 punctuation-related features, even though they could also be 
considered as such. All the features just listed are shown in Table 5.8 below. 
  
 
243 
 
Table 5.8 Linguistic/paralinguistic information in features commonly selected by IG or CFS 
Table 15 
Features commonly 
selected by IG or CFS 
Gloss 
Linguistic or paralinguistic 
information 
alla there [sic] adverb 
aqui here [sic] adverb 
aquí here adverb 
asi so [sic] adverb 
así so adverb 
más more adverb 
mejor better adverb 
mucho much adverb 
muy very adverb 
seguramente sure adverb 
siquiera even, neither adverb 
bienvenido welcome conversational  
buena good conversational  
cierto true, really conversational  
información information conversational  
ReplyMessage conversational  
saludos regards conversational  
username1 (a username) conversational  
username2 (a username) conversational  
verdad truth, really conversational  
ay ouch emotional 
feo ugly emotional 
ha hehe emotional 
hahahah hehe emotional 
hahahaha hehe emotional 
humilde poor, humble emotional 
impresionante shocking emotional 
ja hehe emotional 
jaja hehe emotional 
jajajajajaja hehe emotional 
jeje hehe emotional 
KeyboardEmoticon emotional 
pobresito poor emotional 
RedupliPunct2 emotional 
RedupliPunct3 emotional 
RedupliPunct4 emotional 
   
 
244 
 
Table 5.8 (continued) 
 
Features commonly 
selected by IG or CFS 
Gloss 
Linguistic or paralinguistic 
information 
RedupliPunct5 emotional 
RedupliPunct6 emotional 
RedupliPunct7more emotional 
EmoticonImage emotional 
triste sad emotional 
wow wow emotional 
a to function word 
alguien somebody function word 
asta until [sic] function word 
de of function word 
el the, he [sic] function word 
esto this function word 
le to him/her, him/her function word 
los the function word 
me to me, me function word 
mi my, me [sic] function word 
nadie nobody function word 
nos to us, us function word 
para for function word 
paraQue for which function word 
por by function word 
porQue because of which, because [sic] function word 
pues then function word 
que that function word 
se to him/her, him/her function word 
tal such function word 
Unos some function word 
Ustedes you function word 
Yo I function word 
K that netabbrev/function word 
Ke that netabbrev/function word 
Pq because netabbrev/function word 
Ps then netabbrev/function word 
X x, by netabbrev/function word 
^^^ punctuation 
Ellipsis punctuation 
   
 
245 
 
Table 5.8 (continued) 
 
Features commonly 
selected by IG or CFS 
Gloss 
Linguistic or paralinguistic 
information 
EOPpunctuation (punctuation before return) punctuation 
FinalCommaMark (comma) punctuation 
FinalPunct!Mark (exclamation point) punctuation 
FinalPunct.Mark (period) punctuation 
FinalPunct?Mark (question mark) punctuation 
InitialCommaMark (comma) punctuation 
InitialPunct:Mark (colon) punctuation 
quoteMark (quotation mark) punctuation 
Return punctuation 
stressedA punctuation 
stressedE punctuation 
stressedI punctuation 
stressedO punctuation 
stressedU punctuation 
vowelDiacritics punctuation 
CommentBold structural 
CommentFontColor structural 
CommentFontSize structural 
Hyperlink structural 
Image structural 
textHyperlink structural 
TTRatio (type-token ratio) structural 
Cabron fucker swearing 
Culeros fuckers swearing 
Hijos children swearing 
Ingada fucked swearing 
Madre mother swearing 
Pex fart swearing 
pinches fucking swearing 
Puta bitch swearing 
Puto faggot swearing 
Putos faggots swearing 
Uta bitch swearing 
We dude swearing 
Wey dude swearing 
      
 
 
 
246 
 
 Adding to a total of 107, the features shown in Table 3.8 represent three fourths of the set 
of accumulated features that have rendered the best results in this project, i.e., three fourths of all 
the features selected by either IG or CFS. These features also include 39 out of 42 features, or 
93% of the features that get selected 10 or more times with IG. Namely, these features represent 
the 93% of the accumulated feature set in 30 out 39 experiments with this technique, over three 
fourths of all experiments. As to CFS, the 107 features discussed in this section represent an even 
larger proportion of all experiments with this reduction technique. These 107 features include 36 
out of 39 features, or 92% of the features that get selected 4 or more times with this technique. 
This means that these features represent the 92% of the accumulated feature set used in 36 out 39 
experiments. Given these numbers, it is possible to argue that the features discussed in this 
section represent the majority of the features consistently selected by IG and CFS across all 
experimental corpora. 
 In order to close the analysis and discussion in this chapter, a few general observations 
should be made about all the 107 features discussed in this section. First, they all represent fairly 
topic-independent information. They represent conversational structure conventions, forms 
conveying emotional content, forms of swearing, punctuation-related marks, function words, and 
media-specific structural features. Second, the features discussed in this section do not require a 
in-depth linguistic analysis of the texts used for both training and testing the statistical model of 
an author. In third place, despite the last two comments, these features seem to capture stylistic 
decisions or habits that allow a quantitative class attribution method to obtain a high success rate 
in an authorship attribution task. Finally, since these features do not require a in-depth linguistic 
analysis of the texts in the task corpus, they can easily be tagged with a fully automated process, 
 
247 
 
which contributes dramatically to the efficiency of the authorship attribution approach of which 
they are a component.  
 
248 
 
CHAPTER 6: CONCLUSION 
This chapter, the last one in this project, is composed of three sections. In the first section, the 
answers to the research questions that guided this whole project will be given and discussed. 
Originally presented in Chapter 1, these questions helped in setting up a number of expectations 
for the experiments conducted in this project. With these questions, it became clear that this 
project was aimed at, among other things, achieving a specific success rate with its experiments, 
evaluating multiple authorship attribution configurations, proposing and testing novel authorship 
features, applying attribution methods to multiple corpora of various sizes, and offering an 
interpretation of the linguistic components of the most successful experiments conducted here. 
The first section of this chapter will discuss if and how these expectations were met. In the 
second section, I will list a number of gaps or shortcomings that were identified in the study of 
authorship analysis. These gaps have motivated a number of decisions that guided this project. 
The gaps that this project has attempted to fill are related to both the study of authorship 
attribution in general and the specific context of authorship attribution studies that target Spanish 
data. The second section of this chapter will summarize a number of gaps that were mentioned in 
the various chapters of this dissertation and will discuss how this study has filled these gaps and 
contributed to the study of authorship analysis in a number of ways. Finally, the last section will 
comment on two tasks (an authorship analysis task and a text classification task) that could not 
be covered in this project, given restrictions of time and space. As will be discussed in this third 
section, the work done in this project could be extended to tasks such as author profiling for 
online forum users or automatic detection of drug-dealing related electronic communications 
with a fair investment of time and effort. For the experiments presented in Chapter 4, a work 
pipeline has already been set up. In this pipeline, a series of feature taggers have been 
 
249 
 
implemented and a number of so-called wrappers (programs that interact with other programs, 
such as the classifiers) have been created. Adapting this pipeline to the above-mentioned tasks 
should not require a great amount of work. Therefore, this chapter will finish discussing a few 
potential applications to which the kind of work presented here could be straightforwardly 
extended in the future. 
6.1 REVISITING THE RESEARCH QUESTIONS 
1) Is it possible to match and improve the best results obtained with DA in authorship attribution, 
while using a fully automated tagging of features?  
Yes. After designing 40 different authorship attribution configurations (by combining eight 
different feature sets with five classifiers), a number of these approach configurations 
accomplish the goal of matching or improving the best results obtained with DA in authorship 
attribution (Rico-Sulayes, 2011). The eight approach configurations that manage to obtain equal 
or better results, compared to the best results obtained previously with DA in a similar task, rely 
solely on automatically tagged features. 
2) By combining a number of different classifiers and feature sets, are there any combination of 
them that achieve this goal? If so, which classifiers and feature sets can match and improve these 
previous results? 
Eight out of 40 authorship attribution configurations can match or improve the best results 
obtained with DA in an equivalent task. Among these eight configurations, six manage to 
improve the previously achieved accuracy of 0.925. These six configurations, ordered by the 
accuracy they obtain, are: MNB with non-normalized, IG-selected features (1.0), MNB with 
normalized, IG-selected features (0.975), SVMs with normalized/non-normalized, IG-selected 
features, and MNB with normalized and with non-normalized, frequency-selected features 
 
250 
 
(0.95). Two more configurations also match the accuracy obtained in Rico-Sulayes (2011), 
SVMs with normalized and with non-normalized, CFS-selected features. Something that is worth 
mentioning is that, as can be observed in these results, all of the approach configurations that 
accomplished the goal implied in the first research question use some form of reduced feature 
set. Four out of the eight approaches that achieved this goal used IG, two approaches applied 
CFS, and two more approaches used selected features based on their frequency. 
3) Regarding the best results obtained in the whole set of experimental corpora, are there any 
novel features proposed in this project that contribute to these results? Do these novel features 
contribute to achieving the goal of matching and improving previous research with DA? 
Yes. Looking at the features selected by the reduction techniques that render the best results in 
this project, a great number of the novel features I proposed were selected. All but one of the 
novel features proposed among lexical features get selected. These include features for five 
stressed vowels, for any vowel with a diacritic, and for the presence of punctuation before the 
carriage return in reflowable text. Also, two syntactical features from the novel multi-unit 
function word list proposed are chosen. Finally, all novel structural features proposed in this 
project, a total of nine, get selected. These include six features for reduplicated punctuation, two 
for hyperlinks, and the carriage return in html. Therefore, not only did the proposal of novel 
features contribute noticeably to achieving the best results in this project, but it also made it 
possible to match and improve the accuracy in previous work with DA. 
4) Are the best results obtained consistent across a range of values for the number of subjects in 
the set of potential authors? 
Yes. The configuration with the best performance in the whole range of values for number of 
subjects, the configuration with MNB, non-normalized data, and IG-selected features, obtains a 
 
251 
 
consistently high accuracy across all experimental corpora. With an averaged accuracy of 0.947 
in all experiments, this configuration obtains an accuracy that remains above 0.9 in the entire set 
of 39 corpora. It is worth noting that a number of other configurations with a high performance 
(the ones with SVMs and NB) show a tendency in their accuracy to drop as the number of 
subjects increases. As far as MNB uses a reduced set, this tendency was much less evident in this 
classifier in experiments that have up to 40 subjects in the set of potential authors. The accuracy 
obtained by this classifier does show a tendency to drop when the classifier is fed the set of all 
features. 
5) What kind of linguistic or palinguistic information is used to render the best results obtained 
in the authorship attribution task targeted in this project? 
Examining the linguistic patterns in the features selected by the reduction techniques that render 
the best results, it is revelead that a great proportion of the features that get selected in most of 
the experiments are topic-independent. These features represent conversational structure 
conventions, forms conveying emotional content, forms of swearing, punctuation-related marks, 
function words, and media-specific structural features. Given their characteristics, these features 
do not require a deep linguistic analysis of the texts used in the task. Despite representing 
grammatically shallow information, these features seem to capture stylistic decisions or habits 
that permit a quantitative authorship attribution approach to obtain a high success rate in a great 
number of experiments and improve the best results obtained with DA in an equivalent task. 
6.2 DISCUSSION OF CONTRIBUTIONS TO AUTHORSHIP ANALYSIS 
At the beginning of this chapter, it was mentioned that this study has attempted to fill a number 
of gaps in the literature that has targeted both the study of authorship attribution when applied to 
Spanish data and the study of authorship analysis in general. Section 3.3 surveyed all authorship 
 
252 
 
analysis studies that have targeted Spanish data, a total of six research papers (MacMenamin, 
2002; Nazar and Sánchez Pol, 2007; Rico-Sulayes, 2011; Spassova, 2008; Spassova, 2009; 
Spassova and Turell, 2007). With the brief presentation of approaches and experimental 
decisions in this six authorship analysis studies, four important gaps were identified in the core 
elements of their authorship analysis approaches (i.e., in their selection/reduction of authorship 
features and their use of class attribution methods). First, the above summarized studies selected 
as features either exhaustive (usually long) lists of various n-grams, namely of various element 
sequences, such as word sequences (Nazar and Sánchez Pol, 2007; Spassova, 2009; Spassova 
and Turell, 2007), or shorter lists of lexical, syntactical, and structural elements (MacMenamin, 
2002; Rico-Sulayes, 2011; Spassova, 2008). Given this fact, this is the first study employing 
Spanish data that has used both of these types of features. Doing this has contributed to the 
specific body of literature that targets authorship analysis with Spanish data because it widens 
significantly the options for the first stage of research-driven feature selection. In the general 
context of authorship analysis, this is important because the use of the two types of features 
above described has been explored both in a number of studies that perform authorship 
attribution in languages other than Spanish (Abbasi and Chen, 2008; Gamon, 2004; Grieve, 
2007; Koppel et al., 2009) and in many author profiling studies (Corney, 2003; Koppel et al., 
2002; 2005; 2008; 2009; Zhang et al., 2009). Besides contributing to expanding the options of 
feature selection in authorship analysis with Spanish data, the inclusion of long lists of unigrams 
classified as lexical features, along with shorter lists of different category-specific elements, has 
contributed directly to the high success rate of the experiments conducted, as discussed in 
Chapter 5. 
 
253 
 
 A second gap was identified in Section 3.3 regarding the five quantitative authorship 
analysis studies that target Spanish data (Nazar and Sánchez Pol, 2007; Rico-Sulayes, 2011; 
Spassova, 2008; Spassova, 2009; Spassova and Turell, 2007). The gap in these quantitative 
studies is that they apply few class attribution methods on their Spanish data. With the exception 
of one study, in which a classifier developed by the researchers is implemented (Nazar and 
Sánchez Pol, 2007), the other four studies just listed use DA as their only classification method 
for various authorship analysis tasks (including authorship attribution and author profiling). 
Therefore, there is an important gap in the application of machine learning classifiers to the study 
of authorship analysis with Spanish data. This study has applied to Spanish data all the most 
common, as well as the ones reported as the most successful, machine learning classifiers in the 
authorship analysis community (including both authorship attribution and author profiling 
studies). The four machine learning methods tested (C4.5, MNB, NB, and SVMs) have not only 
expanded the scope of the classification methods employed by the particular body of research 
this project is part of, but they have also surpassed in a number of experiments the best accuracy 
rate rendered by the statistical method DA. 
 The third gap mentioned in Section 3.3 regarding the body of literature that targets 
authorship attribution in Spanish data has to do with the feature reduction techniques employed. 
This gap, as mentioned before, is probably related to the lack of experimentation with various 
class attribution methods in Spanish data, although it is not a necessary consequence of it. Only 
built-in, DA related feature reduction techniques had been used among the studies that use 
Spanish data. Rico-Sulayes (2011) compares two techniques, Wilks’ Lambda and Mahalanobis. 
Spassova (2009) uses the former technique in her authorship attribution experiments and the 
latter for author profiling. These feature reduction techniques are not present in any of the studies 
 
254 
 
that use class attribution methods other than DA. This study has used three different feature 
reduction techniques, all of them external, namely, non-related to specific classification methods 
or to particular software implementations. 
 There was a fourth gap in the authorship attribution studies that use Spanish data which 
was identified in Section 3.3. Only one study (Rico-Sulayes, 2011) with data in this language 
uses data from social media, or electronic communications in general, and that study has been 
the origin of this research project. The current project has used the same data as the study just 
cited but it has extended the scope of this study in various aspects, as has just been discussed 
above and will be further commented on in the rest of this section. 
 Beyond the formerly discussed gaps, which are related to authorship analysis with 
Spanish data, another gap identified in Section 3.1.2 applies to all authorship attribution studies 
in general. As mentioned in that section, feature reduction techniques are common in authorship 
attribution studies; over half the studies surveyed in Section 3.1.2 used some form on feature 
reduction. However, not many of them tested different settings for those techniques, and only 
one study compared two different reduction techniques (Rico-Sulayes, 2011). The current project 
has surpassed the limited experimentation of feature reduction techniques in Rico-Sulayes 
(2011), which used only built-in reduction techniques in a statistical classifier. This dissertation 
has explored the use of three external feature reduction techniques (i.e., non-related to software 
implementations) and has compared their performance in all their possible combinations with 
five different classifiers (which included four machine learning classifiers) and two forms of data 
representation (normalized and non-normalized). Unlike what is reported in Rico-Sulayes 
(2011), where no accuracy difference was obtained with the use of two reduction techniques, the 
 
255 
 
current project has rendered a range of values in the broad experimentation with feature 
reduction techniques.  
 There is also a pair of important contributions this study has made while attempting to fill 
the gaps above discussed. A contribution of this project to the general study of authorship 
attribution is the application and evaluation of a new feature reduction technique, correlation-
based feature subset selection (CFS). This technique has contributed to achieving some of the 
highest accuracy rates among the 40 different configurations tested in this project. Besides, this 
technique has also been instrumental to find out that the most successful sets of features have 
much in common, not only in the specific features they share, but also in the type of linguistic 
information that characterizes their features. 
 The last contribution of this project is the consequence of expanding the options in the 
authorship attribution approach components. With a wider range of features to be selected, 
multiple reduction techniques to improve the set of these features, and various classifiers to 
process them, many of the approach configurations tested have surpassed in a number of 
experiments the best accuracy rate rendered by the statistical method DA in an authorship 
attribution task. This has been shown and discussed in detail in section 5.1.4. Although the 
various contributions just presented have been mentioned in the different chapters of this project, 
summarizing and discussing them in this section should help put in perspective the significance 
of this dissertation. 
6.3 FUTURE WORK 
At the beginning of this chapter, two tasks have been mentioned as possible future applications 
of the kind of work done to conduct the experiments in this project. One of these tasks is another 
authorship analysis task, author profiling. The task of author profiling, as defined in Section 2.2, 
 
256 
 
consists of characterizing the author of an anonymous piece of text across a number of 
categories, e.g., the author’s gender, dialect, and language nativeness. As was noted in sections 
3.1.3, 3.1.4, and 3.2.2, the most important components of an authorship attribution approach 
(feature selection, feature reduction and class attributions, respectively) are also shared with 
author profiling. Extending the work done in this project to the task of author profiling would 
then imply mostly the collection of new data. The users’ profiles in Foro Blog del Narco (2010) 
do not include authors’ demographic information useful for author profiling, such as the 
categories mentioned above. After collecting data that suits the goals of author profiling, namely 
data that includes relevant users’ demographic information, setting up a number of experiments 
would also include assembling one or more corpora that allow the researcher to explore the 
demographic variables he or she is interested in addressing. All this work, however, should not 
represent a major challenge for any programmer familiar with basic text manipulation. 
Knowledge about the tools that have been implemented in this project should also allow a 
programmer to design corpora that can be processed by the taggers and classifiers used in this 
project in a seamless manner. 
 The other task that could be easily targeted building upon the work done in this project is 
a text classification task, an automatic detection of drug-dealing related online forums posts in 
specific, possibly of electronic communications in general. Identifying drug-trafficking related 
online forum posts (when compared to other drug-trafficking related and non-drug-trafficking 
related posts) would also require a collection of new data, namely of the last posts mentioned. 
The advantage of targeting this rather general text classification task after performing 
experiments in authorship attribution and author profiling would be that this third task could 
benefit from both the data collection performed for the two authorship attribution tasks just 
 
257 
 
mentioned and the already implemented work pipeline, which consists of taggers and classifiers. 
Although this last classification task could be better described as a topic/genre classification 
rather than an authorship analysis task, it could shed light on how distinctive the language of 
drug-dealing related online forums is. If contributions from the forum employed are successfully 
discriminated from other forums posts, potential applications could be developed to identify 
drug-trafficking related communications of the type collected for this study. 
 The two new tasks just presented (author profiling for online forum users and automatic 
detection of drug-dealing related online forum posts) were actually part of the original set of 
goals in this project. However, even if beginning the experimentation with these tasks should not 
require too much time, tuning the approaches in order to accomplish fairly good results, 
presenting the data collected, explaining the tuning process, and most importantly, interpreting 
the linguistic components of the new experiments (whatever their results may be) is beyond the 
restrictions of time and space in this dissertation. In any case, these tasks remain relevant and 
interesting tasks I hope to target in the near future. 
  
 
258 
 
APPENDIX A: FUNCTION WORD N-GRAMS IN SPANISH 
 
Function Word N-
gram Gloss Category 
a causa de because of preposition 
a causa del because of preposition 
a condición de que provided conjuction 
a cuenta de on account of preposition 
a cuenta del on account of preposition 
a diferencia de unlike preposition 
a diferencia del unlike preposition 
a espaldas de behind preposition 
a espaldas del behind  preposition 
a favor de for preposition 
a favor del for preposition 
a fin de que in order that conjuction 
a la par de at par with preposition 
a la par del at par with  preposition 
a medida que to the extent that conjuction 
a menos que unless conjuction 
a partir de starting from preposition 
a partir del starting from  preposition 
a pesar de despite preposition 
a pesar del despite  preposition 
a saber namely conjuction 
a través de across preposition 
a través del across preposition 
abajo de under preposition 
abajo del under preposition 
además de in addition to preposition 
además de que besides conjuction 
además del in addition to preposition 
además del que besides conjuction 
además que besides conjuction 
adentro de inside preposition 
adentro del inside preposition 
 
  
 
259 
 
Function Word N-
gram Gloss Category 
afuera de outside preposition 
afuera del outside preposition 
al lado de beside preposition 
al lado del beside preposition 
alrededor de around preposition 
alrededor del around preposition 
antes de prior to preposition 
antes de que before conjuction 
antes del prior to preposition 
antes del que before conjuction 
arriba de up preposition 
arriba del up preposition 
así como as well as preposition 
atrás de behind preposition 
atrás del behind preposition 
cerca de close to preposition 
cerca del close to preposition 
con respecto a regarding preposition 
con respecto al regarding preposition 
con tal de que provided conjuction 
con todo notwithstanding conjuction 
de acuerdo a according to preposition 
de acuerdo al according to preposition 
de acuerdo con according to preposition 
de otra manera otherwise conjuction 
de todos modos anyway conjuction 
debajo de under preposition 
debajo del under preposition 
debido a due to preposition 
debido al due to preposition 
delante de in front of preposition 
delante del in front of preposition 
dentro de inside of preposition 
dentro de lo que as to conjuction 
dentro del inside of preposition 
desde que since conjuction 
  
 
260 
 
Function Word N-
gram Gloss Category 
después de after preposition 
después de que after conjuction 
después del after preposition 
después del que after conjuction 
detrás de behind preposition 
detrás del behind preposition 
en caso de in case of conjuction 
en caso de que in case that conjuction 
en caso del in case of conjuction 
en contra de against preposition 
en contra de against preposition 
en contra del against preposition 
en contra del against preposition 
en lo que as far as conjuction 
en lugar de instead of preposition 
en lugar del instead of preposition 
en nombre de on behalf of preposition 
en nombre del on behalf of preposition 
en vez de instead of preposition 
en vez del instead of preposition 
encima de on top of preposition 
encima del on top of preposition 
enfrente de in front of preposition 
enfrente del in front of preposition 
entre que while conjuction 
excepto que except that conjuction 
fuera de outside of preposition 
fuera del outside of preposition 
hasta que until conjuction 
junto a beside preposition 
junto al beside preposition 
lejos de far from preposition 
lejos del far from preposition 
luego de after preposition 
luego del after preposition 
 
  
 
261 
 
Function Word N-
gram Gloss Category 
más allá de beyond preposition 
más allá del beyond preposition 
más de more than preposition 
más del more than preposition 
menos que except that preposition 
mientras que whereas conjuction 
no obstante nevertheless conjuction 
opuesto a opposite preposition 
opuesto al opposite preposition 
para que so that conjuction 
por causa de because of preposition 
por causa del because of preposition 
por debajo de below preposition 
por debajo del below preposition 
por ello therefore conjuction 
por encima de above preposition 
por encima del above preposition 
por eso therefore conjuction 
por lo tanto therefore conjuction 
por que because of which conjuction 
por supuesto que of course conjuction 
próximo a next preposition 
próximo al next preposition 
puesto que since conjuction 
si bien although conjuction 
siempre que whenever conjuction 
sin embargo however conjuction 
tan pronto como as soon as conjuction 
      
 
  
 
262 
 
APPENDIX B: FREQUENT KEYBOARD-BASED EMOTICONS IN SPANISH 
ONLINE REFERENCES 
 
Keyboard-Based 
Emoticon Gloss 
Keyboard-Based 
Emoticon Gloss 
:-( sad face :-O surprised 
:'-) happy and crying :-o surprised 
:-) happy face :-P sticking out tongue 
:-/ skeptical  :-p sticking out tongue 
:-@ shouting <:-) crazy 
:-> sarcastic  8-) face with glasses 
:-C bored :( sad child 
:-c bored :) happy child 
:-D big smile B-) face with glasses 
:-I thinking, indifferent xD laughing 
;-) blinking x-D laughing 
        
 
 
 
  
 
263 
 
APPENDIX C: FEATURES SELECTED USING IG 
 
Features selected with 
IG > 0 Gloss # corpora Category 
CommentBold 39 structural 
FinalPunct.Mark (period) 39 lexical 
CommentFontColor 38 structural 
Ellipsis 38 lexical 
RedupliPunct2 38 structural 
SmileyImage 38 structural 
stressedA 38 lexical 
vowelDiacritics 38 lexical 
stressedO 37 lexical 
FinalCommaMark (comma) 36 lexical 
Return 36 structural 
stressedI 36 lexical 
ReplyMessage 35 structural 
ke that (netabbrev) 34 lexical 
EOPpunctuation (punctuation before return) 33 lexical 
KeyboardEmoticon 32 structural 
RedupliPunct7more 31 structural 
información information 31 lexical 
jeje hehe 28 lexical 
bienvenido welcome 27 lexical 
RedupliPunct6 26 structural 
porQue because of which, because [sic] 26 syntactical 
que that 25 lexical 
más more 24 lexical 
RedupliPunct3 23 structural 
saludos regards 22 lexical 
zetas z's, (drug cartel) 21 lexical 
k that (netabbrev) 19 lexical 
FinalPunct!Mark (exclamation point) 17 lexical 
cdg (drug cartel) 17 lexical 
RedupliPunct4 16 structural 
RedupliPunct5 16 structural 
   
 
264 
 
 Features selected 
with IG > 0 Gloss 
# 
corpora Category 
jaja hehe 15 lexical 
CommentFontSize 14 structural 
ingada (swear word) 14 lexical 
Image 13 structural 
InitialCommaMark (comma) 13 lexical 
me to me, me 12 lexical 
tamaulipas (Mexican state name) 12 lexical 
por by, for 11 lexical 
wey dude, (swear word) 11 lexical 
aquí here 10 lexical 
fué was 10 lexical 
aqui here [sic] 9 lexical 
el the, he [sic] 9 lexical 
muy very 9 lexical 
stressedE 9 lexical 
stressedU 9 lexical 
triste sad 9 lexical 
ja hehe 8 lexical 
los the 8 lexical 
puto (swear word) 8 lexical 
FinalPunct?Mark (question mark) 7 lexical 
diego (first name) 6 lexical 
trajeron brought 6 lexical 
x x, by (netabbrev) 6 lexical 
z z, (cartel member) 6 lexical 
a to 5 lexical 
eres are 5 lexical 
ha hehe 5 lexical 
humilde poor, humble 5 lexical 
jajajajajaja hehe 5 lexical 
mi my, me [sic] 5 lexical 
pobresito poor 5 lexical 
pq because (netabbrev) 5 lexical 
textHyperlink 5 structural 
 
  
 
265 
 
Features selected 
with IG > 0 Gloss 
# 
corpora Category 
Hyperlink 4 structural 
hay there is/are 4 lexical 
matamoros (Mexican city name) 4 lexical 
matan kill 4 lexical 
mty (Mexican state name) 4 lexical 
pex (swear word) 4 lexical 
puta (swear word) 4 lexical 
tal such 4 lexical 
ustedes you 4 lexical 
yo I 4 lexical 
InitialPunct:Mark (colon) 3 lexical 
alguien somebody 3 lexical 
andan go 3 lexical 
asta until [sic] 3 lexical 
ay ouch 3 lexical 
buena good 3 lexical 
calderon (last name) 3 lexical 
culeros (swear word) 3 lexical 
decia said [sic] 3 lexical 
feo ugly 3 lexical 
fuga escape 3 lexical 
nadie nobody 3 lexical 
niños children 3 lexical 
onda wave, stuff 3 lexical 
persona person 3 lexical 
pinches (swear word) 3 lexical 
putos (swear word) 3 lexical 
saber know 3 lexical 
se to him/her, him/her 3 lexical 
sea were 3 lexical 
tabla wood 3 lexical 
uta (swear word) 3 lexical 
username1 (a username) 3 lexical 
laredo (Mexican city name) 2 lexical 
maten kill 2 lexical 
paraQue for which 2 syntactical 
username2 (a username) 2 lexical 
  
 
266 
 
Features selected 
with IG > 0 Gloss 
# 
corpora Category 
verdad truth, really 2 lexical 
wow wow 2 lexical 
TTRatio (type-token ratio) 1 structural 
^^^ 1 lexical 
alla there [sic] 1 lexical 
asi so [sic] 1 lexical 
así so 1 lexical 
buen good 1 lexical 
cabron (swear word) 1 lexical 
cierto true, really 1 lexical 
de of 1 lexical 
dos two 1 lexical 
esto this 1 lexical 
gente people 1 lexical 
gobierno government 1 lexical 
guerra war 1 lexical 
hahahah hehe 1 lexical 
hahahaha hehe 1 lexical 
hijos children 1 lexical 
impresionante shocking 1 lexical 
le to him/her, him/her 1 lexical 
madre mother 1 lexical 
mejor better 1 lexical 
mucho much 1 lexical 
narcos drug dealers 1 lexical 
nos to us, us 1 lexical 
para for 1 lexical 
pasa pass 1 lexical 
paso pass 1 lexical 
ps then (netabbrev) 1 lexical 
pues then 1 lexical 
quiero want 1 lexical 
quoteMark (quotation mark) 1 lexical 
save knows [sic] 1 lexical 
seguramente sure 1 lexical 
 
  
 
267 
 
Features selected 
with IG > 0 Gloss 
# 
corpora Category 
siquiera even, neither 1 lexical 
tener have 1 lexical 
tipo type, guy 1 lexical 
todo all 1 lexical 
unos some 1 lexical 
we dude, (swear word) 1 lexical 
        
  
  
 
268 
 
APPENDIX D: FEATURES SELECTED USING CFS 
 
Features selected 
with CFS Gloss 
# 
corpora Category 
FinalPunct.Mark (period) 39 lexical 
CommentBold 35 structural 
Ellipsis 35 lexical 
RepliedMessage 35 structural 
SmileyImage 35 structural 
Return 34 structural 
FinalCommaMark (comma) 33 lexical 
RedupliPunct2 32 structural 
EOPpunctuation (punctuation before return) 27 lexical 
stressedO 27 lexical 
ke that (netabbrev) 26 lexical 
CommentFontColor 23 structural 
bienvenido welcome 23 lexical 
porQue 
because of which, because 
[sic] 23 syntactical 
RedupliPunct3 21 structural 
RedupliPunct6 21 structural 
RedupliPunct7more 20 structural 
saludos regards 18 lexical 
zetas z's, (drug cartel) 18 lexical 
FinalPunct!Mark (exclamation point) 16 lexical 
que that 16 lexical 
jaja hehe 15 lexical 
RedupliPunct4 11 structural 
Image 10 structural 
más more 10 lexical 
por by 8 lexical 
aqui here [sic] 7 lexical 
tamaulipas (Mexican city name) 7 lexical 
RedupliPunct5 6 structural 
el the, he [sic] 6 lexical 
puto (swear word) 6 lexical 
z z, (drug cartel member) 6 lexical 
 
  
 
269 
 
Features selected 
with CFS Gloss 
# 
corpora Category 
KeyboardEmoticon 5 structural 
ja hehe 5 lexical 
me to me, me 5 lexical 
x by, for (netabbrev) 5 lexical 
Hyperlink 4 structural 
puta (swear word) 4 lexical 
wey dude, (swear word) 4 lexical 
sea were 3 lexical 
stressedA 3 lexical 
FinalPunct?Mark (question mark) 2 lexical 
cdg (drug cartel) 2 lexical 
hay there is/are 2 lexical 
información information 2 lexical 
los the 2 lexical 
nadie nobody 2 lexical 
se to him/her, him/her 2 lexical 
tal such 2 lexical 
vowelDiacritics 2 lexical 
CommentFontSize 1 structural 
InitialCommaMark (comma) 1 lexical 
TTRatio (type-token ratio) 1 structural 
cabron (swear word) 1 lexical 
calderon (Last name) 1 lexical 
guerra war 1 lexical 
le to him/her, him/her 1 lexical 
mi my, me [sic] 1 lexical 
muy very 1 lexical 
narcos drug dealers 1 lexical 
paraQue for which 1 syntactical 
ps then (netabbrev) 1 lexical 
quoteMark (quotation mark) 1 lexical 
textHyperlink 1 structural 
verdad truth, really 1 lexical 
        
  
 
270 
 
REFERENCES 
Abbasi, A., & Chen, H. (2005). Applying Authorship Analysis to Extremist-Group Web Forum 
Messages. IEEE Intelligent Systems, 20(5), 67-75. 
Abbasi, A., & Chen, H. (2008). Writeprints: A Stylometric Approach to Identity-Level 
Identification and Similarity Detection. ACM Transactions on Information Systems, 
26(2), 1-29. 
Agar, M. (2012, January 12). Mexican Drug War Deaths Top 47,500. The Telegraph. Retrieved 
March 25, 2012, from http://www.telegraph.co.uk/news/interactive-graphics/graphic-of-
the-day/9010312/Drug-war-deaths-top-47500.html 
Aksoy, S., & Haralick, R. M. (2001). Feature Normalization and Likelihood-Based Similarity 
Measures for Image Retrieval. Pattern Recognition Letters, 22(5), 563-582. 
Alpaydin, E. (2010). Introduction to Machine Learning: Adaptive Computation and Machine 
Learning (2nd ed.). Cambridge, MA: MIT.  
Argamon, S., Šarić, M., & Stein, S. S. (2003). Style Mining of Electronic Messages for Multiple 
Authorship Discrimination: First Results. Proceedings of the 9th ACM SIGKDD 
International Conference on Knowledge Discovery and Data Mining. 
aunm@s, Enciclopedia Latinoamericana. (2012). Emoticones. Retrieved April 17, 2012, from 
http://www.aunmas.com/doc/show_doc.php?doc=emoticones&sec=1 
Austin, J. L. (1962). How to do things with words. Oxford: Oxford University. 
Baayen, H., van Halteren, H., Neijt, A., & Tweedie, F. (2002). An Experiment in Authorship 
Attribution (pp. 29-37). Proceedings of JADT 2002: Sixth International Conference on 
Textual Data Statistical Analysis. 
Blog del Narco. (2012). Retrieved September 18, 2012, from http://www.blogdelnarco.com/ 
 
271 
 
Bolle, R. M., Connell, J. H., Pankanti, S., Ratha, N. K., & Senior, A. W. (2004). Guide to 
Biometrics. New York, NY: Springer-Verlag. 
Borderland Beat. (2011, September 15). Nuevo Laredo, the Silent War. Retrieved March 19, 
2011, from http://www.borderlandbeat.com/2011/09/nuevo-laredo-silent-war.html 
Bucholtz, M. (2009). From Stance to Style: Gender, Interaction, and Indexicality in Mexican 
Immigrant Youth Slang. In A. Jaffe (Ed.), Stance: Sociolinguistic Perspectives (pp. 146-
170). Oxford: Oxford University.  
Burns, R. B., & Burns, R. A. (2008). Business Research Methods and Statistics Using SPSS. UK: 
Sage. 
Burrows, J. (2002). Delta: A Measure of Stylistic Difference and a Guide to Likely Authorship. 
Literary and Linguistic Computing, 17(3), 267-86. 
Burrows, J. (2007). All the Way Through: Testing for Authorship in Different Frequency Strata. 
Literary and Linguistic Computing, 22(1), 27-47. 
Caver, J. F. (2009). Novel Topic Impact on Authorship Attribution. Unpublished MA thesis, 
Naval Postgraduate School. Retrieved October 27, 2011, from 
http://www.dtic.mil/dtic/tr/fulltext/u2/a514246.pdf 
Cedillo, J. A. (2011, October 30). Mexico Creates a Police Force to Combat Organized Crime. 
The Costa Rica News. Retrieved March 16, 2012, from 
http://thecostaricanews.com/mexico-creates-police-force/8330 
Cevallos, D. (2007, October 16). Most Dangerous Latin American Country for Journalists. Inter 
Press Service. News Agency. Retrieved January 23, 2011, from 
http://ipsnews.net/news.asp?idnews=39677 
 
272 
 
Chaski, C. E. (2001). Empirical Evaluations of Language-Based Author Identification 
Techniques. Forensic Linguistics, 8(1), 1-65. 
Chaski, C. E. (2005). Who’s At The Keyboard? Authorship Attribution in Digital Evidence 
Investigations. International Journal of Digital Evidence, 4(1), 1-13. 
Chaski, C. E. (2007). The Keyboard Dilemma and Authorship Identification. In P. Craiger & S. 
Shenoi (Eds.), Advances in Digital Forensics III (pp. 133-146). Ney York, NY: Springer. 
CNN. (August 19, 2010) [Television broadcast]. Retrieved October 23, 2010, from 
http://edition.cnn.com/video/#/video/world/2010/08/19/romo.mexico.narco.blog.cnn 
Código Federal de Procedimientos Penales. Diario Oficial de la Federación, Mexico (Aug. 19, 
2010). 
Córdoba, J., & Luhnow, D. (2011, January 13). In Mexico, Death Toll in Drug War Hits Record. 
The World Street Journal. Retrieved January 15, 2011, from http://online.wsj.com 
Corney, M. (2003). Analysing E-mail Text Authorship for Forensic Purposes. Unpublished MA 
thesis, Queensland University of Technology. Retrieved October 28, 2011, from 
http://eprints.qut.edu.au/16069/1/Malcolm_Corney_Thesis.pdf 
Corney, M., de Vel, O., Anderson, A., & Mohay, G. (2002). Gender Preferential Text Mining of 
E-mail Discourse. Proceedings of the 18th annual Computer Security Applications 
Conference (pp. 282–289). Piscataway, NJ: IEEE. 
Corson, D. (2001). Language Diversity and Education. Mahwah, NJ: Lawrence Erlbaum 
Associates. 
Coulthard, M. (2004). Author Identification, Idiolect, and Linguistic Uniqueness. Applied 
Linguistics, 25(4), 431-447. 
 
273 
 
Crystal, D. (1975). Paralinguistics. In J. Benthall & T. Polhemus (Eds.), The body as a medium 
of expression (pp. 162-174). London: Institute of Conteporary Arts. 
de Vel, O. Y., Corney, M. W., Anderson, A. M., & Mohay, G. M. (2002). Language and Gender 
Author Cohort Analysis of E-mail for Computer Forensics. Digital Forensics Research 
Workshop. Syracuse, NY. 
Eckert, P. (2000). Linguistic Variation as Social Practice: The Linguistic Construction of 
Identity in Belten High. Cornwall, UK: Blackwell. 
Eckert, P., & McConnell-Ginet, S. (2003). Language and Gender. Cambridge, UK: Cambridge 
University. 
El Foro del Narco. (2012). Retrieved March 26, 2012, from http://www.elnarcoenmexico.com/ 
elnorte.com. (2012). Retrieved March 19, 2011, from http://www.elnorte.com/ 
English.com.mx. (2008). AAE (Abreviaciones, Acrónimos y Emoticones). Retrieved April 17, 
2012, from http://www.englishcom.com.mx/tips/aae.html 
Epp, S. S. (2004). Discrete Mathematics with Applications (3rd ed.). Belmont, CA: Brooks/Cole. 
Everything Development. (2001, September 6). Yule's Characteristic. Retrieved January 26, 
2012, from http://everything2.com/title/Yule%2527s+Characteristic 
Facebook Español. (2011, February 4). Emoticones para Facebook, Trucos, Símbolos, Caritas y 
Más. Retrieved April 17, 2012, from http://facebook-espanol.com/tags/corazones-
comunes-emoticones-msn/ 
Finegan, E. (1990). Variation in Linguists’ Analyses of Author Identification. American Speech, 
65(4), 334-340. 
Forman, G. (2003). An Extensive Empirical Study of Feature Selection Metrics for Text 
Classification. Journal of Machine Learning Research, 3(1), 1289-1305. 
 
274 
 
Foros Blog del Narco. (2010). Retrieved September 12, 2010, from 
http://www.foro.blogdelnarco.com/ 
Foros de México. (2012). Retrieved February 10, 2012, from http://forosdemexico.com/ 
Frontera al Rojo Vivo. (2012). Retrieved March 19, 2011, from 
http://gruporeforma.elnorte.com/libre/offlines/mty/frontera/ 
Gales, T. (2010). Ideologies of Violence: A Corpus and Discourse Analytic Approach to Stance 
in Threatening Communications. PhD dissertation, University of California, Davis. 
Retrieved November 18, 2010, from http://linguistics.ucdavis.edu/pics-and-
pdfs/Gales%20Dissertation.pdf 
Gamon, M. (2004). Linguistic Correlates of Style: Authorship Classification with Deep 
Linguistic Analysis Features. Proceedings of the 20th International Conference on 
Computational Linguistics: Vol.4 (pp. 611-617). Stroudsburg, PA: Association for 
Computational Linguistics. 
Georgakopoulou, A., & Goutsos, D. (2004). Discourse Analysis: An Introduction (2nd ed.). 
Edinburgh: Edinburgh University.  
Gibbons, J. (2003). Forensic Linguistics: An Introduction to Language in the Justice System. 
Malden, MA: Blackwell. 
GNU Operating System (2012). GNU Wget. Retrieved September 18, 2012, from: 
http://www.gnu.org/software/wget/ 
Goodman, J. D. (2011, September 15). In Mexico, Social Media Become a Battleground in the 
Drug War. The New York Times: The Lede. Retrieved September 18, 2011, from 
http://thelede.blogs.nytimes.com/2011/09/15/in-mexico-social-media-becomes-a-
battleground-in-the-drug-war/ 
 
275 
 
Grant, T. (2007). Quantifying Evidence in Forensic Authorship Analysis. International Journal 
of Speech, Language and the Law, 14(1), 1-25. 
Grant, T. (2010). Txt 4n6: Idiolect Free Authorship Analysis? In M. Coulthard, and A. Johnson 
(eds.) The Routledge Handbook of Forensic Linguistics (pp. 508-522). New York, NY: 
Routledge. 
Grant, T., & Baker, K. (2001). Identifying Reliable, Valid Markers of Authorship: A Response to 
Chaski. Forensic Linguistics, 8(1), 66-79. 
Grieve, J. (2007). Quantitative Authorship Attribution: An Evaluation of Techniques. Literary 
and Linguistic Computing, 22(3), 425-442. 
Hatcher, L. (1994). A Step-by-Step Approach to Using the SAS System for Factor Analysis and 
Structural Equation Modeling. Cary, NC: The SAS Institute. 
Hall, M. A. (1999). Correlation-based Feature Selection for Machine Learning. Unpublished 
MA thesis, The University of Waikato. Retrieved March 22, 2012, from 
http://www.cs.waikato.ac.nz/~mhall/thesis.pdf 
Harrison, S. (1998). E-mail Discussions as Conversation: Moves and Acts in a Sample from a 
Listserv Discussion. Linguistik online, 1, 4. Retrieved April 30, 2012, from 
http://www.linguistik-online.de/harrison.htm 
Hirst, G., & Feiguina, O. (2007). Bigrams of Syntactic Labels for Authorship Discrimination of 
Short Texts. Literary and Linguistic Computing, 22(4), 405-417. 
Holmes, D. I. (1994). Authorship Attribution. Computers and the Humanities, 28(2), 87–106. 
Honore, A. (1979). Some Simple Measures of Richness of Vocabulary. Association for Literary 
and Linguistic Computing Bulletin, 7(2), 172-177. 
 
276 
 
Hoover, D. L. (2003). Another Perspective on Vocabulary Richness. Computers and the 
Humanities, 37(2), 151-178. 
Howald, B. S. (2008). Authorship Attribution under the Rules of Evidence: Empirical 
Approaches - a Layperson’s Legal System. International Journal of Speech, Language 
and the Law, 15(2), 219-247. 
International Freedom of Expression Exchange. (2011, January 5) At Least 44 Journalists Killed 
for their Work in 2010, Say IFEX Members. Retrieved January 23, 2011, from 
http://www.ifex.org/international/2011/01/05/2010_killed_lists/ 
Jockers, M. L., Witten, D. M., & Criddle C. S. (2008). Reassessing Authorship of the Book of 
Mormon Using Delta and Nearest Shrunken Centroid Classification. Literary and 
Linguistic Computing, 23(4), 465-491. 
Juola, P. (2006). Authorship Attribution for Electronic Documents. In M. Olivier & S. Shenoi 
(Eds.),  Advances in Digital Forensics II (pp. 119-130). New York, NY: Springer 
Juola, P. (2007). Future Trends in Authoship Attribution. In P. Craiger & S. Shenoi (Eds.), 
Advances in Digital Forensics III (pp. 119-132). New York, NY: Springer. 
Juola, P. (2008). Authoship Attribution. Hanover, MA: Now Publishers. 
Juola, P., & Baayen, H. (2005). A Controlled –Corpus Experiment in Authorship Attribution by 
Cross-Entropy. Literary and Linguistic Computing, 20(1), 59-67. 
Jurafsky, D., & Martin, J. H. (2008). Speech and Language Processing: An Introduction to 
Language Natural Processing, Computational Linguistics, and Speech Recognition (2nd 
ed.). Upper-Saddle River, NJ: Pearson-Prentice Hall. 
Koppel, M., Argamon, S., & Shimoni, A. R. (2002). Automatically Categorizing Written Texts 
by Author Gender. Literary and Linguistic Computing, 17(4), 401-412. 
 
277 
 
Koppel , M., Schler, J., & Argamon, S. (2009). Computational Methods in Authorship 
Attribution.  Journal of the American Society for Information Science and Technology, 
60(1), 9-26. 
Koppel, M., Schler, J., Argamon, S., & Messeri, E. (2006). Authorship Attribution with 
Thousands of Candidate Authors. SIGIR 2006: Proceedings of the 29th Annual 
International ACM SIGIR Conference on Research and Development in Information 
Retrieval (pp. 659-660). Seattle, WA. 
Koppel, M., Schler, J., & Messeri, E. (2008). Authorship Attribution in Law Enforcement 
Scenarios. In C.S. Gal, P. Kantor, & B. Saphira (Eds.),  Security Informatics and 
Terrorism: Patrolling the Web (pp.111-119). Amsterdam: IOS. 
Koppel, M., Schler, J., & Zigdon, K. (2005). Determining an Author's Native Language by 
Mining a Text for Errors. Proceedings of the Eleventh ACM SIGKDD International 
Conference on Knowledge Discovery and Data Mining. Chicago, IL. 
Labov, W. (2001). Principles of Linguistic Change: Volume 2: Social Factors. Cornwall, UK: 
Blackwell. 
Lara, L. F. (Ed.). (1996). Diccionario del Español Usual en México. México: El Colegio de 
México. 
Lauría, C., & O’Connor, M. (2010, September). Silence or Death in Mexico’s Press: Crime, 
Violence, and Corruption Are Destroying the Country’s Journalism. (Special report). 
New York: Committee to Protect Journalists. Retrieved January 23, 2011, from 
http://www.cpj.org/reports/2010/09/silence-or-death-in-mexicos-press.php 
Manning, C. D., Raghavan, P., & Schütze, H. (2008). Introduction to Information Retrieval. New 
York, NY: Cambridge. 
 
278 
 
Math Forum. (2012). Browse High School Logarithms. The Math Forum @ Drexel. Retrieved 
February 2, 2012, from http://mathforum.org/library/drmath/sets/high_logs.html 
McLachlan, G. J. (2004). Discriminant Analysis and Statistical Pattern Recognition. Hoboken, 
NJ: John Wiley & Sons. 
McMenamin, G.R. (2002). Forensic Linguistics: Advances in Forensic Stylistics. Boca Raton, 
FL: CRC. 
Mikkelson, B., & Mikkelson, P. (2011). Cone of Silence. Retrieved September 11, 2012, from 
http://www.snopes.com/autos/techno/icecream.asp 
Mikros, G. K., & Argiri, E. K. (2007). Investigating Topic Influence in Authorship Attribution. 
In B. Stein, M. Koppel, & E. Stamatatos (Eds.), Proceedings of the SIGIR 2007 
International Workshop on Plagiarism Analysis, Authorship Identification, and Near-
Duplicate Detection, PAN 2007. 
Miller, C. R., and Shepherd, D. (2004). Blogging as Social Action: A Genre Analysis of the 
Weblog. In L.J. Gurak, S. Antonijevic, L. Johnson, C. Ratliff, and J. Reyman (Eds.), Into 
the Blogosphere: Rhetoric, Community, and Culture of Weblogs. Retrieved January 23, 
2011, from http://blog.lib.umn.edu/blogosphere/women_and_children.html 
Nazar, R., & Sánchez Pol, M. (2007). An Extremely Simple Authorship Attribution System. In 
M.T. Turell, J. Cicres, & M. S. Spassova (Eds.), Proceedings of the 2nd European IAFL 
Conference on Forensic Linguistics / Language and the Law 2006. Barcelona: 
Documenta Universitaria. 
NuevoLaredoEnVivo. (2012). Retrieved September 18, 2012, from 
http://nuevolaredoenvivo.es.tl/ 
 
279 
 
Olsson, J. (2008). Forensic linguistics: An Introduction to Language, Crime, and the Law (2nd 
ed.). London: Continuum. 
Orebaugh, A., & Allnutt, J. (2009). Classification of Instant Messaging: Communications for 
Forensics Analysis. The International Journal of Forensic Computer Science, 4(1), 22-
28. 
Ortega-Garcia, J., Gonzalez-Rodriguez, J., & Marrero-Aguilar, V. (2000). AHUMADA: A Large 
Speech Corpus in Spanish for Speaker Characterization and Identification. Speech 
Communication, 31, 255-264. 
Owyang, J. (2008). Understanding the Difference between Forums, Blogs, and Social Networks. 
Web Strategy. Retrieved January 23, 2011, from http://www.web-
strategist.com/blog/2008/01/28/understanding-the-difference-between-forums-blogs-and-
social-networks/ 
Paltridge, B. (2006). Discourse Analysis: An Introduction. London: Continuum.  
Peng, F., Schuurmans, D., Keselj, V., & Wang, S. (2003). Language Independent Authorship 
Attribution Using Character Level Language Models. Proceedings of the Tenth 
Conference on European Chapter of the Association for Computational Linguistics: Vol. 
1 (pp. 267-274). Stroudsburg, PA: Association for Computational Linguistics. 
Petrovska-Delacretaz, D., Chollet, G., & Dorizzi, B. (2009). Guide to Biometric Reference 
Systems and Performance Evaluation. London: Springer-Verlag. 
Planas, R. (2012, January 13). A Murder Every Half Hour in Mexico's Drug War. 
NYDailyNews.com: Daily News. Retrieved March 25, 2012, from 
http://articles.nydailynews.com/2012-01-13/news/30621243_1_drug-cartels-fight-drug-
war-alejandro-poire 
 
280 
 
Poyatos, F. (1993). Paralanguage: A Linguistic and Interdisciplinary Approach to Interactive 
Speech and Sound. Philadelphia, PA: John Benjamins. 
Procuraduría General de la República. (2010). Denuncia Ciudadana. Retrieved March 19, 2012, 
from http://www.pgr.gob.mx/denuncia/denuncia.asp# 
Raghavan, S., Kovashka, A., & Mooney, R. (2010). Authorship Attribution Using Probabilistic 
Context-Free Grammars. Proceedings of the 48th Annual Meeting of the Association for 
Computational Linguistics (pp. 38-42). 
Real Academia Española. (2005). Diccionario Panhispánico de Dudas (1st ed.). Madrid: 
Santillana. 
Real Academia Española. (2012). CREA. Retrieved March 25, 2012, from 
http://corpus.rae.es/creanet.html 
Rico-Sulayes, A. (2011). Statistical Authorship Attribution of Mexican Drug Trafficking Online 
Forum Posts. International Journal of Speech, Language and the Law, 18(1), 53-74. 
Rome, J.E., Miasnikov, A. D., & Haralick, R.M. (2003). Data Modelling and Description: A 
Guide to Using the SYLModel. (Tech. Rep. No. TR-2003011). New York, NY: Graduate 
Center of CUNY, Department of Computer Science. 
Rudman, J. (1998). The State of Authorship Attribution Studies: Some Problems and Solutions. 
Computers and the Humanities, 31, 351-365. 
Schiffrin, D., Tannen, D., & Hamilton, H. (Eds). (2001). The Handbook of Discourse Analysis. 
Oxford, UK: Blackwell. 
Schler, J., Koppel, M., Argamon, S., & Pennebaker, J. (2005). Effects of Age and Gender on 
Blogging. Proceedings of AAAI Spring Symposium on Computational Approaches for 
Analyzing Weblogs (pp. 199-205). 
 
281 
 
Searle, J. R. (1969). Speech Acts: An Essay in the Philosophy of Language. Cambridge: 
Cambridge University.  
Searle, J. R. (1975). Indirect Speech Acts. In P. Cole and J. L. Morgan (Eds.), Syntax and 
Semantics: Speech Acts (pp. 41-58). New York: Academic Press. 
Sichel, H. S. (1975). On a Distribution Law for Word Frequencies. Journal of the American 
Statistical Association, 70(351), 542-547. 
Simpson, E. H. (1949). Measurement of Diversity. Nature, 163, 688. 
Smith, M.W. A. (1990). Attribution by Statistics: A Critique of Four Recent Studies. Revue, 
Informatique et Statistique dans les Sciences Humaines, 26, 233-251. 
Solan, L. M., & Tiersma, P. M. (2004). Author Identification in American Courts. Applied 
Linguistics, 25(4), 448-465. 
Solan, L. M., & Tiersma, P. M. (2005). Speaking of Crime: The Language of Criminal Justice. 
Chicago: University of Chicago. 
Spassova, M. S. (2008). Las Perífrasis Verbales del Español en la Atribución Forense de Autoría. 
In R. Monroy, and A. Sánchez (Eds.), 25 Años de Lingüística en España: Hitos y Retos. 
Actas del XXVI Congreso de AESLA (pp. 605-614). Murcia: Universidad de Murcia. 
Spassova, M. S. (2009). El Potencial Discriminatorio de las Secuencias de Categorías 
Gramaticales en la Atribución Forense de Autoría de Textos en Español. Unpublished 
doctoral dissertation, Universitat Pompeu Fabra, Barcelona. 
Spassova, M. S., & Turell, M. T. (2007). The Use of Morphosyntactically Annotated Tag 
Sequences as Markers of Authorship. In M.T. Turell, J. Cicres, and M. S. Spassova 
(Eds.), Proceedings of the 2nd European IAFL Conference on Forensic Linguistics / 
Language and the Law 2006 (pp. 229-237). Barcelona: Documenta Universitaria. 
 
282 
 
Stamatatos, E., Fakotakis, N., & Kokkinakis, G. (2001). Computer-Based Authorship Attribution 
without Lexical Measures. Computers and the Humanities, 35, 193-214. 
StatSoft, Inc. (2011). Electronic Statistics Textbook. Tulsa, OK: StatSoft. Retrieved February 5, 
2012, from: http://www.statsoft.com/textbook/ 
Stenström, A. (1994). An Introduction to Spoken Interaction. New York: Longman. 
Stevenson, M. (2011, September 24). Woman Decapitated in Mexico for Web Posting. 
Associated Press. Retrieved September 25, 2011, from 
http://hosted.ap.org/dynamic/stories/L/LT_DRUG_WAR_MEXICO?SITE=TXKER&SE
CTION=HOME&TEMPLATE=DEFAULT 
Tambouratzis, G., & Vassiliou, M. (2007). Employing Thematic Variables for Enhancing 
Classification Accuracy Within Author Discrimination Experiments. Literary and 
Linguistic Computing, 22(2), 207-224. 
Tannen, D. (1990). You Just Don't Understand: Women and Men in Conversation. New York, 
NY: Morrow. 
Tannen, D. (1994). Gender and Discourse Style. New York, NY: Oxford University. 
Tannen, D. (1995). Talking from 9 to 5: Women and Men at Work. New York, NY: Morrow. 
Tannen, D. (2005). Conversational Style: Analyzing Talk among Friends. (New ed.). New York 
and Oxford: Oxford University. 
Tannen, Deborah. (2007). Talking Voices: Repetition, Dialogue, and Imagery in Conversational 
Discourse (2nd ed.). Cambridge: Cambridge University. 
Tearle, M., Taylor, K., & Demuth, H. (2008). An Algorithm for Automated Authorship 
Attribution Using Neural Networks. Literary and Linguistic Computing, 23(4), 251-270. 
 
283 
 
Thomson, R., & Murachver, T. (2001). Predicting gender from electronic discourse. British 
Journal of Social Psychology, 40(2), 193-208. 
Timm, N. H. (2002). Applied Multivariate Analysis. New York, NY: Springer-Verlag. 
UNED, Instituto Universitario de Educación a Distancia. (2002). Emoticones. Guía de 
Actividades de WebCT. Retrieved April 17, 2012, from 
http://www.uned.es/iued/guia_actividad/emoticones.htm 
Vanderveken, D., & Kubo, S. (Eds.). (2001). Essays in Speech Act Theory. Philadelphia, PA: 
John Benjamins. 
WebProWorld. (2004, April 5). A Vanilla Problem. Retrieved March 21, 2012, from 
http://www.webproworld.com/webmaster-forum/threads/8689-A-vanilla-problem 
Witten, I. H., Frank, E., & Hall, M. A. (2011). Data Mining: Practical Machine Learning Tools 
and Techiniques (3rd ed.). Burlington, MA: Morgan Kaufmann. 
WM Consulting. (2012). Knowledge is Security. Retrieved September 27, 2012, from 
https://sites.google.com/site/policereform/ 
Wolfram, W., & Schilling-Estes, N. (2006). American English: Dialect and Variation (2nd ed.). 
Cornwall, UK: Blackwell. 
Yule, G.U. (1944). The Statistical Study of Literary Vocabulary. Cambridge, England: 
Cambridge University. 
Zhang, Y., Dang, Y., & Chen, H. (2009). Gender Difference Analysis of Political Web Forums: 
An Experiment on an International Islamic Women’s Forum. Proceedings of the 2009 
IEEE International Conference on Intelligence and Security Informatics (pp. 61-64). 
Piscataway, NJ: IEEE. 
 
284 
 
Zheng, R., Li, J., Chen, H. & Huang, Z. (2006). A Framework for Authorship Identification of 
Online Messages: Writing-Style Features and Classification Techniques. Journal of the 
American Society for Information Science and Technology, 57(3): 378-393. 
Zheng, R., Qin, Y., Huang, Z., & Chen, H. (2003). Authorship Analysis in Cybercrime 
Investigation. Proceedings of the 1st NSF/NIJ Symposium, ISI2003 (pp. 59-73). Berlin: 
Springer-Verlag. 
Zvetco Biometrics. (2010). Biometric Knowledge Center. Retrieved March 23, 2011, from 
http://www.zvetcobiometrics.com/Support/definitions.jsp 
 
