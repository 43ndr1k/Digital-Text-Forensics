Proceedings of the Fifth International Conference on Machine Learning and Cybernetics, Dalian, 13-16 August 2006 
1-4244-0060-0/06/$20.00 ©2006 IEEE 
4551 
TOWARD UNIFICATION OF SOURCE ATTRIBUTION PROCESSES AND 
TECHNIQUES 
FOAAD KHOSMOOD, ROBERT LEVINSON 
Department of Computer Science University of California at Santa Cruz 
E-MAIL: foaad@soe.ucsc.edu, levinson@soe.ucsc.edu 
Abstract: 
Automatic Source Attribution refers to the ability for an 
autonomous process to determine the source of a previously 
unexamined piece of writing. Statistical methods for source 
attribution have been the subject of scholarly research for well 
over a century. The field, however, is still missing a definitive 
currency of established or agreed-upon classes of features, 
methods, techniques and nomenclature. This paper represents 
continuation of research into the basic attribution problem, as 
well as work towards an eventual source attribution standard. 
We augment previous work which utilized in-common, non-
trivial word frequencies with neural networks on a more 
standardized data set. We also use two other techniques: 
Phrase-based feature sets evaluated with naïve Bayesians and 
bi-gram feature sets evaluated with the nearest neighbor 
algorithm. We compare the three and explore methods of 
combining the techniques in order to achieve better results. 
 
Keywords:  
Source attribution; neural networks; naïve 
Bayesian; authorship attribution; n-grams; Meta 
predictors 
 
1.   Introduction 
 
Natural languages are ambiguous, imprecise, subject 
to interpretations and logical contradictions. These aspects 
make natural language processing one of the oldest and 
most difficult unsolved problems in artificial intelligence. It 
is precisely these same attributes, however, that allow 
humans to develop distinct styles of literary expression. 
Every conscious and unconscious choice allowed within the 
confines of the written communication contributes to the 
uniqueness of every writer and to some extent, every piece 
of writing. It seems a daunting task to identify, detect and 
exploit a definitive pattern of writing style. Yet this process 
is probably done hundreds of thousands of times a year by 
any number of school teachers across the world. An 
average high school English teacher in the United States 
can become fully familiar with dozens of individual writing 
patterns in a few weeks given only scattered samples and 
limited interaction with the authors. Can a computer 
achieve the same results? 
      As in genre detection [3], In order to classify any 
writing source, we must understand the elements that are 
important in determining style. This latter task is not trivial 
and could represent the most difficult part in solving this 
problem. We can at least make an attempt in defining the 
possible types of features that could be used in a particular 
technique. This information, together with the details of any 
machine learning technique represents the two basic 
elements of any source attribution engine.  
We use “source attribution” as to not restrict ourselves 
to a single human author. Attribution to groups of authors, 
as well as distinctive technical styles is possible and useful 
yet doesn’t fit the definition of “authorship attribution.” For 
a brief discussion on “source” versus “authorship” concepts, 
as well as a single-technique autonomous attribution engine, 
see [1].  
       Defining the scopes of possible feature sets, as well as 
different techniques is necessary in determining the best 
combination of classifiers yielding the highest performance.  
 
2.   Meta attribution process 
 
In order to better understand and cross apply peer 
research, as well as to foster more effective communication 
in any field, standards are necessary. Within automatic 
source attribution, such standards are still lacking. In recent 
years, some problem benchmarks have been developed 
allowing objective comparison of performances. However, 
process and technique standards are still lacking.   
       Patrick Juola of Duquesne University created a set of 
authorship attribution benchmarks [9] as part ALLC/ACH 
2004 which turned into an ongoing authorship attribution 
contest. This was a significant milestone in this field 
because while general machine learning benchmarks have 
existed for a long time, there were little specific authorship 
attribution benchmarks available openly on the Internet. 
The contest attracted several distinguished researchers in 
Proceedings of the Fifth International Conference on Machine Learning and Cybernetics, Dalian, 13-16 August 2006 
4552 
this field utilizing different techniques to find the solution 
to the benchmark set.  
But what are the elements of these techniques? Are 
there commonalities or structural isomorphism that can be 
exploited to further refine each technique or perhaps 
combine more than one technique? [10] In this section, we 
present a proposal for a “Meta process” which could be an 
important step in subdividing and classifying source 
attribution techniques to ultimately foster better 
communication in this area. 
As a basis for such a Meta process, we begin with the 
benchmark data provided by Juola and assume its format to 
be a standard for automatic source attribution input. The 
basic structure of Juola’s problem sets consists of a training 
set of documents for each author, and another set of testing 
samples. We designate these in the following manner. A set 
of authors, [A1, A2, A3…AN]; sets of training documents 
for each author [TA1, TA2, TA3…TAN]; A designation for 
each training document within a set TAX  which is [TAX(1), 
TAX(2) TAX(3)…TAX(M)]; And finally a set of unmatched 
testing samples [S1, S2, S3…SL]. Source attribution is 
completed by assigning each testing sample to a training set 
or to “none of the above.” For example:  
S1 -> A3 
S2 -> A1 
S3 -> “none” 
S4 -> A2                        ( 1 ) 
As with any machine learning problem, the input 
documents are often not compared directly. We first, need 
to develop a uniform vocabulary for the attribution process. 
Keselj, et. al. use these to develop “profiles” for each of the 
possible sources [2]. We extract features by applying an 
extraction function for each member of the vocabulary set 
per input document or document set. The result of such an 
extraction function is typically a real-valued number 
indicating some level of presence or frequency, but it can 
be any function of the vocabulary.  
We can thus present two high level functions: A 
feature selector and a feature extractor. A feature selector 
function accepts a set of corpora and outputs a feature 
vector which serves as a vocabulary for the feature 
extractor. The input corpora are either just the training set, 
or the result of the training set put through a pre-processing 
function, PP(). For example, if the training documents are 
in HTML, a pre-processing step would remove all HTML 
tags.  
FS(PP(TA)) = V                                        (2) 
There are great varieties of feature selection 
algorithms. Many for example, are based on words or n-
grams. In order to get some types of features, external 
information is necessary and additional data must be passed 
into the typical feature selection algorithm.  For example, if 
the feature selection algorithm seeks to produce a feature 
set based on non-trivial words, a list of non-trivial words in 
the relevant language must be input into the function, along 
with the training corpora.  
FS(PP(TA), I1, I2, I3, I4…) = VA             (3) 
After the features are designated, statistics for each 
feature must be gathered per input document as per the 
instructions of a feature extractor functions FE(). A feature 
extractor, accepts the vocabulary output by FS along with 
one corpus based on the training set and produces a feature 
vector indexed by the vocabulary, V, with one or more 
values associated with each member of V. 
FE(VA, PP(TA1)) = FA1                      (4) 
Note that the feature extractor function will also need 
to operate on each testing sample seamlessly. 
FE(VA, PP(S1)) = FS1                                      (5) 
The final step is a classifier function, C() which 
accepts all extracted features from the training set, and one 
vector from the testing set and performs the attribution. 
Depending on the particular classification algorithm, a 
separate training phase may be required.  
C(FS1, FA1, FA2… FAN ) = FA1                               (6) 
Classifier functions could be simple statistical 
calculations or they could be wrappers for what we 
normally call learning algorithms. These functions accept a 
vector of training set features and a series of testing set 
features. Attribution is assigned between two feature 
vectors, FS1, and FA1 for instance. By inference, we conclude 
that S1 -> A1.  
As an example, we can encode the process created in 
[1]. This technique used in-common, non-trivial word 
frequencies as features, and an adaptively dimensioned 
multilayer artificial neural network as the classifier. Figure 
1, demonstrates the process graphically. 
When capturing figure 1, according to our Meta 
attribution process, we get: 
CANN-dynamic(FEf(FScntw(PPrnrh(TA), NTWEnglish), PPrnrh(Sx)), 
FEf(FScntw(PPrnrh(TA), NTWEnglish), PPrnrh(TA1)), 
FEf(FScntw(PPrnrh(TA), NTWEnglish), PPrnrh(TA2)), 
… 
FEf(FScntw(PPrnrh(TA), NTWEnglish), PPrnrh(TAN))) 
(7) 
Where: 
PPrnrh is a preprocessing function which removes html and 
removes numbers and symbols (rhrn) from a document; 
FScntw is the feature selector function with the in-common, 
non-trivial words as features; 
NTWEnglish is a vector of non-trivial words (NTW) in English. 
This is required as outside information for FS to work; 
FEf is the feature extractor function which produces the 
simple frequencies (f) of in-common, non-trivial words; 
Proceedings of the Fifth International Conference on Machine Learning and Cybernetics, Dalian, 13-16 August 2006 
4553 
Sx is the sample being attributed in this run of the classifier 
function; 
TA1 are the training sets; 
CANN-dynamic is the dynamic artificial NN, classifier; 
Thus, an X->M attribution can result where M is between 1 
and N. 
 
 
Figure 1. Attribution procession[1] 
 
3.   Techniques 
 
In this section, we use the attribution process to define 
and perform three different source attribution techniques on 
a benchmark data set.  
The first technique is in fact the same one as described 
in the example in the previous section. In-common, non-
trivial word frequencies as features classified by a dynamic 
neural network. The full details are described in [1] 
although for a different data set. We use the same method 
on a new data set for comparison and demonstration 
purposes. 
The second technique is in fact, significantly different 
both in feature selection and evaluation. It demonstrates 
cooperation and process quite nicely. 
The previous technique concentrated on word 
occurrences and word frequencies evaluated with neural 
nets. In this process, we concentrate on phrase-based 
statistics evaluated with the naïve Bayes classifier. 
We use the CMU based Link Grammar software to 
derive phrase blocks from natural English sentences. Link 
Grammar is quite complex and its main use concerns the 
logical interpretation of the natural language through 
“linked” blocks of small sub-sentence structures. For our 
purposes, we only use the phrase parser component of Link 
Grammar. Given an English sentence, the phrase parser is 
able to output the logical structures of the sentence at the 
top level and recursively down. For example for the 
sentence "The quick brown fox jumped over the lazy dog,” 
Link Grammar is able to output this 
(S (NP The quick brown fox) 
   (VP jumped 
       (PP over 
           (NP the lazy dog)))) 
(8) 
where S, NP, VP and PP represent different sub-sentence 
phrase types. For a complete list of phrase types designated 
by Link Grammar, see table 1. For more explanation and 
documentation please see the Link Grammar web page.[5] 
 
Table 1. Phrase types in link grammar [5] 
PHRASE  EXPLANATION 
"ADJP" adjective phrase 
"ADVP" adverb phrase 
"NP" noun phrase 
"PP" prepositional phrase 
"PRT" particle 
"QP" number expression 
"S" clause: main or dependent 
"SBAR" 
embedded clauses with "that",  
relative clauses with a pronoun,  
dependent clauses with a conjunction,  
or indirect questions 
"SINV" a clause with s-inversion 
"VP" verb phrase 
"WHADVP" for "when" if used as conjunction 
"WHNP" 
one-word constituent to contain  
relative pronouns 
"WHPP" 
two-word constituent WHPP contains  
the preposition plus the relative pronoun 
     
For the feature set we are interested in, we can reduce 
the above to a nested set of phrase types, without the actual 
words in the sentence. For example: 
[S[NP][VP[PP[NP]]]]     (9) 
We can now designate a feature set we are interested 
in exploiting. For this feature set, we use the 13 phrase 
types produced by Link Grammar and two measures of 
sentence complexity that we also derive using Link 
Grammar output, for a static total of 15 features. The two 
measures of sentence complexity are as follows: 
1. “Depth”: is the highest level of nested phrases within one 
sentence.  
2. “Phrase Count”: is the number of phrases that Link 
Grammar designates in each sentence. There are some 
sentences that have long clauses and thus relatively fewer 
clauses, versus ones that have more clauses that are shorter. 
Note that since our features are static and pre-defined, 
our feature selection function FS(), is already hard-coded. 
Proceedings of the Fifth International Conference on Machine Learning and Cybernetics, Dalian, 13-16 August 2006 
4554 
In order to fit Link Grammar with our feature 
extraction process analysis from above, we can consider a 
simple sentence extractor as an initial pre-processing 
function. This function simply looks through a piece of text 
and separates each sentence into a new paragraph by itself, 
so that it can be in the form the Link Grammar parser 
expects for batch processing. Then, the sentences can be fed 
into the Link Grammar parser with the appropriate options 
and the results run through another pre-processing function 
where the actual words of the sentences are removed and 
the nested phrase types displayed in a simpler format. So 
far, we have three operations which constitute part of our 
feature extraction process. 
PPword remover(LGphrase parser(PPsentence ex tractor(X)))      (10) 
Our feature extraction function, operates on 
identifying the frequencies of each of the 13 phrase types 
within a document. The occurrence of each phrase type is 
counted and the total divided by the total number of phrases 
found in the document. For the two measures of complexity, 
we extract the average “depth” and the average phrase per 
sentence counts. Thus we end up with a feature vector F, 
consisting of 15 real numbers designating frequency of 
different phrase occurrences and average sentence 
complexity. 
For the actual attribution process we use the Naïve 
Bayes Classifier. As before, the classifier accepts a sample 
and a number of training corpora, producing a mapping of 
the from S->M. 
CNaiveBayes(FS1, FA1, FA2… FAN )                     (11) 
The third and final method is n-grams based feature set 
evaluated by a simple nearest neighbor technique based on 
a dissimilarity calculation described in [2]. In this technique, 
a set of character based n-grams are considered the 
vocabulary. Their frequency of occurrence within each 
source corpus is calculated and compared to the same 
frequency vector from the sample files.  
As in the previous two cases some pre-processing is 
necessary. In this case, all characters are converted to their 
capitalized form, all symbols, punctuation marks and 
numbers are removed, and all sets of separators (such as 
space, new line, tab, etc.) are converted into a single “_” 
character for ease of comparison. We designate all these 
preprocessing operations with a function PPngrams () 
accepting input text.  
The feature selector in this case is either a full set or 
some subset of the n-grams present in the pre-processed 
text. In [2], the authors use most frequently occurring n-
grams. For our purposes we use a set of all occurring bi-
grams which may be as high as 262 = 676 but is typically 
between 300 and 450. The feature extractor function, FE(), 
calculates frequencies of occurrence for each feature.  The 
CNN() classifier function,  accepts the F vectors (output 
from FE()) from each corpus along with the one from the 
sample text and uses nearest neighbor to perform the 
attribution.  
Having explored three distinct methods of 
classification, we can now consider a combination that may 
yield better results than either of the methods individually. 
Boosting, Bagging, mixture of experts and weighted voting 
are some examples. [9] At a high level, a combination or 
Meta predictor can be expressed as follows: 
M(CNaiveBayes(FS1, FA1, FA2… FAN ), CANN(FS1, FA1, FA2… FAN ), 
CNN(FS1, FA1, FA2… FAN ))                     (12) 
Note that the Meta predictor is not necessarily trained 
with the same data that the individual classifiers are trained 
with. Training the Meta learner as a whole requires 
feedback on the errors and thus would alter the internal 
regression process of each of the classifiers. That is to say, 
the classifiers would not behave as they would if there was 
no Meta predictor. It may therefore be preferable to accept 
trained classifiers as inputs into the Meta predictor. This, 
however, requires an additional set of training documents 
for the Meta predictor. 
 We are now ready to compare the feature vectors of the 
sources versus testing samples to make attribution 
determination. 
 
4.   Benchmark data 
 
We use the ad-hoc authorship attribution contest 
material accessible from Patrick Juola’s Duquesne 
University web page. The contest material consists of many 
different types of problems spanning 4 different languages. 
We evaluated problems A-E for this paper. Problems A and 
B were short English essays written by students. Problem C 
consisted of a set of novels which were much longer in 
depth. Problem D and E were plays.  
Table 2 outlines the problems we considered for this 
paper. 
Table 2.  Problems A-E 
PR. # AUTHORS # TRAINING DOCS # SAMPLES 
A 13 38 13
B 13 38 13
C 4 7 9
D 3 12 4
E 3 12 4
 
5.   Experiments and results 
      
We begin with problem A and compare the results of 
the two methods. Method 1 is the in-common, non-trivial 
word frequencies / neural nets. Method 2 is the sentence 
Proceedings of the Fifth International Conference on Machine Learning and Cybernetics, Dalian, 13-16 August 2006 
4555 
level phrase and complexity analysis evaluated by Naïve 
Bayes. Method 3 is common bi-gram analysis using Nearest 
Neighbor.  
Table 3 is a summery of the results obtained from 
applying each individual method. The M1, M2 and M3 
columns record the number of correct attributions using 
each of the methods. Taking into account, overlapping 
correct attributions, a total number of attributions can be 
calculated assuming an ideal meta-classifier that takes full 
advantage of each of the attribution methods. The last 
column calculates an ideal correct “hit-rate” in this example. 
Given 100% efficiency in cooperation, we can increase the 
performance beyond any of the individual methods, as 
demonstrated by problems A, B and C. 
 
Table 3. Problem A-E results 
Pr. 
Test 
cases M1 M2 M3 
M1 OR 
M2 OR 
M3 
% 
correct 
(ideal) 
A 13 3 3 6 9 69%
B 13 3 3 1 7 54%
C 9 5 4 4 8 89%
D 4 2 1 3 3 75%
E 4 2 0 1 2 50%
 
It’s perhaps natural that we should see tremendous 
improvement for both methods in problem C (over A and 
B). This problem has a large corpus. The documents are 
tens of pages long (as opposed to less than one page in 
Problems A and B) and thus, one can conclude more 
evidence available for attribution. Note that the in-common 
words on this problem were much higher than before as 
well, and that’s due to the increased similarity of the 
vocabulary usage between the various training documents. 
A 8/9 theoretical maximum can be achieved if the three 
methods are combined with maximum efficiency.  
It appears problems D and E were unusually difficult 
for Method #2. These problems were English language 
plays and what these results show is that there is much less 
consistency in phrase types in dialogue than in descriptive 
narratives such as those found in Problem C’s novels. 
Because of the low performance of Method 2, not much 
could be gained by combining the two methods in these 
problems.  
 
6.   Unification 
 
The last column on table 3 represents an ideal 
performance given the perfect meta-classifier. But such a 
result remains ideal unless an adaptive combination of the 
multiple classifiers can be constructed. This is what we 
alluded to as the “meta predictor” function M(). Meta-
predictors can be constructed in two flavors: The first can 
re-evaluate a method prediction by considering some 
intermediate values present in each predictor and 
combining them in a function to make another prediction. 
This approach essentially discounts the individual 
predictors’ predictions and attempts a smarter, more 
comprehensive prediction using the information from all 
the individual predictors. The second flavor is a more 
functional approach where the predictions made by the 
individual predictors are preserved. The Meta-predictor 
constructs a combination only of the final results from each 
predictor, performing, in essence a weighted combination 
of the predictors, rather than a prediction of solutions. 
For the purposes of this project, we opt for the 
functional approach. This means that the primary job of our 
Meta-predictor is to weigh each individual predictor in 
order to maximize the overall accuracy. 
One problem when dealing with different prediction 
processes is normalization. For example in problem A, 
sample 1, the Neural Net method assigns numbers ranging 
from 0 to 0.95 to each of the candidate sources. On the 
same problem, the Naïve Bayes calculation assigned 
numbers ranging from 0 to 0.157. The n-gram method 
assigned numbers in the range 185 to 240 where the lowest 
number representing least dissimilarity was chosen as the 
attribution candidate. Here, simple normalization will not 
suffice because of the differences between methods, we 
require something stronger: Standard scores. 
Standard Score is the distance between each value and 
the mean in terms of the standard deviation. It is calculated 
by the formula in figure 1. 
 
 
Figure 1. Standard Score 
 
Within each method, we first normalize the entire 
sample/source table by subtracting the mean from each 
assigned value, then dividing the result by the standard 
deviation of all values in the table. Then, for each sample, 
we calculate the standard score of all the different 
assignments made to the sources. We then proceed with 
selection of the highest (lowest in case of M3) value in 
absolute value terms to designate our predictions. Doing 
this for all three methods gives us a normalized set of 
standard scores, representing confidence in predictions that 
were made. These confidences can be used in our Meta 
processing application. For each sample problem, we 
choose the (Table 4).  
 
Proceedings of the Fifth International Conference on Machine Learning and Cybernetics, Dalian, 13-16 August 2006 
4556 
Table 4. Problem a confidences in standard scores 
Sample  M1 M2 M3 
1 2.781141 1.569181 0.387133
2 1.037492 0.568434 2.278415
3 1.42489 0.982579 0.977922
4 2.862032 0.597054 1.480626
5 2.52313 0.614164 2.008434
6 1.595202 1.494938 0.773015
7 1.717796 1.464381 1.790587
8 2.551099 1.495554 1.306358
9 1.495512 1.513549 1.378424
10 2.682015 0.620429 2.624927
11 2.851659 0.664209 2.812118
12 1.901653 1.541497 2.21323
13 2.778587 1.507817 1.542682
*Shaded cells represent correct answers from the ideal 
results 
 
Given this table, one formula we could use to 
construct the Meta predictor is a weighted ArgMax across 
the three methods for each sample:  Z=ARGMAX (M1*A1, 
M2*A2, M3*A3).  
Leaving all the coefficients as 1, would mean samples 
4, 8, 9, 10 and 11 would be misclassified, i.e. the Meta 
predictor would choose a Method other than what is shaded 
above. An assignment of A1=57, A2=101 and A3=111, 
would however correctly classify 8 of the ideal 9 cases, 
missing only sample #13. 
 
7.   Conclusions and Future Work 
 
Despite being useful tools, computational methods for 
source attribution are still suffering from a distinct lack of 
standards. It is hoped that by decoupling different 
techniques and operations as much as possible from the 
overall attribution technique, we could much better 
compare and contrast tools and methods. We can reduce the 
complexity of the problem, as well as facilitate multi-
technique solutions much easier.  
Our aim in this paper is to begin work toward an 
abstract Meta process for attribution as well as experiment 
with distinct methods of attribution and explore possibilities 
of efficient combinations in order to maximize performance.  
The contest material we have utilized represents one of 
the best open-source benchmarks available on the Internet 
in terms of breadth. Narrower problems have also been 
considered and excellent research has resulted from them 
[6][7][8]. There are, however some obvious problems with 
the contest that should be considered for such benchmarks 
in the future. For example, it is actually very difficult to 
verify the “automatic” nature of any kind of attribution 
technique. A researcher could be familiar with the contest 
input material and able to test with actual contest 
documents before hand. There could be problem specific 
biases introduced into the method, even without knowing 
the correct final attribution. Secondly, the researchers are 
able to tweak or essentially use different techniques on 
different problems. This may be an easier way to score high, 
but it will necessarily hurt the universality of the solution. It 
is difficult to enforce such rules in an online contest. It is 
impractical, at the moment, to not make available the 
contest training documents before the solution programs are 
solicited. But we hope that given enough standards and 
constraints, future contests can solicit entrants without 
showing any of the material beforehand. 
Future work in this area will have to advance along the 
dual track of 1) designing efficient standards and to 
facilitate easier communication and 2) advancing the art 
itself by experimenting with new techniques for attribution 
and 3) further explore the possibilities of effective 
combinatory or “Meta” classifiers.  
 
8.   References 
 
[1] Khosmood, Foaad and Kurfess, Franz, “Automatic Source 
Attribution of Text: A Neural Networks Approach,” In IJCNN-05, 
Montreal, Canada, June 2005. 
[2] Keselj, Vlado and Peng, Fuchun and Cercone, Nick and Thomas, 
Calvin. “N-gram-based Author Profiles for Authorship Attribution.” 
In Proceedings    of the Conference Pacific Association for 
Computational Linguistics, PACLING'03, Dalhousie University, 
Halifax, Nova Scotia, Canada, August 2003. 
[3] Brett Kessler, B and Nunberg, G., Schuetze H. “Automatic Text 
Genre Detection.” In Proceedings of the 35th Annual Meeting of the 
Association for Computational. Linguistics and the 8th Meeting of 
the European Chapter of the Association for Computational 
Linguistics, pages 32-38, Morgan Kaufmann Publishers, San 
Francisco CA, 1997. 
[4] Negnevitsky, Michael. Artificial Intelligence, First edition 2002. 
Addison     Wesley. ISBN 0-201-71159-1. 
[5] Link Grammar documentation, CMU. 
http://www.link.cs.cmu.edu/link/ Accessed March, 2006. 
[6] N. Fakotakis E. Stamatatos and G. Kokkinakis. “Computer-based 
Authorship Attribution without Lexical Measures.” Computers and 
the Humanities, Volume 35, Issue 2, May 2001, pp. 193-214 
[7] N. Fakotakis E. Stamatatos and G. Kokkinakis. “Automatic 
Authorship Attribution.” In Proceedings EACL-99. 1999. 
[8] N. Fakotakis E. Stamatatos and G. Kokkinakis. 2000. “Automatic 
Text Categorization in Terms of Genre and Author.” Computational 
Linguistics, 26(4), 471-495. 
[9] Patrick Juola: Ad-hoc authorship attribution contest 2004. 
http://www.mathcs.duq.edu/~juola/authorship_contest.html 
[10] Jakob Vogdrup Hansen, “Combining Predictors: Comparison of five 
Meta Machine Learning Methods,” Information Sciences vol. 119, 
1999. 
