COMBINATORIAL MARKOV RANDOM FIELDS
AND THEIR APPLICATIONS TO
INFORMATION ORGANIZATION
A Dissertation Presented
by
RON BEKKERMAN
Submitted to the Graduate School of the
University of Massachusetts Amherst in partial fulfillment
of the requirements for the degree of
DOCTOR OF PHILOSOPHY
February 2008
Computer Science
c© Copyright by Ron Bekkerman 2008
All Rights Reserved
COMBINATORIAL MARKOV RANDOM FIELDS
AND THEIR APPLICATIONS TO
INFORMATION ORGANIZATION
A Dissertation Presented
by
RON BEKKERMAN
Approved as to style and content by:
James Allan, Chair
W. Bruce Croft, Member
Erik Learned-Miller, Member
Andrew Cohen, Member
Andrew Barto, Department Chair
Computer Science
To my grandmother, who is my soul,
to my mother, who is my mind,
to my wife, who is my heart,
and to my daughter, who is my life.
ACKNOWLEDGMENTS
This work was supported in part by the Center for Intelligent Information Re-
trieval, in part by SPAWARSYSCEN-SD grant number N66001-02-1-8903, in part by
the Defense Advanced Research Projects Agency (DARPA) under contract number
HR0011-06-C-0023, and in part by Google Inc. Any opinions, findings and conclu-
sions or recommendations expressed in this material are the author’s and do not
necessarily reflect those of the sponsor.
I would like to thank my advisor, Prof. James Allan, for his positive attitude
toward my work, for his patience and unfailing optimism in difficult situations, for his
brilliant ideas and inspiring vision of the research field. I am grateful to Prof. W. Bruce
Croft for being an honest, fair opponent in research topics and a valuable aid in
non-research issues. I thank Prof. Erik Learned-Miller for many hours of fruitful
discussions and collaborative work. I thank Prof. Andrew Cohen for useful feedback
and viewing my work from a different angle. I thank Prof. Andrew McCallum for
introducing me to the area of graphical models. I thank Prof. Mehran Sahami for
providing intellectual and financial support to my research. I am deeply thankful
to Prof. Shlomo Zilberstein for his generous help prior to and throughout my PhD
studies. I thank Prof. Sridhar Mahadevan and Prof. Micah Adler for being great
teachers. I am especially grateful to Prof. Ran El-Yaniv for introducing me to science,
teaching me research principles, guiding me throughout my academic career, being a
mentor and a friend.
I would like to thank people who helped me in my thesis work. First, I am thank-
ful to Dr. Victor Lavrenko, who put my preliminary ideas into perspective and made
v
an important contribution to my work in its initial stages. I thank Prof. Leslie Pack-
Kaelbling, Prof. Polina Golland, Prof. Chris Pal, Prof. Rina Dechter, and Dr. Uri
Lerner for interesting discussions and notational clarifications. I thank Prof. Yoram
Singer, Prof. Nir Friedman, and Dr. Noam Slonim for valuable suggestions for im-
proving my methods. I thank Dr. Melinda Gervasio for providing priceless data. I am
very grateful to my co-authors Dr. Koby Crammer, Dr. Hema Raghavan, Dr. Jeon
Jiwoon, Prof. Koji Eguchi, Aron Culotta, and Gary Huang for investing their time
and effort in our mutual projects. I thank fellow lab members Dr. Fernando Diaz, Ao
Feng, David Mimno, Dr. Vanessa Murdock, Mark Smucker, Dr. Trevor Strohman, and
Dr. Charles Sutton for their generous technical assistance. Finally, I thank the (cur-
rent and former) departmental staff Kate Moruzzi, Sharon Mallory, Leeanne Leclerc,
Pauline Hollister, and Andre Gauthier for their prompt responses to numerous in-
quiries from my side.
I would like to thank people who made my family’s and my stay in the Pioneer
Valley enjoyable and comfortable. First, I thank Dr. Vanessa Murdock and her family
for being our guides to the Valley, for making us feel at home far from our home. I
thank my colleagues Dr. Hema Raghavan, Dr. Jeon Jiwoon, Dr. Ramesh Nallapati,
Mark Smucker, and Ben Wellner for their precious friendship. I thank Lena Bloch
for bringing music to our life. I am sincerely grateful to our Israeli friends in the
Valley: Prof. Hava Siegelmann, the Katz family, the Shenhar family, the Ofir family,
the Avishay family, Susan Moser, Yariv Levy, Nati Lenchner, Yariv Hofstein, Dan
Mason, and especially Inbar Bluzer for their moral support and warmth.
I would like to thank our personal friends for always being around, even if they live
thousands of miles away from us. Their incomplete list includes: the Spirt family, the
Averbouch family, the Akselrod family, the Lederberg family, the Zarzhevsky family,
the Zacharias famaly, the Lezhak family, the Etinberg family, the Tsitrin family, the
Rubinov family, the Gabrilovich family, the Malik family, Yael Weisberger, Tali Stern,
vi
Yuval Scharf, Eyal Gordon, Evgeny Panman, Prof. Wendy Wang, Dmitri Shtilman,
and, of course, Dasha Olshanetskaya.
Finally, I would like to thank my family for their unconditional love and tremen-
dous support. I thank all the Bekkermans in Israel, Russia, and Canada for being my
real family. I thank my parents-in-law, Tatyana and Alexander Nikitin, for accepting
me as their own son. I thank my father, Vladimir, for always thinking about me. I
am deeply grateful to my daughter, Naomi, for bringing light to my life. I do not
find appropriate words to express my gratitude to my mother, Faina, who devoted
her entire life to me. Every word in the rest of this thesis is a word of appreciation
to my wife, Anna, my other self.
vii
ABSTRACT
COMBINATORIAL MARKOV RANDOM FIELDS
AND THEIR APPLICATIONS TO
INFORMATION ORGANIZATION
FEBRUARY 2008
RON BEKKERMAN
B.Sc., TECHNION—ISRAEL INSTITUTE OF TECHNOLOGY
M.Sc., TECHNION—ISRAEL INSTITUTE OF TECHNOLOGY
Ph.D., UNIVERSITY OF MASSACHUSETTS AMHERST
Directed by: Professor James Allan
We propose a new type of undirected graphical models called a Combinatorial
Markov Random Field (Comraf) and discuss its advantages over existing graphical
models. We develop an efficient inference methodology for Comrafs based on com-
binatorial optimization of information-theoretic objective functions; both global and
local optimization schema are discussed. We apply Comrafs to multi-modal cluster-
ing tasks: standard (unsupervised) clustering, semi-supervised clustering, interactive
clustering, and one-class clustering. For the one-class clustering task, we analytically
show that the proposed optimization method is optimal under certain simplifying
assumptions. We empirically demonstrate the power of Comraf models by comparing
them to other state-of-the-art machine learning techniques, both in text clustering
and image clustering domains. For unsupervised clustering, we show that Comrafs
consistently and significantly outperform three previous state-of-the-art clustering
viii
techniques on six real-world textual datasets. For semi-supervised clustering, we
show that the Comraf model is superior to a well-known constrained optimization
method. For interactive clustering, Comraf obtains higher accuracy than a Support
Vector Machine, trained on a large amount of labeled data. For one-class clustering,
Comrafs demonstrate superior performance over two previously proposed methods.
We summarize our thesis by giving a comprehensive recipe for machine learning mod-
eling with Comrafs.
ix
TABLE OF CONTENTS
Page
ACKNOWLEDGMENTS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . v
ABSTRACT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .viii
LIST OF TABLES . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .xiv
LIST OF FIGURES . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .xvi
CHAPTER
1. INTRODUCTION . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1
2. PRELIMINARIES . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
2.1 Markov Random Fields . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
2.2 Three major learning paradigms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
2.3 Clustering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
3. COMBINATORIAL MARKOV RANDOM FIELDS . . . . . . . . . . . . . . 12
3.1 Combinatorial random variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12
3.2 Combinatorial Markov Random Fields . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
3.3 Algorithmic aspects of inference in Comrafs . . . . . . . . . . . . . . . . . . . . . . . . . 15
3.4 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
4. COMRAFS FOR MULTI-MODAL CLUSTERING . . . . . . . . . . . . . . . . 20
4.1 Choosing an objective function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
4.2 Clustering as combinatorial optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
4.3 Multi-way Distributional Clustering (MDC) . . . . . . . . . . . . . . . . . . . . . . . . . 25
4.3.1 Computational complexity of MDC . . . . . . . . . . . . . . . . . . . . . . . . . . 30
4.4 Clique-wise MDC . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31
4.5 Related work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32
x
4.6 Experimentation: email clustering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
4.6.1 Evaluation measure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34
4.6.2 Datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35
4.6.2.1 20 Newsgroups . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36
4.6.2.2 Enron Email Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37
4.6.2.3 CALO Email Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37
4.6.3 Baseline algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38
4.6.4 Implementation details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39
4.6.5 Comparative results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40
4.6.5.1 Experimentation with clustering schedule . . . . . . . . . . . . . 43
4.6.5.2 Experimentation with the length of local search . . . . . . . 44
4.6.6 Model analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46
4.6.7 Multi-modal clustering for social network analysis . . . . . . . . . . . . . 48
4.7 Experimentation: Web appearance disambiguation . . . . . . . . . . . . . . . . . . . 48
4.7.1 Related work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51
4.7.2 Evaluation criterion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51
4.7.3 WAD dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52
4.7.4 Baseline: link structure model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54
4.7.5 Comparative results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56
4.8 Experimentation: clustering scientific papers . . . . . . . . . . . . . . . . . . . . . . . . 59
4.9 Experimentation: clustering documents by genre . . . . . . . . . . . . . . . . . . . . . 61
4.9.1 Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64
4.9.2 Comparative results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64
4.10 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68
5. COMRAFS FOR SEMI-SUPERVISED LEARNING . . . . . . . . . . . . . . 70
5.1 Semi-supervised clustering with Comrafs . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70
5.1.1 Experimentation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72
5.2 Transfer learning with Comrafs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73
5.3 Interactive clustering with Comrafs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75
5.3.1 Related work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76
5.3.2 Interactive clustering scenario . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77
5.3.3 Clustering by sentiment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79
xi
5.3.4 Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80
5.3.5 Experimental setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81
5.3.6 Comparative results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83
5.4 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84
6. COMRAFS FOR ONE-CLASS CLUSTERING . . . . . . . . . . . . . . . . . . . . 86
6.1 Related work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 89
6.2 One-class clustering of words . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 90
6.3 Min-Entropy algorithm for one-class clustering in text . . . . . . . . . . . . . . . . 93
6.3.1 Relaxation of the uniformity assumption . . . . . . . . . . . . . . . . . . . . . 95
6.4 One-class co-clustering (OCCC) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 96
6.4.1 Heuristic for choosing the size of word cluster . . . . . . . . . . . . . . . . . 98
6.5 The Latent Topic/Background (LTB) model . . . . . . . . . . . . . . . . . . . . . . . . 98
6.6 Experimentation with OCCC and LTB . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100
6.6.1 Web appearance disambiguation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101
6.6.2 Re-ranking Web retrieval results . . . . . . . . . . . . . . . . . . . . . . . . . . . . 102
6.6.3 Detecting the topic of the week . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105
6.7 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 108
7. IMAGE CLUSTERING WITH COMRAFS . . . . . . . . . . . . . . . . . . . . . . 110
7.1 Related work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111
7.2 Multi-modal clustering objective, revisited . . . . . . . . . . . . . . . . . . . . . . . . . 112
7.3 Comraf*: a lightweight version of the Comraf model . . . . . . . . . . . . . . . . . 113
7.3.1 Inference in Comraf* . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115
7.4 Modalities of an image collection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 116
7.4.1 Rectangular blobs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 116
7.4.2 Blobs constructed by Comraf models . . . . . . . . . . . . . . . . . . . . . . . . 117
7.5 Experimentation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 119
7.5.1 Datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 120
7.5.2 Comparative results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 121
7.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 123
xii
8. CONCLUSION . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 126
BIBLIOGRAPHY . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 131
APPENDICES
A. PROOF OF THEOREM 6.2.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 140
B. DETAILS OF EM ALGORITHM FOR ONE-CLASS
CLUSTERING . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 142
xiii
LIST OF TABLES
Table Page
4.1 Statistics of email datasets. Number of distinct words and
number of correspondents are after preprocessing. . . . . . . . . . . . . . . . . . 36
4.2 Micro-averaged accuracy (± standard error of the mean, when
applicable) on the six datasets. The SVM supervised classification
accuracies are obtained with 4-fold cross validation. “OOM”
means “out of memory”: WEKA was unable to cluster 20NG, on
a 4GB RAM machine. Bold numbers are the best results over
all. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41
4.3 Macro-averaged accuracy (± standard error of the mean) on
CALO and Enron datasets. Each number is an average over ten
independent runs. Bold numbers are the best results over all. . . . . . . . 42
4.4 Micro-averaged accuracy (± standard error of the mean) on
CALO and Enron datasets. Each number is an average over ten
independent runs. Comrafs models are 2-modal, 3-modal and
4-modal, with the sequential optimization applied at each node.
Bold numbers are the best results over all. . . . . . . . . . . . . . . . . . . . . . . . 43
4.5 Statistics of the WAD dataset. Categories are different
namesakes or other in case if the page does not refer to any of the
namesakes. The last column shows the number of pages that
actually mention the person of our interest. . . . . . . . . . . . . . . . . . . . . . 53
4.6 Web appearance disambiguation results. Bi-modal Comraf
results are averaged over 4 independent runs, with the standard
error of the mean reported after the ± sign. . . . . . . . . . . . . . . . . . . . . . 57
4.7 Clustering scientific papers. Comraf models for clustering: (a)
documents and title words; (b) documents and citations; (c)
documents, title words and citations in a tree-structured model;
(d) documents, title words and citations in a loopy model; (e)
documents and abstract words. The bottom line is the
micro-averaged clustering accuracy obtained by those models. . . . . . . 60
xiv
4.8 Clustering by genre. Micro-averaged clustering accuracy on the
BNC corpus, averaged over four independent runs. Standard error
of the mean is shown after the ± sign. Comraf results with other
POS tuples, besides bigrams, are in Figure 4.9(left). The
BOW+POS hybrid setup is only applicable in Comrafs. . . . . . . . . . . . . 64
4.9 Performance of various methods per genre. For each genre we
show a list of sizes (in number of documents) of this genre’s
representation in various clusters. We sort this list by the size of
the representation from the largest to the smallest. An asterisk
after the number of documents means that this genre is dominant
in the corresponding cluster. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66
5.1 Clustering by sentiment. Clustering accuracy of Comraf models
(both interactive and non-interactive) is compared with clustering
accuracy of k-means and LDA, as well as with classification
accuracy of SVM. All results are averaged over four independent
runs. Standard error of the mean is shown after the ± sign. . . . . . . . 82
6.1 Most highly ranked words by OCCC and LTB, on the WAD
dataset. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 102
6.2 Re-ranking Web retrieval results: We compare one-class
clustering accuracy of our OCCC (with heuristic from
Section 6.4.1) and LTB (initialized with πi = 0.5) models with the
accuracy of the original Google rank lists, of one-class SVM
(OC-SVM) and of one-class Information Bottleneck (OC-IB) [28]
with l2-norm. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103
6.3 One-class clustering accuracy on the “topic of the week”
detection task. The accuracies are macro-averaged over the 26
weekly data chunks. Standard error of the mean is presented after
the ± sign. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 108
7.1 Categories (and their sizes) of the IsraelImages dataset. . . . . . . 120
7.2 Micro-averaged clustering accuracy on IsraelImages. All
IB/Comraf results are averaged over 10 independent runs with the
standard error of the mean reported after the ‘±’ sign. . . . . . . . . . . . . 121
7.3 Micro-averaged clustering accuracy on Corel. All IB/Comraf
results are averaged over 10 independent runs with the standard
error of the mean reported after the ‘±’ sign. . . . . . . . . . . . . . . . . . . . . 122
xv
LIST OF FIGURES
Figure Page
2.1 An example of a Markov Random Field. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
4.1 A Comraf graphs for: (a) hard version of Information Bottleneck; (b)
information-theoretic co-clustering; (c) one of the possible
4-modal Comrafs. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
4.2 A schematic view of bi-modal MDC with a simple, non-weighted
round-robin schedule. At each iteration black clusters are split
and then white clusters are merged. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
4.3 Comraf graphs for 2-modal, 3-modal and 4-modal Comrafs used in
our experiments. We consider interactions between combinatorial
random variables that correspond to documents Dc, words W c,
email correspondents Cc and email Subject lines Sc. Note that we
use only tree-structured models, as they are simpler than loopy
models and on the email foldering task they show comparable
results to those obtained with loopy models (see Section 4.6.6 for
a discussion). In Section 4.8 we present a result when a loopy
model is significantly superior to a tree-structured one. . . . . . . . . . . . . 35
4.4 Clustering accuracies as a function of the length of local search in
sequential MDC: ‘0.5’ on the x-axis means that the MDC’s
optimization routine was executed over one half of the data points
(chosen uniformly at random), while ‘3’ means that the
optimization routine was executed over every data point 3 times.
All our results are averaged over 10 independent runs. . . . . . . . . . . . . . 44
4.5 Experimenting with various Comraf graphs on mgervasio. . . . . . . . . . . . 46
4.6 Relevant and irrelevant Web pages according to the Link Structure
model. Relevant pages are within the δ-radius from the Core
Connected Component. White, gray and black colors indicate that
the pages are retrieved by three different queries. . . . . . . . . . . . . . . . . . 55
xvi
4.7 Precision/recall curve of the MDC algorithm. Points correspond to
consequent iterations of the algorithm (merges of Web page
clusters). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58
4.8 Comraf graphs for: (a) 1-way document clustering with POS
unigrams as an observed r.v. (shaded node); (b) 2-way clustering
of documents and POS bigrams (same as for POS 3-grams or
4-grams); (c) 2-way clustering with BOW; (d) 3-way clustering
with POS bigrams and BOW. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63
4.9 Clustering by genre. Micro-averaged clustering accuracy of Comraf
models as a function of: (left) size of POS n-gram (1-grams,
2-grams, 3-grams and 4-grams); (right) threshold on low frequency
words—a point i on the X axis means that in this experiment
words that appear in less than i documents are removed. . . . . . . . . . . . 68
5.1 Comraf graphs for: (left) semi-supervised clustering; (right)
clustering with transfer learning. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72
5.2 Plots (a)-(e): comparing accuracies of the semi-supervised Comraf
and the constrained optimization method on five email datasets.
Plot (f): the semi-supervised Comraf’s resistance to noise in
labeled data. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74
5.3 Interactive clustering by sentiment. Micro-averaged clustering
accuracy over various users: (left) over interactive learning
iterations (with original seed words only, after one correction step
and after two correction steps). The horizontal line is SVM
performance (after feature extraction using a given list of
sentimental words, and after training on over 20K documents);
(right) over categories of the dataset after two correction steps. . . . . . 83
6.1 (left) The simplest generative model; (right) Latent
Topic/Background model (Section 6.5). . . . . . . . . . . . . . . . . . . . . . . . . . . 91
6.2 An illustration of possible distributions of word counts in one-class
clustering: (left) uniform case; (right) multinomial case. Words
whose counts are above the threshold are considered relevant.
Note that in the multinomial case counts of some relevant words
can be lower than counts of non-relevant words. . . . . . . . . . . . . . . . . . . 92
xvii
6.3 The accuracy (as defined in Section 6.6, averaged over 100
independent runs) of identifying R in a simulation of the
generative process, over various values of the constant from
Equation (6.1) for the sampling size N . In Equation (6.1), the
value of this constant is set to 16. Here we show that the value of
2 is enough in practice. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 93
6.4 Web appearance disambiguation. (left) OCCC accuracy as a function
of the word cluster size; (right) accuracy of LTB (with the
underlying EM algorithm) over various initializations of πi
parameters: LTB shows a more robust behavior than OCCC,
however LTB’s maximal result (80.2%) is slightly inferior to the
OCCC’s (82.4%). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101
6.5 One-class clustering results on the “topic of the week” detection
task. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107
7.1 Comraf* models: (a) for images Gc and words W c from their
captions; (b) for images, words and colors Cc; (c) for images,
words, colors and blobs Bc; (d) straightforward generalization to
any number of modalities. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 114
7.2 (left) A Comraf model for simultaneously clustering images Gc and
their rectangular regions Rc, while taking into account words W c
from image captions, colors Cc and texture data T c; (right) a
translation of this model into a two-step Comraf*: the first
Comraf* is for clustering regions into blobs, whereas the second
Comraf* is for clustering images based on these blobs. . . . . . . . . . . . . 118
7.3 Experimentation with various numbers of: (left) colors on
IsraelImages in a 3-node images/words/colors Comraf*; (center)
colors for clustering regions in the 2-step Comraf* on IsraelImages;
(right) blobs on Corel in a 3-node images/words/blobs Comraf*.
Our baseline is the 2-node images/words clustering result. Left
and right graphs show the same trend: after reaching a certain
number of colors (256) or blobs (2000), the results vary only
insignificantly. The central graph, however, shows that too many
colors for clustering regions can hurt. . . . . . . . . . . . . . . . . . . . . . . . . . . . 123
7.4 Corel dataset. The first row shows clustering results using only
words. Swimmers and swimming tigers are clustered together
because they share common terms like “water” and “swim”. The
second and the third rows show clustering results using both
words and blobs. The swimmers and the swimming tigers are now
in two different clusters with other similar images. . . . . . . . . . . . . . . 125
xviii
7.5 IsraelImages dataset. People portraits and pictures of the menorah
monument are clustered together using caption words because
they have a word ‘Knesset’ (the Israeli parliament) in common:
the individuals are Knesset members, while the menorah
monument is placed in front of the Knesset building. The problem
is resolved after the color modality is added. . . . . . . . . . . . . . . . . . . . . 125
xix
CHAPTER 1
INTRODUCTION
Graphical models have proven themselves to be a useful tool in machine learning,
showing excellent results in information retrieval [81], natural language processing
[93], computer vision [45], and a variety of other fields [57]. A striking benefit of
using graphical models is the availability of black-box inference algorithms; once a
model is designed, it is usually straightforward to apply an existing optimization
procedure to make inferences in the model. Nonetheless, existing graphical models
have certain limitations, both within supervised and unsupervised frameworks (that
is, when training data is available or unavailable, respectively).
Supervised learning problems are usually solved using either generative graphical
models (i.e. Bayesian networks [87]) or discriminative graphical models (such as con-
ditional random fields [66]). While the goal of inference in generative models is to
estimate model parameters represented jointly with the data, the goal of inference in
discriminative graphical models is to estimate model parameters given the data, in
a conditional manner. The major problem of the discriminative approach is that in
order to construct a useful model, a large amount of labeled data is required. If the
amount of available labeled data is not enough for training a model, it often over-
fits : i.e. it performs well on data similar to the training data, but shows significantly
worse results on “unexpected” data instances. Unfortunately, it is usually impossible
to decide whether the amount of available training data is enough for constructing an
effective model. Also, a supervised model can perform poorly if trained on low-quality,
noisy data.
1
Unsupervised learning tasks are often performed using generative graphical models
(discriminative models are inapplicable to these tasks). The structure of a genera-
tive model describes a hypothetical procedure according to which the data was pre-
sumably generated. To design a generative model, practitioners traditionally make
assumptions about the model’s structure, based on domain knowledge, the need for
computational tractability, or both. Such assumptions may be inappropriate and
thus introduce undesired bias into the model. Another potentially problematic issue
is that modern generative models consist of thousands or even millions of nodes—
such models are difficult to fit, analyze and learn from data (model learning can easily
become infeasible if no significant restrictions on the class of models are made).
Since both generative and discriminative graphical models have significant draw-
backs, other types of graphical models are emerging, which now becomes an active
topic in machine learning. Recently, McCallum et al. [79] proposed a model that
combines generative and discriminative training. LeCun and Huang [69] proposed
energy-based models which allow optimization of non-normalized objective functions
factorized over a graphical model. However, both models are proposed only within
the framework of supervised learning.
In this thesis, we develop a new type of graphical model that has the following
characteristics:
• Unsupervised or semi-unsupervised flavor. The model is not overly de-
pendent on the quantity and quality of training data, but rather is applicable
to the cases when no or little labeled data is available. Even if the amount of
labeled data is sufficiently large, the model does not assume the data’s purity,
but takes advantage of this data by maximizing agreement of unlabeled and
labeled data in a semi-unsupervised setup.
• Minimal bias; minimal prior knowledge to be incorporated. The num-
ber of assumptions made on the model structure is as small as possible. In
2
particular, no generative assumptions are made, which minimizes the risk of
making assumptions that are misleading or unnecessary.
• Compactness, ability for model learning and comprehensive analysis
of the model behavior. Graphical models with millions of nodes are difficult
to comprehend and analyze. We take into account the fact that learning the
model structure can be optimal only for small models [27].
To meet the criteria above, we construct a graphical model which is intrinsically
different from existing graphical models. The most important difference is that in
our model, a certain portion of the model complexity is transferred from its graph
topology into its nodes, such that a resulting model consists of a small number of
“rich” nodes. It turns out that such a model is straightforwardly applicable to multi-
modal learning problems.
Multi-modal learning is a learning framework in the environment where multiple
views (or modalities) of the data are available. For example, in the text domain,
a set of documents is one modality of the data, while a set of words within those
documents is another modality. In fact, most real-world datasets are multi-modal.
Multi-modality of the data can be observed in a variety of research fields, such as:
• Text processing: documents, words, authors, titles, part-of-speech tags;
• Image processing: images, colors, texture, blobs, interest points, caption
words;
• Video processing: video signal, audio signal, frames, subtitles, transcripts;
• Bioinformatics: patients, tissues, samples, genes, proteins, compounds;
• Web information retrieval: Web pages, words, hyperlinks, markup primi-
tives;
3
• Data mining: movies, actors, directors, production companies;
and many others.
Three decades ago McGurk and MacDonald published their pioneering work [80]
that revealed the multi-modal nature of speech perception: sound and moving lips
compose one system, so to better process audio signals, an audio/video interaction
should be modeled. Since then, machine learning researchers have widely exploited
data multi-modality, using a variety of approaches, such as multi-modal neural net-
works [32], multivariate information bottleneck [46], and multi-view expectation max-
imization [21].
We propose a graphical model for multi-modal learning, only one node of which is
assigned for each modality, while edges represent statistical interactions between the
modalities. Since such interactions are symmetric, the resulting model is undirected,
i.e. they adopt the Markov Random Field (MRF) formalism. All the applications that
we consider in this thesis will be of the multi-modal nature, however, in our future
work, we will explore other types of possible applications.
The model we propose has the desired characteristics listed above:
• Multi-modality discloses the high-level structure of data, being therefore a cheap
and easily available form of supervision. Indeed, while obtaining labeled exam-
ples is expensive, deciding which data views are relevant to a particular task
in hand is usually straightforward. Taking advantage of this additional, struc-
tural knowledge allows us to successfully solve unsupervised and semi-supervised
problems.
• The only domain knowledge incorporated into the model is availability / us-
ability of multiple modalities and their interaction patterns. No assumptions
about prior distributions, latent variables and data point-wise interactions are
made which minimizes the model’s bias.
4
• Meaningful models can consist of just a handful of nodes, allowing easy analysis.
For example, the problem of choosing the most influential interactions between
nodes can be straightforwardly solved by testing a number of potentially good
combinations (or even all possible combinations, in case of models with only
few nodes).
In this thesis we explore a range of multi-modal clustering tasks, including one-
class clustering. We consider only discrete tasks over finite datasets, and we note that
those tasks have a combinatorial nature: given a dataset of n instances, the standard
(hard) clustering is the problem of partitioning these instances into k groups, whereas
one-class clustering is the problem of selecting k instances—both are well-known com-
binatorial problems. Therefore, we represent these learning tasks as combinatorial
optimization. In multi-modal cases, our goal is to simultaneously solve multiple com-
binatorial optimization problems, one for each data modality.
To summarize, the contributions of this thesis are as follows:
1. We propose a new type of graphical model called a Combinatorial Markov Ran-
dom Field (Comraf) that has beneficial properties (as discussed above): it mod-
els a high-level structure of the data, represented as a handful of “rich” nodes
(that correspond to data modalities) and interactions between them. The inner
structure of Comraf’s nodes is apparent and therefore does not require an ex-
plicit graphical representation in the model, which results in a light and elegant
layout.
2. We show that Comrafs are a natural modeling framework for multi-modal prob-
lems, able to obtain excellent results on real-world tasks. For each task, a par-
ticular objective function is designed that best fits the task. Therefore, Comrafs
are more flexible than most graphical models which are mainly limited to using
maximum likelihood (ML) or maximum a posteriori (MAP) objectives.
5
3. We apply Comrafs to unsupervised, semi-supervised, interactive and one-class
clustering tasks. We represent each task as a combinatorial optimization prob-
lem of the multi-modal nature. To our knowledge, most of the proposed tasks
are novel: we are not aware of previous work on semi-supervised multi-modal
clustering, interactive multi-modal clustering, or one-class multi-modal cluster-
ing.
4. We design information-theoretic objective functions for our models. In the case
of one-class clustering, we show that optimizing our objective function leads to
an optimal solution, under some simplifying assumptions. Also, in the case of
multi-modal clustering, we show that incorporating our objective function into
the Comraf model nicely generalizes previous successful clustering models.
5. We propose combinatorial optimization methods for solving our learning prob-
lems, for each of which we design efficient combinatorial algorithms and analyze
their computational complexity
6. Overall, we present a formal framework for multi-modal learning that brings
together two research areas: graphical models and combinatorial optimization.
The rest of this thesis is organized as follows: in Chapter 2 we provide some
necessary background; in Chapter 3 we describe the Comraf model; after which we
discuss each Comraf application in turn: (unsupervised) clustering in Chapter 4,
semi-supervised and interactive clustering in Chapter 5, and one-class clustering in
Chapter 6. In Chapter 7, we summarize previous chapters by exploring a variety of
Comraf modeling possibilities on an example of image clustering. In Chapter 8, we
conclude and discuss advanced Comraf problems that are not described in depth in
this thesis, such as multi-modal ranking.
6
CHAPTER 2
PRELIMINARIES
In this chapter, we first provide background information on graphical models,
and in particular on Markov Random Fields. We then present three major machine
learning paradigms: supervised, semi-supervised, and unsupervised learning. Finally,
we concentrate on data clustering—the most important application of unsupervised
learning—for which we give some necessary definitions and insights.
2.1 Markov Random Fields
A graphical model is a tuple (G,P ), where G is a graph whose nodes correspond to
random variables X = {X1, . . . , Xm} and whose edges E denote interactions between
these variables; P is a joint probability distribution defined over X. Let us use a
short notation P (x) = P (X1 = x1, . . . , Xn = xn), where each xi is a value from Xi’s
domain.
Definition 2.1.1 A graphical model (G,P ) is called a Markov Random Field (MRF)
if the following two conditions hold:
• (Positivity) ∀x : P (x) > 0
• (Markovianity) Let X1, X2 and X3 be three disjoint subsets of random variables
in X. We have that
P (X1,X2|X3) = P (X1|X3)P (X2|X3)
7
(i.e. X1 and X2 are conditionally independent given X3) iff every path between
a node from X1 and a node from X2 contains a node from X3.
The Markov blanket of a node Xi is defined as the set of all the immediate neigh-
bors of Xi in G. The Markovianity can then be restated as having each variable Xi
conditionally independent of the rest of the model, given its Markov blanket. Note
symmetric dependencies between nodes in an MRF—those dependencies are repre-
sented in G by undirected edges. Consequently, an MRF is often referred to as an
undirected graphical model.
An important observation of an MRF is that the joint distribution P is given
but (in most cases) not fully observed. The goal of an inference procedure in an
MRF is then to answer questions about the distribution P , such as what is the most
likely assignment x∗ = {x∗1, . . . , x∗m} to variables {X1, . . . , Xm} (this task is called
the Most Probable Explanation—MPE, see, e.g., [75]). Naturally, answering most
such questions is NP-hard since it potentially requires considering every possible
assignment. Thus, most inference techniques fall into the category of approximation
methods.
Definition 2.1.2 A distribution P is called a Gibbs distribution if it can be written
in the form
P (x) =
1
Z
exp
(∑
C
fC(xC)
)
,
where
• C is a clique in the graph G;
• fC is a real-valued function defined over values of random variables from C;
• Z is a normalization factor.
We refer to functions fC as log-potential functions (this term reflects the fact that their
exponents are traditionally referred to as potential functions). The normalization
factor Z is called a partition function.
8
3
X 1
X 2
X 4
X
Figure 2.1. An example of a Markov Random Field.
First proven by Julian Besag [19], the Hammersley-Clifford theorem states that
Theorem 2.1.3 The tuple (X , G) is an MRF if and only if P is a Gibbs distribution.
Note that log-potential functions can be defined on cliques of any size, however,
smaller cliques are usually preferred from the computational point of view. For exam-
ple, consider an MRF from Figure 2.1 where X1, X2, X3, X4 are multinomial random
variables, each with 10,000 possible values. We can consider two cliques of size 3,
i.e. X1 = {X1, X2, X3} and X2 = {X2, X3, X4} and then the joint distribution P (x)
can be factorized over those cliques as:
P (x) =
1
Zf
exp
∑
i
fi(x1) exp
∑
i
fi(x2),
such that each log-potential function fi will have to have 10
12 values. Inference in a
model like that can be infeasible in practice. We can also consider five cliques of size 2
(i.e. the edges {X1, X2}, {X1, X3}, {X2, X3}, {X2, X4}, and {X3, X4}), and factorize
the joint distribution accordingly. In this case, the log-potentials fi will have to have
only 108 values which is, in many cases, feasible.
2.2 Three major learning paradigms
In this thesis, we employ MRFs for solving unsupervised and semi-supervised
learning problems. When possible, we compare our results with the ones of supervised
9
learning methods, such as a Support Vector Machine (SVM) [109]. The most widely
studied type of supervised learning problems is classification: a model is trained on
(a large number of) data instances, each of which was a priori associated with one
or more target classes (we say that it was labeled). The model is then applied to
associate other, unlabeled data instances with the target classes. Obviously, it is
burdensome to collect and label the training data.
In contrast, in unsupervised learning, the model is built to fit the data as it is,
where no labeled instances are necessary. Data clustering is the main example of
unsupervised learning problems. There are two versions of data clustering: hard and
soft. In hard clustering, we partition the set of data instances into groups (clusters)
such that these groups are as homogeneous as possible (according to a given criterion).
Soft clustering is applied when data instances can belong to more than one cluster:
each data instance is associated with all the clusters according to a certain probability
distribution. In this thesis, we will consider only the hard clustering task, leaving the
soft clustering case for future work.
Unsupervised learning problems are usually solved in graphical models using the
maximum likelihood (ML) framework (see, e.g., [22]), where model parameters that
best explain the data are estimated. Most ML methods deal with approximating Zf ,
which is generally a hard task, because Zf depends on the particular choice of fi’s
and is a sum over all the possible configurations. In this thesis, we will apply the
MPE framework instead, for the reasons that will be clear later.
Semi-supervised learning is usually viewed from two difference perspectives: (a) as
training a supervised model while taking advantage of available unlabeled instances;
(b) as building an unsupervised model that takes advantage of some labeled data,
whose amount is not enough to train a supervised model. In this thesis, we focus on
the latter type, which is often called semi-supervised clustering [116].
10
2.3 Clustering
Most existing data clustering algorithms belong to one of two categories: hierar-
chical (top-down or bottom-up) and flat. A flat algorithm starts with data instances
distributed over k clusters (where k is the desired number of clusters) and reorga-
nizes / updates the clusters until convergence. A top-down hierarchical algorithm
is initialized with one cluster containing all data instances, which is then iteratively
split into portions until the desired number of clusters k is achieved. A bottom-up
hierarchical algorithm starts with singleton clusters (one data instance per cluster)
and merges clusters iteratively until, again, k is reached. An obvious drawback of
flat algorithms as compared to hierarchical ones is in the fact that flat procedures are
often heavily dependent on their initialization: most of them perform poorly when
initialized at random. Many heuristics have been proposed that come up with mean-
ingful initial clusters (see, e.g., [40]), however, most of these heuristics are domain
specific. Therefore, in this thesis we concentrate on hierarchical clustering schema,
although we occasionally mention flat methods as well (see e.g. Section 4.4).
11
CHAPTER 3
COMBINATORIAL MARKOV RANDOM FIELDS
In this chapter, we first introduce the notion of a combinatorial random vari-
able, then propose Combinatorial Markov Random Fields (Comrafs), and develop an
inference technique for Comrafs based on combinatorial optimization.
3.1 Combinatorial random variables
Definition 3.1.1 A combinatorial random variable (or combinatorial r.v.) Xc is a
discrete random variable defined over a combinatorial set.
A combinatorial set in mathematical parlance means a set of all subsets, parti-
tionings, permutations etc. of a given finite set. To capture this intuition, we define
a finite set A as combinatorial if its size is exponential with respect to another finite
set B, i.e. log |A| = O(|B|). As an example, a combinatorial r.v. Xc can be defined
over all the outcomes of lotto 6 of 49, in which 6 balls are selected from 49 enumer-
ated balls to produce an outcome of the lottery. In this case, set B consists of 49
balls, while set A consists of
(
49
6
)
possible choices of 6 balls from B. In a fair lottery,
the distribution of Xc is uniform: each outcome is drawn with probability 1/
(
49
6
)
.
However, in an unfair lottery, some outcomes are more probable than others.
It is easy to come up with other examples of combinatorial r.v.’s: over all the
possible translations of a sentence, over all the possible orderings in a ranked list
of retrieved documents, etc. In Chapter 4 we consider combinatorial r.v.’s over all
partitionings of a given set; in Chapter 6 we consider combinatorial r.v.’s over all
subsets of a given set.
12
From the theoretical perspective, a combinatorial r.v. behaves exactly as an or-
dinary discrete random variable with a finite domain. However, from the practical
point of view, a combinatorial r.v. is different: in most real-world cases, the event
space of Xc is so large that the distribution P (Xc) cannot be explicitly specified.
Moreover, the Most Probable Explanation (MPE) task (see Chapter 2) for combina-
torial r.v.’s can be computationally hard. Considering an unfair lottery example, in
which the distribution of Xc is flat (close to uniform), say, the probability of value
{7, 23, 29, 35, 48, 49} is 0 and the probability of value {4, 18, 28, 37, 39, 43} is 2/(49
6
)
,
while the rest of the values still have the probability 1/
(
49
6
)
. An exponentially long
sampling process is required to detect the most probable value.
3.2 Combinatorial Markov Random Fields
Definition 3.2.1 A combinatorial Markov random field (Comraf) is a Markov Ran-
dom Field, at least one node of which is a combinatorial random variable.
In this thesis, we will consider only Comraf models, every node of which is a
combinatorial r.v. As in any other MRF, random variables in Comraf models can be
in either a hidden or observed state. A combinatorial r.v. is hidden if it can take any
value from its event space. A combinatorial r.v. is observed if its value is preset and
fixed. Chapter 4 presents Comraf models with only hidden variables. In Chapters 5
and 7 we introduce observed random variables to Comraf models.
An edge eij = (X
c
i , X
c
j ) in a Comraf graph corresponds to a statistical interaction
between combinatorial r.v.’s Xci and X
c
j . A presence or an absence of edge eij artic-
ulates whether Xci and X
c
j stay in a tight statistical interaction or not. For example,
consider three nodes in a Comraf graph for an email collection, one of which (M c)
corresponds to the modality of email messages, another (Ac) to the authors of the
messages, and the third one (Sc) to the subject lines. Obviously, email messages stay
in statistical interactions both with their authors and their subjects. However, it is
13
not straightforward whether the authors’ modality interacts with the subject lines.
Indeed, the subject line in the first message of an email thread was given by its sender,
while all the other messages in this thread are often “forced” to use the same subject
line, possibly given by another sender. Therefore, it might be natural to have edges
(M c, Ac) and (M c, Sc) in the Comraf graph, and to drop the (Ac, Sc) edge.
One might argue that, while the (Ac, Sc) interaction is not clearly present, it
might still exist in the data. As in any graphical model, there is a tradeoff between
the Comraf’s adequacy and the computational complexity of its inference procedure.
The larger the Comraf model is, the more difficult the inference would be. Thus, it
is the practitioner’s responsibility to decide which edges will be present in the model
and which will be absent. Let us emphasize this again: since Comraf models are
usually compact, a model learning procedure can be used to automatically infer the
optimal set of model’s edges. Keeping in mind the model learning option, we leave it
for our future research. Also note that, as in any other MRF, the lack of statistical
interaction between variables Xci and X
c
j (and therefore the absence of edge eij in a
Comraf graph) implies conditional independence of Xci and X
c
j given the rest of the
model.
As discussed above in Section 3.1, even simple inference tasks (trivially performed
on ordinary random variables) are computationally hard for combinatorial random
variables. Since every combinatorial r.v. carry a large portion of a Comraf complex-
ity, even small Comraf models (of just a few nodes) remain non-trivial. Inference in
Comrafs is thus viewed from a different perspective than inference in other graphical
models. Usually, an inference procedure is composed from traversing the graph G
and performing computations at the graph’s nodes. In most graphical models, where
nodes are ordinary random variables, the computation step is simple, while traversing
the (large) graph is a resource-demanding process. In these cases, it is very important
to keep track of numerous intermediate computations. Simplicity and homogeneity
14
of such process play a crucial role in those models. For example, it is impractical to
optimize different objective functions in different regions of the graph G. These con-
siderations dramatically restrict practitioners in their choice of an objective function
for their models. Most graphical models optimize the maximum likelihood objec-
tive (see Chapter 2). However, the situation is different for Comrafs. In Comrafs,
computations performed in each node are the most intensive part of the inference
process. Traversing the graph however is relatively inexpensive as the number of
nodes is small in comparison to other models. Thus, practically unrestricted variety
of objective functions can be considered, both probabilistic and non-probabilistic,
homogenous and heterogenous in various regions of G.
Let us now show that optimizing an arbitrary objective function over G can be
represented in terms of an MPE inference in a Comraf. As discussed in Chapter 2,
the joint distribution of random variables in an MRF is factored over the graph G as:
P (x) =
1
Zf
exp
∑
i
fi(x),
where log-potential functions fi are arbitrary functions defined over cliques in G.
If we fix the log-potentials fi for each clique, the partition function Zf becomes a
constant. Thus, in the MPE inference, we directly optimize a non-normalized linear
combination of the log-potential functions:
x∗ = arg max
x
P (x) = arg max
x
exp
∑
i
fi(x) = arg max
x
∑
i
fi(x), (3.1)
which now solely depends on the choice of the log-potentials.
3.3 Algorithmic aspects of inference in Comrafs
As we have mentioned in Section 3.1, in most cases it is infeasible to explicitly
specify the distribution P (Xc), i.e. to represent it as a probability table in which
15
each value is assigned a certain probability mass. In such situations, estimating the
joint distribution of all the Comraf nodes becomes even less possible. A somewhat
traditional approach to dealing with the problem of combinatorial explosion is to
transfer the probabilistic setup to the continuous space. However, it is well known
(see, e.g., [86]) that such a transformation may potentially cause significant approxi-
mation errors. Another alternative is to apply a local search in the event space L of
Xc. Yet another possibility is to apply more sophisticated combinatorial optimization
methods, such as Branch and Bound [67].1 In this thesis, we choose the local search
approach. Let us proceed with definitions.
Definition 3.3.1 A transaction is an elementary operation in traversing the event
space L of a combinatorial r.v. Xc.
In the other words, a transaction is a jump operation between neighboring points
in the event space L (i.e., closest values of Xc). For each particular learning task,
the event space of a combinatorial r.v. will be defined differently, and so will be a
transaction. For now, let us assume that we know how to move from one value of Xc
to another.
Definition 3.3.2 A path in L is a sequence of transactions. A path is called advan-
tageous if it leads to a more likely value of Xc, otherwise it is disadvantageous.
In a Comraf model with more than one combinatorial r.v., the most straightfor-
ward version of an inference algorithm would be a variation of the Iterative Condi-
tional Modes (ICM) method [20]. ICM optimizes the objective (3.1) for each node of
an MRF iteratively (in a round-robin fashion), given its Markov blanket. A possible
drawback of this approach can be evidenced when the linear combination (3.1) is
1Branch and Bound has been used for (uni-modal) clustering by Koontz et al. [62], however it is
questionably applicable to multi-modal learning due to its high computational complexity.
16
Input:
G – Comraf graph of nodes {Xc1 , . . . , Xcm} and edges E
P (Xi, . . . , Xm) – joint probability distribution of data, factorized over G
l – number of optimization iterations
Output:
Most likely xc1,l, . . . , x
c
m,l
Initialization:
For i = 1, . . . , m do
Select a point in Li to be an initial value xci,0 of X
c
i
Compute the initial joint P (xc1,0, . . . , x
c
m,0), factorized over G
Main loop:
For j = 1, . . . , l do
Select variable Xci′ for optimization
Construct advantageous path
(
xci′,j−1 → xci′,j
)
in Li′
For all i 6= i′ do xci,j = xci,j−1
Algorithm 1: A template of an MPE procedure in Comrafs.
taken over log-potential functions fi, which are intrinsically different in their mag-
nitude and/or semantics (such that explicitly taking their linear combination might
not be beneficial). For these situations, we propose another version of an inference
algorithm, called clique-wise optimization (CWO), which is a variation of a local op-
timization method in an MRF. Similarly to ICM, the CWO algorithm iterates over
nodes in the MRF. For each node, a clique that contains this node is chosen and the
objective (3.1) is optimized with respect to the chosen clique only, i.e. independently
of the rest of the model. Sutton and McCallum [102] apply a similar method (called
piecewise training) in a supervised setting. Bouvrie [23] proposes to approach the
multi-modal clustering problem by iteratively applying a bi-modal clustering algo-
rithm. To some extend, Bouvrie’s method can be considered as a special case of
CWO.
A template pseudo-code for the MPE approximation in a Comraf is given in
Algorithm 1. For each combinatorial r.v. Xci in the Comraf, we first select and fix
its initial value as a point in the event space Li. We then round-robin over each X
c
i ,
for which we search for an advantageous path in Li. When this path is constructed,
we fix its destination point to be a new value of Xci and move to another node. We
17
repeat this procedure l times. To transform this template into an actual algorithm,
we need to make the following choices:
• How to select initial values for each combinatorial r.v. in the Comraf.
• How to determine an ordering for variables in the optimization procedure (and
an ordering of cliques in CWO). One obvious approach is a plain or weighted
round-robin, but more sophisticated choices can also be made.
• How to construct an advantageous path in L.
We will address these points in the following chapters of this thesis.
3.4 Summary
While technically being graphical models, Comrafs are very different from existing
graphical models: all Comraf models we propose in this thesis are small models with
‘rich’ nodes, while existing graphical models are usually large models with ‘simple’
nodes. No existing inference techniques are applicable to Comrafs (as they cannot
deal with nodes as complex as combinatorial r,v,’s), so we have developed a new
inference framework for Comrafs.
The major advantage of Comrafs over existing graphical models is that Comrafs
provide a more flexible modeling environment: existing models are able to model
data only in terms of the graph G, while their objective function and their infer-
ence algorithm are generic rather than task-specific. Usually, this property is not
considered to be a drawback of the graphical model framework: once the graph G is
designed for a certain task, it is straightforward to apply an existing inference method
to this graph. However, existing inference methods are approximations to the NP-
hard inference problem, and thus make various assumptions that can potentially be
inappropriate for the particular task being solved. The main disadvantage of generic
18
inference methods is that they make the same assumptions for every task. A practi-
tioner can choose one of a handful of existing inference methods (such as mean-field,
variational approximation, belief propagation, Gibbs sampling etc. [58]) for her task,
some of which can work better for this task while some can work worse, but none is
specific for the task.
Comrafs, in contrast, have three degrees of freedom: designing the graph G, the
objective and the inference algorithm, all specific for the task in hand. And as we will
show below, this flexibility leads to constructing models that demonstrate excellent
performance on various unsupervised and semi-supervised learning tasks.
19
CHAPTER 4
COMRAFS FOR MULTI-MODAL CLUSTERING
Multi-modal (hard) clustering is a problem of simultaneously constructing m par-
titionings of m data modalities, e.g. of documents, their words, authors, titles etc.
When clustering modalities simultaneously, one can overcome the statistical sparse-
ness of the data representation, leading to a dense, smoothed joint distribution of
the modalities that would result in (hypothetically) more accurate clusterings than
the ones obtained when each modality is clustered separately. Based on our previous
work [10, 12], we will empirically justify this hypothesis. In this chapter, we pro-
pose a Comraf model for multi-modal clustering (for motivation and discussion, see
Chapter 1). Let us first introduce the notation.
Let s1, s2, ..., sN be a dataset of N i.i.d. samples drawn from some discrete distri-
bution. Let X = {x1, x2, ..., xn} be the set of n unique values comprising the event
space from which samples si are drawn. We now define a random variable X such
that P (X = xi) is given by the empirical frequencies of samples with value xi in the
dataset (i.e., X has a multinomial distribution estimated using maximum likelihood).
Define a hard clustering xc to be a partitioning of X . Let X c = {xc1, xc2, ..., xcK} be
the combinatorial set of all K partitionings of X , where K is exponential in the size
of X . We will refer to the subsets of the j-th partitioning xcj as {x̃j,1, x̃j,2, ..., x̃j,kj}.
That is, the first subscript in x̃j,i is the index of a particular partitioning, and the
second subscript is the index of a subset (a cluster) within that partitioning.
Define X̃j to be a random variable over the subsets (clusters) in a partitioning
xcj, with the probability of selecting a cluster defined as the probability of selecting
20
any one of its members: P (X̃ = x̃j,i) =
∑
x∈x̃j,i P (x). Finally, define X
c to be
a combinatorial r.v. with the event space X c. In this thesis, we shall use parallel
notation for different modalities of data, replacing the “x’s” in the above notation
with variables appropriate for the data source. For example, wi would represent a
specific word in a dataset, w̃ would be a cluster, wc would be a partitioning of words,
and so on.
4.1 Choosing an objective function
As discussed in Section 3.2, interactions between combinatorial r.v.’s are repre-
sented by edges in a Comraf graph. To use the objective from Equation (3.1), we
should choose relevant cliques in the Comraf graph and define log-potential functions
over these cliques. To make the inference feasible, we consider only the smallest
cliques, i.e. adjacent pairs. Since our inference objective allows us to use complicated
log-potential functions (see, again, Section 3.2), we use the mutual information (MI)
between r.v.’s defined over values of adjacent nodes. Let xci and x
c
j be such values
(particular partitionings of two modalities). A log-potential is then defined:
f(xci , x
c
j) = I(X̃i; X̃j) =
∑
i′,j′
P (x̃i,i′ , x̃j,j′) log
P (x̃i,i′ , x̃j,j′)
P (x̃i,i′)P (x̃j,j′)
. (4.1)
Our motivation for choosing MI as a log-potential function is as follows: a linear
combination of MI terms has traditionally been used as a clustering criterion, both
in uni-modal clustering methods, such as Information Bottleneck (IB) [106], and in
bi-modal methods [35]. Slonim et al. [97] generalize the IB clustering criterion to a
multivariate case: in place of mutual information, they use Multi-Information1
I(X̃1; . . . ; X̃m) =
∑
i′1,...,i′m
P (x̃i1,i′1 , . . . , x̃im,i′m) log
P (x̃i1,i′1 , . . . , x̃im,i′m)
P (x̃i1,i′1) . . . P (x̃im,i′m)
, (4.2)
1For alternative definitions and discussions on Multi-Information, see [114, 54].
21
which naturally factorizes over a directed graphical model. With little effort, we
can show that Multi-Information also factorizes over a tree-structured undirected
graphical model, reducing to a sum of pairwise MI terms defined over edges of the
tree. However, in the case of an arbitrary Comraf graph, Multi-Information cannot
be fully factorized. In general, objective functions based on high order statistics
(including Multi-Information) are problematic for loopy Comraf graphs. From a
statistical viewpoint, it is not clear whether we can extract reliable estimates for the
full joint distribution P (X̃1, . . . , X̃m). Still, we can approximate Multi-Information
by a sum of pairwise MI terms. Estimating the quality of such an approximation
remains an open question.
Thus, substituting log-potentials (4.1) into the MPE inference model (3.1), our
objective function for multi-modal clustering with Comrafs is then:
xc∗ = arg max
xc
P (xc) = arg max
xc
∑
(Xci ,X
c
i′ )∈E
I(X̃i; X̃i′). (4.3)
This maximization is performed subject to constraints on the cardinalities ki = |X̃i|,
i = 1, . . . , m (i.e., the number of clusters is fixed). Without these constraints, the
maximization would lead to a degenerative case of all singleton clusters. Note that
these constraints do not necessarily imply the use of a flat clustering scheme (see
Chapter 2). In a particular clustering algorithm, clusters can be split or merged,
after which the number of clusters is fixed and the optimization of the objective
function (4.3) is performed.
We apply the ICM scheme (see Section 3.3) to multi-modal clustering: we iterate
over combinatorial r.v.’s in the Comraf graph, and at each iteration (over node Xci )
we construct the most likely clustering xc∗i by optimizing the objective function (4.3).
It is important to note that in the general case the objective (4.3) has O(|X|2) terms.
However, at each ICM iteration only one node is optimized, therefore the objective
22
c
Y
X
c
c
Y
X
c
D
c
A
c
T
c
W
c
(a) (b) (c)
Figure 4.1. A Comraf graphs for: (a) hard version of Information Bottleneck; (b)
information-theoretic co-clustering; (c) one of the possible 4-modal Comrafs.
is reduced to:
xc∗i = arg max
xci
∑
i′: (Xci ,X
c
i′ )∈E
I(X̃i; X̃i′) (4.4)
that sums over only O(|X|) neighbors of Xci (i.e. its Markov blanket).
The resulting model has two important special cases:
• A hard version of Information Bottleneck [106]. In Information Bot-
tleneck, given two modalities X and Y , a clustering xc∗ is constructed that
maximizes information about Y (and minimizes information about X):
xc∗ = arg max
xcj
(
I(X̃j; Y )− βI(X̃j; X)
)
, (4.5)
where β is a Lagrange multiplier. The compression constraint I(X̃j; X) can
be omitted if the number of clusters is fixed: |X̃j| = k. Consider graph G in
Figure 4.1(a), where a shaded Y c represents an observed variable.2 Over the
only clique in G, we define one log-potential which is the mutual information
I(X̃j; Y ). The MPE optimization objective for such Comraf is then:
xc∗ = arg max
xcj
P (xcj, y
c) = arg max
xcj
I(X̃j; Y ),
2For discussion on observed variables see Chapters 5 and 7.
23
subject to |X̃j| = k, which is clearly equivalent to the Information Bottleneck
objective (4.5).
• Information-theoretic co-clustering [35] is a task of simultaneously cluster-
ing two modalities X and Y , while minimizing the information loss I(X; Y ) −
I(X̃j, Ỹj) under the constraint |xcj| = k1 and |ycj | = k2. Note that I(X; Y )
is a constant for a given dataset. This scheme is a special case of a Comraf
as well: given graph G in Figure 4.1(b), in analogy to the Comraf model of
Information Bottleneck, we define the only log-potential I(X̃j; Ỹj). Then the
information-theoretic co-clustering can be represented as an MPE inference in
this Comraf:
(xc∗, yc∗) = arg max
xcj ,y
c
j
P (xcj, y
c
j) = arg max
xcj ,y
c
j
I(X̃j; Ỹj).
4.2 Clustering as combinatorial optimization
Given a variable X with n values clustered into k clusters, the combinatorial
r.v. Xc has kn values, all of which can be represented as points in an n-dimensional
lattice L: a point xc = (i1, i2, . . . , in) corresponds to the fact that value x1 of X
belongs to the i1-th cluster, value x2 belongs to the i2-th cluster, . . ., value xn belongs
to the in-th cluster.
3 In the lattice L there is a (possibly non-unique) point xc∗ =
(i∗1, i
∗
2, . . . , i
∗
n) which is most likely. Since the lattice consists of an exponential number
of points, the task of finding the most likely point can be computationally hard.
In lattice L, a transaction (see Definition 3.3.1) is interpreted as an operation of
transferring a value xj from cluster x̃i to cluster x̃i′ , i.e. (. . . , ij, . . .) → (. . . , i′j, . . .),
where ij 6= i′j.
3Recall that we consider only hard clustering: P (x̃ij |xj) = 1, that is, a value xj is assigned only
to the ij-th cluster.
24
Note that we can view both splits and mergers of clusters as transactions. A split
of a cluster ij′ is a transaction (. . . , ij′ , . . .) → (. . . , i′j′ , . . .), where ∃j 6= j′ : ij′ = ij
and ∀j 6= j′ : i′j′ 6= ij. That is, cluster ij′ contained at least two elements (xj
and xj′), one of which (xj′) has been transferred into a newly created cluster i
′
j′ .
A merger of clusters ij′ and i
′
j′ is a transaction (. . . , ij′ , . . .) → (. . . , i′j′ , . . .), where
∃j 6= j′ : i′j′ = ij and ∀j 6= j′ : ij′ 6= ij, i.e. cluster ij′ contained only one element
that has been added to the existing cluster i′j′ so that the cluster ij′ does not exist
anymore. These operations will help us to represent both agglomerative (bottom-up)
and divisive (top-down) clustering schema as inference in Comrafs.
By applying splits, mergers and other transactions, we construct a path in the
lattice L. Our goal is to make this path as advantageous as possible, such that a
clustering at the end of this path will be the most probable clustering that could be
found. Thus, we view the process of clustering a set X as an MPE approximation of
a combinatorial r.v. Xc, where the MPE is approximated using a local search in the
lattice L. To perform the local search, we apply the simplest, greedy combinatorial
optimization method—hill climbing : at each ICM iteration, we attempt to construct
the most advantageous path in L, given the available computational resources.
Let us discuss particular algorithms in more detail in the next section.
4.3 Multi-way Distributional Clustering (MDC)
In this section we describe our scheme for clustering m modalities that aims at
maximizing our objective function (4.3). This scheme is called Multi-way Distribu-
tional Clustering (MDC) [10]. Let G be a Comraf graph over combinatorial random
variables Xci , i = 1, . . . ,m. For each edge eii′ in graph G we are given a contingency
table Tii′ that provides the corresponding co-occurrence counts of the modalities Xi
and Xi′ . The input to the algorithm is the graph G, the tables Tii′ , as well as m
desired cardinalities k1, . . . , km of the final partitionings, and a clustering schedule
25
(the sequence of variables for optimization in the ICM loop, see below for details).
The output of the algorithm is m partitionings X̃i, i = 1, . . . , m, each of cardinality
ki = |X̃i|.
The desired cardinalities k1, . . . , km are essential parameters of MDC, as our
method cannot infer them. We believe that the problem of inducing the optimal
number of clusters is generally ill-defined: imagine a dataset that is situated on a
plane in a triangle, each corner of which consists of three triangles of data instances
(27 instances overall). It is hard to decide what the best number of clusters would be
in this case: three or nine. Admittedly, not all machine learning researchers would
agree with this argument. Some existing clustering methods attempt to solve the
problem of optimal number of clusters (such as, e.g., the Chinese Restaurant Pro-
cess [30]). Still and Bialek [101] come up with the optimal number of clusters in an
Information Bottleneck setting. While their method is well justified theoretically, it
could not induce a meaningful number of clusters in our experiments.
To compute the objective function (4.3) we will need the following definitions
and identities, where for the current discussion we re-notate X = Xi, Y = Xj and
T = Tii′ :
NXY =
∑
x∈X; y∈Y
T (x, y);
p(x̃, ỹ) =
1
NXY
∑
x∈x̃; y∈ỹ
T (x, y);
I(X̃; Ỹ ) =
∑
x̃∈X̃;ỹ∈Ỹ
p(x̃, ỹ) log
p(x̃, ỹ)
p(x̃)p(ỹ)
, (4.6)
where p(x̃) =
∑
ỹ∈Ỹ p(x̃, ỹ), and p(ỹ) =
∑
x̃∈X̃ p(x̃, ỹ).
Pseudo-code for the multi-way distributional clustering (MDC) algorithm is given
in Algorithm 2. For simplicity, the pseudo-code abstracts away several details that
26
Input:
G – Comraf graph of nodes {Xc1 , . . . , Xcm} and edges E
Tii′ – contingency tables for each eii′ ∈ E
Sup, Sdown - bottom-up/top-down partition of {1, . . . ,m}
Sl = i1, i2, . . . , il – clustering schedule, where each ij ∈ {1, . . . , m}
Output:
Most likely clusterings xc1,l, . . . , x
c
m,l
Initialization:
For each i = 1, . . . , m do
If i ∈ Sdown then
Place all values of Xi in one cluster
Else If i ∈ Sup then
Place each value of Xi in a singleton cluster
Main loop:
For each ij from Sl do
Split/merge phase:
If ij ∈ Sdown then
Split each cluster in xci,j uniformly at random to two halves
Else If ij ∈ Sup then
Merge each cluster in xci,j with its closest peer
Optimization phase:
For each values x of Xij do
Pull x out of its current cluster
Place x into a cluster, s.t. objective function (4.4) is maximized
Algorithm 2: Multi-Way Distributional Clustering (MDC).
are not essential for understanding the general idea but can be important for actual
applications. We now discuss the algorithm and then provide the necessary details.4
The main loop of the algorithm is controlled by two parameters:
• Partition (Sup, Sdown) of the set of variable indices. If i ∈ Sup, then the variable
Xi is clustered using a bottom-up procedure. Otherwise (i.e. i ∈ Sdown), Xi is
clustered via the top-down procedure.
• Clustering schedule Sl = i1, . . . , il, which is a sequence of variable indices. The
schedule Sl determines the order of processing the variables. While this mecha-
nism allows for great flexibility, we always apply it in a straightforward manner
where the sequence Sl specifies a (weighted) round-robin schedule. For exam-
ple, in the case of bi-modal clustering (with two variables X1 and X2), we
4An efficient C++ implementation of MDC that was used in our experimental study can be
downloaded from http://sourceforge.net/projects/comraf.
27
3210
Figure 4.2. A schematic view of bi-modal MDC with a simple, non-weighted round-
robin schedule. At each iteration black clusters are split and then white clusters are
merged.
take (ignoring, for the moment, the desired cluster cardinalities) Sdown = {1},
Sup = {2} and Sl = 1, 2, 1, 2, . . . , 1, 2. A schematic view of MDC (for this
bi-modal instance) is given in Figure 4.2.
We propose two versions of the optimization phase of our algorithm: sequential
and shuffled :
• In the sequential version, we iterate over all values xi of Xi, in a random order
(determined via a permutation selected uniformly at random). We assign xi
into its “best” cluster, i.e. such cluster that the objective from Equation (4.4)
is maximized. Note that this optimization routine is similar to and inspired by
the sequential Information Bottleneck (sIB) clustering algorithm [96]. We then
iterate over all the values of Xi once again, in order to further optimize the
objective, i.e. two optimization passes are performed overall.
• In the shuffled version, we repeat the following step a predefined number of
times:5 we uniformly at random select a data point xi and a cluster x̃j, and
assign xi into x̃j if this transaction increases the value of our objective. The
5We set it equal (for fair comparison) to the number of iterations in the sequential version.
28
shuffled approach opens the door to improving scalability of MDC (the number
of iterations is constant and can be chosen arbitrarily small, at the cost of
decreasing performance) and to parallelization.
Note that both sequential and shuffled procedures can never decrease the objective
function. However, cluster mergers usually decrease it, so the optimization is non-
convex in the general case.
The choice of index partition (Sup, Sdown) is based on the following two cru-
cial observations. First, for practical applications it is computationally infeasible
to apply bottom-up procedures for all the variables. Second, applying only top-
down procedures is likely to be useless, in terms of the clustering quality. This
is easy to see when considering bi-modal applications, with respect to two vari-
ables X and Y . The objective function reduces to I(X̃; Ỹ ) and we start with
xc and yc each being a single cluster containing all points. Clearly, in this case
I(X̃; Ỹ ) = 0. We now split X̃ to get X̃ = {x̃1, x̃2}. For any (x̃1, x̃2)-partition
we have H(Ỹ |X̃) = −∑i p(x̃i, Ỹ ) log p(Ỹ |x̃i) = 0, since p(Ỹ |x̃i) = 1. Therefore,
I(X̃; Ỹ ) = H(Ỹ )−H(Ỹ |X̃) = H(Ỹ ) = 0, and the corrective step of the algorithm is
useless here. The subsequent split of Ỹ strictly optimizes the objective function, but
the resulting clustering is optimized to correlate with the initial random split of the
X variable. This way, all the subsequent partitions are optimized with respect to a
meaningless, random partition. A similar argument applies to the general MDC and
implies that at least one of the clustering procedures must not be computed top-down.
A natural choice for clustering this variable would be a bottom-up method because
its initialization phase (singleton clusters) does not require any prior knowledge to be
incorporated (for a discussion, see Chapter 2).
As mentioned above, in all our applications we construct (weighted) round robin
schedules Sl = i1, . . . , il. In order to accommodate the required cardinalities k1, . . . , km
of clusterings xc1, . . . , x
c
m, the MDC algorithm performs the following number of iter-
29
ations: li = dlog kie for i ∈ Sdown, and li = dlog(|Xi|/ki)e for i ∈ Sup. Thus, each
index i appears li times in the sequence Sl, in a (weighted) round-robin fashion.
4.3.1 Computational complexity of MDC
We now analyze the time complexity of the sequential version of MDC6 for a non-
weighted round-robin schedule. The complexity issue should be taken into account
when forming the partition (Sup, Sdown), because the time complexity of the algorithm
depends on u = |Sup|, i.e. on the number of modalities clustered bottom-up. Let
|X| = max(|X1|, . . . , |Xm|), the size of the largest support of variables X1, . . . , Xm.
At each iteration, (sequential) MDC performs three nested loops:
1. Pass over each value of Xi: O(|X|) times;
2. For each value of Xi, pass over each cluster in X̃i: |X̃i| = O(|X|) times;
3. For each cluster in X̃i, pass over clusters in all the other clusterings (excluding
X̃i itself): O(m|X|) = O(|X|) times (the number of clustered variables, m, is a
constant in our case).
Since the number of iterations is n = O(log |X|), in the worst case (when u > 1)
the time complexity is O(n|X|3) = O(|X|3 log |X|). This complexity can be burden-
some in some real-world applications. Note, however, that for each variable Xi, which
is clustered top-down, at each iteration j the number of clusters is |Xij | = O(ki) =
O(1). Thus, when u = 1, either loop 2 or loop 3 is performed O(1) times, and the
overall running time is O(|X|2 log |X|), which is affordable for many applications.
In the bi-modal case, at each iteration the size of one clustering is doubled, and
at the next iteration the size of the other clustering is halved. Therefore, at each
6The complexity of our implementation of the shuffled version is the same as the one of the
sequential version, because we choose to fix the number of iterations in the shuffled version equal to
the number of iterations in the sequential version.
30
iteration |X̃1| · |X̃2| ≤ 2|X|, i.e. the constant under the ‘big-O’ is only 2. The (non-
hierarchical) co-clustering algorithm of [35] has the same complexity O(|X|2 log |X|),
but with a larger constant under the ‘big-O’.
Based on this analysis, in all our experiments we fix u = 1, i.e., only one variable is
clustered bottom-up. Finally, note that if variable Xi has a small support, |Xi| ¿ |X|,
then the decision whether i ∈ Sup or i ∈ Sdown can be made independently of time
complexity considerations.
4.4 Clique-wise MDC
As we discussed in Section 3.3, global optimization of the objective function (3.1)
is not always beneficial. As an alternative, we proposed a clique-wise optimization
(CWO) procedure. In this section, we propose a clique-wise version of the MDC
algorithm, which is inspired by Bouvrie’s algorithm [23]. Its pseudocode is given
in Algorithm 3. To keep the procedure as simple as possible, we consider only the
smallest cliques, i.e. edges in the Comraf graph G. In contrast to the original MDC
that iterates over nodes in G, the CWO version iterates over edges, in a round-robin
fashion. For every edge eii′ , the algorithm performs the MPE optimization of a
portion of G that consists of only one edge eii′ and its vertices X
c
i and X
c
i′ . This
optimization is performed independently of the rest of the model. The best values of
Xci and X
c
i′ found during this optimization step are then used as initial values for the
next optimization steps.
In this setup, an application of hierarchical clustering appears unnatural: after
the j-th optimization iteration over one clique, the constructed clusterings xci,j and
xci′,j are supposed to have the desired number of clusters (ki and ki′ respectively).
Using these clusterings as initial values of further optimization steps leaves no room
for exploring the clustering hierarchy. For this reason, and also for simplicity, at each
optimization step we apply a flat clustering method, similar to the sequential Infor-
31
Input:
G – Comraf graph of nodes {Xc1 , . . . , Xcm} and edges E
Tii′ – contingency tables for each eii′ ∈ E
k1, . . . , km – the desired number of clusters for each node
S′l = (i1i
′
1), . . . , (ili
′
l) – clustering schedule, where each pair ii
′ corresponds to edge eii′
Output:
Most likely clusterings xc1,l, . . . , x
c
m,l
Initialization:
For each i = 1, . . . , m do
Distribute all values of Xi uniformly at random over ki clusters
Main loop:
For each (iji′j) from S
′
l do
For each value x of Xij do
Pull x out of its current cluster
Place x into a cluster, s.t. I(X̃ij ; X̃i′j ) is maximized
For each value x of Xi′j do
Pull x out of its current cluster
Place x into a cluster, s.t. I(X̃ij ; X̃i′j ) is maximized
Algorithm 3: Clique-wise MDC.
mation Bottleneck [96]. Quite surprisingly, the results of this flat clustering procedure
are comparable to the ones of the original (more complex) MDC (see Section 4.6.5).
The computational complexity of each sequential optimization step is O(k2|X|),
where |X| is the size of the largest support among the variables in X, and k is the
largest final number of clusters. The number of iterations is O(m2), as the number of
edges in graph G is in the worst case quadratic in the number of combinatorial random
variables. The resulting complexity is then O(m2k2|X|), which is asymptotically
linear in the size of the data. However, the constants can be very large. Still, in
practical cases, Comraf models are very compact such that the m2 constant is not
restrictive, and the clique-wise MDC is substantially faster than its original ICM-
based version.
4.5 Related work
The study of distributional clustering based on co-occurrence data using informa-
tion theoretic objective functions was initiated by Pereira et al. [88]. Much of the
subsequent related work is inspired by that paper and the Information Bottleneck
32
(IB) ideas of Tishby et al. [106]. In this context, the first work considering two-way
clustering of both words and documents is by Slonim and Tishby [99], which is subse-
quently improved by El-Yaniv and Souroujon [39], and then more thoroughly studied
by Dhillon et al. [35].
The more general Multivariate Information Bottleneck (mIB) framework [97] also
considers simultaneous clusterings based on interaction between variables, as we pro-
pose here. For two variables (bi-modal clustering) the algorithm proposed here can
be viewed as a particular implementation of the “hard case” of mIB. However, for
more than two variables, the framework we propose here is not a special case of the
mIB framework since the interactions between variables in mIB are described via
a directed Bayesian network, in which cycles cannot be factorized to pairwise de-
pendencies (see Section 4.1). Our scheme employs undirected graphs that represent
pairwise interactions, and therefore do not preclude loops. It is important to note
that our clustering algorithm (MDC) is inspired by the sequential IB method [96].
Finally, we note that the idea of multi-modal clustering also appears in Bouvrie [23],
where multiple clusterings are constructed by an iterative application of a bi-modal
clustering algorithm, and the resulting system is applied to computer vision tasks.
4.6 Experimentation: email clustering
In this section, we present our experimental results on the document clustering
task. Two particular tasks we consider are similar to each other: (a) automatic
categorization of email into folders; (b) automatic routing of newsgroup messages
into appropriate newsgroups.
Email foldering is a rich and multi-faceted problem, with many difficulties that
make it different from traditional topic-based categorization. Email users create new
folders, and let other folders fall out of use. Email folders do not necessarily corre-
spond to simple semantic topics—sometimes they correspond to unfinished todo tasks,
33
project groups, certain recipients, or loose agglomerations of topics. It is important
to note that email content and foldering habits differ drastically from one email user
to another—so while automated methods may perform well for one user, they may
fail horribly for another. In this thesis, however, we test the Comraf’s performance
on email clustering under a simplifying assumption that folders roughly correspond
to semantic topics. In our future work, we will adapt our clustering system to specific
needs of particular users.
Despite the fact that clustering is rarely used as a stand-alone application—it
is usually a part of another, more global task—we choose to focus on evaluating
the quality of the clustering results per se, i.e. not with respect to the global task.
This way, our evaluation is not skewed by various aspects of a particular real-world
problem.
4.6.1 Evaluation measure
Following [96, 35] and many other works, we use micro-averaged accuracy for
evaluation of our clustering methods. Let xc be a clustering of the data X . Let T be
the set of ground truth categories. We fix the number of clusters to match the number
of categories |xc| = |T | = k. For each cluster x̃j, let γT (x̃j) be the maximal number
of x̃j’s elements that belong to one category. Then, accuracy Acc(x̃j, T ) of a cluster
x̃j with respect to C is defined as Acc(x̃j, T ) = γT (x̃j)/|x̃j|. The micro-averaged
accuracy of the clustering xc is:
Accm(x
c, T ) =
∑k
j=1 γT (x̃j)∑k
j=1 |x̃j|
=
∑k
j=1 γT (x̃j)
|X | . (4.7)
In Section 4.6.5 also present macro-averaged accuracy results, where the macro-
averaged accuracy is defined as:
AccM(x
c, T ) =
∑k
j=1 Acc(x̃j, T )
k
. (4.8)
34
D
c c
W D
c
C
c
W
c
D
c
C
c
c
W
c
S
Figure 4.3. Comraf graphs for 2-modal, 3-modal and 4-modal Comrafs used in our
experiments. We consider interactions between combinatorial random variables that
correspond to documents Dc, words W c, email correspondents Cc and email Subject
lines Sc. Note that we use only tree-structured models, as they are simpler than
loopy models and on the email foldering task they show comparable results to those
obtained with loopy models (see Section 4.6.6 for a discussion). In Section 4.8 we
present a result when a loopy model is significantly superior to a tree-structured one.
4.6.2 Datasets
We evaluate the Comraf models on six text datasets. In addition to the standard
benchmark 20 Newsgroups dataset (20NG) we use five real-world email directories.
Three of them belong to participants in the CALO project7 and the other two belong
to former Enron employees.8
On the 20NG dataset we apply a bi-modal Comraf where the modalities are mes-
sages (documents) and words. CALO and Enron datasets are particularly useful for
evaluating 3-modal and 4-modal Comrafs. Here we take as variables (1) messages;
(2) words; (3) people names associated with messages—we consider the entire list of
correspondents (both senders and recipients); and (4) email Subject lines, represented
by their bags of words. Comraf graphs for the three setups are given in Figure 4.3.
Table 4.1 provides basic statistics of the six datasets. For details on collecting the
CALO and Enron data, see [14]. Below we briefly describe the data and preprocessing
steps undertaken.
7http://www.ai.sri.com/project/CALO
8The Web page of the original Enron Email Dataset is http://www.cs.cmu.edu/~enron. Our
preprocessed Enron email directories can be obtained from http://www.cs.umass.edu/~ronb/
enron_dataset.html.
35
Dataset Size Min/max Number of Number of Number of
class size distinct words correspondents classes
CALO:acheyer 664 3/72 2863 67 38
CALO:mgervasio 777 6/116 3207 61 15
CALO:mgondek 297 3/94 1287 50 14
Enron:kitchen-l 4015 5/715 15579 2278 47
Enron:sanders-r 1188 4/420 5966 933 30
20NG 19997 997/1000 39764 N/A 20
Table 4.1. Statistics of email datasets. Number of distinct words and number
of correspondents are after preprocessing.
4.6.2.1 20 Newsgroups
The 20 Newsgroups (20NG) corpus contains 19997 messages taken from the Usenet
newsgroups collection.9 Each message is assigned into one or more semantic categories
and the total number of categories is 20, all of which are of about the same size. Most
of the documents have only one semantic label, however it turns out that about 4.5%
of documents have two or more labels. Those documents are simply duplicated in the
dataset (one copy per category). In this thesis, for easier replicability of our results,
we decided to refrain from taking steps of any kind to resolve the duplication issue.
We preprocess the 20NG dataset as described in [11]. First, we remove message
headers and markup (such that only the subject lines and actual text remained).
Next, we filter out lines that seem to be part of binary files sent as attachments
or pseudo-graphical text delimiters. A line is considered to be a “binary” (or a
delimiter) if it is longer than 50 symbols and contains no white spaces. Overall, we
remove 23057 such lines (most of them appeared in a handful of articles). Finally,
we represent documents as their Bags-Of-Words, lower the case of letters and remove
stopwords as well as low-frequency words.
9http://kdd.ics.uci.edu/databases/20newsgroups/20newsgroups.html
36
4.6.2.2 Enron Email Dataset
The archived email from many of the senior management of Enron Corporation
was subpoenaed, and is now in the public record. The data consists of over 500,000
email messages from the email accounts of 150 people. The dataset is provided by
SRI after major clean-up and removal of attachments. The dataset version we use
was released on February 3, 2004.
Although the size of the dataset is large, many users’ folders are sparsely popu-
lated. We use the email directories of two former Enron employees: kitchen-l and
sanders-r. Those directories are among the largest ones in the dataset.
We remove standard non-topical folders “all documents”, “calendar”, “contacts”,
“deleted items”, “discussion threads”, “inbox”, “notes inbox”, “sent”, “sent items”
and “ sent mail”. We then flatten all the folder hierarchies and remove all the
folders that contain fewer than three messages. We also remove the X-folder field in
the message headers that actually contains the class label. As for 20NG, we finally
represent documents as their Bags-Of-Words, lower the case of letters and remove
stopwords and low-frequency words.
4.6.2.3 CALO Email Dataset
A smaller but also significant corpus of real-world, foldered email has been created
as part of the CALO DARPA/SRI research project. This corpus consists of snapshots
of the email folders of 196 users, containing approximately 22,000 messages. From
the February 2, 2004 snapshot of CALO directories, we select three users with large
number of messages: acheyer, mgervasio, and mgondek. As in the preprocessing
step of the Enron datasets, we first remove standard non-topical folders (“Inbox”,
“Drafts”, “Sent” and “Trash”). Then the folder hierarchy is flattened, and folders
that contain fewer than three messages are removed. Finally, as for all the other
37
datasets, we represent documents as BOW, lowercase the text and filter out stopwords
and low-frequency words.
4.6.3 Baseline algorithms
We compare the performance of Comraf clustering algorithms with the following
five well known benchmark clustering algorithms:
1. K-means. We use the SimpleKMeans implementation of WEKA [112];
2. Agglomerative Information Bottleneck (aIB). A simple, deterministic
uni-modal Information Bottleneck clustering algorithm [98];
3. Sequential Information Bottleneck (sIB). A randomized uni-modal Infor-
mation Bottleneck clustering algorithm [96], which exhibited striking perfor-
mance in the text domain;
4. Information-theoretic co-clustering (ITCC). A bi-modal clustering algo-
rithm [35] (see Section 4.1);
5. Latent Dirichlet Allocation (LDA). A popular generative model for repre-
senting document collections, proposed by Blei et al. [22]. Each document is
represented as a distribution of topics, and parameters of those distributions are
learned from the data. Documents are then clustered based on their posterior
distributions (given the topics). We use Xuerui Wang’s LDA implementation
[78] that applies Gibbs sampling with 10000 sampling iterations.10
Note that the latter three algorithms are widely considered to be state-of-the-art in
unsupervised text categorization.
10We also tried David Blei’s LDA-C [22] that implements variational approximation and obtained
significantly inferior accuracy.
38
To gain some perspective on the performance of the unsupervised methods we
tested, we also report on the results of a trivial “random clustering”, which simply
places each document in a random cluster. At the other extreme, we report on
the categorization accuracy of a supervised application of a support vector machine
(SVM), applied with linear kernel and with cross-validated parameter tuning (using
the same setup as described in Bekkerman et al. [11]). We stress that the supervised
categorization accuracy cannot be directly compared with the clustering accuracy,
however, it provides some perspective on on datasets’ “complexities”.
4.6.4 Implementation details
The following technical details are important for replicating our experimental
results:
1. Unless stated otherwise, we use the bottom-up scheme for documents and the
top-down scheme for all the other clusterings.
2. As discussed in Section 4.3, we merge each document cluster with its closest
peer. Following Slonim & Tishby [98], we choose the Jensen-Shannon divergence
between clusters as the underlying “metric”.
3. At the MDC’s last iteration (at which the required number of document clus-
ters is obtained), we apply the optimization routine after merging each pair of
clusters.
4. We perform 10 random restarts at each iteration of MDC. For a fair comparison,
we perform the same number of random restarts in our implementations of both
sIB and ITCC algorithms.
5. We use the same clustering schedule Sl for every dataset. The schedule starts
with splits of top-down clusters—as discussed in Section 4.3, it cannot start with
a merger of document clusters otherwise the objective function (4.3) would be
39
0. Also, we notice that it is not beneficial to start merging document clusters
before a significant number of word clusters is obtained, otherwise the objec-
tive function is still too close to 0. Thus, before doing the first iteration over
documents, we perform four iterations over words, and continue with a plain
(non-weighted) round-robin then.
4.6.5 Comparative results
Micro-averaged accuracy (averaged over ten independent runs,11 whenever appli-
cable) for the six datasets is reported in Table 4.2. It is evident that the results of our
bi-modal Comraf clustering (with the underlying MDC algorithm) are significantly
superior to those obtained by other methods. The only statistically insignificant im-
provement is recorded for MDC over sequential IB on the CALO:acheyer dataset;
all the other gaps are statistically significant. Of particular importance is the striking
69.5% micro-averaged accuracy achieved by the bi-modal MDC on 20NG.12 This im-
pressive result is 12% higher than the best previously reported result on this dataset.
Specifically, a micro-averaged accuracy of 57.5% on 20NG is reported for sIB in [96].
This result is obtained with only 2,000 “most discriminating” words. Also, in that
work, duplicated and small documents are removed, leaving only 17,446 documents.
In our implementation of sIB, our use of almost 40,000 words leads to 61% accuracy
on the entire dataset of 19,997 documents. More than 5% absolute improvement is
also obtained on Enron:kitchen-l and CALO:mgondek datasets.
11Randomized algorithms, such as MDC, may obtain different results each time they are applied
to the same dataset. We perform ten independent runs of each randomized algorithm on the same
data, and compute the mean of the obtained results, as well as the standard error of the mean.
12In [10] we reported on a slightly better result of MDC on 20NG. This better performance was
obtained using a cluster balancing heuristic that reduced the probability of small clusters to be
further split and of large clusters to be further merged. Later we discovered that this heuristic is
not uniformly effective across datasets and we therefore abandoned it.
40
Method CALO: CALO: CALO: Enron: Enron: 20NG
acheyer mgervasio mgondek kitchen-l sanders-r
Random 17.8± 0.5 18.3± 0.3 32.4± 0.1 17.9± 0.1 35.4± 0.1 6.3± 0.1
K-means 24.7 24.1 37.0 29.6 45.5 OOM
Agglom. IB 36.4 30.9 43.3 31.0 48.8 26.5
Sequent. IB 47.0± 0.5 35.1± 0.6 68.2± 1.2 34.6± 0.5 63.1± 0.6 61.0± 0.7
ITCC 46.1± 0.3 34.2± 0.5 63.4± 1.1 31.8± 0.2 60.2± 0.4 57.7± 0.2
LDA 44.3± 0.4 38.5± 0.4 68.0± 0.8 36.7± 0.3 63.8± 0.4 56.7± 0.6
2-modal Comraf 47.8± 0.4 42.4± 0.4 75.9± 0.6 42.4± 0.6 67.4± 0.3 69.5± 0.7
(sequential)
2-modal Comraf 47.1± 0.4 44.0± 1.0 75.5± 0.5 41.6± 0.8 67.6± 0.3 67.2± 0.8
(shuffled)
SVM 65.8± 2.9 77.6± 1.0 92.6± 0.8 73.1± 1.2 87.6± 1.0 91.3± 0.3
(supervised)
Table 4.2. Micro-averaged accuracy (± standard error of the mean, when appli-
cable) on the six datasets. The SVM supervised classification accuracies are obtained
with 4-fold cross validation. “OOM” means “out of memory”: WEKA was unable to
cluster 20NG, on a 4GB RAM machine. Bold numbers are the best results over all.
Surprisingly, on CALO and Enron datasets, the sequential version of MDC and
its shuffled version obtain almost identical results (the difference is statistically in-
significant). Note that in both versions we perform the same number of optimization
steps. However, on 20NG, sequential MDC is significantly superior. This can be
explained by the fact that sequential MDC is guaranteed to iterate over all the data
instances, while shuffled MDC is not. On smaller datasets (CALO and Enron), the
number of optimization steps is large enough to make the shuffled version iterate over
(almost) every data instance. On a larger dataset (20NG), however, shuffled MDC
is less likely to iterate over every data instance, and therefore is sub-optimal.
Table 4.3 shows macro-averaged accuracy results on CALO and Enron datasets.
Compared with micro-averaged accuracy, macro-averaged accuracy favors smaller
clusters over larger clusters. We can see in the table that Comraf’s sequential MDC
method is still significantly better than the baselines (here we show the results of
only three most prominent baselines: sIB, ITCC and LDA). The only exception
is an insignificant improvement MDC achieves over sIB on the Enron:kitchen-l
41
Method CALO: CALO: CALO: Enron: Enron:
acheyer mgervasio mgondek kitchen-l sanders-r
Sequent. IB 57.4± 0.7 53.1± 0.7 65.9± 0.6 46.7± 0.4 69.2± 0.7
ITCC 57.3± 0.4 50.0± 1.2 67.0± 0.8 43.3± 0.4 65.6± 0.4
LDA 53.0± 0.6 52.2± 0.8 63.6± 0.7 39.1± 0.2 66.7± 0.2
2-modal Comraf 59.9± 0.5 58.5± 0.7 76.9± 0.8 47.0± 0.7 74.6± 1.1
(sequential)
Table 4.3. Macro-averaged accuracy (± standard error of the mean) on CALO
and Enron datasets. Each number is an average over ten independent runs. Bold
numbers are the best results over all.
dataset. Note that the macro-averaged accuracies shown in Table 4.3 are in most
cases higher than micro-averaged accuracies (Table 4.2). This implies that small
clusters constructed by the discussed clustering methods are generally cleaner than
large clusters.
As shown in Table 4.4, our tri-modal Comraf (documents/words/correspondents)
consistently improves the bi-modal Comraf performance on the CALO email datasets.
On mgervasio, the addition of correspondents’ modality leads to an impressive ab-
solute improvement of 10%. On Enron email, however, tri-modal Comraf shows
mixed results: a significant improvement on sanders-r and a drop on kitchen-
l. A closer inspection reveals that the email correspondent input stream in Enron
datasets is extremely noisy. That is, the information on the same person can be
represented in dozens of different formats, delimiters between separate records are
sometimes non-existent, and many email messages have very long lists of recipients
(which would probably imply that email data not always strongly correlate with the
recipient data).
When comparing the ICM and CWO optimization methods for Comrafs (see Sec-
tion 3.3), we can see that ICM usually outperforms CWO. However, CWO is a some-
what simpler and significantly faster method (for a discussion, see Section 4.4).
Our experimentation with 4-modal Comraf (documents/words/correspondents/
subject lines) on the CALO datasets shows further (insignificant) improvement over
42
Method CALO: CALO: CALO: Enron: Enron:
acheyer mgervasio mgondek kitchen-l sanders-r
2-modal Comraf—ICM 47.8± 0.4 42.4± 0.4 75.9± 0.6 42.4± 0.6 67.4± 0.3
3-modal Comraf—ICM 49.1± 0.4 52.4± 0.7 80.1± 0.7 40.2± 0.3 69.0± 0.4
3-modal Comraf—CWO 47.2± 0.3 48.4± 0.5 76.1± 1.2 39.5± 0.5 63.9± 0.2
4-modal Comraf—ICM 50.2± 0.6 54.1± 0.5 80.9± 0.5 34.2± 0.2 63.1± 0.4
4-modal Comraf—CWO 47.6± 0.2 48.6± 0.6 78.7± 1.1 38.7± 0.4 63.4± 0.4
Table 4.4. Micro-averaged accuracy (± standard error of the mean) on CALO
and Enron datasets. Each number is an average over ten independent runs. Comrafs
models are 2-modal, 3-modal and 4-modal, with the sequential optimization applied
at each node. Bold numbers are the best results over all.
the tri-modal Comraf performance. On Enron, in contrast, a significant drop can be
observed. An important observation is that the subject line modality is substantially
sparser than other modalities in the Enron datasets. It is evident that the addition of
a sparse modality appears to be non-beneficial for multi-modal clustering. A formal
method for learning a Comraf model structure is emerging, which we leave for our
future work (for a discussion, see Section 4.6.6 below).
4.6.5.1 Experimentation with clustering schedule
On CALO data, we test another algorithmic setup of the bi-modal MDC, in which
both words and documents are clustered bottom-up. The results are very similar to
our original bi-modal MDC accuracies. However, this setting is not applicable to
larger datasets: taking constants into account, on the 20NG dataset the bottom-up
version of MDC would be 300 times slower than the original (top-down / bottom-up)
MDC.
In addition, we test a reverse clustering schedule, where we apply bottom-up
clustering to words and top-down clustering to documents. On the 20NG dataset,
we perform five splitting iterations over documents (obtaining 32 clusters) and then
apply the last exhaustive clustering iteration as explained in Section 4.6.4, reduc-
ing the number of clusters to 20. The micro-averaged clustering accuracy obtained
by the reverse schedule is 69.3 ± 0.4%, which is statistically indistinguishable from
43
0 0.5 1 2 3 4
0.44
0.46
0.48
0.5
search length
ac
cu
ra
cy
CALO:acheyer
 
 
2−modal Comraf
LDA
0 0.5 1 2 3 4
0.37
0.39
0.41
0.43
0.45
search length
ac
cu
ra
cy
CALO:mgervasio
 
 
2−modal Comraf
LDA
0 0.5 1 2 3 4
0.66
0.69
0.72
0.75
0.78
search length
ac
cu
ra
cy
CALO:mgondek
 
 
2−modal Comraf
LDA
0 0.5 1 2 3 4
0.62
0.64
0.66
0.68
0.7
search length
ac
cu
ra
cy
Enron:sanders−r
 
 
2−modal Comraf
LDA
Figure 4.4. Clustering accuracies as a function of the length of local search in
sequential MDC: ‘0.5’ on the x-axis means that the MDC’s optimization routine was
executed over one half of the data points (chosen uniformly at random), while ‘3’
means that the optimization routine was executed over every data point 3 times. All
our results are averaged over 10 independent runs.
the the original MDC’s performance. Note that the reverse scheme is significantly
faster than the original MDC (8 clustering iterations vs. 21 iterations on 20NG). On
email datasets, similar results are obtained in three of the five cases, whereas in two
others the reverse schedule shows significantly poorer performance (3% decrease on
CALO:mgervasio and 7% decrease on Enron:kitchen-l).
4.6.5.2 Experimentation with the length of local search
Figure 4.4 presents the micro-averaged clustering accuracy of sequential MDC
(in a bi-modal Comraf) as a function of the length of local search performed in the
lattices of all possible word and document clusterings. Recall that in Algorithm 2
44
we perform a local search (i.e. an optimization phase), in which every data point is
sequentially pulled out of its cluster and assigned into a cluster such that the objective
function is maximized. In their sequential IB algorithm, Slonim et al. [96] propose
to execute such an optimization routine a number of times, up to the convergence
of the objective function to its local maximum. Their approach has a drawback
of a potentially unlimited execution time: while it is guaranteed that the objective
function occasionally converges, it is uncertain how long this can take.
In our MDC’s implementation, we perform the optimization routine twice (see
Section 4.3), in order to approach the local maximum, while not setting our stopping
criterion at achieving the full convergence. In this section, we ask the question whether
or not the length of the local search is a crucial parameter of our system. Our
experiment is conducted as follows: in a bi-modal Comraf, we set the length of
the optimization routine to be a function of the number of data points (words or
documents). We start with the case where we explore only one quarter of the data
(chosen uniformly at random), then we try one half, and then we perform from 1 to 4
full passes (over all the data points). We perform this experiment on 4 email datasets
(excluding the large kitchen-l and 20NG collections).
As can be seen on Figure 4.4, the correlation of local search length and the cluster-
ing accuracy is quite weak, as soon as at least one pass over all the data is performed.
In some cases, shorter searches are quite effective (such that the one on mgondek),
while in the others (sanders-r) a significant drop is recorded. Searches longer than
two data sizes are generally not beneficial: while a (rather insignificant) improvement
can usually be seen, the run time increase trades off against this improvement.
Finally, let us emphasize that we approximate a local maximum of our objective
function. Following Slonim [95], we note that obtaining a global maximum is very
unlikely in our non-convex combinatorial optimization environment, where (in the
worst case) all possible configurations should be tested in order to achieve a global
45
0.4
0.45
0.5
0.55
ac
cu
ra
cy
Clustering accuracy on various Pairwise Interaction Graphs
Figure 4.5. Experimenting with various Comraf graphs on mgervasio.
optimum. Since the number of possible configurations is astronomical even in the
smallest real-world setups, approximating a global maximum is practically impossible.
4.6.6 Model analysis
As shown in Section 4.6, multi-modal clustering based on more than two entities
may or may not improve performance relative to the bi-modal clustering. Given a
Comraf graph (and the corresponding pairwise data), an interesting question is which
of the pairwise interactions can contribute useful information to clustering the target
variable.
We investigate this problem with respect to the mgervasio dataset. Specifically,
we test all possible Comraf graphs and measure their effectiveness in accurately clus-
tering the target variable. Figure 4.5 summarizes our findings (for better visibility,
we present only the most interesting cases). As can be seen at the figure, the choice
46
of a Comraf graph can dramatically affect the clustering performance (within a 15%
accuracy range). Also, this experiment illustrates the fact that model learning is
feasible in Comrafs (which usually contain a small number of nodes).
Some variables can be crucial for obtaining good clustering results, while some
others can be unnecessary or even harmful. For example, when substituting the
words variable with email subjects, a decrease in the results can always be seen (nat-
urally, email bodies provide more information than email subjects). In contrast, the
correspondents variable plays a positive role in foldering email of mgervasio. Some-
what surprisingly, the bi-modal documents/correspondents clustering setup leads to
a 6% absolute improvement over the ordinary documents/words setup. A possible
explanation is that most of the folders in this dataset are created according to people
groups in the email owner’s social network.
Some interactions are more important than others. For example, in the docu-
ments/correspondents/titles triangle, a missing documents/correspondents interac-
tion can cause a 10% drop in the accuracy. However, when crucial interactions are
selected, adding other interactions would not significantly affect the performance,
but rather will add a certain computational burden. Therefore, a desirable goal
would be to select only crucial interactions, which are the ones presented in Fig-
ure 4.3. When using the CWO inference method, however, constructing the full
Comraf graph is sometimes beneficial. For example, in a tri-modal setting, the ad-
dition of the correspondents/words interaction leads to a significant improvement
in the (micro-averaged) document clustering accuracy on three of the five datasets:
51.1±0.4% vs. 48.4±0.5% on mgervasio, 42.2±0.4% vs. 39.5±0.5% on kitchen-l,
and 68.8± 0.2% vs. 63.9± 0.2% on sanders-r.
47
4.6.7 Multi-modal clustering for social network analysis
The goal of multi-modal clustering presented in this chapter can be not only to
document clustering, but also word clustering or clustering of people for the purposes
of social network analysis. We apply our tri-modal Comraf to simultaneously clus-
ter email messages, their words and correspondents, and evaluate the quality of the
constructed clusters of email correspondents. To obtain the ground truth data, we
asked Dr. Melinda Gervasio, the creator of the CALO:mgervasio email directory,
to classify her 61 correspondents to semantic groups. She created four categories:
SRI management, SRI CALO collaborators, non-SRI CALO participants and other
SRI people not involved in the CALO project.
We evaluate two clusterings—one constrained to produce four clusters, the other
to produce eight. Both produced results are highly correlated with Melinda Gervasio’s
labelings. In our four-cluster setup, the category of SRI management is united with
the category of non-SRI people, while the category of SRI CALO collaborators (the
largest one) is split to two clusters. The forth category (other SRI people) forms a
single clean cluster, and the borders between the categories are successfully identified,
leading to 62.3± 1.4% accuracy averaged over four independent runs.
In the eight-cluster result, categories of SRI management and non-SRI people are
almost perfectly split to two different clusters, while other SRI employees still form
one cluster, and the category of SRI CALO participants is now distributed over five
clusters, one of which contains only one person who is Melinda Gervasio herself. The
overall precision of the eight-cluster system is as high as 76.6± 2.8%.
4.7 Experimentation: Web appearance disambiguation
In this section, we illustrate the application of Comraf clustering to a real-world
task. In [13] we introduced Web appearance disambiguation (WAD) as the problem
of inferring a model M that provides a binary function f(d, h,K) answering whether
48
or not a Web page d refers to a particular person h, given the background knowledge
K. For simplicity, we consider only the case when h’s name is explicitly mentioned
in the page d. The problem might be easy when h’s name is unique, but becomes
difficult when h has a common name, such as “Tom Mitchell”. Moreover, we do not
know a priori whether a given person h has a unique name or not.
Note that the WAD problem is similar to, but not a special case of the problem
of person name disambiguation. In person name disambiguation, given a collection
of documents all of which mention a person name, the goal is to distinguish between
documents that mention different people who have this name. In WAD, in contrast,
the goal is to find a subset of the document collection in which the person of interest
is mentioned, while filtering out documents that mention unrelated namesakes. To
our opinion, the WAD setup is more realistic than person name disambiguation in the
context of Web search, where one is usually interested in finding information about
a particular person, rather than about all people with the same name.
As perfect background knowledge K is in most cases unavailable, the disambigua-
tion decision must be made using some limited available information. Note that given
no background knowledge at all, the WAD problem becomes ill-defined: in order to
automatically perform the task, the person h must have an electronic representation,
which cannot be constructed without any prior knowledge about the person. If K
includes training data—pages that are related or unrelated to the person—the WAD
problem is reduced to a binary classification task. In this thesis, however, we consider
an unsupervised scenario.
We notice that as soon as we are given not just one, but at least two names of
people who are known to belong to one social network, the WAD problem becomes
well-defined and solvable. An example can be “Tom Mitchell” and “William Cohen”.
Since William Cohen’s name appear in conjunction with Tom Mitchell’s, it is appar-
ent that we refer to William Cohen the CMU Professor, and not to the former US
49
Secretary of Defense. It is a rare case that two people in one social network have two
namesakes in another. However, the probability of having a collision like that is not
zero. We can minimize this probability by considering N > 2 names. To summarize,
our background knowledge K is a list of names of people who are believed to belong
to h’s social network.
In a recent followup paper [113], Yang et al. claim that obtaining a few names
of people who belong to the same social network is very hard. However, it is usu-
ally not the case. In many real-world cases a person name appears in a context of
other people’s names. These can be co-authors of a scientific paper, recipients of the
same email message, attendants of a meeting or a conference etc. It is important to
note that two people can belong to the same social network without even knowing
each other. For instance, given two randomly chosen names of machine learning re-
searchers h1 and h2, who may or may not be acquaintances, the disambiguation task
is nevertheless likely to be solved, as Web pages referring to h1 and h2 are likely to
be close in content, or close in the Web graph (the graph of hyperlinks).
In this section, we address the WAD problem as a clustering task in a Comraf.
For each person h (out of a list of N people from one social network), we retrieve
nh documents that mention h’s name. The resulting collection of N · nh documents
is clustered using the MDC method in a Comraf. For this task, our Comraf model
is very simple (see Figure 4.3 left): we simultaneously cluster documents are their
words.
Out of the k document clusters constructed, we choose one cluster to be the
subset of documents that mention people of interest, and we delete all the other
clusters that potentially mention unrelated namesakes. Our criterion for choosing
the “relevant” cluster is the level of interconnectedness of documents in the cluster:
for each document di we construct a set Li of its hyperlinks (see Section 4.7.4 for
the precise definition of Li); for each document cluster cj we construct a set CLj =
50
⋃
(di,di′ )∈cj (Li ∩ Li′), i.e. the union of pairwise intersections of hyperlink sets; finally
we cluster c with the largest set CL. In Section 6.6.1 we propose another, possibly
more adequate solution to the WAD problem.
4.7.1 Related work
Prior to our paper [13] where the WAD framework was introduced, only a handful
of papers addressed the problem of person name disambiguation. Some work was
done on person name disambiguation in a collection of scientific papers [51]. In the
Web domain, we are aware of three related works [4, 74, 43], within the general frame-
work of entity coreference (see, e.g. [83, 49]). Agglomerative clustering is applied in
all three. Bagga and Baldwin [4] use agglomerative clustering over traditional vector
space models of text windows around a personal name mention. Mann and Yarowsky
[74] propose a richer document representation involving automatically extracted fea-
tures. Their clustering technique however can be basically used only for separating
two people with the same name. Fleischman and Hovy [43] construct a MaxEnt clas-
sifier to learn distances between documents that are then clustered. This method
needs to be provided with a large training set. Since 2005, many followup papers
have been published, see [76, 111, 113] and about 30 others.
4.7.2 Evaluation criterion
To define our evaluation criterion, let c be the constructed cluster of documents
that we believe refer to people of our interest, and let cr be its portion consisting of
documents that actually refer to people of our interest. Let Dr be a portion of the
dataset D, that consists of documents referring to people of our interest. Precision
of the cluster c is then defined as Prec = |cr|/|c|, recall as Rec = |cr|/|Dr|, and
F-measure, standardly, as (2 Prec Rec)/(Prec+Rec).
51
4.7.3 WAD dataset
For evaluation of our methods, we gathered and labeled a dataset of 1085 Web
pages. In this section we describe the dataset and provide some interesting insights
into its structure.
From the Feb 2, 2004 snapshot of the CALO email data (see Section 4.6.2), we
selected one folder from Dr. Melinda Gervasio’s email directory and extracted 12
person names that appeared in headers of messages found in this folder. The names
are primarily of SRI employees and CS professors from various universities. All of
the individuals are likely to be present on the Web.
In May 2004, these 12 names (in quotation marks, i.e. treated as phrases) were
issued as queries to Google and for each query the first 100 pages were retrieved. We
manually filtered the pages, removing pages in non-textual formats, HTTPD error
pages and empty pages. We labeled the remaining pages by the occupation of the
individuals whose name appeared in the query. In 10 out of 12 cases, the names were
heavily ambiguous, thus pages representing 187 different people were retrieved given
the 12 names of people in Melinda Gervasio’s social network. In some cases, it was
difficult to decide to which of the namesakes the page referred. To determine this, we
often performed manual Web investigations. Table 4.5 shows some statistics of the
dataset.
Finally, all the pages were cleaned of their HTML markup and scripts. All the
URLs mentioned in the pages were extracted and placed at the end of each page,
together with the URL of the page itself. The dataset is publicly available at http:
//www.cs.umass.edu/~ronb/name_disambiguation.html.
The most ambiguous personal name among the twelve is Tom Mitchell. Although
the CMU Professor’s pages are prevalent over all the others, 37 different Tom Mitchells
can be distinguished in the 100 first Google hits, including professors in different fields,
musicians, executive managers, an astrologist, a hacker and a rabbi. Two personal
52
Person name Position Number of Number of Number of
pages categories relevant pages
Adam Cheyer SRI Manager 97 2 96
William Cohen CMU Professor 88 10 6
Steve Hardt SRI Engineer 81 6 64
David Israel SRI Manager 92 19 20
Leslie Pack Kaelbling MIT Professor 89 2 88
Bill Mark SRI Manager 94 8 11
Andrew McCallum UMass Professor 94 16 54
Tom Mitchell CMU Professor 92 37 15
David Mulford Stanford Undergrad 94 13 1
Andrew Ng Stanford Professor 87 29 32
Fernando Pereira UPenn Professor 88 19 32
Lynn Voss SRI Engineer 89 26 1
OVERALL: 1085 187 420
Table 4.5. Statistics of the WAD dataset. Categories are different namesakes
or other in case if the page does not refer to any of the namesakes. The last column
shows the number of pages that actually mention the person of our interest.
names out of the 12, Adam Cheyer and Leslie Pack Kaelbling, seem to be unique
in the Internet. However, for either of them, one page was retrieved that did not
contain any part of their names. These two pages were put into respective categories
other. Two other people, David Mulford and Lynn Voss, seem to have very little
Web presence. Only one page out of the 100 was related to any of the two. William
Cohen’s and David Mulford’s namesakes are well known politicians: the former US
Secretary of Defense William S. Cohen and the current US Ambassador to India
David C. Mulford. Naturally, the distributions of Cohen’s and Mulford’s pages are
heavily biased toward the politicians who are well represented on the Web.
An interesting phenomenon is observed for the names David Israel and Bill Mark.
Many of pages that responded to these queries only accidently contain the two words
adjacent to each other: Bill Mark’s pages often refer to mark-ups of certain bills, or
just list people’s first names (e.g. “Thanks Bill, Mark!”), while some of David Israel’s
pages discuss Israeli history and King David. None of these pages were removed from
the dataset, despite the fact that they are clearly unrelated to a particular living
person.
53
A major challenge for the WAD system is the pages of Bill Mark and Fernando
Pereira. Both researchers have namesakes who are also researchers in Computer
Science: another Bill Mark is a UTexas Professor, while another Fernando Pereira is
a Professor at Instituto Superior Técnico in Portugal. We term these pairs “doubles”.
To separate them is an especially difficult task. The opposite problem occurs with
Steve Hardt: he appears on the Web not only as an SRI engineer, but also as a creator
of an online game. We ourselves are actually unsure whether this is one person or
two different people, but most likely this is one person.
4.7.4 Baseline: link structure model
As our baseline, we propose a one-class clustering method based on link structure
analysis of Web pages (see [13] for some additional details).13 Let graph GLS =
(D,HL) be the Link Structure Graph over a set of Web pages D, where HL is a set
of hyperlink connections between Web pages in D. We say that two Web pages di
and di′ have a hyperlink connection, if the sets of their hyperlinks, Li and Li′ , have
a non-empty intersection: Li ∩ Li′ 6= ∅. Let us now define the set of hyperlinks.
For a Web page d, we define a function URL(d) to be the domain of d’s URL with
its first directory in case if this directory exists. For example. given page d1 with URL
http://www.cs.umass.edu/~ronb/timeline.html the function URL(d1) will return
www.cs.umass.edu/~ronb. Given page d2 with URL http://www.cs.umass.edu/
the function URL(d2) will return www.cs.umass.edu. By this, we capture the intu-
ition that full URLs can be too specific, while URLs’ domains can be too general.
Define a set POP to be a set of URLs with extremely popular domains, such as
www.amazon.com. The popularity of a domain is determined using operator :link
of Google’s command line. For a Web page d, define a set HOP (d) as a set of Web
pages that can be reached from d while following d’s hyperlinks.
13In [18] we proposed another link analysis method, based on a heuristic search in the Web graph.
54
Figure 4.6. Relevant and irrelevant Web pages according to the Link Structure
model. Relevant pages are within the δ-radius from the Core Connected Component.
White, gray and black colors indicate that the pages are retrieved by three different
queries.
Definition 4.7.1 A set of hyperlinks Li of a Web page di is defined as
Li = (URL(di) ∪ URL(HOP (di))) \ POP.
That is, Li is di’s URL and URLs that appear in di, after a generalization (using the
function URL) and removal of URLs with too popular domains.
The graph GLS consists of a number of connected components. Our task is to
find a Core Connected Component (CCC) of Web pages that mention people of our
interest. We naturally expect Web pages from CCC to interconnect much more than
non-CCC Web pages would interconnect. Of special importance is that CCC pages
referring to different people are likely to interconnect, while non-CCC pages referring
to different people would probably not connect to each other. We could have decided
that the Maximal Connected Component (MCC) of graph GLS would be the core
connected component. However, there can be a case where the MCC consists only of
Web pages retrieved in response to a single query—this can happen when pages of
one person h are heavily interconnected. If this person h appears to be an irrelevant
namesake, such MCC will be totally irrelevant. Therefore, we come up with the
following definition:
55
Definition 4.7.2 Denote Core Connected Component (CCC) c0 as the largest con-
nected component in GLS that consists of pages retrieved by more than one query.
Definition 4.7.3 The Link Structure Model MLS is a pair (CC, δ), where CC is the
set of all connected components of the graph GLS (note that c0 ∈ CC), and δ is a
distance threshold.
Our intuition is that the pages of the CCC and of a few connected components that
are close to the CCC refer to people of our interest, while the others do not. Figure 4.6
illustrates this intuition. To find the connected components that are close to CCC,
we apply the popular cosine similarity measure, while introducing a novel variation
of the tfidf term weighting function, that we call Google tfidf :
Google tfidf(w) =
tf(w)
log Google df(w)
, (4.9)
where Google df(w) is the estimated total results count of the term w if provided as
a query to Google. This document frequency count appears to be the most adequate
measurement of the commonness of the term w. The estimated total results counts
of words in our dataset were obtained using Google API.14
We do not explicitly set the distance threshold δ. Instead, given that in our dataset
(see Section 4.7.3) roughly one third of all Web pages refer to people of our interest,
we set δ such that one third of the pages in the dataset are within the threshold.15
4.7.5 Comparative results
Along with our baseline method from Section 4.7.4, we implemented greedy ag-
glomerative clustering (as applied in the related work [4, 74, 43]), based on the cosine
14http://www.google.com/apis/
15As in any unsupervised learning problem, the choice of the desired number of clusters or, dually,
of the cluster sizes, is a problematic issue. We do not attempt to address this issue here; instead,
we fix the size of the desired cluster based on our domain knowledge.
56
Method Precision Recall F-measure
Agglomerative 61.7 53.3 57.2
Link Structure 84.2 71.8 77.5
2-modal Comraf 87.3± 1.7 71.3± 2.5 78.4± 0.9
Table 4.6. Web appearance disambiguation results. Bi-modal Comraf results
are averaged over 4 independent runs, with the standard error of the mean reported
after the ± sign.
similarity measure between clusters and the augmented tfidf weighting function from
Equation (4.9). We did not measure interconnectedness of the clusters, we simply
chose the cluster whose F-measure was the highest among all the clusters. The moti-
vation for this choice was that we would like to show that our methods overcome the
best possible results of agglomerative clustering.
The summary of the results is in Table 4.2. As it can be seen from the table, both
link structure and MDC methods significantly outperform agglomerative clustering,
while MDC shows slightly better performance than the link structure method. A
relatively high deviation in precision and recall of the MDC algorithm is caused by
the fact that it never ends up with clusters of exactly the same size. Interestingly,
this deviation almost does not affect the F-measure: the precision trades off quite
well against the recall.
Analyzing the results by person, we can see that for quite a few people both
precision and recall are amazingly high, e.g. for David Israel, Leslie Pack Kaelbling,
Andrew McCallum, and Andrew Ng. It is also notable that the only relevant page of
David Mulford (the Stanford student) is found. As could be anticipated, the worst
precision is for Bill Mark and and Fernando Pereira, because both of them have
“doubles”. However, only 9 of 23 pages that refer to Bill Mark the UTexas Professor
appear in the category of relevant pages. The worst recall is for Steve Hardt and
Adam Cheyer. This can be easily explained for Steve: most of his pages refer to an
online game he created—relevance of these pages would be too difficult to determine.
57
0 0.2 0.4 0.6 0.8 1
0
0.2
0.4
0.6
0.8
1
recall
pr
ec
is
io
n
Figure 4.7. Precision/recall curve of the MDC algorithm. Points correspond to
consequent iterations of the algorithm (merges of Web page clusters).
As for Adam, the low result is a bit surprising, but it still makes sense: Adam’s name
often appears in an industrial context, while the language of most correctly-found
pages is purely academic—many of Adam’s pages fall too far from the central cluster.
Unfortunately, the single relevant page about Lynn Voss was not found, probably for
the same reason: it uses an industrial vocabulary.
The problem of disambiguating the “doubles”—the two Bill Marks and two Fer-
nando Pereiras who all work in Computer Science—can in fact be handled within
the Comraf framework. At some intermediate stages during the course of the MDC
algorithm the most interconnected cluster is relatively small but extremely clean.
Figure 4.7 shows the precision/recall curve for one run of the MDC algorithm. It can
be seen in the graph that when the recall of the relevant cluster is around 45% (there
are five clusters overall), the precision is very high (above 98%).16 This cluster con-
tains two pages of Bill Mark the SRI Manager and none of the pages of Bill Mark the
UTexas Professor; it also contains 15 pages of Fernando Pereira the UPenn Professor
and only one page of Fernando Pereira the Professor of Instituto Superior Técnico.
16Notably, when the recall is around 15% (17 clusters overall), we obtain 100% precision.
58
4.8 Experimentation: clustering scientific papers
In this section, we test the Comraf model on another type of data: a collection of
scientific papers. The goal of this experiment is as follows. From the model analysis
in Section 4.6.6 we can infer that in many cases tree-structured models perform com-
parably to loopy models. The question that we ask in this section is whether there
exists a case where a loopy model performs significantly better than a corresponding
tree-structured one. In Section 4.6.6 we provided an evidence for the advantage of
loopy models, where the underlying inference method is CWO. In this section, we
show the advantage of loopy models when the underlying algorithm is MDC.
The evidence given in this section has an important implication: as discussed in
Section 4.1, if a Comraf graph is tree-structured, then our objective function (4.3)
is a factorized version of Multi-Information (4.2). That is, Comraf models of a tree
structure are equivalent in their modeling power to the hard version of multivariate
Information Bottleneck (mIB) [97] where the Multi-Information is used. Loopy Com-
raf models, however, are not equivalent to mIB. As we show below, in some cases
loopy Comraf models obtain higher results than corresponding tree-structure ones,
which means that in those cases the Comraf framework is preferable over mIB.
Our dataset was created by David Mimno from a repository of scientific papers
collected for the REXA project.17 The dataset consists of 4887 conference papers,
published at ten venues: ACL, ICCV, ICRA, IJCAI, KDD, NIPS, SIGIR, SIGMOD,
STOC, and WWW. In our data, a significant number of papers belong to each of
the ten venues: between 224 and 933 papers. From the paper titles, we extracted
1436 words, each of which appeared in at least 2 titles. We also extracted 9841 words
from paper abstracts, each of which appeared in at least 2 abstracts. Citations in the
papers were automatically co-referenced using the REXA software system. Again, as
17http://rexa.info/
59
(a) (b) (c) (d) (e)
c
WT
D
c
D
c
c
C
c
WT
D
c
c
C
c
WT
D
c
c
C
D
c
c
AW
38.8± 0.5% 40.7± 0.7% 55.0± 0.7% 61.4± 0.6% 63.9± 0.7%
Table 4.7. Clustering scientific papers. Comraf models for clustering: (a) doc-
uments and title words; (b) documents and citations; (c) documents, title words
and citations in a tree-structured model; (d) documents, title words and citations in
a loopy model; (e) documents and abstract words. The bottom line is the micro-
averaged clustering accuracy obtained by those models.
in the case of words, we removed citations that appeared in only one paper, resulting
in 11,143 distinct citations.
Our goal is to cluster documents by their venues. We consider five Comraf models
presented in Table 4.7. First, we test two bi-modal Comrafs, where documents D are
clustered with their title words WT and with their citations C. Second, we experi-
ment with two tri-modal Comrafs (tree-structured and loopy), where D, WT and C
are clustered simultaneously. Finally, we present a bi-modal Comraf for clustering
documents D and abstract words WA.
Our underlying clustering method is a sequential MDC (see Section 4.3). We
cluster words and citations top-down, while clustering documents bottom-up. Our
clustering schedule is a plain round-robin. The algorithm stops when the desired
number of document clusters (i.e. 10, which equals the number of venues) is reached.
The micro-averaged clustering accuracy results are presented in the bottom line of
Table 4.7. As can be seen, neither title words nor citations are good document repre-
sentations. Only about 40% accuracy is obtained in a bi-modal Comraf using either
title words or citations. However, the result of a tree-structured tri-modal Comraf
(where documents are clustered simultaneously with title words and citations) is no-
60
tably 15% higher. Of particular importance is that adding the title words/citations
interaction improves this result by another 6% accuracy (on the absolute scale). Since
a loopy Comraf model like this is not equivalent to any model in the multivariate IB
framework, this result demonstrates the superiority of the Comraf modeling frame-
work over multivariate IB.18
The Comraf model that achieves the best performance on the scientific paper
clustering task is the one where papers are represented over words in their abstracts
(see the last column in Table 4.7). Adding another modality to this setup (such as
citations) causes a significant drop in the clustering accuracy. This result implies that
the abstract words’ modality is dense enough and much less noisy than title words or
citations. Whenever the abstracts’ data is available, using it would be preferable over
using the other modalities. However, if the abstracts’ data is unavailable, we show
that using a combination of two noisy modalities such as title words and citations
leads to almost the same result.
4.9 Experimentation: clustering documents by genre
So far, we have considered clustering documents by their topic. Topics, however,
are not the only way in which someone might want to select groups of documents.
Aspects such as genre, opinion, authorship, style, author’s mood, and so on are
interesting dimensions along which clustering results might break. In this section,
we focus on techniques appropriate for such non-topical clustering, with a particular
emphasis on genre. Although the field of non-topical (supervised) classification is
well explored in the literature (a lot of work was done on classification by genre
18Note that this is not the only advantage of Comrafs over multivariate IB models. The Comraf
framework is substantially simpler and more intuitive (e.g. the multivariate IB introduces in-space
and out-space concepts which are unnecessary in Comrafs). In contrast, Comraf inference algorithms
are more complex and effective than those proposed for the multivariate IB (see Section 4.6.5 for a
discussion).
61
[59, 60, 42, 71, 92], by text authorship [77, 3], by writer’s gender [63], tone [107, 85]
and mood [82], as well as by familiarity with the topic of the discussion [64]), we
believe that the problem of genre clustering had not been comprehensively studied
before we approached it in [9, 15].
To apply the Comraf framework to the task of clustering by genre, we first have to
decide about modalities that would best match the task. Documents are labeled with
genres on the basis of external criteria such as intended audience, purpose and activity
type [70]. The notion of genre can be described in terms of the syntax/semantics
duality of text: documents of different genres use different syntactic constructions
and/or different vocabulary. It is not obvious whether syntactic or semantic features
play a major role in clustering documents by genre. We propose to take advantage
of both. Besides the document modality, we consider two other modalities: words
(that correspond to documents’ vocabularies) and Part-Of-Speech (POS) n-grams
(that correspond to the syntactic structure of text). POS n-grams are extracted from
sentences in an incremental manner: the first n-gram starts with the POS tag of the
first word in the sentence, the second one starts with the tag of the second word etc.
For example, out of the sentence
<PNP>It <VBZ>’s <AT0>a <AJ0>real <NN1>holiday <PUN>.
we extract four trigrams:
PNP_VBZ_AT0, VBZ_AT0_AJ0, AT0_AJ0_NN1, AJ0_NN1_PUN.
Given a document collection, let D be a random variable over its documents, W be
a random variable over its words, and S be a random variable over the POS n-grams
of its words. We apply a multi-modal Comraf model (Section 3) for constructing a
clustering dc∗ of documents, a clustering wc∗ of words and/or a clustering sc∗ of POS
n-grams, by maximizing the objective derived from Equation (4.3). In this section,
we consider four Comraf models for clustering by genre:
62
D
c
S
c
D
c
S
c
D
c c
W D
c
S
c
W
c
(a) (b) (c) (d)
Figure 4.8. Comraf graphs for: (a) 1-way document clustering with POS unigrams
as an observed r.v. (shaded node); (b) 2-way clustering of documents and POS
bigrams (same as for POS 3-grams or 4-grams); (c) 2-way clustering with BOW; (d)
3-way clustering with POS bigrams and BOW.
1. POS unigrams: Since the number of POS tags in any tagging system is
relatively small, it makes no sense to cluster POS unigrams. Therefore, we
apply a 1-way model for clustering documents using the Comraf graph shown
in Figure 4.8(a). The objective function from Equation (4.3) in this simple case
has the form of I(D̃; S).
2. POS n-grams, where n > 1. The number of unique POS n-grams of order
higher than 1 is exponential in n, so clustering them would be necessary. We
perform a 2-way clustering with the Comraf graph from Figure 4.8(b) and the
objective I(D̃; S̃).
3. Bag-Of-Words: The number of unique words in our dataset is comparable
with the number of POS trigrams, so in analogy to the previous model, we
perform a 2-way clustering with the Comraf graph of Figure 4.8(c) and the
objective I(D̃; W̃ ).
4. BOW+POS hybrid: We combine contextual information of BOW and stylis-
tic information of POS n-grams into a 3-way clustering model, where we simul-
taneously cluster documents, words and bigrams of POS tags. Over the Comraf
graph of Figure 4.8(d), we maximize the sum I(D̃; S̃) + I(D̃; W̃ ).
63
Doc representation k-means LDA Comraf
Bag-Of-Words 9.1% 55.4± 0.1% 55.7± 0.2%
POS bigrams 23.2% 44.7± 0.2% 51.0± 0.2%
BOW + POS bigr n/a n/a 58.5± 0.6%
Table 4.8. Clustering by genre. Micro-averaged clustering accuracy on the BNC
corpus, averaged over four independent runs. Standard error of the mean is shown
after the ± sign. Comraf results with other POS tuples, besides bigrams, are in
Figure 4.9(left). The BOW+POS hybrid setup is only applicable in Comrafs.
4.9.1 Dataset
We evaluate our models on the British National Corpus (BNC) [24]. We employ
David Lee’s ontology of BNC genres [70] with 46 genres covering most aspects of mod-
ern literature such as fiction prose, biography, technical report, news script and others.
To perform fair evaluation using micro-averaged clustering accuracy (Section 4.6.1),
we choose 21 largest categories, for each of which we uniformly at random choose 32
documents, so our resulting dataset consists of 672 documents. The BNC texts are
formatted using the SGML markup language. We remove all markup, lowercase the
text, and delete stopwords and low-frequency words. All words in the BNC corpus
are semi-manually tagged using 91 POS tags, four of which refer to punctuation. The
resulting dataset has 63,634 unique words; and 5864 POS bigrams. Since the overall
number of unique POS trigrams and fourgrams is prohibitively large, we apply more
aggressive term filtering: we consider trigrams that appear in at least 10 documents
(44,499 trigrams overall) and fourgrams that appear in between 10 and 99 documents
(114,476 fourgrams).
4.9.2 Comparative results
We compare the results of Comraf models (with the MDC optimization algorithm)
with the results of k-means (Weka implementation), as well as of Latent Dirichlet
Allocation (LDA). As in Section 4.6, we use Xuerui Wang’s LDA implementation [78]
64
that performs Gibbs sampling with 10000 sampling iterations. Table 4.8 summarizes
our results.
As we can see from Table 4.8, MDC achieves more than 50% accuracy with both
BOW and POS bigram document representations. Note that a random assignment
of documents into clusters would lead to about 5% accuracy on our dataset, so above
50% accuracy is an impressive result for a purely unsupervised method on a large,
well-balanced dataset. The LDA+BOW system obtains exactly the same accuracy as
MDC+BOW does. However, LDA demonstrates strictly inferior performance (lower
than MDC by 6% absolute) on the POS bigram representation. We can also see that
MDC+BOW significantly outperforms MDC+POS (by more than 4% absolute). This
observation may imply that contextual features (such as words) play a more important
role for genre classification than stylistic features (such as POS n-grams).
To give some insight on the differences in MDC performance on BOW and POS
bigrams, we present Table 4.9 that shows the distribution of documents of each genre
over the generated clusters. For each genre we show a list of sizes (in number of
documents) of this genre’s representation in various clusters. We sort this list by
the size of the representation from the largest to the smallest. An asterisk after the
number of documents means that this genre is dominant in the corresponding cluster.
A heavy tailed distribution (such as the one of W non ac soc science) implies that
the genre is spread over many clusters which is clearly a failure. In contrast, a peaked
distribution (e.g., of W non ac tech engin) with an asterisk on its largest component
means that the genre was successfully identified.
As we can see from the table, MDC performs similarly on BOW and POS bigrams.
However, some significant differences can be found. For example, genres W biography,
W commerce and W institut doc are successfully identified by MDC+BOW but not
by MDC+POS, while MDC+POS better recognizes W newsp brdsht nat social and
65
Genre MDC with POS MDC with BOW LDA with BOW MDC with BOW
bigrams and POS bigrams
W ac humanities arts 9* 6* 6 4 2 2 1 1 1 9* 6* 5 5 3 2 1 1 7 6 5 5 4 4 1 9 6* 5 5 4 1 1 1
W ac nat science 23* 4 2 2 1 24* 6 1 1 12* 11* 9 27* 4 1
W ac polit law edu 14* 8 5 2 1 1 1 20* 5 2 2 1 1 1 19* 7 4 2 17 6 4 2 1 1 1
W ac soc science 11* 9* 6 5 1 12* 10* 7 1 1 1 12* 9* 8* 1 1 1 16* 7 6 3
W advert 14* 11 3 2 2 18* 3 3 2 2 1 1 1 1 22* 2 2 2 1 1 1 1 23* 2 1 1 1 1 1 1 1
W biography 15* 8 6 1 1 1 12 7 6 3 2 1 1 16* 6 4 2 2 1 1 16* 6 6* 2 1 1
W commerce 10* 5 5 4 2 2 1 1 1 1 13 10 6 1 1 1 16 5 4 2 2 1 1 1 9* 9 4 3 3 2 1 1
W fict prose 22* 7 3 25* 6 1 30* 2 24* 6 2
W institut doc 15* 6 5 5 1 18 6 4 1 1 1 1 17* 7 4 2 2 14 11* 3 1 1 1 1
W newsp brdsht nat 25* 5 1 1 28* 1 1 1 1 30* 2 27* 2 2 1
arts
W newsp brdsht nat 26* 2 1 1 1 1 32* 28* 2 1 1 31* 1
commerce
W newsp brdsht nat 32* 32* 30* 2 32*
report
W newsp brdsht nat 9 7 4 4 2 2 1 1 1 1 11* 6 4 3 2 2 1 1 1 1 10 7 6 3 2 1 1 1 1 14 6 3 2 2 2 1 1 1
social
W news script 32* 32* 31* 1 32*
W non ac humanities 11* 8 3 2 2 2 1 1 1 1 9* 6 5 3 3 2 2 2 10* 7 5 3 2 2 2 1 14* 5 3 3 2 1 1 1 1 1
arts
W non ac nat science 14* 5* 3 2 2 2 1 1 1 1 18* 11 2 1 11* 9 7 2 2 1 29* 1 1 1
W non ac polit law edu 11* 4 4 3 3 2 2 1 1 1 11 10* 5 3 2 1 10* 10* 3 3 2 2 1 1 10* 6 5 5 2 2 1 1
W non ac soc science 5 5 4 3 3 2 2 2 2 1 1 1 1 7 5 4 4 3 2 2 2 2 1 7 6 5 5 3 2 1 1 1 1 5 5 4 3 3 3 2 2 2 1 1 1
W non ac tech engin 32* 32* 32* 32*
W pop lore 11 6 6 5 4 10* 9* 4 4 2 2 1 12 8 6 3 2 1 16* 8 3 2 2 1
W religion 11* 5 4 4 2 2 1 1 1 1 18* 6 2 1 1 1 1 1 1 20* 6* 2 1 1 1 1 18* 6* 3 1 1 1 1 1
Table 4.9. Performance of various methods per genre. For each genre we
show a list of sizes (in number of documents) of this genre’s representation in vari-
ous clusters. We sort this list by the size of the representation from the largest to
the smallest. An asterisk after the number of documents means that this genre is
dominant in the corresponding cluster.
W pop lore. A 3-way MDC with both BOW and POS that would take advantage of
the both approaches may have a good chance to show even better results.
Indeed, we obtain a strong result with the 3-way MDC: 58.5% accuracy. The
last column of Table 4.9 presents the analysis of this result by genre. For many
genres (such as W non ac nat science) we enlarge their dominant representations.
We also manage to identify four of the five genres that were in disagreement be-
tween BOW and POS models (as discussed above). However, we no longer recognize
W ac polit law edu, which indicates that the results might potentially be improved
even more.
One could argue that the direct comparison of results obtained by the BOW and
POS bigram models is actually unfair because the number of BOW features is one
66
order greater than the number of POS bigrams, so that the BOW model naturally
outperforms the POS bigram model because it just contains more information. How-
ever, this argument cannot be empirically proved. We test MDC with POS trigrams
and fourgrams, as well as with POS unigrams, and show that while the MDC perfor-
mance with unigrams is significantly lower than with bigrams, trigrams and fourgrams
do not significantly improve the results of bigrams. In Figure 4.9(a) we can see that
when moving from bigrams to trigrams and fourgrams, the graph has a slightly pos-
itive slope, however the results become noisier (the standard error becomes higher)
which diminishes statistical significance of the improvement. A conclusion that can
be made from this experiment is that the Bag-Of-POS-bigrams model appears to be
rich enough to capture genres of documents.
A common belief is that stopwords and other high frequency words can be good
features for discrimination of documents by genre (see, e.g. [100]). It is interesting to
see whether we can support this hypothesis with empirical evidence. To show this, we
conduct the following experiment. We put various thresholds on the low frequency
words in the BOW representation of the documents. We consider four such thresholds:
our initial setup, when we filter out words that appear in less than 3 documents, as
well as three new ones: 10, 20 and 50 documents. Note that the new thresholds and
especially the most restrictive one (50) leave us with highly frequent words only: since
our dataset consists of 672 documents, filtering out words that appear in less than
50 documents causes removal of over 93% of unique words from the dataset. We run
MDC on the four representations. Figure 4.9(b) shows results of this experiment. We
can see that although the graph has a negative slope, the decrease in the results is
insignificant. With 7% of words from the original dataset the MDC system obtains
only 2.5% lower accuracy than with 38% of words (where the rest appear in only one
or two documents and can be removed with high confidence). This result confirms
that high frequency words are important for genre classification.
67
1 2 3 4
0.470
0.495
0.520
0.545
0.570
ngram size
ac
cu
ra
cy
Comraf accuracy with POS ngrams
3 10 20 50
0.470
0.495
0.520
0.545
0.570
threshold on low frequent words
ac
cu
ra
cy
Comraf accuracy with BOW
Figure 4.9. Clustering by genre. Micro-averaged clustering accuracy of Comraf
models as a function of: (left) size of POS n-gram (1-grams, 2-grams, 3-grams and
4-grams); (right) threshold on low frequency words—a point i on the X axis means
that in this experiment words that appear in less than i documents are removed.
4.10 Summary
In this chapter, we have proposed the objective function for Comraf clustering and
presented two inference methods in Comrafs: a global optimization method (MDC)
and a local optimization method (CWO). Comraf models have been successfully ap-
plied to document clustering. We have tested Comrafs on a variety of clustering
tasks:
• On email clustering (see Section 4.6), a bi-modal Comraf is compared with three
state-of-the-art clustering methods. It outperforms a (uni-modal) sequential IB
method because it benefits from the multi-modal nature of the data. The ad-
vantage of the bi-modal Comraf over the bi-modal (flat) ITCC method suggests
that the power of our inference algorithm stems from a better exploitation of
the clustering hierarchy. The Comraf model demonstrates superior performance
in comparison to LDA—a generative graphical model—because Comrafs pro-
vide a more flexible modeling environment (see Section 3.4). Also, we provide
evidence that extending a bi-modal Comraf to 3-modal and 4-modal setups can
further improve document clustering results.
68
• In Section 4.7 we apply a Comraf model to the real-world task of Web ap-
pearance disambiguation (WAD) of people names. We show that it slightly
outperforms a strong baseline method that employs link structure analysis of
Web pages. In [13] we show that the best results are achieved when using a
hybrid of the Comraf clustering and link structure analysis. In Section 6.6.1
we will show a better method for WAD that is based on one-class clustering of
documents.
• In Section 4.8 we address the question of whether Comrafs have more modeling
power than the previously proposed multivariate IB framework [97]. We provide
an example for strict superiority of Comraf models.
• Finally, in Section 4.9 we apply Comrafs to a non-topical document clustering
task. We focus on clustering by genre where a lexical modality (e.g. words) are
used in conjunction with a stylistic modality (POS n-grams). Similar Comraf
models can be applied to document clustering according to other non-topical
criteria, such as readability. In Section 5.3 we will extend the non-topical clus-
tering model to a semi-supervised case and test it on clustering by author’s
sentiment.
Being a valid graphical model, a Comraf takes advantage of modeling abilities of
existing graphical models. For example, we can introduce an observed state through
which some prior knowledge can be represented. The next chapter describes a result-
ing model.
69
CHAPTER 5
COMRAFS FOR SEMI-SUPERVISED LEARNING
The Comraf model is a convenient framework for performing semi-supervised clus-
tering [16, 17] (see Section 5.1), transfer learning [17] (see Section 5.2), and interactive
clustering [15] (see Section 5.3). Prior to presenting details of particular Comrafs, let
us define the concepts of hidden and observed states in the Comraf model. A combi-
natorial r.v. is hidden if it can take any value from its event space. A combinatorial
r.v. is observed if its value is preset and fixed.
5.1 Semi-supervised clustering with Comrafs
Semi-supervised clustering is a clustering task that takes advantage of labeled
examples. Usually, semi-supervised clustering is performed when the number of
available labeled examples is not sufficient to construct a good classifier (e.g., the
constructed classifier would overfit), or when the the labeled data is noisy or skewed
to a few classes. Assuming that most of the labeled data is accurate, our goal is to
incorporate it into the (unsupervised) Comraf model.
In this thesis, we consider only a uni-labeled case where each labeled data point
xi|ni=1 belongs to one ground truth category tj|kj=1. We propose an intrinsic Com-
raf approach for incorporating labeled data into clustering (by introducing observed
nodes to a Comraf graph), and compare it with existing seeding [7] and constrained
optimization [110] schema.
Intrinsic approach. Comrafs offer an elegant method for incorporating labeled
data, which does not require any significant changes in the clustering model proposed
70
in Chapter 4. First, note that labels define a natural partitioning of the labeled data:
for each label tj let x̃0j be a subset of X labeled with tj, i.e. x̃0j = {xi|ti = tj}. We
now define a r.v. X̃0 over the partitioning x
c
0 = {x̃0j|j = 1, . . . , k}, and we also define
a combinatorial r.v. Xc0 over all the possible partitionings of the set X . Since the
partitioning x̃c0 is given to us, the variable X
c
0 is observed, with x
c
0 being its fixed
value. Observed combinatorial random variables appear shaded on a Comraf graph.
The objective function from Equation (4.4) and the MPE inference procedure remain
unchanged (with the only difference being that there is no need for optimizing the
observed nodes): at each ICM iteration the current node is optimized with respect to
the fixed values of its neighbors, whereas the values of the observed nodes are fixed
by definition.
Constrained optimization. Wagstaff and Cardie [110] perform semi-supervised
clustering with two types of boolean constraints. The must-link constraint ml equals
1 if two equally labeled data points are assigned into different clusters; the cannot-link
constraint cl equals 1 if two differently labeled data points are assigned into the same
cluster. A clustering objective function incorporates the constraints, e.g. in Comrafs
(Equation (4.4)) for each combinatorial r.v. Xci it is:
xc∗i = arg max
xci
∑
i′: (Xci ,X
c
i′ )∈E
I(X̃i; X̃i′)−
∑
i′
wi,i′ mli,i′ −
∑
i′
wi,i′ cli,i′ ,
where the weights wi,i′ are set at +∞, which means that all constraints must be sat-
isfied. Note that in the general case we are free to choose any non-negative weights.
In order to fairly compare two semi-supervised methods, for both of them we must
use the same underlying clustering algorithm. We use the MDC algorithm (see Sec-
tion 4.3) in both cases.
Seeding [7] is a method of constructing the initial clustering of both labeled
and unlabeled data points, for which the must-link and cannot-link constraints are
71
D
c c
W
0D
c
D
c c
W
c
0W
Figure 5.1. Comraf graphs for: (left) semi-supervised clustering; (right) clustering
with transfer learning.
satisfied. This method is applied to Comraf clustering by adapting the initialization
step of the MDC algorithm (see Algorithm 2): for each node Xci we select an initial
point in lattice Li that satisfies the seeding constraints. Note that, in contrast to the
constrained optimization scheme described above, in the seeding scheme the cluster-
ing objective function remains unchanged, such that the seeding constraints may no
longer be satisfied during the course of the MDC algorithm.
5.1.1 Experimentation
Figure 5.1(left) shows a Comraf graph for the intrinsic scheme of semi-supervised
clustering. Together with a combinatorial r.v. Dc over document clusterings and
a combinatorial r.v. W c over word clusterings, we introduce an observed node Dc0,
whose value dc0 is a given partitioning of labeled documents. With a random variable
D̃0 defined over the clusters in d
c
0, our objective derived from Equation (4.3) is:
(dc∗, wc∗) = arg max
dc,wc
I(D̃; W̃ ) + I(D̃; D̃0) + I(W̃ ; D̃0).
As mentioned above, the ICM optimization procedure remains unchanged and iterates
over nodes Dc and W c only (the observed node Dc0 shall not be optimized).
It is interesting to note that the seeding approach to the semi-supervised clustering
appears to be useless when applied to Comrafs. Despite the sophisticated initializa-
tion, the optimization procedure leads to the same local maxima of the objective, as
72
in the case of trivial initialization. When applied to document clustering, the MDC
algorithm with seeding and without seeding demonstrates the same performance. Be-
low we compare our intrinsic Comraf scheme with the constrained optimization only,
which is naturally robust to the choice of a particular optimization method.
On the CALO and Enron datasets described in Section 4.6.2, we conduct the
following experiment: for each dataset, we uniformly at random select 10%, 20%,
or 30% of the data and refer to it as labeled examples while the rest of the data
is considered unlabeled. We apply both intrinsic and constrained methods on these
three setups and plot the micro-averaged accuracy (calculated on unlabeled data only)
vs. the percentage of labeled data used. The results (in terms of clustering accuracy
as defined in Section 4.6.1) are shown in Figure 5.2. As we can see from the figure,
both methods unsurprisingly improve the unsupervised results, while the intrinsic
Comraf method usually outperforms the constrained method.
On the 20NG dataset, we select 10% of data to be labeled. The constrained
method obtains 74.8 ± 0.6% accuracy, while the intrinsic method obtains 78.9 ±
0.8% accuracy (over 5% and 9% absolute improvement to the unsupervised result,
respectively).
The intrinsic scheme is resistant to noise. To show this, we conduct the following
experiment: on CALO datasets with the 20%/80% labeled/unlabeled split, we arbi-
trarily corrupt labels of 10%, 20% and 30% of the labeled data. Figure 5.2(f) shows
that clustering accuracy remains almost unchanged for all three datasets.
5.2 Transfer learning with Comrafs
Transfer learning is the problem of applying the knowledge learned in one task
to effectively solve another learning task. In this section, we represent the acquired
knowledge as a partitioning ỹc0 pre-built for data Y that can be used for constructing
a partitioning x̃c of data X . We note that the intrinsic scheme for semi-supervised
73
0 10 20 30
0.46
0.48
0.5
0.52
0.54
percentage of labeled data
ac
cu
ra
cy
acheyer
 
 
intrinsic
constrained
0 10 20 30
0.4
0.43
0.46
0.49
0.52
percentage of labeled data
ac
cu
ra
cy
mgervasio
 
 
intrinsic
constrained
0 10 20 30
0.72
0.75
0.78
0.81
percentage of labeled data
ac
cu
ra
cy
mgondek
 
 
intrinsic
constrained
(a) (b) (c)
0 10 20 30
0.4
0.45
0.5
0.55
percentage of labeled data
ac
cu
ra
cy
kitchen−l
 
 
intrinsic
constrained
0 10 20 30
0.67
0.69
0.71
0.73
percentage of labeled data
ac
cu
ra
cy
sanders−r
 
 
intrinsic
constrained
0 10 20 30
0.4
0.5
0.6
0.7
0.8
percentage of noise in labels
ac
cu
ra
cy
intrinsic model, 20% labeled data
 
 
acheyer
mgervasio
mgondek
(d) (e) (f)
Figure 5.2. Plots (a)-(e): comparing accuracies of the semi-supervised Comraf and
the constrained optimization method on five email datasets. Plot (f): the semi-
supervised Comraf’s resistance to noise in labeled data.
clustering presented in Section 5.1 above allows us to directly use labeled data not only
from X but also from another collection Y . Thus, in analogy to the semi-supervised
case, we introduce an observed combinatorial r.v. Ỹ c0 with a fixed value ỹ
c
0. During
the inference process, we construct x̃c∗ that maximizes agreement (in terms of mutual
information) with the labeled data ỹc0, while applying the same objective function as
in Equation (4.3) and the same ICM optimization procedure.
We set up a transfer learning experiment as follows. We notice that in two of
the CALO datasets (acheyer and mgervasio) similar topics are discussed. Our
hypothesis is that known categories of one dataset can improve the clustering results
on another dataset. To test this hypothesis, we first consider one dataset to be
labeled, while the other one is unlabeled, and then vice versa. However, since the
two datasets do not consist of the same documents, we decide to use word clusters of
the labeled dataset. We first cluster words distributed over categories of the labeled
74
dataset, as described in [11]. Then we introduce the constructed word clustering
as an observed node W c0 into the Comraf graph (see Figure 5.1 right) and perform
the ICM inference. Using this scheme we improve the micro-averaged clustering
accuracy on mgervasio by 3% absolute over unsupervised clustering, but we do
not see any change in accuracy on the acheyer dataset. This preliminary result
demonstrates the usability of Comrafs for transfer learning; other types of Comraf
models for transfer learning are emerging.
5.3 Interactive clustering with Comrafs
In interactive clustering of text collections, the user is actively involved in the
process of clustering documents, their features, or both (see, e.g., [53]). Being thus
provided with some level of supervision, the interactive clustering scheme can be
viewed as an instance of semi-supervised learning. In Sections 5.1 and 5.2 above,
we have shown how to incorporate prior knowledge into the Comraf graph G, while
using the same objective or inference algorithm as in unsupervised clustering. Here,
we incorporate prior knowledge into our inference algorithm, preserving the Comraf
graph and the objective (4.3) of the unsupervised case.
In [15], we proposed interactive clustering as a unified framework for clustering
document collections according to nearly any criterion of the users choice: docu-
ments’ style, readability, credibility; authors’ age, mood, sentiment, familiarity with
the topic etc. (for the beginning of the discussion and an example of clustering by
genre, see Section 4.9). The user is first asked to choose modalities (or types of fea-
tures) suitable for clustering by the desired criterion. In clustering by genres, for
example, documents may be represented over sequences of Part-Of-Speech (POS)
tags, punctuation marks, stopwords, as well as over general words as captured in the
standard BOW representation. The user is next asked to provide a few examples of
features (seed features) of the chosen types, if such examples are intuitive and can
75
be obtained without much effort—e.g., when clustering by authors mood, words like
‘angry’, ‘happy’, ‘upset’ might be easily suggested.
The clustering system then represents documents based on the users choice and
applies a Comraf clustering method. When seed features are provided, the system
iteratively clusters documents represented over the chosen features and then enriches
feature sets with other useful features. The user can choose to intervene (or not) after
each iteration, in order to fix possible mistakes made by the system on the feature
level (no document labeling is required).
In this section, we illustrate the effectiveness of our approach on clustering by
author’s sentiment [107]. In clustering by sentiment, data categories correspond to
different levels of the authors’ attitude to the discussed topic (e.g. liked/disliked, sat-
isfied/unsatisfied etc.) The categories can be finer grained (strongly liked / somewhat
liked etc.)—as long as it is possible to distinguish between two adjacent categories.
We perform interactive clustering within a bi-modal Comraf framework, where doc-
uments and words are clustered simultaneously. The user is involved in the process
of clustering words (it is easier for the user to be involved in clustering words than in
clustering documents [90]).
5.3.1 Related work
There has been work on interactive topical clustering where the user corrects clus-
tering errors on a document basis [8], but that effort is more time consuming than
feedback on features [90]. Other recent work has had the user select important key-
words for (supervised) categorization, thereby leveraging the user’s prior knowledge
[31, 90]—approaches that are more like that of our framework. Raghavan et al. [90]
further support this direction in the finding that users can identify useful features
with reasonable accuracy as compared to an oracle. Liu et al. [72] experiment with
labeling words instead of documents for text classification, providing the user with a
76
list of candidate words from which to select potentially good seed words, based on
which a training set is constructed from a set of unlabeled documents. A classifier
is then constructed given this training set. Liu et al.’s document representation is
the standard BOW, which has strong topical flavor, and therefore cannot be used for
clustering by arbitrary criteria (for example, our preliminary experiments show that
BOW is not appropriate for clustering by author’s mood). In addition, Liu et al.’s
method involves the user only at the initial step (selecting seed words), limiting the
user’s control of the classification process.
Although the supervised task of classification by sentiment has been widely ad-
dressed in the literature (see, e.g. [84] and references therein), clustering by sentiment
has been very sparsely covered. Turney [107] performs a binary clustering of product
reviews by authors’ sentiment, where only two clusters of documents are constructed:
positive reviews and negative reviews. We are not aware of previous work on cluster-
ing by sentiment that goes beyond the binary approach. In this section, however, we
cluster reviews into four groups, corresponding to the categories of strongly positive,
somewhat positive, somewhat negative and strongly negative reviews.
5.3.2 Interactive clustering scenario
Here we provide a step-by-step recipe for clustering documents by a particular
criterion that the user has in mind:
1. Specify the number of clusters: Learning the natural number of clusters
still remains an open problem. We do not attempt to solve it in this thesis,
instead the user is asked to specify the desired number of clusters.
2. Specify feature types: A list of various feature types is provided to the user.
Examples of such types are: bag of words or word n-grams, POS tags or POS tag
n-grams, punctuation, parse subtrees and other types of syntactic and semantic
patterns that can be extracted from text. Such a list can hypothetically include
77
a large variety of feature types that would respond to everyone’s needs. From
this list the user is asked to choose one or more types that best serve the
particular clustering criterion.
3. Give examples of features: For each feature type chosen, the user should at-
tempt to construct (small) sets of seed features that correspond to each category
of documents. Sometimes this task is easy: e.g., if the clustering criterion is
authors’ sentiments, then words such as ‘excellent’, ‘brilliant’ etc. would corre-
spond to the category of positive documents, while ‘terrible’, ‘awful’ etc. would
correspond to the negative category. However, when such sets cannot be easily
constructed (e.g. it is non-trivial to come up with good feature examples for
clustering by genre—see Section 4.9), the user can skip this step.
4. Default clustering: If m feature types are chosen, but no seed features are
provided by the user, the standard (unsupervised) clustering scheme is applied
(see Chapter 4).
5. Interactive Clustering: For the cases when the user has provided seed fea-
tures for some of the feature types, we propose a new model for Comraf clus-
tering, which combines regular clustering of non-seeded variables with an incre-
mental, bootstrapping procedure for seeded variables:
(a) Represent documents as distributions over the sets of seed features. Ig-
nore documents with zero probability given the seed features. Cluster the
remaining documents using a Comraf clustering method.
(b) Stop if most documents have been clustered (for details, see Section 5.3.3
below).
(c) Represent all features of the clustered documents as distributions over the
document clusters. Ignore features that have zero probability given the
78
clustered documents. Cluster the remaining features using the distribu-
tional clustering method.
(d) Select feature clusters that contain the original seed words. Let the user
revise the selected clusters: noisy features can be deleted; misplaced fea-
tures can be relocated; new features can be added. The revised clusters of
features are the new sets of seed features. Go to 5(a).
5.3.3 Clustering by sentiment
Following the procedure described in Section 5.3.2 above, after choosing the num-
ber of clusters and particular feature types, the user is asked to select a few seed
features for each category. For clustering by sentiment, as well as for somewhat sim-
ilar tasks of clustering by authors’ mood or by familiarity with the topic, relevant
feature types may be words or word n-grams (i.e. semantic features). However, for
other quite close tasks, e.g. clustering by authors’ age, not only semantics but also
syntax can matter: children, for instance, use certain words more often than adults
do; children also tend to use primitive (and sometimes erroneous) syntactic construc-
tions (“me going bye-bye” etc.). In this section, for simplicity, we experiment with
word features only.
The task of selecting “sentimental” seed words has two issues. First, it is easier to
come up with words that correspond to extreme sentimental categories (‘spectacular’,
‘horrible’), but it is difficult to choose seed words for intermediate, mild categories.
Nevertheless, as we will see in Section 5.3.6 users usually succeed in accomplishing
this task. Second, in our early experiments, users consistently tended to choose
words that were out of the vocabulary of a given dataset. Inspired by Liu et al. [72],
we decided to provide the users with a word list, to narrow her search only to the
dataset vocabulary. Unlike Liu et al. [72], whose task is topical clustering, we cannot
automatically predict which words would be relevant. Instead, we employ Zipf’s law
79
and provide the user with a list of words from the interior of the frequency spectrum.
We anticipate such a list to contain the most relevant seed words.
We then perform an iterative process of clustering that allows user’s involvement
in between clustering iterations. We apply a bi-modal Comraf model: we first cluster
documents that contain the selected seed words and then we cluster all words of these
documents. In the latter step, our seed word groups are enriched with new words that
have been clustered together with the original seed words. The user is then asked
to edit the new seed word groups, in order to correct possible mistakes made by
the system (word removal, relocation and addition is allowed). By this, a clustering
iteration is completed and the next iteration can be executed.
Since the seed word groups have been enlarged, we can expect that a set of doc-
uments that contain these seed words is now larger as well, so that the clustering
process will cover more and more documents from iteration to iteration. The process
stops when no more documents are added to the pool. Documents that have never
been covered (the ones that contain no seed words from the largest seed word groups)
are considered to be clustered incorrectly. An alternative approach to guarantee the
algorithm’s convergence would be to require enlargement of seed word groups such
that at least one document is added to the clustering at each iteration. The algorithm
would then stop when the entire dataset is covered. We choose the former approach
because (a) we do not want to put additional constraints either on the user or on
the Comraf clustering model; (b) in each real-world dataset there can be documents
whose sentimental flavor is hard to identify—it would not be beneficial to force such
documents into any of the sentimental clusters.
5.3.4 Dataset
We evaluate our interactive clustering system on a dataset of movie reviews. Our
dataset consists of 1613 reviews written on “Harry Potter and the Goblet of Fire
80
(2005)” that we downloaded from IMDB.com in May 2006.1 The data was preprocessed
exactly as the BNC corpus (Section 4.9.1). We ignore reviews that do not have rating
scores assigned by the user. The IMDB’s scoring system is from 1 (the worst) to
10 (the best). Based on our extensive experience with IMDB.com, we translate these
scores into four categories as follows: scores 1 to 4 are translated into the category
strongly disliked (292 documents), scores 5 to 7 are translated into somewhat disliked
(454 documents), scores 8 and 9 into somewhat liked (447 documents), and score 10
is translated into the category strongly liked (420 documents). We do not introduce
a neutral category because there are very few neutral reviews on IMDB.com.
5.3.5 Experimental setup
On the task of clustering by sentiment, we compare our method’s performance
with that of k-means and LDA (Section 4.6.3), as well as with the performance of
an SVM classifier trained on 22,476 movie reviews. The training data for the SVM
consisted of reviews of 46 popular Hollywood movies released in 2005, of the same
genre as Harry Potter. The reviews and genre labels of movies are obtained from
IMDB.com. Again, we ignore reviews without user-assigned rating.
To compare our Comraf clustering with other clustering methods, we again use
the micro-averaged clustering accuracy, as described in Section 4.6.1. It is not obvious
however how to compare Comraf clustering results with SVM classification results.
In [16], we show that the clustering accuracy can be directly compared with the
(standard) classification accuracy if a constructed clustering is well-balanced, meaning
1Bo Pang [84] maintains a popular dataset of movie reviews that, unfortunately, does not fully
correspond to our task because (a) we want to differentiate the problem of clustering by sentiment
from the topical clustering—for this reason our dataset contains reviews written on one movie only,
so that the topic of all the reviews is potentially the same; (b) movie ratings in Bo Pang’s dataset
are extracted from the reviews’ text, which is an error-prone procedure, whereas in our dataset the
ratings are assigned by the reviewers using an HTML form which leaves no room for errors.
81
Doc repres. k-means LDA Comraf SVM
BOW 28.2 37.0± 0.2 40.3± 0.8 39.1± 0.3
Sentim. list 29.0 40.2± 0.5 43.0± 0.9 41.3± 0.6
Interactive clustering (Oracle) 47.1± 0.2 n/a
Simulated classification (Oracle) 46.3± 0.1
Table 5.1. Clustering by sentiment. Clustering accuracy of Comraf models (both
interactive and non-interactive) is compared with clustering accuracy of k-means and
LDA, as well as with classification accuracy of SVM. All results are averaged over
four independent runs. Standard error of the mean is shown after the ± sign.
that each category prevails exactly in one cluster. It appears that all our clusterings
obtained using the Comraf model are well-balanced.
The system is evaluated on five users who are familiar with the task of document
clustering. The users were explained the idea behind interactive clustering and pro-
vided a brief description of the dataset. They were given a list of 563 words that
appeared in 50 ≤ n < 500 documents in our dataset. The users proceeded as de-
scribed in Section 5.3.2. Also, we construct an oracle as follows: for each category t
we select 25 most frequent words that belong to a given list of sentimental words2 and
their distribution over the categories has a peak at t. Unlike human users, the oracle
does not provide feedback between clustering iterations. To some extent, the oracle’s
performance can be considered as an upper bound to results obtained in practice,
when a human user is involved.
We perform a simulated classification (SC) experiment analogous to the one of Liu
et al. [72] (see a description in Section 5.3.1), where the seed words are provided by
our oracle. We replace an ad-hoc kNN-like clustering in Liu et al.’s implementation
by our effective Comraf clustering, and a Naive Bayes classifier by an SVM.
2Our list of 4295 sentimental words was obtained as described in [38].
82
User 1 User 2 User 3 User 4 User 5
35
40
45
50
A
cc
ur
ac
y
 
 
SVM
Initial seed words
1 correction step
2 correction steps
User 1 User 2 User 3 User 4 User 5 SVM
30
35
40
45
50
55
60
A
cc
ur
ac
y
 
 
Strongly Disliked
Somewhat Disliked
Somewhat Liked
Strongly Liked
Figure 5.3. Interactive clustering by sentiment. Micro-averaged clustering accuracy
over various users: (left) over interactive learning iterations (with original seed words
only, after one correction step and after two correction steps). The horizontal line is
SVM performance (after feature extraction using a given list of sentimental words,
and after training on over 20K documents); (right) over categories of the dataset after
two correction steps.
5.3.6 Comparative results
Table 5.1 summarizes our observations. Surprisingly, with BOW features, our
Comraf clustering method performs as well as an SVM trained on a large amount
of data (Row 1). The good performance of our unsupervised method (with BOW)
indicates that the constructed topical clustering sheds some light on reviewers’ sen-
timents, which can occur when the reviewers have a consensus on certain aspects of
the movie, e.g. liked the actors but disliked the plot etc.
After feature selection according to our list of sentimental words, the Comraf
achieves a significant boost in accuracy surpassing the SVM (Row 2). Using an
oracle in our interactive clustering setup (Row 3) improves the performance even
further, while the SC result (Row 4) is only slightly (but significantly) inferior. These
two results are close because the training set of SC is identical to the clustering
constructed at the first iteration of the Comraf algorithm. Since its size appears to
be over 3/4 of the entire dataset, there is almost no room for the actual diversity in
performance of the two methods.
83
Figure 5.3 (left) shows the micro-averaged clustering accuracy for each user and
each iteration. For three of the five users, selection of the initial seed words is suf-
ficient to obtain significantly higher accuracy than the best result of the SVM. User
2 has significantly lower accuracy than the baseline to begin with, but over the two
correction steps is able to provide the necessary feedback so as to obtain an improve-
ment in accuracy, equaling the baseline. We found that User 2 was fairly conservative
in her assessment of terms in the beginning marking only 26 terms, while User 1
(the one with the best average performance) marked 58 terms, 23 of which were in
common with User 2. User 4 reported that she aggressively removed words at the
first correction step, which caused a noticeable drop in the performance.
Figure 5.3 (right) shows the accuracy per class, per user at the end of 3 itera-
tions. User 1 and User 2 have near identical accuracies on the two extreme categories
(strongly liked and strongly disliked), but User 1 has higher accuracies on the inter-
mediate categories, resulting in higher micro-averaged accuracy. It is apparent from
this figure that users are able to come up with good features for the two extreme cat-
egories, but have difficulties with the intermediate categories. The figure also shows
the performance of SVM (with sentiment features). It is interesting to note that the
SVM’s pattern of behavior is almost identical to the interactive Comraf’s.
5.4 Summary
In this chapter, we have shown that Comrafs can be straightforwardly applied
to semi-supervised learning, while either adjusting the Comraf graph or the Comraf
inference algorithm. As the semi-supervised setup can be viewed as an instance of a
supervised setup, we can make a statement that Comrafs are applicable to the entire
spectrum of machine learning tasks.
On the task of semi-supervised clustering, we showed that Comraf models out-
perform a popular constrained optimization method. We also showed that Comraf
84
models are very robust to the noise in data labels. Our preliminary results demon-
strate applicability of the Comraf framework to transfer learning, which has a variety
of interesting applications.
We also applied the Comraf framework to non-topical clustering of documents, by
introducing the interactive clustering model. We showed that interactive clustering,
which is a semi-supervised version of an unsupervised clustering scheme, can poten-
tially outperform one of the best supervised learning methods (SVM), trained on a
large amount of labeled data. This result raises an important question that has not
been widely addressed in the machine learning literature: for a particular unlabeled
dataset, would it be more beneficial to train a supervised model on similar, yet differ-
ent, data (such as, train a classifier on reviews of movie A and apply it to reviews of
movie B), or it would be better to construct a clustering model that takes advantage
of some limited knowledge on the unlabeled data, as provided by the user? It appears
that the former approach is quite popular. In this chapter we provided evidence that
the latter approach can be more effective.
85
CHAPTER 6
COMRAFS FOR ONE-CLASS CLUSTERING
As we discussed in Section 3.4, each Comraf model is a trinity of a Comraf graph
G, an objective function that is factored over G, and an inference procedure for op-
timizing this objective function. So far, we have experimented with various Comraf
graphs (with or without observed nodes) for multi-modal clustering, where the objec-
tive is a sum of pairwise Mutual Information terms (3.1), and the inference procedure
is a variant of MDC (see Section 4.3). Exploring the variety of Comraf graphs led
us to proposing models for email clustering (Section 4.6), clustering scientific papers
(Section 4.8), document clustering by genre (Section 4.9), semi-supervised cluster-
ing (Section 5.1), and clustering with transfer learning (Section 5.2). In Section 5.3,
however, we went beyond this scope and proposed an enhancement to the MDC infer-
ence procedure that led to an interactive clustering model. Note that the interactive
clustering model exploits the same Comraf graph and objective function as other
multi-modal clustering methods we proposed. This chapter goes further in inves-
tigating the role of the objective function in Comrafs. Specifically, we focus on the
problem of constructing an objective function that best suits a particular application.
We address this problem on a representative task of one-class clustering, which
is the task of identifying the most coherent subset of documents (the core) from a
given pool of documents. This pool can be generated by a search engine (as a set
of documents retrieved on a given query); also, this pool can be an email Inbox, a
repository of scientific papers etc. One-class clustering is a technically simpler task
than the general (multi-class) clustering: on a given dataset, a binary (as opposed
86
to a k-ary) predicate is constructed that answers the question of whether or not a
data instance belongs to the core. This simplicity allows for a theoretical analysis of
optimality of the one-class clustering method proposed.
Similar to many other unsupervised learning problems, the problem of one-class
clustering is generally ill-posed as one can argue that the shortest document in a col-
lection satisfies the criterion of being the most coherent subset. We resolve that issue
by introducing a parameter k, which is the number of documents in the core subset.
This parameter is analogous to the number of clusters in (multi-class) clustering, the
number of outliers [105] or the radius of Bregmanian ball [28] in other formulations
of one-class problems.
Note that formally the problem of one-class clustering is a special case of the
general, multi-class clustering: one-class clustering is a problem of constructing n −
k + 1 clusters of n data instances, where one cluster is of size k and all the others
are singletons. However, since explicit modeling of singleton clusters appears to be
useless, from the practical point of view the two problems become different: methods
applicable for one-class clustering are generally unapplicable to multi-class clustering
and vice versa. Also note that the problem of one-class clustering is a compliment to
an unsupervised formulation of the outlier detection problem [1, 105]: once the core
cluster is constructed, all the non-core data instances are considered outliers.
Speaking in terms of Comraf models, for one-class clustering we define combi-
natorial random variables over all the possible subsets of a modality (or, in other
words, over its powerset). Recall that for multi-class clustering we defined combina-
torial random variables over all the possible partitionings of a modality. Although
the Comraf graph’s layout appears to be the same for both tasks, the one-class clus-
tering objective function is different from that of multi-class clustering, and so is the
inference procedure. In this chapter, we construct step-by-step the objective function
87
and inference procedure, starting from an artificially simplified case and ending with
the real-world application.
Our working assumption throughout this chapter is that the core documents share
a (relatively) small lexicon, while the remaining documents (the noise) do not have
much in common (i.e. they are randomly drawn from the pool of all existing documents
written in the English language). Our methods, however, will work equally well in
situations when the noise has some structure, meaning that some non-core documents
share their topics.
We describe the simplest Comraf model with only two modalities: documents
and words. Despite its simplicity, this setup allows for three different approaches to
one-class clustering of documents:
• Identify the shared lexicon (the subset of relevant words), i.e. solve the one-
class clustering problem for words. A document will then be considered a part
of the core if it contains enough relevant words. We describe this setup in
Section 6.2. The fundamental question we answer in that section is whether or
not the subset of relevant words can be identified in document collections of
feasible size. We show analytically that, under some simplifying assumptions,
the subset of relevant words can be optimally identified in document collections
of log-linear size (in the size of the vocabulary).
• Directly identify the core documents, based on their distributions over words.
This setup is in the focus of Section 6.3. In that section, we propose our
information-theoretic objective function. We derive a simple uni-modal algo-
rithm for optimizing this objective. We show that the proposed algorithm is
optimal under the assumptions imposed in Section 6.2. We then relax these
assumptions and adjust our objective function to the real-world case.
88
• Perform one-class co-clustering (OCCC), while simultaneously identifying the
subset of relevant words and the subset of core documents (see Section 6.4). We
generalize the algorithm proposed in Section 6.3 to the bi-modal setup. The
resulting OCCC algorithm significantly outperforms the uni-modal one.
In Section 6.5, we propose another, probabilistic objective function for our task: the
likelihood that a document belongs to the core. Inspired by Huang and Mitchell [53],
we apply an EM inference algorithm to the resulting model.
We evaluate our information-theoretic and probabilistic models on two applica-
tions: (a) Web appearance disambiguation (see Section 4.7)—our methods outper-
form the algorithm proposed in [13]; and (b) re-ranking information retrieval re-
sults [65, 36]—we significantly improve the accuracy of original Google’s ranked lists,
as well as of one-class (unsupervised) SVM and one-class Information Bottleneck [28].
Note that our models can also be applied to other real-world tasks, e.g. to spam de-
tection, news filtering, image retrieval, and basically to any task where a common
subset of features can be identified in a subset of data instances.
6.1 Related work
Many previously proposed one-class clustering methods (see [105, 28, 50], and
references therein) are vector-space methods, where the goal is to find a convex body
of small volume that contains as many data instances as possible. Despite that binary
vector-space methods have proven themselves to be very effective in the text domain,
one-class vector-space methods are problematic. In binary methods, the decision
boundary is linear (with or without applying the kernel trick [29]). In (vector-space)
one-class methods, however, the boundaries are essentially elliptic, which is unnatural
in the highly multidimensional text domain: core documents tend to lie on a lower-
dimensional manifold (see [68]), while elliptic boundaries tend to capture too much
space around it.
89
An alternative solution suggested in [13] (and discussed in Section 4.7) is to sim-
ulate one-class clustering in text by first applying traditional multi-class clustering,
after which one of the clusters in chosen. Intuitively, this approach makes a wrong de-
sign choice: structure is artificially forced on the space of non-core documents, which
may not have any underlying structure. The models described in this chapter, in
contrast, achieve the main goal of one-class clustering—to identify the most coherent
subset of objects—without imposing structural or topological constraints.
Our one-class clustering models have interesting cross-links with models applied to
other Information Retrieval tasks. For example, a model similar to our information-
theoretic one-class clustering, is proposed by Zhou and Croft [115] for query per-
formance prediction. Tao and Zhai [104] describe a pseudo-relevance feedback model,
which is similar to our probabilistic one-class clustering (see discussion in Section 6.6.2).
These types of cross-links are common when the models are general enough and rel-
atively simple. In this work we pay particular attention to the simplicity of our
models, such that they are feasible for theoretical analysis as well as for efficient
implementation.
6.2 One-class clustering of words
We are given a dataset D of n documents, each of which is represented as a vector
of words, with no importance to their order (i.e., bag-of-words). We assume that
D has a core Dk of k documents written on one topic, while the rest of the (n − k)
documents are noise. LetR be the lexicon of relevant words and G ⊃ R be the general
lexicon of D (i.e. all distinct words of D). Let us denote m = |G| and mr = |R| the
sizes of the two lexicons, where mr ¿ m. Assuming that the core is not too small
( k
n
À 0), our intuition is that a word belongs to R if it is more frequently used in D
than it would be used in general English. For example, many occurrences of the words
“reinforcement”, “regression”, “classifier” in D indicate that they are relevant, as the
90
g
z
r
|d|
n
w
g
zy
r
|d|
n
w
Figure 6.1. (left) The simplest generative model; (right) Latent Topic/Background
model (Section 6.5).
probability of observing the same frequency of these words in non-core documents is
very low. Our first task is to determine which words belong to R.
We attempt to solve this problem by introducing a simple generative model of
documents (see the left panel of Figure 6.1). Given a dataset D of size n, for each
word token in every document, we first decide if it is drawn from a distribution Pr(W )
over the the set R of relevant words, or from a distribution Pg(W ) over the set G of
all words in D, and then we choose the word w accordingly. Both Pr(W ) and Pg(W )
are multinomial, where the former has a much smaller support. Note that in our
model, for each word token w, the decision whether it is drawn from Pr(W ) or from
Pg(W ) is made independently of the rest of the model, and thus we can think of the
dataset D as a single document of length N = n|d| (here and in the next section, we
assume that all the documents are of the same length |d|).
To make the following theoretical analysis easier, let us assume (quite unrealisti-
cally) that distributions Pr(W ) and Pg(W ) are uniform rather than multinomial. In
Section 6.3.1 we relax this assumption by flattening multinomials using a correction
term. Under the uniformity assumption, an algorithm for identifying relevant words
is straightforward: obtain a sample of size N and choose words with counts above
a certain threshold to be in R (see an illustration in Figure 6.2 left). The major
drawback of this algorithm is that we should know the exact value of the threshold
91
Figure 6.2. An illustration of possible distributions of word counts in one-class
clustering: (left) uniform case; (right) multinomial case. Words whose counts are
above the threshold are considered relevant. Note that in the multinomial case counts
of some relevant words can be lower than counts of non-relevant words.
(an estimation is not enough here). An alternative algorithm would be: obtain a
sample of size N , sort words in decreasing order of their counts and choose the first
mr words to be in R. Clearly, the two algorithms are asymptotically equivalent (they
identify the same set R if the sample size N is large enough).
An important question is how large should be the sample size N so that the
sets of relevant and non-relevant words will be separable. For instance, if N = O(m2)
samples are required, the algorithm described above will be infeasible in any real-world
case. In the following theorem, we prove that a log-linear sample size is enough. Let
us first introduce some notation. For a document di and a word token wij, let π be the
probability of drawing wij from the pool of relevant words R, that is: P (Zij = 1) = π.
Let pw =
mr
m
= |R||G| be a fraction of relevant words in the dataset’s vocabulary.
Theorem 6.2.1 To determine the set R with probability 1− δ, we need at most
N = 16
m
π
ln
m
δ
(6.1)
samples, under a (weak) constraint of pw < 2π.
The proof of this theorem is relatively straightforward—it involves an application
of the Chernoff bound and the union bound. We prove this theorem in Appendix A.
Now, under the uniformity assumption and conditions imposed in Theorem 6.2.1,
we can identify the set R of relevant words with arbitrarily high probability. The
92
0.125 0.25 0.5 1 2 4 8 16
0.9
0.95
1
constant in calculation of N
ac
cu
ra
cy
Figure 6.3. The accuracy (as defined in Section 6.6, averaged over 100 independent
runs) of identifying R in a simulation of the generative process, over various values
of the constant from Equation (6.1) for the sampling size N . In Equation (6.1), the
value of this constant is set to 16. Here we show that the value of 2 is enough in
practice.
relevance of a document is then determined by the cumulative relevance of words
occurring in the document. Consequently, the core Dk will consist of k documents,
each of which contains more words from R than any document from D \ Dk.
We simulated the generative process for various values of π and pw. We saw that
in practice many fewer sampling iterations were required for identifying the set R
with 100% accuracy. In Equation (6.1), the constant in calculating N is set to 16.
We tuned the value of this constant, and showed that the value of 2 is generally
enough to perfectly identify R. Figure 6.3 outlines some results on synthetic data
that has similar characteristics to our WAD dataset (see Section 4.7.3): we choose
m = 12000, π = pw = 0.2, and δ = 0.01. For N = 330, 000, which is the size of the
WAD dataset, we obtain 98.5% accuracy. This implies that if words in text datasets
were indeed distributed uniformly, the one-class clustering problem would be easy.
6.3 Min-Entropy algorithm for one-class clustering in text
Obviously, the trivial one-class clustering algorithm from Section 6.2 above is ap-
plicable only under the restrictive uniformity assumption. Sticking to the uniformity
assumption for now, we propose an alternative formal criterion, which in Section 6.3.1
will be adjusted to the practical case. Based on this criterion, we design an algorithm
that directly identifies the core, and show that this algorithm is optimal under the
93
uniformity assumption. Let us define a word entropy of the dataset D as:
H(W ) = HP (P (W )) = −
∑
w∈G
P (w) log P (w) = −
∑
d∈D,w∈G
P (d, w) log P (w), (6.2)
where P is an empirical distribution of words in D: given that a word w occurs Nw∈d
times in a document d, and Nw times in the entire dataset, we let P (d, w) =
Nw∈d
N
and P (w) =
∑
d P (d, w) =
Nw
N
. Define a document-word entropy of a document d as:
Hd(W ) = −
∑
w∈G
P (d, w) log P (w) = −
∑
w∈d
P (d, w) log P (w). (6.3)
Note that the word entropy (6.2) is additive: H(W ) =
∑
d∈D Hd(W ). The document-
word entropy Hd(W ) captures our intuition of a core document: documents that
mainly use frequent words have low Hd(W ). To see this, we factorize the joint
P (d, w) = P (d)P (w|d), and assume that all documents have a uniform prior P (d) =
1
n
. Thus, Hd(W ) is the expectation of − log P (w) according to the word frequency
P (w|d) in d, which is small if d uses a lot of frequent words.
Based on this observation, for each subset Dk of size k, we define our objective as
Dk’s contribution to the word entropy (6.2):
Hk(W ) =
∑
d∈Dk
Hd(W ) = −
∑
d∈Dk,w∈G
P (d, w) log P (w). (6.4)
We argue that the most coherent subset Dk is the one that minimizes this objective.
To find the most coherent Dk, we use the following simple, greedy Min-Entropy
algorithm:
1. Sort documents according to their word entropy portion (6.3), in increasing
order.
2. Select the first k documents. Eliminate all the rest.
94
Since our objective (6.4) is additive in documents, its global minimum is found by the
above algorithm.
We now show that this algorithm is optimal under the uniformity assumption.
Indeed, if the dataset D is large enough, then according to Theorem 6.2.1 (with high
probability) any relevant word w has a lower word-score − log P (w) than any non-
relevant word, because relevant words are more frequent in D. Since we assume that
all documents are of the same length (|d| is constant), the Min-Entropy algorithm
chooses documents that contain more relevant words than any other document in
the dataset. But this is exactly the main property of the core, as discussed in Sec-
tion 6.2. Therefore, the Min-Entropy algorithm identifies the core. We summarize
this observation in the following theorem:
Theorem 6.3.1 If the dataset D is large enough, then with high probability over
datasets, the Min-Entropy algorithm is optimal for the one-class clustering problem
under the uniformity assumption.
6.3.1 Relaxation of the uniformity assumption
In practice, distributions Pr(W ) and Pg(W ) are multinomial rather than uniform
(see illustration in Figure 6.2 right). We modify the theory presented above to this
case by exploiting the fact that entropy of a distribution can be viewed as Kullback-
Leibler (KL) divergence between this distribution and a uniform one. In place of the
entropy from Equation (6.2), we propose to use KL divergence:
KL(P ||Q) =
∑
w∈G
P (w) log
P (w)
Q(w)
=
∑
d∈D,w∈G
P (d, w) log
P (w)
Q(w)
, (6.5)
where Q(w) is an estimation of the true probability of a word occurrence in the English
language. This modification can be thought of as an adjustment of the empirical word
95
distribution in D to the uniform one. An algorithm analogous to Min-Entropy aims
at finding a subset Dk that maximizes its portion in (6.5):
KLk(P ||Q) =
∑
d∈Dk,w∈G
P (d, w) log
P (w)
Q(w)
, (6.6)
Thus, we identify the core Dk as a subset of documents containing many words that
occur in D more frequently than in general English. Following [94, 13], we exploit
Web counts of words: we estimate Q(w) as a normalized count of w in the Web. The
Web counts are obtained using Google API.
6.4 One-class co-clustering (OCCC)
As discussed in Section 4.1, co-clustering is a special case of multi-modal clus-
tering, where only two interacting modalities are considered. In the text domain,
co-clustering usually implies clustering documents D and words W , either sequen-
tially [99], or iteratively [39].
In the one-class clustering case, the co-clustering framework is interpreted as con-
structing one cluster of core documents, together with one cluster of relevant words.
The co-clustering idea has special importance for one-class clustering, as we want to
diminish the influence of non-relevant words on the process of selecting core docu-
ments. In many real-world cases, where |R| ¿ |G|, the mass of non-relevant words
in the mixture p(W ) is dominant, while only relevant words are responsible for a
document to be relevant. Reducing this mass is the goal of one-class co-clustering.
By examining Equation (6.6), it is natural to define a score of word relevance as:
s(w) = log
P (w)
Q(w)
. (6.7)
such that our objective function (6.6) is the weighted average of these scores. For
co-clustering we propose to replace the objective (6.6) with the following:
96
KLk(P ||Q) =
∑
d∈Dk,w∈R
P ′(d, w) log
P (w)
Q(w)
, (6.8)
where P ′(d, w) = P (d, w)/(
∑
w∈R P (d, w)) is a joint distribution of documents and
(only) relevant words. Because of the re-normalization introduced, it is not obvious
how to find the global optimum of the objective (6.8). We thus propose to approx-
imate it using a simple, sequential One-Class Co-Clustering (OCCC) algorithm: we
first build a cluster of relevant words based on which we build a cluster of core doc-
uments,1 as follows:
1. Sort words according to their scores from Equation (6.7), in decreasing order.
2. Select a subset R of first mr words.
3. Represent documents as bags-of-words over R (delete counts of all words from
G \ R).
4. For each document d, calculate its portion in Equation (6.8):
KLd(P ||Q) =
∑
w∈R
P ′(d, w) log
P (w)
Q(w)
=
∑
w∈R∩d
P ′(d, w) log
P (w)
Q(w)
, (6.9)
5. Sort documents according to their scores from Equation (6.9), in decreasing
order.
6. Select a subset Dk of the first k documents.
Despite its simplicity, the OCCC algorithm shows excellent results on real-world data
(see Section 6.6). The algorithm’s complexity is particularly appealing: O(N), where
N is the number of word tokens in D.
1In this simplest algorithm, word clustering is analogous to feature selection, in which selected
features correspond to only one class of the data. In more complex algorithms though, this analogy
will be less obvious.
97
6.4.1 Heuristic for choosing the size of word cluster
The choice of mr can be crucial. While not proposing a comprehensive method
for choosing mr, we propose a useful heuristic. The distribution of scores s(w) for
relevant words can be modeled by a normal distribution with mean µr À 0 and
variance σ2r . Analogously, the distribution of word scores for non-relevant words is
modeled by a normal distribution with mean µnr = 0 and variance σ
2
nr. We assume
that all the words with negative scores are non-relevant. Since the normal distribution
is symmetric, we further assume that the number of non-relevant words with negative
scores equals the number of non-relevant words with positive scores. Therefore, our
estimate of total non-relevant words is twice the number of words with negative
scores, and the number of relevant words can thus be estimated as mr = m − 2 ·
#{words with negative scores}.
6.5 The Latent Topic/Background (LTB) model
Here we revise our generative model from Section 6.2 and propose another one-
class clustering algorithm based on probabilistic inference. Our new generative model
is shown in the right panel of Figure 6.1. For each document di, Yi is a Bernoulli
random variable where Yi = 1 corresponds to di being relevant. For each word token
wij, Zij is a Bernoulli random variable where Zij = 1 means that wij is sampled from
the multinomial distribution Pr(W ) over relevant words, otherwise it is sampled from
the general multinomial distribution Pg(W ) over all words in D.
Following [53], we admit that not all words in a relevant document should be
relevant. In our model, if a document belongs to the core (Yi = 1), for each its
word we make a decision (based on Zij) whether it is sampled from Pr(W ) or Pg(W ).
However, if a document does not belong to the core (Yi = 0), each its word is sampled
from Pg(W ), i.e. P (Zij = 0|Yi = 0) = 1.
98
We use the Expectation-Maximization (EM) algorithm to learn parameters of
our model from the dataset. We now describe the model parameters Θ. First, the
probability of a document belonging to the core is denoted by P (Yi = 1) =
k
n
= pd
(this parameter is fixed and will not be inferred from data). Second, for each document
di, we maintain a probability of each its word being relevant (given that the document
is relevant), P (Zij = 1|Yi = 1) = πi for i = 1, . . . , n. Third, for each word wl|ml=1 we let
P (wl|Zl = 1) = pr(wl) and P (wl|Zl = 0) = pg(wl). The overall number of parameters
is n + 2m + 1, one of which (pd) is preset. The dataset likelihood is then:
P (D) =
n∏
i=1
[pd P (di|Yi = 1) + (1− pd)P (di|Yi = 0)] =
=
n∏
i=1

pd
|di|∏
j=1
[πipr(wij) + (1− πi)pg(wij)] + (1− pd)
|di|∏
j=1
pg(wij)

 .
At each iteration t of the EM algorithm, we first perform the E-step, where we com-
pute the posterior distribution of hidden variables {Yi} and {Zij} given the current
parameter values Θt and the data D. Then, at the M-step, we compute the new
parameter values Θt+1 that maximize the model log-likelihood given Θt, D and the
posterior distribution.
The initialization step is crucial for the EM algorithm. Our pilot experimentation
showed that if distributions Pr(W ) and Pg(W ) are initialized as uniform, the EM
results are close to random. Therefore, we borrow an idea from our OCCC model
(Section 6.4) and initialize word probabilities proportional to their relevance scores
from Equation (6.7). Initialization of πi parameters, which are the ratio of relevant
words in relevant documents, is a problem analogous to determining the word cluster
size in OCCC (see Section 6.4.1). We do not propose the optimal way to initialize πi
parameters, however, as we show later in Section 6.6, the EM algorithm appears to
be quite robust to the choice of πi, namely, πi = 0.5 (or close to that) leads to a good
result.
99
Input:
D – the dataset
s(wl) – score for each word wl|ml=1, from Equation (6.7)
T – number of EM iterations
Output: Posteriors P (Yi = 1|di,ΘT ) for each document di|ni=1
Initialization:
for each document di initialize π1i
for each word wl initialize p1r(wl) =
1
Sr
exp(s(wl)); p1g(wl) =
1
Sg
exp(−s(wl)),
where Sr and Sg are normalization factors
Main loop:
For each t = 1, . . . , T do
E-step:
for each document di compute αti = P (Yi = 1|di, Θt)
for each word token wij compute βtij = P (Zij = 1|Yi = 1, wij , Θt)
M-step:
for each document di update πt+1 = 1|di|
∑
j β
t
ij
for each word wl update
pt+1r (wl) =
∑
i α
t
i
∑
j δ(wij = wl) β
t
ij∑
i α
t
i
∑
j β
t
ij
; pt+1g (wl) =
Nw −
∑
i α
t
i
∑
j δ(wij = wl) β
t
ij
N −∑i αti
∑
j β
t
ij
Algorithm 4: EM algorithm for one-class clustering using the LTB model.
The EM procedure is sketched in Algorithm 4. We omit minor details, see Ap-
pendix B for more detailed description of the algorithm. After T iterations, we sort
the documents according to αi in decreasing order and choose the first k documents
to be the core. The complexity of our implementation of Algorithm 4 is O(TN). To
avoid overfitting, we set T to be a small number: in our experiments we fix T = 5.
6.6 Experimentation with OCCC and LTB
To define our evaluation criteria, let C be the constructed cluster and let Cr
be its portion consisting of documents that actually belong to the core. Preci-
sion is then defined as Prec = |Cr|/|C|, recall as Rec = |Cr|/k and F-measure as
(2 Prec Rec)/(Prec+Rec). In all our experiments we fix |C| = k, such that precision
equals recall and is then called one-class clustering accuracy, or just accuracy.
100
0 2000 4000 6000 8000 10000 12000
0.6
0.7
0.8
0.9
size of word cluster
ac
cu
ra
cy
 o
f d
oc
 c
lu
st
er
OCCC algorithm
0 0.2 0.4 0.6 0.8 1
0.6
0.7
0.8
0.9
EM algorithm
estimated portion of relevant words
ac
cu
ra
cy
Figure 6.4. Web appearance disambiguation. (left) OCCC accuracy as a function
of the word cluster size; (right) accuracy of LTB (with the underlying EM algorithm)
over various initializations of πi parameters: LTB shows a more robust behavior than
OCCC, however LTB’s maximal result (80.2%) is slightly inferior to the OCCC’s
(82.4%).
6.6.1 Web appearance disambiguation
The Web appearance disambiguation (WAD) task is described in Section 4.7.
WAD is a classic one-class clustering task, that was solved in that section using a
simulated one-class clustering method: multiple clusters are constructed, out of which
one cluster is then selected. Here we propose a more effective solution.
We test our methods on the WAD dataset (Section 4.7.3). The dataset consists
of the 1085 pages, out of which 420 are relevant, so we apply our algorithms with
k = 420. At a preprocessing step, we binarize document vectors and remove low
frequency words (both in terms of P (w) and Q(w)). The results are summarized in
Figure 6.4. On its left panel, the x-axis corresponds to the hypothetic number of
relevant words, and the y-axis to accuracy. The best OCCC performance is obtained
with mr = 2200 words: 82.4% accuracy, while the F-measure reported in Section 4.7.5
is 78.4% (on a cluster with less than 420 documents—its recall is only 71.3%).
As can be seen from the left panel of Figure 6.4, the OCCC performance is robust:
accuracy above 80% is obtained with a word cluster of any size in the 1000-3000
range. The heuristic from Section 6.4.1 suggests a cluster size of 1000. The right
panel of Figure 6.4 shows the LTB accuracy over various initialization values of the
πi parameter (the fraction of relevant words in core documents). We can infer from
101
# OCCC LTB # OCCC LTB # OCCC LTB
1 cheyer artificial 8 mlittman proceedings 15 gorfu kaelbling
2 kachites learning 9 hardts computational 16 billmark andrew
3 quickreview cs 10 meuleau reinforcement 17 pomdps conference
4 adddoc intelligence 11 dipasquo papers 18 ml95 markov
5 aaai98 machine 12 shakshuki cmu 19 agentus stanford
6 kaelbling edu 13 xevil aaai 20 megacanje models
7 mviews algorithms 14 sangkyu workshop
Table 6.1. Most highly ranked words by OCCC and LTB, on the WAD dataset.
this plot that LTB is even more robust to parameter initialization than OCCC: any
but very large (i.e. πi ≈ 1) values can be chosen.
Finally, Table 6.1 lists the top 20 words according to the models learned by OCCC
and by LTB. The OCCC algorithm sorts words according to their score s(w), such
that words that often occur in the dataset but rarely in the Web, are on the top of
the list. These are mostly last names or login names of researchers, venues etc. The
EM algorithm of LTB is given the OCCC’s word rank list as an input to initialize
p1r(w) and p
1
g(w), which are then updated at each M-step. In the LTB column, words
are sorted by p5r(w). The high quality of the LTB list is due to explaining away in
our generative model (via the Yi nodes). Still, OCCC (marginally) outperforms LTB
on this dataset: the maximal result obtained by OCCC is 82.4% accuracy, while LTB
obtains 80.2% accuracy.
6.6.2 Re-ranking Web retrieval results
Modern search engines are usually successful in identifying relevant documents for
a given general-type query. However, in most cases some of the top-ranked documents
have only marginal relation to the query. For example, querying Google for Beatles,
many top-ranked documents indeed talk about the quartet, however, one can see a
document about the Apple Corps vs. Apple Computer trial (which is certainly not
about Beatles), and some other clearly non-relevant documents.
102
QUERY GOOGLE OC-SVM OC-IB OCCC LTB
Godfather 0.444 0.407 0.400 0.852 0.926
Bunker Hill 0.487 0.590 0.821 0.897 0.923
Beatles 0.400 0.457 0.571 0.629 0.771
Table 6.2. Re-ranking Web retrieval results: We compare one-class clustering
accuracy of our OCCC (with heuristic from Section 6.4.1) and LTB (initialized with
πi = 0.5) models with the accuracy of the original Google rank lists, of one-class SVM
(OC-SVM) and of one-class Information Bottleneck (OC-IB) [28] with l2-norm.
In this section, we leverage the high quality of Web retrieval results and attempt to
improve them even further. Our assumption is that relevant documents are topically
close to each other, while non-relevant documents can be on any topic. We notice that
as soon as a few relevant documents appear among the n top-ranked results, we can
apply our one-class clustering methods to the task of re-ranking those results, where
the goal is to move relevant documents up in the ranked list, while moving non-
relevant ones down the list. In one-class clustering, we identify the most coherent
subset (i.e. the core) from a set of n documents. Assuming that core documents are
relevant, while non-core documents are non-relevant, we re-organize the ranked list
such that core documents are now located above non-core ones, while preserving the
initial ordering within both the core and non-core subsets.
Note that the problem of one-class clustering for re-ranking Web retrieval results
is similar to the problem of pseudo-relevance feedback (see, e.g. [104]). However,
the two problems are still fundamentally different. In pseudo-relevance feedback, one
assumes that the first k documents in a ranked list are relevant, and re-ranks the
rest of the ranked list based on that assumption. In one-class clustering, in contrast,
we make a weaker assumption that the k relevant documents exist within the first n
documents in a ranked list. Our task is then to discover those k documents and place
them on the top of the ranked list.
103
We test the resulting system on three small datasets that we created for this
chapter. Each of them contains 100 first Google hits retrieved on a certain query,
labeled as relevant / non-relevant with regards to the major meaning of the query.
These queries are:
• Godfather. While the word Godfather is ambiguous, a query Godfather most
probably refers to the popular movie/book2—other readings are considered non-
relevant. Among the set of 100 documents, 27 were annotated as relevant.
• “Bunker Hill”. The phrase Bunker Hill is not ambiguous, and a user who
types such a query is presumably interested in information about the Bunker
Hill battle and/or monument. However, some Bunker Hill mentions are not
directly related to the historical event, e.g. Bunker Hill Community College or
Bunker Hill Presbyterian Church. This dataset contains 39 relevant documents.
• Beatles. The obvious reading of the query Beatles is the name of the legendary
quartet. All the 100 first Google hits refer to the quartet, however only 35 of
them provide information about the quartet, such as their biography or discog-
raphy, while this is (almost) certainly the type of information a user expects to
retrieve on query Beatles.
We compare our methods with two previously proposed one-class clustering tech-
niques: an unsupervised one-class SVM and a one-class Information Bottleneck (see [28]
for details on those methods). Our results are shown in Table 6.2; together with the
two baselines, we list the accuracies of the original Google’s ranked lists, where the
first k documents are considered the core, while the rest of n− k documents are con-
sidered the noise. Our methods clearly outperform the baselines, while LTB shows
better performance than OCCC.
2According to imdb.com, The Godfather is the world’s most popular film to date.
104
6.6.3 Detecting the topic of the week
As we discussed in this chapter’s introduction, the real-world data rarely consists
of a clean core and uniformly distributed noise. Usually, the noise has some structure,
namely, it may contain coherent components. With this respect, one-class clustering
can be used to detect the largest coherent component in a dataset, which is an integral
part of many applications. In this section, we solve the problem of automatically
detecting the topic of the week (TW) in a newswire stream, i.e. detecting all articles
in a weekly news roundup that refer to the most broadly discussed event.
The TW detection task can be considered as a subtask of Topic Detection and
Tracking (TDT) [2], and is closely related to:
• Generating topic overviews [103]. A topic overview is a set of keywords that
best describe the discussed topic. Using the one-class clustering terminology,
such set is the cluster of relevant words. In our OCCC approach, we generate
both a subset of core documents and a subset of relevant words. In LTB, we
rank documents and words according to their likelihood of belonging to the
core.
• Discovering thematic changes [103, 52]. Major topics (represented both as
subsets of documents and as their descriptive words) are changing with time.
In our work, we deal with those changes by discretizing the timeline into weeks.
A topic that was most broadly discussed one week, may or may not remain so
the next week.
• Quantifying trends [44]. The trend quantification task aims at discovering
how large a certain topic is, without necessarily mapping documents to topics.
In TW detection, however, the task is to discover which topic is the largest one.
Also, trend quantification is an intrinsically supervised task, while TW detection
can be formulated both in terms of supervised and unsupervised learning.
105
We evaluate the TW detection task on the TDT-5 dataset3, which consists of 250
news events spread over a time period of half a year, and 9,812 documents In English,
Arabic and Chinese (translated to English), annotated by their relationship to those
events.4 The largest event in TDT-5 dataset (#55106, titled “Bombing in Riyadh,
Saudi Arabia”) has 1,144 relevant documents, while 66 out of the 250 events have only
one relevant document each. We split the dataset to 26 weekly chunks (to have 26
full weeks, we delete all the documents dated with the last day in the dataset, which
decreases the dataset’s size to 9,781 documents). Each chunk contains from 138 to
1292 documents. Over each chunk, we applied our one-class clustering methods in
four setups:
• OCCC with the mr heuristic (from Section 6.4.1).
• OCCC with optimal mr. We unfairly choose the number mr of relevant words
such that the resulting accuracy is maximal. This setup can be considered as the
upper limit of the OCCC’s performance, which can be hypothetically achieved
if a better heuristic for choosing mr is proposed.
• LTB initialized with πi = 0.5. As we show in Sections 6.6.1 and 6.6.2 above,
if πi parameters are initialized with 0.5, the LTD model shows good results.
• LTB initialized with πi = pd. We notice a significant deviation in the core’s
size among our 26 datasets. Quite naturally, the number of relevant words in
a dataset depends on the number of core documents. For example, if the core
is only 10% of a dataset, it is unrealistic to assume that 50% of all words are
relevant. In this setup, we condition the ratio of relevant words on the ratio of
core documents.
3http://projects.ldc.upenn.edu/TDT5/
4We take into account only labeled documents, while ignoring unlabeled documents that can be
found in the TDT-5 data.
106
1 2 3 4 5 6 7 8 9 10 11 12 13
0
0.2
0.4
0.6
0.8
1
week
ac
cu
ra
cy
Performance of OCCC and LTB on the "topic of the week" task
14 15 16 17 18 19 20 21 22 23 24 25 26
0
0.2
0.4
0.6
0.8
1
week
ac
cu
ra
cy
 
 
OCCC with the m
r
 heuristic
OCCC with the optimal m
r
LTB initialized with π
i
 = 0.5
LTB initialized with π
i
 = p
d
Figure 6.5. One-class clustering results on the “topic of the week” detection task.
One-class clustering accuracies per week are shown in Figure 6.5. These results
reveal very interesting observations. First, OCCC methods tend to outperform LTB
only on datasets where the results are quite low in general (less than 60% accuracy).
Specifically, on weeks 2, 4, 11 and 16 the LTB models demonstrates extremely poor
performance. While investigating this phenomenon, we discovered that in two of the
four cases LTB was able to construct very clean core clusters, however, those clusters
corresponded to the second largest topic rather than to the largest one. For example,
on the week-4 data, topic #55077 (“River ferry sinks on Bangladeshi river”) was
107
Method Accuracy
OCCC with the mr heuristic 61.4± 4.5%
OCCC with optimal mr 68.3± 3.6%
LTB initialized with πi = 0.5 65.3± 7.3%
LTB initialized with πi = pd 68.0± 5.9%
Table 6.3. One-class clustering accuracy on the “topic of the week” de-
tection task. The accuracies are macro-averaged over the 26 weekly data chunks.
Standard error of the mean is presented after the ± sign.
discovered as the largest and the most coherent one. In that dataset, topic #55077
is represented by 20 documents, while topic #55063 (“SARS Quarantined medics in
Taiwan protest”) is represented by 27 documents, such that topic #55077 is the second
largest one. Another interesting observation is that the (completely unsupervised)
LTB model can obtain very high results on some of the data chunks. For example,
on weeks 5, 8, 19, 21, 23, 24, 25 LTB’s accuracy is above 90%, with a striking 100%
on week-23.
The one-class clustering accuracies, macro-averaged over the 26 weekly chunks,
are presented in Table 6.3. As we can see, both LTB models outperform the OCCC
variation where the mr heuristic is applied. Moreover, even the optimal choice of
mr does not cause OCCC to perform significantly better than LTB. The dataset-
dependent initialization of LTB’s πi parameters (πi = pd) appears to be preferable
over the dataset-independent one (πi = 0.5).
6.7 Summary
We have addressed the problem of inducing objective functions in Comraf models.
For the task of one-class clustering, we proposed an information-theoretic and a proba-
bilistic objective functions, as well as algorithms for their optimization. The proposed
algorithms are very simple, very efficient and still surprisingly effective. More sophis-
ticated algorithms (e.g. better optimization of the objective function in OCCC) are
108
emerging. Also, since the Comraf framework allows straightforward generalization
of OCCC to one-class clustering with many modalities, it will be interesting to see
whether one-class clustering results can be improved by adding more modalities, such
as author names or hyperlinks.
Our evaluation of one-class clustering models on the re-ranking task is preliminary.
It gives positive signals in the Web search case, where queries are of the general type
and unlikely to be ambiguous. Also, one-class clustering is likely to be useful in Topic
Detection and Tracking. However, our pilot experimentation in the ad-hoc retrieval
domain shows rather negative results. In ad-hoc retrieval, our main assumption that
the noise has no or little structure is generally wrong. For example, querying TREC
1 and 2 data for acid rain, the majority of 1000 retrieved documents are actually
weather reports, most probably because all the other documents in the collection are
even less relevant. Since one-class clustering methods do not take the query into
account, and since the weather reports’ subset is the largest and the most coherent
one in the set of retrieved documents, our re-ranking hurts the ranking results on that
query. Evaluating one-class clustering methods on other related tasks is the subject
of our future work.
109
CHAPTER 7
IMAGE CLUSTERING WITH COMRAFS
In this chapter, we revise the Comraf clustering mechanism, proposed in Chap-
ter 4. Based on the concept of observed combinatorial random variables (discussed
in Chapter 5), we adapt the Comraf model to the case where the data consists of
both sparse modalities (which need to be clustered) and dense modalities (not to be
clustered). We also generalize the Comraf clustering objective function, making it
more flexible and adjustable to a variety of real-world tasks. These two innovations
finalize the development of the Comraf framework toward giving a comprehensive
recipe for modeling with Comrafs, which we present in Chapter 8.
We focus here on multi-modal clustering of image collections, particularly of those
where images are associated with textual captions.1 Besides the caption words’ modal-
ity, we consider visual modalities, both global (such as colors, texture) and local
(regions, blobs). For details, see Section 7.4 below. Image clustering can be a use-
ful component in a retrieval system [26], it can also be a stand-alone application,
for example, for constructing semantic groups of image retrieval results [108], or for
browsing image collections [5]. Unfortunately, existing uni-modal clustering methods
often demonstrate poor performance on the image clustering task. In this chapter,
we show that by employing the multi-modal learning paradigm we can significantly
improve image clustering results.
Multi-modal clustering of images has an important difference when compared to
multi-modal clustering of documents. Document features, such as words, POS tags
1A preliminary version of this work [12] was published at CVPR 2007.
110
etc. are situated in a discrete, finite space. Two textual features can be either identical
or not. Some visual features, in contrast, are unique. These are local features, such
as interest points [91], image regions [56] etc. An affinity metric should be defined to
estimate similarity of those features. We find at least two disadvantages in working in
the affinity space. First, the choice of the affinity metric is often arbitrary. Second, the
affinity metric is defined for each pair of data points, which makes the computational
complexity of related clustering algorithms quadratic in the best case. In this thesis,
we aim at avoiding the explicit definition of the affinity metric (see Section 7.4.2).
7.1 Related work
The idea of clustering images using both low-level image features and surrounding
text (i.e. grouping together visually similar and semantically related images) has
attracted close attention of the research community. Barnard et al. [5] propose a
generative hierarchical model for image clustering, in which every node generates
words and blobs based on the given probability distributions for that node. Higher
level nodes generate more general terms and lower level nodes generate more specific
terms. The EM algorithm is used to fit the model. This approach can handle only
two feature types (words, blobs); to handle more types, the model and the learning
procedure must be revised.
Cai et al. [25] cluster Web image search results using visual, textual and link anal-
ysis. They extract text relevant to the image using a vision-based page segmentation
algorithm. First, only text and hyperlink data is used to cluster images. The resulting
clusters are clustered again using low-level image features. Loeff et al. [73] apply a
similar approach: they calculate a histogram of gradient magnitude of the pixel val-
ues from every interest point and then cluster images using these local features with
global color histograms and surrounding text. Both Cai et al. and Loeff et al. use
spectral clustering methods (where the affinity scores for every pair of data instances
111
of every modality must be calculated), which are computationally infeasible in many
other multi-modal applications.
Bipartite spectral graph partitioning [34] is useful for co-clustering two modalities
such as documents and words. Gao et al. [47] extend this method to handle one
more modality. In their tripartite graph model, nodes are arranged in three layers:
words, images and image features. To handle more modalities, Gao et al. [48] propose
another method that is most closely related to our work: they organize modalities in
a star structure of interrelationships, where a central modality is connected to all the
others. They treat this problem as fusion of multiple pairwise co-clustering problems.
Each sub-problem is solved using the bipartite graph partitioning method.
Our approach has a few advantages over the others. First, our method has no
practical limitation in the number of modalities as long as the pairwise interaction
data is available—the addition of a modality increases the computational complexity
only linearly. Second, our model can cluster multiple modalities while taking into
account other modalities, which do not have to be clustered. Third, our information-
theoretic clustering method does not rely on hard-to-obtain affinity matrices of in-
dividual modalities. Instead, easily computable contingency tables of interacting
modalities are used. Overall, we propose a general framework for clustering multime-
dia collections, which can be straightforwardly applied to video data, sound tracks,
hypertext etc. as well as to any of their combinations.
7.2 Multi-modal clustering objective, revisited
In Section 4.1, we proposed an objective function for multi-modal clustering as
the sum of pairwise Mutual Information between interacting clusterings:
xc∗ = arg max
xc
P (xc) = arg max
xc
∑
(Xci ,X
c
i′ )∈E
I(X̃i; X̃i′),
112
subject to |X̃i| = ki, where i = 1, . . . , m. In Section 3.3 we discussed one disadvantage
of a global objective function like that: Mutual Information terms can significantly
vary in their magnitude, dependently on the support size of corresponding variables.
Summing these terms together can cause an undesired effect of artificial preference
of some interactions over the others. A natural generalization of this objective would
be to consider a weighted linear combination of pairwise Mutual Information terms:
xc∗ = arg max
xc
∑
(Xci ,X
c
i′ )∈E
βii′I(X̃i; X̃i′), (7.1)
where the weights βii′ are chosen using some domain knowledge. An obvious choice
of the weights is such that all the Mutual Information terms are to be brought to the
same scale. Another factor for choosing the weights is to make them correspond to
various importance levels of various interactions. For example, if images are clustered
based on their captions and their color histograms, the images/captions interaction
can have a heavier weight than the weight of the images/colors interaction.
In some cases, weights βii′ can be adjusted during the course of an inference
algorithm, in an annealing framework. Also, the weights can be learned using a
model learning procedure. Both these extensions are left for our future work.
7.3 Comraf*: a lightweight version of the Comraf model
In previous chapters we made it obvious that, in most real-world situations, a
practitioner is interested in clustering only one modality (images, in our case), which
we call here a target modality. This implies that not every modality has to be
clustered: if a representation of a modality is dense enough, clustering it may cause
an underestimation of the joint (an effect known as oversmoothing), which may hurt
clustering results of the target modality. For example, if images are distributed over
113
G
c
c
W
G
c
C
c
W
c
c
G
c
B
c
C
c
W
c
G
c
B
c ?
C
c
W
(a) (b) (c) (d)
Figure 7.1. Comraf* models: (a) for images Gc and words W c from their captions;
(b) for images, words and colors Cc; (c) for images, words, colors and blobs Bc; (d)
straightforward generalization to any number of modalities.
256 colors, it makes no sense to simultaneously cluster images and colors because the
distributions are already dense enough.
In this section, we propose a special case of Comraf models, in which only the tar-
get modality is clustered, while the representations of all the other modalities are as-
sumed to be dense enough. Each unclustered modality is associated with an observed
combinatorial random variable. Recall that a combinatorial random variable is de-
fined over all the possible clusterings of a given set. In case of unclustered modalities,
the observed value of a corresponding combinatorial random variable is a clustering
of all singleton clusters. For example, given a set {red, green, blue}, the observed
value of a corresponding combinatorial random variable is {{red}, {green}, {blue}}.
Each observed combinatorial random variable of an unclustered modality is con-
nected by an edge with a hidden combinatorial random variable of the target modality.
Observed nodes are not connected to each other because they are statistically indepen-
dent by definition. Hence, the resulting topology of the Comraf model is an asterisk
with the target modality in the center. We call such a model Comraf*. Examples of
Comraf* graphs are given in Figure 7.1. Even though only one modality is clustered
in Comraf*, it is still a model for multi-modal clustering, as multiple modalities are
involved in the clustering process.
114
Recall that in Chapter 4 we considered Comraf models for multi-modal clustering
where each modality should be clustered. In Comraf* models only one modality is
clustered. The general Comraf model, however, takes care of any number of dense
and sparse random variables. In Section 7.4.2 we present a Comraf model for si-
multaneously clustering images and their local features, while incorporating other
(unclustered) modalities. Since the simultaneous clustering can be computationally
hard, we also show how to reduce the computational burden by translating such
a Comraf model into a series of Comraf* models, each of which is then optimized
separately.
7.3.1 Inference in Comraf*
In Comraf*, where all the edges are attached to Xc0 and all the leaves are observed
combinatorial random variables, Equation (7.1) is transformed into:
xc∗0 = arg max
xc
m−1∑
i=1
βiI(X̃0; X̃i) = arg max
xc
m−1∑
i=1
βiI(X̃0; Xi), (7.2)
since X̃i = Xi for the unclustered modalities. As always, we have the |X̃0| = k
constraint.
To compute the weighted sum of pairwise mutual information from Equation (7.2),
the following procedure is used. The input of the procedure is an (empirical) joint
distribution P (X0, Xi) of the underlying data of each interacting pair (X
c
0, X
c
i ). For
a given partitioning xc0, the distribution P (X̃0, Xi) is computed using the cumulative
rule P (x̃0; xi) =
∑
x0∈x̃0 P (x0, xi). Marginals P (X̃0) and P (Xi) are obtained through
the marginalization P (x̃0) =
∑
xi
P (x̃0, xi) and P (xi) =
∑
x̃0
P (x̃0, xi). Now we have
all the ingredients to calculate the mutual information:
I(X̃0; Xi) =
∑
x̃0,xi
P (x̃0, xi) log
P (x̃0, xi)
P (x̃0)P (xi)
.
115
To perform an inference in Comraf*, we apply a version of our MDC algorithm
(see Section 4.3), where either top-down or bottom-up clustering procedures is used
for clustering X0. In the top-down procedure, we start with one cluster that contains
all the values of X0 and split it until the required number of clusters is obtained (while
interleaving with the optimization routine). In bottom-up clustering, we start with
all singleton clusters and merge them until, again, reaching the required number of
clusters.
The computational complexity of the top-down algorithm is O(l|X0|
∑m−1
j=1 |Xi|),
and of the bottom-up algorithm O(l|X0|2
∑m−1
j=1 |Xi|), where l is a (fixed) number of
clustering iterations. Note that an arbitrary number of leaves (unclustered modalities)
can be incorporated into the Comraf* model, while adding new modalities increases
the complexity only linearly.
7.4 Modalities of an image collection
In this work, along with images, we consider three other modalities. The first
one is words from image captions. We remove stopwords and apply a simple ‘s’-
stemming (removal of plural suffixes). A joint probability of an image g and a word
w is P (g, w) = Nw∈g|W | , where Nw∈g is the number of occurrences of w in g’s caption, |W |
is the total number of words. Another modality is colors appearing in images. The
joint probability distribution of colors and images is obtained from color histograms,
as a number of pixels of color c in image g divided by the total number of pixels in
all images. The third modality is blobs, as described below.
7.4.1 Rectangular blobs
Blobs (or visual terms) are a special type of image content representation based on
a fixed vocabulary. To generate blobs, images are first segmented into regions, which
are then clustered across all images. Blobs are the resulting region clusters. Each
116
image is mapped onto the set of blobs which leads to in a representation analogous
to the bag-of-words (BOW) in text processing.
Barnard and Forsyth [6] and Duygulu et al. [37] segment images into semantically
coherent regions using Blobworld and Normalized-Cuts algorithms, respectively. Un-
fortunately, these algorithms do not always produce segmentations accurate enough
for further use. Jeon and Manmatha [56] and Feng et al. [41] use a rectangular grid
to segment images and report better results on an image retrieval task. We apply
the same set of blobs as in [41], built using the following procedure. Images are first
segmented to regions using a 6 × 4 grid. Then, for each region, a feature vector is
constructed that contains texture and color information: Gabor texture filters with
4 orientations and 3 scales are used to construct 12 dimensional texture features; the
mean, standard deviation and skewness of RGB and LAB components are computed
to build 18 dimensional color features. The resulting 30 dimensional feature vectors
are clustered using k-means.
7.4.2 Blobs constructed by Comraf models
As discussed in Section 7.4.1 above, a clustering process is involved in constructing
blobs from rectangular regions, represented by color and texture features. Naturally,
since Comrafs are models for multi-modal clustering, an intrinsic Comraf model can be
used for simultaneously clustering images and their regions. Co-clustering of images
and features has been recently described in literature [89], however, Comrafs have
an additional power over co-clustering methods: Comrafs can incorporate multiple
modalities, both sparse (that are to be clustered) and dense (that are not).
Figure 7.2 (left) shows a Comraf model for clustering images G simultaneously
with their regions R, taking into account color C and texture T information of the
regions, as well as the colors and caption words W of the images. Obviously, more
edges and nodes can be added to the model, depending on the data’s availability.
117
G
c
T
c
c
W
c
R
c
C
2.
G
c
1.
c
c
R
c
R
T
C
c
c
W
c
C
Figure 7.2. (left) A Comraf model for simultaneously clustering images Gc and their
rectangular regions Rc, while taking into account words W c from image captions,
colors Cc and texture data T c; (right) a translation of this model into a two-step
Comraf*: the first Comraf* is for clustering regions into blobs, whereas the second
Comraf* is for clustering images based on these blobs.
In Section 7.3.1 we mentioned that the input of a Comraf inference procedure is
a set of pairwise probability tables P (Xi, Xi′) for each edge in the Comraf graph. An
interesting case is the (Gc, Rc) edge between image and region combinatorial random
variables in Figure 7.2 (left). Unlike colors and caption words, each region is unique,
so for each region r and each image g, their joint probability is P (r, g) = 1|R| if r ∈ g,
and 0 otherwise (where |R| is the total number of regions in the dataset). Such a
probability mass function is useless for clustering regions, because only regions that
belong to the same image can be clustered together. A possible way to resolve this
problem would be to estimate this probability by giving a portion of its mass to
P (r, g) even if r /∈ g. Such an estimation can be made based on computing an affinity
metric between regions of various images, which is computationally hard: O(|R|2|r|),
where |r| is the size of any region.
Comrafs offer an elegant solution to this problem: since regions are clustered not
only based on images, but also based on colors and texture, neither of which has
this problem, we still can use our objective function from Equation (7.1). As long as
images are clustered in parallel with regions, Equation (7.1) allows grouping together
regions that belong to the same image cluster, as desired. Therefore, we apply the
Comraf model from Figure 7.2 (left) as it is. We choose to cluster images bottom-up
118
and regions top-down. In our objective (7.1), we cope with the fact that I(R̃; G̃) is
two orders of magnitude larger than I(R̃; C̃) and I(R̃; T̃ ), by setting the weights of
the latter two terms to 100.
The simultaneous clustering of images and regions is a time consuming process:
its complexity is O(|R| |G| (|C| + |T | + |W |)). We propose a light-weight version of
this model, in which inference is done in two steps : first, regions are clustered based
on their color and texture features, and then images are clustered based on colors,
caption words and region clusters. Such a model is equivalent to two Comraf* models
applied one after another, as presented in Figure 7.2 (right). This model’s complexity
is plausible: O(|R| (|C|+|T |)+|G| (|C|+|R̃|+|W |)), where |R̃| is the number of region
clusters. Moreover, in Section 7.5.2 we show that the performance of the two-step
Comraf* is plausible as well: on one of our datasets, it obtains clustering accuracy
comparable to the one of a general Comraf. Generalizing the two-step setting, it is
easy to see that any Comraf can be translated into a series of Comraf* models.
7.5 Experimentation
We experiment with a variety of particular Comraf* models (see examples in Fig-
ure 7.1), as well as with the general Comraf models from Figure 7.2. The experiments
are conducted using our open-source Comraf clustering tool.2 In all our models, im-
ages are clustered agglomeratively. All our results are averaged over 10 independent
runs, with the standard error reported. As a baseline, we use the k-means algorithm
(SimpleKMeans implementation of WEKA3), where images are represented as BOW
of their captions. Also, our 2-node Comraf* model is equivalent to the hard-clustering
version of Information Bottleneck (IB) [106] (see Section 4.1 for discussion), hence
2http://sourceforge.net/projects/comraf
3http://cs.waikato.ac.nz/ml/weka
119
Category # of images Category # of images
Birds 152 Christianity 191
Desert 172 Islam 96
Flowers 165 Judaism 187
Trees 190 Personalities 188
Food 187 Symbols 130
Housing 165 OVERALL: 1823
Table 7.1. Categories (and their sizes) of the IsraelImages dataset.
we use it as our baseline as well. For evaluation of our clustering results, we use
micro-averaged accuracy (Section 4.6.1) of the constructed image clustering.
7.5.1 Datasets
We demonstrate the performance of our clustering methods on two datasets: a
subset of the benchmark Corel dataset and a new multimedia dataset, which we refer
to as IsraelImages , collected by us especially for this work.
The Corel subset4 has already been used in various previous research projects
[37, 55, 41]. The dataset consists of 5,000 images from 50 Corel Stock Photo CDs.
Each CD contains 100 images on the same topic, such as “Sunrises and Sunsets”,
“Mountains of America” and “Wild Animals”. Every image has a caption and an
annotation. The caption is a brief description of the scene and the annotation is a list
of objects that appear in the image. An example of an image caption is “Man And Boy
Fishing Mountain Lake”, while “Tree People Mountain Water” is an annotation for
this image. Overall 371 words are used to annotate the collection. The original dataset
has 4,500 training images and 500 test images. Since our model does not require
training, we use 4,500 training images for our experiments and save the remaining
500 images for future use.
4http://kobus.ca/research/data/eccv_2002
120
Method Accuracy
k-means: images over caption words 22.0%
IB: images/caption words 44.2± 1.0%
IB: images/colors 24.4± 0.2%
Comraf*: images/words/colors 54.2± 0.9%
General Comraf: Figure 7.2 (left) 68.6± 1.0%
Two-step Comraf*: Figure 7.2 (right) 69.0± 0.6%
Table 7.2. Micro-averaged clustering accuracy on IsraelImages. All
IB/Comraf results are averaged over 10 independent runs with the standard error
of the mean reported after the ‘±’ sign.
The second dataset consists of 1823 images downloaded from IsraelImages.com.
The images reflect different aspects of Israel scenery and/or society and are grouped
into 11 categories (see Table 7.1). Each image is 375 by 250 pixels and has a 1 to 18
word long caption. This dataset is available to the research community.5
7.5.2 Comparative results
Our results on the IsraelImages dataset are reported in Table 7.2. Adding the
color modality to the caption BOW improves the clustering result by 10% (on an
absolute scale), whereas adding the regions (in a 2-step Comraf* scheme) leads to an
additional 15% improvement. These findings demonstrate the value of multi-modal
setting in image clustering. The general Comraf model from Figure 7.2 (left) is not
able to outperform the 2-step Comraf*. This is probably due to the fact that color and
texture information is more important for clustering regions than the correspondence
between regions and image clusters.
We also experiment with various levels of color granularity in a 3-node Comraf*
setting (from Figure 7.1b)—the results are presented in Figure 7.3 (left). As can be
seen, if the color information is detailed enough (above 216 colors), the difference in
the results is statistically insignificant. Figure 7.3 (center) shows the results of the
5http://www.cs.umass.edu/~ronb/image_clustering.html
121
Method Accuracy
k-means: images over caption words 22.0%
IB: images/caption words 46.6± 0.5%
IB: images/colors 22.5± 0.2%
IB: images/blobs (see Section 7.4.1) 24.7± 0.3%
Comraf*: images/words/colors 55.3± 0.5%
Comraf*: images/words/blobs 59.4± 0.5%
Comraf*: images/words/colors/blobs 60.1± 0.3%
Two-step Comraf*: Figure 7.2 (right) 61.2± 0.4%
IB: images/annotation words 58.6± 0.3%
Table 7.3. Micro-averaged clustering accuracy on Corel. All IB/Comraf
results are averaged over 10 independent runs with the standard error of the mean
reported after the ‘±’ sign.
2-step Comraf* over various numbers of colors for clustering regions. Generally, less
colors are needed for clustering regions than for clustering images: 216 colors appear
to be too many.
A summary of our results on the Corel dataset is presented in Table 7.3. It
shows surprisingly similar trends as for IsraelImages. On a 3-node setup with caption
words and blobs we obtain 59.4% accuracy, which is especially impressive given that a
random assignment of images into 50 clusters would lead to 2% accuracy (our result
is 30 times above random). Adding the color modality improves this result only
insignificantly (as expected, since blobs already incorporate the color information,
among with texture). The success of 3-node and 4-node Comraf* clustering models
is also supported by the fact that they outperform a 2-node supervised clustering
model, in which images are clustered with respect to their annotations assigned by
human experts.
The 2-step Comraf* shows some further (minor) improvement over the 1-step
Comraf* models. Here, in contrast to IsraelImages, 8 colors are enough for clustering
regions, and adding more colors causes a significant drop in the performance. We
suspect that the Corel dataset is “too simple”: it contains many images that are
122
27 125 216 343 512 729 1000
0.4
0.45
0.5
0.55
0.6
0.65
0.7
number of colors for image clustering
ac
cu
ra
cy
Clustering accuracy on IsraelImages
 
 
images/words
images/words/colors
8 27 64 216
0.4
0.45
0.5
0.55
0.6
0.65
0.7
number of colors for region clustering
ac
cu
ra
cy
Clustering accuracy on IsraelImages
 
 
images/words
2−step Comraf*
5001000 2000 3000 5000
0.4
0.45
0.5
0.55
0.6
0.65
0.7
number of blobs
ac
cu
ra
cy
Clustering accuracy on Corel
 
 
images/words
images/words/blobs
Figure 7.3. Experimentation with various numbers of: (left) colors on IsraelIm-
ages in a 3-node images/words/colors Comraf*; (center) colors for clustering re-
gions in the 2-step Comraf* on IsraelImages; (right) blobs on Corel in a 3-node
images/words/blobs Comraf*. Our baseline is the 2-node images/words clustering
result. Left and right graphs show the same trend: after reaching a certain number of
colors (256) or blobs (2000), the results vary only insignificantly. The central graph,
however, shows that too many colors for clustering regions can hurt.
almost identical to each other, therefore more advanced clustering models lead to no
(or minor) gain over the simpler ones.
Analogously to our IsraelImages experiment with various sizes of color sets, we
test various numbers of blobs on Corel. In previous work [37, 55], the number of
blobs is set to 500, to (roughly) correspond to the number of annotation keywords.
Here we show that 500 blobs are not enough for clustering: when moving from 1000
to 2000 blobs, a significant boost in the system’s performance can be seen.
Figures 7.4 and 7.5 are illustrations of the quality of multi-modal setup: unrelated
groups of images are mixed together when the clustering is based only on caption
words, whereas they are nicely separated when a visual modality is added.
7.6 Summary
In this chapter, we have introduced the Comraf framework for clustering multime-
dia collections. We have also proposed a family of lightweight Comraf models called
Comraf*, which demonstrate excellent performance on clustering two real-world im-
age collections. To further improve the image clustering results, a semi-supervised
123
Comraf setting (see Chapter 5) can be used, in which a few labeled examples are
taken into account in the clustering process. We plan to experiment with this setting
in our future work.
Designing general Comraf models for image clustering (in the flavor of the model
shown in Figure 7.2 left) is an ongoing process. Extensive experimentation will lead
to discovering the optimal Comraf setting for clustering multimedia collections.
124
(a) Clustering results using only caption words, Corel dataset
(b) Clustering results using words and blobs, Corel dataset
Figure 7.4. Corel dataset. The first row shows clustering results using only words.
Swimmers and swimming tigers are clustered together because they share common
terms like “water” and “swim”. The second and the third rows show clustering results
using both words and blobs. The swimmers and the swimming tigers are now in two
different clusters with other similar images.
(a) Clustering results using only caption words, IsraelImages dataset
(b) Clustering results using words and color histograms, IsraelImages dataset
Figure 7.5. IsraelImages dataset. People portraits and pictures of the menorah mon-
ument are clustered together using caption words because they have a word ‘Knesset’
(the Israeli parliament) in common: the individuals are Knesset members, while the
menorah monument is placed in front of the Knesset building. The problem is resolved
after the color modality is added.
125
CHAPTER 8
CONCLUSION
In this thesis we have introduced Combinatorial Markov Random Fields (Comrafs)—
a novel, generic framework for statistical modeling that consists of three basic com-
ponents:
1. An undirected graph with nodes being statistical objects of “rich” structure and
edges being interactions between those objects;
2. An objective function (either probabilistic or non-probabilistic) factored over
the graph;
3. A method for optimizing the objective.
We have applied the Comraf framework to multi-modal learning, which is a learning
problem in the environment where multiple views (or modalities) of the data are
available. Based on the material presented in previous chapters, we can give an
ultimate recipe for solving multi-modal learning problems with Comrafs:
1. Come up with a few modalities for a particular dataset. In most cases, it is
easy to come up with two modalities: one for data instances, another for their
features. Once a few data types or feature types are available, they can be
represented as modalities. Note that a modality is a set over which a proba-
bility distribution can be defined. Comrafs are unlikely to be useful in cases
where data instances are represented as feature vectors, where each feature is
intrinsically different from the others (e.g. where feature vectors consist of four
126
features: color, size, temperature, and price, such that it is difficult to define a
probability distribution over this feature set).
2. Decide which modalities interact with each other. This decision should be made
upon availability of contingency information for each pair of modalities, as well
as based on domain knowledge (e.g. whether or not it is natural to see these
modalities interacting). For example, say we are given three modalities images,
their colors, and their caption words. Captions definitely interact with images,
as well as colors interact with images. However, we can assume that captions do
not interact with colors, as it is a very rare case where captions directly describe
colors in an image. Note that a decision about presence / absence of interactions
is analogous to defining conditional independencies in other types of graphical
models: the number of interactions should be kept as low as possible in order
to keep the model tractable.
3. For a particular learning task, decide which modalities should be optimized and
which should be observed. Observed modalities usually provide some level of
supervision to the model: using observed modalities, prior knowledge can be
represented. Also, a modality can be observed if its size is very small, such that
a distribution defined over this modality is statistically dense.
4. Represent those modalities that are to be optimized as hidden combinatorial
random variables, and those that are not as observed combinatorial random vari-
ables. A combinatorial r.v. can be defined over a set of possible partitionings,
subsets, partial orderings of a modality, or over other types of combinatorial
sets, according to the particular problem being solved.
5. Represent each combinatorial r.v. as a node in a graph in which undirected
edges correspond to interactions. We now have finished building the Comraf
graph.
127
6. Represent the learning task as optimization of an objective function that is de-
fined over nodes and edges in the Comraf graph. Choose the objective function
that best suits the task. The choice of objective function can be made based on
previously published work in the field (as in Chapter 4), or based on theoretical
analysis (as in Chapter 6), as well as based on some pilot research or other
considerations.
7. Since optimizing the objective function simultaneously over the entire Comraf
graph appears to be intractable, propose a method for traversing the Comraf
graph in order to perform iterative optimization of the objective. In this thesis,
we have discussed two such methods: Iterative Conditional Modes (ICM) and
Clique-wise Optimization (CWO)—see Section 3.3. ICM can be considered a
global optimization method, as the objective is optimized at each node condi-
tionally on the rest of the model. In contrast, CWO is a local optimization
method, as the objective is optimized over each clique independently of the rest
of the model.
8. At each node / clique, apply a combinatorial optimization method for optimizing
the objective. In Section 4.3, we proposed two simple and greedy combinatorial
optimization methods (sequential and shuffled), both of which explore the local
neighborhood of an initial configuration. More sophisticated methods, such as
Branch and Bound, can be used as well.
9. As the global optimum of the objective function is unlikely to be found, propose
a stopping criterion of the optimization procedure. In our experimentations
with multi-modal clustering, we halted the optimization procedure as soon as
the desired number of clusters was achieved.
We applied the proposed framework to multi-modal clustering (Chapter 4), semi-
supervised learning (Chapter 5), and one-class clustering (Chapter 6). Both text and
128
image domains (Chapter 7) were explored. Three important issues have not been
addressed in this thesis—we leave them for our future work:
• Multi-modal ranking, which is another application of the Comraf framework.
In the multi-modal ranking problem, one simultaneously ranks a number of
modalities, given rankings of the other modalities. One example of multi-modal
ranking comes from the data mining / collaborative filtering area: given a
ranking of movies, the task is to simultaneously rank its directors and actors
who starred in those movies. The goal of such system would be to adequately
measure popularity of celebrities. Another example comes from image retrieval:
given a ranked list of documents, retrieved on a certain query, the task is to
simultaneously rank images in these documents and their local features (blobs
or interest points). Our intuition here is that the simultaneous ranking would
improve the quality of image ranked lists. Note that the layout of Comraf graphs
for multi-modal ranking is the same as for multi-modal clustering. However,
the objective function and optimization method should be specific for the multi-
modal ranking task.
• Scalability issue in Comrafs. Unfortunately, the current version of our MDC
implementation for multi-modal clustering is very slow. Given that each opti-
mization iteration is repeated ten times (i.e. ten random restarts), a straightfor-
ward enhancement would be to perform those random restarts in parallel on ten
machines. Another possible enhancement would be to limit the search length
in the shuffled version of MDC, or to parallelize the shuffling steps using the
MapReduce paradigm [33].
• Model learning in Comrafs. It turns out that the main factor for achieving
good clustering results with Comraf models is the good choice of modalities and
129
their interactions. It is desired to construct a system that could a priori decide
whether the available modalities would be helpful or harmful.
In addition to being a useful framework for multi-modal learning, Comrafs can go
beyond it: Comraf nodes do not necessarily have to represent data modalities. Also,
random variables of rich structure may not necessarily be of the combinatorial nature,
so Comrafs have good potential for a generalization into a new framework. We call it
Non-Bayesian Networks (NBN). Development of such a framework is also the subject
of our future work. An interesting question yet to be answered is how to model NBN’s
nodes (which are structurally rich) in a finer-grained manner. A possible answer is
to use a lower-level NBN at each node of the (upper-level) NBN, by which we build
a telescopic model. Constructing such a model would resemble designing an object-
oriented software system, which has a direct connection with the power framework of
Object-Oriented Bayesian Networks [61]. Developing Object-Oriented Non-Bayesian
Networks would be the final goal of this research.
To conclude, the contributions of this thesis are:
1. Proposing Comrafs—a novel framework for statistical modeling that brings to-
gether two research fields: graphical models and combinatorial optimization.
2. Applying this framework to a variety of problems in multi-modal learning, such
as multi-modal clustering, semi-supervised clustering, interactive clustering,
one-class clustering etc.
3. Proposing model layouts, objective functions and optimization procedures for
each of these problems.
4. Showing empirical advantage of Comrafs over previous state-of-the-art methods
on various real-world tasks, such as Web appearance disambiguation, document
clustering by genre and author’s sentiment, organization of image galleries etc.
130
BIBLIOGRAPHY
[1] Aggarwal, C. C., and Yu, P. S. Outlier detection for high dimensional data. In
Proceedings of the ACM SIGMOD International Conference on Management of
Data (2001), pp. 37–46.
[2] Allan, J., Ed. Topic detection and tracking: event-based information organiza-
tion. Kluwer Academic Publishers, 2002.
[3] Argamon, S., Koppel, M., Fine, J., and Shimoni, A. R. Gender, genre, and
writing style in formal written texts. Text 23, 3 (2003).
[4] Bagga, A., and Baldwin, B. Entity-based cross-document coreferencing using
the vector space model. In Proceedings of COLING-ACL-17 (1998), pp. 79–85.
[5] Barnard, K., Duygulu, P., and Forsyth, D. A. Clustering art. In Proceedings of
CVPR (2001), pp. 434–441.
[6] Barnard, K., and Forsyth, D. A. Learning the semantics of words and pictures.
In Proceedings of ICCV-8 (2001), pp. 408–415.
[7] Basu, S., Banerjee, A., and Mooney, R. J. Semi-supervised clustering by seeding.
In Proceedings of ICML-19 (2002), pp. 19–26.
[8] Basu, S., Bilenko, M., and Mooney, R. J. A probabilistic framework for semi-
supervised clustering. In Proceedings of SIGKDD-10 (2004), pp. 59–68.
[9] Bekkerman, R., Eguchi, K., and Allan, J. Unsupervised non-topical classi-
fication of documents. Tech. Rep. IR-472, Center of Intelligent Information
Retrieval, UMass Amherst, 2006.
[10] Bekkerman, R., El-Yaniv, R., and McCallum, A. Multi-way distributional clus-
tering via pairwise interactions. In Proceedings of ICML-22 (2005), pp. 41–48.
[11] Bekkerman, R., El-Yaniv, R., Tishby, N., and Winter, Y. Distributional word
clusters vs. words for text categorization. Journal of Machine Learning Research
3 (2003), 1183–1208.
[12] Bekkerman, R., and Jeon, J. Multi-modal clustering for multimedia collec-
tions. In Proceedings of CVPR-07, the IEEE Computer Society Conference on
Computer Vision and Pattern Recognition (2007).
131
[13] Bekkerman, R., and McCallum, A. Disambiguating web appearances of people
in a social network. In Proceedings of WWW-14 (2005).
[14] Bekkerman, R., McCallum, A., and Huang, G. Automatic categorization of
email into folders: benchmark experiments on Enron and SRI corpora. Tech.
Rep. IR-418, CIIR, UMass Amherst, 2004.
[15] Bekkerman, R., Raghavan, H., Allan, J., and Eguchi, K. Interactive clustering
of text collections according to a user-specified criterion. In Proceedings of
IJCAI-20 (2007).
[16] Bekkerman, R., and Sahami, M. Semi-supervised clustering using combinatorial
MRFs. In Proceedings of ICML-23 Workshop on Learning in Structured Output
Spaces (2006).
[17] Bekkerman, R., Sahami, M., and Learned-Miller, E. Combinatorial Markov
Random Fields. In Proceedings of ECML-17 (2006).
[18] Bekkerman, R., Zilberstein, S, and Allan, J. Web page clustering using heuristic
search in the web graph. In Proceedings of IJCAI-20 (2007).
[19] Besag, J. Spatial interaction and statistical analysis of lattice systems. Journal
of the Royal Statistical Society 36, 2 (1974), 192–236.
[20] Besag, J. On the statistical analysis of dirty pictures. Journal of the Royal
Statistical Society 48, 3 (1986).
[21] Bickel, S., and Scheffer, T. Multi-view clustering. In Proceedings of ICDM’04,
the 4th IEEE International Conference on Data Mining (2004).
[22] Blei, D. M., Ng, A. Y., and Jordan, M. I. Latent Dirichlet allocation. Journal
of Machine Learning Research 3 (2003), 993–1022.
[23] Bouvrie, J. Multi-source contingency clustering. Master’s thesis, EECS, MIT,
2004.
[24] Burnard, L. User reference guide for the British National Corpus. Tech. rep.,
Oxford University Computing Services, 2000.
[25] Cai, D., He, X., Li, Z., Ma, W.-Y., and Wen, J.-R. Hierarchical clustering
of WWW image search results using visual, textual and link information. In
Proceedings of the 12th international conference on Multimedia (2004), pp. 952–
959.
[26] Chen, Y., Wang, J. Z., and Krovetz, R. Clue: Cluster-based retrieval of images
by unsupervised learning. IEEE Transactions on Image Processing 14, 8 (2005),
1187–1201.
[27] Chickering, D. M. Optimal structure identification with greedy search. Journal
of Machine Learning Research 3 (2003), 507–554.
132
[28] Crammer, K., and Chechik, G. A needle in a haystack: local one-class opti-
mization. In Proceedings of ICML-21 (2004).
[29] Cristianini, Nello, and Shawe-Taylor, John. An Introduction to Support Vector
Machines and Other Kernel-based Learning Methods. Cambridge University
Press, 2000.
[30] D. Aldous, D. Exchangeability and related topics. In Ecole d’Ete de Probabilities
de Saint-Flour XIII 1983. Springer, 1985, pp. 1–198.
[31] Dayanik, A., Lewis, D. D., Madigan, D., Menkov, V., and Genkin, A. Construct-
ing informative prior distributions from domain knowledge in text classification.
In Proceedings of SIGIR-29 (2006).
[32] de Sa, V.R. Unsupervised Classification Learning from Cross-Modal Environm
ental Structure. PhD thesis, University of Rochester, 1994.
[33] Dean, J., and Ghemawat, S. Mapreduce: simplified data processing on large
clusters. In Proceedings of Symposium on Operating System Design and Imple-
mentation (2004).
[34] Dhillon, I. S. Co-clustering documents and words using bipartite spectral graph
partitioning. In Proceedings of SIGKDD-7 (2001), pp. 269–274.
[35] Dhillon, I. S., Mallela, S., and Modha, D. S. Information-theoretic co-clustering.
In Proceedings of SIGKDD-9 (2003), pp. 89–98.
[36] Diaz, F. Regularizing ad hoc retrieval scores. In Proceedings of CIKM-05, the
14th ACM international conference on Information and knowledge management
(2005), pp. 672–679.
[37] Duygulu, P., Barnard, K., de Freitas, N., and Forsyth, D. Object recognition
as machine translation: Learning a lexicon for a fixed image vocabulary. In
Proceedings of ECCV-7 (2002).
[38] Eguchi, K., and Lavrenko, V. Sentiment retrieval using generative models. In
Proceedings of EMNLP (2006).
[39] El-Yaniv, R., and Souroujon, O. Iterative double clustering for unsupervised
and semi-supervised learning. In Advances in Neural Information Processing
Systems (NIPS-14) (2001).
[40] Fayyad, U. M., Reina, C., and Bradley, P. S. Initialization of iterative refinement
clustering algorithms. In Proceedings of SIGKDD-4 (1998), pp. 194–198.
[41] Feng, S. L., Manmatha, R., and Lavrenko, V. Multiple bernoulli relevance mod-
els for image and video annotation. In Proceedings of CVPR (2004), pp. 1002–
1009.
133
[42] Finn, A., Kushmerick, N., and Smyth, B. Genre classification and domain
transfer for information filtering. Advances in Information Retrieval LNCS
2291 (2002).
[43] Fleischman, M. B., and Hovy, E. Multi-document person name resolution. In
Proceedings of ACL-42, Reference Resolution Workshop (2004).
[44] Forman, G. Quantifying trends accurately despite classifier error and class
imbalance. In Proceedings of SIGKDD-12 (2006), pp. 157–166.
[45] Freeman, W. T., Pasztor, E. C., and Carmichael, O. T. Learning low-level
vision. IJCV 40, 1 (2000), 25–47.
[46] Friedman, N., Mosenzon, O, Slonim, N., and Tishby, N. Multivariate informa-
tion bottleneck. In Proceedings of UAI-17 (2001).
[47] Gao, B., Liu, T.-Y., Qin, T., Zheng, X., Cheng, Q.-S., and Ma, W.-Y. Web
image clustering by consistent utilization of visual features and surrounding
texts. In Proceedings of the 13th international conference on Multimedia (2005).
[48] Gao, B., Liu, T.-Y., Zheng, X., Cheng, Q.-S., and Ma, W.-Y. Consistent bi-
partite graph co-partitioning for star-structured high-order heterogeneous data
co-clustering. In Proceeding of SIGKDD-11 (2005), pp. 41–50.
[49] Gooi, C. H., and Allan, J. Cross-document coreference on a large scale corpus.
In Proceedings of HLT/NAACL (2004).
[50] Gupta, G., and Ghosh, J. Robust one-class clustering using hybrid global and
local search. In Proceedings of ICML-22 (2005), pp. 273–280.
[51] Han, H., Giles, L., Zha, H., Li, C., and Tsioutsiouliklis, K. Two supervised
learning approaches for name disambiguation in author citations. In Proceedings
of JCDL-4 (2004).
[52] Havre, S., Hetzler, E., Whitney, P., and Nowell, L. Themeriver: Visualizing
thematic changes in large document collections. IEEE Transactions on Visual-
ization and Computer Graphics 8, 1 (2002), 9–20.
[53] Huang, Y., and Mitchell, T. Text clustering with extended user feedback. In
Proceedings of SIGIR-29 (2006), pp. 413–420.
[54] Jakulin, A., and Bratko, I. Testing the significance of attribute interactions. In
Proceedings of ICML-21 (2004).
[55] Jeon, J., Lavrenko, V., and Manmatha, R. Automatic image annotation and re-
trieval using cross-media relevance models. In Proceedings of SIGIR-26 (2003),
pp. 119–126.
134
[56] Jeon, J., and Manmatha, R. Using maximum entropy for automatic image
annotation. In Proceedings of the 5th International Conference on Image and
Video Retrieval (2004), pp. 24–32.
[57] Jordan, M. I. Graphical models. Statistical Science 19 (2004), 140–155.
[58] Jordan, M. I., and Weiss, Y. Graphical models: Probabilistic inference. MIT
Press, 2002. In M. Arbib (ed.), The Handbook of Brain Theory and Neural
Networks.
[59] Karlgren, J., and Cutting, D. R. Recognizing text genres with simple metrics
using discriminant analysis. In Proceedings of the 15th International Conference
on Computational Linguistics (1994), pp. 1071–1075.
[60] Kessler, B., Nunberg, G., and Schütze, H. Automatic detection of text genre. In
Proceedings of the 35th Annual Meeting of the Association for Computational
Linguistics (1997), pp. 32–38.
[61] Koller, D., and Pfeffer, A. Object-oriented Bayesian networks. In Proceedings
of UAI-13 (1997), pp. 302–313.
[62] Koontz, W. L. G., Narendra, P. M., and Fukunaga, K. A branch and bound
clustering algorithm. IEEE Transactions on Computers 24 (1975).
[63] Koppel, M., and Shimoni, A. R. Automatically categorizing written texts by
author gender. Literary and Linguistic Computing 17, 4 (2002), 401–412.
[64] Kumaran, G., Jones, R., and Madani, O. Biasing web search results for topic
familiarity. In Proceedings of the ACM 14th Conference on Information and
Knowledge Management (2005), pp. 271–272.
[65] Kurland, O., and Lee, L. Pagerank without hyperlinks: structural re-ranking
using links induced by language models. In Proceedings of the 28th annual
international ACM SIGIR conference (2005), pp. 306–313.
[66] Lafferty, J., McCallum, A., and Pereira, F. Conditional random fields: Prob-
abilistic models for segmenting and labeling sequence data. In Proceedings of
ICML-18 (2001), pp. 282–289.
[67] Land, A. H., and Doig, A. G. An automatic method for solving discrete pro-
gramming problems. Econometrica 28 (1960), 497–520.
[68] Lebanon, G. Riemannian Geometry and Statistical Machine Learning. PhD
thesis, CMU, 2005.
[69] LeCun, Y., and Huang, F. J. Loss functions for discriminative training of
energy-based models. In Proceedings of the 10th International Workshop on
Artificial Intelligence and Statistics (2005), pp. 206–213.
135
[70] Lee, D. Y.-W. Genres, registers, text types, domains and styles: clarifying the
concepts and navigating a path through the bnc jungle. Language Learning &
Technology 5, 3 (2001), 37–72.
[71] Lee, Y.-B., and Myaeng, S. H. Text genre classification with genre-revealing
and subject-revealing features. In Proceedings of the 25th International ACM
SIGIR Conference (2002), pp. 145–150.
[72] Liu, B., Li, X., Lee, W. S., and Yu, P. S. Text classification by labeling words.
In Proceedings of AAAI-19 (2004), pp. 425–430.
[73] Loeff, N., Alm, C. O., and Forsyth, D. A. Discriminating image senses by
clustering with multimodal features. In Proceedings of COLING/ACL (2006),
pp. 547–554.
[74] Mann, G. S., and Yarowsky, D. Unsupervised personal name disambiguation.
In Proceedings of CoNLL-7 (2003), pp. 33–40.
[75] Marinescu, R., and Dechter, R. AND/OR branch-and-bound for graphical
models. In Proceedings of IJCAI-19 (2005).
[76] Matsuo, Y., Mori, J., Hamasaki, M., Ishida, K., Nishimura, T., Takeda, H.,
Hasida, K., and Ishizuka, M. Polyphonet: an advanced social network extrac-
tion system from the web. In Proceedings of WWW-15, the 15th international
conference on World Wide Web (2006), pp. 397–406.
[77] Matthews, R. A. J., and Merriam, T. V. N. Neural computation in stylome-
try I: An application to the works of Shakespeare and Fletcher. Literary and
Linguistic Computing 8, 4 (1993), 203–209.
[78] McCallum, A., Corrada-Emmanuel, A., and Wang, X. Topic and role discovery
in social networks. In Proceedings of IJCAI-19 (2005), pp. 786–791.
[79] McCallum, A., Pal, C., Druck, G., and Wang, X. Multi-conditional learning:
Generative/discriminative training for clustering and classification. In Proceed-
ings of AAAI-21 (2006).
[80] McGurk, H., and MacDonald, J. Hearing lips and seeing voices. Nature 264,
5588 (1976), 746–748.
[81] Metzler, D., and Croft, W. B. A Markov random field model for term depen-
dencies. In Proceedings of SIGIR-28 (2005).
[82] Mishne, G. Experiments with mood classification in blog posts. In Proceedings
of the 1st Workshop on Stylistic Analysis of Text for Information Access (2005).
[83] Ng, V., and Cardie, C. Improving machine learning approaches to coreference
resolution. In Proceedings of ACL-40 (2002).
136
[84] Pang, B., and Lee, L. Seeing stars: Exploiting class relationships for sentiment
categorization with respect to rating scales. In Proceedings of ACL-43 (2005),
pp. 115–124.
[85] Pang, B., Lee, L., and Vaithyanathan, S. Thumbs up? Sentiment classification
using machine learning techniques. In Proceedings of the 2002 Conference on
Empirical Methods in Natural Language Processing (2002), pp. 79–86.
[86] Papadimitriou, C. H., and Steiglitz, K. Combinatorial optimization: algorithms
and complexity. Prentice-Hall, 1982.
[87] Pearl, J. Probabilistic reasoning in intelligent systems: networks of plausible
inference. Morgan Kaufmann, 1988.
[88] Pereira, F., Tishby, N., and Lee, L. Distributional clustering of English words.
In Proceedings of the 30th Annual Meeting of the Association for Computational
Linguistics (1993), pp. 183–190.
[89] Qiu, G. Image and feature co-clustering. In Proceedings of ICPR-17 (2004),
pp. 991–994.
[90] Raghavan, H., Madani, O., and Jones, R. InterActive feature selection. In
Proceedings of IJCAI-19 (2005), pp. 841–846.
[91] Schmid, C., Mohr, R., and Bauckhage, C. Comparing and evaluating inter-
est points. In Proceedings of the Sixth International Conference on Computer
Vision (1998).
[92] Seki, Y., Eguchi, K., and Kando, N. Analysis of multi-document viewpoint
summarization using multi-dimensional genres. In AAAI Spring Symposium
on Exploring Attitude and Affect in Text: Theories and Applications (2004),
pp. 150–153.
[93] Sha, F., and Pereira, F. Shallow parsing with conditional random fields. In
Proceedings of HLT-NAACL (2003).
[94] Shamma, D. A., Owsley, S., Hammond, K. J., Bradshaw, S., and Budzik, J.
Network arts: exposing cultural reality. In Proceedings of WWW-04, the 13th
international World Wide Web conference, Alternate track (2004), pp. 41–47.
[95] Slonim, N. The Information Bottleneck: Theory and Applications. PhD thesis,
The Hebrew University, 2003.
[96] Slonim, N., Friedman, N., and Tishby, N. Unsupervised document classification
using sequential information maximization. In Proceedings of SIGIR-25 (2002).
[97] Slonim, N., Friedman, N., and Tishby, N. Multivariate information bottleneck.
Neural Computation 18, 8 (2006), 1739–1789.
137
[98] Slonim, N., and Tishby, N. Agglomerative information bottleneck. In Advances
in Neural Information Processing Systems 12 (NIPS) (2000), pp. 617–623.
[99] Slonim, N., and Tishby, N. Document clustering using word clusters via the
information bottleneck method. In Proceedings of SIGIR-23 (2000), pp. 208–
215.
[100] Stamatatos, E., Fakotakis, N., and Kokkinakis, G. K. Text genre detection using
common word frequencies. In Proceedings of the 18th International Conference
on Computational Linguistics (2000), pp. 808–814.
[101] Still, S., and Bialek, W. How many clusters? an information-theoretic perspec-
tive. Neural Computation 16, 12 (2004), 2483–2506.
[102] Sutton, C., and McCallum, A. Piecewise training of undirected models. In
Proceedings of UAI-21 (2005).
[103] Swan, R., and Allan, J. Automatic generation of overview timelines. In Pro-
ceedings of SIGIR-23 (2000), pp. 49–56.
[104] Tao, T., and Zhai, C. A two-stage mixture model for pseudo feedback. In
Proceedings of the 27th annual international ACM SIGIR conference (2004),
pp. 486–487.
[105] Tax, D. M. J., and Duin, R. P. W. Outliers and data descriptions. In Proceed-
ings of the 7th Annual Conference of the Advanced School for Computing and
Imaging (2001), pp. 234–241.
[106] Tishby, N., Pereira, F., and Bialek, W. The information bottleneck method,
1999. Invited paper to the 37th Annual Allerton Conference on Communication,
Control, and Computing.
[107] Turney, P. D. Thumbs up or thumbs down? Semantic orientation applied
to unsupervised classification of reviews. In Proceedings of the 40th Annual
Meeting of the Association for Computational Linguistics (2002), pp. 417–424.
[108] Uematsu, Y., Kataoka, R., and Takeno, H. Clustering presentation of web image
retrieval results using textual information and image features. In Proceedings of
the 24th IASTED international conference on Internet and multimedia systems
and applications (2006), pp. 217–222.
[109] Vapnik, V. N. The Nature of Statistical Learning Theory. Springer-Verlag,
1995.
[110] Wagstaff, K., and Cardie, C. Clustering with instance-level constraints. In
Proceedings of ICML-17 (2000).
[111] Wan, X., Gao, J., Li, M., and Ding, B. Person resolution in person search
results: Webhawk. In Proceedings of CIKM-14, the 14th ACM international
conference on Information and knowledge management (2005), pp. 163–170.
138
[112] Witten, I. H., and Frank, E. Data Mining: Practical machine learning tools
and techniques, 2nd ed. Morgan Kaufmann, 2005.
[113] Yang, K.-H., Chiou, K.-Y., Lee, H.-M., and Ho, J.-M. Web appearance dis-
ambiguation of personal names based on network motif. In Proceedings of the
2006 IEEE/WIC/ACM International Conference on Web Intelligence (2006),
pp. 386–389.
[114] Yeung, R.W. A new outlook of Shannon’s information measures. IEEE trans-
actions on information theory 37, 3 (1991).
[115] Zhou, Y., and Croft, W. B. Query performance prediction in web search environ-
ments. In Proceedings of the 30th Annual International ACM SIGIR Conference
(2007).
[116] Zhu, X. Semi-supervised learning literature survey. Tech. Rep. TR 1530, Uni-
versity of Wisconsin-Madison, 2005.
139
APPENDIX A
PROOF OF THEOREM 6.2.1
First, note that since both distributions Pr and Pg are uniform, then P (Wij =
w|Zij = 1) = 1mr and P (Wij = w|Zij = 0) = 1m .
Let us now compute the marginal P (Wij = w). For a relevant word wr, let us
denote it P (Wij = wr) = pr:
pr = P (Wij = wr) = P (Wij = wr|Zij = 1)P (Zij = 1) +
P (Wij = wr|Zij = 0)P (Zij = 0) = 1
mr
π +
1
m
(1− π) (A.1)
For a non-relevant word wn, denote P (Wij = wn) = pn:
pn = P (Wij = wn) = P (Wij = wn|Zij = 1)P (Zij = 1) +
P (Wij = wn|Zij = 0)P (Zij = 0) = 0 · π + 1
m
(1− π) = 1
m
(1− π)
We assume that the difference between these two probabilities is substantial, that is
pr − pn = π/mr >> 0. Let τ be their arithmetic mean:
τ =
1
2
(pr + pn). (A.2)
For each word w, we introduce a random variable Xw of its count (the number of
its occurrences in the dataset), which is distributed binomially : if w is relevant, then
Xw ∼ Bi(pr, N) and its mean is prN ; if w is non-relevant, then Xw ∼ Bi(pn, N) with
mean pnN . We are interested in bounding the probability that Xw ≤ τN for relevant
words, and that Xw ≥ τN for non-relevant words.
Using Chernoff bound for a relevant word w, we have:
P (Xw ≤ τN) ≤ exp
(
−N (pr − τ)
2
2pr
)
≤ ². (A.3)
For a non-relevant word w we have:
P (Xw ≥ τN) ≤ exp
(
−N (pn − τ)
2
3pn
)
≤ ². (A.4)
Solving (A.3) and (A.4) simultaneously with respect to N , we have:
N ≥ max
(
2pr ln
1
²
(pr − τ)2 ,
3pn ln
1
²
(pn − τ)2
)
,
140
and then substituting τ from (A.2):
N ≥ max
(
8pr ln
1
²
(pr − pn)2 ,
12pn ln
1
²
(pr − pn)2
)
=
8pr ln
1
²
(pr − pn)2 ,
where we use the given constraint that pw < 2π (and thus 3pn < 2pr, so the first term
is always greater than the second one). Substituting pr− pn = πpwm , and applying the
definition of pr from (A.1), we get:
8pr ln
1
²
(pr − pn)2 = 8
π + pw − πpw
pwm
· p
2
wm
2
π2
ln
1
²
≤ 8pwm
π2
ln
1
²
≤ 16m
π
ln
1
²
,
where we used the fact that π + pw − πpw ≤ 1 and that pw < 2π. Finally, we choose
the value of N to be the minimum among all the possible choices:
N = 16
m
π
ln
1
²
.
Putting it all together: What is the probability that there exists a word w which was
not detected correctly? Using the union bound we get:
P
(
∪w∈R(Xw ≤ τN)
⋃
∪w/∈R(Xw ≥ τN)
)
≤ ²m = δ,
so ² = δ
m
, and then
N = 16
m
π
ln
m
δ
,
which is log-linear in m. ¤
141
APPENDIX B
DETAILS OF EM ALGORITHM FOR ONE-CLASS
CLUSTERING
Given the graphical model from Figure 6.1, the joint distribution is:
P ({Y }, {Z}, {w}) =
∏
i
P (Yi)
∏
j
[P (Zij|Yi)P (wij|Zij)] (B.1)
Note that we represent a document di as its Bag-Of-Words: di , {wi1, wi2, . . . , wi|di|}.
Let us now define EM parameters Θ:
P (Yi = 1) = pd (B.2)
For each document di|ni−1: P (Zij = 1|Yi = 1) = πi (B.3)
P (Zij = 1|Yi = 0) = 0 (B.4)
For each word wl|ml=1: P (wl|Zl = 1) = pr(wl) (B.5)
P (wl|Zl = 0) = pg(wl) (B.6)
Using this notation, the marginal distribution of a document is written as:
P (di) =
∑
Yi
P (Yi)
∑
Zi1
P (Zi1|Yi)P (wi1|Zi1)
∑
Zi2
P (Zi2|Yi)P (wi2|Zi2) . . .
∑
Zi|di|
P (Zi|di||Yi)P (wi|di||Zi|di|)
=
∑
Yi
P (Yi)
|di|∏
j=1
(P (Zij = 1|Yi)P (wij|Zij = 1) + P (Zij = 0|Yi)P (wij|Zij = 0))
= pd
|di|∏
j=1
(πi pr(wij) + (1− πi) pg(wij)) + (1− pd)
|di|∏
j=1
pg(wij) (B.7)
E-step
Given the current set of parameters Θk at iteration k, for each document di and
each word wij in di, we compute the posteriors:
P̃ k(Yi = 1|di, Θk) = P (di|Yi = 1, Θ
k)P (Yi = 1|Θk)
P (di|Θk)
142
=
pd
∏|di|
j=1
(
πki p
k
r(wij) + (1− πki ) pkg(wij)
)
pd
∏|di|
j=1
(
πki p
k
r(wij) + (1− πki ) pkg(wij)
)
+ (1− pd)
∏|di|
j=1 p
k
g(wij)︸ ︷︷ ︸
denote αki
P̃ k(Yi = 1, Zij = 1|di, Θk) = P̃ k(Yi = 1|di, Θk)P (Zij = 1|Yi = 1, wij, Θk)
= P̃ k(Yi = 1|di, Θk)×
P (wij, Zij = 1|Yi = 1, Θk)
P (wij, Zij = 1|Yi = 1, Θk) + P (wij, Zij = 0|Yi = 1, Θk)
= P̃ k(Yi = 1|di, Θk) π
k
i p
k
r(wij)
πki p
k
r(wij) + (1− πki ) pkg(wij)︸ ︷︷ ︸
denote βkij
, αki βkij
P̃ k(Yi = 1, Zij = 0|di, Θk) = P̃ k(Yi = 1|di, Θk)
(1− πki ) pkg(wij)
πki p
k
r(wij) + (1− πki ) pkg(wij)︸ ︷︷ ︸
this term is (1− βkij)
, αki (1− βkij)
P̃ k(Zij = 1|di, Θk) = P̃ k(Yi = 1, Zij = 1|di, Θk) , αki βkij
P̃ k(Zij = 0|di, Θk) = 1− P̃ k(Zij = 1|di, Θk) , 1− αki βkij
M-step
We maximize:
Q(Θk+1|Θk) =
∑
i
E[log(P (Yi, {Zij}, {wij}|Θk+1)|P̃ k]
=
∑
i
E
[
log
(
P (Yi|Θk+1)
∏
j
P (Zij|Yi, Θk+1)
∏
j
P (wij|Zij, Θk+1)
)
|P̃ k
]
=
∑
i
E
[
log
(
P (Yi|Θk+1)
) |P̃ k
]
︸ ︷︷ ︸
denote A
+
∑
i,j
E
[
log
(
P (Zij|Yi, Θk+1)
) |P̃ k
]
︸ ︷︷ ︸
denote B
+
∑
i,j
E
[
log
(
P (wij|Zij, Θk+1)
) |P̃ k
]
︸ ︷︷ ︸
denote C
The A portion should not be optimized, because pd is a constant in our setting.
B =
∑
i,j
E
[
log
(
P (Zij|Yi, Θk+1)
) |P̃ k
]
143
=
∑
i,j
P̃ k(Yi = 1, Zij = 1|di) log
(
P (Zij = 1|Yi = 1, Θk+1)
)
+
∑
i,j
P̃ k(Yi = 1, Zij = 0|di) log
(
P (Zij = 0|Yi = 1, Θk+1)
)
+
∑
i,j
P̃ k(Yi = 0, Zij = 1|di) log
(
P (Zij = 1|Yi = 0, Θk+1)
)
+
∑
i,j
P̃ k(Yi = 0, Zij = 0|di) log
(
P (Zij = 0|Yi = 0, Θk+1)
)
=
∑
i,j
αki β
k
ij log
(
πk+1i
)
+
∑
i,j
αki (1− βkij) log
(
1− πk+1i
)
+
∑
i,j
0 log (0) +
∑
i,j
1 log (1)
C =
∑
i,j
E
[
log
(
P (wij|Zij, Θk+1)
) |P̃ k
]
=
∑
i,j
P̃ (Zij = 1|di) log
(
P (wij|Zij = 1, Θk+1)
)
+
∑
i,j
P̃ (Zij = 0|di) log
(
P (wij|Zij = 0, Θk+1)
)
=
∑
i,j
αki β
k
ij log
(
pk+1r (wij)
)
+
∑
i,j
(1− αki βkij) log
(
pk+1g (wij)
)
Now we compute derivatives of Q(Θk+1|Θk) with respect to πk+1i , pk+1r (wl), pk+1g (wl)
and find their values. First, let us find the optimal value of πk+1i .
∂Q
∂πk+1i
=
∂B
∂πk+1i
=
∂
∂πk+1i
[ ∑
j
αki β
k
ij log π
k+1
i +
∑
j
αki (1− βkij) log(1− πk+1i )
]
= αki
[∑
j
βkij
1
πk+1i
−
∑
j
(1− βkij)
1
1− πk+1i
]
= 0
πk+1i =
∑
j β
k
ij∑
j β
k
ij +
∑
j(1− βkij)
=
1
|di|
∑
j
βkij
Second, let us find the optimal value of pk+1r (wl):
∂Q
∂pk+1r (wl)
=
∂C
∂pk+1r (wl)
144
=
∂
∂pk+1r (wl)
[∑
i,j
αki β
k
ij log
(
pk+1r (wij)
)
+ λ
(
1−
m∑
l
pk+1r (wl)
)]
=
∂
∂pk+1r (wl)
[∑
i,j
δ(wij = wl) α
k
i β
k
ij log
(
pk+1r (wl)
)
+ λ
(
1−
m∑
l
pk+1r (wl)
)]
=
∑
i,j δ(wij = wl) α
k
i β
k
ij
pk+1r (wl)
− λ = 0
pk+1r (wl) =
1
λ
∑
i,j
δ(wij = wl) α
k
i β
k
ij
1 =
m∑
l
pk+1r (wl) =
m∑
l
1
λ
∑
i,j
δ(wij = wl) α
k
i β
k
ij =
1
λ
∑
i,j
αki β
k
ij
λ =
∑
i,j
αki β
k
ij
pk+1r (wl) =
∑
i,j δ(wij = wl) α
k
i β
k
ij∑
i,j α
k
i β
k
ij
=
∑
i α
k
i
∑
j δ(wij = wl) β
k
ij∑
i α
k
i
∑
j β
k
ij
Finally, let us find the optimal value of pk+1g (wl):
∂Q
∂pk+1g (wl)
=
∂C
∂pk+1g (wl)
=
∂
∂pk+1g (wl)
[∑
i,j
(1− αki βkij) log
(
pk+1g (wij)
)
+ λ
(
1−
m∑
l
pk+1g (wl)
)]
=
∂
∂pk+1g (wl)
[∑
i,j
δ(wij = wl)(1− αki βkij) log
(
pk+1g (wl)
)
+λ
(
1−
m∑
l
pk+1g (wl)
)]
=
∑
i,j δ(wij = wl)(1− αki βkij)
pk+1g (wl)
− λ = 0
pk+1g (wl) =
∑
i,j δ(wij = wl)(1− αki βkij)∑
i,j(1− αki βkij)
=
Nw −
∑
i α
k
i
∑
j δ(wij = wl) β
k
ij
N −∑i αki
∑
j β
k
ij
EM algorithm
To compute αi and βij efficiently, let us use the following relations:
αi =
pd
∏|di|
j=1 (πi pr(wij) + (1− πi) pg(wij))
pd
∏|di|
j=1 (πi pr(wij) + (1− πi) pg(wij)) + (1− pd)
∏|di|
j=1 pg(wij)
=
1
1 + 1−pd
pd
∏
j
pg(wij)
πi pr(wij)+(1−πi) pg(wij)
=
1
1 + 1−pd
pd
∏
j
1
πi
pr(wij)
pg(wij)
+1−πi
145
=
1
1 +
1−pd
pd
∏
j
(
πi pr(wij)
(1−πi) pg(wij)+1
)
(1−πi)
βij =
πi pr(wij)
πi pr(wij) + (1− πi) pg(wij) =
1
1 + 1−πi
πi
pg(wij)
pr(wij)
=
1
1 + 1πi pr(wij)
(1−πi) pg(wij)
Let us denote γkij =
πki p
k
r (wij)
(1−πki ) pkg(wij)
. Then we have
αki =
1
1 +
1−pd
pd∏
j(γ
k
ij+1)(1−πki )
(B.8)
βkij =
1
1 + 1
γkij
(B.9)
Algorithm:
1. Initialization:
(a) For each document di: π
0
i ← pw.
(b) For each word wl: p
0
r(wl) ← score(wl)∑
l′ score(wl′ )
, and p0g(wl) ←
1
score(wl)∑
l′
1
score(wl′ )
.
2. For each document di:
(a) For each word wij calculate γ
k
ij, and then β
k
ij using (B.9).
(b) Accumulate
∏
j(γ
k
ij + 1)(1− πi). Calculate αki using (B.8).
(c) Accumulate
∑
j β
k
ij. Calculate π
k+1
i ← 1|di|
∑
j βij.
3. Over all documents, accumulate ψk ← ∑i αki
∑
j β
k
ij.
4. Rank documents in decreasing order of αki . Stop if the ranking has not changed
since the previous iteration.1
5. For each word wl
(a) Over all documents, accumulate %kl ←
∑
i α
k
i
∑
j δ(wij = wl) β
k
ij.
(b) Calculate pk+1r (wl) ← %
k
l
ψk
, and pk+1g (wl) =
Nwl−%kl
N−ψk .
6. k ← k + 1. Go to 2.
1Alternatively, the algorithm can be terminated after a predefined number of EM iterations.
146
