 Novel representations and 
methods in text classification  
Manuel Montes, Hugo Jair Escalante 
Instituto Nacional de Astrofísica, Óptica y Electrónica, Mexico. 
http://ccc.inaoep.mx/~mmontesg/ 
http://ccc.inaoep.mx/~hugojair/ 
{mmontesg, hugojair}@inaoep.mx 
 
7th Russian Summer School in Information Retrieval 
Kazan, Russia, September 2013 
CONCEPT-BASED REPRESENTATIONS 
FOR TEXT CATEGORIZATION 
Novel represensations and methods in text classification 
2 
7th Russian Summer School in Information 
Retrieval    Kazan, Russia, September 2013 
Outline 
• The bag-of-concepts representation 
• Distributional term representations (DTRs) 
– Document occurrence representation (DOR) 
– Term co-occurrence representation (TCOR) 
• Using DTRs in short-text classification 
• Random indexing 
– Definition and computation 
– Incorporating syntactic information 
– Results on text classification 
• Final remarks 
3 
7th Russian Summer School in Information Retrieval    
Kazan, Russia, September 2013 
Bag-of-words representation 
• Many text classification methods adopt the BoW 
representation because its simplicity and efficiency.  
• Under this scheme, documents are represented by 
collections of terms, each term being an 
independent feature. 
–  Word order is not capture by this representation 
– There is no attempt for understanding  documents’ 
content 
 
4 
7th Russian Summer School in Information Retrieval    
Kazan, Russia, September 2013 
Main problems 
• BoW ignores all semantic information; it simply 
looks at the surface word forms 
– Polysemy and synonymy are big problems 
• BoW tend to produce very sparse representations, 
since terms commonly occur in just a small subset of 
the documents  
– This problem is amplified by lack of training texts and 
by the shortness of the documents to be classified. 
 
We need representations at concept level! 
 
5 
7th Russian Summer School in Information Retrieval    
Kazan, Russia, September 2013 
Bag-of-concepts 
• Addresses the deficiencies of the BoW by considering 
the relations between document terms. 
• BoC representations are based on the intuition that 
the meaning of a document can be considered as the 
union of the meanings of their terms. 
• The meaning of terms is related to their usage; it is 
captured by their distributional representation. 
– Document occurrence representation (DOR) 
– Term co-occurrence representation (TCOR) 
6 
7th Russian Summer School in Information Retrieval    
Kazan, Russia, September 2013 
Alberto Lavelli, Fabrizio Sebastiani, and Roberto Zanoli. Distributional term representations: an experimental 
comparison. Thirteenth ACM international conference on Information and knowledge management (CIKM '04). 
New York, NY, USA, 2004 
Document occurrence representation (DOR) 
• DOR representation is based on the idea that the 
semantics of a term may be view as a function of the 
bag of documents in which the term occurs.  
 
7 
d1 d2 … dn 
t1 
t2 
: wi,j 
tm 
Representation of terms 
7th Russian Summer School in Information Retrieval    
Kazan, Russia, September 2013 
Intuitions about the weights 
 
 
 
 
• DOR is a dual version of the BoW representation, 
therefore: 
– The more frequently ti occurs in dj, the more 
important is dj for characterizing the semantics of ti 
– The more distinct the words dj contains, the smaller 
its contribution to characterizing the semantics of ti. 
8 
7th Russian Summer School in Information Retrieval    
Kazan, Russia, September 2013 
Representing documents using DOR 
• DOR is a word representation, not a document 
representation. 
• Representation of documents is obtained by the 
weighted sum of the vectors from their terms. 
 
9 
d1 d2 … dn 
t1 
t2 
: wi,j 
tm 
d1 d2 … dn 
d1 
d2 
: wi,j 
dn 
Word representation 
Word–Document matrix 
Document representation 
Document–Document matrix 
SUM 
7th Russian Summer School in Information Retrieval    
Kazan, Russia, September 2013 
Term co-occurrence representation (TCOR) 
• In TCOR, the meaning of a term is conveyed by the 
terms commonly co-occurring with it; i.e. terms are 
represented by the terms occurring in their context 
 
10 
t1 t2 … tm 
t1 
t2 
: wi,j 
tm 
Representation of terms 
7th Russian Summer School in Information Retrieval    
Kazan, Russia, September 2013 
Intuitions about the weights 
 
 
 
 
• TCOR is the kind of representation traditionally used 
in WSD, therefore: 
– The more words tk and tj co-occur in, the more 
important tk is for characterizing the semantics of tj 
– The more distinct words tk co-occurs with, the smaller 
its contribution for characterizing the semantics of tj. 
11 
7th Russian Summer School in Information Retrieval    
Kazan, Russia, September 2013 
Representing documents using TCOR 
• TCOR, such as DOR, is a word representation, not a 
document representation. 
• Representation of documents is obtained by the 
weighted sum of the vectors from their terms. 
 
12 
t1 t2 … tm 
t1 
t2 
: 
tm 
t1 t2 … tm 
d1 
d2 
: 
dn 
Word representation 
Word–Word matrix 
Document representation 
Document–Word matrix 
SUM 
7th Russian Summer School in Information Retrieval    
Kazan, Russia, September 2013 
DOR/TCOR for text classification 
13 
7th Russian Summer School in Information Retrieval    
Kazan, Russia, September 2013 
Juan Manuel Cabrera, Hugo Jair Escalante, Manuel Montes-y-Gómez. Distributional term representations for short 
text categorization.  14th International Conference on Intelligent Text Processing and Computational Linguistics 
(CICLING 2013). Samos, Greece, 2013. 
Experiments 
• Short-text categorization based on distributional 
term representations (DOR and TCOR)  
– They reduce the sparseness of representations and 
alleviates, to some extent, the low frequency issue. 
• Our experiments aimed to: 
– Verify the difficulties of the BoW for effectively 
representing the content of short-texts 
– Assess the added value offered by concept-based 
representations over the BoW formulation 
14 
7th Russian Summer School in Information Retrieval    
Kazan, Russia, September 2013 
Evaluation datasets 
• We assembled two types of collections: 
– Whole documents for training and test 
– Whole documents for training and titles for test 
15 
Reuters-R8 
EasyAbstracts 
Cicling 2002 
7th Russian Summer School in Information Retrieval    
Kazan, Russia, September 2013 
Short-text classification with BoW 
16 
7th Russian Summer School in Information Retrieval    
Kazan, Russia, September 2013 
Conclusions (1) 
• Acceptable performance was obtained when regular-
length documents were considered 
– SVM obtained the best results for most configurations 
of data sets and weighting schemes 
• The performance of most classifiers dropped 
considerably when classifying short documents 
– The average decrement of accuracy was of 38.66% 
• Results confirm that the BoW representation is not 
well suited for short-text classification 
17 
7th Russian Summer School in Information Retrieval    
Kazan, Russia, September 2013 
Using DOR/TCOR for short-text classification 
18 
7th Russian Summer School in Information Retrieval    
Kazan, Russia, September 2013 
Conclusions (2) 
• DOR and TCOR clearly outperformed BoW for most 
configurations. 
– In 62 out of the 90 results the improvements of DTRs 
over BoW were statistically significant 
• In average, results obtained with DOR and TCOR 
were very similar.  
– DOR is advantageous over TCOR because it may result 
in document representations of much lower 
dimensionality. 
19 
7th Russian Summer School in Information Retrieval    
Kazan, Russia, September 2013 
Bag of concepts by random indexing 
• BoC approaches tend to be computationally 
expensive. 
• They are based on a co-occurrence matrix of order 
w×c; w = terms, and c = contexts (terms or documents) 
• Random indexing produce these context vectors in a 
more computationally efficient manner: the co-
occurrence matrix is replaced by a context matrix of 
order w×k, where k << c. 
20 
7th Russian Summer School in Information Retrieval    
Kazan, Russia, September 2013 
Magnus Sahlgren and Rickard Cöster. Using bag-of-concepts to improve the performance of support vector 
machines in text categorization. 20th international conference on Computational Linguistics (COLING '04). 
Stroudsburg, PA, USA, 2004. 
Random indexing procedure (1) 
• First step: a unique random representation known as 
“index vector” is assigned to each context. 
– A context could be a document , paragraph or 
sentence 
– Vectors are filled with -1, 1 and 0s. 
 
21 
D1 
D2 
Dn 
Documents 
k << n 
Index Vectors (IV) 
1 -1 
1 -1 
1 -1 
0                                                   k 
7th Russian Summer School in Information Retrieval    
Kazan, Russia, September 2013 
Random indexing procedure (2) 
• Second step: index vectors are used to produce context 
vectors by scanning through the text 
 
 
 
 
• Third step: build document vectors by adding their 
terms’ context vectors. 
 
22 
D1: Towards an Automata Theory of Brain 
D2:  From Automata Theory to Brain Theory 
 
1 -1 
1 -1 
 0                                                               k 
1 1 -1 -1 The context vector for brain 
di:  “From Automata Theory to Brain Theory” 
                        CV1         CV2         CV3    CV2 
 
di will be represented as the weighted sum of these vectors: 
 
                   a1CV1+a2CV2+a3CV3+a2CV2               a1, a2, a3 are idf-values 
7th Russian Summer School in Information Retrieval    
Kazan, Russia, September 2013 
Limitations of BoC representations 
• BoC representations ignore the large amount of 
syntactic data in the documents not captured 
implicitly through term context co-occurrences 
• Although BoC representations can successfully model 
some synonymy relations, since different words with 
similar meaning will occur in the same contexts, they 
can not model polysemy relations. 
• Solution: a representation that encodes both the 
semantics of documents, as well as the syntax of 
documents 
23 
7th Russian Summer School in Information Retrieval    
Kazan, Russia, September 2013 
Random indexing with syntactic information 
• Multiplicative bidding procedure: 
– For each PoS tag, generate a unique random vector 
for the tag of the same dimensionality as the term 
context vectors.  
– For each term context vector, we perform element-
wise multiplication between that term’s context 
vector and its identified PoS tag vector to obtain our 
combined representation for the term. 
– Finally, document vectors are created by summing the 
combined term vectors. 
24 
7th Russian Summer School in Information Retrieval    
Kazan, Russia, September 2013 
Jonathan M. Fishbein and Chris Eliasmith. Methods for augmenting semantic models with structural information 
for text classification. 30th European conference on Advances in information retrieval (ECIR'08).  Glasgow, UK, 
2008. 
An alternative procedure 
• Circular convolution procedure: 
– For each PoS tag, generate a unique random vector for 
the tag of the same dimensionality as the term context 
vectors 
– For each term context vector, perform circular 
convolution, which binds two vectors : 
 
 
 
– Finally, document vectors are created by summing the 
combined term vectors 
25 
term 
tag 
term-tag 
7th Russian Summer School in Information Retrieval    
Kazan, Russia, September 2013 
Circular convolution as binding operation  
•  Two properties that make it appropriate to be used 
as a binding operation: 
– The expected similarity between a convolution and its 
constituents is zero, thus differentiating the same 
term acting as different parts of speech in similar 
contexts.  
• Gives high importance to syntactic information 
– Similar semantic concepts (i.e., term vectors) bound 
to the same part-of-speech will result in similar 
vectors; therefore, usefully preserving the original 
semantic model. 
• Preserves semantic information 
7th Russian Summer School in Information Retrieval    
Kazan, Russia, September 2013 
26 
Results on text classification 
• The goal of the experiment was to demonstrate that 
integrating PoS data to the text representation is 
useful for classification purposes. 
• Experiments on the 20 Newsgroups corpus; a linear 
SVM kernel function was used; all context vectors 
were fixed to 512 dimensions 
27 
7th Russian Summer School in Information Retrieval    
Kazan, Russia, September 2013 
Final remarks 
• BoC representations constitute a viable supplement 
to word based representions. 
• Not too much work in text classification and IR 
– Recent experiments demonstrated that TCOR, DOR 
and random indexing results outperform those from 
traditional BoW; in CLEF collections improvements 
have been around 7%. 
• Random indexing is efficient, fast and scalable; 
syntactic information is easily incorporated. 
 
 
28 
7th Russian Summer School in Information Retrieval    
Kazan, Russia, September 2013 
References 
• Ron Bekkerman, Ran El-Yaniv, Naftali Tishby, and Yoad Winter. Distributional word clusters 
vs. words for text categorization. Journal of Machine Learning Research. March 2003. 
• Juan Manuel Cabrera, Hugo Jair Escalante, Manuel Montes-y-Gómez. Distributional term 
representations for short text categorization.  14th International Conference on Intelligent 
Text Processing and Computational Linguistics (CICLING 2013). Samos, Greece, 2013. 
• Jonathan M. Fishbein and Chris Eliasmith. Methods for augmenting semantic models with 
structural information for text classification. 30th European conference on Advances in 
information retrieval (ECIR'08).  Glasgow, UK, 2008. 
• Alberto Lavelli, Fabrizio Sebastiani, and Roberto Zanoli. Distributional term representations: 
an experimental comparison. Thirteenth ACM international conference on Information and 
knowledge management (CIKM '04). New York, NY, USA, 2004.  
• Magnus Sahlgren and Rickard Cöster. Using bag-of-concepts to improve the performance of 
support vector machines in text categorization. 20th international conference on 
Computational Linguistics (COLING '04). Stroudsburg, PA, USA, 2004. 
• Dou Shen, Jianmin Wu, Bin Cao, Jian-Tao Sun, Qiang Yang, Zheng Chen, and Ying Li. Exploiting 
term relationship to boost text classification. 18th ACM conference on Information and 
knowledge management (CIKM '09). New York, NY, USA, 2009.  
• Peter D. Turney and Patrick Pantel. From Frequency to Meaning: Vector Space Models of 
Semantics. Journal of Artificial Intelligence Research, vol. 37, 2010.  
7th Russian Summer School in Information Retrieval    
Kazan, Russia, September 2013 
29 
CONCISE SEMANTIC ANALYSIS 
Novel represensations and methods in text classification 
7th Russian Summer School in Information 
Retrieval Kazan, Russia, September 2013 
30 
Outline 
• Bag of concepts (again!) 
 
• Concise semantic analysis  
 
• Concise semantic analysis for author profiling 
 
• Meta-features for authorship attribution 
 
• Other bag-of-concept approaches 
7th Russian Summer School in Information Retrieval 
Kazan, Russia, September 2013 
31 
Bag of concepts (again) 
• Under the bag-of-concepts formulation a document 
is represented as a vector in the space of concepts 
 
• Concepts can be defined/extracted in different ways 
7th Russian Summer School in Information Retrieval 
Kazan, Russia, September 2013 
32 
Text 
Categorization 
Representation 
Document 
Novel 
Methods 
Bag of concepts (again) 
• So far we have seen representations that are 
unsupervised: techniques proposed for other tasks 
than classification and that do not take into account 
information of labeled examples 
 
• Can we define concepts that take into account 
information from labeled documents? 
 
7th Russian Summer School in Information Retrieval 
Kazan, Russia, September 2013 
33 
Bag of concepts (again) 
• Supervised bag-of-concepts:  encode inter-class 
and/or intra-class information into concepts 
 
• A simple (yet very effective) approach for building 
concepts/features in a supervised fashion: 
– Concise semantic analysis: associate a concept with a 
class 
 
 
7th Russian Summer School in Information Retrieval 
Kazan, Russia, September 2013 
34 
Concise semantic analysis 
• Underlying idea: to associate a concept with each 
class of the categorization problem.  
 
• After all, in text categorization classes are usually tied 
with words/topics/concepts.  
– E.g., when classifying news into thematic classes: 
politics, religion, sports, etc.   
 
• Implicitly CSA reduces dimensionality, sparseness 
and incorporate document-term and term-class 
relationships  
7th Russian Summer School in Information Retrieval 
Kazan, Russia, September 2013 
35 
Concise semantic analysis 
• Two stage process (as before, e.g., with DOR/TCOR) 
– Represent a term in the space of concepts 
– Represent documents in the space of concepts 
 
 
7th Russian Summer School in Information Retrieval 
Kazan, Russia, September 2013 
36 
Accommodation Steinhausen - 
Exterior View Blumenau, Brazil 
 October 2004 
Document 
Accommodation:  
Concept-based representation 
Steinhausen: 
. . . 
. . . 
. . . 
. . . 
. . . 
. . . + 
. . . 
Terms 
Representation 
for  
terms 
Representation for  
document 
Concepts 
Concise semantic analysis 
 
• Stage 1: Represent a term in the space of concepts 
 
 
7th Russian Summer School in Information Retrieval 
Kazan, Russia, September 2013 
37 




k k
ik
ikji
dlength
tdtf
CdHCtw
))(1log(
)),(1log(
),(),(






otherwise
Ctobelongsd
CdH
ik
ik
0
1
),(
C1 C1 … Ck 
t1 
t2 
: wi,j 
t|V| 
),( ji Ctw
Intuition:               Measures the association of term ti with class Cj ),( ji Ctw
term-concept matrix 
Concise semantic analysis 
 
• Stage 2: Represent documents in the space of 
concepts: 
 
 
7th Russian Summer School in Information Retrieval 
Kazan, Russia, September 2013 
38 



kj dt
kjijjkd dttfrfCtwCdw ),(),(),(







c
a
tdtfdttfrf jkkj 2log),(),(
C1 C1 … Ck 
d1 
d2 
: wd(i,j) 
dN 
),( ji Ctw
Intuition:                  Measures the association of terms in the 
document with class Cj 
),( jkd Cdw
a: # Positive documents that contain tj 
c: # Documents that contain tj 
document-concept matrix 
Concise semantic analysis 
• CSA for text categorization: 
7th Russian Summer School in Information Retrieval 
Kazan, Russia, September 2013 
39 
A variant of CSA for Author Profiling 
• The Author Proling (AP) task consists in knowing as much 
as possible about an unknown author, just by analyzing a 
given text 
– How much can we conclude about the author of a text 
simply by analyzing it? 
 
• Applications include business intelligence, computer 
forensics and security 
 
• Unlike Authorship Attribution (AA), on the problem of AP 
we does not have a set of potential candidates. Instead 
of that, the idea is to exploit more general observations 
(socio linguistic) of groups of people talking or writing 
 
7th Russian Summer School in Information Retrieval 
Kazan, Russia, September 2013 
40 
A variant of CSA for Author Profiling 
• Initially some works in AP have started to explore the 
problem of detecting gender, age, native language, 
and personality from written texts  
 
• AP can be approached as a single-label multiclass 
classification problem, where profiles are the classes 
to discriminate 
 
• From the point of view of text classification, we 
would have a set of training documents, labeled 
according to a category (e.g., man and woman). 
 
 
 
7th Russian Summer School in Information Retrieval 
Kazan, Russia, September 2013 
41 
A variant of CSA for Author Profiling 
 
• Stage 1: Represent a term in the space of concepts 
(one concept per profile) 
 
 
7th Russian Summer School in Information Retrieval 
Kazan, Russia, September 2013 
42 
 






k k
ik
ikji
dlength
tdtf
CdHCtw
)(
),(
1log),(),(
C1 C1 … Ck 
t1 
t2 
: wi,j 
t|V| 
),( ji Ctw
Intuition:               Measures the association of term ti with class Cj ),( ji Ctw
term-concept matrix 




k k
ik
ikji
dlength
tdtf
CdHCtw
))(1log(
)),(1log(
),(),(
A variant of CSA for Author Profiling 
 
• Stage 2: Represent documents in the space of 
concepts (one concept per profile): 
 
 
7th Russian Summer School in Information Retrieval 
Kazan, Russia, September 2013 
43 



kj dt
kjijjkd dttfrfCtwCdw ),(),(),(
C1 C1 … Ck 
d1 
d2 
: wd(i,j) 
dN 
),( ji Ctw
Intuition:                  Measures the association of terms in the 
document with class Cj 
),( jkd Cdw
document-concept matrix 



kj dt
jk
k
ij
jkd tdtf
dlength
Ctw
Cdw ),(
)(
),(
),(
A variant of CSA for Author Profiling 
• AP is not a thematic task and therefore the use of simple word 
occurrences may not work. Thus we also considered non-
thematic attributes as well 
7th Russian Summer School in Information Retrieval    
Kazan, Russia, September 2013 
44 
Concepts were generated 
for each modality 
separately and we 
concatenated the space 
of multimodal concepts 
PAN13’s Author Profiling task 
• The proposed representation was evaluated in the 
PAN AP 2013, track:  
– Two corpora English and Spanish 
– To determine the gender (male or female) and age 
(13-17, 23-27, 33-47) for each document.  
 
 
 
• A linear SVM classifier was used for classification 
7th Russian Summer School in Information Retrieval 
Kazan, Russia, September 2013 
45 
Instances  Blogs  Vocabulary 
English  236,000 413, 564 180, 809, 187 
Spanish 75,900 125,453 21,824,190 
CSA in AP at PAN-13 
• Official results: 
7th Russian Summer School in Information Retrieval 
Kazan, Russia, September 2013 
46 
7th Russian Summer School in Information Retrieval 
Kazan, Russia, September 2013 
47 
CSA in AP at PAN-13 
• Official results: 
Conclusions: CSA for AP 
• The proposed representation obtained the best result for 
the PAN’13-AP track (avg. performance was evaluated) 
 
• Besides our proposal is much more efficient than most 
formulations  (it was less efficient than two other 
approaches) 
 
• Too much potential on the use of CSA for non-thematic 
tasks  
 
• There are other ways of expanding the concept space to 
encode useful information 
 
 7th Russian Summer School in Information Retrieval 
Kazan, Russia, September 2013 
48 
Multimodal concepts 
• Multimodal information resulted very 
effective for AP 
 
• Are there other ways of encoding 
multimodal information into the 
document representation? 
– Basic approaches: late fusion / early 
fusion of multimodal attributes 
– Meta-features based concepts: 
associate a feature/concept with the 
similarity to multimodal prototypes 
 7th Russian Summer School in Information Retrieval    
Kazan, Russia, September 2013 
49 
Multimodal meta-features 
• Idea: to extract features from first-level attributes to 
derive a multimodal representation  that can 
improve the performance of first-level attributes 
– First level attributes: attributes/representations 
obtained from text directly (e.g., BoW, ngrams, POS-
based features, stylistic)  
 
• Process: 
– Extract first-level attributes 
– Obtain multimodal prototypes  
– Meta-features: Estimate the similarity of documents 
to each prototype 
 
 
 
 
7th Russian Summer School in Information Retrieval 
Kazan, Russia, September 2013 
50 
Multimodal meta-features 
7th Russian Summer School in Information Retrieval    
Kazan, Russia, September 2013 
51 
… … 
• First level attributes are clusted (e.g. via k-means) 
For each modality k-clusters 
are generated  
 
The centers of each cluster 
define multimodal 
prototypes 
Multimodal meta-features 
• Meta-features for a document are derived by 
estimating the similarity of the document to each of 
the m*k prototypes 
7th Russian Summer School in Information Retrieval    
Kazan, Russia, September 2013 
52 
Sm,k 
Multimodal meta-features 
 
7th Russian Summer School in Information Retrieval 
Kazan, Russia, September 2013 
53 
Multimodal meta-features 
• Application to authorship attribution. First level 
features: 
7th Russian Summer School in Information Retrieval 
Kazan, Russia, September 2013 
54 
Multimodal meta-features 
• Some experimental results 
7th Russian Summer School in Information Retrieval 
Kazan, Russia, September 2013 
55 
Multimodal meta-features 
7th Russian Summer School in Information Retrieval 
Kazan, Russia, September 2013 
56 
 
Conclusions Metafeatures for AA 
• Multimodal and similarity-based metafeatures aim to 
represent a document by its similarity with 
documents of the same and different classes 
 
• Metafeatures (combined with first-level features) 
resulted very effective for AA 
 
• Higher improvements were observed when more 
authors are involved in the problem 
 
• Metafeatures can be considered a type of concepts  
7th Russian Summer School in Information Retrieval    
Kazan, Russia, September 2013 
57 
Relation with other approaches 
• Latent semantic indexing: Concepts are derived via 
SVD, concepts are the principal components of the 
term-document matrix 
 
• Topic models: Concepts are probability distributions 
over words, they can be obtained in different ways 
(pLSI, LDA, etc.) 
 
• Deep learning: Concepts are the outputs of 
hierarchical neural networks that aimed to 
reconstruct documents 
7th Russian Summer School in Information Retrieval    
Kazan, Russia, September 2013 
58 
Final remarks 
• Concept-based representations encode useful 
information into the representation of 
terms/documents 
 
• This useful information can be defined in different 
ways, depending on what we want to emphasize or 
characterize from documents 
– Term/document occurrence/co-occurrence 
– Inter-intra class information 
– Multimodal similarity 
– …. 
 
 7th Russian Summer School in Information Retrieval    
Kazan, Russia, September 2013 
59 
