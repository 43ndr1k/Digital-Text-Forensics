Comparing word, character, and phoneme n-grams for subjective utterance
recognition
Theresa Wilson1, Stephan Raaijmakers2
1School of Informatics, University of Edinburgh, Edinburgh, UK
2TNO Information and Communication Technology, Delft, The Netherlands
twilson@inf.ed.ac.uk, stephan.raaijmakers@tno.nl
Abstract
In this paper, we compare the performance of classifiers trained
using word n-grams, character n-grams, and phoneme n-grams
for recognizing subjective utterances in multiparty conversa-
tion. We show that there is value in using very shallow linguistic
representations, such as character n-grams, for recognizing sub-
jective utterances, in particular, gains in the recall of subjective
utterances.
1. Introduction
Many NLP tasks rely on bag-of-words and n-grams as an im-
portant source of features. However, there is growing evidence
that shallow linguistic representations, such as character n-
grams, do indeed capture important linguistic distinctions and
may even outperform word n-grams, at least for some tasks.
In the past few years, character n-grams have successfully been
used for named-entity recognition [1], predicting authorship [2],
web page genre identification [3], and sentence-level subjectiv-
ity classification [4]. In this paper, we consider how well shal-
low linguistic representations, specifically n-grams of charac-
ters and phonemes, match up to a traditional representation of
word n-grams for the task of recognizing subjective utterances
in multiparty conversation.
Recognizing subjective content in multiparty conversation,
for example, recognizing the opinions and sentiments that par-
ticipants express during discussion in a meeting, is an important
and growing area of research. When recorded and archived,
meetings become a part of the organizational knowledge, but
their value is limited by the ability of tools to search and sum-
marize meeting content. While progress has been made on rec-
ognizing primarily objective meeting content, for example, in-
formation about the topics that are discussed and who is as-
signed to work on given tasks, there has been fairly little work
specifically directed toward recognizing subjective content.
Although character n-grams have previously been used for
subjectivity recognition in text [4], whether they also will prove
useful for recognizing subjectivity in speech is an open ques-
tion. Aside from the obvious differences in speech versus text,
there are challenges to working with speech, such as word errors
from the automatic speech recognition (ASR), that will most
certainly influence the performance of any system, whether
word level or character level.
If shallow character n-grams do prove useful for recogniz-
ing subjectivity in speech, this opens up an interesting possi-
bility: using a data representation based on phonemes. Auto-
matic phoneme recognition has a much lower latency than ASR,
which is currently much slower than real time. Recently there
has been a push toward developing real-time and on-line sys-
tems for processing meeting data. However, any system that
relies on words and characters will be hobbled by the speed of
the ASR. Phoneme n-grams may offer the possibility of moving
toward real-time, on-line system for subjectivity recognition in
speech.
2. Related Work
There has been a significant amount of research on subjectiv-
ity recognition in text, ranging from work at the phrase level to
work on classifying sentences and documents. Sentence-level
subjectivity classification (e.g., [5, 6]) is the research in text
closest to utterance-level subjectivity recognition in speech. Of
the sentence-level research, the most similar is work by Raai-
jmakers and Kraaij [4] comparing word-spanning character n-
grams to word-internal character n-grams for subjectivity clas-
sification. They found that character n-grams that span words
perform the best. We also use character n-grams that span
words in our experiments, but the data on which we evaluate
is very different: Raaijmakers and Kraaij carry out their experi-
ments on data from the news domain.
Other research on recognizing subjective content in multi-
party conversation is work by Wrede and Shriberg [7] on recog-
nizing hotspots in meetings, and work by Somasundaran et al.
[8] on recognizing sentiments in meetings. The features used by
Wrede and Shriberg are very different than what we use: They
focus exclusively on the contribution of various prosodic fea-
tures for identifying utterances of high involvement. Including
prosodic features are part of our plans for future work. Soma-
sundaran et al. use lexical features, including lists of strongly
positive, negative, and subjective words. In the future, we see
such lexical features as providing additional knowledge. We
plan to investigate how such knowledge can best be integrated
with the shallow linguistic representation of the data that we
explore in our experiments.
To the best of our knowledge, there is no other work on
emotion or subjectivity recognition in speech that investigates
the use of character or phoneme n-grams.
3. Data
For this work we use 13 meetings from the AMI Meeting Cor-
pus [9]. Each meeting has four participants and is approxi-
mately 30 minutes long. The participants play specific roles
(e.g., Project Manager, Marketing Expert) and together func-
tion as a design team. Within the set of 13 meetings, there are a
total of 20 participants, with each participant taking part in two
or three meetings as part of the same design team. Meetings
with the same set of participants represent different stages in
Accepted after peer review of full paper
Copyright © 2008 ISCA
September 22-26, Brisbane Australia1614
the design process (e.g., Conceptual Design, Detailed Design).
The meetings used in the experiments have been annotated
for subjective content [10]. As part of the annotations, sub-
jective utterances and subjective questions in the meetings were
identified. A subjective utterance is a span of words (or possibly
sounds) where a private state is being expressed either through
choice of words or prosody. A private state [11] is an inter-
nal mental or emotional state, including opinions, beliefs, sen-
timents, emotions, evaluations, uncertainties, and speculations,
among others. (1) and (2) below are examples of subjective ut-
terances.
(1) so I believe the the advanced functions should
maybe be hidden in a drawer, or something like
that from the bottom of it
(2) people uh additionally aren’t aren’t liking the
appearance of their products
Subjective questions are questions in which the speaker is elic-
iting the private state of someone else. In other words, the
speaker is asking about what someone else thinks, feels, wants,
likes, etc., and the speaker is expecting a response in which the
other person expresses what he or she thinks, feels, wants, or
likes. For example, both (3) and (4) below are subjective ques-
tions.
(3) Do you like the large buttons?
(4) What do you think about the large buttons?
Wilson [10] reports an inter-annotator agreement of 0.56 kappa
for recognizing both subjective utterances and subjective ques-
tions.
For the experiments in this paper, we automatically classify
the subjectivity of dialogue act segments, which are available as
part of the AMI corpus. If a dialogue act segment overlaps with
either a subjective utterance or a subjective question, it is con-
sidered subjective. Although subjective questions themselves
are not expressing private states, we believe identifying them
is important for recognizing subjectivity in dialogue. In part,
this is because the answers to subjective questions are likely
to be subjective. Also, subjective questions and subjective ut-
terances use much of the same terminology. To separate them
when trying to recognize subjectivity would make both types of
utterances harder to identify.
The 13 meetings used in the experiments contain a total of
14,941 dialogue act segments. The number of segments per
meeting ranges between 600 and 2200.
4. Experimental Setup
To answer our questions about whether character n-grams pro-
vide an advantage over word n-grams and whether phoneme n-
grams might prove useful for recognizing subjectivity in speech,
we conduct six different experiments. The experiments vary
with respect to the type of data representation (words, charac-
ters, or phonemes) and whether the data is from the reference
transcription of the meeting or was automatically recognized.
The six experiments are listed in Table 1.
The PREF phonemes were produced through dictionary lookup
on the words in the reference transcription. Words not in the
dictionary are excluded from the experiment. The ASR tran-
scription was provided by [12]. They report a 24% word error
rate. The automatically recognized phonemes, which include
markers for silences were produced in real time by the Brno
Table 1: Six experiments.
WREF: words from the reference transcription
CREF: characters from the reference transcription
PREF: phonemes identified from words (reference)
WASR: words from the ASR transcription
CASR: characters from the ASR transcription
PASR: automatically recognized phonemes
Table 2: Bag-of-character and bag-of-phonemes representations
of a short utterance.
Everything I have is kinda background
char WB e v e r y t h i n g WB i WB h a v e WB i s
WB k i n d a WB b a c k g r o u n d WB
phon eh v r ih th ih ng sil ay sil hh ae v sil ih z sil k
ay n t ax sil b ae k g r aw n d sil
University of Technology phoneme recognizer [13] and has a
42% error rate.1
For these different data types, we used a bag representation:
a multiset representation, i.e., a set with repetition. This type of
representation, usually consisting of words (bag of words) is
commonly used for document classification purposes.
Table 2 gives an example of a short sentence and its rep-
resentation in terms of characters and phonemes. Notice that
the bag of characters has specific markers marking the word
boundaries (WB); the ‘sil’ marker in the phoneme representa-
tion indicates a silence.
We use BoosTexter [14] AdaBoost.MH for the experiments.
We chose to use BoosTexter because, in addition to it having a
proven track record for working well for many NLP tasks, the
tool’s parameters allow for easy trial of many different n-gram
configurations.
For each experiment, we perform 13-fold cross validation.
Each meeting constitutes a separate fold for testing, e.g., all the
segments from meeting 1 make up the test set for fold 1. Then,
for a given fold, the segments from the remaining 12 meet-
ings are used for training and parameter tuning: roughly 90%
of these segments are used for training and 10% for parameter
tuning. The assignment to training versus tuning set was ran-
dom, with the only constraint being that a segment could only
be in the tuning set for one fold of the data.
For each experiment, BoosTexter parameter selection is
performed separately for each fold using the training and tuning
set for that fold. Using a grid search over the BoosTexter param-
eter space, we varied the number of rounds of boosting (N=100,
500, 1000, 2000, 5000), the length of the n-gram (n=1,2,3,4,5)
and the type of n-gram. BoosTexter can be run with three differ-
ent n-gram configurations: n-gram, s-gram, and f -gram. For
the default configuration (n-gram), BoosTexter searches for n-
grams up to length n. For example, if n = 3, BoosTexter will
consider 1-grams, 2-grams, and 3-grams. For the s-gram con-
figuration, BoosTexter will in addition consider sparse n-grams
(i.e., n-grams containing wildcards), such as the * idea. For the
f -gram configuration, BoosTexter will only consider n-grams
of a maximum fixed length, e.g., if n = 3 BoosTexter will only
consider 3-grams. The set of parameters that produces the best
performance on the tuning set in terms of subjective F1 score is
used to train the final classifier for that fold.
1The high error rate reflects the real-time nature of the phoneme
recognizer. Much lower error rates are possible, but with higher latency.
1615
Table 3: Average accuracy, recall, precision and F1 scores over
13 folds for the six tasks.
Accuracy Recall Precision F1
RAND 51.0 41.4 43.4 42.4
WREF 71.0 54.6 71.2 61.5
CREF 71.3 58.8 69.6 63.5
PREF 70.2 56.1 68.9 61.5
WASR 68.7 50.6 68.6 57.9
CASR 67.7 52.6 66.0 58.1
PASR 65.3 49.1 62.3 54.7
Table 4: Significance results using non-parametric Wilcoxon
signed rank test for zero median. All significance results are
reported for p < .05. For a given row-column pair (r, c), a ’<’
(’>’) value means: the row task r performs significantly worse
(better) than the column task c. A ’=’ means that no significant
difference could be established.
WREF CREF PREF WASR CASR PASR
WREF X < = > > >
CREF > X > > > >
PREF = < X > > >
WASR < < < X = >
CASR < < < = X >
PASR < < < < < X
5. Results and discussion
Table 3 shows the results for the baseline followed by the results
for the six experiments. The baseline (RAND) is a classifier that
randomly marks segments in the test set as subjective based on
the distribution of the subjective class in the training data. In the
table, overall accuracy is given first, followed by subjective re-
call, precision and F1 score. All values in the table are averages
over the 13 folds. The highest scores for each of the metrics for
REF and ASR experiments are given in bold.
How do the shallow linguistic representations compare to
the word n-grams? For the reference transcription, the character
representation outperforms the word representation. In fact the
character n-grams on reference transcriptions give the best per-
formance out of all the experiments. CREF has only a slightly
higher accuracy than WREF, but the difference in subjective re-
call is notable. CREF improves recall by 4.2 points (7.6%) with
only a 1.6 point drop in precision, resulting in a significant im-
provement in subjective F1 score. Improvements in recall are
also seen when comparing PREF to WREF (1.5 points), and
when comparing CASR to WASR (2 points). For these exper-
iments, however, the improvements in recall are offset by the
drops in precision. Table 4 shows the results of significance
testing comparing subjective F1 scores across the different ex-
periments.
Although the phonemes from the reference transcriptions
do not outperform the reference bag of words, they do achieve
a similar level of performance. This suggests that phoneme n-
grams are a promising route to pursue for fast, on-line subjec-
tivity recognition in multiparty conversation. The PASR rep-
resentation does give the lowest performance of the six bag-
of-X experiments. Still, the automatic phonemes handily out-
perform the random baseline, even with the high error rate re-
ported for recognizing the automatic phonemes. Given that the
performance of the phoneme n-grams is highly reliant on the
performance of the automatic phoneme recognition, we can ex-
pect improvements in the results for subjectivity classification
as automatic phoneme recognition is improved.
One obvious question is why the very shallow character
n-grams, and to a lesser extent the phoneme n-grams, do ac-
tually prove useful. In a large part, this is due to there just
being more data at the character or phoneme level, which
helps to alleviate problems of data sparsity and allows for
more general features. At the same time, the shallow n-
grams are able to capture enough linguistic information to ef-
fectively discriminate the classes. Table 5 gives examples of
some of the character n-grams that were consistantly among
the top identified by BoosTexter for each fold. The first col-
umn gives the character n-grams. A star (*) indicates a wild-
card. The second column gives the following conditional prob-
ability: Prob(segment=“subjective”|segment contains n-gram).
The probability of a given segment being subjective is 43%.
The last column gives examples of words that match the char-
acter n-gram.
The character n-grams in Table 5 capture a variety of dif-
ferent types of linguistic information. Some of the character
n-grams clearly just identify individual words that are good in-
dicators of either the non-subjective or subjective class. For
example, the presence of contracted will decreases the proba-
bility of a segment being subjective.2 On the other hand, the
presence of the words it, be, and maybe increase the probabil-
ity of a segment being subjective. The n-grams “k-a-y” and
“m-m-WB” match expressions such as okay and mm-hmm that
are likely to be backchannels and hence have a low probability
of being subjective. Some character n-grams that were learned
capture part-of-speech classes that are known to be correlated
with subjective language, such as adverbs (captured by “l-y-
WB”) and modals (captured by “o-u-l-d”). Other character n-
grams generalize over a set of words, many of which are used
to express subjectivity but would not logically be grouped to-
gether. The n-gram “h-i-n” matches not just variants of think
but also thing, everything, and anything, the presence of which
all increase the probability of a segment being subjective. The
n-gram “a-n-t” matches variants of want as well as important,
vibrant, fantastic, advantage, disadvantage, relevant, and sig-
nificant. This n-gram in particular shows the ability to general-
ize that we gain from the character n-grams. Although want and
important occur many times in the training data, and individu-
ally they provide plenty of information for a machine learner to
generalize from, the remaining words only occur in the train-
ing data from 1–3 times. With only one or two instances, it is
hard to know whether a word is actually a good indicator for a
particular class. However, if more general groups can be iden-
tified that include these low-frequency words, these words then
become much more useful.
While useful generalizations can be learned from the char-
acter n-grams, the last two examples in Table 5 show why they
also give a lower precision. The n-gram “o-*-d-WB”, for exam-
ple, matches a variety of subjective words including good, fond,
avoid, beyond, and bold, but it also matches non-subjective
words such as cord, record, respond, and road. Similarly, “n-
o-t” matches the words not and another, both of which increase
the probability of a segment being subjective. Unfortunately, it
2Note that the same is not true for uncontracted will, the presence
of which actually increases the probability (to 52%) of a segment being
subjective.
1616
Table 5: Examples of character-based n-gram patterns.
char n-gram Prob Examples of matching words
’-l-l 29% contracted will
k-a-y 17% ’kay and okay
m-m-WB 15% mm-hmm, mm
WB-i-t 61% it, it’s
b-e-WB 76% be, maybe
l-y-WB 71% -ly adverbs
o-u-l-d 75% would, could, should
h-i-n 76% think*, thing, everything, anything
a-n-t 77% want*, important, importantly, vibrant, fantastic, advantage, disadvantage, relevant, significant
o-*-d-WB 74% good, told, fond, hold, avoid, food (for thought), beyond, bold, cord, record, respond, road
n-o-t 71% not (76%), note (22%), another (62%)
also matches the word note, the presence of which decreases the
probability of a segment being subjective.
6. Conclusions
In this paper, we investigate the use of word n-grams, character
n-grams, and phoneme n-grams for recognizing subjective ut-
terances in multiparty conversation. We show that there is value
in using very shallow character and phoneme representations.
With the exception of phoneme n-grams from an automatic,
real-time, phoneme recognition system, the shallow linguistic
representations all give increases in subjective utterance recall
compared to word n-grams. The character n-grams from the
reference transcriptions give the best results out of all the ex-
periments, significantly outperforming word n-grams in terms
of subjective recall and F1 score, with a 7.6% increase in re-
call and a 3.3% increase in F1 score. Although the automatic
phoneme n-grams give the lowest performance, they still sig-
nificantly outperform the baseline, showing promise for mov-
ing toward phonemes for subjectivity detection in on-line and
real-time systems.
7. Acknowledgments
We would like to thank Igor Szoke for providing the automatic
phonemes.
This work was supported by the European IST Programme
through the AMIDA Integrated Project FP6-0033812.
8. References
[1] D. Klein, J. Smarr, H. Nguyen, and C. Manning, “Named
entity recognition with character-level models,” in Proc.
of CoNLL-2003, 2003.
[2] E. Stamatatos, “Ensemble-based author identification us-
ing character n-grams,” in Proc. of TIR’06, 2006.
[3] I. Kanaris and E. Stamatatos, “Webpage genre identifica-
tion using variable-length character n-grams,” in Proc. of
ICTAI 2007, 2007.
[4] S. Raaijmakers and W. Kraaij, “A shallow approach to
subjectivity classification,” in Proc. of ICWSM’08, 2008.
[5] E. Riloff and J. Wiebe, “Learning extraction patterns for
subjective expressions,” in Proc. of EMNLP-03, 2003.
[6] H. Yu and V. Hatzivassiloglou, “Towards answering opin-
ion questions: Separating facts from opinions and iden-
tifying the polarity of opinion sentences,” in Proc. of
EMNLP-03, 2003.
[7] B. Wrede and E. Shriberg, “Spotting “hot spots” in meet-
ings: Human judgments and prosodic cues,” in Proc. of
EUROSPEECH, 2003.
[8] S. Somasundaran, J. Ruppenhofer, and J. Wiebe, “Detect-
ing arguing and sentiment in meetings,” in Proc. of SIG-
dial, 2007.
[9] J. Carletta, S. Ashby, S. Bourban, M. Flynn, M. Guille-
mot, T. Hain, J. Kadlec, V. Karaiskos, W. Kraaij, M. Kro-
nenthal, G. Lathoud, M. Lincoln, A. Lisowska, I. Mc-
Cowan, W. Post, D. Reidsma, and P. Wellner, “The AMI
meeting corpus,” in Proc. of the Measuring Behavior Sym-
posium on “Annotating and Measuring Meeting Behav-
ior”, 2005.
[10] T. Wilson, “Annotating subjective content in meetings,” in
Proc. of LREC, 2008.
[11] R. Quirk, S. Greenbaum, G. Leech, and J. Svartvik, A
Comprehensive Grammar of the English Language. New
York: Longman, 1985.
[12] T. Hain, L. Burget, J. Dines, G. Garau, M. Karafiat,
M. Lincoln, J. Vepa, and V. Wan, “The AMI system for the
transcripton of meetings,” in Proc. of ICASSP’07, 2007.
[13] P. Schwarz, P. Matejka, and J. Cernocky, “Towards lower
error rates in phoneme recognition,” in Proc. of Intl. Conf.
on Text, Speech and Dialogue, 2004.
[14] R. E. Schapire and Y. Singer, “BoosTexter: A boosting-
based system for text categorization,” Machine Learning,
vol. 39, no. 2/3, pp. 135–168, 2000.
1617
