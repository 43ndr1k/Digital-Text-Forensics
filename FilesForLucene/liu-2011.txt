Combining integrated sampling with SVM ensembles for learning
from imbalanced datasets
Yang Liu a, Xiaohui Yu a,b, Jimmy Xiangji Huang b,⇑, Aijun An c
a School of Computer Science and Technology, Shandong University, Jinan, China
b School of Information Technology, York University, Toronto, Canada
cDepartment of Computer Science and Engineering, York University, Toronto, Canada
a r t i c l e i n f o
Article history:
Received 16 July 2009
Received in revised form 25 September
2010
Accepted 12 November 2010
Available online 17 December 2010
Keywords:
Data sampling
Classification
Imbalanced data mining
a b s t r a c t
Learning from imbalanced datasets is difficult. The insufficient information that is associ-
ated with the minority class impedes making a clear understanding of the inherent struc-
ture of the dataset. Most existing classification methods tend not to perform well on
minority class examples when the dataset is extremely imbalanced, because they aim to
optimize the overall accuracy without considering the relative distribution of each class.
In this paper, we study the performance of SVMs, which have gained great success in many
real applications, in the imbalanced data context. Through empirical analysis, we show that
SVMs may suffer from biased decision boundaries, and that their prediction performance
drops dramatically when the data is highly skewed. We propose to combine an integrated
sampling technique, which incorporates both over-sampling and under-sampling, with an
ensemble of SVMs to improve the prediction performance. Extensive experiments show
that our method outperforms individual SVMs as well as several other state-of-the-art
classifiers.
Crown Copyright  2010 Published by Elsevier Ltd. All rights reserved.
1. Introduction
A standard two-class classification method usually makes the simple assumption that the classes to be discriminated
should have a comparable number of instances. In accordance, most current classification systems are designed to optimize
the overall performance rather than considering the relative distribution of each class. However, this might be problematic in
practice. Many real-world datasets are highly skewed, in which most of the cases belong to a larger class and far fewer cases
belong to a smaller, yet usually more interesting class. Examples of applications with such datasets include searching for oil
spills in radar images (Kubat, Holte, & Matwin, 1998), telephone fraud detection (Fawcett & Provost, 1997), credit card fraud-
ulent detection (Chan & Stolfo, 1998; Chen, Ma, & Ma, 2009), diagnosis of weld flaws (Warren Liao, 2008), and text catego-
rization (Dumais, Platt, Heckerman, & Sahami, 1998; Ertekin, Huang, Bottou, & Giles, 2007; Stamatatos, 2008). Consequently,
these systems tend to misclassify the minority class examples as majority, and lead to a high false negative rate. In such
applications, the cost is usually high when a classifier misclassifies the small (positive) class instances. Take network intru-
sion as an instance. The number of malicious intrusions to a website is small compared with millions regular accesses every
day. However, the loss of might be huge if the illegitimacy is associated with the leakage of private internal data or the
disorder of website functionalities. To solve this problem, two categories of techniques have been proposed: sampling
approaches and algorithm-based approaches. Generally, sampling approaches include methods that over-sample the minority
0306-4573/$ - see front matter Crown Copyright  2010 Published by Elsevier Ltd. All rights reserved.
doi:10.1016/j.ipm.2010.11.007
⇑ Corresponding author.
E-mail addresses: yliu@sdu.edu.cn (Y. Liu), xhyu@yorku.ca (X. Yu), jhuang@yorku.ca (J.X. Huang), aan@cse.yorku.ca (A. An).
Information Processing and Management 47 (2011) 617–631
Contents lists available at ScienceDirect
Information Processing and Management
journal homepage: www.elsevier .com/ locate/ infoproman
class to match the size of the majority class (Guo & Viktor, 2004; Ling & Li, 1998; Solberg & Solberg, 1996), and methods that
under-sample the majority class to match the size of the minority class (Chen, Liaw, & Breiman, 2004; Kubat et al., 1998,
1997; Wilson &Martinez, 2000). Algorithm-based approaches aim at improving a classifier’s performance based on its inher-
ent characteristics. For example, methods that are particular tailored for Decision Trees, Neural Networks (MLPs), Naive
Bayes systems, etc.
This paper is concerned with improving the performance of the Support Vector Machines (SVMs) on imbalanced data sets.
SVMs have gained success in many applications, such as text mining and hand-writing recognition. However, when the data
is highly imbalanced, the decision boundary obtained from the training data is biased toward the minority class. Most ap-
proaches proposed to address this problem have been algorithm-based (Akbani, Kwek, & Japkowicz, 2004; Veropoulos,
Cristianini, & Campbell, 1999; Wu & Chang, 2004), which attempt to adjust the decision boundary through modifying the
decision function, including adjusting the kernel function, changing the intercept, etc. We take a complementary approach
and study the use of sampling as well as ensemble techniques to improve SVM’s performance.
First, our observation indicates that using over-sampling alone as proposed in previous work (e.g. SMOTE Akbani et al.,
2004) can introduce excessive noise and lead to ambiguity along decision boundaries. We propose to integrate the two types
of sampling strategies by starting with over-sampling the minority class to a moderate extent, followed by under-sampling
the majority class to the similar size. This is to provide the learner with more robust training data. We show by empirical
results that the proposed sampling approach outperforms over-sampling alone irrespective of the parameter selection.
We further consider using an ensemble of SVMs, which we call EnSVM, to boost the performance. A collection of SVMs is
trained individually on the processed data, and the final prediction is obtained by combining the results from those individual
SVMs. We then show that the generalization capability of EnSVM can be further improved by retaining only a subset of the
component SVM classifiers, and propose a new approach called EnSVM+, which utilizes genetic algorithms to perform clas-
sifier selection.
In summary, we make the following contributions:
1. We carefully design a series of experiments to demonstrate that over-sampling alone could mislead the decision bound-
ary of SVM when the data are highly skewed.
2. We propose a novel strategy to combine two types of sampling methods, answering the call to achieve optimal perfor-
mance by balancing the class distribution.
3. We propose the ensemble of SVM (EnSVM) model to integrate the classification results of weak classifiers constructed
individually on the processed data, and develop a genetic algorithm-based model called EnSVM+ to further boost the
performance of classification through classifier selection. The effectiveness of the proposed models is confirmed by
experiments.
The rest of the paper is organized as follows. We discuss the related work in Section 2. In Section 3, we review some basic
concepts of SVMs, and investigate the effects of class imbalance problem on SVMs. Section 4 discusses how to re-balance the
data, and Section 5 presents EnSVM and EnSVM+. In Section 6, we describe our benchmark data and report the experimental
results, and Section 7 concludes the paper.
2. Related work
The class imbalance problem has recently attracted considerable attention in the machine learning community.
Approaches to addressing this problem can be divided into two main directions: sampling approaches and algorithm-based
approaches. Sampling is a popular strategy to handle the skewness as it simply re-balances the data at the data preprocess-
ing stage; therefore can be deployed on top of many existing classification approaches (Chen et al., 2004; Guo & Viktor, 2004;
Kubat et al., 1998, 1997; Ling & Li, 1998; Solberg & Solberg, 1996; Sun, Lim, & Liu, 2009; Wilson & Martinez, 2000). Moreover,
it may function as an evaluation standard in a way that if a complex algorithm for handling imbalanced data does not out-
perform the existing classifiers with the simplest sampling scheme, it should be considered ineffective. Despite the goodness,
sampling approaches have obvious deficiencies: (1) Under-sampling majority instances may lose potential useful informa-
tions against the negative class. (2) Over-sampling positive class by forcing the classifier deliberately to learn from the
redundant duplicates would induce the problem of over-fitting. In addition, over-sampling increases the size of the training
dataset, which may induce much more extra computational cost. Current literatures report that under-sampling approaches
are generally superior to over-sampling approaches (Drummond & Holte, 2003; Japkowicz, 2000). Nonetheless, an opposite
conclusion has been made while the new instances introduced by over-sampling are generated in an intelligent way. As one
successful example, the SMOTE algorithm (Chawla, Bowyer, Hall, & Philip Kegelmeyer, 2002) over-samples the minority
class by generating artificially interpolated data. The algorithm starts with searching for the K-nearest neighbors for each
minority instance, and then for each neighbor, it randomly selects a point from the line connecting the neighbor and the
instance itself. Finally, this data point is included as a new minority instance. By adding the ‘‘new’’ minority instances into
training data, it is expected that the over-fitting problem can be alleviated. It has been reported that SMOTE has achieved
favorable results in many class imbalance studies (Chawla et al., 2002, Chawla, Lazarevic, Hall, & Bowyer, 2003).
Algorithm-based approaches include methods in which existing classifiers are adjusted to improve the performance for
imbalanced datasets. For example, to solve the class imbalance problem, some studies proposed to re-balance the class
618 Y. Liu et al. / Information Processing and Management 47 (2011) 617–631
distribution or assign different penalties for misclassification in the cost functions for decision tree inductions (Chen et al.,
2004; Drummond & Holte, 2003; Weiss & Provost, 2003).
Meanwhile, SVMs have established themselves as a successful approach for various machine learning tasks, and the class
imbalance issue has also been addressed in some studies. Through empirical observations, Wu and Chang (2004) report that
when the data is highly imbalanced, the decision boundary determined by the training data is largely biased toward the
minority class. As a result, the false negative rate that associates with the minority class might be high. To compensate
for the skewness, they propose to enlarge the resolution around the decision boundary by revising kernel functions. Along
this direction, Veropoulos et al. (1999) use pre-specified penalty constants on Lagrange multipliers for different classes;
Akbani et al. (2004) combine SVMs with the SMOTE over-sampling strategy with the cost function in their approach. Instead
of believing that SVMs may also suffer from the skewness of the data, there are also studies argue that SVMs are immune to
the class imbalance problem, because the classification decision boundary in SVMs is determined only by a small quantity of
support vectors (Japkowicz & Stephen, 2002). As a result, the large volume of instances that belong to the majority class are
redundant. To testify which case is correct and further investigate the problem, in this paper, we will study how the decision
boundary would change with respect to the different imbalance ratios, and explain the underlying implications accordingly.
Using an ensemble of weak classifiers to boost the classification performance has been reported to be effective in skewed
data. This strategy usually exploits a collection of individually trained classifiers, and integrates their prediction results to
make the final classification decision. For example, Chen et al. (2004) use random forests to unite the results of decision trees
induced from bootstrapping the training data, and Guo and Viktor (2004) apply data boosting to improve the performance on
hard examples that are difficult to classify. However, most current studies are confined to decision tree inductions, which
have been proved to be ill-suited for the class imbalance problem, as decision tree-based algorithms tend to favor short trees,
but most minority examples are likely to exist in the leaves of long trees (Han & Kamber, 2000).
3. Background
In this section, we first recall some background knowledge about how SVMs function as a classifier; then we demonstrate
how they act in the context of class imbalance problem with empirical studies.
3.1. Support vector machines
In a classification problem, SVMs determine a hyperplane in the space of possible inputs. This hyperplane will attempt to
separate the positives from the negatives by maximizing the distance from the hyperplane to the nearest of the positive and
negative examples. Intuitively, such classifications will be valid for those testing data that are close, but not necessarily iden-
tical to the training data.
In this section, we briefly describe the basic concepts in two-class SVM classification. Assume that there is a collection of n
training instances Tr = {xi, yi}, where xi 2 RN and yi 2 {1, 1} for i = 1, . . . , n. Suppose that we can find some hyperplane which
linearly separates the positive from negative examples in a feature space. The points x belonging to the hyperplane must
satisfy
w  xþ b ¼ 0; ð1Þ
where w is normal to the hyperplane and b is the intercept. To achieve this, given a kernel function K, a linear SVM searches
for Lagrange multiplier ai (i = 1, . . . , n) in Lagrangian
Lp  12 jjwjj
2 
Xn
i¼1
aiyiðxi wþ bÞ þ
Xn
i¼1
ai; ð2Þ
such that the margin between two classes, denoted with 2jjwjj, is maximized in the feature space (e.g., as described in Burges,
1998). Furthermore, in the ai optimizing process, Karush Kuhn Tucker (KKT) conditions which require
Xn
i¼1
aiyi ¼ 0; ð3Þ
must be satisfied. To predict the class label for a new case x, we need to compute the sign of
f ðxÞ ¼
Xn
i¼1
yiaiKðx; xiÞ þ b: ð4Þ
If the sign function is greater than zero, x belongs to the positive class, and the negative otherwise.
In SVMs, support vectors (SVs) are of crucial importance to the training set. They theoretically lie closest to the decision
boundary; thus form the margin between two sides. If all other training data were removed, and training process was
repeated, the same separating hyperplane would still be constructed. Note that there is a Lagrange multiplier ai for each
training instance. In this context, SVs correspond to those points for which ai > 0; other training instances have ai = 0. This
fact gives us the advantage of classifying by learning with only a small number of SVs, as all we need to know is the position
of the decision boundary which lies right in the middle of the margin; other training points can be considered redundant.
Y. Liu et al. / Information Processing and Management 47 (2011) 617–631 619
Further, it is of prime interest in the class imbalance problem because SVMs could be less affected by the negative instances
that lie far away from the decision boundary even if there are many of them.
The linear separable case is the ideal situation. In practice, however, the training data are almost always noisy, i.e., con-
taining errors due to various reasons. For example, some examples may be labeled incorrectly. Besides, practical problems
may have some randomness. Even for two identical input vectors, their class labels might be different. For SVM to be useful,
it needs to allow noise in the training data. However, with noisy data the linear separable SVM will not find a solution as the
constraint cannot be met. To allow errors in data, we can relax the margin constraints by introducing slack variables. Con-
sequently, in the case of non-separable data, 1-norm soft-margin SVMs minimize the Lagrangian
Lp ¼ 12 jjwjj
2 þ C
X
i
ni 
X
i
ai yiðxi wþ bÞ  1þ nif g 
X
i
lini; ð5Þ
where ni, i = 1, . . . , n are positive slack variables, C is a parameter selected by the user with a larger C indicating a higher pen-
alty to errors, and li are Lagrange multipliers introduced to enforce ni being positive. Similarly, corresponding KKT conditions
have to be met for the purpose of optimization.
In addition to the above cases where positive and negative examples can be linearly separated, i.e., the decision boundary
must be a hyperplane, for many real-life datasets, the decision boundary could be nonlinear. To deal with nonlinearly sep-
arable data, we can transform the input data from its original space into another space so that a linear decision boundary can
separate examples in the transformed space, called feature space. Thus, the basic idea is to map the data in the input space X
to a feature space F via a nonlinear mapping /. To avoid inducing huge computational cost through this transformation, we
can apply various kernel functions which compute dot product in feature space using only input vectors, such as Polynomial
Kernel and Gaussian RBF Kernel. The additional virtue of this kernel trick is that we would never need to explicitly know
what / is. More details of SVM method can be found in many data mining tutorials and textbooks, such as Burges (1998)
and Liu (2006).
3.2. Effects of class imbalance on SVMs
To investigate how decision boundaries are affected by the imbalance ratio, i.e., the ratio between the number of negative
examples and positive examples, we conduct a series of experiments and demonstrate the result in this section. To start off,
we first generate a synthetic data which can be used in the binary classification problem. The numbers of positive examples
and negative examples are the same. We then manipulate the imbalance ratio by bootstrapping different amount of exam-
ples and allocating them to the two classes. For instance, for imbalance ratio equals to 10:1, 200 negative examples and 20
positive examples are assigned to the two classes respectively. For the purpose of better visualization, we work on the linear
SVM case with two dimensional data, and believe that the same situation may apply to the nonlinear separable case with the
assist of kernel functions although the result cannot be easily displayed due to the curse of dimensionality.
First, Fig. 1 shows the case of classifying a balanced training dataset, where crosses and circles represent the instances from
positive and negative classes respectively. From Fig. 1, we notice that instances from both classes are distinctly separated by a
line close to the ‘‘ideal boundary’’ (y = 1), as it is almost of equal length to both sides. We then reform successive new datasets
with different degrees of data skewness by removing instances from the positive and adding instances to the negative. Fig. 2
reflects the data distributionwhen imbalance ratios vary from 10:1 to 300:1. From Fig. 2a, we find that if the imbalance ratio is
moderate, the boundary will still be close to the ‘‘ideal boundary’’. This observation demonstrates that SVMs could be robust
and self-adjusting; they are thus able to alleviate the problemarising frommoderate imbalance. Nonetheless, as the imbalance
ratio becomes larger and larger, as illustrated in Fig. 2b and c, the boundaries get evidently biased toward theminority class. As
a consequence, making predictions with such an approach may lead to a high false negative rate.
0 1 2 3 4 5 6 7 8
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2 class one
class two
support vector
class one margin
decision boundary
class two margin
Fig. 1. Learning with SVM over balanced data.
620 Y. Liu et al. / Information Processing and Management 47 (2011) 617–631
4. Re-balancing the data
We have shown that SVMs may perform well while the imbalance ratio is moderate in Section 3.2. Nonetheless, their per-
formance could still suffer from extreme data skewness. To cope with this problem, in this section, we study the use of sam-
pling techniques to balance the data.
4.1. Under-sampling
Under-sampling approaches have been reported to outperform over-sampling approaches in previous literatures. How-
ever, under-sampling throws away potentially useful information in the majority class; it thus could make the decision
boundary trembles dramatically. For example, given the imbalance ratio as 100:1, in order to get a close match for the minor-
ity, it might be undesirable to throw away 99% of majority instances.
Fig. 3 illustrates such a scenario, where the majority class is under-sampled to keep the same number of instances as in
the minority class, but a considerable number of support vectors lie far away from the ideal boundary y = 1. Accordingly,
making predictions with such SVMs may result in a very low accuracy.
4.2. Over-sampling
Considering that simply replicating the minority instances tends to induce over-fitting, using interpolated data is often
preferred in the hope of supplying additional and meaningful information on the positive class. SMOTE is the method that
has been mostly cited along this line.
However, the improvement of integrating SVMs with the SMOTE algorithm can be limited because of its dependence on
the proper selection of the number of nearest neighbors K as well as imbalance ratios. Basically, the value of K determines
0 1 2 3 4 5 6 7 8
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2 class one
class two
support vector
class one margin
decision boundary
class two margin
0 1 2 3 4 5 6 7 8
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2 class one
class two
support vector
class one margin
decision boundary
class two margin
0 1 2 3 4 5 6 7 8
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2 class one
class two
support vector
class one margin
decision boundary
class two margin
Fig. 2. Boundary changes with different imbalance ratios.
Y. Liu et al. / Information Processing and Management 47 (2011) 617–631 621
howmany new data points will be added into the interpolated dataset. Fig. 4 shows how the decision boundary changes with
different K values. Fig. 4a shows the original class distribution with the imbalance ratio being 100:1. Fig. 4b demonstrates
that the classification boundary is relatively smoothed when K has a small value, e.g., K = 2 in our example; nonetheless,
it is still biased toward the minority class. This is due to the fact that SMOTE actually provides little information on the
minority class; hence over-sampling in this case should be considered as a type of ‘‘phantom-transduction’’. When the
0 1 2 3 4 5 6 7 8
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2 class one
class two
support vector
class one margin
decision boundary
class two margin
Fig. 3. Under-sampling majority instances.
0 1 2 3 4 5 6 7 8
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2 class one
class two
support vector
class one margin
decision boundary
class two margin
0 1 2 3 4 5 6 7 8
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2 class one
class two
support vector
class one margin
decision boundary
class two margin
0 1 2 3 4 5 6 7 8
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2 class one
class two
support vector
class one margin
decision boundary
class two margin
Fig. 4. Using SMOTE with different K values.
622 Y. Liu et al. / Information Processing and Management 47 (2011) 617–631
interpolated dataset is considerably enlarged as K increases, as shown in Fig. 4c, ambiguities could arise along the current
boundary, because SMOTE makes the assumption that the instance between a positive class instance and its nearest neigh-
bors is also positive. However it may not always be true in practice. As a positive instance is very close to the boundary, its
nearest neighbor is likely to be negative, and this possibility may increase as K and the imbalance ratio become larger. As a
result, the new data instance, which actually belongs to the negative class, is mislabeled as positive, and the induced decision
boundary, as shown in Fig. 4c, could be inversely distorted to the majority class.
4.3. Combination of two types of samplings
To address problems arising from using either of the two sampling approaches alone, we integrate them together. Given
an imbalance ratio, we first over-sample the minority instances with the SMOTE algorithm to some extent, and then under-
sample the majority class so that both sides have the same or similar amount of instances. To under-sample the majority
class, we use the bootstrap sampling approach with all available majority instances, provided that the size of the newmajor-
ity class is the same as that of the minority class after running SMOTE. The benefit of doing so is that this approach inherits
the strength of both strategies, and alleviates the over-fitting and information loss problems.
In addition, to avoid taking risks of inducing ambuities along the decision boundary, we choose to filter out the ‘‘impure’’
data first before sampling. In this context, an instance is considered ‘‘impure’’, if and only if two of its three nearest neighbors
provide different class labels other than that of itself. The algorithm used for pruning impurities is given in Algorithm 1. This
idea is inspired by the Edited Nearest Neighbor Rule(ENN) ( Wilson & Martinez, 2000), which was originally used to remove
unwanted instances from the majority class. In our work, however, to further reduce the uncertainty from both classes, such
a filtering process is taken on each side.
Algorithm 1. [Impurity Pruning Algorithm (IP)]
Input: Labeled example set E = {xi, yi}, where yi 2 {1, 1} is the class label of example xi 2 Rn, and i 2 [1, n].
Output: Purified set Enew
1. Noisy set N = /
2. for i = 1 to n do
3. Find three nearest neighbors of xi
4. if yi differs from two of the labels that are provided by its three nearest neighbors then
5. N = N [ {xi, yi}
6. end if
7. end for
8. Enew = E  N
5. Ensemble of SVMs
Recently, ensemble techniques have been applied in a broad spectrum of scenarios in order to improve the performance
of weak classifiers. The basic idea is to first train a collection of learners independently, and then combine the individual
output of each learner to obtain the final output. The rationale is that by aggregating results from the individual learners,
the noise resulting from bootstrapping at randomwill be reduced, and the ensemble is expected to be more robust than each
of the individuals. Nevertheless, most current work is confined to learning with decision trees. For instance, Random Forest
grows a set of trees with the CART/C4.5 algorithm to improve the average performance. However, this strategy might be
problematic in our context. The reason is that decision tree-based algorithms tend to favor ‘‘short trees’’ as developing
too many tiers may induce the problem of over-fitting, but in the context of class imbalance, ‘‘deep’’ branches of a tree usu-
ally play a vital role in identifying the minority class. Previous studies have attempted to cut off some of the nodes as a trade-
off, but how to determine the appropriate level of pruning is still an open problem.
In this section, we present methods based on ensembles of SVM classifiers. We first describe a method that integrates an
ensemble of SVM classifiers with aforementioned over-sampling and under-sampling strategies, and then further improve it
by building selective ensembles that aim to retain only the ‘‘best’’ classifiers.
5.1. EnSVM: The base model
The base model with an ensemble of SVM classifiers is graphically illustrated in Fig. 5. Re-balancing is still necessary in
this context because when learning from extremely imbalanced data, it is very likely that a bootstrap sample used to train an
SVM in the ensemble is composed of few or even none of the minority instances. Hence, each component learner of the
ensemble would suffer from severe skewness, and the improvement of using an ensemble would be limited. Our proposed
method, called EnSVM, is formally presented in Algorithm 2.
Y. Liu et al. / Information Processing and Management 47 (2011) 617–631 623
Algorithm 2. [EnSVM]
Input: Training set Etrain = {hx1, y1i, . . . , hxn, yni}, Testing set Etest = {xn+1, . . . , xn+l}, Number of nearest neighbors: K,
Number of component SVMs: M
Output: Predicted class label for Etest: {yn+1, . . . , yn+l}
1. E1 = IP(Etrain) /remove impurities with the Algorithm 1/
2. Split E1 into minority class P and majority class N
3. E2 = SMOTE(P, K) /over-sample P with SMOTE algorithm/
4. S = Size(E2) /get the size of E2/
5. for i = 1 to M do
6. Ni = Bootstrap(N, S) /get bootstrap samples of N, and each sample has S instances/
7. Ti = E2 [ Ni /form a new training subset/
8. Build a component classifier SVMi with Ti
9. Feed SVMi with Etest, and get a predicted class label array Arri
10. end for
11. for j = 1 to l do
12. Find yn+j by casting a majority vote over Arr1, Arr2, . . . , Arrm
13. end for
As described in Section 4.3, we start re-balancing the data by filtering out impurities which may induce ambiguities. Then,
the minority class is over-sampled with the SMOTE method to smooth the decision boundary (Chawla et al., 2002). That is,
for each positive instance, it finds the K nearest neighbors, draws a line between the instance and each of its K nearest neigh-
bors, and then randomly selects a point on each line to use as a new positive instance. In this way, K  n new positive
instances are added to the training data, where n is the number of positive instances in the original training data. After that,
we under-sample the majority class instances M times to generate M bootstrap samples so that each bootstrap sample has
the same or similar size as the over-sampled positive instances. The next step is to combine each bootstrap sample (of the
PositiveNegative
Filter
Oversample
Negative1
Bootstrap
Negative
PositiveNegative2 NegativeN
Bootstrap
… … 
Positive
Training1 … … Training2 TrainingN
Combine
Train Train Train
Classifier1 Classifier2 … … ClassifierN
Testing
data
Ensemble
System
Decision
Fig. 5. The EnSVM algorithm.
624 Y. Liu et al. / Information Processing and Management 47 (2011) 617–631
majority class) with the over-sampled positive instances to form a training set to train an SVM. Therefore, M SVMs can be
obtained from M different training sets. Finally, the M SVMs form an ensemble to make a prediction on a test instance
through majority voting. In our experiments reported in Section 6, we set M to be 10, and an arbitrary class is selected when
there is a tie in the vote.1
5.2. EnSVM+: Selective ensembles
In EnSVM, all the individual SVM classifiers are used to reach a final classification decision. As we will show below, in
many cases, using only a subset of the classifiers may lead to better generalization capabilities due to possible reductions
in the coupling of individual classifiers. Inspired by Zhou, Wu, and Tang (2002), which presents promising results on com-
bining many but not all neural networks in an ensemble, we investigate how we can improve the prediction accuracy
through selective ensemble.
We first show why using only a subset of the available classifiers in an ensemble could result in better generalization
accuracy. To see this, suppose there are L test instances with the target output being y1, y2, . . . , yL, where yj 2 {1, 1} denotes
the true class label. Let the actual output of the ith SVM classifier on the jth instance be ŷij 2 f1;þ1g.
Since majority voting is used, the actual output of the ensemble on the jth instance can be expressed as ŷj ¼ sgnð
PM
i¼1ŷijÞ,
where M is the number of component classifiers in the ensemble, and sgn(x) is a signum function that takes the value of 1 if
x > 0, 0 if x = 0, and 1 if x < 0.
If we consider the generalization error of a classifier on an instance to be 0 if the actual output agrees with the target
output, 1 if they are completely opposite (e.g., 1 vs. +1), and 12 if the actual output is 0 (in the case of a tie in voting), then
the generalization error of the ensemble on the jth instance can be defined as j ¼ 12 jŷj  yjj. Thus the total generalization
error the ensemble is
E ¼ 1
2
XL
j¼1
jŷj  yjj: ð6Þ
Now let us consider what will happen if one of the component SVM classifiers is excluded from the ensemble. For the jth
instance, if
PM
i¼1ŷij
  > 1, then apparently removing one component classifier from the ensemble will not affect the final out-
put of the ensemble. Therefore, in the rest of the discussion, we only focus on the cases where
PM
i¼1ŷij
  6 1.
Suppose the kth classifier is excluded from the ensemble. Then the total generalization error becomes
Ek ¼ 12
XL
j¼1
sgn
XM
i¼1;i–k
ŷij
 !
 yj

: ð7Þ
Comparing Eqs. (6) and (7), it is easy to show that there exist cases where EP Ek. As an extreme example, when all com-
ponent classifiers are the exactly the same, E = Ek; therefore, the generalization capability of the ensemble is unaffected by
the exclusion of the kth component. As another example, consider the case where L = 1, y1 = 1,
PM
i¼1ŷi1 ¼ 0, and ŷk1 ¼ 1.
Then using properties of the sgn function, we can show that
E Ek ¼ 12 sgn
XM
i¼1
ŷi1  ŷk1
 !
 sgn
XM
i¼1
ŷi1
 ! !
¼ 1
2
:
That is, the generalization error can be reduced if the kth component classifier is excluded.
To summarize, for an ensemble of SVM classifiers, ensembling many of them may be better than ensembling all of them.
In order to benefit from this observation, we propose EnSVM+, in which a selective ensemble of SVM classifiers are employed.
It is worth noting that identifying the optimal subset of classifiers to be included in the ensemble can be viewed as an
optimization problem, but its solution can be very difficult due to the combinatorial nature of the problem. Therefore, here
we propose an approximate solution based on the genetic algorithm (Goldberg, 1989), which has been shown to be a suc-
cessful approach to stochastic optimization.
The basic idea of the proposed algorithm is to try to evolve a population of bits (a bit vector b), where the value of each bit
(1 or 0) corresponds to the inclusion or exclusion of a component classifier in the ensemble. A validation set V is used in order
to evaluate the goodness of the individual classifiers. Let ErrVb be the validation error of the ensemble corresponding to the bit
vector b on the validation set V. Apparently, a smaller ErrVb value indicates a better vector b. Therefore, we use f ðbÞ ¼ 1=ErrVb
as the fitness function used by the genetic algorithm. Formally,
f ðbÞ ¼ 1
ErrVb
¼ 1PjV j
j¼1 sgn
P
16i6M^bi¼1ŷij
 
 yj
  : ð8Þ
The result of the genetic algorithm is an optimal bit vector b, which is used to determine which component SVM to keep
in the ensemble. The resulting ensemble can then be used for prediction.
1 Practically an uneven number is usually applied to avoiding the potential conflict in voting.
Y. Liu et al. / Information Processing and Management 47 (2011) 617–631 625
6. Empirical evaluation
In this section, we first introduce the evaluation measures used in our study, and then describe the datasets. After that, we
report the experimental results that compare our proposed approach with other methods.
6.1. Evaluation measures
The evaluation measures used in our experiments are based on the Confusion Matrix. Table 1 illustrates a confusion matrix
for a two-class problem with positive and negative class values.
Traditionally, the performance of a machine learning algorithm is usually evaluated with ‘‘accuracy’’, denoted with
Accuracy ¼ TPþTNTPþFNþFPþTN. However, this could be inappropriate when the data is highly imbalanced. For example, in a
two-class problem with class distribution of 95:5, a straightforward method of guessing all instances as negative would
achieve an accuracy of 95%. In the context of class imbalance problem, however, the higher rate of correct detection on
the minority class is particularly required. Moreover, considering the confusion matrix, we notice that the class distribu-
tion (the proportion of positive to negative examples) is implied by the relationship between the first and the second line
of Table 1, i.e., any measurement which employs values from both lines will be sensitive to class skewnesses. Hence,
‘‘accuracy’’ is obviously not suitable any more. Recently, some researchers in this area have realized this problem, and
proposed different metrics accordingly. With the help of confusion matrix, our performance measures are expressed as
follows:
 G-mean ¼ ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffia  aþp , where a ¼ TNTNþFP and aþ ¼ TPTPþFN;
 F-measure ¼ 2PrecisionRecallPrecisionþRecall , where precision ¼ TPTPþFP and recall ¼ TPTPþFN.
Apparently, G-mean is based on the recalls of both classes. The benefit of selecting this metric is that it can measure how
balanced the combination scheme is. If a classifier is highly biased toward one class (such as the majority class), the
G-mean value is still low. For example, if a+ = 0 and a = 1, which means none of the positive examples is identified,
G-mean = 0. In addition, F-measure combines the recall and precision on the positive class. It measures the overall perfor-
mance on the minority class. Besides, we utilize the ROC analysis (Swets, 1988) to assist the evaluation. A ROC curve dem-
onstrates a trade-off between true positive and false positive rates provided with different classification parameters, and
illustrates how the accuracy on positive instances drops with the error rate on majority instances. Informally, one point
in ROC space is superior to another if it is closer to the northwest corner (TP is higher, but FP is lower). Thus, ROC curves
allow for a visual comparison of classifiers: the larger the area below the ROC curve, the higher classification potential of
the classifier.
6.2. Benchmark data
We use eight datasets as our testbeds. Four of them are from the UCI Machine Learning Repository (Frank & Asuncion,
2010) and the rest four datasets are clinical data from Institute of Clinical Evaluative Science (ICES from Canada) and National
Cancer Institute (NCI from United States). Each dataset in this study is randomly split into training and test subsets of the
same size, where a stratified manner is employed to ensure that the training and test sets have the same imbalance ratio.
The four UCI datasets are spambase, letter-recognition, pima-Indians-diabetes and abalone, which have been extensively
adopted in evaluating the performance of various types of classifications. Here the datasets carefully are selected based
on the following principles.
1. Obtained from real applications.
2. Demonstrate distinctive feature characteristics.
3. Vary considerably in size and imbalance ratio.
4. Maintain sufficient amount of instances to keep the classification performance.
Besides, we particularly choose four clinical datasets to testify the effectiveness of our proposed method as we believe
that one of the important motivations of imbalanced classification is to diagnose rare medical conditions. Three of the data-
sets from ICES are used for the detections of myocarditis, hypertension, and smoker respectively, and the medical compound
Table 1
Two-class confusion matrix.
Predicted Positive Predicted Negative
Actual Positive TP (True Positive) FN (False Negative)
Actual Negative FP (False Positive) TN (True Negative)
626 Y. Liu et al. / Information Processing and Management 47 (2011) 617–631
dataset (mcd) from NCI is for discovering new compounds capable of inhibiting the HIV virus.2 The information on each data-
set exploited in our experiments is tabulated in Table 2.
6.3. Experimental results
In this section, we first compare our proposal to a collection of the state-of-the-art solutions. To demonstrate the effec-
tiveness of our proposed data processing strategy, we then compare its performance with other similar methods. Finally, we
investigate the algorithm parameter K, which allows users to fine tune the system for optimal performance, and study how
the choice of various parameter values affects the classification accuracy wrt. different methods.
6.3.1. Comparison with alternative methods
In this section, we compare the performances of our proposed EnSVM and EnSVM+ methods with those of five other
methods:
1. single SVM without re-sampling the data,
2. single SVM with over-sampling using SMOTE (Akbani et al., 2004) (without applying cost functions),
3. random forest with balanced training data from under-sampling (Chen et al., 2004),
4. random forest with our combined sampling method, and
5. single SVM with our combined sampling method.
In our experiments, for all the SVMs, we employed Gaussian RBF kernels of the form K(xi, xj) = exp(cjxixjj2) of C-SVMs.
For each method we repeated our experiments ten times, computed average G-mean values and F-measures. Results in
terms of G-mean are shown in Table 3, where SVM denotes the single SVM method with the original training data, SMOTE
represents over-sampling the minority class and then training a system with single SVMs, Rand-Forest1 denotes under-
sampling the majority class and then making an ensemble with C4.5 decision trees, Rand-Forest2 denotes sampling data with
our combined method, followed by forming an ensemble with C4.5, AvgSVM denotes the average performance of 10 single
SVMs with our sampling method, EnSVM is our ensemble method with the combined sampling method, and EnSVM+ is our
selective ensemble method. For the first two datasets, the K values for SMOTE, EnSVM, and EnSVM+ can only be set to be 1
since their imbalance ratio is 2:1. For each of other datasets, we test two K values: the smallest value and the highest value.
The smallest value is 1 for all the data sets, and the highest value depends on the imbalance ratio of the data set, e.g., the
maximum value is 9 for the spambase data.3
Tables 3 and 4 show the performance for each method in terms of the G-mean and F-measure respectively. We can see
that EnSVM or EnSVM+ achieves the highest G-mean and F-mean on all datasets. In particular, with our proposed EnSVM+ and
EnSVM, big improvements can be observed on the datasets where the imbalance ratios are large. Besides, exploiting selective
ensembles with EnSVM+ may further boost the classification performance compared with using all classifiers in EnSVM, and
the algorithms tend to enjoy a better performance while K is large.
Finally, to verify whether there exists a statistically significant difference between our proposal and other methods, we
conduct Student’s t-test with a form of T = Z/s, where Z is
ffiffiffi
n
p
X=r, s is r̂=r, and X is the sample mean of the data, n is the
sample size, r is the population standard deviation of the data, r̂ is the sample standard deviation. As a case study, we com-
pare the baseline SVM method with EnSVM+ under two scenarios, namely EnSVM+ with K = 1 and EnSVM+ with K = highest.
The t-values of the test are then calculated with G-mean measure, and the outcomes are 2.7255 and 2.5267 respectively.
Using a table of values from Student’s t-distribution, we found that the p-value is 2.365 with a threshold of 0.05. As a result,
Table 2
Benchmark datasets.
Dataset Size Attributes Imbalance ratio
Letter 20,000 16 2:1
Pima 768 9 2:1
Spambase 3068 57 10:1
Abalone 4280 8 40:1
Myocarditis 12,000 74 31:1
Hypertension 22,051 19 19:1
Smoker 9486 9 12:1
mcd 29,508 6 100:1
2 We have also developed a software package for automatic diagnosis of diseases based on our proposed methods. It is part of the project supported by the
Institute of Clinical Evaluative Science, and the program will be available upon request.
3 In Table 3, from top to bottom, the optimal c obtained empirically in using SVMs is 1.0  102, 5.0  105, 7.0  102, and 102 respectively. In addition, C is
set to be 1000 for each case.
Y. Liu et al. / Information Processing and Management 47 (2011) 617–631 627
the null hypothesis is rejected in favor of the alternative hypothesis, meaning SVM and EnSVM+ are different under the
significant test.
In addition to the fact that our proposal yields a better result at large, we also make the following observations on the data
studied.
1. By comparing the results from the four SVM methods, we can see that (1) using SMOTE to over-sample the data is better
than SVM without any sampling; (2) using the ensemble method together with the combined sampling method achieve
the best results.
2. By comparing the two Random Forest methods, using the combined sampling method is better than using only the under-
sampling method on most datasets.
3. Between the Random Forest method and the ensemble of SVMs method, the ensemble of SVMs performs better.
In Section 6.1, we present a brief example of how class distribution may affect the classification accuracy. To further shed
some light on this phenomenon, we studied a+ and a separately with real datasets, and demonstrate the result in Table 5.
Basically, we find that a generally outperforms a+ for all datasets, and the fact becomes more prominent as the imbalance
ratio getting larger, meaning all classifiers tend to obtain a better overall performance at the loss of sacrificing the classifi-
cation accuracy of some positive examples. Nonetheless, among various methods explored, EnSVM and EnSVM+ are the ones
that got least affected. Comparing with other methods, EnSVM and EnSVM+ always maintain the highest true positive rate
while keeping a decent true negative accuracy. In addition, we notice that compared to SVM-based approaches, under-
sampling the majority followed by combining a group of decision trees merely outperforms the regular SVM in most cases.
The observation indicates that the superiority of making an ensemble with decision trees could be restricted in the problem
of class imbalance.
6.3.2. Effectiveness of impurity pruning
In Section 4.3, we introduce an algorithm of filtering out impurity examples that may potentially mislead the decision
boundary. In this experiment, we compare the performance of our proposal with an alternative that lacks the preprocessing
step of filtering impure examples. We first select the letter and mcd datasets which have very different imbalance ratios, and
then implement the following methods: (1) a single SVM without any preprocessing step, denoted as SVM; (2) a single SVM
with data impurity pruning as described in Algorithm 1, denoted as SVM + IP; (3) a single SVM with our proposed method
which considers both impurity pruning and data sampling, denoted as SVM + IP + Sampling; (4) the baseline EnSVM model,
where K is assigned the highest value, denoted as EnSVM. We finally compare the performance in terms of G-mean and F-
measure, and the results are shown in Fig. 6.
For the case where the imbalance ratio is low, conducting various data preprocessing methods on the letter dataset does
not have a significant impact in terms of G-mean and F-measure, as their values are already high in the baseline SVM
method. However, for the mcd data, where the imbalance ratio is as high as 100, a clear advantage can be observed when
Table 3
Performance in terms of G-mean.
Dataset SVM SMOTE
(K = 1)
SMOTE
(K = highest)
Rand-
Forest1
Rand-
Forest2
AvgSVM EnSVM
(K = 1)
EnSVM
(K = highest)
EnSVM+
(K = 1)
EnSVM+
(K = highest)
Letter 0.9551 0.9552 0.9552 0.9121 0.9281 0.9563 0.9566 0.9566 0.9587 0.9587
Pima 0.6119 0.7320 0.7320 0.7358 0.7002 0.7419 0.7503 0.7503 0.7350 0.7350
Spam 0.8303 0.8364 0.8580 0.8593 0.9050 0.8592 0.8616 0.8988 0.8680 0.9151
Abalone 0.6424 0.6279 0.7800 0.7403 0.7644 0.8001 0.8697 0.8562 0.8770 0.8429
Myocarditis 0.7199 0.7212 0.7291 0.7226 0.7276 0.7413 0.7366 0.7517 0.7588 0.7675
Hypertension 0.7301 0.7349 0.7397 0.7334 0.7455 0.7498 0.7768 0.7480 0.7959 0.7908
Smoker 0.8190 0.8816 0.8970 0.8710 0.9060 0.9114 0.9187 0.9057 0.9264 0.9421
mcd 0.4195 0.4477 0.5817 0.5724 0.5886 0.5902 0.6091 0.6330 0.6157 0.6381
Table 4
Performance in F-measure.
Dataset SVM SMOTE
(K = 1)
SMOTE
(K = highest)
Rand-
Forest1
Rand-
Forest2
AvgSVM EnSVM
(K = 1)
EnSVM
(K = highest)
EnSVM+
(K = 1)
EnSVM+
(K = highest)
Letter 0.9396 0.9394 0.9394 0.8806 0.9062 0.9406 0.9412 0.9412 0.9421 0.9421
Pima 0.5078 0.6567 0.6567 0.6678 0.6165 0.6686 0.6790 0.6790 0.6628 0.6628
Spam 0.7719 0.7685 0.7655 0.7533 0.7918 0.7498 0.8023 0.8109 0.7579 0.8094
Abalone 0.2256 0.2145 0.3242 0.2018 0.2316 0.3206 0.3507 0.3901 0.3302 0.3488
Myocarditis 0.1701 0.1707 0.1752 0.1713 0.1744 0.1805 0.1839 0.1961 0.1999 0.2066
Hypertension 0.2855 0.2897 0.2943 0.2896 0.3075 0.3126 0.3462 0.3366 0.3680 0.3626
Smoker 0.6775 0.7927 0.8356 0.7678 0.8654 0.8753 0.9213 0.8675 0.9799 0.9590
mcd 0.0406 0.0426 0.0936 0.0812 0.0711 0.0571 0.0748 0.1053 0.0736 0.0767
628 Y. Liu et al. / Information Processing and Management 47 (2011) 617–631
impurity pruning is performed. In addition, adopting our proposed sampling strategy could further boost the performance
(SVM + IP + Sampling), and the best result is generated by taking an ensemble (EnSVM).
6.3.3. Comparison with other data processing methods
To further validate our claim that data processing plays an important role in improving classifiers’ performance, we carry
out the following experiment. Given a training data subset, we can bootstrap the majority class instances, such that the vol-
ume of the current majority class is T times that of the minority class, where T is a positive integer smaller than the original
imbalance ratio of the data set. By combining a bootstrapped set of majority instances with all minority instances, we can
form a new training data. By varying the value of T, we can construct a collection of training sets, then learn a collection of
SVMs from training sets, and make an ensemble of SVMs. Table 6 shows experimental results from such an ensemble of
SVMs on abalone and mcd datasets. These two datasets are adopted here because from Tables 3 and 4 we have realized that
sampling strategy might be especially crucial while the imbalance ratio is high. In this experiment, eight classifiers are inte-
grated in the ensemble with both datasets, but the value of T varies. In particular, for abalone data, T is 1, 5, . . . , 30, 35 respec-
tively; for mcd dataset, T equals to 1, 13, . . . , 85, 97 respectively. By comparing the results in this table and the results in
Tables 3 and 4, we conclude that this type of ensemble of SVMs does not perform as well as the ensemble of SVMs that uses
our proposed sampling method. Besides, we realize that compared to G-mean, F-measure is more likely to get smaller with
the increase of data volume and imbalance ratio. According to the definition presented in Section 6.1, it is easy to see that F-
measure is positively correlated with its component Precision, which can be further described as
Precision ¼ TP
TP þ FP ¼
aþ  Actual Positive
aþ  Actual Positiveþ ð1 aÞ  Actual Negative :
Table 5
a+ and a prediction accuracy.
Dataset SVM SMOTE
(K = 1)
SMOTE
(K = highest)
Rand-
Forest1
Rand-
Forest2
AvgSVM EnSVM
(K = 1)
EnSVM
(K = highest)
EnSVM+
(K = 1)
EnSVM+
(K = highest)
Letter a+ 0.9461 0.9470 0.9470 0.9006 0.9063 0.9485 0.9487 0.9487 0.9518 0.9518
a 0.9643 0.9635 0.9635 0.9237 0.9505 0.9640 0.9646 0.9646 0.9656 0.9656
Pima a+ 0.4851 0.6567 0.6567 0.6200 0.6418 0.6746 0.6866 0.6866 0.6797 0.6797
a 0.7720 0.8160 0.8160 0.8731 0.7640 0.8160 0.8201 0.8201 0.7949 0.7949
Spam a+ 0.7071 0.7225 0.7750 0.7857 0.8795 0.7879 0.7639 0.8500 0.7742 0.8602
a 0.9749 0.9683 0.9498 0.9397 0.9313 0.9370 0.9718 0.9505 0.9731 0.9735
Abalone a+ 0.4437 0.4245 0.6518 0.6201 0.6589 0.6909 0.8226 0.7819 0.8365 0.7596
a 0.9301 0.9289 0.9335 0.8730 0.8869 0.9265 0.9195 0.9375 0.9195 0.9353
Myocarditis a+ 0.6640 0.6666 0.6800 0.6693 0.6773 0.7040 0.6853 0.7066 0.7200 0.7333
a 0.7805 0.7803 0.7818 0.7801 0.7817 0.7806 0.7918 0.7997 0.7997 0.8033
Hypertension a+ 0.6579 0.6661 0.6742 0.6624 0.6770 0.6833 0.7250 0.7087 0.7577 0.7486
a 0.8102 0.8107 0.8115 0.8120 0.8210 0.8227 0.8323 0.7895 0.8360 0.8354
Smoker a+ 0.7452 0.8616 0.8877 0.8425 0.9027 0.9136 0.9219 0.9014 0.9294 0.9452
a 0.8999 0.9021 0.9062 0.9004 0.9094 0.9093 0.9154 0.9101 0.9245 0.9390
mcd a+ 0.1926 0.2239 0.3624 0.3548 0.3841 0.4012 0.4123 0.4309 0.4212 0.4384
a 0.9137 0.9034 0.9336 0.9231 0.9019 0.8682 0.8999 0.9297 0.8997 0.9000
Fig. 6. Effectiveness of impurity pruning.
Y. Liu et al. / Information Processing and Management 47 (2011) 617–631 629
With the increase of data volume and imbalance ratio, the value of Actual Negative becomes very large rapidly compared
to other variables; therefore Precision drops dramatically. As a consequence, the value of its F-measure becomes very small.
6.3.4. Parameter selection
In addition to the imbalance ratio, the selection of K may also impact on the prediction accuracy of SMOTE, EnSVM, and
EnSVM+. Based on our observation in Section 6.3.1, we realized that for an individual classifier, such as EnSVM, it could be
difficult to track an optimal K for the best classification result, as it might be affected by various factors, such as the imbal-
ance ratio and the underlying data structure. Nonetheless, we found that the ensembled-based solution may generally out-
perform other alternative methods irrespective of different K values.
To further justify our claim, we present a ROC analysis result with the spambase dataset. This dataset is considered here
since it has a moderate imbalance ratio and instance volume. The original spambase has an imbalance ratio of 10; therefore,
in this experiments, we test K from 1 to 9. Through experiments in Section 6.3.1, we notice that SMOTE achieves a better
performance comparing to other state-of-the-art methods; we hence adopt it as a representative to compare with our pro-
posal. Finally, we depict the ROC curves of the two approaches in Fig. 7. Clearly, EnSVM+ outperforms the other two methods
in general.
7. Conclusions
This paper introduces a new approach to learning from imbalanced datasets through making an ensemble of SVM clas-
sifiers and combining both over-sampling and under-sampling techniques. We first show in this study that using SVMs for
class prediction can be influenced by the data imbalance, although SVMs can adjust itself well to some degree of data imbal-
ance. To cope with the problem, re-balancing the data is a promising direction, but both under-sampling and over-sampling
have limitations. In our approach, we integrate the two types of sampling strategies together. Over-sampling the minority
class provides complementary knowledge for the training data, and under-sampling alleviates the over-fitting problem. In
addition, we propose to use ensembles of SVMs to enhance the prediction performance. Through extensive experiments with
real application data, our proposed method is shown to be effective and superior to several other methods with different
data sampling methods or different ensemble methods. We are now working on a method for automatically determining
the value of K based on the data set characteristics in order to optimize the performance of EnSVM and EnSVM+.
Acknowledgments
This work was supported by the National Natural Science Foundation of China (NSFC 60903108), the Program for New
Century Excellent Talents in University (NCET-10-0532), Communications and Information Technology Ontario (CITO),
and Discovery Grants from the Natural Sciences and Engineering Research Council of Canada (NSERC).
0 0.2 0.4 0.6 0.8 1
0
0.2
0.4
0.6
0.8
1
False Positive Rate
Tr
ue
 P
os
iti
ve
 R
at
e
EnSVM+
EnSVM
SMOTE
Fig. 7. ROC curve of spambase dataset.
Table 6
Data processing evaluation.
Dataset G-mean F-measure
Abalone 0.7864 0.3145
mcd 0.5200 0.0696
630 Y. Liu et al. / Information Processing and Management 47 (2011) 617–631
References
Akbani, R., Kwek, S., & Japkowicz, N. (2004). Applying support vector machines to imbalanced datasets. In ECML (pp. 39–50).
Burges, C. J. C. (1998). A tutorial on support vector machines for pattern recognition. Data Mining and Knowledge Discovery, 2(2), 121–167.
Chan, P. K., & Stolfo, S. J. (1998). Toward scalable learning with non-uniform class and cost distributions: A case study in credit card fraud detection.
Knowledge Discovery and Data Mining, 164–168.
Chawla, N. V., Bowyer, K. W., Hall, L. O., & Philip Kegelmeyer, W. (2002). Smote: synthetic minority over-sampling technique. Journal of Artificial Intelligence
Research, 16, 321–357.
Chawla, N. V., Lazarevic, A., Hall, L. O., & Bowyer, K. W. (2003). Smoteboost: Improving prediction of the minority class in boosting. In PKDD (pp. 107–119).
Chen, C., Liaw, A., & Breiman, L. (2004). Using random forest to learn imbalanced data. Technical report 666. Statistics Department, University of California at
Berkeley.
Chen, W., Ma, C., & Ma, L. (2009). Mining the customer credit using hybrid support vector machine technique. Expert Systems with Applications, 36(4),
7611–7616.
Drummond, C., & Holte, R. C., 2003. C4.5, class imbalance, and cost sensitivity: Why under-sampling beats over-sampling? In Workshop on learning from
imbalanced datasets II held in conjunction with ICML ’2003.
Dumais, S., Platt, J., Heckerman, D., & Sahami, M. (1998). Inductive learning algorithms and representations for text categorization. In CIKM ’98: Proceedings
of the seventh international conference on information and knowledge management (pp. 148–155).
Ertekin, S., Huang, J., Bottou, L., & Giles, L. (2007). Learning on the border: Active learning in imbalanced data classification. In CIKM ’07: Proceedings of the
sixteenth acm conference on conference on information and knowledge management (pp. 127–136).
Frank, A., & Asuncion, A. (2010). UCI machine learning repository. Irvine, CA: University of California, School of Information and Computer Science. Available
from <http://archive.ics.uci.edu/ml>.
Fawcett, T., & Provost, F. J. (1997). Adaptive fraud detection. Data Mining and Knowledge Discovery, 1(3), 291–316.
Goldberg, D. E. (1989). Genetic algorithms in search, optimization, and machine learning. Addison-Wesley Professional. January.
Guo, H., & Viktor, H. L. (2004). Learning from imbalanced data sets with boosting and data generation: The databoost-im approach. SIGKDD Explorations,
6(1), 30–39.
Han, J., & Kamber, M. (2000). Data mining: Concepts and techniques. Morgan Kaufmann.
Japkowicz, N. (2000). The class imbalance problem: Significance and strategies. In: Proceedings of the 2000 international conference on artificial intelligence
(IC-AI ’2000) (Vol. 1, pp. 111–117).
Japkowicz, N., & Stephen, S. (2002). The class imbalance problem: A systematic study. Intelligent Data Analysis, 6(5), :429–449.
Kubat, M., Holte, R. C., & Matwin, S. (1998). Machine learning for the detection of oil spills in satellite radar images. Machine Learning, 30(2–3), 195–215.
Kubat, M., & Matwin, S. (1997). Addressing the curse of imbalanced training sets: One-sided selection. In Proceedings of the 14th international conference on
machine learning (pp. 179–186).
Ling, C. X., & Li, C. (1998). Data mining for direct marketing: Problems and solutions. In: KDD (pp. 73–79).
Liu, B. (2006). Web data mining: Exploring hyperlinks, contents, and usage data. New York: Springer-Verlag Inc.
Solberg, A., & Solberg, R. (1996). A large-scale evaluation of features for automatic detection of oil spills in ers sar images. In International geoscience and
remote sensing symposium (pp. 1484–1486).
Stamatatos, E. (2008). Author identification: Using text sampling to handle the class imbalance problem. Information Processing & Management, 44(2),
790–799.
Sun, A., Lim, E.-P., & Liu, Y. (2009). On strategies for imbalanced text classification using svm: A comparative study. Decision Support Systems, 48(1), 191–201.
Swets, J. A. (1988). Measuring the accuracy of diagnostic systems. Science, 240(4857), 1285–1293.
Veropoulos, K., Cristianini, N., & Campbell, C. (1999). Controlling the sensitivity of support vector machines. In International joint conference on artificial
intelligence (IJCAI99).
Warren Liao, T. (2008). Classification of weld flaws with imbalanced class data. Expert Systems with Applications, 35(3), 1041–1052.
Weiss, G. M., & Provost, F. J. (2003). Learning when training data are costly: The effect of class distribution on tree induction. Journal of Artificial Intelligence
Research, 19, 315–354.
Wilson, D. R., & Martinez, T. R. (2000). Reduction techniques for instance-based learning algorithms. Machine Learning, 38(3), 257–286.
Wu, G., & Chang, E. Y. (2004). Aligning boundary in kernel space for learning imbalanced dataset. In ICDM (pp. 265–272).
Zhou, Z.-H., Wu, J., & Tang, W. (2002). Ensembling neural networks: Many could be better than all. Artificial Intelligence, 137(1-2), 239–263.
Y. Liu et al. / Information Processing and Management 47 (2011) 617–631 631
