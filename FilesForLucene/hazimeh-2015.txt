A Study on Authorship Attribution
Hussein Hazimeh
University of Illinois at Urbana-Champaign
hazimeh2@illinois.edu
Abstract
The authorship attribution task using statisti-
cal and computational methods has a long his-
tory and a variety of applications. The state-
of-the-art authorship attribution techniques
rely on machine learning classifiers and large
stylometric feature sets that try to capture the
writing print of authors. In this paper we
study several problems in authorship attribu-
tion that have not been thoroughly investigated
in the literature. We run experiments to quan-
tify the minimum number of articles per au-
thor needed to get a good attribution accuracy.
We also study different combinations of stylo-
metric features and learning algorithms while
varying the number of candidate authors. This
allows us to get an idea about the maximum
number of authors for which we can still make
reliable inference. Finally, we shed light on
the problem of imbalance where some of the
authors in the candidate set have many articles
while others have only few, and we empiri-
cally show that the problem can solved using
traditional techniques.
1 Introduction
The task of authorship attribution is an old and in-
teresting problem that dates back to the Medieval
period when it was used determine the credibility
of literary works. Nowadays, authorship attribution
has several important applications in the judicial sys-
tem to determine the authenticity of suicide notes,
harassing messages, and the authors of source code
of malicious programs (Stamatatos, 2009; Chaski,
2005). Its applications are burgeoning in other fields
as well; for example search engines can greatly ben-
efit from authorship attribution techniques to differ-
entiate the papers of authors having the same first
and last names. Formally, the pure version of the
authorship attribution problem can be stated as fol-
lows: given a collection of texts whose authors are
known and an anonymous document, the task is to
determine which of these authors wrote the anony-
mous document.
This problem has been extensively studied in
the literature and several methodologies have been
used. The early statistical works were based on vec-
tor space models where simple stylometric features
(such as average sentence length) are used to rep-
resent documents, and a distance measure is used
to quantify the similarity between the anonymous
document and another document whose author is
known. The state of the art authorship attribution
methods are based on machine learning techniques
where the problem is viewed as a standard multi-
class classification problem, and a large number of
stylometric features is utilized. The machine learn-
ing systems are currently using a variety of features
including lexical, character-based, syntactic, and se-
mantic features, and are being trained using different
algorithms (Koppel et al., 2009).
In this paper we study several aspects of the au-
thorship attribution problem that have not been in-
vestigated thoroughly in the literature. The first as-
pect is investigating the effect of the corpus size on
the accuracy of the attribution. In some applications
there are thousands of articles available for each au-
thor, while in others few texts or articles are avail-
able. This motivates the question: What is a “good”
number of articles to have for the attribution method
to give a good accuracy? The second aspect is the ef-
fect of the number of authors on the attribution accu-
racy. Intuitively, as the number of authors increases
we expect the accuracy to go down, and this is well
known in multiclass classification problems where
the accuracy decreases as the number of classes in-
creases. However, it is important to quantify this
decrease in accuracy in order to get a bound on the
maximum number of authors for which we can still
make reliable attribution. This triggers us to search
for the combinations of features and learning algo-
rithms that give the most robust results as the num-
ber of candidate authors increases. The third aspect
that has not been well documented in the literature
is the problem of imbalance (Stamatatos, 2009). In
many situations we have a plethora of textual ev-
idence for some authors but only few for others,
however most of the research has been focused on
datasets that are balanced in terms of the number
of training articles per author. Thus, we also try to
study if the current proposed attribution methods can
deal with the imbalance problem.
2 Relevant Work
As this paper studies the problem of authorship at-
tribution in a machine learning framework, we will
only report on some of the state-of-the-art methods
used by the machine learning community. For a
comprehensive review of some of the older methods
based on the vector space model and distance heuris-
tics see (Koppel et al., 2009). The majority of the
current attribution methods are based on the classi-
cal text categorization problem while using features
that are more suited to the nature of the attribution
task. Thus, articles written by authors are viewed
as vectors of numerical features and the learning al-
gorithm tries to minimize an empirical classification
loss function. As the research on learning algorithms
has led to great improvements over the last decades,
the main hurdle for the attribution problem remains
in the choice of features. Now we present some of
the main features used for attribution tasks, and then
we report on some of the major algorithms and re-
sults.
2.1 Stylometric Features
Function Words Function or stop words are very
common words that carry no semantic significance
but are used to structure sentences. These words
include but are not limited to conjunctions, prepo-
sitions, and pronouns. They are usually ignored
in standard text classification tasks, however, for
authorship attribution they have proven to be one
of the best features for discriminating between au-
thors (Argamon and Levitan, 2005; Zhao and Zobel,
2007). It has also been reported that function words
are used unconsciously by authors and are indepen-
dent of the text’s topic which favors their usage (Sta-
matatos, 2009).
Syntactic Features These features are based on
the output of syntactic analysis of texts using natu-
ral language processing tools. For instance, a widely
used syntactic feature is the frequency of Part of
Speech (POS) tags that have shown to be a very good
discriminator between authors. However, one main
setback when using POS tags is accuracy of the POS
tagger especially for authors whose writing is known
to be complex in terms of sentence structure (Sta-
matatos, 2009; Koppel et al., 2009).
Content Words These are words that carry a se-
mantic meaning and are peculiar to the author. For
example, one author might always prefer to use the
word “great” while another author would almost al-
ways use the word “excellent” to express the same
idea. The main problem with these features is that
they are not robust over the change in topics; these
words may be very dependent on the genre of the
writing and on the mood of the author (Koppel et
al., 2009).
Character n-grams Short character n-grams
(Bigrams and Trigrams) are reported to have the
highest discriminatory power across other stylo-
metric features in many setups (Stamatatos, 2009).
(Grieve, 2007) have shown that character Bigrams
can achieve up to 94% accuracy when discriminat-
ing between 2 authors. It should be also noted that
the size of n-grams should not be increased much
to avoid capturing content specific features (Grieve,
2007; Koppel et al., 2009)
A variety of learning algorithms have been
employed and in most of the cases support vector
machines have shown to perform at least as good
as other learning algorithms (Stamatatos, 2009).
(Koppel et al., 2009) have shown that Bayesian
multiclass regression and one variant of Winnow
can slightly outperform SVMs on certain types
of features. As for the attribution accuracy, the
values vary a lot in the literature due to the different
genres of texts, features, and setups. But the trend
is that for a small number of authors and a sufficient
number of documents accuracies in the order of
80% and 90% can be achieved when the number of
authors is below 10. For a larger number of authors,
the accuracy drops to lower values (Grieve, 2007;
Chaski, 2005).
3 Experiment Setup
3.1 Corpus
Our experiments are based on The Guardian’s arti-
cles between 2009 and 2014. The corpus is formed
of three datasets: the large dataset which contains
100 authors with an average of 1600 articles per au-
thor, the medium dataset which contains 100 authors
and an average of 37 articles per author, and the
small dataset containing 100 authors with 10 arti-
cles each. The average article length in the whole
corpus is 675 words. We extracted the articles using
the Guardian’s API and processed each article by re-
moving all the HTML content not corresponding to
actual text (such as images, hyperlinks, and ads) and
then performed tokenization using NLTK.
3.2 Feature Extraction
We used 3 main categories of features in our ex-
periments: Function Words (FWs), Part of Speech
(POS) Tags, and Character Bigrams. The FWs used
are based on a list of 337 words from Sequence Pub-
lishing. For each article in the corpus, we use the
frequency of each of the FWs as a feature so each
article is represented as decimal vector of dimen-
sionality 337. The POS tags were extracted from
each article using NLTK trained on the Penn Tree-
bank which outputs one of 45 predefined categories
for each word. We used the frequency of each of the
possible POS tags as a feature so each article is rep-
resented as a decimal vector of dimensionality 45.
For the character Bigrams, we extracted the top 1000
most frequent Bigrams from the corpus and used the
frequencies of each of the top Bigrams as features.
Thus, each article is represented as a decimal vector
of dimensionality 1000 when using the character Bi-
grams. We also experimented with combinations of
these features as presented in the evaluation section.
Feature Type Count Example
Function Words 337 And
POS Tags 45 Noun Phrase
Char Bigrams 1000 bu
Table 1: Features Used in the Experiments
3.3 Learning Algorithms
We used 5 different algorithms implemented in
Scikit Learn in the learning phase. The first is Sup-
port Vector Machine (SVM) where we used the soft
version of the algorithm as a binary classifier and the
One vs All (OvA) scheme to extend it to multiclass.
We also experimented with the One vs One (OvO)
and Error Correcting Codes Decomposition (while
varying the code length) and both gave very simi-
lar similar results to the OvA. Thus, we report all
the results using the OvA scheme. The second algo-
rithm is Logistic Regression (with L2 regularization)
used along with the OvA scheme. The main moti-
vation behind using Logistic Regression is to check
if it can give similar performance to SVMs, given
that its computationally more efficient than SVMs
(based on Scikit Learn implementations). The third
algorithm is Random Forests which is an ensemble
algorithm that uses bootstrapping when sampling
from the training data and trains full length deci-
sion trees on subsets of the training data. In the
surveyed literature we did not see any usage of Ran-
dom Forests, and given their versatility and ability
to learn non-linear concepts (Breiman, 2001) we de-
cided to try them for the task of authorship attribu-
tion. The fourth and fifth algorithms are Gaussian
Naive Bayes and simple Decision Trees using the
Gini Impurity heuristic (we also tried the Informa-
tion Gain heuristic which appeared to be inferior).
The purpose of including the last two algorithms is
to check if the data can be separated using simple
hypotheses that are computationally much more ef-
ficient when compared to the first three algorithms.
Note that only the first three algorithms have pa-
rameters to tune, while Gaussian Naive Bayes has no
parameter to tune and the Decision Tree is allowed
to grow to full length and thus has no variable pa-
rameters. We fixed the number of trees in Random
Forests to 5000 in all experiments since this number
was giving very good results for almost all runs (we
tried increasing the number to 10000 and got no im-
provement of statistical significance). We tuned the
parameter C (which is the penalizing factor for the
empirical risk) for SVM and Logistic Regression us-
ing 5-fold Cross Validation. In all the experiments,
we train on 70% of the training instances and use the
remaining 30% for testing.
4 Evaluation and Discussion
4.1 Effect of Corpus Size
Our experimental hypothesis states the following:
There is a minimum number of articles per au-
thor for which the accuracy of attribution wouldn’t
change, even if we keep on increasing the number of
articles per author. The intuition behind this hypoth-
esis is that we are identifying each author by looking
at “samples” of his or her stylistic features. For ex-
ample each sample can be a vector of Function Word
frequencies. Assuming that the stylistic features of
each author are invariant we expect that there is a
point at which drawing more samples would not give
any more information about the distribution of sty-
lometric features.
To study the effect of the number of articles per
author on the attribution accuracy we ran the fol-
lowing experiment on the small, medium, and large
datasets which contain 10, 37, and 1600 articles per
author (on average), respectively. For each dataset,
we used Function Words as features and trained 10
SVM classifiers: the first on 10 authors, the sec-
ond on 20, ..., and the tenth on 100 authors. The
plot of the variation of accuracy for each dataset is
shown in Figure 1. The results show that the large
and medium datasets have the same accuracies on
most of the points with slight variations on others.
However, the small data set has lower accuracies on
all the points and the rate of accuracy drop as the
number of authors increases is much steeper. For in-
stance, the accuracy dropped from 68% to 35 % as
the number of authors increased from 10 to 40, how-
ever for the other two datasets the drop was much
smaller: from 75% to 59%. These findings verify
our initial hypothesis; we have found that for a very
small number of articles per author (10 in our case)
the attribution accuracy is relatively low and this can
be explained by the small sample size which does
not explain the real distribution of Function Words
accurately. However, when the number of articles
per author is large enough (37 in our case), the at-
tribution accuracy does not vary even if we keep on
increasing the number of articles per author to thou-
sands (1600 per author in our case) since the number
of articles in the medium dataset was large enough
to represent the distribution. It should also be noted
that the length of each article will have an effect on
the attribution accuracy, so the figures we got may
not apply to cases where the length of articles is
much different than that in our corpus (the average
article length in our corpus is 675 words).
4.2 Effect of Number of Authors
In this section we compare the performance of dif-
ferent combinations of features and algorithms as
the number of authors increases. The purpose is to
find the feature set that can give the best results for
a high number of authors, in addition to the features
that are robust (i.e. their performance wouldn’t vary
much as the number of authors increases). All the
experiments in this section are based on the medium
dataset as it proved to exhibit similar performance to
the large dataset. We trained all the algorithms men-
tioned in the Experimental Setup section on the fol-
lowing 5 different sets of features: Function Words,
POS Tags, Char Bigrams, Function Words + POS
Tags, and Function Words + POS Tags + Char Bi-
grams. For each set of features and each algorithm,
we evaluate the attribution performance on 10 up to
100 authors in increments of 10. Tables 2 and 3
show the attribution accuracy in the cases of 10 and
100 authors respectively, for the different combina-
tions of features and algorithms. The highest accu-
racy for each feature type is colored in red, however
it should be noted that the output of the SVM, logis-
tic regression, and random forests are very close and
in most cases the difference in performance is not
statistically significant (based on a 95% confidence
interval).
Tables 2 and 3 show that Character Bigrams have
Figure 1: Graph showing the effect of the number of articles per author on attribution accuracy. The small
dataset contains 10 articles per author, the medium has an average of 37 per author, and the large has an
average of 1600 articles per author.
Logistic Regression SVM Random Forest Gaussian NB Decision Tree
FW 0.736 0.744 0.769 0.364 0.380
POS 0.701 0.678 0.727 0.587 0.438
FW+POS 0.777 0.777 0.777 0.388 0.479
Bigrams 0.864 0.826 0.843 0.686 0.537
FW+POS+Bigrams 0.818 0.810 0.860 0.570 0.628
Table 2: Attribution Accuracy for 10 Authors
the highest performance compared to all other com-
binations of features in case of small and large sets
of authors. The performance difference between Bi-
grams and the first 3 feature combinations in tables
2 and 3 is statistically significant (based on non-
overlapping 95% confidence intervals). One non-
intuitive observation is that the combination of FWs,
POS tags, and Bigrams has a slightly lower perfor-
mance when compared to Bigrams alone. The rea-
son behind this observation might be due to the fact
that the Bigrams are highly correlated with the func-
tion words and POS tags. For instance, the word
“at” which appears in the list of function words is
captured by the Bigrams. Thus, when combining all
three types of features, many of the individual fea-
tures will be highly correlated or even duplicated (as
in the case of “at” , “or”, “as”, etc..) which will
not give the learning algorithm any new informa-
tion compared to the case of Bigrams alone. We
also tried performing Principal Component Analy-
sis to select the best 1000 features when combin-
ing all feature types, however this did not lead to
performance improvement over Bigrams alone. The
third best performing feature set is the combination
of Function words and POS tags which outperforms
the cases of FWs alone or POS tags alone. As for the
algorithms, Logistic Regression, SVM, and Random
Forests have roughly the same performance and the
Logistic Regression SVM Random Forest Gaussian NB Decision Tree
FW 0.476 0.431 0.472 0.200 0.177
POS 0.485 0.454 0.469 0.371 0.178
FW+POS 0.582 0.585 0.569 0.222 0.201
Bigrams 0.697 0.639 0.632 0.463 0.253
FW+POS+Bigrams 0.679 0.687 0.657 0.319 0.277
Table 3: Attribution Accuracy for 100 Authors
Figure 2: Graphs showing the variation of accuracy as the number of author increases
slight differences can be attributed to the tuning pro-
cess. But in general, Logisitc Regression was up to
5 times faster than SVM when running the experi-
ments which might give it an advantage if the appli-
cation requires real time training.
Figure 2 shows the change in attribution accuracy
as the number of authors increases in increments
of 10. The red curve corresponds to Logistic Re-
gression trained over Bigrams which gave the high-
est performance over other choices of features. The
blue curve is the result of an SVM trained over the
combination of Function Words and POS tags (note
that the SVM slightly outperformed logistic regres-
sion in this case). One observation is that the differ-
ence in accuracy between the Bigram and the combi-
nations of FWs and POS tags is roughly constant as
the number of authors increases and is around 10%.
Also, the attribution accuracy appears to go down by
roughly 2% as the number of authors increases by
10. Therefore, we can conclude based on the empir-
ical observations that Bigrams and the combination
of FWs+POS are equally robust; i.e. they have the
same rate of accuracy drop as the number of candi-
date authors increases.
4.3 Effect of Imbalance
To study the attribution accuracy when some authors
have much larger number of articles than others we
conducted the following experiment. We selected 5
authors with 1000 articles each (denote these authors
as Group A) and another 5 authors with 10 articles
each (denote these authors as Group B). We trained,
Group A Group B
SVM 0.832 0.125
Weighted SVM 0.7754 0.5151
(a) Attribution Accuracy for an imbalanced
set of 10 Authors using FW+POS as features.
Group A: 5 authors with 1000 articles each.
Group B: 5 authors with 10 articles each
Group A Group B
SVM 0.948 0.183
Weighted SVM 0.928 0.8
(b) Attribution Accuracy for an imbalanced set
of 10 Authors using Character Bigrams as fea-
tures. Group A: 5 authors with 1000 articles
each. Group B: 5 authors with 10 articles each
on the set of 10 authors, an SVM over FWs and an-
other one over Bigrams. As shown in tables 4a and
4b, when training using a regular soft SVM the ac-
curacy for the authors with few articles (Group B) is
very low. In the case of FWs+POS, the accuracy for
Group is 0.125 which is slightly above the random
guessing in this case (i.e. we can guess the correct
author with probability 0.1). To deal with this issue,
we tried a weighted version of SVM where the pa-
rameter C is divided by the class frequency for each
class. The weighted SVM results are also shown in
tables 4a and 4b. The weighted SVM scheme sig-
nificantly improved the results for Group B, with a
small reduction in the accuracy for Group A. Thus,
the problem of imbalance can be partly solved us-
ing standard techniques for dealing with the class
imbalance problem such as the weighted SVM we
employed here.
5 Conclusion
In this paper we studied several aspects of the au-
thorship attribution task that have not been thor-
oughly investigated in the literature. We quantified
the effect of the number of articles needed to get a
good attribution accuracy and found that tens of doc-
uments per author can behave very similarly to the
case of thousands of documents per author. We also
studied the different combinations of features and
learning algorithms and concluded that character Bi-
grams can give the best results for any number of au-
thors, and we showed that Logistic Regression and
Random Forests can slightly outperform SVMs in
some setups. Finally we studied the problem of im-
balance in authorship attribution and partially solved
the problem using a weighted version of the SVM.
References
Shlomo Argamon and Shlomo Levitan. 2005. Measur-
ing the usefulness of function words for authorship at-
tribution. In In Proceedings of the 2005 ACH/ALLC
Conference.
Leo Breiman. 2001. Random forests. Machine Learn-
ing, 45(1):5–32.
Carole E. Chaski. 2005. Who’s at the keyboard? au-
thorship attribution in digital evidence investigations.
IJDE, pages –1–1.
Jack Grieve. 2007. Quantitative authorship attribution:
An evaluation of techniques. Literary and Linguistic
Computing, 22(3):251–270.
Moshe Koppel, Jonathan Schler, and Shlomo Argamon.
2009. Computational methods in authorship attribu-
tion. J. Am. Soc. Inf. Sci. Technol., 60(1):9–26, Jan-
uary.
Efstathios Stamatatos. 2009. A survey of modern author-
ship attribution methods. J. Am. Soc. Inf. Sci. Technol.,
60(3):538–556, March.
Ying Zhao and Justin Zobel. 2007. Searching with
style: Authorship attribution in classic literature. In
Proceedings of the Thirtieth Australasian Conference
on Computer Science - Volume 62, ACSC ’07, pages
59–68, Darlinghurst, Australia, Australia. Australian
Computer Society, Inc.
