A Stuctural Approach to Authorship
Attribution using Dependency
Grammars
Victor Wennberg
Victor Wennberg
Fall 2012
Thesis project, 15 hp
Supervisor: Johanna Högberg
Examiner: Johanna Högberg
Bachelor of Science in Computer Science programme, 180 hp

Abstract
Authorship attribution is an important problem, with many appli-
cations of practical use in the real-world. One principal constraint
in dealing with this problem is related to the type of text being
written, and for what purpose — its context. The context of a
text has consequences on the stylistics of the resulting text.
This thesis presents an approach to the problem attempting to
avoid the implications of context by analyzing grammatical struc-
tures, in practice dependency structures derived by computerized
parsing software. For classification, latent semantic indexing is
employed. Results are presented in terms of a comparison, in
terms of performance, with a similar approach based on phrase-
structure trees.
The corpus used in these experiments is a subset of the ICWSM2009
corpus, provided by the International Conference on Weblogs and
Social Media. The subset contains only blog posts, and shows a
high degree of variance in a number of aspects, such as attributes
in the authors and actual textual content.
In conclusion, the approach to the problem of attributing author-
ship appears to be significantly weaker than its phrase-structure
counterpart. The outcome is further discussed, and possible ap-
proaches beyond the realm of authorship attribution is identified.

Acknowledgements
Thanks to Johanna Högberg for supervising this thesis project, providing clear
descriptions when necessary and answering my questions throughout the project!
Thanks also to Lars Bergström for his work on the thesis project upon which this
project has been based.
I also want to thank everyone who knowingly or unknowingly of this project was sup-
portive of me during the, sometimes stressful, work on this thesis project. Thanks!

Contents
1 Introduction 1
2 Preliminaries 3
2.1 Authorship Attribution . . . . . . . . . . . . . . . . . . . . . . . . . 3
2.2 Dependency Grammars . . . . . . . . . . . . . . . . . . . . . . . . . 4
2.3 Latent Semantic Indexing . . . . . . . . . . . . . . . . . . . . . . . . 6
2.4 Unique Terms and Term Selection . . . . . . . . . . . . . . . . . . . 7
2.5 Feature Reduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
3 Problem Approach 9
3.1 Processing of the Corpus . . . . . . . . . . . . . . . . . . . . . . . . . 9
3.2 Dependency Parser . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
3.3 Document Selection . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
3.4 Term Selection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
3.5 Classification . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
3.6 Attribution of Authorship . . . . . . . . . . . . . . . . . . . . . . . . 14
3.7 Comparison of Methods . . . . . . . . . . . . . . . . . . . . . . . . . 15
3.8 Implementation Details . . . . . . . . . . . . . . . . . . . . . . . . . 15
4 Results 17
5 Discussion 21
5.1 Corpus . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
5.2 Term Extraction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
5.3 Latent Semantic Indexing . . . . . . . . . . . . . . . . . . . . . . . . 22
5.4 Parser . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
5.5 Statistical Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
5.6 Implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
6 Conclusions and Future Work 25
References 25
7

1(28)
1 Introduction
A widely recognized definition of authorship attribution is the science of inferring
characteristics of documents written by a particular author [12]. Historically, docu-
ments under unknown, or disputed, authorship have perplexed human beings and
awoken their curiosity as to the true source. From history, a number of particularly
notable examples can be identified such as the book the Hitler Diaries published in
1983, and the conflicts regarding its authorship. Another well known example is the
controversies regarding Shakespeare and his plays[7].
However, deciding authorship only represents the most obvious use of techniques for
authorship attribution. In practice, the field encapsulates a wide range of diverse
application areas such as combating cybercrime[24], de-anonymizing authors on the
Internet, thereby providing notions for the whole concept of Internet anonymity[2].
It might not be apparent at a first glance, but techniques for authorship attribution
are readily applicable to other forms of production, such as source code, works of
art, and music[10].
Historical approaches to the problem are mostly based on, in some cases naive,
stylometric approaches. These include, but are not limited to, quantitiave analysis of
word usage and word frequencies, analysis of richness of vocabulary[16]. The advent
of complex computerized techniques for artificial intelligence and classification have
been a catalyst in the development of novel approaches.
The obstacles involved in attempting to solve this problem is primarily the size of
the author set, that is, the number of potential authors. Another parameter with
significant implications on the tractability of the problem is the amount of data from
which to draw conclusions as to the characteristics of the authors. It appears that for
achieving good performance, despite these obstacles, requires novel approaches[2].
Another problem that is present in attempting to attribute authorship is the context
in which the text was written, and the substantial consequences this has on the
stylistics of the written text. The approach to the problem presented in this thesis
project primarily attempts to circumvent the constraints imposed by context by
employing grammatical dependencies derived from dependency grammar structures,
generated by computerized parsers. For classification latent semantic indexing (LSI)
is utilized.
The experiments within the project is carried out on a subset, described later, of
the IWCSM2009 corpus, containing blog posts from a large number of different
authors[1]. The corpus has a number of interesting details such as high variance
regarding both authors and the context of the documents contained.
The main objective, also encapsulating the working hypothesis, of the project is ana-
lyzing this approach in terms of a comparison with another, highly similar, approach
2(28)
based on phrase-structure trees. The metric of interest is strictly performance-
related, but other issues are brought up as a topic of discussion.
3(28)
2 Preliminaries
This section aims to equip the reader with a basic understanding of the concepts
applied in this report, to enable him or her to easier make sense of the material.
2.1 Authorship Attribution
As mentioned in the Introduction-Section, authorship attribution is the problem
of extracting characteristics from an author and subsequently using them in the
analysis of whether or not this author has composed another piece, or given the
nature of the problem how likely he or she is to have authored that piece[12].
One might ask, how is it possible to attribute authorship of texts? As David Holmes,
stylometrist at the College of New Jersey so eloquently puts it, “People’s unconscious
use of every-day words comes out with a certain stamp”. The very existence of
authors leaving their characteristic marks on their texts can be inferred from this
quote. The possibility of authorship attribution is further reinforced by the fact that
even primitive approaches do yield, although limited, results.
An alternative approach to explaining how authorship attribution is possible is by
looking at a text as a series of choices made by its author. From these choices,
patterns characteristic of its author emerge. By comparing these patterns the au-
thorship of texts can be determined.
By extrapolating the idea of patterns in the choices made by authors, the techniques
developed for authorship attribution can be re-purposed for other forms of produc-
tions in which the creator is frequently faced with choices. Examples include entities
such as artwork, works of music and computer source code.
As to the factors that dictate how difficult a given instance of this problem is to solve,
author set size stands out. The author set is simply a set containing authors that
could have possibly authored the text. Another important parameter is the amount
of text with known authorship available for analysis, that is deriving characteristic
patterns.
Another issue that is a limiting factor for most approaches to the problem suggested
is the context of the texts. Authors tend to change the stylistics of their language
greatly depending on the purpose, or context, of the text being written [7]. In prac-
tice this means that documents from the same author written in different contexts
need not to share any characteristics.
A few other issues worth mentioning is the notion that the style of an author tend to
change during his or her life span. Another issue, similar to the impact of context, is
the use of stylistic envelopes. A stylistic envelope is a passage within a text written
4(28)
in a different style than the surrounding text, primarily used to distinguish dialog
of different characters[7].
Techniques for authorship attribution range from those developed on human beings,
mostly literary scientists, analyzing texts by hand and determining whether or not
it is likely to have been written by one of the authors of the candidate author
set. Approaches to authorship attribution based on using computerized analysis are
often called non-traditional authorship attribution. A few approaches that have been
suggested throughout history to deal with this problem are:
• Analyzing texts in terms of word- or sentence length. This is not a reliable
method.
• Function words: Frequency, position and/or immediate context of certain
context-free words. A context-free word is a word that are used independent
of the context of the text.
• Vocabulary richness: Attempts to measure the richness or diversity of an au-
thors vocabulary, thereby creating a personal profile.
Computerized authorship attribution, that obviously can be done on a much larger
scale than its human counterpart has a vast array of practical application areas, not
limited to:
• Signal intelligence: Identifying text written by for example known terrorists
or criminals
• Forensics: Evaluate the likelihood that a certain text connected to a crime was
written by a suspect
• Solving issues of authorship within literature history[7]: Did Shakespeare write
his own plays?
• De-anonymizing authors on the Web: Identification of a real-world person
responsible for texts published online.
• Detecting hoaxes, frauds and deception online[3]: Automation of Internet spam
detection
2.2 Dependency Grammars
Dependency grammars are a collection of modern syntactic theories, all based on the
idea that words constituting a sentence have dependency relations between them.
The roots of this idea can be traced back to the work by Lucien Tesniere, who
described a sentence as a set of words, with connections between them representing
dependencies between a superior- and an inferior word[19].
A number of representational styles exist for these structures, satisfying different
graph theoretical properties such as acyclicity, multigraphness, etc. An example of
a representational style is basic dependencies, providing a connected, rooted tree[8].
5(28)
Figure 1: Example of a dependency grammar tree structure, with relational labels
included
For an example of a dependency structure using the basic dependencies representa-
tional style, see Figure 1.
For large-scale applications, these representations are commonly extracted using
computerized software called parsers. A number of different parsers are freely avail-
able today, such as MaltParser[14]. MaltParser is a designated dependency parser.
The dependency relations within a sentence can also be extracted from a phrase-
structure tree. The Stanford NLP Parser[13] provides facilities for producing a
dependency structures using a number of different representational styles.
In addition to the raw graphical structures with nodes denoting words and vertexes
denoting relationships, parsers provide additional information to the dependency
relations in the graph. These labels provide a description of the nature of the
relation between the terms.
In regards to these different approaches to parsing, a noticeable trade-off between
speed and accuracy in terms of parsing is present. Designated dependency parsers
achieve noticeable lower accuracy, but much improved speed compared to using con-
stituent parsers and subsequent conversion of its output to dependency structures[6].
6(28)
2.3 Latent Semantic Indexing
Latent semantic indexing, or latent semantic analysis, is a method for machine learn-
ing, that relies on a mathematical technique called singular value decomposition. It
is used to identify patterns in the relationship between terms and concepts within an
unstructured mass of text. The main ideas underlying LSI is that words used in the
same contexts tend to have similar meanings. Experiments has shown that there are
a number of correlations between the workings of LSI and the way humans process
and categorize text[20]. The strength of latent semantic indexing lies in its ability
to overcome the two most problematic constraints in dealing with Boolean keyword
queries: synonymy and polysemy. That is different words with similar meaning, and
words with different, but related meanings, respectively[20].
Latent semantic indexing works by constructing a matrix representation, denoted the
term-document matrix, containing columns for each unique term and rows for each
document. The value at each position in this matrix denote the number of times the
term, given by the column, occurr in the document represented by the row. Terms,
also called features, in this context is simply an attribute in the document.
Commonly, a weighing function is applied to the values within the term-document
matrix, aiming to increase the performance of the model. These functions attempt
to emphasize terms of importance, and disregard those deemed irrelevant. Term
functions are commonly partitioned into global- and local term weighing functions.
The documents that are encapsulated within the term-document matrix are then
projected onto a space of dimensionality k, by applying singular value decomposi-
tion. In effect this partitions the terms into k latent classes. This k-space is said
to represent the important information from the original matrix, and also latent
information from said matrix. It is within this k-space that the actual comparisons
of documents is performed, in terms of vector comparisons.
The choice for dimensionality for the space on which the documents that was used to
construct the model, commonly denoted the k value, is one of the most challenging
tasks when using latent semantic indexing. First, the choice of k-value has linear
impact on the memory usage and computational time in the algorithm. The choice
of k-value also has direct consequences for the actual performance of the algorithm.
Choosing a too high k value will introduce too much noise, and choosing a k-value
too low will imply poor ability to draw conclusions within the model. There are
some systematic approaches to choosing the k value, such as evaluating the entropy,
but in practice these are rather poor. Thus, the generally recognized approach to
choosing k-value is simply trial and error[5].
Latent semantic indexing has a large number of practical applications, primarily
within the field of information retrieval. These include: information discovery, rela-
tionship discovery, automatic keyword annotation of images and interpreting source
code. Another application related to the very topic of this thesis is authorship
attribution, differentiating approaches only by the method for choosing terms.
The complexity of latent semantic indexing leads to the widespread use of established
libraries for this task. Canonical examples include SVDLIBC[15] and LAPACK[17].
By employing a standardized library such as these, the probability of errors are
7(28)
mitigated.
2.4 Unique Terms and Term Selection
Up to this point, terms have only been discussed in vague terms, but being the most
primitive object in the context of latent semantic indexing these have substantial im-
plications for the performance. In this context terms are used to extract information
about the couplings between documents.
The primary difficulty in the selection of terms is that of finding the optimal balance
between the amount of structure and information that is retained in the terms, while
still providing a basis for establishing conclusions regarding the connections between
documents. First, terms need to be present in more than one documents. In data
mining, the number of documents that contain the term is denoted its support-
value[18]
Selecting appropriate terms is hard enough already in the string case, and these
difficulties are aggravated in attempting to extract structures from graph-like struc-
tures, such as trees, compared to more primitive units such as words. Thus, a scheme
for the extraction of terms is required and has to negotiate the difficulties earlier
described.
A topic from the scientific field of data mining that relates to this is the frequent
substructure pattern problem, stated as follows: Given a graph data set, D, and a
minimum support-value, enumerate the substructures from the data set having a
support-value no less than the stipulated minimum value[11].
2.5 Feature Reduction
Another topic related to term selection is feature reduction, sometimes called feature
set selection. The goal is to attenuate the number of terms, while at least retaining
model performance. The principal methodology of these techniques is to pick the
terms deemed most important, yielding a subset of terms. This subset is supposed to
contain the terms from the original set thought to be the most relevant, or expressive,
in terms of construction of the latent semantic indexing model. The use of frequent
substructure patterns, as described earlier with a minimum support-value can be
considered one form of feature set selection[23].
In effect, the feature set selection reduces the dimensionality of the term-document
matrix, thus providing implications on memory usage and computational require-
ments. The performance of the model is also obviously modulated, given the cor-
relation between term selection and model performance. Correct employment of a
feature reduction technique can also increase the ability to generalize in the model.
8(28)
9(28)
3 Problem Approach
This chapter explains how the experiments are conducted. The objective is to pro-
vide a reader desiring to reproduce the experiments with the facilities to do so at
his or her own will. For brevity, no explanations or motivations for the choices are
presented in this chapter, instead these are left for Chapter 5.
Due to time constraints, no thorough description concerning the phrase-structure
experiments will be given, even though ultimately this project aims to statistically
establish the difference in terms of performance between the phrase-structure ap-
proach and dependency approach. The reader wishing to gain a deeper understand-
ing regarding the phrase-structure tests are referred to Lars Bergström’s thesis[4].
For the reader content with a rough outline of Bergström’s approach, it is identical
to this approach up to the choice of tree structures.
3.1 Processing of the Corpus
This section describes the corpus in general, the processing that was undertaken
in order to make the corpus more manageable in terms of size, and also the pre-
processing employed to facilitate subsequent parsing. Note that all of this work,
except the final processing into dependency tree structures, was performed by Lars
Bergström, as a part of his thesis project[4].
The corpus used is commonly known as the ICWSM2009 corpus. The source of
the data that makes up the corpus is the now defunct Tailrank.com (formerly
Spinn3r.com). The website focused on providing feeds of content from the World
Wide Web, distributing the content currently being discussed. The corpus in its
entirety consists of nearly 200 gigabytes of both blog- and news posts in the form of
RSS-feeds. The corpus was published in conjunction with the International AAAI
Conference on Weblogs and Social Media, and is available to the public[1].
This particular corpus is interesting from several perspectives. First, blog posts
are generally different from many other forms of text in terms of for example the
language used. Further, there are no boundaries regarding the context of the texts
in the corpus, implying a wide variety of different type texts. The corpus is also
composed of texts from a set of authors having very high discrepancy, considering
many different attributes.
A subset of this gigantic set of data is then created by selecting authors and their
posts satisfying the following set of criteria[4]:
• All posts written by the author is in English (derived from meta-data)
10(28)
• Total of between 10000 and 100000 characters of posts per authors
• Authors have between 25 and 60 distinct posts satisfying the other criteria
• Every post contains no less than 400 characters
• All posts are of known authorship and source, as defined by the post meta-data
This subset is further made more easily manageable by a computerized parser by a
number of pre-processing passes, each performing one item on the list below:
• Replacement of URL-addresses with its hostname
• Removal of any occurrences of whitespaces and newlines past one
• Repeated interpunctuation signs are replaced with one occurrence
• Accumulating all variants of citation marks and apostrofs under one common
character
• Removal of text within the li and blockquote HTML tags
• Removal of non-ASCII characters
Table 1 Corpus statistics for the original data, and post- filtering, phrase-structure
parsing (denoted P-S) and dependency grammar parsing (denoted Dep.). Values
left out are not available.
Original Filtered P-S Dep.
Number of posts 12873609 316260 191683 191683
Number of unique authors 834744 10580 7506 7506
Number of sentences - - 4638035 4567654
Avg. post length (characters) 2150 3143 1590 1539
Avg. posts per author 15 30 26 26
Avg. sentences per document - - 19.0 18.0
Avg. sentence length (char) - - 80.7 81.6
After above phases the pre-processing, each sentence is parsed using the Stanford
NLP Parser[13], producing the phrase-structure representation for each sentence.
Table 1 shows a dissection of the document statistics before the different phases.
Note that the difference in number of sentences in the Filtered-column as compared
to the columns for the parsings imply that the sentence was unable to be parsed.
The final step in the process of preparing the corpus for experiments is the further
processing of the phrase-structure parser output to generate dependency structures.
Although this process achieves remarkably high accuracy, it is however unable to
process some sentences as apparent from Table 1. The total number of sentences
without dependency representations are 70381. It is important to note that no
documents are left completely void of dependency structures. The fact that sentences
without dependency parsings are simply ignored during the experiments is discussed
in the Discussion chapter.
11(28)
3.2 Dependency Parser
The data constituting the corpus are organized by author and further divided nat-
urally into documents, corresponding to blog entries. These are parsed using the
Stanford Parser[13] and stored in the database used by the main framework, re-
taining the original author/document hierarchy. However, as parsers operate with
textual sentences as the smallest unit this leads to another level in the hierarchy.
3.3 Document Selection
This document describes the selection process for documents. That is how docu-
ments are chosen and further partitioned into the training set and the query set.
Table 2 Parameters considered the document selection process. Note that the mean-
ingful ranges for some of these parameters are confined by the filtering procedure as
described earlier in this section
Parameter Description
authors Number of distinct authors to include documents from
queryDocs The amount documents used to query the classifier
authorMinItems Min. amount of documents for a certain author to be considered
authorMaxItems Max. amount of documents belonging to a certain author
authorMinChars Min. characters for documents to be applicable
authorMaxChars Max. characters for documents to be considered
At first a number of authors are selected at random in accordance with a number
of parameters. These parameters can be seen in Table 2, note that some limit the
total number of authors in the selection by imposing requirements on them. Once
the authors have been selected, all their documents are retrieved from the database
and partitioned into a training set and a query set. The number of documents in
these sets are also dependent on the parameters set.
The training set is used for constructing the model, and the query set are used to
test its performance. Each document in the query set represents one query to the
system after the model are constructed.
The only parameter fixed in the tests conducted in this project are queryDocs, in this
report always set to 1. The reason for using this parameter value is to maximize the
size of the training set for a given set of authors. Further all the parameters consider-
ing the eligibility of authors (authorMinItems, authorMaxItems, authorMinChars
and authorMaxChars) are ignored, because they are already constrained implicitly
by the processing of the corpus.
3.4 Term Selection
This section describes the extraction of terms from the corpus in more detail.
12(28)
Given the dependency structures as presented earlier in this report, a number of
choices is available in the context of extracting terms. First, there is the perhaps
most apparent option as to whether to consider words and/or relation. The tests
conducted within the context of this project only regards relations between nodes,
and not the words themselves.
Table 3 Example grouping of relationship labels derived from the Stanford NLP
Parser manual[8].
Group identifier Member relationship labels
aux aux, auxpass, cop
arg arg, agent, acomp, attr, ccomp, xcomp
complm, obj, doj, iobj, pobj, mark
rel, subj, nsubj, nsubjpass, csubj, csubjpass
mod mod, abbrev, amod, appos, advcl, purpcl
preconj, infmod, mwe, portmod, adv, mod
neg, rcmod, quantmod, nn, npadvmod, num
number, prep, poss, possesive, prt
Further, a form of feature set reduction is applied to the terms extracted after
stripping them of their words, while keeping the relations. This is implemented by
creating so called mergers, or groups consisting of a number of relational labels.
The reason why this reduces the number of features, or terms, is that by lowering
the amount of possible relational labels, the domain of the possible terms is also
restricted.
Presented in Table 3 is the relational merger used for the tests within this report.
These groupings are derived from, and constructed, with the hierarchical structure
presented within the Stanford NLP Parser manual[8] as a starting point. All possible
labels that are not included in the table referred to in this paragraph are not placed
in any particular group but rather make up on a new one containing only that label.
It is important to note that the identifiers for the groupings presented in the table are
merely to facilitate human understanding, and have no computational significance.
Table 4 Example of subtree generation of height 1, at most 2 children. The root
node is presented and the children with their respective relation to the root node.
The merged relation are presented within parentheses, relations not grouped are
represented by -.
Term Root node Child #1 Rel. Child #2 Rel.
1 submitted Bills nsubjpass (arg) were auxpass (aux)
2 submitted Bills nsubjpass (arg) by prep (mod)
3 submitted were auxpass (aux) by prep (mod)
4 Brownback Senator nn (mod) Republican appos (mod)
5 ports and cc (-) immigration conj (-)
A clarifying example of the term selection, as performed within the scope of this
thesis, is provided in Table 4. The example is derived from the dependency tree
13(28)
in the introductory section, Figure 1. Terms are systematically extracted from the
trees by enumerating all possible combinations of subtrees such that its height is 1,
and it has 2 child nodes. Note that the ordering amongst the children is irrelevant,
as this is handled explicitly when comparing terms.
As shown in Table 4, a total of 5 distinct subtrees are generated from this particular
dependency structure. The table includes words, as well as the labellings for the
dependency relations. The grouped relations are also presented, when applicable,
within braces. It is important to note that even though words are included in the
table, they are otherwise ignored.
3.5 Classification
Continuing, all the terms are extracted from the documents that constitute the
training set mentioned earlier. Based on these terms a term-document matrix for
the latent semantic indexing is formed. The columns in this matrix represent every
term that was encountered, on a global basis. Further emphasizing this, every term
that was encountered at least once in the extraction of terms from the documents will
have a corresponding column in this matrix. The rows in the term-document matrix
represent the documents that constitute the training set. Entries in the matrix
denote the term frequency for the term represented by the row of the entry, in the
document represented by the row. Term frequency is the number of occurrences for
the term.
Note that any rows containing only zeros is removed from the matrix. A row con-
sisting of only zeroes correspond to an empty document.
The process of weighing the values constituting the matrix is done in according with
the term frequency-inverse document frequency (abbreviated tf*ldf). This weighing
approach emphasizes terms appearing in few documents. The idea is that these
terms are more characteristic, or unique.
The next step in processing this matrix is the singular value decomposition (SVD)
of the matrix. Mathematically, this process consists in decomposing the term-
document matrix, A, into A = USV T . The resulting matrix U is a m×n unitary
matrix, V is a n×n unitary matrix and S is a diagonal matrix that contains the
eigenvalues. Moreover, approximations of rank k are created by using the first k
columns of U and V and the first k columns and rows of S. The resulting sub-
matrices are denoted Uk, Vk and Sk respectively.
Upon querying the latent semantic indexing system, a n×1 matrix is constructed.
This matrix is generally denoted the query vector, q. This matrix is a representation
of the query document, where the i:th column denotes the term frequency, or number
of occurrences, for the unique term denoted by the i:th column in the term-document
matrix.
However, this query vector still complies to coordinates in the space corresponding
to the original term-document matrix. So, in order to query the reduced k space,
coordinates for this space has to be derived. In practice, these coordinates are
calculates by decomposing the query vector, q, into q = qTUkS−1k .
14(28)
In querying, this query vector, q, is related to the documents constituting the reduced
space of dimensionality k constructed earlier. This is done by calculating the cosine
similarity, that is the angle between two vectors. In this case, the angles calculated
will be between the query vector q and all the documents, denoted di. The highest of
these cosine similarities will be the document most similar to the query vector when
considering the terms in the original term-document matrix, or rather what was
retained after reducing it to k dimensions. The cosine similarity is a value between
0 and 1. The mathematical formula for the cosine similarity is:
sim(q,di) =
qḋi
|q||di|
The strength of using a metric such as the cosine similarity in assessing the similar-
ities between vectors is that by presenting a single value, the issue of high dimen-
sionality and the inability in humans to interpret these spaces are resolved.
3.6 Attribution of Authorship
Figure 2: Illustration of the scoring approach utilizing summation of cosine simi-
larities by author, using a cut-off value of 10.
The primary method for attribution of authorship is to consider the n documents
with the highest cosine similarity in comparison with the query vector, q. This n
value is just a cut-off value. The cosine similarities are then summed according to
author, creating a sum of similarities for each author having documents within these
n. The author whose summed cosine similarities is the highest is the guess regarding
authorship. The process described in this paragraph is clarified in Figure 2. The
example uses a cut-off value of 10. In the figure di denote the distinct documents
and ai denote the unique authors. The sub-graph in the upper-right corner show
the summed cosine similarities, by author.
15(28)
Another possible approach as to the attribution of authorship that is based on the
k best results having the highest cosine similarities is by creating an entry in a table
for every author having documents within this set of k results. Note that this subset
is sorted. Further, this collection of resulting cosine similarities are traversed and
for every document, the author who wrote it is assigned points according to the
documents position in this list. The amount of points alotted are 1 divided by the
current documents position within the list. The author that has the highest amount
of points at the end is guess provided.
3.7 Comparison of Methods
A Wilcoxon signed-rank test is employed for comparing the two approaches statis-
tically. The test aims to assess hypotheses that consider whether or not the mean
of the population ranks differ. Thus, it is a paired difference test and is useful when
the populations cannot be assumed to come from a normal distribution[22].
A number of assumptions have to be satisfied for the test to be applicable[21]:
1. Data is paired
2. Pairs are chosen in a random, and independent manner
3. Data is measured on an interval scale, to enable calculation of differences
A coarse outline of the test procedure contains calculating the absolute value of the
differences for all pairs, and assigning ranks depending on the sorted differences.
Finally the test statistic is produced by summing the absolute value of the sum of
signed ranks for all terms.
3.8 Implementation Details
The implementation of the approach, as described earlier in this section, is realized
as an extension to an already existing framework for authorship attribution. This
system is called Lind, and was described already by Lars Bergström in his thesis[4].
The Lind-framework encapsulates facilities for performing authorship attribution
using latent semantic indexing, and employs a number of different approaches to
term selection. Examples included are stop words and phrase-structure trees, as
well as an approach based on the combination of these. The dependency structure
approach as described in this report is implemented in the manner of these two.
Underlying the system, managing the corpus and all related data, is a MySQL-
database, providing a uniform approach to retrieval.
The framework is primarily coded using the Java programming language, but certain
units employ external programs. An example of an external program used is a stand-
alone C application for performing the computations involved in the singular value
decomposition. For this, functionality in SVDLIBC[15] is used. Further, for man-
agement and manipulation of matrices, the Colt Framework for High Performance
16(28)
Scientific and Technical Computation in Java[9] is used.
17(28)
4 Results
Table 5 This table describes a selection of relevant parameters for fine-tuning Lind,
and the respective values used in the tests.
Parameter Value
authors Varies
queryDocs 1
authorMinItems −1 (ignored)
authorMaxItems −1 (ignored)
authorMinChars −1 (ignored)
authorMaxChars −1 (ignored)
k (for LSI) 150, unless explicitly stated otherwise
n (cut-off in attribution) 20
Table 5 provide a summarizing for the values used for the difference parameters
described earlier. For more information about the parameters and their involvement
in the actual process of analysis, the reader is referred to the section dealing with
the respective parameters, Section Document Selection.
Table 6 Mean number of documents contained in the training set for different sizes
of the author set (n). The number of documents is the average over 25 test runs.
n Number of documents
10 251
20 497
30 769
40 1007
50 1290
60 1525
70 1761
80 2002
90 2264
100 2498
Table 6 shows the mean number of documents for the 25 runs conducted for different
settings for the number of authors considered.
Table 7 illustrates the number of unique terms, as well as the total number of terms
(that is the sum of each terms term frequency) for both the tests using dependency
grammar trees and phrase-structure trees.
18(28)
Table 7 Statistics for the number of terms for different author set sizes (n), showing
the number of unique- and the total number of terms for phrase-structure (UP and
TP respectively) and dependency (UD and TD respectively). The numbers are the
mean over 25 executions.
n UP TP UD TD
10 3190 60405 514 64661
20 5300 183222 628 183962
30 6794 266567 678 256605
40 7302 336198 667 322080
50 8457 436846 692 418574
60 8706 510476 718 509641
70 9617 598914 727 582616
80 10079 672018 715 667327
90 10715 761398 738 733228
100 11314 866720 750 841779
0
0.1
0.2
0.3
0.4
0.5
10 20 30 40 50 60 70 80 90 100
C
o
s
in
e
Number of authors
Maximum cosine assigned to the correct author
Figure 3: Mean summed similarities for different author set sizes (n =
10,20, ...,100) for phrase-structure trees (o), dependency trees (x) and
chance (+)
Figure 3 contain mean summed cosine similarities for authors, for a number of
different author set sizes (n). o indicates phrase-structure trees, x dependency trees
and + chance.
19(28)
0
0.5
1
1.5
2
10 20 30 40 50 60 70 80 90 100
C
o
s
in
e
Number of authors
Maximum cosine assigned to the correct author
Figure 4: Mean score for different author set sizes (n) for phrase-structure trees
(o), dependency trees (x) and chance (+)
Figure 4 show the average scores for different author set sizes (n= 10,20, ...,100) for
phrase-structure trees (o), dependency trees (x) and chance (+).
Table 8 Results from the Wilcoxon signed-rank test in the form of p-values for
different author set sizes, n.
n p-value
10 0.0000005
20 0.0000000
30 0.0001019
40 0.0000043
50 0.0000554
60 0.0000020
70 0.0023172
80 0.0000031
90 0.0000009
100 0.0000001
Table 8 presents the p-values for a number of different author set sizes. With Psid
and Psis denoting the mean summed cosine similarities for the 25 distinct runs, the
20(28)
hypothesis being tested is:
H0 : µd−µs > 0
21(28)
5 Discussion
In addition to more speculative discussions, this section motivates the choices made,
and the pros and cons associated with them.
5.1 Corpus
The parameters relating to the corpus raises the questions as to how much impact the
particular choice of parameter value had on performance, and whether using another
corpus would change the outcome. As already mentioned the corpus displays a high
degree of discrepancy, both in terms of the authors but also in the context of the
documents.
The fact that the corpus has such a high degree of variance regarding the context
of the texts most likely impacts on the performance. However, this does not neces-
sarily mean that is has a negative impact. Quite the contrary, it could actually aid
performance. This is due to the fact that variance in terms of context most likely is
at a level above individual authors, meaning documents written by a certain author
are likely to share contextual traits. As the attribution is carried out in terms of
comparing documents, this might tend towards lowering the score of other authors
due to contextual effects[7].
Further, the linguistic nature of postings on the Internet are likely to impact the
results. This stems from the idea that text published on Internet blogs are more
likely to contain grammatical errors, and tokens such as smileys that make the text
harder to parse. It is possible that sentences unable to be parsed actually express
more characteristics of individual authors.
In addition, the nature of the corpus and most importantly the fact that it is a
collection of Internet postings are obviously very important also. One problem that
can be identified is the usage of acronyms that are commonplace on the Web, but
unable to be parsed by the parser used. One could argue that these have as much,
if not greater, value in terms of expressing characteristic features of authors.
5.2 Term Extraction
Intuitively, the choices made during the extraction of the terms, are highly important
for the performance of the model. A number of caveats can be identified which
adheres to the selection procedure. First, the somewhat crude issue of achieving
overlap between the terms of the documents in such a way that conclusions can be
drawn. By choosing terms in such a way that no overlap is present, terms simply
22(28)
become useless as they are the least significant unit in the context of LSI. On the
other hand, having too high degree of overlap between the terms of documents imply
that no conclusions can be drawn either.
Another, albeit more intricate, aspect that can elaborated further is how to actually
conduct the selection of terms. It may seem impractical to just enumerate all the
trees of a certain size from the structures. A more refined approach that may
yield better results in the real-world would be to actively try to find terms having
a support-value believed to match the optimal level of overlap, and ranking the
importance of terms in accordance with the support value. The implications that this
approach has on the model is that by actively choosing the important structures, a
lesser degree of responsibility is placed upon the classification model for emphasizing
and suppressing terms.
By analyzing the statistics regarding the number of unique terms, and the total
number of terms presented in 7, it can be argued that the unique terms for the
dependency structures are saturated too early. This could have direct consequences
in terms of performance of the latent semantic indexing model. For example, it
emphasizes the weighing of terms further.
Personally, I believe that by modifying the way in which terms are selected, and the
nature of the information contained within them, that performance can be improved
significantly. However, no systematic approaches to term selection exist, and a huge
amount of variations has to be considered. Further, it is a time consuming task
as actual testing has to be conducted for every method. Therefore no systematic
attempts in assessing term selection has thus been performed.
An alternative approach, not implemented herein due to time constraints is to con-
duct term selection in two phases. The first phase would aim to find terms within
documents from a specific author, looking for documents appearing in as many of
these as possible. The second phase would be to select what terms from each author
to consider by using those that occur in as few documents from other authors as
possible. The amount of terms selected per author would of course have be tweaked
for performance.
5.3 Latent Semantic Indexing
An issue when using latent semantic indexing is to unravel the optimal choice of
dimensionality for the vector space, the k-value. Even though there are some ap-
proaches to estimate this value, none of them are of any practical use when applied
to a real-world problem domain. As the comparison presented in this thesis is the
result of tests using different author set sizes (n = 10,20, ...,100) the most rational
approach is to simply conduct tests for the different author set sizes, attempting to
find the optimal dimensionality (k-value) by analyzing the outcome.
A critical reader might pose criticism against the fact that in the comparison between
the methods, the same dimensionality (k-value) are applied to both. The main
critique here is partly that the sheer number of terms are significantly different
between the two methods, as seen in table 7. This implies that for the phrase-
23(28)
structure trees, more information is disregarded as compared to the dependency tree
approach. Further, regardless of the number of unique terms, no analysis have been
conducted for finding the number of terms that actually have impact on the models
ability to infer relationships within the model. This have not been done for either
of the approaches.
Term weighing is also a factor in the performance of the model. Choosing the right
approach to weighing is analogous to the choice of dimensionality, that is k-value,
a craft rather than a science. One could argue that the optimal choice in terms of
weighing is determined by the nature of, and features contained in, the text being
analyzed.
A possible approach to weighing, that could either stand on its own or act as an
auxiliary, is to add weighing that does take the authorship of document rows within
the term-document matrix into account. One intuitive idea is to emphasize terms
in proportion to how often they appear within the documents of a given author,
compared to the inverse of how often they appear within the matrix globally. This
would attempt to find terms that are common within texts from each author, but
rare on a global level — that is characteristic terms.
An alternative approach to the construction of the term-document matrix can also
be identified. Instead of constructing the model by considering documents as stand-
alone entries, it is possible to merge all documents from a certain author. The
principal idea behind this is to, taking all documents from an author into account, to
derive a set of characteristics from the author. This would lead to every author being
represented by one row in the term-document matrix. Intuitively this approach does
however ignore the partitioning of text into distinct documents, and thus information
is lost in this regard.
5.4 Parser
As mentioned earlier, a number of parsers are available. The reason for using the
Stanford NLP parser is that it was used in the Lars Bergströms thesis project[4] and
that the parser output from the phrase-structure parsing was provided. This output
could then be used to produce the dependency relations, effectively cutting down
the time to produce the dependency relation to one tenth of the original time. The
fact that the same was also used aids in the analysis when comparing the methods.
Another potential consequence originating from the choice of parsing software is the
remarkably high degree of accuracy that could be achieved. In fact, every document
from the original data set used in Lars Bergströms thesis[4] has at least one sentence
with a valid dependency representation. In practice this means that all documents
eligible for partaking in the analysis can be included. An alternative way of dealing
with these documents is to simply exclude them, however this might lead to the
withdrawal of all documents from that author in the case when the author no longer
satisfies the constraints imposed, as described in the Problem Approach section.
It is also likely that the choice of parsing software had a major role in the fact that
such a high degree of accuracy could be achieved in the parsing of the sentences
24(28)
composing the documents. In fact, every document from the original data used
for testing has at least 1 sentence with a successfully generated dependency repre-
sentation. This means that in practice, all documents are eligible for testing. An
alternative way of dealing with this issue is simply excluding these documents, and
possibly every document by its author depending if the constraints described earlier
are no longer satisfied.
5.5 Statistical Analysis
Critique could be raised against the fact that there exists a mismatch in the number
of sentences that have their dependency relations parsed. This comes from the fact
that the parser was unable to process all the phrase-structure parsings of sentences.
One could say that the phrase-structure approach to authorship attribution is fa-
vored for these documents. However, conveyed to a real-world situation the fact that
the dependency parsing can not process as many sentences as the phrase-structure
parsing has to be taken into account. By looking at the shortcoming in this way
it can be seen as a natural consequence of the choice of approach rather than a
problem.
5.6 Implementation
Limited primarily by the time constraints imposed, testing has not been extensively
conducted. This does not mean the system has not been tested at all, but that the
tests have been analogous with sanity tests, rather than systematic.
In the absence of time constraints, the employment of a more systematic approach
to testing would be favorable. In particular, tests aimed at assessing the correctness
for the more mathematically intensive parts of the system could prove beneficial.
These parts are more prone to errors being more obscure and less apparent.
In conclusion, the lack of systematic testing does in no way directly imply the
presence of errors in the implementation. However, it does increase the likelihood
of it containing actual errors.
25(28)
6 Conclusions and Future Work
In face of the p-values resulting the Wilcoxon signed-rank test, the null hypothesis
can be rejected. It is apparent that this approach is significantly weaker than its
counterpart utilizing phrase-structure trees. This implies that dependency struc-
tures are less expressive in terms of author characteristics than phrase-structure
trees. Further, presented the poor performance of this approach it appears to lack
the performance capacity required to be used solely.
However, one can not deduce from the results that the use of dependency trees
are of no merit in dealing with authorship attribution. It is also likely that the
dependency structures, and the inherent metrics, could prove to be efficient used
as an auxiliary for other, already established, techniques such at stop words. For
example, an additional dimension for the stop words could be added. The level of
the stop word in the dependency grammar structure for the sentence containing it.
Further, it is the very nature of this alternative approach to representing sentences
of text that further metrics about words can be extracted compared to a standard
sentence. These metrics, whoms extraction is trivial, could prove to be of use in
dealing with problems such as text classification.
26(28)
27(28)
Bibliography
[1] ICWSM-11 – 5th International AAAI Conference, 2011.
http://www.icwsm.org/data/ [Online; accessed 05-April-2012].
[2] N. Z. Gong A. Narayanan, H. Paskov and J. Bethencourt. On the feasibility
of internet-scale author identification. In In Proceedings of the 33rd conference
on IEEE Sympsoium on Security and Privacy. IEEE, 2012.
[3] S. Afroz, M. Brennan, and R. Greenstadt. Detecting hoaxes, frauds, and de-
ception in writing style online. In Proceedings of the 33rd conference on IEEE,
Symposium on Security and Privacy. IEEE.
[4] L. Bergström. Syntaxbaserad författarigenkänning. 2010.
[5] R. B. Bradford. An empirical study of required dimensionality for large-scale
latent semantic indexing applications. In Proceedings of the 17th ACM confer-
ence on Information and knowledge management, CIKM ’08, pages 153–162,
New York, NY, USA, 2008. ACM.
[6] D. Cer, M. de Marneffe, D. Jurafsky, and C. D. Manning. Parsing to stanford
dependencies: Trade-offs between speed and accuracy. In In LREC 2010, 2010.
[7] D. H. Craig and A. F. Kinney. Shakespeare, Computers, and the Mystery of
Authorship. Cambridge University Press, 2009.
[8] M. de Marneffe and C. D. Manning. Stanford dependencies man-
ual. http://nlp.stanford.edu/software/dependencies manual.pdf [Online; vis-
ited 2012-April-08].
[9] CERN European Organization for Nuclear Research. Colt, 2012.
http://acs.lbl.gov/software/colt/ [Online; accessed 05-May-2012].
[10] G. Frantzeskou, E. Stamatatos, S. Gritzalis, and S. Katsikas. Effective iden-
tification of source code authors using byte-level information. In Proceedings
of the 28th international conference on Software engineering, ICSE ’06, pages
893–896, New York, NY, USA, 2006. ACM.
[11] Andres Gago Alonso, Jose Medina Pagola, Jesus Carrasco-Ochoa, and Jose
Martinez-Trinidad. Mining frequent connected subgraphs reducing the number
of candidates. In Walter Daelemans, Bart Goethals, and Katharina Morik, ed-
itors, Machine Learning and Knowledge Discovery in Databases, volume 5211
of Lecture Notes in Computer Science, pages 365–376. Springer Berlin / Hei-
delberg, 2008.
[12] P. Juola. Authorship attribution. Found. Trends Inf. Retr., 1(3):233–334, 2006.
28(28)
[13] D. Klein and C. D. Manning. Accurate unlexicalized parsing. In Proceedings of
the 41st Annual Meeting on Association for Computational Linguistics - Vol-
ume 1, ACL ’03, pages 423–430, Stroudsburg, PA, USA, 2003. Association for
Computational Linguistics.
[14] J. Nivre, J. Hall, J. Nilsson, A. Chanes, G. Eryigit, S. Kubler, S. Marinov,
and E. Marsi. Maltparser: A language-independent system for data-driven
dependency parsing. Natural Language Engineering, 13(02):95–135, 2007.
[15] D. Rohde. SVDLIBC, 2012. http://tedlab.mit.edu/˜dr/SVDLIBC/ [Online;
accessed 08-May-2012].
[16] E. Stamatatos. A survey of modern authorship attribution methods. Journal of
the American Society for Information Science and Technology, 60(3):538–556,
2009.
[17] Univ. of Tennessee; Univ. of California, Berkeley; Univ. of Colorado
Denver; and NAG Ltd. LAPACK — Linear Algebra PACKage, 2012.
http://www.netlib.org/lapack/ [Online; accessed 12-May-2012].
[18] Wei Wang, Qing-Qing Yuan, Hao-Feng Zhou, Ming-Sheng Hong, and Bai-Le
Shi. Extracting frequent connected subgraphs from large graph sets. J. Comput.
Sci. Technol., 19(6):867–875, November 2004.
[19] Wikipedia. Dependency grammar — Wikipedia, the Free Encyclopedia, 2012.
[Online; accessed 02-May-2012].
[20] Wikipedia. Latent semantic indexing — Wikipedia, the Free Encyclopedia,
2012. [Online; accessed 02-May-2012].
[21] Wikipedia. Wilcoxon signed-rank test — wikipedia, the free encyclopedia, 2012.
[Online; accessed 21-May-2012].
[22] F. Wilcoxon. Individual comparisons by ranking methods. Biometrics Bulletin,
1(8):80–83, 1945.
[23] Y. Yang and J. Pedersen. A comparative study on feature selection in text
categorization. 1997.
[24] R. Zheng, Y. Qin, Z. Huang, and C. Hsinchun. Authorship analysis in cyber-
crime investigation. In Hsinchun Chen, Richard Miranda, Daniel Zeng, Chris
Demchak, Jenny Schroeder, and Therani Madhusudan, editors, Intelligence and
Security Informatics, volume 2665 of Lecture Notes in Computer Science, pages
959–959. Springer Berlin / Heidelberg, 2003.
