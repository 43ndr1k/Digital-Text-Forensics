Language chunking, data
sparseness, and the value of a long
marker list: explorations with word
n-grams and authorial attribution
............................................................................................................................................................
Alexis Antonia, Hugh Craig and Jack Elliott
Centre for Literary and Linguistic Computing, University of
Newcastle, Australia
.......................................................................................................................................
Abstract
The frequencies of individual words have been the mainstay of computer-assisted
authorial attribution over the past three decades. The usefulness of this sort of
data is attested in many benchmark trials and in numerous studies of particular
authorship problems. It is sometimes argued, however, that since language as
spoken or written falls into word sequences, on the ‘idiom principle’, and since
language is characteristically produced in the brain in chunks, not in individual
words, n-grams with n higher than 1 are superior to individual words as a source
of authorship markers. In this article, we test the usefulness of word n-grams for
authorship attribution by asking how many good-quality authorship markers are
yielded by n-grams of various types, namely 1-grams, 2-grams, 3-grams, 4-grams,
and 5-grams. We use two ways of formulating the n-grams, two corpora of texts,
and two methods for finding and assessing markers. We find that when using
methods based on regularly occurring markers, and drawing on all the available
vocabulary, 1-grams perform best. With methods based on rare markers, and all
the available vocabulary, strict 3-gram sequences perform best. If we restrict
ourselves to a defined word-list of function-words to form n-grams, 2-grams
offer a striking improvement on 1-grams.
.................................................................................................................................................................................
1 Introduction
The n-gram principle is to work with a combination
of units, whether these units are words, letters, char-
acters, parts of speech, or any other well-defined
feature. In this article, we test the usefulness of
word n-grams for authorship attribution by asking
how many good-quality authorship markers are
yielded by n-grams of various types, namely
1-grams, 2-grams, 3-grams, 4-grams, and 5-grams.
We use two ways of formulating the n-grams, two
corpora of texts, and two methods for finding and
assessing markers. We relate the abundance or scar-
city of markers produced by the various types of
n-gram in the different trials to the larger factors
underlying the operations of word n-grams: the in-
crease in data sparseness—more classes, each with
fewer members—with longer n-grams, which leads
to fewer useful authorship markers; the increase in
the number of markers yielded with longer n-grams,
which provides a larger pool of possible markers;
and the inherent ‘chunkiness’ of language, which
Correspondence:
Hugh Craig, Centre for
Literary and Linguistic
Computing, University of
Newcastle, NSW 2308,
Australia
Email:
hugh.craig@newcastle.edu.au
Literary and Linguistic Computing  The Author 2013. Published by Oxford University Press on
behalf of ALLC. All rights reserved. For Permissions, please email: journals.permissions@oup.com
1 of 17
doi:10.1093/llc/fqt028
 Literary and Linguistic Computing Advance Access published May 22, 2013
 at A
egean U
niversity on A
ugust 29, 2013
http://llc.oxfordjournals.org/
D
ow
nloaded from
 
might be expected to give longer n-grams an advan-
tage. We find that 1-grams, 2-grams, 3-grams, and
4-grams all perform well in particular trials, while
moving to 5-grams does not in general provide
better performance. When using methods based
on regularly occurring markers, and drawing on
all the available vocabulary, 1-grams perform best.
With methods based on rare markers, and all the
available vocabulary, strict 3-gram sequences per-
form best. If we restrict ourselves to a defined
word-list of function-words to form n-grams,
2-grams offer a striking improvement than 1-grams.
2 Previous Discussions and
Theoretical Considerations
Individual words—1-grams—have been used as
markers in attribution work since the 1960s.
Different strata of individual words have been
used, from the commonest function words to rare
lexical words, and numerous methods have been
used to combine their discriminating power, from
Principal Components Analysis to Random Forests.
A series of notable studies have each settled a vexed
authorship question using individual words
(Ellegård, 1962; Mosteller and Wallace, 1964;
Burrows, 1999; Holmes, 2003; Burrows, 2005;
Holmes, 2010). Methods based on individual
words perform well in assigning texts to authors
in many different benchmark tests and have been
widely adopted. Burrows’s Delta is a good example
(Burrows, 2002; Hoover, 2004; Rybiki and Eder,
2011). The performance of 1-grams in relation to
attribution is well understood after extensive testing
and calibration. There is no doubt that they are
highly effective as markers of authorship.
A strong challenge has been mounted to the pre-
eminence of individual words in attribution, how-
ever, with arguments in favour of longer n-grams
based on the conviction that language is predomin-
ately ‘chunked’, and that idiom, rather than gram-
mar, is the first principle of language analysis. Ian
Lancashire, for instance, argues that
An author’s signature can be found in his rep-
etend clusters, the networks of repeating fixed
phrases and collocations in a text. These
clusters are fragmentary realizations of sche-
mata in his long-term memory. Just as we
recognize a face by its combinations of
visual features, or a signature by its stroke as-
semblages, so we can recognize Shakespeare’s
writing – in the way he believed we do – by its
repeating verbal clusters. (Lancashire, 1997,
p. 137)
‘Repeating fixed phrases’ are what we call here strict
n-grams, i.e. sequences of words as they appear in
the text, and ‘collocations’ are words that appear
within a specified window of each other. The
short-term memory holds two seconds or so of lan-
guage, three or four words. It is through this
memory, the phonological loop, that language is
learned and retrieved; thus words are stored in
phrases and collocations in long-term memory
(Lancashire, 2010, pp. 89–94). For Lancashire an
‘identifiable [authorial] idiolect’ (Lancashire, 2010,
p. 137) depends not on the use of individual words
but on the combination of these short repeated
phrases, which are mainly composed of function
words, with networks of significant lexical words.
In an earlier discussion, he offers some examples
of these networks in a comparison of the language
of the Funeral Elegy for William Peter with the lan-
guage of canonical Shakespeare. This poem is now
known to be by John Ford, but when Lancashire was
writing, in 1997, its authorship was in dispute.
Lancashire notes that in lines 157–82 of the elegy
there are numerous ‘fixed phrases’, what we are call-
ing strict n-grams, in common with Shakespeare’s
Sonnets, such as ‘The many’, ‘day of’, ‘will not’
(Lancashire, 1997, p. 180). The passage and the
Sonnets also have in common some ‘collocates’ or
pairs of lexical words appearing near each other.
Among Lancashire’s examples are ‘CONSUME’
and ‘LIFE’ and ‘TOMB’ and ‘LIE’. For this purpose
the words are lemmatized, so that ‘CONSUM’ST’
can be paralleled with ‘CONSUME’ and
‘INTOMB’D’ with ‘TOMB’. Lancashire comments:
‘Collected within only twenty-five lines, the parallels
make one general point strongly: the same mind
could be responsible for both the elegy and the son-
nets. The common word-networks, small and large,
could be explained by a single long-term associative
memory’ (Lancashire, 1997, p. 180).
A. Antonia et al.
2 of 17 Literary and Linguistic Computing, 2013
 at A
egean U
niversity on A
ugust 29, 2013
http://llc.oxfordjournals.org/
D
ow
nloaded from
 
Brian Vickers has argued that rare word 3-grams
provide superior authorship markers to those fur-
nished by 1-grams. Like Lancashire, he relies on a
prior understanding of the nature of language,
though in his case on work on collocations as
explored in corpus linguistics rather than on neuro-
linguistics. Vickers argues that using words in se-
quences, rather than singly, is essential to effective
authorship attribution: ‘Attribution studies, in
order to succeed, need a linguistic theory and meth-
odology responsive to the fundamental feature of
natural languages, that they ‘weave’ together words
of all kinds in order to create meaning’ (Vickers,
2011, p. 135). ‘[A]tomistic approaches’, based on
single words, are bound to fail in attributing author-
ship in a mode like drama, where function words
‘may tell you something about characters but
cannot reliably indicate authorship’,1 and lexical
words are too sensitive to subject matter (Vickers,
2009, p. 42). Vickers cites John Sinclair, the pioneer
of collocation study in linguistics, who championed
what Sinclair called ‘the principle of idiom’ as a rival
to the ‘open-choice principle’: ‘a language-user has
available to him or her a large number of semi-con-
structed phrases that constitute single choices, even
though they might appear to be analysable into seg-
ments’ (quoted in Vickers, 2011, p. 136).2 Vickers’s
trigrams are unusual sets of three consecutive words
appearing in a mystery text and in an authorial
corpus. He has applied his method to the
Additions to The Spanish Tragedy (Vickers, 2011,
2012) and to the canon of Thomas Kyd (Vickers,
2008).
Thus, the argument runs, language as produced
falls into regular larger units, according to the idiom
principle, and language production may well work
in the brain more by prefabricated units than by
individual words arranged by syntax, and these pat-
terns are better captured by word sequences than by
individual words. Yet, in favour of 1-grams, there is
an important part of style that pertains to individual
words, as the editing process tells us, whether we
seek the mot juste or struggle to improve an awk-
ward sentence. Chunks or grammatical structures
are certainly important, but they can also be seen
to provide slots that can be filled in different ways.
To advance the debate some more specific and
direct testing of individual words against word com-
binations in authorship work is needed, and this is
the aim of the current study.
Researchers have already presented results from
studies comparing the usefulness of separate words
and word sequences in attribution. Hoover found
that very frequent sequences of two words some-
times, though not always, gave better results than
the most frequent individual words (Hoover, 2002).
He reported that the frequencies of 3-grams,
4-grams, and 5-grams were so low in his corpus
that they were not likely to be useful, and indeed
classification results with 3-grams were poor
(Hoover, 2002, p. 162). In a later study he compared
the effectiveness of frequent 1-grams, frequent strict
2-grams, and frequent collocations of pairs of words
within a varying window in attribution. He found
that the collocations often improved on results from
both 1-grams and strict 2-grams (Hoover, 2003).
Argamon and Levitan responded by taking the
same set of novels Hoover had used for his test
but using multiple segments from each novel
rather than a single one. They found that function
words gave superior results to frequent pairs or fre-
quent collocations. They conclude that ‘most of the
discriminating power of collocations is due to the
frequent words they contain (and not the colloca-
tions themselves), thus frequent words outper-
formed collocations, given sufficient data’
(Argamon and Levitan, 2005, p. 2). Eder compared
the performance of various word n-grams and letter
n-grams in classifying authors in text sets in four
different languages. He found that, for his texts in
English, word 1-grams were always the most suc-
cessful, out-performing the other word-grams, all
the letter n-grams tested, and the two combinations
of word and letter n-grams he tried. The results were
strikingly different for the other languages, however.
For the Polish texts, for example, letter 6-grams
were the most successful (Eder, 2011, pp. 109–11).
Most recently Hoover compared the performance of
1-grams, 2-grams, and 3-grams in authorship tests
in a 19th-century novels corpus and in an American
poetry corpus. For both corpora he found that
‘Word bigrams are much less effective than words
and trigrams do a poor job’ (Hoover, 2012, p. 1).
He also replicated Vickers’s rare 3-grams method
Explorations with word n-grams and authorial attribution
Literary and Linguistic Computing, 2013 3 of 17
 at A
egean U
niversity on A
ugust 29, 2013
http://llc.oxfordjournals.org/
D
ow
nloaded from
 
with his text sets, and found that texts known to be
by other authors shared remarkably high numbers
of rare 3-grams with the target author. Using
Vickers’s method with these texts would thus
have led to an erroneous attribution (Hoover,
2012, p. 3).3
One component of the study reported by Peng,
Shuurmans, and Wang is to compare various
methods and n-grams types in classifying texts by
author in a corpus of texts in Modern Greek. In
this case methods using 1-grams proved to be the
most successful (Peng et al., 2004, Table 3). Patrick
Juola ran an open authorship attribution competi-
tion in 2004 and classifies the methods used by
entrants according to whether they used ‘N-
grams or similar measures or syntax in conjunction
with lexical statistics’ or ‘simple lexical statistics’,
reporting that methods using the latter ‘tended to
perform substantially worse’ (Juola, 2008, p. 297).
Of the five methods with the best success rates,
Juola’s summary shows that the best one used in-
dividual words, with occasional recourse to parts
of speech classes, the next best used word n-grams
higher than one and grammatical-class n-grams
higher than one, the third 1-, 2-, and 3-grams,
the fourth strings of letters, and the fifth 2-grams
with some recourse to 3- and 4-grams (Juola, 2008,
pp. 293–6). Sanderson and Günter report that in
their corpus of articles by journalists word 1-grams
performed better in an authorship classification
task than any of the other word n-grams tested,
and character n-grams in turn performed better
than word 1-grams. They suggest that data sparse-
ness is an influence in this result (Sanderson and
Günter, 2006). Finally, R. M. Coyotl-Morales and
her co-authors found that, in classifying a corpus
of Mexican poems by author, using word 1-grams
and word 2-grams together worked better than
including 3-grams as well, probably because of
what they call the ‘data explosion’ with the
longer n-grams in conjunction with their limited-
size corpus. They improved on this base-line, how-
ever, with a method selecting word n-grams by
frequency rather than by the size of n (Coyotl-
Morales et al., 2006).
There are two purely statistical factors influen-
cing the quantity and quality of markers yielded by
n-gram types that are generally mentioned only in
passing in these studies, and deserve fuller consid-
eration. Longer word n-grams yield sparser data,
with more classes, each with fewer instances. This
is an accentuation of the situation with 1-grams,
which already have a distribution ‘characterised by
the presence of large numbers of words with very
low probabilities of occurrence’ (Baayen, 2001 pp.
54–5). In the simplest case, with random data, the
number of different n-grams increases exponentially
with the length of n-grams, and with it the number
of n-grams with only one instance. This combin-
atory principle is a powerful force against which
the recurring patterns of linguistic structure have
to contend to produce the repeated n-grams
needed for attribution. Stamatatos puts it this way:
‘the representation produced by this approach
[2-grams, 3-grams, etc.] is very sparse, since most
of the word combinations are not encountered in a
given (especially short) text making it very difficult
to be handled effectively by a classification algo-
rithm’ (Stamatatos, 2009, p. 6).
The disadvantage becomes clear when one con-
siders the limit case of very long sequences that are
never repeated within a corpus and thus are of no
value to attribution. The most frequent 1- and
2-grams have a normal distribution, more or less,
with all its attendant advantages for attribution. At
its best, with a very common individual word like
the, we can expect at least one instance even in a
very short sample, and a sizeable number of in-
stances in longer ones. We can make firm predic-
tions about the number of instances we expect in a
given class of texts, and that can govern inferences
about the class a disputed text belongs to. Longer
word n-grams follow a Poisson distribution, subject
to the ‘law of rare events’, meaning that it is hard to
read any significance into the presence or absence of
one of these markers in any given text.
Against this is a second principle, that longer
n-grams yield more markers because they offer
more possible combinations. They increase the
pool of markers, and therefore the chances of find-
ing markers that respond to a particular case.
Counting a corpus by grammatical class (how
many nouns, how many pronouns, etc.) yields just
a few variables, with little chance of producing a
A. Antonia et al.
4 of 17 Literary and Linguistic Computing, 2013
 at A
egean U
niversity on A
ugust 29, 2013
http://llc.oxfordjournals.org/
D
ow
nloaded from
 
reliable marker for a particular authorial discrimin-
ation. Counting by lemma yields rather more vari-
ables. Counting by word yields still more, counting
by 2-grams more again, and so on, each time
increasing the chances that the more finely grained
data will yield better markers.
The particularities of the data are the third factor
in the relativities of the various kinds of n-grams.
Word order is more important in a language like
English, with few case markings, for instance, than
in a language like Latin, with many, and this will
affect the number of repeated word combinations
(Eder). Equally, the more predictable and con-
strained the text type, the more repeated 2-grams,
3-grams and so on we can expect. The two quite
different corpora in the present study should, how-
ever, permit some more general conclusions, albeit
strictly for English-language texts.
3 Data and Methods for this Study
In this study the n-gram units are words—words as
they appear orthographically in the texts used—and
we use both straightforward sequences of words,
what we refer to as ‘strict n-grams’, and also co-
occurrences of a select list of words, or ‘skip n-
grams’. For skip 2-grams we find the first instance
of one of the listed words, then move to the next of
them, ignoring any intervening unlisted words. The
second 2-gram begins with the second of these
words and adds the third, and so on. For the
3-grams and beyond, the same procedure is ex-
tended into trios, quartets and quintets of listed
words.
The first corpus is a set of 174 English Renais-
sance plays, dating from between 1576 and 1642.
They are all well-attributed, single-author plays,
and all plays written for performance, rather than
so-called ‘closet’ plays, and all original plays rather
than translations. Here we take three playwrights
with larger canons as our sample authors. The
texts are early printed versions, and thus in old
spelling, but edited so that all the common function
words are in modern spelling and separately count-
able; ‘I’m’, for instance, is edited so that it can be
counted as ‘I am’. The second corpus consists of 254
articles written for Victorian periodicals.4 Here min-
imal intervention is needed as the texts are in
modern spelling.
We look first for regularly occurring markers that
are common, or rare, in an authorial set and cor-
respondingly rare or common in the complemen-
tary set of plays by others. We also look for very
unusual markers, those which occur in the authorial
set but do not occur at all in the corresponding set
by others, or which occur in works by others and
never in the authorial set. Both methods work by
counting segments or works where markers appear,
rather than overall frequencies. Thus a cluster of
occurrences in a single segment has no bearing on
the results—it only registers as one occupied
segment.
Our main interest in the n-grams, as already
noted, is to compare the quantity and quality of
authorial markers that are yielded as we move
from 1-grams through the range to 5-grams. For
each authorial test we withdrew the plays of the
chosen author from the main set and compared
the authorial set with the set of the remaining
plays. For the first kind of test, based on
Burrows’s Zeta (Burrows, 2007), we set the author-
ial set as the ‘base set’ and the non-authorial set as
the ‘counter set’ and took each n-gram type in turn
and worked out an index of its distinctiveness or
otherwise in the base set according to the following
formula:
number of segments in the base
set with one or more instance
 !
total number of segments in the base set
 
þ
number of segments in the counter
set with no instance
 !
total number of segments in the counter set
 
We also found an index value for the distinctiveness
of each word in the non-authorial set, by making
this set the base set and the authorial set the counter
set and applying the same formula.
We collected the index values for the 500
types with the highest index. We thus have ten
sets of 500 index values for each of the three au-
thors, given the five different types of n-gram and
Explorations with word n-grams and authorial attribution
Literary and Linguistic Computing, 2013 5 of 17
 at A
egean U
niversity on A
ugust 29, 2013
http://llc.oxfordjournals.org/
D
ow
nloaded from
 
the commoner-in-author and rarer-in-author tests.
The Zeta index score is a measure of the distinct-
iveness of a marker in the authorial set as opposed
to the non-authorial one, and thus indicates the
usefulness of the marker for attribution. The score
is one straightforward answer to the question, ‘To
what extent does this feature appear regularly in one
sample, and rarely in the other?’
We also used a second method to identify mar-
kers, a variant of Burrows’s Iota test (Burrows,
2007). We first discarded all n-grams which ap-
peared in a counter set, and of the remainder, all
n-grams that appeared only once in the base set.
We were left with a set of repeated unique markers
of the base set, i.e. n-grams that appeared twice or
more in the base set and never in the counter set. In
this second test, we used the number of these mar-
kers as the measure of the fruitfulness of a given kind
of n-gram for authorial classification. The question
here is, ‘Does this feature appear more than once in
one sample, and never in the other?’
The measures we chose, the average of the top
500 Zeta index values, and the number of Iota mar-
kers, are simple and transparent measures of the
degree to which two language samples are different.
We analysed the quality and quantity of authorial
markers in this way, rather than success in classifi-
cation, so as not to introduce more arbitrary factors
through a choice of classification methods. Hoover,
for example, notes that his chosen method, cluster
analysis, varies greatly in accuracy, depending on the
particular type of cluster analysis used and on the
particular combination of variables employed
(Hoover, 2002, p. 170, n. 14). By stopping earlier
in the classification process we hope to provide a
more primitive, but more robust, estimate of the
usefulness of different kinds of n-gram for
attribution.
4 Results for a Sample Author
In a later part of the article we present results for the
six authors in the two corpora, but it is convenient
to display and discuss the results for just one author
to introduce our methods. We turn first to the
Victorian periodicals corpus, which is the simpler
case, being all in modern spelling. We work through
the results for one author, Anne Mozley, who has
the largest authorial group, with twenty articles out
of the total of 254. Mozley (1809–91) was an essayist
for the Christian Remembrancer, of which she was
co-editor, contributing long review articles on gen-
eral topics. She also wrote for the short-lived
Bentley’s Quarterly Review, for the Saturday Review
and for Blackwood’s Magazine (Jordan, 2008).
We begin with strict n-grams using all available
words. These are strict in the sense that they take all
the words in order as they appear in the text. They
are in Stubbs’s words ‘a recurrent uninterrupted
string of orthographic word-forms’ (Stubbs, 2007,
p. 90). Here and elsewhere the words are treated as
in a concordance, rather than as in a dictionary,
with no attempt to differentiate homographs by
sense and accepting the forms as they appear as
separate items, as in prints, printing, printed, etc.
We excluded a manually prepared list of proper
nouns and adjectives. Our first measure of effective-
ness as an authorial marker is the Zeta index score.
In this case the base set is the group of twenty art-
icles by Mozley and the counter set is the remaining
234 articles by others in the corpus. The articles
were all divided into 2,000-word segments, with
the residue added to the last segment. Mozley’s 20
articles yielded 69 segments, and the 234 articles by
others yielded 1055. Fig. 1 shows the Zeta index
scores for the highest-scoring 500 markers produced
by the various n-gram sets. The higher the Zeta
index score, the more the marker is preponderant
in Mozley articles as opposed to those by others.
The highest scores are to the left. Table 1 lists the
highest-scoring marker for each n-gram set.
Overall, as Fig. 1 shows, the 1-grams had the
highest scores across the span. The 2-grams were
not far behind. There is a gap to the 3-grams,
then finally 4-grams, and 5-grams are the lowest,
quite close in scores. The 1-grams data produce
the highest scoring marker on average, and also
the markers with the average highest score for
each group up to 500. The 2-grams data are a
close second, the 3-grams next, followed by the
4-grams, and then the 5-grams. Evidently the pat-
terning inherent in language, which would favour
larger units, does not overcome the tendency to
A. Antonia et al.
6 of 17 Literary and Linguistic Computing, 2013
 at A
egean U
niversity on A
ugust 29, 2013
http://llc.oxfordjournals.org/
D
ow
nloaded from
 
sparseness in this data. Students of authorship seek-
ing effective authorial markers in this corpus would
be wise to look to 1-grams and then 2-grams as their
source. There are still numerous markers that do
show a preponderance or a marked scarcity in au-
thors among the 3-grams, 4-grams, and 5-grams,
but they are less powerful and therefore inherently
less effective for attribution.
The Zeta index favours more frequently occur-
ring n-gram types and in this test the 1-grams, with
the densest data, yielded the most and best markers,
with the others in descending order of productivity
according to length. Evidently any advantage accru-
ing to longer n-grams from grammar and language
chunking and from the increased number of pos-
sible markers did not outweigh their disadvantage
through increased sparseness.
A single summary statistic for the productivity of
strict all-words n-gram types is the total of the Zeta
index scores for the highest 500 n-grams, divided by
500. Table 2 gives these figures for the trial using the
Mozley set as the base set and for the complemen-
tary trial using the set of article segments by others
as the base set.
1
1.1
1.2
1.3
1.4
1.5
1 14 27 40 53 66 79 92 10
5
11
8
13
1
14
4
15
7
17
0
18
3
19
6
20
9
22
2
23
5
24
8
26
1
27
4
28
7
30
0
31
3
32
6
33
9
35
2
36
5
37
8
39
1
40
4
41
7
43
0
44
3
45
6
46
9
48
2
49
5
Z
et
a 
in
de
x 
sc
or
e
1-grams
2-grams
3-grams
4-grams
5-grams
Fig. 1 Zeta index scores for the top 500 markers yielded by five all-words strict n-gram types in a comparison of 2,000-
word segments by Mozley with 2,000-word segments by others
Table 1 Highest-scoring n-gram markers for five all-words strict n-gram types in a comparison of 2,000-word segments
by Mozley with 2,000-word segments by others
Set Marker # segments by Mozley
where it appears
Per cent # segments by
others where it
does not appear
Per cent Zeta index
score
1-grams experience 54 78% 735 70% 1.479
2-grams of thought 41 59% 900 85% 1.447
3-grams we do not 45 65% 773 73% 1.385
4-grams a matter of course 9 13% 1027 97% 1.104
5-grams as a matter of course 9 13% 1035 98% 1.111
Explorations with word n-grams and authorial attribution
Literary and Linguistic Computing, 2013 7 of 17
 at A
egean U
niversity on A
ugust 29, 2013
http://llc.oxfordjournals.org/
D
ow
nloaded from
 
We also tested the various n-gram sets for prod-
uctivity of rarer markers. Attribution scholars some-
times look to the presence or otherwise of markers
unique to their authors as a way of testing the
authorship of disputed texts. One practice is to
identify promising n-grams in the disputed text,
filter out any of these markers that appear at all in
a large comparable control set, then find if any of
these markers appear in the works of the candidate
author (Vickers, 2008, 2011, 2012; Jackson, 2007).
This can be compared to Burrows’s Iota. The as-
sumption is that writers tend to repeat some very
rare markers, so the presence or otherwise of these
markers in an anonymous text is evidence about
their authorship. We therefore sought rare markers
that appeared in more than one Mozley article, with
the idea that if an article was detached from the set
and treated as anonymous it could be connected to
the main set by its instance of the rare marker that
appears at least once in the rest of the set (Table 3).
Here, we see the way the rival principles of
marker abundance and marker density play out in
this particular language sample. We can expect that
the higher the value of n, the more n-grams there
will be, because of the laws of combination, but we
can also expect that each of them will be repeated
less often. Linguistic considerations then cut across
these purely mathematical expectations: grammat-
ically governed word combinations, and authorial
habits, are likely to mean that 2-grams, 3-grams
and so on are repeated more often in language
data than in random data. In the event, 3-grams
prove to be the most productive overall, providing
the largest number of markers appearing in more
than one article. For the other categories, markers
appearing in more than 2, 3, 4, or 5 articles, 2-grams
are the most productive. We can presume that when
seeking this sort of less commonly occurring
marker, i.e. when privileging rarity, the advantages
of data density are less important, and sensitivity to
language chunking, and the availability of abundant
possible markers, prevail. In this case, for the stron-
ger markers, the 2-grams would seem to offer the
best trade-off between decreasing density and
increasing ‘chunkiness’ and marker abundance.
The 2-gram as where was the outstanding marker
in this test. Following the rule, it did not appear
at all in the 234 articles by writers other than
Mozley, and it appeared in ten out of the twenty
Mozley articles. Mozley uses this phrase most often
to introduce a quotation, thus: ‘Jeremy Taylor
abounds in illustration sure to excite a smile, what-
ever the context; as where he defines the weak rea-
soner: [quotation from Taylor follows]’ (Mozley,
1871, p. 69). It is a quirk of hers, not shared by
any of the other writers in the corpus, and if it
occurred in a disputed article, would be the begin-
nings of a case to attribute it to her.
Function words have an important role in both
the aspects of language that contribute to pattern-
ing, syntactic structures, and language production
in chunks. With this in mind, we also experimented
with function word skip n-grams. We defined a set
of function words as follows:
a, about, above, after, again, against, all,
almost, along, although, am, among, amongst,
an, and, another, any, anything, are, art, as, at,
back, be, because, been, before, being, besides,
beyond, both, but, by, can, cannot, canst,
could, dare, did, didst, do, does, doing, done,
dost, doth, down, durst, each, either, enough,
ere, even, ever, every, few, for, from, had,
hadst, has, hast, hath, have, having, he, hence,
her, here, him, himself, his, how, i, if, in, into, is,
it, itself, least, like, many, may, me, might,
mine, more, most, much, must, my, myself, nei-
ther, never, no, none, nor, not, nothing, now, o,
of, off, oft, often, on, one, only, or, other, our,
Table 2 All-words strict n-grams using sets of articles by Mozley and by others, in 2,000-word segments: total Zeta
Index score for the top 500 markers, divided by 500
1-grams 2-grams 3-grams 4-grams 5-grams
Author as base set 1.179 1.148 1.072 1.036 1.021
Segments by others as base set 1.099 1.089 1.053 1.022 1.009
The cell with highest-scoring n-gram type is shaded.
A. Antonia et al.
8 of 17 Literary and Linguistic Computing, 2013
 at A
egean U
niversity on A
ugust 29, 2013
http://llc.oxfordjournals.org/
D
ow
nloaded from
 
ours, ourselves, out, over, own, past, perhaps,
quite, rather, round, same, shall, shalt, she,
should, since, sith, so, some, something, some-
what, still, such, than, that, the, thee, their,
them, themselves, then, there, these, they,
thine, this, those, thou, though, through,
thus, thy, thyself, till, to, too, under, until,
unto, up, upon, us, very, was, we, well, were,
wert, what, when, where, which, while, whilst,
who, whom, whose, why, will, with, within,
without, would, ye, yet, you, your, yours, your-
self, yourselves
The list is intended to cover the forms used in Early
Modern English, for that part of the study, as well as
in Victorian English, so it includes some that were
obsolete by the 19th century, such as the various
pronoun and verb thou forms. Our 1-grams in
this part of the study consisted of any occurrence
of any of these words. We found function-word skip
2-grams by looking for the first occurrence of any of
these words in a text, then skipping to the next, then
for the next 2-grams started with the second word
and skipping to the next occurrence of any word in
the list. We counted skip 3-grams by moving from
the first word on the list, to the second, then to the
third, and so on.
To clarify the two kinds of word n-gram we can
take the first sentence of Mozley’s review of Adam
Bede by George Eliot—‘The reign of romance is an
extending one’ (Mozley, 1859a, p. 304)—and iden-
tify all the different n-grams it will yield under the
present scheme (Table 4).
Fig. 2 shows the top 500 function-words skip
n-gram markers in a comparison of Mozley article
segments with segments from articles by others.
There are only 191 1-grams in total (as per the
list) and the Zeta index scores for many of these are
below 1, i.e. these words are in fact preponderant in
the others’ articles rather than in Mozley’s. In this
the effect of the shallow pool of choice for markers is
clear. The 2-grams are the most productive of good
Mozley markers. The deeper pool means that there
are more than 500 markers with a Mozley prepon-
derance. The 3-, 4-, and 5-grams set are less pro-
ductive but quickly exceed the 1-gram set. Thus the
2-grams seem as before to be the best compromise
between the various factors. It is remarkable that the
function word skip 2-grams provide markers not so
inferior to the all-words strict 1- and 2-grams, des-
pite having a base set of less than 200, compared to
the base set of 10s of 1000s enjoyed by the all-words
strict n-grams. That would suggest that function-
word skip n-grams do reflect something of the pat-
terning of language. The highest-scoring 5-gram is
the.the.the.the.the, occurring in eight of the sixty-
nine Mozley segments, 12%, and absent in 1033 of
the 1055 segments by the others, 98%. Mozley evi-
dently has a predilection for long lists. A single sen-
tence, or rather sentence fragment, from her review
of Tennyson’s Idylls of the King contains several
series of instances of the uninterrupted by any
other function word from the list, in one case ex-
tending to six: ‘The aspen, the poplar, the willow, the
slow barge-laden river with its lilies, the waving
corn-field meeting the distant horizon, the meadows
deep in pasture, the farmsteads, the elms, the bee-
haunted limes, the minster bell sounding the hours;
always the city within sound or sight or conscious
neighbourhood’ (Mozley, 1859b, p. 164).
Turning to the rarer markers, the function words
are mostly frequent and evenly distributed so
they are less fruitful in unique-to-Mozley markers.
Table 5 shows the results for a rare-words test as in
Table 3.
The 1-grams yield no markers at all—there are
no individual function words from the list that
appear in two or more Mozley articles, but not in
any non-Mozley articles. The 4-grams set yields the
largest number of markers appearing in more than
Table 3 Test of the productivity of various strict all-
words n-gram sets for markers unique to Mozley:
number of markers found in more than one, more than
two, more than three, more than four, and more than five
Mozley articles and not in articles by others
>1 >2 >3 >4 >5
1-grams 64 10 3 2 0
2-grams 811 69 10 3 1
3-grams 976 62 9 0 0
4-grams 397 16 1 0 0
5-grams 88 2 0 0 0
3-grams yield the most markers above the baseline cutoff, but
2-grams yield the most markers at higher levels.
Explorations with word n-grams and authorial attribution
Literary and Linguistic Computing, 2013 9 of 17
 at A
egean U
niversity on A
ugust 29, 2013
http://llc.oxfordjournals.org/
D
ow
nloaded from
 
one Mozley article, the 3-grams set yields the largest
number appearing in more than two, and the 2-
grams set provides the single strongest marker:
over.over does not appear in the articles by others,
but appears in four Mozley articles. This test favours
rarity, and that is the province of the longer
n-grams, though sparseness finally does prevail,
and productivity falls off with the 5-grams.
0.7
0.8
0.9
1
1.1
1.2
1.3
1.4
1.5
1 15 29 43 57 71 85 99 11
3
12
7
14
1
15
5
16
9
18
3
19
7
21
1
22
5
23
9
25
3
26
7
28
1
29
5
30
9
32
3
33
7
35
1
36
5
37
9
39
3
40
7
42
1
43
5
44
9
46
3
47
7
49
1
Z
et
a 
In
de
x 
sc
or
e
1-grams
2-grams
3-grams
4-grams
5-grams
Fig. 2 Zeta index scores for the top 500 markers yielded by five function word skip n-gram types for Mozley articles in
2,000-word segments versus segments by others
A. Antonia et al.
10 of 17 Literary and Linguistic Computing, 2013
 at A
egean U
niversity on A
ugust 29, 2013
http://llc.oxfordjournals.org/
D
ow
nloaded from
 
5 Consolidated Victorian
Periodicals Results
To broaden the base for the results we also looked at
the authors with the next two largest sets of articles
in the corpus. They are Thomas Babington Mac-
aulay (1800–59), parliamentarian, colonial adminis-
trator, and author of a famous History of England
and of numerous review articles, which were repub-
lished in three volumes and reprinted many times
(Thomas, 2008); and John Duke Coleridge (1821–
94), a lawyer who was the nephew of Samuel Taylor
Coleridge, and the son of the judge Sir John Taylor
Coleridge (Pugsley, 2008).
Table 6 shows the numbers of articles, segments,
and words for each of the three authorial sets and
their complementary sets of articles by others.
We carried out trials as for Mozley, but rather
than present all of these, we have consolidated the
results using some summary statistics. We chose the
total Zeta index scores of the top 500 markers
divided by 500 as the summary statistic for the
Zeta trials. Table 7 gives the results for the three
authors in the four kinds of trial, with the Mozley
results included.
Using all the words in strict n-grams, and with
the author as base set, 1-grams and 2-grams are the
best performers. Using the articles by others as the
base set in the same sort of test, the 1-grams are
the winners. The authorial sets are always much
smaller than the non-authorial ones, so the total
number of markers is larger when the non-authorial
articles are the base set, making for a larger pool
overall, which seems to give the 1-grams an advan-
tage. The winners with the function-word skip
n-grams are 2-grams and 3-grams. The 1-grams
are only 191 in total so could never win this
competition.
We consolidated the results further by finding
the average Zeta index score for each of the methods
(Table 8).
The 1-grams markers are the best performers for
the first method, and the 2-grams for the second. Of
these two, the 2-grams average score is the higher. If
we had to choose one method for authorial attribu-
tion for this set, therefore, it would be function-
words skip 2-grams.
To summarize the results of the Iota trials, we
chose the number of unique markers which appear
in more than one of the base set where the authorial
set is the base set, and the number of unique mar-
kers which appear in more than twelve articles in
the base set as the summary statistic for the Iota test
with the articles by others as the base set (Table 9).
Table 4 N-grams in a sample sentence
1-grams 2-grams 3-grams 4-grams 5-grams
All-words
strict n-grams
the, reign, of,
romance, is, an,
extending, one
the reign, reign of,
of romance,
romance is, is an,
an extending,
extending one
the reign of, reign of
romance, of
romance is,
romance is an, is
an extending, an
extending one
the reign of ro-
mance, reign of
romance is,
of romance is an,
romance is an
extending, is an
extending one
the reign of romance
is, reign of
romance is an, of
romance is an
extending,
romance is an
extending one
Function-words
skip n-grams
the, of, is,
an, one
the of, of is,
an one
the of is, of is an,
is an one
the of is an, of is
an one
the of is an one
Table 5 Test of the productivity of various function-
words skip n-gram sets for markers unique to Mozley:
number of markers found in more than one, more than
two, more than three, more than four, and more than five
Mozley articles and not in articles by others
>1 >2 >3 >4 >5
1-grams 0 0 0 0 0
2-grams 19 5 1 0 0
3-grams 275 18 0 0 0
4-grams 465 16 0 0 0
5-grams 223 2 0 0 0
Explorations with word n-grams and authorial attribution
Literary and Linguistic Computing, 2013 11 of 17
 at A
egean U
niversity on A
ugust 29, 2013
http://llc.oxfordjournals.org/
D
ow
nloaded from
 
Here the pattern is consistent with each trial. The
winners are successively 3-grams, 1-grams, 4-grams,
and 2-grams. The larger pool of potential markers
when the non-authorial articles are the base set evi-
dently advantages the shorter n-grams. The larger
pool with all-words strict n-grams also advantages
shorter n-grams. 5-grams are never the winners.
We consolidated the results as before, finding the
overall average for the two methods (Table 10).
The results here have to be treated with caution,
since we are mixing two thresholds within the re-
sults for each method. However, this does give us a
rough guide. 3-grams markers could be regarded as
the best performers in the all-words strict n-grams
and 4-grams in the function-words strict n-grams. If
we had to choose one method this time it would be
all-words strict 3-grams.
6. Results for Early Modern
English plays
We tested the three authors from the plays set with
the most plays in the same fashion, William
Shakespeare, with 28 plays, Thomas Middleton,
with 18 plays, and Ben Jonson, with 17 plays.
Table 11 summarizes this data set.
For the tables we use the same summary of the
results as for the periodical article sets results.
The patterns in Table 12 follow those in Table 7
with the all-words strict n-grams, but this time with
the function-words skip n-grams the 2-grams are
the winners in all six trials. 3-grams, 4-grams, and
5-grams do not figure among the winners.
Table 13 provides averages for the two methods.
As with Table 8, the 1-grams are the best per-
formers in the all-words strict n-grams, and the
2-grams in the function-words skip n-grams. This
time, however, the former has a higher average than
the latter. Here, then, the all-words 1-grams would
be the best choice for a single attribution method.
Table 14 shows the results for the Iota trials.
The pattern is exactly as in Table 9. With these
rarer markers, the 1-grams only win where they
have the deepest pools to draw on, using all avail-
able words and the non-authorial sets as the base. In
the case of function-word skip n-grams, the
1-grams provide almost no usable markers. The ex-
ception is the word sith, which does not appear in
Table 7 Victorian periodicals articles; average Zeta index score for the top 500 markers, using 2,000-word segments
1-grams 2-grams 3-grams 4-grams 5-grams
All-words strict n-grams Author as base set Mozley 1.179 1.148 1.072 1.036 1.021
Macaulay 1.117 1.126 1.077 1.039 1.021
Coleridge 1.165 1.180 1.111 1.063 1.036
Segments by others as base set Mozley 1.099 1.089 1.053 1.022 1.009
Macaulay 1.120 1.106 1.045 1.016 1.007
Coleridge 1.101 1.088 1.046 1.020 1.008
Function-words skip n-grams Author as base set Mozley 0.377 1.229 1.205 1.139 1.101
Macaulay 0.375 1.344 1.383 1.326 1.242
Coleridge 0.386 1.155 1.120 1.079 1.055
Segments by others as base set Mozley 0.373 1.208 1.223 1.183 1.124
Macaulay 0.377 1.150 1.139 1.091 1.053
Coleridge 0.366 1.100 1.077 1.046 1.024
The cell with the highest score in each row is shaded.
Table 6 Victorian periodicals articles: summary statistics
for six sets and the corpus overall
Articles 2,000-word
segments
Total words
Mozley 20 69 162,883
Non-Mozley 234 1,055 2,342,626
Macaulay 19 146 308,438
Non-Macaulay 235 978 2,197,071
Coleridge 15 39 91,850
Non-Coleridge 239 1,085 2,413,659
Corpus 254 1,124 2,505,509
A. Antonia et al.
12 of 17 Literary and Linguistic Computing, 2013
 at A
egean U
niversity on A
ugust 29, 2013
http://llc.oxfordjournals.org/
D
ow
nloaded from
 
either the Jonson or the Middleton plays but does
appear in thirty-one plays by others, resulting in a
value of 1 in both Jonson and Middleton cells. With
the author as base set, 3-grams yield the largest
number of usable markers using all the words in
strict n-grams, and 4-grams yield the most when
using function-words skip n-grams.
Table 15 shows the averages for the two methods.
As with Table 10, the figures are very much a rough
guide since we are mixing results for two different
thresholds.
As with the Victorian periodicals results
(Table 10), 3-grams markers are the best performers
in the all-words strict n-grams; 4-grams are the best
performers in the function-words skip n-grams; and
the all-words strict 3-grams have the higher average
score.
7 Discussion and Conclusion
The results indicate that no one style of n-gram out-
shines the others in providing authorial markers and
that attributionists would be wise to keep an open
mind about the usefulness of each. 1- and 2-grams
appear regularly as the best source of authorial
Table 9 Victorian periodicals articles; number of Iota markers above a threshold, using whole articles
1-grams 2-grams 3-grams 4-grams 5-grams
All-words strict
n-grams
Author as base set
(more than one article with an instance)
Mozley 64 811 976 397 88
Macaulay 115 2487 4078 2591 1085
Coleridge 10 234 430 239 68
Segments by others as base set
(more than 12 articles with an instance)
Mozley 999 402 132 60 29
Macaulay 660 320 80 32 10
Coleridge 1506 615 150 61 21
Function-words
skip n-grams
Author as base set
(more than one article with an instance)
Mozley 0 19 275 465 223
Macaulay 0 27 816 1905 1570
Coleridge 0 18 159 198 79
Segments by others as base set
(more than 12 articles with an instance)
Mozley 7 986 466 114 70
Macaulay 4 727 293 60 19
Coleridge 3 1973 755 207 73
The cell with the highest score in each row is shaded.
Table 8 Victorian periodicals articles: overall average Zeta Index score for the top 500 markers for each of the two
methods in Table 7
1-grams 2-grams 3-grams 4-grams 5-grams
All-words strict n-grams 1.130 1.123 1.067 1.033 1.017
Function-words skip n-grams 0.376 1.198 1.191 1.144 1.100
The cell with the highest score in each row is shaded.
Table 10 Victorian periodicals articles: average number of markers above a threshold for each of the two methods in
Table 9
1-grams 2-grams 3-grams 4-grams 5-grams
All-words strict n-grams 559.00 811.50 974.33 563.33 216.83
Function-words skip n-grams 2.20 355.40 401.80 548.40 392.20
The cell with the highest score in each row is shaded.
Explorations with word n-grams and authorial attribution
Literary and Linguistic Computing, 2013 13 of 17
 at A
egean U
niversity on A
ugust 29, 2013
http://llc.oxfordjournals.org/
D
ow
nloaded from
 
markers, 3- and 4-grams appear sometimes, and
5-grams never. Despite the commentary quoted ear-
lier, 1-grams sometimes are the best option.
Function-word skip 2-grams are not widely used
in published attribution studies but the results show
they do provide abundant good markers when used
in Zeta tests, which favour regularity rather than
rarity. The function words which are their compo-
nents have the advantages of appearing very regu-
larly, and they are less obviously related to subject
matter than rarer lexical words, though their fre-
quencies do probably reflect genre. Function
words by definition reflect syntactical structures
and function word skip 2-grams share in this
aspect, while retaining some data density and pro-
viding finer granularity and abundance through the
good offices of combinatorics.
It is worth remembering, though, that combin-
ations do not necessarily enhance the power of in-
dividual words as markers. The top two Zeta index
scores for Shakespeare among the function words
are doth, at 1.28, and hath, at 1.26. The highest
scoring 2-gram doth appears in is the.doth, Zeta
Index 1.09, and the highest scoring 2-gram hath
appears in is he.hath at 1.26. The power of func-
tion-word skip 2-grams is not so much in individ-
ual high-rating markers, reflecting some highly
predominant trick of style, but rather in producing
an abundance of mid-range markers.
We have not addressed the question of whether
regularly occurring markers are more effective than
rare markers, or vice versa. The former are closer to
Table 12 Early Modern English plays: average Zeta Index score for the top 500 markers, using 2,000-word segments
1-grams 2-grams 3-grams 4-grams 5-grams
All-words
strict n-grams
Author as base set Shakespeare 1.179 1.148 1.072 1.036 1.021
Middleton 1.112 1.123 1.061 1.026 1.015
Jonson 1.090 1.098 1.045 1.019 1.011
Segments by others as base set Shakespeare 1.076 1.060 1.026 1.009 1.004
Middleton 1.082 1.073 1.028 1.010 1.004
Jonson 1.084 1.069 1.027 1.010 1.004
Function-words
skip n-grams
Author as base set Shakespeare 0.380 1.068 1.039 1.018 1.009
Middleton 0.385 1.105 1.063 1.026 1.013
Jonson 0.379 1.091 1.056 1.024 1.011
Segments by others as base set Shakespeare 0.380 1.060 1.029 1.011 1.005
Middleton 0.377 1.083 1.041 1.016 1.006
Jonson 0.381 1.077 1.034 1.013 1.005
The cell with the highest score in each row is shaded.
Table 11 Early Modern English plays: summary statistics
for six sets and the corpus overall
Plays 2,000-word
segments
Total words
Shakespeare 28 301 625,219
Non-Shakespeare 146 1,336 2,810,715
Middleton 18 160 337,879
Non-Middleton 156 1,477 3,098,055
Jonson 17 207 430,887
Non-Jonson 157 1,430 3,004,057
Corpus 174 1,637 3,435,934
Table 13 Early Modern English plays: overall average Zeta Index score for the top 500 markers for each of the two
methods in Table 12
1-grams 2-grams 3-grams 4-grams 5-grams
All-words strict n-grams 1.104 1.095 1.043 1.018 1.010
Function-words skip n-grams 0.380 1.081 1.043 1.018 1.008
The cell with the highest score in each row is shaded.
A. Antonia et al.
14 of 17 Literary and Linguistic Computing, 2013
 at A
egean U
niversity on A
ugust 29, 2013
http://llc.oxfordjournals.org/
D
ow
nloaded from
 
a normal distribution, which has important advan-
tages in reliability, as discussed above. Rare markers
suffer from the inherent difficulty that it is hard to
tell if an instance is there by chance or following
some consistent pattern. However, combining rare
markers into a composite variable, as many of the
classification methods in use do, will help overcome
some of these problems. The rare markers have
lower Zeta index scores than regularly occurring
markers, and lower scores on Student’s t test, in
which the difference between means is moderated
by the standard deviation of the combined sets. If
we collect the 500 highest scoring all-words 1-grams
on the Zeta index for Mozley’s work versus the rest,
we find there are no rare markers in the list. The
same happens with the 500 all-words strict 1-grams
with the lowest t-test probabilities. If we turn to the
strict all-words 3-grams, we find no rare markers
among the top 500 on Zeta index scores, and just
fory-one among the top 500 on t-test probabilities.
By these measures, then, rare markers are less useful
for attribution than regularly occurring ones. Rare
markers, however, have a persistent appeal because
each of them is likely to be easy to understand as
evidence, and they are certainly simpler to discuss in
presenting a case. Rare words or rare longer frag-
ments can be quoted as part of the argument, while
densities of groups of words such as ‘the’, or ‘at’, or
of ‘I am’ or ‘in a’ are difficult to relate to the reading
experience. Word sequences are inherently more
complex than 1-grams and may seem intuitively to
deserve a special weighting as evidence, even if, as a
marker in a test like the ones used here, each one
can only count as a single instance. Moreover, there
are advantages in having more than one test, espe-
cially if it can be shown that the various tests are
independent of each other (Craig and Kinney, 2009,
pp. 25–6, 38).
Perhaps the most surprising finding in the work
reported in this article is that individual-word vari-
ables, 1-grams, sometimes yield more authorial
markers than sequences, despite what some com-
mentators have predicted on the basis of the inher-
ent ‘chunkiness’ of language. The present study
confirms Stamatatos’s observation in his review of
authorship studies that ‘the classification accuracy
achieved by word n-grams is not always better than
individual word features’ (Stamatatos, 2009, p. 6),
Table 14 Early Modern English plays: number of Iota markers above a threshold of recurrences
1-grams 2-grams 3-grams 4-grams 5-grams
All-words
strict n-grams
Author as base set
(more than one play with an instance)
Shakespeare 601 4889 6522 2927 730
Middleton 244 1822 2541 1519 265
Jonson 405 2538 3349 1604 432
Segments by others as base set
(more than 12 plays with an instance)
Shakespeare 309 117 24 8 0
Middleton 702 287 72 31 6
Jonson 507 154 38 17 2
Function-words
skip n-grams
Author as base set
(more than one play with an instance)
Shakespeare 0 123 3060 5178 1829
Middleton 0 35 999 1798 744
Jonson 0 52 1493 2697 980
Segments by others as base set
(more than 12 plays with an instance)
Shakespeare 0 246 100 14 0
Middleton 1 950 318 30 6
Jonson 1 575 184 13 1
The cell with the highest score in each row is shaded.
Table 15 Early Modern English plays: average number of markers above a threshold for each of the two methods in
Table 14
1-grams 2-grams 3-grams 4-grams 5-grams
All-words strict n-grams 461 1635 2091 1018 239
Function-words skip n-grams 0 330 1026 1622 593
The cell with the highest score in each row is shaded.
Explorations with word n-grams and authorial attribution
Literary and Linguistic Computing, 2013 15 of 17
 at A
egean U
niversity on A
ugust 29, 2013
http://llc.oxfordjournals.org/
D
ow
nloaded from
 
and Argamon and Levitan’s remark that ‘it has
proven quite difficult to improve on the general
usefulness of function words for stylistic attribution’
(Argamon and Levitan, 2005, p. 1). Pragmatically
this suggests that those working in attribution
should not overlook the use of 1-grams as a
source of markers. In terms of the understanding
of language, it suggests that English, at least, does
work as a combination of individual words, as well
as in chunks and patterns. It is also possible that
some different kinds of variables with more flexible
rules might capture better the hypothesized domin-
ance of chunks and patterns in language. It may be,
as one reviewer of this article suggested, that a
sophisticated system of single-word nodes and col-
locations, arrived at statistically, might model the
associative structure we know to characterize lan-
guage production and thus provide an amalgam of
1-grams, skip 2-grams, and longer sequences that
could serve as a superior authorship classification
system. On the present evidence, and confining our-
selves to n-grams as discrete features, however, the
picture is of language as a mixed system of individ-
ual words and combinations, and the take-home
advice for those involved in authorial attribution
is to keep an open mind about using n-grams of
various sizes of n for particular problems.
Acknowledgements
We are grateful to John Burrows and to two an-
onymous reviewers for valuable suggestions for the
paper.
References
Argamon, S. and Levitan, S. (2005). Measuring the Use-
fulness of Function Words for Authorship Attribution.
Proceedings of the 2005 ACH/ALLC Conference. Vic-
toria, BC. http://grape.hcmc.uvic.ca:8080/ach/site/pdf.
xq?id¼162 (accessed 18 January 2013).
Baayen, R. H. (2001). Word Frequency Distributions.
Dordrecht: Kluwer Academic.
Burrows, J. (1999). A Computational Approach to the
Rochester Canon. In Love, H. (ed.), The Complete
Works of John Wilmot, Earl of Rochester. Oxford:
Clarendon, pp. 681–95.
Burrows, J. (2002). ‘Delta’: a measure of stylistic differ-
ence and a guide to likely authorship. Literary and
Linguistic Computing, 17: 267–87.
Burrows, J. (2005). Andrew Marvell and the ‘painter sat-
ires’: a computational approach to their authorship.
Modern Language Review, 100: 281–97.
Burrows, J. F. (2007). All the way through: testing for
authorship in different strata. Literary and Linguistic
Computing, 22: 27–47.
Burrows, J. and Craig, H. (2012). Authors and characters.
English Studies, 93: 292–309.
Coyotl-Morales, R.M., Villaseñor-Pineda, L., and
Montes-y-Gómez, M. (2006). Authorship Attribution
Using Word Sequences. Progress in Pattern Recognition,
Image Analysis and Applications. 11th Iberoamerican
Congress on Pattern Recognition. Berlin: Springer,
pp. 844–53, http://link.springer.com/chapter/10.1007%
2F11892755_87?LI¼true#page-1 (accessed 18 January
2013).
Craig, H. and Kinney, A. F. (2009). Methods. In
Craig, H. and Kinney, A. F. (eds), Shakespeare,
Computers, and the Mystery of Authorship. Cambridge:
Cambridge University Press, pp. 15–39.
Eder, M. (2011). Style-markers in authorship attribution:
a cross-language study of the authorial fingerprint.
Studies in Polish Linguistics, 6: 99–114.
Ellegård, A. (1962). A Statistical Method for Determining
Authorship: The ‘Junius’ Letters, 1769–1772. Göteborg:
Acta Universitatis Gothoburgensis.
Ellis, N. C. (2003). Constructions, chunking, and connec-
tionism: the emergence of second language structure. In
Doughty, C. J. and Long, M. H. (eds), Handbook of
Second Language Acquisition. Oxford: Blackwell,
pp. 63–103.
Holmes, D. I. (2003). Stylometry and the Civil War: the
case of the Pickett letters. Chance: Journal of the
American Statistical Association, 16: 18–25.
Holmes, D. I. (2010). The diary of a public man: a case
study in traditional and non-traditional authorship at-
tribution. Literary and Linguistic Computing, 25:
179–97.
Hoover, D. L. (2002). Frequent word sequences and stat-
istical stylistics. Literary and Linguistic Computing, 17:
157–80.
Hoover, D. L. (2003). Frequent collocations and authorial
style. Literary and Linguistic Computing, 18: 261–86.
Hoover, D. L. (2004). Testing Burrows’ Delta. Literary
and Linguistic Computing, 19: 453–75.
A. Antonia et al.
16 of 17 Literary and Linguistic Computing, 2013
 at A
egean U
niversity on A
ugust 29, 2013
http://llc.oxfordjournals.org/
D
ow
nloaded from
 
Hoover, D. L. (2012). The rarer they are, the more
there are, the less they matter. DH2012. Hamburg,
http://www.dh2012.uni-hamburg.de/conference/progra
mme/abstracts/the-rarer-they-are-the-more-there-are-
the-less-they-matter/ (accessed 13 January 2013).
Jackson, MacD. P. (2007). New research on the dramatic
canon of Thomas Kyd. Research Opportunities in
Medieval and Renaissance Drama, 47: 107–27.
Jordan, E. (2008). Mozley, Anne (1809–1891). In
Goldman, L. (ed.), Oxford Dictionary of National
Biography. Oxford: Oxford University Press. http://
0-www.oxforddnb.com.library.newcastle.edu.au/view/
article/19477 (accessed 24 January 2013).
Juola, P. (2008). Authorship attribution. Foundations and
Trends in Information Retrieval, 1: 233–334.
Lancashire, I. (1997). ‘Empirically determining Shake-
speare’s Idiolect. Shakespeare Studies, 25: 171–85.
Lancashire, I. (2010). Forgetful Muses: Reading the Author
in the Text. Toronto: University of Toronto Press.
Mosteller, F. and Wallace, D. L. (1964). Inference and
Disputed Authorship: The Federalist. Reading, MA:
Addison-Wesley.
Mozley, A. (1859a). Adam Bede and recent novels.
Bentley’s Quarterly Review, 1: 433–72.
Mozley, A. (1859b). Rev. of Alfred Tennyson, Idylls of the
King (London: Moxon, 1850). Bentley’s Quarterly
Review, 2: 159–94.
Mozley, A. (1871). Illustration. Blackwood’s Edinburgh
Review, 110: 69–107.
Peng, F., Shuurmans, D., and Wang, S. (2004).
Augmenting naive Bayes classifiers with statistical lan-
guage models. Information Retrieval Journal, 7: 317–45.
Pugsley, D. (2008). Coleridge, John Duke, first Baron
Coleridge (1820–1894). In Goldman, L. (ed.), Oxford
Dictionary of National Biography. Oxford: Oxford Uni-
versity Press, http://0-www.oxforddnb.com.library.new
castle.edu.au/view/article/5886?docPos¼1 (accessed 24
January 2013).
Rybicki, J. and Eder, M. (2011). Deeper delta across
genres and languages: do we really need the most fre-
quent words? Literary and Linguistic Computing, 26:
315–21.
Sanderson, C. and Günter, C. (2006). Short Text
Authorship Attribution Via Sequence Kernels, Markov
Chains and Author Unmasking: An Investigation.
Proceedings of the International Conference on Empirical
Methods in Natural Language Processing. Sydney, Aus-
tralia, pp. 482–91. http://itee.uq.edu.au/conrad/pdfs/
sanderson_emnlp_2006.pdf (accessed 18 January 2013).
Stamatatos, E. (2009). A study of modern authorship at-
tribution methods. Journal of the American Society for
Information Science and Technology, 60: 538–56.
Stubbs, M. (2007). An example of frequent English
phraseology: distributions, structures and functions.
In Facchinetti, R. (ed.), Corpus Linguistics 25 Years
On. Amsterdam and New York: Rodopi, pp. 89–106.
Thomas, W. (2008). Macaulay, Thomas Babington, Baron
Macaulay (1800–1859). In Goldman, L. (ed.), Oxford
Dictionary of National Biography. Oxford: Oxford
University Press. http://0-www.oxforddnb.com.library.
newcastle.edu.au/view/article/17349?docPos¼1 (ac-
cessed 24 January 2013).
Vickers, B. (2008). Thomas Kyd, Secret Sharer. Times
Literary Supplement, 18: 13–15.
Vickers, B. (2009). The marriage of philology and in-
formatics. British Academy Review, 14: 41–4.
Vickers, B. (2011). Shakespeare and authorship studies in
the twenty-first century. Shakespeare Quarterly, 62:
106–42.
Vickers, B. (2012). Identifying Shakespeare’s additions to
The Spanish Tragedy (1602): a new(er) approach.
Shakespeare, 8: 13–43.
Notes
1 For a demonstration that authorial distinctiveness and
consistency may subsume variation by character in
1-grams data, see Burrows and Craig (2012).
2 See Ellis (2003) for a review of research on ‘chunking’
in language production.
3 For another critique of Vickers’s method, see Jackson
(2007).
4 Full lists of the items in these corpora are available
online by following the links at the foot of www.new
castle.edu.au/centre/llc/publications/n-grams.
Explorations with word n-grams and authorial attribution
Literary and Linguistic Computing, 2013 17 of 17
 at A
egean U
niversity on A
ugust 29, 2013
http://llc.oxfordjournals.org/
D
ow
nloaded from
 
