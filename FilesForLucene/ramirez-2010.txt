Automatic performer identification in commercial monophonic Jazz performances
Rafael Ramirez *, Esteban Maestre, Xavier Serra
Information Systems and Telecommunications Department, Pompeu Fabra University, Tanger 122, 080018 Barcelona, Spain
a r t i c l e i n f o
Article history:
Available online 4 January 2010
Keywords:
Expressive performance
Information retrieval
Signal processing
Music
Classification techniques
a b s t r a c t
We present a pattern recognition approach to the task of identifying performers from their interpretative
styles. We investigate how professional musicians express their view of the musical content of musical
pieces and how to use this information in order to automatically identify performers. We apply sound
analysis techniques based on spectral models for extracting deviation patterns of parameters such as
pitch, timing, amplitude and timbre characterising both the internal structure of notes and the musical
context in which they appear. We describe successful performer identification case studies involving
monophonic audio recordings of both score-guided and commercial improvised performances.
 2010 Elsevier B.V. All rights reserved.
1. Introduction
Performers manipulate sound properties such as pitch, timing,
amplitude and timbre. These manipulations are clearly distin-
guishable by the listeners and often are reflected in concert atten-
dance and recording sales. Expressive music performance research
(for an overview see Gabrielsson, 1999, 2003) investigates the
manipulation of these sound properties in an attempt to under-
stand and recreate expression in performances.
While in most forms of western classical music performers
strictly follow a score specification of a piece, in other music genres
such as Jazz, musicians are often encouraged to deviate signifi-
cantly from the score (if the score exists at all) by adding notes,
embellishments and rhythm variation. This is often referred as
improvisation. In the past, expressive performance research has
investigated manipulations of different sound properties in
score-driven performances (e.g. Widmer, 2002) as well as different
types of deviations from the score in popular music (e.g. Lopez de
Mantaras and Arcos, 2002; Ramirez et al., 2008).
In this paper we concentrate on automatic performer identifica-
tion based on expressive content extracted from monophonic
audio recordings (i.e. recordings of one instrument playing one
note at a time). In particular, we investigate performer identifica-
tion in both score-driven and improvisation saxophone jazz
recordings. Expressive-content based performer identification
raises particularly interesting questions but has nevertheless re-
ceived relatively little attention in the past. Given the current capa-
bilities of current audio analysis systems, we believe that
identification of performers based on their playing styles is a prom-
ising research topic in music information retrieval. This work is
based on our previous work on expressive performance modelling
(Ramirez and Hazan, 2006; Ramirez et al., 2008).
The rest of the paper is organized as follows: Section 2 sets the
background for our research reported here. Section 3 describes
how we process the audio recordings in order to extract informa-
tion about both the internal structure of notes (i.e. intra-note infor-
mation) and the musical context in which they appear (i.e. inter-
note information). Section 4 describes our approach to perfor-
mance-driven performer identification. Section 5 describes two
case studies on identifying performers based on their playing styles
and discusses the results, and finally, Section 6 presents some con-
clusions and indicates some areas of future research.
2. Background
Expressive music performance studies the manipulation of
sound properties such as pitch, timing, amplitude and timbre in
an attempt to understand and recreate expression in performances.
There has been much speculation as to why performances contain
expression. Hypothesis include that musical expression communi-
cates emotions (Juslin and Sloboda, 2001) and that it clarifies mu-
sical structure (Kendall and Carterette, 1990), i.e. the performer
shapes the music according to her own intensions (Apel, 1972).
In any case, understanding and formalizing expressive music per-
formance is an extremely challenging problem which in the past
has been studied from different perspectives (for an overview see
Gabrielsson, 1999, 2003). The main approaches to empirically
studying expressive performance have been based on statistical
analysis (e.g. Repp, 1992), mathematical modeling (e.g. Todd,
1992), and analysis-by-synthesis (e.g. Friberg et al., 2006). In all
these approaches, it is a person who is responsible for devising a
theory or mathematical model which captures different aspects
0167-8655/$ - see front matter  2010 Elsevier B.V. All rights reserved.
doi:10.1016/j.patrec.2009.12.032
* Corresponding author. Tel.: +34 935421365; fax: +34 935422202.
E-mail addresses: rafael.ramirez@upf.edu (R. Ramirez), esteban.maestre@upf.
edu (E. Maestre), xavier.serra@upf.edu (X. Serra).
Pattern Recognition Letters 31 (2010) 1514–1523
Contents lists available at ScienceDirect
Pattern Recognition Letters
journal homepage: www.elsevier .com/locate /patrec
of musical expressive performance. The theory or model is later
tested on real performance data in order to determine its accuracy.
More recently, machine learning techniques (Mitchell, 1997) have
been applied in order to automatically induce an expressive perfor-
mance model from a training set (e.g. Widmer and Goebl, 2004).
The majority of the research on expressive music performance,
either empirical or machine-learning-based, has focused on classi-
cal piano for which notation (i.e. a score) is available.
2.1. Empirical expressive performance research
There are a number of approaches which address expressive
performance without using machine learning techniques. One of
the first attempts to provide a computer system with musical
expressiveness is that of Johnson (1992). Johnson developed a
rule-based expert system to determine expressive tempo and artic-
ulation for Bach’s fugues from the Well-Tempered Clavier. The rules
were obtained from two expert performers.
A long-term effort in expressive performance modeling is the
work of the KTH group (Bresin, 2002; Friberg et al., 2000, 2006).
Their Director Musices system incorporates rules for tempo, dy-
namic and articulation transformations. The rules are obtained
from both theoretical musical knowledge, and experimentally by
using an analysis-by-synthesis manual approach. The rules are di-
vided into differentiation rules which enhance the differences be-
tween scale tones, grouping rules which specify what tones
belong together, and ensemble rules which synchronize the voices
in an ensemble.
Canazza et al. (1997) developed a system to analyze the rela-
tionship between the musician’s expressive intentions and her per-
formance. The analysis reveals two expressive dimensions, one
related to loudness (dynamics), and another one related to timing
(rubato).
Dannenberg et al. (1998) investigated the trumpet articulation
transformations using (manually generated) rules. They developed
a trumpet synthesizer which combines a physical model with an
expressive performance model. The performance model generates
control information for the physical model using a set of rules
manually extracted from the analysis of a collection of perfor-
mance recordings.
2.2. Machine-learning-based expressive performance research
Previous research addressing expressive music performance
using machine learning techniques has included a number of ap-
proaches. Lopez de Mantaras and Arcos (2002) report on SaxEx, a
performance system capable of generating expressive solo saxo-
phone performances in Jazz. Their system is based on case-based
reasoning, a type of analogical reasoning where problems are
solved by reusing the solutions of similar, previously solved prob-
lems. In order to generate expressive solo performances, the case-
based reasoning system retrieves from a memory containing
expressive interpretations, those notes that are similar to the input
inexpressive notes. The case memory contains information about
metrical strength, note duration, and so on, and uses this informa-
tion to retrieve the appropriate notes. One limitation of their sys-
tem is that it is incapable of explaining the predictions it makes
and it is unable to handle melody alterations, e.g. ornamentations.
Ramirez et al. (2006) have explored and compared diverse ma-
chine learning methods for obtaining expressive music perfor-
mance models for Jazz saxophone that are capable of both
generating expressive performances and explaining the expressive
transformations they produce. They propose an expressive perfor-
mance system based on inductive logic programming which in-
duces a set of first order logic rules that capture expressive
transformation both at an inter-note-level (e.g. note duration,
loudness) and at an intra-note-level (e.g. note attack, sustain).
Based on the theory generated by the set of rules, they imple-
mented a melody synthesis component which generates expres-
sive monophonic output (MIDI or audio) from inexpressive
melody MIDI descriptions.
With the exception of the work by Lopez de Mantaras et al. and
Ramirez et al., most of the research in expressive performance
using machine learning techniques has focused on classical piano
music (e.g. Dovey, 1995; Van Baelen and De Raedt, 1996; Widmer,
2001; Tobudic and Widmer, 2003), where often the tempo of the
performed pieces is not constant. Thus, these works focus on global
tempo and loudness transformations.
2.3. Expressive performance research and performer identification
The use of expressive performance models (either automatically
induced or manually generated) for identifying musicians has re-
ceived little attention in the past. This is mainly due to two factors:
(a) the high complexity of the feature extraction process that is re-
quired to characterize expressive performance, and (b) the ques-
tion of how to use the information provided by an expressive
performance model for the task of performance-based performer
identification. To the best of our knowledge, the only group work-
ing on performance-based automatic performer identification is
the group led by Gerhard Widmer. Saunders et al. (2004) apply
string kernels to the problem of recognizing famous pianists from
their playing style. The characteristics of performers playing the
same piece are obtained from changes in beat-level tempo and
beat-level loudness. From such characteristics, general perfor-
mance alphabets can be derived, and pianists’ performances can
then be represented as strings. They apply both kernel partial least
squares and Support Vector Machines to this data.
Stamatatos and Widmer (2005) address the problem of identi-
fying the most likely music performer, given a set of performances
of the same piece by a number of skilled candidate pianists. They
propose a set of very simple features for representing stylistic char-
acteristics of a music performer that relate to a kind of ‘average’
performance. A database of piano performances of 22 pianists play-
ing two pieces by Frédéric Chopin is used. They propose an ensem-
ble of simple classifiers derived by both subsampling the training
set and subsampling the input features. Experiments show that
the proposed features are able to quantify the differences between
music performers.
2.4. Current study
This paper describes a machine learning approach to investigate
how skilled musicians express their view of the emotional content
of musical pieces and how to use this information in order to auto-
matically distinguish among performers. We extract features from
monophonic audio recordings at two levels: intra-note and inter-
note. The intra-note features represent the internal structure of a
note (e.g. note attack), while the inter-note features represent as-
pects of the musical context in which the note appears (e.g. pitch
interval with previous note). In particular, we study deviations of
parameters such as pitch, timing, amplitude and timbre both at
an inter-note-level and at an intra-note-level. This is, we analyze
the pitch, timing (onset and duration), amplitude (energy mean)
and timbre of individual notes, as well as the timing and amplitude
of individual intra-note events. We focus on saxophone audio per-
formances where timing and pitch measurements present a chal-
lenge compared to e.g. MIDI piano performances.
Roughly, the basic idea of our approach to performer identifica-
tion is to establish a performer-dependentmapping from inter-note
features to a repertoire of inflections characterized by intra-note
features. As an analogy, the inter-note features may be seen as a
R. Ramirez et al. / Pattern Recognition Letters 31 (2010) 1514–1523 1515
literary text, while the repertoire of inflections (i.e. the intra-note
features) is like a typeface or style of handwriting that different
performers use to render the text in different ways. Our approach
to performer identification is motivated by our pervious work
(Ramirez et al., 2006) on expressive music performance synthesis.
In (Ramirez et al., 2006) we consider a set of inflections (character-
ized by intra-note features) and use the note musical context (char-
acterized by inter-note features) in order to predict the type of
inflection to be used in that context. We use particular instances,
i.e. audio samples, of the type of inflection predicted to synthesize
expressive performances from inexpressive score descriptions. It
is clear that by using a particular performer’s samples the synthe-
sized pieces ‘sound’ like played by that performer. Thus, it seems
reasonable to apply the inverse process for performer identification.
3. Audio analysis
In this section,we describe howwe extract a description of a per-
formedmelody for monophonic recordings (for a comparison of the
method reported here and other methods see Gómez et al. (2003)).
For each note in the recordings, we are interested in obtaining a set
of descriptors characterising the internal structure of the note (i.e.
attack level, sustain duration, sustain slope, amount of legato with
the previous note, amount of legato with the following note, mean
energy, spectral centroid and spectral tilt) and a set of descriptors
characterising the musical context in which the note appears (i.e.
relative pitch and duration of the neighboring notes as well as the
musical structures to which the note belongs). Audio analysis data
obtained with these methods has already been used for expressive
performance rule induction (Ramirez and Hazan, 2006), intra-note
feature prediction (Ramirez et al., 2005), and genetic algorithms-
based expressive performance modelling (Ramirez et al., 2008).
3.1. Description scheme
We extract descriptors related to different temporal scales:
 Some features are defined as instantaneous or related to an anal-
ysis frame, such as energy, fundamental frequency, spectral cen-
troid and spectral tilt.
 We also obtain intra-note/inter-note segment features, i.e.
descriptors attached to a certain intra-note segment (attack, sus-
tain and release segments, considering the classical ADSR model
(Bernstein and Cooper, 1976)) or transition segment. After
observing the shape of the energy envelope of recorded notes,
we realized that most of the notes did not present a clear decay
segment but a fairly constant slope sustain segment. Thus, we
decided not to consider decay and sustain segments separately,
but just a linear sustain segment with variable slope.
 Note features or descriptors attached to a certain note are also
extracted.
We considered that the proposed features constitute a simple
but concise scheme for representing the typical expressive nuances
in which we are interested.
3.2. Extraction of inter-note features
The first step once the low-level descriptors have been ex-
tracted for each frame is to get a melodic description of the audio
phrases consisting on the exact onset and duration of notes, and
the corresponding MIDI equivalent pitch. We base our melody
transcription on the extraction of two different onset streams,
the first based on energy, and the second based on fundamental
frequency. Energy onsets are first detected following a band-wise
algorithm that uses some psycho-acoustical knowledge (Klapuri,
1999). In a second step, fundamental frequency transitions are
also detected. Finally, both results are merged to find note bound-
aries (see Fig. 1). We compute note descriptors using the note
boundaries and the low-level descriptors values. The low-level
descriptors associated to a note segment are computed by averag-
ing the frame values within this note segment. Pitch histograms
are used to compute the pitch note of each note segment, as
found in (McNab et al., 1996). This is done to avoid taking into
account mistaken frames in the fundamental frequency mean
computation. First, frequency values are converted into cents.
Then, we define histograms with bins of 100 cents and hop size
of 5 cents and we compute the maximum of the histogram to
identify the note pitch. Finally, we compute the frequency mean
for all the points that belong to the histogram. The MIDI pitch
is computed by quantization of this fundamental frequency mean
over the frames within the note limits. An extended explanation
of the methods we use for melodic description can be found in
(Gómez et al., 2003).
3.2.1. Note transitions
For characterizing note detachment, we also extract some fea-
tures of the note-to-note transitions describing how two notes
are detached. For two consecutive notes, we consider the transition
segment starting at the first note’s release and finishing at the at-
tack of the following one. Both the energy envelope and the funda-
mental frequency contour (schematically represented by EXX and f0
in Fig. 1) during transitions are studied in order to extract descrip-
tors related to articulation. We measure the energy envelope min-
imum position tc (see also Fig. 2) with respect to the transition
duration as (1). This descriptor has proven useful when recon-
structing amplitude envelopes during transitions
ETPOSmin ¼
tc
tend  tinit ð1Þ
We compute a legato descriptor as described next. First, we join
start and end points on the energy envelope contour by means of
a line Lt representing the smoothest case of detachment. Then, we
compute both the area A2 below energy envelope and the area A1
between energy envelope and the joining line Lt and define our le-
gato descriptor as shown in (2). The relevance of this descriptor was
assessed in (Maestre and Gomez, 2005)
LEG ¼ A1
A1 þ A2 ¼
R tend
tinit
ðLtðtÞ  EXXðtÞdtÞ
R tend
tinit
LtðtÞdt
ð2Þ
Fig. 1. Schematic view of the melodic description process. Note onsets are extracted based on the study of energy and fundamental frequency.
1516 R. Ramirez et al. / Pattern Recognition Letters 31 (2010) 1514–1523
3.2.2. Musical analysis
It is widely recognized that expressive performance is a multi-
level phenomenon and that humans perform music considering a
number of abstract musical structures. After having computed
the note descriptors as above, and as a first step towards providing
an abstract structure for the recordings under study, we decided to
use Narmour’s theory of perception and cognition of melodies
(Narmour, 1990, 1991) to analyse the performances.
The implication/realization model proposed by Narmour is a
theory of perception and cognition of melodies. The theory states
that a melodic musical line continuously causes listeners to gener-
ate expectations of how the melody should continue. According to
Narmour, any two consecutively perceived notes constitute a me-
lodic interval, and if this interval is not conceived as complete, it is
an implicative interval, i.e. an interval that implies a subsequent
interval with certain characteristics. That is to say, some notes
are more likely than others to follow the implicative interval.
Two main principles recognized by Narmour concern registral
directionand intervallic difference. The principle of registral direc-
tion states that small intervals imply an interval in the same regis-
tral direction (a small upward interval implies another upward
interval and analogously for downward intervals), and large inter-
vals imply a change in registral direction (a large upward interval
implies a downward interval and analogously for downward inter-
vals). The principle of intervallic difference states that a small (five
semitones or less) interval implies a similarly-sized interval (plus
or minus 2 semitones), and a large interval (seven semitones or
more) implies a smaller interval. Based on these two principles,
melodic patterns or groups can be identified that either satisfy or
violate the implication as predicted by the principles. Such pat-
terns are called structures and are labeled to denote characteristics
in terms of registral direction and intervallic difference. Fig. 3
shows prototypical Narmour structures. A note in a melody often
belongs to more than one structure. Thus, a description of a melody
as a sequence of Narmour structures consists of a list of overlap-
ping structures. We parse each melody in the training data in order
to automatically generate an implication/realization analysis of the
pieces. Fig. 4 shows the analysis for a fragment of a melody.
3.3. Extraction of intra-note features
Once we segment the audio signal into notes, we perform a
characterization of each of the notes in terms of its internal fea-
tures. We described in detail and evaluated the procedure for car-
rying out intra-note segmentation in (Maestre and Gomez, 2005).
3.3.1. Intra-note segmentation
The intra-note segmentation method is based on the study of
the energy envelope contour of the note. Once onsets and offsets
are located, we study the instantaneous energy values of the anal-
ysis frames corresponding to each note. This study is carried out by
analyzing the envelope curvature and characterizing its shape, in
order to estimate the limits of the intra-note segments.
When observing the note energy envelopes from the saxophone
recordings, we identify that there are usually three segments (at-
tack, sustain and release (Bernstein and Cooper, 1976)) needed to
conform a description that fits the model schematically repre-
sented in Fig. 5. We discarded the decay segment due to the gen-
eral characteristics of the notes within the performances.
In order to extract these three characteristic segments, we study
the smoothed derivatives in a similar way as presented in (Jenssen,
1999), where partial amplitude envelopes are modeled for isolated
sounds. The main difference is that we analyze the notes in their
musical context, rather than isolated. In addition, only three linear
segments are considered. Moreover, instead of studying the contri-
bution of all the partials, we obtain general intensity information
from the total energy envelope characteristic. The procedure is car-
ried out as follows.
Considering the energy envelope as a differentiable function
over time, the points of maximum curvature can be considered
as the local maximum variations of the first derivative of the signal
energy (second derivative extremes), that is, the local maxima or
minima of the second derivative.
Due to the characteristics of the audio signal, the energy enve-
lope must be previously smoothed by low-pass filtering, since
there are typically too many second derivative extremes. The
low-pass filtering is carried out by means of a variable-width
Gaussian convolution. Several smoothing steps are carried out in
order to find a good cut-off frequency of the smoothing filter.
The smoothed envelope should not differ much to the original
one to avoid loss of localization due to the filtering effect. Thus,
for each smoothing step, the error em at smoothing stepm between
original and current envelope is computed. This is carried out by
means of (3), where N is the length of the envelope in frames,
Fig. 2. Schematic view of the transition segment characterization.
Fig. 3. Prototypical Narmour structures.
Fig. 4. Narmour analysis of a melody fragment.
R. Ramirez et al. / Pattern Recognition Letters 31 (2010) 1514–1523 1517
env is the original envelope and envm is the smoothed envelope at
step m
em ¼ 1N
XN
k¼1
envðkÞ  envmðkÞj j
env ð3Þ
Starting from a low cut-off frequency f0init, this frequency is in-
creased each smoothing step until the error em gets lower than a
certain threshold eth, empirically selected. Then, we compute the
three first derivatives of the last smoothed envelope. Frame posi-
tions and corresponding y-values of second derivative extremes
are stored. Afterwards, these characteristic points are sorted by
the second derivative modulus, and the n highest positions are se-
lected to build up the set of characteristic points F. Of course, when
the total number of third derivative zero-crossings is less than n, the
set is F shortened.
Both note onset and offset are added as characteristic points to
the set F. The slope defined by each pair of consecutive character-
istic points on the envelope is computed (4), where i and j denote
frame positions. A minimum slope duration (measured in frames)
Dfr is defined relative to the note duration as the five per cent of
the note length N for excluding the possible too high valued slopes
near the note limits
8i; j 2 F such as i 6 jþ Dfr; si;j ¼ envmðjÞ  envmðiÞj i ð4Þ
Finally, the two pairs of points defining, respectively, the most po-
sitive and most negative slope values from the remaining slopes
after discarding are extracted. The end of the attack segment fAE is
defined as the frame position corresponding to second point of
the maximum slope, while the start of the release segment position
fRB is defined as the first point of the minimum slope. This is stated
in * Eqs. (5) and (6) and depicted in Fig. 6
sM ¼ siM ;jM ¼ maxðsi;jÞ; f AE ¼ jM ð5Þ
sm ¼ sim ;jm ¼ minðsi;jÞ; f RB ¼ im ð6Þ
The attack is defined as the segment between the note onset and
the end of the most positive of the computed slopes, while the re-
lease segment is defined as the segment between the start of the
most negative of the computed slopes and the note offset. Sustain
is restricted to the remaining segment. When the end of attack
and the start of release limits of a note coincide, it is considered that
the note does not have a sustain segment (see Fig. 7).
3.3.2. Intra-note segment characterization
Once we have found the intra-note segment limits, we describe
each one by its duration (absolute and relative to note duration),
start and end times, initial and final energy values (absolute and
relative to note maximum) and slope. For the stable part of each
note (sustain segment), we extract an averaged spectral centroid
and spectral tilt in order to have timbral descriptors related to
the brightness of a particular execution. We compute the spectral
centroid as the frequency bin corresponding to the barycenter of
the spectrum, expressed as (7), where fft is the fast Fourier trans-
form of a frame, N is the size of the fast Fourier transform, and k
is the bin index. For the spectral tilt, we perform a linear regression
of the logarithmic spectral envelope between 2 kHz and 6 kHz, and
get the slope expressed in dB/Hz
SC ¼
PN
k¼1kjfftðkÞjPN
k¼1jfftðkÞj
ð7Þ
4. Performance-driven performer identification
In this section, we describe our approach to the problem of
identifying performers from their playing style. Our approach con-
sists of modelling each performer’s playing style so that when pre-
sented with a new performance, an informed judgement may be
made as regards which of the performer’s models most closely
matches the new performance. We introduce the different note
descriptors we use to characterize audio performances (computed
as described in the previous section), as well as the different pat-
tern recognition techniques involved in the system.
4.1. Note descriptors
We characterize each performed note by the following two sets
of features:
 Intra-note features. The intra-note features represent the internal
structure of a note which is specified as intra-note characteris-
tics of the audio signal. The set of intra-note features we have
included in the research reported here are the note’s attack level,
sustain duration, sustain slope, amount of legato with the previ-
ous note, amount of legato with the following note, mean
energy, spectral centroid and spectral tilt. This is, each per-
formed note is characterized by the tuple
(AtackLev, SustDur, SustSlo, LegLeft, LegRight, EnergyM, SpecCen,
SpecTilt)
 Inter-note features. The inter-note features represent both prop-
erties of the note itself and aspects of the musical context in
which the note appears. Information about the note includes
note pitch and note duration, while information about its melo-
dic context includes the relative pitch and duration of the neigh-
boring notes (i.e. previous and following notes) as well as the
EXX
t
NOTENOTE
SUSTAIN
RELEASE
ATTACK RELEASE
SILENCE NOTE SILENCE
ATTACK RELEASE
ATTACK
RELEASE
Fig. 5. Schematic view of the proposed energy envelope-based intra-note segmentation.
1518 R. Ramirez et al. / Pattern Recognition Letters 31 (2010) 1514–1523
Narmour structures to which the note belongs (Nar1, Nar2 and
Nar3 denote the three Narmour structures with the considered
note in position 1, 2 and 3, respectively). The note’s Narmour
structures are computed by performing the musical analysis
described in Section 3.2. Thus, each performed note is contextu-
ally characterized by the tuple
(Pitch, Dur, PrevPitch, PrevDur, NextPitch, NextDur, Nar1, Nar2,
Nar3)
4.3. Algorithm
A central question to be asked before attempting to build a sys-
tem to automatically identify a musician by his or her playing style
is: how is this task performed by a music expert? The answer de-
pends on the music genre and the instrument being played.
Clearly, timing, dynamics and timbre aspects of the performance
are all important for identifying particular performers, the relative
importance of each aspect is not straightforward. On the one side
Fig. 6. Top figure: original (solid line) and smoothed (red dashed line) envelopes of a sax note for a value of eth = 0.05. Bottom figure: selected characteristic points are
denoted with a square within extremes of the second derivative of the smoothed envelope (red dashed line). (For interpretation of the references in colour in this figure
legend, the reader is referred to the web version of this article.)
3.2 3.4 3.6 3.8 4 4.2 4.40
5
10
15
20
25
NOTE NOTE NOTE NOTE
ATTACK SUSTAIN
S R A S R
En
er
gy
SA R
RELEASE
time (sec.)
A
Fig. 7. Energy envelope and its linear approximation of a real excerpt with intra-note segment limits marked.
R. Ramirez et al. / Pattern Recognition Letters 31 (2010) 1514–1523 1519
of the instruments spectrum we could identify the singing voice in
which the timbre aspect of the performance (i.e. the singer’s partic-
ular voice timbre) is of paramount importance for identifying a
particular singer. On the other side of the spectrum we could iden-
tify the piano for which timing and dynamics are the most impor-
tant cues to identify a particular performer while the timbre is of
almost no importance. In the case of saxophone we conjecture that
most of the cues for performer identification come from the timbre
of the notes performed by the saxophonist. That is to say, while
timing information is certainly important and is useful to identify
a particular musician most of the information relevant for identify-
ing a performer is the timbre characteristics of the performed
notes. In this respect, the saxophone is similar to the singing voice
in which most of the information relevant for identifying a singer is
simply his or her voice’s timbre. Thus, the algorithm to identify
performers from their playing style reported in this paper aims
to detect patterns of notes based on their timbre content. Roughly,
the algorithm consists of generating a performance alphabet by
clustering similar (in terms of timbre) individual notes, inducing
for each performer a classifier which maps a note and its musical
context to a symbol in the performance alphabet (i.e. a cluster),
and given an audio fragment identify the performer as the one
whose classifier predicts best the performed fragment. More for-
mally, we are ultimately interested in obtaining a classifier MC of
the following form:
MC(MelodyFragment(n1,. . .,nk))? Performers
where MelodyFragment(n1,. . .,nk) is the set of melody fragments
composed of notes n1,. . .,nk and Performers is the set of possible sax-
ophonists to be identified. For each performer i to be identified we
trained another classifier CLi of the following form:
CLi(CNote)? AlphabetSymbol
where CNote is the set of notes played by performeri represented by
their inter-note features, i.e. each note in Note is represented by the
tuple (Pitch, Dur, PrevPitch, PrevDur, NextPitch, NextDur, Nar1, Nar2,
Nar3) as described before, and AlphabetSymbol is the set of clusters
generated by clustering all the notes performed (by all performers)
using their intra-note features.
In order to obtain the classifiers MC and CLi we use and explore
several machine learning techniques. The machine learning tech-
niques considered in this paper are the following:
 K-means clustering. Clustering techniques apply when there is no
class to be predicted but rather when the instances are to be
divided into natural groups. In k-means clustering (k is the num-
ber of clusters), k points are chosen at random as cluster centers,
each instance is assigned to the nearest cluster center, for each
cluster a new cluster center is computed by averaging over all
instances in the cluster, and the whole process is repeated with
the new cluster centers. Iteration continues until the same
instances are assigned to each cluster in consecutive rounds.
In this paper, we apply fuzzy k-means clustering where Instances
can belong to several clusters with different ‘degrees of
membership’.
 Decision trees. A decision tree classifier recursively constructs a
tree by selecting at each node the most relevant attribute. This
process gradually splits up the training set into subsets until
all instances at a node have the same classification. The selection
of the most relevant attribute at each node is based on the infor-
mation gain associated with each node of the tree (and corre-
sponding set of instances). We have applied the decision tree
building algorithm C4.5 (Quinlan, 1993).
 Support Vector Machines (SVM). SVM (Cristianini and Shawe-Tay-
lor, 2000) take great advantage of using a non-linear attribute
mapping that allows them to be able to predict non-linear mod-
els (though they remain linear in a higher dimension space).
Thus, they provide a flexible prediction, but with a higher com-
putational cost necessary to perform all the computations in the
higher dimensional space. The classification accuracy of SVM
largely depends on the choice of the kernel evaluation function
and the parameters which control the amount to which devia-
tions are tolerated (denoted by epsilon). In this paper we have
explored SVM with linear and polynomial kernels (2nd, 3rd
and 4th order).
 Artificial Neural Networks (ANN). ANN learning methods provide
a robust approach to approximating a target function. In this
paper we apply a gradient descent back propagation algorithm
(Chauvin, 1995) to tune the neural network parameters to best
fit the fMRI training set. The back propagation algorithm learns
the weights for a multi layer network, given a network with a
fixed set of units and interconnections. We set the momentum
applied to the weights during updating to 0.2 and the learning
rate (the amount the weights are updated) to 0.3. We use a
fully-connected multi layer neural network with one hidden
layer (one input neuron for each attribute and one output neu-
ron for each class).
 Lazy Methods. Lazy Methods are based on the notion of lazy
learning which subsumes a family of algorithms that store the
complete set of given (classified) examples of an underlying
example language and delay all further calculations until
requests for classifying yet unseen instances are received. In this
paper we have explored the k-Nearest Neighbor (k-NN) algo-
rithm (with k 2 {1,2,3,4,7}) which is capable of handling noisy
data well if the training set has an acceptable size. However,
k-NN does not behave well in the presence of irrelevant
attributes.
 Ensemble Methods. One obvious approach to making more reli-
able decisions is to combine the output of several different mod-
els. In this paper we explore the use of methods for combining
models (called ensemble methods) generated by machine learn-
ing. In particular, we have explored voting, stacking, bagging
and boosting. In many cases they have proved to increase predic-
tive performance over a single model. In the voting method, a set
of n different classifiers are trained on the same training data
using different learning algorithms (in this paper we applied
decision trees, SVM, ANN, and 1-NN), and prediction is per-
formed by allowing all n classifiers to ’vote’ on class prediction;
the final prediction is the class that gets the most votes. Stacking
train n learning algorithms (here we applied decision trees, SVM,
ANN, and 1-NN) in the same training data and train another
learning algorithm, the ’meta-learner’, (we applied decision
trees) to learn to predict the class from the predictions of the
base learners. Bagging draws n bootstrap samples from the train-
ing data, trains a given learning algorithm (here we consider
decision trees) on each of these n samples (producing n classifi-
ers) and predicts by simple voting of all n classifiers. Boosting
generates a series of classifiers using the same learning algorithm
(here we applied decision trees) but differently weighted exam-
ples from the same training set, and predicts by weighted major-
ity vote (weighted by accuracy) of all n classifiers.
We segmented all the recorded pieces into audio segments rep-
resenting musical phrases. Given an audio fragment denoted by a
list of notes [N1, . . . ,Nm] and a set of possible performers denoted
by a list of performers [P1, . . . ,Pn], classifier MC identifies the per-
former as follows:
MC([N1,. . .,Nm], [P1,. . .,Pn])
for each performer Pi
1520 R. Ramirez et al. / Pattern Recognition Letters 31 (2010) 1514–1523
Scorei = 0
for each note Nk
PNk = intra-note_features (Nk)
CNk = inter-note_features (Nk)
(Xk1, . . . ,Xkq) = cluster_membership (PNk)
for each performer Pi
Clusteri,k = CLi(CNk)
Scorei ¼ Scorei þ Xi;kCluster
return PM such that ScoreM = max (Score1, . . . , Scoren)
This is, for each note in the melody fragment the classifierMC com-
putes the set of its intra-note features, the set of its inter-note fea-
tures and, based on the note’s intra-note features, the cluster
membership of the note for each of the clusters (X1, . . . ,Xq are the
cluster membership for clusters 1, . . . ,q, respectively). Once this is
done, for each performer Pi its trained classifier CLi(PN) predicts a
cluster representing the expected type of note the performer
would have played in that musical context. This prediction is based
on the note’s inter-note features. The score Scorei for each per-
former i is updated by taking into account the cluster membership
of the predicted cluster (i.e. the greater the cluster membership of
the predicted cluster, the more the score of the performer is
increased). Finally, the performer with the higher score is returned.
Clearly, the classifiers CLi play a central role in the output of
classifier MC. For each performer, CLi is trained with data extracted
from the performer’s performance recordings. We have explored
different classifier induction methods (described above) for obtain-
ing each classifier CLi. The whole procedure for training classifiers
CLi is as follows:
1. Collect all training recordings by all performers.
2. Segment notes in the training recordings.
3. For each segmented note N, compute its intra-note description
PN.
4. Using the intra-note description of all segmented notes, apply
fuzzy k-means clustering (resulting in k clusters of notes, each
cluster corresponding to a set of similar notes in terms of their
intra-note description).
5. For each performer Pi,
 Collect training recordings for that performer.
 For each segmented note N in the performer’s recordings,
compute N’s inter-note description CN.
 Build a classifier (e.g. a decision tree) using the inter-note
features as attributes and its cluster (computed in step 4)
as class.
6. Return the resulting classifier (e.g. the decision tree) CLi for each
performer Pi.
The motivation for inducing the classifiers as described above is
that we would like to devise a mechanism to capture which (per-
ceptual) type of notes are played in a particular musical context
by a performer. By clustering the notes of all the performers based
on the notes’ intra-note features, we intend to obtain a number of
sets, each containing perceptually similar notes (e.g. notes with
similar timbre). By building a decision tree based on the inter-note
features of the notes of a performer, we intend to obtain a classifier
which predicts what type of notes a performer performs in a par-
ticular musical context.
4.4. Evaluation
We evaluated the induced classifiers by performing the stan-
dard 10-fold cross validation in which 10% of the melody frag-
ments is held out in turn as test data while the remaining 90% is
used as training data. When performing the 10-fold cross valida-
tion, we leave out the same number of melody fragments per class.
In order to avoid optimistic estimates of the classifier performance,
we explicitly remove from the training set all melody fragment
repetitions of the hold out fragments. This is motivated by the fact
that musicians are likely to perform a melody fragment and its rep-
etition in a similar way. Thus, the applied 10-fold cross validation
procedure, in addition to holding out a test example from the train-
ing set, also removes repetitions of the example.
5. Case studies
In this section we present two case studies on identifying per-
formers from their playing style, one in which the musicians per-
formed the pieces by reading a score in a controlled studio
environment, and another consisting of solo CD commercial per-
formances. Note that the availability of the score in the first case
study allows a complete analysis of the musical context of each
performed note and enables us to establish a very complete map-
ping from this context to particular expressive transformations.
However, in order to apply a unified methodology to both case
studies (in the other case study the score of the performance is
not available) we decided to discard the information provided by
the score.
5.1. Controlled Studio Environment Performances
5.1.1. Training data
The training data used in this case study are monophonic
recordings of four Jazz standards (Body and Soul, Once I loved, Like
Someone in Love and Up Jumped Spring) performed by three differ-
ent professional saxophonists in a controlled studio environment
(the pieces were recorded in the Audiovisual Institute’s recording
studio at the Pompeu Fabra University, expressly for the experi-
ment). The musicians were instructed to perform the selected
pieces following a metronome and were asked not to introduce
ornamentations. Each piece was performed at two different tempi
(the tempi depended on the piece, e.g. for Body and Soul the tempi
were 65 and 50 bpm). For each note in the training data, its intra-
note features and inter-note features were computed. The perfor-
mance tempo was added as an extra feature to the set of inter-note
features.
5.1.2. Results
There were a total of 792 notes available for each performer. We
segmented each of the performed pieces in phases and obtain a to-
tal of 120 short phrases and 32 long phrases for each performer.
The length of the obtained phrases and long phrases ranged from
5 to 12 notes and 40 to 62 notes, respectively. The expected classi-
fication accuracy of the default classifier (one which chooses ran-
domly one of the three performers) is 33% (measured in correctly
classified instances percentage). In the short-phrase case, the aver-
age accuracy and the accuracy obtained for the most successful
trained classifier was 97.0% and 98.4%, respectively. In the long-
phrase case, the average accuracy and the accuracy obtained for
the most successful trained classifier was 96.7% and 98.0%, respec-
tively. The correctly classified instances percentage for each learn-
ing method is presented in Table 1. The results for short and long
phrases seem to indicate that it is indeed feasible to train success-
ful classifiers to identify performers from their playing style using
the considered perceptual and contextual features. It must be
noted that the performances in our training data were recorded
in a controlled environment in which the gain level was constant
for each performer. Some of the features (e.g. attack level) included
in the perceptual description of the notes take advantage of this
property and provide very useful information in the learning
R. Ramirez et al. / Pattern Recognition Letters 31 (2010) 1514–1523 1521
process. However, this recording requirement is not realistic in a
general setting where we may obtain performances recorded un-
der very different circumstances. In order to determine to which
extent the results extend to a general setting, we consider com-
mercial recordings from famous saxophonists in the next section.
5.2. Solo commercial performances
5.2.1. Training data
The data used in this case study are monophonic audio com-
mercial recordings of improvisations performed by four famous
Jazz saxophonists: Billie Pierce (Aria’s Prance, Chelsea Bridge, In Your
Own Sweet Way), Joe Henderson (Lush Life, Modinha), Branford Mar-
salis (St Thomas) and Kenny Garrett (Last Sax). As opposed to the
experiments reported in the previous section, here each musician
performed a set of different pieces. Each of the recordings is ana-
lyzed as described before: the recordings are segmented into notes
and for each note its intra-note and inter-note features are com-
puted. In order to have a similar number of instances (notes and
musical phrases) we selected a subset of the recordings.
5.2.2. Results
On average, there were a total of 820 notes available for each
performer. We segmented each of the performed pieces in phases
and obtain an average of 130 short phrases and 17 long phrases for
each performer. The length of the short phrases and long phrases
ranged from 4 to 10 notes and 30 to 60 notes, respectively. The ex-
pected classification accuracy of the default classifier (one which
chooses randomly one of the four performers) is 25% (measured
in correctly classified instances percentage). In the short-phrase
case, the average accuracy and the accuracy obtained for the most
successful trained classifier was 71.9% and 74.7%, respectively. In
the long-phrase case, the average accuracy and the accuracy ob-
tained for the most successful trained classifier was 71.1% and
74.9%, respectively. The correctly classified instances percentage
for each learning method is presented in Table 2. The results for
short and long phrases seem to indicate that the considered in-
tra-note and inter-note features are indeed useful for training suc-
cessful classifiers to identify performers from their playing style. It
must be noted that the performances in our training data were re-
corded in a non controlled environment.
5.2.3. Discussion
The difference between the results obtained in the case studies
and the accuracy of a baseline classifiers, i.e. the classifier guessing
at random, indicates that the intra-note and inter-note features
presented contain sufficient information to identify the studied
set of performers, and that the machine learning methods explored
are capable of learning performance patterns that distinguish these
performers. It is worth noting that every learning algorithm inves-
tigated (decision trees, SVM, ANN, k-NN and the reported ensemble
methods) produced considerably better than random classification
accuracies. This supports our statement about the feasibility of
training successful classifiers for the case studies reported.
As mentioned before, the performances in the first case study
were recorded in a controlled environment in which the gain level
was constant for each performer. Some of the features (e.g. attack
level) included in the perceptual description of the notes take
advantage of this property and provide very useful information
in the learning process. However, for the performances in the sec-
ond case study we do not have information about the recording
conditions (in fact the conditions surely were very different for dif-
ferent performers). This may explain the difference in accuracies
between the first and second case studies.
We have selected three types of musical segment lengths: 1-
note segments, short-phrase segments (4–12 notes), and long-
phrase segment (30–62 notes). Evaluation using 1-note segments
results in poor classification accuracies, while short-phrase seg-
ments and long-phrase segment evaluation results in accuracies
well above the accuracy of a baseline classifier. Interestingly, there
is no substantial difference in the accuracies for short-phrase and
long-phrase segment evaluation which seems to indicate that in
order to identify a particular performer it is sufficient to consider
a short-phrase segment of the piece, i.e. the identification accuracy
does not increase substantially by considering a longer segment.
The poor results of the 1-note evaluation may indicate that
although intra-note features are very important, it is not sufficient
to consider them in a one note basis. Just as a human Jazz expert
would have problems identifying saxophonists form listening to
one note audio files, the trained classifiers are not able to identify
the performers reliably given this limited information. As soon as
there are more notes involved together with the context in which
they appear, the trained classifier (just as a Jazz expert) is able to
identify the musician.
It is worth mentioning that ideally we would have liked to con-
sider in all the experiments a data set containing the same set of
pieces for each performer. However, in the second case study it
was impossible to get hold of the same pieces by the different per-
formers. This contrasts with other approaches (e.g. Saunders et al.,
2004) in which the same set of performed pieces is available for the
different performers.
One issue which is not clear from the reported case studies is
what features are mostly responsible for the identification results.
As mentioned before, we conjecture that a great part of the cues for
performer identification in saxophone performances come from
the timbre of the notes, i.e. the intra-note features. This is to say,
while timing information is certainly important and is useful to
identify a particular musician most of the information relevant
for identifying a performer is the timbre characteristics of the per-
formed notes. In order to investigate this hypothesis we have per-
formed an additional experiment in which we have trained
performer classifiers based only on the note-level timing and
Table 1
Classification accuracy for the 1-note, short-phrase and long-phrase cases (in
correctly classified instances percentage) for the controlled studio environment
performances case study.
1-Note Short-phrase Long-phrase
Decision trees 37.4 95.1 95.8
Support Vector Machines 41.5 97.5 96.5
Artificial Neural Networks 39.8 97.5 95.6
k-Nearest Neighbor 31.2 97.5 96.5
Bagging (decision trees) 38.6 98.4 98.0
Boosting (decision trees) 39.4 95.1 96.2
Voting (decision trees,
SVM, ANN, 1-NN)
42.7 97.5 97.2
Stacking (decision trees,
SVM, ANN, 1-NN)
44.9 97.5 97.9
Table 2
Classification accuracy for the 1-note, short-phrase and long-phrase cases (in
correctly classified instances percentage) for the solo commercial performances case
study.
1-Note Short-phrase Long-phrase
Decision trees 23.6 68.3 67.4
Support Vector Machines 27.6 74.7 74.9
Artificial Neural Networks 25.4 72.6 71.9
k-Nearest Neighbor 24.3 71.0 72.2
Bagging (decision trees) 26.4 70.4 66.3
Boosting (decision trees) 24.8 71.2 67.8
Voting (decision trees,
SVM, ANN, 1-NN)
28.3 70.8 72.4
Stacking (decision trees,
SVM, ANN, 1-NN)
29.0 76.3 76.3
1522 R. Ramirez et al. / Pattern Recognition Letters 31 (2010) 1514–1523
energy information in the performances. We have induced models
predicting the timing (i.e. note duration) and energy (i.e. note
mean energy) using the data from the controlled studio environ-
ment performances (first case study). For each performer Pi consid-
ered we have trained a model Mi predicting for a given note the
pair (Duration,Energy) representing the duration transformation
and mean energy variation for that note. As before, for each note
in an input melody, we use this information to update score Si of
performer Pi.After considering all notes in the melody, the per-
former with maximum score is the one identified by the system.
In the short-phrase case, the average accuracy and the accuracy ob-
tained for the most successful trained classifier (obtained with the
Support Vector Machine algorithm) was 48% and 53%, respectively.
In the long-phrase case, the average accuracy and the accuracy ob-
tained for the most successful trained classifier (obtained with the
boosting algorithm) was 48% and 51%, respectively. These results
seem to indicate that there is some performer-specific information
in the duration-energy models but the models are certainly more
accurate at identifying interpreters when intra-note information
is included. Having said that, it is clear that these results depend
on the set of performers under consideration, i.e. some performers
may differ from each other mainly by their timbre while others
may differ also in the way they apply dynamic and tempo transfor-
mations in their performances.
6. Conclusions
In this paper, we concentrated on the task of automatic identi-
fication of saxophone performers based on their playing style. We
have applied sound analysis techniques to monophonic audio
recordings in order to extract features such as pitch, timing, ampli-
tude and timbre, characterising both the internal structure of notes
and the musical context in which they appear. We explored and
compared different machine learning techniques for this task.
The main contribution of this work is the development of success-
ful classifiers for the identification of performers not only on
recordings obtained in a controlled environment, but on CD com-
mercial recordings. The results obtained indicate that the extracted
features contain sufficient information to identify the studied set of
performers, and that the machine learning methods explored are
capable of learning performance patterns that distinguish these
performers. We plan to extend the set of intra-note descriptors
with relevant descriptors such as vibrato and extend our approach
to performance-based performer identification in polyphonic mul-
ti-instrument audio recordings.
Acknowledgements
This work was supported by the Spanish Ministry of Science
and Innovation under grants TIN2006-14932-C02-01 ProSeMus
Project and TIN2009-14247-C02-01 DRIMS Project.
References
Apel, W., 1972. Harvard Dictionary of Music. Harvard University Press, Cambridge,
MA.
Bernstein, A.D., Cooper, E.D., 1976. The piecewise-linear technique of electronic
music synthesis. J. Audio Eng. Soc. 6 (24), 446–454.
Bresin, R., 2002. Articulation rules for automatic music performance. In: Proc. 2001
Internat. Comput. Music Conf.. International Computer Music Association, San
Francisco.
Canazza, S., De Poli, G., Roda, A., Vidolin, A., 1997. Analysis and synthesis of
expressive intention in a clarinet performance. In: Proc. 1997 Internat. Comput.
Music Conf.. International Computer Music Association, San Francisco, pp. 113–
120.
Chauvin, Y. et al., 1995. Backpropagation: Theory, Architectures and Applications.
Lawrence Erlbaum Assoc.
Cristianini, N., Shawe-Taylor, J., 2000. An Introduction to Support Vector Machines.
Cambridge University Press.
Dannenberg, R.D., Pellerin, H., Derenyi, 1998. A study of trumpet envelopes. In: Proc.
Internat. Comput. Music Conf. (ICMC), San Francisco.
Dovey, M.J., 1995. Analysis of Rachmaninoff’s piano performances using inductive
logic programming. In: Eur. Conf. on Machine Learning. Springer-Verlag.
Friberg, A., Bresin, R., Fryden, L., 2000. Music from motion: Sound level envelopes of
tones expressing human locomotion. J. New Music Res. 29 (3), 199–210.
Friberg, A., Bresin, R., Sundberg, J., 2006. Overview of the KTH rule system for
musical performance. Adv. Cognit. Psychol. 2 (2–3), 145–161 (Special Issue on
Music Performance).
Gabrielsson, A., 1999. The performance of music. In: Deutsch, D. (Ed.), The
Psychology of Music, second ed. Academic Press.
Gabrielsson, A., 2003. Music performance research at the millennium. Psychol.
Music 31 (3), 221–272.
Gómez, E., Klapuri, A., Meudic, B., 2003. Melody description and extraction in the
context of music content processing. J. New Music Res. 32.
Jenssen, K., 1999. Envelope model of isolated musical sounds. In: Proc. COST G-6
Workshop on Digital Audio Effects (DAFx), Trondheim.
Johnson, M.L., 1992. An expert system for the articulation of Bach fugue melodies.
In: Baggi, D.L. (Ed.), Readings in Computer-Generated Music. IEEE Computer
Society, pp. 41–51.
Juslin, P.N., Sloboda, J.A., 2001. Musical Emotion: Theory and Research. Oxford
University Press, New York.
Kendall, R.A., Carterette, E.C., 1990. The communication of musical expression.
Music Percept. 8 (2), 129–164.
Klapuri, A., 1999. Sound onset detection by applying psychoacoustic knowledge.
In: Proc. IEEE Internat. Conf. on Acoustics, Speech and Signal Processing,
ICASSP.
Lopez de Mantaras, R., Arcos, J.L., 2002. AI and music, from composition to
expressive performance. AI Mag. 23 (3).
Maestre, E., Gomez, E., 2005. Automatic characterization of dynamics and
articulation of monophonic expressive recordings. In: Proc. 118th AES
Convention, Barcelona, Spain.
McNab, R.J., Smith, Ll.A., Witten, I.H., 1996. Signal Processing for Melody
Transcription. SIG working paper, vol. 95-22.
Mitchell, T.M., 1997. Machine Learning. McGraw-Hill.
Narmour, E., 1990. The Analysis and Cognition of Basic Melodic Structures: The
Implication Realization Model. University of Chicago Press.
Narmour, E., 1991. The Analysis and Cognition of Melodic Complexity: The
Implication Realization Model. University of Chicago Press.
Quinlan, J.R., 1993. C4.5: Programs for Machine Learning. Morgan Kaufmann, San
Francisco.
Ramirez, R., Hazan, A., 2006. A tool for generating and explaining expressive music
performances of monophonic Jazz melodies. Internat. J. Artif. Intell. Tools 15 (4),
673–691.
Ramirez, R., Hazan, A., Maestre, E., 2005. Intra-note features prediction model for
jazz saxophone performance. In: Proc. 2005 Internat. Comput. Music Conf.,
Barcelona, Spain.
Ramirez, R., Hazan, A., Maestre, E., Serra, X., 2006. A Data Mining Approach to
Expressive Music Performance Modeling. Multimedia Data Mining and
Knowledge Discovery. Springer-Verlag.
Ramirez, R., Hazan, A., Maestre, E., Serra, X., 2008. A genetic rule-based expressive
performance model for Jazz saxophone. Comput. Music J. 32 (1), 38–50.
Repp, B.H., 1992. Diversity and commonality in music performance: An analysis of
timing microstructure in Schumann’s ‘Traumerei’. J. Acoust. Soc. Amer. 104.
Saunders, C., Hardoon, D., Shawe-Taylor, J., Widmer, G., 2004. Using string kernels to
identify famous performers from their playing style. In: Proc. 15th Eur. Conf. on
Machine Learning (ECML’2004), Pisa, Italy.
Stamatatos, E., Widmer, G., 2005. Automatic identification of music performers with
learning ensembles. Artif. Intell. 165 (1), 37–56.
Tobudic, A., Widmer, G., 2003. Relational IBL in music with a new structural
similarity measure. In: Proc. Internat. Conf. on Inductive Logic Programming.
Springer-Verlag.
Todd, N., 1992. The dynamics of dynamics: A model of musical expression. J. Acoust.
Soc. Amer. 91.
Van Baelen, E., De Raedt, L., 1996. Analysis and prediction of piano performances
using inductive logic programming. In: Internat. Conf. in Inductive Logic
Programming, pp. 55–71.
Widmer, G., 2001. Discovering strong principles of expressive music performance
with the PLCG rule learning strategy. In: Proc. 12th Eur. Conf. on Machine
Learning (ECML’01), Freiburg, Germany. Springer-Verlag, Berlin.
Widmer, G., 2002. Machine discoveries: A few simple, robust local expression
principles. J. New Music Res. 31 (1), 37–50.
Widmer, G., Goebl, W., 2004. Computational models of expressive music
performance: The state of the art. J. New Music Res. 33 (3), 203–216.
R. Ramirez et al. / Pattern Recognition Letters 31 (2010) 1514–1523 1523
