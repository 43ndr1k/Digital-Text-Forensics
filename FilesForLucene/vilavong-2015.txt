International Journal of Computer Science and Telecommunications [Volume 6, Issue 7, July 2015]                                           8 
Journal Homepage: www.ijcst.org 
 
 
Souksan Vilavong
1
 and Khanh Phan Huy
2
 
1
Champasak University, Ban Chat Sanh, Pakse, Laos 
2
Danang University of Technology, The University of Danang, Danang, Vietnam 
1
ssuchedu@yahoo.com, 
2
khanhph29@gmail.com 
 
 
 
 
Abstract—Text categorization is one of the most important role 
in many applications in natural language processing (NLP). The 
task of text classification is assignment of free text document to 
one or more predefined categories based on their content. 
Whereas a wide range of methods have been applied to English 
text classification, relatively very few studies have been done on 
Lao text. In this paper, we present methodology for Lao 
document presentation and two of the best machine learning 
techniques, which have namely Radial Basis Function (RBF) 
network and support vector machines (SVM), to classify the 
documents. Experimental results revealed that these approaches 
could achieve an average about 82% accuracy. Additionally, we 
also analyze the advantages and disadvantages of each approach 
to find out the best method in specific circumstances.  
 
Index Terms— Machine Learning, Comparison, Natural 
Language Processing (NLP) and Lao Text Categorization 
 
I. INTRODUCTION 
EXT categorization has been one of the most popular 
problem in natural language processing. It is based 
module in many applications and most of categorization 
systems classify documents into one or more given predefined 
categories. It has been utilized in many application areas such 
as, spam filtering [25]; language identification [18]; genre 
classification [27]; customer relationship management [7],  
web page classification [23], text sentinel classification [31] 
and astronomy [15]. Depending on the user's requirement, 
each document can be categorized into multiple, exactly one, 
or no category at all [2]. 
Text categorization can be considered and resolve by many 
methods and machine learning is one of the most performance 
method. Text categorization based on machine learning 
techniques is a general inductive process automatically builds 
a classifier by learning, from a set of predefined categories 
of documents, the characteristics of the categories. For 
instance, Naive Bayes classifiers [32], N-gram [16], k-Nearest 
Neighbor (k-NN) [13] and Support Vector Machine (SVM) 
[14],... Many research publications that studies evaluated the 
performance of these methods based on accuracy of document 
classification, and most of the studies focused on automatic 
text categorization for documents, which were written in 
English. Previous works on Lao text categorization is very 
limited. This may be due to the nature of Lao language and 
also to the lack of Lao language resources such as labeled 
corpus. 
The automatic text categorization process foresees a set of 
tasks universally recognized by the research community [1]. 
These tasks include features extract, feature selection and 
feature weighting processes are performed. Moreover, these 
tasks also include training task in a machine learning classifier 
is trained using a set of labeled documents. Finally, the last 
task is the Testing task the accuracy of the classifier is 
evaluated by using a set of pre-labeled documents (i.e. test-set) 
that are not used in the training phase. In this paper, we have 
used two supervised classification models for Lao text 
categorization. It presents an empirical comparison of these 
supervised machine learning classifiers (SVN, RBF). In order 
to evaluate those classification models, we collected Lao 
predefined categorized from online Lao newspapers archives, 
namely; Lao News Agency, and Vientianetimes News. This 
corpus was collected from five categories: Politics, 
Economics, Crime, Education, Tourism and Sport
1
.  
The rest of this paper is organized as follows: Section II 
presents an overview about some related works in the area of 
Lao text categorization. Some basic concepts for text 
categorization and our approaches for text categorization are 
described in Section III. Next in Section IV, we present the 
experiment setup and discuss on the experimental results. 
Finally, we describe about conclusion and indicate future 
works in Section V. 
II. RELATED WORKS 
Text categorization stands at the cross junction to modern 
information retrieval and machine learning. In last decade, 
there are many previous works to categorize english 
documents automatically [8]. Applications of machine 
learning techniques help to reduce the manual effort required 
in analysis and the accuracy of the systems also improved 
through the use of these techniques. For instance, N-gram [4, 
 
1 http://kpl.gov.la. 
T 
Comparison on Some Machine Learning Methods for Lao 
Text Categorization 
ISSN 2047-3338 
Asaf Sarajlic and Mirko Skrbic                                                                             9 
11]; k-NN [4, 30]; Decision Tree [3] and SVM [14]. 
Lao is similar to other South East Asian languages, such as: 
Chinese, Thai, Vietnamese, etc. A text is a string of symbols 
with no explicit word boundary. Spaces between syllables are 
rarely used for separating is main difficulty in many tasks of 
natural language processing. May text categorization 
publications on these language has played important role for 
our purpose on Lao language such as Association rule [5], N-
Gram [28], Naive Bayes [17] for Chinese; SVM [19]; Bag Of 
Words(BOW) [12]; N-Gram [12], L-KNN [20],... Especially 
in terms of spoken and writing system, Lao has closest 
relationships with Thai language, so many researches about 
text categorization on Thai have an direct influence on Lao 
language. For instance, SVM  [6] Naive Bayesian, Decision 
Tree, k-Nearest Neighbor and RBF network [21]. These 
results have several implications for Lao text classification 
problem. Many researchers attempted to obtain better 
classification algorithms performance for automatic text 
categorization.  SVM and RBF network are considered as the 
common methods to text categorization [24, 29]. Therefore, 
they have been treated as the base method for categorizing 
text. Thus, in this paper, we include SVM and RBF network, 
and our feature selection, in our experiment to find out the 
most suitable method in categorizing Lao text. 
III. OUR APPROACHES 
In our approach, we proposed model for Lao text 
categorization as Figure 1. This model can be summarized in 
two sub-components that are pre-processing and classification 
component. Pre-processing component will transform internet 
documents to text documents (plain text), after reading the 
input text document by the proposed system which divides 
that text document into features which are also called (tokens, 
words, terms or attributes); indexing corpus and weights 
computation on these texts for building model. It represents 
that text document in a vector space as a vector whose 
components are that features and their weights which are 
computed by the frequency of each feature in that text 
document. 
 
 
Figure 1.  The process of Lao text categorization system 
A. Word segmentation 
We used Htttrack
2
 tool for get html code of all news from 
website the they is divided into two sets: Training set and Test 
set. The first step in text categorisation is to transform 
documents, which typically are strings of characters, into a 
representation suitable for the learning algorithm and the 
classification task. Because the corpus were gotten from 
website so that the text transformation usually involves of the 
following processes: removing HTML tags, removing 
stopwords as Figure 2, and performing word stemming. The 
stopwords are frequent words that carry no information (i.e., 
pronouns, prepositions, conjunctions etc.). After text 
transformation, we use word segmentation method in research 
of Srithirath et al [26]. The results of this step will be used for 
features statistics processing. 
 
 
 
Figure 2.  The list some stopwords in Lao 
B. Indexing 
Vector space model is one of the most commonly document 
representation methods for text categorization. In the vector 
space model, documents are represented by vectors of words. 
Usually, one has a collection of documents which is 
represented by a word by document matrix M, where each 
entry represents the occurrences of a word in a document. 
 ikM m                           (1) 
where ikm is weight of word i in document k. The number of 
rows, N of the matrix M corresponds to the number of words 
in dictionary and can be very large. So that, the high 
dimensionality of the feature space is a major characteristic, or 
difficulty of text categorization problems. We will present 
methods to reduce dimensionality of the matrix in next 
section. There are several ways of determining the weight ikm
of word i in document k. In this work, we use one of the most 
effective weighting scheme, which is Entropy weighting [9]. 
In this scheme, ikm is given by: 
 
  1
1
log 1.0 1 log
log
ND
ij ij
ik ik
j i i
f f
m f
ND n n
   
          
 (2) 
where: 
 ikf be the frequency of word i in document k; 
 ND be the number of documents in the collection; 
 
2 https://www.httrack.com 
International Journal of Computer Science and Telecommunications [Volume 6, Issue 7, July 2015]                                          10 
 ni the total number of times word i occurs in the 
whole collection. 
  1
1
log
log
ND
ij ij
j i i
f f
ND n n
  
  
   
                                (3) 
is the average uncertainty or entropy of word i. This quantity 
is -1 if the word is equally distributed over all documents and 
0 if the word occurs in only one document. 
C. Dimensionality reduction 
In text categorization field, features are extracted from 
documents are often expressed as vector pattern (keyword, 
weight). As more number of documents, more number of 
words are extracted and the feature space could contain more 
than several hundreds to thousands words. One of the most 
important module in dimensionality reduction is feature 
selection method. The weight can be computed by different 
methods, such as information gain (IG) and gain ratio [10]. 
Information Gain measures the number of bits of information 
obtained for category prediction by knowing the presence or 
absence of a word in at document. Let 1 2, ,..., Kc c c  denote the 
set of possible categories. The information gain of a word w is 
defined as follow: 
         
     
1 1
1
log log
log
K k
j j j
j j
k
j j
j
IG w P c P w P c w P c w
P w P c w P c w
 

  

 

 (4) 
where  jP c can be estimated from the fraction of 
documents in the total collection that belongs to category cj 
and  P w  from the fraction of documents in which the word 
w occurs. Moreover  jP c w can be computed as the fraction 
of documents from category cj that have at least one 
occurrence of word w and  jP c w  as the fraction of 
documents from category cj that does not contain word w. The 
information gain is computed for each word of the training set 
and the words whose information gain is less than some 
predetermined threshold are removed. Gain ratio is an 
extension of information gain which selects words that have 
maximized the ratio of its gain divided by its entropy [9]. The 
gain ratio of word w is defined as: 
 
   
 
H cate H cate w
G w
H w

                                (5) 
 
where H is the entropy. 
D. Classification methods 
As we mentioned before, we choose two of the best 
efficient classifier methods for some languages which has very 
close relationship with Lao and used them for Lao text 
classification; SVM and RBF neural network methods due to 
their simplicity, effectiveness and accurateness. Brief 
descriptions of these methods are given, as follows: 
1) Support Vector Machine Learning 
SVM is a robust machine learning methodology which 
shows high performance on text classification [14]. Depending 
on the purpose of classification, the SVM can be constructed 
as a linear or nonlinear model. Let us consider a for instance, 
given that the training dataset X contains n labeled sample 
vectors    1 1, ,..., ,n nx y x y , where each xi is a feature vector 
of the document i and each yi is the class label of the document 
i. The linear SVM uses a weight vector w and a bias term b to 
classify a new example x, by creating a predicted class label 
f(x) as given in below: 
   ,f x sign w x b                                   (6) 
For the non-separable case, the training errors are allowed 
so that the linear SVM finds the vector w by minimizing the 
objective function over all n training samples as shown: 
1
21
( , )
2
i
i
n
T w w C 

                                   (7) 
under the constraints that 
   1.. : , 1 , 0i i i ii n y w x b         
However, for the problem of text classification using SVM 
method, the selection of properties for each classification are 
extremely important issues, it decided to classify the 
efficiency of algorithm. In this work, we employed the Platt’s 
SMO algorithm [22] with default parameter for building the 
support vector machine classification model. 
2) Radial Basis Function Network 
RBF network has been widely used for pattern 
classification, function approximation and text classification. 
The RBF network is a three-layer feed-forward neural 
network, between the input and the output layers there is a 
"hidden layer". In the training phase, vectors are input to the 
first layer and fanned out to the hidden layer. In the latter, a 
cluster of RBF functions turn the input to output, adjusting the 
weight of the input to the hidden layer. Then, under the target 
vector’s supervising, the weigh of the output vector of the 
hidden layer is adjusted. When clustering texts, the Euclidean 
Distance between the input vectors and the weight vectors, 
which have been adjusted by training process, is calculated. 
Each input sample is sorted to a class. Then the output layer 
collects samples belonging to same classes and organizes an 
output vector, the final clustering. 
Asaf Sarajlic and Mirko Skrbic                                                                             11 
In the hidden nodes, the activation function is usually 
chosen as Gaussian Function, the input of node i is the product 
of threshold bi and the Euclidean Distance between weight 
vector W and input vector X: 
 
2
*q qi j ji i
j
k x w b                                  (8) 
Where, 
q
ix is the
thj component of the thq input vector, jiw
is the weight between the
thj node in the input layer and the 
thi  node in the hidden layer, 
ib is a threshold to control the 
accuracy of the Gaussian Function. The output of the same 
node is as follows: 
    
2 2
*q q qi i j ji i
j
r exp k exp x w b
 
     
 
            (9) 
Instead of adjusting ib , we can use the parameter of spread 
in Neural Network Toolbox of Matlab 7.0 to control the 
performance of the network. The larger spread is, the 
smoother the function approximation will be. The input of the 
output layer is weight sum of the output of the nodes in hidden 
layer. The activation function is linear, so the output of the 
whole network, in response to the qth component of the input, 
is shown as: 
1
*
i
q
i i
n
y r V

                                           (10) 
where iv  is 
thi component of weight vector from the hidden 
layer to the output layer. 
As is discussed above, RBF has a strong capability of 
approximation to the kernel vector in a limited part of the 
whole net. The training of the RBF network should be divided 
into two processes. The first is unsupervised learning, which 
adjusts the weight vector between the input and hidden layer. 
The other is supervised learning, which adjusts the weight 
vector between the hidden and output layer. Three parameters 
should be given before training: input vector, target vector and 
the threshold value, in Matlab, the spread. 
IV. EXPERIMENTS AND RESULTS 
A. Experimental setup 
In order to evaluate the used classification algorithms, 
several experiments have been conducted. We have measured 
the performance of these classification algorithms on manually 
classified Lao corpus collected from online Lao newspapers 
archives from Vientianetimes News, and Lao News Agency. 
This corpus contains 4000 documents that are different in 
length and divided into eight categories: Politics, Economics, 
Crime, Education, Tourism, and Sport. The training corpus 
contains 90% (3600) documents in six different categories, 
and the rest 10% (400) documents are used as testing samples, 
with average about 65 documents in each category. To 
measure the performance of these classification methods, we 
use the results of calculating Precision and Recall: 
1. Accuracy (A): Is the ratio between the number of text 
documents which were correctly categorized and the total 
number of documents. 
i i
i
i i i i
TP TN
A
TP TN FP FN


  
                       (11) 
where 
iTP  (true positives) is the number of text documents 
correctly classified in category
ic  , iTN  (true negatives) is the 
number of text documents correctly classified as not belonging 
to category
ic , iFP  (false positives) is the number of text 
documents incorrectly classified in category ic , and iFN  (false 
negatives) is the number of text documents incorrectly 
classified as not belonging to category ic . 
2. Error rate (E): Is the ratio between the number of text 
documents which were not correctly categorized and the total 
number of text documents. 
1 i ii i
i i i i
FP FN
E Ac
TP TN FP FN

  
  
                      (11) 
3. Precision (P): Is the percentage of correctly categorized 
text documents among all text documents that were assigned 
to the category by the classifier. 
i
i
i i
TP
P
TP FP


                                       (12) 
4. Recall (R): Is the percentage of correctly categorized text 
documents among all text documents belonging to that 
category. 
i
i
i i
TP
R
TP FN


                                       (13) 
B. Experimental results 
Some our experimental results are listed in the following: 
 
 
Figure 3.  Experimental Result of the SVM algorithm 
International Journal of Computer Science and Telecommunications [Volume 6, Issue 7, July 2015]                                          12 
 
Figure 4.  Experimental Result of the RBF algorithm 
 
Figure 5.  Experimental Result for comparison of the RBF network and SVM 
 
Figure 6.  Diagram for comparison of the RBF network and SVM 
V. CONCLUSION AND FUTURE WORK 
This paper presents some research on Lao text 
categorization using machine learning techniques. SMV 
algorithm is more simple than RBF but not easy to find set of 
parameters for many language. In general, the RBF algorithm 
is selected for constructing the classification model since it 
gives better results than other. However, with both algorithm, 
there are many documents were misclassified into other 
groups. We investigated this problem and observed that the 
training dataset contains a small number research’s corpus and 
the results of word segmentation has clearly affect for actual 
text categorization. Future work will investigate the impact of 
skewed class distribution of the training dataset to the 
accuracy of the classification model. Moreover, we will 
improve the accuracy of the classification model by 
considering improving word segmentation task and other 
features such as part of speech of words, semantic of phrase, 
etc. 
REFERENCES 
[1] KDD '07: Proceedings of the 13th ACM SIGKDD 
International Conference on Knowledge Discovery and Data 
Mining, New York, NY, USA, 2007. ACM. 618070. 
[2] Hamood Alshalabi, Sabrina Tiun, Nazlia Omar, and 
Mohammed Albared. Experiments on the use of feature 
selection and machine learning methods in automatic malay 
text categorization. Procedia Technology, 11(0):748 _ 754, 
2013. 4th International Conference on Electrical Engineering 
and Informatics, {ICEEI} 2013. 
[3] Chidanand Apté, Fred Damerau, and Sholom M. Weiss. 
Automated learning of decision rules for text categorization. 
ACM Trans. Inf. Syst.,12(3):233_251, July 1994. 
[4] William B. Cavnar and John M. Trenkle. N-grambased text 
categorization. In In Proceedings of SDAIR-94, 3rd Annual 
Symposium on Document Analysis and Information Retrieval, 
pages 161-175, 1994. 
[5] Ding-An Chiang, Huan-Chao Keh, Hui-Hua Huang, and 
Derming Chyr. The chinese text categorization system with 
association rule and category priority. Expert Systems with 
Applications, 35(1_2):102 -110, 2008. 
[6] N. Chirawichitchai, P. Sa-nguansat, and P. Meesad. 
Developing an e_ective thai document categorization 
framework base on term relevance frequency weighting. In 
Knowledge Engineering, 2010 8th International Conference on 
ICT and, pages 19-23, Nov 2010. 
[7] Kristof Coussement and Dirk Van den Poel. Integrating the 
voice of customers through call center emails into a decision 
support system for churn prediction. Information Management, 
45(3):164-174, 2008. 
[8] Anirban Dasgupta, Petros Drineas, Boulos Harb, Vanja 
Josifovski, and Michael W. Mahoney. Feature selection 
methods for text classi_cation. In Proceedings of the 13th 
ACM SIGKDD International Conference on Knowledge 
Discovery and Data Mining, KDD '07, pages 230-239, New 
York, NY, USA, 2007. ACM. 
[9] Susan T. Dumais. Improving the retrieval of information from 
external sources. Behavior Research Methods, 23(2):229_236, 
June 1991. 
[10] Irene Diaz Jose Ranilla Elias F. Combarro, Elena Montanes 
and Ricardo Mones. Introducing a family of linear measures 
for feature selection in text categorization. IEEE Transactions 
on Knowledge and Data Engineering, pages 1223_1232. 
[11] Johannes Fürnkranz. A study using n-gram features for text 
categorization, 1998. 
[12] Vu Cong Duy Hoang, Dien Dinh, Nguyen Le Nguyen, and 
Hung Quoc Ngo. A comparative study on vietnamese text 
classi_cation methods. In Research, Innovation and Vision for 
the Future, 2007 IEEE International Conference on, pages 
267_273, March 2007. 
[13] Shengyi Jiang, Guansong Pang, Meiling Wu, and Limin 
Kuang. An improved k-nearest-neighbor algorithm for text 
categorization. Expert Syst. Appl., 39(1):1503_1509, January 
2012. 
[14] Thorsten Joachims. Text categorization with suport vector 
machines: Learning with many relevant features. In 
Proceedings of the 10th European Conference on Machine 
Learning, ECML '98, pages 137_142, London, UK, UK, 1998. 
Springer-Verlag. 
[15] Huaizhong Kou, Amedeo Napoli, and Yannick Toussaint. 
Application of text categorization to astronomyeld. In Natural 
Language Processing and Information Systems, 10th 
International Conference on Applications of Natural Language 
to Information Systems, NLDB 2005, Alicante, Spain, June 15-
17, 2005, Proceedings, pages 32_43, 2005. 
[16] Jingyang Li and Maosong Sun. Non-independent term 
selection for chinese text categorization. Tsinghua Science 
Technology, 14(1):113 _ 120, 2009. 
[17] Lei LI, Yu guang HUANG, and Zhong wan LIU. Chinese text 
classi_cation for small sample set. The Journal of China 
Asaf Sarajlic and Mirko Skrbic                                                                             13 
Universities of Posts and Telecommunications, 18, Supplement 
1(0):83 _ 89, 2011. 
[18] Kavi Narayana Murthy and Guntur Bharadwaja Kumar. 
Language identi_cation from small text samples. Journal of 
Quantitative Linguistics, 13(1):57_80, 2006. 
[19] Giang-Son Nguyen, Xiaoying Gao, and Peter Andreae. 
Vietnamese document representation and classi_cation. In Ann 
Nicholson and Xiaodong Li, editors, AI 2009: Advances in 
Arti_cial Intelligence, volume 5866 of Lecture Notes in 
Computer Science, pages 577_586. Springer Berlin 
Heidelberg, 2009. 
[20] Tu-Anh Nguyen-Hoang and Kiem Hoang. Frequent subgraph-
based approach for classifying vietnamese text documents. In 
Joaquim Filipe and José Cordeiro, editors, ICEIS, volume 24 of 
Lecture Notes in Business Information Processing, pages 
299_308. Springer, 2009. 
[21] Nattira Muangmala Phimphaka Taninpong. Classification of 
thai independent study in statistics using data mining 
techniques, 2013. 
[22] John C. Platt. Sequential minimal optimization: A fast 
algorithm for training support vector machines. Technical 
report, ADVANCES IN KERNEL METHODS - SUPPORT 
VECTOR LEARNING, 1998. 
[23] Xiaoguang Qi and Brian D. Davison. Web page classi_cation: 
Features and algorithms. ACM Comput. Surv., 
41(2):12:1_12:31, February 2009. 
[24] Monica Rogati and Yiming Yang. Highperforming feature 
selection for text classi_cation. In Proceedings of the Eleventh 
International Conference on Information and Knowledge 
Management, CIKM '02, pages 659_661, New York, NY, 
USA, 2002. ACM. 
[25] G. Sakkis, I. Androutsopoulos, G. Paliouras, V. Karkaletsis, 
C.D. Spyropoulos, and P. Stamatopoulos. A memory-based 
approach to antispam filtering for mailing lists. Information 
Retrieval, 6(1):49_73, 2003. cited By 117. 
[26] A. Srithirath and P. Seresangtakul. A hybrid approach to lao 
word segmentation using longest syllable level matching with 
named entities recognition. In Electrical 
Engineering/Electronics, Computer, Telecommunications and 
Information Technology (ECTI-CON), 2013 10th International 
Conference on, pages 1_5, May 2013. 
[27] Efstathios Stamatatos, George Kokkinakis, and Nikos 
Fakotakis. Automatic text categorization in terms of genre and 
author. Comput. Linguist., 26(4):471_495, December 2000. 
[28] M. Suzuki, N. Yamagishi, and Yi-Ching Tsai. Chinese text 
categorization using the character ngram. In Information 
Theory and its Applications (ISITA), 2012 International 
Symposium on, pages 722_726, Oct 2012. 
[29] Ah-Hwee Tan and et al. Text categorization, supervised 
learning, and domain knowledge integration. In IN 
PROCEEDINGS OF KDD-2000: WORKSHOP ON TEXT 
MINING, pages 113_114. ACM, 2000. 
[30] Songbo Tan. An e_ective re_nement strategy for knn text 
classi_er. Expert Syst. Appl., 30(2):290_298, February 2006. 
[31] Suge Wang, Deyu Li, Xiaolei Song, Yingjie Wei, and Hongxia 
Li. A feature selection method based on improved _sher's 
discriminant ratio for text sentiment classi_cation. Expert 
Systems with Applications, 38(7):8696 _ 8702, 2011. 
[32] Wei Zhang and Feng Gao. An improvement to naive bayes for 
text classi_cation. Procedia Engineering, 15(0):2160 _ 2164, 
2011. {CEIS} 2011. 
 
 
 
 
 
