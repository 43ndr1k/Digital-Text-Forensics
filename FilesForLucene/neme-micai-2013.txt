Identification of the Minimal Set of Attributes
That Maximizes the Information towards the
Author of a Political Discourse: The Case of the
Candidates in the Mexican Presidential Elections
Antonio Neme1,2, Sergio Hernández3, and Vicente Carrión4
1 Complex Systems Group, Universidad Autónoma de la Ciudad de México
San Lorenzo 290, México, D.F. México
2 Institute for Molecular Medicine, Finland
neme@nolineal.org.mx
3 Postgraduation Program in Complex Systems, Universidad Autónoma de la Ciudad
de México
4 CINVESTAV IDS, México D.F.
Abstract. Authorship attribution has attracted the attention of the
natural language processing and machine learning communities in the
past few years. Here we are interested in finding a general measure of
the style followed in the texts from the three main candidates in the
Mexican presidential elections of 2012. We analyzed dozens of texts (dis-
courses) from the three authors. We applied tools from the time series
processing field and machine learning community in order to identify the
overall attributes that define the writing style of the three authors. Sev-
eral attributes and time series were extracted from each text. A novel
methodology, based in mutual information, was applied on those time se-
ries and attributes to explore the relevance of each attribute to linearly
separate the texts accordingly to their authorship. We show that less
than 20 variables are enough to identify, by means of a linear recognizer,
the authorship of a text from within one of the three considered authors.
Keywords: authorship attribution, mutual information, genetic algo-
rithms.
1 Introduction
Authorship attribution (AA) and stylistics have attracted the attention of differ-
ent practitioners from areas as diverse as computer science, philosophy, the arts,
mathematics, and engineering. AA refers to the task of identifying the author of
a text from a group of possible candidate authors [1], whereas stylistics refer to
the identification of attributes that may lead to unequivocally identify an author
[4]. Both tasks have witnessed an outstanding advance in recent years. However,
there are still a lot of open questions.
One of the open questions is the identification of the minimum set of attributes
that can lead to the identification of the author. Several attributes have been
I. Batyrshin and M. González Mendoza (Eds.): MICAI 2012, Part II, LNAI 7630, pp. 81–90, 2013.
c© Springer-Verlag Berlin Heidelberg 2013
82 A. Neme, S. Hernández, and V. Carrión
proposed, for example, the use of certain words and the lack of use of other [1].
In general, the concept of bag of words is frequently mentioned and, although
relevant results have emerged, there are even more questions to be answered
[2]. Writers use language following different ways to express their ideas. This
variation in language allows the authorship attribution possible [3].
Several algorithms are able to identify the author of a given text, but, however,
most of them lack of explanatory properties. For example, some kernel methods
present good performance, but the model is unable to show what attributes are
really relevant.
In this contribution, we present results regarding the identification of the min-
imal set of attributes that define a specific feature space. We are interested in
such a feature space in which the mutual information between the coordinates
of the points (texts) and the label (author of the text) is maximal. We present
results regarding three authors, that happen to be the main candidates for the
Mexican presidential elections of 2012. We selected the texts from several polit-
ical discourses of those authors for two main reasons. The first one is that the
subject in all texts for the three authors is very similar and is mainly in the
economic public services, and taxes themes. This allows an easier isolation of
the stylistics as it is not affected by a large variety of themes. The second reason
is that of there are several texts available from candidates. Finally, it is relevant
to know at least some aspect of stylistics from political leaders.
The rest of this contribution is presented as follows. In section 2 we describe
the attributes that will lead to definition of the stylistics of the authors. In
section 3 we present our proposal to identify the minimum set of attributes that
define a space such that the coordinates of texts in that space give the maximum
information about the author (class). In section 4 several results are described,
and in section 5 some conclusions are discussed.
2 Attributes and Stylistics
Several attributes have been proposed as marks in order to discern the stylistics
of an author [1]. Also, many features have been proposed to be relevant for the
AA task, as vocabulary size or the use of certain words or structures [2]. Here we
will focus our attention in attributes about the way authors make use of words.
We refer to words as the vocabulary but also to punctuation marks.
In this contribution, we consider two kinds of attributes that are extracted
from each text. The first one consists of probabilities of appearance of certain
words. The second group consists of the mutual information of several time series
constructed from texts. Fig. 1 show those attributes and an identification name.
Texts are represented as sequences of symbols, so they are transformed to
series of integers. From the vocabulary for each text, each word is assigned an
integer in order of appearance. The first word to appear in the text will be
assigned to 0, the second non-repeated word 1, and so on. For example, the
sentence S = In the city as well as in each neighborhood... is transformed to the
sequence T = {0, 1, 2, 3, 4, 3, 0, 5, 6, ...}. The word in is assigned to code 0 as it
The Minimal Set of Attributes for Authorship Attribution 83
Fig. 1. The included attributes. Some are scalars, some are probability distributions,
and others are mutual information functions (see next section).
is the first word. The second appearance of in is also assigned code 0. In this
contribution, there is no difference between upper and lower cases.
Time series from texts are relevant in the vision about stylistics we follow.
Several time series were constructed from each text, and they are generated
measuring the distance (counting the number of words) between consecutive ap-
pearances of certain words. For example, a certain time series that measures the
distance (number of words) between consecutive appearances of the comma may
reads as {3, 9, 40, 11}, which means that the number of words between the second
and first appearance is 3, the distance between the second and third appearances
is 9, and so on. Fig. 2 shows an example of the construction of the time series
for the comma. Time series for the following instances were constructed:
– the comma
– sentence length (number of words between them)
– number of sentences per paragraph
– the most common word excluding the comma and the word the
– the most common word excluding articles and prepositions
– the word the
However, time series per se only give some visual details, and more processing
on them is necessary.
Texts may present different lengths, so a normalizing scheme is needed in
order to compare time series. At the same time, time series are not analyzed
directly. In general several tools from the time series and signal processing fields
can be applied in order to extract subtle and non-evident patterns [5]. Several
attributes can be extracted from time series, such as the power spectrum, the
Lyapunov exponent, and many others [6]. In this contribution, we applied mutual
information function (MIF).
MIF is an information measure. Once we know the state of a system, How
much information does that give about the state of which a second system?
84 A. Neme, S. Hernández, and V. Carrión
Fig. 2. Time series construction. It is shown the case for distance (number of words)
between consecutive appearances of comma. Only the first 31 appearances are shown.
MIF is based in Shannon’s information theory. It is based on entropic-related
concepts. The entropy H of the training set is defined as:
H = −
#classes∑
i=1
pilog(pi) (1)
where the number of classes corresponds to the number of authors and is defined
as #classes, and pi is the probability of randomly chose an input vector whose
class is i. The mutual information between two random variables quantifies how
much information is gained about the possible state one of them once we know
the actual state of the other variable. It is a measure of correlation [8]. Mutual
information between two random variables X and Z is expressed as Φ(X ;Z),
where X is in this work one of the attributes of the input vectors and Z is the
class or label of those vectors. It is defined as:
Φ(X ;Z) =
ns∑
i
#statesinZ∑
j
P (i, j)log
P (i, j)
P (i)P (j)
(2)
The number of states in Z is the number of classes, and ns is the number of states
in X . If X is a continuous variable, then it can be discretized into ns different
states. P (i) is the marginal probability that a randomly chosen text belongs to
a certain state, P (j) is the marginal probability that the text belongs to class
(author) j, and P (i, j) is the joint probability that the text is in state i and
belongs to author j. In general, for artificial datasets with no noise, all entropy
in the label (class) can be removed from the list of attributes X̄ that define
the high-dimensional feature space. That is, Φ(X̄ ;Z) = H . Mutual information
between the compound system of all attributes or variables and Z (Φ(X̄ ;Z))
tends to dissipate all entropy in the label. That is, when ns → ∞, Φ(X̄ ;Z) → H .
The Minimal Set of Attributes for Authorship Attribution 85
When the correlation between two systems (or random variables) is the mutual
information, we are considering high-order momentum able to capture non-linear
correlations in data [10,8].
When MIF is applied to a time series, the second system (or random variable)
is constructed as a shift applied to the time series. The length of that shift is
shown in the x axis. The graph of MIF then responds the question of how much
information is achieved once we know the state of a system (the time series)
with respect to the next state it will present (k = 1), two steps ahead (k = 2),
and so on. Fig. 3
We call the whole set of attributes T
Fig. 3.Mutual information function (MIF) of the distance between commas time series.
One time series for each one of the considered authors is shown.
3 The Proposed Model
Each text is transformed to a point in a high-dimensional space. The coordi-
nates of each text are determined by the attributes described in the previous
section. Points in this space may be the input data to a classification machine
like multilayer perceptrons, identification trees or support vector machines so
that a mapping between the coordinates and the label (the author) is found.
If the number of attributes, that is, the dimension of the feature space, is high
then the procedure followed to find such mapping, known as training process,
may be very time consuming. In other cases, as in SVM, the new generated
very-high-dimensional space lack explanatory power, that is, it may not be clear
what attributes are really relevant in the classification task. On the other hand,
classifiers based in trees such as C4.5 [12] offer an explicit explanation about the
classification task. C4.5 and related methods, although relevant and useful in
86 A. Neme, S. Hernández, and V. Carrión
many situations, suffer from a major drawback: their greedy strategy may lead
them to local optimum.
We are interested in finding a subset A ∈ T such that |A| ≤ K such that
the mutual information from A to the classes (author’s name) is maximal. Let
Φ(X,Y ) be the mutual information between systems X and Y , and Note that
this task is not equivalent to that performed by C4.5 related algorithms. We are
not interested in classification by means of mutual information. We are trying
to find a subspace such that coordinates in that space give as much information
about the label or class as possible, and any machine learning algorithm can be
fed with vectors in that space A, instead of being fed with vectors from space
T whose dimensionality is higher. In that sense, our task is slightly similar to
that of testors [13], in which a matrix of differences is systematically explored
to identify those features that correctly classify patterns.
Once again, we intend to find an attribute or feature space such that the
mutual information between points representing texts in that space and authors
is as maximum as possible. In order to do this, the mutual information of a
compound system is needed. That is, if there is only one attribute then the MI
(mutual information) is calculated straightforward. In the case of two continuous
attributes X
′
and Y
′
and ns is the number of states in which each attribute is
to be discretized (X and Y ), a compound system Z is constructed as follows.
Z ′i = Xi × ns + Yi and Z = discretize(Z ′, ns). For more than two attributes,
the procedure is applied recursively.
4 Results
The analyzed texts are shown in fig 4, along with the date they were dictated
as a public discourse and a short title. All texts were preprocessed to remove
transcribed messages from the public and other irrelevant information.
Fig. 5 shows the vocabulary as a function of number of words for the analyzed
texts (see fig. 4). It is observed that, although one of the authors present a
significant lower vocabulary size, it is still an attribute unable to give a lot of
information about the author.
Fig. 6 shows the mutual information between some individual attributes in T
and the class (author of the text). The dimension of space T , that is the number
of attributes, is D ∼ 500. That would require a lot of effot to train a multilayer
perceptron. For a SVM, that may be an easy task, but we are interested in the
identification of attributes that give information about the class. The function
that maps from that space to the label space is not explored in this contribution.
SVM does not offer such explanation.
The náive scheme to construct the space A from T will be to select the K
most informative variables. Such strategy is followed, for example, by C4.5 and
related algorithms, but that greedy strategy leads to local optima. In fig. 6
can be see an example of the failure of such strategy. If attributes V and T
were rejected and H and ANPL (see fig. 1) were selected, the opportunity that
the compound system V, T to be selected would not be explored, and, as can
The Minimal Set of Attributes for Authorship Attribution 87
Fig. 4. The texts considered in this contribution. The authors are the three main
candidates for Mexican presidential elections in 2012. 41 texts were selected for author
EPN, 25 for author AMLO and 24 for author JVM. The selection criteria was that
text should be within a certain range of number of words.
Fig. 5. Vocabulary size as a function of text length (number of words)
88 A. Neme, S. Hernández, and V. Carrión
Fig. 6. Mutual information between some of the attributes listed in fig. 1 and the text
author (class)
be seen, a compound system (in this case formed as the ratio) was in fact a
better option. Thus, we want to explore space T instead of disregarding some
of the variables from the beginning. For probability distributions and mutual
information functions, the number in the parenthesis represent the component.
For example PDSL(4) refers to the probability of sentences of 4 words.
The space generated by K attributes from space T is called A. The number
of possible spaces A is the number of permutations of K positions available to D
different attributes C(D,K). The exhaustive search for the case here presented
is prohibitively time consuming for K > 3. Thus, a search scheme is needed.
We applied an heuristic search method, the genetic algorithm, in order to find
at most K attributes from T that generate a space such that Φ(A;Class) is
maximum.
We implemented a genetic algorithm in Python, with elitism and probabilities
of mutation of 0.05 and crossover of 0.9. Population size was settled to 100 and
the algorithm was allowed to run for 500 epochs.
Note that the algorithm identifies a space A of dimension D ≤ K. That space
is not easily observed once D > 3. In order to visualize the distribution of the
analyzed texts in that space, a mapping algorithm is needed. We decanted our
options towards the self-organizing map (SOM) as it is a powerful visualization
tool. SOM is able to present in a low-dimensional space an approximate distri-
bution that resembles the actual distribution of vectors in the high-dimensional
input space [14]. It outperforms common mapping tools such as principal com-
ponent analysis as SOM is able to account for high-order statistics, instead of at
most second-order (variance) [15]. In fig. 7 several maps obtained by SOM are
presented. It can be observed that, indeed, there are detectable general distribu-
tion patterns that may allow to discriminate the author. Texts do not necessarily
form clusters: once again, we are interested in an attribute space such that mu-
tual information between the distribution and the author of a text is maximized.
Clusters are only one way in which that mutual information can be maximized,
but there are many others. Our methodology finds a family of those distributions.
The Minimal Set of Attributes for Authorship Attribution 89
Fig. 7. SOM (low-dimensional approximations) of feature spaces A
In fig. 7, besides the self-organizing map, it is shown the space A. Now, a
machine with universal approximation capabilities such as the multilayer per-
ceptron can be applied to space A, instead of being fed by data in space T . The
fact that data can be explored in order to identify the combination (subspaces)
of attributes that offer the maximum mutual information decreases the training
time. Also, it points to the most relevant joint variables, which are in A. It may
be possible to add new variables to A as the mutual information will not decrease
[11]. However, eliminating attributes from A may have dramatic consequences,
as the removed variable may be of great relevance.
90 A. Neme, S. Hernández, and V. Carrión
5 Conclusions
In the tasks of authorship attribution and computational stylistics, it is of major
interest to identify a set of attributes that can offer as much information as
possible about the author of the text. Here, we have systematically explored
schemes for detecting a subset of a large number of variables that can maximize
information about the author. A genetic algorithm that constructs a space of
at most K attributes such that it maximized the information about the class or
author of the text was implemented.
The methodology here described can be applied to any kinds of texts. Here,
we reported results for a special case, regarding political discourses, but in our
project (not enunciated for double blind review purposes) we are applying these
and other methodologies in order to study computational stylistics and style
evolution.
References
1. Juola, P.: Authorship attribution. NOW Press (2008)
2. Stamatatos, E.: A survey of modern authorship attribution methods. J. of the
American Soc. for Information Science and Technology 60(3), 538–556 (2010)
3. Neme, A., Cervera, A., Lugo, T.: Authorship attribution as a case of anomaly
detection: A neural network model. Int. J of Hybrid Intell. Syst. 8, 225–235 (2011)
4. Manning, C., Schutze, H.: Foundations of statistical natural language processig.
MIT Press (2003)
5. Abarbanel, H.: Analysis of observed chaotic data. Springer (1996)
6. Kantz, H., Schreiber, T.: Nonlinear time series analysis, 2nd edn. Cambridge Press
7. Shannon, C.E.: A Mathematical Theory of Communication. Bell System Technical
Journal 27, 379–423, 623–656 (1948)
8. Cellucci, C.J., Albano, A.M., College, B., Rapp, P.E.: Statistical Validation
of Mutual Information Calculations. Phy. Rev E. 71(6) (2005), 10.1103/Phys-
RevE.71.066208
9. Santos, J., Marques de Sá, J., Alexandre, L., Sereno, F.: Optimization of the error
entropy minimization algorithm for neural network classification. In: ANNIE V.
14 of Intelligent Engineering Systems Through Art. Neural Net, pp. 81–86. ASME
Press, USA (2004)
10. Silva, L., Marques de Sá, J., Alexandre, L.: Neural Network Classification using
Shannon’s Entropy. In: ESANN 13th European Symp. on Art. Neural Net (2005)
11. Cover, T., Thomas, J.: Elements of information theory, 2nd edn. Wiley (2006)
12. Quinlan, R.: Programs for Machine Learning. Morgan Kaufmann Publishers (1993)
13. Cortes, M.L., Ruiz-Shulcloper, J., Alba-Cabrera, E.: An overview of the evolution
of the concept of testor. Pattern Recognition 34, 753–762 (2001)
14. Kohonen, T.: Self-organizing maps, 2nd edn. Springer (2000)
15. The Self-Organizing Maps: Background, Theories, Extensions and Applications.
Studies in Computational Intelligence (SCI), vol. 115, pp. 715–762 (2008)
