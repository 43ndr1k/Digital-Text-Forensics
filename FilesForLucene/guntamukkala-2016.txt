A Novel Automated Approach to Evaluate the Transparency of Privacy Policies
by
Niharika Guntamukkala
A Thesis
Presented to
The University of Guelph
In partial fulfilment of requirements
for the degree of
Master of Science
in
Computer Science
Guelph, Ontario, Canada
c© Niharika Guntamukkala, May, 2016
ABSTRACT
A Novel Automated Approach to Evaluate the Transparency of Privacy Policies
Niharika Guntamukkala Advisor:
University of Guelph, May, 2016 Dr. Gary Grewal
Dr. Rozita Dara
Research findings suggest that online privacy policies are often long, hard to
understand and contain insufficient information. Therefore it is suggested that privacy
policies should be transparent to help users make informed decisions about their
personal information and avoid the risk of information misuse. In order to help users
gauge the level of transparency, it would be beneficial to have an automated system
that evaluates the transparency of a given privacy policy and provides useful feedback.
In this thesis, we propose an automated transparency evaluation system to analyze
the privacy policies based on their content and style. This tool gives a transparency
score by analyzing the content factors completeness and unambiguity using machine-
learning techniques and style factor readability using existing readability formulas,
to provide feedback to the users and warn them about the risks associated with their
information privacy. The system is able to achieve F-measure scores greater than
70% in classifying a privacy policy as ‘most transparent’, ‘moderately transparent’ or
‘least transparent’.
Acknowledgments
First, I would like to thank my advisors, Dr. R. Dara, Dr. G. Grewal for their
assistance throughout my research. Without their knowledge and guidance, I would
not have been able to learn as much as I have through my studies. I am also thankful
to Dr. F. Song, for his suggestions towards my thesis and Dr. B. Nonnecke, for
reviewing my thesis and providing his valuable feedback. I would like to thank Dr.
R. Akalu for providing his insightful feedback and engaging his students for taking
the surveys. I would also like to thank Kushal Jaisingh and Tosan Atele-Williams for
their time and inputs towards the surveys. I would like to thank Dr. E. Costante for
providing their dataset. Special thanks to Curtis for annotating the datasets. Also,
I would like to thank my parents and my brother for their support throughout my
studies, your love and encouragement means the world to me.
iii
Table of Contents
List of Tables vi
List of Figures vii
1 Introduction 1
1.1 Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2
1.2 Thesis Statement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
1.3 Objectives and Contributions . . . . . . . . . . . . . . . . . . . . . . 4
1.4 Research Approach . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
1.5 Thesis Organization . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
2 Literature Review 8
2.1 Transparency . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
2.1.1 Challenges of Analyzing Privacy Policies . . . . . . . . . . . . 9
2.2 Transparency Enhancing Technologies . . . . . . . . . . . . . . . . . . 9
2.3 Usable Privacy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
2.4 Existing Privacy Policy Analysis Tools . . . . . . . . . . . . . . . . . 12
2.5 Privacy Protection Acts and Principles . . . . . . . . . . . . . . . . . 14
2.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
3 Background Techniques 18
3.1 Transparency Evaluation Factors . . . . . . . . . . . . . . . . . . . . 18
3.2 Knowledge Discovery Process . . . . . . . . . . . . . . . . . . . . . . 19
3.3 Text Mining . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
3.4 Text Preprocessing . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
3.5 Feature Space Selection . . . . . . . . . . . . . . . . . . . . . . . . . . 22
3.6 Learning Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
3.7 Performance Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . 31
3.8 Style Criteria . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
3.8.1 Readability Formulas . . . . . . . . . . . . . . . . . . . . . . . 33
3.9 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34
4 Proposed Approach 36
4.1 Problem Statement . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36
4.2 Proposed Approach . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36
4.2.1 Content Criteria . . . . . . . . . . . . . . . . . . . . . . . . . 39
4.2.2 Style Criteria . . . . . . . . . . . . . . . . . . . . . . . . . . . 40
iv
4.3 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41
5 Experimental Setup 42
5.1 Text Mining of Privacy Policies . . . . . . . . . . . . . . . . . . . . . 42
5.2 Evaluation Criteria . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44
5.3 Privacy Policy Corpus . . . . . . . . . . . . . . . . . . . . . . . . . . 55
5.4 Data Preprocessing . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56
5.5 Feature Space Selection . . . . . . . . . . . . . . . . . . . . . . . . . . 57
5.6 Classifier Parameter Tuning . . . . . . . . . . . . . . . . . . . . . . . 63
5.7 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64
6 Results and Analysis 66
6.1 Style Factor: Readability . . . . . . . . . . . . . . . . . . . . . . . . . 66
6.2 Content Factor: Completeness . . . . . . . . . . . . . . . . . . . . . . 67
6.2.1 Unfiltered Approach . . . . . . . . . . . . . . . . . . . . . . . 67
6.2.2 Keyword-based Filtering Approach . . . . . . . . . . . . . . . 68
6.3 Unambiguity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71
6.3.1 Unfiltered Approach . . . . . . . . . . . . . . . . . . . . . . . 71
6.3.2 Co-occurrence-based Filtering Approach . . . . . . . . . . . . 72
6.4 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73
6.5 Transparency Scoring System . . . . . . . . . . . . . . . . . . . . . . 78
7 Conclusions 82
7.1 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82
7.2 Limitations and Future Work . . . . . . . . . . . . . . . . . . . . . . 84
Bibliography 87
A Privacy Policy Acts and Dataset 92
A.1 Privacy Protection Acts and Principles . . . . . . . . . . . . . . . . . 92
A.2 Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100
B Feature Space 103
B.1 Completeness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103
B.2 Unambiguity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103
C Surveys 107
C.1 Survey 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107
C.2 Survey 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111
D Classifier Performance for Completeness and Unambiguity 115
v
List of Tables
3.1 Example of Word Co-occurrence Frequency Matrix . . . . . . . . . . 25
3.2 Confusion Matrix/ Contingency Table . . . . . . . . . . . . . . . . . 31
3.3 Scoring Scheme Based on Grade Level for FGL . . . . . . . . . . . . 35
4.1 Criteria of a Transparent Privacy Policy . . . . . . . . . . . . . . . . 38
5.1 Goal-Based Evaluation Criteria for Completeness . . . . . . . . . . . 46
5.1 Goal-Based Evaluation Criteria for Completeness . . . . . . . . . . . 47
5.1 Goal-Based Evaluation Criteria for Completeness . . . . . . . . . . . 48
5.1 Goal-Based Evaluation Criteria for Completeness . . . . . . . . . . . 49
5.2 Keywords Identified for each Section . . . . . . . . . . . . . . . . . . 54
5.3 An example of word co-occurrence matrix showing a small subset words
from the privacy policies . . . . . . . . . . . . . . . . . . . . . . . . . 60
6.1 Readability Results using FRES readability formula . . . . . . . . . . 66
6.2 Readability Results representing grade levels using FGL readability
formula . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67
6.3 Completeness: Average F-measures and Standard Deviations for Lin-
ear SVM and Random Forest Classifiers for 10 Iterations . . . . . . . 77
6.4 Unambiguity: Average F-measures and Standard Deviations for Ran-
dom Forest and Logistic Regression Classifiers for 10 Iterations . . . . 77
6.5 Section Ranking by Technical participants and Subject Matter Experts 79
6.6 Section Weightage . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80
6.7 Performance Results of Transparency Evaluation System . . . . . . . 81
A.1 Privacy Protections Acts and Principles- OECD, FTC FIPP and PIPEDA 93
A.2 Keywords Identified for each Section from Privacy Protection Acts and
Principles . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101
A.3 List of Online Privacy Policy Websites . . . . . . . . . . . . . . . . . 102
B.1 Top 50 unigrams based on word frequency . . . . . . . . . . . . . . . 105
B.2 Top 50 word co-occurance frequencies . . . . . . . . . . . . . . . . . . 106
D.1 Completeness: Average F-measures and Standard Deviations for LR,
KNN and MNB for 10 Iterations . . . . . . . . . . . . . . . . . . . . . 116
D.2 Unambiguity: Average F-measures and Standard Deviations for LSVM,
KNN and MNB for 10 Iterations . . . . . . . . . . . . . . . . . . . . . 117
vi
List of Figures
1.1 Transparency Evaluation Framework . . . . . . . . . . . . . . . . . . 7
3.1 SVM classifier, circles and squares representing different classes . . . 27
3.2 Random Forest Ensemble Classifier . . . . . . . . . . . . . . . . . . . 28
3.3 An Example of K-Nearest Neighbour Classification . . . . . . . . . . 29
3.4 An Example of 5-Fold Cross Validation . . . . . . . . . . . . . . . . . 32
5.1 Knowledge Discovery Process Workflow . . . . . . . . . . . . . . . . . 44
5.2 A sample response by a subject matter expert for the Marks & Spencer
privacy policy survey . . . . . . . . . . . . . . . . . . . . . . . . . . . 50
5.3 Sample ambiguous sentences chosen by a technical participants for
questions related to unambiguity . . . . . . . . . . . . . . . . . . . . 51
5.4 LDA on Privacy Policy Text . . . . . . . . . . . . . . . . . . . . . . . 53
6.1 Completeness: Average F1-Scores using Baseline Features - Unfiltered 69
6.2 Completeness: Average F1-Scores using Chi-Squared Features - Unfil-
tered . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70
6.3 Completeness: Average F1-Scores using Baseline Features - Filtered . 70
6.4 Completeness: Average F1-Scores using Chi-Squared Features - Filtered 71
6.5 Unambiguity: Average F1-Scores using Baseline Features - Unfiltered 73
6.6 Unambiguity: Average F1-Scores using Chi-Squared Features - Unfiltered 74
6.7 Unambiguity: Average F1-Scores using Baseline Features - Filtered . 74
6.8 Unambiguity: Average F1-Scores using Chi-Squared Features - Filtered 75
B.1 Word Cloud Representing Frequency of Unigrams . . . . . . . . . . . 104
vii
Chapter 1
Introduction
The main objective of this research is to develop an automated approach for
evaluating the transparency of online privacy policies in order to help Internet users
make more informed decisions about their information privacy. A privacy policy is
considered transparent if it provides complete information in a clear and understand-
able manner without any ambiguity [48]. The proposed approach analyzes three main
contributing factors to transparency: completeness, unambiguity and readability. A
score is provided for each of these criteria. Completeness of a privacy policy allows
the user to understand the extent of relevant information present within the privacy
policy. Unambiguity of a privacy policy warns the user to what extent the privacy
policy is ambiguous. Readability of a privacy policy helps the user to understand
whether or not the given privacy policy is easy to comprehend. The readability of a
privacy policy is evaluated using readability formulas that look at factors such as the
length of a sentence, word count, syllable count, and character count in order to pro-
vide an ease of readability score, along with the grade level required to comprehend
the text.
Completeness and unambiguity are the criteria used to assess the content of a pri-
vacy policy using machine learning and text mining approaches. A final transparency
score of a privacy policy is calculated by the proposed approach by combining the
individual scores. This chapter introduces our research by giving a brief overview of
the importance of privacy and the existing issues with online privacy policies. Section
1.1 describes the motivation for this research. Subsequently, Section 1.2 describes the
thesis statement. Furthermore, Section 1.3 presents the main objectives and contri-
butions of our work. A brief overview of the proposed approach and experimental
1
methodology is described in Section 1.4. Finally, we close this chapter by providing
information about the organization of this thesis in Section 1.5.
1.1 Motivation
Information privacy is the right of a user to determine when, how and to what
extent their information is shared with others. With advancements in data processing
technologies, as well as the increasing availability of personal information and privacy
breaches, it is becoming even more important to safeguard personally identifiable
information [31]. As a solution to this problem and to give users control over their
data, the Federal Trade Commission (FTC) [2] has proposed that organizations
provide privacy policies to inform their consumers about their privacy practices and
help them protect their personal information.
In a real world usage scenario, most people are not aware that while browsing
on the Internet they leave an electronic record of their movements, which uninten-
tionally reveals a lot of personal information to people and organizations that track
such data. Usually, organizations (such as e-commerce) are interested in acquiring in-
formation about consumers’ online surfing habits, preferences and purchases. Online
behavioural advertising collects personal information identifying users individually
and revealing their purchase patterns. Some organizations also sell or trade the col-
lected information to third parties as a tradable commodity. Organizations frequently
use devices such as cookies and spyware to track the online behaviour of customers.
These devices collect information such as IP addresses, websites visited, purchase
history, and emails [31]. In addition, a cookie is an identifier created by a website
and stored in a visitor’s browser so that when a visitor returns to a previously visited
website, the cookie can be used by the website to identify a returning visitor and
present the visitor with customized information. Though cookies can improve the
overall experience of navigating the Web, they are also a way for organizations to
track the movements of Internet users and contribute to the building of personal pro-
files of users’ behaviour, habits and buying patterns [31]. Spyware refers to software
2
designed to intercept or take partial control of a computer’s operation without the
consent of the owner. The main concern with these types of online surveillance is
that they usually happen without the users’ knowledge or consent [31]. Therefore, it
is very important for the organization to inform the users about their data processing
practices transparently in their privacy statements.
Online privacy policies are meant to inform the users (e.g., consumers or data
subjects) about how their information, especially personal information, is being col-
lected and used, as well as serving as a basis for decision making. However, research
suggests that privacy policies are often difficult to understand, contain incomplete in-
formation and are even misinterpreted [7, 59]. Further to this, they contain legalistic
words and phrases that may be difficult for a normal user to understand. Pollach et
al. suggest that organizations purposely use legalistic jargon and phrases to divert
the attention and confuse the users [49]. As the privacy policies are long and hard
to comprehend, users often do not pay attention while reading them. One study [45]
made an attempt to calculate the cost of reading the privacy policies. It concluded
that the estimated cost of reading the privacy policies is approximately 200 hours
per year, which is worth about $3,534 annually per American Internet user. In addi-
tion, privacy policies contain insufficient information to help the users protect their
personal information [7]. Hence, users often cannot find relevant information in the
privacy policies to address their concerns. As a result, end users facing privacy-related
decisions are unaware of how their data is collected and used [7].
In addition, the lack of a standardized privacy policy makes it even more diffi-
cult for users to understand privacy policies, due to the lack of awareness of what
information should be present in these policies. Therefore, it is recommended by the
privacy protection acts and principles that organizations be transparent about their
data-handling practices by enforcing privacy policies that are transparent [2, 5].
Transparent privacy policies are proposed as a solution to the above problems
[4]. A transparent privacy policy provides information to users on what data are being
collected, and how the data are being used [52]. This, in turn, helps organizations
to build trust with their customers. Without transparency, users cannot provide the
3
meaningful consent necessary for a businesses to collect their personal information
[6]. Therefore, transparency should be the key characteristic of a privacy policy when
organizations collect sensitive data. Organizations should be open about the disclo-
sure, collection, storage and usage of personal data, as it makes them accountable
and also boosts users’ trust [6, 52].
1.2 Thesis Statement
Transparency of privacy policies can be evaluated using an automated tool that
measures content factors such as completeness and unambiguity of policies and style
factors such as readability. In general, privacy policies should be transparent and
contain important information about the data-handling practice of the organizations
to inform the Internet users about the potentials risks associated with their personal
information. However, research findings show that online users often do not read the
privacy policies as they are long and hard to understand [7]. Therefore, an automated
tool that analyzes an existing online privacy policy that provides a transparency score
based on the information provided in the privacy policy would be beneficial to the
users, guiding them in the right direction, and helping them make informed decisions
about their personal information privacy.
In this thesis, we propose an automated transparency evaluation system to an-
alyze privacy policies based on their content and style. This tool will give a trans-
parency score by analyzing the content factors completeness and unambiguity and
the style factor readability to provide useful feedback to the users and warn them
about potential risks that may be associated with their personal information.
1.3 Objectives and Contributions
Research suggests that a large number of companies disclose insufficient infor-
mation about their data collection, sharing, and security practices [49, 59]. This
clearly shows that privacy policies do not provide information about the data-handling
4
practices in a satisfactory manner despite the fact that it is important for privacy poli-
cies to be complete and understandable. Therefore, a tool that evaluates the trans-
parency of the privacy policies would help the users in protecting their individual
privacy.
We propose to analyze privacy policies based on two factors - content and style
as discussed in [20, 53]. Completeness and unambiguity criteria are identified for
analyzing the content of a privacy policy, whereas, the readability criteria is identified
to analyze the style of a privacy policy. The style criterion, readability, is evaluated
using existing readability formulas. The evaluation of content criteria completeness
and unambiguity is a knowledge-extraction problem that requires access to high-
quality data, criteria to objectively evaluate the policies, and algorithms for data
preprocessing and information extraction. The goal of this research is to develop an
automated transparency evaluation approach considering three contributing criteria
of transparency: completeness, unambiguity and readability. These three criteria are
selected based on the various definitions of transparency as discussed in Chapter 4.
The main contributions of this thesis is three-fold:
• We propose a detailed evaluation criteria to measure the degree to which a
privacy policy is complete or unambiguous.
• Due to the lack of a benchmark annotated corpus, we create an annotated
corpus containing 100 privacy policies by utilizing the evaluation criteria. This
dataset will be made available to other researchers for their research.
• We develop an automated approach to objectively evaluate the online privacy
policies by considering their content and style. More specifically, we evalu-
ated the completeness and unambiguity of the content using machine learning
techniques, and the style criterion readability using readability formulas.
5
1.4 Research Approach
Several steps are involved in proposing an automated transparency evaluation
approach. To the best of our knowledge, no criteria exists for measuring the trans-
parency of a privacy policy. Therefore, we need to identify several criteria to measure
the transparency of a privacy policy. First, we identify several contributing factors to
a transparent privacy policy from the definitions of transparency. Then, we aim to
reduce the problem and focus on two important criteria, content and style, we select
completeness and unambiguity factors for content criteria and readability factors for
the style criteria. Measuring the readability of privacy policy is achieved using exist-
ing readability formulas. Completeness and unambiguity are evaluated using machine
learning approach. As this is one of the early attempts to examine the transparency
of privacy policies in the context of content completeness and unambiguity we uti-
lized the knowledge discovery process. First, we create a dataset containing online
privacy policies in the format of raw text suitable for the research and propose an
evaluation criteria to measure each of the factors contributing towards privacy policy
transparency. Using the evaluation criteria and the feedback we collect from a survey
of 10 users, we annotate the privacy policies and perform the necessary preprocess-
ing steps to make the data ready for the task of text classification. Furthermore,
we chose a selection of several classifiers suitable for the text classification problem
and analyze their behaviour and performance with different feature sets. These clas-
sifiers include Support Vector Machines (SVM), Random Forest (RF), Multinomial
Naive Bayes (MNB), K-Nearest Neighbours (KNN), Logistic Regression (LR). Conse-
quently, based on the results from various experiments with different sets of features,
we compare the prediction performance of the classification systems. Finally, we rec-
ognize the best performances and combine the resulting values from completeness,
unambiguity and readability measures to give the final transparency score for a pri-
vacy policy. Figure 1.1 shows the workflow of different steps involved in the scoring
process of privacy policies. Finally, we compare the transparency scores against the
manual annotations and user rating to assess the accuracy of the system.
6
Figure 1.1: Transparency Evaluation Framework
1.5 Thesis Organization
This thesis is organized into seven chapters. Chapter 2 discusses the previous
work in the literature in the area of privacy policy analysis, transparency and usable
privacy. Chapter 3 provides the background information in the area of machine learn-
ing, and text mining. It also gives and in-depth look into the classification approaches
used in this thesis. In Chapter 4, the problem statement, the proposed approach and
methodology are described in-depth. Chapter 5 presents the experimental setup in-
cluding the dataset, evaluation criteria, annotations, feature space, and classification
approaches. Subsequently, Chapter 6 presents the experiment results and important
findings. This chapter also compares the performance of different classifiers using dis-
tinct feature sets and and provides a detailed analysis of the results. Finally, Chapter
7 highlights the most important achievements of the research along with describing
the ideas that can be pursued in the future work.
7
Chapter 2
Literature Review
This literature review is organized as follows. Sections 2.1 provides a brief de-
scription of transparency and the challenges of analyzing privacy policies. Section
2.2 is a brief overview of various transparency enhancing technologies. Section 2.3
describes the projects and tools related to usable privacy. Furthermore, Section 2.4
covers some of the existing approaches and tools for analyzing privacy policies. Subse-
quently, Section 2.5 presents a brief overview of the privacy protection acts and prin-
ciples that are considered while developing the proposed privacy policy transparency
evaluation approach. Finally, Section 2.6 summarizes the literature and explains how
the work in this thesis differs from the existing approaches and tools proposed in the
literature.
2.1 Transparency
Today, there are more than 850 social media sites and more than 2 billion people
have access to the Internet; this makes data and information more accessible than
ever [7]. The distributed nature of the world wide web and services such as e-
health, online shopping, and many others make it difficult for users to keep track of
where their personal information is stored, who is handling their information, and
the purposes for which it is used. This raises concerns over personal information
privacy due to data leakage and mishandling of data because of different possible
attacks that are being invented. Moreover, the main concern for people is the lack of
transparency in the way government or organizations deal with individuals’ privacy
[7]. This in turn causes a lack of trust between the individuals and organizations as
they are not completely aware of how their personal information is used. Therefore,
8
transparency plays a key role in encouraging individuals to make informed decisions
about their personal information privacy, which in turn has the potential to restore
the trust between individuals and organizations [4]. In the context of privacy policies,
transparency means that information must be provided in a clear and understandable
manner without any ambiguity. It should contain accurate and complete information
that is easily accessible and maintains accountability. In short, there is a growing
need for privacy policies to be transparent and cover all the important information
so that the end users will be able to make more informed decisions regarding their
privacy.
2.1.1 Challenges of Analyzing Privacy Policies
There are many challenges associated with analyzing privacy policies that are
discussed below.
• There are a very limited number of studies focusing on analyzing the privacy
policies. Most of these are in the area of usable privacy and propose tools to
improve the usability of privacy policies. To the best of our knowledge, there is
no current study that focuses on analyzing different aspects of transparency in
a detailed manner.
• A framework that evaluates the transparency of privacy policies does not exist,
which makes it harder to make any comparisons.
• Benchmark annotated data is not available. Therefore, different contributing
criteria for transparency and a detailed evaluation scheme to analyze these
criteria need to be proposed. As a result, different stages of the knowledge
discovery process need to be analyzed.
2.2 Transparency Enhancing Technologies
In information privacy, transparency is a legal privacy principle, which was orig-
inally derived from the European Union Data Protection Directive 95/46/ EC [1].
9
There are several transparency tools that were developed and are still in progress that
differ in a number of important characteristics such as control and verification, target
audience (data subjects and auditors/proxies), scope, trust requirements, informa-
tion presented, technologies used and security requirement. One of the main goals of
any transparency enhancing tool is to increase a person’s knowledge of what data is
being collected, how it is being used, and with whom it is being shared [33]. It is
crucial to decide whether the data are processed in a legal manner and whether they
are accurate. Some of the transparency enhancing tools are Transparent Accountable
Datamining Initiative (TAMI), Privacy Bird, Privacy and Identity Management for
Europe (PRIME), Privacy evidence, and an amazon book recommendation system
[30, 33]. It is observed that the majority of tools promote privacy awareness. Most of
the tools also attempt to provide a better understanding of privacy, or provide insight
into third party data sharing and data collection. Some of the tools also attempt to
increase the usability of privacy policies by proposing better visualization solutions to
help users find information easily. However, none of the available tools explain or give
insight into which data is processed and how it is being processed. Another drawback
of these tools is in the area of control and verification. There are very few tools that
are interactive i.e., have the ability to let the user control their information. This is
mainly due to the fact that the companies see the information as an asset and do not
want to give control to the user, and thereby lose the information. Moreover, none of
the existing transparency enhancing technologies focus on analyzing the transparency
of privacy policies in general. Consequently, the goal of this research is to develop
a transparency evaluation system to help users make informed decisions about their
privacy [30, 33].
2.3 Usable Privacy
In order to notify users of critical privacy practices it is not enough to analyze
policy content, but rather the results must also be presented in a format that is easily
understandable to the user. Therefore, a clear and concise, user-friendly privacy
10
policy layout is always desirable. In particular, privacy labels or icons may help to
succinctly display privacy practices [33]. Therefore, research has been conducted in
the area of usable privacy and to increase the usability of privacy policies. Layered
Privacy Policies is a policy display format first introduced by the law firm Hunton
& Williams [38, 39]. This format involves a summarized version of a privacy policy
created using a step-by-step process. This summary has standardized headings for
the policy information, but the information itself is provided in free-form natural
language text. However, a recent research study suggests that layered policies are
not good at helping consumers understand privacy compared to full natural language
policies [38].
Inspired by food nutrition labels, Cranor and collaborators [38] have also de-
veloped and experimented with privacy nutrition labels intended to simplify the pre-
sentation of privacy policies by displaying those elements most important to users in
a standardized format that is both succinct and easy to interpret. User evaluations
suggested that the privacy nutrition label allows consumers to find information more
quickly and accurately compared to relying directly on plain natural language privacy
policies. Privacy nutrition labels are shorter and easier to read and interpret than
natural language policies. Their standardized tabular format allows users to learn
where to look for answers to particular questions and facilitates comparison between
policies. Also, privacy icons are proposed by PrimeLife [33], KnowPrivacy [33],
and the Privacy Icons project [33] in order to aid in providing visual clues to users.
However, care needs to be taken that the meaning of the icons is clear to the users
[33, 38, 39]. All these studies focus on improving the usability of privacy policies
and presenting the privacy policy information in a clear and concise manner. Our
research differs from the above work in that it focuses on analyzing the content of
privacy policies to increase transparency rather than usability.
11
2.4 Existing Privacy Policy Analysis Tools
Privacy policy evaluation is a relatively new research area that has started to
gain prominence in recent years. Some existing studies have focused on examining
the drawbacks of privacy policies, and have raised concerns over their length, clarity
and readability [11, 34, 43, 49]. The work in [11] evaluated the readability of privacy
policies using readability formulas and found that users need a collage education to
comprehend the complex sentences in the privacy policy text. Another study [43]
used five different readability metrics: Flesch Readability Ease (FRE), Flesch Grade
Level (FGL), Gunning Fog Index, Simple Measure of Gobbledygook (SMOG), and
Automated Readability Index (ARI) to measure the readability of Google Top 1000
and Fortune 500 policy documents and the results suggested that all the privacy
policies are difficult to read. To help the requirements engineers, the above study
also proposed an automated text mining solution to derive the requirements hidden
inside the privacy policy statements using LDA (topic modelling) and Goal mining.
These techniques are used in this research in order to develop a goal-based evaluation
criteria and to derive keywords (LDA) for text classification for the evaluation of
completeness of the privacy policies.
Attempts have also been made to develop tools that can assess the content of
privacy policies [17, 54]. In particular, completeness analyzer is a tool developed to
analyze the content of an arbitrary privacy policy [17]. In this approach, machine
learning techniques were used to check for the completeness of a given privacy policy
based on the presence of a set of sections within the policy. Specifically, the privacy
policies were parsed and annotated one paragraph at a time. Each paragraph in
the privacy policy was annotated with the section label it belonged to and several
classifiers like Naive Bayes, k-NN, SVM and LSVM were tested [17]. This tool
produced a cumulative score for privacy policy completeness based on weights given
to each category. One of the drawbacks to this study is that it utilized a small
dataset, containing only 40 privacy policies. Moreover, parsing privacy policies one
paragraph at a time is not only time consuming, but prone to errors. Unfortunately,
12
as the annotations are done one paragraph at a time, it was not clear how similar text
belonging to different sections is dealt with. It should be noted that the work in [17]
does not provide the evaluation criteria that the authors had used for annotating the
paragraphs in the privacy policies. In addition, no further analysis was performed to
assess the feature space significance and effectiveness [17].
The Platform for Privacy Preferences Project (P3P) is a protocol created by
World Wide Web Consortium (W3C), for allowing websites to declare their intended
use of information they collect about web browser users. P3P is a machine-readable
language that helps to express a websites data management practices through pri-
vacy policies. Users may set their privacy preference on their P3P-enabled browser.
Before loading a web page, the browsers P3P agent checks whether the website’s
privacy policy falls within the users’ privacy setting; if it does, then the page loads
automatically, and if it doesn’t the user is prompted [18].
SPARCLE, is a privacy management system that was developed to assist an or-
ganization write and enforce privacy policies [15]. It helps the authors of privacy
policies to write privacy policies that are compliant with organizations’ privacy prin-
ciples. The SPARCLE system takes privacy policies written in natural language and
uses Natural Language Processing (NLP) techniques to parse the policy text, identify
policy elements and generate machine-readable enforceable format EPAL or XACML
of the privacy policy [15]. The limitation of this P3P approach is that it requires
server-side adoption which is not easily obtained. According to [15] only 20% of
websites among E-commerce top 300 are P3P enabled.
Hermes is a tool developed to evaluate a privacy policy based on ambiguity
[49, 54]. Singular Value Decomposition (SVD) and Latent Semantic Analysis (LSA)
techniques are used to analyze the input text (i.e. privacy policy). The tool outputs
a list of potential ambiguities along with an ambiguity score. This tool compares any
given privacy policy with a “typical” privacy policy template they created and looks at
the similarity between both policies using LSA. The main limitation of this tool is that
it compares a privacy policy with a “typical” privacy policy, which is subjective. The
scope of this research is limited to deriving word relations and potential ambiguities
13
using LSA.
The work in [59] proposes a proof of concept browser extension named Privee
that summarizes privacy policies for users based on the FTC Fair Information Practice
Principles (FIPP) notice and choice principle. This tool performs the automatic
classification of privacy policies based on the analysis of “essential” policy terms
and crowdsourcing. It attempts to retrieve privacy policy analysis results from an
online policy repository, or performs automatic classifications when no such results are
available. It combines rule-based and machine learning for classification. A dataset
consisting of 50 privacy polices and a limited number of categories were considered
for the classification task: Collection, Encryption, Ad Tracking, Limited Retention,
Profiling, and Ad Disclosure. Though the classifiers achieved an overall F-1 score of
90%, it is noted that the main limitation of the automatic classification method is
the ambiguity of natural language in privacy policies. Therefore, more work in the
area of analyzing the types of information in policies, best features and algorithms,
algorithms for resolving ambiguities were suggested as future work as this tool has
several limitations in these areas [59].
AT&T privacy bird is a P3P [19] user agent that displays a bird icon in the
browser title bar that changes colour and shape to indicate whether or not a website’s
P3P policy matches a user’s privacy preferences. A green bird denotes that the site
matches a user’s preferences, a yellow bird denotes that the site does not have a P3P
policy, and a red bird denotes that the site does not match a user’s preferences. The
biggest concern is that a yellow bird appears at most websites because most websites
are not P3P-enabled [19].
2.5 Privacy Protection Acts and Principles
There are three major legal privacy protection principles/acts that are used to
provide the general privacy requirements with which organizations should comply:
Organization for Economic Co-operation and Development (OECD) guidelines [5]
for data protection, the FTC FIPP [2] and the Personal Information Protection and
14
Electronic Documents Act (PIPEDA) [47].
• The PIPEDA applies to the commercial transactions of organizations that oper-
ate in Canada’s private sector. It provides 10 major privacy principles: account-
ability, identifying purposes, consent, limiting collection, limiting use, disclosure
and retention, accuracy, safeguards, openness, individual access and challenging
compliance.
• Internationally, the OECD Privacy Principles provide the most commonly used
privacy framework [5]. They are reflected in existing and emerging privacy
protection laws and serve as the basis for the creation of privacy programs and
additional principles. The OECD guidelines define eight privacy principles: col-
lection limitation, purpose specification, data quality, individual participation,
security safeguards, openness, use limitation, and accountability. These princi-
ples are intended to protect personal information privacy along with free flow
of information between different organizations and countries.
• The FTC FIP forms a set of guidelines regarding fair use of information about
individuals. The organizations in the USA are encouraged to adhere to the FIP
but cannot be enforced to comply with the principles. Those principles include
notice/awareness, choice/consent, security/integrity, access/participation, and
enforcement/redress. Each organization defines its own privacy policies based
on the different privacy principles and acts. These policies cover some of the
major privacy requirements that an organization should enforce in their data
processing practices [2]. For example, when websites collect information from
customers, the organization needs to inform customers why and how the data is
collected, with whom it is shared, how long it will be retained, and how it will
be used (notice/awareness principle in FIP). They should also obtain customer
consent on how to use the collected data and provide opt-in/opt-out choices for
customers (choice/consent principle). Furthermore, they should also mention
how the customer can access the data to make changes (access/participation
principle in FIP).
15
For the purpose of this research, OECD, PIPEDA and FIP principles are used to
derive the eight important sections of a privacy policy for classification. In ad-
dition, these principles are also used to develop the evaluation criteria and derive
a set of keywords to annotate these sections for evaluating both completeness and
unambiguity.
2.6 Summary
The previous work on various factors regarding information privacy transparency
was discussed in this chapter. We have also reviewed the existing work in the area
of privacy policy analysis. It is observed that none of existing tools provide a com-
prehensible evaluation system for privacy policy transparency. Most of the previous
works have limited scope and are constrained to handling small numbers of categories
(or features and datasets), making it difficult to effectively analyze the privacy poli-
cies. In contrast to all the previous work, the approach that we present here differs
from the previous works in the following ways:
• Proposes a unique evaluation framework that analyzes the content and style of
privacy policies.
• Provides a comprehensible analysis of privacy policy transparency based on
completeness, unambiguity and readability.
• Uses a novel evaluation criteria to annotate a dataset containing 100 privacy
policies.
• Proposes a detailed goal-based evaluation criteria and keyword features to eval-
uate the completeness and unambiguity of each privacy policy category.
• Conducts an in-depth keyword feature analysis and proposes algorithms for
feature selection for both completeness and unambiguity evaluation.
• Uses a set of two readability metrics to measure readability of privacy policies.
16
• Proposes an approach to combine the scores of different criteria to compute the
overall score for the transparency of a privacy policy.
17
Chapter 3
Background Techniques
In order to provide necessary background knowledge for the research presented
in this thesis, we discuss several fundamental concepts. In particular, Section 3.1
describes the factors of evaluation for transparency. Subsequently, Section 3.2 gives a
brief overview of the different stages of the knowledge discovery process. Furthermore,
various text mining techniques used for preprocessing and deriving the feature space,
classification algorithms and the metrics used for evaluating the performance of clas-
sifiers are described in Section 3.3 to 3.7. Furthermore, Section 3.8 gives an overview
of the two readability formulas used to evaluate the style criteria; i.e., readability of
privacy policies. Finally, Section 3.9 summarizes the background.
3.1 Transparency Evaluation Factors
As proposed by Ding et al., in a paper related to authorship verification [20], the
evaluation framework that we have proposed considers the content and style of the
privacy policies. In this paper, an author’s writing is analyzed based on its content
and style, when the content is relevant to a specific subject and style includes syntax
and structural features such as word length, sentence length, word frequency, parts
of speech, and punctuations among others. Similarly, in our research, we analyze
the privacy policies based on their content and style. Completeness and unambiguity
criteria are selected to analyze the content of the privacy policies, whereas the style is
evaluated based on the readability criteria. An extensive explanation of how we have
decided to focus on these three criteria in explained in Chapter 4. The content criteria
are evaluated using machine learning approaches; on the other hand, readability is
measured using existing readability formulas. An automated tool based on machine
18
learning requires an annotated dataset, but as this study is one of the early studies
in this research area, a benchmark annotated dataset is unavailable. Therefore, it
is important for us to follow the different stages of knowledge discovery in order to
propose an automated solution to evaluate the transparency of the privacy policies.
3.2 Knowledge Discovery Process
In order to propose a solution for solving the issues with transparency of privacy
policies, we need to understand and follow the different stages of the knowledge
discovery process.
• Identification of Problem: The first step is to identify a problem and explore
different possible solutions. As described above, a machine learning approach
is proposed to evaluate the content on the privacy policies. Therefore, the
problem is to propose a machine learning method to analyze the completeness
and unambiguity of privacy policies.
• Data Collection: The next step of knowledge discovery is creating a dataset
to solve the problem. In this study, we created a dataset of 100 online privacy
policies derived from most visited websites.
• Data Preprocessing: In the third step the dataset is is annotated and pre-
processed, therefore deriving a feature set. In this study we develop, a detailed
evaluation criteria to annotate the privacy policies for their completeness and
unambiguity. The evaluation criteria is then used to annotate the privacy poli-
cies. Finally, various text preprocessing techniques are used to derive the feature
space.
• Feature Selection: In this step, the dimensionality of the feature space is
reduced using feature selection or feature transformation techniques. The more
relevant features are chosen to train the classifiers. In this research, the Chi-
squared feature selection technique and other filtering techniques are used to
reduce the dimensionality of the feature set.
19
• Classification: In the final step, the classifiers are trained using the feature
set derived in the previous step. Finally, the results are interpreted and the
best performances are noted. In this research, five classifiers are trained on
different feature sets for completeness and unambiguity. The performances of
the classifiers are compared and the classifier achieving high accuracy is selected
Based on our research, three measurable criteria for transparency are identified: read-
ability, completeness and unambiguity. Readability is measured using readability
formulas. Completeness and unambiguity are measured using text mining approach
following the steps involved in the knowledge discovery process.
3.3 Text Mining
Text mining refers to the process of deriving high quality information from a text
using patterns and trends [8]. Text classification is a text mining task that involves
classifying a blob of text or a document into predefined categories. For example, if
di is a document in a set of documents D and {c1, c2, , cx} is a set of categories,
text classification assigns a category cx to a document di [8]. We used supervised
learning, where the classifier is trained on pre-labeled data. For supervised learning,
the given dataset is spilt into two sets, a training set, and a testing set. The training
set is used to train the classification model. The final performance of the classifier is
evaluated using the testing set. In this thesis, supervised classification techniques are
applied, all of which use a labelled privacy policy corpus. The learning algorithms
chosen for this research are described in the Section 3.6. The other text mining tasks
and techniques including text preprocessing and feature extraction are described in
Sections 3.4 and 3.5, respectively.
For the supervised learning, after developing the evaluation criteria and anno-
tating the dataset, the training set is preprocessed. First, tokenization, stop word
removal, stemming, vectorization, n-grams preprocessing steps are used to preprocess
the training examples of the privacy policy corpus. Second, various text mining tech-
niques are used to reduce the dimensionality of the feature space. Finally, classifiers
20
are trained with the help of the derived features to make predictions for the unknown
test examples.
3.4 Text Preprocessing
One of the most important steps in Information Retrieval (IR) is text preprocess-
ing which aims to prepare the text for classification or clustering. The preprocessing
involves many steps that result in a feature set. The preprocessing steps include:
tokenization, stop word removal, stemming, vectorization, n-grams. These methods
are the most commonly used techniques in text mining. A brief description of these
preprocessing steps is given below.
• Tokenization: Tokenization is a process of breaking a blob of text into words,
phrases, or symbols called tokens. These tokens are further used as an input
for parsing or text mining. The main aim of the tokenization is the exploration
of the words in a sentence. Tokenization is useful for lexical analysis both in
linguistics and in computer science [32].
• Stop word removal: Privacy policies may contain a lot of words that are
not useful for training the classifier. There are useless words such as auxiliary
verbs, conjunctions, articles and organization names. These words are called
stop words. We constructed a list of such words which are removed as a part of
preprocessing task [32].
• Stemming: Stemming is the process of condensing the variant forms of a word
into a common representation, the stem. For example, the words: ‘collection’,
‘collected’, ‘collecting’, ‘collects’ could all be reduced to a common representa-
tion ‘collect’. This is one of the widely used techniques in text processing for
information retrieval. It is assumed that given a query with the word ‘collecting’
implies an interest in documents containing the words ‘collection’ or ‘collects’
or ‘collected’. The Porter Stemming algorithm [48] is used to do the stemming
because of its high performance. This algorithm is based on suffix stripping
21
and it is considered to be the most popular stemming algorithms used in text
mining [32].
• Vectorization: After tokenization, stop word removal and stemming, the to-
kens and their weights need to be vectorized in a matrix. This step involves
representing tokens in a vector form [33]. Most textual vectorization approaches
are based on vectorial representation of texts using Term Frequency - Inverse
Document Frequency (TF-IDF) measure. This is a statistical model that is
intended to reflect how a word is important to a document in a corpus. The
TF-IDF value increases importance to the number of times a word appears in
the document [32].
• N-grams: In the field of computational linguistics and probability, an n-gram
is a continuous sequence of n items from a given sequence of text. An n-
gram could be any combination of letters or words. For example, Unigrams
(1-grams) are single words, Bigrams (2-grams) are sequences of two words,
Trigrams (3-grams) are sequences of three words and so on [32]. After we
tested the performance of classifiers on unigrams, bigrams and trigrams, we
observed that classifiers achieved best performance using bigram features for
completeness and unambiguity evaluation.
3.5 Feature Space Selection
After text preprocessing, the next step involves deriving the features for training
the learning algorithms. One of the main concerns of text classification is that the
number of unique features could easily reach into the tens of thousands. Therefore,
dimensionality reduction plays an important role in text classification as it helps in
applying some innovative learning algorithms. The dimensionality of feature space
can be reduced using a feature-selection method that eliminates the features that
are irrelevant for the classification. Another benefit of feature selection is that it
reduces overfitting and yields better accuracy [32]. We used the Chi-squared [48]
22
feature selection method to reduce the dimensionality of feature space obtained after
preprocessing. In addition, we used Latent Dirichlet Allocation (LDA) [56], a topic
modelling tool to extract keyword features associated with each section in the privacy
policy for the evaluation of completeness. Finally, we also used word co-occurrence
to derive the features for the evaluation of unambiguity. A brief description of each
of these tools is given below.
Chi-Squared
Chi-Squared (χ2) is a statistical test that measures expected divergence from the
distribution, if feature occurrence is independent of the class value. If the feature
is independent of the class then the χ2 score is close to 0 otherwise it is close to
1. A feature with higher χ2 score is more informative. This method is chosen as
it is considered to be one of the most effective methods of feature selection in text
classification [23, 25, 58]. In NLP, χ2 identifies the most relevant terms in a collection
of documents. It produces meaningful insights about the data and is also useful
to improve performances and computational efficiency. χ2 is computed after the
vectorization step as follows:
χ2 =
N(AD − CB)2
(A+ C)(B +D)(A+B)(C +D)
(3.1)
where f is a feature, t is a target class to predict, N is the total number of
observations, A is the number of times that f and t co-occur, B is the number of
times that f occurs without t, C is the number of times that t occurs without f, and
D is the number of times neither t nor f occur [32].
Latent Dirichlet Allocation (LDA)
LDA is a probabilistic topic modelling technique which models discrete datasets
such as text [13]. It treats documents as the probability distribution of topics and
simplifies the generative process of the text, which helps to handle large text sets
more efficiently. It is a three-tier Bayesian model, which includes documents, topics
23
and words. Each document is expressed as a mixture of topics where each topic is a
probability distribution of the fixed set of words. The three main steps involved in
the basic modelling process of LDA are:
• To establish a document-word co-occurrence matrix and then to model a train-
ing set.
• To obtain model parameters using inference methods such as document-topic
matrix and topic-word matrix.
• To predict the topic probability distribution of new documents to express text
information using the learned model.
Therefore, given a document corpus, LDA makes an attempt to learn the topic
representation of each document and the words associated with each topic [13].
This method is selected as it is known to achieve promising results in modelling text
collections [56]. LDA is used to derive the keywords associated with each of the
8 important sections in a privacy policy (topics). These keywords are used during
feature selection for completeness evaluation.
Word Co-occurrence
Word co-occurrence analysis is a text mining technique that is used for text
classification and to construct ontologies. The main aim of this technique is to find
similarities between word pairs and to find word pairs that co-occur most often. A
word co-occurrence matrix is a N x N matrix where N corresponds to the total number
of unique words in a corpus. A cell xij contains the number of times xi co-occur with
word xj within a particular context such as a sentence or a certain window of x
words. The upper and lower triangles of the matrix are identical as co-occurrence is a
symmetric relation [40, 46]. Similar to the approach followed in [22], the frequency
of word co-occurrence between a set of ambiguous words and important keywords
(identified using LDA) found in privacy policies is used to obtain the final feature set
for evaluating the unambiguity of privacy policies to improve the performance of the
24
Table 3.1: Example of Word Co-occurrence Frequency Matrix
A B C D E
A 0 1 3 2 3
B 1 0 1 0 1
C 3 1 0 2 2
D 2 0 2 0 4
E 3 1 2 4 0
classifiers by adding more discriminative features to the feature set. The ambiguous
words are identified in one of the previous studies which analyzed privacy policies
[49]. An example showing the word co-occurrence frequency matrix for the text A D
C E A D F E B A C E D using a 2-word moving window (w = 2) is depicted in Table
3.1.
3.6 Learning Algorithms
A variety of supervised classification algorithms have been used to evaluate the
completeness and unambiguity of privacy policies in the classification step: Linear
Support Vector Machines (LSVM), K-Nearest Neighbours (KNN), Random Forests
(RF), Multinomial Naive Bayes (MNB) and Logistic Regression (LR). Based on the
problem (i.e., text classification), size of the dataset, and number of features, we
selected the classifiers that are most capable of text classification and that can handle
high dimensional feature spaces and small datasets. The five supervised classification
techniques that we chose to investigate in the context of the transparency evaluation
system are described below.
Linear SVM (LSVM)
Support Vector Machine (SVM) [16] is a supervised learning algorithm used
for classification. The main objective of SVM is to find a hyperplane which gives
the largest margin that separates the classes in the training data. The margin is
25
the distance between the given hyperplane and the closest training data point, and is
measured in relation to both sides of the hyperplane. Large margins are preferred over
smaller margins in order to give better generalization error as smaller margins may
result in overfitting and poor performance on new data. The actual support vectors
of a SVM consist of the training data points that lie closest to the hyperplane. These
support vectors are used to define the decision boundary, and overall margin distance.
Once the hyperplane with the largest margin is found, new objects are easily classified
based on which side of the hyperplane they fall on. Fig 3.1 illustrates the setup of
SVM classifier. Kernels functions are applied to the dataset before running a SVM.
A kernel function is used to map the dataset into a higher dimensional space to find
the hyperplane easily. The four basic kernels are: polynomial, radial basis function,
sigmoid and linear [16]. The use of some kernels require a user to tune various
parameters to achieve higher accuracy. Tuning can be done manually or by a grid
search through the parameter space. We used LSVM which is SVM using a linear
kernel function as it is most commonly used method for multiclass classification for
decomposing the problem. This algorithm is chosen as it is very effective in high
dimensional feature spaces and works well for text classification [35, 48].
Random Forest (RF)
Random forest [14] is an ensemble method for classification which uses multiple
decision trees at the training time and outputs the class that is the mode (most
frequent) of the classes predicted by the individual trees. In the ensemble classification
method the results of several classifiers are combined and weighted to make the final
prediction (Fig 3.2). The ultimate goal of an ensemble system is to combine the
outputs of multiple classifiers in such a way that it outperforms the performance of
a single classifier. This classifier is chosen as it helps in improving the predictive
accuracy for new data examples and also reduces over-fitting and performs well on
high dimensional data [57]. Other strengths of the RF classifier include fast runtimes
and its ability to deal with unbalanced or missing data [14].
26
Figure 3.1: SVM classifier, circles and squares representing different classes
27
Figure 3.2: Random Forest Ensemble Classifier
K-Nearest Neighbour (KNN)
K Nearest Neighbour (KNN) [10] is a classification technique that performs
classification for a new unseen example based on its similarity to already seen training
examples. It is considered a lazy learner because it does a small amount of work in
the training phase as it stores all the training examples. In the testing phase, during
classification of unseen examples, KNN searches the training set and finds the K
closest records based on the Euclidean distance to the new example. These records
are called nearest neighbours [10, 48]. This algorithm is selected due to its proven
ability to be very effective for different problem domains, including text classification
as a result of its simplicity and performance [28, 41]. Fig 3.3 shows the visual
representation of this process. The star represents a test example that we would like
to classify, the yellow and purple circles represent training examples for Class A and
Class B, respectively. Depending on the value of K the predicted class of the new test
instance will be different. For example, if K = 3, the star will be classified as Class
B. If K = 6, the star will be classified as Class A.
28
Figure 3.3: An Example of K-Nearest Neighbour Classification
Multinomial Naive Bayes (MNB)
The Naive Bayes (NB) [51] classifier is a probabilistic classifier which is based on
Bayes theorem. It is one of the most basic text classification techniques with various
applications in spam detection, document classification, and sentiment analysis.
A NB classifier assumes that the attributes in a feature vector are independent
of one another, given the class label y. When given a set of attributes X = {Xi , ...,
Xk}, a NB classifier will calculate the probability of y given X, where y is a class,
using Eq. 3.2:
p(y|x) = p(y)p(x|y)
p(x)
(3.2)
The class label with the highest probability is considered as the predicted class
for the attribute set in question. NB uses training data to estimate the value of P(xi |
y). If the attributes are categorical, the estimation is calculated by taking the fraction
of training instances in class y that have the attribute value xi [51].
Usually Multinomial Naive Bayes (MNB), a variant of NB, is used when the
multiple occurrences of the words are critical in the classification problem such as in
29
topic classification [51]. The MNB variation estimates the conditional probability of
a particular word or token given a class as the frequency of term t (Tt) in documents
belonging to class y as shown in Eq. 3.3:
p(t|y) = Tyt∑
t′V Tyt′
(3.3)
Thus, this variation takes into account the number of occurrences of term t from
class y in training documents, which also includes multiple occurrences of terms [51].
This classifier is chosen due to its simplicity and high performance on various text
classification datasets. It is also proven that MBN outperforms other variant models
of NB for high dimensional feature spaces [29, 37, 44].
Logistic Regression (LR) (aka logit, MaxEnt)
Logistic regression [27] estimation obeys the maximum entropy principle, and
thus logistic regression is sometimes called “maximum entropy modelling”. LR is a
type of regression that predicts the probability of the occurrence of an event by fitting
data to a logistic function (logit). Like many other forms of regression analysis, it
uses several numerical or categorical variables for prediction. LR is selected for this
research because it is usually preferred for text classification especially while dealing
with high dimensional feature space and provides highly accurate predictions [26, 27].
Similar to NB, the goal of LR is to estimate P(y|x), but the method for doing so is
different. LR trains a classifier by explicitly maximizing the likelihood of σ (β * x )
for all documents x in the training data, where σ is called the hypothesis function.
For LR, σ is chosen to be the logistic function as shown in Eq. 3.4.
σ(t) =
et
et + 1
=
1
1 + e−t
(3.4)
This function has a domain [0, 1] and range [-∞,∞], which makes it a convenient
approximation of a probability. For a classification problem, given a test example x,
unlike NB, LR directly estimates the conditional probability of assigning a class label
y to the example by Eq. 3.5 shown below:
30
Table 3.2: Confusion Matrix/ Contingency Table
Predicted
Positive Negative
True TP TN
Actual
False FP FN
p(y|x) = 1
1 + e−yαT x
(3.5)
where α is the model parameter.
3.7 Performance Evaluation
Cross validation is an iterative process where a dataset is partitioned into k folds.
In each of these cross validation iterations, k-1 folds are used for training purposes and
testing is performed on the remaining fold. The average of the performance results is
considered as a reliable value for representing the performance of the classifier. Fig
3.4 illustrates an example of 5-fold cross validation, where the dataset is divided into
5 folds and where 4 folds are used for training purposes and the remaining one fold
is used for validation.
There are several measures that can be used to evaluate the performance of a
classifier: Accuracy, Precision, Recall and F-measure [32]. Each of these measures
has its own set of limitations. As a result, a measure that is appropriate for the
problem should be selected. In this context, True Positive (TP) refers to the total
number of examples in the dataset that were a part of the positive class and were
predicted correctly. True Negative (TN) refers to the total number of examples in
the dataset that were a part of the negative class and were predicted correctly. False
Positive (FP) refers to the total number of examples in the dataset that were a part
of the negative class and were predicted incorrectly. False Negative (FN) refers to the
total number of examples in the dataset that belong to the positive class but were
predicted incorrectly [32].
31
Figure 3.4: An Example of 5-Fold Cross Validation
Accuracy measures the total number of true predictions of the classifier out of
the total number of all possible predictions. It is very important for the dataset to
be balanced, i.e. the dataset should contain a balanced number of examples for each
class [32]. Refer to Eq. 3.6.
Accuracy =
TP + TN
TP + TN + FP + FN
(3.6)
Precision evaluates how precise the classifier predicts. It is the number of true
positives predicted by the classifier out of all the positive labels assigned to the
examples in the dataset [32] Refer to Eq. 3.7.
Precision =
TP
TP + FP
(3.7)
Recall measures the ability of a classifier to recall positive classes. It calculates
the amount of truly predicted positive classes out of the number of total actual positive
classes [32]. Refer to Eq. 3.8.
Recall =
TP
TP + FN
(3.8)
F-measure considers both precision and recall. It is the weighted average of
precision and recall. F-measure combines precision and recall measures that assesses
the tradeoff between precision and recall [32]. Refer to Eq. 3.9.
32
F =
2 ∗ precision ∗ recall
precision+ recall
(3.9)
Table 3.2 illustrates the Confusion Matrix/ Contingency Table, a metric that assists
with performance evaluation which consists of the above defined concepts. For all of
the experiments performed in this thesis, the accuracy, precision, recall and F-measure
of the five classifiers identified in Section 3.6 were calculated and the F-measure is
reported.
In this research, the classifiers were trained using 3-fold cross-validation and their
performances were compared by considering their F-measure. The F-measure metric
is chosen because is it known to give more reliable results when there is an imbalance
in the classes [32] and in our experiments it was observed that the classes were
not perfectly balanced. To ensure reliability and reproducibility of the results, each
experiment was repeated 10 times and the average testing F-measure resulting from
10 iterations was reported in Chapter 6.
3.8 Style Criteria
The readability feature is chosen among all the various features used to analyze
the style of a text. This criterion looks at the various shallow features of a piece of text
such as the number of sentences, length of a sentence, average number of words, and
average number of syllables per word among others to measure the readability. All
these attributes help to further understand the ease with which one can read the text
and also in producing an estimate of the level of education required to comprehend
the text.
3.8.1 Readability Formulas
Readability refers to the ease with which text can be read and understood. In
other words, readability refers to the quality of being easily comprehensible to com-
mon users. Readability of a text mainly depends on the content (how well the content
is organized or structured) and other features like sentence length, number of letters,
33
number of syllables etc. [24]. Various metrics are available to measure text readabil-
ity. Following are the two readability formulas selected to evaluate the readability of
privacy policies in the corpus. These two formulas were considered because one for-
mula is focused on readability ease while the other is focused on grade level required
to comprehend the text.
• Flesch Readability Ease Score (FRES): This formula measures the read-
ability of a blob of text and outputs a number between 0-100. It uses average
sentence length and average number of syllables per word along with some
predefined factors to measure the readability. Refer to Eq. 3.10.
FRES = 206.835− 1.015( words
sentences
)− 84.6(syllables
words
) (3.10)
The following FRES scores help in guiding the user to assess the readability of
a piece of text: 90-100 : Very Easy, 80-89 : Easy, 70-79 : Fairly Easy, 60-69 :
Standard, 50-59 : Fairly Difficult, 30-49 : Difficult, 0-29 : Very Confusing. The
final readability score of a privacy policy, a number between 1 and 100 will be
scaled to a number between 1 and 10 by dividing by 10.
• Flesch Grade Level (FGL): FGL compares syllables and sentence lengths. It
predicts the grade level one requires to comprehend the given text. For example,
a score of 9.3 means that a ninth grader would be able to read the document.
The FGL score is calculated using Eq. 3.11:
FGL = 0.39(
words
sentences
) + 11.8(
syllables
words
)− 15.59 (3.11)
We translated the FGL score to a score between 1 - 10 (10 - Very easy to read,
1 - Very difficult to read) as shown in Table 3.3.
3.9 Summary
In this chapter, the process of knowledge discovery and various text mining
techniques were described. Furthermore, five distinct classifiers, LSVM, RF, KNN,
34
Table 3.3: Scoring Scheme Based on Grade Level for FGL
Age Range Grade Level Translated Score Readability Level
<11 yrs 1 - 6 10 Very Easy
12 7 9
13 8 8
14 9 7 Easy
15 10 6
16 11 5
17 12 4 Difficult
18 13 3
19 14 2
>20 yrs >= 15 1 Very Difficult
MNB and LR were introduced and discussed briefly. The suitability of different
performance evaluation measures were also discussed for evaluating the performance
of the classifiers. The classifiers mentioned above are chosen based on the type of
the problem (text classification), size of the dataset and high dimensionality features.
The two readability formulas used for measuring readability of privacy policies were
also described in this chapter.
35
Chapter 4
Proposed Approach
In this chapter, the problem statement and the proposed approach are discussed
briefly. Section 4.1 describes the problem. Subsequently, Section 4.2 presents the
proposed approach and description for the evaluation of content and style factors
for each of the three identified contributing criteria - completeness, unambiguity and
readability. Finally, Section 4.3 summarizes the key points covered in this chapter.
4.1 Problem Statement
As discussed in Chapter 1 and Chapter 2, there are several problems associated
with privacy policies. They are often long, hard to understand and contain incomplete
information. Transparency is recognized as a solution to some of these problems.
Therefore, privacy policies need to be transparent so that average individuals are
informed of the consequences of sharing their personal information. In order to help
the users gauge the level of transparency, it would be beneficial to have an approach
that evaluates the transparency of a given privacy policy and provides feedback to
the users. Therefore, the primary goal of this research is to develop an automated
approach to evaluate the transparency of a given online privacy policy.
4.2 Proposed Approach
As discussed in the previous chapters there has been very little research reported
in the area of privacy policy transparency and none of the existing tools focus on
analyzing the privacy policy for its transparency. Lack of benchmark dataset and
results, and lack of research in this area are some of the main challenges associated
36
with this problem. Therefore, we had to propose a solution to solve this problem from
the root level. The first step was to find the definition of transparency and examine
those factors contributing to transparency.
Our research identifies several criteria for a transparent privacy policy from the
definitions of Transparency - completeness, unambiguity, readability, accountability,
accessibility, and accuracy. Table 4.1 shows the different definitions of transparency
and highlights the words associated with the criteria derived from the definitions.
Accessibility refers to whether online privacy policies are easily accessible to the user.
Websites should provide links to privacy policies in such a way that they are easily
found. Since our focus is on content and style of privacy policies, evaluating this
criterion is beyond the scope of our research. Accountability is a prominent privacy
principle where organizations are obligated to follow the law and enforce the data
privacy practices proposed by those laws [6]. Although this is a very critical criterion
to be considered, we will not be able to measure this aspect of transparency as it
could be evaluated only through government privacy inspections or audits. Accuracy
is another aspect of transparent privacy policies, where the online polices need to be
accurate and the organizations should ensure they follow and enforce the policies they
propose in their privacy policy without any loop holes [6]. Similar to accountability,
this aspect is not measurable without human intervention as it requires the practices of
the organization to be examined through audits and subject matter inspections. The
only remaining criteria that are measurable with an automated tool are completeness,
unambiguity and readability. Readability is concerned with style features such as
word length, sentence length among others, while completeness and unambiguity
are more focused on the content of the privacy policies. Although all of these goals
contribute towards the overall transparency of a privacy policy, in this thesis, we focus
on completeness, unambiguity and readability by developing an automated approach
to assess each of these criteria.
As discussed, to develop and propose an evaluation scheme for evaluating the
transparency of a privacy policy, we prominently considered two factors - content
and style. The content and style factors are described in the Sections 4.2.1 and 4.2.2
37
Table 4.1: Criteria of a Transparent Privacy Policy
Definitions of Transparency Goals of Transparent Privacy Policy
“Transparency is open and conveniently accessible information. It is
understandable, reasonable, and quality information that can help the
public make informed decisions, provide meaningful oversight, and provide
valuable input to public officials. The two key phrases are ‘understandable’,
meaning placed in context, and ‘quality’, meaning complete, accurate, and
timely.” [4].
“Conveys openness but also guarantees secrecy.” [7].
“It is concerned with the quality of being clear, obvious and understandable
without doubt or ambiguity.” [50].
“Information transparency is defined as the degree of visibility and accessibility
of information.” [36].
“It is a characteristic of government, companies, organizations and individuals of
being open in the clear disclosure of information, rules, processes and actions, As
a principal public officials, civil servants, managers and directors of companies
and organizations, and board trustees have a duty to act visibly, predictably, and
understandably to promote participation and accountability.” [3]
Completeness
Unambiguity
Readability
Accountability
Accessibility
Accuracy
38
respectively.
4.2.1 Content Criteria
The content criteria for evaluating the transparency of a privacy policy include
evaluating the privacy policy in terms of its completeness and unambiguity by closely
looking at its content. To examine the content criteria, we use machine learning
approaches in order to automate the evaluation of the transparency of privacy poli-
cies. However, a lack of benchmark datasets and criteria to objectively evaluate the
transparency of privacy policies causes us to tackle this problem from its foundations
using knowledge discovery process. The following are the steps that we propose to
evaluate the content criteria i.e., completeness and unambiguity of privacy policies.
• Understanding the problem : First, we tried to understand what makes a
privacy policy complete and unambiguous. To achieve this for completeness, we
adopted a goal-based approach and examined important sections recommended
by the privacy protection acts and principles (FTC, OECD, PIPEDA) that
should exist in a privacy policy. For unambiguity, we again adopted a goal-based
approach to evaluate the privacy policy based on its unambiguity.
• Generating annotated data : We generated an annotated dataset which
accurately represented the content of the privacy policy in terms of the com-
pleteness and unambiguity of section. A corpus consisting of 100 online privacy
policies was selected based on various criteria - most visited websites based
on web traffic from Alexa.com [9], gold standard privacy policies [12], Eu-
ropean privacy policies [17] and other criteria discussed in Chapter 5 Section
5.3 (Refer to Appendix A.3. for the list of websites). In addition to the goal-
based approach for completeness, we conducted a survey to understand the user
perspective. Certain keywords were also identified from the survey responses,
privacy protection acts and principles and LDA for each section (Refer to Ap-
pendix C.1. for the survey). The survey responses, keywords and goal questions
were used as an evaluation criteria to annotate the dataset for completeness.
39
Similarly, survey responses, word co-occurrence between ambiguous words and
keywords and a set of goal questions were used as an evaluation criteria to an-
notate the dataset for unambiguity. The evaluation criteria for completeness
and unambiguity was also validated by the subject matter experts (that in-
cluded one lawyer and three legal practitioners with expertise in information
privacy) in order to ensure reliability. The final annotations for the dataset
were performed by two graduate students in computer science.
• Data Preprocessing : The dataset was preprocessed using the most commonly
used text mining techniques such as tokenization, stemming, stop word removal,
and vectorization.
• Feature Selection : The feature space consisting of thousands of features was
reduced using Chi-squared dimensionality reduction techniques and the filtering
algorithms we proposed. Important keyword features were also identified using
the subject matter experts, past studies and LDA. Finally, four different feature
sets were derived and used for training the classifiers.
• Classification : The classifiers were trained on both the unfiltered feature sets
and the feature sets derived from our proposed feature selection and filtering
algorithms. The experiment results for different feature sets were compared
and the feature set and the classifier resulting in best prediction capability was
selected.
As knowledge discovery is an iterative process, we repeatedly experimented to enhance
each of the steps described above to obtain more satisfactory and refined results.
Chapter 5 describes each of these steps for both completeness and unambiguity in
more detail.
4.2.2 Style Criteria
The style criterion for evaluating the transparency of a privacy policy includes
evaluating the privacy policy in terms of its readability by closely looking at its
40
shallow features such as length, word count, syllable count among others. Readability
formulas can be used to evaluate the style attributes of privacy policies. Chapter 3
describes two readability formulas - FRES (for readability ease score), and FGL (for
grade level). Both of these formulas consider different attributes of the text such
as number of words in a sentence, number of syllables per word, and number of
characters per word to compute the readability [24].
The final readability score was calculated by taking the mean of the two scores
as shown in Eq. 4.1.
R = 100 ∗
[
FRES + FGL
2
]
(4.1)
where the constant 100 was used to scale the result to [0-100].
4.3 Summary
In this chapter, the problem statement and proposed approach are discussed.
Transparency is recognized as the solution to the issues associated with online pri-
vacy policies such as difficulty to understand, ambiguity and insufficient content.
Then the contributing factors of a transparent privacy policy are proposed based on
the definitions of transparency. Finally, for the content and style criteria, three mea-
surable factors - completeness, unambiguity and readability are selected in order to
conduct further research. We propose to use machine learning algorithms to evaluate
the content of privacy policies. The readability of privacy policies is evaluated using
two existing readability metrics - FRES, FGL.
41
Chapter 5
Experimental Setup
This chapter describes the experimental setup for evaluating the content factor.
In particular, the text mining approach is described briefly in Section 5.1. Sections
5.2 to 5.6 describe the different stages of the knowledge discovery process including
the evaluation criteria, dataset, text preprocessing, feature-space extraction, experi-
mental approaches and parameter tuning of classifiers involved in the evaluation of
completeness and unambiguity. Finally, Section 5.7 gives a summary of the chapter.
5.1 Text Mining of Privacy Policies
In this chapter, we focus on the content factor that was addressed in the previous
chapter. We provide a more detailed explanation of each of the steps involved to build
the automated approach for the two criteria - completeness and unambiguity.
To start with the process of knowledge discovery, we began by examining the
problem and analyzing some of the possible solutions to solve the problem. In this
case, our main goal was to propose an approach that measures the degree of com-
pleteness or unambiguity for a given privacy policy. To do this, we first examined
privacy protection acts and principles such as FIPP, OECD and PIPEDA and iden-
tified eight important sections that any privacy policy needs to cover. This exercise
helped in providing a direction to solve the problem of completeness. Therefore, we
deemed a privacy policy to be complete if it contained all these eight sections. The
eight privacy policy sections are as follows: collect, choice, cookies, access, purpose,
retention, share, and security. A privacy policy may contain all or a subset of these
eight sections. Therefore, from the machine learning context, this is a multiclass
classification problem, which means that a privacy policy may be assigned multiple
42
classes (among the 8 sections). There are two approaches to solve this problem:
• The first approach is to tackle this problem as a multiclass classification prob-
lem, where multiple classes are assigned to a single privacy policy. We con-
ducted a preliminary analysis using this approach where the results were not
very promising. Due to the lack of rich datasets containing different possible
combinations of sections, this approach resulted in sparsity of the feature space.
Therefore, this approach was not reliable.
• The second approach is splitting this multiclass classification problem into eight
binary classification problems. Eight classifiers were trained for completeness,
and for each section each privacy policy was annotated for its completeness (0 -
incomplete, 1 - complete). Similarly, eight classifiers were trained for unambigu-
ity, and for each section each privacy policy was annotated for its unambiguity
(0 - ambiguous, 1 - unambiguous). Since this approach showed more promising
results we focused on this approach for the remainder of the experiments.
In order to begin with the experimentation and training the classifiers, we devel-
oped an evaluation criteria and annotated the privacy policy dataset for completeness
and unambiguity. After completing annotations based on the evaluation criteria, the
privacy polices were preprocessed using the following preprocessing steps: Tokeniza-
tion, Stop word removal, Stemming and Vectorization; then the final feature space
was obtained. Chi-squared feature selection method was used to reduce the size of the
feature space. Subsequently, binary classifiers were trained for predicting the com-
pleteness and unambiguity of each section using different approaches with different
sets of features. Finally, the feature space and the classifier that achieved the best
results were chosen and the final transparency score was calculated and compared
with the manual annotations and user ratings from the survey. Fig 5.1 illustrates
the workflow of the iterative knowledge discovery process applied for the evaluation
of completeness and unambiguity. The following sections describe each stage of the
knowledge discovery process in more detail.
43
Figure 5.1: Knowledge Discovery Process Workflow
5.2 Evaluation Criteria
To annotate the privacy policy corpus based on completeness and unambiguity,
we needed to first develop an evaluation criteria. To propose the evaluation criteria of
44
completeness, we examined privacy protection acts and principles and developed a set
of goal questions. We utilized the sections recommended by these acts and principles
which allowed us to develop a detailed description of what each section needed to
cover in order to be considered complete. We also proposed a set of goals expressed
in the form of questions for each section, which were used for annotating the sections
for their completeness. Table 5.1 gives the description of the sections, goal questions
(questions that need to be answered in each section) and legal privacy protection acts
and principles. Descriptions and goals of each section were validated by the subject
matter experts to ensure reliability and accuracy with the help of a survey.
In addition to the above, we created and conducted a survey to better understand
the perspective of the users on what makes a privacy policy complete or unambiguous.
The survey contained 20 questions related to the completeness and unambiguity of
the eight sections for a given privacy policy (Refer to Appendix C.1. for the survey).
In the survey, the participants were asked their opinion on each of the eight sections
identified. Participants were asked whether or not a section existed in the privacy
policy. They were also asked to copy and paste the sections from the privacy policies
that they considered complete to a text box so that we could further analyze their
responses as well as identify the important keywords. Participants were also asked
whether or not a section in a privacy policy is ambiguous. They were also asked
to copy and paste the words or sentences that they found ambiguous so that their
responses could be further analyzed and the ambiguous words or phrases in relation
to the keywords could be identified. Finally, the participants were asked to comment
on the overall completeness and unambiguity of the privacy policy and to share their
concerns. The survey was taken by 10 fourth year undergraduate students with
backgrounds in computer science. Their inputs were collected and acted as part of
their fourth year course assignment. The results of the survey were anonymized and
aggregated for analysis. Based on the total number of participants available to take
the survey, we selected a subset of 10 privacy policies that belonged to a variety
of applications from the corpus, whereby each participant was assigned two privacy
policy surveys and each privacy policy survey was taken by three individuals (Refer
45
to Appendix C.1. for the survey). The survey responses were not directly used to
annotate the dataset, but they were used to create and improve the evaluation criteria
for completeness and unambiguity annotations. Subsequently, this survey was also
taken by the subject matter experts (a lawyer and 3 legal practitioners) and their
inputs were directly reflected upon improving the keywords and evaluation criteria
for completeness and unambiguity. The final evaluation criteria was reviewed and
validated by the subject matter experts.
Fig. 5.2 shows a sample response of a participant from the survey for a question
related to the completeness of data collection section of a privacy policy. Fig. 5.3
shows the sample ambiguous sentences from privacy policies given as a response to
questions related to the unambiguity of privacy policies by the survey participants.
Table 5.1: Goal-Based Evaluation Criteria for Completeness
Section Description Goal Questions Privacy Protection Acts
and Principles
Access This section states how a
user can access their per-
sonal information and keep
it accurate. It provides
steps to how user can mod-
ify/ update their personal
information.
Does the privacy policy give
users access to update/cor-
rect/delete their informa-
tion? Does the privacy pol-
icy provide directions to ac-
cess/modify information ei-
ther by directly contact-
ing them or using online
account?
FIPP: Access - “Websites
would be required to offer
consumers reasonable access
to the information a Website
has collected about them,
including a reasonable op-
portunity to review informa-
tion and to correct inaccura-
cies or delete information.”
[2].
OECD: Access - “Data sub-
jects must be allowed to ac-
cess and make corrections to
inaccurate data.” [5]
46
Table 5.1: Goal-Based Evaluation Criteria for Completeness
Section Description Goal Questions Privacy Protection Acts
and Principles
Choice This section describes the
user’s privacy choices such
as option to opt out, not
get tracked. Choices should
be explicitly discussed for
opt-in/opt-out, deactivating
their account, and other
options.
Does the privacy policy
provide users choice to
opt-in or opt-out of market-
ing/newsletters/cookies/3rd
party sharing etc.? Does the
privacy policy provide a list
of choices/ rights the users
have, in order to protect
their personal information
privacy?
FIPP: Choice - “Websites
would be required to of-
fer consumers choices as to
how their personal identify-
ing information is used be-
yond the use for which the
information was provided.”
[2]
OECD: Consent - “Data
should not be disclosed
without the data subject’s
consent.” [5]
Collect This section discloses how
and what information the
company may collect from
the users. It is expected that
the policies explicitly refer
to personally identifiable in-
formation (PII) that is being
collected.
Does the privacy policy
mention what PII (or per-
sonal data) is collected?Does
the privacy policy indicated
how the personal informa-
tion is collected? Does
the privacy policy state
the affects of not providing
personal information?
FIPP: Notice: “Websites
would be required to provide
consumers clear and con-
spicuous notice of their in-
formation practices, includ-
ing what information they
collect, how they collect it.”
[2]
OECD: Notice - “Data sub-
jects should be given no-
tice when their data is being
collected.” [5]
47
Table 5.1: Goal-Based Evaluation Criteria for Completeness
Section Description Goal Questions Privacy Protection Acts
and Principles
Cookies This section explains the
website use of cookies and
other technologies like web
beacons, pixel tags, gifs
etc. It also states the pur-
pose of using cookies and
information stored by the
cookies.
Does the privacy policy
mention about the usage
of cookies/other technolo-
gies? Does the privacy
policy indicate what infor-
mation is gathered through
cookies/other technologies?
FIPP: Notice: “Websites
would be required to provide
consumers clear and con-
spicuous notice of their in-
formation practices, includ-
ing what information they
collect, how they collect it
(e.g., directly or through
non-obvious means such as
cookies).” [2]
Purpose This section describes the
purpose of information col-
lection. It describes for what
purposes the company will
be using users’ data.. They
should also confirm that the
data will not be used for
other purposes.
Does the privacy policy indi-
cate the purpose of collect-
ing personal information?
Does the privacy policy
mention whether the infor-
mation is used for any other
purposes?
FIPP: Notice: “Websites
would be required to provide
consumers clear and con-
spicuous notice of their in-
formation practices, includ-
ing what information they
collect, how they collect it,
how they use it.” [2]
OECD: Purpose - “Data
should only be used for the
purpose stated and not for
any other purposes.” [5]
Retention This section describes the
websites process in retaining
users’ personal information.
In general the details may
cover the purpose, duration
and reasons of retention of
personal data.
Does the privacy policy no-
tify user of the retention
period of the data col-
lected? Does the privacy
policy mention if it is pos-
sible to delete entire infor-
mation that is collected even
from the backups?
“Under Directive
2002/58/EC on privacy
in electronic communi-
cations, data generated
by the use of electronic
communications services
must in principle be erased
or made anonymous when
those data are no longer
needed for the transmission
of a communication.” [1]
48
Table 5.1: Goal-Based Evaluation Criteria for Completeness
Section Description Goal Questions Privacy Protection Acts
and Principles
Security This section states the web-
site’s practices and stan-
dard in protecting user’s in-
formation. It may refer
to the security technologies
applied by the website as
well as company’s policies
that regulate the employees’
practices.
Does the privacy policy in-
dicate the security technolo-
gies used to protect users
personal information?Does
the privacy policy indicate
the steps taken by the
organization employees to
protect the users’ personal
information?
FIPP: Security - “Websites
would be required to take
reasonable steps to protect
the security of the infor-
mation they collect from
consumers.” [2]
OECD: Security - “Col-
lected data should be kept
secure from any potential
abuses.” [5]
Share This section states whether
the company will share
users’ information with
external parties. It also
states with whom the users’
personal information is
shared and under what cir-
cumstances. It is expected
that the policies explicitly
indicate with which affili-
ates, or service providers the
users’ personal information
will be shared.
Does the privacy policy in-
dicate whether the personal
information is sold/ rented
to 3rd parties? Does
the privacy policy mention
what information is shared
with service providers/con-
tractors etc. and under
what circumstances? Does
the privacy policy explic-
itly indicate the names of
affiliates/ service providers?
FIPP: Notice: “Websites
would be required to provide
consumers clear and con-
spicuous notice of their in-
formation practices, includ-
ing what information they
collect, how they collect it,
how they use it, whether
they disclose the informa-
tion collected to other enti-
ties, and whether other en-
tities are collecting infor-
mation through the site.”
[2]
OECD: Disclosure - “Data
subjects should be informed
as to who is collecting their
data.” [5]
In addition, we extracted a set of important keywords to aid us in annotating the
privacy polices that can also be used during feature selection process. The keywords
were extracted from various sources such as by parsing privacy protection acts and
49
Does the privacy policy state what personal information may be collected by the
website?
• No - the policy does not talk about personal information collection
• Yes - the policy talks about personal information collection (Please
copy and paste the sentences that talk about what personal
information is collected by the website)
Answer by a participant:
When you make purchases from Marks & Spencer (including purchases made in-
store on any device, over the phone, or via our catalogue order forms or browse
this Website) we may collect the following personal data about you:
your name, age and sex;
your billing and delivery postal addresses, phone (including mobile phone), fax
and e-mail details;
where you have registered with us, your user name and password;
where you place an order with us, your payment card details;
your communication and shopping preferences;
your browsing and shopping activities;
your date of birth;
your interests, feedback and survey responses; and
your location
We may also collect some or all of the above personal data about you when you
access and browse this Website or any third party microsite (as described below),
including when you sign up to receive Marks & Spencer newsletters. We may also
collect some or all of this personal data from third parties who have your consent
to pass your details to us.
Figure 5.2: A sample response by a subject matter expert for the Marks & Spencer
privacy policy survey
50
”You should note that in using the Urban Dictionary service, your information
will travel through third party infrastructures which are not under our control”
”WebMD may combine Personal and Non-Personal Information collected by
WebMD about you, and may combine this information with information from
external sources.”
”Information that WebMD collects about you may be combined by WebMD with
other information available to WebMD through third parties for research and mea-
surement purposes, including measuring the effectiveness of content, advertising
or programs.”
”To prevent unauthorized access, maintain data accuracy and ensure the appro-
priate use of information, we have put in place commercially reasonable physical,
technical and administrative controls to protect your information. Please note that
no method of transmission over the Internet is 100% secure.”
”We may use and store information about your location to provide features of our
Services, such as Tweeting with your location, and to improve and customize the
Services, for example, with more relevant content like local trends, stories, ads,
and suggestions for people to follow.”
”You can choose not to provide certain information, but then you might not be
able to take advantage of many of our features. - How does this work?”
”Information from Other Sources: We might receive information about you from
other sources and add it to our account information. Click here to see examples
of the information we receive. - Click here link makes no sense. - Where exactly
do they get it?”
Figure 5.3: Sample ambiguous sentences chosen by a technical participants for
questions related to unambiguity
51
principles, survey responses and by processing 700 privacy policies and using the word
frequencies and topic words extracted using LDA.
As noted above, we used the privacy protection acts and principles (refer to
Table 5.1) as well as the survey responses to identify the most important keywords
for each completeness section. In addition, we also used LDA to gather additional
keywords from different topics identified by the algorithm (refer to Fig. 5.4). More
keywords were added by extracting the word frequencies by processing 700 privacy
policies. Approximately 1300 most frequent words were extracted and the top 100
words were analyzed. About 70 out of the top 100 words belong to the keywords
that we have extracted. Furthermore, we accounted for different variants of words,
and also included the synonyms for the keywords. Table 5.2 summarizes a subset
of keywords identified for each section. The goal questions and keywords were also
validated by the subject matter experts to add more credibility to our annotations.
With the help of the survey responses, keywords, and goal questions, we annotated
our 100 privacy policies. The annotation process was performed independently by
two individuals with computer science backgrounds. The inter-annotator agreement
is 95%, which illustrates the reliability of the annotations. Appendix A.1 shows the
description for the privacy protection acts and principles associated with each of the
eight important sections identified in privacy policies. The important words found in
the parsed privacy protection acts and principles are listed in Appendix A.2. These
words are used as the seed words to gather more related keywords found in privacy
policies using LDA.
Similar to the goal-based approach used in developing the evaluation criteria for
completeness we created three goals questions that are common to all eight sections.
• Goal Question 1: Is this section complete? This question is important because
in our understanding, if certain information is missing from a section, i.e. if a
section is incomplete, then it can automatically be considered as ambiguous as
it lacks clarity due to incomplete information. Hence, in this case we used the
information we had from the annotations of completeness sections. Therefore,
if the section was incomplete then it was annotated as ambiguous.
52
Figure 5.4: LDA on Privacy Policy Text
• Goal Question 2: Does the text under this section contain sentences with
ambiguous words co-occurring with keywords identified from this section in a
window of 5 words? It is not always the case that if a section is complete then it
should be unambiguous; the section can still be ambiguous. Here, we again used
the information gained from the evaluation criteria of completeness - keywords
identified for each section. We looked for combinations of keywords co-occurring
with ambiguous words. For example, in the context of ambiguity related to
section ‘collect’, the features {may collect} {occasionally gather} {may receive}
53
Table 5.2: Keywords Identified for each Section
Section Keywords
Access
correct, access, change, update, modify, edit, delete, settings, profile, account,
preferences, accurate
Choice choice, opt-in, opt-out, unsubscribe, subscribe, consent, choose, do not track
Collect
collect, information, personal, identifiable, telephone number, ip address, mobile,
sex, device, email address, name, date of birth, age, receive, account, credit card,
location information, username, password, contacts list, zip code, postal code,
log data, registration,, mailing address, phone, limited, first name, last name,
browsing history, gather
Cookies
cookies, web baecons, ip address, track, session, analytical information, flash,
gifs, pixel tags, other technologies
Purpose
ads, use, services, verifying, purposes, improve, site, advertising content,
market research, fraud prevention, marketing, communication, improve
products, identification, authentication, promotions, personalize, provide,
internal operations, advertising
Retention
retention, retain, keep, data, backup, remove, time, longer, discard, close,
long, longer, period
Security
secure, protect, transmit, data, limited, access, safe harbor, unauthorized, ssl,
socket layer, encryption, physical, measures, security, restrict, fraud, detection,
safeguards
Share
analytics, companies, affiliated , organizations, businesses, contractors, share,
divulge, disclose, rent, sell, law, legal, regulation, third, party, transfer,
release, service providers, other companies, marketing purposes, partners, financial
institutions, subsidiaries, disclosure
are more relevant compared to other co-occurring features such as {may be}
{occasionally we} {may website} {likely arrange}. Similarly, the sentence, “In
some cases, we may share your personal information with third parties”,
is more ambiguous than “We will share your personal information with the
third parties listed below”. The window size for calculating the frequencies for
the co-occurrence matrix was chosen by experimentation. The average sentence
length of the privacy policy corpus was calculated (22 words per sentence) and
54
different window sizes were tested [5, 10, 15, 20] and the final length of the
sliding window was set to 5 based on the performance accuracy of the classifiers
trained to measure the unambiguity.
• Goal Question 3: Is the context of the section ambiguous? Compared to a
machine, humans have the ability to distinguish between ambiguous and unam-
biguous sentences more accurately. Therefore, a section may be annotated as
ambiguous or unambiguous based on its context.
A set of ambiguous words and phrases often found in privacy policies are ex-
tracted by some of the previous research [49, 54]. Therefore, the three goal questions
described above along with the survey responses and ambiguous words and phrases
were used as evaluation criteria to annotate the sections of the privacy policies based
on their unambiguity.
5.3 Privacy Policy Corpus
Since this is one of the first studies that examined the transparency of privacy
policies, there is no benchmark corpus of policies available for research. The dataset
used for this research included 100 privacy policies selected from the top 6 most
visited websites in 15 website categories (90 privacy policies) from Alexa.com [9].
These policies were collected from a wide variety of applications including travel,
news, sports, social media, science, financial, health, and many other categories.
In addition, ten European privacy policies (barclays, british airways, eurail, tesco,
marksandspencer, bet365, paddypower, rbs6nations, uefa, boden) were also chosen
as they follow rigid privacy rules and regulations [17]. This privacy policy corpus was
improved (some privacy policies were added and removed) to enhance the positive and
negative class balance for classification and we have also included some gold standard
privacy policies [12]. After making these changes to the corpus, we ensure that the
total number of privacy policies stayed at 100.
55
5.4 Data Preprocessing
We used a 3-fold cross validation technique to separate the training set and
the test set to avoid the reuse of training set examples while testing. Therefore,
the dataset was split accordingly and the privacy policies in the training set were
preprocessed before training the classifiers.
The privacy policy dataset was preprocessed to derive a set of features for com-
pleteness and unambiguity. In both sets of experiments involving the evaluation of
completeness and unambiguity, the dataset was preprocessed and the feature set was
extracted using the following steps:
• Tokenization: The privacy policies in the training set were tokenized.
• Stop word removal: All the auxiliary words and commonly used English words
were removed. We have also added the organization names, numbers and days
of the week which did not contribute towards the feature space used for training
the classifiers.
• Stemming: In order to reduce the total number of features, stemming method
was used to group the words that have the same stem.
• Vectorization: Finally the tokens were vectorized and put in a vector form to
make it easier for the classifiers to train. Two different vectorization algorithms
were tested - TF-IDF and count. TF-IDF vectorizer showed more promising
results compared to the count vectorizer for both completeness and unambigu-
ity experiments. Therefore, TF-IDF vectorizer was chosen for vectorization of
tokens.
In this process, 1-gram, 2-gram and 3-gram features were also tested and the n-
grams resulting in more promising results were chosen. Therefore, the final feature
space consisted of 4000 bigram features for completeness and 5000 bigram features
for unambiguity. As the total number of features was in the order of thousands, we
attempted to use Chi-squared dimensionality reduction method to reduce the number
56
of features. We have also developed our own feature filtering algorithms to filter the
features and select the features that were more relevant for training the classifiers.
The feature selection process and the different features sets used for training the
classifiers is described in the next section.
5.5 Feature Space Selection
Classification of the privacy policies based on completeness criteria involved 8
binary classification tasks for each of the 8 important sections in a privacy policy. We
examined two different approaches to evaluate the completeness of privacy policies:
• Unfiltered Approach: all of the privacy policies were preprocessed (i.e., tokeniza-
tion, stop word removal, and vectorization) and the resulting features were used
to train the classifiers.
• Keyword-based Filtering Approach: all of the privacy policies were preprocessed
and the resulting features were filtered further based on the important keywords
identified for each section. Table 5.2 summarizes a list of all the keywords iden-
tified for each section. These keywords were identified using: 1) goal keywords
extracted using LDA [13], 2) survey results, and 3) privacy protection acts and
principles.
Initially, for the evaluation of completeness, the privacy policies were prepro-
cessed (tokenization, stop word removal, stemming and vectorized) and the final fea-
ture space consisted of 4000 bigram features. In the Unfiltered Approach, all 4000 fea-
tures were utilized to train the classifiers. On the other hand, for the keyword-based
filtering approach, these 4000 features were further filtered based on the keywords
identified for each section. Filtering the keywords resulted in reducing the feature
space to approximately half of the original size (˜ 2000 features). Refer to Algorithm
1 for the steps used for filtering the features. This approach resulted in 8 different
feature sets, each containing bigrams of the keywords related to a specific section. For
each of these approaches, classifiers were trained and tested using two feature sets:
57
a baseline feature set which uses all of the features, and a Feature selection (Chi)
feature set, using the top 100 best features that were selected by the Chi-squared
feature selection method. Appendix B.1 shows the prominent features keywords used
for completeness evaluation.
Similar to the methodology used for evaluating completeness, two approaches
were tested for evaluating unambiguity of privacy policies: an Unfiltered Approach
and a Co-occurrence Based Filtering Approach. These approaches differ in the feature
sets that they used for training the classifiers.
• Unfiltered Approach: all of the privacy policies were preprocessed (i.e., tokeniza-
tion, stop word removal, and vectorization) and the resulting features were used
to train the classifiers.
• Co-occurrence Based Filtering Approach: all of the privacy policies were pre-
processed and the resulting features were filtered further based on the word co-
occurrence between ambiguous words and keywords. Our approach was that an
ambiguous word co-occurring with a keyword (i.e., an important word) makes
the sentence more ambiguous and causes more concern to the reader when
compared to the case where an ambiguous word co-occurs with a normal non-
keyword. Using the above approach, the frequencies computed by the word
co-occurrence matrix between the ambiguous word and a keyword were used to
filter the feature space in order to keep the most discriminative features while
training the classifiers.
Initially, the privacy policies were preprocessed (tokenization, stop word removal,
stemming and vectorized) and the final feature space consisted of 5000 bigram fea-
tures. In the Unfiltered Approach, all 5000 features were utilized to train the clas-
sifiers. On the other hand, for the Co-occurrence Based Filtering approach, based
on the ambiguous word and keyword co-occurrence frequency the 5000 features were
further filtered for each section. Therefore, out of the 5000 features, the features
containing a combination of ambiguous word and a keyword with frequency >0 were
selected. Table 5.3 shows a small example of the word co-occurrence matrix. Refer
58
Algorithm 1 Algorithm for feature space selection for Completeness
1: procedure Completeness
2: feature-set1 = preprocess(d, (2,2)) . bigram features
3: feature-set2 = Chi(feature-set1, 100)
4: feature-set3 = KeywordFiltering(d)
5: feature-set4 = Chi(feature-set3, 100)
6: classifiers = [SVM LR RF MNB KNN]
7: featuresets = [feature-set1 feature-set2 feature-set3 feature-set4]
8: for each classifier clf in classifiers do
9: for each featureset feat in featuresets do
10: train(clf, feat)
11: end for
12: end for
13: end procedure
14:
15: procedure KeywordFiltering
16: for each policy p in dataset d do
17: feat = preprocess(d,(2,2)) . feat: bigram features
18: for each f in feat do
19: if f contains a keyword k then
20: features = features.append(f)
21: end if
22: end for
23: end for
24: return features
25: end procedure
59
to Algorithm 2 for the steps used for filtering the features. Filtering the features
resulted in reducing the feature space to approximately 1000 features. This approach
resulted in 8 different feature sets, each containing bigram features related to a specific
section. For each of these approaches, classifiers were trained and tested using two
feature sets: Baseline feature set, using all the features and Feature selection (Chi)
feature set, using top 100 best features that were selected by the Chi-squared feature
selection method. Appendix B.2. shows the top 50 features used for unambiguity
evaluation.
Table 5.3: An example of word co-occurrence matrix showing a small subset words
from the privacy policies
may certain share collect perhaps works
may 0 1 365 466 0 0
certain 1 0 24 57 0 3
share 365 24 0 0 1 0
collect 466 57 0 0 6 0
perhaps 0 0 1 6 0 7
works 0 3 0 0 7 0
Therefore, four features spaces were tested for both completeness and unambi-
guity classification task:
• Feature Set 1: Unfiltered feature space (baseline) - TF-IDF features resulting
from preprocessing steps.
• Feature Set 2: Unfiltered feature space (Chi-squared) - top 100 features
resulting from applying the Chi-squared feature selection method.
• Feature Set 3: Filtered feature space (baseline) - features resulting from fil-
tering the initial set of features based on keywords (for completeness), word
co-occurrence frequencies between ambiguous words and keywords (for unam-
biguity).
60
Algorithm 2 Algorithm for feature space selection for Unambiguity
1: procedure Unambiguity
2: feature-set1 = preprocess(d, (2,2)) . bigram features
3: feature-set2 = Chi(feature-set1, 100)
4: feature-set3 = WordCo-occurrenceFiltering(d)
5: feature-set4 = Chi(feature-set3, 100)
6: classifiers = [SVM LR RF MNB KNN]
7: featuresets = [feature-set1 feature-set2 feature-set3 feature-set4]
8: for each classifier clf in classifiers do
9: for each featureset feat in featuresets do
10: train(clf, feat)
11: end for
12: end for
13: end procedure
14:
15: procedure WordCo-occurrenceFiltering
16: for each policy p in dataset d do
17: feat = preprocess(s,(2,2)) . feat: bigram features
18: for each f in feat do
19: . co-occurrence window size = 5
20: if f contains a keyword k AND f contains an ambword amb AND word-
co-occurrence-matrix(k, amb) >0 then
21: features = features.append(f)
22: end if
23: end for
24: end for
25: return features
26: end procedure
61
• Feature Set 4: Filtered feature space (Chi-squared) - top 100 features resulting
from applying Chi-squared feature selection method to the above feature set.
62
5.6 Classifier Parameter Tuning
The Python scikit-learn [48] environment is used for experiments for evaluating
completeness and unambiguity. It is a Python based environment that provides simple
and efficient tools for data mining and data analysis. It provides a range of supervised
and unsupervised learning algorithms via a consistent interface in Python. The main
reason for choosing Python scikit-learn is because it is robust, easy to use, clean API,
well documented, and has large developer community and forums. Moreover, Python
is arguably one of the easier languages to learn and use compared to other languages
[48].
Hyper Parameters of the SVM classifier:
The main goal of SVM, as discussed in Section 3.6, is to find a hyperplane h in
the training set that best discriminates between the classes in the training data. This
is done by maximizing the margin between h and the closest class points, known as
support vectors. Once this hyperplane is found, new objects are classified based on
which side of the hyperplane they fall on. The details of the SVM function can be
found in scikit-learn [48].
Linear kernel is chosen for the SVM as it has been shown to give a good general
performance [8]. The penalty parameter C which defines the cost of misclassification
is set to 1 and 5 for completeness and unambiguity, respectively, through experimen-
tation after trying different values for this parameter. The class weights parameter is
set to auto to avoid the issue with unbalanced classes, which uses the values of y to
automatically adjust weights inversely proportional to class frequencies.
Hyper Parameters of the RF classifier: As discussed in Section 3.6 random
forest is an ensemble classifier which involves an ensemble of multiple decision trees.
The predictions of all the decision trees is combined to make the final decision. The
details of the RF function can be found in scikit-learn [48]. The total number of
estimators, which is the total number of trees in the forest parameter is set to 10 and
20 for completeness and unambiguity after looking at the performance of the classifier
using different values through experimentation.
63
Hyper Parameters of the KNN classifier: The KNN classifier, as discussed
in Section 3.6, classifies a new instance by gathering the k nearest training records to
the new instance, based on a given distance function, and assigning the new instance
the majority class out of the k training records. In this case we used a standard KNN
based on Euclidean distance. Details of the KNN function syntax can be found in the
scikit-learn [48]. The value of k for this training set is set to 5 for both completeness
and unambiguity by experimentation. Other parameters include weight function,
distance metric and power parameter p for the distance metric. These parameters
are set to uniform, Minkowski and 2 for both completeness and unambiguity. The
weight parameter sets the weight function used in prediction. All points in each
neighbourhood are weighted equally when this parameter is set to uniform. Minkowski
is the distance metric used for the tree with power parameter p=2 for the Euclidean
distance.
Hyper Parameters of the MNB classifier: As discussed in Section 3.6,
MNB classifier is a variant of NB classifier which is a probabilistic classifier based
on the Bayes theorem. This takes multiple occurrence of a word into account while
computing the probabilities. The details of the MNB function can be found in scikit-
learn [48]. The Laplace smoothing parameter alpha is set to 0.001 and 0.1 for
completeness and unambiguity, respectively, by experimentation.
Hyper Parameters of the LR classifier: As discussed in Section 3.6, LR is a
type of regression that predicts the probability of occurrence of an event by fitting the
training data to a logistic function. The details of LR function syntax can be found
in scikit-learn [48]. The penalty parameter is set to L1 and the inverse regularization
strength C is set to 0.1 and 0.01 for completeness and unambiguity respectively by
experimentation.
5.7 Summary
In this chapter, the text classification problem for evaluating completeness and
unambiguity is discussed. The classification problem has been divided into 8 binary
64
classification problems. For each of completeness and unambiguity different stages of
knowledge discovery involving dataset creation, evaluation criteria for annotations,
parameter tuning, feature space and experimental approaches are described. The
privacy policy corpus includes 100 privacy policies derived from the most visited
websites. A survey was conducted to help us understand user perspective and to
annotate the privacy policy sections. In addition, for completeness a goal-based
evaluation approach was proposed along with a set of keywords related to each of the
8 sections for annotating the policies for their completeness. The survey responses
along with a set of ambiguous words and goal questions were used for annotating
the privacy policies for their unambiguity. The policies were preprocessed and a final
set of features were extracted. The classifier hyper parameters for both completeness
and unambiguity were tuned by experimentation. Finally, two different approaches
were proposed for comparison: Unfiltered approach and Filtered approach (based
on keyword for completeness and word co-occurrence for unambiguity). For each of
these approaches two sets of features were used for training the classifiers: baseline
(all features) and Chi-squared (top 100 features) features.
65
Chapter 6
Results and Analysis
In this chapter we discuss the results and analysis for readability, completeness
and unambiguity of privacy policies. In particular, Sections 6.1, 6.2 and 6.3 describe
the results obtained for readability, completeness and unambiguity, respectively. Sec-
tion 6.4 provides a brief discussion of the results. Section 6.5 provides details about
the scoring scheme and the final results obtained by the transparency evaluation
system.
6.1 Style Factor: Readability
Based on the following readability scale, the mean of two readability metrics for
the 100 privacy policies is analyzed. Table 6.1 and Table 6.2 show the breakdown of
the number of privacy policies that fall under each of these categories based on their
readability ease score, and grade levels using FRES, and FGL, respectively.
Table 6.1: Readability Results using FRES readability formula
Readability Scale # of Privacy Policies
9-10 : Very Easy 0
8-8.9 : Easy 0
7-7.9 : Fairly Easy 1
6-6.9 : Standard 9
5-5.9 : Fairly Difficult 16
3-4.9 : Difficult 51
0-2.9 : Very Confusing 23
66
Table 6.2: Readability Results representing grade levels using FGL readability
formula
Grade Level # of privacy policies
1 - 6 0
7 0
8 1
9 0
10 1
11 7
12 12
13 18
14 22
>= 15 39
As shown in Table 6.1 and 6.2, it can be noted that only 10% of privacy polices
are readable and 90% fall under difficult or very difficult categories. Similarly, 80%
of privacy policies require university level education to comprehend their text.
6.2 Content Factor: Completeness
6.2.1 Unfiltered Approach
We first used the baseline feature set to train the classifiers (Fig. 6.1). The
F-measure results suggest that LSVM and RF outperform KNN, LR and MNB. F-
measures greater than 75% were achieved by all of the classifiers, except for the
sections ‘choice’ (65.42%), and ‘retention’ (67.32%). The lower performance for these
classifiers is due to uneven distribution of positive and negative examples of the data.
Distribution of positive and negative examples was as follows: choice (34% positive
vs. 66% negative) and retention (29% positive vs. 71% negative).
The performance of all the classifiers for most of the sections improved by 1%-
5% using Chi-squared feature selection method (Fig. 6.2) due to the reduction of the
67
total number of features. The classifiers showed improvement for ‘choice’ (67.65%)
and ‘retention’ (69.45%) sections compared to the previous results.
Among the two sets of experiments, the experiments conducted using Chi-squared
feature selection method produced better results compared to baseline features. This
result is expected because the size of the feature space is greatly reduced due to feature
selection method which helped in improving the performance of all the classifiers.
To solve the imbalance issue and to improve the performance of classifiers in
‘choice’ and ‘retention’ sections, Synthetic Minority Over-sampling Technique (SMOTE)
[21, 42] is applied to balance the class distribution for choice and retention sections.
This method improved the performance of LSVM and KNN classifiers by∼ 1%−1.5%.
Another method provided by the scikit-learn library LSVM algorithm was also applied
to improved the classifier performance for these two sections. In order to improve the
performance of ‘choice’ and ‘retention’ sections, for LSVM classifier, the class weights
parameter was set to ‘auto’, which adjusted the weights inversely proportional to
class frequencies in the sample. This method helped in improving the F-measure of
classification model trained on unbalanced data [48]. As a result, the F-measure for
‘choice’ and ‘retention’ increased to 69.67% and 71.56%, respectively.
6.2.2 Keyword-based Filtering Approach
In the second approach, the feature set of 4000 bigrams was filtered using the
keywords identified for each section. The F-measure of all the classifiers improved
by 2%-5% for all 8 sections using filtered feature sets (Fig 6.3). The performance of
LSVM, and RF was the best compared to KNN, MNB and LR for all the 8 sections.
In another attempt to improve the F-measure, the Chi-squared feature selec-
tion method was applied on the filtered feature sets. The results for LSVM and
KNN remained unchanged (Fig. 6.4). However, the RF classifier showed significant
improvement (up to 13%) and also outperformed the other classifiers in all the sec-
tions. This classifier showed notable improvement for the ‘retention’ (∼ 13%) section
compared to the baseline results.
Based on these results, it can be concluded that the experiments conducted using
68
the Chi-squared feature selection method showed better generalization ability than
the ones trained with just the baseline feature set. The classification accuracy for
the ‘choice’ (75.78%), and ‘retention’ (73.82%) showed notable improvement in com-
parison with the unfiltered approach for RF classifier. The RF and LSVM classifiers
outperformed other classifiers in a majority of sections using the Chi feature selec-
tion method. Though the RF (74%) performs better than LSVM (72.5%) for the
‘retention’ section, there was no significant difference between the performances of
both of these classifiers after taking their standard deviations into consideration for
10 iterations. Overall, the performance of the classifiers using filtered feature sets
(both baseline and Chi-squared) was better than the unfiltered feature sets for most
of the sections.
Figure 6.1: Completeness: Average F1-Scores using Baseline Features - Unfiltered
69
Figure 6.2: Completeness: Average F1-Scores using Chi-Squared Features - Unfiltered
Figure 6.3: Completeness: Average F1-Scores using Baseline Features - Filtered
70
Figure 6.4: Completeness: Average F1-Scores using Chi-Squared Features - Filtered
6.3 Unambiguity
6.3.1 Unfiltered Approach
Two feature sets were tested for the unfiltered approach - baseline (all features -
5000 TF-IDF features) and Chi-squared top 100 features. For the initial experiments
with baseline feature sets it was observed that MNB and RF outperformed the remain-
ing classifiers in terms of their F-measure results. Except for ‘access (61%)’, ‘choice
(67%)’ and ‘share (67%)’ sections all the remaining sections achieved F-measures
≥ 70% (Fig 6.5)
Using the feature selection method increased the F-measures of these three sec-
tions ‘access (64%)’, ‘choice (70%)’ and ‘share (73%)’ while the accuracy results of
the other sections remained unchanged. In this case, the RF classifier outperformed
all the other classifiers in terms of F-measure (Fig 6.6).
Overall, in the unfiltered approach the feature set used after feature selection
71
helped in producing better results compared to the baseline feature set. It should
also be noted that F-measures for all the classifiers and for all the sections were in
the range 70%-80%. The main reason for the low F-measure was due to the presence
of some irrelevant features in the feature set that hampered the performance of the
classifiers. In order to solve this issue and to add more discriminative features to the
feature set an improved feature filtering method was applied and is discussed in the
next section.
6.3.2 Co-occurrence-based Filtering Approach
As proposed by Figueiredo et al. in the work [22], in a similar context, it
should be noted that the occurrence of an individual ambiguous word may not provide
good discriminative features for classifying a privacy policy section as ambiguous
or unambiguous. Therefore, a similar approach as shown in [22] was applied to
extract the features for measuring the unambiguity of the privacy policy. In this case,
the co-occurrence of keywords (Table 5.1) and ambiguous words were considered to
extract the features. Therefore, the TF-IDF features obtained after preprocessing
the privacy policy text were further filtered based on the features resulting from word
co-occurrence of ambiguous words and keywords.
Filtering the features based on the co-occurrence frequency between ambiguous
words and keywords showed a significant amount of improvement (∼ 3% − 17%)
in classification accuracies and F-measures for all the sections. Using the baseline
feature set, all the classifiers achieved classification accuracy ≥ 75%. The accuracies
of ‘access (76%)’, ‘choice (76%)’ and ‘cookies (76%)’ was particularly low compared
to other sections. Except for KNN all the remaining classifiers showed significant
improvement in F-measures (range 76% - 88%) (Fig 6.7).
After using the feature selection method the accuracies of these three sections
‘access (79%)’, ‘choice (79%)’ and ‘cookies (79%)’ (Fig 6.8) increased while the accu-
racies of other sections showed slight improvement (3%-5%) or remained unchanged.
Overall, the RF and LR classifiers produced the best F-measure results compared to
other three classifiers for majority of sections (Table 6.3). Both of these classifiers
72
performed significantly well and achieved high F-measures for ‘collect’, ‘purpose’ and
‘retention’ categories compared to the remaining classifiers. As there was no signifi-
cant difference between the results of both of these classifiers after considering their
standard deviation over 10 iterations, it may be concluded that both the classifiers are
suitable for solving this classification problem. Also, there was a significant amount
of improvement in the performance of the classifiers using this co-occurrence based
filtering approach when compared to the unfiltered approach.
Figure 6.5: Unambiguity: Average F1-Scores using Baseline Features - Unfiltered
6.4 Discussion
The results for the readability evaluation show that only 10 privacy policies out
of the 100 were readable, while all the others fell under ‘difficult’ or ‘fairly difficult’
to read categories. This shows that in general, privacy policies are difficult to read
and understand and need a certain level of education to comprehend them as they
contain lengthy sentences and difficult words.
73
Figure 6.6: Unambiguity: Average F1-Scores using Chi-Squared Features - Unfiltered
Figure 6.7: Unambiguity: Average F1-Scores using Baseline Features - Filtered
74
Figure 6.8: Unambiguity: Average F1-Scores using Chi-Squared Features - Filtered
For completeness, we observed that the LSVM and RF classifiers illustrate supe-
rior performances for the majority of sections (≥ 5 sections) compared to the remain-
ing classifiers. To understand the impact of the keywords features and increase the
performance of the ‘retention’ classifiers, we manually compared the 100 features that
resulted from the Chi feature selection method for both the approaches. We found
that there was about 65% overlap between the features. However, for the unfiltered
feature space, the Chi feature selection method resulted in a significant number of
irrelevant features. As a result, the F-measure for the ‘retention’ section using the RF
classifier for the unfiltered approach was 59.78% while the F-measure for keyword-
based filtering approach increased to 73.82%. This shows that the keywords that
we have identified for each section were appropriate for distinguishing complete and
incomplete categories. We plan to validate this finding and the set of keywords with
more privacy policies in future.
As discussed earlier, performances of all the classifiers for the sections ‘choice’,
and ‘retention’ were low due to class imbalance. To solve this issue two approaches
75
were tested - oversampling using SMOTE and adjusting the weights inversely pro-
portional to class frequencies in the sample by setting the class weights parameter to
‘auto’ for LSVM classifier. The former method improved the results slightly (∼ 1% for
choice and retention sections). The latter method helped in improving the F-measure
of classification model trained on unbalanced data [48]. Thus, the F-measure for
‘choice’ and ‘retention’ increased from 67.65% to 69.67% and from 69.45% to 71.56%
respectively.
For unambiguity, it was observed that MNB and RF classifiers achieved better
results for the unfiltered approach, but the classification results for all the sections
were considerably low, below 80%. Therefore, a novel feature filtering approach based
on word co-occurrence frequency was applied. In this approach, the features were fil-
tered based on the co-occurrence frequency between ambiguous words and keywords
identified for each section. This approach improved the performance of all the classi-
fiers significantly and classifiers RF and LR showed a great amount of improvement
and demonstrated superior performance compared to other classifiers for 5 sections.
A t-test was also performed to compare the results of LSVM and RF for com-
pleteness and LR and RF for unambiguity at the 95% confidence interval. From the
t values, it was observed that there was no significant difference between both the
sets of classifiers at the 95% confidence interval. This shows that both LSVM and
RF are robust and produce generalizable results for completeness classification for
the majority of sections. Similarly, for unambiguity classification, both RF and LR
classifiers illustrated superior performances compared to other classifiers for majority
of sections. To highlight the impact of filtering and feature selection methods and to
get a better understanding of how filtering the features based on the keywords and/
or ambiguous words identified for each section helped in improving the performance
of the classifiers we have summarized the results of RF and LSVM for completeness
and RF and LR for unambiguity in Tables 6.3 and 6.4, respectively. The results for
the remaining classifiers for both completeness and unambiguity are included in the
Appendix D.
It is worth noting that we selected the most commonly visited website’s privacy
76
policies for our study. Our results suggest that the service providers are not paying
attention to sections such as ‘choice’, ‘access’, and ‘retention’ in the privacy policies.
We intend to publish a set of recommendations regarding the content of the privacy
policies based on these and future findings.
Table 6.3: Completeness: Average F-measures and Standard Deviations for Linear
SVM and Random Forest Classifiers for 10 Iterations
Section
Linear SVM Random Forest
Baseline Chi Baseline Chi
Unfiltered Filtered Unfiltered Filtered Unfiltered Filtered Unfiltered Filtered
Access 85.35 (0.19) 88.56 (0.45) 88.43 (0.76) 88.96 (0.68) 75.56 (3.36) 77.34 (3.48) 83.22 (3.66) 88.24 (2.49)
Choice 65.42 (1.22) 70.87 (0.78) 67.65 (0.89) 75.67 (0.45) 67.65 (2.47) 67.71 (1.79) 67.66 (2.98) 75.78 (1.57)
Collect 91.21 (0.12) 94.65 (0.29) 94.67 (0.43) 94.67 (0.43) 91.67 (0.76) 94.21 (0.65) 94.13 (0.98) 94.67 (0.55)
Cookies 76.67 (0.98) 79.91 (0.88) 76.98 (0.87) 79.89 (0.64) 76.87 (0.98) 79.67 (0.96) 76.98 (0.98) 79.78 (0.95)
Purpose 83.91 (0.65) 85.53 (0.78) 85.89. (0.78) 85.90 (0.34) 85.75 (0.76) 85.89 (0.67) 85.81 (0.54) 85.97 (0.51)
Retention 67.32 (1.12) 71.76 (0.93) 69.45 (0.66) 71.45 (1.97) 59.78 (3.11) 60.87 (2.79) 65.54 (3.48) 73.82 (3.94)
Security 88.56 (0.76) 88.21 (0.76) 88.78 (0.66) 88.78 (0.47) 82.65 (3.06) 83.76 (2.24) 85.67 (1.70) 89.42 (2.84)
Share 91.67 (0.19) 91.78 (0.23) 91.72 (0.14) 91.76 (0.11) 91.76 (0.17) 91.87 (0.23) 84.67 (0.34) 91.82 (0.55)
Table 6.4: Unambiguity: Average F-measures and Standard Deviations for Random
Forest and Logistic Regression Classifiers for 10 Iterations
Section
Random Forest Logistic Regression (Max Entropy)
Baseline Chi Baseline Chi
Unfiltered Filtered Unfiltered Filtered Unfiltered Filtered Unfiltered Filtered
Access 59.67 (1.09) 72.71 (1.09) 59.67 (2.01) 77.43 (1.23) 54.54 (1.32) 72.72 (1.33) 56.67 (1.09) 78.32 (1.23)
Choice 63.63 (2.21) 72.71 (2.20) 69.68 (1.32) 75.74 (2.11) 60.61 (2.32) 69.63 (2.31) 66.67 (1.98) 75.75 (2.34)
Collect 69.69 (1.88) 75.76 (1.98) 72.72 (5.54) 84.45 (2.43) 69.69 (3.67) 78.79 (2.54) 69.65 (2.65) 85.85 (2.20)
Cookies 72.73 (3.45) 75.76 (3.22) 75.76 (3.43) 78.77 (1.37) 60.61 (2.43) 72.72 (1.99) 63.61 (2.54) 78.78 (2.01)
Purpose 69.69 (4.13) 78.78 (3.32) 69.65 (2.56) 81.83 (1.65) 60.62 (2.22) 75.74 (2.89) 63.62 (2.65) 82.82 (1.76)
Retention 69.69 (3.22) 80.81 (2.45) 69.65 (1.34) 86.64 (2.32) 60.61 (1,89) 87.85 (1.45) 63.65 (1.76) 87.87 (1.07)
Security 75.75 (2.54) 78.77 (1.76) 78.79 (4.54) 77.83 (1.56) 69.68 (1.69) 75.73 (1.89) 72.72 (1.71) 78.78 (1.17)
Share 66.67 (2.45) 78.75 (2.11) 72.72 (2.44) 78.77 (2.33) 60.62 (1.77) 72.72 (1.65) 66.67 (1.67) 77.79 (1.57)
77
6.5 Transparency Scoring System
As described in Chapter 3, two formulas were selected to evaluate the readability
of privacy policies in the corpus: FRES, FGL. After computing the two scores in the
range 1-10 for the two readability formulas, the cumulative score for readability was
calculated using Eq. 6.1:
R = 100 ∗
[
FRES + FGL
2
]
(6.1)
where the constant 100 was used to scale the result to [0-100].
In order to propose a scoring scheme for completeness and unambiguity, one of
the important decisions to make is to derive a weighting scheme for the eight sections
in the privacy policy. A transparency score can be calculated based on the importance
(i.e., weights) of a section and its completeness and unambiguity results obtained from
the classifier.
A survey was conducted to help us validate the completeness and unambiguity
scores and to assign weights to the individual sections based on their importance. It
contained 20 questions related to the completeness and unambiguity of the 8 sections
of a privacy policy. The participants were asked to rate the completeness and unam-
biguity of each of the eight sections of a given privacy policy. They were also asked
to rank each section based on their privacy preferences and give an overall score for
completeness and unambiguity. This survey was taken by 18 technical participants
and 4 subject matter experts and 22 privacy policies were selected from the corpus.
Each participant was assigned two privacy policy surveys and each privacy policy
survey was taken by two individuals (Refer to Appendix C.2. for the survey). The
survey responses were used to validate the final scores evaluated by the system and
also to determine the weights for each section.
Table 6.5 shows the section ranking results obtained from the technical partici-
pants of the survey and the subject matter experts. The final ranking of the sections
was determined by combining the rankings assigned to both groups of participants.
According to the rankings, weights that add up to 100% were assigned to each section
78
Table 6.5: Section Ranking by Technical participants and Subject Matter Experts
Technical Participants Subject Matter Experts Final Ranking
Collect (1) Purpose (1) Purpose (1+2=3)
Purpose (2) Choice (2) Collect (1+3=4)
Share (3) Collect (3) Choice (2+4=6)
Choice (4) Access (4) Share (7+3=10)
Security (5) Security (5) Access (6+4=10)
Access (6) Cookies (6) Security (5+5=10)
Cookies (7) Share (7) Cookies (6+7=13)
Retention (8) Retention (8) Retention (8+8=16)
(Refer to Table 6.6).
Based on the predictions of the classifiers, we provide a completeness score to
help users better evaluate the overall completeness of a privacy policy. The score was
calculated by combining classifiers’ output with some predefined weights (refer to Eq.
6.2).
C = 100 ∗
n∑
i=1
cn ∗ wn (6.2)
where n is the number of privacy policy sections (n = 8), c is classifier output
(between 0-1: 1 - complete 0 - incomplete), w is the weightage assigned to the section
n, and the constant 100 is used to scale the results to [0-100].
Similar to the above approach, the unambiguity score is calculated using the Eq.
6.3:
U = 100 ∗
n∑
i=1
un ∗ wn (6.3)
where n is the number of privacy policy sections (n = 8), u is classifier output
(between 0-1: 1 - unambiguous 0 - ambiguous), w is the weightage assigned to the
section n, the constant 100 is used to scale the results to [0-100].
79
Table 6.6: Section Weightage
Section Weight
Purpose 25%
Collect 20%
Choice 15%
Share 10%
Access 9%
Security 8%
Cookies 7%
Retention 6%
The final transparency score was computed by multiplying the above results for
each criteria by weights assigned to each criteria (based on importance of the criteria)
as shown in Eq. 6.4:
T = wrR + wcC + wuU (6.4)
where R, C and U are the scores for readability, completeness and unambiguity re-
spectively. wr, wc, wu refer to the corresponding weights assigned to readability,
completeness and unambiguity.
Based on the final rankings for the sections, we assigned weights for each section
used in Eq. 6.2 and Eq. 6.3 to compute the completeness and unambiguity scores.
Table 6.6 show the weights for each section.
As discussed in Section 6.1, 90% of privacy policies are difficult to read and
have scores under 40 for their readability. Therefore, we assigned lower weight to
the readability criterion (20%) while the other two factors, completeness and unam-
biguity, were assigned 40% weight towards the final transparency score. The final
transparency scores were calculated using Eq. 6.5. These scores were compared with
the scores computed using the manual annotations and user scores from the surveys
to verify the performance of the system.
80
T = 0.2R + 0.4C + 0.4U (6.5)
The privacy policies were categorized into three categories based on their final
transparency score - least transparent (0 - 33), moderately transparent (33 - 67) and
most transparent (67 - 100). As we included a set of 100 privacy policies in our dataset
and we used 3-fold cross validation, in each iteration the test set contained 33 privacy
policies. For one of the iterations, based on the transparency scores computed using
the manual annotations, 14 privacy policies belonged to least transparent category,
15 belonged to moderately transparent category and 4 belonged to most transparent
category. We calculated the performance accuracies and F-measures for each of the
iterations to check the performance of the entire system. Table 6.7 shows the per-
formance evaluation of the transparency evaluation system that includes the average
F-measure and accuracy, for each of the three categories for 10 iterations using RF
classifier which is among the top two best performing classifiers for both completeness
and unambiguity.
Table 6.7: Performance Results of Transparency Evaluation System
Category Accuracy F-measure
Least Transparent 84.85 81.48
Moderately Transparent 75.76 71.43
Most Transparent 90.91 72.73
From the results, it is evident that the system performed reasonably well in
categorizing the ‘Least transparent’ privacy policies. The systems’ F-measure scores
were above 70% for all the categories. The suggested scoring system is just a proof
of concept approach developed to verify the transparency evaluation framework and
more work needs to be done in this area. The results of the system can be improved
further by adding more policies to the dataset, by refining the feature selection process
and by performing usability tests to improve the scoring system.
81
Chapter 7
Conclusions
7.1 Summary
Research suggests that privacy policies lack transparency because they are long,
difficult to understand, and often contain insufficient information. Therefore, in this
thesis, we propose an automated transparency evaluation system that measures the
level of transparency for a given privacy policy and also provides a transparency score
to help the users make informed decisions about their information privacy. We ana-
lyzed the privacy policies based on two factors, content and style. Furthermore, we
focused on three important contributing criteria of transparency: readability, com-
pleteness and unambiguity. Completeness and unambiguity were evaluated using
machine learning approaches, while readability formulas were used to measure the
readability of privacy policies.
Readability of a privacy policy is measured by taking the average of two scores
computed using two readability formulas: FRES, and FGL. The results show that 90%
of the privacy policies are difficult to read. As a result, the main focus of this research
was to develop a machine learning based approach to measure the completeness and
unambiguity of privacy policies. Due to the lack of existing research and benchmark
annotated corpus, we had to go over different stages of knowledge discovery to analyze
the completeness and unambiguity of privacy policies. This included developing an
evaluation criteria for both completeness and unambiguity, creating an annotated
dataset (for supervised classification), data preprocessing, feature space generation,
and classification.
Our methodology was as follows: 8 important sections that must be present in
a privacy policy were identified using some privacy protection acts and principles
82
(FIPPA, OECD, PIPEDA). Then the goal-based evaluation criteria for evaluating
the completeness and unambiguity of the privacy policy sections were developed.
The privacy policy sections for completeness were annotated with the help of survey
responses and some of the keywords identified by parsing the privacy policy acts, LDA
and based on the word frequencies of 700 privacy policies. Similarly, goal questions,
survey responses and ambiguous words identified in a previous study [49] and their
co-occurrence with the keywords were used as a guideline to annotate the privacy
policy sections for their unambiguity. Finally, the privacy policies were preprocessed
and four feature sets were created and used for the classification task.
• Feature Set 1: Unfiltered feature space (baseline) - TF-IDF features resulting
from the preprocessing steps
• Feature Set 2: Unfiltered feature space (Chi-squared) - 100 features resulting
from applying the Chi-squared feature selection method
• Feature Set 3: Filtered feature space (baseline) - features resulting from fil-
tering the initial set of features based on keywords (for completeness), word
co-occurrence frequencies between ambiguous words and keywords (for unam-
biguity)
• Feature Set 4: Filtered feature space (Chi-squared) - 100 features resulting from
applying Chi-squared feature selection method to the above feature set
A binary classification approach was used for the evaluation of completeness and
unambiguity of each of the eight sections. Therefore, a section was classified as either
‘complete’ or ‘incomplete’ and either ‘unambiguous’ or ‘ambiguous’. For each of the
eight sections, five classifiers that are apt for this text classification problem were
selected - LSVM, KNN, RF, LR, MNB. Each classifier was trained using each of the
four feature sets described above and their F-measures were reported for 10 iterations.
The results for completeness suggest that LSVM and RF classifiers achieved the best
results using the Feature set 3 and 4. Similarly, RF and LR classifiers achieved the
best results using the Feature sets 3 and 4 for unambiguity evaluation. In both
83
the cases, for most of the sections the classifiers achieved an F-measure over 85%.
Classifiers assigned to sections such as ‘choice’, ‘retention’, ‘access’, and ‘cookies’
under perform (accuracy in the range 70% - 80%). One reason could be that the
service providers are not paying attention to discuss issues related to these sections in
their privacy policies or the privacy policies do not include sufficient information for
these sections. A final transparency score was computed by combining the results from
readability, completeness and unambiguity. These scores were compared with the
scores calculated using the manual annotations and user responses from the surveys.
The overall transparency score results show that the system performs reasonably
well in categorizing privacy policies based on their level of transparency and achieves
F-measure scores over 70%.
7.2 Limitations and Future Work
Although this research has reached its goals of proposing an automatic trans-
parency evaluation framework, there are some limitations:
• A small group of fourth year students’ input was used to improve completeness
and unambiguity evaluation criteria. It would have been beneficial to engage
a diverse group of users with different legal and technical levels of expertise to
better understand the perspective of diverse group of users. Due to the time
limitation as well as the scope of this study, we were not able to conduct a more
comprehensive survey.
• The approach proposed in this study is envisioned to only warn the user, through
a score, about the level of transparency (‘most transparent’, ‘moderately trans-
parent’ or ‘least transparent’) of a given privacy policy. Although this scoring
strategy will enable the user to pay more attention to the content of the privacy
policy, it will not provide specific recommendations about the actions that a
user may take to mitigate the potential risks.
• In the future, if the terminologies used in legal writing (or important sections in
84
privacy policy or legal privacy protection acts) changes, the keywords identified
in this research will not be the most effective. Therefore, the list of keywords
need to be validated and updated whenever needed.
• Although the transparency evaluation framework is able to correctly classify a
given privacy policy as ‘most transparent’, ‘moderately transparent’ or ‘least
transparent’ in the majority of cases, it is still possible for a privacy policy
that is least transparent to achieve a high transparency score. This scenario
occurs if a privacy policy contains most of the important keywords but does
not communicate complete information related to each section. Furthermore,
a privacy policy may be ambiguous without using ambiguous words. These
special cases could be avoided using semantic analysis where sufficiently rich
representation of word relations in the text are derived. Although semantic
analysis is known to be computationally expensive [55], this study could benefit
from an in depth analysis of the context/ semantics of the text.
The method explored in this thesis also provides motivation for further evaluation
of the transparency of privacy policies. Even though this thesis presents a novel frame-
work for evaluation of privacy policy transparency, and a benchmark dataset that can
be used by other researchers, there are still some areas of possible improvement which
can be applied in various steps of a transparency evaluation approach.
We identify the following areas as the future direction:
• The privacy policy corpus used in this research is small, containing 100 privacy
policies. A richer dataset could be helpful in getting greater insight about the
features and could also be helpful in achieving higher quality results.
• Since we have developed the evaluation criteria, more privacy policies can be
manually or automatically annotated using semi-supervised classification.
• Further exploration into other contributing criteria of transparency such as
accountability, accuracy, accessibility can be performed for privacy policies.
85
• Involving the subject matter experts to further evaluate the criteria, keywords,
and an in-depth investigation of privacy policy content will enable us to provide
useful recommendations for the policy makers, law firms, auditors and average
Internet users.
• A more concrete transparency scoring system needs to be developed to com-
bine all the results. Engaging more participants through surveys and conduct-
ing usability studies will provide a better understanding of how privacy policy
transparency can be communicated. In addition, Pareto Efficiency can replace
the weighting approach for scoring different sections of the privacy policies.
86
Bibliography
[1] Directive 95/46/ec of the european parliament and of the council. pages 23–31,
1995.
[2] Privacy online: A report to congress. Federal Trade Commission, 1998.
[3] The anti-corruption plain language guide. Transparency International, 2009.
[4] Transparency and privacy clashing paradigms in a web 2.0 words, 2012.
[5] Oecd guidelines on the protection of privacy and transborder flows of personal
data. Organisation for Economic Co-operation, 2013.
[6] Online privacy transparency. Office of Privacy Commissioner of Canada, 2013.
[7] A. Acquisti, I. Adjerid, and L. Brandimarte. Gone in 15 seconds: The limits
of privacy transparency and control. Security Privacy, IEEE, 11(4):72–74, July
2013.
[8] C. Aggarwal and C. Zhai. A survey of text classification algorithms. In C. C.
Aggarwal and C. Zhai, editors, Mining Text Data, pages 163–222. Springer US,
2012.
[9] Alexa. The top ranked sites in each category, 2014.
[10] N. S. Altman. An introduction to kernel and nearest-neighbor nonparametric
regression. American Statistician - AMER STATIST, 46(3):175–185, 1992.
[11] D. B. Annie I. Anton, Julia B. Earp. The lack of clarity in financial privacy
policies and the need for standardization. Technical report, IEEE Security and
Privacy, 2003.
[12] A. I. Anton, J. B. Earp, and A. Reese. Analyzing website privacy requirements
using a privacy goal taxonomy. In Requirements Engineering, 2002. Proceedings.
IEEE Joint International Conference on, pages 23–31, 2002.
[13] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent dirichlet allocation. J. Mach.
Learn. Res., 3:993–1022, Mar. 2003.
[14] L. Breiman. Random forests. Machine Learning, 45(1):5–32, 2001.
[15] C. Brodie, C.-M. Karat, J. Karat, and J. Feng. Usable security and privacy: A
case study of developing privacy management tools. In Proceedings of the 2005
Symposium on Usable Privacy and Security, SOUPS ’05, pages 35–43, New York,
NY, USA, 2005.
87
[16] C. Cortes and V. Vapnik. Support-vector networks. Mach. Learn., 20(3):273–297,
Sept. 1995.
[17] E. Costante, Y. Sun, M. Petković, and J. den Hartog. A machine learning solution
to assess privacy policy completeness: (short paper). In Proceedings of the 2012
ACM Workshop on Privacy in the Electronic Society, WPES ’12, pages 91–96,
New York, NY, USA, 2012.
[18] L. Cranor. Web privacy with P3P. ” O’Reilly Media, Inc.”, 2002.
[19] L. F. Cranor, P. Guduru, and M. Arjula. User interfaces for privacy agents.
ACM Trans. Comput.-Hum. Interact., 13(2):135–178, June 2006.
[20] Y. Ding, X. Meng, G. Chai, and Y. Tang. User identification for instant messages.
In B.-L. Lu, L. Zhang, and J. Kwok, editors, Neural Information Processing,
volume 7064 of Lecture Notes in Computer Science, pages 113–120. Springer
Berlin Heidelberg, 2011.
[21] S. M. A. Elrahman, S. M. A. Elrahman, and A. Abraham. A review of class
imbalance problem. Journal of Network and Innovative Computing, 1(ISSN 2160-
2174):332–340, 2013.
[22] F. Figueiredo, L. Rocha, T. Couto, T. Salles, M. A. Gonçalves, and W. Meira Jr.
Word co-occurrence features for text classification. Inf. Syst., 36(5):843–858, July
2011.
[23] G. Forman. An extensive empirical study of feature selection metrics for text
classification. J. Mach. Learn. Res., 3:1289–1305, Mar. 2003.
[24] E. Fry. A readability formula that saves time. Journal of Reading, 11(7):pp.
513–516, 575–578, 1968.
[25] X. Geng, T.-Y. Liu, T. Qin, and H. Li. Feature selection for ranking. In Proceed-
ings of the 30th Annual International ACM SIGIR Conference on Research and
Development in Information Retrieval, SIGIR ’07, pages 407–414, New York,
NY, USA, 2007. ACM.
[26] A. Genkin, A. Genkin, D. D. Lewis, and D. Madigan. Large-scale bayesian
logistic regression for text categorization. TECHNOMETRICS, page 2007.
[27] W. H. Greene. Econometric analysis, volume 7. Pearson Education, 2012.
[28] E.-H. Han, G. Karypis, and V. Kumar. Text categorization using weight ad-
justed k-nearest neighbor classification. In Proceedings of the 5th Pacific-Asia
Conference on Knowledge Discovery and Data Mining, PAKDD ’01, pages 53–65,
London, UK, UK, 2001. Springer-Verlag.
[29] J. Han, M. Kamber, and J. Pei. Data mining: concepts and techniques. Elsevier,
2011.
88
[30] H. Hedbom. A survey on transparency tools for enhancing privacy. In V. Matyáš,
S. Fischer-Hübner, D. Cvrček, and P. Švenda, editors, The Future of Identity
in the Information Society, volume 298 of IFIP Advances in Information and
Communication Technology, pages 67–82. Springer Berlin Heidelberg, 2009.
[31] H. Hung and Y. Wong. Information transparency and digital privacy protection:
are they mutually exclusive in the provision of eservices? Journal of Services
Marketing, 23(3):154–164, 2009.
[32] M. Ikonomakis, M. Ikonomakis, S. Kotsiantis, and V. Tampakas. Text classifi-
cation using machine learning techniques. WSEAS Transactions on Computers,
4(8):966–974, August 2005.
[33] M. Janic, J. P. Wijbenga, and T. Veugen. Transparency enhancing tools (tets):
An overview. In Proceedings of the 2013 Third Workshop on Socio-Technical
Aspects in Security and Trust, STAST ’13, pages 18–25, Washington, DC, USA,
2013.
[34] C. Jensen and C. Potts. Privacy policies as decision-making tools: An evaluation
of online privacy notices. In Proceedings of the SIGCHI Conference on Human
Factors in Computing Systems, CHI ’04, pages 471–478, New York, NY, USA,
2004.
[35] T. Joachims. Text categorization with suport vector machines: Learning
with many relevant features. In Proceedings of the 10th European Confer-
ence on Machine Learning, ECML ’98, pages 137–142, London, UK, UK, 1998.
Springer-Verlag.
[36] Z. K. Information transparency of business-to-business electronic markets: A
game-theoretic analysis. Management Science, 50(5):670–685, 2004.
[37] S. M. Kamruzzaman, F. Haider, and A. R. Hasan. Text classification using data
mining. 09 2010.
[38] P. G. Kelley, J. Bresee, L. F. Cranor, and R. W. Reeder. A ”nutrition label” for
privacy. In Proceedings of the 5th Symposium on Usable Privacy and Security,
SOUPS ’09, pages 4:1–4:12, New York, NY, USA, 2009.
[39] P. G. Kelley, L. Cesca, J. Bresee, and L. F. Cranor. Standardizing privacy
notices: An online study of the nutrition label approach. In Proceedings of the
SIGCHI Conference on Human Factors in Computing Systems, CHI ’10, pages
1573–1582, New York, NY, USA, 2010.
[40] F. Lancia. Word co-occurrence and theory of meaning, 2005.
[41] B. Li, S. Yu, and Q. Lu. An improved k-nearest neighbor algorithm for text
categorization. arXiv preprint cs/0306099, 2003.
[42] R. Longadge and S. Dongre. Class imbalance problem in data mining review.
arXiv preprint arXiv:1305.1707, 2013.
89
[43] A. Massey, J. Eisenstein, A. Anton, and P. Swire. Automated text mining
for requirements analysis of policy documents. In Requirements Engineering
Conference (RE), 2013 21st IEEE International, pages 4–13, July 2013.
[44] A. McCallum, A. McCallum, and K. Nigam. A comparison of event models for
naive bayes text classification. 1998.
[45] A. M. McDonald and L. F. Cranor. Cost of reading privacy policies, the. I/S:
A Journal of Law and Policy for the Information Society, 4:543, 2008.
[46] S. Momtazi, S. Khudanpur, and D. Klakow. A comparative study of word
co-occurrence for term clustering in language model-based sentence retrieval.
In Human Language Technologies: The 2010 Annual Conference of the North
American Chapter of the Association for Computational Linguistics, HLT ’10,
pages 325–328, Stroudsburg, PA, USA, 2010. Association for Computational
Linguistics.
[47] C. Parliament. Personal information protection and electronic documents act.
Consolidated Acts, SC 2000, c, 5:13, 2000.
[48] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel,
M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos,
D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine
learning in Python. Journal of Machine Learning Research, 12:2825–2830, 2011.
[49] I. Pollach. What’s wrong with online privacy policies? Commun. ACM,
50(9):103–108, Sept. 2007.
[50] A. Prechal and M. De Leeuw. Dimensions of transparency: The building blocks
for a new legal principle? 0:51–61, 2007.
[51] J. D. M. Rennie, J. D. M. Rennie, L. Shih, J. Teevan, and D. R. Karger. Tack-
ling the poor assumptions of naive bayes text classifiers. IN Proceedings of the
twentieth international conference on machine learning, pages 616–623, 2003.
[52] M. E. L. S. Prechal. General Principles of EC Law in a Process of Development.
Austin, TX: Wolters Kluwer, 2008.
[53] E. Stamatatos, G. Kokkinakis, and N. Fakotakis. Automatic text categorization
in terms of genre and author. Comput. Linguist., 26(4):471–495, Dec. 2000.
[54] J. W. Stamey and R. A. Rossi. Automatically identifying relations in privacy
policies. In Proceedings of the 27th ACM International Conference on Design of
Communication, SIGDOC ’09, pages 233–238, New York, NY, USA, 2009. ACM.
[55] A.-H. Tan. Text mining: promises and challenges. South East Asia Regional
Computer Confederation, Sigapore, 1999.
90
[56] J. Tang, Z. Meng, X. Nguyen, Q. Mei, and M. Zhang. Understanding the limiting
factors of topic modeling via posterior contraction analysis. In Proceedings of The
31st International Conference on Machine Learning, pages 190–198, 2014.
[57] B. Xu, Y. Ye, and L. Nie. An improved random forest classifier for image classi-
fication. In Information and Automation (ICIA), 2012 International Conference
on, pages 795–800, June 2012.
[58] Y. Yang and J. O. Pedersen. A comparative study on feature selection in text
categorization. In Proceedings of the Fourteenth International Conference on
Machine Learning, ICML ’97, pages 412–420, San Francisco, CA, USA, 1997.
Morgan Kaufmann Publishers Inc.
[59] S. Zimmeck and S. M. Bellovin. Privee: An architecture for automatically ana-
lyzing web privacy policies. In Proceedings of the 23rd USENIX Conference on
Security Symposium, SEC’14, pages 1–16, Berkeley, CA, USA, 2014.
91
Appendix A
Privacy Policy Acts and Dataset
A.1 Privacy Protection Acts and Principles
Table A.1. shows the description from the privacy protection acts and principles
for the corresponding privacy policy sections and highlights some of the keywords
selected for each of the privacy policy sections.
92
Table A.1: Privacy Protections Acts and Principles- OECD, FTC FIPP
and PIPEDA
Category Privacy Legislation
Access FIPP “Access, refers to an individuals ability both to access data about him or herself
i.e., to view the data in an entitys files and to contest that datas accuracy and
completeness. Access is essential to improving the accuracy of data collected,
which benefits both data collectors who rely on such data, and consumers who
might otherwise be harmed by adverse decisions based on incorrect data. It
also makes data collectors accountable to consumers for the information they
collect and maintain about consumers, and enables consumers to confirm that
Web sites are following their stated practices. Web sites would be required to
offer consumers reasonable access to the information a Web site has collected
about them, including a reasonable opportunity to review information and to
correct inaccuracies or delete information.” marketing back to consumers) and
external secondary uses (such as disclosing data to other entities).”
OECD “Access - data subjects should be allowed to access their data and make cor-
rections to any inaccurate data.” marketing back to consumers) and external
secondary uses (such as disclosing data to other entities).”
PIPEDA “Personal information shall be as accurate, complete, and up-to-date as is
necessary for the purposes for which it is to be used. The extent to which
personal information shall be accurate, complete, and up-to-date will depend
upon the use of the information, taking into account the interests of the indi-
vidual. Information shall be sufficiently accurate, complete, and up-to-date to
minimize the possibility that inappropriate information may be used to make
a decision about the individual. An organization shall not routinely update
personal information, unless such a process is necessary to fulfil the purposes
for which the information was collected. Personal information that is used
on an ongoing basis, including information that is disclosed to third parties,
should generally be accurate and up-to-date, unless limits to the requirement
for accuracy are clearly set out.”
93
Choice
FIPP “The Choice principle relates to giving consumers options as to how any per-
sonal information collected from them may be used for purposes beyond those
necessary to complete a contemplated transaction. Under the Choice principle,
data collectors must afford consumers an opportunity to consent to secondary
uses of their personal information, such as the placement of consumers names
on a list for marketing additional products or the transfer of personal infor-
mation to entities other than the data collector. Web sites would be required
to offer consumers choices as to how their personal identifying information is
used beyond the use for which the information was provided (e.g., to consum-
mate a transaction). Such choice would encompass both internal secondary
uses (such as marketing back to consumers) and external secondary uses (such
as disclosing data to other entities).”
OECD “Consent - data should not be disclosed without the data subjects consent .”
Collect
FIPP “Notice principle states that consumers should be given clear and conspicuous
notice of an entitys information practices before any personal information is
collected from them, including: identification of the entity collecting the data,
the uses to which the data will be put, and the recipients of the data; the
nature of the data collected and the means by which it is collected; whether
provision of the requested data is voluntary or required; and the steps taken
by the data collector to ensure the confidentiality, integrity and quality of the
data. Notice, then, requires more than simply making an isolated statement
about a particular information practice. Web sites would be required to pro-
vide consumers clear and conspicuous notice of their information practices,
including what information they collect, how they collect it (e.g., directly or
through non-obvious means such as cookies), how they use it, how they provide
Choice, Access, and Security to consumers, whether they disclose the infor-
mation collected to other entities, and whether other entities are collecting
information through the site.”
OECD “Notice - data subjects should be given notice when their data is being
collected.”
94
PIPEDA “The collection of personal information shall be limited to that which is nec-
essary for the purposes identified by the organization. Information shall be
collected by fair and lawful means. Organizations shall not collect personal
information indiscriminately. Both the amount and the type of information
collected shall be limited to that which is necessary to fulfil the purposes iden-
tified. Organizations shall specify the type of information collected as part
of their information-handling policies and practices, in accordance with the
Openness principle. The requirement that personal information be collected
by fair and lawful means is intended to prevent organizations from collect-
ing information by misleading or deceiving individuals about the purpose for
which information is being collected. This requirement implies that consent
with respect to collection must not be obtained through deception.”
Cookies FIPP “Web sites would be required to provide consumers clear and conspicuous
notice of their information practices, including what information they col-
lect, how they collect it (e.g., directly or through non-obvious means such as
cookies), how they use it, how they provide Choice, Access, and Security to
consumers, whether they disclose the information collected to other entities,
and whether other entities are collecting information through the site.”
Purpose
FIPP “Web sites would be required to provide consumers clear and conspicuous
notice of their information practices, including what information they col-
lect, how they collect it (e.g., directly or through non-obvious means such as
cookies), how they use it, how they provide Choice, Access, and Security to
consumers, whether they disclose the information collected to other entities,
and whether other entities are collecting information through the site.”
95
OECD “The Purpose Specification Principle is closely associated with the two sur-
rounding principles, i.e. the Data Quality Principle and the Use Limitation
Principle. Basically, Paragraph 9 implies that before, and in any case not later
than at the time data collection it should be possible to identify the purposes
for which these data are to be used, and that later changes of purposes should
likewise be specified. Such specification of purposes can be made in a number
of alternative or complementary ways, e.g. by public declarations, information
to data subjects, legislation, administrative decrees, and licences provided by
supervisory bodies. According to Paragraphs 9 and 10, new purposes should
not be introduced arbitrarily; freedom to make changes should imply com-
patibility with the original purposes. Finally, when data no longer serve a
purpose, and if it is practicable, it may be necessary to have them destroyed
(erased) or given an anonymous form. The reason is that control over data
may be lost when data are no longer of interest; this may lead to risks of theft,
unauthorised copying or the like. Data should only be used for the purpose
stated and not for any other purposes.”
96
PIPEDA “The purposes for which personal information is collected shall be identified
by the organization at or before the time the information is collected. The
organization shall document the purposes for which personal information is
collected in order to comply with the Openness principle (Clause 4.8) and the
Individual Access principle. Identifying the purposes for which personal infor-
mation is collected at or before the time of collection allows organizations to
determine the information they need to collect to fulfil these purposes. The
Limiting Collection principle requires an organization to collect only that in-
formation necessary for the purposes that have been identified. The identified
purposes should be specified at or before the time of collection to the individ-
ual from whom the personal information is collected. Depending upon the way
in which the information is collected, this can be done orally or in writing. An
application form, for example, may give notice of the purposes. When personal
information that has been collected is to be used for a purpose not previously
identified, the new purpose shall be identified prior to use. Unless the new
purpose is required by law, the consent of the individual is required before in-
formation can be used for that purpose. For an elaboration on consent, please
refer to the Consent principle. Persons collecting personal information should
be able to explain to individuals the purposes for which the information is
being collected.”
Retention
OECD “Under Directive 2002/58/EC on privacy in electronic communications, data
generated by the use of electronic communications services must in principle
be erased or made anonymous when those data are no longer needed for the
transmission of a communication, except where, and only for so long as, they
are needed for billing purposes, or where the consent of the subscriber or user
has been obtained.”
97
PIPEDA “Personal information shall not be used or disclosed for purposes other than
those for which it was collected, except with the consent of the individual or
as required by law. Personal information shall be retained only as long as
necessary for the fulfilment of those purposes. Organizations using personal
information for a new purpose shall document this purpose. Organizations
should develop guidelines and implement procedures with respect to the re-
tention of personal information. These guidelines should include minimum
and maximum retention periods. Personal information that has been used to
make a decision about an individual shall be retained long enough to allow the
individual access to the information after the decision has been made. An or-
ganization may be subject to legislative requirements with respect to retention
periods. Personal information that is no longer required to fulfil the identi-
fied purposes should be destroyed, erased, or made anonymous. Organizations
shall develop guidelines and implement procedures to govern the destruction
of personal information.”
Security
FIPP “Security, refers to a data collectors obligation to protect personal information
against unauthorized access, use, or disclosure, and against loss or destruction.
Security involves both managerial and technical measures to provide such pro-
tections. Web sites would be required to take reasonable steps to protect the
security of the information they collect from consumers.”
98
OECD “Security and privacy issues are not identical. However, limitations on data
use and disclosure should be reinforced by security safeguards. Such safeguards
include physical measures (locked doors and identification cards, for instance),
organizational measures (such as authority levels with regard to access to data)
and, particularly in computer systems, informational measures (such as enci-
phering and threat monitoring of unusual activities and responses to them).
It should be emphasized that the category of organizational measures includes
obligations for data processing personnel to maintain confidentiality. Para-
graph 11 has a broad coverage. The cases mentioned in the provision are to
some extent overlapping (e.g. access/ disclosure). ”Loss” of data encompasses
such cases as accidental erasure of data, destruction of data storage media
(and thus destruction of data) and theft of data storage media. “Modified”
should be construed to cover unauthorized input of data, and “use” to cover
unauthorized copying. Collected data should be kept secure from any potential
abuses.”
99
PIPEDA “Personal information shall be protected by security safeguards appropriate
to the sensitivity of the information. The security safeguards shall protect
personal information against loss or theft, as well as unauthorized access, dis-
closure, copying, use, or modification. Organizations shall protect personal
information regardless of the format in which it is held. The nature of the
safeguards will vary depending on the sensitivity of the information that has
been collected, the amount, distribution, and format of the information, and
the method of storage. More sensitive information should be safeguarded
by a higher level of protection. The methods of protection should include:
(a) physical measures, for example, locked filing cabinets and restricted ac-
cess to offices; (b) organizational measures, for example, security clearances
and limiting access on a need-to-know basis; and (c) technological measures,
for example, the use of passwords and encryption. Organizations shall make
their employees aware of the importance of maintaining the confidentiality of
personal information. Care shall be used in the disposal or destruction of per-
sonal information, to prevent unauthorized parties from gaining access to the
information.”
Share
FIPP “Web sites would be required to provide consumers clear and conspicuous
notice of their information practices, including what information they col-
lect, how they collect it (e.g., directly or through non-obvious means such as
cookies), how they use it, how they provide Choice, Access, and Security to
consumers, whether they disclose the information collected to other entities,
and whether other entities are collecting information through the site.”
OECD “Data subjects should be informed as to who is collecting their data.”
A.2 Dataset
100
Table A.2: Keywords Identified for each Section from Privacy Protection Acts and
Principles
Section Keywords
Access
access, accurate, accuracy, update, corrections,
inaccurate, up-to-date, view, incorrect
Choice choice, choices, consent
Collect
data, collection, collecting, collected, collector,
personal, information, limited
Cookies cookies
Purpose
purpose, purposes, notice, use, used, identify,
identified
Retention
erased, anonymous, longer, long, retained,
periods, retention
Security
security, protect, protection, loss, measures,
protections, unauthorized, passwords, encryption,
confidentiality, threat, safeguards, physical, enciphering,
destruction
Share entities, disclosed
101
Table A.3: List of Online Privacy Policy Websites
Online Privacy Policy Websites
9gag amazon bleacherreport topshop weather
alibaba google chase boden buzzfeed
bank of america paypal aol footlocker comcast
blizzard ted etsy toshiba github
disney urbandictionary groupon bet365 wikia
ebay webmd huffington paddypower washingtonpost
expedia yahoo ziffdavis rbs6nations victoriasecret
facebook yelp instagram uefa curse
imdb ally myfitnesspal unilever time
netflix booking steampowered barclays thekitchn
nih cnn dropbox britishairways kijiji
microsoft ehow tumblr hm nhl
reddit goal flipkart asos bmo
stackexchange howstuffworks quikr reuters bestbuy
target gap icicibank marksandspencer ask
timeanddate skype jabong dailymotion apple
tripadvisor stumbleupon theladbible taboola kaiserparmanente
twitch dictionary theguardian eurail corus
twitter walmart tesco craigslist pfizer
wordpress weebly dailymail diply novartis
102
Appendix B
Feature Space
B.1 Completeness
Table B.1. shows the top 50 words from the privacy policy corpus based on their
frequency. Figure B.1 shows a pictorial representation of most frequent words in the
form of a word cloud.
B.2 Unambiguity
Table B.2. shows the top 50 word co-occurance frequencies from the privacy
policy corpus.
103
Figure B.1: Word Cloud Representing Frequency of Unigrams
104
Table B.1: Top 50 unigrams based on word frequency
Unigram Frequency Unigram Frequency
personal 1848 business 323
information 1688 request 320
cookies 1318 opt 316
collect 1304 marketing 312
third 1297 send 295
party 1139 mail 286
privacy 1107 protect 283
data 1018 disclose 270
services 953 changes 269
policy 892 computer 256
account 749 preferences 249
access 712 ads 245
address 698 notice 244
advertising 671 location 235
email 647 providers 235
contact 617 partners 224
share 584 technologies 223
mobile 494 keep 201
content 474 store 200
identifiable 462 card 176
security 411 beacons 158
purposes 406 transfer 155
105
Table B.2: Top 50 word co-occurance frequencies
Word Co-occurance Frequency Word Co-occurance Frequency
’may’, ’disclose’ 139 ’may’, ’use’ 888
’may’, ’cookies’ 377 ’may’, ’collect’ 466
’may’, ’choose’ 102 ’may’, ’access’ 187
’may’, ’share’ 365 ’may’, ’parties’ 393
’such as’, ’use’ 183 ’for example’, ’use’ 119
’may’, ’changes’ 90 ’may’, ’opt’ 74
’such as’, ’collect’ 124 ’certain’, ’cookies’ 69
’some’, ’cookies’ 99 ’such as’, ’cookies’ 93
’consent’, ’use’ 88 ’like’, ’use’ 67
’may’, ’security’ 81 ’such as’, ’share’ 57
’such as’, ’parties’ 52 ’may’, ’transfer’ 60
’certain’, ’collect’ 57 ’certain’, ’share’ 24
’such as’, ’changes’ 10 ’reasonable’, ’access’ 13
’certain’, ’choices’ 11 ’authorized’, ’security’ 42
’various’, ’purposes’ 6 ’reasonable’, ’measures’ 17
’good faith’, ’disclose’ 6 ’from time to time’, ’change’ 17
’some’, ’collected’ 19 ’various’, ’use’ 33
’reasonable’, ’use’ 23 ’sometimes’, ’use’ 26
’authorized’, ’use’ 43 ’with your consent’, ’use’ 6
’some’, ’purposes’ 15 ’reasonable’, ’security’ 28
’some’, ’security’ 9 ’may’, ’rent’ 7
’consent’, ’transfer’ 19 ’reserve the right’, ’change’ 17
106
Appendix C
Surveys
C.1 Survey 1
1. Does the privacy policy state what personal information may be collected by
the website?
(a) No - the policy does not talk about personal information collection
(b) Yes - the policy talks about personal information collection (Please copy
and paste the sentences that talk about what personal information is
collected by the website)
2. Do you feel that the information provided about personal information collection
is clear?
(a) Yes - The information provided about personal information collection is
clear (unambiguous)
(b) No - The information provided about personal information collection is
unclear(ambiguous)(Please copy and paste the sentences that you felt are
ambiguous or contradictory)
3. Does the privacy policy state the purpose of collecting personal information?
(a) No - the policy does not state the purpose of collecting personal information
107
(b) Yes - the policy states the purpose of collecting personal information
(Please copy and paste the sentences that talk about the purpose of per-
sonal information collection)
4. Do you feel that the information provided about the purpose of collecting
personal information is clear?
(a) Yes - The information provided about the purpose of collecting personal
information is clear(unambiguous)
(b) No - The information provided about the purpose of collecting personal
information is unclear(ambiguous) (Please copy and paste the sentences
that you felt are ambiguous or contradictory)
5. Does the privacy policy state whether and under what circumstances and with
whom the website will share users personal information?
(a) No - the policy does not address users concerns about personal information
sharing
(b) Yes - the policy addresses users concerns about personal information shar-
ing (Please copy and paste the sentences that talk about sharing personal
information)
6. Do you feel that the information provided about personal information sharing
is clear?
(a) Yes - The information provided about personal information sharing is
clear(unambiguous)
(b) No - The information provided about personal information sharing is un-
clear(ambiguous) (Please copy and paste the sentences that you felt are
108
ambiguous or contradictory)
7. Does the privacy policy provide information about users privacy choices like
opt-in/opt-out’, ’Do Not track’ options towards information collection?
(a) No - the policy does not provide information about users privacy choices
(b) Yes - the policy provides information about users privacy choices (Please
copy and paste the sentences that talk about choice options available to
the user)
8. Do you feel that the information provided about users privacy choice options is
clear?
(a) Yes - The information provided about users privacy choice options is
clear(unambiguous)
(b) No - The information provided about users privacy choice options is un-
clear(ambiguous) (Please copy and paste the sentences that you felt are
ambiguous or contradictory)
9. Does the privacy policy state the techniques used (e.g. cookies, web beacons,
pixel tags, GIFs, IP address tracking etc.) to collect additional information
from the users?
(a) No - the policy does not states the techniques used to collect additional
information from the users
(b) Yes - the policy states the techniques used to collect additional informa-
tion from the users (Please copy and paste the sentences that talk about
techniques used for collecting information from the users)
109
10. Does the privacy policy state for how long the personal information is retained?
(a) No - the policy does not state for how long the personal information is
retained
(b) Yes - the policy states for how long the personal information is retained
(Please copy and paste the sentences that talk about what retention of
personal information)
11. Does the privacy policy state the security technologies (e.g. SSL or access
control policies etc.) applied by the website to protect the users’ personal
information?
(a) No - the policy does not state the security technologies applied toe protect
the personal information
(b) Yes - the policy states the security technologies applied to protect the
personal information (Please copy and paste the sentences that talk about
security/protection of personal information)
12. Does the privacy policy provide information about users’ right to access and
modify the information collected by the website?
(a) No - the policy does not provide information about users’ right to access
and modify their personal information
(b) Yes - the policy provides information about users’ right to access and
modify their personal information (Please copy and paste the sentences
that talk about how users can access/modify personal information)
13. As a user, do you have any concerns regarding the privacy policy you read, that
are not already addressed within the policy? Please specify
110
14. Did you find any phrases or sentences within the privacy policy that are am-
biguous or contradictory? Please specify.
15. On a scale of 1 to 10 (10 - most complete and 1 - least complete), please rate
the privacy policy you read based on its completeness. Please comment about
the completeness of the privacy policy and your rating.
16. On a scale of 1 to 10 (10 - most ambiguous and 1 - least ambiguous), please
rate the privacy policy you read based on its ambiguity. Please comment about
the ambiguity of the privacy policy and your rating.
17. Please give a score between 1 to 100 for the transparency of the privacy policy
you read.
C.2 Survey 2
1. On a scale of 1 to 10 (10 - most complete and 1 - least complete), please rate
the privacy policy ‘Data Collection’ section based on its completeness. Please
comment about the completeness of this section and your rating.
2. On a scale of 1 to 10 (10 - most complete and 1 - least complete), please rate the
privacy policy ‘Purpose of Data Collection’ section based on its completeness.
Please comment about the completeness of this section and your rating.
3. On a scale of 1 to 10 (10 - most complete and 1 - least complete), please rate
the privacy policy ‘Data Sharing’ section based on its completeness. Please
comment about the completeness of this section and your rating.
4. On a scale of 1 to 10 (10 - most complete and 1 - least complete), please rate
111
the privacy policy ‘User rights and Choices’ section based on its completeness.
Please comment about the completeness of this section and your rating.
5. On a scale of 1 to 10 (10 - most complete and 1 - least complete), please rate
the privacy policy ‘Access/ Change’ section based on its completeness. Please
comment about the completeness of this section and your rating.
6. On a scale of 1 to 10 (10 - most complete and 1 - least complete), please rate
the privacy policy ‘Data Security’ section based on its completeness. Please
comment about the completeness of this section and your rating.
7. On a scale of 1 to 10 (10 - most complete and 1 - least complete), please
rate the privacy policy ‘Cookies and Other Technologies’ section based on its
completeness. Please comment about the completeness of this section and your
rating.
8. On a scale of 1 to 10 (10 - most complete and 1 - least complete), please rate
the privacy policy ‘Data Retention’ section based on its completeness. Please
comment about the completeness of this section and your rating.
9. On a scale of 1 to 10 (10 - least ambiguous and 1 - most ambiguous), please
rate the privacy policy ‘Data Collection’ section based on its ambiguity. Please
comment about the ambiguity of this section and your rating.
10. On a scale of 1 to 10 (10 - least ambiguous and 1 - most ambiguous), please rate
the privacy policy ‘Purpose of Data Collection’ section based on its ambiguity.
Please comment about the ambiguity of this section and your rating.
11. On a scale of 1 to 10 (10 - least ambiguous and 1 - most ambiguous), please
rate the privacy policy ‘Data Sharing’ section based on its ambiguity. Please
112
comment about the ambiguity of this section and your rating.
12. On a scale of 1 to 10 (10 - least ambiguous and 1 - most ambiguous), please
rate the privacy policy ‘User rights and Choices’ section based on its ambiguity.
Please comment about the ambiguity of this section and your rating.
13. On a scale of 1 to 10 (10 - least ambiguous and 1 - most ambiguous), please
rate the privacy policy ‘Access/ Change’ section based on its ambiguity. Please
comment about the ambiguity of this section and your rating.
14. On a scale of 1 to 10 (10 - least ambiguous and 1 - most ambiguous), please
rate the privacy policy ‘Data Security’ section based on its ambiguity. Please
comment about the ambiguity of this section and your rating.
15. On a scale of 1 to 10 (10 - least ambiguous and 1 - most ambiguous), please
rate the privacy policy ‘Cookies and Other Technologies’ section based on its
ambiguity. Please comment about the ambiguity of this section and your rating.
16. On a scale of 1 to 10 (10 - least ambiguous and 1 - most ambiguous), please
rate the privacy policy ‘Data Retention’ section based on its ambiguity. Please
comment about the ambiguity of this section and your rating.
17. On a scale of 1 to 10 (10 - most complete and 1 - least complete), please rate
the privacy policy you read based on its completeness. Please comment about
the completeness of the privacy policy and your rating.
18. On a scale of 1 to 10 (10 - most ambiguous and 1 - least ambiguous), please
rate the privacy policy you read based on its ambiguity. Please comment about
the ambiguity of the privacy policy and your rating.
113
19. Please give a score between 1 to 100 for the transparency of the privacy policy
you read.
20. Please rank the following 8 sections of a privacy policy based on your preference
(1 - most preferred and 8 - least preferred).
114
Appendix D
Classifier Performance for Completeness and
Unambiguity
115
T
ab
le
D
.1
:
C
om
p
le
te
n
es
s:
A
ve
ra
ge
F
-m
ea
su
re
s
an
d
S
ta
n
d
ar
d
D
ev
ia
ti
on
s
fo
r
L
R
,
K
N
N
an
d
M
N
B
fo
r
10
It
er
at
io
n
s
S
e
ct
io
n
L
R
K
N
N
M
N
B
B
a
se
li
n
e
C
h
i
B
a
se
li
n
e
C
h
i
B
a
se
li
n
e
C
h
i
U
n
fi
lt
e
re
d
F
il
te
re
d
U
n
fi
lt
e
re
d
F
il
te
re
d
U
n
fi
lt
e
re
d
F
il
te
re
d
U
n
fi
lt
e
re
d
F
il
te
re
d
U
n
fi
lt
e
re
d
F
il
te
re
d
U
n
fi
lt
e
re
d
F
il
te
re
d
A
cc
e
ss
78
.0
9
(0
.6
5)
78
.1
9
(0
.6
6)
78
.7
9
(0
.5
2)
78
.7
9
(0
.7
4)
82
.1
1
(1
.1
3)
88
.1
4
(0
.3
8)
88
.1
1
(0
.2
5)
88
.3
2
(0
.2
3)
85
.1
1
(0
.6
7)
88
.1
1
(0
.3
5)
78
.7
9
(0
.4
5)
78
.7
9
(0
.1
6)
C
h
o
ic
e
63
.6
4
(0
.2
5)
63
.6
4
(0
.8
4)
63
.6
4
(0
.7
6)
63
.6
4
(0
.4
7)
63
.1
2
(0
.7
5)
63
.4
3
(1
.2
1)
66
.0
2
(0
.5
7)
69
.1
1
(0
.5
4)
63
.2
1
(0
.4
3)
63
.4
3
(0
.4
5)
63
.6
4
(0
.6
9)
63
.6
4
(0
.5
6)
C
o
ll
e
ct
89
.1
1
(0
.8
7)
90
.9
0
(0
.5
2)
90
.9
0
(0
.3
5)
90
.9
1
(0
.7
5)
91
.0
4
(0
.4
1)
94
.4
1
(0
.4
8)
94
.0
9
(0
.4
4)
94
.2
9
(0
.5
6)
91
.3
3
(0
.7
5)
94
.2
3
(0
.5
8)
90
.9
0
(0
.5
6)
90
.9
1
(0
.4
9)
C
o
o
k
ie
s
76
.0
2
(0
.6
5)
76
.4
3
(0
.9
5)
72
.7
9
(0
.9
1)
78
.7
9
(0
.8
6)
76
.1
3
(0
.7
4)
79
.3
4
(0
.2
3)
76
.3
4
(1
.0
7)
79
.1
3
(0
.8
5)
76
.2
1
(0
.3
3)
78
.3
2
(0
.1
3)
72
.7
9
(0
.9
9)
78
.7
9
(0
.7
6)
P
u
rp
o
se
82
.1
7
(0
.6
5)
81
.8
2
(0
.7
8)
81
.8
2
(0
.8
5)
81
.8
2
(0
.2
7)
85
.2
4
(0
.6
4)
85
.4
1
(0
.4
3)
85
.2
3
(0
.6
8)
85
.4
3
(0
.8
7)
85
.0
8
(1
.2
7)
85
.4
3
(0
.1
5)
81
.8
2
(1
.3
3)
81
.8
2
(0
.6
1)
R
e
te
n
ti
o
n
58
.1
5
(0
.6
5)
57
.5
8
(0
.3
6)
60
.6
1
(0
.6
8)
57
.5
8
(0
.2
9)
64
.3
2
(0
.3
9)
70
.3
2
(0
.4
5)
64
.1
2
(1
.1
7)
70
.3
4
(0
.6
3)
63
.1
4
(0
.7
7)
63
.4
4
(0
.8
4)
63
.6
4
(0
.6
8)
60
.6
1
(0
.6
6)
S
e
cu
ri
ty
81
.2
3
(0
.8
2)
81
.3
3
(0
.7
4)
69
.6
9
(0
.5
5)
66
.6
7
(0
.6
3)
85
.1
2
(0
.4
0)
85
.3
4
(0
.4
5)
85
.2
3
(0
.9
4)
85
.3
2
(0
.3
2)
88
.2
3
(0
.1
9)
88
.3
2
(0
.6
4)
72
.6
7
(0
.2
3)
66
.6
7
(0
.4
5)
S
h
a
re
85
.0
5
(0
.3
9)
85
.3
5
(0
.6
0)
84
.8
5
(0
.1
5)
84
.8
5
(1
.2
8)
91
.1
1
(0
.3
7)
91
.4
2
(0
.4
1)
91
.0
4
(0
.4
1)
91
.4
2
(0
.5
0)
91
.0
2
(0
.7
2)
91
.3
4
(0
.7
1)
84
.8
5
(0
.8
3)
84
.8
5
(0
.7
3)
116
T
ab
le
D
.2
:
U
n
am
b
ig
u
it
y
:
A
ve
ra
ge
F
-m
ea
su
re
s
an
d
S
ta
n
d
ar
d
D
ev
ia
ti
on
s
fo
r
L
S
V
M
,
K
N
N
an
d
M
N
B
fo
r
10
It
er
at
io
n
s
S
e
ct
io
n
L
S
V
M
K
N
N
M
N
B
B
a
se
li
n
e
C
h
i
B
a
se
li
n
e
C
h
i
B
a
se
li
n
e
C
h
i
U
n
fi
lt
e
re
d
F
il
te
re
d
U
n
fi
lt
e
re
d
F
il
te
re
d
U
n
fi
lt
e
re
d
F
il
te
re
d
U
n
fi
lt
e
re
d
F
il
te
re
d
U
n
fi
lt
e
re
d
F
il
te
re
d
U
n
fi
lt
e
re
d
F
il
te
re
d
A
cc
e
ss
59
.6
7
(0
.4
7)
75
.7
7
(0
.7
7)
59
.6
7
(0
.1
7)
75
.7
8
(1
.2
5)
54
.5
3
(0
.6
7)
69
.6
6
(0
.0
7)
59
.6
7
(0
.2
9)
71
.7
2
(0
.8
4)
60
.6
4
(0
.4
5)
75
.7
6
(0
.4
5)
63
.6
2
(0
.3
2)
78
.7
8
(0
.6
5)
C
h
o
ic
e
66
.6
7
(0
.6
1)
75
.7
4
(0
.3
5)
66
.6
7
(0
.3
2)
75
.7
6
(0
.9
5)
60
.6
1
(0
.8
5)
72
.7
1
(0
.4
5)
63
.6
3
(0
.8
7)
72
.7
6
(0
.3
3)
63
.6
2
(0
.7
8)
72
.7
2
(0
.3
1)
63
.6
4
(0
.7
8)
72
.7
6
(0
.3
7)
C
o
ll
e
ct
69
.6
7
(0
.4
1)
72
.7
8
(0
.1
1)
69
.6
7
(0
.6
5)
72
.7
4
(0
.4
4)
66
.6
7
(0
.9
8)
72
.7
5
(0
.7
6)
69
.6
2
(1
.1
2)
75
.7
7
(0
.8
5)
72
.7
4
(0
.4
3)
75
.7
7
(0
.2
3)
69
.6
7
(0
.6
5)
78
.7
8
(0
.5
6)
C
o
o
k
ie
s
69
.6
7
(0
.7
9)
69
.6
9
(0
.7
1)
69
.6
7
(0
.3
1)
78
.7
9
(0
.5
7)
63
.6
7
(0
.7
5)
72
.7
6
(0
.3
9)
63
.6
7
(0
.6
5)
72
.7
5
(0
.6
4)
75
.7
3
(0
.4
2)
75
.7
9
(0
.8
9)
69
.6
8
(0
.3
7)
75
.7
8
(0
.9
8)
P
u
rp
o
se
60
.6
4
(0
.3
6)
72
.7
1
(0
.2
9)
63
.6
6
(0
.6
7)
75
.7
4
(0
.4
5)
63
.6
2
(1
.2
2)
75
.7
7
(0
.8
9)
69
.6
1
(0
.8
1)
78
.7
9
(0
.1
7)
65
.6
7
(0
.7
1)
72
.7
2
(0
.7
4)
65
.6
9
(0
.9
5)
75
.7
7
(0
.5
1)
R
e
te
n
ti
o
n
63
.6
2
(0
.6
3)
78
.7
4
(0
.3
6)
63
.6
7
(0
.4
3)
78
.7
8
(0
.2
8)
62
.6
7
(0
.8
6)
66
.6
7
(0
.6
5)
63
.6
2
(0
.2
7)
69
.6
5
(0
.7
7)
63
.6
3
(0
.8
6)
78
.7
8
(0
.7
9)
66
.6
9
(0
.8
7)
80
.8
0
(1
.2
7)
S
e
cu
ri
ty
73
.7
2
(0
.9
2)
75
.7
2
(0
.5
6)
73
.7
2
(0
.2
9)
75
.7
3
(1
.1
4)
60
.6
9
(0
.4
3)
69
.6
2
(1
.1
9)
62
.6
5
(1
.0
6)
72
.7
6
(0
.2
6)
78
.7
6
(0
.3
9)
78
.7
9
(0
.9
5)
75
.7
6
(0
.7
2)
78
.7
8
(0
.3
7)
S
h
a
re
63
.6
1
(0
.8
8)
72
.7
6
(0
.6
2)
66
.6
7
(0
.8
2)
72
.7
4
(0
.5
7)
60
.6
1
(0
.5
9)
72
.7
5
(0
.4
9)
63
.6
7
(1
.1
6)
72
.7
2
(0
.5
4)
66
.6
7
(0
.3
7)
75
.7
5
(0
.8
3)
69
.6
3
(0
.6
5)
75
.7
5
(0
.1
6)
117
