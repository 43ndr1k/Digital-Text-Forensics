 
M. Lee et al. (Eds.): ICONIP 2013, Part II, LNCS 8227, pp. 368–375, 2013. 
© Springer-Verlag Berlin Heidelberg 2013 
Deep Neural Networks for Source Code Author 
Identification 
Upul Bandara and Gamini Wijayarathna 
Department of Industrial Management, University of Kelaniya, Sri Lanka 
upulbandara@gmail.com, gamini@kln.ac.lk 
Abstract. Plagiarism and copyright infringement are major problems in aca-
demic and corporate environments. Importance of source code authorship attri-
bution arises as it is the starting point of detection for plagiarism, copyright  
infringement and law suit prosecution etc. There have been many research re-
gard to this topic. Majority of these researches are based on various algorithms 
which compute similarity amongst source code files. However, for this Paper 
we have proposed Deep Neural Network (DNN) based technique to be used for 
source code authorship attribution. Results proved that DNN based author iden-
tification brings promising results once compared the accuracy against pre-
viously published research. 
Keywords: Restricted Boltzmann Machine, Deep Neural Networks, Source 
Code Authorship Attribution. 
1 Introduction 
Source code authorship attribution has many applications in different fields such as 
source code plagiarism detection, digital forensics, and intellectual property infringe-
ment [1]. Since magnitude of source code repositories are growing very rapidly, it is 
impractical to use manual techniques for source code authorship attribution. There-
fore, automatic techniques could be ideal solutions for identifying authors of source 
codes.  
In this paper we investigate deep neural networks for source code authorship at-
tribution task. Training deep neural networks are known to be hard. It is empirical-
ly shown that the standard backpropagation algorithm could easily get stuck in a 
local minimum. Therefore, until recently neural networks have been limited to one 
or two hidden layers [2]. However, training deep neural networks using greedy 
layer-wise pre-training before fine-tuning using backpropagation enables to over-
come above limitation [3]. Therefore, we employed that technique to train our 
deep neural network. Our system was evaluated with several datasets and results 
have shown that performance is very close to the state of art techniques in the 
source code identification field. 
 Deep Neural Networks for Source Code Author Identification 369 
 
2 Previous Work 
A few research papers have been written on source author identification using ma-
chine learning techniques.  Following briefs such important techniques we found are 
useful during literature surveying phase. 
Lange and Mancoridis [4] have proposed a source code authorship attribution me-
thod using source code metric histograms and genetic algorithm. From source code 
metrics, an optimum set of source code metrics were selected using genetic algo-
rithms. Selected metrics were used as the input for the nearest neighbor classifier and 
the system is capable of identifying the true author of each source code file with 55 
percent accuracy. 
Burrows and Tahaghoghi [5] described a system for source code author identifica-
tion using an information retrieval approach. First, the n-gram tokens were generated 
from each source code file. Then the generated tokens were indexed in a search en-
gine. During the testing period each test document was converted into a collection of 
n-gram tokens and was compared with indexed source code files. According to the 
paper, their system is capable of identifying true authors with 67 percent of accuracy. 
Frantzeskou et al. [1] described a technique called, Source Code Author Profiles 
(SCAP) for authorship attribution. SCAP is based on generating byte level n-gram 
author profiles. In order to classify a test source code file, its profile is compared with 
the pre-calculated training author profiles and the most likely author is the one who 
has the least dissimilar profile. According the paper, accuracy of the SCAP method is 
very equal to 100 percent. 
Application of decision tree techniques and programming style metrics for source 
code author identification are described by Elenbogen and Seliya [6]. Performance of 
their system was evaluated with a dataset consisted of 82 source code files belonging to 
12 authors and they have achieved 74.70 percent of accuracy. Shevertalov et al. [7] 
described a source code discretization based method for generating source code author 
profiles. They evaluated their system with a dataset consisting of 75 000 Java source 
code files, belonging to 20 authors and reported 75 percent of classification accuracy. 
3 Training the Deep Neural Network 
3.1 Restricted Boltzmann Machine (RBM) 
RBM is a bipartite graph in which visible units represent the input data and hidden 
units represent features using undirected connections [8]. RBM has been used for 
various supervised and unsupervised applications such as, dimensionality reduction, 
classification, collaborative filtering, and clustering. RBM was invented by Smo-
lensky [9], however, it became very popular after introducing fast learning algorithms 
by Hinton [10]. RBM is an energy based model and the joint energy between visible 
and hidden units is defined by Eq. 1. 
, ;   (1)
370 U. Bandara and G. Wijayarathna 
 
Where , ,  represent the parameters of the model.  represents the 
symmetric weight between  visible unit and the  hidden unit.  denotes con-
nection between bias term and the  visible unit.  denotes the connection be-
tween bias unit and the  hidden unit.  and  represent the visible and hidden 
vectors respectively. 
The joint probability distribution of visible and hidden units is given by Eq. 2 and 
Eq. 3 and  is known as the partition function. , ; 1 , ;  (2)
, ;  (3)
The probability that the model assigns to the visible vector  is given by Eq. 4. ; 1 , ;  (4)
By taking the derivatives of Eq. 1 with respect to model parameters we can derive 
following learning rules. ;   (5)
;   (6)
;  (7)
Where, . is the expected value evaluated on data distribution and . shows the expected value calculated on model distribution. 
Contrastive divergence is a recipe widely used for training RBM. It approximates 
the gradient of log likelihood of RBM using Gibbs sampling. For this research we 
used contrastive divergence for training RBMs. 
4 Source Code Authorship Attribution System 
Under this heading we describe the main components of our source code authorship 
attribution system. 
Fig.1 (a) shows the high-level architecture of the system. Source codes were con-
verted to code metrics and used as the input for the system. Although, a large number of 
metrics can be generated from source code files, Lange and Mancoridis [4] have con-
ducted an extensive research and identified eight source code metrics, which shows the 
best performance in the context of source code author identification. However, some of 
 Deep Neural Networks for Source Code Author Identification 371 
 
these metrics are not fully independent from others. For example, trail-space, and trail-
tab (measure the trailing whitespaces and tabs at the end of a line) are very similar to 
each other. Therefore, we represent these two as a single metric called "TrailTabSpace-
Calculator" in our system. Similarly, we combined all the metrics which are not fully 
independent from others, as single metrics. Moreover, we have introduced three more 
code metrics. Altogether, there were nine metrics as shown in the Table 1. 
 
 
Fig. 1. (a) The high-level architecture of the source code author attribution system, (b) The steps 
required to implement the deep neural network with three hidden layers and one output layer 
Table 1. Source code metrics used for source code author identification 
Metric name Code Description 
LineLengthCalculator LLC This metric measures the number of characters in 
one source code line 
LineWordsCalculator LWC This metric measures the number of words in one 
source code line 
AccessCalculator ACL Java uses the four levels of access control: public, 
protected, default and private. This metric calcu-
lates the relative frequency of these access levels 
used by the programmers 
CommentsFrequency-
Calculator 
CFC Java uses three types of comments. This metric 
calculates the relative frequency of those com-
ment types used by the programmers 
IndentifiersLengthCal-
culator 
ILC This metric calculates the length of each identifier 
of Java programs 
InLineSpaceInlineTab-
Calculator 
INT This metric calculates the whitespaces that occurs 
on the interior areas of non-whitespace lines 
TrailTabSpaceCalcula-
tor 
TTS This metric measures the whitespace and tab  
occurrence at the end of each non-whitespace line 
UnderscoresCalculator USC This metric measures the number of underscore 
characters used in identifiers 
IndentSpaceTabCalcu-
lator 
IST This metric calculates the indentation whitespaces 
used at the beginning of each non-whitespace line 
372 U. Bandara and G. Wijayarathna 
 
The source code metrics as it is cannot be used as the input of the source code au-
thor identification system. Therefore, metrics were converted to a stream of tokens. 
This process is better explained with an example. 
Table 2. Source code metric and token frequencies generated by “LineLengthCalculator” 
metric 
Line length Number of occurrences Token Token frequency 
1 2 LLC_1 2 
8 12 LLC_8 12 
14 3 LLC_14 3 
21 1 LC_21 1 
 
Consider a metric generated by the "LineLengthCalculator" for a particular source 
code file as shown in the first two columns of the Table 2. This code metric generates 
a set of tokens and token frequencies as depicted in last two columns of the Table 2. 
Similarly, we converted all the metrics generated from source codes into a set of to-
kens with token frequencies. These tokens together with token frequencies are used as 
the input for our system. 
Fig.1 (b) shows the sequence of operations required to construct the deep neural 
network of the system. Source code metrics generated from the "Source Code Metrics 
Generator" module of the system are used as the input for the first RBM. Output of 
the fist RBM is used as the input of the second RBM. Following the same procedure 
we trained three RBMs. Finally, the pre-trained DNN was created by adding the out-
put layer on top of the last RBM. 
5 Training and Evaluation 
We have measured the performance of our system using five datasets. Find below the 
details about these five datasets. 
Dataset I was created by LangeandMancoridis [4]. It consists of Java source code 
files belonging to 10 authors. These files were extracted from the Sourcefore1 web-
site. Then we created Dataset II by downloading Java source code files from free and 
open source projects in the Internet. It consists of Java source code files belonging to 
10 authors. Dataset III consists of Java source code files belonging to eight authors 
and it was created by using source codes shipped with the Java Development Kit2. 
Dataset IV consists of Java files downloaded from Plant Source Code3 website and it 
contains code files belonging to five authors. Finally, we created Dataset V by using 
Java codes freely available with several programming books published by ApressInc4. 
Details information about these five datasets is given in Table 3. 
                                                          
1 http://sourceforge.net/ [Accessed: 2012, March 12] 
2 http://www.java.com/en/download/index.jsp [Accessed: 2012, April 02] 
3 http://www.planet-source-code.com/ [Accessed: 2012, August 01] 
4 http://www.apress.com/ [Accessed: 2012, August 03] 
 Deep Neural Networks for Source Code Author Identification 373 
 
Table 3. Detailed information for datasets used in this study. Program lengths are measured by 
means of Lines of Codes (LOC) 
Property I II III IV V 
Number of authors 10 10 8 5 9 
No of source code files 1644 780 475 131 520 
Minimum files per author 61  28 33 20 5 
Maximum files per author 377 128 118 36 118 
Size of the smallest file (LOC) 7 28 91 8 20 
Size of the largest file (LOC) 3691 15052 4880 654 1135 
Average LOC per author 29857 44620 26690 2452 6268 
 
We divided first dataset into three subsets called as training, cross validation, and 
testing. The rest of the datasets were divided into two training and testing subsets. The 
system also consists of several hyper-parameters as given below and which were es-
timated using the cross validation dataset. 
1. Learning rate of RBMs. 
2. Regularization parameter of the neural network. 
3. Number of logistic units in hidden layer 1, hidden layer 2, and hidden layer 3. 
Presently, manual search and grid search are the most widely used techniques for 
estimating optimum values for the hyper-parameters of learning systems. However, 
Bergstra and Bengio [11] have shown that random search is more effective than ma-
nual and grid search. Hence, we employed random search as the method for finding 
optimum values for the hyper-papers of our system. 
We initiated the training process with a batch of 50 source code files. With each 
iteration training dataset size was increased by 50 and process continued until 850 
files in the final training dataset. For each iteration, we tested the accuracy of the sys-
tem, 25 times with randomly selected values for hyper-parameters using the cross 
validation dataset. The accuracy of the cross validation dataset was plotted as scatter 
graphs as shown in Fig 2(a) to Fig 2(q). Next, we selected the best cross validation 
accuracy from each iteration and drew the cross validation accuracy vs. training batch 
size graph as depicted in Fig 2(s). We used it has a tool for selecting training batch 
sizes and hyper-parameter values for other datasets. 
Table 4. Percentage accuracies on five datasets mentioned in Table 3 
System I II III IV V 
Lange and Mancoridis[12] 55.00% - - - - 
Bandara and Wijayarathna[13] 86.64% - - - - 
Bandara and Wijayarathna[14] 92.82% 93.64% 90.78% 77.42% 89.62% 
This paper 93.65% 93.22% 93.62% 78.12% 89.62% 
 
 
374 U. Bandara and G. Wijayarathna 
 
 
Fig. 2. Classification accuracy of cross validation dataset as a function of hyper-parameters of 
the system. (a)- (q) cross validation accuracy vs. number of iterations for different training 
batch sizes (r) cross validation accuracy vs. training batch size 
6 Conclusion and Future Works 
The paper has investigated Deep Neural Networks for source code authorship attribu-
tion. There we have used Restricted Boltzmann Machine for pre-training hidden  
layers of deep neural networks. Test results confirm a considerable improvement in 
accuracy compared to the existing published methods for author identification. 
Literature also suggest that, for pre-training of deep neural network, not only the 
Restricted Boltzmann Machine algorithm but also other algorithms like Autoassocia-
tors, Denoising Auto Encoders, and Predictive Sparse Coding are used. Therefore, it 
should be an interesting alternative to investigate performance of other pre-training 
algorithms in this regard. 
References 
1. Frantzeskou, G., Stamatatos, E., Gritzalis, E., Katsikas, S.: Source Code Author Identifica-
tion Based on N-gram Author Profiles. In: Maglogiannis, I., Karpouzis, K., Bramer, M. 
(eds.) Artificial Intelligence Applications and Innovations. IFIP, vol. 204, pp. 508–515. 
Springer, Boston (2006) 
2. Larochelle, H., Bengio, Y., Louradour, J., Lamblin, P.: Exploring Strategies for Training 
Deep Neural Networks. Journal of Machine Learning Research 10, 1–40 (2009) 
3. Hinton, G.H.: Reducing the dimensionality of data with neural networks. Science,  
504–507 (2006) 
 Deep Neural Networks for Source Code Author Identification 375 
 
4. Lange, R., Spiros, M.: Using code metric histograms and genetic algorithms to perform au-
thor identification for software forensics. In: 9th Annual Conference on Genetic and Evo-
lutionary Computation, London, pp. 2082–2089 (2007) 
5. Burrows, S., Tahaghoghi, S.: Source Code Authorship Attribution using N-Grams. In: Wu, 
A. (ed.) Source Code Authorship Attribution using N-Grams, Melbourne, Melbou Austra-
liarne, pp. 32–39 (2007) 
6. Elenbogen, B., Seliya, N.: Detecting outsourced student programming assignments, pp. 
50–57 (January 2008) 
7. Shevertalov, M., Kothari, J., Stehle, E., Mancoridis, S.: On the Use of Discretized Source 
Code Metrics for Author Identification. In: Proceedings of the 2009 1st International Sym-
posium on Search Based Software Engineering, Washington, DC, USA, pp. 69–78 (2009) 
8. Hinton, G., Deng, L., Yu, D., Dahl, G., Mohamed, A.-R., Jaitly, N., Senior, A., Van-
houcke, V.: Deep Neural Networks for Acoustic Modeling in Speech Recognition. IEEE 
Signal Processing Magazine, 82–97 (November 2012) 
9. Smolensky, P.: Information processing in dynamical systems: Foundations of harmony 
theory. In: Parallel Distributed Processing Explorations in the Microstructure of Cognition, 
pp. 194–281 (1986) 
10. Hinton, G.: To recognize shapes, first learn to generate images. Progress in Brain Re-
search 165(3), 535–547 (2007) 
11. Bergstra, J., Bengio, Y.: Random Search for Hyper-Parameter Optimization. The Journal 
of Machine Learning Research, 281–305 (2012) 
12. Lange, R., Mancoridis, S.: Using code metric histograms and genetic algorithms to per-
form author identification for software forensics. In: Proceedings of the 9th Annual Confe-
rence on Genetic and Evolutionary Computation, GECCO 2007 (2007) 
13. Bandara, U., Wijayarathna, G.: A machine learning based tool for source code plagiarism 
detection. International Journal of Machine Learning and Computing 1(4), 337–343 (2011) 
14. Bandara, U., Wijayarathna, G.: Source code author identification with unsupervised fea-
ture learning. Pattern Recognition Letters 34(3), 330–334 (2013) 
