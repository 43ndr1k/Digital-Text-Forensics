Author Identification of E-mail Messages with OLMAM Trained
Feedforward Neural Networks
Nikolaos Ampazis, Helen Iakovaki and George Dounias
Department of Financial and Management Engineering
University of the Aegean
82100 Chios, Greece
Abstract
The OLMAM algorithm (Optimized Levenberg-
Marquardt with Adaptive Momentum) is a variant of
the Levenberg-Marquardt algorithm for training multi-
layer feedforward neural networks. OLMAM has been
shown to obtain excellent solutions in difficult classifi-
cation problems where other computational intelligence
techniques usually achieve inferior performances. In
this paper we apply OLMAM to the problem of author
identification of e-mail messages which is a challenging
classification problem due to the special characteristics
of the data. We performed a number of experiments with
a corpus of real-world e-mail messages (Enron corpus).
The performance of the proposed method was compared
with the performances achieved by Naive-Bayes and
SVM classifiers. Author identification with OLMAM was
found to be significantly better compared with the other
methods even if the author wrote about different topics.
1 Introduction
The widely spread use of electronic mail (e-mail) as
a form of communication between governments, indus-
tries and individuals can be accounted for by the dra-
matic growth of computer technology as well as by e-
mail’s obvious delivery advantages. However, disad-
vantages also exist including unsolicited marketing mes-
sages (spam), spread of computer worms and viruses,
phishing, unauthorised conveyancing of sensitive infor-
mation, mailing of offensive or threatening material, etc.
The senders of such abusive forms of e-mails always
try to conceal their identities so as to avoid exposure.
Among the practices they employ can be quoted the
forgery of their address by routing their e-mails through
anonymous mail servers and the modification of the e-
mails contents and of the header information. In order
to be able to bring offending users to justice it is im-
portant to trace the original authors of such e-mails and
provide empirical evidence concerning their practices.
Authorship attribution of e-mails can be considered
a categorization problem. However, the difference be-
tween authorship attribution and other text classifica-
tion tasks lies in the difficulty to define which features
of the document to use in order to classify it by its au-
thor. For example, compared to longer, formal text doc-
uments, e-mails are generally brief with few sentences
and paragraphs which may also contain a large number
of grammatical errors. In addition, the author’s com-
position style used in e-mails can evolve quite rapidly
over time. Such characteristics render the classification
of e-mails very difficult compared to standard text clas-
sification. During the last decades a variety of possible
attributes have been proposed, whose majority has orig-
inated mainly from research in the area of information
retrieval [1] [2].
A variety of machine learning methods have been
proposed for authorship attribution. Most notable ex-
amples include Markov Chains [3], Neural Networks
[4], Discriminant Analysis [5], and Support Vector Ma-
chines (SVMs) [6]. In this paper we apply feedforward
neural networks trained with the OLMAM (Optimized
Levenberg-Marquardt with Adaptive Momentum) algo-
rithm [7]. The algorithm is a variant of the well known
Levenberg-Marquardt method (LM) and derives from
the formulation of the training task as a constrained opti-
mization problem. It yields iterations similar to the LM
learning rule with the inclusion of an adaptive momen-
tum term. The effectiveness of the algorithm has been
verified in various studies of difficult neural networks
benchmarks and classification problems [7] and there-
fore is challenging to verify its performance in the prob-
lem of e-mail authorship attribution.
The Enron dataset [8] is used as a benchmark case
19th IEEE International Conference on Tools with Artificial Intelligence
1082-3409/07 $25.00 © 2007 IEEE
DOI 10.1109/ICTAI.2007.165
413
Authorized licensed use limited to: IEEE Xplore. Downloaded on November 19, 2008 at 12:53 from IEEE Xplore.  Restrictions apply.
study and we provide a performance comparison across
two popular classifiers, namely Naive Bayes (NB), and
SVM. We show that OLMAM achieves surpisingly good
results and in all cases its performance is statistically
similar to that of the well-established SVM classifier, an
uncommon fact in the cases where feedforward neural
networks are compared to SVMs in classification tasks.
2 Enron Email Dataset and Feature Con-
struction
A large set of email messages, the Enron corpus,
was made public during the legal investigation concern-
ing the Enron corporation. The raw corpus is currently
available on the web at ”http://www-2.cs.cmu.edu/ en-
ron/”. The dataset consists of 517,431 messages that
belong to 150 users, mostly senior management of the
Enron Corp. By scanning the ”Sent” folders belonging
to six users we obtained a total of 1327 email messages
which we use in this paper. The users in question are:
Sally Beck (Chief Operating Officer), Vincent Kaminski
(Head of Quantitative Modeling Group), Louise Kitchen
(President of EnronOnline), Michelle Lokay (Adminis-
trative Assistant), Richard Sanders (Assistant General
Counsel) and William Williams III (Senior Analyst). No
identifiable threads were found in any of these folders
which means that the various e-mails sent by these users
covered various topics.
There were many design choices in the feature con-
struction. In this paper, our main focus is on the perfor-
mance of OLMAM compared to other widely used clas-
sifiers for the task of e-mail authorship attribution rather
than on the selection of the best available features that
can potentially improve a classifier’s performance. For
this reason we used the traditional bag-of-words docu-
ment as a baseline representation: messages were repre-
sented as vectors of word counts. We considered words
as sequences of alphabetic, digit and underscore char-
acters appearing anywhere in the email header or body.
Words were downcased, the 100 most frequent words
and those which appeared only once in the training set
were removed, and the remaining words were counted in
each message to compose a vector. We removed MIME
attachments and did not apply stemming. In future work,
richer representations could be considered, such as the
different treatment of different sections of each e-mail
(for example, the creation of distinct features for words
appearing in the header, body, signature, attachments,
etc) and the incorporation of a named entity recognizer.
3 The OLMAM Algorithm
The Optimized Levenberg Marquardt with Adaptive
Momentum (OLMAM) algorithm is a very efficient
second-order algorithm for training feedforward neu-
ral networks and, in some cases, it has been shown to
achieve the best training results on standard benchmark
datasets ever reported in the neural networks literature
[7].
The main idea in the formulation of the algorithm is
that while trying to minimize the neural network’s cost
function, a one-dimensional minimization in the direc-
tion dwt−1 followed by a second minimization in the
direction dwt does not guarantee that the cost function
has been minimized on the subspace spanned by both of
these directions.
For a feedforward neural network with K output
units and a set of P training patterns, the Mean Square
Error (MSE) cost function is defined as
E(w) =
1
2
P∑
p=1
K∑
i=1
(d
(p)
i − y(p)i )2 (1)
where y(p)i and d
(p)
i denote the output activations and de-
sired responses respectively, and w is the column vector
containing all the weights and thresholds of the network.
This can be achieved by the selection of conjugate
directions which form the basis of the CG method [9].
Two vectors dwt and dwt−1 are non-interfering or mu-
tually conjugate with respect to ∇2E(wt) when
dwt
T∇2E(wt)dwt−1 = 0 (2)
Therefore, the objective is to reach a minimum of the
cost function of equation (1) with respect to the synaptic
weights, and to simultaneously maximize the quantity
Φt =dwt
T∇2E(wt)dwt−1 without compromising the
need for a decrease in the cost function.
The strategy which we adopt for the solution of this
contrained optimization problem follows the methodol-
ogy for incorporating additional knowledge in the form
of constraints in neural network training originaly pro-
posed in [10]. This strategy yields the following weight
update rule for the neural network
dwt = − λ1
2λ2
[(JTt J t +μtI)]
−1∇E(wt)+ 1
2λ2
dwt−1
(3)
where J t is the Jacobian matrix of first derivatives at
step t and λ1 and λ2 can be evaluated in terms of known
quantities. Equation (3) is similar to the Levenberg-
Marquard (LM) weight update rule with the important
differences that in equation (3) there is an additional
adaptive momentum term and that the LM step is multi-
plied with an adaptive factor which controls its size. The
414
Authorized licensed use limited to: IEEE Xplore. Downloaded on November 19, 2008 at 12:53 from IEEE Xplore.  Restrictions apply.
quantity μt can be selected as in [11]. Further details on
the OLMAM algorithm can be found in [7].
4 Naive Bayes
The Naive Bayes classifier is commonly used to pro-
vide a baseline in text categorization. While applying
Bayes rule, a document di is assigned to category cj0 if
j0 = arg max
j=1,...,m
P (cj |di, θ)
= arg max
j=1,...,m
P (cj |θ)P (di|cj , θ) (4)
The likelihoods P (di|cj , θ) are computed using the
(naive) independence assumption that
P (di|cj , |θ) =
|di|∏
k=1
P (wk|cj , θ) (5)
where w1, ..., w|di| are words of the document di. The
parameters θ are estimated from the training set, usu-
ally using a multinomial or a multiple Bernoulli model.
More details on the Naive Bayes classifier can be found
in [12].
5 Support Vector Machines
Support Vector Machines (SVM) were first intro-
duced as a new class of machine learning techniques by
Vapnik [13] and are based on the structural risk mini-
mization principle. An SVM seeks a decision surface
to separate the training data points into two classes and
makes decisions based on the support vectors that are
selected as the only effective elements from the training
set. The goal of SVM learning is to find the optimal sep-
arating hyper-plane (OSH) that has the maximal margin
to both sides of the data classes. This can be formulated
as:
Minimize
1
2
w
T
w
subject to yi(wxi + b) ≥ 1 (6)
where yi is the decision of SVM for pattern xi and b
is the bias of the separating hyperplane. After the OSH
has been determined the SVM makes decisions based on
the globally optimized separating hyper-plane by find-
ing out on which side of the OSH the pattern is located.
This property makes SVM highly competitive with other
traditional pattern recognition methods in terms of pre-
dictive accuracy and efficiency.
The SVM paradigm can be extended to non-linear
models by mapping the input space into a higher dimen-
sional feature space based on a suitable kernel function.
An example of such a function is the widely used Gaus-
sian kernel defined as:
K(x, y) = exp
−||x−y||2
2σ2 (7)
The idea of the Support Vector Machine is to find a
model which guarantees the lowest classification error
by controlling the model complexity (VC-dimension)
based on the structural risk minimization principle.
This avoids over-fitting, which is the main problem for
other learning algorithms. So the distinctive advantage
of SVMs are their ability to process high-dimensional
problems as is the case with text classification.
6 Results and Discussion
In our experiments we compared the performance
of OLMAM, against the Naive Bayes, and Support
Vector Machine classifiers on our pre-processed En-
ron dataset as described in section 2. For all classi-
fiers the dataset was randomly divided 10 times into
train and test data using 60% and 40% of the sam-
ples respectively each time. For each feedforward neu-
ral network configuration trained with OLMAM (i.e.
for each different selection of hidden nodes) we per-
formed a total of 100 training trials resulting from train-
ing the network 10 times for each of the 10 random
splits of the dataset. Each different training trial was
performed by initializing the networks weights in the
range [-0.1, 0.1]. In each trial the maximum number
of epochs was set to 500. The maximum number of
epochs was set to this value since experiments with a
variety of other datasets have shown that OLMAM can
usually converge within a relatively small number of
training epochs [7]. In each of the 100 training trials,
training was considered successful whenever Fahlman’s
”40-20-40” criterion was satisfied [14], a standard crite-
rion used for convergence when feedforward neural net-
works are trained in classification tasks. All experiments
were carried in MATLAB using the LMAM/OLMAM
Neural Network Toolbox which is publicly available at
”http://authorid.fme.aegean.gr/toolbox”.
For the Naive Bayes classifier we used the Rain-
bow/Libbow software package developed by Andrew
McCallum [15] which is available for download
from ”http://www.cs.cmu.edu/∼mccallum/bow”.
For the SVM classifier we used the LIBSVM
library developed by Chih-Chung Chang and
Chih-Jen Lin [16] which is publicly available at
”http://www.csie.ntu.edu.tw/∼cjlin/libsvm/”. For train-
ing the SVM on the six different classes corresponding
to each one of the six e-mail authors we used the
one-against-all multi-class method for multi-label
classification using an SVM with a Gaussian kernel.
415
Authorized licensed use limited to: IEEE Xplore. Downloaded on November 19, 2008 at 12:53 from IEEE Xplore.  Restrictions apply.
Number of Hidden Nodes Testing Accuracy
0 97.05 ± 1.7%
2 97.96 ± 0.8%
3 98.01 ± 1.1%
4 98.18 ± 1.2%
5 98.51 ± 0.9%
6 98.74 ± 0.2%
7 98.96 ± 0.4%
8 99.15 ± 0.2%
9 99.40 ± 0.1%
10 99.18 ± 0.2%
Table 1. Testing accuracies averaged over
all the training/test splits, with the stan-
dard error of the mean, using OLMAM with
various number of features and hidden
nodes.
Naive Bayes SVM
96.76 ± 0.15% 97.94 ± 0.37%
Table 2. Testing accuracies averaged over
all the training/test splits, for Naive Bayes
and SVM.
In order to evaluate the performance of all classi-
fiers for the six category classification problem we used
the Testing Accuracy criterion which is the percentage
of cases in the test dataset that is classified correctly
by each classifier. Table 1 shows the results obtained
for different neural network architectures (all utilizing a
single hidden layer of neurons) trained with OLMAM
on the Enron dataset. The classification accuracy re-
ported for each neural network configuration is the av-
erage over the 10 training trials for all 10 random splits
of the dataset. In Table 2 we report the average accu-
racy over all 10 random split of the dataset for the Naive
Bayes and SVM classifiers.
From these tables we can observe that in all cases the
classification accuracy of OLMAM surpasses the one
obtained by SVM except from the case where no hidden
nodes where used. The best overall classification accu-
racy (99.40%) was obtained with a neural network with
9 hidden nodes. The performance of the Naive Bayes
classifier is inferior compared to the other two classi-
fiers, however, it could likely be improved by applying
more sophisticated feature selection techniques.
To assess statistical significance of the results we per-
formed the paired-t test on accuracies obtained from all
the training/test splits for the best trained OLMAM net-
work (with 9 hidden nodes) and SVM. We present in
Table 3 the classifier rankings for beck-s, kaminski-v,
Enron user OLMAM SVM
beck-s 98.60 ± 1.4% √ 98.80 ± 1.2%
kaminski-v 99.40 ± 0.5% √ 99.00 ± 0.5%
kitchen-l 86.48 ± 1.6% √ 55.25 ± 1.1%
lokay-m 99.17 ± 0.5% √ 99.07 ± 0.2%
sanders-r 74.34 ± 1.7% √ 73.67 ± 1.4%
williams-w3 99.00 ± 0.5% √ 99.00 ± 0.5%
Table 3. Results on Enron dataset com-
paring the best trained OLMAM network
(9 hidden nodes) with SVM. Each entry is
the average accuracy ± the standard error.
Results at entries marked by
√
are statis-
tically significantly better at the 0.05 (0.01)
level from a paired t test.
kitchen-l, lokay-m, sanders-r and williams-w3 which de-
note the ”Sent” folder of each of the six former Enron
employees in the order mentioned in section 2. We can
see from this table that the OLMAM classifier achieves
surprisingly good results on discriminating between dif-
ferent authors and that in all cases achieves statistically
similar performance as the well-established SVM clas-
sifier.
7 Conclusions
In this paper we utilized a highly efficient second
order neural network training algorithm (OLMAM) for
the construction of an efficient classifier for e-mail au-
thorship attribution. Performance comparisons were in-
cluded in the paper between the OLMAM methodology
and two well known classifiers, namely Naive Bayes and
SVM. OLMAM manages to train very efficiently a feed-
forward neural classifier under various parameter set-
tings, when attempting to discriminate between differ-
ent authors. The best performance of the proposed ap-
proach was obtained when OLMAM trained a network
with 9 hidden nodes where it provided a classification
accuracy of 99.40% (see Table 1). It is also interesting
to note that the method also provides statistically similar
performance with the more popular and more complex-
to-implement SVM classifier, which is not always the
case when feedforward neural networks are compared
to SVMs in classification tasks. Further experimentation
is underway, in order to test and compare the efficiency
of the OLMAM methodology with larger e-mail corpora
and a variety of feature extraction techniques which will
result in richer e-mail representations.
416
Authorized licensed use limited to: IEEE Xplore. Downloaded on November 19, 2008 at 12:53 from IEEE Xplore.  Restrictions apply.
References
[1] O. de Vel., “Mining e-mail authorship,” in Work-
shop on Text Mining. ACM International Conference
on Knowledge Discovery and Data Mining (KDD
2000), 2000.
[2] O. de Vel., A. Anderson, M. Corney and G. Mohay,
“Mining email content for author identification foren-
sics,” in SIGMOD: Special Section on Data Mining
for Intrusion Detection and Threat Analysis, 2001.
[3] D. Khmelev, “Disputed authorship resolution using
relative entropy for markov chain of letters in a text,”
in Conference Int. Quantitative Linguistics Associa-
tion, Prague, 2000.
[4] F. Tweedie, S. Singh, and D. Holmes, “Neural net-
work applications in stylometry: The federalist pa-
pers,” Computers and the Humanities, vol. 30, no.
1-10, 1996.
[5] R. Bosch and J. Smith, “Separating hyperplanes
and the authorship of the disputed federalist papers,”
CAmerican Mathematical Monthly, vol. 105, no. 7,
pp. 601608, 1998.
[6] J. Diederich,J Kindermann,E. Leopold and G. Paass,
“Authorship attribution with support vector ma-
chines,” Applied Intelligence, vol. 19, pp. 109123,
2003.
[7] Ampazis, N. and Perantonis, S. J, “Two highly ef-
ficient second order algorithms for training feedfor-
ward networks,” IEEE Transactions on Neural Net-
works, vol. 13 (5), pp. 1064–1074, 2002.
[8] B. Klimt, Y. Yang, “The enron corpus: A new
dataset for email classification research,” in Euro-
pean Conference on Machine Learning, Pisa, Italy,
2004.
[9] J. C. Gilbert and J. Nocedal, “Global convergence
properties of conjugate gradient methods for opti-
mization,” SIAM J. on Optimization, vol. 2, no. 1,
1992.
[10] Perantonis, S. J. and Karras, D. A., “An efficient
constrained learning algorithm with momentum ac-
celeration,” Neural Networks, vol. 8(2), pp. 237–239,
1995.
[11] M. T. Hagan, and M. Menhaj, “Training feedfor-
ward networks with the Marquardt algorithm,” IEEE
Transactions on Neural Networks, vol. 5, no. 6, pp.
989–993, 1994.
[12] A. McCallum and K. Nigam, “A comparison of
event models for naive bayes text classification,” in
AAAI-98 Workshop on Learning for Text Categoriza-
tion, 1998.
[13] Vapnik, V., The Nature of Statistical Learning The-
ory, Wiley, New York, 1998.
[14] Fahlman, S. E., “An empirical study of learn-
ing speed in back propagation networks,” Tech.
Rep.CMU-CS-88-162, Carnegie Mellon University,
1988.
[15] Andrew Kachites McCallum, “Bow: A toolkit for
statistical language modeling, text retrieval, classifi-
cation and clustering,” http://www.cs.cmu.edu/ mc-
callum/bow, 1996.
[16] Chih-Chung Chang and Chih-Jen Lin, LIBSVM: a
library for support vector machines, 2001.
417
Authorized licensed use limited to: IEEE Xplore. Downloaded on November 19, 2008 at 12:53 from IEEE Xplore.  Restrictions apply.
