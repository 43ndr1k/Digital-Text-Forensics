Identifying Topics by using Word Distribution
Motoi NAKAYAMA Takao MIURA
Dept.of Elect. and Elect. Engineering, Hosei University
Kajinocho 3-7-2, Koganei, Tokyo, Japan
Abstract
In this work, we examine and verify a topic word model
which says each topic can be identified by means of
word distribution under same author, and by using
Random Projection, one of the dimension reduction
techniques, we show we can obtain efficient and effec-
tive processing to the model. We examine Shakespeare
works and show we can identify scenes correctly to
their dramas.
1 Introduction
Authorship problem is one of the hot controversies over
a period of one century. This issue concerns how we can
identify authors by examining textual and some other
features. Typical applications of the discussion are
whether Shakespeare was really alive or not, and the
analysis of the threatening letters on Glico-Morinaga
case in Japan1[3]. There has been proposed several
techniques based on stylometry so far such as exam-
ining word length, sentence length and the number of
functional words (i.e., while, on) to examine and an-
alyze authorship, but it is well-known that there exist
unremarkable variation within an identical author[6].
Another interesting problem comes from a topic
identification. A topic means a subject of interests
or discussion, and by topic identification, we means
an issue what topic a text document concerns about.
This technique allows us to automate document clas-
sification as well as summarization, and to estimate
context-sensitive information for efficient retrieval.
Several investigations reveal a fact that it is eas-
ier to extract useful information from text putting
stress on relationship with topic (or theme) rather
than authorship[6]. Now we know author-topic model
which means we can identify authorship depending
on probability distribution on topics, and also we
know topic-word model which says we can identify
topics over multi-nominal probability distribution on
words[7]. Once we believe a topic-word model to a
1The Glico-Morinaga case is a famous extortion case in
Japan, primarily directed at the Japanese industrial confec-
tionaries Ezaki Glico and Morinaga, and currently remains
unsolved. The entire case spanned 17 months from the ini-
tial kidnapping of the president of Glico to the last known
communication from the prime suspect, a person or group
known only as the "The Monster with 21 Faces". See
http://en.wikipedia.org/wiki/Glico-Morinaga-case for de-
tail.
given collection of documents, we can identify (esti-
mate) a topic to each document by examining the word
distribution. Let us note that any documents may con-
tain multiple topics but here we assume a document is
identical to a topic to examine topic identification ef-
fectively and efficiently. The typical examples are news
articles, transaciption of news broadcasting; every news
article contains single topic.
In research activity of Information Retrieval (IR), it
is common to describe a world of interests by means
of Vector Space Model (VSM), where every document
is represented as a vector over words, and similarity
of two documents can be calculated easily as an inner
product of the two vectors[2]. However the similarity
depends on what kinds of weight methods to words
such as term frequency (TF) or inverse document fre-
quency (IDF), and very often we see high dimensional
processing. It is hard to obtain efficiency and excellent
precision. That's why there have been proposed several
techniques of dimension reduction by which we have ef-
ficient processing while keeping good precision[2].
In this work, we examine and verify topic word
model, and, by using dimension reduction techniques,
we show we can obtain efficient and effective processing
to the model.
Section 2 contains some introduction of topic word
model and how to evaluate the model. In section 3
we review dimension reduction and how we apply the
techniques to our topic model verification. In section
4 we show experimental results. We conclude our dis-
cussion in section 5.
2 Topic Word Model
A topic word model is an assumption that every topic
corresponds to its own multi-nominal probability dis-
tribution over words, i.e., each word is selected prob-
abilistically with multi-nominal distribution. This
means we can estimate which topic a document likely
corresponds to, by examining the word distribution
and identifying the probability distribution. The situ-
ation is similar to classification in Machine Learning,
where we have training samples in advance and exam-
ine which class is the most likely.
One of the problems is what does a word mean.
Syntactically a word is a character string separated
by space or punctuation, but the problem is not re-
ally simple: how can we think about compound words
(such as "U.S.A."), idioms (a group of words carrying
a different meaning when used together, such as "get
1-4244-1190-4/07/$25.00 C2007 IEEE. 245 PACRIM'07
used to") or collocation (the way that some words oc-
cur regularly when other words are used, such as "not
only ... but also"). In n-gram approach, we consider
successive n words as a unit, to cope with these aspects,
but we may have many erroneous words because we
ignore delimits. In this investigation, we examine and
verify whether a topic word model works correctly or
not using n-gram approach (n = 1, 2).
Given textual information, we remove stop words in
advance and apply stemming procedure to the informa-
tion. Then we extract two distributions (say, one for
training data and another for test data) of words from
one document, and examine whether the two distribu-
tions are really independent.
To examine the distributions, it seems nice to ap-
ply test of independence, but it does not really since
the distribution is too sparse to do smoothing. In this
investigation, we obtain X2 values to see how inde-
pendent the two distributions are. For each word wi,
we have two frequencies ai and bi from training data
and test data respectively. Then we define X2 value
as follows:
x2 = n1 (bi-ai (1)
If the two distributions are almost identical, we must
have small X2 by definition. Given a test document,
we calculate the word distribution and examine X2
value to training data of each topic. Then we select
the least X2 to estimate topics.
Assume we have q documents totally and p docu-
ments estimated correctly. Then we define correctness
ratio as below:
p (2)
q
3 Information Retrieval and Di-
mension Reduction
Under given conditions, we want to obtain text docu-
ments extracted from a given collection. To do that,
it is common to describe a document d by means of
several features which come from words appeared in
the documents. A Vector Space Model consists of
words dimensions with some feature values such as
frequency[2]. A text document d is described by us-
ing words WI, .., wn appeared in d
d = (VI, ---,Vn)
Note vi means a weight to the i-th word wi, such as
frequency. Possibly two documents d1, d2 are similar
with each other if the word distributions are similar.
The similarity is captured and described as the inner
product (d1, d2). To get to desired documents, we uti-
lize the similarity.
This approach is useful for information retreival be-
cause of its simplicity, but is sometimes hard to ob-
tain efficiency. This is because the answers depend on
weight methods and very often we see high dimensional
processing and defficiency.
Several techniques of dimension reduction have been
proposed so far[2]. By this approach, after we project
all the documents onto local parts and we put our at-
tention on them for retrieval, then we can apply IR
techniques efficiently.
There are two major techniques in dimensionality
reduction, Latent Semantic Indexcing (LSI) and Ran-
dom Projection (RP). LSI comes from linear algebraic
theory where we can specify sharp characterization of
dimension reduction. And very often we have excellent
results. On the other hand, it takes much time to the-
oretical calculation of Singular Value Decomposition
(SVD). Also incremental change of documents causes
the calculation from scratch. So we can't apply LSI to
dynamic environment in a straightforward manner.
Random Projection (RP) is another technque to re-
duce dimensionality by means of random number gen-
eration thus it is really efficient. Also the technique
is independent of documents but dependent on prob-
ability theory, and we don't need dimension reduc-
tion again under dynamic environment. On the other
hand, sometimes we don't have good results always
and sometimes we can't fully reduce dimensionality[4].
In this investigation, we discuss RP technique since
we want to identity a topic to unknown documents. In
the following, we assume d and N mean the number
of all the words and all the documents respectively. A
collection of the documents are described by a word-
document matrix X = ((xij)), denoted as Xd N, where
xij means a frequency of i-th word in a j-th document.
We want to reduce d x N matrix X into XRP which
has k x N, k < d.
To do that, we generate a Random Projection (RP)
matrix R = ((rij)) of k x d dimension. We define
Dimension Reduction of X by R as the matrix X RPk N
as follows:
X/7N R kxdXdxN (3)
We give the matrix Rk,d = ((rij)) in a probabilistic
manner as follows[1] where p means a probability:
+1 (p= 1/6)
rij = X3. (p= 2/3)
-I (p = 1/6)
(4)
It takes time of only O(k x d) to generate R and
O(d x k x N) for projecting data under d words, N doc-
uments and k-reduced dimension. Moreover, it should
be noted that the less k we have, the more efficiently
the process goes.
We extract any row vector from XdxN of training
data collection given in advance, and generate R. We
evaluate the results of correctness ratio with respect to
reduced dimension.
4 Experiments
In this section we show some experimental results to
see how our technique works well.
246
4.1 Preliminaries
In this experiment, we examine 10 works by Shake- 1 1/2 YES 70 10/3 y
2 1/2 YES 71 10/3 \Vspeare as test corpus and show the usefulness of our 3 2/2 YES 72 1/4 'Y
approach[5]. 4 2/2 YE 74 24 'V5 2/2 YES ~~~~74 2/4
Cl) A Midsummer Night's Dream (5 chapters 9 scenes) 7 2/2 YES 76 2/4
C2) As You Like It (5 chapters 22 scenes) 8 2/2 YES 77 3/4
C3) Cymbeline (5 chapters 26 scenes) 9 2/2 YES 78 3/4
C4) Hamlet (5 chapters 20 scenes) 10 3/2 YES 79 3/4
C5) Othello (5 chapters 15 scenes) I11 3/2 YES 80 3/412 3/2 YES 81 444 1C6) Julius Caesar (5 chapters 18 scenes) 13 3/2 NO 82 444 1
C7) King JIohn (5 chapters 16 scenes) 14 42 NO 83 4/4
C8) Richard II (5 chapters 19 scenes) 15 4/2 YES 84 444 1
C9) HnyVIII (5 chapters 17 scenes) 16 52 YES 85 4/4Henry ~~~~ ~ ~~~ ~~~~~~~~~~~1752NO 86 4/4CIO) The Tempest (5 chapters 9 scenes) 18 52 YES 87 444 1
19 6/2 YES 88 5/4Here we consider each work as a topic, and every first 20 6/2 YES 89 54
chapter in each topic as a training data. In fact, we21/2 YS 9 5422 6/2 YES 91 6/4examine the distribution of the words in the first chap- 23 7/2 YES 92 6/4
tejrs of 10 topics afteir remouving stop words anid stemii-24 82 YS 9 425 82 YES 94 7/4
ming. Then, we extract the words from each of the 138 26 8/2 YES 95 7/4
scnsin other chapters and remove stop words and do 27 8/2 NOS 96 7/4
stemming. By comparing the distribution with all the 29 9/2 YES 98 9/4
30 9/2 YES 99 9/4training ones, we examine the scene and identify which 31 9/2 YES 100 10/4
topic the scene belongs to. 32 10/2 YES 101 1/
33 10/2 YES 102 25A table 1 contains the number of training words in 34 1/3 YES 103 25
each topic after preprocessing where "'words" means 35 1/3 YES 104 2/5
36 2/3 YES 105 2/5 \the number of words. 37 2/3 YES 106 3/
We examine both 1-gram model and 2-gram model 38 2/3 YES 107 3/5 1
39 2/3 YES 108 3/5 1in experiment 1 and 3 respectively for the purpose of 40 2/3 YES 109 35the topic word model verification, while, in experiment 41 3/3 NO 110 35
42 3/3 NO 111 45 'y2, we analyze the net-effect of correctness ratio caused 43 3/3 NO 112 45
b DRP Inorcse eetmaetpc in temf 244 3/3 YES 113 /
our ~~~ ~~~ ~~~~~~~~~~~~~4533YES 114 /
values, or, in a form of ranking. We say a topic of a 46 3/3 NO 11½ 6/5
47 3/3 NO 116 6/5 1scene is correctly i'dentified if the true answer appears 48 4/3 NO 117 65
in the best 3 topics. Let us note that the table 1 con- 49 4/3 YES 118 65
0 4/3 NO 119 65 'ytains the number of words in a sense of 1-gram model. 1 4/3 YES 120 75
The result by RtP technique depends on a matrix that 52 53 NO 121 75
53 5/3 ~~~NO 122 7/5is randomly generated, and we examine our RP exper- 54 53 YES 123 75
iment 10 times and take the average. 5 3 YES 124 756 6/3 YES 125 7/5
57 63 YES 126 7/54.2 Results 58 6/3 YES 127 8/ 1
59 73 YES 128 8/5
Let us show our results. A table 1 illustrates the cor- 60 7/3 YES 129 8/
61 7/3 YES 130 8/rectness ratio (correctness") of the experiment 1. 2 73 YS 11 85 1
63 8/3 NO 132 8/5 1
topic wordscorrectness ~~~64 8/3 No 133 9/5 '
cl 65 0. 65 8/3 YES 134 9/5 1
C2 783 100.0 66 8/3 NO 135 95 'y
C3 1171 40.0 67 9/3 YES 136 95
C4 1327 33.3 68 9/3 YES 137 9/5 1
C5 1132 75.0 69 10 YES 138 1 10/5 -
co 849 86.7 Tpcieu cuo eut
C7 503 100.0 Table 2: TpcIetfcto eut
C8 1120 46.7
C9 1138 76.9
CIO 1067 100.0
(average) ~72.46
Table 1: Topic Identification in 1-gram model diesoncretnes dropai
9923 72.46 00%
9000 71.09 1.89A table 2 contains all the details of the experiment 1, 0 725 09
i.e., to which topic each scene is identified and whether 3000 72.10 0.50
2000 72.32 0.19the results are correct or not ("YES" means correct 0 67.10 7.40
and "NO" incorrect) 300 65.72 93
Uur second experiment concerns dimension reduc- 190 60.49 17 90
tion by RtP. Let us show the result. The table shows the 180 59.13 18.40
170 57.83 20.19relationship between reduced dimension (" dimension")10 5.7 89
and the correctness ratio ( correctness") "Drop" 140 58.48 19.29
mashow worse the result goes by dimension reuuc- 130 57 17 21 10
tion. Finally a table 4 contains the number of words Table 3: Dimension Reduction and Correctand the correctness ratio in a sense of 2-gram model.
247
tness
topic words correctness
C1 1029 100 0
C2 1481 15.8
C3 1991 15.0
C4 2075 60.0
C5 1672 8.3
C6 1255 100.0
C7 710 100.0
C8 1915 53.3
C9 1592 53.9
CIO 1765 14.3
Table 4: Topic Identification in 2-gram model
4.3 Discussion (Experiment 1)
In the table 1 we have 72.46 % of correctness ratio
and we can say the topic word model is valid in our
case. By closer look at the table, we see the lower
ratios about both C3 and C4. C3 and C4 contain
many words and another table 5 shows the number of
common words between the training data of 2 topics.
There is no sharp distortion to C3 nor C4, but we see
slightly large amount of common words here. Similar
discussion holds for C8.
_ C2 C3 T C4 - C5 C6o C7 - C8 C9 CIO
C1 221 267 - 266 - 249 206 - 142 - 246 245 228
C2 - 352 322 320 263 186 309 321 280
C3 439 427 326 231 380 431 392
C4 447 354 213 407 407 381
C5 289 214 349 386 352
C6 - 178 303 301 312
C7 = = = = = = 219 209 193
C8 :- 355 348
C9 358
Table 5: Common Words between 2 Topics
4.4 Discussion (Experiment 2)
In the table 3, we see 67% of correctness ratio with 500
dimension, and 19.30 % drop of the correctness ratio
with 140 dimension (98.59% reduction ratio). These
mean dimension reduction by RP works very well with
topic identification. A table 6 shows how we change
topic identification with 140 dimension compared to no
dimension reduction2. In the table "NO/YES" means
incorrect identification changed to correct one, and
"YES/NO" means correct to incorrect. "NoChange"
means there is no change of identified topics and "Un-
known" means no distinction (50% trials say YES and
50% NO). Clearly there is little change found and di-
mension reduction doesn't cause severe effect to the
topic word model.
topic NO/YES YES/NO NoChange Unknown
C1 0 0 100 0
C2 0 35.29 64.71 11.76
C3 9.09 0 90.9 36.36
C4 18.75 81.25 25
C5 0 0 100 50
C6 0 23.08 76.92 15.38
C7 0 28.57 71.43 7.14
C8 38.46 0 61.54 15.38
C9 16.67 33.33 50 8.33
CIO 0 85.71 14.29 0
Table 6: Changes of Topic Identification (140 dimen-
sion)
4.5 Discussion (Experiment 3)
A table 4 shows the correctness ratios which is rather
worse compared to the table 1 except Cl, C6 and C7.
2Remember the values are the average ratios of 10 times tri-
als.
But we see there are many scenes identified to C1, C6
and C7 as illustrated in a table 7. Also we see small
amount of words appeared in Cl, C6 and C7, as shown
in the table 4.
Topic I st l2nd |3rd
C1I 92 39
Co 3 9 96
C7 III 27 0
Table 7: Scenes Identified to Cl, C6 and C7 in 2-gram
model
Another table 8 shows the comparison of the num-
ber of words appeared only once in 1-gram model and
2-gram model. We see that, in 1-gram model, the
56.12% of the words appear many times while 8.47% in
2-gram model. x2 values depend on how many words
of multiple appearance we have, and 2-gram model
may cause distortion for topic identification.
In conclusion, we can't assume a topic word model
of our case in 2-gram model.
dimension only once rati of multiple appearance
2-gram 65166 59648 8.47%
1-gram 9923 4354 56 .12%
Table 8: Words Appeared Only Once
5 Conclusion
In this investigation, we have proposed dimensional-
ity reduction suitable for topic identification, and dis-
cussed Random Projection is really useful. By experi-
mental results, it is shown that we can assume a topic
word model in 1-gram approach and that RP tech-
nique allows us to keep good validity even if we reduce
98.59% of dimensionality. On the other hand, in a case
of 2-gram approach, we can't assume topic word model
itself.
References
[1] Achloiptas, D.: Database-friendly random projec-
tions, ACM-PODS 2001, pp.274-281
[2] Grossman, D. and Frieder, O.: Information Re-
trieval - Algorithm and Heuristics, Kluwer Aca-
demic Press, 1998
[3] Murakami, M.: Who is Shakespeare ?,
BungeiShunju-Sha, 2004 (in Japanese)
[4] Oh'uchi, H., Miura, T. and Shioya, I.: Document
Retrieval using Projection by Frequency Distribu-
tion, IEEE ICTAI, 2005, pp.356-361
[5] The Complete Works of William Shakespeare,
http://shakespeare.mit.edu/works.html
[6] E.Stamatos, N.Fakotakis, G.Kokkinakis: Auto-
matic Authorship Attribution, EACL, 1999
[7] M.Steyvers, P.Smyth, T.Griffiths : Probabilistic
Author-Topic Models for Information Discovery,
KDD, 2004
248
