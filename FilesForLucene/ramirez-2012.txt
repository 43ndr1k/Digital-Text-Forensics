96 IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 16, NO. 1, FEBRUARY 2012
A Rule-Based Evolutionary Approach to Music
Performance Modeling
Rafael Ramirez, Esteban Maestre, and Xavier Serra
Abstract—We describe an evolutionary approach to one of the
most challenging problems in computer music: modeling how
skilled musicians manipulate sound properties such as timing
and amplitude in order to express their view of the emotional
content of musical pieces. Starting with a collection of audio
recordings of real performances, we apply a sequential-covering
genetic algorithm in order to obtain computational models for
different aspects of expressive performance. We use these models
to automatically synthesize performances with the timing and
energy expressiveness that characterizes the music generated
by a professional musician. The reported results indicate that
evolutionary computation is an appropriate technique for solving
the problem considered. Specifically, our evolutionary algorithm
provides a number of potential advantages over other supervised
learning algorithms, such as a method for non-deterministically
obtaining models capturing different possible interpretations of
a musical piece.
Index Terms—Artificial intelligence, genetic algorithms, intel-
ligent systems, music.
I. Introduction
IN THE PAST, evolutionary computation [5] has been con-sidered in musical applications. One of the music domains
in which evolutionary computation has made most impact is
music composition. A large number of evolutionary systems
for composing musical material has been proposed (e.g., [3],
[11], [27], [39]). In addition to music composition, evolution-
ary computing has been considered in music improvisation
applications where an evolutionary algorithm typically models
a musician improvising (e.g., [1]). Nevertheless, there has
been very little research focusing on the use of evolutionary
computation to tackle one of the most challenging problems in
computer music: modeling how skilled musicians manipulate
sound properties (e.g., timing) in order to express their view
of the emotional content of musical pieces. This problem is
referred to as expressive performance modeling.
Traditionally, expressive performance has been studied us-
ing empirical approaches based on statistical analysis (e.g.,
[34]), mathematical modeling (e.g., [38]), and analysis-by-
synthesis (e.g., [6]). In all these approaches, it is a person
Manuscript received March 22, 2007; revised March 5, 2010 and June 23,
2010; accepted August 14, 2010. Date of publication September 22, 2011;
date of current version January 31, 2012. This work was supported by the
Spanish Ministry of Science and Innovation, under Grant TIN2009-14247-
C02-01, DRIMS Project.
The authors are with Universitat Pompeu Fabra, Barcelona 08018,
Spain (e-mail: rafael.ramirez@upf.edu; esteban.maestre@upf.edu;
xavier.serra@upf.edu).
Color versions of one or more of the figures in this paper are available
online at http://ieeexplore.ieee.org.
Digital Object Identifier 10.1109/TEVC.2010.2077299
who manually devises a theory or a mathematical model which
captures different aspects of musical expressive performance.
The theory or model is later tested on real performance data
in order to determine its accuracy.
In this paper, we describe an approach to investigate musical
expressive performance based on evolutionary computation.
Instead of manually modeling expressive performance, we let
a computer execute a sequential covering genetic algorithm to
automatically discover regularities and performance principles
from real data: audio recordings of real performances of
musical pieces. The algorithm combines sequential covering
[22] and genetic algorithms [10]. The sequential-covering
component of the algorithm incrementally constructs a set
of rules by learning new rules one at a time, removing the
positive examples covered by the latest rule before attempting
to learn the next rule. The genetic component of the algorithm
learns each of the new rules by applying a genetic algorithm.
The algorithm provides an interpretable specification of the
expressive principles applied to the performance of a piece of
music and, at the same time, it provides a generative model
of expressive performance, i.e., a model capable of generating
a computer music performance with the timing and energy
expressiveness that characterizes human generated music.
The use of evolutionary techniques for modeling expressive
music performance provides a number of potential advantages
over other supervised learning algorithms. In this context, the
evolutionary approach allows us to:
1) explore and analyze the induced expressive model as it
“evolves”;
2) guide and interact with the evolution of the model;
3) obtain different models resulting from different execu-
tions of the algorithm.
This last point 3 is of particular interest in the task of
modeling expressive music performance since it is desirable
to obtain non-deterministic models capturing the different
possible interpretations a performer may produce for a given
piece. In this paper we exploit 3 to induce non-deterministic
timing and energy expressive performance models.
The rest of this paper is organized as follows. Section
II reports on related work. Section III describes how we
extract a set of acoustic features from the audio recordings
in order to obtain a symbolic description of the different
expressive parameters embedded in the recordings. In Section
IV, we describe our evolutionary approach for inducing an ex-
pressive music performance computational model, and finally
1089-778X/$26.00 c© 2011 IEEE
RAMIREZ et al.: A RULE-BASED EVOLUTIONARY APPROACH TO MUSIC PERFORMANCE MODELING 97
Section V presents some conclusions and indicates some areas
of future research.
II. Related Work
A. Evolutionary Computation
Evolutionary computation has been considered with grow-
ing interest in musical applications [25]. A large number
of experimental systems using evolutionary techniques to
generate musical compositions have been proposed, including
Cellular Automata Music [23], a Cellular Automata Music
Workstation [12], CAMUS [24], MOE [4], GenDash [40],
CAMUS 3-D [20], Vox Populi [19], Synthetic Harmonies [2],
Living Melodies [3], and Genophone [18]. Composition sys-
tems based on genetic algorithms generally follow the standard
genetic algorithm approach for evolving musical materials
such as melodies, rhythms and chords. Thus, such composition
systems share the core approach with the one presented in this
paper. For example, Vox Populi [19] evolves populations of
chords of four notes, each of which is represented as a 7-bit
string. The genotype of a chord therefore consists of a string
of 28 bits (e.g., 1001011 0010011 0010110 0010101) and
the genetic operations of crossover and mutation are applied
to these strings in order to produce new generations of the
population. The fitness function is based on three criteria:
melodic fitness, harmonic fitness, and voice range fitness.
The melodic fitness is evaluated by comparing the notes of
the chord to a reference value provided by the user, the
harmonic fitness takes into account the consonance of the
chord, and the voice range fitness measures whether or not
the notes of the chord are within a range also specified by
the user. Evolutionary computation has also been considered
for improvisation applications such as [1], where a genetic
algorithm-based model of a novice jazz musician learning
to improvise was developed. The system evolves a set of
melodic ideas that are mapped into notes considering the chord
progression being played. The fitness function can be altered
by the feedback of the human playing with the system.
Nevertheless, very few works focusing on the use of evolu-
tionary computation for expressive performance analysis have
been done. In the context of the ProMusic project, Grachten
et al. [8] optimized the weights of edit distance operations by
a genetic algorithm in order to annotate a human jazz perfor-
mance. They presented an enhancement of edit distance based
music performance annotation. In order to reduce the number
of errors in automatic performance annotation, they used an
evolutionary approach to optimize the parameter values of cost
functions of the edit distance. In another study, Hazan et al.
[9] proposed an evolutionary generative regression tree model
for expressive rendering MIDI performances. Madsen et al.
[15] presented an approach to exploring similarities in music
classical piano performances based on simple measurements
of timing and intensity in 12 recordings of a Schubert piano
piece. The work presented in this paper is an extension to our
previous work [32], where we induced expressive performance
classification rules using a genetic algorithm. Here, in addition
to considering classification rules, we consider regression
rules, and while in [32] rules are independently induced by
the genetic algorithm, here we apply a sequential covering
algorithm in order to cover the whole example space.
B. Other Machine Learning Techniques
There have been several approaches for addressing expres-
sive music performance using machine learning techniques
other than evolutionary techniques. The most related work to
the research presented in this paper is the work by Lopez de
Mantaras et al. [14] and Ramirez et al. [30], [31].
Lopez de Mantaras et al. reported on SaxEx, a performance
system capable of generating expressive solo performances
in jazz. Their system is based on case-based reasoning, a
type of analogical reasoning where problems are solved by
reusing the solutions of similar, previously solved problems.
In order to generate expressive solo performances, the case-
based reasoning system retrieves, from a memory containing
expressive interpretations, those notes that are similar to the in-
put inexpressive notes. The case memory contains information
about metrical strength, note duration, and so on, and uses this
information to retrieve the appropriate notes. However, their
system does not allow one to examine or understand the way
it makes predictions.
Ramirez et al. explored and compared different machine
learning techniques for inducing both, an interpretable expres-
sive performance model (characterized by a set of rules) and
a generative expressive performance model. Based on this,
they described a performance system capable of generating
expressive monophonic jazz performances and providing “ex-
planations” of the expressive transformations it performs. The
work described in this paper has similar objectives but by using
a genetic algorithm it incorporates some desirable properties:
1) the induced model may be explored and analyzed while
it is “evolving;” 2) it is possible to guide the evolution of
the model in a natural way; and 3) by repeatedly executing
the algorithm different models are obtained. In the context of
expressive music performance modeling, these properties are
very relevant.
With the exception of the work by Lopez de Mantaras et al.
and Ramirez et al., most of the research in expressive per-
formance using machine learning techniques has focused on
classical piano solo music (e.g., [37], [42]), where the tempo
of the performed pieces is not constant and melody alterations
are not permitted (in classical music melody alterations are
considered performance errors). Thus, in these works the focus
is on global tempo and energy transformations while we are
interested in note-level timing and energy transformations as
well as in melody ornamentations which are a very important
expressive resource in jazz.
The induction of expressive performance models using
machine learning techniques has also been applied to the
identification of musicians from their playing styles. In this
context, Saunders et al. [35] applied string kernels to the
problem of recognizing famous pianists from their playing
style. The characteristics of performers playing the same piece
are obtained from changes in beat-level tempo and beat-
level loudness. From such characteristics, general performance
alphabets can be derived, and pianists’ performances can then
be represented as strings. They applied both kernel partial least
98 IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 16, NO. 1, FEBRUARY 2012
squares and support vector machines to this data. Stamatatos
and Widmer [36] addressed the problem of identifying the
most likely music performer, given a set of performances of
the same piece by a number of skilled candidate pianists. They
proposed a set of very simple features for representing stylistic
characteristics of a music performer that relate to a kind of
“average” performance. A database of piano performances of
22 pianists playing two pieces by Frédéric Chopin is used.
They proposed an ensemble of simple classifiers derived by
both subsampling the training set and subsampling the input
features. Experiments show that the proposed features are
able to quantify the differences between music performers.
Ramirez et al. [33] investigated how jazz saxophone players
express their view of the musical content of musical pieces
and how to use this information in order to automatically
identify performers. They studied deviations of parameters
such as pitch, timing, amplitude, and timbre both at an
inter-note level and at an intra-note level. Their approach to
performer identification consists of establishing a performer
dependent mapping of inter-note features (essentially a score
whether or not the score physically exists) to a repertoire of
inflections characterized by intra-note features. They presented
a successful performer identification case study.
III. Melodic Description
In this section, we describe how we extract a description
of a performed melody for monophonic recordings (for a
comparison of the method reported here and other methods,
see [7]). For each note in the recordings, we are interested
in obtaining a set of descriptors characterizing the musical
context in which the note appears (i.e., relative pitch and
duration of the neighboring notes as well as the musical
structures to which the note belongs).
A. Low-Level Descriptors Computation
The main low-level descriptors used to characterize expres-
sive performance are instantaneous energy and fundamental
frequency.
1) Energy Computation: The energy descriptor is com-
puted on the spectral domain, using the values of the ampli-
tude spectrum at each analysis frame. In addition, energy is
computed in different frequency bands as defined in [13], and
these values are used by the algorithm for note segmentation.
2) Fundamental Frequency Estimation: For the estimation
of the instantaneous fundamental frequency we use a harmonic
matching model derived from the two-way mismatch proce-
dure (TWM) [17]. For each fundamental frequency candidate,
mismatches between the harmonics generated and the mea-
sured partials frequencies are averaged over a fixed subset of
the available partials. A weighting scheme is used to make
the procedure robust to the presence of noise or absence of
certain partials in the spectral data. The solution presented in
[17] employs two mismatch error calculations. The first one is
based on the frequency difference between each partial in the
measured sequence and its nearest neighbor in the predicted
sequence. The second is based on the mismatch between each
harmonic in the predicted sequence and its nearest partial
neighbor in the measured sequence. This two-way mismatch
helps to avoid octave errors by applying a penalty for partials
that are present in the measured data but are not predicted,
and also for partials whose presence is predicted but which
do not actually appear in the measured sequence. The TWM
mismatch procedure has also the benefit that the effect of any
spurious components can be counteracted by the presence of
uncorrupted partials in the same frame.
First, we perform a spectral analysis of all the windowed
frames, as explained above. Second, the prominent spectral
peaks are detected. These spectral peaks are defined as the
local maxima in the spectrum which magnitude is greater than
a threshold. The spectral peaks are compared to a harmonic
series and a TWM error is computed for each fundamental
frequency candidate. The candidate with the minimum error
is chosen to be the fundamental frequency estimate.
After a first test of this implementation, some improvements
to the original algorithm were added to deal with some of the
algorithm’s shortcomings.
1) Peak selection: a peak selection routine has been added
in order to eliminate spectral peaks corresponding to
noise. The peak selection is done according to a mask-
ing threshold around each of the maximum magnitude
peaks. The form of the masking threshold depends on
the peak amplitude, and uses three different slopes de-
pending on the frequency distance to the peak frequency.
2) Context awareness: we take into account previous values
of the fundamental frequency estimation and instrument
dependencies to obtain a more adapted result.
3) Noise gate: a noise gate based on some low-level signal
descriptors is applied to detect silences, so that the
estimation is only performed in non-silent segments of
the sound.
B. Note Segmentation
Note segmentation is performed using a set of frame de-
scriptors, which are fundamental frequency and energy (in
different frequency bands). Energy onsets are first detected
following a band-wise algorithm that uses psycho-acoustical
knowledge [13]. In a second step, fundamental frequency
transitions are also detected. Finally, both results are merged
to find the note boundaries (onset and offset information).
C. Note Descriptor Computation
We compute note descriptors using the note boundaries
and the low-level descriptors values. The low-level descriptors
associated to a note segment are computed by averaging the
frame values within this note segment. Pitch histograms have
been used to compute the pitch note and the fundamental
frequency that represents each note segment, as found in [21].
This is done to avoid taking into account mistaken frames in
the fundamental frequency mean computation.
First, frequency values are converted into cents, by the
following formula:
c = 1200 ·
log ( f
fref
)
log2
(1)
RAMIREZ et al.: A RULE-BASED EVOLUTIONARY APPROACH TO MUSIC PERFORMANCE MODELING 99
Fig. 1. Prototypical Narmour structures.
where fref = 8.176. Then, we define histograms with bins
of 100 cents and hop size of 5 cents and we compute the
maximum of the histogram to identify the note pitch. Finally,
we compute the frequency mean for all the points that belong
to the histogram. The MIDI pitch is computed by quantization
of this fundamental frequency mean over the frames within the
note limits.
D. Musical Analysis
It is widely recognized that expressive performance is
a multi-level phenomenon and that humans perform music
considering a number of abstract musical structures. After
having computed the note descriptors as above, and as a first
step toward providing an abstract structure for the recordings
under study, we decided to use Narmour’s theory of perception
and cognition of melodies [26] to analyze the structure of the
music pieces performed.
The implication/realization model is a theory of percep-
tion and cognition of melodies. The theory states that a
melodic musical line continuously causes listeners to generate
expectations of how the melody should continue. Any two
consecutively perceived notes constitute a melodic interval and
if this interval is not conceived as complete, it is an implicative
interval, i.e., an interval that implies a subsequent interval with
certain characteristics. That is to say, some notes are more
likely than others to follow an implicative interval. Two main
principles recognized by Narmour concern registral direction
and intervallic difference. The principle of registral direction
states that small intervals imply an interval in the same
registral direction (a small upward interval implies another
upward interval and analogously for downward intervals), and
large intervals imply a change in registral direction (a large
upward interval implies a downward interval and analogously
for downward intervals). The principle of intervallic difference
states that a small (five semitones or less) interval implies a
similarly-sized interval (plus or minus 2 semitones), and a
large interval (seven semitones or more) implies a smaller
interval. Based on these two principles, melodic patterns or
groups can be identified that either satisfy or violate the impli-
cation as predicted by the principles. Fig. 1 shows prototypical
Narmour structures.
A note in a melody often belongs to more than one structure.
Thus, a description of a melody as a sequence of Narmour
structures consists of a list of overlapping structures. Each
melody in the training data is parsed in order to automatically
generate an implication/realization analysis of the pieces.
Fig. 2 shows the analysis for a fragment of a melody.
IV. Learning the Expressive Performance Model
In this section, we describe our approach to learning
an expressive music performance model from performance
Fig. 2. Narmour analysis of All of Me.
recordings. Our aim is to obtain a model capable of
both explain and automatically generate expressive music
performances.
A. Training Data
The training data used in our paper are monophonic audio
recordings (i.e., recordings of one instrument playing one note
at a time) of four jazz standards (Body and Soul, Once I Loved,
Like Someone in Love, and Up Jumped Spring) performed by a
professional musician while reading a score of the piece. Each
piece was performed at 11 different tempos around the nominal
tempo. For each piece, the nominal tempo was determined by
the musician as the most natural and comfortable tempo to
interpret the piece. Also for each piece, the musician identified
the fastest and slowest tempos at which a piece could be
reasonably interpreted. Interpretations were recorded at regular
intervals around the nominal tempo (5 faster and 5 slower)
within the fastest-slowest tempo limits. The data set is com-
posed of 4360 performed notes. Each note in the training data
is annotated with its corresponding performed characteristics
and a number of attributes representing both properties of the
note itself and aspects of the context in which the note appears.
Information about the note includes note duration and the
note metrical position within a bar, while information about
its melodic context includes performed tempo, information on
neighboring notes as well as the Narmour structure in which
the note appears (we focused on the Narmour group in which
the note appears in third position since this provides the best
indicator of the degree to which the note is expected).
B. Learning Task
We are concerned with note-level expressive transforma-
tions (w.r.t. the score), in particular transformations of note
duration, onset and energy, as well as note alteration (e.g.,
ornamentation). Initially, for each expressive transformation,
we approach the problem as a classification problem, e.g., for
note duration transformation we classify each note to belong
to one of the classes lengthen, shorten, or same-dur. Once we
obtain a classification mechanism capable of classifying all
notes in our training data, we apply a regression algorithm in
order to produce a numerical value representing the amount of
transformation to be applied to a particular note. The complete
algorithm is detailed in the next section.
The performance classes that interest us are lengthen,
shorten and same-dur for duration transformation, advance,
delay, and same-ons for onset deviation, soft, loud, and
medium for energy, and ornamentation and none for note
alteration. A note is considered to belong to class lengthen,
if its performed duration is 20% longer (or more) than its
nominal duration, i.e., its duration according to the score.
Class shorten is defined analogously. A note is considered
to be in class advance if its performed onset is 5% of a
100 IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 16, NO. 1, FEBRUARY 2012
bar earlier (or more) than its nominal onset. Class delay is
defined analogously. A note is considered to be in class loud
if it is played louder than its predecessor and louder than the
average level of the piece. Class soft is defined analogously.
We decided to set these boundaries after experimenting with
different ratios. The main idea was to guarantee that a note
classified, for instance, as lengthen was purposely lengthened
by the performer and not the result of a performance inexac-
titude. A note is considered to belong to class ornamentation
if a note or group of notes not specified in the score has been
introduced in the performance to embellish the note in the
melody, and to class none otherwise.
C. Algorithm
We applied a genetic sequential covering algorithm to the
training data. Roughly, the algorithm incrementally constructs
a set of rules by learning new rules one at a time, removing the
positive examples covered by the latest rule before attempting
to learn the next rule. Rules are learned using a genetic algo-
rithm evolving a population of rules with the usual mutation
and crossover operations.
For each class of interest (e.g., lengthen, shorten, same),
we collect the rules with best fitness during the evolution of
the population. For obtaining rules for a particular class of
interest (e.g., lengthen) we consider as negative examples the
examples of the other two complementary classes (e.g., shorten
and same).
In the case of note duration, onset, and energy, once we
obtain the set of rules covering all the training examples, for
each rule, we apply linear regression to the examples covered
by the rule in order to obtain a linear equation predicting
a numerical value. This leads to a set of rules producing a
numerical prediction and not just a nominal class prediction.
In the case of note alteration we do not compute a numerical
value. Instead we simply store the set of covered examples by
the rule. Later, for generation, we apply a standard k-nearest
neighbor algorithm to select one of the covered examples by
the rule and adapt the selected example to the new melodic
context, i.e., transpose the ornamental note(s) to fit the melody
key and ornamented note pitch. The algorithm is as follows:
GeneticSeqCovAlg(Class, Fitness, Threshold, p, r, m, Examples)
Pos = examples which belong to Class
Neg = examples which do not belong to Class
Learned_rules = {}
While Pos do
P = generate p hypotheses at random
For each hypothesis h in P,
Compute fitness(h)
While max(fitness(h))<Threshold and #generations<400 do
Create a new generation Pnew
P = Pnew
For each h in P,
Compute fitness(h)
NewRule = the hypothesis in P that has the highest fitness
Rpos = members of Pos covered by NewRule
Compute PredictedValue(Rpos)
NumericNewRule = NewRule with Class replaced by
Regression(Rpos)
Learned_rules = Learned_rules + NumericNewrule
Pos = Pos - Rpos
Return Learned_rules
The outer loop learns new rules one at a time, removing the
positive examples covered by the latest rule before attempting
TABLE I
Parameter Values of the Genetic Algorithm
Parameter Identifier Value
Crossover rate r 0.8
Mutation rate m 0.05
Population size p 200
to learn the next rule. The inner loop performs a genetic search
through the space of possible rules in search of a rule with
high accuracy. At each iteration, the outer loop adds a new
rule to its disjunctive hypothesis, Learned rules. The
effect of each new rule is to generalize the current disjunctive
hypothesis (i.e., increasing the number of instances it classifies
as positive) by adding a new disjunct. At this level, the search
is a specific-to-general search starting with the most specific
hypothesis (i.e., the empty disjunction) and terminating when
the hypothesis is sufficiently general to cover all training
examples. NumericNewRule is a rule where the consequent
Regression(Rpos) is a linear equation
X = w0 + w1a1 + w2a2 + . . . + wkak
where X is the predicted value expressed as a linear combina-
tion of the attributes a1, . . . ak of the training examples with
predetermined weights w0, . . . wk. The weights are calculated
using the set of positive examples covered by the rule Rpos
by linear regression. In the case of note alteration, i.e.,
when dealing with ornamentations, Regression(Rpos) is
simply the set examples covered by the rule.
The inner loop performs a finer-grained search to determine
the exact form of each new rule. This is done by applying
a genetic algorithm with the usual parameters r, m, and p,
specifying the fraction of the parent population replaced by
crossover, the mutation rate, and population size, respectively.
The values for these parameters were determined empirically
and are presented in Table I. In the inner loop, a new
generation is created as follows.
1) Select: probabilistically select (1 − r)p members of P
to add to Ps. The probability of Pr(hi) of selecting
hypothesis hi from P is
Pr(hi) =
Fitness(hi)
(hj)
(1 ≤ j ≤ p).
2) Crossover: probabilistically select (r∗p)2 pairs of hypoth-
esis from P [according to Pr(hi) above]. For each pair,
produce an offspring by applying the crossover operator
(see below) and add it to Ps.
3) Mutate: choose m percent of the members of Ps with
uniform probability and apply the mutation operator (see
below).
1) Hypothesis Representation: The hypothesis space of
rule preconditions consists of a conjunction of a fixed set
of attributes (see Fig. 3, Tables II, and III). Each rule is
represented as a bit-string as follows: the previous and next
note duration are represented each by five bits (i.e., much
shorter, shorter, same, longer and much longer), previous and
next note pitch interval are represented each by five bits (i.e.,
much lower, lower, same, higher, and much higher), metrical
RAMIREZ et al.: A RULE-BASED EVOLUTIONARY APPROACH TO MUSIC PERFORMANCE MODELING 101
Fig. 3. Hypothesis representation.
strength by five beats (i.e., very weak, weak, medium, strong,
and very strong), tempo by three bits (i.e., slow, nominal,
and fast) and Narmour groups by three bits (coding the eight
groups in Fig. 2). For example, in our representation the rule
“if the previous note duration is much longer and its pitch is
the same and it is in a very strong metrical position and the
current note appears in Narmour group R then lengthen the
duration of the current note”
is coded as the binary string
00001 11111 00100 11111 00001 111 110 001.
The exact meaning of the adjectives which the particular
bits represent are as follows. Previous and next note durations
are considered much shorter if the duration is less than half of
the current note, shorter if it is shorter than the current note but
longer than its half, and same if the duration is the same as the
current note. Much longer and longer are defined analogously.
Previous and next note pitches are considered much lower if
the pitch is lower by a minor third or more, lower if the pitch
is within a minor third, and same if it has same pitch. Higher
and much higher are defined analogously. The note’s metrical
position is very strong, strong, medium, weak, and very weak
if it is on the first beat of the bar, on the third beat of the
bar, on the second or fourth beat, offbeat, and in none of the
previous, respectively. The piece was played at slow, nominal,
and fast tempos if it was performed at a speed slower of more
than 15% of the nominal tempo (i.e., the tempo identified as
the most natural by the performer), within 15% of the nominal
tempo, and faster than 15% of the nominal tempo, respectively.
In the case of the note’s Narmour groups we decided to code
only one Narmour group for each note. This is, instead of
specifying all the possible Narmour groups for a note, we
select the one in which the note appears in third position (if
there is no such group, we consider one in which the note
appears either in first or second position, in that order).
2) Genetic Operators: We use the standard single-point
crossover and mutation operators with two restrictions. In
order to perform a crossover operation of two parents the
crossover points are chosen at random as long as they are
on the attributes sub string boundaries. Similarly the mutation
points are chosen randomly as long as they do not generate
inconsistent rule strings, e.g., only one class can be predicted
so exactly one 1 can appear in the last three bit sub string.
3) Fitness Function: The fitness of each hypothesized rule
is based on its classification accuracy over the training data.
In particular, the function used to measure fitness is
tpα
(tp + fp)
TABLE II
Hypothesis Bit Encoding
Attribute Bit Meaning
Previous duration Bit 1: much shorter than current note
Bit 2: shorter than current note
Bit 3: same than current note
Bit 4: longer than current note
Bit 5: much longer than current note
Next duration Bit 1: much shorter than current note
Bit 2: shorter than current note
Bit 3: same than current note
Bit 4: longer than current note
Bit 5: much longer than current note
Previous interval Bit 1: much lower than current note
Bit 2: lower than current note
Bit 3: same than current note
Bit 4: higher than current note
Bit 5: much higher than current note
Next interval Bit 1: much lower than current note
Bit 2: lower than current note
Bit 3: same than current note
Bit 4: higher than current note
Bit 5: much higher than current note
Metrical strength Bit 1: very weak
Bit 2: weak
Bit 3: medium
Bit 4: strong
Bit 5: very strong
Tempo Bit 1: slow
Bit 2: nominal
Bit 3: fast
Narmour group 000 = “P” group
001 = “D” group
010 = “ID” group
011 = “IP” group
100 = “VP” group
101 = “R” group
110 = “IR” group
111 = “VR” group
Class Bit 1: shorten/delay/soft/ornamentation
Bit 2: same-duration/same-onset/medium/none
Bit 3: lengthen/advance/loud/none
where tp is the number of true positives, fp is the number
of false positives, and α is a constant which controls the
true positives to false positives ratio. Often α is set to 1
which results in the standard fitness function tp(tp+fp) . This
fitness function favors individuals covering a small number
of true positive and 0 false positives (resulting in a fitness
value of 1) over individuals covering a large number of true
positives and 1 false positive (resulting in a fitness value of
less than 1). In our application this is an undesirable property
of the fitness function since we are interested in inducing
general expressive performance rules covering a large number
of examples (possibly including a small number of false
positives). Thus, in our algorithm we have set α = 1.15 which,
for our application, is a good compromise between coverage
and accuracy.
D. Results
These results presented in this section were obtained by
performing a 10-fold cross validation on the data. For each fold
we executed our algorithm 30 times and computed the average
of the results. At each fold, we removed the performances sim-
ilar to the ones selected in the test set, i.e., the performances
102 IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 16, NO. 1, FEBRUARY 2012
TABLE III
Definitions of Note’s Properties and Relationships
Relation Definition
N1 much shorter than N2 Dur(N1) < 12Dur(N2)
N1 shorter than N2 12Dur(N2) ≤ Dur(N1) < Dur(N2)
N1 same than N2 Dur(N1) = Dur(N2)
N1 longer than N2 Dur(N2) < Dur(N1) ≤ 12Dur(N2)
N1 much longer than N2 12Dur(N1) < Dur(N2)
N1 much lower than N2 Pitch(N1) + 3 < Pitch(N2)
N1 lower than N2 Pitch(N2) − 3 ≤ Pitch(N1) < Pitch(N2)
N1 same than N2 Pitch(N1) = Pitch(N2)
N1 higher than N2 Pitch(N2) < Pitch(N1) ≤ Pitch(N2) + 3
N1 much higher than N2 Pitch(N2) + 3 < Pitch(N1)
Metrical strength of N is very strong N’s onset is on beat 1
Metrical strength of N is strong N’s onset is on beat 3
Metrical strength of N is medium N’s onset is on beat 2 or 4
Metrical strength of N is weak N’s onset is offbeat
Metrical strength of N is very weak none of the above
Tempo is slow Tempo < 85% of nominal tempo
Tempo is nominal 85% of nominal tempo < Tempo < 115% nominal tempo
Tempo is fast 115% of nominal tempo < Tempo
of the same piece at tempos within 10% of performances in
the test set.
1) Classification Rules: Tables IV and V show the re-
sults obtained by our sequential covering genetic algorithm
and by a decision tree induction algorithm for the different
expressive performance models. A decision tree induction
algorithm recursively constructs a tree by selecting at each
node the most relevant attribute. This process gradually splits
up the training set into subsets until all instances at a node
have the same classification. The selection of the most rele-
vant attribute at each node is based on the information gain
associated with each node of the tree (and corresponding set
of instances). We have applied the decision tree building algo-
rithm C4.5 [28], [29] with postpruning (using subtree raising)
as implemented in the WEKA data mining software [41]. In
the case of our algorithm, the obtained classification accuracy
(measured in correctly classified instances percentage) of the
duration, onset, energy, and melody alteration models are
84.17%, 80.26%, 83.67%, and 91.03%, respectively. These
results are consistently higher than the ones obtained by
applying the C4.5 algorithm (only the onset model accuracy
is slightly lower than the accuracy obtained by C4.5). The
average number of rules generated by our algorithm for the
duration model is 97.3, 102.5 for the onset model, 91.6 for the
energy model, and 14.1 for the melody alteration model. The
number of rules generated by C4.5 is larger for every learned
model.
In order to test the stability of the model with respect to
changes in parameter settings we ran the algorithm with six
combinations of parameter values. The different combinations
of values for parameters r, m, and p are shown in Table VI
and the accuracy of the learnt models based on these values
is shown in Table VII. The parameter settings are organized
in three pairs, in each pair only one parameter is varied.
Although we have not performed an extensive exploration of
the parameters values, the obtained results seem to indicate
that the models are relatively stable to changes in the values
of the algorithm parameters.
TABLE IV
Accuracy (in Correctly Classified Instances Percentage) of
the Duration, Onset, Energy, and Ornamentation Models
Obtained by Our Sequential Covering Genetic Algorithm and
by the C4.5 Decision Tree Induction Algorithm
Model SCGA C4.5
Duration 84.17 80.61
Onset 80.26 80.48
Energy 83.67 78.14
Ornamentation 91.03 88.65
TABLE V
Number of Rules in the Duration, Onset, Energy, and
Ornamentation Models Obtained by Our Sequential Covering
Genetic Algorithm (SCGA) and by the C4.5 Decision Tree
Induction Algorithm
Model SCGA C4.5
Duration 97.3 211
Onset 102.5 89
Energy 91.6 333
Ornamentation 14.1 57
TABLE VI
Genetic Algorithm Parameter Settings
Setting r m p
r1 0.7 0.05 200
r2 0.9 0.05 200
m1 0.8 0.02 200
m2 0.8 0.08 200
p1 0.8 0.05 100
p2 0.8 0.05 300
Fig. 4 shows a typical evolution of the fitness evaluation
for the best duration rule along generations (similar behavior
was observed in the evolution of the onset, energy and
melody alteration rules). As shown in Fig. 4, a substantial
RAMIREZ et al.: A RULE-BASED EVOLUTIONARY APPROACH TO MUSIC PERFORMANCE MODELING 103
TABLE VII
Accuracy (in Correctly Classified Instances Percentage) of
the Duration, Onset, Energy and Ornamentation Models
Obtained by the Different Algorithm Parameter Settings in
Table VI
Model r1 r2 m1 m2 p1 p2
Duration 81.21 84.03 79.43 80.16 83.92 84.46
Onset 71.34 78.82 75.06 77.68 79.84 80.61
Energy 83.16 84.02 83.59 81.87 83.25 82.96
Ornamentation 78.21 85.34 90.76 86.61 90.37 91.82
Fig. 4. Typical evolution of best rule fitness value.
Fig. 5. Coverage for each of the rules in the duration model.
improvement of rule fitness value is observed up to around
generation 250, while limited improvement is observed after
that. Increase of mutation rate initially (i.e., before generation
90) produced faster increase in fitness value but poorer final
fitness values. Figs. 5–8 show for each rule in the duration
model, its coverage, number of true positives, accuracy, and
fitness value, respectively. Similar results were obtained for the
onset and energy models. As it can be seen in the figures, while
there is some correlation among the trends of the coverage,
true positives, and fitness values, there is not correlation with
these and the accuracy values. This fact is expected since the
fitness function was designed to penalize individuals covering
a small number of examples (even if they cover 0 false
positives) over individuals covering a large number of true
positives and few false positives.
2) Regression Models: We have evaluated the expressive
performance model by comparing its transformation predic-
tions and the actual transformations performed by the musi-
cian. Figs. 9 and 10 show the note-by-note duration ratio pre-
dicted by two different models induced by different executions
of the algorithm and compare it with the actual duration ratio
Fig. 6. Number of true positives for each of the rules in the duration model.
Fig. 7. Accuracy for each of the rules in the duration model.
Fig. 8. Fitness value for each of the rules in the duration model.
in the recording. When evaluating the model, we excluded
from the training data the piece to be predicted by the model.
Similar results were obtained for the predicted onset deviation
(see Fig. 11) and energy variation (see Fig. 12). As illustrated
by Figs. 9–12, the induced model seems to accurately capture
the musician’s expressive performance transformations.
The correlation coefficient for the onset, duration, and
energy models is 0.80, 0.84, and 0.86, respectively. These
numbers were obtained, as described at the beginning of this
section, by performing a 10-fold cross validation procedure.
At each fold, we ran the sequential covering genetic algo-
rithm 30 times in order to observe the differences between
the correlation coefficient of different runs. We observed no
substantial differences. The computed correlation coefficient
variances for the onset, duration, and energy models were 0.5,
0.2, and 0.3, respectively. As a result of executing the genetic
algorithm several times we obtained different models. These
104 IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 16, NO. 1, FEBRUARY 2012
Fig. 9. Correlation between model predicted duration values and the actual
performed values in Body and Soul at a tempo of 65.
Fig. 10. Correlation between second model predicted duration values and
the actual performed values in Body and Soul at a tempo of 65.
Fig. 11. Correlation between model predicted onset deviation values and the
actual performed onset deviation values in Body and Soul at a tempo of 65.
Fig. 12. Correlation between model predicted note energy and the actual
performed energy in Body and Soul at a tempo of 65.
models clearly share similar performance trends but at the
same time generate slightly different expressive performances
(e.g., compare Figs. 9 and 10).
In order to obtain a perceptual evaluation of the output
generated by the induced models, we have conducted a series
of blind listening trials. The goal of these trials is to discover
if the participants are able to distinguish between human and
computer generated performances. The experiment involved
21 healthy participants (17 male, 4 female) with age mean of
26.6 years old (between 22 and 34 years old). Out of this 21
participants, 10 declared to have some musical training (more
than 5 years music training). The experiment consisted of
two parts. In part 1, participants were presented with 10 pairs
of synthesized melody fragments, one produced by a human
musician and one generated by the induced computational
models (in each pair the melody was the same). Participants
were asked to identify the melody performed by the human.
The average number of correct answers was 5.09/10. This
indicates that participants are not able to distinguish between
the presented human and computer generated performances.
In order to discard the possibility that the participants could
not distinguish the two types of stimuli in part 1 due to lack
of music sensitivity, in part two participants were presented
with ten pairs of melody fragments: one produced by a
human, and the other a synthesized transcription of the score
of the first one. Again, participants were asked to identify
the melody performed by the human. This time, the average
number of correct answers was 8.14/10. We interpret these
results as a perceptual confirmation of our claim that the
induced model accurately capture the musician’s expressive
performance transformations.
E. Discussion
The difference between the results obtained for the classi-
fication models, and the accuracy of a baseline classifier, i.e.,
a classifier guessing at random, indicates that: 1) the audio
recordings used in the study contain sufficient information to
build an expressive model capable of predicting the different
expressive transformations introduced by the performer, and 2)
the proposed algorithm is capable of learning such a model.
It is worth noting that every run of the algorithm produced
significantly better than random classification accuracies for
every expressive dimension. This supports our claim that evo-
lutionary computation is an appropriate technique for solving
the problem considered.
As mentioned before, the use of evolutionary techniques for
modeling expressive music performance provides advantages
over other supervised learning algorithms. In particular, the
possibility of obtaining different models resulting from differ-
ent executions of the algorithm is of particular interest in the
context of this research. The stochastic component in genetic
algorithms allows us to capture the interpretation variability
of human performers. By storing the output (i.e., rules) of
different executions of the algorithm we allow the user of the
expressive performance system to generate different expressive
performances of the same piece. As shown in Section IV-D,
expressive performances generated in this way, while not being
identical, they capture the style of the musician modeled.
One of the aims of the algorithm is to produce both an
interpretable and a generative model of expressive music
performance. However, rules initially obtained by the algo-
rithm turned out to be difficult to interpret/understand. A
second advantage of the evolutionary approach to performance
modeling is the possibility of guiding the evolution of the
model in a simple way by specializing the genetic operators. In
order to improve the interpretability of the rules, restrictions on
RAMIREZ et al.: A RULE-BASED EVOLUTIONARY APPROACH TO MUSIC PERFORMANCE MODELING 105
the rules form may be introduced. This is, it is straightforward
to impose restrictions on the rules syntax (e.g., allow only
some bit sequences) during the evolution in order to enhance
the interpretability of the induced rules. In this paper, we have
restricted the rules to group the 1s within attribute descriptions.
For instance, for a particular attribute, say previous note
duration, a description of 10101 (interpreted as the previous
note is either much shorter, same duration or much longer than
the current note) is not allowed, instead only descriptions with
contiguous 1s are produced, e.g., 11000 (interpreted as the
previous note is shorter than the current note). This restriction
is implemented by imposing it on the initial population of
rules and then by restricting both the crossover and mutation
operators to maintain it. The accuracy results obtained with
and without syntax restrictions are comparable. After imposing
the described syntax restriction, the obtained accuracies for
the duration, onset, energy, and ornamentation models are
83.4%, 78.6%, 79.1%, and 90.4%, respectively. The reason
is that unrestricted rules are more prone to overfit the data.
The accuracy of unrestricted rule models was considerably
higher when evaluated on the training set compared to when
evaluated by the described 10-fold cross validation procedure.
A third advantage of the evolutionary approach in the
context of this paper is that it allows us to explore and
analyze the induced expressive model as it “evolves.” We
examined some of the classification rules the algorithm
produced at different times of the evolution. As expected,
the most interesting rules, i.e., the rules with highest fitness
value, appear at the end of the algorithm execution. By
examining the induced rules, we observed rules of different
types. Some focus on features of the note itself and depend
on the performance tempo while others focus on the Narmour
analysis and are independent of the performance tempo. Rules
referring to the local context of a note, i.e., rules classifying a
note solely in terms of the timing, pitch and metrical strength
of the note and its neighbors, as well as compound rules that
refer to both the local context and the Narmour structure were
discovered. In order to illustrate the types of rules found we
present some examples of duration rules.
RULE1: 11111 01110 11110 00110 00011 010 010 001
“In nominal tempo, if the duration of the next note is similar
and the note is in a strong metrical position and the note
appears in a D Narmour group then lengthen the current note.”
RULE2: 00111 00111 00011 11111 11100 111 111 100
“If the previous and next notes durations are longer (or equal)
than the duration of the current note and the pitch of the
previous note is higher and we are in a weak metrical position
then shorten the current note.”
RULE3: 01000 11100 01111 01110 00111 111 111 010
”If the previous note is slightly shorter and not much lower
in pitch, and the next note is not longer and has a similar
pitch (within a minor third), and the current note is not on a
weak metrical position, then the duration of the current note
remains the same (i.e., no lengthening or shortening).”
Fig. 13. Expressive performance system interface showing the piano roll of
the unexpressive score (upper half) and the predicted expressive performance
(lower half). In the piano roll, the higher the position of a bar the higher
the pitch of the corresponding note, and the length of a bar represents
the corresponding note duration. The darkness of each note represents its
energy, i.e., volume (the darker the louder). As can be seen, in the expressive
performance the note durations, onsets, and energy have been predicted by
the corresponding models. The slider on the bottom right specifies the tempo
for the predicted performance.
Some of the rules turn out to be of musical interest. For
instance RULE1 above states that a note is to be lengthened
if the two previous notes have the same pitch (i.e., appears
in a D Narmour group) and it has similar duration to the
following note. This rule may represent the performer’s
intention to differentiate the last note of a sequence of notes
with the same pitch.
In this paper, we have decided to focus on single-performer
expressive performance modeling. This is, we have considered
performances recorded by only one musician, as opposed to
performances by several musicians. The rationale for this
is that expressive performance styles across different mu-
sicians can vary considerably and thus learning from data
produced by more than one musician may be problematic.
Two musicians performing the same piece in two substantially
different expressive styles could be treated as noise by the
learning algorithm since the performed notes would appear in
exactly the same musical context but would contain different
expressive transformations. Nevertheless, an interesting area
of research in expressive music performance is the modeling
of the different performance styles of different musicians and
using the models to identify performers (e.g., [33], [36]).
Music Synthesis: Based on the expressive music perfor-
mance model induced we have implemented an expressive
performance system (for a detailed description please refer to
[16]). The system is able to generate either an expressive MIDI
or audio performance from an inexpressive description of a
melody (i.e., the score). Fig. 13 shows the system’s interface.
The system generates the audio sequence based on the
predictions of the expressive performance model and an an-
notated sample database consisting of the original individual
performed notes. First, the expressive-performance modeling
component is fed with the input score, and a prediction of
an enriched note sequence is obtained (see Fig. 13). For each
note in the new sequence, a candidate list containing possible
matching samples from the sample database is generated.
Then, the best note sample sequence is determined by paying
106 IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 16, NO. 1, FEBRUARY 2012
attention both to the cost of the transformations to be applied
and also to the concatenations involved. Selected samples
are analyzed in the spectral domain, and a representation
of their spectral regions is extracted. Notes (samples) are
transformed to fit the predicted note characteristics applying
global note amplitude transformation, pitch shift, and non-
linear time stretch for matching, respectively, dynamics, fun-
damental frequency, and duration of the target sequence. After
that, samples are concatenated by means of amplitude, pitch,
and spectral-shape interpolation applied to the resulting note
transitions to obtain a smooth reconstruction from release and
attack segments of adjacent notes.
The output of the expressive performance-modeling com-
ponent carries time (onset and duration), dynamics (en-
ergy), and melody alteration (ornamentation) information at
a note level (some sample audio performances produced
by our expressive performance system can be found at
www.dtic.upf.edu/∼rramirez/emp).
V. Conclusion
This paper described an evolutionary computation approach
for learning an expressive performance model from recordings
of jazz standards by a skilled saxophone player. Our objective
has been to find a computational model which predicts how a
particular note in a particular context should be played (e.g.,
longer or shorter than its nominal duration). In order to induce
the expressive performance model, we have extracted a set of
acoustic features from the recordings resulting in a symbolic
representation of the performed pieces and then applied a
sequential-covering genetic algorithm to the symbolic data
and information about the context in which the data appear.
The induced models accurately captured the musician’s ex-
pressive performance transformations. In addition, some of
the classification rules induced by the algorithm proved to be
of musical interest. The models produced by our algorithm
have been implemented in a working expressive performance
system. Currently, we are extending our model to be able
to predict intra-note expressive features such as vibrato, and
instantaneous energy.
References
[1] J. A. Biles, “GenJam: A genetic algorithm for generating jazz solos,” in
Proc. ICMC, 1994, pp. 131–137.
[2] E. Bilotta, P. Pantano, and V. Talarico, “Synthetic harmonies: An
approach to musical semiosis by means of cellular automata,” in Proc.
7th Artif. Life, 2000, pp. 537–546.
[3] P. Dahlstedt and M. G. Nordhal, “Living melodies: Coevolution of sonic
communication,” Leonardo, vol. 34, no. 3, pp. 243–248, 2001.
[4] B. Degazio, “La evolucion de los organismos musicales,” in Proc. XXIth
Musica Nuevas Tecnologias: Perspectivas Para Siglo, 1999, pp. 137–
148.
[5] K. A. De Jong, W. M. Spears, and D. F. Gordon, “Using genetic
algorithms for concept learning,” Mach. Learning, vol. 13, nos. 2–3,
pp. 161–188, 1993.
[6] A. Friberg, “A quantitative rule system for musical performance,” Ph.D.
dissertation, Dept. Speech, Music Hearing, Royal Instit. Technol. (KTH),
Stockholm, Sweden, 1995.
[7] E. Gómez, A. Klapuri, and B. Meudic, “Melody description and extrac-
tion in the context of music content processing,” J. New Music Res.,
vol. 32, no. 1, pp. 23–40, Mar. 2003.
[8] M. Grachten, J. L. Arcos, and R. L. de Mantaras, “Evolutionary
optimization of music performance annotation,” in Proc. CMMR, 2004,
pp. 347–358.
[9] A. Hazan, R. Ramirez, E. Maestre, A. Perez, and A. Pertusa, “Modeling
expressive performance: Aggression tree approach based on strongly
typed genetic programming,” in Proc. Eur. Workshop Evol. Music Art,
2006, pp. 676–687.
[10] J. H. Holland, Adaptation in Natural and Artificial Systems. Ann Arbor,
MI: University of Michigan Press, 1975.
[11] A. Horner and D. E. Goldberg, “Genetic algorithms and computer-
assisted music composition,” in Proc. Int. Comput. Music Conf., 1991,
pp. 479–482.
[12] A. Hunt, R. Kirk, and R. Orton, “Musical applications of a cellular
automata workstation,” in Proc. ICMC, 1991, pp. 165–166.
[13] A. Klapuri, “Sound onset detection by applying psychoacoustic knowl-
edge,” in Proc. IEEE ICASSP, Mar. 1999, pp. 3089–3092.
[14] R. L. de Mantaras and J. L. Arcos, “AI and music from composition to
expressive performance,” AI Mag., vol. 23, no. 3, pp. 43–57, 2002.
[15] S. T. Madsen and G. Widmer, “Exploring similarities in music perfor-
mances with an evolutionary algorithm,” in Proc. Int. FLAIRS Conf.,
2005, pp. 80–85.
[16] E. Maestre, R. Ramirez, and X. Serra, “Expressive concatenative syn-
thesis by (re)using samples from real performance recordings,” Comput.
Music J., vol. 33, no. 4, pp. 23–42, 2009.
[17] R. C. Maher and J. W. Beauchamp, “Fundamental frequency estimation
of musical signals using a two-way mismatch procedure,” J. Acoust. Soc.
Am., vol. 95, no. 4, pp. 2254–2263, 1994.
[18] J. Mandelis, “Genophone: An evolutionary approach to sound synthesis
and performance,” in Proc. ALMMA, 2002, pp. 108–119.
[19] J. Manzolli, A. Moroni, F. von Zuben, and R. Gudwin, “An evolutionary
approach applied to algorithmic composition,” in Proc. 6th Brazilian
Symp. Comput. Music, 1999, pp. 201–210.
[20] K. McAlpine, E. R. Miranda, and S. Hogar, “Composing music with
algorithms: A case study system,” Comput. Music J., vol. 23, no. 2, pp.
19–30, 1999.
[21] R. J. McNab, L. A. Smith, and I. H. Witten, “Signal processing for
melody transcription,” in Proc. 19th Australasian Comput. Sci. Conf.,
1996, pp. 22–28.
[22] R. S. Michalski, “On the quasi-minimal solution of the general covering
problem,” in Proc. 1st Int. Symp. Inform. Process., 1969, pp. 125–128.
[23] D. Millen, “Cellular automata music,” in Proc. ICMC, 1990, pp. 314–
316.
[24] E. R. Miranda, “Cellular automata music: An interdisciplinary music
project,” J. New Music Res., vol. 22, no. 1, pp. 3–21, 1993.
[25] E. R. Miranda, “At the crossroads of evolutionary computation and
music: Self-programming synthesizers, swarm orchestras and the origins
of melody,” Evol. Comput., vol. 12, no. 2, pp. 137–158, 2004.
[26] E. Narmour, The Analysis and Cognition of Basic Melodic Structures:
The Implication Realization Model. Chicago, IL: University of Chicago
Press, 1990.
[27] S. Phon-Amnuaisuk and G. Wiggins, “The four-part harmonization
problem: A comparison between genetic algorithms and a rule-based
system,” in Proc. AISB Symp. Musical Creativity, 1999, pp. 140–146.
[28] J. R. Quinlan, “Induction of decision trees,” Mach. Learning, vol. 1,
no. 1, pp. 81–106, 1986.
[29] J. R. Quinlan, C4.5: Programs for Machine Learning. San Francisco,
CA: Morgan Kaufmann, 1993.
[30] R. Ramirez, A. Hazan, E. Gómez, and E. Maestre, “Understanding
expressive transformations in saxophone jazz performances,” J. New
Music Res., vol. 34, no. 4, pp. 319–330, 2005.
[31] R. Ramirez, A. Hazan, E. Maestre, and X. Serra, “A data mining
approach to expressive music performance modeling,” in Multime-
dia Data Mining Knowledge Discovery. Berlin, Germany: Springer,
2006.
[32] R. Ramirez and A. Hazan, “Understanding expressive music perfor-
mance using genetic algorithms,” in Proc. Eur. Workshop Evol. Music
Art, LNCS 3449. 2005, pp. 508–516.
[33] R. Ramirez, E. Maestre, A. Pertusa, E. Gomez, and X. Serra,
“Performance-based interpreter identification in saxophone audio record-
ings,” IEEE Trans. Circuits Syst. Video Technol., vol. 17, no. 3, pp.
356–364, Mar. 2007.
[34] B. H. Repp, “Diversity and commonality in music performance: An
analysis of timing microstructure in Schumann’s ‘Traumerei,”’ J. Acoust.
Soc. Am., vol. 92, no. 5, pp. 2546–2568, 1992.
[35] C. Saunders, D. Hardoon, J. Shawe-Taylor, and G. Widmer, “Using
string kernels to identify famous performers from their playing style,”
in Proc. 15th ECML, 2004, pp. 384–395.
RAMIREZ et al.: A RULE-BASED EVOLUTIONARY APPROACH TO MUSIC PERFORMANCE MODELING 107
[36] E. Stamatatos and G. Widmer, “Automatic identification of music
performers with learning ensembles,” Artif. Intell., vol. 165, no. 1, pp.
37–56, 2005.
[37] A. Tobudic and G. Widmer, “Relational IBL in music with a new
structural similarity measure,” in Proc. Int. Conf. Inductive Logic Pro-
gramming, 2003, pp. 365–382.
[38] N. Todd, “The dynamics of dynamics: A model of musical expression,”
J. Acoust. Soc. Am., vol. 91, no. 6, pp. 3540–3550, 1992.
[39] N. Tokui and H. Iba, “Music composition with interactive evolu-
tionary computation,” in Proc. 3rd Int. Conf. Generative Art, 2000,
pp. 215–226.
[40] R. Waschka, II, “Avoiding the fitness bottleneck: Using genetic
algorithms to compose orchestral music,” in Proc. ICMC, 1999,
pp. 201–203.
[41] I. H. Witten and E. Frank, Data Mining: Practical Machine Learning
Tools and Techniques, 2nd ed. San Mateo, CA: Morgan Kaufmann,
2005.
[42] G. Widmer, “Machine discoveries: A few simple, robust local expression
principles,” J. New Music Res., vol. 31, no. 1, pp. 37–50, 2002.
Rafael Ramirez received the B.S. degree in math-
ematics from the National University of Mexico,
Mexico City, Mexico, and the M.S. degree in ar-
tificial intelligence and Ph.D. degree in computer
science, both from the University of Bristol, Bristol,
U.K.
He is currently an Associate Professor with the
Department of Information and Communication
Technologies, Universitat Pompeu Fabra, Barcelona,
Spain. Prior to joining the Universitat Pompeu Fabra,
he was a Lecturer with the Department of Computer
Science, National University of Singapore, Singapore. His current research
interests include artificial intelligence, music information retrieval, declarative
languages, music perception, and cognition.
Esteban Maestre received the M.A. degree in elec-
trical engineering from the Universitat Politécnica de
Catalunya, Barcelona, Spain, in 2003, and the Ph.D.
degree in computer science and digital communica-
tion from Universitat Pompeu Fabra, Barcelona, in
2009.
After a Research Internship with Philips Research
Laboratories, Aachen, Germany, he was a Teaching
Assistant with the Department of Electronics, Uni-
versitat Politécnica de Catalunya, until 2004. Then,
he joined the Music Technology Group, Universitat
Pompeu Fabra, as a Research Assistant, working also as a Teaching Assistant
with the Department of Technology. His current research interests span
from expressive audio analysis and synthesis to musical gestures coding and
rendering.
Xavier Serra received the Ph.D. degree from Stan-
ford University, Stanford, CA, in 1989, with a thesis
focused on spectral modeling of musical signals.
He is currently an Associate Professor with the De-
partment of Technology, Universitat Pompeu Fabra
(UPF), Barcelona, Spain. He is the Director of the
Music Technology Group, UPF, a group dedicated to
audio content analysis, description, and synthesis.
