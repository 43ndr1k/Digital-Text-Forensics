Expert Systems with Applications 39 (2012) 13480–13491Contents lists available at SciVerse ScienceDirect
Expert Systems with Applications
journal homepage: www.elsevier .com/locate /eswaUsing Wikipedia concepts and frequency in language to extract key terms
from support documents
M. Romero ⇑, A. Moreo, J.L. Castro, J.M. Zurita
Dep. of Computer Science and Artificial Intelligence, University of Granada, Spain
a r t i c l e i n f o a b s t r a c tKeywords:
Automatic Keyword Extraction
Support documents
FAQ
Wikipedia
Word sense disambiguation
Natural Language0957-4174/$ - see front matter  2012 Elsevier Ltd. A
http://dx.doi.org/10.1016/j.eswa.2012.07.011
⇑ Corresponding author. Address: C/Periodista Da
18071 Granada, Spain. Tel.: +34 958244019; fax: +34
E-mail addresses: manudb@decsai.ugr.es (M. Ro
(A. Moreo), castro@decsai.ugr.es (J.L Castro), zurita@dIn this paper, we present a new key term extraction system able to handle with the particularities of
‘‘support documents’’. Our system takes advantages of frequency-based and thesaurus-based approaches
to recognize two different classes of key terms. On the one hand, it identifies multi-domain key terms of
the collection using Wikipedia as knowledge resource. On the other hand, the system extracts specific key
terms highly related with the context of a support document. We use the frequency in language as a cri-
terion to detect and rank such terms. To prove the validity of our system we have designed a set of exper-
iment using a Frequently Asked Questions (FAQ) collection of documents. Since our approach is generic,
minor modifications should be undertaken to adapt the system to other kind of support documents. The
empirical results evidence the validity of our approach.
 2012 Elsevier Ltd. All rights reserved.1. Introduction
Nowadays, the hectic lifestyle force us to a continuous and fast
learning not only in the working environment, but also in our free
time. Fortunately, there are brief documents that help non-experts
users learn the main concepts of any topics. These documents,
known as ‘‘support documents’’ (SD), include the so-called how-
to guides, tutorials, FAQ lists or walkthroughs. Due to the large
quantity of documents published on the Internet, it would be
desirable that we had the use of proper tools or techniques to sup-
port classification, search, or other maintenance activities. In this
sense, an Automatic Keyword Extraction (AKE) method help users
to quickly skim over documents would be unarguably a valuable
contribution. Furthermore, it could also be applied for text summa-
rization, text clustering, and text classification (Wenchao, Lian-
chen, & Ting, 2009).
In classical AKE studies, the documents of the collection are
usually dependent on general-domains (containing world knowl-
edge from public sources covering high diffusion topics). In addi-
tion, each document presents extensive and self-describing
information about its main topic. This is not the case for SD. Their
topics are frequently referred to specific-domains (containing
knowledge from specific issues, usually of limited or private diffu-
sion). Moreover, a SD does not contain every detail of its topic, butll rights reserved.
niel Saucedo Aranda s/n, E-
958243317.
mero), moreo@decsai.ugr.es
ecsai.ugr.es (J.M Zurita).only specific information referred to significant aspects of the
topic.
From a more deeper analysis, we found two classes of important
terms1: (i) ‘‘Multidomain key terms’’ (MKT), meaningful in a multi-
domain context (e.g. ‘virtual assistant’ in Example 1 in Table 1);
and (ii) ‘‘Specific domain key terms’’ (SKT), proper nouns or technical
terms closely related to specific aspects of the document topic (e.g.
‘Interactive Dialog’ in Example 1 in Table 1).
The above mentioned characteristic motivated us to design an
hybrid automatic system to extract key terms from SD. In this
regard, a frequency-based approach seems not appropriate due
to the terms appearing rarely could be actually relevant to the doc-
ument. Similarly, as there is not a significant co-occurrence distri-
bution between terms in SD, a word association-based approach
would also be inappropriate. For these reason, we decided to com-
bine two strategies. (a) A thesaurus-based approach is employed to
detect MKT. Following the actual tendency in the literature, we de-
velop a controlled dictionary of ‘concepts’ drawn from Wikipedia
that includes acronyms, translations, and misspellings. (b) In con-
trast, SKT are hardly contained in any controlled dictionary due to
its technical nature. However, since they are uncommon terms in
their language, we take advantage of frequency in language analy-
sis.2 In addition, candidate key terms are previously filtered out by
means of a modular filter. Since our system is focused on FAQ docu-1 It should be pointed out that in this research we focus on FAQ lists which
rguably are the most used knowledge sharing format of support documents.
2 These values are stored in a dictionary extracted from a word frequency list
vailable at the Corpus of Contemporary American English (COCA) project (Davies,
011).a
a
2
Table 1
Example 1: Domain specific key terms of a FAQ document fragment.
FAQ document fragment Key terms
Q: How does it works? Interactive Dialog, Intelligent Process, Virtual
Assistant, NLP techniques
A: Our virtual assistant use high technology developed by our company: Interactive Dialog, and Intelligent Process.
Thanks to Interactive Dialog process, the interaction between users and the system is extended by means of
recommendations, suggestions and issue lists. Intelligent Process is a virtual conversation system. It uses Natural
Language Processing techniques to understand user’s questions.
M. Romero et al. / Expert Systems with Applications 39 (2012) 13480–13491 13481ments, we believe that it could be an useful tool for monitoring hu-
man FAQ administrators in the task of configuring FAQ retrieval sys-
tems (Tao, Liu, & Lin, 2011).
The rest of this paper is organized as follows. Section 2, offers an
overview of previous work on key term extraction. A brief sum-
mary of Wikipedia and its use to obtain a controlled vocabulary
of concepts is presented in Section 3. Next, we describe the fre-
quency in language feature of terms and its applications. Section
5 describes the structure and functions of our system. The method
of analysis and the experimental validation of our method are out-
lined in Section 6. Finally, Section 7 concludes with a discussion of
results and future research.2. Related works
In this section, we depict the characteristics of the main ap-
proaches in Keyword Extraction. Additionally, we analyse how
each proposal could be applied to extract the two types of key
terms in SD.
First works on this field were based on Machine Learning algo-
rithms. Supervised learning methods treat this task as a classifica-
tion problem using lexical, syntactic or statistical features (or a
mixture of them) of the training labelled data to extract keywords
(Csomai & Mihalcea, 2008; HaCohen-Kerner, Stern, Korkus, & Fredj,
2007; Lee, Isa, Choo, & Chue, 2012; Xu & Lau, 2010; Zhang, H. Xu, &
Li, 2006). In this way, supervised algorithms can extend any other
AKE approaches. The most classical approaches were suggested by
Turney (2000) and Frank, Paynther, Witten, Gutwin, and Nevil-
Manning (1999). In Hulth (2003), the author performed a rule
induction-based method using term frequency, collection fre-
quency, relative position of the first occurrence, and the POS tag
of terms as features. Another well-known proposal is KEA (Witten,
Paynte, Frank, Gutwin, & Nevill-Manning, 1999). This tool employs
a Naïve Bayes algorithm using the TF-IDF measure and the first
occurrence of a term as features. As can be seen, Machine Learning
methods depend on a training data set for and may provide poor
results when the training set does not fit well the processed docu-
ments. Collections of SD contain documents belonging to specific-
domains. Therefore, if new documents are added to a collection,
they belong probably to different domains not considered yet.
Moreover, if a training dataset is constructed, their corresponding
domains would not be properly related to the testing datasets do-
mains. Concluding, a ML approach is not the most suitable strategy
to detect neither multidomain nor specific domain key terms.
Frequency-based approaches use the frequency of appearance
of the terms in the document in order to weight their importance,
mostly according to TF-IDF weighting criterion (Watts & Strogatz,
1998). The first approximation was proposed by Salton and Buck-
ley (1988) and extended by others, such as Kerner (2003). As we
previously commented on Section 1, a frequency-based approach
is not the better option to detect key terms in SD.
More recent, word association-based approaches attend to the
correlations between candidate words. These schemes assume that
semantically similar words tend to occur in similar contexts. Mat-suo and Ishizuka (2004) implemented an algorithm based on the
co-occurrence distribution information of the candidate terms
and the frequent terms of the document, i.e., occurrences in the
same sentence. In their approach, Wartena, Brussee, and Slakhorst
(2010) assume that if a word occurs in a number of documents on
the same topic, it has more discriminative power than a word
occurring in the same number of documents but scattered over dif-
ferent topics. Other statistical key phrase extraction methods with-
out reference corpora can be consulted in Bracewell, Ren, and
Kuriowa (2005) and Paukkeri, Nieminen, Polla, and Honkela
(2008). According to SD, the application of this approach is not
desirable since there is not a significant key term co-occurrence
distribution in them.
Word position-based approaches attend to the outline of the
document, assuming that key terms appear more often in particu-
lar positions of the document (Turney, 2000; HaCohen-Kerner
et al., 2007). Hence, they are applied to structured documents, such
as XML documents, scientific papers or Wikipedia articles. From
the SD perspective (e.g., the set of question–answer pairs of a
FAQ document), their structure is not standard, and it is not en-
ough well-defined to apply these kind of approaches.
Linguistic-based approaches are based on linguistic parsing and
pattern matching using the Part-Of-Speech (POS) tag of terms in:
filtering procedures, as a feature in classification schemes (Khoury,
Karray, & Kamel, 2008), or as pattern in the candidate extraction
process. This strategy is not usually employed to develop a com-
plete system.
Following, graph-based approaches model each document as a
graph where the vertices represent the candidate terms. Any pos-
sible relation between two vertices is a potentially useful connec-
tion that defines the edge between them. Subsequently, graph
analysis algorithms (Brin & Page, 1998; Agirre & Soroa, 2008) are
used to rank the candidates. Mihalcea and Tarau introduced the
TextRank graph-based ranking model (Mihalcea & Tarau, 2008)
that uses the co-occurrence relation of the candidates. In Litvak
and Last (2008), two approaches (supervised and unsupervised)
are compared. Meanwhile, Grineva et al. apply a network analysis
algorithm on semantic graph representation of the document
(Grineva, Grinev, & Lizorkin, 2009). Key terms are selected from
the most dense groups discarding those belonging to sparse
groups.
In last few years, controlled vocabularies have been widely used
to extend classical approaches, not only in Keyword Extraction task
(Bunescu & Pasça, 2006; Coursey, Mihalcea, & Moen, 2008; Gazen-
dam, Wartena, & Brussee, 2010; Grineva et al., 2009; Medelyan,
Witten, & Milne, 2008; Mihalcea & Csomai, 2007; Wenchao et al.,
2009; Wang, Hu, Zeng, & Chen, 2009; Xu & Lau, 2010). Thesau-
rus-based methods extend the previous strategies exploiting the
background knowledge drawn from manually constructed thesau-
rus (Wenchao et al., 2009; Gazendam et al., 2010), or online dictio-
naries or encyclopaedias, such as Wikipedia (Bunescu & Pasça,
2006; Coursey et al., 2008; Medelyan et al., 2008; Mihalcea &
Csomai, 2007; Grineva et al., 2009; Xu & Lau, 2010). Mihalcea
and Csomai implemented the Wikify! system exploiting the link-
ing relation of their candidate terms (Mihalcea & Csomai, 2007).
Fig. 1. Example of how a Wikipedia article’s structure defines a concept.
13482 M. Romero et al. / Expert Systems with Applications 39 (2012) 13480–13491In Coursey et al. (2008), the systems of Mihalcea and Csomai
(2007) and Mihalcea and Tarau (2008) have been associated to
build a hybrid system improving the results of each approach sep-
arately. As we previously commented, MKT in SD will be detected
by this strategy.
Further, there are systems with Keyword Extraction modules for
performing more complex tasks: to find meaningful keyword se-
quences (Chen & Lin, 2011), to find keyphrases for text summariza-
tion (Litvak & Last, 2008), or to provide documents headlines (Xu &
Lau, 2010). In Liu, Lee, Yu, and Chen (2011), the authors employ a
keyword generation model to develop a computer assisted writing
system. A similar strategy is used in Liu, Lee, and Ding (2012) to
construct an intelligent computer assisted blog writing system.3. Wikipedia-based knowledge resource
In this section, we define the dictionary of concepts extracted
from Wikipedia articles that is involved in subsequent processes
of our system (Section 5.3).4 http://www.wordfrequency.info/intro.asp.
5
3.1. Wikipedia-based dictionary of concepts
The free on-line encyclopaedia Wikipedia3 provides an extensive
coverage about a large range of general-domains, and has been suc-
cessfully used in Natural Language Processing applications (Named
Entity Disambiguation (Bunescu & Pasça, 2006), Topic Indexing
(Medelyan et al., 2008), or Text Classification (Gabrilovich & Markov-
itch, 2006; Wang et al., 2009)).
Every Wikipedia article describes a complete entity, so-called
concept (i.e., a complete representation of the information drawn
by one Wikipedia article). A concept is compound by an identifier
name or title, a set of alternative aliases or synonyms, and the cat-
egories to which it belongs. In addition, the number of articles in
which its title appears as a link (numArtHv), and the number of
articles in which its title appears (numArt) are stored as useful
information. Fig. 1 shows an example of how the structure of the
Wikipedia ‘Artificial Intelligence’ article defines a concept.
Following the idea of Wang et al. (2009), we take use of these
concepts to generate a dictionary. In that work the authors use
the dictionary for Text Classification. We extend it for Keyword
Extraction.
To build the dictionary, we discard disambiguation pages, redi-
rect pages, appendixes, those articles belonging to chronological or3 http://en.wikipedia.org.numerical categories (Integers, Perfect numbers, Numbers, Ra-
tional numbers, Real numbers. . .), and those whose titles are a se-
quence of only stop words. Then, a concept is created from each
valid article and it is mapped in a dictionary structure.
4. Term frequency in language
This section depicts a term frequency in language dictionary
based on the English language, used in Section 5.2. For terms not
contained in this dictionary, we design an algorithm to calculate
an approximate simulated frequency based on Google’s search
engine.
4.1. Term frequency dictionary
Word frequency lists have proven to be a useful tool in practical
researches (Stamatatos, Fakotakis, & Kokkinakis, 2000).
We choose the frequency list extracted from the Corpus of
Contemporary American English4 (COCA) because it is the largest
publicly-available, genre-balanced corpus of English.5
Following the structure of the list, we develop a dictionary of
1-grams and 2-grams mapped to their absolute and normalized
frequency. It allows us to find which terms in any document are
less frequent in the language.
4.2. Google-based simulated frequency
It is likely that some important terms are not contained in the
frequency in language dictionary. For this reason, we approximate
heuristically their frequency by considering the number of results
returned by the Google Search engine.
Google search engine is one of the most common linguistic
resources in Computational Linguistics tasks (Al-Eroud, Al-Ramahi,
Al-Kabi, Alsmadi, & Al-Shawakfa, 2011; Chen & Lin, 2011). This is
not unusual due to Google had indexed more than 25 billion web
pages, in 2006. We take advantage of it using the search engine
to obtain an approximation of the language frequency of uncom-
mon terms. The number of results obtained by a search is directly
related with the commonness of the query terms. For example,
most common terms (such as ‘the’, ‘of’, and ‘and’) return the high-
est number of results in a Google query (25,270,000,000 results)This text corpus has 425 million words extracted from spoken, fiction, popular
agazines, newspapers, and academic texts, ant it has been used in Computational
nguistic tasks (Davies, 2010).m
Li
Fig. 2. System architecture.
M. Romero et al. / Expert Systems with Applications 39 (2012) 13480–13491 13483whereas infrequent terms (such as ‘equanimous’ or ‘succumb’) re-
turn a number of results much more few. Moreover, the order of
the words in a query affects to the number of results. In this con-
text, we think that a regression analysis (Kenney & Keeping,
1962) could be helpful to predict the frequency in language value
of terms. It provides a conceptually simple method for investigat-
ing functional relationships between one or more factors and an
outcome of interest. The relationship is expressed in the form of
an equation or a model connecting the response or dependent var-
iable and one or more explanatory or predictor variable.
By considering the frequency in language as the dependant var-
iable, and the number of results in Google as the predictor, we coud
apply the regression analysis strategy (following an a priori sample
size calculator schema (Abramowitz & Stegun, 1965; Cohen, 1988;
Cohen, Cohen, West, & Aiken, 2003)) in order to obtain the regres-
sion equation.6 To access to the details of the complete process, and
the resulting regression equation Appendix B could be consulted.6 Since the Internet is continuously evolving, the regression curve could not be
considered as a static result. Instead, it should be considered as a pre-calculation for
the algorithm that may be periodically repeated in order to gain in reliability.5. System overview
In this section, the multi-stage architecture of our high perfor-
mance Automatic Support-Domain Key Term Extraction (ASKEx)
system is explained. We have considered two classes of key terms
from each SD in a collection: (i) multidomain key terms (MKT), and
(ii) specific domain key terms (SKT).
We thus propose a system composed by the following modules
(Fig. 2). In an initial stage, the preprocessing module prepares the
content of the collection of documents. Then, the algorithm per-
forms at document-level. The candidate extraction module ex-
tracts two sets of candidates. Those terms with a frequency in
language lower than a predefined threshold are Specific-domain
Candidate Terms (SCT). The rest of terms conform the Multidomain
Candidate Terms (MCT) set.
To illustrate each set, let us consider the Example 2 in Table 2.
The term ‘FAQtory Service’ references a software application. This
proper noun has an insignificant degree of use in English, but is
completely meaningful in the specific domain (SCT). In contrast,
the terms ‘question’ or ‘answer’ in the same example belong to
MCT.
Table 2
Example 2: Key terms for ‘Virtual Solutions’ FAQ fragment.
Fragment of FAQ content for ‘Virtual Solutions’ FAQ
Q: What is the function of Intelligent FAQ?
A: An Intelligent FAQ gives specific information on the basis of a internally or
externally pre-established question. By formulating a question, the system
recovers the adequate questions and answers related to your question in
order to obtain the answer.
Q: What is the function of FAQtory Service?
A: FAQtory Service is a software application to create, manage and maintain
intelligent FAQs. Thanks to Intelligent FAQs, the user can obtain
information automatically without the need to search manually. As the
user raises a question, the system shows the related subjects to user. VS
FAQtory also allows to maintain and increase the number of questions and
answers easily basing on the users’ queries and significantly its utility
increases.
Key terms:
Intelligent FAQ, FAQtory Service, Question, Answer
13484 M. Romero et al. / Expert Systems with Applications 39 (2012) 13480–134915.1. Preprocessing module
The goal of this module is to prepare the document collection
for the subsequent steps. For each document in the collection sev-
eral language processing techniques are applied: splitter, POS tag-
ging, stop words removal, and lemmatization. The GPL Library
FreeLing 2.27 was used to implement the preprocessing.
We deal with the different morphological variations of words
by means of lemmatization. For example, ‘link’, ‘linked’ and ‘links’
share the lemmatized form ‘link’. By lemmatizing each concept ti-
tle and synonyms we reduce the candidate set of key terms.5.2. Candidate extraction module
The candidate extraction module obtains a set MCT and a set of
SCT. This process includes a Syntactic Filter stage and a Frequency
Language Extraction module. All the 1-grams and 2-grams of the
document are taken as input.
First, the Syntactic Filter restricts the set of input terms compos-
ing the initial precandidate set. We have empirically contrasted
that relevant key terms are likely nouns or adjectives. Thus, other
ones are filtered out in this stage.
Later, the Frequency Language Extraction module assigns the
frequency in language value to each precandidate term. This value
is obtained from the frequency language dictionary if the term is
stored in it, or by means of the Google-based frequency algorithm
otherwise. If the frequency in language value exceeds a predefined
threshold a, the precandidate is considered as MCT. In the other
case, it is considered a SCT. From this point, each set follows a dif-
ferent procedure.
The a-threshold should be regarded as the degree of common-
ness in language of a term (it establishes how common a term is,
to be considered a specific and support-domain term). In Example
2 in Table 2, low frequently terms as ‘FAQtory service’ or ‘Intelli-
gent FAQ’ would be classified as SCT. In contrast, frequent terms
such as ‘question’ or ‘answer’ would be classified as MCT. The tune
of this threshold only affects on the delimitation of each candidate
set. The adjustment of a-threshold is depicted in Appendix A.
The complete candidate classification process is shown in Table
3: Algorithm 1. In this algorithm, candidate  POS() denotes the POS
tag for the given candidate’s words, freqDict(x) denotes de
frequency in the language for the given term x, googleSearch(x)
denotes de number of Google search results for the given term x,
and regFunction(y) denotes the approximated frequency resulting
of the regression function applied to a given number y.7 http://nlp.lsi.upc.edu/freeling/.5.3. Multidomain candidates’ procedure
As a result, a rank of MCT is obtained. The Multidomain candi-
date procedure refines the MCT rank. In this section we explain
how the Filtering Module discards potentially irrelevant candi-
dates, and how the Term Disambiguation Module unifies the can-
didates to compose the final ranking.5.3.1. Filtering module
Up to now, a specific part-of-speech pattern had been the only
restriction imposed to the candidates, and it is very likely that this
set may be too large. Consequently, we develop a multi-filtering
procedure in order to adjust the MCT set leading a more direct
and efficient ranking process. This Filtering module is composed
by three main parts: (i) a statistical filter, (ii) a frequency filter,
and (iii) a dictionary-based filter.
TF-IDF Filter. The statistical TF-IDF filter discards frequent MCT
that are also frequent in the entire corpus. TF-IDF is a widely used
measure that weights the importance of words according to their
relative frequency of appearance in the document (term frequency,
tf), and the number of documents containing them (document fre-
quency, df) (Eq. (1)). To access a vaster discussion on this method
(Khoury et al., 2008) could be consulted. Finally, those MCT whose
score is lower than a predefined threshold b are filtered out.
scoreðU; SÞ ¼
P
w2U;Sk
2
w  tfUðwÞ  tfSðwÞffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiP
w2UtfUðwÞ
2 
P
w2StfSðwÞ
2
q ð1Þ
kw  idf ðwÞ ¼ log
jDj
jfd 2 D : tfdðwÞ > 0gj
 
ð2Þ
The b-threshold determines the maximum degree of TF-IDF fre-
quency of a term in the collection for being considered as key term.
If a term appears frequently in all the domains of the collection,
then it will hardly be considered key term. The tune of b-threshold
will be depicted in Appendix A.
Frequency Filter. Following the TF-IDF criterion, if a term ap-
pears frequently in any document of the language, it hardly will
be considered key term. For example, adjectives like ‘small’ or ‘dif-
ferent’ appear frequently in texts of any domain. They would be
hardly considered as key terms by human annotators because they
are too generic to clarify the content of any document. Hence, the
Frequency filter removes those MCT that are common in the lan-
guage, i.e. those whose frequency in language value (computed
in the Frequency Language Extraction Module) exceeds a certain
threshold c. Appendix A can be consulted to comprehend how to
adjust the c-threshold.
Dictionary Filter. The last step in the multi-filtering stage tries
to relate every candidate with the concepts previously collected.
Each MCT is searched through the synonyms of the concepts. The
matching concepts are stored as possible concepts for the MCT
(Fig. 3).
Hence, the goal of the Dictionary Filter is twofold. On the one
hand, those terms unrelated to any existing concept are discarded.
On the other hand, every related concept is discovered. Both the
valid MCT and their related concepts are submitted to the Term
Disambiguation module that tries to discern the most appropriate
candidate-concept pair.5.3.2. Term Disambiguation module
This module deals with polysemous candidates, discerning the
intended concept from all the possible meanings within the docu-
ment context.
We adopt a methodology based on the first word sense disam-
biguation strategy in Wang et al. (2009).
Table 3
Algorithm 1: Candidate extraction algorithm.
INPUT:
C: set of 1-grams and 2-grams parsed through the document.
P = {NN,NP,JJ}: POS tag subset (NN = common noun, NP = proper noun,
JJ = adjective).
freqDict: Frequency language dictionary.
a = threshold of frequency language value.
OUTPUT:
multidomainSet: Multidomain Candidate Term set.
specificSet: Specific Candidate Term set.
begin
1: multidomainSet £
2: specificSet £
3: foreach candidatei 2 C do
4: if candidatei  POS() 2 P then
5: if candidatei 2 freqDict then
6: freqValue freqDict(candidatei)
7: elsif
8: numResults googleSearch(candidatei)
9: freqValue regFunction(numResults)
10: end if
11: if freqValue < a then
12: specificSet specificSet [ candidatei
13: elsif
14: multidomainSet multidomainSet [ candidatei
15: end if
16: end if
17: end for
end
M. Romero et al. / Expert Systems with Applications 39 (2012) 13480–13491 13485Given a term and a set of ambiguous concepts, and considering
the document as the disambiguation context, we compute the sim-
ilarity measure by means of their vectorial representation (Eq. (3)).
Thus, the most similar concept to the document is taken.
cosSimð~U; ~VÞ ¼
~U ~V
j~Uj  j~V j
ð3Þ
It is possible that concepts with few synonyms tend to present
the same cosine similarity mean. In this case, the concept whose
hyponyms obtain the higher average of Google’s results in Wikipe-
dia site is taken.
The remarking Term Sense Disambiguation process is shown in
Table 4: Algorithm 2. In this algorithm, x. concepts() returns the set
of ambiguous concepts related to a given candidate x, c.synonyms()
returns the set of synonyms and the title of a given concept c,
c.hyponyms() returns the set of hyponyms for a given concept c,
vectorRep(y) returns the TF-IDF vector representation for a given
string y, cosSimð~u;~vÞ denotes the cosine similarity value computed
for the given vectors~u and ~v , and gWSearch(x) denotes the number
of Google results for the string x on a search in the ‘en.wikipe-
dia.org’ site.
5.3.3. Multidomain ranking module
The last step for the MCT assigns a numeric value to each can-
didate and establishes a ranking. We compute a score based on one
of the most well-known measures used in the literature: key-
phraseness. This measure computes the probability for a candidate
term of being a hyperlink (i.e. an important term in Wikipedia) in
any article of Wikipedia. Terms achieving high keyphraseness are
considered consequently keywords or keyphrases (Mihalcea & Cso-
mai, 2007). For each candidate c, the probability is computed as
follows:
keyphrasenessðcÞ ¼ countðDLinkÞ
countðDaÞ
where count(DLink) is the number of Wikipedia articles in which this
candidate appears as a link (the pre-stored numArtHV value), andcount(Da) is the number of articles in which it appears (the pre-
stored numArt value).
However, the authors of this proposal compute this measure in
terms of the document collection as a whole rather than for an
individual document (Medelyan et al., 2008). In this sense, this
technique assumes that a candidate term has the same discrimina-
tive power in any document of the collection. This consideration
may be right for classical datasets tested in Keyword Extraction
task but this is not the case in SD. One term with high discrimina-
tive power in the domain of one SD will probably be inessential in
any other document of the collection. In the Example 3 in Table 5,
the term ‘operating system’ is a key term in the first document but
no so for the second one. However, the keyphraseness measure
would select (or not) the term as key term in both documents.
For these reasons, we need to extend this score method adding a
different feature.
In order to increase the strength of those terms whose
keyphraseness value is low, we follow a frequency-based strategy
taking into account the number of appearances (appearance fre-
quency) of the synonyms of the concept (related to the candidate)
in the document. Finally, we design a measure to establish an egal-
itarian relation between both keyphraseness and appearance fre-
quency values. Hence, the scoring function for a candidate term t
and his corresponding concept c, can be defined by as:
multidomain scoreðcÞ ¼
ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi
nFreqðcÞ2 þ nKeyphrasenessðcÞ2
q
;
where nFreq(c) denotes the normalized frequency of appearances
on average for the synonyms of the concept c in the document,
and nKeyphraseness(c) denotes the normalized keyphraseness value
of the concept c. Once all the candidates are measured, the top N
MCT in the ranking are considered MKT for the given SD.5.4. Specific candidates’ procedure
As result of this procedure, a rank of SCT is obtained.5.4.1. Specific ranking module
As we previously commented, no filter is needed for the SCT set.
The first filtering multidomain process attends to the TF-IDF value
of candidates. This filter is not essential for SCT, due to such set is
specific for a single document instead of the entire collection. For
the two remaining filtering processes, the language frequency filter
is implicit in the selection of these candidates, and the dictionary
filter is useless regarding the SCT have not to hold a semantically
related concept. Thus, only the ranking procedure is needed.
In this work, we assume that the frequency in language is deter-
minant to detect relevant terms in a bounded context. However,
we cannot rank SCT using the frequency in language value only.
Following a similar criterion (see Section 5.3.3), the appearance
frequency of each term on the document is taken into account:
specific scoreðtÞ ¼
ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi
nLangFreqðtÞ2 þ nFreqðtÞ2
q
;
where nLangFreq(t) is the normalized frequency in language of the
term t, and nFreq(t) is the normalized frequency of the term t in
the document. Finally, the top N specific candidate terms in the
ranking are considered specific key terms for the given document.6. Empirical evaluation
This section reports the empirical results obtained in the evalu-
ation of our system. The validity of our system has also been con-
trasted to other state-of-the-art methods.
Fig. 3. Related concepts for ‘neural network’ term.
13486 M. Romero et al. / Expert Systems with Applications 39 (2012) 13480–134916.1. Wikipedia data
The work on this paper is based on the March 2011 Wikipedia
version, which contains 8,279,325 articles including redirects. The
complete lists of Wikipedia database backup dumps are available
in http://dumps.wikimedia.org/backup-index.html. The resulting
dictionary contains 663,704 concepts organized in 311,761 distinct
categories. Each concept possesses 1.73 synonyms and 2.42 hyp-
onyms on average. The complete list of synonym reaches to
1,148,524 terms.
6.2. COCA word frequency data
The word frequency list is available in http://www.ngrams.info.
The resulting dictionary contains 486,688 1-grams, 1,055,386 2-
grams, and 1,011,768 3-grams. Each entry is composed by the term
as appear in the text, the lemmas of the term, its corresponding
POS tags, and the frequency of the term in the entire corpus.
6.3. Data set
Unfortunately, there is no standard benchmark at our disposal.
In addition, we have been forced to extract manually the key terms
of each document of a SD collection. With the aim of providing a
more rigorous validation, we have also considered a classical
dataset.
The experiments have been performed on two collections. The
first collection is composed by 100 FAQ documents extracted from
the knowledge base of the VS FAQtory application.8 VS FAQtory is a
commercial FAQ retrieval application powered by Virtual Solutions,
an emerging company in Artificial Intelligence and Natural Lan-
guage. We select FAQ documents related to software applications,
companies and university courses (Table 6). Each FAQ document
presents an extension range between 5 and 267 question–answer
pairs. Our second collection is composed by 100 papers in the field
of computer science published from 2008 to 2012. We use Scopus
web application9 to extract them.
Key terms on each document was manually annotated by two
PhD student of computer science and ten undergraduate students
in the field of computer science. Each student was requested to ex-
tract a set of 5 to 20 key terms (including both specific and multi-
domain key terms for the SD) from each document. Each document
was analysed by four students. So, we select as valid those key
terms selected by at least two students. Finally, we obtain two data8 http://www.solucionesvirtuales.es/en-GB/solutions/intelligent-faqs/vs-faqtory.
9 http://www.scopus.com/home.url.sets of 100 documents and their corresponding key terms. Table 7
shows the proportion of key terms to each type. This percentage
had been calculated a posteriori.
6.4. Design principles
Our system was implemented with the following architectural
design principles. Both the dictionary of concepts and the fre-
quency dictionary are stored in a MySQL database (version
14.12). We use the Hibernate10 library (version 3.3.5), as framework
for mapping the dictionaries.
Since Google does not provide the Google search API to obtain
the number of results of a given query, we had implemented a spe-
cific module by means of the HttpComponents11 library (4.0.1). This
module automatically obtains the Google results page for any given
input. Later, the number of results is extracted from the result page.
We use Java SDK 6 (1.6.0_20) to implement the system. Finally,
a dedicated machine with 8 Gb RAM was used for the evaluation.
6.5. Experimental results
In order to evaluate the performance of our system we carried
out an experimental comparison of the proposal against conven-
tional Keyword Extraction methods. Given that conventional
methods were not designed to deal with SD, we selected the most
used methods in the state-of-the-art for ordinary documents. As
commented in Introduction, Machine Learning methods were not
considered in the comparatives because of the nature of specific
documents. Five algorithms are used in this study:
 Yahoo! Terms Extractor is a web service.12 This system pro-
vides a list of significant words or phrases extracted from an
input content through a request URL. We use this application as
baseline. The number of output key terms is not customizable.
 TFxIDF is a well-known unsupervised ranking method. This
conventional algorithm establishes a ranking of candidate terms
in basis of the TF-IDF measure. For obtaining the frequency val-
ues necessaries to apply the TF-IDF measure, our complete FAQ
document collection was used as training dataset. At last step,
the top-K terms are selected as valid key terms. We select K
equal to the number of key terms extracted by our system to
make the comparison more illustrative. We could preselect
the candidates for this ranking algorithm extracting it by our0 http://www.hibernate.org/.
1 http://hc.apache.org/.
2
1
1
1 http://developer.yahoo.com/search/content/V1/termExtraction.html.
Table 4
Algorithm 2: Term Sense Disambiguation algorithm.
INPUT:
~doc: vector representation of the document.
C: multidomain candidate term set
OUTPUT:
ccPairSet: candidate-concept pair set.
begin
1: ccPairSet £
2: foreach candidatei 2 C do
3: cmaxAvgCos 0
4: foreach conceptj 2 candidatei  concepts() do
5: cosSimSum 0
6: foreach synonymk 2 conceptj  synonyms()do
7: ~v  vectorRepðsynonymkÞ
8: actualCosSim cosSimð ~doc;~vÞ
9: cosSimSum cosSimSum + actualCosSim
10: end for
11: actualAvgCos cosSimSumNumberofsynonymsofconceptj
12: if actualAvgCos > maxAvgCos then
13: maxAvgCos actualAvgCos
14: selectedConcept conceptj
15: end if
16: if actualAvgCos = maxAvgCos then
17: gResSum 0
18: maxAvgRes 0
19: foreach hyponymk 2 conceptj  hyponyms() do
20: actualRes gWSearch(hyponymk)
21: gResSum gResSum + actualRes
22: end for
23: actualAvgRes gResSumNumberofhyponymsofconceptj
24: if actualAvgRes P maxAvgRes then
25: maxAvgRes actualAvgRes
26: selectedConcept conceptj
27: end if
28: end if
29: end for
30: ccPair create pair with candidatei and selectedConcept
31: ccPairSet ccPairSet [ ccPair
32: end for
end
M. Romero et al. / Expert Systems with Applications 39 (2012) 13480–13491 13487controlled dictionary. However, we decide not to do it since
specific key term would not be considered.
 Wikify! is a knowledge base system that extracts key terms
candidate from a Wikipedia based controlled vocabulary. This
system ranks the candidates looking at the probability of a
term being keyword in a Wikipedia article. In their work, the
authors established that the number of key terms extracted
by their system is around the 6% of the number of words in
a document. Consequently, the system extracts a number of
key terms equal to the specified ratio of words in each
document.
 TextRank is a graph-based ranking algorithm from extracting
key terms. This system models the input document as a graph
being terms of a certain part-of-speech (nouns and adjectives)
the vertices of the graph. An edge is added between those ver-
tices whose respective terms co-occur within a window of N
words. In their study the authors obtained the better results
with a window of two words. Also, the number of key words
is set to a third of the number of vertices in the graph. We
follow these indications to apply their algorithm to our
document collection. Finally, the system incorporates a post-
processing stage. In this stage, compound key terms are
obtained from combination of the previously founded key
words.
 Longest Common Substring is a hybrid system compounds by
the last two commented algorithms. For each element in the
Wikify! output, the method finds the longest common subse-
quence in all of the TextRank output for the same documentand vice versa. The final output is the union of the two set of
substring that represents the longest fragments found in the
key term set generated by both algorithms.
We evaluate each method by means of the F-measure metric
(6). This metric is the weighted harmonic mean of precision (4)
and recall (5), which are the most common metrics used in the lit-
erature. Given a set of key terms extracted by the system and a set
of key terms manually extracted by humans, Precision is the num-
ber of matched key terms divided by the total number of key terms
extracted by the system; Recall is the number of matched key
terms divided by the total number of human extracted key terms.
As could be seen, the results directly depend on the number of key
terms extracted by our algorithm. To choose the number N of key
terms from each ranking, we conducted the following evaluation.
The maximum number of manually annotated key terms in a given
document from the collection is 20. In the comparison, we have
considered the cases of N = 10, to N = 30 subsequently, for each
ranking. Then we have empirically contrasted that N = 15 was
the best configuration in terms of F-measure. Therefore, we select
the top-15 terms from the specific ranking and the top-15 terms
from the multidomain ranking (30 key terms for FAQ document).
Precission ¼ jfmanually extractedg \ fsystem extractedgjjfsystem extractedgj ð4Þ
Recall ¼ jfmanually extractedg \ fsystem extractedgjjfmanually extractedgj ð5Þ
F measure ¼ 2  Precission  Recall
Precissionþ Recall ð6Þ
Table 5
Example 3: Key terms for ‘The Linux FAQ’ and ‘ACER FAQ’ documents.
Content of ‘The Linux FAQ’a Content of ‘ACER FAQ’ b
Q: Is Linux Unix? Q: My battery is lasting a short time. What can it be?
A: Officially an operating system is not allowed to be called a Unix until it passes
the Open Group’s certification tests, and supports the necessary API’s. Nobody
has yet stepped forward to pay the large fees that certification involves, so we
are not allowed to call it Unix. Certification really does not mean very much
anyway. Very few of the commercial operating systems have passed the Open
Group tests.
A: There are several items related to the battery that can undermine the autonomy
of your device. Acer recommends that the operating system used is the original
factory and that the use of battery is used the least resources possible. This will
maximize autonomy.
Key terms: Key terms:
Linux, Operating system, Open Group Tests Battery, Autonomy, Instruction manual, Cell
a http://tldp.org/FAQ/Linux-FAQ/general.html.
b http://support.acer.com/br/en/faq.aspx.
Table 8
Key term extraction results on SD collection.
Method Performance
Precision Recall F-measure
TFxIDF 19.21 49.40 27.09
Yahoo! Term Extractor 24.38 30.44 17.32
Wikify! 09.89 41.05 15.68
TextRank 13.43 62.45 21.24
LCS 17.68 36.66 22.38
ASKEx 39.37 73.88 50.20
Table 9
Key term extraction results on scientific paper collection.
Method Performance
Precision Recall F-measure
TFxIDF 22.67 41.25 28.78
Yahoo! Term Extractor 38.42 38.75 36.79
Wikify! 20.67 29.16 28.91
TextRank 23.53 48.83 31.37
LCS 59.62 26.85 34.74
ASKEx 32.28 64.41 49.31
Table 10
Key term extraction results of each independent module of ASKEx on SD collection.
Module Performance
Precision Recall F-measure
ASKEx specific key term module 45.00 43.42 42.75
ASKEx multidomain key term module 30.00 29.24 28.63
Table 7
Percentage of key terms by type.
Specific key terms
(%)
Multidomain key terms
(%)
Complete collection 38,37 61,63
Software FAQs 41,66 58,34
Companies FAQs 47,97 52,03
University Courses
FAQs
23,08 76,92
Table 6
Distribution of FAQ documents by category.
Software Companies University courses
Proportion 56,25% 18,75% 25%
13488 M. Romero et al. / Expert Systems with Applications 39 (2012) 13480–134916.5.1. Results of proposed system
In this section, we analyse the results obtained for our Auto-
matic Support-Domain Key Term Extraction System in contrast to
the comparison algorithms.
The results presented in the Table 8 show that our ASKEx sys-
tem outperforms all the comparative methods considering the SD
collection.
Regarding the comparison algorithms, TFIDF obtained the best
scores. This situation confirms our earlier assumption that a great
subset of key terms does not present a close relation between then
and are hardly contained in multidomain controlled vocabularies.
In turn, Wikify! obtained the worst scores in the comparison.
Although the controlled vocabulary obtains promising results han-
dling ordinary documents, is not the case for SD. Moreover, the
keyphraseness measure does not offer good results in this scenario.
TextRank outperforms the controlled vocabulary strategy. This
algorithm presents a candidate extraction process much more per-
missive allowing it to detect both technical terms and proper
nouns. In this way, the algorithm is able to extract specific domain
candidates, but its ranking method relies only on the co-occurring
relation of the candidates whereas SKT are lacking of this kind of
relation. Finally, using LCS provides a higher F-score than consider-
ing TextRank and Wikify! alone. This confirms that considering an
hybrid strategy in SD incurs in the improvement of the quality of
the extracted key terms.
It should be remarked that comparison algorithms were de-
signed to deal with ordinary documents. In this regard, results
should be carefully interpreted. SD context present a number of
difficulties for which our system is particularly robust. For this rea-
son, we have performed an additional experimental validation car-
ried out on a classical dataset. Table 9 show the experimental
results of key term extraction applied to scientific papers dataset.The behaviour of our system has been slightly decreased on the
ordinary collection although it still exhibits the best performance.
Our frequency-based criterion is less effective in this scenario due
to most of the terms in scientific papers present a similar fre-
quency in language value. Hence, the distinction between multido-
main key terms and specific-domain key terms is less noticeable.
In contrast, all the comparative algorithms improve their results
on a ordinary collection, as it was expected. In this scenario, TFxIDF
algorithm showed the worst performance, but its results are quite
similar in both collections. Wikify! obtains an improvement of
13.23% in it F-measure value. Although the scientific domain is still
complex for a multidomain controlled vocabulary, the system is
able to obtain a more fitted candidate set. In addition, keyphrase-
ness measure seems to be more robust on ordinary collections.
TextRank results also experiment an important improvement in
its performance (over 10.13%), outperforming the Wikify! system
again. This is due to the word co-occurrence distribution is more
Fig. 4. Performance evaluation extended by ASKEx modules.
M. Romero et al. / Expert Systems with Applications 39 (2012) 13480–13491 13489pronounced in an ordinary dataset. Consequently, the improve-
ment of Wikify! and TextRank systems leads a more successful
assessment for the LCS hybrid strategy.
6.5.2. The effect of key term division on support document dataset
In this section we study how the two different evaluation meth-
ods affects the performance of ASKEx. One of the specific novelties
of our system is to consider two types of key terms besides the
classical MKT set. We identify terms with low frequency in lan-
guage to enrich the output of the system. Also, we consider those
terms that are important in a multidomain context.
To evaluate the importance of key term division, we show the
results obtained when only one the type of key terms is considered
on our SD dataset (i.e., when only one of the two modules of the
system is executed) in Table 10.
For each type of candidate, we select the top 15 terms of the
ranking. We found that the performance is deteriorated if only
one type of key terms is chosen. However, the results evidence that
even considering only one candidate set, the system improves the
results obtained by the comparison algorithms (Fig. 4).
On the one hand, the multidomain approach outperforms the
results obtained by the rest of the systems except TFxIDF algo-
rithm. Attending to the system Wikify!, the results showed that
the addition of the redirect information of the Wikipedia articles
enriches the controlled vocabulary aiming to improve the candi-
date detection. Moreover, the adapted keyphraseness works better
in specific document context. On the other hand, the module de-
voted to detect SKT still exhibits the best performance among all
other algorithms. Technical terms and proper nouns are crucial
in specific document context. Furthermore, it has been proved that
combining both strategies incurs in a better performance.
7. Conclusions and future work
In this paper, a new key term extraction system able to handle
with the particularities of the support document context has been
proposed. The system has obtained promising results in experi-
mental validation, being compared with some of the most impor-
tant algorithms in Keyword Extraction.
Our method is a hybrid system that takes advantage of the
strengths of the frequency-based and thesaurus-based criteria,
introducing some novelties. In this sense, the system is able to rec-
ognize two different key term sets. On the one hand, it identifies
the main multidomain important terms enclosing general keyterms of the collection. To identify them a dictionary of concepts
drawn from Wikipedia’s articles is used. This dictionary represents
a wide-ranging controlled vocabulary taking an active part in most
of the main modules of the ASKEx system. Moreover, we extended
the so-called keyphraseness method to rank multidomain candi-
date terms adding a document-specific feature (following a fre-
quency-based strategy). This measure has obtained good results
in the SD context. On the other hand, the system extracts specific
key terms highly related with the context of a SD, since specific-do-
main key terms are usually technical terms or proper noun. They
are uncommon in the language so we use the frequency in lan-
guage to detect such terms.
The system’s architecture is highly modularized and can be eas-
ily adapted to other systems or languages. Every module of our
method is easily extensible to other systems. Moreover, both the
dictionary of concepts and the frequency-in-language dictionary
can be extended in many languages. In addition, the system could
be extended to any kind of SD with minor modification.
Evaluation proved that our method produces key terms with
73.88% recall and 39.37% precission. We consider these results are
significantly high having into account the difficulty implicit in SD
context (scattered domains, no significant word co-occurrence dis-
tribution, uncommon terms. . .). Hence, the system seems to be use-
ful for extracting key terms in support documents. As a result, the
system could be useful for helping human administrator of FAQ re-
trieval applications. In addition, we have developed another exper-
iment to prove the validity of our system in a classical dataset, also
outperforming the performance of the comparative algorithms.
However, there is much work ahead. Particularly, the disambigu-
ation strategy could be improved by adding further considerations.
Although the term disambiguation algorithm is important for the
performance of our system, we have adopted a heuristic strategy.
Thus, a deeper analysis of this module would be desirable. On a differ-
ent matter, our dictionary of concepts contains a well-formed hierar-
chy of categories. The use of such hierarchy could enhance the quality
of certain parts of our system, as is the case of the ranking modules or
the term disambiguation module. Finally, how the systems could help
to address more complex tasks (Text Categorization, Text Classifica-
tion, Text Genre Detection. . .) becomes a new challenge.
Acknowledgements
The authors thank the Junta de Andalucía that supported this
article with its project: TIC2009–5011.
13490 M. Romero et al. / Expert Systems with Applications 39 (2012) 13480–13491Appendix A. Google-based regression analysis process
This appendix details the process carried out in order to obtain
the regression equation relating the number of results in Google for
a query (predictor) and the frequency in language value for such
term (dependant variable).
First of all, we collect a subset of N random terms from the dic-
tionary of frequency in language enclosing a range of normalized
frequencies between 0.01 and 0.20 (i.e. absolute frequencies be-
tween 1 and 500,000). We take this upper limit value equal to
0.20 because this value helps to delimit common and uncommon
terms, as can be observed in our experiments (Section 6.5.1). Fol-
lowing an a priori sample size calculator schema (Abramowitz &
Stegun, 1965; Cohen, 1988; Cohen et al., 2003), we set N P 54
the number of samples in the subset. It allows us to obtain a med-
ium anticipated effect size of 0.15 (by convention), a desired statis-
tical power level of 0.8 (by convention) and a q-value of 0.05 (by
convention). Therefore, we establish N = 54. For each term in the
subset, the number of results in Google search engine is collected
using the term as a query. Once we obtain the samples with their
respective values for the two variables, we compute the correlation
coefficient and the q-value to observe the linear association be-
tween the dependant variable (absolute frequency in language)
and the predictor (number of results). The correlation coefficient,
r, is 0.5934 and the q-value is 2.26  106. Given that the q-value
is lower than a = 0.05 (level of significance), we can reject the null
hypothesis, i.e. both variables are correlated and the model is valid.
The resulting regression equation that defines the relation between
frequency and result (x variable) in our work is:
FrequencyInLanguageðxÞ ¼ 1:8  105  xþ 13:493
The linear correlation coefficient could be more accurate but it
allows us to obtain a useful prediction for terms not contained in
the dictionary of frequency in language.Appendix B. Parameter tuning
This appendix depicts the tuning of the three parameters de-
fined in our system (Section 5). These parameters are related with
the tolerance for each candidate set, acting as thresholds in the fol-
lowing situations: (1) a threshold a with a range of values between
0.01 and 1.0 is used to distinguish which 1-grams and 2-grams will
be multidomain candidates, or otherwise specific candidates; (2) a
threshold b with a range of possible values between 0.01 and 1.0 is
used to discards multidomain candidates with a TF-IDF score lower
than it; and (3) a threshold c with a range of possible values be-
tween 0.01 and 1.0 is used to discards multidomain candidates
whose normalized frequency in the language is greater than it.
Hence, these values are directly involved in the candidate extrac-
tion process giving to it more or less flexibility. Here we discuss
how to tune a, b, and c.
We select ten FAQ documents with its corresponding manually
annotated key terms of our collection firstly. Those terms of each
document are manually grouped in two sets of key terms: multido-
main key term (MKT) set, if they are contained in the synonyms or
the name of a concept in our dictionary; and specific key term
(SKT) set, in other case. To select the most appropriate thresholds
for the three involved modules, we set different values for each
one and take the resulting candidate set after passing the corre-
sponding module. Then, we compare the appropriate manually
annotated key term set with the systems’ output set. Hence, we se-
lect the tuning for each threshold that provides us the closest
matching between the key terms sets and the corresponding out-
put sets of the involved module. In more detail, the a-threshold
is set to the lowest value that divide the original set of n-gramsin the following way. The MCT set has to contain at least 70% of
its members contained in the manually annotated MKT set, and
the SCT set has to contain at least 70% of its members contained
in the manually annotated SKT set. The b-threshold is set to the
lowest value that allows to the MCT set (after the TF-IDF filter)
to contain at least 70% of its members contained in the manually
annotated SKT set; the c-threshold is set to the greater value that
allows to the MCT set (after the Frequency filter) to contain at least
70% of its members contained in the manually annotated SKT set.
The resulting values are a = 0.20, b = 0.30, and c = 0.30.References
Abramowitz, M., & Stegun, I. A. (1965). Handbook of mathematical functions. New
York, NY: Dover.
Agirre, E., & Soroa, A. (2008). Using the multilingual central repository for graph-
based word sense disambiguation. In Proceedings of the sixth international
conference on language resources and evaluation (LREC’08) (pp. 1388–1392).
Marrakech, Morocco: European Language Resources Association (ELRA).
Al-Eroud, A. F., Al-Ramahi, M. A., Al-Kabi, M. N., Alsmadi, I. M., & Al-Shawakfa, E. M.
(2011). Evaluating google queries based on language preferences. Journal of
Information Science, 37, 282–292.
Bracewell, D. B., Ren, F., & Kuriowa, S. (2005). Multilingual single document
keyword extraction for information retrieval. In Proceedings of 2005 IEEE
international conference on natural language processing and knowledge
engineering (IEEE NLP-KE’05). Wuhan, China.
Brin, S., & Page, L. (1998). The anatomy of a large-scale hypertextual web search
engine. In Proceedings of the seventh international world wide web conference: Vol.
24. Computer Networks and ISDN Systems (pp. 107–117).
Bunescu, R., & Pasça, M. (2006). Using encyclopedic knowledge for named entity
disambiguation. In Proceedings of the 11th conference of the european chapter of
the association for computational linguistic. EACL 2006 (pp. 9–16). Trento, Italy.
Chen, P.-I., & Lin, S.-J. (2011). Word adhoc network: Using google core distance to
extract the most relevant information. Knowledge-Based Systems, 24, 393–405.
Cohen, J. (1988). Statistical power analysis for the behavioral sciences. Hillsdale, NJ:
Lawrence Earlbaum Associates.
Cohen, J., Cohen, P., West, S. G., & Aiken, L. S. (2003). Applied multiple regression/
correlation analysis for the behavioral sciences ((3rd ed.)). Mahwah, NJ: Lawrence
Earlbaum Associates.
Coursey, K. H., Mihalcea, R., & Moen, W. E. (2008). Automatic keyword extraction for
learning object repositories. In Proceedings of the American Society for
Information Science and Technology Vol. 45. (pp. 1–10).
Csomai, A., & Mihalcea, R. (2008). Linguistically motivated features for enhanced
back-of-the-book indexing. In Proceedings of the 46th annual meeting of the
association for computational linguistics: Human language technologies (ACL-08:
HLT) (pp. 932–940). Columbus, Ohio, USA: The Association for Computer
Linguistics.
Davies, M. (2010). The corpus of contemporary american english as the first reliable
monitor corpus of english. Literary and Linguistic Computing, 25, 447–464.
Davies, M. (2011). Word frequency data from the Corpus of Contemporary
American English (COCA). Available from http://www.wordfrequency.info.
Frank, E., Paynther, G., Witten, I. H., Gutwin, C., & Nevil-Manning, C. G. (1999).
Domain-specific keyphrase extraction. In Proceedings of the 16th international
joint conference on artificial intelligence (IJCAI ’99) (pp. 668–673). Sweden:
Stockholm.
Gabrilovich, E., & Markovitch, S. (2006). Overcoming the brittleness bottleneck
using Wikipedia: Enhancing text categorization with encyclopedic knowledge.
In Proceedings of the 21st National Conference on Artificial Intelligence (pp. 1301–
1306). Boston, MA.
Gazendam, L., Wartena, C., & Brussee, R. (2010). Thesaurus based term ranking for
keyword extraction. In Workshop on database and expert systems applications
(dexa’10) (pp. 49–53). Bilbao, Spain.
Grineva, M., Grinev, M., & Lizorkin, D. (2009). Extracting key terms from noisy and
multitheme documents. In Proceedings of the 18th international conference on
world wide web (WWW’09) (pp. 661–670). Barcelona, Spain: Association for
Computing Machinery.
HaCohen-Kerner, Y., Stern, I., Korkus, D., & Fredj, E. (2007). Automatic machine
learning of keyphrase extraction from short html documents written in hebrew.
Cybernetics and Systems, 38, 1–21.
Hulth, A. (2003). Improved automatic keyword extraction given more linguistic
knowledge. In Proceedings of the 2003 conference on empirical methods in natural
language processing (pp. 216–223). Sapporo, Japan.
Kenney, J. F., & Keeping, E. S. (1962). Linear regression and correlation. Mathematics
of statistics. Princeton.
Kerner, Y. H. (2003). Automatic extraction of keywords from abstracts. In
Proceedings of the seventh international conference on knowledge-based
intelligent information and engineering systems (KES 2003) (pp. 843–849).
Berlin/Heidelberg, Oxford, UK: Springer.
Khoury, R., Karray, F., & Kamel, M. S. (2008). Keyword extraction rules based on a
part-of-speech hierarchy. International Journal of Advanced Media and
Communication, 2, 138–153.
M. Romero et al. / Expert Systems with Applications 39 (2012) 13480–13491 13491Lee, L. H., Isa, D., Choo, W. O., & Chue, W. Y. (2012). High relevance keyword
extraction facility for bayesian text classification on different domains of
varying characteristic. Expert System with Applications, 39, 1147–1155.
Litvak, M., & Last, M. (2008). Graph-based keyword extraction for single-document
summarization. In Proceedings of the workshop on multi-source multilingual
information extraction and summarization (MMIES ’08) (pp. 17–24). Manchester,
UK: Association for Computational Linguistics.
Liu, C. L., Lee, C. H., & Ding, B. Y. (2012). Intelligent computer assisted blog writing
system. Expert System with Applications, 39, 4496–4504.
Liu, C. L., Lee, C. H., Yu, S. H., & Chen, C. W. (2011). Computer assisted writing
system. Expert System with Applications, 38, 804–811.
Matsuo, Y., & Ishizuka, M. (2004). Keyword extraction from a document using word
co-occurrence statistical information. International Journal on Artificial
Intelligence Tools, 13, 157–170.
Medelyan, O., Witten, I. H., & Milne, D. (2008). Topic indexing with Wikipedia. In
Proceedings of the first AAAI workshop on Wikipedia and artificial intelligence
(WIKIAI’08) (pp. 19–24). Chicago, I.L. volume 1.
Mihalcea, R., & Csomai, A. (2007). Wikify!: Linking documents to encyclopedic
knowledge. In Proceedings of the ACM 16th international conference on
information and knowledge management (CIKM’07) (pp. 233–242). Lisbon,
Portugal.
Mihalcea, R., & Tarau, P. (2008). Textrank: Bringing order into texts. In Proceedings of
the 2004 conference on empirical methods in natural language processing (EMNLP
2004) (pp. 404–411). Barcelona, Spain: Association for Computational
Linguistics.
Paukkeri, M. -S., Nieminen, I., Polla, M., & Honkela, T. (2008). A language-
independent approach to keyphrase extraction and evaluation. In Proceedings
of the 22nd international conference on computational linguistics (COLING’08) (pp.
83–86). Manchester, UK.
Salton, G., & Buckley, C. (1988). Term weighting approaches in automatic text
retrieval. Information Processing and Management, 24, 513–523.Stamatatos, E., Fakotakis, N., & Kokkinakis, G. (2000). Text genre detection using
common word frequencies. In Proceedings of the 18th conference on
computational linguistics (COLING ’00) (pp. 808–814). Saarbrücken, Germany:
Association for Computational Linguistics.
Tao, Y. H., Liu, S. C., & Lin, C. L. (2011). Summary of faqs from a topical forum based
on the native composition structure. Expert System with Applications, 38,
527–535.
Turney, P. D. (2000). Learning algorithms for keyphrase extraction. Information
Retrieval, 2, 303–336.
Wang, P., Hu, J., Zeng, H.-J., & Chen, Z. (2009). Using Wikipedia knowledge to
improve text classification. Knowledge and Information Systems, 19, 265–281.
Wartena, C., Brussee, R., & Slakhorst, W. (2010). Keyword extraction using word co-
occurrence. In Workshop on database and expert systems applications (DEXA’10)
(pp. 54–58). Bilbao, Spain.
Watts, D. J., & Strogatz, S. H. (1998). Collective dynamics of ‘small-world’ networks.
Nature, 393, 440–442.
Wenchao, M., Lianchen, L., & Ting, D. (2009). A modified approach to keyword
extraction based on word-similarity. In Intelligent computing and intelligent
systems, 2009. ICIS 2009. IEEE international conference on (pp. 388–392).
Shanghai.
Witten, I. H., Paynte, G. W., Frank, E., Gutwin, C., & Nevill-Manning, C. G. (1999). Kea:
Practical automatic keyphrase extraction. In Proceedings of the fourth ACM
conference on digital library (DL’99) (pp. 254–255). Berkeley, CA, USA: ACM.
Xu, S., & Lau, S. Y. F. C. M. (2010). Keyword extraction and headline generation using
novel word features. In Proceedings of the 24th AAAI conference on artificial
intelligence (AAAI’10) (pp. 1461–1466). Atlanta, Georgia, USA.
Zhang, K., H. Xu, J. T., & Li, J.-Z. (2006). Keyword extraction using support vector
machine. In Advances in web-age information management. Lecture notes in
computer science (Vol. 4016, pp. 85–96). Berlin/Heidelberg: Springer.
