Automatic Classification of Handwritten and Printed 
Text in ICR Boxes 
 
Abhishek Jindal 
Newgen Software Technologies Ltd., 
A-6 Satsang Vihar Marg, Qutab Institutional Area,  
New Delhi – 110067, India 
abhishek.jindal@newgen.co.in 
Mohd Amir 
Newgen Software Technologies Ltd., 
A-6 Satsang Vihar Marg, Qutab Institutional Area,  
New Delhi – 110067, India 
mohd.amir@newgen.co.in
 
 
Abstract—Machine printed and handwritten texts intermixed 
appear in the ICR cells of variety of documents. Recognition 
techniques for machine printed and handwritten text in these 
document images are significantly different. It is necessary to 
separate these two types of texts and feed them to the respective 
engine - OCR (Optical Character Recognition) and ICR 
(Intelligent Character Recognition) engine to achieve optimal 
performance. This paper addresses the problem of classification 
of machine printed and handwritten text from acquired 
document images. Document processors can increase their 
productivity and classify handwritten and printed characters 
inside the ICR cells and feed their images to the appropriate 
OCR or ICR engine for better accuracy. The algorithm is tested 
on variety of forms and the recognition rate is calculated to be 
over 91%. 
Keywords—ICR, OCR, Form Processing, ICR cells, Machine 
Printed Text, Handwritten Text 
I. INTRODUCTION 
Paper-based forms like applications, checks and other 
documents are a popular medium for capturing data for many 
enterprises. The data captured is submitted for electronic 
processing and the content is fed into a business system. 
Information on these documents can be filled with machine 
print, handwriting or a combination of both inside the ICR 
cells. This can be done either manually or through automatic 
form processing. Manual data entry involves human 
involvement and thus it is time consuming and error prone. 
Automatic form processing on the other hand is fast and cost 
effective but the results need not be 100 percent accurate. 
Automatic form processing solutions work best on structured 
forms. Structured forms are static forms that have precisely 
defined page layouts such that templates can be built. 
Automatic data recognition technology aims to feed the data 
retrieved from the ICR cells to the appropriate engine: OCR 
engine or ICR engine which is the challenge. ICR is known to 
have the ability to read handwritten characters whereas OCR 
is having the ability to read the printed characters. Document 
processors can increase their accuracy and improve the 
timings by employing comprehensive automated recognition 
software that incorporates the best of OCR and ICR in a single 
solution. 
An obstacle to ICR systems is the mixture of printed and 
handwritten text in the same image. Each text type should be 
processed using different methods in order to enhance the 
recognition accuracy. Previous works addressed the problem 
of identifying each type by various classification techniques. 
These works utilize neural networks [1-7]; employ linear 
polynomial for discrimination function [8], Fisher [9-12] and 
tree classifiers [13-14], Hidden Markov Model (HMM) [15] or 
minimal distance classifiers [16-17]. In this paper we propose 
the use of various characteristics of the text to classify them 
into handwritten or printed. The main advantage of this, 
compared with other classifiers, is its accuracy, efficiency, 
simplicity and the low computation complexity. 
The various steps involved in classifying text in ICR cells 
of form image as handwritten or printed are pre-processing, 
segmentation at character level, feature extraction and the 
classification. 
II. THE PROPOSED APPROACH 
This system for classifying machine printed and 
handwritten text can be decomposed into three stages, as 
shown in Figure 1. It involves capturing the document, 
preprocessing the captured document, the segmentation of 
zones containing ICR cells to individual components, the 
extracted features of the components and the classification 
process executed by the system. 
A. Form Image 
It considers application forms for various objectives, such 
as account opening forms, loan processing forms, customer 
application forms etc. ICR cells containing printed and 
handwritten characters can be found all over these documents. 
In the proposed approach, document to be processed is 
captured by using a capturing device. Figure 2 shows an 
example of possible images to be processed. 
B. Pre-processing 
Pre-processing aims to get a clean and de-skewed image. It 
is a three-step process. In the first step, input form image is 
binarised using Otsu’s threshold selection method [18].  This 
method reduces a Gray level image to a binary image. It 
assumes that the image to be thresholded contains two classes 
of pixels or bi-modal histogram (e.g. foreground and 
background) then calculates the optimum threshold separating 
those two classes so that their combined spread (intra-class 
variance) is minimal. The binarised image may contain some 
1028978-1-4799-2572-8/14/$31.00 c©2014 IEEE
noise. In the second step, the noise is removed on the basis of 
connected component analysis [19]. Finally, document image 
is de-skewed [20-21]. Figure 3 shows the final pre-processed 
image. 
 
 
Figure 1. Overview of the system 
 
Figure 2. A sample of possible images to be processed 
C.  Component Extraction 
In the Component Extraction, the zones are to be selected 
by the user on the pre-processed form image for extracting the 
features for a specific zone as shown in Figure 4. Selection of 
zones is to be carried out by the user because the proposed 
approach will declare the text inside the selected zone to be 
handwritten or printed. ICR cell detection is to be carried out 
for the selected zones on filled forms. ICR cell detection [22] 
involves identifying the total number of ICR cells with their 
positions. After detection of ICR cells, characters are to be 
extracted from the zones of the form.  It is carried out by 
removing ICR cells boundary lines properly so that final image 
contains only ICR characters for better classification. 
 
 
Figure 3. Final pre-processed image 
 
Figure 4. Selecting the zones for extracting the features zone-
wise 
 
2014 IEEE International Advance Computing Conference (IACC) 1029
Figure 5. Output of Component E
To generate the classification approach, 
scanned at 200 DPI and 300 DPI, having
zones and 716 printed zones are observe
threshold at various levels are computed on
results of these images. 
On the basis of experimental results, for
an ICR zone, horizontal and vertical smear
with a threshold of 0.0033 inches an
respectively so as to join the broken compone
An eight-neighbor connected co
algorithm [19] is then used to get the conne
Figure 5 indicates the components extracte
image where cells drawn depict the cell inf
from ICR cell detection. 
D. Feature Extraction and Classification 
Three features are defined and extracted
zone consists of the continuous ICR cells
These features are extracted only for valid ch
Valid characters can be determined on the ba
positions, obtained in the previous step. Us
the number of components in each cell is i
each ICR cell should consists of maximum o
To identify valid characters, the heights of a
of a cell are recorded. For a cell containin
component, the component with maximum he
only and rests of the components are 
arithmetic mean of the heights of all th
computed as a feature.  For each character, if
difference of its height with the arithmetic m
greater than the tolerance, 0.06 inches, tho
considered to be invalid characters. Toleranc
the basis of experimental results. Featur
Classification phase is explained in subsequen
 
xtraction step 
1663 form zones 
 947 handwritten 
d. Tolerance and 
 the experimental 
 each character of 
ing is carried out 
d 0.0067 inches 
nts.  
mponent-labeling 
cted components. 
d from the form 
ormation received 
 for each zone. A 
 in a single row. 
aracters of a zone. 
sis of the ICR cell 
ing cell positions, 
dentified. Ideally, 
f one component. 
ll the components 
g more than one 
ight is considered 
removed. Then, 
e components is 
 the module of the 
ean of heights is 
se characters are 
e is computed on 
e Extraction and 
t section. 
III.  FEATURE EXTRAC
For the valid characters of e
extracted are: Inter-character g
Baseline. 
A. Inter-Character Gap (ICG)
    For each horizontal line of a
is defined as the distance betw
cells. 
• First phase 
In the first phase, the int
characters i.e. the difference o
mid-point of a valid character 
recorded. Let a zone contains
sorted on the basis of their h
horizontal position of mid-poin
hMid3... hMidn. The difference 
between ith character and (i+1)
by (1). Figure 6 shows the inte
sample zone. In order to elimin
characters present in the ICR c
the minimum value of inter-cha
is greater than (1.5 * Min (ICG
an invalid character.  in (2), 
number which is used to r
consecutive blank spaces or inv
characters having ICGi as th
computed as in (3) which is the
removing blank spaces. 
• Second phase 
After removing the blank 
from the first phase, ICG i i.e
between ith character and (i+
arithmetic mean of the inter-
characters of a zone is calculat
contains n elements having (n-
from first phase as ICG 1, ICG 2
of all the valid inter-character 
as ICG(n-1) which will be repres
module of the difference of i
arithmetic mean is stored as a f
character gap with arithmetic 
i.e. 0.01 inches, then those gaps
of acceptance. If total numbe
bucket of acceptance is (n-1), t
further evaluation on the b
Otherwise, the zone is con
characters. 
 
 
 
TION AND CLASSIFICATION 
ach zone, three features that are 
ap (ICG), Character Height and 
 
 zone, ICGs are computed. ICG 
een characters of the adjacent 
er-character gap between two 
f the horizontal position of the 
with the next valid character is 
 ‘n’ valid elements, which are 
orizontal positions, having the 
t of characters as hMid1, hMid2, 
ICGi, i.e. the inter-character gap 
th character will be represented 
r-character gaps computed for a 
ate the blank spaces and invalid 
ells of the zone, Min (ICGi), i.e. 
racter gaps is computed. If ICGi 
i)), it represents a blank space or 
is rounded to the nearest whole 
epresent the total number of 
alid characters between the two 
e inter-character gap. ICG i is 
 actual inter-character gap after 
spaces and invalid characters 
. the actual inter-character gap 
1)th character is recorded. The 
character gap of all the valid 
ed. Let’s consider a zone which 
1) inter-character gaps obtained 
... ICG (n-1). The arithmetic mean 
gap of a zone will be computed 
ented by (4). For each ICG i, the 
ts inter-character gap with the 
eature.  If the deviation of inter-
mean is greater than tolerance, 
 are eliminated from the bucket 
r of elements lying within the 
hen the zone will be passed for 
asis of other characteristics. 
sidered to have handwritten 
(1) 
   
           (2) 
1030 2014 IEEE International Advance Computing Conference (IACC)
Where ICGi represents the inter-character 
characters and Min (ICGi) represents the min
character gap between all the consecutive cha
 
             
 
Figure 6. Identifying the midpoint of the 
character gap, ICG
B. Height 
    The heights of all the valid characters of a
and passed to a process, which is using the a
the filter, as described below. 
    The arithmetic mean of the heights of val
zone is computed in this phase. If a zone
characters having heights h1, h2, h3... hn. Fi
height of the characters computed for a s
arithmetic mean Height(n) is represented by (
character, the module of the difference of i
arithmetic mean of heights is stored as a
deviation of height of a valid character
threshold, i.e. 0.01 inches, then those charact
from the bucket of acceptance. Tolerance is
basis of experimental results which gives the
number of elements lying within the bucket o
then the zone will be passed further evaluati
baseline. Otherwise, the zone is cons
handwritten characters 
 
  
 
Figure 7. Identifying the height of 
 
C. Baseline 
This feature is used when all the charac
topologically segmented. Here, to identif
distribution of the lowermost pixel of isolat
noted down. It is observed that the dist
characters in lowermost point is regular in
texts, and random in handwritten texts. The 
valid characters of a zone i.e. vertical position
pixel of a character is recorded. 
gap between two 
imum of the inter-
racters. 
          (3) 
 
                        (4) 
 
characters and inter-
i 
 zone is recorded 
rithmetic mean as 
id characters of a 
 contains n valid 
gure 7 shows the 
ample zone. The 
5). For each valid 
ts height with the 
 feature.  If the 
 is greater than 
ers are eliminated 
 computed on the 
 threshold. If total 
f acceptance is n, 
on on the basis of 
idered to have 
          (5) 
the characters 
ters of a zone are 
y a baseline the 
ed components is 
ribution of valid 
 machine-printed 
baseline of all the 
 of the lowermost 
The arithmetic mean of 
characters of a zone is calculat
elements having baseline b1, 
baselines of all the valid charac
as Baseline(n) which will be repre
 
Figure 8. Baseline of the c
For each valid character, th
baseline with the Baseline(n) is sto
of baseline of a valid characte
0.01 inches, then those chara
bucket of acceptance. Toleranc
experimental results which give
the baseline and Tolerance com
number of elements lying withi
then the zone will be cons
Otherwise, the zone is consid
This process is carried out to 
the bucket which are having h
the other characters. 
IV. EX
Three features are extracted
the feasibility and the validity 
zone images scanned at 200 D
printed zones and 518 handwrit
Testing has been carried ou
scanned at 300 DPI. Classifica
printed zones and 1727 handwr
the recognition accuracy achiev
TABLE. I  RECOGNITION ACCU
PRINTED TEXT US
Type of Zone Total Number of Zones 
N
D
Handwritten 1727 
Printed 1665 
 
V. CO
In this paper a set of new
images and an approach to
the baselines of all the valid 
ed. Let a zone contains n valid 
b2, b3... bn. The mean of the 
ters of a zone will be computed 
sented by (6): 
                                             (6) 
 
 
haracters and the tolerance 
e module of the difference of its 
red as a feature. If the deviation 
r is greater than threshold, i.e. 
cters are eliminated from the 
e is computed on the basis of 
s the threshold. Figure 8 shows 
puted for a sample zone. If total 
n the bucket of acceptance is n, 
idered to have printed text. 
ered to have handwritten text. 
eliminate those characters from 
uge deviation in baseline from 
PERIMENTS 
 for each zone. To demonstrate 
of the proposed approach, 967 
PI and 300 DPI containing 449 
ten zones were tested. 
t on another 3392 zone images 
tion scheme is applied on 1665 
itten zones. TABLE I represents 
ed from our solution. 
RACY OF HANDWRITTEN AND 
ING VALIDATING DATA 
umber of Zones 
etected Correctly 
Recognition 
Accuracy 
1662 96.24 
1518 91.17 
NCLUSION 
 features to be extracted from 
 find classification rules for 
2014 IEEE International Advance Computing Conference (IACC) 1031
identifying printed and handwritten text in form documents is 
proposed. The system was implemented and tested on 3392 
form images. Experiments show that the methodology being 
used is robust and applicable to a majority of document types. 
Moreover, the extracted features to represent the printed and 
handwritten words proposed make the system independent of 
the document layout in the identification task. Finally, as the 
approach presents good results by using only classification 
rules; it is also less time consuming then all others 
methodologies used till now. In future, to increase the 
recognition accuracy, other features such as stroke width and 
pixel distribution can be integrated with the proposed 
classification technique. 
ACKNOWLEDGMENT 
We are very grateful to the reviewers for their valuable 
comments. We also extend our gratitude to Mr. Raju Gupta, 
Mr. Prasad Nemmikanti, Mr. Mayank Kumar, Ms. Puja Lal 
and Mr. Dinesh Ganotra of Newgen Software Technologies 
Ltd. for giving their valuable suggestions during this work. 
REFERENCES 
[1] S. Imade, S. Tatsuta, and T. Wada, Segmentation and 
Classification for Mixed Text/Image Documents Using Neural 
Network , Proceedings of the Second International Conference on 
Document Analysis and Recognition, 20-22 Oct., pp. 930 - 934, 
1993. 
[2] S. Violante, R. Smith, and M. Reiss, A Computationally Efficient 
Technique for Discriminating Between Hand-Written and Printed 
Text, IEEE Colloquium on Document Image Processing and 
Multimedia Environments, 2 Nov.,pp. 17/1 - 17/7, 1995. 
[3] I K. Kuhnke, L. Simoncini, and Z. M. Kovacs-V, A System for 
Machine- Written and Hand-Written Character Distinction, 
Proceedings of the Third International Conference on Document 
Analysis and Recognition,v. 2,14 - 16 Aug.,pp 811 - 814, 1995. 
[4] J. E. B. Santos, B. Dubuisson, and F. Bortolozzi, A Non 
Contextual Approach for Textual Element Identification on Bank 
Cheque Images, IEEE International Conference on Systems, Man 
and Cybernetics, v. 4, pp 6 - 9, 2002. 
[5] J. E. B. Santos, B. Dubuisson, and F. Bortolozzi, Characterizing 
and Distinguishing Text in Bank Cheque Images, Proceedings XV 
SIBGRAPI, pp. 203 - 209, 2002. 
[6] F. Farooq, K. Sridharan, and V. Govindaraju, Identifying 
Handwritten Text in Mixed Documents, ICPR 2006, 18th 
International Conference on Pattern Recognition, v. 2, pp. 1142 - 
1145, 2006. 
[7] J. Koyama, M. Kato, and A. Hirose, Local-spectrum-based 
distinction between handwritten and machine-printed characters, 
15th IEEE International Conference on Image Processing, 12-15 
Oct., pp. 1021 - 1024, 2008. 
[8] J. Franke, and M. Oberlander, Writing Style Detection by 
Statistical Combination of Classifiers in Form Reader 
Applications, Proceedings of the 2nd Intern. Conference on 
Document Analysis and Recognition, pp. 581 - 584, 1993. 
[9] S. N. Srihari, Y. C. Shin, V. Ramanaprasad, and D. S. Lee, A 
System to Read Names and Addresses on Tax Forms, Proceedings 
of the IEEE, v. 84, n 7, pp. 1038 - 1049. DOI: 10.1109/5.503302, 
1996. 
[10] Y. Zheng, H. Li, and D. Doermann, The Segmentation and 
Identification of Handwriting in Noisy Document Images, , 
Document Analysis Systems V, Lecture Notes in Computer 
Science, v. 2423, pp. 95-105, 2002. 
[11] Y. Zheng, H. Li, and D. Doermann, Text Identification in Noisy 
Document Images Using Markov Random Field, Proceedings of 
the Seventh International Conference on Document Analysis and 
Recognition, v. 1, pp. 599 - 603, 2003. 
[12] Y. Zheng, H. Li, and D. Doermann, Machine Printed Text and 
Handwriting Identification in Noisy Document Images, , IEEE 
Transactions on Pattern Analysis and Machine Intelligence, v. 26, 
n 3, pp. 337 - 353, 2004. 
[13] U. Pal, and B. B. Chaudhuri, Automatic separation of machine-
printed and hand-written text lines, ICDAR ’99. Proceedings of the 
Fifth International Conference on Document Analysis and 
Recognition, pp. 645-648, 1999. 
[14] U. Pal, and B. B. Chaudhuri, Machine-printed and Hand-written 
Text Line Identification, Pattern Recognition Letters, v. 22, n 3 - 4, 
pp. 431 - 441, 2001. 
[15] J. K. Guo, and M. Y. Ma, Separating Handwritten Material from 
Machine Printed Text Using Hidden Markov Models, , 
Proceedings. Sixth International Conference on Document 
Analysis and Recognition, pp. 439 - 443, 2001. 
[16] E. Kavallieratou, and S. Stamatatos, Discrimination of Machine-
Printed from Handwritten Text Using Simple Structural 
Characteristics, Proceedings of the 17th International Conference 
on Pattern Recognition, ICPR 2004, v. 1, 23 - 26 Aug., pp.437 - 
440, 2004. 
[17] E. Kavallieratou, S. Stamatatos, and H. Antonopoulou, Machine-
Printed from Handwritten Text Discrimination, IWFHR-9 2004, 
9th Intern. Workshop on Frontiers in Handwriting Recognition, 26-
29 Oct., pp. 312 - 316, 2004. 
[18] N. Otsu, A threshold selection method from gray-level histograms, 
IEEE Transactions on Systems, Man, and Cybernetics 9 (1) (1979) 
62 –66. 
[19] Dillencourt, M.B., Samet,H., and Tammininen,M., 1992. General 
approach to Connected-Component Labeling for Arbitrary Image 
Representations, In J.ACM Vol 39, No.2, 1992, pp. 253-280. 
[20] Chih-Hong, K., Hon-Son, D., 2005. Skew Detection of Document 
Images Using Line Structural Information, In Third International 
Conference on Information Technology and Applications 
(ICITA'05) Volume 1. 
[21] Shi, Z., Govindaraju, V., 2003. Skew Detection for Complex 
Document Images Using Fuzzy Runlength. In Seventh 
International Conference on Document Analysis and Recognition 
(ICDAR'03) - Volume 2. 
[22] Agarwal, A., Kumar, P., & Kumar, S. (2006). ICR detection in 
filled form and form removal. In International Conference on 
Computer Application Theory and Application (VISAPP) (1) (pp. 
271-276). 
 
1032 2014 IEEE International Advance Computing Conference (IACC)
