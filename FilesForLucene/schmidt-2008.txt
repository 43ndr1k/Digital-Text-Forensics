Hausarbeit und Bericht: Automatische Autorenerkennung mit
Kollokationen als Klassikationsmerkmale
Armin Schmidt
08/2008
Zusammenfassung
Herk√∂mmliche Ans√§tze zur automatischen Autorenerkennung, der automatischen Zuordnung eines an-
onymen Textes zu einem einzelnen Autoren aus einer Gruppe von Kandidaten, beruhen auf stilistischen
Merkmalen wie Satzl√§nge, H√§ugkeit von Funktionsw√∂rtern, Wortschatz eines Autoren oder auch auf syn-
taktischen Hinweisen wie H√§ugkeit von Nebens√§tzen, der L√§nge einzelner Phrasen, etc. Die vorliegende
Arbeit baut auf der Hypothese auf, dass Kollokationen, Wortgruppen, deren Bedeutung √ºber die einfache
Zusammensetzung der Bedeutungen ihrer Bestandteile hinausgeht, ein wichtiges Merkmal des individuellen
Stils eines Autors seien. Zum Pr√ºfen dieser These wurde ein Experiment durchgef√ºhrt, dessen Ergebnisse
hier vorgestellt werden sollen. Wie sich zeigen wird, muss die Ausgangshypothese abgelehnt werden; ob-
gleich Kollokationen ein Stilmerkmal sein m√∂gen, lassen sie sich mit der hier pr√§sentierten Methode zur
automatischen Autorenerkennung nicht erfolgreich einsetzen.
1 Einleitung
Der Begri automatische Autorenerkennung beschreibt die Problemstellung, einem Text, dessen Verfasser
unbekannt ist, einen einzelnen Autoren aus einer Gruppe von Kandidaten zuzuordnen. Dies ist eine klas-
sische Klassikationsaufgabe, deren Methoden und Ergebnisse nicht auf die Dom√§ne der Autorenerkennung
beschr√§nkt sein m√ºssen, sondern potentiell auf andere Felder der Textklassikation angewandt werden k√∂nnen.
Ich m√∂chte in diesem Abschnitt die Besonderheiten der Problematik kurz skizzieren und einige bisher verfolgte
Ans√§tze knapp √ºberschlagen.
Das Zuordnen eines anonymen Textes zu einem von mehreren m√∂glichen Verfassern ist beispielsweise in der
forensischen Linguistik bei der Analyse von Bekennerschreiben von konkreter Bedeutung, aber auch im Bereich
der Geschichts- oder Literaturwissenschaft, wo, um eine korrekte Interpretation f√ºhren zu k√∂nnen, der Autor
eines Dokuments bekannt sein muss. Normalerweise ist die Anzahl der Kandidaten beschr√§nkt auf einige weni-
ge, in der Praxis auf zwei bis f√ºnf [Stamatatos et al., 2001, Abschnitt 1]. Ausschlaggebend f√ºr die Performanz
eines automatischen Klassikationsalgorithmus ist oft die L√§nge eines Dokuments [z.B. Sichel, 1986]; hier un-
terscheiden sich die Anwendungsdom√§nen stark voneinander. Kriminelle Schreiben wie sie in der forensischen
Linguistik untersucht werden, sind oft sehr kurz und bieten daher relativ wenig Analysematerial, auf das eine
Klassikationsmethode aufbauen kann, welche, und das ist in den hier besprochenen Ans√§tzen der Fall, einen
wohlgeformten Text ohne orthographische Fehler erwartet und aus diesem stilistische Merkmale zu extrahieren
versucht [Olsson, 2008]. Literarische oder historische Texte hingegen zeigen gerade entgegengesetzte Eigen-
schaften: sie sind vergleichsweise lang und eher wohlgeformt. In Bezug auf literarische Texte wurde allerdings
auf die Problematik der Heterogenit√§t hingewiesen [Stamatatos et al., 2001] - sie bestehen oft auf mehreren
Textarten, wie z.B. Dialogen und erz√§hlenden Abschnitten und zielen mitunter gerade auf stilistische Diversit√§t
1
ab, was f√ºr Klassikationsalgorithmen problematisch sein kann. Stamatatos et al. [2001] weist darauf hin, dass
es w√ºnschenswert w√§re, entsprechende Abschnitte zuerst zu erkennen und dann diejenigen f√ºr die Klassikation
zu nutzen, welche am ehesten den Stil des Autors wiedergeben. Ich bin mir jedoch keiner Arbeit bewusst, die
sich dieser Problematik annimmt.
Die hier beschriebenen Experimente streben keine Bearbeitung eines bestimmten Anwendungsszenarios an,
sondern verstehen sich als Versuch, eine allgemeine Klassikationsaufgabe mittels Betrachtung von Kollo-
kationen zu l√∂sen. Dabei steht die Zielsetzung im Vordergrund, ausreichend lange Texte einer bestimmten
Dom√§ne (hier: Zeitungsartikel) so zu klassizieren, dass auch bei einer gr√∂√øeren Anzahl von Kandidaten, in
diesem Fall zehn, der richtige ausgew√§hlt wird. Die Arbeitshypothese soll sein, dass Kollokationen als stilistische
Merkmale geeignet sind, die Aufgabe befriedigend zu l√∂sen.
Die Hausarbeit ist wie folgt aufgebaut: In Abschnitt 1.1 werde ich knapp einige Ans√§tze zur automatischen
Autorenerkennung beschreiben. Abschnitt 2 begr√ºndet die Verwendung von Kollokationen als Klassikati-
onsmerkmale und f√ºhrt kurz in den theoretischen Hintergrund ein. In Sektion 3 ndet die Beschreibung der
Experimente statt. Zun√§chst werde ich dort die Wahl des ausgew√§hlten Korpus legitimieren. Anschlie√øend stelle
ich die Baseline-Methode und deren Ergebnisse vor. Nachfolgend, in Abschnitt 3.3.2, meinen eigenen Ansatz.
Eine Fehleranalyse folgt in Abschnitt 3.3.4. Im Anschluss daran werde ich einige Folgeversuche schildern und
konzis auswerten. Abschnitt 4 stellt das Fazit, in welchem die Ergebnisse in einem globaleren Ma√øe bewertet,
und Hinweise auf zuk√ºnftige Aufgaben gegeben werden sollen.
1.1 √úberblick √ºber andere Ans√§tze
Bis vor kurzem nahmen Ans√§tze zur automatischen Autorenerkennung fast ausschlie√ølich einfache Ober√§-
chenma√øe in Gebrauch, zumeist mit Wortformen als kleinste Einheit. Obwohl es in fr√ºheren Tagen Versuche
gegeben hat, Merkmale auch unterhalb der Wortebene, z.B. in Hinsicht auf die Anzahl Buchstaben [Brinegar,
1963] oder die Anzahl Silben [Fucks, 1952] pro Wort anzuwenden, haben die erfolgreichsten Methoden lexika-
lische Ma√øe herangezogen. Holmes [1994] beschreibt zwei Trends innerhalb der lexikalischen Methoden: zum
einen Ma√øe, die den Wortschatz eines Autors respektive des Textes widerspiegeln. Und zum anderen Ma√øe, wel-
che die H√§ugkeit bestimmter Funktionsw√∂rter messen. Ersteres l√§sst sich beispielsweise durch das Z√§hlen von
W√∂rtern, die nur einmal (Hapax Legomena) bzw. zweimal (Dislegomena) im Text vorkommen, approximieren,
was jedoch zu Ergebnissen f√ºhrt, die stark von der Textl√§nge abh√§ngen. H√§ugkeiten von Funktionsw√∂rtern
hingegen haben den Nachteil, dass Mengen ausgew√§hlter Funktionsw√∂rter schlecht auf verschiedene Grup-
pen von Autoren angewandt werden k√∂nnen. Die Baseline-Methode meiner Experimente orientiert sich frei an
Burrows [1987] und bedient sich der h√§ugsten W√∂rter einer Textsammlung, ohne Unterscheidung zwischen
Inhalts- und Funktionsw√∂rtern.
Ein vielversprechender Ansatz ist Stamatatos et al. [2001], der √ºber die Ebene lexikalischer Ma√øe hinaus-
geht und Klassikationsmerkmale auf Basis der Ausgabe eines Satz- und Chunk-Grenzen-Erkenners (SCBD)
bestimmt. Dabei unterscheidet er low-level measures, wie Satzl√§nge und Anzahl Interpunktionzeichen, phrase-
level measures, wie der L√§nge von Verbal- oder Nominalphrasen, und analysis-level measures, welche die
Ausgabe des SCBD-Tools n√§her betrachten und dessen Ungenauigkeiten verarbeiten. Letzteres beinhaltet z.B.
die Anzahl von W√∂rtern, die SCBD nicht erkannt hat und welche somit als potentielle Fremdw√∂rter gehandhabt
werden k√∂nnen. Jener Artikel hat auch den Ausschlag f√ºr meinen Ansatz gegeben, indem er sagt: In general,
real natural language processing (NLP) (i.e. computational syntactic, semantic, or pragmatic analysis of text)
is avoided since current NLP tools do not manage to provide very high accuracy dealing with unrestricted
text. Die vorliegende Arbeit stellt einen Versuch dar, genau diesen bisher vernachl√§ssigten Bereich anzugehen,
indem sie ein semantisches Ph√§nomen hinsichtlich seines Nutzens zur Autorenerkennung untersucht.
2
2 Kollokationen
Jeder Mensch besitzt eine Menge von Begriichkeiten, die er oder sie sich im Laufe des Heranwachsens und
sp√§ter im Studien- und Berufslebens angeeignet hat. Eine besondere Rolle f√§llt dabei den Idiomen zu, die
sich regional zum Teil deutlich unterscheiden und selbst nur innerhalb der eigenen Familie mehr oder weniger
h√§ug gebraucht werden. Diese Eingebung f√ºhrt mich zu der Annahme, dass Idiome ein potentiell bedeutsames
Merkmal f√ºr die Autorenschaft eines Textes sein k√∂nnte. Idiome in der automatischen Verarbeitung zu verwerten
bringt jedoch drei Schwierigkeiten mit sich: Zum einen sind Listen mit idiomatischen Wendungen schwer zu
beschaen. Zweitens sind solche Listen vermutlich nur schlecht auf verschiedene Dom√§nen oder gar einzelne
Autoren √ºbertragbar. Drittens kommen reine Idiome leider viel zu selten in den zu klassizierenden Texten
vor, gerade in einer Dom√§ne wie Zeitungsartikel, die ich f√ºr meine Experimente genutzt habe. Als Alternative
k√∂nnen daher Kollokationen ins Auge gefasst werden. Diese k√∂nnen mit kleinem Aufwand mittels statistischer
Methoden aus einer Textsammlung extrahiert werden und schlie√øen zudem, so sie denn vorkommen, potentielle
Idiome ein. Kollokationen und Idiome sind zwar vom Standpunkt der lexikalischen Semantik durchaus zu
unterscheiden, dieser Unterschied wird jedoch durch die statistische Berechnung weitestgehend ignoriert.
Der Begri Kollokation ist nicht einheitlich deniert. Eingef√ºhrt von Firth [1957], bezeichnet er zun√§chst ein
regelhaft erwartbares gemeinsames Auftreten zweier oder mehrerer W√∂rter. Zentral ist dabei der Gedanke,
dass Kollokationen einer semantischen, nicht grammatischen Begr√ºndung folgen. Cruse [1986] verfeinert den
Begri, indem er ihn dem des Idioms gegen√ºberstellt und dies im Kontext semantischer Transparenz begr√ºn-
det. Demnach stellt ein Idiom wie den L√∂el abgeben eine einzige minimale semantische Konstituente dar, in
diesem Falle also [sterben]. Eine weitere Einschr√§nkung f√ºr Idiome ist, dass sie nicht modizierbar sind; ?einen
L√∂el abgeben ist genauso wenig eine Kollokation mit verwandter Bedeutung wie ?den L√∂el und die Gabel
abgeben. Kollokationen hingegen sind semantisch transparent, d.h. sie bestehen aus mehreren semantischen
Konstituenten, deren Teile jedoch nur zusammen und im jeweiligen Kontext einen bestimmten semantischen
Inhalt formen. Am Beispiel starker Raucher l√§sst sich dies verdeutlichen: der Ausdruck ist semantisch trans-
parent, denn er besteht aus zwei W√∂rtern, die jeweils eine individuell erschlie√øbare Bedeutung besitzen und
zwei semantische Konstituenten repr√§sentieren, [stark] und [Raucher]. Nichtsdestotrotz selektieren sich ihre
Bedeutungen gegenseitig; stark im Kontext von eine starke Frau dr√ºckt einen anderen semantischen Inhalt
aus, als in Kontexten wie starker Raucher, starker Trinker, etc. Kollokationen sind √ºberdies modizierbar, wie
das Beispiel ein st√§rkerer Raucher belegt. Cruse f√ºhrt weitere Unterschiede in der Klasse der Kollokationen
an. So ist z.B. eine gebundene Kollokation (engl. bound collocation) eine, bei denen zumindest einer der Teile
unikal selektiv ist, d.h. sie kommen in ihrer Bedeutung allein mit einem bestimmten anderen Wort bzw. in
einer bestimmten Kollokation vor. Eine gebundene Kollokation w√§re beispielsweise das englische to foot a bill
(dt. eine Rechnung bezahlen).
In der Computerlinguistik wird der Begri der Kollokation normalerweise weniger eingeschr√§nkt deniert. So
orientieren sich Manning and Sch√ºtze [2003] locker an der Denition von Firth [1957] und beschr√§nken sich
weitestgehend auf die Voraussetzung, dass eine Kollokation beschr√§nkt kompositionell sei, d.h. die Bedeutung
des Ganzen nicht komplett aus der Bedeutung der Teile herleitbar sei. Zumindest aber den feineren Unter-
scheidungsebenen aus Cruse [1986] wird die Behandlung von Kollokationen in Manning and Sch√ºtze [2003]
nicht gerecht. So unterscheiden die dort vorgestellten Techniken nicht zwischen einfachen und gebundenen
Kollokationen, Idiomen oder Eigennamen, extrahieren ggf. auch terminologische Ausdr√ºcke und beschr√§nken
sich weitestgehend auf n-Gramme fester Gr√∂√øe, deren Teile also unmittelbar adjazent sein m√ºssen, um als
Kollokation aus einer Textsammlung isoliert zu werden. Der Hinweis auf diese Problematik soll keine generelle
Kritik an Manning and Sch√ºtze [2003] sein; ich denke, dass der dortige Rahmen als auch m√∂gliche Anwendun-
gen einige theoretische Ungenauigkeiten rechtfertigen. Nichtsdestoweniger m√∂chte ich auf sie hinweisen, denn
der in der vorliegenden Arbeit eingesetzte Kollokationsbegri ist linguistisch ebenfalls weitestgehend nachl√§ssig
deniert und schlie√øt eben genannte Ungenauigkeiten mit ein.
3
3 Experimente
3.1 Korpus
Zum √úberpr√ºfen der These, Kollokationen seien ein gutes Stilmerkmal anhand dessen ein anonymer Text ei-
nem Autoren sicher zugeordnet werden kann, habe ich Artikel aus der Online-Ausgabe der deutschen Zeitung
DIE ZEIT 1 der Jahre 1999 und 2000 verwendet. DIE ZEIT erscheint w√∂chentlich und richtet sich insbeson-
dere an Leser der Bildungsschicht. Ich halte die Zeitung aus mehreren Gr√ºnden gut f√ºr die Aufgabe geeignet.
Zum einen sind, im Vergleich zu Tageszeitungen, die Artikel der ZEIT im Durchschnitt l√§nger. Im Allgemei-
nen gilt: je l√§nger die Texte, desto erfolgreicher ist die automatische Autorenerkennung [Stamatatos et al.,
2001], einfach deshalb, weil aus langen Texten eine gr√∂√øere Anzahl Stilmerkmale extrahiert werden kann. Zum
zweiten besch√§ftigt DIE ZEIT nicht ausschlie√ølich professionelle Journalisten, sondern zu einem gewissen Teil
Fachleute einzelner Sparten, und ver√∂entlicht regelm√§√øig Essays und Meinungsbilder einussreicher Perso-
nen der Gegenwart, wie Schriftsteller, Politiker und Philosophen2. Insbesonders Letztere unterliegen weniger
stark den von der Zeitung vorgesehenen Stilvorgaben, so dass sich die individuellen Stile der Autoren in den
entsprechenden Artikeln realisieren k√∂nnen.
3.1.1 Vorverarbeitung
Da die Artikel des Korpus im HTML-Format vorlagen, waren verschiedene Vorbereitungsschritte notwendig.
Zun√§chst habe ich die Information √ºber die Autoren aller Artikel extrahiert und diejenigen zur Weiterverarbei-
tung gew√§hlt, die im Laufe der Jahre 1999 bis 2000 mindestens 15 und h√∂chstens 30 Artikel verfasst hatten3.
Es lie√øen sich auch Autoren nden, die insgesamt wesentlich mehr Texte geschrieben hatten, welche jedoch
normalerweise zu kurz (< 300 W√∂rter pro Text) und daher f√ºr die Aufgabe der Autorenerkennung ungeeignet
waren (Meistens handelte es sich dabei um kurze Nachrichtentexte oder Glossen). Von den verbleibenden
Autoren wurden diejenigen 10 f√ºr das Experiment gew√§hlt, deren durchschnittliche Textl√§nge am gr√∂√øten war.
Von diesen 10 wiederum wurden die jeweils l√§ngsten 18 Texte ausgew√§hlt, denn je l√§nger ein Text, desto mehr
Information enth√§lt er, die f√ºr die Autorenerkennung wichtig ist. Zur leichteren Weiterverarbeitung habe ich
alle W√∂rter in Kleinschreibung konvertiert und durch Zeilenumbr√ºche voneinander getrennt, so dass in jeder
Zeile genau ein Wort steht. Interpunktionszeichen wurden, so weit es ging, gel√∂scht4. Anschlie√øend wurden
die Daten nach dem Zufallsprinzip5 in Trainings- und Testmenge mit zw√∂lf beziehungsweise sechs Texten pro
Autor geteilt. Algorithmus 1 stellt die Vorverarbeitungsschritte schematisch dar. Tabelle 1 gibt eine √úbersicht
√ºber das f√ºr das Experiment verwendete Datenmaterial. Trainings- und Testmenge zusammen haben eine
Gr√∂√øe von 255.424 W√∂rtern (tokens).
1http://www.zeit.de
2Zur √úbersicht siehe: http://www.zeit.de/autoren/A/index.xml
3Es soll erw√§hnt sein, dass die Meta-Angaben in den HTML-Dateien, wie z.B. Autor oder Rubrik, keiner einheitlichen For-
matierung folgt. Die hier angewandte Extraktionsmethode verwendet regul√§re Ausdr√ºcke, so dass der Autor einiger Texte evtl.
nicht korrekt identiziert werden konnte. F√ºr das Experiment wurden nur solche Artikel benutzt, die einem Autoren eindeutig
zuzuweisen waren.
4Eine Ausnahme hiervon ist der Bindestrich (-), der auch auch zusammengesetzte W√∂rter, insbesondere Nomen, verbindet
und erw√ºnscht ist.
5Zur zuf√§lligen Auswahl von Dateien habe ich das UNIX-Tool shuf benutzt.
4
Algorithm 1 Vorverarbeitungsschritte
1. Auswahl der Autoren mit n Texten, so dass 15 ‚â§ n ‚â§ 30
2. Davon diejenigen 10 Autoren ausw√§hlen, deren Texte durchschnittlich am l√§ngsten sind
3. Von jedem der verbleibenden Autor die 18 l√§ngsten Texte nehmen
4. Zu Kleinschreibung konvertieren, Interpunktionszeichen l√∂schen
5. In Trainings- und Testmengen trennen mit jeweils 12 bzw. 6 Texten pro Autor
Tabelle 1: Korpus
Autor W√∂rter insgesamt durchschn. Textl√§nge Wortschatz Themen
a1 21780 1210 7019 Philosophie
a2 18966 1053 5971 Innenpolitik
a3 26783 1487 6532 Politik/Portraits
a4 31140 1730 8357 Wirtschaft
a5 21534 1196 5501 Telekommunikation
a6 29253 1625 7952 Architektur
a7 20885 1160 5705 Technik
a8 30283 1682 7879 Schule/Beruf
a9 29862 1659 8703 Au√øenpolitik/Russland
a10 24938 1385 7836 Gesellschaft & Technik
gesamt 255.424 1419 
5
3.2 Baseline
3.2.1 Klassikation
Die Baseline-Methode klassiziert einen Text anhand der in ihm vorkommenden Ober√§chenformen. Hierzu
wurden aus allen 180 Texten die 100 h√§ugsten W√∂rter extrahiert. Ihre relative H√§ugkeit in den Texten
der Trainingsmenge wurden als Merkmale f√ºr einen maschinellen Lernalgorithmus verwendet und von diesem
einzeln und f√ºr die jeweilige Klasse, d.h. den jeweiligen Autor, gewichtet. Anschlie√øend wurden die Texte der
Testmenge mittels Weighted Overlap (gewichtete √úberlappung) klassiziert (s.u.).
Zur Traversion des Korpus sowie zur Berechnung der relativen Worth√§ugkeiten habe ich die freie und f√ºr
die maschinelle Sprachverarbeitung konzipierte Python-Programmbibliothek NLTK [Bird, 2006] eingesetzt.
Zur Durchf√ºhrung der Gewichtung sowie zur eigentlichen Klassikation habe ich die ebenfalls freie maschinelle
Lernsoftware TiMBL [Daelemans et al., 2007] angewandt. TiMBL implementiert mehrere Algorithmen und Me-
triken basierend auf einem memory-based (auch: example-based) Lernansatz [Daelemans and van den Bosch,
2005]. Memory-based Learning versucht, neue Situationen mit einer Menge alter gespeicherter Situationen
zu vergleichen und wendet keine abstrakten Klassikationsregeln an. Die Merkmalswerte zum Trainieren und
Testen m√ºssen so aufbereitet werden, dass jede Instanz einer Klasse einer Zeile der Eingabedatei entspricht, die
wiederum die Merkmalswerte in feststehender Reihenfolge enth√§lt. Jede Zeile stellt also einen Merkmalsvektor
dar. Das letzte Feld einer Zeile enth√§lt immer den Klassennamen, in meinem Fall also a1 . . . a10. Bei der Klas-
sikation wird dann jeder Merkmalsvektor der gespeicherten Beispiele mit dem jeweiligen neuen Eingabevektor
verglichen und f√ºr letzteren die √§hnlichste Klasse anhand eines √Ñhnlichkeitsma√øes ermittelt.
F√ºr die Experimente wurden durchweg dieselben Einstellungen genutzt, so dass Unterschiede in der Perfor-
manz allein auf die Auswahl der Merkmale und nicht auf den gerade in Gebrauch genommenen Lernalgorithmus
zur√ºckzuf√ºhren sind. Desweiteren habe ich die Standardeinstellungen verwendet, denn die Experimente zielen
letztlich lediglich darauf ab, einen Vergleich mehrerer Methoden zu gestatten und sollen keine maximal perfor-
manten Ergebnisse erreichen. Eingesetzt wurde dementsprechend die Weighted-Overlap-Metrik f√ºr numerische
Merkmale.6
3.2.2 Ergebnisse
Die Baseline-Methode klassizierte 38 von 60 Texten der Testmenge richtig, was einer Akkuratheit von rund
0,633 bzw. einem durchschnittlichen Fehler von rund 0,366 entspricht. Tabelle 2 zeigt die entsprechende
Konfusionsmatrix. Die Zeilen entsprechen den echten Klassenzugeh√∂rigkeiten, die Spalten den von TiMBL
vorhergesagten Autoren. In der Spalte ganz rechts sind die gerundeten Klassikationsfehler pro Autor angege-
ben.
3.2.3 Fehleranalyse
Es f√§llt auf, dass die fehlerhaft klassizierten Texte nicht zuf√§llig gestreut zu sein scheinen. So wurden bei-
spielsweise 7 Texte f√§lschlicherweise Autor 10 zugeordnet und 6 Texte irrt√ºmlich mit Autor 6 assoziiert (siehe
vorletzte Zeile). Den Autoren 1, 3, 6 und 7 jedoch wurden keine Texte fehlerhaft zugewiesen. Betrachtet man
noch einmal Tabelle 1, so l√§sst sich dieser Umstand nicht durch Gr√∂√øe des Vokabular oder durchschnittliche
Textl√§nge einzelner Autoren erkl√§ren. Auch eine n√§here Betrachtung der Vorkommen einzelner W√∂rter in den
Texten bestimmter Autoren legen hier keine Systematik nahe. Ich vermute daher, dass, entgegen des ersten
Eindrucks, die Streuung fehlerhaft zugeordneter Texte zuf√§llig ist (siehe vergleichsweise Abschnitt 3.3.4).
6Eine Erl√§uterung der maschinellen Lernalgorithmen w√ºrde den Rahmen dieses Berichts leider sprengen. F√ºr eine √ºbersichtliche
Einf√ºhrung verweise ich auf Daelemans et al. [2007], insbesondere Kapitel 5.
6
Tabelle 2: Konfusionsmatrix der Baseline-Methode
echte Klasse ‚Üí a10 a9 a8 a7 a6 a5 a4 a3 a2 a1 Fehler
a10 4 0 0 0 0 0 1 0 1 0 0,33
a9 1 4 0 0 0 1 0 0 0 0 0,33
a8 1 1 0 0 0 0 2 0 2 0 1
a7 0 0 0 4 0 1 1 0 0 0 0,33
a6 0 0 1 0 4 0 1 0 0 0 0,33
a5 0 0 0 0 0 6 0 0 0 0 0
a4 1 0 1 0 0 0 4 0 0 0 0,33
a3 1 0 0 0 0 0 0 5 0 0 0,16
a2 1 0 0 0 0 1 0 0 4 0 0,33
a1 2 0 0 0 0 0 1 0 0 3 0,5
falsch zugeordnete Texte 7 1 2 0 0 3 6 0 3 0
durchschnittl. Fehler: 0,366
3.3 Methode
3.3.1 Berechnung von Kollokationen: der t-Test
Eine h√§ug angewandte Methode zum Finden von Kollokationen in einem Korpus ist der t-Test. Diese Methode
des statistischen Hypothesentests betrachtet den Erwartungswert und die Varianz einer Stichprobe, stellt sie
einer Nullhypothese gegen√ºber und macht eine Aussage dar√ºber, wie sehr die erwartete und die tats√§chliche
Verteilung voneinander abweichen. Ist die Abweichung gro√ø, so kann die Nullhypothese abgelehnt werden;
die Stichprobe muss dann bei einem Signikanzniveau Œ± aus einer anderen Verteilung stammen. Bei Œ± =
0.005 kann die Nullhypothese mit einer Sicherheit von 99,5% abgelehnt werden, wenn t ‚â• 2.576. Dieser
Kondenzwert kann in jedem Statistikbuch nachgeschlagen werden, siehe z.B. Manning and Sch√ºtze [2003,
S. 309]. Der t-Test stellt eine Abbildung:
t= œá
‚Ä≤‚àí¬µ‚àö
s2
N
mit dem Erwartungswert der Stichprobe œá‚Ä≤, dem Erwartungswert der Verteilung ¬µ, der Standardabweichung
s2, und der Stichprobengr√∂√øe N dar.
Will man ihn auf die Berechnung von Kollokationen anwenden, so l√§sst dich der t-Test folgenderma√øen erwei-
tern: Ein Textkorpus wird als Sequenz von Bigrammen betrachtet, deren Stichproben Zufallsvariablen darstel-
len, die den Wert 1 annehmen, wenn das jeweilige Bigramm auftritt, und 0 sonst [Manning and Sch√ºtze, 2003].
Unter Verwendung der Maximum-Likelihood-Methode k√∂nnen wir die zur Berechnung des t-Wertes n√∂tigen
Parameter sch√§tzen, hier z.B. f√ºr das Wortpaar berliner republik:
Die Nullhypothese sei, dass die W√∂rter berliner und republik unabh√§ngig sind:
H0 : P (berliner, republik) = P (berliner)√ó P (republik) =
40
255434
√ó 69
255434
‚âà 4, 23‚àí8
7
Abbildung 1: Die 50 Bigramme mit den h√∂chsten t-Werten:
in der
in den
f√ºr die
mit dem
auf dem
nicht mehr
vor allem
aus dem
nicht nur
f√ºr den
- und
von der
an der
√ºber die
gibt es
in deutschland
auf den
das ist
auf die
aus der
mit einem
er sich
nach dem
um die
sich die
mit den
zum beispiel
in einem
an die
mehr als
dass die
bei der
ist es
am ende
ein paar
mit der
gar nicht
ist das
wenn sie
gegen die
sie sich
an den
in einer
die meisten
nur noch
von den
auch wenn
in dem
bis zum
l√§sst sich
Demnach ist ¬µ = 4, 23‚àí8. Das Wortpaar berliner republik kommt insgesamt 7 mal im Korpus vor; somit ist
der Stichprobenmittelwert œá‚Ä≤ = 7255433 ‚âà 2, 74
‚àí5. Die Standardabweichung kann auf œá‚Ä≤ = 2, 74‚àí5 angen√§hert
werden (siehe Manning and Sch√ºtze [2003], Seite 165). Somit ergibt sich der t-Wert durch
t = 2,74
‚àí5‚àí4,23‚àí8‚àö
2,74‚àí5
255433
‚âà 2, 6417.
Bei einem Signikanzniveau Œ± = 0.005 (Kondenzwert 2.576, s.o.) kann die Nullhypothese abgelehnt werden;
das Wortpaar berliner republik ist eine Kollokation.
Es wurden die t-Werte f√ºr alle im Korpus vorkommenden Bigramme errechnet. Ich habe mich dazu des ge-
samten Korpus bestehend aus allen 180 der 10 Autoren bedient; der Umstand, dass hierbei auch die f√ºr die
Klassizierung vorbehaltene Testmenge von 60 Texten inbegrien ist, stellt keine Schwierigkeit dar. Schlie√ø-
lich wird keinerlei Information (d.h. solche √ºber die Verwendung bestimmter Bigramme durch die Autoren) in
Gebrauch genommen, die sp√§ter induziert werden soll. Au√øerdem sollen gerade diejenigen Kollokationen extra-
hiert werden, die in den Texten der Autoren auch tats√§chlich vorkommen. Abbildung 1 zeigt die 50 Bigramme
mit den h√∂chsten t-Werten. Wie unschwer zu erkennen ist, nden sich unter diesen Bigrammen keine, die
man als Kollokation bezeichnen w√ºrde. Allenfalls zum beispiel k√∂nnte eventuell man als semantisch nicht rein
kompositionell betrachten.
Vermutlich l√§sst sich dieses Ergebnis auf die relativ kleine Gr√∂√øe des Trainingskorpus zur√ºckf√ºhren. Um es zu
verbessern, habe ich einen Filtermechanismus eingebaut, der alle diejenigen Bigramme ignoriert, deren erstes
oder zweites Wort aus weniger als 5 Zeichen besteht. So wird ein Gro√øteil der Funktionsw√∂rter enthaltenden
Bigramme au√øer Acht gelassen und gleichzeitig die Implementierung einer Stoppwortliste umgangen. Abbildung
2 zeigt die 50 Bigramme mit den h√∂chsten t-Werten nach der Filterung.
Neben vielen Personennamen nden sich in dieser Liste andere eindeutige Kollokationen, z.B L√§ndernamen wie
vereinigten staaten, Zeitperioden wie achtziger jahre oder Firmennamen wie france t√©l√©com. Anzumerken sei
8
Abbildung 2: Die 50 Bigramme mit den h√∂chsten t-Werten nach dem Filtern von Stoppw√∂rtern
vereinigten staaten
immer wieder
nicht einmal
gerhard schr√∂der
joschka scher
nicht zuletzt
wladimir putin
achtziger jahre
nicht gerade
angela merkel
milliarden dollar
vergangenen jahres
millionen dollar
alles andere
telecom italia
neunziger jahre
nichts anderes
schon heute
nicht alles
wenigen jahren
diese weise
silicon valley
trotz aller
nicht l√§nger
einem anderen
boris jelzin
wolfgang sch√§uble
schon immer
einen neuen
seine partei
france t√©l√©com
eines tages
statt dessen
einen namen
durch einen
rudolf scharping
diesen tagen
neunziger jahren
prozent aller
heute nicht
nicht immer
heute schon
schon einmal
unter anderem
vielleicht sogar
einen anderen
nicht genug
einer neuen
g√ºnter elsbett
antje vollmer
an dieser Stelle, dass zwar alle diese Bigramme einen t-Wert gr√∂√øer als der kritische Wert 2.576 bei Œ± = 0.005
besitzen, das Signikanzniveau bei meinen Experimenten jedoch keine wichtige Rolle spielt; die t-Werte dienen
vor allem der M√∂glichkeit, Bigramme nach ihrer Wahrscheinlichkeit, eine Kollokation darzustellen, sortieren
und diejenigen Bigramme f√ºr das Training des Klassizierers ausw√§hlen zu k√∂nnen, deren t-Werte am h√∂chsten
sind.
An dieser Stelle soll noch einmal explizit darauf hingewiesen werden, dass die hier eingesetzte Methode neben
der Extraktion vieler Nicht-Kollokationen noch eine zweite Schw√§che besitzt. Denn durch die Verwendung
ausschlie√ølich direkt benachbarter W√∂rtern sowie durch die Beschr√§nkung auf Bigramme werden all solche
Kollokationen √ºbergangen, die aus mehr als zwei W√∂rtern bestehen oder eben durch andere Satzteile unter-
brochen werden k√∂nnen. Abgesehen davon decken sich die Ergebnisse gut mit den Erwartungen von einer
Implementation des t-Tests nach Manning and Sch√ºtze [2003] und den dort vorgestellten Resultaten.
3.3.2 Klassikation
Zwei Dinge scheinen in Bezug auf die gefundenen Kollokationen in einem Text von Bedeutung zu sein: zum
einen die H√§ugkeit, mit der ein Autor eine bestimmte Kollokation benutzt. Es scheint aber gleichzeitig
sinnvoll mit einzubeziehen, mit welcher Wahrscheinlichkeit es sich bei einem Wortpaar denn √ºberhaupt um
eine Kollokation handelt.
Zun√§chst habe ich die 100 Bigramme mit den h√∂chsten t-Werten nach Filterung generiert. Als Merkmale f√ºr
den von TiMBL implementierten maschinellen Lernalgorithmus wurde f√ºr jedes Bigramm im jeweiligen Trai-
ningstext das Produkt aus t-Wert und relativer H√§ugkeit berechnet. Algorithmus 2 zeigt den entsprechenden
Pseudo-Code.
9
Algorithm 2 Berechnung der Merkmale mit Kollokationen
f u n c t i o n getBigram_tScoreMap ( t r a i n i n g d a t a ) :
b i g r amFreqD i s t = ge tB ig ramFreqD i s t ( t r a i n i n g d a t a )
bigram_tScoreMap
f o r e a c h bigram i n b i g ramFreqD i s t :
bigram_tScoreMap [ bigram ] = ca l c u l a t eTSco r e ( bigram )
r e t u r n bigram_tScoreMap
c o l l o c a t i o n s = getBigram_tScoreMap ( t r a i n i n g d a t a )
c o l l o c a t i o n s . f i l t e r S t o pWo r d s ( )
c o l l o c a t i o n s . s o r t ( )
c o l l o c a t i o n s = g e t F i r s t 1 0 0 C o l l o c a t i o n s ( c o l l o c a t i o n s )
f o r e a c h autho r :
f o r e a c h t e x t :
b i g r amFreqD i s t = getB ig ramFreqD i s t ( t e x t )
f o r c i n c o l l o c a t i o n s :
v a l u e = b ig ramFreqD i s t . f r e q ( c [ 0 ] ) ‚àó c [ 1 ]
wr i teToFeatVec ( v a l u e + " ,")
wr i teToFeatVec ( autho r )
3.3.3 Ergebnisse
Die Ergebnisse der beschriebenen Methode sind bescheiden (siehe Abschnitt 3.3.4): Die Akkuratheit betr√§gt
rund 0,167 (10 von 60 Texten wurden korrekt klassiziert). Tabelle 3 zeigt die entsprechende Konfusionsmatrix.
3.3.4 Fehleranalyse
Wieder f√§llt die ungleichm√§√øige Zuordnung der klassizierten Texte zu Autoren auf. Allein 52% der falsch
klassizierten Texte (26 von 50 Texte) wurden Autor 1 zugewiesen. Dieses Ergebnis l√§sst sich leicht nach-
vollziehen, wenn man sich die Verteilung der Kollokationen √ºber die Texte der einzelnen Autoren ansieht. So
kommen in den Trainingstexten von Autor 1 lediglich 20 der 100 als Kollokationen veranschlagten Bigramme
√ºberhaupt vor (siehe Tabelle 4). In den entsprechenden Texten der Autoren 3, 4, 5 und 7 hingegen nden
sich jeweils 45, 47, 47 bzw. 46 solcher Bigramme. Die Vermutung liegt nahe, dass der von TiMBL implemen-
tierte Trainingsalgorithmus Bigramme in Bezug auf einen bestimmten Autoren h√∂her gewichtet, wenn dieser
insgesamt nur wenige solche in seinen Texten benutzt. 7
3.4 Zus√§tzliche Experimente
Zum weiteren Vergleich und ohne gro√øen Mehraufwand lie√øen sich noch einige weitere Experimente umsetzen.
Diese sollen im Folgenden knapp vorgestellt werden.
7Eine noch genauere Analyse w√§re m√∂glich, wenn man sich anschaut, welche Kollokationen von welchen Autoren wie oft
eingesetzt wurden, eventuell sogar in Bezug auf die Verteilung in den 12 Trainingstexten. Dies liefe im Endeekt auf eine Analyse
des maschinellen Lernalgorithmus hinaus, worauf im Rahmen dieser Hausarbeit aus Platz- und Zeitgr√ºnden verzichtet werden
muss.
10
Tabelle 3: Konfusionsmatrix der Kollokationsmethode
echte Klasse ‚Üí a10 a9 a8 a7 a6 a5 a4 a3 a2 a1 Fehler
a10 1 0 1 0 0 0 0 0 1 3 0,83
a9 0 0 0 0 0 0 0 0 0 6 1
a8 1 0 2 0 0 0 2 0 1 0 0,667
a7 2 1 0 0 0 1 0 0 0 2 1
a6 0 0 0 0 0 0 0 0 0 6 1
a5 0 0 1 0 1 0 0 0 0 4 1
a4 1 2 0 0 1 0 0 0 0 2 1
a3 0 1 0 0 1 0 0 0 3 1 1
a2 0 0 2 0 0 0 0 0 2 2 0,667
a1 0 0 0 0 1 0 0 0 0 5 0,167
falsch zugeordnete Texte 4 4 4 0 4 1 2 0 5 26
durchschnittl. Fehler: 0,833
Tabelle 4: Kollokationen pro Autor
Autor Trainingsmenge Testmenge
a1 20 10
a2 32 31
a3 45 31
a4 47 33
a5 47 31
a6 24 20
a7 46 22
a8 33 28
a9 36 14
a10 38 26
11
3.4.1 Kollokationen ohne vorheriges Filtern
F√ºr oder wider Erwarten schneidet die Klassikation anonymer Texte besser ab, wenn diejenigen Bigramme als
Merkmale verwendet werden, die den h√∂chsten t-Wert besitzen, jedoch ohne, dass Stoppw√∂rter enthaltende
Bigramme vorher ausgeltert wurden (siehe Tabelle 1). Merkmalsberechnung, Training und Klassikation
verlaufen abgesehen davon analog zum in Abschnitt 3.3.2 beschriebenen Vorgehen. Wohlgemerkt handelt es
sich dabei nicht um Kollokationen im eigentlichen Sinn. Nichtdestotrotz betr√§gt die Akkuratheit der Methode
30%, was einer richtigen Klassikation von 18 aus 60 Texten entspricht.
3.4.2 Bigramme ohne t-Wert-Berechnung
Die in Abschnitt 3.4.1 vorgestellten Ergebnisse lassen vermuten, dass Bigramme ohne Betrachtung ihrer
Kollokations- bzw. t-Wertes bei der Autorenerkennung erfolgreich sein k√∂nnten. Um dieser These nachzu-
gehen, habe ich unter Verwendung der in NLTK 0.94 [Bird, 2006] zu Verf√ºgung gestellten Funktionalit√§t ein
einfaches Bigramm-Modell des Korpus generiert, in welchem die Wahrscheinlichkeit jedes Bigramms mittels
Maximum-Likelihood-Methode gesch√§tzt wird. Die f√ºr TiMBL erstellten Merkmalsvektoren stellen die 100
h√§ugsten Bigramme dar, deren relative Frequenzen in den jeweiligen Texten der Trainingsmenge die Werte
sind.
Die Methode klassiziert 19 von 60 Texten richtig (Akkuratheit 0.317) und ist damit minimal besser als die
im vorherigen Abschnitt 3.4.1 beschriebene. Diese Herangehensweise kann als √§quivalent zur Baseline gesehen
werden, die Bigramme anstelle von Unigrammen einsetzt. Insofern k√∂nnte man die Performanz, die ja nur
ca. halb so gut wie die der Baseline ist, √ºberraschend nden. De facto ist die Zerstreuung von Bigrammen
jedoch wesentlich gr√∂√øer, d.h. es gibt wesentlich mehr Merkmale, deren Werte 0 betragen, was die niedrige
Akkuratheit erkl√§ren w√ºrde.
3.4.3 Kombination von Kollokationen und Unigrammen
Die folgenden Ans√§tze kombinieren jeweils zwei der bisher vorgestellten Methoden. Die Anzahl der f√ºr das
maschinelle Lernen benutzten Merkmale verdoppelt sich dabei jeweils auf 200.
Als erstes habe ich die Baseline und die Methode mit (gelterten) Kollokationen kombiniert. Dabei el die
Performanz der Baseline auf eine Akkuratheit von 0.47 (28 von 60 Texten korrekt klassiziert). Dies entspricht
einer Akkuratheit von weniger als dem Mittel der beiden Methoden. Wieder l√§sst sich dieses Ergebnis auf die
starke Zerstreuung der Kollokationen zur√ºckf√ºhren, die zus√§tzliches Rauschen in die Trainingsdaten einf√ºhrt.
3.4.4 Kombination von ungelterten Kollokationen und Unigrammen
Die Kombination von Baseline und ungelterten Kollokationen (Abschnitt 3.4.1) bringt eine Verbesserung der
Performanz auf 60% Akkuratheit (36 von 60 Texten korrekt klassiziert). Dies liegt immer noch unter der
Baseline-Performanz (63% Akkuratheit), jedoch nur sehr knapp.
3.4.5 Kombination von Bigrammen und Unigrammen
Wider Erwarten liegt die Performanz der Kombination der Bigramm-Methode (Abschnitt 3.4.2) mit der Baseline
unter der im vorangegangenen Abschnitt 3.4.4 skizzierten Kombination von ungelterten Kollokationen und
Unigrammen. Dies deutet meines Erachtens darauf hin, dass der Unterschied zwischen der Bigramme-Methode
und dem Ansatz mit ungelterten Kollokationen zu vernachl√§ssigen sein d√ºrfte.
12
4 Fazit
In dieser Arbeit habe ich einen Ansatz zur Autorenerkennung vorgestellt, der Texte anhand der in ihnen vor-
kommenden Kollokationen klassiziert. Es handelt sich dabei um den Versuch, die Bedeutungsebene bei der
Bew√§ltigung der Aufgabe mit in Augenschein zu nehmen. Dazu wurde das Korpus in ein Bigramm-Modell
transformiert. F√ºr jedes Bigramm wurde der t-Wert berechnet, der es erm√∂glicht, Bigramme nach ihrer Wahr-
scheinlichkeit zu ordnen, dass sie Kollokationen seien. Die Ergebnisse der Methode wurden vorgestellt und aus-
gewertet. Der Ansatz wurde einer Baseline gegen√ºbergestellt, welche die relative H√§ugkeit einzelner W√∂rter als
Merkmale verwendet. Ein maschineller Lernalgorithmus stellte dabei die ben√∂tigte Klassikationsfunktonalit√§t
zur Verf√ºgung.
Wie bereits angedeutet, sind die Ergebnisse meines Ansatzes eher ern√ºchternd. Mehrere Gr√ºnde scheinen dabei
eine Rolle zu spielen, einige habe ich bereits in den Fehleranalysen diskutiert. Zun√§chst einmal scheint es aber
notwendig, die eigentliche Idee zu hinterfragen. Dass die Bigramme, die f√ºr die Klassizierung verwendet wur-
den, in bei weitem nicht allen F√§llen tats√§chliche Kollokationen sind, ist das Resultat der Extraktionsmethode,
die einen Hypothesentest verwendet (Abschnitte 2 sowie 3.3.1). Zweitens ist festzustellen, dass die Daten-
menge vermutlich einfach nicht ausreicht, um befriedigende Ergebnisse zu bekommen. Gerade beim √úbergang
von Unigrammen zu Bigrammen w√§ren entsprechend mehr Daten n√∂tig, um vergleichbare Ergebnisse zu er-
reichen. Tats√§chlich ist aber auch die urspr√ºngliche Zielsetzung der Verwertung der Bedeutungsebene nicht
erf√ºllt, denn Kollokationen sind zwar ein Ph√§nomen der lexikalischen Semantik, dennoch bewegt sich mein
Klassikationsansatz grunds√§tzlich auf der Ebene der Ober√§chenformen. Denn wo ist der Unterschied zum
traditionellen Ansatz, bestimmte, potentiell stilistisch bedeutsame, W√∂rter zur Klassizierung heranzuziehen?
Ich muss gestehen, dass mir keiner einf√§llt.
Des Weiteren wurde bisher nicht klar, ob Kollokationen tats√§chlich den Stil eines Autors kennzeichnen. Be-
schr√§nkt man sich auf reine Idiome, k√∂nnte dies so sein, m√ºsste jedoch ebenfalls √ºberpr√ºft werden. Ein weiterer
Blick auf Tabelle 2 legt jedoch die Vermutung nahe, dass durch Kollokationen, in diesem Fall ja viele Per-
sonennamen etc., eher das Thema des Textes beschrieben wird. Insofern sollte eine Folgestudie versuchen,
eine Methode zur Extraktion reiner Idiome zu implementieren, welche, so weit es geht, einer theoretischen
Motivation des Begris standh√§lt. Zweitens sollte die Extraktion der Merkmalsvektoren so gestaltet werden,
dass die Merkmalswerte weniger stark gestreut sind und eine bessere Gewichtung zulassen.
Literatur
S. Bird. NLTK: the natural language toolkit. Proceedings of the COLING/ACL on Interactive presentation
sessions, pages 6972, 2006.
C.S. Brinegar. Mark Twain and the Quintus Curtius Snodgrass Letters: A Statistical Test of Authorship.
Journal of the American Statistical Association, 58(301):8596, 1963.
J.F. Burrows. Word-Patterns and Story-Shapes: The Statistical Analysis of Narrative Style. Literary and
Linguistic Computing, 2(2):6170, 1987.
D. A. Cruse. Lexical Semantics, chapter 2.9. Cambridge Textbooks in Linguistics. Cambridge University Press,
1986.
W. Daelemans and A. van den Bosch. Memory-Based Language Processing. Studies in Natural Language
Processing. Cambridge Universitiy Press, 2005.
W. Daelemans, J. Zavrel, K. van der Sloot, and A. van den Bosch. TiMBL: Tilburg Memory-Based Learner.
http://ilk.uvt.nl/downloads/pub/papers/Timbl_6.1_Manual.pdf, December 2007. Version 6.1.
13
J. R. Firth. Modes of meaning. Papers in Linguistics 1934-1951, 1957.
W. Fucks. On Mathematical Analysis of Style. Biometrika, 39(1/2):122129, 1952.
D. Holmes. Autorship Attribution. Computers and the Humanities, 28(2):87106, 1994.
C.D. Manning and H. Sch√ºtze. Foundations of Statistical Natural Language Processing, chapter 5. The MIT
Press, 2003.
J. Olsson. Forensic Linguistics. Continuum, 2008.
HS Sichel. Word frequency distributions and type-token characteristics. Mathematical Scientist, 11(1):4572,
1986.
E. Stamatatos, N. Fakotakis, and G. Kokkinakis. Computer-Based Authorship Attribution Without Lexical
Measures. Computers and the Humanities, 35(2):193214, 2001.
14
