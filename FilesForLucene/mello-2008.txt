L. Rueda, D. Mery, and J. Kittler (Eds.): CIARP 2007, LNCS 4756, pp. 911–920, 2007. 
© Springer-Verlag Berlin Heidelberg 2007 
An Algorithm for Foreground-Background Separation 
 in Low Quality Patrimonial Document Images 
Carlos A.B. Mello 
Department of Computing Systems, University of Pernambuco 
Recife, Brazil, 50720-001 
carlos@dsc.upe.br 
Abstract. In this article, we present a new algorithm to deal with foreground-
background separation in very degraded documents. In particular, our work is 
applied to patrimonial document images which suffer from several types of deg-
radation as aging effects, noise, back-to-front ink interference, etc. Our main 
objective is to correctly classify ink and paper to allow an efficient segmenta-
tion of the image creating high quality monochromatic images. This makes eas-
ier the broadcast of these images through the Internet. The new algorithm is 
based on the classical Shannon definition of entropy and a generalization  
defined as Tsallis Entropy and it is compared to 19 well-known classical algo-
rithms, including DjVu algorithm. It achieved the best results by analyzing pre-
cision, recall, accuracy, specificity, PSNR and MSE. 
Keywords: Document processing, Image thresholding, Entropy. 
1   Introduction 
This research is part of the DocHist Project [8][9][10] for image processing of histori-
cal documents which aims the preservation and broadcast of a file of thousands of 
patrimonial documents. Even more, it is important to improve the readability of the 
digital documents. The archive used in this paper is composed of more than 6,500 
letters and documents which amounts more than 30,000 pages from the end of the 
nineteenth century onwards. 
For preservation purposes, the documents are digitized in 200 dpi resolution in true 
color and stored in JPEG file format with 1% loss for better quality/space storage rate. 
Even in this format each image of a document reaches, in average, 400 KB. In spite of 
the common use of broadband Internet access nowadays, the visualization of a be-
quest of thousand of files is not easy. Even in JPEG all the archive consumes Giga 
Bytes of space. The conversion of the digital images to bi-level comes as a possible 
solution to this problem. 
There are several research efforts in the development of image thresholding or bi-
narization techniques [11]. This is the first step in some image processing applications 
as optical character recognition (OCR). Threshold algorithms search for a cut-off 
value that separates object and background in an image. This value defines which 
colors belong to one or another class. In the case of images of documents these two 
912 C.A.B. Mello 
classes are the paper (the background) and the ink (the foreground). A good threshold 
value for this application is one that preserves in the final bi-level image all the in-
formation content of the document. This is quite a simple task when one deals with 
recent documents where, in general, the paper is almost completely clear which is not 
the case of ancient documents. This type of documents is degraded by the presence of 
background artifacts. For these cases, image enhancement techniques could be used 
first to improve the visual appearance of the image for further thresholding. 
Images of historical documents present some unique features that make a binariza-
tion process very difficult 1) some documents are written on both sides of the paper 
and the ink from one side passes to the other side, creating a back-to-front interfer-
ence (also known as bleed-through effect); 2) some paper sheets are very consumed 
and the paper has darkened over the time (the show-through effect); 3) the last case 
presents the documents where the ink has faded so much that it has almost the same 
color as the paper. Examples of these classes of documents can be seen in Fig. 1. 
 
Fig. 1. Sample documents: (left) a very faded document; (center) a document with darkened 
paper and (right) a document with back-to-front interference 
Next Section discusses some of the researches being developed for processing im-
ages of historical documents. The new proposed method is fully described in Section 3 
and its results are disposed in Section 5, followed by the Conclusions of the paper. 
2   Image Thresholding of Patrimonial Documents 
Thresholding [11] is a classical problem for image processing. There is a great variety 
of algorithms defined for this purpose. Most of them are for general use, but there are 
specific algorithms for historical documents.  
Previous works related to image processing of patrimonial documents can be found 
in literature. The problem of bleed-through interference is dealt in [16] where a canny 
edge detector is used to detect and to suppress undesired background patterns consid-
ering that the writing angle in the foreground opposes the writing angle in the back-
ground. This approach, however, does not deal with horizontal and vertical lines as 
can be found in a handwritten letter "T" for example. The same authors also propose a 
new method to deal with ink bleeding through the matching of the images from both 
sides of the paper which is a very difficult task [17]. 
The authors in [7] propose the use of multi-stage thresholding, i.e., different algo-
rithms are used in different stages of the complete process in order to create the best 
image possible. The authors propose this and they also claim that global thresholding 
algorithms must not be used in this kind of images which is not validated in our work.  
 An Algorithm for Foreground-Background Separation 913 
It is proposed in [3] the use of quadtree decomposition to break down the image 
into sub-regions and to apply different thresholding algorithms in each of these re-
gions. Background removal is also treated in [6] and [1]. 
An algorithm for background normalization is proposed in [15] to decrease the 
background influence and for further binarization. Unfortunately, the method is ad-
justed only for documents written on just one side of the paper. 
A combination of global and local thresholding algorithms is presented in [5] using 
Iterative Global Thresholding (IGT). Sub-areas n by n of the image are analyzed to 
verify if they have more black pixels than they should have. The authors, however, do 
not explain how the size of the sub-areas must be defined. 
Several well-known thresholding algorithms were tested in the images of our ar-
chive. None of them achieved satisfactory results. The tested algorithms are: Brink, 
C-Means, Fisher, Huang, Iterative Selection, Kapur, Kittler, Li-Lee, Mean Grey 
Level, Otsu, Percentage of Black, Pun, Renyi, Two Peaks, Wu-Lu, Yager and Ye-
Danielsson.. A review of these methods can be found in [13] and the results of the 
application of some of them are presented in Fig. 2. 
     
             
Fig. 2. Application of Brink, Huang, Pun, Percentage of Black and Otsu thresholding algo-
rithms in (top) document of Figure 1-right and (bottom) document of Figure 1-center 
3   A New Tsallis-Entropy Based Thresholding Algorithm 
Tsallis entropy [18] has been considered a new information measure. It has been used 
in several image processing applications as Content Based Image Retrieval (CBIR) 
[12] and even thresholding [19][20] (however both papers present just the possibilities 
of the use of Tsallis entropy; certain parameters are not clearly specified in them). 
According to Tsallis, an universal definition of entropy is given by: 
1
)(1
)(
−
−
= ∑
α
α
α
i
ip
SH
 
(1) 
914 C.A.B. Mello 
where p(i) is a probability as in the classical definition of entropy and α is a real pa-
rameter. When α tends to 1, Tsallis entropy reduces to Boltzmann-Gibbs entropy: 
∑−= i ipipSH ))(ln()()(  
 
Shannon’s definition of entropy (H) [14] defines that:  
∑
=
−=
n
i
ii spspSH
0
])[log(][)(
 
(2) 
Eq. 2 settles that if a system can be decomposed into two statistical independent sub-
systems, say A and B, then H has the extensive or additivity property. This means that 
H(A+B) = H(A) + H(B). This fact is used in Pun’s thresholding algorithm, for exam-
ple. Tsallis entropy has a nonextensive property for statistical independent subsys-
tems, defined by the following pseudo addivity entropic rule: 
Hα(A + B) = Hα(A) + Hα(B) +(1 - α) Hα(A) Hα(B) 
However, mathematically, Tsallis entropy (Eq. 1) can be broken into two parts: 
∑∑ −−−=−
−
=
i
i ip
ip
SH α
α
α ααα
)(
1
1
1
1
1
)(1
)(  
∑∑
+== −
−
−
−
−
+
−
=
255
10
)(
1
1
)(
1
1
11
)(
ti
t
i
wb ipip
XX
SH ααα αααα  
⎟
⎠
⎞
⎜
⎝
⎛
−
−
−
+⎟
⎠
⎞
⎜
⎝
⎛
−
−
−
= ∑∑
+==
255
10
)(
1
1
1
)(
1
1
1
)(
ti
w
t
i
b ip
X
ip
X
SH ααα αααα  
where Xb + Xw = 1. It can be defined then that: 
)()()( BHAHSH wb ααα +=  
(3) 
with 
∑
=−
−
−
=
t
i
b
b ip
X
AH
0
)(
1
1
1
)( αα αα  
(4) 
and 
∑
+=−
−
−
=
255
1
)(
1
1
1
)(
ti
w
w ip
X
BH αα αα  
(5) 
In the equations above, t is the threshold value. In our case, t is the most frequent 
color in the image. It is reasonable to consider that this most frequent color is part of 
the background. Hbα is the entropy of the pixels below the color t and Hwα is the en-
tropy of the colors above the threshold t. The variable t is also used to define the val-
ues of Xb and Xw, as Xb is the percentage of colors below t and Xw is the percentage of 
colors above t.  
The α parameter is a real number and it characterizes the degree of nonextensivity. 
Its value is not fixed in Tsallis definition. For thresholding purposes, variations in this 
value can modify the cut-off value. For our project, α is equal to 0.3 for the most part 
of the images, changing in just one case as further explained. 
 An Algorithm for Foreground-Background Separation 915 
At first, the document images are separated into classes. There are three main 
classes of documents:  
• Class 1: documents with few parts of text or documents where the ink has faded; 
• Class 2: common documents with around 10% of text elements; 
• Class 3: documents with more black elements than it should have; this includes 
documents with a black border or documents with bleed-through effect. 
In order to classify an image as one of these classes, we evaluate Shannon entropy 
(H) using Equation 2 but with the logarithmic basis taken as the product of the dimen-
sions of the image. As defined in [4], changes in the logarithmic basis do not alter the 
definition of the entropy. The previous three classes of documents are defined by: 
• H ≤ 0.26: Class 1 documents; 
• 0.26 < H < 0.30: Class 2 documents; 
• H ≥ 0.30: Class 3 documents. 
These boundaries were defined in previous works [9][10] and they were adjusted in 
our new proposal. For example, the sample documents of Fig. 1 belong, from left to 
right, to classes 1 (H = 0.23), 2 (H = 0.29) and 3 (H = 0.32). 
The entropy value can be broken into the entropy of black pixels, Hb, and the en-
tropy of the white pixels, Hw, bounded by a threshold t: 
∑
=
−=
t
i
ii spspHb
0
])[log(][
 
∑
+=
−=
255
1
])[log(][
ti
ii spspHw
 
(6) 
In our case, t is the most frequent color of the image. 
For each of these classes, an analysis must be made to process the images that be-
long to them as can be seen next. The final threshold value, th, is defined by: 
αα wb HmwHmbth ** +=  
where mb and mw are multiplicative constants that are going to be defined for each 
class. Hbα and Hwα can be seen as projections of the Hα value; changes in those values 
(generated by the product by mw or mb) produces changes in Hα itself. 
 
Class 1 Documents: 
As said before, this class involves documents with few ink elements or few text parts. 
This can happens in cases where the letter has just few words or the ink has faded. In 
this class, we can also find most part of the typewritten documents as, in general, the 
typewriter ink is not so strong as handwritten characters making them more suscepti-
ble to degradation of their colors. 
Although the images of this class have similar features in some way, they differ in 
basic aspects as, for example, typewritten documents must occupy a complete sheet of 
paper (opposing the fact that this class groups documents with few text parts). Be-
cause of this, another aspect must be considered within this class. We must consider 
the distribution of the pixels of the original image using the values of Hw or Hb. We 
choose Hw with no loss of generality. For these kind of images, we have: 
916 C.A.B. Mello 
• If (Hw≥0.1), then mb=2.5 and mw=4.5 (typewritten documents with dark ink and 
bright paper); 
• If (0.08<Hw<0.1), then mb=mw=6 and α=0.35 (documents with the ink faded); 
• If (Hw≤ 0.8), then mb=mw=4 (documents with dark ink and paper). 
Class 2 Documents: 
The most common documents just need a boost in Hbα and Hwα  to achieve the best 
threshold value. So, in general, the algorithm defines mb = 2.2 and mw = 3. Some 
darkened documents need another treatment. If a document belongs to class 2 and Hw 
> 0.1, then the value of mw decreases by half (i.e., mw = 1.5), unless the most fre-
quent color is greater than 200 (brighten documents) for which mw = 9. Fig. 6 shows 
sample documents from class 2 darkened or not and their bi-level images. 
 
Class 3 Documents: 
These are the documents with more black pixels than expected in a normal document. 
In this class, we have documents with a black border or documents with back-to-front 
interference. As the ink from one side transposes to the other side, it creates an inter-
mediary element in the image: there is no more just paper or background; the trans-
posed ink is an element between them. In these cases, there is no need to increase the 
dark measures. The system must deal just with the paper and the transposed ink turn-
ing them to white. Because of this, the mb parameter is fixed as 1. In most documents, 
we have mw = 2. Some cases, however, must be considered when the documents have 
brightened paper again. In this class, brighten paper documents are the ones with most 
frequent color (t) greater than 185: 
• If (t >= 185) then 
o If (0.071 < hw < 0.096) then mw = 9; 
o If (0.096 <= hw < 0.2) then mw = 6; 
4   Results 
The proposed algorithm was tested in a set of 200 images that are considered repre-
sentative of the complete file. The results were considered very satisfactory by visual 
inspection. However a most objective measure is also necessary. In this set, 18% of 
the documents belong to the class 1, 40% are from class 2 and 42% from class 3. 
To make a quantitative evaluation of the performance of the new algorithm, its re-
sults are compared against the ground truth knowledge (an ideal image with the back-
ground removed manually). This comparison is made using the concepts of: precision, 
recall, accuracy and specificity. In order to use a more automatic process, our analysis 
is based on the number of pixels correctly classified as paper or ink. For this purpose 
the ideal image is considered as what should be the final target of the algorithm. With 
this in mind, we can have the number of ink pixels correctly classified as ink (TP - 
True Positives), the number of pixels correctly classified as paper (TN - True Nega-
tives), the number of pixels misclassified as ink (FP - False Positive) and number of 
ink elements misclassified as paper (FN - False Negative), defining:  
Precision = TP/(TP + FP)    Recall = TP/(TP + FN) 
Accuracy = (TP + TN)/(TP + TN + FP + FN) Specificity = TN/(FP + TN) 
 An Algorithm for Foreground-Background Separation 917 
Based on these measures, a good algorithm must have: 
• Precision=1: there were no misclassification of the paper elements (FP = 0); 
• Recall=1: there were few mistakes in the classification of the ink elements 
(FN=0); 
 
• Accuracy=1: there was no misclassification at all (FP + FN=0); 
• Specificity=1: every pixel that belongs to the paper were classified as that 
(FP=0). 
Table 1 presents the average result for these four measures applied to a set of 200 
documents binarized by the new proposed algorithm and classical algorithms in com-
parison with their ideal versions. Our algorithm achieved very good values for the 
four measures. We also analyzed the values of PSNR (Peak Signal-to-Noise Ratio) 
and MSE (Mean Square Error). Their average values are also presented in Table 1. 
Table 1. Average values of precision, recall, accuracy, specificity, PSNR and MSE in a set of 
200 bi-level documents generated by the new proposal and classical methods compared with 
their ideal version generated manually 
Algorithm Precision Recall Accuracy Specificity PSNR MSE 
New Algorithm 0.82 0.88 0.97 0.98 21.65 0.03 
Brink 0.91 0.69 0.95 0.98 20,91 0.06 
C-Means 0.88 0.79 0.93 0.99 15.61 0.27 
Fisher 0.95 0.51 0.73 0.99 20.97 0.06 
Huang 0.88 0.80 0.94 0.99 20.63 0.07 
Iterative Selection 0.38 0.48 0.94 0.94 20.27 0.06 
Kapur 0.88 0.79 0.93 0.98 20.30 0.05 
Kittler 0.94 0.73 0.96 0.99 16.33 0.11 
Li-Lee 0.00 0.57 0.89 0.89 20.13 0.04 
Mean Grey Level 0.95 0.71 0.96 0.99 20.21 0.07 
Otsu 0.81 0.81 0.97 0.98 21.18 0.03 
Percentage of  Black 0.99 0.23 0.63 0.99 19.20 0.05 
Pun 0.94 0.69 0.93 0.99 10.41 0.37 
Renyi 0.88 0.77 0.93 0.99 19.54 0.07 
Two Peaks 0.87 0.82 0.95 0.98 8.49 0.62 
Wu-Lu 0.94 0.71 0.95 0.99 18.89 0.06 
Yager 0.99 0.17 0.39 0.91 21.37 0.05 
Ye-Denielsson 0.87 0.77 0.93 0.99 19.86 0.05 
We should expect that the perfect algorithm must have the four measures next to 1, 
high PSNR value and low MSE value. So a good algorithm must have all these fea-
tures at the same time. Our new proposal has the higher PSNR and lower MSE. For 
precision, recall, accuracy and specificity, other algorithms achieved satisfactory 
results (as Otsu, Brink, Mean Grey Level, Huang) but our algorithm has a better per-
formance in average. 
Table 2 presents a second test as our algorithm is compared to images generated by 
DjVu technology [2] which is defined specifically for document image thresholding 
and compression. Table 2 shows the average and standard deviation values of the 
918 C.A.B. Mello 
same measures as before, comparing the images generated by our algorithm and the 
ones created by DjVu and the ideal images. 
Fig. 3 presents some very difficult images and the results of the application of the 
algorithm. In particular, Fig. 3–left presents the same document of Fig. 1-left. This is 
the best response ever achieved by an automatic algorithm for this image without any 
pre-processing technique for contrast enhancement. 
Table 2. Average and standard deviation values of precision, recall, accuracy, specificity, 
PSNR and MSE in a set of 200 bi-level documents generated by the new proposal and DjVu 
technique in comparison with their ideal versions 
Measure DjVu New Algorithm 
Average 0.90 0.82 Precision 
Standard Deviation 0.12 0.12 
Average 0.72 0.88 
Recall 
Standard Deviation 0.24 0.09 
Average 0.90 0.97 
Accuracy 
Standard Deviation 0.20 0.01 
Average 0.99 0.98 
Specificity 
Standard Deviation 0.01 0.02 
Average 19.60 21.65 
PSNR 
Standard Deviation 0.10 2.03 
Average 0.90 0.03 
MSE 
Standard Deviation 0.12 0.01 
Fig. 4 presents a document with differences of illumination along it. Even with this 
problem, our algorithm reached the best global threshold value possible as it can be 
seen in the comparison with classical well-known algorithms as Otsu and DjVu, 
which results are presented in the center part of this figure. 
 
Fig. 3. (top) Sample documents and (bottom) their bi-level images produced by the new  
algorithm 
 An Algorithm for Foreground-Background Separation 919 
Other sample document from another database is shown in Fig. 5. This document 
is available at http://www.site.uottawa.ca/~edubois/documents. Fig. 5 presents a 
zooming into one of these documents and the binary versions generated by Otsu and 
our new algorithm. Again, our method achieved higher values of precision, recall, 
accuracy, specificity, PSNR and lower value of MSE. This shows that our method can 
be applied to other databases of similar features. 
 
Fig. 4. (left) A document with different illumination along it, (right) the binarization produced 
by our new algorithm and at the center the results of the application of Otsu and DjVu 
algorithms 
   
Fig. 5. (left) Zooming into another sample document from a different database; (center) bi-level 
image generated by Otsu algorithm and (right) the one produced by our new algorithm 
5   Conclusions 
This paper presents a new entropy-based thresholding algorithm for images of histori-
cal documents. The algorithm uses both Shannon and Tsallis definition of entropy to 
find the best cut-off value. The algorithm was applied in a set of 200 representative 
images of a file from the 19th century and beginning of the 20th century. The use of the 
algorithm was analyzed by visual inspection and by comparison with perfect bi-level 
images. The values of precision, recall, accuracy and specificity were evaluated for 
the complete set and the algorithm achieved satisfactory results. 
Three classes of documents are identified using the classical Shannon entropy defi-
nition. After this, a set of rules is used to define the best threshold value. For this, 
Tsallis entropy is separated into two components which are boosted in order to define 
the cut-off value. The method proved to be very effective as could be analyzed using 
precision, recall, accuracy, specificity, PSNR and MSE metrics in comparison with 
several well-known thresholding algorithms, including the DjVu technique. 
920 C.A.B. Mello 
References 
1. Antonacopoulos, A., Castilla, C.C.: Flexible Text Recovery from Degraded Typewritten 
Historical Documents. In: Int. Conf. on Pattern Recognition, pp. 1062–1065, Japan (2006) 
2. Bottou, L., Haffner, P., Howard, P.G.: High Quality Document Image Compression with 
DjVu. Journal of Electronic Imaging, 410–425 (1998), http://www.djvu.org 
3. Chen, Y., Leedham, G.: Decompose algorithm for thresholding degraded. Historical 
document images, Vision, Image and Signal Processing 152(6), 702–714 (2005) 
4. Kapur, J.N.: Measures of Information and their Applications. J.Wiley & Sons, Chichester 
(1994) 
5. Kavallieratou, E., Stamatatos, E.: Improving the Quality of Degraded Document Images, 
Int. Conf. on Document Image Analysis for Libraries, pp. 340–349, France (2006) 
6. Kennard, D.J., Barrett, W.A.: Separating Lines of Text in Free-Form Handwritten Histori-
cal Documents. In: Int. Conf. on Document Image Analysis for Libraries, pp. 12–23, 
France (2006) 
7. Leedham, G., et al.: Separating Text and Background in Degraded Document Images - A 
Comparison of Global Thresholding Techniques for Multi-Stage Thresholding. In: Interna-
tional Workshop on Frontiers in Handwriting Recognition, pp. 244–249, Canada (2002) 
8. Mello, C.A.B., et al.: Image Thresholding of Historical Documents: Application to the 
Joaquim Nabuco’s File. In: Digital Cultural Heritage Conference - Eva Vienna, pp. 115–
122, Vienna, Austria (2006) 
9. Mello, C.A.B.: Image Segmentation of Historical Documents: Using a Quality Index. In: 
International Conference on Image Analysis and Recognition, pp. 209–216, Portugal 
(2004) 
10. Mello, C.A.B., et al.: Image Segmentation of Historical Documents. Visual (2000), Mex-
ico (2000) 
11. Parker, J.R.: Algorithms for Image Processing and Computer Vision. John Wiley & Sons, 
Chichester (1997) 
12. Rodrigues, P.S., et al.: Using Tsallis Entropy into a Bayesian Network for CBIR. In: Int. 
Conf. on Image Processing, pp. 1028–1031, Genova (2005) 
13. Sezgin, M., et al.: Survey over image thresholding techniques and quantitative perform-
ance evaluation. Journal of Electronic Imaging, vol. 13(1) (2004) 
14. Shannon, C.: A Mathematical Theory of Communication. Bell System Technology Jour-
nal 27, 370–423 (1948) 
15. Shi, Z., Govindaraju, V.: Historical Document Image Enhancement Using Background 
Light Intensity Normalization. In: International Conference on Pattern Recognition, pp. 
473–476, UK (2004) 
16. Tan, C.L., et al.: Removal of Interfering Strokes in Double-Sided Document Images. In: 
Workshop on Applications of Computer Vision, pp. 16–21, USA (2000) 
17. Tan, C.L., et al.: Restoration of Archival Documents Using a Wavelet Technique. IEEE 
Trans.on Pattern Analysis and Machine Intelligence 24(10), 1399–1404 (2002) 
18. Tsallis, C.: Possible Generalization of Boltzmann-Gibbs statistics. Journal of Statistical 
Physics 52(1-2), 479–487 (1988) 
19. Yan, L., et al.: An Application of Tsallis Entropy Minimum Difference on Image Segmen-
tation, World Congress on Intelligent Control and Automation, pp. 9557–9561, China 
(2006) 
20. Yan, L., et al.: Image Segmentation based on Tsallis-entropy and Renyi entropy and Their 
Comparison. In: Int. Conf. on Industrial Informatics, pp. 943–948, Singapore (2006) 
