 
 
 
 
 
 
 
 
© 2005 
 
Ying Sun 
 
ALL RIGHTS RESERVED 
 
AUTOMATIC ASSESSMENT OF NON-TOPICAL PROPERTIES OF TEXT  
BY MACHINE LEARNING METHODS 
by 
YING SUN 
A Dissertation submitted to the 
Graduate School – New Brunswick 
Rutgers, The State University of New Jersey 
in partial fulfillment of the requirements 
for the degree of 
Doctor of Philosophy 
Graduate Program in Communication, Information and Library Studies 
written under the direction of 
Dr. Paul B. Kantor 
and approved by 
 
____________________________________ 
 
____________________________________ 
 
____________________________________ 
 
____________________________________ 
New Brunswick, New Jersey 
October 2005 
 
ii 
ABSTRACT OF THE DISSERTATION 
AUTOMATIC ASSESSMENT OF NON-TOPICAL PROPERTIES OF TEXT  
BY MACHINE LEARNING METHODS 
by YING SUN 
Dissertation Director: 
Dr. Paul B. Kantor 
 
 
This study takes some first step towards automatic classification of texts with 
regard to non-topical properties, using machine learning techniques together with simple 
linguistic features.  The six properties investigated are: “Accuracy”, “Reliability”, 
“Objectivity”, “Depth”, “Conciseness”, and “Multiple points of views”.  The importance 
of these properties, in satisfying users’ information needs and/or in control of information 
quality, has been widely accepted, and they are amenable to the automatic analysis.  We 
test the learnability of human judgments on these properties at two levels: the general 
level with the goal of constructing global rules from a group of persons’ judgments; and 
the individual level.   
The “combined” corpus, for the purpose of general level learning, is a 
combination of work by multiple judges on 3,200 texts.  There are also five individual 
judgment corpora, with about 500 documents in each.  The judgments are on 10-point 
Likert scales.  Four statistical and machine learning techniques: linear regression, logistic 
regression, decision tree C4.5 and Support Vector Machines (SVM), are applied to the 
automatic assessment task. 
 
iii 
The linear regression experiments show that we cannot adequately differentiate 
texts at 10 distinct levels on each non-topical dimension.  However, the experiments 
demonstrate that binary classification techniques (Logistic Regression and Linear SVM), 
together with the simple language and textual features, can automatically assess the six 
non-topical properties of documents at levels better than chance.  The prediction 
performance gets better when middle range documents are removed from the dataset.  
The classification tasks for “Depth” and “Multi-views” are relatively easier than the other 
four tasks.  With the current set of predictive features, we cannot yet identify the middle 
range documents.   
Automatic assessment of one individual judge’s assignment is slightly, but not 
significantly, better than learning for all judges at once.  For some judges and some 
properties, our method achieved very good classification results.  Linear based learning 
methods, logistic regression and linear SVM, are better tools than Boolean-based 
decision tree (C4.5) method.  The advanced SVM method does not show significant 
superiority over the logistic regression method. The models with good performance tend 
to contain intuitively reasonable features.   
  
 
iv 
Acknowledgements 
 
 
I would like to thank three important groups of people: my committee, my 
wonderful colleagues in ApLab, and my family. Without them, this dissertation would 
not have been possible.   
First, I would like to express my gratitude to my advisor, Dr. Paul B. Kantor, a 
talented and passionate scientist, for his support, guidance, and encouragement 
throughout my doctorate study.  His advice was essential to the completion of this 
dissertation.  More importantly, he has taught me innumerable lessons and insights on the 
workings of academic research in general.  
My thanks also go to the members of my committee, Nina Wacholder, Micheal 
Lesk and David Madigan.  Their hard work and valuable suggestions have helped shape 
this dissertation in important ways.   
I would like to thank my colleges in the HITIQA project: Kwong Bor Ng, Bing 
Bai, Robert Rittman, and Peng Song, for not only helping considerably with data 
collection and processing, but all those good-spirited discussions relating to this research.  
To my lab-mates, thanks for the fun and support. 
Last, I would like to thank my husband Chao for his understanding and love.  My 
parents, Hui-fang and Lian-yu, and my sister Wei receive my deepest gratitude and love 
for their dedication and support from thousand miles away.  
This work is supported in part by the Advanced Research Development Activity 
(ARDA) of the Intelligence Community, under Contract # 2002-H790400-000 to SUNY 
Albany, with Rutgers as a subcontractor. 
 
v 
Table of Content 
 
Abstract  …………………………………………………………………………………..ii 
Acknowledgements ………………………………………………………………………iv 
List of Tables  …………………………………………………………………………..viii 
List of Figures  …………………………………………………………………………....x 
 
Chapter 1: Introduction...................................................................................................1 
Chapter 2: Non-Topical Qualitative Properties of Documents....................................7 
2.1 User – Centered Studies....................................................................................... 8 
2.2 Information – Centered Studies ......................................................................... 10 
2.3 Summary of Properties ...................................................................................... 11 
2.4 Assessment of Qualitative Properties ................................................................ 14 
Chapter 3: Linguistic Features......................................................................................17 
3.1 Stylistic Studies.................................................................................................. 17 
3.1.1 Authorship Attribution Research ................................................................. 18 
3.1.2 Genre Classification..................................................................................... 19 
3.1.3 “Style” and Non-topical Qualitative Properties........................................... 20 
3.2 Linguistic Features as Indicators ....................................................................... 21 
Chapter 4: Automatic Classification Techniques........................................................24 
4.1 Classification through Learning......................................................................... 25 
4.2 Linear Regression .............................................................................................. 27 
4.3 Logistic Regression............................................................................................ 28 
4.4 Decision Tree Learning...................................................................................... 30 
4.5 Support Vector Machines (SVMs)..................................................................... 34 
4.6 Applications ....................................................................................................... 35 
Chapter 5: Research Problems......................................................................................38 
Chapter 6: Methodology, Experimental Design and Evaluation Measures..............41 
6.1 Document Corpora............................................................................................. 41 
6.2 Non-Topical Qualitative Property Judgments ................................................... 43 
 
vi 
6.3 Language Features ............................................................................................. 45 
6.3.1 Character – Level Features .......................................................................... 45 
6.3.2 Lexical – Level Features.............................................................................. 45 
6.3.3 Structural - Level Features........................................................................... 45 
6.3.4 Derivative Features ...................................................................................... 46 
6.4 Experimental Setup............................................................................................ 48 
6.4.1 Training and Testing Subsets....................................................................... 48 
6.4.2 Recoding Schemes for Binary Prediction .................................................... 48 
6.4.3 Experiments Summary................................................................................. 50 
6.5 Learning Tools ................................................................................................... 51 
6.6 Evaluation Measures.......................................................................................... 51 
6.6.1 R-squared ..................................................................................................... 51 
6.6.2 Classification Accuracy ............................................................................... 52 
6.6.3 ROC Analysis .............................................................................................. 53 
Chapter 7: Numerical Prediction over the Combined Corpus ..................................56 
7.1 Results of Linear Regression With All Language Features............................... 57 
7.2 Results With Statistical Best Features ............................................................... 59 
7.3 Binary Prediction ............................................................................................... 60 
Chapter 8: Binary Classification over the Combined Corpus ...................................62 
8.1 Binary Datasets .................................................................................................. 62 
8.2 Logistic Regression............................................................................................ 65 
8.2.1 On Median-based Binary Datasets............................................................... 65 
8.2.2 On Scale-Based Binary Datasets ................................................................. 70 
8.2.3 Comparison of Classification Results on Four Sets of Binary Datasets...... 71 
8.2.4 Learning Models with Powerful and Stable Features .................................. 72 
8.3 Decision Tree Learning (C4.5) .......................................................................... 75 
8.3.1 Decision Tree with All Features .................................................................. 75 
8.3.2 Learning with Stable Feature Set................................................................. 76 
8.3.3 Pruning the Trees ......................................................................................... 78 
8.4 SVM Learning ................................................................................................... 80 
8.5 Models Generated .............................................................................................. 82 
 
vii 
8.6 Summary ............................................................................................................ 86 
Chapter 9: Classification of Document Exemplars .....................................................89 
9.1 Exemplar Corpora.............................................................................................. 90 
9.2 Logistic Regression............................................................................................ 91 
9.2.1 Over Extreme Corpus I ................................................................................ 91 
9.2.2 Over Extreme Corpus II............................................................................... 94 
9.3 Decision Tree Method (C4.5) ............................................................................ 95 
9.4 SVM................................................................................................................... 97 
9.5 Models Generated .............................................................................................. 98 
9.6 Summary of Results over the Combined Corpora ........................................... 100 
Chapter 10: Identifying the Middle Range Documents ..............................................110 
10.1 Test the Extreme Logistic Models over the Combined Corpus....................... 110 
10.2 Differentiation between CLEAR and UNKNOWN ........................................ 113 
10.3 Another Thought.............................................................................................. 115 
10.4 The Middle Range Documents and Inter-Judge Agreement............................ 116 
Chapter 11: Modeling Individual Judgments..............................................................120 
11.1 Numerical Prediction ....................................................................................... 123 
11.2 Binary Classification........................................................................................ 124 
11.2.1 Individual Binary Datasets......................................................................... 124 
11.2.2 Classification Results................................................................................. 126 
11.3 Summary .......................................................................................................... 133 
Chapter 12: Conclusions and Beyond...........................................................................135 
12.1 Summary of Results......................................................................................... 135 
12.2 Conclusions...................................................................................................... 139 
12.3 Limitations of the Study................................................................................... 140 
12.4 Future Work ..................................................................................................... 141 
References .. ....................................................................................................................144 
Appendix 1: Definitions of Six Non-Topical Properties .............................................150 
Appendix 2: Language Features...................................................................................151 
Appendix 3: Experimental Scripts ...............................................................................155 
 
viii 
List of Tables 
 
Table 2.1: Summary of non-topical qualitative properties. .............................................. 12 
Table 3.1: Types of text and linguistic features used as “style markers” ......................... 23 
Table 4.1: The weather sample data (Decision Tree Method).......................................... 31 
Table 6.1: Experimental corpora summary....................................................................... 43 
Table 6.2: Individual datasets. .......................................................................................... 44 
Table 6.3: Linguistic feature sets. ..................................................................................... 47 
Table 6.4: Recoding schemes based on 1-10 scale. .......................................................... 49 
Table 6.5: Summary of learning experiments................................................................... 50 
Table 6.6: Classification table........................................................................................... 52 
Table 7.1: One pair of half-half training/testing set.......................................................... 57 
Table 7.2: Linear regression results over the full corpus.................................................. 58 
Table 7.3: Stepwise linear regression results over the full corpus.................................... 59 
Table 8.1: Binary meanings of qualitative properties....................................................... 63 
Table 8.2: Class distributions in median-based binary datasets ....................................... 64 
Table 8.3: Class distributions in the scale-based binary datasets ..................................... 64 
Table 8.4.  Performance of two logistic regression methods on the median-based binary 
datasets...................................................................................................................... 66 
Table 8.5:  Performance of the SimpleLogistic method on the testing data of three scale-
based binary datasets:  normal binary datasets; extreme high binary datasets; and 
extreme low binary datasets...................................................................................... 70 
Table 8.6:  Number of stable features and the area under curve for 10-fold cross 
validation using SimpleLogistic method over the whole corpus. ............................. 74 
Table 8.7: Decision tree C4.5 classification performance on testing date set, including all 
features...................................................................................................................... 75 
Table 8.8: Classification performance of decision tree learning, using the stable feature 
sets............................................................................................................................. 77 
Table 8.9: Classification performance after tree pruning ................................................. 79 
Table 8.10: Classification performance of SVM on median based binary corpora.......... 81 
Table 8.11: Root features of the decision tree C4.5 models. (Learning method: J48; 
Feature sets: the tree voted stable feature sets; training corpus: the full corpus) ..... 83 
Table 8.12:  Features of the logistic regression and SVM models (Learning method: 
SimpleLogistic and SMO; Feature sets: the voted stable feature sets; training corpus: 
the full corpus) .......................................................................................................... 86 
Table 8.13:  Comparison of classification accuracy across three learning methods on 
median-based binary datasets ................................................................................... 88 
Table 9.1:  Distributions of extreme high/low cases......................................................... 91 
Table 9.2:  Number of stable features and the area under curve (AUC) for 10-fold cross 
validation using SimpleLogistic method over the extreme corpora I. ...................... 92 
Table 9.3:  Number of stable features and the area under curve (AUC) for 10-fold cross 
validation using SimpleLogistic method over the extreme corpora II...................... 94 
Table 9.4: Accuracy and the area under curve (AUC) of decision tree C4.5 method over 
the extreme corpus (10-fold cross validation). ......................................................... 95 
 
ix 
Table 9.5: Accuracy and the area under curve (AUC) of SVM method over the extreme 
corpus (10-fold cross validation). ............................................................................. 98 
Table 9.6: Models generated over the extreme corpora.................................................... 99 
Table 9.7:  Best accuracy scores calculated through ROC analysis ............................... 105 
Table 10.1: Class assignment to create corpus for classification between UNKNOWN 
and CLEAR documents. ......................................................................................... 113 
Table 10.2: Accuracy for classification between CLEAR and UNKNOWN documents.
................................................................................................................................. 114 
Table 11.1: Individual corpora distribution of the six qualitative properties. ................ 121 
Table 11.2: Individual classification tasks...................................................................... 123 
Table 11.3: R2 of stepwise linear regression on the individual datasets. ........................ 124 
Table 11.4: Individual binary datasets ............................................................................ 125 
Table 11.5:  AUC of logistic regression, SVM and C4.5 on the individual datasets. .... 127 
 
 
x 
List of Figures 
 
Figure 1.1: Standard information retrieval model............................................................... 1 
Figure 2.1: Document properties ........................................................................................ 7 
Figure 4.1: Process of machine learning........................................................................... 26 
Figure 4.2: Decision tree for the weather sample data...................................................... 31 
Figure 4.3: Linear support vector machines ..................................................................... 34 
Figure 5.1:  Model of combined IR. ................................................................................. 39 
Figure 6.1: Learning Processes ......................................................................................... 42 
Figure 6.2: ROC space...................................................................................................... 55 
Figure 7.1:  Fitting plots of the linear regression models with all features ...................... 59 
Figure 7.2:  Dichotomization of linear prediction. ........................................................... 61 
Figure 8.1.  ROC curves of the SimpleLogistic method on the testing data sets.  Models 
are trained on the median-based binary datasets....................................................... 67 
Figure 8.2: ROC curves for the testing data set of two methods: (1) logistic regression; 
(2) SimpleLogistic with feature selection (Predicted variable: Median-based binary 
“Multi-views” score)................................................................................................. 69 
Figure 8.3: ROC curves of SimpleLogistic on the testing datasets.  Models are trained on 
the normal scale-based binary datasets. .................................................................... 71 
Figure 8.4.  ROC curves of the SimpleLogistic method with voted stable feature sets for 
10-fold cross validation in the HIGH vs. LOW classification task. ......................... 74 
Figure 8.5:  ROC curves for six qualitative properties, using C4.5 decision tree method 
and the stable sets of features.................................................................................... 77 
Figure 8.6:  ROC curves of original and pruned decision tree classifiers for “Multi-views” 
property ..................................................................................................................... 79 
Figure 8.7: ROC curves of SVM on six median-based binary datasets with selected 
features (10-fold cross validation) ............................................................................ 82 
Figure 9.1: ROC curves of SimpleLogistic on the extreme corpora I and the combined 
corpus........................................................................................................................ 93 
Figure 9.2:  ROC curves of decision tree C4.5 on Extreme I Corpora with selected 
features (10-fold cross validation). ........................................................................... 96 
Figure 9.3: ROC curves of SVM on Extreme Corpora I with selected features (10-fold 
cross validation). ....................................................................................................... 98 
Figure 9.4: Predictive accuracies of logistic regression, C4.5 and SVM on three binary 
datasets, with two feature sets.  Predicted property is “Accuracy”. ....................... 101 
Figure 9.5: Predictive accuracies of logistic regression, C4.5 and SVM on three binary 
datasets, with two feature sets. Predicted property is “Reliability”........................ 101 
Figure 9.6: Predictive accuracies of logistic regression, C4.5 and SVM on three binary 
datasets, with two feature sets. Predicted property is “Objectivity”....................... 102 
Figure 9.7: Predictive accuracies of logistic regression, C4.5 and SVM on three binary 
datasets, with two feature sets.  Predicted property is “Depth”. ............................. 102 
Figure 9.8: Predictive accuracies of logistic regression, C4.5 and SVM on three binary 
datasets, with two feature sets.  Predicted property is “Conciseness”. ................... 103 
Figure 9.9: Predictive accuracies of logistic regression, C4.5 and SVM on three binary 
datasets, with two feature sets.   Predicted property is “Multi-views”. .................. 103 
 
xi 
Figure 9.10: Accuracy ROC curves of Logistic regression, C4.5 and SVM with selected 
features.................................................................................................................... 106 
Figure 9.11: Reliability ROC curves of Logistic regression, C4.5 and SVM with selected 
features.................................................................................................................... 106 
Figure 9.12: Objectivity ROC curves of Logistic regression, C4.5 and SVM with selected 
features.................................................................................................................... 107 
Figure 9.13: Depth ROC curves of Logistic regression, C4.5 and SVM with selected 
features.................................................................................................................... 107 
Figure 9.14: Conciseness ROC curves of Logistic regression, C4.5 and SVM with 
selected features. ..................................................................................................... 108 
Figure 9.15: Multi-views ROC curves of Logistic regression, C4.5 and SVM with 
selected features. ..................................................................................................... 108 
Figure 10.1:  Scatter plots of original categories and the predicted probability on Multi-
views and accuracy properties.  The probabilities are calculated using the logistic 
regression models learned over the extreme corpora I. .......................................... 112 
Figure 10.2: Identifying UNKNOWN documents by fusion of two extreme models. ... 115 
Figure 10.3: Means and standard deviations of inter-judge difference at different levels of 
the average judgment scores. .................................................................................. 118 
Figure 11.1: S03-Conciseness ROC curves of C4.5, SVM and Logistic regression on 
exemplar corpus.  The thin dark line is the ROC for the best classification on the 
combined corpus. .................................................................................................... 129 
Figure 11.2: S04-Depth ROC curves of C4.5, SVM and Logistic regression on exemplar 
corpus.  The thin dark line is the ROC for the best classification on the combined 
corpus...................................................................................................................... 130 
Figure 11.3:  S04-Objectivity ROC curves of C4.5, SVM and Logistic regression on 
exemplar corpus.  The thin dark line is the ROC for the best classification on the 
combined corpus. .................................................................................................... 132 
 
1  
 
Chapter 1: Introduction 
Improving the ability of searching results to satisfy users’ information need has been one 
of the most important tasks for the library and information science researchers.  To 
achieve this goal, proper representations of documents and of users’ needs must be 
established first.   
 
 
 
 
 
 
 
 
Figure 1.1: Standard information retrieval model 
 
Following the standard model of information retrieval (Figure 1.1), we can think 
of an information retrieval process as a composition of three basic components: 
collections of information objects, users, and searching techniques.  Developments in 
Retrieval techniques 
Information Objects Information User 
Representation Query 
Matching 
Mechanism 
Text 
processing 
Information 
needs analysis
2  
 
information technology, especially the personal computer and the Internet, have brought 
significant changes to the user and the information collection components.   
The amount of information accessible has increased exponentially.  Meanwhile, 
because everyone can publish everything on the World Wide Web, and the information is 
not edited by experts, the quality of the information is varied.   
The user group is also exploding and becoming more and more varied.  It is even 
more easy for any person to access a huge amount of information from various sources, 
something that was impossible without computers and Internet.   
However, the new techniques do not much change the fundamental mechanism 
that represents and matches user needs and information objects.  Topical relevance is still 
the most popular and widely accepted property that is used to retrieve and rank 
documents.  Even though researchers (Schamber, 1991; Bruce, 1994; Barry, 1994; Tang, 
et al, 2003) have pointed out dimensions other than topic in users’ information needs, 
such as Accuracy, Point of view, Timeliness etc., a general, task-independent, 
representation of document contents still is the primary document representation in major 
information retrieval systems.  This representation is based on the occurrence and/or 
frequency of words, or phrases.  Document properties other than topic are seldom 
represented, even the people talk a lot about doing so.   
However, as a matter of a principle, for a full and adequate understanding of 
user’s information need, the non-topical properties should not be ignored.  Neither should 
they be dealt with unclearly, as mysterious aspects of relevance.  For some special user 
groups, non-topical properties are of great importance in screening information. 
3  
 
The separation of non-topical properties of text from topical relevance also has 
benefit as a matter of practice.  With a huge and still increasing collection, it is 
unavoidable that the number of documents delivered by current searching mechanisms 
will increase.  A great many returned documents may be filtered out according to some 
non-topical aspects of the user’s information need.   For example, given a query about the 
North Korea nuclear weapons problem, different users will require documents that 
presume different levels of expertise, depending on the user’s background.  Most basic or 
introductory documents should not be delivered to an intelligence analyst who tracks 
information about this problem daily.  Also, depending on the tasks at hand, the analyst 
may require objective documents that represent the latest news about the issue, or 
subjective documents that contain opinions about the problem.  It may take a fair amount 
of time (or even be impossible) for the users to find what they really need manually from 
the returned list.  Automatic assessment of non-topical properties of text provides a 
possible method to help users isolate the items they really need from all returned 
documents.     
There have been some research efforts which focus on integrating the assessment 
of non-topical properties of text with existing topical relevance measure, especially in the 
text filtering and web searching area, and the results are positive (Price and Hersh, 1999; 
Zhu and Gauch, 2000).  These studies suggest that an information system will be more 
efficient if it enables the user to describe his or her requirements in terms of properties 
other than topical relevance, and then combines these criteria with topical relevance 
judgment to determine the set of results for the user.   
4  
 
In this study, we investigate six non-topical properties that are identified as being 
of importance to information users.  They are: Accuracy, Reliability, Objectivity, Depth, 
Conciseness and Multi-views.   
To achieve the goal of complementing current topic-only retrieval mechanisms by 
using the non-topical qualities of text, we have to find a proper quantitative 
representation of the six non-topical properties.  
Currently, metadata and structural features (e.g. hyperlinks) outside the text are 
the main sources of the quantitative representations of non-topical properties of text.  Yet 
there is little work that examines the language of the text itself.  In this study, we test the 
idea of using language features and their patterns as indicators of non-topical properties, 
and the features and patterns will be automatically identified by machine learning 
methods. 
We get the idea that language patterns are possibly linked with non-topical 
properties of text from computational stylistic studies.  The differentiation between 
topical and non-topical properties of text in this study is essentially consistent with the 
differentiation between the concepts of “style” and content in the context of 
computational linguistic studies, which have shown that some patterns in language are 
linked to the “style” of writing.  For example, frequencies of some function words are 
used successfully in the context of authorship attribution (Holmes, 1985). The basic 
theory of these studies is that some aspects of documents, like affect, or genre, are largely 
captured by the style of writing.  The style of writing can be roughly defined as “how the 
author chose to express her topic, from among a very large space of possible ways of 
doing so” (Argamon, et al 2003).  The models of how a specific author writes, or how a 
5  
 
specific type of documents is written, have typically been based on the usage of sets of 
content-independent linguistic feature (also called “style markers”).  Statistic and 
machine learning methods have been applied to classify texts on either author or genre 
using linguistic features as indicators. 
The research reported here rests on two observations.  First, the differences 
between texts themselves are not purely topical, and users’ information needs have 
dimensions other than topicality.  Second, human beings can detect differences other than 
content.  A person can tell the difference between objective and subjective documents, 
and can assess the level of detail of a document.  Of course, a person does not judge a 
text randomly.  Based on the assumption that the judgments are linked to some language 
patterns that he or she recognizes in the text consciously or unconsciously, our first 
hypothesis is that some of the non-topical qualitative properties of texts can be 
automatically assessed using some “shallow” linguistic features.  By “shallow”, we mean 
those features that can be easily computed without any human interaction.  The advantage 
of this approach is that there is no need for outside resources, like lexicons, ontologies or 
deep syntactic analysis.   
However, what is modeled is truly the human judgment with respect to the non-
topical properties.  It is inevitably affected by individual differences.  Our second 
hypothesis is that a model personalized to an individual’s judgment will be better than a 
global model representing all the judges. 
Ideally, we expect to develop a set of stable language models to assess the six 
non-topical properties of text, for a single user, or across a group of users, by learning 
from the judgments of those users.  The language patterns learned then can be used to 
6  
 
create user-defined quality metrics, and incorporate them with the usual topical relevance 
based metrics.   
In Chapters 2, 3, and 4, we review related work on non-topical properties, 
language features and machine learning techniques.  Six non-topical properties of text are 
identified; a list of text and language features and four learning methods are suggested.  
In Chapter 5, we discuss our research problems.  Chapter 6 describes generally our 
experimental design and evaluation measures.  In Chapters 7 to 10, we investigate the 
problems of automatic assessment of text with regard to the six non-topical properties 
across multiple judges.  In Chapter 11, we focus the problems on an individual level.  
Chapter 12 gives the conclusion of this dissertation.  
7  
 
Chapter 2: Non-Topical Qualitative Properties of 
Documents 
Some document properties are called metadata, in traditional information systems such as 
libraries.  Such document properties, for example: author, publication date, etc, generally 
can be obtained without reading the text.  Since there is little ambiguity in such properties 
and they are represented in most information systems as metadata, I call them “hard” 
metadata.   Another set of properties, which one cannot tell without “reading” the text, 
will be called qualitative properties.  The focus of the study is on non-topical qualitative 
properties.  The topic of a document is the one qualitative property that has been included 
in traditional information systems as the most important metadata element, the “subject 
heading”. 
 
  
 
 
 
Figure 2.1: Document properties 
 
Document properties
“Hard” Metadata Qualitative Properties
Authors Title Publication Date Topic Non-topic
8  
 
In this chapter, we review the existing work on the non-topical properties of texts.  
Our review focuses on two aspects of the problem: the non-topical properties that are of 
some importance to information users, and the available automatic assessment methods.  
The studies identifying non-topical properties are divided into two categories.  One is 
from the perspective of users’ information need; while the studies in the second category 
focus more on the text itself.   
2.1 User – Centered Studies 
The notion of considering multiple properties of documents for information retrieval is 
not new.  Some user modeling studies have shown that topical relevance is not the only 
document property that users care about (Barry 1994; Vickery, et al 1987).  More 
complex understanding of aspects of an information need leads to considering non-
topical characteristics of information objects, e.g. coverage, style, or point-of-view.  
Many non-topical criteria have been named in studies which intend to elicit users’ criteria 
in real-life information seeking.   Schamber (1991), after interviewing 30 users, identifies 
ten groups of criteria: accuracy, currency, specificity, geographic proximity, reliability, 
accessibility, verifiability, clarity, dynamism, and presentation qualities.  Barry (1994) 
describes experiments that showed that users consider many factors, including some 
which are non topical, i.e.., depth, clarity, accuracy, recency, etc.  Bruce (1994) identifies 
some “information attributes” (accuracy, completeness, timeliness …) that information 
users might use in their relevance judgments. Tang, et al (2003) identified nine core non-
topical properties of documents in their focus group study with journalists.  All these 
9  
 
studies show the existence of non-topical factors that affect a user’s selection of 
information.   
Even though the non-topical aspects of information needs are identified in user 
modeling studies, there has been less effort to categorize and measure the non-topical 
properties of the documents and to provide matching mechanisms in the field of library 
and information studies.   
However, there are some studies that suggest measures combining both topical 
and non-topical properties.  The most popular compound measure is utility. As early as 
the 1970s, Cooper (1973) suggested that “utility” depends on many non-topical factors, 
including accuracy, credibility, recency, and so on.  Glover et al (1998) describe an agent, 
as part of the University of Michigan Digital Library project, that uses utility functions to 
dynamically re-order web pages.  The utility measure is a combination, by the user, of 
various attributes of the documents including both topical and non-topical properties.  
The attributes available in Glover et al’s studies include: 
• A reading level algorithm optimized for less advanced documents 
• The predicted number of days old as computed by analyzing the full text and 
HTML not only considering the header 
• The number of words per page 
• A measure of the number of homepage like features present 
• A measure of features indicative of a “general” page, such as the keywords 
“links” or “resources” 
• A measure of features indicative of a “research paper” page, such as having an 
abstract or references 
• The number of unique links present on a page 
• The number of unique images present on a page 
• The number of keywords in the query matched on a page 
• The number of sections on a page 
• The depth of a page from the top of a domain in levels 
• An automatically generated summarization of the document 
10  
 
• A query dependent attribute predicting how much a particular page is “about” the 
given query. This attribute is based on word distances, from each other and the 
top of the document, as well as number of occurrences of each term 
• A Binary attribute: true if page was generated by LaTeX2HTML, false otherwise.    
For each user, according her preferences, there is a utility function that combines 
several or all of the attributes in the list. Each attribute has a weight or relative 
importance value.  The retrieved documents are ranked according to this compound 
measure.  We can see that these attributes measure text varieties at different levels.  Some 
refer to the non-topical properties of a text, like reading level.  Some are indicatives of 
text genre, such as “research paper”.  Most of these attributes are simple structural or 
textual features of the text itself.  With this help, users may specify their requirements on 
some non-topical properties by choosing from these attributes those, they believe, that are 
indicative to the properties.  Of course, for each user and each information request, a new 
utility function is required. 
Overall, in the field of library and information science, the non-topical qualitative 
properties of texts are mostly considered in the concept of understanding user’s 
information needs.  There are no systematic studies of how to measure these properties, 
and there are few efforts to implement those measurements in systems.    
2.2 Information – Centered Studies  
Studies in the fields of management information system, database research, software 
engineering and some IR studies refer to this “document properties” issue as “information 
quality” or “data quality”.  The names reflect the main goals of that group of studies – to 
11  
 
control the quality of information.  We call those studies “information-centered” because 
the main focus is on the information objects.  In contrast to user-centered studies, the 
underlying concept is a quality of “goodness”, which is different from neutral document 
characteristic.  In our theoretical framework, the documents will have many qualities, and 
different users have different preferences, among those qualities.  In other words, we do 
not use the word “quality” to mean “good”.  
We mention this area here because there are many efforts to create some 
standards that can be used to evaluate the overall “goodness” of information objects or to 
build a “high quality” information collection. We believe that these standards are a good 
start point from which to examine documents qualitative property categories.  
Some research projects assembled and categorized such properties (Wang & 
Strong 1996, Burgess, Gray, & Fidden 2002), while others compiled multiple catalogs 
(Naumann & Rolker 2000).   
2.3 Summary of Properties 
It is not surprising to find that many identical factors appear in both document 
“goodness” lists and in user-identified non-topical information needs.  A summary of 
these categories of properties is in Table 2.1.  In the table, similar properties from 
different lists are presented in one row.  The last row includes properties that are not 
common to all lists. 
 
 
Documents qualitative properties User information need aspects  
Wang & Strong 
(1996) 
Naumann & 
Rolker (2000) 
Burgess, Gray & 
Fiddian (2002) 
Schamber, (1991) Bruce (1994) Barry (1994) Tang, et al. 
(2003) 
Accuracy Accuracy Accuracy Accuracy Accuracy Objective 
accuracy  
Accuracy 
Objectivity Objectivity Objectivity    Objectivity 
Believability Believability Reliability Verifiability   Reliability 
Reputation Reputation 
Reliability  
Verifiability 
 
 
Reliability of the 
source 
 
  Author/source 
credibility 
Completeness Completeness Coverage versatility  
Completeness 
Specificity 
 
Completeness Depth/Scope Multi-views  
Depth 
Interpretability 
Ease of 
understanding 
Concise 
representation 
Interpretability 
Understandablity 
Concise 
representation 
Presentation quality 
 
 
Clarity 
Other presentation 
qualities. 
  Readability  
Grammar 
correctness 
Verbose and 
conciseness 
Timeliness  
 
Timeliness  
Latency 
Currency Currency 
 
Timeliness   
Value-added 
 
Value-added  
 
Added-value  
 
Content 
Suggestiveness 
  
Representational  
consistency 
Accessibility 
Consistency  
 
Availability 
 Updating 
Consistency 
Cost: 
• Financial 
• Temporal 
Time: 
• Financial 
• Temporal 
 
 
Accessibility 
Geographic 
proximity 
 
 
Treatment 
 
  
Table 2.1: Summary of non-topical qualitative properties. 
12
   
 
13
We include three lists of text property categories from each of the two perspectives 
(information quality control and user information needs).  The last column (Tang, 2003) is 
also a user-centered study.  However, it is different from other user-centered studies in that 
the purpose of the study is to elicit important non-topical document qualitative properties, 
rather than to study the complicated phenomenon of user information needs or relevance 
judgments.  The properties in the last column were obtained through a focus group study with 
news professionals.  
We see a wide variety of properties.  From the measurement point of view, I classify 
the properties into two classes: 
• Quantified qualitative properties: the properties of an information object that 
currently have either a direct quantitative measure or a widely accepted 
indirect quantitative measures.   Currency or timeliness, for example, can be 
indirectly measured by publication date or update date.   
• Un-quantified qualitative properties: the properties have no direct or indirect 
measures that can be obtained from calculation currently, but require human 
judgment. 
In this research, the focus is on those un-quantified qualitative properties.  The nine 
properties identified by Tang, et al (2003) include most of the common aspects of the other 
lists and the data are available for analysis.  We focus in this study on six out of the nine 
properties, aiming at finding a way to assess them automatically, while leaving to future 
work possible extensions of this property set.  Three properties are excluded from this study.  
Author/Source Credibility is believed to be closer to the metadata (author, publisher) rather 
   
 
14
than the text itself.  Considering that our corpora are composed of mostly news articles, the 
grammar of the texts is under editorial control.  So it would be hard, if not meaningless, to 
explore the very small variance of the grammar scores.  Readability is not included because 
there is a pretty mature agreement on predicting the readability level of a text, in the area of 
education, as will be mentioned in the next section.   
2.4 Assessment of Qualitative Properties 
As discussed in previous section, many researchers have noticed the importance of using 
non-topical document properties together with topical relevance as retrieval criteria.  There 
have also been many efforts to identify properties that are critical to users.  But there are few 
studies focusing on how to assess these properties.  Naumann and Rolker (2000) classify 
information qualities into three categories from the perspective of assessment: subject-
criteria, object-criteria, process-criteria; and suggest various measures for each category.  
Most of the assessment methods suggested by Naumann and Rolker, whether for subject or 
object criteria, need continuous input from human beings (either users themselves, or 
experts).  With a huge amount of information, using human judges to do the work manually 
is inefficient or impossible.  Lack of an efficient assessing method is one of the reasons that 
the non-topical properties identified are not widely used in information retrieval engines.   
Information technology automates human information seeking through the 
quantitative representations of document properties.  It is always a challenge for researchers 
to find effective representations for qualitative properties.  The core of IR research has long 
been and will continue be the improvement of quantitative representation of document 
   
 
15
content.  However, some researchers have tried to assess some non-topical properties 
indirectly through quantitative measures which can be obtained automatically by computers.  
Some of the methods are intuitive and widely accepted.  Currency is representative.  Zhu and 
Gauch (2000), in their work which demonstrated the improvement of search efficiency by 
incorporating some non-topical metrics, measured “Currency” by the time stamp of the last 
modification of the document, and “Availability” as the number of broken links on a page.  
Several link-based metrics have been developed as an additional criterion in the hyperlinked 
web environment (Kleinberg, 1998; Chakrabarti et al, 1998; Bharat and Henzinger, 1998).  
Such metrics are generally explained or used as document authority measures.  The Google 
search engine uses the number of links pointing to a Web page as the basis for computing 
Page Rank, a measure of “popularity” or “authority”.   
These studies exemplify the efforts in exploring metrics for non-topical document 
properties.  “Hard” metadata and structural features outside the documents are the main 
sources of the metrics. Yet there is little effort that examines the language of document itself.   
Outside of the field of information retrieval, document properties have been 
connected with the language of document itself.  The connections are based on the 
assumption that language not only expresses the content, but also delivers other things. 
Readability is the one property for which there has been a tremendous amount of 
research on automatic determination it, in the field of education.  Some readability formulas 
(SMOG, FOG) were proposed long before computer-based reading became popular (Giles, 
1990).  Those formulas are based on linguistic features, such as the number syllables per 100 
words, average number of words per sentence, etc. 
   
 
16
Another area in which document properties are connected with linguistic features is 
computational linguistic studies.  These will be reviewed in detail in the next chapter. 
We can see three types of sources as indirect measures of document properties: 
meaningful word counts (topicality); metadata (authority and currency); and linguistic 
features (readability). The qualitative properties focused in this research are most similar to 
Readability.  It is our hope that analyzing the linguistic features of the text may provide some 
rules for automatic assessment of document non-topical qualitative properties. 
In this review, we selected 6 non-topical properties of text that are widely accepted 
and amenable to automatic analysis.  We also decide that the language of the text itself is the 
source of measures for these properties. 
   
 
17
Chapter 3: Linguistic Features 
In this chapter, we review the stylistic studies that are relevant to our work.   The problem 
investigated in this study concerns classification of documents with respect to non-topical 
properties.  The differentiation between topical and non-topical document properties is 
conceptually similar to the differentiation between the concepts of “style” and “content” in 
the context of computational linguistic studies.  Thus language features that are linked to 
some “style” are possible indicators of non-topical text properties.  
3.1 Stylistic Studies 
Style may be roughly defined as the “manner” in which a text is written and is contrasted to 
the “content” of the text.  Studies focusing on the “style” of texts are called Computational 
Stylistics, whose origins are Stylometrics.   Stylometrics, which has been pursued for 
decades, looks for a few features of a text that are statistically significant for identifying a 
given stylistic difference.  Computational stylistics, beyond using hand-selected feature sets 
and statistical analysis, “seeks to apply statistical and machine learning techniques to features 
extracted from text, in order to answer style-related questions” (Whitelaw and Argamon, 
2004).   
   
 
18
The assumption of stylistics research is that the style of any text is linked to the 
author’s choice of words and to the choice about the arrangement and punctuation of these 
words that are actually used in a document.  Each specific choice is affected by several 
factors, the purpose of the text, the author’s characteristics, such as educational background, 
social status and personality, and the expected audience.  When it can be done, identifying 
the “style markers” (text features) permits categorizing or identifying documents with 
particular styles.   
Based on the various causes of “style” difference, stylistic studies are divided into 
two categories: authorship attribution, and genre classification. 
3.1.1 Authorship Attribution Research 
Authorship attribution research uses language patterns to differentiate documents written by 
different authors.   The basic assumption of authorship analysis is that each author has 
“distinctive writing habits” that are displayed as language features.  “Stylometry”1 attempts 
to identify these features and to determine statistical methods to measure these features.  The 
studies indicate that an author’s style is marked by some linguistic features.   
The American Federalist papers problem is one of the well-known authorship 
analysis problems. The original statistical study on the Federalist papers was conducted by 
Mosteller and Wallace in 1964 (1984).  They compared frequencies of a set of function 
words2.  The problem was revisited numerous times subsequently (Holmes, 1998; Bosch and 
                                                 
1 “Stylometry - the statistical analysis of literary style - complements traditional literary scholarship since it 
offers a means of capturing the often elusive character of an author's style by quantifying some of its features.” 
(Homels 1998)  
2 “Function Word - a word which have no lexical meaning, and whose sole function is to express grammatical 
relationships.” (Glossary of linguistic terms, edited by Eugene E. Loos et al, 1999) 
   
 
19
Smith, 1998).  In these studies, specific linguistic features such as unusual diction, frequency 
of certain words, choice of rhymes, vocabulary richness and habits of hyphenation have been 
used as predictors for author attribution.   
Studies on other text corpora also found links between authorship and language 
patterns. Hoover (2001) concludes that the frequencies of very frequent words can 
distinguish texts by different authors.  Stamatatos, et al. (1999) were able to distinguish texts 
by various authors of a weekly newspaper downloaded from the World Wide Web, using a 
set of style markers including sentence count, word count, punctuation mark count, etc.  
3.1.2 Genre Classification 
Classification of texts by genre is another major topic in the stylistic research.  The meaning 
of “style” has thus been expanded from the style of an individual author to the general 
characteristics of a class of text.  Genre, according to Webster’s dictionary, is “a category of 
artistic, musical, or literary composition characterized by a particular style, form, or content”.  
We use “genre” here to refer to all dimensions of style variation other than authorship. 
Douglas Biber has studied text variation, and has found that texts can be considered 
to vary along five dimensions.  In his study, he clustered features according to their 
covariance, to find underlying dimensions (Biber, 1990, 1993; Biber et al. 1998).  The 
STYLISTIQUE program (DiMarco & Hirst 1993) classifies texts according to a 3-
dimensional scheme (clarity-obscurity, concretness-abstraction and staticness-dynamism).  
Michos, et al. (1996) in their studies divide texts into five style categories: public affairs 
style, scientific style, journalistic style, everyday communication style and literary style.  
   
 
20
They identify some “Style Features” for each style category, such as formality, elegance, 
syntactic complexity, and verbal complexity.   
The common goal of these studies is to build some formal representation of stylistic 
knowledge based on linguistic theory.  The next group of studies is more practically oriented.  
The major focus is to solve the problem of automatic classification of texts with regard to 
“style” or “genre”.   
Wolters and Kirsten (1999) used the Brown corpus3 and created four categories based 
on genre: press texts, press texts from high quality broadsheets and magazines, fiction and 
the low-quality subset of fiction.  Wiebe et al. (2000, 2005) seeks to distinguish subjective 
sentences, used to present opinions and evaluation, from sentences used to objectively 
present factual information.  She has investigated this classification at the sentence level and 
finds that the presence and type of adjectives in a sentence is an indicator of whether the 
sentence is objective or subjective.  Finn and Kushmerick (2003) investigated two sample 
genre classification tasks: objective or subjective; and positive or negative.  Whitelaw and 
Argamon (2004) studied the classification of three registers: scam texts, newswire, and 
spoken texts. 
3.1.3 “Style” and Non-topical Qualitative Properties 
It is clear that there is no definitive agreement as to what is meant by genre or style. The 
terms are used with different meanings and at different levels of scope in the reviewed 
studies above. However, these terms both refer to some common, content-independent, 
                                                 
3 The Brown Corpus was a carefully compiled selection of current American English.  It consists of one million 
words of American English texts printed in 1961. The texts for the corpus were sampled from 15 different text 
categories. (Wikipedia) 
   
 
21
characteristics of a group of texts.  Some of the “style” categories (“Objectivity”) overlap 
with the document properties that we defined.  Some of them are related.  For example our 
“Verbosity – Conciseness” property is obviously related with the verbal complexity 
categories.  We will divide these “genre” categories into two levels.  One level will be called 
“genre”, and is a classification based on the purpose of the texts.  Borrowing a definition 
from Michos, et al. (1996), the other level will be called “style features”, which include more 
specific characteristics of the texts.  Each genre is linked to multiple style features.  For 
example, documents in the scientific genre are more likely to be objective and formal than 
those in the literary genre.  In some sense, a genre can be defined by the style features linked 
to it. 
Clearly, the target non-topical properties in this study are highly related to the concept 
of “style features”, even though our non-topical properties are defined more perceptually.  It 
is natural to hope that the feature sets used in Computational Stylistics or Stylometrics 
studies are promising candidates as document qualitative properties indicators.   
3.2 Linguistic Features as Indicators 
Studies have shown that some patterns in language (the actual words and the patterns of 
words that are used in a document) are linked to the processes of writing and reading, and 
thus to “style” in the wider sense.   
Various other types of computable linguistic features have also been proposed in 
stylometric research.  Karlgren and Cutting (1994) use a combination of noun count, “it” 
   
 
22
count, words per sentence average, type/token ratio, etc.  Kessler et al (1997) include 
punctuation mark counts in their analysis.  Stamatatos et al. (2000) use the frequencies of 
occurrence of the most frequent words of the entire written language, which reduces the 
limitation of using the frequencies of occurrence of the most frequent words in a training 
corpus. 
These linguistic features are examples of evidence which is thought to be useful in 
differentiating documents by “style”.  Some studies tried to summarize these linguistic 
features.  In his early review of the analysis of literary style, Holmes (1985) lists a number of 
possible features that can be used in analysis of authorship.  These includes: 
• Word-length: frequency, distributions 
• Syllables: average syllables per word, distribution of syllables per word 
• Sentence-length 
• Distribution of parts of speech 
• Function words 
• Vocabulary or lexical richness measures, such as the Type/Token ratio 
• Word frequency distribution 
According to Rudman, over 1,000 linguistic features have been proposed (Rudman, 
1997).  Tweedie et al. also list a variety of different linguistic features which can be used as 
style markers (Tweedie et al, 1998).  Chaski’s work includes word length, vocabulary 
richness, frequency of function words, punctuation marks, etc. as common “style markers” 
(Chaski, 2001).     
   
 
23
 The variety of features described above indicates that there is some success but there 
is no consensus on which features are best.  The types of “style markers” identified in those 
studies are summarized in the table below and they are used as candidate features in this 
study. 
Type Examples 
Part of speech frequencies Noun, Verb, etc. 
Punctuation Marks frequencies Period, comma 
Length Average words per sentence 
Average paragraph length 
Ratio Type/token ratio 
Verb/noun ratio 
Frequencies of function words, 
common words 
Most frequent words 
 
Table 3.1: Types of text and linguistic features used as “style markers” 
 
In this study, we choose, from these “style markers”, indicators to automatically 
assess the six non-topical properties of texts.  
   
 
24
Chapter 4: Automatic Classification Techniques 
There are three possible ways to classify text on non-topical properties.  The first one is using 
human experts.  This process is obviously very time consuming and costly, so its 
applicability is limited.  The second possible approach is rule-based, similar to those used in 
expert systems or some natural language processing (NLP) modules.  This type of approach 
also requires outside resources, including manual construction of rules and ontologies.  It is 
hard to modify such a classifier.  Compared with these approaches, classification through 
learning techniques has several advantages.  It is easy and economical to construct and to 
update.  It requires information that is easy to obtain (examples of instances that are in, or not 
in, categories).  It is flexible, in that the users can adjust the threshold depending on their task 
requirements.   
Our interest in classification through learning techniques is also based on the belief 
that people’s perceptions of non-topical document qualitative properties are not randomly 
distributed: there are general trends and patterns in the perception of a single person and 
perhaps also within groups of people.   
In this chapter, we introduce several standard, state-of-the art machine learning 
techniques and their applications in text classification.  
   
 
25
4.1 Classification through Learning 
Machine learning offers many techniques for extracting models from data.  According to 
Witten and Frank (2000), “Things learn when they change their behavior in a way that makes 
them perform better in the future.”  Predictive modeling is a class of machine learning 
techniques that can deal with the classification problem.  Predictive modeling includes both 
classification, for categorical prediction, and function approximation, for numerical 
predictions.    
Classification is the process of trying to assign the correct category to unlabeled data, 
given existing classified (labeled) data. Typically the data set is divided into a training data 
set and a test data set.  Elements of a training data set are described by a set of independent 
features and a target variable whose value is available.   A machine learning algorithm is 
applied to the training data set, iteratively, to identify patterns of features in the training data 
set.  This is usually repeated many times until the error is reduced to below some threshold.  
The patterns that result may include many features from the set or only a few of them.  The 
produced pattern is represented in some type of model, such as a decision tree.  Once a 
pattern is chosen, the test data (unknown to the algorithm) are run through the pattern, and 
the error rate is recorded.  Again, this is usually repeated several times with different test 
sets, to get an average error rate.  Functional approximation for numeric prediction is 
classification learning but with numeric “classes”.  An example is linear regression.   
Consider a classification problem, in which each case is assigned to one of the m 
classes {C1, C2, …, Cm}.  There are n input features, X={x1, x2, …, xn}, whose value is 
assumed known for every case. The process of learning is shown in Figure 4.1. 
   
 
26
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 4.1: Process of machine learning 
 
There are several well known statistical classification and machine learning 
algorithms, including Logistic Regression, Decision Trees, Nearest Neighbor Classifiers, 
Naïve Bayes, Neural Networks, and Support Vector Machines (SVMs).  Each of these 
methods has its own advantages and disadvantages, depending on the given problem.  The 
learning algorithms included in this research are Linear Regression, Logistic Regression, 
Decision Trees, and SVMs.  Linear regression and logistic regression represent classical 
Training Data Set 
 
x11 x12 … x1n c1 
x21 x22 … x2n c2 
… 
xn1 xn2 … xnn cn 
Machine 
Learning 
Algorithm 
Identified Pattern: 
fk(X, W) => Ck 
W: the vector of parameters 
X: the vector of input features
Testing Data Set 
 
x11 x12 … x1n ? 
x21 x22 … x2n ? 
… 
xn1 xn2 … xnn ? 
How good is 
the model? 
   
 
27
simple linear models.  The former one is for numerical prediction and the latter is for binary 
classification.  Decision Trees represent learning techniques which represent knowledge 
(relations between attributes) by Boolean logic.  Support Vector Machines use linear models 
to implement nonlinear boundaries between classes.  It is claimed to be particularly 
promising in classification based on contents (Dumais, et al. 1998).  Some other categories, 
particularly instance-based learning, such as Nearest Neighbor method, are not included in 
the study because the entire training data must be scanned to classify each test case, which is 
very time consuming in realistic situations. 
4.2 Linear Regression 
When the dependent variable (outcome or class) is numeric, and all the independent variables 
are numeric, linear regression is a natural technique to consider.  Simply, the method 
expresses a class as a linear combination of the attributes.  The analysis tries to obtain the 
best additive linear regression model: 
NoisexY ii ++= ∑ ββ 0  
where: Y is the dependent variable, 
xi is the value of the ith independent variable, 
βi is the parameter for the ith independent variable. 
Equation 4.1: linear regression function. 
 
 
 
   
 
28
The optimization is achieved by determining parameters βi so as to minimize the sum 
of the squares of differences calculated as Equation 4.2. 
 
∑ ∑−−= 20 )()( iii xYSSE βββ  
Equation 4.2: The sum of the squares of differences 
 
The output of training is a set of numeric parameters β that can be used to predict the 
class of new cases. 
4.3 Logistic Regression 
Logistic regression analysis is a statistical technique for modeling a binary dependent 
variable y (y Є {0, 1}).  In Logistic Regression, a linear combination of one or more 
independent variables is related to the natural log of the odds that the dependent variable 
equals to 1, rather than to the dependent variable itself.  It tries to establish a model that may 
be written as:  
)ixiω(ω
ii
e
xyp
or
x
xyp
xyp(
∑+−+
=
+=
− ∑
0
0
1
1)|(
)
)|(1
)|(ln ωω
  
where:   
p(y|x) is the estimated probability that a given instance has dependent variable y = 
1,  
   
 
29
xi is the ith independent feature of the instance,  
ωi is the weight of independent feature xi, 
ω0 is the constant weight,  
e is the base of natural logarithm, about 2.718. 
Equation 4.3: Logistic regression function 
 
The goal of logistic regression is to correctly predict the category of outcome for 
individual cases.  To accomplish this goal, a model is created that includes all independent 
variables that are useful in predicting the dependent variable, and the weights (ωi) are chosen 
to maximize the conditional probability of the sample data.  The estimates are called 
maximum likelihood.  The likelihood function may be written as: 
∑ −−+= )]1ln()1()ln([)( iiii pypyL ω  
Equation 4.4: Log-likelihood function 
 
During model creation, any feature present in the data can be entered into the model.  
It is also possible to select the features based on statistical criteria, such as the Wald4 or 
likelihood ratio statistic5.  
 The logistic regression model is simply a non-linear transformation of the linear 
regression, but due to the change in objective function, it yields different weights wi. 
(McCullagh and Nelder, 1989).   Its benefits include a firm statistical foundation and a 
probabilistic model useful for “explaining” the data. 
                                                 
4 Wald statistic of an estimated weight equals the weight divided by it standard error. 
5 Likelihood ratio statistic = -2ln(likelihood model 2 / likelihood model 1). 
   
 
30
4.4 Decision Tree Learning 
Decision tree learning takes its name from its tree structure.  A decision tree is a 
representation of a procedure for determining the class of a given instance.  A decision tree 
consists of: 
• Leaf, or answer, nodes that indicate a classification, either positive or negative. 
• Non-leaf, or decision nodes, which contain an attribute name, and branches to 
other decision nodes or leaf nodes, one for each value (if it is categorical) or range 
of value (if it is numeric) of the attribute. 
The tree creating process starts with picking a root node and proceeds to generate 
sub-trees recursively until leaf nodes are reached.  The attributes are picked with the goal of 
building small trees. For example, we might have records reporting on weather conditions for 
playing golf.  In each case, there are four attributes, outlook, temperature, humidity, and 
wind; and the outcome is whether to play or not.  The data is shown in Table 4.1 (Witten & 
Frank, 1999), and Figure 4.2 shows the decision tree. 
 
Predictors Target variable 
Outlook Temperature Humidity Windy Play 
Sunny Hot High False No 
Sunny Hot High True No 
Overcast Hot High False Yes 
Rainy Mild High False Yes 
Rainy Cool Normal False Yes 
   
 
31
Rainy Cool Normal True No 
Overcast Cool Normal True Yes 
Sunny Mild High False No 
Sunny Cool Normal False Yes 
Rainy Mild Normal False Yes 
Sunny Mild Normal True Yes 
Overcast Mild High True Yes 
Overcast Hot Normal False Yes 
Rainy Mild High True No 
Table 4.1: The weather sample data (Decision Tree Method) 
 
  
 
 
 
 
 
Figure 4.2: Decision tree for the weather sample data. 
 
Now the question is how to choose an attribute to split on, so we can achieve a small 
tree.  Considering the weather example, the question is why “outlook” is chosen to be the 
root of the tree rather than ‘humidity”, “temperature” or “windy”.  The Decision Tree method 
uses a measure called Gain Ratio.  The Gain Ratio is built on a basic measure called 
information value or entropy, which is defined on probability distributions (p1, …, pn).  
Outlook 
Overcast 
Play 
Sunny Rainy 
Humidity Windy 
Normal High False 
Play Not Play Play 
True 
Not Play
   
 
32
.log...loglog),...,,( 221121 nnn pppppppppentropy −−−=  
pi is the probability of the ith value. 
n is the number of possible values. 
Equation 4.5: Entropy function 
 
Two kinds of information value are calculated for each possible attribute: the gain 
info and the split info.  The Gain Ratio is calculated by dividing the gain info by the split 
info.  The standard is to choose the attribute that maximizes the gain ratio. 
The gain info indicates the informational value of creating a branch on the attribute, 
which is the difference between information values before and after splitting at the attribute.     
In the weather data set, before making any split, there are nine positive cases and five 
negative cases.  So the information value before splitting is: 
info(before) = entropy (9/14, 5/14) = 0.94. 
If the split is made on the “outlook” attribute, there are three leaf nodes: sunny with 
two positive cases and three negative cases; overcast with four positive cases; and rainy with 
3 positive cases and 2 negative cases.  The entropy of each leaf node is: 
entropy(sunny) = entropy (2/5, 3/5) = 0.97 
 entropy(overcast) = entropy(4/4, 0/4) = 0 
entropy(rainy) = entropy(3/5, 2/5) = 0.97 
so, taking into account the number of instances in each node, the average information value 
of attribute “outlook” is: 
 avg_info(outlook) = 5/14 entropy(sunny) + 4/14 entropy(overcast)+5/14 entropy(rainy)  
   
 
33
           = 0.69. 
This average represent the amount of information that we expect would be necessary 
to specify the class of a new case.  The gain info of splitting at the outlook attribute is: 
gain info(outlook) = info(before) –avg_ info(outlook)  
        = 0.25   
The split info measure is used to punish the attributes with big number of branches.  
For example, outlook has three branches, each has 5, 4, 5 cases respectively.   
split info (outlook) = entropy (5/14, 4/14, 5/14) = 1.58 
This is in fact the information value of the attribute, disregarding any information 
about the class. 
Finally, we can calculate the gain ratio of outlook, which is  
gain ratio (outlook) = gain info / split info = 0.25/1.58 = 0.16 
With the same method, we can calculate that the gain ratios for humidity, temperature 
and windy respectively: 
 gain ratio (humidity) = 0.15 
 gain ratio (temperature) = 0.02 
 gain ratio (windy) = 0.05 
According to the principle, outlook is chosen to be the root over other attributes with 
the highest gain ratio. 
In practical implementation, other ad hoc tests are also used to adjust the selection 
standard. 
   
 
34
4.5 Support Vector Machines (SVMs) 
Support Vector Machines (SVMs) were first introduced by Vladimir Vapnik in 1979 (Burges 
1998).   SVMs were developed to solve classification problems, and they have been gaining 
popularity for text classification since late nineties (Dumais et al, 1998).  
We imagine that every case is represented as a point in a high dimensional Euclidean 
vector space.  Suppose there are some hyperplanes which can separate the positive from the 
negative cases.  The support vector machine algorithm looks for the separating hyperplane 
with largest margin.  In the simplest case of linear support vector machines, the margin is 
defined as the distance of the hyperplane to the nearest positive and negative instances.  The 
positive and negative instances which are closest to the hyperplane are called support vectors.  
 
Figure 4.3: Linear support vector machines 
 
Margin 
maximum margin 
hyperplane 
   
 
35
The output of a linear SVM is the maximum margin hyperplane, which is just a linear 
model.  It might be written as 
bwxxf +⋅=)(  
Equation 4.6: SVM hyperplane function 
 
where w is the normal vector to the hyperplane, and x is the input vector.   Given a new 
instance, it is simply to determine on which side of the hyperplane the new instance lies and 
to assign the corresponding category, e.g. we label the instance with following rule: 
 
 
 
 
 
 
Equation 4.7: SVM classification rule 
4.6 Applications  
As the volume of information available continues to increase, automatic classification 
techniques have been used to help people better find, or filter information.  To date, the most 
widespread application has been for classification based on text content, to support text 
retrieval, or filtering.  Dumais et al. compare the effectiveness of five different automatic 
learning algorithms for text categorization (Dumais et al. 1998). According to their review, 
statistical and machine learning techniques that have been applied to text classification 
include: multivariate regression models, nearest neighbor classifiers, probabilities Bayesian 
where: )(ˆ xy is the predicted label of case x 
1 if 0>+⋅ bwx
-1 otherwise
=)(ˆ xy
   
 
36
models, neural networks, and SVMs.  They draw the conclusion that linear SVMs are 
“particularly promising because they are very accurate, quick to train, and quick to evaluate”.    
Stylistic research has provided some examples of applying automatic classification on 
genre/author/style.  Previously, stylistic research tended to use only statistical methods.  Now 
computational stylistics comprises a growing body of work in which new machine learning 
techniques are used.  Kessler et al. (1997) used logistic regression as their basic method.  
Wolters and Kirsten (1999) explored nearest-neighbor learning algorithms with part-of-
speech tag frequencies and demonstrated effective classification based on genre.  Finn and 
Kushmerick (2003) used the decision tree C4.5 as their main learning algorithm to 
investigate two genre classification tasks.  In Whitelaw and Argamon’s work (2004), 
classification of three registers was performed using three different machine learners: Naïve 
Bayes probabilistic classifier, decision tree and support vector machine.  The best result 
comes from the SVM.  However, the major focus of these studies is on the predictive power 
of the features, rather than on the learning method used.    
In previous studies, the corpora are either standard genre collections or specifically 
generated for the purpose of machine classification.  In the present study, the document 
properties were judged by human participants without any notion that the judgments will be 
used for machine learning.  Thus it is truly subjective human judgments, rather than the 
objective document properties, that are to be linked to the learned patterns of features.   
There is also some related work under the title of “user modeling”, which is “a 
discipline which is concerned with both how information about users can be acquired and 
used by automated systems.”   (Oard 1997)  When users interact with an IR system, the 
system can collect a great amount of information about the users directly and/or indirectly: 
   
 
37
such as their behaviors, their judgments about information objects.  Machine learning 
techniques can analyze the collected information and try to find patterns in it.  The patterns 
“acquired” would then be used to make suggestions to users’ further interaction with the 
system.   
  In recent years, the application of machine learning techniques has appeared in IR 
studies mainly for providing personalized assistance and adaptation, based on patterns of user 
behavior or stereotypes, such as the Antworld system (Kantor, et al. 1999a, 1999b), which 
collects users’ judgments of the web pages based on their relevance to the query.  However, 
few of them focus on user’s judgments of non-topical properties. 
In the Syskill and Webert system (Pazzani et al 1997) a user profile is created based 
on a rating of the user.  Information objects are represented as Boolean feature vectors.  A 
user is asked to classify a number of information objects as either positive examples or 
negative examples in terms of her or his interests.  Using this pre-classified data, the system 
uses machine learning techniques to generate a new profile.  They compared the performance 
of six different machine learning techniques.     
The reported successes of machine learning applied to text classification on topic, and 
on genre, lead us to believe in the possibility of recognizing text non-topical qualitative 
properties with the same methods.  
   
 
38
Chapter 5: Research Problems 
The overall aim of this study is to uncover patterns of language which are correlated with 
non-topical properties of texts.  As shown in Figure 5.1, the work is to explore the shaded 
area which connects the non-topical requirements of information users and the non-topical 
properties of documents.    
Our approach is to use statistical and machine learning techniques for automatic text 
classification with regard to non-topical qualitative properties.  We attempt to identify those 
language features that will lead to classifiers that perform well across multiple corpora and 
multiple human judges, and can easily be built automatically.  We will investigate the 
performance of different types of classifiers.   
 
 
 
 
 
 
 
 
   
 
39
 
 
 
 
 
  
 
 
 
 
 
 
 
 
 
 
 
 
Figure 5.1:  Model of combined IR. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Combined IR System 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Information 
Needs 
Topical 
Requirements
Non-Topical 
Requirements 
Query words & 
Index words match 
in a certain way 
Machine learning 
method: Linguistic 
Feature Models?
Topical 
Properties 
Non-Topical  
Properties 
   
 
40
More specifically, we will try to answer the following questions in the research: 
• Can language features be used as indicators of non-topical qualitative properties 
of texts? 
• How well does the identified language feature set perform on automatic text 
classification based on an individual user’s judgment of non-topical qualitative 
properties?  
•   How well does the identified language feature set perform on automatic text 
classification across multiple users’ judgments of non-topical qualitative properties? 
• How well does each learning technique perform? 
• Is machine learning a feasible technique for automatic text classification based on 
non-topical qualitative properties?  
The target user group of this research is those information users with long-term, 
recurring information needs rather than short-term goals.  This is because users with long-
term goals are more likely have extra requirements regarding properties of documents other 
than topical requirements.  Also, a lengthy learning process, which is an unavoidable for 
applying machine learning techniques, is tolerable for users with long-term goals.   
   
 
41
Chapter 6: Methodology, Experimental Design and 
Evaluation Measures 
In this chapter, we describe generally experiments for categorizing texts according to pre-
defined non-topical property levels.  More details about each experiment are included in the 
corresponding chapter. 
We summarize the general learning processes in Figure 6.1 (next page).  Text files are 
processed using the General Architecture for Text Engineering (GATE) system and a set of 
self-developed Perl scripts, to obtain language features.  All features are saved, together with 
their frequencies.  A separate classifier is learned for each non-topical property and each 
technique.  The details are described in the following sections. 
6.1 Document Corpora 
The corpora of texts used in this research are from the HITIQA project collection, which 
includes two parts.  Part I includes one thousand medium-sized (100 words to 2500 words) 
news articles from the TREC collection, including articles from the Los Angeles Times, Wall 
Street Journal, Financial Times of London, and the Associated Press.  Part II contains texts 
coming from three other sources.  Basic information about the whole collection is 
summarized in Table 6.1. 
   
 
42
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 6.1: Learning Processes 
 
General Architecture 
for Text Engineering 
(GATE) 
Perl Scripts  
(for feature 
counting) 
Text Files 
Language Features 
Date Set 
Non-topical Properties 
Judgments Data Set 
Data Set 
Linear 
Regression 
Logistic 
Regression
Decision 
Tree 
Support Vector 
Machine 
Learning methods 
Evaluate Classifiers 
   
 
43
There are a total of 3,200 documents in the combined collection which will be 
referred as “combined” corpus later in this paper. 
 
 ID Doc Type Number Length Source 
Part I A1 TREC 1,000 100 –2,500 
Articles from the LA Times, 
Financial Times of London, 
and Foreign Broadcast 
Information Service.  
A2 CNS 1,100 166 – 2,875 
Nonproliferation data from the 
Center for Nonproliferation  
Studies (CNS) (1982 – 2001) 
A3 AQUAINT 600 104 –2,175 
Articles from the Associated 
Press Worldstream (APW), 
NY Times (NYT), and 
Xinghua English (XIE)  
Part II 
A4 WEB 500 0 – 2,372 Collected using Google  
 
Table 6.1: Experimental corpora summary 
 
6.2 Non-Topical Qualitative Property Judgments 
The qualitative properties of the texts in the Part I corpus were scored by human in July, 
2002.  The Part II corpus was judged in summer 2003.  Each document in the collection was 
judged in terms of 9 qualitative properties.  The judgment is reported on a 1-10 scale, with 0 
meaning “Not Applicable”.  Each document was judged by two judges.  One judge was a 
student at SUNY Albany, while the other was at Rutgers.  These two scores are assigned to a 
document on each qualitative property.  The judges have the option to save pieces of 
   
 
44
messages which support their judgments.  A study of the data consistency found a judgment 
discrepancy between two institutions (Bai et al. 2004).  Because we have no control of the 
data collection procedure at SUNY Albany, we, in this analysis, use only data collected at 
Rutgers site.  For this work, we analyzed the documents only in terms of the six qualitative 
properties mentioned above.  The detailed information on data collection is reported in Tang 
et al. (2003) and Ng et al. (2003, accepted).   
To study the “learnability” of an individual’s judgment of document qualitative 
properties, we used a special collection of judgment data.  Five judges were invited to do 
more judgments. The information about documents they judged is summarized in Table 6.2. 6 
 
Source 
Judge ID 
CNS AQUAINT WEB 
Total Number of 
documents 
judged 
S01 251 123 86 460 
S02 223 114 73 310 
S03 170 55 85 410 
S04 263 129 88 540 
S05 295 147 98 480 
 
Table 6.2: Individual datasets. 
 
All of the documents judged by such an individual scorer make up a second type of 
corpus.  There are total five such corpora labeled Corpus B1 to B5 respectively. 
 
                                                 
6 Special thanks to HITIQA project team, especially Robert Rittman for coordinating the judges. 
   
 
45
6.3 Language Features 
This section discusses features, the “observable” properties of a text that are associated with 
document qualitative properties.  The feature sets we explored are divided into four wide 
categories as Kessler et al. (1997) did. 
6.3.1 Character – Level Features 
Character-level features included in this study are mainly punctuation marks, other delimiters 
used to mark text categories, like sentences and paragraphs, special symbols, capitalized 
words, and acronyms.  We believe they are important in assessing the document qualitative 
properties.  Examples include counts of question marks, quotation marks, average length of 
word, all upper case words, etc.  
6.3.2 Lexical – Level Features 
This is the set of features usually related to document styles. We include two types of lexical 
level features: special words, and types of words.  Examples of the first group include counts 
of word such as “say”, “seem”, which may be indicators of subjective expression.  The 
second type includes lists of words, such as words used in expressing dates, which are 
common in delivering accurate information.  
6.3.3 Structural - Level Features 
We use counts of part-of-speech tags for this level.  For purpose of automatic assessment, 
these features require texts to be tagged.  We used the GATE (General Architecture for Text 
   
 
46
Engineering) system.  GATE has been developed at the University of Shffield since 1995 and 
has been used in a wide variety of research and development projects (Cunningham et al. 
2000).  
6.3.4 Derivative Features 
These are ratios and other features, derived from previous three levels of features.   
The features are summarized in Table 6.3.  
 
Feature 
levels Types Examples 
Punctuation 
Marks 
periods, question marks, exclamation marks, commas, 
semicolons, colons, dash, ellipsis, parentheses, brackets, 
quotation marks, forward slides, apostrophes, hyphens 
Symbols dollar signs, percent signs, plus signs,  > marks, ampersands 
Character 
Level 
Length 
Average length of words in characters, sentence in words, 
paragraph in words. Length of title, subtitle, leading 
paragraph, and document 
Key Terms "say", "seem", and "expert" 
Unique 
words unique words, unique words excluding stop words 
Entities person, location, organization, and date  
Lexical 
Level 
Declarative 
words 
Explicit Declaration,  Other Assertion, Entities other than 
human proper names 
Structural 
Level POS 
proper noun, personal pronoun, possessive pronoun, 
determiner, preposition, verb in base form, verb in past 
tense, verb in present participle, verb in past participle, 
verb in present tense, verb in ing form 
   
 
47
Derivative 
Features Ratio, 
Quotation 
Average quotation length, average distance between 
subject and declarative verb, Verb/Noun ratio, ADJ/Noun 
ratio. 
 
Table 6.3: Linguistic feature sets. 
 
As mentioned above, we use GATE to obtain the Part Of Speech (POS) frequencies.  
We also use GATE’s Gazetteer function to count the frequencies of lists of words (entities 
and declarative words).  GATE itself has some default entity lists, such as person, location, 
date, etc.      
The original declarative word list was created by a HITIQA project researcher by 
examining the pieces of texts which are saved as evidence when the users make judgments.  
The list includes verbs that have declarative meanings, such as “argue” and “announce”.  
Two other declarative word lists are generated using WordNet.  WordNet is a lexical 
reference system which organizes English nouns, verbs, adjectives, adverbs into synonym 
sets.  It is developed and maintained by the Cognitive Science Laboratory at Princeton 
University (Fellbaum, 1998). We used WordNet to get the hypernyms of the words in the 
original list.  The words which appear in both the original list and the ancestor list are labeled 
R+.  The words, which show up only in the ancestor list, are labeled R-.    
For all the other features, we use Perl Scripts to calculate the frequencies7. 
                                                 
7 Special thanks to Bing Bai and Peng Song for the programming support. 
   
 
48
6.4 Experimental Setup  
6.4.1 Training and Testing Subsets 
Generally, for each classification experiment, 50% of cases are randomly selected to 
compose training set.  This random division is conducted 10 times to make 10 training and 
testing pairs.  Later the results are combined over the 10 runs of training and testing to 
stabilize the features learned.  The stable features are then tested using 10-fold cross-
validation.  In this procedure, the dataset is initially divided into 10 subsets (Groups 1 to 10).  
Next, 9 of the 10 subsets (e.g. Groups 1 to 9) are allocated for training, and the remaining 
subset (Group 10) is kept for validation.  After finishing the validation, a different subset of 9 
groups (e.g. Groups 1 to 8 and 10) is selected for training, and the remaining group (Group 9) 
is used for validation. This training and validation process is repeated 10 times with all 
possible combinations.  It should be noted that at each step the validation data set is not seen 
by the model during model development. 
6.4.2 Recoding Schemes for Binary Prediction 
Instead of distinguishing documents with 10 scores on a qualitative property, we classify 
documents into “high” or “low” (two classes) on each qualitative property.  Since, the 
judgments in HITIQA collection are on a 1-10 scale, some scheme must be used to 
dichotomize the judgments.  We tried several methods to recode the qualitative property 
variables.   
   
 
49
The first type of scheme is based on the examination of the scores assigned for each 
qualitative property by the judges.  For each qualitative property, we choose the median as 
the cut point so as to obtain an evenly distributed binary dataset.  The median of each 
qualitative property’s value is calculated and used to recode the data with the following rules:  
 
 
 
 
  
 
 
 
Equation 6.1: Recoding scheme based on median 
 
The second type of schemes is based on the 0-10 scale itself with different definitions 
of the high and low ranges.  There are three methods proposed: Normal Method, Extreme 
High Method and Extreme Low method.  The recoding schemes are summarized in Table 
6.4.  The normal method is the most intuitive, half-half splitting.  We propose two extreme 
methods to see if the documents located at the extreme ends are easy to separate from others 
on each of the six qualitative dimensions.  
 
Extreme Methods 
Original scale 
Extreme high Extreme low 
Normal Method 
(Both training 
and testing) 
0 Miss Miss Miss 
1-3 0 0 0 
4-5 0 1 0 
6-7 0 1 1 
8-10 1 1 1 
Table 6.4: Recoding schemes based on 1-10 scale. 
b_vi =  
0  for vi < mi 
1  for vi > mi 
Binary random with uniform distribution for vi = mi 
where: b_vi is binary value of qualitative property i, 
            vi is the original value of qualitative property i, 
 mi is the median of qualitative property i.
   
 
50
6.4.3 Experiments Summary 
For each data set, and for each learning method, we test two sets of language features.  First, 
we used all linguistic features identified in previous section.  Since we have more than 100 
predictors, it is more practical if we can reduce the number of predictors without damaging 
the results too much.  We believe that there is a small set of important predictors for each 
qualitative property.  If not, because of the large number of predictors, there is a high risk of 
over-fitting.  For this reason, we also run each learning method with a set of selected 
features.  For each learning method, we created a way to select features.  We will report the 
details in later chapters. 
 
 Linear 
Regression 
Logistic 
Regression 
Decision 
Tree SVM 
10-point 
scale score 
datasets 
Full feature set 
Selected 
features 
- - - 
Combined 
corpus 
Binary score 
datasets (4 ) - 
Full feature set 
Selected 
features 
Full feature set 
Selected 
features 
Full feature set 
Selected 
features 
10-point 
scale score 
datasets  
Full feature set 
Selected 
features 
- - - 
Individual 
Corpora 
Binary score 
datasets - 
Full feature set 
Selected 
features 
Full feature set 
Selected 
features 
Full feature set 
Selected 
features 
 
Table 6.5: Summary of learning experiments. 
 
In Table 6.5, we summarize the experiments we conducted in this work.   
   
 
51
6.5 Learning Tools 
For linear regression, we use the implementation in SPSS.  We use the implementation of 
other three learning methods in the public available Weka machine learning package.  They 
are: weka.classifiers.funcitons.SimpleLogistic, weka.classifiers.trees.J48, and 
weka.classifiers.functions.SMO.   
Weka is developed at the University of Waikato in New Zealand.  It incorporates 
many different Machine Learning algorithms, as well as various data pre- and post-
processing, evaluation methods.  Weka stands for Waikato Environment for Knowledge 
Analysis, and the whole package is written in Java.     
6.6 Evaluation Measures 
We need some measures to evaluate the ability of the various models to reproduce the 
manually assigned text qualitative properties.   
6.6.1 R-squared 
For linear regression, evaluation asks how closely the learned relation line fits the data.  R-
squared provides a relative measure of fit.  R-squared is a descriptive measure between 0 and 
1. An R-squared close to 1.0 indicates that we have accounted for almost all of the variability 
with the variables specified in the mode.  If we have an R-squared of 0.4 then we know that 
the variability of the Y values around the regression line is 1-0.4 times the original variance; 
   
 
52
in other words we have explained 40% of the original variability, and are left with 60% 
residual variability.  
 
∑
∑
−
−
−=−= 2
2
2
)(
)ˆ(
11
yy
yy
SS
SS
R
i
ii
total
error  
iy is the observed dependent variable value in case i, 
 iŷ  is the predicted dependent variable value in case i, 
y is the average of the dependent variable. 
Equation 6.2: Correlation Coefficient (R) 
 
The R-squared measure is used as an indicator of how well our linear regression 
models fit the data. 
6.6.2 Classification Accuracy  
Based on the “true” observation and the machine’s classification, the objects under study are 
generally summarized in the form of Table 6.6. 
 
 Predicted Positive Predicted Negative 
Observed Positive  True Positive False Negative 
Observed Negative False Positive True Negative 
 
Table 6.6: The contingency table for binary classification 
 
   
 
53
There are two kinds of classification errors: false negative and false positive.  In this 
study, we assume equal costs for both kinds of error for the six properties, even though in 
reality, different users may have different utilities or preferences.  Taking “Reliability” as an 
example, our assumption means that missing one piece of reliable information is as serious as 
trusting one piece of unreliable information.  In reality, the first type of error may require a 
higher penalty than the second type. 
Classification accuracy is the common evaluation metric for machine learning 
methods.  Base on our equal cost assumption, it specifies that:  
Total
veTrueNegativeTruePositi
casesofnumbertotal
correctlyclassifiedcasesofnumberAccuracy
+
=
=
 
Equation 6.3: Classification Accuracy 
 
6.6.3 ROC Analysis 
Existing work in classification learning assumes that the goal is to produce categorical 
classifications and to maximize classification accuracy.  Recent work in machine learning 
has pointed out the limitations of classification accuracy; when class distributions are 
skewed, or error costs are unequal, a rule set maximizing accuracy can perform poorly 
(Provost and Fawcett, 2001, Huang et al. 2003).   
ROC stands for Receiver Operating Characteristic analysis.  In ROC space, each 
classifier is represented by a point with two coordinates: the detection rate (or true-positive 
   
 
54
rate TP) and false alarm rate (false-positive rate FP) both depend (together) on the threshold 
or cut point.  They are defined as: 
 
CasesPositiveOfNumberTotal
veTruePositiateDetectionR =          (a) 
 
CasesNegativeOfNumberTotal
iveFalsePositRateFalseAlarm =      (b) 
Equation 6.4: (a) Detection (true-positive) rate;  
(b) False alarm (false-positive) rate 
 
Using these two coordinates, the performance of a classifier can be analyzed with 
respect to actual conditions.  For any specific learning method, choosing different thresholds, 
we will have a family of classifiers.  We plot an ROC curve to represent the whole family of 
classifiers.  Every point on the curve represents a possible classifier.   The closer an ROC 
curve comes to the ideal point (1, 0), which indicates 100% detection and 0% false alarm 
rate, the better the performance of the classifier(s).  The diagonal connecting (0,0) and (1,1) 
indicates performance of trivial classifiers.  Point (0, 0) and point (1, 1) represent the simple 
“all negative” and “all positive” classifiers respectively. 
Besides the shape of a ROC curve, the area under a ROC curve, or simply AUC, is 
another cue for classifier comparison.  An area of 1 represents a perfect test; an area of .5 
represents a worthless test.   
 
 
   
 
55
 
 
 
 
 
 
 
 
 
 
Figure 6.2: ROC space. 
 
A scoring classification method, like logistic regression, outputs scores for each class.  
The scores can be used as threshold to rank instances from most to least likely positive, so as 
to create the ROC curve.  The decision tree, a symbolic classification method, does not 
output any score or rank for each instance.  An ROC curve for a decision tree is generated by 
ranking leaves rather than cases.  The instances in one leaf are classified to one class.  Given 
the original number of positive and negative instances in the leaf, we can calculate the 
detection rate and false alarm rate of the leaf.   
The obvious benefit of ROC analysis is that the curve visualizes the quality of the 
models on a test set, without committing to a classification threshold.  ROC analysis is 
usually used if some conditions of the classification, e.g., the prior probabilities or 
misclassification costs, are unknown or subject to change.   
Trivial Classifiers
False Alarm Rate
D
et
ec
tio
n 
R
at
e 
All Negative 
All Positive 
   
 
56
Chapter 7: Numerical Prediction over the Combined 
Corpus 
In this chapter, we begin our experiments with numerical prediction on the corpus which 
includes multiple human assessments.  We examine the power of linear regression technique 
in simulating combined judgments (on continuous scales) on the six non-topical dimensions.  
Linear models including the full set of features are compared with those including only 
statistically selected features.  The next three chapters describe our experimental results for 
binary classification across individuals and sources.     
From the complete set of texts, we construct a pair of training and test sets. 
Randomly, half of the combined corpus is selected to train the classifiers and the other half is 
used to test the accuracy of result models in reproducing the manual assignments.  The score 
“0” was used to indicate that the qualitative property is not applicable for the text.  So, for 
different properties, such texts are excluded, which makes the number of texts for different 
properties unequal.  Table 7.1 shows the sizes of the 50-50 training and test sets.  
 
Qualitative 
Property 
Num 
Train 
Num 
Test 
Accuracy 1,555 1,562 
Reliability  1,530 1,527 
Objectivity   1,589 1,592 
   
 
57
Depth    1,596 1,596 
Verbose  1,598 1.600 
Multi-views  1,561 1,568 
 
Table 7.1: One pair of half-half training/testing set 
 
In this chapter, we report the results of linear regression.  For each text property, the original 
human judgments use a10 point Likert scale.  Technically, the Likert scale values are 
artificial.  However, they have ordinal meaning.  So the numbers can be treated as either 
numeric or categorical.  In fact, it is believed that ordinal scales with eleven or more levels 
lose little information when compared to continuous scales, and it may be possible to use 
parametric analyses with them.  We treat the numbers as numeric first, and test whether a 
linear combination of some language features can predict those values.  The analyses are 
conducted using linear regression methods implemented with the statistical software SPSS 
for Windows version 11.5, with their default parameters. 
7.1 Results of Linear Regression With All Language Features  
All language features are included in the first round of linear regression analysis.  With 106 
features as independent variables, the total fraction of variance that is explained is only 15% - 
21% (Table 7.2). 
 
Qualitative 
Property 
R2 
Accuracy .12 
   
 
58
Reliability  .15 
Objectivity   .19 
Depth    .19 
Conciseness  .15 
Multi-views  .21 
 
Table 7.2: Linear regression results over the full corpus 
 
The highest percentage of variance we can explain is only 21% (for the “Multiple 
viewpoints” property).  To understand what happened, we looked at the scatter plots of 
regression (predicted scores vs. actual scores of each of the 6 qualitative properties).  They 
all are similar (Figure 7.1).  
As indicated in Figure 7.1, although there is a discernable trend, it would be quite 
difficult to use regression scores for actual score prediction.  
 
Accuracy Reliability 
0
2
4
6
8
10
12
0 2 4 6 8 10 12 14  
0
2
4
6
8
10
12
0 2 4 6 8 10 12 14  
Objectivity Depth 
0
2
4
6
8
10
12
14
-2 0 2 4 6 8 10 12 14 16 18  
0
2
4
6
8
10
12
0 2 4 6 8 10 12 14  
   
 
59
Verbose - Conciseness One sided - Multi-viewed 
0
2
4
6
8
10
12
0 2 4 6 8 10 12  
0
2
4
6
8
10
12
-8 -6 -4 -2 0 2 4 6 8 10 12  
 
Figure 7.1:  Fitting plots of the linear regression models with all features 
 
7.2 Results With Statistical Best Features 
We used the stepwise selection method in the linear regression analysis to reduce the number 
of predictors without damaging the prediction significantly.  The algorithm automatically 
retains only those independent variables which influence the target variable most.  The 
process continues until no feature can be added.  
 
Qualitative 
Property 
R2 Number of 
Predictors 
Accuracy .10 29 
Reliability  .13 23 
Objectivity   .16 23 
Depth    .16 21 
Conciseness  .12 26 
Multi-views  .18 27 
 
Table 7.3: Stepwise linear regression results over the full corpus 
 
   
 
60
Comparing the measure R2 in Table 7.2 with those in Table 7.3, we can see that with 
a loss of no more than 3% of the variance explained, the numbers of predictors in the linear 
models have been tremendously reduced from more than one hundred to no more than 30, a 
reduction of more than 70% in complexity. 
7.3 Binary Prediction 
However, as the results show that with language features as predictors, linear 
regression cannot predict the scores assigned to texts on the six qualitative properties very 
well.   We speculated that if we divided the actual Likert scores into two ranges: “high” and 
“low”, and used the regression scores to predict “high / low” instead of the actual score, we 
might be able to do a much better job.  Taking “Depth” as an example, if we use the arbitrary 
score of 5 as a binary classification threshold, the classification result is as shown in Figure 
7.2.  The plot is divided into four blocks, and the number of cases located in each block is 
marked in the figure.  The total predictive accuracy of this simplified classifier is 63%, with a 
true positive rate of 67.5%, and a true negative rate of 56.2%, which shows that binary 
classification may be more automatable in contrast to numerical score prediction.  
 
 
 
 
 
   
 
61
 
 
 
 
 
 
 
 
 
Figure 7.2:  Dichotomization of linear prediction. 
0
2
4
6
8
10
12
0 2 4 6 8 10 12 14
650 277 
356 313 
   
 
62
Chapter 8: Binary Classification over the Combined 
Corpus 
In this chapter, we report on our classification experiments and results over the combined 
corpus across multiple judges.  The purpose of the experiments is to test whether we can 
construct a general rule for assigning a text to one of the two classes on each non-topical 
dimension.  Because the original judgments are 10-point scores, the first issue is to recode 
the continuous scales into a dichotomous distinction on each dimension.  We examine the 
effect of different dichotomization methods on the learning performance, using logistic 
regression.  Based on these results, the other two methods, decision tree C4.5 and SMO, are 
only applied to the median-based binary datasets. For each learning method, the results of the 
full set of features are compared with the results of the stable features.   
8.1 Binary Datasets  
As shown in the results of linear regression analyses, it seems promising to classify 
documents into two (“high” or “low”) classes on each qualitative property, rather than to 
distinguish 10 scores on a qualitative property.   Before the application of any classification 
method we must transform the 10-point Likert scores into binary scores.  We tried four 
schemes to group documents into two classes as described in Section 6.4.2.  Thus, we 
   
 
63
generated 4 sets of binary datasets.  The meanings of the binary dependent variables are 
described in Table 8.1. 
 
Qualitative Property Low (-1) High (1) 
Accuracy Not Accurate Highly Accurate 
Reliability  Low Reliability High Reliability 
Objectivity   Subjective Objective 
Depth    Low Depth High Depth 
Conciseness Verbose Concise 
Multi-views  One-sided Multi-views 
 
Table 8.1: Binary meanings of qualitative properties 
 
We first examined the median-based binary datasets.  The benefit of this method is 
that we obtain a nearly evenly distributed binary dataset for each property (Table 8.2).  The 
weakness is that the binary boundary is blurred by the random assignment of median cases, 
which increases the difficulty of classification.  However, this deficiency may, in some sense, 
reflect the real difficulty of the classification task.  That is, some documents simply cannot be 
clearly classified into either “Objective” or “Subjective”.  However, if we do not provide a 
third category, a judge may just randomly assign them to the “Objective” or the “Subjective” 
class.  
 
Number of cases 
Qualitative Properties 
-1 1 
Accuracy 1376 1741 
Reliability 1578 1479 
Objectivity 1829 1352 
Depth 1440 1763 
   
 
64
Conciseness 1851 1347 
Multi-views 1585 1544 
 
Table 8.2: Class distributions in median-based binary datasets 
 
We also created three sets of binary datasets based on other interpretations of the 10-
point scale itself.  The three binary datasets are: normal, extreme_high and extreme_low.   
Table 8.3 shows the distributions of cases in the datasets.   
 
Normal Extreme High Extreme low Qualitative 
Properties -1 1 -1 1 -1 1 
Accuracy 629 2488 1712 1405 173 2944 
Reliability 851 2206 1904 1153 350 2707 
Objectivity 1158 2023 2132 1049 598 2585 
Depth 1625 1568 2485 708 907 2286 
Conciseness 1017 2181 2164 1034 333 2865 
Multi-views 2055 1074 2624 505 1422 1707 
 
Table 8.3: Class distributions in the scale-based binary datasets 
 
The distributions show the obvious limitation of the scale based binary datasets: the 
class distributions are skewed seriously in some cases.  To a classification method which 
tries to minimize error, a trivial classifier which always guesses the most populous category 
might beat other more meaningful models.   
   
 
65
8.2 Logistic Regression 
There are two logistic classification algorithms implemented in WEKA package 
version 3-4-5: weka.classifiers.functions.Logistic, and 
weka.classifiers.functions.SimpleLogistic.  We begin our exploration with both of them.  The 
first one implements the standard logistic regression method.  The second one is based on the 
LogitBoost algorithm, and incorporates attribute selection by fitting simple regression 
functions in LogitBoost.  The SimpleLogistic method is claimed to be useful in obtaining 
simple models and preventing over-fitting of the training data (WEKA). 
8.2.1 On Median-based Binary Datasets 
We begin our logistic regression experiment with the median-based binary datasets.  As 
mentioned in Section 4.3, the learning result of a logistic regression analysis is a linear 
model.  The model is used to calculate the probability that a case has a positive value on the 
target variable.  To apply the model in a classification task, a cut point must be chosen.  
Cases with probability bigger than the cut point are classified as positive, while cases with 
probability lower than the cut point are classified as negative.  The model and a cut point 
together make up a classifier.  Each linear model we learned represents a family of 
classifiers.  With the Logistic and SimpleLogistic methods, we build two logistic models for 
each qualitative property.  Using 0.5 as the cut point, we calculate the predictive accuracies 
of the 12 classifiers on the testing data set (Table 8.4).   
   
 
66
In all cases, about 60% of cases in the testing dataset are correctly classified, which is 
better than random guessing.  If we use a baseline of always guessing the most populous 
class, we see that the accuracy performance on “Accuracy”, “Objectivity” and “Conciseness” 
task is just barely above this baseline.  “Multi-views” and “Depth” are the top two properties, 
followed by “Reliability”.  The SimpleLogistic classifier of each qualitative property has a 
slightly higher accuracy than the corresponding Logistic classifier, except for “Multi-views”.  
However, the differences are very small.   
 
Logistic SimpleLogistic Classificatio
n Tasks 
Accuracy 
Baseline Accuracy AUC Accuracy AUC Number of Features 
Accuracy 55.9% 57% .59 59.5% .59 14 
Reliability 51.6% 57% .61 58.7% .62 38 
Objectivity 57.5% 60% .62 62.6% .63 39 
Depth 55.0% 61% .64 63.7% .66 12 
Conciseness 57.9% 60% .61 60.8% .62 27 
Multi-views 50.7% 65% .69 61.6% .68 12 
 
Table 8.4.  Performance of two logistic regression methods on the median-based binary datasets 
 
Predictive accuracy only represents the performance of a model at a specific cut 
point.  To examine the performance of the models, we use ROC analysis.  If we sort the 
probability scores in descending order, and use each unique score as a cut point, we can plot 
a point in the ROC space to represent the corresponding classifier, showing the detection rate 
   
 
67
and false alarm rate at the level of probability.  The curve that connects all such points is the 
ROC curve which represents the model.   
In  
Figure 8.1, we plot the ROC curves, for class “High”, for the six models learned 
using SimpleLogistic method.  Every point along the curves represents a possible cut point to 
discriminate between documents with “high” and “low” value on the property.  Taking 
“Depth” as an example, the detection rate of a point is the ratio of the number of “high-
Depth” documents that would be correctly classified (using the corresponding threshold) to 
the total number of “high-Depth” documents. The associated false alarm rate of a point is the 
ratio of the number of “low-Depth” documents that would be incorrectly classified as “high-
Depth” (using that cutoff) to the total number of “low-Depth” documents.   
 
 
 
 
 
 
 
 
 
 
Figure 8.1.  ROC curves of the SimpleLogistic method on the testing data sets.  Models are 
trained on the median-based binary datasets.   
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
False Alarm Rate
D
e
te
c
ti
o
n
 R
a
te
Accuracy
 Reliability
Objectivity
Depth
Conciseness
Multi-views
   
 
68
 
The closer an ROC curve comes to the ideal point (100% detection and 0% false 
alarm rate), the better the classification model.  All of the ROC curves in Figure 8.1 rise in a 
concave fashion, indicating our classifiers perform better than random assignments.    
The curves in Figure 8.1 are for the “High” class.  For binary classification, we can 
examine the classification power for the “Low” class from the same curve with point (1, 1) as 
the origin point and the x-axis representing the detection rate.  The ROC curves show that the 
classification powers of SimpleLogistic method on all properties are quite similar at the two 
ends of the curves.  It means that the models have similar power in identifying both classes.  
The ROC curves show clearly the differences among the properties vary.  At most of 
the region, the curves of “Multi-views” and “Depth” are above the others.  The detection 
rates of these two properties are about 10% higher than that of others, when the false alarm 
rate is in the range between 0.3 and 0.5.  
In ROC analysis, the area under curve (AUC) is used as a single summary measure of 
a model’s performance.  The AUCs of the generated models are calculated and listed in 
Table 8.4.  It is not surprising that the AUC measures are basically consistent with the 
accuracies because of the nearly balanced corpus.   
The AUCs and accuracies all show that the two implementations of logistic 
regression method in the Weka package have very similar performance.  We present the pair 
of ROC curves of the property with best performance, “Multi-views”, in Figure 8.2, for the 
purpose of comparison.   
If the ROC curve of one model lies above that of another, the method corresponding 
to the higher curve is preferred, no matter what cutoff point is the preference of the user.  In 
   
 
69
Figure 8.2 (next page), the two curves are nearly identical.  We observed similar results for 
other qualitative properties. 
The two logistic methods have nearly identical performance on our classification 
tasks.  However, the SimpleLogistic method generates models with many fewer features.  
This means that not every feature in our sets is equally important in discriminating, texts on 
the six dimensions.  The SimpleLogistic method’s capability of selecting powerful features is 
valuable and we decide to use SimpleLogistic as our logistic regression method in the rest of 
the study. 
 
 
 
 
 
 
 
 
 
 
Figure 8.2: ROC curves for the testing data set of two methods: (1) logistic regression; (2) 
SimpleLogistic with feature selection (Predicted variable: Median-based binary “Multi-
views” score). 
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
False Alarm Rate
D
e
te
c
ti
o
n
 R
a
te
Logistic 
SimpleLogistic
   
 
70
8.2.2 On Scale-Based Binary Datasets 
The results of SimpleLogistic method for three scale-based binary datasets are summarized in 
Table 8.5.  It includes accuracy scores and AUC scores on the testing datasets.   
 
Dataset Measures  Accuracy Reliability Objectivity Depth  Conciseness Multi-
views 
Baseline 80% 72% 64% 51% 68% 66% 
Accuracy 79% 71% 68% 63% 69% 67% 
Normal 
AUC .58 .62 .65 .66 .65 .66 
Baseline 55% 62% 67% 78% 68% 84% 
Accuracy 59% 63% 69% 78% 68% 84% 
Extreme 
High 
AUC .62 .62 .64 - .62 - 
Baseline 94% 88% 81% 72% 90% 55% 
Accuracy 94% 88% 81% 71% 89% 62% 
Extreme 
Low 
AUC .61  - .68 .68 .62 .67 
Table 8.5:  Performance of the SimpleLogistic method on the testing data of three scale-based binary 
datasets:  normal binary datasets; extreme high binary datasets; and extreme low binary datasets. 
 
We found that, in three cases, the learning method gave trivial classifiers, which are 
highlighted in Table 8.5.  On the extreme high “Depth” dataset, the classifier simply put all 
documents in the “low-Depth” category.  The learning method returns the same trivial 
classifier on the extreme high “Multi-views” dataset; all documents are classified as “One-
sided”.  On the opposite, the classifier trained on the extreme low “Reliability” dataset labels 
all documents as “Reliable”.  In addition to these failures, when compared with the baseline 
accuracy of always assigning the most frequent class, the predictive accuracy scores are even 
smaller in four cases.  For example, on the normal “Reliability” dataset, the predictive 
accuracy score is 71%, while a trivial classifier that labels all documents as “High-
Reliability” class gives 72% accuracy.    
   
 
71
In Figure 8.3, which shows the ROC curves on the testing data of the normal datasets, 
we can see that the ROC curves are similar to the curves of the median-based datasets 
(Figure 8.1).  “Depth”, with the largest difference from the baseline performance, appears a 
little easier to predict than others. 
   
 
 
 
 
 
 
 
 
 
 
Figure 8.3: ROC curves of SimpleLogistic on the testing datasets.  Models are trained on the 
normal scale-based binary datasets.   
 
8.2.3 Comparison of Classification Results on Four Sets of Binary Datasets 
The performance on the median-based binary datasets is slightly better on most of the 
properties than that on the other three datasets.  More importantly, it never produces trivial 
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
False Alarm Rate
D
e
te
c
ti
o
n
 R
a
te
Accuracy
Reliability
Objectivity
Depth
Conciseness
Multi-views
   
 
72
classifiers.  In the following binary classification experiments, unless clearly specified, we 
use the median based binary datasets.      
Even though the overall performance of the models, no matter which datasets they are 
learned from, is not very satisfying, these results, obtained without any extra effort to tune or 
optimize feature sets, leave us optimistic about being able to automatically classify 
documents on these six property dimensions.  One possible way to improve the classification 
performance is to enforce the feature selection method.   
8.2.4 Learning Models with Powerful and Stable Features 
Our goal is to build a set of classifiers with powerful and stable features.  The default feature 
selection mechanism in the learning method may return different sets of significant features 
from different training datasets.  To check the stability of the models learned by the logistic 
regression method, we created 9 more training sets by randomly selecting half of the cases 
from corpus A.  We ran the SimpleLogistic method on each training set.  Including the set of 
models we had already learned, we obtain 10 models for each qualitative property.   
On examining the features kept in the 10 models, for each property, we found that 
there is a great deal of variance.  Taking “Accuracy” as an example, a total of 72 unique 
features appear in the 10 models.  Only two of these show up in all 10 models.  27 features, 
37.5% of the whole set, show up in only one model each.  These less frequent features 
represent the unique patterns in the training datasets from which the models containing these 
features are created.  They are not the characteristic of the whole corpus.  These features are 
unstable.  Their appearance depends on the particular way the training dataset is selected.  To 
   
 
73
build a general model for each qualitative property (at least for the current corpus), we should 
avoid such unstable features.   
We pooled the 10 feature lists of each property together and determined the frequency 
of each features in the pool.  The biggest possible value is 10, which means that the feature 
was selected by the SimpleLogistic regression analyses on all 10 training datasets.  After 
sorting the features in descending order by their frequencies, we see the most stable variables 
for each property at the top of the list.  We have used an ad-hoc threshold of 4 - the features 
have to show up in at least 4 out of the 10 models.   
We then ran the SimpleLogistic analysis for each property using only selected stable 
features.  The results are measured using 10-fold cross validation.  We show ROC curves of 
the 10-fold validation, for all six properties, in Figure 8.4 (next page).   It is very much like 
Figure 8.1.   
It is clear that SimpleLogistic regression performs best on “Multi-views” and “Depth” 
properties.  The models built with only stable features achieve a detection rate of 60% at a 
false alarm rate about 33%.  Table 8.6 shows that there is no loss on AUCs for most of the 
classification tasks, compared to the results of the unstable models (the AUC scores are 
copied from Table 8.1 and included in the last column). 
 
Qualitative 
Property 
Number of 
Features 
Predictive 
Accuracy  
AUC AUC of 
Unstable 
Models 
Accuracy 10 61.1% .64 .59 
Reliability  20 60.5% .63 .62 
Objectivity   11 63.7% .66 .63 
Depth    5 64.3% .68 .66 
   
 
74
conciseness 14 62.3% .64 .62 
Multi-views  24 63.5% .69 .68 
 
Table 8.6:  Number of stable features and the area under curve for 10-fold cross 
validation using SimpleLogistic method over the whole corpus. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 8.4.  ROC curves of the SimpleLogistic method with voted stable feature sets for 10-
fold cross validation in the HIGH vs. LOW classification task.      
 
The result shows that there is nearly no loss using the stable variables compared to 
using Statistical Best variables.  This means that we can find a stable group of predictors 
even though the Statistical Best variables are quite varied among the 10 cases.  And on 
average, the stable variables work as well as each individual Statistical Best variable group. 
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
False Alarm Rate
D
e
te
c
ti
o
n
 R
a
te
Accuracy
Reliability
Objectivity
Depth
Conciseness
Multi-views
   
 
75
8.3 Decision Tree Learning (C4.5) 
We used the implementation of the C4.5 decision tree algorithm in the WEKA package 
(weka.classifiers.trees.J48) to conduct all the classification experiments in this section.   
8.3.1 Decision Tree with All Features 
In this set of experiments, we first apply the C4.5 decision tree classifier on 10 half-half 
training and test sets.  The classifiers include all language features.  The dependent variables 
are the median-based variables.   
Table 8.7 shows the average accuracies and the sizes of trees of the classification 
tasks on six properties.  The size of a tree is the number of nodes in the tree.  With similar 
accuracy, the smaller the tree size, the better the performance. 
 
Task Accuracy Size 
Accuracy 54.3% 471 
Reliability 52.3% 451 
Objectivity 56.3% 409 
Depth 56.3% 433 
Conciseness 55.2% 449 
Multi-views 56.2% 409 
Table 8.7: Decision tree C4.5 classification performance 
on testing date set, including all features  
 
We can see that the decision trees fit poorly on the testing data set.  The average 
accuracies are only about 55%.  However, we learned from the logistic regression 
   
 
76
experiments that the features kept in the classifiers are not stable, but are training corpus 
dependent.  We speculate that this might also happen in decision tree learning.  To reduce 
possible over-fitting and instability, we used a similar “voting mechanism” as we applied to 
logistic regression, to obtain a stable set of features for each property.   
8.3.2 Learning with Stable Feature Set 
We examine the 10 trees generated from full set of features.  In a C4.5 decision tree, the 
higher the level of a feature in the tree, the more informative the corresponding feature with 
respect to discriminating between the target classes.  Therefore, we decided to do voting 
based on the levels at which the features are.  We need two thresholds in this feature 
selection process.  First, we must decide how deep we want to go down the tree to pick 
features.  And, second, we need to decide how often a feature must appear in the 10 trees.  
We choose the pair (4, 4) as our thresholds.  Language features appearing in the top four 
levels of at least four trees are chosen as stable features.  The number of features for each 
property is listed in the second column of Table 8.8. 
A 10-fold cross validation learning was conducted with the set of stable language 
features for each qualitative property separately.  The performance is summarized in Table 
8.8.   
 
Task 
Number of 
Stable 
Features 
Size of 
the tree AUC Accuracy 
Accuracy 19 735 .55 56.6% 
Reliability 15 755 .55 54.1% 
Objectivity 15 569 .57 58.9% 
   
 
77
Depth 26 839 .56 57.4% 
Conciseness 12 183 .53 56.9% 
Multi-views 26 873 .56 56.2% 
Table 8.8: Classification performance of decision tree learning, using the stable 
feature sets. 
 
The second column is the number of features used.  We include three descriptors of 
the tree models: the size of the tree, the area under the ROC curve, and the accuracy.  As we 
can see, with about 20% of the features, the predictive accuracies of C4.5 with selected 
features are actually slightly higher than that with all features on all six binary classification 
tasks.  The differences are about 2%.        
To make a direct comparison to the logistic regression results, we plot the ROC 
curves for each decision tree classifier in Figure 8.5.   
 
 
 
 
 
 
 
 
 
 
Figure 8.5:  ROC curves for six qualitative properties, using C4.5 decision tree 
method and the stable sets of features.  
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
False Alarm Rate
D
e
te
ct
io
n
 R
at
e
Accuracy
Reliability
Objectivity
Depth
Conciseness
Multi-views
   
 
78
 
The ROC curves show the poor performance of decision tree method for classifying 
documents in the median-based binary classes.  Logistic regression has about 10% higher 
AUC scores than C4.5.  By comparing Figure 8.4 and Figure 8.5, it is clear that logistic 
regression is better than decision tree on classifying all six qualitative properties.  For 
example, at a 30% false alarm rate, the detection rate of the logistic regression model for 
“Depth” is about 60%, which is 50% higher than the 40% detection rate of the decision tree 
model. 
8.3.3 Pruning the Trees 
Examining Table 8.7 and Table 8.8 carefully, we observe that even with smaller feature set, 
the size of trees does not decrease.  On the opposite, the size increases tremendously.  This is 
caused by over-fitting of the training data set.  
There are two solutions for an over-fitting problem.  One is to stop the tree early, 
before it begins to overfit the data.  However, this is not practical because it is not clear what 
a good stopping point is.  The second method allows over-fitting during training, and then 
prunes the tree on the testing data set to decide best final tree.  The second method is more 
popular in the machine learning community.   
Using the default reduced error pruning option in the WEKA J48 classifier, the 
classification results are summarized in Table 8.9.  
 
Task 
Size of 
the tree 
Accuracy 
   
 
79
Accuracy 263 56.6% 
Reliability 321 54.8% 
Objectivity 259 59.9% 
Depth 271 62.0% 
Conciseness 165 59.3% 
Multi-views 193 58.3% 
 
Table 8.9: Classification performance after tree pruning 
 
 The size of trees decreases nearly two thirds, while the accuracies on testing data set 
increase or stay the same.   
Since the differences in accuracies are not large, it is not surprising that the ROC 
curves of the pruned trees are very similar to those of the original trees in Figure 8.5.   
 
 
 
 
 
 
 
 
 
 
Figure 8.6:  ROC curves of original and pruned decision tree classifiers for “Multi-
views” property 
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
False Alarm Rate
D
e
te
c
ti
o
n
 R
a
te
Pruned Tree
Original Tree
   
 
80
 
We only present the ROC curves on the “Multi-views” property as an example.  The 
area under the pruned curve is .59, which indicates a slightly better performance than the 
original tree.  In Figure 8.6, the pruned ROC curve is always above the original tree curve, 
although the amount of improvement at different cut points varies.  Pruning improves 
performance more in identifying one-sided documents than in “Multi-views”. 
8.4 SVM Learning 
We applied SMO, a support vector machine (SVM) learning algorithm, implemented in the 
WEKA package.  We used a linear kernel, which gives a model linearly weighting the 
various language features.  To obtain proper probability estimates of the hyperplane target 
function, for the purpose of ROC analysis, we use the option that fits logistic regression 
models to the outputs of the support vector machine.  All other options are kept at default 
values.  The experimental procedures are the same as discussed earlier. 
The average predictive accuracies of SVM on the testing sets of the 6 median-based 
binary datasets with all language features are listed in the first column of Table 8.10.   
 
Accuracy 
Task All 
Features
Selected 
Features 
AUC  
with selected 
features 
Number of 
selected 
features 
Accuracy 60.6% 62.6% .64 25 
Reliability 58.8% 59.2% .63 26 
Objectivity 62.1% 63.1% .67 12 
   
 
81
Depth 62.9% 64.7% .69 25 
Conciseness 61.4% 62.8% .63 20 
Multi-views 62.2% 64.3% .70 28 
 
Table 8.10: Classification performance of SVM on median based binary corpora 
  
To identify a stable set of features for each qualitative property, we rank the features 
by the absolute values of their weighting parameters in the function.  The higher the rank, the 
more important the feature.  We chose a value of 0.8 as the cut point, which gives about 20 
powerful features for each hyperplane function.  Then we examine the features appearance in 
the 10 sets of powerful features.  Only those that show up in more than 5 functions are kept 
in the final selected feature set.   The general accuracy of the SMO method with selected 
features is measured using 10-fold cross validation (third column in Table 8.10). 
As we can see, the average accuracy of SVM method is about 60% on all six 
classification tasks.  The predictive accuracies of SVM with selected features are slightly 
higher than with all features.   
The ROC curves of the SVM method are plotted in Figure 8.7.  They are very similar 
to the curves of the logistic regression method.  
 
 
  
 
 
   
 
82
 
 
 
 
 
 
 
 
 
 
Figure 8.7: ROC curves of SVM on six median-based binary datasets with selected features (10-
fold cross validation) 
8.5 Models Generated 
We can examine the models generated by the SimpleLogistic, decision tree C4.5 and SVM 
methods to see what features the classifier is using to make its prediction on each task.  This 
may give us some insight into the classification task, and give us some confidence in the 
models being generated.   If the model gives us rules that seem intuitively related to the 
classification tasks on non-topical qualitative properties, this will give us confidence in the 
validity of the model.  The model may also give us insight into the specific property under 
investigation.   
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
False Alarm Rate
D
e
te
c
ti
o
n
 R
a
te
Accuracy
 Reliability
Objectivity
Depth
Conciseness
Multi-views
   
 
83
 
Task Root Feature 
Accuracy Density of noun 
Reliability Frequency of word “seem” 
Objectivity Frequency of word “seem” 
Depth Frequency of singular noun 
Conciseness Frequency of singular noun 
Multi-views Frequency of money 
 
Table 8.11: Root features of the decision tree C4.5 
models. (Learning method: J48; Feature sets: the tree 
voted stable feature sets; training corpus: the full corpus) 
 
Table 8.11 lists the root features of the six decision trees.  The root node of a C4.5 
decision tree is the feature that has the most discriminating power.  For the “Reliability” 
classification tasks, the root node of the tree is the frequency of the word “seem” in the text.  
It is easy to understand that highly reliable documents tend to have less appearance of the 
word “seem” in the document.  The root node of the “Objectivity” tree is also the frequency 
of the word “seem”.  Documents without the word “seem” are more likely to be objective.  
The “Depth” tree has the frequency of singular nouns (NN) as root.  This makes sense, as 
high “Depth” documents might tend to mention more concepts.    The number of nouns is 
also the root feature of the “Conciseness” tree.  Concise documents tend to have fewer nouns.  
The “Multi-views” tree has the feature “MONEY” as its root.  It does not make intuitive 
sense.  However the second level feature, the frequency of unique personal names, does.  It 
seems likely that a “Multi-views” document often references more persons.  Finally, the 
   
 
84
density of nouns is the root of the accuracy tree.  It is not get clear how the density of nouns 
is indicative of the accuracy of a document.    
The weight parameter of a feature in a logistic regression or SVM model defines the 
effect of the feature on the qualitative property under study.  A feature can have either 
positive or negative effect in the model.  Those features with negative sign have negative 
contribution in identifying a document as a positive case.  We found that the logistic 
regression and the SVM models share a great proportion of features.  These features are 
highlighted in Table 8.12.   
Even though the methods do not perform very well, we find that there are some 
intuitively meaningful features in the models.   
It seems that “Accurate” documents tend to contain more occurrences of “say’ and 
“expert”.  The frequency of regional locations, organizational words, like “bank”, “agency”, 
and the density of date expression also indicate high “Accuracy”.  The presence of the words 
“say”, “expert” is strongly indicative of a document being “Objective”.  One can imagine that 
a document containing many citations from experts is more likely to be “Objective”.  There 
are four common features for “Depth”: the frequency of unique words (type), of possessive 
pronouns, of regional location expressions, and of date units.  It is easy to see that a detailed 
news article contains more regional location expressions.  A document containing more 
unique words is likely to be in high “Depth”, as it probably requires more words to describe 
the event.  It is not clear why fewer possessive pronouns and data unit expressions indicate 
higher “Depth”.  “Concise” documents are more likely to have shorter sentences.  “Multi-
views” documents mention more different persons, and not too much about one single 
person, on average (R_PERSON).   The presence of word “seem” is a strong indication of 
   
 
85
“Subjectivity”, presumably expressing the authors’ perception.  “Seem” is also a negative 
indicator of a document’s “Reliability”, while the presence of the word “expert” is a positive 
indicator.   
Qualitative 
Property 
Features in Logistic 
Regression Model 
Features in SVM models 
Accuracy SAY, LOC_CITY  (-), LOC_REG, 
EXPERT, UH (-), ORG_BASE, 
D_DATE, D_VERB, D_NOUN  
CDG  (-), 
SAY, LOC_CITY  (-), LOC_REG, 
EXPERT, UH (-), ORG_BASE, 
D_DATE, D_VERB, D_NOUN  
VBZ(-),MONEY, MD(-), 
IDENT_KE(-), D_UNIQ, CD, VBP, 
PERSON, NUM_FORW(-), 
COMMA 
Reliability AVG_PARA, AVG_SENT (-),  
JOBTITLE (-), PERS_FUL, CDG  
(-),VBG, LOC_REG, D_BACK (-), 
EXPERT, R_PERSON (-),D_SAY, 
D_DATE, D_NOUN 
IDENT_KE  (-), JJR (-), DEC_MIPL 
(-), SYM (-), SEEM  (-), LOC_PRO 
(-), ADJ_NOUN 
AVG_PARA, AVG_SENT (-),  
JOBTITLE (-), PERS_FUL, CDG  
(-),VBG, LOC_REG, D_BACK (-), 
EXPERT, R_PERSON (-),D_SAY, 
D_DATE, D_NOUN 
COLON, QUOTELEN, VBN(-), 
DEC_MI(-), WRB, VB, IN, 
ORG_GOV(-), PRP(-), WDT, 
D_ORG, D_MONEY 
Objectivity AVG_PARA (-) , AVG_SENT  (-), 
SAY, PDT  (-), EXPERT, WDT, 
D_DATE, D_NOUN 
PRP$  (-), PERS_FIR  (-),SEEM (-),  
AVG_PARA (-) , AVG_SENT  (-), 
SAY, PDT  (-), EXPERT, WDT, 
D_DATE, D_NOUN 
IDENT_KE(-), VBG, DT(-), STOP(-
) 
Depth UNIQWORD, PRP$(-), 
LOC_REG, DATE_KEY(-), 
 YEAR 
D_UNIQ, PRP$(-), LOC_REG, 
DATE_KEY(-), 
PERIOD(-), SAY, JOBTITLE(-), 
PERS_FUL, DATE_U, POS, 
ORG_DEPT, RB(-), JJ(-), 
PERSON_U(-), D_FORW, CD, CC(-
), MARK#(-), ORG_BASE, 
R_PERSON(-), R_ORG, R_DATE, 
R_ADJ 
Conciseness AVG_SENT (-),VBP, NNPS, 
DATE_FES, DATE_UNI,   
D_SAY, RBS, LOC_KEY, IN (-), 
ORG_COM(-), SEEM (-), 
ORG_BASE, D_DECL, D_DATE  
AVG_SENT (-), NNPS, 
DATE_FES, DATE_UNI,   
AVG_WORD(-), IDENT_KE(-), 
ORG_DEPT, TIT_PLC, NNS, 
NUM_FORW, D_BACK, YEAR 
Multi-views UNIQWORD, D_SAY, 
DATE_UNI, NUM_BACK, 
IDENT_KE (-), LOC_COUN(-), 
NUM_FORW, STOP (-), CC (-), 
D_MONEY, R_PERSON (-), 
D_UNIQ, SAY, DATE_UNI, 
NUM_BACK, IDENT_KE (-), 
LOC_COUN(-), NUM_FORW, 
STOP (-), CC (-), MONEY, 
R_PERSON (-), R_DATE, 
   
 
86
R_DATE, D_EXPERT, D_LOC, 
D_VERB, POS, 
JOBTITLE, CURRENCY, RBS, 
DEC_MIPL, , NNS, WP(-),YEAR, 
PERSON_U, MARK$,  
D_EXPERT, D_LOC, D_VERB 
POS, 
PERIOD(-), AVG_SENT(-), CDG(-
), DATE_U(-), VBG, VBD(-)VB(-), 
CD, RIGHT_P(-). D_NOUN 
 
Table 8.12:  Features of the logistic regression and SVM models (Learning method: 
SimpleLogistic and SMO; Feature sets: the voted stable feature sets; training corpus: the full 
corpus) 
8.6 Summary 
The broad goal of this chapter’s analyses is to explore the possibility of automatically 
classifying documents into two categories on six non-topical dimensions.  The results show 
that our methods do not perform very well.  However, we draw the following conclusions: 
Regarding the classification tasks.  We tested two types of classification approaches 
in the previous two chapters: linear prediction and binary classification.  The linear 
regression analyses showed that it is hard to model the 10-point scale.  For all six properties, 
we achieved better than baseline performance in binary classification.  It is known that it is 
cognitively difficult for a person in making meaningful differentiations between certain 
values in more than seven point scales.  We believe that the human judgments, based on a 
scale of 10 points, contain a great deal of noise.  The noise not only affects the linear 
regression results directly, it also affects the binary classification problem.  For any 
dichotomization method, there has to be an arbitrary cut point, and similar documents around 
the point are labeled as examples for two opposite categories.   The destructive potential of 
random classification of median cases, as we mentioned above, makes the situation even 
worse.  We examined the distribution of the original score.  The documents assigned to the 
   
 
87
median scores account for 10% to 20% of the corpus.  Taking “Objectivity” as an example, 
the percentage is 18.8%.  It means that the learning method has to find pattern from a dataset 
in which nearly one fifth of the cases are in fact randomly assigned as positive or negative.  
We hypothesize that it would be easier to differentiate between “exemplars” (documents at 
the two ends of the scale) for a person, and for a machine algorithm.  Therefore, we will test 
the learning methods’ ability to classify documents that were judged extremely high or 
extremely low on each property dimension.     
The difficulty of the classification task on the six dimensions varies.  It seems 
relatively easier to classify documents on “Depth” and “Multi-views” dimensions than on 
other dimensions. 
Regarding the language features.  Our experiments show that not all candidate 
language features are equally important to the classification tasks for these six qualitative 
properties.  The learning methods can reduce the number of features from more than 100 to 
about 20 without damaging the classification performance.  Meanwhile, we learned that the 
features obtained from any one training set are unstable.  Our voting methods work well in 
identifying stable features across multiple training sets.  We will apply these methods in the 
future analyses.  Finally, we found that the learning methods can identify some intuitively 
meaningful indicators for each qualitative property. 
Regarding the learning methods. Linear regression performs a different type of 
task.  It is not directly comparable to other methods.   As for the binary classification, we 
compare the predictive accuracy of Logistic Regression, C4.5 and SVM together (from Table 
8.6, Table 8.9 and Table 8.10, respectively) in Table 8.13.   
 
   
 
88
Task Accuracy 
Baseline 
Logistic 
Regression 
C4.5 SVM 
Accuracy 55.9% 61.1% 56.6% 62.6% 
Reliability 51.6% 60.5% 54.8% 59.2% 
Objectivity 57.5% 63.7% 59.9% 63.1% 
Depth 55.0% 64.3% 62.0% 64.7% 
Conciseness 57.9% 62.3% 59.3% 62.8% 
Multi-views 50.7% 63.5% 58.3% 64.3% 
Table 8.13:  Comparison of classification accuracy across three learning methods 
on median-based binary datasets 
 
Taking “Multi-views” as an example, the predictive accuracies of the three methods 
are all far above the baseline performance of 50.7% accuracy.  The accuracies of Logistic 
regression and SVM are higher than that of decision tree C4.5 by about 5%.  The predictive 
accuracies of logistic regression and SVM are very similar.  From the results of ROC 
analysis, we draw the same conclusion. The AUCs of logistic regression and SVM are very 
similar.  Logistic regression and SVM have substantially higher AUCs than C4.5, with 
differences about 10%.   
 
   
 
89
Chapter 9: Classification of Document Exemplars 
One possible cause of our failure in macro level binary classifications over the combined 
corpus is the arbitrary dichotomization we had to make.  Originally, the users were asked to 
make judgments using a 10-point scale, rather than giving their binary opinions on the 
properties.  Without sacrificing any cases, there will be an arbitrary cut point no matter what 
dichotomization method we used.  The dichotomization process contains within itself two 
potential limitations.  First, documents with same scores or adjacent scores have to be labeled 
to represent either of the two opposite classes on one property.  For example, a document 
with a score of 5 might have much more similarity than difference when compared to a 
document with a score of 6 on the qualitative property.  Secondly, the approach ignores the 
meaning of the scores in the middle of the scale.  A score of 5 might have been assigned 
when a document is neither objective nor subjective, or when a judge could not make a 
definite decision.  However, without a third choice, such documents can be in only one of the 
two classes: either subjective or objective.      
We observe that a large proportion of the cases fall in such ambiguous area around 
the cut points in our corpora.  We speculate that the existence of these ambiguous cases and 
the fact that we push them to the two extreme ends of a dimension arbitrarily degrades the 
performance of our binary classifiers.  It might be an easier task to distinguish between those 
   
 
90
cases that fall at the far ends of each dimension.  They are the real representatives or 
“exemplars” of the two classes, for each non-topical property.   
In this chapter, we create two exemplar corpora.  The three classification techniques, 
together with the corresponding feature selecting methods, are applied to these exemplar 
corpora. 
9.1 Exemplar Corpora 
To create exemplary datasets for each property, we rank all the documents in the 
combined corpus on their 10-point Likert scores for the corresponding property.  Two new 
corpora are generated by selecting about equal numbers of cases from each end of the list 
without any partial inclusion of cases with the same scores.  In the first set of corpora, we try 
to keep around 500 instances for each class, with the exception of Depth, which has more 
extreme cases.  The second set of corpora is much more aggressive than the first one.  We try 
to include as many cases as possible.  Basically, we just remove the cases around the median 
score.  The details of the collection for each property are listed in Table 9.1.  The range in the 
parentheses is the original scores. 
 
Extreme Corpus I Extreme Corpus II 
Task 
High  Low  High  Low  
Accuracy 478 (9-10) 352 (1-4) 1020 (8-10) 1405 (1-6) 
Reliability 486 (9-10) 566 (1-4) 1263 (8-10) 1153 (1-6) 
Objectivity 386 (9-10) 596 (1-3) 1049 (8-10) 1158 (1-5) 
Depth 708 (8-10) 907 (1-3) 1211 (7-10) 1266 (1-4) 
   
 
91
Conciseness 471 (9-10) 619 (1-4) 1034 (8-10) 1017 (1-5) 
Multi-views 505 (8-10) 369 (1) 1379 (5-10) 1422 (1-3) 
Table 9.1:  Distributions of extreme high/low cases.  
 
 
For each property and each extreme corpus, we constructed ten training datasets from 
the complete set of extreme documents for training the feature classifier.  Each training 
dataset contains about 50% of the documents in the extreme corpus.    
9.2 Logistic Regression 
9.2.1 Over Extreme Corpus I 
The WEKA SimpleLogistic method is first run over the 10 extreme I training sets, which 
generates 10 logistic regression functions.  After pooling the features in the 10 functions 
together, we found that the variance still exists.  However, the stability of the feature set 
improves compared to that of the logistic regression on the combined corpus.  Taking 
“Accuracy” again as an example, with a total of 74 features in the pool, 16 features show up 
in at least 5 functions.  Compared to 7 such features out of the 72 features in the models over 
the combined corpus, it is a 50% improvement.  It implies that the noise of the data is 
reduced, as we expected.  For our feature voting method, we used a threshold of 5, which 
means the features have to shown up in at least 5 functions.     
Table 9.2 summarizes the results of learning with all features, as well as with selected 
features.  Column two refers to the accuracy baseline of always choosing the most frequent 
category.  The accuracy scores of all features are the average across the 10 pairs of training 
   
 
92
and testing.  The accuracy scores of selected features are the results of 10-fold cross 
validation.  It shows that no matter which feature set is used, the accuracies of all of the six 
classification tasks are significantly better than the baseline. 
For the purpose of comparison, we include the AUCs of the binary classification 
results over the full combined corpus in the last column of Table 9.2.  There is more than 
10% improvement on five classification tasks.  The “Depth” classification task has the 
smallest increase of 9%.  The biggest improvement is on “Multi-views”, which is 14% higher 
than the performance over the combined corpus.  For all classification tasks, the logistic 
regression models achieve AUC about 75%.  This is a good level of accuracy for our 
classification tasks. 
 
Extreme Corpus I 
All 
Features Selected Features 
Qualitative 
Property 
Accuracy 
Baseline 
Average 
Accuracy 
Predictive 
Accuracy AUC 
Combined 
Corpus 
AUC 
Baseline 
Accuracy 57.6% 70.3% 71.9% .76 .64 
Reliability  53.8% 67.3% 67.9% .74 .63 
Objectivity   60.7% 68.4% 69.5% .78 .66 
Depth    56.2% 68.9% 71.1% .78 .68 
Conciseness 56.8% 68.0% 69.4% .74 .64 
Multi-views  57.8% 72.9% 76.2% .83 .69 
 
Table 9.2:  Number of stable features and the area under curve (AUC) for 10-fold cross validation 
using SimpleLogistic method over the extreme corpora I. 
 
The ROC curves (Figure 9.1) illustrate the improvement more clearly.  The solid lines 
represent the ROC curves of classification tasks on the extreme corpus I.  The dashed lines 
   
 
93
represent the curves on the combined corpus.  For all of our six classification tasks, it is very 
clear that the extreme curve runs far above the combined corpus curve.   
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 9.1: ROC curves of SimpleLogistic on the extreme corpora I and the combined 
corpus. 
 
The logistic regression method seems to be quite powerful when attempting to 
classify documents as “One-sided” or having “Multiple viewpoints”.  When the detection 
rate rises to 80%, the false alarm rate is just about 32%, less than half of the detection rate.  
This is the point corresponding to the classifier using the prior probability as the cut point.   
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
False Alarm Rate
D
e
te
c
ti
o
n
 R
a
te
Accuracy
Reliability
Objectivity
Depth
Conciseness
Multi-views
Accuracy-all cases
Source Reliability - all cases
Objectivity- all cases
Depth - all cases
Conciseness - all cases
Multi-views - all cases
   
 
94
The classifier using probability 0.5 as the cut point gives the point (.76, .27).  Sorting 
documents by their probability of containing multiple viewpoints, there are only 4 in the top 
100 document exemplars that are observed as one-sided documents.  At the other end of the 
sorted list, the first 100 documents contain 83 true one-sided documents.   
9.2.2 Over Extreme Corpus II 
We repeat the analyses over Extreme Corpus II.  The results are summarized in Table 9.3.  
The accuracy scores are better than those of the baseline.  However, the superiority is not as 
obvious as the results on Extreme Corpus I. 
 
All 
Features 
Selected Features 
Qualitative 
Property 
Accuracy 
Baseline Average 
Accuracy 
Number of 
Features 
Accuracy AUC 
Accuracy 57.9% 63.6% 7 65.0% .68 
Reliability  52.3% 61.1% 15 61.4% .67 
Objectivity   52.5% 65.1% 12 65.8% .73 
Depth    51.1% 65.8% 7 66.3% .72 
Conciseness 50.4% 63.7% 11 64.8% .70 
Multi-views  50.8% 63.7% 20 65.7% .72 
 
Table 9.3:  Number of stable features and the area under curve (AUC) for 10-fold cross 
validation using SimpleLogistic method over the extreme corpora II 
 
The results show some improvements compared with that over the full combined 
corpus (about 5% increases on AUC).  However, on average, the AUC is about 5% lower 
than that over extreme corpora I.  The improvement indicates that the learning methods can 
   
 
95
do a better job without those median cases.  As expected, the results show that the difficulty 
of classification increases as the number of ambiguous documents increases in the dataset.   
9.3 Decision Tree Method (C4.5) 
The results of the decision tree method over the exemplar corpora are not as promising as 
those of SimpleLogistic.  The table and figure below show the results of the decision tree 
approach for the extreme classification tasks.  Since the AUC on the Extreme Corpora II is in 
between that on Extreme Corpora I and the full corpus, we decide not to do ROC analysis 
over Extreme Corpora II.  The decision tree gives best results on the “Depth” and “Multi-
views” classification tasks.    
 
Extreme Corpora I Extreme Corpora II 
All 
Features Selected Features 
All 
Features 
Selected 
Features 
Qualitative 
Property 
Accuracy Accuracy AUC Accuracy Accuracy 
Accuracy 60.1 69.8 .65 56.7 60.5 
Reliability  58.5 63.4 .58 55.3 57.5 
Objectivity   61.6 66.5 .65 58.6 62.4 
Depth    62.6 69.1 .70 58.5 66.8 
Conciseness 61.3 66.3 .64 56.3 62.0 
Multi-views  67.9 70.0 .70 58.2 61.8 
 
Table 9.4: Accuracy and the area under curve (AUC) of decision tree C4.5 method over the 
extreme corpus (10-fold cross validation).  
 
 
   
 
96
 
 
 
 
 
 
 
 
 
 
 
Figure 9.2:  ROC curves of decision tree C4.5 on Extreme I Corpora with selected 
features (10-fold cross validation).   
 
We again observe the phenomenon that the selected features perform better than all 
features on both sets of extreme corpora with an average 5% difference.  Generally, the 
difference between the two extreme corpora is about 2-9% in terms of predictive accuracy.   
Using AUC as the evaluation criteria, the performance of decision tree C4.5 on the Extreme 
Corpora I is fair, with regard to “Depth” and “Multi-views”.  The classification results of 
others are not satisfying. 
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
False Alarm Rate
D
e
te
c
ti
o
n
 R
a
te
Accuracy
Reliability
Objectivity
Depth
Conciseness
Multi-views
   
 
97
9.4 SVM 
The table and figure below show the SVM results over the extreme corpora.  The results 
show that given only “high” and “low” documents, SVM together with the language features 
can differentiate text on all six dimensions very well.  The area under curve is about .80 for 
four properties over extreme corpus I.  Selected features have slightly better performance 
than all features.  On average, the difference is 1.6%, which is smaller than that of C4.5.  As 
with the other two methods, the performance of SVM on the Extreme Corpora I is higher 
than that on the Extreme Corpora II.  The SVM method has best performance on the “Multi-
views” property, followed by “Accuracy”, “Objectivity” and “Depth”.   ROC results show 
good to fair predictive performance on all six qualitative properties over Extreme Corpora I.  
The ROC curves of SVM are almost identical to the ROC curves of Logistic regression.  If 
we put the curves in one figure, they almost overlap with each other.  “Accuracy” is the 
exception on which the SVM method has visually distinguishably better performance than 
logistic regression. 
 
Extreme Corpora I Extreme Corpora II 
All 
Features 
Selected Features All 
Features 
Selected 
Features 
Qualitative 
Property 
Accuracy Accuracy AUC Accuracy Accuracy 
Accuracy 71.6 74.5 .79 64.0  65.3 
Reliability  67.7 69.3 .74 61.5  63.4 
Objectivity   68.9 70.5 .79 65.5 66.1 
Depth    70.3 70.5 .78 66.6 68.4 
conciseness 68.1 69.9 .76 63.8 65.4 
Multi-views  74.0 75.8 .84 64.5 66.6 
   
 
98
 
Table 9.5: Accuracy and the area under curve (AUC) of SVM method over the extreme corpus (10-
fold cross validation). 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 9.3: ROC curves of SVM on Extreme Corpora I with selected features (10-fold cross validation). 
9.5 Models Generated 
We examine the models generated by C4.5 and SimpleLogistic methods over the Extreme 
Corpora I to see what features the classifiers are using to make the prediction.   
 
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
False Alarm Rate
D
e
te
c
ti
o
n
 R
a
te
Accuracy
 Reliability
Objectivity
Depth
Conciseness
Multi-views
   
 
99
Classification 
Task 
C4.5 
Tree 
Roots 
Logistic Regression 
Models 
SVM Hyperplanes 
Accuracy D_NOUN ORG_BASE, D_ADJ, D_SAY, 
D_DATE, D_NOUN, 
D_PERSON  
SEEM(-) 
ORG_BASE, D_ADJ, 
D_PERSON, D_DATE, SAY,  
D_NOUN  
QUOTELEN (-),VBZ (-), VBP, 
ORG_DEPT,  PRP$ (-), RB, 
TIT_PLC (-), LOC_REG, 
PERSON_U (-), , D_VERB,  
Reliability D_SAY JOBTITLE(-), CDG  (-), 
D_SAY, D_PERSON, 
D_DATE, D_NOUN  
NNPS, SEEM (-), ORG_BASE, 
R_PERSON(-), 
JOBTITLE(-), CDG  (-), 
D_SAY, D_PERSON, 
D_DATE, D_NOUN  
COMMA, PERS_FUL, LS, 
D_BACK (-), IN, WDT, 
VB_NOUN (-), 
Objectivity WP SAY, POS, RBS (-), PRP$ (-), 
CD, D_EXPERT, D_PERSON, 
D_DATE, D_NOUN  
LOC_REG 
SAY, POS, RBS (-), PRP$(-), 
CD, D_EXPERT, D_PERSON, 
D_DATE, D_NOUN  
COLON(-) 
Depth NN ADD (-), UNIQ_NON, LS (-), 
POS, PRP$ (-), LOC_COUN, 
DT, DATE_KEY (-), D_BACK 
(-), YEAR, STOP(-), MARK#, 
D_ADJ  
COLON (-), TIT_CIV (-), 
D_FORW, R_DATE, D_ADV, 
D_MONEY 
ADD (-), UNIQ_NON, LS (-), 
POS, PRP$ (-), LOC_COUN, 
DT, DATE_KEY (-), D_BACK 
(-), YEAR, STOP(-), MARK# 
(-),D_ADJ  
CC(-), ORG_U (-),NN, 
R_PERSON, 
Conciseness WRB AVG_WORD(-), NNPS, 
ORG_COM (-), ADV_VB (-), 
D_SAY, D_UNIQ, D_LOC , 
D_DATE  
AVG_SENT(-),VBD(-), RP, 
IN(-),D_DECL,   
AVG_WORD(-), NNPS, 
ORG_COM(-),  ADB-VB(-), 
D_SAY, D_UNIQ, D_LOC, 
D_DATE 
CURRENCY, VBP, VBG, LS, 
DEC_MI, LOC_KEY, NNS(-), 
NUM_FORW(-), YEAR(-),TO(-
),D_VERB, D_NOUN 
Multi-views SAY DBQUOTE, D_SAY, POS, 
NUM_FORW, STOP (-), CC  
(-), R_DATE, R_MONEY, 
D_VERB, D_NOUN    
TIT_CIV, CURRENCY, 
ORG_BASE, D_EXPERT, 
D_PERSON 
DBQUOTE, SAY, POS, 
NUM_FORW, SOTP(-), CC(-), 
R_DATE, R_MONEY, 
D_VERB, D_NOUN.  
UNIQ_NON, NNPS(-),UH, 
D_LOC, MONEY, 
 
Table 9.6: Models generated over the extreme corpora. 
 
   
 
100
It is not quite clear how the roots are related with the properties intuitively except for 
“Depth” and “Multi-views”.  For “Depth”, the root changes from the number of unique word 
to the number of NN.   It is easy to see that a high depth document mentions more nouns and 
a “Multi-views” document contains more instances of “say”.   
Most of the features in the SVM and logistic regression models are included in the 
corresponding models learned over the full combined corpus. We highlight the features that 
show up in both the SVM and logistic regression models.  We can see the big overlap clearly.  
This may explain their similar results.  We noticed several interesting new features that do 
not show up in the experiments on the full corpus.  It seems that many superlative adverbs 
(RBS) indicate high probability of “Subjectivity”.  The ratio between adverbs and verbs is a 
negative indicator of “Conciseness”.  The number of quotation marks, as a measure of direct 
quotation, even though not very accurate, is selected as positive indicator of “Multi-views”. 
9.6 Summary of Results over the Combined Corpora  
We have, so far, conducted these classification experiments using three learning methods 
with two sets of features and over three sets of corpora.  We began with the full combined 
corpora and then eliminated more and more non-canonical cases.  For each corpus, we used 
three learning algorithms to build classification models and then tested the models with 10-
fold cross validation.  To summarize the experiments, we first show in the following figures 
the predictive accuracies.  The x-axis represents the corpus and the y-axis records predictive 
accuracy.   
   
 
101
 
 
 
 
 
 
 
 
 
Figure 9.4: Predictive accuracies of logistic regression, C4.5 and SVM on three binary datasets, with two 
feature sets.  Predicted property is “Accuracy”. 
 
 
 
 
 
 
 
 
 
Figure 9.5: Predictive accuracies of logistic regression, C4.5 and SVM on three binary datasets, with two 
feature sets. Predicted property is “Reliability”. 
 
50
55
60
65
70
75
Extreme I Extreme II Whole
Corpus
A
cc
ur
ac
y
SVM-all
SVM-selected
Logistic-all
Logistic-selected
Tree-all
Tree-selected
50
55
60
65
70
75
80
Extreme I Extreme II Whole
Corpus
A
cc
ur
ac
y
SVM-all
SVM-selected
Logistic-all
Logistic-selected
Tree-all
Tree-selected
   
 
102
 
 
 
 
 
 
 
 
 
Figure 9.6: Predictive accuracies of logistic regression, C4.5 and SVM on three binary datasets, with two 
feature sets. Predicted property is “Objectivity”. 
 
 
 
 
 
 
 
 
 
Figure 9.7: Predictive accuracies of logistic regression, C4.5 and SVM on three binary datasets, with two 
feature sets.  Predicted property is “Depth”.  
50
55
60
65
70
75
Extreme I Extreme II Whole
Corpus
A
cc
ur
ac
y
SVM-all
SVM-selected
Logistic-all
Logistic-selected
Tree-all
Tree-selected
50
55
60
65
70
75
Extreme I Extreme II Whole
Corpus
A
cc
ur
ac
y
SVM-all
SVM-selected
Logistic-all
Logistic-selected
Tree-all
Tree-selected
   
 
103
 
 
 
 
 
 
 
 
 
 
Figure 9.8: Predictive accuracies of logistic regression, C4.5 and SVM on three binary datasets, with two 
feature sets.  Predicted property is “Conciseness”. 
 
 
 
 
 
 
 
 
 
 
Figure 9.9: Predictive accuracies of logistic regression, C4.5 and SVM on three binary datasets, with 
two feature sets.   Predicted property is “Multi-views”. 
50
55
60
65
70
75
Extreme I Extreme II Whole
Corpus
A
cc
ur
ac
y
SVM-all
SVM-selected
Logistic-all
Logistic-selected
Tree-all
Tree-selected
50
55
60
65
70
75
80
Extreme I Extreme II Whole
Corpus
A
cc
ur
ac
y
SVM-all
SVM-selected
Logistic-all
Logistic-selected
Tree-all
Tree-selected
   
 
104
 
We observed from the figures that the machine learning methods perform best in 
classifying the most extreme corpus.  It is far easier to produce clear separation of the classes 
on all six dimensions over the “cleanest” extreme corpus I.  The predictive accuracies 
decrease as we include more middle range documents, no matter what learning method and 
what language features are used.  This trend appears in all six qualitative property 
classification tasks.  The extreme most corpus also has the smallest size.  Concerned that the 
difference is caused by the smallest size of corpus, we took subsamples of the extreme corpus 
II and the full corpus with similar size to the extreme I corpus and reran the experiments.  
The results show that the trend in the results persists.  It is promising that these extreme 
corpora are separable using a selected sets of language features on the six qualitative 
dimensions.   
It turns out that optimal performance is maintained with a quite small number of 
selected features, no matter which method is used.  The difference is most obvious when the 
C4.5 decision tree method is used.  No matter which method is used, we observed the 
phenomenon that the features selected depends on the training corpus.     
SVM and the logistic regression approach are capable of achieving good levels of 
performance when attempting to classify over the Extreme Corpora I.  In most of the cases, 
SVM has slightly higher accuracy than logistic regression.  However, our results do not show 
superiority of SVM over the simple logistic regression method.  It seems that the C4.5 
decision tree approach is not suitable for the classification tasks under study.  It performs 
worst, with the only exception of “Depth” with selected features over the Extreme Corpus II.  
   
 
105
We also put the six ROC curves of each qualitative property in one figure to examine 
the general performance of the models rather than that at a specific cut point (Figure 9.10 - 
Figure 9.15).  The ROC figures show generally that we can do fairly well in classifying 
exemplars.  The curves move towards the diagonal as more documents are included in the 
datasets. 
Because we assume equal costs for both kinds of errors, we draw a light dashed line 
in each figure with the slope of the line equals to the skew ratio (number of negative cases / 
number of positive cases).  If we move this line along the (1, 0) (0, 1) diagonal, and stop 
when it tangents with any ROC curve, the classifier at the point gives the best accuracy score, 
under the assumption that the class distribution is the same as that of the training corpus.  
There may be multiple such classifiers and they may belong to different methods.  We 
calculated the accuracy scores of these points and listed in Table 9.7.  These are good level of 
accuracy scores for our classification tasks. 
 
Property Best Accuracy 
Accuracy 74.1% 
Reliability 69.6% 
Objectivity 73.0% 
Depth 72.1% 
Conciseness 70.9% 
Multi-views 76.6% 
Table 9.7:  Best accuracy scores calculated through ROC analysis 
 
We can also evaluate our performance by choosing a false alarm level and comparing 
the detection rates of different curves.  We draw a vertical line in each figure where the false 
alarm rate equals to .20.  The detection rate of “Multi-views” is the highest (.75), followed by 
Depth (.62).  The others have detection rate around 0.57 at the same false alarm rate level. 
   
 
106
 
 
 
 
 
 
     
 
 
Figure 9.10: Accuracy ROC curves of Logistic regression, C4.5 and SVM with selected features. 
 
 
 
 
 
 
 
 
 
Figure 9.11: Reliability ROC curves of Logistic regression, C4.5 and SVM with selected features. 
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
False Alarm Rate
D
e
te
c
ti
o
n
 R
a
te
Logistic - extreme
Logistic - all
C4.5 - extreme
C4.5 - all
SVM - extreme
SVM - all
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
False Alarm Rate
D
e
te
c
ti
o
n
 R
a
te
Logistic - extreme
Logistic - all
C4.5 - extreme
C4.5 - all
SVM - extreme
SVM - all
   
 
107
 
 
 
 
 
 
 
 
 
Figure 9.12: Objectivity ROC curves of Logistic regression, C4.5 and SVM with selected features. 
 
 
 
 
 
 
 
 
 
Figure 9.13: Depth ROC curves of Logistic regression, C4.5 and SVM with selected features.
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
False Alarm Rate
D
e
te
c
ti
o
n
 R
a
te
Logistic - extreme
Logistic - all
C4.5 - extreme
C4.5 - all
SVM - extreme
SVM - all
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
False Alarm Rate
D
e
te
c
ti
o
n
 R
a
te
Logistic - extreme
Logistic - all
C4.5 - extreme
C4.5 - all
SVM - extreme
SVM - all
   
 
108
 
 
 
 
 
 
 
 
 
 
Figure 9.14: Conciseness ROC curves of Logistic regression, C4.5 and SVM with selected features. 
 
 
 
 
 
 
 
 
 
Figure 9.15: Multi-views ROC curves of Logistic regression, C4.5 and SVM with selected features. 
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
False Alarm Rate
D
et
ec
ti
o
n
 R
at
e
Logistic - extreme
Logistic - all
C4.5 - extreme
C4.5 - all
SVM - extreme
SVM - all
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
False Alarm Rate
D
e
te
c
ti
o
n
 R
a
te
Logistic - extreme
Logistic - all
C4.5 - extreme
C4.5 - all
SVM - extreme
SVM - all
   
 
109
 
The ROC curves of SVM and logistic regression are always above the corresponding 
C4.5 curves.  It means that, no matter what criterion is used to choose a cutting point; C4.5 
should not be the learning method for the classification tasks.   
SVM and Logistic regression have very similar curves.  No one curve is always 
above the other.  “Accuracy”, “Objectivity” and “Conciseness” are the three properties for 
which the SVM performance is slightly better that the logistic regression models.  The ROC 
figures show that the curve of SVM runs above the curve of logistic regression in most of the 
region.  For the other three properties, we can see that, the SVM curve and the logistic 
regression curve are almost identical.   
From the observations, we cannot draw the conclusion that SVM is superior to 
logistic regression in classifying documents on the six qualitative properties.  Both methods 
can achieve pretty good results in classifying exemplary cases of the six properties. 
As a whole, our methods can predict a combination of multiple persons’ judgments of 
the six non-topical properties better than chance.  The performance improves as more middle 
range documents are removed.  The results of classifying extreme high and low documents, 
with respect to “Multi-views” and “Depth”, using SVM and logistic regression are very 
promising.  But in a real problem, one will not be given only extreme cases or exemplars, so 
we must ask: are these features and methods capable of detecting the difference between the 
documents with clear assignment and those that are located in the middle of the axis?  In 
order to have a deeper insight into this problem, we set up the experiments in the next 
chapter.  
   
 
110
Chapter 10: Identifying the Middle Range Documents 
If we treat the documents located in the middle of each dimension as “noise” in the dataset, 
we can claim that the classification performance climbs as the noise of the training dataset 
decreases.  The SVM and logistic models trained over the Extreme Corpora I can work very 
well over the test data in the same corpora.  However, in any practical situation, a corpus will 
contain a proportion of documents which are hard to assign to any class.  We need a way to 
deal with such documents.  An ideal solution is that the classifiers learned from the extreme 
corpora can put those documents correctly in the middle ranks.  In this chapter, we begin our 
exploration on this problem by testing the prediction power of the logistic regression models 
learned over extreme corpora I on the full corpora.  We then treat the middle range 
documents as one individual class and test the idea of differentiating them from other 
documents using the language features together with machine learning methods.  Finally, we 
seek to explain the difficulty of this problem by examining inter-judge agreement.  
10.1 Test the Extreme Logistic Models over the Combined Corpus 
Using logistic regression function and the parameters we learned in last chapter, we 
calculated, for each document in the corpus, the probability that the document belongs to the 
high end of each qualitative property.   Ideally, we hope to find a range of the probability 
   
 
111
scores such that we can assign documents with those scores to a category called 
“UNKNOWN”.   
The scatter plots (Figure 10.1) of the predicted probability and the original score 
show that the trends are weak.  We show only the plots for “Multi-views” and “Accuracy” as 
examples, other properties have similar plots.  It is relatively easy to choose a threshold at 
one end of each qualitative dimension.  For example, we can choose 0.5 as the cut point for 
“One-sided” documents.  All documents with probability scores lower than 0.5 are classified 
as “One-sided”.  The plot shows that these will include just a small proportion of documents 
with original score equal to or larger than five.  However, it is hard to find a high-end cutting 
point.  All original categories (1 -10) have a large proportion of documents with a probability 
score larger than 0.8.   “Accuracy”, on the other hand, is one of the properties where a high 
end cutting point is easy to get.  Possible candidates include 0.8, 0.7, even 0.6.  It is 
interesting to observe that, in both cases, the probability scores of the documents in original 
category 0 accumulate densely between 0.4 and 0.7, which intuitively could be the range 
used to assign unknown.  These are the documents for which the corresponding property is 
believed to be inapplicable. 
Obviously, the models learned from the exemplars cannot rank the middle range 
documents as we expected.  We have learned that our method can do a pretty good job in 
differentiating between extremely low and extremely high documents on a qualitative 
property.  If we can automate the process of differentiating extreme documents from which 
people have difficulty placing at one end of a dimension, the problem could be solved in two 
steps.   
 
   
 
112
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 10.1:  Scatter plots of original categories and the predicted probability on Multi-
views and accuracy properties.  The probabilities are calculated using the logistic 
regression models learned over the extreme corpora I.  
 
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0 2 4 6 8 10
Original Accuracy Score
pr
ob
ab
ili
ty
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0 2 4 6 8 10
Original Multi-views Score
Pr
ob
ab
ilit
y 
   
 
113
10.2 Differentiation between CLEAR and UNKNOWN 
We test the idea of automatic classification of documents that are clearly assigned to one end 
of a qualitative dimension from those that are difficulty to assign.  In the corpus for each 
qualitative property, there are 3,199 documents.  A class of CLEAR or UNKNOWN is 
assigned to each document.  The assignment scheme is summarized in Table 10.1. 
 
Table 10.1: Class assignment to create corpus for classification between UNKNOWN and 
CLEAR documents. 
 
Because five corpora (except for “Depth”) are extremely biased, we use a re-sampling 
method in Weka (weka.filters.supervised.instance.Resample) to create corpora in which the 
class distribution is uniform.  
Qualitative 
Property 
Original Score Class Number of 
Cases 
1-4, 9-10 CLEAR 830 Accuracy 
0, 5-8 UNKNOWN 2369 
1-4, 9-10 CLEAR 1051 Reliability  
0, 5-8 UNKNOWN 2148 
1-3, 9-10 CLEAR 984 Objectivity   
0, 4-8 UNKNOWN 2215 
1-3, 8-10 CLEAR 1614 Depth    
0, 4-7 UNKNOWN 1585 
1-4, 9-10 CLEAR 1089 Conciseness 
0, 5-8 UNKNOWN 2110 
1, 8-10 CLEAR 873 Multi-views  
0, 2-7 UNKNOWN 2326 
   
 
114
For each corpus, two sets of features were tested: the full set of features and a filtered 
feature set.  The machine learning algorithms used is SimpleLogistic from the Weka package 
with default settings.  The correlation based feature evaluation method 
(weka.attributeSelection.CfsSubsetEval) and the best forward searching method 
(weka.attributeSelection.BestFirst) included in the Weka package is used to create the 
selected features for each qualitative property.  A 10-fold cross-validation is used to evaluate 
the classification results. 
In the table below, column 2 tells us if the full set of features or selected features have 
been used, column 3 lists the accuracy of using SimpleLogistic method.  
 
Qualitative 
Property 
Features Prediction 
Accuracy 
All .55 Accuracy 
Selected .55 
All .55 Reliability  
Selected .55 
All .58 Objectivity   
Selected .56 
All .56 Depth    
Selected .55 
All .57 Conciseness 
Selected .58 
All .56 Multi-views  
Selected .56 
 
Table 10.2: Accuracy for classification between CLEAR and 
UNKNOWN documents. 
 
   
 
115
The accuracies of logistic regression are barely better than random assignments.  
Neither C4.5 nor SVM can give better results.  As a whole, these results show that with the 
language features under study, logistic regression cannot differentiate CLEAR documents 
from UNKNOWN.   
10.3 Another Thought 
For any property, if we have a model that can separate the extremely high documents 
from the rest of the documents, and a model that can separate the extremely low documents, 
then we can solve the classification problem as illustrated in Figure 10.2.  However, as 
reported in Chapter 8, the performance of the extreme-high and extreme-low models is not 
satisfactory.  So the problem remains unsolved. 
 
 
 
 
 
 
Figure 10.2: Identifying UNKNOWN documents by fusion of two 
extreme models. 
Clearly 
Low 
Clearly 
high
 
        UNKNOWN 
Extreme high model 
Ex
tre
m
e 
lo
w
 m
od
el
 
   
 
116
10.4 The Middle Range Documents and Inter-Judge Agreement 
It is known that machine learning algorithms work under the assumption that the 
observations used for training are the “truth”, given by a trusted source.  We now suspect that 
our success on the exemplars and our failure on the middle range documents may indicate 
that the exemplar judgments are more nearly a “truth” than are those on the middle range 
documents.   In other words, people don’t agree with each other very much about the non-
topical properties of middle range documents.  As mentioned before, the HITIQA project 
collected two judgments on each document’s non-topical properties; we have only used the 
judgment made at Rutgers for our learning experiments.  But we can test our speculation.  To 
do this, we calculated the difference between the two judgments, as well as the average, for 
each qualitative property.  Then we examine the mean and standard deviation of the 
difference at each level of the average score.  If our speculation is true, we expect to see 
bigger mean absolute difference and variance in the middle range than at the end of the axis.   
In the following six figures, the x-axis refers to the average score of two judgments 
on the same documents; the two lines show the mean and standard deviation of the absolute 
inter-judge difference at each average level.  
 
 
 
 
 
   
 
117
 
 
 
 
 
 
(a) Accuracy 
 
 
 
 
 
 
(b) Reliability 
 
 
 
 
 
 
(c) Objectivity 
0.00
1.00
2.00
3.00
4.00
5.00
0.00 2.00 4.00 6.00 8.00 10.00
Average
Di
ffe
re
nc
e
Mean
Standard Deviation
0.00
1.00
2.00
3.00
4.00
5.00
0.00 2.00 4.00 6.00 8.00 10.00
Average
D
iff
er
en
ce
Mean
Standard Deviation
0.00
1.00
2.00
3.00
4.00
5.00
6.00
0.00 2.00 4.00 6.00 8.00 10.00
Average
D
iff
er
en
ce
Mean
Standard Deviation
   
 
118
 
 
 
 
 
 
(d) Depth 
 
 
 
 
 
(e) Conciseness 
 
 
 
 
 
(f) Multi-views 
Figure 10.3: Means and standard deviations of inter-judge difference 
at different levels of the average judgment scores. 
0.00
1.00
2.00
3.00
4.00
0.00 2.00 4.00 6.00 8.00 10.00
Average
Di
ffe
re
nc
e
Mean
Standard Deviation
0.00
1.00
2.00
3.00
4.00
0.00 2.00 4.00 6.00 8.00 10.00
Average
Di
ffe
re
nc
e
Mean
Standard Deviation
0.0000
1.0000
2.0000
3.0000
4.0000
5.0000
0.00 2.00 4.00 6.00 8.00 10.00
Average
Di
ffe
re
nc
e
Mean
Standard Deviation
   
 
119
 
The figures show very clearly that the judgments on the middle range documents 
have much larger differences than those on the extreme or exemplar documents.  People do 
disagree more on the documents in the middle range than they do on the documents at the 
ends of the range.  This confirms our speculation.  In other words, our failure to differentiate 
the middle range documents on each property corresponds to the fact that these are the 
documents on which the human judges, themselves, have the largest disagreement.     
   
 
120
Chapter 11: Modeling Individual Judgments 
The fact that it is hard to automatically predict non-topical properties has two possible 
sources.  First, as we mentioned before, there are documents that really fall in the middle 
range of any dimension.  Secondly, our corpus is in fact the product of the work of multiple 
judges.  These judgments are the result of the interaction between the various judges’ mental 
models and the textual and linguistic features of the texts at various levels.  There are many 
sources of variability on the part of judges: (1) different judges may have different 
interpretations of and personal bias on the rating scales; (2) the understanding of the meaning 
of the six qualitative properties themselves may vary across people; (3) the judgment criteria 
may vary across people.  All these problems suggest that we might be less ambitious.  Instead 
of constructing global models, if we focus on an individual level, our classification 
performance may be much better.  In order to test this hypothesis, we set up the 
corresponding experiments over our individual corpora.  
The HITIQA project has collected five individual corpora.  Each contains one 
individual judge’s work on about 500 documents.  Before applying any learning methods, we 
examine the characteristics of these corpora.   
    
 
 Accuracy Reliability Objectivity Depth Conciseness Multi-views 
S01 
ACCU
10.09.08.07.06.05.04.03.02.0
120
100
80
60
40
20
0
Std. Dev = 1.91  
Mean = 6.7
N = 459.00
 SOUR 10.09.08.07.06.05.04.03.02.01.0
120
100
80
60
40
20
0
Std. Dev = 2.21  
Mean = 6.7
N = 446.00
 OBJE
10.09.08.07.06.05.04.03.02.01.0
120
100
80
60
40
20
0
Std. Dev = 2.02  
Mean = 6.1
N = 459.00
 DEPT
10.09.08.07.06.05.04.03.02.01.0
80
60
40
20
0
Std. Dev = 2.13  
Mean = 5.2
N = 460.00
 VERB 10.09.08.07.06.05.04.03.02.01.0
120
100
80
60
40
20
0
Std. Dev = 2.04  
Mean = 6.2
N = 460.00
 SIDE
10.09.08.07.06.05.04.03.02.01.0
120
100
80
60
40
20
0
Std. Dev = 2.34  
Mean = 4.1
N = 460.00
 
S02 
ACCU
10.09.08.07.06.05.04.03.02.01.0
300
200
100
0
Std. Dev = 1.17  
Mean = 7.5
N = 410.00
 SOUR
10.09.08.07.06.05.04.03.02.01.0
160
140
120
100
80
60
40
20
0
Std. Dev = 1.46  
Mean = 6.9
N = 410.00
 OBJE 10.09.08.07.06.05.04.03.02.01.0
200
100
0
Std. Dev = 1.51  
Mean = 6.9
N = 410.00
 DEPT
10.09.08.07.06.05.04.03.02.01.0
200
100
0
Std. Dev = .96  
Mean = 2.9
N = 410.00
 VERB 10.09.08.07.06.05.04.03.02.01.0
200
100
0
Std. Dev = 1.00  
Mean = 5.2
N = 410.00
 SIDE
10.09.08.07.06.05.04.03.02.01.0
160
140
120
100
80
60
40
20
0
Std. Dev = 1.05  
Mean = 3.0
N = 410.00
 
S03 
ACCU
9.08.07.06.05.04.03.02.01.0
ACCU
Fr
eq
ue
nc
y
120
100
80
60
40
20
0
Std. Dev = 1.26  
Mean = 6.4
N = 310.00
 
SOUR
9.08.07.06.05.04.03.02.01.0
SOUR
F
re
qu
en
cy
80
60
40
20
0
Std. Dev = 1.71  
Mean = 5.9
N = 293.00
 10.09.08.07.06.05.04.03.02.01.0
120
100
80
60
40
20
0
Std. Dev = 1.49  
Mean = 3.1
N = 310.00
DEPT
10.09.08.07.06.05.04.03.02.01.0
DEPT
140
120
100
80
60
40
20
0
Std. Dev = 1.02  
Mean = 2.8
N = 310.00
 VERB
10.09.08.07.06.05.04.03.02.01.0
100
80
60
40
20
0
Std. Dev = 1.43  
Mean = 6.0
N = 310.00
 SIDE
10.09.08.07.06.05.04.03.02.01.0
140
120
100
80
60
40
20
0
Std. Dev = .95  
Mean = 2.5
N = 310.00
 
S04 
ACCU
10.09.08.07.06.05.04.03.02.01.0
200
100
0
Std. Dev = 1.13  
Mean = 7.0
N = 475.00
 SOUR
10.09.08.07.06.05.04.03.02.01.0
300
200
100
0
Std. Dev = 1.16  
Mean = 6.7
N = 468.00
OBJE
10.09.08.07.06.05.04.03.02.01.0
200
100
0
Std. Dev = 1.51  
Mean = 6.7
N = 480.00
DEPT
10.09.08.07.06.05.04.03.02.01.0
100
80
60
40
20
0
Std. Dev = 1.87  
Mean = 5.1
N = 480.00
 VERB
10.09.08.07.06.05.04.03.02.01.0
200
100
0
Std. Dev = 1.42  
Mean = 6.4
N = 480.00
SIDE
10.09.08.07.06.05.04.03.02.01.0
100
80
60
40
20
0
Std. Dev = 2.00  
Mean = 4.4
N = 474.00
 
S05 
ACCU
10.09.08.07.06.05.04.03.02.01.0
300
200
100
0
Std. Dev = 1.29  
Mean = 8.2
N = 525.00
 
SOUR
10.09.08.07.06.05.04.03.02.01.0
300
200
100
0
Std. Dev = 1.30  
Mean = 8.2
N = 481.00
 
OBJE
10.09.08.07.06.05.04.03.02.01.0
300
200
100
0
Std. Dev = 2.30  
Mean = 7.6
N = 539.00
 DEPT 10.09.08.07.06.05.04.03.02.01.0
200
100
0
Std. Dev = 2.27  
Mean = 7.2
N = 537.00
 VERB
10.09.08.07.06.05.04.03.02.01.0
300
200
100
0
Std. Dev = 1.89  
Mean = 7.9
N = 539.00
 
SIDE
10.09.08.07.06.05.04.03.02.01.0
200
100
0
Std. Dev = 2.99  
Mean = 6.1
N = 509.00
 
Table 11.1: Individual corpora distribution of the six qualitative properties.
121
 122    
 
It is very clear that the judgments among five individual judges differ greatly.  There 
are of course two major factors that cause these judgment differences: different corpora were 
assigned to each person; and the judges themselves are different.  However, the documents 
judged by each judge are randomly selected from the combined corpus.  Therefore the first 
factor, corpus difference, should not be an important contributor to the variation in judgments 
among assessors.  The large variation is mainly caused by the individual differences among 
five judges. 
Table 11.1, we can see that the judgments of judge S02 and S05 accumulate heavily 
in one or two consecutive scale values on all six qualitative properties, and the concentration 
is at one end of the axes.  Only a few cases are distributed among other scores.  This means 
that these two assessors believe that, on most qualitative properties, most of the documents 
they judged are quite similar.  In other words, there is for each of these two judges only one 
class, for each qualitative property, adequately presented in the corpus.  Without enough 
examples of cases in each class, there is not enough information in the training corpus for an 
algorithm to learn.  Judge S03’s assignments are relatively scattered.  However, there are 
nearly no instances at the high end of the 10 point scale on “Objectivity”, “Depth” and 
“Multi-views”. In addition, 95.2% of “Objectivity” scores are less than or equal to five.  For 
“Depth” and “Multi-views”, the corresponding percentage is 98.1% and 100% respectively.  
Turning to judge S04, and judgments on “Accuracy”, 75.2% of the documents are assigned a 
score 7 or 8, which leave not enough negative instances for machine learning.  We discard 
those datasets that do not have enough information for learning.  We summarize the corpora 
that are usable in Table 11.2.   
 
 123    
 
User ID Tasks 
S01 
Accuracy,  
Reliability,  
Objectivity, 
 Depth,  
Conciseness,  
Multi-views 
S03 
Accuracy 
Reliability,  
Conciseness 
S04 
Reliability,  
Objectivity,  
Depth,  
Conciseness,  
Multi-views 
   
Table 11.2: Individual classification tasks. 
11.1 Numerical Prediction 
We use half of each corpus for training, and test the linear models on the other half of the 
corpus.  The learning algorithm used is stepwise linear regression implemented in SPSS with 
default settings.   
In the table below, we summarize the R2 value of each experiment.  For the purpose 
of comparison, we include the corresponding results over the combined corpus in the last 
row. 
 
User ID Accuracy Reliability Objectivity Depth Conciseness Multi-views 
S01 .16 .27 .16 .30 .32 .42 
S03 .33 .21 - - .44 - 
 124    
 
S04 - .19 .52 .66 .31 .26 
Combined 
Corpus .10 .13 .16 .16 .12 .18 
 
Table 11.3: R2 of stepwise linear regression on the individual datasets. 
 
As mentioned earlier, R2 provides a measure of how well a qualitative property is 
predicted by a set of language features.  The results indicate that the selected language 
features account for a significant proportion of the variance of user S04’s judgments on 
“Objectivity” and “Depth” (52% and 66%).  The models fit judge S03’s Verbose-
”Conciseness” and S01’s “Multi-views” assessments fairly well too (44% and 42%).  As a 
whole, the R2 scores for predicting individual assessments, in those cases with an adequate 
distribution of training data, are substantially better than for the combined corpus.  However, 
the degree of success varies with different judges and different qualitative properties.      
11.2 Binary Classification  
11.2.1 Individual Binary Datasets 
We create two binary datasets from each individual datasets.  In one of them, all documents 
in the original datasets are included.  A score gives the nearest to 50-50 division of the corpus 
is used as a cut point.  The documents with scores higher than the point are labeled as 1, 
others are labeled as 0.  These are referred as binary individual full corpora (BIFC).   In the 
second dataset, the documents with the median score (and the adjacent score in the other 
class) are removed, which gives datasets that are similar to the Extreme II corpora.  These are 
 125    
 
referred as binary individual exemplar corpus (BIEC).  We do not create corpora similar to 
the Extreme Corpora I, because the number of instances in such a corpus will be very small, 
reducing the reliability of the results.  
The table below shows the details of each dataset.  The original score ranges are 
included in the parentheses.  
 
Full Corpus (BIFC) Exemplar Corpus (BIEC) User ID Property 
1 0 1 0 
S01 Accuracy 270  (7-10) 
189 
(1-6) 
172  
(8-10) 
189  
(1-6) 
 Reliability 199  (8-10) 
247 
(1-7) 
199  
(8-10) 
174 
(1-6) 
 Objectivity 224 (7-10) 
235 
(1-6) 
224 
(7-10) 
157 
(1-5) 
 Depth 200 (6-10) 
260 
(1-5) 
200  
(6-10) 
190 
(1-4) 
 Conciseness 247 (7-10) 
213 
(1-6) 
141 
(8-10) 
213 
(1-6) 
 Multi-views 224 (4-10) 
236 
(1-3) 
224 
(4-10)  
153 
(1-2) 
S03 Accuracy 146 (7-10) 
161 
(1-6) 
60 
(8-10) 
56 
(1-5) 
 Reliability 118  (7-10) 
172  
(1-6) 
118 
(7-10) 
108 
(1-5) 
 Conciseness 130  (7-10) 
177  
(1-6) 
130 
(7-10) 
103 
(1-5) 
S04 Reliability - - 113 (8-10) 
163 
(1-6) 
 Objectivity - - 165 (8-10) 
152 
(1-6) 
 Depth 210 (6-10) 
270 
(1-5) 
210 
(6-10) 
210 
(1-4) 
 Conciseness 263 (7-10) 
217 
(1-6) 
92 
(8-10) 
87 
(1-5) 
 Multi-views 244 (5-10) 
230 
(1-4) 
152 
(6-10) 
230 
(1-4) 
 
Table 11.4: Individual binary datasets 
 
 126    
 
For S04’s judgments on “Reliability” and “Objectivity”, there are 41% and 34% of 
documents with the median scores.  We do not run any experiments on those qualities for 
subject S04, because there is no way to avoid serious skew in class distribution without 
splitting the documents having the median score. 
There are all totally 26 binary individual datasets.  On each of the dataset, we ran 
three learning methods: logistic regression, decision tree C4.5 and SVM, with the feature 
selection methods we developed before. 
11.2.2 Classification Results 
In the table below (next page), are shown the ROC analysis results (AUC) of the models 
produced.  Each one of them is the result of one learning algorithm with its selected features 
on one binary dataset.  We also included in the table the performances achieved in our 
experiment on the three sets of “Combined” binary corpora as the baseline performances for 
each qualitative property.  The Extreme I column refers the best performance over Extreme 
Corpora I in the combined experiments; this is also the best performance we can achieve in 
combined learning of the six properties.  Extreme II refers to the Extreme Corpus II which 
corresponds to the binary individual Exemplar Corpora (BIEC).  Combined Full Corpus 
refers the full corpus, performance on which is used as baseline for the binary individual full 
corpus (BIFC).   
    
 
 
Combined Corpus Results Individual Binary Corpora 
Exemplar Corpus Full Corpus Dataset  Extreme 
I  
Extreme 
II 
Full 
Corpus Logistic SVM C4.5 Logistic SVM C4.5 
S01-Accuracy .79 .68 .64 .68  .68 .60  .62 .62 .55 
S03-Accuracy    .76 .69 .63 .71 .69 .60 
S01-Reliability .74 .67 .63 .72  .73 .63  .70 .69 .63 
S03-Reliability    .70  .70 .57 .70 .73 .62 
S04-Reliability    .67  .69 .61  - - - 
S01-Objectivity .79 .73 .67 .66  .73 .63  .65  .65 .55 
S04-Objectivity    .83  .81 .72  - - - 
S01-Depth .78 .72 .69 .73  .74 .68  .71 .72 .59 
S04-Depth    .93  .90 .86  .87  .86 .80 
S01-Conciseness .76 .70 .64 .67  .71 .66  .67  .66 .58 
S03-Conciseness    .78 .77 .76 .74 .76 .64 
S04-Conciseness    .72  .69 .66  .69  .68 .61 
S01-Multi-views .84 .72 .70 .82  .80 .74 .77 .75 .64 
S04-Multi-views    .76  .76 .68 .72  .72 .61 
 
Table 11.5:  AUC of logistic regression, SVM and C4.5 on the individual datasets. 
 
127
 128    
 
We observe that, in general, the best performance on each individual binary 
dataset is higher than the corresponding results over the combined corpus.  However, the 
superiority of individual classification over the combined corpus classification is not 
significant, in most of the cases.  So we cannot draw the conclusion that, generally, it is 
easier for a machine learning algorithm to predict an individual’s judgments about the six 
qualitative properties of documents than to predict composite judgments.  Only the mean 
AUC of the individual judgments on “Multi-views” is significantly higher than that of the 
combined corpus.  Finally, and most importantly, the individual classification 
performance itself is not really satisfying in terms of AUC score. 
If we use an AUC score of 0.75 as a standard for good classification performance, 
we must admit that only nine out of the 26 binary individual datasets reach this standard.  
In other words, out of the 26 datasets, there are only nine where any learning algorithm, 
together with the language features, can do a good job on the classification problem.  The 
nine datasets are highlighted in Table 11.5.   
These cases of good performance cover five out of six qualitative property 
classification problems (with the exception of “Reliability”).  And each of the three 
judges has at least one set of judgments that can be well predicted.  For the facet of 
“Multi-views”, both SVM and logistic regression achieve better than 0.75 AUC 
performances on the individual exemplar datasets.  They even achieve good performance 
on Judge S01’s individual full dataset.  However, none of them can exceed the best 
performance on the “Combined” situation.   
There are however three individual cases on which the learning algorithms 
outperform the best “Combined” performance: S04-Objectivity, S04-Depth and S03-
 129    
 
Conciseness.   For each of these cases, we plot the ROC curves of C4.5, logistic 
regression and SVM on the individual exemplar corpus, as well as the best curve from 
our “Combined” experiment.  We have found that the numbers of features selected in 
these cases are relatively small, and most of them are intuitively meaningful to the 
classification task. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 11.1: S03-Conciseness ROC curves of C4.5, SVM and Logistic regression 
on exemplar corpus.  The thin dark line is the ROC for the best classification on 
the combined corpus. 
 
It shows that the individual curves and the combined corpus curve are very 
similar when the detection rate is smaller than 0.60.  The individual curves, especially the 
logistic regression curve, are far above the combined corpus curve in the other end of the 
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
False Alarm Rate
D
e
te
c
ti
o
n
 R
a
te
S03-C4.5-extreme
S03-SVM-extreme
S03-Logistic-extreme
Macro-extreme ICombined –extreme I 
 130    
 
curve.  This means that the individual modeling of Judge S03 outperforms the general 
model across multiple judges in identifying ‘Verbose” documents. 
Regarding features, it is very interesting to notice that for Judge S03’s work on 
“Conciseness”, the feature “average sentence length” shows up as very important.  It is 
the root of the C4.5 decision tree; is the only feature left in the logistic regression model, 
and is the feature with the largest weight in the SVM model.  To model Judge S03, we 
need only agree that documents which contain shorter sentences are more likely to be 
“Concise”.  Although this is naïve, it is not an unreasonable operationalization of the 
concept of “Conciseness”.   
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 11.2: S04-Depth ROC curves of C4.5, SVM and Logistic regression on 
exemplar corpus.  The thin dark line is the ROC for the best classification on the 
combined corpus. 
 
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
False Alarm Rate
D
e
te
c
ti
o
n
 R
a
te
S04-C4.5-extreme
S04-SVM-extreme
S04-Logistic-extreme
Macro-extreme ICombined –extreme I 
 131    
 
The results of modeling Judge S04’s understanding of “Depth” not only are 
substantially better than that of the composite model, but also the best performance we 
get in this study.  The logistic regression ROC curve shows that when the detection rate is 
0.50, the false alarm rate is barely above 0.  This means that if the model is used to get 
50% of the “high-depth” documents in the test dataset, few “low-depth” documents are 
mistakenly included.  When the detection rate rises to 0.85, the false alarm rate is only 
0.10. 
To model Judge S04’s understanding of “Depth”, the most important features are 
the numbers of types (number of unique words) and tokens (document length in number 
of words) in a document.  Over the exemplar corpus, logistic regression picks up the 
“type” feature.  That is, the more types a document contains, the more likely it is to be 
judged “in Depth” by Judge S04.  The “token” feature is the root of the C4.5 tree.  SVM 
picks up both of the two features, in the form of the type/token ratio, with a negative 
parameter.  We believe that this is at the root of the good performance in classifying all of 
S04’s “Depth” assessments.      
 
 
 
 
 
 
 
 
 
 
 
 
 
 132    
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 11.3:  S04-Objectivity ROC curves of C4.5, SVM and Logistic 
regression on exemplar corpus.  The thin dark line is the ROC for the best 
classification on the combined corpus. 
 
The logistic regression ROC curve of judge S04-Objectivity is above the curve of 
the general model above the point of 40% detection rate and 10% false alarm rate.  It 
suggests that our model works better in recognizing “Subjective” documents judged by 
S04.  For “Subjectivity”, when the false alarm rate is 10%, the detection rate is 60%. 
It seems that S04’s concept of “Objectivity” does not focus on just one or two 
features.  There is a group of features that show up in the models.  The “density of the 
word ‘say’” (D_SAY) is the only common features in all three models.  According to 
Judge S04, an objective document tends to contain more instances of “say”.  For this 
problem, SVM and logistic regression share many interesting features.  They both 
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
False Alarm Rate
D
e
te
c
ti
o
n
 R
a
te
S04-C4.5-extreme
S04-SVM-extreme
S04-Logistic-extreme
Macro-extreme ICombined –extreme I 
 133    
 
identify comparative adverb (RBR) as positive indicator for “Objectivity”, while 
superlative adverb (RBS) is a negative indicator.  The high frequency of “today”, 
“yesterday” and “tomorrow” (DATE_KEY) in the text indicates “Subjectivity” to this 
judge.  The frequency of STOP word is a negative feature for “Objectivity” in both 
models.  Other features that have positive effect on “Objectivity” include the date 
expression (D_DATE), the word “expert” (EXPERT), and location keys like “ocean, city, 
etc.” (LOC_KEY).   The SVM model shows that the frequency of verb-gerund or present 
participle (VBG) has a substantial negative effect on “Objectivity”, while the Logistic 
regression method picks two other forms of  verbs (VB and VBZ).  They all express the 
present tense and so, it seems that, to Judge S04, the presence of such verbs is an 
indicator of “Subjectivity”.  
11.3 Summary 
The purpose of this chapter’s experiments is to test the hypothesis that machine learning 
algorithm can do better when classifying an individual user’s judgments on the six 
qualitative properties, compared to modeling all judges together with one model.  We 
observed higher performance.  There are a few cases in which we can model an 
individual judge’s understanding of one property very well.  However the difference is 
not large enough for us to tell that the hypothesis is confirmed.  
From the observation that most of the features in the “good” individual models 
are intuitively meaningful, we speculate that machine learning over individual judgments 
might be an effective method of selecting powerful and stable features for each 
 134    
 
classification task.  We believe that there exists a set of features that are linked tightly 
with the properties in this study.  However, to different persons, the importance of these 
features varies.  A feature may be the dominating criterion, for one qualitative property, 
for one person.  For another person, it may be one of several equally important features.  
In the combined datasets, the importance of the features is averaged all the people who 
made the judgments, which makes it hard for these features to standout.     
It seems likely that some users make their judgments based on very simple rules, 
and that these rules would seem meaningful to other users too.  Thus, the study of 
individual judgments may be an effective method of identifying a useful feature set for 
classifying documents on each of the qualitative properties.  This is a direction we would 
like to explore in the future work. .   
 
 
 
 
 135    
 
Chapter 12: Conclusions and Beyond 
12.1 Summary of Results 
As outlined in the initial chapters, word-frequency-based information retrieval does not 
satisfy users’ non-topical information requirements.  Neither does it provide adequate 
description of documents’ non-topical qualitative properties.  In this study, we 
investigated the possibility of using simple language features and machine learning 
methods for categorizing documents on six specific qualitative dimensions.       
Our numerical experiments show that we cannot adequately differentiate 
documents at 10 distinct levels on each non-topical dimension.  On the other hand, the 
binary classification experiments on the combined corpus indicate that the classification 
of documents with regard to the six dimensions can be made with better than random 
performance.  The predictive accuracies are also higher than the naïve baselines of 
always choosing the most frequent category.  However, we feel that the 60% - 65% 
accuracies are not good enough.  Using a relatively strict AUC evaluation criteria, the 
best AUC score we can achieve for each qualitative property on the combined corpus is 
“poor” (.60 - .70).    
We have found that in all the six classification scenarios, the distribution of 
instances varies tremendously across the 10-point scale.  In particular, a great proportion 
 136    
 
of documents is located in the middle range of the scale.  The dichotomization method 
used in the binary classification experiments arbitrarily labels them into two categories.  
This suggests the possibility of achieving better performance by omitting the middle 
range of documents.  The results on the exemplar documents verify this hypothesis.  
Performance improves as more middle range documents are removed from the datasets.  
The best performing SVM method achieves around 70% -75% accuracies on the six 
classification tasks.  These results show that classification of exemplar documents with 
regard to the six dimensions can be made with relatively high accuracy.  Performance for 
all of the six qualitative properties is significantly better than a baseline of always 
choosing the most frequent category.  The AUC scores are in the “fair” (.70 - .80) or 
“good” (.80 - .90) zone.   
As a point of comparison, Wiebe et al. work on the “Subjectivity” problem at the 
level of sentence.  In their latest work (2005), the best performance obtained using a 
Naïve Bayes classifier with automatically generated training data is 70.6% recall and 
79.4% precision.  The recall and precision of their supervised learning is 77% and 81%.  
The features they used include several manually developed resources and syntactic-
analysis-based patterns.  Our linear SVM classifier can achieve 72.4% precision and 
83.5% recall in identifying “Subjective” exemplars, which gives a comparable F 
measure.  There are many differences between the two studies, so the results are not 
comparable.  However, it gives us some hint that the simple language features used here 
may be as good as more advanced features.   
Finn and Kushmerick (2003) have also done some work on “Objectivity”.  The 
average accuracy of classifying web news articles within a domain, with regard to 
 137    
 
“Objectivity”, is 90.5%, using the decision tree C4.5 learning algorithm.  The cross 
domain accuracy is 77.5%.  The best accuracy we can achieve is 73.0% using linear 
SVM; our C4.5 models have accuracy of only 66.5%.  However, their feature set includes 
the full bag-of-words, which may explain its good performance within one domain.  It is 
not clear if their advantage is caused by this additional set of features, or just the 
difference in corpora.         
We formed the subsidiary hypothesis that the poorer performance on the full set 
of documents compared to the extreme documents (exemplars) may simply reflect the 
fact that judgments on the middle range documents are not as reliable as those on 
exemplars.  When we took a closer look at the inter-judge differences on the middle 
range documents, we found that people disagree far more on those documents than they 
do on the documents located at the ends of the scale.  And the disagreement varies more 
in the middle range.   
Looking at the individual datasets on a property-by property and judge-by-judge 
basis, there are more than half of the cases in which judgments on the property are 
heavily accumulated in a few adjacent points.  This suggests that the documents in our 
corpus are quite similar with regard to the six qualitative properties, at least to these five 
judges.  We did not try the learning process on all of the datasets.  Learning experiments 
were run only on the relatively distributed individual datasets.  The ROC analysis results 
on those datasets are slightly better than the corresponding results on the combined 
corpus.  However, this is a small difference.  In summary, very accurate classifiers can be 
learned automatically in some, but not all, individual cases.   
 138    
 
On taking a closer look at the performance of the machine models, it becomes 
clear that some qualitative dimensions are modeled more easily than others.  The “Depth” 
and “Multi-views” classification tasks seem relatively easy, followed by “Objectivity”.  
“Reliability” is the task we have most difficulty with, whether on the combined corpus or 
on the individual corpora.  We believe that its relationship with the language cues is 
simply not as tight as the other two features.  Judgment about a document’s “Reliability” 
is more likely to be affected by the judge’s individual knowledge and preference, and by 
metadata characterizing the source.   
These experiments suggest that decision tree C4.5 is not a suitable learning 
algorithm for the classification tasks under study.  As for the other two methods, there is 
only a small difference between SVM and Logistic Regression.  Comparing linear SVM 
and Logistic Regression on the exemplar corpus, we find that they yield about the same 
accuracy: averages of 71.8% (SVM) vs. 71.0% (Logistic Regression) for selected 
features, and 70.1% (SVM) vs. 69.3% (Logistic Regression) for all features.  The 
accuracy scores for the corresponding C4.5 models are lower, at 67.5% and 62.0% 
respectively.  The average area under curve for C4.5, Logistic Regression and SVM are 
.68, .77 and .78.   
Further comparison between models using the full set of features and the models 
using selected features suggests that, for each qualitative property, when only the most 
discriminating and stable features are used, the performance is as good as, or even better 
than, when all features are used. 
Finally, although we have not conducted any formal tests, the derived models 
appear to be intuitively reasonable.  For example, the SVM representation of the property 
 139    
 
“Depth” for the exemplar documents includes the features number of unique words 
(2.43), determiner (1.52), year expression (3.20), common noun - singular or mass (2.47) 
with large positive weights, and the features list item marker (-1.30), possessive pronouns 
(-2.37), keywords of date, e.g. “today”, “yesterday”, and “tomorrow” (-1.85), stop words 
(-1.95), coordinating conjunction (-1.07) with large negative weights.   The features 
selected for any property vary a lot across individual judge.  It seems that the very 
accurate individual classifiers tend to include only intuitively reasonable features.   
12.2 Conclusions 
Our findings demonstrate that:  
1) It is possible to classify documents, with respect to the six qualitative properties, 
using machine learning techniques together with simple language and textual 
features.   
2) The performance improves as more ambiguous documents are removed from the 
dataset.   
3) With the current set of features, we cannot identify the middle range documents.   
4) We can do pretty well in automatic classification of exemplar documents with 
regard to “Depth” and “Multi-views”.  
5) For others, the performance is fair.  
We conclude that have to learn more about the properties and the way people 
make judgments on them.  
 140    
 
Studying individual judges, we find that: 
1) On average, automatic assessment of one individual judge’s assignment is 
slightly, but not significantly, better than learning a combined model representing 
all the judges.   
2) Our methods can do extremely well in some cases on “Depth” and “Multi-views”.   
3) The models are often intuitively reasonable.  Intuitively meaningful features stand 
out in “very good” individual classifiers. 
About the learning methods, we conclude that: 
1) Linear based learning methods, logistic regression and linear SVM, are better 
tools than the Boolean-based decision tree (C4.5) method.  The advanced SVM 
method does not show significant superiority over the logistic regression method. 
2) The feature selection methods are helpful in generating stable models and 
reducing model size.  
12.3 Limitations of the Study 
Although this study uses a large and detailed collection of data, the results presented here 
have inevitable limitations.  First, there is the limitation of the corpus.  Because it is 
expensive to develop a new large enough training corpus of this sort, we used the 
HITIQA corpus.  This is narrow in terms of topics and styles.  We found that the 
distributions of judgments are unbalanced on all six qualitative properties, with different 
degree of skewness towards one end of the scale or the other.  This may indicate that 
 141    
 
there is not enough variety in the examples, in the dataset.  In addition, we did not control 
the judgment process.  The judges did not know that their judgments would be used for 
machine learning.  Had they known, (or if they had been differently instructed), they 
might have been more careful in their judgments.  In addition, they might have indicated 
that some documents should not be used as examples for machine learning.   
Related to this, there is the problem of the labeling of the documents.  In 
particular, for purposes of binary classification, we need to label the documents “High”, 
or “Low” on the basis of the distribution of the 10-point scale judgments.  This is a 
simplification that ignores variation among documents, and, more importantly, variation 
among judges.   
Our study of individual judgments is based on several persons’ work on different 
sets of documents, which makes the result not comparable with regard to individual 
difference. 
12.4 Future Work 
Our results show that for some properties, we do know how to automatically assess them; 
for others we don’t.  This suggests that we have to learn more about these properties 
themselves.  These properties depend on not only the information object, but also on the 
user’s personal views, experience and background.   We might exclude those properties 
that are highly judge-dependent.  For others, more work is needed in building relatively 
objective definitions and judgment instructions.  
 142    
 
We believe that if we do better work in the definition of the qualitative properties 
and the instructions given to judges, we may get better results.  The way that the corpus is 
annotated in Wiebe et al. (2005) study suggests a relatively “objective” method.  The 
annotators were asked to identify all private state expressions in a sentence and to rate 
the strength of those expressions in four levels: low, medium, high, or extreme.  A 
sentence is labeled as “Subjective” only when there is at least one such expression of 
strength medium or higher.  The annotators received the relatively concrete instructions 
about private state rather than about “Subjectivity”.  Taking “Multi-views” as an 
example, we might ask the judges to mark all the different points-of-view in a document.  
We also believe that if we can have better control of the judgments, we will get some 
clues about how to identify middle range documents. 
We are particularly interested in applications of non-topical classification to 
information retrieval situations where users are often looking for texts with particular, 
quite narrow non-topical properties, as in intelligence analysis, health, business.  Because 
of practical limitations, we focused, in this study on six properties identified in the 
HITIQA project.  These six qualitative properties are the product of a focus group study 
with news professionals.  We would like to conduct future experiments to identify the 
important properties in other specific areas to strengthen and extend current work. 
Meanwhile, we would like to expand the candidate feature set to include 
additional textual and linguistic features about documents.  If a larger dataset is available, 
we would like to explore the effectiveness of function words for assessment of non-
topical properties of text.  Function words have been proved useful as features for 
stylistic text attribution.  We did not include individual function word frequencies in our 
 143    
 
feature set, with the concern that the similar size of features and training texts will 
increase the likelihood of overfitting and damaging the reliability of results.  On the other 
hand, in specific areas, such as business, some content words may be good indicators for 
non-topical text properties.  
Our methods of selecting stable features are ad-hoc rather than algorithmic.  For 
logistic regression and SVM, we set a weight threshold; only features with weights larger 
than the threshold are included.  For decision trees, we only count the features in the first 
t levels of the tree; t is the threshold.  These thresholds were set intuitively based on the 
performance.  A more systematic strategy for variable selection would be desirable.  A 
possible method for assessing the relative importance of variables has been introduced in 
Friedman and Popescu’s (2005) recent work on rule ensembles, and may be helpful.  This 
work also provides a possible path to study the relationship between individual modeling 
and general modeling across judges.  If we have judgments from more than one person, 
we can obtain a general model by “ensembling” the individual models.  
The major area for long term future work is to implement applications that use 
non-topical classification to improve the user’s information-seeking experience.  We 
hope to show that the usefulness of retrieval tools can be significantly improved when 
non-topical properties are among the selection criteria that users can exploit.  
 144    
 
References 
Argamon, S., Koppel, M., Fine, J, and Shimoni, A.R. (2003).  Gender, Genre, and 
Writing style in formal written texts. Text, 23(3). 
Barry, C. L. (1994). User-defined relevance criteria: An exploratory study. Journal of 
the American Society of Information Science, 45(3):149-159. 
Bai, B., Ng, K.B., Sun, Y., Kantor, P., and Strzalkowski, T. (2004). "The institutional 
dimension of document quality judgment." In Proceedings of the 2004 
Annual Meeting of American Society for Information Science and 
Technology. 
Bharat, K. and Henzinger, M.R. (1998) Improved algorithms for topic distribution in 
a hyperlinked environment.  ACM SIGIR Conference on Research and 
Development in Information Retrieval. 
Biber, D. (1990). Methodological issues regarding corpus-based analyses of linguistic 
variations. Literary and Linguistic Computing, 5:257--269.  
Biber, D. (1993). The multidimensional approach to linguistic analysis of genre 
variation: An overview of methodology and findings. Computers and the 
Humanities, 26:331-347.  
Biber, D., Conrad, S., and Reppen, R. (1998).  Corpus linguistics : investigating 
language structure and use. Cambridge ; New York : Cambridge University 
Press. 
Burges, C. J. (1998).  A Tutorial on support vector machines for pattern recognition. 
Data Mining and Knowledge Discovery, 2: 121-167. 
Bosch, R. and J. Smith, J. (1998). Separating hyperplanes and the authorship of the 
disputed Federalist Papers, American Mathematical Monthly, 105(7):601-
608. 
 145    
 
Burgess, M., Gray, A., and Fiddian, N. (2002). Establishing a Taxonomy of Quality 
for Use in Information Filtering. Proceedings of the 19th British National 
Conference on Databases: Advances in Databases, 103-113. 
Bruce, H. W. (1994). A cognitive view of the situational dynamism of user-centered 
relevance estimation. Journal of the American Society for Information 
Science, 45(3),142-148. 
Chakrabarti, S., Dom, B., Gibson, D., Kleinberg, J., Raghvan, p., Rajagopalan, S. 
(1998).  Automatic resource compilation by analyzing hyperlink structure and 
associated text.  Computer Networks and ISDN Systems 30, 65-74. 
Chaski, C.E. (2001). Empirical Evaluations of Language-Based Author Identification 
Techniques. Forensic Linguistics 8(1): 1-65 
Cooper, W. S. (1973). On selecting a measure of retrieval effectiveness, part 1: The 
“subjective” philosophy of evaluation. Journal of the American Society for 
Information Science, 24(2): 87-100. 
Cooper, W. S. (1973). On selecting a measure of retrieval effectiveness, part 2: 
Implementation of the  philosophy. Journal of the American Society for 
Information Science, 24(6): 413-424. 
Cunningham, H., D. Maynard, K. Bontcheva, V. Tablan and Y. Wilks. (2000) 
Experience of using GATE for NLP R&D. In Workshop on Using Toolsets 
and Architectures To Build NLP Systems at COLING-2000, Luxembourg. 
DiMarco, C., and Hirst, G. (1993).  A Computational Theory of Goal-Directed Style 
in Syntax.  Computational Linguistics,  19(3), 452-459. 
Dumais, S., Platt, J., Heckerman, D., and Sahami, M. (1998).  Inductive Learning 
Algorithms and Representations for Text Categorization. In CIKM, 148-155. 
Fellbaum,, C. (ed). (1998). WordNet: An Electronic Lexical Database. The MIT 
Press. 
Finn, A., and Kushmerick, N. (2003). Learning to classify documents according to 
genre. IJCAI-03 Workshop on Computational Approaches to Style Analysis 
and Synthesis.  http://citeseer.ist.psu.edu/finn03learning.html 
Friedman, J. H. and Popescu, B. E. (2005). Predictive Learning via Rule Ensembles. 
http://www-stat.stanford.edu/~jhf/ftp/RuleFit.pdf   retrieved in June 2005. 
 146    
 
Giles, T. D. (1990) The Readability Controversy: A Technical Writing Review. 
Journal of Technical Writing and Communication. 20(2):131-138. 
Glover, E.J., Birmingham, M.P., and Gordon, M. D. (1998).  Improving web search 
using utility theory. In Web Information and Data Management (WIDM'98), 
pages 5-8, Bethesda, MD, 1998. ACM. 
Huang, j., Lu, j. and Ling, C.X. (2003).  Comparing Naïve Bayes, Decision trees, and 
SVM with AUC and accuracy. In proceedings of the Third IEEE International 
Conference on Data Mining. (ICDM’03). 
Holmes, D. (1985) The Analysis of Literary Style – A Review.  Journal of the Royal 
Statistical Society, Series A (General), 148(4): 328 – 341. 
Holmes, D. (1998).  The evolution of stylometry in humanities scholarship.  Literary 
and Linguistic Computing, 13:111-117. 
Hoover, D.L.  (2001)  Statistical Stylistics and Authorship Attribution: an Empirical 
Investigation.  Literary and Linguistic Computing, 16(4)  421-444. 
Kantor, P., Boros, E., Melamed Ben, Neu David J., Meñkov Vladimir, Shi Qin, Kim 
Myung-Ho. (1999a) Ant World. In Proceedings of SIGIR'99 (22nd 
International Conference on Research and Development in Information 
Retrieval), 323.  
Kantor, P., Boros, E., Melamed B., Meñkov, V. (1999b) The Information Quest: A 
Dynamic Model of User's Information Needs. In Proceedings of the 62nd 
Annual Meeting of the American Sociaty for Information Science. 36, 536-
545.  
Karlgren, J. and Cutting, D. (1994). Recognizing test Genres with Simple Metrics 
Using Discrimiant Analysis. Proceedings of the 15th International conference 
on Computational Linguistics (COLING ’94). 
Kessler, B., Nunberg, G., and Schutze, H. (1997).  Automatic Detection of Text 
Genre.  Proceedings Of 35th Annual Meeting of the Association for 
Computational Linguistics (ACL.EACL ’97) 32 - 38. 
Kleinberg, J.M. (1998).  Authoritative sources in a hyperlinked environment. In 
Proceedings of 1998 ACM-SIAM Symposium on Discrete Algorithms, San 
Francisco CA, ACM Press. 
 147    
 
McCullagh, P. and Nelder, J.A. (1989).  Generalized Linear Models. Chapter 4, 101-
123. Chapman and Hall, 2nd edition. 
Michos, S. E., Stamatatos, E., Fakotakis, N. and Kokkonakis, G. (1996)  An 
Empirical Text Categorizing Computational Model Based on Stylistic 
Aspects.  Proceedings of the 8th International Conference on Tools with 
Artificial Intelligence (TAI’96), Toulouse, Francs, 71-77. 
Mosteller, F. and Wallace, D. (1984) Applied Bayesian and Classical Inference: the 
Case of The Federalist Papers. 2nd Edition of Inference and Disputed 
Authorship: The Federalist.  Springer-Verlag.  
Naumann, F., Rolker, C.  (2000).  Assessment Methods for Information Quality 
Criteria. In Proceedings of the International Conference on Information 
Quality (IQ), Cambridge, MA, 2000. 
http://citeseer.ist.psu.edu/naumann00assessment.html 
Ng, K.B., Kantor, P., Tang, R, Rittman, R., Small, S., Song, P., Strzalkowski, T. Sun, 
Y. and Wacholder, N. (2003) Identification of effective predictive variables 
for document qualities. In Proceedings of 2003 Annual Meeting of American 
Society for Information Science and Technology. Medford, NJ: Information 
Today, Inc. 
Ng, K.B., Kantor, P., Tang, R, Bai, B., Rittman, R., Song, P., Strzalkowski, T., Sun, 
Y. and Wacholder, N. (Accepted). Automated Judgment of Document 
Qualities. Paper to be appeared in the Journal of American Society for 
Information Science and Technology. 
Oard, D. W. (1997).  The state of the art in text filtering. User Modeling and User-
Adapted Interaction, 7(3), 141-178. 
Pazzani M.,  Billsus D. (1997)  Learning and revising user profiles: the identification 
of interesting web sites.  Machine learning, 27(3): 313-331. 
Price, S. L., Hersh, W. R. (1999) Filtering Web pages for quality indicators: An 
empirical approach to finding high quality consumer health information on 
the World Wide Web.  Proceedings of the AMIA 1999 Annual Symposium, 
911-915. 
Provost, F. and Fawcett, T.  (2001) Robust Classification for Imprecise 
Environments.  Machine Learning, 42(3): 203-231. 
Rudman, J. (1997)  The state of authorship attribution studies: Some problems and 
solutions.  Computers and the Humanities, 31(4): 351-365. 
 148    
 
Schamber, L. (1991). User’s criteria for evaluation in a multimedia environment. In 
Proceedings of the American Society for Information Science, 126-133, 
Washington, DC. 
Stamatatos, E., Fakotakis, N. and Kokkonakis, G. (1999).  Automatic Authorship 
Attribution.  Proceedings of the 9th Conference of the European Chapter of 
the Association for the Computational Linguistics (EACL’99), Bergen, 
Norway, 158-164. 
Stamatatos, E., Fakotakis, N. and Kokkonakis, G. (2000). Text Genre Detection 
Using Common Word Frequencies. Proceedings Of the 18th International 
Conference on Computational Linguistics (COLONG2000). 
Tang, R., Ng, K.B., Strzalkowski, T., Kantor, P.B. (2003).  Toward machine 
understanding of information quality.  Proceedings of 2003 Annual Meeting 
of American Society for Information Science and Technology, 40, 213-220. 
Tweedie, F.  and Baayen, R. (1998) How variable may a constant be? Measure of 
lexical richness in perspective.  Computers and the Humanities, 32(5):323-
352. 
Vickery, A., Brooks, H.M., Robinson, B., and Vickery, B.C. (1987). A Reference and 
Referral System Using Expert System Techniques. Journal of Documentation 
43: 1-23. 
Wang, R.Y., Strong, D.M.  (1996). Beyond accuracy: What data quality means to 
data consumers.  Journal on Management of Information Systems, 12(4): 5-
34. 
WEKA Documentation. http://www.cs.waikato.ac.nz/~ml/weka/ Retrieved Dec 2004. 
Wiebe, J. (2000). Learning subjective adjective from corpora. Proc. 17th National 
Conference on Artificial Intelligence (AAAI-2000). Austin, Texas 
Wiebe, J. and Riloff, E. (2005). Creating subjective and objective sentence classifiers 
from unannotated texts. Sixth International Conference on Intelligent Text 
Processing and Computational Linguistics (CICLing-2005)  
Witten, I.H., Frank, E.  (1999).  Data Mining: Practical Machine Learning Tools and 
Techniques with Java Implementations.  Morgan Kaufmann. 
 149    
 
Whitelaw, C., and Argamon, S. (2004). Systemic Functional Features in Stylistic 
Text Classification.  AAAI 2004 Fall Symposium Series: Style and Meaning 
in Language, Art, Music and Design.  
Wolters, M., and Kirsten, M. (1999). Exploring the Use of Linguistic Features in 
Domain and Genre Classification.  In Proceedings of EACL ’99. 
Zhu, X., and Gauch, S. (2000). Incorporating quality metrics in 
centralized/distributed information retrieval on the world wide web. In 
Proceedings of the 23rd Annual International ACM/SIGIR Conference, pages 
288-- 295, Athens, Greece, 2000. 
http://citeseer.ist.psu.edu/zhu00incorporating.html   
 150    
 
Appendix 1: Definitions of Six Non-Topical Properties 
(Tang et al. 2003) 
 
Accuracy: The extent to which information is precise and free from known errors. 
 
Reliability: The extent to which you believe that the indicated sources in the text (e.g., 
interviewees, eye-witnesses, etc.) provide truthful account of the story. 
 
Objectivity: The extent to which the document includes facts without distortion by 
personal or organizational biases. 
 
Depth: The extent to which the coverage and analysis of information is detailed. 
 
Verbosity  Conciseness: The extent to which information is well-structured and 
compactly represented. 
 
One-sided  Multi-views: The extent to which information reported contains a variety of 
data sources and view points. 
 151    
 
Appendix 2: Language Features 
Character Level Features 
 
Punctuation Marks & Symbols: 
dbquote  "  --   literal double quotes 
mark#  #      --   # sign 
mark$  $      --   dollar sign 
add_p  + --   plus sign 
right_p  (      --   literal left parenthesis 
left_p  )      --   literal right parenthesis 
comma  ,      --   literal comma 
period  .      --   period 
colon  :      --   colon  
question   ? --   question mark 
exclamat   ! --   exclamation mark 
 
Length: 
avg_word   --  average length of words in characters 
avg_sent   --  average length of sentence in words 
avg_para   --  average length of paragraph in words 
d_len  --  document length in words 
 
Lexical Level Features 
 
Entities (Word lists): 
tit_civ  --  title, civilian 
tit_mil  --  title, military 
tit_pol    --  title, police 
loc_city  --  location, city 
loc_reg  --  location, region 
loc_prov   --  location, province 
loc_coun    --  location, country 
org_com   --  organization, company 
org_gov    --  organization, government 
org_dep  --  organization, department 
jobtitle    --  job title 
date_fes   --  date, festival 
currency   --  currency_unit 
 152    
 
date_key   --  date_key, e.g. "today", "yesterday" and "tomorrow" 
dateunit    --  date_unit, e.g. "day", "week" 
ident_ke   --  ident_key, e.g. "id", "ref" 
loc_gk  --  loc_general_key, e.g. "village", "city" 
loc_key  --  loc_key, e.g. "sea", "ocean" 
cdg    --  cdg, e.g. "Company", "INC" 
stop    --  stop word 
pers_ful   --  person_full, e.g. "Bach", "Bill Clinton" 
pers_fir    --  person_first. e.g. "Zoe", "Zarah", "Yann" 
org_base   --  org_base, e.g. "Bank", "AGENCY" 
org_key  --  org_key, e.g. "Circus", "Electric", "Fire Rescue" 
time_uni   --  time_unit, e.g. "hours", "minutes" 
year    --  year, e.g. "1996", "2001" 
person   --  Person 
location    --  Location 
org    --  Organization 
money    --  Money 
date    --  Date 
 
Declarative Words: 
declarat   --  R list: count of declarative verbs 
dec_mi  --  R- list: count of hyponyms of words in R list found by 
wordnet and are included in R list 
dec_mipl --  R-+  list: count of hyponyms not in R list 
 
Unique Words: 
person_u  --  Number of distinct persons 
org_u    --  Number of distinct organizations 
loc_u    --  Number of distinct locatins 
money_u --  Number of distinct money (probably not useful) 
date_u  --  Number of distinct dates 
uniqword --  number of unique words 
uniq_non --   number of unique words excluding stop words  
 
Key Terms: 
say    --  number of word "say" 
seem    --  number of word "seem" 
expert  --  number of word "expert" 
  
 
Structural Level Features 
 
POS tags: 
CC  --    coordinating conjunction 
CD  --    cardinal number 
DT  --    determiner 
 153    
 
EX  --    existential “there” 
FW  --    foreign word 
IN        --    preposition or subordinating conjunction 
JJ        --    adjective 
JJR       --    adjective - comparative 
JJS       --    adjective - superlative 
LS        --    list item marker 
MD        --    modal auxiliary 
NN        --    common noun - singular or mass 
NNP      --    proper noun - singular 
NNPS    --    proper noun - plural 
NNS      --    common noun - plural 
PDT      --    predeterminer 
POS      --    possessive ending, genitive marker 
PRP      --    personal pronoun 
PRP$    --    possessive pronoun  
RB        --    adverb 
RBR     --    adverb - comparative 
RBS     --    adverb - superlative 
RP        --    particle 
SYM     --    symbol 
TO        --    “to” as preposition or infinitive marker 
UH        --    interjection 
VBD      --    verb - past tense 
VBG      --    verb - gerund or present participle 
VBN      --    verb - past participle 
VBP      --    verb - non-3rd person singular present 
VB        --    verb - base form 
VBZ      --    verb - 3rd person singular present 
WDT     --    wh-determiner 
WP$      --    possessive wh-pronoun 
WP        --    wh-pronoun 
WRB     --    wh-adverb 
 
vb_all  -- the sum of (VBD, VBG, VBN, VBP, VB, and VBZ) 
noun  -- the sum of (NN, NNP, NNPS, NNS) 
 
Derivative Features 
 
Ratios: 
r_person  -- person / person_u  
r_date  --  date / date_u 
r_org  --  org / org_u 
r_loc  -- loc / loc_u 
r_money  --  money / money_u 
vb_noun  -- vb_all / noun 
 154    
 
adj_noun -- (JJ + JJR + JJS) / noun   
adv_vb   --  (RB + RBR + RBS) / vb_all 
 
quotelen   --  average quotation length 
 
d_back  --  Accumulated link distance for backward situation 
num_back --  Number of backward situation 
d_forw  --  Accumulated link distance for forward situation 
num_forw --   Number of forward situation 
 
 155    
 
Appendix 3: Experimental Scripts 
work.sh 
 
#!/bin/bash 
for i in 0 1 2 3 4 5 6 7 8 9 
do 
 
# Creating one pair of half-half split training and test sets  
 
java weak.filters.unsupervised.instance.RemoveFolds –V –N 2 –S $i –i dataset.arff 
–o train.$i.arff 
java weak.filters.unsupervised.instance.RemoveFolds –N 2 –S $i –i dataset.arff –o 
test.$i.arff 
    
# Decision tree C4.5 learning on one pair of half-half split training and test sets 
java weka.classifiers.trees.J48 -i -t train.$i.arff -T /test.$i.arff  > tree/$i.tree.txt  
 
# Logistic Regression learning on one pair of half-half split training and test sets 
 java weka.classifiers.functions.SimpleLogistic -i -H 30 -t train.$i.arff -T test.$i.arff  
> logistic/$i.function.txt 
 
# SVM learning on one pair of half-half split training and test sets 
 java weka.classifiers.functions.SMO -i -t train.$i.arff -T test.$i.arff  > svm/$i.txt 
 
# using perl script to obtain “importance” features from each learned model 
../tools/featureVote.perl logistic/$i.function.txt  >> logistic/votes.txt  
../tools/svm-feature.perl svm/$i.txt >> svm/votes.txt 
../tools/treeTop.perl tree/$i.tree.txt >> tree/votes.txt  
 
done 
 
# count the features’ frequencies in 10 models using PERL script 
../tools/count.perl tree/votes.txt |sort -nrk 2 > tree/c_votes.txt 
../tools/count.perl logistic/votes.txt |sort -nrk 2 > logistic/c_votes.txt 
../tools/count.perl svm/votes.txt |sort -nrk 2 > svm/c_votes.txt 
 
# create Weka required .arff format data files, each file contains only the features that are 
stable in at least $THRESHOLD models  
 ../tools/toArff5.perl ../data.dat $aspect tree/c_votes.txt $THRESHOLD > tree/vote.arff 
 156    
 
  ../tools/toArff5.perl ../data.dat $aspect  logistic/c_votes.txt $THRESHOLD > 
logistic/vote.$j.arff 
   ../tools/toArff5.perl ../data.dat $aspect svm/c_votes.txt $THRESHOLD > svm/vote.arff 
 
# C4.5 10-fold cross validation, with the selected features  
 java weka.classifiers.trees.J48 -i -R –d tree/model.txt -t tree/vote.arff  > 
tree/prune_vote.tree.txt 
 
# Logistic regression 10-fold cross validation, with the selected features  
 java weka.classifiers.functions.SimpleLogistic -i -d logistic/model.txt -t logistic/vote.arff 
>  logistic/vote.function.txt 
 
# SVM 10-fold cross validation, with the selected features  
 java weka.classifiers.functions.SMO -i -d svm/model.txt -t svm/vote.arff  >  
svm/vote.function.txt 
 
 157    
 
Curriculum Vita 
 
Ying Sun 
 
EDUCATION: 
1999-2005 Rutgers University, New Brunswick, NJ  
School of Communication, Information & Library Studies,  
Ph.D. Information Science, October 2005 
 
2002-2003 Rutgers University, Piscataway, NJ  
Department of Computer Science,  
M. S. – Computer Science, October 2003  
 
1996-1999 Peking University, Beijing, China  
Department of Library and Information Studies  
Master of Library and Information Studies, June 1999 
 
1991-1996 Peking University Beijing, China  
Major: Information Studies  
Bachelor of Science, June 1996 
PUBLICATIONS: 
Sun, Y., Kantor, P., Strzalkowski, T., Rittman, R., and Wacholder, N. (2004). Cross 
Evaluation – a pilot application of a new evaluation mechanism. In Proceedings of 
the 2004 Annual Meeting of American Society for Information Science and 
Technology.383-392. 
Bai, B., Ng, K.B., Sun, Y., Kantor, P., and Strzalkowski, T. (2004). "The 
institutional dimension of document quality judgment." In Proceedings of the 
2004 Annual Meeting of American Society for Information Science and 
Technology. 110-118 
Belkin,N.J., Chaleva, I., Cole, M., Li, Y.-L., Liu, L., Liu, Y.-H., Muresan, G., Smith, 
C.-L., Sun, Y., Yuan, X.-J. and Zhang, X.-M. (2003) Rutgers' HARD Track 
Experiments at TREC 2004, in Proceedings of TREC 2004, Gaithersburg, 
November 2004.  
Rittman, R., Wacholder, N., Kantor, P., Ng, K.B., Strzalkowski, T., Sun, Y. (2004). 
Adjectives as indicators of subjectivity in Documents. In Proceedings of the 2004 
Annual Meeting of American Society for Information Science and Technology. 
349-359. 
Wacholder, N., Small, S., Bai, B., Kelly, D., Rittman, R., Ryan, S., Salkin, R., Song, 
P., Sun, Y., Ting, L., Kantor, P., & Strazalkowski, T. (2004). Designing a realistic 
evaluation of an end-to-end interactive question answering system. In 
 158    
 
Proceedings of the 4th International Conference on Language Resources and 
Evaluation (LREC '04), Lisbon, Portugal. 
Ng, K.B., Kantor, P., Tang, R, Rittman, R., Small, S., Song, P., Strzalkowski, T. 
Sun, Y. and Wacholder, N. (2003) Identification of effective predictive variables 
for document qualities. In Proceedings of 2003 Annual Meeting of American 
Society for Information Science and Technology. Medford, NJ: Information Today, 
Inc. 
Tang, M.-C. Sun, Y. (2003). Evaluation of Web-Based Search Engines Using User 
Effort Measures. Library and Information Science Research Electronic Journal 
13(2). 
Belkin, N. J., Keller, A. M., Kelly, D., Perez-Carballo, J., Sikora, C., & Sun, Y. 
(2001). Support for question-answering in interactive information seeking: The 
Rutgers TREC-9 interactive track experience. In D. Harman & E. Voorhees (Eds.), 
TREC-9, Proceedings of the Ninth Text Retrieval Conference. Washington, D.C.: 
GPO.  
  
 
 
