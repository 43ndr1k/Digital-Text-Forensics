7Revista Opus 12 - 2006
A PESQUISA EMPÍRICA EM EXPRESSIVIDADE
MUSICAL: MÉTODOS E MODELOS DE
REPRESENTAÇÃO E EXTRAÇÃO DE INFORMAÇÃO
DE CONTEÚDO EXPRESSIVO MUSICAL
Mauricio Alves Loureiro
Resumo: Este texto busca rever recentes abordagens metodológicas na investigação da
expressividade na performance musical a partir da extração de informação do próprio som, visando
identificar os parâmetros acústicos capazes de descrever o conteúdo expressivo. Este problema
vem despertando interesse de musicólogos, psicólogos, cientistas de computação, engenheiros
e físicos, há quase duas décadas. Um grande número de pesquisa quantitativa em diferentes
aspectos da expressividade musical vem sendo realizado com base em medições de parâmetros
acústicos visando identificar e quantificar a correlação entre variações destes parâmetros e as
intenções do intérprete para comunicar ao ouvinte diferentes aspectos da música que eles tocam.
Palavras Chave: Expressividade musical. Extração de informação musical. Análise empírica da
performance musical.
Abstract: Recent methodological approaches for research in musical expressiveness is here
presented. Starting from music content information extracted from audio recordings of performed
music, this category of  investigation seeks to identify the acoustic parameters capable of describing
the expressive content of a musical performance. Since almost two decades, a great number of
quantitative research on different aspects of musical expressiveness has been accomplished by
musicologists, psychologists, computer scientists, engineers and physicists, based on
measurements of acoustic parameters, in an attempt to identify and to quantify the correlation
between variations of these parameters and the musician’s intentions to communicate to the listener
different aspects of the music they play.
Keywords: Musical Expressiveness. Empirical Analysis of Musical Performance.
Introdução
sDurações, dinâmicas e timbres de uma performance musical variam
consideravelmente entre diferentes intérpretes e mesmo entre duas
performances do mesmo intérprete. Os valores exatos de duração
das notas indicados na partitura nunca são executados, enquanto
que dinâmicas e timbres são especificados subjetivamente por
instruções verbais tais como piano, fortissimo, crescendo,
diminuendo ou adjetivos e locuções relacionadas às intenções
expressivas do compositor. Diferenças entre performances são
percebidas com uma clareza surpreendente, mesmo por ouvintes
8 Revista Opus 12 - 2006
não especializados, o que faz com que uma performance
tecnicamente perfeita, mas inexpressiva, seja quase sempre menos
apreciada que uma interpretação expressiva da mesma partitura,
ainda que contenha alguns erros ou imprecisões. Investigar os
fatores determinantes destas características perceptivas tão
evidentes tem sido o propósito da pesquisa sobre a expressividade
conduzida ou percebida em uma performance musical.
Estudos sobre performance musical estão necessariamente
relacionados à percepção envolvendo questões sobre os
mecanismos de transmissão e percepção de elementos básicos da
música tais como ritmo, tempo, altura, tonalidade, intensidade,
timbre, assim como de agrupamento de notas, frases ou estruturas
maiores, ou mesmo mais abstratas tais como expressividade,
emoção e afeto. No final do século XIX, alguns estudos já se
voltavam para esse tipo de problema, mas foi somente a partir de
meados do século XX que inovações tecnológicas, em especial a
computação científica, viabilizaram metodologias de análise musical
que pudessem partir do material acústico (sinal de áudio),
oferecendo à musicologia possibilidades diferenciadas para abordar
problemas mais próximos à percepção da música. A possibilidade
de extrair diretamente do som categorias de informação de conteúdo
musical nunca antes imaginadas há 50 anos atrás, permitiu o avanço
relativamente recente da pesquisa voltada para a compreensão da
emoção e da expressividade. A partir de representações que utilizam
sistematicamente processamento digital de sinais, modelagem
computacional e estatística, esta categoria de pesquisa busca
mapear estruturas expressivas e estabelecer relações com a
informação extraída do “estímulo musical”.
ESTADO DA ARTE DA PESQUISA EM EXPRESSIVIDADE
MUSICAL
Antecedentes
As primeiras pesquisas empíricas em performance musical foram
9Revista Opus 12 - 2006
conduzidas por Binet e Courtier (1895) no final do século XIX, que
conseguiram registrar a força de pressionamento da tecla do piano,
utilizando um pequeno tubo de borracha posicionado embaixo das
teclas. Pulsos de ar eram formados quando o tubo era pressionado,
os quais controlavam uma agulha que registrava a ação em um
papel em movimento. Este dispositivo possibilitou investigar a
execução de trinados, acentos e variações de dinâmica. O estudo
foi capaz de identificar padrões de ações conduzidas por pianistas
para realizar gestos expressivos, como por exemplo, um acento –
além de imprimir maior tensão na tecla acentuada, o intérprete toca
a nota precedente mais destacada e a nota acentuada um pouco
alongada e mais ligada à nota seguinte (citado em Gabrielsson,
1999, p. 525). Dispositivos eletromecânicos foram utilizados por
Ebhardt (1898) para registrar a pressão nas teclas do piano, que
também identificaram alongamentos em notas acentuadas e por
Sears (1902), que mediu variações na duração de notas de mesmo
valor, na duração de compassos e nas proporções entre durações
de notas de valores distintos tocadas por organistas (citado em
Gabrielsson, 1999, p. 525-526). Um grande número de estudos em
performance musical foi desenvolvido nas décadas de 1920 e 1930,
destacando-se os trabalhos do grupo de pesquisa da Universidade
de Iowa, liderado por Seashore, os quais registraram um enorme
volume de dados coletados de performances no piano, violino e
canto, publicados em vários volumes (Seashore, 1932, 1936, 1937;
Skinner e Seashore, 1937; Seashore, 1938; citado em Gabrielsson,
1999, p. 527).
A partir destas primeiras investigações, a pesquisa em performance
musical tem se desenvolvido em direção à compreensão deste
complicado fenômeno que envolve não apenas o comportamento
do instrumentista frente ao texto que interpreta, mas também os
mecanismos de percepção envolvidos na escuta. Um grande número
de pesquisa quantitativa em diferentes aspectos da expressividade
musical demonstraram que músicos comunicam ao ouvinte, uma
10 Revista Opus 12 - 2006
variedade de características da música que eles interpretam a partir
de pequenas variações de durações, articulações, intensidades,
alturas e timbres.
Repp (1990) mediu as diferenças entre performances de diferentes
pianistas, mas evidenciou também em estudo posterior (Repp, 1992)
inúmeros fatores comuns entre eles, relacionando-os à estrutura
da obra interpretada. Sundberg et al. (1991a) buscou identificar
parâmetros acústicos envolvidos em uma performance musical, com
a finalidade de quantificar “as pequenas e grandes variações de
tempo, dinâmica, timbre e afinação, as quais formam a
microestrutura de uma performance e diferenciam performances
distintas da mesma partitura” (Palmer, 1997, p. 118). Uma vez
quantificadas estas variações, o passo seguinte seria entender onde
reside o “impacto emocional” de uma performance, e como este
impacto é conduzido, tal como sugerido por Gabrielsson (1995) e
Juslin (1997; 2000). Descrever e reconhecer classes de padrões
que possam elucidar a influência destes parâmetros na
expressividade percebida, quase sempre se utilizando-se de Análise
Estatística e Modelagem Computacional, tem sido o foco de um
grande número de estudos em performance musical.
Intenção Expressiva do Intérprete
Desvios de tempo, dinâmica, articulação e timbre não escritos na
partitura e introduzidos pelo intérprete, variam de acordo com a
obra, com o instrumento e com o intérprete. Alguns estudos
buscaram parametrizar as intenções expressivas individuais de
diferentes intérpretes e verificar se tais intenções são percebidas a
partir da mesma codificação (Clarke, 1993; Gabrielsson, 1999).
Seashore já teria sugerido uma possível abordagem que examinasse
como o ouvinte extrai do som as intenções do intérprete, afirmando
que “as relações psicofísicas entre o intérprete  e o ouvinte são
fundamentais para a compreensão das microestruturas da
performance musical” (Seashore, 1938). Alguns estudos abordaram
11Revista Opus 12 - 2006
o problema por esta via, como por exemplos os trabalhos em
percepção musical e aspectos emocionais da performance de
Sloboda (1985), que evidenciaram uma correlação entre as
intenções do intérprete e a percepção dos ouvintes.
Senju e Ohgushi (1987) analisaram 10 performances no violino
realizadas em diferentes nuances, especificadas por denominações
subjetivas tais com fraco, poderoso, brilhante, triste, sofisticado,
belo, como um sonho, elegante, simples, profundo e confirmaram
a correlação entre intenção e percepção observada por Sloboda. A
capacidade dos ouvintes de identificar claramente determinadas
performances como expressivas ou não e de reconhecer
determinadas intenções expressivas, permitiu que Repp (1992)
acreditasse na existência de princípios objetivos que determinam
se uma performance é ou não expressiva ou “musical”.
Gabrielsson e Juslin (1996) lograram evidenciar relações
consistentes entre resultados de testes de percepção de intenções
expressivas e parâmetros acústicos de diferentes performances de
nove músicos profissionais, instruídos para tocar em diferentes
nuances expressivas a partir dos adjetivos alegre, triste, raivoso,
amedrontado, suave, solene e inexpressivo. Canazza, De Poli et
al. (1997) estudaram também a correlação entre a intenção
percebida e parâmetros acústicos extraídos de sete performances
distintas do Concerto em Lá Maior para clarineta e orquestra de
Mozart, diferenciadas por adjetivos sensoriais (duro, mole, pesado,
leve, brilhante, escuro), identificando parâmetros acústicos
determinantes de diferenciações específicas entre as execuções,
além de confirmar a correlação entre os dados relacionados às
intenções do intérprete e à percepção dos ouvintes. Uma validação
complementar foi realizada através de performances sintetizadas a
partir dos parâmetros determinados, com resultados bastante
satisfatórios (Canazza, De Poli e Vidolin, 1997).
12 Revista Opus 12 - 2006
Os resultados destes estudos evidenciaram a complexidade do
problema frente às inúmeras possibilidades que o intérprete pode
escolher para transmitir sua intenção expressiva. Diferentes
motivações expressivas são às vezes transmitidas por efeitos
acústicos similares, do mesmo modo que diferentes efeitos acústicos
podem levar à mesma idéia expressiva, dificultando a formalização
do problema de identificar as causas de características perceptivas
tão evidentes.
Modelos Baseados em Aspectos Específicos da Performance
Vários estudos se dedicaram à modelagem de aspectos específicos
da expressividade musical, como por exemplo: as correlações entre
o ritardando final e o movimento humano (Kronman e Sundberg,
1987; Todd, 1995; Friberg e Sundberg, 1999; Friberg, Sundberg et
al., 2000; Sundberg, 2000; Honing, 2003); a duração de notas de
appogiaturas (Timmers, Ashley et al., 2002); o vibrato (Desain e
Honing, 1996; Shoonderwaldt e Friberg, 2001); a articulação,
envolvendo qualidade de ataque, legato e staccato e suas relações
com o contexto musical local (Dannenberg e Derenyi, 1998; Bresin
e Battel, 2000; Bresin e Widmer, 2000; Bresin, 2001). Sundberg e
Verrillo (1980) identificaram o problema da diversidade de
segmentação de frases feita por cada intérprete individualmente,
para a mesma partitura e propôs que cada intérprete delimita o
início e final das frases a partir de desvios de tempo (rallentando ou
accellerando). Todd  propôs um modelo computacional para os
desvios temporais (Todd, 1985) e de intensidade (Todd, 1992) que
enfatizariam a hierarquia das frases musicais. Utilizando equações
da cinemática o modelo estabelece relações entre variações de
tempo da performance e o comportamento de um corpo em
movimento (Todd, 1995). Clynes (1995) propôs um modelo a partir
da obra interpretada, formalizando padrões de variação de tempo
relacionados a compositores  específicos. Algumas abordagens
utilizaram medições dos gestos físicos do intérprete, como a de
Askenfelt (1986) que mediu o movimento do arco de um violinista
13Revista Opus 12 - 2006
tocando o concerto de Beethoven uma vez suave e outra vez
agressivo e constatou que a versão agressiva foi caracterizada por
uma maior força média do arco sobre a corda e ataques mais
abruptos.
Modelos de Regras Gerativas de Performance.
A idéia de que músicos manipulam parâmetros expressivos de forma
estruturada, previsível e relacionada com a estrutura da música,
vem sendo investigada desde as pesquisas de Seashore (1938).
Recentes estudos buscam formalizar modelos computacionais com
vistas a possibilitar predições a respeito da expressividade de uma
performance musical, envolvendo não apenas a especificação
precisa de parâmetros físicos para descrever a performance, mas
também a quantificação das relações entre os valores medidos a
partir da formalização de regras. Clarke (Clarke, 1988) apresentou
pela primeira vez um modelo generalizado para a expressividade
musical, que propunha um conjunto de nove regras gerativas de
estruturas de agrupamento de tempos e intensidades capazes de
predizer aspectos expressivos da performance a partir de informação
obtida exclusivamente da partitura.
Um sistema de regras quantitativas para estudar a performance
musical vem sendo desenvolvido no Royal Institute of Technologie
(KTH) de Estocolmo há mais de 20 anos. As regras foram
estabelecidas a partir de determinações dos desvios de valores de
parâmetros em relação aos valores nominais da partitura. Estes
desvios foram determinados através da manipulação de parâmetros
tais como tempo, intensidade, afinação e vibrato a partir de valores
teóricos de proporções, tais como entre durações ou intensidades
de notas sucessivas, definidos para um conjunto limitado de classes
de situações musicais, como por exemplo linhas e saltos melódicos,
progressões harmônicas ou estruturas de  frases. Este modelo foi
desenvolvido a partir de métodos de análise-síntese envolvendo
músicos profissionais na avaliação de um grande número de tais
14 Revista Opus 12 - 2006
regras apresentadas pelos  pesquisadores (Sundberg, Askenfelt et
al., 1983; Sundberg, Frydén et al., 1983; Sundberg, Friberg et al.,
1989; Friberg, 1991b, 1991a; Friberg, Frydén et al., 1991; Sundberg,
Friberg et al., 1991b, 1991a; Sundberg, 1993; Friberg, 1995; Friberg,
Bresin et al., 1998; Sundberg, Friberg et al., 2003). Todd propôs um
modelo de regras baseado em uma estrutura de diferentes níveis
expressivos de tempo e dinâmica (Clarke, 1985; Shaffer, Clarke et
al., 1985; Todd, 1985; Gabrielsson, 1987; Shaffer e Todd, 1987;
Todd, 1989a, 1989b; Repp, 1992; Todd, 1992).
O grupo de pesquisa liderado por Gehard Widmer, do Instituo de
Pesquisa Österreichisches Forschungsinstitut für Artificial
Intelligence - ÖFAI de Viena, desenvolveu um modelo baseado em
técnicas de machine learning e data mining, para o reconhecimento
automático de padrões de parâmetros descritores de expressividade
em um grande volume dados. O modelo foi capaz de reconhecer
performances de artistas tais como Rubisntein, Maria João Pires,
Horowitz e Maurizzio Pollini (Widmer, Dixon et al., 2003; Zanon e
Widmer, 2003; Goebl, Pampalk et al., 2004). A robustez do modelo
na classificação e reconhecimento de interpretações, tanto de
músicos profissionais como estudantes, demonstrou sua capacidade
de descrever de quantificar uma performance objetivamente
(Widmer, 1995a, 1995b, 1996, 2000, 2001; Stamatatos, 2002;
Stamatatos e Widmer, 2002; Widmer, Dixon et al., 2003; Widmer e
Tobudic, 2003; Zanon e De Poli, 2003; Widmer e Goebl, 2004). Os
pesquisadores desenvolveram uma ferramenta denominada
Performance Worm, a partir da qual intensidade e duração são
desenhadas em tempo real, fornecendo uma visualização eficiente
de uma “trajetória expressiva” do intérprete. A Figura 1 mostra um
exemplo da  trajetória obtida da performance dos 30 primeiros
compassos do Prelúdio op. 23 No. 6 de Rachmaninov, interpretado
por Vladimir Ashkenazy. O eixo vertical mostra o volume em sones
e o horizontal, o tempo em batidas por minuto, BPM (Goebl, Pampalk
et al., 2004).
15Revista Opus 12 - 2006
FIG.  1: “Performance Worm” de 30 compassos do Prelúdio op. 23 No. 6 de Rachmaninov,
tocado por Vladimir Ashkenazy: tempo em BPM (horiz.); volume em sones (vert.) (Goebl,
Pampalk et al., 2004).
Excelentes revisões bibliográficas sobre a pesquisa em performance
musical e expressividade foram elaboradas por Palmer (1997) e
Gabrielson (1999; 2003), as quais reúnem praticamente toda a
referência bibliográfica da área até o final do século XX.
MODELOS E MÉTODOS DE AQUISIÇÃO DE DADOS.
Medir objetivamente a expressividade musical, envolve a
identificação e medição de parâmetros físicos que possam
representar os recursos utilizados pelo intérprete para comunicar
sua intenção expressiva. O problema é formalizado a partir modelos
de representação que buscam quantificar as intenções expressivas
do intérprete através de um conjunto de parâmetros descritores,
definidos e calculados a partir de informação extraída do sinal de
áudio de gravações da performance. O primeiro passo é determinar
que tipo de informação será utilizada e como será extraída do sinal,
para em seguida definir os parâmetros descritores da performance
ou de aspectos dela a serem investigados.
16 Revista Opus 12 - 2006
Parâmetros descritores da Expressividade Musical.
Assim como a partitura representa a música por meio de uma
seqüência de notas que especifica a altura, a posição temporal, a
intensidade e a instrumentação de cada uma, a performance pode
também ser considerada como uma lista de notas sucessivas,
contendo o mesmo tipo de especificação, porém em maiores níveis
de detalhamento e precisão, acompanhada ainda de informação
adicional sobre cada nota. Nesta representação, as alturas são
estimadas a partir de medições objetivas, ao invés de assumir os
valores quantizados das escalas musicais especificados na partitura.
As durações são extraídas dos instantes de inicio e fim da nota,
medidos do envelope de amplitude.
A metodologia de extração da informação depende dos aspectos
específicos que se quer investigar e da viabilidade técnica. No nível
acústico, a informação disponível está normalmente relacionada à
duração, altura e intensidade dos eventos e à disposição seqüencial
destes eventos. Este tipo de informação é facilmente mensurável
em instrumentos de teclado, pois correspondem exatamente àqueles
parâmetros básicos do protocolo MIDI e podem ser facilmente
obtidos utilizando-se um Disklavier, um piano acústico com
capacidade de registrar os movimentos dos martelos do piano,
fornecendo os instantes de início e fim da nota (note-on e note-off),
níveis de intensidade (velocidade do martelo) e instantes de inicio
e fim de pedal em formato MIDI. Em instrumentos de sopro, cordas
e voz esta informação só pode ser extraído do sinal de áudio da
gravação da performance e nesse caso, outros tipos de informação
acústica podem ser levados em conta, como por exemplo
distribuição espectral, vibrato e afinação (pequenas variações de
altura). No entanto, os procedimentos envolvidos na aquisição de
dados requerem metodologias bem mais complexas, especialmente
para situações polifônicas.
Uma grande diversidade de métodos de extração e processamento
17Revista Opus 12 - 2006
de informação de conteúdo musical pode ser encontrada na literatura
e a grande divergência de abordagem entre eles mostra que não
existe ainda um padrão metodológico para este tipo de
procedimento. Dependendo do que se quer medir diferentes
abordagens podem resultar em estimações bem contrastantes. A
duração do tempo de ataque da nota, por exemplo, é um parâmetro
importante na descrição da articulação e consequentemente
significativo na determinação do perfil expressivo da performance.
Valores estimados de duração de ataque, normalmente da ordem
de dezenas de milissegundos, podem variar significativamente
dependendo dos critérios adotados de limiares mínimos de amplitude
na definição de início e fim da nota, ou do método de estimação do
envelope da nota.
Parâmetros Descritores do Envelope da Nota.
O envelope de amplitude corresponde ao contorno de amplitude
de um som, geralmente dividido em três seções, o ataque, a parte
sustentada e o decaimento. A estimação do envelope é mais
comumente feita através da medida RMS - root mean square - RMS
(raiz da média dos quadrados), que está relacionada com a potência
média de curta duração do sinal. Não existe uma metodologia
padronizada de estimação do envelope de amplitude da nota, que
pode variar muito de acordo com o tipo de dado do problema
investigado. O cálculo do envelope a partir do valor de pico de
amplitude, resulta em variações muito bruscas do valor da amplitude
ao longo de intervalos de tempo curtos demais para que possam
ser percebidos. Por outro lado o valor RMS é perceptualmente mais
relevante, se aproximando mais da maneira como percebemos a
intensidade de um som. O valor RMS de um sinal, x é dado por:
onde k é o número da amostra (instante de temo) e L o tamanho da
janela de cálculo da média, o qual determina a resolução temporal
18 Revista Opus 12 - 2006
do envelope. Janelas mais longas suavizam o envelope e podem
dificultar a detecção correta da região do ataque. É nesta região
que ocorrem grandes flutuações de energia conhecidas como
transientes, sendo por isso muito relevante para a estimação de
parâmetros relacionados à articulação das notas. Janelas mais
curtas salientam a região do ataque, mas podem diminuir a precisão
da detecção do instante final do ataque e de início da parte
sustentada. Uma suavização do envelope através de um filtro passa
baixa de freqüência de corte adequada pode também facilitar a
detecção de cada um dos instantes relevantes do envelope.
Os instantes O(n) e F(n), mostrados na Figura 2, correspondentes
ao início (onset) e ao final de uma nota n, respectivamente, são
extraídos do envelope de amplitude da nota, a partir de estimações
de limiares adequados de amplitude. A segmentação de notas
consecutivas pode não ser viável apenas a partir de níveis de
amplitude, como por exemplo de notas ligadas, gravadas em
ambiente com reverberação, em que o instante de final de nota
pode coincidir com o instante de início da nota seguinte. Nesses
casos, há necessidade também de detectar a altura da nota, que
pode ser feita por técnicas de autocorrelação ou a partir de
processamento no domínio da freqüência.
A Figura 2 mostra também o instante de final de ataque A(n), utilizado
no cálculo de parâmetros descritores relacionados à articulação e
à percepção do timbre da nota. Não existe na literatura um método
de medição que possa descrever inequivocamente o ataque (Park,
2004). Em muitas situações, o instante de final de ataque refere-se
ao instante de amplitude máxima da nota, mas sua detecção pode
demandar procedimentos mais complexos, como por exemplo em
instrumentos não percussivos ou não pinçados como sopros, cordas
e voz, nos quais a amplitude máxima poder ser atingida bem depois
do ataque, ao longo da parte sustentada da nota. Pode-se contornar
o problema estabelecendo-se um valor adequado de limiar máximo
19Revista Opus 12 - 2006
FIG. 2: Instantes de início de nota (O), de final de nota (F) e de final de ataque (A), intervalo
entre inícios de notas sucessivas (IOI), duração da nota (DN) e duração de ataque (DA),
extraídos do envelope de amplitude da nota (adaptado de De Poli, 2004).
de amplitude, mas que  pode não apresentar resultados consistentes
para notas mais longas tocadas nestes instrumentos. Um método
alternativo é apresentado por Tae Hong Park (2004), que detecta
picos de amplitude a partir de variações bruscas em uma versão do
envelope RMS suavizado por um filtro passa-baixa. O instante de
final de ataque é definido como o pico de amplitude do envelope
RMS imediatamente anterior ao pico da versão suavizada, já que
filtros passa-baixa sempre introduzem um atraso no sinal.
Alguns parâmetros temporais comumente utilizados na descrição
de diferentes aspectos da expressividade podem ser facilmente
estimados a partir destes instantes extraídos do envelope de
amplitude da nota:
• Duração Local: IOI(n)  = O(n+1) - O(n)
Intervalo de tempo medido entre os inícios de notas
sucessivas, conhecido como intra-onset-interval (IOI),
correspondente às durações especificadas na partitura. Este
valor é comumente normalizado pelo número de unidades
de tempo (batidas) contidas da nota. Por exemplo, em um
compasso cuja batida é a semínima, a duração de uma
20 Revista Opus 12 - 2006
mínima será dividida por 2 e de uma colcheia multiplicada
por 2.
• Duração Local Nominal
Valor  de duração da nota extraído da partitura. A duração
local nominal é também normalizada pelo número de
unidades de tempo contidas da nota.
• Duração da Nota: DN(n)  = F(n) - O(n)
Intervalo de tempo entre o início e o fim da nota
• Duração de Ataque: DA(n) = A(n) - O(n)
Intervalo de tempo entre o início da nota e o final do ataque.
Stephen McAdams e colegas mostraram que o valor do
logaritmo da duração do ataque (log[A(n) - O(n)])
corresponde mais à nossa percepção de timbre, sendo
portanto mais adequada quando o foco da descrição é a
variação de timbre (McAdams, Winsberg et al., 1995;
Misdariis, Smith et al., 1998).
• Inclinação de Ataque: IA(n)=[Valor de Amplitude em A(n)]¸
DA(n)
• Tempo Médio: MM
Valor médio do metrônomo em BPM (batidas por minuto).
• Tempo Global Principal:
Valor médio do metrônomo em BPM, excluindo-se
passagens que contêm variações consistentes de tempo,
tais como ritardandos e accelerandos.
• Tempo Local: Recíproco de IOI
• Índice de Articulação: AR(n)  = DN(n) ¸ IOI(n)
AR(n) é menor ou igual a 1, sendo igual a 1 para notas
ligadas.
Simon Dixon (2003), do Centro de Pesquisa ÖFAI, desenvolveu
uma ferramenta denominada BeatRoot, para extrair o envelope de
amplitude a partir de suavização do envelope RMS e técnicas de
regressão linear. A Figura 3 mostra o envelope de amplitude (linha
preta contínua) de um sinal de música polifônica (cinza) e as
21Revista Opus 12 - 2006
inclinações dos ataques de cada nota (linha tracejada) extraídos
pelo BeatRoot.
FIG. 3: Envelope de amplitude (linha preta contínua) de um sinal de música polifônica
(cinza) e as inclinações dos ataques de cada nota (linha tracejada) extraídos pelo BeatRoot
(Dixon, 2003).
Parâmetros Descritores Espectrais.
Variações intencionais do timbre são comumente utilizadas por
intérpretes para transmitir suas intenções expressivas em qualquer
instrumento musical. Estas variações são mais salientes em
instrumentos nos quais a ação do instrumentista participa durante
toda a produção do som, como ocorre nos instrumentos de cordas,
de sopros e na voz. Dos parâmetros extraídos do envelope da nota
acima mencionados, o índice de articulação, a duração de ataque
e a inclinação de ataque, estão intimamente relacionados com a
qualidade sonora. Variações intencionais de timbre podem ser
descritas também por um grande número de parâmetros definidos
22 Revista Opus 12 - 2006
a partir de medidas da distribuição espectral do sinal. Poucos
estudos em expressividade musical focalizaram aspectos
relacionados ao timbre, a maioria deles conduzidos pelos
pesquisadores do Centro di Solnologia Computazionale - CSC, da
Universidade de Pádua, Itália (Canazza, De Poli, Rinaldin et al.,
1997; Canazza, De Poli e Vidolin, 1997; De Poli, Rodà et al., 1998;
Canazza, De Poli et al., 2003; De Poli, 2004).
O primeiro passo para a extração deste tipo de informação é
determinar a distribuição espectral do sinal. Em projetos anteriores,
desenvolvemos ferramentas robustas para análise espectral a partir
de método de Quatieri e McAuly, e uma metodologia de redução de
dados para uma representação eficiente da distribuição espectral
utilizando Análise por Componentes Principais (Loureiro, de Paula
et al., 2004a; 2004b), a qual permite definir e estimar parâmetros
espectrais relevantes.
Uma grande variedade de parâmetros derivados da distribuição
espectral para descrever o timbre vem sendo recentemente proposta
e testada por vários grupos de pesquisa inseridos em áreas tais
como, percepção e cognição musical, psicologia da música e
extração e processamento de informação musical (MIR - Music
Information Retrieval). Através de testes subjetivos de similaridade,
Stephen McAdams e colaboradores evidenciaram a correlação entre
uma série de parâmetros espectrais e dimensões de espaços
gerados pela percepção de timbre, corroborando a adequação
destes parâmetros para a descrição deste atributo (McAdams,
Winsberg et al., 1995; Hajda, Kendall et al., 1997; Misdariis, Smith
et al., 1998; Loureiro, de Paula et al., 2001). Um deles é o centróide
espectral, conhecido por sua proeminente correlação com o “brilho”
do som, desde as primeiras pesquisas sobre percepção de timbre.
O Centróide Espectral (CE) é calculado como o centro de gravidade
do espectro de amplitude medido para cada quadro de tempo, ou
seja:
23Revista Opus 12 - 2006
Outro parâmetro identificado pelos estudos do grupo de MacAdams
acima mencionados é a irregularidade espectral (IE), medida como
a diferenciação de amplitude (em Db) entre 3 harmônicos
adjacentes:
harmônica, shimmer, jitter, envelope espectral, sincronia harmônica,
tristimulus, espalhamento espectral, decaimento espectral, fase,
achatamento espectral.
FORMALIZAÇÃO DA ANÁLISE DOS DADOS
Segmentação
Uma questão fundamental na análise da performance musical é a
definição dos critérios de segmentação do material musical. A nota
não é necessariamente a única unidade de segmentação de análise.
Parâmetros descritores espectrais, por exemplo, podem se referir a
níveis de segmentação de duração igual aos quadros de tempo
utilizados para calcular o conteúdo espectral (por exemplo, igual a
23 ms, equivalente à duração de um quadro de 1024 amostras a
Valores médios de centróide espectral e de irregularidade espectral
podem também ser considerados ao longo de períodos de tempo
específicos, como por exemplo a duração de uma nota. Inúmeros
parâmetros descritores de timbre já foram definidos e utilizados em
estudos envolvendo extração e processamento de informação
musical, principalmente aqueles voltados para o reconhecimento
automático de instrumento, como o trabalho de Tee Hong Park
(2004), que define um vasto elenco deste tipo de parâmetros, entre
eles inarmonicidade, expansão/compressão harmônica, inclinação
24 Revista Opus 12 - 2006
um freqüência de amostragem de 44.100 Hz). Por outro lado, a
análise de alguns parâmetros descritores temporais como tempo
local, por exemplo, pode se referir a níveis métricos de segmentação,
que incluem grupos de notas, como por exemplo o nível da pulsação
(normalmente igual  à unidade de tempo do compasso), como
utilizado por Shaffer, Clarke e Todd (1985), ou do compasso, como
utilizado por Todd (1985) e Repp (1992). Timmers, Ashley e
colaboradores (2000) normalizaram vários parâmetros temporais
em relação a este nível, sob a suposição de que o intérprete
normalmente “planeja” suas intenções expressivas em relação à
pulsação local da música.
Análise das Intenções Expressivas.
Uma vez identificados os descritores de uma determinada
característica expressiva, podemos buscar construir um modelo
estatístico que nos forneça o reconhecimento de cada intenção
expressiva. Para verificar se os parâmetros descritores são capazes
de descrever as diferenças significativas entre as performances e
de classificá-las, uma Estatística simples como Análise de Variância
(ANOVA) pode ser conduzida. O passo seguinte é identificar
regularidades na evolução temporal dos desvios destes parâmetros,
medidos em diferentes tipos pré-definidos de performances, e ou
com intérpretes distintos, com o propósito de correlacioná-las com
a estrutura da obra interpretada e com as intenções expressivas do
intérprete. Modelagens estatísticas e computacionais mais
complexas são utilizados para a classificação destes parâmetros
descritores e reconhecimento de padrões das variações medidas.
A validação destes modelos analíticos é comumente alcançada
através de testes subjetivos de identificação de estilos interpretativos
ou intérpretes reconhecidamente peculiares.
As intenções expressivas são avaliadas a partir de performances
distintas da mesma partitura. Battel e Fimbianti (1998), Canazza et
al. (1997; 1997; 1998; 2003) e De Poli et al. , obtiveram diferentes
25Revista Opus 12 - 2006
intenções expressivas solicitando-se aos músicos que executassem
a partitura inspirados por diferentes adjetivos. Esta metodologia já
foi utilizada satisfatoriamente em inúmeros estudos. Várias
categorias de adjetivos vem sendo utilizadas, como por exemplo
sensoriais do tipo suave-duro, leve-pesado, claro- escuro, caloroso-
frio, ou gradações de expressividade tais como sem expressão,
pouco expressivo, muito expressivo, exageradamente expressivo,
etc. Uma seleção adequada de um conjunto de adjetivos,
preferivelmente referentes a categorias contrastantes de intenções
expressivas, deve ser selecionado de acordo com os propósitos do
estudo e pode ser definido a partir de testes piloto.
Desvio Expressivo.
Definido um conjunto de parâmetros acústicos capazes de
encapsular a informação do conteúdo expressivo musical, mede-
se o desvio dos valores destes parâmetros em relação a uma
referência ou norma, definida como “plana” ou “sem expressão”.
Muito freqüentemente a partitura é usada como referência, por
representar a música no domínio simbólico e por ser de fácil acesso,
mas pode trazer algumas desvantagens para a interpretação de
como os ouvintes julgam a expressividade, já que a execução
humana de uma partitura nunca é capaz de realizar literalmente o
que está especificado nela, mesmo se a intenção é tocar
mecanicamente sem qualquer expressão. Dependendo do problema
em questão, pode ser mais adequado considerar a referência como
sendo uma performance neutra na qual o intérprete é solicitado a
executar os valores exatos da partitura, sem qualquer intenção
expressiva (Palmer, 1989). Uma performance média, definida como
uma média aritmética entre diferentes performances, tem sido
também considerada como norma, por exemplo em estudos sobre
preferências estilísticas.
Fontes dos Desvios: diversidades e semelhanças.
Baseados em estudos de Palmer (1996a; 1996b), De Poli identificou
26 Revista Opus 12 - 2006
duas fontes de motivação que levariam o intérprete a realizar estes
desvios expressivos, com vistas a transmitir suas intenções
expressivas: (1) aspectos estruturais da partitura, tais como estrutura
hierárquica de frases, estruturas harmônicas ou melódicas, que são
comuns a todas as performances e traduziriam o conteúdo
expressivo codificado pelo compositor na partitura; (2) intenções
expressivas do intérprete, que são específicas de cada performance
(De Poli, Rodà et al., 1998). O estudo foi capaz de distinguir os
desvios expressivos relacionados a cada uma destas fontes, além
de pontuar também que cada instrumento musical específico tem
seus próprios recursos expressivos: vibrato nas cordas, respiração
e movimentos da língua nos sopros, ataques no piano e percussão,
pinçamento no violão e harpa. A disponibilização destes recursos
em cada instrumento especifico está relacionada à escrita idiomática
do instrumento e é determinante na definição dos parâmetros a
serem utilizados.
Bruno Repp (1992) já havia observado que performances distintas
de uma mesma música apresentam consistentemente tanto
semelhanças quanto diversidades nos padrões de variações
temporais. As semelhanças identificadas por Repp, se apresentaram
mais no nível estrutural global do que as diversidades: semelhanças
entre diferentes intérpretes se referem comumente a alongamentos
de finais de frases. As diversidades foram encontradas na condução
expressiva de gestos melódicos de aproximadamente sete notas
de extensão. Timmers (Timmers, Ashley et al., 2000) estudou a
diversidade entre interpretações distintas de um mesmo trecho
musical, a partir de comparações entre todas as descrições
estruturais possíveis do trecho e manipulações de elementos
estruturais específicos, tais como barras de compasso, vozes
individuais e harmonia. O estudo identificou que interpretações
realizadas a partir da mesma estratégia interpretativa podem
apresentar diferenças significativas nos padrões temporais, e que
intérpretes podem também expressar o mesmo perfil interpretativo
ou expressivo a partir de padrões temporais distintos.
27Revista Opus 12 - 2006
Decomposição da Expressividade.
Mesmo que extraídos individualmente, as intenções expressivas
do intérprete são conduzidas tanto por intermédio da variação de
parâmetros distintos quanto a partir da interação de um conjunto
deles - para enfatizar uma frase ou uma nota, o músico pode
aumentar a intensidade, ou alongar a duração, ou utilizar uma
articulação específica, ou modificar o timbre, ou manipular uma
combinação destes parâmetros. Clarke e Windsor desenvolveram
métodos para decompor estruturas de padrões expressivos em
dimensões elementares com a finalidade de identificar a contribuição
de cada componente individual para o resultado expressivo final
(Clarke e Windsor, 2000). Utilizando a mesma metodologia Windsor
e colegas partiram da hipótese de que se um elemento qualquer de
uma estrutura musical é capaz de ser mapeado em uma
performance, o comportamento específico deste elemento será
preservado em todos os exemplos da mesma estrutura ao longo
desta performance (Windsor, Desain et al., 2006). Os autores
propuseram um sistema intitulado DISSECT (SECT de Structural
Expression Component Theory) para investigar as contribuições
relativas dos vários componentes individuais da expressividade para
o perfil expressivo global da performance.
Conclusão
Em um levantamento exaustivo de toda a pesquisa em performance
musical conduzida até o final do século XX, Gabrielsson constatou
que os estudos que focalizam a extração e a medição de parâmetros
da performance correspondem ao grupo mais numeroso deste
levantamento. Mesmo encapsulando estruturas complexas e muitas
vezes não acessíveis, extrair e medir estes parâmetros envolvem
procedimentos menos complexos. Cresce, no entanto, cada vez
mais o número de trabalhos que buscam interpretar e identificar
princípios gerais que possam estar por trás destes parâmetros
(Gabrielsson, 2003). A análise deste grande volume de dados
obtidos nestes estudos tem conduzido à formulação de uma grande
28 Revista Opus 12 - 2006
variedade de modelos, que buscam descrever como e por que o
músico modifica, às vezes inconscientemente, o que é indicado na
partitura. Os resultados destas análises não apenas contribuirão
para a compreensão deste problema complexo da pesquisa
musicológica, mas poderão também possibilitar o surgimento de
novos conceitos de abordagens pedagógicas objetivas para o ensino
da música, em especial da prática instrumental.
Bibliografia
ASKENFELT, A. “Measurement of Bow Motion Bow Force in Violin Playing”. Journal of the
Acoustical Society of America, v.80, p.1007-1015, 1986.
BATTEL, G. U. e R. FIMBIANTI. “How Communicate Expressive Intentions in Piano Performance”.
Proceedings CIM 1998, Gorizia, Italy. p. 67-70, 1998.
BINET, A. e J. COURTIER. “Recherches Graphiques sur la Musique”. L’Année Psychologique,
n.2, p.201-222, 1895.
BRESIN, R. “Articulation Rules for Automatic Music Performance”. 2001 International Computer
Music Conference, Havana, Cuba: International Computer Music Association. p. 294-297, 2001.
BRESIN, R. e G. U. BATTEL. “Articulation Strategies in Expressive Piano Performance”. Journal
of New Music Research, v.29, p.211-224, 2000.
BRESIN, R. e G. WIDMER. “Production of Staccato Articulation in Mozart Sonatas Played on a
Grand Piano: Preliminary Results”. Speech, Music and Hearing. Quarterly Progress and Status,
p.1-6, 2000.
CANAZZA, S., G. DE POLI, G. DI SANZO e A. VIDOLIN. “Adding Expressiveness to Automatic
Musical Performance”. Proceedings CIM 1998, Gorizia, Italy. p. 71-74, 1998.
CANAZZA, S., G. DE POLI, S. RINALDIN e A. VIDOLIN. “Sonological Analysis of Clarinet
Expressivity”. In: M. Leman (Ed.). Music, Gestalt and Computing: Studies in Cognitive and
Systematic Musicology. Berlin-Heidelberg: Springer Verlag, p.431-440, 1997.
CANAZZA, S., G. DE POLI, A. RODÀ e A. VIDOLIN. “An Abstract Control Space for Communication
of Sensory Expressive Intentions in Music Performance”. Journal of New Music Research, v.32,
n.3, p.281-294, 2003.
CANAZZA, S., G. DE POLI e A. VIDOLIN. “Perceptual Analisys of the Musical Expressive Intention
in a Clarinet Performance”. In: M. Leman (Ed.). Music, Gestalt and Computing: Studies in Cognitive
and Systematic Musicology. Berlin-Heidelberg: Springer Verlag, p.441-450, 1997.
CLARKE, E. F. “Some Aspects of Rythm adn Expression in Performances of Erik Satie’s
“Gnossienne No. 5””. Music Perception, v.2, p.299-328, 1985.
______. “Generative Principles in Music Performance”. In: J. A. Sloboda (Ed.). Generative
Processes in Music: The Psychology of Performance, Improvisation and Composition. Oxford:
Clarendon Press, p.1-26, 1988.
______. “Imitating and Evaluating Real and Transformed Musical Performances”. Music Perception,
n.10, p.317-341, 1993.
CLARKE, E. F. e W. L. WINDSOR. “Real and Simulated Expression: A listening study”. Music
Perception, v.17, p.277-314, 2000.
CLYNES, M. “Microstructural Musical Linguistics: Composer’s Pulses are Liked by the Best
Musicians”. Cognition, v.55, p.269-310, 1995.
DANNENBERG, R. B. e I. DERENYI. “Combining Instrument and Performance Models for High-
Quality Music Synthesis”. Journal of New Music Research, v.27, n.3, 1998.
DE POLI, G. “Methodologies for Expressiveness Modelling of and for Music Performance”. Journal
of New Music Research, v.33, n.3, p.189-202, 2004.
DE POLI, G., A. RODÀ e A. VIDOLIN. “Note-by-Note Analysis of the Influence of Expressive
29Revista Opus 12 - 2006
Intentions and Musical Structure in Violin Performance”. Journal of New Music Research, v.27,
n.3, p.293-321, 1998.
DESAIN, P. e H. HONING. “Modeling Continuous Aspects of Music Performance: Vibrato and
Portamento”. International Music Perception and Cognition Conference, Montreal: McGill University.
p., 1996.
DIXON, S. “On the Analysis of Musical Expression in Audio Signals”. In: (Ed.). Storage and
Retrieval for Media Databases: SPIE and IS&T, 2003.
EBHARDT, K. “Zwei Beiträge zur Psychologie des Rhythmus und des Tempos”. Zeitschrift für
Psychologie und Physiologie des Sinnesorgane n.18, p.99-154, 1898.
FRIBERG, A. “Generative Rules for Music Performance”. Computer Music Journal, n.15, p.57-
71, 1991a.
______. “Generative Rules for Music Performance: A Formal Description of a Rule System”.
Computer Music Journal, v.15, n.2, p.56-71, 1991b.
______. A Quantitative Rule System for Musical Performance. (PhD). Department of Speech
Music and Hearing, Royal Institue of Technology, Stockholm, 1995.
FRIBERG, A., R. BRESIN, L. FRYDÉN e J. SUNDBERG. “Musical Punctuation on the Microlevel:
Automatic Identification and Performance of Small Melodic Units”. Journal of New Music Research,
v.27, n.3, 1998.
FRIBERG, A., L. FRYDÉN, L.-G. BODIN e J. SUNDBERG. “Perfornmance Rules for Computer-
Controlled Contemporary Keyboard Music”. Computer Music Journal, v.15, n.2, p.49-55, 1991.
FRIBERG, A. e J. SUNDBERG. “Does Music Performance Allude to Locomotion? A Model of
Final Ritardandi Derived from Measurements of Stopping Runners”. Journal of the Acoustical
Society of America, v.105, p.1469-1484, 1999.
FRIBERG, A., J. SUNDBERG e L. FRYDÉN. “Music from Motion: Sound Level Enveopes of
Tones Expressing Human Locomotion”. Journal of New Music Research, v.29, p.199-210, 2000.
GABRIELSSON, A. “Once Again: The Theme from Mozart’s Piano Sonata in A Major: A Comparison
of Five Performances”. In: A. Gabrielsson (Ed.). Action and Perception in Rhythm and Music.
Stockholm: Royal Swedish Academy of Music, p.81-103, 1987.
______. “Expressive Intention and Performance. Music, Mind and Machine”. In: R. Steiner (Ed.).
New York: Springer, p.35-47, 1995.
______. “Music Performance”. In: D. Deutsch (Ed.). Psychology of music. New York: Academic
Press, p.506-602, 1999.
______. “Music Performance Research at the Millenium”. Psychology of Music, v.31, p.221-272,
2003.
GABRIELSSON, A. e P. N. JUSLIN. “Emotional Expression in Music Performance: Between the
Performer’s Intention and the Listener’s Experience”. Psychology of Music, v.24 n.1, p.68-91,
1996.
GOEBL, W., E. PAMPALK e G. WIDMER. “Exploring Expressive Performance Trajectories:
Famous Pianists Play Six Chopin Peices”. 8th International Conference on Music Perception and
Cognition (ICMPC’04), Evanston, Illinois. p., 2004.
HAJDA, J. M., R. A. KENDALL, E. C. CARTERETTE e M. L. HARSHBERGER. “Methodological
Issues in Timbre Research”. In: I. Deliège e J. A. Sloboda (Ed.). Perception and Cognition of
Music. Hove: Psychology Press, p.253-306, 1997.
HONING, H. “The Final Ritard: on Music, Motion, and Kinematic Models”. Computer Music Journal,
v.27, p.66-72, 2003.
JUSLIN, P. N. “Emotional Communication in music performance: a functionalist perspective and
some data”. Music Perception, v.14, p.383-418, 1997.
______. “Cue utilization in communication of emotion in music performance: relating performance
to perception”. Journal of Experimental Psychology: Human perception and performance, v.26,
n.6, p.1797-1813, 2000.
KRONMAN, U. e J. SUNDBERG. “Is the Musical Ritard an Allusion to Physical Motion?” In: A.
Gabrielsson (Ed.). Action and Perception in Rhythm and Music. Stockholm: The Royal Swedish
Academy of Music, 1987.
LOUREIRO, M. A., H. B. DE PAULA e H. C. YEHIA. “Sonological Representation of a Musical
30 Revista Opus 12 - 2006
Instrument by Sub-spaces of Spectral Component”. Mikropolyphonie - The Online Contemporary
Music Journal, v.7, 2001.
______. “Representation and Classification of the Timbre Space of a Single Musical Instrument”.
International Speech Communication Association - Tutorial and Research Workshop on Statistical
and Perceptual Audio Processing (SAPA 2004), Jeju, Korea, 2004: International Conference Center.
p. 546-549, 2004a.
______. “Timbre Classification of a Single Music Instrument”. 5th International Conference on
Music Information Retrieval, Barcelona: University Pompeu Fabra. p. 546-549, 2004b.
MCADAMS, S., S. WINSBERG, S. DONNADIEU, G. DE SOETE e J. KRIMPHOFF. “Perceptual
Scaling of Synthesized Musical Timbres: Common Dimensions, Specificities and Latent Subject
Classes”. Psychological Research, v.58, p.177-192, 1995.
MISDARIIS, N. R., B. K. SMITH, D. PRESSNITZER, P. SUSINI e S. MCADAMS. “Validation of
a Multidimensional Distance Model for Perceptual Dissimilarities Among Musical Timbres”.
Proceedings of the 16th International Congress on Acoustics, Woodbury, New York: ASA - The
Acoustical Society of America. p., 1998.
PALMER, C. “Mapping Musical Thought to Musical Performance”. Journal of Experimental
Psychology: Human Perception and Performance, v.15 n.12, p.331-346, 1989.
______. “Anatomy of a Performance: Sources of Musical Expression”. Music Perception, v.13,
p.433-454, 1996a.
______. “On the Assignment of Structure in Music Performance”. Music Perception, v.14 n.1,
p.23-56, 1996b.
______. “Music Performance”. Annual Review of Psychology, v.48, p.115-138, 1997.
PARK, T. H. Towards Automatic Musical Instrument Timbre Recognition. (PhD). Department of
Music, Princeton University, 2004.
REPP, B. H. “Patterns of Expressive Timing in Performances of a Beethoven Minuet by 19 Famous
Pianists”. Journal of the Acoustical Society of America, v.88, p.622-641, 1990.
______. “Diversity and Commonality in Music Performance - An Analysis of Timing Microstructure
in Schumann.s Traumerei”. Journal of the Acoustical Society of America, v.92 n.5, p.2546-2568,
1992.
SEARS, C. H. “A Contribution to the Psychology of Rythm”. American Journal of Psychology,
n.13, p.28-61, 1902.
SEASHORE, C. E., Ed. University of Iowa Studies in the Psychology of Music: Vol. I. The Vibrato.
Iowa city: University of Iowaed. 1932.
______, Ed. University of Iowa Studies in the Psychology of Music: Vol. III. Psychology of Vibrato
in Voice and Instrument. Iowa city: University of Iowaed. 1936.
______. University of Iowa Studies in the Psychology of Music: Vol. IV. Objective Analysis of
Music Performance. Iowa city: University of Iowa 1937.
______. Psychology of Music. New York: McGrow-Hill. Reprinted 1967 by Dover Publications,
New York, 1938.
SENJU, M. e K. OHGUSHI. “How are the Player’s Ideas Conveyed to the Audience?” Music
Perception, v.4, p.311-324, 1987.
SHAFFER, L. H., E. F. CLARKE e N. P. M. TODD. “Meter and Rhythm in Piano Playing”. Cognition,
v.20, p.61-77, 1985.
SHAFFER, L. H. e N. P. M. TODD. “The Interpretative Compponent in Musical Performance”. In:
A. Gabrielsson (Ed.). Action and Perception in Rythm and Music. Stockholm: Royal Swedish
Academy of Music, v.55, p.139-152, 1987.
SHOONDERWALDT, E. e A. FRIBERG. “Towards a Rule-Based Model for Violin Vibrato”.
Workshop on Courrent directions in Computer Music Research, Barcelona: Audiovisual Institute,
Pompeu Fabra University. p. 61-64, 2001.
SKINNER, L. e C. E. SEASHORE. “A Musical Pattern Score of the Last Movement of Beethoven
sonata Opus 27, No. 2”. In: C. E. Seashore (Ed.). University of Iowa Studies in the Psychology of
Music: Vol. IV. Objective Analysis of Music Performance. Iowa city: University of Iowa, p.263-
280, 1937.
31Revista Opus 12 - 2006
SLOBODA, J. A. “Expressive Skill in Two Pianists”. Canadian Journal of Psychology, v.39, p.273-
293, 1985.
STAMATATOS, E. “Quantifying the Differences Between Music Performances: Score vs. Norm”.
2002 International Computer Music Conference, Goteborg, Sweden. p., 2002.
STAMATATOS, E. e G. WIDMER. “Music Performer Recognition Using an ensemble of Simple
Classifiers”. 15th European Conference on Artificial Intelligence, Lyon, Frnace: Amsterdam IOS
Press. p. 335-339, 2002.
SUNDBERG, J. “How can Music be Expressive?” Speech Communication, v.12, p.239-253, 1993.
______. “Four Years of Research on Music and Motion”. Journal of New Music Research, v.29,
p.183-185, 2000.
SUNDBERG, J., A. ASKENFELT e L. FRYDÉN. “Musical Performance. A Synthesis-by-Rule
Aproach”. Computer Music Journal, v.7, p.37-43, 1983.
SUNDBERG, J., A. FRIBERG e R. BRESIN. “Attempts to Reproduce a Pianist’s Expressive
Timing with directtor Musices Performance Rules”. Journal of New Music Research, v.32, p.317-
325, 2003.
SUNDBERG, J., A. FRIBERG e L. FRYDÉN. “Rules for Automated Performances of Ensemble
Music”. Contemporary Music Review, v.3, p.89-109, 1989.
______. “Common Secrets of Musicians and Listeners: An analysis-by-synthesis Study of Musical
Performance”. In: P. Howell, R. West, et al (Ed.). Representing Musical Structure. London:
Academic Press, p.161-197, 1991a.
______. “Threshold and Preference Quantities of Rules for Music Performance”. Music Perception,
v.9, n.1, p.71-92, 1991b.
SUNDBERG, J., L. FRYDÉN e A. ASKENFELT. “What Tells You the Player is Musical? An Analysis-
by-Synthesis Study of Music Performance”. In: J. Sundberg (Ed.). Studies of Music Performance.
Stockholm: Royal Swedish Academy of Music, v.39, p.61-75, 1983.
SUNDBERG, J. e V. VERRILLO. “On the Anatomy of the Ritard: A Study of Timing in Music”.
Journal of the Acoustical Society of America, v.68, p.772-779, 1980.
TIMMERS, R., R. ASHLEY, P. DESAIN e H. HEIJINK. “The Influence of Musical Context on
Tempo Rubato”. Journal of New Music Research, v.29, n.2, p.131-158, 2000.
TIMMERS, R., R. ASHLEY, P. DESAIN, H. HONING e W. L. WINDSOR. “Timing of Ornaments
in the Theme of Beethoven’s Paisiello Variations: Empirical Data and a Model”. Music Perception,
v.20, n.1, 2002.
TODD, N. P. M. “A Model of Expressive Timing in Tonal Music”. Music Perception, v.3, p.33- 58,
1985.
______. “A Computational Model of Rubato”. Contemporary Music Review – “Music, Mind and
Structure”, v.3, n.1, p.69-88, 1989a.
______. “Towards a Cognitive Theory of Expression: The Performance and Perception of Rubato”.
Contemporary Music Review, v.4, p.405-416, 1989b.
______. “The Dynamics of Dynamics: A Model of Musical Expression”. Journal of the Acoustical
Society of America, v.91 n.6, p.3540-3550, 1992.
______. “The Kinematics of Musical Expression”. Journal of the Acoustical Society of America,
v.91, p.1940-1949, 1995.
WIDMER, G. “A Machine Learning Analysis of Expressive Timimg in Pianist’s Performances of
Schumann’s “Träumerei””. KTH Symposium on Grammar for Music Performance, Stockholm:
Department of Speech Communication and Music Acoustics - Royal Institue of Technology. p. 69-
81, 1995a.
______. “Modeling Rational basis for Musical Expression”. Computer Music Journal, v.19, p.79-
96, 1995b.
______. “Learning expressive Performance: The Structure-Level Approach”. Journal of New Music
Research, v.25, p.179-205, 1996.
______. “Large-Scale Induction of Expressive Performance Rules: First Quantitative Results”.
2000 International Computer Music Conference, Berlim: International Computer Music Association.
p. 344-347, 2000.
32 Revista Opus 12 - 2006
______. Using AI and Machine Learning to Study Expressive Music Performance: Project Survey
and First Report. AI Communications, 2001.
WIDMER, G., S. DIXON, W. GOEBL, E. PAMPALK e A. TOBUDIC. “ In Search of the Horowitz
Factor”. AI Magazine, v.24, p.111-130, 2003.
WIDMER, G. e W. GOEBL. “Computational Models of Expressive Music Performance: The State
of the Art”. Journal of New Music Research, v.33, n.3, p.203-216, 2004.
WIDMER, G. e A. TOBUDIC. “Playing Mozart by Analogy: Learning Multi-Level Timing and
Dynamics Strategies”. Journal of New Music Research, v.32, n.3, p.259-268, 2003.
WINDSOR, W. L., P. DESAIN, A. PENEL e M. BORKENT. “A Structurally Guided Method for the
Decomposition of Expression in Music Performance”. Journal of the Acoustical Society of America,
v.119, n.2, p.1182-1193, 2006.
ZANON, P. e G. DE POLI. “Estimation of Time-varying Parameters in Rule Systems for Music
Performance”. Journal of New Music Research, v.32, n.3, p.295-315, 2003.
ZANON, P. e G. WIDMER. “Learning to Recognize Famous Pianists with Machine Learning
Techniques”. In: (Ed.). Proceedings SMAC 03. Stockholm, Sweden, p.581-584, 2003.
Mauricio Alves Loureiro  - É Doutor em Música (University of Iowa, USA), onde estudou também
música eletrônica e computação musical. Engenheiro Aeronáutico formado pelo Instituto
Tecnológico de Aeronáutica – ITA. Iniciou seus estudos de clarineta com o Professor Dieter
Klöcker na Staatliche Hochshule für Musik Freiburg, (Alemanha), com bolsa de estudos do DAAD
(Serviço Alemão de Intercâmbio Acadêmico), graduando-se em 1983. Atuou como clarinetista na
Orquestra Sinfônica Municipal de Campinas, Orquestra Sinfônica do Estado de São Paulo e
como solista da Orquestra Sinfônica Municipal de São Paulo, Orquestra Sinfônica de Minas
Gerais, Orchestergesellschaft Weil am Rhein (Alemanha). Foi membro integrante de vários
conjuntos de música contemporânea: Grupo Nexus de São Paulo, Grupo Experimental de Câmara
de Belo Horizonte, Center For New Music da Universidade de Iowa, Grupo de Música
Contemporânea da UFMG, do qual é fundador e coordenador desde 1992. Atualmente é professor
titular na Escola de Música da UFMG, onde é coordenador do Programa de Pós-Graduação em
Música e pesquisador em Música e Tecnologia, no âmbito dos Grupos de Pesquisa MÚSICA E
TECNOLOGIA e CEFALA ( Centro de Estudos da Fala Acústica Linguagem e Música). Atua
como intérprete de música eletroacústica e computacional e é membro do Comitê Assessor de
Artes no CNPQ.
