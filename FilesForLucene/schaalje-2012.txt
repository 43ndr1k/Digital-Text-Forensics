This article was downloaded by: [University of Aegean]
On: 20 December 2011, At: 02:31
Publisher: Taylor & Francis
Informa Ltd Registered in England and Wales Registered Number: 1072954 Registered office: Mortimer House,
37-41 Mortimer Street, London W1T 3JH, UK
Communications in Statistics - Theory and Methods
Publication details, including instructions for authors and subscription information:
http://www.tandfonline.com/loi/lsta20
Open-Set Nearest Shrunken Centroid Classification
G. Bruce Schaalje a & Paul J. Fields a
a Department of Statistics, Brigham Young University, Provo, UT, USA
Available online: 14 Dec 2011
To cite this article: G. Bruce Schaalje & Paul J. Fields (2012): Open-Set Nearest Shrunken Centroid Classification,
Communications in Statistics - Theory and Methods, 41:4, 638-652
To link to this article:  http://dx.doi.org/10.1080/03610926.2010.529529
PLEASE SCROLL DOWN FOR ARTICLE
Full terms and conditions of use: http://www.tandfonline.com/page/terms-and-conditions
This article may be used for research, teaching, and private study purposes. Any substantial or systematic
reproduction, redistribution, reselling, loan, sub-licensing, systematic supply, or distribution in any form to
anyone is expressly forbidden.
The publisher does not give any warranty express or implied or make any representation that the contents
will be complete or accurate or up to date. The accuracy of any instructions, formulae, and drug doses should
be independently verified with primary sources. The publisher shall not be liable for any loss, actions, claims,
proceedings, demand, or costs or damages whatsoever or howsoever caused arising directly or indirectly in
connection with or arising out of the use of this material.
Communications in Statistics—Theory and Methods, 41: 638–652, 2012
Copyright © Taylor & Francis Group, LLC
ISSN: 0361-0926 print/1532-415X online
DOI: 10.1080/03610926.2010.529529
Open-Set Nearest Shrunken Centroid Classification
G. BRUCE SCHAALJE AND PAUL J. FIELDS
Department of Statistics, Brigham Young University, Provo, UT, USA
Nearest Shrunken Centroid (NSC) classification has proven successful in ultra-
high-dimensional classification problems involving thousands of features measured
on relatively few individuals, such as in the analysis of DNA microarrays. The
method requires the set of candidate classes to be closed. However, open-set
classification is essential in many other applications including speaker identification,
facial recognition, and authorship attribution. The authors review closed-set NSC
classification, and then propose a diagnostic for whether open-set classification is
needed. The diagnostic involves graphical and statistical comparison of posterior
predictions of the test vectors to the observed test vectors. The authors propose a
simple modification to NSC that allows the set of classes to be open. The open-
set modification posits an unobserved class with a distribution of features just
barely consistent with the test sample. A tuning constant reflects the combined
considerations of power, specificity, multiplicity, number of features, and sample
size. The authors illustrate and investigate properties of the diagnostic test and open-
set NSC classification procedure using several example data sets. The diagnostic
and the open-set NSC procedures are shown to be useful for identifying vectors that
are not consistent with any of the candidate classes.
Keywords Authorship attribution; Bayesian analysis; Closed-set classification;
Facial recognition; Linear discriminant analysis; Microarrays; Posterior
prediction; Speaker identification.
Mathematics Subject Classification 62H30.
1. Introduction
Nearest Shrunken Centroid (NSC) classification (Tibshirani et al., 2002, 2003)
was developed to classify tumors of unknown identity in ultra-high-dimensional
situations involving thousands of genes measured via microarrays on relatively few
individuals. NSC’s performance has been compared to that of other methods of
classification (Habtom et al., 2008; Lee et al., 2005; van Delft et al., 2005), and it has
been modified and extended (Dabney and Storey, 2007; Guo et al., 2007; Pang et al.,
2010; Wang and Zhu, 2007). Even though some aspects of it have been questioned
(Dabney, 2005), it remains a popular and useful classification technique. NSC and
similar methods have been applied in other high-dimensional classification problems
Received January 24, 2010; Accepted September 24, 2010
Address correspondence to G. Bruce Schaalje, Department of Statistics, Brigham Young
University, 230 TMCB, Provo, UT 84602, USA; E-mail: schaalje@byu.edu
638
D
ow
nl
oa
de
d 
by
 [
U
ni
ve
rs
ity
 o
f 
A
eg
ea
n]
 a
t 0
2:
31
 2
0 
D
ec
em
be
r 
20
11
 
Open-Set NSC 639
involving mass spectrometry (Levner, 2005), authorship attribution (Jockers et al.,
2009), and facial recognition (Noushath et al., 2006; Thomaz and Gillies, 2005).
The basic NSC method is useful for classification of a test sample into one
of a closed set of candidate classes. In this paper we propose a diagnostic for
whether open-set classification is needed, and based on the general ideas of Real and
Baumann (2000) we develop a modification to NSC that allows the set of candidate
classes to be open. The method will either predict membership in one of the known
candidate classes or predict that the test sample belongs to a class not included in
the candidate set. We then apply the diagnostic and open-set method to several data
sets to illustrate and verify the usefulness of the modifications.
Open-set classification is essential in such applications as speaker identification
(Cepeda, 2007; Deng and Hu, 2003; Wolf et al., 2007), facial recognition (Zhao and
Principe, 2001), and authorship attribution (Koppel et al., 2009; Stamatatos, 2009).
2. Closed-Set NSC Classification
The basic closed-set NSC method involves a vector x of r features (e.g., gene
intensities) that has a different distribution for each of m known candidate classes.
Let fix denote the joint density function of x for candidate class i. If it is known
that one of the m candidate classes is the true identity of the test sample, the
posterior probability that the test sample belongs to class k is
pk  x∗ = fkx
∗k∑m
i=1 fix∗i
(1)
where x∗ denotes the vector of features for the test sample and i denotes the prior
probability that the test sample belongs to class i. The denominator in Eq. (1), called
the normalizing constant, transforms the ratios into meaningful probabilities.
In practice, the densities have to be estimated from training data consisting of ni
samples from classes i = 1     m respectively. A sensible method of classifying the
test sample is to assign the sample to the class with the highest posterior probability;
that is, choose class ĥ where
ĥ = argmaxip̂i  x∗ (2)
and p̂i  x∗ is the posterior probability of class i based on estimated densities. Using
estimated densities, the procedure should approximately minimize classification
errors.
The NSC procedure of Tibshirani et al. (2003) is a special case of this procedure
in which the densities are assumed to be multivariate normal and the features are
assumed to be mutually independent with common variance for each feature across
classes. Thus, after simplification,
pk  x∗ = ke
− 12
∑r
j=1
(
x∗j −kj
	j
)2
∑m
i=1 ie
− 12
∑r
j=1
(
x∗j −ij
	j
)2 (3)
where ij is the mean for class i and feature j, and 	
2
j is the common variance for
feature j across classes.
D
ow
nl
oa
de
d 
by
 [
U
ni
ve
rs
ity
 o
f 
A
eg
ea
n]
 a
t 0
2:
31
 2
0 
D
ec
em
be
r 
20
11
 
640 Schaalje and Fields
The NSC procedure estimates 	2j with s
2
j , the pooled within-class variance for
feature j, and ij by thresholding and shrinking x̄ij , the sample mean of the training
data for feature j and class i, toward x̄j , the mean for feature j across classes. Thus
x̃ij = x̄j + qij × sign
(
x̄ij − x̄j
)×max
[(∣∣x̄ij − x̄j∣∣
qij
− 

)
 0
]
 (4)
qij =
(
sj + 
)√
1/ni + 1/n (5)
where 
 is a shrinkage parameter,  = mediansj j = 1     r, and n =
∑m
i=1 ni.
Hence,
p̂k  x∗ = ke
− 12
∑r
j=1
(
x∗j −x̃kj
sj
)2
∑m
i=1 ie
− 12
∑r
j=1
(
x∗j −x̃ij
sj
)2  (6)
Note that if x̃ij = x̄j for all m classes, feature j has no effect on the posterior
probabilities.
Prior to applying closed-set NSC in a classification analysis, an initial
examination of the data should be carried out to see if x∗ for the test sample is
reasonably near the distribution of x vectors in the training set for at least one of the
candidate classes. A dimension reduction procedure such as principal components
analysis can be used for this even though some information is lost when visualizing
a high-dimensional data set in 2 or 3 dimensions. A high-dimensional dynamic
visualization program such as GGobi (Buja et al., 2007) can also be used.
3. Goodness-of-Fit Diagnostic
After applying the closed-set NSC procedure, a goodness-of-fit check (Gelman et al.,
2004) should be carried out to validate the posterior probabilities and classifications.
One way to do this is to compute several posterior predicted vectors for each
test sample using the NSC posterior probabilities, and then compare the posterior
predictions to the observed test sample vectors. If the predicted and observed vectors
do not generally agree, the classifications are invalid.
Given a set of NSC posterior probabilities p̂i  x∗ i = 1     m for a test
sample with x∗ as the vector of features, posterior predicted vectors are random
draws from the finite mixture distribution
f̂∗y
∗ =
m∑
i=1
f̂iy
∗p̂i  x∗ (7)
In this equation, f̂iy
∗ involves only the selected features (i.e., those features that
have an effect on the posterior probability calculations). To examine the agreement
graphically, a principal components plot of the combined predicted and observed
data is useful.
If several test samples are to be classified by the NSC procedure, a goodness-
of-fit test statistic for the closed-set model can also be constructed based on
Johnson’s (2004) Bayesian goodness-of-fit chi-square test. Johnson (2004) showed
that expected percentiles for the observed test data can be estimated from posterior
D
ow
nl
oa
de
d 
by
 [
U
ni
ve
rs
ity
 o
f 
A
eg
ea
n]
 a
t 0
2:
31
 2
0 
D
ec
em
be
r 
20
11
 
Open-Set NSC 641
predicted values from the fitted Bayesian model. Bins are then defined based on
these estimated percentiles, and the number of observed values in each bin is
compared to the expected number in each bin using a chi-square statistic. Johnson
(2004) showed that such a statistic asymptotically follows the chi-square distribution
with degrees of freedom equal to the number of bins minus one if the model is
correct.
To adapt this test to the present multivariate case, we first computed principal
component scores for the predicted and observed vectors based on the correlation
matrix of the combined predicted and observed vectors. We then defined bins as the
four quadrants of a two-dimensional space with the origin located at the median
values of the first and second principal component scores. If the closed-set model
fits, about 25% of the points defined by the first two principal components of the
observed data should be in each quadrant. For large sets of observations to be
classified, bins could similarly be defined using three or more principal components
or smaller subdivisions of the space of the first two principal component scores.
A preliminary simulation study verified that the null distribution of this test
statistic is, in fact, the chi-square distribution with 3 degrees of freedom. We will
explore detailed statistical properties of this test statistic in a future article.
4. Open-Set NSC Classification
Although closed-set NSC works well for classifying tumors using gene expression
measurements, numerous situations exist in which the correct class may not be
among the candidate classes in the training data. To deal honestly with such a
situation, NSC must be modified. Naive use of closed-set NSC classification will
produce inflated and misleading posterior probabilities if the true class is not among
the candidates. To see this, assume without loss of generality that the test sample
belongs to class m. The posterior probability of class k can be written as
pk  x∗ = fkx
∗k
fmx∗m +
∑m−1
i=1 fix∗i
 (8)
If class m is left out of the candidate set, the denominator in (8) decreases and naive
use of the posterior probability formula yields
p′k  x∗ = fkx
∗k∑m−1
i=1 fix∗i
> pk  x∗ (9)
Thus, when the true class is not included in the training data, the normalizing
constant is too small and all of the calculated posterior probabilities are inflated.
The posterior probabilities lose their meaning, as does the associated classification.
Even conceding that the centroid of the candidate class with the highest posterior
probability has the smallest statistical distance to the test sample, there is no reason
to attach any meaning to the classification results because the smallest distance
might still be a very large distance. The inflation of the posterior probabilities is
insidious because inflated naive probabilities tend to instill false confidence about
the classification results, and because the more discrepant the missing class is from
the other classes, the greater the inflation of the posterior probabilities.
An important extension to NSC classification is therefore to allow the
possibility that the test sample(s) may not belong to any of the observed candidate
D
ow
nl
oa
de
d 
by
 [
U
ni
ve
rs
ity
 o
f 
A
eg
ea
n]
 a
t 0
2:
31
 2
0 
D
ec
em
be
r 
20
11
 
642 Schaalje and Fields
classes. This can be done by positing an unobserved or latent class in addition
to the candidate classes in the training data. We propose a latent class with a
distribution of features just barely consistent with the test sample. Thus, as a simple
extension of the NSC classification model we suggest that posterior probabilities for
the candidates be calculated as
p̂k  x∗ = ke
− 12
∑r
j=1
(
x∗j −x̃kj
sj
)2
∑m
i=1 ie
− 12
∑r
j=1
(
x∗j −x̃ij
sj
)2
+ m+1e− 12
∑r
j=1 a2j
(10)
where
aj = min
(
maxi
∣∣∣∣x
∗
j − x̃ij
sj
∣∣∣∣  
)

and  is a tuning constant representing the maximum allowed distance for
components of x∗ from corresponding components of the centroid of the latent
class. The approximate posterior probability (actually a lower bound) for the latent
class is then
pm+ 1  x∗ = m+1e
− 12
∑r
j=1 a2j
∑m
i=1 ie
− 12
∑r
j=1
(
x∗j −x̃ij
sj
)2
+ m+1e− 12
∑r
j=1 a2j
 (11)
The choice of  is crucial and difficult because  must reflect the combined
considerations of power, specificity, multiplicity, number of features, and sample size
(degrees of freedom). As a rough guideline, the relationship of  to the number of
features, sample size, and nominal specificity is given by
 =
∣∣∣∣−1
(
1− 1/v
2
)∣∣∣∣ (12)
where −1  is the inverse cumulative Student’s t function with  degrees of
freedom, v is the number of features, and  is the desired specificity for a single test
sample. The choice of  also reflects a compromise between power and specificity,
and can be adjusted to account for multiplicity. Equation (12) is based on the
relationship
prob
(∣∣ti∣∣ <  i = 1     ) =  if  =
∣∣∣∣−1
(
1− 1/v
2
)∣∣∣∣ 
As an example, Fig. 1 shows the relationship between  and v for a range of values
of  and  = 095. Equation (12) provides a range of sensible values for , but cross-
validation can also be used to select a particular value for .
5. Examples
To illustrate and test open-set NSC classification, we present two examples in
microarray analysis and two examples in authorship attribution. All code used in
the examples was written using the IML procedure of SAS 9.1 (SAS Institute, 2004).
D
ow
nl
oa
de
d 
by
 [
U
ni
ve
rs
ity
 o
f 
A
eg
ea
n]
 a
t 0
2:
31
 2
0 
D
ec
em
be
r 
20
11
 
Open-Set NSC 643
Figure 1. Values of the tuning constant  as a function of the number of features v and
degrees of freedom , with specificity () set at 95%. Calculations are based on Eq. (12).
5.1. Small Round Blue Cell Tumor Microarray Data
To illustrate open-set NSC classification, we reanalyzed the small round blue cell
tumor microarray data used by Tibshirani et al. (2002) in developing the NSC
method. The data set consisted of four cell lines with arrays of 2,308 genes. The
numbers of microarrays per cell line were 23, 8, 12, and 20 for cell lines 1, 2, 3, and
4, respectively.
We created an open-set situation by treating the 20 arrays for cell line 4 as
test samples to be classified using the data for cell lines 1–3 as training data. As in
Tibshirani et al.’s (2002) article, 
 was set to 4.34. Using closed-set NSC, 1, 13, and
6 test samples were assigned to lines 1, 2, and 3, respectively. Calculated posterior
probabilities for the selected classes ranged from 37% to 94%.
All of these predictions are wrong of course, but in practice it would not be
known. Thus posterior predicted vectors (Eq. 7) were generated and compared
to the observed vectors. In this case, the first two principal components of the
posterior predicted and observed vectors showed that the posterior predicted vectors
were distinct from the observed test vectors (Fig. 2). The goodness-of-fit chi-
square statistic was 60.0, greater than the critical value of 7.81 ( = 005, 3df).
Therefore, the closed-set procedure was not appropriate, and the need for open-set
classification was indicated.
In contrast, using 75% of the training set vectors (cell lines 1–3) as training data
and 25% as test data, the observed tests vectors were not distinct from posterior
predictions (Fig. 3). The goodness-of-fit chi-square statistic was 4.64, less than the
critical value. Therefore, as expected, closed-set classification was appropriate in this
case.
We then used our open-set NSC procedure to classify the 20 samples for cell
line 4 as test data using the data from cell lines 1–3 as training data. We again
set 
 to 4.34. To choose , we designated each of cell lines 1–3 in turn as well
as 10% of the other two cell lines as test data. The value  = 15 maximized the
overall correct classification rate (Fig. 4). Using these values of 
 and , the open-set
D
ow
nl
oa
de
d 
by
 [
U
ni
ve
rs
ity
 o
f 
A
eg
ea
n]
 a
t 0
2:
31
 2
0 
D
ec
em
be
r 
20
11
 
644 Schaalje and Fields
Figure 2. The first two principal components of observed and posterior predicted vectors
for the small round blue cell tumor data. The test vectors did not belong to any of the
candidate classes.
NSC procedure correctly classified 19 of the 20 arrays (95%) for cell line 4 as not
belonging to cell lines 1–3.
In a final test of the open-set NSC procedure, we used 75% of the arrays for all
four cell lines as training data to classify the remaining 25% of the arrays. This was
repeated 100 times using the open- and closed-set procedures. The purpose was to
see if open-set NSC produces accurate classifications even if a closed-set situation
Figure 3. The first two principal components of observed and posterior predicted vectors
for the small round blue cell tumor data. Each of the test vectors belonged to one of the
candidate classes.
D
ow
nl
oa
de
d 
by
 [
U
ni
ve
rs
ity
 o
f 
A
eg
ea
n]
 a
t 0
2:
31
 2
0 
D
ec
em
be
r 
20
11
 
Open-Set NSC 645
Figure 4. Percent correctly classified, as a function of values of  for the small round blue
cell tumor data. A smoother was superimposed on the correct classification rates.
exists. The mean correct classification percentage for the 100 repetitions was 98.2%
(ranging from 81% to 100%) using the closed-set procedure, and 79.8% (ranging
from 47% to 100%) using the open-set procedure.
5.2. Prostate Cancer Microarray Data
The second example involved a much larger set of microarray data. The data
involved 12,533 genes for 52 prostate tumor tissue samples from patients with
prostatic cancer and 50 samples from normal patients (Singh et al., 2002).
We first randomly selected 15 normal and 15 tumor samples as test data.
We set 
 at 2.5 because this value selected 36 genes as active genes, close to the
number selected by Singh et al. (2002). We used closed-set NSC to classify the test
samples using the remaining 72 samples as training data. The diagnostic plot (Fig. 5)
showed that the observed data were randomly distributed around the mean of the
posterior predicted vectors. This agreed with the goodness-of-fit chi-square statistic
of 2.80, which was less than the critical value. Thus, as expected, closed-set NSC was
appropriate. Twenty-nine of the 30 test samples (96.7%) were correctly classified by
closed-set NSC.
For illustration, we then used our open-set NSC procedure to classify the 30
test samples. Twenty-five of the 30 (83.3%) were correctly classified; all five of the
misclassified samples were assigned to an unobserved class.
5.3. Federalist Authorship Data
We examined a data set from a nonmicroarray study involving a large number
of observations but a smaller number of measurements per observation. Jockers
and Witten (2010) recently compared closed-set NSC classification to several other
closed-set procedures for authorship attribution using data on noncontextual word
frequencies. Their comparisons were based on the disputed Federalist papers case
(Mosteller and Wallace, 1964). Using cross-validation, they found the classification
D
ow
nl
oa
de
d 
by
 [
U
ni
ve
rs
ity
 o
f 
A
eg
ea
n]
 a
t 0
2:
31
 2
0 
D
ec
em
be
r 
20
11
 
646 Schaalje and Fields
Figure 5. The first two principal components of observed and posterior predicted vectors
for the prostate cancer data.
error rate for the closed-set NSC procedure to be tied as the lowest of any closed-
set classification procedure. We used similar data to illustrate the goodness-of-fit
diagnostic and the open-set NSC procedure.
The data set consisted of relative frequencies of 109 noncontextual words for
papers of roughly the same length written by Alexander Hamilton (51 papers),
James Madison (14 papers), and John Jay (5 papers). The set also contained data
on 12 papers of disputed authorship, but known to be written by either Madison or
Hamilton. In their famous study, Mosteller and Wallace (1964) attributed all of the
disputed papers to Madison.
We began by treating the 12 disputed papers as test samples to be classified
using the data for papers written by Hamilton or Madison. We used closed-set NSC,
and set 
 to 1.6 based on cross-validation. The diagnostic graph (Fig. 6) showed
that the observed vectors for the disputed papers were distributed randomly among
the predicted vectors from the closed-set model. The goodness-of-fit chi-square was
4.67, less than the critical value. We therefore concluded that the closed-set model
was appropriate for classification of the disputed texts to Hamilton or Madison.
All of the disputed texts were classified to Madison with posterior probabilities
of authorship by Madison ranging from 60.8% to 99.9%. Ten of the 12 posterior
probabilities were greater than 99%.
Using the open-set procedure to classify the disputed papers, with 
 set to 1.6
and  set to 5.0, 8 of the 12 papers were still assigned to Madison, but 4 were
assigned to an unobserved author.
To investigate the power of the goodness-of-fit test, we then used closed-set
NSC to classify the five papers written by Jay to either Hamilton or Madison.
All five of the Jay papers were classified to Madison with posterior probabilities
ranging from 96.2% to 98.8%. However, the goodness-of-fit graphic showed that the
observed vectors did not agree with the predicted vectors (Fig. 7), and the chi-square
D
ow
nl
oa
de
d 
by
 [
U
ni
ve
rs
ity
 o
f 
A
eg
ea
n]
 a
t 0
2:
31
 2
0 
D
ec
em
be
r 
20
11
 
Open-Set NSC 647
Figure 6. The first two principal components of observed and posterior predicted vectors
for closed-set classification of the disputed Federalist papers to either Hamilton or Madison.
value was 8.6, greater than the critical value. Thus, as expected, closed-set NSC
classification was not appropriate in this case.
We used our open-set NSC procedure to classify the five Jay papers using the
Hamilton and Madison papers as training data. We again set 
 to 1.6. We also
set  to 5.0 based on cross-validation. Three of the five of the Jay papers were
correctly assigned to an unobserved author—a result that is impossible for a closed
set procedure.
Figure 7. The first two principal components of observed and posterior predicted vectors
for closed-set classification of the Jay papers to either Hamilton or Madison.
D
ow
nl
oa
de
d 
by
 [
U
ni
ve
rs
ity
 o
f 
A
eg
ea
n]
 a
t 0
2:
31
 2
0 
D
ec
em
be
r 
20
11
 
648 Schaalje and Fields
5.4. Book of Mormon Authorship Data
In the final example we applied our methods to a data set for which open-set
classification is required. The Church of Jesus Christ of Latter-day Saints considers
the Book of Mormon to be a translation of writings of ancient prophets. Others
have proposed various naturalistic theories of its origins. Based on one of these
theories, Jockers et al. (2009) recently used closed-set NSC classification to classify
chapters of the Book of Mormon to one of four 19th century candidate authors and
three control authors. However, it seems unrealistic to consider any set of potential
authors to be a closed set for a book with the provenance of the Book of Mormon.
An open-set approach is more unbiased, conservative, and yet persuasive in such
studies.
We applied closed- and open-set NSC classification to sections of the Book of
Mormon using writings of five 19th century authors as training data, including the
four used by Jockers et al. (2009). To avoid text size problems, we separated the
Book of Mormon into 52 sequential text blocks of approximately 5,000 words. On
all text blocks including both the training texts and the test texts, we measured
107 stylometric features. These included relative frequencies of 71 noncontextual
words, 34 noncontextual word-pattern ratios (Hilton, 1990), and two measures of
vocabulary richness (Holmes, 1992).
We first applied closed-set NSC with 
 set to 2.0 based on cross-validation.
Fifty-one of the 52 Book of Mormon texts were assigned to author 2, with posterior
probabilities ranging from 54.6% to 99.9%. The other was assigned to author 5,
with posterior probability 48.6%. However, the diagnostic graph (Fig. 8) showed the
observed vectors for the Book of Mormon texts to be separate from the predicted
vectors from the closed-set model. The goodness-of-fit chi-square was 53.38, larger
than the critical value. The closed-set model is thus not appropriate for classification
of the Book of Mormon texts in spite of apparent posterior probabilities near 1.
Figure 8. The first two principal components of observed and posterior predicted vectors
for closed-set classification of the Book of Mormon texts.
D
ow
nl
oa
de
d 
by
 [
U
ni
ve
rs
ity
 o
f 
A
eg
ea
n]
 a
t 0
2:
31
 2
0 
D
ec
em
be
r 
20
11
 
Open-Set NSC 649
Figure 9. The first two principal components of observed and posterior predicted vectors
for closed-set classification of the Book of Mormon texts plus texts due to two 19th century
authors.
We then used our open-set NSC procedure to classify the Book of Mormon
texts. We again set 
 to 2.0. We set  to 5.0 based on Fig. 1 and cross-validation.
Only eight of the texts were assigned to author 2. The remaining 44 texts were
assigned to an unobserved author, most with posterior probabilities near 1.
As a further test of the power of the goodness-of-fit procedure, we used writings
of three of the 19th century authors as training data, and used the Book of Mormon
texts plus writings of the other two 19th century authors as test texts. We set 
 to
1.0. The diagnostic graph (Fig. 9) showed the combined test texts to be distinctly
separate from the predicted vectors of the closed-set model. The goodness-of-fit
chi-square was 83.40, larger than the critical value. Thus the closed-set model is
not appropriate for classification of these texts. More importantly, this shows that
power of the goodness-of-fit test was not affected by heterogeneity of test texts.
Authorship attribution of historical texts is a complicated problem in which
statistical attribution is but one component. The point of this example is that there
is danger in blindly applying closed-set NSC classification to problems in which
open-set procedures should be considered. The diagnostic procedure proposed in
this article is thus essential in avoiding misleading conclusions.
6. Conclusions
We developed goodness-of-fit diagnostic tools for determining whether open-set
classification is needed. We also generalized the basic NSC classification procedure
to allow the set of classes to be open. We did not compare our open-set classification
procedure to existing procedures because a literature search failed to produce
any such methods in the microarray literature. Furthermore, a recent survey of
authorship attribution methods (Stamatatos, 2009) noted the vital importance of
development of open-set attribution methods.
D
ow
nl
oa
de
d 
by
 [
U
ni
ve
rs
ity
 o
f 
A
eg
ea
n]
 a
t 0
2:
31
 2
0 
D
ec
em
be
r 
20
11
 
650 Schaalje and Fields
The comparison of posterior predicted vectors with observed test vectors is
a useful diagnostic of the need to use open-set NSC. We presented graphical
and statistical tests for agreement of the observed and posterior predicted vectors.
Application of the closed-set procedure to an open-set problem can produce highly
misleading results. We recommend that the diagnostic test be used whenever
applying the NSC procedure, even for data sets strongly thought to be closed sets.
The tuning constant  for open-set NSC can be chosen by cross-validation.
However, the cross-validation must be designed so that  optimizes correct
classification rates consistent with choices of power, specificity, multiplicity, number
of features, and sample size.
Open-set NSC is useful for identifying vectors that are not consistent with
any of the observed candidate classes. However, it is necessary to be careful not
to conclude that test vectors classified to a nonobserved class all belong to a
single nonobserved class. Further investigation is needed, perhaps using a clustering
procedure.
As expected, misclassification rates for closed-set situations are inflated when
the open-set NSC procedure is used. When exogenous contextual information
strongly suggests that the candidate set is closed, the closed-set NSC procedure
could be used in connection with the diagnostic tool. In all other situations open-set
NSC classification should be used because even in true closed-set applications, the
inflated misclassification rate is worth the benefit of allowing the possibility of an
unobserved class.
We conclude that if there is compelling exogenous evidence that all test samples
belong to one of the training classes, closed-set NSC is appropriate and powerful.
If open-set NSC is used when all test set individuals belong to the training classes,
there is an expected decrease in the correct classification rate. However, if closed-
set NSC procedure is used when open-set NSC should be used, posterior class
membership probabilities can be disastrously misleading. Thus, careful examination
of the diagnostic results must be employed before the results of the closed-set
classifications can be relied on.
In sum, if there is any doubt about whether the training set includes all
appropriate classes, open-set NSC should be used along with the diagnostic tools.
The extensions that have been suggested by other authors for closed-set NSC
classification can be incorporated easily into the open-set NSC procedure.
Acknowledgments
The Neal A. Maxwell Institute for Religious Scholarship and the Department of
Statistics, Brigham Young University provided funding for this project.
References
Buja, A., Lang, D. T., Swayne, D. F. (2007). GGobi: Evolving from XGobi into an extensible
framework for interactive data visualization. Journal of Computational and Graphical
Statistics 43(4):423–444.
Cepeda, L. (2007). Semi-Automatic Speaker Forensic Identification. PhD dissertation,
Department of Computer Science, North Carolina State University.
Dabney, A. R. (2005). Classification of microarrays to nearest centroids. Bioinformatics
21(22):4148–4154.
D
ow
nl
oa
de
d 
by
 [
U
ni
ve
rs
ity
 o
f 
A
eg
ea
n]
 a
t 0
2:
31
 2
0 
D
ec
em
be
r 
20
11
 
Open-Set NSC 651
Dabney, A. R., Storey, J. D. (2007). Optimality driven nearest centroid classification from
genomic data. PLoS ONE 2(10):e1002.
Deng, J., Hu, Q. (2003). Open-set text independent speaker recognition based on set-score
pattern classification. ICASSP 11:73–76.
Gelman, A., Carlin, J. B., Stern, H. S., Rubin, D. B. (2004). Bayesian Data Analysis. 2nd ed.
New York: Chapman & Hall/CRC.
Guo, Y., Hastie, T., Tibshirani, R. (2007). Regularized linear discriminant analysis and its
application in microarrays. Biostatistics 8(1):86–100.
Habtom, W. R., Varghese, R. S., Zhang, Z., Xuan, J., Clarke, R. (2008). Classification
algorithms for phenotype prediction in genomics and proteomics. Front. Biosci. 1:691–
708.
Hilton, J. L. (1990). On verifying worprint studies: Book of Mormon authorship. BYU Studies
30:89–108.
Holmes, D. I. (1992). A stylometric analysis of Mormon scriptures and related texts. Journal
of the Royal Statistical Society A 155:91–120.
Jockers, M. L., Witten, D. M. (2010). A comparative study of machine learning methods for
authorship attribution. Literary and Linguistic Computing 25:215–223.
Jockers, M. L., Witten, D. M., Criddle, C. S. (2009). Reassessing authorship of the Book
of Mormon using delta and nearest shrunken centroid classification. Literary and
Linguistic Computing 23:465–491.
Johnson, V. (2004). A Bayesian 2 test for goodness of fit. Annals of Statistics 32:2361–2384.
Koppel, M., Schler, J., Argamon, S. (2009). Computational methods in authorship
attribution. Journal of the American Society for Information Science and Technology
60(1):9–26.
Lee, J. W., Lee, J. B., Park, M., Song, S.H. (2005). An extensive comparison of recent
classification tools applied to microarray data. Computational Statistics & Data Analysis
48:869–885.
Levner, I. (2005). Feature selection and nearest centroid classification. BMC Bioinformatics
6:68.
Mosteller, F., Wallace, D. L. (1964). Applied Bayesian and Classical Inference: The Case of
the Federalist Papers. New York: Springer.
Noushath, S., Kumar, G. H., Shivakumara, P. (2006). Diagonal Fisher linear discriminant
analysis for efficient face recognition. Neurocomputing 69:1711–1716.
Pang, H., Tong, T., Zhao, H. (2010). Shrinkage-based diagonal discriminant analysis and its
applications in high-dimensional data. Biometrics 65:1021–1029.
Real, E. C., Baumann, A. H. (2000). Open set classification using tolerance intervals,
Conference Record of the Thirty-Fourth Asilomar Conference on Signals, Systems and
Computers 2:1217–1221.
SAS Institute. (2004). SAS/IML 9.1 User’s Guide. Cary, NC: Author.
Singh, D., Febbo, P. G., Ross, K., Jackson, D. G., Manola, J., Ladd, C., Tamayo, P.,
Renshaw, A. A., D’Amico, A. V., Richie, J. P., Lander, E. S., Loda, M., Kantoff, P.
W., Golub, T. R., Sellers, W. R. (2002). Gene expression correlates of clinical prostate
cancer behavior. Cancer Cell 1:203–209.
Stamatatos, E. (2009). A survey of modern authorship attribution methods. Journal of the
American Society for Information Science and Technology 60:538–556.
Thomaz, C. E., Gillies, D. F. (2005). A new Fisher-based method applied to face recognition.
In: Gagalowicz, A., Philips, W., eds. Computer Analysis of Images and Patterns Berlin:
Springer, pp. 596–605.
Tibshirani, R., Hastie, T., Narasimham, B., Chu, G. (2002). Diagnosis of multiple cancer
types by shrunken centroids of gene expression. Proceedings of the National Academy
of Science 99:6567–6572.
Tibshirani, R., Hastie, T., Narasimham, B., Chu, G. (2003). Class prediction by
nearest shrunken centroids, with application to DNA microarrays. Statistical Science
18(1):104–117.
D
ow
nl
oa
de
d 
by
 [
U
ni
ve
rs
ity
 o
f 
A
eg
ea
n]
 a
t 0
2:
31
 2
0 
D
ec
em
be
r 
20
11
 
652 Schaalje and Fields
van Delft, J. H. M., van Agen, E., van Breda, S. G. J., Herwijnen, M. H., Staal, Y.
C. M., Kleinjans, J. C. S. (2005). Comparison of supervised clustering methods to
discriminate genotoxic from non-genotoxic carcinogens by gene expression profiling.
Mutation Research/Fundamental and Molecular Mechanisms of Mutagenesis 575(1–2):
17–33.
Wang, S., Zhu, Z. (2007). Improved centroids estimation for the nearest shrunken centroid
classifier. Bioinformatics 23:972–979.
Wolf, M. B., Park, W. K., Oh, J. C., Blowers, M. K. (2007). Toward open-set text-
independent speaker identification in tactical communications. Proc. 2007 IEEE
Symposium on Computational Intelligence in Security and Defense Applications 7–14.
Zhao, Q., Principe, J. C. (2001). Support vector machines for SAR automatic target
recognition. IEEE Transactions on Aerospace and Electronic Systems 37:643–654.
D
ow
nl
oa
de
d 
by
 [
U
ni
ve
rs
ity
 o
f 
A
eg
ea
n]
 a
t 0
2:
31
 2
0 
D
ec
em
be
r 
20
11
 
