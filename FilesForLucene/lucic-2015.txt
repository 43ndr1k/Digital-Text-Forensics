A syntactic characterization of
authorship style surrounding
proper names
............................................................................................................................................................
Ana Lučić and Catherine L. Blake
University of Illinois at Urbana-Champaign, USA
.......................................................................................................................................
Abstract
Accurately determining who wrote a manuscript has captivated scholars of liter-
ary history for centuries, as the true author can have important ramifications
in religion, law, literary studies, philosophy, and education. A wide array of
lexical, character, syntactic, semantic, and application-specific features have
been proposed to represent a text so that authorship attribution can be estab-
lished automatically. Although surface-level features have been tested extensively,
few studies have systematically explored high-level features, in part due to limi-
tations in the natural language processing techniques required to capture high-
level features. However, high-level features, such as sentence structure, are used
subconsciously by a writer and thus may be more consistent than surface-level
features, such as word choice. In this article, we introduce a new high-level
feature based on local syntactic dependencies that an author uses when referring
to a named entity (in our case a person’s name). The series of experiments in the
contexts of movie reviews reveal how the amount of data in both the training and
test sets influences predictive performance. Finally, we measure authorship con-
sistency with respect to this new feature and show how consistency influences
predictive performance. These results provide other researchers with a new model
for how to evaluate new features and suggest that the local syntactic dependencies
warrant further investigation.
.................................................................................................................................................................................
1 Introduction
Accurately determining who wrote a manuscript
has captivated scholars for centuries, because, de-
pending on the texts involved, the results of author-
ship identification can have a profound effect, for
example, on religious documents to establish cred-
ibility, in the legal world for determining royalties,
in literary history for situating a work within a
canon, and in education to detect plagiarism.
One well-known and controversial example of
authorship attribution surrounds the work of
William Shakespeare. Elliott and Valenza tackled
this problem by subjecting Shakespeare’s plays and
poems to sixty-five computer tests, which revealed
fifty-eight full or partial claimants to Shakespeare’s
works (Elliott & Valenza, 1996). The claim-
ants included well-known authors who were
Shakespeare’s contemporaries, such as Francis
Bacon, Robert Burton, John Donne, Ben Jonson,
Christopher Marlowe, Sir Walter Raleigh, Sir
Philip Sidney, and Edmund Spenser. Elliott and
Valenza’s series of tests eventually led them to
the conclusion that ‘however strong or weak the
external evidence may be connecting William
Shakespeare to William Shakespeare’s poems or
plays, we have not changed it’ (Elliott &
Valenza, 1996, p. 210). The debate surrounding
Correspondence:
Ana Lučić,
University of Illinois at
Urbana-Champaign,
Graduate School of Library
and Information Science,
501 E. Daniel Street,
Champaign,
IL 61820, USA.
E-mail:
alucic2@illinois.edu
Digital Scholarship in the Humanities, Vol. 30, No. 1, 2015.  The Author 2013. Published by Oxford University Press on
behalf of EADH. All rights reserved. For Permissions, please email: journals.permissions@oup.com
53
doi:10.1093/llc/fqt033 Advance Access published on 29 June 2013
Shakespeare’s work continues in the literature
(Foster, 1999; Vickers, 2002; Craig & Kinney,
2009), and scholars continue to identify features
that can help differentiate Shakespeare’s writing
style from the writing of other authors, where
each feature offers a different lens through which
Shakespeare’s writing style can be examined.
Another well-known authorship disambiguation
problem is determining the author of religious
manuscripts such as the Bible (Mealand, 1995;
Mills, 2003; Koppel, 2011) and the Quran
(Sayoud, 2012). An enormous amount of physical
effort that earlier manuscript analyses required
coupled with the desire to ensure an impartial ana-
lysis has motivated the development of automated
methods to distinguish between authors. Although
the underlying premise is the same—that each
author has an individual writing style that can, in
fact, be determined and agreed on—efforts to auto-
mate authorship disambiguation have also led to
increased rigor with respect to evaluation. Topic,
genre, and period of the texts compared should be
tightly controlled in authorship attribution experi-
ments (Grieve, 2007; Stamatatos, 2009) to avoid
emphasizing characteristics of a work other than
writing style. These constraints on the evaluation
influence the nature of author disambiguation
experiments. For example, they require that we
analyze Shakespeare’s comedies separately from his
tragedies, and a third experiment to assess his
poetry. However, it is reasonable to expect that
some of Shakespeare’s writing style transcends
these genre boundaries and may indeed blend
these traits as the greatest of his theatrical pieces
certainly do (Greenblatt, 2005). That having been
said, some genres and literary forms apply tight con-
straints to authorship style, such as a haiku poem,
where avenues for authorship creativity can be more
gravely constrained, being as they are formally cir-
cumscribed by questions of imagistic possibility and
semantic reverberation.
In addition to standardizing the evaluation,
efforts to automate authorship attribution have
resulted in a thousand stylometric features that cap-
ture writing style. Such features can be characterized
as lexical, character, syntactic, semantic, and appli-
cation-specific (Stamatatos, 2009). Lexical features
such as function words or the common words and
character features such as character n-grams used
either individually or in combination with some
other feature have been proven to provide reliable
results (Burrows, 2001, 2002a, 2002b; Baayen et al.,
2002; Hoover, 2004; Kešelj et al., 2003; Houvardas &
Stamatatos, 2006). Function words can provide
more insight into style than content words, as they
represent the glue for a sentence, in that they allow
an author to combine words to form more general
sentence constructs. Although surface-level features
such as function words and character n-grams pro-
vide good predictive performance with respect to
authorship disambiguation, syntactic features, such
as part-of-speech, may provide a more nuanced
view of authorship style. It was suggested that
syntactic features are a more reliable authorial
fingerprint than lexical features because authors
unconsciously use similar syntactic patterns
(Stamatatos, 2009, p. 542).
In this article, we introduce a stylometric feature
that considers the local syntactic dependencies
(LSDs) that surround a named entity. This feature
can be generated automatically using natural lan-
guage processing tools that detect named entities
and generate syntactic dependencies of a sentence.
We explore this stylometric feature in the context of
film reviews where only references to people are
considered: thus, for our purposes, a named entity
may refer to an actor, a character in the movie, a
director, or a producer. Stated differently, we focus
on personal names in the sentence and the syntactic
spaces (grammatical structures) that precede and
follow the personal name.
2 Related Work
By the early 1990s, more than 1,000 features have
been proposed for authorship attribution (Rudman,
1998, p. 360). This large number of features
is divided into five broader categories: lexical, char-
acter, syntactic, semantic, and application-specific
(Stamatatos, 2009), where lexical features have
been most extensively tested. In contrast, high-
level features such as syntactic, semantic, and appli-
cation-specific features have not been tested as
A. Lučić and C. L. Blake
54 Digital Scholarship in the Humanities, Vol. 30, No. 1, 2015
comprehensively. The reason for this is that high-
level features can be more difficult to extract from
the text and the parsers necessary to extract such
features are not available for all languages.
Accuracy of automatic parsers can also be an
issue, especially with semantic parsers where the
errors introduced in the process can influence the
results significantly, thus limiting their use vis-à-vis
other features such as those in the lexical and char-
acter set that can generally be extracted more easily
and with a higher accuracy for English texts.
Some reliable markers of style include frequen-
cies of rewrite rules (Baayen et al., 1996), n-grams
of syntactic labels from partial parsing (Hirst &
Feiguina, 2007), n-grams of parts-of-speech
(Diederich et al., 2003), function words (Garcı́a &
Martı́n, 2007), functional lexical features (Argamon
et al., 2007) (as cited in Luyckx & Daelemans, 2011),
and character trigrams (Luyckx & Daelemans, 2011,
p. 43). Each of these features is found with a
relatively high frequency in the text, and each of
them will almost certainly be available in texts of
different sizes. However, it was suggested that
features that are frequent but also unstable in the
text have a greater discriminating power than those
that are frequent but stable (Koppel et al., 2006).
In this instance, instability is defined as a linguistic
alternative for a word or a synonymous construc-
tion that can be used in place of other words. For
example, some of the function words have linguistic
alternatives (e.g. over can be replaced with above),
whereas some do not (e.g. and and or) (Koppel
et al., 2006). Those features that are both frequent
and unstable can better discriminate between differ-
ent author styles than features that are only frequent
but do not have linguistic alternatives (Koppel et al.,
2006). Combining meaningful features—for
example function words and part-of-speech infor-
mation—can achieve better results than the use of
an individual feature alone. Luyckx and Daelemans
(2011), for example, show that a more heteroge-
neous feature set has a positive influence on per-
formance (p. 46).
Although the type of feature or features used
and their characteristics are important and are dir-
ectly related to the disambiguation performance,
a number of other factors also influence the
performance. As Luyckx and Daelemans (2011)
and also Koppel et al. (2011) point out, the per-
formance of features usually decreases with a
larger set of candidate authors. It was reported
(Argamon et al., 2003) that the increase of an
author set size from two to twenty leads to a per-
formance drop of 40% (as cited in Luyckx &
Daelemans, 2011, p. 37). Luyckx and Daelemans
(2011) also confirmed that increasing the author
set size leads to a drop in performance (p. 52) but
established that this drop is influenced by a number
of factors such as the number of topics in the data
set, corpus size, and the choice of methodological
set-up (p. 52). A larger number of candidate authors
in the data set thus represents a more challenging
task than two to ten candidate authors. For larger
and open sets of authorship attribution corpora,
similarity-based measures are more appropriate
than machine-learning algorithms such as Naı̈ve
Bayes and Support Vector Machine that perform
best when just a few candidate authors are involved
(Koppel et al., 2011, p. 84).
Some of the key elements of any authorship
attribution study include the type of feature or a
combination of features that will be used in an
authorship attribution study, the algorithm that
will be used for text classification, and the number
of authors in the candidate set. The training size of
the corpus (amount and length of training text), test
corpus size (length of unseen text), and distribution
of training and corpus size (Stamatatos, 2009) con-
stitute the other crucial elements of any authorship
attribution experiment. The Luyckx and Daelemans
experiment (2011) revealed that increasing the
amount of training data has a direct/positive effect
on performance in three conducted experiments.
Their results echo Moore’s conclusion that ‘there is
no data like more data’ (2001) but also show that
factors such as the number, variety, and type of
topics seem to play an important role as well as, for
example, the distribution of training and test corpus.
One important part of systematic evaluation of
authorship attribution studies is the comparison of
performance of different features and different
methods across different corpora and different
genres. Such comparative evaluations have been
done in the past (but certainly not extensively
A syntactic characterization of authorship style surrounding proper names
Digital Scholarship in the Humanities, Vol. 30, No. 1, 2015 55
enough as Luyckx (2010) points out), and one not-
able example is the Ad-Hoc Authorship Attribution
Competition organized by the Association for
Literary and Linguistic Computing and the
Association for Computers and the Humanities in
2004 (Juola, 2006). These comparative evaluations
with standardized test corpuses can and do eventu-
ally lead to best practices recommendations and
enable new insights into the nature of different writ-
ing style and the potential of different features for
disambiguation.
Another axis of comparison is provided by cross-
language studies that tackle the problem of how a
proposed method generalizes to different languages.
One recent cross-language study showed that the
selection of texts for training and test has an influ-
ence on predictive performance (Eder & Rybicki,
2013). The study also showed that the English lan-
guage shows a fair stability and resistance with
regard to random permutations of texts in training
when compared with other languages (ibid.). Given
that authorship attribution results vary with respect
to the choice of text included in training and test, an
ideal authorship attribution study should assess the
validity of the training set with a large number of
trials (ibid.).
3 Method and Materials
This section introduces the new stylistic feature and
describes how the feature can be used in automated
authorship attribution.
3.1 The potential role of syntax
Syntactic spaces have been described in a study of
writing styles of James Joyce, Joseph Conrad, and
Daniel Defoe, which identified such spaces as a way
to capture different words and different construc-
tions (Gass, 2012). Each author decides how she is
going to close this syntactic space and determines
the combination of words that will round up a par-
ticular syntactic space of a particular work; however,
the unused syntactic space and its potential to be
filled with different constructions hovers, so to speak
above each work:
Sentences must be understood to contain all
sorts of unused syntactical space. These are
places that could be filled with more words,
but, in any specific instance, aren’t. Instead of,
‘The man at the door was an encyclopedia
salesman,’ we could have written, ‘The weary
shabby-suited man at the door was an
Encyclopaedia Britannica salesman.’ Between
any adjective and its noun, more can nearly
always be added. Sentences are like lattice-
work, like fences, to be left open or prudently
closed, their boards wide or narrow, pointy or
level, the spaces between them, ditto (Gass,
2012, pp. 330–31).
Gass’ insight about unused syntactical spaces
within each sentence goes into the heart of what
distinguishes one writing style from another.
Although we know that each and any sentence
could have been reconfigured differently, masterful
narrators have a way of configuring them in a par-
ticular way and that configuration together with
‘unused syntactical spaces’ becomes a reminder of
what-could-have-been and eventually a mark by
which one style can be distinguished from another.
Sentences play an important role in Gass’ analysis
of different styles of writing: they are one of
the most important building blocks of style. For
example, Gass describes Joyce’s sentences in the
following way:
Any of his [Joyce’s] sentences are immediately
recognizable as such, and belong to rubrics
innumerable; moreover, the various phrases,
lengthy clauses, lists, and repetitions they
contain are given the order, correlation, and
symmetry that any high style requires. That is
to say, they are formed to a fare-thee-well
(Gass, 2012, p. 346).
Certainly, the combination of words and deliberate
selection of words to be used in a sentence are not
the only component of somebody’s style of writing:
the topic of writing, genre and the forms and struc-
tures that accompany a particular genre, the intent
of the work, the period in which the work origi-
nated, as well as the intended audience need also
to be taken into account when discussing some-
body’s style of writing. In fact, these numerous
A. Lučić and C. L. Blake
56 Digital Scholarship in the Humanities, Vol. 30, No. 1, 2015
traits become a defining characteristic of style,
consistent with Rudman’s definition (1998):
We should understand that style is a complex
package consisting of a theoretically unique
combination of thousands of individual
traits – a very large but finite number.
Working with a given attribution problem
means that style is a closed system with a
finite number of style markers (p. 358).
The ‘combination of thousands of individual traits’
and the ‘closed system’ within which these traits
operate continue to inspire ever new representations
of text and new stylistic markers.
3.2 Creating an author profile
We introduce a new feature that is represented by
syntactic spaces, grammatical structures that sur-
round references to personal names. Rather than
examine the syntactic structure of the entire
review—the more common approach in authorship
attribution methods—this study focuses on syntac-
tic structures that surround personal names. In the
context of film reviews, the person reference may be
made to an actor, a director, a producer, or a char-
acter, but references to people are made in a range
of genres, including news articles, novels, and his-
torical accounts. The LSDs captured allow us to
identify grammatical structures that the reviewers
use when they refer to personal names and in this
way examine how and to what degree the individual
reviewers’ styles vary when they talk about people.
The local entity syntax is captured automatically
using the following steps:
(1) Sentences are identified for each review
using Natural Language Toolkit tokenizer
(nltk.org). Sentences longer than 80 words
(1,372 sentences) are discarded due to pro-
cessing time constraints.
(2) The dependency grammatical structure of
each sentence is identified using the probabil-
istic Stanford Lexical parser version 1.6.9
(http://nlp.stanford.edu/software/lex-parser.
shtml).
(3) Personal names in the reviews are identified
using the probabilistic Named Entity
Recognizer (http://nlp.stanford.edu/software/
CRF-NER.shtml) version 1.2.2, which, in add-
ition to personal names, identifies locations
and organizations in a sentence. The predom-
inance of personal names in this collection
(approximately 78% of the total number of
entities) compared with locations (approxi-
mately 10%) and organizations (approxi-
mately 12%) allowed us to focus on one
type of entity in this experiment: personal
names.
(4) Local grammatical constructs (two dependen-
cies, one before and one after each personal
name) are identified by aligning the results of
Step 2 with the results from Step 3.
The local grammar considers only two dependen-
cies that occur before (toward the root of the syn-
tactic tree) and after (toward the leaf of the
syntactic tree) for each personal name. The depen-
dencies are then counted and normalized by divid-
ing the number of syntactic dependencies by
the total number of personal names referenced.
Normalization helps to ensure that reviewers who
write longer reviews and use more personal names
are not given more weight than those who write
shorter reviews and use a smaller number of
named entities.
Syntactic dependencies produced by the Stanford
Lexical parser represent binary relations between
pairs of words in a sentence; these are in turn exam-
ined in accordance to the phrase structure tree that
is produced by the Penn Wall Street Journal
Treebank (de Marneffe et al., 2006). Specifically, as
the first step, the head nouns in each sentence are
identified (primarily content words); following this,
the relations between each governor and depend-
ency word (the word that depends on the governor)
are labeled. The resulting syntactic tree represents
a singly rooted directed acyclic graph with no
re-entrancies (de Marneffe et al., 2006). The syntac-
tic dependencies (grammatical relations) that occur
before the name involve the personal name used as
the dependency in the binary relation, whereas the
syntactic dependencies that occur after the name
involve the same name used as the governor in the
triple (governor, dependency, and name of the re-
lation). It is not uncommon that personal names do
not have any dependents (children) in a sentence, in
A syntactic characterization of authorship style surrounding proper names
Digital Scholarship in the Humanities, Vol. 30, No. 1, 2015 57
which case only grammatical relations that occur
before the name are considered.
Figure 1 shows an example of syntactic depen-
dencies within the syntactic tree graph for a sen-
tence: Ben Affleck and Matt Damon also show up
in small roles written by reviewer 1406078. The pro-
posed method would identify the nominal subject
(nsubj) before Affleck. In this case, Affleck is repre-
sented as a dependency of the governor (root) show
through a nominal subject relation (nsubj), whereas
and after Affleck is represented as a dependency of
the governor Affleck through a coordination relation
(cc). Damon is also represented as a dependency of
the governor Affleck through a conjunction relation
(conj). When a person’s full name is used—such as
Ben Affleck and Matt Damon—the name is connected
to the rest of the syntactic dependency through the
head noun, which in this case is the last name. Thus
there is no need to consider noun compound modi-
fiers (nn) that connect the first to the last name.
Figure 1 shows two examples that would not be
part of our syntactic feature, where Ben is connected
to Affleck and Matt is connected to Damon.
Stanford Lexical parser identified thirty-nine
dependencies used before personal names in the
IMDb62 data set and forty-four after personal
names. Dependencies with the lowest frequency
were eliminated, and the author profiles are created
using the remaining thirteen dependencies before
and sixteen after personal names. Table 1 shows the
list of twenty-nine dependencies that were used as the
main feature in this authorship attribution study:
3.3 Using author profiles to assign
authorship attribution
Authorship attribution methods can be divided into
profile-based, instance-based, and hybrid methods
(Stamatatos, 2009, p. 546). If the goal of the experi-
ment is to characterize a general style of an author,
Fig. 1 Syntactic tree graph. The tree graph highlights syn-
tactic dependencies that occur before and after personal
names (main features of this study)
Table 1 The twenty-nine local syntactic dependencies
(LSDs) considered in this experiment (de Marneffe, M.
& Manning, C.D., 2008) (caption)
Variable Typed dependencies used in the model.
Grammatical relations that are closer to the
root of the dependency tree (with respect to
the personal name) are considered ‘‘before’’
and conversely dependencies that are closer
to the leaf of the tree are considered ‘‘after’’
poss possession modifier before personal name
dep dependent before personal name
appos appositional modifier before personal name
conj conjunction before personal name
nsubjpassive passive nominal subject before personal name
ccomp clausal complement before personal name
advmod adverbial modifier before personal name
pobj object of a preposition before personal name
nsubj nominal subject before personal name
root root before personal name
xcomp open clausal component before personal name
dobj direct object before personal name
iobj indirect object before personal name
possaft possession modifier after personal name
depaft dependent after personal name
copaft copula after personal name
punctaft punctuation mark after personal name
rcmodaft relative clause modifier after personal name
apposaft appositional modifier after personal name
conjaft conjunction after personal name
advmodaft adverbial modifier after personal name
prepaft prepositional modifier after personal name
detaft determiner after personal name
partmodaft participial modifier after personal name
nsubjaft nominal subject after personal name
ccaft coordination after personal name
auxaft auxiliary after personal name
dobjaft direct object after personal name
possessiveaft possessive modifier after personal name
Each reviewer’s profile is represented through twenty-nine LSDs
listed in Table 1 (legend).
A. Lučić and C. L. Blake
58 Digital Scholarship in the Humanities, Vol. 30, No. 1, 2015
then a profile-based approach is selected, and when
the goal is to characterize the style of an individual
document, an instance-based approach is used.
These two approaches differ in how they treat train-
ing documents. In the profile-based approach, indi-
vidual instances of document are concatenated into
one file and the proportions of features are calcu-
lated using all the data, whereas the author profile of
an instance-based approach considers each individ-
ual document separately.
Earlier authorship attribution experiment that
used the IMDb62 data set found that ‘the differences
between individual documents by each author may
be too large to yield a meaningful representation
of the author if they are considered separately’
(Seroussi et al., 2011, p. 8). Given that the average
word length of a movie review in this data set is 314
(minimum is 6 and maximum 1,957 words), we
opted for a profile-based approach in the experi-
ment described in this article.
Our experiments use the vector-based model
to represent features; however, unlike the bag-of-
word-based models, the vector in these experiments
capture the syntactic features described in Section 3.1.
Each set of documents in the training and test set
becomes a single vector, and the distance between
training and test vectors is calculated using cosine
similarity. In the authorship attribution experi-
ments, the smallest cosine angle between the
text of unknown authorship and the training file cor-
responds to the highest similarity found. As with the
training set, named entities are first identified in the
test set and then used to calculate the proportion
for each of the twenty-nine grammatical relations
shown in Table 1.
Many instance-based approaches to authorship
attribution that use smaller candidate sets typically
use machine learning classification algorithms
such as Naı̈ve Bayes, decision trees, and Support
Vector Machine. However, for large and open
candidate sets, similarity measures are more ap-
propriate, as it may be difficult to learn the clas-
sifier for a large number of classes (Koppel et al.,
2011, p. 86). One example is the experiment con-
ducted by Koppel et al. (2011) that considered a
blog corpus and used cosine similarity as the
similarity measure.
3.4 Data set
The publicly available data set (IMDb62) used in
these experiments comprises 62,000 movie reviews
written by 62 reviewers—1,000 reviews per re-
viewer—that were harvested from the IMDb Web
site (imdb.com) in 2009 (Seroussi et al. 2010). Each
author’s reviews were obtained using proportional
sampling without replacement such that for each
reviewer, the 1,000 reviews have the same rating
frequencies as the reviewer’s complete set of reviews.
The data include the userID, reviewID, rating, title,
and content, where the userID and reviewID repre-
sent unique author and review identifiers on the
IMDb Web site, respectively. For example,
user¼ 33913 has the following page on the
imdb.com: http://www.imdb.com/user/ur0033913/
comments and reviewID¼444584 can be found on
the following imdb.com page: http://www.imdb.
com/title/tt0444584. Explicit ratings are filtered
out from the text and minimal processing was
applied to the titles and content of the reviews.
This data set has been used in two earlier experi-
ments that inferred sentiment and ratings from the
movie reviews (Seroussi et al., 2010). The authors
used a text-based model of user similarity, which
they then use to train the classifiers for automatic
inference of sentiments or ratings. The text-based
measures outperformed the rating-based measures
in most cases (ibid., p. 206).
The experiments described in the second study
involve three authorship attribution experiments,
each of which involved the representation of a docu-
ment using the Latent Dirichlet allocation (LDA)
model (Seroussi et al., 2011). The first experiment
involved an authorship attribution set with only
three candidate authors, the second one used the
IMDb62 data set with tens of authors (62), and
the third one drew on a Blog data set consisting of
678,161 blog posts and written by 19,320 authors.
With tens of authors and thousands of authors in
the IMDb62 and the Blog data sets, the LDA-based
representation of the text using tokens and
Hellinger distance to calculate the distance between
texts yielded state-of-the-art performance in terms
of classification accuracy (p. 9). In the case of the
binary authorship attribution problem, which
involved a data set with three candidate authors,
A syntactic characterization of authorship style surrounding proper names
Digital Scholarship in the Humanities, Vol. 30, No. 1, 2015 59
the LDA-based representation of the text was com-
parable with the results obtained when using tokens
directly to represent the text (p. 6). The three ex-
periments confirm the benefits of representing
documents through LDA-based model that can sig-
nificantly reduce the feature space.
The accuracy achieved in this experiment that
used the IMDb62 data set varied between 19 and
68% for the multi-document approach and between
25 and 81% for a single-document approach, de-
pending on how many topics were used in the
model. In contrast to the results presented here,
that work was evaluated with respect to predicting
individual reviews. Seroussi et al. also used the KOP
method (Koppel et al., 2011) on the same collection.
The KOP method assumes that the authors’ style of
writing can be distinguished by different subsets of
token feature space (space-free character 4-grams).
Depending on the number of number of iterations
(k1) and the number of different subsets of token
feature space (k2), the accuracy of KOP method
varied between 9 and 72%. The highest accuracy
was achieved with 400 subsets with more than
30,000 features each (72%) (Seroussi et al., 2011,
p.8). Seroussi et al.’s method (using LDA) outper-
formed the KOP method (Koppel et al., 2011) by
about 15% at 150 topics, and the results were stat-
istically significant.
3.5 Evaluation strategy
The IMDb62 data set represents an authorship at-
tribution set with a fairly large number of candidate
authors, sixty-two. This large number of candidate
authors represents a more challenging task for an
authorship attribution experiment. It is likely that
a feature selected may show better performance
when used on a candidate author set of two and
up to ten authors rather than on a set of sixty-two
candidate authors.
In addition to being characterized by a relatively
large number of candidate authors, the IMDb62 col-
lection satisfies the genre, topic, and period require-
ments. The movie reviews belong to the film
criticism genre (and non-specialist criticism, to de-
limit it further). Given that movies can be seen as an
overarching topic of the reviews, the reviews satisfy
the same topic requirement (although it is also
certainly the case that the discussions of the
movies, plot descriptions, and character analysis
depend on the topic of the movie and thus are
likely to vary throughout collection). Given that
the database originated in 1990, none of the movie
reviews will be older than 20 years, which puts them
in roughly the same period category. Information
such as gender, age, and education level of the re-
viewers, which would be controlled in an ideal
authorship attribution collection, is not available
and thus was not controlled.
The IMDb62 collection was selected because
movie reviews include a large number of references
to personal names. This data set, in other words,
represents a rich background against which we can
analyze how different authors refer to people and
the constructions and patterns that they use when
they discuss other people and characters. References
to people are a frequent, prominent, and also un-
stable feature in this collection. However, although
this feature shares some of the characteristics of
function words, part-of-speech information, charac-
ter n-grams, and other features frequently used in
authorship attribution studies, references to people
also differ from these feature in the following way:
references to people may or may not exist in the
text, or, put differently, not all the reviews in this
collection will include them, and this eventually
places this feature on a different end of the spectrum
of features more commonly used in authorship
attribution studies. On the one hand, references to
people are likely to occur frequently in movie
reviews; on the other hand, not all the reviews in
the data set are bound to contain them. In fact,
8.5% of the reviews in the entire data set do not
contain references to personal names—these reviews
were excluded from the data set, which reduced the
number of available reviews to 56,978 (from the
original 62,000). In search of the features that inter-
est us, we also excluded the sentences from the
reviews that did not include references to personal
names, which left us with approximately a third of
the entire number of sentences in the data set,
353,837 (38.49%) of 919,203, once the sentences
that had more than 80 words were eliminated.
With this reduced data set, we obtained a collection
of 56,978 reviews and 353,837 sentences containing
A. Lučić and C. L. Blake
60 Digital Scholarship in the Humanities, Vol. 30, No. 1, 2015
references to personal names. Although 8.5% of
the reviews cannot be considered or evaluated
using this method, the profile-based approach
used in this study mitigates this fact because the
model is learned by concatenating a number of
reviews into one file rather than on the basis of
individual movie review instances. The large
number of reviews per reviewer in the data set was
beneficial in another way: it allowed us to obtain
different sampling quantities of different text train-
ing/test proportions even after the reviews that had
not included references to personal names were
excluded.
4 Results and Discussion
In addition to predictive accuracy, a good stylistic
feature should appear in most of the texts in
the genre explored, and be a reliable style marker.
The following experiments evaluate the degree to
which these properties hold for the author profile
that is based on local syntax that surrounds a named
entity.
4.1 Distribution of named entities
The proposed stylistic feature is based on how an
author refers to people. Figure 2 shows the distri-
bution of the reviews that contain personal names
over sixty-two authors through a histogram, where
the x-axis reflects the number of reviews that con-
tain personal names. The results show that more
than half of the reviewers (forty-five) make at least
one person reference in almost all of their reviews
(900/1000). And that the majority of the reviewers
use personal names regularly throughout their
reviews. Six of the sixty-two reviewers make fewer
references to personal names, such as reviewer
70535, who has the lowest number of reviews
containing at least one personal name: only 503 of
1000 of his (or her) reviews contain personal names.
Five reviewers use personal names in approximately
650–700 reviews, and eleven use personal names in
approximately 700–900.
Figure 2 captures the overall reviews and Figure 3
shows the distribution of sentences that contain per-
sonal names in the reviews, where the x-axis repre-
sents the number of sentences. The distribution
shown in Figure 3 is less skewed than the overall
reviews. Five reviewers wrote less than 2,000 sen-
tences with at least one personal name, but the ma-
jority of the reviewers (44) have from 2,000 to 8,000
sentences with personal names, and 13 reviewers
each wrote more than 8,000 sentences with personal
names. On average, 5,645 or 38% of the total
number of sentences—those that include and
Fig. 2 Distribution of reviews containing personal names
over sixty-two authors. The x-axis reflects the number of
reviews that include personal names. The histogram indi-
cates that more than half of the reviewers (forty-five) use
at least one reference to a personal name in the majority
of their reviews (900–1,000)
Fig. 3 Distribution of sentences containing personal
names over sixty-two authors. The x-axis reflects the
number of sentences that contain personal names. The
histogram indicates that the majority of the reviewers
(forty-four) use between 2,000 and 8,000 sentences that
contain references to personal names
A syntactic characterization of authorship style surrounding proper names
Digital Scholarship in the Humanities, Vol. 30, No. 1, 2015 61
those that do not include references to personal
names—written by a reviewer will contain at least
one reference to a personal name.
Figure 4 shows the total number of personal
names used for different authors. As expected, this
distribution resembles Figure 3 and shows that the
majority of the reviewers make between 5,000 and
20,000 personal name references in their reviews.
The average number of personal names used per
reviewer is 10,042, which suggests that a reviewer
will typically refer to more than one person in a
sentence.
4.2 Predictive performance with
controlled sampling
As shown in the previous section, the majority of
sentences in movie reviews do not refer to actors,
directors, or producers, specifically, an average
movie review has approximately 15 sentences (min-
imum 1 and maximum 140) and of those, an aver-
age of 6 sentences refer to a person (minimum 1 and
maximum 56). Moreover, the entire collection of
movie reviews contains 1,000 reviews for each
author, but not all reviews refer to people (see
Fig. 2). Our goal in this series of experiments is to
understand the relationship between the quantity
of information in the training and test set and the
predictive performance, so we considered only
those reviews that contained person references.
Specifically, a sample of 650 reviews that contained
at least one person reference by each reviewer was
selected at random for sixty of the sixty-two
authors. Only two authors had fewer than 650
reviews. One reviewer had 503 reviews, so the fol-
lowing averages do not consider this reviewer:
100 reviews in test and 450 reviews in training
sample, 150 reviews in test and 400 reviews or
more in training, and 200 reviews in test and 350
or more reviews in training. Similarly, one author
had 646 reviews and was thus not considered in the
200 test with 450 training average.
Figure 5 shows how the average rank improves
with respect to the amount of data in the training
and test sets, where a perfect system would have the
correct author ranked in position 1. Each point in
this figure is averaged over ten random samples for
each of the authors, meaning that the results of this
study are based on ten iterations of training and test
data (see Eder & Rybicki’s 2013 study referenced in
Section 2). With fifty reviews in the test set for each
author, the average rank improves from 3.6 to 1.8 as
the number of reviews in training set increases from
50 to 450, but near-optimal performance can be
achieved with about 250 reviews in the training
set. Adding an additional 50 reviews to the test
set (for a total of 100) improves the average rank
from 2.9 to 1.3 as the number of reviews in training
increases from 50 to 450. As with the earlier sample
sizes, performance hovers around 1.3 with around
225 reviews in the training set. Adding more records
to the test set improves overall performance, but
there is little difference in the predictive power
of the model between the 150 and 200 reviews
sample. In the 150 review test sample, the average
rank improves from 2.9 to 1.1, and in the 200 review
test sample, the average rank improves from 2.6
to 1.0. As with the 100 sample, performance
improvements slow down when about 225 articles
are in the training set.
In summary, best predictive results can be
achieved using 250 reviews in the training set and
between 150 and 200 reviews in the test set (1.19
and 1.15 average correct attribution rank) or 300
reviews in the training set and 150 or 200 in test
(1.17 and 1.09 average correct attribution rank).
Fig. 4 Distribution of personal names over sixty-two
authors. The x-axis reflects the number of personal
names in reviews. The histogram indicates that the ma-
jority of the reviewers use between 5,000 and 20,000 ref-
erences to personal names
A. Lučić and C. L. Blake
62 Digital Scholarship in the Humanities, Vol. 30, No. 1, 2015
There is a noticeable drop in the average correct
attribution rank when fewer than 250 reviews are
used in the training set or fewer than 150 reviews
are used in test. This suggests that, on average, 1,500
sentences that refer to people are needed for training
and 900 for test to obtain optimal predictive
performance.
Although Figure 5 captures the overall predict-
ive performance, averaging random samples of all
authors obscures our ability to understand
how this new syntactic feature might improve
authorship disambiguation. The data presented in
Figure 6a-d use the same controlled sample
described previously, and show the distribution of
performance.
As Figure 6a indicates, it is only when the
number of reviews in training is 400 and in test
50 that all the authors are in between the first and
fifth rank (1st being the top match). Figure 6b
indicates that starting with 350 reviews and
going up to 450 in training and 100 in test, the
average rank of all the reviewers does not go
beyond 4. Figure 6c indicates that starting with
250 reviews in training and going up to 450 re-
views and 150 in test, all the reviewers are in be-
tween the 1st and 3rd rank and 62% of the
authors or thirty-nine of them have an average
rank of 1 (top match). Figure 6d indicates that
starting with 300 reviews and going up to 450 in
training and 200 reviews in test samples, all the
reviewers are in between 1st and 2nd rank. The
number of authors who are top matched across
ten experiment runs is also large, 70% of the au-
thors (44) are always top matched when 300 and
350 reviews were used in training tables, 80% (50)
reviewers are always top matched with 400 reviews
in training, and 67.8% (42) authors are always top
matched in the experiment that used 450 conca-
tenated reviews in training.
The lowest performance is observed with fifty
reviews in training: the average rank goes up to
16 (50 reviews in test), 13 (100 reviews in test),
14 (150 reviews in test), and 11 (200 reviews in
test). It is noticeable that regardless of the increase
of number of reviews in test, a relatively low number
of reviews in training samples (fifty) is not accom-
panied with a significant increase in accuracy.
However, starting with 200 reviews in training, we
observe performance stabilization and an increase in
accuracy measured through average rank achieved
across ten runs with different ten sampling propor-
tions (50, 100, 150, and 200).
Based on these results, the optimal number of
reviews in training tables begins at 250 and goes
up to 450 and in the test sets starts at 100 and
goes up to 200, although, if we consider having all
the authors in the data set covered by rank 5 to be a
fair result, a good performance is also achieved with
Fig. 5 Averaged attribution performance with 50–200 reviews in test and 100–450 reviews in training that contain at
least one personal name. The line graph indicates that the best predictive results are achieved with 250 reviews in
training and between 150 and 200 reviews in test and also with 300 reviews in training and between 150 and 200 reviews
in test
A syntactic characterization of authorship style surrounding proper names
Digital Scholarship in the Humanities, Vol. 30, No. 1, 2015 63
as few as 200 reviews in training and 100, 150, and
200 reviews in test. Both analyses suggest that ap-
proximately 1,500 (or more) named entity sentences
in training and approximately 900 named entity
sentences (or more) in test are needed for good
predictive performance of this method.
4.3 Predictive performance with less
data/reviews in the test set
The following experiments test an even smaller
amount of data/reviews in the test set using the
same controlled sample from the earlier experiment
(650 reviews that had at least one personal name by
each reviewer). For example, in the 5/100 experi-
ment, the test set comprises five reviews that are
selected at random for each author and 100 reviews
in training. With respect to the earlier experiment,
these results focus on a smaller number of reviews in
the test set and thus allow us to better understand
predictive performance with limited quantities of
text.
As shown in Figure 7, with 100 reviews in train-
ing, the average rank increases from 11.01 to 2.5 as
Fig. 6 (a–d) Distribution of attribution performance with 50–200 reviews in test and 100–450 reviews in training that
contain at least one personal name—averaged over ten random samples. This bar graph indicates that the optimal
performance is achieved with 250 reviews and more (up to 450) in training and 100 reviews in test and more (up to
200). This number of reviews translates into approximately 1,500 named entity sentences or more in training and
approximately 900 named entity sentences or more in test
A. Lučić and C. L. Blake
64 Digital Scholarship in the Humanities, Vol. 30, No. 1, 2015
the number of reviews in test increases from 5 to 45.
Similarly, with 200 reviews in the training set, the
average rank increases from 9.93 to 2.06 as the
number of individual reviews in test increases
from 5 to 45. Similar increases were found in the
300 training set, which showed an increase from
9.88 to 2.21, and the 400 training set that increased
from 9.92 to 1.8. These results suggest that increas-
ing the training set size from 100 to 200 improves
predictive performance, but further increases in the
training set size show only small increases in pre-
dictive performance. In contrast, increasing the
number of reviews in the test set size from 5 to 45
can have a much larger impact on predictive per-
formance by increasing the rank from 10–12 to 2.
Figure 7 shows performance averaged over all
authors in the collection, whereas Figure 8 provides
a more detailed view by providing the distribution
of the rank averaged over ten random samples. With
only 100 reviews in the training set and at least
40 reviews in the test set, the correct author will
be in rank 1 or 2 for more than half of the authors.
As Figure 8a-d indicates, the performance or the
number of authors who are top matched increases
as the number of individual reviews in the test set
increases from 5 to 45 and also as the number of
reviews in training increases from 100 to 400. With
45 reviews in the test set and 100 in training, the
number of authors who are top ranked is 16 or
25.8% accuracy, whereas in the experiment that
used 45 reviews in the test set and 400 reviews in
training, the number of authors who are top ranked
increased from 16 to 22 or 35.5% accuracy.
4.4 Sentence-level predictive
performance
Thus far, performance has been measured with re-
spect to the number of reviews that, on average,
comprise 314 words. The number of sentences per
review, however, varies among authors. Moreover,
the proposed method considers only sentences that
include personal names. Finally, movie reviews are
much shorter than a book or a historical article, so
reporting results with respect to sentences that con-
tain a person name may be helpful for researchers
working in other genres.
The results in Figure 9 suggest that approximately
ten sentences in the test set (about two reviews) are
not sufficient to achieve accurate authorship attribu-
tion. Further, 100–200 named entity sentences in test
are also associated with the higher average rank and
lower performance. As the number of sentences in
both test and training increases, however, so does the
average rank. Eventually, as this sample indicates, the
Fig. 7 Averaged attribution performance with 5–45 reviews in test and 100–400 reviews in training that contain at least
one personal name. The line graph indicates that although increasing the number of reviews in training from 100 to 200
improves predictive performance of this method, further increases in the number of reviews in training is accompanied
with only a small impact on performance. Increasing the number of reviews in test from 5 to 45, however, is
accompanied by a much larger impact on predictive performance: the average rank increases from 10–12 to 2
A syntactic characterization of authorship style surrounding proper names
Digital Scholarship in the Humanities, Vol. 30, No. 1, 2015 65
best result is achieved with approximately 1,000
named entity sentences in the test set and more than
1,000 named entity sentences in the training set. This
result is in accordance with the results from the first
experiment that showed that more than 1,000 sen-
tences in training (1,500) and approximately 900
sentences in test can achieve good disambiguation
results. The results indicate that the increase in the
number of sentences that contain references to
people in both the test and training sets is followed
by an increase in performance, and yet the largest
improvement in performance is associated with the
increased number of sentences in the test set.
4.5 Reliability
In addition to predictive accuracy, features used
to automatically assign authorship attribution are
expected to be relatively stable with respect to an
individual author. For example, you would expect
that the way in which a movie reviewer refers to
actors, producers, and directors would remain
fairly constant for different movies. The following
analysis enables us to understand author variability
with respect to discussing people in a review. At
the same time, the expectation is that increases in
author variability would eventually lead to lower
predictive performance.
Fig. 8 (a–d) Distribution of attribution performance with 5–45 reviews in test and 100–400 reviews in training using
reviews that contain at least one personal name—averaged over ten random samples. The bar graph indicates that the
number of authors who are top matched increases as the number of reviews in test increases from 5 to 45 in test and
from 100 to 400 in training
A. Lučić and C. L. Blake
66 Digital Scholarship in the Humanities, Vol. 30, No. 1, 2015
To examine the relationship between predictive
accuracy and reliability, individual author profiles
were created using reviews that were split evenly
between the training and test sets. For each set,
the average and standard deviation of each individ-
ual feature was calculated and averaged across ten
random samples. Figure 10 shows the relationship
between the average standard deviation (over all
twenty-nine features) for each author and the aver-
age rank. The scatter plot indicates that a higher
average standard deviation is associated with lower
predictive performance. Standard deviations above
0.015 are frequently associated with higher average
rank for an author, and this in turn implies a lower
predictive performance. These results indicate that a
consistency in style—which may, after all, be a result
of a variety of factors that could range from a more
highly developed or sophisticated style to a merely
more predictive style of writing—leads to a better
average rank of an individual author.
As expected, reviewers with the lowest number of
reviews with references to personal names, such as
reviewers 70535 and 783721, have a higher average
rank and thus lower predictive performance. This
Fig. 9 The relationship between average rank and the number of named entity sentences. The scatter plot shows how
increasing the number of sentences from 10 to 1,000 in test increases the average rank. The best performance (of rank 1)
is achieved with approximately 1,000 sentences in test set
0.0250
0.0200
0.0150
0.0100
0.0050
0.0000
0 2 4 6 8 10 12 14 16
Average rank
A
ve
ra
ge
 s
ta
nd
ar
d 
de
vi
at
io
n
Fig. 10 The relationship between standard deviation of
sixty-two individual writing styles with regard to the use
of syntactic dependencies that surround personal names
and the average rank. The scatter plot suggests that a more
variable (less consistent) authorship style (higher average
standard deviation) tends to decrease system performance
(the average rank)
A syntactic characterization of authorship style surrounding proper names
Digital Scholarship in the Humanities, Vol. 30, No. 1, 2015 67
analysis combines both the number of reviews and
consistency of authors, so it is not surprising that
lower standard deviations that imply less variability
and more consistent style of writing are more likely
to achieve the optimal performance of rank 1.
5 Conclusion
This article introduces a new method for automated
authorship disambiguation that leverages grammat-
ical structures that precede and follow an author’s
reference to a named entity. The proposed twenty-
nine high-level features thus combine two natural
language processing techniques, specifically named
entity recognition and dependency parsing. A series
of experiments were then conducted to demonstrate
the utility of the LSD method in the context of per-
sonal names used in a collection of movie reviews,
where the personal names could be actors, charac-
ters, directors, or producers.
Movie reviews are fairly short compared with
other manuscripts such as books, comprising only
314 words on average. In the 1,000 reviews that were
available for each author in the IMDb62 collection,
an average of 5,645 sentences (38%) of the total
number of sentences written by each reviewer refer
to people. Of the 62,000 reviews, only 5,022 (8.5%)
did not contain at least one personal name. The
method proposed here cannot be used with these
reviews and they were excluded from the subsequent
analysis. Further work needs to be done to explore
the degree to which longer texts might remove this
limitation.
The first experiment explored the effect of the
training and test set size on predictive performance.
The results show that the twenty-nine proposed
features achieve good authorship disambiguation,
(an average rank less than 1.5) with 250 reviews
in the training set (1,500 sentences with at least
one personal name) and 150 reviews (900 sentences
with at least one personal name) in the test set.
The predictive performance continued to increase
with increases in the number of reviews to 450 in
the training set and 200 in the test set, but these
increases in the quantity of data did not produce a
significant increase in predictive power.
Interestingly, the proportion of names to tokens
in the IMDb62 collection of movie reviews was
similar to the proportion of names to tokens in
novels, which has been reported as between 2
and 3.5% of the overall number of tokens (Van
Dalen-Oskam, 2012). The same study reported
that the number of number of personal names
varied between 18 (Cormac McCarthy, The Road)
and 5,851 (Tonke Dragt, De brief voor de koning),
with an average of 1,454 personal names in the
64 novels considered (Van Dalen-Oskam, 2012,
Table 4). If novelists do behave in a similar way to
movie reviewers, the approach proposed here could
be used to predict the authorship of a novel (test
set), given another novel with known authorship
(training set).
One way to improve the predictive performance
in an authorship disambiguation setting is to simply
provide more features. For example, earlier experi-
ments with IMDb62 considered hundreds of topic
features and much of the previous work on
automated authorship disambiguation consider
thousands of features. The twenty-nine features
introduced here could certainly be added to the
existing feature collections, and yet such an experi-
mental approach provides little insight into the
underlying predictive power of a single-feature
selection strategy. Instead, this article pushes the
limits on the predictive power of just one feature
strategy so that other researchers can estimate the
amount of information required to use this strategy,
or, alternatively, if the amount of information is set,
they can estimate the predictive performance.
The first experiment was extended by reducing the
number of reviews in the training set from 400 to
100 and the number of reviews in the test set from
45 to 5 (approximately 30 sentences) (see Fig. 7).
The predictive power with this limited data set was
much reduced, with the correct author in the top 10
when only 100 reviews in training and 30 reviews
in the test set were available and the author was in
the top 3 ranks with 400 and 45 reviews in the
training and test set, respectively. Although we do
not recommend using this strategy alone with such
a limited set of data, this approach could be used to
narrow the number of candidate authors from the
initial set of sixty-two to the top ten to fifteen, and
A. Lučić and C. L. Blake
68 Digital Scholarship in the Humanities, Vol. 30, No. 1, 2015
then additional features could be generated for this
subset of authors.
One of our goals was to understand the degree to
which the proposed feature set captures authorship
style. The last experiment introduces a new way to
characterize author consistency with respect to a set
of features. The new approach measures the stand-
ard deviation based on feature vectors that were
created using ten samples drawn at random.
Results support the expectation that better predict-
ive performance for an author can be achieved when
the feature set does not vary greatly. This approach
could be used to identify reliable features for a par-
ticular author. More importantly, we could start to
characterize an author’s style by identifying sets of
features based on their variability.
Identifying an author accurately will continue to
have important consequences in a range of domains
such as scholarship, law, and religion. The results
thus far suggest that the LSD approach that uses
syntactic dependencies that surround personal ref-
erences in the text can achieve good predictive per-
formance (as defined by an average rank of less than
1.5, where a rank of 1 is perfect performance) using
a relatively small number of sentences (1,000–
1,500), and a very small number of features (29)
in movie reviews. Moreover, such results suggest
that exploring the proposed set of features in
other genres is warranted.
Funding
This project is made possible by a grant from the
U.S. Institute of Museum and Library Services
(RE-05-12-0054-12).
References
Argamon, S., Šarić, M., and Stein, S. (2003). Style mining
of electronic messages for multiple authorship discrimin-
ation: First results, Proceedings of the 9th ACM SIGKDD
International Conference on Knowledge Discovery and
Data Mining. Washington DC: Association for
Computing Machinery, pp. 475–80.
Argamon, S., Whitelaw, C., Chase, P., Hota, S. R.,
Garg, N., and Levitan, S. (2007). Stylistic text classifi-
cation using functional lexical features. Journal of the
American Society of Information Science and Technology,
58(6): 802–22.
Baayen, H., van Halteren, H., Neijt, A., and Tweedie, F.
(2002). An experiment in authorship attribution.
Proceedings of JADT 2002: 6th International Conference
on Textual Data Statistical Analysis. St. Malo. France:
Les journées internationales d’analyse statistique des
données textuelles, pp. 29–37.
Baayen, H., van Halteren, H., and Tweedie, F. (1996).
Outside the cave of shadows: using stylistic annotation
to enhance authorship attribution. Literary and
Linguistic Computing, 11(3): 121–32.
Burrows, J. F. (2001). Questions of authorship: attri-
bution and beyond. Computers and the Humanities,
37(1): 5–32.
Burrows, J. F. (2002a). Delta: a measure of stylistic
difference and a guide to likely authorship. Literary
and Linguistic Computing, 17(3): 267–87.
Burrows, J. F. (2002b). The Englishing of juvenal: com-
putational stylistics and translated texts. Style, 36(4):
677–99.
Craig, H. and Kinney, A. F. (2009). Shakespeare,
Computers, and the Mystery of Authorship. Cambridge:
Cambridge University Press.
de Marneffe, M., MacCartney, B., and Manning, C. D.
(2006). Generating typed dependency parses from
phrase structure parses. http://nlp.stanford.edu/manning/
papers/LREC_2.pdf (accessed 1 November 2012).
de Marneffe, M. and Manning, C. D. (2008). The
Stanford typed dependencies manual. http://nlp.
stanford.edu/software/dependencies_manual.pdf (ac-
cessed 1 November 2012).
Diederich, J., Kindermann, J., Leopold, E., and Paass, G.
(2003). Authorship attribution with support vector
machines. Applied Intelligence, 19(1/2): 109–23.
Eder, M. and Rybicki, J. (2013). Do birds of a feather
really flock together, or how to choose training samples
for authorship attribution. Literary and Linguistic
Computing, 28(2): 229–36.
Elliott, W. E. Y. and Valenza, R. J. (1996). And then there
were none: winnowing the Shakespeare claimants.
Computers and the Humanities, 30: 191–245.
Foster, W. D. (1999). The Claremont Shakespeare
authorship clinic: how severe are the problems?
Computers and the Humanities, 32(6): 491–510.
Garcı́a, A. M. and Martı́n, J. C. (2007). Function words
in authorship attribution studies. Literary and
Linguistic Computing, 22(1): 49–66.
A syntactic characterization of authorship style surrounding proper names
Digital Scholarship in the Humanities, Vol. 30, No. 1, 2015 69
Gass, W. H. (2012). Life Sentences: Literary Judgments
and Accounts. New York: Alfred A. Knopf.
Greenblatt, S. (2005). Will in the World: How Shakespeare
Became Shakespeare. New York: W. Norton.
Grieve, J. (2007). Quantitative authorship attribution:
an evaluation of techniques. Literary and Linguistic
Computing, 22(3): 251–70.
Hirst, G. and Feiguina, O. (2007). Bigrams of syntactic
labels for authorship disambiguation of short texts.
Literary and Linguistic Computing, 22(4): 405–17.
Hoover, D. L. (2004). Delta prime? Literary and Linguistic
Computing, 19(4): 477–95.
Houvardas, J. and Stamatatos, E. (2006). N-Gram feature
selection for authorship identification. In Euzenat, J.
and Domingue, J. (eds), Proceedings of the 12th
International Conference on Artificial Intelligence:
Methodology, Systems, and Applications. Berlin:
Springer-Verlag, pp. 77–86.
Juola, P. (2006). Authorship Attribution. Foundations and
Trends in Information Retrieval, 1(3): 233–334.
Kešelj, V., Peng, F., Cercone, N., and Thomas, C. (2003).
N-gram-based author profiles for authorship attribution.
Proceedings of the Conference Pacific Association for
Computational Linguistics. Halifax, Canada: Dalhousie
University, pp. 255–64.
Koppel, M., Akiva, N., and Dagan, I. (2006). Feature
instability as a criterion for selecting potential style
markers: Special Topic Section on Computational
Analysis of Style. Journal of the American Society for
Information Science and Technology, 57(11): 1519–25.
Koppel, M., Akiva, N., Dershowitz, I., and
Dershowitz, N. (2011). Unsupervised decomposition of
a document into authorial components. Proceedings
of the 49th Annual Meeting of the Association for
Computational Linguistics: Human Language
Technologies. Portland, OR: Association for
Computational Linguistics, pp. 1356–64.
Luyckx, K. (2010). Scalability Issues in Authorship
Attribution. Brussels: University Press Antwerp.
Luyckx, K. and Daelemans, W. (2011). The effect
of author set size and data set size in authorship
attribution. Literary and Linguistic Computing, 26(1):
35–55.
Mealand, D. L. (1995). Correspondence analysis of
Luke. Literary and Linguistic Computing, 10(3):
171–82.
Mills, D. E. (2003). Authorship attribution applied to the
bible. Master thesis, Texas Tech University.
Moore, R. (2001). There’s no data like more data (but
when will enough be enough?). Proceedings of the IEEE
International Workshop on Intelligent Signal Processing.
Budapest, Hungary: IEEE.
Rudman, J. (1998). The state of authorship attribution
studies: some problems and solutions. Computers and
the Humanities, 31(4): 351–65.
Sayoud, H. (2012). Author discrimination between the
Holy Quran and Prophet’s statements. Literary and
Linguistic Computing, 27(4): 427–44.
Seroussi, Y., Zukerman, I., and Bohnert, F. (2010).
Collaborative inference of sentiments from texts. In
De Bra, P., Kobsa, A., and Chin, D. (eds), Proceedings
of the 18th International Conference on User Name,
Adaptation, and Personalization, vol. 6075 of Lecture
Notes in Computer Science. Berlin: Springer-Verlag,
pp. 195–206.
Seroussi, Y., Zukerman, I., and Bohnert, F. (2011).
Authorship attribution with latent Dirichlet allocation.
Proceedings of the 15th International Conference on
Computational Natural Language Learning. Portland,
OR: Association for Computational Linguistics,
pp. 181–189.
Stamatatos, E. (2009). A survey of modern authorship
attribution methods. Journal of the American Society
for Information Science and Technology, 60(3):
538–56.
Van Dalen-Oskam, K. (2012). Names in novels: an
experiment in computational stylistics. Literary and
Linguistic Computing, Advance Access published
March 9, 2012.
Vickers, B. (2002). ‘Counterfeiting’ Shakespeare: Evidence,
Authorship and John Ford’s Funerall Elegye.
Cambridge: Cambridge University Press.
A. Lučić and C. L. Blake
70 Digital Scholarship in the Humanities, Vol. 30, No. 1, 2015
