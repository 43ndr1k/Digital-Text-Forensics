Authorship Attribution with Latent Dirichlet Allocation
Yanir Seroussi Ingrid Zukerman
Clayton School of Information Technology
Faculty of Information Technology, Monash University
Clayton, Victoria 3800, Australia
firstname.lastname@monash.edu
Fabian Bohnert
Abstract
The problem of authorship attribution – attributing texts to their original authors – has been an active
research area since the end of the 19th century, attracting increased interest in the last decade. Most of
the work on authorship attribution focuses on scenarios with only a few candidate authors, but recently
considered cases with tens to thousands of candidate authors were found to be much more challenging. In
this report, we propose ways of employing Latent Dirichlet Allocation in authorship attribution. We show
that our approach yields state-of-the-art performance for both a few and many candidate authors, in cases
where these authors wrote enough texts to be modelled effectively.
1 Introduction
The problem of authorship attribution – attributing texts to their original authors – has received considerable
attention in the last decade [6, 18]. Most of the work in this field focuses on cases where texts must be attributed
to one of a few candidate authors, e.g., [11, 4]. Recently, researchers have turned their attention to scenarios
with tens to thousands of candidate authors [7]. In this report, we study authorship attribution with few to many
candidate authors, and introduce a new method that achieves state-of-the-art performance in the latter case.
Our approach to authorship attribution consists of building models of authors and their documents using
Latent Dirichlet Allocation (LDA) [3]. We compare these models to models built from unseen texts to find
the most likely authors of these texts (Section 3.2). Our evaluation shows that our approach yields a higher
accuracy than the method recently introduced by Koppel et al. [7] in several cases where prolific authors are
considered, while requiring less runtime (Section 4).
This report is structured as follows. Related work is surveyed in Section 2. Our LDA-based approach to
authorship attribution is described in Section 3, together with the baselines we considered in our evaluation.
Section 4 presents and discusses the results of our evaluation, and Section 5 discusses our conclusions and plans
for future work.
2 Related Work
The field of authorship attribution predates modern computing. For example, in the late 19th century, Menden-
hall [10] suggested that word length can be used to distinguish works by different authors. In recent years,
increased interest in authorship attribution was fuelled by advances in machine learning, information retrieval,
and natural language processing [6, 18].
Features that are commonly used in authorship attribution range from “shallow” features, such as token and
character n-gram frequencies, to features that require deeper analysis, such as part-of-speech and rewrite rule
frequencies [18]. As in other text processing tasks, Support Vector Machines (SVMs) have delivered highly
accurate attributions, because they are designed to handle feature vectors of high dimensionality [6]. However,
since SVMs are binary classifiers, it is infeasible to use them in authorship attribution scenarios with many
candidate authors [7].
1
In this report, we focus on authorship attribution with many candidate authors. This problem was previously
addressed by Madigan et al. [9] and Luyckx and Daelemans [8], who worked on datasets with texts by 114 and
145 authors respectively. In both cases, the reported results were much poorer than those reported in the binary
case. More recently, Koppel et al. [7] considered author similarity to handle cases with thousands of candidate
authors. Their method, which we use as our baseline, is described in Section 3.1.
Our approach to authorship attribution utilises Latent Dirichlet Allocation (LDA) [3] to build models of
authors from their texts. LDA is a generative probabilistic model that is traditionally used to find topics in
textual data. The main idea behind LDA is that each document in a corpus is generated from a distribution
of topics, and each word in the document is generated according to the per-topic word distribution. Blei
et al. [3] showed that using LDA for dimensionality reduction can improve performance for supervised text
classification. We know of only one case where LDA was used in authorship attribution: Rajkumar et al. [12]
reported preliminary results on using LDA topic distributions as feature vectors for SVMs, but they did not
compare the results obtained with LDA-based SVMs to those obtained with SVMs trained on tokens directly.
Our comparison (Section 4.3) shows that both methods perform comparably.
Nonetheless, the main focus of our work is on authorship attribution with many candidate authors, where it
is infeasible to train SVMs. Our approach employs LDA for authorship attribution without SVM training (Sec-
tion 3.2), yielding state-of-the-art performance in several scenarios (Section 4).
3 Authorship Attribution Methods
This section describes the authorship attribution methods considered in this report. While all these methods can
employ various representations of documents, e.g., token frequencies or part-of-speech n-gram frequencies, we
only experimented with token frequencies.1 This is because they are simple to extract, and can achieve good
performance (Section 4). Further, the focus of this report is on comparing the performance of our methods to
that of the baseline methods. Thus, we leave experiments on other feature types for future work (Section 5).
3.1 Baselines
We consider two baseline methods, depending on whether there are a few or many candidate authors. If there
are only a few, we use Support Vector Machines (SVMs), which have been shown to deliver state-of-the-art
performance on this task [6]. If there are many, we follow Koppel et al.’s [7] approach, which we denote KOP.
The main idea behind KOP is that different pairs of authors may be distinguished by different subsets of
the feature space. Hence, KOP randomly chooses k1 subsets of size k2F (k2 < 1) from a set of F features;
for each of the k1 subsets, it calculates the cosine similarity between a test document and all the documents by
one author (each author is represented by one feature vector); it then outputs the author who had most of the
top matches. KOP also includes a threshold σ∗ to handle cases where a higher level of precision is required,
at the cost of lower recall. If the top-matching author was the top match less than σ∗ times, then KOP outputs
“unknown author”. In our experiments we set σ∗ = 0 to obtain full coverage, as this makes it easier to interpret
the results using a single measure of accuracy.
The time complexity of KOP is as follows. The time it takes to build the feature vectors from the training
documents is O(FA + W ), where W is the number of tokens in the training corpus, and A is the number
of candidate authors. The time it takes to classify a test document of N tokens is O(F + N + k1k2FA) ≈
O(N +k1k2FA). Note that if k1k2 < 1, some features will not to be considered by KOP. Thus, we can assume
that k1k2 ≥ 1 is necessary for KOP to perform well. This is verified in our experiments (Sections 4.4 and 4.5).
3.2 Authorship Attribution with LDA
In this work, we follow the extended LDA model defined by Griffiths and Steyvers [5]. Under the assumptions
of the extended model, given a corpus of M documents, a document i with N tokens is generated by choosing
a document topic distribution θi ∼ Dir(α), where Dir(α) is a T -dimensional symmetric Dirichlet distribution,
and α and T are parameters of the model. Then, each token in the document wij is generated by choosing a
topic from the document topic distribution zij ∼ Multinomial(θi), and choosing a token from the token topic
1Token frequency is the token count divided by the total number of tokens.
2
distribution wij ∼ Multinomial(φzij ), where φzij ∼ Dir(β), and β is a parameter of the model. The model can
be inferred from the data using Gibbs sampling, as outlined in [5] – an approach we follow in our experiments.
Note that the topics obtained by LDA do not have to correspond to actual, human-interpretable topics. A
more appropriate name may be “latent factors”, but we adopt the convention of calling these factors “topics”
throughout this report. The meaning of the factors depends on the type of tokens that are used as input to the
LDA inference process. For example, if stopwords are removed from the corpus, the resulting factors often,
but not necessarily, correspond to topics. However, if only stopwords are retained, as is commonly done in
authorship attribution studies, the resulting factors lose their interpretability as topics; rather, they can be seen
as stylistic markers.
We consider two ways of using LDA in authorship attribution: (1) Topic SVM, and (2) LDA+Hellinger. The
LDA part of both approaches consists of applying a frequency filter to the features in the training documents,
and then using LDA to reduce the dimensionality of each document to a topic distribution of dimensionality T .
Topic SVM. The topic distributions are used as features for a classifier that discriminates between authors.
This approach has been employed in the past for document classification, e.g., in [3], but it has been applied
to authorship attribution only in a limited study that considered just stopwords [12]. In Section 4.3, we present
the results of more thorough experiments in applying this approach to binary authorship attribution. Our results
show that the performance of this approach is comparable to that obtained without using LDA. This indicates
that we do not lose authorship-related information when employing LDA, even though the dimensionality of
the document representations is greatly reduced.
LDA+Hellinger. This method is the main contribution of our report, as it achieves state-of-the-art perfor-
mance in authorship attribution with many candidate authors. In such settings, training binary discriminative
classifiers, such as SVMs, is infeasible even in the one-vs.-all setup, as this would require training many clas-
sifiers. In addition, the number of negative examples for each classifier is much greater than the number of
positive examples, because the number of known texts for a given author is usually small compared to the
number of texts that are not by this author [7].
The main idea behind our approach is to use the Hellinger distance [1] between topic distributions of
documents to find the most likely author of a test document. We propose two representations of an author’s
documents: multi-document and single-document.
• Multi-document (LDAH-M). The LDA model is built based on all the training documents. Given a test
document, we measure the Hellinger distance between its topic distribution and the topic distributions of
the training documents. The author with the lowest mean distance for all of his/her documents is returned
as the most likely author of the test document.
• Single-document (LDAH-S). Each author’s documents are concatenated into a single document (the pro-
file document), and the LDA model is learned from the profile documents.2 Given a test document, the
Hellinger distance between the topic distributions of the test document and all the profile documents is
measured, and the author of the profile document with the shortest distance is returned.
The time it takes to learn the LDA model depends on the number of Gibbs samples S, the number of
tokens in the training corpus W , and the number of topics T . For each Gibbs sample, the algorithm iterates
through all the tokens in the corpus, and for each token it iterates through all the topics. Thus, the time
complexity of learning the model is O(SWT ). Once the model is learned, inferring the topic distribution of
a test document of length N takes O(SNT ). Therefore, the time it takes to classify a document when using
LDAH-S is O(SNT +AT ), where A is the number of authors, and O(T ) is the time complexity of calculating
the Hellinger distance between two T -dimensional distributions. The time it takes to classify a document when
using LDAH-M is O(SNT +MT ), where M is the total number of training documents, and M ≥ A, because
every candidate author has written at least one document.
Typically, it takes more time to learn the LDA models than it does to build the feature vectors in KOP (Sec-
tion 3.1). This is because for LDA, we iterate over all the tokens S times, and we found that setting S = 1000
2Concatenating all the author documents into one document has been named the profile-based approach in previous studies, in
contrast to the instance-based approach, where each document is considered separately [18].
3
is required to yield good results (Section 4.1). However, our LDAH-S approach is likely to be competitive
with KOP in terms of classification time. Recall that the time complexity of classifying a document using KOP
is O(N + k1k2FA), compared to O(SNT + AT ) for LDAH-S. As mentioned in Section 3.1, k1k2 ≥ 1 is
required for KOP to perform well, and thus k1k2FA ≥ FA. If SN ≤ A, then the time complexity of LDAH-S
is dominated by AT , and thus LDAH-S is likely to be faster since T  F (LDA is used for dimensionality
reduction, and thus the number of topics T is substantially lower than the number of features F ). If SN > A,
then the time complexity of LDAH-S is dominated by SNT . In this case, comparing the time complexities
is harder, since N can vary from a few tens to thousands of tokens, depending on the test document, while F
depends on the training dataset, and varies from tens to hundreds of thousands of unique tokens. However, in
datasets with many authors who write relatively short documents, such as blog posts, it is typically the case that
SN < 1, 000, 000 < k1k2F and T < A. This explains our experimental results, which show that the overall
runtime of LDAH-S is much shorter than that of KOP in cases where we obtained the best accuracy for KOP
with k1 > 1 iterations (Sections 4.4 and 4.5).
An advantage of LDAH-S over LDAH-M is that LDAH-S requires much less time to classify a test docu-
ment when many documents per author are available. However, this improvement in runtime may come at the
price of accuracy, as authorship markers that may be present only in a few short documents by one author may
lose their prominence if these documents are concatenated to longer documents. In our evaluation we found
that LDAH-M outperforms LDAH-S when applied to one of the datasets (Section 4.3), while LDAH-S yields
a higher accuracy when applied to the other two datasets (Sections 4.4 and 4.5). Hence, we present the results
obtained with both variants.
4 Evaluation
In this section, we describe the experimental setup and datasets used in our experiments, followed by the
evaluation of our methods. We evaluate Topic SVM for binary authorship attribution, and LDA+Hellinger on
a binary dataset, a dataset with tens of authors, and a dataset with thousands of authors. Our results show that
LDA+Hellinger yields a higher accuracy than Koppel et al.’s [7] baseline method in several cases where prolific
authors are considered, while requiring less runtime.
4.1 Experimental Setup
In all the experiments, we perform ten-fold cross validation, employing stratified sampling where possible. The
results are evaluated using classification accuracy, i.e., the percentage of test documents that were correctly
assigned to their author. Note that we use different accuracy ranges in the figures that present our results for
clarity of presentation. Statistically significant differences are reported when p < 0.05 according to a paired
two-tailed t-test.
We used the LDA implementation from LingPipe (alias-i.com/lingpipe) and the SVM implementa-
tion from Weka (www.cs.waikato.ac.nz/ml/weka). Since our focus is on testing the impact of LDA, we
used a linear SVM kernel and the default SVM settings. For the LDA parameters, we followed Griffiths and
Steyvers [5] and the recommendations in LingPipe’s documentation, and set the Dirichlet hyperparameters to
α = min(0.1, 50/T ) and β = 0.01, varying only the number of topics T . We ran the Gibbs sampling process
for S = 1000 iterations, and based the document representations on the last sample. While taking more than
one sample is generally considered good practice [19], we found that the impact of taking several samples on
accuracy is minimal, but it substantially increases the runtime. Hence, we decided to use only one sample in
our experiments.
4.2 Datasets
We considered three publicly available datasets that cover different writing styles and settings: Judgement,
IMDb62 and Blog. Table 1 shows a summary of these datasets.
The Judgement dataset contains judgements by three judges who served on the Australian High Court
from 1913 to 1975: Dixon, McTiernan and Rich (available for download from www.csse.monash.edu.au/
research/umnl/data). This dataset was created following rumours that Dixon wrote some of McTiernan’s
4
Table 1: Dataset Statistics
Judgement IMDb62 Blog
Authors 3 62 19,320
Texts 1,342 62,000 678,161
Texts per
Author
Dixon: 902
McTiernan: 253
Rich: 187
1,000
Mean: 35.10
Stddev.: 104.99
and Rich’s judgements. All the judgements by Dixon are assumed to have been written by him, while judge-
ments by McTiernan and Rich are assumed to have been written by them only if they were written in periods
when Dixon did not serve on the High Court. In this report, we used only judgements with known authorship,
and considered the Dixon/McTiernan and the Dixon/Rich binary classification cases. We removed numbers
from the texts to ensure that dates could not be used to discriminate between judges. We also removed quotes
to ensure that the classifiers take into account only the actual author’s language use. Employing this dataset in
our experiments allows us to test our methods on formal texts with a minimal amount of noise.
The IMDb62 dataset contains 62,000 movie reviews by 62 prolific users of the Internet Movie database
(IMDb, www.imdb.com, available upon request from the authors of [16]). Each user wrote 1,000 reviews.
This dataset is noisier than the Judgement dataset, since it may contain spelling and grammatical errors, and the
reviews are not as professionally edited as judgements. This dataset allows us to test our approach in a setting
where all the texts have similar themes, and the number of authors is relatively small, but is already much larger
than the number of authors considered in traditional authorship attribution settings.
The Blog dataset is the largest dataset we considered, containing 678,161 blog posts by 19,320 authors [15]
(available for download from u.cs.biu.ac.il/˜koppel). In contrast to IMDb reviews, blog posts can be
about any topic, but the large number of authors ensures that every topic is likely to interest at least some
authors. Koppel et al. [7] used a different blog dataset, consisting of 10,240 authors, in their work on author-
ship attribution with many candidate authors. Unfortunately, their dataset is not publicly available. However,
authorship attribution is more challenging on our dataset than on their dataset, because they imposed some
restrictions on their dataset, such as setting a minimal number of words per author, and truncating the training
and testing texts so that they all have the same length. The dataset we use has no such restrictions.
4.3 LDA in Binary Authorship Attribution
In this section, we present the results of our experiments with the Judgement dataset (Section 4.2), testing the
use of LDA in producing feature vectors for SVMs and the performance of our LDA+Hellinger methods (Sec-
tion 3.2).
In all the experiments, we employed a classifier ensemble to address the class imbalance problem present in
the Judgement dataset, which contains 5 times more texts by Dixon than by Rich, and over 3 times more texts
by Dixon than by McTiernan (Table 1). Dixon’s texts are randomly split into 5 or 3 subsets, depending on the
other author (Rich or McTiernan respectively), and the base classifiers are trained on each subset of Dixon’s
texts together with all the texts by the other judge. Given a text by an unknown author, the classifier outputs
are combined using majority voting. We found that the accuracies obtained with an ensemble are higher than
those obtained with a single classifier. We did not require the vote to be unanimous, even though this increases
precision, because we wanted to ensure full coverage of the test dataset. This enables us to compare different
methods using only an accuracy measure.
Experiment 1. Figure 1 shows the results of an experiment which compares the accuracy obtained using
SVMs with token frequencies as features (Token SVMs) with that obtained using LDA topic distributions as
features (Topic SVMs). We experimented with several filters on token frequency, and different numbers of LDA
topics (5, 10, 25, 50, . . ., 250). The x-axis labels describe the frequency filters: the minimum and maximum
token frequencies, and the approximate number of unique tokens left after filtering (in thousands). We present
5
 45
 50
 55
 60
 65
 70
 75
 80
 85
 90
 95
 100
0
1E-5
9.2
0
5E-5
12.2
0
1E-4
13
0
5E-4
13.8
0
1
14
1E-5
5E-5
3
1E-5
1E-4
3.8
1E-5
5E-4
4.6
1E-5
1
4.8
5E-5
1E-4
0.7
5E-5
5E-4
1.5
5E-5
1
1.7
1E-4
5E-4
0.8
1E-4
1
1
5E-4
1
0.2
A
cc
ur
ac
y
Token Frequency Filter
Token SVM
Majority Baseline
10 Topic SVM
25 Topic SVM
100 Topic SVM
200 Topic SVM
(a) Dixon/Rich
 45
 50
 55
 60
 65
 70
 75
 80
 85
 90
 95
 100
0
1E-5
14
0
5E-5
17
0
1E-4
17.7
0
5E-4
18.6
0
1
18.8
1E-5
5E-5
3
1E-5
1E-4
3.7
1E-5
5E-4
4.5
1E-5
1
4.7
5E-5
1E-4
0.7
5E-5
5E-4
1.5
5E-5
1
1.7
1E-4
5E-4
0.8
1E-4
1
1
5E-4
1
0.2
A
cc
ur
ac
y
Token Frequency Filter
Token SVM
Majority Baseline
10 Topic SVM
25 Topic SVM
100 Topic SVM
200 Topic SVM
(b) Dixon/McTiernan
Figure 1: LDA Features for SVMs in Binary Authorship Attribution (Judgement dataset)
 45
 50
 55
 60
 65
 70
 75
 80
 85
 90
 95
 100
 0  25  50  75  100  125  150  175  200
A
cc
ur
ac
y
Number of Topics
Token SVM
Majority Baseline
Topic SVM
LDAH-S
LDAH-M
(a) Dixon/Rich
 45
 50
 55
 60
 65
 70
 75
 80
 85
 90
 95
 100
 0  25  50  75  100  125  150  175  200
A
cc
ur
ac
y
Number of Topics
Token SVM
Majority Baseline
Topic SVM
LDAH-S
LDAH-M
(b) Dixon/McTiernan
Figure 2: LDA+Hellinger in Binary Authorship Attribution (Judgement dataset)
only the results obtained with 10, 25, 100 and 200 topics, as the results obtained with other topic numbers are
consistent with the presented results, and the results obtained with 225 and 250 topics are comparable to the
results obtained with 200 topics.
Our results show that setting a maximum bound on token frequency filters out important authorship markers,
regardless of whether LDA is used or not (performance drops). This shows that it is unlikely that discriminative
LDA topics correspond to actual topics, as the most frequent tokens are mostly non-topical (e.g., punctuation
and function words).
An additional conclusion is that using LDA for feature reduction yields results that are comparable to those
obtained using tokens directly. While Topic SVMs seem to perform slightly better than Token SVMs, the
differences between the best results obtained with the two approaches are not statistically significant. However,
the number of features that the SVMs must consider when topics are used is usually much smaller than when
tokens are used directly, especially when no token filters are used (i.e., when the minimum frequency is 0 and
the maximum frequency is 1). This makes it easy to apply LDA to different datasets, since the token filtering
parameters may be domain-dependent, and LDA yields good results without filtering tokens.
Experiment 2. Figure 2 shows the results of an experiment which compares the performance of the single
profile document (LDAH-S) and multiple author documents (LDAH-M) variants of our LDA+Hellinger ap-
proach to the results obtained with Token SVMs and Topic SVMs. As in the first experiment, we employ
classifier ensembles, where the base classifiers are either SVMs or LDA+Hellinger classifiers. We did not
use a token filter, since the first experiment indicates that using no filter yields comparable results to using a
filter (Figure 1). Instead, Figure 2 presents the accuracy as a function of the number of topics.
Note that we did not expect LDA+Hellinger to outperform SVMs, since LDA+Hellinger does not consider
inter-class relationships. Indeed, Figure 2 shows that this is the case (the differences between the best Topic
6
 0
 10
 20
 30
 40
 50
 60
 70
 80
 90
 100
 0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1
A
cc
ur
ac
y
k2
k1 = 1
k1 = 10
k1 = 50
k1 = 100
k1 = 200
k1 = 400
(a) KOP Parameter Tuning
 0
 10
 20
 30
 40
 50
 60
 70
 80
 90
 100
 0  50  100  150  200  250  300  350  400
A
cc
ur
ac
y
Number of Topics
KOP: k1 = 400, k2 = 0.2
LDAH-S
LDAH-M
(b) KOP versus LDA+Hellinger
Figure 3: LDA in Authorship Attribution with Tens of Authors (IMDb62 dataset)
SVM results and the best LDAH-M results are statistically significant). However, LDA+Hellinger still deliv-
ers results that are much better than the majority baseline (the differences between LDA+Hellinger and the
majority baseline are statistically significant). This leads us to hypothesise that LDA+Hellinger will perform
well in cases where it is infeasible to train SVMs due to the large number of candidate authors. We verify this
hypothesis in the following sections.
One notable result is that LDAH-S delivers high accuracy even when only a few topics are used, while
LDAH-M requires about 50 topics to outperform LDAH-S (all the differences between LDAH-S and LDAH-M
are statistically significant, except for the Dixon/McTiernan case with 50 topics). This may be because there
are only two authors, so LDAH-S builds the LDA model based only on two profile documents. Hence, even 5
topics are enough to obtain two topic distributions that are sufficiently different to discriminate the authors’ test
documents. The reason LDAH-M outperforms LDAH-S when more topics are considered may be that some
important authorship markers lose their prominence in the profile documents created by LDAH-S.
4.4 LDA in Authorship Attribution with Tens of Authors
In this section, we apply our LDA+Hellinger approaches to the IMDb62 dataset (Section 4.2), and compare
the obtained results to those obtained with Koppel et al.’s [7] method (KOP). To this effect, we first need to
establish a KOP best-performance baseline, which was done by performing parameter tuning experiments for
KOP. Figure 3(a) shows the results of these experiments, and Figure 3(b) shows the results of the comparison
of the accuracies obtained with our LDA+Hellinger methods to the best accuracy yielded by KOP.
Establishing a baseline. We ran KOP on the IMDb62 dataset. As mentioned in Section 3.1, we only varied
the number of iterations k1 and the feature subset size k2, and set the threshold σ∗ to zero to obtain full coverage.
We found that as the value of k1 increases, the value of k2 where the highest accuracy is obtained decreases.
This may be because higher values of k2 yield feature subsets that have more overlap, and thus increasing the
number of subsets k1 is less effective for high k2 values. As expected (Section 3.1), the highest accuracies were
obtained when k1k2 ≥ 1.
Even though we obtained a slight (yet statistically significant) improvement in accuracy when we in-
creased k1 from 200 to 400, we did not run the experiment for values of k1 that are greater than 400. This
is because the runtime cost of such experiments becomes prohibitive, with a per-fold runtime of 93 hours
when k1 = 400 and k2 = 0.2, and it is apparent that a large increase in k1 is required to achieve substantial
accuracy improvements. In addition, our LDAH-S approach yielded results that are much more accurate than
the best result obtained by KOP (Figure 3(b)), while requiring only 15 hours per fold to obtain the highest
accuracy.
Comparing with KOP. For this experiment, we ran our LDA+Hellinger variants with 5, 10, 25, 50, . . ., 300,
350 and 400 topics. The highest LDAH-M accuracy was obtained with 300 topics (Figure 3(b)). However,
LDAH-S yielded a much higher accuracy than LDAH-M. This may be because the large number of training
7
 0
 2
 4
 6
 8
 10
 12
 14
 16
 18
 20
 0  25  50  75  100  125  150  175  200
A
cc
u
ra
cy
Number of Topics
KOP: k1 = 10, k2 = 0.6
LDAH-S
LDAH-M
(a) 1,000 Prolific Authors
 0
 2
 4
 6
 8
 10
 12
 14
 16
 18
 20
 0  25  50  75  100  125  150  175  200
A
cc
u
ra
cy
Number of Topics
KOP: k1 = 1, k2 = 1.0
LDAH-S
LDAH-M
(b) 2,000 Prolific Authors
 0
 2
 4
 6
 8
 10
 12
 14
 16
 18
 20
 0  25  50  75  100  125  150  175  200
A
cc
u
ra
cy
Number of Topics
KOP: k1 = 1, k2 = 1.0
LDAH-S
LDAH-M
(c) 5,000 Prolific Authors
 0
 2
 4
 6
 8
 10
 12
 14
 16
 18
 20
 0  25  50  75  100  125  150  175  200
A
cc
u
ra
cy
Number of Topics
KOP: k1 = 1, k2 = 1.0
LDAH-S
LDAH-M
(d) 19,320 (all) Authors
Figure 4: LDA in Authorship Attribution with Thousands of Authors (Blog dataset)
documents per author (900 in this case) may be too noisy for LDAH-M. That is, the differences between
individual documents by each author may be too large to yield a meaningful representation of the author if they
are considered separately. Finally, LDAH-S requires only 50 topics to outperform KOP, and outperforms KOP
by about 15% for 150 topics. All the differences between the methods are statistically significant.
This experiment shows that LDAH-S models the authors in IMDb62 more accurately than KOP. The large
improvement in accuracy shows that the compact author representation employed by LDAH-S, which requires
only 150 topics to obtain the highest accuracy, has more power to discriminate between authors than KOP’s
much heavier representation, of 400 subsets with more than 30,000 features each.
4.5 LDA in Authorship Attribution with Thousands of Authors
In this section, we compare the performance of our LDA+Hellinger variants to the performance of KOP on
several subsets of the Blog dataset (Section 4.2). For this purpose, we split the dataset according to the pro-
lificness of the authors, i.e., we ordered the authors by the number of blog posts, and considered subsets that
contain all the posts by the 1000, 2000, 5000 and 19320 most prolific authors.3 Due to the large number of
posts, we could not run KOP for more than k1 = 10 iterations on the smallest subset of the dataset and 5 itera-
tions on the other subsets, as the runtime was prohibitive for more iterations. For example, 10 iterations on the
smallest subset required about 90 hours per fold (the LDA+Hellinger runtimes were substantially shorter, with
maximum runtimes of 56 hours for LDAH-S and 77 hours for LDAH-M, when 200 topics were considered).
Interestingly, running KOP for 5 iterations on the larger subsets decreased performance compared to running it
for 1 iteration. Thus, on the larger subsets, the most accurate KOP results took less time to obtain than those of
our LDA+Hellinger variants.
Figure 4 shows the results of this experiment. For each author subset, it compares the results obtained
by LDAH-S and LDAH-M to the best result obtained by KOP. All the differences between the methods are
statistically significant. For up to 2000 prolific authors (Figures 4(a), 4(b)), LDAH-S outperforms KOP by up
to 50%. For 5000 prolific users (Figure 4(c)), the methods perform comparably, and KOP outperforms LDAH-
S by a small margin. However, with all the authors (Figure 4(d)), KOP yields a higher accuracy than both
LDA+Hellinger variants. This may be because considering non-prolific authors introduces noise that results in
3These authors make up about 5%, 10%, 25% and exactly 100% of the authors, but they wrote about 50%, 65%, 80% and exactly
100% of the texts, respectively.
8
an LDA model that does not capture the differences between authors. However, it is encouraging that LDAH-S
outperforms KOP when up to 5000 prolific authors are considered.
The accuracies obtained in this section are rather low compared to those obtained in the previous sec-
tions. This is not surprising, since the authorship attribution problem is much more challenging with thousands
of candidate authors. This challenge motivated the introduction of the σ∗ threshold in KOP (Section 3.1).
Our LDA+Hellinger variants can also be extended to include a threshold: if the Hellinger distance of the
best-matching author is greater than the threshold, the LDA+Hellinger algorithm would return “unknown au-
thor”. We leave experiments with this extension to future work, as our focus in this report is on comparing
LDA+Hellinger to KOP, and we believe that this comparison is clearer when no thresholds are used.
5 Conclusions and Future Work
In this report, we introduced an approach to authorship attribution that models documents and authors using
Latent Dirichlet Allocation (LDA), and considers the distance between the LDA-based representations of the
training documents and test documents when classifying test documents. We showed that our approach yields
state-of-the-art performance in terms of classification accuracy when tens or thousands of candidate authors are
considered, and prolific authors exist in the training data. This accuracy improvement was achieved together
with a substantial reduction in runtime compared to Koppel et al.’s [7] baseline method.
While we found that our approach performs well on texts by prolific authors, there is still room for improve-
ment on authors who have not written many texts – an issue that we will address in the future. One approach
that may improve performance on such authors involves considering other types of features than tokens, such
as parts of speech and character n-grams. Since our approach is based on LDA, it can easily employ different
feature types, which makes this a straightforward extension to the work presented in this report.
In the future, we also plan to explore ways of extending LDA to model authors directly, rather than using it
as a black box. Authors were considered by Rosen-Zvi et al. [13, 14], who extended LDA to form an author-
topic model. However, this model was not used for authorship attribution, and was mostly aimed at topic
modelling of multi-authored documents, such as research papers.
Another possible research direction is to improve the scalability of our methods. Our approach, like Koppel
et al.’s [7] baseline, requires linear time in the number of possible authors to classify a single document. One
possible way of reducing the time needed for prediction is by employing a hierarchical approach that builds a
tree of classifiers based on class similarity, as done by Bickerstaffe and Zukerman [2] for the sentiment analysis
task. Under this framework, class similarity (in our case, author similarity) can be measured using LDA, while
small groups of classes can be discriminated using SVMs.
In addition to authorship attribution, we plan to consider other authorship profiling tasks, such as inferring
an author’s age and gender. We also plan to employ text-based author models in user modelling tasks, such as
rating prediction – a direction that we already started working on when we successfully used our LDA-based
approach to model users for the rating prediction task [17].
Acknowledgments
This research was supported in part by grant LP0883416 from the Australian Research Council. The authors
thank Russell Smyth for the collaboration on initial results on the judgement dataset.
References
[1] Hervé Abdi. Distance. In N. J. Salkind, editor, Encyclopedia of Measurement and Statistics, pages 280–
284. Thousand Oaks (CA): Sage, 2007.
[2] Adrian Bickerstaffe and Ingrid Zukerman. A hierarchical classifier applied to multi-way sentiment detec-
tion. In COLING 2010: Proceedings of the 23rd International Conference on Computational Linguistics,
pages 62–70, Beijing, China, 2010.
9
[3] David M. Blei, Andrew Y. Ng, and Michael I. Jordan. Latent Dirichlet allocation. Journal of Machine
Learning Research, 3(Jan):993–1022, 2003.
[4] Michael Gamon. Linguistic correlates of style: Authorship classification with deep linguistic analysis fea-
tures. In COLING 2004: Proceedings of the 20th International Conference on Computational Linguistics,
pages 611–617, Geneva, Switzerland, 2004.
[5] Thomas L. Griffiths and Mark Steyvers. Finding scientific topics. Proceedings of the National Academy
of Sciences, 101(Suppl. 1):5228–5235, 2004.
[6] Patrick Juola. Authorship attribution. Foundations and Trends in Information Retrieval, 1(3):233–334,
2006.
[7] Moshe Koppel, Jonathan Schler, and Shlomo Argamon. Authorship attribution in the wild. Language
Resources and Evaluation, 2010. Published Online First: 12 January 2010. DOI:10.1007/s10579-009-
9111-2.
[8] Kim Luyckx and Walter Daelemans. Authorship attribution and verification with many authors and limited
data. In COLING 2008: Proceedings of the 22nd International Conference on Computational Linguistics,
pages 513–520, Manchester, UK, 2008.
[9] David Madigan, Alexander Genkin, David D. Lewis, Shlomo Argamon, Dmitriy Fradkin, and Li Ye.
Author identification on the large scale. In Proceedings of the Joint Annual Meeting of the Interface and
the Classification Society of North America, St. Louis, MO, USA, 2005.
[10] Thomas C. Mendenhall. The characteristic curves of composition. Science, 9(214S):237–246, 1887.
[11] Frederick Mosteller and David L. Wallace. Inference and Disputed Authorship: The Federalist. Addison-
Wesley, 1964.
[12] Arun Rajkumar, Saradha Ravi, Venkatasubramanian Suresh, M. Narasimha Murthy, and C. E. Veni Mad-
havan. Stopwords and stylometry: A latent Dirichlet allocation approach. In Proceedings of the NIPS 2009
Workshop on Applications for Topic Models: Text and Beyond (Poster Session), Whistler, BC, Canada,
2009.
[13] Michal Rosen-Zvi, Thomas Griffiths, Mark Steyvers, and Padhraic Smyth. The author-topic model for
authors and documents. In UAI 2004: Proceedings of the 20th Conference on Uncertainty in Artificial
Intelligence, pages 487–494, Banff, AB, Canada, 2004.
[14] Michal Rosen-Zvi, Chaitanya Chemudugunta, Thomas Griffiths, Padhraic Smyth, and Mark Steyvers.
Learning author-topic models from text corpora. ACM Transactions on Information Systems, 28(1):1–38,
2010.
[15] Jonathan Schler, Moshe Koppel, Shlomo Argamon, and James W. Pennebaker. Effects of age and gender
on blogging. In Proceedings of AAAI Spring Symposium on Computational Approaches for Analyzing
Weblogs, pages 199–205, Stanford, CA, USA, 2006.
[16] Yanir Seroussi, Ingrid Zukerman, and Fabian Bohnert. Collaborative inference of sentiments from texts.
In UMAP 2010: Proceedings of the 18th International Conference on User Modeling, Adaptation and
Personalization, pages 195–206, Waikoloa, HI, USA, 2010.
[17] Yanir Seroussi, Fabian Bohnert, and Ingrid Zukerman. Personalised rating prediction for new users using
latent factor models. In Hypertext 2011: Proceedings of the 22nd ACM Conference on Hypertext and
Hypermedia, Eindhoven, The Netherlands, 2011. To appear.
[18] Efstathios Stamatatos. A survey of modern authorship attribution methods. Journal of the American
Society for Information Science and Technology, 60(3):538–556, 2009.
[19] Mark Steyvers and Tom Griffiths. Probabilistic topic models. In Thomas K. Landauer, Danielle S. McNa-
mara, Simon Dennis, and Walter Kintsch, editors, Handbook of Latent Semantic Analysis, pages 427–448.
Lawrence Erlbaum Associates, 2007.
10
