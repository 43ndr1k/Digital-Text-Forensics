Online Conversation Mining for Author Characterization
and Topic Identification
Giacomo Inches
University of Lugano
Lugano, Switzerland
giacomo.inches@usi.ch
Fabio Crestani
University of Lugano
Lugano, Switzerland
fabio.crestani@usi.ch
ABSTRACT
The increasing popularity of online-based services (Twit-
ter, Facebook, IRC, Myspace, blogs, just to mention few
of them) results in a production of a huge amount of novel
documents. These documents present properties that can
not be found in standard edited documents. In particular
the messages generated during the use of Instant Messages
Services (IM) like chat-rooms or Twitter are short, user-
generated and “noisy”. We are investigating two different
but related aspect of the content of these colloquial mes-
sages: the topic identification and the author identification
tasks. In the first case we would like to answer the question
what is the conversation about? while in the second case the
question is who are the people involved in the conversation?.
The combination of these two tasks into a unique model is
a novel and interesting research problem that is the main
topic of our research.
Categories and Subject Descriptors
H.3.1 [Information Systems]: Information Storage And
Retrieval—Content Analysis and Indexing ; I.7 [Computing
Methodologies]: Document and Text Processing
General Terms
Experimentation,Measurement
1. MOTIVATION AND OBJECTIVES
Communication is a primary need of human being and the
advent of the Internet amplified the possibilities of communi-
cation of individuals and the masses [13]. As a result, many
people every day use chat and instant messaging programs
to get in touch with friends or family, or rely on online ser-
vices such as blog or social networks to share their emotions
and thoughts with the Internet community [18].
The increasing popularity of these online-based services
(Twitter, Facebook, IRC, Myspace, blogs, just to mention
few of them) results in the production of a huge amount
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
PIKM’11, October 28, 2011, Glasgow, Scotland, UK.
Copyright 2011 ACM 978-1-4503-0953-0/11/10 ...$10.00.
of documents. It is therefore of great interest to study the
properties of these online user-generated documents. From
a commercial point of view we could identify new trends and
hot topics by mining them [11, 19], to better focus advertise-
ment or new online services. From a policing perspective,
instead, it may allow us to detect misbehavior [20, 2, 41].
It is also interesting from a research point of view to un-
derstand the linguistic or statistical properties of such doc-
uments, to improve the current models and techniques used,
for example, in Information Retrieval or Machine Learning
to link documents together or to discover new relationship
between the users generating the documents.
Therefore, we decided to investigate in this work these
kind of documents, that show the properties of being user-
generated (instead of professionally edited) and colloquial.
In other words we intend to study documents being gener-
ated during the use of Instant Messages Services (IM) like
chat-rooms (1:1 communication) or Twitter (1:N, sometimes
1:1 communication). In particular, we decided to investigate
two different but related aspects of the content of these col-
loquial messages: topic identification and author identifica-
tion. In the first case we would like to answer the question
“what is the conversation about?” while in the second case
the question is “who are the people involved in the conver-
sation?”. These two questions combined in a single model
and applied to the conversational media represent a novel
and interesting problem, which has not yet been treated
extensively in the literature. In fact, this problem cannot
be framed into a single area of research or inside a single
and straight research path, but instead involves different re-
search areas. The biggest research area could be considered
Text-Mining, where the main goal is to extract high-quality
and valuable information from a set of textual document,
making use of automatic or semi-automatic extraction tech-
niques [4]. The techniques we decided to use belong to the
area of Information Retrieval (IR) and Machine Learning
(ML). In IR, some work has already been done in the field
of topic identification, mostly using statistical methods that
lead into Language Models (LM) and Topic Models (TM).
In ML, techniques like Support Vector Machines (SVM) or
EM algorithm have already been used in the field of author
identification. A decisive factor when applying these tech-
niques is feature selection and normalization, especially for
the particular type of documents considered in here. An-
other research area involved is Natural Language Process-
ing (NLP) and its linguistic features, that could be of help
together with the standard features borrowed from the IR
document representation. We are giving a broader overview
19
of these approaches in Section 2. Despite the work already
done in each of the different areas above, the novelty of this
work consists in:
1. the development of author and topic identification tech-
niques suitable for conversational data and
2. the combination of these techniques for author and
topic identification into a single framework.
A recognition of the fact that it is indeed an interest-
ing research topic is the approval of the related ChatMiner1
SNSF2 project proposal in March 2010.
In fact, the language used in conversational messages as-
sumes different characteristics than standard text, a sort of
written speaking (a mixture between writing and speaking).
It contains, in fact, lot of terms specific to this “new”way of
writing, like abbreviations, smileys, reinforcement or shout-
ings, grammatical errors or a lexicon specifically simplified
for the conversational media. For this reason, in the first
part of the work we tried to characterize the similarities
and differences between these new documents exchanged by
users online and the standard documents used in the field
of IR. We decided for standard IR collection since the tech-
niques we are currently using and plan to use are broadly
used in this field (and are also shared with the other related
field: NLP, Text Mining, etc). We found the CAW 2.03 to be
a good dataset for our experiments because it contain a vari-
ety of new online exchanged messages such as IM messages,
Twitter messages (multicast IM), forum and blog messages.
Detailed results of this work are presented in Section 3.2,
where our end-goal is to prove the applicability of standard
IR techniques on these new collections.
One interesting finding of this first study is the similarity
between classical IM messages and Twitter messages. They
shown properties so close that they can be considered of
the same class: online conversation. While the standard
IM involves few users, in Twitter the colloquial messages
are directed to a broader audience (sometimes millions of
users), which sometimes generates answers that bring the
conversation back to a 1:1 or 1:few participants (through
the use of the @... marker). On the other hand, messages
from forum and blog exploit properties which are in-between
conversational documents and standard documents, there-
fore they can be considered of another kind, identified as
“discussion” in [14] (Section 3.2). We directly applied these
findings during our short internship (Spring 2011) at the
AT&T Labs Research in Middletown, NJ, USA. There we
worked on Twitter conversational data, developing a simple
but effective author identification algorithm on documents
relevant to a specific topic (TV program). More details on
this and other previous work are presented in Section 3.
In the next steps we would like to extend the algorithm for
author identification developed while at AT&T to make it
working on a broader set of users, adding more stylometric
features. In the future work we plan to complete and extend
1Mining Conversational Content for Topic Modeling and
Author Identification, grant nr. 200021 130208.
2“The Swiss National Science Foundation (SNSF) is the
most important Swiss agency promoting scientific research.
It supports, as mandated by the Swiss Federal govern-
ment, all disciplines, from philosophy and biology to the
nanosciences and medicine.” http://www.snf.ch/
3http://caw2.barcelonamedia.org/
the topic identification techniques used in the literature to
the Twitter conversational set [24], eventually extending it
to other more specific conversational document collections.
The goal would be to be able to manage an arbitrary number
of topic for each conversation and, later, to match these
topics with the model of the users developed before. A more
detailed discussion on future work can be found in Section
4.
2. PREVIOUS APPROACHES
In this section we are presenting the most relevant studies
in the different areas of our work. Due to the cross-domain
nature of the research, in the first phase of the literature
review and background reading we identified related works
that are: i) a very good introduction or a summary of the
state of the art in the field, and ii) strictly related to the
topics of our work. This is a continuous process and we will
eventually enlarge this overview adding more general or spe-
cific material. There are three facets that we investigated in
order to better frame our work and position it in each of the
different areas: the conversational nature of the documents,
the topic identification and the author identification tasks.
2.1 Conversational Content
Studies on short, unstructured documents like conversa-
tions are quite popular, for example, in the area of Natural
Language Processing, where the goal is to aggregate mes-
sages into a longer conversation (set of related messages)
according to different metrics: time, social structure, rele-
vancy, etc.
There are three main recent works addressing this prob-
lem. In a first work [34], the authors aim at outperforming
the previous state-of-the-art approach, introducing three es-
timators, each modelled with a normal distribution with an
exponential decay. The estimators are: the author context
(set of messages written by the same author) and conversa-
tional context (name mention along the conversation) plus
the temporal context (proximity of a message to the relevant
others). These indicators are used to expand the documents,
where a document is each message exchanged between users.
The expanded messages are then grouped according to a
simple clustering algorithm. They perform better than [7],
which at the time was the state-of-the-art approach. This
later work has also to be mentioned for the creation of a
dataset of human annotated conversations crawled from an
IRC channel. This is an interesting resource that could be
used in future work to test our algorithms of topic or author
identification and also for evaluation. The work of Shen et
al. [27] has to be mentioned because they use a simple Lan-
guage Model (bag of words) with standard TF-IDF distance
measure to identify the different thread, assuming one topic
per thread only. The work of Wang et al. [38] instead,
although being relevant for the thread reconstruction prob-
lem, is less interesting for our work as it deals with fora,
which we proved being of the class “discussion” instead of
conversational messages exchange or simply “chat” (Section
3.2).
In the second interesting work [33] the author wants to
automatically detect the receiver of a message, taking inspi-
ration from previous work on chat disentangling [34, 27],
IR and Topic Detection and Tracking. They introduced
measures of semantic relevance of message and conversa-
tion using a Language Model (LM) with smoothing and a
20
word-word transition probability on a different corpus. This
approach derives from the field of Automatic Translation
and is interesting but requires a different dataset for the
computation of the word-word probabilities, which is a clear
disadvantage (the choice and maintenance of another collec-
tion is not easy).
In the last relevant work [24] the authors analyze Twit-
ter as conversational media and success in reconstructing
the conversation between users. The model employed in
this work is obtained as combination of a conversational
model, based on Hidden Markov Model (HMM) trained with
an Expectation-Maximization (EM) algorithm, and a Topic
Model (TM) based on Latent Discriminant Analysis and
Dirichlet priors (LDA). This is an interesting aspect, be-
cause it highlights the usage of Topic Model on a Twitter
collection. Other interesting aspects of this work are the
usage of Twitter as conversational media, because this rein-
forces the results of our first experiments (Section 3.2), and
the availability of the dataset online, since we could use it
to test our own Topic Models. However, it can not be used
as a testbed for the Author Identification because all the
users mentioned were artificially labelled with a progressive
number.
2.2 Topic modeling
Topic Modeling (TM), represent an evolution of Language
Models (LM), where each document is represented as as a
bag-of-word technique, and assumes a more complicated sta-
tistical process for generating documents [32, 39, 40]. In
LM, in fact, each separate word in a document is identified
and counted so that the document is rendered into a list of
words with a frequency count assigned to each word. This
loses the original word order but enables the documents to
be subjected to simplified statistical analysis. Bag-of-word
indexing is necessary to make the problem computationally
efficient since full statistical analysis of natural language text
at the scale required by modern search engines is beyond the
capabilities of current hardware platforms. In TM-based ap-
proaches to IR, instead, the documents is no longer treated
as distributions over terms, but rather as distributions over
a latent topic space which has much lower dimensionality
than the original term space. In fact, the document repre-
sentation is done using a low dimensionality topic frequency
vector rather than a very high dimensional term frequency
vector. Each document is then seen as a combination of top-
ics, where each topic is itself defined by a distribution over
words. These distributions can be learned directly from the
corpus of documents using standard statistical techniques
such as Dirichlet allocation or Gibbs sampling [10]. TM in
their classical definition were first described in [6] and in
IR have shown relatively limited success to date for adhoc
(standard Web) search [35, 36]. Their widespread adoption
has been obstructed by scaling issues for the estimation tech-
niques used, although much recent work has been invested
in developing algorithms for scaling up TM techniques to
large document collections [35, 1, 29].
Although an attempt at detecting the topic change have
already been applied to short and conversational document
[30, 31, 22, 23] some issues are still open: one of the limits
of the TM is the a-priori definition of the number of topic,
which is an unrealistic assumption in dynamic text genera-
tion environment as the chat or the conversational messages
exchange. Hierarchical Topic Models (HTM) may be a valid
extension to TM to overcome this problem [5, 37].
Furthermore, from a theoretical point of view, TM seems
to be the perfect complement technique when using lexical
feature (word count) in the author identification task: in
TM one looks for the less common term in the documents
to capture the most particular argument being discussed,
while in author identification one want to characterize the
author, therefore looks for the usage of the most common
terms. An attempt toward this direction has already be
done [25, 21], but the application to short document and
conversational messages is yet to be done. Moreover, the
same issues regarding the undetermined topics number has
to be solved.
2.3 Author characterization
The problem of authorship attribution is a classical prob-
lem since ancient time. In IR is a relatively new problem and
recent studies [15, 28, 17] tend to divide the problem into
three categories: authorship identification, similarity detec-
tion, and authorship characterization. Author identification
is the task of finding or validating the author of a document.
Similarity detection aims to find the variations in the writing
style of an author or to find the resemblances between the
writings of different authors, mostly for the purpose of de-
tecting plagiarism. Authorship characterization is the task
of assigning the writings of an author into a set of categories
according to the author’s sociolinguistic attributes. Some
attributes previously investigated in literature are gender,
language background, and education level.
In our work we are trying to solve the first of the prob-
lems presented above, the author identification task. It is an
interesting an actual problem: workshops on author identifi-
cation have been yearly organized4 as part of main interna-
tional conferences (SIGIR, ECAI, CLEF) since 2007. These
workshops, however, are centered on standard edited text
(books, magazine, newspaper) while our research is focused
on conversational content, which is a novel topic. In spite
of focus mismatch, the methods employed by the partici-
pant to the workshops could be of interest, therefore we are
following these events.
An early work on authorship identification of online mes-
sages is presented in [42]. Here the authors observe that the
short nature of online conversational documents makes the
features selection challenging but, on the other hand, this
difficulties could be mitigated by the specificity of the style of
“chatting”of each author. This creates a sort of“writeprints”
for each author. The authors decide which lexical features
(character based: special character, counts, ...), syntactic
features (punctuation, word counts, ... ), structural features
(how an author structure a text) and content-specific fea-
tures (a sort of topic detection) to use. Although identified,
they neglect the use of Part Of Speech (POS) as a syntactic
features, to maintain a cross-language compatibility.
A good classification of features for the authorship at-
tribution can be also find in [28], where another class (se-
mantic) is described. Once selected the features they com-
pared three algorithms (decision tree, neural network, Sup-
port Vector Machines - SVM) to classify the conversational
text, discovering SVM as the best classifier (using different
measure of accuracy). Similar results can be found in an-
other work [19], where the authors introduce other features
4http://pan.webis.de/
21
like emoticons, shoutings5, vocabulary richness to identify
both the sender of a message as well of a receiver. They
tested different classification algorithms (k -Nearest Neigh-
bor -kNN-, Näıve Bayesian -NB-, SVM) and concluded that
“the performance of different classifiers vary throughout the
experiments. The experimental results on the prediction of
user-specific attributes show that NB and SVM perform best
in all settings although the results show that no single clas-
sifier can be the ‘best performer’ ”. These are interesting
conclusions and a good starting point for our research.
We noticed that the aforementioned methods are exclu-
sively Machine Learning (ML) methods. In our research we
will also apply statistical methods like the discriminant anal-
ysis or cross-entropy with other features like the entropy and
measures of compression that other studies [16, 15] identified
as good for short documents.
To conclude, another good-practice for short documents
is the concatenation of documents of the same author into
a single text, to be used as unitary element [28] for the
analysis. This approach make sense since short documents
are already short and need an “enrichment” of information
(Section 3.2) rather than an impoverishment.
3. CHARACTERIZING ONLINE MESSAGES
EXCHANGE
As anticipated in the previous sections, different phases
are involved in our work. We illustrated in Figure 1 a block
diagram that outlines the different steps performed and let
us glimpse the past (vertical stripes), current (dotted pat-
tern) and future work to be done (squared pattern). The
four main blocks represented the four main phases of the
work (sources selection, features selection, analysis, synthe-
sis) and each phase is then sub-divided in different compo-
nents that we are presenting here below.
3.1 Sources
In the first step of our work, the source selection part, e
analyzed different documents’ collections. The goal here is
the characterization of the conversational documents prop-
erties and their similarities/differences with respect to the
standard collections used in Information Retrieval, like for
example the TREC6 ad-hoc collection. In fact, it is impor-
tant to asses the nature of similarity between the conver-
sational documents and the one in standard collections, to
be able to infer the applicability of standard measures (of
distance or similarity: BM25, Kullback-Leibler divergence,
cosine with TF-IDF weighting, etc.) and techniques (Prob-
abilistic, Language or Topic Models) or to develop new ones
which fit better the new datasets. For this purpose we stud-
ied the selected properties of four user-generated short doc-
ument datasets and three more traditional ones, taken from
the TREC ad-hoc collections.
We chose as representative of the first class four datasets
containing different types of documents. These documents
are extracted from Kongregate (Internet Relay Chat of on-
line gamers), Twitter (short messages), Myspace (forum dis-
cussions) and Slashdot (comments on news-posts). They
were first presented at the Workshop for Content Analysis
in Web 2.0 [14] and are divided between training and test-
5A word written all in capital letter or that contains the
same characters repeated more than twice.
6Text REtrieval Conference, http://trec.nist.gov/
ing data7. In our analysis we took into consideration only
the training set for each collection, which was sufficient for
our purposes, conserving the test set for future studies. As
collections representative of IR standard and edited docu-
ments we employed three datasets of similar edited content:
news articles from the Associated Press (AP, all years), the
Financial Times Limited (FT, all years) and the Wall Street
Journal (WSJ, all years). These datasets form a represen-
tative subset of the standard TREC Ad-hoc collection8 and,
although they are similar in the type of content, they cover
different topics: AP and WSJ report news in general, while
FT focuses on markets and finance. We noticed that these
collections show similar topicality to the Myspace and Slash-
dot datasets: Myspace covers themes of campus life, news
& politics and movies, while Slashdot is limited to discus-
sions of politics. The fact that the themes are similar to
news articles is important in order to make all the statisti-
cal comparison between the collections meaningful. As for
the topicality of the Twitter and Kongregate datasets, due
to their conversational and more unpredictable nature, we
could not state precisely what their topicality is. This is in
fact, still an open research problem [23, 11, 31].
At a first very quick look, the documents contained in
each of the CAW datasets share the following properties: i)
short, as also shown in Figure 2, ii) user generated, meaning
that someone not necessarily a professional writer created
them within an online application and iii) “noisy”, because
of the presence of spelling errors, domain specific terms or
abbreviations in the documents.
In the next section we are investigating these and the other
collections, to find discriminative features which allow us to
distinguish between them and help in the tasks of topic and
author identification.
3.2 Documents properties and features
The first study we performed on the different collections
was conducted using different measures, to compare quan-
titatively their characteristics. Beside the document length,
which is a straightforward measure (Figure 2) given the na-
ture of collections, we used other quantities: percentage
of stopwords (most frequent words), percentage of out-of-
dictionary words (being recognized as error in the text by
a standard spell checker), percentage of singletons (words
that appear just once). These quantities give a measure of
similarity of the CAW 2.0 collections with respect to the
TREC Ad-hoc collections. Inspired from the work of Ser-
rano et al. [26], we computed similar distributions on our
collection: the Frequency Spectrum (Zipf’s law) and the
Vocabulary Growth (Heap’s law). These two distributions
are, in fact, established distributions followed by any of the
TREC Ad-hoc dataset and represent, respectively, how com-
mon and rare words are distributed among the collections
and how the vocabulary is growing with respect to the num-
ber of documents in each collections. The results of this
work were quite encouraging, since they highlighted a sim-
ilar statistical behavior to standard collections in the Fre-
quency Spectrum for short documents, although they differ
in the Vocabulary Growth. This can be explained with out-
of-vocabulary terms and singletons, very frequent in short
7Datasets and details available at http://caw2.
barcelonamedia.org/
8Datasets and details available at http://trec.nist.gov/
data/test_coll.html
22
                   
feature
extraction
                             
synthesis
author
topic
topic
+
author
sources
sentiment
twitter
IRC
analysis
                   
feature 
selection
text 
summarization
blog
                   
cosimilarity
burstiness
POS
emoticons
abbreviations
"shoutings"
conversation 
mining
social TV
...
...
evaluation
Figure 1: Block diagram of the framework. Past work (vertical stripes), current work (dotted pattern),
future work (squared pattern), possible extensions/applications (no pattern).
4 12 14 40 100 200 500400 450 460 10'000... ...10
Twitter
SMSKongregate
Myspace
Slashdot
Tripadvisor Trec Blog
Trec AdHoc
FT
WSJ
AP
CAW 2.0
Books
Police Archive
Figure 2: Qualitative representation of the average document length (in number of words) for different
collection. On the left (in red) the CAW 2.0 dataset ; on the right (in blue) the standard TREC Ad-hoc
collection.
documents. These results were presented in a poster during
the 32nd European Conference on Information Retrieval in
March 2010 [12].
Therefore, we continued working on the other distribu-
tions indicated in [26], investigating also documents cosim-
ilarity and burstiness. When using the cosimilarity we em-
ployed the cosine similarity between each pair of documents
in the collection. We observed that user-generated docu-
ments appear less frequently in classes with lower value of
similarity (0.01-0.09), as they become shorter. To the con-
trary, in classes with higher similarity value (0.9-1.00) they
show up more frequently, contrasting the behavior of the
standard documents, too. The latter, in fact, drop down
when we consider the last similarity class. This means that
shorter documents seem to be more similar across them-
selves than the longer ones. This can be explained with the
length of the documents themselves: short documents con-
tain less words (less “information”). Therefore, given two
short documents, there is an high probability that they ap-
pear to be similar even if they are unrelated, just because
they are short. Regarding burstiness, we focused on common
and rare terms for each collection and computed their distri-
bution in each document. We compared them against the ex-
pected number of documents if the words in the vocabulary
were uniformly distributed and identified the “bursty” be-
havior when the two distribution were not overlapping. We
observed a coherent behavior with the results of the previ-
ous experiment: terms belonging to the short user-generated
documents resulted to be less bursty than the ones in the
standard TREC Ad-hoc collections. These results suggested
that in the next step of the work we should eventually pre-
process the documents and expand their informative content
through the use of consolidated techniques, like document
expansion or conversation segmentation.
We continued working on another set of features: the
Part-Of-Speech (POS) tags distribution in the different data-
sets, that could be used as indicator for the writing style of
one author (as also indicated in Section 2.3). We did not
use a specific POS parser and tokenizer tuned on each of
the collections but a generic one, because we wanted to test
the ability of such an approach to the specific collections.
The implementation of a dedicated parser (similar to [8])
could be part of future work or an extension of the work.
From the analysis of this features, we noticed some inter-
collection variations, between the user-generated datasets
and the traditional edited datasets as well as an intra-collection
variation inside the user-generated datasets, between chat-
style and discussion-style documents. This latest differences
were first identified in [41]: “in discussion-style environ-
ments, there are various threads, usually with multiple posts
in each thread. [...] Usually, the discussion within one thread
pertains to one predefined topic. In chat-style communities,
the conversations are more casual, and each post usually
consists of only several words. Most of the time, there is
very little information about the existence (or absence) of a
main topic in such conversations.”. Using different features,
we could reach the same conclusion.
We concluded this part of work with the study of features
complementary to the POS: the distribution of emoticons
and “shoutings”9 among the different collections. These fea-
9Letters in a word token repeated more then twice (e.g.
23
tures, in fact, can be very discriminative for identifying user-
generated content [3], in particular conversational data [11].
The behavior of the distributions is similar and reflect the
nature of the collections: user-generated collections (Kon-
gregate, Twitter, Myspace, Slashdot) contain a high number
of colloquial and informal token, like emoticons and shout-
ings, that are used to improve the expressiveness of the
communication, while in standard and professional edited
document (WSJ, AP, FT ) the communication remains on
a formal and neutral level (having these collection almost
zero counts for emoticons and shoutings and at least 1 order
of magnitude less than the others). As for the POS fea-
tures, beside the inter-class differences highlighted above,
it is also important to notice that we can highlight some
intra-class differences among the user-generated documents:
more chat and colloquial documents (Kongregate and Twit-
ter) contains more emoticons and shoutings occurrences (in
the order of 1 or 2 magnitude levels) than the documents
that are more discussion-style. This study was found inter-
esting and presented with more details to the 9th Interna-
tional Conference on Flexible Query Answering Systems -
FQAS 2011 (still in press).
3.3 Author characterization
In a recent internship at AT&T Labs Research in Middle-
town, NJ, USA, under the supervision of Dr.Andrea Basso,
we could apply some of the features identified in the previous
section to a real problem. Since we already demonstrated
that Twitter can be considered a conversational media (due
to its similarity with online chat conversations), we analyzed
a set of Twitter messages relative to TV program content
and their informative value. The goal of this work was to
identify the most relevant and informative Twitter user or
set of users for a given TV program. We chose five different
popular TV shows (“Dancing with the Stars”, “Modern Fam-
ily”,“Greys Anatomy”,“Desperate Housewives”,“The Mental-
ist”, “Dr. House”) and two news channel (“NBC Nightly
News”,“ABC 20/20”) and all the relevant Twitter messages
for each of the show, including their official Twitter account.
To find the most relevant and informative Twitter user, we
first filtered all the users involved by being verified by Twit-
ter. On all the message of each user we then computed the
POS features (as in the previous section) plus the other sty-
lometric features: abbreviations, emoticons and shoutings.
Moreover, we introduced another stylometric measure, the
entropy, to detect the most informative Twitter user for each
TV program. We introduced also other Twitter specific fea-
tures: number of retweets, number of followers, number of
messages.
All the features except the entropy and the number of
messages (too trivial and biased toward the official users)
where used with the EM clustering algorithm to identify
group of users. We were expecting a more marked division
between classes of relevant and non-relevant users but the
division was not very clear, with some overlap. In other
words, there were some indicators to characterize group of
relevant user (noun, verb, interjection, emphasis, lowercase
kind) but with these only we could not be sure to detect the
relevant group. On the other hand, with the same features
it was possible to identify outliers. The measure of entropy
was instead used to detect the most informative user. In this
mmmm,mmmmaybe). We did not include in this counting the
token containing internet addresses (www,WWW).
case, the most informative is the official Twitter user of the
TV program. To compute the entropy all messages of each
author were indexed to build a per-author corpus; the mea-
sure of entropy per author was obtained from this corpus
and, through it, it was possible to identify the most infor-
mative and official Twitter user of each TV program. This
was expected, since the entropy captures the most diverse
content and the official users writing is the most diverse, not
just relevant, for the particular TV program. Therefore it
would be interesting to see the behavior of the entropy when
applied to a more heterogeneous collection. This is part of
the future work. The analysis presented above is still work
in progress and was only recently published in the 3rd In-
ternational Workshop on Search and Mining User-generated
Contents - SMUC 2011 (still in press).
4. FUTURE WORK
4.1 Author Identification - concluding
In the immediate future we want to continue the work on
author identification started during our internship, combin-
ing the entropy feature with other features. An interesting
one is the measure of compression [9] combined with all the
already computed stylometric features (POS, abbreviations,
shoutings, emoticons, word-case, ... ). We plan to extract
different class of users (not only official or trusted) from our
Twitter dataset and try to classify them according to their
features. The goal here is to identify the properties of each
class of users, considering relevant and not relevant ones.
The other goal is: starting from the features of each group
of users, detect the author of each set of messages within the
set of users (on the Twitter dataset and on the IM dataset).
To achieve these goals we will divide our dataset into testing
and training set, hiding the user names in the training set.
A part of the training set will be used as validation set, if
necessary. For this experiments we will apply the best algo-
rithms identified during the literature review (as in Section
2.3): Näıve Bayes and SVM. To complete the investigation
and to have a better overview on the behavior of this algo-
rithms, we will also try the Discriminant Analysis and the
Cross-Entropy approaches.
4.2 Topic identification
In the near future we will then analyze the IM [7] and
Twitter collections [24] (presented in Section 2.1) to the
purpose of topics extraction from the conversations. These
datasets contain conversations that are already segmented
into thread and can be easily processed. The comparison of
the results obtained on the two different collection is novel
in the literature and our expectation is to observe a similar
behavior between the two. We plan to use the well estab-
lished Latent Dirichlet Analysis (LDA) [6] technique and
eventually adopt its hierarchical version (hLDA), if a better
subdivision in finer topics [5, 37] is needed.
Here there are some issues we will have to face. The first
one is represented by the unstructured nature of the docu-
ments, that may require a preprocessing in terms of error or
spelling correction. There are two techniques we may use to
mitigate this problem: a N-gram based correction or tech-
niques from automatic document translation (on one side
the misspelled words, on the other the correct words). The
second issue is represented by the finite number of topics
required by LDA. In its classical formulation, in fact, the
24
number of topics to be assigned to each term in the vocabu-
lary is a-priori defined. A possible solution of this problem
would be a preliminary unsupervised clustering of the terms,
just to detect the number of possible topics within a conver-
sation. Only in a second step we would apply the (h)LDA
algorithm to associate terms, then documents, to topics.
If we would like to use the same Twitter dataset we are
using for the author identification part, we will first need to
preprocess it to extract the conversations. This can be done
using the established techniques from the chat disentangle-
ments [34, 33, 24]. We will leave this experiment to the end:
after analyzing the results of the (h)LDA algorithm on the
datasets where the conversations have already been identi-
fied, we will decide if it worth applying it to this untangled
collection.
A latter issue in here resides in the evaluation of the re-
sults, because there are not previous topic annotations on
the datasets. We plan to do an user evaluation of the results
obtained, eventually through an external service like Ama-
zon Mechanical Turk. The labelled data can be then later
used also in the next (and last) phase of our work.
4.3 Author-Topic characterization
The integration of the two different models into a single
framework is the last step before the thesis writing. During
the integration we have to combine the techniques of author
characterization with the ones for topic identification. This
will be the main contribution of our work and it is strictly re-
lated to the features identified and the algorithm developed
in the two previous parts (authorship identification and topic
identification). The task will be for sure easier if we will have
one statistical models for each component: one for the au-
thor identification part, one for the topic identification part.
In the case we have different type of models, we should de-
sign a two-step algorithm to contemporarily detect authors
and topics of the conversation. We are expecting interesting
results from this part. One outcome would be the discover-
ing of new groups of users, by topic similarity. These groups
will be further analyzed to understand the nature of this
similarity, eventually identifying suspect behaviors. This is
one of the possible outcomes of the project; other interest-
ing applications could be in the field of advertisement or
recommender systems.
5. REFERENCES
[1] A. Asuncion, M. Welling, P. Smyth, and Y. W. Teh.
On smoothing and inference for topic models. In
Proceedings of the Twenty-Fifth Conference on
Uncertainty in Artificial Intelligence, UAI ’09, pages
27–34, Arlington, Virginia, United States, 2009. AUAI
Press.
[2] R. Bache, F. Crestani, D. Canter, and D. Youngs.
Mining police digital archives to link criminal styles
with offender characteristics. In Proceedings of the
10th international conference on Asian digital
libraries: looking back 10 years and forging new
frontiers, ICADL’07, pages 493–494, Berlin,
Heidelberg, 2007. Springer-Verlag.
[3] K. Balog, M. Bron, J. He, K. Hofmann, E. J. Meij,
M. de Rijke, E. Tsagkias, and W. Weerkamp. The
university of amsterdam at trec 2009: Blog, web,
entity, and relevance feedback. In TREC 2009
Working Notes. NIST, November 2009.
[4] M. Berry. Survey of Text Mining II. Springer, Berlin,
2007.
[5] D. Blei, T. L. Griffiths, M. I. Jordan, and J. B.
Tenenbaum. Hierarchical topic models and the nested
chinese restaurant process. In S. Thrun, L. Saul, and
B. Schölkopf, editors, Advances in Neural Information
Processing Systems 16. MIT Press, Cambridge, MA,
2004.
[6] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent
dirichlet allocation. The Journal of Machine Learning
Research, 3:993–1022, March 2003.
[7] M. Elsner and E. Charniak. You talking to me? a
corpus and algorithm for conversation
disentanglement. In Proceedings of ACL-08: HLT,
pages 834–842, Columbus, Ohio, June 2008.
Association for Computational Linguistics.
[8] K. Gimpel, N. Schneider, B. O’Connor, D. Das,
D. Mills, J. Eisenstein, M. Heilman, D. Yogatama,
J. Flanigan, and N. A. Smith. Part-of-speech tagging
for twitter: Annotation, features, and experiments. In
ACL (Short Papers), pages 42–47. The Association for
Computer Linguistics, 2011.
[9] A. Grewal, T. Allison, S. Dimitrov, and D. Radev.
Multi-document summarization using off the shelf
compression software. In Proceedings of the
HLT-NAACL 03 on Text summarization workshop -
Volume 5, HLT-NAACL-DUC ’03, pages 17–24,
Stroudsburg, PA, USA, 2003. Association for
Computational Linguistics.
[10] T. L. Griffiths and M. Steyvers. Finding scientific
topics. Proceedings of the National Academy of
Sciences, 101(Suppl. 1):5228–5235, April 2004.
[11] S. C. H. Haichao Dong and Y. He. Structural analysis
of chat messages for topic detection. Online
Information Review, 30(5):496 – 516, 2006.
[12] G. Inches, M. Carman, and F. Crestani. Statistics of
online user-generated short documents. In C. Gurrin,
Y. He, G. Kazai, U. Kruschwitz, S. Little, T. Roelleke,
S. Rüger, and K. van Rijsbergen, editors, Advances in
Information Retrieval, volume 5993 of Lecture Notes
in Computer Science, pages 649–652. Springer Berlin
Heidelberg, 2010.
[13] A. Java, X. Song, T. Finin, and B. Tseng. Why we
twitter: understanding microblogging usage and
communities. In WebKDD/SNA-KDD ’07:
Proceedings of the 9th WebKDD and 1st SNA-KDD
2007 workshop on Web mining and social network
analysis, pages 56–65, New York, NY, USA, 2007.
ACM.
[14] J.Codina, A.Kaltenbrunner, J.Grivolla, R. E.Banchs,
and R.Baeza-Yates. Content analysis in web 2.0. In
18th International World Wide Web Conference,
Barcelona, Spain, April 2009.
[15] P. Juola. Authorship attribution. Foundations and
Trends in Information Retrieval, 1(3):233–334, 2006.
[16] P. Juola and H. Baayen. A controlled-corpus
experiment in authorship identification by
cross-entropy. In Literary and Linguistic Computing,
volume 20(Suppl 1), pages 59–67. Kluwer Academic
Publishers, 2003.
[17] M. Koppel, J. Schler, and S. Argamon. Computational
25
methods in authorship attribution. J. Am. Soc. Inf.
Sci. Technol., 60:9–26, January 2009.
[18] B. Krishnamurthy, P. Gill, and M. Arlitt. A few chirps
about twitter. In WOSP ’08: Proceedings of the first
workshop on Online social networks, pages 19–24, New
York, NY, USA, 2008. ACM.
[19] T. Kucukyilmaz, B. B. Cambazoglu, C. Aykanat, and
F. Can. Chat mining: Predicting user and message
attributes in computer-mediated communication. Inf.
Process. Manage., 44(4):1448–1466, 2008.
[20] E. W. Medina. Military textual analysis and chat
research. In Proceedings of the 2008 IEEE
International Conference on Semantic Computing,
pages 569–572, Washington, DC, USA, 2008. IEEE
Computer Society.
[21] N. Naveed, S. Sizov, and S. Staab. Att: Analyzing
temporal dynamics of topics and authors in social
media. In ACM WebSci’11, pages 1–7, June 2011.
WebSci Conference 2011.
[22] J. O’Neill and D. Martin. Text chat in action. In
GROUP ’03: Proceedings of the 2003 international
ACM SIGGROUP conference on Supporting group
work, pages 40–49, New York, NY, USA, 2003. ACM.
[23] D. Ramage, S. Dumais, and D. Liebling.
Characterizing microblogs with topic models. In
ICWSM, pages –1–1, Washington, DC, USA, May
2010. AAAI Press.
[24] A. Ritter, C. Cherry, and B. Dolan. Unsupervised
modeling of twitter conversations. In Human Language
Technologies: The 2010 Annual Conference of the
North American Chapter of the Association for
Computational Linguistics, pages 172–180, Los
Angeles, California, June 2010. Association for
Computational Linguistics.
[25] M. Rosen-Zvi, T. Griffiths, M. Steyvers, and P. Smyth.
The author-topic model for authors and documents. In
Proceedings of the 20th conference on Uncertainty in
artificial intelligence, UAI ’04, pages 487–494,
Arlington, Virginia, United States, 2004. AUAI Press.
[26] M. Serrano, A. Flammini, and F. Menczer. Modeling
statistical properties of written text. PLoS ONE,
4(4):e5372–, 04 2009.
[27] D. Shen, Q. Yang, J.-T. Sun, and Z. Chen. Thread
detection in dynamic text message streams. In SIGIR
’06: Proceedings of the 29th annual international
ACM SIGIR conference on Research and development
in information retrieval, pages 35–42, New York, NY,
USA, 2006. ACM.
[28] E. Stamatatos. A survey of modern authorship
attribution methods. J. Am. Soc. Inf. Sci. Technol.,
60(3):538–556, 2009.
[29] V. H. Thuc and P. Srinivasan. Topic models and a
revisit of text-related applications. In PIKM ’08:
Proceeding of the 2nd PhD workshop on Information
and knowledge management, pages 25–32, New York,
NY, USA, 2008. ACM.
[30] A. P. S. Trevor K. M. Stone. Detection of topic change
in irc chat logs. Website. http://www.trevorstone.
org/school/ircsegmentation.pdf.
[31] V. H. Tuulos and H. Tirri. Combining topic models
and social networks for chat data mining. In WI ’04:
Proceedings of the 2004 IEEE/WIC/ACM
International Conference on Web Intelligence, pages
206–213, Washington, DC, USA, 2004. IEEE
Computer Society.
[32] H. M. Wallach. Structured Topic Models for Language.
PhD thesis, University of Cambridge, 2008.
[33] J. Wang, F. Wang, Z. Yan, and B. Huang. Message
receiver determination in multiple simultaneous im
conversations. IEEE Intelligent Systems,
99(PrePrints), 2010.
[34] L. Wang and D. W. Oard. Context-based message
expansion for disentanglement of interleaved text
conversations. In NAACL ’09: Proceedings of Human
Language Technologies: The 2009 Annual Conference
of the North American Chapter of the Association for
Computational Linguistics, pages 200–208,
Morristown, NJ, USA, 2009. Association for
Computational Linguistics.
[35] Y. Wang, H. Bai, M. Stanton, W.-Y. Chen, and E. Y.
Chang. Plda: Parallel latent dirichlet allocation for
large-scale applications. In Proceedings of the 5th
International Conference on Algorithmic Aspects in
Information and Management, AAIM ’09, pages
301–314, Berlin, Heidelberg, 2009. Springer-Verlag.
[36] X. Wei and W. B. Croft. Lda-based document models
for ad-hoc retrieval. In SIGIR ’06: Proceedings of the
29th annual international ACM SIGIR conference on
Research and development in information retrieval,
pages 178–185, New York, NY, USA, 2006. ACM.
[37] S. Williamson, C. Wang, K. A. Heller, and D. M. Blei.
The ibp compound dirichlet process and its
application to focused topic modeling. In J. Fürnkranz
and T. Joachims, editors, ICML, pages 1151–1158.
Omnipress, 2010.
[38] W. W. C. Y.-C. Wang, M. Joshi and C. Rose.
Recovering implicit thread structure in newsgroup
style conversations. In A. for the Advancement of
Artificial Intelligence, editor, ICWSM, Seattle,
Washington, March 2008. AAAI Press.
[39] X. Yi and J. Allan. Evaluating topic models for
information retrieval. In CIKM ’08: Proceeding of the
17th ACM conference on Information and knowledge
management, pages 1431–1432, New York, NY, USA,
2008. ACM.
[40] X. Yi and J. Allan. A comparative study of utilizing
topic models for information retrieval. In ECIR ’09:
Proceedings of the 31th European Conference on IR
Research on Advances in Information Retrieval, pages
29–41, Berlin, Heidelberg, 2009. Springer-Verlag.
[41] D. Yin, Z. Xue, L. Hong, B. D. Davison,
A. Kontostathis, and L. Edwards. Detection of
harassment on web 2.0. In CAW 2.0 ’09: Proceedings
of the 1st Content Analysis in Web 2.0 Workshop,
Madrid, Spain, 2009.
[42] R. Zheng, J. Li, H. Chen, and Z. Huang. A framework
for authorship identification of online messages:
Writing-style features and classification techniques.
Journal of the American Society for Information
Science and Technology, 57:378–393, February 2006.
26
