Modeling, Analyzing, Identifying,
and Synthesizing Expressive Popular
Music Performances
5
Rafael Ramirez, Esteban Maestre, and Alfonso Perez
Abstract
Professional musicians manipulate sound properties such as pitch, timing,
amplitude, and timbre in order to add expression to their performances.
However, there is little quantitative information about how and in which
contexts this manipulation occurs. In this chapter, we describe an approach to
quantitatively model and analyze expression in popular music monophonic
performances, as well as identifying interpreters from their playing styles.
The approach consists of (1) applying sound analysis techniques based on
spectral models to real audio performances for extracting both inter-note and
intra-note expressive features, and (2) based on these features, training compu-
tational models characterizing different aspects of expressive performance using
machine learning techniques. The obtained models are applied to the analysis
and synthesis of expressive performances as well as to automatic performer
identification. We present results, which indicate that the features extracted
contain sufficient information, and the explored machine learning methods are
capable of learning patterns that characterize expressive music performance.
R. Ramirez (*)
DTIC, Universitat Pompeu Fabra, Tànger, 122-140, 08018 Barcelona, Spain
e-mail: rafael.ramirez@upf.edu
E. Maestre
CCRMA, Stanford University, 660 Lomita Dr, 94350 Stanford, CA, USA
e-mail: esteban@ccrma.stanford.edu
A. Perez
CIRMMT and IDMIL, Schulich School of Music, McGill University,
555 Sherbrooke St. West Montreal, Quebec H3A 1E3, Canada
e-mail: alfonso.perezcarrillo@mail.mcgill.ca
A. Kirke and E.R. Miranda (eds.), Guide to Computing for Expressive Music Performance,
DOI 10.1007/978-1-4471-4123-5_5, # Springer-Verlag London 2013
123
5.1 Introduction
Music performance plays an important role in our culture nowadays. Most people
are able to distinguish between different types of expression in performances.
However, there is little quantitative information about how and in which contexts
expressive performance occurs. Expressive music performance research (for an
overview, see [1, 2]) investigates the manipulation of sound properties such as
pitch, timing, and amplitude in an attempt to understand and recreate expression in
performances.
5.1.1 Overview of Previous Work
One of the first attempts to provide a computer system with musical expressive-
ness is that of Johnson [3]. Johnson developed a rule-based expert system to
determine expressive tempo and articulation for Bach’s fugues from The Well-
Tempered Clavier. The rules were obtained from two expert performers. A long-
term effort in expressive performance modeling is the work of the KTH group
[4–6]. Their Director Musices system incorporates rules for tempo, dynamic,
and articulation transformations. The rules are obtained from both theoretical
musical knowledge, and experimentally by using an analysis-by-synthesis manual
approach. The rules are divided into differentiation rules which enhance the
differences between scale tones, grouping rules, which specify what tones belong
together, and ensemble rules which synchronize the voices in an ensemble.
Canazza et al. [7, 8] implemented a system to analyze the relationship between
the musician’s expressive intentions and his/her performance. The analysis reveals
two expressive dimensions, one related to loudness (dynamics) and another one
related to timing (rubato). Dannenberg and Derenyi [9] investigated the trumpet
articulation transformations using manually generated rules. They developed a
trumpet synthesizer, which combines a physical model with an expressive per-
formance model. The performance model generates control information for the
physical model using a set of rules manually extracted from the analysis of a
collection of performance recordings.
More recently, there have been several approaches to computationally modeling
expressive performance by applying machine learning techniques [10]. Lopez de
Mantaras and Arcos [11] report on SaxEx, a performance system capable of
generating expressive solo saxophone performances in jazz. Their system is based
on case-based reasoning, a type of analogical reasoning where problems are solved
by reusing the solutions of similar, previously solved problems. In order to generate
expressive solo performances, the case-based reasoning system retrieves from a
memory containing expressive interpretations those notes that are similar to the
input inexpressive notes. The case memory contains information about metrical
strength, note duration, and so on and uses this information to retrieve the appropri-
ate notes. One limitation of their system is that it is incapable of explaining the
predictions it makes.
Ramirez et al. [12, 13] have explored and compared diverse machine learning
methods for obtaining expressive music performance models for jazz saxophone
124 R. Ramirez et al.
that are capable of both generating expressive performances and explaining the
expressive transformations they produce. They propose an expressive performance
system based on inductive logic programming which learns a set of first-order logic
rules that capture expressive transformation both at an inter-note level (e.g., note
duration, loudness) and at an intra-note level (e.g., note attack, sustain). Based on
the theory generated by the set of rules, they implemented a melody synthesis
component, which generates expressive monophonic output (MIDI or audio) from
inexpressive melody MIDI descriptions.
With the exception of thework byLopez deMantaras et al. andRamirez et al.,most
of the research in expressive performance using machine learning techni-
ques has focused on classical piano music and thus on global timing and dynamics
(i.e., loudness) transformations. This contrasts most other musical instruments,
which have additional hard-to-measure ways of varying the performance of each
note (e.g., vibrato, glissando, and the ability to vary the dynamics throughout the note).
Widmer [14, 15] reported on the task of discovering general rules of expressive
classical piano performance from real performance data via inductive machine
learning. The performance data used for the study are MIDI recordings of 13
piano sonatas by W.A. Mozart performed by a skilled pianist. In addition to these
data, the music score was also coded. The resulting substantial data consists of
information about the nominal note onsets, duration, metrical information, and
annotations. When trained on the data, an inductive rule-learning algorithm discov-
ered a small set of quite simple classification rules that predict a large number of the
note-level choices of the pianist.
Tobudic and Widmer [16] describe a relational instance-based approach to the
problem of learning to apply expressive tempo and dynamic variations to a piece
of classical music at different levels of the phrase hierarchy. The different phrases
of a piece and the relations among them are represented in first-order logic.
The description of the musical scores through predicates (e.g., contains(ph1,ph2))
provides the background knowledge. The training examples are encoded by
another predicate whose arguments encode information about the way the phrases
were played by the musician. Their learning algorithm recognizes similar phrases
from the training set and applies their expressive patterns to a new piece.
Other inductive approaches to rule learning in music and musical analysis include
[17, 18]. In [17], Dovey analyzes piano performances of Rachmaninoff pieces using
inductive logic programming and extracts rules underlying them. In [18], Van Baelen
extended Dovey’s work and attempted to discover regularities that could be used
to generate MIDI information derived from the musical analysis of the piece.
The use of expressive performance models (either automatically induced
or manually generated) for identifying musicians has received little attention in
the past. Saunders et al. [19] apply string kernels to the problem of recognizing
famous pianists from their playing style. The characteristics of performers playing
the same piece are obtained from changes in beat-level tempo and beat-level
loudness. From such characteristics, general performance alphabets can be derived,
and pianists’ performances can then be represented as strings. They apply both
kernel partial least squares and support vector machines to these data.
5 Modeling, Analyzing, Identifying, and Synthesizing Expressive. . . 125
Stamatatos and Widmer [20] address the problem of identifying the most
likely music performer, given a set of performances of the same piece by a number
of skilled candidate pianists. They propose a set of very simple features for repre-
senting stylistic characteristics of a music performer that relate to a kind of
“average” performance. A database of piano performances of 22 pianists playing
two pieces by Frédéric Chopin is used. They propose an ensemble of simple
classifiers derived by both subsampling the training set and subsampling the input
features. Experiments show that the proposed features are able to quantify the
differences between music performers.
Ramirez et al. [21] develop a machine learning approach to jazz saxophone
performer identification by analyzing the pitch, timing, amplitude, and timbre of
individual notes, as well as the timing and amplitude of individual intra-note events.
They identify performers by establishing a performer-dependent mapping from
inter-note features (essentially a “score” whether or not the score physically exists)
to a repertoire of inflections characterized by intra-note features. Thus, their
approach to interpreter identification strongly relies on the performances’ timbre
content, which makes sense in performances in jazz saxophone. In this chapter,
we consider violin performances. The work described here combines and extends
previous work on expressive performance modeling reported in [13, 22].
Molina et al. [23] proposed an approach for identifying violinists in monop-
honic audio recordings. They considered a database of sonatas and partitas for
solo violin by J.S. Bach and identified performers by capturing their general
expressive footprint based on a characterization of the way melodic patterns are
played as a set of frequency distributions. Performances were transcribed focusing
on the melodic contour, and melodic segments were tagged according to Narmour’s
implication/realization model [24].
5.1.2 Chapter Overview
In this chapter, we describe a general approach to computationally model, analyze,
synthesize, and identify interpreters in expressive performances. The approach
applies machine learning algorithms to automatically discover regularities and
patterns from a corpus of real performance data. The aim is to allow the computa-
tional algorithms to learn autonomously the relevant musical contexts and their
corresponding expressive patterns. The approach attempts to answer a variety of
musicological questions, such as:
• How consistent are the performances of individuals?
• How different are the performance styles of different interpreters?
• Is it possible to automatically identify interpreters from their playing styles?
• How close can a computer-generated performance approximate the expression
introduced by human professional musicians in their performances?
The study of these questions is important in order to:
1. Increase our understanding of the strategies employed by talented professional
musicians to produce expression and emotions in their performances. We believe
126 R. Ramirez et al.
that by examining our trained models, it is possible to gain musicological insight
into the complex task of performing a musical piece expressively.
2. Enable music information retrieval applications such as performer search and
classification. Given the explosion of online music and the rapidly expanding
digital music collections, the development of music information retrieval
applications such as performer search and classification is an important applica-
tion of the work reported here.
3. Implement next generation postproduction systems able to mimic human expres-
sive performance. Implementing this kind of postproduction systems would
bring great advantages to composers, allowing them to listen to their music as it
is likely to be performed and thus guiding them during the composition process. It
is also easy to imagine expressive automatic accompaniment systems for soloists
or even tutoring systems that could evaluate and critique student performances.
The rest of the chapter is organized as follows: Section 5.2 describes how
audio recordings are processed in order to extract information about both the
internal structure of notes (i.e., intra-note information) and the musical context in
which they appear (i.e., inter-note information). Section 5.3 describes our approach
to expressive performance modeling and style-based automatic interpreter identifi-
cation, and finally, Sect. 5.4 presents some conclusions and indicates some areas of
future research.
5.2 Audio Analysis
In this section, we outline how we extract a symbolic description of a performed
melody for monophonic recordings (for a comparison of the method reported here
and other methods, see [25]). We use this melodic representation to provide
description of the performances and apply machine learning techniques to this
representation. Our interest is to obtain, for each performed note, a set of symbolic
features from the audio recording.
5.2.1 Energy and Fundamental Frequency Computation
First of all, we perform a spectral analysis of a portion of sound, called an analysis
frame, whose size is a parameter of the algorithm. This spectral analysis consists of
multiplying the audio frame with an appropriate analysis window and performing a
discrete Fourier transform (DFT) to obtain its spectrum. In this case, we use a frame
width of 46 ms, an overlap factor of 50%, and a Keiser-Bessel 25-dB window.
Then, we compute a set of low-level descriptors for each spectrum: energy and
an estimation of the fundamental frequency. From these low-level descriptors,
we perform a note segmentation procedure. Once the note boundaries are known,
the note descriptors are computed from the low-level values. The main low-level
descriptors used to characterize note-level expressive performance are instanta-
neous energy and fundamental frequency.
5 Modeling, Analyzing, Identifying, and Synthesizing Expressive. . . 127
The energy descriptor is computed on the spectral domain using the values of
the amplitude spectrum at each analysis frame. In addition, energy is computed in
different frequency bands as defined in [26].
For the estimation of the instantaneous fundamental frequency, we use a harmonic
matching model derived from the two-way mismatch procedure (TWM) [27].
For each fundamental frequency candidate, mismatches between the harmonics
generated and the measured partials frequencies are averaged over a fixed subset of
the available partials. A weighting scheme is used to make the procedure robust to the
presence of noise or absence of certain partials in the spectral data. The solution
presented in [27] employs two mismatch error calculations. The first one is based on
the frequency difference between each partial in the measured sequence and its
nearest neighbor in the predicted sequence. The second is based on the mismatch
between each harmonic in the predicted sequence and its nearest partial neighbor in
the measured sequence. This two-way mismatch helps to avoid octave errors by
applying a penalty for partials that are present in the measured data but are not
predicted and also for partials whose presence is predicted but which do not actually
appear in the measured sequence. The TWMmismatch procedure has also the benefit
that the effect of any spurious components or partial missing from the measurement
can be counteracted by the presence of uncorrupted partials in the same frame.
First, we perform a spectral analysis of all the windowed frames, as explained
above. Secondly, the prominent spectral peaks of the spectrum are detected from
the spectrum magnitude. These spectral peaks of the spectrum are defined as the
local maxima of the spectrum which magnitude is greater than a threshold.
The spectral peaks are compared to a harmonic series and a two-way mismatch
(TWM) error is computed for each fundamental frequency candidates. The candi-
date with the minimum error is chosen to be the fundamental frequency estimate.
Note segmentation is performed using a set of frame descriptors, which are
energy computation in different frequency bands and fundamental frequency.
Energy onsets are first detected following a band-wise algorithm that uses some
psychoacoustical knowledge [26]. In a second step, fundamental frequency
transitions are also detected.
5.2.2 Note Descriptors
We compute note descriptors using the note boundaries and the low-level
descriptors values. The low-level descriptors associated to a note segment are
computed by averaging the frame values within this note segment. Pitch histograms
have been used to compute the pitch note and the fundamental frequency that
represents each note segment, as found in [28]. This is done to avoid taking into
account mistaken frames in the fundamental frequency mean computation. First,
frequency values are converted into cents by the following formula:
c ¼ 1200  log2
f
fref
 
(5.1)
128 R. Ramirez et al.
where fref ¼ 8.176 (fref is a the reference frequency of the C0). Then, we define
histograms with bins of 100 cents and hop size of 5 cents and we compute the
maximum of the histogram to identify the note pitch. Finally, we compute the fre-
quency mean for all the points that belong to the histogram. The MIDI pitch is
computed by quantization of this fundamental frequencymean over the frames within
the note limits. Figure 5.1 shows an overview of the melodic description process.
5.2.3 Note Transitions
For characterizing note detachment, we also extract some features of the note-to-note
transitions describing how two notes are detached. For two consecutive notes, we
consider the transition segment starting at the first note’s release and finishing at
the attack of the following one. Both the energy envelope and the fundamental
frequency contour (schematically represented by EXX and f0 in Fig. 5.2) during
transitions are studied in order to extract descriptors related to articulation. We
measure the energy envelope minimum position tc (see also Fig. 5.2) with respect to
the transition duration as in (5.2). This descriptor has proven useful when
reconstructing amplitude envelopes during transitions.
We compute a legato descriptor as described next. The relevance of this descriptor
was assessed in [29]. From Fig. 5.2, we join start and end points on the energy
envelope contour by means of a line Lt representing the smoothest case of detach-
ment. Then, we compute both the area A2 below the energy envelope and the area
A1 between the energy envelope and the joining line Lt and define our legato
descriptor as shown in (5.3):
ETPOSmin ¼
tc
tend  tinit (5.2)
LEG ¼ A1
A1 þ A2 ¼
R tend
tinit
ðLtðtÞ  EðtÞÞdtR tend
tinit
LtðtÞdt
: (5.3)
Fig. 5.1 Schematic view of the melodic description process. Note onsets are extracted based on
the study of energy and fundamental frequency
5 Modeling, Analyzing, Identifying, and Synthesizing Expressive. . . 129
5.2.4 Musical Analysis
After having computed the note descriptors as above, and as a first step towards
providing an abstract structure for the recordings under study, we decided to
use Narmour’s theory of perception and cognition of melodies [24, 30] to analyze
the performances.
The implication/realizationmodel proposed by Narmour is a theory of perception
and cognition of melodies. The theory states that a melodic musical line continu-
ously causes listeners to generate expectations of how the melody should continue.
According to Narmour, any two consecutively perceived notes constitute a
melodic interval, and if this interval is not conceived as complete, it is an implica-
tive interval, i.e., an interval that implies a subsequent interval with certain
characteristics. That is to say, some notes are more likely than others to follow
the implicative interval. Two main principles recognized by Narmour concern
registral direction and intervallic difference. The principle of registral direction
states that small intervals imply an interval in the same registral direction (a small
upward interval implies another upward interval and analogously for downward
intervals), and large intervals imply a change in registral direction (a large upward
interval implies a downward interval and analogously for downward intervals).
The principle of intervallic difference states that a small (five semitones or less)
Fig. 5.2 Schematic view of
the transition segment
characterization
130 R. Ramirez et al.
interval implies a similarly sized interval (plus or minus 2 semitones), and a large
interval (seven semitones or more) implies a smaller interval. Based on these two
principles, melodic patterns or groups can be identified that either satisfy or violate
the implication as predicted by the principles. Such patterns are called structures
and are labeled to denote characteristics in terms of registral direction and inter-
vallic difference. Figure 5.3 shows prototypical Narmour structures. A note in a
melody often belongs to more than one structure. Thus, a description of a melody
as a sequence of Narmour structures consists of a list of overlapping structures.
We parse each melody in the training data in order to automatically generate an
implication/realization analysis of the pieces. Figure 5.4 shows the analysis for a
melody fragment.
5.2.5 Gesture Acquisition
We have acquired bowing motion data by means of two 3D motion tracker sensors,
one mounted on the violin and the other on the bow. We are able to estimate with
great precision and accuracy the position of the strings, the bridge, and the bow.
With the collected data we compute, among others, the following bowing perfor-
mance parameters: bow distance to the bridge, bow transversal position, velocity
and acceleration, bow force, and string being played.
In order to detect bow direction, it is not enough to use detected zero crossings
of bow speed for extracting bow direction because of the possible performed bowed
triplets. Instead, we compute bow speed histograms for each of the note segments
after alignment and get the sign of the histogrammaximumas the indicator for the bow
direction. We also detect ornamentations (mordents). Detection of mordents is carried
out after note segmentation and can be summarized as follows. For each segmented
note, pitch and aperiodicity function is calculated. Pitch curve segments with high
aperiodicity or very short segments are not considered for the detection. The remaining
intra-note pitch curve is then quantized to semitones, and notes with changes of
one tone or semitone inside the note are considered to be part of an ornament.
Fig. 5.3 Prototypical Narmour structures
Fig. 5.4 Narmour analysis of a melody fragment
5 Modeling, Analyzing, Identifying, and Synthesizing Expressive. . . 131
5.3 Expressive Performance Modeling
and Interpreter Identification
In this section, we describe our approach to music performance modeling and
interpreter identification. In particular, we describe how the learnt computational
expressive models can be used for understanding and generating expressive
performances as well as for identifying interpreters based on their playing style.
5.3.1 Training Data
In this work, we focus on Celtic jigs. Celtic jigs are a form of lively folk dance,
as well as the accompanying dance tune, originating in England in the sixteenth
century and today most associated with Irish dance music. Celtic jigs are fast tunes
but slower than reels that usually consist of eighth notes in a ternary time signature
(6/8 time) with strong accents at each beat. The training data used in this research
are monophonic recordings of nine Celtic jigs, each performed by three profes-
sional violinists (the pieces were recorded in the Audiovisual Institute’s recording
studio at the Pompeu Fabra University, expressly for the experiment). Apart from
the tempo (they played following a metronome), the musicians were not given any
particular instructions on how to perform the pieces.
5.3.2 Note Descriptors
We characterize each performed note by a set of inter-note features representing
both properties of the note itself and aspects of the musical context in which the
note appears (see Fig. 5.5). Information about the note includes note pitch (Pitch),
note duration (dur), and note metrical strength (MetrStr) while information about
its melodic context includes the relative pitch and duration of the neighboring notes
(PrevPitch, PrevDur, NextPitch, NextDur), i.e., previous and following notes,
as well as the Narmour structures in which the note appears in first, second, and
third position (Nar1, Nar2, Nar3). The note’s Narmour structures are computed by
performing the musical analysis described in the previous section. The metrical
strength of a note is computed based on the note’s position within the bar in which it
appears. In a 4/4 piece, it is defined as very strong, strong, medium, weak, and very
weak if the note appears in beat 1, 3, 2, or 4, offbeat or anywhere else, respectively.
Thus, each performed note N is contextually characterized by the tuple:
N ¼ (Pitch, Dur, MetrStr, PrevPitch, PrevDur, NextPitch, NextDur, Nar1,
Nar2, Nar3)
We also extract other note features, as described in Sect. 5.3. These features are
the amount of legato with the previous note, the amount of legato with the following
note, and mean energy.
132 R. Ramirez et al.
5.3.3 Evaluation
The results reported in the following sections have been obtained by performing the
standard tenfold cross validation in which 10% of the data is held out in turn as test
data while the remaining 90% is used as training data. When performing the tenfold
cross validation, we leave out the same number of data per class. In order to avoid
optimistic estimates of the classifier performance, we explicitly remove from the
training set all repetitions of the hold-out data. This is motivated by the fact that
musicians are likely to perform a melody fragment and its repetition in a similar
way. Thus, the applied tenfold cross validation procedure, in addition to holding out
a test example from the training set, also removes repetitions of the example.
5.3.4 Expressive Performance Modeling
5.3.4.1 Learning Task
We are concerned with expressive transformations on note duration and note
energy, as well as bow direction and melody alteration. In the case of note duration
and note energy, we have approached the problem both as a classification task and
a regression task. By approaching the problem as a classification task, our aim is
to obtain interpretable rules that can serve as a mean to understand expressive
performance while the regression task allows to obtain model capable of generating
expressive performances of new pieces by considering the score as the sole input.
In the case of bow direction and melody alteration, we have approached the
problem as a classification task only. The reason is that here we are interested in
predicting a class, either up or down for bow direction, and ornamented or not
ornamented for melody alteration. The prediction of these two resulting classifiers
is used for synthesizing an expressive performance of a particular piece. The exact
ornamentation to be performed is determined by applying a nearest neighbor
algorithm on an ornamentation database extracted from the performances.
Fig. 5.5 Inter-note
descriptors
5 Modeling, Analyzing, Identifying, and Synthesizing Expressive. . . 133
The performance classes that interest us are lengthen, shorten, and same for
duration transformation; soft, loud, and same for energy variation; up and down
for bow direction; and ornamentation and none for melody alteration. A note is
considered to belong to class lengthen if its performed duration is 20% longer
(or more) than its nominal duration, e.g., its duration according to the score. Class
shorten is defined analogously. A note is considered to be in class loud if it is played
louder than its predecessor and louder than the average level of the piece. Class soft
is defined analogously. We decided to set these boundaries after experimenting with
different ratios. The main idea was to guarantee that a note classified, for instance,
as lengthen was purposely lengthened by the performer and not the result of a
performance inexactitude. A note is considered to belong to class ornamentation if
a note or group of notes not specified in the score has been introduced in the
performance to embellish the note in the melody and to class none otherwise.
As a regression task we are interested in predicting duration, onset, and energy
deviations expressed as a ratio of the values specified in the score (for energy which is
not specified in the score we take the score value as the average of the energy of all
the notes in the piece). For instance, for duration a predicted value of 1.14 represents a
prediction of 14% lengthening of the note with respect to the score. In the case of
onset prediction, this same value represents a 14% of a quarter note onset delay, and
in the case of energy, it indicates that the note should be played a 14% louder than the
average energy in the piece. For generating expressive performances, we do not treat
melody alterations (e.g., ornamentations) as a regression problem; instead, we simply
classify whether a note should be ornamented, in which case we adapt an ornamented
note in the training data to the new context of the note being generated.
5.3.4.2 Learning Algorithm
We have applied Tilde’s top-down decision tree induction algorithm [31]. Tilde
can be considered as a first-order logic extension of the C4.5 decision tree algorithm
[32]: instead of testing attribute values at the nodes of the tree, Tilde tests logical
predicates. This provides the advantages of both propositional decision trees (i.e.,
efficiency and pruning techniques) and the use of first-order logic (i.e., increased
expressiveness). The increased expressiveness of first-order logic may provide a
more elegant specification of the musical context of a note, and in some cases it
provides a more accurate predictive model [11]. Tilde can also be used to build
multivariate regression trees, i.e., trees able to predict vectors. In our case, the
predicted vectors are the duration and energy transformations.
We have applied the learning algorithm with the following target predicates:
duration/2, energy/2, bowdir/2, and alteration/2 (where/n at the end of the pre-
dicate name denotes the number of arguments the predicate takes). Each target
predicate corresponds to a particular type of transformation: duration/2 refers to
duration transformation, energy/2 to energy transformation, bowdir/2 to bow direc-
tion, and alteration/2 refers to melody alteration (see Fig. 5.6). For learning a
definition of each target predicate, we use the complete training data specialized
for the particular type of transformation, e.g., for duration/3, we used the complete
data set information on duration transformation (i.e., the performed duration
134 R. Ramirez et al.
transformation for each note in the data set). The arguments are the musical piece,
the note in the piece, and performed transformation.
We use (background) predicates to specify both note musical context and
background information. The predicates we consider include context/7, narmour/
2, succ/2, prevdur/2, nextdur/2, prevint/2, nextint/2, metricstr/2, and member/3.
Figure 5.7 illustrates the main background predicates. Predicate context/8 specifies
Fig.5.6 Target predicates
Fig.5.7 Main background predicates: context/8 and narmour/2
5 Modeling, Analyzing, Identifying, and Synthesizing Expressive. . . 135
the local context of a note. Its arguments are note identifier, note’s nominal duration,
duration of previous and following notes, extension of the pitch intervals between the
note and the previous and following notes, and tempo at which the note is played.
Predicate narmour/2 specifies the Narmour groups to which the note belongs.
Its arguments are the note identifier and a list of Narmour groups. Predicate succ
(X,Y) means Y is the successor of X, prevdur(X,Y), nextdur(X,Y), prevint(X,Y),
nextint(X,Y), mean Y is the previous duration ratio, the next duration ratio, the
previous interval, and the next interval of X, respectively. Predicate metricstr(X,Y)
means that Y is the metrical strength of note X, and predicate member(X,L) means X
is a member of list L. Note that succ(X,Y) also means X is the predecessor of Y.
The succ(X,Y) predicate allows the specification of arbitrary size note context by
chaining a number of successive notes together: succ(X1,X2), succ(X2,X3), . . ., succ
(Xn1,Xn), where Xi (1 < i < n) is the note of interest.
5.4 Results
We trained a separate model for each of the three violinists. The induced classifica-
tion rules are of different types. Some focus on features of the note itself while
others focus on the Narmour analysis. Rules referring to the local context of a note,
i.e., rules classifying a note solely in terms of the timing, pitch, and metrical
strength of the note and its neighbors, as well as compound rules that refer to
both the local context and the Narmour structure were discovered. Some of the
induced rules seem to capture performance principles while others seem to be of
less musical interest. The following are examples of induced rules:
[Rule 1]
duration(A, lengthen):-
succ(A, B),
context(A,0.5,_,_,_,_,_),
nextdur(A,1),
metricstr(A,1).
“Lengthen a note if it is on a very strong metrical position and both the note and the
following note duration are eighth notes”
[Rule 2]
duration(A, shorten):-
succ(B, A),
context(A,0.5,_,_,_,_,_),
prevdur(A,1),
metricstr(A,4).
“Shorten a note if it is on a weak metrical position and both the note and the
previous note duration are eighth notes”
Rules 1 and 2 are complementary and are consistent with previous research,
which noted that traditional Irish fiddle music contains swing timing deviations at
the eighth-note level [33]. See later for further evidence provided by the regression
model.
136 R. Ramirez et al.
[Rule 3]
duration(A, shorten):-
metricstr(A,4),
narmour(A,[nargroup(reverse id,1),
nargroup(reverse id,2),nargroup(ir,3)]).
“Shorten a note if it is on a weak metrical position and it appears in reverse ID,
reverse ID, and IR Narmour groups in 1st, 2nd, and 3rd position, respectively.”
For the classification tasks and first interpreter, we obtained a correctly classi-
fied instances percentage of 90%, 87%, 86%, and 92% for duration, dynamics,
ornamentation, and bow direction prediction, respectively. For the second and
third interpreters, we obtained accuracies of 86%, 85%, 88%, and 90% and 76%,
81%, 79%, and 90%, respectively. As mentioned before, these numbers were
obtained by performing tenfold cross validation on the training data. For the
regression task and first interpreter, we obtained correlation coefficients of 0.88
and 0.83 for the duration and energy transformations, respectively. For the second
and third interpreters, we obtained 0.91 and 0.85 and 0.82 and 0.74, respectively.
We have compared the model’s transformation predictions and the actual
transformations performed by the musician on pieces not considered in the
training stage. Figure 5.8 contrasts the note duration deviations predicted by the
model and the deviations performed by the first violinist for a particular piece.
Similar results were obtained for the other two violinists. As illustrated in Fig. 5.8,
the induced models seem to capture accurately the expressive transformations the
musician introduces in the performances.
Expressive deviations at the note level occur in many different musical styles
and depend on the metrical position in the musical structure. For instance, some
jazz pieces are characterized by a particular pattern of timing deviations: the swing
where consecutive eighth notes are performed as long-short patterns [34]. It has
been noted that traditional Irish fiddle music also shows comparable timing
deviations at the eighth-note level [33]. It turned out that the learned duration
model captures this kind of timing deviation. Figure 5.9 compares, at the eighth-
note level, the average predicted duration deviation ratio of notes in strong metrical
position and their successor notes in weak metrical positions.
Synthesis.We fed a violin synthesizer with the prediction results of the models. We
make use of a sample-based spectral concatenative synthesizer that uses
an annotated database of samples from real recordings and segmented using the
techniques described before. The database consists of several musical phrases
combining different dynamics, durations, pitch, and articulations. Note annotations
include bowing information (note bow direction and whether a note is slurred
or tied). Sample selection is based on a weighted Euclidean distance measure
with strict matching for bow direction in a similar way as presented in [35].
After the sample selection stage, pitch shift and time stretch transformations are
applied in order to match note characteristics in the model’s output. In order to
obtain a particular ornamentation instance in addition to a prediction of class
ornamented, we apply a nearest neighbor algorithm (with Euclidean distance on
the note properties) in the ornamentation database, i.e., the set of ornamented note
5 Modeling, Analyzing, Identifying, and Synthesizing Expressive. . . 137
examples. Once a score note has been classified as ornamented, we compute its
distance to every note which was performed ornamented and select the one with
minimum distance. We then retrieve the corresponding ornamentation. We com-
pute the distance as follows:
ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi
Pitchdif 2 þ DurationDif 2 þMetrStrengthDif 2
p
Fig. 5.8 Note deviation ratio for a tune with 89 notes. Comparison between performed and
predicted duration ratios
Fig. 5.9 Average predicted
note deviation ratio for notes
in strong metrical position
and following notes in weak
metrical positions
138 R. Ramirez et al.
where PitchDif is the normalized difference between the pitch of the score note and
the pitch of a note in the predicted class. Similarly, DurationDif is the normalized
difference between the duration of the score note and the duration of a note in the
predicted class, and MetrStrengthDif is the normalized difference between the two
notes’ metrical strength. The selected ornamentation is then transposed to fit the
new note melodic context.
5.4.1 Identifying Performers by Their Playing Style
5.4.1.1 Learning Task
We are interested in obtaining a classifier F of the following form:
FðMelodyFragmentðN1; . . . ;NkÞÞ ! Performers
where MelodyFragment(N1,. . .,Nk) is the set of melody fragments (composed of
notes N1,. . .,Nk) and Performers is the set of possible performers to be identified.
For each performer Pi to be identified, we learn an expressive performance
model Mi predicting the performer timing, energy, and note transitions expres-
sive characteristics (characterized by the note duration, energy mean, and legato
descriptors described in Sect. 5.3):
Mi Noteð Þ ! PerfDur; PerfEner; PerfLeftTrans; PerfRightTransð Þ
where Note is a note in the score represented by its context features, i.e., Note is
represented by the tuple (Pitch, Dur, PrevPitch, PrevDur, NextPitch, NextDur,
Nar1, Nar2, Nar3) as described before, and the vector (PerfDur,PerfEner,
PerfLeftTrans,PerfRightTrans) contains the model’s prediction for how Performer
Pi would play the note in terms of note duration (PerfDur), energy (PerfEner), and
transitions (PerfLeftTrans,PerfRightTrans).
5.4.1.2 Learning Algorithm
For training each ModelMi, we have explored several machine learning techniques,
including support vector machines [36] with 2nd order polynomial kernel, artificial
neural networks [37] fully connected with one input neuron for each attribute, and
one hidden layer with six neurons, k-nearest neighbor (k¼2), model trees, and
ensemble methods (bagging, boosting, voting, and stacking).
All the recorded pieces are segmented into fragments representing musical
phrases. Given a fragment denoted by a list of notes [N1,. . .,Nm] and a set of
possible performers denoted by a list of performers [P1,. . .,Pn], classifier F
identifies the performer as follows:
F([N1,. . .,Nm], [P1,. . .,Pn])
for each performer Pi
Scorei ¼ 0
for each note Nk
5 Modeling, Analyzing, Identifying, and Synthesizing Expressive. . . 139
FNk ¼ features(Nk)
Mi(FNk) ¼ (PDk,PEk,PLTk,PRTk)
for each performer Pi
ScoreNKi ¼ dist([D(NK),E(Nk),LT(Nk),RT(Nk)],[PDk,PEk,PLTk,PRTk)
Scorei ¼ Scorei + ScoreNKi
return Pi (i in {1,. . .,m}) with minimum score,
where
dist X1;X2;X3;X4½ ; Y1; Y2; Y3; Y4;½ ð Þ ¼
ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi
ðX1  Y1Þ2 þ ðX2  Y2Þ2 þ ðX3  Y3Þ2 2= þ ðX4  Y4Þ2 2=
q
For each note in the melody fragment, the classifier F computes the set of the
note contextual features. Once this is done, for each note Nk and for each performer
Pi, performance model Mi predicts the expected duration, energy, and transitions
for Nk. This prediction is based on the note’s contextual features. The score Scorei
for each performer i is updated by taking into account the Euclidean distance
between the note’s actual duration, energy, and transitions and the predicted values.
Finally, the performer with the lower score (i.e., the smaller accumulated distance)
is returned. Clearly, the expressive models Mi play a central role in the output of
classifier F. Note that in computing this distance, we have balanced the weight of
the duration, energy, and transition differences by dividing by 2 the left and right
transition differences. The reason for this is that we consider the left and right
transitions as one prediction.
5.5 Results
There were a total of 806 notes available for each performer. For evaluating the
classification function F, we segmented each of the performed pieces in phases and
obtain a total of 92 phrases for each performer. The classification accuracy of the
baseline classifier (one which chooses randomly one of the three performers) is
33% (measured in correctly classified instances percentage). The average accuracy
and the accuracy obtained for the most successful trained classifier was 76.1% and
81.2%, respectively. The correctly classified instances percentage for each learning
method is presented in Table 5.1. The results seem to indicate that it is indeed
feasible to train successful classifiers to identify performers from their playing style
using the considered features.
5.6 Discussion
Regarding the modeling of expressive deviations, the obtained results indicate that
the features extracted contain sufficient information to detect performance patterns
of particular interpreters and that the machine learning techniques considered are
suitable for learning these patterns. One interesting question is how different the
140 R. Ramirez et al.
performance styles of different interpreters are. We have indirectly investigated this
question by training classifiers for automatically identifying the different
interpreters based on their playing styles. On this line, we found that the difference
between the classification results obtained and the accuracy of a baseline classifier
indicates that the features extracted contain sufficient information to identify the
studied set of performers and that the methods explored are capable of learning
performance patterns that distinguish these performers. It is worth noting that every
learning algorithm investigated (decision trees, SVM, ANN, k-NN, and the reported
ensemble methods) produced considerably better than random classification
accuracies. This supports our statement about the feasibility of training successful
classifiers for the case study reported.
We selected two types of musical segment lengths: 1-note segments and
short-phrase segments. Evaluation using 1-note segments results in poor classifi-
cation accuracies, while short-phrase segments evaluation results in accuracies
well above the accuracy of a baseline classifier. The poor results of the 1-note
evaluation may indicate that although the extracted features are relevant, it is
not sufficient to consider them in a one-note basis. Just as a human expert would
have problems identifying interpreters from listening to one-note audio files,
the trained classifiers are not able to identify the performers reliably given this
limited information. As soon as there are more notes involved together with the
context in which they appear, the trained classifier (just as a music expert) is able to
identify the interpreter.
One issue, which is not clear from the reported results, is what features are
mostly responsible for the identification results. In order to investigate this, we
have performed an additional experiment in which we have applied each model
separately. For the duration model, the obtained best accuracy and average accuracy
are 51.1 and 46.7, respectively. For the energymodel, the obtained best accuracy and
average accuracy are 47.6 and 43.0, respectively. Finally for the transition model,
the obtained best accuracy and average accuracy are 43.9 and 41.6, respectively.
These results seem to indicate that there is some performer-specific information in
the isolated duration, energy, and transition models, but the models are certainly
more accurate at identifying interpreters when considered together.
Table 5.1 Classification accuracy for the 1-note and short phrases (in correctly classified
instances percentage)
Algorithm 1-note Phrase
Decision trees 29.1 69.3
Support vector machines 35.7 80.7
Artificial neural networks 32.9 76.5
k-Nearest neighbor 30.6 72.9
Bagging (decision trees) 31.2 78.1
Boosting (decision trees) 30.5 74.1
Voting (decision trees, SVM, ANN, 1-NN) 32.9 76.5
Stacking (decision trees, SVM, ANN, 1-NN) 31.7 81.2
5 Modeling, Analyzing, Identifying, and Synthesizing Expressive. . . 141
5.7 Conclusions
In this chapter, we outlined past research on modeling expressive performance
using machine learning techniques and presented a particular approach to quantita-
tively model and identify interpreters in expressive audio performances. The
approach is based on the application of sound analysis techniques to monophonic
audio recordings in order to extract pitch, timing, amplitude and transition features,
characterizing both the notes and the musical context in which they appear.
Different machine learning techniques were explored to train computational models
characterizing different aspects of expressive performance, and the resulting
models were used to analyze and generate expressive performances as well as to
automatically identify performers. The results indicate that the features extracted
contain sufficient information and the explored machine learning methods are
capable of learning patterns that characterize expressive music performance.
Questions
1. What are the four musicological questions that this study attempts to answer?
2. Name three areas that could be helped by answers to these questions.
3. How is note segmentation done on the audio stream?
4. What are the two main principles recognized by Narmour in his theory?
5. How many prototypical Narmour structures are there?
6. How is bow direction detected in the gesture acquisition?
7. What levels of metrical strength are defined in the note descriptors?
8. What are some of the traditional deviations found in Irish jig music?
9. Why might the results be poor for the 1-note experiments?
10. What were the most successful and least successful classifiers in the results?
References
1. Gabrielsson A (1999) The performance of music. In: Deutsch D (ed) The psychology of music,
2nd edn. Academic, New York, 579 pages
2. Gabrielsson A (2003) Music performance research at the millennium. Psychol Music
31(3):221–272
3. Johnson ML (1992) An expert system for the articulation of Bach fugue melodies. In: Baggi
DL (ed) Readings in computer-generated music. IEEE Computer Society, Los Alamitos,
pp 41–51
4. Bresin R (2001) Articulation rules for automatic music performance. In Proceedings of the
international computer music conference. International Computer Music Association, San
Francisco, pp 294–297
5. Friberg A, Bresin R, Fryden L (2000) Music from motion: sound level envelopes of tones
expressing human locomotion. J New Music Res 29(3):199–210
6. Friberg A, Bresin R, Sundberg J (2006) Overview of the KTH rule system for musical
performance. Adv Cogn Psychol Spec Issue Music Perform 2(2–3):145–161
142 R. Ramirez et al.
7. Canazza S, De Poli G, Roda A, Vidolin A (1997) Analysis and synthesis of expressive
intention in a clarinet performance. In Proceedings of the 1997 international computer music
conference. International Computer Music Association, San Francisco, pp 113–120
8. Canazza S, De Poli G, Drioli C, Roda A, Vidolin A (2004) Modeling and control of
expressiveness in music performance. Proc IEEE 92(4):286–701
9. Dannenberg RB, Derenyi I (1998) Combining instrument and performance models for
high-quality music synthesis. J New Music Res 27(3):211–238
10. Mitchell TM (1997) Machine learning. McGraw-Hill, New York, 542 pages
11. de Mantaras RL, Arcos JL (2002) AI and music, from composition to expressive performance.
AI Mag 23(3):43–57
12. Ramirez R, Hazan A (2006) A tool for generating and explaining expressive music
performances of monophonic jazz melodies. Int J Artif Intell Tools 15(4):673–691
13. Ramirez R, Hazan A, Maestre E, Serra X (2008) A genetic rule-based expressive performance
model for jazz saxophone. Comput Music J 32(1):38–50
14. Widmer G (2001) Discovering strong principles of expressive music performance with the
PLCG rule learning strategy. In Proceedings of the 12th European conference on machine
learning (ECML’01), Germany. Springer, Berlin, pp 552–563
15. Widmer G (2002) Machine discoveries: a few simple, robust local expression principles.
J New Music Res 31(1):37–50
16. Tobudic A, Widmer G (2003) Relational IBL in music with a new structural similarity
measure. In Proceedings of the international conference on inductive logic programming.
Springer, pp 365–382
17. Dovey MJ (1995) Analysis of Rachmaninoff’s piano performances using inductive logic
programming. In European conference on machine learning. Springer, pp 35–38
18. Van Baelen E, De Raedt L (1996) Analysis and prediction of piano performances using
inductive logic programming. In International conference in inductive logic programming,
pp 55–71
19. Saunders C, Hardoon D, Shawe-Taylor J, Widmer G (2004) Using string kernels to identify
famous performers from their playing style. In Proceedings of the 15th European conference
on machine learning (ECML’2004), Pisa, Italy, pp 384–395
20. Stamatatos E, Widmer G (2005) Automatic identification of music performers with learning
ensembles. Artif Intell 165(1):37–56
21. Ramirez R, Maestre E, Serra X (2010) Automatic performer identification in commercial
monophonic jazz performances. Pattern Recognit Lett 31:1514–1523
22. Ramirez R, Perez A, Kersten S, Maestre E (2008) Performer identification in celtic violin
recordings. In International Society of Music Information Retrieval (ISMIR) conference,
Philadelphia, USA, pp 483–488
23. Molina-Solana M, Arcos JL, Gomez E (2010) Identifying violin performers by their expressive
trends. Intell Data Anal 14(5):555–571
24. Narmour E (1990) The analysis and cognition of basic melodic structures: the implication
realization model. University of Chicago Press, Chicago, 358 pages
25. Gómez E, Klapuri A, Meudic B (2003) Melody description and extraction in the context of
music content processing. J New Music Res 32:33–54
26. Klapuri A (1999) Sound onset detection by applying psychoacoustic knowledge.
In Proceedings of the IEEE international conference on acoustics, speech and signal
processing, ICASSP, pp 3089–3092
27. Maher RC, Beauchamp JW (1994) Fundamental frequency estimation of musical signals using
a two-way mismatch procedure. J Acoust Soc Am 95:2254–2263
28. McNab RJ, Smith LA, Witten IH (1996) Signal processing for melody transcription, working
paper 95/22, Hamilton, New Zealand, University ofWeikato, Department of Computer Science
29. Maestre E, Gomez E (2005) Automatic characterization of dynamics and articulation of
monophonic expressive recordings. In Proceedings of the 118th AES convention, Barcelona,
Spain, pp 36–40
30. Narmour E (1991) The analysis and cognition of melodic complexity: the implication
realization model. University of Chicago Press, Chicago, 321 pages
5 Modeling, Analyzing, Identifying, and Synthesizing Expressive. . . 143
31. BlockeelH, DeRaedt L, Ramon J (1998) Top-down induction of clustering trees. In Shavlik J(ed)
Proceedings of the 15th international conference on machine learning, Madison, Wisconsin,
USA. Morgan Kaufmann, pp 53–63
32. Quinlan JR (1993) C4.5: programs for machine learning. Morgan Kaufmann, San Francisco,
305 pages. ISBN 1–55860–238–0
33. Rosinach V, Traube C (2006) Measuring swing in Irish traditional fiddle music. In Proceedings
of international conference on music perception and cognition, pp 1168–1171
34. Friberg A, Sundstrom J (2002) Swing ratios and ensemble timing in jazz performances:
evidence for a common rhythmic pattern. Music Percept 19(3):333–349
35. Maestre E, Hazan A, Ramirez R, Perez A (2006) Using concatenative synthesis for expressive
performance in jazz saxophone. In Proceedings of international computer music conference,
New Orleans, pp 82–85
36. Cristianini N, Shawe-Taylor J (2000) An introduction to support vector machines. Cambridge
University Press, Cambridge, 190 pages. ISBN 0–521–78019–5
37. Chauvin Y, Rumelhart ED (eds) (1995) Backpropagation: theory, architectures and
applications. Lawrence Erlbaum Assoc, Hillsdale, 549 pages. ISBN 0–8058–1259–8
144 R. Ramirez et al.
