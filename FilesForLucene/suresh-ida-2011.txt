A Stylometric Study and Assessment of Machine
Translators
V. Suresh, Avanthi Krishnamurthy, Rama Badrinath,
and C.E. Veni Madhavan
Department of Computer Science and Automation, Indian Institute of Science,
Bangalore 560 012, India
{vsuresh,rama,cevm}@csa.iisc.ernet.in, avanthi.krishnamurthy@gmail.com
Abstract. In this work we present a statistical approach inspired by
stylometry –measurement of author style– to study the characteristics
of machine translators. Our approach quantifies the style of a translator
in terms of the properties derived from the distribution of stopwords
in its output – a standard approach in modern stylometry. Our study
enables us to match translated text to the source machine translator
that generated them. Also, the stylometric closeness of human generated
text to that generated by machine translators provides handles to assess
the quality of machine translators.
Keywords: Machine Translation, Stylometry, Stopwords, Topic
Models, LDA.
1 Introduction
Modern machine translation approaches [2,3] are often statistical in nature and
could be traced back to the pioneering work of Weaver [9]. Assessment of a ma-
chine translator’s output is important for establishing benchmarks for translation
quality. An obvious way to assess the quality of machine translation is through
the perception of human subjects. Though highly reliable, this approach is not
scalable as the outputs to be assessed could be huge. Hence mechanisms have
been devised to automate the assessment process. In principle such assessment
methods are essentially a study of correlations between human translation and
machine translation.
In this work, we present a scalable approach to assess the quality of machine
translation that borrows features from the study of writing styles, popularly
known as stylometry. Stylometric techniques are useful in ascertaining authorship
of disputed documents as illustrated by the case of authorship attribution of
the disputed documents in the Federalist Papers [10]. Counterintuitively, these
methods [11] often rely on studying the distribution of stopwords – words that do
not necessarily convey syntactic meaning – rather than contentwords, which carry
such emphasis. One of the reasons is that stopwords such as articles, prepositions,
popular verbs, pronouns etc., occur in great profusion that even though the
number of stopwords is small compared to the size of the lexicon, they are
J. Gama, E. Bradley, and J. Hollmén (Eds.): IDA 2011, LNCS 7014, pp. 364–375, 2011.
c© Springer-Verlag Berlin Heidelberg 2011
A Stylometric Study and Assessment of Machine Translators 365
typically around 30% of written text (in terms of word usage) and hence provide
richer statistics for analysis when compared to contentwords.
We first show that our stylometric approach correctly maps a given translated
output to the machine translator that generated it. This is akin to author iden-
tification: measuring the style of the text given and identifying the author whose
style is closest to it. As an extension of this idea, we use our approach to cate-
gorize the efficacy of machine translators based on how close in terms of style,
human generated text are to the text generated by the machine translators. Typ-
ically, a machine translator’s quality is assessed by comparing it against a human
generated reference text on a document by document basis. Non availability of
human reference documents – blogs for example – pose limiting contexts for
such assessment schemes. However, assessment of translators viewed in the con-
text of stylometry, is a problem of identifying whether or not the text possesses
qualities that are usually observed in the ones generated by humans; this can
be done even in the absence of standard per document human references. Thus
our approach offers a scalable way to benchmark the quality of translators even
in the absence of standard references. Our experimental results for the popular
web based translators are in accordance with subjective opinion regarding the
quality of the translated text.
The rest of the paper is organized as follows. In the next section we give
a description of the conventional measures that are used for the purpose of
evaluating the quality of machine translation. Section 3 explains our approach;
this is followed by section 4 that details our experiments and results. In section 5
we summarize the findings of this work and present our concluding remarks.
2 Evaluation Measures
Measures to evaluate the quality of translators have largely depended on compar-
ing a translator’s output with a human generated translation as the reference.
The quality of a translator is ascertained based on overlaps in properties like
number of words in each sentences, preservation of word orderings, editing oper-
ations required to morph one into another, occurrences of n-grams etc., between
the human and the translator generated texts. These have evolved into some im-
portant benchmarks in the field of machine translation evaluation. In following
we outline a few important evaluation measures developed over the past years.
2.1 Edit Distance Based Metrics
Here the minimum number of atomic operations like insertions, deletions etc.,
that must be made on the machine translation in order to transform it to the
reference translation is considered for evaluation. We consider some standard
metrics that come under this category.
Word Error Rate (WER). WER is defined as the ratio of Levenshtein dis-
tance1 between machine translation to reference translation calculated at the
1 http://en.wikipedia.org/wiki/Levenshtein distance
366 V. Suresh et al.
word level to the number of words in the reference translation. Levenshtein dis-
tance is defined as the minimum number of word insertions, substitutions and
deletions necessary to transform one sentence to other.
PositionIndependent Word Error Rate (PI-WER). The major problem
with WER is that it heavily depends on the word order between the two sen-
tences; but the word order of the machine translation might be different from
that of reference translation. To address this problem PIWER [15] was intro-
duced. This metric is similar to WER but it neglects word order completely.
Translation Edit Rate (TER). TER is defined as the number of edits needed
to change a machine translation so that it exactly matches one of the references,
normalized by the average length of the references [16]. Possession of multiple
references is assumed here. The allowed edit operations are insertion, deletion
and substitution of single words or word sequences. This measures the amount
of human work that would be required to post-edit the machine translation to
convert it into one of the references. A related metric called HTER (Human-
targeted TER) is also used. TER usually scores a candidate translation against
an existing reference translation; HTER scores a candidate translation against
a post-edited version of itself.
2.2 N-Gram Based Metrics
These metrics evaluate the translator’s quality based on the co-occurrence of
n-grams between the machine and reference translations.
BLEU(BiLingual Evaluation Understudy). BLEU [4] is a precision based
approach known to exhibit correlations with human judgement on translation
quality. It measures how well a machine translation overlaps with multiple hu-
man translations using n-gram co-occurrence statistics. Final score is a weighted
geometric average of the n-gram scores. In addition to this, to prevent very
short sentences getting high precision scores, a brevity penalty is added. Higher
order n-grams account for fluency and grammaticality – the quality of being
grammatically well formed text. Typically n-gram length of 4 used.
NIST(from National Institute of Standards and Technology). NIST [8]
improves the assessment approach of BLEU by giving weights for the matching n-
gram such that rarer words get more weight. For example, matching of stopwords
– words like a, an, and, the etc., – are given less weight over matching of words
that convey more information. In addition, some other corrections were made:
not being case insensitive, less punitive brevity penalty measure etc.
METEOR (Metric for Evaluation of Translation with Explicit ORder-
ing). METEOR [7] takes into account the precision and recall of unigrams of
the human reference text and the translator’s output. Precision is computed as
the ratio of the number of unigrams in the machine translator’s output that are
also found in the reference translation, to number of unigrams in the candidate
A Stylometric Study and Assessment of Machine Translators 367
translation. For recall, the numerator is the same as in precision, bt the denom-
inator denotes the number of unigrams in the human translation. Precision and
recall are combined using a harmonic mean –with more weight for the former– in
order to compute a correlation between the human and the machine translator’s
output. In addition, this approach also matches synonyms and uses a stemmmer
to lemmatise the words before computing the measure.
ROUGE (Recall Oriented Understudy for Gisting Evaluation).ROUGE
[14] which is a recall oriented metric was developed with the same idea of BLEU.
It is a widely used technique for the evaluation of summaries. It has been shown in
[13] that ROUGE-L, ROUGE-W and ROUGE-S statistics correlates with human
judgements and hence can be used to assess the quality of machine translation.
However, main disadvantages of the above mentioned methods are:
– Many important characteristics like stopword distributions which start to
appear at the document level are lost when evaluation is done at the sentence
level
– Evaluation cannot be done in the absence of a human translation acting as
a reference document.
In addition to this, Callison-Burch et al [1] have showed that BLEU-like metrics
are unreliable at the level of individual sentence due to data sparsity.
To address these issues, we present a novel approach to assess the quality
of machine translation that takes into account only the stopwords present in
the text. Also, in our approach the evaluation is done at the document level,
hence we capture the translator’s behaviour over the entire document rather
than over individual sentences. The overall essence of our approach is to identify
if a machine translation has the characteristics of human generated text or not.
For this purpose an exact reference translation is not required, all that is required
is a corpus that contains a collection of standard human generated translations.
Thus our approach is free from the restrictive requirement of possessing human
reference translations.
Stopwords have been previously used for the purpose of stylometry or author
attribution [10, 11]. As mentioned earlier, stopwords are words that do not usu-
ally convey syntactic meaning but occur in significant quantities in the English
language usage. Our approach uses 571 stopwords from a standard list2.
In our approach, we study the two popular web based translators, namely
the Google Translator3 and Microsoft’s Bing Translator4, and assess the quality
of their translation by finding how close their outputs are to human generated
text. Our subjective impression – after going through the translations of the
French novels, The Count of Monte Cristo, The Three Musketeers and The Les
Miserables – suggests that Google Translator’s output is of a better quality than
Bing’s. Results based on this heuristic are in line with this observation.
2 http://www.ai.mit.edu/projects/jmlr/papers/volume5/lewis04a/
lyrl2004 rcv1v2 README.htm
3 http://translate.google.com
4 http://www.microsofttranslator.com
368 V. Suresh et al.
3 Evaluation Heuristic
Identification of the source translator that generated a given document is based
on the stylometric closeness of the given document to the translators. Likewise,
our assessment of the quality of a machine translator’s output is based on its
overall stylometric closeness to text generated by human translation. The text
mentioned here comprises only of stopwords: all non-stopwords (known as con-
tentwords) are removed from the text prior to applying our heuristics. First,
features are extracted from the machine translators’ outputs and the reference
human translations. Then, the decision as to which translator produces text that
is stylometrically closer to human generated text, is made based on how close
the features corresponding to the human translation are to the features of the
machine translators.
The features we extract are derived from a topic modelling mechanism like the
Latent Dirichlet Allocation (LDA) [5]. LDA is used to extract common topics
from text corpora with the view to finding hidden or latent topic structures in
them. The latent or hidden structures are popularly called as topics. Typically a
topic is a distribution over words in the dictionary. Every word is associated with
every topic but differ in their strength based on probabilities. With topics defined
this way, documents are viewed as a mixture of these topics. Each document is
then viewed as a vector with as many components as the number of topics
wherein each component is indicative of the presence of the corresponding topic
in that document. The components of these vectors viewed as n-tuples (n being
the number of topics) are used as features for our approach. Such an approach
is known to be effective for stylometric purposes [12].
Upon generating the topic vectors, stylometric closeness is measured as fol-
lows: The vectors from the documents are given labels to denote the translator
from which it was derived (Google or Bing in this case) and classified using an
a Support Vector Machine [6] classifier. Given a translated document, its topic
vector (unlabelled) is computed and presented to the classifier. The label pre-
dicted identifies the source translator. SVM’s label prediction is in terms of how
close the unlabelled vector is to the set of labelled vectors belong to the different
classes of tranalators
Likewise, for assessment of translation quality, a similar approach is used.
SVM classifier is built based on the labelled topic vectors from the different
tranalators. The topic vectors derived from the human translated documents
(using LDA) are presented to the classifier. The output of the classifier – the
translator to which they are assigned to – is recorded for each human document.
The translator to which a majority of human translated documents are assigned
is deemed qualitatively better. The experiments and results of applying this
heuristic on French to English translation is described in the following section.
4 Experiments and Results
We considered the original French texts: The Count of Monte Cristo, The Three
Musketeers, Les Miserables and their English translations (human generated)
A Stylometric Study and Assessment of Machine Translators 369
freely available from the Project Gutenberg5. Hereafter, these novels will be re-
ferred to as CM, TM and LM respectively. The original texts were then trans-
lated into English using the Google and Bing translators. Samples of these trans-
lation are shown in Table 1. We mentioned earlier that our approach identifies
the better translator. This statement must be refined in view of the sample
output seen in the table as “our approach identifies the translator that is less
worse”, given that both these translators are woefully inadequate when it comes
to preserving grammatical correctness and the context of the narrative.
Table 1. Human, Google and Bing translation of the opening passage from the Count
of Monte Cristo. It is evident that both the translators fall short even in constructing
grammatically correct sentences. Note that both the machine translators refer to ship
as building! Subjectively however, over many such passages one feels that Google is
better compared to Bing. (To put it better, Google is less worse among the two.)
Human: On the 24th of February, 1815, the look-out at Notre-Dame de la Garde
signalled the three-master, the Pharaon from Smyrna, Trieste, and Naples. As usual,
a pilot put off immediately, and rounding the Chateau d’If, got on board the vessel
between Cape Morgion and Rion island. Immediately, and according to custom, the
ramparts of Fort Saint-Jean were covered with spectators; it is always an event at
Marseilles for a ship to come into port, especially when this ship, like the Pharaon,
has been built, rigged, and laden at the old Phocee docks, and belongs to an owner
of the city.
Google: On February 24, 1815, the lookout of Notre-Dame de la Garde reported the
three-masted Pharaon, from Smyrna, Trieste and Naples. As usual, a coast pilot im-
mediately set the port, shaved Chateau d’If, and went to approach the vessel between
Cape Morgion and the island Rion. Immediately, as usual again, the platform of the
Fort St. John was covered with spectators, for it is always a great affair Marseille that
the arrival of a building, especially when this ship, as the Pharaon was built, rigged,
stowed on the sites Phocaea old, and belongs to an owner of the city.
Bing: 24 February 1815, the surveillance of our Lady of the guard himself the uniden-
tified Pharaon, coming from Smyrna, Trieste, and Naples. As usual, a coastal driver
immediately sailed from the port, rasa If Castle, and alla approach the ship between
the Morgion Cape and island of Rio. Immediately, as usual again, platform of fort
Saint-Jean was covered with curious; because it is always a great case in Marseille
than the arrival of a building, especially when this building, like the Pharaon, was
built, rigged, stowed on Phocaea, old sites and belongs on a shipowner to the city.
4.1 Comparison with Standard Metrics
Before presenting the results based on our approach, we show that the standard
existing approaches give similar results. As the original and translated novels
5 www.gutenberg.org
370 V. Suresh et al.
Table 2. ROUGE scores for CM, TM and LM. G-x and B-x corresponds to the length
of text considered from Google and Bing translators respectively. x ranges from 100 to
500 words in steps of 100.
ROUGE-L ROUGE-W-1.2 ROUGE-W-1.2
Length CM TM LM CM TM LM CM TM LM
G-100 0.550 0.647 0.613 0.178 0.218 0.209 0.296 0.413 0.373
B-100 0.449 0.516 0.532 0.142 0.166 0.175 0.186 0.240 0.255
G-200 0.580 0.672 0.638 0.167 0.202 0.193 0.299 0.417 0.384
B-200 0.486 0.558 0.553 0.136 0.160 0.161 0.197 0.257 0.261
G-300 0.560 0.690 0.650 0.161 0.195 0.183 0.304 0.426 0.389
B-300 0.514 0.579 0.567 0.134 0.157 0.154 0.208 0.265 0.266
G-400 0.613 0.699 0.659 0.158 0.189 0.177 0.308 0.428 0.391
B-400 0.531 0.592 0.577 0.132 0.152 0.149 0.214 0.271 0.268
G-500 0.626 0.707 0.667 0.156 0.184 0.173 0.314 0.432 0.395
B-500 0.546 0.560 0.585 0.132 0.149 0.146 0.221 0.274 0.272
had different number of sentences we could not use BLEU and METEOR for
evaluation. However the results based on ROUGE (which does not depend on
the number of sentences) suggests that Google performs better than Bing.
We show the ROUGE results using following three metrics:
– ROUGE-L: Longest Common Subsequence
– ROUGE-W-1.2: Weighted Longest Common Subsequence with weight factor
as 1.2
– ROUGE-S4: Skip-bigram match allowing 4 gaps
Table 2 shows the ROUGE-L, ROUGE-W-1.2, and ROUGE-S4 results for Google
and Bing applied to the three novels. These values were computed by considering
a chunk of words from each chapters of these novels and applying ROUGE on
them individually and then averaging them to obtain the scores shown in the
table. The number of words in text chunk was varied from 100 to 500 words
in steps of 100. In all these experiments, it was evident that Google performs
better than Bing. In these experiments, we made use of the fact that we were in
possession of the translated as well as the reference documents. In the following
we describe our experiments based on our approach wherein we show that our
results do not depend on the availability of exact references.
4.2 Our LDA Based Approach
Each chapter of these novels was considered as a separate document and was
presented to a Latent Dirichlet Allocation mechanism to compute the topic vec-
tors. We used GibbsLDA++6 with default settings. Prior to this, as mentioned
earlier, only the stopwords are retained. The number of topics to be extracted
is subjective and could be varied. For the present experiments this number was
6 GibbsLDA++: http://gibbslda.sourceforge.net
A Stylometric Study and Assessment of Machine Translators 371
varied from 10 to 25 in steps of 5. These topic vectors are then labelled cor-
respondingly as Google/Bing and used to generate a classifier using Support
Vector Machine as mentioned earlier. For the present work, we used the public
domain tool LIBSVM [6] with default settings.
4.3 Source Identification
First, we check if the approach is capable of identifying the source of the machine
translation – Google or Bing in the present case. Towards this, topic vectors from
a set of selected documents from the Google and Bing output are used to build
an SVM classifier; the remaining documents are used to test whether they are
assigned the correct class or not. The results are shown in Table 3 for different
number of topics. We see that our approach correctly identifies the translator in
most of the cases with high accuracy over topic ranges 10 to 20 (the aberration in
the second row corresponding to topic number 15 needs further investigation). It
can be seen from the table that the training and testing sets are not necessarily
from the same novels, or even from the same author. This shows that the machine
translator have unique quantifiable styles that are independent of style of the
source authors.
Table 3. Test to identify the correct translator. Column one refers to the number of
documents translated with both Google and Bing, Column two represents the number
of the particular machine translated documents used for testing. The last 4 columns
give the percentage of correct classification for different number of topics.
Accuracy(%)
Training set Testing set 10 15 20 25
CM, 117 TM, 67 - Google 83.58 100.00 67.16 97.02
CM, 117 TM, 67 - Bing 77.61 37.31 59.70 98.51
LM, 220 TM, 67 - Google 64.18 98.51 92.54 53.73
LM, 220 TM, 67 - Bing 76.12 98.51 98.51 76.12
LM, 220 CM, 117 - Google 66.67 98.29 82.91 41.88
LM, 220 CM, 117 - Bing 75.21 75.21 94.87 42.74
4.4 Qualitative Assessment
The quality of the translator is judged by the closeness of the topic vectors result-
ing from the human translation to those that are computed from the translated
documents. This is based on label assigned by the SVM classifier (trained with
the translated documents as mentioned earlier) to the topic vectors from the
human documents. More the documents assigned to translator better it is in
terms of its closeness to human style.
We had previously mentioned that Google translation appears to be subjec-
tively better than Bing. Our approach identifies a majority of human generated
documents as closer to Google and hence is in accordance with the subjective
evaluation of the translations. This is shown in Table 4. Note that the training
372 V. Suresh et al.
Table 4. Google vs. Bing Translators. Column one refers to the number of documents
translated with both Google and Bing, Column two represents the number of human
translated documents used for testing. The last 4 columns give the percentage of human
translated documents identified as google for different number of topics.
Classified as Google(%)
Training set Testing set 10 15 20 25
TM, 67 TM, 67 100.00 98.51 92.54 98.51
CM, 117 CM, 117 76.92 66.67 86.33 83.76
LM, 220 LM, 220 61.36 76.82 82.27 50.91
CM, 117 TM, 67 83.58 83.58 79.11 95.52
LM, 220 CM, 117 67.52 93.16 69.23 52.14
LM, 220 TM, 67 68.66 95.52 97.02 47.76
vectors mentioned in the table are from the outputs of Google and Bing transla-
tors and the testing vectors are the ones computed from the human translations.
It can also be seen that the results are almost invariant to the number of topics
chosen. In some cases the entire human corpus is identified as closer to Google
resulting in an accuracy of 100%. It is to be viewed in the following sense. In
real, the output of the classifier is dependent upon which cluster the human doc-
ument is closest to. Ideally the closeness must be defined in terms of a margin
and if both the translators lie within the margin, then it must be deemed as a tie.
We have not employed such margin based tie resolution mechanism and hence
these very high values are in favour of Google. However, it must be noted that
our method was not biased in favour of any one of these translators beforehand,
and hence not having a margin for classification might actually help both these
translators equally likely and it is possible that Bing derives such benefits too.
But it is evident that Google translator is consistently better than Bing (or to
be more accurate, less worse).
We now point our attention to the lower half (fourth row downwards) of
the Table 4: the machine translated documents used for training the classifier
and the set of human documents used for testing belong to different novels and
authors. For example, the machine translation of The Count of Monte Cristo is
compared with the human translation of The Three Musketeers. We observe that
our identifies a majority of the human translated documents as closer to Google
translator. Thus our results are fairly independent of the specific nature of human
translation and it suggests that our approach captures the overall closeness of the
machine translation to human generated text. This establishes the independence
of our approach from the restriction to have an exact human translation of the
original text for assessing the quality of the machine translation. This is useful
in situations where human translations do not exist.
Since LDA is usually employed to cluster meaningful words into their natural
topics (in fact, usually the stopwords are removed from the documents prior
to computing topic groupings as it is assumed that stopwords would uniformly
distribute over topics), one would be interested in knowing how the stopwords
are grouped into topics. Table 5 lists the top five stopwords for five topics from
A Stylometric Study and Assessment of Machine Translators 373
Table 5. A partial listing of topics and the top five words (w) in each with probabilities
(p) for the Human, Google and the Bing corpora out of 20 topics
Topics Human Google Bing
w p w p w p
said 0.080 said 0.087 and 0.085
and 0.076 and 0.081 said 0.076
I will 0.049 will 0.047 it 0.050
be 0.042 go 0.042 is 0.050
is 0.040 is 0.040 will 0.048
the 0.260 the 0.263 the 0.257
and 0.112 and 0.122 and 0.125
II of 0.091 of 0.089 of 0.095
which 0.046 which 0.040 which 0.040
had 0.036 had 0.035 had 0.0357
the 0.157 the 0.158 the 0.153
with 0.076 with 0.068 with 0.075
III for 0.060 was 0.058 for 0.058
was 0.054 for 0.057 was 0.057
would 0.049 would 0.054 would 0.055
a 0.158 a 0.165 a 0.187
of 0.099 of 0.096 of 0.101
IV in 0.076 in 0.074 in 0.077
his 0.065 his 0.066 his 0.065
which 0.065 which 0.056 which 0.053
you 0.134 you 0.145 you 0.150
to 0.078 to 0.068 me 0.080
V me 0.067 me 0.066 to 0.063
i 0.056 i 0.058 i 0.052
have 0.055 have 0.046 have 0.046
an experiment with 20 topics. Note that the topic ordering and the associated
probabilities for the Human, Google and Bing corpus are similar that it is visually
difficult to judge as to which one is closest to the Human corpus in terms of these
properties.
Note that unlike the previous approaches, the judgement is made in terms
of closeness to clusters in the feature space formed by outputs of the machine
translators rather than closeness to the actual translation. In essence, this could
be construed as comparing the human generated document with the entire set
of documents as represented in the feature space of the translators. Hence this
approach is more stringent in terms of assessing the translator’s quality.
These experiments serve to suggest that our heuristic is capable of evalu-
ating translation quality that is in accordance with the human perception. In
addition, unlike an approach like METEOR [7], our approach does not consider
contentwords; this is very useful in reducing the effort required for evaluation –
matching synonyms, considering inflections etc. Though very small in number,
374 V. Suresh et al.
stopwords constitute around 30% of English text and hence seem to possess sta-
tistical properties of higher quality when compared to those resulting from the
contentwords. In addition, this makes it independent of the nature of synonyms
chosen by one translator over another thereby reducing the synonym matching
as done by typical evaluation approaches.
Our results in this section are based on applying our approach to standard
documents like books, however there should be no difficulties in applying it to
assess other web documents like blog posts and reviews. Also, one might argue
that the present assessment mechanism could be gamed – translators could pro-
duce text in such a way that the stopword distributions resemble those from
human generated text. Such a criticism is not without its own merits, hence
we make the following observations: Firstly, we assess standard translators from
reputed sources – Google and Microsoft in this case – wherein earning the trust
of online users is given more weightage than manipulating the output to score
in assessment tests. Secondly, it is not clear how easy it would be to manipulate
the translators to generate output that have stopword distribution resembling
that of human texts (randomly displaying a human generated text is one naive
possibility). Lastly, and more importantly, using stopwords for author identifi-
cation is an accepted approach in stylometry, and our is presented as a bridge
between stylometry and translator assessment mechanisms with the hope that
improvements in one area could be quickly passed on to the other.
5 Conclusions
We have presented a stylometry based approach for assessing the quality of
machine translation in a scalable manner. The present approach borrows from
stylometry the practice of studying the properties of stopwords in texts rather
than the whole text. More importantly, our approach does not require the exact
human translation of the original text – all that is required is the human trans-
lation of any standard text. This would benefit assessing the translation quality
in situations wherein reference documents are not readily available.
References
1. Callison-Burch, C., Osborne, M., Koehn, P.: Re-evaluating the role of BLEU in
Machine Translation Research EACL, pp. 249–256 (2006)
2. Brown, P.F., Pietra, V.J.D., Pietra, S.A.D., Mercer, R.L.: The Mathematics of
Statistical Machine Translation: Parameter Estimation. Comput. Linguist. 19(2),
263–311 (1993)
3. Hutchins, W.J., Somers, H.L.: An Introduction to Machine Translation. Academic
Press, London (1992)
4. Papineni, K., Roukos, S., Ward, T., Zhu, W.-J.: BLEU: A Method for Automatic
Evaluation of Machine Translation. In: ACL 2002: Proceedings of the 40th Annual
Meeting on Association for Computational Linguistics, pp. 311–318 (2002)
5. Blei, D., Ng, A.Y., Jordan, M.I.: Latent Dirichlet Allocation. Journal of Machine
Learning Research 3 (2001)
A Stylometric Study and Assessment of Machine Translators 375
6. Chung Chang, C., Lin, C.-J.: LIBSVM: A Library for Support Vector Machines
(2001)
7. Lavie, A., Agarwal, A.: Meteor: An Automatic Metric for MT Evaluation with
high levels of Correlation with Human Judgments. In: StatMT 2007: Proceedings
of the Second Workshop on Statistical Machine Translation, pp. 228–231 (2007)
8. Doddington, G.: Automatic Evaluation of Machine Translation Quality using N-
gram Co-occurrence Statistics. In: International Conference on Human Language
Technology Research, pp. 138–145 (2002)
9. Weaver, W.: Translation. In: Locke, W.N., Donald Booth, A. (eds.) Machine Trans-
lation of Languages: Fourteen Essays, pp. 15–23. MIT Press, Cambridge (1949)
10. Mosteller, F., Wallace, D.L.: Applied Bayesian and Classical Inference: The Case
of the Federalist Papers. Springer, Heidelberg (1964)
11. Arun, R., Suresh, V., Veni Madhavan, C.E.: Stopword Graphs and Authorship
Attribution in Text Corpora. In: Third IEEE International Conference on Semantic
Computing, Berkeley, CA, USA, pp. 192–196 (September 2009)
12. Arun, R., Suresh, V., Saradha, R., Narasimha Murty, M., Veni Madhavan, C.E.:
Stopwords and Stylometry: A Latent Dirichlet Allocation Approach. In: Neural
Information Processing System (NIPS) Workshop on Applications of Topic models
and Beyond, Vancouver, Canada (December 2009)
13. Lin, C.-Y., Och, F. J.: Automatic evaluation of machine translation quality using
longest common subsequence and skip-bigram statistics. Association for Compu-
tational Linguistics (2004)
14. Lin, C.-Y.: ROUGE: A Package for Automatic Evaluation of Summaries. In: ACL
Workshop on Text Summarization Branches Out (2004)
15. Tillmann, C., Vogel, S., Ney, H., Zubiaga, A., Sawaf, H.: Accelerated Dp based
search for Statistical Translation. In: European Conf. on Speech Communication
and Technology, pp. 2667–2670 (1997)
16. Snover, M., Dorr, B., Schwartz, R., Micciulla, L., Makhoul, J.: A Study of Transla-
tion Edit Rate with Targeted Human Annotation. Association for Machine Trans-
lation in the Americas, pp. 223–231 (2006)
