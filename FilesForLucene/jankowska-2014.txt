Ensembles of Proximity-Based One-Class Classifiers for
Author Verification
Notebook for PAN at CLEF 2014
Magdalena Jankowska, Vlado Kešelj, and Evangelos Milios
Faculty of Computer Science, Dalhousie University
{jankowsk, vlado, eem}@cs.dal.ca
Abstract We use ensembles of proximity based one-class classifiers for author-
ship verification task. The one-class classifiers compare, for each document of the
known authorship, the dissimilarity between this document and the most dissim-
ilar other document of this authorship to the dissimilarity between this document
and the questioned document. As the dissimilarity measure between documents
we use Common N-Gram dissimilarity based on character or word n-grams.
1 Introduction
We describe our submission to the task of Author Identification of the PAN 2014 compe-
tition [5]. This task presents participants with author verification problems, formulated
as follows: “Given a small set (no more than 5, possibly as few as one) of ‘known’ docu-
ments by a single person and a ‘questioned’ document, the task is to determine whether
the questioned document was written by the same person who wrote the known docu-
ment set.”
The required output in the competition task is a real number in the range from 0 to
1, encoding the probability of the positive answer to this question. A probability score
that is less than 0.5 is interpreted as a negative answer; a probability score that is greater
than 0.5 is interpreted as a positive answer; the score of 0.5 is interpreted as the "I don’t
know" answer.
The submissions are evaluated using the measure of area under the ROC curve
(AUC) based on the probability scores, and the c@1 measure [6]. c@1 is equivalent
to accuracy when the "I don’t know" answer is not used. For a given number of cor-
rect answers, the higher number of incorrect answers is replaced by "I don’t know", the
higher is c@1. The final evaluation score in the competition is the product of AUC and
c@1.
The Author Identification at PAN 2014 is similar to the Author Identification task
at PAN 2013, described in [2].
2 Methodology
We use an ensemble of our proximity-based one-class classifiers. The method is de-
scribed in detail in [1]. For the purpose of self-containment we describe our algorithm
below.
1069
Let A = {d1, ..., dk}, k ≥ 2, be a set of "known" documents written by a given
author. Let u be the questioned document which authorship we are to verify.
Our algorithm is a proximity-based one-class classification algorithm, based on
an idea resembling the idea of the k-centers algorithm [8,7]. The algorithm calcu-
lates for each known document di the maximum dissimilarity between this document
and all other known documents Dmax(di, A) as well as the dissimilarity between this
document and the questioned document D(di, u), and finally the dissimilarity ratio
r(di, u, A) =
D(di,u)
Dmax(di,A)
. We apply a threshold θ on the value of M(u,A) that is
the average of the r(di, u, A) over all known documents di, i = 1, ..., k. We classify u
as written by the same author as known documents iff M(u,A) <= θ. Specifically, we
linearly scale the average dissimilarity ratio M(u,A) using the threshold θ, so that the
value of M equal to θ corresponds to the score 0.5, values greater than θ correspond to
the scores between 0 and 0.5, and values less than θ correspond to the scores between
0.5 and 1 (a cutoff is applied, i.e. all values of M(u,A) < θ− cutoff are mapped to the
score 1, and all values of M(u,A) > θ + cutoff are mapped to the score 0).
For the dissimilarity measure between documents we use the Common N-Gram
(CNG) dissimilarity, proposed by Kešelj et al. [4]. For each document a sequence of the
most common n-grams (of characters or words) coupled with their frequencies (nor-
malized by the length of the document) is extracted; such a sequence is called a profile
of the document. The dissimilarity between two documents of the profiles P1 and P2 is
defined as follows:
D(P1, P2) =
∑
x∈(P1∪P2)
(
fP1(x)− fP2(x)
fP1
(x)+fP2
(x)
2
)2
(1)
where x is an n-gram from the union of two profiles, and fPi(x) is the normalized
frequency of the n-gram x in the the profile Pi, i = 1, 2 (fPi(x) = 0 whenever x does
not appear in the profile Pi).
If there is only one known document, we cut it in half to obtain two known docu-
ments. We also truncate all documents in a given problem to the length of the shortest
one. We also make sure that each profile for a given problem has exactly the same
length in cases when the number of distinct n-grams in any of the documents within
given problem is less than the requested length of the profiles.
Ensembles comprise of such classifiers that differ between themselves in at least
one of the following parameters: type of the tokens in n-grams (characters or words),
the length of n-grams, the length of profiles. We used ensembles with weighted vot-
ing [1] in the competition submission. The output probability score of an ensemble is
an arithmetic average of the scores of the single classifiers.
3 Selection of classifiers using training data
We select classifiers for the ensembles separately for each corpus, based on their perfor-
mance on the training datasets. We investigate performance of classifiers, varying their
parameters. The tokens were utf8-encoded characters or turned to uppercase words. For
classifiers based on characters the length of n-grams varied from 3 to 10. For classifiers
1070
based on word n-grams the length of n-grams varied from 1 to 6. The length of profiles
was in {200, 500, 1000, 1500, 2000, 2500, 3000} for both kinds of tokens. This space of
parameters results in 98 single classifiers: 56 character-based ones and 42 word-based
ones. Package Text::Ngrams [3] has been used in the software. For scaling of M values
to the probability scores the cutoff was set to 0.2.
We select for each training corpora separately a fixed odd number of 31 classifiers
that yield the best AUC. Subsequently for each of those classifiers the optimal threshold
is found (i.e., the threshold for which the maximum accuracy is achieved). In an ensem-
ble for a given corpus, the threshold for all classifiers is set to one value: the average of
the optimal thresholds on the training data for the selected single classifiers.
The ranges of AUC and of maximum accuracy (accuracy at the optimum threshold)
for the sets of 31 classifiers are presented in Table 1.
Table 1. Results of experiments on the training corpora in the PAN 2014 competition task Author
Identification.
range of results of 31 classifiers parameters of the classifier
with the highest AUC with the highest AUC
training corpus AUC maximum accuracy token
n-gram
length
profile
length
Dutch essays 0.82− 0.86 0.77− 0.83 character 5 500
Dutch reviews 0.54− 0.56 0.55− 0.59 character 7 200
English essays 0.52− 0.55 0.52− 0.58 word 4 3000
English novels 0.64− 0.74 0.62− 0.71 word 1 500
Greek articles 0.68− 0.79 0.66− 0.77 word 1 500
Spanish articles 0.82− 0.85 0.76− 0.80 word 1 500
We observe that our method performs best for the training corpus for Dutch essays
and Spanish articles. It performs worse on the Greek articles set. For the sets of English
novels and Dutch reviews the performance is low. Most likely the reason behind that
lies in the fact that in these two sets all but one problem have exactly one known docu-
ment. We observed that such problems are especially challenging for our method. This
is most likely because the two halves of a single known document, that we compare the
questioned document with, are much more similar to each other than two different doc-
uments written by the same person. The reasons behind the low results on the English
essays set are not clear to us and require further investigation.
4 Competition results
The results of our submission on the PAN 2014 evaluation set for the Author Identifi-
cation tasks are presented in Table 2. Similarly as in our experiments on the training
1071
data, the sets of English novels, English essays and Dutch reviews were most challeng-
ing for our method, as discussed in section 3. On the English novel set the ensemble
achieved a noticeable lower AUC value then the separate classifiers on the training data.
Our method performed better on the sets of Dutch essays, Spanish articles, and Greek
articles.
Table 2. The results of our submission in the PAN 2014 competition task Author Identification
(as announced on June 11, 2014). The final score is the product of the values of AUC and c@1.
AUC c@1 final score
Dutch essays 0.86892 0.84201 0.73165
Dutch reviews 0.6376 0.56 0.35706
English essays 0.5179 0.54837 0.284
English novels 0.49125 0.45727 0.22464
Greek articles 0.7308 0.68 0.49694
Spanish articles 0.8026 0.73 0.5859
References
1. Jankowska, M., Milios, E., Kešelj, V.: Author Verification Using Common N-Gram Profiles
of Text Documents. In: Proceedings of the 25th International Conference on Computational
Linguistics. COLING ’14 (August 2014)
2. Juola, P., Stamatatos, E.: Overview of the Author Identification Task at PAN 2013. In:
Forner, P., Navigli, R., Tufis, D. (eds.) Working Notes Papers of the CLEF 2013 Evaluation
Labs (September 2013)
3. Kešelj, V.: Perl Package Text::Ngrams (2013), http://www.cs.dal.ca/ vlado/srcperl/Ngrams
4. Kešelj, V., Peng, F., Cercone, N., Thomas, C.: N-gram-based Author Profiles for Authorship
Attribution. In: Proceedings of the Conference Pacific Association for Computational
Linguistics, PACLING’03. pp. 255–264. Dalhousie University, Halifax, Nova Scotia, Canada
(August 2003)
5. PAN: PAN competition, Author Identification (2014), http://www.uni-
weimar.de/medien/webis/research/events/pan-14/pan14-web/author-identification.html
6. Peñas, A., Rodrigo, A.: A Simple Measure to Assess Non-response. In: Proceedings of the
49th Annual Meeting of the Association for Computational Linguistics: Human Language
Technologies - Volume 1. pp. 1415–1424. HLT ’11, Association for Computational
Linguistics, Stroudsburg, PA, USA (June 2011)
7. Tax, D.: One Class Classification. Concept-learning in the absence of counter-examples.
Ph.D. thesis, Delft University of Technology (June 2001)
8. Ypma, A., Ypma, E., Duin, R.P.: Support Objects for Domain Approximation. In:
Proceedings of International Conference on Artificial Neural Networks. pp. 2–4. Springer,
Skovde, Sweden (September 1998)
1072
