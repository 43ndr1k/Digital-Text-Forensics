Accepted Manuscript
Adaptive-Interpolative Binarization with Stroke Preservation for Restoration of
Faint Characters in Degraded Documents
Soumen Bag, Partha Bhowmick
PII: S1047-3203(15)00123-6
DOI: http://dx.doi.org/10.1016/j.jvcir.2015.07.003
Reference: YJVCI 1556
To appear in: J. Vis. Commun. Image R.
Received Date: 20 February 2014
Accepted Date: 2 July 2015
Please cite this article as: S. Bag, P. Bhowmick, Adaptive-Interpolative Binarization with Stroke Preservation for
Restoration of Faint Characters in Degraded Documents, J. Vis. Commun. Image R. (2015), doi: http://dx.doi.org/
10.1016/j.jvcir.2015.07.003
This is a PDF file of an unedited manuscript that has been accepted for publication. As a service to our customers
we are providing this early version of the manuscript. The manuscript will undergo copyediting, typesetting, and
review of the resulting proof before it is published in its final form. Please note that during the production process
errors may be discovered which could affect the content, and all legal disclaimers that apply to the journal pertain.
  
Adaptive-Interpolative Binarization with Stroke
Preservation for Restoration of Faint Characters in
Degraded Documents
Soumen Baga,∗, Partha Bhowmickb
aDepartment of Computer Science and Engineering, Indian School of Mines,
Dhanbad-826004, INDIA
bDepartment of Computer Science and Engineering, Indian Institute of Technology,
Kharagpur-721302, INDIA
Abstract
A novel technique for binarization with stroke preservation of faint charac-
ters in degraded documents is proposed. It works in a multi-scale framework
with an adaptive-interpolative thresholding technique. Instead of computing
a global threshold value, it computes the local threshold values for a small
set of grid points by observing the intensity pattern of the pixels lying in the
concerned grid cells. Estimated thresholds are used, in turn, to compute the
threshold values of all the remaining pixels using a fast-yet-efficient interpola-
tion procedure. To handle noises in degraded images, this grid-based adaptive
thresholding is applied in successively reducing scales to obtain the near-optimal
binarization as a set of connected components. After a post-processing meant
for stroke preservation with these connected components, we get the final out-
put. Exhaustive experimentation and comparison with other existing methods
have been successfully carried out with benchmark datasets and also with our
own datasets.
Key words:
Adaptive thresholding, Degraded document image, Faint character, Grid-based
approach, Multi-scale framework, Stroke preservation.
∗Corresponding author.
Email addresses: bagsoumen@gmail.com (Soumen Bag), bhowmick@gmail.com (Partha
Bhowmick)
Preprint submitted to Elsevier May 8, 2015
  
1. Introduction
Document image binarization is often performed in the preprocessing stage
of different applications related to document image processing such as optical
character recognition (OCR), document image retrieval, word spotting, doc-
ument image indexing, etc. Though document image binarization has been
studied for many years [1, 2], the strategy for thresholding in case of degraded
document images is not yet a fully solved problem. This is due to the difficulty
in modeling different types of document degradation such as uneven illumina-
tion, image contrast variation, smear, marginal noise, and differential fading, as
illustrated in Fig. 1.
Binarization techniques for document images, in principle, can be broadly
classified into two categories: global thresholding and local thresholding. Global
thresholding methods [3–6] calculate a fixed threshold value for the binariza-
tion of the whole image. This approach works well for images of good quality.
However, fixed thresholding often fails if the illumination varies spatially in the
degraded image. In order to account for variations in illumination, the common
solution is adaptive thresholding. The main difference is that different threshold
values are computed in an adaptive manner for each pixel by inspecting its neigh-
bor pixels in the image. This technique provides more robustness to changes in
illumination. A number of adaptive thresholding methods are reported in the
literature [7–16]
As far as the problem of degraded documents is concerned, several works
have been reported in the literature [17, 18]. Leedham et el. [19] have compared
some traditional methods on degraded document images. Gatos et al. [20, 21]
have used different well-known strategies to design an adaptive method for low-
quality historical documents. Kavallieratou and Stamatatos [22] have proposed
a hybrid binarization approach for improving the quality of old documents us-
ing a combination of global and local thresholding. Lu et al. [23] has used a
document image binarization method that segments the text from badly de-
2
  
graded document images accurately. The method is based on the observations
that text documents usually have a background of uniform color and texture,
and the document text within it has a different intensity level compared to
the surrounding background. Tabatabaei and Bohlool [24] have also proposed a
method for separating text from background in badly illuminated document im-
ages. This technique is based on background estimation by using morphological
closing operation. Morphological closing operation is used to compensate for
uneven background illumination. Closing operation is applied to remove small
dark details while living the overall gray and larger dark features relatively
undisturbed. To restore weak connections and strokes of character images in a
degraded document, Chang et al. [25] have proposed an algorithm based on two
stages: the background noise elimination using gray-level histogram equalization
and enhancement of gray levels of characters over and above the surrounding
background using an edge image composition technique. The binary partitioning
is made according to a smoothed and equalized histogram information calcu-
lated in five different steps. To handle noises on different scales in a historical
document, multi-scale analysis [26, 27] is also a promising approach. Moghad-
dam et al. [28] have proposed a multi-scale binarization framework, which can
be used along with any adaptive threshold-based binarization method. But
most of these global and local thresholding methods suffer from the problem of
discontinuity in character strokes in the document, which pose serious problems
during character recognition. But as our basic objective is to use the bina-
rization method as a preprocessing step for an application related to character
recognition, it is very much desired to maintain the proper stroke connectivity
in character images.
To address the above problem, we propose an adaptive-interpolative image
binarization method for degraded document images. In this method, a multi-
scale framework is added to an adaptive version of Otsu’s method [4]. To convert
Otsu’s method to an adaptive model, instead of computing the global threshold
value for the whole image, we compute the local threshold value for each pixel
by observing the intensity behavior of its neighbor pixels. But this adaptive
3
  
method is computationally very expensive. To reduce the computational time,
we use a grid-based approach with a particular scale size. To handle noises
in different scales, we use this grid-based adaptive approach for different scale
values. After a post-processing meant for stroke preservation, we get the final
output. A preliminary version of this work has been published in International
Conference on Image Information Processing (ICIIP), 2011 [29].
Organization of this paper is done as follows. Section 2 give a brief discus-
sion about some related binarization methods. Section 3 describes the proposed
document image binarization methodology. A novel technique for maintain-
ing the connectivity of character strokes is explained in Section 4. Section 5
contains experimental results and comparison of test results with other exist-
ing binarization methods. Concluding remarks and future work are given in
Section 6.
2. Background study
In this section, we give a brief description of some traditional and contem-
porary binarization methods. Firstly, global thresholding by Otsu’s method [4]
is discussed. Then two adaptive methods, one by Niblack’s [10] and the other
by Sauvola’s [12] are discussed. Finally, Moghaddam’s method [28], which is
an adaptive version of Otsu’s method with a multi-scale framework, has been
briefed.
2.1. Otsu’s method [4]
It is a famous global thresholding method that is based on the analysis of
histogram for image thresholding. In this algorithm, the image is considered
to be having two classes of pixels, namely the foreground and the background.
The foreground and the background are denoted as 1 and 0 respectively. The
threshold is calculated so that the combined spread of the two classes or the
intra-class variance is minimal. As minimizing the intra-class variance is same
4
  
as maximizing the inter-class variance, we maximize the function given below.
σ2b (t) = ω0(t)ω1(t) (µ1(t)− µ0(t))2 (1)
where ωi is the probability of class i, µi is the mean of class i, and t is the
threshold value. The related procedure is as follows:
1. Compute the intensity histogram and hence find the intensity probabili-
ties.
2. Set an initial threshold t = t1 and repeat Steps 3 and 4 until it reaches a
final threshold t2.
3. Calculate class probabilities ωi and class means µi.
4. Compute and store σ2b (t).
5. Calculate the value of the t for which σ2b (t) is maximum. This is the
desired threshold, To.
6. For each pixel in the image, if the intensity value is less than To, then
assign it as a background pixel; otherwise, it is a foreground pixel.
Although it is a very fast method, it suffers from the fact that due to global
thresholding, images with localized noises cannot be effectively binarized. Fig-
ure 2(a) shows a result of this method for the sample image shown in Fig. 1(c).
2.2. Niblack’s method [10]
It is one of the pioneering adaptive thresholding methods used in document
image binarization. The method calculates a pixel-wise threshold by shifting a
rectangular window across the image. The threshold Tn(p) for the center pixel
p of the window in the input image I is computed using the function:
Tn(p) = µ(p) + kσ(p) (2)
5
  
where µ(p) and σ(p) are the respective mean and standard deviation of the in-
tensity in the sampling window centered at p, and k is a constant. The method is
designed with the assumption that black pixels and white pixels are represented
by 1 and 0 respectively. The parameter k controls the behavior of the method.
Any pixel p is labeled as a foreground pixel, if I(p) ≥ Tn(p). The parameter k
can have two different roles depending on the target degradation. Usually, for
document images suffering low-intensity degradation, a negative value of k is
used to allow the method to capture the weakened parts of the strokes. In con-
trast, for document images that suffer from interfering degradation, a positive
value of k helps in the removal of high-intensity degradation. Figure 2(b) shows
the performance of this method for a sample image (Fig. 1(c)) from DIBCO-
2009 [30] dataset.
2.3. Sauvola’s method [12]
It is based on another successful adaptive binarization approach similar to
Niblack’s method. The threshold function is defined as:
Ts(p) = µ(p)
(
1 + k
(
σ(p)
δ
− 1
))
(3)
where I, µ(p) and σ(p) are as in Niblack’s formula (2). δ is the dynamic range
of standard deviation, and the parameter k gets positive values. In this method,
δ = 128 with 8-bit gray-level images and k = 0.5 are used to obtain good re-
sults. This binarization formula is designed for gray-scale images in which black
pixels are represented by 0 and white pixels by 1. k is always positive in order
to capture singleton background distributions in small sampling windows; this
helps to achieve a clean background area far away from text strokes. Fig. 2(c)
shows the performance of Sauvola’s method for the input image in Fig. 1(c).
2.4. Moghaddam’s method [28]
In a multi-scale binarization framework, this method can be used along with
any adaptive threshold-based binarization method. This method uses an adap-
6
  
tive modification of Otsu’s method [4]. The framework requires several binariza-
tions on different scales, which is addressed by introduction of fast grid-based
models. This method is suitable for image binarization on degraded historical
documents, and it restores partially weak connections and strokes of character
images. Fig. 2(d) shows an example of image binarization result.
A tabular comparison among the above-described binarization methods and
the proposed method as per their different features is shown in Table 1. Figure 2
shows the output images for a sample image from DIBCO-2009 obtained by
these methods as well as our method.
3. Proposed method
The proposed method of document image binarization works in a four-fold
manner. It first employs an adaptive model of Otsu’s method [4] and then adopts
a grid-based strategy to reduce the computational burden. A multi-scale frame-
work is used over this to increase its effectivity and robustness against noise,
and finally a geometric post-processing is performed to address the connectivity
issues. Figure 3 shows the system architecture of the proposed method. The
algorithmic details of these four stages are explained below.
3.1. Adaptive formulation
There exist several adaptive thresholding models of binarization [10, 12, 28].
However, they are not specially designed for restoration of faint character images
in degraded documents. In our work, to convert Otsu’s method [4] to an adaptive
model, we compute the local threshold value for each pixel in a particular scale
size by observing the intensity behavior of its neighbor pixels. The threshold
used in our adaptive Otsu’s model is as follows.
Ta(p) = f
( |To − To(p)|
δ
− 1
)
(T ′o(p)− To(p)) + To(p) (4)
7
  
f(·) is the unit step function defined as
f(t− t0) = 1, if t ≥ t0
= 0, otherwise.
To = argmax
W
(
σ2W
σ2
)
(5)
To(p) = kargmax
W
(
σ2W (p)
σ2(p)
)
(6)
T ′o(p) = k
′argmax
W
(
σ′
2
W (p)
σ′2(p)
)
(7)
σ2 =
1
|I|
∑
(f(m,n)− Ī)2 (8)
σ2W =
1
|IW |
∑
(fW (m,n)− ¯IW )2 (9)
where, I is the input image, IW is the subimage of I with computational window
size W, |I| (|IW |) is the total number of pixels in I (IW ), Ī ( ¯IW ) is the mean
value, and f(m, n) (fW (m,n)) is the intensity value of the pixel coordinate (m,
n) belongs to I (IW ). Similarly, σ
2
W (p) (σ
′2
W (p)) and σ
2(p) (σ′
2
(p)) represent
the respective variances for the computational window W around pixel p in the
input image IW (I
′
W ) and the entire image, I (I
′). I ′ represents the inverse
image of I (i.e., I ′ = 1− I).
In equation 5, 6 and 7, we use multiple window size to calculate To, To(p),
and T ′o(p) respectively. In equation 6 and 7, k and k
′ are the threshold multipli-
ers, and T ′o(p) denotes To(p) at p for the inverse image I
′. The parameter δ is set
to 0.1 and is a measure of the deviation of the local threshold (To(p)) from the
global one (To). Like other adaptive methods, we use two parameters, namely
k and k′, as threshold multipliers for To(p) and T
′
o(p) respectively, which makes
the algorithm versatile for different types of images. When two bands (i.e., both
foreground and background) are present in the region of interest defined by W ,
To(p) is used as the threshold value; and when only background is present, T
′
o(p)
8
  
is considered. We use a higher value of k (= 2) to discard noises and a lower
value of k′ (= 0.6) to capture foreground with low contrast. In this proposed
method, parameters are set as per our own experimental analysis. We consid-
ered two standard datasets used in DIBCO competition for testing our method.
Apart from that we considered historical documents like George Washingtons
handwritten and old Bangla manuscripts. At the time of preparing dataset, we
took care of the versatility of dataset for degraded documents. We observed that
for all these test documents, the parameters used in our proposed method were
unchanged. This adaptive approach is computationally very expensive, but it
performs usually better than the original Otsu’s method for degraded images,
as found in our experimentation (Section 5).
3.2. Interpolation using grid-based approach
To reduce the computational time, we use a grid-based approach with a scale
size s and grid size gs, where gs = (s − 1)/2, and s is an odd positive integer
greater than 1. The underlying grid G is defined by a set of equi-spaced rows
and equi-spaced columns so that the distance between two consecutive rows
(columns) is gs. A grid point q(i, j) ∈ G is a point defined by the intersection of
ith row and jth column of G. The square region of I with the four grid points,
namely q1 := q(i, j), q2 := q(i+ 1, j), q3 := q(i+ 1, j + 1), and q4 := q(i, j + 1),
as vertices, is defined as the cell c(i, j), as shown in Fig. 4. We first compute
local threshold value Ta(qk) of each grid point qk(1 ≤ k ≤ 4) by considering the
computational window W of size gs × gs centered at qk, using Eqn. 4. Then
the threshold value for each other pixel p ∈ c(i, j) is computed by interpolating
the threshold values of {qk : 1 ≤ k ≤ 4}. The interpolated threshold at p is
estimated as the weighted mean of {Ta(qk) : 1 ≤ k ≤ 4}, where the weights
{dk : 1 ≤ k ≤ 4} are assigned as the reciprocals of corresponding Manhattan
9
  
distances of {qk : 1 ≤ k ≤ 4} from p. The threshold value T̃ (p) is then given by
T̃ (p) =
4∑
k=1
1
dk
Ta(qk)
4∑
k=1
1
dk
(10)
For each grid point q, the threshold T̃ (q) is assigned as Ta(q). Subsequently, the
binarized image Ib is obtained as follows (1 = foreground, 0 = background).
Ib(p) =

1 if

p ∈ G and I(p) ≥ Ta(p)
or
p ∈ I rG and I(p) ≥ T̃ (p)
0 if

p ∈ G and I(p) < Ta(p)
or
p ∈ I rG and I(p) < T̃ (p)
(11)
3.3. Multi-scale framework
To make our algorithm more effective and to handle noises at different scales,
we use the grid-based interpolation in a multi-scale framework. The steps are
as follows.
1. We start with a high scale value s(= min{wI , hI}), wI and hI being the
respective width and height of I; and continue up to s (set as s24 in our
experimentation). For each successive iteration, the scale is reduced by a
factor of 2, and the input of the new iteration is the output of the previous
iteration.
2. For each iteration, we adopt the threshold computation as used in the
grid-based approach and compute the threshold T̃ with the current scale
value.
3. For any pixel p,
• if I(p) > T̃ (p) + 0.2, then I(p) = 1.
10
  
• if I(p) < T̃ (p), then I(p) = 0.
• if T̃ (p) ≤ I(p) ≤ T̃ (p) + 0.2, then I(p) remains unchanged (i.e, p
remains unclassified). Here, 0 and 1 represent background and fore-
ground respectively.
4. Repeat until the scale value reaches s.
Finally, we get a binarized image Ib. If some pixel p remains unclassified
through all iterations, then it is classified as a background pixel (i.e., Ib(p) = 0).
We apply this multi-scale processing to handle noise at different scale. After the
final iteration, if any pixel is not classified then we consider it as noise and mark
them as background. This approach reduces the problem of character stroke
discontinuities for degraded documents.
3.4. Post-processing
We observed that due to very low contrast variation in between the back-
ground and the foreground of a degraded document, the binarized output often
suffers from unexpected discontinuity in the strokes of character images (Fig. 5).
Such discontinuities affect seriously on subsequent character recognition. To
overcome this problem and to make the proposed method more effective, we
apply a post-processing scheme on our binarized output. The main objective
is to maintain the connectivity among different strokes of alphabets in the text
document. The major steps are as follows.
1. Detect all the connected components in the binarized image Ib using
Rosenfeld and Kak method [31]. This is a raster-scan based method which
uses different predefined masks and equivalence table for assigning different
labels to the pixels belong to different connected components. The detailed
methodology is described in [32]. Now, sort them based on their sizes in
descending order and take the first two-third for further processing. This
value is set as per the experimental analysis. Let C = {C1, C2, . . . , Cn} be
this (ordered) set. Our objective is to maintain the desired connectivity
11
  
for large components. We neglect the last one-third components having
smaller size in order to avoid unwanted connectivity among the isolated
components, which are possibly placed singly in the document (e.g., the
dot above the character ‘i’ or ‘j’ in the English alphabet). These values
are set based on the measurement of frequency of appearance of these
type of elements in our test documents. The overall frequency of appear-
ance is near about 70-74%. Due to that reason we have neglected the last
one-third of connected components sorted in decensing order of their sizes.
2. Take each component Ci from the ordered set C and derive its boundary
pixels.
3. For each boundary pixel pi ∈ Ci, take a 3 × 3 window Wi centered at
pi on the original image I. This window size depends on the size of the
letters used in the text. Our experimental dataset contains documents
with different text sizes. As per our experimental analysis, we observed
that this 3×3 window size gives most promising results than other window
sizes. If there lies a pixel pj in Wi such that pj /∈ Ci and I(pj) > 12 T̃ (pi),
then pj is included in Ci. The process is repeated with a new window Wj
of size 3× 3 centered at pj on I to include more pixels, if any, in Ci.
Figure 5 shows a portion of the improved output (cropped and zoomed) after
post-processing. Figure 6 shows the improvement of stroke connectivity for the
entire image.
We have observed that the proposed post-processing technique increases
blurred regions in the resultant image. It looks like an image suffers from Di-
lation. But the Dilation operation depends on the structuring element. Due
to the changes of the shape of it the output becomes different. We applied
Dilation operation with a square shaped structuring element of size 3× 3. The
result (Fig. 7) is poor comparing with our proposed post-processing technique.
As our basic objective is to preserve the character stroke of binarized degraded
documents so that the performance of an OCR on degraded documents become
much more efficient.
12
  
4. Preservation of character stroke connectivity
We observe that after the post-processing, we get improved binarized result,
particularly for a degraded document. But the binarized result still suffers from
the discontinuity in the strokes of character images (Fig. 8(a)). This problem
arises due to a very low contrast in between the foreground and the background
of a document image. We have tested the results of other binarization methods
(reported in Section 2) to check their efficiency in preserving the proper connec-
tivity of character strokes. Figure 9 shows the binarization results of a cropped
image of George Washington’s handwriting (Fig. 5). It is clearly evident that
all these above said methods are often unable to maintain the proper stroke
connectivity. In this section, we propose a novel technique to solve this problem
by a geometric approach.
4.1. Contour extraction and straight line approximation
1. We take a subset Cs ∈ C, containing the connected components with size
≤= 10. This threshold value is empirically set by observing the nature of
input images. Our objective is to detect contours of small components and
make them connected with their basic parts. It is true that instead of set-
ting the threshold value empirically, if we could set it automatically, then
the proposed system would be fully automatic. But in our experimental
part, we have set few threshold values empirically because we cant measure
the stroke width or font size of an input image without the separation of
text from background. Here we have not done any segmentation on text.
Due to that reason, we have set the threshold value as per experimental
analysis.
2. For each connected component Ci ∈ Cs, obtain its contour image Cti using
the contour-tracing method by Rosenfeld’s crack following algorithm [31].
This contour coding technique is defined based on the walk along the edge
or crack between two neighbouring pixels. Then the direction of moves
is just 4 (north, east, south, and west). We call this code as crack code.
13
  
This is similar to chain code too in case of 4-connectivity. The detailed
methodology is described in [32].
3. To perform straight line approximation, we apply Bhowmick and Bhat-
tacharya’s method [33] on each contour image Cti to get an ordered set of
vertices V = {p1, p2, . . . , pn} and a set of edges, E = {e1, e2, . . . , en}, that
captures the structural shape of the contour image. The result of approxi-
mation for such contour image is shown in Fig. 8(c). This method reduces
the number of pixels without altering the structural shape of the object.
We consider only the ordered set of vertices describing the approximation
for subsequent steps.
4.2. Character stroke connectivity using geometric approach
We detect the pointedness property of the contour image by analyzing the
structure of its approximate polygon. A polygon is said to be pointed at a
vertex pi if the internal angle at pi is significantly small. The pointedness at pi
is measured by the area of the triangle pi−1pipi+1, as explained below.
1. We first detect the concavity/convexity of each vertex pi. We consider its
two adjacent vertices, pi−1 and pi+1 in V . We consider the triangle formed
by pi−1(xi−1, yi−1), pi(xi, yi), and pi+1(xi+1, yi+1) as vertices. Then twice
the signed area of this triangle [33] is
∆(pi−1, pi, pi+1) =
∣∣∣∣∣∣∣∣∣
1 1 1
xi−1 xi xi+1
yi−1 yi yi+1
∣∣∣∣∣∣∣∣∣ .
If ∆(·) yields a negative value, then the vertex pi has a concave property
and is marked as L. If the value is positive, then pi has a convex property
and is marked as R (Fig. 10).
2. After detecting the concavity/convexity of all the vertices in V , we get
a sequence of L and R, where L/R indicates the concavity/convexity
14
  
of a vertex. From this sequence, we consider only these vertices la-
beled by R. These vertices may have the pointedness property. Now,
if ∆(pi−1, pi, pi+1) ≤ 0.4 (threshold value used in our experiments), then
we extend the vertex pi along the path with directional angle given by the
mean of the directions of the two vectors pi−1pi and pi+1pi. The vertex pi
is extended within a specified length until it meets any of the foreground
pixels in Ib. We mark all the pixels belonging to that path as foreground.
To make the extended stroke thick, we use a 3× 3 window Wj , placing pj
at the center, and mark all the pixels in Wj as foreground in the binarized
image Ib.
3. We repeat the above steps for all the remaining approximated contour
images to get the proper connectivity in strokes of character images in
Ib. Figure 8(d) shows the improved result after the proper connectivity of
strokes.
Figure 11 shows an example of improved binarized result of Figure 6 after pre-
serving character stroke connectivity.
In our proposed method we have used several parameters and constants.
The overall list of these parameters is given in Table 2.
5. Experimental results and discussion
5.1. Experimental dataset
The proposed document image binarization method has been tested on the
samples taken from the George Washington corpus of handwritten document
images at the Library of Congress [34], handwritten manuscripts of classical
Bengali literature etc. All these samples suffer from poor clarity due to intense
background noise that makes any binarization method difficult to separate the
desired foreground from the background, simultaneously maintaining the proper
connectivity of character strokes in the document image. We have also con-
sidered two other benchmark datasets, named DIBCO-2009 [30] and DIBCO-
2010 [35], used in the document image binarization contests held in ICDAR
15
  
2009 and in ICFHR 2010 respectively. As our main focus is on character stroke
preservation, so we have considered the images having faint characters. We have
mentioned these two new datasets as FDIBCO-2009 and FDIBCO-2010. A few
samples are already shown in Fig. 1.
5.2. Binarization results
We have implemented the proposed method and the four other methods
in C (Intel Core 2 Duo CPU E4500 2.20 GHz, 1GB RAM, Fedora 10, Linux
2008). Figure 12 and 13 show few samples of experimental results of George
Washington’s writing and Bengali text document respectively. Binarization of
Bangla handwritten degraded documents is shown in Fig. 14. We have shown
few sample results of FDIBCO-2009 and FDIBCO-2010 datasets in Fig. 15. We
observe that our method performs well for these types of degraded historical
images and maintain the proper connectivity in different strokes of alphabets in
these images.
5.3. Performance evaluation
The performance evaluation has been done using different evaluation mea-
surement techniques, such as Recall, Precision, F-Measure, Peak Signal-to-Noise
Ratio (PSNR), Negative Rate Metric (NRM), and Misclassification Penalty Met-
ric (MPM). These are the measures used in various evaluation models of docu-
ment image binarization, including ICDAR 2009 document image binarization
contest (DIBCO) [36].
F-Measure: It is defined as
FM =
2× Recall× Precision
Recall + Precision
(12)
where, Recall =
TP
TP + FN
and Precision =
TP
TP + FP
.
TP, FP, and FN denote the respective counts of true positive, false positive, and
false negative pixels required to compute Recall and Precision metrics. Recall
16
  
and Precision metrics have values lying in [0, 1]. As these metrics approach 1,
the results get better. A higher value of FM indicates the efficiency of correct
binarization.
PSNR: It is defined as
PSNR = 10 log
(
c2
MSE
)
(13)
where, MSE is the mean square error and c (= 1) is a constant. Higher value
of PSNR signifies how well an algorithm can retrieve the desire pixels.
NRM: It is defined as
NRM =
FN
FN+TP +
FP
FP+TN
2
(14)
where, TP, FP, TN, and FN represent the number of true positives, false pos-
itives, true negatives, and false negatives respectively. NRM measures pixel
mismatch rate between the ground-truth image and the actual result produced
by an algorithm. A lower value of NRM indicates the better performance of an
algorithm.
MPM: It is defined as
MPM =
∑FN
i=1 d
i
FN +
∑FP
j=1 d
j
FP
2D
(15)
where, diFN and d
j
FP represent the respective distances of the ith false negative
pixel and the jth false positive pixel from the contour of the ground-truth seg-
mentation. The normalization factor D is the sum over all the pixel-to-contour
distances of the ground-truth object. The lower value of metric MPM indicates
how well the output image represents the contour of ground-truth image.
17
  
5.4. Comparison with other methods
The proposed method has been compared with other four well-known meth-
ods, namely Otsu’s [4], Niblack’s [10], Sauvola’s [12], and Moghaddam’s [28]
methods. Otsu’s is a global thresholding method, Niblack’s and Sauvola’s are
adaptive methods, and Moghaddam’s is an adaptive version of Otsu’s method
with a multi-scale framework. The parameters of the thresholding methods such
as the window size, the weights of local mean, standard variation, and dynamic
range of standard variation are all set according to the recommendations within
the reported papers [4, 10, 12, 28]. Figures 16 and 17 show output-wise compar-
ison among these existing binarization methods vis-a-vis our proposed method
on George Washington’s handwritten document (a sample shown in Fig. 1(a))
and Bengali literature. The main drawback of the existing methods is that small
variations on the background region where there is no text in the vicinity, can
pass through these methods as undetected. But our proposed method has the
ability to recover weak and low-intensity parts of the character strokes, and thus
perform well over the other methods by maintaining the proper connectivity in
the strokes of character images in a degraded document.
The performance evaluation among these four methods and our proposed
method has been done using FDIBCO-2009 and FDIBCO-2010 datasets, as
mentioned earlier. To make it more generalized, we divided the whole compar-
ative analysis into two parts. The first one is based on without applying stroke
preservation techniques and the second one is based on with applying stroke
preservation techniques on our proposed method and other existing methods.
The average evaluation results for all test images in these two datasets are shown
in Table 3 and Table 4 respectively. It may be seen from these tables that the
proposed method achieves highest scores in F-Measure, PSNR, NRM, and MPM
for FDIBCO-2009 and FDIBCO-2010 datasets when we applied our proposed
stroke-preserving techniques as shown in Table 4. This shows that our pro-
posed method produces a higher overall precision and preserves the text strokes
connectivity better, for degraded document images. In addition, Figs. 18, 19,
20, and 21 provide a visual comparison of the binarization results of the four
18
  
samples, taken from the FDIBCO-2009 and FDIBCO-2010 datasets, using the
five binarization methods. As evident from these figures, the proposed method
extracts the text properly and maintains the proper stroke connectivity of the
document images that suffer from different types of document degradation.
A comparison of average computational time taken by these methods is
shown in Table 5. We have taken FDIBCO-2009 and FDIBCO-2010 datasets
and two document images from George Washington’s writing and Bengali litera-
ture. It is noticed that Otsu’s method is computationally inexpensive. Further,
as it is based on global thresholding, it shows a poor performance.
5.5. Discussion
The proposed method, we have used an efficient binarization function, which
is an adaptive version of Otsu’s method. We have formulated the function in
such a manner that it can adapt with a high amount of background noise and can
maintain the proper connectivity of character strokes by recovering weak and
low-intensity parts of the strokes. Like other adaptive methods, we have used
two threshold multipliers (k and k′), discussed in Section 3.1, for handling low
contrast between the foreground and the background. To improve the execution
speed, we have used the concept of grid-based computation (Section 3.2). The
thresholding of the pixels within a grid-cell is done using Manhattan distance-
based interpolation technique, as described in Section 3.2. To detect noises
at different scales, we have combined the adaptive method with a multi-scale
framework (Section 3.3).
The superiority in performance of the proposed method is achieved by the
preservation of character stroke connectivity techniques as described in Sec-
tion 3.4 and Section 4. Due to a very low contrast between the foreground
and the background, the proposed method sometimes fails to properly separate
the foreground from the background. We have tested the performance of the
proposed method before and after applying these stroke-preserving techniques
using the images and their corresponding ground truths from FDIBCO-2009 and
FDIBCO-2010 datasets. We have seen that the performance has been improved
19
  
with respect to different evaluation measurements discussed in Section 5.3. It
is also observed that our proposed method performs reasonably better than
other existing methods (except Otsu and Moghaddam) without applying any
stroke-preserving techniques as shown in Table 3. But after adding the proposed
stroke-preserving techniques, the performance of our method has been improved
significantly comparing with other existing method as shown in Table 4. This
observation concludes the efficacy of the proposed method comparing with other
existing methods in terms of preserving characters strokes of faint characters in
degraded documents. This effectiveness can help us to use this proposed bina-
rization method as an efficient preprocessing strategy for any OCR system.
6. Conclusion
We have proposed a novel adaptive-interpolative binarization method for de-
graded document images. The proposed method is an adaptive version of Otsu’s
method and can be used in different scales to handle different types of noises. Ex-
periments are done on different degraded document images from George Wash-
ington corpus of handwritten documents and Bengali literature. An evaluation
study among different well-known binarization methods on FDIBCO-2009 and
FDIBCO-2010 datasets with their ground truths shows that our method is at par
or better with the existing methods. In future, we would use this binarization as
a part of an optical character recognition (OCR) system for degraded documents
of historical importance.
References
[1] M. Sezgin, B. Sankur, Survey over image thresholding techniques and quan-
titative performance evaluation, Journal of Electronic Imaging 13 (1) (2004)
146–165.
[2] O. D. Trier, A. K. Jain, Goal-directed evaluation of binarization methods,
IEEE Transactions on Pattern Analysis and Machine Intelligence 17 (12)
(1995) 1191–1201.
20
  
[3] A. Brink, Thresholding of digital images using two-dimensional entropies,
Pattern Recognition 25 (8) (1992) 803–808.
[4] N. Otsu, A threshold selection method from gray-level histogram, IEEE
Transactions on Systems, Man and Cybernetics 9 (1979) 62–66.
[5] Y. Solihin, C. Leedham, Integral ratio: A new class of global thresholding
techniques for handwriting images, IEEE Transactions on Pattern Analysis
and Machine Intelligence 21 (1999) 761–768.
[6] O. Trier, T. Taxt, Evaluation of binarization methods for document images,
IEEE Transactions on Pattern Analysis and Machine Intelligence 17 (1995)
312–315.
[7] J. Bernsen, Dynamic thresholding of gray-level images, in: International
Conference on Pattern Recognition, 1986, pp. 1251–1255.
[8] D. Bradly, G. Roth, Adaptive thresholding using the integral image, Jour-
nal of Graphics Tools 12 (2) (2007) 13–21.
[9] Q. Huang, W. Gao, W. Cai, Thresholding technique with adaptive window
selection for uneven lighting image, Pattern Recognition Letters 26 (2005)
801–808.
[10] W. Niblack, An Introduction to Image Processing, Prentice-Hall, Engle-
wood Cliffs, NJ, 1986.
[11] J. R. Parker, Gray level thresholding in badly illuminated images, IEEE
Transactions on Pattern Analysis and Machine Intelligence 13 (8) (1991)
813–819.
[12] J. Sauvola, M. Pietikainen, Adaptive document image binarization, Pattern
Recognition 33 (2000) 225–236.
[13] M. Valizadeh, E. Kabir, An adaptive water flow model for binarization of
degraded document images, International Journal on Document Analysis
and Recognition 16 (2) (2013) 165–176.
21
  
[14] J. M. White, G. D. Rohrer, Image thresholding for optical character recog-
nition and other applications requiring character image extraction, IBM
Journal of Research and Development 27 (4) (1983) 400–411.
[15] J. D. Yang, Y. S. Chen, W. H. Hsu, Adaptive thresholding algorithm and its
hardware implementation, Pattern Recognition Letters 15 (1994) 141–150.
[16] Y. Yang, H. Yan, An adaptive logical method for binarization of degraded
document images, Pattern Recognition 33 (5) (2000) 787–807.
[17] J. He, Q. D. M. Do, A. C. Downton, J. H. Kim, A comparison of binariza-
tion methods for historical archive documents, in: International Conference
on Document Image Analysis and Recognition, 2005, pp. 538–542.
[18] P. Stathis, E. Kavallieratou, N. Papamarkos, An evaluation survey of bina-
rization algorithms on historical documents, in: International Conference
on Pattern Recognition, 2008, pp. 1–4.
[19] G. Leedham, S. Verma, A. Patankar, V. Govindaraju, Separating text and
background in degraded document images, in: International Workshop on
Frontiers of Handwriting Recognition, 2002, pp. 244–249.
[20] B. Gatos, I. Pratikakis, S. J. Perantonis, An adaptive binarization technique
for low quality historical documents, in: IAPR International Workshop on
Document Analysis Systems, 2004, pp. 102–113.
[21] B. Gatos, I. Pratikakis, S. J. Perantonis, Adaptive degraded document
image binarization, Pattern Recognition 39 (2006) 317–327.
[22] E. Kavallieratou, E. Stamatatos, Improving the quality of degraded doc-
ument images, in: International Conference on Document Image Analysis
for Libraries, 2006, pp. 340–349.
[23] S. Lu, B. Su, C. L. Tan, Document image binarization using background
estimation and stroke edges, International Journal on Document Analysis
and Recognition 13 (2010) 303–314.
22
  
[24] S. A. Tabatabaei, M. Bohlool, A novel method for binarization of badly
illuminated document images, in: International Conference on Image Pro-
cessing, 2010, pp. 3573–3576.
[25] M. Chang, S. Kang, W. Rho, H. Kim, D. Kim, Improved binarization
algorithm for document image by histogram and edge detection, in: Inter-
national Conference on Document Image Analysis and Recognition, 1995,
pp. 636–643.
[26] K. Berkner, M. J. Gormish, E. L. Schwartz, Multiscale sharpening and
smoothing in Besov spaces with applications to image enhancement, Ap-
plied and Computational Harmonic Analysis 11 (1) (2001) 2–31.
[27] M. Cheriet, Extraction of handwritten data from noisy gray-level images
using a multiscale approach, International Journal of Pattern Recognition
and Artificial Intelligence 13 (5) (1999) 665–684.
[28] R. F. Moghaddam, M. Cheriet, A multi-scale framework for adaptive bi-
narization of degraded document images, Pattern Recognition 42 (2010)
2186–2198.
[29] S. Bag, P. Bhowmick, P. Behera, G. Harit, Robust binarization of degraded
documents using adaptive-cum-interpolative thresholding in a multi-scale
framework, in: International Conference on Image Information Processing,
2011, pp. 1–6.
[30] [link].
URL http://www.iit.demokritos.gr/~bgat/DIBCO2009/benchmark/
[31] A. Rosenfeld, A. Kak, Digital Picture Processing, Academic Press, New
York, San Francisco, London, 1982.
[32] B. Chanda, D. Dutta Majumder, Digital Image Processing and Analysis,
Vol. 2nd, PHI, New Delhi, 2011.
23
  
[33] P. Bhowmick, B. B. Bhattacharya, Fast polygonal approximation of digital
curves using relaxed straightness properties, IEEE Transactions on Pattern
Analysis and Machine Intelligence 29 (9) (2007) 1590–1602.
[34] [link].
URL http://memory.loc.gov/
[35] [link].
URL http://users.iit.demokritos.gr/~bgat/H-DIBCO2010/
benchmark/
[36] B. Gatos, K. Ntirogiannis, I. Pratikakis, Icdar 2009 document image bina-
rization contest (dibco 2009), in: International Conference on Document
Analysis and Recognition, 2009, pp. 1375–1382.
24
  (a) (b)
(c) (d)
Figure 1: Examples of degraded document images on which our algorithm has been tested.
(a) George Washington’s writing [34]; (b) Bengali literature; (c) DIBCO-2009 dataset [30];
(d) DIBCO-2010 dataset [35].
(a) (b)
(c) (d)
(e)
Figure 2: Binarization results for the sample image in Fig. 1(c) by (a) Otsu’s method;
(b) Niblack’s method; (c) Sauvola’s method; (d) Moghaddam’s method; (e) Proposed method.
25
  
interpolation
framework
Preservation of
character stroke
connectivity
Binarized
document
Input
document
Adaptive
Multi-scale
using grid-based
approachformulation
Figure 3: System architecture of the proposed method.
p
q1
q2 q3
q4i
i+ 1
j j + 1
gs
gs
Figure 4: Interpolation of the threshold value at p in a cell ci based on thresholds at the four
grid points (q1, q2, q3, q4) defining ci.
26
  
Figure 5: Improvement of test result using post-processing. Top: Input sample (cropped).
Mid: Disconnected strokes. Bottom: Improved result after post-processing.
27
  
Figure 6: Improvement of test result using post-processing. Left: Binarized sample after grid-
based interpolation in the multi-scale framework. Right: Improved result after post-processing.
Notice that disconnected strokes have regained their connectivities after post-processing.
28
  
Figure 7: Effect of Dilation operation. Left: Binarized sample after grid-based interpolation
in the multi-scale framework. Right: Dilated image using 3 × 3 square structuring element.
Notice that the result is poor comparing with our proposed postprocessing technique.
29
  
(a)
(b)
(c)
(d)
Figure 8: Improvement on the binarized result by preserving stroke connectivity. Input image
is the one shown in Fig. 5. (a) Binarized output with post-processing (cropped); (b) Contour
of the small component; (c) Straight line approximation; (d) Improved result.
30
  
(a)
(b)
(c)
(d)
Figure 9: Inefficiency of the following binarization methods to preserve character strokes con-
nectivity (encircled in red). (a) Otsu’s method; (b) Niblack’s method; (c) Sauvola’s method;
(d) Moghaddam’s method.
pi−1
pi
pi+1
L
pi−1
pi
pi+1
R
Figure 10: Detection of concavity and convexity of a vertex with respect to its neighbor
vertices. Left: Concave (L = left turn). Right: Convex (R = right turn).
31
  
Figure 11: Improvement of test result after preserving character stroke connectivity for Fig. 6.
Notice that the improved words are highlighted by red color.
32
  
Figure 12: Test results by the proposed method on George Washington’s writing. Left: Input
image. Right: Binarized image.
33
  
Figure 13: Test results by the proposed method on Bengali document images. Left: Input
image. Right:Binarized image.
34
  
Figure 14: Test results by the proposed method on Bangla handwritten degraded documents.
Left: Input image. Right:Binarized image.
35
  
Figure 15: Test results by the proposed method on few samples from FDIBCO-2009 and
FDIBCO-2010 datasets. Left: Input image. Right: Binarized image.
36
  
(a) (b) (c)
(d) (e) (f)
Figure 16: Comparison among different binarization methods on a document taken from
George Washington’s handwritings. (a) Input image; (b) Otsu’s method; (c) Niblack’s method;
(d) Sauvola’s method; (e) Moghaddam’s method; (f) Proposed method.
37
  
(a) (b) (c)
(d) (e) (f)
Figure 17: Comparison among different binarization methods on a document taken from
Bengali literature. (a) Input image; (b) Otsu’s method; (c) Niblack’s method; (d) Sauvola’s
method; (e) Moghaddam’s method; (f) Proposed method.
38
  
(a) (b)
(c) (d)
(e) (f)
Figure 18: Comparison among different binarization methods on a sample image
from FDIBCO-2009 dataset. (a) Input image; (b) Otsu’s method; (c) Niblack’s method;
(d) Sauvola’s method; (e) Moghaddam’s method; (f) Proposed method.
39
  
(a) (b)
(c) (d)
(e) (f)
Figure 19: Comparison among different binarization methods on a sample image
from FDIBCO-2009 dataset. (a) Input image; (b) Otsu’s method; (c) Niblack’s method;
(d) Sauvola’s method; (e) Moghaddam’s method; (f) Proposed method.
40
  
(a) (b)
(c) (d)
(e) (f)
Figure 20: Comparison among different binarization methods on a sample image
from FDIBCO-2010 dataset. (a) Input image; (b) Otsu’s method; (c) Niblack’s method;
(d) Sauvola’s method; (e) Moghaddam’s method; (f) Proposed method.
41
  
(a) (b)
(c) (d)
(e) (f)
Figure 21: Comparison among different binarization methods on a sample image
from FDIBCO-2010 dataset. (a) Input image; (b) Otsu’s method; (c) Niblack’s method;
(d) Sauvola’s method; (e) Moghaddam’s method; (f) Proposed method.
42
  
T
a
b
le
1
:
C
o
m
p
a
ri
so
n
a
m
o
n
g
th
e
ex
is
ti
n
g
b
in
a
ri
za
ti
o
n
m
et
h
o
d
s
a
n
d
th
e
p
ro
p
o
se
d
m
et
h
o
d
a
s
p
er
d
iff
er
en
t
fe
a
tu
re
s.
M
et
h
o
d
In
p
u
t
A
d
a
p
ti
ve
G
ri
d
-b
a
se
d
In
te
rp
o
la
ti
o
n
M
u
lt
i-
sc
a
le
P
re
se
rv
in
g
p
at
te
rn
a
p
p
ro
a
ch
a
p
p
ro
a
ch
te
ch
n
iq
u
e
fr
a
m
ew
o
rk
st
ro
ke
co
n
n
ec
ti
v
it
y
O
ts
u
’s
P
ic
tu
re
d
o
cu
m
en
ts
N
o
N
o
N
o
N
o
N
o
N
ib
la
ck
’s
N
or
m
al
d
o
cu
m
en
ts
Y
es
N
o
N
o
N
o
N
o
S
au
vo
la
’s
D
eg
ra
d
ed
d
o
cu
m
en
ts
Y
es
N
o
N
o
N
o
N
o
M
og
h
ad
d
am
’s
D
eg
ra
d
ed
h
is
to
ri
ca
l
Y
es
(O
n
e
th
re
sh
o
ld
Y
es
Y
es
(T
ec
h
n
iq
u
e
is
Y
es
P
a
rt
ia
ll
y
d
o
cu
m
en
ts
in
E
n
gl
is
h
m
u
lt
ip
li
er
is
u
se
d
)
n
o
t
m
en
ti
o
n
ed
)
P
ro
p
os
ed
D
eg
ra
d
ed
h
is
to
ri
ca
l
Y
es
(T
w
o
th
re
sh
o
ld
Y
es
Y
es
(M
a
n
h
a
tt
a
n
Y
es
Y
es
d
o
cu
m
en
ts
in
E
n
gl
is
h
m
u
lt
ip
li
er
s
a
re
u
se
d
;
(S
ec
.
3
.2
)
d
is
ta
n
ce
-b
a
se
d
(S
ec
.
3
.3
)
(S
ec
.
3
.4
an
d
B
en
ga
li
S
ec
.
3
.1
)
te
ch
n
iq
u
e;
S
ec
.
3
.2
)
&
4
)
43
  
Table 2: List of parameters used in the proposed method.
Parameter Description Appeared in Value
ωi Probability of class i Eq. No. 1 –
µi Mean of class i Eq. No. 1 –
δ dynamic range of standard deviation Eq. No. 3 128
k Constant Eq. No. 3 0.5
f(·) Unit step function Eq. No. 4 –
δ Constant Eq. No. 4 0.1
σ2W Variance for the computational window W Eq. No. 5 –
σ2W (p) Variance for the computational window W around pixel p Eq. No. 6 –
k Threshold multiplier Eq. No. 6 –
k′ Threshold multiplier Eq. No. 7 –
I Input image Eq. No. 8 –
I ′ Inverse image of I Eq. No. 8 –
fW (m,n) Intensity value of the pixel coordinate (m, n) belongs to I Eq. No. 8 –
fW (m,n) Intensity value of the pixel coordinate (m, n) belongs to IW Eq. No. 9 –
dk Weights Eq. No. 10 –
G Grid Eq. No. 11 –
s Scale value Section 3.3 min{wI , hI}
s Scale value Section 3.3 s24
C Set of connecte componenets Section 3.4 –
Cti Contour image of connected componenet Ci Section 4.1 –
∆(·) Signed area of a triangle Section 4.2 –
44
  
Table 3: Evaluation measurements among different binarization methods without applying
stroke preservation techniques.
Method Measurement FDIBCO- FDIBCO-
techniques 2009 2010
F-Measure (%) 63.36 34.90
Otsu’s PSNR 12.48 15.45
NRM 0.07 0.60
MPM 0.60 0.15
F-Measure (%) 44.35 31.22
Niblack’s PSNR 6.20 4.24
NRM 0.16 0.15
MPM 0.42 0.58
F-Measure (%) 61.24 38.35
Sauvola’s PSNR 10.16 15.70
NRM 0.42 0.55
MPM 0.23 0.48
F-Measure (%) 73.37 67.39
Moghaddam’s PSNR 11.79 13.39
NRM 0.22 0.22
MPM 0.11 0.12
F-Measure (%) 72.16 69.28
Proposed PSNR 10.47 09.85
NRM 0.14 0.37
MPM 0.17 0.25
45
  
Table 4: Evaluation measurements among different binarization methods with applying stroke
preservation techniques.
Method Measurement FDIBCO- FDIBCO-
techniques 2009 2010
F-Measure (%) 78.28 67.56
Otsu’s PSNR 13.10 15.83
NRM 0.06 0.32
MPM 0.45 0.13
F-Measure (%) 58.55 40.62
Niblack’s PSNR 9.58 6.14
NRM 0.13 0.13
MPM 0.34 0.43
F-Measure (%) 75.78 49.13
Sauvola’s PSNR 12.24 16.10
NRM 0.38 0.40
MPM 0.17 0.39
F-Measure (%) 89.12 89.35
Moghaddam’s PSNR 12.88 14.78
NRM 0.14 0.14
MPM 0.10 0.11
F-Measure (%) 98.51 94.14
Proposed PSNR 14.73 16.49
NRM 0.06 0.11
MPM 0.08 0.10
Table 5: Average computational time (in second) taken by different binarization methods
with applying stroke preservation techniques.
Method FDIBCO- FDIBCO- G-W- B-L-
2009 2010 1.jpg 1.jpg
Otsu’s 0.04 0.07 0.39 0.31
Niblack’s 21.90 25.29 37.70 29.09
Sauvola’s 17.76 22.01 21.39 15.46
Moghaddam’s 4.95 7.60 28.95 19.65
Proposed 1.75 1.99 18.53 12.46
46
  
Highlights 
 A novel technique of binarization with stroke preservation for faint characters.  
 An adaptive-interpolative thresholding technique in a multi-scale framework.  
 Exhaustive experimentation has been carried out with benchmark datasets. 
 The proposed method is found to be robust and better than other existing methods. 
