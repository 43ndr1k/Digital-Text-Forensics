47
Syntax versus Semantics:
Analysis of Enriched Vector Space Models
Benno Stein1 and Sven Meyer zu Eissen1 and Martin Potthast2
Abstract. This paper presents a robust method for the construction
of collection-specific document models. These document models are
variants of the well-known vector space model, which relies on a pro-
cess of selecting, modifying, and weighting index terms with respect
to a given document collection. We improve the step of index term
selection by applying statistical methods for concept identification.
This approach is particularly suited for post-retrieval categorization
and retrieval tasks in closed collections, which is typical for intranet
search.
We compare our approach to “enriched” vector-space-based doc-
ument models that employ knowledge of the underlying language in
the form of external semantic concepts. Primary objective is to quan-
tify the impact of a purely syntactic analysis in contrast to a semantic
enrichment in the index construction step. As a by-product we pro-
vide an efficient and language-independent means for vector space
model construction, whereas the resulting document models perform
better than the standard vector space model.
Keywords vector space model, concept identification, semantic con-
cepts, text categorization, evaluation measures
1 INTRODUCTION
Each text retrieval task that is automated by a computer relies on
some kind of document model, which is an abstraction of the origi-
nal document d. The document model must be tailored well with re-
spect to the retrieval task in question: It determines the quality of the
analysis, and—diametrically opposed—its computational complex-
ity. Though its obvious simplicity the vector space model has shown
great success in many text retrieval tasks [11; 12; 16; 15], and, the
analysis of this paper uses this model as its starting point.
The standard vector space model abstracts a document d toward a
vector d of weighted index terms. Each term t that is included in d
derives from a term τ ∈ d by affix removal, which is necessary to
map morphological variants of τ onto the same stem t. The respec-
tive term weights in d account for the different discriminative power
of the original terms in d and are computed according to some fre-
quency scheme. The main application of the vector space model is
document similarity computation.
In this paper we focus on the index construction step and, in par-
ticular, on index term selection. Other concepts of the vector space
model, such as the term weighting scheme or its disregard of word
order are adopted.
1 Faculty of Media, Media Systems.
Bauhaus University Weimar, 99421 Weimar, Germany
{benno.stein | sven.meyer-zu-eissen}@medien.uni-weimar.de
2 Faculty of Computer Science.
Paderborn University, 33098 Paderborn, Germany
1.1 A Note on Semantics
We classify an index construction method as being semantic if it re-
lies on additional domain knowledge, or if it exploits external in-
formation sources by means of some inference procedure, or both.
Short documents may be similar to each other from the (semantic)
viewpoint of a human reader, while the related instances of the vec-
tor space model do not reflect this fact because of the different words
used. Index term enrichment can account for this by adding synony-
mous terms, hypernyms, hyponyms, or co-occurring terms [7].
Semantic approaches are oriented at the human understanding of
language and text, and, as given in the case of ontological index term
enrichment, they are computationally efficient. However, the appli-
cation of the semantic approaches is problematic, if, for instance,
the document language is unknown or if a document combines pas-
sages from several languages. Moreover, there are situations where
semantic approaches can even impair the retrieval quality: Consider
a document collection with specialized texts, then ontological index
term enrichment will move the specific character of a text toward
a more general understanding. As a consequence, the similarity of
highly specialized text is diluted in favor of less specialized text—
which compares to the effect of adding noise.
1.2 Contributions
We investigate variants of the vector space model with respect to
their classification performance. Starting point is the standard vector
space model where the step of index term selection is improved by a
syntactic approach for concept identification; the resulting model is
compared to semantically enriched vector space models. The syntac-
tic concept identification approach is based on a collection-specific
suffix tree analysis. In a nutshell, the paper’s underlying question
may be summarized as follows:
Can syntactically determined concepts keep up with a
semantically motivated index term enrichment?
To answer this question we have set up a number of text catego-
rization experiments with different clustering algorithms. Since these
algorithms are susceptible to various side effects, we will also present
results that rely on an objective similarity assessment statistic: the
measure of expected density, ρ. Perhaps the most interesting result
may be anticipated: The positive effect of semantic index term en-
richment, which has been reported by some authors in the past, could
hardly be observed in our comprehensive analysis.
The remainder of the paper is organized as follows. Section 2
presents a taxonomy of index construction methods and outlines
commonly used technology, and Section 3 reports on similarity anal-
ysis and unsupervised classification experiments.
48
2 INDEX CONSTRUCTION
FOR DOCUMENT MODELS
This section organizes the current practice of index construction for
vector space models. In particular, we review the concept of a doc-
ument model and propose a classification scheme for both popular
and specialized index construction principles.
A document d can be viewed under different aspects: layout, struc-
tural or logical setup, and semantics. A computer representation d of
d may capture different portions of theses aspects. Note that d is
designed purposefully, with respect to the structure of a formalized
query, q, and also with having a particular retrieval model in mind. A
retrieval model, R, provides the linguistic rationale for the model for-
mation process behind the mapping d 7→ d. This mapping involves
an inevitable simplification of d that should be
1. quantifiable,
2. useful with respect to the information need, and
3. tailored to q, the formalized query.
The retrieval model R gives answers to these points, be it theo-
retically or empirically, and provides a concrete means, ρ(q,d), for
quantifying the relevance between a formalized query q and a docu-
ment’s computer representation d. Note that ρ(q,d) is often speci-
fied in the form of a similarity measure ϕ.
Together, the computer representation d along with the underlying
retrieval model R form the document model; Figure 2 illustrates the
connections.
Let D be a document collection and let T be the set of all terms
that occur in D. The vector space model d of a document d is a vec-
tor of |T | weights, each of which quantifying the “importance” of
some index term in T with respect to d.3 This quantification must
be seen against the background that one is interested in a similarity
function ϕ that maps from the vectors d1 and d2 of two documents
d1, d2 into the interval [0; 1] and that has the following property:
If ϕ(d1,d2) is close to 1 then the documents d1 and d2 are similar;
likewise, a value close to zero indicates a high dissimilarity. Note that
document models and similarity functions determine each other: The
vector space model and its variants are amenable to the cosine simi-
larity (= normalized dot product) in first place, but can also be used
in connection with Euclidean distance, overlap measures, or other
distance concepts.
Under the vector space paradigm the document model construc-
tion process is determined in two dimensions: index construction and
weight computation. In the following we will concentrate on the for-
mer dimension since this paper contributes right here. We have clas-
3 Note that, in effect, the vector space model is a computer representation
of a the textual content of a document. However, in the literature the term
“vector space model” is also understood as a retrieval model with a certain
kind of relevance computation.
Document model
Real-world
document
Formalized
query
Information
need
Layout
view
Conceptual
model
Semantic
view
Structural /
logical view
q ∈Qd ∈D
Computer
representation
of document
d ∈D
q ∈Q
Relevance
computation
Retrieval model R
ρ(q,d)
Linguistic
theory
Figure 2. In the end, an information need q is satisfied by a real-world doc-
ument d. A computer support for this retrieval task requires an abstraction
of q and d towards q and d. The rationale for this abstraction comes from a
linguistic theory and is operationalized by a retrieval model R.
sified the index construction principles for vector space models in
four main classes, which are shown in Figure 1.
Index Term Selection. Selection methods further divide into inclu-
sion and exclusion methods. An important exclusion method is stop-
word removal: Common words, such as prepositions or conjunctions,
introduce noise and provide no discriminating similarity information;
they are usually discarded from the index set. However, there are spe-
cial purpose models (e. g. for text genre identification) that rely on
stopword features [13; 9].
The standard vector space model does not apply an inclusion
method but simply takes the entire set T without stopwords. More
advanced vector space models use also n-grams, i. e., continuous se-
quences of n words, n ≤ 4, which occur in the documents of D.
Since the usage of n-grams entails the risk of introducing noise, not
all n-grams should be added but threshold-based selection methods
be applied, which rely on the information gain or a similar statis-
tic [6].
Index Term Modification. Most term modification methods aim at
generalization. A common problem in this connection is the map-
ping of morphologically different words that embody the same con-
cept onto the same index term. So-called stemming algorithms apply
here; their goal is to find canonical forms for inflected or derived
words, e. g. for declined nouns or conjugated verbs. Since the “unifi-
cation” of words with respect to gender, number, time, and case is a
language-specific issue, rule-based stemming algorithms require the
development of specialized rule sets for each language. Recall that
Index construction principle
Index term selection
Index term modification
Index term enrichment
Index transformation
Stemming
Example for technology:
Co-occurrence analysis
Addition of synonym sets
Singular value decomposition
Inclusion methods
Exclusion methods Stopword removal
Figure 1. A taxonomy of index construction principles for vector space models.
49
the application of language-specific rule sets requires the problem of
language detection both in unilingual and multilingual documents to
be solved.
Index Term Enrichment. We classify a method as term enrich-
ing, if it introduces terms not found in T . By nature, meaning-
ful index term enrichment must be semantically motivated and ex-
ploit linguistic knowledge. A standard approach is the—possibly
transitive—extension of T by synonyms, hypernyms, hyponyms, and
co-occurring terms. The extension shall alleviate the problem of dif-
ferent writing styles, or of vocabulary variations observed in very
small document snippets as they are returned from search engines.
Note that these methods are not employed to address the problem
of polysemy, since the required in-depth analysis of the term context
is computationally too expensive for many similarity search applica-
tions.
Index Transformation. In contrast to the construction methods men-
tioned before, transformation methods operate on all document vec-
tors of a collection D at the same time by analyzing the term-
document matrix, A. A popular index transformation method is latent
semantic indexing (LSI), which uses a singular value decomposition
of A in order to improve query rankings and similarity computations
[2; 1; 8]. For this purpose, the document vectors are projected into
a low-dimensional space that is spanned by the eigenvectors that be-
long to the largest singular values of the decomposition of A.
2.1 Discussion
Index terms that consist of a single word can be found by a skillful
analysis of prefix frequency and prefix length. This idea can be ex-
tended to the identification of compound word concepts in written
text. If continuous sequences of n words occur significantly often,
then it is likely that these words form a concept. Put another way,
concept detection reduces to the identification of frequent n-grams.
n-grams as a replacement for index term enrichment has been an-
alyzed by several authors in the past, with moderate success only [6].
We explain the disappointing results with noise effects, which dom-
inate the positive impact of few additional concepts: Most authors
apply a strategy of “complete extension”; i. e., they add all 2-grams
and 3-grams to the index vector. However, when analyzing the fre-
quency distribution of n-grams, it becomes clear that only a small
fraction of all compound word sequences is statistically relevant.
The advantages of syntactical (statistical) methods for index con-
struction can be summarized as follows:
1. language independence
2. robustness with respect to multi-lingual documents
3. tailored indexes for retrieval tasks on closed collections
An obvious disadvantage may be the necessary statistical mass:
Syntactical index construction cannot work if only few, very small
document snippets are involved. This problem is also investigated
in the next section, where the development of the index quality is
compared against the underlying collection size.
As an aside, statistical stemming and the detection of compound
word concepts are essentially the same—the level of granularity
makes the difference: Stemming means frequency analysis at the
level of characters; likewise, the identification of concepts means fre-
quency analysis at the level of words.
3 ANALYSIS OF
ENRICHED VECTOR SPACE MODELS
Existing reports on the impact of index term selection and index term
enrichment are contradictory [4; 5; 7], and not all of the published
performance improvements could be reproduced [6]. Most of this
research analyzes the effects of a modified vector space model on
typical information retrieval tasks, such as document clustering or
query answering.
Note that clustering results that have been obtained by employing
the same cluster algorithm under different document models may
tell us two things: (i) whether one document model captures more
of the “gist” of the original document d than another model, and,
(ii) whether the cluster algorithm is able to take advantage of this
added value.
A cluster algorithm’s performance depends on various parameters,
such as the cluster number, its randomized start configuration, or pre-
set similarity thresholds, etc., which renders a comparison difficult.
Moreover, there is the prevalently observed effect that different clus-
ter algorithms behave differently sensitive to document model “im-
provements”. From an analysis point of view the following questions
arise:
1. Which cluster algorithm shall define the baseline for a comparison
(the best for the dataset, the most commonly used, the simplest)?
2. Given several clustering results obtained by the same cluster algo-
rithm, which result can be regarded as meaningful (the best, the
worst, the average)?
Especially to the second point less attention is paid in cur-
rent research: Common practice is to select the best result com-
pared to a given reference classification, e. g. by maximizing the F -
Measure value—ignoring that such a combined usage of unsuper-
vised/supervised methods is far away from reality.4
An objective way to rank different document models is to compare
their ability to capture the intrinsic similarity relations of a given col-
lection D. Basic idea is the construction of a similarity graph, mea-
suring its conformance to a reference classification, and analyzing
the improvement or decline of this conformance under some docu-
ment model. Exactly this is operationalized in form of the ρ-measure
that is introduced below; it enables one to evaluate differences in the
similarity concepts of alternative document models without being de-
pendent on a cluster algorithm.5
Hence, the performance analyses presented in this section com-
prise two types of analyses: (i) Experiments that, based on ρ, quantify
objective improvements or declines of a document model, (ii) exper-
iments that, based on the F -Measure, quantify the effects of a docu-
ment model onto different cluster algorithms.
3.1 A Measure of Expected Density: ρ
As before let D = {d1, . . . , dn} be a document collection whose
corresponding computer representations are denoted as d1, . . . ,dn.
A similarity graph G = 〈V, E, ϕ〉 for D is a graph where a node in
V represents a document and an edge (di, dj) ∈ E is weighted with
the similarity ϕ(di,dj).
A graph G = 〈V, E, w〉 is called sparse if |E| = O(|V |); it
is called dense if |E| = O(|V |2). Put another way, we can com-
pute the density θ of a graph from the equation |E| = |V |θ . With
4 This issue is addressed in [14].
5 The ρ-measure was originally introduced in [14], as an alternative for the
Davies-Bouldin-Index and the Dunn-Index, in order to evaluate the quality
of cluster algorithms for text retrieval applications.
50
Sample size
E
x
p
e
c
te
d
 d
e
n
s
it
y
 ρ
standard vector space model
synonym enrichment
hypernym enrichment
n-gram index term selection
 1
 1.2
 1.4
 1.6
 1.8
 2
 2.2
 2.4
 200  300  400  500  600  700  800  900 1000
5 categories
n-gram index term selection
standard vector space model
Sample size
E
x
p
e
c
te
d
 d
e
n
s
it
y
 ρ
 1
 1.2
 1.4
 1.6
 1.8
 2
 2.2
 2.4
 200  300  400  500  600  700  800  900 1000
5 categories
Figure 3. Comparison of the standard vector space model, two semantically enriched models (synonym, hypernym), and a vector space model with syntac-
tically identified concepts (n-gram) in two languages: The left-hand graph illustrates the development of ρ depending on the collection size for an English
document collection; the right-hand graph compares the n-gram vector space model to the standard model for a German document collection.
w(G) := |V | +
∑
e∈E
w(e), this relation extends naturally to
weighted graphs:6
w(G) = |V |θ ⇔ θ =
ln
(
w(G)
)
ln
(
|V |
)
Obviously, θ can be used to compare the density of each induced
subgraph G′ = 〈V ′, E′, w′〉 of G to the density of G: G′ is sparse
(dense) compared to G if the quotient w(G′)/(|V ′|θ) is smaller
(larger) than 1. This consideration provides a key to quantify a doc-
ument model’s ability to capture the intrinsic similarity relations of
G, and hence, of the underlying collection.
Let C = {C1, . . . , Ck} be an exclusive categorization of D in k
distinct categories, that is to say, Ci, Cj ⊆ D with Ci ∩ Cj = ∅
and ∪ki=1Ci = D, and let Gi = 〈Vi, Ei, ϕ〉 be the induced subgraph
of G with respect to category Ci. Then the expected density of C is
defined as follows.
ρ(C) =
k
∑
i=1
|Vi|
|V |
·
w(Gi)
|Vi|θ
, where |V |θ = w(G)
Since the edge weights resemble the similarity of the documents
associated with V , a higher value of ρ indicates a better modeling of
a collection’s similarity relations.
3.2 Syntax versus Semantics:
Variants of the Vector Space Model
Aside from the standard vector space model our analysis compares
the following three vector space model variants:
1. Syntactic Term Selection. Within this variant the index term selec-
tion step also considers syntactically identified concepts, i. e., 2-
grams, 3-grams, and 4-grams. To identify the significant n-grams
the document collection D is inserted into a suffix tree and a sta-
tistical successor variety analysis is applied. The operationalized
principle behind this analysis is the peak-and-plateau method [5],
for which we have developed a refinement in our working group.
6 w(G) denotes the total edge weight of G plus the number of nodes, |V |,
which serves as adjustment term for graphs with edge weights in [0; 1].
2. Semantic Synonym Enrichment. Within this variant of semantic
term enrichment the so-called synsets from Wordnet for nouns are
added [3]; this procedure has been reported to work well for cate-
gorization tasks [7]. Note that adding synonyms to all index terms
of a document vector will introduce a lot of noise, and hence only
the top-ranked 10% of the index terms (respecting the employed
term weighting scheme) are selected for enrichment.
3. Semantic Hypernym Enrichment. This variant of semantic term
enrichment relies also on Wordnet: a sequence of up to four con-
secutive hypernyms is substituted for each noun. The rationale
is as follows. Documents dealing with closely related—but still
different—topics often contain terms which derive from a single
hypernym representing their common category. The enrichment
proposed here yields a stronger similarity between such docu-
ments without generalizing too much.
Index term weighting of both unigrams and n-grams follows the
tf · idf -scheme; stopwords are not indexed and unigram stemming is
done according to Porter’s algorithm.
Discussion. The resulting graphs in Figure 3 as well as the compar-
ison in Table 1 show that the syntactic approach outperforms both
semantic approaches. From the semantic variants only the semantic
hypernym enrichment is above the baseline; note that this happens
even if a large number synsets is added. We explain the results as
follows: Index terms with a high term weight typically belong to a
special vocabulary, and, from a semantic point of view, they are used
deliberately so that adding their synsets will tend to decrease their
importance. Likewise, adding the synsets of low-weighted terms has
no effect other than adding noise since the importance of these terms
will be increased without a true rationale.
Vector space model variant F -min F -max F -av.
(sample size 1000, 10 categories)
standard vector space model —baseline—
synonym enrichment -8% +4% -2%
hypernym enrichment +5% +12% +3%
n-gram index term selection +15% +6% +8%
Table 1. The table shows the improvements of the averaged F -Measure val-
ues that were achieved with the cluster algorithms k-means and MajorClust
for the investigated variants of the vector space model.
51
3.3 Test Corpus and Sample Formation
Experiments have been conducted with samples from RCV1, a short
hand for “Reuters Corpus Volume 1” [10], as well as with documents
from German newsgroup postings.
RCV1 is a document collection that was published by the Reuters
Corporation for research purposes. It contains more than 800,000
documents each of which consisting of a few hundred up to several
thousands words. The documents are tagged with meta information
like category (also called topic), geographic region, or industry sec-
tor. There are 103 different categories, which are arranged within
a hierarchy of the four top level categories “Government, Social”,
“Economics”, “Markets”, and “Corporate, Industrial”. Each of the
top level categories defines the root of a tree of sub-categories, where
each child node fine grains the information given by its parent. Note
that a document d can be assigned to several categories c1, . . . , cp,
and that d does also belong to all ancestor categories of some cate-
gory ci.
Within our experiments two documents di, dj are considered to
belong to the same category if they share the same top level category
ct and the same most specific category cs. Moreover, the test sets are
constructed in such a way that there is no document di whose most
specific category cs is an ancestor of the most specific category of
some other document dj .
The samples were formed as follows: For the analysis of the in-
trinsic similarity relations based on ρ, the sample sizes ranged from
200 to 1000 documents taken from 5 categories. For the analysis of
the categorization experiments, based on cluster algorithms and eval-
uated with the F -Measure, the sample sizes were 1000 documents
taken from 10 categories.7
...
RCV1
Corporate,
Industrial
Economics
Government, Social
Markets
Performance
Insolvency,
Liquidity
Account,
Earnings
Comment,
Forecasts
Annual
results
Figure 4. Category organization of the RCV1 corpus showing the four top
level categories from which “Corporate, Industrial” is further refined.
4 SUMMARY
This paper provided a comparison of syntactical and semantic meth-
ods for the construction of vector space models; the special focus
was index term selection. Interestingly, little attention has been paid
to the mentioned syntactical methods in connection with text retrieval
tasks. Following results of our paper shall be emphasized:
• With syntactically identified concepts significant improvements
can be achieved for categorization tasks.
• The benefit of semantic term enrichment is generally overesti-
mated.
• The ρ-measure provides an “algorithm-neutral” approach to ana-
lyze the similarity knowledge contained in document models.
7 To make our analysis results reproducible for other researchers, meta infor-
mation files that describe the compiled test collections have been recorded;
they are available upon request.
Note that the last point may be interesting to develop accepted
benchmarks to compare research efforts related to document models
or similarity measures.
Though syntactical analyses must not be seen as a cure-all for the
index construction of vector space models, they provide advantages
over semantic methods, such as language independence, robustness,
and tailored index sets. With respect to several retrieval tasks they
can keep up with semantic methods—however, our results give no
room for an over-simplification: Both paradigms have the potential
to outperform the other.
References
[1] Michael W. Berry, Susan T. Dumais, and Gavin W. O’Brien,
‘Using Linear Algebra for Intelligent Information Retrieval’,
Technical Report UT-CS-94-270, Computer Science
Department, (dec 1994).
[2] Scott C. Deerwester, Susan T. Dumais, Thomas K. Landauer,
George W. Furnas, and Richard A. Harshman, ‘Indexing by
Latent Semantic Analysis’, Journal of the American Society of
Information Science, 41(6), 391–407, (1990).
[3] Christiane Fellbaum, WordNet: An Electronic Lexical
Database, MIT Press, 1998.
[4] W. B. Frakes, ‘Term conflation for information retrieval’, in
SIGIR ’84: Proceedings of the 7th annual international ACM
SIGIR conference on Research and development in
information retrieval, pp. 383–389, Swinton, UK, (1984).
British Computer Society.
[5] W. B. Frakes and Ricardo Baeza-Yates, Information retrieval:
Data Structures and Algorithms, Prentice-Hall, Inc., Upper
Saddle River, NJ, USA, 1992.
[6] Johannes Fürnkranz, ‘A Study Using n-gram Features for Text
Categorization’, Technical report, Austrian Institute for
Artificial Intelligence, (1998). Technical Report
OEFAI-TR-9830.
[7] A. Hotho, S. Staab, and G. Stumme, ‘Wordnet improves text
document clustering’, in Proceedings of the SIGIR Semantic
Web Workshop, (2003).
[8] Christos H. Papadimitriou, Hisao Tamaki, Prabhakar
Raghavan, and Santosh Vempala, ‘Latent semantic indexing: a
probabilistic analysis’, in PODS ’98: Proceedings of the 17th
ACM SIGACT-SIGMOD-SIGART symposium on Principles of
database systems, pp. 159–168, New York, NY, USA, (1998).
ACM Press.
[9] Andreas Rauber and Alexander Müller-Kögler, ‘Integrating
automatic genre analysis into digital libraries’, in ACM/IEEE
Joint Conference on Digital Libraries, pp. 1–10, (2001).
[10] T.G. Rose, M. Stevenson, and M. Whitehead, ‘The Reuters
Corpus Volume 1 - From Yesterday’s News to Tomorrow’s
Language Resources’, in Proceedings of the Third
International Conference on Language Resources and
Evaluation, (2002).
[11] G. Salton and M. E. Lesk, ‘Computer Evaluation of Indexing
and Text Processing’, ACM, 15(1), 8–36, (January 1968).
[12] Karen Sparck-Jones, ‘A statistical interpretation of term
specificity and its application in retrieval’, Journal of
Documentation, 28, 11–21, (1972).
[13] E. Stamatatos, N. Fakotakis, and G. Kokkinakis, ‘Text genre
detection using common word frequencies’, in Proceedings of
the 18th Int. Conference on Computational Linguistics,
Saarbrücken, Germany, (2000).
[14] Benno Stein, Sven Meyer zu Eißen, and Frank Wißbrock, ‘On
Cluster Validity and the Information Need of Users’, in
Proceedings of the 3rd IASTED International Conference on
Artificial Intelligence and Applications (AIA 03),
52
Benalmádena, Spain, ed., M. H. Hanza, pp. 216–221,
Anaheim, Calgary, Zurich, (September 2003). ACTA Press.
[15] Michael Steinbach, George Karypis, and Vipin Kumar, ‘A
comparison of document clustering techniques’, Technical
Report 00-034, Department of Computer Science and
Egineering, University of Minnesota, (2000).
[16] Yiming Yang and Jan O. Pedersen, ‘A comparative study on
feature selection in text categorization’, in Proceedings of
ICML-97, 14th International Conference on Machine
Learning, ed., Douglas H. Fisher, pp. 412–420, Nashville, US,
(1997). Morgan Kaufmann Publishers, San Francisco, US.
