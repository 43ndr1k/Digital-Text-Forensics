Authorship Attribution  1 
Methods of Token-Based Authorship Attribution for an English-
Language Online Discussion Community 
 
Emily Stuckey 
Advisor: Margrit Betke 
Department of Computer Science 
Boston University 
emilys@cs.bu.edu 
2004 
 
 
Abstract: Authorship attribution concerns the proof or disproof of the authorship of a 
text, given text known to be by either several candidate authors or one alleged author. 
Most of the work in this field focuses on published works, but it could be applied to 
online discussion as well.  I analyze the performance of several token-based (non-
semantic) authorship attribution algorithms suggested by different researchers on data 
from eight users of an online discussion board over the course of a year.  None of the 
methods were successful, and I propose that semantic-based methods may be better 
suited to this form of communication. 
1. Introduction 
The study of authorship attribution methods involves both multiple means and multiple 
ends.  While some researchers focus on separating parts of the Bible by author, others use 
stylistic indicators as forensic evidence in trials in the United Kingdom.  Similarly, some 
([1] [2]) focus on the not-entirely-conscious use of function words (e.g., "if", "the", "an") 
and punctuation, while others use sophisticated grammatical analyses such as the ratio of 
sentences to possible sentence boundaries ([3]).  Because this area straddles the fields of 
literature and computer science, the work done on this issue has been somewhat 
disorganized until quite recently -- there is no standard corpus on which techniques are 
tested, and no rigorous definition of what kind of text (e.g. formal v. informal, language, 
etc.) is being addressed, or whether the kind of text makes any difference.  Researchers in 
the field have noted ([4]) that both the methods and the text used are too often dictated by 
availability and convenience. 
  
One drawback of much of the research done is the similarity of the text used.  Generally, 
researchers test their methods on published literary works.  Care ([5]) used material from 
Dickens, Emerson, and Keats, among others -- all English book authors; Stamatatos ([3]) 
used Greek newspaper articles.  The effectiveness of an attribution method may be 
affected by the type of input used. 
 
Authorship attribution could be used as an alert system for online communities -- for 
example, to give warning of password theft if a user's posts differ drastically from their 
previous pattern, or to alert administrators to a trouble-making user registering under 
another name.  If an algorithm could be found which identified the author with high 
probability, then administrators could follow its alerts with more direct investigation. 
 
Authorship Attribution  2 
Data from eight posters in an online community over the course of 19 months was used to 
test the cusum method, the zip method, and a list of feature measurements used by de Vel 
et al. ([1]). 
2. Corpus 
The data used was taken from an online English- language discussion board.  This board 
is funded and maintained solely by the users, and administrative decisions (e.g., banning 
of users, creation of new threads) are made by discussion and voting.  The common 
interest of users of the board is science fiction/fantasy television shows, but the 
discussions used as sources of text were from a "general" thread and had no central topic.  
A total of 22 total discussion threads were used (threads are closed at 10,000 posts and 
replaced), dating from September of 2002 (the beginning of the board) to April of 2004.  
More users consented to have their posts included in the study, but eight were finally 
chosen for the greatest quantity of data -- each of the eight had posts in every one of the 
22 threads.  Most of the users had between 90,000 and 200,000 words in their text 
samples, with one having around 34,000 and another 480,000. 
   
Each poster was assigned an ID number: 376, 225, 87, 182, 236, 290, 110, and 57.  Users 
376, 182, 236, 290, 110, and 57 are female, and users 225 and 87 are male.  Five of the 
posters are American-born speakers of English as a first language.  One poster is Israeli 
and has Hebrew as her first language, another is Australian and has worked in the United 
States for three years, and a third was born in Jamaica and spent significant periods of 
time in London and Montreal before moving to the United States in 1993.   
3. Methods 
3.1.1 Cusum 
Cumulative sum techniques are used in quality control and process monitoring to ensure 
that industrial processes are working within specifications.  For the past thirty years or so, 
scholars have applied it to authorship attribution, with varying degrees of success.  Its 
supporters ([6] [2]) claim that it can detect authorship even of speech, and it has been 
used as forensic evidence in several trials in the United Kingdom.  Others ([7] [8]) claim 
that the method lacks statistical measures and relies too much on subjective judgment.   
 
The cusum (or Qsum) authorship attribution technique relies on the theory that the 
interaction between certain stylistic habits -- for example, sentence length and number of 
function words per sentence -- is particular and consistent for one author.  I used sentence 
length and the combination of 2- and 3-letter words per sentence (23l) and words 
beginning with a vowel per sentence (ivw), as this is reported to be the pair of attributes 
most useful for author tracking ([6]). 
 
For a section of text to be analyzed, the mean for each statistic is calculated.  Then the 
deviation from the mean is calculated for each sentence, and the cumulative sum is 
graphed.  Because these stylistic habits are unconscious and not directly related to the 
topic of the writing, the relationship between the two should (according to the method's 
proponents) remain constant for the same author.  To test the authorship of a piece of 
text, the "unknown" piece is inserted in the middle of work known to be by a candidate 
Authorship Attribution  3 
author, and the cumulative sums for the whole are computed.  If the relation between the 
two running sums remains roughly constant (after scaling) throughout the whole piece 
(known and unknown), the unknown text can be attributed to the author.  If the unknown 
piece is by a different author, the relationship between the two will be uneven, and the 
lines will converge or diverge. 
 
As an example, figure 1 is the cusum chart of the first eleven sentences from the third 
chapter of Jane Austen's Pride and Prejudice: 
2 4 6 8 10
-60
-40
-20
0
20
40
60 2 4 6 8 10
-50
-25
0
25
50
 
Fig. 1: Cusum plot of text from Pride and Prejudice.  One line represents sentence length, the other the number of 2- or 
3-sentence and initial vowel words in the sentence. 
 
The two series are plotted on different axes, so that each takes about a third of the space.  
There is some variance, but for the most part the two have the same relation throughout 
the graph. 
 
Figure 2 is the chart for those lines followed by eleven from Mark Twain's Christian 
Scientist, followed by the next twenty from Pride and Prejudice. 
 
Authorship Attribution  4 
0 10 20 30 40
-200
-100
0
100
2000 10 20 30 40
-200
-100
0
100
200
 
Fig. 2: Cusum plot of text drawn from Pride and Prejudice and Christian Scientist. 
 
Proponents of the method believe that text by two different authors will always show a 
divergence such as in the above "mixed" graph. 
 
I tried various sizes of data, and eventually settled on 50 "known" sentences (10 each 
from the first five threads) followed by 50 sentences from an "unknown" file (ten each 
from five of the "test" threads) and 120 "known" sentences (so that the first 50 and last 
100 were from the same author).  These were combined, producing 64 test files.  From 
these, I calculated the cusum series and graphed them in Matlab. 
 
3.1.2 Weighted Cusum 
 
In pursuit of a less subjective analysis of these same measurements, a weighted 
cumulative sum method ([8]) was used.  This method produces a test statistic t which can 
be compared to critical values in widely available t charts to determine whether the 
variation in relation of 2/3- letter/initial-vowel words to sentence length between two texts 
is statistically significant.   
 
The "weight" is p, which is the proportion of words within the text which have the 
measured quality (2/3 letters (23l) or initial vowel (ivw)).  This weight is calculated for 
each text as p = (23l & ivw)/(words).  The cumulative sum (s) is now the running sum of 
((# 23l & ivw in sentence) - p*(words in sentence)).  s now measures the cumulative 
Authorship Attribution  5 
difference between the number of the observed and expected occurrences of 2- or 3- letter 
and initial vowel words.  These values are used to calculate t : 
 
t =    |(pA - pB)|   
  
 square root(((sigA)2 / SUMwA) +  ((sigB)2/ SUMwB)) 
 
(or (|WeightA-WeightB|) / sqrt((VarianceA/Total WordsA) + (VarianceB/Total WordsB)) 
with sig2 calculated using weighted squared successive differences to estimate the 
underlying variance: 
 1/(n-1) * SUMi=1to n-1[{(xi/wi) - (xi+1/wi+1)}2/{(1/wi) + (1/wi+1)}] 
 
This t value measures the evidence against the null hypothesis (that the habit is consistent 
in both texts, and implies the same author). 
 
For example, for the file containing "known" text from user 376 (50 sentences containing 
417 words), the value of p is 0.6618.  An "unknown" file (50 sentences, 394 words) has a 
p of 0.5914.  These values are used to calculate t: 
 
 (0.6618 - 0.5914)/v(0.45/414 + 0.3076/394) = 0.0704/v(0.0011 + .0007807) = 
0.0704/v0.0019 = 0.0704/0.0436 = 1.6151.   
 
This value is compared with the critical value from a t-table to see whether it is above the 
value necessary to reject the null hypothesis with some degree of confidence. 
 
3.2 Zip 
The second attempted technique involved the use of compression software, as described 
by Benedetto, Caglioti, and Loreto[9].  Because compression algorithms seek to reduce 
size by reducing redundancy, they theorized that (other things being equal) a text by one 
author would result in a smaller zipped file than one by two different authors.  Or, 
"Where Lx indicates the length in bits of the zipped file X ... We shall look for the text Ai 
for which the difference L(Ai + x) - LAi is minimum." (pp. 2-3)  
 
The test was performed on the same set of test files as cusum.  I used gzip to compress 
the "known" files individually, and recorded their sizes.  I then made 64 files (pairing 
each "known" text with each "unknown" text), compressed them, and measured, for each 
"unknown" text, which known text, when zipped with the unknown appended to it, had 
the smallest change between zipped "known" and zipped "known and unknown".  That is 
to say, for an unknown file by user x, I identified the known file y which produced the 
smallest zip(y+x)-zip(y).  By Benedetto's theory, the file with the least change (the 
minimum del) would represent the known text by the same author as the unknown text in 
question. 
 
3.3 SVM with de Vel et al.'s Features 
Of all the research in the field, de Vel et al.'s study ([1]) of identification characteristics in 
text from an email list seemed the most likely to prove relevant to attribution in an online 
Authorship Attribution  6 
discussion community, since both media share elements of informality which are not 
present in the works of literature studied by others.  I included most of the features in my 
test, leaving out those which were not relevant to this format (e.g., presence or absence of 
a greeting line).  The features measured for the online discussion group were as follows 
(where M = number of words, V= number of distinct words, C= total number of 
characters in the text, and "hapax legomena" is the number of distinct words which occur 
only once in the text): 
• Vocabulary richness (V/M)  
• Function word frequency distribution  
• Number of short words (three or fewer letters)/M  
• Count of hapax legomena/M  
• Count of hapax legomena/V  
• Total number of alphabetical characters/C  
• Total number of upper-case characters/C  
• Total number of digit characters/C  
• Total number of white-space characters/C 
• Total number of punctuations/C 
• Word length frequency distribution/M 
De Vel et al. used 122 function words (e.g., "or", "that", "the").  In my implementation, I 
used 187.  For the word length frequency distribution, I followed their example and 
measured words of up to 30 letters.  The resulting vectors had 306 features. 
 
These vectors were then fed into a Support Vector Machine.  I used SVMlight ([10]) with 
Anton Schwaighofer's Matlab interface ([11]).  A Support Vector Machine is a way of 
creating binary (in-or-out) classifiers by finding a divider in the input space with the 
maximum distance from the divider to examples both of the "in" and "out" types.  This 
allows it to handle vectors with many features without overfitting. 
 
There were fifteen training vectors from threads 1 through 15, and seven test vectors 
from 16 through 22. 
 
4. Results 
4.1.1 Cusum 
The cusum plots proved entirely unhelpful.  The following are a few graphs of the cusum 
of files which have only one author each (that is to say, the known and unknown texts are 
by the same user): 
 
Authorship Attribution  7 
0 50 100 150 200 250
-200
-150
-100
-50
0
50
100
150
200
0 50 100 150 200 250
-200
-150
-100
-50
0
50
100
150
200
0 50 100 150 200 250
-200
-150
-100
-50
0
50
100
150
200
0 50 100 150 200 250
-250
-200
-150
-100
-50
0
50
100
150
200
250
0 50 100 150 200 250
-200
-150
-100
-50
0
50
100
150
200
0 50 100 150 200 250
-250
-200
-150
-100
-50
0
50
100
150
200
250
 
        Fig. 3: Known: 376 Unknown: 376           Fig. 4: Known: 225 Unknown: 225                 Fig. 5: Known: 57 Unknown: 57 
 
In all these plots, the solid line represents the value for the sentence length and the dotted 
line that for the 23l and ivw count per sentence.  The following graphs resulted from 
"mixed" text (that is, the file contained text by two different authors): 
 
0 50 100 150 200 250
-200
-150
-100
-50
0
50
100
150
200
0 50 100 150 200 250
-250
-200
-150
-100
-50
0
50
100
150
200
250
0 50 100 150 200 250
-200
-150
-100
-50
0
50
100
150
200
0 50 100 150 200 250
-250
-200
-150
-100
-50
0
50
100
150
200
250
0 50 100 150 200 250
-200
-150
-100
-50
0
50
100
150
200
0 50 100 150 200 250
-250
-200
-150
-100
-50
0
50
100
150
200
250
 
        Fig. 6: Known: 376 Unknown: 236             Fig. 7: Known: 110 Unknown: 376           Fig. 8: Known: 290 Unknown: 225 
 
The scales of all these graphs are the same -- -200 to 200 for the dotted line (representing 
23l and ivw counts per sentence) and -250 to 250 for the solid (sentence length). 
 
4.1.2 Weighted cusum 
The weighted cusum method turned out to be a disappointment as well. 
 
The degrees of freedom are given by the number of sentences in A (the "known" text) 
plus the number of sentences in B (the "unknown") minus 2: 98.  The critical t value for a 
5% level of significance with 98 degrees of freedom is 1.984.  54 of the 64 values 
produced by this method were not high enough to conclude that they were written by 
different authors.  In ten cases, the value was high enough -- but two of those ten actually 
were one-author texts. 
 
4.2 Zip 
Despite promising preliminary results, the zip method was no better than cusum. 
Following Care's ([5]) example by using gzip, I received the following values shown in 
Table 1. 
Authorship Attribution  8 
 
 alone 
del 
376 
del 
225 
del 
87 
del 
182 
del 
236 
del 
290 
del 
110 
del 
57 
376 4155 1394 1107 1157 785 2245 939 899 1154 
225 3312 1427 923 1188 778 2252 940 750 1154 
87 4770 1376 1065 1104 747 2172 906 876 1130 
182 3240 1419 953 1167 772 2271 943 756 1177 
236 5876 1373 1050 1131 737 2014 895 885 1132 
290 3020 1394 1071 1165 756 2226 920 907 1173 
110 3596 1401 1075 1169 745 2235 919 890 1162 
57 4026 1401 1069 1155 753 2200 926 893 1137 
Table 1: Change in size of zipped text files.  Each row represents "known" text from one author y; 
each column, the change in size of zipped text when "unknown" text from one author, x, is appended  
(zip(x+y) - zip(y)).  Bold numbers represent "correct" answers. 
 
The "alone" column contains the size of the "known" files zipped by themselves (zip(y)); 
the del columns contain the increase in zipped size produced by appending "unknown" 
text by the indicated author before zipping (zip(y+x) - zip(y)).  Only three of the matches 
produced by using this method were correct (both files had the same author). 
 
4.3 SVM with de Vel et al's Features 
I obtained very poor performance with the SVM as well; in fact, it was unable to do 
better, on any single binary question (by or not by user 376, by or not by user 87, etc.) 
than to guess "no" on all samples.  De Vel et al. ([1]) reported good performance using a 
polynomial kernel with power 3.  I tried various functions for the kernel, including linear, 
polynomial with power from 3 to 10, and even radial basis function with gamma from 0.2 
to 2.0 but was unable to obtain any improvement. 
 
I hesitate to draw too strong a conclusion from this, however, as I was unfamiliar with the 
techniques.  The prior success of others' research implies that some better performance 
might be possible, given the correct circumstances (perhaps more data, or a different 
implementation). 
5. Conclusions 
Although none of the methods described herein worked, this doesn't preclude the possible 
success of authorship attribution on this kind of data, or even of the methods used.  For 
instance, de Vel et al. ([1]) obtained very good results using support vector machines with 
email data, which would seem to share many of the attributes of the discussion board 
data.  Indeed, it seems entirely possible that, given sufficient time and a better 
understanding of the machine, it would prove helpful to attribution even on this problem.  
And any of these methods might succeed, given a greater sample size. 
 
By the same token, however, it is not a great surprise that these algorithms fa iled.  The 
habits upon which they depend may simply not be present in the same form in such an 
informal discourse.   
 
Using the cusum method requires dependable habits of sentence length and short/initial-
vowel word use (there are other statistics sometimes used with the cusum method, and I 
Authorship Attribution  9 
can make no claims about their potential applicability), and both of these may be 
drastically affected by the informality of the context.  Unlike published work, there is no 
pressing need for discussion posts to contain full sentences, or even be grammatically 
correct.  The presence of one- or two-word sentences may throw off any method which 
relies on similar measures. 
 
For the zip method, the problem is similar.  Compression relies on there being repetition 
to reduce -- and repetition which is specific to the user.  In an online forum, idiosyncratic 
expressions are less likely to occur, since posts are usually short and address a particular 
issue.  All the same, one would expect patterns to emerge in the long run.  Perhaps with 
more data, this method would become more effective. 
 
Further work on author identification for online communities might best focus on more 
content-based measures. Some researchers ([12]) suggest using spelling and grammatical 
idiosyncrasies as indicators of authorship, and this might work very well for online 
communication, due to the immediacy and lack of formal stylistic standards. 
 
It may be true of any one of the methods discussed that it would work better given more 
data.  But if so, this is still a strike against the method.  The data used represented more 
than a full year of posting from prolific posters; for a method to require more data than 
this would render it unhelpful.  The advantage of the hypothesized system would be that 
it would provide an alert before administrators might have noticed an authorship issue on 
their own; if it takes more than a year's worth of data before it can detect patterns, and a 
large "unknown" text before it can make an attribution, it cannot serve that function. 
 
So many different methods have been proposed and debated that clear answers about 
what text is attributable, and what methods are best suited for it, may be a long way in the 
future.  For the time being, it would be unwise to rely on any method for sensitive 
judgments or applications.   
 
The prospective goal is not a 100% reliable algorithm in any case -- any algorithm could 
only give a probable attribution.  Authors often change styles, or imitate the style of 
others.  In addition, authors' styles may change over time, and different venues dictate 
different style (I question whether any algorithm would identify the correct author of a 
scientific paper, given informal text by the same author). 
 
Authorship attribution is quite likely not one problem, but many, differing by domain; 
and the solution thereto may differ accordingly.  A scholarly work may best be analyzed 
by one algorithm, a piece of fiction another, and a post to an online board still another.  If 
the best solutions are to be found, however, those interested in the problem must 
overcome the divide between the humanities and the sciences.  Far too often, an author 
from one field misunderstands or underestimates the requirements of the other.  Going 
forward, a more interdisciplinary approach, in this as well as in many other problems, can 
only improve the quality of future work. 
Authorship Attribution  10 
6. References 
1. O. de Vel, A. Anderson, M. Corney, and G. Mohay.  "Mining E-mail Content for 
Author Identification Forensics."  SIGMOD Record 30(4):55-64, 2001. 
2. Bee, Ronald E.  "Statistical Methods in the Study of the Masoretic Text of the Old 
Testament." Journal of the Royal Statistical Society. Series A (General). 
134(4):611-22 (1971).<http://links.jstor.org/sici?sici=0035-
9238%281971%29134%3A4%3C611%3ASMITSO%3E2.0.CO%3B2-L>. 
3. E. Stamatatos, N. Fakotakis, and G. Kokkinakis.  "Computer-Based Authorship 
Attribution Without Lexical Measures."  Computers and the Humanities 
35(2):193-214, 2001. 
4. J. Rudman.  "The State of Authorship Attribution Studies: Some Problems and 
Solutions."  Computers and the Humanities 31:351-65, 1998. 
5. M. Care.  Authorship Attribution: A Comparison of Three Methods. Diss.  University 
of Sheffield, 2003. 
<http://www.dcs.shef.ac.uk/teaching/eproj/msc2003/pdf/m2mc.pdf>. 
6. J. Farringdon.  "How to be a Literary Detective: Authorship Attribution."  Retrieved 
April 5, 2004 from <http://members.aol.com/qsums/QsumIntroduction.html>. 
7. G. K. Barr.  "The Cusum Mechanism--A Review of Analyzing for Authorship by Jill 
M. Farringdon."  Expert Evidence 6(1):43-55, 1998. 
8. M.L. Hilton and D. I. Holmes.  "An Assessment of Cumulative Sum Charts for 
Authorship Attribution." Literary and Linguistic Computing 8(2):73-80, 1993. 
9. D. Benedetto, E. Caglioti, and V. Loreto.  "Language Trees and Zipping."  Physical 
Review Letters 88(4):048702.1-4, 2002. 
10. T. Joachims. SVMlight 7 March 2002. Cornell University.  20 March 2004 
<http://svmlight.joachims.org/>. 
11. Schwaighofer, Anton.  "Matlab Interface to SVM light v.0.92."  August 2002. 
Download. 20 March 2004.  
<http://www.cis.tugraz.at/igi/aschwaig/software.html>. 
12. Koppel, Moshe, and Jonathan Schler.  "Exploiting Stylistic Idiosyncrasies for 
Authorship Attribution."  Proceedings of "IJCAI '03 Workshop on Computational 
Approaches to Style Analysis and Synthesis." 9-15 August, 2003, Acapulco 
Convention Center.  Acapulco, Mexico. 
Additional Sources: 
T. Joachims.  Making large-Scale SVM Learning Practical: Advances in Kernel Methods 
- Support Vector Learning, B. Schölkopf, C. Burges and A. Smola Eds. (MIT 
Press, Cambridge, MA, 1999). 
T. Joachims. "Text Categorization with Support Vector Machines: Learning with Many 
Relevant Features." Proceedings of the European Conference on Machine 
Learning (ECML), Springer, 1998. 
J. Platt.  Support Vector Machines.  17 September 2003.  Microsoft Research.  Retrieved 
20 April 2004. <http://research.microsoft.com/users/jplatt/svm.html>. 
 
