C1.1 
Proceedings of Student/Faculty Research Day, CSIS, Pace University, May 4th, 2007 
 
 
The Use of Stylometry for Email Author Identification: A Feasibility Study 
Robert Goodman, Matthew Hahn, Madhuri Marella, Christina Ojar, Sandy Westcott  
Seidenberg School of CSIS, Pace University 
1 Martine Ave, White Plains, NY, 10606, USA 
{rg39818w, mh60416p, mm03271w, co37098w}@pace.edu, profwestcott@yahoo.com 
 
Abstract 
It is recognized that authors have unique 
writing styles in which it is possible to find a 
simple statistical model. This paper makes an 
attempt to explain the methodology of 
developing and optimizing a stylometry 
program which quantifies the writing styles of 
various authors using 62 stylistic features found 
in emails. The feature extraction and pattern 
classification program was developed for a 
Pace University SSCSIS Doctorate of 
Professional Studies student, Professor Sandy 
Westcott who was completing her thesis on 
stylometry. The C#-based system is used to 
identify the author of an arbitrary email using 
stylometry features. Raw keystroke data, 
collected from an Internet-based Java applet in 
an earlier keystroke biometric study conducted 
by Dr. Mary Valliani, was converted into simple 
text files, appropriate features were extracted 
and a pattern classifier was implemented. 
Introduction 
Stylometry is the study of the unique linguistic 
styles and writing behaviors of individuals in 
order to determine authorship. A person’s 
writing style contains many features that reveal 
an individual’s uniqueness including vocabulary 
usage and richness, use of function words, text 
formatting on a page, and the length and 
structure of sentences. An individual’s 
combined writing features are usually consistent 
throughout all of that person’s writings. 
The investigation of authorship attribution has 
existed for centuries [1, 2] and studies into the 
authorship of famous literature and documents 
have been conducted with the assumption that 
the identity of the author can be determined 
based upon his or her unique writing style 
features. [3] For example, in the 1700’s, 
Edmond Malone questioned whether or not 
Shakespeare really wrote some of the plays 
bearing his name, in the 1960’s, Mosteller and 
Wallace investigated the authorship of the 
Federalist Papers [4], parts of the Bible have 
been studied to determine consistency of 
authorship, and very recently the book, 
“Primary Colors” was studied to determine the 
identity of the anonymous author. Large feature 
sets have been used to determine authorship for 
many of these works and have been based upon 
lexical, syntactic, content, and complexity 
features. [4, 5, 6, 7] This was a long, pains-
taking, manual process centuries ago however 
with the advent of the computer, automated 
methods for feature selection and authorship 
attribution have proven to be quite successful. 
[8, 9] 
Today, one of the challenges that investigators 
face in trying to determine authorship attribution 
is that frequently the only evidence they have to 
work with is a small body of text such as email. 
There are several reasons why email in 
particular posses such a challenge to stylometric 
investigators. First, large feature sets do not 
work well with a small text sample such as 
email. Because the style of writing emails does 
not follow the same style used in prose writing, 
people tend to write shorter sentences skewing 
C1.2 
the sentence length feature calculation and also 
tend to use shorter words skewing the word 
length feature calculation. In addition, email 
writers are more likely to eliminate many 
function words, thereby making it difficult to 
calculate the word frequency feature. Second, 
authorship attribution needs to include more 
than just the email message itself because the 
information contained in the email header or 
attachments can also provide valuable clues to 
an author’s identity. [8]   Third, the format or 
structure of an email can also provide valuable 
clue to an author’s identity. [8] And lastly, some 
email writer’s may use IM (instant message) 
type abbreviations in their writing such as ‘lol’ 
or ‘btw’ which can represent different phrases 
depending upon the context of the message or 
sentence.  
With the proliferation of email correspondence 
and sometimes the use of email for 
inappropriate behavior including criminal 
activity or terrorism, it is important that 
companies and law enforcement be able to 
identify authorship when necessary. 
The goal of this paper is to explain the 
methodology of developing and optimizing a 
stylometry program. This program, in 
conjunction with the Java applet for data capture 
from an earlier biometric study, can be used by 
any researcher attempting to identify keystroke 
patterns in long-text passages such as e-mails. 
Relevance in the context of other work  
The use of stylometry in the most basic form 
has been utilized for as far back as the year 1430 
when Lorenzo Valla attempted to prove that the 
Donation of Constantine was a forgery, an 
argument based partly on a comparison of the 
Latin with that used in authentic 4th century 
documents. [11] 
These techniques have also been used in the 
study of authorship problems of the English 
Renaissance drama when researchers attempted 
to use distinctive patterns from specific 
playwrights to identify authors in uncertain or 
collaborative works. [12] 
More recent studies of author identification of 
email messages have also been published. Many 
publications have discussed the study of 
stylometry in the context of forensic analysis to 
assist in criminal investigations. One such study 
was published in 2001 when Oliver de Vel 
discussed the forensic analysis of text by 
examining characteristics of individual writing 
styles. de Vel also provides a good example of 
stylometry, or author identification when 
discussing the investigation of the Unabomber. 
Forensic analysis of the Unabomber manifesto 
showed distinct and irregular writing 
characteristics. The use of the analysis helped 
authorities finally apprehend the criminal. [8] 
Methodology - The Stylometry Program 
Written in C#, this program combines a simple 
GUI atop the three-phase approach described in 
further detailed: data collection, feature 
extraction, and classification. Users are able to 
select a set of sample emails (with labels for 
known authors) and a set of testing emails by 
unknown authors. The user is also able to select 
from a menu of event selections. Currently 
supported, for example, are two different 
choices — open file and batch process, an 
option to open and analyze several files at once. 
In stylometry, there are important decisions 
made concerning the features to be selected and 
the methods to be used [9]. Sixty-two stylistic 
features are considered for this program. The 
lists of features are provided in Table 1.  
For a comparative analysis, the raw frequency 
counts of each stylistic feature are normalized 
from a 0 to 1 range and classified by the k-
nearest neighbor algorithm using Euclidean 
distance. Authorship of the unknown email is 
then assigned from a majority vote of the k-
nearest neighbor query. Individuals interested in 
seeing or using this program should contact Dr. 
Charles Tappert [ctappert@pace.edu]. 
C1.3 
Data Collection 
The goal was to reproduce the original text from 
the raw keystroke data [Figure 1] that was 
supplied by Dr. Villani. This data included the 
following fields: Entry #, Key, Keycode, 
Location, Press, Release, Duration and Latency.  
The relevant information for this project was the 
Key and Keycode fields.  
The Key contained the actual keystroke(s) that 
were pressed on the keyboard. The Keycode 
was used to identify any non-printing keys such 
as Shift, Alt, Backspace, etc. 
Two products were to be produced from the 
extraction: a reconstruction of the original text 
and a “dirty” file that contained every keystroke 
entered. Any non-printing keystrokes were to 
also be included in the dirty file [Figure 2]. 
Process 
The first step was to make sure the first line was 
skipped as it only contained author information. 
The tilde (~) was used as a delimiter and since 
only the second and third fields were needed 
(Key and Keycode, respectively) those were 
pulled from the raw data. 
The raw data was extracted line by line and 
continually appended to a final string in order to 
reproduce the original text. However, each key 
needed to be analyzed in order to determine 
whether or not it was a printable or non-
printable character. The non-printable characters 
were entered as either a “?” or “” in the raw 
data file and they were replaced with a 
<Backspace>, <Enter> etc. to enable the dirty 
data to be easily readable 
Feature Extraction 
As per Professor Westcott’s needs, there were 
numerous counts to be performed on the 
regenerated raw text received from Dr. Villani. 
Some of these included the number of sentences 
per paragraph, average word length, number of 
words, paragraphs and average number of words 
per paragraph [Figure 2]. 
Process 
Retrieving most of the statistics was straight-
forward. However, the statistics for counting the 
number of sentences that begin in either 
uppercase or lowercase has proven a bit more 
complicated. For example, the sentence, "To 
which P.O. box would Dr. Smith send his 
Figure 1.  Example of Raw Data File 
C1.4 
information?" (without additional logical) 
would derive a count of four sentences with the 
following statistics: 1) Starts with an uppercase 
T, 2) starts with an uppercase O, 3) starts with a 
lowercase b, 4) starts with an uppercase T.  
While the above example is probably unusual, 
statistics can still be drawn from it, although 
those statistics accuracy will be diminished. 
Additional research into Parts-of-Speech 
software and/or regular expressions will be 
needed in order to find the best way to 
accurately determine these statistics. 
Averages were arrived at by simple division. 
For example, dividing the ‘Number of Words’ 
by the ‘Number of Paragraphs’ yielded the 
‘Average Number of Words per Paragraph.’ 
Careful planning was implemented to be certain 
both numbers were ascertained before the 
calculation was performed. 
Classification 
Using ‘train and test on separate files’ mode of 
operation, the output from the feature extractor 
is stored in train set, and the test set has 
extracted features for unknown author raw data.  
The measurements of the features extracted 
were standardized by converting raw 
measurement x to x’ by the formula,  
minmax
min'
xx
xxx
−
−
=  
where min and max are the minimum and 
maximum of the measurement over all samples 
from all subjects. This provides measurement 
values in the range 0-1 to give each 
measurement roughly equal weight, i.e. a 
normalized value. 
A Nearest Neighbor classifier, using Euclidean 
distance, compares the features of the test data 
set against those of the samples in the training 
data set. The top 10 training data sets with the 
smallest Euclidean distance to the test data set 
are identified. The unknown author name for 
each of test data set was declared based on the 
majority vote in k-nearest neighbor files [Figure 
3]. 
Figure 2.  Illustration of Data Collection 
[Reconstructed Email and Dirty File] and Feature 
Extraction [Statistics] 
Figure 3.  Illustration of Classification Phase  
C1.5 
Results 
Two types of experiments were conducted. 
First, a base data set of 596 raw keystroke data 
files from Dr. Villaini’s project was constructed. 
The raw keystroke data files were classified as 
“freedesk” where the author wrote an email in 
free text form and used a desktop keyboard. 
Approximately four types of freedesk emails 
were provided by each author. A rudimentary 
analysis revealed the following characteristics 
of this base data set: 
# of Authors participated  134 
Average # of paragraphs  2.40 
Average # of sentences  8.52 
Average # of words   115 
Average # of words per paragraph 50.4 
Average # of words per sentence 15.4 
Average word length   4.67 
Average # of white space  117 
Average number of commas  3 
Average # of periods   8 
 
Twenty test emails were extracted from the base 
set and compared in order to test the accuracy of 
the program identifying the author. 80% of the 
emails were correctly identified.  
 
The second experiment involved a base data set 
with fifteen plain text emails from 
communications among the team, client and 
professor. A rudimentary analysis revealed the 
following characteristics of this base data set: 
# of Authors participated  6 
Average # of paragraphs  10.8 
Average # of sentences  11.15 
Average # of words   157 
Average # of words per paragraph 14.4 
Average # of words per sentence 13.15 
Average word length   5.2 
Average # of white space  157 
Average number of commas  7.8 
Average # of periods   10.5 
 
Five test emails were then compared to the base 
data set using the set of statistics that were 
extracted from each file. A list of the next 
nearest neighbors was displayed in a list box on 
screen. 80% of the emails were correctly 
identified by the program. 
Conclusions 
This program is a simple, easy-to-use tool to 
permit users, without specialized training, to 
apply a stylometric analysis of biometric 
keystrokes for authorship attribution. It provides 
a starting point for testing and evaluating the use 
of stylometry for email author identification. 
Additional work will be needed to implement an 
array of methods, to determine and apply 
additional features, and to establish a suitable 
user-friendly interface.  
This program focused on keystroke analysis 
which included average word-lengths and 
average sentence-lengths, along with statistics 
like frequency of a period, number of commas, 
etc. (see Table 1). It is simple to visualize 
hundreds of such statistics, but it might be 
problematic to resolve which ones are more 
relevant for authorship attribution. One recent 
resolution to this problem is the support vector 
machine, which combines a number of statistics 
by charting a position for each author in an n-
dimensional space and measuring the distance 
between [10]. 
Additionally, there is an open source project 
called OpenNLP. This is a “group of open 
source projects related to natural language 
processing.” Information about this project can 
be found at this website: 
http://www.codeproject.com/csharp/englishparsi
ng.asp 
Either using the above resource or research into 
other natural language processors could 
potentially expand the ability to collect specific 
statistics and its overall accuracy during 
collection. 
Further documentation and contact information 
is available at the research team’s website: 
C1.6 
http://utopia.csis.pace.edu/cs615/2006-
2007/team2. 
 
 
 
 
 
References 
[1]  Holmes, D. 1998. The Evolution of Stylometry in 
Humanities Scholarship. Literary and Linguistic 
Computing.  
[2]  Holmes. D. 1994. Authorship Attribution. Computers 
and the Humanities. 
[3]  Harald Baayen, Hans van Halteren, Anneke Neijt, Fiona 
Tweedie. An experiment in authorship attribution. JADT 
2002 : 6es Journ´ees internationales d’Analyse 
statistique des Donn´ees Textuelles 
[4]  Mosteller, F. and Wallace, D. L. (1964). Inference and 
Disputed Authorship: The Federalist. Reading, Mass.: 
Addison Wesley. 
[5]  Baayen, H., H. van Halteren, F. Tweedie (1996). 
Outside the cave of shadows: Using syntactic annotation 
to enhance authorship attribution, Literary and 
Linguistic Computing, 11, 1996. 
Table 1. List of 62 features measured 
1. Number of Accents 
2. Number of Left curly braces 
3. Number of Right curly braces 
4. Number of Vertical lines 
5. Number of Tildes 
6. Number of Windows keys 
7. Number of Up keys 
8. Number of Left Shift keys 
9. Number of Right Shift keys 
10. Number of Page Down keys 
11. Number of Insert keys 
12. Number of Home keys 
13. Number of End keys 
14. Number of Down keys 
15. Number of Ctrl keys 
16. Number of Context menu keys 
17. Number of Caps Lock keys 
18. Number of Alt keys 
19. Number of F12 keys 
20. Number of Right keys 
21. Number of Backspace keys 
22. Number of Enter keys 
23. Number of Delete keys 
24. Number of Tab keys 
25. Number of words 
26. Number of sentences 
27. Average words per sentence 
28. Number of paragraphs 
29. Average words per paragraph 
30. Average word length 
31. Number of sentences beginning with upper case
Table 1 Continued 
32. Number of sentences beginning with lower case
33. Number of White spaces 
34. Number of  exclamation points 
35. Number of Number signs 
36. Number of Dollar signs 
37. Number of percent signs 
38. Number of Ampersands 
39. Number of Single quotes 
40. Number of Left parentheses 
41. Number of Right parentheses 
42. Number of Asterisks 
43. Number of Plus signs 
44. Number of Commas 
45. Number of Dashes 
46. Number of Periods 
47. Number of Forward slashes 
48. Number of Colons 
49. Number of Semi-colons 
50. Number of Less than signs 
51. Number of Equal signs 
52. Number of Greater than signs 
53. Number of Question marks 
54. Number of multiple question marks 
55. Number of multiple exclamation marks 
56. Number of ellipsis 
57. Number of At signs 
58. Number of Left square brackets 
59. Number of Back slashes 
60. Number of Right square brackets 
61. Number of Carot signs 
62. Number of Underscores 
C1.7 
[6]  Stamatatos, E., N. Fakotakis & G. Kokkinakis, (2001). 
Computer-based authorship attribution without lexical 
measures, Computers and the Humanities 35, pp. 193—
214. 
[7]  Holmes, D. I. and Forsyth, R. S. (1995). The federalist 
revisited: New directions in authorship attribution. 
Literary and Linguistic computing, 10(2):111-126 
[8]  O. de Vel, A. Anderson, M. Corney and G. Mohay. 
Mining E-mail Content for Author Identification 
Forensics. SIGMOD Record, Vol. 30, No. 4, December 
2001 
[9]  Mealand D. (1997). Measuring Genre Differences in 
Mark with Correspondence Analysis. Literary and 
Linguistic Computing, vol. (12/4). 
[10]  Vapnik, V. Statistical Learning Theory. Wiley: New 
York, NY. 
[11]  Romaine, Suzanne. Socio-Historical Linguistics. 
Cambridge, Cambridge University Press, 1982. 
[12]  Can, F., Patton, J. M. "Change of writing style with 
time." Computers and the Humanities. Vol. 38, No. 1 
(2004), pp. 61-82. 
