ISMIR 2008 – Session 4b – Musical Expression and Meaning
HYBRID NUMERIC/RANK SIMILARITY METRICS FOR
MUSICAL PERFORMANCE ANALYSIS
Craig Stuart Sapp
CHARM, Royal Holloway, University of London
craig.sapp@rhul.ac.uk
ABSTRACT
This paper describes a numerical method for examining
similarities among tempo and loudness features extracted from
recordings of the same musical work and evaluates its effec-
tiveness compared to Pearson correlation. Starting with corre-
lation at multiple timescales, other concepts such as a perfor-
mance “noise-floor” are used to generate measurements which
are more refined than correlation alone. The measurements are
evaluated and compared to plain correlation in their ability to
identify performances of the same Chopin mazurka played by
the same pianist out of a collection of recordings by various
pianists.
1 INTRODUCTION
As part of the Mazurka Project at the AHRC Centre for the His-
tory and Analysis of Recorded Music (CHARM), almost 3,000
recordings of Chopin mazurkas were collected to analyze the
stylistic evolution of piano playing over the past 100 years of
recording history, which equates to about 60 performances of
each mazurka. The earliest collected performance was recorded
on wax cylinders in 1902 and the most recent posted as home-
made videos on YouTube. Table 1 lists 300 performances of
five mazurkas which will be used for evaluation later in this pa-
per since they include a substantial number of recordings with
extracted tempo and loudness features.
Table 1. Collection of musical works used for analysis.
For each of the processed recordings, beat timings in the per-
formance are determined using the Sonic Visualiser audio edi-
tor 1 for markup and manual correction with the assistance of
several vamp plugins. 2 Dynamics are then extracted as smoo-
thed loudness values sampled at the beat positions.[3] Feature
data will eventually be extracted from all collected mazurkas in
the above list, but comparisons made in Section 3 are based on
1 http://www.sonicvisualiser.org
2 http://sv.mazurka.org.uk/download
the processed performance counts in Table 1. Raw data used
for analysis in this paper is available on the web. 3
Figure 1 illustrates extracted performance feature data as a
set of curves. Curve 1a plots the beat-level tempo which is
calculated from the duration between adjacent beat timings in
the recording. For analysis comparisons, the tempo curve is
also split into high- and low-frequency components with lin-
ear filtering. 4 Curve 1b represents smoothed tempo which
captures large-scale phrasing architecture in the performance
(note there are eight phrases in this example). Curve 1c repre-
sents the difference between Curves 1a and 1b which is called
here the desmoothed tempo curve, or the residual tempo. This
high-frequency tempo component encodes temporal accentu-
ation in the music used by the performer to emphasize par-
ticular notes or beats. Mazurka performances contain signifi-
cant high-frequency tempo information, since part of the per-
formance style depends on a non-uniform tempo throughout
the measure—the first beat usually shortened, while the second
and/or third beat are lengthened. Curve 1d represents the ex-
tracted dynamics curve which is a sampling of the audio loud-
ness at each beat location.
Other musical features are currently ignored here, yet are
important in characterizing a performance. In particular, pi-
anists do not always play left- and right-hand notes together,
according to aural traditions, although they are written as si-
multaneities in the printed score. Articulations such as legato
and staccato are also important performance features but are
equally difficult to extract reliably from audio data. Nonethe-
less, tempo and dynamic features are useful for developing nav-
igational tools which allow listeners to focus their attention on
specific areas for further analysis.
Figure 1. Extracted musical features from a recording of
Chopin’s mazurka in B minor, 30/2: a) tempo between beats;
b) smoothed tempo; c) residual tempo (c = a− b); and d) beat-
level dynamics.
3 http://mazurka.org.uk/info/excel
4 The filtering method is available online at
http://mazurka.org.uk/software/online/smoother .
501
ISMIR 2008 – Session 4b – Musical Expression and Meaning
2 DERIVATIONS AND DEFINITIONS
Starting with the underlying comparison method of correlation
(called S0 below), a series of intermediate similarity measure-
ments (S1, S2, and S3) are used to derive a final measurement
technique (S4). Section 3 then compares the effectiveness of
S0 and S4 measurements in identifying recordings of the same
performer out of a database of recordings of the same mazurka.
2.1 Type-0 Score
As a starting point for comparison between performance fea-
tures, Pearson correlation, often called an r-value in statistics,
is used:
Pearson(x, y) =
∑
n
(xn − x̄)(yn − ȳ)√∑
n
(xn − x̄)2
∑
n
(yn − ȳ)2
(1)
This type of correlation is related to dot-product correlation
used in Fourier analysis, for example, to measure similarities
between an audio signal and a set of harmonically related sinu-
soids. The value range for Pearson correlation is−1.0 to +1.0,
with 1.0 indicating an identical match between two sequences
(exclusive of scaling and shifting), and the value 0.0 indicat-
ing no predictable linear relation between the two sequences x
and y.
Correlation values between extracted musical features typi-
cally have a range between 0.20 and 0.97 for different perfor-
mances of mazurkas. Figure 2 illustrates the range of correla-
tions between performances in two mazurkas. Mazurka 17/4 is
a more complex composition with a more varied interpretation
range, so the mode, or most-expected value, of the correlation
distribution is 0.67. Mazurka 68/3 is a simpler composition
with fewer options for individual interpretations so the mode is
much higher at 0.87.
These differences in expected correlation values between
two randomly selected performances illustrate a difficulty in
interpreting similarity directly from correlation values. The
correlation values are consistent only in relation to a particu-
lar composition, and these absolute values cannot be compared
directly between different mazurkas. For example, a pair of
performances which correlate at 0.80 in mazurka 17/4 indicates
a better than average match, while the same correlation value
in mazurka 68/3 would be a relatively poor match. In addition,
correlations at different timescales in the same piece will have
a similar problem, since some regions of music may allow for a
freer interpretation while other regions may have a more static
interpretation.
Figure 2. Different compositions will have different expected
correlation distributions between performances.
Figure 3. Scapeplot for the dynamics in Horowitz’s 1949 per-
formance of mazurka 63/3 with the top eight matching perfor-
mances labeled.
2.2 Type-1 Score
In order to compensate partially for this variability in correla-
tion distributions, scapeplots were developed which only dis-
play nearest-neighbor performances in terms of correlation at
all timescales for a particular reference performance.[3] Ex-
amples of such plots created for the Mazurka Project can be
viewed online. 5
The S1 score is defined as the fraction of area each target
performance covers in a reference performer’s scapeplot. Fig-
ure 3 demonstrates one of these plots, comparing the dynamics
of a performance by Vladimir Horowitz to 56 other recordings.
In this case, Rachmaninoff’s performance of the same piece
matches better than any other performance, since his perfor-
mance covers 34% of the scape’s plotting domain. At second
best, Zak’s performance matches well towards the end of the
music, but covers only 28% of the total plotting domain. Note
that Zak’s performance has the best correlation for the entire
feature sequence (S0 score) which is represented by the point at
the top of the triangle. The S1 scores for the top eight matches
in Figure 3 are listed in Table 2. There is general agreement be-
tween S1 and S0 scores since five top-level correlation matches
also appear in the list.
2.3 Type-2 Score
Scape displays are sensitive to the Hatto effect: if an identical
performance to the reference, or query, performance is present
in the target set of performances, then correlation values at all
time resolutions will be close to the maximum value for the
identical performance, and the comparative scapeplot will show
a solid color. All other performances would have an S1 score of
approximately 0 regardless of how similar they might otherwise
seem to the reference performance. This property of S1 scores
is useful for identifying two identical recordings, but not useful
for viewing similarities to other performances which are hidden
behind such closely neighboring performances.
One way to compensate for this problem is to remove the
best match from the scape plot in order to calculate the next
best match. For example, Figure 4 gives a rough schematic for
5 http://mazurka.org.uk/ana/pcor-perf
502
ISMIR 2008 – Session 4b – Musical Expression and Meaning
Figure 4. Schematic of nearest-neighbor matching method
used in comparative timescapes.
how scapeplots are generated. Capital ‘Q’ represents the query,
or reference, performance, and lower-case lettered points repre-
sent other performances. The scapeplot basically looks around
in the local neighborhood of the feature space and displays
closest matches as indicated by lines drawn towards the query
on the right side of Figure 4. Closer performances will tend to
have larger shadows on the query, and some performances can
be completely blocked by others, as is the case for point “a” in
the illustration.
An S2 score measures the coverage area of the most dom-
inant performance which is assumed to be most similar to the
reference performance. This nearest of the neighbors is then
removed from the search database, and a new scapeplot is gen-
erated with the remaining performances. Gradually, more and
more performances will be removed which allows for previ-
ously hidden performances to appear in the plot. For example,
point “a” in Figure 4 will start to become visible once point “b”
is removed. S2 scores and ranks are independent. As fewer
and fewer target matches remain, the S2 scores will increase
towards 1.0 while the S2 ranks decrease towards the bottom
match.
In Figure 3, Rachmaninoff is initially the best match in terms
of S1 scores, so his performance will be removed and a new
scapeplot generated. When this is done, Zak’s performance
then represents the best match, covering 34% of the scapeplot.
Zak’s performance is then removed, a new scapeplot is calcu-
lated, and Moravec’s performance will have the best coverage
at 13%, and so on. Some of the top S2 scores for Horowitz’s
performance are listed in Table 2.
2.4 Type-3 Score
Continuing on, the next best S2 rank is for Chiu, who has 20%
coverage. Notice that this is greater than Moravec’s score of
13%. This demonstrates the occurrence of what might be called
the lesser Hatto effect: much of Moravec’s performance over-
lapped onto Chiu’s region, so when looking only at the near-
est neighbors in this manner, there are still overlap problems.
Undoubtedly, Rachmaninoff’s and Zak’s performances mutu-
ally overlap each other in Figure 3 as well. Both of them are
good matches to Horowitz, so it is difficult to determine ac-
curately which performance matches “best” according to S2
scores since they are interfering with each others scores and
are both tied at 34% coverage.
In order to define a more equitable similarity metric and re-
move the Hatto effect completely, all performances are first
ranked approximately by similarity to Horowitz using either
Figure 5. Schematic of the steps for measuring an S3 score: (1)
sort performances from similar to dissimilar, (2) remove most
similar performance to leave noise floor, (3) & (4) insert more
similar performances one-by-one to observe how well they can
occlude the noise-floor.
S0 values or the rankings produced during S2 score calcula-
tions. Performances are then divided into two groups, with the
poorly-matching half defined as the performance “noise-floor”
over which better matches will be individually placed.
To generate an S3 score, the non-noise performances are re-
moved from the search database as illustrated in step 2 of Fig-
ure 5, leaving only background-noise performances. Next, non-
noise performances are re-introduced separately along with all
of the noise-floor performances and scapeplot is generated. The
coverage area of the single non-noise performance represented
in the plot is defined as its S3 similarity measurement with re-
spect to the query performance.
This definition of a performance noise-floor is somewhat ar-
Figure 6. Dynascapes for Horowitz’s performance of mazurka
63/3. Top left is a plot of the noise-floor performances, and
the other three plots separate include one of the top matching
performances which can cover most of the noise-floor.
503
ISMIR 2008 – Session 4b – Musical Expression and Meaning
Table 2. Scores and rankings for sample targets to Horowitz’s
1949 performance of mazurka 63/3.
bitrary but splitting the performance database into two equal
halves seems the most flexible rule to use, and is used for the
evaluation section later in this paper. But the cut-off point could
be a different percentage, such as the bottom 75% of ranked
scores, or an absolute cut-off number. In any case, it is prefer-
able that the noise floor does not appear to have any favored
matches, and should consist of uniform small blotches at all
timescales in the scapeplot representing many different per-
formers as is the example shown in Figure 6 (top left part of
the figure). While Rachmaninoff and Zak have equivalent S2
scores, Rachmaninoff’s performance is able to cover 74% of
the noise-floor, while Zak’s is only able to cover 64%.
2.5 Type-4 Score
Type-3 scores require one additional refinement in order to be
useful since performances are not necessarily evenly distributed
in the feature space. The plots used to calculate the S3 scores
are still nearest-neighbor rank plots, so the absolute numeric
distances between performances are not directly displayed. Un-
like correlation values between two performances, S3 scores
are not symmetric: the score from A to B is not the same value
as from B to A. It is possible for an outlier performance to match
well to another performance closer to the average performance
just because it happens to be facing towards the outlier, with
the similarity just being a random coincidence.
Therefore, the geometric mean is used to mix the S3 score
with the reverse-query score (S3r) as shown in Equation 4.
S3 = A ⇒ B measurement (2)
S3r = A ⇐ B measurement (3)
S4 =
√
S3 S3r (4)
The arithmetic mean could also be used, but the geometric
mean is useful since it penalizes the final score if the type-3
and its reverse scores are not close to each other. For exam-
ple, the arithmetic mean between 0.75 and 0.25 is 0.50, while
the geometric mean is lower at 0.43. Greatly differing S3 and
S3r scores invariably indicate a poor match between two perfor-
mances, with one of them acting as an outlier to a more central
group of performances.
Table 2 shows several of the better matches to Horowitz’s
performance in mazurka 63/3, along with the various types of
scores that they generate. S0 is the top-level correlation be-
tween the dynamics curves, and R0 is the corresponding sim-
ilarity rankings generated by sorting S0 values. Likewise, S4
Table 3. Rankings for 17/4 Rubinstein performances. Shaded
numbers indicate perfect performance of a similarity metric.
and R4 indicate the final proposed similarity metric, and the
resulting rankings generated by sorting these scores.
3 EVALUATION
When evaluating similarity measurement effectiveness, a use-
ful technique with a clear ground-truth is to identify recordings
by the same performer mingled among a larger collection of
recordings.[5] Presumably pianists will tend to play more like
their previous performances over time rather than like other
pianists. If this is true, then better similarity metrics should
match two performances by the same pianist more closely to
each other than to other performances by different pianists.
3.1 Rubinstein performance matching
Arthur Rubinstein is perhaps the most prominent interpreter of
Chopin’s compositions in the 20th century, and luckily he has
recorded the entire mazurka cycle three times during his career:
(1) in 1938–9, aged 51; (2) in 1952–4, aged 66, and (3) in 1966,
aged 79.
Table 3 lists the results of ranking his performances to each
other in mazurka 17/4 where the search database contains an
additional 60 performances besides the three by Rubinstein.
The first column in the table indicates which performance was
used as the query (Rubinstein’s 1939 performance, for exam-
ple, at the top of the first row). The target column indicates
a particular target performance which is one of the other two
performances by Rubinstein in the search database. Next, five
columns list three types of rankings for comparison. The five
columns represent four different extracted features as illustrated
in Figure 1, plus the TD column which represents a 50/50 ad-
mixture of the tempo and dynamics features.
For each musical feature, three columns of rankings are re-
ported. R0 represents the rankings from the S0 scores; R3
being the type-3 scoring ranks, and R4 resulting from sorting
the S4 similarity values. In these columns, a “1” indicates that
the target performance was ranked best in overall similarity to
the query performance, “2” indicates that it is the second best
match, and so on (see the search database sizes in Table 1). In
the ranking table for mazurka 17/4 performances of Rubinstein,
the shaded cells indicate perfect performance matches by a par-
ticular similarity metric where the top two matches are both
Rubinstein. Note that there is one perfect pair of matches in
all of the R0 columns which is found in the full-tempo feature
504
ISMIR 2008 – Session 4b – Musical Expression and Meaning
Figure 7. Ability of metrics to identify the same performer in a
larger set of performances, using 3 performances of Rubinstein
for each mazurka. (Lower rankings indicate better results.)
when Rubinstein 1939 is the target performance. No columns
for R3 contain perfect matching pairs, but about 1/2 of the R4
columns contain perfect matches: all of the full-tempo R4 rank-
ings are perfect, and a majority of the desmoothed tempo and
joint tempo/dynamics rankings are perfect. None of the metrics
contain perfect matching pairs for the dynamics features. This
is perhaps due to either (1) the dynamics data containing mea-
surement noise (due to the difficultly of extracting dynamics
data from audio data), or (2) Rubinstein varying his dynamics
more over time than his tempo features, or a combination of
these two possibilities.
Figure 7 shows the average rankings of Rubinstein perfor-
mances for all extracted features averaged by mazurka. The fig-
ure shows S4 scores are best at identifying the other two Rubin-
stein performances for all of the five mazurkas which were used
in the evaluation. Typically S4 gives three to four times better
rankings than the S0 values according to this figure. S3 scores
(used to calculate S4 scores), are slightly better than plain cor-
relation, but sometimes perform worse than correlation in some
mazurkas.
Figure 8 evaluates the average ranking effectiveness by mu-
sical feature, averaged over all five mazurkas. Again S4 scores
are always three to four times more effective than plain corre-
lation. S3 scores are approximately as effective as S0 rankings
for full and smoothed tempo, but perform somewhat better on
residual tempo and dynamics features, probably by minimizing
the effects of sudden extreme differences between compared
feature sequences caused by noisy feature data.
Table 4. Performer self-matching statistics.
Figure 8. Ranking effectiveness by extracted musical fea-
tures, using three performances of Rubinstein for each ma-
zurka. (Lower values indicate better results.)
3.2 Other performers
Rubinstein tends to vary his performance interpretation more
than most other pianists. Also, other performers may tend to
emulate his performances, since he is one of the more promi-
nent interpreters of Chopin’s piano music. Thus, he is a diffi-
cult case to match and is a good challenge for similarity metric
evaluations.
This section summarizes the effectiveness of the S0 and S4
similarity metrics in identifying other pianists found in the five
selected mazurkas for which two recordings by the same pi-
anist are represented in the data collection (only Rubinstein is
represented by three performances for all mazurkas).
Table 4 presents ranking evaluations for performance pairs
in a similar layout to those of Rubinstein found in Table 3. In
all except two cases (for Fou) the S4 metrics perform perfectly
in identifying the other performance by the same pianist. Top-
level correlation was able to generate correct matches in 75%
of the cases. An interesting difference between the two met-
rics occurs when Hor71 is the query performance. In this case
S0 yields a rank of 13 (with 12 other performance matching
better than his 1985 performance), while S4 identifies his 1985
performance as the closest match.
Fou’s performance pair for mazurka 30/2 is also an interest-
ing case. For his performances, the phrasing portion of the full-
and smoothed-tempo features match well to each other, but the
tempo residue does not. This is due to a significant change in
his metric interpretation: the earlier performance has a strong
mazurka metric pattern which consists of a short first beat, fol-
lowed by a lengthened second or third beat in each measure.
His 2005 performance greatly reduces this effect, and beat du-
rations are more uniform throughout the measure in compari-
son to his 1978 performance.
Finally, it is interesting to note the close similarity between
Uninsky’s pair of performances listed in Table 4. These two
performances were recorded almost 40 years apart, one in Rus-
sia and the other in Texas. Also, the first was recorded onto 78
RPM monophonic records, while the later was recorded onto
33-1/3 RPM stereo records. Nonetheless, his two performances
indicate a continuity of performance interpretation over a long
career.
505
ISMIR 2008 – Session 4b – Musical Expression and Meaning
Table 5. Comparison of Cortot performances of mazurka 30/2
(m. 1–48).
4 APPLICATION
As an example application of the derived similarity metrics,
two performances of mazurka 30/2 performed by Alfred Cortot
are examined in this section. One complete set of his mazurka
performances can be found on commercially release recordings
from a 1980’s-era issue on cassette tape “recorded at diverse
locations and dates presumably in the period of 1950–1952.”[1]
These recordings happen to be issued by the same record label
as the recordings of Joyce Hatto, which casts suspicion on other
recordings produced on that label.[4]. A peculiar problem is
that no other commercial recordings exist of Cortot playing any
mazurka, let alone the entire mazurka cycle.
In 2005, however, Sony Classical (S3K89698) released a 3-
CD set of recordings by Cortot played during master classes he
conducted during the late 1950’s, and in this set, there are six
partial performances of mazurkas by Cortot where he demon-
strates how to play mazurkas to students during the class. His
recording of mazurka 30/2 on these CDs is the largest contin-
uous fragment, including 75% of the entire composition, stop-
ping two phrases before the end of the composition.
Table 5 lists the S4 scores and rankings for these two record-
ings of mazurka 30/2, with the Concert Artist’s rankings on the
left, and the Sony Classical rankings to the right. The five dif-
ferent musical features listed by column in previous tables for
Rubinstein and other pianists are listed here by row. For each
recording/feature combination, the top three matches are listed,
along with the ranking for the complimentary Cortot recording.
Note that in all cases, the two Cortot recordings match very
poorly to each other. In two cases, the worst possible ranking
of 35 is achieved (since 36 performances are being compared in
total). Perhaps Cortot greatly changes his performances style in
the span of 6 years between these two recordings late in his life,
although data from Tables 3 and 4 would not support this view
since no other pianists has significantly alter all musical fea-
tures at once, and only Fou significantly changes one musical
feature between performances.
Therefore, it is likely that this particular mazurka recording
on the Concert Artist label was not actually performed by Cor-
tot. Results from further investigation of the other five partial
mazurka performances on the Sony Classical recordings would
help to confirm or refute this hypothesis, but the other examples
are more fragmentary, making it difficult to extract reasonable
amounts of recital-grade performance material. In addition, no
performer in the top matches for the Concert Artist Cortot per-
formance match well enough to have likely recorded this per-
formance, so it is unlikely that any of the other 30 or so per-
formers being compared to this spurious Cortot performance is
the actual source for this particular mazurka recording.
5 FUTURE WORK
Different methods of combining S3 and S3r scores, such as
measuring the intersection between plot areas rather than mea-
suring the geometric mean to calculate S4 should be examined.
The concept of a noise floor when comparing multiple perfor-
mances is useful for identifying features which are common or
rare, and allows similarity measurements to be more consistent
across different compositions, which may aid in the identifica-
tion of pianists across different musical works.[6]
Further analysis of the layout of the noise-floor as seen in
Figure 6 might be useful in differentiating between directly and
indirectly generated similarities between performances. For ex-
ample in this figure, Rachmaninoff’s performance shows more
consistent similarity towards smaller-scale features, which may
indicate a direct influence on Horowitz’s performance style.
Zak’s noise-floor boundary in Figure 6 may demonstrate an in-
direct similarity, such as a general school of performance.
Since the similarity measurement described in this paper
works well for matching the same performer in different record-
ings, and examination of student/teacher similarities may be
done. The analysis techniques described here should be appli-
cable to other types of features, and may be useful with other
underlying similarity metrics besides correlation. For example,
It would be interesting to extract musical features first from
the data with other techniques such as Principle Component
Analysis[2] and use this derived feature data for characterizing
the similarities between performances in place of correlation.
6 REFERENCES
[1] Chopin: The Mazurkas. Alfred Cortot, piano. Concert
Artist compact disc CACD 91802 (2005).
[2] Repp, B.H. “A microcosm of musical expression: I. Quan-
titative analysis of pianists’ timing in the initial measures
of Chopin’s Etude in E major,” Journal of the Acoustical
Society of America, 104 (1998). pp. 1085–1100.
[3] Sapp, C. “Comparative analysis of multiple musical perfor-
mances,” Proceedings of the 8th International Conference
on Music Information Retrieval, Vienna, Austria, 2007.
pp. 497–500.
[4] Singer, M. “Fantasia for piano: Joyce Hatto’s incredible ca-
reer,” New Yorker, 17 Sept. 2007. pp. 66–81.
[5] Stamatatos, E. and G. Widmer. “Automatic identification of
music performers with learning ensembles,” Artificial Intel-
ligence, 165/1 (2005). pp. 37–56.
[6] Widmer, G., S. Dixon, W. Goebl, E. Pampalk, and A. Tobu-
dic. “In search of the Horowitz factor,” AI Magazine, 24/3
(2003). pp. 111–130.
506
