JOURNAL OF THE AMERICAN SOCIETY FOR INFORMATION SCIENCE AND TECHNOLOGY, 57(11):1506–1518, 2006
Current document-retrieval tools succeed in locating
large numbers of documents relevant to a given query.
While search results may be relevant according to the
topic of the documents, it is more difficult to identify
which of the relevant documents are most suitable for a
particular user. Automatic genre analysis (i.e., the ability
to distinguish documents according to style) would be a
useful tool for identifying documents that are most
suitable for a particular user. We investigate the use of
machine learning for automatic genre classification. We
introduce the idea of domain transfer—genre classifiers
should be reusable across multiple topics—which does
not arise in standard text classification. We investigate
different features for building genre classifiers and their
ability to transfer across multiple-topic domains. We also
show how different feature-sets can be used in conjunc-
tion with each other to improve performance and reduce
the number of documents that need to be labeled.
Introduction
There is a vast amount of information available to the
casual user today mainly due to the proliferation of the
World Wide Web; however, it has become difficult to find
the information that is most appropriate to a given query.
While users can usually find relevant information, it is
increasingly difficult to isolate information that is suitable in
terms of style or genre. Current search services take a “one
size fits all approach,” taking little account of the individual
user’s needs and preferences. These techniques succeed in
identifying relevant documents, but the large number of
documents relevant to a given query can make it difficult to
isolate those documents that are most relevant to that query.
Achieving high recall while maintaining precision is very
challenging. The huge volume of information available indi-
cates that new techniques are needed to filter the relevant
documents and identify the information that best satisfies a
user’s information need.
We explore the use of genre to address this issue. By
genre we loosely mean the style of text in the document. 
A genre class is a class of documents that are of a similar
type. This classification is based not on the topic of the doc-
ument but rather on the kind of text used. We have identified
automatic genre analysis as an additional tool that can com-
plement existing techniques and improve the results re-
turned to a user. Genre information could be used to filter or
re-rank documents deemed relevant. The relevance of a par-
ticular document to a given query is dependent on the partic-
ular user issuing the query. We believe that the genre or style
of text in a document can provide valuable additional infor-
mation when determining which documents are most rele-
vant to a particular user’s query.
Machine learning has been widely used to categorize
documents according to topic. In automatic text classifica-
tion, a machine-learning algorithm is given a set of examples
of documents of different topics and uses these examples to
learn to distinguish documents. We consider the use of
machine-learning techniques to automatically categorize
documents according to genre.
The ability to identify the style of text used in a docu-
ment would be a valuable service in any text-retrieval
system. For example, consider a query about “chaos the-
ory.” Different users will require documents which assume
different levels of expertise depending on the user’s techni-
cal background. It would be useful to be able to rank docu-
ments according to the level of technical detail with which
they present their subject. Current information-retrieval
systems would be greatly enhanced by the ability to filter
documents according to their genre class. A high-school
student may require documents that are introductory or
tutorial in style while a college professor may require schol-
arly research documents.
As another example, consider news filtering according to
the topic of the article. Such a service would be improved by
the ability to filter the news articles according to different
genre classes. For example, consider a financial analyst who
tracks daily news about companies in which she is inter-
ested. It would be useful to be able to further classify these
documents as being subjective or objective. One class of
documents would present the latest news about the various
companies of interest while the other class would contain the
opinions of various columnists and analysts about these
Accepted March 24, 2005
© 2006 Wiley Periodicals, Inc. • Published online 14  July 2006 in Wiley
InterScience (www.interscience.wiley.com). DOI: 10.1002/asi.20427
Learning to Classify Documents According to Genre
Aidan Finn and Nicholas Kushmerick
School of Computer Science and Informatics, University College Dublin, Dublin, Ireland.
E-mail: {aidan.finn, nick}@ucd.ie
JOURNAL OF THE AMERICAN SOCIETY FOR INFORMATION SCIENCE AND TECHNOLOGY—September 2006 1507
DOI: 10.1002/asi
companies. Depending on circumstances, the user may require
documents of one class or the other.
Another genre class with useful application is the ability
to identify whether a document is describing something in a
positive or a negative way. This could be used to improve a
recommender system. Products could be recommended on
the basis that they were given a positive review by a reviewer
with similar interests to the target user.
Another application of review classification is the filter-
ing of news-wire articles for financial analysis. Financial
analysts must quickly digest large amounts of information
when making investment decisions. A delay of a few seconds
in identifying important information can result in significant
gains or losses. The ability to automatically identify whether
news about a company is positive or negative would be a
valuable service in such a situation (Lavrenko et al., 2000).
The ability to filter documents according to the level of
technical information presented and the readability of the
document would enable a system to personalize documents
retrieved according to the user’s educational background.
With a suitable set of genre classes, a system with a dual cat-
egory structure that allowed users to browse documents
according to both topic and genre would be useful. Genre
analysis can facilitate improved personalization by recom-
mending documents that are written in a style that the user
finds interesting or a style that is appropriate to the user’s
needs. We consider genre to be complimentary to topic as a
method of recommendation. The two used in conjunction
can improve the quality of a user’s recommendations.
In this article, we make the following contributions:
• To investigate the feasibility of genre classification using
machine learning. We wish to investigate whether machine
learning can successfully be applied to the task of genre
classification.
• To investigate how well different feature-sets perform on 
the task of genre classification. Using two sample genre-
classification tasks, we perform experiments using three dif-
ferent feature-sets and investigate which features satisfy the
criteria for building good genre classifiers.
• To investigate the issues involved in building genre classi-
fiers with good domain transfer. The task of genre classifica-
tion requires additional methods of evaluation. We introduce
the idea of domain transfer as an indication of the perfor-
mance of a genre classifier across multiple-topic domains.
We evaluate each of the feature-sets for their ability to pro-
duce classifiers with good domain transfer.
• To investigate how we can apply active learning techniques
to build classifiers that perform well with small amounts of
training data.
• To investigate methods of combining multiple feature-sets to
improve classifier performance.
Genre Classification
In the introduction, we gave a general outline of what we
mean by genre. Here, we define our interpretation in more
detail, give several examples, and compare our definition
with previous definitions from related research.
What Is Genre?
The term “genre” occurs frequently in popular culture.
Music is divided into genres based on differences in style
(e.g., blues, rock, or jazz). Sample genres from popular fiction
include science fiction, mystery, and drama. Genres are often
vague concepts with no clear boundaries and need not be
disjoint. For a given subject area, there is no fixed set of
genre categories. Identifying a genre taxonomy is a subjec-
tive process, and people may disagree about what constitutes
a genre or the criteria for membership of a particular genre.
The American Heritage Dictionary of the English Language
(Pickett et al., 2000) defines genre as “a category of artistic com-
position, as in music or literature, marked by a distinctive style,
form or content.” Webster’s Third New International Dictio-
nary, unabridged (Gove, 2002) defines a genre as “class; form;
style esp. in literature.”Wordnet (http://wordnet. princeton.edu)
defines genre as “1: a kind of literary or artistic work 2: a style of
expressing yourself in writing 3: a class of artistic endeavor
having a characteristic form or technique.”
Swales (1990) gave a working definition of genre. A genre
is defined as a class of communicative events where there is
some shared set of communicative purposes. This is a loose
definition, and any particular instance of a genre may vary in
how closely it matches the definition; however, instances of
a genre will have some similarity in form or function.
Karlgren (2004) distinguished between a style and a genre.
A style is a consistent and distinguishable tendency to make
certain linguistic choices.Agenre is a grouping of documents
that are stylistically consistent and intuitive to accomplished
readers of the communication channel in question.
From the different definitions, we see that there is no
definitive agreement on what is meant by genre; however,
the common thread among these definitions is that genre
relates to style. The genre of a document reflects a certain
style rather than being related to the content. In general, this
is what we mean when we refer to the genre of a document:
The genre describes something about what kind of document
it is rather than what topic the document is about.
Genre is often regarded as orthogonal to topic. Documents
that are about the same topic can be from different genres.
Similarly, documents from the same genre can be about dif-
ferent topics. Thus, we must separate the identification of the
topic and genre of a document and try to build classifiers that
are topic independent. This contrasts with the aim of other
text-classification tasks; thus, the standard methods of evalu-
ating text classifiers are not completely appropriate. This
suggests the notion of domain transfer—whether genre clas-
sifiers trained on documents about one topic can successfully
be applied to documents about other topics.
We explicitly distinguish between the topic and style of
the document. While assuming that genres are stylistically
different, we investigate the effect of topic on our ability to
distinguish genres. When we evaluate our genre classifiers,
we measure how well they perform across multiple-topic
domains. For genre classification techniques to be generally
useful, it must be easy to build genre classifiers. There are
1508 JOURNAL OF THE AMERICAN SOCIETY FOR INFORMATION SCIENCE AND TECHNOLOGY—September 2006
DOI: 10.1002/asi
Football
Politics
Finance
“Liverpool have revealed
they have agreed a fee
with Leeds United for
striker Robbie Fowler—
just hours after caretaker
boss Phil Thompson had
said that contract talks
with the player were
imminent.”
“Al Gore picked up votes
Thursday in Broward
County as election
officials spent
Thanksgiving weekend
reviewing questionable
presidential ballots.”
“In a move that sent
Enron shares higher after
days of double-digit
declines, Dynegy
confirmed Tuesday that it
is in talks to renegotiate
its $9 billion deal to buy
its rival.”
“The departure of Robbie
Fowler from Liverpool
saddens me but does not
surprise me. What did
come as a shock, though,
was that the club should
agree terms with Leeds,
one of their chief rivals
for the Championship.”
“Democrats are desperate
and afraid. The reality
that their nominee for
President has a
compulsive tendency to
make things up to make
himself look good is
sinking in.”
“The collapse of Enron is
hard to believe, and even
harder to understand. But
in retrospect, there are
some valuable lessons in
the whole mess.”
TABLE 1. Examples of objective and subjective articles from three topic
domains.
Fact Opinion
two aspects to this: (a) Domain transfer: classifiers should be
generally applicable across multiple topics, and (b) learning
with small amounts of training data. When building genre
classifiers, we want to achieve good performance with a
small number of examples of the genre class.
Genres depend on context, and whether a particular genre
class is useful depends on how useful it is for distinguishing
documents from the user’s point of view; therefore, genres
should be defined with some useful user function in mind. In
the context of the Web, where most searches are based on the
content of the document, useful genre classes are those that
allow a user to usefully distinguish between documents
about similar topics.
To summarize, we view a genre as a class of documents
that arises naturally from the study of the language style and
text used in the document collection. Genre is an abstraction
based on a natural grouping of documents written in a simi-
lar style and is orthogonal to topic. It refers to the style of
text used in the document. A genre class is a set of docu-
ments written in a similar style which serves some useful,
discriminatory function for users of the document collection.
Sample Genre Classes
We focused on two sample genres which we use for our
automatic genre-classification experiments. These two gen-
res were identified as functionally useful for Web users:
(a) whether a news article is subjective (i.e., it presents the
opinion of its author) or objective, and (b) whether a review
is positive or negative.
The first genre class we investigate is whether a docu-
ment is subjective or objective. This is a common distinction
in newspaper articles and other media. Many news articles
report some significant event objectively. Other articles,
which often take the form of columns or editorials, offer the
author’s opinion.
Consider the example of financial news. Financial news
sites publish many articles each day. Articles of genre-class
fact may be reporting the latest stock prices and various
events that are likely to influence the stock price of a partic-
ular company. Articles of genre-class opinion may give the
opinions of various financial analysts as to the implications
of the events of the day for future stock prices. Different
users at different times may be better served by articles from
one genre or the other. It would be a useful service for the
user to be able to filter or retrieve documents from each of
these genre classes.
Our second sample genre class is classifying reviews as
being either positive or negative. The ability to automati-
cally recognize the tone of a review could have application
in collaborative recommendation systems. For example, if a
particular movie critic who generally has similar tastes to a
user gives a film a positive review, then that film could be
recommended to the user. Films could be recommended on
the basis of how they are reviewed by critics known to have
similar tastes to a particular user.
Tables 1 and 2 show a selection of document extracts
from our document collection. A human reader can recognize
a subtle difference in style between extracts from subjective
and objective articles and similarly between the positive and
negative reviews. We investigate techniques for automating
this classification.
Our aim in constructing the classifier is to maximize
accuracy both on a single topic and across topics. To this
end, we use datasets from three topic domains—football,
politics, and finance—for the subjectivity classification task
and documents from two topic domains—movie reviews
and restaurant reviews—for the review classification task.
We are interested in how well a classifier trained on docu-
ments from one domain performs in another. We identify the
different topics to determine how well the classifier performs
across multiple topics for the same genre-classification task.
If genre is orthogonal to topic, we should be able to build
classifiers that perform well on topics other than the one
used to build the classifier. For example, a classifier built to
recognize whether a document is subjective or objective by
training it on documents about football ideally should be
able to recognize subjective documents that are about topics
other than football, such as finance or politics (Figure 1).
The practical effort involved in building a genre classifier
is considerable. A human must label a number of examples
to train the classifier. It is essential to minimize this human
effort; therefore, we aim to build genre classifiers with good
domain transfer. Because of the amount of human effort
JOURNAL OF THE AMERICAN SOCIETY FOR INFORMATION SCIENCE AND TECHNOLOGY—September 2006 1509
DOI: 10.1002/asi
FIG. 1. Genre classification for domain transfer.
Related Work
One of the two sample genres we study in our experi-
ments is subjective versus objective news articles. Wiebe
(2000) defined subjectivity classification as distinguishing
sentences used to present opinions and evaluations from
sentences used to objectively present factual information.
She investigated subjectivity classification at the sentence
level and concluded that the presence and type of adjectives
in a sentence are indicative of whether the sentence is
subjective or objective. We seek to perform subjectivity
classification at the document level.
Tong (2001) described a system that focuses on tracking
various entities and the opinions being expressed about
them. The opinions are tracked by monitoring online public-
discussion forums. In particular, they monitor online discus-
sion about movies and determine the level of “buzz” associated
with specific movies as they move from announcement of
release through opening weekend and on to extended distrib-
ution. Opinions are extracted using sentiment models, which
are patterns that capture the way people talk about movies and
use a set of custom lexicons that cover personal emotions,
movie features, and language tone. These also are used to
model the tone of the opinion. It appears that they use heuris-
tics to identify positive and negative opinions being expressed
about particular movies. Our positive- versus negative-review
task seeks to automate this classification process.
Stamatatos, Fakotakis, and Kokkinakis (2000) recog-
nized the need for classifiers that can transfer easily to new
topic domains, without explicitly mentioning domain trans-
fer; however, they did not elaborate on how to evaluate
transfer. Their notion of genre is similar to ours.
Their feature-set is the most frequently occurring words
of the entire written language, and they showed that the fre-
quency of occurrence of the most frequent punctuation
marks contains very useful stylistic information that can
enhance the performance of an automatic text-genre classi-
fier. This approach is domain and language independent and
requires minimal computation. They did not perform any
experiments to measure the performance of their classifier
when it was transferred to new topic domains.
This work is closely related to ours. They identified the
need for domain transfer, but did not further develop this
idea. Their definition of text genre is similar to ours, and two
of the genre classes they identified are similar to our subjec-
tivity classification task. The features they used (viz., stop-
words and punctuation), are similar to our text-statistics
feature-set.
Kessler, Nunberg, and Schutze (1997) argued that genre
detection based on surface cues is as successful as detection
based on deeper structural properties. Argamon, Koppel, and
Avneri (1998) considered two types of features: lexical and
pseudosyntactic. They compare the performance of function
words against part-of-speech trigrams for distinguishing
between different sets of news articles.
Roussinov et al. (2001) viewed genre as a group of docu-
ments with similar form, topic, or purpose, “a distinctive
Movie
Restaurant
“Almost Famous:
Cameron Crowe’s first
film since “Jerry
Maguire” is so engaging,
entertaining and authentic
that it’s destined to
become a rock-era
classic. Set in 1973, this
slightly fictionalized,
semi-autobiographical,
coming-of-age story
revolves around a baby-
faced 15 year old prodigy
whose intelligence and
enthusiasm land him an
assignment from “Rolling
Stone” magazine to
interview Stillwater, an
up-and-coming band.”
“Though the New
American menu at this
neighbourhood treasure
near Capitol Hill is ever
changing, it’s always
beautifully conceived and
prepared and based on
mostly organic
ingredients; the bistro
dishes, paired with a
fabulous, descriptive
wine list, are served in an
offbeat atmosphere.”
“Vanilla Sky: Presumably
Cameron Crowe and Tom
Cruise have some
admiration for “Abre Los
Ojos” the 1998 Spanish
thriller from Alejandro
Amenabar; why else
would they have chosen
to do an English-language
remake? “Vanilla Sky”,
however shows that
respect for ones source
material isn’t enough. It’s
a misbegotten venture
that transforms a flawed
but intriguing original
into an elephantine,
pretentious mess.”
“Hidden in the back of a
shopping mall near
Emory, this Chinese
eatery is so isolated that
diners sometimes feel as
if they’re having a private
meal out; the decor isn’t
much to look at and the
foods nothing special but
it’s decent.”
TABLE 2. Examples of positive and negative reviews from two topic
domains.
Positive Negative
involved in constructing a genre classifier, it should be
reusable across multiple-topic domains. If it has to be
retrained every time it is to be used in a new topic domain, the
amount of work required to maintain it will be considerable; in
a high-volume digital-library scenario, it could be prohibitive.
1510 JOURNAL OF THE AMERICAN SOCIETY FOR INFORMATION SCIENCE AND TECHNOLOGY—September 2006
DOI: 10.1002/asi
type of communicative action, characterized by a socially
recognized communicative purpose and common aspects of
form” (p. 4013). This is a more general view of genre, where
genre is a grouping of similar documents. Some genres are
defined in terms of purpose or function while others are
defined in terms of physical form, but most documents com-
bine the two. Roussinov et al. attempted to identify genres
that Web users frequently face and proposed a collection of
genres that are better suited for certain types of information
need. To this end, they performed a user survey to identify
(a) the purpose for which users search the Web and (b) whether
there was a relation between the purpose of a respondent’s
search and the genre of document retrieved. This resulted in
a proposed set of genres, along with a set of features for each
genre and a user interface for genre-based searching.
Dewdney, VanEss-Dykema, and McMillan (2001) took
the view that genre of a document is the format style. They
define genre as a name or label that serves to identify a set of
conventions used to present information. The conventions
cover both formatting and the style of language used. They
use two feature-sets: a set based on words (traditional bag-of-
words) and a set of presentation features which represent styl-
istic information about the document. They found that pre-
sentation features do consistently better in the sense of higher
classification accuracy than the word-frequency features, and
combining the feature-sets gives a slight improvement. They
concluded that linguistic and format features alone can be
used successfully for sorting documents into different genres.
Rauber and Muller-Kogler (2001) argued that in a tradi-
tional library, non-content-based information such as age of
a document and whether it looks frequently used are impor-
tant distinguishing features and present a method of automatic
analysis based on various surface-level features of the docu-
ment. The approach uses a self-organizing map (SOM;
Kohonen, 1990) to cluster the documents according to struc-
tural and stylistic similarities. This information is then used
to graphically represent documents. In this approach, the
genres are identified from clusters of documents that occur
in the SOM rather than being defined in advance.
Karlgren and colleagues (Karlgren, 1999; Karlgren,
Bretan, Dewe, Hallberg, & Wolkert, 1998; Karlgren &
Cutting, 1994) did several experiments in genre classification.
In 1999, Karlgren showed that texts judged relevant to a set of
Text Retrieval Conference (TREC) queries differed systemat-
ically (in terms of style) from the texts that were not relevant.
In another study, Karlgren et al. (1998) used topical clustering
in conjunction with stylistics-based genre prediction to build
an interactive information-retrieval engine and to facilitate
multidimensional presentation of search results. They built a
genre palette by interviewing users and identifying several
genre classes that are useful for Web filtering. The system was
evaluated by users given particular search tasks. The partici-
pants did not do well on the search tasks, but all but one
reported that they liked the genre-enhanced search interface.
Participants used the genres in the search interface to filter
search results. The search interface described is an example of
how genre classification can aid information retrieval.
Automated Genre Classification
The Machine Learning approach to document classifica-
tion uses a set of preclassified examples to induce a model
which can be used to classify future instances. The classifier
model is automatically induced by examination of the training
examples. The human effort in this process is in assembling
the labeled examples and choosing a representation for the
training examples. A human must initially decide what fea-
tures will be used to describe the training examples, and repre-
sent the training documents with respect to these features.
When using Machine Learning algorithms, we first iden-
tify the concept to be learned. In our case, this is the particu-
lar genre class we are attempting to classify. The output of
the learning algorithm is a concept description that ideally
should be both intelligible and operational. The concept
description should be intelligible in the sense that it can be
understood, discussed, disputed, and interrogated by humans.
It also should be operational in the sense that we can practi-
cally apply it to future examples.
The type of learning we are interested in is classification
learning. In this learning scheme, the learner takes a set of
labeled, preclassified examples. The learner is then expected
to induce ways of classifying unseen examples based on the
preclassified examples given. This form of learning is super-
vised in that the training examples are provided and labeled
by a human overseer.
The training data are a set of instances. Each instance is a
single example of the concept to be learned. Instances are
characterized by a set of attributes where each attribute
measures a certain aspect of the concept being described.
Attributes can be nominal or continuous. Continuous attrib-
utes represent some numerical value that can be measured.
Nominal attributes are categorical, and assign the attribute to
membership of a particular category.
Representing the classification problem as a set of
instances is a restrictive way of formulating the learning
problem. Each instance is characterized by values of a set of
predetermined attributes. The selection of these attributes
can affect the quality of the classifier produced by the learn-
ing algorithm. As part of our experiments, we are interested
in identifying attributes which perform well on the genre-
classification task, can be easily extracted automatically, and
are useful across multiple topics. We used C4.5 (Quinlan,
1993) as our main learning algorithm. C4.5 is a machine-
learning algorithm that induces a decision tree from labeled
examples and can be converted easily to a set of rules for a
human to analyze.
We identified three different sets of features and investi-
gate the utility of each of these for genre classification. Fur-
thermore, we attempted to identify the features which will
lead to classifiers that perform well across multiple-topic
domains and can easily be built automatically. We used two
sample genre tasks to test the utility of three sets of features
for the purpose of automatic genre classification.
We emphasized the ability to transfer to new topic domains
when building our classifiers, and we evaluated different
feature-sets for performance across multiple-topic domains.
JOURNAL OF THE AMERICAN SOCIETY FOR INFORMATION SCIENCE AND TECHNOLOGY—September 2006 1511
DOI: 10.1002/asi
In addition to building classifiers that transfer easily to new
domains, we wanted to minimize the effort involved in
building a genre classifier. We aimed to achieve good per-
formance; that is, prediction accuracy as a function of
amount of training data, with a minimum amount of labeled
data. To this end, we examined the learning rates of our clas-
sifiers and investigated methods of improving this learning
rate using active learning techniques.
The three feature-sets investigated can be thought of as
three independent views of the dataset. We investigated
methods of combining the models built using each feature-set
to improve classifier performance.
Features
We explored three ways to encode a document as a vector
of features: Bag-of-Words (BOW), Part-of-Speech (POS)
Statistics, and Text Statistics.
BOW
The first approach represented each document as a BOW,
a standard approach in text classification. A document is
encoded as a feature-vector, with each element in the vector
indicating the presence or absence of a word in the document.
We wanted to determine how well a standard keyword-based
learner performs on this task. This approach led to feature-
vectors that are large and sparse. We used stemming (Porter,
1980) and stop-word removal to reduce the size of the feature-
vector for our document collection.
This approach to document representation works well for
standard text classification where the target of classification
is the topic of the document; however, in the case of genre
classification, the target concept is often independent of the
topic of the document, so this approach may not perform as
well.
It is not obvious whether certain keywords would be
indicative of the genre of the document. We are interested in
investigating how well this standard text-classification
approach works on the genre-classification tasks. We expected
that a classifier built using this feature-set may perform well
in a single-topic domain, but not very well when domain
transfer is evaluated. By topic domain, we mean a group of
documents that can be regarded as being about the same gen-
eral subject or topic. For example, for the subjectivity classi-
fication task, we have three topic domains: football, politics,
and finance. For the review classification task, we have two
topic domains: restaurant reviews and movie reviews. The
reason we identify different topic domains is that a text
genre class may occur across multiple-topic domains. We
wished to evaluate the domain transfer of a genre classifier.
For example, if a classifier is trained for the subjectivity clas-
sification task using documents from the football domain,
how well does it perform when this classifier is transferred to
the new domain of politics?
It is common in text classification, where the aim it to
classify documents by content, to use a binary representation
for the feature-vector rather encoding the frequencies of the
word’s occurrences. It also is common to filter out com-
monly occurring words, as they do not usefully distinguish
between topics. We are interested in measuring domain
transfer, so we choose the binary vector representation.
POS Statistics
The second approach uses the output of Brill’s (1994)
POS tagger as the basis for its features. It was anticipated
that the POS statistics would sufficiently reflect the style of
the language for our learning algorithm to distinguish between
different genre classes. A document is represented as a vec-
tor of 36 POS features, one for each POS tag, expressed as a
percentage of the total number of words for the document.
The POS features are listed in Table 3. 
This approach uses a POS representation of the documents
rather than the actual words occurring in the document. It was
hoped that this would give a representation that was still
capable of discriminating genre, but independent of the sub-
ject of the document. The POS representation does not reflect
the topic of the document, but rather the type of text used in
the document.
We hope that POS features can be used to differentiate gen-
res in a domain-independent way. If the POS feature-set is
TABLE 3. Part-of-Speech features.
Tag Description Tag Description
CC Coordinating conjunction PP$ Possessive pronoun
CD Cardinal number RB Adverb
DT Determiner RBR Adverb, comparative
EX Existential there RBS Adverb, superlative
FW Foreign word RP Particle
IN Preposition or SYM Symbol
subordinating conjunction
JJ Adjective TO to
JJR Adjective, comparative UH Interjection
JJS Adjective, superlative VB Verb, base form
LS List item marker VBD Verb, past tense
MD Modal VBG Verb, gerund or present
participle
NN Noun, singular or mass VBN Verb, past participle
NNS Noun, plural VBP Verb, non-3rd-person
singular present
NP Proper noun, singular VBZ Verb, 3rd-person
singular present
NPS Proper noun, plural WDT Wh-determiner
PDT predeterminer WP Wh-pronoun
POS Possessive ending WP$ Possessive wh-pronoun
PP Personal pronoun WRB Wh-adverb
1512 JOURNAL OF THE AMERICAN SOCIETY FOR INFORMATION SCIENCE AND TECHNOLOGY—September 2006
DOI: 10.1002/asi
TABLE 4. Text statistic features.
Feature type Features
Document-level statistics sentence length, number of words,
word length
Frequency counts of because been being beneath can can’t
various function words certainly completely could couldn’t
did didn’t do does doesn’t doing don’t
done downstairs each early
enormously entirely every extremely
few fully furthermore greatly had
hadn’t has hasn’t haven’t having he
her herself highly him himself his
how however intensely is isn’t it its
itself large little many may me might
mighten mine mostly much musn’t
must my nearly our perfectly
probably several shall she should
shouldn’t since some strongly that
their them themselves therefore these
they this thoroughly those tonight
totally us utterly very was wasn’t we
were weren’t what whatever when
whenever where wherever whether
which whichever while who whoever
whom whomever whose why will
won’t would wouldn’t you your
Frequency counts of various ! ” $ % & ’ ( ) *  ,  . : ;  ?
punctuation symbols
capable of differentiating genre class, we would expect that it
would do so in a domain-independent manner because it does
not have any information about the topic of the document.
Text Statistics
Our third approach is to use a set of shallow text statistics.
Many of these features were selected because they have been
shown to have discriminatory value between genre classes in
related literature. This feature-set includes average sentence
length, the distribution of long words, and average word
length. Additional features are based on the frequency of
occurrence of various function words and punctuation sym-
bols. Table 4 lists the features used.
Experiments
We evaluated the three feature-sets using two real-world
genre-classification tasks.
Evaluation
We evaluated our classifiers using two measures: accu-
racy of the classifier in a single-topic domain and accuracy
when trained on one topic domain, but tested on another.
Single-domain accuracy. Single-domain accuracy mea-
sures the accuracy of the classifier when it is trained and
tested on instances from the same topic domain. This
measure indicates the classifier’s ability to learn the classifi-
cation task in the topic domain at hand.
Accuracy is defined as the percentage of the classifier’s
predictions that are correct as measured against the known
classes of the test examples. Accuracy is measured using
10-fold cross-validation.
Domain-transfer accuracy. Note that single-topic accu-
racy give us no indication of how well our genre classifier
will perform on documents from other topic domains; there-
fore, we introduced a new evaluation measure, domain
transfer, which indicates the classifier’s performance on
documents from other topic domains.
We measured domain transfer in an attempt to measure the
classifier’s ability to generalize to new domains. For example,
a genre classifier built using documents about football should
be able to recognize documents about politics from the same
genre. Domain transfer is essential in a high-volume digital-
library scenario, as it may be prohibitively expensive to train
a separate genre classifier for every topic domain.
We used the domain-transfer measure as an indicator of the
classifier’s generality. It also gives an indication of how much
the genre-classification task in question is topic dependent or
topic independent.
Domain transfer is evaluated by training the classifier on
one topic domain and testing it on another topic domain. In
addition to measuring the domain-transfer accuracy, we can
calculate the domain-transfer rate. This measures how much
the classifier’s performance degrades when the classifier is
evaluated on new topic domains. A classifier that performs
equally well in the transfer condition as in a single domain
would achieve a transfer score of 1. A classifier whose per-
formance degrades when transferred to new topic domains
would achieve a transfer score of less than 1.
Consider a classification task consisting of a learning
algorithm (C) and a set of features (F). Let D1, D2, . . . , Dn be
a set of topic domains. Let DAD be the performance of C
when evaluated using 10-fold cross-validation in Domain D.
Let D1AD2 denote the performance of classification Scheme C
when trained in Domain D1 and tested in Domain D2. We
will use accuracy as our measure of performance. We define
the domain transfer rate for classification Scheme C as
(1)
We evaluated the quality of a genre classifier using both
single-domain accuracy and domain-transfer accuracy. Ide-
ally, we would hope to get high single-domain accuracy and
a high domain-transfer rate (see Figure 2). A classifier with
high accuracy and low transfer may be useful in some situa-
tions, but not in others.
Experimental Setup and Document Corpora
Table 5 shows the number of documents in the corpus used
for the subjectivity classification experiment. We identified
CDTF 
1
n(n 1) a
n
i1
 a
n
j1; ji
aDi ADj
Di
ADi
b
JOURNAL OF THE AMERICAN SOCIETY FOR INFORMATION SCIENCE AND TECHNOLOGY—September 2006 1513
DOI: 10.1002/asi
TABLE 5. Corpus details for the subjectivity classification experiment.
Topic Opinion Fact Total
Football 174 177 351
Politics 144 145 289
Finance 56 100 156
TABLE 6. Corpus details for the review classification experiment.
Topic Positive Negative Total
Movie 386 337 723
Restaurant 300 331 631
three topic domains: football, politics, and finance. For each
topic, we identified a number of Web sites that specialize in
news from the particular topic domain. Articles were then
automatically spidered from these sites over a period of sev-
eral weeks. The documents were then classified by hand by
the author as being either subjective or objective.
Table 6 shows the details of our document collection for
the review experiment. The collection of review datasets
was somewhat easier than the collection of the subjectivity
datasets because the classification of a document could be
extracted automatically using a wrapper for the particular
site. For example, most movie reviews come with a recom-
mendation mark. A review that awards a film four stars could
be considered a positive review while a review that awards a
film one star could be considered a negative review. Thus,
we automatically extract the classification of a particular re-
view, negating the need to manually classify each document.
The movie reviews were downloaded from the Movie
Review Query Engine.1 This site is a search engine for
movie reviews that extracts movie reviews from a wide
range of sites. If the review contains a mark for the film, the
mark also is extracted. We wrote a wrapper to extract a large
number of movie reviews and their corresponding marks
from this site. The marks from various sites were normalized
by converting them to a percentage, and we then used docu-
ments with high percentages as examples of positive reviews
and vice versa. Marks below 41% were considered negative
while marks of 100% were considered positive. Reviews
with marks in the range 41 to 99% were ignored because
many of them would require a human to label them as posi-
tive or negative.
The restaurant reviews were gathered from the Zagat sur-
vey site.2 This is a site that hosts a survey of restaurants from
the United States and Europe. Users of the site submit their
comments about a particular restaurant and assign marks in
three categories: food, décor, and service. The marks for
these categories are between 1 and 30%, and are the average
for all the users that have provided feedback on that particu-
lar restaurant. The reviews themselves consist of an amalga-
mation of different users’ comments about the restaurant. We
averaged the marks for the three categories to get a mark for
each restaurant. Restaurants with an average mark below
15% were considered negative while those getting marks
above 23% were considered positive.
Evaluation Methods
The standard method of evaluating machine-learning
classifiers is to use cross-validation. We believe that for the
task of genre classification, this alone is not sufficient and
that an extra method of evaluation is needed. To test whether
genre classes are orthogonal to topic, we measured the clas-
sifier’s performance across topics as well as across genres.
We used standard cross-validation to measure accuracy3
within a single-topic domain. We also proposed a domain-
transfer measure to evaluate the classifier’s ability to gener-
alize across topic domains.
Figure 1 shows what we mean by domain transfer. In the
single-domain case, the classifier is trained and tested on
documents from the same topic domain. In the domain-
transfer case, the classifier is trained on documents from one
topic domain and tested on documents from another topic
domain.
Usually, text classification is applied to tasks where topic-
specific rules are an advantage. To scale with large numbers
of topics, this is not the case for genre classification. In the
case of genre classification, topic-specific rules reduce the
generality of the genre classifier. In addition to evaluating
FIG. 2. Desirable performance characteristics of a genre classifier. A use-
ful genre classifier should have both high single-domain accuracy and high
domain accuracy (Region 1). Performance in Region 2 can be useful, but
performance in Region 3 or 4 is undesirable.
1http://www.mrqe.com
2http://www.zagat.com
3Other measures could be used such as precision, recall or F-measure.
1514 JOURNAL OF THE AMERICAN SOCIETY FOR INFORMATION SCIENCE AND TECHNOLOGY—September 2006
DOI: 10.1002/asi
TABLE 7. Single-domain accuracy for subjectivity classification.
Topic BOW POS TS MVE
Football 86.7 82.4 86 88.6
Politics 85.3 83.1 80.3 90.8
Finance 89.7 88.6 83.3 92
Average 87.2 84.7 83.2 90.5
TABLE 8. Single-domain accuracy for review classification.
Topic BOW POS TS MVE
Movie 76.8 59.6 59 74.1
Restaurant 88.5 62.9 94.1 83.4
Average 82.7 61.3 76.6 78.8
the genre classifier’s performance in a single-topic domain,
we also need to evaluate its performance across multiple-
topic domains.
Figure 2 shows single-domain accuracy plotted against
domain-transfer rate, with areas of the graph labeled in order
of desirability. The most desirable classifiers would be in
Region 1 of the graph. These classifiers have both high sin-
gle-domain accuracy and high transfer rate. The next most
desirable are classifiers occurring in Region 2 of the graph.
These classifiers have high single-domain accuracy, but poor
domain transfer. Classifiers occurring in Regions 3 and 4 are
undesirable because they have poor levels of single-domain
accuracy; even if they have good transfer rates, they are not
useful in practice.
Choice of Learning Algorithm
We chose C4.5 as our learning algorithm because it gen-
erated a model that can be converted easily into a set of rules
that can be examined by a human observer. We performed
some initial experiments using different learning algorithms,
but none of them were substantially superior to C4.5 on
these tasks.
Single-Domain Experiments
Table 7 shows the single-domain experiments for the
subjectivity classification task (note that the Multiple Views
Ensemble [MVE] approach shown is described later). The
BOW feature-set performed best in all three topic domains.
The POS feature-set was second-best on average, although
the difference between it and the TS feature-set is insignifi-
cant. All three feature-sets achieved good accuracy on this
classification task, indicating that any of these feature-sets
alone is sufficient for building classifiers within a single-
topic domain; however, BOW was the best performing
feature-set on this task within topic domains. This indicates
that there are keywords within each topic domain that
signify the subjectivity of a document.
Table 8 shows the single-domain results for the review
classification experiment. In both domains, the BOW
approach performed significantly better than did the POS
approach. On average, the BOW approach achieved accuracy
of 82.7%. This is a good level of accuracy for this classifica-
tion task. The POS approach performed poorly in comparison
(61.3% on average).
The BOW approach, therefore, is capable of achieving
good levels of performance when attempting to classify
reviews as positive or negative in a single-topic domain.
The POS approach performed poorly on this classification
task, even in a single-topic domain. The TS approach per-
formed well in the restaurant domain (94.1%), but poorly
on the movie domain (59%). Thus, while its average perfor-
mance was good, it did not perform consistently well in
each domain.
Domain-Transfer Experiments
Table 9 shows domain-transfer results for the subjectivity
classification task. In this case, the POS feature-set per-
formed the best (78.5%) while the BOW feature-set per-
formed the worst (63.7%). Thus, BOW went from being best
when evaluated in a single-topic domain to the worst when
evaluated across multiple-topic domains. This indicates that
while keywords can be used to identify subjective docu-
ments, a model built using these features is more closely tied
to the document collection used for training. Intuitively, we
would expect that the classifier built using the POS statistics
as features would have a more generalizable model of what
constitutes genre than would one built using keywords or
domain-specific, hand-crafted features.
Table 10 shows the domain-transfer results for the review
classification experiment. On average, each feature-set per-
formed to a similar level, with there being less than 1% dif-
ference between them. Each feature-set achieved an average
accuracy of approximately 47%. This level of performance is
no better than that achievable by a simple majority classifier.
The single-domain experiment on this classification task
showed that BOW can achieve high levels of accuracy on
TABLE 9. Domain transfer for subjectivity classification.
Train Test BOW POS TS MVE
Football Politics 58.5 74 63.7 72.3
Football Finance 61.5 78.8 75.6 80.8
Politics Football 76.9 70.7 64.1 76.6
Politics Finance 66.7 90.4 66.7 75.6
Finance Football 76.9 73.2 70.7 81.5
Finance Politics 63 83.7 66.1 76.9
Average 67.3 78.5 67.8 77.3
TABLE 10. Domain transfer for review classification.
Train Test BOW POS TS MVE
Movie Rest 40.1 44.4 50.4 45.3
Rest Movie 55.5 49.8 44.3 52.9
Average 47.8 47.1 47.35 49.1
JOURNAL OF THE AMERICAN SOCIETY FOR INFORMATION SCIENCE AND TECHNOLOGY—September 2006 1515
DOI: 10.1002/asi
this classification task in a single-topic domain; however, the
domain-transfer experiment showed that the BOW approach
fails when the transfer approach is evaluated. The BOW
features which indicate a positive movie review are not
transferable to the restaurant domain, and vice versa. The
POS approach failed in both the single-domain and domain-
transfer experiments.
We conclude that the POS approach is not suitable for the
task of classifying reviews as being either positive or nega-
tive. The BOW approach can achieve good performance in a
single-topic domain, but cannot transfer to new topic domains.
Even though the traditional means of evaluating a classifier
indicate that the BOW achieves good performance, our
experiments indicate that it performs poorly when our extra
domain-transfer condition is evaluated.
Discussion
Our experiments show that it is possible to build genre
classifiers that perform well within a single-topic domain;
however, single-domain performance can be deceiving.
When we further evaluate the classifiers for domain-transfer
performance, it becomes clear that good domain transfer is
more difficult to achieve.
The review classification task is more difficult than is the
subjectivity classification task. All feature-sets achieved
good single-domain accuracy on the latter task while the
POS feature-set also achieved good domain transfer. On the
review classification task, the BOW approach achieved good
single-domain accuracy, but none of the feature-sets
achieved good domain transfer.
From examination of the dataset, reviews from the movie
domain are easily recognizable by a human reader as being
either positive or negative. It is more difficult to discern the
category for many of the restaurant reviews. Recall that the
reviews were classified automatically based on scores
extracted from the source Web site. The restaurant reviews
consisted of an amalgamation of user comments about a par-
ticular restaurant, and often it can be difficult for a reader to
decide whether the reviews are positive or negative. Because
the reviews combine different users’ comments, the style of
the restaurant reviews is different from the style of the movie
reviews, which are written by individual authors. This may
account for some poor performance when domain transfer
was evaluated for the review classification task.
It also is clear that no one feature-set is suitable for both
genre-classification tasks. The BOW feature-set performed
well in a single-topic domain while the POS feature-set per-
formed best on the subjectivity classification task when we
evaluated domain transfer.
Models Generated
We can examine the models generated by C4.5 to see
what features the classifier is using to make its prediction,
which may provide insight into the classification task in
question and confidence in the model being generated. If the
model provides rules that seem intuitively related to the
genre-classification task, this gives us confidence in the valid-
ity of the model. The model also may give us insight into the
genre class under investigation and which features differenti-
ate genre but are not obvious from inspection of the data.
The root node of a C4.5 decision tree is the attribute that
was deemed most informative with respect to discriminating
between the target classes. For the subjectivity classification
task, the BOW approach generated root nodes based on the
words “columnist,” “column,” and “column” for the foot-
ball, politics, and finance domains, respectively. The pres-
ence of these words is strongly indicative of a document
being subjective. It is easy to see that documents containing
these words are likely to be subjective in style, as they are
probably written by a particular columnist giving his or her
opinion. For the review classification task, the BOW approach
generated root nodes based on the words “jolie” and “ro-
mantic” for the movie and restaurant domains, respectively.
The word “jolie” occurring in a movie review means it is
likely to be negative while the word “romantic” occurring in
a restaurant review means it is likely to be positive. This cor-
responds the fact that the movie Tomb Raider (West, 2001)
starring Angelina Jolie was released around the time the
dataset was collected. One can imagine that this is not the
kind of movie that would appeal to film critics and would be
likely to garner negative reviews. It also seems unlikely that
this attribute would have any discriminatory value in the
restaurant-review domain. It also seems reasonable that the
word “romantic” used in relation to a restaurant is likely to in-
dicate a positive review; however, this attribute may in fact
penalize the classifier when transferred to the movie domain,
as it seems plausible that movie reviews containing the word
“romantic” are more likely to be negative rather than positive.
For the subjectivity classification task, the POS approach
generates trees with root nodes DT, RB, and RB for the foot-
ball, politics, and finance, domains respectively. DT refers to
the distribution of determiners (e.g., as, all, any, each, the,
these, those). RB refers to adverbs (e.g., maddeningly,
swiftly, prominently, predominately). Subjective documents
tend to have relatively more determiners and adverbs. On the
review classification task, the POS approach failed to accu-
rately discriminate between positive and negative reviews.
The TS approach generates trees with root nodes based
on the number of words in the document for the football and
politics domains and the distribution of the word “can” for
the finance domain. Shorter documents are more likely to be
objective. It seems likely that objective documents often will
be much shorter than are subjective documents because they
report only some item of news, without any discussion of the
event involved. It is not clear how the distribution of the
word “can” is indicative of the subjectivity of a document.
On the review classification task, the TS approach did not
perform well in the movie domain (59%), but performed sur-
prisingly well on the restaurant domain (94.1%). In this case,
the root node of the generated tree is the number of long
words in the document. Reviews containing a small number
of long words are more likely to be negative.
1516 JOURNAL OF THE AMERICAN SOCIETY FOR INFORMATION SCIENCE AND TECHNOLOGY—September 2006
DOI: 10.1002/asi
Combining Multiple Views
We have investigated the use of three different feature-
sets for the task of genre classification and attempted to
determine which features are better for building general,
transferable classifiers. Our experiments have shown that the
utility of each feature-set depends on the genre-classification
task at hand. We seek to automate as much as possible the
process of building a genre classifier. None of the feature-
sets are generally superior to the others, and it is undesirable
to have to determine the best feature-set for every new
genre-classification task. To further automate the construc-
tion of genre classifiers, we investigated methods of improv-
ing performance by combining feature-sets.
An Ensemble Learner
We can treat the three different feature-sets as different
independent views of the data. We can build a meta-classifier
that combines evidence from classifiers built using each fea-
ture-set to make its prediction.
There are several methods of combining classifiers to
make predictions. Bagging combines the predictions of sev-
eral separate models. The models are built using different
subsets of the training data, and each model votes on the
final prediction. Boosting is similar to bagging, except that
the votes of each model are weighted according to some
scheme such as the model’s success on the training data.
While bagging and boosting combine models of the same
type, stacking combines models built using different learning
models.
Our approach differs from these in that we will combine
models based on our different feature-sets. This multiview,
ensemble learning approach builds a model based on each of
the three feature-sets. A majority vote is taken to classify a
new instance.
The results achieved by the ensemble learner are encour-
aging. For the subjectivity classification task, the results
(Table 7) achieved by this approach (MVE) were better that
those achieved by any of the individual feature-sets. The
domain transfer (Table 9) is almost as good as that achieved
by POS and significantly better that that achieved by the
other feature-sets.
For the review classification task (Table 8), the MVE
approach performed better than did POS and TS, but not as
good as did BOW. In the domain-transfer case (Table 10),
the MVE approach performed best, on average.
This approach to classification exploits the fact that the
three different feature-sets do not all make mistakes on the
same documents. Thus, a mistake made by the model based
on one feature-set can be corrected by the models based on the
other feature-sets. This works best in situations where all three
feature-sets achieve good performance, such as the subjectiv-
ity classification task. When each feature-set performs well, it
is more likely to correct another feature-set’s mistakes.
In cases where some of the feature-sets perform poorly
(e.g., the review classification task), MVE will achieve per-
formance that is proportional to the relative performance of
the individual feature-sets.
It seems likely that for genre-classification tasks where it
is not clear which feature-set is most suitable for the task,
this approach will increase the likelihood of the classifier
performing well.
Multiview Selective Sampling
We aimed to minimize the human effort involved in
building a new genre classifier. To this end, we wanted to
actively select documents for labeling to achieve better per-
formance with less training data.
Figure 3 shows the first 100 points of the learning curves
for the subjectivity classification task averaged over the
FIG. 3. Multiview selective sampling on the subjectivity classification task.
JOURNAL OF THE AMERICAN SOCIETY FOR INFORMATION SCIENCE AND TECHNOLOGY—September 2006 1517
DOI: 10.1002/asi
three topic domains. The y axis shows the average accuracy
taken over 10 trials. The x axis shows the number of training
documents.
The naïve way of choosing documents to add to the train-
ing set is to choose a document at random. This approach is
shown for each of the three feature-sets (BOW_rand,
POS_rand, and TS_rand). The POS learning rate was better
on this task than was the learning rate for the other two
feature-sets.
We wanted to improve the learning rate of the genre clas-
sifiers. One method of improving the learning rate is to use
active learning to selectively sample documents to add to the
training set. The aim is to select training documents that will
most improve the learned model, thus achieving maximum
performance for minimal training data.
The approach we investigated to improve the learning
rate uses the level of agreement between the learned models
based on different feature-sets. The first document to be
labeled is selected at random. The next document to be
labeled is the one where the models based on the three dif-
ferent feature-sets disagree most about the classification.
Applying this approach to our subjectivity classification
task resulted in an improvement in learning rate for all three
feature-sets (i.e., BOW_al, POS_al, TS_al). For each fea-
ture-set, there is little difference initially between the random
and active learning approaches; however, as the classifica-
tion accuracy improves, the active learning approach begins
to exhibit a better learning rate than that with the random
approach. This indicates that the active learning approach
consistently chooses documents that improve the perfor-
mance of the classifier.
Conclusion
In theory, genre and topic are orthogonal; however, our
experiments indicate that in practice they partially overlap.
It may be possible to automatically identify genre in a topic-
independent way, but the results of our domain-transfer
experiments show that the feature-sets we investigated result
in models that are partially topic dependent.
From a single-topic point of view, our approach was very
successful. If we used only the usual methods of evaluation,
we would conclude that genre classification is not a difficult
task and can be achieved easily using standard machine-
learning techniques. On the subjectivity classification task,
all our feature-sets achieved high accuracy while; on the
review classification task, a standard BOW approach achieved
good accuracy.
We have argued that standard methods of evaluation are
not sufficient when evaluating genre classifiers and that, in
addition, the genre classifier’s ability to transfer to new topic
domains also must be evaluated. When we evaluate this
additional aspect of the genre classifiers, we find that it is
difficult to build classifiers that transfer well to new domains.
For the subjectivity classification task, we have shown
that it is possible to build a genre classifier that can automat-
ically recognize a document as being either subjective or
objective. High accuracy in a single-topic domain can be
achieved using any of the three feature-sets we investigated
(i.e., BOW, POS, or TS), but when domain transfer is mea-
sured for this task, the POS feature-set performs best. Over-
all, the POS feature-set is best for this genre-classification
task, as it performs well both in a single-topic domain and
when transferred to new topic domains.
The review classification task is more difficult. Good
accuracy can be achieved in a single-topic domain using the
BOW approach. The POS approach is not suitable for this
genre-classification task. All three feature-sets failed to
achieve good domain transfer on this task.
We also investigated methods of combining the predic-
tions of models based on the different feature-sets and
showed that this improves performance. This approach is
perhaps best when approaching a new genre-classification
problem where it is not clear which feature-set is most suit-
able for the task.
We also showed that the learning rate of the genre classi-
fier can be improved by actively selecting which document
to add to the training set. This selection is based on the level
of disagreement of models built using each feature-set.
These two approaches further facilitate the aim of
automating as much as possible the process of building
genre classifiers. All three feature-sets can be extracted auto-
matically. The ensemble learning approach can give good
performance on the genre-classification task, and the active
learning approach can improve performance on small
amounts of training data.
Future Work
We identified two sample genre-classification tasks.
These particular genre classes could be usefully applied to
improve existing information-retrieval systems. Applica-
tions that utilize genre classification to provide noticeable
benefits to the end user must be developed to determine
whether genre classification can be a useful, practical tech-
nique for improving document-retrieval systems.
In building such systems, it will be useful to identify
additional genres that can improve a user’s ability to filter
documents and reduce the number of documents that are
potentially relevant to them. An expanded genre taxonomy
is needed together with appropriate techniques for automati-
cally identifying genres. We found that the techniques that
were successful on one genre-classification task (i.e., subjec-
tivity classification) were less successful on another genre
classification task (i.e., review classification).
The ability to achieve good domain transfer is important
for genre classifiers. The techniques we used did not provide
a complete separation of genre and topic. Further investiga-
tion is needed to determine methods of identifying genre in
a topic-independent way. We also need to refine methods of
evaluating domain transfer and determine how to meaning-
fully compare the performance of different genre classifiers.
Ideally, once a general genre taxonomy is defined, we need
techniques for automatically constructing genre classifiers
1518 JOURNAL OF THE AMERICAN SOCIETY FOR INFORMATION SCIENCE AND TECHNOLOGY—September 2006
DOI: 10.1002/asi
within this taxonomy. One would hope that there are general
techniques that could be used to build all classifiers for all
genres within a taxonomy and that these genre classifiers will
transfer easily to new topic domains; however, our experience
has shown that this is difficult. Methods for achieving these
aims need further investigation.
Other feature-sets could be generally useful for building
genre classifiers. The addition of further feature-sets also
may improve the performance of the ensemble learner and
active learning approaches.
In general, future work should extend the research on two
genre-classification tasks to a general genre taxonomy. Clas-
sifiers built to identify genre classifiers within this genre tax-
onomy should be easy to build and domain independent. The
other major area for future work is to implement applica-
tions that use genre classification to improve the user’s
experience.
Acknowledgments
This research was funded by Science Foundation Ireland
and the U.S. Office of Naval Research. Thanks to Barry
Smyth for his advice and assistance.
References
Argamon, S., Koppel, M., & Avneri, G. (1998). Routing documents accord-
ing to style. First International Workshop on Innovative Information
Systems, Boston, MA.
Brill, N. (1994). Some advances in transformation-based part of speech
tagging. Proceedings of the 12th National Conference on Artificial
Intelligence (pp. 722–727).
Dewdney, N., VanEss-Dykema, C., & McMillan, R. (2001). The form is the
substance: Classification of genres in text. Paper presented at the Work-
shop on Human Language Technology and Knowledge Management,
Conference of the Association of Computational Linguistics.
Gove, P.B. (2002). Webster’s third new international dictionary, unabridged.
Springfield, MA: Merriam-Webster.
Karlgren, J. (1999). Stylistic experiments in information retrieval. In
T. Strzalkowski (Ed.), Natural language information retrieval. Dordrecht,
The Netherlands: Kluwer.
Karlgren, J. (2004). The wheres and whyfores for studying text genre
computationally. Workshop on Style and Meaning in Language, Art,
Music, and Design, 19th National Conference on Artificial Intelligence,
Washington, DC.
Karlgren, J., Bretan, I., Dewe, J., Hallberg, A., & Wolkert, N. (1998). Itera-
tive information retrieval using fast clustering and usage-specific genres.
In the 8th DELOS Workshop on User Interfaces in Digital Libraries
(pp. 85–92), Stockholm, Sweden.
Karlgren, J., & Cutting, F. (1994). Recognizing text genres with simple met-
rics using discriminant analysis. In Proceedings of the 15th International
Conference on Computational Linguistics (Vol. II, pp. 1071–1075),
Kyoto, Japan.
Kessler, B., Nunberg, G., & Schutze, H. (1997). Automatic detection of text
genre. Proceedings of the 35th annual meeting of the Association for
Computational Linguistics (pp. 32–38).
Kohonen, T. (1990). The self-organising map. Proceedings of IEEE, 78(9),
1464–1479.
Lavrenko, V., Schmill, M., Lawrie, D., Ogilvie, P., Jensen, D., & Allan, J.
(2000). Mining of concurrent text and time-series. Proceedings of the
Sixth ACM SIGKDD International Conference on Knowledge Discovery
and Data Mining (pp. 37–44).
Pickett, J.P., et al. (Eds.). (2000). The American heritage dictionary of the
English language (4th ed.). Boston: Houghton Mifflin.
Porter, M.F. (1980). An algorithm for suffix stripping. Program, 14(3),
130–137.
Quinlan, R. (1993). C4.5: Programs for machine learning. San Mateo, CA:
Morgan Kaufman.
Rauber, A., & Muller-Kogler, A. (2001). Integrating automatic genre analysis
into digital libraries. In E.A. Fox & C.L. Borgman (Eds.), Proceedings of
the First ACM/IEEE-CS Joint Conference on Digital Library (pp. 1–10).
Roussinov, D., Crosswell, K., Nilan, M., Kwasnik, B., Cai, J., & Liu, X.
(2001). Genre based navigation of the Web. In 34th International Confer-
ence on System Sciences (p. 4013).
Stamatatos, E., Fakotakis, N., & Kokkinakis, G. (2000). Text genre detec-
tion using common word frequencies. Proceedings of the 18th Confer-
ence on Computational Linguistics (pp. 808–814).
Swales, J.M. (1990). Genre analysis. Cambridge, UK: Cambridge University
Press.
Tong, R.M. (2001). An operational system for detecting and tracking
opinions in on-line discussions. In SIGIR Workshop on Operational Text
Classification Systems, New Orleans, LA.
West, S. (Director) (2001). Tomb raider [Motion picture]. United Kingdom:
Paramount Pictures.
Wiebe, J.M. (2000). Learning subjective adjectives from corpora. Pro-
ceedings of the 17th National Conference on Artificial Intelligence
(pp. 735–740).
