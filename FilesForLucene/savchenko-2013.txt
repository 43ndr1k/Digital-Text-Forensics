Neural Networks 46 (2013) 227–241Contents lists available at SciVerse ScienceDirect
Neural Networks
journal homepage: www.elsevier.com/locate/neunet
Probabilistic neural network with homogeneity testing in recognition
of discrete patterns set
A.V. Savchenko ∗
National Research University Higher School of Economics, 25/12 Bolshaja Pecherskaja Ulitsa, Nizhny Novgorod 603155, Russia
a r t i c l e i n f o
Article history:
Received 21 October 2012
Revised and accepted 6 June 2013
Keywords:
Statistical pattern recognition
Discrete patterns set
Probabilistic neural network
Homogeneity testing
Face recognition
Authorship attribution
a b s t r a c t
The article is devoted to pattern recognition task with the database containing small number of samples
per class. By mapping of local continuous feature vectors to a discrete range, this problem is reduced to
statistical classification of a set of discrete finite patterns. It is demonstrated that the Bayesian decision
under the assumption that probability distributions can be estimated using the Parzen kernel and the
Gaussian window with a fixed variance for all the classes, implemented in the PNN, is not optimal
in the classification of a set of patterns. We presented here the novel modification of the PNN with
homogeneity testing which gives an optimal solution of the latter task under the same assumption about
probability densities. By exploiting the discrete nature of patterns our modification prevents the well-
known drawbacks of the memory-based approach implemented in both the PNN and the PNN with
homogeneity testing, namely, low classification speed and high requirements to the memory usage. Our
modification only requires the storage and processing of the histograms of input and training samples.
We present the results of an experimental study in two practically important tasks: (1) the problem of
Russian text authorship attribution with character n-grams features; and (2) face recognition with well-
known datasets (AT&T, FERET and JAFFE) and comparison of color- and gradient-orientation histograms.
Our results support the statement that the proposed network provides better accuracy (1%–7%) and is
much more resistant to change of the smoothing parameter of Gaussian kernel function in comparison
with the original PNN.
© 2013 Elsevier Ltd. All rights reserved.1. Introduction
Pattern recognition (Theodoridis & Koutroumbas, 2009) is a
fundamental problem in artificial intelligence, data mining, com-
puter vision, medical diagnostics and decision-support systems.
This problem may usually be formulated in terms of statisti-
cal recognition (Vapnik, 1998; Webb, 2002) of a set of patterns
(Borovkov, 1998): it is required to estimate the class of an input
sample of random variables, with an assumption that all available
information about each class is concluded in certain samples of
observations. This general formulation could be applied to such
crucial tasks as image recognition, voice phonemes recognition,
authorship attribution, etc.
The described problem is usually reduced (Duda, Hart, & Stork,
2001) to a statistical classification of the query sample. The
optimal decision is taken with a minimum Bayes risk principle
(Vapnik, 1998). The unknown probability density, required in this
approach, is usually estimated by means of nonparametric tech-
niques (Györfi, Kohler, Krzyzak, &Walk, 2002; Pagan&Ullah, 1999;
∗ Tel.: +7 9506243285.
E-mail address: avsavchenko@hse.ru.
0893-6080/$ – see front matter© 2013 Elsevier Ltd. All rights reserved.
http://dx.doi.org/10.1016/j.neunet.2013.06.003Rutkowski, 2004b), which can adjust themselves to the data with-
out any explicit specification (Efromovich, 1999; Greblicki, 1978;
Zhang, 2000). They include Parzen’s approach (Parzen, 1962), near-
est neighbor algorithms (Cover & Hart, 1968), etc., which were
proved to converge to the real probability density with probability
one if the training sample size is large (Rutkowski, 2004a;Wolver-
ton & Wagner, 1969).
The widely-used parallel implementation of the nonparamet-
ric approach is a probabilistic neural network (PNN). The PNN al-
gorithm was introduced by Specht (1988, 1990a, 1990b, 1991)
and approximates the class-conditional probability distributions
by finite mixtures of product components under the assumption
that probability distributions can be estimated using the Parzen
window. Usually, likelihood function of a given class as the sum
of Gaussians with a fixed variance for all the classes is applied
(Montana, 1992). This algorithm is called a ‘‘neural network’’ be-
cause of its natural mapping onto a feedforward network (Zhang,
2000) with two hidden layers. Moreover, the mixture components
can be interpreted as probabilistic neurons in neurophysiological
terms (Grim & Hora, 2008). Since 1990s, the PNNs were applied to
many important real-world applications, such as regression and re-
inforcement learning (Heinen & Enge, 2010; Specht, 1991), texture
recognition (Raghu & Yegnanarayana, 1998) face detection and
228 A.V. Savchenko / Neural Networks 46 (2013) 227–241recognition (Lin, Kung, & Lin, 1997), 3D objects and handwritten
digits recognition (Polat & Yıldırım, 2006), optical character recog-
nition (Romero, Touretzky, & Thibadeau, 1997), video image recog-
nition (Aibe, Mizuno, Nakamura, Yasunaga, & Yoshihara, 2004),
phoneme recognition (Maheswari, Kabilan, & Venkatesh, 2009),
sentence alignment (Fattah, Ren, & Kuroiwa, 2006), medical dis-
ease estimation (Mantzarisa, Anastassopoulosb, & Adamopoulosc,
2011), analysis of electroencephalogram signals by Übeyli (2009),
earthquake magnitude prediction (Adelia & Panakkat, 2009) and
partial volume segmentation in brain MR image (Song, Jamshidi,
Lee, & Huang, 2007).
In practice, the PNN is characterized by extremely fast training
procedure and convergence to the Bayes-optimal decision surface
(Specht, 1988). Though it is well known that the PNN is not a
commonly used classifier and its performance is often worse than
other modern classifiers as SVMs (Cortes & Vapnik, 1995), the
PNN is an excellent classifier of a set of patterns, outperforming
such classifiers as MLP (multilayer perceptron) trained by back
propagation (Savchenko, 2012a). Really, this task model sets for
different classes may contain equal patterns which cause the
learning by single pattern to be inefficient. As a matter of fact,
such neural network classifiers as SVM or MLP should be used
to compare the patterns extracted from the model sets, e.g. an
estimation of probability density. This approach shows good
recognition accuracy only if the database contains a lot of samples
per class. Unfortunately, in many practical cases we do not have
enough samples (in the worst case, one sample per class, see Tan,
Chen, Zhou, & Zhang, 2006). On the contrary, conventional PNN
was successfully applied in such task (Savchenko, 2012a).
The PNN was proved to be an asymptotically-optimal rule in
the classification task (Specht, 1990; (Musavi, Chan, Hummels, &
Kalantri, 1994; Rutkowski, 2004a)) if a query object is a single fea-
ture vector. Unfortunately, conventional PNN does not provide an
optimal solution (Borovkov, 1998; Savchenko, 2012a) if the query
object is represented by a set of features with the size approxi-
mately equal to the training set size. Really, in this case the task
should be reduced to a homogeneity testing of query and train-
ing samples (Borovkov, 1998; Kullback, 1997). Hence, we have re-
cently introduced the PNN with homogeneity testing (Savchenko,
2012a), which saves all advantages of the conventional PNN but
yields an optimal decision boundary in statistical recognition of a
set of patterns.Wehave shown in the experimentwithRussian text
authorship attribution (Kukushkina, Polikarpov, & Khmelev, 2001)
with simple features (frequency of punctuation marks in the sen-
tence) that our PNN achieves better accuracy and is much more
resistant to change the smoothing parameter of Gaussian kernel
function (Jones, Marron, & Sheather, 1996).
Unfortunately, our network (Savchenko, 2012a) possesses the
same shortcoming as the conventional PNN (Specht, 1990a). First
of all, it requires large memory to store all training samples (so
called memory-based approach to classification). Second, the clas-
sification speed is low as the network is based on an exhaus-
tive search through all training samples. Hence, in this paper
we explore the possibility to increase the classification speed
(Savchenko, 2012b) and decrease the necessary amount of mem-
ory for our network with preservation of the optimality property.
It is known that if the recognized features are discrete and finite,
then the whole information about the sample is contained in its
histogram (Webb, 2002). For this case the novel recognition crite-
rion is presented here on the basis of our PNN with homogeneity
testing. The thorough experimental study in two practically impor-
tant recognition tasks (authorship attribution and face recognition)
is discussed.
The key concept of this study is the application of the math
model of the recognized object represented by independent iden-
tically distributed (i.i.d.) random variables (local feature vectors)to pattern recognition. As a matter of fact, most widely-used sim-
ple local features are continuous (e.g., gradient orientations (Dalal
& Triggs, 2005; Savchenko, 2012c)) or have too wide range of def-
inition (e.g., 256 intensity levels of gray-scale pixel). We experi-
mentally show that their applicationwith the PNN or the PNNwith
homogeneity testing leads to a significant decrease of accuracy in
comparison with conventional nearest-neighbor methods (Lowe,
2004). It is demonstrated that themapping of such features to a dis-
crete range (e.g, 8 angle intervals in the Lowe’s (2004) SIFTmethod
of object recognition), estimation of their histograms (e.g., HOG,
histograms of oriented gradients proposed by Dalal and Triggs
(2005)) and comparison of these histograms with our criterion
cause either better computing efficiency or much lower error rate.
The rest of the paper is organized as follows: Section 2 presents
statistical recognition of a set of patterns using the conven-
tional PNN (Rutkowski, 2004a; Specht, 1990a, 1990b (Section 2.1)
and the PNN with homogeneity testing (Savchenko, 2012a) (Sec-
tion 2.2). In Section 3, we introduce themodification of our PNN for
recognition of discrete patterns sets. In Section 4, we present the
experimental results in the author identification task (Juola, 2006)
with well-known texts from the Russian literature Moshkov e-
library (2013) and n-grams frequencies features (Kukushkina et al.,
2001; Stamatatos, 2009). Section 5 demonstrate an application of
our approach to the face recognition task (Zhao & Chellappa, 2005).
In Section 5.1 we briefly discuss the color histograms (Rui, Huang,
& Chang, 1999) and the histograms of oriented gradients (Dalal &
Triggs, 2005) as image features widely used in face recognition.
Section 5.2 presents the comparison of our approach with other
methods (Euclidean distance, conventional PNN, Kullback–Leibler
minimum information discrimination principle, Jensen–Shannon
divergence) with well-known facial datasets (AT&T, 2013; FERET,
2013; JAFFE, 2013). Finally, concluding comments are given in
Section 6.
2. Statistical recognition of a set of patterns
2.1. Probabilistic neural network
Let a set X = {xj}, j = 1, n of i.i.d. random variables with
unknown probability distribution P be specified. Here n is a sample
size, xj = {xj;1, . . . , xj;M}—is a vector of features with a fixed
number of dimensions M = const . The recognition problem is to
estimate the class ofX. It is assumed that each class r ∈ {1., , , .R} is
defined by a training set of i.i.d. random variablesXr =

x(r)j

, j =
1, nr with unknownprobability distributionPr . Herenr is a training
sample size, x(r)j is a feature vector withM dimensions.
Following machine learning techniques, the task is reduced to
the classification problem in which training set consists of pairs
x(r)j , r

, r = 1, R, j = 1, nr . Then all vectors xj of query
sample are classified by, e.g., SVMorMLP (Duda et al., 2001). At last,
the classifier outputs for each query vector are combined to make
a final decision (Hsu & Lin, 2002). Unfortunately, such algorithm
does not lead to a high recognition accuracy (Savchenko, 2012a).
Really, though sets X and Xr contain i.i.d. variables, it does not
mean that each feature vector x(r)j may be used to identify class r .
Moreover, training sets for distinct classes may contain identical
feature vectors. For instance, in image recognition each training
sample is a set of pixel intensities, and different images usually
contain pixels with the same intensity. Only the whole sample
Xr should be used to uniquely identify class r . In general, every
training sample contains different number of features, hence, they
cannot be united into one feature vector of a fixed size.
Thus, it is more common to apply the statistical approach
(Borovkov, 1998; Vapnik, 1998), in which each class is assumed to
be fully determined by the distribution Pr , r = 1, R of its feature
A.V. Savchenko / Neural Networks 46 (2013) 227–241 229vector. Thus, the problem is referred to a hypothesis testing for
distribution of X
Wr : P = Pr r = 1, R. (1)
To solve the task (1), the principle of minimum Bayes risk
(Webb, 2002) is applied. The query sampleX is assigned to the class
ν with maximum a-posterior probability
ν = argmax
r∈{1,...,R}
P (X|Wr) · P(Wr). (2)
Here P(Wr) is the prior class probability, P (X|Wr) is a condi-
tional class density (likelihood). In the most practically important
pattern recognition tasks (Savchenko, 2012a) it is assumed that
each class is equiprobable (prior uncertainty):
P(Wr) =
1
R
.
Following the nonparametric approach, the likelihood is
estimated by the given training set with a kernel trick introduced
by Aizerman, Braverman, and Rozonoer (1964)
P̂ (X|Wr) =
1
(nr)n
n
j=1
nr
jr=1
K

xj, x
(r)
jr

(3)
Here K

xj, x
(r)
jr

is a kernel function (Greblicki, 1978; Jenssen,
Erdogmus, Principe, & Eltoft, 2006). For example, the Gaussian
Parzen kernel (Parzen, 1962) is widely used (Rutkowski, 2004a;
Schløler & Hartmann, 1992)
K

xj, x
(r)
jr

=
1
2πσ 2
M/2 exp− 12σ 2 ρ xj, x(r)jr 

. (4)
Here σ = const > 0 is a fixed smoothing parameter (stan-
dard deviation of the Gaussian kernel identical for all classes) and
ρ

xj, x
(r)
jr

is a measure of similarity between features. Tradition-
ally (Specht, 1988), the Euclidean distance square is used as a
ρ

xj, x
(r)
jr

. Based on the estimate (3), the final decision (2) could
be written as
ν = argmax
r∈{1,...,R}
1
(nr)n
n
j=1
nr
jr=1
K

xj, x
(r)
jr

. (5)
The criterion (5) corresponds the PNN for statistical recognition
of a set of patterns problem (1). Its implementation is shown in
Fig. 1.
In contrast with the conventional four-layered PNN (Specht,
1988) with input, pattern, summation and output layers which
classifies one object xj, the network (Fig. 1) contains additional,
production, layer (Savchenko, 2012a) to classify the sample of
objects X.
2.2. Probabilistic neural network with homogeneity testing
Pattern recognition problem is characterized by the unknown
probability distributions Pr of each class r . Thus it is required
to estimate the conditional density (3). It is the key difference
(Borovkov, 1998; Savchenko, 2012a) from the conventional
classification task, in which distributions Pr are given. Hence,
we believe it is better to follow the Borovkov’s approach (1998)
rather than referring pattern recognition task to a statistical testing
of distribution (1). According to this approach, the recognition
problem is reduced to a task of statistical test for homogeneity of
input sample X and training set Xr .
Wr :

xj

, j = 1, n and

x(r)j

, j = 1, nr
have the same probability distributionFig. 1. Conventional PNN in statistical recognition of a set of patterns.
The decision is made with a minimum Bayes risk principle by
using the sample from the united sample space {X,X1, . . . ,XR} to
a class ν
ν = argmax
r∈{1,...,R}
sup
P∗
sup
P∗j ,j=1,R
P ({X,X1, . . . ,XR} |Wr) · P(Wr). (6)
Here P∗ is a possible probability distribution of a sampleX, P∗j is
a possible distribution of jth training sample. Assuming the inde-
pendence of random variables in a united sample{X,X1, . . . ,XR},
the conditional density in (6) could be written as
sup
P∗
sup
P∗j ,j=1,R
P ({X,X1, . . . ,XR} |Wr)
= sup
P∗
P (X|Wr) sup
P∗r
P (Xr |Wr) ·
R
j=1
j≠r
sup
P∗j
P

Xj

or
sup
P∗
sup
P∗j ,j=1,R
P ({X,X1, . . . ,XR} |Wr)
=
sup
P∗
P (X|Wr) sup
P∗r
P (Xr |Wr)
sup
P∗r
P (Xr)
·
R
j=1
sup
P∗j
P

Xj

.
As
R
j=1
sup
P∗j
P

Xj

= const (X)
does not depend on a query sample X, criterion (6) is equivalent to
ν = argmax
r∈{1,...,R}
sup
P∗
P (X|Wr) sup
P∗r
P (Xr |Wr)
sup
P∗r
P (Xr)
· P(Wr).
By dividing this statement on factor supP∗ P (X), which is inde-
pendent on the training sets Xr , we could get the final expression
ν = argmax
r∈{1,...,R}
sup
P∗
P (X|Wr) sup
P∗r
P (Xr |Wr)
sup
P∗
P (X) · sup
P∗r
P (Xr)
· P(Wr) (7)
230 A.V. Savchenko / Neural Networks 46 (2013) 227–241It is known (Borovkov, 1998), that the supremum of the
likelihood is achieved when the valid probability distributions
P∗, P∗r are equal to their optimal maximum likelihood estimates.
Herewith, to evaluate this optimal estimate the combined sample
{X,Xr} is used if the condition Wr is true, i.e. X and Xr are the
samples of the same random variable. Thus, we use (Savchenko,
2012a) nonparametric (Györfi et al., 2002) kernel estimation (3) to
evaluate the supremums of the likelihood in (7):
sup
P∗
P (X|Wr)
=
1
(n + nr)n
n
j=1

n
j1=1
K

xj, xj1

+
nr
jr=1
K

xj, x
(r)
jr

,
sup
P∗r
P (Xr |Wr)
=
1
(n + nr)nr
nr
jr=1
 n
j1=1
K

x(r)jr , xj1

+
nr
jr;1=1
K

x(r)jr , x
(r)
jr;1
 ,
sup
P∗r
P (Xr) =
1
(nr)nr
nr
jr=1
nr
jr;1=1
K

x(r)jr , x
(r)
jr;1

,
sup
P∗
P (X) =
1
nn
n
j=1
n
j1=1
K

xj, xj1

.
Thus, (7) is equivalent to
ν = argmax
r∈{1,...,R}
1
(n + nr)n+nr
×
n
j=1

n
j1=1
K

xj, xj1

+
nr
jr=1
K

xj, x
(r)
jr

×
nr
jr=1
 n
j1=1
K

x(r)jr , xj1

+
nr
jr;1=1
K

x(r)jr , x
(r)
jr;1

×
 1
nn · (nr)nr
n
j=1
n
j1=1
K

xj, xj1

·
nr
jr=1
nr
jr;1=1
K

x(r)jr , x
(r)
jr;1
−1 .
(8)
Finally, we get the following criteria
ν = argmax
r∈{1,...,R}
nn · (nr)nr
(n + nr)n+nr
×
n
j=1
1 +
nr
jr=1
K

xj, x
(r)
jr

n
j1=1
K

xj, xj1


·
nr
jr=1
1 +
n
j1=1
K

x(r)jr , xj1

nr
jr;1=1
K

x(r)jr , x
(r)
jr;1

 . (9)
Expression (9) corresponds to the PNN with homogeneity
testing (Savchenko, 2012a) for statistical pattern recognition task.
Its implementation is shown in Fig. 2.
Here the input layer contains not only the query sample X, but
the united sample {X,X1, . . . ,XR}. It makes no difference between
query and training samples. In the second, pattern, layer the kernel
function for a query sample is added to a training set. The new
division layer is added according to (9). In the production layer weFig. 2. PNN with homogeneity testing in statistical recognition of a set of patterns.
multiply not only the features of an input object X, but also the
features of rth sample Xr .
It could be noticed that if nr → ∞, expression (9) is equivalent
to (5). Really, in asymptotics, the training set Xr fully determines
the probability distribution Pr . Hence, the united sample {X,Xr}
does not provide any additional information.
Unfortunately, our network (Fig. 2) possesses the same
shortcomings (Savchenko, 2012a) as the conventional PNN (Fig. 1).
They both require large memory to store all training samples
and the classification speed is low as the network is based on
an exhaustive search through all training samples (Savchenko,
2012b). In the next section, we show how to speed-up the
recognition accuracy if the features are discrete values.
3. Recognition of finite discrete patterns set
In this section, it is assumed that features xj and x
(r)
j are discrete
and finite. Hence, each feature vector is equal to one of the known
values a1, . . . , aN . Here N ≪ n is a count of distinct discrete
values and ai, i = 1,N is a M-dimensional vector. In this case,
all available information about the sample is contained in its
histogram (Kullback, 1997), and distribution Pr is set as a vector
of probabilities Pr =

pr,i = P (x = ai) , i = 1,N

.
First, we estimate the histogram of each training set

θr,i

, i =
1,N
θr,i =
1
nr
nr
j=1
δ

x(r)j − ai

, (10)
where δ (x) is a discrete delta function. Then the input sample
histogram P̂ = {wi} , i = 1,N is estimated
wi =
1
n
n
j=1
δ

x−j ai

. (11)
A.V. Savchenko / Neural Networks 46 (2013) 227–241 231Hence, the optimal expression (8) is equivalent to
ν = argmax
r∈{1,...,R}
×
N
i=1

N
j=1
Ki,j
n·wj+nr ·θr;j
n+nr
n·wi N
i=1

N
j=1
Ki,j
n·wj+nr ·θr;j
n+nr
nr ·θr,i
N
i=1

N
j=1
Ki,jwj
n·wi
·
N
i=1

N
j=1
Ki,jθr,j
nr ·θr,i .
Here Ki,j = K

ai, aj

, i, j ∈ {1, . . . ,N} are the preliminary
calculated values of kernel (4) for all potential values of feature
vectors. This criterion could be written as
ν = argmin
r∈{1,...,R}
N
i=1

N
j=1
Ki,jwj
n
n+nr
N
j=1
Ki,jwj + nrn+nr
N
j=1
Ki,jθr;j

nwi
×
N
i=1

N
j=1
Ki,jθr,j
n
n+nr
N
j=1
Ki,jwj + nrn+nr
N
j=1
Ki,jθr;j

nr θr,i
. (12)
If we preliminarily calculate the convolution of histograms (10),
(11) with the given kernel (4) and enter the following symbols
w
(K)
i =
N
j=1
Ki,jwj, i = 1,N (13)
and
θ
(K)
r;i =
N
j=1
Ki,jθr,j, i = 1,N, r = 1, R (14)
for the kernel-based estimations of the histograms of the input and
training samples, respectively, we finally get the following optimal
criterion by taking the logarithm of (12),
ν = argmin
r∈{1,...,R}
n
N
i=1
wi log
w
(K)
i
n
n+nr
w
(K)
i +
nr
n+nr
θ
(K)
r;i
+ nr
N
i=1
θr,i log
θ
(K)
r;i
n
n+nr
w
(K)
i +
nr
n+nr
θ
(K)
r;i
. (15)
The network implementation of the criterion (15) is shown in
Fig. 3, As for the general PNN with homogeneity testing (Fig. 2),
the input layer contains the united sample {X,X1, . . . ,XR}. Pattern
layer (Fig. 2) was divided into two layers (Fig. 3), namely pattern
I to evaluate histograms (10), (11) and pattern II to calculate
kernel histograms (13), (14). The third, division, layer contains
the product of histogram value (dashed arrow line) and logarithm
of the ration of the kernel histogram values (solid lines) from
(15). According to (15), in the summation layer there are only 2N
summands (compare with n + nr summands in Fig. 2). At the
output layer, the minimum similarity to the class is extracted. As
a matter of fact, criterion (15) is a nearest neighbor rule (Cover &
Hart, 1968), so it is easy to apply the measure of similarity (15)
than the conventional PNN (Fig. 1) or our general modification
(Fig. 2) in other pattern recognition algorithms (e.g. clustering, see
Theodoridis and Koutroumbas (2009)).
By using the same procedure (10)–(15) we could show that
the conventional PNN (5) for discrete samples classification is
equivalent to
ν = argmin
r∈{1,...,R}
N
i=1
wi log
w
(K)
i
θ
(K)
r;i
. (16)Fig. 3. PNN with homogeneity testing in statistical recognition of a set of discrete
patterns.
The major advantage of the criteria (15) and (16) is that their
computational complexity is n·nrN times lower than the complexity
of the general solutions (9) and (5). This fact is caused by the
reduction of the input and training samples to their histograms
(10), (11).
To summarize, the proposed network (Fig. 3) is a generaliza-
tion of the conventional PNN implementation (16). Its key char-
acteristic is the usage of the query sample X to estimate the joint
probabilistic quantity of the united sample {X,X1, . . . ,XR}. Hence,
the distribution of each class r is estimated by the united sample
{X,Xr}. The histogram of the query sample is the part of the sec-
ond, pattern I, layer, and each model sample became a member of
the first, input, layer (see Fig. 3).
Our modification saves all advantages of the conventional
PNN (Specht, 1988). First of all, it is an excellent training speed
in comparison with the back propagation. The new sample can
be added even in real time applications as it is only necessary
to evaluate its histogram (10) and its convolution with kernel
(14). The network begins to generalize each new observed set
of patterns causing the decision boundary to become closer to
the optimal one. Unlike many networks, the PNN (Figs. 2 and 3)
does not contain recursive connections from the neurons back to
the inputs. Thus, it could be implemented completely in parallel
(Specht, 1990a, 1990b).
The most significant advantage of the proposed classifier (15)
over the PNN (16) is that its decision boundary converges to
the Bayes optimal solution (Borovkov, 1998). The rate of the
convergence is essentially higher than the rate for the classical
PNN (16) in the most acute case of a pattern recognition when the
training sample size is approximately equal to the size of an input
sample nr ≈ n.
One other advantage of the proposed PNN is that the measure
of similarity in (15) is symmetric as the importance of training
and input sets is equivalent in the homogeneity testing. On the
other hand, the similarity measure in (16) is asymmetric. Really,
in statistical classification the model probability distribution
232 A.V. Savchenko / Neural Networks 46 (2013) 227–241(evaluated by the training sample) is much more important than
the input sample distribution. In this task it is supposed that nr ≫
n, so the quality of the model probability distribution estimation is
much higher than the quality of the query sample. This fact is an
additional argument as symmetry is a desired property in many
pattern recognition algorithms (e.g., clustering), see Theodoridis
and Koutroumbas (2009).
In the last part of this sectionwe note the special case of criteria
(15) and (16) ifKi,j = δi,j is a Kronecker delta. In this casew
(K)
i = wi
and θ (K)r;i = θr;i. Hence, (15) is equivalent to
ν = argmin
r∈{1,...,R}

n
N
i=1
wi log
wi · (n + nr)
n · wi + nr · θr;i
+ nr
N
i=1
θr,i log
θr,i · (n + nr)
n · wi + nr · θr;i

. (17)
If n = nr the expression (17) is equivalent to the
nearest-neighbor rule with Jensen–Shannon divergence (Martins,
Figueiredo, Aguiar, Smith, & Xing, 2008) widely-used in pattern
recognition. Moreover, this expression is a well-known criterion of
statistical homogeneity testing on the basis of the Kullback–Leibler
minimum information discrimination principle (Kullback, 1997)
between the united sample {X,Xr} and the optimal unbiased
estimation P̂∗r =

p̂∗r,i

, i = 1,N of the probability density P∗r if
the hypothesisWr is true. Here
p̂∗r,i =
n
n + nr
· wi +
nr
n + nr
· θr;i.
Analogically, if Ki,j = δi,j, criterion (16) is equivalent to the
Kullback–Leibler minimum information discrimination principle
(Kullback, 1997) between the distribution estimations of the input
sample P̂ and r-th training sample P̂r
ν = argmin
r∈{1,...,R}
N
i=1
wi log
wi
θr;i
. (18)
Thus, proposed criterion (15) and the PNN implementation
(16) are the kernel generalization of conventional criterion
for homogeneity testing (17) and sample classification (18),
respectively. We expect the proposed PNN saves all advantages of
the classical PNN proposed by Specht (1988, 1990a)), but the rate
of convergence to the optimal decision should be higher for (15)
than for a classical implementation (16). The next section provides
an experimental evidence to support this claim.
4. Experimental results
In this section, we demonstrate the proposed modification of
PNN (15) in the pattern recognition problem from the linguistic
analysis. It is required to identify the author of a Russian text
fragment (Kukushkina et al., 2001; Savchenko, 2012a). The training
sets contain other text extracts. We use the following eight well-
known large Russian texts: ‘‘Anna Karenina’’ and ‘‘Resurrection’’ by
L. Tolstoy, ‘‘Idiot’’ and ‘‘Crime and punishment’’ by F. Dostoevsky,
‘‘Dead souls’’ and ‘‘Taras Bulba’’ by N. Gogol, ‘‘And Quiet Flows the
Don’’ and ‘‘Virgin Soil Upturned’’ by M. Sholokhov. All texts (in
original) were taken from the corpus (Moshkov e-library (2013)).
The state-of-the-art feature in texts authorship attribution –
frequency of character n-grams (Stamatatos, 2009) – was used in
this experimental. This feature set is known to show a good quality
in Russian authorship attribution (Kukushkina et al., 2001). We
repeated texts preprocessing on the basis of the best algorithm
choosed in this paper. Namely, words beginningwith capitals were
omitted (including the first words of sentences). Finally, we retainonly words with length greater than 3 characters. From each such
word we extracted 1000 most frequent n-grams from the union of
all training fragments and calculate the frequency of each n-gram
in each fragment.
We compare the accuracy of the proposed PNN (15) with its
conventional implementation (16). The distance ρ

xj, x
(r)
jr

in
(4) is evaluated as a discrete delta function, i.e. ρ

xj, x
(r)
jr

=
δ

xj − x
(r)
jr

. Every neural network was implemented as a parallel
application with Java Runtime Environment 1.7 on a modern
laptop (Intel Core i7 CPU 2.0 GHz, 6 Gb RAM).
The accuracy was estimated by the following procedure. One
randomly chosen fragment of each text with fixed size (in
characters) was added to the training set. The test set contains
10 randomly chosen fragments of each text (i.e., 8 · 10 = 80
samples). The error rate was estimated by these 80 tests. The total
recognition quality was estimated as an arithmetical mean of the
error rate by 100 such experiments of training and test set selection
(i.e., 80 · 100 = 8000 classifications).
In the first experiment the frequency of bigrams was used.
In the first case both training and test sets were generated by
fragments of 25000 characters (the sample size n = 7000 . . . 8000,
the number of histogram bins N = 510 . . . 600). The box-
plot diagrams of dependence of classification error rate on the
smoothing parameter σ of the Gaussian kernel function (4) are
shown in Fig. 4.
The average training time ttr is equal to 1 ms for both (15) and
(16). Here the average error rate for the proposed criterion (15) is
at 4.5%–30% less than the error rate for PNN implementation (16).
In the second case the experiment with bigrams was repeated,
but the training sets were generated from 100000 characters in
a fragment (i.e. the sample size n = 29 000 . . . 31 000, N =
600 . . . 670). The test set was still generated by 25000 character-
fragments. The results are illustrated with a box-plot diagram in
Fig. 5. The average training time ttr is equal to 2.5 ms for both (15)
and (16).
In Fig. 5 one can see that the recognition accuracy is extremely
better in comparison with the previous experiment (see Fig. 4).
The minimal error rate of criterion (15) is at 5%–6% less than
the minimal error rate of the conventional PNN (16). The most
significant quality indicator of the proposed network (Fig. 3) is the
robustness of error rate dependence on the smoothing parameter.
Really, the error rate for the proposed criterion (15) is always
less than 30% for 25000-character training fragments and 17% for
100000-character fragments. At the same time, accuracy of the
traditional PNN (16) varies enormously. Atworst, the average error
rate is equal to 71% for 25000-character fragments and 62% for
1000000-character training fragments.
At last, the experiment was repeated, but the frequency of
trigrams was selected as a feature set. The results are shown in
box-plot diagrams in Fig. 6 for 25000-character training fragments
(here n = 4000 . . . 5000, N = 900 . . . 1000) and in Fig. 7
for 1000000-character fragments (n = 18 000 . . . 19 000, N =
900 . . . 1000).
In these experiments the best error rate for (15) is at 2.5%–12%
less than for the original PNN (16). Again, the accuracy is more
stable to the deviation of σ for our approach.
The best recognition results (average error rate ± standard
deviation) for all our experiments are summarized in Table 1.
To provide a comparison with state-of-the-art classifiers, we
used SVM from libSVM library (2013) which showed the best
results in various texts categorization tasks (Manning, Raghavan, &
Schütze, 2008). Unfortunately, SVM for character n-gram features
with string kernel (Lodhi, Saunders, Shawe-Taylor, Cristianini, &
Watkins, 2002) was not able to converge to any decision (marked
A.V. Savchenko / Neural Networks 46 (2013) 227–241 23375
70
65
60
55
50
45
35
25
20
15
5
10
0
30
40
75
70
65
60
55
50
45
35
25
20
15
5
10
0
30
40
0.21 0.27 0.33 0.39 0.45 0.51 0.57 0.63 0.69 0.75 0.21 0.27 0.33 0.39 0.45 0.51 0.57 0.63 0.69 0.75
a b
Fig. 4. Dependence of average error rate (in %) on σ for 25000 characters of each fragment both in training and test sets, features: bigrams, (a) conventional PNN (16) (b)
PNN with homogeneity testing (15).a b
Fig. 5. Dependence of the average error rate (in %) on σ 100000 characters in training and 25000 characters in test sets, features: bigrams, (a) conventional PNN (16) (b)
PNN with homogeneity testing (15).0
a b
Fig. 6. Dependence of the average error rate (in %) on σ for 25000 characters of each fragment both in training and test sets, features: trigrams, (a) conventional PNN (16)
(b) PNN with homogeneity testing (15).as ‘-’ in Table 1). It was caused by the fact that equal n-grams may
appear in distinct documents (see our explanation in the beginning
of Section 2.1). Hence, we tried SVMwith RBF kernel trained on the
n-gram histograms. Additionally, for bigram features (letter pairs),
the Markovmodel for sequences of characters and the comparison
of author rankswith the Kullback–Leibler divergencewas explored
(see Kukushkina et al., 2001 for details).
Here SVM for n-gram histograms does not lead to the best
solution as the number of model samples, i.e., the number of text
fragments available for each author is not enough. However, it is
characterized by the better accuracy than the PNN (except one casewith bigrams and 25000 characters in the training texts). Thus,
we experimentally proved that the PNN’s performance is often
worse than other modern classifiers. Nevertheless, general SVM’s
accuracy is lower than the recognition rate of the algorithm of
Russian texts authorship attribution from Kukushkina et al. (2001)
with Markov model of character bigram. However, its efficiency is
not so high as for our PNN with homogeneity testing as the size of
training samples is too small to learn the conditional probabilities
of letter pairs needed for Markov model.
Finally, the average recognition time (per one sample) for all
networks is presented in Table 2. Here we put the CPU time for
234 A.V. Savchenko / Neural Networks 46 (2013) 227–241a b
Fig. 7. Dependence of the average error rate (in %) on σ for 100000 characters in training and 25000 characters in test sets, features: trigrams, (a) conventional PNN (16)
(b) PNN with homogeneity testing (15).Table 1
The average error rate of the author identification by text fragment.
Features Criterion Number of characters
25000—training set,
25 000—test set
100000—training set,
25 000—test set
Frequency of bigrams PNN (16) 24.3%±11.1% 12.1%±12.4%
SVM with RBF, feature—n-gramm – –
SVM with RBF, feature—histogram of n-gram 25.4%±7.1% 8.6%±4.6%
Markov chain, Kullback–Leibler divergence 22.1%±8.1% 6.4%±4.4%
PNN with homogeneity testing (15) 19.7%±9.0% 6.1%±4.4%
Frequency of trigrams PNN (16) 21.5%±8.5% 4.8%±4.6%
SVM with RBF, feature—n-gramm – –
SVM with RBF, feature—histogram of n-gram 17.9%±7.4% 4.6%±3.2%
PNN with homogeneity testing (15) 14.4%±7.5% 2.4%±3.2%Table 2
The average recognition time (in ms) of the author identification by text fragment.
Features Criterion Number of characters
25000—training set,
25 000—test set
100000—training set,
25 000—test set
Frequency of bigrams PNN (5) 7931.5±132.6 32590.4±519.1
PNN, discrete patterns (16) 15.5±1.3 20.4±2.9
SVM with RBF, feature—n-gramm – –
SVM with RBF, feature—histogram of n-gram 8.6±0.9 10.2±1.6
Markov chain, Kullback–Leibler divergence 11.7±1.3 16.9±1.6
PNN with homogeneity testing (9) 15109.9±167.1 61304.9±658.5
PNN with homogeneity testing, discrete patterns (15) 29.1±2.6 37.7±3.1
Frequency of trigrams PNN (16) 6479.2±108.7 26107.5±439.2
PNN (5) 29.2±3.8 33.5±4.0
SVM with RBF, feature—n-gramm – –
SVM with RBF, feature—histogram of n-gram 14.7±2.4 14.8±2.5
PNN with homogeneity testing (9) 11654.2±189.1 45620.4±596.5
PNN with homogeneity testing, discrete patterns (15) 51.4±4.9 51.5±4.9common PNN (5) and PNN with homogeneity testing (9). Their
accuracies are not shown in Table 1 as they are equal to the
accuracies of (16) and (15), respectively.
From this table we could notice that our PNN (9), (15) requires
twice more CPU time than the PNN (5), (16) to classify one
sample. This fact confirms the analysis of expressions (15) and
(16): the latter contains twice more summands. The performance
of both PNNs for discrete patterns (15), (16) and trigram features
practically does not depend on the size of the input and query
samples as these criteria only depend on the number of histogram
bins N which is equal to 1000 in our experiments (compare with
the results in Table 2 for conventional PNN (5) and our PNN (15)).
At the same time, for bigrams 1000000-character texts require
more CPU time because large texts contain 700–800 distinct
bigrams (compare with 500–600 bigrams in 25000-character
texts). It is remarkable that for common PNN (5) and the PNNwith
homogeneity testing (9) the recognition time for trigrams is lower
than the time for bigrams features because the text of fixed length
contains lower number of trigrams than bigrams. The last note onTable 2 is the known fact that SVM’s computing efficiency exceeds
the PNN’s performance in several times.
Based on our results we could draw the following conclusion.
First, the classification accuracy of the conventional PNN (16) is
always less than the accuracy of the proposed generalization (15).
The gain is more if the size of the input set is approximately equal
to the size of each training set n ≈ nr . Second, the task of proper
choice of the best value of the smoothing parameter σ of the
Gaussian kernel (4) for the proposed criterion (15) is not as acute
as for the traditional criterion (16). Third, the author identification
quality of the synthesized criterion (9) with frequency of trigrams
as a feature is rather good even in comparison with the known
results for Russian texts (Kukushkina et al., 2001) (see Table 1).
5. Application to face recognition
In this section, we explore the application of the proposed
criterion (15) to the face recognition task (Zhao & Chellappa, 2005).
In Section 5.1 we discuss the features sets widely used in this task.
A.V. Savchenko / Neural Networks 46 (2013) 227–241 235Section 5.2 presents the experimental results with AT&T, FERET
and JAFFE faces datasets.
5.1. Feature sets
Let a set of R > 1 grayscale frontal face photos Xr =

x(r)uv

(u = 1,Ur , v = 1, Vr ) be specified. Here R is a database size,
x(r)uv ∈ {1, 2, . . . , xmax} is the intensity of an image point with
coordinates (u, v); r ∈ {1, . . . , R} is the image number, and are
the rth image height andwidth, respectively, xmax is themaximum
level of intensity. Each image corresponds to a person c(r) ∈
{1, . . . , C}, C ≤ R is an amount of persons (classes). It is required
to assign a query photo X = [xuv] (u = 1,U , v = 1, V ) to one of
the C classes where U and V are the query imagewidth and height,
respectively. The problemcanbe factorized into two essential parts
(Tse & Lam, 2008): (1) feature extraction; (2) similarity measure
and classifier design.
There are various feature sets which are widely used in
computer vision—color intensities, HSV representation of color,
texture representation, shape, etc. In this section, we describe
two conventional feature sets: traditional color histograms (Rui
et al., 1999; Wong, Cheung, & Po, 2003; Yoo, Kim, & You, 2007)
and modern gradient orientation histograms (modification of
the HOG method introduced by Dalal and Triggs (2005)). For
the second part of the face recognition algorithm. We calculate
local features by dividing images into a regular grid to provide
illumination and outlier robust appearance-based correspondence
with some leeway for small spatial deviations due tomisalignment
(Savchenko, 2012b, 2012c; Tan & Triggs, 2010). However, to
simplify formulas in this section we suppose that the whole image
is considered. To apply the classifiers discussed in the previous
sections, histogram-based approach is used. At first, each image
is divided into a regular grid with K1 rows and K2 columns
(Lowe, 2004; Savchenko, 2012c). Then the histogram Hr (k1, k2) =
h(r)1 (k1, k2) , . . . , h
(r)
N (k1, k2)

of used features is evaluated for
each cell (k1, k2), k1 ∈ {1, . . . , K1}, k2 ∈ {1, . . . , K2}, where N is
the number of bins in the histogram.
In this paper, we use two popular local features: color intensity
and gradient orientation. The elements of the histogram of color
intensity (color histogram) of the rth model image is estimated by
the following expression
h(r)i (k1, k2)
=
K1 · K2
U · V

Uk1
K1

u=

U(k1−1)
K1

+1

Vk2
K2

v=

V(k2−1)
K2

+1
δ

x(r)uv
N

− i

,
i = 1,N. (19)
Here δ (x) =

1, x = 0,
0, otherwise is a discrete Dirac delta-function and
⌊·⌋ is a floor function. Color histograms comparison has widely
been used in image retrieval (Rui et al., 1999). Unfortunately,
images with the same visual information, but with shifted color
intensity, may significantly degrade if the conventional method of
direct histograms comparison is used. Actually, if the input image
X is one of the template images Xr from the database but all pixels
are decolorized, its color histogram will be quite different from
the histogram of rth model image. Eventually, we decided to use
shifting of histograms (Yoo et al., 2007) after their evaluation (11)
to reduce illumination influence.
Our second feature set is the modification (Savchenko, 2012b)
of the HOG algorithm (Dalal & Triggs, 2005). At first, we evaluatethe gradient magnitude and angle for each pixel of the image
m(r)u,v =
1
2
x(r)u+1,v+1 − x(r)u,v+ x(r)u+1,v − x(r)u,v+1 , (20)
θ (r)u,v = tg
−1 x
(r)
u+1,v+1 − x
(r)
u,v
x(r)u+1,v − x
(r)
u,v+1
. (21)
We divide the whole orientation definition range [−π; π ] into
N regular segment (thus, each segment’s width is 2πN ). Then, we
evaluate its histogram using relative magnitude as a weight given
in Box I
Here again H(·) is a Heaviside function and
m(r) (k1, k2) = max
u∈

U(k1−1)
K1

+1,...,

Uk1
K1

;
v∈

V(k2−1)
K2

+1,...,

Vk2
K2

m(r)u,v.
To estimate the similaritymeasure in (4), the following rounded
distance is used
∀iθ;1, iθ;2 ∈ {1, . . . ,N} ρ

iθ;1, iθ;2

=

iθ;1 − iθ;2 , iθ;1 − iθ;2 ≤ N2
N −
iθ;1 − iθ;2 , iθ;1 − iθ;2 > N2 .
Histograms corresponding to a query image are compared to
the histograms of all the images stored in the database using some
measure of similarity
ρ (X, Xr) =
K1
k1=1
K2
k2=1
ρ (Hr (k1, k2) ,H (k1, k1)) . (23)
Here ρ (Hr (k1, k2) ,H (k1, k1)) is any similarity measure. In
Savchenko (2012c) we showed that alignment histograms in ∆-
neighborhood (∆ ≥ 0) of each cell can cause the decrease of error
rate in comparison with (23)
ρ(∆) (X, Xr)
=
K1
k1=1
K2
k2=1
min
|∆1|6∆,|∆2|6∆
ρ (Hr (k1 + ∆1, k2 + ∆2) ,
H (k1, k1)) . (24)
Expression (23) is a special case of (24) if ∆ = 0.
5.2. Experimental study results
In this section,we present the results of automatic classification
of single facial images with three popular datasets. The AT&T
(former ORL) database is well-known with a various face
foreshortening on the image. It contains 400 photos of 40 persons
(10 photos per person). The Japanese Female Facial Expression
(JAFFE) dataset contains 213 images of 10 female persons (more
than 20 photos per person). The latter dataset is used in either
face classification or facial expression recognition tasks. Finally,
we selected 2720 face-to-face images of 994 persons from FERET
dataset which is de-facto standard in the evaluation of face
recognition methods.
The images were preliminarily processed with OpenCV library
(2013) to detect faces. After that the median filter with window
size (3 × 3) was used to remove noise. For color histograms
comparisonwe divided detected faces in a K1 ×K2 regular grid and
the number of bins in the histogram is N = 32. The ATT contains
images of small size (U = 112, V = 92), hence the followed grid
sizes were selected as optimal in terms of the recognition quality:
K1 = 4, K2 = 3, i.e. the feature vector size is 32 · 3 · 4 = 384.
236 A.V. Savchenko / Neural Networks 46 (2013) 227–241
2)h(r)i (k1, k2) =
K1 · K2
U · V

Uk1
K1

u=

U(k1−1)
K1

+1

Vk2
K2

v=

V(k2−1)
K2

+1
m(r)uv

H

θ
(r)
uv −
2(i−1)−N
N π

− H

θ
(r)
uv −
2i−N
N π

m(r) (k1, k2)
. (2
Box I.23
21
19
17
15
13
11
9
7
5
23
21
19
17
15
13
11
9
7
5
0.30 0.35 0.39 0.44 0.48 0.53 0.57 0.62 0.66 0.71 0.30 0.35 0.39 0.44 0.48 0.53 0.57 0.62 0.66 0.71
a b
Fig. 8. Dependence of average error rate (in %) on σ : histogram of gradients orientation comparison, AT&T dataset, np = 2 photos per person, (a) conventional PNN (16) (b)
PNN with homogeneity testing (15).Detected faces in the other datasets are larger, hence we fixed
K1 = 7, K2 = 5, i.e. the feature vector size is 32 · 5 · 7 = 1120.
In color histograms comparison we found experimentally that the
distance (24) does not provide better accuracy than (23) if ∆ > 0.
Thus, we compared the histograms with expression (23).
For gradient orientation histograms we used N = 8-
conventional value of segment count in the SIFT algorithm (Lowe,
2004). It was found that setting ∆ to 1 caused the significant
improvement of recognition quality (Savchenko, 2012c, 2013).
Therefore, we provide here the results for two cases. First is the
distance (24) with ∆ = 0. In this case, we set K1 = K2 = 5
(i.e. the feature vector size is 8 · 5 · 5 = 200). In the second case,
distance (24)with∆ = 1was applied and the grid sizewas fixed to
K1 = K2 = 10 (the feature vector size is 8 ·10 ·10 = 800). All these
parameters provide the best recognition accuracy for both datasets
and feature sets (Savchenko, 2012b, 2012c).
The recognition accuracy was estimated by the following cross-
validation procedure. At first, the number of photos per one person
np = const is fixed. For each person, we randomly choose np
photos and put them into the model database. Other photos are
put into the test set. Then we estimate the error rate of test set
recognition. This experiment is repeated 20 times. Finally, we
estimate the mean and the standard deviation of the error rate
for all experiments. As face recognition often requires real-time
processing, all criteria were implemented via C++ programming
language in Visual C++ 2010 compiler with optimization by speed.
The Windows Thread API was used to implement parallel neural
networks in 8 threads (Savchenko, 2013).
First, we explore the dependence of the accuracy of recognition
with AT&T dataset on the smoothing parameter of the Gaussian
kernel (4) for proposed criterion (15) and the PNN implementation
(16). The histogram of oriented gradients (20)–(23) is applied as
one of the most frequently-used image features, nowadays. (Dalal
& Triggs, 2005; Lowe, 2004). We extract np = 2 photos per face,
i.e., the model database size is 80 photos. The box-plot diagram of
the error rate is shown in Fig. 8.
These results (Fig. 8) are quite similar to our results in author
attribution task (Figs. 4–7). Really, the accuracy for the proposed
PNN with homogeneity testing (15) is higher than the accuracy
of (16) and is more stable to the deviation of the smoothing
parameter.In the next experiment, we present the dependence of the error
rate on np. Only small values of np are used (np ∈ {1, . . . , 5}). The
following criteria are explored:
1. nearest neighbor rule with conventional Euclidean distance
between histograms.
2. linear SVM. Non-linear SVM with the RBF kernel does not
lead to the best accuracy as the size of the feature vector is large
in all experiments (similar results are presented by Manning et al.
(2008)). It is probably the best classifier if the number of elements
in the training set is high (Jia & Martinez, 2009).
3. conventional PNN (5). We fixed the smoothing parameter
σ = 3.5 as it showed the best accuracy (see Fig. 8a).
4. the PNN for classification of the discrete patterns set (16),
σ = 3.5
5. nearest neighbor rule with the Kullback–Leibler divergence
(18). It is the limit of (16) if σ → 0. As some of the feature values
are equal to zero, Lagrangian smoothing was applied to color and
gradient orientation histograms.
6. nearest-neighbor rule with the Jensen–Shannon divergence.
This criterionwas proved to showa good image recognition quality
(Martins et al., 2008). As we stated in Section 3, it is the limit of the
proposed PNN (16) if σ → 0 and n = nr , r = 1, R. Again, the
Lagrangian smoothing of the histogram should be used.
7. the PNN with homogeneity testing (9), σ = 3.5
8. the proposed PNNwith homogeneity testing for classification
of the discrete patterns set (15), σ = 3.5
In image recognition task the PNN (5) and its modification (9)
are not usually equivalent to (16) and (15). Really, such features
as gradient orientation or color intensity are either continuous
or characterized by wide definition domain. Reducing the task
to feature histograms comparison is much more stable to noise
and/or pixels deviations. For example, experiments show (Lowe,
2004) the superiority of the mapping of gradient orientation to
N = 8 regions with π4 width. The same holds true for color
intensities (xmax = 255) which are often mapped to N = 32 or
N = 64 parts (Yoo et al., 2007).
The results for comparison of color histograms (19) and
gradient orientation histograms (20)–(22) and AT&T dataset are
shown in Table 3. We bolded the best result for each np and each
feature set.
A.V. Savchenko / Neural Networks 46 (2013) 227–241 237Table 3
The face recognition average error rate for AT&T dataset.
Features Number of photos per person
1 2 3 4 5
Color histogram, ∆ = 0 Euclidean 31.0%±2.0% 14.3%±4.6% 7.9%±3.1% 2.8%±2.9% 3.0%±2.2%
SVM 31.0%±2.2% 15.2%±4.7% 9.8%±4.0% 4.0%±3.0% 4.4%±3.7 %
PNN (5) 29.9%±2.0% 15.3%±3.0% 9.4%±3.4% 4.9%±3.0% 4.6%±2.6%
PNN, discrete patterns (16) 26.1%±3.5% 14.4%±2.2% 7.5%±3.6% 4.2%±2.7% 3.4%±1.9%
Kullback–Leibler 26.5%±1.7% 15.1%±3.0% 8.1%±4.4% 3.8%±1.9% 3.2%±1.8%
Jensen–Shannon 23.0%±2.2% 12.9%±3.0% 7.1%±2.3% 3.4%±1.8% 2.5%±1.5%
PNN with homogeneity testing (9) 24.9%±1.9% 14.7%±2.7% 9.0%±3.0% 4.4%±2.0% 3.9%±1.9%
PNN with homogeneity testing, discrete patterns
(15)
22.5%±1.7% 12.0%±3.0% 6.6%±3.7% 2.8%±1.9% 1.5%±1.7%
Histogram of oriented gradients,
∆ = 0
Euclidean 30.9%±4.1% 16.9%±2.4% 11.1%±2.4% 7.7%±1.6% 5.4%±1.6%
SVM 31.0%±3.1% 16.9%±3.3% 8.8%±1.4% 5.6%±2.1% 3.4%±1.4%
PNN (5) 33.9%±3.9% 20.0%±3.0% 12.8%±1.8% 9.2%±2.2% 4.9%±1.1%
PNN, discrete patterns (16) 32.1%±4.1% 18.7%±3.1% 11.0%±2.2% 7.8%±2.2% 4.5%±1.2%
Kullback–Leibler 34.1%±3.8% 18.4%±2.5% 10.4%±2.6% 7.2%±2.1% 4.6%±0.9%
Jensen–Shannon 30.8%±4.4% 16.9%±2.6% 10.5%±2.4% 7.8%±1.6% 4.2%±1.5%
PNN with homogeneity testing (9) 31.9%±3.9% 18.3%±3.3% 12.1%±2.7% 8.9%±1.9% 4.8%±1.8%
PNN with homogeneity testing, discrete patterns
(15)
29.4%±3.0% 16.6%±3.2% 9.3%±2.9% 6.8%±1.4% 4.1%±1.7%
Histogram of oriented gradients,
∆ = 1
Euclidean 23.5%±2.6% 11.6%±2.2% 7.1%±2.5% 3.4%±1.5% 2.4%±1.4%
SVM 22.9%±2.7% 11.0%±2.0% 6.9%±1.9% 3.2%±0.9% 2.0%±1.1%
PNN (5) 23.1%±2.7% 12.7%±1.7% 7.2%±2.2% 3.7%±1.0% 2.6%±1.1%
PNN, discrete patterns (16) 21.9%±2.2% 10.3%±1.7% 6.1%±1.9% 2.8%±0.9% 1.9%±1.0%
Kullback–Leibler 24.3%±2.7% 12.5%±2.2% 7.5%±2.2% 3.8%±1.3% 2.3%±1.4%
Jensen–Shannon 21.0%±3.2% 9.7%±1.9% 5.3%±2.2% 2.3%±1.0% 1.2%±0.7%
PNN with homogeneity testing (9) 24.0%±2.9% 11.8%±1.8% 6.6%±2.3% 3.2%±1.1% 2.0%±1.0%
PNN with homogeneity testing, discrete patterns
(15)
20.5%±2.3% 9.5%±1.8% 5.2%±2.1% 2.2%±0.9% 1.0%±0.7%Table 4
The average recognition time (in µs) per one query face for AT&T dataset.
Criterion Database size (R)
Features Color histogram, ∆ = 0 Histogram of oriented gradients,
∆ = 0
Histogram of oriented
gradients, ∆ = 1
40 200 40 200 40 200
Euclidean 87±18 447±76 72±3 350±13 181±7 695±23
SVM 19±3 59±178 14±2 40±10 46±6 91±18
PNN (5) 5.5E7±6580 3.2E8±21760 6.3E7±3850 3E8±12580 2E8±8750 9.4E7±37810
PNN, discrete patterns (16) 199±34 872±158 118±26 413±30 1592±49 7100±201
Kullback–Leibler 183±12 802±60 141±18 400±95 1627±109 7426±325
Jensen–Shannon 294±18 1260±82 224±46 643±238 3328±242 14910±891
PNN with homogeneity testing (9) 8.5E7±5360 3.3E8±19000 1.0E8±7760 4.7E8±32500 4.9E8±29790 2.3E9±95130
PNN with homogeneity testing, discrete patterns (15) 315±18.8 1314±98 192±27 661±127 3496±334 15869±919Here we would like to emphasize the following notes. First
and the most important, mapping features to a discrete range in
(15), (16) leads to a significant improvement of recognition quality
in comparison with our original network (9) and conventional
PNN implementation (5). Second, SVM classifier does not provide
quite better accuracy than the nearest neighbor with Euclidean
distance if the size of feature vector is large (in comparisonwith the
database size R and the number of models np for each class). Only
in the case of the HOG scheme with ∆ = 0 in which feature vector
contains 200 elements, SVM leads to the best decision if np ≥ 3.
However, conventional implementation of the PNN (5) shows even
worth quality than SVM. Third, as we said earlier, the nearest-
neighbor rule with the Kullback–Leibler divergence is the special
case of (16) if discrete Dirac function (limit of Gaussian when
σ → 0) is applied. However, Table 3 shows that the error rate
of the PNN (16) and the Kullback–Leibler minimum information
discrimination principle (Kullback, 1997) are not close to each
other. The limit of our PNN implementation (15) if n = nr is the
nearest neighbor criterion with the Jensen–Shannon divergence,
and the recognition quality of our general criterion (15) is quite
close to the Jensen–Shannon distance quality. Nevertheless, theproper choice of the smoothing parameter in our criterion (15)
may increase the recognition accuracy (Tables 3, 5 and 7) without
any computational cost (Tables 4, 6 and 8). Thus, this experiment
confirms our note about the stability of the PNNwith homogeneity
testing to the deviation of its parameters in comparison with the
PNN (16).
The average recognition time of one face for the lowest and the
highest values of np is presented in Table 4. To show large numbers
scientific notation (5.5E7 = 55 000 000) is used in this table.
Here again (like in our experiments with authorship attribu-
tion) SVM demands the lowest CPU time to recognize one face and
the proposed network (Fig. 3) requires twice more computations
than the PNN implementation (16). All face images have the same
size (U = 112, V = 92), and Table 4 demonstrates that the com-
puting efficiency of (15), (16) exceeds the same indicator for con-
ventional PNN realizations (9), (5) in U ·V ·Ur ·VrN·K1·K2 times. Moreover, the
performance of either conventional PNN or the PNN with homo-
geneity testing (Savchenko, 2012a) does not allow to implement it
in such real-time applications as face recognition from video. As a
matter of fact, this task requires frames to be processed in 50 ms.
Most of this time is held in face detection stage (Savchenko, 2012c).
238 A.V. Savchenko / Neural Networks 46 (2013) 227–241Table 5
The face recognition average error rate for JAFFE dataset.
Features Number of photos per person
1 2 3 4 5
Color histogram, ∆ = 0 Euclidean 16.9%±5.7% 8.0%±3.6% 4.8%±2.7% 4.6%±2.8% 2.9%±2.6%
SVM 15.1%±3.9% 7.9%±2.6% 5.7%±2.3% 4.5%±2.7% 3.4%±3.1%
PNN (5) 17.7%±2.7% 12.4%±4.4% 8.0%±3.9% 7.3%±5.3% 6.0%±3.8%
PNN, discrete patterns (16) 16.5%±2.6% 9.6%±4.6% 6.9%±3.9% 6.8%±5.4% 5.6%±3.9%
Kullback–Leibler 15.9%±5.5% 12.1%±4.8% 7.9%±4.0% 6.6%±4.8% 4.7%±4.4%
Jensen–Shannon 17.0%±4.6% 7.6%±3.4% 4.5%±3.1% 4.8%±3.7% 4.2%±3.3%
PNN with homogeneity testing (9) 16.3%±4.6% 8.9%±3.4% 6.6%±2.6% 5.9%±3.9% 3.8%±3.1%
PNN with homogeneity testing, discrete patterns (15) 14.2%±4.7% 6.8%±3.2% 4.6%±2.4% 4.0%±3.9% 2.5%±3.1%
Histogram of oriented gradients, ∆ = 0 Euclidean 18.1%±8.2% 7.2%±5.5% 6.3%±6.3% 4.7%±2.9% 3.1%±2.7%
SVM 18.8%±7.9% 9.6%±3.9% 6.9%±5.1% 4.6%±4.4% 2.9%±3.1%
PNN (5) 19.5%±9.5% 10.0%±6.7% 7.0%±6.5% 5.4%±4.4% 3.9%±4.0%
PNN, discrete patterns (16) 18.9%±9.3% 8.6%±6.8% 5.5%±6.5% 4.9%±4.5% 3.6%±4.2%
Kullback–Leibler 19.7%±9.2% 9.5%±7.3% 6.1%±6.6% 4.9%±4.6% 4.2%±4.0%
Jensen–Shannon 17.6%±8.2% 7.2%±5.0% 5.8%±4.9% 4.3%±3.2% 3.1%±2.7%
PNN with homogeneity testing (9) 18.0%±8.0% 7.2%±5.0% 5.8%±4.9% 4.3%±3.2% 3.1%±2.7%
PNN with homogeneity testing, discrete patterns (15) 16.8%±7.8% 6.3%±5.0% 4.7%±4.9% 3.5%±1.8% 2.8%±2.6%
Histogram of oriented gradients, ∆ = 1 Euclidean 17.4%±9.6% 8.3%±6.3% 5.9%±6.3% 3.6%±4.4% 2.1%±2.9%
SVM 17.5%±9.9% 8.2%±6.0% 5.5%±4.6% 3.4%±4.4% 1.8%±2.8%
PNN (5) 15.7%±9.0% 7.1%±4.1% 5.8%±4.4% 3.6%±4.6% 1.9%±1.6%
PNN, discrete patterns (16) 13.6%±8.8% 5.3%±4.2% 4.1%±4.5% 2.0%±4.6% 1.0%±1.5%
Kullback–Leibler 16.3%±9.3% 6.7%±5.2% 4.6%±5.0% 2.5%±3.2% 1.7%±1.5%
Jensen–Shannon 12.8%±7.6% 5.3%±3.9% 4.2%±4.3% 2.4%±2.6% 1.1%±1.7%
PNN with homogeneity testing (9) 14.0%±7.5% 6.6%±3.6% 5.1%±4.1% 2.9%±2.2% 1.7%±1.6%
PNN with homogeneity testing, discrete patterns (15) 12.2%±7.5% 5.0%±3.5% 4.0%±4.1% 2.1%±2.3% 1.1%±1.7%Table 6
The average recognition time (in µs) per one query face for JAFFE dataset.
Criterion Database size (R)
Features Color histogram, ∆ = 0 Histogram of oriented gradients,
∆ = 0
Histogram of oriented
gradients, ∆ = 1
10 50 10 50 10 50
Euclidean 246±30 916±167 21±3 115±14 52±9 192±38
SVM 89±27 127±148 3±0.4 8±2 7±1 14±6
PNN (5) 2.0E8±9510 9.4E8±31230 8E7±12510 4.4E8±3E4 2.8E8±2E5 1.2E9±86710
PNN, discrete patterns (16) 509±65 2151±356.5 39±15 169±82 532±72 2063±215
Kullback–Leibler 511±72 1932±295 44±7 181±40 509±108 1960±311
Jensen–Shannon 797±91 3482±516 66±9 223±34 1032±154 4271±738
PNN with homogeneity testing (9) 3.2E8±13270 1.4E9±26480 1.6E8±6400 6E8±24430 4.8E8±1E4 2E9±35490
PNN with homogeneity testing, discrete patterns (15) 855±94 3577±514 71±12 221±66 940±87 4128±223The error rate and recognition time for JAFFE dataset are
shown in Tables 5 and 6. These results confirm our conclusions
made above. However, SVM is not the best classifier here for
the distance (23) and the HOG features even for large np as the
images for one person are not so close as for AT&T database.
Also, the accuracy improvement of the proposed PNN (15) over
the Jensen–Shannon divergence for color histograms comparison
is much more noticeable. Once more, the histogram-based PNNs
(15), (16) are U ·V ·Ur ·VrN·K1·K2 times faster than (9) and (5), respectively
(in average, the size of detected faces for JAFFE is U = 150,
V = 140).
Tables 7 and 8 summarized the results for FERET dataset. This
database does not contain the same count of test photos for each
person. Each person is presented by at least two photos, thus, here
we fixed the training set size R so that at least one photo of each
person is stored in training set and one other photo in test sets.
This dataset contains photos with various illumination and object
occlusion, so it is more complex than AT&T and JAFFE. As one can
notice from Table 7, SVM here is even worth than the nearest
neighbor with Euclidean distance. This fact is explained by the
insufficient number of model images per each class and the strong
distinction between images of one class. The latter circumstance
prevents efficient generalization in a learning procedure. However,
even in this case the proposed PNN with histograms comparison
(15) shows the best recognition quality.The memory usage (per one image) for each classifier (except
SVM, which is implemented in libSVM library) is shown in Table 9.
We assume that every histogram item and gradient orientation are
stored as a double (8 bytes), and the color intensity requires 1 byte.
Here one can notice that the original PNN (Fig. 1), (5) and its
modification (Fig. 2), (9) require much more memory as they have
to store all local features. On the other hand, histogram-based
methods deal only with histograms. The proposed modification of
PNN with homogeneity testing (15) and the PNN implementation
(16) require twice more memory than the other histogram-based
nearest-neighbor classifiers such as (17), because they have to
process the convolution of histograms with a kernel (13), (14).
Based on all our results (Tables 3–9), we could draw the
following conclusions. First, the gradient orientation histograms
(20)–(22) and distance (24) with ∆ = 1 showed the best
results in practically all experiments (with one exception—JAFFE
dataset, Euclidean distance, np = 1). Second, the quality of
the widely-used Euclidean distance is the worst among other
criteria. Third, the proposed approach (Fig. 3) is again much
more robust to the deviation of the smoothing parameter. Really,
criterion (16) showed practically the same results as its special
case, namely, the Jensen–Shannon divergence. On the contrary,
the error rate of limiting case of the conventional PNN – the
Kullback–Leibler divergence – is sometimes much higher than the
error rate of the PNN (16). And, fourth, our approach which is
A.V. Savchenko / Neural Networks 46 (2013) 227–241 239Table 7
The face recognition average error rate for FERET dataset.
Features Database size (R)
1030 1110 1370 1730
Color histogram, ∆ = 0 Euclidean 42.7%±0.6% 40.7%±0.9% 30.7%±1.9% 21.0%±0.3%
SVM 42.5%±0.7% 40.1%±1.0% 29.7%±1.9% 22.6%±0.3%
PNN (5) 42.6%±0.6% 40.0%±0.8% 30.3%±2.0% 18.1%±0.3%
PNN, discrete patterns (16) 38.7%±0.5% 38.1%±0.9% 29.2%±2.3% 15.7%±0.5%
Kullback–Leibler 39.6%±0.6% 35.2%±0.8% 24.7%±2.2% 18.2%±0.6%
Jensen–Shannon 37.8%±0.5% 33.3%±0.8% 22.5%±2.1% 15.8%±0.4%
PNN with homogeneity testing (9) 39.5%±0.5% 34.1%±1.1% 24.1%±2.2% 17.2%±0.5%
PNN with homogeneity testing, discrete patterns (15) 37.0%±0.6% 32.5%±0.7% 21.7%±2.0% 15.4%±0.3%
Histogram of oriented gradients, ∆ = 0 Euclidean 29.0%±1.2% 25.0%±1.3% 16.4%±1.5% 9.7%±0.3%
SVM 29.3%±1.4% 24.7%±1.2% 15.8%±1.3% 13.4%±0.4%
PNN (5) 29.3%±1.2% 26.2%±1.4% 17.5%±1.4% 10.0%±0.5%
PNN, discrete patterns (16) 26.8%±1.3% 25.1%±1.0% 16.3%±1.6% 7.8%±0.3%
Kullback–Leibler 27.0%±1.0% 22.7%±0.9% 13.4%±1.5% 8.3%±0.2%
Jensen–Shannon 26.0%±1.1% 21.6%±0.9% 12.5%±1.4% 7.5%±0.3%
PNN with homogeneity testing (9) 27.2%±1.3% 23.9%±0.9% 15.8%±1.6% 9.4%±0.4%
PNN with homogeneity testing, discrete patterns (15) 25.6%±1.2% 21.3%±0.7% 12.0%±1.5% 6.8%±0.3%
Histogram of oriented gradients, ∆ = 1 Euclidean 25.7%±1.4% 21.2%±1.0% 12.2%±2.1% 7.2%±0.2%
SVM 25.8%±1.4% 21.6%±1.3% 12.9%±2.3% 7.0%±0.2%
PNN (5) 25.6%±1.3% 20.0%±1.0% 12.2%±1.7% 7.1%±0.3%
PNN, discrete patterns (16) 22.8%±1.4% 17.6%±1.1% 9.9%±1.4% 5.2%±0.3%
Kullback–Leibler 24.2%±0.8% 19.5%±1.0% 10.9%±1.5% 6.5%±0.1%
Jensen–Shannon 22.9%±0.7% 17.8%±0.9% 9.8%±1.3% 5.2%±0.6%
PNN with homogeneity testing (9) 24.3%±1.3% 19.8%±0.8% 10.7%±1.7% 6.8%±0.8%
PNN with homogeneity testing, discrete patterns (15) 22.5%±1.3% 16.8%±0.5% 8.6%±1.5% 4.5%±0.3%Table 8
The average recognition time (in µs) per one query face for FERET dataset.
Criterion Database size (R)
Features Color histogram, ∆ = 0 Histogram of oriented gradients,
∆ = 0
Histogram of oriented gradients,
∆ = 1
1030 1730 1030 1730 1030 1730
Euclidean 2516±30 4766±167 1722±34 3003±57 5549±108 8276±173
SVM 2437±27 4531±149 1886±31 2843±55 5601±110 7925±170
PNN (5) 6E9±71540 9E9±100807 9.3E9±92910 1.4E10±108705 4E10±121054 7.2E10±141559
PNN, discrete patterns (16) 10721±128 17088±481 3211±170 5048±218 59900±566 93085±1005
Kullback–Leibler 9249±180 15430±500 3179±314 5222±281 55378±713 83735±1010
Jensen–Shannon 18897±512 29037±407 4955±398 8121±447 110778±1294 176835±4235
PNN with homogeneity testing (9) 1.1E10±61824 1.6E10±90155 1.5E10±82987 2.6E10±119952 8.1E10±137626 1.4E11±160911
PNN with homogeneity testing, discrete patterns (15) 19982±442 30987±517 5133±383 8569±428 108774±1186 179266±5631Table 9
Approximate random access memory usage (in Kb) per one image.
Features Dataset
AT&T JAFFE FERET
Color histogram, ∆ = 0 Euclidean/Kullback–Leibler/Jensen–Shannon 3 8.75 8.75
PNN (5)/PNN with homogeneity testing (9) 10.06 20.51 25
PNN, discrete patterns (16)/PNN with homogeneity testing, discrete patterns (15) 6 17.5 17.5
Histogram of oriented gradients, ∆ = 0 Euclidean/Kullback–Leibler/Jensen–Shannon 1.56 1.56 1.56
PNN (5)/PNN with homogeneity testing (9) 78.91 164.0 197.5
PNN, discrete patterns (16)/PNN with homogeneity testing, discrete patterns (15) 3.13 3.13 3.13
Histogram of oriented gradients, ∆ = 1 Euclidean/Kullback–Leibler/Jensen–Shannon 6.25 6.25 6.25
PNN (5)/PNN with homogeneity testing (9) 78.91 164.0 197.5
PNN, discrete patterns (16)/PNN with homogeneity testing, discrete patterns (15) 12.5 12.5 12.5based on the reduction of face recognition task to the statistical
testing for homogeneity (in either PNN with homogeneity testing
(9), (15) or the Jensen–Shannon divergence), showed superior
results over the traditional approach with reduction to statistical
classification (conventional PNN (5), (16) and the Kullback–Leibler
minimum information discrimination principle) and machine
learning (SVM) with only several exceptions. The accuracy of
the PNN implementation (16) and our criterion (15) becomes
practically equal if the number of persons per class np increases.
If np is low, the increase of the accuracy for the proposed PNN (15)
over the PNN (16) is 1%–3%.6. Conclusion and future work
It is well-known (Theodoridis & Koutroumbas, 2009) that
the performance of the PNN is usually lower than that of the
other modern classifiers such as SVM. This fact is sometimes
explained (Savchenko, 2012b) by a too naive assumption of
feature independence which is used to estimate the class
probability density (3). Our research provides another reason:
if the classes are given only with the training samples, pattern
recognition task should be reduced to the statistical testing for
homogeneity. This reason clearly explains the theoretical cause
240 A.V. Savchenko / Neural Networks 46 (2013) 227–241of the fact that similarity measures widely used in homogeneity
testing (Jensen–Shannon divergence, chi-square distance (Srisuk &
Kurutach, 2003), Kullback–Leibler (Kullback, 1997) criterion (17))
show in practice better accuracy than the optimal classification
algorithms, such as (16) or (18).
In this study we proposed the novel modification (15),
(Fig. 3) of the PNN with homogeneity testing (Savchenko, 2012a)
in statistical recognition of a set of discrete patterns. We
experimentally proved that this network is in some terms better
than the conventional PNN (15). It is demonstrated that our
network (Fig. 3) proposed here can be used in various pattern
recognition tasks, such as image classification and text author
identification. The best accuracy of the proposed network (15)
exceeds the accuracy of the PNN (16) on 1%–5% for two crucial
tasks, namely, text authorship attribution (Table 1) and face
recognition (Tables 3, 5 and 7). Our experiments demonstrate that
though state-of-the art-classifiers such as SVM are characterized
by the lowest classification speed in practically all our experiments
(Tables 2, 4, 6 and 8) and the better accuracy than the PNN (5), it is
not the best choice even in comparison with the nearest neighbor
rule and Euclidean distance if the size of feature vector is large (in
comparison with the database size R and the number of models
per each class). Moreover, it was shown in the experiment with
authorship attribution, that equal feature vectors may appear in
distinct classes, so SVM for character n-grams was not able to
converge to any decision (Table 1). Thus, in the task of recognition
of a set of patterns SVM should be applied to the histograms of
features.
Themost remarkable result of our study is the fact thatmapping
features to a discrete range in (15), (16) leads to a significant im-
provement not only of the recognition speed but also of the recog-
nition accuracy (2%–4%) in comparison with our original network
(9) and conventional PNN implementation (5) in image recognition
(Tables 3–8). Thus, though most widely-used features are either
continuous or have a wide range of definition, the math model of
recognized object as a sample of i.i.d. discretized local feature vec-
tors seems to be more appropriate than the usage of continuous
features. An additional research here should be devoted to an en-
hancement of this model by using the Markov property as in the
algorithm of Russian text author identification (Kukushkina et al.,
2001). Actually, joint probability in (3)may be estimated by factor-
ization (as in HMM with the Viterbi algorithm, or in probabilistic
graphical models). Such an estimation is potentially able to lead to
a significant improvement of recognition quality if the size of the
training set is enough.
Another advantage of the proposed network is that the proper
choice of the smoothing parameter σ of the Gaussian kernel (4)
is not as complex as for the conventional PNN (Jones et al., 1996;
Mao, Tan, & Ser, 2000). Our experimental study showed that the
criterion (15) is much more resistant to change of σ than the
PNN (16). Thus, our network (Figs. 2 and 3) is expected to achieve
better quality in time-varying environment (Rutkowski, 2004a).
Moreover, this stability to the choice of σ explains the recognition
performance similarity of the proposed modification (15) and the
nearest neighbor rulewith the Jensen–Shannon divergence. Really,
the latter is the special case of our network if the discrete delta
function (limit of Gaussian kernel if σ → 0) is used as a kernel
(see Eq. (17) for the Kullback–Leibler test for homogeneity) and
query and training samples have equal size. At the same time, the
performance of original PNN’s implementation (16) and its special
case, the Kullback–Leibler minimum information discrimination
principle, is quite distinct as the choice of the smoothing parameter
in (16) is much more significant. As a matter of fact, our criterion
(15) does not require the Gaussian Parzen kernel (4) to estimate
the probability density. Other modern density estimators which
do not have the drawbacks of the Parzen window (Jenssen et al.,2006), could be applied with our network. Really, the assumption
that probability distributions can be estimated using Parzen kernel
with Gaussian window with a fixed variance for all the classes is
generally not true. Hence, a study of the proposed PNN’s quality
dependence on the probability density estimator is one of themain
directions in its further development.
It is necessary to mention that our network (Fig. 3) has several
shortcomings. First of all, our network is not as general as the tra-
ditional PNN because we require the network input to be a sample
with the size which is the same order of magnitude as the train-
ing sample size. Second, the classification speed is twice higher
than the speed of (16) and much exceeds the CPU time for modern
classifiers (e.g. SVM). However, this fact is not a real obsta-
cle in practical pattern recognition tasks (author identification,
image recognition), as the training sample size is not usually
large. Nevertheless, it is necessary to provide an experimental
research of the application of criterion (15) with any approxi-
mate nearest-neighbor methods (e.g., our directed enumeration
method, Savchenko, 2012b) to increase the image recognition
speed. Really, though our criterion demonstrated superior results
over the Euclidean metric (see Tables 3, 5 and 7), its computa-
tional complexity is rather high. Criterion (15) requires such op-
erations as division and logarithms, hence, it is 4–20 times slower
than the nearest neighbor rule with SVM or the Euclidean distance
(Tables 4, 6 and 8). Thus, the increase of the computational effi-
ciency remains the main problem for practical implementation of
our criterion (15).
The other direction for further research of our PNN with ho-
mogeneity testing (Figs. 2 and 3) could be related to its applica-
tion to other significant pattern recognition tasks. For instance, the
idea of estimating the features on the basis of the united sample
{X,Xr} can be used in the speech segmentation task (Qiao, Shi-
momura, & Minematsu, 2008). Really, phonemes are supposed to
be a homogeneous speech signals (Maheswari et al., 2009), thus
phoneme extraction could be made with an enhancement of our
network.
Acknowledgment
This study was carried out within ‘‘The National Research
University Higher School of Economics’ Academic Fund Program
in 2013–2014, research grant No. 12-01-0003’’.
References
Adelia, H., & Panakkat, A. (2009). A probabilistic neural network for earthquake
magnitude prediction. Neural Networks, 22(7), 1018–1024.
Aibe, N., Mizuno, R., Nakamura, M., Yasunaga, M., & Yoshihara, I. (2004).
Performance evaluation system for probabilistic neural network hardware.
Artificial Life and Robotics, 8(2), 208–213.
Aizerman, M. A., Braverman, E. M., & Rozonoer, L. I. (1964). Theoretical foundations
of the potential function method in pattern recognition learning. Automation
and Remote Control, 25, 821–837.
Borovkov, A. A. (1998). Mathematical statistics. Amsterdam: Gordon and Breach
Science Publishers.
Cortes, C., & Vapnik, V. N. (1995). Support-vector networks.Machine Learning , 20(3),
273–297.
Cover, T. M., & Hart, P. E. (1968). Nearest neighbor pattern classification. IEEE
Transactions on Information Theory, 13, 21–27.
Dalal, N., & Triggs, B. (2005). Histograms of oriented gradients for human detection.
In Proceedings, international conference on computer vision & pattern recognition
(pp. 886–893).
Duda, R. O., Hart, P. E., & Stork, D. G. (2001). Pattern classification. New York: Wiley.
Efromovich, S. (1999). Nonparametric curve estimation. Methods, theory and
applications. New York: Springer-Verlag.
Fattah, M. A., Ren, F., & Kuroiwa, S. (2006). Probabilistic neural network based
English-Arabic sentence alignment. In LNCS: Vol. 3878. Proceedings, interna-
tional conference on computational linguistics and intelligent text processing
(pp. 97–100).
Greblicki, W. (1978). Asymptotically optimal pattern recognition procedures with
density estimates. IEEE Transactions on Information Theory, 24, 250–251.
Grim, J., & Hora, J. (2008). Iterative principles of recognition in probabilistic neural
networks. Neural Networks, 21(6), 838–846.
A.V. Savchenko / Neural Networks 46 (2013) 227–241 241Györfi, L., Kohler, M., Krzyzak, A., & Walk, H. (2002). A distribution-free theory of
nonparametric regression. New York: Springer-Verlag.
Heinen, M. R., & Enge, P. M. (2010). An incremental probabilistic neural network
for regression and reinforcement learning tasks. In LNCS/LNAI: Vol. 6353.
Proceedings, international conference on artificial neural networks (pp. 170–179).
Hsu, C.-W., & Lin, C.-J. (2002). A comparison of methods for multiclass support
vector machines. IEEE Transactions on Neural Networks, 13(2), 415–425.
Jia, H., & Martinez, A. M. (2009). Support Vector Machines in face recognition with
occlusions. In Proceedings, IEEE computer society conference on computer vision
and pattern recognition (pp. 136–141).
Jenssen, R., Erdogmus, D., Principe, J. C., & Eltoft, T. (2006). Some equivalences
between kernel methods and information theoretic methods. Journal of VLSI
Signal Processing , 45, 49–65.
Jones, M. C., Marron, J. S., & Sheather, S. J. (1996). A brief survey of bandwidh
selection for density estimation. Journal of the American Statistical Association,
91, 401–407.
Juola, P. (2006). Authorship attribution. Foundations and Trends in Information
Retrieval, 1(3), 233–334.
Kullback, S. (1997). Information theory and statistics. New York: Dover Pub.
Kukushkina, O. V., Polikarpov, A. A., & Khmelev, D. V. (2001). Using literal
and grammatical statistics for authorship attribution. Problems of Information
Transmission, 37(2), 172–184.
Lin, S., Kung, S. Y., & Lin, L. (1997). Face recognition/detection by probabilistic
decision-based neural network. IEEE Transactions on Neural Networks, 8,
114–132.
Lodhi, H., Saunders, C., Shawe-Taylor, J., Cristianini, N., & Watkins, C. (2002). Text
classification using string kernels. Journal of Machine Learning Research, 2,
419–444.
Lowe, D. (2004). Distinctive image features from scale-invariant keypoints.
International Journal of Computer Vision, 60(2), 91–110.
Maheswari, N. U., Kabilan, A. P., & Venkatesh, R. (2009). Speaker independent
phoneme recognition using neural networks. Journal of Theoretical and Applied
Information Technology, 6(2), 230–235.
Manning, C., Raghavan, P., & Schütze, H. (2008). Introduction to information retrieval.
Cambridge University Press.
Mantzarisa, D., Anastassopoulosb, G., &Adamopoulosc, A. (2011). Genetic algorithm
pruning of probabilistic neural networks in medical disease estimation. Neural
Networks, 24(8), 831–835.
Mao, K. Z., Tan, K.-C., & Ser, W. (2000). Probabilistic neural-network structure
determination for pattern classification. IEEE Transactions on Neural Networks,
11, 1009–1016.
Martins, A. F. T., Figueiredo,M. A. T., Aguiar, P. M. Q., Smith, N. A., & Xing, E. P. (2008).
Nonextensive entropic kernels. In Proceedings, the 25th international conference
on machine learning (pp. 640–647).
Montana, D. (1992). A weighted probabilistic neural network. Advances in Neural
Information Processing Systems, 4, 1110–1117.
Musavi, M. T., Chan, K. H., Hummels, D. M., & Kalantri, K. (1994). On the
generalization ability of neural-network classifier. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 16, 659–663.
Pagan, A., & Ullah, A. (1999). Nonparametric econometrics. Cambridge, U.K.:
Cambridge Univ. Press.
Parzen, E. (1962). On estimation of a probability density function andmode. Annals
of Mathematical Statistics, 33, 1065–1076.
Polat, O., & Yıldırım, T. (2006). Pattern recognition without feature extraction
using probabilistic neural network. In LNCIS: Vol. 345. Proceedings, intelligent
computing in signal processing and pattern recognition (pp. 402–409).
Qiao, Y., Shimomura, N., & Minematsu, N. (2008). Unsupervised optimal phoneme
segmentation: objectives, algorithmand comparisons. In Proceedings, IEEE inter-
national conference on acoustics, speech and signal processing (pp. 3989–3992).
Raghu, P. P., & Yegnanarayana, B. (1998). Supervised texture classification
using a probabilistic neural network and constraint satisfaction model. IEEE
Transactions on Neural Networks, 9, 516–522.
Romero, R. D., Touretzky, D. S., & Thibadeau, G. H. (1997). Optical Chinese
character recognition using probabilistic neural networks. Pattern Recognition,
3, 1279–1292.
Rui, Y., Huang, T., & Chang, S. F. (1999). Image retrieval: current techniques,
promising directions and open issues. Visual Communication and Image
Representation, 10, 39–62.
Rutkowski, L. (2004a). Adaptive probabilistic neural networks for pattern
classification in time-varying environment. IEEE Transactions on Neural
Networks, 15(4), 811–827.
Rutkowski, L. (2004b). New soft computing techniques for system modeling, pattern
classification and image processing. New York: Springer-Verlag.Savchenko, A. V. (2012a). Statistical recognition of a set of patterns using novel
probability neural network. In LNCS/LNAI: Vol. 7477. Proceedings, international
conference on artificial neural networks and pattern recognition (pp. 93–103).
Savchenko, A. V. (2012b). Directed enumeration method in image recognition.
Pattern Recognition, 45(8), 2952–2961.
Savchenko, A. V. (2012c). Face recognition in real-time applications: comparison of
directed enumeration method and K − d trees. In LNBIP: Vol. 128. Proceedings,
international conference on business information research (pp. 187–199).
Savchenko, A. V. (2013). Real-time image recognition with the parallel directed
enumeration method. In LNCS/LNAI: Vol. 7963. Proceedings, international
conference on vision systems (pp. 123–132).
Schløler, H., & Hartmann, U. (1992). Mapping neural network derived from the
parzen window estimator. Neural Networks, 5(6), 903–909.
Song, T., Jamshidi, M. M., Lee, R. R., & Huang, M. (2007). A modified probabilistic
neural network for partial volume segmentation in brain MR image. IEEE
Transactions on Neural Networks, 18(5), 1424–1432.
Specht, D. F. (1988). Probabilistic neural networks for classification, mapping, or
associative memory. In IEEE international conference on neural networks, Vol. I
(pp. 525–532).
Specht, D. F. (1990a). Probabilistic neural networks. Neural Networks, 3, 109–118.
Specht, D. F. (1990b). Probabilistic neural networks and the polynomial Adaline
as complementary techniques for classification. IEEE Transactions on Neural
Networks, 1(1), 111–121.
Specht, D. F. (1991). A general regression neural network. IEEE Transactions on
Neural Networks, 2(6), 568–576.
Srisuk, S., & Kurutach, W. (2003). Face recognition using a new texture
representation of face images. In Proceedings of electrical engineering conference
(pp. 1097–1102).
Stamatatos, E. (2009). A survey of modern authorship attribution methods. Journal
of the American Society for Information Science and Technology, 60(3), 538–556.
Tan, X., Chen, S., Zhou, Z. H., & Zhang, F. (2006). Face recognition from a single image
per person: a survey. Pattern Recognition, 39(9), 1725–1745.
Tan, X., & Triggs, B. (2010). Enhanced local texture feature sets for face recognition
under difficult lighting conditions. IEEE Transactions on Image Processing , 19(6),
1635–1650.
Tse, S.-H., & Lam, K.-M. (2008). Efficient face recognition with a large database, In
Proceedings, 10th IEEE international conference on control, automation, robotics
and vision (pp. 944–949).
Theodoridis, S., & Koutroumbas, C. (2009). Pattern recognition (4th ed.). London:
Elsevier/Academic Press.
Übeyli, E. D. (2009). Probabilistic neural networks combined with wavelet
coefficients for analysis of electroencephalogram signals. Expert Systems, 26(2),
147–159.
Vapnik, V. N. (1998). Statistical learning theory. New York: Wiley.
Webb, A. R. (2002). Statistical pattern recognition. New York: Wiley.
Wolverton, C. T., & Wagner, T. J. (1969). Asymptotically optimal discriminant
functions for pattern classification. IEEE Transactions on Information Theory, 15,
258–265.
Wong, K. M., Cheung, C. H., & Po, L. M. (2003). Dominant color image retrieval
using merged histogram. In Proceedings, international symposium on circuits and
systems, 2 (pp. 908–911).
Yoo, G.-H., Kim, B. K., & You, K. S. (2007). Content-based image retrieval using
shifted histogram. In LNCS: Vol. 4489. Proceedings, international conference on
computer science (pp. 894–897).
Zhang, G. P. (2000). Neural networks for classification: a survey. IEEE Transactions
on Systems, Man, and Cybernetics — Part C: Applications and Reviews, 30(4),
451–462.
Zhao, W., & Chellappa, R. (Eds.) (2005). Face processing: advanced modeling and
methods. London: Elsevier/Academic Press.
Web references
AT&T faces dataset, (March 15, 2013) http://www.cl.cam.ac.uk/research/dtg/
attarchive/facedatabase.html.
FERET dataset, (March 15, 2013) http://www.itl.nist.gov/iad/humanid/feret/feret_
master.html.
JAFFE dataset, (March 15, 2013) http://www.kasrl.org/jaffe.html.
libSVM library, (March 15, 2013) http://www.csie.ntu.edu.tw/~cjlin/libsvm/.
OpenCV library, (March 15, 2013) http://opencv.willowgarage.com/wiki/.
The e-library of Maxim Moshkov, (March 15, 2013) http://www.lib.ru.
