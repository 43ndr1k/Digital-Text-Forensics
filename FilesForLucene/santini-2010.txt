Chapter 5
Cross-Testing a Genre Classification Model
for the Web
Marina Santini
5.1 Introduction
The main aim of the experiments described in this chapter is to investigate ways of
assessing the robustness and stability of an Automatic Genre Identification (AGI)
model for the web. More specifically, a series of comparisons using four genre col-
lections are illustrated and analysed. I call this comparative approach cross-testing.
Cross-testing exploits existing genre collections that are publicly available,1 which
have been built for individual needs and shared by their creators, thus allowing con-
structive comparative experiments. Thanks to these, big steps forward have been
made in the last few years, regardless the absence of official genre benchmarks and
test collections. Yet, the current state of AGI is one of fragmentation and tentative-
ness, and automatic genre research still lingers in the starting phase.2
The lack of benchmarks and test collections is only one of the reasons behind
AGI cautious progress. Other reasons are well summarized in the five points listed
by Sharoff (in this book) and discussed, from different perspectives, by all the other
authors contributing to this volume, namely (1) the lack of an established genre list,
(2) the unclear relation between traditional and web genres, (3) the need to classify
large quantities of web documents quickly, (4) the design of the genre inventory,
and (5) the identification of emerging genres.
From the outside, one might wonder why it is so difficult to harness and cre-
ate consent among textual categories that we habitually employ in our everyday
M. Santini (B)
KYH, Stockholm, Sweden
e-mail: marinasantini.ms@gmail.com
1 Many of them are available through the WEBGENREWIKI, at <http://purl.org/net/webgenres>.
Copyright of genre collections built with web material may vary according to national laws. The
copyright of the web pages contained in the genre collections used in this chapter is held by the
author/owner(s) of the web pages. These web pages are used for research purposes only.
2 A quite long initial phase, if we consider that this research field was initiated in 1994 with
Karlgren and Cutting’s extensively cited paper based on the Brown Corpus and discriminant
analysis [18].
A. Mehler et al. (eds.), Genres on the Web, Text, Speech and Language
Technology 42, DOI 10.1007/978-90-481-9178-9_5,
C© Springer Science+Business Media B.V. 2010
87
88 M. Santini
life. Who is not familiar with one or more of the following genres: EDITORIALS,
INTERVIEWS, LETTERS TO THE EDITOR, CLASSIFIED, WEATHER REPORTS etc. in
newspapers and magazines; or BLOGS, FAQS, HOME PAGES, PERSONAL PROFILES,
ACADEMIC PAPERS, SUGGESTIONS, HINTS, DIY GUIDES, HOW-TOS, NARRA-
TIVES, INSTRUCTIONS, ADVERTISING, etc. on the web or other digital environ-
ments? Surprisingly, findings show that agreeing on the genre labels to be applied
to documents is not so straightforward as one could imagine [23, 30].
From a terminological point of view, there is a great variation in the use of genre
labels. There are problems with synonyms, with similarity between and across gen-
res, with the level of generality or specificity that genre labels represent and so
on. In these conditions it is very difficult to make decisions about the definition of
a genre palette. Experience from Meyer zu Eissen and Stein [22], Rosso [24] and
Chapter 4 by Crowston et al. (this book) shows the problems related to the definition
of genre taxonomies. Connected to the terminological elusiveness is the problem of
genre evolution. New genres are spawned continuously in web communities (e.g.
see Chapter 13 by Paolillo et al., this book), and social networks have certainly
novel genres in store for us. Facebook WALL or LinkedIn’s PUBLIC PROFILE seem
to be good candidates in this respect. Some ideas on how to detect new genres
have been mentioned, e.g. by Shepherd et al. [34] who suggest adaptive learning.
However, human acknowledgment of new or evolving genres might be slower than
their automatic detection, since genres require social recognition, at least within the
community where a new genre is envisaged (e.g. cf. the historical analysis of BLOG
creation in Blood [4]).
From a perceptive point of view, experiments have shown that individuals have
a differing perception and recognition of genres [24, 30]. The low understanding
of how genres are perceived and how their labels are used by humans is a major
drawback for genre annotation tasks, since raters tend to disagree when deciding on
the genres to assign to documents. In this respect, the experience by Berninger et al.
[1] and Chapter 7 by Sharoff (this book) are very instructive. Both experiences show
that it is both difficult to instruct people consistently and to get strong agreement.
Only the good will of the corpus builders, lots of dedicated time, financial resources
and, last but not least, resolute and clear-cut decisions led to the finalization of
KRYS-01 and Sharoff’s English-Russian genre collections.
There is also a problem of sheer classification. The genre classes mentioned
above cannot be levelled to one dimension. There are hierarchical relations and hor-
izontal relations (cf. [14]). For instance, we can deal with supergenres (e.g. ADVER-
TISING), subgenres (e.g. WEATHER REPORTS), genres at basic level (e.g. EDITORI-
ALS),3 etc. In which way these different levels of generality affect an AGI classifier?
Some experiments have shown that an AGI classifier performs better when the level
of classes is consistent. However, we currently know very little about the relation
between the granularity of genre classes and classification performance.
3 Cf. Lee [19] for the application of these three levels following the prototype theory to genre.
5 Cross-Testing a Genre Classification Model for the Web 89
Another compelling issue concerns the ontological nature of genre. It would be
intriguing to delve into the special traits that distinguish genre from other textual
categories, such as topic, domain, style, registers or sentiment. Although a good
attempt to shed some light on these relations has been made by Lee [19] for the
genre annotation of the British National Corpus (BNC), one practical solution is
to conflate all textual categories into the catch-all term “text categories”, in line
with the Lancaster-Oslo/Bergen Corpus (LOB)4 or Brown corpus5 built about 50
years ago.
It is undoubtedly true that the term “genre” is loosely applied in everyday life
to a number of conceptually heterogeneous classes, as highlighted in Chapter 2 by
Karlgren (this book) underpins through the analyses of Yahoo! directory. His claim
is also supported by booksellers’ catalogues. For instance, under the tab “Browse
Genres”,6 Amazon UK lists many disparate categories, from genres (i.e. BIOGRA-
PHIES AND MEMOIRS) to mere descriptive labels (i.e. Subjects within Arts). As
a matter of fact, one trend in AGI experiments to date has been the separation
of genre from other textual categories and, above all, from topic. Many authors
argue that topic and genre are orthogonal to each other (see Chapter 8 by Stein
et al., this volume). However, others, like Vidulin et al. [41] experiment with
mixed textual categories, from genres like FAQS or ERROR MESSAGES to sub-
jects like “Childrens”’, to functions like “Gateway”, to less transparent labels like
“Content delivery” (see Table 5.18 for the description of these categories). Some
correlations between genres and other activities have been explored, e.g. the one
between genre and tasks [11] through an ad-hoc built corpus. Conflating genre and
topic into the anodyne label of “document types” is common practice in IR (e.g.
see [44–47]), although the collections they used are mostly topical. Interestingly,
though, corpus linguists studying language variation have shown recently that sub-
ject categories are not clearly well defined on linguistic grounds, as shown by the
findings by Biber and Kurjian [3] who have applied multi-dimensional analysis to
two Google categories, i.e. Home and Science.
In short, as this abbreviated list of issues shows, there is an ongoing heated dis-
cussion in AGI research. One practical shortcoming of this never ending debate is
the absence of common and shared genre resources. This lack hinders the creation
of agreed upon genre framework, thus affecting the progress of AGI research.
A temporary remedy to this lack is the practice of cross-testing. This practice has
been possible because some researchers have shared their own collections within the
genre community. This has allowed a number of comparative experiments (some of
them are listed in Section 5.5) that provide insights into AGI problems.
4 See <http://en.wikipedia.org/wiki/LOB_Corpus>, retrieved April 2009.
5 See <http://en.wikipedia.org/wiki/Brown_Corpus>, retrieved April 2009.
6 See <http://www.amazon.co.uk/Books-Categories/b/ref=sv_b_1?ie=UTF8&node=1025612>,
retrieved April 2009.
90 M. Santini
In this chapter, I will leverage on the practice of cross-testing to assess the robust-
ness of a simple genre classification model7 that will be described in the next sec-
tions. This model is provocatively simple and is used maieutically here to show that
genre can be captured with high accuracy (i.e. 86–96%) with any kind of features
(from high-level linguistic attributes to low-level byte n-grams) and any kind of
algorithms (from the elementary inferential/rule-based approach described in this
chapter to sophisticated statistical/mathematical methods) when genre models are
evaluated in a restricted and relatively clean in vitro settings. When one attempts
to approximate the population of web genres by introducing lot of noise and many
different characterizations of genre classes, it is difficult to understand the signifi-
cance of the results. In short, the experiments described here show that the diverse
definitions of the concepts of genres have a strong bearing on the characterization of
genre classes, thus affecting the generability of AGI models as a whole. Ultimately,
this chapter is nothing more than a strong encouragement to investigate more exten-
sively the robustness of AGI models for the web in less conventional experimental
settings in the future.
The chapter is organized as follows: Section 5.2 lists and describes the genre
collections employed to cross-test the model; Section 5.3 briefly explains the genre
palette and the features; and Section 5.4 presents the model and its motivation. In
Section 5.5 – the most articulated part of the chapter – results are reported. First,
the model is cross-tested on four genre collections independent from each other on
a single label (Sections 5.5.1, 5.5.2 and 5.5.3). Then, the accuracy of the model on
multilabelling is tested (Section 5.5.4). Section 5.6 contains the discussion of the
findings and Section 5.7 concludes the chapter.
5.2 Approximating Genre Population on the Web
Since the web is in constant flux, it is almost impossible to compile a representative
corpus/sample of the web as a whole (the multi-lingual web), or only of a single
language, like the English web. There are estimates of the number of indexed web
pages,8 which is a daily growing number, but we do not know anything about the
proportions of the different types of text on the web. Interesting approaches have
been proposed to automatically create corpora from the web, but these methods are
biased towards the construction of corpora having topic or domain as priority, rather
than genre. From a statistical point of view, when the composition of a population is
unknown, the best solution is to extract a large random sample and draw inferences
from that sample. However, deciding the size of this random sample is not a trivial
issue. In this chapter I temporarily override this problem by using some available
genre collections to cross-test the model performance. Although the total amount of
7 This model has already been presented to the genre community with a partial evaluation in Santini
[27, 29, 33].
8 In April 2005 – when the genre model described in this chapter was designed and built – Google
could search 8,058,044,651 web pages.
5 Cross-Testing a Genre Classification Model for the Web 91
the web pages of the combined genre collections used here is only 6404 (virtually
a drop in the web ocean), this amount is the largest ever used in AGI experiments
with one exception, namely the CMU genre corpus [6]. This corpus, containing
9705 documents divided into seven genres without any noise, is no longer available
and, as far as I know, it has never been used in other experiments. I conjecture
that the composite corpus of 6404 web pages described in this chapter well rep-
resents a noisy environment like the web, where documents come from disparate
communities, enact different genre conventions and classification schemes, and not
necessarily belonging to a recognized genre.
5.2.1 Noise
The impact of noise9 on genre classification results has been little explored in
AGI. The only explicit investigation was carried out by Shepherd et al. [34].
They compared the performance of two classifiers on three subgenres (i.e. 93
PERSONAL HOME PAGE, 94 CORPORATE HOME PAGE and 74 ORGANIZATIONAL
HOME PAGES) with and without noise (i.e. 77 non-home pages). Predictably, they
results show that there is a deterioration of performance when noise is introduced.
In their case, the “noise” was represented by documents that did not fall into the
three subgenres to be identified, but belonged to other genres. In their experiment,
Shepherd et al. [34] conflated into one single class all the genre classes not being
“home pages”. Decisions about the size and the proportion of this class were not
underpinned. I will call this type of noise structured noise because this noise is
represented by well-defined genre classes that should always be a negative for a
classifier.
A slightly different approach to structured noise is used Chapter 6 by Kim and
Ross (this book) and Vidulin et al. [41]. Kim and Ross considered the 24 classes
of KRYS-01 as noise with respect to the performance of their classifier on the
7-webgenre collection. In their case, noise is represented by 24 well defined genre
classes, each of them represented by a relatively small number of documents (at
most 90), while the 7 web genres are represented by 190 web pages each. Noisy
classes represent around 60%, and the 7 web genres about 40%. The size and the
proportion of this structured noise are not underpinned by any hypotheses. But accu-
racy results on the 7-webgenre collection is very good (see Table 5.5).
Interesting information on the impact of the proportion of structured noise can be
derived also from Vidulin et al. [41], though their corpus is quite small with respect
to the number of classes (see the description of MGC below). Their genre palette
is supposed to represent all the genres on the web (but their proportions seem to be
arbitrary). They build 20 individual subclassifiers and perform a binary classifica-
tion, i.e. one class against the remaining 19. These 19 classes can be considered as
9 The concept of “noise” can be applied to different situations. For example, while in Stubbe et al.
[37] “noise” refers to orthographical errors, in the present study “noise” refers to documents that
straddle to more than one genre and to documents that belong to no genre.
92 M. Santini
a kind of structured noise. In this scenario, Vidulin et al.’s [41] accuracy results are
high (94%),10 while their F-measure average on 20 genres is moderate (50%).
One problem with structured noise is that it requires a major annotation effort
because all the classes that the supervised classifier should consider as negative
examples must be clearly defined and labelled. Additionally, the underlying hypoth-
esis is quite compelling because it presupposes that all documents fall into well-
defined genres. As it will be stressed in Section 5.4, this is not always the case
with documents on the web. Many web documents might simply not belong to
any genre or embody several genres. For this reason, I tried to explore whether
unstructured noise, which is pervasive in real-world conditions, can be handled in
some way.
SANTINIS (see next section) incorporates the unstructured noise of the SPIRIT
sample. The initial observations formulated in 2005 were that it is difficult to anno-
tate by genre the whole web or a large or representative slice of it (due the annotation
problems described in the Introduction), so the challenge is to devise a classification
model robust to the unknown. The classification model presented here was provoca-
tively designed with this idea in mind and in reaction to the fully supervised Machine
Learning (ML) approach, which I employed extensively in other experiments.
The composition of the SPIRIT sample has remained completely unknown up to
very recently. But I carried out a preliminary annotation in summer 2008 to have
some insights of this unknown. My annotation reveals that the SPIRIT sample con-
tains a large proportion of web pages that could not be labelled either because I did
know if they belong to any genre at all, or because genre labels did not come to my
mind. The difficulty of formulating or uttering a genre name is the main drawback
of a genre labelling activity carried out outside any real need, context or task.
The SPIRIT samples contain also some genres that are in the model’s palette.
This would be confusing for a standard fully supervised ML classifier, which can
only handle structured noise.
5.2.2 Description of the Corpora Used for Cross-Testing
The four genre collections briefly described below have been built by different peo-
ple, for different purposes, having different priorities in mind. In the break down
that follows, I will point out the main differing characteristics, with respect to com-
position and size, collection method and annotation, main purpose, assumptions or
hypotheses, and noise.
10 As the authors point out “By splitting the multi-labeled ML problem into 20 binary sub-
problems, we got 20 unbalanced data sets with high numbers of negative and low number of
positive examples. Sub-classifiers that would recognize only negative examples would still be
highly accurate” [41].
5 Cross-Testing a Genre Classification Model for the Web 93
5.2.2.1 SANTINIS
Composition and size. Santini’s web corpus (henceforth SANTINIS) includes the
BBC web genre corpus and the 7-webgenre collection, which is, together with KI-04
(see below) a de facto standard in AGI. The four BBC web genres (20 web pages
each) are: EDITORIALS, DIY MINI-GUIDES, SHORT BIOGRAPHIES and FEATURE
ARTICLES. The seven novel web genres (200 web pages each) are: BLOGS, ESHOPS,
FAQS, FRONT PAGES, LISTINGS, PERSONAL HOME PAGES and SEARCH PAGES.
Language: English. BBC and novel genres represent the known part of the web,
i.e. about 60% of the sample. The SPIRIT sample contains 1,000 random English
web pages extracted from the SPIRIT collection [15]. The SPIRIT sample amounts
to about 40%. It is chronologically older than the rest of the SANTINIS (it was
crawled in 2001) and represents the unknown and unclassified part of the web. The
selection of the genres and the proportions of the different parts are purely arbitrary.
This corpus was created in 2005. See Table 5.15 in the Appendix.
Collection method and annotation. The annotated part of this collection has not
been manually labelled. The collection has been collected and annotated applying
the principles of “objective sources” and “consistent genre granularity” [26]. The
concept of being “objective” does not refer to any undeniable self-evident reality
(if such a thing exists). It refers to social or public behaviour and naming habits.
Basically, the principle of “objective sources” exploits the socio-cultural aspect of
the concept of genre. In simple words, it relies on the membership of web pages
in genre-specific archives or portals and uses their membership in these containers
as evidence of an automatic membership in a specific genre, no matters who and
how many decided that a certain web pages is an appropriate member for a specific
archive. In order to avoid biases, it is safe to download web pages from several
independent genre-specific archives or portals. Also the title of the documents can
be used as public acknowledgment of a certain genre. The “objective sources” that
I used to build this collection were then selected on the basis of the genre names
included in the palette that I wished to explore (see Section 5.3.1). For example,
the PERSONAL HOME PAGE genre class was selected from URL or archives con-
taining the string “personal home page”.11 Arguably, a genre collection annotated
by objective sources tends to be more representative for intra-genre variation and
closer to real-world conditions than a collection annotated relying on the genre
stereotypicality that two or a few more people have in mind. Additionally, anno-
tating a collection using objective sources is faster. The genre labels derived with
the principle of “objective sources” do not exclude the co-existence of other genres
in a web page.
The principle of “consistent genre granularity” relies on the intuition that an AGI
classifier performs better when level of generality of genre classes is consistent.
This collection has been built with classes at basic level (cf. the prototype theory
summarized in Lee [19]), because this is the level, according to the prototype the-
ory, where genre classes are better acknowledged and discriminated (cf. [19]). The
11 The list of objective sources is listed in Santini [29, Appendix A].
94 M. Santini
class LISTING, however, is a super-genre included in the collection for experimental
purposes (see [26]).
Main purpose, assumptions or hypotheses. The main purpose of SANTINIS
composition is to provide a noisy environment (presumably similar to the real web)
to assess the performance of an AGI classifier. As described in the previous para-
graphs, SANTINIS is heterogeneous in many ways. Regardless the labels assigned
and the selection method, the underlying assumption is that each web page might
belong to zero, one or multiple genres. However, the performance is assessed on the
available labels. These labels might be refined and augmented in the future.
Noise. In this collection, noise can be paraphrased as “DON’T KNOW”. Basically,
the SPIRIT sample included in SANTINIS represented the noise that can be found
on the web. Simply put, the SPIRIT sample is a random slice of the web whose
content is unknown. Therefore, it contains not only genres that are different from
those included in the model’s genre palette, but also genres that might be in the
palette. Since we do not know the number and the distribution of genres on the
web, this DON’T KNOW class is an attempt to bypass the constraint underlying ML-
based models, where the documents must be necessarily pre-assigned to known and
well-defined classes. In July 2008 I provided the SPIRIT sample with some genre
labels. It is important to stress that the SPIRIT genre annotation is to be considered
a starting point, not a validated annotation. In the future, annotation by other people
(maybe through a social network) can be added and validated through agreement
coefficients.
In my manual annotation of the SPIRIT sample, I used also “judgements”,
namely overlabelling and zerolabelling. Overlabelling is counted as a NM (No
Match) and indicates that the model’s genre palette did not contain the genre I had
in mind. For example, SPRT_002_060_117_0058030 is an ERROR MESSAGE, but
this genre is not present in the palette. In these cases, I used the label with NOTHING
SUITABLE IN THE GENRE PALETTE. My expectation is that the model assigns no
genre, i.e. I expect zerolabelling (as in the case of SPRT_010_049_112_0055944
and SPRT_022_009_162_0080850). Some other time, I could assign one of 15
genres, but the others that came to my mind were not in the model’s palette. In
these cases I assigned the label NOTHING ELSE SUITABLE IN THE GENRE PALETTE
with the expectation that the labels belonging to the palette could be matched. The
SPIRIT sample contains also web pages for which I could not find a genre. In this
case I used the label IDK (i.e. I DON’T KNOW). I considered the IDK pages as
NC (Not Classified) because I could not say whether or to what extent the model
was correct in its classification. The stand-off annotation of the SPIRIT sample is
available online.12
Although not validated, this annotation gave me an idea of the composition of the
SPIRIT sample. I assigned zero, one or more of the 15 genre classes of the model
palette, up to four genre labels. The limit of four genre labels was the spontaneous
12 The spreadsheet containing my standoff annotation is available at <http://sites.google.com/site/
marinasantiniacademicsite/>: see my_manual_genre_labelling_1000SPIRIT_webpages_NOVEM
BER2008_ matching_with_the_initial_corpus.xls.
5 Cross-Testing a Genre Classification Model for the Web 95
boundary that I found to be comfortable when manually annotating web pages. This
limit can be discussed and compared with other experiences in the future.13
5.2.2.2 KI-04
Size. The KI-04 includes 1,295 English web pages (HTML documents), but only
800 web pages (100 per genre) were used in the experiment described in Meyer
zu Eissen and Stein [22]. KI-04 is a de facto standard in AGI, together with the
7-webgenre collection. Language: English. See Table 5.16 in the Appendix.
Collection method and annotation. The KI-04 corpus was collected using book-
marks from about five people. Some genres were extended to get a better balance.
The corpus was sorted by three people, one of whom wrote a bachelor thesis (in
German) on the corpus building process. One of the creators (S. Meyer zu Eissen)
checked many of the pages, and most of the sorting complied with his understanding
of the genre categories. The download date was January, 2004.
Main purpose, assumptions or hypotheses. The KI-04 corpus was built follow-
ing a palette of eight genres suggested by a user study on genre usefulness [22].
Hence, the main purpose of this collection is to represent genres that are useful in
retrieval tasks.
Noise. KI-04 does not contain any class representing noise.
5.2.2.3 HGC
Size. The Hierachical Genre Collection (henceforth HGC) contains 32 genre classes,
40 files per class. Language: English. Collected in 2005/2006. 1,180 HGC web
pages were used in the cross-testing experiments illustrated in this chapter. HGC
is described in Stubbe and Ringlstetter [36] and Stubbe et al. [37]. No detailed
description of the genre classes is provided. See Table 5.17 in the Appendix.
Collection method and annotation. This collection was manually selected and
annotated by Andrea Stubbe. She tried to gather a broad distribution of topics for
each genre in order to avoid bias.
Main purpose, assumptions or hypotheses. HGC relies on the assumption that
genre should exclusively represent the dimensions of the form and function of a
text. The classification ought to be task oriented and hierarchical. It has to be logi-
cally consistent and complete. A certain text can be assigned to different classes, but
13 It would be interesting to define the amount of the critical mass for genre annotation, i.e. to
establish the point when the majority agrees on a number of labels for the same document. It seems
that genre annotation based of the agreement of small number of people (2, 3, 4, or a few more)
does not guarantee reliability. For instance Mikael Gunnarson, made the following observations
on the ARTICLE genres included in the KI-04 corpus, which is defined as “Documents with longer
passages of text, such as research articles, reviews, technical reports, or book chapters” [22]. In this
class, Gunnarsson found: a book announcement, a redirect page, a table of contents, bibliography,
three documents authored in German, 2 commercial portrayals, 2 help pages, 2 discussion pages,
1 link list, and 1 personal homepage among the 127 articles (personal communication). Although
intra-genre variation is, in my opinion, a positive characteristic, as well as a certain degree of noise,
after Gunnarsson’s breakdown one might wonder about the criteria for representing a genre class.
96 M. Santini
this should not be the norm. HGC is based on the genre palette proposed by Dewe
et al. [7], but it contains a finer grained hierarchy of genres, which, presumably,
meet the demands of genre focused corpus construction.
Noise. HGC includes the NOTHING class, containing error messages, empty
pages, and frame sets.
5.2.2.4 MGC
Size. The Multi-Labelled Genre Collection (henceforth MGC) was built by Mitja
Luštrek and Andrej Bratko and consists of 1,539 web pages classified into 20 genres.
Each web page can belong to multiple genres. This collection is described in Vidulin
et al. [41]. Language: English. See Table 5.18 in the Appendix.
Collection method and annotation. The corpus was manually labelled with gen-
res by two independent annotators. Their labels disagreed on about a third of the web
pages in the corpus, so these were reassessed by a third and sometimes even a fourth
annotator. The web pages were collected from the Internet using three methods,
i.e highly-ranked Google hits for popular keywords; gathered random web pages;
finally, searching for web pages belonging to the genres underrepresented to that
point to obtain a more balanced corpus.
Main purpose, assumptions or hypotheses. Genre categories were chosen with
the intention to cover the whole Internet. The genre of a web page is intended to
represent the communicational intention that shapes the page.
Noise. MGC does not contain any class representing noise.
5.3 The Web as Communication
The genre model presented here emphasizes the linguistic and pragmatic aspects
of the web. Web pages are instantiations of communicative situations where lan-
guage is used to interact in and with a context. The model relies and builds upon
Biber’s observation [2, p. 33] saying that linguistic features can be used to derive
the communicative situation in which texts have been produced, thus identifying
their communicative purposes. The genre palette and feature set (both described
below) rely on this assumption.
5.3.1 Genre Palette
Choices for the genre palette (i.e. the genres that the model can automatically iden-
tify) depend on the specific type of genre-enabled application. Presumably, a genre
palette for a digital library will be different from a palette for web retrieval, for
intranet searches, or for applications for corpus linguistics. Arguably, we cannot cre-
ate a genre palette containing all the genres in use, since they amount to thousands
(cf. [13]), which would also be very confusing for end users. Therefore, choices
must be made. It is important to note that the palette used in this experiment is not
5 Cross-Testing a Genre Classification Model for the Web 97
an “ideal” or an “all-purpose” palette. As emphasized above, each field of appli-
cation will work out the best palette and the best nomenclature/taxonomy for its
specific needs. The aim of the genre palette employed here is to investigate genres
at different granularity or level of specificity, namely four rhetorical genres, four
standardized genres coming from one domain (the BBC domain) and seven common
web genres (see Table 5.1). This genre palette is static and non adaptive. This means
that no suggestions are proposed to automatically incorporate new genre labels in
the initial set.
In order to simplify the terminology, I will refer to these categories in the follow-
ing way: “rhetorical genres”14 indicate four rhetorical patterns, while “web genres”
indicate both the BBC genres and the novel web genres. The idea is that all of them
are genres, though with different characteristics. This view is very similar to the one
proposed by Bruce ([5]; see also Chapter 15 by Bruce, this book), where social gen-
res (my web genres) are built upon cognitive genres (my rhetorical genres). These
two genre levels are tightly interrelated but they highlight different aspects in textual
communication. While rhetorical/cognitive genres represent universal communica-
tive purposes, social/web genres are historical entities with a life cycle. On the one
hand, rhetorical/cognitive genres help harness the instability of the web or other
noisy digital environments, because they are more stable than social genres. On the
other hand, social/web genres, that come and go, and are very linked to technology,
can be analysed and identified in terms of the communicative purpose they convey.
It can also be said that rhetorical genres are more general and social genres more
specific. It is worth highlighting that the computation of rhetorical genres as an
Table 5.1 Genre palette
Rhetorical Genres (A.K.A. Cognitive genres or text types)
(1) Descriptive_narrative
(2) Explicatory_informational
(3) Argumentative_persuasive
(4) Instructional
Traditional BBC web genres
(5) BBC DIYs
(6) BBC editorials
(7) BBC short biographies
(8) BBC feature articles
Novel web genres
(9) Blogs
(10) Eshops
(11) FAQs
(12) Online newspaper front pages
(13) Listings
(14) Personal home pages
(15) Search pages
14 Following Biber’s tradition [2], I had named them “text types” in my previous publications.
98 M. Santini
intermediate step is useful if we see genres as conventionalised and standardized
cultural objects raising expectations about the purposes of communication. For
example, what we expect from an BBC EDITORIAL is an “opinion” or a “comment”
by the editor, which represents, broadly speaking, the view of the newspaper or mag-
azine. Opinions are a form of ARGUMENTATION. ARGUMENTATION is a rhetorical
genre expressed by a combination of linguistic features (the facets). If a document
shows a high probability of being argumentative, i.e. if it has a high gradation of
ARGUMENTATION, this document has a good chance of belonging to argumentative
genres, such as EDITORIALS, SERMONS, PLEADINGS, and ACADEMIC PAPERS. It
has less chances of being a STORY or a BIOGRAPHY, which are narrative genres.
5.3.2 Linguistically- and Functionally-Motivated Features
The genre model relies on features that I call facets. Broadly speaking, the word
“facet” indicates an “aspect” of a situation, a concept, and so on. I used the word
“facet” because each facet represents an “aspect” of communication. My facets are
macro-features, i.e. they contain several micro-features. The advantage of these fea-
tures is that they allow inference. While, shallow features are often unmeaningful
for human understanding (e.g. character or byte n-grams), facets are higher order
features can be easily understood and employed for reasoning.
For example, the first person facet includes first person pronouns, singular and
plural. The first person facet indicates that the communication context is related to
the text producer. A high frequency of first person facets in a text signals an impres-
sionistic or subjective stance of the text producer. While in previous genre clas-
sification approaches, pronouns were used individually without any further inter-
pretation, with the first person facet my aim is to interpret, or assess, whether first
person pronouns indicate a particular stance in communication, and if this stance is
linked to a genre. For instance, a high frequency of first person facet is often used
in ARGUMENTATIVE genres, like COMMENTS and OPINIONS that can be found in
newspapers and magazines.
I created 100 facets (listed in Table 5.19, in the Appendix). Facets can be refined
and their number increased if they prove to be useful for AGI. The motivation, cre-
ation extraction, and drawbacks of facets are fully described in Santini [25, 29].
5.4 The Genre Model
The simple genre model that I describe and cross-test in this chapter has been
built to fill a specific gap, namely the computability of the relation between genres
and rhetorical patterns. In this respect, it complements more habitual approaches
to AGI.
The model challenges two commonplaces in AGI, namely the predilection for
shallow features and the use of fully-supervised ML approaches.
5 Cross-Testing a Genre Classification Model for the Web 99
The use of shallow features is well justified because they are supposed to
be potentially crosslingual and computationally inexpensive (cf. Chapter 7 by
Sharoff; Chapter 8 by Stein et al., this book). Shallow features that have been used
recently include n-characters n-grams [17], byte n-grams [21], harmonic descriptors
(Chapter 6 by Kim and Ross, this book), and POS trigrams (Chapter 7 by Sharoff,
this book).
These features with standard or adapted classifiers have high performance in
small genre collections (see Table 5.5). However, the actual potential of these fea-
tures in larger collections, for multi-lingual genre classification or in a more realistic
environment is virtually unknown, although Chapter 7 by Sharoff (this book) and
WEGA (Chapter 8 by Stein et al., this book; Stein and Meyer zu Eissen [35]) show
some preliminary attempts towards multi-linguality. Additionally, the performance
of the two genre-enabled existing IR applications relying on shallow features –
namely, WEGA (Chapter 8 by Stein et al., this book and X-Site [11] – still requires
substantial enhancements.
Another common stance in AGI is the preference for fully supervised ML algo-
rithms. Unfortunately, there are several disturbing factors that hinder the plain appli-
cation of fully supervised ML to AGI. As ML models are build from examples, the
penury of genre annotated material and the approximation of the genre population
are major stumbling blocks. Current genre collections are tiny, ranging from the
200 web pages used in Chapter 7 by Sharoff (this book) to the 1,539 web pages
used in Vidulin et al. [41], to the 3,685 pdf files employed in Chapter 6 by Kim
and Ross (this book). Above all, these collections are very subjective, since they
have all been built for individual or specific needs, collected with differing selec-
tion methods, following different conception of genres, including different genre
palettes. Additionally, the performance of fully supervised methods relies heavily
on the ideal combination of the following elements: a predictable population, an
ideal genre palette, and large quantities of manually annotated stereotypical web
pages. Consequently, it is difficult to draw any significant conclusions about the
effectiveness of AGI since current findings are based on the tiny sizes of exist-
ing genre collections. Although there are ideas on how to create larger corpora of
consensual genre-annotated material,15 these ideas have not been implemented yet.
In sum, being AGI in such a preliminary stage of research, there is no reason for
not exploring other approaches that are alternative and complementary to shallow
features,16 manual annotation and ML.
The genre model, described and cross-tested in the following sections, explores
the possibility of encoding genre knowledge in the classification model, rather than
deriving it from stereotypical examples. This model has no ambition of becoming
an industrial or commercial prototype, at least not in the implementation presented
here. This is simply an explorative model that investigates the power of language
15 For example, Rosso suggested that genre tags could be added (with a special genre-enabled tool)
within social networks (personal communication).
16 Cf. also the interesting experiments with “heavy” visual features carried out by Levering et al.
[20] in order to detect subgenres.
100 M. Santini
in detecting genre classes and the relationship between rhetorical patterns and web
genres.
The model relies upon linguistically rich features and layout information, and
follows a multilabel scheme. It has been devised thinking of an open digital envi-
ronment, like the web, where the level of noise is high and the population is difficult
to approximate. Its task is to apply either no genre – when a document is highly
individualized – or one genre – when the document belongs to a single genre – or
multiple genres – when a document contains several genres or is hybrid. The unit
of analysis is an English individual web page, including boilerplates and naviga-
tional text.
Empirical observations have led me to include the attributes of genre hybridism
and individualisation in the characterization of the genre of web pages [28]. These
two attributes account for classification hurdles, and help pinpoint the range of flex-
ibility that an automatic genre classification system should have. I suggested that
genre hybridism accounts for multi-genre classification, whereas individualisation
accounts for zero-genre classification. Such a broad range of flexibility is not per-
mitted by standard discrete single-label supervised classification algorithms. The
efficacy of ML multi-label classifier, like LIBSVM, has been little explored in AGI
(one experience is described in Vidulin et al. [42]).
The model goes beyond the single-label assignment and does not require any
annotation of web pages by genre. This model has two main characteristics: (i) it
makes a clear-cut distinction between rhetorical genres and web genres, and (ii) it
is based on inference rather than supervised learning.
The first original trait relies on the separation of the concepts of rhetorical genres
and web genres, where rhetorical genres represent a middle layer between func-
tionally interpreted features – the facets (Section 5.2) – and web genres. This inter-
mediate level gives flexibility to an automatic genre classification system because
rhetorical genres are linguistic devices that represent the purpose of communica-
tion, and are more universal than genres since they span across all cultures and all
times. Web genres, on the other hand, are (like all other social genres, see Bruce
[5]) cultural artefacts, linked to a historical context, and in constant evolution. By
using rhetorical genres, an analysis will remain possible on all media (printing, the
web, mobile phones, etc.) even if genres evolve and texts cannot be safely ascribed
to any existing genre (zero-genre assignment), or if texts show several genres at the
same time (multi-genre assignment), since rhetorical genres help relate genres to
one another across old and new media.
The second original trait relies on inference rather than supervised ML. More
precisely, rhetorical genres are inferred using a modified form of Bayes’ theorem –
the odds-likelihood or subjective Bayesian method – and web genres are derived
using a few inferential if-then rules. For this reason, I refer to this model as the
inferential model. Inference is possible thanks to high-level, linguistically rich and
functionally motivated features (Section 5.2). While shallow features are opaque to
human understanding, deeper linguistic features allow functional interpretation of
texts. Biber [2] showed that functionally interpretable, linguistic features correlated
through factor analysis return textual dimensions (Biber’s text types) that can tell us
5 Cross-Testing a Genre Classification Model for the Web 101
something about the communicative situation in which a text has been produced.17
The ultimate goal is to investigate to what extent complex linguistic features can
allow genre classification with noisy and heterogeneous corpora, more similar to a
real-world scenario. For this reason, the inferential model will be cross-tested with
a number of different genre collections.
5.4.1 Methodology
The model implements a simplified form of Bayes’ theorem called odds-likelihood
or subjective Bayesian method, suggested by Duda and co-workers [9, 10] to handle
uncertainty in PROSPECTOR, a rule-based system for classifying mineral explo-
ration prospects. The main reason for choosing the odds-likelihood form of Bayes’
theorem is that the model is very simple, but allows more complex reasoning
through the use of weights. Like the standard Bayesian version, the odds-likelihood
method is based on probabilities. In the flow, probabilities are converted into odds.
Odds and probabilities contain exactly the same information and are interconvert-
ible. But odds are not limited to the range 0–1, like probabilities. In other words,
odds is a positive integer (without any limitation) that tells us how much more
likely one hypothesis is than the other. Odds are usually used for games of chance,
where the probabilities are expressed in the form of integer-to-integer (e.g. “six-
to-one”) where the first figure represents the number of ways of failing to achieve
the outcome and the second figure is the number of ways of achieving a favourable
outcome. The main difference between the regular Bayes models and the subjective
one is that in the latter attributes are NOT considered to be equally important, but
are, instead, weighted according to their probability value. Therefore, in the odds-
likelihood version of Bayes’ theorem much of the effort is devoted to weighing
the contributions of different pieces of evidence in establishing the match with a
hypothesis. These weights are confidence measures: Logical Sufficiency (LS) and
Logical Necessity (LN). LS is used when the evidence is known to exist (larger
value means greater sufficiency), while LN is used when evidence is known NOT
to exist (a smaller value means greater necessity). One important point to make is
that the facets, i.e. the pieces of evidence on which the model relies upon, are not
just either present or absent: their presence or absence is considered to be uncertain.
Therefore, LS and LN can be viewed as the limits of the interval in which lies the
value indicating the degree to which a facet influences the prior probability of H
(the hypothesis). In this implementation of the model, LS was set to 1.25 and LN
was set to 0.8 on the basis of previous experience and empirical adjustments. While
in this implementation of the model, LS and LN have a single value for all the
facets (i.e. LS is always 1.25 and LN always 0.8), it is also possible to compute a
17 “The notion of function is closely associated with the notion of situation. A primary motivation
for analysis of the components of situation is the desire to link the functions of particular linguistic
features to variation in the communicative situation” [2, p. 33].
102 M. Santini
weight for each facet, because some facets can be more indicative than others. But
in order to do so, it would be necessary to have a corpus of documents already
classified by rhetorical genres, which is not a trivial endeavour, as pointed out
earlier.
5.4.2 Flow and Hypotheses
The inferential model is based on the following steps:
1. Extraction, count and normalization of linguistic facets.
2. Conversion of normalized counts into z-scores, which represent the deviation
from the “norm” coming out from the web corpus.
3. Conversion of z-scores into probabilities, which means that facet frequencies are
seen in terms of probabilities distribution.
4. Calculation of prior odds from prior probabilities of a rhetorical genre. The prior
probability for each of the four rhetorical genres was set to 0.25 (all rhetorical
genres were given an equal chance to appear in a web page). Prior odds are
calculated with the formula:
prior_Odds(H)=prior_Prob(H)/1-prior_Prob(H)
5. Calculation of weighted facets, or multipliers (M). If a facet, i.e. a piece of evi-
dence (E), has a probability >= 0.5, LS is applied, otherwise LN is applied.
Multipliers are calculated with the following formulae:
if Prob (E)>=0.5 then
M(E)=1+(LS-1)(Prob(E)-0.5)/0.25
if Prob (E)<0.5 then
M(E)=1-(1-LN)(0.5-Prob(E)/0.25
6. Multiplication of weighted probabilities together, according to the co-occurrence
decided by the analyst on the basis of previous studies.
7. Posterior odds for the rhetorical genre is then calculated by multiplying prior
odds (Step 5) with co-occurrence of weighted facets (Step 7).
8. Finally, posterior odds is re-converted into a probability value with the following
formula:
Prob(H)=Odds(H)/1+Odds(H)
At the end of this flow, the model returns the inferred rhetorical genre associated
to a score. Scores are interpreted in terms of degree or gradation. For example, a
web page with a score of 0.9 of being argumentative shows a very high degree,
or gradation, of ARGUMENTATION. As explained later, the different gradations are
independent from each other. In other words, the different scores accounting for
the four rhetorical genres in a web page do not sum up to 1.0, but they simply
indicate the gradation, and not the proportion, of a certain rhetorical genre. Scores
5 Cross-Testing a Genre Classification Model for the Web 103
are then ranked in descending order (the highest score gets the first position). After
the ranking, two hypotheses are tested:
1. The first hypothesis says that the combination of a number of rhetorical genres
is sufficient to derive four BBC web genres – EDITORIALS, DIY MINI-GUIDES,
SHORT BIOGRAPHIES, and FEATURE ARTICLES –, more traditional in their tex-
tuality. This hypothesis is tested with rules that combine only rhetorical genres,
without any additional features. An example of these rules to derive BBC genres
is shown in Box 5.1. These rules are used to assess the classification accuracy of
four BBC web genres immersed in increasingly larger corpora (see Table 5.3).
2. The second hypothesis says that the combination of two predominant rhetorical
genre, i.e. the top-ranked rhetorical genres, plus a combination of additional traits
is sufficient to derive seven web genres – BLOGS, ESHOPS, FAQS, FRONT PAGES,
LISTINGS, PERSONAL HOME PAGES, and SEARCH PAGES –, more influenced by
the functionalities allowed by the web. This hypothesis is tested with rules take
combines rhetorical genres plus additional features. An example of theses rules
to derive webgenres is shown in Boxes 5.2 and 5.3. These rules are used to assess
the classification accuracy of seven novel web genres immersed in increasingly
larger corpora (see Table 5.4).
Box 5.1 Rules for BBC DIY Mini-Guides
if (text_type_1=/instructional_1|argumentative_persuasive_1/)
if (text_type_2=/argumentative_persuasive_2|instructional_2/)
if (text_type_3/expository_informational_3|descriptive_narrative_4/)
Box 5.2 Positive Rules for Blogs
if (text_type_1=descr_narrat_1|argum_pers_1)
then add 1 to goodBlogCandidate
if (text_type_2=descr_narrat_2|argum_pers_2)
then add 1 to goodBlogCandidate
if (page_length=LONG)
then add 1 to goodBlogCandidate
if (blog_words >= 0.5 probabilities)
then add 1 to goodBlogCandidate
if goodBlogCandidate >=3
then good BLOG candidate.
Box 5.3 Negative Rules for Blogs
if (frontpage_words > blog_words)
then subtract 1 to goodBlogCandidate
104 M. Santini
There is no special reason for combining only two predominant rhetorical genres
instead of three or more. The basic assumption is that web pages are mixed. With
BBC web genres, I stress the intra-genre linguistic mixture, while with seven novel
web genres I include additional traits, like layout and functionality, in the genre
profiling. Obviously, web pages may contain many other rhetorical genres, not only
the ones included in this implementation of the model. This palette is a starting point
and can be enlarged and adjusted in future.
Simple if-then rules combine inferred rhetorical genres with additional traits for
determining web genres in web pages. The main reason for not applying odds-
likelihood here is that the combination of rhetorical genres with other features is
more tentative; inference rules allow us to better understand how a conclusion is
reached. Naturally, any future enhancement of this model would include the devel-
opment and debugging of more sophisticated rules and the setting of thresholds.
The number of positive rules, i.e. those that confirm the presence of positive
attributes for the genre under assessment, is very limited:
• 3 rules for each of the BBC genres;
• 4 rules for BLOGS;
• 7 rules for ESHOPS;
• 5 rules for FAQs;
• 8 rules for FRONT PAGES;
• 5 rules for LISTINGS;
• 4 rules for PHP;
• 9 rules for SEARCH PAGE.
The number of negative rules, i.e. those that disconfirm the presence of positive
attributes for the genre under assessment is very low for BLOGS, and slightly higher
for other web genres.
5.5 Results
As emphasized earlier, the main purpose of this chapter is to explore how to assess
the robustness of genre models in noisy scenarios representing the web. These sce-
narios are represented by a number of genre collections, which will be used in com-
bination with each other and in isolation. In this section, results are described. First,
the performance on the single labels of the four BBC genres and the seven novel
genres is shown (Sections 5.5.1, 5.5.2 and 5.5.3). Then the performance achieved
on the 7-webgenre collection is compared with the results obanained by other AGI
models and differing feature sets (Section 5.5.4). Finally, an attempt to assess multi-
labelling is illustrated (Section 5.5.5).
For single label experiments, the evaluation measure employed to compare
results is accuracy, i.e. the percentage of correct guesses on the BBC and the
7-webgenre collections.
5 Cross-Testing a Genre Classification Model for the Web 105
Fig. 5.1 Excerpt from the output of the inferential model
For the multilabelling, matching accuracy and overlap coefficients are computed
on the SPIRIT sample.
It is worth noting that the model has not been adjusted, adapted or optimised
when applied to the different genre collections. Classification settings and feature
set have been kept constant. An excerpt of the classification output is shown in
Fig. 5.1. All the spreadsheets containing the inferential model’s classification are
available online.18
5.5.1 Cross-Testing Performance on Single Labels: BBC
and 7-Webgenre Collections
In this subsection, the model scalability in noisy environments is tried out by pro-
gressively increasing the size of the web corpus in three steps. In the first step, the
model is applied to 2,480 web pages; in the second step on 3,685 web pages; in the
final step on 6,404 web pages. The classification performance is compared on the
single labels of the BBC and 7-webgenre collections.
5.5.1.1 SANTINIS (2,480 Web Pages)
The inferential model achieves an accuracy of about 86% on SANTINIS (see
Table 5.2, forth column) on the single labels of the 7-webgenre collection. The
18 See all the excel files whose names start with “GIMs” at http://sites.google.com/site/
marinasantiniacademicsite/.
106 M. Santini
accuracy of 86% returned by the model is a good achievement for a first imple-
mentation, especially if we consider that the standard Naïve Bayes classifier returns
an accuracy of about 67% (see Table 5.2, third column). Although SVM achieves
an accuracy of about 89% (see Table 5.2, second column), i.e. about +3% more
than my genre model, it is worth stressing that both Naive Bayes and SVM standard
classifiers were run on 1,400 web pages, i.e. they were built only on the 7-web-
collection (Section 5.3). The inferential model, on the other hand, is run on the
noisy SANTINS, made of 2,480 web pages (Section 5.3). This means that the model
is robust to a certain level of noise, represented the SPIRIT sample and the BBC
collection.
Although theoretically unsound, for explanatory purposes I built an UNKNOWN
class. This means that I built an SVM classifier with SANTINIS 2,480 web pages
and 100 facets. Web pages belonging to the BBC collection and the SPIRIT sample,
i.e. 1,080 web pages, were labelled as DONTKNOW.
The stratified 10-fold-cross-validated accuracy (seed 1) on the SVM model built
with eight classes is about 76%, i.e. about –13% of the accuracy achieved with
a corpus of 1,400 web pages. In this respect, the inferential model is much more
corpus independent, returning an accuracy of about 86% on 2,480 web pages, i.e.
about +10% more than the model built with SVM on eight classes. The DONT-
KNOW class (Fig. 5.2, column “h”) causes many misclassifications. This confirms
the observation made by Shepherd et al. [34] that the introduction of noise – in this
case, unclassified web pages – significantly decreases the accuracy of the a fully
supervised classifier.
In order to test its robustness to scalability, an increase of corpus size was simu-
lated by adding first the KI-04, and finally both HGC and MGC.
Table 5.2 Comparing accuracies: SVM, Naïve Bayes and the inferential genre model
SVM (1,400 Naïve Bayes Inferential model
Web genres web pages (%) (1,400 web pages (%) (2,480 web pages)
Blogs 96 92 91% (18 bad blog; 182
good blog)
Eshops 88 76 83% (4 bad eshop; 166
good eshop)
FAQs 94.5 67 88.5% (23 bad FAQ; 177
good FAQ)
Front pages 100 98 97% (6 bad frontpage; 194
good frontpage)
Listings 80 29 75.5% (49 bad listing; 151
good listing)
Pers. home pages 79 27 77% 46 (bad PHP; 154
good PHP)
Search pages 85 82 88% (24 bad spage; 176
good spage)
Total About 89% About 67% About 86%
5 Cross-Testing a Genre Classification Model for the Web 107
Fig. 5.2 SVM confusion matrix: misclassifications caused by the DONTKNOW class
5.5.1.2 SANTINIS+KI-04 (3,685 Web Pages) & SANTINIS+
KI-04+HGC+MGC (6,400 Web Pages)
The accuracies achieved with the inferential models on BBC web genres and on the
7 novel genres are shown in Tables 5.3 and 5.4.
Differences in accuracy are statistically (chi-square19) not significant for the
BBC web genres (Table 5.3), but significant for the 7 novel web genres (Table 5.4).
On the first enlarged corpus (i.e. 3,685 web pages), the accuracy achieved on
a single genre is about 81%. These results are encouraging, since a size increase
of 35% causes only 5% decrease in accuracy. The same is true with the second
enlarged corpus (6,404 web pages), where the accuracy achieved on a single genre
is about 76%. This means that a size increase of 60% causes only 10% decrease in
accuracy. It is interesting to note that the size increase is noisy, i.e. the corpus has
been enlarged with additional genre collections containing diverse genres, not just
incrementing the size of existing, well-defined genre classes. As mentioned above,
Table 5.3 Accuracies on the four BBC web genres
Accuracies on
Accuracies on Accuracies SANTINI SANTINIS + KI-
BBC SANTINIS (2,480 web + KI-04 (3,685 web 04+HGC+MGC
Web Genres pages) pages) (6,404 web pages)
BBC DIY 95% (1 bad DIY; 19
good DIY)
85% (3 bad DIY; 17
good DIY)
85% (3 bad DIY; 17
good DIY)
BBC editorial 75% (5 bad editorial;
15 good editorial)
75% (5 bad editorial;
15 good editorial)
70% (6 bad editorial;
14 good editorial)
BBC bio 85% (3 bad bio; 17
good bio)
75% (5 bad bio; 15
good bio)
85% (3 bad bio; 17
good bio)
BBC features 60% (8 bad feature; 12
good feature)
50% 10 bad feature; 10
good feature
60% (8 bad feature;
12 good feature)
Total About 79% About 71% About 75%
19 Chi-square calculator: <http://www.physics.csbsju.edu/cgi-bin/stats/contingency_form.sh?
nrow=2&ncolumn=2>. (April 2009)
108 M. Santini
Table 5.4 Accuracies on the seven novel web genres
Accuracies on
SANTINIS+KI-
Accuracies on Accuracies on 04+HGC+MGC
7 novel SANTINIS (2,480 SANTINI + KI-04 (6,404 web
web genres web pages) (3,685 web pages) pages)
Blogs 91% (18 bad blog; 182
good blog)
72% (56 bad blog; 144
good blog)
93% (14 bad
blog; 186
good blog)
Eshops 83% (34 bad eshop;
166 good eshop)
78.5% (43 bad eshop;
157 good eshop)
66% (68 bad
eshop; 132
good eshop)
FAQs 88.5% (23 bad FAQ;
177 good FAQ)
84% (32 bad FAQ; 168
good FAQ)
88.5% (23 bad
FAQ; 177
good FAQ)
Front pages 97% (6 bad frontpage;
194 good frontpage)
96.5% (7 bad
frontpage; 193 good
frontpage)
61% (78 bad
frontpage;
122 good
frontpage)
Listings 75.5% (49 bad listing;
151 good listing)
74% (52 bad listing;
148 good listing)
75% (50 bad
listing; 150
good listing)
Personal home
pages
77% 46 bad PHP; 154
Good PHP
77.5% (45 bad PHP;
155 good PHP)
81% (38 bad
PHP; 162
good PHP)
Search pages 88% (24 bad spage;
176 good spage)
85.5% (29 bad spage;
171 good spage)
69% (62 bad
spage; 138
good spage)
Total About 86% About 81% About 76%
such an environment presumably represents the unpredictable population of the web.
Apparently, the model is robust enough to withstand, to some extent, the chaos of
the web.
5.5.2 Performances of Other Single-Label Models
on the 7-Webgenre Collection
It is interesting to compare the performances that other researches have obtained on
the 7-webgenre collection, which has been extensively used in other genre exper-
iments. As mentioned in Section 5.3, this collection (together with KI-04) has
become a de facto standard in AGI research. Table 5.5 shows some results.
When the 7-webgenre collection is used in isolation (rows 1–7), classification
results increase with the number of features. Accuracies are all very high: from
88.8% achieved with 100 features (row 3) to 96.5 with >3,000 features (row 6).
Since long vectors always raise the suspicion of overfitting, Kanaris and Stamatatos
[17] have cross-checked their features by classifying the 7-webgenre collection
using features extracted from the KI-04 corpus with a very good accuracy, i.e.
5 Cross-Testing a Genre Classification Model for the Web 109
Ta
bl
e
5.
5
Pe
rf
or
m
an
ce
s
on
th
e
7-
w
eb
ge
nr
e
co
lle
ct
io
n
w
ith
an
d
w
ith
ou
tn
oi
se
A
cc
ur
ac
y
on
th
e
#
of
w
eb
7-
w
eb
ge
nr
e
R
ow
E
xp
er
im
en
t
Fe
at
ur
es
C
la
ss
ifi
er
pa
ge
s
G
en
re
co
lle
ct
io
ns
co
lle
ct
io
n
(%
)
1
Sa
nt
in
i[
26
]
11
8
(f
un
ct
io
n
w
or
ds
,
PO
Ss
,p
un
ct
ua
tio
n,
ge
nr
e-
sp
ec
ifi
c
w
or
ds
,
H
T
M
L
ta
gs
)
SV
M
1,
40
0
7-
w
eb
ge
nr
e
co
lle
ct
io
n
90
.6
2
Sa
nt
in
i[
26
]
14
0
PO
S
tr
ig
ra
m
s
SV
M
1,
40
0
7-
w
eb
ge
nr
e
co
lle
ct
io
n
89
.4
3
Sa
nt
in
i[
26
]
10
0
fa
ce
ts
(s
ee
Se
ct
io
n
5.
3.
2)
SV
M
1,
40
0
7-
w
eb
ge
nr
e
co
lle
ct
io
n
88
.8
4
W
al
tin
ge
r
an
d
M
eh
le
r
[4
3]
R
an
ke
d
pr
ofi
le
s
of
th
ei
r
n-
gr
am
fr
eq
ue
nc
ie
s
(#
of
fe
at
ur
es
N
/A
)
C
at
eg
or
y
pr
ofi
lin
g
(9
su
b
m
od
el
s
as
so
ci
at
ed
to
on
e
ca
te
g.
)
1,
40
0
7-
w
eb
ge
nr
e
co
lle
ct
io
n
93
5
M
as
on
et
al
.[
21
]
M
os
tf
re
qu
en
t1
,0
00
7-
g
pe
r
w
eb
pa
ge
G
en
re
co
m
pa
ri
so
n
m
et
ho
d
(b
as
ed
on
a
ce
nt
ro
id
fe
at
ur
e
se
tf
or
ea
ch
ge
nr
e)
1,
40
0
94
.6
6
K
an
ar
is
an
d
St
am
at
at
os
[1
6]
>
3,
00
0
ch
ar
ac
te
r
n-
gr
am
+
st
ru
ct
ur
al
in
fo
rm
at
io
n
SV
M
1,
40
0
7-
w
eb
ge
nr
e
co
lle
ct
io
n
96
.5
7
K
an
ar
is
an
d
St
am
at
at
os
[1
7]
7,
20
3
ch
ar
ac
te
r
n-
gr
am
an
d
st
ru
ct
ur
al
fe
at
ur
es
SV
M
1,
40
0
7-
w
eb
ge
nr
e
co
lle
ct
io
n
96
.6
8
C
ha
pt
er
6
by
K
im
an
d
R
os
s
(t
hi
s
vo
lu
m
e)
7,
43
1
ha
rm
on
ic
de
sc
ri
pt
or
s
SV
M
3,
45
2
7-
w
eb
ge
nr
e
co
lle
ct
io
n
+
24
K
R
Y
S-
01
ge
nr
es
(s
tr
uc
tu
re
d
no
is
e)
96
110 M. Santini
Ta
bl
e
5.
5
(c
on
tin
ue
d)
A
cc
ur
ac
y
on
th
e
#
of
w
eb
7-
w
eb
ge
nr
e
R
ow
E
xp
er
im
en
t
Fe
at
ur
es
C
la
ss
ifi
er
pa
ge
s
G
en
re
co
lle
ct
io
ns
co
lle
ct
io
n
(%
)
9
Sa
nt
in
ie
ta
l.
[3
3]
10
0
fa
ce
ts
In
fe
re
nt
ia
lm
od
el
2,
48
0
7-
w
eb
ge
nr
e
co
lle
ct
io
n
+
st
ru
ct
ur
ed
no
is
e
(B
B
C
ge
nr
es
)
an
d
un
st
ru
ct
ur
ed
no
is
e
(S
PI
R
IT
sa
m
pl
e)
86
10
Sa
nt
in
i[
29
]
10
0
fa
ce
ts
In
fe
re
nt
ia
lm
od
el
3,
68
5
7-
w
eb
ge
nr
e
co
lle
ct
io
n
+
st
ru
ct
ur
ed
no
is
e
(B
B
C
ge
nr
es
+
K
I-
04
)
an
d
un
st
ru
ct
ur
ed
no
is
e
(S
PI
R
IT
sa
m
pl
e)
81
11
Sa
nt
in
i(
th
is
ch
ap
te
r)
10
0
fa
ce
ts
In
fe
re
nt
ia
lm
od
el
6,
40
4
7-
w
eb
ge
nr
e
co
lle
ct
io
n
+
st
ru
ct
ur
ed
no
is
e
(B
B
C
ge
nr
es
+
K
I-
04
+
H
G
C
+
M
G
C
)
an
d
un
st
ru
ct
ur
ed
no
is
e
(S
PI
R
IT
sa
m
pl
e)
76
5 Cross-Testing a Genre Classification Model for the Web 111
95.2%. The best performance achieved by Kanaris and Stamatatos is 96.6% using
7,203 features (row 7). Waltinger and Mehler propose a graph-based method capa-
ble of classifying the 7-webgenre collection with competitive accuracy20 (row 4).
However, the experiments listed in row 1–7 are carried out on a very small collec-
tion (i.e. 1,400 web pages), balanced (200 web pages per genre), and without any
noise. It is very unlikely that the web can be represented in this way. A step forward
towards a more diversified representation of genres of web documents is done in
Chapter 6 by Kim and Ross (this book). They reach an accuracy of 96% on the
7-webgenre collection while classifying it together with other 24 genres from the
KRYS-01 pdf collection (the overall accuracy on 31 genres is about 70%). How-
ever, the high number of features (7,421) and the small size of the corpus (3,685
documents) are suspicious elements for assessing the model’s generality and corpus-
independence. Additionally, all the 3,685 web documents are supposed to fall into
well-defined genre classes. This means that only structured noise is represented in
this experiment.
The inferential model uses only 100 features and shows an accuracy of 86% when
the 7-webgenre collection is classified in a corpus of 2,480 web pages including
structured and unstructured noise. Its accuracy decreases to 81% when the corpus
is extended up to 3,685 web pages, and to 76% in a very noisy corpus of 6,404
web pages (see also Section 5.5.1). I propose these three accuracies as baselines
for future experiments with structured and unstructured noise and an increasing
corpus size.
5.5.3 Cross-Testing Performance on Single Labels: Mapped
Web Genres
In this subsection, the inferential model’s exportability is tried out by mapping some
of the web genres in the genre collections (i.e. KI-04, HGC and MGC) to my palette.
The model performance on these mapped genres is shown in the tables below.
The performance on KI-04 mapped genres (Table 5.6) is stable when the corpus
is increased, and in line with the accuracy achieved by Meyer zu Eissen and Stein
[22]. Differences in accuracy are not significant.
Although Stubbe et al.’s [37] results are not directly comparable, given the dif-
ferent evaluation measures employed by the authors, Table 5.7 tells us that the per-
formance on HGC mapped genres is not depreciable on 6404 web pages. The most
penalized genre is certainly FEATURES.
Conversely, the performance on MGC is much lower than the results reported
in Vidulin et al. [41]. Only PERSONAL BLOGS, which in MGC corresponds to web
pages that have been labelled both as PERSONAL and as BLOG (double label), per-
form satisfactorily on 6404 web pages (Table 5.8).
20 The same method can be used for language identification and subject-based text classification.
112 M. Santini
Table 5.6 Cross-testing on KI-04 mapped genres
Accuracies from Meyer Accuracies on
zu Eissen and Stein [22] Accuracies on SANTINIS+KI-
Three of KI-04’s on 800 web pages with SANTINI + KI-04 04+HGC+MGC
genres discriminant analysis (3,685 web pages) (6,404 web pages)
KI-04 linklists
(mapped to my
listings)
67.6% (out of 100 web
pages)
63.9% (out of 205
web pages)
62.4% (out of 205
web pages)
KI-04 portrayal-priv
(mapped to my
personal home
page)
67.7% (out of 100 web
pages)
83.3% (out of 26
web pages)
82.5% (out of 126
web pages)
KI-04 shops
(mapped to my
eshop)
66.9% (out of 100 web
pages)
71.8% (out of 167
web pages)
66.4% (out of 167
web pages)
Total 67.4% 73% 70.4%
Table 5.7 Cross-testing on HGC mapped genres
Stubbe et al. [37] on
1,280 web pages
Three of HGC Accuracies on SANTINIS+KI-
mapped genres Precision (%) Recall (%) 04+HGC+MGC (6,404 web pages)
Features (mapped to my
BBC feature articles)
53.8 35 12.5% (5 correct guesses out of 40
HGC features)
Blogs (mapped to my
personal blogs)
92.9 65 76.2% (32 correct guesses out of 42
HGC blogs)
FAQs (mapped to my
FAQs)
86.7 65 63.4% (26 correct guesses out of 41
HGC FAQs)
Total 77.8 55 50.7%
Table 5.8 Cross-testing on MGC mapped genres
Vidulin et al. [41] on 1539
web pages
Four MGC mapped Accuracies on SANTINIS+KI-04+
genres Accuracy (%) Precision Recall HGC+MGC (6,404 web pages)
Personal blogs (mapped
to my personal blogs)
(All kinds of
blogs) 96
71 56 79.4% (27 correct guesses out of 34
MGC personal blogs)
All kinds of shopping
(mapped to my
eshop)
97 89 38 37.9% (25 correct guesses out of 66
MGC shopping pages)
All kinds of faqs
(mapped to my FAQs)
99 94 77 47.1% (33 correct guesses out of 70
FAQs)
All kinds of index
pages (mapped to my
listings)
85 53 42 32.6% (74 correct guesses out of
227 index pages)
Total 94.25 76.75 71 49.25%
5 Cross-Testing a Genre Classification Model for the Web 113
5.5.4 Cross-Testing Performance on Single Labels: HCG and MCG
in Isolation
In this section, the exportability of the inferential model is tried out on HCG
and MCG in isolation. Performance on the mapped genres can be compared
when the model is applied to these collections in isolation and to the increased
corpus.
5.5.4.1 HGC
When the model is run on the HGC in isolation, the performance on the three
mapped genres is much higher than when it is run on 6,404 web pages (Table 5.9).
Interestingly, the performance on the FEATURE genre is perfect when the model is
applied to HGC alone.
Table 5.9 Accuracies on HGC
Accuracies on the mapped genres Accuracies on SANTINIS+
Three of HGC mapped of HGC in isolation (1,180 web KI-04+HGC+MGC (6,404 web
genres pages) pages)
Features (mapped to my
BBC feature articles)
100% (40 correct guesses out of
40 HGC features)
12.5% (5 correct guesses out of 40
HGC features)
Blogs (mapped to my
personal blogs)
73.8% (31 correct guesses out of
42 HGC blogs)
76.2% (32 correct guesses out of
42 HGC blogs)
FAQs (mapped to my
FAQs)
87.8% (36 correct guesses out of
41 HGC FAQs)
63.4% (26 correct guesses out of
41 HGC FAQs)
Total 87.2% 50.7%
5.5.4.2 MGC
The performance on MGC is quite idiosyncratic. When the model is run on MGC
in isolation, the performance on the four mapped genres is slightly lower than
when it is run on 6,404 web pages (differences on accuracy are significant).
Apparently, the model performs better when MGC is immersed in a larger corpus
(Table 5.10).
5.5.5 The SPIRIT Sample: An Attempt to Assess Multilabelling
5.5.5.1 Matching Accuracy (SANTINIS)
In this section I will describe the manual matching that I carried out in order to com-
pare and analyse the similarities and discrepancies between my genre annotation and
the model classification of the SPIRIT sample on SANTINIS (i.e. 2,480 web pages,
see Section 5.2.2). This comparison has been very time consuming and I performed
it in order gain a deeper insight into the model’s classification behaviour. For a first
114 M. Santini
Table 5.10 Accuracies on MGC
Accuracies on the mapped Accuracies on SANTINIS+
genres of MGC in isolation KI-04+HGC+ MGC
Four MGC mapped genres (1,539 web pages) (6,404 web pages)
Personal blogs (mapped to
my personal blogs)
79.4% (27 correct guesses
out of 34 MGC personal
blogs)
79.4% (27 correct guesses out
of 34 MGC personal blogs)
All kinds of shopping
(mapped to my eshop)
18.2% (12 correct guesses
out of 66 MGC shopping
pages)
37.9% (25 correct guesses out
of 66 MGC shopping pages)
All kinds of faqs (mapped
to my FAQs)
47.1% (33 correct guesses
out of 70 FAQs)
47.1% (33 correct guesses out
of 70 FAQs)
All kind of index pages
(mapped to my listings)
23.7% (54 correct guesses
out of 227 index pages)
32.6% (74 correct guesses out
of 227 index pages)
Total 42.1% 49.25%
assessment of this comparison, I used the approach similar to that utilized by Freund
et al. [12]. Although the principle is the same, my coding is slightly different and
follows the criteria listed in Table 5.11.
Figure 5.3 shows a excerpt of my manual annotation of the SPIRIT sample,
while Fig. 5.4 illustrates the output of the inferential model on the SPIRIT sample.
The first file (SPRT_002_060_117_0058000) was classified as a FRONTPAGE and
a LISTING genre by me. The model correctly guessed FRONTPAGE, but not LIST-
ING. According to the model, this page could also be a good candidate SHORT
BIOGRAPHY and EDITORIAL genres. In this case, only a FM (Fair Match) is scored
since there is only one match between my labels and the model’s labels.21 Matching
results are shown in Table 5.12.
Table 5.11 Matching criteria
PM = Perfect Match All my genre labels match all the predicted genre labels
EM = Excellent Match 4 of my genre labels match the predicted genre labels (e.g.
when the model assigns 5 or more genre labels to the
same web page)
VG = Very Good Match At least 3 of my genre labels match the predicted genre
labels
G = Good Match At least 1 of my genre labels match the predicted genre
labels
FM = Fair Match At least 1 of my genre labels match the predicted genre
labels
NM = No Match No matches between my annotation and the model
classification
NC = Not Classified Either the genres are not in the model’s palette, or i do not
know how to classify the page
21 The spreadsheet containing the matches is available at <http://sites.google.com/site/
marinasantiniacademicsite/>: see my_manual_genre_labelling_1000SPIRIT_webpages.xls.
5 Cross-Testing a Genre Classification Model for the Web 115
Fig. 5.3 SPIRIT manual annotation
Fig. 5.4 SPIRIT genre classification by the inferential model
116 M. Santini
Table 5.12 Results of the matching
PM = Perfect Match 4 0.4% (Including 2 zerolabelling)
EM = Excellent Match 3 0.3%
VG = Very Good Match 33 3.3%
G = Good Match 92 9.2%
FM = Fair Match 279 27.9%
NM = No Match 415 41.5% (Including 118 overlabelling)
NC = Not Classified 174 17.4%
Total number of web pages 1,000
By summing up PM, EM, VG, G, and FM, we get a percentage of 41.1%.
If we exclude from this preliminary assessment NC web pages (i.e. 17.4%), we
basically have a parity between misclassifications (NM) and matching accuracy
(PM+EM+VG+G+FM). These results are in line with Freund et al. [12], WEGA
[31], where similar assessment criteria were applied.
5.5.5.2 Overlap Coefficients
In this section, I list the overlap coefficients that result from the comparison between
my manual annotation and the actual results returned by the model on the SPIRIT
sample, immersed in three corpora of different size.
The overlap coefficients are functions that measure the agreement in the attribute
sets of two objects. There are many different overlap coefficients. They measure the
similarity between the items in two vectors. Here two common coefficients are used,
the Dice and the Jaccard coefficients. Both measures range from 0.0 (no overlap) to
1.0 (perfect overlap).
In Table 5.13, the overlap coefficients are measured on 1,000 web pages of the
SPIRIT sample including the 380 web pages labelled as IDK (i.e. I DO NOT KNOW),
NOTHING SUITABLE IN THIS PALETTE and NOTHING ELSE SUITABLE IN THIS
PALETTE.
In Table 5.14, the overlap coefficients are measured on 620 web pages of the
SPIRIT sample excluding the 380 labelled as IDK (i.e. I DO NOT KNOW), NOTHING
SUITABLE IN THIS PALETTE and NOTHING ELSE SUITABLE IN THIS PALETTE.
In both cases, the overlap coefficients are quite low. This can be explained by
the fact that my manual annotation is limited to a small number of labels (max
four), while the inferential model tends to overextends some genres, for instance
EDITORIAL.
Table 5.13 Overlap coefficients on 1,000 SPIRIT web pages
Web corpora Jaccard Dice
1,000 spirit within 2,400 web pages (SANTINIS) 0.12 0.18
1,000 spirit within 3,685 web pages
(SANTINIS+KI-04)
0.12 0.18
1,000 spirit within 6,400 web pages
(SANTINIS+KI-04+HGC+MGC)
0.10 0.15
5 Cross-Testing a Genre Classification Model for the Web 117
Table 5.14 Overlap coefficient 620 SPIRIT web pages
Web corpora Jaccard Dice
620 spirit web pages within 2,400 web pages (SANTINIS) 0.17 0.26
620 spirit within 3,685 web pages (SANTINIS+KI-04) 0.18 0.26
620 spirit within 6,400 web pages
(SANTINIS+KI-04+HGC+MGC)
0.15 0.22
5.6 Discussion
The inferential model appears to be robust when cross-tested on three noisy genre
collections of increasing size. Unfortunately, these findings are not directly compa-
rable with other results, because this experimental setting has never tried out earlier.
Hopefully, the results presented here can be used as baselines for future experiments.
Performance on the four BBC web genres (only 80 web pages all in all) are
stable (see Table 5.3). On the seven novel web genres, the initial performance
on SANTINIS (i.e. 86%), only decreases of 5% (i.e. 81%) when the initial cor-
pus is increased of 35% (i.e. SANTINIS+KI-04). Even more encouragingly, an
increase of 60% of the initial corpus (i.e. SANTINIS+KI-04+HGC+MGC) only
causes 10% decrease (i.e. 76%) with respect to the initial performance (i.e. 86%).
(see Table 5.4).
Table 5.5 appears to be very informative. It shows that accuracy increases with
the number of features, but then it is hard to assess if this high dimensionality
ensures also a certain degree of generability of the resulting genre models. More
experimentation with structured and unstructured noise is certainly wished for. For
the time being, it seems encouraging that a small amount of corpus-independent fea-
tures (i.e. the 100 facets) achieves an accuracy of 76% on the 7-webgenre collection
in a very noisy corpus of 6,404 web pages. It would be interesting to investigate in
future whether a slight increase in the number of facets (e.g. up to 200) would also
enhance accuracy.
The inferential model’s performance is also appreciable on the mapped genres of
KI-04 (see Table 5.6). Its performance equals that reported in Meyer zu Eissen and
Stein [22] and keeps steady also when the corpus is drastically increased in size.
Performance is very good when it is applied to HGC mapped genres in isolation
(1,180 web pages), while an unexpected drop of accuracy occurs on the FEATURE
genre, when HGC mapped genres are cross-tested on the enlarged corpus (6,404
web pages) (see Tables 5.7 and 5.9). Leaving the error analysis of the feature genre
drop to future work, it appears that the model can be safely and effectively exported.
This is indeed the main advantage of a corpus independent classification model.
However, the performance is not too brilliant on the MGC mapped genres (see
Tables 5.8 and 5.10). While the accuracy on MGC PERSONAL BLOGS is remark-
ably stable regardless the size of the underlying corpora (1,539 web pages vs. 6,404
web pages), the other MGC mapped genres show a more controversial composition.
For instance, the MGC SHOPPING class mapped to my model’s ESHOPS includes
online stores, classified ads, price comparators and pricelists. The model performs
118 M. Santini
disappointingly on this SHOPPING class because it was not designed to cover classi-
fied ads and price comparators. In short, the low performance on MGC SHOPPING,
FAQS and INDEX genres is due, I conjecture, to the composition of these classes.
This collection seems to be quite hard to handle (cf. also [17]) possibly because
of the characterization of genre classes underlying its construction. More in-depth
error analysis and additional comparative experiments will shed more light on this
behaviour.
Both the multi-labelling assessment described here and other experiments (cf.
[42]) show that multi-labelled genre classification has a long way to go. The per-
formance on multi-labelled classification is problematic for a number of reasons.
On the one hand, overlap coefficients (see Tables 5.13 and 5.14) show that that my
manual annotation needs to be discussed, agreed upon and validated. On the other
hand, the model classification skills need to be refined and optimised, e.g. with
the introduction of a threshold that skims off less probable genre labels. However,
matching accuracy (Table 5.12) conforms to the assessment of the WEGA genre
add-on [31]. These can be considered the current baselines for genre multilabel
classification.
5.7 Conclusion and Future Work
The genre model presented in this chapter has been conceived to be as corpus-
independent as possible. Hence, it is not derived from a single, supposedly rep-
resentative corpus. The model is grounded on the findings of previous linguistic
and textual genre analyses. It relies on NLP tools that extract linguistic knowledge
from texts. The design is based on the intuition that encoding genre knowledge in
the model rather than deriving through ML might ensure more resilience in noisy
environments, like the web. Being corpus independent, the model does not depend
on genre-annotated examples and can be applied in a situation where not all the
web pages are labelled by the genres included in the model palette. Corpus inde-
pendence is important when dealing with genre classes, because, as pointed out
earlier, annotating a web page by genre is one of the major burdens of AGI research.
Essentially, the inferential model tries to make the best of theoretical and empirical
findings documented by genre analysts by encoding them in hard-coded rules, rather
than relying on the learning from small genre collections assembled with subjective
criteria.
The 7-webgenre collection and the other collections included in the experiments
have been used to evaluate results and not for learning genre classes.
The model employs only 100 facets. It would be interesting to investigate in the
future if the increase of the number of facets would positively affect classification
results.
The inferential model is also topic-independent and relies on the correlation
between genres and the rhetorical patterns that express the communicative purposes
5 Cross-Testing a Genre Classification Model for the Web 119
in a text. It implements a zero-to-multi classification scheme where a web page can
be assigned to one, more, or none of the genres of the palette.
The genre palette is purely experimental. This palette is just one of the many
plausibly useful genre palette for genre-enabled applications.
The model has been cross-tested with four different genre corpora. Performance
has been monitored through scalability (i.e. size increase), exportability to genre
collections (i.e. HGC and MGC in isolation), and multi-labelling. However, multi-
labelling genre classification is such in a premature stage that any further experience
will be beneficial.
All in all, the model has been cross-tested with 6,404 genre-annotated web
pages. I conjecture that this final composite corpus of 6,404 web pages well rep-
resents a noisy environment like the web. In this difficult scenario, the model
shows some robustness and stability, but results need to be enhanced. The results
shown here must be taken as baseline that will hopefully be overperformed in future
experiments.
Results on MGC show that diverse definitions of the concept of genre have a
strong bearing on the characterization of genre classes. When the conception of
genre is not so distant, as in the case of HGC, results are more encouraging. This
means that the diverse definitions of the concept of genre might have a strong bear-
ing on the characterization of genre classes, thus affecting the generability of genre
models as a whole.
One urgent need is the creation of genre reference corpora for evaluation pur-
poses (a proposal can be found in Santini and Sharoff [32]). The construction of
these corpora would entail profitable discussion and the formation of more consent
around the concept of genre for AGI. A preliminary genre palette for reference cor-
pora has already been proposed in Rehm et al. [23]. This palette is a good starting
point for future debate.
More generally, a view of genre population on the web could be provided by
the application of webometric techniques. Chapter 12 by Lennart Björneborn (this
book) shows some interesting relations between the genres of academic websites.
Other approaches are also possible. For instance, a genre-oriented replication of the
experiences described in Thelwall [38–40] could undoubtedly provide new insights
and a better understanding of the dynamics underlying genre use on the web.
Semi-supervised ML techniques and the exploitation of the tagging habits
encouraged by social networks are certainly paths to be explored to assemble large
quantities of genre-annotated material.
Appendix
The appendix contains tables describing the genre corpora used in the experiments
explained in Chapter 5.
120 M. Santini
SANTINIS (2,480 Web Pages). Cf. Also Santini ([29], Appendix B)
Table 5.15 SANTINIS composition
no. of web
SANTINIS pages Description
Noise A.K.A. the
spirit
sample
1,000 A randomly selected sample of
english web pages from the SPIRIT
collection
Blogs These are personal blogs where the
author (a blogger) expresses
whatever s/he thinks, fears or
experiences using entries posted in
reverse chronological order
Eshops 200 Eshops are interactive documents,
with their own purpose (i.e. selling
products), rhetorical function
(persuade potential buyers),
textual conventions (i.e. use of
exhortations and a special
typographical organization), and
expectations (i.e. prices, pictures,
short descriptions, offers, etc.). An
eshop is often organized as a list of
products with prices
FAQs 200 Questions and answers can be
organized in different ways. For
example, a FAQs can be a single
document with a regular pattern of
question + answer sequence; or
each page can contain a single
question and an answer; or all the
questions are listed in one page
hyperlinked to other pages
(answers are provided in one
different page per question); and
similar
Front pages 200 A front page is the first page of a
newspaper. In the paper world it
was mainly a component of a
newspaper bearing the initial part
of the most important articles. On
the web, a front page has a more
autonomous status than in the
paper world
5 Cross-Testing a Genre Classification Model for the Web 121
Table 5.15 (continued)
no. of web
SANTINIS pages Description
Listings 200 Namely hotlists, sitemaps, tables
of contents, and checklists.
broadly speaking, a list is a
synthetic way of delivering
information or giving
instructions. the visual
organization into bullets or
numbers provides a visual
support that replaces other
linguistic devices, such as
connectives. Lists can be seen
as a textual solution where
juxtaposition prevails over
subordinating constructions
Personal home
pages
200 A personal home page is a page
published and maintained by an
individual. Personal home
pages have been defined as
“narrative of self-evaluation”
and analysed also in terms of
the construction of identity.
However, Döring [8] finds that
only 42% of the personal home
pages listed in the university
directories corresponded to
the image of the typical
self-presentation page
Search pages 200 Search pages belong to the fourth
generation sites, those that
focus on the architecture with
dynamic content
BBC editorials 20 Argumentative statements of
views that are considered to be
representative of a newspaper
as a whole
BBC DIY
mini-guides
20 Include some general information
about the project, duration time,
etc.; a list of tools needed to
operate; a short headline
introducing the topic of the
following paragraph; finally a
sequence of instructions
122 M. Santini
Table 5.15 (continued)
no. of web
SANTINIS pages Description
BBC short
biographies
20 Characterized by narration.
Several recurrent linguistic
features can be identified in a
biography, for instance the past
tense is used to report events
together with temporal markers
and location markers
BBC features 20 Articles about a specific subject or
theme
Total 2,480
KI-04 (1,205 Web Pages). Cf. Also Meyer zu Eissen and Stein [22]
Table 5.16 KI-04 composition
KI-04 no. of web pages Description
Articles 127 Documents with long passages of text,
such as research articles, reviews,
technical reports, or book chapters
Download pages 151 Pages on which freeware, shareware,
demo versions of programs etc. Can be
downloaded
Link collections 205 Documents which consist of link lists for
the main part
Portrayal (priv.) 126 Private self-portrayals, i.e. typical private
homepages with informal content
Discussions 127 All pages that provide forums, mailing
lists or discussion boards
Helps 139 All pages that provide assistance, e. g.
Q&A or FAQ pages
Portrayal (non-priv) 163 Web appearances of companies,
universities, and other public
institutions. That is, home or entry
or portal pages, descriptions
of organization and mission,
annual reports, brochures, contact
information, etc
Shops 167 All kinds of pages whose main purpose is
product information or sale
Total 1,205
5 Cross-Testing a Genre Classification Model for the Web 123
HGC (Used 1,180 for Crosstesting). Cf. Also Stubbe et al. [37]
Table 5.17 HGC composition
# of web
HGC pages
A Journalism 320
1 Commentary 40
2 Review 40
3 Portrait 40
4 Marginal note 40
5 Interview 40
6 News 40
7 Feature 40
8 Reportage 40
B Literature 120
9 Poem 40
10 Prose 40
11 Drama 40
C Information 360
12 Science 40
13 Explanation 40
14 Receipt 40
15 FAQ 40
16 Lexicon 40
17 Bilingual 40
18 Presentation 40
19 Statistics 40
20 Code 40
D Documentation 120
21 Law 40
22 Official 40
23 Protocol 40
E Dictionary 160
24 Person 40
25 Catalogue 40
26 Resources 40
27 Timeline 40
E Communication 160
28 Mail, talk 40
29 Forum, guestbook 40
30 Blog 40
31 Form 40
F Nothing 40
32 Nothing 40
Total 1, 280 1, 280
124 M. Santini
MGC (1,539 Web Pages). Cf. Also Vidulin et al. [41]
Table 5.18 MGC composition
Genre Assignments Description of the communicative
MGC (no. of web pages) purpose
Blog 77 Presents updates on what is going on
with an entity
Children’s 105 Presents content in a simple and colorful
way specifically suited for children
Commercial/
promotional
121 Web pages are intended to invoke the
visitor’s interest in goods or services,
typically for commercial gain
Community 82 Type web page involves the visitor in the
creation of the page and enables
interaction with other visitors
Content delivery 138 Delivers content that is not a part of the
page. Entertainment web pages
entertain the visitor
Entertainment 76 Presents jokes, puzzles, horoscopes,
games
Error message 79 Tells the visitor to go away
FAQ 70 Are intended to help a user to solve
common problems by answering
frequently asked questions
Gateway 77 Transfers the visitor to another page.
Index transfers the visitor to a selection
of multiple other pages
Index 227 Presents lots of links to other web pages
Informative 225 Conveys objective information of
permanent interest suitable for general
population
Journalistic 186 Conveys mostly objective information on
current events
Official 55 Conveys information with legal or
otherwise official consequences
Personal 113 Conveys subjective, personal information
in an informal way
Poetry 72 Presents poems and lyrics with intention
to evoke emotions
Pornographic/adult 68 Web pages have intention to sexually
arouse the visitor
Prose 67 Fiction presents story about real or
fictional event in artistic form with
intention to evoke imagination and
emotions
Scientific 76 Conveys objective information suitable
for experts
Shopping 66 Web pages sell goods or services online
User input 84 Solicits the visitor’s input
5 Cross-Testing a Genre Classification Model for the Web 125
100 Facets
Table 5.19 100 facets
1. Predicators 42. Discoursal connectives 76. That omission
2. Nominals 43. Temporal connectives 77. Comparative clause
3. First person 44. While 78. Relative clause
4. Second person 45. Whereas 79. Phenomenon registering
5. Third person 46. When 80. Con. action recording
6. Inanimate pronoun 47. Since 81. Action recording
7. Present tense 48. If 82. Phenomenon identifying
8. Past tense 49. As 83. Phenomenon linking
9. Imperative 50. To verb 84. Quality attributing
10. Active voice 51. Concession clause 85. Phenomenon identifying mod
11. Passive voice (initial) 86. Act. demanding com.
12. Time markers 52. Concession clause 87. Layout (HTML)
13. Location markers (final) 88. Typography (HTML)
14. Instrument 53. Concession clause 89. Functionality (HTML)
15. Manner (special) 90. Navigability [general] (HTML)
16. Negative particles 54. Contrast clause 91. Navigability [external] (HTML)
17. Probability verbs 55. Exception clause 92. Navigability [internal] (HTML)
18. Necessity verbs 56. Reason clause (initial) 93. Web page length [in words]
19. Existential there 57. Reason clause (final) 94. Blog words
20. Expressiveness 58. Space clause (initial) 95. Eshop words
21. Colon 59. Space clause (final) 96. FAQs words
22. Question mark 60. Time clause (initial) 97. Front page words
23. Quotes 61. Time clause (final) 98. Listing words
24. Activity verbs 62. Time clause 99. Personal home page words
25. Communication verbs (instructional) 100. Search page words
26. Mental verbs 63. Time clause (split)
27. Causative verbs 64. Conditional clause
28. Occurrence verbs (initial)
29. Existence verbs 65. Conditional clause
30. Aspectual verbs (final)
31. Enumerative connectives 66. Conditional clause
32. Equative connectives (special)
33. Reinforcing connectives 67. Result clause
34. Summative connectives 68. Similarity clause
35. Appositive connectives 69. Complex np
36. Resultative connectives 70. Verb+that clause
37. Inferential connectives 71. Adjective+that clause
38. Reformulatory connectives 72. Wh clause
39. Replacive connectives 73. Adjective+to clause
40. Antithetic connectives 74. Verb+ing clause
41. Concessive connectives 75. Purpose clause
126 M. Santini
References
1. Berninger V., Y. Kim, and R. Ross. 2008. Building a document genre corpus: A profile of the
KRYS I corpus. Corpus profiling for information retrieva and natural language processing.
Workshop Held in Conjunction with IIiX 2008, 18th Oct 2008. London.
2. Biber, D. 1988. Variations across speech and writing. Cambridge, UK: Cambridge University
Press.
3. Biber, D. and Kurjian, J. (2007). Towards a taxanomy of web registers and text types: a multi-
dimensional analysis. In Corpus linguistics and the web, eds., M. Hundt, N. Nesselhauf, and
C. Biewer, 109–131. Rodopi – Amsterdam – New York.
4. Blood, R. 2000. Weblogs: A history and perspective. Rebecca’s pocket. http://www.
rebeccablood.net/essays/weblog_history.html. Accessed 7 Sep 2000.
5. Bruce, I. 2008. Academic writing and genre. A systematic analysis. London-New York: Con-
tinuum International Publishing Group Ltd.
6. Dewdney, N., C. Vaness-Dikema, and R. Macmillan. 2001. The form is the substance: Clas-
sification of genres in text. In Proceedings of the 39th Annual Meeting of the Association for
Computational Linguistics and 10th Conference of the European Chapter of the Association
for Computational Linguistics. Toulouse.
7. Dewe, J., J. Karlgren, and I. Bretan. 1998. Assembling a balanced corpus from the internet. In
Proceedings of the 11th Nordic Conference of Computational Linguistics. Copenhagen.
8. Döring, N. 2002. Personal home pages on the web: A review of research. Journal of Computer-
Mediated Communication (JCMC) 7(3).
9. Duda, R., J. Gasching, and P. Hart. 1979. Model design in the prospector consultant system for
mineral exploration. In Expert systems in the micro-electronic age, ed. D. Michie, 153–167.
Edinburgh: Edinburgh University Press. Reprinted in 1984.
10. Duda, R., P. Hart, and N. Nilsson. 1981. Subjective methods for rule-based inference system.
In Readings in artificial intelligence, eds. B. Weber and N. Nilsson, 192–199. Palo Alto, CA:
Tioga Publishing Company.
11. Freund, L. 2008. Exploiting task-document relations in support of information retrieval
in the workplace. Doctoral dissertation, Faculty of Information Studies, University of
Toronto, Toronto. http://faculty.arts.ubc.ca/lfreund/Publications/Freund_Luanne_S_200811_
PhD_thesis.pdf
12. Freund, L., C.L.A. Clarke, and E.G. Toms. 2006. Genre classification for IR in the workplace.
In Proceedings of Information Interaction in Context (IIiX 2006) Copenhagen, Denmark.
13. Görlach, M. 2004. Text types and the history of English. Berlin-New York: Mouton de Gruyter.
14. Heyd, T. 2008. Email Hoaxes. Form, function, genre ecology. Amsterdam; Philadelphia, PA:
J. Benjamins Publishing Company.
15. Joho, H., and M. Sanderson. 2004. The SPIRIT collection: An overview of a large web col-
lection. SIGIR Forum, 38(2), December 2004.
16. Kanaris, I. and E. Stamatatos. 2007. Webpage genre identification using variable-length char-
acter n-grams. In Proceedings of the 19th IEEE Int. Conf. on Tools with Artificial Intelligence.
Washington, DC.
17. Kanaris, I., and E. Stamatatos. 2009. Learning to recognize webpage genres. Information Pro-
cessing and Management 45(5):499–512.
18. Karlgren, J., and D. Cutting. 1994. Recognizing text genre with simple metrics using dis-
criminant analysis. In Proceedings of the 15th International Conference on Computational
Linguistics (COLING 1994). Kyoto.
19. Lee, D. 2001. Genres, registers, text types, domains, and styles: Clarifying the concepts and
navigating a path through the BNC Jungle. Language Learning & Technology 5(3):37–72.
20. Levering, R., M. Cutler, and L. Yu. 2008. Using visual features for fine-grained genre classi-
fication of web pages. In Proceedings of the 41st Hawaii International Conference on System
Sciences. Big Island, Hawaii.
5 Cross-Testing a Genre Classification Model for the Web 127
21. Mason, J., M. Shepherd, and J. Duffy. 2009. An n-gram based approach to automatically iden-
tifying web page genre. In Proceedings of the 42nd Annual Hawaii International Conference
on System Sciences. Big Island, Hawaii.
22. Meyer zu Eissen, S., and B. Stein. 2004. Genre classification of web pages: User study and
feasibility analysis. In Advances in artificial intelligence, eds. S. Biundo, T. Frühwirth, and
G. Palm, 256–269. Berlin: Springer.
23. Rehm, G., M. Santini, M. Mehler, P. Braslavski, R. Gleim, A. Stubbe, S. Symonenko,
M. Tavosanis, and V. Vidulin. 2008. Towards a reference corpus of web genres for the evalu-
ation of genre identification systems. In Proceedings of LREC 2008, May 28–30. Marrakech,
Morocco.
24. Rosso, M. 2008. User-based identification of Web genres. Journal of the American Society for
Information Science and Technology 59(7):1053–1072.
25. Santini, M. 2005. Building on syntactic annotation: Labelling subordinate clauses. In Proceed-
ings of the Workshop on Exploring Syntactically Annotated Corpora (held in conjunction with
Corpus Linguistics 2005 Conference). Birmingham.
26. Santini, M. 2006. Common criteria for genre classification: Annotation and granularity. In
Proceedings of the Workshop on Text-based Information Retrieval (TIR-06) (held in conjunc-
tion with ECAI 2006). Riva del Garda.
27. Santini, M. 2007a. Automatic genre identification: Towards a flexible classification scheme.
BCS IRSG Symposium: Future Directions in Information Access 2007 (FDIA 2007a) (held
in conjunction with the European Summer School on IR (ESSIR 2007)), Tuesday, 28th and
Wednesday, 29th of Aug. Glasgow.
28. Santini, M. 2007b. Characterizing genres of web pages: Genre hybridism and individualiza-
tion. In Proceedings of the 40th Hawaii International Conference on System Sciences (HICSS-
40). Hawaii.
29. Santini, M. 2007c. Automatic identification of genre in web pages. PhD thesis, University of
Brighton, Brighton.
30. Santini, M. 2008. Zero, single, or multi? Genre of web pages through the users’ perspective.
Information Processing and Management 44(2):702–737.
31. Santini, M., and M. Rosso. 2008. Testing a genre-enabled application: A preliminary assess-
ment. In Proceedings of Future Direction in Information Access (FDIA-2008). BCS, London.
32. Santini, M., and S. Sharoff. 2009. Web genre benchmark under construction. Journal for Lan-
guage Technology and Computational Linguistics (JLCL) 24(1):129–145.
33. Santini, M., R. Power, and R. Evans. 2006. Implementing a characterization of genre for auto-
matic genre identification of web pages. In Proceedings of the 21st International Conference
on Computational Linguistics and 44th Annual Meeting of the Association for Computational
Linguistics (ACL/COLING 2006). Main Conference Poster Paper. Sydney.
34. Shepherd, M., C. Watters, and A. Kennedy. 2004. Cybergenre: Automatic identification of
home pages on the web. Journal of Web Engineering 3(3–4):236–251.
35. Stein, B., and S. Meyer zu Eissen. 2008. Retrieval Models for Genre Classification. Scandina-
vian Journal of Information Systems (SJIS) 20(1):91–117.
36. Stubbe, A., and C. Ringlstetter. 2007. Recognizing Genres. In Abstract Proceedings of the
Colloqium “Towards a Reference Corpus of Web Genres” (held in conjunction with Corpus
Linguistics 2007), 27 Jul 2007, eds. M. Santini and S. Sharoff. Birmingham.
37. Stubbe, A., C. Ringlstetter, and K. Schulz. 2007. Genre to classify noise – noise to classify
genre. In Proceedings of the IJCAI-2007 Workshop on Analytics for Noisy Unstructured Text
Data, 8 Jan 2007. Hyderabad, India. International Journal on Document Analysis and Recog-
nition (IJDAR), Dec 2007.
38. Thelwall, M. 2008a. Text in social network web sites: A word frequency analysis of Live
Spaces. First Monday 13(2).
39. Thelwall, M. 2008b. Quantitative comparisons of search engine results. Journal of the Ameri-
can Society for Information Science and Technology 59(11):1702–1710.
128 M. Santini
40. Thelwall, M. 2008c. Extracting accurate and complete results from search engines: Case study
Windows Live. Journal of the American Society for Information Science and Technology
59(1):38–50.
41. Vidulin, V., M. Luštrek, and M. Gams. 2007. Using genres to improve search engines. In
Proceedings of Towards Genre-enable Search Engines: The Impact of Natural Language Pro-
cessing Workshop, Sept 2007. Borovets, Bulgaria.
42. Vidulin, V., M. Luštrek, and M. Gams. 2009. Multi-label approaches to web genre identifica-
tion. Journal for Language Technology and Computational Linguistics (JLCL) 24(1):97–114.
43. Waltinger, U., and A. Mehler. 2009. The feature difference coefficient: Classification by means
of feature distributions. In Proceedings of the Conference on Text Mining Services (TMS 2009),
159–168. Leipzig, Germany.
44. Xu, J., Y. Cao, H. Li, N. Craswell, and Y. Huang. 2007. Searching documents based on rele-
vance and type. In Proceeding of ECIR 2007. Rome, Italy.
45. Yeung, P., S. Büttcher, C. Clarke, and M. Kolla. 2007a. A Bayesian approach for learning
document type relevance. ECIR 2007. Rome.
46. Yeung, P., C. Clarke, and S. Büttcher. 2007b. Improving retrieval accuracy by weighting doc-
ument types with clickthrough data. SIGIR’07. Amsterdam, The Netherlands.
47. Yeung, P., L. Freund, and C. Clarke. 2007c. X-Site: A workplace search tool for software engi-
neers. System demo presented at the 30th International ACM SIGIR Conference. Amsterdam.
