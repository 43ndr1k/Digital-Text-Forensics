                                        Computer Speech and Language 
                                  Manuscript Draft 
 
Manuscript Number: CSL15-21 
 
Title: Anomaly Detection Approach for Authorship Verification: a Multivariate Gaussian Model 
 
Article Type: SI: SLSP 2014 
 
Keywords: Authorship verification; anomaly detection; multivariate  Gaussian distribution 
 
Abstract: Authorship verification is the task of determining if a given text is written by a candidate 
author or not. In this paper, we present a first study on using an anomaly detection approach for the 
authorship verification task. We have considered two variations of this approach, a weakly 
supervised probabilistic method and an unsupervised 
distance-based method, both based on a multivariate Gaussian distribution. To evaluate the 
effectiveness of the proposed approach, we conducted experiments on a classic French corpus. Our 
preliminary results show that the weakly supervised probabilistic method can achieve a high 
verification performance that can reach an F1 score of 85% while the unsupervised distance-based 
method achieves an F1 score of 83%. Thus, these methods can be very valuable for authorship 
verification. 
 
 
 
 
to be delivered offline. 
 
*Covering Letter
We model the authorship verification task as an anomaly detection problem 
We propose two methods based on the same approach (weakly supervised /unsupervised) 
Results show a high verification performance especially for the weakly-supervised one 
The proposed methods outperform the classification-based baselines 
 
*Highlights (for review)
 1 
 2 
 3 
 4 
 5 
 6 
 7 
 8 
 9 
10 
11 
12 
13 
14 
15 
16 
17 
18 
19 
20 
21 
22 
23 
24 
25 
26 
27 
28 
29 
30 
31 
32 
33 
34 
35 
36 
37 
38 
39 
40 
41 
42 
43 
44 
45 
46 
47 
48 
49 
50 
51 
52 
53 
54 
55 
56 
57 
58 
59 
60 
61 
62 
63 
64 
65 
Anomaly Detection Approach for Authorship 
Verification: a Multivariate Gaussian Model 
 
Mohamed Amine Boukhaled, Jean-Gabriel Ganascia 
LIP6 (Laboratoire d’Informatique de Paris 6), Université Pierre et Marie Curie and 
CNRS (UMR7606), ACASA Team, 4, place Jussieu,  
75252-PARIS Cedex 05 (France), 
{mohamed.boukhaled, jean-gabriel.ganascia}@lip6.fr 
 
Abstract. Authorship verification is the task of determining if a given 
text is written by a candidate author or not. In this paper, we present a 
first study on using an anomaly detection approach for the authorship 
verification task. We have considered two variations of this approach, a 
weakly supervised probabilistic method and an unsupervised distance-
based method, both based on a multivariate Gaussian distribution. To 
evaluate the effectiveness of the proposed approach, we conducted exper-
iments on a classic French corpus. Our preliminary results show that the 
weakly supervised probabilistic method can achieve a high verification 
performance that can reach an F1 score of 85% while the unsupervised 
distance-based method achieves an F1 score of 83%. Thus, these methods 
can be very valuable for authorship verification. 
Keywords:  Authorship verification; anomaly detection; multivariate 
Gaussian distribution 
1 Introduction 
Authorship verification is a special case of the authorship attribution problem. 
The authorship attribution problem can be generally formulated as follows: 
given a set of candidate authors for whom samples of written text are availa-
ble, the task is to assign a text of unknown authorship to one of these candi-
date authors (Stamatatos 2009). This task has been addressed mainly as a 
problem of multi-class discrimination, or as a text categorization task 
(Sebastiani 2002). In the authorship verification problem, though, we are giv-
en samples of texts written by a single author and are asked to assess if a 
given different text is written by this author or not (Koppel et al. 2009). As a 
categorization problem, modifying the original attribution problem in this way 
*Manuscript
Click here to view linked References
 1 
 2 
 3 
 4 
 5 
 6 
 7 
 8 
 9 
10 
11 
12 
13 
14 
15 
16 
17 
18 
19 
20 
21 
22 
23 
24 
25 
26 
27 
28 
29 
30 
31 
32 
33 
34 
35 
36 
37 
38 
39 
40 
41 
42 
43 
44 
45 
46 
47 
48 
49 
50 
51 
52 
53 
54 
55 
56 
57 
58 
59 
60 
61 
62 
63 
64 
65 
makes the task of authorship verification significantly more difficult partly 
because building a characterising model of one author is much harder than 
building a distinguishing model between two authors (Koppel & Schler 2004). 
        
      Authorship verification has two key steps. First, an indexing step based 
on style markers is performed on the text using some natural language pro-
cessing techniques such as tagging, parsing, and morphological analysis. Then, 
an identification step is applied using the indexed markers to verify the validi-
ty of the authorship. Many style markers have been used to characterise writ-
ing styles, from early studies based on sentence length and vocabulary rich-
ness  (Yule 1944) to more recent and relevant works based on function words 
(Holmes et al. 2001 ,  Zhao & Zobel 2005), punctuation marks (Baayen et al. 
2002), part-of-speech (POS) tags (Kukushkina et al. 2001), parse trees 
(Gamon 2004) and  character-based features (Kešelj et al. 2003). There is an 
agreement among researchers that function words are the most reliable indica-
tor of authorship (Stamatatos 2009).  
     The verification step can be addressed as a one-class problem (written-by-
the-author) or as a binary classification problem (written-by-the-author as 
positive vs not-written-by-the-author as negative). However, both of these 
formulations of the problem have drawbacks. In the case of binary classifica-
tion, one should collect a reasonable amount of representative texts of the 
entire “not-written-by-the-author” class, which is difficult, if not impossible. In 
the case of one-class classification, one does not take advantage from negative 
examples that we do not actually lack for them even though they are not rep-
resentative of the entire class.  
Instead of that, in this paper we address the authorship verification problem 
as an anomaly detection problem where texts written by the candidate author 
are seen as normal data while texts not written by that author are seen 
anomalous data.  We propose an anomaly detection approach with two differ-
ent variations: the first variation is based on a weakly supervised probabilistic 
model and the second variation is based on an unsupervised distance-based 
model. However, both of them are based on a multivariate Gaussian distribu-
tion.  
     The rest of the paper is organised as follow. We first give an overview of 
the anomaly detection problem in section 2 and then describe our approach in 
section 3. We than experimentally validate the proposed method in section 4 
using a classic French corpus. Finally we use the best performing method, that 
is the weakly supervised probabilistic method, to settle a literary mystery 
case. 
 1 
 2 
 3 
 4 
 5 
 6 
 7 
 8 
 9 
10 
11 
12 
13 
14 
15 
16 
17 
18 
19 
20 
21 
22 
23 
24 
25 
26 
27 
28 
29 
30 
31 
32 
33 
34 
35 
36 
37 
38 
39 
40 
41 
42 
43 
44 
45 
46 
47 
48 
49 
50 
51 
52 
53 
54 
55 
56 
57 
58 
59 
60 
61 
62 
63 
64 
65 
2 Anomaly Detection 
Anomaly detection is a challenging task which consists of identifying patterns 
in data that do not conform to expected (normal) behaviour. These non-
conforming patterns are called anomalies or outliers (Chandola et al. 2009). 
Anomaly detection has been successfully used in many applications such as 
fault detection, radar target detection and hand written digit recognition 
(Markou & Singh 2003).   
This technique has also been used to deal with textual data for various pur-
poses such as detecting novel topics, events, or news stories in a collection of 
documents or news articles (Chandola et al. 2009). Anomaly detection is 
based on the idea that one can never train a classification algorithm on all the 
possible classes that the system is likely to encounter in real application. 
Anomaly detection is also suitable for situations in which the class imbalance 
problem can affect the accuracy of classification (see Fig. 1) (Wressnegger et 
al. 2013).  
 
 
 
Fig. 1. The anomaly detection and the classification learning schemas 
Many anomaly detection techniques fall under the statistical approach of 
modelling data based on its statistical properties and using this information to 
estimate whether a test sample comes from the same distribution or not 
(Markou & Singh 2003). Another common method for anomaly detection is 
the one-class SVM that determines a hyper sphere enclosing the normal data 
 1 
 2 
 3 
 4 
 5 
 6 
 7 
 8 
 9 
10 
11 
12 
13 
14 
15 
16 
17 
18 
19 
20 
21 
22 
23 
24 
25 
26 
27 
28 
29 
30 
31 
32 
33 
34 
35 
36 
37 
38 
39 
40 
41 
42 
43 
44 
45 
46 
47 
48 
49 
50 
51 
52 
53 
54 
55 
56 
57 
58 
59 
60 
61 
62 
63 
64 
65 
(Heller et al. 2003). In this contribution, we describe and use two anomaly 
detection methods for authorship verification that straightforwardly follow the 
definition given above. These two methods are discussed in the next section.  
3 Proposed approach 
In our approach, we address the authorship verification task as an anomaly 
detection problem where texts written by a given author  are seen as normal 
data, while texts not written by that author  are seen anomalous data.  Sec-
tion 3.1 describes the first variation based on an unsupervised distance-based 
model. Then, the second variation based on a weakly supervised probabilistic 
model is presented in Section 3.2. The choice and the nature of the style 
markers used to describe the texts are presented in the section 3.3. 
3.1 Unsupervised distance -based method  
The approach to anomalous text detection is to train a -dimensional multi-
variate Gaussian model on the style markers extracted from sample of text 
written by an author . Every newly arriving text (data instance) that we 
went to verify as written by  or not is contrasted with the model of normali-
ty, and a distance is computed. In fact, for -dimensional multivariate nor-
mally distributed data; the values are approximately chi-square distributed 
with  degrees of freedom. In this case, the multivariate outliers can simply be 
defined as observations having a large squared Mahalanobis distance 
(Filzmoser 2004). Thus, the computed distance describes the likelihood of the 
new text to have been written by  compared to the average data instances 
seen during the training. If the distance surpasses a predefined threshold ∝, 
the instance is considered an anomaly and the text is considered not to have 
been written by the author .  
As a threshold, the quantile of the chi-square distributed (eg., 97,5% quantile 
) can be considered. Such method has been already successfully used 
(Rousseeuw & Van Zomeren 1990).  
The method can be formulated into three steps as follow: Let   be a -
dimensional vector representing the text		( = 1,… ,). (A vector of   style 
markers’ frequency)  
1. Train a Multivariate Gaussian distribution model () on the normal data. 
This is done by estimating the two distribution parameters:  the multivariate 
location  μ  and the covariance matrix		 : 
 1 
 2 
 3 
 4 
 5 
 6 
 7 
 8 
 9 
10 
11 
12 
13 
14 
15 
16 
17 
18 
19 
20 
21 
22 
23 
24 
25 
26 
27 
28 
29 
30 
31 
32 
33 
34 
35 
36 
37 
38 
39 
40 
41 
42 
43 
44 
45 
46 
47 
48 
49 
50 
51 
52 
53 
54 
55 
56 
57 
58 
59 
60 
61 
62 
63 
64 
65 
μ = 	  	∑ ()  (1) 
 = 	  	∑ () − μ() − μ
	   (2) 
2. Given a new instance	x, compute the Mahalanobis distance 	D(x) :  
D(x) = (x − μ)Σ (x − μ)		       (3) 
3. Predict the anomaly ( y = 1 ) of the instance x  given the distance 
old		∝ : 
y = "0				if				D(x) <	∝	1				if				D(x) ≥	∝                (4) 
For the experimentation, two different thresholds have been considered: ∝ for 
the 95% Chi-squared quantile and ∝( for the 97,5% Chi-squared quantile.  
3.2 Weakly supervised probabilistic model 
Unlike the first method, in this second variation we use a probabilistic anoma-
ly detection method that can benefit from anomalous examples for the author-
ship verification process which is also based on a multivariate Gaussian mod-
elling. Given the fact that unsupervised anomaly detection approaches have 
difficulties to match the required detection rates in many tasks and there ex-
ists a need for labelled data to guide the model generation (Görnitz et al. 
2014), this method is weakly supervised in the sense that it takes into consid-
eration a small amount of representative anomalous data for the model gener-
ation. 
The approach to anomalous text detection is the same as the previous method 
for the first step. That is, on have to train a multivariate Gaussian distribu-
tion model on the style markers extracted from sample of text written by an 
author	. Then, every newly arriving text (data instance) that we went to 
verify as written by  or not is contrasted with the probabilistic model of 
normality, and in this case, a probability of normality is computed instead of 
a distance. The probability describes the likelihood of the new text to have 
been written by  compared to the average data instances seen during the 
training. If the probability does not surpass a predefined threshold		), the in-
stance is considered an anomaly and the text is considered not to have been 
written by the author	. To define the probability threshold, we cross-validate 
over a data set containing both anomalous and non-anomalous data and we 
set the threshold to the value that maximizes the authorship verification per-
 1 
 2 
 3 
 4 
 5 
 6 
 7 
 8 
 9 
10 
11 
12 
13 
14 
15 
16 
17 
18 
19 
20 
21 
22 
23 
24 
25 
26 
27 
28 
29 
30 
31 
32 
33 
34 
35 
36 
37 
38 
39 
40 
41 
42 
43 
44 
45 
46 
47 
48 
49 
50 
51 
52 
53 
54 
55 
56 
57 
58 
59 
60 
61 
62 
63 
64 
65 
formance on this cross-validation data set. This threshold is used afterward on 
the test set.  
As before, the method can be formulated into three steps as follow: Let   be 
a -dimensional vector representing the text		( = 1,… , :  
1. Train a Multivariate Gaussian distribution model  on the normal data. 
This is done by estimating the two distribution parameters:  the multivariate 
location  μ  and the covariance matrix		 : 
μ 	 	  	∑   (5) 
 	 	  	∑   μ  μ
	   (6) 
2. Given a new instance	, compute the probability	* :  
* 	 
(+
,
- |/|
0
-	
	exp	 (   μ   μ     (7) 
3. Predict the anomaly (3 	 1) of the instance  given the probability thresh-
old		) : 
3 	 "1	4	* & 	)0	4	* ' 	)   (8) 
3.3 Style markers 
The nature of the style markers used as attributes to describe and to get an n-
dimensional vector representing the text is very important and determines the 
applicability of our method. In fact, the nature of these attributes should re-
spect the Gaussian assumption made to train the multivariate Gaussian mod-
el. 
 
 
 
 
 
 
Fig. 2. The probability of frequency of the French function word "de" has a Gaussian 
behavior 
 1 
 2 
 3 
 4 
 5 
 6 
 7 
 8 
 9 
10 
11 
12 
13 
14 
15 
16 
17 
18 
19 
20 
21 
22 
23 
24 
25 
26 
27 
28 
29 
30 
31 
32 
33 
34 
35 
36 
37 
38 
39 
40 
41 
42 
43 
44 
45 
46 
47 
48 
49 
50 
51 
52 
53 
54 
55 
56 
57 
58 
59 
60 
61 
62 
63 
64 
65 
Table 1. List of the French function words used in our experiment (thirty most fre-
quent ones in the training set) 
 
 
 
 
 
 
 
For our experiment, we chose to test this method on two types of style mark-
ers separately. Each text in our data set is mapped onto a vector of the fre-
quency of the most frequent function words (see Table 1.) and a vector of the 
frequency of POS-tags (see Table 2.).  
There are two main reasons for using the frequency of function words as at-
tributes. First, because of their high frequency in a written text, function 
words are very likely to have a Gaussian behaviour (see Figure 2). Secondary, 
function words, unlike content words, are difficult to consciously control, thus 
they are more independent from the topic or the genre of the text (Chung & 
Pennebaker 2007). In fact, Koppel and Schler (2004) found that all the work 
of distinguishing the styles of different authors is accomplished with a small 
set of features containing frequent function words. Based on that information 
and to get a right balance between the features-set size and the dataset size, 
we limit our study to the most 30th frequent function words. The part-of-
speech-based markers are also shown to be very effective because they partly 
share the advantages of function words. 
4 Experimental Validation 
4.1 Data Set 
To test the effectiveness of our method, we used novels written by: Balzac, 
Dumas and France. This choice was motivated by our special interest in stud-
1. le 
2. la 
3. l' 
4. un 
5. une 
6. sa 
7. s' 
8. son 
9. ce 
10. les 
11. des  
12. du 
13. d' 
14. je 
15. au 
16. de 
17. et 
18. à 
19. il 
20. que 
 
21. en 
22. qui 
23. elle 
24. dans 
25. qu' 
26. pour 
27. vous 
28. plus  
29. sur 
30. on  
 
 1 
 2 
 3 
 4 
 5 
 6 
 7 
 8 
 9 
10 
11 
12 
13 
14 
15 
16 
17 
18 
19 
20 
21 
22 
23 
24 
25 
26 
27 
28 
29 
30 
31 
32 
33 
34 
35 
36 
37 
38 
39 
40 
41 
42 
43 
44 
45 
46 
47 
48 
49 
50 
51 
52 
53 
54 
55 
56 
57 
58 
59 
60 
61 
62 
63 
64 
65 
ying the classic French literature of the 19th century, and by the availability 
of electronic texts from these authors on the project Gutenberg website1 and 
in the Gallica electronic library2. Our choice of authors was also affected by 
the fact that we wished to get a challenging problem since these three authors 
are knows to have relatively comparable syntactic styles. More information 
about the data set used for the experimentation is summarized in Table 3. 
Table 2. List and description of the part-of-speech tags used in our experiment 
 
 
For each of the three authors mentioned above, we collected 4 novels, so that 
the total number of novels was 12. The next step was to divide these novels 
into smaller pieces of texts in order to have enough data instances (artificial 
documents) to train and test the probabilistic model.  
                                                          
1 http://www.gutenberg.org/ 
2 http://gallica.bnf.fr/ 
POS tag Description 
DET 
NC 
VINF 
VPR 
VPP 
V 
P 
PONCT 
CS 
NPP 
ADJ 
CC 
PROREL 
CLS 
CLR 
CLO 
PRO 
ADV 
P+D 
ET 
 
Determiner 
Common noun 
Infinitive verb form 
Present participle verb form 
Past participle 
Other verb forms (imperative, subjunctive,…) 
Preposition 
Punctuation mark 
Subordination conjunction 
Proper noun 
Adjective 
Coordination conjunction 
Relative pronoun 
Subject clitic pronoun 
Reflexive clitic pronoun 
Object clitic pronoun 
Full pronoun 
Adverb 
Preposition+determiner amalgam 
Foreign word 
 
 1 
 2 
 3 
 4 
 5 
 6 
 7 
 8 
 9 
10 
11 
12 
13 
14 
15 
16 
17 
18 
19 
20 
21 
22 
23 
24 
25 
26 
27 
28 
29 
30 
31 
32 
33 
34 
35 
36 
37 
38 
39 
40 
41 
42 
43 
44 
45 
46 
47 
48 
49 
50 
51 
52 
53 
54 
55 
56 
57 
58 
59 
60 
61 
62 
63 
64 
65 
Table 3. Data set used in our experiment 
 
 
 
 
Researchers working on authorship attribution in literary texts have used dif-
ferent dividing strategies. For example, Hoover (2003)  decided to take just 
the first 10,000 words of each novel as a single text, while Argamon and 
Levitan (2005) treated each chapter of each book as a separate text. In our 
experiment, we chose simply to chunk each novel into approximately equal 
parts of 2000 words, which is below the threshold proposed by Eder (2013)  
specifying the smallest reasonable text size to achieve good attribution. This 
increases the degree of the difficulty of the task. 
4.2 Verification Protocol  
In our experiment of the unsupervised distance-based method, function words 
were first extracted. Each text is then represented by a vector  567 	
{9, 9(, … , 967} of normalized frequencies of occurrence of the top 30 function 
words in the corpus. Then, for each author, we used 75% of the data generat-
ed by texts written by him to estimate the parameters of the representing 
multivariate Gaussian model, and 25% of the data from each author as testing 
set.    
In our experiment of weakly supervised method, we consider 2 types of style 
markers. The corpus was POS tagged and function words were extracted. 
Each text is then represented by two vectors 		5; = {9, 9(, … , 9;}, one for the 
normalized frequencies of occurrence of the top 30 function words in the cor-
pus,  and  another for the normalized frequencies of occurrence of POS-tags. 
The normalization of the vectors of frequency representing a given text was 
done according to the size of the text. Then, for each author, we used 75% of 
the data generated by texts written by this author to estimate the parameters 
of the model representing this author, and 20% of the data from each author 
for testing it. The remaining 5% data was merged with 5% of the data (anom-
alous data) generated by each one of the other authors and was used as a 
cross-validation set to estimate the probability threshold		).    
To get a reasonable estimate of the expected generalization performance, we 
used resampling with replacement for the two methods. The training and test-
Author Name # of texts 
Balzac, Honoré de 
Dumas, Alexandre 
France, Anatole 
126 
190 
128 
 
 1 
 2 
 3 
 4 
 5 
 6 
 7 
 8 
 9 
10 
11 
12 
13 
14 
15 
16 
17 
18 
19 
20 
21 
22 
23 
24 
25 
26 
27 
28 
29 
30 
31 
32 
33 
34 
35 
36 
37 
38 
39 
40 
41 
42 
43 
44 
45 
46 
47 
48 
49 
50 
51 
52 
53 
54 
55 
56 
57 
58 
59 
60 
61 
62 
63 
64 
65 
ing process was done 10 times. The overall authorship verification perfor-
mance is taken as the average performance over these 10 runs. For evaluating 
the verification performance, we used the standard measures, calculating pre-
cision (<), recall (5), and = where (>< for true positive, =< for false positive, 
=? for false negative):  
@ 	 A@A@BC@                   (9) 
D 	 A@A@BCE                       (10) 
CF 	 GD@DB@                     (11) 
4.3 Baselines 
To evaluate the effectiveness of the proposed methods we used one-class SVM 
as baseline for the unsupervised method and binary SVM classifier as baseline 
for the weakly supervised method. The one-class SVM was trained and tested 
on the same data used to train and test the multivariate Gaussian model re-
spectively. The binary SVM classifier was trained on both the data used to 
train the weakly supervised probabilistic model and the data used to estimate 
the probability threshold, and it was tested on the same data as the probabil-
istic model. The overall baselines classification performances are taken as the 
average performance over the 10 runs as well. 
4.4 Results  
The results of measuring the verification performance for the two different 
methods in our experimental validation are summarized in what follows. The-
se results show in general the superiority of the proposed methods over the 
baselines in terms of	= on the one hand, and the superiority of the weakly 
supervised method over the unsupervised method on the other hand. These 
results also show in general a better performance when using frequent function 
words than POS-tag for both the proposed method and the baselines. 
     The preliminary results of measuring the verification performance in our 
experimental validation for the unsupervised distance-based method against 
the ons-calss SVM are summarized in Table 4. One can notice the clear supe-
riority of the proposed method over the baseline. Our study here indicates 
that the proposed unsupervised verification method combined with features 
 1 
 2 
 3 
 4 
 5 
 6 
 7 
 8 
 9 
10 
11 
12 
13 
14 
15 
16 
17 
18 
19 
20 
21 
22 
23 
24 
25 
26 
27 
28 
29 
30 
31 
32 
33 
34 
35 
36 
37 
38 
39 
40 
41 
42 
43 
44 
45 
46 
47 
48 
49 
50 
51 
52 
53 
54 
55 
56 
57 
58 
59 
60 
61 
62 
63 
64 
65 
based on frequent function words can achieve a high verification performance 
(e.g., =  = 0.83). As one can expect, increasing the authorship distance 
threshold ∝(	>	∝) will result in higher recall and lower precision but without 
significant effect on the = score. By contrast, the one-class SVM performs 
particularly poorly on this task 
Table 4. Comparaison of average results of the unsupervised authorship verificatifor 
for the three authors using one-class SVMs and the proposed Unsupervised Anomaly 
Detection (UAD) method 
 
 
 
The results of measuring the verification performance for the two different 
style markers for the weakly supervised against the binary SVM are summa-
rized in Table 5 for function words and in Table 6 for POS tags. 
Our study here indicates that the weakly supervised anomaly detection meth-
od combined with features based on frequent function words can achieve a 
high verification performance (e.g., F1 = 0.85). The binary SVM achieved 
relatively good results but doesn’t outperform the probabilistic model; this 
shows that the authorship verification problem should not be handled as a 
binary class problem unless a sufficient amount of representative negative 
data is present to avoid the class imbalance problem. The function words are 
shown in these results to be more relevant for characterizing the authorial 
style (F1 = 0.85 for function words vs F1 = 0.77 for POS tags).  
Table 5. Results of the weakly supervised authorship verification using frequent func-
tion words  
 
Table 6. Results of the weakly supervised authorship verification using frequent POS-
tags  
 
Method P R F1 
One-class SVMs 0,34 0,50 0,40 
UAD ( ∝	) 0,82 0,85 0,83 
UAD ( ∝(	) 0,79 0,89 0,83 
Method P R F1 
Binary SVMs 0,86 0,75 0,80 
Multivariate Gaussian Model 0,82 0,88 0,85 
Method P R F1 
Binary SVMs 0,81 0,58 0,67 
Multivariate Gaussian Model 0,69 0,89 0,77 
 1 
 2 
 3 
 4 
 5 
 6 
 7 
 8 
 9 
10 
11 
12 
13 
14 
15 
16 
17 
18 
19 
20 
21 
22 
23 
24 
25 
26 
27 
28 
29 
30 
31 
32 
33 
34 
35 
36 
37 
38 
39 
40 
41 
42 
43 
44 
45 
46 
47 
48 
49 
50 
51 
52 
53 
54 
55 
56 
57 
58 
59 
60 
61 
62 
63 
64 
65 
 
Finally, these results are in line with previous work that claimed that anomaly 
detection approaches, originating from a supervised classifier (such as one-
class SVM), are inappropriate and hardly detect new and unknown anomalies, 
and that anomaly detection techniques needs to be grounded in the unsuper-
vised learning paradigm (Görnitz et al. 2014). The results suggest also that 
supervising the anomaly detection process with even a small amount of anom-
alous data can increase the verification performance.  
5 A Classic French Literary Mystery:  “Le Roman de 
Violette” 
In this section, we apply our probabilistic method to settle one of the classic 
French literary mysteries. “Le Roman de Violette” 3 is a novel published in 
1883. The authorship of this novel has still not been determined. Even though 
the novel was edited under the name of Alexandre Dumas, some literary crit-
ics state that a serious candidate for its authorship is “La Marquise de Man-
noury d’Ectot”. But this hypothesis cannot be definitely proved, partly be-
cause there is only one known book written by that author, which limits the 
quantity of text available to validate the computational authorship identifica-
tion methods including our method.     
     We applied the best performing proposed authorship verification method, 
which is the weakly supervised one to handle this case. Since there is not 
enough available text written by “La Marquise de Mannoury d’Ectot” to verify 
whether she is the writer of “Le Roman de Violette” or not, we set Alexandre 
Dumas as the author candidate that we want to verify as the writer or not. 
We trained the probabilistic model based on frequent function words on texts 
written by Alexandre Dumas. The only known book written by “La Marquise 
de Mannoury d’Ectot” was used as the representative anomalous text to set 
the probability threshold. Finally, the verification test was performed on the 
“Roman de Violette”. The authorship probability produced by the novel using 
our proposed method is under the threshold needed to validate the authorship. 
This result suggests that the novel “Le Roman de Violette” was indeed not 
written by Alexandre Dumas. 
                                                          
3 http://ero.corneille-moliere.com/?p=page52&m=ero&l=fra 
 1 
 2 
 3 
 4 
 5 
 6 
 7 
 8 
 9 
10 
11 
12 
13 
14 
15 
16 
17 
18 
19 
20 
21 
22 
23 
24 
25 
26 
27 
28 
29 
30 
31 
32 
33 
34 
35 
36 
37 
38 
39 
40 
41 
42 
43 
44 
45 
46 
47 
48 
49 
50 
51 
52 
53 
54 
55 
56 
57 
58 
59 
60 
61 
62 
63 
64 
65 
6 Conclusion 
In this paper, we have presented a study on using an anomaly detection ap-
proach for the authorship verification task. We have considered two variations 
of this approach, a weakly supervised probabilistic method and an unsuper-
vised distance-based method, both based on a multivariate Gaussian distribu-
tion. To evaluate the effectiveness of the proposed method, we conducted ex-
periments on a classic French literary corpus. Our preliminary results show 
that the probabilistic method can achieve a high verification performance that 
can reach an F1 score of 85%.  
     Based on the current study, we have identified several future research 
directions. First, we will explore incorporating the non-verification option into 
our probabilistic model. In fact, in the field of authorship identification, the 
non-attribution option is better than a false attribution. Second, this study 
will be expanded to include more style markers. Third, we intend to experi-
ment with other languages and text sizes using standard corpora employed in 
the field at large. 
References 
Argamon, S. & Levitan, S., 2005. Measuring the usefulness of function words for 
authorship attribution. In Proceedings of the Joint Conference of the 
Association for Computers and the Humanities and the Association for Literary 
and Linguistic Computing. 
Baayen, H. et al., 2002. An experiment in authorship attribution. In 6th JADT. pp. 
29–37. 
Chandola, V., Banerjee, A. & Kumar, V., 2009. Anomaly detection: A survey. ACM 
Computing Surveys (CSUR), 41(3), p.15. 
Chung, C. & Pennebaker, J.W., 2007. The psychological functions of function words. 
Social communication, pp.343–359. 
Eder, M., 2013. Does size matter? Authorship attribution, small samples, big problem. 
Literary and Linguistic Computing, p.fqt066. 
Filzmoser, P., 2004. A multivariate outlier detection method. In Proceedings of the 
Seventh International Conference on Computer Data Analysis and Modeling. pp. 
18–22. 
 1 
 2 
 3 
 4 
 5 
 6 
 7 
 8 
 9 
10 
11 
12 
13 
14 
15 
16 
17 
18 
19 
20 
21 
22 
23 
24 
25 
26 
27 
28 
29 
30 
31 
32 
33 
34 
35 
36 
37 
38 
39 
40 
41 
42 
43 
44 
45 
46 
47 
48 
49 
50 
51 
52 
53 
54 
55 
56 
57 
58 
59 
60 
61 
62 
63 
64 
65 
Gamon, M., 2004. Linguistic correlates of style: authorship classification with deep 
linguistic analysis features. In Proceedings of the 20th international conference 
on Computational Linguistics. p. 611. 
Görnitz, N. et al., 2014. Toward supervised anomaly detection. arXiv preprint 
arXiv:1401.6424. 
Heller, K. et al., 2003. One class support vector machines for detecting anomalous 
windows registry accesses. In Workshop on Data Mining for Computer Security 
(DMSEC), Melbourne, FL, November 19, 2003. pp. 2–9. 
Holmes, D.I., Robertson, M. & Paez, R., 2001. Stephen Crane and the New-York 
Tribune: A case study in traditional and non-traditional authorship attribution. 
Computers and the Humanities, 35(3), pp.315–331. 
Hoover, D.L., 2003. Frequent collocations and authorial style. Literary and Linguistic 
Computing, 18(3), pp.261–286. 
Kešelj, V. et al., 2003. N-gram-based author profiles for authorship attribution. In 
Proceedings of the conference pacific association for computational linguistics, 
PACLING. pp. 255–264. 
Koppel, M. & Schler, J., 2004. Authorship verification as a one-class classification 
problem. In Proceedings of the twenty-first international conference on Machine 
learning. p. 62. 
Koppel, M., Schler, J. & Argamon, S., 2009. Computational methods in authorship 
attribution. Journal of the American Society for information Science and 
Technology, 60(1), pp.9–26. 
Kukushkina, O. V, Polikarpov, A.A. & Khmelev, D.V., 2001. Using literal and 
grammatical statistics for authorship attribution. Problems of Information 
Transmission, 37(2), pp.172–184. 
Markou, M. & Singh, S., 2003. Novelty detection: a review—part 1: statistical 
approaches. Signal processing, 83(12), pp.2481–2497. 
Rousseeuw, P.J. & Van Zomeren, B.C., 1990. Unmasking multivariate outliers and 
leverage points. Journal of the American Statistical Association, 85(411), 
pp.633–639. 
Sebastiani, F., 2002. Machine learning in automated text categorization. ACM 
computing surveys (CSUR), 34(1), pp.1–47. 
Stamatatos, E., 2009. A survey of modern authorship attribution methods. Journal of 
the American Society for information Science and Technology, 60(3), pp.538–
556. 
 1 
 2 
 3 
 4 
 5 
 6 
 7 
 8 
 9 
10 
11 
12 
13 
14 
15 
16 
17 
18 
19 
20 
21 
22 
23 
24 
25 
26 
27 
28 
29 
30 
31 
32 
33 
34 
35 
36 
37 
38 
39 
40 
41 
42 
43 
44 
45 
46 
47 
48 
49 
50 
51 
52 
53 
54 
55 
56 
57 
58 
59 
60 
61 
62 
63 
64 
65 
Wressnegger, C. et al., 2013. A close look on n-grams in intrusion detection: anomaly 
detection vs. classification. In Proceedings of the 2013 ACM workshop on 
Artificial intelligence and security. pp. 67–76. 
Yule, G.U., 1944. The statistical study of literary vocabulary, CUP Archive. 
Zhao, Y. & Zobel, J., 2005. Effective and scalable authorship attribution using function 
words. In Information Retrieval Technology. Springer, pp. 174–189. 
 
