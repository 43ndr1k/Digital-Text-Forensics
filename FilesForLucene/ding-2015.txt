0
A Visualizable Evidence-driven Approach for Authorship Attribution
STEVEN H. H. DING, Concordia Institute for Information Systems Engineering, Concordia University
BENJAMIN C. M. FUNG, School of Information Studies, McGill University
MOURAD DEBBABI, Concordia Institute for Information Systems Engineering, Concordia University
The Internet provides an ideal anonymous channel for concealing computer-mediated malicious activities,
as the network-based origins of critical electronic textual evidence (e.g., emails, blogs, forum posts, chat
log etc.) can be easily repudiated. Authorship attribution is the study of identifying the actual author of
the given anonymous documents based on the text itself, and, for decades, many linguistic stylometry and
computational techniques have been extensively studied for this purpose. However, most of the previous
research emphasizes promoting the authorship attribution accuracy and few works have been done for the
purpose of constructing and visualizing the evidential traits; also, these sophisticated techniques are diffi-
cult for cyber investigators or linguistic experts to interpret. In this paper, based on the EEDI (End-to-End
Digital Investigation) Framework we propose a visualizable evidence-driven approach, namely VEA, which
aims at facilitating the work of cyber investigation. Our comprehensive controlled experiment and strati-
fied experiment on the real-life Enron email data set both demonstrate that our approach can achieve even
higher accuracy than traditional methods; meanwhile, its output can be easily visualized and interpreted
as evidential traits. In addition to identifying the most plausible author of a given text, our approach also
estimates the confidence for the predicted result based on a given identification context and presents visu-
alizable linguistic evidence for each candidate.
Categories and Subject Descriptors: K.4.1 [Computers and Society ]: Public Policy Issues —Abuse and
crime involving computers; I.7.5 [Document and Text Processing]: Document Capture—Document anal-
ysis; I.2.7 [Natural Language Processing]: Text analysis
General Terms: Design, Algorithms, Experimentation
Additional Key Words and Phrases: Authorship attribution, computational linguistics, text mining, cyber
forensics
ACM Reference Format:
ACM Trans. Info. Syst. Sec. 0, 0, Article 0 ( 0), 30 pages.
DOI:http://dx.doi.org/10.1145/0000000.0000000
1. INTRODUCTION
Research in authorship attribution on anonymous documents is experiencing a contin-
uing exponential growth in recent years because a reliable authorship attribution tech-
nology is useful and valuable in many fields: literary science, sociolinguistic research,
Psycholinguistics, social psychology, forensics, and medical diagnosis, etc. [Daelemans
2013]. Especially under the globalized and decentralized nature of the Internet, the
communications of malicious activities (e.g., illegal material distribution, ransom, and
harassment, etc. [Abbasi and Chen 2008; Iqbal et al. 2013]) can be easily hidden or
The research is supported in part by Discovery Grants from the Natural Sciences and Engineering Research
Council of Canada (NSERC).
Author’s addresses: Benjamin C. M. Fung (corresponding author), School of Information Studies, McGill
University, Montreal, QC, Canada H3A 1X1; email: ben.fung@mcgill.ca
Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted
without fee provided that copies are not made or distributed for profit or commercial advantage and that
copies show this notice on the first page or initial screen of a display along with the full citation. Copyrights
for components of this work owned by others than ACM must be honored. Abstracting with credit is per-
mitted. To copy otherwise, to republish, to post on servers, to redistribute to lists, or to use any component
of this work in other works requires prior specific permission and/or a fee. Permissions may be requested
from Publications Dept., ACM, Inc., 2 Penn Plaza, Suite 701, New York, NY 10121-0701 USA, fax +1 (212)
869-0481, or permissions@acm.org.
c© 0 ACM 1094-9224/0/-ART0 $15.00
DOI:http://dx.doi.org/10.1145/0000000.0000000
ACM Transactions on Information and System Security, Vol. 0, No. 0, Article 0, Publication date: 0.
0:2 Ding et al.
repudiated. Authorship analysis techniques are capable of delving into the informa-
tion from different linguistic levels and of identifying the textual identity trace, which
potentially greatly facilitates the work of cyber forensic investigators and sustains the
social accountability. Stylometry even has been employed as evidence in a law court
[Brennan et al. 2012].
The study of authorship attribution has a long-standing history [Mosteller and Wal-
lace 1964] and many linguistic stylometry and computational techniques have been
developed for solving this problem. These methods have demonstrated outstanding ef-
fectiveness in identifying the actual authors; however, those techniques that achieve
the highest accuracy always involve sophisticated, obscure computational models [Sta-
matatos 2009] and their output is too simple to use as evidence in a court of law. Some
of these models, such as neural network and support vector machine, can be hardly in-
terpreted by an investigator as black-box approaches. Other relatively simple models
also require time and resources to obtain justifiable result through manual inspection.
These issues handicap traditional methods from being widely applied to the real-
life lawsuits as convincing evidence. Practically, computational stylometry is calling
for ‘more explanation as opposed to purely quantitative measure’ [Daelemans 2013].
A better approach should provide explainable and presentable convincing traces as
evidence.
Most of the previous research did not measure the degradation of their methods’
performance as the quantity/quality of the available information degraded simultane-
ously, which is also noted by Solan [2013]. These models are mostly evaluated only on
formal writings, which are relatively long, informative, well-structured, and free from
grammatical errors. On the contrary, short snippets are relatively casual, and their
stylometric features have larger variation. As shown in recent research [Koppel et al.
2011; Luyckx and Daelemans 2011; Narayanan et al. 2012], authorship attribution
accuracy is greatly and directly affected by many objective factors (e.g., text length,
number of known author samples, etc.) due to the unstructured nature of the text it-
self. It is critical for authorship analysis researchers to conduct attribution evaluation
experiments in varying attribution scenarios in order to ‘exclude a bogus conclusion
based on inadequate data’ [Solan 2013] when applied to real-life legal cases.
In this paper, we present a visualizable evidence-driven approach, namely VEA, for
the purpose of facilitating the work of cyber investigation and the decision-making pro-
cess in a law court. Our approach is driven by evidence and based on the lazy learning
scheme [Narayanan et al. 2012]. Basically, our method searches inside the anonymous
document for all the writing styles of different linguistic modalities as evidence and
matches them to the pre-built candidate profiles. Evidence from different linguistic
modalities are combined by using confidence estimation. Finally, it visualizes all the
evidence on the given hypotheses, and it is able to present a visual discrimination be-
tween hypotheses. Besides, it also provides an estimated confidence value based on the
quality of the evidence and the amount of available information in a given attribution
scenario. More importantly, we modeled the attribution scenario and conducted our
experiments in varying situations (i.e., varying length of text, varying candidate size,
etc.) to fully evaluate our method.
In the authorship attribution problem, a set of candidate authors, along with their
corresponding individual writing samples, are available, and the task is to identify the
most plausible author among these candidates based on the given anonymous docu-
ment [Mosteller and Wallace 1964; Holmes 1998; Iqbal et al. 2013]. In most of the pre-
vious studies, the candidate sets involved in their scenarios are mostly of size ranging
from 2 to 20. Some recent studies [Koppel et al. 2011; Narayanan et al. 2012] present
the authorship attribution problem with thousands of possible candidate authors and
try to solve it in a scalable way. In this case, it is more appropriate to firstly employ
ACM Transactions on Information and System Security, Vol. 0, No. 0, Article 0, Publication date: 0.
A Visualizable Evidence-driven Approach for Authorship Attribution 0:3
Fig. 1. A sample fingerprint minutiae matching diagram generated by using fingerprint software and data
from NEUROtechnology2.
scalable methods from these studies to determine a potential candidate subset, and
then use other relatively more accurate techniques to figure out the most plausible
conclusion and derive the justifiable result.
An open-set authorship attribution problem is a variant of the original authorship
attribution problem [Koppel et al. 2011]. In this research problem, the solution is
allowed to output an alternative “unknown” option to indicate that the actual au-
thor could not be found or determined from the given candidate set based on pre-
sented available information. In fact, any solutions that are capable of outputting a
monotonous probability indicating the confidence of a predicted result can be applied
to this problem by setting an appropriate threshold on this output probability value.
The authorship attribution problem is similar to the text classification problem. The
plain text classification task is tough inherently due to unstructured nature of textual
data. By unifying the feature vector and extracting the vector for each sample text,
the textual data can be transformed into structured samples, which is the typical and
traditional authorship attribution solution [Holmes 1994; Stamatatos 2009]. However,
the deviation of each element inside the vector is still strongly affected by the length
of available text. Online texts are mostly very short and, therefore, contain limited
information about the writing style [Iqbal et al. 2013], which causes a larger fluctua-
tion around the mean value in the unified feature vector. This introduces difficulties
in achieving higher accuracy due to the presence of more outliers.
In order to retain reasonable accuracy in the identification task, we try to maximize
the information gained from the given anonymous document and combine both statis-
tical similarity and data mining techniques to develop a hybrid model using the lazy
learning mechanism. Specifically, our contributions are summarized as follows:
— To the best of our knowledge, this is the first trial to design an authorship attri-
bution approach with the goal of promoting not only the accuracy measure, but also
the interpretability and the visualizability of the predicted result. From the very be-
ginning this approach is designed from the perspective of collecting evidence. We sys-
tematically outlined our approach by employing the EEDI (End-to-End Digital Investi-
gation) framework [Bosworth et al. 2012], one of the recognized forensic processes used
in digital forensics investigations. By doing this, we are able to construct a cumulative
evidentiary effect supporting the final output result, and the construction process can
be easily explained using the EEDI framework.
2The software NEUROtechnology used to generate this diagram is available at http://www.neurotechnology.
com/
ACM Transactions on Information and System Security, Vol. 0, No. 0, Article 0, Publication date: 0.
0:4 Ding et al.
— Our approach is concise in design, and its output is visualizable. Inspired by the
visualization of fingerprint matching in Figure 1, where the correlations among fin-
gerprint minutiae can be visually compared, rather than presenting a simple numeric
result we devise an approach visualizing all the supporting evidence on top of our
visual representation of hypotheses. We are able to present a visual discrimination
among these hypotheses and present detailed supporting evidence. More importantly,
we systematically conducted our experiments under varying authorship attribution
scenarios in order to fully evaluate our approach. Our experiments demonstrate that
our approach achieves the state-of-art attribution accuracy, while the output evidence
is visualizable, presentable, and explainable.
— Based on the specific context of the given authorship attribution problem, our
approach is also able to estimate a confidence value. Based on those scenario-related
features that we identified, our method can accurately model and predict the final clas-
sification accuracy. Moreover, to our best knowledge and differing from previously em-
ployed voting-based ensemble methods such as [Koppel et al. 2011], it is the first trial
to combine multiple classifiers by normalizing their scoring vector using individually
estimated confidence values on given classification contexts. We consider classifiers
built on features of different linguistic modalities separately. We explain the necessity
of this step by arguing that stylistic features from different linguistic modalities have
different capacity in determining the actual author and varying sensitivity to the ob-
jective conditions in a given scenario. This is due to the unpredictable coherence of
writing style among known authors’ sample writings, and it is in accordance with our
observations in the experiments. In addition, our approach is extensible, where other
features from different linguistic modalities or non-linguistic features can be further
added as additional events.
The rest of this paper is organized as follows: Section 2 reviews and discusses recent
development and issues in authorship analysis. Section 3 elaborates our Visualizable
Evidence-driven Approach of authorship attribution in detail. Section 4 evaluates our
proposed method VEA on the Enron real-life dataset. Section 5 concludes this paper.
2. RELATED WORKS
The history of authorship attribution backed up by computational and statistical meth-
ods can be dated from the 19th century [Stamatatos 2009]. Contributions to this area
can be broadly categorized from three aspects: the involved stylometric features, the
employed attribution techniques, and the attacks against authorship attribution tech-
niques. Previous research mainly focuses on promoting quantitative evaluation and
few have been done for visualization or explanation. Most explanations for the choice
of features and algorithmic parameters are simply driven by the classification accu-
racy. In this section we are going to discuss several recent related works and research
trends in authorship analysis research. An inclusive survey on the complete history
is beyond the scope of this work. Broader comprehensive surveys can be referred to
[Holmes 1994; Juola 2006; Stamatatos 2009].
2.1. Stylometric Features
Stylometry is the solution of authorship recognition by investigating the linguistic
characteristics inside the given text document, and stylometric features are those
linguistic marks that could qualify or quantify these linguistic characteristics [Sta-
matatos 2009; Brennan et al. 2012]. Stylometric features can be categorized into dif-
ferent linguistic levels [Daelemans 2013; Stamatatos 2009], or, more precisely, linguis-
tic modalities [Sapkota et al. 2013; Solorio et al. 2011]. Various features of different
modalities have demonstrated their effectiveness in distinguishing human writing pat-
ACM Transactions on Information and System Security, Vol. 0, No. 0, Article 0, Publication date: 0.
A Visualizable Evidence-driven Approach for Authorship Attribution 0:5
terns. These modalities include lexical [Koppel et al. 2006; Halteren 2007; Savoy 2012],
character-based [Koppel et al. 2011; Koppel et al. 2012; Escalante et al. 2011], syntac-
tic [Kim et al. 2011; Sidorov et al. 2013; Raghavan et al. 2010], semantic [Hedegaard
and Simonsen 2011; Seroussi et al. 2011; Seroussi et al. 2012] and application-specific
modality [Cristani et al. 2012].
Among all these stylometric features, the character n-gram model in character-based
linguistic modality performs the best, and it is comparatively more robust against
the others [Luyckx and Daelemans 2011; Koppel et al. 2011]. The character n-gram
model actually captures information crossing different modalities [Houvardas and Sta-
matatos 2006]; for example, a frequent ‘ed’ bigram in a character-based modality may
also carry the frequent usage of past tense in a syntactic modality. However, as pointed
out in [Narayanan et al. 2012], solutions using these features also take the risk of cap-
turing the context rather than the authors’ writing style. Regarding the relationship
between stylometric modalities, Sapkota et al. [2013] employed the word “orthogonal”
to assimilate them as independent components. To our best knowledge, the correlation
among linguistic modalities has not been formally investigated in previous author-
ship studies. In this work, we are not going to evaluate whether correlations may exist
among linguistic modalities, but we argue that they have different capacity in attribut-
ing the correct author based on the given problem context.
Stylometric feature sets involved in previous studies can also be divided into two
groups: the unified feature set and the class-specific feature set. Under the unified
feature set, which is employed by most previous solutions, every candidate is modeled
using the same set of features; however, under the class-specific feature set, candidates
are given distinct feature sets and a model is learned for each candidate. As shown by
Abbasi and Chen [2008] and Iqbal et al. [2013], the distinct algorithmic feature set can
better distinguish among candidates’ writing styles and achieve higher performance.
2.2. Attribution Techniques
After the selection of the specific feature scheme, attribution techniques are employed
to predict the actual author of a given snippet. Attribution techniques can be di-
vided into similarity-based approach [Peng et al. 2003; Halteren 2007; Koppel et al.
2011] and model-based approach [Sanderson and Guenter 2006; Lambers and Veen-
man 2009]. The similarity-based approach employs distance functions [Savoy 2012]
to quantify the proximity between a candidate profile and a given anonymous docu-
ment, while the model-based approach builds complicated models to classify the given
document. For the supervised-unsupervised distinction in model learning, previous
methods fall into to the supervised and semi-supervised categories. Those solutions
that achieve the best performance on benchmark data sets are mostly related to ma-
chine learning models.3 Among the model-based approaches, the SVM-based approach
[Abbasi and Chen 2008] and the association-rule-based approach [Iqbal et al. 2013]
achieve higher accuracy due to the fact that they both consider the combination of
feature values among the high-dimensional space. Other machine-learning models,
including decision tree, neural network [Tweedie et al. 1996], meta-learning [Koppel
et al. 2007] and clustering [Layton et al. 2013], are also employed to solve the prob-
lem of authorship attribution. Typically, one-versus-all SVM is chosen as the standard
method when comparing different stylometric features because it has a better multi-
class classification capacity [Duan and Keerthi 2005].
Even though a model-based approach can achieve higher quantitative performance,
most of them involve a complicated computational model, and it is difficult to interpret
its decision-making process. The similarity-based approach is much easier to visualize
3Contest organized in 2004 ALLC/ACH
ACM Transactions on Information and System Security, Vol. 0, No. 0, Article 0, Publication date: 0.
0:6 Ding et al.
and interpret because it retains a monotonous linear relationship between evidence
and conclusion: the smaller the distance between author profile and the targeted doc-
ument, the more similar writing styles they possess.
2.3. Ensemble Method
Recent studies in authorship analysis demonstrate a trend of employing ensemble
methods to combine several separately trained classifiers due to the fact that multiple
classifiers can better fit into sample data and boost the attribution accuracy. In [Kop-
pel et al. 2011], multiple classifiers are built based on different feature sets that are
randomly selected from all available space-free character 4-grams, and the final out-
put depends on their votes. In [Kourtis and Stamatatos 2011], a co-training approach
is employed by using two classifiers. In [Narayanan et al. 2012], an agreement-based
combination of nearest neighbor model and SVM model achieve higher identification
accuracy for blog data. Also in [Raghavan et al. 2010], higher performance is achieved
by employing the votes from classifiers built on different feature sets.
However, all of these works consider classifiers equally weighted. Based on different
classification contexts (e.g., the length of an anonymous snippet, candidate score dis-
tribution, etc.), classifiers built by using features of varying linguistic modalities will
have varying capacity to attribute the author correctly. It is more rational to weight
them accordingly: under the specific classification context, the one that can better dis-
criminate writing style should be weighted more. In our approach, each classifier is
built based on features from different linguistic modalities, and it is weighted based
on its demonstrated consistency among prior written samples. In the literature of ma-
chine learning, there are lots of works that have studied the boosting, stacking, and
ensemble methods, but few of them have been applied to the authorship attribution
problem. Our purpose is not to show that our approach is advantageous over these ap-
proaches, but rather we try to illustrate such an approach can promote the prediction
accuracy and the interpretability. As a whole it is one step forward real-life application
of authorship attribution techniques. A fully comparison between our approach and
other stacking approach is out of the scope of this work.
2.4. Adversary Stylometry
From the perspective of the adversary, several studies are trying to circumvent au-
thorship attribution techniques [Kacmarcik and Gamon 2006; Juola and Vescovi 2010;
Brennan et al. 2012]. The most influential study is by Brennan et al. [2012]. They con-
ducted an experiment on the effectiveness of stylometry obfuscation and imitation. By
recruiting volunteers and using the Amazon Mechanical Turk4 platform, they asked
participants to submit their prior written samples and then write an imitation passage
and an obfuscation passage (no guideline was given to participants on how to obfuscate
or imitate). Their results demonstrate that there is a significant drop in identification
accuracy when it comes to these attacks. Also the accuracy drops when it comes to
one-step, two-step translation attacks.
However, their experimental setup may not truly reflect the effectiveness of their ob-
fuscating approach. First, the decrease in identification accuracy is mostly caused by
the mismatch of context between the obfuscated passages and the training passages.
Obfuscated passages are about the description of participants’ neighbours while pre-
existing writing samples are mostly “scholarly”, and thus more formal. Second, their
experiment also combined and split passages to generate known author writing sam-
ples, which may also lead to a high contextual correlation among samples. As we know,
word-level tokens are good at capturing contextual and thematic correlation [Fung
4https://www.mturk.com/mturk/welcome
ACM Transactions on Information and System Security, Vol. 0, No. 0, Article 0, Publication date: 0.
A Visualizable Evidence-driven Approach for Authorship Attribution 0:7
Fig. 2. Overview of VEA in EEDI framework.
et al. 2003]. We ran our model based on pure lexical n-gram on their data set and
it showed a high correlation of word-level n-gram among training samples (86.01%
identification accuracy for 45 authors; around 500 tokens per sample), with a low cor-
relation between obfuscated texts and training texts. Also in the study of Juola [2012],
a method for detecting the obfuscated texts is proposed using character 3-grams and
word 3-grams. Their experiments also demonstrated a large difference in n-gram us-
age between pre-existing samples and obfuscated samples. The difference in the n-
gram usage pattern implies the contextual and thematic variations, which naturally
leads to the unsatisfactory result when it comes to authorship attribution techniques
that employ character bigrams and trigrams.
2.5. Attribution Result and Its Visualization
Most of the aforementioned studies simply display the most plausible candidate as
their output result. Some recent research is able to add an estimated value indicating
the attribution confidence [Koppel et al. 2011; Narayanan et al. 2012]. However, due
to the fact that authorship analysis techniques are not reliable enough to be widely
recognized, this kind of simple output will still raise doubts when applied in real-life
cases. Instead, visualized evidence corroborating why this candidate author is selected
to be the most plausible one will be more helpful. The only work that we found on
formally visualizing attribution output is by Abbasi and Chen [2006]. Nonetheless,
the visual representation of the Writeprint , which consists of a coordinate graph for
each single feature, cannot scale-up with large number of features and it is difficult to
compare different Writeprints holistically.
3. VISUALIZABLE EVIDENCE-DRIVEN APPROACH (VEA) FOR AUTHORSHIP ATTRIBUTION
In this section, we present our visualizable evidence-driven approach for the author-
ship attribution problem, addressing the issues and problems mentioned in Section 1.
For the purpose of promoting its interpretability and explainability, our approach is
designed according to the nine processes defined by the End-to-End Digital Investiga-
tion framework (EEDI) [Bosworth et al. 2012]. Considering that every digital crime
fundamentally consists of a source point and a destination point, the EEDI frame-
work is a structured flow of processes to establish an evidence chain connecting these
ACM Transactions on Information and System Security, Vol. 0, No. 0, Article 0, Publication date: 0.
0:8 Ding et al.
two points. EEDI is a popular framework employed by digital investigators due to its
capacity of structurally organizing multiple evidence sources to test the conclusion.
We design our approach by adopting the EEDI framework, based on the fact that
the authorship attribution problem can also be fundamentally regarded as consisting
of two points: hypothesis and conclusion. By elaborating the linguistic evidences to es-
tablish an evidentiary chain, we can connect these two points together and thus enable
our approach to present the completed chain as visualized evidence. Also, the process
of chain construction can be easily explained by employing the EEDI framework. The
briefs of procedures employed are outlined in Figure 2.
To begin with, we formally define the authorship identification problem with a prob-
ability confidence value output, as mentioned in Section 1. To be consistent in termi-
nology, in this paper “candidates” or “candidate authors” refer to the potential authors
of the anonymous message, and “author” or “actual author” refer to the true author
of the anonymous message. Let C = {C1, C2, . . . , CN} be a set of N candidate authors
and M = {M1,M2, ...,MN} be a set of their corresponding writing samples where Mi
denotes the set of known samples authored by Ci. The task is to identify the actual
author of given anonymous snippet ω from the candidate set C based on the informa-
tion available in M . Furthermore, the algorithm should be able to output a probability
value p ∈ [0, 1], which denotes the algorithm’s confidence in its predicted result on the
given problem context: p = 0 indicates an unreliable result, while p = 1 indicates a
fully reliable result.
At the same time, we also formally define the term authorship hypothesis (see Defi-
nition 3.1). Basically an authorship hypothesis is a statement that claims a candidate
to be the author of a given anonymous snippet ω. According to the problem defined
above, where N candidate authors are involved, N hypotheses are thus formulated,
respectively targeting on each candidate in C.
Definition 3.1. (authorship hypothesis) Given an unknown author snippet ω and a
known candidate Ci, a hypothesis in the authorship attribution problem is the state-
ment that candidate Ci authored snippet ω.
3.1. Collecting Evidence
The first phase in the original EEDI framework is Collecting Evidence [Bosworth et al.
2012]. This phase is to detect and collect potential evidence from all available sources
of information. The type of evidence may vary, for example, to identify an intrusion;
evidentiary types could be logs of system access, logs of network packages, and fire-
wall logs, etc. They required different collection and preprocessing methods. Under
the EEDI framework, evidence of different types are grouped together and initiated
into independent events, which will be passed to the next process of EEDI.
Accordingly, based on the given anonymous snippet ω, during this phase our task is
to identify all the linguistic evidence. Likewise, linguistic characteristics reflected on
the given snippet ω are of varying types based on their particular linguistic modali-
ties (e.g., syntactic, lexical, and character-based, etc.), and linguistic characteristics of
certain modality require specific techniques for feature extraction [Stamatatos 2009].
Thus, we group evidence into independent events based on their linguistic modalities,
and construct them respectively.
We start this phase by defining the term evidence unit. Let F (ω) = {f1, f2, . . . , fu}
denote the universe of writing style features extracted from the anonymous snippet ω.
Basically, an evidence unit is defined as one specific writing style feature element with
its associated scoring vector (see Definition 3.2). The similarity metric we employed to
describe the correlation between candidate Ci and the linguistic feature feum will be
ACM Transactions on Information and System Security, Vol. 0, No. 0, Article 0, Publication date: 0.
A Visualizable Evidence-driven Approach for Authorship Attribution 0:9
Table I. Employed linguistic features. Features in the examples are extracted from the text: ‘it is noticed and appreci-
ated ’ and the corresponding Part-of-Speech tag sequence is ‘PRP VBZ VBN CC VBN ’. The n-gram is extracted in an
overlapping manner.
Modality Characteristics Details Examples
Lexical Word Level N-gram Length:1-8 ‘It’, ‘it is’, ‘it is noticed’, and ‘is noticed’, etc.
Character Character Level N-gram Length:1-8 ‘no’, ‘not’, ‘notic’, ‘tice’, ‘notice’, ‘a’, ‘an’, and ‘nd’, etc.
Syntactic POS N-gram Length:1-8 ‘PRP VBZ VBN’, ‘CC VBN’, and ‘VBN CC VBN’, etc.
discussed in Section 3.2 (See Equation 3). Evidence unit is the minimum scoring unit
and minimum visualization unit, which will be further discussed in Section 3.2.
Definition 3.2. (evidence unit) Evidence unit eum is formulated as set {feum , ~veum}:
given a certain linguistic feature feum , ~veum ∈ RN is a numeric vector (v1, ..., vi, ..., vN ),
where N indicates the number of candidates in C, and value vi indicates the score
describing the correlation between candidate Ci and the linguistic feature feum .
The linguistic writing characteristics employed in this paper include lexical modal-
ity, character modality, and syntactic modality. Specifically they include lexical word
n-gram, character level n-gram, and syntactic level Part-of-Speech n-gram [Stamatatos
2009]. For the Part-of-Speech tagging we used the pre-trained Maxent model from
Opennlp5. Refer to Table I for detailed information and examples. The length of these
n-grams varies from 1-8 because we can hardly find any n-gram present repetitively
with length more than 8 in the dataset. We employ n-gram technique because previ-
ous studies [Koppel et al. 2011; Savoy 2012; Sidorov et al. 2013] show its effectiveness
in capturing the writing style. Also, they are comparatively easier to visualize and
present as evidence units; more details will be discussed in Section 3.5.
Algorithm 1 Event Construction (EC)
Input number of candidates N , linguistic type Type, anonymous snippet ω
Output event ev
1: Tev ← Type . associate this event with the given type of linguistic modality
2: features← extract all linguistic characteristics of type Tev from snippet ω
3: for m = 1 to |features| do
4: ~veuevm ∈ R
N , ~veuevm ← {0} . initialize as a zero vector
5: feuevm = features[m] . Pair each feature with a new evidence unit
6: EUev ← EUev ∪ {euevm}
7: end for
8: return ev
To preserve the explainability of our approach, unlike previous research, we do not
employ any feature selection techniques such as methods in [Yang and Pedersen 1997].
That means we employ the full set of n-grams rather than an optimal top-K subset.
Previous research, such as [Houvardas and Stamatatos 2006], demonstrate that such
a top-K culled subset can already achieve high accuracy in the authorship attribution
problem, but it is difficult to explain why and how this parameter K, which indicates
the size of employed features, is chosen. In the previous research, the optimal K value
is learned from the presented experimental results and it is assumed that this value
would work accordingly against other data. Moreover, forensic investigation prefers
completeness and selecting a subset of evidence require an explanation. Taking the
5available at http://opennlp.apache.org
ACM Transactions on Information and System Security, Vol. 0, No. 0, Article 0, Publication date: 0.
0:10 Ding et al.
full set can avoid such issue. Even though this approach introduces high runtime com-
plexity, it is acceptable in an investigation scenario to run it only once for the purpose
of collecting evidence. We believe that this trade-off between explainability and run-
time complexity is reasonable.
Definition 3.3. (event) Given an event evn denoted by {Tevn , Confevn , ~Vevn , EUevn},
Tevn is the type of linguistic modality with which this event is associated, EUevn is a
set of evidence units such that ∀euevnm ∈ EUevn , feuevnm is of type Tevn . Also ~Vevn ∈ R
N is
a numeric vector of size N that describes to what extent this event evn supports each
predefined hypothesis, and Confevn ∈ [0, 1] is a numeric value that indicates the con-
fidence that this event will arrive at its conclusion based on the present classification
context.
We define event as a set of evidence units of same linguistic modality and other asso-
ciated properties (see Definition 3.3). Based on the selected linguistic feature scheme,
the extraction procedure is shown in Algorithm 1. The input includes the number of
candidates in C, linguistic modality type Type, and the anonymous snippet ω. In Line
2, all features of given linguistic type are extracted from the anonymous snippet ω.
Based on our selected features, all the n-grams of given length 1 to 8 are thereby ex-
tracted and then assigned to the evidence units (see Line 5).
For each linguistic modality, we construct an event by using Algorithm 1. After event
constructions, all the events will be passed into the next process, as shown in Figure 2.
In our case, three events are created: a lexical event (1-8 word n-grams), a character
event (1-8 character n-grams), and a syntactic event (1-8 Part-of-Speech n-grams).
Example 3.4. Considering a sample text ‘it is’, to construct the lexical event ev1 in
our case, firstly all the n-grams are extracted as evidence units: feuev11 = ‘it’, feuev12 = ‘it
is’, and feuev13 = ‘is’. Assuming that we have two candidate authors thus C = {C1, C2},
after applying Algorithm 1 the feature vectors for these evidence units are ~veuev11 =
(0, 0), ~veuev12 = (0, 0) and ~veuev13 = (0, 0).
3.2. Analysis of Individual Event
The second phase in EEDI process flow is to analyze each event independently. The
goal in this phase is to isolate each event and access the impact of each event on
the overall investigation problem individually [Bosworth et al. 2012]. Correspondingly,
during this phase in our algorithm, we are going to independently assess each event
with respect to its contribution in the overall author identification problem. For each
event, two analyses are conducted:
— Scoring: to score each hypothesis (i.e., to score each candidate author) based on the
given event’s feature set, and determine which hypothesis is more plausible to be the
correct one.
— Consistency analysis: to evaluate the feature set of a given event regarding its ca-
pability of distinguishing the writing styles among different candidates based on all
known samples M .
The first analysis adopts the similarity-based approach to score each hypothesis,
and it is shown in Algorithm 2. To begin with, by using tf− idf scoring scheme and
regarding all the extracted n-grams from an event as an unified feature vector, N + 1
numeric vectors are constructed: one numeric vector ~a for anonymous snippet (Line 2)
and N candidate author numeric vectors (~c in Line 7).
Although there exist other scoring functions that may achieve higher identifica-
tion accuracy [Martineau et al. 2009] [Lambers and Veenman 2009], we use the
ACM Transactions on Information and System Security, Vol. 0, No. 0, Article 0, Publication date: 0.
A Visualizable Evidence-driven Approach for Authorship Attribution 0:11
PV `
PV `
2
1
PV 1
PV2
PV
θ
2
θ1
n-gram1
n-gram3
n-gram2
Fig. 3. A sample 3 n-grams space. Each n-gram represents one dimension.
tf−idf scheme [Zobel and Moffat 1998] for its simplicity. As in Equation 1 and Equa-
tion 2, the tf score captures the normalized frequency of a given n-gram, and the
idf score gives weight to each n-gram by considering its discriminant power. Variable
|AuthorsEverUsed(gram)| represents the number of candidate authors that ever used
n-gram gram in their writing samples. The constant b is used to avoid the divide-by-
zero problem, and it is typically chosen as 1. We set b as 0.1, and in this way it is
in a smaller order of magnitude when compared with |AuthorsEverUsed(gram)|. For
the anonymous text we only consider the tf score. Other scoring schemes could be
employed by considering them as separate events, which could be explored in future
studies.
tf(gram,Mi) =
frequency(gram,Mi)
maxGramFrequency(Mi)
(1)
idf(gram) = log
(
N
b+ |AuthorsEverUsed(gram)|
)
(2)
After the construction of aforementioned N + 1 numeric vectors, a final score is de-
rived for each hypothesis (candidate) by comparing the distance between each candi-
date vector ~c and the vector for anonymous snippet ~a. Here we adopt the dotproduct
distance to derive this score, as shown in Line 12 in Algorithm 2.
similarity( ~Pi, ~Pω) = proj ~Pω
~Pi × ‖ ~Pω‖
= ‖ ~Pi‖ × cos(Θi)× ‖ ~Pω‖
= ‖ ~Pi‖ ×
~Pi · ~Pω
‖ ~Pi‖ × ‖ ~Pω‖
× ‖ ~Pω‖ = ~Pi · ~Pω
(3)
Considering a sample 3 n-gram space in Figure 3, ~PV1, ~PV2, and ~PV , respectively,
are the style vectors of candidate1, candidate2, and the anonymous snippet ω. In previ-
ous work such as [Koppel et al. 2011] where n-gram related features are employed, the
cosine distance [Salton and Buckley 1988] is generally used to measure the distance
between vectors. It only considers the included angles between vectors: the difference
between Θ1 and Θ2 in the example. However, the difference in writing style is reflected
in both n-gram coverage and normalized frequency of n-gram usage. Regarding the di-
rection of ~PV as the anonymous snippet’s writing style, we take the projection ~PV ′1 of
ACM Transactions on Information and System Security, Vol. 0, No. 0, Article 0, Publication date: 0.
0:12 Ding et al.
~PV1 on ~PV and the projection ~PV ′2 of ~PV2 on ~PV for comparison. The projection models
the amount of demonstrated evidence from a given vector and shows the strength of
support of the vector in this direction. The distance function is shown in Equation 3,
and for the ease of computation we multiply the norms of the anonymous vector, which
is independent to the values of other vectors, and finally derive the dotproduct distance
function.
Algorithm 2 Event-based Scoring (ES)
Input event ev, writing samples M , anonymous snippet ω
Output scoring vector: ~s
1: ~s ∈ RN , ~s← {0} . create a numeric vector of size N
2: ~a ∈ R|EUev|, ~a← {0}
3: for m = 1 to |EUev| do
4: ~a[m] = tf(feuevm , ω) . this vector is for anonymous snippet ω
5: end for
6: for i = 1 to N do
7: ~c ∈ R|EUev|, ~c← {0} . this vector is for candidate author i
8: for m = 1 to |EUev| do
9: ~c[m] = tf(feuevm , Mi) × idf(feuevm ) . here feature feuevm is a n-gram
10: ~veuevm [i]← ~c[m]× ~a[m] . global values which will be used for evi-
dence unit visualization
11: end for
12: ~s[i] = ~a · ~c
13: end for
14: return ~s
At the end of the first analysis (see Line 10 of Algorithm 2), each evidence unit’s
scoring vector ~v is updated with the corresponding score vi that describes the correla-
tion between candidate i and this given linguistic feature. This updated value will be
used in the visualization process elaborated in Section 3.5.
Algorithm 3 shows the second analysis. As defined in Definition 3.3, each event is
represented as a set of linguistic features. The goal of this analysis is to evaluate fea-
tures of a given event with respect to their demonstrated consistency and discriminant
power among the known-author writing samples M . Such properties vary for different
linguistic modalities under the given identification context (e.g., anonymous snippet
length, size of known-author writing, and number of candidates, etc.). Hence, we treat
each event as a stand-alone similarity-based classifier. Then a confidence value is es-
timated for each event in an isolated manner by building linear models. The features
used to model an identification context is listed in Table II. In this way, an event is the
minimum confidence estimation unit.
To proceed with this analysis, a 10-fold cross validation test is conducted by par-
titioning all the available writing samples from M into ten groups of roughly equal
size (Line 1 in Algorithm 3). Of these ten groups, one group is selected as the test set,
then the remaining nine groups are used to build events following Algorithm 1 and
to predict the author of samples from the test set by using Algorithm 2 (Line 7-8 in
Algorithm 3). The candidate with the highest score output (Line 9-10 in Algorithm 3)
will be the predicted result. The next step is to collect vector values for the attributes
listed in Table II as a sample (Line 14 in Algorithm 3). After iterating documents from
the test set, the precision value of the test is calculated (Line 17 in Algorithm 3) and
padded to each sample vector in this fold as values of target attribute (Line 18-20 in
ACM Transactions on Information and System Security, Vol. 0, No. 0, Article 0, Publication date: 0.
A Visualizable Evidence-driven Approach for Authorship Attribution 0:13
Algorithm 3 Event-based Identification (EI)
Input known-author writing samples M , candidate set C, event ev, anonymous snip-
pet ω
Output event ev
1: folds← split(M ) . split M into 10 folds for cross-validation; each fold in-
cludes nine training groups and one testing group
2: samples← ∅; . create an empty set of samples; each sample follows at-
tributes in Table II
3: for each fold in folds do
4: foldSamples← ∅; . an empty set of samples following attributes in Table II
5: correctGuess← 0
6: for each doc in TestSetfold do
7: ev′ ← EC(N, Tev, doc) . corresponds to Algorithm 1
8: scores← ES(ev′, TrainSetfold, doc) . corresponds to Algorithm 2
9: index =IndexOfMaxValue(scores)
10: predictedAuthor = C[index]
11: if predictedAuthor is ActualAuthor(doc) then
12: correctGuess = correctGuess+ 1
13: end if
14: sample← GenerateSample(scores, doc) . collect feature values (Table II)
15: foldSamples← foldSamples ∪ {sample}
16: end for
17: precision← correctGuess|TestSetfold| . calculate the precision value for this fold.
18: for each sample in foldSamples do
19: sample← pad the vector sample with precision as target attribute.
20: end for
21: samples← samples ∪ foldSamples
22: end for
23: Modelev ← buildModel(samples) . build a prediction model for this event ev
using precision as target attribute
24: ~Vev ← ES(ev, M , ω) . collect sample from current classification context
25: Confev ←Modelev.predict(~Vev, ω) . estimate confidence
26: return ev
Table II. Features for confidence estimation (identification context).
scoreavg average score in scoring vector (~Vev)
scoremax maximum score in scoring vector (~Vev)
scoremin minimum score in scoring vector (~Vev)
distmax−runnerup gap statistic between max and the runner-up
testlength number of tokens in testing (anonymous) document ω
tokenscommon number of shared tokens between M and ω
Algorithm 3). This process is repeated ten times and each group is used as the test
set exactly once. Based on the collected samples, a linear model is built for each event
(Line 23 in Algorithm 3) .
In Line 24, the event derives a scoring vector for given candidates based on the
anonymous snippet ω by using Algorithm 2. Based on this scoring vector, a sample is
created following attributes in Table II, and then it is fed into the built model to derive
the predicted precision value, which will be used as the confidence value (Line 25 in
Algorithm 3)
ACM Transactions on Information and System Security, Vol. 0, No. 0, Article 0, Publication date: 0.
0:14 Ding et al.
Regarding the employed attributes to model the identification context, in addition to
using the ‘gap statistic’ that describes the gap between max score and the runner-up in
[Narayanan et al. 2012; Koppel et al. 2011; 2006], we also include more attributes that
describe the scoring distribution including the maximum, the minimum, the average,
and the length of testing document. Our experiment in Section 4.4 shows that these
attributes are all significantly important for confidence estimation. However, we do not
include the size of known-author writings, because when we conduct the 10-fold cross
validation process (Line 3 to Line 22 in Algorithm 3), the intercept value in the built
linear model already reflects its effect as baseline.
3.3. Event Normalization
Algorithm 4 Confidence-based Normalization (CN)
Input event ev, anonymous snippet ω
Output event ev
1: for i=1 to N do
2: ~Vev[i] = ~Vev[i]× Confev . normalize score for this event
3: end for
4: for m = 1 to |EUeu| do
5: for i = 1 to N do
6: ~euevm [i] = ~eu
ev
m [i]× Confev . normalize the score inside each evidence unit
7: end for
8: end for
9: return ev
The event normalization process under the EEDI framework is to normalize all evi-
dentiary data of the same type from different sources into the same measurement level
and to further consider the possibility of combining them [Bosworth et al. 2012]. For
example, different events from different sources may have varying timing formats or
different time zone settings; in order to chain them together, these formats must be
normalized.
Accordingly, in our approach, after the previous process each event now has a scoring
vector, while they have different confidence values, which means they have different
performance levels on discriminating candidates. Before considering the combination
of evidentiary data from these events, normalization of performance for each event
must be done. Hence, we conduct our normalization step by multiplying the scoring
vector with corresponding confidence value for each event (Line 2 in Algorithm 4).
Also, correspondingly, we update the numeric vectors stored inside all evidence units
of each event by multiplying the original score with the confidence value (Line 4 to 8
in Algorithm 4). After normalization, all the events are passed into the next process.
3.4. Secondary-level Correlation
Under the EEDI framework, this process is to examine the correlation between events
and to consider ways of combining the evidence into an evidentiary chain [Bosworth
et al. 2012]. In our case, accordingly, all the events from previous process are combined
to derive a unidimensional score for each candidate author. The idea is to summa-
rize the fine-grained evidence of different linguistic modalities into a single kind of
evidence: the linguistic evidence.
The procedure for event combination is shown in Algorithm 5. Since in previous
process all the events have been normalized into the same identification performance
level, the final scoring vector is simply the sum of the scoring vector from each input
ACM Transactions on Information and System Security, Vol. 0, No. 0, Article 0, Publication date: 0.
A Visualizable Evidence-driven Approach for Authorship Attribution 0:15
Algorithm 5 Event Combination (EC)
Input writing samples M , candidate set C, set of event EV , anonymous snippet ω
Output author, confidence value p
1: ~fs ∈ RN , ~fs← {0} . initialize final scoring vector with 0
2: conf ∈ R|EV |, conf ← {0} . a vector of confidence values
3: for n = 1 to |EV | do
4: for i = 1 to N do
5: ~fs[i] = ~fs[i] + ~Vevn [i]
6: end for
7: conf [n] = Confevn
8: end for
9: prediction← IndexOfMaxValue( ~fs) . determine the prediction result
10: author ← C[prediction]
11: agreedConf ∈ R|EV |, conf ← {0}
12: for n = 1 to |EV | do
13: if evn agrees prediction then
14: agreedConf [n] = conf [n]
15: else
16: agreedConf [n] = −1
17: end if
18: end for
19: p = max(agreedConf ) . estimate the final confidence value
20: return author, p
event. In this algorithm, Line 1 to 8 combine scoring vectors from all input events,
and Line 9 determines the prediction result as the candidate author that achieves the
highest score.
p =
EV
max
evn
P (predicted author | evn)
=
EV
max
evn
{
Confevn , if evn agrees on final predicted author
0, otherwise
(4)
To combine multiple confidence values of different classifiers, typical approaches in-
clude Product Rule, Max Rule, Min Rule, and Majority Vote Rule, etc. [Kittler et al.
1998]. Here we combine the Max Rule and Majority Vote Rule to derive our final esti-
mated confidence value. As Line 12-19 in Algorithm 5 shows, the final confidence value
is determined as the maximum estimated confidence value among all the events that
agree on the final output candidate (also see Equation 4, predicted author indicates the
final output prediction from Line 9 Algorithm 5).
Previous research [Koppel et al. 2011; Narayanan et al. 2012] mostly combine clas-
sifiers using the ensemble method and derive the final result in a voting manner. Dif-
ferently from these, we combine classifiers or, rather, events, in our case, in the scoring
vector level and each scoring vector is normalized by the estimated confidence (see
Equation 5, ~fs[k] is the final score for candidate k as used in Line 1 Algorithm 5, and
~Vevn [k] is the final score for candidate k in the scoring vector of event evn as defined in
Definition 3.3). Our experiment demonstrates that this approach can achieve higher
accuracy.
ACM Transactions on Information and System Security, Vol. 0, No. 0, Article 0, Publication date: 0.
0:16 Ding et al.
~fs[k] =
EV∑
evn
~Vevn [k]× Confevn (5)
3.5. Chain of Evidence Construction
In this process, under the EEDI framework evidences are aligned on a timeline, and
based on this timeline a coherent chain of evidence is developed [Bosworth et al. 2012].
This chain of evidence is able to connect the starting point and ending point of the
criminal incident. However, in our solution, temporal priority among all linguistic evi-
dence is nonexistent. Based on the employed dot-point distance, the cumulative effect
of evidences is instead established from hypotheses to conclusion.
~fs[k] =
EV∑
evn
EUevn∑
euevnm
~veuevnm [k] (6)
At this point, based on the input events the cumulative effect to derive the final uni-
dimensional score for each hypothesis can be expressed as Equation 6 by employing
the intermediate results stored in evidence units according to Algorithm 2 and Algo-
rithm 4. ~fs[k] refers to the final score for candidate k in Algorithm 5 Line 1, which
is also the same variable in Equation 5 but is calculated using different intermediate
results.
The task of this process is to visualize all the evidence units with respect to their
proximity to each hypothesis. The visually cumulative effect of all evidence units
should be able to reflect the difference between candidate scores ~fs[k]. Formally, a
visual measurement function vf should have the following property:
Property 1. (proportionally visualizable) Given a set of hypotheses H, we say they
are proportionally visualizable over a visual effect function vf if they satisfy: ∀Hk ∈ H
vf(Hk) ∝ ~fs[k].
To begin with, hypotheses are visualized. As defined in Definition 3.1, the hypothesis
is the statement that an anonymous snippet ω is authored by one specific author. Given
N candidates in C, we thus have N hypotheses, and each hypothesis is represented
by the raw tokens extracted from the anonymous snippet ω with the corresponding
statement about one specific candidate.
As shown in Figure 4, two hypotheses are presented as examples . Each hypothesis
is represented by the hypothetical statement on the title along with the following ev-
idence extracted from anonymous snippet ω: the first row represents character level
tokens, the second row represents word level tokens, and the third row represents
Part-Of-Speech tokens. To make the representation simpler and clearer, in the first
row we display the character tokens with a transparent font colour so that each char-
acter token can be easily matched to the lexical token beneath.
After presenting the visualizations of hypotheses we are going to visualize all evi-
dent units (defined in Definition 3.2) by colouring each evidence unit’s tokens in the
above representations of hypotheses. The colour is determined by how affiliated an
evidence unit is to the given hypothesis. An evidence unit hereby is our smallest visu-
alization unit.
To colour the tokens the HSL colour scheme is employed. The HSL scheme encodes
colour by using three parameters: Hue, Saturation, and Lightness. Hue represents the
selected tint ranging from 0 to 360, and in most cases it is used as a qualitative rep-
resentation in data visualization: the difference in kinds reflected in the difference of
ACM Transactions on Information and System Security, Vol. 0, No. 0, Article 0, Publication date: 0.
A Visualizable Evidence-driven Approach for Authorship Attribution 0:17
Fig. 4. Evidentiary chain visualization: hypothesis representations and the visualized evidence units.
tint. Saturation controls its colourfulness (from 0 to 100), and Lightness measures how
much light should be reflected from this colour, ranging from 0 (appears as black) to
100 (appears as white); 50 is normal [Çelik et al. 2012]. Lightness is visually suitable
as a quantitative/sequential data representation. Dark equals more is a standard car-
tographic convention [Harrower and Brewer 2003] and the difference of lightness can
still be perceived by people with red-green colour vision impairments [Harrower and
Brewer 2003]. Thus we adopt the lightness value representing the scores of evidence
units.
Based on our observation, given an evidence unit euevnm and its scoring vector ~veuevnm ,
in most cases the range of this vector range(~veuevnm ) is only a small fraction of the overall
score range. Simply picking up the lightness value of the given evidence unit euevnm , for
hypothesis k based on its score ~veuevnm [k], will naturally lead to the imperceptible visual
discrimination among hypotheses. Hence, instead of visualizing the original scores, we
visualize dif(euevnm , k) in Equation 7, which represents how the original score differs
from the minimum score in that scoring vector. The constant α is used to shift the
range, avoiding assigning a blank background on euevnm for hypotheses k when ~veuevnm [k]
equals min(~veuevnm ), in order to visually distinguish the absence of a n-gram (blank
background) and the presence of a n-gram but for a given hypothesis it is the minimum
score in the scoring vector.
ACM Transactions on Information and System Security, Vol. 0, No. 0, Article 0, Publication date: 0.
0:18 Ding et al.
To calculate the value dif(euevnm , k) for each hypothesis k on each evidence unit euevnm ,
the global range maxR of the scaled difference is first calculated by using first three
equations in Equation 7. The range of the scaled difference in scoring vectors is calcu-
lated for each event and then all ranges are combined to reach maxR (globally maxi-
mum scaled difference in all scoring vectors).
range′(eum) = max(~veum)−min(~veum)
maxRevn = max({euevnm ∈ EUevn | range′(euevnm )})
maxR = max({evn ∈ EV | maxRevn}) + α
dif(euevnm , k) =
~veuevnm [k] + α−min(~veum)
maxR
(7)
Example 3.5. Considering a sample text ‘your organization’ and two candidates C1
and C2. Assuming we only employ the lexical event EV = {ev1} so we have three
lexical n-grams euev11 ‘your’, eu
ev1
2 ‘organization’ and eu
ev1
3 ‘your organization’. Also
we assume that their corresponding scoring vectors are ~veuev11 = (0.3, 0.6), ~veuev12 =
(0.8, 0.5) and ~veuev13 = (0.1, 0.2). In this case, range
′(euev11 ) = 0.3, range′(eu
ev1
2 ) = 0.3,
range′(euev13 ) = 0.1 and maxRev1 = 0.3. By setting α = 0.1, since we only use one event,
we have maxR = 0.4. Applying the 4th function in Equation 7 on each evidence unit’s
scoring vectors then we have dif(euev11 , 1) = 0.25, dif(eu
ev1
1 , 2) = 1, dif(eu
ev1
2 , 1) = 1,
dif(euev12 , 2) = 0.25, dif(eu
ev1
3 , 1) = 0.25, and dif(eu
ev1
3 , 2) = 0.5.
The linguistic feature we chose is based on the n-gram model, where each evidence
unit is represented as a sequence of tokens. As such, different evidence units may
share the same token in the hypothesis representation. Accordingly, each evidence
unit is coloured in an overlapping manner.
LHktokenn(eu
evn
m ) =
{
LHktokenn − η × dif(eu
evn
m , k), if eu
evn
m stem from tokenn
LHktokenn otherwise
(8)
Given a visual representation of hypothesis Hk, we start by initializing all tokens’
backgrounds with a maximum lightness value (i.e., the background colour reflects
100% light and appears to be blank), and then we enumerate tokens in the hypothe-
ses representation to apply Equation 8. Given a tokenn in Hk, for each previously
extracted evidence unit euevnm , if feuevnm stems from tokenn then the token’s lightness
value degrades by the multiplication of degradation factor η and its normalized vari-
ant score dif(euevnm , k). Degradation factor η ∈ (0, 100] controls the contrast between hy-
potheses and can be designated by the user or empirically as 100.0/MaxMatch, where
MaxMatch indicates the maximum number of evidence units that can stem from the
same token. euevnm stems from tokenn means that the evidence units euevnm partially or
completely originates from the tokenn. For example, the evidence unit “your organiza-
tion” can stem from the token “your” in phrase “to your organization” but not from the
token “your” in phrase “your teams”.
Since this “stem” mapping between tokens and evidence units is identical for all the
hypotheses, given the same evidence unit the lightness value of a token is inversely
proportional to the score dif(euevnm , k) of the hypothesis. In this way, it is also inversely
proportional to the original score ~veuevnm [k] (see Equation 9).
Example 3.6. Continue Example 3.5. For the word ‘your’, there are two n-grams,
euev11 ‘your’ and eu
ev1
3 ‘your organization’ that stem form this word. By setting η = 30,
the lightness of the word ‘your’ for candidate C1 (i.e., hypothesis H1) LH1yours = 100 −
ACM Transactions on Information and System Security, Vol. 0, No. 0, Article 0, Publication date: 0.
A Visualizable Evidence-driven Approach for Authorship Attribution 0:19
30× dif(euev11 , 1)− 30× dif(eu
ev1
3 , 1) = 100− 30× 0.25− 30× 0.25 = 85. Correspondingly,
for candidate 2 (i.e., hypothesis 2) LH1yours = 100− 30× 1− 30× 0.5 = 55.
LHktokenn(eu
evn
m ) ∝ dif(euevnm , k)−1
∝ (~veuevnm [k] + α−min(~veuevnm ))
−1 ∝ ~veuevnm [k]
(9)
Our selected visual function vfVEA(Hk) for hypothesis k is the global darkness of
its visual representation, denoted by GD(Hk), which is inversely proportional to the
global lightness GL(Hk) function. We assume that the global lightness value is con-
tributed by the cumulative lightness of all tokens on the representation. This assump-
tion is reasonable when the anonymous snippet is short. GD(Hk) is formulated in
Equation 10.
vfVEA(Hk) = GD(Hk) ∝ GL(Hk)−1 (10)
It can be shown that this visual function satisfies Property 1 as follows: First, the
global lightness function GL(Hk) for hypothesis k is formulated as the cumulative
lightness of all tokens (see Step 1 in Equation 11). By combining Equation 9, the
GL(Hk) function is inversely proportional to the final score of hypothesis k (see Step
2-5 in Equation 11). ~fs[k] refers to the same variable in Equation 5 and 6.
GL(Hk) =
tokens(Hk)∑
tokenn
EV∑
evn
EUevn∑
euevnm
LHktokenn(eu
evn
m )
∝
( tokens(Hk)∑
tokenn
EV∑
evn
EUevn∑
euevnm
dif(euevnm , k)
)−1
∝
( tokens(Hk)∑
tokenn
EV∑
evn
EUevn∑
euevnm
~veuevnm [k]
)−1
∝
( EV∑
evn
EUevn∑
euevnm
~veuevnm [k]
)−1
∝
(
~fs[k]
)−1
(11)
In this way, by combining Equation 10, the visual function GD(Hk) is proportional
to the final score of hypothesis k (see Equation 12). Thus, our selected presentation of
hypothesis and evidence unit satisfies Property 1 over visual function GD(Hk), which
indicates that the darker the hypothesis representation’s holistic colour is, the higher
final score this hypothesis possesses.
vfVEA(Hk) = GD(Hk) ∝ GL(Hk)−1 ∝ ~fs[k] (12)
After all the aforementioned colouring is done, one can conclude that the hypothe-
sis with the most holistically darkest colouring representation is the most plausible
one. As the example in Figure 4 demonstrates, representation of hypothesis 2 is more
holistically darker than that of hypothesis 1, and thus the corresponding candidate,
candidate Y, is the plausible author.
In addition, we construct an evidence unit cumulative scoring diagram, as shown in
Figure 5. An area with a different colour represents a different hypothesis, and the one
ACM Transactions on Information and System Security, Vol. 0, No. 0, Article 0, Publication date: 0.
0:20 Ding et al.
0.00
0.01
0.02
0.03
0.04
0.05
1
2
6
5
1
7
6
1
0
1
1
2
6
1
5
1
1
7
6
2
0
1
2
2
6
2
5
1
2
7
6
3
0
1
3
2
6
3
5
1
3
7
6
4
0
1
4
2
6
4
5
1
4
7
6
5
0
1
5
2
6
5
5
1
Feature Index
C
u
m
u
la
ti
v
e
 F
e
a
tu
re
 S
c
o
re
author
candidate Y
candidate X
Fig. 5. Cumulative evidence unit scoring diagram: the serial that achieves the highest score at the end of
x-axis is for the most plausible candidate.
Table III. Confidence estimation.
Events Estimated confidence
n-gram (lexical level) 0.8311
n-gram (character level) 0.9560
n-gram (syntactic level) 0.6867
Voted Maximum 0.9560
that achieves the highest score at the end of x-axis is the most plausible one. If many
candidates are involved, or the given anonymous text is too long, the cumulative visual
discrimination will be difficult to perceive in Figure 4, while this scoring diagram is
still able to show which hypothesis achieves the highest final score, and the detailed
evidence can still be referred to the visualized evidence.
At the end of this phase, we also list all the estimated confidence values in Table III.
In this example, since all three events agreed on same plausible hypothesis, the overall
confidence value is simply the maximum: one.
3.6. Corroboration
Note that linguistic evidence is only one kind of event, other non-linguistic evidence
exists related to the criminal incident and may support the authorship identification
problem. Evidence may include system logs, network logs, or IP-related information
from ISP, or even the socioeconomic relationship between each candidate and this in-
cident. By including this process, linguistic evidence for this authorship attribution
problem becomes a stand-alone event, and investigators can further connect the lin-
guistic and non-linguistic events to corroborate their final hypothesis on the incident.
4. EXPERIMENTAL RESULTS
The objective of the experiment is to evaluate our approach with respect to the identi-
fication accuracy and robustness under varying circumstance in the authorship attri-
bution problem. The dataset that we adopted is the Enron Email dataset, which was
made public by the Federal Energy Regulatory Commission [Shetty and Adibi 2004].
This dataset contains 517,424 emails from 151 users. Email data tend to be relatively
short compared to other literature works and bring more challenges to the authorship
identification problem.
As previous work demonstrated, the identification context (i.e., the available sam-
ples, and available hypotheses/candidates, etc.) of the authorship attribution problem
ACM Transactions on Information and System Security, Vol. 0, No. 0, Article 0, Publication date: 0.
A Visualizable Evidence-driven Approach for Authorship Attribution 0:21
(26,0.5)
(55,0.75)
(320,0.99)
0.00
0.25
0.50
0.75
1.00
0 100 200 300
Email length in terms of words 
P
e
rc
e
n
ta
g
e
Empirical Distribution Function
10.49
0.00
0.01
0.02
0 100 200 300
Email length in terms of words 
D
e
n
si
ty
Kernel Density & Histogram
Fig. 6. Dataset analysis.
strongly affects the solution’s performance, while most of the previous experiments
by design failed to test their model systematically. To avoid other possible explana-
tions of our experimental results, we first conducted statistical analysis of the dataset
and then conducted both controlled sampling experiments and stratified randomized
experiments.
4.1. Dataset Preprocessing, Analysis, and Experimental Setups
We started by conducting preprocessing procedures on this dataset. The first proce-
dure extracted the body from each email and the second procedure cleaned up the
identity-related information. The extraction procedure was completed by using a set
of regular expressions that removed the ‘forward’ and ‘reply’ part of the email as well
as all the header information. Removing the identity-related information is relatively
more complex. We completed this procedure by employing following steps:
— We utilized the regular expressions to replace URL links with the ‘< link/ >’ tag.
— We utilized the Name Finder in OpenNLP6 project to replace all the found name
entries with the ‘< name/ >’ tag.
— We fetched the employee information from the data set and generated a list of first
names and a list of last names. We replaced all the tokens that were exactly the same,
case ignored, as the names in these two lists with the ‘< name/ >’ tag.
— Based on the above name lists, we found all the tokens that had exactly 1 string
editing distance [Levenshtein 1966] to the names, case ignored, and replaced these
tokens with the ‘< name/ >’ tag. We assume that the author of a given email can
only make one character mistake when typing his or another’s first/last name.
— Also based on the employee information, we constructed a list of short names, by
concatenating the first character of a first name and that of the last name. We found
these tokens and replaced them with the ‘< name/ >’ tag in the last sentence for
each email.
After preprocessing we analyzed the distribution of email length for this data set.
As plotted in Figure 6, we conducted the Empirical Distribution analysis, the Kernel
Density analysis, and the Histogram analysis. These diagrams show that most of the
emails inside this dataset are of length less than 11 tokens. According to the criteria
6available at http://opennlp.apache.org
ACM Transactions on Information and System Security, Vol. 0, No. 0, Article 0, Publication date: 0.
0:22 Ding et al.
concluded in [Burrows 2007], at least 1000 emails per author are required to guarantee
a good identification result. This introduces a great challenge to authorship identifica-
tion solutions when it comes to a context with a small number of writing samples. For
the length distribution, emails of length ranging from 1 to 26 tokens comprise 50% of
the total, emails of length ranging from 1 to 55 tokens comprise 75% of the total, and
99% of the total are emails of length ranging from 1 to 320 tokens.
In order to systematically test our approach, we designed two experiments: a con-
trolled experiment and a stratified randomized sampling experiment. The first experi-
ment is to evaluate the performance of our approach under different authorship attri-
bution contexts and to evaluate its performance degradation as the available informa-
tion systematically degrades. The second experiment is to simulate the real authorship
identification scenario, where emails of varying lengths are sampled for each candidate
author, and, in most cases, the size of known-author writing samples is unbalanced.
The authorship attribution problem can be regarded as a multi-class text classifica-
tion problem: we classify the anonymous snippet into a set of predefined classes (i.e.,
candidate authors) based on the known samples from each class (i.e., known-author
writing samples). We evaluate our approach with respect to the classification accuracy,
which indicates the percentage of anonymous snippets that are correctly classified.
For all the experiments described below, we adopt the 10-fold cross validation test,
where the emails for each author are split into 10 groups. For a total of 10 iterations,
each is used as a validation set exactly once (used as anonymous samples) and the
remaining 9 groups are used as known author samples. The final accuracy measure is
the average of accuracy values of these 10 iterations.
4.2. Controlled Experiment
In this experiment, we sampled documents randomly multiple times under controlled
conditions and systematically tested our approach with respect to its identification ac-
curacy. First, based on previous work, we identified the three most critical factors that
significantly affect authorship attribution performance: the size of known-author writ-
ings, the size of the candidate set, and the document length. We counted the document
length with respect to the number of tokens that it had. The size of known-author
writings is measured by the number of documents (i.e., emails). We did not break a
complete email or reconstruct an email by concatenation. The following are the se-
lected factors and their selected value intervals:
— The distribution of the email length naturally leads us to conduct experiments on
three different levels: emails of length 1-26 tokens (50%), emails of length 27-55 to-
kens (25%), and emails of length 56-320 tokens (24%).
— For the size of samples for each author, we selected 20, 40, 80, and 120 emails.
— For the size of candidate set, we chose the typical values: 2, 5, 10, 20 authors.
Since each candidate author is regarded as a class in a classification problem, it has
its own accuracy value (number of samples that are identified correctly) during the 10-
fold validation. In this case, because each author has the same controlled number of
known writing samples, our problem can be attributed to the balanced-class classifica-
tion problem. Hence, we only adopted the Macro Average [Savoy 2012] to calculate the
overall accuracy value in each round. Macro average accuracy is simply the average of
all accuracy, where all the classes are equally weighted.
By controlling the combination of the aforementioned conditions, we conducted three
tests. The first one was conducted by isolating each event in order to systematically test
the difference between the events with respect to their identifying accuracy. The second
one was conducted by employing the complete VEA approach in Section 3 to compare
its performance with other typical approaches. Since our approach of combining events
ACM Transactions on Information and System Security, Vol. 0, No. 0, Article 0, Publication date: 0.
A Visualizable Evidence-driven Approach for Authorship Attribution 0:23
50
100
0
10
20
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Num of SamplesNum of Candidates
All emails of length 27-55 tokens
50
100
0
10
20
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Num of SamplesNum of Candidates
Id
en
ti
fic
at
io
n
A
cc
ur
ac
y
(M
ac
ro
A
vg
)
All emails of length 1-26 tokens
50
100
0
10
20
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Num of SamplesNum of Candidates
Id
en
ti
fic
at
io
n
A
cc
ur
ac
y
(M
ac
ro
A
vg
)
All emails of length 56-320 tokens
For all three diagrams:
the upper surface:
Lexical n−gram
the intermediate surface:
Character n−gram
the lowest surface:
Syntatic n−gram
0.2
0.4
0.6
0.8
1
Fig. 7. Performance comparison between isolated events. For all the diagrams, the upper surface is lexical
n-gram event, the intermediate surface is character n-gram event and the lowest surface is POS n-gram
event.
(i.e., linguistic modalities) can be attributed as an ensemble method, we also compared
our approach with the typical voting ensemble method.
Figure 7 shows the experimental result of test one, in which each event is tested in
an isolate manner by employing Algorithm 2. In each diagram, the Num of Candidates
axis represents the size of candidate authors, and the Num of Samples axis represents
the size of samples for each author in the 10-fold validation. The z axis indicates the
macro average accuracy under the given values of x and y. Also, the colour of the gra-
dient surface indicates the accuracy value: the brighter the colour, the higher accuracy
value the point has. For all three diagrams in this figure, the upper surface is the
event for lexical n-gram, which means it achieves the best identifying accuracy across
all given conditions, and the intermediate surface is the event character n-gram, also
on the bottom, the lowest surface is for the event Part-Of-Speech n-gram.
ACM Transactions on Information and System Security, Vol. 0, No. 0, Article 0, Publication date: 0.
0:24 Ding et al.
50
100
0
10
20
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Num of SamplesNum of Candidates
All emails of length 27-55 tokens
50
100
0
10
20
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Num of SamplesNum of Candidates
Id
en
ti
fic
at
io
n
A
cc
ur
ac
y
(M
ac
ro
A
vg
)
All emails of length 1-26 tokens
50
100
0
10
20
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Num of SamplesNum of Candidates
Id
en
ti
fic
at
io
n
A
cc
ur
ac
y
(M
ac
ro
A
vg
)
All emails of length 56-320 tokens
For all three diagrams:
the upper surface:
our approach
the intermediate surface:
SVM (2302 features)
the lowest surface:
J48 (2302 features)
0.2
0.4
0.6
0.8
1
Fig. 8. Performance comparison between approaches. For all the diagrams, the upper surface is VEA, the
intermediate surface is the stylometric j48 and the intermediate surface is the stylometric SVM.
The three diagrams in Figure 7 show that as the available information decreases
in the identification context, the identification accuracy for all isolated events drops
significantly. Lexical n-gram performs the best across all the given conditions, but it is
significantly affected by the length of the given anonymous document, while the POS
n-gram event appears to suffer less from this condition even though it achieves at most
around 80% accuracy. Also, as the size of candidate increases, performance of the event
Lexical n-gram appears to drop more slowly than the other two surfaces.
This result indicates that evidence of different linguistic modalities has different
degrees of sensitivity to the conditions of the given investigation scenario. Hence, for
a confidence estimation task, where a confidence value is part of the identification
result implying how reliable this result is, a distinct model should be built for each
linguistic modality. Also, when combining evidence from these modalities, they should
be weighted accordingly.
ACM Transactions on Information and System Security, Vol. 0, No. 0, Article 0, Publication date: 0.
A Visualizable Evidence-driven Approach for Authorship Attribution 0:25
Table IV. Employed features for SVM and J48 (2302 features in total).
Feature type Features Count Example
Static feature
Lexical features 105 Ratio of digits and vocabularyrichness, etc.
Function words 150 Occurrence of after
Punctuation marks 9 Occurrences of punctuation !
Structural features 15 Presence/absence of greetings
Domain-specific features 13 Occurrences of word contract,time, and draft, etc.
Gender-preferential features 10 Ratio of words ending with ful
Dynamic feature
Top 2000 word n-grams, character
n-grams, and Part-of-Speech n-grams
ranked by the occurring frequency
2000 ‘It is noticed’, ‘notic’, ‘tice’, and‘PRP VBZ VBN’, etc.
Figure 8 shows the experimental results of the second test. In this experiment we
compare the performance of VEA to the other two typical stylometric techniques. The
selected stylometric feature set of these two approaches consists of 2302 stylometric
features, as shown in Table IV. The first 302 static features are used and discussed in
[Iqbal et al. 2013]. We also included the top 2000 n-grams ranked by their occurring
frequency. n ranges from 1 to 4. Previous AA research already experimentally demon-
strated that frequency value carries enough stylistic information and outperform the
information gain scheme [Stamatatos 2009]. We also tried information gain for feature
selection but we did not notice significant different in their performance. As we are not
comparing which feature selection scheme is better, here we only show the result us-
ing frequency. Two attribution techniques were selected here for comparison: SVM and
j48, which demonstrated the most comparable performance in [Iqbal et al. 2013]. We
choose the libSVM [Chang and Lin 2011] for SVM implementation and J48 decision
tree C4.5 implementation in weka7.
As indicated in Figure 8, which is the same diagram representation used in Figure 7,
our VEA approach consistently outperforms the other two typical approaches. Even
though the given anonymous document is only of length 1-26, it can still achieve more
than 85% accuracy in a two-candidate scenario. Also, as the diagrams show, our VEA
approach is more robust against information drops with respect to the candidate size
and the available known-author samples.
In the third test we compared our VEA approach with an ensemble method based
on voting, an ensemble method combining events using the classification error rate
and the lexical n-gram-event-only approach. The classification error rate is collected
by conducting a 10-fold cross validation test on the known-author writing samples.
Thus each of the known-author writing samples is tested exactly once. We use similar
equations in AdaBoost [Freund and Schapire 1995] to derive the weight for each event.
Firstly the error rate is calculated using the first equation in Equation 13, and Xdoci =
1 if the given event incorrectly classify document doci, otherwise Xdoci = 0. Then the
weight of event evn is calculated using the second equation in Equation 13. If the
weight is less than zero we treat it as zero (i.e., if the event cannot correctly classify
50% of the samples then the weight will be zero). Finally each classifier is combined to
derive final prediction by using weighted voting.
The experimental result is shown in Figure 9. The Y axis represents Macro Average
accuracy and the X axis stands for the combination of conditions. For example, ‘2-120’
stands for 2 candidate authors, each of whom has 120 writing samples. As the dia-
gram illustrates, our VEA approach promotes the identifying accuracy and performs
better than all the others, especially when the given documents are short. It always
7http://www.cs.waikato.ac.nz/ml/weka/
ACM Transactions on Information and System Security, Vol. 0, No. 0, Article 0, Publication date: 0.
0:26 Ding et al.
0.5
0.55
0.6
0.65
0.7
0.75
0.8
0.85
0.9
2−
12
0
2−
80
2−
40
5−
12
0
5−
80
5−
40
10
−1
20
10
−8
0
10
−4
0
20
−1
20
20
−8
0
20
−4
0
Condition
A
cc
u
ra
cy
Document Length 1−26
0.65
0.7
0.75
0.8
0.85
0.9
0.95
2−
12
0
2−
80
2−
40
5−
12
0
5−
80
5−
40
10
−1
20
10
−8
0
10
−4
0
20
−1
20
20
−8
0
20
−4
0
Conditions
A
cc
u
ra
cy
error rate lexical ngram VEA voting
Document Length 27−55
0.8
0.85
0.9
0.95
2−
12
0
2−
80
2−
40
5−
12
0
5−
80
5−
40
10
−1
20
10
−8
0
10
−4
0
20
−1
20
20
−8
0
20
−4
0
Conditions
A
cc
u
ra
cy
Document Length 56−320
Fig. 9. Performance comparison between VEA, voting ensemble, and lexical n-gram event. Axis x represents
the combination of condition (e.g., 2-120 represents a scenario with 2 candidates and 120 emails for each of
them). Part of the line for the series error rate is omitted as it is below than the lower bound of other series.
outperforms the voting ensemble approach, and it performs better than the pure lexi-
cal n-gram approach, except in 3 scenarios. By using the pair-wise t-test, it also turns
out that the VEA approach out-performs the others.
error(evn) =
1
10× |M |
M∑
doci
Xdoci weightevn = log
1− error(evn)
error(evn)
(13)
4.3. Stratified Randomized Sampling Experiment
In this section, we describe the second experiment. In order to simulate the actual
authorship identification task, we conducted the stratified randomized experiment,
where the sample size for each author is unbalanced and the variant in document
length of the samples is much larger. In this experiment, the number of emails that
we randomly sampled (without replacement) for each candidate depends on how many
emails this candidate actually has in the whole dataset. We also manually examine
and conduct preprocessing steps for each email with respect to its identity-related
information to avoid the explanation that the high accuracy is simply attributed to the
capture of identity-related information rather than the writing style.
Both the Macro Average and Micro Average accuracy measures are employed in this
experiment. As mentioned above, macro average is simply the average of accuracy
value from each author (i.e., class in classification problem). Micro Average accuracy
employs the confusion matrix to calculate the accuracy value for multi-class classifica-
tion [Savoy 2012]. Typically Micro Average will yield better results in an unbalanced
classification problem because it gives more weight to the class that has more samples.
For example, in a 2-class classification problem, if for the first class 1 sample is cor-
rectly classified out of 10, and for the second class 19 are correctly classified out of 20,
the Macro Average accuracy is simply (1/10 + 19/20)/2 = 0.525 but the Micro Average
is (1 + 19)/(10 + 20) = 0.667.
The experimental result is shown in Figure 10. The labels on the x axis indicate the
given scenario. For instance, ‘2a’ means a stratified sampling on two random authors
while ‘2b’ is another stratified sampling on two random authors. The y axis represents
ACM Transactions on Information and System Security, Vol. 0, No. 0, Article 0, Publication date: 0.
A Visualizable Evidence-driven Approach for Authorship Attribution 0:27
0.75
0.8
0.85
0.9
0.95
1
2a 2b 2
c
2d 2
e 2f 2g 2h 2
i 2j 3a 3b 3
c
3d 3
e 3f 4a 4b 4
c
4d 4
e 5a 5b 5
c
5d 5
e 6a 6b 6
c
6d 7
a
7b 7
c
7d
Di erent cases of strati"ed sampling based on the given number of candidates
A
cc
u
ra
cy
Macro−Avg Micro−Avg
Fig. 10. Performance of VEA on unbalanced-class problem. Axis x represents the case of sampling (e.g., 2a
is a sampling with two authors while 2b is another sampling with two different authors).
Table V. Confidence estimation result.
Variable Coefficient z value Pr(>| z |)
scoreavg 1.204e+ 01 7.429 1.10e− 13
scoremax −4.234e+ 00 −5.747 9.07e− 09
scoremin −7.368e+ 00 −4.333 1.47e− 05
distmax−runnerup 2.032e+ 00 4.818 1.45e− 06
testlength 4.775e− 04 5.004 5.63e− 07
tokenscommon 5.811e− 04 4.378 1.20e− 05
MAE : 0.057536618 R2 : 0.90564199
the accuracy value, and two serials in the diagram respectively stand for the Macro
Average and the Micro Average. As shown in this diagram, our VEA approach can still
handle unbalanced class problems and achieve good identifying accuracy with respect
to both Macro Average and Micro Average.
4.4. Confidence Estimation
In this section, we present our confidence estimation results. To verify how well our
selected features can model the identification precision value, we first collected the
input samples for building the estimation model from all previous runs of the VEA ap-
proach in the above experiments. Specifically, these samples were collected from Line
23 in Algorithm 3 based on the features in Table V. These samples have been padded
with the prediction precision on test set (see Line 17-20 in Algorithm 3). This test is
to evaluate whether the features we selected can model the output precision value.
The regression modeling result is on the first 6 rows in Table V, which includes the
estimated coefficients and the standard z-test for each coefficient. In this table, all the
z values indicate that our selected features all significantly affect the target precision
attribute. Note that the gap statistic distmr [Koppel et al. 2006] does affect the predic-
tion result but the distribution related features in scores (i.e., socreavg, scoremax) play
relatively more important and stable roles.
MAE =
1
|AllTest|
|AllTest|∑
i
|EstimatedConfidencei − ClassificationPrecisioni| (14)
ACM Transactions on Information and System Security, Vol. 0, No. 0, Article 0, Publication date: 0.
0:28 Ding et al.
Also, in order to verify whether our estimation model can actually predict the ac-
curacy value of the unseen data (unseen scenarios), we collect all the estimated confi-
dence values from VEA in all of the above experimental runs. Specifically, these pre-
dicted values come from Line 19 in Algorithm 5. We also gather the corresponding
actual accuracy value in the testing phase in all our 10-fold cross-validation experi-
ments. By comparing these predicted accuracy values and actual accuracy values, its
performance on the unseen data can be evaluated. Both Mean Absolute Error and R2
statistics are shown in the last row in Table V. The MAE value, calculated using Equa-
tion 14, indicates that on average our predicted confidence value has a 5% difference
to the actual accuracy value, and the R2, which closes to 1, indicates good prediction.
5. CONCLUSIONS
In this paper, we present our Visualizable Evidence-driven Approach (VEA) for the
authorship attribution problem. To facilitate its interpretability and explainability, it
is designed according to the EEDI (End-to-End Digital Investigation) framework and
it is able to visualize and corroborate the linguistic evidence supporting our output
attribution results. Also, we conducted comprehensive experiments to fully evaluate
our VEA approach and have shown that it can achieve state-of-art authorship attri-
bution accuracy. We have noticed the scalability issues of this method; when dealing
with a scenario with more than 20 candidates, it is more suitable to identify a small
subset of candidates using other scalable methods, and after that employ our method
to construct cumulative visualized evidence.
Our future research will focus on the following directions. Firstly, to evaluate the per-
formance of the proposed approach regarding the precision-vs-recall measure (e.g., F-
measure and ROC cure) on the open-set authorship problem, systematic experiments
are required to be carefully designed and conducted. Secondly, tremendous works al-
ready existed in the literature of machine learning for classifier combination, and we
will explore deeper for higher accuracy in the future. Also the proposed visualization
scheme is not very applicable for long anonymous document. One possible solution is
to remove n-grams that share similar lightness among hypotheses.
ACKNOWLEDGMENTS
The authors would like to thank the reviewers and the editor for the thorough reviews and valuable com-
ments, which significantly improve the quality of this article. The research is supported in part by NSERC
through Discovery Grants (356065-2013).
REFERENCES
Ahmed Abbasi and Hsinchun Chen. 2006. Visualizing authorship for identification. In Proceedings of the
IEEE International Conference on Intelligence and Security Informatics (ISI) (Lecture Notes in Computer
Science (LNCS)), Vol. 3975. Springer.
Ahmed Abbasi and Hsinchun Chen. 2008. Writeprints: A stylometric approach to identity-level identification
and similarity detection in cyberspace. ACM Transactions on Information Systems (TOIS) 26, 2 (2008).
Seymour Bosworth, Michel E. Kabay, and Eric Whyne. 2012. Computer Security Handbook. Wiley.
Michael Brennan, Sadia Afroz, and Rachel Greenstadt. 2012. Adversarial stylometry: Circumventing au-
thorship recognition to preserve privacy and anonymity. ACM Transactions on Information and System
Security (TISSEC) 15, 3 (2012).
John Burrows. 2007. All the way through: testing for authorship in different frequency strata. Literary and
Linguistic Computing 22, 1 (2007).
Tantek Çelik, Chris Lilley, and L David Baron. 2012. CSS Color Module Level 3. (2012).
Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM: a library for support vector machines. ACM Transac-
tions on Intelligent Systems and Technology (TIST) 2, 3 (2011).
Marco Cristani, Giorgio Roffo, Cristina Segalin, Loris Bazzani, Alessandro Vinciarelli, and Vittorio Murino.
2012. Conversationally-inspired stylometric features for authorship attribution in instant messaging.
In Proceedings of the 20th ACM International Conference on Multimedia. ACM.
ACM Transactions on Information and System Security, Vol. 0, No. 0, Article 0, Publication date: 0.
A Visualizable Evidence-driven Approach for Authorship Attribution 0:29
Walter Daelemans. 2013. Explanation in computational stylometry. In Proceedings of the 14th International
Conference on Computational Linguistics and Intelligent Text Processing (CICLing) (Lecture Notes in
Computer Science (LNCS)), Vol. 7817. Springer.
Kai-Bo Duan and S Sathiya Keerthi. 2005. Which is the best multiclass SVM method? An empirical study.
In Proceedings of the 6th International Workshop on Multiple Classifier Systems (MCS) (Lecture Notes
in Computer Science (LNCS)), Vol. 3541. Springer.
Hugo Jair Escalante, Thamar Solorio, and Montes-y-Gómez. 2011. Local histograms of character ngrams for
authorship attribution. In Proceedings of the 49th Annual Meeting of the Association for Computational
Linguistics: Human Language Technologies. Association for Computational Linguistics.
Yoav Freund and Robert E Schapire. 1995. A desicion-theoretic generalization of on-line learning and an
application to boosting. In Computational learning theory. Springer.
Benjamin C.M. Fung, Ke Wang, and Martin Ester. 2003. Hierarchical document clustering using frequent
itemsets. In Proceedings of the 3rd SIAM International Conference on Data Mining (SDM). SIAM.
Hans Van Halteren. 2007. Author verification by linguistic profiling: An exploration of the parameter space.
ACM Transactions on Speech and Language Processing (TSLP) 4, 1 (2007).
Mark Harrower and Cynthia A Brewer. 2003. ColorBrewer.org: an online tool for selecting colour schemes
for maps. The Cartographic Journal 40, 1 (2003).
Steffen Hedegaard and Jakob Grue Simonsen. 2011. Lost in translation: Authorship attribution using frame
semantics. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:
Human Language Technologies. Association for Computational Linguistics.
David I Holmes. 1994. Authorship attribution. Computers and the Humanities 28, 2 (1994).
David I Holmes. 1998. The evolution of stylometry in humanities scholarship. Literary and Linguistic Com-
puting 13, 3 (1998).
John Houvardas and Efstathios Stamatatos. 2006. N-gram feature selection for authorship identification.
In Proceedings of the 12th International Conference on Artificial Intelligence: Methodology, Systems, and
Applications (AIMSA) (Lecture Notes in Computer Science (LNCS)), Vol. 4183. Springer.
Farkhund Iqbal, Hamad Binsalleeh, Benjamin Fung, and Mourad Debbabi. 2013. A unified data mining so-
lution for authorship analysis in anonymous textual communications. Information Sciences 231 (2013).
Patrick Juola. 2006. Authorship attribution. Foundations and Trends in Information Retrieval 1, 3 (2006).
Patrick Juola. 2012. Detecting stylistic deception. In Proceedings of the Workshop on Computational Ap-
proaches to Deception Detection. Association for Computational Linguistics.
Patrick Juola and Darren Vescovi. 2010. Empirical evaluation of authorship obfuscation using JGAAP. In
Proceedings of the 3rd ACM Workshop on Artificial Intelligence and Security. ACM.
Gary Kacmarcik and Michael Gamon. 2006. Obfuscating document stylometry to preserve author
anonymity. In Proceedings of the COLING/ACL on Main Conference Poster Sessions. Association for
Computational Linguistics.
Sangkyum Kim, Hyungsul Kim, Tim Weninger, Jiawei Han, and Hyun Duk Kim. 2011. Authorship classifi-
cation: a discriminative syntactic tree mining approach. In Proceedings of the 34th International ACM
SIGIR Conference on Research and Development in Information Retrieval. ACM.
Josef Kittler, Mohamad Hatef, Robert PW Duin, and Jiri Matas. 1998. On combining classifiers. IEEE Trans-
actions on Pattern Analysis and Machine Intelligence 20, 3 (1998).
Moshe Koppel, Jonathan Schler, and Shlomo Argamon. 2006. Authorship attribution with thousands of
candidate authors. In Proceedings of the 29th annual International ACM SIGIR Conference on Research
and Development in Information Retrieval. ACM.
Moshe Koppel, Jonathan Schler, and Shlomo Argamon. 2011. Authorship attribution in the wild. Language
Resources and Evaluation 45, 1 (2011).
Moshe Koppel, Jonathan Schler, Shlomo Argamon, and Yaron Winter. 2012. The “Fundamental Problem” of
Authorship Attribution. English Studies 93, 3 (2012).
Moshe Koppel, Jonathan Schler, and Elisheva Bonchek-Dokow. 2007. Measuring Differentiability: Unmask-
ing Pseudonymous Authors. Journal of Machine Learning Research 8 (2007).
Ioannis Kourtis and Efstathios Stamatatos. 2011. Author identification using semi-supervised learning. In
Proceedings of the 2011 CLEF Conference on Multilingual and Multimodal Information Access Evalua-
tion (Lab and Workshop Notebook Papers).
Maarten Lambers and Cor J Veenman. 2009. Forensic authorship attribution using compression distances
to prototypes. In Proceedings of the 3rd International Workshop on Computational Forensics (IWCF)
(Lecture Notes in Computer Science (LNCS)), Vol. 5718. Springer.
Robert Layton, Paul Andrew Watters, and Richard Dazeley. 2013. Automated unsupervised authorship anal-
ysis using evidence accumulation clustering. Natural Language Engineering 19, 1 (2013).
ACM Transactions on Information and System Security, Vol. 0, No. 0, Article 0, Publication date: 0.
0:30 Ding et al.
Vladimir I Levenshtein. 1966. Binary codes capable of correcting deletions, insertions and reversals. Soviet
Physics Doklady 10 (1966).
Kim Luyckx and Walter Daelemans. 2011. The effect of author set size and data size in authorship attribu-
tion. Literary and Linguistic Computing 26, 1 (2011).
Justin Martineau, Tim Finin, Anupam Joshi, and Shamit Patel. 2009. Improving binary classification on text
problems using differential word features. In Proceedings of the 18th ACM Conference on Information
and Knowledge Management. ACM.
Frederick Mosteller and David Wallace. 1964. Inference and disputed authorship: The Federalist. Addison-
Wesley.
Arvind Narayanan, Hristo Paskov, Neil Zhenqiang Gong, John Bethencourt, Emil Stefanov, Eui
Chul Richard Shin, and Dawn Song. 2012. On the feasibility of internet-scale author identification.
In Proceedings of the 2012 IEEE Symposium on Security and Privacy (SP).
Fuchun Peng, Dale Schuurmans, Shaojun Wang, and Vlado Keselj. 2003. Language independent authorship
attribution with character level n-grams. In Proceedings of the 10th conference on European chapter of
the Association for Computational Linguistics-Volume 1. Association for Computational Linguistics.
Sindhu Raghavan, Adriana Kovashka, and Raymond Mooney. 2010. Authorship attribution using proba-
bilistic context-free grammars. In Proceedings of the ACL 2010 Conference Short Papers. Association for
Computational Linguistics.
Gerard Salton and Christopher Buckley. 1988. Term-weighting approaches in automatic text retrieval. In-
formation Processing & Management 24, 5 (1988).
Conrad Sanderson and Simon Guenter. 2006. Short text authorship attribution via sequence kernels,
Markov Chains and Author Unmasking: An Investigation. In Proceedings of the 2006 Conference on
Empirical Methods in Natural Language Processing. Association for Computational Linguistics.
Upendra Sapkota, Thamar Solorio, Manuel Montes-y-Gómez, and Paolo Rosso. 2013. The Use of orthogonal
similarity relations in the prediction of authorship. In Proceedings of the 14th International Conference
on Computational Linguistics and Intelligent Text Processing (CICLing). Lecture Notes in Computer
Science (LNCS), Vol. 7817. Springer Berlin Heidelberg.
Jacques Savoy. 2012. Authorship attribution based on specific vocabulary. ACM Transactions on Information
Systems (TOIS) 30, 2 (2012).
Yanir Seroussi, Fabian Bohnert, and Ingrid Zukerman. 2012. Authorship attribution with author-aware
topic models. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguis-
tics: Short Papers-Volume 2. Association for Computational Linguistics.
Yanir Seroussi, Ingrid Zukerman, and Fabian Bohnert. 2011. Authorship attribution with latent Dirichlet
allocation. In Proceedings of the 15th Conference on Computational Natural Language Learning. Asso-
ciation for Computational Linguistics.
Jitesh Shetty and Jafar Adibi. 2004. The Enron email dataset database schema and brief statistical report.
Information Sciences Institute Technical Report, University of Southern California 4 (2004).
Grigori Sidorov, Francisco Velasquez, Efstathios Stamatatos, Alexander Gelbukh, and Liliana Chanona-
Hernández. 2013. Syntactic dependency-based n-grams: More evidence of usefulness in classification.
In Proceedings of the 14th International Conference on Computational Linguistics and Intelligent Text
Processing (CICLing) (Lecture Notes in Computer Science (LNCS)), Vol. 7816. Springer.
Lawrence M Solan. 2013. Intuition versus algorithm: The case of forensic authorship Attribution. Brooklyn
Journal of Law and Policy 21, 551 (2013).
Thamar Solorio, Sangita Pillay, Sindhu Raghavan, and Manuel Montes-y-Gómez. 2011. Modality specific
meta features for authorship attribution in web forum posts.. In Proceedings of the 5th International
Joint Conference on Natural Language Processing. Asian Federation of Natural Language Processing.
Efstathios Stamatatos. 2009. A survey of modern authorship attribution methods. Journal of the American
Society for Information Science and Technology 60, 3 (2009).
Fiona J Tweedie, Sameer Singh, and David I Holmes. 1996. Neural network applications in stylometry: The
Federalist Papers. Computers and the Humanities 30, 1 (1996).
Yiming Yang and Jan O. Pedersen. 1997. A Comparative study on feature selection in text categorization. In
Proceedings of the 14th International Conference on Machine Learning (ICML).
Justin Zobel and Alistair Moffat. 1998. Exploring the similarity space. In ACM SIGIR Forum, Vol. 32. ACM.
Received TBD; revised TBD; accepted TBD
ACM Transactions on Information and System Security, Vol. 0, No. 0, Article 0, Publication date: 0.
