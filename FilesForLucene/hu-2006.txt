Stylistic Analysis on Reviews of Humanities Objects 
Xiao HU, J. Stephen DOWNIE, Jin Ha LEE 
University of Illinois at Urbana-Champaign 
 
1. Introduction:  
Reviews on humanities materials provide important complementary information for humanities scholars. Our 
previous work has shown that text mining techniques can help analyze a large amount of humanities reviews written 
by customers among the general public (Hu et al 2006). Customer reviews record how the materials are propagated 
among and consumed by the public, thus serve as a rich resource for obtaining user-generated metadata. In addition 
to customer reviews, there are also professional reviews on humanities objects which are written by experts including 
humanities scholars. These reviews bear professional criticism on the humanities materials, and thus can provide 
additional evidence for humanities scholars in analyzing the materials. Both kinds of reviews offer an opportunity to 
investigate the context in which humanities objects fit. Studying these reviews will help enable more robust 
context-sensitive search against humanities materials. 
 
This study is to examine the differences between the two kinds of reviews and to answer two research questions: 1. 
Are there significant differences between expert reviews and customer reviews? 2. If there are, what are the unique 
patterns in each of the two categories? We conducted preliminary experiments on reviews from amazon.com, the 
largest online store of various humanities materials including books and music. On amazon.com, many book and 
music objects have both “Editorial” reviews and “Customer” reviews. The former are written by editors in 
amazon.com who can be seen as experts while the latter are written by arbitrary users among the general public. 
Since we are particularly interested in humanities objects, we selected two product categories on amazon.com which 
are most relevant to humanities: British classic literature books and classical music. We downloaded both editorial 
and customer reviews in the two categories, and the data statistics are shown in Table 1. It is noteworthy that the 
length of customer reviews is highly variable. Also, customer reviews on classical music have a smaller vocabulary 
than editorial reviews. 
Table 1: Statistics of experiment datasets 
 Literature Books Classical Music 
Review Category Editorial Customer Editorial Customer 
Number of Reviews  1425 1425 1843 1843 
Total number of terms 165,028 332,538 268,244 317,312 
Vocabulary size 18,303 24,991 26,518 24,693  
Mean of review length (terms) 115.81 233.36 145.55 172.17 
Std Dev of review length (terms) 83.13 215.31 77.76 180.68 
 
2. Classification Experiments: 
In order to see if there are significant differences between editorial and customer reviews, we built a Naïve Bayesian 
(NB) classification model on the review data sets. For the sake of preserving all the stylistic features in the original 
documents, we only stripped out HTML tags, leaving stopwords, punctuations and upper/lower cases untouched in 
the data preprocessing step. The results of classification experiments with 10 fold cross-validation are shown in Table 
2. Average accuracy of about 86% is a strong indication that the two categories are very different.  
Table 2: NB classification on editorial and customer reviews 
 Literature Books Classical Music 
Mean of accuracy  86.71% 85.97% 
Std Dev of accuracy 1.90% 1.65% 
 
As suggested in previous studies on stylistics (e.g. Stamatatos et al 2002, Argamon et al 2003), function words can 
be very useful in differentiating writing styles. We conducted a classification experiment using only function words. 
As shown in Table 3, the classification accuracies are comparable to those using all terms, which verifies that 
function words are good features. 
Table 3: NB classification on editorial and customer reviews using only function words 
 Literature Books Classical Music 
Review Categories Editorial Customer Editorial Customer 
Number of Reviews  1425 1425 1843 1843 
Total number of function words 76,659 180,312 126,543 170,283 
Function word vocabulary size  388 415 394 414 
Mean of Accuracy 83.81% 85.41% 
Std Dev of Accuracy 3.21% 1.60% 
 
3. Feature Analysis:  
This step is to find unique patterns in each kind of reviews. In the case of binary classification, Naïve Bayesian text 
classifiers can rank the terms according to their relative importance in the construction of the categorization model 
(Hu & Downie 2006). As function words are particularly useful, we list in Table 4 the top 10 function words in each 
review category on British literature books. 
Table 4: Top function words in reviews on British classic literature books 
Top function words in editorial reviews (E) Top function words in customer reviews (C) 
Word Frequency in E Frequency in C Word Frequency in E Frequency in C  
eighty 6 0 definitely 0 48 
unto 3 0 maybe 0 46 
million 11 2 i'd 0 45 
eighteenth 39 11 actually 3 127 
hundredth 2 0 won't 1 48 
forty 11 3 i've 4 112 
whence 2 0 i'm 4 108 
seventh 2 0 you're 3 72 
thousand 10 3 that's 4 86 
seventy 4 1 someone 6 97 
 
As we can see from Table 4, numbers, both ordinal and cardinal, are important features for editorial reviews on 
British literature books while important features for customer reviews include: 
1) adverbs: definitely, actually, maybe, possibly, etc.  
2) contractions: I’d, won’t, you’re, that’s, wouldn’t , etc. 
3) terms referring to personal experiences: I’ve, I’m, I, me, myself, my, you, yourself, etc. 
 
For reviews on classical music, we examined all the features including function words, content words and 
punctuations. The top terms in each of the two review categories can be summarized as follows: 
Important features for editorial reviews:  
1) Artist names spelled with proper diacritical characters: e.g. Bartók; Takács  
2) Music terms: e.g. songwriter, bassist, pastiche 
 
Important features for customer reviews:  
1) Variations of artist names using only basic Latin letters: e.g. Bartok; Takacs 
2) Quotations (“&quot;” in XML documents) 
3) Terms referring to personal experiences: e.g. am, me, myself, I 
4) Nonstandard words and marks: e.g.: _ , cd , cds 
 
Some of the observed differences seem very reasonable. Ordinary readers tend to connect humanities objects with 
personal experiences, and like to use informal writing style including contractions and nonstandard words. Both 
experts and common readers refer to artists, but very few readers would bother inputting proper diacritical characters. 
They use basic Latin letters instead. It is interesting to see that common readers use more quotations than experts, 
and when reviewing classic literature, experts like to use numbers.  
 
4. Conclusions and Future Work:  
We conducted classification experiments on editorial/professional reviews and customer reviews on humanities 
objects by applying a Naïve Bayesian (NB) text classifier to a unigram text model. Results show that the two kinds 
of reviews are very different. By using the NB feature ranking method, we found interesting and unique features in 
each kind of reviews. Such features are important in enriching digital humanities repositories and facilitating 
context-sensitive search. In future work, we will examine more features such as HTML tags, Part-of-Speech tags, 
and word classes.   
 
References:  
Argamon S., Koppel M., Fine J., Shimoni A. (2003). Gender, Genre and Writing Style in Formal Written Texts, Text 
23(3) 
X. Hu, J. S. Downie (2006). Stylistics in Customer Reviews of Cultural Objects, In Proceedings of the 2nd SIGIR 
Stylistics for Text Retrieval Workshop
X. Hu, J. S.Downie, M. C. Jones (2006). Criticism Mining: Text Mining Experiments on Book, Movie and 
Music Reviews, In Proceedings of the joint conference of the Association for Literary and Linguistic Computing 
and the Association for Computers and the Humanities 
Stamatatos, E., Fakotakis, N., Kokkinakis, G. (2000). Text Genre Detection Using Common Word Frequencies, 
In Proceedings of 18th International Conference on Computational Linguistics 
 
