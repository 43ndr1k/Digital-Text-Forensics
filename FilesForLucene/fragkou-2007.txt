 
Segmentation of Greek Text by Dynamic Programming  
 
 
Pavlina Fragkou, Vassilios Petridis   
Department of Electrical and Computer Engineering, Faculty of Engineering, Aristotle University of 
Thessaloniki 54124, Greece 
fragou@egnatia.ee.auth.gr, petridis@eng.auth.gr 
Athanassios Kehagias 
Department of Math., Phys., and Computer Sciences, Faculty of Engineering, Aristotle University of 
Thessaloniki 54124, Greece 
kehagiat@ auth.gr 
 
 
Abstract 
 
We introduce a dynamic programming algorithm to 
perform linear segmentation of concatenated texts by 
global minimization of a segmentation cost function 
which consists of: (a) within-segment word similarity 
(expressed in terms of the generalized density of the text 
dotplot) and (b) prior information regarding segment 
length. Our algorithm is evaluated on two Greek text 
collections and proves that it outperforms several other 
algorithms because it performs global optimization of a 
global cost function. 
 
1. Introduction 
 
The text segmentation problem of concatenated texts 
can be stated as follows: given a text which consists of 
several parts (each part dealing with a different subject) it 
is required to find the boundaries between the parts ([1, 3, 
4, 10, 11, 13]).  A starting point to this is the calculation 
of the within segment similarity based on the assumption 
that parts of a text having similar vocabulary are likely to 
belong to a coherent topic segment. While some authors 
have used fairly sophisticated word co-occurrence 
statistics ([3, 4, 5, 13]) some evaluate the similarity 
between all parts of a text ([3, 4, 10, 11, 14]), while others 
only between adjacent sentences ([5, 6]). To penalize 
deviations from the expected segment length several 
methods use the notion of the “length-model” ([6, 10]). 
Dynamic programming is often used in order to calculate 
the globally minimal segmentation cost ([6, 11, 14, 15]). 
Other authors segment their similarity matrix using 
divisive clustering ([3, 4, 12]). 
Among current approaches to text segmentation we 
can distinguish the improvement of the dotplotting 
technique ([16]) and the improvement of Latent Semantic 
Analysis for text segmentation ([2]).  
Text segmentation can be applied to a variety of 
scientific areas such as noun phrase chunking, tutorial 
dialogue segmentation and text summarization. This 
indicates its importance to a number of applications 
especially in the case where highly inflected languages 
such as the Greek language are used.  
Our approach combines elements from several 
previously published text segmentation algorithms. More 
specifically, we perform linear segmentation of 
concatenated texts by minimizing a segmentation cost 
which consists of two parts: (a) within-segment word 
similarity and (b) prior information about segment length. 
The minimization is effected by dynamic programming, 
which guarantees that the globally optimal segmentation 
is obtained. Some of our results have appeared in a 
preliminary version in ([7]).  
 
2. Method and algorithm 
 
2.1. Text representation 
 
       A text consists of words which are organized in 
sentences where segment boundaries occur only at the 
ends of sentences. Consider a text which contains T 
sentences and L distinct words. We define a T x T 
similarity matrix D as follows (s, t = 1, 2,…, T) 
 






=
≠
otherwise 0,
 t, s and dcommon wor aleast at  have t and s if ,1
,tsD         
 
2.2. Dynamic Programming 
 
       A segmentation is a partition of the set {1,2,…,T} 
into K segments (where K is a variable number and K ≤ T) 
of the form of a vector t = ( 0t , 1t , …., Kt ), where 0t , 1t , 
…., Kt are the segment boundaries. Following the 
approach of [6] we use a dynamic programming algorithm 
which decides the locations of the segment boundaries by 
calculating the globally optimal splitting ton the basis of 
a similarity matrix (or a curve), a preferred fragment 
length and a cost function defined. Given a similarity 
matrix D and the parameters µ, σ, γ and r (described in 
the sequel) the dynamic programming algorithm tries to 
find the global minimum of a segmentation cost function 
),,,;( γσµ rtJ  with respect to t.  This optimal segmentation 
specifies not only the optimal positions of the segment 
boundaries but also the optimal number of segments K. 
       The “segmentation cost” function J(t) is defined as 
follows:  
( )
( )∑
∑ ∑Κ
= −
+= +=−














−
⋅−−
⋅
−−
⋅=
⋅−−⋅=
− −
1 1
1 1
,
2
2
1
01
1 1)1(
2
)()1()(),,,;(
κ
γ
σ
µγ
γγγσµ
r
kk
t
ts
t
tt
ts
kk
tt
D
tt
tJtJrtJ
k
k
k
k
 
A “good” segmentation t is characterized by small values 
of )(1 tJ , which indicates small deviation from the expected 
segment length calculated from training data, where 
µ denotes the mean value and σ  the standard deviation.  
A “good” segmentation t is also characterized by high-
density submatrices of D in the numerator of )(0 tJ  which 
indicate strong within-segment similarity corresponding 
to the k-th segment. In the denominator of )(0 tJ , the 
parameter r=2 corresponds to the area of the submatrix. In 
the case r≠2, )(0 tJ  corresponds to a “generalized density” 
which balances the degree of influence of the surface with 
regard to the “information” (i.e. the number of 1’s) 
included in it. Our algorithm can be found in a form of 
pseudocode in [7].  
 
3. Experiments – Results  
 
We present two groups of experiments which use 
Greek texts. We evaluate the algorithm using the 
following three indices: Precision, Recall and 
Beeferman’s KP  metric ([1]). Precision is defined as “the 
number of the estimated segment boundaries which are 
actual segment boundaries” divided by “the number of the 
estimated segment boundaries”. Recall is defined as “the 
number of the estimated segment boundaries which are 
actual segment boundaries” divided by “the number of the 
true segment boundaries”. Beeferman’s metric KP  
measures the proportion of “sentences which are wrongly 
predicted to belong to different segments” or “sentences 
which are wrongly predicted to belong to the same 
segment”. A variation of the KP  measure named 
WindowDiff index was proposed by Pevzer and Hearst 
([9]) and remendies several problems of the KP .  
 
3.1. Greek Texts 
 
While several papers regarding the segmentation of 
English texts have appeared in the literature, we are not 
aware of any similar work regarding Greek texts. 
Furthermore, because Greek is a highly inflected language 
the segmentation problem is harder for Greek. 
For the experiments, we use a text collection 
compiled from a corpus (Stamatatos’corpus1 [12]) 
comprising of text downloaded from the website 
http://tovima.dolnet.gr. Stamatatos et al. constructed a 
corpus collecting texts which includes essays on science, 
culture, history etc from ten different authors, where 30 
texts were selected from each author. Each of the 300 
texts of the collection of articles compiled from this 
newspaper is pre-processed using the POS tagger 
developed by G. Orphanos ([8]) which is based on a 
Lexicon capable of assigning full morphosyntactic 
attributes to 876.000 Greek word forms. For our 
experiments, every noun, verb, adjective or adverb in the 
text was substituted by its lemma, determined by the 
tagger. For those words that their lemma was not 
determined by the tagger, no substitution was made. The 
difference of the two groups of experiments presented in 
the sequel lies in the length of the created segments and 
the number of authors used for the creation of the texts to 
segment, where each text being a concatenation of ten text 
segments. Each author is characterized by her/his 
vocabulary, hence our goal is to segment the text into the 
parts written by the various authors. 
 
3.2. Experiment group 1 
 
The collection of texts used for the first group of 
experiments consists of 6 datasets: Set0, ..., Set5. Each of 
those datasets differ in the number of authors used for the 
generation of the texts to segment, as listed in Table 1. 
For each of the above datasets, we constructed four 
subsets ((3, 11), (3,5), (6,8) and (9,11)), which differ in 
the number of the sentences appearing in each segment. 
The first (second) value in each pair corresponds to the 
                                                
1 The authors would like to thank professor E. Stamatatos for providing 
us the corpus of Greek articles. 
 
smallest (largest) number of sentences which a segment 
may contain, or any number in that range. Hence Set0 
contains 4 subsets: Set01, Set02, Set03 and similarly for 
Set1 to Set5. The datasets Set*1 are the ones which 
correspond to subset (3, 11), the datasets Set*2 are the 
ones which correspond to subset (3,5), and so on. In order 
to construct the segments of each text for each dataset we 
selected each time a random number of consecutive lines 
from a randomly selected text belonging to an author of 
that set. Special attention was paid to the selection of 
appropriate number of lines according to the subset in 
question. 
 
TABLE 1 
LIST OF THE SETS COMPLIED IN THE 1ST GROUP OF EXPERIMENTS 
USING GREEK TEXTS AND THE AUTHOR’S TEXTS USED FOR EACH 
OF THOSE. 
Dataset Authors No. of 
docs 
per set 
Set0 Kiosse, Alachiotis 60 
Set1 Kiosse, Maronitis 60 
Set2 Kiosse, Alachiotis, Maronitis 10 
Set3 Kiosse, Alachiotis, Maronitis, Ploritis 120 
Set4 Kiosse, Alachiotis, Maronitis, Ploritis,Vokos 150 
Set5 All Authors 300 
 
Once we have created a dataset, we split it into a 
training set and a test set and use the training data to 
compute µ, σ and optimalγ and r values then we evaluate 
the algorithm on (previously unseen) test data.  
The calculation of those values is performed 
following a 5-fold procedure for each of the six datasets, 
where the “best” combination of values µ, σ, γ and r i.e. 
the one which yields the minimum KP  value, was 
calculated (we let γ take the 20 values 0.00, 0.01, 0.02, ..., 
0.09, 0.1, 0.2, 0.3, ..., 1.0 and let r take the values 0.33, 
0.5, 0.66, 1). After selecting the appropriate values for the 
four parameters, we run our algorithm on the test data. 
Table 2 lists the values of Precision, Recall and KP  
obtained by our algorithm as well as those of Choi’s and 
Utiyama’s algorithms on the same task averaged over all 
datasets which have segments of same length. It can be 
seen that in all cases our algorithm performs significantly 
better than both Choi’s and Utiyama’s algorithms.  
 
3.3. Experiment group 2 
 
The second group of experiments also uses 
Stamatatos’ collection. We constructed a single dataset 
which contains 200 texts, with every author represented in 
each text. Each text is the concatenation of ten segments. 
For each segment we do the following. 
1. We select randomly an author among the 10, named I. 
2. We select randomly a text (named k) among the 30 
available that belong to the I author. Let Z be the number 
of paragraphs that k-th text contains. 
3. We select randomly a number l (1 ≤ l ≤ Z) 
corresponding to the number of paragraphs that the 
generated segment will contain. 
4. We select randomly a number m (1 ≤ m ≤ Z-l) 
corresponding to the “starting paragraph”. Thus the 
segment contains all the paragraphs of text k starting from 
paragraph m and ending at the paragraph m + l. 
The procedure described above gives texts which are 
longer than the ones used in Experiment Group 1. Hence 
the segmentation task in the current group is more 
difficult than the previous one. Table 3 lists the results we 
obtained using our algorithm and the ones by Choi and 
Utiyama. It can be seen again that our algorithm performs 
better than both Choi’s and Utiyama’s algorithms. 
 
TABLE 2 
 THE PRECISION, RECALL AND KP  VALUES OBTAINED BY ALL 
ALGORITHMS FOR THE 1ST GROUP OF EXPERIMENTS. 
 
Algorithm Dataset Precision Recall KP  
Set*1 (3-11) 64.90% 61.77% 15.69% 
Set*2 (3-5) 85.13% 85.11% 6.45% 
Set*3 (6-8) 90.51% 90.51% 2.54% 
Ours 
Set*4 (9-11) 91.92% 91.92% 1.29% 
Set*1 (3-11) 61.64% 61.66% 18.43% 
Set*2(3-5) 71.70% 71.70% 16.93% 
Set*3 (6-8) 68.29% 68.29% 15.37% 
Choi 
Set*4 (9-11) 66.75% 66.75% 13.93% 
Set*1 (3-11) 64.00% 61.10% 17.37% 
Set*2 (3-5) 70.00% 54.70% 20.79% 
Set*3 (6-8) 75.42% 73.03% 10.84% 
Utiyama & 
Isahara  
Set*4 (9-11) 73.13% 74.29% 8.83% 
 
TABLE 3.  
THE PRECISION, RECALL AND KP  VALUES OBTAINED BY ALL 
ALGORITHMS  FOR THE 2ND GROUP OF EXPERIMENTS. 
Algorithm Precision Recall KP  
Ours 60.60% 57.00% 11.07% 
Choi 44.62% 44.62% 19.44% 
Utiyama & Isahara 56.76% 67.22% 12.28% 
 
4. Conclusions 
 
We have presented a new text segmentation 
algorithm which we applied to the segmentation of Greek 
texts. On both groups of experiments our algorithm 
outperforms Choi’s and Utiyama’s algorithms even in the 
case of texts exhibiting strong variation as far as the 
average length is concerned. Our algorithm is 
characterized by:  
1. Dotplot similarity, which captures global similarities, 
i.e. similarities between every pair of sentences in the 
document. Dotplots have also been used by Choi ([3, 4]), 
Reynar ([11]) and Xiang and Hongyuan ([14]). Hearst’s  
([5]), and Heinonen ([6])’s cost function depends only on 
the similarity of adjacent sentences, hence it is local,  
Utiyama and Isahara’s cost function ([13]) considers 
similarities of all sentences within each segment within-
segment statistics, hence it is “somewhat” global while 
Ponte and Croft ([10]) compute the similarities of all 
sentences which are at most n sentences apart. 
2. Generalized density. We consider sentences similar 
when they share even a single word. However the special 
characteristic in our function (r ≠ 2) greatly improves our 
performance as opposed to [3, 5, 13, 14]. 
3. Length model. A term for the expected length of 
segments has been used by Ponte and Croft ([10]) and 
Heinonen ([6]). However, Choi ([3, 4]), Reynar ([11]), 
among others, do not use a length model. We observed 
that the use of the length model greatly enhances the 
performance of our algorithm. 
4. Dynamic programming effects global optimization of 
the cost function and hence is a very critical factor in the 
success of our algorithm. As far as we know, the only 
other authors who have used dynamic programming are 
Ponte and Croft ([10]), Heinonen ([6]), Xiang ([14]) and, 
implicitly, Utiyama and Isahara ([13]) (their shortest path 
algorithm is actually a dynamic programming algorithm) 
as opposed to Choi ([3, 4]) and Reynar ([11]) who use 
divisive clustering which performs a local optimization. 
5. Training data. Our algorithm depends crucially on the 
availability of training data, for the estimation of the 
parameters µ, σ, γ and r. Training data are also used by 
Choi in contrast to Utiyama and Isahara’s algorithm.  
6. Finally, for the segmentation of Greek texts we should 
not overlook the importance of the POS tagger; if the 
Greek words were not lemmatized, the vocabulary of the 
text collection would increase by an order of magnitude, 
making the segmentation problem much harder. 
We believe that our algorithm outperforms Choi’s 
and Utiyama’s algorithms because it performs global 
optimization of a global cost function. This should be 
compared to the local optimization of global information 
(used by Choi) and the global optimization of local 
information (used by Heinonen).  Bestgen ([2]) proposed 
that our algorithm can benefit from the addition of 
semantic knowledge for capturing semantic relations 
between words appearing in sentences, a challenging step 
especially for the case of Greek texts. 
 
5. References 
 
[1] Beeferman, D., Berger, A. and Lafferty, J. (1999). Statistical 
models for text segmentation. Machine Learning, 34, 177-210. 
[2] Bestgen, Y. (2006). Improving Text Segmentation Using 
Latent Semantic Analysis: A Reanalysis of Choi, Wiemer-
Hastings Deterministic and Moore (2001). Computational 
Linguistics, 1, 5-12. 
[3] Choi, F.Y.Y. (2000). Advances in domain independent linear 
text segmentation. In Proc. of the 1st Meeting of the North 
American Chapter of the ACL, 26-33. 
[4] Choi, F.Y.Y., Wiemer-Hastings, P. & Moore, J. (2001). 
Latent semantic analysis for text segmentation. In Proceedings 
of the 6th Conf. on EMNLP, 109 -117. 
[5] Hearst, M. A. (1994). Multi-paragraph segmentation of 
expository texts. In Proceedings of the 32nd Annual Meeting of 
the ACL, 9-16. 
[6] Heinonen, O. (1998). Optimal Multi-Paragraph Text 
Segmentation by Dynamic Programming. In Proc. of 17th 
COLING -ACL’98, 1484-1486. 
[7] Kehagias, Ath., Nicolaou A., Fragkou P. & Petridis V. 
(2004).Text Segmentation by Product Partition Models and 
Dynamic Programming. Mathematical & Computer Modelling, 
39, 209-217. 
[8] Orphanos, G. & Christodoulakis, D. (1999). Part-of-speech 
disambiguation and unknown word guessing with decision trees. 
In Proc.of EACL’99. 
[9] Pevzner, L. & Hearst, M. (2002). A critique and 
improvement of an evaluation metric for text segmentation. 
Computational Linguistics, 28(1), 19–36. 
[10] Ponte, J. M. & Croft, W. B. (1997). Text segmentation by 
topic. In Proc. of the 1st Europ. Conf. on Research and 
Advanced Technology for Digital Libraries, 120 - 129. 
[11] Reynar, J.C. (1994). An automatic method of finding topic 
boundaries. In Proc. of the 32nd Annual Meeting of the ACl, 
331-333. 
[12] Stamatatos, E., Fakotakis, N. & Kokkinakis, G. (2001). 
Computer-based authorship attribution without lexical measures. 
Computer and the Humanities, Kluwer Academic Publishers, 
35, 193 - 214. 
[13] Utiyama, M., & Isahara, H. (2001). A statistical model for 
domain independent text segmentation. In Proc. of the 9th 
EACL, 491-498. 
[14] Xiang J. & Hongyuan Z. (2003). Domain-independent Text 
Segmentation Using Anisotropic Diffusion and Dynamic 
Programming. In Proc. of the 26th ACM SIGIR Confl. 
[15] Yaari, Y. (1999). Intelligent exploration of expository texts. 
Ph.D. thesis. Bar-Ilan University. 
[16] Ye, N., Zhu, J., Luo, H.,Wang, H. & Zhang, B. (2005). 
Improvement of the dotplotting method for linear text 
segmentation. Natural Language Processing and Knowledge 
Engineering, 636- 641. 
 
