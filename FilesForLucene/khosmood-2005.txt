Proceedings of International Joint Conference on Neural Networks, Montreal, Canada, July 31 - August 4, 2005
0-7803-9048-2/05/$20.00 ©2005 IEEE
 
 
Automatic Source Attribution of Text: A Neural 
Networks Approach 
 
Foaad Khosmood and Franz Kurfess, Ph.D. 
Department of Computer Science 
California Polytechnic State University 
San Luis Obispo, CA 93407 
foaadk@yahoo.com, fkurfess@calpoly.edu 
 
 
 
ABSTRACT 
 
Recent advances in automatic authorship attribution have 
been promising. Relatively new techniques such as N-gram 
analysis have shown important improvements in accuracy [2]. 
Much of the work in this area does remain in the realm of 
statistics best suited for human assistance rather than 
autonomous attribution [6]. While there have been attempts at 
using neural networks in the area in the past, they have been 
extremely limited and problem-specific [7]. This paper addresses 
the latter points by demonstrating a practical and truly 
autonomous attribution process using neural networks. 
Furthermore, we use a word-frequency classification technique 
to demonstrate the feasibility of this process in particular and 
the applications of neural networks to textual analysis in general. 
Key Words: neural networks, computational linguistics, 
authorship attribution, source attribution. 
 
I. INTRODUCTION 
 
We define automatic source attribution as the ability for an 
autonomous process to determine the source of a previously 
unexamined piece of text. A software system designed to 
follow such a process would analyze a set of input corpora, 
and construct a neural network to engage in attribution. It 
would then train the network with the corpora; apply the 
sample texts and determine attribution. For our source 
recognition problem, our system constructs a 5 layer, 420 
Million-connection neural network. It is able to correctly 
attribute sample texts, previously unexamined by the system. 
Specifically, we conduct three sets of experiments to test the 
ability of the system: broad categorization, narrow 
categorization and minimal-sample categorization.  
 
     An automatic source attribution system must be able to 
digest a set of text corpora with known sources in order to 
determine the source or literary originator of a new piece of 
writing. The word “automatic” is meant to emphasize the 
desired absence of human intervention in the attribution 
process. Most of the work in source or authorship attribution 
is currently done with heavy involvement of humans. Even 
many computerized or statistical methods serve merely as 
assistants to human decision makers [6] who still to some 
extent subjectively evaluate the writing. This is true of almost 
all the famous authorship attribution cases. For example some 
statistical methods were used in assisting human specialists in 
determining The Federalist Papers dispute (some claimed by 
both Hamilton and Madison) [7].  
 
     We build on previous experience to make ours a problem-
independent autonomous system. 
 
II. SOURCE VERSUS AUTHORSHIP 
 
     Many of the previous works within the field of computer 
science refer to this area as “authorship attribution.”  
 
     For a variety of reasons, we believe “source attribution” is 
a more accurate description of our experiments. The works of 
different individuals can appear together as part of the same 
unit with the same style and linguistic distinction. Associated 
Press news stories for example may be written by several 
different individuals but they all adhere to the same 
established writing style and may report about the same 
subject or even the same incident. The Bible and technical 
manuals are also examples of distinctive “sources.” 
 
     There are thus multiple factors that constitute “source.” 
Two of the most important ones are originator and subject 
matter. It is important to note that each of these spheres of 
contribution have shifting scopes that depend on other sources 
they are being distinguished from. Originator, for example, 
could mean “Shakespeare,” or “British author” or “English 
language author” depending on what else it’s being compared 
to. Similarly subject could be relatively narrow such as “US 
Foreign Policy in Latin America 1999-2000” or broad like 
“Love” or “Life.”  
2718
 
FIGURE 1, SOURCE 
 
III. ATTRIBUTION STRATEGY 
 
     We use a common-word-frequency strategy to solve the 
attribution problem with neural networks. In this technique, a 
set of non-trivial words that are common to all corpora are 
derived from the set. These words make up the input layer of 
our neural network. The strength of each input signal is 
determined by the frequency of the word’s occurrence in a 
particular corpus. The output layer consists of neurons each 
representing one source.  
 
We design a 5 layer, back-propagation neural network with 
sigmoid activation functions and random initial weights. The 
size of the input and output layers are dynamically 
determined based on the problem set (i.e. the number of 
shared words and the number of corpora.) We fix the size of 
the middle three layers to 100, 50 and 25 neurons respectively.  
 
IV. SYSTEM DESIGN AND TOOLS 
 
The system accepts both a set of corpora and a set of 
“source-less” texts, each to be attributed to one of the corpora. 
The output layer consists of a small number of neurons each 
representing a source. 
 
     Our system is constructed on a Linux machine with X 
Windows support and PDP++ tool kit including CSS scripting 
support. Our program checks a certain directory for newly 
placed folders, each of which are named after one source and 
contain the corpus of training work for that source. The text 
or html files are properly parsed and analyzed for words. We 
used GNU Flex, BASH and a number of GNU tools to derive 
the set of in-common words. We use the PDP++ scripting 
features (CSS) to automatically specify a variable-sized 
neural net, train it and extract results. 
 
 
FIGURE 2, ATTRIBUTION PROCESS 
 
     Our word extractor ignores all lexemes with numerals and 
symbols, all XML style tags and a small set (roughly 40) of 
common English grammatical operators such as “and,” and 
“or.” Some symbols such as quotation mark, single quote and 
hyphen are removed and the surrounding two spaces are 
joined. All capital letters are converted to lower-case. After 
extraction, all repetitions are deleted and another 125 words 
most frequent in the English language, eliminated. The list of 
frequent words is provided by Kenneth Beare and is available 
for download on www.about.com.  
 
     In-common words are determined by examining all 
remaining words in all corpora. These words represent the 
universe of comparison among all the corpora. Each of the 
words has a corresponding input neuron in our network. For 
every word on the list, we determine the frequency in each 
source. Frequency is calculated by dividing the number of 
occurrences by the total number of words post-extraction but 
pre-elimination. The divisor thus will still not include counts 
of symbols and numbers but will likely contain counts of 
frequent English words, as well as repetitions. 
 
     Having derived the necessary information from the 
corpora, the system creates a CSS script that specifies the 5-
layer neural network. Normalized versions of the in-common, 
non-trivial, word frequencies are applied as input for training 
data sets. The corresponding corpus code constitutes the 
output layer.  
2719
V. EXPERIMENTS AND CORPORA 
      
     We begin testing the system with three sets of source 
attribution experiments in mind.  
 
1. Broad categorization experiment: We apply 5 
diverse and well known textual corpora and sample 
texts from each for testing. 
2. Declining sample size experiment: We apply 
successively smaller portions of a sample text to the 
trained system and observe accuracy in attribution. 
3. Narrow categorization experiment: We consider 
categories of a single broad source as separate 
narrow sources and test the system’s ability to 
distinguish sample texts between these new sources.  
 
     For the purposes of the first experiment we select the 
following five sources of text. Table 1 contains a description 
and size of each source. 
 
TABLE 1, SOURCE CORPORA 
SOURCE DESCRIPTION 
NUM. 
WORDS 
Bible (B) American Standard Bible 31,102 
Chomsky (C) 
Select Political Writings 1990-
2004 4,823 
Linux (L) 
Comments in Kernel Source 
Code 20,212 
Poe (P) All poems + "Fall of Usher" 10,362 
Shakespeare 
(S) 
Hamlet, Julius Caesar, 
Othello and R+J. 30,184 
 
VI. SAMPLE TEXTS AND RESULTS 
 
     Our system derives 672 in-common, non-trivial words for 
this corpus set. Our neural network dimensions are thus 
(672x100x50x25x5). The network generally converges within 
about 500 epochs.  We use random initial weights for all 
connections. We measure convergence in our back 
propagation network by when the SSSE (Sum of Sum of 
Squared Errors) of the output neurons becomes insignificant. 
To calculate SSSE, our system subtracts each output neuron 
value from its intended training target (error). The error 
values of all the neurons in the output set are squared (SE) 
and added together (SSE). Finally the process is repeated for 
every distinct output set -which in this case is five- and those 
results are added together (SSSE.) 
 
      We applied the following sample texts (Table 2) for the 
broad categorization experiment. Results were positive in 
every case. 
 
     The confidence value is 100 times the output level of the 
correct output neuron. The neural output value ranges from 0 
to 1 and is inversely related to SSE. 
 
TABLE 2, BROAD CATEGORIZATION RESULTS 
S# DESCRIPTION SRC 
 
WRDS 
CONFI-
DENCE 
1 Bible – entire corpus B 31,102 99.0591% 
2 
Chomsky – entire 
corpus C 4,823 98.6381 
3 Linux - entire corpus L 20,212 99.1525 
4 Poe - entire corpus P 10,362 99.0206 
5 
Shakespeare - entire 
corpus S 30,184 98.9395 
6 
200 line subset of Bible 
corpus B 2,718 93.0880 
7 
1019 line subset of 
Linux corpus L 2,624 91.4564 
8 
260 line subset of Poe 
corpus P 1,193 99.2502 
9 
600 line subset of 
Shakespeare corpus S 1,567 98.2209 
10 
500 lines of King James 
Bible B 2,389 98.0366 
11 
excerpt from 
"Hegemony," Chom '03 C 942 98.2122 
12 Interview in 2004 C 3,204 95.1804 
13 Interview in 1991 C 8,016 87.3992 
14 BLOG entries 2003-4 C 3,651 97.2593 
15 
1000 line Ethernet 
comments L 4,093 96.4314 
16 VFS documentation L 2,084 97.0880 
17 
Tell-Tale Heart and 
Cask of Amontillado P 2,537 99.2060 
18 King Lear S 1,764 98.9652 
19 Mark:5 (New Am. Bible) B 722 72.3709 
 
     Samples 1 through 5 are the actual source corpora. The 
high confidence values of these results stems from the low 
SSSE value and confirm the convergence of the neural 
network [1]. Samples 6 through 9 are small, random, 
contiguous subsets of the corpora and thus are known to the 
system. These results demonstrate our algorithm’s relative 
independence from sample text size in broad categorization. 
The vocabulary usage pattern being recognized by the neural 
net is being exhibited in samples which are about an order of 
magnitude smaller than their respective corpora proper. But 
the system’s confidence in attributing the subsets is not 
necessarily as high as for the corpus. Sample size will be 
explored further below in our declining sample size 
experiment. 
 
     Samples 10 through 19 have not been utilized in the 
training of the network and are thus true tests of our broad 
categorization experiment. The lowest performing piece is the 
last one from the New American Standard Bible which is 
attributed with 72% confidence. Two factors are most likely 
responsible for this score. The first is size. The NASB sample 
has the lowest word-count of all the samples testes. While we 
explore declining sample size with Chomsky corpus below, 
we must keep in mind that the Bible is a broader and less 
homogeneous corpus than Chomsky. Secondly, the NASB 
represents a significant and deliberate linguistic deviation 
2720
from both the American Standard Bible and the King James 
Bible. The NASB translators strived for literalism with 
particular emphasis on original idioms and phraseology in 
original languages. One commentator has described it as 
“more Greek, less English” [9].  
 
VII. DECLINING SAMPLE SIZE EXPERIMENT 
 
     Using the same neural network that produced the above 
results, we applied successively smaller portions of Noam 
Chomsky’s political writing to the system. The piece is the 
first chapter from Chomsky’s 2004 book titled “Hegemony or 
Survival: America’s Quest for Global Dominance” and is 
available for free online. The entire chapter contains 942 
words. We apply the entire chapter (which is attributed with 
high confidence) and 6 other subsets ranging from 942 to 118 
words. Each chapter subset is produced by simply removing a 
paragraph from the end of the previous subset. The results are 
noted in table 3. They are remarkable in their accuracy of 
attribution, despite unusually small sizes. Even a sample of 
118 words was correctly attributed to the Chomsky corpus 
with high confidence. 
 
Confidence in "Hegemony" sample
92.0000%
93.0000%
94.0000%
95.0000%
96.0000%
97.0000%
98.0000%
99.0000%
942 769 722 521 322 194 118
# words
co
nf
id
en
ce
 
FIGURE 3, DECLINING SAMPLE SIZE EXPERIMENT 
 
VIII. NARROW CATEGORIZATION EXPERIMENT 
 
     Our narrow categorization experiment concentrates on 
Shakespeare entirely. We utilize our automated process to 
construct a new neural network and train it with a new set of 
corpora consisting of Shakespeare’s comedies, tragedies and 
sonnets. The system should be able to attribute sample texts 
to each category of Shakespearian writing [3].  
 
TABLE 3. NARROW CATEGORIZATION SOURCES 
SOURCE DESCRIPTION 
NUM. 
WORDS 
Shakespeare’s 
comedies (C) 
Merchant of Venice, 
Midsummer Night’s Dream, 
Taming of the Shrew, 
Twelfth Night 66,366 
Shakespeare’s 
sonnets (S) Sonnets 100 through 150 4,331 
Shakespeare’s 
tragedies (T) 
Hamlet, Julius Caesar, 
Othello, Romeo and Juliet 70,409 
 
     Our process determined 962 common, non-trivial words in 
the three sources of comedies, tragedies and sonnets. A 5 
layer, 360,750,000 connection (962x100x50x25x3), back-
propagation network was created. As in the first two 
experiments, the sigmoid activation function and random 
initial weights were used. No special learning curve or 
acceleration variables were used and all neurons had identical 
thresholds. Once the training data were applied the network 
converged in about 150 epochs on average. 
 
     Since all three corpora are originated from Shakespeare 
and the writing styles are similar, the challenge becomes more 
significant for the neural network. We emphasize that the 
presence of key words, i.e. words inherently associated with 
one corpus, is not helpful in our process. “Romeo” for 
example is associated with a tragedy, but the word will not be 
part of our neural net since it is not shared among comedy, 
tragedy and sonnet corpora. 
 
     Table 4 contains results of the narrow categorization 
problem.  
 
TABLE 4, NARROW CATEGORIZATION RESULTS 
S# DESCRIPTION SRC 
 
WRDS  CONF. 
1 
All’s Well that 
Ends Well C 403 99.5769 % 
2 
Comedy of 
Errors C 1256 99.4556 
3 
Measure for 
Measure C 1125 (98.6647)* 
4 
Much Ado 
About Nothing C 317 (89.6729)* 
5 As You Like It C 137 77.6646 
6 
Antony and 
Cleopatra T 1448 (89.0783)* 
7 King Lear T 1835 (98.1022)* 
8 Macbeth T 495 98.5415 
9 Coriolanus T 921 97.9012 
10 
Titus 
Andronicus T 540 74.9043 
11 
Antony and 
Cleopatra T 18384 97.4471 
12 King Lear T 19126 (73.3908)* 
13 The Tempest C 11733 50.0296 
14 
Marry Wives of 
Windsor C 15960 99.3469 
15 Sonnets 1-10 S 884 97.9331 
16 Sonnets 1-20 S 1788 98.3348 
17 Sonnets 1-60 S 5250 98.3229 
18 Sonnets 1-80 S 6946 98.3157 
19 Sonnets 1-99 S 8612 97.8108 
()* = incorrectly attributed 
 
     The network made five incorrect attributions in the narrow 
categorization problem, clearly indicating a more difficult 
process. In addition, three other cases (samples #5, #10, #13) 
had confidence levels below 78%. Categorizing any of the 
2721
sonnet sets is uncontroversial. The difficulty lies mostly in 
telling the difference between a comedy and a tragedy. Two 
comedy texts, excerpts from Measure for Measure and Much 
Ado about Nothing, are categorized as tragedies. Two 
tragedies, King Lear and Antony and Cleopatra are 
incorrectly categorized as comedies. 
 
     The results in table 4 are from a single training of the 
network, but are representative of about 20 similar runs. With 
minor exceptions, the same five incorrect attributions were 
made each time and the confidence levels of the correct 
attributions were very similar. We can thus eliminate random 
initial weights as a performance factor. 
 
     A closer examination reveals that sample text length has a 
more powerful affect on the process in narrow categorization. 
In the case of Antony and Cleopatra, the network incorrectly 
considered a 1448 word excerpt (sample #6) as a comedy. 
However, an 18,384 word piece of the same play (sample #11) 
was correctly identified as a tragedy. Sample #7 which is a 
small section of King Lear was incorrectly attributed to 
comedy with high confidence. Sample #12, which was the 
entire play was still attributed incorrectly but with a weaker 
confidence level. Text sample size appears to not be a 
conclusive factor in determining attribution, since some 
smaller pieces were attributed correctly. Larger pieces, 
however, are less likely to be unrepresentative of the target 
writing style. 
 
     This particular experiment is further complicated by 
Shakespeare’s writing itself. Shakespearian tragedies often 
contain elements of genuine humor (such as court jesters) or 
“dark” humor which could have the same general linguistic 
signature as comedies. Likewise Shakespearian comedies do 
often contain serious subjects, violence, jealousy and disputes 
more often associated with tragedies. 
 
IX. CONCLUSIONS AND FUTURE WORK 
 
     In the realm of text attribution, we believe a distinction 
between “source” and “authorship” is necessary. Using our 
own criteria we were able to include the Bible and Linux 
comments as sources, even though both sets of corpora have 
multiple authors. Furthermore, one author (like Shakespeare 
or Chomsky) can have more than one style of writing which 
should be detectable.  
 
     Our source attribution experiments proved positive. In the 
broad categorization experiment we were able to show this 
common-word frequency technique works extremely well 
with our sample set. This means that word usage rate by itself 
is a powerful indicator of a source’s style or distinctiveness. 
Lexical approaches such as the one we utilized have an 
advantage over syntactic or natural language processing 
techniques in that they need not access any pre-existing 
knowledge about language structure, form or meaning. Our 
attribution process can be improved even further by including 
other lexical criteria such as sentence-level statistics and word 
proximity matrices. Our neural network-centric approach is 
also compatible with N-gram analysis. Common N-gram 
frequencies could replace common-word frequencies with 
little change to the actual process. Future work will examine 
comparison and cooperation between different lexical 
techniques in source attribution.  
 
     Our declining sample size problem produced surprising 
results. We were not expecting very small samples such as 
118 words to be attributed with such high degree of 
confidence. Although interesting possibilities have been 
raised, no solid conclusion can be drawn as of this point 
without further experimentation. There are a number of other 
dependencies that future work will have to consider. We may 
certainly expect different results with a narrower 
categorization problem. In addition, we will have to run 
experiments with random pieces of small texts in order to 
help eliminate anomalies. 
 
     Although our narrow categorization network yielded some 
false attributions, we were able to categorize 14 of the 19 
pieces successfully. One explanation may be that genre 
specific words, which are typically important clues, were 
eliminated due to our common-word-frequencies technique. 
One interesting experiment would be to combine a “bag of 
words” approach which does value unique words, with 
common-word-frequencies. The use of the neural net 
possibilities here is still promising.  
 
     Reducing the size of the output layer appears to accelerate 
convergence. In experiment 1, the network converged in an 
average of 500 epochs while it was only 150 epochs in 
experiment 3. Distinctiveness of the corpora in this case was 
not a factor in convergence length since the Shakespearian 
corpora in experiment 3 proved less distinctive. More 
experimentation can be done with regards to the size of 
training corpora and number of categories.  
 
     The automated process aspect of our design was 
demonstrated with our software. We now have some 
intertwining scripts and programs that together constitute an 
“attribution machine.” This machine depends on a 
dynamically defined embedded neural net to achieve 
attribution.  
 
     What we’ve demonstrated here is the digitization and 
analysis of one aspect of writing style with neural nets. 
Although, perhaps the most important aspect, word 
frequencies are not the only non-lexical indicators of a piece 
of writing. Other measures such as symbol usage, word, 
sentence and paragraph lengths, and word proximity usage 
are also available. Effectiveness of neural nets in a 
comprehensive source attribution solution will depend on 
2722
how organically one can interlace difference aspects of 
writing into a single connectionist categorization system.  
 
X. ACKNOWLEDGEMENTS 
 
     The authors would like to extend sincere thanks and 
gratitude to Dr. Susan Opava, Susan Rock and the Research 
and Graduate Programs office at the California Polytechnic 
State University for their help and contribution. Furthermore, 
we thank Cal Poly department of Computer Science and 
important feedback provided by professors Gene Fisher, 
Clark Turner and Charles Dana. 
 
XI. REFERENCES 
 
[1] Engelbrecht, Andries P. Computational Intelligence: An 
      Introduction. 2002. John Wiley and Sons.  
      ISBN 0-470-   84870-7. 
[2] Keselj, Vlado and Peng, Fuchun and Cercone, Nick and 
      Thomas, Calvin. N-gram-based Author Profiles for 
      Authorship Attribution. In Proceedings of the Conference 
      Pacific Association for Computational Linguistics, 
      PACLING'03, Dalhousie University, Halifax, 
      Nova Scotia, Canada, August 2003. 
[3] Brett Kessler, B and Nunberg, G., Schuetze H. Automatic 
Text Genre Detection. In Proceedings of the 35th Annual Meeting of the 
Association for Computational. Linguistics and the 8th Meeting of the 
European Chapter of the Association for Computational Linguistics, 
pages 32-38, Morgan Kaufmann Publishers, San Francisco CA, 1997. 
[4] Negnevitsky, Michael. Artificial 
Intelligence. First edition 2002. Addison Wesley. ISBN 0-201-71159-1. 
[5] Srinivas, B and A. Joshi. Supertagging: A 
Approach to Almost Parsing. Computational Linguistics, 25(2) 1999, 
237-265. 
[6] N. Fakotakis E. Stamatatos and G. Kokkinakis. 
Computer-based Authorship Attribution without Lexical Measures. 
Computers and the Humanities, Volume 35, Issue 2, May 2001, pp. 193-
214 
[7] N. Fakotakis E. Stamatatos and G. Kokkinakis. 
Automatic Authorship Attribution. In Proceedings EACL-99. 1999. 
[8] N. Fakotakis E. Stamatatos and G. 
Kokkinakis. 2000. Automatic Text Categorization in Terms of Genre and 
Author. Computational Linguistics, 26(4), 471-495. 
[9] The New American Standard Bible. 
      Bible Researcher. accessed 4/2005. 
      http://www.bible-researcher.com/nasb.html.
 
2723
