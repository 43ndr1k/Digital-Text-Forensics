Stylometric classification of
different translations of the same
text into the same language
............................................................................................................................................................
Michael A. Covington
Institute for Artificial Intelligence, The University of Georgia, GA,
USA
Iris Potter
Linguistics Program, The University of Georgia, GA, USA
Tony Snodgrass
Institute for Artificial Intelligence, The University of Georgia, GA,
USA
.......................................................................................................................................
Abstract
Quantitative stylometry of ten translations of the same Bible passage into English,
followed by Ward clustering, produces a dendrogram that reflects the well-known
history and intent of the translations. We conclude that quantitative stylometry
combined with clustering is a useful tool for reconstructing literary history.
.................................................................................................................................................................................
1 Introduction
This article shows how the literary history and relation-
ships of a set of translations are reflected in quantitative
computer measurements of their style (stylometry).
The translations are of the Bible into English language.
The resulting classification, generated automatically by
statistical clustering, closely matches the known history
and design goals of the various translations.
Translators have distinctive styles, even when
they do not aim to have a distinctive style (Baker
2000; Wang and Li, 2012; Ji 2012; Ji and Oakes,
2012). Distinguishing translators by stylometry is
akin to author identification (Juola 2006; Koppel
et al., 2009; Stamatatos 2009) except that the con-
tent of the translation is dictated by the content of
the original; only the translator’s style is variable.
This eliminates the vexing problem of searching
for words that are topic- or content-invariant.
Indeed, word-frequency–based classification does
not necessarily successfully identify translators
(Rybicki 2012; but cf. Ke 2012). In some ways, the
challenge resembles computational studies of scribal
copying of manuscripts (van Dalen-Oskam 2012)
except that the permissible variation is much larger.
There is no consensus as to what stylometric
properties best indicate authorship (or in our case
translatorship), but sentence length and vocabulary
diversity both have often been advocated (Juola
2006, section 4.2). Because Bible translations have
different stylistic goals, ranging from literary ele-
gance to easy readability, we expect different trans-
lations to prefer a smaller or larger English
vocabulary. Because the original text came from an-
cient manuscripts lacking punctuation, sentence
length is up to the translator to a greater extent
than would be the case with translations from a
modern language. The translator also has some dis-
cretion as to whether to express complex ideas nom-
inally or verbally; this is reflected in propositional
Correspondence:
Michael A. Covington,
Institute for Artificial
Intelligence, The
University of Georgia,
Athens, GA 30602, USA.
Email:
mc@uga.edu
Literary and Linguistic Computing  The Author 2014. Published by Oxford University Press on
behalf of EADH. All rights reserved. For Permissions, please email: journals.permissions@oup.com
1 of 4
doi:10.1093/llc/fqu008
 Literary and Linguistic Computing Advance Access published March 18, 2014
 at A
egean U
niversity on M
arch 24, 2014
http://llc.oxfordjournals.org/
D
ow
nloaded from
 
idea density, a psycholinguistic measurement that
has not heretofore been used stylometrically.
2 Choice and Acquisition of Texts
The Bible was chosen because a large number of
English translations exist, many of them are inter-
related in interesting ways, and abundant historical
and critical studies exist (e.g. Metzger 2001). Within
the Bible, the specific text chosen was the Gospel of
Mark because of its relative simplicity. Chapter 16 of
Mark was omitted because it contains a passage that
is missing from many of the oldest manuscripts and
is omitted from some translations.
With three exceptions, machine-readable texts of
Mark 1–15 were obtained from online sources, spe-
cifically the following (which were verified on 22
August 2012):
English Standard Version (ESV), http://www.
biblestudytools.com/esv/
New King James Version (NKJV), http://www.
biblestudytools.com/nkjv/
Revised Standard Version (RSV), http://www.
biblestudytools.com/rsv/
New Revised Standard Version (NRSV), http://
www.biblestudytools.com/nrs/
New American Standard Bible (NASB), http://
www.biblestudytools.com/nas/
Good News Translation (GNT), http://www.
biblestudytools.com/gnt/
New International Version (NIV), http://www.
biblestudytools.com/niv/
Three translations were not available online and
were scanned and OCRed from printed editions.
They are as follows:
The New English Bible (NEB) with the Apocrypha,
London: Oxford University Press and
Cambridge: Cambridge University Press, 1970.
The Revised English Bible (REB) with the
Apocrypha, New Rochelle, N.Y.: Oxford
University Press and Cambridge: Cambridge
University Press, 1989.
The Jerusalem Bible, Garden City, N.Y.:
Doubleday, 1968.
All texts were manually proofread and cleaned up
to ensure uniform formatting.
Despite its historical importance, the King James
Version (Authorized Version) of 1611, from which
several of these other translations are derived, was
not included because it is not written in present-day
English and is therefore incompatible with one of our
stylometric tools, Computerized Propositional Idea
Density Rater (CPIDR). Small amounts of archaic lan-
guage in the RSV and smaller amounts in other trans-
lations are not believed to have caused a problem.
3 Stylometric Measurements
Table 1 shows the results of stylometric measure-
ments on the ten sample texts.
3.1 Mean sentence length
Mean sentence length for each translation was mea-
sured by computer on the basis of punctuation. The
software used, Georgia Language Analysis Tools, is
not released for use outside our laboratory, but re-
sults are easily replicable.
3.2 Vocabulary diversity (moving-
average type-token ratio)
Vocabulary diversity was measured as moving-aver-
age type-token ratio (MATTR) over a 500-word
window (Covington and McFall, 2010) using the
software that we developed for this purpose
(http://www.ai.uga.edu/caspr).
Unlike a plain type-token ratio, this measure-
ment is independent of the length of the text. A
Table 1 Stylometric measurements of the ten translations
Translation Mean sentence
length
Idea
density
Vocabulary diversity
(MATTR-500)
ESV 18.3898 0.55 0.405
GNT 17.4827 0.53 0.426
Jerusalem 18.2593 0.527 0.43
NASB 22.9231 0.537 0.416
NEB 18.0567 0.557 0.442
NIV 15.2569 0.531 0.436
NKJV 20.1523 0.557 0.408
NRSV 19.9372 0.562 0.42
REB 17.6503 0.555 0.44
RSV 21.5165 0.55 0.407
M. A. Covington et al.
2 of 4 Literary and Linguistic Computing, 2014
 at A
egean U
niversity on M
arch 24, 2014
http://llc.oxfordjournals.org/
D
ow
nloaded from
 
500-word window is placed at all possible positions
in the text, the type-token ratio within the window
is computed, and the results are averaged.
3.3 Idea density
Propositional idea density was measured using
CPIDR 3.2 (Brown et al., 2008; http://www.ai.
uga.edu/caspr). This is a measurement of the
number of propositions (‘ideas’, pieces of infor-
mation that could be true or false) divided by the
number of words. For example, The big dog
barked at night contains three propositions (dog
is big, dog barked, it happened at night) and six
words, for an idea density of 0.5, which is typical
for English. Idea density is a standard measure-
ment used in psycholinguistics (Kintsch 1974),
but its application to stylometry is relatively
new (Covington 2009).
4 Clustering
Ward hierarchical clustering of the measurements
was performed using version 9 of the JMP statistics
package (SAS Institute, Inc., http://www.jmp.com)
and replicated with R (http://www.r-project.org).
That is, a tree diagram was created automatically,
reflecting the similarity or dissimilarity of the trans-
lations based on all three measurements.
Before clustering, the measurements were scaled
to have a mean of 0 and a standard deviation of 1;
otherwise, excessive weight would have been given
to the one with the largest numerical values. The
distance measure used was Euclidean distance.
The resulting tree diagram (dendrogram) is shown
in Fig. 1. The position of the crossbar joining any two
translations or groups reflects how similar they are.
For example, the crossbar joining NEB and REB is
farther to the right (away from the root of the tree)
than the crossbar joining GNT to Jerusalem because
NEB and REB are more similar to each other.
Varying the clustering method, we found that
Ward clustering, complete clustering, and average
clustering produced the same tree structure.
Results changed little when we switched from
Euclidean to Manhattan distance.
5 Results and Interpretation
As noted in our comments at the right-hand edge of
Fig. 1, the clustering algorithm did an impressive
Fig. 1 Dendrogram produced by Ward clustering of the data in Table 1, with our comments added at the right.
Horizontal position of the crossbar indicates similarity; thus, NEB and REB are more similar to each other than GNT
and Jerusalem
Stylometric classification
Literary and Linguistic Computing, 2014 3 of 4
 at A
egean U
niversity on M
arch 24, 2014
http://llc.oxfordjournals.org/
D
ow
nloaded from
 
job of classifying the translations according to their
well-known history and intent.
The ‘standard versions’ that aim to preserve fa-
miliar wording from the King James Version all
came out in a single group. The RSV and NKJV
are both revisions of the King James Version
based on somewhat different choices of ancient
manuscripts (a difference not reflected in the meas-
urements). The ESV and NRSV are revisions of the
RSV. The NASB is the result of a separate project
intended to produce a similar product.
The versions intended for smooth, easy readabil-
ity in English also came out together. The GNT and
Jerusalem Bible, both of which tend toward para-
phrase, were distinguished from the NIV, which is
more literal.
Finally, the New English Bible and its revision,
the Revised English Bible, came out in a class by
themselves. This translation project aimed for ele-
gant, literary British English to a much greater
extent than other translation projects, which are
either American or relatively neutral between
American and British usage.
We conclude from this somewhat unusual set of
examples that quantitative stylometry and clustering
can produce classifications that reflect or reveal lit-
erary history, even when the only literary creativity
involved is translation.
Acknowledgement
An earlier version of this article was presented at the
Southeastern Conference on Linguistics (SECOL),
April 2010. The authors are indebted to Bryan
Whitfield for helpful comments.
References
Baker, M. (2000). Towards a methodology for investigat-
ing the style of a literary translator. Target, 12: 241–66.
Brown, C., Snodgrass, T., Kemper, S. J., Herman, R.,
and Covington, M. A. (2008). Automatic measurement
of propositional idea density from part-of-speech tag-
ging. Behavior Research Methods, 40(2): 540–5.
Covington, M. A. (2009). Idea Density: A Potentially
Useful Characteristic of Retrieved Documents.
Proceedings, IEEE SoutheastCon. Atlanta, GA, March
2009.
Covington, M. A. and McFall, J. D. (2010). Cutting the
Gordian knot: the moving-average type-token ratio
(MATTR). Journal of Quantitative Linguistics, 17:
94–100.
Ji, M. (2012). Hypothesis testing in corpus-based literary
translation studies. In Oakes, M. P. and Ji, M. (eds),
Quantitative Methods in Corpus-based Translation
Studies. Amsterdam: John Benjamins, pp. 53–72.
Ji, M. and Oakes, M. P. (2012). A corpus study of early
English translations of Cao Xueqin’s Hongloumeng. In
Oakes, M. P. and Ji, M. (eds), Quantitative Methods in
Corpus-based Translation Studies. Amsterdam: John
Benjamins, pp. 177–208.
Juola, P. (2006). Authorship attribution. Foundations and
Trends in Information Retrieval, 1(3): 233–334. http:/dx.
doi.org/10.1561/1500000005.
Ke, S. W. (2012). Clustering a translational corpus. In
Oakes, M. P. and Ji, M. (eds), Quantitative Methods
in Corpus-based Translation Studies. Amsterdam: John
Benjamins, pp. 149–74.
Kintsch, W. (1974). The Representation of Meaning in
Memory. Hillsdale, NJ: Lawrence Erlbaum Associates.
Koppel, M., Schler, J., and Shlomo, A. (2009).
Computational methods in authorship attribution.
Journal of the American Society for Information Science
and Technology, 60: 9–26.
Metzger, B. M. (2001). The Bible in Translation: Ancient
and English Versions. Grand Rapids, MI: Baker.
Oakes, M. P. and Ji, M. (2012). Quantitative Methods in
Corpus-based Translation Studies. Amsterdam: John
Benjamins.
Rybicki, J. (2012). The great mystery of the (almost) in-
visible translator. In Oakes, M. P. and Ji, M. (eds),
Quantitative Methods in Corpus-based Translation
Studies. Amsterdam: John Benjamins, pp. 231–48.
Stamatatos, E. (2009). A survey of modern authorship
attribution methods. Journal of the American Society
for Information Science and Technology, 60: 538–56.
van Dalen-Oskam, K. (2012). The secret life of scribes:
exploring fifteen manuscripts of Jacob van Maerlant’s
Scolastica (1271). Literary and Linguistic Computing, 27:
355–72.
Wang, Q. and Li, D. (2012). Looking for translator’s fin-
gerprints: a corpus-based study on Chinese translations
of Ulysses. Literary and Linguistic Computing, 27: 81–93.
M. A. Covington et al.
4 of 4 Literary and Linguistic Computing, 2014
 at A
egean U
niversity on M
arch 24, 2014
http://llc.oxfordjournals.org/
D
ow
nloaded from
 
