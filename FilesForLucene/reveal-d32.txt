D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 1 OF 139 
REVEAL 
FP7-610928 
 
REVEALing hidden concepts in Social Media 
 
 
 
 
Deliverable D3.2 
Multimedia Linking and Mining 
 
 
 
 
Editor(s): Katerina Andreadou, Symeon Papadopoulos, Markos Zampoglou 
Responsible Partner: CERTH 
Status-Version: Final – v1.4 
Date: 03/03/2016 
EC Distribution: Public 
 
Project Number: FP7-610928 
Project Title: REVEAL 
 
  
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 2 OF 139 
Title of Deliverable: Multimedia Linking and Mining 
Date of Delivery to the EC: 30/06/2015 
 
Workpackage responsible 
for the Deliverable: 
WP3 – Multimedia Analysis and Indexing 
Editor(s): Katerina Andreadou, Symeon Papadopoulos, Markos Zampoglou 
Contributor(s): 
Katerina Andreadou, Lazaros Apostolidis, Christina Boididou, 
Yiannis Kompatsiaris, Symeon Papadopoulos, Stamatis Rapanakis, 
Emmanouil Schinas, Markos Zampoglou, Anastasia Krithara, 
Andreas Grivas, George Paliouras, Gregory Katsios 
Reviewer(s): SINTEF, DW, NCSR’D 
Approved by: All Partners 
 
Abstract: Multimedia items shared on social media platforms contain 
information, either the content itself or the content metadata, which 
can be analysed, processed and extracted to lead to the discovery 
of links between media items, to their aggregation and 
summarization, and to the extraction of indicators that are valuable 
for the verification of content and of the real-world events and 
stories that is depicted in it. In this document, we highlight the 
research and technical challenges involved in solving these 
problems, and we present several techniques of mining information 
concerning the content authenticity and the relations among 
content items. Furthermore, we present the research innovations 
and experimental results achieved during the second year of the 
project, and we give a detailed report of the prototype 
implementations demonstrating this year’s achievements. 
Keyword List: social multimedia, indexing, multimedia search, crawling, clustering, 
geotagging, image forensics, event summarization, text stylometry, 
fake detection 
 
  
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 3 OF 139 
DOCUMENT DESCRIPTION 
 
Document Revision History 
Version Date 
Modifications Introduced 
Modification Reason Modified by 
V0.1 05/03/2015 Basic structure and TOC created CERTH 
V0.2 01/04/2015 TOC updated NCSR 
V0.3 15/04/2015 Added section crawler CERTH 
V0.4 12/05/2015 Added section fake tweet detection and clusterer CERTH 
V0.5 13/05/2015 Added section image manipulation detection CERTH 
V0.6 27/05/2015 Added section multimedia summarization CERTH 
V0.7 02/06/2015 
Added abstract, executive summary, introduction, 
architecture 
CERTH 
V0.8 04/06/2015 Added sections relation extraction and author profiling NCSR 
V0.9 09/06/2015 Added appendix about modules innovations, editing CERTH 
V1.0 16/06/2015 Added missing references and evaluation results NCSR 
V1.1 17/06/2015 Add Sentiment Detection module ATC 
V1.2 24/06/2015 Further revision and improvements  CERTH 
V1.3 29/06/2015 Final refinements CERTH 
V1.4 04/03/2016 Corrected several undefined references and typos CERTH 
  
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 4 OF 139 
CONTENTS 
 
EXECUTIVE SUMMARY .................................................................................... 13 
1 INTRODUCTION ........................................................................................ 14 
1.1 SUMMARY AND STRUCTURE OF THE DOCUMENT ....................................................... 14 
1.2 CONCEPTUAL OVERVIEW OF THE WP3 MODULES ...................................................... 14 
1.3 CONTRIBUTIONS AND BREAKTHROUGHS ............................................................... 16 
2 MEDIA REVEALR ....................................................................................... 18 
2.1 SCOPE ...................................................................................................... 18 
2.2 OVERVIEW OF MEDIA REVEALR ........................................................................ 20 
2.3 VISUAL CLUSTERING ...................................................................................... 21 
2.4 USER INTERFACE .......................................................................................... 23 
2.5 USE CASES ................................................................................................. 25 
2.6 IMPROVEMENT IN YEAR 2: WEB IMAGE SIZE PREDICTION ........................................... 28 
3 TEXT-BASED RELATION EXTRACTION ....................................................... 35 
3.1 PROPOSED APPROACH .................................................................................... 35 
3.2 EXPERIMENTAL EVALUATION ............................................................................. 38 
4 MULTIMEDIA SUMMARIZATION ................................................................ 41 
4.1 RELATED WORK ........................................................................................... 42 
4.2 APPROACH DESCRIPTION ................................................................................ 43 
4.3 EXPERIMENTS AND RESULTS ............................................................................. 46 
5 FAKE TWEET DETECTION .......................................................................... 51 
5.1 RELATED WORK ........................................................................................... 52 
5.2 IMAGE VERIFICATION CORPUS ........................................................................... 53 
5.3 DESCRIPTION OF THE APPROACH ........................................................................ 55 
5.4 EXPERIMENTAL EVALUATION ............................................................................. 60 
5.5 CONCLUSIONS ............................................................................................. 65 
5.6 MEDIAEVAL 2015 VERIFYING MULTIMEDIA USE TASK .............................................. 66 
6 AUTHOR PROFILING ................................................................................. 67 
6.1 RELATED WORK ............................................................................................ 67 
6.2 PROPOSED APPROACH .................................................................................... 69 
6.3 EXPERIMENTAL EVALUATION ............................................................................. 70 
7 IMAGE TAMPERING DETECTION ............................................................... 73 
7.1 STATE-OF-THE-ART ....................................................................................... 73 
7.2 IMAGE FORENSICS IN THE WILD ......................................................................... 79 
7.3 THE WILD WEB TAMPERED IMAGE DATASET ........................................................... 81 
7.4 EXPERIMENTAL EVALUATION ............................................................................. 83 
7.5 TOOLBOX PROTOTYPE IMPLEMENTATION ................................................................ 92 
8 CONCLUSIONS & FUTURE WORK .............................................................. 97 
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 5 OF 139 
9 REFERENCES ............................................................................................. 99 
9.1 RELATED WORK ......................................................................................... 122 
9.2 SENTIMENT CORPUS .................................................................................... 123 
9.3 DESCRIPTION OF THE APPROACH ...................................................................... 123 
9.4 EXPERIMENTAL EVALUATION ........................................................................... 126 
9.5 CONCLUSIONS ........................................................................................... 128 
 
 
  
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 6 OF 139 
LIST OF FIGURES 
 
FIGURE 1: CONCEPTUAL DIAGRAM OF THE USE OF THE WP3 MODULES ON STREAMS OF SOCIAL 
MULTIMEDIA. MEDIA REVEALR IS A COMPLETE SYSTEM ENCOMPASSING MOST OF THE MODULES IN 
THE ABOVE FIGURE. .......................................................................................... 15 
FIGURE 2: CONCEPTUAL DIAGRAM OF THE USE OF THE WP3 MODULES ON SPECIFIC ITEMS OF INTEREST
 .................................................................................................................. 15 
FIGURE 3: HURRICANE SANDY TWEET WITH A MORE THAN ONE-YEAR-OLD THUNDERSTORM PHOTO .... 18 
FIGURE 4: COMPARISON OF CLUSTERING METHODS ........................................................... 22 
FIGURE 5: COLLECTIONS VIEW ................................................................................... 24 
FIGURE 6: ITEMS VIEW ............................................................................................ 24 
FIGURE 7: CLUSTERS VIEW ....................................................................................... 25 
FIGURE 8: ENTITIES VIEW ........................................................................................ 26 
FIGURE 9: TWEET REFERENCING COURTNEY LOVE'S FACEBOOK POST ...................................... 27 
FIGURE 10: NEAR DUPLICATE SEARCH WITH LOOSE FILTER ................................................... 27 
FIGURE 11: CLUSTER ITEMS FOR THE BOSTON MARATHON BOMBINGS ..................................... 28 
FIGURE 12: GUARDIAN ARTICLE ABOUT ISRAEL ELECTIONS. ONLY THE MAIN ARTICLE IS RELEVANT TO 
THE ARTICLE TOPIC. .......................................................................................... 29 
FIGURE 13: FREQUENCIES OF IMAGE WIDTHS .................................................................. 30 
FIGURE 14: RELATION EXTRACTION MODULE WORKFLOW ..................................................... 35 
FIGURE 15 OVERVIEW OF THE SUMMARIZATION PROCESS USED BY MULTIMEDIASUMMARIZER .......... 41 
FIGURE 16 EFFECT OF DUMPING FACTOR D ON P@10, S@5, MRR AND Α-NDCG@10 ................. 50 
FIGURE 17: FAKE MEDIA CONTENT SPREAD DURING THE MALAYSIAN AIRLINES BREAKING NEWS STORY
 .................................................................................................................. 51 
FIGURE 18: BASE FRAMEWORK REPRESENTATION .............................................................. 55 
FIGURE 19: BAGGING SCHEME ................................................................................... 59 
FIGURE 20: AGREEMENT-BASED RETRAINING (FUSION) ...................................................... 59 
FIGURE 21: ACCURACY OF USER CLASSIFIERS .................................................................. 64 
FIGURE 22: ACCURACY OF CLASSIFIERS BUILT ON THE AGREED SAMPLES .................................. 64 
FIGURE 23: ACCURACY OF CLASSIFIERS BUILT ON THE INITIAL AND AGREED SAMPLE ..................... 65 
FIGURE 24: FAILED CASES: HURRICANE SANDY AND COLUMBIAN CHEMICALS ............................ 65 
FIGURE 25: GROUPS OF FEATURES .............................................................................. 69 
FIGURE 26: DISTRIBUTION OF TWEETS ACCORDING TO AGE (LEFT) AND GENDER (RIGHT). ............. 70 
FIGURE 27: DETECTING A REAL-WORLD COPY-MOVE FORGERY. TOP: LEFT: ORIGINAL IMAGE. RIGHT: 
FORGED IMAGE. BOTTOM: LEFT: DETECTION USING BLOCK SEARCH WITH ZERNIKE MOMENT 
DESCRIPTOR (22.4 SEC) RIGHT: DETECTION WITH SURF KEYPOINTS (1.2 SEC). ................. 76 
FIGURE 28: A SECOND EXAMPLE OF REAL-WORLD COPY-MOVE FORGERY. DETECTION WITH ZERNIKE 
MOMENT DESCRIPTOR TOOK 27.1 SEC, AND DETECTION WITH SURF KEYPOINTS 2.7 SEC. NOTICE 
HOW THE TWO METHODS EACH DETECTED DIFFERENT (BUT BOTH TRUE) COPY-MOVED AREAS. 
PARAMETERS WERE MANUALLY ADAPTED TO ACHIEVE DETECTION, AND ARE DIFFERENT FROM THOSE 
IN FIGURE 27. ................................................................................................ 76 
FIGURE 29: A DEPICTION OF THE TYPICAL INTERCEDING STAGES BETWEEN THE COMMITTING OF A NEW 
FORGERY AND THE FORENSIC ANALYSIS OF AN IMAGE FOR TAMPERING, IN A REAL-WORLD 
SCENARIO. ..................................................................................................... 80 
FIGURE 30: SAMPLES FROM OUR WILD WEB DATASET. TOP: LEFT: AN “OCCUPY” DEMONSTRATION OF 
AIRLINE PILOTS’ UNIONS CONVERTED INTO AN ANTI-CHEMTRAIL DEMONSTRATION. RIGHT: A SCENE 
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 7 OF 139 
FROM THE MOVIE BACK TO THE FUTURE, WITH THE DESTINATION DATE ALTERED FROM 
21/10/2015 TO 05/07/2010. BOTTOM: RIGHT: A SUPPOSED “DOUBLE SUNSET” CAPTURED BY 
THE CURIOSITY ROVER. TWO SUNS FROM A STAR WARS FILM HAVE BEEN SPLICED INTO A REAL 
MARTIAN LANDSCAPE. LEFT: A SUPPOSED IMAGE OF A FLOODED MALL WITH SWIMMING SHARKS. 
THE ORIGINAL IS A PHOTO OF THE FLOODED UNION STATION, INTO WHICH TWO SHARKS WERE 
SPLICED. ....................................................................................................... 81 
FIGURE 31: EXAMPLES OF A FORGERY'S VARIANTS. LEFT: ORIGINAL FORGERY. CENTER: POST-SPLICE. 
RIGHT: CROPPING. ........................................................................................... 82 
FIGURE 32: TOP: THREE VERSIONS OF THE SAME FORGERY, IN WHICH WE WERE UNABLE TO DEDUCE THE 
MOST LIKELY TO CONTAIN DETECTABLE TRACES OF TAMPERING, THUS KEEPING EACH AS A SEPARATE 
CASE.  BOTTOM: THE SOURCE IMAGES OF THE FORGERY. ............................................... 83 
FIGURE 33: FROM LEFT TO RIGHT: A FORGERY, THE ORIGINAL SOURCE, AND THE THREE BINARY MASKS 
CREATED BY US ATTEMPTING TO MODEL THE SEPARATE STEPS OF THE FORGERY. ..................... 83 
FIGURE 34: LEFT: THE BAYER FILTER MOSAIC. RIGHT: THE LATTICE SHOWING ACQUIRED (A) AND 
INTERPOLATED (I) PIXELS FOR THE GREEN CHANNEL, IN THE BAYER FILTER (SOURCE: [97]). .... 84 
FIGURE 35: EXAMPLES OF APPLYING THE IMPLEMENTED ALGORITHMS ON EXPERIMENTAL DATASETS. TOP 
ROW: [97], [102] AND [103] APPLIED ON AN IMAGE FROM THE COLUMBIA UNCOMPRESSED 
DATASET. SECOND AND THIRD ROWS: [107], [110]A AND [111] APPLIED ON IMAGES FROM THE 
VIPP SYNTHETIC AND VIPP REALISTIC DATASETS. NOTICE HOW THE “SPLICE” IN ROW 3 IS 
ESSENTIALLY A COPY-MOVE FORGERY, AND IS EASILY DETECTED BY ALL THREE METHODS. ......... 86 
FIGURE 36: EVALUATION BY COMPARING THE MEDIANS INSIDE AND OUTSIDE THE MASK (WHITE 
CIRCLE). DUE TO THE UNIFORM BACKGROUND, THE MEDIAN OUTSIDE THE MASK IS VERY LOW, BUT 
FROM A REAL-WORLD PERSPECTIVE THIS IS A COMPLETELY FALSE OUTPUT. THE ALGORITHM USED IS 
[107] OVER A RESCALED IMAGE OF THE VIPP REALISTIC DATASET. .................................. 87 
FIGURE 37: SUCCESSFUL DETECTION RESULTS OVER THE WILD WEB DATASET. NOTICE HOW THE COPY-
MOVE FORGERY OF FIGURE 27 (THIRD FROM LEFT) WAS DETECTED USING THE SPLICING DETECTION 
ALGORITHM OF [110]. ....................................................................................... 89 
FIGURE 38: AN IMAGE FROM THE WILD WEB DATASET, ONE BINARY MASK RELATED TO THE CASE, AND 
THE OUTPUT OF [102] OVER THE IMAGE. WHILE THE MATCH IS VERY EXACT, AND IT IS POSSIBLE 
THAT THIS IS A CORRECT DETECTION OF THE LAST SPLICING OPERATION PERFORMED ON THE 
COMPOSITE IMAGE, IT IS ALSO LIKELY THAT THE LETTERS RETURNED HIGH-FREQUENCY NOISE IN 
TERMS OF IMAGE CONTENT, AND THE FINDING IS IRRESPECTIVE OF THE ACTUAL TAMPERING 
PROCESS. ...................................................................................................... 90 
FIGURE 39: SUCCESSFUL APPLICATION OF [111]OVER THE WILD WEB DATASET. TOP ROW: A FORGERY 
(LEFT) AND THE MEAN SQUARED DIFFERENCE BETWEEN THE IMAGE AND ITS RECOMPRESSED 
VERSIONS AT VARIOUS QUALITIES (30-100). BOTTOM ROW: ABSOLUTE IMAGE DIFFERENCES AT 
THE LOCAL MINIMA: FROM LEFT TO RIGHT, 60, 91 AND 100 –WHICH IS NOT A LOCAL MINIMUM PER 
SE, BUT OFTEN RETURNS GHOSTS, SIMILAR TO ERROR LEVEL ANALYSIS (ELA), SO WE INCLUDE IT 
IN OUR ANALYSIS. IN OUR CASE, CLEAR GHOSTS ARE RETURNED AT BOTH 91 AND 100, INDICATING 
AND LOCALIZING A SPLICE, WHILE THE SMALL LOCAL MINIMUM AT 60 RETURNS PURE NOISE. ..... 91 
FIGURE 40: A TAMPERED IMAGE, THE LOCALIZATION MAP FROM OUR MATLAB IMPLEMENTATION OF 
[107], AND THE LOCALIZATION MAP FROM OUR JAVA PROTOTYPE IMPLEMENTATION. ............... 93 
FIGURE 41: A TAMPERED IMAGE, THE LOCALIZATION MAP FROM OUR MATLAB IMPLEMENTATION OF 
[102], AND THE LOCALIZATION MAP FROM OUR JAVA PROTOTYPE IMPLEMENTATION. 
DISCREPANCIES ARE DUE TO THE DIFFERENT DWT IMPLEMENTATIONS. BOTH LOCALIZATION MAPS 
HAVE BEEN MEDIAN-FILTERED FOR DISPLAY PURPOSES. ................................................. 94 
FIGURE 42: A TAMPERED IMAGE AND THE CORRESPONDING LOCALIZATION MAPS FROM OUR JAVA 
IMPLEMENTATION OF [111] AT JPEG QUALITIES 85, 93 AND 95, (WHICH ARE ENTIRELY IDENTICAL 
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 8 OF 139 
TO THOSE FROM THE MATLAB TOOLBOX), AS WELL AS 100 (WHICH IS STRICTLY NOT A JPEG 
GHOST BUT RATHER AN ERROR LEVEL ANALYSIS RESULT. .............................................. 95 
FIGURE 43: SEMEVAL 2014 RANKING OF OUR EARLY MODEL VERSION (TWITTER 2014 CORPUS).... 126 
FIGURE 44: WP3 BATCH MODULES ............................................................................ 129 
FIGURE 45: WP3 RESTFUL API METHODS' OVERVIEW ON SWAGGER-UI ............................... 130 
FIGURE 46: IMAGE VERIFICATION REQUEST AND RESPONSE ................................................ 131 
FIGURE 47: CURRENT CRAWLS REQUEST AND RESPONSE ................................................... 132 
 
  
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 9 OF 139 
 
LIST OF TABLES 
 
TABLE 1: RESEARCH BREAKTHROUGHS .......................................................................... 16 
TABLE 2: TWITTER EVENT DATASETS ............................................................................ 25 
TABLE 3: N-GRAMS, TOKENS AND THEIR SCORE FOR SMALL AND BIG IMAGES ........................... 31 
TABLE 4: HTML METADATA FEATURES ........................................................................... 31 
TABLE 5: COMPARATIVE RESTULTS (F-MEASURE) .............................................................. 33 
TABLE 6: DATASET FOR EVALUATION OF THE RELATION EXTRACTION MODULE ............................. 38 
TABLE 7: PRECISION OF THE EVALUATION RESULTS: FRACTION (TOTAL) OF RELATIONS .................. 39 
TABLE 8: RESULTS FROM FA CUP DATASET ..................................................................... 40 
TABLE 9: COMPARISON OF SUMMARIZATION METHODS IN TERMS OF PRECISION. .......................... 49 
TABLE 10: COMPARISON OF SUMMARIZATION METHODS IN TERMS OF DIVERSITY ......................... 49 
TABLE 11: PERFORMANCE OF MULTIMEDIA SUMMARIZER ACROSS DIFFERENT EVENT CATEGORIES ...... 50 
TABLE 12: RESOURCES USED FOR EACH EVENT ................................................................ 54 
TABLE 13: LIST OF EVENTS INCLUDED IN THE CORPUS ........................................................ 54 
TABLE 14: ITEM AND USER FEATURES........................................................................... 56 
TABLE 15: LIST OF ABBREVIATIONS USED ...................................................................... 58 
TABLE 16: LIST OF TRIALS ........................................................................................ 61 
TABLE 17: ACCURACY RESULTS FOR THE ENTIRE SET OF FEATURES CASE (AF). AGREEMENT LEVELS 
BETWEEN THE IC
B
 AND UC
B
 CLASSIFIERS, AGREED, DISAGREED AND OVERALL ACCURACY FOR EACH 
TRIAL AND MODEL (AC
B
, (AC+BC)
B
). ................................................................... 61 
TABLE 18: ACCURACY RESULTS FOR THE BASELINE FEATURES CASE. AGREEMENT LEVELS BETWEEN THE 
IC
B
 AND UC
B
 CLASSIFIERS, AGREED, DISAGREED AND OVERALL ACCURACY FOR EACH MODEL (AC
B
, 
(AC+BC)
B
) AND EACH TRIAL. ............................................................................. 62 
TABLE 19: ACCURACY RESULTS FOR THE TIME-INDEPENDENT SET OF FEATURES CASE (TF). AGREED AND 
OVERALL ACCURACY FOR EACH MODEL (AC
B
, (AC+BC)
B
) AND EACH TRIAL. ........................ 63 
TABLE 20: ACCURACY RESULTS FOR THE FEATURE SELECTION CASE (FS). AGREED AND OVERALL 
ACCURACY FOR EACH MODEL (AC
B
, (AC+BC)
B
) AND EACH TRIAL. ................................... 63 
TABLE 21: FEATURES USED FOR EACH SUBTASK ............................................................... 70 
TABLE 22: TOP 10 SYSTEMS FOR GENDER SUBTASK BASED ON AVERAGE ACCURACY OVER LANGUAGES 71 
TABLE 23: TOP 10 SYSTEMS FOR AGE SUBTASK BASED ON AVERAGE ACCURACY OVER LANGUAGES 
(ENGLISH & SPANISH) ....................................................................................... 71 
TABLE 24: TOP 15 SYSTEMS FOR PERSONALITY TRAITS USING AVERAGE ROOT MEAN SQUARED ERROR 
OVER ALL LANGUAGES ........................................................................................ 72 
TABLE 25: TOP 10 SYSTEMS OVERALL USING AVERAGE GLOBAL METRIC DEFINED BY PAN ............... 72 
TABLE 26: PERFORMANCE METRICS FOR A NUMBER OF  COPY-MOVE DETECTION ALGORITHMS [91] .... 74 
TABLE 27: TIME AND MEMORY REQUIREMENTS FOR A NUMBER OF COPY-MOVE DETECTION ALGORITHMS 
[91] ............................................................................................................ 75 
TABLE 28: A LIST OF IMAGE SPLICING BENCHMARK DATASETS. FORMAT: THE FORMAT OF THE IMAGES 
CONTAINED. MASKS: THE PRESENCE OR ABSENCE OF GROUND-TRUTH BINARY MASKS GIVING THE 
LOCATION OF THE SPLICE. (MANUAL MEANS THE MASKS WERE MANUALLY CONSTRUCTED BY US 
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 10 OF 139 
USING THE ORIGINAL AND SPLICED IMAGES). # IMAGES: THE NUMBER OF AUTHENTIC/SPLICED 
IMAGES IN THE DATASET. .................................................................................... 79 
TABLE 29: RESULTS OF OUR IMPLEMENTED ALGORITHMS OVER THE EXPERIMENTAL DATASETS AND THEIR 
ALTERED (RECOMPRESSED AND RESCALED) VERSIONS. WITHIN EACH CELL, THE TOP ROW 
CORRESPONDS TO TRUE POSITIVES (TRUE NEGATIVES) BY COMPARING THE MEDIAN UNDER THE 
VALUE MASK TO THE MEDIAN VALUE OUTSIDE IT, WHILE THE BOTTOM ROW CORRESPONDS TO OUR, 
HUMAN-CENTRIC MEASURE. ................................................................................. 88 
TABLE 30: ALGORITHM EVALUATION OVER THE WILD WEB DATASET. ...................................... 89 
TABLE 31: LEARNING CORPUS ANNOTATED TWEETS ......................................................... 123 
TABLE 32: DATASET FOR TRAINING/ TESTING ................................................................ 126 
TABLE 33: PERFORMANCE ON TEST CORPUS .................................................................. 126 
TABLE 34: EMOTICONS MAPIING ............................................................................... 127 
TABLE 35: DELIVERABLE FILES/FOLDER STRUCTURE ........................................................ 133 
TABLE 36: NUMBER OF SUCCESSFUL DETECTIONS PER METHOD AND PER CLASS. IN CLASS NAMES, A 
NUMBER AT THE END INDICATES A DIFFERENT VERSION OF THE SAME FORGERY, WHILE AN 
UNDERSCORE FOLLOWED BY A LETTER INDICATES A DIFFERENT BINARY MASK FOR THE SAME 
VERSION. “#” INDICATES THE NUMBER OF TOTAL NUMBER OF COLLECTED IMAGES FOR THAT 
VERSION. .................................................................................................... 138 
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 11 OF 139 
DEFINITIONS, ACRONYMS AND ABBREVIATIONS 
 
Acronym Title 
ALE Approximate Laplacian Eigenmaps 
α-nDCG α-Normalized Discounted Cumulative Gain 
API Application Programming Interface 
AVS Average Visual Similarity  
CFA  Color Filter Array 
CPM Clique Percolation Method 
DBSCAN Density-Based Spatial Clustering of Applications with Noise  
DCT Discreet Cosine Transform  
DoW Description of Work 
DWT Discreet Wavelet Tranform 
ELA  Error Level Analysis 
EMR Elastic MapReduce 
Exif Exchangeable Image Fileformat 
FFT  Fast Fourier Transform 
GIF Graphics Interchange Format 
HTML HyperText Markup Language  
IC Item Classifier  
JNA  Java Native Access 
JPEG Joint Photographic Experts Group 
MDS Multi-Document Summarization 
MRR Mean Reciprocal Rank 
MPQA Multi-Perspective Question Answering 
NER Named Entity Recognition 
NLP Natural Language Processing 
NN Nearest Neighbour 
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 12 OF 139 
PCA Principal Component Analysis  
PFA Purple Fringing Aberration 
PNG Portable Network Graphics 
POS Part-Of-Speech 
PRNU Photo Response Non-Uniformity 
RE Relation Extraction 
REST Representational State Transfer 
SCAN Structural Clustering Algorithm for Networks 
SURF Speeded Up Robust Features 
SVM Support Vector Machines 
TF-IDF Term Frequence Inverse Document Frequency 
UC User Classifier 
URL Uniform Resource Locator 
VLAD Vector of Locally Aggregated Descriptors  
WOT Web Of Trust 
WP Work Package 
 
 
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 13 OF 139 
Executive Summary 
The objective of multimedia analysis research (WP3) is to develop a dedicated toolbox, which will 
facilitate the indexing of user-generated content with the dual goal of: 
 Discovering high-quality, relevant and trustworthy media content; 
 Detecting fraudulent, misleading and duplicate content. 
The goal of this deliverable is to document a) the extensions of the modules implemented during the 
first year of the project, b) describe the prototype implementations of new modules (developed within 
tasks T3.2 and T3.3), and c) provide an overall view with respect to the achieved advancements of 
the current state-of-the-art.  
In particular, the deliverable describes the progress made along the following directions: 
 The finalization of the multimedia indexing components (T3.1). This includes the extension of 
the crawler with a web image size prediction system that can decide whether an image is big 
enough to be worth fetching using only the image URL and surrounding HTML elements, 
hence resulting in considerable gains in terms of crawling efficiency. In addition, the 
multimedia indexing toolbox have been endowed with a sentiment analysis module, an 
aggregation mechanism, and an exploratory web user interface that enable end users to 
explore a large stream of social and Web multimedia content around an event of interest.  
 The extension of the multimedia relation discovery and linking components (T3.2). An 
improved version of the relation extraction module is described. In addition, a novel 
multimedia summarization module is presented that can select the most representative and 
high-quality images around a news story or event of interest. Both modules contribute to the 
extraction of non-obvious and useful summaries and associations between content items 
from large collections of Web multimedia.  
 The presentation of novel multimedia forensic analysis methods and a prototype 
implementation of a multimedia forensic toolbox (T3.3). Three new modules are introduced, 
which perform fake tweet detection, author profiling and manipulation detection, all aimed to 
deal with the highly challenging nature of multimedia verification using complimentary 
approaches from the fields of text stylometry, web mining and image forensics. 
The described scientific advances are accompanied by high-quality modular implementations that 
have been thoroughly evaluated and tested and can be used both as stand-alone components and 
as part of the integrated REVEAL platform. Hence, in the third and final period of the project, the 
emphasis will be placed on two main activities: a) developing improved multimedia forensics 
approaches (T3.3) to tackle the identified challenges, and b) refining and further maturing the 
existing modules in accordance with the end user feedback.  
 
  
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 14 OF 139 
1 Introduction 
1.1 Summary and structure of the document 
This document presents research in the context of WP3 of the REVEAL project performed during the 
second project year. This resulted in a set of module implementations and a number of evaluation 
studies for each of the three tasks in WP3: “Multimedia Indexing Toolbox”, “Multimedia Relation 
Discovery” and “Linking and Multimedia Forensic Analysis”. In more detail, our work in these tasks is 
discussed in terms of seven distinct modules, extending existing ones (developed during the first 
year of the project) or offering new functionality to the REVEAL platform: 
 Media REVEALr crawling, indexing and aggregation 
 Sentiment analysis 
 Text-based relation extraction 
 Multimedia summarization 
 Fake tweet detection 
 Author profiling 
 Image tampering detection 
In this section we briefly give an overview of the main research challenges tackled until now as well 
as the main contributions and achievements in the second year of the REVEAL project, within the 
context of WP3. Consecutively, each of the following sections is dedicated to a module, presenting 
our progress in detail, and demonstrating our scientific and technical advances, the conducted 
evaluations and the resulting implementations. Eventually we conclude with our future plans for the 
third and final year of the project. 
1.2 Conceptual overview of the WP3 modules 
In Figure 1 and Figure 2, we give a conceptual overview of the way the WP3 modules can be used to 
analyse, process, aggregate and eventually provide a valuable context for streams of incoming 
social multimedia items as well as for specific standalone items in question. In the case of the 
modules depicted in the first figure, the produced context and value stems from analysing large 
amounts of multimedia items with the goal of aggregating the results. For instance, the multimedia 
clusterer groups together images, which depict the same scene, and the summarizer, attempts to 
find a characteristic image to represent the produced cluster. On the other hand, the modules 
depicted in the second figure are called on a per-item basis to produce different reports, all aimed to 
deal with the veracity of content, either the authenticity of the image or the originality of the 
accompanying text.  
More technical details on the exact framework architecture can be found in Appendix C. Additionally, 
the WP3 RESTful API methods are exposed on an online service, called Swagger, which allows to 
query the API and visualize the response. By defining the API in a Swagger-compliant way, one can 
use the comprehensive Swagger-UI to understand and interact with the remote service. More 
Swagger details and screenshots can also be found in Appendix C. 
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 15 OF 139 
  
 
Figure 1: Conceptual diagram of the use of the WP3 modules on streams of social multimedia. Media 
REVEALr is a complete system encompassing most of the modules in the above figure. 
 
  
 
Figure 2: Conceptual diagram of the use of the WP3 modules on specific items of interest 
 
 
 
Social 
Media Item
Social 
Media Item
Social 
Media Item
Named Entity 
Recognition
Sentiment 
Analysis
Visual Indexer
Relation 
Discovery
Multimedia 
Clusterer
Multimedia 
Summarizer
Aggregated Sentiment
Negative
Neutral
Positive
Trending Content / Summaries
Visual Clusters
Named Entities & Relations
Focused Web 
Crawler (CERTH)
Social Media 
Crawler (ITINNO)
Stream of Social 
Multimedia
Cluster
Cluster
Cluster
Similarity 
Retriever
Fake Post 
Detection
Author Profiler
Verification
Manipulation 
Detection
Author Profiling
Fake Tweet Report
Manipulation Report
Item of interest

D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 16 OF 139 
1.3 Contributions and Breakthroughs 
The main goal of WP3 is to deliver a versatile framework for the indexing and search of social 
multimedia content, as well as for the extraction of hidden aspects of it with particular emphasis on 
tracing cues that facilitate the assessment of the veracity and quality of posted content. A working 
prototype with indexing and advanced search and mining capabilities is already running, the main 
components of which, along with the extensions implemented during year 2 are presented in section 
2. Sections 2 - 8 present the novel capabilities and functionalities of this framework. An overview of 
the research contributions achieved until now is provided in Table 1 and a more detailed description 
of the innovations introduced by each module can be found in Appendix A. 
Table 1: Research breakthroughs 
Research Area Contributions / Breakthroughs 
Focused Image 
Crawling (T3.1) 
 High throughput of relevant images 
 Filtering and processing of relevant image at crawl time 
Relation Extraction 
(T3.2) 
 Extended method for text-based relation extraction 
 Evaluation of the approach in three datasets 
 Integration of the module to the REVEAL platform 
Image Clustering 
(T3.2) 
 Efficient on-demand extraction of multimedia clusters 
from the web and social media 
 Capability for incremental clustering 
Multimedia 
Summarization 
(T3.2) 
 Novel method for summarizing a collections of images 
around a topic with a number of representative images 
 Combination of text, visual, temporal and social cues to 
improve relevance and diversity of summary 
Author profiling 
(T3.3) 
 State-of-the-art investigation 
 Implementation of a first approach 
 Participation in PAN challenge about author profiling 
Fake Tweet 
Detection (T3.3) 
 Novel method for detecting fake tweets based on their 
textual content and metadata 
 Creation of large corpus of fake tweets around popular 
events or news stories 
 Organization of new task in MediaEval 2015: “Verifying 
Multimedia Use” 
Multimedia 
Forensics (T3.3) 
 Creation of a large corpus of manipulated images from 
real-world scenarios  
 Testing and evaluation of several different manipulation 
detection algorithms 
 Preliminary implementation of a forensic toolbox for 
detecting image tampering operations 
 
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 17 OF 139 
Each of the following sections includes at least one major contribution to the progress of REVEAL as 
well as the state-of-the-art in the respective research field. In section 2 we present a technique for 
predicting the size of images based solely on their URL and HTML metadata, which is integrated into 
the extended focused image crawling framework, implemented during the first project year. As far as 
multimedia linking is concerned, we extended the existing capabilities with an incremental 
implementation of a well-established algorithm for clustering large-scale image datasets. These 
extensions were incorporated in the multimedia analysis framework, the so-called Media REVEALr 
system. In section 3, we present a text-based relation extraction system, which can be used to 
extract high-level predicates from sentences alongside their subjects and objects is presented. In 
section 4, we present our progress in multimedia summarization and ranking. We present a novel 
method for summarizing a collection of images around a topic using a set of representative images 
of the collection, where the most representative images are chosen based on both visual features 
and textual information from the corresponding tweets.  
One of the areas of increased focus in year 2 has been the area of Multimedia Forensic Analysis, 
where a strong foundation was established to support the development of a multimedia forensic 
toolbox during the third year of the project. In this context, considerable breakthroughs have been 
achieved. Two new verification corpora have been collected and are available: the first one is 
composed of fake tweets (containing images) around several real-world events and the second one 
comprising a large number of digitally manipulated online images. In section 5 we present the first 
dataset, the MKLab Image Verification Corpus, as well as a novel method for detecting fake tweets 
using features extracted from the tweet and the user posting it. Furthermore, the dataset we created 
will be the basis for the organization and participation of the CERTH team to the MediaEval 2015 
“Verifying Multimedia Use” task1. In section 6, we present a first approach for text-based author 
profiling. The goal is to categorize tweet authors based on their age and/or gender, in order to assist 
the verification process. Finally in section 7, we present the second dataset, consisting of forged 
images collected from the Internet, called the “Wild Web tampered image dataset”. The state-of-the-
art in image forensics, in the form of different methods for detecting manipulations in images, have 
been applied to the image dataset, as well as multiple well-established evaluation datasets and the 
results of this evaluation are presented. Following the evaluations and certain considerations 
concerning the established evaluation protocols in the field, we identify the major gaps given the 
needs in the context of REVEAL. Finally, we present a preliminary implementation of the multimedia 
forensic toolbox, containing portable implementations of a number of algorithms from the state-of-
the-art that gave the best results in our evaluations. The final section summarizes the contributions 
presented in this document, and lays the directions for our future research and development steps. 
  
                                                     
1http://www.multimediaeval.org/mediaeval2015/verifyingmultimediause/index.html 
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 18 OF 139 
2 Media REVEALr 
2.1 Scope 
Modern online social networks, such as Twitter and Instagram, are nowadays important sources for 
publishing information and content around breaking news stories and incidents related to public 
safety, ranging from natural disasters and aeroplane crashes to terrorist attacks and industrial 
accidents. A crucial issue regarding such information and content is the extent that they can be relied 
upon and used for improving the situational awareness and operational capabilities of decision 
makers.  
Breaking events, for instance a natural catastrophe (e.g., Hurricane Sandy), a terrorist attack (e.g., 
Boston Marathon bombings), an aeroplane crash or a massive protest, naturally attract the attention 
of locals and Internet users, who in turn flood the social networks with personal comments, stories, 
images and videos. Popular and widespread subjects, however, tend to also cause an abundance of 
fake media content.  An image might be photoshopped in order to convey a certain message or 
opinion concerning the subject in question, and it might often be maliciously manipulated in order to 
trigger the public opinion and provoke a specific reaction. It might even be the case that a real 
picture of a past event is retweeted and presented as depicting current events. An example of this 
can be seen in Figure 3, where a picture of a thunderstorm over New York dating from 2011 during a 
tornado alert was massively retweeted as “a picture of Hurricane Sandy descending in New York” in 
late 2012. 
 
Figure 3: Hurricane Sandy tweet with a more than one-year-old thunderstorm photo 
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 19 OF 139 
Mining social media content has arisen as an important problem in the last decade due to the 
increasing tendency of users to produce and share content, in the form of text messages, images 
and videos [1]. This problem has been extensively examined from various perspectives. Marcus et 
al. in [2], presented a novel system for visualizing and summarizing events on Twitter with the goal of 
offering to the end users a comprehensive chronological representation, highlighting the relevant 
peaks of activity. Mathioudakis and Koudas proposed TwitterMonitor, a system for real-time trend 
detection over the Twitter stream [3]. This automatically identifies emerging topics and provides 
users with meaningful and comprehensive statistics and related information. Some other related 
approaches attempt to identify the virality of content based on community detection and graph 
analysis methods. For instance, in [4], Weng et al. propose a system for predicting successful 
memes based on their early spreading patterns.  
Gradually the focus has shifted to real-time mining approaches, because the need for real-time 
analysis becomes very challenging as the amount of data increases and existing systems fail to cope 
with the rate and scale of the incoming content. Moreover, the dynamic nature of social network 
streams such as Twitter, calls for specially designed approaches, which take this dynamic nature 
and the massive size of the data into account. For instance, in [5], Xie et al. attempt to comprehend 
the diffusion of videos containing one or more memes and to predict the lifespan of such a video and 
hence, the conveyed message. In [6], Petkos et al. propose a semi-supervised method for clustering 
multimedia items in order to predict social events. An additional application of such methods, which 
is based on the visual content of social media items, is the identification of the history of an image, 
once a certain number of image copies or near-duplicates has been gathered [7]. 
Given the proliferation of noisy, irrelevant and fake content posted to such platforms, two important 
requirements for systems supporting the information access needs in incidents, such as the ones 
described above, include the support for understanding the “big picture” around the incident and the 
verification of particular pieces of posted content. To this end, we propose a scalable and efficient 
content-based media crawling and indexing framework featuring a novel and resilient near-duplicate 
detection approach and intelligent content- and context-based aggregation capabilities (e.g. 
clustering, named entity extraction).  
Although a variety of text and social media data analysis approaches have been previously proposed 
in similar settings with the goal of mining information out of big sets of social network posts (e.g. 
topic detection and social graph analysis), they lack support for improved situational awareness and 
content verification. In particular, the proposed system offers the following unique capabilities: 
 it enables the precise and resilient identification of near-duplicate images and videos (based 
on selected keyframes) in a stream of social media content even in the presence of overlay 
graphics and fonts; 
 it supports the identification and comparative view of multiple independent sources of 
content that discuss the same incident; 
 it extracts and aggregates the named entities extracted from the collected social media 
messages and presents them through an intuitive and visually appealing interface with the 
goal of improving the situational awareness over the incident of interest.  
 
In the following, we evaluate the system using both reference benchmark datasets as well as 
datasets collected around real-world incidents, and we describe the ways it contributes to the 
improvement of the situational awareness and journalistic verification in breaking news situations, 
like natural disasters. This work is documented in detail in a paper [8] that was presented in PAISI 
2015 (Pacific Area Workshop on Intelligence and Security Informatics). 
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 20 OF 139 
2.2 Overview of Media REVEALr 
The Media REVEALr consists of the following interoperating components: The crawler is responsible 
for collecting text messages, images and their respective metadata from a number of social networks 
(in this work we focus on Twitter) and the Web (e.g. specific news sites). The tweet texts are 
processed by the named entities extraction modules and the extracted named entities are stored in a 
Mongo database. Regarding the processing of visual content, the main component of the system is 
the visual feature extractor, which extracts a feature vector from every incoming image (or video 
frame). Subsequently, for every processed image, the metadata is saved in a Mongo database, and 
the feature vector is provided to the visual indexer, which constitutes a highly optimized search 
structure that supports near-duplicate image retrieval.  As a last step, the clustering component 
organizes the collected images into groups based on their visual similarity; the information about the 
resulting clusters is again stored in the mongo database. Extensive technical details of the 
aforementioned technical components were provided in D3.1.  
The proposed architecture enables the provision of numerous capabilities through a single API: 
advanced keyword-based queries, search queries based on the image metadata stored in MongoDB 
(e.g., width, height, publication date, name of the publisher) and most importantly visual similarity 
search queries, which, given an image, attempt to retrieve near-duplicates. The interaction of the 
user with the system services takes place through a web-based user interface that is described in 
detail in section 2.4. 
An added value in our system lies in the ways the data is aggregated to facilitate the user in 
identifying the context of use of a particular media item within a large collection of seemingly 
unrelated images, and in revealing hidden relations and dependencies among the multimedia items. 
Three types of aggregation are offered:   
 visual aggregation by creating clusters of images based on visual similarity; 
 entity aggregation by extracting named entities from the accompanying text and then 
grouping images together based on the entity occurrences; 
 aggregated sentiment by performing sentiment analysis on the accompanying text 
 
For visual aggregation, the clusters are created in batch mode after the collection of the images has 
been completed using the DBSCAN algorithm and consequently each cluster is represented by its 
most representative image. This is defined as the image with the largest amount of occurring 
keywords in the accompanying text. This helps the user to easily grasp the visual context of a 
specific story. The clustering algorithm at the moment is based solely on visual features, in particular 
the Euclidean distance between the PCA-reduced SURF-VLAD vectors. 
Entity aggregation is performed using the Named Entity Recognition process described in D3.1 and 
subsequently computing the frequency of every named entity in the corpus of collected content 
items. Eventually, entities are ranked according to the frequency of their appearance.  This way, the 
prominent aspects of an incident (persons, events, locations etc.) stand out and offer a concise 
semantic view of the incident of interest. 
For producing an aggregated sentiment report, the multimedia content items are analysed and a 
supervised learning approach is used to generate a prediction concerning the polarity of the 
accompanying text, whether it is positive, negative or neutral. This is not yet integrated into the demo 
application but in the future, it could provide an additional way of aggregating content items 
according to the sentimental context their text expresses. More details on sentiment analysis can be 
found in Appendix B. 
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 21 OF 139 
2.3 Visual clustering 
The purpose of the multimedia clusterer is to group together images which depict the same scene or 
belong to the same story. By aggregating semantically or visually similar images in clusters and 
presenting the most important or relevant clusters in a comprehensive UI, we can facilitate the work 
of the end users or journalists, which would otherwise have to manually do this kind of filtering. The 
clusterer is a batch component which can be called on demand on a per-collection basis in order to 
apply the clustering algorithm on the items of the collection in question. The global architecture of the 
system imposes the following requirements, which directed us to experiment with certain algorithms 
and disregard others from the early beginning: 
 The number of clusters cannot be specified a priori because the crawler gradually collects 
more images and it can be stopped at an arbitrary point in time, so there is no possibility to 
predict the final number of clusters that will be appropriate. For this reason, algorithms such 
as k-means and its extensions cannot be used in this scenario. 
 The algorithm must be robust to outliers and take into account that a big amount of data 
might be useless noise. 
 The algorithm must be incremental; it must be possible to gradually cluster more data as 
they arrive by taking already existing clusters into consideration. 
 
DBSCAN  
Although it is not incremental, we first experimented with BSCAN (density-based spatial clustering of 
applications with noise [9]), which is one of the mostly used and cited data clustering algorithms in 
the scientific literature. DBSCAN requires two parameters: 
 ε (eps): the distance the defines the ε-neighborhood of a point 
 the minimum number of points required to form a dense region 
DBSCAN starts with an arbitrary unvisited starting point, retrieves this point’s ε-neighborhood given a 
list of available points and if the ε-neighborhood contains sufficiently many points, a cluster is 
created. Otherwise, the point is labeled as noise; it might yet later become part of another cluster. 
As a basis, we used the apache commons implementation2 which is open source and available as a 
maven dependency. We tested the algorithm with a number of real-world datasets and then we 
manually checked the formed clusters to verify that they are visually consistent and the results were 
satisfactory. The drawback of this method is the fact that DBSCAN is not incremental, which means 
that as the crawler collects more images, every time the clustering component is called, it has to 
discard all existing clusters and cluster all available data from scratch. 
Incremental DBSCAN  
In order to overcome this issue, we implemented am incremental variation of the DBSCAN algorithm 
by adding new functionality to the existing apache commons source code. In pseudocode, the new 
incremental algorithm can be expressed as follows (the red part is the addition to the simple 
DBSCAN algorithm): 
 
DBSCAN(D, eps, MinPts) 
   C = 0 
   for each unvisited point P in dataset D 
   if (P isNeighbor of an existing cluster) 
                                                     
2 https://commons.apache.org/proper/commons-
math/apidocs/org/apache/commons/math3/ml/clustering/DBSCANClusterer.html 
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 22 OF 139 
         add P to this cluster 
      mark P as visited 
      NeighborPts = regionQuery(P, eps) 
      if sizeof(NeighborPts) < MinPts 
         mark P as NOISE 
      else 
         C = next cluster 
         expandCluster(P, NeighborPts, C, eps, MinPts) 
 
The way we determine if a point P is a neighbor of an existing cluster is by checking whether its 
distance to all members of this cluster is smaller than eps. This procedure is fairly fast, because in 
the case the new point P is not a neighbor (which is the common case), it suffices to check the 
distance to one member of the cluster, find out that the distance is bigger than eps and then skip the 
rest and proceed to the next cluster.  
The improvement of using this approach becomes evident in Figure 4: the base DBSCAN needs 
exponentially more time to cluster the collection because for every iteration, the number or elements 
to be clustered doubles. On the other hand, the incremental variation employs the approach we 
described above to categorize elements to the already existing clusters, so the clustering time for 
every iteration remains almost constant. This feature is crucial to the functionality of the overall 
system as the base DBSCAN algorithm would not scale in a real-world scenario, where we would 
have millions of images related to a subject. The performance would be severely harmed due to time 
constraints but also and most importantly due to memory limitations, as the apache commons 
implementation demands all the items to be loaded into memory before the system can start 
clustering them. 
 
Figure 4: Comparison of clustering methods 
1
10
100
1000
10000
100000
1000000
55 55 93 134173218245283320346387421462485516547578614646684736773809845
Ti
m
e 
in
 m
ill
is
ec
o
n
d
s
Number of clusters
Clustering a collection of 25000 images
Incremental Simple
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 23 OF 139 
2.4 User interface 
The user interface3 consists of different views, which, combined with the real-time analysis of the 
collected data, facilitate the exploration of all available data aspects and the evaluation of the 
produced results. In the first view (Figure 5), all available crawled collections are presented, 
accompanied by some statistics, such as the creation date, the time the last item was inserted, the 
total duration of the crawl, the total number of images and videos, the keywords for the crawl and the 
current state. The state is signified by a coloured bar which is grey for waiting, green for running and 
red for stopped or finished. Every collection is represented by a card and the user can interact with it 
in the following ways: stop a running crawl, delete a collection, or click on it for further exploration, 
which navigates the user to the next three views. 
In the second view (Figure 6), one can observe all items of the collection of interest. For every item, 
a multitude of related information is available: the date it was published, the related text (e.g. the 
tweet), how many times it was shared, the name of the author and a thumbnail of their profile picture. 
One can click on the user thumbnail to visit the user’s profile page or on the image to see a bigger 
version and the full text. Additionally, this view offers a variety of search options. The user can drag 
and drop an image or video in the search box to search for similar images. He/she can also use the 
search dialog to filter the images by several criteria: image dimensions, the date it was published, the 
username of the publisher or some text search terms. 
The third and fourth views (Figure 7, Figure 8) are the mining and aggregation views. The cluster 
view presents the clusters that were created as a result of the DBSCAN algorithm. For every cluster 
there is a representative image and some additional information, such as the number of items in the 
cluster. By clicking on the cluster item, one can navigate in a detailed view where all the items of the 
cluster are available. In the named entities view one can explore the extracted named entities that 
can additionally be grouped by appearance (often, occasionally, seldom) or by type (person, 
location, organization, other). By clicking on a named entity bubble, the user can again navigate to a 
detailed view where all items containing this entity are presented. 
 
                                                     
3 http://reveal-mklab.iti.gr:8084 
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 24 OF 139 
 
Figure 5: Collections view 
 
Figure 6: Items View 
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 25 OF 139 
 
Figure 7: Clusters View 
2.5 Use cases 
Here, we go through some case studies based on Twitter datasets that were collected around a 
number of recent incidents of interest with respect to public safety: a) the Malaysia MH370 missing 
flight incident that took place on March 8, 2014, b) the Boston Marathon bombings taking place on 
April 15, 2013. As will be seen in the next sections, the proposed system offers valuable insights into 
hidden aspects of the collected content, which would not be straightforward to gain unless one went 
through all the items trying to find correlations and underlying relations. The aforementioned two 
datasets along with five more datasets (Hurricane Sandy, #BringBackOurGirls crisis in Nigeria, MV 
Sewol sinking, Columbian Chemicals hoax) and the SNOW dataset [10] (a set of tweets around 
newsworthy subjects spanning a time interval of interest), were inserted and explored through a 
locally deployed version of the tool. The tweet IDs and image URLs for all seven datasets are 
publicly available4. 
Table 2: Twitter event datasets 
Collection Description Images Users Clusters 
snow One day of Twitter data 33840 26877 184 
sandy Hurricane Sandy 25765 25110 28 
malaysia Malaysian Airlines flight MH370 24784 22175 940 
                                                     
4 http://mklab.iti.gr/files/media_revealr_data.zip 
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 26 OF 139 
boston Boston Marathon bombings 5487 4683 181 
ferry South Korea ferry sinking 2524 1921 94 
girls Girls kidnapped in Nigeria 1250 1163 56 
chemicals Chemicals plant explosion hoax 377 307 13 
Incident summary view (malaysia) 
As a first step, grouping the named entities extracted from the set of collected tweets by type, we can 
get a quick grasp of all the involved parties and the main information about the event (Figure 8). We 
can directly deduce that the main location of the event is Malaysia, with a couple of secondary 
locations being involved, namely China and Beijing (where the flight was heading) as well as places 
where the search effort extended, for instance Australia, India, Perth, Indian Ocean; in addition, the 
principal organizations involved are Malaysia Airlines, CNN, Reuters and Boeing. 
What at first glance strikes us as odd is the fact that the main persons related to the story are 
Courtney Love and Kurt Cobain, followed by the names of passengers, crew members as well as 
politicians and officials, who often made statements about the developments of the ongoing search 
for the missing aircraft. For instance Najib Razak, the Malaysian Prime Minister, Mohen Wang, a 2-
year old passenger onboard, Rajah Bomoh, a kind of shaman who performed rituals to locate the 
missing plane. The reason why Courtney Love appears as the most prominent one among the 
person named entities is that she posted a picture on her Facebook page claiming that she had 
located the plane, this picture went subsequently viral on social media and eventually Love's and 
Cobain's names were connected to the MH370 story through this mishap. By clicking on the 
respective items on the UI, we can see some of the tweets referencing the original post from 
Courtney Love (Figure 9). As becomes evident from this case, named entity extraction can be a 
valuable tool when filtering multimedia content, as it enables the user to disregard all tweets 
referencing Courtney Love as irrelevant ones. 
 
Figure 8: Entities View 
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 27 OF 139 
 
Figure 9: Tweet referencing Courtney Love's Facebook post 
Near duplicate search (boston) 
A further analysis step in breaking news situations, such as the Boston Marathon bombings, is the 
near-duplicate search functionality, which is a valuable assisting tool when looking for repostings and 
retweets of the same multimedia content. In Figure 10, we used the image in the center as a query 
and configured the search filter to be loose (lower similarity threshold). It is evident from the results 
that, apart from retweets of the same image, the system is also able to find images which have been 
color adjusted, images with overlays as well as images that contain the image in question as their 
part (splicing). 
 
Figure 10: Near duplicate search with loose filter 
 
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 28 OF 139 
Image clustering and message comparison (boston) 
On the other hand, if we take a closer look at the image clusters, which are created based on the 
visual similarity of the images, we can observe how different people comment on the same image 
when retweeting and which tweets have the highest penetration (i.e. are retweeted more often). An 
interesting observation concerns the way in which the degree of confidence expressed by the tweet 
text increases as the time goes by and the information gets cross-checked from other sources. For 
instance, after the Boston marathon bombing, there was intense discussion on Twitter about a high-
resolution photo of one of the suspects (Figure 11). The first tweet reports “Clearest pic I’ve seen yet 
of one of the #BostonMarathon bombing suspects. Recognize white cap bro?''. Fifty minutes later 
there is another one: “Reported photo of suspect 2 in Boston bombings emerges on Facebook” And 
then some hours later there is another, which is the most retweeted one: “Just in. New image 
surfaces online that purports to show Boston bombings suspect two”. 
 
Figure 11: Cluster items for the Boston Marathon bombings 
2.6 Improvement in year 2: Web image size prediction 
In the context of using Web and social media content for Media REVEALr, we had to perform large-
scale image crawling based on a set of supplied keywords, which was described in detail in D3.1. A 
number of focused crawling algorithms have been proposed in order to increase the harvest rate (the 
number of relevant web pages discovered by the crawler) and the target precision (the number of 
relevant crawl links). Link context algorithms rely on the lexical context of the URL within its parent 
page [11], graph structure algorithms take advantage of the structure of the Web around a page to 
find nodes that lead to many relevant Web pages [12], and semantic analysis algorithms utilize 
ontologies for semantic classification [13]. Yet, no comprehensive focused crawling solution exists in 
literature that is targeted to image content. 
One might use several criteria for evaluating the relevance of a Web image to the supplied 
keywords: whether the alternate text of the img element contains any of the keywords, whether the 
containing web page title contains any of the keywords, etc. Nonetheless, a common problem is that 
many images of very small dimensions, which are typically of no interest to the analysis process, 
may meet such criteria, thus introducing a considerable amount of noise in the collected content. For 
instance, consider a crawl topic around the recent election in Israel. In Figure 12, apart from the main 
central image which is clearly on topic, there is a multitude of other image elements that are 
irrelevant: the icons on the left prompting the user to share the article on social networks, the images 
on the right that link to other articles, as well as banners, logos and icons that belong to the website 
theme. This fact influences the performance and speed of image crawling solutions, as their main 
bottleneck pertains to fetching the content. To this end, one might use approaches such as 
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 29 OF 139 
boilerpipe5, which remove the surplus clutter around the main textual content; however, such 
approaches are time consuming and in many cases not effective, e.g., when there is no main article 
and image in the web page. 
 
Figure 12: Guardian article about Israel elections. Only the main article is relevant to the article topic. 
To address this limitation, we explore the challenge of predicting the size of images on the Web 
based only on their URL and information extracted from the surrounding HTML code. We present 
two different methodologies: The first one is based on a common text classification approach using 
the n-grams or tokens of the image URLs and the second one relies on the HTML elements 
surrounding the image. Eventually, we combine these two techniques, and achieve considerable 
improvement in terms of accuracy, leading to a highly effective filtering component that can 
significantly improve the speed and efficiency of the image crawler. This work is described in detail in 
a paper [14] presented at CBMI 2015 (13th International Workshop on Content-Based Multimedia 
Indexing). 
Data Collection 
For our work, we used a sample of the data from the July 2014 Common Crawl6 set, which is over 
266TB in size and contains approximately 3.6 billion web pages. Since for technical and financial 
reasons, it was impractical and unnecessary to download the whole dataset, we created a 
MapReduce job to download and parse the necessary information using Amazon Elastic MapReduce 
(EMR). The data of interest include all images and videos from all web pages and metadata 
extracted from the surrounding HTML elements. To complete the task, we used 50 Amazon EMR 
medium instances, resulting in 951GB of data in gzip format. The following statistics were extracted 
from the corpus: 
 3.6 billion unique images 
 78.5 million unique domains 
                                                     
5 https://code.google.com/p/boilerpipe 
6 http://blog.commoncrawl.org/2014/08/july-2014-crawl-data-available 
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 30 OF 139 
 ~8% of the images are big (width and height bigger than 400 pixels) 
 ~40% of the images are small (width and height smaller than 200 pixels) 
 ~20% of the images have no dimension information 
 
In Figure 13 the frequencies of image widths are displayed (the height distribution is similar). One 
can observe that the chosen threshold of 400 pixels is meaningful because it marks an order of 
magnitude difference in the relative frequencies of image heights and widths. 
 
Figure 13: Figure Frequencies of image widths 
 
Textual features  
An n-gram in our case is a continuous sequence of n characters from the given image URL. The 
main hypothesis we make is that URLs which correspond to small and big images differ substantially 
in wording. For instance, URLs from small images tend to contain words such as logo, avatar, small, 
thumb, up, down, pixels. On the other hand, URLs from big images tend to lack these words and 
typically contain others. If the assumption is correct, it should be possible for a supervised machine 
learning method to separate items from the two distinct classes. 
To perform the training of the model, we first collected the most frequent n-grams of the training 
image URLs where n={3,4,5}. Subsequently we ranked the collected n-grams by frequency. 
Then, for every URL, we constructed the feature vector as follows: for every n-gram of the f most 
frequent ones, the respective vector position is 1 if the URL contains the n-gram and 0 otherwise. 
We will refer to this method as NG. We ran our experiments for f=1000, and f=2000. The values for n 
and f are arbitrary, they are however the result of preliminary testing with smaller datasets.  
The disadvantage of the previous method is that, although the frequencies of the n-grams are taken 
into account, what is not considered is the correlation of the n-grams to the two classes, BIG and 
SMALL.  For instance, if an n-gram is very frequent in both classes, it makes sense to get rid of it 
and not consider it as feature. On the other hand, if an n-gram is not very frequent (not one of the 
first 1000 or 2000 that we take into account in the previous case), but it is very characteristic of a 
specific class, we should include it in the feature vector. To this end, we perform feature selection by 
taking into account the relative frequency of occurrence of the n-gram in the two classes, BIG and 
SMALL. We will refer to this method as NG-trf, standing for term relative frequency. 
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 31 OF 139 
In a variation of the aforementioned approach, we decided to replace n-grams with the tokens 
produced by splitting the image URLs by all non-alphanumeric characters. The employed regular 
expression in Java is \\W+ and the feature extraction process is the same as described above, but 
with the produced tokens instead of n-grams. We will refer to this method as TOKENS-trf. The n-
grams and tokens with the highest scores for the two classes are displayed in Table 3. 
Table 3: n-grams, tokens and their score for SMALL and BIG images 
n-gram 
SMALL 
score n-gram 
BIG 
score tokens 
SMALL 
score tokens BIG score 
humb 33473 600 50755 png 31510 Jpg 65501 
hum 33266 S1600 44963 gif 22796 s1600 44934 
thu 31847 S160 44799 s45 20923 uploads 12504 
thum 31830 S16 44732 c 20085 Com 9328 
thumb 31760 160 42086 s72 19237 Photobucket 8960 
ata 27242 201 29156 images 16164 Albums 8798 
atar 25143 AAA 22036 1 8689 s640 8710 
vata 25139 mblr 19155 thumbnail 5682 content 6805 
vatar 25128 Blr 19154 0 5440 2012 6353 
 
A further improvement over the previous method is to increase the impact of the overall frequency of 
an n-gram on the feature selection. This results in the exclusion of all n-grams with high frequencies 
in both classes. To this end, we introduce another metric, which is a variation of the tf-idf (term 
frequency - inverse document frequency). We will refer to it as tsrf-idf (term squared relative 
frequency - inverse document frequency) and it is defined in the below equations for the BIG and 
SMALL classes respectively: The feature extraction process is the same as described before, with 
the only difference that the computed score is the tsrf-idf instead of just the difference of frequencies, 
as was the case before.  
𝑆𝑏𝑖𝑔(𝑛) =
𝑓𝑏𝑖𝑔(𝑛)
2−𝑓𝑠𝑚𝑎𝑙𝑙(𝑛)
2
𝑓𝑏𝑖𝑔(𝑛)
           𝑆𝑠𝑚𝑎𝑙𝑙(𝑛) =
𝑓𝑠𝑚𝑎𝑙𝑙(𝑛)
2−𝑓𝑏𝑖𝑔(𝑛)
2
𝑓𝑠𝑚𝑎𝑙𝑙(𝑛)
 
Non-textual features 
An alternative non-textual approach does not rely on the image URL text, but rather on the metadata, 
that can be extracted from the image HTML element. More specifically, the selected features are 
presented in Table 4. The idea behind their choice is for them to reveal cues regarding the image 
dimensions. For instance, the first five features, which correspond to different image suffixes, were 
selected due to the fact that most real-world photos are in JPG or PNG format, whereas BMP and 
GIF formats usually point to icons and graphics. Additionally, there is a greater likelihood that a real-
world photo has an alternate or parent text than a background graphic or a banner. The features 
hasWidth, width, hasHeight and height of the table are extracted using the regular 
expression \\d+x\\d+)+|(w|h|s)_?\\d+|\\d+px|(width|height|w|h)=\\d+|_\\d+\\.. 
Table 4: HTML metadata features 
Name Description 
suffix_ JPEG 1 if the image URL has a JPG suffix, 0 
otherwise 
suffix_ PNG 1 if the image URL has a PNGsuffix, 0 
otherwise 
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 32 OF 139 
domDepth Depth of the image element in the DOM tree 
domSimblings Number of siblings of the image element in 
the DOM tree 
hasWidth 1 if a width value could be extracted from 
the image URL, 0 otherwise 
width Extracted width value 
samedomain 1 if the image and originating web page URLs 
have the 
same hostname, 0 otherwise 
domElement_IMG 1 if the DOM element is <img>, 0 otherwise 
domElement_LINK 1 if the DOM element is <link>, 0 otherwise 
hasAltText 1 if the alt text of the image element is not 
null, 0 otherwise 
altTextLength Length of the alt text if it exists 
Hybrid 
Our final approach is a hybrid of the NG-tsrf-idf method and method V, which was found to be the 
best performing combination. TOKENS-trf combined with method V was also tested but the results 
were slightly worse. The goal of the hybrid method is to achieve higher performance by taking into 
account both textual and non-textual features. Our hypothesis is that the two methods will 
complement each other when aggregating their results as they rely on different kinds of features: the 
n-gram classifier might be best at classifying a certain kind of images with specific image URL 
wording, while the non-textual features classifier might be best at classifying a different kind of 
images with more informative HTML metadata.  
Evaluation 
For training we used one million images (500K small and 500K big) and for testing 200 thousand 
(100K small and 100K big). To avoid complications stemming from the class imbalance problem (one 
class represented by more instances than the other one), we used equal numbers of small and big 
images both for training and testing. In fact, we found out that in the dataset, there were 
approximately ten times more small than big images, which is to be expected, if we take into account 
all the icons and image resources used for building websites, which do not really represent content 
but rather decorative elements. 
The described supervised learning approaches were implemented based on the Weka library7. We 
performed numerous preliminary experiments with different classifiers (LibSVM, Random Tree, 
Random Forest), and Random Forest (RF) was found to be the one striking the best trade-off 
between good performance and acceptable training times. The main parameter of RF is the number 
of trees. Some typical values for this are 10, 30 and 100, while very few problems would demand 
more than 300 trees. The rule of thumb is that more trees lead to better performance; however, they 
simultaneously increase considerably the training time. 
The comparative results for different number of trees for the Random Forest algorithm are displayed 
in Table 5. In this document we include some indicative results, the detailed evaluation and results 
are presented in the aforementioned paper. As a measure of accuracy, we use the F1 score (or F-
measure), which considers both precision and recall, using a weighted average as illustrated in the 
formula: 
𝐹1 = 2 ×
𝑝𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 × 𝑟𝑒𝑐𝑎𝑙𝑙
𝑝𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 + 𝑟𝑒𝑐𝑎𝑙𝑙
 
                                                     
7 http://www.cs.waikato.ac.nz/ml/weka 
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 33 OF 139 
The first column of the table contains the method name, the second one the number of trees used in 
the RF classifier, the third one the number of features used, and the remaining columns contain the 
F-measures for the SMALL class, the BIG class and the average. The reported results lead to 
several interesting conclusions.  
Doubling the number of n-gram features improves the accuracy in all cases. For instance, for 
100 trees, the NG-tsrf-idf method with 1000 n-grams has an average F1 score of 0.885 whereas for 
2000 trees, the score increases to 0.892. The improvement is bigger for the NG method (~1.2%), 
and somewhat smaller for the NG-trf, NG-tsrf-idf and TOKENS-trf methods (~0.7%). 
Adding more trees to the Random Forest classifier improves the accuracy in all cases. For 
instance, the NG-tsrf-idf method with 1000 n-grams has an average F1 score of 0.874 for 10 trees, 
0.882 for 30 trees, and 0.885 for 100 trees. The improvement is greater for all methods when going 
from 10 to 30 trees than when going from 30 to 100 trees. Additionally, the NG method seems to be 
more influenced by the number of trees in comparison to the other methods. 
Overall the NG-tsrf-idf and TOKENS-trf have the best performance, followed closely by NG-trf. 
Importantly, the NG-tsrf-idf and the NG-trf methods perform much better than the NG method, ~5% 
and ~4% respectively, which means that selecting the features (n-grams) instead of just considering 
the $n$ most frequent ones, really makes a difference in performance. Finally, the performance of 
the non-textual feature classifier lies in between the simple NG and the rest. 
The NG-tsrf-idf variation was also applied to the TOKENS method with the aspiration of improving 
the TOKENS-trf performance, as is the case of NG-tsrf-idf . Unfortunately the employed algorithm 
favours tokens that are highly class-specific (which tend to occur much less often), resulting in many 
image URLs with empty or almost empty feature vectors. As a result, the classification performance 
in that case dropped significantly. A legitimate question is why this fact does not influence the NG-
tsrf-idf method. Our assumption is that class-specific tokens are rarer than class-specific n-grams, 
mainly because of their size: n-grams have a maximum length of 5 but there is no length limit for 
tokens. For instance, many of them are longer than 10 characters. 
To evaluate the hybrid method, we use the same test set of 200K images (100K big and 100K 
small). For the NG-tsrf-idf classifier, we use the classification model built with 100 trees because of 
its considerably higher performance and for the non-textual features classifier, the model built with 
30 trees, as the one with 100 trees is not significantly better and it requires three times more memory 
for loading. One can observe that the hybrid method outperforms all standalone methods, its best 
result being 4% higher than the NG-tsrf-idf  best result.  
Table 5: Comparative restults (F-measure) 
Method RF trees Features F1small F1big F1avg 
TOKENS -trf 10 1000 0.876 0.867 0.871 
TOKENS -trf 30 1000 0.887 0.883 0.885 
TOKENS -trf 100 1000 0.894 0.891 0.893 
TOKENS -trf 10 2000 0.875 0.864 0.870 
TOKENS -trf 30 2000 0.888 0.828 0.885 
TOKENS -trf 100 2000 0.897 0.892 0.895 
NG-tsrf-idf 10 1000 0.876 0.872 0.874 
NG-tsrf-idf 30 1000 0.883 0.881 0.882 
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 34 OF 139 
NG-tsrf-idf 100 1000 0.886 0.884 0.885 
NG-tsrf-idf 10 2000 0.883 0.878 0.881 
NG-tsrf-idf 30 2000 0.891 0.888 0.890 
NG-tsrf-idf 100 2000 0.894 0.891 0.892 
features 10 23 0.848 0.846 0.847 
features 30 23 0.852 0.852 0.852 
features 100 23 0.853 0.853 0.853 
hybrid  0.935 0.935 0.935 
 
  
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 35 OF 139 
3 Text-based relation extraction 
Extracting meaningful, short descriptions from the raw text contained in tweets can greatly assist an 
investigator in making sense out of the sheer volume of data they have to face when following an 
event via Twitter. During the second year of the project, we extended the approach proposed in D3.1 
for extracting meaningful events from tweets, in the form of subject-predicate-object sentences. In 
this section, we describe the details of the extended approach, as well as the experimental 
evaluation of our proposed methodology. 
3.1 Proposed approach 
We adopt the state-of-the-art approach to relation extraction [15] and further enhance it for the task 
of event extraction from tweets. In our approach we consider any action, which can be observed in 
the physical world, to constitute an event [16]. We assume that events are indicated by non-stative 
(dynamic) verbs. Dynamic verbs describe an action, such as 'kick', 'meet', 'visit', as opposed to 
stative verbs, such as 'believe', 'like', 'consider', etc. This relation extraction approach enables us to 
extract predicates from a sentence (corresponding to the verbs indicating events) together with their 
subjects and objects. For example, the sentence: "The match starts on Sunday" will result in the 
following relation: The match (Subject) - starts (Predicate) - on Sunday (Object). 
Objects of the relations often contain event facets that uniquely characterize events in spatial, 
temporal and social dimensions (e.g. place, date, organizers, and participants). Thus, this approach 
allows for more fine-grained event extraction as opposed to clustering or topic modeling-based 
approaches which operate with the bag-of-words model, which tend to blend together several 
lexically similar events. 
Figure 14: Relation extraction module workflow 
We have extended the initial approach to relation extraction with a few pre-processing steps in order 
to clean the input data and annotate it with named entities. After the pre-processing we extract 
relations, link them to named entities and rank according to their frequencies. The resulting pipeline 
summarizing our approach is presented in section 3.1. In the rest of this section, the different 
modules of our approach are described in details. 
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 36 OF 139 
Spam Detection 
Here we define spam as useless uninformative or malformed messages, which are unlikely to 
provide us with any meaningful information. Our goal is to pre-process the raw data from Twitter and 
deliver to the end user only useful and relevant information. Therefore, we attempt to filter out 
meaningless and misleading messages already on the first stage of our pipeline. 
In the first place, we use a freely distributed black-list of domain names8 in order to exclude tweets 
containing links that point to the untrusted web sites. Next, we calculate a “spam score” for each of 
the remaining tweets and exclude the tweets that receive the score higher than the empirically 
learned threshold value. The “spam score” is calculated as the number of spam-associated tokens 
[17] [18] divided by the total number of tokens in the tweet: 
𝑠𝑝𝑎𝑚_𝑠𝑐𝑜𝑟𝑒 =
|𝑈| + |𝐻| + |𝐿| + |𝑆| + |𝑁|
|𝑇|
 
where: 
 |U|: number of user mentions (e.g. @themichaelowen); 
 |H|: number of hashtags (e.g. #DavidGill); 
 |L|: number of web links (e.g. http://t.co/my55ZOoAko); 
 |S|: number of spam words (from the predefined list9, e.g. duty free, poker, casino); 
 |N|: number of non-word characters (e.g. %, !); 
 |T|: total number of tokens in the tweet. 
The bigger the value of the “spam score”, the more likely that the tweet contains spam. We 
conducted an experiment spanning numerous trials to choose the optimal threshold value for the 
spam score and arrived at the value of 0:74. Further one, we identified 3% of the tweets in our 
datasets as spam and, therefore, excluded them from the next stages in our pipeline. 
Linguistic pre-processing 
All the tweets that passed through the Spam Detection module, are further considered in the 
Linguistic Pre-processing module. The pre-processing steps include tokenization, user mentions 
resolution, further text cleaning and sentence splitting. 
Tokenization is used to identify the tokens that will be replaced or removed from the text, such as 
URLs, user mentions, etc. First, we exploit tweet meta-data to resolve user mentions to their 
canonical names. In particular, each tweet that contains user mentions carries a list of the 
corresponding full user names from the Twitter database. Thus, we substitute the user mentions in 
the tweet text with the corresponding full names using the tweet metadata. For example, 
@themichaelowen is resolved to Michael Owen. 
Named Entity Recognition 
In this module, we identify named entities mentioned in the text of the tweet, as well as their types. 
For example, the tweet containing the following snippet: "@DavidGill walks out of FIFA meeting in 
Sao Paulo", gets annotated with the named entities: David Gill - Person, FIFA - Organization and 
Sao Paulo -Location. 
We used Stanford Named Entity Recognizer (Stanford NER) [19] for detecting named entities in 
tweets. According to the benchmark evaluation reported in [20], Stanford NER achieves highest 
                                                     
8http://www.squidguard.org/blacklists.html 
9http://notagrouch.com/wp-content/uploads/2009/12/wordpress-blacklist-words.txt 
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 37 OF 139 
average precision on all three datasets of tweets, when compared with other state-of-the-art Twitter-
tailored algorithms. Stanford NER detects the following types of named entities: Location, Person, 
Organization, Date, Money, etc.6. Due to our pre-processing procedure we also detect the entities 
\hidden" within the user mentions and hashtags (e.g. @DavidGill). This would not be feasible, when 
applying the Stanford NER on the original tweets. 
Relation Extraction 
The core of our approach is based on extracting relations from the pre-processed tweets. Relation is 
a triple that consist of subject, predicate and object. Subject and object are entities, predicate is the 
relation between these entities. For example, the sentence: "The match starts on Sunday" will result 
in the following relation: The match (Subject) - starts (Predicate) - on Sunday (Object). 
We considered three state-of-the-art systems for the task of relation extraction: ReVerb [21], Ollie 
[22] and ClausIE [15]. ClausIE was reported to significantly outperform Ollie by the number of 
propositions extracted [6]. However, it has not been previously applied to social media data. 
Therefore, we ran our own experiments to compare the results returned by ReVerb and ClausIE. 
Subsequently, we chose ClausIE as the best-suited baseline system. 
In ClausIE relation triples are extracted from clauses, parts of a sentence that express coherent 
pieces of information [15]. The clauses are identified based on the results from the dependency 
parser that helps to reveal the syntactic structure of an input sentence. In particular, ClausIE is using 
Stanford unlexicalized dependency parser [23]. 
Additionally, ClausIE has an option to return an n-ary predicate by decomposing the object of the 
relation into several arguments. This option can be useful for extracting complex relations, that 
consist of several independent but overlapping parts, such as place and time relations. For example, 
the sentence: "The match starts on Sunday at Wembley" will result in the following relation: The 
match(Subject) - starts (Predicate) - "on Sunday", "at Wembley" (Object). 
We made several modifications to the original implementation of ClausIE in order to adapt it to the 
task of extracting the relations describing events. 
Specifically, we enforce omitting the following types of clauses from the relation extraction process: 
 conditional clauses (If-clauses), e.g. "If @Chelsea wins I will celebrate till morning!!!!!!!!" 
 clauses rooted in a stative verb, e.g. "I believe @Chelsea is the actual winner!" 
Conditional clauses are used to speculate about what might happen, what could have happened, 
and what we wish to happen. Stative verbs describe mental state of an agent, but do not signify any 
action. For example, the following verbs are stative: hate, love, believe, prefer, want, suppose, etc. 
Relation Selection 
We designed a post-processing step for selecting relations that will appear in the final results. For 
this we chose the Frequent Pattern Mining approach that helps us to reveal the recurrent information 
patterns following the assumption that input data from Twitter is often abundant and redundant. 
Additionally, we employ the following heuristic technique: for the relation to be selected it has to 
contain popular (frequently occurring) named entities. In this way we get rid of the trivial results, e.g. 
"I - ate pizza - for breakfast", but retain the relations such as: "President Obama - ate pizza - for 
breakfast", if they are reported by a considerable number of tweets. 
Therefore, we combine the results from Relation Extraction (RE) and Named Entity Recognition 
(NER) modules produced on the previous stages. In particular, we select only those relations that 
contain named entities in subject and/or object of the relation. The intuition behind this approach 
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 38 OF 139 
enriching relations with NER annotations is that events in real-life are often associated with the 
corresponding named entities: dates, places and participants. Hints about importance of the relations 
and named entities are given from their frequencies count. We assume that widely discussed news 
are more likely to be of importance and interest to the users of our system. Therefore, in order to link 
NER and RE results we identify frequent named entities and then select frequent relations, in which 
these entities occur. We use several approaches to select relations between the named entities 
described below. 
Firstly, we detect the named entities that occur most frequently in the tweets (~10 entities for each of 
the datasets), e.g. Chelsea, Drogba, Ramires. We also identify the most frequently co-occurring 
pairs of named entities (~5 pairs per dataset), e.g. Chelsea and Liverpool, Putin and Ukraine. Then, 
we identify the following relations that hold between named entities: 
 Relations in which the most frequently occurring entities appear in subject or object of the 
relation; 
 Relations that hold between pairs of the most frequently co-occurring entities; 
 Relations for every combination of entity types pairs from the set: [Person, Organization, 
Location, Date], e.g. between Person and Organization, Person and Person, Location and 
Organization, Person and Date etc. 
Finally, we calculate the support for each of the selected relations, i.e. number of tweets from which 
the same relation was extracted, and use it for ranking of the relations. The topmost relations are 
reported in the final results. 
3.2 Experimental evaluation 
Datasets 
We conducted experiments using three different Twitter datasets (see Table 6). All datasets are 
centered on one or several major events discussed on social media. We have deliberately selected 
the datasets containing event-related tweets for our evaluation with the goal to uncover the details 
surrounding these events using our approach. 
The FACup dataset was created within the Social Sensor project and covers the events during the 
last match of the Football Association Challenge Cup [24]. 
The SNOW dataset [10] is an attempt to capture the footprint in the social media regarding several 
important international events: uprising in Ukraine (#ukraine, #euromaidan), protests in Venezuela 
(#Venezuela), major Bitcoin exchange theft (#bitcoin), etc. The third dataset was collected in June 
2014 and contains ~270.000 tweets that were extracted using the hashtag #WorldCup2014. 
Table 6: Datasets for Evaluation of the relation extraction module 
 
 
 
Evaluation method 
We manually evaluated the results by annotating the relations returned on the last stage of our 
pipeline. Each of the annotators (3 in total) independently considered perceived correctness and 
usefulness (importance) of the relations by looking up the original text of a sample tweet, from which 
the relation was extracted by the system. 
Dataset # Tweets Hashtags 
FA Cup ~20.000 #FACupFinal 
SNOW ~1.000.000 #ukraine, #euromaidan, #Venezuela, #bitcoin 
World Cup ~270.000 #WorldCup2014 
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 39 OF 139 
The relation was marked as Correct, if the information it provides naturally follows from the original 
text of the tweet and does not contradict the message conveyed in it. Negation handling is a good 
example for potential errors in the results returned by the system. If the original tweet reports, that 
Chelsea did not play better than Liverpool, the relation has to communicate the same fact and not 
the opposite. For example, Chelsea - play better - than Liverpool relation should be marked as 
Incorrect in this case. 
Furthermore, all correct relations were further evaluated with respect to perceived importance for the 
end user of the system. The importance of a relation is harder to evaluate than its correctness, 
because of the complexity and subjectivity in the notion of importance with respect to an information 
piece. In general, a relation is considered Important, when it is perceived as being descriptive and 
potentially useful. Meaningless and uninformative relations are marked as Not important, 
respectively. Collective discussion of the individual annotations resulted in a consensus and a single 
final evaluation table was constructed. Afterwards, we summarized our evaluation results by 
counting the number of relations for each of the classes: 
Correct & Important, Correct & Not important and Incorrect relations (see Table 7). We calculated the 
ratios and the total number of evaluated relations separately for each of the datasets. The last row of 
the evaluation table highlights the average precision values across the three datasets. 
Table 7: Precision of the evaluation results: fraction (total) of relations 
Dataset Incorrect Correct 
Not important Important 
FA Cup 0.17 (8) 0.17 (8) 0.66 (32) 
SNOW 0.1 (21) 0.14 (32) 0.76 (168) 
World Cup 0.1 (18) 0.19 (35) 0.71 (134) 
Average 0.12 (47) 0.17 (75) 0.71 (334) 
 
The average precision of our approach was estimated to be 88% taking into account all correctly 
extracted relations. However, less than 3/4 of the relations returned by the system were considered 
as potentially valuable for the end users of the system (see Correct & Important in Table 7). 
The most frequent relations that were selected using our approach from FA Cup dataset are listed in 
Table 8. These five relations provide a short summary of the event by revealing the names of the 
teams, the place where the game took place, the winner and the final score, as well as the scorer. 
The timestamps of the tweets can disambiguate the mentions "now", reveal date of the event and 
indicate the "hot spots" on the game timeline, such as the last relation in Table 8. 
Relations extracted from the SNOW dataset are less homogeneous, containing various political 
statements, business and sport announcements, as well as snapshots of historical events. Sample 
relations (with their support): Ukraine's leaders - warn - "of Crimea separatism threat" (106); Chelsea 
fans - attending - "the Galatasaray match", "on 26 Feb" (84). The World Cup dataset is another noisy 
collection containing many tweets not related to the football championship. Nevertheless, the three 
top-most relations reveal the major conflict in the football association: director David Gill - walks out - 
"of FIFA meeting in Sao Paulo" (902); director David Gill - says - "Sepp Blatter should stand down" 
(901); FA Vice-Chairman David Gill - calls on -"Sepp Blatter not to stand for re-election as FIFA 
President" (481). 
In general, due to our broad definition of 'event' (as any kind of action reflected in a physical world) 
relations can be extracted from virtually any collection of tweets. However, in order to achieve 
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 40 OF 139 
comprehensive results the tweets need to be previously clustered according to the common topic, 
e.g. using a set of hashtags. 
We performed only limited experimental evaluation for the proof-of-concept of our approach and 
cannot quantitatively compare our results with other approaches to event extraction. Moreover, the 
relation extraction algorithm is currently computationally rather expensive, which might prevent us 
from running the system on Twitter stream data in real time. 
Table 8: Results from FA Cup dataset 
Subject Predicate Object Count Sample tweet 
The Chelsea 
players 
are throwing "Robbie Di Matteo 
high in the air" 
129 
RT @chelseafc: What 
celebrations! The Chelsea players 
are throwing Robbie Di Matteo 
high in the air. And catching ... 
Chelsea Chelsea have 
won 
"17 major trophies", 
"now" 
58 
RT @chelseafc: Chelsea have 
now won 17 major trophies. 
We've caught Tottenham who 
are on the same total. 
Liverpool are out "for the second half" 27 "for the second half" 
Chelsea beat "Liverpool 2-1 to 
win the FA Cup at 
Wembley" 24 
RT @premierleague: Chelsea 
beat Liverpool 2-1 to win the 
FA Cup at Wembley, their 
fourth win in six years in the 
competition. #cfc #lfc ... 
Liverpool is "much", "pretty", 
"giving every 
Chelsea fan a heart 
attack right now" 
21 
RT @espn: Liverpool is pretty 
much giving every Chelsea 
fan a heart attack right now: 
http://t.co/MGxAkv94 
 
  
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 41 OF 139 
4 Multimedia Summarization  
From the perspective of a journalist following an unfolding event through a micro-blogging platform 
(e.g., Twitter), it is easy to become lost by the sheer amount of data included in the process. In this 
section, we present our research in the fields of multimedia analysis and linking, with respect to 
concisely grouping and summarizing large sets of tweets, using their implicit inter-relations, for users 
to easily evaluate and manage at a glance. The challenge for this component is twofold: 
 The application of the proposed algorithms to large scale settings  
 The application in online settings, the continuous processing of incoming content in an 
incremental fashion 
The purpose of the multimedia summarizer is to select a set of representative images from a set of 
images related to a specific and broad topic. Our intention is to use an event-related set of social 
media items, to create a visual summary that describes the main highlights of the event. As visual 
summary we define a set of images that are highly relevant to the event and contain visually, the key 
aspects of the event. Note that, although the goal is to select a subset of images to form a visual 
summary, Multimedia Summarizer makes use of all the available social media items, even those that 
do not carry any associated multimedia content. Multimedia Summarizer is a batch component which 
can be called on demand on a per-collection basis in order to apply the summarization algorithm on 
the items of the collection of interest. 
First, we calculate the significance of each message, based on the social attention it receives (i.e. 
the number of reposts). Then, we apply topic modeling to discover the underlying aspects of the 
event and assign messages to the detected topics. Next, we calculate the relevance of the message 
to the topic it belongs to. Finally, we use DivRank, a graph-based ranking algorithm, to obtain a set 
of relevant and significant messages that at the same time maximize the coverage of the event by 
selecting the maximum possible number of topics and minimize redundancy across selected 
messages. An overview of summarization process is depicted in Figure 15. This work is described in 
detail in a paper presented in ICMR 2015 (ACM International Conference on Multimedia Retrieval). 
 
Figure 15 Overview of the summarization process used by MultimediaSummarizer 
The following sections present the state of the art in event summarization, describe our contributions 
and present a comparative evaluation of our proposed approach on real-world data. Finally, 
subsection 4.2 describes our contribution towards an incremental implementation of a multimedia 
clustering algorithm that we designed for the needs of REVEAL. 
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 42 OF 139 
4.1 Related Work 
Text-based Event Summarization 
A substantial body of work exists in literature on the problem of textual summarization of micro-blogs, 
which is a special case of the multi-document summarization (MDS) problem. One of the first MDS 
approaches relies on the computation of centroids, based on textual content. Namely, the summary 
of a set of documents, represented by tf∙idf vectors, consists of those documents that are closest to 
the centroid of the set [25]. Graph-based approaches have also been proposed to detect salient 
sentences from multiple documents, with LexRank [26] being the most notable among them. First, a 
graph of sentences is constructed, with the textual similarity between two sentences serving as the 
connection between them. Then, the saliency of each sentence is calculated using some centrality 
measure, such as the Eigenvector Centrality or the PageRank algorithm. 
However, the brevity of the text, the existence of noisy documents, and the diversity of the underlying 
topics in a set of micro-blog documents make the summarization problem much more challenging 
compared to the traditional MDS. In addition, the temporal dimension that arises from the time-
stamped micro-posts and the social interaction between users in these platforms is totally ignored by 
the aforementioned methods. To this end, a lot of methods [27] [28] [29] [30] have been proposed in 
the literature, which incorporate not only the textual information of the documents, but also their 
temporal dimension and their social features. The core idea in the majority of previous works is the 
segmentation of the document set into coherent topics or sub-events and the selection of the most 
"representative" documents in each segment. Other works focus on the creation of visualizations that 
summarize the key concepts of events, as presented through social media. TwitInfo [2] is a system 
for summarizing events on Twitter by using a timeline-based display that highlights peaks of high 
activity. Alonso and Shiells [31] create timelines for football games, annotated with the key aspects 
of the event, in the form of popular tags and keywords. Dork et al. [32] propose an interface for large 
events comprising several visualizations (e.g., tag clouds) to support their interactive presentation.  
Multimedia Event Summarization 
Taking into account the increasing use of multimedia content in micro-blog platforms, there have 
been many studies that consider visual information along with the textual content of micro-blog 
messages. Bian et al. [33] proposed a multimodal extension of LDA that detects topics by capturing 
the correlations between textual and visual features of micro-blogs with embedded images. The 
output of this method is a set of representative images that describe, in a visual way, the underlying 
event. A slightly different problem is tackled by Lin et al. [34]. Unlike other methods that generate 
summaries as sets of messages or images, that method aims to create a storyline from a set of 
event-related multimedia objects. A multi-view graph of objects is constructed, where two types of 
edges capture the content similarity, visual and textual, along with the temporal proximity among 
objects. Then a time-ordered sequence of important objects is obtained via graph optimization. 
McParlane et al. in [35] propose a method to select and rank a diverse set of images with high 
degree of relevance to the event. An interesting part of their work is the use of external websites as 
sources of multimedia content, when the amount of embedded images is insufficient for the creation 
of a meaningful visual summary. They use visual features first to discard irrelevant images and 
images of low quality, and then to detect near duplicates among them to increase diversity. Then, 
they apply several ranking methods to select a small number of images that describe the event. To 
our knowledge, there is a lot of space for improvements on the problem of multimedia 
summarization, as most of the related methods are mainly based on the textual and temporal 
information and ignore the richness of visual and social signals in social media. To this end, our 
proposed framework incorporates textual, visual, temporal and social features to support the 
generation of visual summaries from event-focused social media content. 
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 43 OF 139 
4.2 Approach Description 
Representation of Social Media Items and Aggressive Filtering  
We represent each message m as a tuple {id, ts, C, E, u, p} where id is a unique identifier of the 
message, ts its publication time, C the content, E is the set of detected named entities and mentions 
contained in this message, u an identifier of the posting user, and p the number of times that this 
message has been reposted. Content C consists of two parts: textual and visual. The textual part of 
the content (Ctext) is represented as a tf∙idf vector vm, where the tf part is the frequency of a term in 
the message normalized by the maximum frequency in the message. Due to the short length of the 
documents in microblogging platforms, this component often equals to one. The inverse document 
frequency (idf) of each term is calculated over the whole set of messages. Note that we extend tf∙idf 
by using a constant boosting factor b to give more weight to terms that are expected to be 
particularly relevant for the sub-event, i.e. named entities and mentions. In other words, if a term w is 
a recognized named entity, its weight is given by b∙tfw∙idfw. The intuition is that two messages that 
share the same set of named entities or mention the same user, have a higher probability of 
belonging to the same topic. The visual part (Cvisual) is optional, as not all items are associated with 
multimedia. In case they are, we represent them using the combination of Speeded Up Robust 
Features (SURF) with the VLAD scheme as implemented in [36]. 
Content quality plays a key role in the generation of informative, but concise summaries. To this end, 
we first apply a set of heuristic rules to discard a significant amount of the initial set of event-related 
messages that are considered noisy and of low quality. More specifically, we apply two types of 
filters on the messages. The first is based on the textual content and is applied on items that do not 
contain any embedded image. The second one is based on visual features and is applied only to 
messages with embedded images. Regarding text-based filtering, we discard a message if it has 
very short text (e.g., less than six terms) and mentions more than three users, or contains more than 
three URLs or hashtags. The core idea behind the aforementioned filtering rules is that messages of 
that type do not carry enough textual content to be usable in a summary. Also the co-existence of a 
URL with many popular hashtags or mentions is a strong indication that the corresponding message 
is spam that aims to redirect the user to the website pointed by the URL. Also, in order to discard 
messages that have an incorrect or incomplete syntactic structure, we apply Part-Of-Speech tagging 
and keep only messages that match the following regular expression: 
regex =  (determiner?  adjective ∗  noun +  verb) + 
Thus, we keep only items containing at least one sentence that consists of at least a noun followed 
by a verb. Determiners and adjectives are optional. Finally we keep only original messages and 
discard all the reposts. However, for each original message we keep the number of times it has been 
reposted by other users, and we use it as a signal of the social attention it receives over time. 
Regarding visual filtering, first we discard small images, i.e. images having width or height less than 
200px. To discard memes, screenshots and images having heavy text we use the semi-supervised 
method presented in [37] to build a model that detects these types. Typically, using a set of labelled 
and unlabeled images, represented as normalized VLAD vectors, we consider a similarity graph and 
we construct the Approximate Laplacian Eigenmaps (ALE) of this graph. More precisely, each ALE 
vector is a low dimensional representation of an image that captures in a compact way the position 
of the image in the manifold of the similarity graph. Intuitively, images of the same type will share the 
same neighbors and subsequently will have similar ALE representations. Finally, we use the set of 
labeled images and their ALEs to train an SVM classifier that classifies images into four types: 
memes, screenshots, images with overlay text, and real photos. 
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 44 OF 139 
Multigraph generation 
Given a set of message M = {m1,m2, ..., mn} we construct a multi-graph GM = {V, Etextual, Evisual, 
Esocial, Etime} where the vertex vi∈V corresponds to message mi. Etextual is a set of undirected 
edges expressing the textual similarity between nodes. For textual similarity we used the well-known 
cosine similarity between the corresponding tf∙idf vectors. Evisual is a set of undirected edges that 
represent the visual similarity between messages with images. Visual similarity is based on the L2 
distance between the corresponding SURF+VLAD vectors. Note that we add an edge in Etextual or 
Evisual, only if the textual or visual similarity between the corresponding nodes is higher than 
thtextual or thvisual respectively. Introduction of textual and visual thresholds at this step aims to 
prune the graph, make it more sparse, and avoid the addition of "noisy" associations between nodes. 
The directed unweighted edges of Esocial are based on the social interactions between users: we 
connect two messages mi and mj , with a directed edge from mjto mi, if message mj is a direct reply 
to mi. Finally, the directed edges Etime are based on the temporal proximity between messages. The 
temporal proximity (TS) between two messages mi, mj, published with difference Δt = |ti - tj|, is 
modeled by using the Gaussian kernel function: 
𝑇𝑆(𝛥𝑡) = exp (
−𝛥𝑡2
2𝜎2
) 
Parameter σ controls the spread of the sub-events within the main event. In general, the optimal 
value of σ depends on the type of the event, because sub-events are wider in events of certain 
types, thus requiring a higher value for σ and vice versa. The direction of an edge is from mj to mi, 
meaning that message mj is posted after mi. 
Visual Deduplication 
As mentioned before, we handle de-duplication of messages by keeping only original messages and 
discarding explicit reposts. However, there are duplicates for which there is no explicit connection. 
This is more obvious in case of visual content, as users can post the same image or near duplicates 
found in different sources, e.g., different news web sites. To handle this high degree of visual 
redundancy we use the Clique Percolation Method, presented in [38], to find sets of messages that 
are visual duplicates. In particular, we use the CPM on sub-graph Gvisual = {V, Evisual} to generate 
cliques of visual duplicates. We represent message cliques in a similar manner as single messages. 
More specifically, message clique mc is a tuple {id, Mmc, ts, C, E, p} , where id is a unique identifier 
of the clique, Mmc is the messages of the clique, ts is the mean value of publication time of the 
messages in Mmc, and p is the aggregated value of reposts of each message. Regarding the textual 
part of the content we use a merged tf∙idf vector Vmc. 
Topic Detection 
To detect the topics on a main event we opted for a graph clustering algorithm, namely the Structural 
Clustering Algorithm for Networks (SCAN) [39]. SCAN is applied on a graph G = {V, E}, where nodes 
correspond to the filtered set of event-related messages and message cliques and edges E 
represent the content-based similarities between adjacent nodes. Apart from content similarity, we 
also use social interactions to add edges that enhance the density of inter-topic links. Namely, we 
connect two messages if the one is a reply to the other, as the probability that these messages 
belong to the same topic is very high. We apply SCAN on the message graph, to identify dense sub-
graphs of messages. These sub-graphs represent the topics that exist in the stream of messages. 
Hence, each topic is represented as a set of highly connected messages in the graph. Once topics T 
are detected, we use the messages Mi associated with each topici  to calculate a merged tf∙idf vector 
Vi that describe its content, in a similar manner to how we calculate merged vectors for cliques. 
However, a substantial amount of messages is kept outside of the detected clusters. These 
messages are divided into two categories, hubs and outliers. Hubs are bridges to more than one 
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 45 OF 139 
clusters, while outliers are messages that are not related to any of the clusters. Some of these 
messages can be considered as non-informative messages that cannot contribute valuable 
information to the summary. However this is not the case for all of them, as some messages, despite 
not belonging to any cluster, may include valuable information that attracts a lot of social attention. 
This is more obvious in case of messages with images. Such messages could have little textual 
information, therefore very low textual similarity to the other messages. Moreover, visual content 
could be different, even between messages of the same topic. Therefore, it is likely that important 
images could be left un-clustered. To this end, we do not discard the unassigned messages, but we 
form single item clusters and use them in the ranking process, which will be described in the 
following. 
Message Selection and Ranking 
Our goal is to calculate an overall importance score for each of the messages or message cliques of 
the filtered set, and rank them according to it. The importance score of a message m or clique mc is 
a combination of two factors: a) the social attention it receives over time, and b) the significance of 
the topic it may belong to. 
Social Attention. The popularity of a message or clique, i.e. the number of the reposts it receives 
over time, can be considered as a measure of the social attention it receives. A high value of social 
attention, indicates implicitly an important and hence representative message regarding the event. 
We measure social attention using the following equation: 
𝑆𝑎𝑡𝑡(𝑚) =  log2 (𝑝 + 𝜆) 
where p is the number of reposts and λ a smoothing parameter. We opted for the use of a 
logarithmic function due to the fact that the number of reposts in social media follows a power law 
distribution. 
Topic Coverage. The association of a message with a detected topic is a strong indication of its 
importance. Namely, a message that is part of a topic contributes valuable information about an 
aspect of the event and should get a high importance score. However some messages of a topic are 
more representative than others. Also some topics are more significant than others, hence 
messages from these topics should receive higher scores. To this end, we quantify the topic 
coverage of a message using the following equations: 
𝑆𝑐𝑜𝑣(𝑚) = cos(𝑣𝑚 , 𝑉𝑖) ∗ 𝑆(𝑡𝑜𝑝𝑖𝑐𝑖) 
𝑆(𝑡𝑜𝑝𝑖𝑐𝑖) = exp (
|𝑀𝑖|
𝑚𝑎𝑥𝑘∈𝑇(|𝑀𝑘|)
) 
Its first part captures the relevance to the topic and is calculated as the textual similarity of m to the 
topic centroid Vi. Its second part captures the significance S of the underlying topic, so that 
messages from largest clusters get higher scores. 
The overall significance score of a message or clique is the product between its social attention and 
the respective topic relevance 
𝑆𝑠𝑖𝑔(𝑚) = 𝑆𝑎𝑡𝑡(𝑚) ∗ 𝑆𝑐𝑜𝑣(𝑚) 
Image Ranking and Diversification 
The motivation behind computing the significance score is to generate a diverse set of images in the 
top ranked positions of the summary. However, there are images that are considered relevant to an 
event and extremely popular, but they are not specific to the event of interest. For example, an 
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 46 OF 139 
image depicting the flag of Ukraine could be considered to be relevant for an event about the 
Ukraine crisis, but it does not provide important information about the event. To tackle this, we 
introduce a specificity factor that penalizes such images. Image specificity is a measure of how much 
information a specific message provides for a specific event; in other words, whether the message is 
common or rare across all topics of the event. In a similar manner as [35], we use the de-duplication 
technique presented before to measure the number of topics |TI| that contain an image I. Then we 
calculate an idf-like score for each image using the following equation: 
𝑆𝑠𝑝𝑒𝑐(𝐼) = 𝑙𝑜𝑔(
|𝑇|
|𝑇𝐼|
) 
where |T| is the total number of topics in the event and |TI| is the number of topics containing image I. 
Finally, the image selection score S(I) of image I is the product of the significance score Ssig and the 
image specificity score Sspec. 
To incorporate diversity into the score calculation, we employ DivRank [40], a variant of PageRank 
that aims at diversity. We use the multi-graph GM that was created initially, to get a directed sub-
graph GV = {VV, EV}. Vertices VV are the subset of messages that contain an embedded image and 
will be used in the generation of a visual summary. For the creation of the set EV , we combine the 
two sets Evisual and Etime. In particular, for each pair of vertices vi, vj∈ VV , we create a weighted 
directed edge e∈  EV with the same direction as the corresponding edge in Etime. The weight of this 
edge is the product of visual similarity and time proximity between the adjacent vertices. To ensure 
convergence of DivRank, we normalize the weights of the edges, such that the sum of the adjacent 
out-edges of each message equals to one. To calculate the new selection score, we apply DivRank 
using the following iterative scheme: 
𝑟 = 𝑑𝑊−1𝑟 + (1 − 𝑑)ℎ⃗⃗ 
𝑊 = 𝑑𝑊𝑟 + (1 − 𝑑)ℎ⃗⃗ 
Vector 𝑟 holds the DivRank scores and d is a dumping factor that controls the impact of the initial 
score to the re-ranking procedure. The initial value of matrix W is the adjacency matrix derived from 
the directed graph GV . Also, instead of using a uniform value for priors ℎ⃗⃗, we use the value of the 
calculated score of each image in the graph. Specifically, the prior h[i] of the ith node in the graph that 
corresponds to image Ii is h[i] = S(Ii). 
4.3 Experiments and Results 
Dataset and experimental setting 
To evaluate the proposed framework, we conducted a set of experiments in the dataset of McMinn et 
al. [41] that contains more than 500 events of different domains. We used the 50 largest events in 
terms of tweets, as in the work of McParlane et al. [35]. These events range from sports events, e.g., 
the Sochi winter Olympics, to political events such as the Ukraine crisis and Venezuelan protests. 
The dataset contains 364,005 tweets in total, while each event is associated with 4730 tweets on 
average. However, due to suspended accounts and deleted messages we managed to get only 
296,160 of these tweets. About 3,51% of these, i.e.12,772 tweets, contain an embedded image. 
In [35], the authors used CrowdFlower10 to create relevance judgments for the top five images 
selected for summarization for each of the 50 events. This resulted in the generation of judgments 
                                                     
10 http://www.crowdflower.com/ 
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 47 OF 139 
for a very small percentage of the images in the dataset. To this end, we follow the same approach 
as [35] to create relevance judgments for the union of images selected as summaries by all the 
methods used in the evaluation. In order to have a better insight into the performance of the methods 
we selected 20 images for each event. We asked from a group of human annotators to evaluate how 
relevant and representative are the selected images to the corresponding event. We ensured that 
each pair received three judgments at least, from different users. The group of annotators comprised 
20 persons 24-32 years old, educated in the field of computer science, having some experience in 
the use of Twitter and social media. The task given to annotators was the following: 
Task Description: You are presented with an image and an event title (describing a "trending" topic 
in Twitter). For each image and event title, you are asked to answer the following question: 
Question: Is this image relevant to the event? 
Possible Answers: 
1. The image is clearly not relevant to the event. 
2.  The image is probably not relevant to the event, but I am not entirely sure. 
3. The image is somewhat relevant to the event, but I have my doubts on whether I would like 
to see it in a photo coverage of the event. 
4. The image is clearly relevant to the event, and I would like to see it in a photo coverage of 
the event. 
For the text representation, we used several open source projects to analyze the text of the tweets. 
For tokenization we opted for the StandardAnalyser provided by Lucene11, which performs well in 
English text. For named entity detection we used the Stanford NER library with the default 3-class 
model. For part of speech tagging we used the Stanford POS Tagger, but we opted for the Twitter-
specific POS model from the ARK research group12. For visual features, we extracted Speeded Up 
Robust Features (SURF) from each image of the dataset. Then we used four codebooks of 128 
visual words (in total 512) to quantize each descriptor and used the VLAD scheme to aggregate the 
descriptors of each image into a single vector of 64x512 = 32,768 dimensions. Finally, we used PCA 
to create a 1024-dimensional L2-normalized reduced vector that represents the visual content of the 
image. 
For the generation of multi-graph GM, we retrieve the k = 500 nearest neighbors of each message in 
terms of textual, visual and temporal similarity. The visual and textual similarity thresholds were 
empirically set to 0.5 and 0.6 respectively. Parameter σ2 of the temporal kernel was empirically set to 
24 hours as most of the important sub-events in the dataset last less than a day. In other words, the 
temporal proximity between tweets in the same day is more than 0.6. In the topic detection step, we 
set the parameters of SCAN to μ= 2 and ε= 0.65. Finally, in the ranking step with DivRank we set d = 
0.75 to the most of the experiments.  
Evaluation metrics and baselines 
We applied multimedia summarizer to the dataset to generate a representative summary for each of 
the contained events. In particular, we ranked the images according to their DivRank score and kept 
the top N as the summary. We evaluated the average performance of our method in a similar 
manner as [35] by calculating the following metrics: 
                                                     
11 http://lucene.apache.org/core/ 
12http://www.ark.cs.cmu.edu/TweetNLP 
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 48 OF 139 
 Precision (P@N): The percentage of images among the top N that are relevant to the 
corresponding event, averaged among all events. We calculate precision for N equal to 1, 5, 
and 10. 
 Success (S@N): The percentage of events, where there exist at least one relevant image 
amongst the top N returned, for N=10. 
 Mean Reciprocal Rank (MRR): Computed as 1/r, where r is the rank of the rth relevant 
image returned, averaged over all events. 
 α-normalized Discounted Cumulative Gain(α-nDCG@N): measures the usefulness, or 
gain, of the returned images based on their position in the summary (N=10). 
 Average Visual Similarity (AVS@N): measures the average visual similarity among all 
pairs of images in the top N selected images, averaged over all events. Lower AVS values 
are preferable since they imply higher diversity in terms of visual content. 
We compare the proposed scheme with several methods for image ranking. Note that we applied the 
same filtering and de-duplication steps to all methods. More specifically, we evaluated the following 
summarization methods: 
Random: randomly selects N images from the (filtered) set of images as the summary set. 
 MostPopular: picks up the most popular images in terms of reposts. This corresponds with 
ranking based on the Satt score. 
 LexRank: uses the graph G = {V, E} and ranks the nodes using the LexRank algorithm [26]. 
Then, it selects the top N nodes that contain images. 
 TopicBased: selects the most relevant messages from the most significant topics according 
to the score Scov. 
 P-TWR: ranks images in descending order using the weighting scheme described in [35]. 
 S-TWR: groups the tweets of each event into sub-clusters and select the highest ranked 
tweet of each cluster using the weighting scheme of [35]. 
Results 
Table 9 contains several precision-oriented metrics for both the proposed and the competing 
methods. Not surprisingly, the worst results for all the metrics are those of Random selection. 
Regarding P@N the best results were achieved from the Multimedia Summarizer. For P@1, 
popularity-based methods, such as Most-Popular and P-TWR, achieved very good results as would 
be expected. This means that the image having the highest value of popularity, has a higher 
possibility of being relevant to the event. However, the performance of these two methods drops 
significantly for P@5 and P@10. This is explained by the fact that although some image might be 
considered as irrelevant or marginally relevant, it could still attract the attention of OSN users for a 
number of other reasons (e.g., it could be funny), and would therefore be highly ranked by popularity-
based methods. Success for the top 10 retrieved images is high for all methods, even for the 
Random one. However, even in this case our method achieves a better value of S@10. The average 
mean reciprocal rank (MRR) is also higher for our method, with the popularity-based method 
achieving the next best results. Note that the average performance for this metric for the popularity 
based methods is benefiting from the cases that the most popular image is relevant. This mainly 
occurs when the number of reposts of an image gets extremely high values, e.g., hundreds or 
thousands of reposts. However, in events where there are no such images the performance drops 
significantly. In contrast our method handles successfully such cases, as it does not solely rely on 
the popularity of images, but also considers their association with the underlying topics. 
 
 
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 49 OF 139 
Table 9: Comparison of summarization methods in terms of precision. 
Method P@1 P@5 P@10 S@10 MRR 
Random 0.391 0.400 0.405 0.800 0.562 
MostPopular 0.522 0.469 0.446 0.848 0.669 
LexRank 0.456 0.452 0.420 0.847 0.611 
TopicBased 0.457 0.473 0.469 0.847 0.620 
P-TWR 0.521 0.486 0.437 0.826 0.673 
S-TWR 0.478 0.452 0.435 0.869 0.661 
Multimedia 
Summarizer 
0.587 0.518 0.544 0.913 0.728 
 
Table 10 presents a comparison among methods in terms of their diversity performance. According 
to it, the Multimedia Summarizer achieves the best value of α-nDCG@10, with S-TWR having the 
second best performance. This indicates that the use of the DivRank algorithm resulted in a more 
diverse set of relevant images compared to the other methods. Compared to the S-TWR method that 
aims to achieve diversity by using clustering of images, our method achieves an α-nDCG score that 
is improved by a factor of 7%. It is noteworthy that this improvement is achieved without sacrificing 
precision, as P@10 compared to S-TWR is also improved by 25%. In case of average visual 
similarity between images the best result is obtained by S-TWR. Our method has somewhat worse 
performance in terms of AVS@5, where it is ranked second, while for AVS@10, it is ranked third. 
The worst results in terms of AVS are obtained using LexRank. This is reasonable as LexRank is 
based on the PageRank algorithm, and hence it favors images that are highly connected, i.e. images 
that are highly similar in terms of visual content. One should be cautious regarding the interpretation 
of AVS: although it is a reasonable measure of diversity, it is solely based on the use of visual 
features, hence it might not be able to capture the users' perception. In addition, it is expected that 
the inclusion of irrelevant images in the set of selected, would result in lower values for AVS, but this 
is obviously not desirable 
 
Table 10: Comparison of summarization methods in terms of diversity 
Method α-nDCG@10 AVS@5 AVS@10 
Random 0.657 0.024 0.019 
MostPopular 0.717 0.022 0.018 
LexRank 0.685 0.081 0.056 
TopicBased 0.689 0.035 0.027 
P-TWR 0.717 0.020 0.016 
S-TWR 0.722 0.011 0.010 
Multimedia 
Summarizer 
0.774  0.018  0.021 
 
The events in the test dataset belong to six categories, as shown in Table 11. Each of these 
categories has different characteristics, thus the performance of our method differs among them. For 
example, the Arts & Entertainment category is more prone to duplicate messages and images, e.g., 
tweets with images of celebrities shared by users. The best P@10 measure is obtained for events 
about Science &Technology, but this should be taken with caution, as this category contains very 
few events. The second best P@10 is obtained for events about Arts & Entertainment. This can be 
explained by the fact that these events refer mostly to celebrities and the corresponding images 
usually depict them in a manner that is relevant to the event. Regarding average visual similarity, the 
best value is achieved for events about disasters & accidents. This is easily explained, taking into 
account that images of this type, e.g., earthquakes, can be very different in terms of their visual 
information even in cases they refer to the same event. 
 
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 50 OF 139 
Table 11: Performance of Multimedia Summarizer across different event categories 
Category P@10 α-nDCG AVS@10 
Law & Politics 0.536 0.729 0.047 
Arts & Entertainment 0.700 0.721 0.048 
Science & Technology 0.800 0.869 0.059 
Disasters & Accidents 0.450 0.492 0.013 
Sports 0.500 0.624 0.025 
Miscellaneous 0.386 0.606 0.053 
 
Finally, we study how parameter d of DivRank affects the precision and diversity of Multimedia 
Summarizer, using different values of d, from 0 to 1, and calculating P@10, S@5, MRR and α-
nDCG@10 for each of them. The results are depicted in Figure 16. The worst results for all metrics 
are obtained for d = 0. Essentially, in this marginal case, the re-ranking procedure of DivRank is not 
performed as the first part of Equations 9 and 10 is equal to zero. The best results are achieved for 
0.7 > d > 0.8, but even for d > 0.8 the performance remains almost steady for most of the metrics. 
The slight decrease for d > 0.8 can be explained by the fact that for such extreme values of d, 
DivRank attempts to create a more diverse set of images, thus it is more likely to introduce some 
irrelevant images in the top ranks of the result set. 
 
Figure 16 Effect of dumping factor d on P@10, S@5, MRR and α-nDCG@10 
 
  
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 51 OF 139 
5 Fake Tweet Detection 
Recent years have seen a tremendous increase in the use of social media platforms such as Twitter 
and Facebook as a means of sharing news content and multimedia. The simplicity of the sharing 
process has led to large volumes of news content propagating over social networks and reaching 
huge numbers of readers in very short time. Especially multimedia content (images, videos) can very 
quickly attract a huge audience, i.e. become viral, due to the fact that they are instantly consumed. 
Given the speed of the news spreading process and the competition of news outlets and individual 
news sources, as well as the lack of adequate tools of automated verification, it is not at all trivial to 
avoid the spread of fake media content. In particular, when a news event breaks (e.g. a natural 
disaster), and new information is of primary importance, news professionals turn to social media 
(largely on Twitter) to source potentially interesting and informative content. It is exactly this setting, 
when the risk of fake content becoming viral is the highest. 
The consequences of fake content reaching a very large part of the population can be quite severe. 
For example, fake images circulated on social media (Figure 17 illustrates two examples), when the 
Malaysia Airlines passenger flight disappeared on 8th March. Given the increasing amounts of fake 
content on the Web and the severe consequences stemming from its uncontrolled spreading, there 
is a profound need for means of identifying and debunking such content. However, the lack of 
available information for assessing the veracity of user-generated content makes the problem of fake 
detection very challenging. Notably, “traditional” digital media verification techniques employed by 
journalists13, e.g. looking into the Exif metadata of content or getting in touch with the person that 
published it, are often not possible or very slow due to the characteristics of social platforms. 
 
Figure 17: Fake media content spread during the Malaysian airlines breaking news story 
Motivated by the above limitations, there has recently been an effort to build automatic tools that can 
help with the detection of fake content. One of the first attempts [42] has adopted a supervised 
learning approach, in which tweets and the associated accounts are represented with a set of 
informative features (e.g. length of tweets, number of hashtags, etc.), and a number of tweets known 
to be fake and real are then used to train a classifier to distinguish between fake and real. In fact, 
when using content from an event Ex to train fake detection classifiers and content from a completely 
different event Ey to test their accuracy, we found that the observed detection accuracy drops 
significantly, indicating very poor generalization behaviour [43], and hence limited utility of the 
devised approach in real-world settings. 
                                                     
13http://verificationhandbook.com/ 
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 52 OF 139 
To address the identified limitations, in this work we introduce a robust machine learning framework 
for detecting tweets containing fake multimedia content. More specifically, our contributions include 
the following: 
 We propose the use of additional verification features, including the use of slang words, 
readability indices, WOT (Web Of Trust), centralities (indegree and harmonic) scores and 
Alexa rankings of linked URLs for tweets, and several new Twitter account profile features. 
 We employ and investigate the effectiveness of feature selection and bagging techniques to 
build highly accurate and robust classification models. In addition, we propose a new semi-
supervised learning scheme that ensures higher generalization performance for content 
disseminated in the context of new events. The proposed scheme leverages the agreement 
of individual classifier decisions on newly arriving tweets with the goal of retraining the 
highest performing one with the new tweets in order to help the trained model adapt to the 
new event. 
 We make available an image verification dataset14(corpus) comprising fake and real tweets 
(containing media content) that were published in a variety of events that are quite different 
in nature. We perform a thorough experimental study on this dataset and demonstrate the 
high accuracy and reliability of the proposed fake detection framework. 
 
In summary, in Chapter 5.2 we describe the dataset that we have made available to the research 
community on GitHub, which can be used as a benchmark for similar verification tasks, in Chapter 
5.3 we analyse in detail our approach and in Chapter 5.4 we evaluate our method. 
5.1 Related Work 
Although fake detection in the context of shared media content is a relatively new problem, there is a 
large body of work on a number of related problems. Web page spamming is examined in [44], [45], 
[46] and [47]. Qazvinian et al. study the problem of rumor detection in microblogs by building a 
dataset of manually annotated tweets. They test the effectiveness of three different types of features, 
content-based, network-based, and microblog-specific memes [48]. Given the rise of social networks 
as an interaction platform, Seo et al. studied how rumors spread on them by making the assumption 
that rumors are initiated from only a small number of sources and injecting a number of monitor 
nodes in the network [49]. Additionally, Mendoza et al. studied the propagation of tweets that carry 
false rumors during emergencies, concluding that rumors are more questioned by users than true 
information. This conclusion led them to the assumption that it is possible to detect rumors on social 
networks using aggregate tweet analysis [50]. 
Several approaches include the study of user behavior on social networks as well as the 
classification of accounts to spammers and non-spammers [51], [52], [53], [54] . Using as an 
example the Mumbai blasts, Gupta et al. studied the patterns of activity of users during the crisis and 
demonstrated that (a) non-authority users post more than other groups of users and (b) inaccurate 
information is more dominant [55]. Chu et al. focused on the classification of human, bot and cyborg 
accounts on Twitter by observing the differences among them in terms of tweeting behavior, tweet 
content, and account properties [56]. Recent research has also been oriented towards the discovery 
of credible information sources in social media [55], [57] . For instance, Gupta et al. look at how fake 
content spread on Twitter in the hours and days following the Boston Marathon bombings of 2013, 
with particularly interesting findings on the role of fake accounts in misinformation and the way, 
                                                     
14https://github.com/MKLab-ITI/image-verification-corpus 
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 53 OF 139 
verified and supposedly authoritative accounts, play a key role in propagating fake content [58]. 
Similarly, Castillo et al. focused on automatic methods for assessing the credibility of a given set of 
tweets. In particular, they analyze microblog postings related to trending topics and classify them as 
credible or not credible based on a number of features [59].  
Compared to previous related works, we focus on the performance of trained models on new events 
and devise techniques to ensure improved generalization performance, while we also investigate the 
importance of individual features for the detection accuracy. 
5.2 Image verification corpus 
We manually collect and make publicly available an image verification corpus. To create this dataset, 
we collected historical data using the Topsy15 and Twitter APIs. Topsy provides the possibility to 
search with certain keywords, either words and phrases or hashtags, while it offers the option to filter 
the results according to the type (tweet, image or link) and the time period they were posted. The 
Twitter API is used by defining a set of related keywords. We use Topsy to collect data for past 
events (more than a month old), while Twitter API for more recent events or events we were tracking 
live looking for the possible appearance of fake content. 
Defining a set of keywords K for each event with the help of the online resources of Table 12, we 
gather tweets around events spotted on social media and we collect a set of tweets T.  Afterwards, 
we define a set of unique fake pictures that spread on Twitter, as verified from online resources 
(articles and blogs), and we create the fake image set IF. By following the same procedure, we form 
the real image set IR. We use these sets as seeds to create our reference verification corpus TC⊆ T. 
This corpus includes only those tweets that contain at least one image of the predefined sets of 
images IF, IR. However, in order not to restrict the tweets to only those that point to the exact seed 
image URLs, we also employ a scalable visual near-duplicate search strategy as described in [60] 
[8]. More specifically, we extract compact descriptors from the collected images using a 
VLAD+SURF descriptor-aggregator combination. The extracted vectors are further encoded using 
Product Quantization, thus making the following Nearest Neighbour (NN) search more efficient. We 
use the sets of fake and real images as visual queries to the NN algorithm and for each query we 
check if each image tweet from the T set exists as an image item or a near-duplicate image item of 
the IF or the IR set. We implement the similarity search in order to extend the coverage of the dataset, 
taking into account the images that are not identical but very similar to the ones included in the seed 
sets. To ensure near-duplicity, we empirically set a minimum threshold of similarity tuned for high 
precision. However, a small amount of the images exceeding the threshold are eventually irrelevant 
to the ones in the seed set. To remove those, we conduct a manual verification step on the extended 
set of images.  
Additionally, in order to balance our amount of fake and real samples on our data, we collect tweets 
with media content from big events, on which no fake information was spotted. We assume real any 
tweet on this event with the only restriction that some media content is attached to it. We try to 
integrate real events of similar nature with the fake ones. For example, we integrate tweets for the 
real event of emergency landing of the JetBlue Airways flight, considering the integration of fake 
content from the event of Malaysia Airlines flight. As the aim of our work is to boost the 
generalization properties of the predictive framework, we integrate every single tweet in our corpus 
regardless of its language. The aim is to create a comprehensive corpus, which contains any 
possible fake tweet. Although in item-based features we use a group of text-based attributes for 
                                                     
15http://www.topsy.com 
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 54 OF 139 
prediction resulting to contain a lot of missing values in our data (due to the integration of tweets not 
formulated in the supported languages), we prefer to keep them in the corpus. Including events from 
the cases we described previously, being very different from each other, we intend to cover various 
representations of fake instances. Moreover, while building the item-based models, we consider in 
our training set only the unique items of our corpus, eliminating the retweets in order to avoid over-
fitting the very popular tweets. On the other hand, on the user-based training, we use every Twitter 
user from the dataset. Thus, we include the users not only from the unique tweets but also the re-
tweeters, as they offer additional information to the model training.  
Table 12: Resources used for each event 
ID Event URL 
E1 Hurricane Sandy http://en.wikipedia.org/wiki/Hurricane_Sandy 
E2 Boston Marathon http://en.wikipedia.org/wiki/Boston_Marathon_bombings 
E3 Sochi Olympics http://paleofuture.gizmodo.com/8-viral-sochi-olympics-photos-
that-are-total-lies-1517429839 
E4 Bring Back Our Girls http://mashable.com/2014/05/09/bring-back-our-girls-false-photo/ 
E5 Malaysia Airlines http://www.theepochtimes.com/n3/553181-malaysia-airlines-flight-
mh370-crash-photo-video-are-fake/ 
E6 Columbian Chemicals http://en.wikipedia.org/wiki/Columbian_Chemicals_Plant_explosio
n_hoax 
E7 Ferry MV Sewol http://en.wikipedia.org/wiki/Sinking_of_the_MV_Sewol 
E8 Passport http://gawker.com/man-trapped-abroad-because-his-toddler-got-
creative-wit-1584570881 
E9 Rock Elephant https://twitter.com/Pweaunay/status/449243704151527424 
E10 Underwater Hotel https://twitter.com/ADayana66/status/455346277661020160 
E11 Livr App http://livr-app.com/ 
E12 Pig Fish https://twitter.com/oliviadodson_/status/445043948969398272 
E13 Jet Blue https://twitter.com/airlivenet/status/512682112680734720 
 
Table 13 shows the whole list of the events Ei for which we gather tweets. We provide the number of 
Item and User features we used separately, since as we mentioned above, we may consider multiple 
users retweeting the same item (but we consider the item only once). 
Table 13: List of events included in the corpus 
ID Event Features Fake Real All 
E1 Hurricane Sandy Item 6081 1822 7903 
  User 9604 2879 12483 
E2 Boston Marathon Item 283 171 454 
  User 247 399 646 
E3 Sochi Olympics Item 400 3727 4127 
  User 391 437 828 
E4 Bring Back Our Girls Item 154 927 1081 
  User 627 2207 2834 
E5 Malaysia Airlines Item 458 0 458 
  User 1738 0 1738 
E6 Columbian Chemicals Item 237 0 237 
  User 1965 0 1965 
E7 Ferry MV Sewol Item 0 1912 1912 
  User 0 1815 1815 
E8 Passport Item 41 0 41 
  User 333 0 333 
E9 Rock Elephant Item 50 10 60 
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 55 OF 139 
  User 401 15 416 
E10 Underwater Hotel Item 48 0 48 
  User 432 37 469 
E11 Livr App Item 17 0 17 
  User 21 0 21 
E12 Pig Fish Item 11 0 11 
  User 11 0 11 
E13 Jet Blue Item 0 22 22 
  User 0 807 807 
 
5.3 Description of the approach 
We first describe the overall framework (0), and then proceed to its key elements. 
Framework Overview 
Figure 18 depicts the main parts of the overall fake detection framework and their inter-connections. 
The framework relies on two individual classification models, one based on item (tweet) features and 
a second based on user account features. Bagging is used to ensure higher reliability in the training 
process of the Item Classifier (IC) and the User Classifier (UC), and an agreement-based retraining 
strategy (fusion) is employed with the goal of improving the accuracy of the overall framework. 
 
 
Figure 18: Base framework representation 
5.3.1.1 Feature Extraction 
Given the set of available tweets T, we extract two types of features: item- and user-based. All 
extracted features are listed in Table 14. The ones written in bold are those that are investigated for 
a first time in this work.  
Tweet features: One may consider the features as tweet-, text- and language-based. Tweet-based 
features include the retweet count, the has external link, and the number of URLs. For 
evaluating the links shared, we include the WOT score (Web Of Trust), the harmonic and 
indegree centralities as well as the Alexa rankings metrics (to be explained below). Text-based 
features include the length of tweet and the number of words it contains. Also, we include 
features such as the number of question and exclamation marks and the number of 
uppercase characters included in the tweet text. The third category includes the language-
based features: to take into account the sentiment of the tweet, we compute the number of 
positive and negative words it contains, relying on a predefined list of sentiment words. We 
consider three different languages English, Spanish and German, for each of which we make use of 
a different list of sentiment words. For English we use the list provided by Jeffrey Breen16, for 
Spanish the Spanish adaptation for ANEW [61] and for German the Leipzig Affective Norms for 
                                                     
16https://github.com/jeffreybreen/twitter-sentiment-analysis-tutorial-201107 
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 56 OF 139 
German [62]. We detect the text language of each one of the tweets using a language detection 
library17. While for the tweets formulated in the above languages the sentiment words were defined, 
there was an amount of tweets formulated in other languages for which no sentiment words lists 
were found. In order to handle this problem and take these tweets into account during the 
experiments, we consider the values of them as missing. The result of the feature extraction process 
produces a list of tweet features F
T
 for each tweet of the T corpus.  
Table 14: Item and User Features 
Item Features FT 
length of tweet* num of words* 
   contains question mark* contains exclamation mark* 
   num of question marks* num of exclamation marks* 
   contains happy emoticon* contains sad emoticon* 
   contains 1st order pronoun* contains 2nd order pronoun* 
   contains 3rd order pronoun* num of uppercase characters* 
   num of negative senti words* num of positive senti words* 
   num of mentions* num of hashtags* 
   num of URLs* num of retweets 
   num of slangs* has colon* 
   has external link* WOT score 
   has please* numNouns* 
   readability* indegree centrality* 
   harmonic centrality* alexa country rank 
   alexa delta rank alexa popularity 
alexa reach rank  
User Features FU 
num of friends num of followers 
   follower-friend ratio num of times listed 
   user has a URL* is verified* 
   num of tweets has bio description* 
   has location* has existing location* 
   WOT score num of media content 
   account age tweet ratio 
   has profile image* has header image* 
   indegree centrality* harmonic centrality* 
   alexa country rank alexa delta rank 
   alexa popularity alexa reach rank 
 
To enrich the list, in the current work, we add the number of nouns a tweet contained, if it contains 
the word please (has please) and also if a colon appeared in the text (has colon). Additionally, 
we include as a feature if an external link is provided in the tweet (has external link). As 
external, we consider exclusively the links redirecting to another website excluding the ones linking 
to the media content of the tweet. The existence of a link created the need to check for its reliability. 
The WOT metric18 is a way to find how trustworthy a website is, using reputation ratings by Web 
users. For each link we spot, we find the WOT score using the WOT API (WOT score). For the link 
                                                     
17https://code.google.com/p/language-detection/ 
18https://www.mywot.com/ 
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 57 OF 139 
evaluation, we also include the indegree and harmonic centralities19, rankings computed based 
on the links of the web forming a graph. Credibility checking of the links is also done using four 
different Alexa metrics (alexa country rank, alexa popularity, alexa delta rank and 
alexa reach rank) with the Alexa rankings API20, which basically evaluates the frequency of 
visits on various websites. Except for the above, we add a feature for the number of slang 
words that the tweet text contains. For this purpose, we use slang words in English21 and Spanish22 
provided online. Finally, in order to investigate if the readability of the tweet text is related to its 
veracity, we use the Flesch Reading Ease method23. This method computes the readability of the 
text by measuring its complexity and provides a score in the range [0, 100], with 0 representing the 
very hard-to-read text and 100 the very easy-to-read text. 
User-based features: The number of friends and followers along with whether the user is 
verified by Twitter (is verified) are some of the twitter-based features we use. We also take into 
account whether he/she has biography description in his/her profile (has bio description). 
Checking also whether a link is provided in his/her description (user has a URL), we apply once 
again the WOT score and the harmonic and indegree centralities in order to compute the 
credibility of the source. The number of media content items a user posted (num of media 
content), the tweets per day made (tweet ratio), whether the user provides profile (has 
profile image) and header image (has header image) are also included to the list of features. 
Finally, we calculate the age of the Twitter account for each user (account age), completing the 
list. 
5.3.1.2 Data pre-processing 
The next step of the framework includes data pre-processing, cleaning and transformation. In order 
to handle the missing values of some features in our training dataset, we use Linear Regression to 
predict them. Following this technique, we consider the attribute with the missing value as a 
dependent (class) variable in order to approximate and fill in the values. We apply Linear Regression 
only to numeric features as the method cannot support the prediction of the Boolean values. Note 
that only feature values from the training set are used in this process. Data transformation includes 
normalization to scale the attributes to the range [-1, 1]. 
5.3.1.3 Classification models 
Proceeding to the training process of our dataset, we use the Item and User features to build two 
individual classifiers based on Random Forests, the Item classification model IC (Item Classifier), 
and the User classification model UC (User Classifier), each of which is based on the respective set 
of features. Furthermore, to assess the ability of the classifiers to distinguish between fake and real, 
we enforce them using bagging technique. Additionally, we perform feature investigation to test the 
performance of the classifiers with various sets of features. In the next sections, we describe 
thoroughly the prediction models we implement. 
Feature Analysis 
This step is performed in order to detect the combination of the features that are useful for predicting. 
For this reason, we test various subsets of features. 
                                                     
19http://wwwranking.webdatacommons.org/more.html 
20http://data.alexa.com/data?cli=10&dat=snbamz&url=google.gr 
21http://onlineslangdictionary.com/word-list/0-a/ 
22http://www.languagerealm.com/spanish/spanishslang.php 
23http://simple.wikipedia.org/wiki/Flesch_Reading_Ease 
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 58 OF 139 
Firstly, we test our models using the whole amount of features, the basic set of features (Table 14) 
and then using just those features presented in our previous work, the previous set of features. 
Additionally, the assumption that the time-dependent features, such as the number of tweets, the 
number of followers or the number of users following a user may influence the predictive process led 
us to perform feature selection maintaining only the time-independent features and build separate 
models for the static features. 
Finally, we apply feature selection to our basic set of features. Feature selection is a useful part of 
the data analysis in order to detect which features are important for the prediction and how the 
features are related to each other. We perform this technique in order to maintain and use for the 
model construction only the subset of features that provide additional information to the prediction 
process. A feature selection algorithm combines a search technique for proposing new feature 
subsets, along with an evaluation measure which scores the different feature subsets. In our work, in 
order to evaluate the contribution of each feature, we use a CfsSubsetEval evaluation technique. 
This evaluates a set of attributes by considering the individual predictive ability of each feature along 
with the degree of redundancy. 
In order to refer to the different subsets of the features we created, we use some abbreviations, 
presented in Table 15. 
Table 15: List of abbreviations used 
 Classifiers 
IC Item Classifier 
UC User Classifier 
AC Agreed samples based Classifier 
BC  Best Classifier from cross-validation 
 Other 
b  bagging technique applied 
all total amount of samples used 
AF All Features 
FS Feature Selection 
TF Time Features 
 
Bagging 
Unequal numbers of fake and real tweets use to force us to reduce the number of the samples used 
for training and ignore some data while building a model. In order to take advantage of the whole 
training dataset, we use bagging, an ensemble learning technique. Bagging tends to improve the 
stability and accuracy of the method, as it predicts using the average result of numerous predictors. 
Given the whole training dataset D of size n, bagging creates m different datasets Si of size k, 
including an equal number of samples of each class. Note that some samples may appear 
concurrently in different datasets Si. This leads to the creation of m different item-based classifiers IC 
and m different user-based classifiers UC. The groups of IC and UC classifiers that come from the 
bagging technique are referred as ICb and UCb respectively. A graphical representation of the 
process is presented in Figure 19. The final prediction result for each of the testing samples is 
calculated using the majority vote of the m prediction results. 
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 59 OF 139 
 
Figure 19: Bagging scheme 
Agreement-based retraining 
The next step of our approach includes an agreement-based retraining in order to improve the 
prediction accuracy in view of new events. Specifically, after testing separately the ICb and UCb 
classifiers, we combine their outputs in a semi-supervised learning fashion. After gathering their 
individual predictions for each sample of the test set, we compare their individual predictions, thus 
dividing the test set in two subsets, the agreed and disagreed samples. These two subsets are 
treated differently by the classification framework. Figure 20graphically illustrates the adopted 
process.  
Assuming that the agreed predictions are correct with high accuracy (we accept a small error 
percentage in the class assignment by the classifiers), we use them as new training samples in order 
to build a new classifier, which we call agreed classifier AC, for predicting the disagreed samples. 
We also combine the bagging-based classifiers to construct the model ACb. 
In the second approach, as we observe in the figure, we add the agreed samples to the initial 
existing model. We select every time one from the initial models ICb, UCb and concretely, this model 
that performs better on the separate cross-validation process we apply to them (cross-validation 
takes place using only the training set). We refer to this as BC (Best Classifier). To investigate 
different learning settings, the new updated model is transformed in three different ways: the AC+BC 
which contains an equal number of samples from each class, the (AC+BC)b, which is the outcome of 
the bagging technique and finally the (AC+BC)all, which results from using the entire set of samples. 
Table 15 summarizes the abbreviations we use to refer to the different cases. 
 
Figure 20: Agreement-based retraining (fusion) 
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 60 OF 139 
The accuracy of the whole process is calculated by summing up the accuracy of the fusion of the 
classifiers (accuracy of the agreed set) and the accuracy of the AC model (while testing the 
disagreed), assigning as a weight in each model the percentage of samples they are called to 
predict. If a is the overall accuracy, we calculate it according to the following formula:  
𝑎 = 𝑎𝑓𝑢𝑠𝑖𝑜𝑛 ∗ 𝑤𝑎𝑔 +  𝑎𝐴𝐶 ∗ 𝑤𝑑𝑖𝑠 
where wag, wdis refer to the percentage of samples where the individual agree and disagree 
respectively. When the updated model is used, we follow the same formula by replacing the AC 
classifier with the AC+BC classifier. 
5.4 Experimental evaluation 
The next step is to apply the classifiers we created as described in the previous sections, to assess 
the performance of different learning settings. 
The aim of our experiments is to test the accuracy of each model constructed, when it is called to 
classify samples from new previously unseen events. This is an important point as the nature of the 
fake tweets posted might be quite different, since they originate from different events. Therefore, we 
want to assure that the performance of each classifier is consistent and independent of the type of 
the event. For this reason, in our process, we organize the dataset we acquired in the following 
ways.  
We use as test sets each one of the Ei events, i=1,2,..,7. We do not use the rest of the events 
E8,..,E13 for testing, as the number of the tweets contained are quite few and the results will not be 
representative. For example, for testing E1, we create our training model using the rest of the events 
E2,E3,...,E13. We avoid to use tweets from the same event in the training and testing set as the 
accuracy result might be falsely high. The above procedure is performed for each of the seven 
events. To further explore the potential of our models, we split the events and create two groups of 
training and testing datasets. In the first training group, we include events E1, E3, E4, E9, E10, E13, 
while for testing we use events E2, E7, E8, E11, E12, E5, E6. In the same way, the second training 
set consists of events E5, E9, E10, E3, E2 and E7, while the respective test set comprised events 
E1, E4, E11, E6 and E13 (Table 16). 
In each trial we perform, we start by creating the initial IC and UC classifiers by selecting each time 
an equal number of fake and real samples, thus inevitably excluding an amount of the available 
tweets. Moreover, we create the corresponding bagging classifiers. Comparing the performance of 
the initial and bagging classifiers, we observe that the bagging contribution to the outcome is very 
important. We can graphically assure this result in Figure 21, in which we present the IC and UC 
models built for each case. Bagging classifiers, in both cases, outperform the classifiers built on a 
specific dataset, either by a percentage of 2%-11% in case of IC or 1%-13% in case of UC. We can 
also assure that the advantage of bagging is coherent, even in different kinds of events. However, in 
some cases, like in T5, the initial classifiers may have a lead in the accuracy, but averagely bagging 
classifiers overpass them. Taking into account the above results, we choose to make use of the 
bagging models for the agreement-based retraining technique. 
Next, we test our approach using each set of features we describe in the Feature Analysis section. 
We describe extensively the trials performed below, separately for each set. 
 
 
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 61 OF 139 
Table 16: List of trials 
Trial Training Testing 
T1 E2..E13 E1 
T2 E1 & E3..E13 E2 
T3 E1,E2 & E4..E13 E3 
T4 E1..E3 & E5..E13 E4 
T5 E1..E4 & E6..E13 E5 
T6 E1..E5 & E7..E13 E6 
T7 E1..E6 & E8..E13 E7 
T8 E1,E3,E4,E9,E10,E13 E2,E5,E6,E7,E8,E11,E12 
T9 E2,E3,E5,E7,E9,E10 E1,E4,E6,E11,E13 
 
We firstly use the entire set of features for testing the agreement- retraining approach. Table 17 
shows the scores obtained separately for each trial. The first two columns present the agreement 
levels achieved. Observing them, we notice that on average the two classifiers’ predictions (ICb, UCb) 
agree in the majority of tweets (column  “Agreement percentage”), amounting to 82.44%, on which 
the average agreed accuracy (column “Agreed accuracy”) is extremely high (97.32%). We also 
remark that the higher the agreement level is, the higher the accuracy we achieve. The next two 
columns present the accuracy of the disagreed samples, while using the ACb and the (AC+BC)b 
models respectively. Similarly, the overall columns show the results while combining the accuracy of 
the agreed and disagreed samples as calculated by the formula we described in previous section. 
The scores in the two cases are on average very similar. Finally, in the last two columns of the table, 
we also give the scores of the ICb and UCb classifiers. In this case, we predict the whole set of the 
testing set using separately these models, without using the agreement-retraining technique. We 
make these trials to justify that the agreement-retraining technique performed actually much better 
than the individual classifiers. Observing the scores of the individual classifiers (88.89% and 
88.30%), we see the advantage of the retraining approach models (93.15% and 93.73% 
respectively). This observation is very important, because if we assume that it is applied universally, 
the effectiveness of the agreement-based retraining technique cannot easily be questioned. 
Table 17: Accuracy results for the entire set of features case (AF). Agreement levels between the IC
b
 
and UC
b
 classifiers, agreed, disagreed and overall accuracy for each trial and model (AC
b
, (AC+BC)
b
). 
Trial 
Agreement 
percentage 
Agreed 
accuracy 
AC
b
 
disagreed 
accuracy 
(AC+BC)
b
 
disagreed 
accuracy 
AC
b
 overall 
(AC+BC)
b
 
overall 
IC
b
 UC
b
 
T1 78.83 98.41 82.18 88.21 94.94 96.20 79.57 85.19 
T2 73.87 97.48 62.95 61.69 88.81 88.53 83.71 86.49 
T3 76.57 89.97 35.39 45.31 77.14 79.47 77.37 83.82 
T4 88.98 99.76 72.34 72.34 96.67 97.10 92.18 96.39 
T5 66.07 93.44 83.64 95.21 90.04 94.06 88.81 68.67 
T6 98.77 100 100 47.03 100 99.34 99.11 99.66 
T7 93.66 99.89 99.62 98.37 99.88 99.80 98.77 94.69 
T8 82.44 98.06 78.79 78.30 94.75 94.62 91.95 87.35 
T9 82.78 98.90 80.67 73.15 96.13 94.47 88.55 92.46 
Average 82.44 97.32 77.28 73.29 93.15 93.73 88.89 88.30 
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 62 OF 139 
An important part of the experimentation is to assure that the adaptation of the aforementioned new 
features is really useful in the prediction process. Thus, we create the same models for our samples 
and we apply the same trials, using only the baseline features which are derived from our previous 
work (see Table 14). Table 18 presents the scores of the models for each trial. Apparently, the 
agreement-retraining models (ACb overall and (AC+BC)b overall columns) perform once again much 
better than the individual ICb and UCb models. However, comparing these results with the ones of 
the Table 17, we conclude that the models built with the entire set of features produce higher scores 
in all the cases. That means that the adaptation of the new features is useful for the prediction 
process. 
Table 18: Accuracy results for the baseline features case. Agreement levels between the IC
b
 and UC
b
 
classifiers, agreed, disagreed and overall accuracy for each model (AC
b
, (AC+BC)
b
) and each trial. 
Trial 
Agreement 
percentage 
Agreed 
accuracy 
AC
b
 
disagreed 
accuracy 
(AC+BC)b 
disagreed 
accuracy 
AC
b
 overall (AC+BC)b 
overall 
IC
b
 UC
b
 
 T1 47.07 55.39 52.35 57.09 53.78 56.29 55.81 49.25 
T2 45.19 48.62 52.79 46.28 50.90 47.35 46.28 52.50 
T3 55.79 68.50 70.67 69.73 69.48 69.07 69.56 51.10 
T4 56.02 36.97 45.87 38.60 40.87 37.68 37.80 47.60 
T5 50.67 97.22 92.99 99.43 95.15 98.30 97.70 50.13 
T6 75.35 99.24 79.45 39.49 94.00 85.31 75.44 98.81 
T7 71.93 98.02 90.25 83.84 95.92 94.16 92.89 76.21 
T8 56.08 80.91 78.51 76.52 79.86 79.00 79.06 55.62 
T9 47.61 55.13 50.18 50.34 52.54 52.62 51.40 53.48 
Average 56.19 71.11 68.11 62.36 70.28 68.86 67.32 59.41 
 
The set of features that we adopt contain features that change over time and others that are static. 
To make this feature-based classification work, we needed to have some knowledge of the time-
dependent features’ impact on the class prediction. Thus, the same trials are performed using only 
the time-independent (static) set of features. We note these features with the * symbol in Table 14. 
Table 19 shows the results while using the models. Here, we present just the final scores of the 
agreement-retraining approach, not giving the details of each step of the process. In this case, we 
observe that the percentages achieved are quite lower than those of the basic set of features. That 
means that the time-dependent features are quite useful for the improvement of the prediction 
accuracy. 
In the next step, we apply the trials using feature selection. We perform feature selection on every 
step of the procedure and separately for the Item and User features. The dominant features for the 
Item case are the num of negative senti words,num of positive senti words, 
alexa country rank, alexa reach rank, length of tweet, indegree 
centrality, harmonic centrality and alexa delta rank. For the User case the 
dominant ones are the indegree centrality, harmonic centrality, alexa reach 
rank, alexa popularity, WOT score and alexa country rank.  
 
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 63 OF 139 
 
Table 19: Accuracy results for the time-independent set of features case (TF). Agreed and overall 
accuracy for each model (AC
b
, (AC+BC)
b
) and each trial. 
Trial AC
b
 overall (AC+BC)
b
 overall 
T1 84.49 89.97 
T2 64.89 61.87 
T3 72.40 72.96 
T4 83.20 81.90 
T5 96.10 97.58 
T6 96.83 98.01 
T7 99.59 99.39 
T8 85.70 85.60 
T9 70.30 70.20 
Average 83.72 84.16 
 
The resulted prediction accuracies for each trial are presented in Table 20. Comparing the results 
with the ones presented on Table 17, we observe that the feature selection process does not offer 
much knowledge in predicting as the scores achieved are very similar, even lower than those of the 
basic set of features. Thus, we remark once again the advantage of the models built with the basic 
set of features. 
Table 20: Accuracy results for the feature selection case (FS). Agreed and overall accuracy for each 
model (AC
b
, (AC+BC)
b
) and each trial. 
Trial AC
b
 overall (AC+BC)
b
 overall 
T1 89.06 89.05 
T2 90.81 89.39 
T3 87.95 87.25 
T4 98.02 98.55 
T5 71.20 74.80 
T6 100 100 
T7 99.22 97.60 
T8 93.44 92.33 
T9 93.92 93.83 
Average 91.51 91.42 
In order to get a better idea of the obtained results, we present them graphically in Figure 22 and 
Figure 23. Figure 22 shows the accuracy scores of the classifiers that are built on the agreed 
samples using each set of features. Averagely, the various classifiers seem to perform similarly. 
However, the one built using bagging and the entire set of features overpass the accuracies in the 
other cases in the majority of the trials. This can be also assured from the Table 17 where we remark 
that the average score in this case is 93.15%. Additionally, the model built with the selected features 
performs quite similarly, achieving 91.51%. 
Figure 23 presents the scores while using the agreed samples together with the initial ones. Here, 
we observe that the AF case with bagging performs on average best among the cases. 
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 64 OF 139 
 
Figure 21: Accuracy of user classifiers 
 
Figure 22: Accuracy of classifiers built on the agreed samples 
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 65 OF 139 
 
Figure 23: Accuracy of classifiers built on the initial and agreed sample 
Finally, in order to investigate a few failure cases of our approach, we present some tweet examples 
that it failed to classify correctly. The tweet in Figure 24, which refers to Hurricane Sandy, was 
classified by the AC model, built on the agreed tweets, as fake. Possible reasons for failing might be 
the values of some features, such as, for example, the number of uppercase characters, as fake 
tweets tend to have larger values of uppercase letters. We also present a tweet with fake content, 
from the Columbian Chemicals hoax story that has been classified as real. The failure in this case 
might be caused from the small number of hashtags the tweet contains. Moreover, the tweet text 
may increase the readability score and affect the prediction, thinking that real tweets, as being more 
reliable, should have better text quality. 
 
  
 
Figure 24: Failed cases: Hurricane Sandy and Columbian Chemicals 
5.5 Conclusions 
In this work, we focus mainly in building a robust fake detection framework using different machine-
learning techniques. However, we face some expected challenges during its development. In the 
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 66 OF 139 
dataset collection, although we gather fake tweets from different events covering a variety of 
possible fake tweets, language variation complicated our problem. Some of the Item features, such 
as the number of sentiment words and slang words are supported only for a few languages, leading 
to the appearance of missing values in a lot of cases. Additionally, the readability feature 
performance is possibly unreliable in languages other than English. We assume that the variety of 
languages is the reason for the lower prediction accuracy of the IC in several events. 
Apart from the challenges, we also remark that the accuracy scores we achieve in this approach we 
presented are much higher than those of our previous work [43]. Previously, using cross-validation 
techniques initially and decision tree for training, we achieved a maximum of 75% detection 
accuracy. In the past we had only experimented with two events, while in this work we include a 
large number of events while at the same time significantly enriching the list of Item and User 
features. However, we believe that the main improvement stems from the use of feature selection, 
bagging and agreement-based retraining. 
In terms of applying the proposed approach in real-time settings, one should be cautious of the 
following caveat: the agreement-based retraining requires a number of samples from the new event 
in order to be applied. Hence, for the first set of arriving items, it is not possible to rely on this 
improved step. Yet, the rate at which new items arrive in the context of breaking news events could 
quickly provide the algorithm with a sizeable set of tweets. 
Besides the proposed fake detection framework, the work described above attempts to create an 
experimental fake detection benchmark that can be used to objectively assess the performance of 
competing approaches in a number of different cases. For this development, it is important to 
investigate the plausible techniques, but also to integrate the suitable data. In this direction, we make 
our verification corpus publicly available in GitHub24 in order to give the opportunity to other 
researchers to use it and update it with additional data. We believe that it is important to build such a 
dataset in order to contribute to the on-going scientific discourse on these matters by providing a 
common benchmark that everybody can experiment with. 
5.6 MediaEval 2015 Verifying Multimedia Use Task25 
The work described in this section is the basis for the organization of a new task in the MediaEval 
2015 benchmark. The task targets researchers from several communities including multimedia, 
social network analysis, computer vision, and natural language processing and deals with the 
automatic detection of manipulation and misuse in Web multimedia content. Its aim is to lay the basis 
for a future generation of tools that could assist media professionals in the process of verification. 
Examples of manipulation include both malicious tampering-doctoring of images and videos, such as 
splicing and removal/addition of elements, as well as other kinds of misuse such as reusing 
previously captured multimedia content in different contexts (e.g., in a new, unrelated event). The 
definition of the task is the following: "Given a tweet and the accompanying multimedia item (image 
or video) from an event that has the profile to be of interest in the international news, return a binary 
decision representing verification of whether the multimedia item reflects the reality of the event in 
the way purported by the tweet."  
                                                     
24https://github.com/MKLab-ITI/image-verification-corpus 
25 http://www.multimediaeval.org/mediaeval2015/verifyingmultimediause/ 
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 67 OF 139 
6 Author profiling 
The exponential rise in content produced by social media has brought forward a massive amount of 
user generated data. However, most of the time, we know very little about the users who produce the 
content, unless they themselves publish personal information in a structured and openly accessible 
way on Twitter, Facebook, their personal webpage or blog etc. 
Being able to process raw user content such as text and extract useful information about its creator 
is of growing importance, since social media produce a huge amount of raw data which remains 
unexploited. Characteristics such as age, gender, personality traits and geolocation are just the 
beginning of a list of user attributes that would contribute useful additional information. Being able to 
correctly estimate some of these characteristics from text, such as posts or tweets, would have 
important implications from a marketing, security and forensic point of view. 
The recent interest in this direction from the natural language processing and computer science 
community has been sparked, apart from the gargantuan amount of data that exists, by the evolution 
of machine learning techniques and libraries to work with large data. Such techniques, make it 
relatively easy to try out different methodologies and feature combinations that would have been very 
difficult in the past, in addition to making machine learning accessible to a larger group of computer 
scientists. This fact, combined with the need to extract useful information from user content, has led 
to the development of what is referred to in general as author profiling. 
However, within sociolinguistics, studies on gender and its manifestations in language have a long 
history, predating the current spur of interest by the computer science and natural language 
processing community [63] [64]. Yet, the bulk of this knowledge has not been integrated and taken 
into account in most of the prevailing quantitative approaches. 
Author profiling also has a degree of commonality with author attribution and identification, both 
dimensions of computer forensics. They are linked to the extent that the former and the latter both try 
to capture good distinguishing features from user generated text, namely, stylometric features. 
However, author profiling intends to capture features that a whole group of people have, in 
comparison to author attribution where recognition of the style of an individual is pursued. 
6.1 Related work 
One key aspect of author profiling has its roots in stylometry, namely, recognition of the style and 
patterns of the way somebody writes. Stylometry predates computers and was originally applied to 
handwritten text where handwriting style was also a key factor. The first scientific approaches to the 
problem are considered to be those of Mendenhall in the 19th century on the work of Shakespeare. 
The first statistical approaches to stylometry were by Yule and Zipf followed by Mosteller and 
Wallace, who worked on the disputed federalist papers in 1963 [65]. For a detailed survey on 
stylometry from the vantage point of author attribution see [66] and [67]. 
Stylometry has traditionally been considered to work when large amounts of text authored by a 
person exist. In more recent work, however, interest has turned towards use of smaller texts, tweets 
for example, to extract stylometric features and predict authorship [68] [69] [70]. Topic, register and 
style are entwined in text and it is very difficult to model them independently. Therefore, it is 
considered important that the texts are of the same topic, register and type, otherwise the results 
obtained could be arbitrarily biased. An example that shines light on this caveat can be found in [71], 
where the authors showed that gender distinction in blogs was due to different blog preferences. 
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 68 OF 139 
Women tended to create diary blogs while men tended to create filter blogs, and therefore the 
distinguishing factor for gender was encoded in the choice of web blog type and not strictly in style of 
writing. Author profiling has encompassed many different classes in which to profile users. Gender  
[72] [73] [74] [75] [71] [76] [77] [78] [79], age [76] [77] [78], geolocation [78], political orientation [78] 
and personality traits [80] being the most common. 
Rao et al. [78] suggest an approach of focused crawling on twitter starting from accounts based on 
the target class to profile. They used sororities and fraternities as well as male and female hygiene 
products as starting points for the crawl in the gender case. Author profiling has been attempted on 
many different types of text and platforms on which users generate content, such as formal texts 
[81], blogs [77] [82] [71], twitter [74] [73] [78] [79] [75], social networks [76] and YouTube [83]. As can 
be noticed above, author profiling has turned towards social media where texts are generally of a 
shorter format. Early work in author profiling, and more specifically, in gender identification 
considering formal texts extracted from the British National Corpus is that of Argamon et al. [72] [81]. 
Bamman et al [75] approached gender profiling of users from a more sociolinguistic vantage point, 
and argued that individuals whose language does not match the general model of their gender, tend 
to have less same-gender social connections than average. In [79], the authors tried through 
gamification to quantify how well humans can distinguish age and gender by reading tweets. 
TweetGenie26, a game that allows users to guess the age and gender of a user, was used to harvest 
a corpus of human guesses. Results show that 84% of the users correctly predicted gender, while for 
age, user predictions are on average off by 5.75 years. 
As far as author profiling on tweets is concerned, an important decision involves how to use the 
tweet text data. A viable option is to consider each tweet as a separate document. However, 
because of the 140 character limit imposed by Twitter, documents will generally be too short to 
capture many significant features. Concatenating tweets into larger documents has therefore 
prevailed. Experiments regarding the number of tweets used per document creation have been 
carried out, although examples are from the domain of author attribution. 
[68] monitored results for documents created by joining 25, 50, 75 and 100 tweets. When using an 
external test set concatenating 25 tweets to form a document had the best results. When using 
cross-validation, accuracy increased as the number of tweets joined increased, but the rate of 
increase dropped after 50. [69] searched for the optimal number of tweets to join by carrying out an 
incremental search in sizes ranging from 0 - 200 with 20 step intervals. They concluded that adding 
an additional 20 tweets to the document improves accuracy up to 120 tweets. After that 
concatenating an addition 20 tweets to the document results in changes in accuracy that are 
negligible. There have been many workshops and competitions that have materialized interest in 
Author Profiling. PAN, an evaluation lab on uncovering plagiarism, authorship and social software 
misuse, has featured an Author Profiling Task since 2013 [84]. Also, RepLab 2014 [85] hosted an 
author profiling task in which contestants were asked to classify users in one of the following 
categories: journalist, professional, authority, activist, investor, company or celebrity using tweets 
from users that belong to those domains. 
Kaggle has also featured a contest on personality prediction, in which participants were asked to 
predict the personality traits of Machiavellianism, Narcissism, Openness, Conscientiousness, 
Extraversion, Agreeableness, and Neuroticism using twitter usage and text27.  
                                                     
26 http://www.tweetgenie.nl/ 
27https://www.kaggle.com/c/twitter-personalityprediction 
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 69 OF 139 
6.2 Proposed approach 
Features 
We created two groups of features, stylometric and structural. 
The structural group of features aim to trace characteristics of the text and are dependent on the use 
of Twitter. Features such as counts of @mentions, hashtags and URLs in a user's tweets. 
With the stylometric group of features we tried to capture characteristics of context that a user 
generates non automatically. Features tested were tf-idf of ngrams, bag of n-grams, n-gram graphs 
[86], bag of words, tf-idf of words, bag of smileys, counts of words that were all in capital letters and 
counts of words of size 1-20. 
 
Figure 25: Groups of features 
Pre-processing 
In our approach, preprocessing seems to be an important factor for this task. Because texts are 
tweets, they contain twitter specific information entangled in the text (hashtags, @replies and URL 
links). Therefore, an important decision involves deciding how to correctly deal with this bias. 
Tweets also contain a large amount of quotations and repeated robot text, which may be structurally 
important but should be stylometrically insignificant. 
The information that somebody is making quotations may be interesting, but the actual words and 
language he/she uses are not his/hers, and therefore should not be considered when trying to create 
a profile for that user. 
In our approach, a different preprocessing pipeline was applied to each group of features as 
described above. There was no preprocessing done for Structural features. Stylometric feature 
preprocessing encompassed removing any HTML found in the tweets, removing Twitter bias such as 
@mentions, hashtags and URLs and removing exact duplicate tweets after removing Twitter-specific 
text. In particular, @username and URLs were removed as a whole, while hashtags were stripped of 
the hashtag character #. 
Classifier 
We used machine learning tools from the scikit-learn library [87]. For the age and gender subtask we 
used an SVM with a RBF kernel and a SVM with a linear kernel respectively. In the case of the age 
subtask, we also used class weights since the distribution of instances in the classes were not 
proportional. 
Regarding the personality traits, SVR regression with a linear kernel was used. For each subtask the 
features were concatenated into a big matrix and were scaled and normalized. Scaling was 
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 70 OF 139 
performed in the columns (features) such that the values were between -1 and 1 with 0 mean and 
unit variance. Normalization was performed along rows (instances) so that each row had unit norm. 
6.3 Experimental evaluation 
As a benchmark, we took part in PAN 2015. PAN, held as part of the CLEF conference is an 
evaluation lab on uncovering plagiarism, authorship, and social software misuse. In 2015, PAN 
features three tasks, plagiarism detection, author identification and author profiling. We chose to 
participate in the author profiling task as we considered it was most relevant to Reveal. The 2015 
Author Profiling task challenged competitors to predict gender, age, and five personality traits 
(extroversion, stability, openness, agreeableness and conscientiousness) in four languages (English, 
Spanish, Italian, Dutch). The distribution of tweets according to age and gender in the 2015 training 
set can be seen in Figure 26. 
 
Figure 26: Distribution of tweets according to age (left) and gender (right). 
For the task we proposed a coherent grouping of features combined with appropriate pre-processing 
steps for each group. The idea was to create an easily comprehensible, extensible and 
parameterizable way to add features as groups. In this way it will be easy to try different 
combinations of pre-processing and features.  summarizes the selected features for all subtasks. 
Table 21: Features used for each subtask 
 
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 71 OF 139 
Results 
Our system in the results is labelled as grivas15. In PAN 2015, systems were evaluated using 
accuracy for the gender and age subtasks and average Root Mean Squared Error (RMSE) for the 
personality subtask. In order to obtain a global ranking the following  formula was used: 
((1 − RMSE) + joint accuracy)/2 
In the context of PAN 2015 there were 22 system submissions for tasks in English, 21 for Spanish 
and Dutch and 19 for Italian. Below we review our results for each subtask considering the top 
submissions. Our submission was in the top 2 systems in all languages regarding the gender 
subtask and ranked 1st on average over all languages.  
 
Table 22: Top 10 systems for gender subtask based on average accuracy over languages 
We did not do as well in the age prediction were our system was 7th when considering average 
accuracy scores for English and Spanish – the age subtask was for those two languages only. 
 
Table 23: Top 10 systems for age subtask based on average accuracy over languages (English & 
Spanish) 
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 72 OF 139 
Our system was not tailored for personality trait prediction and not surprisingly didn’t perform well in 
this subtask ranking 13th.  
 
Table 24: Top 15 systems for personality traits using average root mean squared error over all 
languages 
Overall using the average of the above global metric over all languages, our system ranks 3rd .  
 
Table 25: Top 10 systems overall using average global metric defined by Pan 
 
  
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 73 OF 139 
7 Image tampering detection 
Image manipulations for both well-meaning and malicious purposes have been a common practice 
since the times of film photography, but the advent of digital photography and image processing 
software has resulted in powerful editing tools being available to everyone, including laypeople. For 
more than a decade now, the research community has been exploring algorithms through which to 
either automatically detect tampering in digital images or offer semi-automatic detection, i.e. provide 
tools to assist human forensic investigators in making a final decision concerning the authenticity of a 
document and/or the nature and location of the purported forgery. The applications of such 
algorithms range from evaluating news sources to courtroom forensic evaluation of digital media 
evidence.  In the past, methods have been proposed attempting to detect, identify and possibly 
localize multimedia tampering operations, covering a wide range of different media types, file formats 
and tampering actions. 
With REVEAL’s priorities in mind, we set out to explore, classify, implement and advance the current 
state-of-the-art in image tampering, especially in the context of images circulating in web 
environments, and more specifically over social media platforms. In contrast to the rest of the 
modules presented in this document, where textual information and context are utilized for the 
evaluation of an item’s authenticity, the work presented in this section focuses on image tampering 
detection using exclusively image content, completely ignoring context and even metadata. 
During the first months of this task, our initial review of existing approaches for image tampering 
detection was put in context by identifying the additional limitations that Web and Social Media 
environments place on image forensics algorithms. We then proceeded with large-scale evaluations 
of the most promising existing methods over experimental datasets, assessing their robustness to 
the typical transformations applied by Social Media platforms on uploaded images. We consecutively 
formed a new evaluation dataset drawn from well-known, confirmed real-world forgeries, upon which 
we attempted to assess the performance of today’s most successful approaches. Following the 
large-scale evaluations conducted on our data, we then moved on to initiate the implementation of 
an image forensics module for image tampering detection within the REVEAL platform, guided by 
our research findings. 
7.1 State-of-the-art 
Automatic forgery detection in images is an active field for more than a decade, and a large number 
of methods have been proposed in the past. The field has now reached a certain degree of maturity, 
as indicated by the number of existing survey papers on the subject [88], [89], [90]. 
Image tampering detection methods can be divided into three broad categories, with respect to the 
tampering practices they each aim to detect. These practices are splicing, copy-move forgery, and 
image processing operations such as rescaling, cropping or histogram adjustments. 
Image splicing refers to the practice of copying a part of one image and into another, so as to give 
the false impression that an additional element was present in a scene at the time that the 
photograph was captured. Copy-moving, on the other hand, is the practice of taking a part of an 
image and copying it within the same image. This can both be used to falsely add more information 
(e.g. make a crowd seem larger) or reduce it (e.g. erase people by copy-moving the background 
over them). Finally, under the term “image processing operations” we include a broad spectrum of 
alterations, such as rescaling or intensity histogram adjustments, which generally tend to be well-
intended and are rather rarely used to modify an image's semantic content and misguide the 
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 74 OF 139 
viewers, although, in some contexts, increasing the artistic value of an image (even a news photo) 
through such operations may be considered a malicious practice -this is, for example, the case in 
many journalistic photo competitions28,29. Still, with respect to verifying social media news sources, 
this could be classified as a fairly innocuous practice. 
Copy-move detection 
From a forensic investigator’s perspective, copy-move forgery detection is generally performed using 
common image content search algorithms, applied within the same image, by seeking internal 
replications of blocks, patches or keypoints. In a typical copy-move detection pipeline, the descriptive 
information (blocks or keypoint descriptors) are extracted from the image, then a matching step 
seeks the best matches within the image, and then a post-processing step attempts to organize 
these matches into coherent candidates of copy-moving, in the face of simple translation, filtering or 
affine transformations. This is conducted by attempting to group neighbouring matches into clusters, 
and estimating the parameters of possible affine transforms that could best explain the matches 
found in a region. This is used to eliminate spurious matches and still be able to build models that 
are robust to transformations other than simple translation. 
The major current research problems faced by existing methods are achieving robustness with 
respect to transformations of the replicated patch, and tackling the high computational demands of 
exhaustive within-image search. A recent large scale survey and evaluation of the state-of-the-art 
can be found in [91]. 
The results presented in [91] are highly relevant to our own research, as they present evaluations of 
the vast majority the state of the art with respect to retrieval performance, robustness in the face of 
alterations and computational demands. The presented framework is also accompanied by a C++ 
implementation, which, to our knowledge, remains the most efficient copy-move detection 
implementation toolbox publicly available until today. 
Table 26: Performance metrics for a number of  copy-move detection algorithms [91] 
Method Precision Recall F1 
BLUR 88.89 100.00 94.12 
BRAVO 87.27 100.00 93.20 
CIRCLE 92.31 100.00 96.00 
DCT 78.69 100.00 88.07 
DWT 84.21 100.00 91.43 
FMT 90.57 100.00 95.05 
HU 67.61 100.00 80.67 
KPCA 87.27 100.00 93.20 
LIN 94.12 100.00 96.97 
LUO 87.27 100.00 93.20 
PCA 84.21 100.00 91.43 
SIFT 88.37 79.17 83.52 
SURF 91.49 89.58 90.53 
SVD 68.57 100.00 81.36 
ZERNIKE 92.31 100.00 96.00 
Average 85.54 97.92 90.98 
 
                                                     
28https://www.david-campbell.org/2009/12/06/photographic-manipulation-%E2%80%93-the-new-world-press-photo-rule/ 
29http://www.spiegel.de/international/world/growing-concern-that-news-photos-are-being-excessively-manipulated-a-
898509.html 
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 75 OF 139 
The evaluation covers an extensive range of feature extraction, matching and post-processing 
algorithms proposed in the past, and is accompanied by an experimental dataset. Table 26presents 
the Precision, Recall and F1 measures from [91], for all algorithms tested, in the case of simple 
translation without further image processing operations (e.g. blurring) performed on the copy-moved 
patch, and without regard for pixel-level accuracy. 
Table 27: Time and memory requirements for a number of copy-move detection algorithms [91] 
Method Feature Matching Postpr. Total Mem. 
BLUR 12059.66 4635.98 12.81 16712.19 924.06 
BRAVO 488.23 5531.52 156.27 6180.42 154.01 
CIRCLE 92.29 4987.96 19.45 5103.43 308.02 
DCT 28007.86 7365.53 213.06 35590.03 9856.67 
DWT 764.49 7718.25 119.66 8606.50 9856.67 
FMT 766.60 6168.73 8.07 6948.03 1732.62 
HU 7.04 4436.63 5.36 4452.77 192.51 
KPCA 6451.34 7048.83 88.58 13592.08 7392.50 
LIN 12.41 4732.88 36.73 4785.71 346.52 
LUO 42.90 4772.67 119.04 4937.81 269.52 
PCA 1526.92 4322.84 7.42 5861.01 1232.08 
SIFT 15.61 126.15 469.14 610.96 17.18 
SURF 31.07 725.68 295.34 1052.12 19.92 
SVD 843.52 4961.11 7.65 5816.15 1232.08 
ZERNIKE 2131.27 4903.59 27.23 7065.18 462.03 
Average 3549.41 4829.22 105.72 8487.63 2266.43 
 
In copy-move forgery detection, the issue of efficiency is a major one, falling directly within 
REVEAL’s field of interest. Table 27 presents the time (in seconds) taken to classify an entire 
dataset at each step of the copy-move detection pipeline, and the theoretical lower bound for 
memory requirements (in Megabytes). It is clear that there exists a 10-fold to 50-fold time difference 
between keypoint-based and block-based methods. Our own experiments (Figure 27, Figure 28) 
confirmed the significant discrepancy between block-based and keypoint-based methods as well, 
and concluded that block-based methods are, at the moment, unpractical for a real-time framework. 
Overall, copy-move detection methods are fairly straightforward in their conception, and, as they rely 
on visual cues, reasonably robust to potential further alterations of the forged image (e.g. rescaling, 
recompression).  
 
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 76 OF 139 
 
  
Figure 27: Detecting a real-world copy-move forgery. Top: Left: original image. Right: Forged image. 
Bottom: Left: detection using block search with Zernike moment descriptor (22.4 sec) Right: Detection 
with SURF keypoints (1.2 sec). 
 
  
Figure 28: A second example of real-world copy-move forgery. Detection with Zernike moment 
descriptor took 27.1 sec, and detection with SURF keypoints 2.7 sec. Notice how the two methods each 
detected different (but both true) copy-moved areas. Parameters were manually adapted to achieve 
detection, and are different from those in Figure 27. 
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 77 OF 139 
While the research field remains active, recently proposed methods commonly attempt to increase 
performance without regard for computational cost [92], [93], [94]. More recent, if less exhaustive 
surveys of the field [95], [96] demonstrate the same tendency in current approaches. On the other 
hand, as will be shown in paragraph 0, a number of splicing detection approaches can also be used 
for copy-move detection, as copy-moving often causes disturbances in the same patterns that 
splicing algorithms aim to detect. Such methods are particularly fast, and commonly do not require 
external parametrization, thus covering a large part of the need for specialized copy-move forgery 
detection. 
Splicing detection 
While copy-move detection algorithms seek repeated visual patterns within an image, splicing 
detection algorithms are based on a different premise: the assumption -and prerequisite- for these 
algorithms to work, is that the spliced region essentially carries information that, whilst possibly 
invisible to the eye, is in some significant aspect different than in the rest of the image. There are 
multiple modes of information that can be used to make this distinction. 
Images captured using digital cameras are almost always registered using a single sensor overlaid 
with a Color Filter Array (CFA). The result of overlaying a CFA over the sensor is that the captured 
information contains one value per image pixel, corresponding to a single color channel. Since an 
image requires three values per pixel, the missing values are estimated using an interpolation 
algorithm.  An overview of this process has been covered in the REVEAL deliverable D3.1. While the 
pixel value patterns resulting from CFA interpolation are subtle, they can be detected, and 
consecutively any local disruption in these patterns can be interpreted as the result of splicing. 
Splicing can detect the CFA patterns in multiple ways: one, not all cameras use the same CFA array 
or interpolation algorithm, so splices taken with different devices may stand out. Secondly, splices 
are often filtered or rescaled, which disrupts these patterns, and thirdly, even simply misaligning the 
splice with the rest of the image disrupts the pattern –in the latter case, since typical CFA array 
patterns follow a 2×2 grid, there is a 75% chance that any splice (or even copy-move) forgery will 
disrupt it.  Using local variations in the CFA interpolation statistics has shown encouraging results for 
splicing detection in the past [97], [98] –on the other hand, CFA patterns are extremely sensitive to 
any modification, even JPEG compression, which means that such methods can only operate on raw 
or losslessly compressed images. 
A further family of camera-based forgery detection methods concerns those based on Photo 
Response Non-Uniformity (PRNU) noise: different camera models and devices tend to introduce 
differently distributed noise in images, either because of their sensor and lens particularities or 
because of the specific post-processing used. Essentially, each individual camera device carries a 
unique noise signature that can be used to identify images taken with it. Many important algorithms 
have been proposed in the past, attempting to identify forgeries by detecting PRNU patterns [99], 
[100], [101]. A common feature of such algorithms is the fact that they require a large number of 
images taken from the same device to be available, in order to train a model of PRNU noise. 
Currently, this requirement completely rules out the possibility of using these methods in the context 
of a web-based, social media image analysis framework, since it is usually impossible to know which 
camera model was used to capture an image –especially so since many social media platforms tend 
to erase metadata from images- and realistically impossible to have a dataset of trained noise 
distribution models for every camera, cellphone and tablet available on the market at any time. 
On the other hand, the presence of inconsistencies in the distribution of local noise can be used to 
detect splicing without having to specifically identify the PRNU noise patterns of the specific camera 
used, by using a general noise identification model. Thus, in [102] images are Wavelet-filtered, and 
noise levels are estimated for each region separately from the high-frequency channel. 
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 78 OF 139 
Inconsistencies in the local standard deviation of different regions can be used to suggest different 
origins for these regions, hence a splice. In a similar rationale, in [103] local noise variance is 
estimated using the observation that different image sub-bands tend to have constant and positive 
kurtosis values in their coefficients, from which local discrepancies in noise variance can be 
estimated. 
An entirely different approach followed by a large part of the bibliography that attempts to take 
advantage of the particularities of JPEG compression to detect image alterations. One such 
technique is to look for disruptions of the square-block pattern imposed on the image by JPEG 
compression [104] [105] [106]. Even in high qualities, the fact that JPEG DCT-quantizes information 
in 8×8 blocks, aligned to the top left of the image, suggests that every splice may disturb the pattern 
left by a previous compression, unless it is perfectly aligned to the JPEG grid (a perfect alignment, if 
the splice is placed randomly, has a 1/64 chance of occurring). Disturbances in the 8×8 block pattern 
are used to suggest the presence of a splice. Another important JPEG-related phenomenon is the 
fact that, when a JPEG image is re-saved at a different quantization level without disrupting the block 
structure, a periodic pattern is created on the DCT coefficient histograms. A number of methods 
attempt to exploit the fact that, since the spliced region will have a different quantization history, the 
distribution of its DCT coefficients will stand out from that of the rest of the image [107] [108]. 
Another approach is to detect artefacts that are caused by non-aligned double JPEG compression, 
i.e. when the JPEG grid is shifted between compressions. This corresponds both to the scenario 
where a JPEG-compressed splice is placed misaligned within a JPEG image, and when a non-
compressed (blurred, rescaled or computer-generated) splice is placed in a JPEG image which has 
undergone alterations to its grid alignment (e.g. cropping) [109] [110]. Finally, a radically different 
approach is that of the "JPEG Ghosts", based on the observation that older JPEG quantizations tend 
to leave traces on the DCT coefficients that remain even following further recompressions. In [111] 
this is used to locally detect splices, by recompressing the image in multiple quantization levels and 
detecting the one that gives the best match for the candidate region –this must correspond to the 
compression level of the splice, if that was different from the original image. 
There also exists a small group of algorithms for the detection of image splicing that aim to detect 
discontinuities based on the spatial features of the spliced image, for example by detecting abrupt 
edges through Gray-Level Run Length features, [112] or through Local Binary Patterns over the 
Steerable Pyramid transformed image [113]. Finally, a number of algorithms have been proposed in 
the past that aim to take advantage of specific phenomena in image capture, in order to detect 
splicing. Three characteristic such cases are [114], where inconsistencies in lens-caused 
aberrations, and specifically a phenomenon called Purple Fringing Aberration (PFA) can be used to 
detect splicing in an image, [115] where inconsistencies in motion blur are used to detect image 
regions of different origins, and [116], where inconsistencies in shadow parameters are treated as 
indications of splicing. While very effective in particular cases, such approaches run the danger of 
being effective only under specific circumstances, for example [116] requires matte surfaces and 
outdoors environments, while [115] depends on the presence of motion blur. 
As a final note on our survey of the state-of-the-art, it should be noted that the taxonomy applied 
here is in no case absolute, since splicing detections methods can be used to detect copy-move 
forgeries. For example, a copy-move attack can disturb the JPEG grid in the target area where the 
copying occurs, or, if the spliced area is somehow resampled (scaled, sheared) or filtered (e.g. 
smoothed), it could even disturb image noise patterns, or cause an absence of double quantization 
artifacts. Examples of such cases will be presented in subsection 7.4. 
 
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 79 OF 139 
Experimental datasets 
Currently in research, there exist a number of benchmark datasets for the evaluation of tampering 
detection algorithms. Of these, the dataset of [91] and the CoMOFoD dataset [117] concern Copy-
Move detection, while the Columbia Image Splicing Detection Evaluation Dataset, the Columbia 
Uncompressed Image Splicing Detection Evaluation Dataset [118], the CASIA Tampered Image 
Detection Evaluation Database30 and the Visual Information Processing and Protection Group 
dataset [119] concern image splicing. However, none of these datasets are entirely appropriate for 
the real-world forensic scenario we have in mind. 
Table 28 helps highlight certain limitations of the aforementioned datasets with respect to the task of 
Web image verification. One is that the image format in most datasets is different from JPEG, which 
is the prevalent format on the Web: for example, among the contents of the Common Crawl 
Corpus31, 87% of identifiable image suffixes correspond to JPEG (.jpg, .jpeg). With respect to our 
task, within the Wild Web dataset we collected for our experiments using an exhaustive search, 
described in subsection 7.3, 95% of a total of 13577 forged images were in JPEG format. In contrast, 
only the VIPP datasets and a small part of CASIA v2.0 contain JPEG images. This leaves out the 
significant part of the bibliography that takes advantage of the effects of JPEG compression. 
Secondly, the absence of ground-truth masks in some datasets makes it very hard to evaluate 
algorithms producing localized results, which can be a crucial requirement for human investigators. 
Finally, a common characteristic of all datasets is their “neatness”, meaning that the images 
contained in them have undergone at most two lossy compressions, one as original images, and one 
following the splice. 
Table 28: A list of image splicing benchmark datasets. Format: the format of the images contained. 
Masks: the presence or absence of ground-truth binary masks giving the location of the splice. (Manual 
means the masks were manually constructed by us using the original and spliced images). # images: 
the number of authentic/spliced images in the dataset. 
Name Format Masks # images 
Columbia BMP grayscale No 933/912 
Columbia Uncomp. TIFF Uncomp. Yes 183/180 
CASIA TIDE v2.0 TIFF Uncomp. , JPEG, BMP No 7491/5123 
IPP Synth. JPEG Yes 4800/4800 
VIPP Real. JPEG Manual 63/68 
 
7.2 Image forensics in the wild 
The entire bulk of the methods presented in subsection 7.1 share a common characteristic: the 
detection accuracy of the algorithms presented degrades with each consecutive image 
transformation. Rescaling, histogram adjustments, cropping, further splicing (e.g. overlaying text or 
watermarks), or even plain resaving as a JPEG of different quality can seriously disrupt the 
performance of practically all algorithms. This is especially true for splicing detection: while copy-
move detection algorithms tend to degrade gracefully as the image is consecutively altered, any 
disturbance on the distribution of noise or high-frequency content of an image can radically violate 
the theoretical prerequisites of splicing detection algorithms, leading to a complete and immediate 
breakdown (at least in theory) of the entire detection process. 
                                                     
30http://forensics.idealtest.org 
31http://commoncrawl.org/ 
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 80 OF 139 
However, from our perspective, we are well aware that alterations on published images, such as 
rescaling, cropping, splicing or illumination adjustments are an extremely common practice in web 
contexts. Not only do users tend to apply such operations on images prior to sharing, but, even in the 
best of cases, many image sharing or social media platforms also automatically process images, 
without the user explicitly requesting it, or even actually desiring it (Figure 29). 
 
Figure 29: A depiction of the typical interceding stages between the committing of a new forgery and 
the forensic analysis of an image for tampering, in a real-world scenario. 
Twitter is the social medium of choice for journalists through which to follow unfolding events. In 
parallel, Facebook is by far the most successful social media platform today, which means that, by 
the time an image reaches a journalist, it is possible that it may have passed through Facebook at 
some point in its life cycle. Consequently, it was deemed essential for our work to identify the ways in 
which these social media platforms alter images and what to anticipate when collecting images from 
them. We thus attempted to reverse engineer the types of transformations they apply on images. 
In both cases, investigative accounts were set up, and images of various resolutions, compressed 
using two of the most popular internet formats (PNG and JPEG) at various qualities, were uploaded 
on these two social media platforms. The images were then downloaded again, and analysed in 
order to identify the transformations that had been applied to them by the platform. Our analysis 
concluded that the platforms use the following sets of rules for image uploading: 
Twitter: 
 If the image is larger than 2048 × 1024, rescale it using a Lanczos-3 kernel to fit these 
dimensions, while retaining the aspect ratio. 
 If the image is in PNG format, and is >3MB in size, convert to JPEG. 
 In any case that the final image is in JPEG format, re-save it at a quality factor of 75. 
Facebook: 
 If the image is larger than 2048 × 2048, rescale it to fit these dimensions while retaining the 
aspect ratio. 
 If the image is in PNG format, convert to JPEG. The only exception is for images uploaded 
to pages classified as “fan” or “business” that a) do not contain transparency and b) are 
already sufficiently small. 
Source (possibly 
untraceable) 
Original 
forgery 
Publication in 
web platforms 
Altered & re-saved 
versions 
Forensic 
analysis 
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 81 OF 139 
 In any case that the final image is in JPEG format, re-save it at a quality factor ranging 
between 70 and 91. We have been unable to identify the conditions for choosing the quality 
factor for each image 
While our investigation only covered these two popular platforms, the emerging pattern is clear. 
When dealing with such images, we can normally expect that, in the best case, a JPEG 
recompression will have taken place, and, when dealing with large original images, we can 
reasonably expect that the image will also have been rescaled. 
A second important issue in this area is the treatment of metadata from social media platforms. Past 
investigations, conducted in the same spirit as our own, but with respect to alterations performed on 
image metadata by such platforms32 observed that the entire metadata set is erased in most cases. 
This is another way in which social media destroy traces of manipulations since metadata , and 
especially Exif, could be used to explore the modification history of the image. 
7.3 The Wild Web tampered image dataset 
Within the context of REVEAL, we aim to bring multimedia verification to the web, featuring real-time, 
large-scale evaluations of real data (in our case, images), with a focus on news items. We were thus 
interested in evaluating the ability of the current state of the art to detect real, current or past 
forgeries -some of them referred to as “hoaxes”- circulating the Web, that had managed at their time 
to deceive journalists and/or simple users. 
 
 
Figure 30: Samples from our Wild Web dataset. Top: Left: an “Occupy” demonstration of airline pilots’ 
unions converted into an anti-chemtrail demonstration. Right: a scene from the movie Back To The 
Future, with the destination date altered from 21/10/2015 to 05/07/2010. Bottom: Right: a supposed 
“double sunset” captured by the curiosity rover. Two suns from a Star Wars film have been spliced into 
a real martian landscape. Left: a supposed image of a flooded mall with swimming sharks. The original 
is a photo of the flooded Union Station, into which two sharks were spliced. 
                                                     
32http://www.embeddedmetadata.org/social-media-test-procedure.php 
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 82 OF 139 
Of course, they would have to be confirmed forgeries, in the sense that the originals would have to 
have been already located and identified, and each case would have to have been unambiguously 
explained and closed. 
We were able to identify 82 such cases. However, for each case, we realized that a large number of 
different image versions existed, at different scales, and with different resave histories, for which it 
was impossible to identify and isolate the original first forgery uploaded by each perpetrator. We 
know that, in a realistic application of the forensic investigation scenario, the investigator will often 
have access to one or more alternative versions of the same image, which will have already 
undergone various transformations. To emulate this scenario, we downloaded all instances of each 
forgery that we could locate using Google reverse image search. Following a removal of exact 
duplicate files (using file hashes, not visual content), we were left with 13,577 images. 
 
Figure 31: Examples of a forgery's variants. Left: original forgery. Center: post-splice. Right: cropping. 
In further examining the resulting image collections, we concluded that the variants of the original 
forgery in each category could be organized into one or more of three categories (Figure 31):  
1. Versions of the same image at various scales, often at different aspect ratios, where it is not 
always obvious which version is closer to the original forgery 
2. Cropped versions of the original image, ranging from a few missing rows or columns, to 
significant clippings. 
3. What we call “post-splices”: watermarks, frames, or tongue-in-cheek splices added over the 
original forged images. 
In the course of building our evaluation framework, we decided to separate the obvious post-splices 
and croppings from the rest of the dataset thus keeping 9,751 images that were most likely to 
resemble (or be) the forgery originally uploaded by the perpetrator. Especially for the croppings, the 
reason was not only the degrading they cause in detection performance, but also the difficulties in 
creating ground-truth binary masks for them. In the case of rescaling (even at different aspect ratios), 
the same binary masks could apply to all instances, following a proportionate rescaling of the mask. 
On the other hand, each cropping would require its own binary mask, whose creation would be a 
very labour-intensive task. An exception to this practice was the cases where it was impossible to 
deduce which version of a forgery was closer to the original forgery (Figure 32). In these occasions, 
each version was kept in the dataset as a separate case with its own mask, bringing the number of 
classes to 92. 
Creating ground-truth masks was also a challenge, even when the originals were readily available. In 
many cases, multiple areas of the image contained different splices, possibly committed at different 
times. In those cases, we created binary masks for each case, and evaluated them separately, 
expecting to identify the analysis as a success even if one mask gave a match (Figure 33). This 
brought our dataset to 101 unique masks for evaluation over the 92 image groups. We call this 
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 83 OF 139 
dataset the Wild Web Image Manipulation Dataset, and intend to make it public in the hope of 
assisting future research towards real-world image forensic analysis. 
 
 
Figure 32: Top: Three versions of the same forgery, in which we were unable to deduce the most likely 
to contain detectable traces of tampering, thus keeping each as a separate case.  Bottom: the source 
images of the forgery. 
     
Figure 33: From left to right: a forgery, the original source, and the three binary masks created by us 
attempting to model the separate steps of the forgery. 
Having completed our preliminary investigation and data collection, we proceeded with a large-scale 
evaluation of the state-of-the-art, focusing on real-world conditions, in order to assess which 
methods might prove appropriate for REVEAL. 
Besides the Wild Web dataset, we decided to emulate the effects of typical social media –imposed 
operations on the existing experimental datasets as well. Thus, two additional versions of each 
dataset presented in Table 28 was created. One included all the images, resaved as JPEG at a 
quality level of 75, and the second included the same images, rescaled at 75% their original size, 
and again resaved as JPEG of quality 75. Thus the two major disruptive operations applied on 
images uploaded to social media were emulated on the existing research datasets, to estimate the 
degradation caused by these operations on image detection. 
7.4 Experimental evaluation 
Having completed our preliminary investigation and data collection, we proceeded with a large-scale 
evaluation of the state-of-the-art, focusing on real-world conditions, in order to assess which 
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 84 OF 139 
methods might prove appropriate for REVEAL. The evaluation methodology and results presented 
here, along with the Wild Web dataset, were also published in the Web Multimedia Verification 
workshop, in the context of the 2015 IEEE ICME conference [120]. 
Besides the Wild Web dataset, we decided to emulate the effects of typical social media–imposed 
operations on the existing experimental datasets as well. Thus, two additional versions of each 
dataset presented in Table 28 was created. One included all the images, resaved as JPEG at a 
quality level of 75, and the second included the same images, rescaled at 75% their original size, 
and again resaved as JPEG of quality 75. Thus the two major disruptive operations applied on 
images uploaded to social media were emulated on the existing research datasets, to estimate the 
degradation caused by these operations on image detection. 
We then proceeded to create or acquire implementations of all recent successful splicing detection 
algorithms. Thus, the code for [108] [97], [110], [103], [114] was provided by the authors, while we 
further proceeded to implement [107], [102], [111], [112], [106]. In all cases, the code was in 
MATLAB. Of these, [108], [97], [110], [107], [102] and [111] provided reasonably stable and 
generalizable results to allow for large-scale evaluations. We will describe these algorithms in more 
detail below.  
The method proposed in [97] is based on detecting inconsistencies (artifacts) in CFA interpolation 
patterns. The algorithm begins by presuming to know the actual CFA pattern used by the camera 
when the picture was captured. Assuming a 2×2 CFA pattern, one channel will have 2 captured 
values and 2 interpolated values per block, while the other two channels will only have one captured 
value and 3 interpolated values. For the channel having the 2 values(usually Green), we form a 
model of the relationship between the geometric mean of captured values and the geometric mean 
of the interpolated values. Using an EM algorithm, a mixture model is trained with respect to the 
presence or absence of CFA artifacts over the image, and a likelihood map is then generated for 
each area. 
A major issue with this method is its reliance on knowing beforehand the CFA pattern used to 
capture the image. While the Bayer filter mosaic (Figure 34) is extremely common in modern 
cameras and is assumed in the standard implementation, it is generally possible that the image 
might be following a different pattern, or that an imperceptible operation, such as cropping one line 
from the left or top might disrupt this assumption. 
 
Figure 34: Left: the Bayer filter mosaic. Right: the lattice showing Acquired (A) and Interpolated (I) 
pixels for the green channel, in the Bayer filter (Source: [97]). 
In our experiments, we extended the methodology of [10] with the CFA pattern detection algorithm 
used in [121], which allows us to re-estimate the CFA pattern used in each image separately. The 
pattern is chosen out of a list of possible ones, which include all reasonable candidates, both those 
used in modern cameras and those that could result from cropping the image. 
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 85 OF 139 
[102] is a noise-based method, based on the assumption that noise can be isolated in the high 
frequency sub-bands of a wavelet decomposition. A noise variance estimation method is drawn from 
[122], and image blocks are then merged based on similar variance values, effectively achieving 
image segmentation based on local noise variances. In our implementation, the merging step is 
skipped and the noise variance map is the algorithm’s final output, to bring the method in line with 
other approaches. 
The method proposed in [107] is based on the detection of double JPEG quantization artifacts, and 
in particular their absence from certain parts of the image. Since JPEG compression produces a 
distinctive detectable periodicity in the histogram of each JPEG coefficient, it is possible to estimate 
the parameters of this periodicity for an image, and then evaluate the extent in which each image 
region conforms to it or not. Areas whose coefficients appear to be derived from a uniform 
distribution, rather than the periodic pattern estimated for the image, are then assumed to belong to 
a spliced region which has been blurred, scaled or otherwise resampled so as to destroy the double 
quantization pattern. 
[108] was proposed as a direct improvement over [107] by including a step for estimating the 
quantization matrix of the first compression, and using this information to model the probability of 
each block being either doubly or singly quantized. This solves the problem of estimating the 
histogram periodicity from the entire image indiscriminately as in [107], since this means that even 
the singly-quantized parts of the image are used for estimation. [108] reports better results than 
[107], however we opted to retain both methods in our evaluations, for reasons that will become 
apparent. 
[110] actually includes two separate algorithms, one aimed at detecting aligned double JPEG 
compression artifacts, and one aimed at detecting non-aligned JPEG compression artifacts, denoted 
in the rest of this report as [110]A and [110]NA respectively. [110]A falls within the same family of 
methods as [107] and [108], bearing significant resemblance to [108], while [110]NA assumes that 
the base image (upon which the splice was pasted) underwent a second JPEG compression after 
being cropped by a number of rows and/or columns that was not a direct multiple of 8, and thus the 
second recompression was not aligned to the first. Essentially, a similar methodology is followed for 
estimation of the spliced areas, with the additional step of using an EM algorithm for detecting the 
optimal values for (r,c), where r and c the number of rows and columns that were cropped, 
respectively, i.e. the shift in the JPEG grid between the two compressions. 
Finally, [111] is based on the concept of the JPEG Ghost. The authors demonstrate that when a 
splice from one image, that had been compressed with a JPEG quality Q1 is inserted in a different 
image, and the composite is then compressed at a different JPEG quality Q2, a trace (ghost) of the 
original compression remains in the composite image, in the area of the splice, in the DCT coefficient 
patterns. Furthermore, the ghost can also be found in the spatial pixel domain, especially if the effect 
is taken over larger image blocks. The practical result of this effect is that, if the image is 
recompressed at quality Q1 and subtracted from the composite, there will be quantization noise 
found throughout the image area, except for the spliced region, since the ghost will have found a 
perfect match. In practice, the authors suggest recompressing the image in multiple qualities and 
subtracting each from the original composite, to detect local minima in the differences, which should 
account for JPEG ghosts. This generates a series of image difference maps. Then, if splicing from a 
JPEG image of different quality has occurred, artifacts should occur at that image quality. 
While such a framework is difficult to automatize, when supervised by a human user it is fairly easy 
to detect splices, as objects that stand out (or disappear) at specific compression qualities, different 
from the rest of the image, are highly likely to be originating from different sources. 
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 86 OF 139 
 
 
 
 
Figure 35: Examples of applying the implemented algorithms on experimental datasets. Top row: [97], 
[102] and [103] applied on an image from the Columbia Uncompressed Dataset. Second and third rows: 
[107], [110]A and [111] applied on images from the VIPP Synthetic and VIPP Realistic datasets. Notice 
how the “splice” in row 3 is essentially a copy-move forgery, and is easily detected by all three 
methods. 
Our aim was to evaluate all implemented methods on both our artificial and Wild Web datasets. Prior 
to proceeding with the evaluations, however, we had to deal with the issue of choosing an evaluation 
protocol and measure of success. When evaluating tampering detection algorithms over a dataset, 
the criteria used for determining success are integral to the evaluation. While many algorithms return 
a binary result for the entire image [105], [112], [109], thus simplifying evaluation at the cost of 
localization, many other return a surface, whose values correspond to local phenomena, as in Figure 
35. In the latter case, evaluation of experimental results requires the existence of a binary ground 
truth mask, signifying the actual location of the tampered region. Consecutively, a typical evaluation 
protocol is to contrast values of the surface under the mask with those outside it. For example, we 
can estimate the statistical median of the map values for the pixels/blocks under the mask with the 
median of those outside it [119], or evaluate the Kolmogorov-Smirnov statistic on the two value 
distributions [111]. While such protocols have served the research community well in the past, we 
consider them inappropriate for our use case for two reasons. One is their potential for poor 
localization, in cases where large areas outside the mask return high values, but not large enough 
for classification to be recognized as a failure by the median or the KS statistic. Figure 36 presents 
such an actual example, where the resulting localization is clearly wrong, but because the region 
outside the mask is dominated by zero (blue) values enough for the median to be zero, it is finally 
classified as a success. Second and most important is their unrealistic concept of a false positive: 
such protocols can declare a false positive only when they find significant differences in a 
predetermined area in a non-spliced image. From the perspective of a non- or semi-expert 
investigator, detection algorithms ought to return reliable, clearly interpretable results highlighting the 
forged areas of the image and only those. Any results whose area does not match the ground truth 
should be considered false, and so should non-spliced images returning any salient map area, 
anywhere on the image. 
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 87 OF 139 
 
Figure 36: Evaluation by comparing the medians inside and outside the mask (white circle). Due to the 
uniform background, the median outside the mask is very low, but from a real-world perspective this is 
a completely false output. The algorithm used is [107] over a rescaled image of the VIPP Realistic 
dataset. 
In our proposed evaluation approach the output produced by each algorithm is first binarized using a 
method-appropriate threshold (i.e. for each algorithm, the possible thresholds reflect the meaning 
and range of the values returned), and then image morphological processing operations (such as 
opening and closing) are applied, to remove noisy and retain connected regions. The resulting binary 
images are then compared to the ground truth mask, to evaluate the quality of the match. The 
criterion for comparing the mask produced by the algorithm to the ground truth binary mask is 
expressed by: 
𝐸(𝐴, 𝑀) =
∑(𝐴 ∩ 𝑀)2
∑(𝐴) × ∑(𝑀)
 
where A signifies the binary, processed algorithm output, M is the ground truth mask, and ∑(x) is the 
area of a binary mask x. Experimentally, any value of E(A,M) above 0.65 suggests a very good 
match which is very unlikely to have resulted by chance.  
The detection of false positives, on the other hand, follows a different approach. In contrast to other 
protocols, we are not expected to have a binary mask for each non-spliced image. Instead, following 
a similar thresholding and morphological processing step, we expect the algorithm output not to 
demonstrate any salient regions. The presence of such a region is classified as a false positive. 
While this means that, essentially, a different criterion is used for true positive and false positive 
detection, nonetheless we hold this protocol to more closely reflect our real-world task requirements. 
We first ran the available algorithms on the experimental data set and their modified (recompressed 
and rescaled) versions. Out of all the available datasets, we chose Columbia Uncompressed and the 
two VIPP datasets, because they offered binary masks for evaluation. For comparison, we present 
results both using a typical evaluation protocol (comparing the median feature values inside and 
outside the mask, Table 29, top values) and our thresholding protocol (bottom values). In both 
approaches, the first value reflects the detection rate (True Positives) while the value in parentheses 
is the false alarm rate (False Negatives). For the typical methodology of comparing the medians 
inside and outside the mask, the binary mask used for the non-spliced images was a parallelogram 
in the centre of the image, similar to [119]. For the difference between medians, we chose thresholds 
that gave FN rates no larger than 0.05. For our own protocol, we applied multiple fixed and adaptive 
thresholds, and a battery of different morphological operations. For both positive and negative 
examples, we kept the best (i.e., closer to ground truth) result, to reflect the ability of a human 
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 88 OF 139 
inspector to detect the presence or absence of distinctive patterns. In our evaluation approach, there 
is no general threshold to set that applies both for True Positives and False positives, as we attempt 
to model the perception of a human evaluator. This means that the approach does not allow for 
adjusting FP/FN rates to create a ROC curve. Instead, using an array of morphological operations, 
we attempt to find whether the returned map can perceptually match the ground-truth binary mask. 
Table 29: Results of our implemented algorithms over the experimental datasets and their altered 
(recompressed and rescaled) versions. Within each cell, the top row corresponds to True Positives 
(True Negatives) by comparing the median under the value mask to the median value outside it, while 
the bottom row corresponds to our, human-centric measure. 
Dataset [107] [108] [97] [110]A [110]NA [102] 
Col.U. - - 0.89 (0.05) - - 0.39 (0.04) 
Orig. - - 0.66 (0.16) - - 0.12 (0.57) 
VIPP-S. 0.47 (0.05) 0.51 (0.05) 0.15 (0.05) 0.57 (0.01) 0.28 (0.05) 0.13 (0.05) 
Orig. 0.44 (0.27) 0.52 (0.00) 0.01 (0.23) 0.58 (0.09) 0.04 (0.25) 0.04 (0.74) 
VIPP-R. 0.54 (0.04) 0.58 (0.04) 0.04 (0.04) 0.70 (0.04) 0.28 (0.04) 0.20 (0.04) 
Orig. 0.41 (0.46) 0.38 (0.09) 0.09 (0.22) 0.23 (0.30) 0.03 (0.39) 0.04 (0.90) 
Col.U. - - 0.05 (0.05) - - 0.09 (0.05) 
JPEG - - 0.00 (0.20) - - 0.02 (0.86) 
VIPP-S. 0.30 (0.04) 0.43 (0.04) 0.17 (0.05) 0.39 (0.05) 0.16 (0.05) 0.10 (0.05) 
JPEG 0.26 (0.30) 0.30 (0.10) 0.01 (0.28) 0.23 (0.27) 0.01 (0.29) 0.04 (0.74) 
VIPP-R. 0.32 (0.04) 0.36 (0.04) 0.14 (0.04) 0.51 (0.04) 0.17 (0.04) 0.20 (0.04) 
JPEG 0.13 (0.44) 0.17 (0.29) 0.00 (0.25) 0.14 (0.46) 0.01 (0.43) 0.02 (0.90) 
Col.U. - - 0.03 (0.04) - - 0.11 (0.05) 
resamp. - - 0.00 (0.24) - - 0.04 (0.79) 
VIPP-S. 0.05 (0.05) 0.05 (0.05) 0.05 (0.04) 0.05 (0.05) 0.05 (0.05) 0.06 (0.05) 
resamp. 0.00 (0.23) 0.00 (0.00) 0.00 (0.23) 0.00 (0.15) 0.00 (0.29) 0.00 (0.84) 
VIPP-R. 0.13 (0.04) 0.12 (0.06) 0.03 (0.04) 0.23 (0.04) 0.17 (0.04) 0.18 (0.04) 
resamp. 0.00 (0.47) 0.00 (0.00) 0.00 (0.28) 0.03 (0.25) 0.01 (0.47) 0.01 (0.47) 
 
The results of Table 29 show how typical evaluation methodologies tend to overestimate the 
performance of most algorithms, especially with respect to False Positives. By extending the search 
for false positives to anywhere in the image instead of a comparative binary mask, we see that most 
methods –perhaps with the notable exception of [108]- tend to produce some persistent alert at 
some region in a non-tampered image. In this sense, however, the worst offender is by far [102], 
whose false alarm rates are extremely high. 
A second observation is the tendency of algorithms to specialize: [97] and [102] tend to perform far 
better in the Columbia Uncompressed dataset than in the VIPP ones, while [107],  [108], and [110], 
being JPEG-based methods, can only operate on the VIPP dataset. 
A third major observation is the extreme sensitivity of the algorithms to image transformations, 
especially using our own evaluation framework. The most sensitive method seems to be [97], which 
breaks down entirely with any image alteration. In parallel, it is interesting to observe that, under the 
typical evaluation methodology, the algorithm performance seems to slightly increase with image 
transformations. This is clearly an artefact of typical evaluation frameworks, where random results 
can easily occur due to noise. Overall, the results on the experimental frameworks are rather 
discouraging, as they indicate that the typical alterations that web platforms apply on images can 
completely disrupt the best detection algorithms available today. 
The second set of evaluation runs was conducted over the Wild Web dataset. The results can be 
seen in Table 30. We present the number of cases where an algorithm achieved correct detection for 
at least one image of each case. The number in parentheses (“Unique”) indicates the number of 
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 89 OF 139 
cases where a particular algorithm was the only one to achieve detection for that case. In this 
evaluation, we included one additional algorithm. [111] is more difficult to evaluate with respect to 
false positives, as its output tends to be rather noisy and consists of multiple images, many of which 
are bound to contain images that are, to an extent, salient. In an automatic detection framework, this 
method would produce numerous false positives, as some area is always expected to stand out at 
some level. On the other hand, visual inspection (with the semantic awareness of a human 
investigator) tends to give very good results, so we opted to include it separately. However, its 
localization also tends to be relatively poor, so we reduced the area matching threshold to 0.45 - still 
high enough to discard random findings. For fairness, we did not take the findings from [111] into 
account when counting unique hits from the other methods, while unique findings from [111] are 
counted against all findings from all other methods. 
Table 30: Algorithm evaluation over the Wild Web dataset. 
 [107] [108] [97] [110]A [110]NA [102] [111] 
Detections 13 12 1 8 5 15 29 
(Unique) (4) (1) (0) (1) (2) (6) (10) 
 
 
 
Figure 37: Successful detection results over the Wild Web dataset. Notice how the copy-move forgery of 
Figure 27 (third from left) was detected using the splicing detection algorithm of [110]. 
Overall, out of 82 cases, 57 (47 when including [111]) gave no detection, while 25 (35) had at least 
one method correctly detect and localize the forgery in at least one image. However, visual 
inspection suggests that even our approach may be overestimating the output, especially for [111] 
and [102], with real recognition capabilities being even below that level. 
Of the seven algorithms tested on our data, the least successful by far is the CFA-based method of 
[97]. Indeed, this algorithm also seriously underperformed in all versions of the VIPP dataset and the 
recompressed/rescaled versions of the Columbia Uncompressed dataset. Since the CFA 
interpolation patterns are extremely sensitive to JPEG compression, as well as any alteration or 
resampling of the pixel values, it was generally expected that it would not be able to withstand the 
JPEG compression of the VIPP dataset, or the following recompressions and rescaling of our social 
media emulations. Similarly, on the Wild Web dataset, the method was successful in only one case, 
demonstrating the worst performance of all methods tried. The explanation for this result is again to 
be expected, since images in the web are rarely expected to be in the pristine state required by [97] 
to function properly. 
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 90 OF 139 
On the other hand, out of the JPEG-based algorithms, [107] and [108] are by far the most 
successful, and remain closely related to each other. They detect the same phenomena, and to a 
certain extent they significantly overlap on the cases they can successfully detect. Surprisingly, [107] 
gives slightly better results, although we should keep in mind that is also more prone to false 
positives. 
On the subject of methods that are prone to false positives, [102] and [111] both appear to be 
particularly successful. Both in shared and unique detections, they contribute a significant portion of 
the final successful detections. However, we should keep in mind that these two methods are also 
the noisiest -especially [102] tends to return results often affected by differences in local pixel 
variance (Figure 38). This means that, in many cases where the spliced area has also different 
variance although this is semantically not suspicious, such as in the third binary mask of Figure 33, 
where the “page” border surrounding the photo would wield a different noise distribution, not 
because of the splicing but because of the visual characteristics of the image content in question. 
 
Figure 38: an image from the Wild Web dataset, one binary mask related to the case, and the output of 
[102] over the image. While the match is very exact, and it is possible that this is a correct detection of 
the last splicing operation performed on the composite image, it is also likely that the letters returned 
high-frequency noise in terms of image content, and the finding is irrespective of the actual tampering 
process. 
The method of [111] on the other hand is less prone to produce content variance-related results, and 
its detections are theoretically better supported, and is overall more reliable. The problem is that 
[111] produces a large volume of output, since one map is produced per JPEG compression quality. 
This leads to a sequence of 50-70 maps per image, where a human user can assess if, at some 
level, an object is absent while the rest of the scene is present, or vice versa. There are ways to 
attempt to automatize the output, by detecting local minima in the mean image difference –a local 
minimum can correspond to a match found between the splice and the source image, and, as a 
result, can be used to isolate only the quantization levels that yield interesting results. Even following 
such a filtering process, however, one such evaluation can yield 5 or 6 different maps, each of which 
can contain a correct detection of the spliced area -or not (Figure 39). This means that, in any image 
evaluation, we will be presented with a sequence of images, some of which will certainly contain 
spurious, noise- and variance- related results. Evaluation by a human analyst would most certainly 
help discern actual candidate splices from spurious, noise-related output, but an automatic 
binarization scheme would most likely result in at least one “finding”, either true of false-flag, for 
every and each image submitted. 
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 91 OF 139 
 
 
Figure 39: Successful application of [111] over the Wild Web dataset. Top row: a forgery (left) and the 
mean squared difference between the image and its recompressed versions at various qualities (30-
100). Bottom row: absolute image differences at the local minima: from left to right, 60, 91 and 100 –
which is not a local minimum per se, but often returns ghosts, similar to Error Level Analysis (ELA)33, so 
we include it in our analysis. In our case, clear ghosts are returned at both 91 and 100, indicating and 
localizing a splice, while the small local minimum at 60 returns pure noise. 
A deeper analysis of the results over the Wild Web dataset gives a more detailed perspective on the 
situation. In the vast majority of classes where the tampering was correctly detected, only a small 
number of images in that class gave a positive result. With the exception of a small number of 
classes where the tampered region was large, where [102] and [111] managed to yield a true 
positive over a large number of instances, only a tiny minority of images was properly classified. 
However, the successful classifications in the classes where noise-based methods gave good 
success rates could always be attributed not to actual splicing detection, but to differences in local 
image variance between the splice and the rest of the image (which could be dismissed by a human 
analyst as false alarms). 
A detailed analysis of the number of successes per case/variant/mask/algorithm combination is 
provided in Appendix E. By surveying the classification performance of the evaluated algorithms for 
each class, we can come to certain observations. 
On the other hand, [102] and [111] give disproportionately high success rates in a few classes, 
especially in cases where the spliced area is very large. In parallel, in most other cases, only a 
handful of forgeries are correctly detected. Indeed, in total, out of the 9,751 images in the dataset, 
1386 (14.34%) were correctly detected. However, leaving out three top classes where [102] and 
[111] methods gave very high success rates (“BlackLion”,”WeBuiltDebt”,”WorkMoose”) and which 
have very large masks, there are only 333 successful detections in 8580 files, which gives us a final 
3% detection rate per-image.  
This gives us a clear picture concerning the efficiency of tampering detection methods in the wild: 
while, in a large-scale data collection process, we can be reasonably hopeful that an instance of an 
image will be downloaded which will give a successful detection, in any isolated case where we are 
examining an image using any state-of-the-art detection algorithm, chances are that we will not be 
able to detect a forgery.  
This is even truer with respect to images posted in social media platforms, which have undergone 
transformations such as rescaling or recompression. As was already made clear in Table 29, such 
                                                     
33 http://fotoforensics.com/ 
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 92 OF 139 
operations heavily disrupt most splicing detection algorithms, to the point of full collapse. Thus, it is 
likely that what we have established, by creating the Wild Web tampered image dataset is to gather, 
for a number of cases where it was available, the original forgery or a very close resave of that 
image, for which some algorithms manage to detect the forgery, and a vast number of further 
resaves (rescaled, slight cropped, histogram-equalized) upon which all known algorithms fail entirely. 
However, the research we presented in subsection 7.2 suggests that images originating from social 
media sources will most likely fall into the second category, meaning that, at the moment, the task of 
performing forensic analysis on images originating from social media sources remains an open 
research question. 
7.5 Toolbox prototype implementation 
Having concluded the formation of an evaluation framework, the collection and organization of the 
Wild Web tampered image dataset and the large-scale evaluations of the state of the art over our 
data, we began the development of the image forensics–related modules of the REVEAL Multimedia 
Manipulation Detector Toolbox. During this stage in the project, the Toolbox is aimed to operate at 
the prototype level, offering the core functionalities of a chosen set of tampering detection 
algorithms, and a rudimentary API for evaluation. 
Based on our evaluations, we chose three algorithms out of our Matlab toolbox, which showed the 
most promise in real-world conditions. These were [107], [102] and [111]. The three of them cover a 
relatively diverse range of phenomena (JPEG double quantization, noise patterns, JPEG Ghost), and 
were responsible for most of the successful detections during our experiments with the Wild Web 
dataset. 
All three algorithms were implemented as Java packages within the same project, sharing many of 
the common image manipulation functionalities necessary for each algorithm while retaining a 
modular, extensible structure. The choice of Java as the core language was made with an eye 
towards system-independence, since the Toolbox is aimed to be deployed server-side, on possibly 
unknown platforms whose exact configurations might also be unknown. 
With respect to external dependencies, we only used open, standard library implementations for our 
code. Specific dependencies are explained below, alongside the forensic algorithm that requires 
them. Besides these, one major dependency concerned the need to process images stored in TIFF 
format, which, although not very common in real-world internet situations, is necessary for evaluating 
our implementations on experimental datasets. The Java Advanced Imaging input-output (Jai-
imageio), provided by Java.net (https://java.net/projects/jai-imageio) has to be included in the project. 
Overall, as will be explained below, while some concessions were made on the subject of system 
independence in the form of native library dependencies, these were kept to a minimum in order to 
maximize portability, at least in Windows and Linux systems. This degree of portability was achieved 
simply with a small need for a few configuration steps. In the following sub-paragraphs, we will 
describe the current specifics of the three algorithm implementations, their dependencies, APIs and 
functionalities. 
JPEG double quantization inconsistencies 
Our Java implementation of [107] closely follows our previous Matlab implementation. In its current, 
prototype form, the class takes a filename as input, and outputs a localization map of per-block 
probability of tampering, while at the same time writing to disk a PNG file containing a visualization of 
the same localization map. 
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 93 OF 139 
[107] detects splicing by searching for inconsistencies in the periodicity of DCT coefficient 
histograms. This means that, for the algorithm to operate, we must be in possession of the DCT 
coefficient tables of a JPEG image. Currently, to our knowledge there exists a single way of directly 
acquiring this information from a JPEG file, save from reading and decoding the binary stream 
manually, and this is the libjpeg C library. 
Libjpeg was developed and is maintained by the Independent JPEG group (http://www.ijg.org/), and 
its current version is 9a. It is open, free for use, modification and redistribution (including commercial 
applications) and currently used by a wide range of image handling software. It is provided as 
source, to be compiled per-system, although it is shipped as part of many Linux distributions 
(including Ubuntu). It offers a C API for importing JPEG images and their header information, and a 
Java API exists on a variant of the core library (http://libjpeg-turbo.virtualgl.org/). 
However, directly extracting the raw quantized DCT coefficients (and, if needed, the quantization 
tables) is far from straightforward, even using libjpeg. In our Matlab experiments, we used the 
jpeg_toolbox, which is a Matlab API based on libjpeg, and used precompiled Windows Mex files. 
However, for Java integration, we had to develop a small, simple extension to libjpeg, using which 
we could export the necessary tables. We wrote a short C interface (ExportDCT.c), sharing libjpeg’s 
headers, which exposes the intermediate DCT data stored during image loading by libjpeg. The 
interface, like libjpeg itself, has no dependencies on native system libraries, and uses only core C 
functionalities so that it can easily be compiled in any platform. 
Calling C binaries from Java was achieved using the Java Native Access (JNA) library, which is 
available for all common operating systems. In order to minimize the dependency on C 
functionalities, our C interface simply returns a sequential array of integer pointers containing the 
DCT coefficients, plus the dimensions of the DCT array, so it can be reconstructed using Java. Thus, 
for the JPEG DQ inconsistencies detector to function, the system requires libjpeg and JNA, plus 
compiling our C interface (ExportDCT.c) and adding the binary to the classpath.  
In terms of dependencies, all other functionalities of the implementation are purely Java-based and 
fully portable. Most of the implementation is based on simple arithmetic operations, which do not call 
for specialized libraries. One exception is the Fast Fourier Transform (FFT) implementation that is 
required for periodicity detection. Our current choice was to incorporate a well-established 
implementation (org.apache.commons.math3.transform.FastFourierTransformer) which, while not 
the fastest available, is appropriate for our prototype implementation due to its portability. However, 
to the extent that user requirements demand a faster implementation of the algorithm, and since the 
FFT is definitely going to be a computational bottleneck, we retain the option of replacing it with a 
native, possibly distributed or even GPU-based implementation, to ensure better performance. 
 
Figure 40: A tampered image, the localization map from our Matlab implementation of [107], and the 
localization map from our Java Prototype implementation. 
Figure 40 shows an image from the Wild Web dataset, the localization map from the Matlab 
implementation used in our experiments so far, and the localization map from our Java 
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 94 OF 139 
implementation. Despite a few minor discrepancies mostly due to differences in the FFT 
implementation in the two environments, it is clear that the output from the Java implementation is 
equally reliable to our MATLAB original –even, in that case, slightly less prone to noise. 
Noise variance 
Our Java implementation of [102] was also built with portability in mind, and in this case we 
succeeded in building an entirely Java-based library. At the heart of the algorithm in [102] rests a 
two-dimensional Discreet Wavelet Transform (2-D DWT) using a Daubechies-8 wavelet, which is 
used to select the High-High frequency image sub-band, in which the noise content of the image is 
assumed to rest. Of the various DWT Java implementations currently available, we chose one of the 
simplest: Mark H. Bishop’s "Discrete Wavelet Transforms, a Java Implementation", publicly available 
on the Code Project34 performs a one-dimensional DWT with a number of standard filters, including 
the Daub-8. We incorporated the code in our pilot and used the definition of the 2-D DWT (i.e., 
filtering the image by rows, and then filtering the output by columns) to isolate the high-frequency 
content. Following the DWT, the per-block noise variance is detected using simple arithmetic 
operations, and the output map is produced. In this version of the algorithm, local values correspond 
to noise variance, and thus cannot offer a specific estimate on the probability of the image being 
tampered. Instead, localized and intense inconsistencies can be considered as suggestive of 
tampering (Figure 41). 
 
Figure 41: A tampered image, the localization map from our Matlab implementation of [102], and the 
localization map from our Java Prototype implementation. Discrepancies are due to the different DWT 
implementations. Both localization maps have been median-filtered for display purposes. 
JPEG Ghosts 
The algorithm of [111] is also JPEG-based, however, in contrast to [107], no special feature of the 
JPEG encoding is required. Instead, the image can be loaded using any common image loading 
library, and then processed as a raster map, from which the “ghosts” of previous JPEG 
compressions may be detected. Indeed, JPEG ghosts can appear even in non-JPEG, lossless files 
(e.g. PNG), if they have undergone JPEG compressions in the past. 
The implementation presented in the original paper [111] is Matlab-based, and conceptually simple: 
the image is loaded and then consecutively re-saved and re-loaded at different JPEG qualities using 
Matlab’s imwrite and imread functions. The resulting images are subtracted from the original, and the 
output is processed to create the tampering localization maps. This approach, while algorithmically 
adequate in the context of experimental research, is computationally highly inefficient, as it requires 
a massive amount of disk read/write operations, which, in a scalable platform such as REVEAL could 
prove disastrous. In our implementation, we proceeded to develop a memory-based method of the 
algorithm, using the disk only for the initial reading and the storage of the final output –of course, in 
the final implementation even those could be replaced with memory input/output operations. 
                                                     
34 http://www.codeproject.com/Articles/825687/Discrete-Wavelet-Transforms-a-Java-Implementation 
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 95 OF 139 
One fundamental issue with the algorithm of [111] is its inherent inability to produce a simple output 
that can be visually inspected by a non-expert, and produce in parallel a clear confidence value on 
the result. In its original form, the algorithm produces a large number of images –one per possible 
JPEG compression quality- one (or a few) of which contain traces of ghosts, while the rest contain 
compression noise. In their own research results, Farid et al evaluate the output values of all 
resulting images using ground-truth masks which are known in advance, and declare a True Positive 
whenever at least one image demonstrates different values under the mask than outside it. This 
approach is inadequate for our needs, as a) we cannot guarantee that the ground truth mask will be 
known in advance and b) such an approach ignores the possibility of false positives. Essentially, this 
is the same problem we dealt with before with our proposed evaluation protocol. However, in this 
case we need a different way to tackle the issue, so as to make it easily interpretable by a human 
(possibly non-expert) investigator. 
One feasible solution is to output all images produced by the algorithm, and display them in 
sequence as a video or animated GIF, in which JPEG Ghosts will appear as sudden black holes or 
bright regions, corresponding to semantically significant parts of the image, which the investigator 
will be able to identify visually. This solution is relatively simple to implement, given the status of our 
current Prototype Toolbox, however we opted for a more nuanced approach, based on an 
observation by Farid [111], that when comparing the mean image difference between the suspect 
image and its various recompressed versions, often local minima appear at the JPEG qualities from 
which the Ghosts originated. This means that we can simply choose a small number of images 
(usually from 0 to 5) which are the most likely candidates to contain the Ghost. We thus proceed with 
locating and returning the differences between the suspect image and the qualities that produce local 
minima, and also include the difference at 100% quality. While the latter does not strictly fall within 
the JPEG Ghost theory, the residue produced by this subtraction is what Error Level Analysis (ELA) 
detects, a method which has been popularized by the online FotoForensics35 service, and whose 
output we incorporated in the JPEG Ghost algorithm due to its high relevance and similarity to JPEG 
Ghosts, alongside its high efficiency. 
 
Figure 42: A tampered image and the corresponding localization maps from our Java implementation of 
[111] at JPEG qualities 85, 93 and 95, (which are entirely identical to those from the Matlab toolbox), as 
well as 100 (which is strictly not a JPEG Ghost but rather an Error Level Analysis result. 
Implementation details 
The implementation has been tested and has demonstrated full functionality on Netbeans IDE 8.0.2, 
Java 1.7.0_51 and 1.8.0_40, in Windows 7 64-bit and Ubuntu 14.04 64-bit (the latter in a Virtualbox 
virtual machine). Noise-based and Ghost-based detection should be directly portable to any other 
system with an up-to-date Java platform, while DCT-based detection would first require locally 
compiling and linking the necessary C libraries –although, with respect to Web-based application, 
64-bit Windows and Linux servers ought to cover the vast majority of use-cases. 
                                                     
35 http://fotoforensics.com/ 
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 96 OF 139 
The currently distributed package MultimediaManipulationPackage.zip contains three subdirectories, 
(jpeg-8d_distribution, ExportDCT and MultimediaManipulationToolbox) and the accompanying 
documentation.jpeg-8d_distribution contains version 8d of libjpeg, which can also be downloaded 
from http://www.ijg.org/.ExportDCT contains our bridging C code for extracting the DCT coefficients 
from a JPEG file, and MultimediaManipulationToolbox is the actual Netbeans project containing the 
Toolbox. The documentation includes a folder and file list, as well as dependency installation 
instructions for Windows 64 and Linux systems. The latter concerns both libjpeg and ExportDCT. 
The Toolbox in its current state was implemented as a Netbeans 8 project, containing each algorithm 
as a separate package, plus one package for generic utility classes and a main API class which is 
currently used for demonstration. The project produces a single JAR file which takes a URL as input, 
produces a number of output maps in the form of images written to disk, and returns the filenames 
for these images alongside value and confidence metrics, in the form of a ForensicAnalysis object. 
The .jar also includes a main class that performs processing on  a demo image whose filename is 
hardcoded and is shipped alongside the toolbox. 
pkg01_DoubleQuantization, pkg07_waveletnoisemap and pkg08_JPEGGhostcontain the three 
algorithms. Each package contains a Main class with a main() method, which takes a filename 
(String) as input and writes an output image to disk containing the image evaluation map. 
Furthermore, the each package contains the main algorithm class, which, when called by the main() 
class, produces a ProbabilityMaporNoiseMap, depending on the algorithm, which gives the values of 
the output map and can be used for confidence evaluation, as well as a DisplaySurface object of 
type ImageBuffer which contains the visualization of the values using the JET colormap (which was 
also used for the figures of this section). The JPEG Ghosts algorithm, being different in nature, 
buildsGhostMaps a List of BufferedImage objects, and GhostQualities, the list of corresponding 
JPEG qualities at which these images were produced. As the Toolbox is at an experimental stage, 
no API was developed to export these values, however they are public and can be taken as such by 
importing a class in a different project. 
Besides these core classes, the Utils package is the utility package containing all necessary image 
and mathematical processing operations, such as filtering and converting numerical arrays to and 
from images. Furthermore, precompiled versions of the 64-bit Windows .dll files are contained in a 
folder named “/win32-x86-64/”, such as JNA requires. The sources project also contain two folders 
which are not used, but for which it was considered preferable to be carried and compressed in the 
.jar file: AuxiliaryLibs contains the precompiled.so files for Linux, in the hope that they might work in a 
new system without compiling anew, while Images contains all three demo input images and all the 
expected (correct) output images by the three algorithms. 
In its current format, the project has ToolboxAPI as a main class which in turn calls 
pkg01_DoubleQuantization.main(),pkg07_waveletnoisemap.main() and pkg08_JPEGGhost.main() 
using try/catch mechanisms, to ensure that if one package fails for some reason, the rest will still 
proceed. As our work progresses, the toolbox will expand from its current form to provide a richer 
API (or GUI) and incorporate further algorithms in sync with our research findings and progress in 
the field. 
  
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 97 OF 139 
8 Conclusions & Future Work 
This deliverable describes several contributions to the research areas defined in the REVEAL project 
by contributing novel algorithms for image crawling, retrieval and clustering, fake tweet detection, 
event summarization and relation extraction, as well as evaluating the state of the art in image 
forensics and author profiling, with respect to the needs of REVEAL. Furthermore, we have 
contributed two datasets to the research community, with which we hope to foster further research 
towards our aims. Finally, we have functional implementations of the toolboxes required for the 
REVEAL platform which, during the following year are to be further expanded and refined as we 
make further progress.  
Besides an introductory summary of the contributions and novelties achieved so far, we also 
provided an overview of the overall architecture in the shape it is now taking, and finally presented 
each new component, alongside the results achieved, evaluations run, contributions to the scientific 
community in the form of datasets and code, and finally the implementations of these tools currently 
incorporated in the REVEAL platform, where appropriate. 
As the project runs its second year, technical details have been finalized and the majority of the WP3 
components have been implemented, integrated and are now fully functional. The next step, 
scheduled for September-October 2015 is making the WP3 modules interoperable with modules 
from other work packages, a process which will result in the first integrated REVEAL platform 
prototype. During both design and development, we have ensured that each module is independent, 
and communicates with the orchestrator by means of a messaging mechanism. This process 
safeguards us from unforeseen compatibility or scalability issues that might have arisen otherwise 
during integration, and could have allowed individual module issues to compromise the entire 
platform. 
During the third year of the project, we will mainly concentrate on task T3.3 by continuously refining 
the existing implementations, testing new approaches and extending the modules with new 
functionalities, which were envisioned by the project requirements. Furthermore, we also have some 
concrete ideas for the modules of task T3.2, which will continue running until month 28. We plan to 
extend the proposed summarization method described in section 4 by using more advanced topic 
modelling techniques that identify not only topics but also hierarchies and relations between them. 
Regarding ranking we plan to investigate the use of co-ranking algorithms to rank mutually text and 
image nodes in a more principled way. Eventually, we intend to integrate additional features such as 
users’ popularity, influence and trustworthiness, as recent research indicates that these could 
improve the results, and especially the quality of the selected images. Additionally we would like to 
extend the verification corpus described in section 0 and we aspire to make it known to the research 
community by organizing a task in MediaEval 2015, so that it is adopted as a reference benchmark 
for fake tweet detection tasks.  
In terms of task T3.3, our results in PAN 2015 concerning author profiling described in section 6 
show that there is considerable room for improvement in the age and personality trait subtasks. In 
the gender subtask, our results indicate that n-grams performed well as features for the gender 
classification task in a variety of languages, however, a more general assessment which 
encompasses different tweet corpora of variable sizes is needed in order to justify our conclusions. 
In the age and personality trait subtasks we need to reconsider the features used. Specifically, a 
more sophisticated and case tailored selection of features is needed in order to improve them. 
 
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 98 OF 139 
During the third year of the project, we will focus on user attributes which are important in the context 
of REVEAL and gradually converge to a form that will be most useful and rewarding for the 
journalistic as well as the enterprise scenario. Next immediate steps towards improving our results 
are feature reconsideration as mentioned above, as well as a more twitter focused approach to 
natural language processing which could mitigate some of the inherent difficulties of processing short 
noisy text snippets. Future research steps include combining twitter user graph information when 
profiling users, as well as generalizing results to other datasets through transfer learning. 
As far as the multimedia forensics research area is concerned described in section 7, we have 
concluded the first stage of our work, by surveying the field, locating and reproducing the most 
important recent algorithms, and identifying the common benchmark datasets used in research 
evaluations. Furthermore, we identified the areas in which the state of the art is inadequate for our 
particular task, and made a first effort in bridging that gap by introducing a task-appropriate dataset. 
In our next steps, we will move towards the same direction of detecting forgeries in images that have 
been altered (resaved, rescaled, and cropped) by web platforms. We intend to achieve this firstly by 
maintaining, distributing and possibly expanding the Wild Web dataset, so as to foster research in 
the desired direction, and secondly by researching more robust detection methods for such images. 
This could entail the identification of novel traces that are left behind by forgeries, and are not so 
easily disrupted by image alterations, of the introduction of machine learning frameworks in the hope 
that more robust features will emerge. In the first case, the preliminary results of our evaluations 
already point to certain algorithms that are more robust than others, and we will continue our 
investigation. In the second case, we will attempt to employ the current state of the art in machine 
learning, trained in datasets of tampered and re-altered images (such as the Wild Web dataset), in 
our effort to extract distinctive features that distinguish forged images from camera originals. 
  
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 99 OF 139 
9 References 
 
[1]  C. Silverman, "Lies, Damn Lies and Viral Content. How news websites spread (and debunk) 
online rumors, unverified claims and misinformation.," Columbia Journalism School. Tow 
Center for Digital Journalism., 2015. 
[2]  A. Marcus, M. S. Bernstein, O. Badar, D. R. Karger, S. Madden and R. C. Miller, "Twitinfo: 
Aggregating and visualizing microblogs for event exploration," in SIGCHI Conference on 
Human Factors in Computing Systems, New York City, NY, 2011.  
[3]  M. Mathioudakis and N. Koudas, "Twittermonitor: Trend detection over the twitter stream," in 
ACM SIGMOD International Conference on Management of Data, New York City, NY, USA, 
2010.  
[4]  L. Weng, F. Menczer and Y. Ahn, "Predicting successful memes using network and community 
structure," in ICWSM-14 The 8th Internation Conference on Weblogs and Social Media, Ann 
Arbor, MI, 2014.  
[5]  L. Xie, A. Natsev, J. R. Kender, M. Hill and J. R. Smith, "Visual memes in social media: 
Tracking real-world news in youtube videos," in 19th ACM International Conference on 
Multimedia, MM'11, New York City, NY, USA, 2011.  
[6]  G. Petkos, S. Papadopoulos and Y. Kompatsiaris, "Social event detection using multimodal 
clustering and integrating supervisory signals," in 2nd ACM International Conference on 
Multimedia Retrieval, ICMR'12, New York City, NY, USA, 2012.  
[7]  L. Kennedy and S. F. Chang, "Internet image archaeology: Automatically tracing the 
manipulation history of photographs on the web," in 16th ACM International Conference on 
Multimedia, MM'08, New York City, NY, USA, 2008.  
[8]  K. Andreadou, S. Papadopoulos, L. Apostolidis, A. Krithara and Y. Kompatsiaris, "Media 
REVEALr: A Social Multimedia Monitoring and Intelligence System for Web Multimedia 
Verification," in Pacific Area Workshop on Intelligence and Security Informatics, Ho Chi Minh 
City, Viet Nam, 2015.  
[9]  J. Sander, M. Ester, H.-P. Kriegel and X. Xu, "Density-Based Clustering in Spatial Databases: 
The Algorithm GDBSCAN and Its Application," Data Mining Knowledge Discovery, vol. 2, no. 2, 
pp. 169-194, 1998.  
[10]  S. Papadopoulos, D. Corney and L. M. Aiello, "SNOW 2014 data challenge: Assessing the 
performance of news topic detection methods in social media," in SNOW 2014 Data Challenge 
co-located with 23rd International World Wide Web Conference (WWW 2014), Seoul, Korea, 
2014.  
[11]  M. Hersovici, M. Jacovi, Y. S. Maarek, D. Pelleg, M. Shtalhaim and S. Ur, "The shark-search 
algorithm, an application: Tailored web site mapping," in 7th International Conference on World 
Wide Web (WWW7), 1998.  
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 100 OF 139 
[12]  S. Chakrabarti, M. van den Berg and B. Dom, "Focused crawling: A new approach to topic-
specific web resource discovery," in 8th International Conference on World Wide Web (WWW 
'99), New York City, NY, USA, 1999.  
[13]  A. Maedche, M. Ehrig, S. Handschuh, R. Volz and L. Stojanovic, "Ontology-focused crawling of 
documents and relational metadata," in 11th International World Wide Web Conference (WWW 
2002), Honolulu, Hawai, 2002.  
[14]  K. Andreadou, S. Papadopoulos and Y. Kompatsiaris, "Web image size prediction for efficient 
focused image crawling," in CBMI (13th International Workshop on Content-Based Multimedia 
Retrieval), Prague, 2015.  
[15]  L. Del Corro and R. Gemulla, "ClausIE: clause-based open information extraction," in WWW, 
2013.  
[16]  W. R. Van Hage, V. MalaisÃ©, R. Segers, L. Hollink and G. Schreiber, "Design and use of the 
Simple Event Model (SEM)," Web Semantics: Science, Services and Agents on the World 
Wide Web, vol. 9, no. 2, 2011.  
[17]  F. Benevenuto, G. Magno, T. Rodrigues and V. Almeida, "Detecting spammers on twitter," in 
Collaboration, electronic messaging, anti-abuse and spam conference (CEAS), 2010.  
[18]  M. McCord and M. Chuah, "Spam detection on twitter using traditional classifiers," Springer, 
2011.  
[19]  C. D. Manning, M. Surdeanu, J. Bauer, J. Finkel, S. J. Bethard and D. McClosky, "The Stanford 
CoreNLP Natural Language Processing Toolkit," in ConferenceProceedings of 52nd Annual 
Meeting of the ACL, 2014.  
[20]  L. Derczynski, D. Maynard, G. Rizzo, M. van Erp, G. Gorrell, R. Troncy, J. Petrak and K. 
Bontcheva, "Analysis of named entity recognition and linking for tweets," Information 
Processing & Management, vol. 51, no. 2, 2015.  
[21]  Y. Hu, A. John, D. D. Seligmann and F. Wang, "What Were the Tweets About? Topical 
Associations between Public Events and Twitter Feeds.," in ICWSM, 2012.  
[22]  M., M. Schmitz, R. Bart, S. Soderland and O. Etzioni, "Open language learning for information 
extraction," in ConferenceProceedings of the 2012 Joint Conference on Empirical Methods in 
Natural Language Processing and Computational Natural Language Learning, 2012.  
[23]  D. Klein and C. D. Manning, "Accurate unlexicalized parsing," in ConferenceProceedings of the 
41st Annual Meeting on ACL, 2003.  
[24]  L. M. Aiello, G. Petkos, C. Martin, D. Corney, S. Papadopoulos, R. Skraba, A. Goker, I. 
Kompatsiaris and A. Jaimes, "Sensing trending topics in Twitter," in Multimedia, 2013.  
[25]  D. Radev, H. Jing, M. Stys and D. Tam, "Centroid-based summarization of multiple 
documents.," Information Processing & Management, vol. 40, no. 6, pp. 919-938, 2004.  
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 101 OF 139 
[26]  G. Erkan and D. R. Radev, "LexRank: graph-based lexical centrality as salience in text 
summarization," Journal of Artificial Intelligence Research, pp. 457-479, 2004.  
[27]  J. Nichols, J. Mahmud and C. Drews, "Summarizing sporting events using twitter," in 
Proceedings of the 2012 ACM international conference on Intelligent User Interfaces, 2012.  
[28]  F. C. T. Chua and S. Asur, "Automatic Summarization of Events from Social Media.," in 
ICWSM, 2013.  
[29]  C. Shen, F. Liu, F. Weng and T. Li, "A Participant-based Approach for Event Summarization 
Using Twitter Streams.," in HLT-NAACL, 2013.  
[30]  D. Chakrabarti and K. Punera, "Event Summarization using Tweets," in ICWSM, 2011.  
[31]  O. Alonso and K. Shiells, "Timelines as summaries of popular scheduled events," in 
Proceedings of the 22nd international conference on World Wide Web companion, 2013.  
[32]  M. Dork, D. Gruen, C. Williamson and S. Carpendale, "A visual backchannel for large-scale 
events," Visualization and Computer Graphics, IEEE Transactions on, vol. 16, no. 6, pp. 1129-
1138, 2010.  
[33]  J. Bian, Y. Yang and T.-S. Chua, "Multimedia summarization for trending topics in microblogs," 
in Proceedings of the 22nd ACM international Conference on information & knowledge 
management, 2013.  
[34]  C. Lin, C. Lin, J. Li, D. Wang, Y. Chen and T. Li, "Generating event storylines from microblogs," 
in Proceedings of the 21st ACM international conference on Information and knowledge 
management, 2012.  
[35]  P. J. McParlane, A. J. McMinn and J. M. Jose, "Picture the scene...;: Visually Summarising 
Social Media Events," in Proceedings of the 23rd ACM International Conference on Information 
and Knowledge Management, 2014.  
[36]  E. Spyromitros-Xioufis, S. Papadopoulos, I. Kompatsiaris, G. Tsoumakas and I. Vlahavas, "A 
Comprehensive Study Over VLAD and Product Quantizationin Large-Scale Image Retrieval," 
2014.  
[37]  E. Mantziou, S. Papadopoulos and Y. Kompatsiaris, "Large-scale semi-supervised learning by 
approximate laplacian eigenmaps, VLAD and pyramids," in Image Analysis for Multimedia 
Interactive Services (WIAMIS), 2013 14th International Workshop on, 2013.  
[38]  G. Palla, I. Derenyi, I. Farkas and T. Vicsek, "Uncovering the overlapping community structure 
of complex networks in nature and society," Nature, vol. 435, no. 7043, pp. 814-818, 2005.  
[39]  X. Xu, N. Yuruk, Z. Feng and T. A. Schweiger, "Scan: a structural clustering algorithm for 
networks," in Proceedings of the 13th ACM SIGKDD international conference on Knowledge 
discovery and data mining, 2007.  
[40]  Q. Mei, J. Guo and D. Radev, "Divrank: the interplay of prestige and diversity in information 
networks," in Proceedings of the 16th ACM SIGKDD international conference on Knowledge 
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 102 OF 139 
discovery and data mining, 2010.  
[41]  A. J. McMinn, Y. Moshfeghi and J. M. Jose, "Building a large-scale corpus for evaluating event 
detection on twitter," in Proceedings of the 22nd ACM international Conference on information 
& knowledge management, 2013.  
[42]  A. Gupta, H. Lamba, P. Kumaraguru and A. Joshi, "Faking sandy: characterizing and 
identifying fake images on twitter during hurricane sandy.," in In Proceedings of the 22nd 
international conference on World Wide Web companion (pp. 729-736). International World 
Wide Web Conferences Steering Committee., 2013.  
[43]  C. Boididou, S. Papadopoulos, I. Kompatsiaris, S. Schifferes and N. Newman, "Challenges of 
computational verification in social multimedia. In Proceedings of the companion publication of 
the 23rd international conference on World wide web companion (pp. 743-748).," in 
International World Wide Web Conferences Steering Committee., April,2014.  
[44]  Z. Gyöngyi and H. Garcia-Molina, "Link spam alliances.," in In Proceedings of the 31st 
international conference on Very large data bases (pp. 517-528). VLDB Endowment., 2005.  
[45]  Z. Gyöngyi, H. Garcia-Molina and J. Pedersen, "Combating web spam with trustrank.," in In 
Proceedings of the Thirtieth international conference on Very large data bases-Volume 30 (pp. 
576-587). VLDB Endowment., 2004.  
[46]  C. Castillo, D. Donato, A. Gionis, V. Murdock and F. Silvestri, "Know your neighbors: Web 
spam detection using the web topology.," in In Proceedings of the 30th annual international 
ACM SIGIR conference on Research and development in information retrieval (pp. 423-430). 
ACM., 2007.  
[47]  L. Becchetti, C. Castillo, D. Donato, S. Leonardi and R. Baeza-Yates, "Link-Based 
Characterization and Detection of Web Spam.," in In AIRWeb (pp. 1-8)., 2006.  
[48]  V. Qazvinian, E. Rosengren, D. Radev and Q. Mei, "Rumor has it: Identifying misinformation in 
microblogs. In Proceedings of the Conference on Empirical Methods in Natural Language 
Processing (pp. 1589-1599)," in Association for Computational Linguistics., 2011.  
[49]  E. Seo, P. Mohapatra and T. Abdelzaher, "Identifying rumors and their sources in social 
networks.," in In SPIE Defense, Security, and Sensing (pp. 83891I-83891I). International 
Society for Optics and Photonics., 2012,May.  
[50]  M. Mendoza, B. Poblete and C. Castillo, "Twitter Under Crisis: Can we trust what we RT?.," in 
In Proceedings of the first workshop on social media analytics (pp. 71-79). ACM., 2010,July.  
[51]  G. Stringhini, C. Kruegel and G. Vigna, " Detecting spammers on social networks.," in In 
Proceedings of the 26th Annual Computer Security Applications Conference (pp. 1-9). ACM., 
2010,December.  
[52]  F. Cheong and C. Cheong, "Social Media Data Mining: A Social Network Analysis Of Tweets 
During The 2010-2011 Australian Floods.," in PACIS, 11, 46-46., 2011.  
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 103 OF 139 
[53]  K. Canini, B. Suh and P. Pirolli, "Finding credible information sources in social networks based 
on content and social structure.," in In Privacy, Security, Risk and Trust (PASSAT) and 2011 
IEEE Third Inernational Conference on Social Computing (SocialCom), 2011 IEEE Third 
International Conference on (pp. 1-8). IEEE., 2011,October.  
[54]  K. Starbird, G. Muzny and L. Palen, "Learning from the crowd: Collaborative filtering 
techniques for identifying on-the-ground Twitterers during mass disruptions.," in Proc. 
ISCRAM., 2012.  
[55]  A. Gupta and P. Kumaraguru, "Twitter explodes with activity in mumbai blasts! a lifeline or an 
unmonitored daemon in the lurking?.," 2012.  
[56]  Z. Chu, S. Gianvecchio, H. Wang and S. Jajodia, "Who is tweeting on Twitter: human, bot, or 
cyborg?.," in In Proceedings of the 26th annual computer security applications conference (pp. 
21-30). ACM., 2010,December.  
[57]  J. Martinez-Romo and L. Araujo, "Detecting malicious tweets in trending topics using a 
statistical analysis of language.," Expert Systems with Applications, 40(8), 2992-3000., 2013.  
[58]  A. Gupta, H. Lamba and P. Kumaraguru, "$1.00 per rt# bostonmarathon# prayforboston: 
Analyzing fake content on Twitter. In eCrime Researchers Summit (eCRS), 2013 (pp. 1-12).," 
in IEEE, 2013,September.  
[59]  C. Castillo, M. Mendoza and B. Poblete, "Information credibility on twitter.," in In Proceedings 
of the 20th international conference on World wide web (pp. 675-684). ACM., 2011,March.  
[60]  E. Spyromitros-Xioufis, S. Papadopoulos, I. Kompatsiaris, G. Tsoumakas and I. Vlahavas, "An 
empirical study on the combination of SURF features with VLAD vectors for image search.," in 
In Image Analysis for Multimedia Interactive Services (WIAMIS), 2012 13th International 
Workshop on (pp. 1-4). IEEE., 2012.  
[61]  J. Redondo, I. Fraga, I. Padrón and M. Comesaña, "The Spanish adaptation of ANEW 
(affective norms for English words)," Behavior research methods, vol. 39, no. 3, pp. 600-605, 
2007.  
[62]  P. Kanske and S. A. Kotz, "Leipzig affective norms for German: A reliability study," Behavior 
research methods, vol. 42, no. 4, pp. 987-991, 2010.  
[63]  P. Eckert and S. McConnell-Ginet, Language and gender, Cambridge University Press, 2003.  
[64]  J. Holmes and M. Meyerhoff, The handbook of language and gender, vol. 25, John Wiley & 
Sons, 2008.  
[65]  F. Mosteller and D. L. Wallace, "Inference in an authorship problem: A comparative study of 
discrimination methods applied to the authorship of the disputed Federalist Papers," Journal of 
the American Statistical Association, vol. 58, no. 302, pp. 275-309, 1963.  
[66]  E. Stamatatos, "A survey of modern authorship attribution methods," Journal of the American 
Society for information Science and Technology, vol. 60, no. 3, pp. 538-556, 2009.  
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 104 OF 139 
[67]  P. Juola, "Authorship attribution," Foundations and Trends in information Retrieval, vol. 1, no. 
3, pp. 233-334, 2006.  
[68]  G. Mikros and K. Perifanos, "Authorship Attribution in Greek Tweets Using Author’s Multilevel 
N-Gram Profiles," 2013.  
[69]  R. Layton, P. Watters and R. Dazeley, "Authorship Attribution for Twitter in 140 Characters or 
Less," in Cybercrime and Trustworthy Computing Workshop (CTC), 2010 Second, 2010.  
[70]  R. Schwartz, O. Tsur, A. Rappoport and M. Koppel, "Authorship Attribution of Micro-
Messages.," in EMNLP, 2013.  
[71]  S. C. Herring and J. C. Paolillo, "Gender and genre variation in weblogs," Journal of 
Sociolinguistics, vol. 10, no. 4, pp. 439-459, 2006.  
[72]  S. Argamon and A. R. Shimoni, "Automatically categorizing written texts by author gender," 
Literary and Linguistic Computing, vol. 17, pp. 401-412, 2003.  
[73]  J. D. Burger, J. Henderson, G. Kim and G. Zarrella, "Discriminating Gender on Twitter," in 
ConferenceProceedings of the Conference on Empirical Methods in Natural Language 
Processing, Stroudsburg, PA, USA, 2011.  
[74]  C. Fink, J. Kopecky and M. Morawski, "Inferring Gender from the Content of Tweets: A Region 
Specific Example.," in ICWSM, 2012.  
[75]  D. Bamman, J. Eisenstein and T. Schnoebelen, "Gender identity and lexical variation in social 
media," Journal of Sociolinguistics, vol. 18, no. 2, pp. 135-160, 2014.  
[76]  C. Peersman, W. Daelemans, L. Van Vaerenbergh and ACM, "Predicting age and gender in 
online social networks," in ConferenceProceedings of the 3rd international workshop on Search 
and mining user-generated contents, 2011.  
[77]  J. Schler, M. Koppel, S. Argamon and J. W. Pennebaker, "Effects of Age and Gender on 
Blogging.," in AAAI Spring Symposium: Computational Approaches to Analyzing Weblogs, 
2006.  
[78]  D. Rao, D. Yarowsky, A. Shreevats, M. Gupta and ACM, "Classifying latent user attributes in 
twitter," in ConferenceProceedings of the 2nd international workshop on Search and mining 
user-generated contents, 2010.  
[79]  D. Nguyen14, D. Trieschnigg14, A. S. Dogruöz23, R. Gravel, M. Theune, T. Meder and F. de 
Jong, "Why gender and age prediction from tweets is hard: Lessons from a crowdsourcing 
experiment," 2014.  
[80]  J. Golbeck, C. Robles, M. Edmondson, K. Turner and IEEE, "Predicting personality from 
twitter," in Privacy, Security, Risk and Trust (PASSAT) and 2011 IEEE Third Inernational 
Conference on Social Computing (SocialCom), 2011 IEEE Third International Conference on, 
2011.  
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 105 OF 139 
[81]  S. Argamon, M. Koppel, J. Fine and A. R. Shimoni, "Gender, genre, and writing style in formal 
written texts," TEXT, vol. 23, pp. 321-346, 2003.  
[82]  A. Mukherjee, B. Liu and Association for Computational Linguistics, "Improving gender 
classification of blog authors," in ConferenceProceedings of the 2010 conference on Empirical 
Methods in natural Language Processing, 2010.  
[83]  K. Filippova and Association for Computational Linguistics, "User demographics and language 
in an implicit social network," in ConferenceProceedings of the 2012 Joint Conference on 
Empirical Methods in Natural Language Processing and Computational Natural Language 
Learning, 2012.  
[84]  F. Rangel, P. Rosso, M. Koppel, E. Stamatatos and G. Inches, "Overview of the author profiling 
task at PAN 2013," Notebook Papers of CLEF, pp. 23-26, 2013.  
[85]  E. Amigó, J. Carrillo-de-Albornoz, I. Chugur, A. Corujo, J. Gonzalo, E. Meij, M. de Rijke and D. 
Spina, "Overview of RepLab 2014: author profiling and reputation dimensions for Online 
Reputation Management," Springer, 2014, pp. 307-322. 
[86]  G. Giannakopoulos, V. Karkaletsis, G. Vouros and P. Stamatopoulos, "Summarization System 
Evaluation Revisited: N-gram Graphs," ACM Trans. Speech Lang. Process., vol. 5, no. 3, pp. 
5-1, oct 2008.  
[87]  F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. 
Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. 
Perrot and E. Duchesnay, "Scikit-learn: Machine Learning in Python," Journal of Machine 
Learning Research, vol. 12, pp. 2825-2830, 2011.  
[88]  J. Redi, W. Taktak and J.-L. Dugelay, "Digital image forensics: a booklet for beginners," 
Multimedia Tools and Applications, vol. 51, no. 1, pp. 133-162, 2011.  
[89]  G. K. Birajdar and V. H. Mankar, "Digital image forgery detection using passive techniques: A 
survey," Digital Investigation, vol. 10, no. 3, pp. 226-245, 2013.  
[90]  C. M. Stamm, M. Wu and K. J. Ray Liu, "Information Forensics: An Overview of the First 
Decade," IEEE Access, vol. 1, no. 1, pp. 167-200, 2013.  
[91]  V. Christlein, C. Riess, J. Jordan, C. Riess and E. Angelopoulou, "An Evaluation of Popular 
Copy-Move Forgery Detection," IEEE Transactions on Information Forensics and Security, vol. 
7, no. 6, pp. 1841-1854, 2012.  
[92]  K. Li, H. Li, B. Yang, Q. Meng and S. Luo, "Detection of Image Forgery Based on Improved 
PCA-SIFT," in Computer Engineering and Networking, vol. 277, , Springer International 
Publishing, 2014, pp. 679-686. 
[93]  I. Amerini, L. Ballan, R. Caldelli, A. Del Bimbo, L. Del Tongo and G. Serra, "Copy-move forgery 
detection and localization by means of robust clustering with J-Linkage," Signal Processing: 
Image Communications, vol. 28, no. 6, pp. 659-669, 2013.  
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 106 OF 139 
[94]  K. Sunil, D. Jagan and M. Shaktidev, "DCT-PCA Based Method for Copy-Move Forgery 
Detection," in ICT and Critical Infrastructure: Proceedings of the 48th Annual Convention of 
Computer Society of India- Vol II, 2014.  
[95]  M. Ali Qureshi and M. Deriche, "A review on copy move image forgery detection techniques," in 
Multi-Conference on Systems, Signals & Devices (SSD), 2014 11th International, , 2014.  
[96]  N. Diane, W. Nanda, S. Xingming and F. Kue Moise, "A Survey of Partition-Based Techniques 
for Copy-Move Forgery Detection," The Scientific World Journal, vol. 1, no. 1, pp. 1-13, 2014.  
[97]  P. Ferrara, T. Bianchi, A. De Rosa and A. Piva, "Image Forgery Localization via Fine-Grained 
Analysis of {CFA} Artifacts," IEEE Transactions on Information Forensics and Security, vol. 7, 
no. 5, pp. 1566-1577, 2012.  
[98]  A. C. Gallagher and T. H. Chen, "Image authentication by detecting traces of demosaicing," in 
Computer Vision and Pattern Recognition Workshops, 2008. CVPRW'08. IEEE Computer 
Society Conference on, 2008.  
[99]  M. Chen, J. Fridrich, M. Goljan and J. Lukás, "Determining image origin and integrity using 
sensor noise," Information Forensics and Security, IEEE Transactions on, vol. 3, no. 1, pp. 74-
90, 2008.  
[100]  C.-T. Li and Y. Li, "Color-decoupled photo response non-uniformity for digital image forensics," 
Circuits and Systems for Video Technology, IEEE Transactions on, vol. 22, no. 2, pp. 260-271, 
2012.  
[101]  G. Chierchia, G. Poggi, C. Sansone and L. Verdoliva, "A Bayesian-MRF approach for PRNU-
based image forgery detection," Information Forensics and Security, IEEE Transactions on, 
vol. 9, no. 4, pp. 554-567, 2014.  
[102]  B. Mahdian and S. Saic, "Using noise inconsistencies for blind image forensics," Image and 
Vision Computing, vol. 27, no. 10, pp. 1497-1503, 2009.  
[103]  S. Lyu, X. Pan and X. Zhang, "Exposing Region Splicing Forgeries with Blind Local Noise 
Estimation," International Journal of Computer Vision, vol. 110, no. 2, pp. 202-221, 2014.  
[104]  S. Ye, Q. Sun and E.-C. Chang, "Detecting digital image forgeries by measuring 
inconsistencies of blocking artifact," in Multimedia and Expo, 2007 IEEE International 
Conference on, , 2007.  
[105]  W. Luo, Z. Qu, J. Huang and G. Qiu, "A Novel Method for Detecting Cropped and 
Recompressed Image Block," in International Conference on Accoustics Speech and Signal 
Processing, , 2007.  
[106]  Y.-L. Chen and C.-T. Hsu, "Detecting Recompression of JPEG Images via Periodicity Analysis 
of Compression Artifacts for Tampering Detection," IEEE Transactions on Information 
Forensics and Security, vol. 6, no. 2, pp. 396-406, 2011.  
[107]  Z. Lin, J. He, X. Tang and C.-K. Tang, "Fast, automatic and fine-grained tampered {JPEG} 
image detection via {DCT} coefficient analysis," Pattern Recognition, vol. 42, no. 11, pp. 2492-
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 107 OF 139 
2501, 2009.  
[108]  T. Bianchi, A. De Rosa and A. Piva, "Improved DCT coefficient analysis for forgery localization 
in JPEG images," in Acoustics, Speech and Signal Processing (ICASSP), 2011 IEEE 
International Conference on, , 2011.  
[109]  T. Bianchi and A. Piva, "Detection of Nonaligned Double {JPEG} Compression Based on 
Integer Periodicity Maps," IEEE Transactions on Information Forensics and Security, vol. 7, no. 
2, pp. 842-848, 2012.  
[110]  T. Bianchi and A. Piva, "Image Forgery Localization via Block-Grained Analysis of JPEG 
Artifacts," IEEE Transactions on Information Forensics and Security, vol. 7, no. 3, pp. 1003-
1017, 2012.  
[111]  H. Farid, "Exposing digital forgeries from JPEG ghosts," IEEE Transactions on Information 
Forensics and Security, vol. 4, no. 1, pp. 154-160, IEEE Transactions on Information Forensics 
and Security.  
[112]  X. Zhao, J. Li, S. Li and S. Wang, "Detecting digital image splicing in chroma spaces," in Digital 
Watermarking, , Springer Berlin Heidelberg, 2011, pp. 12-22. 
[113]  G. Muhammad, M. H. Al-Hammadi, M. Hussain and G. Bebis, "Image forgery detection using 
steerable pyramid transform and local binary pattern," Machine Vision and Applications, vol. 
25, no. 4, pp. 985-995, 2014.  
[114]  I. Yerushalmy and H. Hel-Or, "Digital Image Forgery Detection Based on Lens and Sensor 
Aberration," International Journal of Computer Vision, vol. 92, no. 1, pp. 71-91, 2011.  
[115]  P. Kakar, N. Sudha and W. Ser, "Exposing Digital Image Forgeries by Detecting Discrepancies 
in Motion Blur," IEEE Transactions on Multimedia, vol. 13, no. 3, pp. 443-452, 2011.  
[116]  Q. Liu, X. Cao, C. Deng and X. Guo, "Identifying Image Composites Through Shadow Matte 
Consistency," IEEE Transactions on Information Forensics and Security, vol. 6, no. 3-2, pp. 
1111-1122, 2011.  
[117]  D. Tralic, I. Zupancic, S. Grgic and M. Grgic, "CoMoFoD - New database for copy-move forgery 
detection," in ELMAR, 2013 55th International Symposium, 2013.  
[118]  Y.-F. Hsu and S.-F. Chang, "Detecting Image Splicing using Geometry Invariants and Camera 
Characteristics Consistency," in Multimedia and Expo, 2006 IEEE International Conference on. 
IEEE, 2006.  
[119]  M. Fontani, T. Bianchi, A. De Rosa, A. Piva and M. Barni, "A Framework for Decision Fusion in 
Image Forensics Based on Dempster-Shafer Theory of Evidence," IEEE Transactions on 
Information Forensics and Security, vol. 8, no. 4, pp. 593-607, 2013.  
[120]  M. Zampoglou, S. Papadopoulos and I. Kompatsiaris, "Detecting image splicing in the Wild 
(Web)," in Web Multimedia Verification, 1st International Workshop on,, Torino, 2015.  
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 108 OF 139 
[121]  A. E. Dirik and N. D. Memon, "Image tamper detection based on demosaicing artifacts," in 
International Conference on Image Processing, IEEE Proceedings of, 2009.  
[122]  D. L. Donoho and J. M. Johnstone, "Ideal spatial adaptation by wavelet shrinkage," Biometrika, 
vol. 81, no. 3, pp. 425-455, 1994.  
[123]  P. Boldi, A. Marino, M. Santini and S. Vigna, "BUbiNG: massive crawling for the masses," in 
23rd international conference on World wide web companion (WWW Companion '14), Republic 
and Canton of Geneva, Switzerland, 2014.  
[124]  P. Boldi, B. Codenotti, M. Santini and S. Vigna, "Ubicrawler: A scalable fully distributed web 
crawler," Software: Practice & Experience, 2004.  
[125]  M. Ester, H.-P. Kriegel, S. Jorg and X. Xiaowei, "A density-based algorithm fro discovering 
clusters in large spatial databases with noise," 1996.  
[126]  L. Zhang, R. Ghosh, D. Mohamed, H. Meichun and L. Bing, "Combining lexicon-based and 
learning-based methods for Twitter," Hewlett Packard Laboratories, 2011. 
[127]  L. Zhang, R. Ghosh, D. Mohamed, H. Meichun and L. Bing, "Combining Lexicon-based and 
Learning-based Methods for Twitter," Hewlett Packard Laboratories, 2011. 
[128]  P. Bo, L. Lillian and V. Shivakumar, "Thumbs up? Sentiment Classification using Machine 
Learning," in Empirical Methods in Natural Language Processing (EMNLP), 2002.  
[129]  T. Wilson, H. Paul, S. Swapna, K. Jason, W. Janyce, C. Yejin, C. Claire, R. Ellen and P. 
Siddharth, "OpinionFinder: A system for subjectivity analysis," in Empirical Methods in Natural 
Language Processing, 2005.  
[130]  Y. He, A. Harith and D. Zhou, "Exploring English Lexicon Knowledge for Chinese Sentiment 
Analysis," in CIPS-SIGHAN, 2010.  
[131]  P. P. Pak Alexander, "witter as a corpus for sentiment analysis and opinion mining," in Seventh 
conference on International Language Resources and Evaluation (LREC’10), Valleta, Malta, 
2010.  
[132]  B. Luciano and F. Junlan, "Robust sentiment detection on Twitter from biased and noisy data," 
in 23rd International Conference on Computational Linguistics, 2010.  
[133]  D. Dmitry, T. Oren and R. Ari, "Enhanced sentiment learning using Twitter hashtags and 
smileys," in 23rd International Conference on Computational Linguistics, 2010.  
[134]  K. Efthymios, W. Theresa and M. Johanna, "Twitter Sentiment Analysis: The Good the Bad 
and the OMG!," i-sieve, 2011. 
[135]  O. Owoputi, B. OConnor, C. Dyer, K. Gimpel, N. Schneider and N. Smith, "Improved part-of-
speech tagging for online conversational text with word clusters," in NAACL-HLT, Atlanta, 
Georgia, USA, 2013.  
[136]  G. Alec, B. Richa and H. Lei, "Twitter Sentiment Classification using Distant Supervision," 
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 109 OF 139 
Stanford, 2009. 
[137]  B. G. Assefa, "KUNLPLab: Sentiment Analysis on Twitter Data," in SemEval, 2014.  
[138]  H. Hussam, B. Patrice and B. Frederic, "Sentiment Lexicon-Based Features for 
SentimentAnalysis in Short Text," in SemEval, 2015.  
[139]  S. Papadopoulos and Y. Kompatsiaris, "Social multimedia crawling for mining and search," 
IEEE Computer, vol. 47, no. 5, pp. 84-87, 2014.  
[141]  F. Rangel, P. Rosso, I. Chugur, M. Potthast, M. Trenkmann, B. Stein, B. Verhoeven and W. 
Daelemans, "Overview of the 2nd Author Profiling Task at PAN 2014," in 
ConferenceProceedings of the Conference and Labs of the Evaluation Forum (Working Notes), 
2014.  
[142]  J. M. Digman, "Personality structure: Emergence of the five-factor model," Annual review of 
psychology, vol. 41, no. 1, pp. 417-440, 1990.  
[143]  S. Papadopoulos, D. Corney and L. M. Aiello, "SNOW 2014 Data Challenge: Assessing the 
Performance of News Topic Detection Methods in Social Media.," in SNOW-DC@WWW, 2014.  
[144]  E. Iosif and A. Potamianos, "Similarity computation using semantic networks created from web-
harvested data," 2014.  
[145]  G. Tang, Y. Xia, W. Wang, R. Lau and F. Zheng, "Clustering Tweets using Wikipedia 
Concepts," in ConferenceProceedings of the Ninth LREC’14, 2014.  
[146]  K. Kandasamy and P. Koroth, "An integrated approach to spam classification on Twitter using 
URL analysis, natural language processing and machine learning techniques," in Electrical, 
Electronics and Computer Science (SCEECS), 2014 IEEE Students’ Conference on, 2014.  
[147]  A. Gangemi, E. Hassan, V. Presutti and D. Reforgiato, "FRED as an Event Extraction Tool," 
Detection, Representation, and Exploitation of Events in the Semantic Web, 2013.  
[148]  T. Ploeger, M. Kruijt, L. Aroyo, F. D. Bakker, I. Hellsten and A. Fokkens, "Extractivism: 
Extracting activist events from news articles using existing NLP tools and services," in The 12th 
International Semantic Web Conference (ISWC), 2013.  
[149]  M. B. Habib and D. Keulen, "Unsupervised improvement of named entity extraction in short 
informal context using disambiguation clues," in SWAIE 2012 : Semantic web and information 
extraction, 2012.  
[150]  M. Cordeiro, "Twitter event detection: Combining wavelet analysis and topic inference 
summarization," in Doctoral Symposium on Informatics Engineering, DSIE, 2012.  
[151]  P. Project, "PortDial Project free data deliverable D3.1," in 
https://sites.google.com/site/portdial2/deliverables-publication, 2012.  
[152]  A. Ritter, O. Etzioni, S. Clark and others, "Open domain event extraction from twitter," in 
ConferenceProceedings of the 18th ACM SIGKDD international conference on Knowledge 
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 110 OF 139 
discovery and data mining, 2012.  
[153]  A. Ritter, O. Etzioni, S. Clark and others, "Open domain event extraction from twitter," in 
ConferenceProceedings of the 18th ACM SIGKDD international conference on Knowledge 
discovery and data mining, 2012.  
[154]  T. Arrskog, P. Exner, H. k. Jonsson, P. Norlander and P. Nugues, "Hyperlocal event extraction 
of future events," Boston, USA, 2012.  
[155]  B. Tsolmon, A.-R. Kwon and K.-S. Lee, "Extracting social events based on timeline and 
sentiment analysis in twitter corpus," Springer, 2012.  
[156]  P. Agarwal, R. Vaithiyanathan, S. Sharma and G. Shroff, "Catching the Long-Tail: Extracting 
Local News Events from Twitter.," in ICWSM, 2012.  
[157]  G. Van Oorschot, M. Van Erp and C. Dijkshoorn, "Automatic extraction of soccer game events 
from twitter," in Proc. of the Workshop on Detection, Representation, and Exploitation of 
Events in the Semantic Web, 2012.  
[158]  P. Exner and P. Nugues, "Using semantic role labeling to extract events from Wikipedia," in 
ConferenceProceedings of the Workshop on Detection, Representation, and Exploitation of 
Events in the Semantic Web (DeRiVE), 2011.  
[159]  K. D. Rosa, R. Shah, B. Lin, A. Gershman and R. Frederking, "Topical Clustering of Tweets," 
ConferenceProceedings of the ACM SIGIR: SWSM, 2011.  
[160]  J. Lin, R. Snow and W. Morgan, "Smoothing Techniques for Adaptive Online Language 
Models: Topic Tracking in Tweet Streams," in ConferenceProceedings of the 17th ACM 
SIGKDD International Conference on Knowledge Discovery and Data Mining, 2011.  
[161]  Y. Song, H. Wang, Z. Wang, H. Li and W. Chen, "Short Text Conceptualization Using a 
Probabilistic Knowledgebase," in ConferenceProceedings of the 22nd IJCAI - Volume Volume 
Three, 2011.  
[162]  X. Liu, S. Zhang, F. Wei and M. Zhou, "Recognizing Named Entities in Tweets," in 
ConferenceProceedings of the 49th ACL: Human Language Technologies - Volume 1, 2011.  
[163]  W. R. van Hage, V. MalaisÃ©, M. Van Erp and G. Schreiber, "Linked open piracy," in 
ConferenceProceedings of the sixth international conference on Knowledge capture, 2011.  
[164]  A. Fader, S. Soderland and O. Etzioni, "Identifying relations for open information extraction," in 
ConferenceProceedings of the Conference on Empirical Methods in Natural Language 
Processing, 2011.  
[165]  S. Choudhury and J. G. Breslin, "Extracting semantic entities and events from sports tweets," 
in ’Making Sense of Microposts’: Big Things Come in Small Packages, 2011.  
[166]  S. Choudhury and J. G. Breslin, "Extracting semantic entities and events from sports tweets," 
in ’Making Sense of Microposts’: Big Things Come in Small Packages, 2011.  
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 111 OF 139 
[167]  A.-M. Popescu, M. Pennacchiotti and D. Paranjpe, "Extracting events and event descriptions 
from twitter," in ConferenceProceedings of the 20th international conference companion on 
World wide web, 2011.  
[168]  F. Hogenboom, F. Frasincar, U. Kaymak and F. De Jong, "An overview of event extraction 
from text," in Workshop on Detection, Representation, and Exploitation of Events in the 
Semantic Web (DeRiVE 2011) at Tenth International Semantic Web Conference (ISWC 2011), 
2011.  
[169]  C. Fellbaum, "WordNet: An electronic lexical database. 1998," 2010.  
[170]  S. Petrović, M. Osborne and V. Lavrenko, "Streaming First Story Detection with Application to 
Twitter," in Human Language Technologies: The 2010 Annual Conference of the North 
American Chapter of the ACL, 2010.  
[171]  F. Wu and D. S. Weld, "Open information extraction using Wikipedia," in 
ConferenceProceedings of the 48th ACL, 2010.  
[172]  R. Hoffmann, C. Zhang and D. S. Weld, "Learning 5000 relational extractors," in 
ConferenceProceedings of the 48th ACL, 2010.  
[173]  N. Shuyo, Language Detection Library for Java, 2010.  
[174]  T. Sakaki, M. Okazaki and Y. Matsuo, "Earthquake Shakes Twitter Users: Real-time Event 
Detection by Social Sensors," in ConferenceProceedings of the 19th WWW, 2010.  
[175]  T. Khot, "Clustering Twitter Feeds using Word Co-occurrence," Computer Sciences 
Department, 2010.  
[176]  A. Karandikar, "Clustering short status messages: A topic model based approach," 2010. 
[177]  J. Zhu, Z. Nie, X. Liu, B. Zhang and J.-R. Wen, "StatSnowball: a statistical approach to 
extracting entity relationships," in ConferenceProceedings of the 18th WWW, 2009.  
[178]  M. Mintz, S. Bills, R. Snow and D. Jurafsky, "Distant supervision for relation extraction without 
labeled data," in ConferenceProceedings of the Joint Conference of the 47th ACL and the 4th 
International Joint Conference on Natural Language Processing of the AFNLP: Volume 2-
Volume 2, 2009.  
[179]  O. Etzioni, M. Banko, S. Soderland and D. S. Weld, "Open information extraction from the 
web," 2008.  
[180]  G. Petasis, V. Karkaletsis, G. Paliouras and C. Spyropoulos, "Learning context-free grammars 
to extract relations from text.," in ECAI, 2008.  
[181]  B. Rosenfeld and R. Feldman, "Using corpus statistics on entities to improve semi-supervised 
relation extraction from the web," in ACL, 2007.  
[182]  M. Banko, M. J. Cafarella, S. Soderland, M. Broadhead and O. Etzioni, "Open information 
extraction for the web," in IJCAI, 2007.  
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 112 OF 139 
[183]  R. C. Bunescu and R. Mooney, "Learning to extract relations from the web using minimal 
supervision," in ACL, 2007.  
[184]  D. Davidov, A. Rappoport and M. Koppel, "Fully unsupervised discovery of concept-specific 
relationships by web mining," 2007.  
[185]  S. Brody, "Clustering clauses for high-level relation detection: An information-theoretic 
approach," in ACL, 2007.  
[186]  F. Xu, H. Uszkoreit and H. Li, "A seed-driven bottom-up machine learning framework for 
extracting relations of various complexity," in ACL, 2007.  
[187]  Y. Shinyama and S. Sekine, "Preemptive information extraction using unrestricted relation 
discovery," in ConferenceProceedings of Human Language Technology Conference of the 
North American Chapter of ACL, 2006.  
[188]  S. Sekine, "On-demand information extraction," in ConferenceProceedings of the 
COLING/ACL, 2006.  
[189]  O. Etzioni, M. Cafarella, D. Downey, A.-M. Popescu, T. Shaked, S. Soderland, D. S. Weld and 
A. Yates, "Unsupervised named-entity extraction from the web: An experimental study," 2005.  
[190]  R. J. Mooney and R. C. Bunescu, "Subsequence kernels for relation extraction," in Advances in 
neural information processing systems, 2005.  
[191]  R. McDonald, F. Pereira, S. Kulick, S. Winters, Y. Jin and P. White, "Simple algorithms for 
complex relation extraction with applications to biomedical IE," in ConferenceProceedings of 
the 43rd ACL, 2005.  
[192]  X. Carreras and L. Màrquez, "Introduction to the CoNLL-2005 shared task: Semantic role 
labeling," in ConferenceProceedings of the 9th Conference on Computational Natural 
Language Learning, 2005.  
[193]  J. R. Finkel, T. Grenager and C. Manning, "Incorporating Non-local Information into Information 
Extraction Systems by Gibbs Sampling," in ConferenceProceedings of the 43rd ACL’05), 2005.  
[194]  S. Zhao and R. Grishman, "Extracting relations with integrated information using kernel 
methods," in ConferenceProceedings of the 43rd ACL, 2005.  
[195]  R. C. Bunescu and R. J. Mooney, "A shortest path dependency kernel for relation extraction," 
in ConferenceProceedings of the conference on Human Language Technology and Empirical 
Methods in Natural Language Processing, 2005.  
[196]  F. Ciravegna and A. Lavelli, "LearningPinocchio: Adaptive information extraction for real world 
applications," 2004.  
[197]  N. Kambhatla, "Combining lexical, syntactic, and semantic features with maximum entropy 
models for extracting relations," in ConferenceProceedings of the ACL 2004 on Interactive 
poster and demonstration sessions, 2004.  
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 113 OF 139 
[198]  D. Zelenko, C. Aone and A. Richardella, "Kernel methods for relation extraction," 2003.  
[199]  A. McCallum, "Mallet: A machine learning for language toolkit," 2002.  
[200]  J. Kleinberg, "Bursty and Hierarchical Structure in Streams," in ConferenceProceedings of the 
Eighth ACM SIGKDD, 2002.  
[201]  F. Ciravegna and others, "Adaptive information extraction from text by rule induction and 
generalisation," in IJCAI, 2001.  
[202]  D. Laney, "3D Data Management: Controlling Data Volume, Velocity, and Variety," 2001. 
[203]  E. Agichtein and L. Gravano, "Snowball: Extracting relations from large plain-text collections," 
in ConferenceProceedings of the fifth ACM conference on Digital libraries, 2000.  
[204]  S. Brin, "Extracting patterns and relations from the world wide web," 1999.  
[205]  Y. Yang, T. Pierce and J. Carbonell, "A Study of Retrospective and On-line Event Detection," in 
ConferenceProceedings of the 21st Annual International ACM SIGIR, 1998.  
[206]  D. Yarowsky, "Unsupervised word sense disambiguation rivaling supervised methods," in 
ConferenceProceedings of the 33rd ACL, 1995.  
[207]  M. A. Hearst, "Automatic acquisition of hyponyms from large text corpora," in 
ConferenceProceedings of the 14th conference on Computational linguistics-Volume 2, 1992.  
[208]  Z. S. Harris, "Distributional structure," in Word, Vol 10, 1954, 146-162, 1954.  
[209]  H. Becker, M. Naaman and L. Gravano, "Beyond Trending Topics: Real-World Event 
Identification on Twitter.," ICWSM, {2011}file.  
[210]  L. B. Hu Minqing, "Mining and summarizing customer reviews," in ACM SIGKDD international 
Conference on Knowledge discovery and data mining, 2004.  
[211]  J. Schler, "The Importance of Neutral Examples for Learning Sentiment," in Workshop on the 
Analysis of Informal and Formal Information Exchange during Negotiations (FINEXIN), 2005.  
 
 
  
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 114 OF 139 
I. APPENDIX A: Modules Innovations 
Module Name Web Crawler Delivery date 1/2/2015 
Module Overview Section 2, D3.1 
Crawls images with metadata from the Web given a set of keywords. 
Based on existing work? (e.g. from other project or open source code) 
This modules extends an existing open source web crawler implemented in Java: the BUbiNG web crawler 
[123] 
Based on implementation of specific algorithms? (which? why?) 
BUbiNG has the standard structure of a web crawler and its implementation is based on the authors’ 
experience with the UbiCrawler [124]. Its job distribution is based on modern high-speed protocols in order 
to achieve very high throughput. 
Innovation introduced 
On top of the base implementation, the module employs heuristics for performing focused image crawling 
based on a given set of keywords. It also integrates a mechanism for web image size prediction prior to 
fetching, which improves its efficiency in terms of harvest rate and network bandwidth consumption.  
Is this considered a core innovation for the project? Why? 
The module implements a focused web image crawler with optimal performance, which can be used as a 
stand-alone component, and hence can be considered as a core innovation for the project. 
What benchmarks will be used to evaluate the module performance? 
Gain in network bandwidth consumption when using the web image size prediction mechanism. 
F-measure for the different image size prediction methods to quantify and compare their performance. 
Ground truth can be easily collected by fetching large amounts of images found from web pages (e.g., by 
sampling the Common Crawl dataset). 
Partners Involved and related WP/Task(s) 
CERTH, WP3 / T3.1 
 
Module Name Multimedia indexing Delivery date 1/3/2015 
Module Overview Section D3.1 
Indexes images based on visual features for supporting near-duplicate search. 
Based on existing work? (e.g. from other project or open source code) 
In the context of the SocialSensor FP7 project, CERTH, developer and owner of the module, has already 
published a journal paper describing the feature extraction and aggregation approach [3], and made the 
module available as an open-source project. CERTH is continuously supporting and extending the 
functionalities of the multimedia indexing library. 
Based on implementation of specific algorithms? (which? why?) 
The module implements a highly optimized version of CSURF+VLAD features. The choice was based on their 
increased speed and efficiency and on the results of experiments, as presented in [36]. 
Innovation introduced 
The optimized version of CSURF+VLAD features is an important novelty. Additionally the module is 
deployable in the form of a REST server and as a Storm module. This makes it more suitable compared to 
existing solutions in the market for supporting large-scale and highly efficient visual indexing. 
Is this considered a core innovation for the project? Why? 
Although the module can be launched as a web service with a simple REST API and will work out-of-the-box 
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 115 OF 139 
on any system with minimal configuration, it cannot be considered as a core innovation for the project, 
since the bulk of the research work was conducted within SocialSensor. 
What benchmarks will be used to evaluate the module performance? 
Benchmarking the framework includes measuring the indexing speed as well as measuring the mean 
average precision of the retrieved results to a large number of visual similarity queries on a number of 
existing experimental image datasets (Holidays, Paris and Oxford buildings). 
Partners Involved and related WP/Task(s) 
CERTH, WP3 / T3.1 
 
Module Name Multimedia similarity retriever Delivery date 1/3/2015 
Module Overview Section D3.1 
Finds images that are visually similar to the input image. The module requires that a visual index, 
constructed with the use of the multimedia indexing module (see previous table), is available. 
Based on existing work? (e.g. from other project or open source code) 
Same as for the multimedia indexing module (see previous table), the module was developed within the 
SocialSensor project; however, the descriptor filtering step (cf. Innovation introduced below) was 
developed within REVEAL. 
Based on implementation of specific algorithms? (which? why?) 
The module implements a state-of-the-art similarity search algorithm from the literature based on the 
Euclidean distance of features vectors, but using Product Quantization and Inverted Very Fast Asymmetric 
Distance Computation (IVFADC) to considerably speed up the identification of similar images. 
Innovation introduced 
It is deployable in the form of a REST server and as a Storm module. This makes it more suitable compared 
to existing solutions in the market for supporting large-scale and highly efficient image near-duplicate 
search. In addition, the module integrates a descriptor filtering step prior to the construction of the VLAD 
vector that results in improved retrieval resilience in cases where overlay fonts and graphics have been 
pasted on the query image. 
Is this considered a core innovation for the project? Why? 
The module implements a highly optimized and resilient similarity retrieval method, which can be 
integrated as a reverse image search service in other systems or exposed as a stand-alone component. 
What benchmarks will be used to evaluate the module performance? 
Benchmarking the framework includes indexing existing experimental image datasets such as Holidays, 
Oxford and Paris buildings, along with a large number of distractor images, measuring the retrieval mean 
average precision and speed and comparing those results with other methods of the scientific literature. 
Partners Involved and related WP/Task(s) 
CERTH, WP3 / T3.1 
 
Module Name Linguistic Analysis Delivery date 30/05/2014 
Module Overview Section D3.1 
Identify Named Entities from text 
Based on existing work? (e.g. from other project or open source code) 
Yes, based on Stanford coreNLP and Stanford Named Entity Recognizer toolkits 
Based on implementation of specific algorithms? (which? why?) 
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 116 OF 139 
Based on Stanforf coreNLP, well suited for such problems 
Innovation introduced 
Pre-processing steps to deal with social media data 
Is this considered a core innovation for the project? Why? 
No, as it is based on existing work 
What benchmarks will be used to evaluate the module performance? 
SNOW dataset 
Partners Involved and related WP/Task(s) 
NCSR’D’, WP3/T3.1 
 
Module Name Semantic Segmentation Delivery date 30/05/2014 
Module Overview Section D3.1 
Cleans HTML texts from tags 
Based on existing work? (e.g. from other project or open source code) 
Based on boilerpipe 
Based on implementation of specific algorithms? (which? why?) 
Based on boilerpipe implementation, as it is well suited for such problems 
Innovation introduced 
No innovation 
Is this considered a core innovation for the project? Why? 
No 
What benchmarks will be used to evaluate the module performance? 
Manual evaluation on several given websites. 
Partners Involved and related WP/Task(s) 
NCSR’D’, WP3/T3.1 
 
Module Name Sentiment Analysis module Delivery date  
Module Overview Section Appendix B 
Identifies if a text has positive, negative or neutral sentiment. It is designed especially for sentiment analysis 
of short texts (tweets). Works for texts in the English language 
Based on existing work? (e.g. from other project or open source code) 
The module has been developed from a relevant module of the TruthNest product of ATC. 
Based on implementation of specific algorithms? (which? why?) 
The module uses a machine learning based approach which has proven to give the best results as compared 
to other implementations. The SemEval 2013 and 2014 top ranked results use an SVM classifier with several 
features. We have based our implementation on these implementations. 
Innovation introduced 
The main innovation is the use of a unique set of features (used in the SVM classifier) that can be easily 
computed and are adequate to achieve acceptable results compared to the state of the arts solutions. The 
special characteristics of text in social media are taken into consideration. It is deployable in the form of a 
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 117 OF 139 
REST server and can be deployed as a Storm module. This makes it more suitable for supporting large-scale 
and highly efficient sentiment analysis services.  
Is this considered a core innovation for the project? Why? 
The module implements a method to detect the sentiment of a text which can be integrated as an analysis 
service in other systems or exposed as a stand-alone component. 
What benchmarks will be used to evaluate the module performance? 
We have used SemEval 2014 conference datasets in order to be able to compare its performance directly 
with similar modules. 
Partners Involved and related WP/Task(s) 
ATC, WP3 / T3.1 
 
Module Name Multimedia clusterer Delivery date 1/3/2015 
Module Overview Section 2, D3.1 
Groups together images that depict the same scene out of a large input collection.  
Based on existing work? (e.g. from other project or open source code) 
The module is based on the apache commons open-source implementation of the DBSCANClusterer. 
Based on implementation of specific algorithms? (which? why?) 
The DBSCAN algorithm [125] as it a state-of-the-art clustering algorithm, well suited for such problems. 
Innovation introduced 
The module implements a fast and efficient incremental variation of the DBSCAN algorithm.  
Is this considered a core innovation for the project? Why? 
No. 
What benchmarks will be used to evaluate the module performance? 
Benchmarking the module includes comparing the clustering speed of the incremental variation to the 
standard DBSCAN implementation, as well as empirically assessing the formed clusters in terms of their 
visual consistency.  
Partners Involved and related WP/Task(s) 
CERTH, WP3 / T3.2 
 
Module Name Multimedia Summarizer Delivery date 30-6-2015 
Module Overview Section 4 
Given a set of social media messages about an event, multimedia summarizer aims to select a subset of 
images derived from them, that, at the same time, maximizes the relevance of the selected images and 
minimizes their redundancy. To this end, the module uses a topic modeling technique to capture the 
relevance of messages to event sub-topics. Then it applies a graph-based algorithm to produce a diverse 
ranking of the selected high-relevance images.  
Based on existing work? (e.g. from other project or open source code) 
Early work on the research problem of multimedia summarization took place in the context of SocialSensor. 
However, the current implementation of the module was completely carried out within REVEAL. 
Based on implementation of specific algorithms? (which? why?) 
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 118 OF 139 
The main algorithms used in this work are three: 1) Clique Percolation method that is used to detect cliques 
of near duplicate items (in terms of textual and visual content) 2) a graph-based clustering algorithm named 
SCAN (Structural Clustering Algorithm for Networks) that is used to find subtopics in the set of social media 
items and 3) a variant of PageRank named DivRank that ranks the images of the set.  
Clique Percolation method was chosen as it is a state of the art method in clique detection. The advantages 
of using SCAN are that it does not require the number of clusters to be fixed and also it is applied on a graph 
of social media items that is required from the other steps of the procedure. By reusing this graph the 
overall efficiency of the module increases. DivRank was chosen as it promotes diversity, compared to other 
algorithms as PageRank that tend to promote relevant but similar items. Also, it is applied on a graph that is 
derived from the initial graph of items, therefore the computational cost is minimum.   
Innovation introduced 
The module take into account multiple aspects of the summarization problem in a single framework. First, it 
captures different notions of similarity (textual, visual, temporal, social), while the use of sophisticated 
graph-based methods, Clique Percolation for near-duplicate removal, SCAN for topic detection, and DivRank 
for diversity-oriented ranking, enables the extraction of high-quality visual summaries of events. 
Is this considered a core innovation for the project? Why? 
Given the amount of social media items related to real-world events, the summarization problem is 
important to any application that presents content from social media. Browsing on collections around 
events and topics is made much easier as the end-user can find the most significant, representative and 
diverse multimedia items without the need for deep exploration of the collection. The innovation is core as 
the module can be used independent of the REVEAL platform.     
What benchmarks will be used to evaluate the module performance? 
The module has already been evaluated in a dataset that contains 364k tweets related to 500 real-world 
events. About 3.5% of these, i.e. 12.7k tweets, contain an embedded image.  
We evaluated the average performance of the module across the 50 largest events by calculating the 
following metrics: Precision@N, Success@N, Mean Reciprocal Rank, α-normalized Discounted Cumulative 
Gain, Average Visual Similarity. 
Partners Involved and related WP/Task(s) 
CERTH-ITI, WP3 (Τ3.2) 
 
Module Name Relation Discovery Delivery date 30/05/2014 
Module Overview Section 3 
Discovers relations between entities in text 
Based on existing work? (e.g. from other project or open source code) 
The first approach is based on ClausIE project. A second approach is being considered which will not be 
based on ClausIE 
Based on implementation of specific algorithms? (which? why?) 
Based on ClausIE implementation. 
Innovation introduced 
Pre and post-processing steps to deal with social media data, as well as small modification to the algorithm 
itself. 
Is this considered a core innovation for the project? Why? 
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 119 OF 139 
The first version no, but most probably the second one. 
What benchmarks will be used to evaluate the module performance? 
SNOW dataset 
Partners Involved and related WP/Task(s) 
NCSR’D’, WP3/T3.2 
 
Module Name Fake post detector Delivery date 30-6-2015 
Module Overview Section 5 
Identifies posts that are fake based on features and metadata extracted from the post and its contributor. 
Based on existing work? (e.g. from other project or open source code) 
The research work for the development of this module started within the SocialSensor project, and has 
been considerably extended within REVEAL. 
Based on implementation of specific algorithms? (which? why?) 
The module uses for classification the RandomForest model, implemented by WEKA. 
Innovation introduced 
This module proposes the introduction of a large variety of data features that exploit the post content as 
well as the information of the post contributor. It also makes use of posts that are formulated in various 
languages and they are derived from a diverse set of events and sources, boosting its generalization ability. 
At the same time, it uses an ensemble learning approach based on two different models that can effectively 
detect the fake content in new events. 
Is this considered a core innovation for the project? Why? 
Yes, since the module can be used as a stand-alone service independent of the REVEAL platform.   
What benchmarks will be used to evaluate the module performance? 
A dedicated dataset was created, called the Image Verification Corpus, to evaluate the accuracy of the fake 
detection capabilities offered by the module. 
Partners Involved and related WP/Task(s) 
CERTH, WP3 / T3.3 
 
Module Name Stylometry Delivery date 30/05/2016 
Module Overview Section 6 
Identify author profile 
Based on existing work? (e.g. from other project or open source code) 
No 
Based on implementation of specific algorithms? (which? why?) 
No 
Innovation introduced 
New algorithm for author profiling 
Is this considered a core innovation for the project? Why? 
Yes, as it introduces combination of different features for the task 
What benchmarks will be used to evaluate the module performance? 
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 120 OF 139 
PAN challenge dataset on twitter 
Partners Involved and related WP/Task(s) 
NCSR’D’, WP3/T3.3 
 
Module Name Multimedia manipulation detector Delivery date 1/6/2016 
Module Overview Section 7 
Discovers manipulations (by photo editing software) on a media item, in particular splicing and copy-move 
operations. 
Based on existing work? (e.g. from other project or open source code) 
Code has been developed from scratch within the REVEAL project. Two open source libraries (JPEGlib, 
ojAlgo) are currently incorporated to implement necessary functionalities. 
Based on implementation of specific algorithms? (which? why?) 
The module implements three selected algorithms from the current state-of-the-art [107] [102] [111]. The 
algorithms were chosen based on our evaluations of the state of the art concerning  performance in real-
world conditions [120]. Further development will lead to the incorporation of novel algorithms, more suited 
to the real-world task. 
Innovation introduced 
The module is currently under development and focuses on assessing a number of state-of-the-art 
approaches. The key targeted innovation is to make the module applicable to arbitrary online content in 
contrast to existing forensics approaches that are only applicable to content that is of a certain quality (i.e. 
not having undergone multiple resaves). 
Is this considered a core innovation for the project? Why? 
This module intends to extend the state-of-the-art with respect to forgery detection on images downloaded 
from the web, a task which has received relatively little attention from the research community thus far. 
What benchmarks will be used to evaluate the module performance? 
The algorithms so far implemented have been tested against existing forged image datasets, including a 
new dataset that we have made available to the research community [4]. Both well-accepted and novel 
metrics are used for evaluations, and any further algorithms proposed and implemented will be evaluated 
within the same context. 
Partners Involved and related WP/Task(s) 
CERTH, WP3 / T3.3 
 
  
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 121 OF 139 
II. APPENDIX B: Sentiment Analysis 
Sentiment analysis is the process of detecting the contextual polarity of text. In other words, it 
determines whether a piece of writing is positive, negative or neutral. A common use case for this 
technology is to discover how people feel about a particular topic. Their opinions and feelings are 
often sourced from microblogging websites, like Twitter. This is due to nature of microblogs on which 
people post real time messages about their opinions on a variety of topics, discuss current issues, 
complain, and express sentiment for products and services. There is no limit to the range of 
information conveyed by tweets therefore there is a rising interest to analyze this content in the 
information retrieval community. 
Several challenges emerge when applying sentiment analysis in the social media domain. Tweets 
and texts are short: a sentence or a headline rather than a document.  The language used is very 
informal, with creative spelling and punctuation, misspellings, slang, new words, URLs, and genre-
specific terminology and abbreviations, such as, RT for “re-tweet” and #hashtags, which are a type of 
tagging for Twitter messages. How to handle such challenges so as to automatically mine and 
understand the opinions and sentiments that people are communicating has only very recently been 
the subject of research. 
Sentiment analysis can be used to determine sentiment on a variety of levels. It can score the entire 
text as positive or negative, and it can also score the sentiment of individual words or phrases in the 
document. The sentiment analysis detector we developed is designed to score the entire text, not 
individual words. Given a message, we classify whether the message is of positive, negative, or 
neutral sentiment. For messages conveying both a positive and negative sentiment, the stronger 
sentiment is chosen. We believe that in the case of short text messages, especially Twitter, the 
sentiment of the overall tweet is usually sufficient to describe the sentiment of the entities contained. 
Furthermore, the use of deep linguistic analysis and coreference resolution tools are of limited 
effectiveness in the microblogging domain. 
Our intention is to use the module in a stream of tweets in order to be able to detect the users overall 
sentiment over the discussed topic. For example, we might want to analyze the replies to a certain 
tweet. In case a tweet does not express any sentiment, the module considers it as having neutral 
sentiment. The module is a batch component which can be called on demand on a per collection 
basis in order to apply the sentiment analysis detection on the items of the collection in question. 
The sentiment analysis detector is open source and can be compared directly to similar 
implementations of a related benchmarking initiative, namely SemEval, which is organized in the last 
years, and in particular the task Sentiment Analysis in Twitter - Message Polarity Classification, 
which describes the problem we encounter. We implemented a solution that tries to adapt ideas from 
the best ranking submitted papers, while being simple and effective. 
Our solution targets Twitter data and classifies the tweets into three categories: positive, negative 
and neutral. This case is more difficult than having to assign sentiment into two classes (positive/ 
negative). It is noteworthy that the top scored submissions in SemEval have an F-measure value of 
about 70% in this task. The importance of the neutral class is stated and the learning corpus 
contains text annotated as neutral, not only positive/negative. 
In most academic papers of sentiment analysis that use statistical approaches, researchers tend to 
ignore the neutral category under the assumption that neutral texts lie near the boundary of the 
binary classifier. Moreover it is assumed that there is less to learn from neutral texts comparing to 
the ones with clear positive or negative sentiment. The authors of [126] showed in their research that 
the introduction of the neutral category can even improve the overall accuracy. 
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 122 OF 139 
In general, if a learned model is applied on a different domain, the performance may drop 
significantly. The learning corpus used by our implementation has been provided by the SemEval 
workshop and it has been created by a range of topics, including a mixture of entities, products and 
events. It is designed in a way to reduce the dependency on a specific domain. While an out-of-
domain trained baseline algorithm may not be efficient for sentiment analysis of a specific category 
(e.g. legal documents), it may also not be best suited for sentiment analysis of entities, products and 
events in Twitter, as it is trained on a too general domain. 
The number of features used when using machine learning techniques affects the overall 
performance (time and resources needed) of the classification. Since this module will be probably 
used in combination with other metrics to provide an overall analysis of a bunch of tweets, it should 
be fast and memory efficient. In this respect, it is important to keep the number of features low, to 
make restricted use of dictionary lookup operations and to perform NLP tasks (such as part of 
speech tagging) efficiently. We selected a limited number of features during our feature analysis 
process and make use of an efficient POS tagger designed for Twitter. Therefore we offer an 
implementation capable to batch process tweets. 
First, we perform natural language processing of the tweets to extract their features. We investigate 
several tweet characteristics (emoticons, negation, and syntax) and make use of popular sentiment 
lexicons. We have used the filter method to select features from a set of candidate features and use 
the top ranked of them. Finally we use an SVM classifier to classify the vector in one of the three 
categories (positive/negative/neutral) by using a trained model. 
9.1 Related Work 
To determine whether a document or a sentence expresses a positive or negative sentiment, two 
main approaches are commonly used: the lexicon-based approach and the machine learning-based 
approach. The lexicon-based approach [126] determines the sentiment or polarity of opinion via 
some function of opinion words in the document or the sentence. The machine learning-based 
approach typically trains sentiment classifiers using features such as unigrams or bigrams [128]. 
Most techniques use some form of supervised learning by applying different learning techniques 
such as Naive Bayes, Maximum Entropy and Support Vector Machines. These methods need 
manual labeling of training examples for each application domain. 
There are also some approaches that utilize both the opinion words/lexicon and the learning 
approach. For example, Wiebe and Riloff [129] used a subjectivity lexicon to identify training data for 
supervised learning for subjectivity classification. A similar idea was also applied to sentiment 
classification of reviews in [130], which classifies reviews into two classes, positive and negative, but 
no neutral class, which makes the problem much easier. 
While most sentiment analysis methods were proposed for large opinionated documents (e.g. 
reviews, blogs), some recent work has addressed microblogs. Supervised learning is the dominant 
approach. The authors of [131] built a sentiment classifier to classify tweets into positive, negative 
and neutral classes. Barbosa and Feng [132] proposed a twostep classification method. It first 
classified tweets as subjective and objective, and then classifies the subjective tweets as positive or 
negative. In [133] many Twitter characteristics and language conventions (e.g. hashtags and smiley) 
were utilized as features. There are also several online Twitter sentiment analysis systems (e.g. 
Sentiment14036 and Twitter Sentiment37). These approaches mainly used supervised learning. 
                                                     
36 http://www.sentiment140.com/ 
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 123 OF 139 
9.2 Sentiment Corpus  
We used the SemEval 2014 Task 938 corpus to assess our results. While a few Twitter sentiment 
datasets have been created, they were either small or proprietary, such as the i-sieve corpus [134] or 
relied on noisy labels obtained from emoticons or hashtags. Most of them are restricted to a specific 
domain and do not contain text in the form of tweets. The corpus of SemEval covers a range of 
topics, including a mixture of entities (e.g., Gaddafi, Steve Jobs), products (e.g., Kindle, Android 
phone), and events (e.g., Japan earthquake, NHL playoffs). Keywords and Twitter hashtags were 
used to identify messages relevant to the selected topic. In the following table we see an example of 
the tweets. All Twitter datasets were sampled and annotated in the same way. 
Tweet Sentiment 
Gas by my house hit $3.39!!!! I'm going to Chapel Hill on Sat. :) Positive 
Why is it so hard to find the @TVGuideMagazine these days? Went to 3 
stores for the Castle cover issue. NONE. Will search again tomorrow... 
Negative 
@TrevorJavier the heat game may cost alot more...and plus I would 
rather see Austin Rivers play 
Neutral 
Halloween may be the second worst holiday, right after valentine's day Negative 
It's midnight on the east coast which means its @nickjonas birthday! 
HAPPY 20th BIRTHDAY NICK J!!!!!!!!!! &lt;333 
Positive 
    Table 31: Learning corpus annotated tweets 
The tweets provided in the corpus had to be downloaded. Since sharing data is a violation of 
Twitter’s terms of service, we had to use the Twitter API to download the data. The download 
process required significant amount of time due to the API limitations. Some of the tweets did not 
exist because the respective users deleted their account content. We have downloaded in this way 
9,684 tweets. All tweets are in the English language. 
9.3 Description of the approach 
Process Overview 
 
The unique characteristics of Twitter data pose new problems for current lexicon-based and learning-
based sentiment analysis approaches. Some features of this type of data make natural language 
processing challenging. For example, the messages are usually short and the language used can be 
very informal, with misspellings, creative spellings, slang, URLs and special abbreviations. 
Furthermore some fields in the tweet text can be removed since they do not add value in the 
sentiment analysis process. This is performed in the data pre-processing step. We use an SVM 
linear classifier to assign the tweets in the three categories. 
Data pre-processing 
 
A good tokenization seems very important for Twitter data. We used the popular ArkTwitterNLP [135] 
which is suitable for tweets. The Part of Speech (POS) tagset of the tagger contains tags that 
correspond to emoticons, urls, at-mentions and hashtags. 
 
                                                                                                                                                                   
37 http://www.csc.ncsu.edu/faculty/healey/tweet_viz/tweet_app/ 
38 http://alt.qcri.org/semeval2014/task9/ 
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 124 OF 139 
We tokenize each tweet using this tagger and keep a tweet representation that does not contain any 
unnecessary tokens. The remaining tokens are the unigrams of the corresponding tweet and will be 
the basis of the feature attributes. For each tweet, we have removed tokens of URLs, e-mail 
addresses, Twitter usernames, numeric tokens, and symbols. If a token had 3 or more repeated 
characters, they were replaced by one repetition of the character. We have also eliminated words of 
1 character length, temporal words (e.g. today, tomorrow), and personal pronouns. We should note 
that stopwords have not been removed, since their removal worsened the results. 
Feature Extraction 
 
We performed feature analysis using the filter method of the popular Weka39 machine learning 
software. We implemented several popular features and tried to combine them. Our purpose was to 
use the learning corpus to search for the best subset of attributes in the dataset. Weka provides an 
attribute selection tool. The process is separated into two parts: 
- Attribute Evaluator: Method by which attribute subsets are assessed.  
- Search Method: Method by which the space of possible subsets is searched. 
We have used ChiSquaredAttributeEval, which evaluates the worth of an attribute by computing the 
value of the chi-squared statistic with respect to the class. Pearson’s chi-square test has been 
popularly used for feature selection in machine learning [127]. The basic idea is that if a term is more 
likely to occur in positive or negative tweets, it is more likely to be a sentiment indicator. That is, we 
need to find out how dependent a term w is with respect to the positive tweets or negative tweets of 
the learning corpus. The larger the chi-square value, the more dependent w is with respect to the 
sentiment category. 
The Search Method is the structured way in which the search space of possible attribute subsets is 
navigated based on the subset evaluation. We have selected the Ranker method which simply ranks 
the attributes by their individual evaluations. 
We created a Weka input file (.arff extension) with all the features under evaluation 
programmatically. We selected the sparse input file format, where only the features that exist are 
declared, along with their position in the vector. We execute the evaluator and save the ranked 
attributes. After experimenting with Weka and trying several attribute evaluators, we decided on the 
features to use. Some of the features are: 
1. N-grams. 
 
Bag of words features (unigrams and bigrams) are the simplest to retrieve features from a tweet. We 
note the existence/ nonexistence of an n-gram as a feature. Through the feature selection process 
we have selected the unigrams and bigrams that had the highest ranking. About 880 n-grams were 
selected. No stemming occurred and no stopwords were removed, as their removal worsened the 
results. Ngrams with frequency less than 3 in the overall dataset were not taken into consideration in 
order to keep the number of features low (about 11.500) and as their ranking value was very limited. 
2. Sentiment lexicons. 
 
The lexicon-based approach depends on opinion (or sentiment) words, which are words that express 
positive or negative sentiments. Words that encode a desirable state (e.g., "great" and "good") have 
a positive polarity, while words that encode an undesirable state have a negative polarity (e.g., "bad" 
and "awful"). We have used: 
                                                     
39 http://www.cs.waikato.ac.nz/ml/weka/ 
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 125 OF 139 
 
- MPQA Subjectivity Lexicon. MPQA (Multi-Perspective Question Answering) Subjectivity 
Lexicon40 is a lexicon of over 8,000 subjectivity single-word clues, each clue is classified as 
positive or negative. 
 
- Bing Liu Lexicon. A list41 of positive and negative opinion words or sentiment words for English 
(around 6800 words). The list was compiled over many years starting from the paper. 
 
- NRC Hashtag Sentiment Lexicon. NRC Hashtag Sentiment Lexicon42 contains tweet terms 
with scores, positive score indicates association with positive sentiment, whereas negative score 
indicates association with negative sentiment. It has entries for 54,129 unigrams and 316,531 
bigrams; the scores are computed using PMI over a corpus of tweets. 
 
The tweet text tokens are searched in these lexicons and if they exist, a score is formulated that 
aggregates each tokens sentiment value in the lexicon. If the sum of the scores for all tokens in the 
tweet is positive, the feature has the value 1. From 3 lexicons, 3 features are extracted respectively. 
 
3. Swear words list 
 
We created a list of swearing words that have negative meaning in most contexts. The existence of a 
word from this list in the tweet’s text is used as a feature. 
 
4. Negations words. 
 
A negation word or phrase usually reserves the opinion expressed in a sentence. Negation words 
include "no", "not", etc. Whether a term contains a negation word is used as a feature. 
 
5. Emoticons. 
 
A regular expression has been used to detect them. If the tweet contains a positive/ negative 
emoticon, the respective feature has the value 1. 
 
All feature types are combined into a single feature vector. We have used the SVM classifier to 
handle this sparse vector, as we describe in the following section. 
Sentiment classifier 
 
The selection of the appropriate classifier in this domain is a problem that has already been 
discussed in the paper [136]. The authors reported that the SVM outperformed the Naïve Bayes and 
Maximum Entropy classifiers. SVM classifiers were used in the top ranked submissions of SemEval 
2013 that had similar features with the ones we use. 
 
We have used LibSVM open source software with a linear kernel. Each entry in the vector 
corresponds to the value of the respective feature. For example, in case of the unigrams features, 
each feature is a single word found in a tweet. If the feature is present, the value is 1, but if the value 
is absent, then the value is 0. We prefer to use feature presence, as opposed to a count, so that we 
do not have to scale the input data, which speeds up overall processing. 
 
We have used cross-validation to determine the value of C parameter of the SVM algorithm. C is a 
regularization parameter that controls the tradeoff between achieving a low training error (classify 
instances correctly) and a low testing error (e.g. ability to generalize the classifier to unseen data). A 
                                                     
40 http://mpqa.cs.pitt.edu/lexicons/subj_lexicon/ 
41 http://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html#lexicon 
42 http://www.saifmohammad.com/WebPages/lexicons.html 
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 126 OF 139 
large value of C does a better job of getting all the training instances classified correctly while a lower 
value will classify more instances but with incorrectly. A value of 0.2 has been set, which gives 
slightly better results in comparison to the value of 1, which is usually preferred in most SVM based 
solutions. The epsilon parameter (tolerance when optimizing the error criterion) has been set to 
0.001, which is the default value in LibSVM implementation. 
9.4 Experimental evaluation 
The evaluation of our sentiment analysis detection module has been done against the test data 
provided by SemEval 2015 organizers. The number of tweets in each category in the training and in 
the testing data corpus are shown in the following table. 
 Positive Negative Neutral Total 
Training data 3643 1461 4580 9684 
Testing data 1574 601 1638 3813 
Table 32: Dataset for training/ testing 
The metric for evaluating the module has been the average F-measure for each class (positive, 
negative, neutral). This overall f-measure in the three categories is 64.59%. 
 Positive Negative Neutral Overall 
Precision 71.37% 53.19% 68.1% 64.22% 
Recall 67.24% 58.75% 69.29% 65.09% 
F-Measure 69.25% 55.83% 68.69% 64.59% 
Table 33: Performance on test corpus 
An earlier version of the sentiment detector has been submitted to the SemEval 2014 conference 
and it scored the 27th position among 51 participants in the Twitter2014 dataset.  
 
Figure 43: Semeval 2014 ranking of our early model version (Twitter 2014 corpus) 
The top ranking participation had a score of 70%. Our submission achieved an F-measure score of 
63%. We believe that our updated model would rank even higher. 
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 127 OF 139 
We tried to reduce the feature vector size. Initially we excluded the unigrams and bigrams that 
occurred only once. We gradually excluded the unigrams and bigrams with frequency 1, 2, 3 and 4. 
We noticed that after removing the n-grams of frequency 3, the F-measure worsened, especially in 
the negative category. Therefore we decided to remove the n-grams of frequency less than 3. After 
the preprocessing step removals described earlier, the final attribute vector contains 11.576 features. 
There are multiple emoticons that can express positive emotion and negative emotion. We have 
mapped the following emoticons as positive/ negative. 
Positive emoticons Negative emoticons 
      >:]      :-)      :)        :o)      :]      :3      :c) 
      :>      =]      8)      =)      :}      :^)      >:D 
      :-D      :D      8-D      8D      x-D      xD 
      X-D      XD      =-3      =3      8-)      :-)) 
      <3      (-;            : )      ;)      <3      :P 
      ;-)      :*      C:      ♥ 
      >:[      :-(      :(      :-c      :c      :-<      :< 
      :-[      :[      :{      >.>      <.<      >.< 
      :-||      D:<      D:      D8      D;      D= 
      DX      v.v      D-\':      </3      (-_-\') 
      (-_-;)      (o_O)      :@      o-o      -_- 
Table 34: Emoticons Mapiing 
Results and Discussion 
 
In this section we will explore the usage of our features and comment on the sentiment detector 
results. While the overall performance of the detector is close to the average, there is significant 
room for improvement. Especially the negative tweets sentiment detection has an f-measure of only 
55.83%. We believe that this is explained due to the following reasons. 
Firstly, we should adapt a weighting schema for each class. Since the training data is unbalanced, 
the weighting schema adjusts the probability of each label. The SemEval participants in [137] and  
[138] use a L1/L2 Logistic Regression classifier respectively. The first claim that they got an 
accuracy of 6% increase from the baseline feature representation. Both have used the LIBLINEAR 
implementation (Rong-En Fan et al.). The hypermeter optimization and regularization, they applied 
10 fold cross validation on the training set. 
Secondly, we should use more features derived from the lexicons. The top score submission (NRC-
Canada) in their analysis tries to find the most useful group of features in the classification. Apart 
from n-grams, they use at least 4 features related to lexicons compared to the one we use. These 
lexicons features have a high impact on correct classification, as their analysis shows. 
Third, we should use more features in general. Our created model consists of 11576 features of 
which only 7 are different than unigrams and bigrams. Features such as POS of the n-grams, term 
position/frequency, Z score and others are not computed, while they have a proven impact in 
improving the sentiment accuracy. We plan to use some of these features and move to a logistic 
regression classifier.  
Future Work 
 
Machine learning techniques perform well for classifying sentiment in tweets. We believe that the 
accuracy could still be improved. Below is a list of ideas we think could help in this direction.  
Dependency parsing. Our algorithms classify the overall sentiment of a tweet. The polarity of a 
tweet may depend on the perspective you are interpreting the tweet from. For example, in the tweet 
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 128 OF 139 
“Arsenal won Liverpool :)”, the sentiment is positive for Arsenal and negative for Liverpool. The Ark 
Tweet NLP team offers an open source dependency parser (TweetboParser) for English tweets. 
Using this we may indicate which noun is mainly associated with the verb and the classification 
would take place accordingly. This may allow “Arsenal won Liverpool :)” to be classified differently 
from “Liverpool won Arsenal :)”. 
Domain-specific tweets. Our classifier has an accuracy of 64.59% for tweets from several domains. 
If limited to particular domains (such as news from the financial sector) we believe that our classifier 
would perform better. 
Finding opinionated tweets. There are several publications were the tweets are firstly classified as 
expressing an expression or not and then are classified as positive/ negative. Since our attention is 
most for positive/ negative comments, we could remove the neutral tweets and use a binary classifier 
for the opinionated tweets.   
Internationalization. Less than 50% of tweets are in English. It should be possible to extend our 
approach in order to classify sentiment in other languages. 
9.5 Conclusions 
The purpose of the sentiment analysis detector is to identify the sentiment of tweets which is 
particular useful in several cases as for example if you need to analyze a series of responses to a 
topic. By grouping positive/ negative responses to a topic we can further analyze the responses and 
bring into attention the aspects of critique, facilitating the work of the end users or journalists. 
We have experimented with a feature based model. We used some of the features proposed in past 
literature and some of our own. Support vector machines learning algorithm can achieve acceptable 
accuracy for classifying sentiment when using a binary feature vector. Although Twitter messages 
have unique characteristics compared to other corpora, machine learning algorithms are shown to 
classify tweet sentiment with similar performance. 
Our results showed that more features should be used to achieve a top ranked classification 
performance. While our approach is adequate to achieve a baseline performance, we should try to 
use a weighting schema for each class as is the case for a Logistic Regression classifier. We have 
also verified the importance of sentiment lexicon features along with n-gram features in the 
classification performance. The evaluation of our sentiment detector on SemEval 2014 test data set 
has an F measure of 64.59%. 
  
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 129 OF 139 
III. Appendix C: Architecture summary 
In Figure 44 we give a brief overview of the WP3 modules architecture. Most of the WP3 
components work in batch mode, either per item (similarity retriever, stylometry, manipulation 
detection, fake post detection, sentiment analysis) or per collection (multimedia clusterer and 
summarizer).  
 
Figure 44: WP3 batch and storm modules 
Subsequently, the produced results are stored either in Solr for the text analysis modules or in 
Mongo DB for the multimedia analysis modules. The visual index is a dedicated search structure 
which enables very fast and efficient image indexing and reverse image search. It is a Storm 
component, created at crawl time and used as input for the similarity retrieval, multimedia clusterer 
and summarizer modules. 
The framework’s RESTful API is Swagger43-compliant and the methods can be called and tested 
using the comprehensive and user friendly Swagger-UI44. A demo version is available online45. The 
                                                     
43 http://swagger.io/ 
44 https://github.com/swagger-api/swagger-ui 
45 http://160.40.51.20:8080/swagger 
Multimedia 
Clusterer Cluster request
Multimedia 
Summarizer
Similarity 
Retriever
Manipulation 
Detection
Stylometry 
(Author Profiler)
Verification
Fake Post 
Detection
on demand 
verification request 
(per item)
RabbitMQ
Summarization request
Aggregation
RabbitMQ
on demand requests 
(per collection)
Mongo DB
Visual 
Index
Solr
Sentiment 
Analysis
Visual Indexer
WP3 Storm Topology
RabbitMQ stream of 
multimedia items
RabbitMQ
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 130 OF 139 
Swagger-UI gives the possibility to expose all available API methods (Figure 45) along with 
descriptions of their functionalities and their input fields. Additionally, one can call the respective 
methods and the server response is then also displayed on Swagger-UI (Figure 46 and Figure 47). 
 
Figure 45: WP3 RESTful API methods' overview on Swagger-UI 
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 131 OF 139 
 
Figure 46: Image verification request and response 
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 132 OF 139 
 
Figure 47: Current crawls request and response 
  
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 133 OF 139 
IV. Appendix D: Deliverable file structure 
 
In this section, we provide a comprehensive overview of the deliverable file and folder structure, 
which contains all executables accompanied by the necessary resources and sample data. The 
deliverable package contains one folder for each delivered module. At Table 35, each header 
contains information about the name of the module, the name of module folder (in brackets) and the 
respective section of this document where the module’s functionality is described. The left column 
contains file and folder names and the column on the right the description. 
Table 35: Deliverable files/folder structure 
Media REVEALr files 
(media_revealr) 
Description 
Section 2 
reveal.war The war file to be deployed on the web server 
README.txt A ReadMe file with guidelines on how to build, deploy and 
use the Media REVEALr system 
Relation discovery files 
(relation discovery) 
Description 
Section 3, Deliverable D3.1 
RelationDiscovery-1.0-SNAPSHOT-jar-
with-dependencies.jar 
The actual executable JAR file 
default.json A sample json file containing tweets intended for testing 
the relation discovery functionality 
README A ReadMe file with guidelines on how to execute the JAR 
Sentiment Analysis files  
(sentiment analysis) 
Description 
Appendix A 
/resources/arktweetnlp/model.20120919 Ark Tagger Tweet NLP file 
/resources/lexicons/negative-words.txt Bing Liu Lexicon file 
/resources/lexicons/NRCEmotionLexicon.txt NRC Emoticon Lexicon file 
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 134 OF 139 
/resources/lexicons/positive-words.txt Bing Liu Lexicon file 
/resources/lexicons/swear_words.txt Swear words file 
/resources/lexicons/unigrams-pmilexicon.txt NRC Hashtag Sentiment Lexicon file 
/resources/lexicons/unigrams-pmilexicon-
sentim140.txt 
NRC Hashtag Sentiment Lexicon file 
/resources/semeval-2014-model The model file created by libsvm software. 
/resources/semeval-2014-model-features The features used by our model, one feature per line. 
Indexing, search and clustering files 
(indexing_search_clustering) 
Description 
Section 2, Deliverable D3.1 
mi.jar The actual executable JAR file with all necessary 
dependencies 
README.txt A simple ReadMe file with guidelines on how to call the 
executable in order to test the supplied functionalities 
test_images (folder) Some sample images from the Holidays dataset 
(http://lear.inrialpes.fr/~jegou/data.php) to help with testing 
learning_files (folder) This folder contains several codebook files and a PCA file 
necessary for the visual feature extraction. They are 
loaded in memory and used by the executable on runtime. 
Multimedia Summarizer 
(multimedia_summarizer) 
Description 
Section 4 
./data/tweets.json This file contains social media items related to Sandy 
Hurricane in json format (one item per line). 
./data/fake.txt          This file contains a set of ids that correspond to items with 
fake images related to Sandy hurricane. 
./data/real.txt          This file contains a set of ids that correspond to items with 
real images related to sandy hurricane. 
./data/graphs/                   This directory contains three graphs generated using the 
items of ./data/tweets.json. 
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 135 OF 139 
./data/graphs/itemsGraph.graphml       A file that keeps a graph of items connencted with edges 
that represent textual,visual,social,and temporal similarity. 
./data/graphs/textualGraph.graphml           A file that keeps a graph of items connencted with edges 
that represent textual similarity.     
./data/graphs/visualGraph.graphml      A file that keeps a graph of items connencted with edges 
that represent visual similarity. 
./data/text_index        A directory that contains the Lucene index of items in 
./data/tweets.json 
./data/visual_index A directory that contains the visual index of items in 
./data/tweets.json 
./data/learning_files A directory that contains the learning files needed for 
visual features extraction and indexing 
./data/output/clusters.tsv       Clusters that correspond to sub-topics of items in 
./data/tweets.json 
./data/output/summary.txt        A summary of 30 items generated using the items in 
./data/tweets.json 
Fake tweet detection 
 (verify_jar) 
Description 
Section 5 
verify.jar The file that contains the software for classifying the tweets 
resources The folder that contains the necessary files for the feature 
extraction from the tweets 
train.json A sample file that contains the training data used. Each 
line of the file has the format: 
<tweet_in_json_format> <tab> <label> 
The <tweet_in_json_format> represents a tweet as 
provided by the Twitter API and contains all the information 
of the tweet itself and the user who posts it. The <label> 
represents the fake or real label of the tweets.  
test.json A sample file that contains the testing data used. Each line 
of the file has the same format as the train.json file. 
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 136 OF 139 
Multimedia Manipulation Detector 
Toolbox  files 
(ImageForensicsPackage_Jun15) 
Description 
Section 7 
Auxiliary_Sources/jpeg-8d_distribution This folder contains the libjpeg library, v. 8d, needed for 
reading raw JPEG compression parameters. In our case, it 
is necessary for Windows systems, since in most Linux 
distributions a compiled version can be found in the 
respective repositories. Without it, DCT-based analysis is 
impossible.  
Compilation instructions can be found in “Dependency 
installation instructions.docx”. 
Auxiliary_Sources/ExportDCT This small library, written by us, provides a simple API for 
exporting DCT coefficients from JPEG images in a simple, 
straightforward data structure in the form of an Integer 
pointer array. It essentially functions as a bridge between 
libjpeg and our Java software, and It is necessary for both 
Windows and Linux systems. Compilation instructions (and 
directions to precompiled versions) can be found in 
“Dependency installation instructions.docx”. 
Executable This is the JAR application.  
These files are necessary for running DCT-based 
detection in Windows Netbeans. For jar execution, the two 
files should be alongside the .jar file, or within the Windows 
path (although the latter is not recommended, as other 
versions of libjpeg are also used by other applications). 
AuxiliaryLibs If Linux compilation of the C code is impossible, these 
libraries might function as a replacement, placed in 
LD_LIBRARY_PATH (although for Linux it is highly 
recommended to use a native libjpeg library, and to 
recompile ExportDCT). 
MultimediaManipulationToolbox.jar This is the actual JAR file 
1.Ferguson.jpg 
2.Columb.tif 
3.Flag.jpg 
These images are here for testing/demo purposes 
 
ExportDCT.dll 
libjpeg-8.dll 
libExportDCT.so 
libjpeg.so.8 
These are the libraries required for DQ analysis. See the 
accompanying documentation for their proper use. 
 
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 137 OF 139 
lib/ 
 
This folder contains the accompanying Java libraries 
necessary for the project. These are: 
commons-io-2.4.jar 
commons-lang3-3.4.jar 
commons-math3-3.5.jar 
jai_imageio.jar 
JavaDWT_Accelerated.jar 
jna-4.1.0.jar 
ojalgo-37.1.jar 
 
  
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 138 OF 139 
V. Appendix E 
 
Table 36: Number of successful detections per method and per class. In Class names, a number at the 
end indicates a different version of the same forgery, while an underscore followed by a letter indicates 
a different binary mask for the same version. “#” indicates the number of total number of collected 
images for that version. 
Class name # [107] [108] [97] [110]A [110]NA [102] [111]  # [107] [108] [97] [110]A [110]NA [102] [111] 
9-11Tourist_a 311 0 0 0 0 0 0 0 ObamaHandRonPaul 40 0 0 0 0 0 0 0 
9-11Tourist_b  0 0 0 0 0 0 87 ObamaPhone1 181 0 0 0 0 0 0 0 
AmericanFlag 7 0 0 0 0 0 0 4 ObamaPhone2 92 0 0 0 0 0 0 0 
AntofagastaWave 63 0 0 0 0 0 1 27 ObamaSmoking1 631 0 0 0 0 0 1 1 
BackToTheFuApr 86 0 0 0 0 0 0 0 ObamaSmoking2 234 0 0 0 0 0 0 0 
BackToTheFuJul1 41 6 0 0 0 0 0 10 ObamaSmoking3 74 0 0 0 0 0 0 0 
BackToTheFuJul2 23 0 0 0 0 0 0 0 ObamaSmokingHat 92 4 4 0 0 0 25 50 
BackToTheFuJul3 5 0 0 0 0 0 0 0 ObamaSmokingMult 33 0 0 0 0 0 1 0 
BackToTheFuJun 68 0 0 0 0 0 0 0 OpenSky 46 0 0 0 0 0 0 0 
BlackLion 713 6 3 0 0 0 477 703 OrwellCamera 42 0 0 0 0 0 0 0 
BlackLivesMatter 8 0 0 0 0 0 2 0 PalinHead 38 8 4 0 5 0 0 6 
BurstInKFC 18 0 0 0 0 0 0 0 Palin_Gun 582 0 0 0 0 0 0 0 
BushBook 44 0 0 0 0 0 0 0 PilotSelfie 67 0 0 0 0 0 0 2 
BushFishingNOrleans 67 0 0 0 0 7 0 0 PutinScreen1 9 0 0 0 0 0 0 1 
BushGuitar 6 0 0 0 0 0 0 0 PutinScreen2 70 0 0 0 0 0 0 0 
BushPhone 250 0 0 0 0 0 0 0 RMoney 1 0 0 0 0 0 0 0 
ChemtrailProtest 36 0 0 0 0 0 0 0 RaccoonCat 198 0 0 0 0 0 0 0 
Connecticut 15 0 0 0 0 0 0 0 RobsAStore 38 2 0 0 0 0 0 1 
CrabQuai 44 0 0 0 0 0 0 0 RushmoreHelicopters 20 0 0 0 0 0 0 0 
DoubleDeckerRace 87 0 0 0 0 0 0 27 RushmoreHelicopters2 12 0 0 0 0 0 0 0 
EngineFire 212 0 0 0 0 0 0 0 SatanicGecko1 103 0 0 0 0 0 0 0 
FiveHeadedSnake 157 0 0 0 0 0 0 0 SatanicGecko2 19 0 0 0 0 0 0 0 
FloatingRock 398 0 0 0 0 0 0 0 SharkBehindBoy 186 0 0 0 0 0 0 0 
FloodClass 35 0 0 0 0 0 7 33 SharkHelicopter 100 0 0 0 0 0 0 0 
FoxHounds 195 0 0 0 0 0 0 0 SharkMall 6 0 0 0 0 0 0 0 
FuIraq 59 0 0 0 0 0 0 0 SharkStreet 389 0 0 0 0 0 0 0 
FujiLenticularClouds 64 0 0 0 0 0 0 0 SharkStreetHigh 36 0 0 0 0 0 0 0 
GaysNotAllowed 45 0 0 0 0 0 0 0 SharkStreetPorch 210 0 0 0 0 0 0 0 
GiantSquid 422 0 0 0 0 0 0 0 SnakeGirl 53 1 3 0 0 0 0 26 
GolfCarrier 41 0 0 0 0 0 0 0 SochiDToiletFrame 63 0 0 0 0 0 0 1 
GolfCarrier_Tiger 36 0 0 0 0 0 0 0 SochiDToiletJClau 36 0 0 0 0 0 0 8 
IraqiRefugees 86 0 0 0 0 0 0 0 SochiDToiletRank 42 0 0 0 0 0 0 5 
IslandMoonStar 192 0 0 0 0 0 0 0 SochiDToiletRead 14 0 0 0 0 0 0 0 
KerryFonda1_a 138 0 0 0 0 0 0 0 SochiDToiletStark 11 0 0 0 0 0 0 11 
KerryFonda1_b  13 31 0 19 4 6 82 SochiDToiletTake 8 0 0 0 0 0 2 8 
KerryFonda1_c  18 32 0 19 1 16 84 SochiDToiletTooMany 13 8 7 0 3 0 1 10 
KerryFonda2_a 20 0 0 0 0 0 0 0 SochiDoubleVPutin 3 0 0 0 0 0 0 0 
KerryFonda2_b  0 5 0 0 2 0 2 StonehengeMeteorite 204 0 0 0 0 0 0 0 
KerryFonda2_c  0 0 0 0 0 1 9 TornadoLightning 104 0 0 0 0 0 0 0 
KerryFonda3 6 0 0 0 0 0 0 0 TripleTornado 85 0 0 0 0 0 0 0 
D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 
 
PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 
PROJECT COORDINATOR: INTRASOFT INTERNATIONAL S.A. WWW.REVEALPROJECT.EU 
  PAGE 139 OF 139 
KerryLaVey_a 60 2 5 0 3 0 56 46 U2Cassette 132 0 0 0 0 0 0 0 
KerryLaVey_b  0 0 0 0 0 0 0 UkraineShotDown 9 0 0 0 0 0 0 0 
KerryLaVey_c  0 0 0 1 0 0 40 WeBuiltDebt 112 0 2 1 1 3 90 67 
KittyRaft 48 3 2 0 0 0 0 0 WhaleCayak 25 0 0 0 0 0 0 12 
LennonChe 4 0 0 0 0 0 0 0 WitchSpider 158 0 0 0 0 0 0 0 
MalaysiaCrash 51 1 0 0 0 0 0 2 WorkMoose_a 261 0 0 0 1 0 131 235 
MarilynSpanishBook 16 1 1 0 1 0 1 2 WorkMoose_b  0 0 0 0 0 0 0 
MarsTwoSuns 2 0 0 0 0 0 0 2 WorkMoose_c  0 0 0 0 0 0 26 
MonstersInc 1 0 0 0 0 0 0 0 WrongHand1 14 0 0 0 0 0 0 0 
MoonBeach 62 0 0 0 0 0 0 0 WrongHand2 18 0 0 0 0 0 0 0 
MoonMountain 595 0 1 0 0 0 0 1          
  
