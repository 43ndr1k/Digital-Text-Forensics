 
 
 
 
 
 
 
A QUANTITATIVE FUTURE FOR THEOLOGICAL INQUIRY? 
PROBLEMS AND POSSIBILITIES 
 
(Formerly entitled: The Large Scale Features Of Intertextual 
Variation in the Greek New Testament and its Implication on Authorship Attribution) 
 
 
 
 
by 
 
 
 
 
 
 
 
James A. Libby 
 
 
 
 
 
 
 
Theological Research Seminar 
McMaster Divinity School 
January 15th 2007 
 1
 
 
Mathematics is the language in which God has written the universe.  
- Galileo Galilei 
 
Despite a surprisingly long history of use in Biblical studies, computational methods 
remain itinerants, wandering stars, so to speak, in the theological firmament. Precisely because 
these methods are mathematical they seem out of place in an ecliptic largely traced by historical 
and traditio-critical approaches. Given that Galileo and other figures central to the development 
of science in the West believed that a deeply fundamental relationship existed between 
mathematics and theology,1 and given that the rise of the critical program in theology was an 
explicit endorsement of the scientific program of the Enlightenment, how is it, then, that 21st-
century mainstream Biblical scholarship has largely bypassed mathematics in the theological 
enterprise?  This general question spawns a number of related questions.  First, why did 
quantitative methods fail to emerge alongside historical-critical methods as the Enlightenment 
unfolded? Second, if the omission of quantitative methods was some sort of oversight, are 
quantitative methods really ready to take a seat within the limited pantheon of commonly 
accepted methods in Biblical studies?2 To answer these questions our study will pursue three 
separate lines of inquiry. First, we will explore the history of the application of quantitative 
methods to theology.  In addition we will consider the Enlightenment and pre-Enlightenment 
milieu that predisposed quantitative methods to a secondary status in theology. Second, through 
a retrospective meta-analysis of quantitative research studies in theology, we will explore how 
explicit failures of experimental and research design have served to further attenuate the 
                                                 
1 Notable examples include Newton in mathematics and physics, Boyle in Chemistry, Bacon in scientific theory 
(which we call method today), and Maxwell and Faraday in electricity and magnetism. 
2 With the advent of the digital computer, there arose a barely contained form of technocratic elitism that modern 
quantitative methods would soon and definitively write its victories large across the theological heavens. (Baird, 
“Content”, passim.)  For a more recent and more muted form of this conception see Bartholomew, “Probability,” 
passim. 
 2
acceptance of quantitative methods within theological scholarship. From this analysis, informed 
by more rigorous experimental and research design principles, we will propose a set of general 
criteria appropriate to certain kinds of theological inquiry. Last, by implementing these criteria 
we will demonstrate a worked example of how quantitative methods, appropriately staged within 
an adequate research formalism, can contribute unique insights to current issues in Biblical 
scholarship. 
1. The Secondary Status of Theological Methods in Critical Theological Inquiry:  
An Historical Perspective 
In attempting to understand why quantitative methods became central in the sciences but 
secondary in theology requires reflecting not only on the historical development of science and 
theology in the West, but also delineating what is meant by “quantitative methods.” It should be 
noted from the outset that the terminology used in the scholarly literature to describe the use of 
quantitative methods applied to literature is both non-standardized and particularistic. 
Sometimes, for instance, the terminology is drawn from the originating discipline (e.g. corpus 
linguistics, statistical linguistics3, etc), sometimes from its methods (e.g. stylometry, 
stylochronometry), sometimes by its outcomes (e.g. authorship attribution) and sometimes it 
even carries a more generic label (e.g. Content Analysis,4 Computer-Aided Research.)  To avoid 
further nomenclature proliferation, “quantitative methods” as we use it in this study refers the 
application of any numerical approach applied to the Biblical corpus or related texts in order to 
address one or more of the following issues; source criticism, the synoptic problem/Q, literary or 
text criticism, linguistic structure, genre, style, tenor/mode elicitation, dating, and authorship 
attribution. 
                                                 
3 Statistical linguistics, Radday’s term, is perhaps the most accurate description of the kind of quantitative research 
performed  historically in Biblical studies (Radday, “Unity,” 30.)  
4 Baird, “Content,” 255. 
 3
Historical Review:5  It would be easy to assume that quantitative methods are a relatively 
new development in the broader history of methods applied to the theological task. Such is not 
the case. Simple frequency-based or “counting” analyses actually antedate the modern critical 
enterprise by more than a millennia, and can be found in the writings of Augustine.6  Further 
impetus for the development of computational methods however, awaited both the development 
of a richer quantitative toolkit and a real reason to use it. Both conditions were met in the early 
1800’s. This century witnessed the rise of both the historical-critical program7 as well as the 
development of what might be called the quantitative program, the first systematic quantitative 
methods applied to literature. Each program shared an identical lineage; each was fueled by an 
Enlightenment-era optimism in the utility of empirical methods, and each had been released from 
confessional constraints8 to apply their disciplines to the Biblical texts. By the mid-1800’s, 
however, the historical-critical program began to enjoy a wider acceptance than quantitative 
methods for reasons that we will detail later. Nevertheless, quantitative researchers in the 1800’s 
were hardly quiescent, pioneering the fields of authorship attribution, style and chronology.9  
Despite modest successes, however, these inaugural quantitative studies were hampered in three 
                                                 
5 This review was originally included in my paper, “Exploring the Secret Garden of Computational Linguistics:  A 
Brief Survey of the New Growth with a Particular Look at One Bloom” and presented at the Linguistics Institute of 
Ancient and Biblical Greek held at McMaster Divinity School in Hamilton, Ontario on August 17th, 2007.  
6 With respect to the Biblical corpus, simple counts of lexical or syntactic forms were employed by exegetes from 
the earliest days of the Christian era, just as they are today, to indicate prominence or emphasis. An example of the 
counting of passages and lexical forms to make a theological rather than an exegetical point can be found as early as 
Augustine. Kannengiesser, Handbook of Patristic Exegesis, 337. 
7 Which, for the purposes of this review, we date to the early nineteenth century and the inaugural source critical 
efforts of J.G. Eichorn and W. de Wette ).  
8 Johann Gabler’s Oration is commonly recognized as providing the formal impetus to source, text and literary 
critical efforts, but such efforts were already underway even before the Enlightenment through the labors of scholars 
like Richard Simon and John Mill. (Baird, “History,” 17–28.) 
9 Examples include the early developmental contributions such as Lewis Campbells’ work with hapaxes (Campbell, 
The Sophistes, 1867), Dittebberger’s development of the chronological ordering of texts (Dittenberger, 
“Sprachliche,”1881), Mendenhall’s studies on word-length (Mendenhall, “Characteristic,”1887), Walbe’s work with 
adjectives (Walbe, Syntaxis, 1888) and Wincenty Lutoslawski’s characterization of style in the works of Plato 
(Lutoslawski, “Principies,”1898.)  Although these were inaugural efforts, the sophistication of the analysis was 
limited to word counts, and its impact on the critical universe overall, was minimal 
 4
fundamental ways. First, the mathematical tools used were comparatively primitive. These early 
studies could do little more than describe interesting patterns in the data; they lacked the 
quantitative capability to make definitive statements either probabilistically or causally. Second, 
rudimentary data searching capabilities, of course, did not exist. Third, some quantitative 
problems were actually theoretically solvable using the mathematics of the day, but intractable in 
practice because arriving at a solution required years of hand-calculations to complete. 
Fortunately, the next century would provide answers to all three of these obstacles.  
The first of these three impediments was effectively removed when significant 
fundamental progress was made in a number of independent fields of mathematics in the early 
years of the 20th century. By the end of World War II, and in part because of it, the major 
outlines of statistics10, as well as exploratory and confirmatory mathematics were in place.11 
These newly minted mathematical disciplines made it possible for the first time to link certain 
kinds of observations to conclusions – and to do so quantitatively rather than qualitatively. The 
next two constraints were lifted simultaneously by the emergence of the digital computer in the 
1950’s.12 Subsequently, a small vanguard of researchers began to study ancient corpora with the 
vigor one would expect when a new and powerful tool arrives on the scene. A few of the more 
notable achievements related to the quantitative analysis of Biblical texts included the following: 
                                                 
10 Most notably, the mid-1920’s the modern era of inferential statistics was inaugurated with the work of R.A. 
Fisher. (Fischer, Statistical Methods,1971) 
11 Important in this period was the development of the mathematical theory surrounding sentence length and literary 
vocabulary by Yule (1938,1944).  
12 Even though the 1950’s era, digital computer, slow by our standards, nonetheless made whole classes of 
previously intractable problems solvable.  
 5
 
• John Ellison implemented the earliest computer-based method applied to NT criticism 
(1950–51) and applied statistical analysis to the texts (1967).13  
• Mosteller and Wallace published their classical monograph on the authorship of the 
Federalist Papers (1963).14  (We include it here because of its seminal effect on 
Biblical stylometry.) 
• A.Q. Morton performed stylistic studies of Paul (1961–64).15  
• Yehuda T. Radday performed stylistic studies on the Hebrew Bible and addressed the 
authorship of Genesis and Isaiah (1970-1985).16  
• F. I. Andersen analyzed Hebrew syntax (1970).17 
• A. Kenny inaugurated the first systematic multivariate study addressing issues of 
style and authorship in the NT (1986).18    
 
The next phase followed a predictable trajectory. After the initial ebullience surrounded 
the field, researchers and Biblical scholars took a hard, second look at computational approaches. 
Much of the early research was eventually challenged, and deservedly so. Contradictory 
conclusions had been made by scholars addressing the same issue, and sometimes while looking 
at the same data.19  Pretentious and premature pronouncements were made,20 and controlled 
experiments were, for the most part, absent. In almost all cases the rigor required to appropriately 
harness these new methods had been underestimated. By the mid-1990’s the field had reached a 
nadir, and even early supporters wondered in print about the viability of the discipline.21 Since 
the turn of the 21st century (a relatively long time in computational terms) however, a resurgence 
                                                 
13 Ellison, “Computers and the Testaments,” 160–169. 
14 Mosteller and Wallace, “Inference,” 275–309. 
15 Morton and McLeman, Christianity in the Computer Age.  
16 Radday, “Isaiah,” 65–73 and Radday, Genesis. 
17 Andersen, Verbless Clause in the Pentateuch. 
18 Kenny, Stylometry. 
19 “It seems that for every paper announcing an authorship attribution method that "works" or a variation of one of 
these methods, there is a counter paper pointing out crucial flaws.”  Rudman, Joe et al. “The State of Authorship 
Attribution Studies.” 
20 The early damage caused by overreaching claims launched from the computational camp has been noted (and 
parodied) by Alexander Gross in quite an entertaining way. Gross, Is Evidence Based Linguistics the Solution? 
21 It was during this time that Robert Kraft wrote his somewhat dour assessment of the discipline: “We have, in 
short, not come very far in actually realizing the promise offered by computer-assisted research… although much of 
the groundwork has been laid out.”   (Kraft, “Computers and Textual Criticism of the NT [Greek Scriptures].”) 
 6
in the application of quantitative methods to literature is clearly in evidence. Two reasons lie 
behind this resurgence. First, in the last 40 years whole new sub-disciplines of mathematics have 
evolved or matured, yielding an emergent set of advanced quantitative techniques capable of 
addressing questions relevant to literary disciplines. Second, advances in hardware have made 
these computationally intensive techniques practical, while advances in software have made 
previously inaccessible techniques far more broadly available. While the complete number of 
recent techniques applied to literary studies is quite large, a subset of these emergent or newly 
available techniques should provide ample evidence of the renewed vigor within the field.  These 
techniques include, but are not limited to: Cluster analysis,22  Markov Models,23 Maximum 
Likelihood Estimation,24 Correspondence Analysis,25 Discriminant Analysis,26 Machine 
Learning,27 Principal Component Analysis,28 Bayesian Multinomial Logistic Regression,29 
Bayesian Approaches for change points,30 Genetic Algorithms,31 and Neural Networks.32 Lastly, 
it should be mentioned that as part of this resurgence, new groups of researchers are performing 
quantitative literary and Biblical analysis, not, intriguingly, because they are interested in the 
texts per se, but because the texts provide an inexpensive and standardized data set upon which 
to test their approaches. A summary literature review demonstrates that this quantitative revival 
has emerged almost simultaneously from at least four points of the academic compass; 
linguistics, Biblical studies, computational linguistics, and cognitive sciences. In point of fact, it 
                                                 
22 Barr, “Two Styles,” 235–247,  Hoover, “Multivariate Analysis,” 341–360,  
Hoover and Thomas, “The Authorship of the Postscript,” 59–75, Yang “Information,” 473–483. 
23 Rand, “Markov Models,” 1299–1303. 
24 Cortina-Borja and Chappas, “A stylometric analysis,” 285–312. 
25 Mannion, “Sentence-length and Authorship Attribution,” 497–508. 
26 Schumm, “A Discriminant Analysis,” 274–276, Cortina-Borja, and Chappas. “A stylometric analysis.” 285–312, 
Collins, “Detecting Collaborations in Text,” 15–36, Somers, “Authorship,” 407–429.  
27 Stamatatos,“Authorship Attribution,” 823–838. 
28 Burrows, “Lucy,” 259–283, Holmes, “A widow,” 159–179, Somers, “Authorship,” 407–429.  
29 Madigan et al., “Bayesian Multinomial Logistic Regression,” 509–516. 
30 Girón, et al., “Bayesian Analysis,” 19–30. 
31 Mirolli, “How Can We Explain,” 312. 
32 Tambouratzis, “Applying the SOM Model,” 1–11. 
 7
now appears that we are witnessing the dawn of an era in which many of the researchers 
performing quantitative work upon the Biblical texts may not be trained in the Biblical 
disciplines or in the Biblical languages. While this may cause us pause from the perspective of 
research design, a further discussion of this issue must await another time.   
Given this brief history we can now proceed to explore suggested causes for why Biblical 
criticism has developed as it has, as well as investigate what the future portends for the increased 
exercise of quantitative methods within Biblical studies.  This will be our next task. 
 
 
 8
 
2. Why Did Quantitative Methods in Theology Not Develop Alongside Historical 
Critical Methods? Suggested Causes. 
Given the preceding background we are now more fully prepared to return to the question 
we asked at the outset of this inquiry. Why is it that quantitative methods, the lingua franca of 
the scientific enterprise, remain virtually absent from the 21st theological conversation – 
especially since the Enlightenment threw open the doors to the “scientific” exploration of the 
text? We suggest three answers, with the first two answers coming from our own time. First, as 
we mentioned in the historic review, quantitative methods have only recently begun to recover 
from a period of misapplication and inadequate experimental rigor which far too often 
characterized the field in the latter half of the 20th century. Undoubtedly, this singular fact has 
deterred some researchers from appropriating quantitative methods in their own work.  Second, 
we think it safe to say that because of their training many scholars in the theological disciplines 
have no particular appetite for mathematical methods. This observation immediately leads to the 
third, more fundamental reason, again drawn from history. While the deleterious effects of the 
fragmentation in theology which unwittingly arose from Johann Gabler’s inaugural lecture at the 
University of Altdorf are well understood,33 there was a prior fragmentation that just as directly 
affected how theology currently views quantitative methods. This prior fragmentation was more 
fundamental, we suggest, than Gabler’s in the sense that it was a fragmentation not within the 
theological disciplines themselves but between the larger academic disciplines. The historical 
moment in which this fragmentation event occurred can be traced to the rise of the medieval 
                                                 
33 It is ironic that Gabler intention to unify dogmatic fragmentation resulted in a whole new era of fragmentation. In 
Carson’s words, “Gabler argued that close inductive work on the biblical texts themselves would bring about much 
greater unanimity among scholars....The first part of the call was largely heeded…the second part…a fresh 
reconstruction of systematic theology, was by and large ignored…” (Carson, “Systematic,” 90.  
 9
universities in Europe in the 11th and 12th centuries. The medieval university system after it 
initially promoted scholasticism, eventually created two separate courses of study, the trivium 
and the quadriga. The trivium was to be studied first and the quadrivium was to be studied 
second, leading finally to theology as the ultimate level of study (the “queen of the sciences.”)  
We suggest that this decision over 800 years ago to separate the academic disciplines, has proved 
epochal. This marked the beginning of the academic and intellectual fragmentation in the West. 
It disaggregated the study of the physical sciences from both the arts and from theology itself. In 
time what began as a separation in sequence became a separation in content that endures today. 
Thus, by Gabler’s time, as in ours today, theologians no longer progressed from the humanities 
(the trivium) through the sciences (quadrivium) into theology. This has resulted in theologians 
whose education has excluded formal training in the sciences.  
The second historical fragmentation, Gabler’s, we have already mentioned. Yet, in a 
fundamental sense, Gabler’s fragmentation is a step-child of that first fragmentation. Gabler 
sought to harness the analytical strengths of the emerging sciences in the service of dogmatics 
and systematics. This was understandable. By Gabler’s time the empirical successes of the 
Enlightenment were everywhere evident – and the optimism that attended the Baconian method 
was nowhere more evident that at the universities. The question for Gabler, then, who like his 
contemporaries had doubtless been endued with the Enlightenment ethos, was not whether to use 
science to heal the dogmatic rifts – but which science ought he to use?  His answer was, in a 
word, exegesis: “the legitimate interpretation of the passages pertinent to consider.”34 For 
Gabler, the sensitive scientific treatment of the text required a sequence; first linguistic and 
exegetical, and second historical and theological. But we ask – why was mathematics not also 
considered?  The reasons behind any other persons’ inner intentions, of course, are never  
                                                 
34 Gabler, Oration, 498.  
 10
transparent, but it was certainly not because mathematics was peripheral to science in Gabler’s 
time. By this time it was central. Neither was it because the Enlightenment lacked champions for 
mathematics as the constitutional core of science. Indeed, it was precisely this era that 
popularized the term “pure science” in reference to mathematics. We would suggest, rather, that 
the medieval fragmentation just discussed delimited Gabler’s thinking at this juncture. 
Mathematics for Gabler, as with the rest of his peers, was only useful as a descriptor of physical 
phenomena, but inappropriate for revealed religion. This separation, we suggest, was intuitively 
adopted by others in the Enlightenment because of the decision made by scholastic educators 
five hundred years before. 
Returning to today, the scientific (i.e. historical–critical) program, given impetus by  
Gabler, has now been in operation for over two hundred years. How, we might ask, has it 
performed? While some scholars, like Markus Barth, believe that given time, the critical guild  
will be capable of demonstrating the “absolute bankruptcy of any quest for verifiable historical 
bedrock [in regard to] the canonical texts”,35 other scholars do not concur. While much 
productive and beneficial work has been produced by the critical program, increasingly the 
program is being questioned regarding its prior theoretical commitments as well as the validity of 
a number of its results. Some, for instance, have noted that current critical endeavors display an 
unevenness in method, and seem to have a propensity to yield atomistic outcomes (Carson).36 
Others complain that the program “delights in sophistication so that the ‘real meaning’ of the text 
runs counter to what the text says in its plain reading.” (Seitz)37  Even more fundamentally, one 
might ask, whether, as measured by external interdisciplinary standards, some of the methods of 
                                                 
35 Dickensen, “Markus Barth,” 104. 
36 Carson, “Systematic Theology,” 91. 
37 McConville,“Biblical Theology,” 136. 
 11
historical and traditio-critical praxis, can properly be termed scientific.38  Given these 
reservations concerning the current state of the Biblical criticism, we would propose that a 
vigorous quantitative program, be considered, one capable of operating in parallel with the more 
traditional historical methods. This is an option that Gabler, because of his training, did not 
consider. 
  
                                                 
38 Specifically, the historical critical method as practiced in the theological disciplines is absent an observational 
method, does not evidence repeatable observations, and has no quantitative apparatus to independently verify the its 
propositions, and results in displays poor internal consistency.  
 12
 
3a.  Once More unto the Breach:  
A Reconstituted Approach to the Application of Quantitative Methods in Biblical Studies 
 
In the latter half of the 20th century, as we have previously mentioned, quantitative 
methods were applied to literary studies without proper experimental and research design 
controls. This produced in Rudman’s words, “studies governed by expediency…[characterized 
by] flawed statistical techniques; corrupted primary data; lack of expertise in allied 
fields;…[and] inadequate treatment of errors.”39 Rudman, accordingly, has suggested a number 
of point solutions to these endemic problems  including, centrally, the need to “construct a 
correct and complete experimental design…”40 
We have adapted and expanded Rudman’s insights and integrated them with systems 
theory,41 experimental42 and quasi-experimental design43 and failure-mode analyses44 to develop 
a list of design criteria appropriate for stylistic studies of ancient epigraphic corpora such as that 
                                                 
39 Rudman, 351. 
40 Rudman, introduction. 
41 Bagh has written a very helpful article which describes that scientists have approached doing science (which is the 
process of understanding complex systems) from different perspectives. Hence there are multiple systems theories 
based on physical, mathematical, information theoretical, social science, philosophical models. The author presents 
a unified approach of sorts. Regardless of the “flavor” of systems science used however, the central contribution is 
that explicit “guardrails” so to speak, must be put in place to insure that the doing of science results in objective 
findings, not just the regurgitation of presuppositions which have acquired a new nomenclature. (Bagh, “Major 
Systems Theories,” 79-108.)      
42 Excellent general introductions to experimental design may be inspected in monograph 23 and 74 in the 
Quantitative Applications for in the Social Series published by Sage Publications. See Brown, et al.. Experimental 
Design and Analysis. See also Spector, Paul E., Research Designs. More detailed treatments can be found in such 
resources as Atkinson, A.C. and A.N. Donev., Optimal Experimental Designs. Another more advanced treatment 
arguing for simultaneity in perturbations is Ledolter, Johannes and Arthur J Swersey, Testing 1 - 2 – 3 Experimental 
Design. 
43 A quasi–experimental design is one in which random assignment between groups is not possible, but which 
admits the use of control groups or multiple waves of measurement. These designs are necessary when the data has 
already been collected or is static such as when the synchronic or diachronic analysis of a static corpus is 
contemplated.  
44 Failure mode analysis can be coupled with an understanding of how severe a given failure will be to the system 
currently under study (Failure Mode Effects Analysis or FMEA). FMEA is an engineering and quality control 
process. It assesses the ways a system or process may fail and what the effects of that failure will likely be.  
 13
found in the Biblical texts. Two orienting comments concerning these criteria may be helpful. 
First, these ten criteria are general criteria appropriate for authorship and stylistic studies. They 
are not appropriate for all quantitative applications, nor do they replace the need for developing a 
customized experimental formalism. Second, these criteria are designed to meet the central 
objectives of quantitative studies, namely, that they produce results that are valid (adequately 
describe the data while minimizing error), significant (demonstrate statistical differentiation), 
comprehensive (explore all contributing effects), causal (causally rather than associatively relate 
effects to conclusions), and unary (admit of one conclusion). The design criteria are as follows:  
• Criteria 1: Stylistic markers must be demonstrated to be stylistic rather than assumed a 
priori. 
• Criteria 2: All available potential stylistic markers must be tested, not a subset.  
• Criteria 3: Stylistic markers should be drawn from all layers of linguistic structure (i.e. 
morphology, syntax, micro and macro, clause and discourse level relations.)  
• Criteria 4:  Statistical and Experimental Design (ED) principles must be adhered to 
strictly. (i.e. Statistical tests of significance should meet all pre-requisites and use 
conservative statistical criteria. Experimental and quasi-experimental designs should be 
used systematically and followed explicitly.) 
• Criteria 5:  The analysis cycle should include simultaneous multivariate analysis, and if 
appropriate, time-series based decomposition. 
• Criteria 6: Multiple independent mathematical methods should be used to maximize 
analytical coherence. (i.e. Wherever possible, multiple independent approaches to a given 
problem should be employed to provide a consistency check and to reduce the probability 
of artifacts or counterfactuals related to a single methodology.)   
• Criteria 7: Statistical inferential hypothesis testing should be used exhuatively in the 
initial phases of data description. (i.e. Inferential statistics is a field that performs tests of 
hypotheses to determine if two or more sets of data are significantly different.) 
• Criteria 8:  Tests of alternative causes for observed effects should be performed 
exhaustively (e.g. analysis of covariance.)   
• Criteria 9: When predictive analytics are used, they must be employed with a full suite of 
validation approaches. This protects against design errors such as over-fitting. 
• Criteria 10:  Formal approaches to causal inference should be employed to differentiate 
between association and causation. Both positive and negative causal tests should be 
employed. 
 14
 
3b. Turning Theory into Practice: 
A Worked Example of a Criteria-Driven Experiment Design with Results Applied to 
Authorship Issues in the Greek New Testament  
We have just argued that quantitative methods, properly executed within the constraints 
of a careful experimental design, can contribute substantially to Biblical studies. A worked 
example should aid in transitioning this discussion from the merely theoretical to the practical. 
We will focus our efforts on a relative hot-bed of activity within biblical studies, the question of 
authorship and style within the Greek New Testament. A review of authorship studies reveals 
that one of the central fulcrums of the stylistic debate has turned historically on how stylistic 
variation is to be interpreted. Specifically, is it true that large stylistic variations, ipso facto, are 
to be assigned to authorial causes? This assumption, often unstated, has guided the interpretation 
of stylistic findings since the inauguration of the inquiry. But, again we ask, is it true? Or might, 
alternatively, the largest amount of stylistic variation in the Koine Greek NT corpus admit of 
other causes — such as register or genre?   If it can be demonstrated that the largest amount of 
stylistic variation in a given document or corpus is due to factors other than authorship, then the 
stylistic conclusions of prior studies will necessarily need to be reconsidered. To explore this 
hypothesis, we will employ the design criteria just developed and analyze the data using two 
modern quantitative techniques, correspondence analysis, and statistical hypothesis testing.  
An Explanation of the Experimental Design Approach Used 
Per the requirements of the experimental design criteria just described,45 we have not 
chosen any stylistic markers a priori. Instead, we have used correspondence analysis (see section 
2.2) to analyze the potential stylistic content of all 1113 fully inflected 
                                                 
45 See criteria one and two. 
 15
grammatical/morphological forms46 that can potentially occur in the Greek New Testament.47  
Moreover, since every word in the Greek NT occurs in one and only one of these grammatical 
forms, this analysis is comprehensive, leaving no words out of the analysis (per criteria two.)  In 
addition, we have added another layer of data, syntax, to our analysis. While this layer is not 
exhaustive (since no comprehensive syntactical database yet exists) it does nonetheless contain 
56 syntactical forms48 within the Greek NT. Hence, our data analysis draws stylistic markers 
from multiple layers of linguistic structure (per criteria three.) For the remainder of this 
discussion we will refer to these combined morphological and syntactical forms as “grammatical 
forms.” 
An Introduction to the Quantitative Methodology of Correspondence Analysis and an 
Interpretation of Initial Findings 
Correspondence analysis49 (CA) is a powerful multivariate analysis tool that among its 
other capabilities,50 can visualize the relationship between a set of objects (such as the 27 books 
of the Greek New Testament) and the “characteristics” of the object (such as the frequency of 
grammatical or syntactical forms.) CA starts with a “crosstabulation” table which is simply a 
frequency table of how many times, in our case, grammatical forms occur within each book. In 
the abbreviated example table in Figure 1, the columns are the texts (Greek NT books) and the 
rows the grammatical or syntactical forms. A single cell within the table, therefore, indicates 
how many times that particular grammatical form occurs in that particular book.  
                                                 
46 A “fully inflected grammatical form” is simply one of the declensions or parsing that a given word displays in the 
Greek NT. For instance, a Koine Greek noun has 5 cases, 3 genders, and two numbers, yielding 30 (5x3x2) potential 
morphological forms for that noun. Summing across all parts of speech, a word in the Greek NT may therefore 
appear in one and only one of 1113 distinct grammatical forms. 
47 Our database for this exercise is derived from the currently version of the GRAMCORD Greek NT database, 
which itself is a tagged version of the 27th edition of the Nestle-Aland (NA 27 critical eclectic text. 
48 The few syntactical forms we have omitted from the analysis have been removed to prevent overlap with existing 
syntactical forms chosen. 
49 Benzécri, “Statistical analysis.” 
50 Greenacre, Theory and Applications. 
 16
 
 
Figure 1. 
 
The central benefit of using correspondence analysis is that it mathematically “processes” 
frequency data (in our case occurrences of grammatical forms within books) into easily 
interpretable distances in a visual space. In this way New Testament books that are similar to one 
another grammatically and syntactically appear closer to one another in a graphic projection of 
the data, and texts that are dissimilar are farther away from one another. The resulting 
correspondence analysis of the 1166 forms and the 27 books of the New Testament can be 
inspected in Figure 2.51 
                                                 
51 In all reduced space projections included in this paper, the CA algorithm was procured from the Data Theory 
Scaling System Group at the University of Leiden, and incorporated into the multivariate visualization and 
simulation software, PositionSolve, developed by Decision Support Sciences (www.decsionsupportsciences.com) 
where the author is the managing director. The distance measure was Chi-square, the data standardization was via 
RCMEAN, and the data normalization employed was symmetrical. In some of these projections, the vectors which 
represent the grammatical forms are shortened to better reveal the positioning of the NT books. 
 17
Figure 2. 
Interpretively, the 27 NT books are located as spheres in this space. The grammatical 
forms are located at the “end” of the blue lines. (Note that for visual clarity the labels of these 
forms have been suppressed.)  Out of the 1166 potential grammatical/syntactical forms that could 
have occurred in the Paulines, 543 of these forms occurred at least once. For the purposes of 
statistical validity we have represented here only those 238 forms that both occurred at least 30 
times in the Greek NT and demonstrate the ability to differentiate between books52. These 238 
forms, then, may be considered stylistic in the sense that they are the forms in the Greek NT that 
differentiate maximally between the texts of the Greek NT. In interpreting this space, recall that 
                                                 
52 This ability to maximally differentiation between books was determined using a statistic, the average absolute 
value of the adjusted standardized residual (AAVASR.) 
Experiment: G-27-1166-0-CA-0 
All_Forms_by_All_NT_Books_1166_Sym_White_NoAtts.jp
g 
Historical 
Narrative 
Books 
Epistles 
 18
books similar to one another on these grammatical forms appear closer together. Hence, I John is 
grammatically distinct from the rest of the NT, while the synoptic gospels, when seen in the light 
of the entire variation of the Greek NT, are actually very close grammatically to one another.  
With this as background, let’s recall our objective. What does this calculated space tell us 
about the legacy assumption that the largest structures of style are necessarily authorial?  If this 
were the case, one would expect that the reduced space would evidence books clustered by 
author. Instead, the most macroscopic feature of this space is the clear separation between 
genres, specifically, the historical narrative books (below the center) and the “smear” of epistles 
(above the center.) Moreover, Revelation, the only example of apocalyptic in the NT, is very 
remote from all the rest of the books, as is I John.53 It is significant, then, that at the most 
macroscopic “tree-top” level (i.e. three dimensions) the books cluster by genre rather than by 
date or authorship. 
To further explore whether the large scale stylistic variation in the Greek New Testament 
represents genre or authorship requires identifying which books or book groupings are 
statistically rather than merely visually different from one another. A test appropriate for this 
task is the pairwise t-test.54  This test can be used to discover if the distance between any single 
test book or books (e.g. Colossians) exceeds the average “spread” within a reference set of books 
(such as the Pastorals). In table 3 we have compared the statistical results of the t-test to the two 
leading critical theories of Pauline authorship, namely, the four epistle theory of the older 
Tübingen school (the Hauptbriefe ) and the current mainstream critical view that accepts seven 
epistles as Pauline.55  The results of the t-test and how the three leading theories of authorship 
                                                 
53 It appears to us that I John with its almost Manichean imagery, its simple syntax, direct address, and short 
sentences is very distinct from other NT books, as indeed this grammatical landscape demonstrates.  
54 The t-test was run using the full 26 dimensions of the space, not just the three dimensions in figure two. 
55 Romans, I Corinthians, II Corinthians, Galatians, Philippians, I Thessalonians, and Philemon. 
 19
and genre performed in explaining statistically derived stylistic variation can be inspected in 
Figure 3 below.  
 
Statistical Differences Between Test and Reference Corpora in the Greek NT Compared to 
Two Theories of Authorship and a Theory of Genre: 
Correspondence Analysis: The 27 Greek NT Books by 238 Grammatical Forms 
 
 
Expected Significant Difference if 
Greatest Variation Is Due to 
Authorship 
Texts in the Test 
Set 
Reference 
Set 
Signifi-
cance: 
(< 0.05 is 
significant) 
Signifi-
cantly 
different? 
Minority View: Paul 
wrote the  
(Hauptbriefe only) 
Mainstream  
View: Paul 
wrote seven 
books) 
Expected 
Significant 
Difference if 
Variation is 
due to Genre 
(See Table 4.)  
I and II Thess. Hauptbriefe 0.06763544 N Y n/a N 
I Thessalonians Hauptbriefe 0.09045563 N Y N N 
II Thessalonians Hauptbriefe 0.31370448 N Y Y N 
Col and Ephesians. Hauptbriefe 0.00044748 Y Y Y Y 
Colossians Hauptbriefe 0.02562886 Y Y Y Y 
Ephesians Hauptbriefe 0.01159022 Y Y Y Y 
Three Pastorals Hauptbriefe 0.00000042 Y Y Y Y 
I Timothy, Titus Hauptbriefe 0.00000008 Y Y Y Y 
II Timothy Hauptbriefe 0.00736964 Y Y Y Y 
Pastorals, Philemon Hauptbriefe 0.00001198 Y Y n/a Y 
Philippians Hauptbriefe 0.22404702 N Y N N 
Petrines Hauptbriefe 0.00048126 Y Y Y Y 
I Peter Hauptbriefe 0.00707124 Y Y Y Y 
II Peter  Hauptbriefe 0.00218515 Y Y Y Y 
Hebrews Hauptbriefe 0.01440956 Y Y Y Y 
Col and Ephesians Critical-7 0.00002231 Y n/a Y Y 
Colossians Critical-7 0.00819867 Y n/a Y Y 
Ephesians Critical-7 0.00091266 Y n/a Y Y 
Three Pastorals Critical-7 0.00000000 Y n/a Y n/a 
I Timothy, Titus Critical-7 0.00000000 Y n/a Y n/a 
II Timothy Critical-7 0.00024486 Y n/a Y n/a 
II Thess Critical-7 0.62718606 N n/a Y n/a 
Petrines Critical-7 0.00000011 Y n/a Y Y 
I Peter Critical-7 0.00045612 Y n/a Y Y 
II Peter  Critical-7 0.00004439 Y n/a Y Y 
Hebrews Critical-7 0.00035233 Y Y Y Y 
Matt Mark Chi-square N Y Y N 
Matt Luke Chi-square N Y Y N 
Mark Luke Chi-square N Y Y N 
Luke Acts Chi-square Y N N Y 
Ephesians I Peter Chi-square N Y Y N 
Phil; I,II Thess. R,C, I Cor 0.12333603 N Y N N 
I Thessalonians R,C, I Cor 0.07354284 N Y N N 
II Thessalonians R,C, I Cor 0.30863299 N Y N N 
Correct    12/24 (50%) 25/32 (78%) 30/30 (100%) 
Figure 3.  
 20
Interpretation of the Table:  The interpretation of the current table56 is straightforward. 
The column entitled “Test Set” contains the book(s) being tested to see if they are statistically 
different from the books listed in the “Reference Set” column. The statistical column labeled 
“significance” contains the result of the test (a value less than 0.05 means that the two sets of 
books are significantly different.)  The next column represents that significance as a Y (yes) or 
an N (no.) The next two columns present the two leading theories concerning authorship, and the 
third column presents the theory related to genre. These three columns contain what the 
particular theory states concerning the relationships between the books in question. For example, 
in the first row, inspect the column entitled, “Minority View: Paul wrote the Hauptbriefe only.” 
It will be recalled that the older Tübingen school concluded that Paul did not write either of the 
Thessalonian epistles. Accordingly, there is a “Y” in that cell, indicating a significant difference 
stylistically between the Thessalonian epistles and the Hauptbriefe is expected. The next column 
(the seven-epistle Pauline hypothesis) is to be understood similarly. In the last column, however, 
we present a genre-based scheme that asserts that genre, not authorship, defines the significant 
distances. Thus, on the basis of genre, this theory, for instance, would expect that the Pastorals, a 
personal hortatory paraenetic to an individual would be significantly different stylistically than 
the Hauptbriefe, a doctrinal epistle written to a church. In all three columns the cells in the table 
are colored red if the theory does not match the statistical result. A cell is labeled “n/a” if it posits 
a comparison inappropriate to that theory.   
Results of the table: Inspection of the table reveals that the minority critical view (i.e. the four-
epistle Pauline hypothesis) deviates from the statistical results in twelve out of the 24 tests run. 
The majority critical view (the seven-epistle Pauline hypothesis) fares much better, but it still 
deviates from the statistical findings in seven out of the thirty two tests. Moveover, this second 
                                                 
56 Tests of more pairings are currently in process. 
 21
view proposes, naturally, that the synoptic gospels were written by three writers, but the 
statistical data considers them to be indistinguishable.57  The close clustering of the three  
synoptic gospels, underscores the problem faced by the almost ubiquitous historical assumption 
that the major features of intertextual variation are due to authorship.  If this is the case, then the 
statistical indistinguishability between the synoptic gospels necessitates that either the synoptic 
gospels are indeed written by the same author, which no one contends, or that the borrowing 
from both Markan primary material and Q is so ubiquitous that that common material 
overwhelms the unique stylistic material of these texts.58 Lastly, the traditional view of Pauline 
authorship asserts that Paul wrote all 13 epistles attributed to him, but that stylistic variation is to 
be understood as reflective of genre or register rather than authorship. Using only a slightly 
modified version of the genre classification found in introductory New Testament texts, this 
theory classifies all thirty of these statistical tests correctly. The genre classification used is 
found in figure 4.  
Figure 4. 
                                                 
57 It would be interesting indeed to remove the purported Q material from the synoptic gospels and retest to see if the 
synoptics are then recognized as separate documents statistically.   
58 The later hypothesis is viable one and will be investigated.  
Genres in the Greek NT 
Epistolary Historical Narrative 
Corporate Personal 
Gospel -
Synoptic  
Gospel-
Apologia 
Historical 
Narrative 
Didactic/ 
Theo-
logical 
Jewish 
Didactic 
Didactic 
Paranesis 
General Jewish 
Para-
nesis 
Paranesis Direct 
Address 
Apoc-
alyptic 
Matthew John Acts Romans Hebrews Coloss. I Peter James I Timothy I John Revelation
Mark   I Cor.  Ephes. II Peter  II Timothy II John  
Luke   II Cor.   Jude  Titus III John  
   Galatians   II John  Philemon   
   I Thess.   III John     
   II Thess.        
   Philipp.        
 22
 
Conclusion: 
In this inquiry we have explored the relationship of quantitative methods to the more traditional 
historical critical methods within Biblical studies. Given our discussion we suggest the following 
summary findings:  
• Quantitative methods have been excluded from the more mainstream theological 
methods, we suggest, unconsciously, rather than peremptory, having become a 
victim of both medieval and Enlightenment era fragmentation events.  
• A truly scientific approach to the text analysis should not be considered identical 
to the historical-critical program, and many of the historical-critical approaches 
fail to meet modern interdisciplinary tests of scientific validity. 
• Our worked example demonstrates that genre rather than authorship far better 
explains the large scale features of inter-textual variation in the Greek New 
Testament. By demonstrating that the largest amount of stylistic variation in the 
Greek New Testament is due to genre rather than authorship, we suggest that the 
vast majority of prior studies that assumed, ipso facto, that large stylistic variation 
equates to authorial variation will necessarily need to be reconsidered. 
 
Last, if the quantitative program to which we allude should gain more traction in Biblical 
studies, we would submit the following implications on their role theologically:  
• Quantitative methods may say virtually nothing concerning theology initially, but, 
like exegesis, their outcomes can be used theologically. 
• Quantitative methods are not a panacea, nor in any sense a replacement for 
exegesis, but should function in parallel with it.  
• Quantitative methods can contribute to the hermeneutical task by elucidating how 
certain syntactical constructs are used. This comparison contributes to the 
interpretive exercise. 
• Prospectively, in time we suggest that quantitative methods may indeed make 
historic contributions to the task of theology by contributing substantially to 
source, authorship, genre and dating issues. 
 
Last, we ponder if quantitative methods might not be the vanguard of an emergent set of 
interdisciplinary efforts that may be useful in healing some of the fracture that so exercised 
Johann Gabler.  If so, we think it safe to surmise that Gabler would be pleased.  
 23
 
Bibliography 
Andersen, Francis I. Hebrew Verbless Clause in the Pentateuch. JBL Monograph Series, 14. 
Nashville & New York: Abingdon, 1970. 
 
Bahg, Chang-Gen. “Major Systems Theories Throughout the World.” Behavioral Science 35:2 
(1990) 79-108. 
 
Baird, Arthur. “Content Analysis and the Computer: A Case-study in the Application of the 
Scientific Method to Biblical Research.” Journal of Biblical Literature 95:2 (1976) 255–277. 
 
Baird, William. History of New Testament Research From Deism to Tübingen. Minneapolis: 
Fortress, 1976. 
 
Barr, GK. “Two Styles in the New Testament Epistles.”  Literary and Linguistic Computing 18:3 
(2003) 235–247. 
 
Benzécri, J. P. “Statistical Analysis as a Tool to Make Patterns Emerge from Data.” In: 
Methodologies of Pattern Recognition, edited by S. Watanabe. New York: Academic Press. 
1969. 
 
Brown, Steven R and  Lawrence E. Melamed. Experimental Design and Analysis. Vol 74 of the 
series Quantitative Applications in the Social Sciences. edited by Michael S. Lewis-Beck. 
Thousand Oaks: Sage Publications, 1990. 
 
Burrows, John and Hugh Craig. “Lucy Hutchinson and the Authorship of Two Seventeenth-
Century Poems: A Computational Approach.” Seventeenth Century 16:2 (2001) 259–283. 
 
Campbell, Lewis. The Sophistes and Politicus of Plato. Oxford: Clarendon Press, 1867. 
 
___________. Review of Untersuchungen uber Plato, by Constantin Ritter. The Classical Review, 
3:1/2 (1889) 28–29. 
 
Carson, D. “Systematic Theology and Biblical Theology,” in New Dictionary of Biblical 
Theology, edited by T. D. Alexander and B. S. Rosner, 89–104. Leicester/Downers Grove: Inter-
Varsity Press, 2000. 
 
Cortina-Borja, Mario, and Constantinos Chappas. “A stylometric analysis of newspapers, 
periodicals and news scripts.” Journal of Quantitative Linguistics 13:2–3 (2006) 285–312. 
 
Dickenson Charles, “Markus Barth and Biblical Theology: A Personal Review,” HBT 17:96–116 
 
Dittenberger, Wilhelm. “Sprachliche Kriterien für die Chronologie der platonischen 
Dialoge.” Hermes 16 (1881). 
 24
Ellison, John W. “Computers and the Testaments” In Computers in Humanistic Research, edited 
by. Edmund A. Bowles, 160–169. New York: Prentice-Hall, 1967. 
 
Fisher, R.A. Statistical Methods for Research Workers. 14th ed. New York: Macmillan 
Publishing Company, (1970). 
 
Gabler, Johann P  Appendix: "An Oration on the Proper Distinction Between Biblical and 
Dogmatic Theology and the Specific Objectives of Each” In The Flowering of Old Testament 
Theology: A Reader in Twentieth-Century Old Testament Theology, edited by Ollenburger, Ben 
C. et al., 489-502. Winona Lake, IN: Eisenbrauns, 1992. 
 
Girón, Javier et. al. “Bayesian Analysis of a Multinomial Sequence and Homogeneity of Literary 
Style.” American Statistician 59:1 (2005) 19–30. 
 
Greenacre, M. J. Theory and Applications of Correspondence Analysis. London: Academic 
Press, 1984. 
 
Gross, Alexander. “Is Evidence Based Linguistics the Solution? Is Voodoo Linguistics the 
Problem?” No pages. (A seminar paper presented at the LACUS Conference. Dartmouth, 2005.) 
Online: http://languag2.home.sprynet.com/f/evidence.htm#top 
 
Holmes, D.I. “Stylometry: Its Origins, Development and Aspirations.” (Paper presented at the 
annual meeting of the Joint International Conference of the Association for Computers and the 
Humanities and the Association for Literary & Linguistic Computing. Kingston, ON., 1997.) 
 
Hoover, Daniel L. “Multivariate Analysis and the Study of Style Variation.” Literary and 
Linguistic Computing 18:4 (2003) 341–360. 
 
Hoover, David L. and Thomas N. Corns. “The Authorship of the Postscript to An Answer to a 
Booke Entitled, An Humble Remonstrance.” Milton Quarterly  38:2 (2004) 59–75. 
 
Juola, Patrick et al. “A Prototype for Authorship Attribution Studies.” Literary & Linguistic 
Computing 21:2 (2006) 169–178.  
 
Kannengiesser, Charles. Handbook of Patristic Exegesis: the Bible in Ancient Christianity In the 
Series Bible in Ancient Christianity.  Leiden, Boston: Brill, 2004. 
 
Kenny, Anthony. A Stylometric Study of the New Testament. Oxford: Clarendon Press, 1986. 
 
Kraft, Robert A. “Computers and Textual Criticism of the NT [Greek Scriptures] (1994 gofer).” 
No pages.  Accessed 18 May 2007.  Online: 
http://ccat.sas.upenn.edu/gopher/other/courses/rels/735/textcriticism/nttcart.new. 
 
Libby, James A. “Exploring the Secret Garden of Computational Linguistics: A Brief Survey of 
the New Growth with a Particular Look at One Bloom” 2007. (unpublished.) 
 
 25
Ledolter, Johannes and Arthur J. Swersey. Testing 1 - 2 – 3 Experimental Design with 
Applications in Marketing and Service Operations. Stanford: Stanford University Press, 2007. 
 
Lutosławski, Wincenty. “Principes de stylométrie”. Revue des études grecques 41 (1898) 61– 
81. 
 
Lutoslawski, Wincenty. The Origin and Growth and Plato's Logic. London: Longmans, Green 
and Co., 1897. 
 
Madigan, David, et al. “Bayesian Multinomial Logistic Regression for Author Identification.” 
AIP Conference Proceedings  803:1 (2005) 509–516. 
 
Mannion, David and Peter Dixon. “Sentence-length and Authorship Attribution: the Case of 
Oliver Goldsmith.” Literary & Linguistic Computing 19:4 (2004) 497–508. 
 
McConville, J. G. “Biblical Theology: Canon and Plain Sense (Finlayson Memorial Lecture 
2001),” Scottish Bulletin of Evangelical Theology 19 (2001) 129–133. 
 
Mirolli, Marco and Domenico Parisi. “How can we explain the emergence of a language that 
benefits the hearer but not the speaker.” Connection Science 17:3–4 (2005) 307–324. 
 
Mendenhall, T. C.  “The Characteristic Curves of Composition” Science, 11 (1887) 237-49. 
 
Michaelson, S. and A. Q. Morton. “The New Stylometry: A One-Word Test of Authorship for 
Greek Writers.” The Classical Quarterly  22:1 (1972) 89–102. 
 
Morton, A. Q. and J. McLeman. Christianity in the Computer Age. New York: Harper & Row, 
1964. 
 
Mosteller, Frederick, and David L. Wallace. “Inference in an Authorship Problem,” Journal of 
the American Statistical Association 58:302 (1963) 275–309. 
 
Murphy-O'Connor, Jerome. Paul the Letter-Writer: His World, His Options, His Skills. GNS. 
Collegeville, MN: Liturgical Press, 1995.  
 
Radday, Yehuda T. An Analytical Linguistic Concordance to the Book of Isaiah. The Computer 
Bible; v 02. Missoula, Mont.: Scholars, 1975. 
 
Radday, Yehuda T., and Haim Shore. Genesis: An Authorship Study in Computer-Assisted 
Statistical Linguistics. Analecta biblica 103. Rome: Biblical Institute, 1985. 
 
Radday, Yehuda T., and Dieter Wickmann. “Unity of Zechariah Examined in the Light of 
Statistical Linguistics.” ZAW 87 1 (1975): 30-55. 
 
Rand, William. “Markov Models of Literary Style for Authorship Identification.” Fluctuation & 
Noise Letters 2:4 (2002) 1299–1303. 
 26
 
Rudman, Joe et al. “The State of Authorship Attribution Studies” (Paper presented at the annual 
meeting of the Joint International Conference of the Association for Computers and the 
Humanities and the Association for Literary & Linguistic Computing. Kingston, ON. 1997.) 
 
Schumm, Walter R. “A Discriminant Analysis of Whissell's New Testament Data: on the 
Statistical Trail of The Author Of Hebrews.” Psychological Reports 98:1 (2006) 274–276. 
 
Stamatatos, Efstathios. “Authorship Attribution Based On Feature Set Subspacing Ensembles.” 
International Journal on Artificial Intelligence Tools 15:5 (2006) 823–838. 
 
Spector, Paul E. Research Designs. Vol 23 of the series Quantitative Applications in the Social 
Sciences. Edited by Michael S. Lewis-Beck. Thousand Oaks: Sage Publications, 1981. 
 
Tambouratzis, George, et al. “Applying the SOM Model to Text Classification According to 
Register and Stylistic Content.” International Journal of Neural Systems 13:1 (2003) 1–11. 
 
Walbe, Ernst. Syntaxis Platonicae Specimen. Bonn: Bonnae, 1888. 
 
Yang, Albert, C.C et al. “Information categorization approach to literary authorship disputes.” 
Physica A 329 (2003) 473–483. 
 
Yule, George U. “On Sentence-Length as a Statistical Characteristic of Style in 
Prose; with application to two cases of disputed authorship.” Biometrika 30 (1938) 363–390. 
 
Yule, George U. The Statistical Study of Literary Vocabulary. Cambridge: Cambridge 
University Press, 1944. 
 
