1340 IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 16, NO. 7, SEPTEMBER 2008
Towards High-Quality Next-Generation
Text-to-Speech Synthesis: A Multidomain Approach
by Automatic Domain Classification
Francesc Alías, Member, IEEE, Xavier Sevillano, Joan Claudi Socoró, and Xavier Gonzalvo
Abstract—This paper is a contribution to the recent advance-
ments in the development of high-quality next generation text-to-
speech (TTS) synthesis systems. Two of the hottest research topics
in this area are oriented towards the improvement of speech ex-
pressiveness and flexibility of synthesis. In this context, this paper
presents a new TTS strategy called multidomain TTS (MD-TTS)
for synthesizing among different domains. Although the multido-
main philosophy has been widely applied in spoken language sys-
tems, few research efforts have been conducted to extend it to the
TTS field. To do so, several proposals are described in this paper.
First, a text classifier (TC) is included in the classic TTS architec-
ture in order to automatically conduct the selection of the most
appropriate domain for synthesizing the input text. In contrast to
classic topic text classification tasks, the MD-TTS TC should not
only consider the contents of text but also its structure. To this end,
this paper introduces a new text modeling scheme based on an as-
sociative relational network, which represents texts as a directional
weighted word-based graph. The conducted experiments validate
the proposal in terms of both objective (TC efficiency) and subjec-
tive (perceived synthetic speech quality) evaluation criteria.
Index Terms—Speech synthesis, text processing.
I. INTRODUCTION
T HERE has been a very noticeable development overthe last 20 years in the text-to-speech (TTS) synthesis
research field. In particular, TTS systems have moved from
diphone-based approaches, with only one instance per unit, to
unit selection or corpus-based strategies, using large speech
corpora containing multiple instances per unit [1], [2]. In this
change of paradigm, the TTS research community has borrowed
several aspects of the philosophy and some specific techniques
(e.g., search algorithms, cost functions, etc.) from the automatic
speech recognition (ASR) field [3]. During the last decade, this
convergence has been stressed by the application of hidden
Markov models (HMMs) to conduct speech synthesis (e.g., see
[4], [5] and related works) as opposed to classic concatenative
strategies. HMM-based TTS synthesis allows higher flexibility
thanks to speech signal parameterization, but it is still not
Manuscript received June 29, 2007; revised November 13, 2007. Published
August 13, 2008 (projected). This work was supported in part by the IntegraTV-
4all project under Grant FIT-350301-2004-2 of the Spanish Science and Tech-
nology Council. The associate editor coordinating the review of this manuscript
and approving it for publication was Dr. Steve Renals.
The authors are with the Grup de Recerca en Processament Multimodal,
Enginyeria i Arquitectura La Salle, Universitat Ramon Llull, Quatre Camins,
2, 08022 Barcelona, Spain (e-mail: falias@salle.url.edu; xavis@salle.url.edu;
jclaudi@salle.url.edu; gonzalvo@salle.url.edu).
Digital Object Identifier 10.1109/TASL.2008.925145
Fig. 1. Different approaches to TTS research towards perfect unconstrained
speech synthesis, as a function of the task difficulty and the obtained synthetic
speech quality—adapted from [7] and [8]—with the multidomain proposal
superimposed.
capable of achieving the typical high speech quality obtained
by unit-selection concatenative approaches [6].
However, whatever the synthesis approach taken, the final
purpose of any text-to-speech system is the generation of per-
fectly natural synthetic speech from any input text. In this quest,
two complementary strategies have been followed historically
(see Fig. 1), which constitute a tradeoff between speech natu-
ralness and system flexibility [7], [8]: 1) general purpose TTS
synthesis (GP-TTS), which prioritizes the flexibility of the ap-
plication at the expense of the achieved synthetic speech quality,
and 2) limited domain TTS (LD-TTS), which restricts the scope
of the input text (as done in initial ASR systems [8]) so as to
obtain high quality synthetic speech [9]. Driven by the success
of limited-domain speech understanding systems [3] and multi-
media applications (e.g., [10]) there has been a growing interest
in developing commercial systems based on LD-TTS.
According to [6] and [11], next-generation TTS systems are
asked to deal with: expressivity (emotions, speaking styles, etc.)
[12], flexibility (multilinguality, voice transformation, voice
impersonation, etc.) [5], spontaneity (whispering, pausing,
laughing, etc.) [13], and even singing,1 among others. Thus,
these TTS systems should be able to produce the message using
1For instance, a Synthesis of Singing Challenge was held at the Inter-
Speech2007 conference.
1558-7916/$25.00 © 2008 IEEE
Authorized licensed use limited to: Agean University. Downloaded on October 16, 2008 at 12:48 from IEEE Xplore.  Restrictions apply.
ALÍAS et al.: TOWARDS HIGH-QUALITY NEXT-GENERATION TTS SYNTHESIS 1341
Fig. 2. Modification of the classic block diagram of a TTS synthesis system by
the inclusion of an automatic domain classification module.
the most appropriate prosody, speaking style, etc., an issue that
can be faced by extracting more information from the input text.
For this reason, a new research direction has emerged in the
TTS field towards extending text analysis beyond the typical
capabilities of TTS systems. Several recent papers can be found
in the literature focused on this issue by, for instance, extracting
the user attitude from text [14] or guessing the underlying
emotion of the message [15]–[17]—see also references therein.
In this context, as an approach to increase the TTS systems
flexibility while trying to maintain a speech quality equivalent
to that of LD-TTS, we introduced the idea of developing mul-
tidomain TTS (MD-TTS) systems in order to synthesize among
different domains with high speech naturalness [18]. This ap-
proach can be seen as a step further in the convergence of the
different key modules of multidomain spoken language systems
(i.e., the ASR and TTS modules), using information regarding
the domain of the conversation for improving the quality of the
synthetic output. For instance, depending on the particular im-
plementation of the TTS system, knowing the domain of the
input text allows to: 1) help in the text normalization process
(e.g., if the input text belongs to a mathematical domain, the
text “1/2” should be translated into “half” instead of “January
the second”); 2) choose the most appropriate prosodic model
or consider different prosodic patterns during unit search [19];
3) select the corresponding subcorpus for corpus-based tiering
approaches [10], [20] or guide the unit selection process by
weighting the domain units accordingly for blending methods
[15], [21]; 4) control the signal processing module depending on
the speech characteristics of that domain (e.g., voice quality in-
terpolation [22]); or 5) activate the voice transformation module
to resemble the target domain, if necessary; etc.
Therefore, MD-TTS systems need to know, at run time, which
domain is the most suitable for synthesizing the input text so as
to obtain the highest speech quality. Thus, if domain detection
is to be conducted in a fully automatic manner from the raw
input text, it is necessary to redefine the classic architecture of
TTS systems by including a domain classification module (see
Fig. 2).
There is a large amount of research on text classification (see
[23] and [24] for an extensive review). Traditional text clas-
sification (TC) techniques are mainly focused on thematic (or
topic) documents categorization. In this context, documents are
represented by only considering the occurrence of the key terms
that constitute the texts (often, after filtering function words and
stemming), thus, ignoring their relationships and text structure
[23]. Although topic information is useful for organizing the
speech corpus, relying solely on text contents is insufficient for
considering the inherent sequential nature of speech (which is
related to prosody and coarticulation issues). Thus, proper TC
for MD-TTS should consider both thematic and stylistic aspects
of text, like in other TC-related applications such as authorship
attribution or genre detection [25]. Equally important, TC for
MD-TTS should consider all the terms and punctuation marks
appearing in the text, not only because function words filtering
would induce the loss of valuable information concerning text
structure, but also because texts input to TTS systems can be
very short, e.g., only one sentence.
In this paper, we first describe the related work regarding
multidomain spoken language systems and corpora (Section II).
Second, the specific implementation of a multidomain TTS
system following a tiering corpus-based synthesis technique is
described (Section III). Next, a global and a reduced variant
of a novel graph-based text representation model designed
for conducting text classification within the TTS framework
are introduced (Section IV). Then, several experiments re-
garding text classification in the MD-TTS context are described
(Section V). Finally, we discuss several issues related to the
proposal, outlining some interesting future research directions
(Sections VI and VII).
II. RELATED WORK
This section describes the main issues related to multidomain
spoken language systems, emphasizing the role that the auto-
matic speech recognition module plays in these kind of systems,
and the motivations for exporting the multidomain strategy to
the TTS field.
Although other speech-related research fields have adopted
multidomain philosophies so as to improve the naturalness and
usability of spoken language systems (see Section II-A), the re-
search in the TTS field has been largely an exception to this rule.
This situation is motivated by two issues: first, the fact that early
TTS systems were quite capable of facing general-purpose syn-
thesis (with acceptable intelligibility), in contrast to ASR sys-
tems, which had to focus on restricted tasks (e.g., single-speaker
digit dictation) to achieve reasonable performances [8]. Second,
it is worth noting that TTS systems have often played a sec-
ondary role in multidomain spoken language systems, which
typically make use of general-purpose TTS systems, since they
are often only asked to give an intelligible message to the user
(e.g., see [26]).
A. Multidomain Spoken Language Systems
The development of multidomain applications is one of
the recent research directions in spoken language systems
(SLS) [27]. Most multidomain SLS (MD-SLS), excluding
general-purpose dictation systems, operate over a finite set
of domains of interaction, e.g., different destinations in call
routing, several topics in translation systems, or different
subdomains in complex dialog systems [28]. Knowing the
domain of communication (in this context, a domain generally
corresponds to a topic), allows to improve the performance and
Authorized licensed use limited to: Agean University. Downloaded on October 16, 2008 at 12:48 from IEEE Xplore.  Restrictions apply.
1342 IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 16, NO. 7, SEPTEMBER 2008
efficiency of the constituting modules of SLS [29], for instance,
by 1) selecting the most appropriate language model of a
speech recognizer (thus, reducing its perplexity), 2) adapting
the dialog manager strategy for reducing the number of di-
alogue turns, 3) dynamically loading the required resources
according to the current domain of interaction, 4) helping
the comprehension module to disambiguate word senses, or
5) controlling out-of-domain queries—see, e.g., [27], [28], and
[30] and references therein.
One of the main research areas where multidomain strategies
have been more extensively applied is ASR, where automatic
domain classification plays a salient role. The following sections
describe both issues.
1) Multidomain ASR: Automatic speech recognition systems
have evolved from single-speaker, isolated word, small vocabu-
lary tasks to speaker-independent large-vocabulary continuous
speech applications [8]. This evolution has affected the two key
elements constituting ASR systems: the acoustic and the lin-
guistic models. These models have suffered a transformation
from highly controlled approaches (e.g., grammatical rules or
task-and-speaker dependent acoustic models) to more flexible
strategies (e.g., multispeaker acoustic models or stochastic lan-
guage models), thus, being able to satisfy more complex and
generic needs. One of the most critical problems, which derives
from the increase in the number of users and the vocabulary size,
is the mismatch between the training and test data characteris-
tics. These misadjustments can be caused by the evolution of the
domain of interaction (e.g., change of the most common words
used to interact), speaking styles variations (e.g., due to mood
changes), etc. (see [31] for more information). This issue can
be tackled by adapting both the linguistic [32] and acoustic [33]
models to the task and/or user speech particularities—see [31]
for a general review of the most commonly applied adaptation
techniques.
2) Domain Classification Strategies: Assigning domains to
user utterances at run time is a key issue in MD-SLS [27]. In
this context, the domain selection process can be user-guided,
by explicitly using a predefined set of keywords, or dialog-
guided, i.e., implicitly detected from speech recognition hy-
potheses [27], [28], [30]. The former simplifies the task of as-
signing domains but reduces the usability of the SLS. The latter
allows natural navigation thanks to automatic domain classifi-
cation, but it must be able to extract (reliable) information from
short utterances (e.g., typical queries are between 10 and 20
words long [30]) despite speech recognition errors [28]—a far
more complicated task than topic classification of articles or
broadcast news, where very large data collections are used [23],
[30].
B. Deeper Analysis of Text in TTS Systems
The typical analysis of the input text conducted in TTS sys-
tems has been usually restricted to tasks related to natural lan-
guage processing such as text normalization, pausing predic-
tion, part-of-speech tagging, etc. However, several recent works
have been focused on extracting more information from the
input text, e.g., trying to determine the attitude [14] or the emo-
tion [15]–[17], [34], [35] from text. In [14], the correlation be-
tween the prosodic variations and the inclusion of adjectives
(weighted by the accompanying adverbs) expressing positive
or negative attitudes (with different levels of intensity) is ana-
lyzed and demonstrated. In [15], a Dictionary of Affect is used
to detect and score the emotional keywords present in text—fol-
lowing a similar approach to the one described in [34]—ad-
justing the unit selection cost function to guide the unit search
according to the emotional contents of the input text. The exper-
iments are conducted on a unique expressive corpus composed
of three different emotions (neutral, happy, and angry). The au-
thors conclude that the subjective emotional perception is pro-
portional to the number of emotional words in the input text. In
[16] and [35], a similar approach is presented by defining a dic-
tionary of emotional words (adjectives, names or verbs), but also
using part-of-speech tagging and linear classification to deter-
mine the emotion from text.2 From another point of view, works
such as [36] and [37] make use of more complex knowledge like
semantic and common sense networks, respectively—more in-
formation can be found elsewhere.3
In [18], we introduced the seminal idea of synthesizing dif-
ferent domains within the same TTS system by introducing an
automatic domain classification module in the classic TTS ar-
chitecture, thus covering the niche between GP and LD-TTS. On
one hand, the TTS task difficulty is increased due to the manage-
ment of multiple domains, but, on the other hand, the achieved
speech quality aims to be equivalent to that of LD-TTS when
the input text is assigned to the correct domain, thanks to the
direct correspondence between style and domain. Hence, this
approach can represent an advancement towards perfect uncon-
strained speech synthesis (see Fig. 1). The proposal was theo-
retically analyzed (no speech corpus was recorded), obtaining
remarkable computational savings (again observed when devel-
oping the weather forecast application described in [10]) besides
keeping good theoretical speech quality.4 Subsequently, in [39]
we presented a hierarchical text classifier based on independent
component analysis (ICA), which was capable of 1) organizing
the contents of the corpus in a hierarchical manner (obtaining a
structure similar to the one depicted in Fig. 3), and 2) classifying
the texts to be synthesized according to the learned structure.
Both works were developed on collected Catalan and Spanish
news articles.
III. MULTIDOMAIN CORPUS-BASED TTS
Although corpus-based TTS systems are able to generate
high-quality synthetic speech, it is a commonplace that a
dramatic decrease in speech quality occurs when the input
2In this context, research has been mainly focused on storytelling texts (e.g.,
fairy tales) due to their emotional context [16], [17].
3See, for instance, www.clairvoyancecorp.com/Research/Workshops/AAAI-
EAAT-2004/home.html
4Computed as the average segment length [38] of the sequence of units re-
trieved from the speech corpus (although it is obvious that not always the largest
set of units attains the best synthetic results, it was the only measure we could
use without having a speech corpus available).
Authorized licensed use limited to: Agean University. Downloaded on October 16, 2008 at 12:48 from IEEE Xplore.  Restrictions apply.
ALÍAS et al.: TOWARDS HIGH-QUALITY NEXT-GENERATION TTS SYNTHESIS 1343
Fig. 3. Block diagram of a multidomain corpus-based text-to-speech synthesis
system, including automatic domain assignment by text classification and a
tiering speech corpus.
text mismatches the corpus domain coverage, for both gen-
eral-purpose [38], [40] and, more obviously, limited domain
TTS syntheses [10]. This is due to the fact that the quality of
the speech generated by corpus-based TTS systems is highly
dependent on the style and coverage of the recorded speech
corpus [41].
Several proposals have appeared in the literature trying to al-
leviate the consequences of domain mismatch. The most repre-
sentative examples of these proposals are: 1) the adaptation of
a GP-TTS system to the target domain including small speech
corpora of that domain [38], [40], 2) the design of speech cor-
pora based on the emotional contents of the text composing
the corpus [42], and 3) the definition of a multidomain TTS
(MD-TTS) approach to deal with the desired domains altogether
[18]. All these proposals are based on the fact that knowing
which is the most appropriate domain for synthesizing the input
text allows much more proper delivery [43]—provided that that
domain is properly synthesized from the speech corpus.
Although particularized on a tiering corpus-based approach
in this paper (see Section III-B), the MD-TTS philosophy can be
adapted to any other corpus typology or even synthesis strategy
other than corpus-based (e.g., HMM-based, or hybrid solutions
[6]). Therefore, the MD-TTS architecture allows a flexible and
adaptable TTS system design and implementation that can be
tuned according to the application needs or domain characteris-
tics. In any case, the architecture of the TTS system must be
modified by the inclusion of a domain classification module,
which will interact with the remaining elements of the TTS
system (see Fig. 2). In the following paragraphs, a discussion
on domain classification strategies and a description of a specific
implementation of a corpus-based MD-TTS system following a
tiering approach are presented.
A. Domain Classification: Concepts and Strategies
The selection of the most suitable synthesis domain is often
closely related to the determination of the most appropriate syn-
thesis speaking style for a given input text, which usually de-
pends on paralinguistic (speakers relationship, mood, message
intention, etc.) and extralinguistic (speaker’s age, sex, person-
ality, etc.) information [44]. For instance, the sentence “There
is a lot of food in the fridge,” can be spoken joyfully or with
heavy sarcasm, depending on the context of communication. Al-
though the study of these issues lies beyond the scope of this
work, there are cases in which the style of delivery or even the
speaker’s gender can be inferred from the meaning and/or the
structure of the input text (e.g., natural synthesis of positive or
negative messages requires using appropriate prosodic patterns
[14], [21], or some sentences are expected to be spoken by a
boy or a girl [41]). In other cases, certain speaking styles can be
readily discarded for synthesizing some types of sentences [45]
(e.g., command utterances do not convey sadness or fear [20],
or complex sentences are not usually spoken by children [41]).
As regards the implementation of the domain classification
strategy, it can be defined as an external task to the TTS system
itself or it can be included in the TTS architecture as an auto-
matic module. In the former case, domain selection can be ei-
ther a manual process (which is the simplest method, allowing
the user to change among domains by hand [8], as in some dia-
logue systems or multimodal applications), or a supervised one
(usually related to systems where the message domain is known
beforehand, by tagging the input text accordingly [10], [20] or
by means of concept-to-speech synthesis [8]).
In this paper, domain assignment is regarded as an automatic
process, which relies on the input text solely. The most appro-
priate synthesis domain is inferred from text thanks to the direct
relationship between domain and speaking style, obtaining the
highest possible synthetic speech quality [43], and/or reducing
the computational cost of the unit selection process [18]. For
this reason, it is necessary to go beyond the typical text anal-
ysis of TTS systems. In the current version of our approach,
this module is implemented by an automatic text classification
technique based on a vector space model representation of texts,
which includes information about the frequency and collocation
of words plus the structure of text [18] (see Section IV).
B. Tiering Corpus-Based MD-TTS Architecture
As aforementioned, the MD-TTS approach requires in-
cluding a domain classification module, which will interact
with the modules found in the classic corpus-based TTS sys-
tems architecture, such as the NLP, unit-selection and digital
signal processing modules. Moreover, corpus-based MD-TTS
synthesis requires using a multidomain speech corpus, which
can be implemented following distinct corpus typologies [41],
[43]: 1) tiering, that is, defining an independent subcorpus
for each domain [20], [42], or 2) blending, which consists in
mixing different corpus subsets into a unique corpus, generally
including a large general-purpose core [21], [38], [40]. Fig. 3
depicts a corpus-based MD-TTS system based on a tiering
Authorized licensed use limited to: Agean University. Downloaded on October 16, 2008 at 12:48 from IEEE Xplore.  Restrictions apply.
1344 IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 16, NO. 7, SEPTEMBER 2008
approach, which is employed throughout the experiments
presented in Section V.
It is to note that the creation of multidomain speech corpora
has also been handled by the HMM TTS research community,
where the tiering and blending strategies are called style depen-
dent modeling and style mixed modeling, respectively (see [5]
and related works)—style corresponds to a particular kind of do-
main in the MD-TTS context, where the term domain can stand
for emotion, speaking style, topic, etc. Moreover, notice the sim-
ilarity between these concepts and the MD-ASR approaches de-
scribed in Section II-A1.
IV. TEXT CLASSIFICATION FOR MD-TTS SYNTHESIS
This section describes the proposal for implementing the do-
main classification module in the MD-TTS framework, whose
objectives are to 1) learn a multidomain model upon the training
texts that constitute each domain of the multidomain speech
corpus, and 2) select the most appropriate domain(s) for syn-
thesizing the input text. The starting requirements for designing
this module—besides looking for the highest classification per-
formance—are the following:
• adapt the text classification system to the particularities of
text-to-speech synthesis, taking specially into account the
importance of classifying texts as short as one sentence;
• minimize the classification algorithm complexity so as to
avoid overloading the text-to-speech conversion process.
Most text classification strategies are mainly thematically ori-
ented, thus treating texts as a collection of isolated words, ig-
noring their order, relationships and text structure (the so-called
bag-of-words model) [23]. In this context, function words (e.g.,
prepositions and articles) and punctuation marks are commonly
filtered out (stop listing), besides usually reducing words to their
lemmas (stemming). However, stylistic (nonthematic) text clas-
sification tasks, such as authorship attribution (i.e., the identi-
fication of the author of the text) or genre detection (e.g., lit-
erary, scientific, etc.) do consider function words distribution,
part-of-speech tags, words and sentence length, vocabulary rich-
ness, among other parameters [24], [25].
Therefore, the text classification task involved in MD-TTS
lies between thematic and stylistic classification. Whereas text
contents is useful to organize the text in the (tiering) multido-
main corpus, it seems insufficient to determine the best way to
pronounce a given text, making it necessary to include some
information about the structure and sequentiality of text (see
Sections IV-B and IV-C1).
A. Associative Relational Networks
To deal with the aforementioned stylistic aspects of text, it
is essential to make use of a text representation technique ca-
pable of codifying them. To this end, the developed TC system
represents the texts by means of an associative relational net-
work (ARN), a graph-based model for representing informa-
tion, which was initially introduced in the context of visual rep-
resentation of documents [46]. However, in our approach, the
nodes of the graph represent the terms of the text (i.e., words and
punctuation marks) and their connections describe the co-occur-
Fig. 4. Word-based associative relational network, inspired by the visual rep-
resentation of documents of the Galaxy of News described in [46].
rences between them (see Fig. 4). Each node contains a weight
defining the relevance of its corresponding term, and each
connection is weighted by the relationship strength between the
linked terms , by also considering their order (i.e., not nec-
essarily , as opposed to visual documents representa-
tion [46]).
As a result, the ARN encodes the structure and the sequen-
tiality of text (modeled as a run of pair wise co-occurrences of
terms), which are essential for classifying texts in the MD-TTS
framework.
B. Weighting the Network
Once the ARN architecture is defined, it is necessary to
assign specific values to the network weights. In particular,
the nodes will basically contain information of text contents
whereas the internodal connections will be used to represent
and extract structural text patterns. For the time being, the the-
matic features weighting the relevance of each term which
are employed in this work are: 1) term frequency (TF) (i.e.,
the number of times a term occurs in a document) inverse
document frequency (i.e., the singularity of that term across the
collection), denoted as TFIDF [47], and 2) a newly proposed
weight called inverse term frequency (ITF), which is defined as
ITF
TF
TF (1)
where denotes the cardinality of a set hereafter—in this case,
represents the number of terms of document —and TF
is the term frequency of the th term in that document. ITF can
be interpreted as a local approximation of IDF [23], since it
weighs each term according to its prominence within each text
(or document), instead of considering its distribution across the
whole training text collection.
By its own definition, the ARN allows considering structural
resemblance between texts when conducting classification. In
this paper, the co-occurrence frequency (COF) of each consec-
Authorized licensed use limited to: Agean University. Downloaded on October 16, 2008 at 12:48 from IEEE Xplore.  Restrictions apply.
ALÍAS et al.: TOWARDS HIGH-QUALITY NEXT-GENERATION TTS SYNTHESIS 1345
Fig. 5. Building the (a) global ARN and (b) domain ARN-Fs, from D to D , as a function (f) of the global ARN representation of , built from
their corresponding training documents D = fD ; . . . ; D g. In the graphs, “” denotes filled nodes, “” represents empty nodes and dashed lines symbolize
inexistent co-occurrences. ( ~p 2 ) within the VSM defined by the global ARN (see example of Table I).
utive pair of terms (i.e., the number of times that two terms
are contiguously placed in the text) and the number of consec-
utive term pairs (see Section IV-C3) are considered as stylistic
(structural) features—see Section VI for possible extensions.
C. Using the ARN for Conducting Text Classification in
Corpus-Based TTS Synthesis
The following paragraphs describe, first, how the information
included in text is represented and parameterized, and second,
the training and testing processes for using the associative rela-
tional network with classification purposes in the corpus-based
TTS synthesis framework. Notice that, as it can be observed
from Section V, the ARN model can be used for classifying ei-
ther texts as short as 1 sentence or large paragraphs (e.g., of
30-sentence length).
1) Turning the ARN Model into a Vector-Based Classifier:
In order to make use of the information embedded in the ARN,
it is necessary to define a suitable model for conducting the
classification task on a set of categories . There are several
possibilities to exploit this information; however, up to now,
the TC system included in the corpus-based MD-TTS archi-
tecture represents the ARN contents on a vector space model
(VSM) [47], following the “Choose the best to modify the least”
corpus-based philosophy [48]. According to the general defini-
tion of the VSM, each document of the collection is repre-
sented as a vector of weights within the vector space built from
the term set [47]—see (2).
(2)
where is the total number of terms contained in the text
collection, and represents the weighting of term in
document .
Thus, each term defines a dimension of the multidimensional
vector space model of , where the documents of the
training collection are represented as vectors. Thanks to this
vector representation, the algebraic operators (e.g., vector
distances) will be applicable to conduct text classification, as
TABLE I
SYMBOLIC EXAMPLE OF DOMAIN PATTERN VECTORS ~p (ARN-F
D ; 1  n  jCj) AND TEXT TO BE CLASSIFIED t ACCORDING TO THE
GLOBAL ARN, GIVEN THREE DIFFERENT DOMAINS D ;D AND D .
SYMBOLS f! ; ! ; . . . ; ! g REPRESENT THE TERM AND
CO-OCCURRENCE WEIGHTS OF THE MODELED TEXTS
described afterwards. Moreover, notice that in the current ap-
proach, the dimensions of the VSM correspond to the thematic
and stylistic features extracted from text. Thus, the documents
will be represented according to (3), defined as a generaliza-
tion of (2), since it includes the co-occurrence weights of the
term collection that composes the ARN (see Fig. 4).
(3)
where represents the weighting of ordered
co-occurrences between terms and in document . The
multidimensional vector space defined in (2) becomes
, since it integrates all terms with their co-occur-
rences (i.e., each term can appear contiguously in text with the
rest of the terms and itself).
2) Training the ARN-Based Text Classifier: The training
process consists of building an ARN for each of the do-
mains contained in the corpus, which are composed of the
corresponding subset of training documents . In order
to obtain a consistent representation of data across all the
domains, a global ARN is firstly built from all the training
texts [see Fig. 5(a)]. Next, this global ARN is used as a ref-
erence for building each domain’s ARN, obtaining what we
have called Full ARN of the th domain (denoted as ARN-F
), as its components follow the order indicated
Authorized licensed use limited to: Agean University. Downloaded on October 16, 2008 at 12:48 from IEEE Xplore.  Restrictions apply.
1346 IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 16, NO. 7, SEPTEMBER 2008
by the global ARN [see Fig. 5(b)]. The training stage finishes
after deriving a vectorial representation of each ARN-F
yielding pattern vectors.5
3) Classifying the MD-TTS Input Texts: Given a text
input to the TTS system, it is first represented according to
the global ARN model derived in the training stage [see (3)],
obtaining its corresponding vector . Next,
this vector can be compared to each of the pattern vectors
through a similarity measure. Finally, the input text is assigned
to the domain(s) attaining the highest similarity. The com-
parison process can be done by simply computing a cosine
similarity distance between vectors [47]. Nevertheless, the
cosine classification similarity can be enriched by including a
multiplicative factor that takes into account the global structural
resemblance between the compared texts [see (4) and (5)]; thus,
exploiting higher order similarity features extracted from the
ARN model beyond the first-order adjacency in the text (i.e.,
co-occurrence frequency)
(4)
(5)
The pattern length (PL) is defined as the length of the longest
sequence of identical consecutive terms appearing in the same
order in the input text and each domain , after representing
them on the global common space ( and pattern vector ,
respectively) [18]
(6)
(7)
where is the number of terms of text is the index com-
puting the number of coincident consecutive co-occurrences be-
tween the text to be classified and the considered domain, that
is, is the co-occurrence frequency be-
tween terms and of input text and, finally, is its cor-
responding value within the pattern vector, which must exist.6
As a result, the PL only computes the co-occurrences appearing
both in the input text and the considered domain
, as indicated in (7).
Furthermore, if the pattern length is computed as the sum
of consecutive co-occurrences matching between the compared
vectors [see (7)], we obtain the cumulative PL (cPL) following
(8):
(8)
5Representing the information contained in the domain D according to the
ARN-F.
6The index sequence I = fi ; i ; . . . ; i g contains the position of
terms of text ~t referenced to their position on the global ARN.
As it can be observed from (6) and (8), both structural sim-
ilarity parameters are normalized by the total number of terms
; thus, so as to avoid fictitiously bi-
asing their value due to input text length. Please note that in the
specials case when , both PL and cPL will be assigned
a zero value—see Appendix B for a toy example showing PL
and cPL computation.
4) Reduced ARN Model: Due to the extremely high dimen-
sionality of the global representation space,7 the classification
of each input text is a computationally demanding task, since
it requires going through the whole global ARN before con-
ducting classification. Moreover, the vector representation of
will be typically very sparse, which results in a reduction of the
separability properties of the pattern vectors, yielding poorer
text classification efficiency [49]. In order to improve domain
separability and minimize the computational cost of the classifi-
cation task, a second ARN-based strategy called Reduced ARN
(ARN-R) is introduced.
The main idea of the ARN-R model is based on the substitu-
tion of the full comparison space (built from the global ARN) by
the VSM derived from the ARN generated from the input text
. Hence, during the classification stage, each domain is rep-
resented according to the ARN-R before conducting the com-
parison in order to obtain a common representation space. That
is, the domain ARN building process depicted in Fig. 5 is now
conducted by substituting the global ARN by the ARN gen-
erated from the input text . In this sense, the computational
complexity of representing on the global ARN space is sub-
stituted by the cost of representing each domain in the ARN-R
space, which in general will be much lower.
Obviously, the ARN-R is just an approximation of the
complete training data representation provided by the ARN-F,
as the ARN-R misses most of the information stored in the
full space generated from the training documents
. Anyhow, it can be algebraically proved that the ARN-R is
close to the best possible approximation of the ARN-F on the
input text space in the least mean square sense, as described in
Appendix A.
V. EXPERIMENTS
The experiments have been conducted on a 2.5 h Spanish
multidomain speech corpus recorded by a female professional
speaker. The speech corpus is composed of 2590 sentences
extracted from an advertising database, which are grouped into
three different domains following a tiering approach: educa-
tion-training (916 sentences), technology (833 sentences), and
cosmetics (841 sentences). Each domain was recorded using
a predefined speaking style according to [50]: happy-elation
7In this approach, the training texts are fully represented, without stop listing
or stemming, and the co-occurrences are also included, obtaining very large
vectors.
Authorized licensed use limited to: Agean University. Downloaded on October 16, 2008 at 12:48 from IEEE Xplore.  Restrictions apply.
ALÍAS et al.: TOWARDS HIGH-QUALITY NEXT-GENERATION TTS SYNTHESIS 1347
(HAP), neutral-mature (NEU), and sensual-sweet8 (SEN),
respectively. Thanks to the correspondence defined in [50]
between speaking styles and domain contents, the automatic
text classification module is able to select the most appropriate
speaking style from text.
The analyzed text classification algorithms are trained on the
80% of corpus sentences and tested on the remaining ones fol-
lowing a tenfold random subsampling strategy to obtain statisti-
cally reliable results. In order to evaluate the performance of the
TC algorithms in terms of classification efficiency (computed by
the classic measure, the harmonic mean of precision and re-
call [23]), the labeled sentences have been randomly grouped
into pseudo-documents (hereafter, documents). This is done to
evaluate the performance of the proposed text classifiers when
the number of sentences per document decreases, moving from
a standard TC task (with many sentences per document) to a
typical TTS scenario, with only one sentence per document. To
that effect, a sweep ranging from 1 to 30 sentences per docu-
ment is conducted.
Moreover, in its current implementation, the unit selection
module is adjusted to extract the longest sequence of consecu-
tive units from the domain indicated by the text classifier, using
a simple cost function [10]. The target prosody (pitch, dura-
tion, and energy) is predicted by the natural language processing
module following the approach described in [12].
A. Baseline Method Selection
The goal of this first experiment is to select a baseline TC al-
gorithm as a reference to validate the performance of the ARN-
based TC proposals. In the context of thematic TC, support
vector machines (SVMs) are regarded as the best performing
classifiers [23]. However, as the TC in the implemented tiering
corpus-based MD-TTS system is only trained with the texts cor-
responding to the recorded speech, SVM becomes an unsuitable
option to implement the domain classification module. This is
due to the unbalance between the high dimensionality of the
feature space where MD-TTS text classification is conducted
and the comparatively much smaller size of
the training document collections in this context. When there are
not enough examples to represent the training space accurately,
SVMs decrease their performance dramatically, being even un-
able to operate [51], which it was what happened in the informal
experiment conducted using the SVM software [52]. For
instance, if linear kernels are to be used, examples are
needed to properly model the -dimensional feature space [53],
while in our experiments and the
number of examples is .
As a consequence of the aforementioned argument, we
looked for other TC strategies than SVM, covering different
approaches as a baseline for solving this classification problem.
First, a basic nearest-neighbor (NN) classifier using TFIDF
weighted terms as features is analyzed [23]. This technique is
based on representing each document as a vector in a VSM built
from the training set. At classification time, each test document
8It is a warm, soft, and pleasant speaking style with some whispering nature.
Fig. 6. Classification efficiency of the analyzed baseline methods across the
sentences per document sweep.
is assigned to the category of the most similar training docu-
ment, according to a cosine distance. Second, a probabilistic
TC algorithm based on bigrams is also analyzed. The first idea
was to represent each domain by its own probabilistic language
model obtained from the word pairs distribution across the doc-
uments of that domain. However, it was necessary to substitute
words by characters, as in [54], due to the low statistical robust-
ness of word-based probabilistic language models caused by
the small size of the training collection (a problem equivalent
to the one affecting SVM9). The input text is assigned to the
domain attaining the highest membership probability. Finally,
an ICA-based TC is applied to the problem. This technique,
which considers topics as latent random variables, makes use
of term extraction for better thematic identification (a latent
semantic space is built from the independent components that
constitute the basis of the information represented in the text).
In previous works, the ICA-based TC has been successfully
applied for semi-supervised text classification and hierarchiza-
tion of document corpora, by identifying the correspondence
between text independent components and domains [39].
Fig. 6 depicts the performance of the analyzed baseline
methods, in terms of average , across the conducted sweep of
sentences per document. It can be observed that the NN method
shows the best global behavior, followed by the probabilistic
TC, whereas ICA-based TC suffers a rapid worsening due to
the fact that this is a predominantly thematic approach (the
smaller the size of the documents, the more difficult the extrac-
tion of latent topics becomes). Hence, the NN classifier was
selected as the baseline state-of-the art method for validating
the ARN-based proposals.
B. Objective Performance of the ARN-Based Proposals
The following paragraphs analyze the performance of the
proposed ARN-based text classifiers denoted as ARN-F and
ARN-R, respectively. To that effect, four different text parame-
terizations are considered. We compare the influence of using
TFIDF versus ITF as thematic features, besides considering
9Both issues may be tackled by considering larger texts collections than the
recorded ones, but this issue is left for future work.
Authorized licensed use limited to: Agean University. Downloaded on October 16, 2008 at 12:48 from IEEE Xplore.  Restrictions apply.
1348 IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 16, NO. 7, SEPTEMBER 2008
Fig. 7. Classification efficiency of the ARN-based and NN TC methods across the sentences per document sweep for different text parameterizations. (a) ARN-F
versus NN. (b) ARN-F NCOF versus ARN-R.
structural information by including co-occurrence frequencies
(COF) or not (NCOF) in the vectors built from the graph-based
ARN. Moreover, we also analyze the impact of using similarity
measures which incorporate structural information by means
of pattern length (PL) and its cumulative version (cPL) [see
(6) to (8)].
1) Text Parameterization: Fig. 7 presents the classification
efficiency results of the compared TC techniques across the pre-
defined sweep, using the cosine distance as the similarity mea-
sure. It can be observed that the ARN-based methods achieve
better results than the baseline NN classifier for any text pa-
rameterization when classifying one sentence long documents,
which is the most demanding classification scenario. Notice this
is also the case in the most part of the sweep. However, both
global representation methods (ARN-F and NN) are negatively
affected by the inclusion of COF—due the dramatic increase
of the feature space dimensionality— achieving their optimal
performance for the TFIDF NCOF parameterization. In con-
trast, ARN-R even experiences a slight performance improve-
ment when COF is considered. Moreover, ARN-R achieves its
optimal performance when ITF is selected as the thematic fea-
ture—with very similar results between COF and NCOF param-
eterizations. As a conclusion, it can be stated that ARN-R, de-
spite being an approximation of ARN-F, behaves more robustly
in terms of the parameterization employed besides achieving
equal or slightly better classification results in every step of the
sweep (in particular, ARN-R is the best classifier in the hardest
categorization scenario, i.e., with one sentence per document,
attaining an average ). Finally, notice that all TC ap-
proaches suffer from the decrease of sentences per document,
which highlights the importance of finding a TC tuned to solve
the domain classification task within the TTS synthesis frame-
work satisfactorily.
2) Similarity Measures: Fig. 8 presents a global compar-
ison regarding the use of stylistically weighted similarity mea-
sures for both ARN-based text classifiers. The ARN-F based
text classifier experiences a notable improvement when the co-
Fig. 8. Averaged classification efficiency of TC methods across the sentences
per document sweep of Fig. 7 for different similarity measures.
sine distance is enriched with PL and cPL weightings, attaining
an average relative improvement of 14.2% and 19% on , re-
spectively. On the contrary, ARN-R is nearly unaffected by the
inclusion of these factors in the similarity measure. As a con-
clusion, the structural weighting of the cosine distance affects
the ARN-F-based TC positively, since it makes up for the dra-
matic increase of vectors length (each domain is represented by
means of a large single vector), whereas this effect is less clear
for the ARN-R classifier, since the managed vectors are of lower
dimensionality.
C. Subjective Results of the MD-TTS System
As the final goal of introducing a text classifier into the
MD-TTS system architecture is achieving high-quality syn-
thetic speech besides improving system flexibility, several
listening preference tests were conducted in order to validate
its naturalness subjectively. These experiments are intended
to analyze the influence of correct and wrong domain clas-
sification decisions on the synthetic speech quality obtained
by the MD-TTS approach when classifying texts as short as
Authorized licensed use limited to: Agean University. Downloaded on October 16, 2008 at 12:48 from IEEE Xplore.  Restrictions apply.
ALÍAS et al.: TOWARDS HIGH-QUALITY NEXT-GENERATION TTS SYNTHESIS 1349
Fig. 9. Distribution of user preferences between synthetic results from correct
domain and wrong domain classifications according to the manual labeling, in-
cluding the indistinct votes. (a) Comparing the correct classifications of happy
and sensual domains versus neutral domain syntheses, and misclassifications
(yielding to wrong syntheses) versus manually labeled domain syntheses (right-
most bar plot), indicating the 95% confidence levels. (b) Detail of the 12 happy
sentences votes. (c) Detail of the 15 sensual sentences votes.
one sentence. The correctness of the text classifier decisions is
evaluated by taking into account the manual labels assigned
to each document (the so called ground truth). Since the im-
plemented MD-TTS follows a tiering speech corpus typology,
the TTS system selects both the target prosody pattern and the
subcorpus according to the automatically assigned domain.
The evaluators (24 members of our University) were asked to
select, by means of a web interface, the most appropriate/natural
version (according to the sentence meaning) between two ran-
domly ordered synthetic results obtained from the same input
sentence. The evaluators were able to 1) listen to the generated
files as many times as needed before taking a decision, and 2) se-
lect an indistinct option when they could not decide between the
synthetic versions compared (equally good or bad). For this ex-
periment, the ARN-R based text classifier was used, since it at-
tained the best performance among the analyzed TC approaches
at one sentence level.
1) Subjective Evaluation of Correct Domain Classifications:
The first preference test analyzes the achieved results when the
correct domain (according to the ground truth) is chosen. As a
MD-TTS system which makes correct decisions is essentially a
LD-TTS system in terms of speech quality, comparing its results
to the ones obtained from the neutral domain is somehow equiv-
alent to comparing LD-TTS to GP-TTS.10 Both correct and neu-
tral domain syntheses make use of the prosodic pattern corre-
sponding to its speaking style. The test was conducted on 27
correctly classified sentences (12 happy and 15 sensual), which
were selected by applying a simple greedy algorithm tuned to
obtain phonetically balanced sentences.
The results indicate a significant preference for the cor-
rectly classified domain outcomes over the reference neutral
syntheses, for both happy and sensual domains [see the two
left-most bar plots in Fig. 9(a)]. Moreover, the test on the
sensual domain reveals higher preference for the correct do-
main syntheses compared to the happy results (75.8% and
71.5%, respectively), besides reducing the preferences to the
neutral synthesis (17.2% and 24.3%, respectively). As it can
be observed from Fig. 9(b) and (c), this is due to the fact that
some sentences from the happy domain are preferred when
synthesized in a neutral style, e.g., sentences 4 (“Libro de la
competición 93”) and 7 (“En teoría, una escuela de nego-
cios”)—“93 competition’s book” and “In theory, a business
school,” in English.
2) Subjective Evaluation of Wrong Domain Classifications:
The second test evaluates the perceptual impact of wrong
automatic text classifications with respect to the ground truth.
Hence, this experiment is equivalent to comparing worst-case
MD-TTS to LD-TTS synthesis, besides validating if the depen-
dence between the style of delivery and the assigned domain
is relevant in these wrong classification cases. To that effect,
nine sentences misclassified by the automatic text classification
module (listed in Table II) were presented to the evaluators.
As it can be observed from the right-most bar plot in Fig. 9(a),
there is also a significant preference for the manually labeled
domain results (65.8%) versus the syntheses coming from
the incorrectly classified ones (29.1%), labeled as correct and
wrong in the figure, respectively. However, the preference gap
(36.8%) is significantly lower than in the previous tests (47.2%
and 58.6% for the happy and sensual domains), showing a
10It is worth noting that the technology domain is not large enough to be
properly called as a reliable general purpose speech corpus, however, it is used
as reference regarding to what could be achieved by general purpose synthesis.
Authorized licensed use limited to: Agean University. Downloaded on October 16, 2008 at 12:48 from IEEE Xplore.  Restrictions apply.
1350 IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 16, NO. 7, SEPTEMBER 2008
TABLE II
LIST OF WRONGLY CLASSIFIED SENTENCES EXTRACTED FROM
THE SPEECH CORPUS USED TO VALIDATE THE SYNTHETIC SPEECH
QUALITY OF THE TIERING MD-TTS PROPOSAL
higher trend to select the syntheses obtained from the wrongly
classified domain. Moreover, according to the users’ feedback,
this experiment involved the most difficult elections (e.g., a
larger number of turns were needed before deciding).
Hence, the smaller preference gap, besides the much less
clear preference pattern across evaluators (already observed in
[55]11), somehow correlate with the automatic domain misclas-
sifications, which mostly occur in sentences with no clear do-
main membership (e.g., “Soluciones a medida”—“Tailor-made
solutions,” in English). However, there is still room for improve-
ment to avoid misclassifications of sentences like “Pero no se
pueden sustraer al perfume” (last sentence in Table II), which
contains the word perfume, indicating its membership to the cos-
metics domain, or sentences 4 and 7 of the happy domain [see
Fig. 9(b)]. Therefore, it seems necessary to include some deeper
semantic analysis in future approaches.
VI. DISCUSSION
In this paper, the MD-TTS system has been implemented fol-
lowing a tiering speech corpus typology. As it is well known, the
tiering approach is very costly, since each new speaking style the
system is asked to synthesize requires the design and recording
of its corresponding speech corpus. Nevertheless, the concept
behind MD-TTS synthesis can be exported to other speech syn-
thesis strategies and corpus typologies, since our final goal is
improving the synthesis flexibility besides obtaining high syn-
thetic speech quality. To that effect, the MD-TTS philosophy
allows selecting the most appropriate synthesis configuration
11We detected a slight (but not statistically significant) trend to select neutral
syntheses when coming from wrong classifications (four sentences of Table II).
Hence, in future experiments we want to compare general purpose TTS versus
the MD-TTS approach to analyze the ill effects caused by the misclassifications
more exhaustively.
(technique, corpus typology, signal processing, etc.) for a partic-
ular speaking style. The following paragraphs discuss the porta-
bility of the proposal and the quality of the obtained synthetic
results.
One of the key elements of the MD-TTS approach is the
flexibility of the introduced system architecture. First, notice
that this architecture allows conducting: 1) general-purpose TTS
synthesis (with a single generic speech corpus), 2) limited do-
main TTS synthesis (with a single restricted domain corpus),
and 3) multidomain TTS synthesis with different domains (with
flat or hierarchical corpus structure, depending on the domains’
contents and their acoustic characteristics). Moreover, these do-
mains can be explicitly incorporated as independent subcorpora
(e.g., [41], [42]), or as small appendices completing a generic
purpose corpus (e.g., [15], [21], [38], [40]), or even as a result
of dividing the corpus (with the same acoustic characteristics)
in different subdomains (e.g., journalistic texts: politics, society,
culture, [39]). Second, the introduced architecture may be
implemented by means of different synthesis strategies, such as:
1) corpus-based techniques (e.g., the tiering corpus depicted in
Fig. 3 can be extended as desired, provided that each subdo-
main is large enough to conduct unit selection), 2) HMM-based
synthesis (e.g., with domain-dependent statistical models) (see
[45], [5] and related works), or 3) hybrid solutions (e.g., see [6]).
Finally, we would like to note that the architecture makes it pos-
sible to use other speech corpus typologies than tiering (e.g.,
blending or mixed approaches), thus, allowing the use of all the
speech units during the unit selection process if necessary [15],
[19], [21].
Following the same idea of flexibility, the directed
word-based ARN model allows conducting: 1) thematic
classification, by only considering key words (after stop listing
and stemming)—hence, turning the graph-based architecture
into a bag-of-words approach—2) stylistic classification, like
authorship attribution or genre detection, by including the
appropriate features in the model, or 3) domain classification
of texts as short as one sentence, like in the current approach.
Thus, the ARN model can somehow be regarded as a generic
text representation that includes all terms and their order in the
text, allowing the most appropriate term weighting according
to the target task. In this context, higher order structural rela-
tionships other than the ones introduced in this work (based on
co-occurrence word frequencies) may be considered in future
works. Furthermore, we want to point out that updating the
ARN is very easy. If new training texts are to be incorporated to
the document collection, 1) the full ARN only needs to update
the global ARN model with the new texts besides rebuilding
the domain ARNs, and 2) the reduced ARN only needs to build
the ARNs of the new domains, as the space of classification is
defined by the input text.
As pointed out throughout this paper, the synthesis process
of the implemented MD-TTS system is based on the direct cor-
respondence between domain contents and speaking styles es-
tablished by [50]. Thanks to this relationship, the TTS system is
capable of delivering the message with the appropriate speaking
style in most cases (i.e., when the input text is assigned to the
Authorized licensed use limited to: Agean University. Downloaded on October 16, 2008 at 12:48 from IEEE Xplore.  Restrictions apply.
ALÍAS et al.: TOWARDS HIGH-QUALITY NEXT-GENERATION TTS SYNTHESIS 1351
correct domain), yielding a performance equivalent to that of
LD-TTS synthesis systems.
As regards the synthetic speech quality, we would like to point
out that the mismatch between prosodic features and acoustic
segments causes notable quality degradation when comparing
current results to the ones presented in [55]. In that work, the
prosodic pattern was set to fit the characteristics of the happy
and sensual domains even when the neutral speech corpus was
selected when evaluating correct classification synthetic results.
As a result the listening tests presented an overwhelming pref-
erence for the correct classification results compared to the ref-
erence ones.
VII. CONCLUSION
Next-generation high-quality text-to-speech synthesis sys-
tems are not only asked to generate high-quality synthetic
speech (like limited-domain approaches) but also to be flexible
enough to adapt to any application needs or speech signal char-
acteristics, besides being capable of delivering natural (e.g.,
spontaneous or expressive) speech. This paper has introduced
our proposal towards improving the flexibility of high quality
TTS systems by considering multiple domains, named multido-
main TTS synthesis. This proposal belongs to a recent research
direction focused on incorporating deeper text analysis for
taking TTS systems a little closer to human behavior—which
often includes humor changes, different speaking styles, etc.,
within the same conversation. In that sense, the MD-TTS ap-
proach follows a counterpart evolution to multidomain spoken
language systems, borrowing the idea of automatic domain
assignment from deeper analysis of the input data—leaving
paralinguistic and extralinguistic issues, which are defined by
the context of the conversation, for further research.
Therefore, the MD-TTS approach can constitute a generic
framework for developing any kind of application involving
speech synthesis, by including or not (depending on the ap-
plication requirements) the automatic domain assignment of
the input text. In that sense, the introduced architecture allows
changing the point of view when developing a TTS system: in-
stead of predefining a synthesis strategy plus a corpus typology
so as to meet the application requirements, the flexibility of
the MD-TTS architecture allows adapting the TTS system
modules to the acoustic needs of each desired output speaking
style—e.g., from speaking styles with a particular voice quality
(e.g., the whispering nature of the sensual domain) that need
to be explicitly recorded to be properly delivered [22], [44], to
styles realistically generated through signal processing modifi-
cations, such as good or bad news synthetic messages that can
be obtained from a general purpose corpus [21].
Moreover, the described ARN-based automatic text classifi-
cation proposal tackles satisfactorily the problem of classifying
texts as short as one sentence, by taking into account both the-
matic and structural features of text after representing it on a
graph-based model including all words and punctuation marks.
It is important to note that, to date, the proposal for conducting
text classification only takes into account the raw input text
without including external semantic knowledge (e.g., WordNet
[16]), an issue that is left for future investigations. Moreover, the
training text database corresponds to the texts of the recorded
speech corpus, since the MD-TTS was implemented following
a tiering corpus-based approach. However, we are currently con-
sidering the possibility of enlarging the training collection with
texts not included in the speech corpus so as to generalize the
domain classification process (an issue that will become more
interesting as the employed synthesis strategy becomes more
flexible, besides reducing the out-of-vocabulary problem at the
same time—see Appendix A). Furthermore, the current imple-
mentation of the text classification module is being optimized
towards reducing its computational cost and improving its clas-
sification efficiency when classifying input texts shorter than
one sentence, besides studying its applicability to other text
classification tasks. Finally, in future works we want to explore
techniques capable of inferring the speaking style (e.g., user atti-
tude or emotion) directly from text, without resorting to the cor-
respondence between domains and speaking styles, which has
been the basis of the current implementation of the MD-TTS
proposal.
In terms of the synthetic speech quality, the conducted sub-
jective experiments show a nice correlation between evaluators’
preferences and TC assignments, validating the performance of
the ARN-based TC perceptually. Specifically, the collected sub-
jective results reveal that, when MD-TTS works properly (i.e.,
it is equivalent to LD-TTS), users significantly prefer MD-TTS
synthesis results to general-purpose equivalent syntheses, like
in [21], [38], and [40]. Moreover, when MD-TTS assigns the
input sentence to a domain other than the one it was originally
recorded (i.e., wrong domain classification), evaluators showed
lower (though still significant) preference for the results synthe-
sized from the manually labeled domain, as misclassifications
mainly occur on texts the meaning of which does not convey a
clear domain membership. In addition to the presented exper-
iments, we also compared the synthesis from the correctly as-
signed domain to the general-purpose TTS with all the domains
gathered in a single database. However, no significant differ-
ences were observed between the set of units retrieved from the
whole database and those selected from the assigned domain,
since only the longest path of units has been considered as the
cost function in the current experiments—thus making this ex-
periment meaningless in current conditions. Nevertheless, we
are planning to conduct new experiments after grouping the do-
mains into a common database (i.e., simultaneously using mul-
tidomain speech databases as a single speech database) after fin-
ishing our current research on reliable subjective cost function
weight tuning [56].
APPENDIX I
ALGEBRAIC JUSTIFICATION OF THE ARN-R MODEL
In this section, the ARN-R approach is presented as the
best ARN-F approximation (in the least mean square sense)
by means of algebraic arguments. The ARN-F model can be
represented in a real vector space , which is built
Authorized licensed use limited to: Agean University. Downloaded on October 16, 2008 at 12:48 from IEEE Xplore.  Restrictions apply.
1352 IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 16, NO. 7, SEPTEMBER 2008
Fig. 10. ARN-based text representations for a reference domain D and a text t to be classified in that domain, considering ARN  F = ARN D. “”
represents empty nodes and dashed lines symbolize inexistent co-occurrences. (a) ARN-D built from text “The weather in Barcelona is fantastic.” (b) Input text
t =“The weather is fantastic,” represented according to ARN-D of figure (a).
from the training documents collection . In this vector space,
the pattern vectors representing each domain
and the vectors modeling the text to be classified—with
active and null components,12 where
—are represented (see the example of
Table I). Following the same idea, the ARN-R can also be
considered to be defined in a real vector space , where
and . The last inequality stands
for terms of which are not represented in the global ARN
(i.e., out-of-vocabulary words (OOV).13)
Moreover, within the vector space defined by the global ARN,
it is possible to define a vector subspace gen-
erated by a vector basis composed of
orthonormal vectors defined by the no null components of
vector (see Table IV). By making use of this basis , the pat-
tern vector can be optimally approximated in the subspace
as , in terms of the minimum square error, by means of simple
orthogonal projection (see (9))
(9)
Any other projection (nonorthogonal) of vectors on the
subspace , will achieve a higher approximation error—com-
puted as the Euclidean norm [57].
Notice the relationship between the data representation on
vector space defined by the ARN-R—i.e., —(see
Table III) and the one obtained when projecting the informa-
tion on the vector subspace defined by the
global ARN (see Table IV). It can be observed that using the
ARN-R strategy is equivalent to approximate the domain pat-
tern vectors on the vector subspace with the minimum square
error. Moreover, besides the change on the order of the vector
components—which does not affect the distance computation
results—there is a subtle difference of null com-
ponents within the pattern vectors due to the OOV words con-
tained in the text to be classified. However, these null cells, on
one hand, do not affect the result of the dot product of vectors
and , and on the other hand, will affect uniformly all the
computations and comparisons when using the cosine distance
through the vector norm of .
12L is the number of parameters (related to words, punctuation marks,
co-occurrences, etc.) representing the input text, and M is the resulting
number of parameters after representing it in the global ARN.
13As in any other deterministic machine learning based process, whatever not
seen during the training process is not considered for classification purposes.
However, this is not a critical issue of our approach according to objective ex-
perimental results.
TABLE III
DATA REPRESENTATION OF TABLE I CONTENTS WITHIN THE VSM ON SPACE
DEFINED BY THE ARN-R BUILT FROM THE TEXT TO BE CLASSIFIED t
TABLE IV
DATA REPRESENTATION OF TABLE I EXAMPLE ACCORDING TO VECTOR
SUBSPACE V CREATED FROM THE ORTHONORMAL BASIS B = f~b ;~b g
DEFINED BY THE M = 2 ACTIVE COMPONENTS OF ~t ,
REPRESENTED ACCORDING TO THE GLOBAL VSM
Nevertheless, it is important to note that the ARN-R approach
implies losing a certain amount of information contained in the
global representation of pattern vectors of ARN-F , which
will affect the similarity computation through the vector norm
values. However, as it can be observed in the experiments de-
scribed in Section V, this problem does not have a clear im-
pact on the achieved results. Nevertheless, we shall continue
studying the particularities of the ARN-R approach in future
experiments.
APPENDIX II
STRUCTURAL FEATURES: PL AND CPL COMPUTATION
Fig. 10 presents a toy example of the ARN-based text repre-
sentation of a hypothetical domain built from a single sen-
tence and a similar input text to be classified. As it can be
noticed, the terms not observed during the training process are
not represented in the ARN built from the text , since it is the
ARN-D which defines the comparison space (i.e., in this ex-
ample, ). For instance, the connection
“Barcelona-is” of Fig. 10(b) is null, i.e., , whereas
“weather-is” does not exist in , i.e., .
Authorized licensed use limited to: Agean University. Downloaded on October 16, 2008 at 12:48 from IEEE Xplore.  Restrictions apply.
ALÍAS et al.: TOWARDS HIGH-QUALITY NEXT-GENERATION TTS SYNTHESIS 1353
TABLE V
COMPUTATION OF PL AND CPL OF TEXT OF FIG. 10(b), GIVEN AN INDEX
SEQUENCE I = f1; 2; 5; 6; 7g, YIELDING ! = f! ; ! ; ! ; ! g,
OBTAINED WHEN INDEXING THE FIVE-TERMS TEXT REFERRED TO THE
DOMAIN ARN OF FIG. 10(a) AFTER BEING VECTORIZED AS ~p
Table V shows the computation of PL and cPL for the ex-
ample depicted in Fig. 10 (with ), according to (6) to
(8). The index vector contains the positions of text terms ref-
erenced to the ARN model (e.g., terms 3 and 4 are not included
in text, thus, their indexes are not considered). As a result, since
the largest set of consecutive terms is two, ;
however, thanks to considering there are
two sets of consecutive terms. Finally, notice that has been
omitted since .
ACKNOWLEDGMENT
The authors would like to thank P. Barnola, D. García,
I. Iriondo, J. A. Montero, and O. Guasch for their help and also
all participants involved in the subjective experiments.
REFERENCES
[1] Y. Sagisaka, “Speech synthesis by rule using an optimal selection of
non-uniform synthesis units,” in Proc. ICASSP, New York, 1988, pp.
679–682.
[2] A. Black and P. Taylor, “Automatically clustering similar units for unit
selection in speech synthesis,” in Proc. EuroSpeech, Rhodes, Greece,
1997, pp. 601–604.
[3] M. Ostendorf and I. Bulyko, “The impact of speech recognition on
speech synthesis,” in Proc. IEEE Workshop Speech Synthesis, Santa
Monica, 2002, pp. 99–106.
[4] H. Zen and T. Toda, “An overview of Nitech HMM based speech
synthesis system for Blizzard Challenge 2005,” in Proc. InterSpeech,
Lisbon, Portugal, 2005, pp. 93–96.
[5] J. Yamagishi and T. Kobayashi, “Average-voice-based speech synthesis
using HSMM-based speaker adaptation and adaptive training,” IEICE
Trans. Inf. Syst., vol. E90D, no. 2, pp. 533–543, Feb. 2007.
[6] A. Black, H. Zen, and K. Tokuda, “Statistical parametric speech syn-
thesis,” in Proc. ICASSP, Honolulu, HI, 2007, vol. IV, pp. 1229–1232.
[7] J. Yi and J. Glass, “Natural-sounding speech synthesis using variable-
length units,” in Proc. ICSLP, Sydney, Australia, 1998, pp. 1167–1170.
[8] P. Taylor, “Concept-to-speech synthesis by phonological structure
matching,” Philosophical Trans. R. Soc., Series A, vol. 356, no. 1769,
pp. 1403–1416, 2000.
[9] A. Black and K. Lenzo, “Limited domain synthesis,” in Proc. ICSLP,
Beijing, China, 2000, vol. 2, pp. 411–414.
[10] F. Alías, I. Iriondo, L. Formiga, X. Gonzalvo, C. Monzo, and X.
Sevillano, “High quality Spanish restricted-domain TTS oriented to a
weather forecast application,” in Proc. InterSpeech, Lisbon, Portugal,
2005, pp. 2573–2576.
[11] G. Bailly, N. Campbell, and B. Möbius, “ISCA special session: Hot
topics in speech synthesis,” in Proc. EuroSpeech, Geneva, Switzerland,
2003, pp. 37–40.
[12] I. Iriondo, F. Alías, and J. Socoró, “Prosody modelling of Spanish for
expressive speech synthesis,” in Proc. ICASSP, Honolulu, HI, 2007,
vol. IV, pp. 821–824.
[13] S. Sundaram and S. Narayanan, “An empirical text transformation
method for spontaneous speech synthesizers,” in Proc. EuroSpeech,
Geneve, Switzerland, 2003, vol. 2, pp. 1221–1224.
[14] Y. Sagisaka, T. Yamashita, and Y. Kokenawa, “Generation and percep-
tion of F markedness for communicative speech synthesis,” Speech
Commun., vol. 46, no. I, pp. 376–384, 2005.
[15] G. Holer, K. Richmond, and R. Clark, “Informed blending of databases
for emotional speech synthesis,” in Proc. InterSpeech, Lisbon, Por-
tugal, 2005, pp. 501–504.
[16] C. Ovesdotter, D. Rolh, and R. Sproat, “Emotions from text: Machine
learning for text-based emotion prediction,” in Proc. HLT/EMNLP,
Vancouver, BC, Canada, 2005, pp. 579–586.
[17] V. Francisco and P. Gervás, “Automated mark up of affective infor-
mation in English texts,” Lecture Notes in Comput. Sci., no. 4188, pp.
375–382, Sep. 2006.
[18] F. Alías, I. Iriondo, and P. Barnola, “Multi-domain text classification
for unit selection text-to-speech synthesis,” in Proc. 15th Int. Congr.
Phonetic Sci. (ICPhS), Barcelona, Spain, 2003, pp. 2341–2344.
[19] F. Campillo and E. R. Banga, “A method for combining intonation
modelling and speech unit selection in corpus-based speech synthesis
systems,” Speech Commun., vol. 48, no. 8, pp. 941–956, Aug. 2006.
[20] W. Johnson, S. Narayanan, R. Whitney, R. Das, M. Bulut, and C. La-
Bore, “Limited domain synthesis of expressive military speech for an-
imated characters,” in Proc. IEEE Workshop Speech Synthesis, Santa
Monica, CA, 2002, pp. 163–166.
[21] W. Hamza, R. Bakis, E. M. Hide, M. A. Picheny, and J. F. Pitrelli, “The
IBM expressive speech synthesis system,” in Proc. ICSLP, Jeju Island,
Korea, 2004, pp. 2577–2580.
[22] O. Turk, M. Schröder, B. Bozkurt, and L. Arslan, “Voice quality inter-
polation for emotional text-to-speech synthesis,” in Proc. InterSpeech,
Lisbon, Portugal, 2005, pp. 797–800.
[23] F. Sebastiani, “Machine learning in automated text categorisation,”
ACM Comput. Surveys, vol. 34, no. 1, pp. 1–47, 2002.
[24] F. Sebastiani, “Text categorization,” in Text Mining and its Applica-
tions, A. Zanasi, Ed. Southampton, U.K.: WIT Press, 2005, ch. 4,
pp. 109–129.
[25] E. Stamatatos, G. Kokkinakis, and N. Fakotakis, “Automatic text cat-
egorization in terms of genre and author,” Comput. Linguist., vol. 26,
no. 4, pp. 471–495, 2000.
[26] D. Pérez-Piñar and C. García, “Application of confidence measures for
dialogue systems through the use of parallel speech recognizers,” in
Proc. InterSpeech, Lisbon, Portugal, 2005, pp. 2785–2788.
[27] K. Rüggenmann and I. Gurevych, “Assigning domains to speech recog-
nition hypotheses,” in Proc. HLT-NAACL Workshop Spoken Lang. Un-
derstanding for Conversational Syst. and Higher Level Linguist. Inf.
for Speech Process., Boston, MA, 2004, pp. 70–77.
[28] I. Lane, T. Kawahara, T. Matsui, and S. Nakamura, “Dialogue speech
recognition by combining hierarchical topic classification and lan-
guage model switching,” IEICE Trans. Inf. Syst., vol. E88D, no. 3, pp.
446–454, 2005.
[29] J. Allan, “Perspectives on information retrieval and speech,” in Lecture
Notes in Computer Sci. (Workshop on Inf. Retrieval Tech. Speech Ap-
plicat.), 2001, vol. 2273, pp. 1–10.
[30] K. Asami, T. Takezawa, and G. Kikui, “Topic detection of an utterance
for speech dialogue processing,” in Proc. ICSLP, Denver, USA, 2002,
pp. 1977–1980.
[31] J. Bellegarda, “Statistical language model adaptation: Review and per-
spectives,” Speech Commun., vol. 42, no. 1, pp. 93–108, 2004.
[32] J. Diéguez, C. García, and A. Cardenal, “Effective topic-tree based lan-
guage model adaptation,” in Proc. InterSpeech, Lisbon, Portugal, 2005,
pp. 1289–1292.
[33] Y. Aiikita and T. Kawahara, “Language model adaptation based on
PLSA of topics and speakers,” in Proc. ICSLP, Jeju Island, Korea,
2004, pp. 1045–1048.
[34] F. Sugimoto, K. Yazu, M. Murakami, and M. Yoneyama, “A method to
classify emotional expressions of text and synthesize speech,” in Proc.
1st Int. Symp. Control, Commun. Signal Process., Hamrnamet, Tunisia,
2004, pp. 611–614.
[35] J. Tao and T. Tan, “Emotional Chinese talking head system,” in Proc.
the 6th Int. Conf. Multimodal Interfaces (ICMI), State College, PA,
2004, pp. 273–280.
[36] Z.-J. Chuang and C.-H. Wu, “Emotion recognition from textual input
using an emotional semantic network,” in Proc. ICSLP, Denver, CO,
2002, pp. 2033–2036.
[37] H. Liu, H. Lieberman, and T. Selker, “A model of textual affect sensing
using real-world knowledge,” in Proc. 8th Int. Conf. Intell. User Inter-
faces, Miami, FL, 2003, pp. 125–132.
[38] M. Chu, C. Li, P. Hu, and E. Cahng, “Domain adaptation for TTS sys-
tems,” in Proc. ICASSP, Orlando, FL, 2002, pp. 453–456.
Authorized licensed use limited to: Agean University. Downloaded on October 16, 2008 at 12:48 from IEEE Xplore.  Restrictions apply.
1354 IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 16, NO. 7, SEPTEMBER 2008
[39] X. Sevillano, F. Alías, and J. Socoró, “ICA-based hierarchical text clas-
sification for multi-domain text-to-speech synthesis,” in Proc. ICASSP,
Montreal, QC, Canada, 2004, vol. 5, pp. 697–700.
[40] V. Fischer, J. Botella, and S. Kunzmann, “Domain adaptation methods
in the IBM trainable text-to-speech system,” in Proc.ICSLP, Jeju Is-
land, Korea, 2004, pp. 1165–1168.
[41] A. Black, “Unit Selection and Emotional Speech,” in Proc. Eu-
roSpeech, Geneve, Switzerland, 2003, pp. 1649–1652.
[42] A. Lida, N. Campbell, F. Higuchi, and M. Yasumura, “A corpus-based
speech synthesis system with emotion,” Speech Commun., vol. 40, no.
1,2, pp. 161–187, 2003.
[43] A. Black, “Perfect synthesis for all of the people all of the time,” in
Proc. IEEE Workshop Speech Synthesis, Santa Monica, CA, 2002, pp.
167–170.
[44] N. Campbell, “Developments in corpus-based speech synthesis: Ap-
proaching natural conversational speech,” IEICE Trans. Inf. Syst., vol.
E88D, no. 3, pp. 376–383, 2005.
[45] J. Yamagishi, K. Onishi, T. Masuko, and T. Kobayashi, “Acoustic mod-
elling of speaking styles and emotional expressions in HMM-based
speech synthesis,” IEICE Trans. Inf. Syst., vol. E88D, no. 3, pp.
502–509, 2005.
[46] E. Rennison, “Galaxy of News: An approach to visualizing and under-
standing expansive news landscapes,” in Proc. ACM Symp. User Inter-
face Software Technol., 1994, pp. 3–12.
[47] G. Salton, “Automatic Text Processing: The Transformation,” in Anal-
ysis, and Retrieval of Information by Computer. New York: Addison-
Wesley, 1989.
[48] M. Balestri, A. Paechiotti, S. Quazza, P. L. Salza, and S. Saridri,
“Choose the best to modify the least: A new generation concatenative
synthesis system,” in Proc. EuroSpeech, Budapest, Hungary, 1999,
vol. 5, pp. 2291–2294.
[49] C.-L. Isbell and P. Viola, “Restructuring sparse high dimensional data
for effective retrieval,” Adv. Neural Inf. Process. Syst., vol. 11, pp.
480–486, 1999.
[50] N. Montoya, “El uso de la voz en la Publicidad Audiovisual Dirigida a
los niños y su Eficacia Persuasiva,” Ph.D. dissertation, Univ. Autònoma
de Barcelona, Barcelona, Spain, 1999.
[51] M. Sassano, “Virtual examples for text classification with support
vector machines,” in Proc. Conf. Empirical Methods in Natural Lang.
Process., 2003, pp. 208–215.
[52] T. Joachims, SVMlight 2000 [Online]. Available: http://ais.gmd.de/
~thorsten/svm_light/
[53] J. Shawe-Taylor and N. Cristianini, Kernel Methods for Pattern Anal-
ysis. Cambridge, U.K.: Cambridge Univ. Press, 2004.
[54] W. Cavnar and J. Trenkle, “N-gram-based text categorization,” in Proc.
3rd Annu. Symp. Document Anal. Inf. Retrieval, 1994, pp. 161–175.
[55] F. Alías, J. Socoró, X. Sevillano, I. Iriondo, and X. Gonzalvo, “Multi-
domain text-to-speech synthesis by automatic text classification,” in
Proc. InterSpeech, Pittsburgh, PA, 2006, pp. 267–274.
[56] F. Alías, X. Llorà, L. Formiga, K. Sastry, and D. E. Goldberg, “Efficient
interactive weight tuning for TTS synthesis: Reducing user fatigue by
improving user consistency,” in Proc. ICASSP, Toulouse, France, May
2006, vol. I, pp. 865–868.
[57] B. Noble and J. Daniel, Applied Linear Algebra. Englewood Cliffs:
Prentice-Hall, 1988.
Francesc Alías (S’05–M’07) received the B.Sc.
degree in telecommunications engineering and the
M.Sc. and Ph.D. degrees in electronics engineering
from Enginyeria i Arquitectura La Salle, Universitat
Ramon Llull, Barcelona, Spain, in 1997, 1999, and
2006, respectively.
From 1999 to 2004, he was a Research Assistant
and a Practices/Demonstrating Teacher at the De-
partment of Communications and Signal Theory,
Enginyeria i Arquitectura La Salle, also becoming
an Assistant Teacher in 2004. In September 2007, he
joined the Acoustic Area of the Department of Audiovisual Technologies of
the same faculty as a Researcher and Assistant Teacher. His current research
interests include speech and audio processing, analysis, synthesis and recogni-
tion, multimodal systems, artificial intelligence, text analysis, and new teaching
methodologies. From 2000 to 2004, he was a Ph.D. student granted by the
Departament d’Universitats i Societat de la Informació (DURSI), Generalitat
de Catalunya. He has authored or coauthored over 50 papers in scientific
journals and conferences.
Dr. Alías has been a member of the IEEE Signal Processing Society since
2005, and he is currently a member of the Speech Synthesis Special Interest
Group and the Special Interest Group on Iberian Languages of the Interna-
tional Speech Communication Association (ISCA) and the Speech Technolo-
gies Spanish Network.
Xavier Sevillano received the B.Sc. degree in
telecommunications engineering and the M.Sc.
degree in electronics engineering from Enginy-
eria i Arquitectura La Salle, Universitat Ramon
Llull (URL), Barcelona, Spain, in 1997 and 2000,
respectively, and the M.S. degree in project manage-
ment from URL in 2002. He is currently pursuing
the Ph.D. degree at URL focused on multimodal
clustering.
Since 2000, he has been an Assistant Teacher and
Researcher at the Department of Communications
and Signal Theory, Enginyeria i Arquitectura La Salle. His current research
interests are text analysis, multimodal fusion, speech technologies, and cluster
ensembles for robust multimedia data clustering. He has authored or coauthored
over 25 papers in scientific journals and conferences.
Mr. Sevillano is currently a member of the Association for Computing Ma-
chinery (ACM).
Joan Claudi Socoró received the B.Sc. degree in
telecommunications engineering and the M.Sc.,
and Ph.D. degrees in electronics engineering from
Enginyeiria i Arquitectura La Salle, Universitat
Ramon Llull, Barcelona, Spain, in 1993, 1995, and
2002, respectively.
He has been with the Department of Communica-
tions and Signal Theory, Enginyeria i Arquitectura
La Salle, since 1992, first as a Practices/Demon-
strating Teacher and, since 1995, as Researcher and
Assistant Teacher. His current research interests
include speech and audio processing, analysis, synthesis and recognition, and
multimodal systems. From 1996 to 1998, he was a Ph.D. student granted by the
Departament d’Universitats i Societat de la Informació (DURSI), Generalitat
de Catalunya. He has authored or coauthored over 90 papers in scientific
journals and conferences.
Dr. Socoró received the 1999/2000 Rosina Ribalta Research Consolation
Prize for the best Ph.D. thesis in Information Technologies and Communi-
cations by the Epson Foundation. He has been a member of COST-251 and
COST-262 and he is currently member of the Speech Technologies Spanish
Network.
Xavier Gonzalvo received the B.Sc. degree in
telecommunications engineering and M.Sc. degrees
in electronics engineering from Enginyeria i Arqui-
tectura La Salle , Universitat Ramon Llull (URL),
Barcelona, Spain, in 2002 and 2004, respectively.
He is currently pursuing the Ph.D. degree at URL
focused on HMM-based text-to-speech synthesis.
He was with the Department of Communications
and Signal Theory, Enginyeria i Arquitectura La
Salle, as an Assistant Researcher from 2003 to
March 2008. His current research interests include
speech processing analysis, speech synthesis and recognition, multimodal
systems, dialog systems, and array processing. He has authored or coauthored
over 15 papers in scientific journals and conferences.
Mr. Gonzalvo is currently a member of International Speech Communication
Association (ISCA) and the Speech Technologies Spanish Network.
Authorized licensed use limited to: Agean University. Downloaded on October 16, 2008 at 12:48 from IEEE Xplore.  Restrictions apply.
