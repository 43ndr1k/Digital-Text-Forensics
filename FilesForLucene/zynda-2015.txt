Feature Frequency Profiles for Automatic Sample
Identification using PySpark
Gregory Zynda
Texas Advanced Computing
Center
University of Texas at Austin
gzynda@tacc.utexas.edu
Niall Gaffney
Texas Advanced Computing
Center
University of Texas at Austin
ngaffney@tacc.utexas.edu
Mehmet Dalkilic
School of Informatics and
Computing
Indiana University
dalkilic@indiana.edu
Matthew Vaughn
Texas Advanced Computing
Center
University of Texas at Austin
vaughn@tacc.utexas.edu
ABSTRACT
When the identity of a next generation sequencing sample
is lost, reads or assembled contigs are aligned to a database
of known genomes and classified as the match with the most
hits. However, any alignment based methods are very ex-
pensive when dealing with millions of reads and several thou-
sand genomes with homologous sequences. Instead of relying
on alignment, samples and references could be compared and
classified by their feature frequency profiles (FFP), which
is similar to the word frequency profile (n-gram) used to
compare bodies of text. The FFP is also ideal in a metage-
nomics setting to reconstruct a mixed sample from a pool
of reference profiles using a linear model or optimization
techniques. To test the robustness of this method, an assort-
ment of samples will be matched to complete references from
NCBI Genome. Since a MapReduce framework is ideal for
calculating feature frequencies in parallel, this method will
be implemented using the PySpark API and run at scale on
Wrangler, an XSEDE system designed for big data analyt-
ics.
Keywords
feature frequency profile, hadoop distributed file system
1. INTRODUCTION
In the realm of natural language processing the idea of
n-grams, or overlapping subsequences of n items, is com-
monly used to summarize and model large bodies of text.
Documents are broken down into word or character n-grams
and the occurrence of each n-gram is tallied. Besides show-
ing popular n-grams and relationships between entities, this
is also a simple way to transform text into numerical vec-
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full cita-
tion on the first page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
PyHPC2015 November 15-20, 2015, Austin, TX, USA
c© 2015 ACM. ISBN 978-1-4503-4010-6/15/11. . . $15.00
DOI: 10.1145/2835857.2835862
tors for quantitative analysis. N-grams have been previously
used for genre classification [11], spam detection [7], and au-
thorship identification [8].
Computational biology often borrows from other fields
as technology progresses. Because DNA is long sequential
chain of values, methods are often adapted from time-series
analysis, but methods from natural language processing can
be used as well. Synonymous to n-grams, computational
biology has k-mers, which are subsequences of overlapping
k-length nucleotides along strands of DNA. While the idea
is simple, it is robust enough to be a universal statistic for
genomes of greatly different sizes an origin. K-mers have
been used for error detection [6], sequence assembly [14],
repetitive element detection [9], sample classification [12],
and genome comparison [10].
Using k-mers for sequence comparison is advantageous
over common methods because they donâĂŹt require align-
ment. Whenever large sequences are compared, local align-
ment methods are used to find the best match while also
allowing for differences. Alignment isn’t used for compar-
ing kmers, because they are just counted. All differences in
sequence result in different overall levels or k-mers unique
to a specific dataset. In the context of sample read iden-
tification, each of the hundreds of millions of reads from
a next-generation sequencing run need to be aligned to a
database of each possible genome. This is the same process
that occurs when a read is identified with BLAST [2]. Local
alignment is an O(N2) dynamic programming method, so
repeating it for large numbers of references isn’t ideal. The
NCBI genome archives will continue to grow at an exponen-
tial rate as shown in Figure 1, making any alignment-free
methods attractive.
One of the most popular models for text analysis is MapRe-
duce, which maps functions to pieces of data in parallel,
and then reduces the result together. Since the preprocess-
ing and statistical methods are very similar, we propose the
use of Apache Spark for distributed k-mer frequency calcu-
lation of reference genomes and sequencing reads [13]. Not
only will Spark’s partitioning methods split the data be-
tween workers and to disk when there is too much to hold
in memory, but it will also orchestrate the parallel compu-
tations taking place. This will circumvent the drawbacks of
code that have high memory requirements and only function
Figure 1: Number of complete genomes archived by
NCBI genome.
single-node in shared memory like jellyfish [4].
2. RELATED WORK
K-mer frequencies have been previously utilized for both
genome comparison and sequencing read classification. Sims
et al. utilized k-mer feature frequency profiles to compare
genomes of varying lengths using the Jensen-Shannon diver-
gence. They then constructed phylogenies based on their
computed divergences which closely resembled the official
taxonomic phylogenies from NCBI. Wood and Salzberg cre-
ated the tool Kraken to match reads to their genus of origin
based on k-mer presence. Kraken does this by construct-
ing a database from a collection of genome references and
a known taxonomy to collapse k-mers to their lowest com-
mon ancestor. Because our database is distributed across
multiple executors instead of together on a single computer,
our implementation skips this preprocessing step and relies
on the quantity of the information in the genome references
over the quality of the taxonomy.
Kraken has been shown to be very precise when classifying
reads, but is inaccessible to the average user for two main
reasons. First, it requires at least 70GB of system mem-
ory to run and most XSEDE systems have 2GB of memory
per core. This means some kind of large-memory resource
is required. Second, it is highly dependent on its database.
Kraken requires that each genome be included in an accom-
panying taxonomy. Sequencing is getting cheaper and hap-
loid assembly is push-button since the advent of long-read
technologies [3], but curated taxonomies can lag behind.
Kraken also requires that the database be rebuilt whenever
samples change, and this is a time-consuming process bound
by disk operations.
Apart from sequence classification and k-mer counting,
here has been some recent work to support bioinformatics
analysis on the Spark platform with the ADAM suite [5].
However, this is not general purpose and specifically targets
the storage and analysis of specific types of files. ADAM
also focuses on analyses that translate well to tabular data,
like point mutations in the variant call format. These point
mutations are sets of mutation coordinates, and an analysis
across samples is well suited to Spark’s current native data
analysis abilities.
Figure 2: PySpark workflow for reference k-mer
counting.
3. SPARK FOR GENOMICS
Even with the push of ADAM and large companies that
specialize in MapReduce like Google starting to get into the
field of genomics, the platform is still gaining traction within
the community. We however, saw that the feature frequency
profile was ideal for transforming genomic data into vec-
tors more suited to a MapReduce framework. We imple-
mented sparkmer using PySpark, the Python API to the
Spark platform. PySpark allows for full Spark utilization
without writing any non-python code or multiple command
line calls like with Hadoop Streaming. Besides MapReduce
becoming more accessible, Spark has full access to python’s
67,000 packages.
4. APPROACH
4.1 Reference k-mer Counting
After transferring a collection of Fasta references to the
hadoop distributed file system (HDFS), all k-mers, for a
specified k, are counted as laid out in Figure 2. First, all
fasta files are filtered to remove header sequences and line
breaks, and then concatenated into a contiguous sequence.
For reference sets with large genomes, like eukaryotes, se-
quences can be split into sub-sequences with k-bp (k-mer)
of overlap and repartitioned for evenly distributed process-
ing.
k-mers for each section are then computed and immedi-
ately transformed into an index. The indices are calculated
by treating each k-mer as a base-4 representation of a base-
10 index, where {A:0, G:1, C:2, T:3}. We decided to use
the base-4 index method as opposed to the more commonly
used hash index after having two key collisions when count-
ing 3-mers with 210 valid keys. Not only does our indexing
method avoid collisions, but it also runs in constant time
Figure 3: PySpark workflow for classifying input
reads.
and does not require a lookup table for the inverse.
Reads will be classified based on Jaccard Similarity, to
index arrays are transformed into Python sets. The sets
will not only allow for efficient unions and intersections dur-
ing when calculating similarity and reducing, but also is a
sparse representation of k-mer presence. Sparse data types
are necessary when counting k-mers because a dense vector
to keep track of all 20-mers requires 4TB of memory. We also
experimented with sparse vector structures, but they were
unnecessary since we never use the count, just the presence
of a k-mer.
4.2 Classifying Reads
Reads are then transformed into sets of k-mer indices us-
ing the same method for genomes and classified as shown
in Figure 3. While developing this process, we experienced
over-allocation errors from the executors after calculating
read k-mers, no matter the number of partitions. After map-
ping the partitions of our data and counting the number of
records in each and plotting the histogram (Figure 4), we dis-
Figure 4: Histogram illustrating the default parti-
tioning of the reads RDD after k-mer calculation.
covered that a majority of the Resilient Distributed Dataset
(RDD) partitions were being left empty. We are not sure if
the low complexity read names all hashed to the same par-
titions or filtering and joining the reads caused the problem,
but neither shuffling nor repartitioning had any effect on the
distribution of the data. We finally fixed this by manually
partitioning the reads with an integer index. This forced a
uniform distribution of the data across the partitions, so no
executors were overwhelmed later in the analysis.
Large quantities of reads and large reference databases
leads to huge number of pairwise comparisons. To reduce
the number of required comparisons, we filter our database
down to 150 probable candidates. These candidates are de-
termined by first creating a global k-mer set by reducing all
input read sets with with a union. This global k-mer set is
then compared to all 11,112 bacteria and virus genomes, and
the 150 references with the highest similarity are kept. Then
each of the read sets is compared to each of the 150 best-
candidate references by mapping the Jaccard Similarity to
the Cartesian product of the two RDDs and reducing each
read by the maximum similarity as the final classification.
5. METHODOLOGY
We tested sparkmer on a 26 node (1 master, 25 workers)
hadoop instance on the XSEDE system Wrangler. Wrangler
is an ideal platform for Spark becuase each node has 4TB
of EMC flash for the hadoop distributed file system, making
Sparks ability to spill partitions to disk as efficient as pos-
sible. To validate sparkmer, we used the same 11,112 ref-
erences (5,242 bacterial and 5,870 viral) used in the database
for Kraken. Then, sparkmer was run using the same HiSeq accuracy
dataset created to test the accuracy of Kraken [1]. The
HiSeq accuracy dataset consists of 10,000 reads from 10 sources
of origin:
• A. hydrophila
• B. cereus
• B. fragilis
• M. abscessus
• P. fermentans
• R. sphaeroides
• S. aureus
• S. pneumoniae
• V. cholerae
• X. axonopodis
Each of the 10 sources contributed 1,000 reads. Lastly,
HiSeq accuracy was converted to fastq format, a standard
format for reads coming from a next-generation sequencer.
6. RESULTS
No matter how dissimilar a read was to the 150 possible
candidates it is compared to by sparkmer, an identity is al-
ways assigned. While this practice makes sense since each
fragment of DNA had to originate from some organism, it
does not ensure that the species of origin is present in the
reference database. Kraken goes the opposite route and will
label reads as unknown when there is an insufficient number
of k-mers unique to a specific genus. To facilitate this unique
requirement, Kraken has a default k-mer size of 31, keeping
memory requirements high. The differences between these
two methods can be obviously seen in the classification pre-
cision, which is the number of correctly-classified reads over
the total number of classified reads, in Table 1. Kraken had a
precision rate of 98% while sparkmer had 70%. These results
may seem disparate, but Kraken left 20% (2,000) of its reads
unclassified, even while using the extremely large 31-mers for
analysis. Even though sparkmer classified all 10,000 reads,
its sensitivity, or the number of correctly-classified reads of
the total number of reads, was still lower than Krakenś, but
it also used 15-mers for the analysis.
Table 1: Accuracy of read classification from spark-
mer and Kraken.
sparkmer Kraken
Correct 6951 7760
Incorrect 3049 131
Unclassified 0 2109
Precision 69.5% 98.3%
Sensitivity 69.5% 77.6%
To test how well sparkmer scaled on our 24 worker nodes,
we ran the HiSeq Accuracy test with 60, 90, 120, and 150
executors. Sparkmer experienced strong scaling up until 150
executors, where the network traffic when reducing by key
ended up being the bottleneck. Sparkmer’s fastest runtime
was 1 hour; classifying 167 reads per minute on average.
Even though sparkmer could take advantage more proces-
sors than the 24 that Kraken ran on, Kraken was much faster
and classified reads at a rate of 1422 reads per minute on
Wrangler.
7. CONCLUSION
Based on the runtime analysis, PySpark is an ideal frame-
work for counting k-mers, because sparkmer counted them
across all 11,112 genomes in 3 minutes. Even though spark-
mer was 10 times slower than Kraken for actual classifica-
tion tasks, it still shows promise. Sparkmer was a naive
approach to a difficult problem, and it was exciting to see
Figure 5: Runtime of sparkmer on increasing num-
bers of executors.
it not only become feasible using the PySpark framework,
but also scale without manually orchestrating communica-
tion and each task.
These initial results were promising, so we plan on ex-
ploring new ways to improve the performance of sparkmer.
The final distance reduction by read name was one of the
costliest because of the sheer number of keys to reduce by.
The PySpark streaming API is still immature, but we hope
to use is to process large files of reads in small windows
so there are fewer keys to reduce by. This may even al-
low for the inclusion of all genomes in the final similarity
computation. If not, we will continue using a filtered set
of probable candidates to reduce the number of pairwise
comparisons. However, we will improve on this step by col-
lapsing extremely similar references to reduce the number
of redundant genomes. Hopefully these changes not only in-
crease the throughput of sparkmer in the future, but also in-
crease the precision, so a commodity system running spark-
mer can compete with the Kraken running on large-memory
resources.
8. AVAILABILITY
sparkmer is written in Python using the PySpark API and
is available for download from https://github.com/zyndagj/
sparkmer.
9. ACKNOWLEDGMENTS
This work is funded by the National Science Foundation
under the Award DBI-1265383.
10. REFERENCES
[1] Kraken accuracy data.
https://ccb.jhu.edu/software/kraken/dl/accuracy.tgz.
Accessed: 2015-10-18.
[2] S. F. Altschul, W. Gish, W. Miller, E. W. Myers, and
D. J. Lipman. Basic local alignment search tool.
Journal of molecular biology, 215(3):403–410, 1990.
[3] C.-S. Chin, D. H. Alexander, P. Marks, A. A.
Klammer, J. Drake, C. Heiner, A. Clum, A. Copeland,
J. Huddleston, E. E. Eichler, et al. Nonhybrid, finished
microbial genome assemblies from long-read smrt
sequencing data. Nature methods, 10(6):563–569, 2013.
[4] G. Marçais and C. Kingsford. A fast, lock-free
approach for efficient parallel counting of occurrences
of k-mers. Bioinformatics, 27(6):764–770, 2011.
[5] M. Massie, F. Nothaft, C. Hartl, C. Kozanitis,
A. Schumacher, A. D. Joseph, and D. A. Patterson.
Adam: Genomics formats and processing patterns for
cloud scale computing. EECS Department, University
of California, Berkeley, Tech. Rep.
UCB/EECS-2013-207, 2013.
[6] P. Melsted and B. V. Halldórsson. Kmerstream:
Streaming algorithms for k-mer abundance estimation.
Bioinformatics, 30(24):3541–3547, 2014.
[7] A. Ntoulas, M. Najork, M. Manasse, and D. Fetterly.
Detecting spam web pages through content analysis.
In Proceedings of the 15th international conference on
World Wide Web, pages 83–92. ACM, 2006.
[8] F. Peng, D. Schuurmans, and S. Wang. Augmenting
naive bayes classifiers with statistical language models.
Information Retrieval, 7(3-4):317–345, 2004.
[9] A. L. Price, N. C. Jones, and P. A. Pevzner. De novo
identification of repeat families in large genomes.
Bioinformatics, 21(suppl 1):i351–i358, 2005.
[10] G. E. Sims, S.-R. Jun, G. A. Wu, and S.-H. Kim.
Alignment-free genome comparison with feature
frequency profiles (ffp) and optimal resolutions.
Proceedings of the National Academy of Sciences,
106(8):2677–2682, 2009.
[11] E. Stamatatos, N. Fakotakis, and G. Kokkinakis. Text
genre detection using common word frequencies. In
Proceedings of the 18th conference on Computational
linguistics-Volume 2, pages 808–814. Association for
Computational Linguistics, 2000.
[12] D. E. Wood and S. L. Salzberg. Kraken: ultrafast
metagenomic sequence classification using exact
alignments. Genome Biol, 15(3):R46, 2014.
[13] M. Zaharia, M. Chowdhury, M. J. Franklin,
S. Shenker, and I. Stoica. Spark: cluster computing
with working sets. In Proceedings of the 2nd USENIX
conference on Hot topics in cloud computing,
volume 10, page 10, 2010.
[14] D. R. Zerbino and E. Birney. Velvet: algorithms for de
novo short read assembly using de bruijn graphs.
Genome research, 18(5):821–829, 2008.
