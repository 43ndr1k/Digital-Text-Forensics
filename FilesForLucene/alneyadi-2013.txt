Adaptable N-gram Classification Model for Data 
Leakage Prevention 
Sultan Alneyadi, Elankayer Sithirasenan, Vallipuram Muthukkumarasamy 
School of Information and Communication Technology 
Griffith University 
Gold Coast Campus, Australia 
sultan.alneyadi2@griffithuni.edu.au, {e.sithirasenan, v .muthu} @griffith.edu.au 
Abstract- Data confidentiality, integrity and availability are 
the ultimate goals for all information security mechanisms. 
However, most of these mechanisms do not proactively protect 
sensitive data; rather, they work under predefined policies and 
conditions to protect data in general. Few systems such as 
anomaly-based intrusion detection systems (IDS) might work 
independently without much administrative interference, but 
with no dedication to sensitivity of data. New mechanisms 
called data leakage prevention systems (DLP) have been 
developed to mitigate the risk of sensitive data leakage. 
Current DLPs mostly use data fingerprinting and exact and 
partial document matching to classify sensitive data. These 
approaches can have a serious limitation because they are 
susceptible to data misidentification. In this paper, we 
investigate the use of N-grams statistical analysis for data 
classification purposes. Our method is based on using N-grams 
frequency to classify documents under distinct categories. We 
are using simple taxicap geometry to compute the similarity 
between documents and existing categories. Moreover, we 
examine the effect of removing the most common words and 
connecting phrases on the overall classification. We are aiming 
to compensate the limitations in current data classification 
approaches used in the field of data leakage prevention. We 
show that our method is capable of correctly classifying up to 
90.5 % of the tested documents. 
Keywords- Data leakage prevention; N-grams; N-gram 
profiles 
I. INTRODUCTION 
The loss of data through conventional means such as 
deliberate network attacks, eavesdropping and intrusion can 
be handled using firewalls, VPNs and IDS systems. 
However, these systems may not work properly if the loss 
happens spontaneously due to an authorized user. Since 
most devastating data leaks are accidental [1], there should 
be some security controls to handle this problem. Data 
leakage prevention systems or DLPs are especially 
developed to identify, monitor, detect and prevent the 
leakage of sensitive data. These systems are relatively new 
when compared with older and more mature security 
mechanisms. Therefore, there are many approaches 
developed by security venders to address this growing 
problem [I] from different perspectives. Some DLPs 
analyse the context by considering the number of 
transactions between senders and recipients, timing and data 
formats to detect abnormality. While others restrict the 
access to applications and data repositories, by simply 
blocking USB ports. On the other hand, more sophisticated 
DLPs apply content analysis techniques to identify and 
protect sensitive data. This type of DLPs looks directly into 
the transmitted data and tries to analyse the content. 
Depending on the analysis performed, sensitive data can be 
detected using data fingerprints, exact or partial document 
matching and statistical analysis [2]. One of the problems 
associated with data fingerprints and document matching is 
the use of hash values as a reference to classify sensitive 
data. A tiny change on the original data can produce a 
totally different hash value, which leads to wrong 
classification. This susceptibility can be considered as a 
drawback when using data fingerprints and document 
matching. Some studies try to overcome this problem by 
representing documents by multiple hashes [3], 
implementing Rabin fingerprinting [4] and using Piecewise 
hashing [5]. However, these solutions have limited 
capabilities and can be affected by various possible text 
obfuscation. On the other hand, statistical content analysis is 
another approach used in content analysis DLPs, where 
there is no need to keep a record of an exact reference or 
fingerprints. More importantly, modified or unstructured 
data can still be tracked and classified. Also, unlike 
deterministic techniques, newly created data may be 
classified without the need for intensive processing. 
In the literature, there is a little research conducted to 
address the advantages of using statistical analysis in DLPs. 
On the other hand, in linguistics, statistical analysis is a well­
studied area and it is widely used for classification purposes 
and pattern recognition. The statistical analysis is mostly 
dependent on the usage of N -grams, which can be words or 
characters. These N-grams can be represented in many sizes 
and formats, which make them easy to analyse and process. 
The use of N-grams statistical analysis provides durability 
even with the most complex written languages such as 
Arabic and Chinese [6][7]. This analysis is also used in many 
data mining applications, where the frequency of N-grams 
can help to predict and reconstruct noisy data [8]. The use of 
N-grams for classification purposes is also used in other 
fields, such as malicious code and genre detection [9][10]. 
In previous studies, text classification is used to some 
extent to prevent the leakage of sensitive data. Hart et al. 
978-1-4799-1319-0/13/$31.00 ©2013 IEEE 
[11] introduced a method based on machine learning to 
classify enterprise documents as sensiti ve or not. This 
approach uses Support Vector Machines (SVMs) algorithm 
to classify three types of data: Enterprise pri vate, Enterprise 
public and Non-Enterprise. The data was represented by the 
most frequent binary weighted unigrams, i.e. words, found 
across all corpora. The method was able to identify 97% of 
data leaks with a false negative rate of 3.0%. Unfortunately, 
this method classifies data to public or private only; ignoring 
more flexible classification levels like top secret, secret and 
confidential. In reality, this method can obstruct the work 
process making the enforcement of security polices a 
difficult task. Also, Sokolova et al. [12] presented a system 
to detect the sensitive data in heterogeneous text. This 
method was based on the separation of possible and 
impossible containers of personal health information. On the 
first analysis phase, an N-gram classification tool was used 
to eliminate non-English text. This method was able to detect 
all possible containers of sensitive data; however, this result 
comes with high false positive rate. 
Moreover, with no dedication towards sensitive data 
protection, N-gram statistical analysis was used in text 
classification for the purpose of categorising articles. 
Cavnar and Trenkle [13], used character N-grams to classify 
documents according to their language and topic, where 
frequency-sorted N-gram profiles were used to relate 
documents to a specific topic or language such as English, 
French and Spanish. Although classifying documents 
according to their language resulted in 99.8% accuracy, 
classifying them according to the topic achieved only 80% 
accuracy. Furthermore, Stamatatos [14] introduced a 
method to check style variation to detect possible text 
plagiarism. This intrinsic plagiarism detection method uses 
character N-grams within a sliding window, which 
compares common N -grams and hence reports any variation 
within a document. This method provides an unconventional 
way to detect plagiarism, where there is no need for existing 
data fingerprints. However, it only detects roughly half of 
plagiarised sections within a document. 
In our study we investigate the use of N-grams statistical 
analysis for document classification purposes. We propose 
an enhanced classification mechanism using word-N-grams, 
by creating profiles for documents intended to be classified. 
These profiles are compared with existing categories to 
define the shortest similarity distance. Our aim in this 
research is to classify new documents without the need for 
using exact references or data fingerprints. Instead, a 
document should be classified under a category if its profile 
scores llll lmum distance. Moreover, we introduce 
automatic category profile updates with the most common 
N-gram for each category. This is done to ensure the 
inclusion of the latest and most important N-grams within a 
topic. Also, we illustrate the effectiveness of removing 
common words and phrases on the overall classification. 
By addressing these issues we intend to improve the 
accuracy of data classification methods used in data leakage 
prevention systems. 
This paper is divided as follows: Section II discusses the 
N-gram classification methodology. Section III describes 
experiments performed in our research and presents the 
results. Section IV analyses the findings from the previous 
section. In section V, the limitations of our research are 
discussed. Section VI concludes this paper and proposes 
some future directions. 
( �:.�:."�� )��� ,/ 
,� � j ( }-O���;ic� ) 
( Database U Network ) 
,\�. /\, . , ,�.<, (" 
"- �� 
Fig 1. A general overview of main topics tested and the relationship 
with sub-topics 
II. WORD N-GRAM CLASSIFICATION 
In this research, our hypothesis is based on Zipf's law 
[15]. Zipf suggested that the number of occurrences of a 
word in a piece of document indicates its importance. In 
other words, the most frequent word has the highest rank 
while the least frequent word has the lowest rank. We also 
built our assumptions on the idea that it is possible to relate 
a document to an existing category or class, providing the 
presence of sufficient varieties of related N-grams between 
the document and the category. Taking all the previous 
conditions into account, we designed a classification model 
to classify documents into different topics, even though 
these topics are related. Unlike in [11], where the 
classification was limited to private or public documents, 
our classification is more detailed. For example if, 
"Information security" is a sensitive topic within an 
enterprise, does it mean that all the sub-topics are sensitive 
as well? Therefore, it is important to have a method which 
is able to distinguish between related topics and assign 
different classification levels, such as top secret, secret, 
confidential, and restricted [16], instead of generalising the 
term "sensitive". Adding such a feature to the eXlstmg 
DLP's capabilities should increase the significance towards 
sensitive data. In real life scenarios, it totally depends on the 
enterprise to decide the topic classification level; hence we, 
are not discussing this in our paper. 
Figure 1 illustrates main topics and related sub-topics, 
which we considered in this paper. For the purpose of 
testing our classification model, we have intentionally 
picked six topics under the more general field "Information 
security", so we can demonstrate the effectiveness of our 
method with related topics. The topics we have selected are: 
Antivirus (A), Data Leakage Prevention (D), Encryption 
(E), Firewall (F), Intrusion Detection Systems (I) and 
Virtual Private Networks (V). We gathered articles from 
various online resources such as pcmag.com, 
scmagazine.com and others, to represent the documents in 
our experiment. All the documents went through a profile 
creation process which includes: 
• Breaking down the document into single word N­
grams. 
• Sorting all N-grams from the most frequent to the 
least frequent. 
• Removing all stop words, common words and 
phrases. 
Next, we created six category profiles for each topic. 
The same procedure was adopted for the creation of a 
category profiles; however, the source of the category 
profile was Wikipedia [17]. It should be noted that we are 
not restricting our research to a certain source; in fact, we 
can build our category profile from any source as long as we 
can get a variety of N-grams for a specific topic. 
The classification process begins by comparing the 
document profile with each category profile in order to 
classify a document under a relevant topic, i.e. category. We 
use simple "out of place" distance [13] or taxicap geometry 
to compute the similarity between the document and the 
category. A document may be classified under a category 
when a minimum distance is identified. Figure 2 illustrates 
the document classification process. 
According to the classification level assigned to the 
chosen category, the tested document should inherit 
sensitivity properties from that category. Achieving this 
would help in first, assigning a classification level to new 
documents, second, overcoming classification problems 
such as exact data matching and data fingerprints. In other 
words, a document can be classified without the need for 
subjective human opinion; therefore this might ease the use 
of security policies and access rights. And in a dedicated 
DLP system, a document may be easily identified and 
protected without any prior knowledge of it. 
A. N-gram Creation 
To create N-grams for our profiles we used the online 
free tool ktNgram [18]. The tool is used for linguistic 
analysis and it is capable of creating N-grams of different 
sizes and types i.e. character-grams and word-grams. In our 
research we are using the tool to create single word N-grams 
and to sort these N-grams by frequency. The use of single 
words instead of two or three words was discussed in [19] 
and it was shown that due to the limited number of created 
bi-grams and tri-grams created, fewer related N-grams were 
Remove 
Common 
Words 
The 
Virtual 
A 
VPN 
IP 
Network 
In 
Private 
I 
New 
Document 
Frequency 
Sorted 
T
t
XI 
��Iter 
Are . ==::J 
To 
In 
It Cat. Profile Doc. Profile 
VPN Virtual 
IP 
IP 
Address Network 
VPNs Private 
Network Tunnel 
Tunnel Server X-count 
Packet Packet 
.... " Profile Update--­
Compare 
Category _ 
Profiles and 
Choose the ==== 
Minimum 
VPN 
X-count 
File 
Server 
Private 
Layer 
Frequency 
Sorted 
After Each 
Test 
Fig 2. An overview of the N-gram classification process and the X­
counts extraction 
representing the topic which affected the classification 
accuracy. The use of character N-grams was discussed in 
[13] for the purposes of classification, but the overall results 
were modest with only 80% correct classification. 
Therefore, the use of single word in our classification 
profiles was the best option to use considering previous 
limitations. 
B. Common Words Removal 
To increase the accuracy of our classification model, we 
have removed the most common words in the English 
language from the category and document profiles. The 
most common words in English according to the Oxford 
English Corpus (OEC) include 100 words of prepositions, 
adjectives, verbs and nouns. This list of words was 
developed after the analysis of more than one billion words, 
which considered the largest record of words. OEC claims 
that the top 10  words (the, be, to, of, and, a, in, that, have, 
and I) account for 25% of the Oxford English corpus, and 
the top 100 words account for 50% of the same corpus [20]. 
Furthermore, we have added common transition words and 
phrases to the previous list such as (moreover, however, 
thus, hence, etc,), Removing these words without affecting 
the document semantic should reduce the excess distance 
when performing the distance calculation, The total number 
of common words and phrases considered for removal from 
documents reached 400. 
C. Updating Category Profiles 
Some encouraging results from our previous work were 
reported in [19] for updating category profiles. It is possible 
that a category profile will not have most N -grams related to 
a topic; therefore, we have introduced a way to update the 
category profile after every successful classification. This is 
done by adding the top ranked X-counts (N-grams not 
present in the category profile) extracted after the 
classification process, to category profiles. These X-counts 
are gathered in an exclusive file for each category, only if a 
specific document happens to be correctly classified. The 
exclusive X-counts file is frequency-sorted and any new 
insertion is placed on the bottom of the list. These X -counts 
can move up and down according to the number of 
occurrences. 
After testing 1 80 documents, we created six files with 
most frequent X-counts for each category. Maintaining 
these files will ensure that category profiles are equipped 
with the most relevant N-grams. Figure 2 shows the 
extraction of X-counts from the classification process and 
then the addition to the exclusive X-counts file. 
Since it is better to work with fixed size profiles to avoid 
wrong classification, we have chosen the top 100 N-grams 
within each topic to create category profiles (see 3.1). 
Working with variable category profile sizes can favor 
smaller profiles over the correct ones. This is because our 
classification model is picking the category with the 
smallest distance. 
D. Distance Calculation 
For the distance calculation we applied taxicap geometry 
or "out of place" count. This simple comparison between 
the position of an N -gram in the document profile and the 
equivalent N-gram in the category profile can reflect how 
similar a document is to a category. In an ideal scenario, two 
identical documents must have distance value of zero. But 
since we are not referring to an exact reference, we should 
expect a smaller distance between a document and the 
matching category. Figure 3 shows an example of how the 
distance is calculated. The term "X-count" refers to absent 
N-grams in the category, so we denote its value with the 
maximum distance "total number of N -grams". The overall 
distance is calculated using equation (I). 
To calculate the distance between the document and the 
category profiles we developed a program in MATLAB. 
This program searches for the first available N -gram in the 
document profile and its equivalent in the category profile. 
If the N-gram is found in the category profile, then an "out 
of place" distance will be calculated, otherwise the program 
will report an X-count. Moreover, if the document happens 
to be related to the topic as for the overall distance, the 
extracted X-counts will be stored in the exclusive X-count 
file. 
Top Rank 
Lowest Rank 
Cat. Profile 
VPN 
IP 
Virtual/ 
Address 
VPNs 
Network/'/ 
Doc. Profile 
Virtual 2 
y" VPN I 
IP I 
/Network 2 
Private -----·-X-count 
Tunnel I 
Tunnel Server 
Packet --- Packet 
X-count 
o 
Overall 
=7 + 2 X-counts 
DIstance 
Fig 2. Distance calculation example 
(X-counts x number of N-grams in Doc) + "out a/place" distance (I) 
III. EXPERIMENTS AND RESULTS 
We started the experiments by preparing 30 documents 
and one category profile for each topic. A total of 180 
documents and six category profiles were considered in the 
first test. The aim at this point was to relate as many 
documents as possible to the corresponding categories. 
Without any removal of common words/phrases and with no 
category profile update, the initial results were showing a 
total of 142 correctly classified documents. This gives an 
overall of 78.8% correct classification. It was noted that 
category (D) scored the lowest, where only 14 documents 
were correctly classified. And by referring to its profile we 
discovered that the common words (and, to, in, or, of, a) 
were among the top 10  N-grams. Although the rest of the 
profile contained relevant N-grams to category (D), it was 
obvious that the top N-grams carried more weight towards 
the semantics of the document. Also, by examining the X­
counts reported after the classification process, we found 
that N-grams like (Files, leaving, critical, encryption, 
protect, loss, leak, insider, block, disclosed) were not 
included in the category profile. These results from our 
previous work [19] encouraged us to update the category 
profiles dynamically. 
A. Defining Category Profile Size 
From the first experiment of 1 80 documents, we tested 
our approach for an automatic category profile update. 
There were 142 correctly classified documents, which we 
considered for updating the category profiles, while all X­
counts from wrong classifications were ignored. We took 
into consideration that large category profiles may introduce 
undesired large distance; therefore we have limited the size 
of the update to 50 new N -grams. Generally, It is difficult to 
know exactly how many N-grams would define an 
appropriate profile size, but from our previous findings, a 
size 50 N-grams was the most accurate among a scale 
including (25, 50, 100, 150, 200, 250, 300) N-grams [19]. 
We showed that very small category profiles i.e. 25 
introduce many misclassifications because of limited N­
grams. Although N-grams were relevant to the topic, they 
were also shared by many categories. Also, very large 
category profiles like 300 N-grams resulted in many 
misclassifications because of the large distance they 
produced. Therefore, the final category profile size selected 
was 100 N-grams, including 50 N-grams from the main 
category body obtained from Wikipedia and 50 N-grams 
from the exclusive X-count file. 
B. Initial Test 
To overcome the problem of the first test, we processed 
all documents again by removing all common 
words/phrases from both documents and category profiles. 
This filtration process insured that the top N-grams are 
essentially related to the topic. Also, this process reduced 
the overall distance significantly by eliminating space 
occupied by common words. Previously, this space caused 
important N-grams to be lower in rank, which means farther 
distance between relevant N-grams. Moreover, category 
profiles were updated using the most frequent X-counts 
found in the first experiment. The total number of correctly 
classified documents increased after these steps. A total of 
163 documents out of 1 80 were correctly classified which 
gives an overall of 90.5% correct classification. It was 
evident that removing the common words/phrases and 
updating category profiles increased the accuracy of the N­
gram classification process. All the categories scored better 
after the removal of common words except for category (E). 
The reason behind this is that "Encryption" as a topic has 
wide range of sub-topics, and N-grams available on its 
profile are shared among other categories such as DLP and 
VPN. Table I shows the total number of correct 
classification for each category before and after the removal 
of common words/phrases and updating category profiles. 
TABLE T. EFFECT OF REMOVING COMMON 
WORDS/PHRASES AND UPDATING CATEGORY PROFILES ON 
THE CLASSIFICATION 
Category A D E F V 
Total 
Overall 
(out of 180) 
Before Removing 
Common Words 29 14 29 24 26 20 142 78.8% 
After Removing 
30 29 22 29 29 24 163 90.5% 
Common Words 
C. Overall Classification 
After the initial test we considered a more 
comprehensive experiment of 360 documents, 60 documents 
from each category. We followed the same procedures as 
before removing common words and using updated category 
profiles. An overall of 90.56% correct classification was 
achieved, which is equivalent to 326 correctly classified 
documents. Table II shows the overall classification results. 
Categories: Antivirus, DLP and IDS achieved the best 
results of 95% correct classification, while Encryption 
achieved the lowest result of 75% correct classification. 
TABLE II. 
Categories Anti.V 
Antivirus 
DLP 
Encryption 
Firewall 
IDS 
VPN 
Correct 59 
Percentage 98.3% 
Overall 
OVERALL CLASSTFICA TION RESULTS OF 360 
DOCUMENTS 
DLP 
59 45 
98.3% 75.0% 
Firewall IDS VPN 
55 57 51 
91.6% 95.0% 85.0% 
90.5% 
As mentioned previously, this is because Encryption is a 
comprehensive topic and it may include many relevant N­
grams from other categories. This can be avoided by 
introducing sub-categories under Encryption such as DES, 
AES and RSA. Moreover, some documents were biased 
towards a specific wrong category such as in Encryption and 
VPN. For example, there were 11 documents from 
Encryption classified under DLP and 7 documents from 
VPN were classified under Firewall. This indicates that 
these categories have many N-grams in common. Therefore, 
it is worth noting that if one of those categories is sensitive, 
the other one might share similar level of sensitivity. In 
reality, considering second best category can help in 
avoiding data leak even if the classification is wrong. 
IV. ANALYSTS 
From these experiments, it is seen that document 
classification using N-gram frequency overcomes some 
problems found in fingerprinting and data matching 
classification methods. However, 10% of incorrect 
classification may still introduce considerable amount of 
risk of data leakage. 
In this section we analyse some of the findings from the 
experiments in order to improve the classification results 
further. 
A. N-gram Detection Rate 
We studied the effect of document size on the 
classification process, to improve our classification model. 
A "document size" can refer to the size in kilobytes or the 
number of distinct N-grams within a document. And since 
we are mainly focusing on the N-grams analysis, we refer to 
the number of distinct N-grams in a document by 
"document size". It is worth mentioning that there was no 
direct link between the size of documents and the 
classification results. Documents of different sizes were 
classified both correctly and incorrectly. Moreover, we 
noticed that small documents have higher N-gram detection 
rate than large documents. In other words, the percentage of 
relevant N-grams was higher in small documents. Although 
most large documents had lower N-gram detection rate, 
picking the smallest distance was sufficient to choose the 
correct category. The detection rate is calculated using 
equation (2). 
ycounts 
Detection rate= ----'-- --- x 100 (2) 
No. of Ng rams in Doc. 
Where: y-counts = Number of N-grams detected = (Number 
of N-grams in the document - x-counts) 
Considering the correctly classified documents only, we 
studied the relationship between the document size and the 
N-gram detection rate. Figure 4 shows the relationship 
between the N-gram detection rate and the document size; 
where it is clear that smaller documents have detection rate 
up to 35% and bigger documents have low detection rate 
down to 5%. 
Observing that the relationship is proportional, lower N­
gram detection rate does not mean less relevant N-grams. 
For example, documents with 200 N-grams have an 
approximate detection rate of 17%, which is equivalent to 
32 N-grams. Also, documents with 800 N-grams have an 
approximate detection rate of 10%, which is equivalent to 
80 N-grams. Since we are extracting results from correctly 
classified documents, these figures can help approximating 
the number of relevant N-grams within related documents. 
Knowing the approximate number of related N-grams 
within a document can indicate how close a document to a 
category is. 
40 
35 
!l 
i! 30 
= � 25 
� 20 
"C 
5 15 
E 
�10 
Z 
0 
0 200 400 600 800 1000 1200 1400 1600 1800 2000 
O. of N-grams In document 
Fig 3. Relationship between the document size and the N-gram 
detection rate 
B. Distance without Common Words and Phrases 
From the initial experiment of 180 documents, we 
studied the effect of removing common words/phrases on 
the overall distance. Some documents were classified 
incorrectly, although they contained more related N-grams. 
This is because there was excess distance caused by the 
common words occupying the profiles space. By comparing 
the recorded distances before and after the removal of the 
common words, we were able to show the difference in two 
distinct lines. The space located between the two lines 
caused some document to be misclassified. Figure 5 shows 
the effect of removing common word on the distance. 
From the more comprehensive experiment we have shown 
that removing common words from documents gave up to 
90.5% correct classification. This is because we were able to 
remove unnecessary distance created by these words. 
3000 
2500 
� � 2000 1---------------------. • With 
8 
�1500 I-----------------� 
!� 
�lOOO ----------------� 
O\'erall Distance 
common 
words 
_Without 
common 
words 
Fig 4. Effect of removing common words from documents and 
categories profiles on the overall distance 
We anticipate that removing more common words and 
phrases can reduce the excess distance even more. However, 
this should be done with care without affecting the 
document classification, since removing common words 
especially nouns may directly affect the semantics of the 
document. 
C. Unrelated Topics 
We showed the feasibility of using our N-grams 
classification model with related document, where all 
documents belong to one of the six categories. However, in 
reality, DLP systems deal with both related and unrelated 
data. Therefore, we tested unrelated documents to show that 
our method can distinguish between related and unrelated 
documents. We gathered 20 documents form the topics 
"computer forensics" and "virtualization". Although these 
topics are not totally isolated from information security, the 
distances produced by the tested documents were bigger 
than distances produced by documents related directly to the 
field of information security. This approach might be used 
as an initial step to check the relevancy of a document to the 
area of interest. Figure 6 shows the recoded distances for 
unrelated documents, where all the results came above the 
"relevancy line". Also, it was noted that bigger documents 
yield bigger distance, which makes it easier to decide 
whether a specific document is relevant or not. 
Developing relevance lines for more general fields such 
as Information security, viltualization and computer 
forensics, could narrow down and accelerate the 
classification process. Moreover, considering computational 
challenges, we assume that applying this method can reduce 
systems overhead significantly. 
In addition, a relevancy map can be drawn by combining 
multiple relevancy lines. As in Figure 1, an overview of 
interconnected fields can help III better understanding of 
classification. 
700000 
600000 1--------------
500000 
� � 400000 
__ ReI.ted �300000 I--_______ �����L----
• Uorelated 
200000 
100000 I-- ----..c-- ----=--J-------
Sj�����a!�����;��ffi��� 
No.ofN-grann. 
Fig 5. Position of unrelated documents above the relevance line. 
D. Precision and Recall 
It is important to clearly understand the results of our 
experiments and identify the strengths and weaknesses. We 
considered precision and recall measures to evaluate the 
performance of our classification model. When running an 
experiment using a set of documents and categories, 
precision is the ratio between correctly classified documents 
and the total number of retrieved documents. Recall is 
defined as the ratio between correctly classified documents 
under a correct category and the number of desired correct 
classifications. By referring to Table III, we can infer that 
our method was able to achieve a high precision in two 
categories: Encryption and VPN, meaning that these 
categories were able to classify related documents only and 
nearly ignored incorrect ones. The worst precision recorded 
was for DLP, where this category was involved in 14 wrong 
classifications. A high recall was recorded for Anti virus 
and DLP categories by scoring 0.98, which means that most 
of the desired documents were correctly classified. The 
worst recorded recall score was 0.75, and was recorded for 
Encryption. This means most desired documents were 
classified incorrectly. An average of 0.91 precision and 0.90 
recall was achieved and it is considered high comparing 
with other related works such as in [6]. 
TABLE III. PRECISION AND RECALL MEASURES 
Category Precision Recall 
Antivirus 0.937 0.983 
DLP 0.819 0.983 
Encryption 0.978 0.750 
Firewall 0.821 0.917 
IDS 0.950 0.950 
VPN 0.981 0.850 
Average 0.914 0.906 
Considering the effect of updating category profiles on the 
overall classification, we suggest that testing more 
documents will update the profiles with the most relevant N­
grams. This will improve the precision and recall measures. 
Furthermore, there are some other factors affecting the 
precision measure such as shared N-grams in two or more 
categories. For example, N-grams like (packet, IP, address, 
network) can have high ranks within both VPN and Firewall 
categories, thus, incorrect classification is probable. 
Therefore, shared N-gt·ams should be handled with care to 
avoid low precision measure. 
V. LIMITATIONS 
This work is part of an ongoing research in the field of 
data leakage prevention. In particular, it aims to narrow 
down the data classification problems related to sensitive 
data leakage. Although the above results showed some 
encouraging classification performance using the N -grams, 
further improvements are still needed. This work does not 
omit the importance of using data fingerprints and file 
matching for data classification. Rather, we are trying to 
compensate these techniques when they are ineffective. It is 
also worth mentioning that our method is lacking extended 
and more comprehensive experiments. We have tested 360 
documents against six categories, from which all the results 
were obtained. Moreover, our method might not be effective 
when word synonyms and special characters are being used. 
Some authors may prefer to use unusual wordings and 
special characters to convey messages. Classifying these 
documents might be challenging without proper update to 
category profiles. Another challenging problem facing most 
data classification methods including ours is when 
cryptography is used. Classifying encrypted documents are 
considered by far the most challenging task, and without 
proper cryptanalysis tools it might be impossible to identify 
such documents. 
VI. CONCLUSION AND FUTURE WORK 
In this paper we have reported the effectiveness of using 
N-gram frequency for the purpose of documents 
classification. Our work is dedicated to the field of data 
leakage prevention to compensate eXlstmg data 
classification techniques used in DLP systems. We have 
examined various aspects of using one word N-grams to 
create profiles for documents and corresponding categories. 
For instance, we verified the effect of removing common 
words and phrases from documents on the overall 
classification. Also, we showed the importance of extracting 
X-counts from tested documents, and updating category 
profiles. By doing this, we ensured the incl usion of the most 
relevant N-grams inside a category profile. Moreover, our 
initial experiment showed improvements in the overall 
correct classification after the removal for common 
words/phrases and updating category profiles. The 
percentage of the correct classification increased from 
78.8% to 90.5%. Our method was able to correctly classify 
326 documents which gives 90.5% correct classification of a 
total number of 360 documents. In addition, we examined 
results from related and unrelated documents, and we were 
able to establish a relevancy line which can be considered as 
a detection baseline. Moreover, the method achieved high 
levels of recall and precision, as it was able to score an 
average of 0.91 precision and 0.90 recall. Comparing with 
existing methods our precision and recall for our proposed 
system are relatively high. 
In our future work we aim to use larger data sets, which 
should include documents of various topics and sensitivity 
levels. We will examine the ability to establish relevancy 
lines for more generic topics such as medicine, sports, 
business etc. This can help in accelerating the classification 
process by immediately eliminating unrelated documents 
and considering only related ones. Also, we showed 
relationships between some categories like Encryption with 
DLP and Firewall with VPN. These relationships resulted in 
many incorrect classifications and low precision because of 
shared N-grams. To avoid such problem, we are aiming to 
treat common N-grams as neutral, where shared N-grams 
among two or more categories can be ignored. 
REFERENCES 
[I] InfoWatch. Global Data Leakages & Insider Threats Report: 
20 11 [Online]. Available: 
hUp :Iii nfowatch .com/ si tes/ defaul tlfi I es/reportiln fo Watc h _global 
_data_leakageJeporc2011.pdf 
[2] R. Mogull. Understanding and Selecting a Data Loss Prevention 
Solution [Online]. Available: 
hups:/ / securosi s. com/assets/Ii brary /reports/D LP -Whi tepaper. pd f 
[3] L. A. ALON KANTOR, YOA V KIRSCH, URI BIALIK, 
"Methods For Document-To-Template Matching For Data-Leak 
Prevention," USA Patent US20 I 00254615 A I, 20 I O. 
[4] X. Shu and D. D. Yao, "Data Leak Detection As a Service," in 
Security and Privacy in Communication Networks, ed: Springer, 
2013, pp. 222-240. 
[5] J. Kornblum, "Identifying almost identical files using context 
triggered piecewise hashing," digital investigation, vol. 3, pp. 
91-97,2006. 
[6] L. Khreisat, "Arabic text classification using N-gram frequency 
statistics a comparative study," in Conference on Data Mining 1 
DMIN'06 I, 2006, p. 79. 
[7] S. Zhou and J. Guan, "Chinese documents classification based 
on N-grams," Computational Linguistics and Intelligent Text 
Processing, pp. 31-50, 2002. 
[8] Z. Su, Q. Yang, Y. Lu, and H. Zhang, "Whatnext A prediction 
system for web requests using n-gram sequence models," in 
Web Information Systems Engineering, 2000. Proceedings of 
the First International Conference on, 2000, pp. 214-221. 
[9] R. Moskovitch, D. Stopel, C. Feher, N. Nissim, and Y. Elovici, 
"Unknown malcode detection via text categorization and the 
imbalance problem," in Intelligence and Security Informatics, 
2008. lSI 2008. IEEE International Conference on, 2008, pp. 
156-161. 
[10] E. Stamatatos, N. Fakotakis, and G. Kokkinakis, "Text genre 
detection using common word frequencies," in Proceedings of 
the 18th conference on Computational linguistics-Volume 2, 
2000, pp. 808-814. 
[II] M. Hart, P. Manadhata, and R. Johnson, "Text classification for 
data loss prevention," in Privacy Enhancing Technologies, 
2011,pp. 18-37. 
[12] M. Sokolova, K. EI Emam, S. Rose, S. Chowdhury, E. Neri, E. 
Jonker, and L. Peyton, "Personal health information leak 
prevention in heterogeneous texts," in Proceedings of the 
[13] 
[14] 
[15] 
[16] 
[17] 
[18] 
[19] 
[20] 
Workshop on Adaptation of Language Resources and 
Technology to New Domains, 2009, pp. 58-69. 
W. B. Cavnar and 1. M. Trenkle, "N-gram-based text 
categorization," Ann Arbor MI, vol. 48113, pp. 161-175, 1994. 
E. Stamatatos, "Intrinsic plagiarism detection using character n­
gram profiles," threshold, vol. 2, pp. 1,500, 2009. 
G. K. Zipf, Human behavior and the principle of least effort. 
Massachusetts: Addison Wesley, 1949. 
C. E. Landwehr, C. L. Heitmeyer, and 1. McLean, "A security 
model for military message systems," ACM Transactions on 
Computer Systems (TOCS), vol. 2, pp. 198-222, 1984. 
Wikipedia. Available: http://www.wikipedia.org/ 
W. H. Fletcher. (2012). kjNgram (J.3.1 ed.). Available: 
http://kwicfinder.comlktN gramlktN gramHelp.html 
S. Alneyadi, E. Sithirasenan, and V. Muthukkumarasamy, 
"Word N-gram Based Classification for Data Leakage 
Prevention," in TrustCom, Melbourne (in press), 2013. 
(2013). The DEe: Facts about the language. Available: 
http://oxforddictionaries.com/words/the-oec-facts-about-the­
language 
