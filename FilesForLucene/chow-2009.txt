IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 20, NO. 9, SEPTEMBER 2009 1385
Multilayer SOM With Tree-Structured Data
for Efficient Document Retrieval and
Plagiarism Detection
Tommy W. S. Chow and M. K. M. Rahman
Abstract—This paper proposes a new document retrieval (DR)
and plagiarism detection (PD) system using multilayer self-orga-
nizing map (MLSOM). A document is modeled by a rich tree-struc-
tured representation, and a SOM-based system is used as a compu-
tationally effective solution. Instead of relying on keywords/lines,
the proposed scheme compares a full document as a query for per-
forming retrieval and PD. The tree-structured representation hi-
erarchically includes document features as document, pages, and
paragraphs. Thus, it can reflect underlying context that is diffi-
cult to acquire from the currently used word-frequency informa-
tion. We show that the tree-structured data is effective for DR and
PD. To handle tree-structured representation in an efficient way,
we use an MLSOM algorithm, which was previously developed by
the authors for the application of image retrieval. In this study,
it serves as an effective clustering algorithm. Using the MLSOM,
local matching techniques are developed for comparing text docu-
ments. Two novel MLSOM-based PD methods are proposed. De-
tailed simulations are conducted and the experimental results cor-
roborate that the proposed approach is computationally efficient
and accurate for DR and PD.
Index Terms—Document retrieval (DR), multilayer self-orga-
nizing map (MLSOM), plagiarism detection (PD), tree-structured
representation.
I. INTRODUCTION
T HE easy access to the Internet has made disseminatingknowledge across the world much easier. Documents
can easily be searched, copied, saved, and reused for different
purposes. Document retrieval (DR) and categorization has also
become important in facilitating the handling of large number of
documents. As a result, plagiarism detection (PD) has become
increasingly important whenmore information can be electroni-
cally exchanged, reused, and copied. DR refers to finding similar
documents to a given query. Document categorization is another
text-related application, where a given document is required to
be labeled with a class or classes with an automated system to
reduce human labor. Categorization task can be performed by
Manuscript received June 26, 2008; revised October 15, 2008 and February
02, 2009; accepted April 21, 2009. First published July 28, 2009; current version
published September 02, 2009. This work was supported by the City University
of Hong Kong under Grant 7001998-570.
T. W. S. Chow is with the Department of Electronic Engineering, City Uni-
versity of Hong Kong, Kowloon, Hong Kong (e-mail: eetchow@cityu.edu.hk).
M. K. M. Rahman was with the Department of Electronic Engineering, City
University of HongKong, Kowloon, HongKong. He is nowwith theDepartment
of Electrical and Electronic Engineering, United International University, Dhaka
1209, Bangladesh (e-mail: masuk@eee.uiu.ac.bd).
Color versions of one or more of the figures in this paper are available online
at http://ieeexplore.ieee.org.
Digital Object Identifier 10.1109/TNN.2009.2023394
machine learning tool such as support vector machine (SVM)
[1]. SVM performs the classification task by constructing hy-
perplanes in a multidimensional space that separates different
classes [2]. The extracted class information of documents can be
utilized inDR system.Other thanmatching text content, DR also
uses rank information of documents such asGoogle’s PageRank.
Ranking of document can be performed with SVM-based inter-
active retrieval systems that learn fromusers’ relevance feedback
[3]. In [4], the problem of DR is formulated as binary classifi-
cation using SVM, where document collection is classified into
relevant or nonrelevant; the relevant documents are then returned
in the retrieval result. Much research work on DR has been
reported in the literature but further improvement in the way of
speeding up the retrieval time and increasing the accuracy has
always beenwelcome. On the other hand, plagiarism is the act of
copying text from one’s work without permission or proper ac-
knowledgement.ResearchworkonPD is scarce.Currently, there
are several approaches of conducting document searching. Most
of the traditional methods are “keywords”-based searching,
e.g., Google, which relies on the keywords provided by users.
The search engine usually provides ranked documents mostly
based on the occurrence of the keywords. Searching similar
documents for a query document is another type of DR. It is
flexible to users, because a query document may consist of few
keywords, paragraphs, and so on. Thus, retrieval by a query
document is powerful but computationally complex compared
with the “keyword-based searching” method. In addition to
retrieval, there are search engines, e.g., Vivisimo, that are able to
cluster retrieved documents [5] into different groups. While DR
facilitates knowledge sharing immensely, it makes partly or fully
copying of others’ work into one’s documents easy. PD [6]–[9]
is a research area that is closely related to DR.With the immense
growth in document collections, PD is difficult because it is
not easy to develop an effective and computationally efficient
method that can model a document accurately. Currently, most
models use crude document features, but they are usually
unable to detail a document description. Thus, there is a need to
improve document-feature representation in order to improve
the performance of DR and PD.
Traditional DR methods rely on term-based related models
[10]–[14] such as Boolean, probabilistic, language, and vector
space model (VSM). Boolean model uses Boolean operators on
query words. It is simple but unable to provide any ranking of
the retrieved documents. Probabilistic model uses the idea of as-
signing a relevance score to each term of a document, whereas
the score is related to the occurrence of that term in some known
1045-9227/$26.00 © 2009 IEEE
1386 IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 20, NO. 9, SEPTEMBER 2009
relevant documents. Apart from these models, language model
[14], a statistical method of ranking of documents, has become
quite popular. VSM model, the most popular and widely used
comparedwith others models, counts the frequency of each term
of a vocabulary for a given document, where vocabulary is a
list of terms/words used for feature description. A term weight
vector is then constructed using this “term frequency” together
with “document frequency,” where document frequency is the
number of documents where the term appears. Similarity be-
tween two documents is performed by using “cosine” measure
or any other distance function [11]. VSM model thus requires
a lengthy vector, because the number of words involved is usu-
ally huge. This causes a significant increase in computational
burden making the direct use of VSM model impractical for
handling a large data set. To overcome this limitation, latent se-
mantic indexing (LSI) [15] was developed to compress the fea-
ture vector of the VSM model into low dimension. In addition
to feature compression, LSI model is useful in encoding the se-
mantics [16], [17]. Besides the LSI, self-organizing map (SOM)
was employed for document-feature projection [18], [19] and
dimensionality reduction. In [20], LSI is used to encode the
semantic concept to some extent. In [21], it was suggested to
use information extraction (IE) to improve DR systems perfor-
mance. According to [22], DR is to retrieve relevant documents
from database, and IE is to extract relevant information from the
retrieved documents. IE has been developed through a series of
Message Understanding Conferences [22]. Other content such
as image has also been included to enhance the retrieval system
[20]. In [23], thesaurus is added to widen the searching space
by incorporating similar words to the query words. It is useful
in some extent because a semantic context can be represented
by different sets of words. It shows that using domain specific
thesaurus is more effective than using generic thesaurus on its
own.
It is worth noting that all the above described methods use
flat feature representation. In fact, flat feature representations
are a crude representation of a document. For example, two doc-
uments containing similar term frequencies may be of different
contextually when the spatial distribution of terms is very dif-
ferent, i.e., school, computer, and sciencemean different things
when they appear in different parts of a page compared to school
of computer science that appear together. Thus, it is important
to account contextual similarity based on the ways that words
are used throughout the document, i.e., sections and paragraphs.
But, until now, most document models incorporate only term
frequency and do not include such contextual information. Tree
structure, a rich data representation, is found to be effective in
including the spatial information. Details of tree structure will
be elaborated on in later section of this paper.
Speeding up the retrieval operation is also an equally impor-
tant issue. In [24], it was suggested that clustering of large doc-
ument data set could be useful, because it narrows the searching
scope by comparing a query to a group of documents that are
clustered according to their nature. The searching effort is re-
duced compared with the mechanism of searching the whole
database. Fuzzy concept [25] together with clustering technique
[26] is also used in DR. Other than clustering, researchers also
proposed a newfile system [27] to speed up the retrieval process.
SOM is a versatile unsupervised neural network used for gen-
erating a topologically ordered map and clusters. Based on a
topologically ordered map of documents, SOM facilitates users
to find similar documents that are close to each other on the
SOMmap. SOM is computationally efficient in performing fast
DR [28], because it only compares a query document with neu-
rons instead of all documents in the database. There are vari-
ants of SOM, e.g., WEBSOM [18] and latent semantic indexing
and SOM (LSISOM) [15], that are used for document visualiza-
tion and interactive browsing. Another interesting application of
SOM for hierarchical document organization and browsing can
be found in [5] and [29]. The topological organization of content
(TOC) method [5], which can be seen as hierarchical clustering,
spans a set of 1-D growing SOMs. It is computationally effec-
tive and scalable for large data set. Despite all the successes, all
these approaches rely on typical term-frequency information.
In order to encode a document in a better way, we propose to
use tree-structured representation. Tree structure is a powerful
approach that has been successfully applied to numerous appli-
cations of pattern recognition [30]–[34]. We developed multi-
layer SOM (MLSOM) [35] for handling unsupervised learning
of tree-structured data, which was used in clustering, visualiza-
tion, retrieval, and classification [32], [35]. In this paper, we use
MLSOM for handling DR and PD.
Most traditional DRs are performed using the global char-
acteristics of a document, i.e., overall term-frequency informa-
tion. On the other hand, copying a small part of text is regarded
as plagiarism, although it does not make the “plagiarized doc-
ument” globally more similar to the “source document.” There
is a need to analyze two documents locally for detecting plagia-
rism in contrast with global similarity analysis in the retrieval
process. Plagiarism can be of different types in nature, from
copying text to copying idea [36]. The scope of our present work
focuses on copying text with or without minor modification of
the words and grammar. According to [36], there exist three dif-
ferent broad categories of detecting textual plagiarism: 1) com-
paring the query over a database [7]–[9], 2) comparing core sen-
tences/phrases through a search engine [37], and 3) comparison
by stylometric analysis [38]. Most methods fall within the first
category where detection is performed by dividing documents
into small blocks such as a paragraph [9], a sentence [7], or
a word [8]. The matching between two documents is carried
out based on the overlapping of those blocks. These blocks are
registered against a hash table. Thus, all documents of a data-
base are hashed in a modular form. A query document is sim-
ilarly divided into small blocks that are subsequently matched
in the hash table for PD. In fact, most of the existing systems
fail to detect plagiarism by paraphrasing (changing words of a
copied paragraph) [36], because they do not account the overlap
when plagiarism is done by changing words and structure of
sentences. Overlap measure can be carried out in different ways.
In [39], a method based on linguistically motivated plagiarism
patterns was proposed that allows partial match when sentences
are made different by changing words or structure. To scale
up PD for a large database, a string-matching fingerprint-based
system [40] was developed. It calculates the document finger-
print from character level instead of words. The process is sped
up but at the expense of losing semantic information. Another
CHOW AND RAHMAN: MULTILAYER SOMWITH TREE-STRUCTURED DATA FOR EFFICIENT DR AND PD 1387
string-matching algorithm can be found in [6]. It uses suffix
trees to detect plagiarism at an enhanced speed. It requires less
space, but suffix trees [6] do not scale well and the algorithm is
only applicable to a small data set. Most of these approaches are
only suitable for a word-by-word matching over a limited docu-
ment collection. According to [9], most existing copy detection
mechanisms employ an exhaustive sentence-based comparison
to match a potential plagiarized document with all the registered
documents. This approach is not scalable due to the potentially
large number of registered documents and the large number of
sentences in each document. Also the security level of the ex-
isting mechanisms is quite weak; a plagiarized document could
easily bypass the detection mechanism by performing a minor
modification on each sentence. In [9], a tree-structured doc-
ument feature was proposed and the method was further im-
proved in [41]. But the method uses a nonnumeric feature rep-
resentation making it unable to utilize SOM or other clustering
technique to speed up the handling of a large database.
This paper proposes a novel approach using a three-level tree
representation and MLSOM. Document contents are hierarchi-
cally organized into different layers of a SOM, i.e., paragraphs
are organized at the bottom layer, pages are organized at the
middle layer, and the whole documents are organized at the top
layer. Different layers play different roles; the top layer per-
forms document clustering, and the other layers compress local
features. The global and local document characteristics orga-
nized at the top and bottom layers are essential for DR and PD,
respectively. The system also supports a fast relevance feedback
that improves the retrieval accuracy significantly.
In summary, the contribution of this paper is twofold. First,
we propose a tree-structured document model for DR and PD.
The model enables global and local characteristics of a docu-
ment to be included in a hierarchical way. Thus, it enhances
DR accuracy and enables PD. Second, we present an efficient
system for both DR and PD, where MLSOM [35] serves as a
computationally efficient clustering algorithm. The method can
serve as a unified platform for performing both DR and PD.
Also, it should be noted that the described DR mechanism does
not rely on comparing keywords/lines; it relies on comparing
the text of the whole document. The rest of this paper is orga-
nized as follows. Section II presents the details of document fea-
ture representation and feature extraction. Section III provides
the details of the MLSOM training. Section IV describes how
DR and PD are performed. Experimental results are presented
and analyzed in Section V. The conclusion is finally drawn in
Section VI.
II. STRUCTURED REPRESENTATION OF DOCUMENT FEATURES
To extract the tree structure, a document is partitioned into
pages that are further partitioned into paragraphs. We have
developed a Java code to perform such segmentation, and have
considered only “html” documents at this stage. In “html”
format document, paragraphs can be easily identified using
html tags. First, a document is segmented into a number of
paragraphs. The paragraphs are then merged into pages using a
minimum threshold value for the number of words on a page.
It is noted that any text appearing within the html tags, which
are used for formatting, is not accounted for word count or
document feature extraction.
The document partitioning process can be summarized as
follows.
1) Partition the document into paragraph blocks using the
html paragraph tag “ ” and new line tag “ .”
2) Merge the subsequent paragraph blocks to form a new page
until the total number of words of the merged blocks ex-
ceeds a page threshold value 1000. There is no minimum
threshold for the last page. The page blocks are formed.
3) Now each page is split into a smaller blocks using more
html tags: “ ,” “ ,” “ ,” “ ,” etc. Merge these
subsequent blocks in the same fashion as in step 2) to
form a new paragraph until total number of words of the
merged blocks exceeds a page threshold value 100. The
minimum threshold for the last paragraph of a page is kept
at 40; otherwise the paragraph is merged with the previous
paragraph.
In html documents, there is neither definite page boundary/
break nor any rule for the minimum/maximum number of words
for paragraphs/pages. But the use of a threshold of a word count
enables us to form a hierarchical structure containing the char-
acteristics from global to local. The contents are structured in a
way of “document pages paragraph” tree. This is an ef-
fective way of generating a tree structure, and it can be further
improved by using a form of “document sections para-
graphs” or “document sections sentences,” but it demands
more complex algorithm for partitioning a document.Fig. 1(d)
illustrates the three-level tree-structured representation of a doc-
ument. The root node at the first level represents the whole doc-
ument, the second level nodes represent different pages, and
the third level nodes represent paragraphs of the pages. Nodes
contain compressed features describing frequency distribution
of different words. Nodes at different levels contain the same
word-frequency features, but they are extracted from different
contents of the document. Two documents having similar word
histograms at root nodes can be completely different in terms of
semantics/context, because different spatial distributions of the
same set of words can result in different meaning/context. This
is what is reflected by the discriminative lower parts of the tree
data (second/third level nodes).
Tree-structured data can be used effectively for both DR
and PD. For DR, similarity between two documents can be
measured using the whole tree, where root nodes give us global
similarity and the third-level nodes provide local similarity.
Tree structure can also provide information of plagiarism
through the third-level nodes which encode the local spatial
information of word distribution. To extract the features, word
histograms are computed for the nodes at different levels.
We then apply principal component analysis (PCA), a tool
to project higher dimensional data into lower dimensional
feature without losing much statistical information, to the
word-histogram vector. Fig. 1(a)–(c) details the block diagrams
for extracting tree-structured features. In preprocessing, word
extraction, vocabulary construction, and generation of PCA
projection matrix are performed. Then, a document is portioned
into a three-level tree, and nodes’ features are computed using
the feature–extraction module shown in Fig. 1(a). The overall
1388 IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 20, NO. 9, SEPTEMBER 2009
Fig. 1. (a–c) Block diagrams of feature extraction procedures. (d) Document representation. (a) Feature extractionmethod (FEM). (b) Preprocessing. (c) Extraction
of tree-structured feature. (d) Document representation by tree-structured feature.
procedure of extracting tree-structured feature can be summa-
rized as follows.
1) Extract words from all the documents in a database and
apply stemming to each word. Porter stemming algorithm
[42] is used to extract stemof eachword, and stems are used
for feature extraction instead of original words. In this way,
“work,” “works,” and “worked” are all considered being the
same word. Remove the stop words (set of common words
such as “the,” “is,” etc.). Store the stemmed words together
with the information of term frequency (the frequency
of a word in all documents) and the document frequency
(the number of documents where a word appeared).
2) Vocabulary construction: Using term frequency and
document frequency , we calculate the weight of each
word that is similar to the term weighting in VSM [43]
(1)
where the inverse document frequency
, and is the total number of documents in the
whole database. We sort the words by using weight value
in descending order and select the first words. Alterna-
tively, one can select words having a weight value above a
threshold. The choice of depends on the database.
CHOW AND RAHMAN: MULTILAYER SOMWITH TREE-STRUCTURED DATA FOR EFFICIENT DR AND PD 1389
3) Partition the document into pages and pages into para-
graphs. Construct the document tree. The root node con-
tains the histogram of a whole document, the second-level
nodes are used for pages, and the third-level nodes are used
for paragraphs. Besides the histogram features, a set of in-
formation is saved for each node that describes how the
nodes are related to each other. They include: 1) a node
index, 2) the level in tree, 3) the parent node index, and
4) the child node index.
4) For each document, calculate word histograms
for paragraphs, where is
the number of times the corresponding word appears in
a paragraph. The histograms for the pages and the whole
documents can be obtained by hierarchical relationship
and ,
where and are histograms corresponding to a
document and a page, respectively. Finally, we normalize
the histogram
(2)
where is a normalization function. The histogram
vector can be normalized in a number of ways to form the
normalized histogram vector
Mean norm: (3)
Max norm: (4)
VSM norm: (5)
5) Use the normalized histogram to construct the PCA projec-
tion matrix . To save the computational burden, we con-
struct the matrix only at the root nodes.We have used the
MATLAB tool [44] to compute the PCA projection matrix.
6) Project the node features (normalized histogram) into
lower dimensional PCA feature by using PCA projection
matrix [44]
(6)
where is the projection matrix of dimension , and
is the dimension of the projected feature. It should be
noted that the histogram features for the first-, second-,
and third-level nodes are extracted from a document, a
page, and a paragraph, respectively. The projected features
in are ordered according to
their statistical importance. In our study, was set to
200, but we further reduced the number of features used
for the root node and second-level nodes. Thus, the new
dimensions of the PCA feature for the first-, second-, and
third-level nodes are 100, 150, and 200, respectively.
7) Save the vocabulary base and the projection matrix for
making features of a new query document. The tree-struc-
tured features of a query document are extracted in the same
way [see feature extraction method (FEM) in Fig. 1(a)] but
excluding steps 1), 2), and 5), because they are required to
compute only onceover thedatabase. Thus, negligible com-
putational time to segment a query document and compute
PCA feature using the projection matrix is required.
III. SOM TRAINING
A. Self-Organizing Map
A basic SOM consists of neurons located on a regular
low-dimensional grid that is usually in 2-D. The lattice of a 2-D
grid is either hexagonal or rectangular. Each neuron has a -di-
mensional feature vector . The SOM algo-
rithm is iterative. During the training process, the neurons are
updated in a way that their feature vectors finally become rep-
resentative of different data clusters in an orderly fashion. The
iterative SOM training algorithm can be stated as follows.
Step 1. Set iteration .
Step 2. Randomly select a sample data vector from a
training set.
Step 3. Compute the distances between and all feature
vectors. The winning neuron, denoted by , is the neuron with
the feature vector closest to
(7)
is a similarity function to compute similarity between
and .
Step 4. Update the winner neuron and its neighbor neurons.
The weight-updating rule in the sequential SOM algorithm
can be written as
otherwise.
(8)
is the learning rate which decreases monotonically with
iteration
(9)
where is the initial learning rate, and is an exponential
decay constant set to 3 throughout our experiments. is a set
of neighboring neurons of the winning neuron, and is
the neighborhood kernel function that defines the closeness of
a neighborhood neuron to the winning neuron at position
. The neighborhood function also decreases
gradually during the learning process
(10)
where is the width of the neighborhood function that
decreases with iteration
(11)
where is the initial width and is a time constant set to the
maximum number of iterations.
Step 5. Stop when the maximum iteration is reached, or set
and go to step 2.
1390 IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 20, NO. 9, SEPTEMBER 2009
Fig. 2. (a). Illustration of the mapping of a three-level tree-structured data into a three-layer MLSOM. (b) SOM input representation for the node A. (c) Block
diagram illustrating mapping of a tree-structured data into MLSOM.
B. Multilayer SOM (MLSOM)
Conventional SOM cannot represent tree-structured data be-
cause the SOM input is only in a form of a flat vector. MLSOM
[35], a kind of extended SOM model, was developed for pro-
cessing tree-structured data. Its multilayered structure is spe-
cially designed to represent nodes of a tree data in a way of
level-by-level. A tree data consists of nodes at different levels;
a node consists of two types of information including the node
features and its child nodes. In an MLSOM, there are as many
SOM layers as the number of levels in the tree, i.e., three dif-
ferent SOMs for a three-layer MLSOM. Nodes of each level are
processed by the corresponding layer of the MLSOM. Fig. 2(a)
illustrates how a three level tree data is mapped into a three-layer
MLSOM. First, the third-level nodes, containing features of dif-
ferent paragraphs, are mapped to the layer 3 SOM output. The
winner neurons of different nodes or paragraphs ( , , and )
are represented by their corresponding position vectors ( , ,
and ). It is worth noting that at layer 3 there are huge numbers
of child nodes representing the identity of all paragraphs. Using
the layer 3 SOM, we are able to compress all the paragraphs
into 2-D position vectors. Thus, the layer 2 SOM input vector
consists of the second-level node (page) features and the posi-
tion vectors. In this way, we can form a compressed input vector
CHOW AND RAHMAN: MULTILAYER SOMWITH TREE-STRUCTURED DATA FOR EFFICIENT DR AND PD 1391
for layer 2 SOM. Fig. 2(b) depicts how the second-layer input
vector is formed using the feature of a node and the position
vectors , , and . Fig. 2(b) also shows how the position
vectors are mapped from the outputs of the layer 3 SOM into a
structured format forming the input vector of the layer 2 SOM.
At last, the root nodes are similarly processed at layer 1 SOM.
As the root node represents the whole document tree, all docu-
ments are thus organized and associated with different winner
neurons at the top layer SOM. It should be noted that the third-
and second-layer SOMs serve as feature compression that com-
press the paragraph and page information, respectively. In this
way, the whole tree can be compactly represented by the root
node at the first-layer SOM. The whole procedures are summa-
rized in the block diagram shown in Fig. 2(c).
In the tree data, a node at the th level is represented by
, where
represents the th feature of node ,
is a normalized 2-D position vector of a
neuron that compactly represents th child node, and
is the maximum number of child nodes of the nodes at the
th level. The position vectors are mapped in the vector
according to spatial position that will be
discussed later in this section. A node may have less than
number of child nodes, and some of may contain zero vector
. Theweight vector of a neuron at the th layer is repre-
sented by ,
where is the th weight and is a 2-D vector.
The following function computes the similarity between a node
and a neuron in finding the winner neuron:
where
if
otherwise
(12)
where and are weight parameters, and is a Euclidean
distance function. The first part of the expression computes the
global similarity using the node feature, and the secondpart com-
putes the local similarity using position vectors of child nodes. In
the bottom-level nodes, only the first part of (12) is used. It was
noted that provides weighting among different child nodes. It
is set to 1 by default, and is only modified during relevance feed-
back. In the first part of (12), put relative emphasis between the
global and local similarity measure. A larger value of , ,
implies that emphasis isplacedonglobal information, andasmall
value of , , implies that local similarity is emphasized in
comparingdocuments.The systemprovidesflexibility tousers to
change thevalueof tobalance thesimilaritymeasureaccording
to their expectation. The effect of will also be discussed in later
section of this paper.Fig. 3 illustrates the block diagrams of the
MLSOM training. First, the third-layer SOM is trained with
third-level nodes. Then, the inputs of the second-level nodes are
formed by combining the nodes’ features and position vectors
of the corresponding child nodes [Fig. 2(b)]. Using these SOM
Fig. 3. Training sequences of MLSOM for the third-level tree-structured data.
inputs, the second-layer SOM is trained. Similarly, the top-layer
SOM is trained by the SOM inputs for the root nodes. The
whole training procedures are described as follows.
Training Procedures of the MLSOM
(1) Initialize SOM for all layers
(2) Loop from the third to first layer
(3) Select all the nodes of all tree data at the same level
as the SOM layer
(4) Create an input vector for each node by combining
node’s feature with mapped child nodes’
position vectors.
(5) Loop for training SOM
(6) Randomly select an input vector
(7) Find a winner neuron for the input vector on
the current SOM layer
(8) Update the current SOM layer
(9) End of training loop
(10) For each node, find the final winner neuron on the
current SOM layer and save its position vector that
will be used in the input of its parent node.
(11) Unless it is the top layer, train a 1-D SOM using all
position vectors on current level. This will be used to
map position vectors in step (4).
(12) End of layer loop
1392 IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 20, NO. 9, SEPTEMBER 2009
In step (4), we map the position vectors of child nodes into
the SOM input vector. The mapping of position vectors is con-
ducted using a simple 1-D SOM that is trained in step (11).
The mapping is required because it needs to make a meaningful
matching between two sets of nodes using the second part of
(12). It compares a child node of the first set with only one sim-
ilar child node of another set. By performing this procedure,
the two position sets can be compared by a speedy and mean-
ingful “one-to-one” matching, instead of by a “many-to-many”
matching [32]. The 1-D SOM is trained by all the position vec-
tors over the database at a particular level. After
training the 1-D SOM, the following procedures are applied to
map a set of position vectors.
Mapping of Position Vectors Using
SOM of Neurons
Set ,
Loop for each ,
Find three most matched neurons , , and for
If , and
else if , and
else if , and
else and
end if
End
Return
The above procedures differ from [32] in a way that two
or more position vectors in these procedures can be merged
together by averaging. Thus, is set to a value that is lower
than the actual maximum number of child nodes. The length
of the input vector that represents child nodes is then reduced,
which results in a reduction of the dimension of SOM weight
vector. The decrease of is useful when the maximum
number of child nodes in the database is large. The computa-
tional complexity of conducting one epoch of MLSOM training
is , where is the number of
nodes at the th level of trees, is the number of neurons at
the th SOM layer, is the number of input features of a node
at the th level, and is the maximum number of children
nodes of all the trees at the th level. Also, it must be noted that
the computational cost of the 1-D SOM is negligibly small.
IV. MLSOM-BASED DOCUMENT RETRIEVAL
AND PLAGIARISM DETECTION
A. Preprocessing
The preprocessing for DR can be summarized as follows.
MLSOM Preprocessing
1) Train the MLSOM with all the tree-structured data of the
database.
2) For the top-layer SOM, save the index of document data
against their activated neurons. This will be used in the DR.
3) For the third-layer SOM, save the index of documents to-
gether with their node index (in the tree structure) against
their activated neurons. This will be used in the PD. No
preprocessing is required for the second-layer SOM.
4) Save the weight matrix of the MLSOM network and 1-D
SOM used for mapping. Save the root node inputs of all
documents constructed during training, which will be used
for relevance feedback. Thus, the MLSOM, together with
previously saved vocabulary base and PCA projection ma-
trix, is ready to perform retrieval operation.
B. Document Retrieval and Relevance Feedback
A document is overall represented by the root node of its tree
structure; the root nodes are processed at the top-layer SOM.
Each document is indexed against its winner neuron at the top
layer. This document association, which has been carried out
in the preprocessing stage, is used for performing retrieval. The
MLSOM-based retrieval system is summarized as follows.
MLSOM Retrieval
1) For a given query document, extract its tree structure. Cal-
culate node features using the prestored vocabulary base
and PCA projection matrix.
2) Match the nodes of the tree level-by-level from the bottom
layer to the top layer, and find the most matched neurons
on the top layer.
3) Go through the sorted neurons in descending order and
append their associated documents into the retrieval list
until at least documents are found. is the number of
documents to be retrieved and is defined by users. Upon
including documents from the last neuron, total number of
documents can be higher than .
4) Sort the documents of retrieved list according to query
through comparing root node inputs by (12) and return the
first documents to users. This sorting helps to deliver
better ranking in the retrieval results in terms of relevancy.
To provide additional support of relevance feedback, the
system interface allows users to browse through documents.
Users can select documents, which are more relevant to their
target, as feedback to the system. The relevance feedback
uses both query modification and modifying weight. First, the
weight parameter is modified as follows:
(13)
(14)
(15)
where and are the number of relevant and irrelevant
documents, respectively, in the retrieved list. Equation (13)
incorporates the effect of positive and negative feedback from
relevant and irrelevant documents, respectively, while (14)
and (15) are used for normalization. Equation (13) can be
explained as follows. As the position vectors are mapped
CHOW AND RAHMAN: MULTILAYER SOMWITH TREE-STRUCTURED DATA FOR EFFICIENT DR AND PD 1393
according to the spatial position of the map, each element in
reflects certain types of nodes. In other
word, they represent cluster centers of SOM units. Frequent
presence of a nonzero position vector among the relevant doc-
uments implies a need to incorporate a higher weight for that
particular position vector, and vice versa. In this study, root node
features are not used for weight-based relevance feedback, be-
cause they do not provide promising results with the relevance
feedback. The relevance feedback is also enhanced by using
query modification [32]. The modified query data
is obtained by averaging all the root node inputs
of the
query and relevant documents
(16)
(17)
where is the previously defined binary function in (12), is
the number of relevant documents, is the root node input
of the th relevant document, is the root node input for the
query document, and is the first position vector of . Using
the modified query input vector , DR is performed by the
following procedures.
Procedure of Document Retrieval Using RF
1) Match the modified query input vector with the neu-
rons of the top SOM layer using new in (12).
2) Sort the neurons of the top layer in descending order ac-
cording to similarity with .
3) Go through the sorted neurons in descending order and
append their associated documents into the retrieval list
until at least documents are found. is the number of
documents to be retrieved as defined by users.
4) Sort the documents (bymatching their root node input with
). Return the first documents to users.
It is worth noting that the above described relevance feedback
procedures can be completed in a single or several iterations.
C. Plagiarism Detection Using Retrieval
PD can be carried out in two ways. In the plagiarism de-
tection using retrieval (PDR) method, we use the top layer to
retrieve documents in the usual DR operation. This restricts
the number of possible document candidates to a much smaller
number compared with the total number of documents in the
database. We are then able to make local comparison using a
paragraph-to-paragraph similarity to detect plagiarism. This
method relies on DR in which the “source document” shares a
contextual topic similar to plagiarized document. The “source
document” can then be confined in the retrieval list. The PDR
method is summarized as follows.
1) Extract the tree-structured data of the query document.
2) Retrieve the most relevant documents through normal DR
process. User can assign a low value of in (12) for this
task to emphasize local similarity.
3) Compare the query to the retrieved documents using third-
level nodes. Comparison of two documents using the third-
level nodes can be performed in several ways. The plagia-
rism distance between the query document and the can-
didate document is defined as
(18)
where are paragraph features (from the third-level
nodes) from documents . In (18), the func-
tion detects the best match of a query paragraph . The
detection results obtained from all paragraphs of the query
document are then summed up by the function. We use
the following three functions in our experiments:
if
otherwise
(19)
if
otherwise
(20)
(21)
where
if
otherwise
and is the number of paragraphs in query document .
Here, is an offset value for detecting plagiarism, and is
a predefined large value for function when no match is
found.
4) Find the candidate documents having a value lower than
. Sort them according to ascending value of and return
them to the user.
Besides the above detection process, a simple retrieval can be
performed in case that we do not want to filter results by setting
offset value of . To perform a retrieval by using local sorting,
simpler functions , , and
are used.
D. Plagiarism Detection Using Local Association
In the plagiarism detection using local association (PDLA)
approach, the bottom layer is used for directly retrieving the
suspected documents without taking global similarity between
the two documents into account. Paragraphs of all documents
in the database are clustered in the bottom layer. In the PDLA
method, we compare the paragraphs of the query document
1394 IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 20, NO. 9, SEPTEMBER 2009
with the neurons to retrieve the possible candidates of “source
documents” of plagiarism. The procedures are summarized as
follows.
1) Extract the tree-structured data of the query document. Say
it has number of nodes in the third level.
2) For each of the third-level nodes, find the best matching
neuron. Stack index of associated document data together
with index of third-level nodes in the tree. Thus, a separate
list is made for each of the nodes. In the case that no
associated document is found in the neuron, the node is
ignored.
3) Using the list of each node, compute the distance be-
tween the third-level node of the query and that from the
associated documents
(22)
4) Make a new list by merging the document indexes from
number of lists generated from the third-level nodes of
query, and their corresponding value. In this combined
list, duplicate presence of an index is possible because of
multiple matches among the third-level nodes from the
query and an associated document.
5) Remove the index of a document having a value above
an offset .
6) Find the duplicate indexes, and merge them by computing
a new value of as
(23)
where is the number of duplicates found. In (23),
the plagiarism distance is reduced, because multiple
matches are found between the query and an associated
document.
7) Sort the list according to ascending value of and return
the associated documents.
These two detection methods are implemented in our study for
thorough comparisons. The choice for the value of and de-
pends on the feature value of third-level nodes. We will discuss
both and in detail in the next section.
V. EXPERIMENTAL RESULTS
The document database, named “Html_CityU1,” consists of
26 categories. Each category contains 400 documents making
a total of 10 400 documents. There are 1040 test documents
randomly selected from the 26 categories, i.e., 26 40. The re-
maining 9360 documents were used for training. In order to pro-
vide a real-life and demanding testing platform, we have estab-
lished this database with document size ranged from one page to
about 200 pages, and few hundred words to over 20 000 words.
For each of the 26 categories, 400 documents are retrieved from
Google using a set of keywords. The set of keywords for a
category is different from that of the others, but some of the
keywords may be shared among different categories.1 After the
MLSOMwas trained, the test set was used to verify the retrieval
1The database can be downloaded from www.ee.cityu.edu.hk/~twschow/
Html_CityU1.rar
TABLE I
DISTRIBUTION OF NODES IN DATA SETS
accuracy. Node distribution in different tree levels of the docu-
ment data is listed in Table I. According to the node distribution,
the sizes of the top, middle, and bottom layers of MLSOMwere
set at 30 30, 36 36, and 42 42. The initial learning rate
was set to 0.3. The initial radius of the neighborhood function
was set to half-length of the square grid at a SOM layer. The
number of total training iterations was set to 28 080, 28 459, and
114 223 (which are the rounded multiples of the number of data
nodes in the corresponding level) for the top, middle, and bottom
layers, respectively. In this study, the values of the above param-
eters were observed to be a good choice. All simulations were
performed using MATLAB 7 on a PC with Intel 1.8-GHz and
2-GB memory.
A. Document Retrieval
In this section, we present detailed experimental results from
the MLSOM-based retrieval system. We will also compare the
results with direct method using tree data. In the direct method
of retrieval, the query tree data is compared in the database,
and documents with lower tree distance are returned to the user.
Two document tree data are compared with the fol-
lowing tree distance function :
(24)
(25)
where is the root node of the query tree, is the weight
parameter similar to (12), is the distance function, is
the feature vector for the th node of the query, is the
number of child nodes of the node , is a child node of
, and contains the child nodes’ index of the node .
The distance function is a recursive one; the first part contains
distance in global node’s feature, and the second part contains
the average distance in local child nodes through recursion.
is the same weight as used in (12). We used , and cosine
distance function for as follows:
(26)
(27)
(28)
In (12), we used distance (in the first part) to compute the
similarity between the nodes. We conducted simulations using
and cosine distance. We also include results from the LSI
approach [15] without using SOM. In the LSI approach, two
CHOW AND RAHMAN: MULTILAYER SOMWITH TREE-STRUCTURED DATA FOR EFFICIENT DR AND PD 1395
documents are compared directly by calculating the distance be-
tween their LSI features. We do not include the classical VSM
model in our comparative study, as it is too computational and
resource demanding for implementation. To summarize the re-
trieval results, we used each document data from the test set as a
query. The retrieval accuracy is computed by precision defined
as follows:
precision
number of correct documents retrieved
number of total documents retrieved
(29)
Fig. 4 summarizes the precision results of retrieving 360 doc-
uments. It also compares the effect of different values of param-
eter in (12). Fig. 4 includes the results for using two different
normalization methods for document feature: “mean norm” and
“VSM norm.” For each normalization, Fig. 4 shows the results
of the first retrieval as well as the retrieval result after relevance
feedback. The relevance feedback can consistently improve the
retrieval results using “mean norm.” In our experiments, the
“max norm” did not perform well compared with other two and
its results are excluded here. With the increase in the number of
documents retrieved, the precision drops with different slopes
depending on the approach/parameter settings. The complete
precision plots for retrieving unto 360 documents are shown in
Fig. 4.
It is noted that the above results were obtained by using
distances in all cases. Fig. 5 summarizes their precisions for
using different combinations of normalization and distance
functions. The distance function and deliver similar
performance, and -based results are not included sepa-
rately. The choice of normalization and distance function is not
straightforward. For example, in using direct method and “VSM
norm,” the cosine distance function performs better when a
larger number of retrieving documents is considered, and
distance function performs better for retrieving a small number
of documents. On the other hand, when mean normalization is
considered for the direct method, the scenario changes. Again,
the whole scenario changes when we consider a SOM method
instead of a direct method.
It is observed that the SOM approach using “mean norm”
delivers better results than the direct method. In this case,
the superior performance exhibited by the SOM is believed
to be attributed to the coexistence of the SOM properties of
“organizing/clustering,” “topological ordering,” and “nonlinear
projection.” It is also worth noting that the normalization of a
feature plays an important rule in making choice between the
SOM and direct methods. While the SOM approach performs
better with “mean norm,” the direct method usually performs
better with “VSM norm.” Analyzing the above results, it is
obvious that the choices of an appropriate value of and
normalization method are dependent upon the type of the
retrieval method. Overall, the SOM approach performs the
best using “mean norm,” “ ,” and distance. Using
these settings, Table II summarizes the precision results of
different approaches together with their computational time.
The table also summarizes the results of “recall” that is defined
as follows:
recall
number of correct documents retrieved
number of total documents in relevant class
(30)
Fig. 4. Precision of DR under different normalization methods: SOM (mean),
SOM with RF (mean), SOM (VSM), SOM with RF (VSM) for different  . (a)
    , (b)     , and (c)     .
The reported results using precision and recall indicate that
the proposed SOM approach with tree-structure features can
provide better results compared with traditional LSI approach
that uses flat features. It is noted that the basic feature of our
approach and traditional approach lies in the same root of
the term-frequency information. Our approach incorporates
the same feature in a global-to-local structural way. As an
advantage of this approach, users can easily specify the relative
emphasis between global and local characteristics by choosing
1396 IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 20, NO. 9, SEPTEMBER 2009
Fig. 5. Precision of document retrieval by SOM, SOM with RF and direct method for different distance functions and normalization methods. (a) Mean norm,
   distance. (b) VSM norm,    distance. (c) Nmean norm, cosine distance. (d) VSM norm, cosine distance.     for all cases.
TABLE II
COMPARATIVE RESULTS ON DIFFERENT APPROACHES FOR DOCUMENT RETRIEVAL
an appropriate value of during retrieval. These advantages
are discussed in Sections V-B and V-C.
Table III summarizes the retrieval results from MLSOM for
using different types of features, i.e., global, local, and tree
structured. For global features, the PCA features of the root
nodes are used with the nodes at levels 2 and 3 being ignored.
For local feature, we exclude the features of the root nodes. It
is evident that the tree-structured data perform better compared
with global or local features individually. To ensure the gener-
alization of the proposed system, we also included the retrieval
results from the tenfold cross-validation sets. Each of the
validation sets consists of 1040 documents without any overlap
among the sets, so that all data are used for testing. Table IV
summarizes the precision and the recall results from the tenfold
cross-validations sets. The results from ten validation sets are
similar indicating a stable performance of the system. At last,
we study the robustness of the MLSOM over SOM initializa-
tion. We summarize the precision results over different training
sessions in Table V. It is observed that the MLSOM is able to
deliver similar results under different initializations.
CHOW AND RAHMAN: MULTILAYER SOMWITH TREE-STRUCTURED DATA FOR EFFICIENT DR AND PD 1397
TABLE III
RETRIEVAL RESULTS FROM MLSOM USING DIFFERENT TYPES OF FEATURES
TABLE IV
RETRIEVAL RESULTS (PRECISION) FROM MLSOM USING TENFOLD CROSS-VALIDATION SETS
TABLE V
PRECISION RESULTS FROM DIFFERENT TRAINING SESSIONS
The computational time listed in Table II excludes the feature
extraction process for query that was conducted offline. It is ob-
vious that using tree-structured features, the direct method is
computationally demanding, because of the recursive many-to-
many matching in the second part of (25). This procedure is
computationally heavier compared to the one-to-one matching
in (12) by MLSOM. Table II indicates that the required com-
putational time of MLSOM can be up to 120 times shorter than
the time of the direct method. Thus, the SOM algorithm is ca-
pable of reducing the massive computational cost, and is able to
maintain similar or better retrieval accuracy.
B. Plagiarism Detection Using Retrieval
1) Rank of Source Document: In DR, the comparative perfor-
mance between tree-structured feature and traditional LSI may
not be good enough, because document classes are collected
using Google, which is based on keyword-based searching that
reflects only global similarity among documents. The advantage
of the proposed method can be easily comprehended when we
test some DR using plagiarized documents. In making the pla-
giarized document sets, only a small part (approximately equal
to a paragraph) of a test document was copied from a document
of the training set called source document. Thus, a plagiarism
document set is made from our test set, consisting of 78 doc-
uments, three documents from each category. Fig. 6 shows an
example of such copying. We also made a second set from the
above plagiarism set by changing sentences in the copied text.
These minor changes including change of tense, active to pas-
sive, few words, etc., show an example of such copying. Unless
specified, the following experiments were conducted on the first
plagiarism set.
In this section, 500 documents were retrieved using each of
the plagiarized documents as a query, andwe observe the rank of
the source document in the results. The rank indicates the posi-
tion of source document in the retrieved document list. A lower
rank value is desired for easy detection. Table VI shows the av-
erage ranks of a source document in the retrieval results for 78
1398 IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 20, NO. 9, SEPTEMBER 2009
Fig. 6. Typical example of plagiarized documents (Top) Source document. (Bottom) Plagiarized document. The highlighted parts are the plagiarized text.
queries of plagiarism documents. The results summarize the ef-
fect of emphasis between global and local (different values of
) in detecting plagiarized document. Table VI includes failed
detection indicating the percentage of retrieval cases that failed
to include the source document. Based on the average rank and
failed detection, a composite rank is calculated. It is clear that
we can achieve better rank of source document by equally em-
phasizing on global and local characteristics of a docu-
ment. Emphasizing more on global characteristics causes higher
failed detection. At last, we investigate the effect of presence
of source document in the first number of documents from the
retrieved list. Table VII summarizes the results of presence of
source documentwhen it is 1, 20, and 50. In this experiment, we
used normal retrieval method without incorporating any special
technique for PD. In the next section, we investigate the positive
effect of incorporating a local sorting procedure.
2) Retrieval With Local Sorting: To effectively retrieve the
source document of plagiarism, we incorporate an additional
sorting process after the retrieval. As described in Section IV-C,
the retrieved documents are sorted according to their similari-
CHOW AND RAHMAN: MULTILAYER SOMWITH TREE-STRUCTURED DATA FOR EFFICIENT DR AND PD 1399
TABLE VI
RANKS OF SOURCE DOCUMENT OF PLAGIARISM IN NORMAL DOCUMENT RETRIEVAL RESULTS
TABLE VII
PRESENCE OF SOURCE DOCUMENT OF PLAGIARISM IN RETRIEVAL
Fig. 7. Presence of source document of plagiarism in retrieval against the
number of document investigated.
ties in terms of the third-level nodes. Instead of using the func-
tions in Section IV-C, we used a modified and simple set of
sorting functions: , , and
. Using the local sorting
, Table VI shows new rank results. Table VII includes new
results of presence of source document when is considered
to be 1, 20, and 50. The results are summarized for different “ ”
values in (12). In our results, mean norm and “ ” are the
best retrieval setting for PD. Using these settings and the local
sorting, Fig. 7 shows the complete plots of presence of source
document for different sorting functions in MLSOM retrieval.
While function fits best for detecting a single paragraph,
function is more effective in detecting multiple plagiarized
sections. The function , being hybrid of the above two, in
general works well. Fig. 7 also includes a result from the plain
LSI approach without incorporating any sorting. These results
demonstrate how tree-structured features possess the advantage
over the traditional global flat features. Thus, the tree-structured
features serve us better by including local characteristics of a
document as well as users’ flexibility of assigning emphasis be-
tween global and local characteristics of a document.
3) Automatic Plagiarism Detection: In this subsection, we
consider PD in an automatic way rather than looking for prob-
able source document in the retrieval results. To make the binary
decision on the presence of source document, we follow the pro-
cedures in Section IV-C, and set a small offset value for that
is used for local sorting process. Here, 1000 documents are re-
trieved and sorted by the local sorting procedures. Table VIII
shows the results of automatic PD against different offset values
of . The table also includes false detection, which indicates the
case when other than source document is detected as plagiarism
source. It is observed that a higher value of results in unex-
pected false detection. We observed that the standard deviation
of average feature value over all third-level nodes
is 3.9. Using a value of around 3.9 delivers the best result that
maintains a good balance between source detection and false
detection. Fig. 8(a) shows the plot of source detection, false de-
tection, and failed detection against different values of . With
an increase of , the source/false detection increases, while the
failed detection drops. An optimum choice of detection is to find
a value of giving source detection as high as possible, and
keeping a false/failed detection as low as possible. Fig. 8 shows
that a value of around exhibits such a balanced perfor-
mance. Table VIII also includes the detection results from the
second set of plagiarism documents. Using , the source
detection rate drops by 3.85%, which is due to the modification
on the copied text.
1400 IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 20, NO. 9, SEPTEMBER 2009
Fig. 8. PD statistics against different offset value: (a) plagiarism by retrieval and (b) plagiarism through local association.
TABLE VIII
DETECTION RESULTS OF PDR/PDLA AGAINST DIFFERENT VALUES OF   OR 
TABLE IX
COMPUTATIONAL TIME (IN SECOND) FOR PLAGIARISM DETECTION
C. Plagiarism Detection Through Local Association
This section investigates a more effective PD method using
the bottom SOM layer of MLSOM. This approach, described in
Section IV-D, does not need to go through normal DR process.
The bottom layer is used to organize the third-level nodes repre-
senting local paragraphs. For a given query document, the third-
level nodes are matched with the neurons of the bottom layer.
The associated documents only from the winner neurons are re-
turnedwith a rank according to the distance between paragraphs.
Table VIII shows the results obtained from different values of
offset . The results show that an 88% source detection and a
false detectionbelow48%aremaintained.But the false detection
is slightly higher than that of the previous approach. The basic
difference between the two approaches is that the first approach
uses the global document similarity to restrict a list of suspected
documents, while the second approach uses local similarities to
search through the clusters of all documents at the bottom layer.
As a result, the source detection rate is increased by 23%. Simi-
larly to the previous approach,we found a good tradeoff between
source detection and false detection by selecting the value of
around . Table VIII presents the results of the second set
ofplagiarismdocuments that includemodifiedcopied text.Using
, the source detection rate is 2.57%less than that of theorig-
inal plagiarismdata set. The performance difference between the
two plagiarism sets is slightly less than that from using the PDR
method. Compared with PDR with PDLA, the above observa-
tions indicate that PDLA is a more effective method to detect
modified type of plagiarism text.
The comparative computational time is shown in Table IX.
The listed time is the searching time that excludes the feature ex-
CHOW AND RAHMAN: MULTILAYER SOMWITH TREE-STRUCTURED DATA FOR EFFICIENT DR AND PD 1401
traction of a query document. The average computational time
of segmentation and feature extraction for a query document is
around 1 s making real-time application possible. The required
computational time of PD varies from document to document
depending upon the size of a document. In summary, PDLA
can be up to ten times faster than PDR, because local associ-
ation does not need to compare a paragraph of a query to all
paragraphs in the database. On the other hand, even though
candidate documents are limited, the PDR needs to handle a
many-to-many matching by (18), which is rather computation-
ally demanding. However, in handling a massive increase in the
database size, the computational time of PDR will not signifi-
cantly increase, because it restricts its plagiarism searching to a
limited retrieved document set. On the other hand, PDLA may
need an increase in time, as the number of locally associated
documents will increase accordingly with the size of a data-
base. However, the PDLAmethod is more effective in detecting
plagiarism.
For a very large scale implementation of DR and PD, it is
difficult to process all documents in a single SOM module, or
to update SOM when the database is required to be updated
after a period of time. We can consider modular SOM approach,
in which database of different domains can be processed over
manyMLSOMmodules. This can be a future work on DR using
SOM. In our future work, we will investigate detecting sophis-
ticated plagiarism that involves more than simple copy of words
in a paragraph.
VI. CONCLUSION
A new approach of DR and PD using tree-structured doc-
ument representation and MLSOM is proposed. We have
shown that tree-structured representation enhances the retrieval
accuracy by incorporating local characteristics with traditional
global characteristics. The proposed method provides flexi-
bility to assign relative emphasis between the local and global
characteristics of a document. Together with the support of
relevance feedback, the proposed system provides improved
accuracy in DR. Hierarchical organization of global and local
characteristics enables the MLSOM to be used for PD. Two
methods of PD are proposed. The first one is an extension
of the DR method with additional local sorting. The second
method uses document association on the bottom layer of
MLSOM. Our obtained results indicate that the second method
consistently delivers higher detection rate.
Computational cost is an important issue, because this ap-
plication usually deals with a large database. In our work, the
MLSOM serves as an efficient computational solution for prac-
tical implementation; its computational time can be up to 120
times faster than the direct method.
ACKNOWLEDGMENT
The authors would like to thank the anonymous reviewers for
providing useful comments.
REFERENCES
[1] S. Deng andH. Peng, “Document classification based on support vector
machine using a concept vector model,” in Proc. IEEE/WIC/ACM Int.
Conf. Web Intell., Dec. 2006, pp. 473–476.
[2] C. J. C. Burges, “A tutorial on support vector machines for pattern
recognition,” Data Mining Knowl. Disc., vol. 2, no. 2, pp. 121–167,
1998.
[3] T. Onoda, H. Murata, and S. Yamada, “SVM-based interactive docu-
ment retrieval with active learning,” New Gen. Comput., vol. 26, no. 1,
pp. 49–61, 2008.
[4] R. Nallapati, “Discriminative models for information retrieval,” in
Proc. 27th Annu. Int. ACM SIGIR Conf. Res. Develop. Inf. Retrieval,
2004, pp. 64–71.
[5] R. T. Freeman and H. Yin, “Web content management by self-organi-
zation,” IEEE Trans. Neural Netw., vol. 16, no. 5, pp. 1256–1268, Sep.
2005.
[6] K. Monostori, A. Zaslavsky, and H. Schmidt, “MatchDetectReveal:
Finding overlapping and similar digital documents,” in Proc. Inf. Re-
sources Manage. Assoc. Int. Conf. Challenges Inf. Technol. Manage.
21st Century, Anchorage, AK, 2000, pp. 955–957.
[7] S. B. J. Davis and H. Garcia-Molina, “Copy detection mechanisms for
digital documents,” in Proc. ACM SIGMOD Annu. Conf., 1995, pp.
398–409.
[8] N. Shivakumar and H. Garcia-Molina, “Building a scalable and ac-
curate copy detection mechanism,” in Proc. 1st ACM Conf. Digit. Li-
braries, Bethesda, MD, 1996, pp. 160–168.
[9] A. Si, H. V. Leong, and R. W. H. Lau, “CHECK: A document plagia-
rism detection system,” in Proc. ACM Symp. Appl. Comput., Feb. 1997,
pp. 70–77.
[10] R. B. Yates and B. R. Neto, Modern Information Retrieval. Reading,
MA: Addison-Wesley/Longman, 1999.
[11] J. Zobel and A. Moffat, “Exploring the similarity space,” ACM SIGIR
Forum, vol. 32, no. 1, pp. 18–34, 1998.
[12] C. Buckley and J. Walz, “SMART at TREC-8,” in Proc. Text Retrieval
Conf., 1999, pp. 577–582.
[13] S. Robertson and S. Walker, “Okapi/Keenbow at TREC-8,” in Proc.
Text Retrieval Conf., vol. 99, pp. 151–162.
[14] J. M. Ponte and W. B. Croft, “A language modeling approach to in-
formation retrieval,” in Proc. 21st Annu. Int. ACM SIGIR Conf. Res.
Develop. Inf. Retrieval, Melbourne, Australia, 1998, pp. 275–281.
[15] S. Deerwester, S. Dumais, G. Furnas, T. Landauer, and R. Harshman,
“Indexing by latent semantic analysis,” J. Amer. Soc. Inf. Sci., vol. 41,
pp. 391–407, 1990.
[16] M. W. Berry, S. T. Dumais, and G. W. O’Brien, “Using linear algebra
for intelligent information retrieval,” SIAM Rev., vol. 37, no. 4, pp.
573–595, 1995.
[17] C. H. Papadimitriou, P. Raghavan, H. Tamaki, and S. Vempala, “Latent
semantic indexing: A probabilistic analysis,” J. Comput. Syst. Sci., vol.
61, no. 2, pp. 217–235, 2000.
[18] T. Honkela, S. Kaski, K. Lagus, and T. Kohonen, “WEBSOM-self-or-
ganizing maps of document collections,” in Proc. Workshop Self-Or-
ganizing Maps, Espoo, Finland, Jun. 4–6, 1997, pp. 310–315.
[19] N. Ampazis and S. Perantonis, “LSISOM, a latent semantic indexing
approach to self-organizing maps of document collections,” Neural
Process. Lett., vol. 19, no. 2, pp. 157–173, 2004.
[20] R. Zhao andW.Grosky, “Narrowing the semantic gap—Improved text-
based web document retrieval using visual features,” IEEE Trans. Mul-
timedia, vol. 4, no. 2, pp. 189–200, Jun. 2002.
[21] J. Bear, D. Israel, J. Petit, and D. Martin, “Using information extrac-
tion to improve document retrieval,” in Proc. 6th Text Retrieval Conf.,
Gathiersburg (MD), Nov. 1997, pp. 367–377.
[22] R. Gaizauskas and Y. Wilks, “Information extraction: Beyond docu-
ment retrieval,” J. Documentation, vol. 54, no. 1, pp. 70–105, 1998.
[23] H. Chen, K. J. Lynch, K. Basu, and D. T. Ng, “Generating, integrating,
and activating thesauri for concept-based document retrieval,” IEEE
EXPERT, ser. Artificial Intelligence in Text-Based Information Sys-
tems, vol. 8, no. 2, pp. 25–34, Apr. 1993.
[24] N. Rooney, D. Patterson, M. Galushka, and V. Dobrynin, “A scaleable
document clustering approach for large document corpora,” Inf.
Process. Manage., vol. 42, pp. 1163–1175, 2006.
[25] S. M. Chen, Y. J. Horng, and C. H. Lee, “Document retrieval using
fuzzy-valued concept networks,” IEEE Trans. Syst. Man Cybern. B,
Cybern., vol. 31, no. 1, pp. 111–118, Feb. 2001.
[26] Y. J. Horng, S. M. Chen, Y. C. Chang, and C. H. Lee, “A new method
for fuzzy information retrieval based on fuzzy hierarchical clustering
and fuzzy inference techniques,” IEEE Trans. Fuzzy Syst., vol. 13, no.
2, pp. 216–228, Apr. 2005.
[27] D. H. C. Du, S. Ghanta, K. J. Maly, and S. M. Sharrock, “An efficient
file structure for document retrieval in the automated office environ-
ment,” IEEE Trans. Knowl. Data Eng., vol. 1, no. 2, pp. 258–273, Jun.
1989.
1402 IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 20, NO. 9, SEPTEMBER 2009
[28] A. Georgakis, C. Kotropoulos, A. Xafopoulos, and I. Pitas, “Marginal
median SOM for document organization and retrieval,” Neural Netw.,
vol. 17, no. 3, pp. 365–377, 2004.
[29] D. Merkl, S. H. He, M. Dittenbach, and A. Rauber, “Adaptive hierar-
chical incremental grid growing: An architecture for high-dimensional
data visualization,” in Proc. 4th Workshop Self-Organizing Maps/Adv.
Self-Organizing Maps, Kitakyushu, Japan, Sep. 2003, pp. 293–298.
[30] Z. Wang, M. Hagenbuchner, A. C. Tsoi, S. Y. Cho, and Z. Chi, “Image
classification with structured self-organization map,” in Proc. Int. Joint
Conf. Neural Netw., 2002, vol. 2, pp. 1918–1923.
[31] M. Hagenbuchner, “Extension and evaluation of adaptive processing of
structured information using artificial neural networks,” Ph.D. disserta-
tion, Faculty Inf., Univ. Wollongong, Wollongong, N.S.W., Australia,
2002.
[32] T. W. S. Chow, M. K. M. Rahman, and S. Wu, “Content based image
retrieval by using tree-structured features and multi-layer SOM,” Pat-
tern Anal. Appl., vol. 9, no. 1, pp. 1–20, 2006.
[33] T. W. S. Chow and M. K. M. Rahman, “A new image classification
technique using tree-structured regional features,” Neurocomputing,
vol. 70, no. 4–6, pp. 1040–1050.
[34] P. Salembier and L. Garrido, “Binary partition tree as an efficient rep-
resentation for image processing, segmentation, and information re-
trieval,” IEEE Trans. Image Process., vol. 9, no. 4, pp. 561–576, Apr.
2000.
[35] M. K. M. Rahman, W. P. Yang, T. W. S. Chow, and S. Wu, “A flexible
multi-layer self-organizing map for generic processing of tree-struc-
tured data,” Pattern Recognit., vol. 40, no. 5, pp. 1406–1424, 2007.
[36] F. Kappe and B. Zaka, “Plagiarism—A survey,” J. Universal Comput.
Sci., vol. 12, no. 8, pp. 1050–1084, 2006.
[37] S. Niezgoda and T. P. Way, “SNITCH: A Software tool for detecting
cut and paste plagiarism,” inProc. 37th Special Interest Group Comput.
Sci. Edu. Tech. Symp., New York, Mar. 2006, pp. 51–55.
[38] S. M. Zu Eissen and B. Stein, “Intrinsic plagiarism detection,” in Proc.
28th Eur. Conf. IR Res., London, U.K., 2006, vol. 3936, pp. 565–569.
[39] N. Kang, A. Gelbukh, and S. Y. Han, “PPChecker: Plagiarism pat-
tern checker in document copy detection,” in Lecture Notes in Com-
puter Science. Berlin, Germany: Springer-Verlag, 2006, vol. 4188,
pp. 661–667.
[40] N. Heintze, “Scalable document fingerprinting,” in Proc. 2nd USENIX
Workshop Electron. Commerce, Oakland, CA, Nov. 1996, pp. 18–21.
[41] P. Iyer and A. Singh, “Document similarity analysis for a plagiarism
detection system,” in Proc. 2nd Indian Int. Conf. Artif. Intell., pp.
2534–2544.
[42] M. F. Porter, “An algorithm for suffix stripping,” Program, vol. 14, no.
3, pp. 130–137, 1980.
[43] G. Salton and C. Buckley, “Term weighting approaches in automatic
text retrieval,” Inf. Process. Manage., vol. 32, no. 4, pp. 431–443, 1996.
[44] E. Høgh-Rasmussen, “BBTools—A Matlab toolbox for black-box
computations,” Neurobiol. Res. Unit, Copenhagen Univ. Hospital,
Copenhagen, Denmark, 2005 [Online]. Available: http://nru.dk/soft-
ware/bbtools/
Tommy W. S. Chow received the B.Sc. (1st Hons)
degree and the Ph.D. degree from the Department of
Electrical and Electronic Engineering, University of
Sunderland, Sunderland, U.K.
Currently, he is the Professor in the Depart-
ment of Electronic Engineering, City University
of Hong Kong. He has been working on different
consultancy projects with the Mass Transit Railway,
Kowloon-Canton Railway Corporation, Hong Kong.
He has also conducted other collaborative projects
with the Kong Electric Co., Ltd., the MTR Hong
Kong, and Observatory Hong Kong on the application of neural networks for
machine fault detection and forecasting. He is an author and coauthor of over
130 journal articles related to his research, five book chapters, and one book.
His main research has been in the area of neural networks, machine learning,
pattern recognition, and fault diagnosis.
Dr. Chow received the Best Paper Award at the 2002 IEEE Industrial Elec-
tronics Society Annual Meeting in Seville, Spain. He is the Associate Editor of
Pattern Analysis and Applications, and the International Journal of Information
Technology.
M. K. M. Rahman received the B.S. degree in elec-
trical and electronic engineering from Bangladesh
University of Engineering and Technology (BUET),
Dhaka, Bangladesh, in 2001, and the Ph.D. degree in
electronic engineering from City University of Hong
Kong, Hong Kong, in 2007.
Currently, he is an Assistant Professor in the De-
partment of Electrical and Electronic Engineering,
United International University, Dhaka, Bangladesh.
He has been engaged in research activities in the
field of pattern recognition since 2002. He published
a number of papers in recognized international journals. His research interests
include pattern recognition and neural networks. Based on neural networks, he
developed a number of applications such as image retrieval and classification,
face identification.
