c© 2006 by Bei Yu. All rights reserved.
AN EVALUATION OF TEXT CLASSIFICATION METHODS FOR LITERARY STUDY
BY
BEI YU
B.E., University of Science and Technology of China, 1996
M.E., Chinese Academy of Sciences, 1999
DISSERTATION
Submitted in partial fulfillment of the requirements
for the degree of Doctor of Philosophy in Library and Information Science
in the Graduate College of the
University of Illinois at Urbana-Champaign, 2006
Urbana, Illinois
Abstract
Text classification methods have been evaluated on topic classification tasks. This thesis
extends the empirical evaluation to emotion classification tasks in the literary domain. This
study selects two literary text classification problems - the eroticism classification in Dickin-
son’s poems and the sentimentalism classification in early American novels - as two cases for
this evaluation. Both problems focus on identifying certain kinds of emotion - a document
property other than topic. This study chooses two popular text classification algorithms -
naive Bayes and Support Vector Machines (SVM), and three feature engineering options -
stemming, stopword removal and statistical feature selection (Odds Ratio and SVM) - as the
subjects of evaluation. This study aims to examine the effects of the chosen classifiers and
feature engineering options on the two emotion classification problems, and the interaction
between the classifiers and the feature engineering options.
This thesis seeks empirical answers to the following research questions: 1) is SVM a better
classifier than naive Bayes regarding classification accuracy, new literary knowledge discovery
and potential for example-based retrieval? 2) is SVM a better feature selection method than
Odds Ratio regarding feature reduction rate and classification accuracy improvement? 3)
does stop word removal affect the classification performance? 4) does stemming affect the
performance of classifiers and feature selection methods?
Some of our conclusions are consistent with what are obtained in topic classification,
such as Odds Ratio does not improve SVM performance and stop word removal might harm
classification. Some conclusions contradict previous results, such as SVM does not beat
naive Bayes in both cases. Some findings are new to this area - SVM and naive Bayes select
iii
top features in different frequency ranges; stemming might harm feature selection methods.
These experiment results provide new insights to the relation between classification methods,
feature engineering options and non-topic document properties. They also provide guidance
for classification method selection in literary text classification applications.
iv
Table of Contents
List of Tables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . viii
List of Figures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ix
Chapter 1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1
Chapter 2 Text classification algorithms . . . . . . . . . . . . . . . . . . . 7
2.1 The naive Bayes algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
2.2 The Support Vector Machines (SVM) algorithm . . . . . . . . . . . . . . . . 10
Chapter 3 Feature engineering . . . . . . . . . . . . . . . . . . . . . . . . . 12
3.1 Bag-of-words vs. complex document representations . . . . . . . . . . . . . . 12
3.2 Feature engineering options . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
3.2.1 Feature reduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
3.2.2 Feature re-weighting . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
3.3 Feature merging . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
3.3.1 Case merging . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
3.3.2 Stemming . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
3.3.3 Word clustering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
3.4 Arbitrary feature selection . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
3.4.1 Removing rare words . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
3.4.2 Removing stop words . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
3.4.3 Task-dependent feature selection . . . . . . . . . . . . . . . . . . . . 20
3.5 Feature selection based on statistical criteria . . . . . . . . . . . . . . . . . . 20
3.5.1 The filtering approach . . . . . . . . . . . . . . . . . . . . . . . . . . 21
3.5.2 The wrapper approach . . . . . . . . . . . . . . . . . . . . . . . . . . 22
3.5.3 The interaction between feature selection methods and classification
algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
Chapter 4 Beyond topic classification . . . . . . . . . . . . . . . . . . . . . 25
4.1 Style and author classification . . . . . . . . . . . . . . . . . . . . . . . . . . 26
4.2 Genre classification . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
4.3 Sentiment classification . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
4.4 Evaluation of non-topic classification . . . . . . . . . . . . . . . . . . . . . . 30
v
Chapter 5 Literary text classification and evaluation . . . . . . . . . . . . 32
5.1 The emergence of literary text mining . . . . . . . . . . . . . . . . . . . . . . 32
5.2 Literary text classification . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34
5.3 Evaluation of text classification methods in literary domain . . . . . . . . . . 36
5.4 The eroticism classification of Dickinson’s poems . . . . . . . . . . . . . . . . 37
5.5 The sentimentalism classification of early American novel chapters . . . . . . 40
Chapter 6 Research questions and experiment design . . . . . . . . . . . 43
6.1 Evaluation methods and measures . . . . . . . . . . . . . . . . . . . . . . . . 44
6.1.1 Test methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44
6.1.2 Performance measures . . . . . . . . . . . . . . . . . . . . . . . . . . 44
6.1.3 KLD - word similarity measure . . . . . . . . . . . . . . . . . . . . . 47
6.2 Experiment design . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47
6.2.1 Experiment 1: choosing document representation . . . . . . . . . . . 48
6.2.2 Experiment 2: stemming . . . . . . . . . . . . . . . . . . . . . . . . . 48
6.2.3 Experiment 3: removing stop words . . . . . . . . . . . . . . . . . . . 48
6.2.4 Experiment 4: feature selection - objective evaluation . . . . . . . . . 49
6.2.5 Experiment 5: feature selection - subjective evaluation . . . . . . . . 49
6.2.6 Experiment 6: cross feature selection . . . . . . . . . . . . . . . . . . 50
6.2.7 Experiment 7: learning curve . . . . . . . . . . . . . . . . . . . . . . 50
6.2.8 Experiment 8: confidence curve . . . . . . . . . . . . . . . . . . . . . 50
6.3 Algorithm implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51
Chapter 7 Case one - Dickinson eroticism classification . . . . . . . . . . 53
7.1 choosing document representation . . . . . . . . . . . . . . . . . . . . . . . . 53
7.2 Case merging . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55
7.2.1 Macro evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55
7.2.2 Micro evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55
7.3 Stemming . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58
7.3.1 Macro evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58
7.3.2 Micro analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59
7.4 Removing stop words . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59
7.4.1 Stop words as common words . . . . . . . . . . . . . . . . . . . . . . 60
7.4.2 Stop words as function words . . . . . . . . . . . . . . . . . . . . . . 61
7.5 Feature selection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62
7.5.1 naive Bayes self feature selection - objective evaluation . . . . . . . . 63
7.5.2 naive Bayes self feature selection - subjective evaluation . . . . . . . . 63
7.5.3 SVM self feature selection - objective evaluation . . . . . . . . . . . . 64
7.5.4 SVM self feature selection - subjective evaluation . . . . . . . . . . . 65
7.5.5 Comparing naive Bayes and SVM as feature selection methods . . . . 65
7.6 Cross feature selection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69
7.7 Learning curve . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70
7.8 Confidence curve . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70
7.9 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71
vi
Chapter 8 Case two - sentimental novel chapter classification . . . . . . . 73
8.1 choosing document representation . . . . . . . . . . . . . . . . . . . . . . . . 73
8.2 Stemming . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75
8.2.1 Macro evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75
8.2.2 Micro evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76
8.3 Removing stop words . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 78
8.3.1 Stop words as common words . . . . . . . . . . . . . . . . . . . . . . 79
8.3.2 Stop words as function words . . . . . . . . . . . . . . . . . . . . . . 79
8.4 Feature selection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80
8.4.1 Naive Bayes self feature selection - objective evaluation . . . . . . . . 80
8.4.2 Naive Bayes self feature selection - subjective evaluation . . . . . . . 80
8.4.3 SVM self feature selection - objective evaluation . . . . . . . . . . . . 81
8.4.4 SVM self feature selection - subjective evaluation . . . . . . . . . . . 83
8.4.5 Comparing naive Bayes and SVM as feature selection methods . . . . 84
8.5 Cross feature selection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 87
8.6 Learning curve . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 88
8.7 Confidence curve . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 88
8.8 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 89
Chapter 9 Conclusions and future work . . . . . . . . . . . . . . . . . . . . 91
9.1 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 91
9.2 Future research directions . . . . . . . . . . . . . . . . . . . . . . . . . . . . 93
Appendix A Toward discovering potential data mining applications for
literary criticism . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 95
A.1 Corpora construction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97
A.2 Step 1 finding unique scholar vocabulary . . . . . . . . . . . . . . . . . . . . 97
A.3 Step 2 finding overlapping vocabulary . . . . . . . . . . . . . . . . . . . . . 99
A.4 Step 3 literary scholar survey . . . . . . . . . . . . . . . . . . . . . . . . . . 100
A.4.1 Survey questions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101
A.4.2 Survey answers analysis . . . . . . . . . . . . . . . . . . . . . . . . . 102
A.5 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 104
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 106
vii
List of Tables
5.1 Vocabulary comparison . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41
6.1 Binary classification results in a contigency table . . . . . . . . . . . . . . . . 45
7.1 Dickinson document representation comparison . . . . . . . . . . . . . . . . 54
7.2 Dickinson case merging - 10CV macro evaluation . . . . . . . . . . . . . . . 55
7.3 Dickinson case merging - micro evaluation . . . . . . . . . . . . . . . . . . . 56
7.4 Dickinson stemming - 10CV macro evaluation . . . . . . . . . . . . . . . . . 58
7.5 Dickinson stemming - micro analysis . . . . . . . . . . . . . . . . . . . . . . 60
7.6 Dickinson common words . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61
7.7 Dickinson function words . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61
7.8 The effect of naive Bayes self feature selection on eroticism classification . . . 63
7.9 The effect of naive Bayes self feature selection on eroticism classification . . . 63
7.10 Dickinson SVM self feature selection . . . . . . . . . . . . . . . . . . . . . . 64
7.11 The effect of SVM self feature selection on eroticism classification . . . . . . 65
7.12 Feature selection method cross comparison . . . . . . . . . . . . . . . . . . . 69
8.1 Sentimentalism document representation comparison . . . . . . . . . . . . . 74
8.2 Sentimentalism SVM top boolean features . . . . . . . . . . . . . . . . . . . 74
8.3 Sentimentalism SVM top raw frequency features . . . . . . . . . . . . . . . . 75
8.4 Sentimentalism stemming - macro evaluation . . . . . . . . . . . . . . . . . . 75
8.5 Sentimentalism full stemming - micro analysis . . . . . . . . . . . . . . . . . 77
8.6 Sentimentalism common words . . . . . . . . . . . . . . . . . . . . . . . . . . 79
8.7 Sentimentalism function words . . . . . . . . . . . . . . . . . . . . . . . . . . 79
8.8 The effect of naive Bayes self feature selection on sentimentalism classification 81
8.9 Sentimentalism naive Bayes top boolean features . . . . . . . . . . . . . . . . 81
8.10 Incremental SVM self feature selection . . . . . . . . . . . . . . . . . . . . . 82
8.11 Sentimentalism SVM top boolean features . . . . . . . . . . . . . . . . . . . 83
8.12 Feature selection method cross comparison . . . . . . . . . . . . . . . . . . . 87
A.1 KDD keyword frequency comparison between MUSE and ANC-NYTIMES . 100
viii
List of Figures
7.1 Illustration: color coding poems (RGB) . . . . . . . . . . . . . . . . . . . . . 62
7.2 Figure: naive Bayes and SVM feature ranking agreement . . . . . . . . . . . 66
7.3 Figure: Dickinson SVM feature ranks and weights . . . . . . . . . . . . . . . 67
7.4 Figure: Dickinson naive Bayes feature ranks and weights . . . . . . . . . . . 67
7.5 Figure: Dickinson SVM feature ranks and frequencies . . . . . . . . . . . . . 68
7.6 Figure: Dickinson naive Bayes feature ranks and frequencies . . . . . . . . . 68
7.7 Figure: Dickinson learning curves . . . . . . . . . . . . . . . . . . . . . . . . 70
7.8 Figure: Dickinson confidence curves . . . . . . . . . . . . . . . . . . . . . . . 71
8.1 Figure: KLD rank and values . . . . . . . . . . . . . . . . . . . . . . . . . . 83
8.2 Figure: naive Bayes and SVM feature ranking agreement . . . . . . . . . . . 84
8.3 Figure: Sentimentalism SVM feature ranks and weights . . . . . . . . . . . . 85
8.4 Figure: Sentimentalism naive Bayes feature ranks and weights . . . . . . . . 86
8.5 Figure: Sentimentalsim SVM feature ranks and frequencies . . . . . . . . . . 86
8.6 Figure: Sentimentalism naive Bayes feature ranks and frequencies . . . . . . 87
8.7 Figure: sentimentalism learning curves . . . . . . . . . . . . . . . . . . . . . 88
8.8 Figure: sentimentalism confidence curves . . . . . . . . . . . . . . . . . . . . 89
A.1 TF of Selected KDD keywords common in MUSE but not common in ANC-
NYTIMES . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101
ix
Chapter 1
Introduction
With the fast growth of the number of electronic text documents, automatic text classifica-
tion becomes an important tool for information organization and knowledge discovery from
text data. KDNuggets 1 January 2006 poll on “what text mining tasks have you done in
2004-5?” ranked text classification as the second most common text mining tasks conducted
last year, right after document clustering.
Text classification is also called text categorization, or predictive text mining [100]. As
a supervised learning approach, text classification consists of two steps - training and pre-
dicting. A set of documents D = {d1, d2, . . . , dn} with pre-assigned class memberships
L = {l1, l2, . . . , ll} serve as the training examples {(di, lj)}. Each example di is defined as
a document vector in the feature space F = {f1, f2, . . . , fm}. The most common document
representation is the bag-of-words (BOW) model, which simplifies the representation of a
document to a word vector with each dimension corresponding to a unique word present
in the document. In another word the document is treated as a bag of words with no or-
der. In the training step, a classification model is learned from the training documents. In
the predicting step, the learned classification model predicts the class memberships of new
unlabeled documents.
Text classification methods can be used for three purposes. The first is information orga-
nization - a classifier can learn the target category concepts (e.g. news article about trade,
acquisition, etc.) from the training documents, and then assign new documents into these
1KDnuggests (http://www.kdnuggets.com/) is the leading data mining information source. KD stands
for Knowledge Discovery
1
predefined categories. The second purpose is knowledge discovery - a successful classifier
can provide new insights to understand a target concept by revealing the the correlation
between the features and the concept. For example, linear classifiers can be represented as
a linear function of the features. The weights of the features in the decision function corre-
spond to their discriminative power in classification. Therefore the most correlated features
provide an approximate description of the learned category concept. The third purpose is
example-based retrieval - a classifier might be able to learn from a small number of seed
training documents with the help of semi-supervised learning and active learning methods,
and then retrieve more documents similar to the training examples from a large collection.
But a good candidate classifier should have fast learning curve and strong confidence in the
prediction.
Text classification methods have been well developed over the past twenty years. There
are three lines of research for classification methods. The first is new classification algorithm
development - to find advanced algorithms with smaller generalization error. The second is
feature engineering, such as feature selection and feature re-weighting, in order to find the
most relevant features and the best feature weights. Smaller feature sets also lead to smaller
and more generalizable classification models. The third is classification method evaluation.
With the availability of many classification algorithms and feature engineering choices, eval-
uation of classification methods is important to provide guidelines for method selection in
applications. Because of the subjectivity in the class concept definition, analytical evalua-
tion of text classifiers is difficult. Therefore empirical experiments became the common text
classification evaluation methods [90].
The major text classification methods have been evaluated on topic classification tasks
using some benchmark data sets, such as the Reuters-21578 news collection and the Usenet
newsgroup collection. Some topic classification evaluation results have been widely accepted.
For example, Support Vector Machines (SVM) is currently the best text classifier [42, 104,
27].
2
These evaluation data sets were limited to news and web documents; the evaluation
tasks were limited to topic classification. Text documents can be categorized by many
criteria - topics, styles, genres, sentiment, etc. Recently more attention has been drawn to
classification tasks based on document properties other than topic, such as style, genre, or
sentiment classifications. When extending the classification methods to the new problems,
most research focused on searching for new problem-relevant features. The evaluation of
classification methods were neglected in the new problem setting.
However, previous studies have showed that document properties interact with cluster-
ing methods [68]. Recent research results in sentiment classification started to show that
document properties interact with classification methods too. For example, Pang et al. [75]
has found that SVM did not beat naive Bayes in movie review classification.
Evidence has also been found that classification algorithms interact with feature selection
methods in topic classification [66]. Forman [33] found that no feature selection improves
SVM performance; Mladenic [66] found that SVM feature selection is better than Odds Ratio
for naive Bayes. Regretfully no feature selection was done in Pang’s SVM and naive Bayes
comparison. We might wonder if SVM cannot beat naive Bayes in sentiment classification,
will it still be a better feature selection method than naive Bayes?
There are also negative conclusions regarding some common document preprocessing
techniques, such as stemming and stopword removal, in non-topic classification tasks. Verb
tenses are shown to be useful for customer intent classification. Personal pronouns proved
to be useful for homepage classification [62].
Furthermore, the previous classification method evaluation focused on the classification
performance for the information organization purpose. The other classification use scenar-
ios, feature-category correlation analysis and example-based retrieval, were not taken into
consideration in the evaluation.
Will a classifier with high classification performance also excel in correlation analysis
and example-based retrieval? What are the relations between the classification algorithms,
3
the feature engineering options and the document properties? Do those widely accepted
evaluation results obtained from topic classification still valid across document properties?
This thesis chooses a new kind of data - the American literature - to evaluate classifica-
tion methods in non-topic classification tasks and all the three classification use scenarios .
Because no benchmark data is available in the literary domain this thesis selects two literary
text classification problems - the eroticism classification in Dickinson’s poems and the senti-
mentality classification in early American novels - as two cases for this study. Both problems
focus on identifying certain kinds of emotion - a document property other than topic.
This thesis chooses two popular text classification algorithms - naive Bayes and SVM,
and three feature engineering options - stemming, stopword removal and statistical feature
selection (Odds Ratio and SVM) - as the subjects of evaluation. This thesis aims to examine
the effects of the chosen classifiers and feature engineering options on the two emotion
classification problems, and the interaction between the classifiers and the feature engineering
options. The evaluation results will be compared against the conclusions obtained in the
topic classification setting to examine the interaction between classification methods and
document properties.
This thesis seeks empirical answers to the following research questions:
1. Is SVM a better classifier than naive Bayes regarding classification accuracy, new
literary knowledge discovery and potential for example-based retrieval?
2. Is SVM a better feature selection method than Odds Ratio regarding feature reduction
rate and classification accuracy improvement?
3. Does stop word removal affect the classification performance?
4. Does stemming affect the performance of classifiers and feature selection methods?
The experiment results show that SVM is not a universal winner in literary text clas-
sification. After feature selection naive Bayes achieves high accuracies in both cases while
4
SVM succeeds in the sentimentality classification only. SVM and naive Bayes select their
top features at different frequency ranges. SVM tends to pick high frequent and discriminant
words, which are usually within the scholars’ prior knowledge. Naive Bayes tends to pick
words unique to each category, which often surprise the scholars and possibly lead to new
knowledge discovery. The two classifiers choose different top features but reach comparable
performances for the sentimentality classification. Hence for the purpose feature-category
correlation analysis the two methods should be used as complemental to each other rather
than one over the other.
Neither classifiers approve to be effective for eroticism retrieval, which indicates eroticism
is a complicated concept for bags-of-words representation. Sentimentalism is more straight-
forward with both classifiers achieving fast learning curve and strong prediction confidence.
This study also finds that Odds Ratio is better than SVM as feature selection method for
naive Bayes. However Odds Ratio cannot improve the SVM performance. Without feature
selection the stemmed and unstemmed features obtain similar classification accuracies in
both cases. The micro level analysis finds that the effects of good mergings and bad mergings
are neutralized overall. Stemming does not affect feature selection in eroticism classification,
but it negatively affect both feature selection methods, especially SVM, in sentimentalism
classification.
The stop words obtained from the Brown corpus are also overly common and useless in
sentimentality classification. However, the Brown stop words are mostly uncommon in the
Dickinson collection. Personal pronouns - the group of function words usually treated as
stop words - turns out to be highly relevant features for eroticism classification.
This study extends the empirical evaluation of text classification methods to emotion
classification tasks in the literary domain. Some conclusions are consistent with what are
obtained in topic classification, such as Odds Ratio does not improve SVM performance
and stop word removal might harm classification. Some conclusions contradict previous
results, such as SVM does not beat naive Bayes in both cases. Some findings are new to
5
this area - SVM and naive Bayes select top features in different frequency ranges; stemming
might harm feature selection methods. These experiment results provide new insights to the
relation between classification methods, feature engineering options and non-topic document
properties. They also provide guidance for classification method selection in literary text
classification applications.
This thesis is organized as 9 chapters. Chapter 1 is the introduction. Chapter 2, 3 review
the state-of-the-art in text classification theories and techniques. Chapter 2 introduces text
classification algorithms with emphasis on naive Bayes and SVM. Chapter 3 details the fea-
ture engineering techniques with emphasis on feature selection options. Chapter 4 describes
different types of text classification problems, especially the non-topic text classification
tasks. Chapter 5 introduces the application of text classification in the literary domain and
why this new area is suitable for extensive evaluation for text classification methods. This
chapter also describes the two particular literary text classification problems which will be
studied in this thesis. Chapter 6 describes the experiment design. Chapter 7 and 8 describe
the evaluation results in the eroticism classification case and the sentimentalism classification
case. Chapter 9 concludes this study with future plans. The appendix describes a compar-
ative vocabulary analysis and scholar survey on literary text classification, which initially
motivated this thesis research.
6
Chapter 2
Text classification algorithms
With the increasing need of information organization and knowledge discovery from text
data, many supervised learning algorithms have been used for text document classification.
Sebastiani’s comprehensive survey [90] summarized the following inductive learning methods
for text classification:
• probabilistic classifiers (such as naive Bayes)
• decision trees (such as C4.5)
• regression methods (such as Linear Least-Squares Fit (LLSF))
• online-learning methods (such as Perceptron)
• the Rocchio method borrowed from information retrieval
• neural networks
• lazy learners (such as k-Nearest Neighbors (kNN))
• Support Vector Machines (SVM)
• ensemble learning methods
Some empirical studies have been conducted to compare these methods [27, 104, 42].
Among these methods, naive Bayes and Support Vector Machines are always in the com-
parison list. naive Bayes - a generative classifier - is considered a simple but effective clas-
sification algorithm [65, 26, 87]. It is often used as a baseline algorithm. Support Vector
7
Machines - a discriminative classifier - is considered the best text classification method to
date [42]. The following two section introduces the technical details of the two algorithms.
2.1 The naive Bayes algorithm
The naive Bayes classifier is a highly practical Bayesian learning method. It is based on
the simplifying assumption that the feature values are conditionally independent given the
target value [65]. There are many variations of naive Bayes implementations. McCallum [62]
summarized two major naive Bayes models for text classification - the multi-variate Bernoulli
model and the multinomial model. The multi-variate Bernoulli model takes features with
boolean values (word presence or absence). The multinomial model takes features with
non-negative integer values (word frequencies). Both models assume the feature conditional
independence and disregard word order.
The multi-variate Bernoulli model is also called binary independence model. Given a
training document set D with vocabulary V = w1, w2, . . . , wm, a document is represented as
a binary word feature vector with length m: d = (w1, w2, . . . , wm). Each word feature wj is
“1” if the word occurs in the document, and “0” if it does not occur. This model does not
take into account the word frequencies and document length. Given a classification problem
with l classes, the class membership of a document is determined by
cNB = arg max
l
P (cl|d) = arg max
l
P (cl)P (d|cl) = P (cl)
∏
i
P (wi|cl)
wi(1 − P (wi|cl))
(1−wi)
(2.1)
where P (wi|cl) is estimated as
P (wi|cl) =
nc + mp
n + m
(2.2)
n is the total number of documents. nc is the total number of documents where wi
8
occurs. Sometimes a word never occurs in a category, and thus nc = 0. The smoothing
techniques are needed to save a small amount of probability for words that might appear in
test documents but not in the training data. Here we use the m-estimate in case nc = 0.
For boolean features p = 1
2
and m is usually set to 2.
For the multinomial model, the value of each feature wi is its frequency in the document.
Denote l(cl) as the total length of all the documents in class cl, and |V | as the vocabulary
size, then P (wi|cl) is estimated as
P (wi|cl) =
freq(wi) + 1
l(cl) + |V |
(2.3)
and
P (cl|d) = P (cl)
∏
i
P (wi|cl)
wi. (2.4)
This estimation uses Laplace smoothing (also called “add-one” smoothing).
In the multinomial model the trained classifier should remain the same if we scramble
the words in a document and concatenate all the document examples in each class into
one single example. Hence the individual document length is not involved with probability
estimations.
According to previous research [62, 58], the multi-variate Bernoulli model works well on
data sets with small vocabulary, but multinomial model works better at large vocabulary
sizes. So the multinomial model is more popular in text categorization applications.
For a binary classification problem, the predictions for both models are determined by
the following class posterior ratio PR
PR =
P (c1|d)
P (c2|d)
(2.5)
Document d is classified as c1 if PR > 1 and c2 otherwise. PR can be understood as a
9
confidence measure for the naive Bayes predictions. After the log transformation, a positive
logPR value represents the confidence of assigning the example to c1, and a negative PR
represents the confidence of assigning the example to c2. The larger the |logPR| value, the
more confident the classifier is for the prediction. PR can be used as a scoring method to
rank the prediction results.
2.2 The Support Vector Machines (SVM) algorithm
SVM is a supervised learning method that tries to maximize the generalization and thus over-
come the overfitting problem [99, 13]. Given the training examples (x1, y1), (x2, y2), . . . , (xl, yl),
SVM tries to maximize the margins of the decision boundary by finding the maximum of
the functional
W (α) =
l
∑
i=1
αi −
1
2
l
∑
i
l
∑
j
αiαjyiyjK(xi, xj) (2.6)
subject to the constraints
l
∑
i=1
αiyi = 0, αi ≥ 0, i = 1, 2, . . . , l. (2.7)
The document examples on the margins have non-zero αi values. They are called the
Support Vectors (SV). The other documents have zero αi values, and considered not con-
tributative to the classification.
In the above formula K(xi, xj) is the kernel function. Although SVM can handle non-
linear boundaries with the kernel tricks, studies show that linear kernel suits the text cat-
egorization problem well, and that polynomial kernel and RBF kernel do not improve the
performance significantly [55]. Therefore we stick to the simple linear kernel K(xi, xj) = xi·xj
in this study.
10
Given a test example x, the linear decision function is
f(x) = w · x + b (2.8)
where
w =
l
∑
i=1
αiyixi (2.9)
and
b = yi − w · xi (2.10)
The classification decision is D = sign(f(x)).
The value of the decision function, SVM’s output for each prediction, can be considered
as a kind of confidence criterion of the prediction. The larger the absolute value, the farther
the point is from the decision boundary, and therefore the classifier is more “sure” of the
prediction.
This is a rough mapping between the SVM output and the prediction confidence. A
sigmoid model
P (y = 1|f) =
1
1 + exp(Af + B)
(2.11)
can be used to transform the SVM output to class posterior [79, 27].
11
Chapter 3
Feature engineering
Feature engineering is as important as algorithm selection for text classification [33]. Many
feature engineering options are available to change the number and the values of the features.
This chapter will firstly review the candidate document representations for text data, which
determines the feature spaces. Then it will review the feature engineering options for the
“bag-of-words” feature space.
3.1 Bag-of-words vs. complex document
representations
To date “bag-of-words” (BOW) is the most popular document representation model for text
classification. The BOW model simplifies the representation of a document to a word vector
with each dimension corresponding to a word. The value of a dimension could be word
presence/absence, word frequency, normalized word frequency, or word tfidf value.
The BOW model holds an assumption that the document is simply a collection of in-
dividual words without order – in other words, the context of a word does not affect its
meaning. To relax this assumption, more sophisticated document representations have been
explored to encode the contextual information.
In 1995 Cohen [19] used RIPPER, a rule-based classifier, to evaluate the word position
relation predicates as text representation. Such first-order representation did not signif-
icantly improve the classification performance. In 1999 Scott and Matwin [89] used the
12
same algorithm to evaluate other document representations based on various syntactic and
semantic relations between words (phrases, synonyms and hypernyms). Again, the new
representations did not produce significant performance improvements.
Due to the high dimensionality of text representation, statistical methods became more
preferable than rule-based learners. Cohen [20] summarized two ways to encode context
information for statistical classifiers. One is to build a linear classifier that uses complex
features; the other is to learn a nonlinear classifier with the features in the form of a function
of simple word features.
However, experiment results showed that complex features did not help statistical clas-
sifiers gain significant performance improvements either. In 1992 Lewis [57] used a prob-
abilistic classifier to evaluate the effectiveness of syntactic phrases and phrase clusters as
text representations. His results showed that these complex features were inferior to word-
based features for the probabilistic text classifier. In 1998 Dumais et al. [27] found that
NLP-derived phrases did not produce improvements for SVM. In 2004 Moschitti and Basili
[69] undertook a comprehensive study of complex linguistic features for text classfication.
After evaluating various context-encoded representations using four corpora and three linear
classifiers (Rocchio, RPC and SVM), they concluded that noun phrases and word senses do
not improve the linear text classifiers’ performance.
With the popularity of the SVM algorithm, the SVM kernel trick offers a new approach
to learn nonlinear text classifiers. Joachims [42] found that polynomial kernels and RBF
kernels were better than linear SVM to classify the top ten Reuters categories. Lodhi et al.
[60] even considered documents as symbol sequences and proposed new string subsequence
kernel (SSK) and n-gram kernel (NGK). But to their disappointment the evaluation results
on the top ten Reuters categories demonstrated that SSK and NGK could not beat the
simple linear word kernel. To date linear SVM is still the most frequently used SVM model
for text classification [27, 104, 33, 37].
13
3.2 Feature engineering options
In general there are two kinds of feature engineering options for the BOW model: feature
reduction and feature reweighting.
Text data presents unique characteristics which make feature reduction important and
difficult. Firstly, the dimensionality of the feature space is high, and so is the risk of model
overfitting to the training data. Feature reduction methods can help the classification algo-
rithm generate smaller model to reduce overfitting. But aggressive feature reduction might
hurt the classification performance because many features are relevant. On the other hand,
high reduction rate is possible because many features are redundant. A good feature reduc-
tion method should take into account both relevance and redundancy [43].
Feature reweighting strategies adjust the weights of the features instead of removing
them out of the feature set. Often based on heuristics, feature reweighting techniques are
expected to assign heavier weights to more important features and produce performance
improvements in consequence.
The rest of this section briefly overviews the feature reduction approach and the feature
re-weighting approach. The next section will focus on three feature reduction methods -
feature merging, arbitrary feature selection and statistical feature selection.
3.2.1 Feature reduction
In text classification tasks the number of word features is usually large, but in many cases
the number of available training documents is relatively small. Therefore a complicated
model will be learned from the small training set with the large feature set, which leads to
large generalization error.
According to the statistical learning theory, with probability at least (1 − δ),
ErrD(h) < ErrS(h) +
√
[kV C(H) + ln(1/δ)]/m (3.1)
14
This formula explains the relation between the true error ErrD(h), the empirical error
ErrS(h), and the training set size m and the hypothesis space size H . The VC dimension
for an N -dimensional linear function is N + 1, so for a linear classifier, V C(H) is N + 1.
This formula shows that the upper bound of the true error increases with smaller training
set size and larger feature space. Hence feature reduction is important to reduce noise, avoid
overfitting and reduce computational cost.
3.2.2 Feature re-weighting
Compared to the feature reduction strategies, feature reweighting techniques aim to adjust
the values of the features to improve classification effectiveness. The value of a word feature
could be represented as boolean (presence or absence), or the raw count of its occurrences
in the document, or the normalized word count by the document length, or the normalized
word count by the word’s inversed document frequency (or called TFIDF).
TFIDF is originally an indexing technique for information retrieval. Then it was borrowed
to preprocess documents for classification tasks. Joachims [42] reported positive results of
using TFIDF for the SVM classifier to classify top ten Reuters categories. TFIDF punishes
the common words with high document frequencies. However, in some cases common words
turn out to be relevant features. For example, prepositions are useful for joint venture
document classification [84]; function words are even the main feature set for computational
stylistic analysis. Furthermore, TFIDF does not take into account the category information.
Hence the usefulness of TFIDF is actually task-dependent for text classification.
3.3 Feature merging
Feature merging means to merge multiple features into one feature. Popular feature merging
techniques include case merging, stemming and word clustering.
15
3.3.1 Case merging
As a “default” operation in many text mining tools, case merging assumes that capitalized
words or uppercase words bear the same meaning as their lowercase counterparts. For most
English texts, capitalized words often occur at the beginning of a sentence, and therefore
carry word location information, but in the BOW model it is reasonable to discard such
information.
However, there is an exception for everything. In literary texts, the writers are recognized
by their unique writing styles. Dickinson is known for her unconventional capitalization.
Many words, especially nouns, were capitalized no matter where they occurred. A Dickinson
scholar explained it as an old-fashioned emphasis borrowed from German. Dickinson used
both “Joy” and “joy”, but always used ”Nature” instead of “nature”. So would “Joy” and
“joy” bear different meanings in terms of erotics? One can not take it for granted that case
does not matter for Dickinson.
3.3.2 Stemming
Stemming ia also a technique borrowed from information retrieval. A stemmer determines
the stem form of a inflected word. Porter Stemmer [80] is the most popular English stemmer.
Its source code can be obtained online [80]. The Porter Stemmer removes the common
morphological and inflexional endings from English words. The Porter Stemmer cannot
process irregular nouns and verbs, but it is believed that this shortcoming does not affect
its overall performance seriously because of the limited irregular words in English. Extra
look-up tables for irregular nouns and verbs can be obtained as complemetary to Porter’s
rule-based stemmer 1 2.
For a text classification task, the stemming process conflates a group of word features
1An irregular verb list is available at http://www.learnenglish.de/Level1/IRREGULARVERBS.htm.
2An irregular noun list is available at http://www.esldesk.com/esl-quizzes/irregular-nouns/irregular-
nouns.htm.
16
with the same stems into one single feature, under the assumption that words with the same
stem contribute the same way to classification. But there is no consistent conclusion on the
effectiveness of stemming in both information retrieval [41, 18] and text categorization [89,
84, 90]. For example, singular and plural nouns and different verb forms contribute differently
to terrorism document classification [84]. Verb tenses are also important in customer intent
analysis. As an example, the intent information is wiped off if “buy”, “bought”, “will buy”
and “won’t buy” are conflated into one word “buy”.
3.3.3 Word clustering
Another idea of feature reduction is to group “similar” features together into metafeatures.
Various methods have been proposed based on different “similarity” measures.
Lewis [57] used Reciprocal Nearest Neighbor (RNN) method [71] to cluster semantically
related terms. The evaluation results showed that word clusters were no better than single
words for text classification.
Latent Semantic Indexing (LSI) [24, 76] is an unsupervised learning method for dimension
reduction. The singular value decomposition (SVD) of the term-by-document matrix results
in a reduced-dimension vector for each term and each document. LSI was originally used
for information retrieval. It was later used for text classification by referring the document
collection as training examples and the queries as test examples. The membership of a test
example is determined by its similarity to the class centroids [28], or by the votes of a range
of different training examples [107]. As an unsupervised learning method, LSI does not take
into account category information. To date the effectiveness of LSI for text classification
is evaluated on topic classification tasks. It is not clear if LSI is also effective for text
classfication based on non-topic document properties.
Distributional clustering is another word clustering method applied to document classifi-
cation [7]. Two words are joined as one if they contribute similarly to classification. Joining
similar words will reduce the data sparseness and thus result in more reliable estimates. The
17
similarity between two word distributions is measured by a variant of the Kullback-Leibler
divergence.
3.4 Arbitrary feature selection
Some features are selected or removed based on certain heuristics or domain knowledge,
such as removing rare words and overly common words, and selecting words with specified
linguistic properties (e.g. parts-of-speech).
3.4.1 Removing rare words
Term frequency thresholding is the simplest feature selection method. According to Zipf’s
law [52] most words occur only once or a few times in a document collection. Removing
rare words can largely reduce the feature set size. Removing rare words is also important to
avoid overfitting because most documents can be distinguished with a combination of a few
rare words.
3.4.2 Removing stop words
In information retrieval the extremely common words (such as “the”, “of”, etc.) are usually
removed from the queries and the document indices to save space and speed up searches.
These common words are also called “stop words”.
Naturally stopword removal was borrowed from information retrieval to text classification
in order to reduce feature space. But stopwords proved not to be always useless for text
classification. As mentioned in the feature re-weighting section, the usefulness of stopwords
in text classification is actually task-dependent. For example the pronoun “my” was found to
be a very useful word feature to identify student homepages [62]. Prepositions were also found
to be highly discriminative features in joint venture document classification [84]. Stopwords
18
are even the major stylistic markers in genre analysis, stylistic analysis and authorship
attribution [4, 40].
So are there “stop words” for text classification? It depends on the definition of the “stop
words” and their relations to the category. Since common words are mostly function words,
the concepts “stop words”, “common words”, and “function words” are usually considered
as synonyms in information retrieval and text classification. But “common words” and
“function words” are overlapping but not equivalent concepts.
Korfhage defined stop words generally as the ones that will be ignored in information
processing [52]. Sometimes “stopwords” are defined as highly frequent words in a collection,
which is equivalent to the concept of “common words” [6, 47]. Under this definition, the stop
word selection criterion is the word frequency. Because the cutting threshold is an arbitrary
decision, the size of a stop word list could range from a few words to a few hundred. This
selection criterion is also domain dependent. For example “data” is not a common word
in every day English, but it is a highly frequent word in data mining literature. A stop
word list generated by frequency count in one collection might not be appropriate for other
domains. However, some stopwords generated from benchmark corpora are widely used in
text classification tasks [98].
“Stop words” are sometimes defined as the function words. The number of function
words is stable for English, so they are also called “closed-class” words. Function words
do not carry concrete meaning, and therefore do not contribute much to query-document
matching and topic categorization [61].
However, function words have important roles in grammar. Biber [10, 11] used factor
analysis to discover variations in English registers. Biber’s results showed the close relations
between word parts-of-speech and the document genres 3. Function words are identified as
important genre indicators.
3Despite the arguments on the difference between the concepts “register” and “genre”, in this thesis the
two terms are used as synonyms.
19
In summary function words could be relevant features for genre or style related text
classification. Arbitrary stopword removal does not consider category information and might
not be reliable for non-topic classification problems.
3.4.3 Task-dependent feature selection
The controversies that happened in the above arbitrary feature reduction operations demon-
strate the importance of domain knowledge in feature engineering. For example, for the
sentimentalism classification problem the classifiers are expected to learn a preditive model
based on the sentimental vocabulary. As domain experts, some literary scholars suggested
excluding proper nouns, most of which are character names. In sentimental novels, some
characters were “designed” to be more sentimental than others. If a classifier learns the
character names as discriminative features, such classifier will be useless in searching for
sentimental content in other novels.
3.5 Feature selection based on statistical criteria
Statistical feature selection techniques choose the best feature subset for the classifiers based
on statistical analysis of the features’ correlation to the categories. Such feature selection is
expected to remove noise in the data and improve the classification performance.
Because induction of minimal structures is an NP-hard problem [44], heuristic search is
usually used to select optimal feature subset. John et al. proposed two feature selection
model - the filter model and the wrapper model [44]. In the filter approach the feature
selection is independent of the classification method. In the wrapper approach the evaluation
function in the same classification method is used to select the best feature subset.
20
3.5.1 The filtering approach
The filtering approach has been extensively studied in text categorization research. Relevant
experiment results are summarized as below.
In 1997 Yang and Pedersen [105] compared five feature selection methods - document
frequency thresholding (DF), mutual information (MI), information gain (IG), χ2 statistic
(CHI) and term strength (TS), teamed up with two classification methods - KNN and LLSF.
They reported that information gain was one of the best feature selection measures.
But in 1999 Mladenic and Grobelnik [67] reached different conclusion after comparing ten
feature selection methods teamed up with multinomial naive Bayes classifier. They reported
that Odds Ratio was the best while information gain performed similar to random guess.
In 1998 Joachims [42] compared the effects of information gain on five classification
methods (naive Bayes, kNN, C4.5, Rocchio, and SVM) on the top ten Reuters categories.
Joachims reported that naive Bayes achieves its best performance with all the features. In
other words, information gain contributed negatively to naive Bayes classifier, but it did
help k-NN, Rocchio and C4.5 reduce features and improve classification performance.
Joachims also reported that SVM was insensitive to information gain feature selection.
In other words SVM maintained stable performance whatever the top N features were se-
lected by the information gain measure. Forman [33] conducted a thorough experiment to
investigate the effect of filtering approaches on SVM. His experiment results showed that
none of the available feature selection algorithms can help SVM beat its performance with
all the features.
From the above experiment results we conjecture that 1) information gain improves the
performance of k-NN, Rocchio and C4.5, but not naive Bayes; 2) Odds Ratio improve naive
Bayes performance; and 3) No feature selection can improve SVM performance. It seems
the feature filtering methods interact with classification methods sometimes. If the feature
filters are not independent from the classifiers, are the classifiers’ internal feature selection
21
measures (wrappers) a better choice? The next section reviews the wrapper approach.
3.5.2 The wrapper approach
For binary classification, both naive Bayes and SVM can rank the features by their relevance
to the category and thus select the most discriminative subset during the learning process.
For the multinomial naive Bayes, the assignment of class c1 or c2 is determined by the
following class posterior ratio PR. The weight of a word xi with frequency value is its class
conditional probability ratio wi, which determines its contribution to the class posterior
probability.
PR =
P (c1|d)
P (c2|d)
=
P (c1)
P (c2)
∏
xi
(
P (xi|c1)
P (xi|c2)
)xi (3.2)
logPR = log
P (c1)
P (c2)
+
∑
i
log
P (xi|c1)
P (xi|c2)
· xi = b +
∑
i
wi · xi (3.3)
wi = log
P (xi|c1)
P (xi|c2)
(3.4)
For the multi-variate Bernoulli model, The class posterior ration PR and the weight wi
of a word xi with boolean value are
logPR = log
P (c1)
P (c2)
+
∑
i
1 − P (xi|c1)
1 − P (xi|c2)
+
∑
i
(log
P (xi|c1)
1 − P (xi|c1)
− log
P (xi|c2)
1 − P (xi|c2)
) · xi
= b +
∑
i
wi · xi
wi = log
P (xi|c1)
1 − P (xi|c1)
− log
P (xi|c2)
1 − P (xi|c2)
) (3.5)
Mladenic and Grobelnik [67] found that Odds Ratio works best for naive Bayes among
22
seven common feature selection methods. The four variants of Odds Ratio they studied had
no significant difference in their effect on naive Bayes classification performance. Actually the
standard Odds Ratio is exactly the same feature weighting measure in multi-variate Bernoulli
naive Bayes, and the variant LogProbRatio logP (w|pos)/P (w|neg) is the multinomial naive
Bayes feature ranking measure we derived above. So for naive Bayes the internal feature
selection seems the best choice. A drawback of Odds Ratio is the low reduction rate. Odds
Ratio cannot reduce the features to very small subset, considering no empty doc as the meta
rule [105].
All the aforementioned feature selection methods evaluate each features independently
from other features. If two features are redundant to each other, they will get the same
weight based on these methods. Unfortunately Text classification tasks happen to share the
characteristic that many redundant features exist [42]. So these feature selection methods
can not remove redundant features effectively.
SVM-based feature selection differs from the other methods in that it takes into account
the features and the documents simutaneouly, and therefore could help remove redundant
features. Guyon [37] used SVM as feature selection methods to reduce 7129 gene features to
8 features that are relevant to certain cancers. Since SVM can self-select the best features, it
was then used as a stand-alone feature selection method to reduce redundant features[33, 66]
or explain the learned category concept[106].
During the training process, SVM learns a number of Support Vectors (SVs), each as
a relevant training examples. The complexity of a SVM model can be measured with the
number of SVs it generates. In a good (simple) SVM model the number of SVs is only a
small proportion to the total number of training documents. For example, the number of
support vectors for the top 10 common Reuters categories are mostly between 400 to 1000
4. 1714 SVs were generated for the “earn” category and 2210 for the “acq” category, still
4The results are obtained after repeating Joachims’ SVM experiment in [42]. Special thanks for Joachims
for providing the preprocessed Reuters data.
23
small compared to the training example number close to 10,000. In consequence, only the
features occurred in the SVs will participate in the prediction computation (as shown in the
formula below). All the other features that do not occur in the SVs will get zero weight. For
example, with no feature reduction only 2091 out of 27658 features have non-zero weight in
the SVM model learned from the Reuters “corn” category . Therefore a good SVM model
only selects a small number of relevant features for classification.
w =
l
∑
i=1
αiyixi (3.6)
3.5.3 The interaction between feature selection methods and
classification algorithms
We have learned that Odds Ratio works best for naive Bayes and SVM self-selection works
best for SVM. So are the wrapper approaches generally better than the filter approaches?
Mladenic et al. [66] further studied the interaction between the feature selection methods
and the linear classifiers. Their experiment results show that a feature selection method
which is statistically consistent with the classifier is not guaranteed to be the best. In other
words, self-selection is not always the best choice. In their experiment, the SVM-selected
features work best for all the three algorithms under investigation - naive Bayes, Perceptron
and SVM. It seems SVM has been the winner in both classification and feature selection. But
these evaluations are conducted on topic classification tasks. No results have been reported
on SVM feature selection on other classification tasks.
24
Chapter 4
Beyond topic classification
Chapter 2 and 3 introduced available classification algorithms and feature engineering op-
tions. These methods have been tested on some text collections based on bag-of-words
document representation. The following text collections are often used benchmark data sets
for text categorization algorithm evaluation:
• the Reuters-21578 ModApte collection
The Reuters-21578 collection is the most widely used test collection for text catego-
rization [59]. The documents in this collection appeared on the Reuters newswire in
1987. The ModApte Split version contains 9,603 training documents and 3,299 test
documents. The collection has 135 topic categories, some of which have no or few
positive examples.
• the OHSUMED collection
The OHSUMED test collection [93] consists of 348,566 MEDLINE references. The
fields of a reference entry include the article’s title, abstract and MeSH indexing terms.
When used for text categorization evaluation, the MeSH indexing terms serve as the
labels and the articles’ title and abstract as the document body.
• the 20 UseNet newsgroups collection
The 20 UseNet Newsgroups collection contains 19,997 messages from 20 newsgroups
[54]. The subject and body of each message forms the content of a document, and the
newsgroup names become the categorie labels. Different from the above two collections,
25
this collection uses a larger vocabulary with email dialog writing style.
The documents in the above three test collections are all categorized by topics to meet
the needs of topic classification evaluation, because the mainstream text classification tasks
deal with topic classification. However, some researchers [55, 9] argued that the Reuters
corpus is an “easy” corpus because most stories are shorter than 100 words with restricted
use of vocabulary. Furthermore, the non-topic properties of a document - such as genre,
style, and even attitude (sentiment) - could also be the categories for text documents. In the
past ten years genre-based classification, style-based classification, and especially sentiment
classification become active research areas. The rest of this chapter reviews the related work
in these three subareas of text classification.
4.1 Style and author classification
Style is often considered as orthogonal to topicality for computatinal stylistics research.
Researchers looked for topic-irrelevant stylistic markers which were beyond the writers’ con-
scious control. Under the assumption that each writer has a unique “signature” of writing
style, stylometry study and authorship attribution are two closely-related research topics.
As Craig [21] pointed out, if you can tell the authors apart, you must have learned about
their style differences. Authorship attribution can be traced back to more than a hundred
years ago when Mendenhall applied statistical word analysis for author identification [64].
Mosteller and Wallace [70] used statistical methods to analyze the disputed authorship of
the Federalist Papers. Holmes [39, 40] gave comprephensive reviews of literary style analysis
and authorship attribution before text mining techniques were introduced to the field.
In recent years, supervised learning methods have been applied to style and author
classification. The traditional style markers, such as common word frequencies and text
statistics (including average sentence length, average word length, etc.) proved to be effective
features for inductive learning based style and author classification. With the development
26
of NLP techniques, large text collections can be part-of-speech tagged quickly. POS trigrams
were found also effective as stylistic markers [3]. Many text classification algorithms have
been used for style and author classification, such as neural network [96], decision tree [3],
winnow [51] and SVM [25, 35, 23].
4.2 Genre classification
Different from style, genre is considered more as a multi-dimensional concept crossing topi-
cality and style [48, 10, 11, 49]. However many stylistic markers were found to be effective
features for genre classification as well.
Biber [10] identified seven dimensions of genre using factor analysis - involved vs infor-
mational production, narrative vs non-narrative discourse, explicit vs situation-dependent
reference, overt expression of persuasion/argumentation, abstract vs non-abstract style, on-
line informational elaboration marking stance, and academic hedges. Karlgren [48] used
Biber’s linguistic features and discriminant analysis to identify the genres in the Brown Cor-
pus. The method was successful in binary classification, but the error rate increased fastly
with the increasing number of categories.
Kessler proposed the concept of genre as a bundle of facets , and some are easier for
computers to recognize than others [49]. In his experiments, statistical stylistic markers
were chosen as the feature sets. 499 texts in the Brown corpus were selected as the test
collection. Two algorithms, logistic regression and neural network, were compared for genre
classification. The results showed that neural networks outperformed logistic regression.
Kessler explained that 1) logistic regression holds the assumption of feature independence
while neural network does not, therefore feature interaction is important for genre classi-
fication; 2) no feature selection was done for logistic regression, but cross validation and
cross-entropy feature selection was done for neural network, therefore feature selection is
important to avoid overfitting, given the fact of high dimensional feature space for genre
27
classification data.
Finn and Kushmerick [32] compared the performance of three feature sets - bag of words
(BOW), part-of-speeches (POS) and text statistics (TS) in genre classification tasks, includ-
ing document objectivity/subjectivity classification and positive/negative opinion classifica-
tion. C4.5 was the only classifier used in the study. The results show that all the three feature
sets performed well in the subjectivity/objectivity classification although BOW is slightly
better than the other two. In the opinion classification task, BOW is significantly better
than the other two feature sets. In the domain transfer experiments, the results showed
that POS is the best feature set to train cross domain classifier for subjectivity/objectivity
classification. No feature sets performed well in the domain transfer opinion classification
task.
Santini [88] used naive Bayes, POS n-grams and a subset of the BNC documents for
genre classification. Her results showed that trigrams are better than bigram and unigram
words, and that arbitrary feature selection (pre-set term frequency minimum and maximum)
improves classification performance.
Stamatatos et al. [92] found that common word frequencies, a set of stylistic markers, can
be used to classify text genres. The results also showed that the common words extracted
from large corpus is more reliable than the ones from a small training corpus.
Besides the heuristics of feature selection, statistical measures of corpus-independent
feature sets were proposed for general style-based text classification. Koppel et al. [50]
measured the stability of a feature by whether it can be replaced by alternative feature
without changing the meaning of the text. The stability of a feature can be measured
through machine translation methods. For both the Reuters collection and the BNC corpus,
nouns, especially proper nouns and plural nouns, are highly stable features. The stabilities of
verbs and function words distribute more normally. Combining with the Balanced Winnow
classificatin algorithm, the stability feature selection methods outperformed Odds Ratio - a
statistical feature selection method popular in topic classification.
28
4.3 Sentiment classification
In 1999 Jacob Nielson predicted the growth of reputation managers on the Internet [72, 73].
With the increasing amount of review and blog data [78, 74, 36], sentiment classification is
in demand to identify the attitude/affect/tonality in a text document. Hundreds of papers
have been published in the past few years. Esuli compiled a bibliography for sentiment
classification [31].
Both supervised and unsupervised learning methods have been employed in sentiment
classification [29]. According to the granularity of text unit, sentiment could be the property
of a word, a sentence, a paragraph, or a document. The information extraction based (un-
supervised learning) approaches usually consider smaller text units as the atomic sentiment
units, such as words [85, 102, 95] and sentences [38]. A document’s sentiment is then the
sum of the sentiment of the smaller units. Because there is no training data in the unsuper-
vised learning approaches, all kinds of external resources were utilized to generate sentiment
orientation knowledge bases [101, 17, 94, 95] to assist sentiment classification.
Supervised learning approaches consider sentiment at the level of larger text units, such as
sentence, paragraph and document. Most supervised learning approaches conduct sentiment
classification at the document level. Previous topic classification methods are naturally ex-
tended to this new domain. Chaovalit and Zhou [15] compared supervised learning approach
and the unsupervised learning approach on the movie review data. The results showed that
supervised learning approach is more accurate than unsupervised learning method.
However, the traditional classifiers did not perform as well on sentiment classification
as on topic classification. Many researchers realized that sentiment is a concept far more
complicated than topic [1, 75]. To improve the sentiment classification accuracies, more
linguistic features were extracted to represent sentiment in the documents. Due to the
connection between sentiment and style and genre, the style markers and genre indicators
have been exploited in sentiment classification tasks [22, 5, 8].
29
Most of these experiments choose one classifier to compare the performance of different
feature sets. Only a few studies have been conducted on the possible interaction between
classification model and the new problem [22, 75, 83]. Coicidentally, Pang et al. [75], Dave
et al. [22], and Read [83] all compared naive Bayes and SVM on web review classification.
Pang et al. [75] used movie review data and simple unigram word features to compare three
classification methods - naive Bayes, Maximum Entropy and SVM. The results showed that
word presence/absence a better feature representation than word frequency. SVM is worse
than naive Bayes using word frequency features and slightly better than naive Bayes using
word presence/absence features. But overall no method was significantly better than the
others. Dave [22] experimented more feature sets with both naive Bayes and SVM. Read [83]
studied the dependence of naive Bayes and SVM classification models on domain and time.
Again, SVM did not beat naive Bayes in sentiment classification as in topic classification.
4.4 Evaluation of non-topic classification
The experiment results listed above indicated that the classifier performance comparison re-
sults obtained from topic classification might not be transferable to sentiment classification,
or more generally, classification based on other document properties. There might be inter-
action between the text classifiers and the document properties. Morato [68] investigated
the interaction between two unsupervised learning methods - K-means and Chen’s algorithm
- and four document properties “genre”, “register”, “domain terminology” and “document
structure”. The results showed that K-means has dependence on domain terminology and
document structure while Chen’s algorithm has dependence on all the four document prop-
erties. However, few related works have been done to study this problem in the supervised
learning setting.
The previous experiment results in the non-topic classification tasks have shown that
different feature sets suit different classification problems. Indeed, searching for the best
30
candidate features characterizes the major effort in non-topic classification. However, the
feature reduction techniques, an important means to avoid overfitting, were neglected in
the trials of new feature sets. Mladenic et al. [67, 66] has found evidence of the interac-
tion between classification models and feature selection methods in the topic classification
area. Will their conclusion still hold for non-topic classification tasks? What are the rela-
tions between classification models, feature selection methods and the document properties?
Extensive evaluation of the classification methods is needed to answer these questions.
As Sebastiani [90] pointed out, analytical evaluation of text classification is difficult
because of the subjectivity of document category definition. Therefore empirical evaluation
become the major evaluation approach. A big challenge for empirical evaluation of non-topic
classificaton is the lack of benchmark data. Some efforts have been done to solve the problem,
Juola [45] has constructed some ad hoc data sets for authorship attribution competition.
Some customer review collections have been released for sentiment classification. However
it is difficult to build gold standard before the characteristics of these document properties
being thoroughly explored. Experiment results on more ad hoc classification problems are
valuable for the evaluation of classification methods in the non-topic classification settings.
The next chapter will introduce literary text classification, a new text classification area
with abundant non-topic classification tasks and data sets.
31
Chapter 5
Literary text classification and
evaluation
This chapter introduces literary text classification, a new text classification area with abun-
dant non-topic classification tasks. They provide a testbed for extensive text classification
evaluations in non-topic classification settings.
This chapter firstly introduces the emergence of literary text mining and the role of
text classification in literary text mining. It then summarizes the potential applications of
text classification in the literary domain. In the end it analyzes why the literary domain is
suitable for extensive evaluation of text classification methods.
5.1 The emergence of literary text mining
The development of information technology and digital libraries is changing the means of
scholarly communication and research. With the increased abundance of digital material,
making use of the digital collections becomes one vital issue in digital libraries. Future
digital libraries should not only support information organization and access, but also assist
knowledge discovery from digital collections [53, 16].
Over the past decade, text mining techniques have been used for knowledge discovery
from text collections in many domains, such as scientific literature, business documents,
newswires and web pages. Text mining tasks include but are not limited to document classi-
fication and clustering, topic detection and tracking, trend analysis, information extraction,
concept hierarchy construction, text summarization, and question answering. Researchers
have also started to integrate these text mining techniques into digital libraries to provide
32
new information services [103].
Humanities scholars are one large user group in digital libraries [56]. Also in the past ten
years many humanities computing projects (e.g. Perseus, Orlando, VWWP) have digitized
terabytes of full-text humanities resources. Literary texts account for a large amount of the
currently available digital humanities resources.
For decades scholars have used individual text mining techniques to solve literary research
problems, such as authorship attribution [70, 46], stylistic analysis [14], ordering transcript
versions [91], genre analysis [82], and ontology-based information extraction [2].
Recently some scholars who are specialized in humanities computing have started pio-
neering work toward building generic text mining softwares for literary study. The TAPor
project 1 are trying to build an infrastructure for literary text analysis across Canada. The
Nora project 2 in US aims at developing a web-based text mining and visualization tool to
explore 18-19th British and American literary text collections.
Despite the efforts in humanities computing area, current literary text mining applica-
tions have not been accepted by the mainstream literary critic communities. In other words,
the essays published in the Literary and Linguistic Computing journal and the the Ameri-
can Literature History journal do not share common topics. At the same time it has been a
challenge for the Nora project to find representative literary-criticism problems that can be
addressed with data mining tools and techniques.
Why is literary text mining difficult? How can data mining get involved in literary criti-
cal scholarship? Literary scholars attributed the difficulties to the methodological differences
in scientific research and humanities scholarship, and proposed possible solutions from their
perspective [86, 81, 63]. The critical literature analysis and scholar survey described in the
Appendix investigates this problem from data mining perspective. This study found that
scholars usually assemble examples to form their arguments. But the reasoning logic that
1Project TAPor: Text Analysis Portal for Research http://tapor.humanities.mcmaster.ca/home.html
2Project Nora: Web-based Text Mining and Visualization for Humanities Digital Libraries
http://www.noraproject.org
33
scholars use to construct their arguments from evidences are far more than algorithmic,
therefore data mining involvement at the argument construction level will be difficult. How-
ever, data mining can assist collecting the low-level evidences and examples by intelligent
pattern searching and user-oriented document organization. Automating or semi-automating
such processes will strengthen the navigation of large humanities digital library collections
- a function urgently needed by the scholars [30]. Automatic text classification is one of the
data mining technqiues to serve this goal.
5.2 Literary text classification
Some well-studied literary text mining problems were modeled as text classification problems,
such as authorship attribution [46, 96, 70]. A classification algorithm learns a classifier from
the candidate authors’ other works, and then uses the classifier to predict the authorship of
the disputed literary works.
The critical literature analysis and scholar survey in Appendix A confirmed that text
classification is one of the heavily used “scholarly primitive” [97]. The scholars organized
the literary texts according to various criteria. For example, scholars might organize poems
by their lyrical characteristics, rhythms, forms, and subgenres (e.g. ekphrastic poems 3,
historicist catalog poems 4, etc.) The survey also found that all the three purposes of
classification (see Chapter 1) are involved in literary text classification.
Text classification can be used for example-based retrieval in large digital collections, for
example the search for poems in the same subgenre. Usually a scholar knows a few poems
in a subgenre but needs to collect more similar ones to study them. Such information needs
are common in literary research. The task is similar to information retrieval, except that
3Ekphrasis is the “literary representation of visual art” (Hefferman). An ekphrastic poem is written in
response to all kinds of artworks, including drawings, paintings, sculpture, dance, movie, etc.
4Professor Ted Underwood describes it as “a speaker looks at an object (the moon or the sea, or whatever)
and thinks about all the different civilizations that have seen the same object, imagining what they may
have felt, and implicitly contrasting them to the present.
34
the queries are replaced with document examples. Such search function is especially useful
when a user’s information need is difficult to formulate as a keyword-based query 5.
Sometimes feature-category correlation analysis is the real purpose of literary text clas-
sification. In such cases the literary scholar already knows the text collection well, and
wants to use classification as a way to analyze the target concept, and possibly generate new
hypotheses from the classification and feature analysis results. For example, decision-tree
classification and concept-tree clustering methods were used to explore the correlation be-
tween the graphic properties and the genres of Shakespeare’s plays [82]. Scholars know that
“Othello” is a tragedy, but they discovered from the clustering results that “Othello” is more
like a comedy in terms of theatrical structure characteristics. This discovery confirmed the
conclusion of a previous critical study that “Othello” has much in common with comedy.
Sometimes prediction and correlation analysis are both purposes of literary text clas-
sification. For example in authorship attribution, the scholars need to identify the most
discriminative stylistic markers as evidence to validate the authorship attribution. There-
fore the above two tasks are often the two sides of a coin, just as Craig [21] concluded that
“classification and description can be mutually supportive: the first confirms the validity
of the second, while the second helps to establish the stylistic mechanisms underlying a
successful classification.”
The scholars may also apply the classifier learned from one text collection to another
one as a means of comparative study. For example, scholars may want to use the classifier
learned from Dickinson’s erotic poems to study the erotic language use in Victorian women
writers’ works.
In summary, typical use scenarios of text classification for literary study include:
1. prediction
5Ellis and Oldman [30] reported the dissatisfaction with digital libraries by English scholars. They
complained that keyword search cannot return comprehensive results, and current digital libraries deprive
English scholars of freely browsing, a traditional library activity which helps them to discover knowledge
through serendipity.
35
2. feature-category correlation analysis
3. example based retrieval
4. comparative study
5.3 Evaluation of text classification methods in
literary domain
Literary text mining is a new research area. There is no empirical comparison about the
effectiveness of current text classification methods on literary text classification tasks. Most
literary text classification research focuses on one single method without comparing to other
methods. Discriminant analysis was used for authorship attribution [21]. Perceptron was
used to classify philosophical text segments to three classes ”perception”, ”knowledge” and
”mind” [77]. Cross-methods comparison results will be more valuable toward choosing ap-
propriate algorithms for future literary text classification applications.
On the other hand, literary text classification differs from current text classification in
other domains in the following ways:
1. data - literary texts exibit varieties of language because of their long history and
creative characteristics;
2. category labels - literary scholars assign many kinds of text category labels by top-
ics, styles, genres, authors, eras, and many other literary concepts that cross these
categories;
3. use scenarios - literary scholars use classifiers also as example-based retrieval tools as
well as feature-category correlation analysis tools.
Therefore literary text classification problems provide a new testbed for extensive text
classification, which is to date limited to the topic classification setting. The rest of this
36
chapter introduces two literary text classification problems - the eroticism classification in
Dickinson’s poems and sentimentalism classification of early American novel chapters.
Both problems are collected from the Nora project. All the documents are xml files
tagged with the TEI-LITE schema. The documents are borrowed from multiple institutions,
including the Institute for Advanced Technology in the Humanities in University of Virginia,
University of Maryland, Indiana University, and University of North Carolina at Chapel Hill,
the Library of Congress, Brown University, etc.
5.4 The eroticism classification of Dickinson’s poems
The eroticism in Dickinson’s poems have been a primary research topic in Dickinson over
the past decades. The scholars have been arguing about what counts as and constitutes the
erotic in Dickinson. To study the erotic language patterns in Dickinson’s poems, a group of
Dickinson scholars in University of Maryland at College Park compiled a Dickinson erotic
poem collection which consists of 269 XML-encoded letters comprising nearly all the cor-
respondence between the poet Emily Dickinson and Susan Huntington (Gilbert) Dickinson,
her sister-in-law. The long letters consisting of both erotic and not-erotic contents were
excluded from the collection. The scholars assessed the 269 letters as either erotic or not.
Finally 102 letters were labeled as erotic, and 167 not-erotic.
The scholars also listed a set of linguistic indicators for eroticism based on their memory.
Interestingly, some indicators in the list never occurred in the poems. This is an evidence
that computational methods are needed to extend the scholars’ memory.
With the poems labeled as erotic or not, text classification methods can be employed to
automatically find “erotic vocabulary” - the linguistic indicators which characterize the eroti-
cism in the poems. An observation of the scholars’ classification process helps us understand
the characteristics of the concept “eroticism”.
Scholars read the poems and assessed the erotics based on the text and its social and
37
historical context. Below are two examples showing how scholars decipher the eroticism in
the poems.
DEAmsEDCSHDhb195.1 is an erotic poem. A scholar’s comments are shown to the right
of each line with erotic meaning 6.
The Bumble of a Bee - # alliteration, mirror of animal sound.
A Witchcraft, yieldeth me. # yieldeth - erotic; witchcraft - erotic
If any ask me ‘‘Why - " #
’Twere easier to die #
Than tell! # shame
#
The Red upon the Hill # red - erotic color
Taketh away my will - # like yielding
If Anybody sneer, # shame
Take care, for God is near - #
That’s all! #
#
The Breaking of the Day - # special significance
Addeth to my Degree - #
If any ask me ‘‘how" - #
Artist who drew me so - # God - I can’t help myself.
Must tell! # I’m yielding to Nature.
# God made me that way.
#
Emily - #
DEAmsEDCHDh293.1 is not an erotic poem. A scholar’s comments why it is not erotic
is also shown to the right of each line.
‘‘Nature" is what # Nature - distance
We See - # See - rationality
The Hill - the # Hill
Afternoon - # Afternoon
Squirrel - Eclipse - # Squirrel, Eclipse
the Bumble bee - # Bumble Bee
Nay - Nature is # above is a list of Nature we see
Heaven - #
Nature is what #
6In American and European literature witchcraft is all about female power subverting and more authority.
Salem witch trials were a release of prepressed sexual energy.
38
we hear - # hearing only slightly less rational than seeing
The Bobolink - # Bobolink
the Sea - # Sea
Thunder - the # Thunder
Cricket - # Cricket
Nay - Nature is # another list of Nature we hear
Harmony - # rationalized concept
Nature is what #
We know - # less rational than hearing
Yet have no Art #
to say - #
So impotent Our # not erotic
Wisdom is #
To her Simplicity #
#
Emily - #
Comparing the two poems and their comments we can see that the scholars have some
eroticism metaphors in their mind, such as words about animal sounds “Bumble”, erotic
images “witchcraft”, and colors (“red”). Some metaphors cannot be expressed as single
words, such as the reference to “God” - “Artist who drew me so”, and the feeling of shame.
The scholars also have some metaphors identify poems that are not erotic, such as the
rational senses (“see”, “hear”, “know”) and images (natural animals and phenomena). Also,
some non-erotic metaphors can not be expressed as single words, such as parallelism - a kind
of rhetorical devices. The implicit metaphors will be a great challenge for text classification
methods. So this classification problem is harder than topic classification.
It is interesting that “Bumble Bee” occurred in both poems but with different interpreta-
tions. The phrase “The Bumble of a Bee” is interpreted as erotic because it is an alliteration
and mirroring animal sounds is usually considered as erotic. In the second poem “the Bum-
ble Bee” is not erotic because it is just one item in a list of things of Nature. However, the
two expressions are exactly the same with the bag-of-words representation and stop word
removal. Furthermore, the first poem is all about “me” - a stop word in the stopword list
drawn from the Brown Corpus [34]. Will stop word removal affect the classification of erotic
39
poems? Are there any difference between “Bee” and “Bees”, or “Woman” and “Women” in
terms of eroticism? Will stemming affect the classification of erotic poems?
Just as what we have observed in the above two poems, Dickinson is known for her
unconventional capitalization. A Dickinson expert explained it as an old-fashioned emphasis
borrowed from German. Many words, especially nouns, were capitalized no matter where
they occurred. For example, Dickinson used both “Joy” and “joy”, but always used ”Nature”
instead of “nature”. So would “Joy” and “joy” bear different meanings in terms of erotics?
Will case merging affect the classification of erotic poems?
According to the Zipf’s law, most words appear very few times in a document collection.
The Dickinson poem collection is no exception. The Dickinson collection has 3984 unique
word tokens with 2731 (68.5%) occurs only once and 546 (13.7%) occurs only twice.
However, most Dickinson’s poems are very short. The average document length is 61.45
words. 190 out of the 269 poems are shorther than the average length. In consequence
the average word frequence is as low as 4 for the Dickinson collection (see table 5.1). It is
not surprising that literary documents use richer vocabularies. However, with most words
occur only a few times in the collection, few individual word’s discriminative power can cover
the entire collection. Hence a large classification model with many relevant features will be
generated. How will the low average word frequence affect the performance of classification
algorithms? How can feature selection help reduce the model size in this case? This study
tries to answer these questions with the classification experiments.
5.5 The sentimentalism classification of early
American novel chapters
The sentimental novels are a subgenre in early American novels. To explore what linguistic
patterns characterize the genre of sentimentalism, two literary scholars at the University of
Virginia constructed a data collection of five representative sentimental novels “Uncle Tom’s
40
Cabin”, “Incidents in the Life of a Slave Girl”, “Charlotte: a Tale of Truth”, “Charlotte’s
Daughter”, and “Minister’s wooing”. The scholars in UVA assessed the sentimentality score
of each of the 184 chapters. The scores scale from 1 to 10. Half point is also allowed.
Sometimes the scores assessed by the two scholars differ significantly. Hence both the average
and the difference of the two scores are also recorded.
1. The Minister’s Wooing - Seaf702, 42 chapters, author Stowe
2. Uncle Tom’s Cabin - Seaf709, 45 chapters, author Stowe
3. Incidents in the Life of a Slave Girl - Jeaf490, 41 chapters, author Jacobs
4. Charlotte: A Tale of Truth - eaf325, 35 chapters, author Rowson
5. Charlotte’s Daughter - eaf331, 20 chapters, author Rowson
The scholars also grouped the scores into three categories - low, medium, and high. A
chapter is categorized as “high-sentimental” if its score is higher than 6.5, “low-sentimental”
if the score is lower than 3.5, and “medium-sentimental” if the score is between 3.5 and 6.5.
A total of 95 chapters were then labeled as “high”. Only 6 chapters were labeled as “low”,
so the category “low” and “medium” were merged as “low”, which contains 89 chapters.
The average length of the chapters are 3057 words. The shortest one has about 500
words and the longest one has 10,000 words. But 152 out of 184 chapters are shorter than
5000 words. The documents in this collection are much longer and than the Dickinson poem
collection and the Reuters collection. Many non-sentimental segments are mixed with the
sentimental segments in the sentimental chapters, which bring lots of noise to the data.
DataSet Total words Unique words average document length average word frequence
Dickinson 16643 3984 61.45 4.18
Sentimentalism 562506 21311 3057 26.40
Reuters 1460681 27658 114.15 52.81
Table 5.1: Vocabulary comparison
41
Similar to the Dickinson scholars, the UVA scholars also have some sentimentalism
metaphors in their mind. According to the scholars, an incidence of death is definitely
a 10 - the most sentimental. The words “sorrow”, “anguish”, “tears” and so on are also
highly sentimental. Some characters are more sentimental than the others, so the scholars
try to exclude the names of the characters from their assessment. The concept of “sentimen-
talism” is more straightforward than “eroticism” because in the latter case Dickinson tried
to hide her feelings and her poems are known to be “difficult” to read. The UVA scholars
expect the text classifier to learn a “sentimental vocabulary” and see whether it contains
new knowledge. They also would like to use the learned classifier to find more sentimental
chapters in other novels.
42
Chapter 6
Research questions and experiment
design
This thesis aims to extend the empirical evaluation of classification methods to the emotion
classification tasks in the literary domain. The three classification use scenarios will be
evaluated respectively. This evaluation chooses two popular text classification algorithms -
naive Bayes and SVM, and three feature engineering options - stemming, stopword removal
and statistical feature selection (Odds Ratio and SVM) - as the subjects of evaluation. This
thesis aims to examine the effects of the chosen classifiers and feature engineering options
on the two emotion classification problems, and the interaction between the classifiers and
the feature engineering options.
This chapter describes the design of experiements to answer the following research ques-
tions:
1. Is SVM a better classifier than naive Bayes regarding classification accuracy, new
literary knowledge discovery and potential for example-based retrieval?
2. Is SVM a better feature selection method than Odds Ratio regarding feature reduction
rate and classification accuracy improvement?
3. Does stop word removal affect the classification performance?
4. Does stemming affect the performance of classifiers and feature selection methods?
43
6.1 Evaluation methods and measures
6.1.1 Test methods
A classification algorithm’s performance can be evaluated with a hold-out test or cross
validatiaon test.
A hold-out test is often used to test a classifier’s prediction outcome. In a hold-out
test the entire labeled data set is split into training and testing subsets. classifiers can be
compared with their prediction performances on the test data. The ModApte of Reuters
data [59] is an example of hold-out test.
When the labeled collection is small, one hold-out test is not reliable. But an aver-
age performance can be evaluated with multiple randomized hold-out tests with the same
train/test split ratio, or cross validation. A paired t-test can be used to measure the statis-
tical significance of the difference between the average performances.
Cross Validation (CV) is a simple heuristic evaluation. It is applicable for all classification
methods. In the setting of m-fold cross-validation, a training set is randomly divided into
m-disjoint subsets (folds) of equal size. The classifier is trained m times, each time with a
different fold held out as the validation set. For a data set with n examples, an extreme case
of cross validation is called “leave-one-out” (LOOCV) when m = n. LOOCV is affordable
for small data sets, but m = 10 is a more popular choice for larger data sets.
In this study the 10-fold cross validation is the default test method if no train/test split
is specified. The multiple randomized hold-out tests are used in the learning curve and
confidence curve experiments.
6.1.2 Performance measures
Accuracy, precision, recall, F-measure and BEP are popular prediction performance mea-
sures.
In an evaluation experiment, each algorithm returns a prediction of either positive or
44
negative for each test document. The classification results can be presented as a contingency
table (see table 6.1). “TP” is the number of true positive predictions; “FP” is the number
of false positive predictions; “TN” is the number of true negative predictions; “FN” is the
number of false negative predictions.
Ground Truth
prediction positive negative
positive TP FP
negative FN TN
Table 6.1: Binary classification results in a contigency table
Accuracy is defined as the proportion of correct predictions
Acc =
TP + TN
TP + TN + FP + FN
(6.1)
Precision is defined as the ratio of true positive predictions to the total number of positive
predictions.
Prec =






TP
TP+FP
if TP + FP > 0
0 if TP + FP = 0
Recall is defined as the ratio of true positive predictions to the total number of positive
examples
Rec =







TP
TP+FN
if TP + FN > 0
0 if TP + FN = 0
Precision and recall describe different aspects of the classification performance, but the
comparison between classifiers is hard with two scores. Fβ-measure, the harmonic mean of
precision and recall, was proposed to get one single measure for convenient system compar-
ison.
45
Fβ =






(1+β2)Prec×Rec
β2(Prec+Rec)
if Prec + Rec > 0
0 if Prec + Rec = 0
Usually β = 1 is chosen to give precision and recall equal weight in the F -measure.
F1 =







2×Prec×Rec
Prec+Rec
if Prec + Rec > 0
0 if Prec + Rec = 0
For binary classification, the decision functions for both naive Bayes and SVM are scoring
functions. A score higher than a threshold means a positive prediction and otherwise a
negative prediction. Moving the score threshold will change the prediction outcome. The
break-even point (BEP) measure is used to make the evaluation result invariant to the score
threshold. BEP measures the point where precision equals recall. For the ranked prediction
results, the BEP value is the accuracy of the top N predictions (N is the total number
of positive examples in the test set). We can see that BEP examines only a portion of
prediction results. It fits the extremely skewed binary classification problems [42], but not
all classification problems. Another problem with BEP is that no binary prediction decision
is made under this measure. The scholars want to know the classifier’s actual decision, so
BEP measure is not adopted in this evaluation.
The macro-averaged or micro-averaged F1 is often used in multi-class evaluation [67,
105, 104], and accuracy is often used in binary classification evaluation [7, 75, 37]. This
study focuses on binary classification, so accuracy is chosen as the classification performance
measure. SVM has a cost factor to tune the training error ratio between positive exampels
and negative examples, which is similar to the prediction score threshold tuning. Hence the
cost factor with the best classification accuracy will be used in this study. No parameter
can be tuned for the naive Bayes model, so the score threshold is still 0.
46
6.1.3 KLD - word similarity measure
The effect of feature engineering process can be evaluated from macro and micro levels.
The overall classification performance can serve as the macro level evaluation. Some special
measures can be used to evaluate the micro level impact of a feature engineering option
on the involved individual features. The following word similarity measure can be used to
evaluate the impact of feature merging on individual features.
Baker and McCallum designed a variant of Kullback-Leibler divergence to measure the
similarity between words with regard to their contributions to classification [7]. In other
words, if two words “vote” similarly to the classificationt tasks, they can be merged as one.
The KL divergence between the class distributions induced by two words wt and ws is
defined as
D(P (C|wt)||P (C|ws)) =
|C|
∑
j=1
P (cj|wt)log(
P (cj|wt)
P (cj|ws)
) (6.2)
The KL divergence is not symmetric, and thus not a similarity measure. To solve this
problem, the KL divergence of each distribution to their mean distribution is averaged by
their weight.
P (wt) · D(P (C|wt)||P (C|wt ∨ ws)) + P (ws) · D(P (C|ws)||P (C|ws ∨ wt)) (6.3)
where
P (C|wt ∨ ws) =
P (wt)
P (wt) + P (ws)
P (C|wt) +
P (ws)
P (ws) + P (wt)
P (C|ws) (6.4)
6.2 Experiment design
The following sets of experiments will be conducted on the eroticism and sentimentalism
data to answer the above research questions.
47
6.2.1 Experiment 1: choosing document representation
For each algorithms, compare the classification accuracies according to the applicable docu-
ment representaions from the choices of boolean, word frequency, normalized word frequency
and tfidf. Choose the best representation for the future experiments. The initial feature set
for the eroticism classification is the full vocabulary without occurred-once words. According
to the scholars’ domain knowledge, the feature set for the sentimentalism classification is the
content words - nouns without proper nouns, verbs, adjectives and adverbs. The minimum
word frequency is set to 5.
6.2.2 Experiment 2: stemming
This experiment evaluates the effect of full or partial stemming on classification performance
at macro and micro levels. At the macro level, it compares the classification accuracies
without stemming, with nouns and verbs stemming and with full stemming. At the micro-
level evaluation it examines different stemming cases (good, bad, or neutral) based on the
averaged KL-divergence ranking.
Because of Dickinson’s unconventional capitalization, case merging is especially evaluated
in a way similar to stemming. Both are feature merging strategies.
6.2.3 Experiment 3: removing stop words
This experiment evaluates the effect of stop word removal on the two classification problems.
Two definitions of stop words are examined separately. For the definition of stop words as
“common words”, the stop word list generated from the Brown Corpus is used to examine
if a general-purpose common word list is still applicable for the new classification problems.
For the definition of stop words as “function words”, the part-of-speech tagged function word
groups - pronouns, prepositions and conjunctions, modals and determiners - are evaluated
respectively regarding their effects on classification.
48
6.2.4 Experiment 4: feature selection - objective evaluation
The effectiveness of classifiers to find small subset of highly correlated features can be mea-
sured objectively or subjectively. User’s satisfaction and sense-making can serve as the
subjective measure. The objective measure involves two aspects: 1) the feature reduction
rate measures the compact level of the feature subset; 2) the classification performance
measures the reliability of the correlations.
Feature reduction rate describes the proportion of features to be removed from the orig-
inal feature set. The bottom line for a reduced feature set is the “no empty documents”
meta rule [105] - every document should have at least one features in this set.
This experiment examines the effects of feature selection objectively. The features are
sorted by their absolute weights in the decision function. The top (heaviest) 10%, 20%,
30%, 40%, 50%, 60%, 70%, 80% and 90% features are selected as new vocabularies to build
smaller document vectors. The feature reduction rates are compared across algorithms. The
accuracies of the classifiers learned from these selected features are compared to see if there
are significant increase or decrease.
To examine the interaction between stemming and feature selection, the above exper-
iments are repeated on stemmed features and unstemmed features. Their results will be
compared.
6.2.5 Experiment 5: feature selection - subjective evaluation
The sorted feature weights are presented to the scholars to see if the top features look
reasonable for them and if they can discover new knowledge from the extracted emotion
vocabulary.
49
6.2.6 Experiment 6: cross feature selection
This experiment examines the performance of naive Bayes with the top SVM features and
SVM with the top naive Bayes features, and compare them with naive Bayes with self-
selected top features and SVM with self-selected top features. The results will conclude
which is the better feature selection method for naive Bayes and SVM respectively.
6.2.7 Experiment 7: learning curve
Besides the classification performance, the learning curve and confidence curve are also
useful measures to estimate a classifer’s potential as example-based retrieval tool. A learning
curve describes a classifier’s classification performance growth with the increasing number
of training examples. The turning point where the curve becomes flat shows the minimum
number of training examples needed for best performance. A classifier’s prediction results
can be sorted by its confidence in each prediction. A confidence curve describes the speed
of performance decrease with the decreasing confidence.
This experiment examines how many training examples should be needed for the clas-
sification task. In this experiment, 10% examples will be reserved as test examples. The
experiment repeats 50 times at each of the training set size from 10%, 20%, 30%, 40%, 50%,
60%, 70%, 80%, to 90%. The averaged classification accuracies will be used to draw the
learning curve.
6.2.8 Experiment 8: confidence curve
This experiment compares the algorithms’ confidence in the predictions. The example set is
randomly split to 60% training set and 40% testing set. The prediction results are sorted by
the classifier’s confidence measure. Here they are the prediction scores for both algorithms.
Coverage is defined as a proportion of the predictions ranked by the classifier’s confidence
measure. For example, after the classifier outputs the ranked prediction results, the 10%
50
coverage will include the top 10% predictions, and the 100% coverage will include all the
prediction results. The prediction accuracies will be obtained at coverage 10%, 20%, 30%,
40%, 50%, 60%, 70%, 80%, 90%, and 100%. The algorithm with a confidence curve closer
to the upper-right of the graph is more confident at the same accuracy rate.
6.3 Algorithm implementation
The multi-variate Bernoulli naive Bayes and the multinomial naive Bayes algorithms are
implemented based on the description in Chapter 6.2 of Mitchell’s Machine Learning text-
book [65], which is also the basic design for some available naive Bayes softwares such as
RAINBOW.
The SVM-Light package is used for the SVM evaluations 1. The package provides pre-
diction values for ranking, but it needs to be revised to output feature ranking results. This
package has been used in many text categorization evaluations.
The Porter stemmer can recognize past tense and present tense verbs by applying the
“-ed/-ing” stemming rules. But the stemmer cannot separate plural nouns from third-person
singular verbs because it uses the same rule “-s” on both of them. For example, the stemmer
chops “sets” to “set” no matter if the context is a verb phrase “sets up” or a noun phrase
“feature sets”. Consequently the stemming rules by themselves can not separate nouns and
verbs. So the effect of noun and verb stemming are evaluated jointly in this study.
The Brill part-of-speech tagger [12] is used to tag the two text collections. The tagger
is trained on modern English corpus. One might doubt its effectiveness on the literary text
written several hundred years ago. It is actually not a serious concern because word-sense
disambiguation (WSD) - the hardest task for the tagger -is not involved in this study. For
the eroticism classification, only function word tags are used, and the function word tags are
usually stable. For the sentimentality classification, the four content word groups are used
1The SVM-Light package is downloaded from http://svmlight.joachims.org/
51
together, so the tagger just needs to separate content words from function words, which is
an easy task.
52
Chapter 7
Case one - Dickinson eroticism
classification
This chapter describes the classification method evaluation results on the Dickinson collec-
tion. The first section compares document feature representations as described in Chapter
6.2.1. The best feature representation model is used in the rest experiments. The second and
third sections evaluate the impact of two feature merging methods, case merging and stem-
ming, on the classification performances. The experiment setting is described in Chapter
6.2.2. The fourth section evaluates the effect of stopwords on the classification performance
as described in Chapter 6.2.3. Based on the previous feature engineering results, the fifth
section compares naive Bayes and SVM regarding their classification accuracy and feature
selection effectiveness subjectively and objectively. The experiment setting is described in
Chapter 6.2.4 and Chapter 6.2.5). Finally we evaluate the potential of naive Bayes and
SVM as example-based retrieval tools according to their learning curves (described in Chap-
ter 6.2.7) and the confiden curves (described in Chapter 6.2.8).
7.1 choosing document representation
The vocabulary of the Dickinson collection vocabulary consists of 3984 word tokens. 2731
of the 3984 unique tokens occur only once. To avoid overfitting, these occurred-once tokens
are excluded from the feature set. The remaining feature set contains 1253 word features.
Table 7.1 shows the 10-fold cross validation results for naive Bayes and SVM using four
document feature representations - boolean, word frequency, normalized word frequency and
word tfidf.
53
All the four representations are applicable to SVM, which achieves the best average ac-
curacy with the normalized word frequency representation. According to the paired t-test
results, these average accuracy differences are not significant at p<0.05. The top ranked fea-
tures are also similar across the representations. However, the normalized word frequency
representation is more convenient than the other ones for SVM because the important pa-
rameter C is normalized to 1 for the C-SVC method in the svm-light package. No further C
parameter tuning is need. Therefore the normalized word frequency representation is chosen
for the rest SVM experiments.
For SVM, the generated model contains more than 200 support vectors, which confirms
the prediction in chapter 5 that large model will be generated for the Dickinson collection.
Only boolean and word frequency representation are applicable to naive Bayes, which
achieves better performance with the word frequency representation. Again, the paired t-
test result shows that such average accuracy difference is not significant at p<0.05. The
word frequency representation is chosen for the rest naive Bayes experiments because of the
higher popularity of multi-normial naive Bayes model [62].
Both naive Bayes and SVM algorithms achieve the accuracy rate close to 70%, signifi-
cantly higher than the baseline majority vote approach. But the difference between naive
Bayes and SVM is not significant at p<0.05.
Algorithm Representation 10CV Acc
SVM(j=1.8) boolean 67.0
SVM(j=1.5) frequency 65.1
SVM(j=1.5) normalized 68.8
SVM(j=1.5) tfidf 65.9
naive Bayes boolean 65.4
naive Bayes frequency 67.0
Majority Vote 62.1
Table 7.1: Dickinson document representation comparison
54
7.2 Case merging
This experiment evaluates the impact of case merging on the classification accuracies. Ac-
cording to the previous experiment results, this time the normalized frequency representation
is used for SVM and the frequency representation for naive Bayes. The macro evaluation
experiment examines the overall effect of case merging on classification accuracies using 10-
fold cross validation. The micro evaluation experiment uses a variation of KL-divergence to
measure the strength of each case merging event.
7.2.1 Macro evaluation
case merging feature set SVM naive Bayes
Before 1253 68.7 66.9
After 1049 70.3 68.5
Table 7.2: Dickinson case merging - 10CV macro evaluation
No significant classification accuracy changes are observed in the experiment results as
shown in table 7.2 for both naive Bayes and SVM. Hence case merging does not affect the
classification accuracies at the macro level. But case merging brings an extra benefit that
the feature set is further reduced from 1253 tokens to 1049 tokens.
7.2.2 Micro evaluation
In this experiment, the 514 case merging events are sorted by KLD - their average KL
divergence values to the mean. The minimum KLD value is 0, which means the merging
does not hurt the classification. The higher the KLD value, the greater the merging will
affect the classification negatively.
Table 7.3 shows some representative case merging events on both ends. Some mergings
are good, such as (“Dream” and “dream”), (“Place” and “place”), and (“Road” and “road”).
But some mergings are bad, such as (“Joy” and “joy”), (“Royal” and “royal”), (“Red” and
55
“red), and (“Love” and “love”).
Lets use “Joy” and “joy” as an example. The following contexts of “Joy” and “joy” show
that “Joy” occurs only in non-erotic poems, in abstract thinking situation; but “joy” occurs
in only erotic poems, in everyday life situations. It is evident that Dickinson used “Joy” and
“joy” for different meanings in her poems. They should be treated as two different features
instead of merging to one.
wt ws (f(wt, c1), f(wt, c2)) (f(ws, c1), f(ws, c2)) KLD
Joy joy (0, 3) (5, 0) 1.451
Royal royal (2, 0) (0, 2) 1.386
· · ·
Red red (3, 1) (0, 2) 0.824
· · ·
Love love (3, 11) (8, 3) 0.281
· · ·
Dream dream (4, 0) (3, 0) 0
Place place (0, 2) (0, 6) 0
Road road (0, 3) (0, 3) 0
· · ·
Table 7.3: Dickinson case merging - micro evaluation
The contexts of “Joy” are listed below:
1. DEAmsEDCSHDh287.1 “March is the Month of Expectation”
But pompous
Joy
Betrays us, as
his first
Betrothal
Betrays a
Boy.
2. DEAmsEDCSHDh355.1 “The Treason of an Accent”
The Treason
of an Accent
Might vilify
the Joy -
To breathe -
corrode the
56
rapture
Of Sanctity
to be
3. DEAmsEDCSHDhb91.1 “A Promise is firmer than a Hope”
Boundlessness - Expanse cannot
be lost -
Not Joy, but
a Decree
Is Deity -
His Scene,
Infinity -
The contexts of “joy” are listed below:
1. DEAmsEDCSHDh245.1.txt
Could she have guessed that it would be -
Could but a Crier of the joy
Have climbed the distant hill! -
2. DEAmsEDCSHDhb173.1.txt
I want to send you joy, I have
half a mind to put up one
of these dear little Robin’s, and . . .
3. DEAmsEDCSHDhl1.1.txt
I cant believe you are coming -
but when I think of it, and tell
myself it’s so, a wondrous joy comes
over me, and my old fashioned life . . .
be done. I think with you, dear
Susie, and Mat by me again, I
shall be still for joy. I shall not . . .
way dear Child, he has not done
to this day, and any portion of
which, I would savor with
joy, might I but obtain it.
57
In summary, case merging does not affect the Dickinson poem classification significantly
at the macro level. But this does not mean all the case mergings do not matter. On the
contrary, good and bad merging events occurred simultaneously, although their effects are
neutralized overall.
7.3 Stemming
This experiment evaluates the impact of stemming on the Dickinson poem classification.
The evaluation measures are the same as the ones used in the case merging experiments
because both stemming and case merging are feature merging tools. The Porter stemmer
requires all the words converted into lower case, so this experiment starts with the feature
set with case merging completed. After stemming, the feature set size is further reduced
from 1049 to 911 features.
7.3.1 Macro evaluation
stemming feature set SVM naive Bayes
Before 1049 70.3 68.5
Porter 959 70.3 68.8
Full 911 69.5 70.2
Table 7.4: Dickinson stemming - 10CV macro evaluation
Table 7.4 compares the classification accuracies without stemming, with Porter stemming,
and with both Porter stemming and irregular nouns and verbs stemming. The paired t-test
results did not find any significant accuracy changes among the three experiments. Hence
overall the stemming does not affect classification significantly. Still, the feature set is
reduced by 13% after stemming.
58
7.3.2 Micro analysis
In this experiment, the 109 stemming events are sorted by their KLD values. More than 109
stemming events occurred in the data set, but only the stemming events resulting in feature
merging were counted in here.
Almost all of the 109 stemming events involve nouns and verbs stemming, except only four
adjective and adverb stemming and merging - “lonely”, “beautiful”, “lovely” and “silently”.
So we can approximately say that nouns and verbs stemming do not affect the Dickinson
classification overall.
Lets examine the representative stemming events as shown in table 7.5. Some stemming
and merging events are bad, such as merging “hearts” with “heart”, “women” wit “woman”
and “thought” with “think”. Some stemming and merging events are good, such as merging
“silently” with “silent”. It is evident that for some nouns, the singular forms are more
relevant to erotic poems while the plural forms more relevant to non-erotic poems. Words
like “woman” and “heart” are more self-portraying than their plural forms “women” and
“hearts”.
In summary both case merging and stemming merging do not significantly affect the clas-
sification performance overall, but good mergings and bad mergings occur simultaneously.
The process of case merging and stemming results in 27% feature reduction.
7.4 Removing stop words
This set of experiments examines the effect of stop word removal on Dickinson poem clas-
sification performance. The first experiment adopts the concept of stop words as ‘common
words, and examine if the Brown stop word list is still a valid stop word list for the Dickinson
collection. The second experiment treats stop words as function words, and examine the
effect of specific groups of function words on classification performance.
59
wt ws (f(wt, c1), f(wt, c2)) (f(ws, c1), f(ws, c2)) KLD
hearts heart (0, 2) (16, 7) 1.029
heart heart (16, 5) (16, 7) 0.006
· · ·
thinking think (0, 2) (13, 9) 0.785
thought think (1, 5) (13, 9) 0.259
think think (12, 2) (13, 9) 0.096
· · ·
loved love (3, 0) (16, 17) 0.643
lovely love (1, 2) (16, 17) 0.040
love love (11, 14) (16, 17) 0.002
loving love (1, 1) (16, 17) 0.0004
· · ·
woman woman (5, 0) (8, 3) 0.229
women woman (3, 3) (8, 3) 0.060
· · ·
silently silent (2, 0) (4, 0) 0
silent silent (2, 0) (4, 0) 0
· · ·
Table 7.5: Dickinson stemming - micro analysis
7.4.1 Stop words as common words
Sort the Dickinson vocabulary by document frequency and we can see that no words oc-
cur in every document. The five words with the highest document frequencies are “the”
(DF=219/269), “a” (DF=153/269), “and” (DF=128/269), “to” (DF=154/269) and “of”
(DF=153/269).
The Brown stoplist consists of 425 most common words in the Brown corpus. 306 of the
425 common words were found in Dickinson poems. Most of them are not “common” at all
in the Dickinson collection. Table 7.6 shows that SVM achieves 68.4% accuracy and naive
Bayes 67.0% using the 306 stop words only as the features. The results are comparable to
the best accuracies obtained in previous experiments. In other words, the 306 stop word
features work as well as the 911 word features for the Dickinson poem classification. In
summary they should not be treated as “stop words”.
60
feature set size doc coverage SVM naive Bayes
All 911 100% 69.5 70.2
Brown stoplist 306 100% 68.4 67.0
Table 7.6: Dickinson common words
7.4.2 Stop words as function words
Since there are barely common words in the Dickinson poems, lets examine the other concept
of stop words - function words. In this experiment, the function words are extracted and
grouped by their parts-of-speech. One can hardly imagine that a general purpose part-of-
speech tagger trained from the Wall Stree Journal corpus would be applicable to poem texts.
Luckily, the parts-of-speech of function words are stable. Therefore the regular part-of-speech
taggers should be as effective as the dictionary lookup approach to identify function words.
The Brill tagger was used to extract function words from the Dickinson collection. A
manual evaluation of a sample of 10 poems confirms that part-of-tagger can be used to
identify function words. Some function word groups (such as interjections) are fairly small
and cover only a small portion of the collection. They are not suitable as feature sets for
classification.
The following experiment examines the effects of four function word groups - the deter-
miners, pronouns, modals, and prepositions plus conjunctions on Dickinson poem classifica-
tion. Table 7.7 shows that classification results with these feature sets. The determiners,
modals, and prep-conj groups did not beat the trivial classifier. However, the 29 pronouns
achieved the classification accuracies as well as what are obtained in previous experiments,
except that this feature set covers 87.4% documents in the Dickinson collection.
feature set size doc coverage SVM naive Bayes
Pronoun 29 87.4% 68.4 66.6
Modal 14 46.8% 52.1 47.6
Prep-Conj 67 98.9% 56.2 57.3
Determiner 19 95.5% 59.8 60.3
Table 7.7: Dickinson function words
61
For largest Woman's
Heart I knew -
'Tis little I can do -
And yet the largest
Woman's Heart
Could hold an Arrow - too -
And so, instructed by
my own,
I tenderer, turn me to.
(a) erotic poem
I - then -
shut the door -
Lest my beseeching 
face - at last -
Rejected - be - of Her? 
(b) not-erotic poem (test
example)
A little bread, a crust - a crumb,  
A little trust, a Demijohn,  
Can keep the soul alive,  
Not portly - mind!  
But breathing - warm -  
Conscious, as old 
Napoleon
The night before the Crown!  
A modest lot, a fame petite,  
A brief campaign of sting and sweet,  
Is plenty! is enough!  
A sailor's business is the Shore -  
A soldier's -
Balls!  
Who asketh more  
Must seek the neighboring life!  
(c) not-erotic poem
Figure 7.1: Illustration: color coding poems (RGB)
In summary the Dickinson collection does not have overly common words, which might
be the case for all poems. The common word list extracted from general-purpose corpus
turns out to be an effective feature set for Dickinson erotic poem classification. Although
some function words are not useful, pronouns are effective features for this classification
task. The next set of experiments are going to examine statistical feature reduction. So here
no stop words are removed from the feature set. Lets see if any common words or function
words will rank high with the statistical feature selection measure.
7.5 Feature selection
As Chapter 3 as introduced, feature selection is an approach to find smaller and more robust
classification model as well as explain the correlation between features and categories. Figure
7.1 uses the naive Bayes feature weights to color code Dickinson’s poems. We can see that
the left one is erotic and the right one not erotic. The middle one is something in between.
We can also see that it is a test example because some of its features do not occur in the
training examples and thus not colored.
This set of experiment examines the effect of the self feature selection measures of naive
Bayes and SVM on the Dickinson poem classification performance. The two feature selection
methods will be compared from three aspects: 1) their contributions to classification accu-
racy improvement, 2) reduction rates, and 3) the informativeness of the top discriminative
62
features to the Dickinson scholars. The first two aspects are objective, and the third one is
subjective.
7.5.1 naive Bayes self feature selection - objective evaluation
feature selection feature set naive Bayes
Before 911 68.5
After (60%) 547 81.0
Table 7.8: The effect of naive Bayes self feature selection on eroticism classification
with stemming no stemming
percent features accuracy features accuracy
100% 911 68.5 1253 68.8
90% 820 75.0 1128 72.5
80% 729 76.2 1003 78.8
70% 638 78.4 877 82.5
60% 547 81.0 752 -
50% 456 - 627 -
40% 364 - 501 -
30% 273 - 376 -
20% 182 - 251 -
10% 91 - 125 -
Table 7.9: The effect of naive Bayes self feature selection on eroticism classification
The paired t-test result (table 7.8) shows that the difference before and after feature
selection is significant at p < 0.05. With the increase of the feature reduction rate, the
average accuracy steadily increases from 68.5% to 81.0%, when the reduction rate is 40%.
However naive Bayes self feature selection can only reduce the feature set up to 40% without
generating empty documents.
7.5.2 naive Bayes self feature selection - subjective evaluation
The scholars found the top naive Bayes selected features fairly informative. Actually they
discovered the words “write”, “mine”, and “Vinne” as new erotic indicators:
63
Besides possessiveness, “mine” connotes delving deep, plumbing, penetrating
- all things we associate with the erotic at one point or another. So “mine”
should have already been identified as a “likely hot” word, but has not been,
oddly enough, in the extensive critical literature on Dickinsons desires. “Vinnie”
(Dickinsons sister Lavinia) was also labeled by the data mining classifier as one
of the top five “hot” words. At first, this word appeared to be a mistake, a
choice based on proximity to words that are actually erotic. Many of Dickinsons
effusive expressions to Susan were penned in her early years (written when a
twenty-something) when her letters were long, clearly prose, and full of the daily
details of life in the Dickinson household. While extensive writing has been done
on the blending of the erotic with the domestic, of the familial with the erotic,
and so forth, the determination that “Vinnie” in and of itself was just as erotic as
words like “mine” or “write” was illuminating. The result was a reminder of how
or why some words are considered erotic: by their relationship to other words.
While a scholar may un-self-consciously divide epistolary subjects within the
same letter, sometimes within a sentence or two of one another, into completely
separate categories, the data mining classifier will not. Remembering Dickinson’s
“A pen has so many inflections and a voice but one,” the data mining has made
us, in the words of our subject expert, “plumb much more deeply into little four
and five letter words, the function of which I thought I was already sure, and
has also enabled me to expand and deepen some critical connections I’ve been
making for the last 20 years.”
7.5.3 SVM self feature selection - objective evaluation
feature selection feature set SVM 10CV
Before 911 69.6
After (10%) 91 75.5
Table 7.10: Dickinson SVM self feature selection
Table 7.11 shows that SVM classification accuracies increase with some fluctuations dur-
ing feature reduction. But the paired t-test result shows that no accuracy differences before
and after feature selection are significant at p < 0.05. So the SVM feature reduction does
not affect its classification accuracy significantly. However, we can at least say that SVM
with top 10% features performs as well as with all the features.
64
with stemming no stemming
percent features accuracy features accuracy
100% 911 69.6 1253 68.8
90% 820 67.7 1128 66.9
80% 729 71.0 1003 67.3
70% 638 71.4 877 69.1
60% 547 71.8 752 72.1
50% 456 72.5 627 73.3
40% 364 73.2 501 73.6
30% 273 74.7 376 73.2
20% 182 74.0 251 74.0
10% 91 75.5 125 73.6
Table 7.11: The effect of SVM self feature selection on eroticism classification
7.5.4 SVM self feature selection - subjective evaluation
The top SVM features include many pronouns, such as “you”, “I”, “my”, “me”, “your” and
“her”. The Dickinson scholars agree that the top features make sense because the pronouns
are necessary to construct personal conversations. But the scholars did not find “new”
knowledge from the top SVM feature list.
7.5.5 Comparing naive Bayes and SVM as feature selection
methods
The two feature ranking methods generally agree upon which features are “erotic” and which
are not. Figure 7.2 shows the scatter plot of the two feature weight measures. Most features
fall into the first and third quadrants. However no strong correlation can be seen from the
scatter plot because the two methods choose different kinds of features as the top ones.
There are only 27 shared features in both top 100 feature lists.
Figure 7.3 and 7.4 plot the relation between the feature ranks and their weights. The
weight values are normalized as proportional to the top feature weight. Figure 7.3 shows
that the SVM feature weights decrease quickly and smoothly from top rank to bottom rank.
The small number of top features mainly affect the classification decision. The rest features
65
-1
-0.5
 0
 0.5
 1
-1 -0.5  0  0.5  1
N
B
 w
ei
gh
t
SVM weight
NB SVM feature weight comparison
NB SVM feature weights
NB weights
SVM weights
agreement
Figure 7.2: Figure: naive Bayes and SVM feature ranking agreement
are not important due to the small feature weights. This explains the SVM high reduction
rate from one aspect. Figure 7.4 shows that there are large numbers of naive Bayes features
with same weights. The feature values decrease slowly. Most middle ranked features still
have heavy feature weights. Because of the large number of heavy weight features, naive
Bayes can not achieve high reduction rate.
The above experiment results show that SVM has much higher feature reduction rate than
naive Bayes does. To explain why that happens lets first examine the relation between the
feature ranks and their frequencies. Figure 7.5 shows that high frequent words accumulate
at the top SVM feature ranks. Therefore a small feature subset is enough to cover the whole
collection without generating empty documents. On the contrary, figure 7.6 shows that low
frequent words dominate the top naive Bayes feature ranks. Most high frequent words rank
in the middle, so larger feature subset is needed to avoid generating empty documents. In
consequence naive Bayes cannot achieve high reduction rate.
The different relations between the feature ranks and frequencies might also attribute
66
-0.6
-0.4
-0.2
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  100  200  300  400  500  600  700  800  900  1000
w
ei
gh
t
feature rank
SVM feature ranks and weights
SVM
Figure 7.3: Figure: Dickinson SVM feature ranks and weights
-1
-0.8
-0.6
-0.4
-0.2
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  100  200  300  400  500  600  700  800  900  1000
w
ei
gh
t
feature rank
Naive Bayes feature ranks and weights
Naive Bayes
Figure 7.4: Figure: Dickinson naive Bayes feature ranks and weights
67
 0
 500
 1000
 1500
 2000
 2500
 3000
 0  100  200  300  400  500  600  700  800  900  1000
fr
eq
ue
nc
y
feature rank
SVM feature ranks and frequencies
SVM
Figure 7.5: Figure: Dickinson SVM feature ranks and frequencies
 0
 500
 1000
 1500
 2000
 2500
 3000
 0  100  200  300  400  500  600  700  800  900  1000
fr
eq
ue
nc
y
feature rank
Naive Bayes feature ranks and frequencies
Naive Bayes
Figure 7.6: Figure: Dickinson naive Bayes feature ranks and frequencies
68
to the different informativeness of the two feature selection methods. naive Bayes selects
the unique words in each category as top features, which are usually in low frequencies.
The scholars felt surprised at first but managed to make sense of them later. A possible
reason: because the occurrences are limited, it is easier for the scholars to figure out if the
features are really erotic or not. On the contrary, SVM chooses the high frequent words
as top features. Some of them are within the scholars’ prior knowledge and therefore not
interesting any more, such as the pronouns. For the ones that surprise the scholars, the
large number of contexts around the feature occurrences prevents the scholars from intuitive
decision whether they are erotic or not.
7.6 Cross feature selection
Mladenic et al. [66] found that SVM feature selection is better than naive Bayes (Odds
Ratio) for naive Bayes on the Web data. But the cross feature selection experiment on
the Dickinson collection reaches a different conclusion. Table 7.12 shows that naive Bayes
achieves 76.2% accuracy with the top 10% SVM features, not as good as with 60% naive
Bayes features. In this case naive Bayes self feature selection is better than SVM feature
selection.
On the other hand, the 60% naive Bayes features did not improve the SVM performance.
This result is consistent with the results in related work that no external feature selection
can improve SVM performance.
classifier feature set 10CV accuracy
naive Bayes SVM 10% 76.2
SVM naive Bayes 60% 73.5
Table 7.12: Feature selection method cross comparison
In general the SVM model is more robust because it relies on more frequent features.
However, the Dickinson collection has limited high frequent and discriminative features. The
69
 0.64
 0.66
 0.68
 0.7
 0.72
 0.74
 0.76
 0.78
 0.8
 0.82
 0.84
 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9
A
cc
ur
ac
y
Percent of training examples
Dickinson eroticism classification - learning curve
SVM
NB
Figure 7.7: Figure: Dickinson learning curves
large number of low frequent features provide a chance for naive Bayes algorithm to combine
them to reach higher classification accuracy.
7.7 Learning curve
Figure 7.7 shows that naive Bayes has a better learning curve than SVM. The naive Bayes
accuracy increases faster with the increase of training examples. However a plateau is never
reached for both algorithms, which indicates that the model generated from current data
might not be generalizable.
7.8 Confidence curve
Figure 7.8 shows that the general trends of the two confidence curves are similar. For both
algorithms the classification accuracies decrease at similar speed with the decrease of the
confidence.
70
 0.7
 0.75
 0.8
 0.85
 0.9
 0.95
 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1
A
cc
ur
ac
y
Coverage
Dickinson confidence curve
SVM
Naive Bayes
Figure 7.8: Figure: Dickinson confidence curves
7.9 Summary
This chapter presented the evaluation results on the eroticism classification task. Although
case merging and stemming do not affect the overall classification performance, some good
mergings and bad mergings occur simultaneously. Selective merging might further im-
prove the performance. Overly common words are hardly found in the Dickinson collection.
General-purpose Brown stop word list is not applicable to the Dickinson data because some
stop words, such as pronouns, are highly discriminative features.
Before feature reduction naive Bayes and SVM reach similar classification accuracies.
After feature reduction naive Bayes achieves significantly higher classification accuracy than
SVM does. However SVM has much higher feature reduction rate than naive Bayes. The
two algorithms choose features at different frequency range.
SVM picks high frequent and discriminant words, which are usually within the scholars’
prior knowledge. Naive Bayes picks words unique to each category, which surprise the schol-
ars and result in new knowledge discovery. The poor learning curves of both classification
71
algorithms show that neither one is effective for example-based eroticism retrieval, which
indicates eroticism is a complicated concept for bags-of-words representation.
The next chapter will describe the evaluation results on the sentimentalism classification
task and compare the sentimentalism classification evaluation and the eroticism classification
evaluation.
72
Chapter 8
Case two - sentimental novel chapter
classification
This chapter repeats the classification method evaluation experiments on the sentimental
novel collection, and compares the the evaluation results obtained from the two literary text
collections.
8.1 choosing document representation
The sentimentalism scholars in UVA are most interested in content words. But they sug-
gested that proper nouns be excluded from the feature set becuase most of them are character
names. A sentimentalism classifier aims to learn the sentimental language rather than the
character designs in particular novels. For the latter case, the learned classifiers are not
generalizable to other sentimental documents.
The vocabulary of the sentimentalism collection consists of 19585 word tokens. Because
the average chapter length is much longer than that of the Dickinson poems, the minimum
word frequency is arbitrarily set to 5. Again, the Brill tagger is used to extract content
words - nouns (without proper nouns), verbs, adjectives and adverbs. The final feature set
consists of 5704 unique words.
Table 8.1 shows the 10-fold cross validation results for naive Bayes and SVM using four
document feature representations - boolean, word frequency, normalized word frequency and
word tfidf.
The sentimentalism collection has 95 highly sentimental (positive) chapters and 89 low
sentimental (negative) chapters. It is a fairly balanced data set, so the j parameter is set to 1
73
Algorithm Representation 10CV Acc
SVM(j=1) boolean 62.3
SVM(j=1) frequency 60.8
SVM(j=1) normalized 62.6
SVM(j=1) tfidf 60.4
naive Bayes boolean 65.1
naive Bayes frequency 65.3
Majority Vote 51.6
Table 8.1: Sentimentalism document representation comparison
for SVM. The paired t-test results show that the accuracy differences between them are not
significant. But surprisingly, this time the top ranked features are not similar across different
presentations. According to the heuristics the scholars used in their scoring process, the
following top ranked boolean features are more correlated to sentimentalism than the other
types of top features. Hence the boolean representation is chosen for the rest experiments.
feature rank weight
sorrow 1 0.030512
die 2 0.024936
affection 3 0.024363
pray 4 0.023771
anguish 5 0.023683
heaven 6 0.023256
loved 7 0.023165
bed 8 0.023118
beloved 9 0.023018
sacrifice 10 0.021975
Table 8.2: Sentimentalism SVM top boolean features
For naive Bayes the paired t-test result shows that the difference between boolean and
frequency representations is also not significant. The top ranked features obtained from both
representations are also similar. The scholars’ heuristics are not enough to decide which top
feature lists are more relevant. For the purpose of a fair comparison with SVM, the boolean
representation is also chosen for future experiments.
For SVM, the generated model contains almost all the examples as support vectors, which
74
feature rank weight
heart 1 0.007855
poor 2 0.005988
for 3 0.005049
will 4 0.005022
little 5 0.004848
has 6 0.004720
dear 7 0.004152
love 8 0.003637
no 9 0.003577
one 10 0.003544
Table 8.3: Sentimentalism SVM top raw frequency features
indicates that sentimentalism is another “hard” concept for bag-of-words representation at
the chapter level.
Both naive Bayes and SVM achieve the accuracies significantly higher than the trivial
majority vote baseline. But the difference between naive Bayes and SVM is not significant.
8.2 Stemming
This experiment evaluates the impact of stemming on the sentimentalism classification. At
the macro level we compare the classification accuracies without stemming, with nouns and
verbs stemming only, and with full stemming. At the micro level we examine individual
word form conflations.
8.2.1 Macro evaluation
stemming feature set SVM naive Bayes
before 5704 62.3 65.1
nouns and verbs 4263 66.8 65.5
All 3669 66.7 64.9
Table 8.4: Sentimentalism stemming - macro evaluation
Table 7.4 shows that the classification accuracies are all around 65% whether stemming
75
or not. The paired t-test results did not find any significant accuracy changes among the
three results. About 2/3 stemming events are noun and verb forms stemming. The stemming
does not hurt the classification accuracies significantly, but the feature set is reduced by 36%
after full stemming.
8.2.2 Micro evaluation
In this experiment, the 1288 stemming and merging events are sorted by the KLD measure
(see definition at 6.1.3.
Lets examine the representative stemming events as shown in table 8.5 1. Some confla-
tions are bad, such as merging “wildness” with “wild”, “neatness” with “neat”, and “details”
with “detail”. Some conflations are good, such as merging “difficulties” with “difficulty” and
“wheels” and “wheel”.
Lets look at the word “wildness” as an example. “Wildness” occurred in six chapters, all
related to sentimental scenes, but “wild” occurs in both high sentimental and low sentimental
chapters with similar frequencies. The contexts of “wildness” are extracted as below:
1. Uncle Tom’s Cabin - Chapter 9
The woman was now sitting up on the settle, by the fire.
She was looking steadily into the blaze, with a calm, heart-broken
expression, very different from her former agitated
wildness.
2. Uncle Tom’s Cabin - Chapter 27
There was a piercing wildness in the cry; the blood
flushed into St. Clare’s white, marble-like face, and the first
tears he had shed since Eva died stood in his eyes.
3. Uncle Tom’s Cabin - Chapter 34
1The frequencies in this table are document frequencies because the boolean feature representations are
used in these experiments. Sometimes the sum of the frequencies of the unstemmed word forms does not
equal to that of the final stem because some word forms occur in the same document and their merged
document frequency remain the same as 1
76
wt ws P (C|wt) P (C|ws) avgKL
wildness wild (6, 0) (21, 21) 0.583
wild wild (17, 21) (21, 21) 0.003
· · ·
neatness neat (5, 0) (11, 11) 0.541
neat neat (7, 11) (11, 11) 0.013
· · ·
detail detail (0, 6) (11, 9) 0.583
details detail (11, 4) (11, 9) 0.038
· · ·
pitying piti (6, 0) (30, 24) 0.515
pitiful piti (2, 3) (30, 24) 0.041
pitied piti (4, 4) (30, 24) 0.005
pity piti (26, 18) (30, 24) 0.001
· · ·
difficulties difficulti (6, 6) (18, 18) 0
difficulty difficulti (12, 12) (18, 18) 0
wheels wheel (4, 4) (11, 11) 0
wheel wheel (7, 7) (11, 11) 0
· · ·
Table 8.5: Sentimentalism full stemming - micro analysis
There was a graceful and compassionate sweetness in her
voice and manner, as she said this, that formed a strange
contrast with the former wildness.
4. The Minister’s Wooing - Chapter 23
When Mrs. Marvyn had drawn Mary with her
into her room, she seemed like a person almost in
frenzy. She shut and bolted the door, drew her
to the foot of the bed, and, throwing her arms
round her, rested her hot and throbbing forehead
on her shoulder. She pressed her thin hand over
her eyes, and then, suddenly drawing back, looked
her in the face as one resolved to speak something
long suppressed. Her soft brown eyes had
a flash of despairing wildness in them, like that
of a hunted animal turning in its death-struggle
on its pursuer.
5. Charlotte’s Daughter - Chapter 18
Her attire consisted of a soiled travelling dress,
77
which had once been rich and showy-her countenance
, though thin and wasted, was flushed and feverish
, and there was a wildness in her eyes which
told the saddest tale of all, that not only was the
wretched lady forsaken by friends and fortune, but
at least partially deprived of the blessed light of
reason.
6. Charlotte: A Tale of Truth - Chapter 33
She then made an effort to get out of bed; but
being prevented, her frenzy again returned, and
she raved with the greatest wildness and incoherence
. Mrs. Beauchamp, finding it was impossible for
her to be removed, contented herself with ordering
the apartment to be made more comfortable, and procuring
a proper nurse for both mother and child?
and having learnt the particulars of Charlotte’s
fruitless application to Mrs. Crayton from honest
John, she amply rewarded him for his benevolence,
and returned home with a heart oppressed with many
painful sensations, but yet rendered easy by the
reflexion that she had performed her duty towards
a distressed fellow-creature.
In summary good mergings and bad mergings occur simultaneously, but stemming reaches
36% feature reduction rate without significantly decreasing the classification accuracy.
8.3 Removing stop words
This set of experiments examines the effect of stop word removal on sentimentalism clas-
sification. The first experiment adopts the concept of stop words as common words, and
examines if the Brown stop word list is still a valid stop word list for the sentimentalism
collection. The second experiment treats stop words as function words, and examines the
effects of individual function word groups on sentimentalism classification.
78
8.3.1 Stop words as common words
404 out of the 425 Brown stop words occur in the sentimentalism collection, 285 of which
are tagged as content words by the Brill tagger. Table 8.6 shows that the accuracies of SVM
and naive Bayes with the Brown stop word features are close to the trivial classifier, and
significantly lower than what are obtained with content word features. In other words, the
Brown common words are not a discriminative feature set overall for the sentimentalism
classification.
feature set size doc coverage SVM naive Bayes
Content word 3669 100% 69.5 70.2
Brown stoplist 404 100% 53.9 53.8
Table 8.6: Sentimentalism common words
8.3.2 Stop words as function words
The following experiment examines the effects of four function word groups - the prepositions
and conjunctions, pronouns, modals, and determiners on the sentimentalism classification.
The results (see table 8.7) show that none of the function word groups achieved accuracies
significantly higher than the trivial classifier.
feature set size doc coverage SVM naive Bayes
Pronoun 27 100% 59.2 56.3
Modal 15 100% 54.9 55.9
Prep-Conj 88 100% 52.4 52.6
Determiner 22 100% 49.2 54.7
Table 8.7: Sentimentalism function words
In summary most words in the Brown stoplist are still stop words in the sentimentalism
collection. These words are not discriminative features in the sentimentalism classification.
None of the four function word groups are discriminative fature sets either, which confirmed
the scholars’ heuristic that content words are more relevant in this case.
79
8.4 Feature selection
This set of experiment examines the effects of the self feature selection measures of naive
Bayes and SVM on the sentimentalism classification performance from two objective aspects
- their contributions to classification accuracy improvement and reduction rates, and one
subjective aspect - the informativeness of the top discriminative features to the literary
scholars. To examine the interaction between stemming and feature selection, the objective
evaluation of feature reduction is repeated under the condition of stemming and no stemming.
8.4.1 Naive Bayes self feature selection - objective evaluation
This experiment examines the naive Bayes self feature selection (Odds Ratio) measure. The
paired t-test result (table 8.8) shows that the classification accuracy improves significantly
after feature selection. With the increasing feature reduction rate, the 10-fold cross validation
average accuracy steadily increases from around 70% to around 90%. The reduction rate is
as high as 90%.
Table 8.8 also shows that feature reduction without stemming produces more accuracy
improvement, although the difference is not significant at p < 0.05. The feature reduc-
tion results in 80% reduction rate with stemming and 90% without stemming. Hence the
stemming is not a necessary, if not harmful, for sentimentalism classification.
8.4.2 Naive Bayes self feature selection - subjective evaluation
Some heuristic sentimental words are included in the best 10% naive Bayes features, such as
“die”, “sorrow”, and “agony”. However, many top features (see table 8.9) do not make sense
to the scholars although they happened to occur in the sentimental chapters. For example,
the words “paternal” and “payment” both occur in eight high sentimental chapters, but
never in low sentimental chapters. But the contexts surrounding them are not sentimental.
One reason could be that the chapters are too long and thus a number of non-sentimental
80
with stemming no stemming
features accuracy features accuracy
100% 3669 70.2 5704 65.4
90% 3302 68.8 5134 67.4
80% 2935 71.5 4563 73.6
70% 2568 76.5 3993 77.7
60% 2201 80.6 3422 80.4
50% 1835 81.7 2852 83.8
40% 1468 84.1 2282 86.3
30% 1101 88.7 1711 89.2
20% 734 88.0 1141 91.8
10% 367 - 570 92.0
Table 8.8: The effect of naive Bayes self feature selection on sentimentalism classification
words are mistakenly learned as sentimental because of their co-occurrence with sentimental
words.
feature rank weight
shrink 1 2.326275
paternal 2 2.220914
measured 3 2.220914
spared 4 2.220914
storms 5 2.103131
payment 6 2.103131
wildness 7 1.969600
vow 8 1.969600
infamy 9 1.969600
pitying 10 1.969600
Table 8.9: Sentimentalism naive Bayes top boolean features
8.4.3 SVM self feature selection - objective evaluation
This experiment examines SVM self feature selection measure. Table 8.10 shows that after
stemming the SVM classification accuracies fluctuate with the increase of reduction rate. No
accuracy differences are significant before and after feature selection at p < 0.05. The SVM
model generated with stemmed features contains 183 support vectors. There are only 184
training examples, so the SVM model is hardly generalizable to future sentimental document
81
prediction.
On the contrary, without stemming SVM classification accuracy steadily improves with
the decreasing feature set size. With top 10% not-stemmed features the SVM classifier
achieves 91.9% accuracy. This time the generated SVM model contains 152 support vectors.
The smaller model provides more generalizability for the example-based retrieval application
in the future.
In the eroticism classification, stemming reduces 13% features and does not harm the
feature selection. But in the sentimentalism classification the reduction rate is as high as
36%. Such high reduction rate of stemming might be the reason for its negative impact on the
consequent feature selection if stemming blindly neutralized a large number of discriminative
features. Figure 8.1 compares the KLD values of stemming mergings in the eroticism and
sentimentalism collections. We can see that the KLD values for the top 500 mergings are
still high in the sentimentalism collection. However, neither macro or micro level evaluation
catches the harm of stemming to feature selection.
In summary SVM feature reduction without stemming improves the classification accu-
racy significantly. Stemming has strong negative effect on feature reduction for SVM in the
sentimentalism classification. Stemming is not a good choice if statistical feature selection
is to be conducted after all.
with stemming no stemming
features accuracy features accuracy
100% 3669 69.5 5704 67.0
90% 3302 66.3 5134 67.1
80% 2935 65.7 4563 72.8
70% 2568 66.3 3993 76.7
60% 2201 65.2 3422 82.4
50% 1835 62.0 2852 85.9
40% 1468 62.0 2282 88.8
30% 1101 61.4 1711 89.7
20% 734 63.6 1141 91.2
10% 367 66.3 570 91.9
Table 8.10: Incremental SVM self feature selection
82
 0
 0.2
 0.4
 0.6
 0.8
 1
 1.2
 0  500  1000  1500  2000  2500  3000  3500
K
LD
 v
al
ue
s
KLD rank
KLD rank and values
eroticism
sentimentalism
Figure 8.1: Figure: KLD rank and values
8.4.4 SVM self feature selection - subjective evaluation
Similar to the naive Bayes top features, the SVM top features (see table 8.11) mixed straight-
forward sentimental indicators and bewildering words. Regretfully the scholars have not
found “new” knowledge about sentimentalism from the top SVM feature list either.
feature rank weight
throw 1 0.067381
others 2 0.067184
pure 3 0.065252
beloved 4 0.064711
to-morrow 5 0.064680
die 6 0.062710
sorrow 7 0.062019
still 8 0.061522
sake 9 0.060919
reason 10 0.060077
Table 8.11: Sentimentalism SVM top boolean features
83
-1
-0.5
 0
 0.5
 1
-1 -0.5  0  0.5  1
N
B
 w
ei
gh
t
SVM weight
NB SVM feature weight comparison
NB SVM feature weights
NB weights
SVM weights
agreement
Figure 8.2: Figure: naive Bayes and SVM feature ranking agreement
8.4.5 Comparing naive Bayes and SVM as feature selection
methods
Both naive Bayes and SVM algorithms reach comparable classification accuracies with their
own top 10% (570) features. However, there are only 124 shared features in the two top 10%
feature lists.
Figure 8.2 shows the scatter plot of the weights obtained from the two feature selection
measures. One can see clearly the points disperse away from the x=y line toward both
ends of the axes. In other words, the two weighting measures agree basically upon the low
weighted features, but they disagree upon the weights of the heaviest features.
Figure 8.3 and 8.4 plot the relation between the feature ranks and their weights. Similar
to the SVM feature rank and weight relation in the Dickinson eroticism classification, figure
8.3 shows that the SVM feature weights decrease quickly and smoothly from top rank to
bottom rank. The small number of top features mainly affect the classification decision. The
rest features are not important due to the small feature weights. This explains the SVM high
84
-1
-0.8
-0.6
-0.4
-0.2
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  1000  2000  3000  4000  5000  6000
w
ei
gh
t
feature rank
feature ranks and weights
SVM
Figure 8.3: Figure: Sentimentalism SVM feature ranks and weights
reduction rate from one aspect. Different from the corresponding results in the Dickinson
experiments, the feature ranks and weights contours for naive Bayes (figure 8.4) and SVM
are similar, except that SVM feature weights decrease faster. This is consistent with the
results that the two methods have similar feature reduction rate for the sent collection.
Figure 8.5 and 8.6 show the relations between feature ranks and their frequencies. Similar
to the results in the Dickinson experiment, the top naive Bayes features are all low frequent
words, while the frequencies of the top SVM features are more distributed across the range.
In the sentimentalism case, the literary scholars did not find new knowledge from either
feature lists. But again the SVM features list agrees with the scholars upon the heuristic
features, which leads to the possibility of using the learned SVM sentimentalism model for
future sentimental novel prediction.
85
-1
-0.8
-0.6
-0.4
-0.2
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  1000  2000  3000  4000  5000  6000
w
ei
gh
t
feature rank
Naive Bayes feature ranks and weights
Naive Bayes
Figure 8.4: Figure: Sentimentalism naive Bayes feature ranks and weights
 0
 20
 40
 60
 80
 100
 120
 140
 160
 180
 200
 0  1000  2000  3000  4000  5000  6000
fr
eq
ue
nc
ie
s
feature rank
feature ranks and frequencies
SVM
Figure 8.5: Figure: Sentimentalsim SVM feature ranks and frequencies
86
 0
 20
 40
 60
 80
 100
 120
 140
 160
 180
 200
 0  1000  2000  3000  4000  5000  6000
fr
eq
ue
nc
ie
s
feature rank
feature ranks and frequencies
naive Bayes
Figure 8.6: Figure: Sentimentalism naive Bayes feature ranks and frequencies
8.5 Cross feature selection
For the Dickinson eroticism classification naive Bayes self feature selection (Odds Ratio) is
better than SVM for naive Bayes. For the sentimentalism classification table 8.12 shows
that the top 10% features of both naive Bayes and SVM achieve comparable classification
accuracies on both algorithms. We already know that the two feature sets have only 22%
overlap, but they are both strong discriminative features for both algorithms.
classifier feature set 10CV accuracy
naive Bayes naive Bayes 10% 92.0
naive Bayes SVM 10% 92.6
SVM naive Bayes 10% 92.3
SVM SVM 10% 91.9
Table 8.12: Feature selection method cross comparison
87
 0.55
 0.6
 0.65
 0.7
 0.75
 0.8
 0.85
 0.9
 0.95
 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9
A
cc
ur
ac
y
Percent of training examples
sentimentality classification - learning curve
SVM
NB
Figure 8.7: Figure: sentimentalism learning curves
8.6 Learning curve
Figure 8.7 shows that SVM performance improves fast with the increase of training example
numbers. The learning curve hits the turning point when the number of training examples
reaches 40%, and levels off since then. The naive Bayes has a high initial point, which is
88% accuracy with 10% training examples. The performance does not improve greatly with
the increase of training examples. Overall naive Bayes needs fewer training example to reach
stable performance.
8.7 Confidence curve
Figure 8.8 shows the confidence curves of naive Bayes and SVM. We can see that the naive
Bayes prediction accuracy decreases more slowly with the decreasing confidence. Overall
naive Bayes has better learning curve and confidence curve, and therefore a better candidate
method for example-based retrieval application.
88
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1
A
cc
ur
ac
y
Top x percent of prediction
prediction confidence curve
SVM
Naive Bayes
Figure 8.8: Figure: sentimentalism confidence curves
8.8 Summary
This chapter presented the classification and feature engineering experiment results on the
sentimentalism classifiction task. Most Brown stop words are still stop words for the senti-
mentalism collection because they can not discriminate sentimental chapters from the others.
Function words are also useless. No classifiers learned from the four function word groups
achieve significantly better accuracies than the trivial majority vote baseline. The features
with or without stemming do not make significant difference in the initial classification
experiment. But stemming has negative impact on feature selection, especially SVM.
Both naive Bayes and SVM algorithms achieve over 90% accuracies after feature reduc-
tion. But similar to what happened in the Dickinson case, the best feature subsets they
select have only 22% overlapped. Although no new knowledge has been found from the
feature, the top SVM features are consistent with the scholars’ prior knowledge. The top
naive Bayes features are hard to understand intuitively for the scholars.
Naive Bayes has better learning curve and confidence curve than SVM. A naive Bayes
89
classifier with stable accuracy can be learned with fewer training examples. The prediction
accuracy drops more slowly with the decrease of confidence. Overall naive Bayes is a better
chandiate method for example-based sentimentalism retrieval.
90
Chapter 9
Conclusions and future work
This chapter summarizes the evaluation results obtained in this study, and discuss future
research toward answering the new questions raised from the results.
9.1 Summary
This study finds that SVM does not beat naive Bayes in the three use scenarios of literary
text classification. After feature selection naive Bayes achieves high accuracies in both cases
while SVM succeeds in the sentimentality classification only. Naive Bayes helps the Dickinson
scholars find new knowledge while SVM only confirms their prior knowledge. However the
eroticism in Dickinson’s poem is such a complicated concept that neither methods with
bag-of-words features obtains a satisfactory learning curve to be qualified for example-based
retrieval tools. Neither methods help the Sentimentalism scholars find new knowledge, but
SVM top features match the scholars’ prior knowledge better than naive Bayes top features.
Naive Bayes has stronger learning curve and confidence curve, but both methods demonstrate
to be good candidates for example-based sentimentalism retrieval tools.
Odds Ratio is a better feature selection method than SVM for naive Bayes. In both
evaluation cases the naive Bayes self feature selection (Odds Ratio) significantly improves
its classification accuracy. A drawback of Odds Ratio is its low reduction rate in some
cases. Odds Ratio does not improve SVM performance; SVM self feature selection is the
best for SVM. SVM self feature selection achieves high reduction rate. SVM maintains the
classification accuracy with top 10% features in the eroticism classification and significantly
91
improves its accuracy in the sentimentalism classification.
A further comparison of the selected features by naive Bayes and SVM in both cases
shows that the two algorithms pick top features at different frequency range. Their selection
results share only a small number of features in common. SVM prefers high frequency and
discriminant words while naive Bayes prefers unique words which are often less frequent. For
the Dickinson case the large number of low frequency words results in the success of naive
Bayes in both classification and feature-category correlation anlaysis. For a possible new
erotic indicator, a low frequent one is easier for the scholars to figure out if it is really erotic
or not. The usage difference of a high frequent word among categories is harder to detect for
humans. For the sentimentalism case two different classififiers with different features achieve
same high accuracy on the same data set. For the feature-category correlation analysis
purpose, the two algorithms should be used as complemental to each other rather than one
over the other.
Although SVM does not prove to be superior to naive Bayes in literary text classification,
SVM holds an advantage as a parametric method that its number of generated support
vectors indicate the model’s complexity and thus its generalizability for future prediction.
In both cases stemming does not change the classification performance significantly before
self feature selection, although micro level stemming analysis finds that both good mergings
and bad mergings occur at the same time. However, current stemming analysis, macro or
micro, fails to catch the negative impact of stemming on statistical feature selection for the
sentimentalism classificatio. There is also no evidence that stemming improves classification
accuracies. Hence blind arbitrary feature reduction methods like stemming should be avoided
when statistical feature selection methods are available.
The stop words in the Brown list are still useless in the sentimentalism classification.
Content words instead of the function words prove to be the most relevant features for
sentimentalism classification. However, the Brown stop words are mostly uncommon in the
Dickinson collection. Actually few words are common in the Dickinson poems, and probably
92
all the poems. Personal pronoun - the group of function words usually treated as stop words
- turns out to be highly relevant features for erotic poem classification. So stop word removal
is risky for data sets as unique as poems.
In summary this study extends the empirical evaluation of classification methods to
emotion classification tasks in the literary domain. The experiment results provide new
insights to the relation between classification methods, feature engineering options and non-
topic document properties. They also provide guidance for classification method selection
in literary text classification applications.
9.2 Future research directions
The experiment result in this study might have raised more questions than it answered.
This section introduces new research problems inspired from this evaluation work and future
research toward solving these problems.
1. Thorough empirical and analytical evaluation across document properties
Previous classification method evaluations were conducted in the topic classification
setting. This thesis evalutes two classification methods in the emotion classification
setting. We can see that the topic classification evaluation results are not all trans-
ferable to emotion classification. Evaluation across document properties is needed
to fully understand the interaction between the classification methods and document
properties. Besides the empirical evaluation, analytical evaluation with simulation on
artificial data across document properties might provide explanations to the empirical
evaluation results, and also guidance to evaluate the linguistic characteristics of a text
collection before the trial of classification methods.
2. Thorough evaluation of the interaction between classification models and feature se-
lection methods
93
The traditional classifier evaluations treated feature selection as a feature preprocessing
approach, which are supposed to be fair for all classifiers. This thesis, joining previ-
ous research results, has provided evidence of the interaction between classifiers and
feature selection methods across document properties. However, this thesis foucsed
on two classifiers and their self feature selection methods only. Evaluation of other
combinations of classifiers and feature selection methods should be conducted to get
the thorough empirical experience on their interactions. To obtain a fair comparison of
all the text classifiers, they should be re-evaluated without feature selection and with
the best feature selection method for each classifier. feature redundancy
3. Ensemble learning for text classification
In this thesis we find that SVM and naive Bayes choose different discriminative features,
which represent different aspects of a concept. Can we build more robust classifiers by
combining the most discriminative features contributed by multiple classifiers? Will
the document properties have interaction with the ensemble learning approaches? We
need more evaluations.
94
Appendix A
Toward discovering potential data
mining applications for literary
criticism
Current literary text mining applications are limited in a few areas - authorship attribution,
computational stylistics, genre analysis and versioning machines. These applications have
not been accepted by the mainstream literary critic communities. Mcgann [63] described
this dilemma in his book “Radiant Textuality: Literature After the World Wide Web”:
Digital technology used by humanities scholars has focused almost exclusively on
methods of sorting, accessing, and disseminating large bodies of materials, and on
certain specialized problems in computational stylistics and linguistics. In this
respect the work rarely engages those questions about interpretation and self-
aware reflection that are the central concerns for most humanities scholars and
educators. Digital technology has remained instrumental in serving the technical
and precritical occupations of librarians and archivists and editors. But the
general field of humanities education and scholarship will not take the use of
digital technology seriously until one demonstrates how its tools improve the
ways we explore and explain aesthetic works - until, that is, they expand our
interpretational procedures.
Recently humanities computing researchers are trying to build generic text mining soft-
wares for literary study. For example, the Nora project [?] aims at developing a web-based
text mining and visualization tool to explore 18-19th British and American literary text
collections. As Mcgann has pointed out, it has been a challenge for the Nora project to find
representative literary-criticism problems that can be addressed with data mining tools and
95
techniques.
How can we find the opportunities to get text mining involved in literary critical scholar-
ship? An inductive approach is to keep track of the specialized literary text mining applica-
tions and summarize their commonalities. A deductive approach is to find potential literary
data mining applications from second literature analysis and user studies. This chapter de-
scribes such an exploratory study toward discovering potential data mining applications in
literary criticism.
The basic belief underneath this approach is that in order to better adapt data mining
techniques to literary criticism, one has to grasp the unique characteristics of literary re-
search and to leverage its uniqueness and its similarity with data mining tasks. Buckland
(Buckland, 1999) claimed that vocabulary is a central concept in information transition be-
tween domains. Comparing the vocabularies between the corpora in different domains may
shed light on discovering the similarity and difference in the research activities between these
domains. Unique vocabulary might reflect unique research activities in a domain. Vocab-
ulary overlap might reflect the potential connection between the research activities in two
different domains.
This study try to answer the following questions:
1. Do literary scholars have unique vocabulary which indicate their research activities?
2. Does this unique vocabulary help finding new data mining applications?
3. Do literary scholars and data mining researchers have overlapping vocabulary?
4. Does this overlapping vocabulary help find new data mining applications?
To answer these questions a three-step research plan was designed to map research ac-
tivities between data miners and literary scholars as reflected in the vocabulary use in their
research publications. Step 1 is to find the unique scholar vocabulary; step 2 is to find
the vocabulary overlapping between data mining and literary criticism; step 3 is a literary
96
scholar survey to investigate whether the overlapped vocabulary is the common research
activities in the two domains.
A.1 Corpora construction
Three corpora have been constructed for the vocabulary analysis in stage 1 and 2. The first
is the data mining corpus (named “KDD”) which consists of 442 ACM SIGKDD conference
paper abstracts from 2001 to 2005. The ACM SIGKDD conference has been the premier
international conference on data mining. The paper titles and abstracts are extracted from
the ACM Digital Portal. We do not use full text because it contains too many technical
details that are not relevant to literary research.
The second is the literary criticism corpus (named “MUSE”) which consists of 84 ELH
Journal articles and 40 ALH articles downloaded from Project Muse, all on the subject of
the 18th and 19th century British and American literature. The selection is based on the
subject indexes assigned by the publisher. The plain text versions are generated by removing
all the tags and quotations from the corresponding HTML versions.
The third is the New York Times subset of American National Corpus (named “ANC-
NYTIMES”) which consists of thousands of news articles with more than 2 million words.
This “everyday English” corpus serves as a contrast group to test if the discovered similarities
between the research behaviors in data mining and literary study are significant.
A.2 Step 1 finding unique scholar vocabulary
In this step the scholar vocabulary in the MUSE corpus is compared against the vocabulary
in ANC - a benchmark English corpus. Firstly, the plain text MUSE documents are part-
of-speech tagged using GATE. Document frequency (DF) and term frequency (TF) serve as
the basic indicators for a term’s popularity in a collection. Arbitrary DF is defined as the
97
number of documents that contain the term. Normalized DF is defined as the percentage
of the arbitrary DF in the collection (denote as ”DF-pcnt”). Arbitrary TF is defined as the
term’s total number of occurrences in the whole collection. Normalized TF is defined as the
proportion per million words (denote as ”TF-ppm”).
We focus on the verbs specifically because of the heuristic that verbs are indicators of
research activities and methodology. The verbs in both corpora are cascade sorted by their
DF and TF. A verb is considered unique or unusual if its usage is different from that in
ANC-NYTIMES corpus. A unique MUSE verb list is generated by comparing the verbs in
MUSE and ANC-NYTIMES, also cascade sorted by DF and TF. The top 10 unique verbs
are “naturalizing”, “narrating”, “obviate”, “repudiate”, “Underlying”, “misreading”, “desir-
ing”, “privileging”, “mediating”, and “totalizing”. A few literary scholars and data mining
researchers reviewed the unique MUSE verb list. They think the results are interesting,
partly because they ring true to the discipline, but data mining researchers arent sure they
help with finding literary data mining applications.
Prior to review the unique MUSE verb list, a literary scholar once independently picked
out some representative verbs (with both DF and TF between 5 and 10) in critical literature
setting: “clarifies”, “cleared”, “Knowing”, “destabilizes”, “analyzing”, “annotated”, “juxta-
posed”, “evaluated”, “recapitulates”, “merit”, “detail”, “portraying”, and “stemming”.
Obviously the two verb lists do not overlap at all. Actually, the representative verbs
(except ”recapitulates”) picked out by the literary scholar turn out to be common in ANC-
NYTIMES corpus too. After examining the unique MUSE verb list, two literary scholars
were surprised to find many unexpected unique verbs, which means their uniqueness is
beyond the scholars’ awareness.
Aparently literary scholars are not explicitly aware of what are the unique research activ-
ities at the vocabulary-use level. They might be able to summarize their scholarly primitives
as Unsworth did in (Unsworth, 2000), but does not help computer scientist to understand
the data mining needs in literary criticism.
98
A.3 Step 2 finding overlapping vocabulary
In this step the scholar vocabulary in the MUSE corpus is compared against the data mining
vocabulary in the KDD corpus. Firstly the keywords are extracted from KDD paper titles.
These keywords identify mainstream data mining activities and methods in data mining.
Then the occurrences of these KDD keywords are compared between the MUSE and the
ANC-NYTIMES corpora to identify the keywords common in both KDD and MUSE but
not in ANC-NYTIMES.
In the data mining keywords extraction process, non-stop words are extracted and
stemmed (using Porter Stemmer) from paper titles and sorted only by their TF. 18 out
of 102 non-stop stemmed title words with TF¿5 are identified as the representative data
mining keywords. The left out terms include general terms (e.g. “approach”), technical
terms (e.g. “bayesian”), terms about specific data (e.g. “gene”), and terms with different
meaning in MUSE (e.g. “tree”). KDnuggets poll results 1 show that these keywords reflect
the major data-mining tasks and methods.
Table 1 compares the frequencies of the 18 words between MUSE and ANC-NYTIMES. It
shows that 11 data mining keywords are common in literary essays but not in news articles.
Figure 1 visualizes their significant differences in TF-ppm. The 11 keywords stand for
models, frameworks, patterns, sequences, associations, hierarchies, classifications, relations,
correlations, similarities, and spatial relations. It’s not surprising that none of these keywords
can be found in MUSE essay titles. The context of the keywords extracted from KDD
abstracts and MUSE full text also has little in common.
In the left 7 KDD keywords, “rule”, “serial/seri” and “decis” are common in both cor-
pora, “cluster” and “stream” are common in neither of them. Interestingly “network” and
“graph(ic)” are much more common in ANC-NYTIMES. It seems literary scholars do not
1KDnuggets April 2006 poll on “Data mining/analytic methods you used frequently in the last year?”
http://www.kdnuggets.com/polls/2006/data mining methods.htm; KDnuggets January 2006 poll on “What
text mining tasks have you done in 2004-5?” http://www.kdnuggets.com/polls/2006/text mining tasks.htm
99
think much in graphic models.
This vocabulary comparison shows us that literary scholars are actually “data miners”,
except that they look for different kinds of knowledge. For example, in terms of pattern
discovery, literary scholars look for “narrative patterns”, “marriage patterns”, “patterns
of plot”, etc. But data miners concern pattern in a more abstract manner - “sequential
patterns”, “association patterns”, “topological patterns”, etc.
keyword kdd-tf muse-df:tf muse-df-pcnt:tf-ppm nytimes-df:tf nytimes-df-pcnt:tf-ppm
cluster 52 13:17 10:22 50:56 1:24
model 40 99:438 80:562 335:597 8:254
pattern 29 49:121 40:155 167:207 4:88
network 23 30:76 24:97 325:724 8:307
classif
classifi
35 14
19
:64 11
15
:82
16
50
:74 0
1
:32
rule 19 81:210 65:269 708:1409 17:598
associ 15 103:479 83:614 762:1161 18:493
graph
graphic 15
2
15 :25
2
12 :32
7
244 :504
0
6 :214
stream 15 19:26 15:33 103:117 2:50
serial
seri
10 20
80
:213 16
65
:273 32
537
:946 1
13
:403
relat
relationship
10 11798 :1367
94
79
:1753
386
323 :911
9
8
:487
framework 10 38:69 31:88 25:28 1:12
correl 9 20:30 16:38 15:21 0:9
similar
similarli
9 99
66
:475 53
80
:609
484
77
:649 12
2
:276
spatial 7 19:55 15:71 8:8 0:3
decis 7 57:161 46:206 683:1110 16:471
hierarch
hierarchi
6 2041 :178
16
33
:229
4
4 :45
0
0
:13
sequenc
sequenti
6 348 :80
6
27
:103
4
51 :71
0
1
:30
Table A.1: KDD keyword frequency comparison between MUSE and ANC-NYTIMES
A.4 Step 3 literary scholar survey
The comparative vocabulary analysis has identified some keywords shared by data mining
and literary study. The goal of the literary scholar survey is to evaluate if the overlapping
100
0 200 400 600 800 1000 1200 1400 1600 1800
model
pattern
classif(i)
associ
relat(ionship)
framework
correl
similar(li)
spatial
hierarch(i)
sequen(c/ti)
frequency (ppm)
ke
yw
or
d
muse
nytimes
Figure A.1: TF of Selected KDD keywords common in MUSE but not common in ANC-
NYTIMES
vocabulary really reflects shared research methods and tasks. To explore the relationship
between data mining research methods and tasks and literary study methods and tasks, the
survey is consequently designed to include two parts - the first half of questions address the
mapping between the scholars argument constructing logic and the data mining algorithmic
reasoning logic (question 1-3); the second half address the mapping between the scholars-
interested patterns and the patterns that can be formally defined and found with data mining
algorithms (question 4-6).
A.4.1 Survey questions
1. Can you briefly summarize any literary study question or problem you are addressing
in an essay you are currently working on or recently published? For that essay, what
was the basic argument, and what kinds of things constitute the evidence?
2. How do you form the argument from the evidence?
101
3. In the critical essays you have read, do you find anyone (including yourself) simplify
problems or structure their thinkings according to some idealized theoretical frame-
work?
4. Do you look for any kind of patterns during your study? What are they? How do you
explore them?
5. In the course of your research do you often group/categorize/classify literary docu-
ments? If so, what are some examples of classes or categories? If not, are there other
structures that you seek in the documents you work with?
6. For those classes or categories, do you use any explicit textual patterns (e.g. keywords)
to categorize the documents? Are there any categories for which you can’t categorize
the documents with explicit textual patterns?
A.4.2 Survey answers analysis
The survey questions were sent by email to the scholars in the Nora project team and other
scholars who had expressed interest in the Nora project. This is a purposeful sample because
these scholars represent the group of scholars who know about data mining better than the
others. 5 scholars replied by email. The time to complete the survey varies between 15
minutes and 1 hour.
Question 1 collects the ad hoc literary study problems the scholars were working on. The
answers include the following literary criticism tasks
• determining historical context of poems
• comparing versions and editions of certain electronic works
• correlating the rise of a particular concept and the fall of another one
• Describing a pattern of thought
102
• Explaining the player engagement characteristics of a certain kind of videogames
Question 2 explores the reasoning logic the scholars use to construct their arguments from
evidence. We got three kinds of answers. 3 scholars said they documented and assembled
some examples; 1 scholar thought it depends on the research tasks; 1 scholar thought the
question was hard to answer.
Question 3 asks if scholars really use simplified models to structure their thinkings. 3
scholars said “yes” because reductionism is a useful heuristic; 1 scholar disagreed and argued
that scholars complicate issues rather than simplify them. 1 scholar thought the question
was too vague to answer.
Question 4 investigates the patterns that literary scholars often look for. The following
patterns and their occurrences in the answers are listed below:
• Generic repetition (2)
• Keywords and phrases (2)
• Term frequency changes (2)
• Structural unit variations (1)
• Assumptions or motivations (1)
• Conceptual metaphors (1)
• Patterns across genre (1)
Question 5 explores the scholarly practice of a particular pattern recognition - document
classification/categorization. The unanimous answers show that classification is a scholarly
primitives. The scholars describe their purpose of classification as 1) initiate a discovery
process and 2) collect examples for genre analysis. They classified texts by versions, editions,
103
genres or sub-genres, chronology, and many other criteria. One scholar doubted if data
mining can help with the sub-genre classification.
The answers to question 5 reveals the multiple purposes of literary text classification.
One scholar mentioned a sub-genre classification task - collecting “historicist catalog poem”.
This is another example of “example-based search” scenario, similar to the “ekphrastics”
example. Another scholar mentioned that classification by itself is not the ultimate purpose,
the subsequent (feature-category correlation) analysis is the real purpose instead.
Question 6 explores the usefulness of explicit textual patterns as feature set for literary
text categorization. All the scholars agreed that explicit textual patterns are useful hints,
and that keyword search is a useful tool. But they spent more words on emphasizing the
limitations of this feature set. Scholars also considered other characteristics of the literary
texts, such as particular forms, rhythms, structures, etc. Some of them cannot be separated
from the texts, or cannot even be represented as keywords.
Summarizing the scholars’ answers to question 5 and 6 we can see that the scholars trust
keyword search and are dubious of NLP and data mining algorithms for literary document
classification.
A.5 Conclusion
This study did not find opportunities of data mining involvement at the argument construc-
tion level. Actually simple topic analysis shows that many MUSE essays are trying to build
connections between writers, characters, concepts, and social and historic backgrounds. As
an evidence, 56 out of 84 ELH essays and 24 out of 40 ALH essays titles contain ”and” - one
of the parallel structure indicator. But genre is the only topic that can be mapped directly
to text mining application - document categorization.
But the survey results show that data mining can assist collecting evidence and exam-
ples by identifying keywords and phrases, collocated keywords and structural patterns, or
104
contributing to document organization, or collecting examples in genres and sub-genres.
According to the scholars’ feedback on the question quality, question 2 and 3 are too
broad and therefore not much informative in decoding the scholars’ reasoning logic. Another
question is raised - is it hard for scholars to explain their own reasoning logic? Or are there
simply no regularities?
The scholars answers to questions 4,5,6 provide rich information on how to design literary
text classification and search tools. It seems that the conversation with scholars became
easier when it came to particular kinds of patterns. This demonstrates that surveying
scholars on particular data mining technique related tasks is an effective approach for data
mining researchers to find new applications in literary domain.
The scholar survey confirmed the comparative vocabulary analysis results that classifi-
cation and correlation analysis are typical research problems in the literary domain. This
exploratory helps identified the four typical literary text classification scenarios introduced
in the introduction, which is also the focus of the whole thesis.
105
References
[1] R. Agrawal, S. Rajagopalan, R. Srikant, and Y. Xu. Mining newsgroups using networks
arising from social behavior. In Proceedings of the 12 WWW Conference, pages 529–
535, 2003.
[2] D. Archer, J. Culpeper, and P. Rayson. Love - ’a familiar or a devil’? an exploration
of key domains in Shakespeare’s comedies and tragedies. In Keyword Extraction in
Information Retrieval Workshop, ACH/ALLC 2005, University of Victoria, Canada,
2005.
[3] S. Argamon, M. Koppel, and G. Avneri. Routing documents according to style. In
Proceedings of the First International Workshop on Innovative Information Systems,
1998.
[4] S. Argamon, M. Saric, and S. Stein. Learning algorithms and features for multiple
authorship discrimination. In Proceedings of IJCAI’03 Workshop on Computational
Approaches to Style Anslysis and Synthesis, 2003.
[5] A. Aue and M. Gamon. Customizing sentiment classifiers to new domains: a case
study. In Submitted to RANLP-05, the International Conference on Recent Advances
in Natural Language Processing, Borovets, BG, 2005.
[6] R. Baeza-Yates and B. Ribeiro-Neto. Modern Information Retrieval. Addison Wesley,
1999.
[7] L. D. Baker and A. K. McCallum. Distributional clustering of words for text classifi-
cation. In ACM SIGIR’98, pages 96–103, 1998.
[8] F. Baron and G. Hirst. Collocations as cues to semantic orientation. In Proceedings of
the AAAI Spring Symposium on Exploring Attitude and Affect in Text: Theories and
Applications, Stanford, US, 2004.
[9] R. Bekkerman, R. El-Yaniv, N. Tishby, and Y. Winter. Distributional word clusters
vs. words for text categorization. Journal of Machine Learning Research, 3(7-8), 2003.
[10] D. Biber. Variations Across Speech and Writing. Cambridge University Press, 1988.
[11] D. Biber. Dimensions of Register Variation. Cambridge Univeristy Press, 1995.
106
[12] E. Brill. Transformation-based error-driven learning and natural language processing:
A case study in part-of-speech tagging. Computational Linguistics, 21(4):543–566,
1995.
[13] C. J. Burges. A tutorial on support vector machines for pattern recognition. Data
Mining and Knowledge Discovery, 2:121–167, 1998.
[14] F. Can and J. Patton. Change of writing style with time. Computers and the Human-
ities, 38(1):61–82, 2004.
[15] P. Chaovalit and L. Zhou. Movie review mining: a comparison between supervised and
unsupervised classification approaches. In Proceedings of HICSS-05, the 38th Hawaii
International Conference on System Sciences, page 112c, 2005.
[16] H. Chen. Towards building digital library as an institution of knowledge. In NSF Post
Digital Library Futures Workshop, 2003.
[17] P. Chesley, B. Vincent, L. Xu, and R. Srihari. Using verbs and adjectives to automati-
cally classify blog sentiment. In Proceedings of AAAI-CAAW-06, the Spring Symposia
on Computational Approaches to Analyzing Weblogs, Stanford, US, 2006.
[18] K. Church. One term or two? In Proceedings of SIGIR’95, 1995.
[19] W. Cohen. learning to classify English text with ilp methods. In L. D. Raedt, editor,
Advances in inductive logic programming. IOS Press, Amsterdam, NL, 1995.
[20] W. Cohen and Y. SINGER. Context-sensitive learning methods for text categorization.
ACM Transactions on Information Systems, 17(2):141–173, 1999.
[21] H. Craig. Authorial attribution and computational stylistics: if you can tell authors
apart, have you learned anything about them? Literary and Linguistic Computing,
14(1):103–113, 1999.
[22] K. Dave, S. Lawrence, and D. Pennock. Mining the peanut gallery: Opinion extraction
and semantic classification of product reviews. In Proceedings of WWW03 Conference,
Budapest, Hungary, 2003.
[23] O. de Vel, A. Anderson, M. Corney, and G. Mohay. Mining E-mail content for author
identification forensics. SIGMOD Record, 30(4):55–64, 2001.
[24] S. C. Deerwester, S. T. Dumais, T. K. Landauer, G. W. Furnas, and R. A. Harshman.
Indexing by latent sematnic analysis. Journal for the American Society for Information
Science, 41(6):391–407, 1990.
[25] J. Diederich, J. Kindermann, E. Leopold, and G. Paass. Authorship attribution with
support vector machines. Applied Intelligence, 19(1-2), 109-123.
[26] P. Domingos and M. Pazzani. On the optimality of the simple Bayesian classifier under
zero-one loss. Machine Learning, 29:103–130, 1997.
107
[27] S. Dumais, J. Platt, D. Heckerman, and M. Sahami. Inductive learning algorithms
and representations for text categorization. In CIKM’98, 1998.
[28] S. T. Dumais. Using lsi for information filtering: Trec-3 experiments. In D. Har-
man, editor, The Third Text REtrieval Conference (TREC3), pages 219–230. National
Institute of Standards and Technology Special Publication 500-225, 1995.
[29] S. D. Durbin, J. N. Richter, and D. Warner. A system for affective rating of texts. In
Proceedings of OTC-03, 3rd Workshop on Operational Text Classification, Washington,
US, 2003.
[30] D. Ellis and H. Oldman. The English literature researcher in the age of the Internet.
Journal of Information Science, 31(1):29–36, 2005.
[31] A. Esuli. A bibliography on sentiment classification, last visited: 10/21/2006.
[32] A. Finn and N. Kushmerick. Learning to classify documents according to genre. In
Proceedings of IJCAI’03 Workshop on Computational Approaches to Style Anslysis
and Synthesis, 2003.
[33] G. Forman. An extensive empirical study of feature selection metrics for text catego-
rization. Journal of Machine Learning Research, 3:1289–1305, 2003.
[34] W. Frakes and R. Baeza-Yates. Information Retrieval: Data Structures and Algo-
rithms. Prentice Hall, Englewood Cliffs, NJ, 1992.
[35] M. Gamon. Linguistic correlates of style: authorship classification with deep linguistic
analysis features. In COLING 2004, 2004.
[36] M. Gowan. Public opinions: Inside user reviews. PC World, 21(No.5), May 2003.
[37] I. Guyon, J. Weston, S. Barnhill, and V. Vapnik. Gene selection for cancer classification
using support vector machiness. Machine Learning, 46(1-3):389–422, 2002.
[38] V. Hatzivassiloglou and J. M. Wiebe. Effects of adjective orientation and gradability on
sentence subjectivity. In Proceedings of COLING-00, 18th International Conference on
Computational Linguistics, pages 299–305, Saarbrücken, GE, 2000. Morgan Kaufmann.
[39] D. Holmes. The analysis of literary style - a review. The Journal of the Royal Statistical
Society, 148(4):328–341, 1985.
[40] D. Holmes. Authorship attribution. Computers and the Humanities, 28:87–106, 1994.
[41] D. Hull. Stemming algorithms - a case study for detailed evaluation. Journal of the
American Society for Information Science, 47(1):70–84, 1996.
[42] T. Joachims. Text categorization with Support Vector Machines: Learning with many
relevant features. In European Conference on Machine Learning, 1998.
108
[43] T. Joachims. Learning to Classify Text Using Support Vector Machines, Methods,
Theory and Algorithms. Springer, 2003.
[44] G. H. John, R. Kohavi, and K. Pfleger. Irrelevant features and the subset selection
problem. In Machine Learning: Proceedings of the Eleventh International Conference,
pages 121–129, San Francisco, CA, 1994. Morgan Kaufmann Publishers.
[45] P. Juola. Ad-hoc authorship attribution competition, last visited: 10/21/2006.
[46] P. Juola and H. Baayen. A controlled-corpus experiment in authorship identification
by cross-entropy. Literary and Linguistic Computing, 2005.
[47] D. Jurafsky and J. Martin. Speech and Language Processing: an Introduction to Natural
Language Processing, Computational Linguistics, and Speech Recognition. Prentice
Hall, 2000.
[48] J. Karlgren and D. Cutting. Recognizing text genres with simple metrics using dis-
criminant analysis. In Proceedings of COLING’94, 1994.
[49] B. Kessler, G. Nunberg, and H. Schutze. Automatic detection of text genre. In
Proceedings of ACL/EACL 97, 1997.
[50] M. Koppel, N. Akiva, and I. Dagan. A corpus-independent feature set for style-
based text categorization. In Proceedings of IJCAI’03 Workshop on Computational
Approaches to Style Analysis and Synthesis, 2003.
[51] M. Koppel, S. Argamon, and A. Shimoni. Automatically categorizing written texts by
author gender. Literary and Linguistic Computing, 17(4):401–412, 2002.
[52] R. R. Korfhage. Information Storage and Retrieval. John Wiley and Sons, 1997.
[53] C. Lagoze. NSF DL position paper. In NSF Post Digital Library Futures Workshop,
2003.
[54] K. Lang. 20 usenet newsgroup collection, last visited: 10/15/2006.
[55] E. Leopold and J. Kindermann. Text categorization with support vector machines.
how to represent texts in input space? Machine Learning, 46:423–444, 2002.
[56] M. Lesk. The future of digital libraries. In NSF Post Digital Library Futures Workshop,
2003.
[57] D. D. Lewis. An evaluation of phrasal and clustered representations on a text cat-
egorization task. In Proceedings of SIGIR’92, pages 37–50, Copenhagen, Denmark,
1992.
[58] D. D. Lewis. Naive (bayes) at forty: The independence assumption in information
retrieval. In Proceedings of ECML’98, 1998.
[59] D. D. Lewis. Reuters-21578, distribution 1.0, last visited: 10/15/2006.
109
[60] H. Lodhi, C. Saunders, J. Shawe-Taylor, N. Cristianini, and C. Watkins. Text classi-
fication using string kernels. Journal of Machine Learning Research, 2:419–444, 2002.
[61] C. Manning and H. Schutze. Foundations of Statistical Natural Language Processing.
MIT Press, 1999.
[62] A. McCallum and K. Nigam. A comparison of event models for naive Bayes text
classification. In AAAI 98 Workshop on Learning for Text Categorization, 1998.
[63] J. Mcgann. Radiant Textuality: Literature After the World Wide Web. Palgrave, 2001.
[64] T. Mendenhall. The characteristic curves of composition. Science, 214:237–249, 1887.
[65] T. M. Mitchell. Machine Learning. McGraw-Hill, 1997.
[66] D. Mladenic, J. Brank, M. Grobelnik, and N. Milic-Frayling. Feature selection using
linear classifier weights: Interaction with classification models. In ACM SIGIR ’04,
pages 234–241, 2004.
[67] D. Mladenic and M. Grobelnik. Feature selection for unbalanced class distribution
and Nave Bayes. In Proceedings of the Sixteenth International Conference on Machine
Learning (ICML), pages 258–267, 1999.
[68] J. Morato, J. Llorens, G. Genova, and J. Moreiro. Experiments in discourse analysis
impact on information classification and retrieval algorithms. Information Processing
and Management, 39:825–851, 2003.
[69] A. Moschitti and R. Basili. Complex linguistic features for text classification: a com-
prehensive study. ECIR’04, Lecture Notes in Computer Science, 2997:181–196, 2004.
[70] F. Mosteller and D. Wallace. Inference and Disputed Authorship: The Federalist.
Addison-Wesley, Reading, Massachusetts, 1964.
[71] F. Murtagh. A survey of recent advances in hierarchical clustering algorithms. The
Computer Journal, 26(4):354–359, 1983.
[72] J. Nielsen. The reputation manager. Alertbox, 02/08/1998.
[73] J. Nielsen. Reputation managers are happening. Alertbox, 09/05/1999.
[74] G. Notess. Consumers’ revenge: Online product reviews and ratings. Econtent, 23(2),
2002.
[75] B. Pang, L. Lee, and S. Vaithyanathan. Thumbs up? sentiment classification using
machine learning techniques. In Proceedings of EMNLP-02, the Conference on Em-
pirical Methods in Natural Language Processing, pages 79–86, Philadelphia, US, 2002.
Association for Computational Linguistics.
110
[76] C. Papadimitriou, P. Raghavan, H. Tamaki, and S. Vempala. Latent semantic indexing:
A probabilistic analysis. Technical report, Computer Science Division, U.C. Berkeley,
1997.
[77] J. Pasquale and J. G. Meunier. Categorisation techniques in computer-assisted reading
and analysis of texts (CARAT) in the humanities. Computers and the Humanities,
37(1):111–118, 2003.
[78] L. Pitt, P. Berthon, R. Watson, and G. Zinkhan. The internet and the birth of real
consumer power. Business Horizons, 45(4), 2002.
[79] J. C. Platt. Probabilistic outputs for support vector machines and comparisons to
regularized likelihood methods. In P. J. Bartlett, B. Schlkopf, D. Schuurmans, and
A. J. Smola, editors, in Large-Margin Classifiers. the MIT Press, 2000.
[80] M. Porter. An algorithm for suffix stripping. Program, 14(3):130–137, 1980.
[81] S. Ramsay. Toward an algorithmic criticism. Literary and Linguistic Computing,
18(2):209–219, 2003.
[82] S. Ramsay. In praise of pattern. In ”The Face of Text” - 3rd Conference of the
Canadian Symposium on Text Analysis (CaSTA), 2004.
[83] J. Read. Using emoticons to reduce dependency in machine learning techniques for
sentiment classification. In Proceedings of ACL-05, 43nd Meeting of the Association
for Computational Linguistics, Ann Arbor, US, 2005. Association for Computational
Linguistics.
[84] E. Riloff. Littlewords can make a big difference for text classification. In SIGIR’95,
pages 130–136, 1995.
[85] E. Riloff and J. Wiebe. Learning extraction patterns for subjective expressions. In Pro-
ceedings of the 2003 Conference on Empirical Methods in Natural Language Processing
(EMNLP-03), 2003.
[86] G. Rockwell. What is text analysis, really? Literary and Linguistic Computing,
18(2):209–219, 2003.
[87] M. Sahami, S. Dumais, D. Heckerman, and E. Horvitz. A bayesian approach to filtering
junk e-mail. In AAAI-98 Workshop on Learning for Text Categorization, 1998.
[88] M. Santini. A shallow approach to syntactic feature extraction for genre classification.
Technical report, University of Brighton, UK, 2004.
[89] S. Scott and S. Matwin. Feature engineering for text classification. In ICML’99, 1999.
[90] F. Sebastiani. Machine learning in automated text categorization. ACM Computing
Surveys, 34(1):1–47, 2002.
111
[91] M. Spencer, B. Bordalejo, L. Wang, A. C. Barbrook, L. Mooney, P. Robinson,
T. Warnow, and C. Howe. Analyzing the order of items in manuscripts of the Canter-
bury tales. Computers and the Humanities, 37(1):97–109, 2003.
[92] E. Stamatatos, N. Fakotakis, and G. Kokkinakis. Text genre detection using common
word frequencies. In Proceedings of 18th International Conference on Computational
Linguistics, 2000.
[93] TREC. The ohsumed document collection, last visited: 10/15/2006.
[94] P. Turney. Thumbs up or thumbs down? Semantic orientation applied to unsuper-
vised classification of reviews. In Proceedings of ACL-02, 40th Annual Meeting of the
Association for Computational Linguistics, pages 417–424, Philadelphia, US, 2002.
Association for Computational Linguistics.
[95] P. D. Turney and M. L. Littman. Measuring praise and criticism: Inference of semantic
orientation from association. ACM Transactions on Information Systems, 21(4):315–
346, 2003.
[96] F. Tweedie, S. Singh, and D. Holmes. Neural network applications in stylometry: The
federalist papers. Computers and the Humanities, 30(1):1–10, 1996.
[97] J. Unsworth. Scholarly primitives: what methods do humanities researchers have
in common, and how might our tools reflect this? In Symposium on Humanities
Computing: formal methods, experimental practice, King’s College, London, 2000.
[98] C. van Rijsbergen. Information Retrieval. Butterworths, London, 1979.
[99] V. Vapnik. The Nature of Statistical Learning Theory. Springer, 1999.
[100] S. Weiss, N. Indurkhya, T. Zhang, and F. Damerau. Text mining: Predictive Methods
for Analyzing Unstructured Information. SpringerVerlag, 2004.
[101] C. Whissell. The dictionary of affect in language. In R. Plutchik and H. Kellerman, ed-
itors, Emotion: Theory, Research and Experience, chapter 5, pages 83–113. Academic
Press, Inc., 1989.
[102] J. Wiebe. Learning subjective adjectives from corpora. In Proceddings of AAAI 2000,
2000.
[103] I. H. Witten, K. J. Don, M. Dewsnip, and V. Tablan. Text mining in a digital library.
International Journal on Digital Libraries, 4(1):56–59, 2004.
[104] Y. Yang and X. Liu. A re-evaluation of text categorization methods. In Proceedings of
the 22nd Annual International ACM SIGIR Conference on Research and Development
in Information Retrieval (SIGIR’99), pages 42–49, Berkeley, California, 1999.
[105] Y. Yang and J. Pedersen. A comparative study on feature selection in text categoriza-
tion. In Proceedings of ICML’97, 1997.
112
[106] H. Yu. Data Mining Via Support Vector Machines: Scalability, Applicability, and
Interpretability. PhD thesis, Computer Science Department, University of Illinois at
Urbana-Champaign, 2004.
[107] S. Zelikovitz and H. Hirsh. Using lsi for text classification in the presence of background
text. In Proceedings of CIKM’01, pages 113–118, 2001.
113
