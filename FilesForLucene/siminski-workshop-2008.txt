Neuro-fuzzy Approach to Sentence Boundary Verification
in Polish Text
Krzysztof  Simiński, Silesian University of  Technology
Abstract 
The sentence boundary verification is the crucial
task  in  the  natural  language  processing.  The  text
analysis is very often based on sentences. The origin
of  the problem originates from the haplography of
the full stop. Many approaches have been proposed
(as  direct,  character,  feature  and  phrase  analyses).
This  paper  presents  the  attempt  at  applying  the
neuro-fuzzy approach to the feature analysis of  the
lexems  in  the  neighbourhood  of  the  full  stop  in
question. The results of  the experiments reveal that
the  neuro-fuzzy  systems  achieve  similar  or  better
results  with  two  or  three  rules  whereas  the  crisp
system generated rule bases with more than 20 rules.
1 Introduction
The sentence boundary disambiguation compri-
ses two tasks. The first one is the determination of
sentence  boundary  in  text  with  no  punctuation
marks (sometimes also with no case discrimination).
The  latter  is  the  verification  of  the  sentence
boundaries  in  texts  with  punctuation  marks  and
preserved  letter  cases.  This  paper  deals  with  the
verification of  the sentence boundaries. 
The problem of  sentence boundary verification is
important  in  natural  language processing.  The text
analysis  is  often  based  on  sentences  (as  in  Polsyn
parser for Polish language [16]). The certain kind of
circular relation can be noticed: The text analysis is
based on sentences and the sentence boundary dis-
ambiguation  needs  the  syntactical  analysis  of  the
text.  The Polish language is an inflecting language so
some stages of  text analysis can be conducted basing
on the morphological analysis of  the lexems without
detailed syntactical one. To cut this  circulum vitiosum
the sentence boundary verification is  based on the
morphological features analysis.
The problem of  sentence boundary verification
originates from the multiple functions of  full stop in
Polish. The functions of  full stop are: the ending the
sentence, denoting the ordinal numeral, abbreviation,
omission,  thousand  separator  in  numbers.  The
additional problem is the haplography. The full stop
is written only once when it should be used twice.
The full stop being the part of  the phrase at the end
of  the sentence merges with the full stop ending the
sentence. So there cannot be two adjacent full stops
(the exception is the omission usage ‘...’). 
2 Methods
The  most  commonly  used  methods  are:  direct
analysis,  the analysis  of  characters,  the analysis  of
features and finally the analysis of  phrases [13].
2.1 The direct analysis
This  method  simply  implements  the  situations
when the full  stop denotes the sentence boundary
and  when  does  not.  The  method  requires  the
collaboration  of  linguists  and  engineers.  The  new
cases often involve changes in the parser code, so the
maintenance is expensive and prone to errors [17].
2.2 The character analysis
The method is based on the regular expressions.
These are used to extract the situation when the full
stops are not used as sentence boundary signs. The
common  usage  are  the  Internet,  e-mail  and  IP
addresses,  numbers,  dates,  IP  addresses,  initials,
enumerations [3, 17].
2.3 The feature analysis
This  the  most  popular  approach  to  sentence
boundaries  verification.  The lexem in question,  eg.
the  full  stop  (rarely  the  question  and  exclamation
marks),  is  analysed  with  some  preceding  and
following  lexems.  This  series  of  adjacent  lexems
builds  up the window (called also a  context).  The
window is usually symmetrical with the same number
of  preceding and following lexems. The width of  the
window is usually 3 [8, 17], 5 [9], 7 [14, 9] or 9 [9].
Wider windows are practically not used.
The  features  extracted  for  sentence  boundary
disambiguation are commonly: the length of  lexem,
the part of  speech it represents (verb, noun, adverb,
...), the case (upper or lower case), whether it is an
abbreviation, whether the lexem is a Roman number
[13, 8, 17, 12].
The  main  methods  for  prediction  based  on
feature analysis are: decision trees [9], decision rules
267
X International PhD Workshop
OWD’2008,  18–21 October 2008
[14], hidden Markov models [18], maximum entropy
[11], artificial neural networks [9].
2.4 Phrase analysis
The  paper  [13]  proposes  the  intuitive  linguistic
approach to the task in question. It is based on the
phrases. Some of  them that have full stops cannot
denote  the  sentence  boundary  because  of  the
semantic role in the sentence, eg.:  prof., tzw., tzn., or
np. Some phrases with full stop can stand at the end
of  the  sentence  and  their  full  stop  then  is  haplo-
graphic, eg.: itd., itp., p. n. e.
This approach is specific for the language, but the
relatively  simple  heuristics  outperforms  other
methods of  sentence boundary disambiguation.
3 Neuro-fuzzy approach for feature analysis
This papers is an attempt at applying the neuro-
fuzzy approach to the feature analysis.  The neuro-
fuzzy systems combine the fuzzy approach of  the
fuzzy  inference  systems  with  the  ability  to  extract
and  generalise  knowledge.  Many  neuro-fuzzy
systems have been proposed as ANFIS [2], Mamdani
system,  Takagi-Sugeno-Kang  system,  system  with
parametrized  consequences  –  ANNBFIS  [1,  6,  7],
system  with  logical  interpretation  of  fuzzy
implication – ANFIS [5].
The  neuro-fuzzy  system  combines  the  fuzzy
approach and the ability of  inductive extraction of
knowledge and subsequently its  generalisation.  The
crucial  part  of  the  neuro-fuzzy  system is  the rule
base.  The  rule  base  is  the  set  of  rules  (fuzzy
implications). The ith rule can be written as
Ri : X is Ai ⇒Y is Bi  , (1)
where X =[ x1 , x2 , ... , xn]
T and Y are  linguis-
tic variables. A and B are fuzzy linguistic terms
and  is  a parameter vector of  the consequence
linguistic term. 
Each fuzzy variable in the premise of  the rule is
represented with a Gaussian membership function.
The membership values for all attributes have to be
aggregated to establish the premise. As a aggregation
operator  the  T-norm  is  used.  There  are  many  T-
norms  defined  as  minimum  (Zadeh’s  T-norm),
product (algebraic T-norm), the more sophisticated
T-norm are Łukasiewicz’s, Fodor’s, drastic, Einstein’s
T-norms.  The  T-norm  is  the  equivalence  of  the
conjunction in crisp domain. In shift from fuzzy to
crisp  domain  the  T-norm  degenerates  into
conjunction  operator.  In  ANNBFIS  and  HS-47
systems the algebraic T-norm (product) is used.
Having elaborated the rules  premises  the fuzzy
implication  is  applied.  In  these  systems  the
Reichenbach implication is used. This implication is
defined as
I x , y =1x xy.
The fuzzy  sets  in  the  conclusions  of  rules  are
different  from the Gaussian  sets  in  premises.  The
sets in conclusion are isosceles triangles. The width
of  the  base  of  each  triangle  is  constant  in
elaborating the answer of  the system, but is tuned in
learning  procedure.  The  height  of  each triangle  is
determined by the firing strength of  the appropriate
rule.  As  it  has  been  mentioned  before,  the  firing
strength  of  the  rule  is  the  T-norm  of  the
membership values of  all attributes. The localization
of  the set core is calculated as a linear combination
of  the  input  attributes.  This  is  why  the  system is
sometimes  called  –  neuro-fuzzy  inference  system
with moving consequences. The triangle sets of  all
rules are aggregated into one fuzzy set. This set is a
fuzzy  output  of  the  system.  Then  this  set  is
defuzzified  in  order  to  get  the  crisp  output.  The
defuzzification procedure in the ANNBFIS and HS-
47 system is  MICOG (modified center  of  gravity)
method. 
The above described  scheme of  the  systems is
depicted in the figure 1. 
with parameterized consequences.
The only problem left to solve is the extraction
of  rule base from the presented data. 
In the ANNBFIS system the task domain is split
into  regions  by  means  of  the  fuzzy  clustering.
Having split the domain the regions are tuned. The
parameters of  the regions (the core location and the
standard deviation) in premises and the width of  the
triangle sets in consequences are tuned by means of
gradient  method.  This  is  why  the  membership
functions in the premises have to be differentiable.
The linear combination coefficients of  the set core
localization in the consequences are tuned with the
least square method.
In  the  HS-47  system  the  domain  is  split
hierarchically. In each iteration the worst region (the
region  with  the  largest  contribution to  the  system
error) is split into two regions. Then all regions are
tuned  in  the  same  way  as  they  are  tuned  in
268
ANNBFIS.  Having tuned all  the regions the error
values are calculated for each data tuple. The error
and the expected values of  the output attribute are
added to the tuples for the next partition.
4 Experiments
The neuro-fuzzy systems used in this paper are
ANNBFIS  and  the  neuro-fuzzy  system  with
hierarchical domain split – HS-47.
The data for analysis have been extracted form
Polish  newspaper  text.  This  genre  contains  more
situation of  sentence boundaries than belles-lettres.
The morphological analysis of  the texts was done
with Advanced Linguistic Server (LAS) developed in
the Institute of  Informatics at the Silesian University
of  Technology [15, 4]. The corpus of  128 texts (with
114 635 words) has been split into training and test
parts. The training part (44 texts) was the source of
2198  feature  tuples.  There  have  been  prepared
dataset with window width of  5, 7 and 9 elements.
The features of  lexems are: the kind of  punctuation
mark in question (‘.’, ‘!’, ‘?’), for the rest of  lexems in
context:  the  length  (the  character  number),  is
capitalised?, is a numeral?, is a Roman numeral?, is
known word (to the LAS analyser)?, is a punctuation
mark? [13]
As  reference  method  for  the  neuro-fuzzy
approach  the  decision  tree  algorithm,  maximum
entropy  and  antificial  neural  network  have  been
selected.
The decision tree and subsequently the rules has
been induced with C4.5 algorithm [10].
The  artificial  neural  network  was  the tree layer
network with one input neuron for each feature and
one output neuron. The activation function for all
the neurons was the sigmoid function. The weights
of  the  neurons  were  adjusted  by  means  of  back
propagation. 
5 Results
The results of  experiments are presented in the
table 1. The classification error is the ratio of  wrong
classifications.  The  reference  classification  error  is
the error when all full stops are treated as ends of
sentences.  The  table  1  presents  the  classification
errors  for  test  texts  elaborated  with  different
methods and window width.
The C4.5 algorithm created rule bases of  21 rules
(5-lexem window), 20 rules (7-lexem window) and 24
rules  (9-lexem  window).  The  neuro-fuzzy  systems
achieved the similar (or better) classification with 2, 3
or 5 rules.
The experiments  with large window reveal  that
for the best result the window should comprise only
a few elements [9].
Tab. 1.
The results of sentence boundary verification.
method error [%] rules window
C4.5 4.90 21 5
C4.5 5.36 20 7
C4.5 4.90 24 9
annbfis 5.66 10 5
annbfis 7.37 21 5
annbfis 4.91 2 5
annbfis 5.07 3 5
annbfis 5.09 5 5
hs-47 4.43 2 5
hs-47 4.50 3 5
hs-47 4.67 5 5
annbfis 4.80 2 7
annbfis 4.89 3 7
annbfis 4.56 5 7
hs-47 4.32 2 7
hs-47 4.63 3 7
hs-47 5.47 5 7
annbfis 5.77 2 9
annbfis 4.96 3 9
annbfis 6.12 5 9
hs-47 4.52 2 9
hs-47 4.24 3 9
hs-47 5.06 5 9
maxent 6.1 – 9
ANN 5.2 – 5
reference error 18.8 – –
6 Discussion
The  C4.5  algorithm  creates  the  crisp  tree  and
then extracts the crisp rules. The number of  rules is
automatically  determined  by  the  algorithm.  In  the
task in question the C4.5 creates the rule bases with
more than 20 rules,  what  makes the interpretation
practically  unintelligible.  The  neuro-fuzzy  system
achieve  similar  precision  and  error  with  very  few
(two or three) rules. The results reveal also that the
incrementation of  number of  rules has an adverse
impact  on  the  precision.  The  number  of  rules
reaching up to the one in crisp approach deteriorates
results. The results also highlight that the expansion
of  the window does not lead to the improvement of
error reduction. This phenomenon is also reported
by other researchers [9]. 
7 Summary
The problem of  sentence boundary verification
in  Polish  text  is  not  widely  discussed  [13].  This
papers deals with applying the neuro-fuzzy approach
to feature analysis. 
The paper presents the results of  the applying the
neuro-fuzzy  approach  to  the  sentence  boundary
verification  –  an  important  problem  in  natural
language  processing.  The  experiments  have  been
executed on the real life Polish temporary newspaper
269
texts. The text corpus has been split into train and
test sets.  The results  elaborated by the two neuro-
fuzzy systems are similar or better than the results of
the crisp approach.  The important  fact  is  that  the
neuro-fuzzy systems achieve the most precise results
with  only  a  few  rules  (2  –  3),  whereas  the  crisp
algorithm generates more than 20 crisp rules. 
The neuro-fuzzy approach achieved better results
than the maximum entropy method.
The results also reveal that it is useless to extend
window width to more than few lexems. The wider
window does not improve precision and prediction
abilities  of  the system and lengthens the time and
grows the expenses of  computation. 
Bibliography
[1]Czogała E., and Łęski J., Fuzzy and Neuro-Fuzzy
Intelligent  Systems.  Series  in  Fuzziness  and Soft
Computing.  Physica-Verlag,  A Springer-Verlag
Company, 2000.
[2] Jang  J.-S.  R.,  ANFIS:  Adaptive-Network-Based
Fuzzy  Inference  System.  IEEE  Transactions  on
Systems,  Man,  and  Cybernetics,  23:665–684,
1993.
[3] Jassem K. Przetwarzanie tekstów polskich w systemie
tłumaczenia  automatycznego  POLENG.
Wydawnictwo Naukowe UAM, 2006.
[4] Sławomir  Kulików.  Implementacja  serwera  analizy
lingwistycznej  dla  systemu  THETOS  –  translatora
tekstu na język migowy.  Studia Informatica,  24(3
(55)):171–178, 2003.
[5] Łęski J. Systemy neuronowo-rozmyte. Wydawnictwa
Naukowo-Techniczne, Warszawa, 2008.
[6] Łęski J.  and Czogała E.,  A new artificial  neural
network  based  fuzzy  inference  system  with  moving
consequents  in  if-then  rules  and  selected  applications.
BUSEFAL, 71:72–81, 1997.
[7] Łęski J.  and Czogała E.,  A new artificial  neural
network  based  fuzzy  inference  system  with  moving
consequents  in  if-then  rules  and  selected  applications.
Fuzzy Sets and Systems, 108(3):289–297, 1999.
[8]Agarwal K. and Shneider M.,  Sentence boundary
detection using a maxent classifier.
[9] Palmer  D.  and  Hearst  M.  A.,  Adaptive
multilingual  sentence  boundary  disambiguation.
Computational Linguistics, 23(2):241–267.
[10]Quinlan R., C4.5: Programs for Machine Learning.
Morgan Kaufmann Publishers, 1993.
[11]Ratnaparkhi A. A simple introduction to maximum
entropy  models  for  natural  language  processing.
Technical  report,  Institute  for  Research  in
Cognitive Science,  University of  Pennsylvania,
1997.
[12]Reynar J. C. and Ratnaparkhi A.,  A maximum
entropy  approach  to  identifying  sentence  boundaries.
Proceedings  of  the  Fifth  Conference  on
Applied  Natural  Language  Processing,  pages
16–19, 1997.
[13]Simiński K., Sentence boundary verification in Polish
text.  In  Marek  Kurzyński,  Edward  Puchała,
Michał  Woźniak,  and  Andrzej  śołnierek,
editors,  Computer  Recognition  Systems  2,
volume  45  of  Advances  in  Soft  Computing,
pages  493–499.  Springer  Berlin  / Heidelberg,
2008.
[14]Stamatatos E.,  Fakotakis  N., and Kokkinakis
G.,  Automatic  extraction  of  rules  for  sentence
boundary  disambiguation. Proceedings  of  the
Workshop  in  Machine  Learning  in  Human
Language  Technology,  Advance  Course  on
Artificial Intelligence, pages 88–92, 1999.
[15]Suszczańska  N.,  Forczek  M.  and  Migas  A.,
Wieloetapowy  analizator  morfologiczny. Speech and
Language Technology, 4:155–165, 2000.
[16]Suszczańska  N.  and  Simiński  K.,  The  Polsyn
Parser. In Zygmunt Vetulani, editor, Proceeding
of  3rd  Language  &  Technology  Conference,
pages  345–349,  Poznań,  2007.  Human
Language  Technologies  as  a  Challenge  for
Computer Science and Linguistics.
[17]Walker D. J., Clements D. E., Darwin M. and
Amtrup  J.  W.,  Sentence  boundary  detection:  A
comparison of  paradigms for improving MT quality. In
MT  Summit  Proceedings  VIII,  September
2001.
[18]Wang  H.  and  Huang  Y.  Bondex  –  a  sentence
boundary detector, 2003.
Author:
Krzysztof  Simiński
Institute of  Informatics
Silesian Technical University
ul. Akademicka 16
44-100 Gliwice
Krzysztof.Siminski@polsl.pl
270
