2013 INTERNATIONAL CONFERENCE ON COMPUTING, ELECTRICAL AND ELECTRONIC ENGINEERING (ICCEEE) 
640 
 
 An Improved Semantic Plagiarism Detection Scheme 
Based on Chi-squared Automatic Interaction Detection 
 
 
Ahmed Hamza Osman
1
, Naomie Salim
2
  
 
 
1
Universiti Teknologi Malaysia, Faculty of Computer Science and Information Systems,  Skudai, Johor,Malaysia, 
2 International University of Africa, Faculty of Computer Studies, Khartoum, Sudan 
Tel +60147747409/+607 5532208   Fax  +607 5532210 
Email: ahmedagraa@hotmail.com  
 
 
Abstract—This paper introduces an improved semantic text 
plagiarism detection technique based on Chi-squared Automatic 
Interaction Detection (CHAID). The proposed technique analyses and 
compares text based on semantic allocation for each term inside the 
sentence. It also captures the underlying semantic meaning in terms 
of the relationships between its concepts via Semantic Role Labeling 
(SRL). SRL offers significant advantages when generating arguments 
for each sentence semantically. Voting for each argument generated 
by the CHAID technique, in order to select important arguments, is a 
main feature of the proposed method.  Only the most important 
arguments selected by the CHAID method were used in the similarity 
calculation process. Testing was done using the CS11 and PAN-PC-
10 datasets. The results show that this proposed method improves the 
SRL plagiarism detection method as well as exhibiting improved 
performance in terms of Recall, Precision and F-Measure when 
compared to current methods of plagiarism detection.  
 
Index Terms—Plagiarism Detection, Semantic Role Labeling, 
CHAID algorithm. 
I. INTRODUCTION 
Plagiarism practices could be fuzzier rather than clear and more 
complex than trivial copy and paste.  In general, there are various 
forms of plagiarism, such as straight plagiarism, simple 
plagiarism with footnotes, complex plagiarism using footnotes, 
plagiarism that uses citations but without quotation marks, and 
paraphrasing as plagiarism. The practice of plagiarism is a form 
of academic high treason because it undermines the entire 
scholarly enterprise. Plagiarized news, magazine articles and web 
resources are the areas of concern in this plagiarism issue. We 
have conducted an improvement study aimed at semantic 
plagiarism detection to reveal the hidden plagiarism practices 
committed by university students, and to investigate the 
academician’s experience in identifying plagiarism. Our study 
proved that university professors need computerized solutions for 
the purpose of detecting idea plagiarism.  
Semantic Role Labeling (SRL) is one of the Natural Language 
Processing techniques that were used in many fields such as text 
summarization [17], text clustering [10] and text categorization 
[14]. In this paper, a plagiarism detection scheme using the 
CHAID algorithm for selecting important arguments based on 
their similarity score is proposed. CHAID is a highly efficient 
statistical technique for segmentation and prediction. It evaluates 
all the values of potential predictor features using a statistical 
significance test criterion. 
The rest of the paper is organized as follows: 
Section II provides a description of the related work in 
plagiarism detection. In Section III, a full description of the 
underlying idea involved in our method is covered. Corpus and 
datasets, including performance measures, are presented in 
Section IV. Section V discusses the experimental design of our 
proposed method. Section VI provides a description of the results 
and discussion of the proposed method, and 
Section VII concludes the paper. 
II. RELATED WORK 
According to Alzahrani’s Survey on Plagiarism Detection 
Methods [1], common plagiarism detection techniques rely on 
character-based methods to compare the suspected document 
with the original document. Identical strings can be detected 
either exactly or partially using character-matching approaches.  
Recently, Burrows  [2] Proposed a new method to paraphrase 
acquisition via crowdsourcing and machine learning. The 
proposed method touched on one of the important research gaps 
in the plagiarism detection field, where it focused on two issues, 
acquisition via crowdsourcing, and acquisition of passage-level 
samples. The first issue challenge is automatic quality assurance; 
without such a means the crowdsourcing paradigm is not 
effective, and without crowdsourcing the creation of test corpora 
is unacceptably expensive for realistic order of magnitudes.  
A cluster based detection method was the basis of the method 
introduced by Du Zou, Wei-jiang Long and Zhang Ling [5], 
which was used in the learning management system implemented 
by SCUT to find cases of plagiarism in Network Engineering 
courses. Du Zou’s method was divided into three steps:  pre-
978-1-4673-6232-0/13/$31.00 ©2013 IEEE 
 
 
 
641 
 
selecting by narrowing the focus using the same fingerprint over 
and over again, locating fragments between documents by 
employing a find and merge function and, finally, the cluster 
method was brought into play in the post-processing stage to 
solve any merging errors.  
Muhr  [8] proposed external and intrinsic plagiarism detection 
using a Cross-Lingual Retrieval and Segmentation System. This 
system was a hybrid combining both internal and external 
detection techniques to uncover the plagiarized text in both 
translated and non-translated texts. Their study used German and 
Spanish texts that had been translated into English. Heuristic 
post-processing was used to create an information retrieval 
problem and once the results are discovered, the original 
documents are divided into overlapping blocks using a Lucene 
instance. Suspected documents are also divided using a 
consecutive set of Boolean queries performed on the Lucene 
index. A sequence analysis was performed against suspected 
passages as part of the post-processing step.     
One of the novel methods in plagiarism detection is a citation-
based technique that was proposed by Gipp and Beel [6]. The 
method is used for identifying academic documents that were 
read and used without referring to that document.  
 
We show through the literature review the propoed methods 
still need to improve the detecting technique to capture 
plagiarized parts, especially in semantically plagiarized parts. 
III. THE PROPOSED METHOD USING SEMANTIC ROLE 
LABELLING AND CHAID ALGORITHM 
The proposed method has four main steps; which are: 
1. Data Pre-processing  
a. Text segmentation 
b. Stop words removal and stemming 
2. Semantic Role Labeling Extraction 
3. Concept Extraction  
4. CHAID Algorithm Identification  
All these steps will be further discussed in the following sections: 
A. Data Pre-processing  
In this paper, the data pre-processing step comprised three sub-
steps, which were text segmentation, stop word removal and 
word stemming. Text segmentation divided a text document into 
sentences.  Some research focuses on text processing techniques 
in different fields, including intrusion detection (Sharma , 2007). 
The technology of stop words removal for deleting meaningless 
words was used. Stemming algorithm to remove the affixes 
(prefixes and suffixes) in a word in order to generate its root 
word was also applied. This step extracted the significant words 
from the text and ignored the remaining words. This may have 
adversely affected the similarity between documents.       
B. Semantic Role Labelling and Arguments Extraction 
In general, Semantic Role Labelling is a process used to identify 
and label arguments in a text.  The principle idea is that a 
document level semantic analysis determines all the arguments 
among other concepts in the document. It can be extended to the 
characterization of parts-of-speech such as to determine 
‘Subject’, ‘Object’, ‘Verb’ or ‘Adverb’. During the roles 
labelling process, each word in the suspected and source 
sentences are labelled with their corresponding roles. In this 
study, semantic role labelling based on the sentence level was 
proposed as a novel method for plagiarism detection. 
Semantic Role Labelling (SRL) aims to detect the arrangement 
similarity between concepts of the documents and possible 
semantic similarity between both documents. This step in the 
study used the role labels of the concepts for the documents and 
collected them as groups. The groups that were used in this 
method provided a quick guide to capture the suspected part of 
the document. A situation for the plagiarist can be shown through 
the following example: 
 
Example (1): 
My boss made the decision yesterday (original sentence). 
The decision was made by my boss yesterday (suspected 
sentence). 
By using the Online Demo of SRL 
(http://cogcomp.cs.illinois.edu/page/research), the produced 
arguments are: 
My boss made the decision yesterday  
 
Fig.1. Analysis for original sentence in Example 1 using SRL 
 
The decision was made by my boss yesterday  
 
Fig.2. Analysis for suspected sentence in Example 1 using SRL  
 
Figures 1 and 2 illustrate the analysis for the suspected sentence 
using SRL in the example above. We note that the structure of 
the two sentences above may differ if the active versus passive 
voice or synonyms and antonyms are used. Actually, these 
sentences can be semantically the same. It was noted that the 
SRL captures the arguments (subject, object, verb and indirect 
object) of a sentence despite changing the places for the labels 
inside the sentences. This capturing supports our proposed 
method in plagiarism detection if the comparison is applied 
based on the arguments of the sentence using SRL. 
In the SRL similarity scheme [9],  the terms in the original 
documents and the suspected documents were compared. When 
 
 
642 
 
we find two terms are identical, we look directly to the argument 
label that contains those terms, and then compare the sentences 
that convey these terms. This step compares the argument labels 
of possible sentences that have been plagiarized with 
corresponding argument labels in original sentences. The 
comparison between the terms must be done in the right way. If 
we compare the terms in Arg0 (subject) in the suspected text with 
all the other arguments in the original text to determine the 
plagiarism ratio, it can be wrong. For instance, it is not fair to 
compare the subject with Time argument (Arg-TMP) with the 
subject with Adjective argument (Arg-O).  
In example 1, the comparison used in many techniques such as 
string matching  or n-gram compares each word in the suspected 
sentence with each word in the original sentence. The term ‘boss’ 
will be compared with the terms ‘decision’, ‘made’, ‘boss’ and 
‘yesterday’. Not only is this comparison improper, but it also 
takes time for comparison. The function of our proposed method 
is to focus on the comparison between the arguments of the 
suspected sentence with similar arguments in the original 
sentence. Our proposed SRL method can compare subject with 
subject, verb with verb etc. This will lead us to decrease the 
number of comparisons. Each argument in suspected documents 
will only be compared with a similar argument in original 
documents.  
C. Concept Extraction 
Concept extraction is a main step in our detection method. In this 
study, this is considered as deriving terms with their synonyms 
from the Wordnet database. Synonymy is one of the lexical 
semantic relations, which are the associations between the 
meanings of terms. In this step, we conduct our concept 
extraction process based on the WordNet Thesaurus database. 
The process consists of the following: given a terms of text 
document that are extracted after the pre-processing step; then, it 
is mapped onto the WordNet Thesaurus database, using the 
WordNet synset (Synonym set) from the terms used in the text of 
the document. WordNet is organized around the notion of synset. 
Each synonym set contains concepts that are synonyms in a given 
context. To make the previous presented information about 
concept extraction clearer, an example of synset from the 
WordNet Thesaurus database should be provided. For example, 
the term ‘car’ can be {car, auto, automobile, machine, motor 
car,} all these terms belong to the same synset and can refer to 
the same concept. A synset is usually further described by a 
gloss: ‘4-wheeled, referring to an internal combustion engine’. 
Synsets can be related by semantic relations with each other, such 
as hyponymy (between specific and more general concepts), 
meronymy (between parts and wholes) etc. The previous example 
of the term ‘car’ is described through Figure 3. 
(http://wordnet.princeton.edu/wordnet/related-projects/) 
 
 
Fig.3. Synsets Related to the Term ‘Car’. 
 
In the above example, the synset of the term ‘Car’ {car, auto, 
automobile, machine, motor car} is related to:  
 General concept or the hypernym: e.g. {motor vehicle, 
automotive vehicle}   
 Specific concepts or hyponym: e.g. {cruiser, squad car, 
patrol car, police car, prowl car} and {cab, taxi, hack, 
taxicab}   
 Parts it is composed of: e.g. {bumper}, {car door}, {car 
mirror} and {car window}.   
Each of these synsets is again associated to other synsets as is 
described in {motor vehicle, automotive vehicle} that is 
associated with {vehicle}, and {car door} that is related to other 
parts: {hinge, flexible joint}, {armrest}, {door lock}.  
By means of these and other semantic relations, terms and 
concepts in the text will be extracted and used in our proposed 
method. 
D. CHAID Algorithm Identification 
Due to the advantages of the CHAID algorithm, we found that it 
can be used to improve the plagiarism detection results for our 
proposed method. The main reason that the CHAID algorithm 
was chosen is because this algorithm can predict and select 
important features from the data set. We will adopt this algorithm 
to select important arguments from the sentences.  
 Important Arguments Based on CHAID Algorithm 
Arguments similarity scores were calculated based on the SRL 
similarity measure that was described by Ahmed Osman et.al. 
[9]. All the similarity scores between the arguments were 
organized in a table called The Similarity Scores Table. This 
table was used as a features input of the CHAID algorithm. The 
features were represented by the arguments and total similarity 
between these arguments, where the instances of the dataset were 
represented by the number of suspected and original documents. 
The output of the CHAID algorithm will be a number of 
important arguments to improve our previous proposed method 
[9]. 
Binning and merging mechanism of the CHAID algorithm  
CHAID is a non-binary decision tree; that is, it can produce more 
than two categories at any particular level in the tree. The 
decision or split made at each node is still based on a single 
argument variable, but can result in multiple branches. The 
 
 
643 
 
mechanism of the CHAID algorithm to predict the important 
arguments that can be used in the final matching process between 
the suspected and original documents will be explained in the 
following steps: 
Binning of Predictors 
In this step, weight and frequency cases play an important role in 
the binning of a predictor. The case weight obtains unequal 
treatment to the records in a corpus. The contribution of a record 
in the analysis is weighted in proportion to the population that the 
record represented in the sample when the case weight is applied. 
On the other hand, a frequency field reflects the total number of 
observations represented by each record. It is good for analysing 
combined data, in which one record can represent more records. 
The total summation of a frequency field value should be 
equivalent to the total number of observations in the sample.  
Arguments similarity scores fields between original and 
suspected documents are binned into sets of ordinal groups. This 
process is set out below:  
 
1. Total similarity score values between the original and 
suspected documents ( ) are sorted. Where, i is a record 
number.   
2. Starting with the smallest similarity score, the relative 
weight or frequency of each unique value is calculated. 
Similarity score value less than or equal to the current 
total similarity score value  is calculated as; 
 
 
Where wk is the weight for record k ;cfi = current value 
in the total similarity score. 
3. The argument's similarity score into which a value 
belongs is determined by comparing the relative 
frequency with the argument percentile cut points 0.10, 
0.20, 0.30, etc.   The cut point is the default value 
normally used in CHAID algorithm. 
 
where w is the total weighted frequency for all similarity 
scores records in the training data, , and  
 
 
 
The value of g is taken according to the two conditions below: 
 If there is a discrepancy between the bin index for this 
similarity score value and the bin index for the previous 
similarity score value, a new bin is added to the bin list.  
The cut point for the new bin is set at the current similarity 
score value.   
 If there is no discrepancy between the bin index for this 
similarity score value and it is the same as the bin index for 
the previous similarity score value, the cut point for that bin 
to the current similarity score value is updated.  
 
Usually, CHAID will try to generate k equal ten bins by default. 
However, once the number of records containing a set of records 
with the same score has a large jointed weighted frequency, the 
binning may result in a smaller number of bins. 
The proposed method used a CHAID algorithm for two purposes. 
First, to select important predicated arguments fields from all the 
arguments of suspected and original documents. Then, to test the 
selected arguments with all arguments which were generated by 
SRL to examine the statistical significance test between them. By 
doing this, the results were improved. Secondly, the CHAID 
algorithm was also used as a statistical significance test criterion 
whereby the results obtained by CHAID become more 
significant.    
IV. PLAGIARISM DETECTION CORPUS 
The PAN-PC-10 corpus [11] comprises 27,073 text documents, 
15,925 sets of suspicious documents and 11,148 sets of source 
documents generated using the artificial plagiarism program. The 
document length varies from one page to several hundred pages. 
50% of the suspicious documents are non-plagiarized and 50% 
contains plagiarism cases. These cases were added randomly 
from the suspicious documents [11].  
A weakness of the PAN-PC is that the majority of the plagiarism 
cases were generated artificially [15]. Clough and Stevenson [4] 
reported on the CS11 human short answer questions corpus that 
contains 100 cases of plagiarism. It provides samples of 
plagiarized short passages created with different levels of 
plagiarism. The advantage of the Clough and Stevenson corpus is 
that it is developed and simulated by a human and not by the 
programs; wherein the behaviour scenario of plagiarized users is 
more natural.  
More description of the short answer question corpus was found 
in http://ir.shef.ac.uk/cloughie/resources/plagiarism_corpus.html 
and the corpus is a free download. 
Our proposed methods used the PAC-PC-10 corpus  [11]. This is 
due to semantic features simulation for plagiarism cases such as 
text paraphrasing. The system corpus that was created has 
different behaviour to that of the human corpus. Due to this 
reason, we studied the behaviour of the plagiarized users based 
on artificial plagiarism level and human plagiarism level by using 
Weight Argument Scheme [9]. To examine our proposed method 
(1) 
(2) 
(3) 
 
(4) 
 
 
644 
 
at human level, we used the human short answer questions corpus 
[4] that was created as a result of PAN-PC corpus weakness.  
A. Performance Measures 
This section discusses performance measures of plagiarism 
detection algorithms. The common performance measures used 
in plagiarism detection algorithms are Precision and Recall. A 
recent study by Potthast  [11, 12, 16] presented a micro-averaged 
and a macro-averaged variant. A F-measure or granularity is 
another important measure that was used in plagiarism detection 
evaluation [13].  
For  evaluating  the  proposed  detection technique,  we  use  
standard-de-facto that was used for semantic detection techniques 
in PAN-PC-10 competition [13] with the micro-averaged 
Precision and Recall. The micro-averaged Precision and Recall 
of R under S are defined as follows: 
 
Where, S and R denote sets of plagiarism cases and detections, s denotes 
plagiarized passages in a plagiarized document, r denotes associated and 
allegedly plagiarized passages in a document.   
 
 
The F-measure is the harmonic mean of precision and recall and 
is calculated using equation 18 below: 
 
V. EXPERIMENTAL DESIGN  
The experiments examined the amount of detecting of plagiarized 
sentences from the original documents. The experiments were 
performed on 1,000 suspected documents selected randomly 
based on the nature of dataset which we choose a random number 
of variate documents that consist of different types of plagiarism, 
different size (large, medium and small) and different type of 
plagiarism and topics. Each one of these documents was 
plagiarized from one or more original documents according to 
the PAN-PC-10 dataset. The proposed technique was applied by 
searching for the suspected documents within the original 
documents. The documents were divided into various groups 
with each group having a certain number of documents. The 
documents increased for each group with each testing of 
comparison. The process started with five documents in the first 
group. Then, five more were added to the first group and then 10, 
20, 40 and 100 respectively. The aim of this grouping process is 
to study the behaviours of the plagiarized user for each argument 
so it can be trained. After studying the behaviour of the 
arguments, the experiments were applied cross 1,000 documents. 
Each group was chosen as an input variable in FIS and all 
arguments as instances were recorded. Then the output is a total 
similarity score across these groups. The values of the input 
variable are a similarity score between any similar pair of 
arguments.  The experiments were applied across these groups up 
to 100 documents as training of the data. Then the proposed 
method was tested across 1,000 documents. It was observed that 
by using FIS, important arguments could be selected. Another 
experiment was conducted using CS11 human corpus (Clough 
and Stevenson, 2009) to test our proposed method. The CS11 
corpus was discussed in Section 7. 
The similarity between the arguments of the suspected document 
and original document was calculated according to the Jaccard 
coefficient that can be defined in the following equation: 
 
Where, C (ArgSj) = concepts of the argument sentence in the suspected 
document; Ci (ArgSk) = concepts of the argument sentence in the original 
document.  
We then calculated the similarity between the suspected 
document and original document based on the following 
equation: 
 
Where, SimCi(ArgSj, ArgSk) is similarity between arguments sentence j in 
suspected document containing concept i and arguments sentence k in original 
document containing concept i, l = no. of concepts, m = no. of arguments 
sentence in suspected document, n = no. of arguments sentence in the original 
document. 
VI. RESULTS AND DISCUSSION 
The suspected documents were plagiarized in different ways, 
such as by a simple copy and paste, changing some terms with 
their corresponding synonyms, and modifying the structure of the 
sentences (paraphrasing).  
 
TABLE 1.  RESULTS ACROSS THE SET OF DOCUMENTS 
 
 
Table 1 illustrates the results obtained from the trains performed 
on the selected set of documents. Each row represents a group of 
(8) 
(5) 
(6) 
(7) (10) 
(9) 
 
 
645 
 
documents that are used to explain the arguments during the 
similarity calculation. 
As indicated in the columns in Table 1, there are 19 arguments 
that have been extracted using the SRL. Table 2 illustrates these 
types that appeared in Table 1.  
 
TABLE 2.  ARGUMENT TYPES AND THEIR DESCRIPTIONS 
 
Table 2 shows the types of arguments that were used in the 
experiments and their description or meaning. The results of the 
similarity calculation in terms of recall, precision and f-measure 
are given in Table 3. 
 
TABLE 3.  RESULTS AFTER SIMILARITY CALCULATION 
 
Table 3 shows the similarity between the suspected and original 
documents for each set of documents. It can be observed that all 
the score values in recall measure are above 0.80 while all the 
score values in precision and f-measure are more than 0.58. All 
the scores in Table 3 seem to give good results because they are 
greater than 0.5 (Jaccard similarity threshold) but still attempts 
were made to improve these scores to obtain higher similarity 
values. 
The arguments similarity score is calculated based on an SRL 
similarity measure that was proposed and described by Osman  
[9].  
For a plagiarized behaviour, users tried to focus on the important 
terms to modify them into their work. Only important arguments 
with a high effect on sentences should be targeted to change. 
Several target selection methods are available, all of them 
intending to predict as many important targets of the data as 
possible. One of these methods CHAID algorithm. 
After we applied the CHAID technique, it was noted that the 
plagiarizing user does not focus on all arguments of the 
sentences, hence some arguments are ignored. These arguments 
are called unimportant arguments.   
Important arguments were selected to improve the similarity 
score by CHAID process. 
Figure 4 demonstrates the sample result of our proposed method 
using the CHAID algorithm tree.   
 
Fig.4. Sample Result of Our Proposed Method Using CHAID 
Algorithm Tree.   
 
In Figure 4, the similarity scores between the corresponding 
suspected arguments and original arguments were first extracted 
using SRL techniques and then organized in the Similarity Score 
Table. These scores were used for the CHAID prediction model 
construction. For CHAID prediction model construction, we have 
used the data mining component of SPSS Clementine tool (SPSS 
2011). SPSS Clementine was employed as data mining software 
for our proposed method with the CHAID algorithm. The 
difference between SPSS Clementine software and other tools is 
that its data-processing is through the use of nodes, which are 
then linked together to form a stream frame. Additionally, data 
visualization and results can be presented to users after the 
mining process has been done.  
A tree-based CHAID prediction model for the important 
arguments in plagiarism detection was constructed using the 
CHIAD algorithm that is shown in Figure 4. Each node in Figure 
4 contains the details of node-id (ID), number of data records or 
samples (N) and the possible outcome of the class variable with 
high probability. There are 126 nodes generated and distributed 
at five levels, resulting from 19 if-then conditions to predict the 
important arguments that can affect the plagiarism. These 19 
conditions refer to the number of arguments that were generated 
using the SRL technique. The tree starts with the top decision 
node (ID=0) with 1,000 samples of the data set and the whole 
data set is divided by the CHAID algorithm into eight partitions 
based on the values of splitting predictive variable (SIM or total 
similarity score). The binning and merging mechanism of these 
samples was described in Sections 3. 
 The first level of the tree contains eight nodes with 90, 100, 210, 
100, 200, 110, 20 and 170 samples respectively. This level shows 
a majority of cases associated with A0 (Subject) argument. The 
splitting percentage of those samples is 100%. The predicted 
values of this level are distributed between 0.620 and 0.980. For 
example, node 0 has a predicted value equal to 0.686; this 
indicates the highly influencing predictive score in this node. The 
highly influencing predictive score in node 1 is also equal to 
0.529 etc. Each node contains a number of records belonging to 
the predicted range. For example, node 2 contains 100 samples 
that are located in the range between 0.620 and 0.760. 
 
 
646 
 
 The second level of the tree has a different number of nodes. 
Each node of level 1 has a different number of the child nodes in 
level 2 with a unique ID. For example, node-id = 1 in level 1 has 
6 child nodes with different ID ordered from id-no 9 to id-no 14, 
and each child node at level 2 has a different number of samples; 
for example,  node-id 9 has 20 samples, node-id 10 has 10 
samples and so on. 
Figure 5 demonstrates the selected arguments of our proposed 
method using the CHAID algorithm tree.   
 
 
Fig.5.  Important Arguments Selected by the CHAID Algorithm 
 
Figure 5 demonstrates the important arguments that were selected 
by the CHAID algorithm. The column demonstrates either 
variable importance, which indicates the relative importance of 
each argument in estimating the CHAID model. The X-axis 
shows the relative importance values of the selected arguments. 
Based on the CHAID algorithm, if the relative value of the 
argument is greater that 0, then the argument will be classified as 
important, otherwise the argument will be classified as 
unimportant. On the other hand, the Y-axis shows the selected 
important arguments among all the input arguments. We noted 
that the selected arguments are (A1, A0, A1A0, A2, Loc, O, V 
and A0A1) and the rest of the arguments (MNR, TMP, ADV, 
DIS, MOD, NEG, PNC, A3, A4, DIR and EXT) were not 
selected as important arguments.  
Table 4 shows that selection of arguments by using algorithms 
gives a good result for plagiarism detection as compared to 
results obtained from the comparison of all arguments without 
discrimination.   
 
TABLE 4: EVALUATION RESULTS AFTER SELECTION OF THE 
IMPORTANT ARGUMENTS. 
 
 
The results based on CS11 corpus illustrated in Figures 6, 7 and 
8, which shows the comparison cross CS11 corpus, the Precision, 
Recall and F-measure for plagiarism classes (Heavy, Light and 
Cut-and-paste) obtained from our proposed method, was 
compared with the results from Chong , [3] using Naïve Bayes 
classifier with a set of all features, best features and Ferret 
Baseline method [7]. These methods were discussed earlier in 
Section 2. We select these methods for comparison because they 
use the CS11 human short answers questions corpus.  
 
Figure 6 demonstrates the comparison between the proposed 
method and other methods based on the heavy plagiarism class. 
We noted that the proposed method achieved the best scores in 
terms of Recall, Precision and F-measure.  
 
 
Fig.6. Comparison Results with Heavy Plagiarism Class Cross 
CS11 Corpus 
 
Figure 7 demonstrates the comparison between the proposed 
method and other methods based on the light plagiarism class. 
We noted that the proposed method achieved the best scores in 
terms of Recall, Precision and F-measure.  
 
 
Fig.7.  Comparison Results with Light Plagiarism Class Cross 
CS11 Corpus 
 
Figure 8 demonstrates the comparison between the proposed 
method and other methods based on the cut-and-paste plagiarism 
class. We noted that the proposed method achieved the best 
scores in terms of Recall, Precision and F-measure.  
 
 
Fig.8. Comparison Results with Cut-and Paste Plagiarism Class 
Cross CS11 Corpus 
 
TABLE 5: THE COMPARISON OF AVERAGE RECALL, PRECISION AND 
F-MEASURE BETWEEN THE PROPOSED METHODS AND THE 
PARTICIPATED METHODS IN PAN-PC-10  [11]. 
 
 
647 
 
 
 
Figure 9 and Table 5 demonstrates the comparison between the 
SRL-CHAID method with the top 5 participated methods in 
PAN-PC-10 [11]. Our proposed method achieved better results 
in terms of F-measure.  
 
 
Fig.9. Comparison results with other participated methods in 
PAN-PC-10 [11]. 
 
The time complexity of the proposed method was also calculated 
based on independent machine time complexity.  We found that 
our proposed method belongs to the  Class in terms of 
efficiency.  
VII. CONCLUSIONS AND FUTURE WORKS 
In this paper, a semantic plagiarism detection scheme based on 
the CHAID method was proposed and discussed. The proposed 
technique analysed and compared text based on semantic 
allocations for each term inside a sentence. Semantic Role 
Labelling offered significant advantages when it came to 
generating arguments for each sentence semantically. This was 
used to capture the semantic similarity between the sentences.  
Choosing each argument generated by the CHAID algorithm in 
order to select important arguments was another feature of the 
proposed method.  
In the future, we would suggest combining SRL-CHAID and 
SRL-FIS with Genetic Algorithms to optimize the results that are 
obtained by using the CHAID algorithm. For instance, a method 
can be developed to determine all the important arguments that 
make a greater contribution to a sentence than any other 
arguments in a sentence.   
ACKNOWLEDGMENT 
This work is supported by IDF in Universiti Teknologi Malaysia. 
The authors would like to thank the International University of 
Africa (IUA) and Research Management Centre (RMC) 
Universiti Teknologi Malaysia for the support and incentive 
extended in making this study a success. 
REFERENCES 
[1] S. M. Alzahrani, N. Salim, and A. Abraham, "Understanding Plagiarism 
Linguistic Patterns, Textual Features, and Detection Methods," Systems, 
Man, and Cybernetics, Part C: Applications and Reviews, IEEE 
Transactions on, vol. PP, pp. 1-1, 2011. 
[2] S. Burrows, M. Potthast, and B. Stein, "Paraphrase Acquisition via 
Crowdsourcing and Machine Learning," Transactions on Intelligent Systems 
and Technology (ACM TIST)(to appear), 2012. 
[3] M. Chong, L. Specia, and R. Mitkov, "Using Natural Language Processing 
for Automatic Detection of Plagiarism," Proceedings of the 4th 
International Plagiarism, 2010. 
[4] P. Clough and M. Stevenson, "Creating a Corpus of Plagiarised Academic 
Texts," 2009. 
[5] Du Zou, Wei-jiang Long, and Zhang Ling, "A Cluster-Based Plagiarism 
Detection Method," CLEF (Notebook Papers/LABs/Workshops) 2010  
[6] B. Gipp and J. Beel, "Citation based plagiarism detection: a new approach to 
identify plagiarized work language independently," 2010, pp. 273-274. 
[7] C. Lyon, J. A. Malcolm, and R. G. Dickerson, "Detecting short passages of 
similar text in large document collections," Empirical Methods in Natural 
Language Processing, 2001. 
[8] M. Muhr, R. Kern, M. Zechner, and M. Granitzer, "External and Intrinsic 
Plagiarism Detection using a Cross-Lingual Retrieval and Segmentation 
System," Lab Report for PAN, 2010. 
[9] A. H. Osman, N. Salim, M. S. Binwahlan, R. Alteeb, and A. Abuobieda, 
"An improved plagiarism detection scheme based on semantic role 
labeling," Applied Soft Computing, vol. 12, pp. 1493-1502, 2012. 
[10] N. Ozgencil, N. McCracken, and K. Mehrotra, "A Cluster-Based 
Classification Approach to Semantic Role Labeling," New Frontiers in 
Applied Artificial Intelligence, pp. 265-275, 2008. 
[11] M. Potthast, A. Barrón-Cedeño, A. Eiselt, B. Stein, and P. Rosso, "Overview 
of the 2nd international competition on plagiarism detection," Notebook 
Papers of CLEF, vol. 10, 2010. 
[12] M. Potthast, A. Barrón-Cedeño, A. Eiselt, B. Stein, and P. Rosso, "Overview 
of the 3nd international competition on plagiarism detection," Notebook 
Papers of CLEF 11, vol. 10, 2011. 
[13] M. Potthast, B. Stein, A. Barrón-Cedeño, and P. Rosso, "An evaluation 
framework for plagiarism detection," 2010, pp. 997-1005. 
[14] S. Shehata, F. Karray, and M. S. Kamel, "AN EFFICIENT MODEL FOR 
ENHANCING TEXT CATEGORIZATION USING SENTENCE 
SEMANTICS," Computational Intelligence, vol. 26, pp. 215-231, 2010. 
[15] E. Stamatatos, "Plagiarism detection using stopword n‐grams," Journal of 
the American Society for Information Science and Technology, vol. 62, pp. 
2512-2527, 2011. 
[16] B. Stein, M. Potthast, P. Rosso, A. Barrón-Cedeno, E. Stamatatos, and M. 
Koppel, "Fourth international workshop on uncovering plagiarism, 
authorship, and social software misuse," in ACM SIGIR Forum, 2011, pp. 
45-48. 
[17] L. Suanmali, N. Salim, and M. Binwahlan, "SRL-GSM: A Hybrid Approach 
based on Semantic Role Labeling and General Statistic Method for Text 
Summarization," Journal of Applied Sciences, vol. 10, pp. 166-173, 2010. 
