Preprocessing and Feature Extraction Techniques
for Multimodal Interactive Transcription of Text
Images
Alejandro H. Toselli, Verónica Romero, Moisés Pastor and Enrique Vidal
April 17, 2009
Abstract
To date, automatic handwriting recognition systems are far from being perfect and
heavy human intervention is often required to check and correct the results of such sys-
tems. This “post-editing” process is both inefficient and uncomfortable to the user. An
example is the transcription of historic documents: State-of-the-art handwritten text
recognition technology is not suitable to perform this task automatically and expensive
paleography expert work is needed to achive correct transcriptions. As an alternative to
post-editing, a multimodal interactive approach is proposed here, where user feedback
is provided by means of touch-screen pen strokes and/or more traditional keyboard and
mouse operation. User’s feedback directly allows to improve system accuracy, while
multimodality increases system ergonomy and user acceptability. Multimodal inter-
action is approached in such a way that both the main and the feedback data streams
help each-other to optimize overall performance and usability. Empirical tests on three
cursive handwritten tasks suggest that, using this approach, significant amounts of user
effort can be saved with respect to the conventional, non-interactive, post-editing pro-
cess.
0.1 Introduction
Lately, the paradigm for Pattern Recognition (PR) systems design has been shifting
from the concept of full-automaton, i.e. systems where no human intervention is as-
sumed, to systems where the decision process is affected by human feedback. One
remarkable PR example where this feedback can be successfully used is handwritten
document transcription. This task is becoming an important research topic, specially
because of the increasing number of on-line digital libraries publishing large quanti-
ties of digitized legacy documents. The vast majority of these documents, hundreds
of terabytes worth of digital image data, remain waiting to be transcribed into a tex-
tual electronic format that would provide historians and other researchers new ways of
indexing, consulting and querying these documents.
These transcriptions are usually carried out by experts in paleography, who are
specialized in reading ancient scripts, characterized, among other things, by different
calligraphy/print styles from diverse places and time periods. How long experts take
to carry out a transcription of one of these documents depends on their skills and ex-
perience. For example, to transcribe many of the pages of one of the documents used
in the experiments reported in section 0.7.1, they would typically spend more than half
an hour per page.
State-of-the-art cursive handwritten text recognition systems (HTR) can by no means
substitute the experts in this task. HTR systems have indeed proven useful for restricted
applications involving form-constrained handwriting and/or fairly limited vocabulary
(such as postal addresses or bank check legal amounts), achieving in this kind of tasks
relatively high recognition accuracy [31, 5]. However, in the case of unconstrained
handwritten documents (such as old manuscripts and/or unconstrained, spontaneous
text), current HTR technology typically only achieves results which are far from being
directly acceptable in practice.
Therefore, once the full recognition process of one document has finished, heavy
human expert revision is required to really produce a transcription of standard qual-
ity. The human transcriptor is, therefore, responsible for verifying and correcting the
mistakes made by the system. Given the high error rates involved, such a post-editing
solution is quite inefficient and uncomfortable for the human corrector.
An interactive scenario allows for a more effective approach. Here, the automatic
HTR system and the human transcriptor cooperate to generate the final transcription of
the text images. The rationale behind this approximation is to combine the accuracy
provided by the transcription expert with the efficiency of the HTR system. We call
this approach “Computer Assisted Transcription of Text Images” (CATTI) [36, 30]. It
follows similar ideas as those previously applied to computer assisted translation [4, 1]
and speech transcription [27, 40], where experiments and real field tests have shown
that, by capitalizing on the human feedback, this kind of systems can save significant
amounts of overall human effort.
As will be discussed later, human feedback signals in interactive systems rarely be-
long to the same domain as the one the main data stream comes from, thereby entailing
some sort of multimodality. Of course, this is actually the case in CATTI, where the
main data are text images and feedback consists of keystrokes and/or pointer position-
ing actions. Nevertheless, at the expense of loosing the deterministic accuracy of the
1
elantiguos que en castillo en llamaradasudadanosci ,
Figure 1: Top: illustration of CATTI multimodal user-interaction using a touch-screen. Bottom:
page fragment showing a line image being processed, with a partially corrected system sugges-
tion (in grey and black roman font) and the (previous) corrections made by the user through pen
strokes and handwriting input marked in red.
traditional keyboard and mouse, more ergonomic multimodal interfaces are possible.
It is worth noting, however, that the potential increase in user-friendliness comes at
the cost of acknowledging new errors coming from the decoding of the feedback sig-
nals. Therefore, solving the multimodal interaction problem amounts to achieving a
modality synergy where both main and feedback data streams help each-other to opti-
mize overall accuracy. These ideas have recently explored in the context of computer
assisted translation, where speech signals are used for feedback [39, 40].
Among many possible feedback modalities for CATTI, we focus here on touch-
screen operation, which is perhaps the most natural modality to provide the required
feedback in CATTI systems. Figure 1 (top) shows a user interacting with a CATTI
system by means of a touch-screen. Both the original text image and the successive
off-line HTR system’s transcription hypotheses can be easily aligned and jointly dis-
played on the touchscreen, as shown in figure 1 (bottom). This way, the user corrective
feedback can be quite naturally provided by means of pen strokes, exactly registered
over the text produced by the system, which are fed to a an on-line HTR subsystem.
A uniform technology, based on Hidden Markov Models (HMMs) [2, 19, 42, 35],
2
is used in this work both for the main, off-line HTR system and for the on-line feedback
HTR subsystem. In both cases, HMMs are used in the same way as they are used in the
current automatic speech recognition (ASR) systems [25]. The most important differ-
ences lay in the type of input feature vectors sequences; while they represent acoustic
data in the case of ASR, line-image features and point coordinates of handwritten pen
strokes constitute the input sequences for on- and off-line HTR, respectively.
In this paper we describe the CATTI framework and present the multimodal version
of this framework discussed above (MM-CATTI). We also present a comprehensive se-
ries of experiments to assess the capabilities of the techniques proposed for CATTI and
MM-CATTI. The results of these experiments clearly suggest that significant amounts
of human effort can be saved using these techniques.
Clearly, in these interactive scenarios, assessing system performance should ulti-
mately require human work and judgement. However, as we will see later, the very
convenient and successful PR assessment paradigm based on labeled corpora is still
applicable here to obtain adequate estimates of human effort required to achieve the
goals of the considered tasks. To this end, corpora corresponding to three handwritten
text transcription tasks are considered, including the well-known and publicly available
IAMDB corpus [17]. In addition to this kind of off-line text-image corpora, evaluating
MM-CATTI requires on-line, touch-screen data to be decoded by the feedback subsys-
tem. While in MM-CATTI this is the very essence of human interaction, we will show
how the production of this kind of on-line data can be properly simulated, again by us-
ing a publicly available on-line handwritten corpus (UNIPEN [8]). This way (some of)
the experiments carried out in this paper are fully reproducible by strictly using public
resources.
In the next section we first introduce a formal PR framework for multimodal in-
teraction. Then CATTI and MM-CATTI are presented under this framework in sec-
tions 0.3 and 0.4. A general description of the off- and on-line text processing subsys-
tems is given in sections 0.5 and 0.6. Experimental results are reported in section 0.7.
Finally, section 0.9 summarizes the work presented and draws directions for future
research.
0.2 Interactive Pattern Recognition and Multimodal In-
teraction
The idea of interaction between humans and machines is by no means new. In fact,
historically, machines have mostly been developed with the aim of assisting human
beings in their work. Since the introduction of computer machinery, however, the
idea of fully automatic devices that would completely substitute the humans in certain
types of tasks, has been gaining increasing popularity. This is particularly the case in
areas such as PR. Scientific and technical research in this area have followed the “full
automation” paradigm traditionally, even though, in practice, full automation often
proves elusive or unnatural in many applications where technology is expected to assist
rather than replace the human agents.
Placing PR within the human-interaction framework requires fundamental changes
3
in the way we look at problems in this area. Interestingly, these changes entail impor-
tant research opportunities which hold promise for a new generation of truly human-
friendly PR devices. Some ideas within this Interactive Pattern Recognition (IPR)
framework were presented in [40], where tight interrelations between user feedback,
multimodality and adaptive learning are examined.
Following these ideas we review in this section how human feedback can be directly
used to improve the system performance and discuss the multimodal issues entailed by
the resulting IPR framework. Since adaptive learning is not considered in this paper,
system operation is supposed to be driven by a fixed statistical model M .
In traditional PR [7], for a fixed model M and a given input x, a best hypothesis is
one which maximizes the posterior probability Pr(h | x):
ĥ = argmax
h∈H
PM (h | x) (1)
Now, interaction allows adding more conditions, that is:
ĥ = arg max
h∈H
PM (h | x, f) (2)
where f stands for the feedback, interaction-derived informations; e.g., in the form of
partial hypotheses or constraints on H. The new system hypothesis, ĥ, may prompt
the user to provide further feedback informations, thereby starting a new interaction
step. The process continues this way until the system output is acceptable by the user1.
Clearly, the richer the feedback signals f in (2) the greater the opportunity to obtain
better ĥ. But solving the maximization (2) may be more difficult than in the case of
our familiar PM (h | x). Adequate solutions are discussed in the following sections for
HTR applications.
It is interesting to note that, in general, the interaction feedback informations f do
not naturally belong to the original domain from which the main data, x, comes from.
This entails some sort of multimodality, apart from the possible multimodal nature of
the input or feedback signals themselves.
For the sake of simplicity, we assume that neither x nor f are multimodal. There-
fore, eq. (2) corresponds to a fairly conventional modality fusion problem which can be
straight-forwardly re-written as:
ĥ = argmax
h∈H
PM (x, f | h) · PM (h) (3)
where PM (h) is the prior probability of the hypothesis. In many applications it is
natural and/or convenient to assume independence of x and f given h. Consider for
instance that x is an image and f the acoustic signal of a speech command possibly
describing the image contents. In this case, a naı̈ve Bayes decomposition leads to:
ĥ ≡ arg max
h∈H
PMX (x | h) · PMF (f | h) · PMH (h) (4)
1This is a first-order approach, where ĥ is derived using only the feedback obtained in the previous
iteration step. More complex, higher-order models are not considered in this paper.
4
which allows for a separate estimation of independent models, MX , MF and MH for
the image and speech components, and the speech command language, respectively2.
As will be discussed later in this paper, the maximization in eq. (4) can often be ap-
proached by means of adequate extensions of techniques available to solve the corre-
sponding search problems in the traditional non interactive/multimodal framework.
0.3 Computer Assisted Transcription of Text Images
(CATTI)
In this section we discuss the application of the IPR framework outlined in section 0.2
to the transcription of handwritten documents. This application is called Computer
Assisted Transcription of Text Images (CATTI) [36]. In conventional handwritten text
recognition [2, 19, 33] the input pattern x is a sequence of feature vectors describing a
text image along its horizontal axis (see section 0.5.2 for details). On the other hand,
system hypotheses are sequences of transcribed words, w, from a certain language.
This way, eq. (1) is rewritten as:
ŵ = argmax
w
P (w | x) = argmax
w
P (x | w) · P (w) (5)
where P (x | w) is given by morphological word models and P (w) by a language
model. As it will be discussed in section 0.5.3, morphological word models are con-
catenation of character Hidden Markov Models (HMMs) [11], whereas n-grams are
used for language modelling [11].
In the CATTI framework, the user is directly involved in the transcription process
since he/she is responsible of validating and/or correcting the HTR output during the
process. According to the IPR paradigm, the system should take into account the cur-
rent state to improve the following predictions. The process starts when the system
makes an initial prediction consisting in a whole transcription of (some adequate seg-
ment of) the input image. Then, the user reads this prediction until a transcription error
is found. At this point, the user corrects this error, generating a new, extended prefix
(the previous validated prefix plus the amendments introduced by the user). This new
prefix is used by the HTR system to attempt a new prediction, thereby starting a new
cycle that is repeated until a final correct transcription is achieved.
An example of this process is shown in figure 2. It is worth noting in this example
that non-interactive post-editing would have required the user to correct six errors from
the original recognized hypothesis whereas, with the interaction feedback, only two
user-corrections (the red color text in the final transcription T ) are necessary to get the
final error-free transcription.
Ergonomics and user preferences dictate exactly when the system should start a
new cycle. Typically, it can start after each new user keystroke or after each user-
entered whole word. A timed system reaction scheme is also possible, where new
predictions are computed only upon detecting a short period of user inactivity. Even
though keystroke-level interaction is most preferable in practice, for the sake of clarity,
2To simplify notation, PMZ (z | . . .) will be denoted as P (z | . . .) from now on.
5
x
STEP-0 p
ŝ ≡ ŵ antiguas     cuidadelas     que   en  el Castillo sus    llamadas       
p′ antiguas    cuidadelas     que   en  el Castillo sus    llamadas        
STEP-1 κ antiguos    cuidadelas     que   en  el Castillo sus    llamadas        
p antiguos    ciudadanos    que   en  el Castillo sus    llamadas       
ŝ antiguos    ciudadanos    que   en  el Castillo sus    llamadas       
p′ antiguos    ciudadanos    que   en  el Castillo sus    llamadas       
STEP-2 κ antiguos    ciudadanos    que   en  Castilla    sus    llamadas    
p antiguos    ciudadanos    que   en    Castilla     se     llamaban      
ŝ antiguos    ciudadanos    que   en    Castilla    se     llamaban       
p′ antiguos    ciudadanos    que   en    Castilla    se     llamaban       
FINAL κ antiguos    ciudadanos    que   en    Castilla    se     llamaban   #  
p ≡ T antiguos    ciudadanos    que   en    Castilla    se     llamaban       
Figure 2: Example of CATTI interaction to transcribe an image of the Spanish sentence “an-
tiguos ciudadanos que en Castilla se llamaban”. Initially the prefix p is empty, and the system
proposes a complete transcription ŝ ≡ ŵ of the input image x. In each interation step the user
reads this transcription, accepting a prefix p′ of it. Then, he or she types in some keystrokes,
κ, to correct some words of the transcription provided by the system, thereby generating a new
prefix p (the accepted one p′ plus the text κ added by the user). At this point, the system suggests
a suitable continuation ŝ of this prefix p and this process is repeated until a complete and correct
transcription of the input signal is reached. In the final transcription, T , the user-typed text is
typeset in red color. In this example the estimated post-editing effort (WER) is 6/7 (86%), while
the corresponding interactive estimate (WSR) is 2/7 (29%, assuming a whole-word correction in
step-1). This results in an estimated effort reduction of 1–29/86 (66%).
only whole-word interactions will be considered in the present study. This will allow
us to properly compare the estimated user-effort reduction achieved by CATTI with
respect to conventional post-editing of automatic transcriptions.
A good estimate of post-editing effort is given by the popular Word Error Rate
(WER), which explicitly measures the minimum number of whole-word edit opera-
tions needed to edit an automatic transcription into the correct text, relative to the total
number correct words. Similarly, the human-interaction effort can be estimated as the
number of words in the system predictions that have to be interactively corrected to
achieve a perfect transcription, relative to the total number of correct words. This mea-
sure is often called “Word Stroke Ratio” (WSR) [1, 27]. Clearly, the relative difference
between WER and WSR will give us an fair idea of the relative amount of human effort
that a CATTI system can save. For the example shown in figure 2, the WER is 86%
and (assumming a whole-word correction in the first step) the WSR is 29%, yielding
an estimated effort reduction of 66%.
Formally, the CATTI framework can be seen as an instantiation of the problem
formulated in eq. (2) where, in addition to the given image x, a user-validated prefix p
of the transcription is available. This prefix, which corresponds to the feedback f in
eq. (2), contains information from the previous system’s prediction plus user’s actions,
6
in the form of amendment keystrokes. The HTR system should try to complete this
prefix by searching for the most likely suffix ŝ (ĥ in eq. (2)), according to:
ŝ = arg max
s
P (s | x, p) = arg max
s
P (x | p, s) · P (s | p) (6)
Eq. (6) is very similar to eq. (5), where w is the concatenation of p and s. The main
difference is that here p is given. Therefore, the search must be performed over all
possible suffixes s of p and the language model probability P (s | p) must account for
the words that can be written after the fixed prefix p.
In order to solve eq. (6), the image x can be considered split into two fragments,
xb1 and x
m
b+1, where m is the length of x. By further considering the boundary point b
as a hidden variable, we can write:
ŝ = arg max
s
∑
0≤b≤m
P (x, b | p, s) · P (s | p) (7)
We can now make the naı̈ve (but realistic) assumption that the probability of xb1
given p does not depend on the suffix and the probability of xmb+1 given s does not
depend on the prefix and, approximating the sum over all the possible segmentations
by the dominating term, eq. (7) can be rewritten as:
ŝ ≈ argmax
s
max
0≤b≤m
P (xb1 | p) · P (x
m
b+1 | s) · P (s | p) (8)
This optimization problem entails finding an optimal boundary point, b̂, associated
with the optimal suffix decoding, ŝ. That is, the signal x is actually split into two
segments, xp = xb̂1 and xs = x
m
b̂+1
, the first one corresponding to the prefix and the
second to the suffix. Therefore, the search can be performed just over segments of
the signal corresponding to the possible suffixes and, on the other hand, we can take
advantage of the information coming from the prefix to implement the language model
constraints modelled by P (s | p).
0.3.1 Language Modelling and Search
P (s | p) can be approached by adapting an n-gram language model so as to cope
with the consolidated prefix. A conventional n-gram would provide a model for the
probability P (w), where w is the concatenation of p and s, i.e the whole sentence. But
now a part of w (p) is fixed. Therefore, some changes are needed.
Let p = wk1 be the consolidated prefix and s = w
l
k+1 be a possible suffix. P (s | p)
can be computed as3:
P (s | p) =
P (p, s)
P (p)
≈
∏l
i=1 P (wi | w
i−1
i−n+1)
∏k
i=1 P (wi | w
i−1
i−n+1)
=
l
∏
i=k+1
P (wi | w
i−1
i−n+1) (9)
3As in [41], we assume that for any string z, the substring zji denotes the empty strings if j < i.
7
Training samples
de la edad media
de edad media
de la epoca media
de la epoca actual la
de la media de
de actual la
Prefix = en la
Original Bigram
edad
mediala
epoca
actual
epoca
ed
ad
media
la
med
ia
media
eda
d
la
actual
de
actua
l
dede
Linear model for the prefix
en la laen
Combined model
edad
mediala
epoca
actual
epoca
ed
ad
media
med
ia
media
eda
d
la
actual
de
actua
l
laenen
la
de
Figure 3: Example of CATTI dynamic language model building. First, a bigram for the training
set of the figure is obtained. Then, a linear model which accounts for the prefix “en la” is
constructed. Finally, these two models are combined into a single prefix-constrained model.
Moreover, for the first n− 1 terms of this factorization (k+1 to k+n−1 ), we have
additional information coming from the already known words wkk−n+2. So eq. (9) can
be split as:
P (s | p) ≃
k+n−1
∏
i=k+1
P (wi | w
i−1
i−n+1) ·
l
∏
i=k+n
P (wi | w
i−1
i−n+1)
=
n−1
∏
j=1
P (sj | p
k
k−n+1+j , s
j−1
1 ) ·
l−k
∏
j=n
P (sj | s
j−1
j−n+1) (10)
where pk1 = w
k
1 = p and s
l−k
1 = w
l
k+1 = s. The first term of eq. (10) accounts for the
probability of the n−1 words of the suffix, whose probability is conditioned by words
from the validated prefix, while the second is the usual n-gram probability for the rest
of the words in the suffix.
We can explicitly rely on eq. (8) to implement a decoding process in one step, as
in conventional HTR systems. The decoder should be forced to match the previously
validated prefix p, followed by some suffix ŝ according to the constraints (10) [27].
This can be achieved by building a special language model which can be seen as the
“concatenation” of a linear model which strictly accounts for the successive words in
p and the “suffix language model” of eq. (10). This is illustrated in figure 3. Owing to
the finite-state nature of this special language model, the search involved in eq. (8) can
be efficiently carried out using the well known Viterbi algorithm [11].
8
It should be noted that a direct adaptation of the Viterbi algorithm to implement
these techniques would lead to a computational cost that grows quadratically with the
number of words of each sentence. This can be problematic for large sentences and/or
for fine-grained (character-level) interaction schemes. Nevertheless, using word-graph
techniques similar to those described in [29, 1, 16], very efficient, linear cost search
can be easily achieved.
0.4 Multimodal Computer Assisted Transcription of Hand-
written Text Images (MM-CATTI)
In CATTI applications the user is repeatedly interacting with the system. Hence, the
quality and ergonomics of the interaction process is crucial for the success of the sys-
tem. As discussed in section 0.3, traditional peripherals like keyboard and mouse can
be used to unambiguously provide the feedback associated with the validation and
correction of the successive system predictions. Nevertheless, using more ergonomic
multimodal interfaces should result in an easier and more comfortable human-machine
interaction, at the expense of the feedback being less deterministic to the system. Dif-
ferent possibilities can be explored: gaze and gesture tracking, spoken commands etc.
Here we will focus on touchscreen communication, which is perhaps the most natu-
ral modality to provide the required feedback in CATTI systems. As shown in Figure 1
(bottom), the successive system’s transcription hypotheses can be easily displayed on
the touchscreen and user feedback corrections can be made through on-line pen-strokes
and text which are exactly written over the text produced by the system. The use of
a touchscreen for post-editing and for interactively correcting the output of a speech
recognizer have been considered in [32] and [16], respectively.
More formally speaking, let x be the input image and p a user-validated prefix of
the transcription. Let t be the on-line touchscreen pen strokes4 provided by the user.
These data are related to the suffix suggested by the system in the previous interation
step and are typically aimed at accepting or correcting parts of this suffix. Moreover,
the user may additionally type some keystrokes (κ) on the keyboard in order to correct
(other) parts of this suffix and/or to add more text. Using this information, the system
has to suggest a new suffix, s, as a continuation of the previous prefix p, conditioned
by the on-line touchscreen strokes t and the typed text κ. That is, the problem is to find
s given x and a feedback information composed of p, t and κ, considering all possible
decodings, d, of the on-line data t (i.e., letting d be a hidden variable).
According to this very general discussion, it might be assumed that the user can
type with independence of the result of the on-line handwritten decoding process. How-
ever, it can be argued that this generality is not realistically useful in practical situations.
Alternatively, it is much more natural that the user waits for a specific system outcome
(d̂) from the on-line touchscreen interaction data (t), prior to start typing amendments
(κ) to the (remaining part of the previous) system hypothesis. Furthermore, this allows
the user to fix possible on-line handwritten recognition errors in d̂.
4Pen strokes are assumed to be sequences of real-valued vectors. See section 0.6 for details.
9
In this more pragmatic and simpler scenario, each interaction step can be formu-
lated in two phases. In the first one, the system relies on the input image x and the
previous transcription prefix p, in order to search for a transcription suffix ŝ:
ŝ = argmax
s
Pr(s | x, p) (11)
Once ŝ is available the user produces some (may be null) on-line touchscreen data t
and, in the second phase, the system has to decode t into a word (or word sequence), d̂:
d̂ = arg max
d
P (d | x, p, ŝ, t) (12)
Finally, the user can enter adequate amendment keystrokes κ, if necessary, and
produce a new consolidated prefix, p, based on the previous p, d̂, κ and parts of ŝ. The
process will continue in this way until p is completely accepted by the user as a full
correct transcription of x.
An example of this kind of inter-leaved off-line image recognition and on-line
touchscreen interaction is shown in figure 4. In this example, we are assuming that
on-line handwriting is the modality preferred by the user to make corrections, relay-
ing on the keyboard mainly (or only) to correct eventual on-line text decoding errors.
Note that the potential increase in comfort of this setting comes at expense of a hope-
fully small number of additional interaction steps using the keyboard. Assuming, for
simplicity, that the cost of correcting an on-line decoding error is just similar to that
of another on-line touchscreen interaction, in this example the user would need three
interactive corrections using MM-CATTI, compared with the six post-editing word cor-
rections required by the original, fully off-line recognized hypothesis. Although fig-
ure 4 may suggest otherwise, we should remind that, as discussed in section 0.3, only
whole-word interactions are considered in the present work. Furthering this assump-
tion, but without loose of generality, we consider here that, in each interaction, the user
only attempts to correct a single word; that is, d̂ consists in a single, whole word.
Since we have already dealt with eq. (11) in section 0.3 (eq. 6-8), we focus now on
eq. (12). As compared with eq. (2), here the triplet (x, p, ŝ) and t would correspond
to the two modalities x and f , respectively. Therefore, assumptions and developments
similar to those of eq. (3-4) lead to:
d̂ ≡ argmax
d
P (d | x, p, ŝ) · P (t | d) (13)
As in section 0.3, P (t | d) is modelled by (HMM) morphological models of the
word(s) in d (see section 0.6.3 for details). On the other hand, here, P (d | x, p, ŝ) can
be provided by a language model constrained by information derived from the input
image x, the previous prefix p and by the suffix ŝ produced at the beginning of the
current iteration. Eq. (13) may lead to several scenarios depending on the assumptions
and constraints adopted for P (d | x, p, ŝ). We examine some of them hereafter.
The simplest one corresponds to a conventional, non-interactive on-line HTR set-
ting, where all the available conditions are ignored; i.e., P (d | x, p, ŝ) ≡ P (d). This
scenario is considered here as a baseline.
10
x
STEP-0 p
ŝ ≡ ŵ antiguas     cuidadelas     que   en  el Castillo sus    llamadas       
p′, t antiguas     cuidadelas     que   en  el Castillo sus    llamadas       
STEP-1 d̂ antiguos    cuidadelas     que   en  el castillo sus    llamadas         
κ
p antiguos    ciudadanos    que   en  el Castillo sus    llamadas       
ŝ antiguos    ciudadanos    que   en  el Castillo sus    llamadas       
p′, t antiguos    ciudadanos    que   en  el Castillo sus    llamadas       
STEP-2 d̂ antiguos    ciudadanos    que   en    Castigos   sus    llamadas      
κ antiguos    ciudadanos    que   en   Castilla  sus    llamadas      
p antiguos    ciudadanos    que   en    Castilla     se     llamaban      
ŝ antiguos    ciudadanos    que   en    Castilla    se     llamaban       
p′, t antiguos    ciudadanos    que   en    Castilla    se     llamaban       
FINAL κ antiguos    ciudadanos    que   en    Castilla    se     llamaban   #  
p ≡ T antiguos    ciudadanos    que   en    Castilla    se     llamaban       
Figure 4: Example of multimodal interaction with a CATTI system, to transcribe an image of
the Spanish sentence “antiguos ciudadanos que en Castilla se llamaban”. Each interaction step
starts with a transcription prefix p that has been fixed in the previous step. First, the system
suggests a suffix ŝ and the user handwrites some touchscreen text, t, to amend ŝ. This defines
a correct prefix p′, which can be used by the on-line HTR subsystem to obtain a decoding of t.
After observing this decoding, d̂, the user may type additional keystrokes, κ, to correct possible
errors in d̂ (and perhaps to amend other parts of ŝ). A new prefix, p, is built from the previous
correct prefix p′, along with the decoded on-line handwritten text, d̂, and the typed text κ. The
process ends when the user types the special character “#”. System suggestions are printed in
boldface and typed text in typewriter font. All user corrections are shown in red color. In the
final transcription, T , typed text is also underlined. In this example, assumming all interactions
as whole-word corrections, the post editing WER is 6/7 (86%), while the MM-CATTI WSR is
3/7 (43%); i.e., 2 touch-screen + 1 keyboard word corrections.
A more informative setting arises by taking into account information derived from
the previous off-line HTR prediction ŝ. The touchscreen data t allows us to unumbigu-
ously divide ŝ into a first part, ŝa, which the user accepts as correct, followed by the
remaining (wrong) word(s), ŝe, which the user tries to correct using touchscreen pen
strokes. Therefore, we can assume an error-conditioned model such as P (d | x, p, ŝ)
≡ P (d | ŝe); clearly, knowing the word(s) that the user has already deemed incorrect
should prevent the on-line decoder making the same error(s).
If, in addition to ŝe, the information derived by the accepted prefix is also taken into
account, a particularly useful scenario arises. In this case the decodings of t are further
constrained to be a suitable continuations of the prefix accepted so far, p′ = p ŝa; that
is: P (d | x, p, ŝ) ≡ P (d | p′, ŝe). This multimodal model [37], which will be referred
to as MM-CATTI, is the one studied in more detail in the present article.
Finally, the most informative scenario corresponds to additionally using the infor-
11
mation of the input image, as in (13). In this case the on-line HTR decoder should find
suitable continuations that are also good partial off-line transcriptions of the input text
image. This potential source of increased performance is left for future studies.
0.4.1 Language Model and Search for MM-CATTI
Language modelling and search techniques needed for the on-line HTR feedback sub-
system in MM-CATTI are essentially similar to those described in section 0.3.1 for the
main, off-line HTR system. Language model constraints are implemented on the base
of n-grams, depending on each multimodal scenario considered.
The simplest baseline scenario does not take into account any interaction-derived
information and P (d) could be provided by the same n-gram used for the off-line
decoder. However, if only single whole-word touchscreen corrections are assumed, as
discussed in the previous subsection, only uni-grams actually make sense.
The whole-word assumption also simplifies the error-conditioned model, P (d | ŝe),
because only the first (wrong) word of ŝe is to be taken into account. Let e be this wrong
word. Therefore the error-conditioned language model probability can be written as:
P (d | e) =





0 d = e
P (d)
1 − P (e)
d 6= e
(14)
Finally, in MM-CATTI (under the same single whole-word assumption), the lan-
guage model probability is approximated by P (d | p, e) (where, to avoid cumbersome
notation, we use here p for the accepted prefix denoted p′ in the previous subsection).
That is, the on-line HTR subsystem should produce a hypothesis d̂ for the touchscreen
strokes t, taking in account a user-accepted prefix, p, and the first wrong word, e, in the
off-line HTR suggestion. In this case, arguments similar to those in section 0.3.1 apply
and we can use equation (10) changing s with d, leading to:
P (d | p, e) =





0 d = e
P (d | pkk−n+2)
1 − P (e | pkk−n+2)
d 6= e
(15)
where k is the length of p.
A simple implementation of eq. (15) is shown in figure 5. In this example, p =“dela”
and the user wants to correct the wrong off-line recognized word “media”, by hand-
writting the word “edad” (for example) on the touchscreen. If the on-line HTR sub-
system uses a bi-gram model, it is conditioned by the context word “la” (which is now
the initial state) and the word transition edge “media” is disabled.
As shown in the example, and unlike it happened in CATTI, the linear language
model of the prefix p is no longer required, because the corresponding on-line touch-
screen data of the prefix p do no exist in this case. Moreover, as we are assuming only
single whole-word corrections, only the direct transitions from the starting node (the
“la” node in the example) need be considered.
12
Original Bi-gram Model (L)
edad
mediala
epoca
actual
epoca
ed
ad
media
la
med
ia
media
eda
d
la
actual
de
actua
l
dede
Conditioned Bi-gram Model (Ld)
edad
la
epoca
epoca
eda
d
media
Figure 5: Example of MM-CATTI dynamic bi-gram language model generation. L is the
original bi-gram model used by off-line HTR system, whereas Ld is the bi-gram sub-model,
derived from L, which takes as initial state that corresponding to the prefix “la”. This simplified
language model is used by the on-line HTR sub-system to recognize the touchscreen handwritten
word “edad”, intended to replace the wrong off-line recognized word “media”, which was
disabled in Ld.
As in CATTI searching (section 0.3.1), owing to the finite-state nature of the n-
gram language model, the search involved in equation (15) can be efficiently carried
out using the Viterbi algorithm [11].
0.5 Main Off-line HTR System Overview
The off-line HTR system used here follows a classical architecture composed of three
modules: a) preprocessing, aimed at correcting image degradations and geometry dis-
tortions and devoted to decompose page images into their constituent line images; b)
feature extraction, where a vector sequence representation of each line image is ob-
tained; and c) recognition, which obtains a most likely word sequence for the sequence
of feature vectors. The following subsections describe the three modules in some detail.
0.5.1 Preprocessing
Image degradation is quite common a problem in many text images and more so in
ancient documents [6]. Typical degradations include the presence of smear and skew,
backgrounds with big variations and uneven illumination, spots due to the humidity
or marks resulting from the ink that goes through the paper (commonly called bleed-
through). In addition, other kinds of difficulties appear in these images, such as differ-
ent character styles and sizes, underlined and/or crossed-out words, etc. The combina-
tion of these problems contributes to make the recognition process difficult. Therefore,
preprocessing becomes essential to reduce the impact of these problems, as well as to
extract the actual (line) images of the text to be recognized. A survey of preprocessing
13
techniques proposed for text images can be seen in [24, 20]. In this work, the follow-
ing preprocessing steps take place in order: skew correction, background removal and
noise reduction, line extraction, slant correction and size normalization.
Skew is the angle between the horizontal and the (dominant) direction of the lines
on which the writer aligned the text. Skew correction is carried out globally on each
page image by searching for the angle which maximizes the variance of the horizontal
projection profile of the de-skewed text lines [22, 34] (see figure 6).
Background removal and noise reduction are performed by applying a 2-dimensional
median filter [14] on the whole page image and subtracting the result from the origi-
nal image. This is often followed by a grey-level normalization to increase the fore-
ground/background image contrast (see figure 6).
Line detection is based on the horizontal projection profile of the optimally de-
skewed input image. Local minima in this curve are potential cut-points between
consecutive text lines (see panel a on figure 7). Obviously, clear separation is not
always possible and cut-points detection needs to be adequately combined with con-
nected components techniques [19]. The panel b on figure 7 shows some line images
obtained with this method.
(a) (b)
Figure 6: Preprocessing example: a) original image; b) result after skew correction, background
removal, noise reduction and increase of contrast.
The slant is the angle between the vertical and the dominant direction of the writ-
ten vertical strokes. Slant correction is applied to each previously separated line image.
Much in the same way as in the case of line detection, the slant is computed by search-
ing for the angle which maximizes the variance of the vertical projection profile of
14
(a) (b)
(c)
(d)
(e)
Figure 7: Preprocessing example: a) image with cutting lines computed from the horizontal pro-
jection profile; b) separated line images from the highlighted region; c) a separated line image);
d) slant correction; e) size normalization.
the de-slanted text [22]. This tends to render the written text strokes in an upright po-
sition (see panel d of figure 7) and significantly improves the accuracy of the HMM
recognition techniques.
Finally, (non-linear) size normalization aims at making the optimally de-slanted
line images invariant to character size and attempts to reduce large areas of background
pixels which remain on the image because of the presence of long ascenders and de-
scenders of some letters [28] (see panel e of figure 7).
0.5.2 Feature Extraction
As our HTR system is based on Hidden Markov Models (HMMs), each preprocessed
text line image has to be represented as a sequence of feature vectors. Several ap-
proaches have been proposed to obtain this kind of sequences [2, 19, 3]. The approach
used in this work follows the ideas described in [2].
15
First, a grid is applied to divide the text line image into M×N squared cells. In this
work, M = 20 was chosen empirically and M is such that N/M equals the original
line image aspect ratio. Each cell is characterized by the following features: average
gray level, horizontal gray level derivative and vertical gray level derivative. To obtain
smoothed values of these features, a s × s cells analysis window, I(i, j), 1 ≤ i, j ≤ s,
centered at the current cell, is used in the computations (with s = 5 in this work) [34].
The smoothed cell-averaged gray level, g, is computed through convolution with
two 1-d Gaussian filters, wi and wj :
g =
1
s2
s
∑
i=1
s
∑
j=1
I(i, j) · wi · wj (16)
wi = exp
(
−
1
2
(i − s/2)2
(s/4)2
)
wj = exp
(
−
1
2
(j − s/2)2
(s/4)2
)
The smoothed horizontal derivative, dh, is calculated as the slope of the line which best
fits the horizontal function of column-average gray level in the analysis window. The
fitting criterion is the sum of squared errors weighted by a 1-d Gaussian filter:
dh =
(
∑s
j=1 wj gj
) (
∑s
j=1 wj j
)
−
(
∑s
j=1 wj
)(
∑s
j=1 wj gj j
)
(
∑s
j=1 wj j
)2
−
(
∑s
j=1 wj
)(
∑s
j=1 wj j
2
)
(17)
where gj is the column-average gray level at column j, defined by:
gj =
∑s
i=1I(i, j)
s
The vertical derivative, dv , is computed in a similar way.
Columns of cells (also called frames) are processed from left to right and a feature
vector is constructed for each frame by stacking the three features computed in their
constituent cells. Hence, at the end of this process, a sequence of M 60-dimensional
feature vectors (20 normalized gray-level components and 20 horizontal and vertical
derivatives components) is obtained. Figure 8 shows graphically an example of the
feature vectors sequence for a word image.
0.5.3 Character, Word and Language Modelling and Search
Characters are modeled by continuous density left-to-right HMMs, with a Gaussian
mixture per state. The number of states and Gaussians define the total amount of pa-
rameters to be estimated. Therefore, these numbers need to be empirically tuned to op-
timize the overall performance for the given amount of training vectors available. In all
the experiments of this work, HMMs with 6 states and 64 (diagonal) Gaussian densities
per state have been used. The character HMMs are trained using a well-known instance
of the EM algorithm called forward-backward or Baum-Welch re-estimation [11]. Fig-
ure 8 shows an example of how a HMM models two feature vector subsequences cor-
responding to the character “a”.
16
0.3
0.7 0.8
0.2
0.9
0.1
0.8
0.2
0.7
0.3
Figure 8: Example of feature-vector sequence and HMM modeling of instances of the character
“a” within the Spanish word “cuarenta” (“forty”). The model is shared among all instances
of characters of the same class. The zones modelled by each state show graphically subse-
quences of feature vectors (see details in the magnifying-glass view) compounded by stacking
the normalized grey level and its both derivatives features.
Each lexical entry (word) is modelled by a stochastic finite-state automaton which
represents all possible concatenations of individual characters that may compose the
word. By embeeding the character HMMs into the edges of this automaton, a lexical
HMM is obtained. These HMMs estimate the word-conditional probabilities P (x | w)
of eq. (5), as well as those of eq. (6)–(8) for CATTI.
Finally, the concatenation of words into text lines or sentences is modelled by a bi-
gram language model, with Kneser-Ney back-off smoothing [13, 15], estimated from
the given transcriptions of the trained set. Bi-gram models estimate the probability
P (w) in eq. (5), and are the basis for the “dynamic”, prefix-conditioned CATTI lan-
guage models P (s | p) of eq. (10), as explained in section 0.3.1.
To train all these HMMs and language models, a corpus is required in which each
training image is accompained by its correct transcription. These transcriptions must
accurately describe all the elements appearing in each handwritten text image, such as
(lowercase and uppercase) letters, symbols, abbreviations, etc.
Once all the character, word and language models are available, recognition of
new test sentences can be performed. Thanks to the homogeneous finite-state (FS)
nature of all these models, they can be easily integrated into a single global (huge) FS
model, on which eq. (5) is easily solved. Given an input sequence of feature vectors,
the output word sequence hypothesis corresponds to a path in the integrated network
that, with highest probability, produces the input sequence. This optimal path search
is very efficiently carried out by the well known (beam-search-accelerated) Viterbi
17
algorithm [11]. This technique allows integration to be performed “on the fly” during
the decoding process. In this way, only the memory strictly required for the search is
actually allocated. The Viterbi algorithm can also be easyly adapted to solve the eq. (8)
required in the CATTI interactive framework, as explained in section 0.3.
It should be mentioned that, in practice, HMM and bi-grams (log-)probabilities are
generally “balanced” before being used in equations (5) or (8). This is carried out by
using a “Grammar Scale Factor” (GSF), the value of which is tuned empirically.
0.6 On-line HTR Subsystem Overview
The on-line HTR subsystem is intended to decode the feedback touchscreen data for
multimodal text correction; i.e. to recognize the pen strokes (words) written by the
user in succesive CATTI interactions in order to correct or replace the errors produced
by the main, off-line HTR decoder. In general, touchscreen data consists of a series of
pen-positions (xt, yt), sampled at regular time instants t = 1, 2, . . . Each sample of
this “trajectory” can be acompained by information about the pen pressure, or at least
by one bit indicating whether the pen is actually touching the screen or it is “up”. In
this work no pressure information is used.
The conceptual architecture adopted for the on-line HTR subsystem is analogous
to that used in the main off-line HTR system, with exception of the preprocessing and
feature extraction modules, which are explained hereafter.
0.6.1 Preprocessing
An overview of preprocessing techniques for on-line HTR can be seen in [9]. In this
work, the preprocessing of each trajectory involves only two simple steps: repeated
points elimination and noise reduction. Repeated points appear in a trajectory when
the pen remains down and motionless for some time. These uninformative data are
trivially removed, along with the points marked as “pen-up”. Noise in pen strokes is
due to erratic hand motion and inaccuracy of the digitalization process. To reduce this
kind of noise, a simple smoothing technique is used which replaces every point (xt, yt)
in the trajectory by the mean value of its neighbors [10]. Note that the temporal order
of the data points is preserved throughout these preprocessing steps.
0.6.2 Feature Extraction
Each preprocessed trajectory is transformed into a new temporal sequence of 7-dimensional
real-valued feature vectors [38]. These time-domain features are point locations, first
and second time derivatives and curvature.
Normalized Horizontal and Vertical Position: the coordinate pairs of each trajec-
tory point are linearly scaled and translated to obtain new pairs of values (xt, yt), so
that yt is in the range [0, 100] and the original aspect-ratio of the trajectory is preserved.
18
Normalized First Derivatives: x′t and y
′
t are calculated using the method given
in [44]:
x′t =
∆xt
||∇||
y′t =
∆yt
||∇||
(18)
where,
∆xt =
r
∑
i=1
i · (xt+i − xt−i) ∆yt =
r
∑
i=1
i · (yt+i − yt−i) ||∇|| =
√
∆xt
2 + ∆yt
2
and r defines a window of size 2r + 1 which determines the neighbor points involved
in the computation. Setting r = 2 has provided satisfactory results in this work.
It is worth noting that the normalization of derivatives by ||∆|| implicitly entails
an effective writing speed normalization. In our experiments, this has proved to lead
to better results than using explicit speed normalization preprocessing techniques such
as trace segmentation, based on resampling the trajectory at equal-length (rather than
equal time) intervals [23, 43].
Second derivatives: x′′t and y
′′
t , are computed in the same way as the first derivatives,
but using x′t and y
′
t instead of xt and yt.
Curvature: kt, is the inverse of the local radius of the trajectory in each point. It is
calculated as:
kt =
x′t · y
′′
t − x
′′
t · y
′
t
(
x′t
2 + y′t
2
)3/2
(19)
Although this feature is an explicit combination of the previous features, it has lead to
slightly but consistently improved results in our experiments.
0.6.3 Modelling and Search
Modelling and search for on-line recognition follow almost the same schemes used in
off-line recognition, described in section 0.5.3.
As in the off-line case, we use continuous density left-to-right character HMMs
with Gaussian densities assigned to each state mixture. However, instead of using a
fixed number of states for all HMMs, it is variable for each character class. The number
of states sc chosen for each HMM character class Mc was computed as sc = lc/f ,
where lc is the average length of the sequences of feature vectors used to train Mc,
and f is a design parameter measuring the average number of feature vectors modelled
per state (state load factor). This rule of setting up sc tries to balance modelling effort
across states and, for our task, has significantly improved the recognition accuracy. On
the other hand, lexical modelling is carried out in exactly the same way as in the off-line
HTR case, both for regular on-line HTR and for the on-line feedback HTR subsystem
used in the MM-CATTI approach.
Language modelling and search are simpler in this case because, as discussed in
section 0.4.1, we have restricted our present MM-CATTI study to single whole-word
touchscreen corrections. That is, the language models used in the MM-CATTI search
only allow one word per user-interaction. As mantioned in section 0.5.3, a GSF is also
used here in practice to balance the HMM and language model probabilities of eq. (13).
19
0.7 Experimental Framework
The experimental framework adopted to assess the effectiveness of the CATTI and
MM-CATTI approaches proposed in this work is described in the following subsec-
tions. This includes details of the different corpora and performance measures used.
0.7.1 Corpora
Three corpora with similar HTR difficulty were employed in the experiments. Two
of them, ODEC-M3 and IAMDB, contain handwritten text in modern Spanish and
English, respectively. IAMDB is publicly available, thereby serving as a reference to
compare the obtained results. The third corpus, CS, consists of cursive handwritten
page images in old Spanish, which allow us to report preliminary results on the kind
of lagacy documents mentioned in section 0.1. In addition, to train the on-line HTR
feedback subsystem and test the MM-CATTI approximation, the on-line handwriting
UNIPEN corpus, which also is publicly available, was chosen.
ODEC-M3
This corpus consists of images of casual handwritten Spanish paragraphs. It was com-
piled from spontaneous answers extracted from survey forms made for a telecommu-
nication company5. These answers were written by a heterogeneous group of people,
without any explicit or formal restriction. In addition, since no guidelines were given
as to the kind of pen or the writing style to be used, paragraphs are very variable and
noisy. Many of them were written using different case and font types, variable sizes
and include words which are underlined, crossed-out or contain orthographic mistakes,
unusual abbreviations, symbols, etc. Examples of these difficulties are shown in fig. 9.
Because of some of these difficulties, line extraction was carried out in a semi-
automatic way, based on a conventional line-extraction method mentioned in section 0.5.1.
Most of the phrases were processed automatically, but manual supervision was applied
to difficult line-overlapping cases such as that shown in fig. 9 (top-left panel). By ad-
equately pasting the lines extracted from each paragraph, a single-line (long) image
which encompasses the whole paragraph was obtained. This resulted in 913 binary
images, which were partitioned into a training set of 676 images and a test set of 237
images. The transcriptions of all the images are also available, containing 16, 371
words with a vocabulary of 3, 308 different words. All this information is summarized
in the table 1. More information about this corpus can be found in [35].
IAMDB
This corpus was compiled by the Research Group on Computer Vision and Artificial
Intelligence (FKI) at Institute of Computer Science an Applied Mathematics (IAM)
in Bern (Switzerland). The IAM Handwriting Database [17, 18] (IAMDB) consists
of grey-level images of unconstrained handwritten English text forms. It is publicly
5Data kindly provided by ODEC, S.A. (www.odec.es)
20
DIFFERENT STYLES
DIFFICULT LINE SEPARATION
UNUSUAL ABBREVIATIONS
VARIABLE STROKE THICKNESS
CROSSED-OUT WORDS ORTHOGRAPHIC MISTAKES
Figure 9: Examples of difficulties found in several paragraphs of the ODEC-M3 Corpus.
Table 1: Basic statistics of the ODEC-M3 corpus and its standard partition.
Number of: Training Test Total Lexicon
writers/phrases 676 237 913 –
words 12,287 4,084 16,371 3,308
characters 64,666 21,533 86,199 80
accessible and freely available upon request for non-commercial research purposes6.
The IAMDB text images correspond to handwritten texts copied from the Lancaster-
Oslo/Bergen Corpus [12] (LOB), which encompasses around 500 printed English texts
of about 2,000 words each and about one million total running words.
The IAMBD version 3.0 (the latest at this moment) is composed of 1,539 scanned
text pages, handwritten by 657 different writers. No restriction was imposed on the
writing style or the type of pen to be used. This dataset is also provided at sentence
image level. Line detection and extracion, as well as merging lines into whole sentence
line-images, was carried out by the IAM institute [19]. Fig. 10 shows examples of
(short) handwritten sentence images from this corpus. This corpus was partitioned into
a training set composed of 2,124 sentences, handwritten by 448 different writers, and a
writer independent test set composed of 200 sentences written by 100 writers. Table 2
summarizes all this information.
Note that the amount of data available for training the (n-gram) language models for
this task (the whole LOB corpus) is very much larger than the amount of data contained
in the trasncriptions of the available text images. Following [45], we take advantage of
this oportunity by using the whole LOB corpus (except the 200 sentences of the image
test set) for n-gram training, while setting a reduced vocabulary which encompasses
only the 8, 938 different words found in the IAMDB text images.
6http://iamwww.unibe.ch/˜fki/iamDB/
21
Figure 10: Examples of handwritten sentences from the IAMDB corpus.
Table 2: Basic statistics of the IAMDB corpus and its standard partition.
Number of: Training Test Total Lexicon
writers 448 100 548 –
sentences 2,124 200 2,324 –
words 42,832 3,957 46,789 8,938
characters 216,774 20,726 237,500 78
CS manuscript
This corpus was compiled from a legacy Spanish manuscript from the XIX century
identified as “Cristo-Salvador” (CS), which was kindly provided by the Biblioteca Va-
lenciana Digital (BiVaLDi)7. This is a rather small document composed of 53 color
images of text pages, written by a single writer. Some examples are shown in figure 11.
The page images were preprocessed and divided into lines, as described in sec-
tion 0.5.1. The results were visually inspected and the few detection errors (around
4%) were manually corrected, resulting in a data-set of 1,172 text line images. Ex-
amples of automatically extracted line images were shown in figure 7. It is worth
mentioning that, unlike the two previous corpora, in this case the extrated lines are not
merged into sentence or paragraph images. The transcriptions of these line images are
also available, containing 10,911 running words with a vocabulary of 3,408 different
words.
Two different partitions, page (or “soft”) and book (or “hard”) are defined for this
data-set [30]. Here we only consider the (easier) page partition. Its test set contains 491
samples corresponding to the last ten lines of each document page, whereas the training
set is composed of the 681 remaining lines. Table 3 summarizes this information.
It is important to remark that this corpus has quite a small ratio (around 2) of train-
ing running-words to lexicon-size. This is expected to result in undertrained (n-gram)
language models, which clearly increases the difficulty of the recognition task and pre-
7http://bv2.gva.es
22
Figure 11: Examples of pages images from CS corpus.
Table 3: Basic statistics of the CS corpus and its page partition.
Number of: Training Test Total Lexicon
pages 53 53 53 –
lines 681 491 1,172 –
words 6,432 4,479 10,911 3,408
characters 36,699 25,460 62,159 78
vents CATTI or MM-CATTI to take much advantage of prefix-derived constraints.
UNIPEN Corpus
In order to make (some of) the experimets fully reproducible using publicly avail-
able data, the production of touchscreen feedback data has been simulated using the
UNIPEN corpus [8].
More specifically, the UNIPEN Train-R01/V07 dataset8 has been considered.
It comes organized into several categories [8] such as lower and upper-case letters,
digits, symbols, isolated words and full sentences. However, the UNIPEN isolated
words category does not contain all (or almost none of) the required word instances to
8For a detailed description of this dataset, see http://www.unipen.org.
23
be handwritten by the user in the MM-CATTI interaction process with the IAMDB text
images. Therefore, they were generated by concatenating random character instances
from three UNIPEN categories: 1a (digits), 1c (lowercase letters) and 1d (symbols).
To increase realism, the generation of each of these words was carried out employ-
ing characters belonging to a same writer. Three different writers were randomly cho-
sen, taking care that sufficient samples of all the characters needed for the generation
of the required word instances were available from each writter. The selected writers
are identified by their name initials as AR, BH and BR. The partition into training and
test sets for each of these categories and writers are shown in table 4.
Table 4: Basic statistics of the UNIPEN categories 1a,1c and 1d and their corresponding
partition definitions for the three selected writers, AR, BH and BR.
Number of: Train
Test
Lexicon
WrAR WrBH WrBR
digits (1a) 15,767 62 69 50 10
letters (1c) 55,355 1,006 880 1,007 26
symbols (1d) 16,452 172 230 316 32
All Together 87,574 1,240 1,179 1,373 68
0.7.2 Assessment Measures
Two kinds of measures have been adopted to asses the CATTI systems. On the one
hand, the quality of non-interactive transcriptions can be properly assessed with the
well known Word Error Rate (WER). It is defined as the minimum number of words
that need to be substituted, deleted or inserted to convert a sentence recognized by the
system into the corresponding reference transcription, divided by the total number of
correct words. The WER is a good estimate of post-editing user effort.
On the other hand, as previously mentioned, the effort needed by a human tran-
scriber to produce correct transcriptions using a CATTI system is estimated by the
Word Stroke Ratio (WSR), which can be also computed using the reference transcrip-
tions. After each CATTI hypothesis, the longest common prefix between the hypothesis
and the reference is obtained and the first unmatching word from the hypothesis is re-
placed by the corresponding reference word. This process is iterated until a full match
with the reference is achieved. Therefore, the WSR can be defined as the number of
(word level) user interactions that are necessary to achieve the reference transcription
of the text image considered, divided by the total number of reference words.
This definition makes WER and WSR comparable. Moreover, the relative differ-
ence between them gives us a good estimate of the reduction in human effort that can be
achieved by using CATTI with respect to using a conventional HTR system followed
by human post-editing. This estimated effort reduction will be denoted as “EFR”.
Finally, since only single-word corrections are considered, the conventional classi-
fication error rate (ER) will be used to assess the accuracy of the on-line HTR fedback
24
subsystem under the different constraints entailed by the MM-CATTI interaction pro-
cess.
0.8 Results
Different experiments have been carried out to assess the feasibility and potential of the
approaches proposed in this work. Some of the experiments correspond to basic, non-
interactive (off- and on-line) handwritten text recognition tasks, aimed at stablishing
baseline performance figures. The other experiments are explicitly devoted to asses the
CATTI and MM-CATTI approaches here proposed.
0.8.1 Base-line Recognition Results for off- and on-line HTR sys-
tems
Baseline off-line and on-line HTR results (WER and ER) are obtained using the basic
systems explained in sections 0.5 and 0.6, respectively.
Off-line HTR
Conventional, non-interactive HTR experiments were performed on the three off-line
corpora described in section 0.7.1: ODEC-M3, IAMDB and CS. As commented in
section 0.5.3, HMMs with 6 states and a 64-Gaussian mixture density per state were
employed to model character morphology in all the cases. On the other hand, bi-gram
language models were trained from the respective training transcriptions of each cor-
pus. Table 5 shows the HTR WER(%) obtained for the different corpora. These results
correspond to GSF values optimized for each language model. The WER obtained for
IAMDB (25.8%) is comparable with non-interactive, state-of-the-art results published
for this data-set [45].
Table 5: Performance of the basic, non-interactive off-line HTR system for the three selected
off-line corpora.
ODEC-M3 IAMDB CS
WER (%) 25.0 25.8 34.1
On-line HTR
As discussed in section 0.7.1, the on-line handwritten words needed to assess the per-
formance of the on-line HTR feedback subsystem were generated using the UNIPEN
datasets 1a (digits), 1c (lowercase letters) and 1d (symbols).
All the samples were preprocessed using the preprocessing and feature extraction
methods described in sections 0.6.1 and 0.6.2. In order to tune the parameters of the 68
on-line character HMMs needed, experiments were carried out on each of the 1a, 1c
and 1d UNIPEN categories, partitioned into the training and a test sets shown in the
table 4. In this case 16 (diagonal) Gaussian densities were found to be optimal for the
25
HMM state mixtures. On the other hand, the state load factor (f ) was tuned through
isolated character classification experiments, with best results obtained for f = 10.
The classification error rates (ER(%)) obtained for digits, letters and symbols (using
the test characters belonging to the three previously selected writers) were 1.7%, 5.9%
and 21.8%, respectively. These results are comparable with the state-of-the-art results
obtained for this dataset [26, 21].
In order to establish a baseline for the on-line HTR feedback subsystem word de-
coding accuracy, a word recognition experiment was carried out. Since only single
words are to be recognized, a 1-gram language model was trained for each task. On
the other hand, as previously discussed, the words needed to test the HTR feedback
subsystem for each task were generated by concatenating the feature vector sequences
of the corresponding characters. Table 6 shows the basic statistics of the data used
in this experiment, along with the WER achieved by the non-interactive on-line HTR
subsystem in the different tasks, using GSF values optimized for each language model.
Table 6: Statisitics of the sets of on-line words generated to be used as feedback to correct
the off-line HTR errors corresponding to each of the three off-line HTR tasks, and baseline
performance of the basic on-line HTR subsystem for these sets, without using any interaction-
derived contextual information (i.e., using plain 1-grams).
Corpus #Wrds #Uniq-Wrds Lexicon WER(%)
ODEC-M3 786 389 3,308 5.3
IAMDB 778 522 9,938 3.6
CS 1,421 756 3,408 5.0
0.8.2 CATTI Results
The CATTI approach presented in section 0.3 was applied to the three off-line HTR
tasks considered in this work, using the same GSF values used for the baseline, non-
interactive results presented in section 0.8.1. Table 7 shows the estimated interactive
human effort (WSR) computed for each task, in comparison with the corresponding
estimated post-editing effort (WER from table 5). It also shows the estimated effort
reduction (EFR), computed as the relative diference between WER and WSR.
According to these results, to produce 100 words of a correct transcription in the
ODEC-M3 task, for example, a CATTI user shuld have to type only less than 20 words;
the remaining 80 are automatically predicted by CATTI. In contrast, a post-editor
Table 7: Performance of non-interactive off-line HTR (WER) and CATTI (WSR), along with
the relative difference between them (Estimated Effort-Reduction – EFR). All reported results
are percentages.
Corpus WER WSR EFR
ODEC-M3 25.0 19.4 22.4
IAMDB 25.8 21.8 15.5
CS 34.1 32.1 5.9
26
should have read all the (approximately) 100 words produced by the non-interactive
HTR system, find the 25 errors and type in the adequate word corrections.
On the other hand, when interactive transcription is compared with post-editing,
from every 100 potential (non-interactive) word errors, the CATTI user should have to
interactively correct only less than 78. The remaining 22 errors would be automatically
corrected by CATTI, thanks to the fedback information derived from the interactive
corrections.
It is interesting to realize that CATTI is more effective for lines or sentences that
have several errors; clearly, if a sentence has just one (word) error, it must be interac-
tively corrected by the user and the best CATTI can do is to keep the remaining text
unchanged. Obviously, this is not guaranteed by eq. (6) and, in the worst case, a single
word change made by the user may lead to more errors. To analize this behaviour,
figure 12 presents WER, WSR and EFR values for incresing initial numbers of errors
per sentence, for ODEC-M3 and IAMDB (similar tendences are observed for CS).
-10
 0
 10
 20
 30
 40
 50
 60
1 2 3 4 5 6 7 >7
Number of Errors per Phrase
ODEC-M3
 EFR
 WSR
 WER
-10
 0
 10
 20
 30
 40
 50
 60
1 2 3 4 5 6 7 >7
Number of Errors per Phrase
IAMDB
 EFR
 WSR
 WER
Figure 12: WER, WSR and EFR (all in %) for varying number of errors per sentence, for
ODEC-M3 (left) and IAMDB (right) corpora.
As expected, the estimated effort reduction increases with the number of errors per
sentence, which clearly assess the ability of CATTI to correct more than one error per
interaction step in sentences with several misrecognized words. Also, for sentences
with a single error, CATTI does not help at all or is even worse than post-edditing.
Therefore, in practice, a good implementation of a CATTI user interface should allow
the user to disable CATTI predictions when doing some (single-word) corrections.
Taking this in account, table 8 shows the same results of table 7, but excluding from
the computation all the sentences with zero and one errors. As expected, the estimated
effort reductions are better under this assumption.
0.8.3 MM-CATTI Results
The aim of these experiments is to assess the effectiveness of MM-CATTI in the sce-
narios described in section 0.4. Multimodal operation offers ergonomy and increased
27
Table 8: Performance of non-interactive off-line HTR (WER) and CATTI (WSR), along with the
relative difference between them (Estimated Effort-Reduction – EFR), excluding the sentences
with zero and one post-editing errors. All reported results are percentages.
Corpus WER WSR EFR
ODEC-M3 33.3 25.5 23.4
IAMDB 29.7 24.8 16.5
CS 40.9 38.2 6.5
usability at the expense of the system having to deal with non-deteriminstic feedback
signals. Therefore, the main concern here is the accuracy of the on-line HTR feedback
decoding and the experiments aim to determine how much this accuracy can be boosted
by taking into account information derived from the proper interaction process.
Table 9 presents the writer average feedback decoding error rates for the differ-
ent corpora considered and three language models which embody increasingly strong
interaction-derived constraints. The first one is the plain unigram estimate of P (d),
already reported in table 6 as a baseline. The second is an error-conditioned unigram
estimate of P (d | e) (eq. 14). The third model is a prefix-and-error conditioned bi-
gram estimate of P (d | p, e) (eq. 15). All these models are derived from the original
language models employed for the main, off-line HTR system, as explained in sec-
tion 0.4.1. In all the cases, the same GSF value (20) has been used. As observed in
table 9, feedback decoding accuracy increases significantly as more interaction-derived
constraints are taken into account.
Table 9: Writer average MM-CATTI feedback decoding error rates for the different corpora
and three language models: plain unigram (U, baseline), error-conditioned unigram (Ue) and
prefix-and-error conditioned bigram (Be). The relative accuracy improvement for Be is shown
in the last column. The same GSF value (20) is used in all the cases.
Corpus
Feedback ER (%) Rel. Improv. (%)
U Ue Be Be
ODEC-M3 5.3 4.9 2.9 44
IAMDB 3.6 3.4 2.5 30
CS 5.0 4.6 4.3 16
Individual recognition error rates of each of the three writers are plotted in fig. 13
for the different language models and corpora. It is interesting to note the poor results
obtained for writer “AR”, particularly for the Spanish tasks. An analysis of the results
reveals that most of the “AR” Spanish errors correspond to very frequent short words
that contain the character “a”, such as “a”, “la”, etc. A close look at the “AR” sam-
ples of this character shows that they are written in a particularly unusual style, which is
very poorly represented through the UNIPEN samples of the training partition. Clearly,
should this happen in a real situation, the on-line HMMs would obviously be re-trained
with samples of the user and the expected accuracy would became closer to that of writ-
28
ers “BH” and “BR”. In this case, the Be ER results of table 9 for ODEC-M3, IAMDB
and CS, respectively, would become: 1.6%, 1.9% and 1.3%, and the corresponding
relative improvements: 47%, 36% and 25%.
 0
 2
 4
 6
 8
 10
 12
U Ue Be U Ue Be U Ue Be
E
rr
or
 R
at
e 
(%
)
Language Models
ODEC-M3 IAMDB CS
WrAR
WrBH
WrBR
Figure 13: MM-CATTI feedback decoding error rates for different writers, corpora and prefix-
constrained language models. Writer “AR” writes some Spanish-frequent letters with a special
style not well represented through the training partition.
Table 10 summarizes all the CATTI and MM-CATTI results obtained in this work.
The third and forth columns, show the CATTI WSR decomposed into the percentage of
corrections successfully made through the on-line HTR feedback modality (the touch-
screen, TS) and those for which the feddback decoder failed and had to be entered by
means of the keyboard (KBD). These figures correspond to the three-writers averaged
decoding errors reported in table 9. The last two columns show the overall estimated
effort reductions (EFR) in the CATTI and MM-CATTI approaches. The MM-CATTI
EFR is calculated under the simplifying (but reasonable) assumption that the cost of
keyboard-correcting a feedback on-line decoding error is similar to that of another
on-line touchscreen interaction step. That is, each KBD correction is counted twice:
one for the failed touch-screen attempt and another for the keyboard correction itself.
Of course, if the unusual writer “AR” were not taken into account, much better MM-
CATTI EFR figures, very close to those of CATTI would be obtained.
0.9 Summary and Conclusions
In the interactive computer-assisted transcription of handwritten text images (CATTI),
the corrections made by a human transcriptor become part of a prefix of the final target
transcription. This prefix is used by CATTI to suggest a new suffix that the human
transcriptor can accept or modify in an iterative way until a satisfactory, correct target
transcription is finally produced. Empirical tests presented in this work clearly support
the benefits of using this approach rather than traditional HTR followed by human
post-editing.
29
Table 10: From left-to-right: post-editing corrections (WER), interactive corrections needed
(WSR), contributions of both modalities: on-line touch-sccreen input (TS) and keybord input
(KBD), and overall estimated effort reduction (EFR) achieved by the proposed approaches. All
reported results are percentages.
Corpus WER
CATTI MM-CATTI Overall EFR
WSR TS KBD CATTI MM-CATTI
ODEC 25.0 19.4 18.8 0.6 22.4 20.0
IAMDB 25.8 21.8 21.2 0.6 15.5 13.2
CS 34.1 32.1 30.7 1.4 5.9 1.8
We have also estudied the use of on-line touch-screen handwritten pen strokes as an
alternative means to input the required CATTI correction feedback. We call this multi-
modal approach “MM-CATTI”. From the results, we observe that the use of this more
ergonomic feedback modality comes at the cost of only a reasonably small number of
additional interaction steps needed to correct the eventual feedback decoding errors.
The number of these extra steps is keept very small thanks to the MM-CATTI ability to
use interaction-derived constraints to considerably improve the on-line HTR feddback
decoding accuracy.
The advantage of CATTI and MM-CATTI over traditional HTR followed by post-
editing goes beyond the good estimates of human effort reductions achieved by CATTI
or MM-CATTI. The proposed interactive approaches constitute a much more natural
way of producing correct text. So, when difficult transcription tasks with high WER
are considered, expert users generally refuse to post-edit conventional HTR output. In
contrast, with an adequate user interface, CATTI or MM-CATTI let the users be dy-
namically in command: if predictions are not good enough, then the user simply keeps
typing at her own pace; otherwise, she can accept (partial) predictions and thereby save
both thinking and typing effort.
Our future works in this direction aim at taking further advantage of interation
derived constraints. For instance, CATTI can automatically change its prediction as
soon as the user has pointed the place where the next error is found, thereby trying to
anticipate up-coming user corrections [29]. On the other hand, as mentioned in sec-
tion 0.4, the MM-CATTI fedback decoding accuracy can be further improved by using
other informations derived from the main off-line HTR process. In particular, using the
original text image being processed offers hope for significant improvement. Another
important CATTI/MM-CATTI oportunity to be considered is to take advantage of the
correct transcriptions that are being continuously produced during the interactive work
to improve the underlying models using Adaptive Learning techinques. Finally, our
plans for future work include to carry out adequate field tests to assess the validity of
our assumptions and estimations under real working conditions of expert (paleograhy)
transcribers of handwritten (historic) documents.
30
0.10 Acknowledgments
This work has been supported by the EC (FEDER), the Spanish MEC under grant
TIN2006-15694-C02-01 and by the research programme Consolider Ingenio 2010:
MIPRCV (CSD2007-00018).
31
Bibliography
[1] S. Barrachina, O. Bender, F. Casacuberta, J. Civera, E. Cubel, S. Khadivi, A. Lagarda H.
Ney, J. Tomás, and E. Vidal. Statistical approaches to computer-assisted translation. Com-
putational Linguistics, page In press, 2008.
[2] I. Bazzi, R. Schwartz, and J. Makhoul. An Omnifont Open-Vocabulary OCR System for
English and Arabic. IEEE Trans. on PAMI, 21(6):495–504, 1999.
[3] A. Brakensiek, J. Rottland, A. Kosmala, and G. Rigoll. Off-Line Handwriting Recognition
Using Various Hybrid Modeling Techniques and Character N-Grams. In 7th Int. Work-
shop on Frontiers in Handwriting Recognition (IWFHR), pages 343–352, Amsterdam, The
Netherlands, 2000.
[4] J. Civera, J. M. Vilar, E. Cubel, A. L. Lagarda, S. Barrachina, F. Casacuberta, E. Vidal,
D. Picó, and J. González. A syntactic pattern recognition approach to computer assisted
translation. In A. Fred, T. Caelli, A. Campilho, R. P.W. Duin, and D. de Ridder, editors,
Advances in Statistical, Structural and Syntactical Pattern Recognition, Lecture Notes in
Computer Science. Springer-Verlag, 2004.
[5] G. Dimauro, S. Impedovo, R. Modugno, and G. Pirlo. A new database for research on
bank-check processing. In 8th Int. Workshop on Frontiers in HandwritingRecognition,
pages 524–528, 2002.
[6] F. Drira. Towards restoring historic documents degraded over time. In DIAL’06: Proceed-
ings of the Second International Conference on Document Image Analysis for Libraries
(DIAL’06), pages 350–357, Washington, DC, USA, 2006. IEEE Computer Society.
[7] R. O. Duda and P. E. Hart. Pattern Classification and Scene Analysis. J. Wiley and Sons,
1973.
[8] I. Guyon, L. Schomaker, R. Plamondon, M. Liberman, and S. Janet. UNIPEN Project of
On-Line Data Exchange and Recognizer Benchmarks. In Proc. of the 14th International
Conference on Pattern Recognition, pages 29–33, Jerusalem (Isral), 1994.
[9] B. Q. Huang, Y. B. Zhang, and M. T. Kechadi. Preprocessing techniques for online hand-
writing recognition. In ISDA ’07: Proceedings of the Seventh International Conference
on Intelligent Systems Design and Applications, pages 793–800, Washington, DC, USA,
2007. IEEE Computer Society.
[10] S. Jaeger, S. Manke, J. Reichert, and A. Waibel. On-Line Handwriting Recognition:
The NPen++ Recognizer. International Journal on Document Analysis and Recognition,
3(3):169–181, 2001.
[11] F. Jelinek. Statistical Methods for Speech Recognition. MIT Press, 1998.
[12] S. Johansson, E. Atwell, R. Garside, and G. Leech. The Tagged LOB Corpus, User’s
Manual. Norwegian Computing Center for the Humanities, Bergen, Norway, 1996.
32
[13] S. M. Katz. Estimation of Probabilities from Sparse Data for the Language Model Com-
ponent of a Speech Recognizer. IEEE Trans. on Acoustics, Speech and Signal Processing,
ASSP-35:400–401, March 1987.
[14] E. Kavallieratou and E. Stamatatos. Improving the quality of degraded document images.
In DIAL ’06: Proceedings of the Second International Conference on Document Image
Analysis for Libraries (DIAL’06), pages 340–349, Washington, DC, USA, 2006. IEEE
Computer Society.
[15] R. Kneser and H. Ney. Improved backing-off for n-gram language modeling. International
Conference on Acoustics, Speech and Signal Processing (ICASSP), 1:181–184, 1995.
[16] P. Liu and F. K. Soong. Word graph based speech rcognition error correction by handwrit-
ing input. In ICMI ’06: Proceedings of the 8th international conference on Multimodal
interfaces, pages 339–346, New York, NY, USA, 2006. ACM.
[17] U. Marti and H. Bunke. A full English Sentence Database for Off-line Handwriting Recog-
nition. In 5th International Conference on Document Analysis and Recognition, pages
705–708, 1999.
[18] U. Marti and H. Bunke. The IAM-database: an English Sentence Database for Off-line
Handwriting Recognition. International Journal on Document Analysis and Recognition,
pages 39–46, 2002.
[19] U.-V. Marti and H. Bunke. Using a Statistical Language Model to improve the prefor-
mance of an HMM-Based Cursive Handwriting Recognition System. Int. Journal of Pat-
tern Recognition and Artificial In telligence, 15(1):65–90, 2001.
[20] L. O’Gorman and R. Kasturi, editors. Document image analysis. IEEE Computer Society
Press, Los Alamitos, CA, USA, 1995.
[21] M. Parizeau, A. Lemieux, and C. Gagn. Character Recognition Experiments using Unipen
Data. In Proceedings of the Sixth International Conference on Document Analysis and
Recognition, pages 481–485, 2001.
[22] M. Pastor, A. Toselli, and E. Vidal. Projection Profile Based Algorithm for Slant Removal.
In International Conference on Image Analysis and Recognition (ICIAR’04), Lecture Notes
in Computer Science, pages 183–190, Porto, Portugal, September 2004. Springer-Verlag.
[23] M. Pastor, A. H. Toselli, and E. Vidal. Writing Speed Normalization for On-Line Hand-
written Text Recognition. In Proc. of the Eighth International Conference on Document
Analysis and Recognition (ICDAR ’05), pages 1131–1135, Seoul, Korea, August 2005.
[24] R. Plamondon and S. N. Srihari. On-Line and Off-Line Handwriting Recognition: A Com-
prehensive Survey. IEEE Trans. on PAMI, 22(1):63–84, 2000.
[25] L. Rabiner. A Tutorial of Hidden Markov Models and Selected Application in Speech
Recognition. Proc. IEEE, 77:257–286, 1989.
[26] E. H. Ratzlaff. Methods, Report and Survey for the Comparison of Diverse Isolated Char-
acter Recognition Results on the UNIPEN Database. In Proc. of the Seventh International
Conference on Document Analysis and Recognition (ICDAR ’03), volume 1, pages 623–
628, Edinburgh, Scotland, August 2003.
[27] L. Rodrı́guez, F. Casacuberta, and E. Vidal. Computer Assisted Transcription of Speech.
In Proceedings of the 3rd Iberian Conference on Pattern Recognition and Image Analysis,
volume 4477 of LNCS, pages 241–248, Girona (Spain), June 2007.
[28] V. Romero, M. Pastor, A. H. Toselli, and E. Vidal. Criteria for handwritten off-line text size
normalization. In Procc. of The Sixth IASTED international Conference on Visualization,
Imaging, and Image Processing (VIIP 06), Palma de Mallorca, Spain, August 2006.
33
[29] V. Romero, A. H. Toselli, J. Civera, and E. Vidal. Improvements in the Computer Assisted
Transcription System of Handwritten Text Images. In Proc. of the 8th International Work-
shop on Pattern Recognition in Information Systems (PRIS’08), pages 103–112, Barcelona,
Spain, June 2008.
[30] V. Romero, A. H. Toselli, L. Rodrı́guez, and E. Vidal. Computer assisted transcription
for ancient text images. In International Conference on Image Analysis and Recogni-
tion (ICIAR 2007), volume 4633 of LNCS, pages 1182–1193. Springer-Verlag, Montreal
(Canada), August 2007.
[31] S. N. Srihari and E. J. Keubert. Integration of Handwritten Address Interpretation Tech-
nology into the United States Postal Service Remote Computer Reader System. In Fourth
International Conf. Document Analysis and Recognition, volume 2, pages 892–896, Ulm,
Germany, August 1997.
[32] B. Suhm, B. Myers, and A. Waibel. Multimodal Error Correction for Speech User Inter-
faces. ACM Transactions on Computer-Human Interaction, 8(1):60–98, March 2001.
[33] A. H. Toselli. Reconocimiento de Texto Manuscrito Continuo. PhD thesis, Departamento
de Sistemas Informáticos y Computación. Universidad Politécnica de Valencia, Valencia
(Spain), March 2004. Advisor(s): Dr. E. Vidal and Dr. A. Juan (in Spanish).
[34] A. H. Toselli, A. Juan, D. Keysers, J. Gonzlez, I. Salvador, H. Ney, E. Vidal, and F. Casacu-
berta. Integrated Handwriting Recognition and Interpretation using Finite-State Models.
Int. Journal of Pattern Recognition and Artificial Intelligence, 18(4):519–539, June 2004.
[35] A. H. Toselli, A. Juan, and E. Vidal. Spontaneous Handwriting Recognition and Clas-
sification. In Proceedings of the 17th International Conference on Pattern Recognition,
volume 1, pages 433–436, Cambridge, United Kingdom, August 2004.
[36] A. H. Toselli, V. Romero, L. Rodrı́guez, and E. Vidal. Computer Assisted Transcription
of Handwritten Text. In 9th International Conference on Document Analysis and Recog-
nition (ICDAR 2007), pages 944–948. IEEE Computer Society, Curitiba, Paraná (Brazil),
September 2007.
[37] A. H. Toselli, V. Romero, and E. Vidal. Computer assisted transcription of text images and
multimodal interaction. In Proceedings of the 5th Joint Workshop on Multimodal Interac-
tion and Related Machine Learning Algorithms, volume 5237 of LNCS, pages 296–308.
Utrecht, The Netherlands, September 2008.
[38] A. H. Tosellli, M. Pastor, and E. Vidal. On-Line Handwriting Recognition System for
Tamil Handwritten Characters. In 3rd Iberian Conference on Pattern Recognition and
Image Analysis, volume 4477 of Lecture Notes in Computer Science (LNCS), pages 370–
377. Springer-Verlag, Girona (Spain), June 2007.
[39] E. Vidal, F. Casacuberta, L. Rodrı́guez, J. Civera, and C. Martı́nez. Computer-assisted
translation using speech recognition. IEEE Transaction on Audio, Speech and Language
Processing, 14(3):941–951, 2006.
[40] E. Vidal, L. Rodrı́guez, F. Casacuberta, and I. Garcı́a-Varea. Interactive pattern recognition.
In Proceedings of the 4th Joint Workshop on Multimodal Interaction and Related Machine
Learning Algorithms, volume 4892 of LNCS, pages 60–71. Brno, Czech Republic, June
2007.
[41] E. Vidal, F. Thollard, F. Casacuberta C. de la Higuera, and R. Carrasco. Probabilistic finite-
state machines - part II. IEEE Transactions on Pattern Analysis and Machine Intelligence,
27(7):1025–1039, 2005.
[42] A. Vinciarelli, S. Bengio, and H. Bunke. Offline Recognition of Unconstrained Handwrit-
ten Texts Using HMMs and Statistical Language Models. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 26(6):709–720, june 2004.
34
[43] V. Vuori, J. Laaksonen, E. Oja, and J. Kangas. Speeding Up On-line Recognition of Hand-
written Characters by Pruning the Prototype Set. In Proc. of the Sixth International Con-
ference on Document Analysis and Recognition (ICDAR ’01), pages 0501–0507, Seattle,
Washington, September 2001.
[44] S. Young, J. Odell, D. Ollason, V. Valtchev, and P. Woodland. The HTK Book: Hidden
Markov Models Toolkit V2.1. Cambridge Research Laboratory Ltd, March 1997.
[45] M. Zimmermann, J.-C. Chappelier, and H. Bunke. Offline grammar-based recognition of
handwritten sentences,. IEEE Transactions on Pattern Analysis and Machine Intelligence,
28(5):818–821, May 2006.
35
