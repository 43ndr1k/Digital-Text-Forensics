Stylistics in Customer Reviews of Cultural Objects 
Xiao Hu 
Graduate School of Library and Information Science 
University of Ilinois at Urbana-Champaign 
xiaohu@uiuc.edu 
J. Stephen Downie 
Graduate School of Library and Information Science 
University of Ilinois at Urbana-Champaign 
jdownie@uiuc.edu 
 
1. INTRODUCTION 
Online customer reviews of cultural objects (e.g., books, movies, 
music, etc.) are written by users themselves. These user-generated 
reviews thus provide an important resource for researchers who 
are interested in creating better access mechanisms for users as 
they seek out and consume cultural materials. Furthermore, 
customer reviews at retail websites (e.g., amazon.com) and 
community review websites (e.g., epinions.com) are often well 
organized and thus provide researchers with valuable “ground 
truth” data with regard to such things as genre labels, artist names, 
quality ratings, etc. A deep level understanding of how users 
express themselves with regard to the use of cultural objects can 
contribute enormously to the construction of more useful and 
powerful automated retrieval and recommendation tools. 
This overview paper describes a series of text mining and 
stylistics experiments conducted over the last year. The interested 
reader is guided toward to [2,5,6] where we have previously 
published various sub-sections of these experiments. Our work 
has focused upon one particularly data-rich collection of user-
generated reviews of music, books and movies, namely 
epinions.com. At epinions.com, each review is associated with 
both a genre label and a numerical quality rating expressed as a 
number of stars (from 1 to 5) with higher ratings indicating more 
positive opinions. Also in each music review, there is a field 
called “Great Music to Play While:” where the reviewer provides 
a usage suggestion for the reviewed piece. In this work, the 
dataset contains reviews selected and downloaded from the most 
popular genres represented on epinions.com. The distribution of 
reviews across genres and ratings was made as evenly as possible 
to eliminate analytic bias. Table 1 illustrates the movie, book and 
music genre taxonomies used in our experiments.  
The following questions comprise the motivational framework for 
our analyses:  
1. (a) Can we determine from the text what genre a review is 
talking about? (b) Are there distinctive features that contribute 
to the “style/language” used to talk about the genres of cultural 
objects under consideration? 
2. (a) Can we determine from the review text the quality rating a 
user assigned to the object? (b) Are there distinctive features 
that contribute to the “style/language” used to talk about 
assigned quality ratings?  
3.  (a) Can we determine from the review text what kind of usage 
the reviewer suggests for a particular piece of music? (b) Are 
there distinctive features that contribute to the “style/language” 
associated with each usage?  
For three questions above, we approached the (a) part of each 
question as a text categorization problem and applied the well-
known multinomial Naïve Bayesian (NB) text categorization 
model [10] to classify the reviews according to the specific set of 
classes associated with genre, quality rating and usage. For 
question 3 (a) we also used agglomerative hierarchical clustering 
to assist us in modeling the usage data into superclasses. For each 
of the (b) parts we employed two feature extraction methods: (1) 
frequent pattern mining (FPM) [4] for extracting frequently 
concurrent terms within sentences; and, (2) Naïve Bayesian 
feature ranking (NBFR) [11] for extracting the most 
distinguishing unigram terms for each category. 
Table 1. Book, movie and music genres used in experiments 
 
The same data preprocessing and modeling techniques were 
applied to all experiments. HTML tags were removed and the 
documents tokenized. If not specified otherwise, stop words and 
punctuation marks were not stripped as previous studies suggest 
these provide useful stylistic information [1,8]. Tokens were 
stemmed to lemmas. Documents were represented as vectors 
where each attribute value was the frequency of occurrence of a 
distinct term (i.e., unigram/multinomial model). The experiments 
were implemented in the D2K/T2K (Data/Text-to-Knowledge) 
framework which facilitates the fast prototyping of the text mining 
and stylistic analysis experiments [3,9]. 
2. EXPERIMENTS ON OBJECT GENRES 
Table 2 provides an overview of the genre classification tests on 
the book, movie and music reviews (5250 reviews in total). The 
genres involved are shown in Table 1. The overall precisions 
 
Permission to make digital or hard copies of all or part of this work for 
personal or classroom use is granted without fee provided that copies are 
not made or distributed for profit or commercial advantage and that 
copies bear this notice and the full citation on the first page. To copy 
otherwise, or republish, to post on servers or to redistribute to lists, 
requires prior specific permission and/or a fee. 
SIGIR’06, Workshop on Computational Analysis of Stylistics in Text 
Retrieval, August 10, 2006, Seattle, WA, U.S.A. 
Copyright 2006 ACM 1-58113-000-0/00/0004…$5.00. 
Books Movies Music 
Action & Thrillers Action/Adventure Blues 
Juvenile Fiction Children Classical 
Humor Comedies Country 
Horror Horror/Suspense Electronic 
Music & Performing Arts Musical & Performing Arts Gospel 
Science Fiction & Fantasy Science-Fiction/Fantasy Hardcore/Punk 
Mystery & Crime Documentary Heavy Metal 
Dramas International Biography & 
Autobiography Education Jazz Instrument 
Romance Japanimation Pop Vocal 
 War R&B 
  Rock & Pop 
generated by the NB classifier are impressively high (67.70% to 
78.89%) when compared to the baseline of random selection 
(8.33% to 11.11%). 
An analysis of the confusion matrices of the above experiments 
was conducted to tell us which genres are more distinguishable 
from the others and which genres are more prone to 
misclassification. Experimental results [5] show the identification 
of some genres is very reliable, such as “Music & Performing 
Arts” book reviews (89% accuracy) and “Children” movie 
reviews (95% accuracy). Other results indicate interesting 
confusion patterns, such as “Education” and “Documentary” 
movie reviews (31% confusion). High confusion values appear to 
indicate that such genres semantically overlap. Furthermore, such 
confusion values may also indicate pairs of genres that create 
similar impressions and impacts on users. For example, there 
might be a formal distinction between the “Education” and 
“Documentary” genres but the two genres appear to affect 
significant numbers of users in similar, interchangeable ways. 
Other apparent confusions include “Drama” and “Comedy” 
movies reviews (17% confusion), “Mystery” and “Action” book 
reviews (20% confusion), as well as “Blues” and “Rock” music 
reviews (29% confusion). 
Table 2: Genre classification experiments 
 Book Movie Music 
Number of genres 9 11 12 
Reviews in each genre  200 150 150 
Term list size (terms) 41,060  47,015  47,864  
Mean of review length (words) 1,095  1,514  1,547  
Std Dev of review length (words) 446  672  784  
Mean of precision * 72.18% 67.70% 78.89% 
Std Dev of precision 1.89% 3.51% 4.11% 
*: 5- fold random cross-validation on book and movie reviews, 3- fold 
random cross-validation on music reviews 
 
3. QUALITY RATING EXPERIMENTS  
Each review in epinions.com is associated with a quality rating 
assigned to the reviewed item by the reviewer. The ratings are 
represented as number of stars (from 1 to 5 stars with more stars 
indicating a more positive rating). The ratings given by the 
reviewers provide a ground truth of sentimental tendency (i.e. like 
or dislike), which can be used to analyze the stylistic features in 
the review text expressing feelings and opinions. In our 
experiments, we first tested the classification of reviews according 
to quality rating as a five-class problem (i.e., classification classes 
representing the individual ratings (1, 2, 3, 4 and 5 stars)). Next 
we conducted two binary classification experiments: (1) negative 
and positive review “group” identification (i.e., 1 or 2 stars versus 
4 or 5 stars); and (2) ad extremis identification (i.e., 1 star versus 
5 stars) [5]. Table 3 presents the classification results.  
The results across all media show the same pattern: the 
classification precision scores for the binary rating tasks are quite 
strong (79.75% to 85.94%). The five-class scores are substantially 
weaker (36.70% to 44.82%). However, upon examination of the 
confusion matrix of the music review five-class categorization 
(Table 4), it is apparent that the system is “reasonably” confusing 
adjacent categories (e.g., 1 star with 2 stars, 4 stars with 5 stars, 
etc.). Confusion matrices of book and movie review also show the 
same pattern [5]. In Section5, we present an analysis of the 
features that contributed to the success of the music review binary 
rating classification experiment. 
Table 3: Rating classification results 
Experiment  
(number of reviews per class) 
Mean of 
Accuracy 
Std Dev of 
Accuracy 
Book Reviews (5- fold random cross-validation) 
1 star … 5 stars  (200 ) 36.70% 1.15% 
1, 2 stars vs. 4, 5 stars (400) 80.13% 4.01% 
1 star vs. 5 stars (300) 80.67% 2.16% 
Movie   Reviews  (5- fold random cross-validation) 
1 star … 5 stars (220) 44.82% 2.27% 
1, 2 stars vs. 4, 5 stars (440) 82.27% 2.02% 
1 star vs. 5 stars (400) 85.75% 1.20% 
Music   Reviews  (5-fold cross-validation) 
1 star … 5 stars (200) 44.25% 2.63% 
1, 2 stars vs. 4, 5 stars (400) 79.75% 3.59% 
1 star vs. 5 stars (400) 85.94% 3.58% 
 
Table 4: Five-class music review confusion matrix* 
Music reviews 1 star 2 stars 3 stars 4 stars 5 stars 
1 star 0.61 0.24 0.07 0.05 0.02 
2 stars 0.24 0.15 0.36 0.15 0.09 
3 stars 0.11 0.13 0.41 0.20 0.15 
4 stars 0.03 0.06 0.10 0.32 0.48 
5 stars 0 0 0.09 0.11 0.80 
*: The first row represents prediction, and the first column 
represents ground truth. 
4. USAGE EXPERIMENTS 
For reviews in the music domain, epinions.com provides a ready-
made list of thirteen recommended usages prepared by the editors. 
Each review can be associated with at most one recommended 
usage indicated in a field at the end of the review called “Great 
Music to Play While:”. A reviewer can simply choose one of the 
recommended usages as his/her suggestion on when to use the 
music. Our analyses in this section focus on the eleven usage 
categories shown in Figure 1. Two original usage categories were 
ignored for there were not enough reviews recommending them to 
guarantee meaningful analyses. 
An initial NB text categorization experiment on the 11 usage 
categories was run with each category represented by 180 review 
documents i.e., 1,980 reviews were examined (Experiment A). As 
shown in Table 5, the average accuracy of the NB text 
categorization on the original 11 classes is 19.55%. Even though 
the accuracy is better than random guessing (1/11 = 9%), it still 
indicates that predicting suggested usages using a unigram NB 
model is not as straightforward as our earlier NB experiments on 
genre or quality rating. To achieve better results, more features 
(e.g., part of speech) and wider contextual information (e.g., bi-
gram, tri-gram, etc., models) may need to be considered. This is 
part of our on-going work.  
Notwithstanding the disappointing initial NB classification 
results, the associated confusion matrix suggested to us that some 
of the usages are closely related with each other. For example, 
“Going to sleep” and “Listening” both indicate quiet, relaxing 
occasions and have an 11.1% confusion rate. We therefore 
hypothesized that the generation of superclasses that amalgamated 
the individual usage classes into broader groupings would provide 
a more meaningful model of the data. To avoid biases in manually 
grouping the data, we generated superclasses by applying 
agglomerative hierarchical clustering to the confusion matrix 
resulting from Experiment A. Hierarchical clustering has long 
been applied to analyzing confusion matrices in social sciences 
because frequent confusions between categories can be seen as a 
measure of similarity [7]. Using the complete linkage algorithm, 
we obtain the clusters as shown in Figure 1.  
 
From Figure 1 we see two major groups which can be generally 
named “Relaxing” (involving little tension) and “Stimulating” 
(going to be active):  
Relaxing: “Going to Sleep”, “Listening”, “Reading or Studying”, 
“Romancing” and “Cleaning the House”; 
Stimulating: “At Work”, “Hanging with Friends”, “Getting 
Ready to Go Out”, “Driving”, “Waking Up” and “Exercising”.  
 
If we cut off the clusters at distance level of about 0.85 (see 
Figure 1) we can see four sub-groups. We have named these sub-
groups “Relaxing 1” and “Relaxing 2”, “Stimulating 1” and 
“Stimulating 2”. 
Based on these groupings of similar classes, we set up two more 
NB text categorization experiments with the purpose of seeing 
how much improvement the groupings provide in predicting 
usage from the review texts.  
Experiment B: 2-class categorization: Relaxing vs. Stimulating; 
Experiment C: 4-class categorization: Relaxing 1 vs. Relaxing 2 
vs. Stimulating 1 vs. Stimulating 2. 
Descriptive statistics and the results of these experiments are 
reported in Table 5. The superclass classification experiments ran 
at rates much better than the baseline, providing evidence that the 
superclasses identified in the aforementioned clustering 
experiment are not arbitrary and the unigrams in our review data 
can predict suggested usages at the superclass level. 
 
Table 5: Data statistics and results of experiments on usages 
Experiment A B C 
Number of classes 11 2 (R, S)* 4 (R1,R2,S1,S2)* 
Reviews in a class 180 900 360 
Term list size (terms) 36,561 34,759 30,637 
Mean of review length (words) 838.75 839.03 825.45 
Std. Dev. of review length (words) 511.39 509.96 514.38 
Mean. accuracy** 19.55% 65.72% 42.60% 
Std. Dev. of accuracy 2.89% 3.15% 4.60% 
*: R: Relaxing; S: Stimulating; **: from 10 fold cross-validation 
 
5. FEATURE STUDIES 
Feature studies are essential for stylistic analysis. Derived from 
the aforementioned classification experiments, a fundamental 
question is what makes the various classes distinguishable, i.e., 
what features played influential roles in the categorization 
experiments and what were their relative importance. This section 
presents the results from the two feature analysis techniques we 
applied after running each of our NB classification experiments: 
FPM and NBFR. Because we are primarily interested in the music 
information retrieval implications of this line of research, we 
concentrated our analytic effort on the music review data. 
5.1 Frequent term pattern mining 
Frequent pattern mining (FPM) finds patterns consisting of items 
that frequently occur together in individual transactions [4]. It is 
traditionally applied to help discover such association rules as “if 
a customer bought product A, he/she will probably buy product B 
as well”. Since we have been using unigram text models in our 
experiments, an item here is a unigram term. The transactions in 
our case are the single sentences in the music review texts. We 
chose sentence-level transactions because sentences better 
contextualize the usage of concurrent terms than their usage at a 
broader document level. Prior to running the FPM analyses the 
texts were subjected to part-of-speech (POS) tagging and filtering. 
The D2K/T2K toolkit was again used for the tagging, filtering and 
FPM analyses. 
5.1.1 Noun patterns in reviews on different genres 
Section 2 of this paper showed that reviews of music in various 
genres can be predicted from the review text with fairly high 
accuracy. Now we dig into a finer level of analysis to investigate 
the reasons for the success of the classification tests. Specifically, 
we used the FPM technique to discover whether distinguishing 
patterns in music reviews regarding different genres could be 
found. To illustrate the point in a concise manner, we chose four 
popular genres from the total 12 genres used in Section 2: 
Classical, Country, Heavy Metal and Jazz Instrument. For this 
task, we only examined nouns as our preliminary NBFR (section 
5.2) experiments consistently ranked nouns the highest. Upon 
examining the most frequent patterns consisting of single terms, 
we found the same top-ranked terms across all the four genres. 
These terms were very general terms in the music domain such as 
“music” “song”, “album”, “track”, etc. This observation prompted 
us to consider deeper contexts, and thus we moved our analytic 
focus to double-term patterns. A pattern with multiple terms 
means these terms are frequently concurrent within individual 
sentences. Table 6 lists the most frequent double-term patterns in 
the four genres.  
Going to sleep 
Listening  
Reading or studying 
Romancing 
Cleaning the house 
At work 
Hanging out with friends 
Getting ready to go out 
Driving  
Waking up 
Exercising 
Figure 1: Hierarchical clustering on 11 usage classes  
Table 6:  Most frequent double-term patterns in four genres 
Classical Country Heavy Metal Jazz Instrument 
cd music 
music piec 
piec piano 
piano concerto 
orchestra symphoni 
music record 
piano op 
music work 
music time 
music compos 
violin concerto 
cd piec 
cd record 
twain shania 
dixi chick 
station union 
guitar steel 
tim mcgraw 
cash johnni 
titl track 
song titl 
krauss alison 
drum guitar 
countri radio 
song beat 
style song 
album song 
song guitar 
riff guitar 
guitar bass 
drum guitar 
song lyric 
song riff 
song choru 
solo guitar 
song track 
album track 
band album 
band song 
music jazz 
liner note 
drum bass 
jazz album 
album song 
jazz song 
guitar bass 
tenor sax 
solo song 
piano bass 
mile davi 
solo piano 
section rhythm 
Although there are still general words embedded in these patterns, 
there are interesting patterns in these double-term lists. Note the 
terms are stemmed and the order of the terms in each pattern is 
arbitrary. These patterns include instruments closely associated 
with each genre: piano and violin for Classical; steel guitar for 
Country music; bass guitar for Heavy Metal; and tenor sax for 
Jazz. Representative artist names are high up in the Country music 
list. The notion of “riff” appears in the Heavy Metal list but no 
other lists. In our future work, we will mine frequent patterns in 
adjectives, adverbs and verbs to see how customers express their 
feelings about different genres.  
5.1.2 Positive and negative descriptive patterns  
Based on the good results of our quality rating classification 
experiments (Section 3), we wanted next to investigate the 
influential features in the review text expressing strong positive 
and strong negative opinions. Therefore, we only considered the 
reviews examined on our ad extremis quality rating experiments 
(i.e., 5 star vs. 1 star). The dataset statistics is presented in Table 
7. As nouns are usually neutral in terms of semantic orientation 
(e.g., positive or negative), we used adjectives, adverbs and verbs 
as candidate descriptive terms. We used the same POS tagging, 
filtering and FPM techniques described earlier however stop 
words (except for negatives) were removed. 
Table 7: Statistics of positive and negative music reviews 
Reviews Positive  Negative  
Total reviews 400 400 
Total sentences 63118 30053 
Total words 1027713 447603 
Avg. (STD ) sentences per review 157.80 (75.49) 75.13 (41.62) 
Avg. (STD) words per sentence 16.28 (14.43) 14.89 (12.24) 
Again, we started by examining the most frequent single-term 
patterns, and found the single-term patterns in positive and 
negative reviews to be surprisingly similar. As shown in Table 8, 
the top two single-term patterns are the same for both positive and 
negative reviews. It is interesting to see that “good”, a 
semantically positive word is on the top of the list of negative 
reviews! And about one-third of all sentences in negative reviews 
contain the word “good” which only occurs in one-fourth of the 
positive review sentences. To understand these counterintuitive 
results, we manually analyzed the contexts of “good” in negative 
reviews and found that there are roughly five categories of 
“good” used in a negative context:  
1. Negation:   e.g. "Nothing is good."  
2. Song titles:  e.g. "Good Charlotte, you make me so mad." 
3. Rhetoric:  e.g. " And this is a good ruiner: … " 
4. Faint praise: e.g. "the only good thing… is the packaging.”  
5. Expressions: e.g. "You all have heard … the good old cliché." 
This observation indicates that looking at single terms separately 
is not sufficient to tell the difference in expressing positive and 
negative opinions. Therefore, we dug into deeper contexts to mine 
double-term and triple-term frequent patterns. Table 8 presents the 
top-ranked extracted patterns. It is apparent that the double-term 
patterns in both lists are still quite similar: 3 out of 5 most 
frequent patterns are the same for positive and negative reviews. 
However, obvious differences finally show up in the comparison 
of the triple-term patterns: 9 out of 10 most frequent patterns in 
negative reviews contain negatives (“not”, “don’t”), while only 1 
pattern in the positive reviews contains a negative. Furthermore, 9 
of the most frequent triple-term patterns in the positive reviews 
describe “pleasant” music experiences with 7 of these focusing on 
things “melodic.” Thus, it is only at the triple-term level that we 
are able to tell the differences in sentimental tendency 
expressions, and thus to have a meaningful explication of the high 
classification accuracy in distinguishing positive and negative 
reviews described in Section 3. 
Table 8: Most frequent patterns ranked by frequency 
LEVEL Positive reviews Negative reviews 
Single term not (3417 sentences) 
good (1621 sentences)  
not (1915 sentences) 
good (1025 sentences) 
Double terms 
not good  
not realli 
realli good  
not listen  
not great 
not good  
not bad  
not realli  
not sound  
realli good  
Triple terms 
sing open melod 
sing smooth melod 
sing fill melod 
sing smooth open 
not realli good 
sing lead melod 
sound realli good 
sing plai melod 
accompani sing melod 
sing soft melod 
not realli good 
not realli listen  
bad not good  
bad not sound  
pretti tight spit 
bad not don’t 
realli not don’t 
realli bad not 
pretti bad not 
not sing sound 
 
5.2 Naïve Bayesian feature ranking 
Our second method of feature analysis, NBFR, is based on the 
Naïve Bayesian text categorization model. In the case of binary 
classification, NB text classifiers can be used to rank the features 
according to their relative importance in the construction of the 
categorization model [11].  
The NB model predicts the class label of a testing document di, by 
comparing the value of the following equation to 0.  
)/(
)/(
log),(
)(
)(
log
)/(
)/(
log
1 jt
jt
V
t
it
j
j
ij
ij
CwP
CwP
dwc
CP
CP
dCP
dCP
¬
+
¬
=
¬
∑
=
(1)  
where w1 ,w2 ,...,wn are terms occurring in document di, V is the 
vocabulary of terms occurring in the training document set. If the 
value of (1) is greater than 0, document di is predicted as in class 
Cj, otherwise it is predicted as not in Cj. 
From equation (1), we can see how each term in document di 
contributes to the class prediction. The first item in the right side 
of (4) does not depend on terms in any documents. And c (wt ,di), 
the frequency count of a term wt in a testing document di, is not 
part of the NB model parameters (as the model is built on training 
data), so the NB model ranks important features in each category 
according to the value of the log ratio in the summation in (1).  
5.2.1 Important features in usage superclasses 
The usage experiments presented in Section 4 showed that the 
recommended usages of music can be grouped into superclasses, 
and these superclasses are distinguishable by a unigram NB 
model. It is informative to examine the unigram term features 
used in the usage categorization experiments. Particularly, we 
were interested to find those features that actually went into the 
modeling of each of the 2 superclasses discovered in the cluster 
analysis. Therefore, we opened the black box of the NB 
classification model to see how the model ranks the most 
important features in each of the superclasses in Experiment B in 
Section 4.  
Table 9 shows that the top-ranked terms in each usage 
superclasses discovered by the NBFR technique are mostly artist 
names, indicating users tend to have similar opinions in relating 
artists to certain usages.  
The artist-usage relation has been verified by applying a binomial 
exact test [12] on those artists that were represented by at least 10 
reviews in our dataset. Table 10 presents the artist-usage pairs 
whose relations tested to be significant at p < 0.05. For example, 
Black Sabbath’s music is recommended most often for use “At 
Work”. Similarly, a significant plurality of reviewers 
recommended listening to Nirvana when “Going to Sleep”. 
Table 9: Top ranked terms in usage superclasses 
Relaxing Stimulating 
Botti (Chris) 
Shelby (Lynne) 
Bethany (Joy) 
Debelah (Morgan) 
Mckennitt (Loreena) 
Pontiy(Jean Luc) 
Shabazz (lyricist) 
Tru 
nightwish 
Tarja (Turunen) 
Dio (Ronnie James) 
Roca (Zach De La Roca) 
Slade (British band) 
Incubus (band) 
Edan (rap artist) 
Twiztid (band) 
KJ (KJ52) 
blue 
Serj (Tankian) 
Stooges (The) 
Terms in ()’s added for clarity and were taken from source reviews. 
 
Table 10: Significant relations between artists and usages 
Artist Usage p value 
AFI Waking Up 0.03252 
Black Sabbath At Work 0.00028 
Celine Dion Romancing 0.02499 
Dream Theater Listening 0.01862 
Metallica Waking Up 0.03252 
Nirvana_(USA) Going to Sleep 0.01862 
 
6. DEMO: A D2K/T2K ITINERARY  
Our experiments are implemented using the D2K/T2K framework 
which contains many ready-to-use data/text mining modules and 
itineraries (Figure 2). It also integrates a few powerful and well-
known NLP tools such as General Architecture for Text 
Engineering (GATE), Brill taggers, and various parsers. Because 
of its modular approach, D2K/T2K allows for the fast prototyping 
of text mining and stylistic experiments. We will demonstrate in 
D2K/T2K one of the above experiments on a small (for time’s 
sake) exemplary dataset.  
 
Figure 2: An example itinerary in D2K/T2K framework 
7. CONCLUSIONS 
This paper presents a series of text analyses of user-generated 
reviews of cultural objects. Simple NB models were able to 
distinguish the reviews with regard to the genres, quality ratings 
and recommended usages of the reviewed objects. Meaningful 
features were identified using FPM and NBFR techniques. Our 
findings can be used to help future researchers connect users’ 
opinions to cultural objects and thus facilitate information access 
via novel, user-oriented facets. 
8. ACKNOWLEDGMENTS 
This work is supported by the National Science Foundation under 
Grant No. NSF IIS-0327371 and the A. W. Mellon Foundation. 
9. REFERENCES 
[1] Argamon, S., and Levitan, S. Measuring the isefulness of 
function words for authorship attribution.In Proceedings of 
the 17th Joint International Conference of ACH/ALLC. 2005 
[2] Downie, J. S. and Hu, X. Review mining for digital libraries: 
Phase II. In Proceedings of the 6th ACM/IEEE Joint 
Conference on Digital Libraries (JCDL) 2006 [In press] 
[3] Downie, J. S., Unsworth, J., Yu, B., Tcheng, D., Rockwell, 
G., and Ramsay, S. J. A revolutionary approach to 
humanities computing: tools development and the D2K data-
mining framework. In Proceedings of the 17th Joint 
International Conference of ACH/ALLC. 2005 
[4] Han, J., Pei, J., and Yin, Y. Mining frequent patterns without 
candidate generation. In Proceedings of the ACM SIGMOD 
2000 
[5] Hu, X., Downie, J. S. and Jones, M. C. Criticism mining: text 
mining experiments on book, movie and music reviews. In 
Proceedings of the Digital Humanities 2006, [In press].  
[6] Hu, X., Downie J.S., West K., and Ehmann A. Mining music 
reviews: promising preliminary results. In Proceedings of the 
6th International Symposium on Music Information Retrieval 
(ISMIR). 2005 
[7] Levine, D. Multivariate analysis of the visual information 
processing of numbers, Multivariate Behavioral Research 
vol. 12  no. 3, 1977 
[8] Stamatatos, E., Fakotakis, N., and Kokkinakis, G. Text genre 
detection using common word frequencies. In Proceedings of 
18th International Conference on Computational 
Linguistics. 2000 
[9] Welge, M., et al. Data to knowledge (D2K): an automated 
learning group report. NCSA, University of Illinois at 
Urbana-Champaign, 2003. (http://alg.ncsa.uiuc.edu) 
[10] Yang, Y. and Liu, X. A re-evaluation of text categorization 
methods. In Proceedings of the twenty-second Annual ACM 
Conference on Research and Development in Information 
Retrieval (SIGIR), 1999 
[11] Yu, B. An evaluation of text classification methods for 
literary study.” Dissertation Proposal, GSLIS, UIUC, 2006, 
Available: http://www.noraproject.org/beiyu-proposal-Jan04-
2006.pdf 
[12] M. Buntinas, and G. M. Funk. Statistics for the Sciences, 
Brooks/Cole/Duxbury, 2005.   
 
