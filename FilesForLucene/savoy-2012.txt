This article was downloaded by: [University of Aegean]
On: 13 April 2012, At: 08:02
Publisher: Routledge
Informa Ltd Registered in England and Wales Registered Number: 1072954
Registered office: Mortimer House, 37-41 Mortimer Street, London W1T 3JH, UK
Journal of Quantitative Linguistics
Publication details, including instructions for authors and
subscription information:
http://www.tandfonline.com/loi/njql20
Authorship Attribution: A
Comparative Study of Three Text
Corpora and Three Languages
Jacques Savoy a
a Computer Science Department, University of
Neuchatel, Switzerland
Available online: 13 Mar 2012
To cite this article: Jacques Savoy (2012): Authorship Attribution: A Comparative Study
of Three Text Corpora and Three Languages, Journal of Quantitative Linguistics, 19:2,
132-161
To link to this article:  http://dx.doi.org/10.1080/09296174.2012.659003
PLEASE SCROLL DOWN FOR ARTICLE
Full terms and conditions of use: http://www.tandfonline.com/page/terms-and-
conditions
This article may be used for research, teaching, and private study purposes.
Any substantial or systematic reproduction, redistribution, reselling, loan, sub-
licensing, systematic supply, or distribution in any form to anyone is expressly
forbidden.
The publisher does not give any warranty express or implied or make any
representation that the contents will be complete or accurate or up to date. The
accuracy of any instructions, formulae, and drug doses should be independently
verified with primary sources. The publisher shall not be liable for any loss,
actions, claims, proceedings, demand, or costs or damages whatsoever or
howsoever caused arising directly or indirectly in connection with or arising out
of the use of this material.
Authorship Attribution: A Comparative Study of Three
Text Corpora and Three Languages*
Jacques Savoy
Computer Science Department, University of Neuchatel, Switzerland
ABSTRACT
The first objective of this paper is carry out three experiments intended to evaluate
authorship attribution methods based on three test-collections available in three different
languages (English, French, and German). In the first we represent and categorize 52 text
excerpts written by nine authors and taken from 19th century English novels. In the
second we work with 44 segments from French novels written by eleven authors, mostly
from the 19th century. In the third we extract 59 German text excerpts from novels
published mainly during the 19th and the beginning of the 20th century, written by 15
authors. The second objective is to analyse performance differences obtained when using
word types or lemmas as text representations, and the third objective is to evaluate three
authorship attribution schemes, the first of which uses principal component analysis
(PCA), the second applies the Delta approach, and the third corresponds to a new
authorship attribution method based on specific vocabulary. This concept is computed
for a given text (or author profile) and then compared with the entire corpus. Based on
this information, we show how a distance measure can be derived and by means of the
nearest neighbor approach we suggest a simple and efficient authorship attribution
scheme. Based on three test collections and using either word types or lemmas as features,
we demonstrate that the suggested classification scheme performs better than the PCA
method, and slightly better than the Delta approach.
1. INTRODUCTION
Due to the large amount of textual information made freely available, a
variety of text categorization tasks have surfaced during the last decade
(Weiss et al., 2010). In this study we focus on the authorship attribution
*Address correspondence to: Jacques Savoy, Computer Science Department, University
of Neuchatel, rue Emile Argand 11, 2000 Neuchatel, Switzerland. Tel: þ41 32 718 2700.
Fax: þ41 32 718 2701. E-mail: Jacques.Savoy@unine.ch
Journal of Quantitative Linguistics
2012, Volume 19, Number 2, pp. 132–161
http://dx.doi.org/10.1080/09296174.2012.659003
0929-6174/12/19020132  2012 Taylor & Francis
D
ow
nl
oa
de
d 
by
 [
U
ni
ve
rs
ity
 o
f 
A
eg
ea
n]
 a
t 0
8:
02
 1
3 
A
pr
il 
20
12
 
question (Love, 2002; Juola, 2008), whereby the correct author of a given
document must be determined based on text samples written by known
authors. This general problem has already been dealt with in different
ways, such as the ‘‘closed class’’ attribution problem described in this
study where the real author is possibly one of several known candidates.
When limiting the number of possible authors to two, we tackle them
with a ‘‘binary’’ or two-case classification problem, a classic example of
which are the Federalist Papers (Mosteller & Wallace, 1964; Holmes &
Forsyth, 1995). At the limit, authorship attribution is considered a
‘‘verification’’ question involving the determination of whether or not a
given author did in fact write a document (Koppel et al., 2009). Finally,
when unable to precisely identify the correct writer, we may just want to
discover certain demographic or psychological information about the
author (‘‘profiling’’) (Argamon et al., 2009).
In all these text categorization problems (Sebastiani, 2002) our first
objective is to represent the corresponding document by a numerical
vector comprising relevant features (word types or lemmas in this study)
useful in distinguishing between several authors (or generally categories).
This process implies the selection of the most pertinent features (Yang &
Peterson, 1997) or the generation of new synthetic features (PCA) useful
for identifying the differences between the authors’ writing styles. In a
second stage we need to weight them according to their importance in the
underlying textual representation and also to their relative discriminative
power. Finally, through applying a classification scheme, the system
could assign the most appropriate author (or category) to a given input
text (‘‘single-label categorization’’ problem).
The rest of this paper is organized as follows. Section 2 presents a brief
overview of related authorship attribution work while Section 3 provides
an overview of our three test collections. Section 4 describes three
authorship attribution algorithms, namely the PCA technique, the Delta
method and our Z-score-based approach. A comparative evaluation is
performed based on word types or lemmas. Finally, the last section
presents the main conclusions that can be drawn from this study.
2. RELATED WORK
Various noteworthy literature surveys on authorship attribution have
recently been published (Love, 2002; Juola, 2006; Zheng et al., 2006;
AUTHORSHIP ATTRIBUTION 133
D
ow
nl
oa
de
d 
by
 [
U
ni
ve
rs
ity
 o
f 
A
eg
ea
n]
 a
t 0
8:
02
 1
3 
A
pr
il 
20
12
 
Koppel et al., 2009; Stamatatos, 2009). In promoting authorship
attribution solutions based on statistics, the first methods proposed were
based on a unitary invariant value reflecting the particular style of a given
author, and varying from one to another (Holmes, 1998). Other studies
pursuing this objective have suggested different statistics related to the
type-token ratio (e.g. Herdan’s C, Guiraud’s R or Honoré’s H), the
proportion of word types occurring once or twice (e.g. Sichel’s S), the use
of lexical richness or other word distribution factors, including average
word length, mean sentence length, Yule’s K-measure, and even the slope
of Zipf’s empirical distribution (Baayen, 2008). To these we could also
add a few simple statistics, including letter occurrence frequencies
(Merriam, 1998), mean number of syllables per word, number of hapax
legomena (words occurring once) together with their relative positions in
a sentence (Morton, 1986), etc. None of these measures has proved to be
very satisfactory however (Hoover, 2003; Grieve, 2007), due in part to
the way word distributions are ruled by a large number of very low
probability elements (‘‘large number of rare events’’ or LNRE) (Baayen,
2001).
Instead of limiting ourselves to a single value we could apply a
multivariate analysis to capture each author’s discriminative stylistic
features (Holmes & Crofts, 2010). The most important studies applicable
here (Burrows, 1992; Binonga & Smith, 1999; Craig & Kinney, 2009) are
those related to PCA (Lebart et al., 1998). In this case new composite
features are generated as a linear combination of given terms, which
could then be applied to represent documents as points within a new
space. Then, to determine who might be the possible author of a new text
we could simply search through the closest document (Hoover, 2006).
The author of this nearest document is probably also the author of the
text in question. For such approach to be effective the distance measure
definition would be of prime importance and in this regard, the new
orthogonal synthetic dimensions generated by the PCA method would
represent a clear definition.
In a third stage, recent studies pay more attention to various categories
of topic-independent features that might closely reflect an author’s style.
In this perspective we can identify three main sources. First, at the lexical
level are word occurrence frequency (or character n-grams) of some
selected terms, punctuation frequency, along with several other
representational marks (Grieve, 2007). Special attention has been given
to function words (determiners (e.g., the, an, . . .), prepositions (in, of, . . .),
134 J. SAVOY
D
ow
nl
oa
de
d 
by
 [
U
ni
ve
rs
ity
 o
f 
A
eg
ea
n]
 a
t 0
8:
02
 1
3 
A
pr
il 
20
12
 
conjunctions (or, but, . . .), pronouns (she, our, . . .), certain verbal forms
(is, was, would, . . .)) appearing in numerous authorship attribution
studies (Burrows, 2002). Given that the precise definition of function
word lists is questionable, researchers have suggested a wide variety of
lists. Burrows (2002) for example suggests that the top n most frequent
word types (with n¼ 40 to 150), Holmes and Forsyth (1995) use a list of
49 high-frequency words, Baayen and Halteren (2002) establish a list of
50 words, while Jockers et al. (2008, p. 491) suggest one containing 110
entries, while the list compiled by Zhao and Zobel (2007) contains 363
words. Finally, Hoover (2006) put forward a list of more than 1000
frequently occurring words, including both function words and lexical
words (nouns, adjectives, verbs, adverbs).
Secondly, at the syntactic level we could account for ‘‘part-of-speech’’
(POS) information through measuring the distribution, frequency,
patterns or various combinations thereof. Thirdly, some authors have
suggested considering structural and layout features, including the total
number of lines, number of lines per sentence or per paragraph,
paragraph indentation, number of tokens per paragraph, presence of
greetings or particular signature formats, as well as features derived from
HTML tags. Additional features could also be considered, such as
particular orthographic conventions (e.g. British vs. US spelling) or the
occurrence of certain specific spelling errors. The resulting number of
potential features that might be considered can get rather long. For
example, Zheng et al. (2006) compiled a list of 270 possible features.
Instead of applying a general-purpose classification method, Burrows
(2002) designed a more specific Delta classifier based on the ‘‘mean of the
absolute difference between the z-scores for a set of word-variables in a
given text-group, as well as the z-scores for the same set of word-
variables in a target-group’’. This method is based on the 40 to 150 most
frequently occurring word tokens while Hoover (2004b) suggested this
scheme could be improved by considering the top 800 most frequent
words. A few variants of the Delta method have also been put forward
(Hoover, 2004a; 2007), along with various other interpretations of this
same scheme (Stein & Argamon, 2006; Argamon, 2008). In all cases the
underlying assumption was that a given author’s style would best be
reflected by identifying the use of function words (or by very frequent
words) together with their occurrence frequencies, rather than relying on
a single vocabulary measure or more topic-oriented terms. Recently,
Jockers and Witten (2010) showed that the Delta method could surpass
AUTHORSHIP ATTRIBUTION 135
D
ow
nl
oa
de
d 
by
 [
U
ni
ve
rs
ity
 o
f 
A
eg
ea
n]
 a
t 0
8:
02
 1
3 
A
pr
il 
20
12
 
the performance levels achieved by the Support Vector Machines (SVM)
method.
As a typical literacy example related to the current study, we could
mention the recent work done by Marusenko and Rodionova (2010),
who suggested 51 potential features capable of distinguishing between
two given authors (Molière vs. Corneille). These features were mainly
structural (number of simple sentences, complex sentences, parenthesis
phrases), or syntactic (number of conjugated verb forms, direct objects,
subjects, subordinate clauses without a conjugated form of a verb, etc.).
The numerous and complex features in this study also generate a
complicated classification scheme, and its advantages over simpler
categorization approaches are not clearly revealed in a definitive manner.
A thorough study on the dispute ‘‘Molière vs. Corneille’’ was recently
published (Labbé, 2009), and similar questions were also raised about
some of Shakespeare’s works (Craig & Kinney, 2009).
In summary, it seems reasonable to suggest that we should make use of
vocabulary features. We could thus conclude that not only the presence
or absence of words but also their occurrence frequencies might reveal
the underlying and unknown ‘‘fingerprint’’ of a particular author, during
a given period and relative to a particular genre and topic. It is known
however that word frequencies could change over time and use (Hoover,
2006), as could topics and genres (e.g. poetry or romance, drama or
comedy, prose or verse) (Burrows, 2002; Hoover, 2004b; Labbé, 2007).
3. EVALUATION CORPORA
Even though there is an empirical tradition in the authorship attribution
domain, it has not benefited from a relatively large number of publicly
available corpora like the information retrieval field (Manning et al.,
2008). As such comparisons between reported performances and general
trends regarding the relative merits of various feature selections,
weighting schemes and classification approaches are difficult to assess
with the required precision. Moreover, the results obtained on any given
corpus must be confirmed through evaluating other corpora (different
genres, subjects and languages) and thus verifying whether good
performance levels obtained were accurate or simply due to document
collection’s hidden and unknown characteristics. For this reason when
planning our experiments we decided to make use of three different
136 J. SAVOY
D
ow
nl
oa
de
d 
by
 [
U
ni
ve
rs
ity
 o
f 
A
eg
ea
n]
 a
t 0
8:
02
 1
3 
A
pr
il 
20
12
 
corpora written in three natural languages, namely English, French, and
German. Moreover, unlike various previous studies on English language
corpus only (Grieve, 2007), we wanted to include other languages
presenting greater inflectional variability than English (e.g. French) or
clearly more complex morphology, such as that of the German language.
Our first experiment will be based on the Oxquarry corpus composed
of 52 segments and around 10,000 tokens in length. Drawn from English
novels written during the end of the 19th century and beginning of the
20th century, this corpus created by G. Ledger had been used in previous
authorship attribution experiments (Labbé, 2007). As shown in the
Appendix, the corpus contains more or less contemporaneous novels,
each coded with a series of alphanumeric tags such as 1A–1Z followed by
2A–2Z, for a total of 52 segments, comprising nine distinct authors and
16 novels. The three writers (Chesterton, Forster, and Tressel) were
represented, for example, by three passages extracted from one of their
works, while another 12 texts were selected from four of Hardy’s novels
(Jude, Madding, Well Beloved, and Wessex Tales).
The selection of these texts was done by another person (G. Ledger)
and based on the prior intuition that correct authorship attribution
would be difficult to establish based on vocabularies alone. The samples
were extracted from the Gutenberg Project (see www.gutenberg.org) and
edited so they would be easily machine-readable. For each text, we
replaced certain system punctuation marks in UTF-8 coding with their
corresponding ASCII code symbols, and replaced single (‘’) or double
quotation marks (‘‘’’) with the (‘) or (‘‘) symbols, and also removed a few
diacritics found in certain English words (e.g., résumé). The diacritics
were kept for the French and German texts, but we replaced the ß with a
double ss in the German excerpts. During this same process we also
expanded contracted forms or expressions (e.g. don’t into do not) to
standardize their spelling forms, and the same principle was applied for
the German language (e.g. im into in dem, aufs into auf das). Considering
the high standards associated with the Gutenberg project, we did not
check the texts against the original works, thus assuming they were
accurate.
As the basis for our experiments we selected the word types or the
lemmas (headword or dictionary entry). In the first case, each distinct
word form had its own entry (e.g. house and houses) while for lemmas we
conflated all inflected forms under the same entry (e.g. writes, wrote,
written were regrouped under the lemma write while houses, house under
AUTHORSHIP ATTRIBUTION 137
D
ow
nl
oa
de
d 
by
 [
U
ni
ve
rs
ity
 o
f 
A
eg
ea
n]
 a
t 0
8:
02
 1
3 
A
pr
il 
20
12
 
house, etc.); in order to do so we used the part-of-speech (POS) tagger for
the English language developed by Toutanova and Manning (2000),
Labbé’s system (Labbé, 2001) for French and the TreeTagger (Schmid,
1995) for German. When given a string of words as input, such systems
are able to add the corresponding POS tag to each term. For example
from the sentence ‘‘Then he explained his invention of which he was
inordinately proud’’ the POS tagger returns ‘‘Then/RB he/PRP explained/
VBD his/PRP$ invention/NN of/IN which/WDT he/PRP was/VBD inordinately/
RB proud/JJ ./.’’. Tags in this sequence may be attached to nouns (NN,
noun, singular, NNS noun, plural, NNP proper noun, singular), verbs (VB,
base form or lemma, VBD past tense, VBP non-3rd-person singular present,
VBZ 3rd-person singular present), adjectives (JJ, JJR adjective in
comparative form), personal pronouns (PRP), possessive pronouns
(PRP$), prepositions (IN), and adverbs (RB).
With this information and the English corpus we were then able to
derive the lemma by removing the plural form of nouns (e.g. animals/NNS
! animal/NN) or by substituting inflectional suffixes of verbs (e.g.
punishes/VBZ ! punish/VB). For the French and German languages, the
POS tagger system was able to determine the lemmas automatically. Its
precise delimitation however was not always clear. We did not consider
for example the two lemmas I/PRP and me/PRP as dissimilar and thus we
merged them under the common headword I. We considered the
distinction between the two grammatical cases (I, subject or nominative
case vs. me direct object or in the accusative case) to be of secondary
importance, and thus decided to group both forms under the same entry.
We also applied the same conflation to the pronouns we and you. For the
French and German languages, the POS tagger made similar substitu-
tions (e.g. me with je, or mich, mir with ich). This conflation approach can
be viewed as a step towards representation based on more abstract lexical
information. Finally, compared to a word-based scheme, lemmas remove
certain dependencies between word variables. For example, when
counting word types, distinct entries are applied for the verb to be in
its various forms. When the word I occurs for example with a relatively
high frequency, we can expect to find a relatively large number of
occurrences of the form am. Thus when considering a lemma-based
representation, the underling dependency between the terms I and am
decreases.
The POS tagging could be done manually but it’s a costly operation.
Even though such automatic tagging systems may not be perfect (e.g.
138 J. SAVOY
D
ow
nl
oa
de
d 
by
 [
U
ni
ve
rs
ity
 o
f 
A
eg
ea
n]
 a
t 0
8:
02
 1
3 
A
pr
il 
20
12
 
Brill’s (1995) automatic POS tagger achieved an accuracy rate of around
96.7%), manual tagging cannot be consider an error-free process
(Fortier, 2000). Thus, given the high accuracy rate achieved by current
automatic POS taggers, we would not support manual over automatic
tagging.
Table 1 provides an overall picture of our three corpora. Under the
label ‘‘English’’ it lists the number of text excerpts, the number of distinct
authors, and the total number of lemmas (with digits and numbers). For
the English corpus only we did not account for punctuation symbols, due
to the fact that there were missing in a few English text excerpts. As
indicated in the fourth row, the determinant the (30,048 occurrences) is
the most frequent lemma while be is the second most frequent (19,912
occurrences). The fifth and sixth rows show the total number of distinct
lemmas (20,400) (or vocabulary size), and the number of distinct word
types (23,872) contained in the English corpus. The bottom part of Table
1 displays useful statistics on general information about typical text
fragments. For example, the mean number of lemmas per text is 9948
(median: 9939; standard deviation: 66.9), and the difference between the
maximum and minimum number of lemmas is rather short (10,118 –
9795¼ 323), thus leading us to assume that each text has a similar size.
In our second experiment we used a corpus of novels written in French,
mainly during the 19th century, and as such we were not concerned with
copyright issues. This corpus is composed of 44 texts, each around 10,000
word tokens in length, and for which the lemmatization was done by D.
Table 1. Statistics on English, French and German corpora.
English French German
Number of text excerpts 52 44 59
Number of distinct authors 9 11 15
Total number of lemmas 517,123 439,532 594,513
Most frequent lemmas the (30,048) le (38,270) , (50,176)
Number of distinct lemmas 20,400 13,919 31,725
Number of distinct word types 23,872 25,841 45,752
Mean number of lemmas/text 9948 9989 10,076
Median number of lemmas/text 9939 10,022 10,085
Standard deviation (lemmas/text) 66.9 145.4 42.5
Min number of lemmas/text 9795 (1T) 9611 (T# 23) 9999 (T# 11)
Max number of lemmas/text 10,118 (2C) 10,239 (T# 29) 10,149 (T# 37)
Mean number of distinct lemmas/text 1984 1634 1987
AUTHORSHIP ATTRIBUTION 139
D
ow
nl
oa
de
d 
by
 [
U
ni
ve
rs
ity
 o
f 
A
eg
ea
n]
 a
t 0
8:
02
 1
3 
A
pr
il 
20
12
 
Labbé (2001). This corpus was created by E. Brunet and used by D.
Labbé in some experiments. The corresponding full text is also available
at the Gutenberg project web site and more precise information on the
authors and work titles is given in the Appendix. To identify each text,
we simply used the numbers from 01 to 44. The content of this corpus is
more systematic in the sense that there are eleven distinct authors, and
two works were selected for each, and for each novel the corpus contains
exactly two text excerpts.
For the French experiment, the top part of the third column of Table 1
lists general information on this corpus, while the bottom part shows a
few statistics describing a typical text excerpt. The top part thus shows
that the definite determiner le (38,270 occurrences) is the most frequent
lemma, followed by the comma (30,782 occurrences), and the lemma de
(of) (27,382). We should also mention that French morphology is more
complex than the English, where nouns and adjectives comprise
variations in genre (masculine vs. feminine) and number (singular vs.
plural), and more distinct suffixes can be attached to verbs. The
determinant the for example can be translated into French as le
(masculine, singular), la (feminine, singular), les (plural), and even the
form l (singular, and elision before a front vowel).
As shown in the bottom part, the mean size (computed in number of
French lemmas) is rather similar (9989) to the English corpus (9948), yet
with a higher standard deviation (145.4). The difference between the
longest (Text 29) and the shortest (Text 23) is greater than that found in
the English corpus (10,239 – 9612¼ 627).
For our third experiment wanted to deal with a more complex
language (from a morphological point of view), so we chose German
literary works. This corpus is composed of text excerpts taken from
novels written mainly during the 19th century and the beginning of the
20th century (works without copyright). This selection comprises 59
texts, each around 10,000 word tokens in length extracted from the
Gutenberg project web site. More information on the 15 distinct authors
and work titles can be found in the Appendix. To identify each German
text, we simply used the numbers from 01 to 59.
The last column in Table 1 lists the comma (50,176 occurrences) as the
most frequent lemma in our German corpus, followed by d (46,323
occurrences) corresponding to the definite determinant with its word type
variations, such as der (e.g. masculine nominative singular or plural
dative), die (e.g. feminine nominative singular or plural nominative), das
140 J. SAVOY
D
ow
nl
oa
de
d 
by
 [
U
ni
ve
rs
ity
 o
f 
A
eg
ea
n]
 a
t 0
8:
02
 1
3 
A
pr
il 
20
12
 
(e.g. neutral nominative singular), along with the related forms den (e.g.
masculine accusative singular), dem (e.g. masculine or neutral dative,
singular), des (e.g. masculine or neutral genitive singular), etc.
Finally as depicted in the bottom part of Table 1, a typical German
text excerpt contains 10,076 lemmas. Its length has a standard deviation
of 42.5, a value inferior than that of both the English and French
corpora.
4. TEXT CLASSIFICATION MODELS
To design and implement an authorship attribution system we
needed to choose a text representation scheme as well as a classifier
model. The following section describes the common representation
used in our experiments. As classifier scheme, we chose the ‘‘principal
component analysis’’ (PCA) coupled with the nearest neighbor
approach (Section 4.2), representing the multivariate paradigm in
authorship attribution. As a second strategy, the Delta rule (Section
4.3) provides an example of a more modern paradigm based on a
reduced number of function words. The Z-score used to measure the
term specificity is described in Section 4.4, while the last section
defines a technique used to measure the distance between text pairs
and evaluates our suggested authorship attribution method applying
the nearest neighbor approach.
4.1 Text Representation and Feature Selection
In our experiments we used two distinct textual representations, one
based on word types and the second on lemmas. Even though Kešelj et al.
(2003) found character n-gram representation could be effective in
authorship attribution, we preferred a method capable of clearly
verifying the text representation generated. For word types, surface
forms (tokens) can be used to directly represent the distinct features
taken into account (e.g. go, goes, gone). With lemma-based representa-
tion, we wanted to freeze one source of possible variation between the
three languages (inflectional morphology) when comparing our results.
In an extreme case for example, the French language possesses 41 distinct
forms of the verb être (to be). A recent study (Fautsch & Savoy, 2009)
demonstrated that significant performance differences could occur, and
they tend to favour the lemmas when comparing representations based
AUTHORSHIP ATTRIBUTION 141
D
ow
nl
oa
de
d 
by
 [
U
ni
ve
rs
ity
 o
f 
A
eg
ea
n]
 a
t 0
8:
02
 1
3 
A
pr
il 
20
12
 
on word types or lemmas, at least with the English language in
information retrieval.
As with all text categorization problems, we had to work with a feature
space comprising a large number of dimensions, and not all terms (word
types or lemmas) were very useful in distinguishing between all possible
authors. As a first dimension reduction scheme we might have removed
all terms having an occurrence frequency of one (hapax legomena) or
two. Upon inspecting the English corpus we found that this pruning
scheme would remove 11,823 lemmas (8768 hapax þ 3055 lemmas occur
twice) out of 20,400, and the resulting percentage reduction would be
around 58%. When analysing the word types in the English corpus, we
found similar reduction proportions, with 10,087 hapax and 3683 types
occurring exactly twice. By ignoring these two cases, however, the feature
space could be reduced from 23,872 word types to 10,102 (a reduction of
around 57.7%).
As a second pruning scheme we might have removed terms (word types
or lemmas) having a document frequency (df, or the number of texts in
which they appear) equal to one or two. Using document frequency as
selection criteria was also found to be effective in other text categoriza-
tion problems, as mentioned by Yang & Pedersen (1997): ‘‘This suggests
that DF (document frequency) thresholding is not just an ad hoc
approach to improve efficiency (as it has assumed in the literature of text
categorization and retrieval), but a reliable measure for selecting
information features. It can be used instead of IG (information gain)
or CHI (w 2-test) when the computation (quadratic) of these measures is
too expensive.’’
For this reason we selected this second pruning technique, even though
the two techniques provide similar results, in our context at least. When
ignoring lemmas having a document frequency of one in the English
corpus, we removed 10,060 lemmas, and when accounting for lemmas
having a df¼ 2, we ignored 3252 additional terms. Adding both cases
allowed us to reduce the feature space from 20,400 lemmas to 7087 (a
reduction of around 65.3%). When analysing word types a similar
pattern could be detected. Starting with 23,872 types we therefore
reduced the feature space to 8765 by ignoring word types appearing in
one document (11,252 types) or two (3854).
With the French and German corpus we detected similar patterns.
With the French lemmas for example we started with 13,919 distinct
lemmas from which 5983 occurred in one document and 2175 in two
142 J. SAVOY
D
ow
nl
oa
de
d 
by
 [
U
ni
ve
rs
ity
 o
f 
A
eg
ea
n]
 a
t 0
8:
02
 1
3 
A
pr
il 
20
12
 
texts. By ignoring these very small frequencies, we reduced the feature
space from 13,919 to 5761 (a relative reduction of 58.6%). For the
German corpus we started with 31,725 lemmas, from which we removed
19,593 terms appearing in one document and 4054 in two, leaving a
feature space of 8078 lemmas (a reduction of 58.8%).
4.2 Principal Component Analysis and Nearest Neighbor
As a first authorship attribution approach we suggested applying PCA
(Lebart et al., 1998), making it possible to graphically view any affinities
between the various texts. In authorship attribution studies (Burrows,
1992; Binonga & Smith, 1999; Craig & Kinney, 2009), this is a known
method. As input it needs a contingency table (also called a lexical table),
with columns corresponding to the texts (object of interest) and rows to
the terms (lemmas or word types). Table 2 illustrates a brief example and
reveals that detecting possible similarities between texts is not an easy
task based only on such table.
As a means of viewing similarities between texts, the PCA method
could be considered a generalization of scatterplot method by providing
one of the best ways of visualizing the affinities and dissimilarities
between text representations. To achieve this, PCA will generate a new
space having fewer dimensions that are ordered and orthogonal to each
other (principal components). This method does not select some of the
given terms but it does generate new dimensions as linear combination of
given variables.
Table 2. Ten most frequent lemmas extracted from English corpus.
Lemma 1A
Hardy
1B
Butler
1C
Morris
1D
Stevenson
1E
Butler
1F
Stevenson
1G
Conrad
the 709 557 606 421 592 540 502
be 346 420 411 482 467 444 376
he 484 536 102 146 464 390 464
and 323 273 322 339 291 374 260
of 321 250 432 275 222 310 319
I 114 212 204 815 279 480 541
an 292 204 230 271 141 290 347
to 267 267 281 309 307 271 238
have 176 240 136 171 203 156 168
in 201 124 158 166 114 161 150
AUTHORSHIP ATTRIBUTION 143
D
ow
nl
oa
de
d 
by
 [
U
ni
ve
rs
ity
 o
f 
A
eg
ea
n]
 a
t 0
8:
02
 1
3 
A
pr
il 
20
12
 
Based on input in the form of a lexical table, the system first
computes the correlation coefficients between the texts. As an
alternative normalization process, we could standardize the term
frequencies by subtracting their mean and dividing by their standard
deviation, although Holmes (1992) demonstrated that both strategies
return similar results. After transforming the raw data into a new
feature space, the system computes projections of each point (text)
into hyperplanes having fewer dimensions (or plane when limited to
two dimensions). During the generation of its various principal
components, the system accounts for a decreasing proportion of the
underling variability (or variance). The first coordinate reflects the best
distance but is limited to one dimension (a line) for showing the
respective distances between texts. When displaying the first two
principal components, a two-dimensional graphic view is obtained (or
plan) depicting the location of the various texts, at the best real
distance (based on all terms).
Based on the English corpus, we selected the top 50 most frequent
lemmas before applying the PCA method. Holmes and Crofts (2010)
suggested using 100 lemmas, Burrows (2002) between 40 and 150. After
applying the PCA method using the R system (Crawley, 2007; Husson
et al., 2011), we obtained the results shown in Figure 1. This graph shows
the relative position of each English text according to the two most
important dimensions. As indicated in the two axes, the first principal
coordinate corresponds to 20.5% of the total variance while the second
corresponds to 11.5%. This two-dimensional figure thus reflects 20.5% þ
11.5%¼ 32% of the total variability. In order to identify each text
excerpt, we add the first letter of the corresponding author to each text
identifier. At the top right corner for example the label ‘‘F 2V’’
corresponds to Forster’s Room with a View while ‘‘B 2A’’ corresponds
to Butler’s Erewhon.
In Figure 1, an excerpt located near to the origin corresponds to a text
very similar to the mean profile generated by all documents in the
underlying corpus. As shown in our study’s analysis this mean
characteristic can be attributed to ‘‘C 1G’’ (Conrad’s Lord Jim), or ‘‘T
1Y’’ (Trestel’s Ragged Trousered Philanthropists). On the other hand
those texts located far from the origin tend to have very distinct
frequency profiles, as evidenced by ‘‘M 2B’’ (Morris’s Dream of John
Ball) shown in the bottom as well as for a second fragment of this word,
as shown by the point labelled ‘‘M 1J’’.
144 J. SAVOY
D
ow
nl
oa
de
d 
by
 [
U
ni
ve
rs
ity
 o
f 
A
eg
ea
n]
 a
t 0
8:
02
 1
3 
A
pr
il 
20
12
 
An inspection of Figure 1 might also allow us to determine the author
of a given text by looking at its nearest neighbor. Although limited to the
first two most important dimensions, the left part of Figure 1 does
suggest that the author of ‘‘C 1T’’ (Conrad’s Almayer’ Folly) could be the
same as the that of ‘‘C 1V’’ (Conrad’s Lord Jim), or perhaps ‘‘C 2K’’
(Conrad’s Almayer’ Folly), a classification strategy corresponding to the
nearest neighbor method (or k 7 NN, with k¼ 1) (Witten & Franck,
2005). As such we could determine the closest neighbor of each text and
then assign the author of this closest neighbor as the probable author. To
facilitate the visualization of this assignment, for each text we added a
straight line to its closest neighbor. The same information is displayed in
Figure 2 but using the French collection, and the German corpus in
Figure 3.
Table 3 reports the performance achieved by the PCA with the nearest
neighbor approach using the top 50, 100 and 150 most frequent terms
Fig. 1. Graphical representation of distances between 52 English texts based on first two
axes of principal component analysis (50 lemmas).
AUTHORSHIP ATTRIBUTION 145
D
ow
nl
oa
de
d 
by
 [
U
ni
ve
rs
ity
 o
f 
A
eg
ea
n]
 a
t 0
8:
02
 1
3 
A
pr
il 
20
12
 
(word types or lemmas) and considering the first two or five principal
components (under the label ‘‘2 axis’’ or ‘‘5 axis’’). When limited to the
first two principal components, we obtain the situation depicted in
Figures 1 to 3, while under five axes, we account for all those dimensions
reflecting more than 5% of the total variability. Thus this limit of five was
not selected on an arbitrary basis, but rather on a decision rule capable of
distinguishing between the axes, the subset really capable of being
pertinent in reflecting the affinities and dissimilarities. Usually, when
including more than five dimensions in our experiments, we would add a
few and sometimes non-significant distances, tending sometimes to
represent more noise than useful information.
With the English corpus, we obtained the most effective performance
when considering the top 100 frequently occurring terms and the first five
principal components, with the same performance levels using word types
or lemmas (48 correct attributions over 52). It seems that using word
Fig. 2. Nearest neighbor representation of each of the 44 texts based on principal
component analysis (50 lemmas, French corpus).
146 J. SAVOY
D
ow
nl
oa
de
d 
by
 [
U
ni
ve
rs
ity
 o
f 
A
eg
ea
n]
 a
t 0
8:
02
 1
3 
A
pr
il 
20
12
 
types tends to produce better effectiveness than lemma-based representa-
tion. It is interesting to note that using more terms (e.g. 150 instead of
100) does not result in improved performance, but rather it decreases
response quality. When analysing performance levels achieved by
Fig. 3. Nearest neighbor representation of each of 59 texts based on principal component
analysis (50 lemmas, German corpus).
Table 3. PCA Evaluation using 2 or 5 principal components with 50 to 150 terms.
English French German
Types Lemmas Types Lemmas Types Lemmas
50 2 axis 48.1% 36.5% 34.1% 31.8% 22.0% 30.5%
50 5 axis 88.5% 86.5% 68.2% 68.2% 62.7% 62.7%
100 2 axis 61.5% 57.7% 45.4% 54.6% 32.2% 39.0%
100 5 axis 92.3% 92.3% 68.2% 70.4% 52.5% 66.1%
150 2 axis 63.5% 57.7% 43.2% 54.6% 35.6% 23.7%
150 5 axis 80.8% 84.6% 68.2% 68.2% 61.0% 69.5%
AUTHORSHIP ATTRIBUTION 147
D
ow
nl
oa
de
d 
by
 [
U
ni
ve
rs
ity
 o
f 
A
eg
ea
n]
 a
t 0
8:
02
 1
3 
A
pr
il 
20
12
 
including more than five principal components, performance fluctuates,
sometimes improving when more dimensions are taken into account,
and sometimes decreasing. Thus we prefer maintaining a clear decision
rule (considering all dimensions reflecting more than 5% of the
variability).
Using the French collection, the best performance was achieved by
considering the top 100 most frequently occurring lemmas and the first
five principal components. This level corresponds to 31 correct
attributions out of 44, while a performance of 68.2% indicates 30
correct assignments, a rather small difference. Thus we can conclude that
using 50, 100 or 150 word types or lemmas with the first five dimensions
resulted in similar performance levels. Unlike the English corpus, using a
lemma-based representation tended to result in more effectiveness than
word-based. With the German collection, the best result were obtained
with 150 most frequent lemmas and using the first five principal
components (41 correct attributions out of 59). As for the French
language, the lemma-based representation tended to produce better
effectiveness (the use of 150 terms with two axes seemed to be an
exception to this general rule).
4.3 Delta Rule
To determine the probable author of a given text, Burrows (2002)
proposed accounting for the most frequent terms (and particularly
function words) together with their occurrence frequencies, compared
to those of a reference group of texts (or corpus). In the original
proposition Burrows (2002) suggested considering the most frequent
word types, but in our study we evaluated both word types and
lemmas. This choice was motivated by the fact that the difference could
be small for the English language with its rather simple morphology but
could be larger for other languages with more inflections and irregular
verb forms (e.g. German).
When comparing two texts, Burrows (2002) suggested that the
second important aspect was not the use of absolute frequencies, but
rather their standardized scores. These values can be obtained by
subtracting the mean and then dividing by the standard deviation (Z-
score). Once this dimensionless quantity has been obtained for each
term, it can then be compared to those obtained from other texts. We
could compute the Z-score of each term ti (word type or lemma) in a
text through determining its term frequency tfij in document Dj, as
148 J. SAVOY
D
ow
nl
oa
de
d 
by
 [
U
ni
ve
rs
ity
 o
f 
A
eg
ea
n]
 a
t 0
8:
02
 1
3 
A
pr
il 
20
12
 
well as its mean (meani) and standard deviation (sdi) for term ti as
shown in Equation 1:
Z scoreðtijÞ ¼
tfij meani
sdi
ð1Þ
To compute the corresponding mean and standard deviation, for each
author we could define a profile by concatenating all texts written by the
same writer. Based on the Z-score value attached to each term, we could
then compute a distance between each pair of texts or between a disputed
text Dj and the different author profiles Ak. Then, by having a set of terms
ti, for i¼ 1, 2, . . . , m, the Delta value (denoted D) could be obtained by
applying Equation 2:
DðDj;AkÞ ¼ 1=m 
Xm
i¼1
Z scoreðtijÞ  Z scoreðtikÞ
  ð2Þ
In this formulation, the same importance is attached to each term.
Large differences would occur when for a given term both Z-scores are
large and have opposite signs. In these cases, one author tended to use
the underlying term more frequently than the mean while the other
employed it very infrequently. On the other hand when for all terms the
Z-scores were very similar, the distance between the disputed text and the
author profile would be small, indicating this author had probably
written that document.
The Delta method was originally based on the 40 to 150 most frequently
occurring word types and applied successfully in the English Restoration
poetry corpus (Burrows, 2002). Hoover (2004b) demonstrated that this
method could be effective in a prose corpus (American English texts from
the end of the 19th century and beginning of the 20th century). In our
study we applied this attribution approach on relatively small English
excerpts (around 10,000 word tokens), compared with Hoover’s study in
which text excerpts comprised 10,000 to 39,000 word tokens, with a mean
value of 27,000. Moreover, we applied the Delta method to other
languages other than English to verify whether they might effective.
In Table 4 we reported the results achieved using the Delta method
with the 50 to 150 most frequent terms (word types or lemmas) and using
three different languages. As a general trend, we concluded that
accounting for 150 terms seemed to be a good choice, thus confirming
Burrows’ conclusion (2002). The differences between word-based and
AUTHORSHIP ATTRIBUTION 149
D
ow
nl
oa
de
d 
by
 [
U
ni
ve
rs
ity
 o
f 
A
eg
ea
n]
 a
t 0
8:
02
 1
3 
A
pr
il 
20
12
 
lemma-based results were usually rather small. With the three
languages, the lemma-based representation tended to provide the best
effectiveness. Finally, the performance levels achieved with the English
collection were very good, although slightly lower with the French, and
with the German corpus this method produced good results overall. This
difference might be related to the fact that we had fewer authors in the
English corpus (9) vs. 11 for the French, or 15 for the German.
4.4 Z-Score and Specific Vocabulary
As a new authorship attribution approach, we suggested representing
each text based on selected terms (word types or lemmas in the current
article) corresponding to the text’s specific vocabulary, as proposed by
Muller (1992) and developed by Savoy (2010). To define and measure a
term’s specificity, we needed to split the entire corpus into two disjoint
parts denoted P0 and P1. For a given term ti, we computed its occurrence
frequency in the set P0 (value denoted tfi0) and its occurrence frequency
in the second part P1 (denoted tfi1). In the current context, the set P0
corresponded to the disputed text while P1 represented all other texts.
Thus, for the entire corpus the occurrence frequency of the term ti
became tfi0 þ tfi1. The total number of word tokens in part P0 (or its size)
was denoted n0, similarly with P1 and n1, and the size of the entire corpus
was defined by n¼ n0þn1.
The distribution of a term ti was assumed to be a binomial distribution
with parameters n0 and Prob[ti], representing the probability of randomly
selecting the term ti from the entire corpus. Based on the maximum
likelihood principle, this probability would be estimated directly as
shown in Equation 3:
Prob½ti ¼
tfi0 þ tfi1
n
ð3Þ
Table 4. Evaluation of Delta method using 50 to 150 terms.
English French German
Types Lemmas Types Lemmas Types Lemmas
50 96.4% 98.1% 86.4% 86.4% 79.7% 81.4%
100 98.1% 96.2% 81.8% 90.9% 84.7% 88.1%
150 96.2% 98.1% 90.9% 93.2% 84.7% 88.1%
150 J. SAVOY
D
ow
nl
oa
de
d 
by
 [
U
ni
ve
rs
ity
 o
f 
A
eg
ea
n]
 a
t 0
8:
02
 1
3 
A
pr
il 
20
12
 
This first approach did involve certain problems (Manning & Schütze,
2000), particularly concerning those terms never occurring in the corpus,
which were assigned a probability of 0. In our opinion however the word
distributions resemble that of a LNRE (large number of rare events)
(Baayen, 2001), and we would therefore suggest smoothing the
estimation of the underlying probability p as ðtfio þ tfi1 þ lÞ=ðnþ ljVjÞ,
where l is a smoothing parameter and jVj the vocabulary size (values
given in Table 1) (Lidstone’s law). This modification would slightly shift
the probability density function’s mass towards rare and unseen words
(or words that do not yet occur). In the absence of a clear theory for
justifying a particular value for the parameter l in our experiments we set
this value to 0.1. We chose to do so because we did not want to
unnecessarily assign a large probability to rare words. Moreover, in
certain circumstances the maximum likelihood estimation would provide
a better estimate (Gale & Church, 1994), thus justifying a smaller value
for the parameter l. Finally, when compared with the Good-Turing
approach (Sampson, 2001), this smoothing technique was rather easy to
implement.
Based on Prob[ti], the probability of selecting the term ti, we could
repeat this draw n0 time to obtain an estimate of the expected number of
its occurrences within the part P0, that is n0 Prob[ti]. We could then of
course compare this expected number to the observed number (namely
tfi0), and any large differences between these two values would indicate a
deviation from the expected behaviour. To obtain a more precise
definition of large we could account for the variance of the underlying
binomial process (defined as n0 Prob[ti]  (17Prob[ti])). The final
standardized Z-score for term ti using the partition P0 and P1 is defined
by Equation (4):
Z scoreðti0Þ ¼
tfi0  n0  Prob½tiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi
n0  Prob½ti  ð1 Prob½tiÞ
p ð4Þ
Based on the Z-score value for a given term, we could then verify
whether it was used proportionally with roughly the same frequency in
both parts (Z-score value close to 0). On the other hand, when a term had
a positive Z-score larger than d (e.g. 2), we could consider it as being
over-used or belonging to the specific vocabulary found in part P0. A
large negative Z-score (less than 7d) indicated than the corresponding
term was under-used in P0 (or similarly over-used in the part P1).
AUTHORSHIP ATTRIBUTION 151
D
ow
nl
oa
de
d 
by
 [
U
ni
ve
rs
ity
 o
f 
A
eg
ea
n]
 a
t 0
8:
02
 1
3 
A
pr
il 
20
12
 
Knowing that the Z-score is assumed to follow a Gaussian (or Normal
N(0, 1)) distribution within the limits of d ¼ 1, we would find that
68.26% of the terms belonged to the common vocabulary, while 15.87%
were part of the specific vocabulary. Based on this definition Savoy
(2010) was able to determine the specificity of the vocabulary used by J.
McCain and B. Obama during the latest US presidential campaign. In
these speeches for example the terms jobs, health, or Bush characterize the
democrat candidate while nuclear, government, and judicial are used with
a much greater frequency by J. McCain.
Using the German corpus, we analysed the 15 most significant Z-scores
on a per author basis. To derive an author profile, we simply averaged the
Z-scores achieved by all texts written by the same person. In this set, we
could first find terms used more or less exclusively by the corresponding
writer in one of their works. These lemmas (or word types) might
correspond to main characters names (e.g. Wilhelm in Goethe’s Die
Leiden des jungen Werther, K. in Kafka’s novel Der Prozess, or Tonio in
Mann’s Tonio Kröger), geographical names or locations (e.g. Venedig and
Hotel in Mann’s Der Tod in Venedig), or words related to the main
characters or actions (e.g. Advokat and Prokurist in Der Prozess). Within
the higher Z-scores we could also find certain frequent words, such as wir
(we), ; (semicolon), ich (I), sie (she/they) in Goethe’s profile, allerdings
(however) in Kafka’s profile, und (and), , (comma), Meer (sea) in T.
Mann, or : (colon), oh (interjection),mein (my), reden (to talk), and ich (I)
in Nietzsche’s most significant terms. As we can see, some lemmas (e.g. ich
(I)) may appear in two distinct profiles having high Z-score values.
Another interesting view is to list the most frequent terms (lemmas in
this case) with their Z-scores in different author’s profile. Table 5 reports
the 15 most frequent lemmas extracted from the German corpus. The
first column indicates the lemmas in decreasing order of their respective
occurrence frequency, while the following five columns show the mean Z-
scores obtained for these lemmas in the corresponding author’s profile. In
the first row, we could infer that the comma (,) was used more
significantly in the works of Hesse (Z-score 4.8) and T. Mann (4.62) when
compared to the rest of our German corpus. Goethe also tended to
follow a similar pattern (2.63), while Nietzsche (72.37) or Kafka
(71.78) used this punctuation symbol significantly less. Our analysis of
the eighth row revealed that Goethe (based on the three novels included
in our corpus) employed more often ich (I) (Z-score 4.76) but the real
champion of the pronoun ich (I) is Nietzsche (7.51). This personal
152 J. SAVOY
D
ow
nl
oa
de
d 
by
 [
U
ni
ve
rs
ity
 o
f 
A
eg
ea
n]
 a
t 0
8:
02
 1
3 
A
pr
il 
20
12
 
pronoun was rejected by T. Mann (Z-score 78.07) or Kafka (74.66)
who clearly preferred the pronoun er (he) (Z-score respectively of 3.11
and 4.89), while Hesse used this pronoun significantly more than other
writers (Z-score of 6.3).
4.5 Z-Score Distance and Evaluation
The previously defined Z-score was attached to each term, and from
these values we could estimate the distance between two texts. For two
documents Dj and Dk, and a set of terms ti, for i¼ 1, 2, . . . , m, the Z-
score metric between them was defined by Equation (5):
DistðDj;DkÞ ¼
1
m
Xm
i¼1
Z scoreðtijÞ  Z scoreðtikÞ
 2 ð5Þ
When both Z-scores were very similar for all terms the resulting distance
was small, meaning that the same author probably wrote both texts.
Moreover, the difference raised to the power of two tended to reduce the
impact of any differences less than 1.0, mainly occurring in the common
vocabulary. On the other hand, large differences could also occur for a
given term, when both Z-scores were large and had opposite signs. In this
Table 5. The top 15 most frequent lemmas and their corresponding Z-scores, according to
five author profiles (German corpus).
Lemma Goethe Nietsche Kafka Hesse Mann T.
1 , 2.63 –2.37 –1.78 4.80 4.62
2 d –3.66 –0.75 3.39 –5.80 3.31
3 . –4.20 –4.66 –2.76 0.54 –0.44
4 und –2.79 –0.57 –5.51 2.42 4.91
5 sein –1.13 0.72 –0.01 4.14 1.58
6 er –4.70 –9.52 4.89 6.30 3.11
7 ein –1.01 –2.68 –3.45 –0.20 1.20
8 ich 4.76 7.51 –4.66 1.55 –8.07
9 ’’ 2.18 –1.82 3.26 4.90 –0.80
10 in –1.83 –1.96 –1.31 –0.33 1.97
11 zu 2.02 0.67 3.07 –1.98 0.30
12 sie 3.76 –4.30 1.81 –5.00 –0.15
13 haben –0.78 –2.93 2.59 4.03 –3.48
14 sich 1.42 –3.17 3.29 –3.21 1.56
15 nicht 0.67 0.40 3.60 1.23 –2.60
AUTHORSHIP ATTRIBUTION 153
D
ow
nl
oa
de
d 
by
 [
U
ni
ve
rs
ity
 o
f 
A
eg
ea
n]
 a
t 0
8:
02
 1
3 
A
pr
il 
20
12
 
case one author tended to use the underlying term more frequently than
the mean (term specific to this author) while for the other this term was
under-used.
With the English and French corpora, the Z-score distance made it
possible to correctly classify all the texts, while with the German
collection, the authorship attribution approach was able to correctly
identify around 85% of the cases (50 over 59). Unlike the two other
methods, this suggested approach was parameter free, so Table 6
contains just one row.
5. CONCLUSION
Authorship attribution problems involve a number of interesting
challenges. In this study, we investigate these problems in the context
of works of literature written in three different languages (English,
French, and German) and for each corpus we process around ten distinct
authors. Unlike several previous studies limited to a single corpus,
usually written in English, we are able to base our conclusions on a
broader and more solid foundation. In empirical studies the use of more
than one test collection must be the norm and in our opinion analysing
corpora comprising works in more than one language allows us to obtain
a better overview of the relative merits of the various methods.
In this paper we first apply the PCA method in which we use word-
frequency (or lemma-based) information to visualize similarities and
dissimilarities between text excerpts. As data visualization tool PCA
defines a new ordered set of orthogonal dimensions in which we can place
the text points and as such constitutes a real advantage over other
approaches. Through working with the resulting space and applying a
distance measure, we are able to apply a nearest-neighbor learning
scheme, even though the success rates obtained with the three corpora are
not perfect.
Table 6. Evaluation of the Z-score method.
English French German
Types Lemmas Types Lemmas Types Lemmas
100% 100% 100% 100% 83.1% 84.7%
154 J. SAVOY
D
ow
nl
oa
de
d 
by
 [
U
ni
ve
rs
ity
 o
f 
A
eg
ea
n]
 a
t 0
8:
02
 1
3 
A
pr
il 
20
12
 
As a second authorship attribution method we apply the Delta rule
proposed by Burrows (2002). Based on the top 50 to 150 most
frequently occurring word types or lemmas, we demonstrate that
when making evaluations using the English, French or German
corpus, this method provides a high success rate (around 90%).
Moreover, we demonstrate the robustness of this method by applying
it on two other corpora written in a language other than English.
When considering lemmas instead of the word types originally
proposed, we also show that similar effectiveness can be obtained
through this approach.
Our work on a third method suggests that a new classification scheme
based on term Z-scores could also be useful. The Z-score defined by
Muller (1992) is used to define the vocabulary specific to a given text, and
in this study to reflect the author’s style. Instead of considering the n
most frequent words (Burrows, 2002; Hoover, 2004), the model proposed
accounts for all terms, word types or lemmas. It is our opinion, however,
that the weighting of feature according to their occurrence frequencies
should be an important aspect and would also constitute a reasonable
strategy. As for the need to design a robust classifier capable of
generalization, we believe that ignoring terms having only small
occurrence or document frequencies would be an appropriate method
for pruning the feature space’s high dimensionality. As a result, as shown
in Table 5 with our German corpus, this approach based on the Z-score
may provide some useful indications about author similarities and
differences. Based on a distance measure, we suggest using this approach
as an authorship attribution method. Evaluations made on the English,
French, and German corpora show high success rates, clearly better than
those of the PCA approach, and slightly better than those associated with
the Delta rule, although the differences between the two tests seem too
small to be of any real significance.
When comparing word types and lemmas as text representation within
the three approaches, both tend to provide similar results. At the limit,
we observe slight although hardly significant improvement when
applying a lemma-based method within the Delta and Z-score-based
approaches. In further studies certain questions remain of course to be
addressed. Some examples include an evaluation of the Z-score measure’s
reliability in authorship attribution and also its impact on text
representation quality when using word bigrams or trigrams (or lemmas)
in a complementary manner.
AUTHORSHIP ATTRIBUTION 155
D
ow
nl
oa
de
d 
by
 [
U
ni
ve
rs
ity
 o
f 
A
eg
ea
n]
 a
t 0
8:
02
 1
3 
A
pr
il 
20
12
 
ACKNOWLEDGMENTS
The author would like to thank D. Labbé for providing us the English and French
corpora, together with a version of the French lemmatized corpus. This work was
supported by the Swiss NSF, under Grant #200021-129535.
REFERENCES
Argamon, S. (2008). Interpreting Burrows’s Delta: Geometric and probabilistic
foundations. Literary and Linguistic Computing, 23(2), 131–147.
Argamon, S., Koppel, M., Pennebaker, J. W., & Schler, J. (2009). Automatically profiling
the author of an anonymous text. Communications of the ACM, 52(2), 119–123.
Baayen, H. R. (2001). Word Frequency Distributions. Dordrecht: Kluwer Academic Press.
Baayen, H. R., & Halteren, H. V. (2002). An experiment in authorship attribution. In
Proceedings of the 6th JADT’2002, St Malo, pp. 69–75.
Baayen, H. R. (2008). Analyzing Linguistic Data: A Practical Introduction to Statistics
using R. Cambridge: Cambridge University Press.
Binonga, J. N. G., & Smith, M. W. (1999). The application of principal component
analysis to stylometry. Literary and Linguistic Computing, 14(4), 445–465.
Brill, E. (1995). Transformation-based error driven learning and natural language
processing: A case study in part-of-speech tagging. Computational Linguistics,
21(4), 543–565.
Burrows, J. F. (1992). Not unless you ask nicely: The interpretative nexus between
analysis and information. Literary and Linguistic Computing, 7(1), 91–109.
Burrows, J.F. (2002). Delta: A measure of stylistic difference and a guide to likely
authorship. Literary and Linguistic Computing, 17(3), 267–287.
Craig, H., & Kinney, A. F. (Eds) (2009). Shakespeare, Computers, and the Mystery of
Authorship. Cambridge: Cambridge University Press.
Crawley, M. J. (2007). The R Book. London: John Wiley & Sons.
Fautsch, C., & Savoy, J. (2009). Algorithmic stemmers or morphological analysis: An
evaluation. Journal of the American Society for Information Sciences and
Technology, 60(8), 1616–1624.
Fortier, P. A. (2000). Le codage des données textuelles. In Proceedings of the 5th
JADT’2000, Lausanne, pp. 221–227.
Gale, W. A., & Church, K. W. (1994). What is wrong with adding one? In N. Oostdijk &
P. de Hann (Eds), Corpus-Based Research into Language (pp. 189–200). San Diego:
Harcourt Brace.
Grieve, J. (2007). Quantitative authorship attribution: An evaluation of techniques.
Literary and Linguistic Computing, 22(3), 251–270.
Holmes, D. I. (1992). A stylometric analysis of Mormon scripture and related texts.
Journal of the Royal Statistical Society A, 155(1), 91–120.
Holmes, D. I., & Forsyth, R. S. (1995). The Federalist Revisited: New directions in
authorship attribution. Literary and Linguistic Computing, 10(2), 111–127.
Holmes, D. I. (1998). The evolution of stylometry in humanities scholarship. Literary and
Linguistic Computing, 13(3), 111–117.
156 J. SAVOY
D
ow
nl
oa
de
d 
by
 [
U
ni
ve
rs
ity
 o
f 
A
eg
ea
n]
 a
t 0
8:
02
 1
3 
A
pr
il 
20
12
 
Holmes, D. I., & Crofts, D. W. (2010). The Diary of a Public Man: A case study in
traditional and non-traditional authorship attribution. Literary and Linguistic
Computing, 25(2), 179–197.
Hoover, D. L. (2003). Another perspective on vocabulary richness. Computers and the
Humanities. 37, 151–178.
Hoover, D. L. (2004a). Delta prime? Literary and Linguistic Computing, 19(4), 477–495.
Hoover, D. L. (2004b). Testing Burrows’s Delta. Literary and Linguistic Computing,
19(4), 453–475.
Hoover, D. L. (2006). Stylometry, chronology and the styles of Henry James. Proceedings
of Digital Humanities, pp. 78–80.
Hoover, D. L. (2007). Updating Delta and Delta prime. GSLIS, University of Illinois, pp.
79–80.
Husson, F., Lê, S., & Pagès, J. (2011). Exploratory Multivariate Analysis by Example
Using R. London: CRC Press.
Jockers, M. L., Witten, D. M., & Criddle, C. S. (2008). Reassessing authorship of the
Book of Mormon using Delta and nearest shrunken centroid classification. Literary
and Linguistic Computing, 23(4), 465–491.
Jockers, M. L., & Witten, D. M. (2010). A comparative study of machine learning
methods for authorship attribution. Literary and Linguistic Computing, 25(2), 215–
223.
Juola, P. (2006). Authorship attribution. Foundations and Trends in Information Retrieval,
1(3), 233–334.
Kešelj, V., Peng, F., Cercone, N., & Thomas C. (2003). N-gram-based author profiles for
authorship attribution. Proceedings of the Conference Pacific Association for
Computational Linguistics, PACLING’03, Halifax, pp. 255–264.
Koppel, M., Schler, J., & Argamon, S. (2009). Computational methods in authorship
attribution. Journal of the American Society for Information Science & Technology,
60(1), 9–26.
Labbé, D. (2001). Normalisation et lemmatisation d’une question ouverte. Journal de la
Société Française de Statistique, 142(4), 37–57.
Labbé, D. (2007). Experiments on authorship attribution by intertextual distance in
English. Journal of Quantitative Linguistics, 14(1), 33–80.
Labbé D. (2009). Si deux et deux font quatre, Molière n’a pas écrit Dom Juan. Paris: Max
Milo.
Lebart, L., Salem, A., & Berry, L. (1998). Exploring Textual Data. Dordrecht: Kluwer.
Love, H. (2002). Attributing Authorship: An Introduction. Cambridge: Cambridge
University Press.
Manning, C. D., & Schütze, H. (2000). Foundations of Statistical Natural Language
Processing. Cambridge: The MIT Press.
Manning, C. D., Raghavan, P., & Schütze, H. (2008). Introduction to Information
Retrieval. Cambridge: Cambridge University Press.
Marusenko, M., & Rodionova, E. (2010). Mathematical methods for attributing literary
works when solving the ‘‘Corneille–Molière’’ problem. Journal of Quantitative
Linguistics, 17(1), 30–54.
Merriam, T. (1998). Heterogeneous authorship in early Shakespeare and the problem of
Henry V. Literary and Linguistic Computing, 13, 15–28.
Morton, A. Q. (1986). Once. A test of authorship based on words which are not repeated
in the sample. Literary and Linguistic Computing, 1(1), 1–8.
AUTHORSHIP ATTRIBUTION 157
D
ow
nl
oa
de
d 
by
 [
U
ni
ve
rs
ity
 o
f 
A
eg
ea
n]
 a
t 0
8:
02
 1
3 
A
pr
il 
20
12
 
Mosteller, F., & Wallace, D. L. (1964). Inference and Disputed Authorship, The Federalist.
Reading, MA: Addison-Wesley. Reprinted 2007.
Muller, C. (1992). Principes et méthodes de statistique lexicale. Paris: Honoré Champion.
Nugues, P. M. (2006). An Introduction to Language Processing with Perl and Prolog.
Berlin: Springer-Verlag.
Sampson, G. (2001). Empirical Linguistics. London: Continuum.
Savoy, J. (2010). Lexical analysis of US political speeches. Journal of Quantitative
Linguistics, 17(2), 123–141.
Sebastiani, F. (2002). Machine learning in automatic text categorization. ACM
Computing Survey, 14(1), 1–27.
Schmid, H. (1995). Improvements in part-of-speech tagging with an application to
German. Proceedings of the ACL SIGDAT-Workshop.
Stamatatos, E. (2009). A survey of modern authorship attribution methods. Journal of the
American Society for Information Science and Technology, 60(3), 433–214.
Stein, S., & Argamon, S. (2006). A mathematical explanation of Burrows’s Delta.
Proceedings of Digital Humanities, Paris, France, July 2006.
Toutanova, K., & Manning, C. (2000). Enriching the knowledge sources used in a
maximum entropy part-of-speech tagging. Proceedings of the Joint SIGDAT
Conference on Empirical Methods in Natural Language Processing and Very Large
Corpora (EMNLP/VLC–2000), Hong Kong, pp. 63–70.
Weiss, S. M., Indurkhya, N., & Zhang, T. (2010). Fundamentals of Predictive Text
Mining. London: Springer-Verlag.
Yang, Y., & Pedersen, J. O. (1997). A comparative study of feature selection in text
categorization. Proceedings of the 14th International Conference on Machine
Learning ICML, Nashville, pp. 412–420.
Zhao, Y., & Zobel, J. (2007). Searching with style: Authorship attribution in classic
literature. Proceedings of the 30th Australasian Computer Science Conference
(ACSC2007), Ballarat, pp. 59–68.
Zheng, R., Li, J., Chen, H., & Huang, Z. (2006). A framework for authorship
identification of online messages: Writing-style features and classification
techniques. Journal of the American Society for Information Science & Technology,
57(3), 378–393.
158 J. SAVOY
D
ow
nl
oa
de
d 
by
 [
U
ni
ve
rs
ity
 o
f 
A
eg
ea
n]
 a
t 0
8:
02
 1
3 
A
pr
il 
20
12
 
APPENDIX
Table A.1. Description of Oxquarry corpus content.
Id Author Short Title Id Author Short Title
1A Hardy Jude 2A Butler Erewhon revisit.
1B Butler Erewhon revisit. 2B Morris Dream of JB
1C Morris News from nowhere 2C Tressel Ragged TP
1D Stevenson Catriona 2D Hardy Jude
1E Butler Erewhon revisit. 2E Stevenson Ballantrae
1F Stevenson Ballantrae 2F Hardy Wessex Tales
1G Conrad Lord Jim 2G Orczy Elusive P
1H Hardy Madding 2H Conrad Lord Jim
1I Orczy Scarlet P 2I Morris News from nowhere
1J Morris Dream of JB 2J Hardy Well beloved
1K Stevenson Catriona 2K Conrad Almayer
1L Hardy Jude 2L Hardy Well beloved
1M Orczy Scarlet P 2M Morris News from nowhere
1N Stevenson Ballantrae 2N Conrad Almayer
1O Conrad Lord Jim 2O Forster Room with a view
1P Chesterton Man who was 2P Forster Room with a view
1Q Butler Erewhon revisit. 2Q Conrad Almayer
1R Chesterton Man who was 2R Stevenson Catriona
1S Morris News from nowhere 2S Hardy Madding
1T Conrad Almayer 2T Hardy Well beloved
1U Orczy Elusive P 2U Chesterton Man who was
1V Conrad Lord Jim 2V Forster Room with a view
1W Orczy Elusive P 2W Stevenson Catriona
1X Hardy Wessex Tales 2X Hardy Well beloved
1Y Tressel Ragged TP 2Y Orczy Scarlet P
1Z Tressel Ragged TP 2Z Hardy Madding
AUTHORSHIP ATTRIBUTION 159
D
ow
nl
oa
de
d 
by
 [
U
ni
ve
rs
ity
 o
f 
A
eg
ea
n]
 a
t 0
8:
02
 1
3 
A
pr
il 
20
12
 
Table A.2. Detailed description of French corpus content.
Id Author Title
1, 23 Marivaux La Vie de Marianne
2, 24 Marivaux Le Paysan parvenu
3, 25 Voltaire Zadig
4, 26 Voltaire Candide
5, 27 Rousseau La Nouvelle Héloı̈se
6, 28 Rousseau Emile
7, 29 Chateaubriand Atala
8, 30 Chateaubriand La Vie de Rancé
9, 31 Balzac Les Chouans
10, 32 Balzac Le Cousin Pons
11, 33 Sand Indiana
12, 34 Sand La Mare au diable
13, 35 Flaubert Madame Bovary
14, 36 Flaubert Bouvard et Pécuchet
15, 37 Maupassant Une Vie
16, 38 Maupassant Pierre et Jean
17, 39 Zola Thérèse Raquin
18, 40 Zola La Bête humaine
19, 41 Verne De la terre à la lune
20, 42 Verne Le secret de Wilhelm Storitz
21, 43 Proust Du côté de chez Swann
22, 44 Proust Le Temps retrouvé
160 J. SAVOY
D
ow
nl
oa
de
d 
by
 [
U
ni
ve
rs
ity
 o
f 
A
eg
ea
n]
 a
t 0
8:
02
 1
3 
A
pr
il 
20
12
 
Table A.3. Detailed description of German corpus content.
Id Author Title
1, 2 Goethe Die Wahlverwandtschaften
3, 4 Goethe Die Leiden des jungen Werther
5, 6 Goethe Wilhelm Meisters Wanderjahre
7, 8 Morike Mozart auf der Reise nach Prag
9, 10 Keller Die Leute von Seldwyla – Band 1
11, 12 Keller Die Leute von Seldwyla – Band 2
13 Heyse L’Arrabbiata
14, 15 Heyse Beatrice
16, 17, 18 Heyse Der Weinhueter
19 Raabe Deutscher Mondschein
20, 21 Raabe Zum wilden Mann
22, 23 Fontane Unterm Birnbaum
24, 25, 26 Nietzsche Also sprach Zarathustra
27, 28 Nietzsche Ecce Homo
29 Hauptmann Bahnwärter Thiel
30, 31 Hauptmann Der Ketzer von Soana
32, 33 Falke Der Mann im Nebel
34, 35 Mann, H. Flöten und Dolche
36 Mann, H. Der Vater
37, 38 Mann, T. Der Tod in Venedig
39, 40 Mann, T. Tonio Kröger
41 Mann, T. Tristan
42, 43 Kafka Der Prozeß
44, 45 Kafka Die Verwandlung
46 Kafka In der Strafkolonie
47, 48 Wassermann Caspar Hauser
49, 50 Wassermann Der Mann von vierzig Jahren
51, 52 Wassermann Mein Weg als Deutscher und Jude
53, 54 Hesse Drei Geschichten aus dem Leben Knulps
55, 56, 57 Hesse Siddhartha
58, 59 Graf Zur Freundlichen Erinnerung
AUTHORSHIP ATTRIBUTION 161
D
ow
nl
oa
de
d 
by
 [
U
ni
ve
rs
ity
 o
f 
A
eg
ea
n]
 a
t 0
8:
02
 1
3 
A
pr
il 
20
12
 
