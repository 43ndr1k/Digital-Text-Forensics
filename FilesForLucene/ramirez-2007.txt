356 IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY, VOL. 17, NO. 3, MARCH 2007
Performance-Based Interpreter Identification in
Saxophone Audio Recordings
Rafael Ramirez, Esteban Maestre, Antonio Pertusa, Emilia Gómez, and Xavier Serra
Abstract—We propose a novel approach to the task of identi-
fying performers from their playing styles. We investigate how
skilled musicians (Jazz saxophone players in particular) express
and communicate their view of the musical and emotional content
of musical pieces and how to use this information in order to auto-
matically identify performers. We study deviations of parameters
such as pitch, timing, amplitude and timbre both at an inter-note
level and at an intra-note level. Our approach to performer iden-
tification consists of establishing a performer dependent mapping
of inter-note features (essentially a “score” whether or not the
score physically exists) to a repertoire of inflections characterized
by intra-note features. We present a successful performer identifi-
cation case study.
Index Terms—Artificial intelligence, audio recordings, music,
signal processing.
I. INTRODUCTION
AKEY CHALLENGE in the area of music information,given the explosion of online music and the rapidly
expanding digital music collections, is the development of effi-
cient and reliable music search and retrieval systems. One of the
main deficiencies of current music search and retrieval systems
is the gap between the simplicity of the content descriptors that
can be currently extracted automatically and the semantic rich-
ness in music information. Conventional information retrieval
has been mainly based on text, and the approaches to textual
information retrieval have been transferred into music infor-
mation retrieval. However, music contents and text contents
are of a very different nature which very often makes textual
information retrieval unsatisfactory in a musical context. It has
been widely recognized that music retrieval techniques should
incorporate high-level music information.
In this paper, we focus on the task of identifying famous
performers from their playing style using high-level descrip-
tors extracted from audio recordings. The identification of per-
formers by using the expressive content in their performances
raises particularly interesting questions but has nevertheless re-
Manuscript received July 21, 2006; revised November 21, 2006. This
work was supported in part by the Spanish Ministry of Education and
Science under Grants TINC2003-07776-C02-01 ProMusic Project and
TIN2006-14932-C02-01 ProSeMus Project. This paper was recommended by
Associate Editor E. Izquierdo.
R. Ramirez, E. Maestre, E. Gómez, and X. Serra are with the Music Tech-
nology Group, Pompeu Fabra University, Barcelona 08003, Spain (e-mail:
rramirez@iua.upf.edu).
A. Pertusa is with the DLSI, Alicante University, Alicante 03080, Spain.
Color versions of one or more of the figures in this paper are available online
at http://ieeexplore.ieee.org.
Digital Object Identifier 10.1109/TCSVT.2007.890862
ceived relatively little attention in the past. Given the capabilities
of current audio analysis systems, we believe expressive-con-
tent-based performer identification is a promising research topic
in music information retrieval. This work is based on our pre-
vious work on expressive performance modeling [17], [18].
The data used in our investigations are audio recordings
of real performances by famous Jazz saxophonists. The use
of audio recordings, as opposed to MIDI recordings where
data analysis is simplified, poses substantial difficulties for the
extraction of music performance information. However, the
obvious benefits of using real audio recordings widely com-
pensate the extra effort required for the audio analysis. We use
sound analysis techniques based on spectral models [23] for ex-
tracting high-level symbolic features from the recordings. The
spectral model analysis techniques are based on decomposing
the original signal into sinusoids plus a spectral residual. From
the sinusoids of a monophonic signal it is possible to extract
high-level information such as note pitch, onset, duration,
attack and loudness among other information. In particular,
for characterizing structure in saxophone performances, we
are interested in two types of features: intra-note features
representing the internal structure of performed notes and
inter-note features representing information about the music
context in which expressive events occur. We use the software
SMSTools1 which is an ideal tool for preprocessing the signal
and providing a high-level description of the audio recordings.
Once the relevant high-level information is extracted we apply
machine learning techniques [13] to automatically discover
regularities and expressive patterns for each performer. We use
these regularities and patterns in order to identify a particular
performer in a given audio recording. We discuss different
machine learning techniques for detecting the performer’s
expressive patterns, as well as the perspectives of using sound
analysis techniques on arbitrary polyphonic audio recordings.
The rest of the paper is organized as follows. Section II sets
the background for the research reported here. Section III de-
scribes how we process the audio recordings in order to ex-
tract both intra-note and inter-note information. Section IV de-
scribes our approach to performance-driven performer identi-
fication. Section V describes a case study on identifying per-
formers based on their playing style and discusses the results,
and finally, Section VI presents some conclusions and indicates
some areas of future research.
II. BACKGROUND
Music performance plays a central role in our musical cul-
ture today. Concert attendance and recording sales often reflect
1[Online]. Available: http://www.iua.upf.es/sms.
1051-8215/$25.00 © 2007 IEEE
RAMIREZ et al.: PERFORMANCE-BASED INTERPRETER IDENTIFICATION IN SAXOPHONE AUDIO RECORDINGS 357
people’s preferences for particular performers. The manipula-
tion of sound properties such as pitch, timing, amplitude and
timbre by different performers is clearly distinguishable by the
listeners. Expressive music performance studies the manipula-
tion of these sound properties in an attempt to understand ex-
pression in performances. There has been much speculation as
to why performances contain expression. Hypothesis include
that musical expression communicates emotions [30] and that
it clarifies musical structure [31], i.e., the performer shapes the
music according to her own intensions [32].
Understanding and formalizing expressive music perfor-
mance is an extremely challenging problem which in the past
has been studied from different perspectives, (e.g., [2], [7],
[22]). The main approaches to empirically studying expressive
performance have been based on statistical analysis (e.g., [20]),
mathematical modeling (e.g., [26]), and analysis-by-synthesis
(e.g., [6]). In all these approaches, it is a person who is re-
sponsible for devising a theory or mathematical model which
captures different aspects of musical expressive performance.
The theory or model is later tested on real performance data in
order to determine its accuracy. The majority of the research
on expressive music performance has focused on the perfor-
mance of musical material for which notation (i.e., a score)
is available, thus providing unambiguous performance goals.
Expressive performance studies have also been very much
focused on (classical) piano performance in which pitch and
timing measurements are simplified.
This paper describes a machine learning approach to inves-
tigate how skilled musicians (Jazz saxophone players in partic-
ular) express and communicate their view of the musical and
emotional content of musical pieces and how to use this infor-
mation in order to automatically distinguish among performers.
We study deviations of parameters such as pitch, timing, ampli-
tude and timbre both at an inter-note-level and at an intra-note-
level. This is, we analyze the pitch, timing (onset and duration),
amplitude (energy mean) and timbre of individual notes, as well
as the timing and amplitude of individual intra-note events. We
focus on saxophone performance where timing and pitch mea-
surements present a greater challenge compared to the mea-
surements in piano performances (this is due to the fact that in
piano performances certain expressive resources, e.g., vibrato
and glissando, are absent).
Roughly, the basic idea of our approach to performer iden-
tification is to establish a performer-dependent mapping from
inter-note features (essentially a “score” whether or not the
score physically exists) to a repertoire of inflections charac-
terized by intra-note features. As an analogy, the inter-note
features may be seen as a literary text, while the repertoire of
inflections (i.e., the intra-note features) is like a typeface or
style of handwriting that different performers use to render
the text in different ways. Our approach to performer identi-
fication is motivated by our previous work [19] on expressive
music performance synthesis. In [19] we consider a set of
inflections (characterized by intra-note features) and use the
note musical context (characterized by inter-note features) in
order to predict the type of inflection to be used in that context.
We use particular instances, i.e., audio samples, of the type
of inflection predicted to synthesize expressive performances
from inexpressive score descriptions. It is clear that by using a
particular performer’s samples the synthesized pieces “sound”
like played by that performer. Thus, it seems reasonable to
apply the inverse process for performer identification.
Previous research addressing expressive music performance
using machine learning techniques has included a number of
approaches. Lopez de Mantaras et al. [10] report on SaxEx, a
performance system capable of generating expressive solo sax-
ophone performances in Jazz. One limitation of their system is
that it is incapable of explaining the predictions it makes and it
is unable to handle melody alterations, e.g., ornamentations.
Ramirez et al. [17] have explored and compared diverse ma-
chine learning methods for obtaining expressive music perfor-
mance models for Jazz saxophone that are capable of both gen-
erating expressive performances and explaining the expressive
transformations they produce. They propose an expressive per-
formance system based on inductive logic programming which
induces a set of first order logic rules that capture expressive
transformation both at an inter-note level (e.g., note duration,
loudness) and at an intra-note level (e.g., note attack, sustain).
Based on the theory generated by the set of rules, they im-
plemented a melody synthesis component which generates ex-
pressive monophonic output (MIDI or audio) from inexpressive
melody MIDI descriptions.
With the exception of the work by Lopez de Mantaras et al.
and Ramirez et al., most of the research in expressive perfor-
mance using machine learning techniques has focused on clas-
sical piano music where often the tempo of the performed pieces
is not constant. The works focused on classical piano have fo-
cused on global tempo and loudness transformations while we
are interested in both intra-note and inter-note level tempo and
loudness transformations.
Widmer [28] reported on the task of discovering general rules
of expressive classical piano performance from real perfor-
mance data via inductive machine learning. The performance
data used for the study are MIDI recordings of 13 piano sonatas
by W.A. Mozart performed by a skilled pianist. In addition
to these data, the music score was also coded. The resulting
substantial data consists of information about the nominal note
onsets, duration, metrical information and annotations.
Tobudic et al. [25] describe a relational instance-based ap-
proach to the problem of learning to apply expressive tempo
and dynamics variations to a piece of classical music, at dif-
ferent levels of the phrase hierarchy. Their learning algorithm
recognizes similar phrases from the training set and applies their
expressive patterns to a new piece.
Other inductive approaches to rule learning in music and
musical analysis include [1], [5]. In [5], Dovey analyzes piano
performances of Rachmaniloff pieces using inductive logic
programming and extracts rules underlying them. In [1], Van
Baelen extended Dovey’s work and attempted to discover
regularities that could be used to generate MIDI information
derived from the musical analysis of the piece.
Nevertheless, the use of expressive performance models, ei-
ther automatically induced or manually generated, for identi-
fying musicians has received little attention in the past. This is
mainly due to two factors: 1) the high complexity of the feature
extraction process that is required to characterize expressive per-
358 IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY, VOL. 17, NO. 3, MARCH 2007
formance and 2) the question of how to use the information pro-
vided by an expressive performance model for the task of perfor-
mance-based performer identification. To the best of our knowl-
edge, the only group working on performance-based automatic
performer identification is the group led by Gerhard Widmer.
Saunders et al. [21] apply string kernels to the problem of rec-
ognizing famous pianists from their playing style. The charac-
teristics of performers playing the same piece are obtained from
changes in beat-level tempo and beat-level loudness. From such
characteristics, general performance alphabets can be derived,
and pianists’ performances can then be represented as strings.
They apply both kernel partial least squares and support vector
machines to this data.
Stamatatos and Widmer [24] address the problem of identi-
fying the most likely music performer, given a set of perfor-
mances of the same piece by a number of skilled candidate
pianists. They propose a set of very simple features for repre-
senting stylistic characteristics of a music performer that relate
to a kind of “average” performance. A database of piano per-
formances of 22 pianists playing two pieces by Frédéric Chopin
is used. They propose an ensemble of simple classifiers derived
by both subsampling the training set and subsampling the input
features. Experiments show that the proposed features are able
to quantify the differences between music performers.
III. MELODIC DESCRIPTION
In this section, we outline how we extract a description of
a performed melody for monophonic recordings. We use this
melodic representation to provide a inter-note and intra-note de-
scription of the performances and apply machine learning tech-
niques to these extracted features. This is, our interest is to ob-
tain for each performed note, a set of intra-note features and a
set of inter-note features from the audio recording. The set of
intra-note features includes descriptors such as the note’s attack
level, sustain duration, sustain slope, amount of legato with the
previous note, amount of legato with the following note, mean
energy, spectral centroid and spectral tilt. The set of inter-note
features includes the relative pitch and duration of the neigh-
boring notes (i.e., previous and following notes) as well as the
musical structures to which the note belongs.
A. Extraction of Inter-Note Features
First of all, we perform a spectral analysis of a portion of
sound, called analysis frame, whose size is a parameter of the al-
gorithm. This spectral analysis consists of multiplying the audio
frame with an appropriate analysis window and performing a
discrete Fourier transform (DFT) to obtain its spectrum. In this
case, we use a frame width of 46 ms, an overlap factor of 50%,
and a Keiser–Bessel 25 dB window. Then, we compute a set
of low-level descriptors for each spectrum: energy and an esti-
mation of the fundamental frequency. From these low-level de-
scriptors we perform a note segmentation procedure. Once the
note boundaries are known, the note descriptors are computed
from the low-level values.
As mentioned before, the main low-level descriptors used
to characterize note-level expressive performance are instanta-
neous energy and fundamental frequency.
Energy Computation: The energy descriptor is computed on
the spectral domain, using the values of the amplitude spectrum
at each analysis frame. In addition, energy is computed in dif-
ferent frequency bands as defined in [9], and these values are
used by the algorithm for note segmentation.
B. Fundamental Frequency Estimation
For the estimation of the instantaneous fundamental fre-
quency we use a harmonic matching model derived from
the two-way mismatch procedure (TWM) [11]. For each
fundamental frequency candidate, mismatches between the
harmonics generated and the measured partials frequencies
are averaged over a fixed subset of the available partials. A
weighting scheme is used to make the procedure robust to the
presence of noise or absence of certain partials in the spectral
data. The solution presented in [11] employs two mismatch
error calculations. The first one is based on the frequency
difference between each partial in the measured sequence and
its nearest neighbor in the predicted sequence. The second is
based on the mismatch between each harmonic in the predicted
sequence and its nearest partial neighbor in the measured
sequence. This TWM helps to avoid octave errors by applying
a penalty for partials that are present in the measured data
but are not predicted, and also for partials whose presence is
predicted but which do not actually appear in the measured
sequence. The TWM mismatch procedure has also the benefit
that the effect of any spurious components or partial missing
from the measurement can be counteracted by the presence of
uncorrupted partials in the same frame.
First, we perform a spectral analysis of all the windowed
frames, as explained above. Second, the prominent spectral
peaks of the spectrum are detected from the spectrum magni-
tude. These spectral peaks of the spectrum are defined as the
local maxima of the spectrum which magnitude is greater than
a threshold. The spectral peaks are compared to a harmonic
series and a TWM error is computed for each fundamental
frequency candidates. The candidate with the minimum error is
chosen to be the fundamental frequency estimate.
After a first test of this implementation, some improvements
to the original algorithm where implemented to deal with some
errors of the algorithm.
• Peak selection: A peak selection routine has been added
in order to eliminate spectral peaks corresponding to
noise. The peak selection is done according to a masking
threshold around each of the maximum magnitude peaks.
The form of the masking threshold depends on the peak
amplitude, and uses three different slopes depending on
the frequency distance to the peak frequency.
• Context awareness: We take into account previous values
of the fundamental frequency estimation and instrument
dependencies to obtain a more adapted result.
• Noise gate: A noise gate based on some low-level signal
descriptor is applied to detect silences, so that the estima-
tion is only performed in nonsilent segments of the sound.
Note segmentation is performed using a set of frame descrip-
tors, which are energy computation in different frequency bands
and fundamental frequency. Energy onsets are first detected fol-
lowing a band-wise algorithm that uses some psycho-acoustical
RAMIREZ et al.: PERFORMANCE-BASED INTERPRETER IDENTIFICATION IN SAXOPHONE AUDIO RECORDINGS 359
knowledge [9]. In a second step, fundamental frequency transi-
tions are also detected. Finally, both results are merged to find
the note boundaries (onset and offset information).
Note Descriptors: We compute note descriptors using the
note boundaries and the low-level descriptors values. The low-
level descriptors associated to a note segment are computed
by averaging the frame values within this note segment. Pitch
histograms have been used to compute the pitch note and the
fundamental frequency that represents each note segment, as
found in [12]. This is done to avoid taking into account mistaken
frames in the fundamental frequency mean computation. First,
frequency values are converted into cents, by the following for-
mula:
(1)
where ( is a the reference frequency of the ).
Then, we define histograms with bins of 100 cents and hop size
of 5 cents and we compute the maximum of the histogram to
identify the note pitch. Finally, we compute the frequency mean
for all the points that belong to the histogram. The MIDI pitch is
computed by quantization of this fundamental frequency mean
over the frames within the note limits.
Musical Analysis: It is widely recognized that expressive per-
formance is a multilevel phenomenon and that humans perform
music considering a number of abstract musical structures. After
having computed the note descriptors as above, and as a first step
towards providing an abstract structure for the recordings under
study, we decided to use Narmour’s theory of perception and
cognition of melodies [14], [15] to analyze the performances.
The implication/realization model proposed by Narmour is
a theory of perception and cognition of melodies. The theory
states that a melodic musical line continuously causes listeners
to generate expectations of how the melody should continue.
The nature of these expectations in an individual are motivated
by two types of sources: innate and learned. According to Nar-
mour, on the one hand, we are all born with innate informa-
tion which suggests to us how a particular melody should con-
tinue. On the other hand, learned factors are due to exposure to
music throughout our lives and familiarity with musical styles
and particular melodies. According to Narmour, any two con-
secutively perceived notes constitute a melodic interval, and if
this interval is not conceived as complete, it is an implicative
interval, i.e., an interval that implies a subsequent interval with
certain characteristics. That is to say, some notes are more likely
than others to follow the implicative interval. Two main prin-
ciples recognized by Narmour concern registral direction and
intervallic difference. The principle of registral direction states
that small intervals imply an interval in the same registral direc-
tion (a small upward interval implies another upward interval
and analogously for downward intervals), and large intervals
imply a change in registral direction (a large upward interval
implies a downward interval and analogously for downward in-
tervals). The principle of intervallic difference states that a small
(five semitones or less) interval implies a similarly-sized in-
terval (plus or minus two semitones), and a large interval (seven
semitones or more) implies a smaller interval. Based on these
Fig. 1. Prototypical Narmour structures.
Fig. 2. Narmour analysis of All of Me.
two principles, melodic patterns or groups can be identified that
either satisfy or violate the implication as predicted by the prin-
ciples. Such patterns are called structures and are labeled to de-
note characteristics in terms of registral direction and intervallic
difference. Fig. 1 shows prototypical Narmour structures. A note
in a melody often belongs to more than one structure. Thus, a de-
scription of a melody as a sequence of Narmour structures con-
sists of a list of overlapping structures. We parse each melody in
the training data in order to automatically generate an implica-
tion/realization analysis of the pieces. Fig. 2 shows the analysis
for a fragment of a melody.
C. Extraction of Intra-Note Features
Once we segment the audio signal into notes, we perform
a characterization of each of the notes in terms of its internal
features.
Intra-Note Segmentation: The proposed intra-note segmenta-
tion method is based on the study of the energy envelope contour
of the note. Once onsets and offsets are located, we study the in-
stantaneous energy values of the analysis frames corresponding
to each note. This study is carried out by analyzing the envelope
curvature and characterizing its shape, in order to estimate the
limits of the intra-note segments.
When observing the note energy envelopes from the saxo-
phone recordings, we identify that there are usually three seg-
ments (attack, sustain, and release [29]) needed to conform a de-
scription that fits the model schematically represented in Fig. 3.
We discarded the decay segment due to the general characteris-
tics of the notes within the performances.
In order to extract these three characteristic segments, we
study the smoothed derivatives in a similar way that presented
in [8], where partial amplitude envelopes are modeled for iso-
lated sounds. The main difference is that we analyze the notes in
their musical context, rather than isolated. In addition, only three
linear segments are considered. Moreover, instead of studying
the contribution of all the partials, we obtain general intensity
information from the total energy envelope characteristic. The
procedure is carried out as follows.
Considering the energy envelope as a differentiable function
over time, the points of maximum curvature can be considered
as the local maximum variations of the first derivative of the
signal energy (second derivative extremes), that is, the local
maxima or minima of the second derivative.
Due to the characteristics of the audio signal, the energy enve-
lope must be previously smoothed by low-pass filtering, since
there are typically too many second derivative extremes. The
low-pass filtering is carried out by means of a variable-width
360 IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY, VOL. 17, NO. 3, MARCH 2007
Fig. 3. Schematic view of the proposed energy envelope-based intra-note segmentation.
Gaussian convolution. Several smoothing steps are carried out
in order to find a good cut-off frequency of the smoothing filter.
The smoothed envelope should not differ much to the original
one to avoid loss of localization due to the filtering effect. Thus,
for each smoothing step, the error at smoothing step be-
tween original and current envelope is computed. This is carried
out by means of (2), where is the length of the envelope in
frames, is the original envelope and is the smoothed
envelope at step
(2)
Starting from a low cut-off frequency , this frequency is
increased each smoothing step until the error gets lower than
a certain threshold , empirically selected. Then, we compute
the three first derivatives of the last smoothed envelope. Frame
positions and corresponding values of second derivative ex-
tremes are stored. Afterwards, these characteristic points are
sorted by the second derivative modulus, and the highest po-
sitions are selected to build up the set of characteristic points .
Of course, when the total number of third derivative zero-cross-
ings is less than , the set is shortened.
Both note onset and offset are added as characteristic points
to the set . The slope defined by each pair of consecutive char-
acteristic points on the envelope is computed (3), where and
denote frame positions. A minimum slope duration (measured
in frames) is defined relative to the note duration as the
five per cent of the note length for excluding the possible too
high valued slopes near the note limits
such as
(3)
Finally, the two pairs of points defining, respectively, the
most positive and most negative slope values from the re-
maining slopes after discarding are extracted. The end of the
attack segment is defined as the frame position corre-
sponding to second point of the maximum slope, while the start
of the release segment position is defined as the first point
of the minimum slope. This is stated in
(4)
(5)
The attack is defined as the segment between the note onset
and the end of the most positive of the computed slopes, while
the release segment is defined as the segment between the start
of the most negative of the computed slopes and the note offset.
Sustain is restricted to the remaining segment. When the end
of attack and the start of release limits of a note coincide, it is
considered that the note does not have a sustain segment.
Intra-Note Segment Characterization: Once we have found
the intra-note segment limits, we describe each one by its dura-
tion (absolute and relative to note duration), start and end times,
initial and final energy values (absolute and relative to note max-
imum) and slope. For the stable part of each note (sustain seg-
ment), we extract an averaged spectral centroid and spectral tilt
in order to have timbral descriptors related to the brightness of
a particular execution. We compute the spectral centroid as the
frequency bin corresponding to the barycenter of the spectrum,
expressed as (6), where is the fast fourier transform of a
frame, is the size of the fast fourier transform, and is the
bin index. For the spectral tilt, we perform a linear regression of
the logarithmic spectral envelope between 2 and 6 kHz, and get
the slope expressed in decibels per hertz (dB/Hz)
(6)
IV. PERFORMANCE-DRIVEN INTERPRETER IDENTIFICATION
In this section, we describe our approach to the problem of
recognizing famous saxophonists from their playing style. In
particular, we introduce the different note descriptors we use
to characterize the internal and inter-note note properties (com-
puted as described in the previous section), as well as the dif-
ferent algorithms we apply to identify performers from their
playing style.
A. Note Descriptors
We characterize each performed note by the following two
sets of features:
RAMIREZ et al.: PERFORMANCE-BASED INTERPRETER IDENTIFICATION IN SAXOPHONE AUDIO RECORDINGS 361
Fig. 4. Energy envelope and its linear approximation of a real excerpt with intra-note segment limits marked.
• Intra-note features. The intra-note features represent the
internal structure of a note which is specified as intra-note
characteristics of the audio signal. The set of intra-note
features we have included in the research reported here
are the note’s attack level, sustain duration, sustain slope,
amount of legato with the previous note, amount of legato
with the following note, mean energy, spectral centroid and
spectral tilt. This is, each performed note is characterized
by the tuple
• Inter-note features. The inter-note features represent both
properties of the note itself and aspects of the musical con-
text in which the note appears. Information about the note
includes note pitch and note duration, while information
about its melodic context includes the relative pitch and du-
ration of the neighboring notes (i.e., previous and following
notes) as well as the Narmour structures to which the note
belongs. The note’s Narmour structures are computed by
performing the musical analysis described in Section III-A.
Thus, each performed note is contextually characterized by
the tuple
B. Algorithm
One of the first questions to be asked before attempting to
build a system to automatically identify a musician by his or
her playing style is how is this task performed by a music ex-
pert? In the case of Jazz saxophonists our hypothesis is that most
of the cues for performer identification come from the timbre
or “quality” of the notes performed by the saxophonist. That is
to say, while timing information is certainly important and is
useful to identify a particular musician most of the information
relevant for identifying a performer is the timbre characteristics
of the performed notes. In this respect, the saxophone is similar
to the singing voice in which most of the information relevant
for identifying a singer is simply his or her voice’s timbre. Thus,
the algorithm to identify performers from their playing style re-
ported in this paper aims to detect patterns of notes based on
their timbre content. Roughly, the algorithm consists of gener-
ating a performance alphabet by clustering similar (in terms of
timbre) individual notes, inducing for each performer a classi-
fier which maps a note and its musical context to a symbol in the
performance alphabet (i.e., a cluster), and given an audio frag-
ment identify the performer as the one whose classifier predicts
best the performed fragment. More formally, we are ultimately
interested in obtaining a classifier of the following form
where is the set of melody
fragments composed of notes and
is the set of possible saxophonists to be identified. For each
performer to be identified we trained another classifier
of the following form:
where is the set of notes played by per-
former represented by their inter-note features,
i.e., each note in is represented by the tuple
as described before, and
is the set of clusters generated by
clustering all the notes performed (by all performers) using
their intra-note features.
In order to obtain the classifiers and we use and
explore several machine learning techniques. The machine
learning techniques considered in this paper are the following:
K-means clustering, decision trees [16], support vector ma-
chines (SVM) [4], artificial neural networks (ANN) [3], lazy
methods, and ensemble methods.
We segmented all the recorded pieces into audio segments
representing musical phrases. Given an audio fragment denoted
by a list of notes and a set of possible performers
362 IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY, VOL. 17, NO. 3, MARCH 2007
denoted by a list of performers , classifier
identifies the performer as follows:
for each performer
for each note
-
-
for each performer
return such that
This is, for each note in the melody fragment the classifier
computes the set of its intra-note features, the set of its
inter-note features and, based on the note’s intra-note features,
the cluster membership of the note for each of the clusters
( are the cluster membership for clusters ,
respectively). Once this is done, for each performer its
trained classifier predicts a cluster representing
the expected type of note the performer would have played in
that musical context. This prediction is based on the note’s
inter-note features. The score for each performer
is updated by taking into account the cluster membership of
the predicted cluster (i.e., the greater the cluster membership
of the predicted cluster, the more the score of the performer
is increased). Finally, the performer with the higher score is
returned.
Clearly, the classifiers play a central role in the output
of classifier . For each performer, is trained with data
extracted from the performer’s performance recordings. We
have explored different classifier induction methods (described
above) for obtaining each classifier . The whole procedure
for training classifiers is as follows.
1) Collect all training recordings by all performers.
2) Segment notes in the training recordings.
3) For each segmented note , compute its intra-note descrip-
tion .
4) Using the intra-note description of all segmented notes,
apply fuzzy -means clustering (resulting in clusters of
notes, each cluster corresponding to a set of similar notes
in terms of their intra-note description).
5) For each performer :
• collect training recordings for that performer;
• for each segmented note in the performer’s record-
ings, compute ’s inter-note description ;
• build a classifier (e.g., a decision tree) using the inter-
note features as attributes and its cluster (computed in
step 4) as class.
6) return the resulting classifier (e.g., the decision tree)
for each performer .
The motivation for inducing the classifiers as described above
is that we would like to devise a mechanism to capture which
(perceptual) type of notes are played in a particular musical con-
text by a performer. By clustering the notes of all the performers
based on the notes’ intra-note features, we intend to obtain a
number of sets, each containing perceptually similar notes (e.g.,
notes with similar timbre). By building a decision tree based on
the inter-note features of the notes of a performer, we intend to
obtain a classifier which predicts what type of notes a performer
performs in a particular musical context.
C. Evaluation
We evaluated the induced classifiers by performing the stan-
dard ten fold cross validation in which 10% of the melody frag-
ments is held out in turn as test data while the remaining 90% is
used as training data. When performing the ten fold cross vali-
dation, we leave out the same number of melody fragments per
class. In order to avoid optimistic estimates of the classifier per-
formance, we explicitly remove from the training set all melody
fragment repetitions of the hold out fragments. This is motivated
by the fact that musicians are likely to perform a melody frag-
ment and its repetition in a similar way. Thus, the applied ten
fold cross validation procedure, in addition to holding out a test
example from the training set, also removes repetitions of the
example.
V. CASE STUDY
Important forms of performance in Western tonal music in-
clude performing music following a score, performing music
by heart, performing improvised melodies, and playing by ear.
With exception of the first form of performance, in the other
forms of performance there is no notation (e.g., score) avail-
able. The task of identifying performers using the expressive in-
formation in their performances is only realistic if we consider
performances for which we do not have the score the musician
followed to produce the performance. Thus, the question is: how
to characterize the events in an expressive performance in order
to capture their intra-note features and the musical context in
which they appear? Our approach to this question is to study the
intra-note features of an expressive performance by analyzing
each note in a performance and building a performance alphabet
of events, and by mapping the musical context in which the note
appears to the symbols in the alphabet. In this way, we are able
to describe a performance as a sequence of symbols in the per-
formance alphabet and to characterize the musical context in
which these symbols appear. A second question is: how to use
this characterization in order to identify a musician in a new per-
formance? Our approach to this question is to encode the new
performance as a string of symbols in the performance alphabet
and then to compare this string with the sequence of symbols
each performer is expected to play.
In this section, we present a case study on identifying per-
formers from their playing style. We consider a set of mono-
phonic recordings performed by reading a music score. Note
that the availability of the score allows a complete analysis of
the musical context of each performed note and enables us to
establish a very complete mapping from this context to partic-
ular expressive transformations. However, in order to obtain a
RAMIREZ et al.: PERFORMANCE-BASED INTERPRETER IDENTIFICATION IN SAXOPHONE AUDIO RECORDINGS 363
TABLE I
CLASSIFICATION ACCURACY FOR THE 1-NOTE, SHORT-PHRASE, AND
LONG-PHRASE CASES (IN CORRECTLY CLASSIFIED INSTANCES PERCENTAGE)
unified methodology (in other case studies the score of the per-
formance may not necessarily be available) we decided to dis-
card the information provided by the score.
A. Monophonic Performances
Training Data: The training data used in this case study are
monophonic recordings of four Jazz standards (Body and Soul,
Once I loved, Like Someone in Love, and Up Jumped Spring)
performed by three different professional saxophonists in a con-
trolled studio environment. Each piece was performed at two
different tempos. For each note in the training data, its inter-note
and intra-note features were computed.
Results: There were a total of 792 notes available for each
performer. We segmented each of the performed pieces in
phases and obtain a total of 120 short phrases and 32 long
phrases for each performer. The length of the obtained phrases
and long phrases ranged from 5 to 12 notes and 40 to 62
notes, respectively. The expected classification accuracy of the
default classifier (one which chooses randomly one of the three
performers) is 33% (measured in correctly classified instances
percentage). In the short phrase case, the average accuracy and
the accuracy obtained for the most successful trained classifier
was 97.03% and 98.42%, respectively. In the short long case,
the average accuracy and the accuracy obtained for the most
successful trained classifier was 96.77% and 98.07%, respec-
tively. The correctly classified instances percentage for each
learning method is presented in Table I. Clearly, the results
for short and long phrases are statistically significant which
indicates that it is indeed feasible to train successful classifiers
to identify performers from their playing style using the consid-
ered intra-note and inter-note features. It must be noted that the
performances in our training data were recorded in a controlled
environment in which the gain level was constant for each
performer. Some of the features (e.g., attack level) included in
the intra-note description of the notes take advantage of this
property and provide very useful information in the learning
process. This recording requirement is not realistic in a general
setting where we may obtain performances recorded under very
different circumstances. However, we have tested our algorithm
with performances recorded under different circumstances and
obtained similar results.
B. Discussion
The difference between the results obtained in the case study
and the accuracy of a baseline classifier, i.e., the classifier
guessing at random, indicates that the intra-note and inter-note
features presented contain sufficient information to identify
the studied set of performers, and that the machine learning
methods explored are capable of learning performance pat-
terns that distinguish these performers. It is worth noting that
every learning algorithm investigated (decision trees, SVM,
ANN, k-NN, and the reported ensemble methods) produced
significantly better than random classification accuracies. This
supports our statement about the feasibility of training suc-
cessful classifiers for the case study reported. However, note
that this does not necessary imply that it is feasible to train
classifiers for arbitrary performers.
We have selected three types of musical segment lengths:
1-note segments, short-phrase segments (4–12 notes), and long-
phrase segment (30–62 notes). As expected, evaluation using
1-note segments results in poor classification accuracies, while
short-phrase segments and long-phrase segment evaluation re-
sults in accuracies well above the accuracy of a baseline clas-
sifier. Interestingly, there is no substantial difference in the ac-
curacies for short-phrase sand long-phrase segment evaluation
which seems to indicate that in order to identify a particular per-
former it is sufficient to consider a short phrase segment of the
piece, i.e., the identification accuracy does not increase substan-
tially by considering a longer segment.
VI. CONCLUSION
In this paper, we focused on the task of identifying performers
from their playing style using note descriptors extracted from
audio recordings. In particular, we concentrated in identi-
fying Jazz saxophonists and explored and compared different
machine learning techniques for this task. We characterized
performances by representing each note in the performance by a
set of intra-note features corresponding to the internal structure
of the note, and a set of inter-note features representing the
context in which the note appears. We presented successful
classifiers for a three-class classification task: identifying sax-
ophonists in monophonic performances. The results obtained
indicate that the intra-note and inter-note features presented
contain sufficient information to identify the studied set of
performers, and that the machine learning methods explored
are capable of learning performance patterns that distinguish
these performers. We are currently extending our approach
to performance-based performer identification in polyphonic
multi-instrument audio recordings.
REFERENCES
[1] E. Van Baelen and L. De Raedt, “Analysis and prediction of piano per-
formances using inductive logic programming,” in Int. Conf. Inductive
Logic Programming, 1996, pp. 55–71.
[2] R. Bresin, “Articulation rules for automatic music performance,”
in Proc. Int. Computer Music Conf., San Francisco, CA, 2002, pp.
294–297.
[3] Y. Chauvin et al., Backpropagation: Theory, Architectures and Appli-
cations. Hillsdale, NJ: Lawrence Erlbaum, 1995.
364 IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY, VOL. 17, NO. 3, MARCH 2007
[4] N. Cristianini and J. Shawe-Taylor, An Introduction to Support Vector
Machines. Cambridge, U.K.: Cambridge Univ. Press, 2000.
[5] M. J. Dovey, “Analysis of Rachmaninoff’s piano performances using
inductive logic programming,” in Proc. Eur. Conf. Machine Learning,
1995, pp. 279–282.
[6] A. Friberg, R. Bresin, L. Fryden, and J. Sunberg, “Musical punctuation
on the microlevel: automatic identification and performance of small
melodic units,” J. New Music Res., vol. 27, no. 3, pp. 217–292, 1998.
[7] A. Gabrielsson, “The performance of music,” in The Psychology of
Music, D. Deutsch, Ed., 2nd ed. New York: Academic, 1999.
[8] K. Jenssen, “Envelope model of isolated musical sounds,” in Proc.
COST G-6 Workshop on Digital Audio Effects (DAFx), Trondheim,
Germany, 1999, pp. 35–40.
[9] A. Klapuri, “Sound onset detection by applying psychoacoustic knowl-
edge,” in Proc. IEEE Int. Conf. Acoustics, Speech a nd Signal Process.,
ICASSP, 1999, pp. 3089–3092.
[10] R. Lopez de Mantaras and J. L. Arcos, “AI and music, from compo-
sition to expressive performance,” AI Magazine, vol. 23-3, pp. 43–57,
2002.
[11] R. C. Maher and J. W. Beauchamp, “Fundamental frequency esti-
mation of musical signals using a two-way mismatch procedure,” J.
Acoust. Soc. Amer., vol. 95, pp. 2254–2263, 1994.
[12] R. J. McNab, L. A. Smith, and I. H. Witten, “Signal processing for
melody transcription,” in Proc. 19th Aust. Comp. Sci. Conf., 1996, pp.
301–307.
[13] T. M. Mitchell, Machine Learning. : McGraw-Hill, 1997.
[14] E. Narmour, The Analysis and Cognition of Basic Melodic Structures:
The Implication Realization Model. Chicago, IL: Univ. Chicago
Press, 1990.
[15] ——, The Analysis and Cognition of Melodic Complexity: The Impli-
cation Realization Model. Chicago, IL: Univ. Chicago Press, 1991.
[16] J. R. Quinlan, C4.5: Programs for Machine Learning. San Francisco,
CA: Morgan Kaufmann, 1993.
[17] R. Ramirez, A. Hazan, E. Maestre, and X. Serra, “A data mining
approach to expressive music performance modeling,” in Multimedia
Data Mining and Knowledge Discovery. New York: Springer-Verlag,
2006.
[18] R. Ramirez and A. Hazan, “Modeling expressive music performance
in jazz,” in Proc. 18th Florida Artificial Intelligence Research Society
Conf. (FLAIRS 2005), Clearwater Beach, FL, 2005, pp. 86–91.
[19] ——, “A learning scheme for generating expressive music perfor-
mances of jazz standards,” in Proc. Int. Joint Conf. Artif. Intell.,
Edinburgh, U.K., 2005, pp. 1628–1629.
[20] B. H. Repp, “Diversity and commonality in music performance: an
analysis of timing microstructure in Schumann’s Traumerei’,” J.
Acoust. Soc. Amer., vol. 104, pp. 2546–2568, 1992.
[21] C. Saunders, D. Hardoon, J. Shawe-Taylor, and G. Widmer, “Using
string kernels to identify famous performers from their playing style,”
presented at the 15th Eur. Conf. Mach. Learning, Pisa, Italy, 2004.
[22] C. E. Seashore, Ed., Objective Analysis of Music Performance. Iowa
City: Univ. Iowa Press, 1936.
[23] X. Serra and S. Smith, “Spectral modeling synthesis: a sound anal-
ysis/synthesis system based on a deterministic plus stochastic decom-
position,” Comp. Music J., vol. 14, no. 4, pp. 12–24, 1990.
[24] E. Stamatatos and G. Widmer, “Automatic identification of music per-
formers with learning ensembles,” Artif. Intell., vol. 165, no. 1, pp.
37–56, 2005.
[25] A. Tobudic and G. Widmer, “Relational IBL in music with a new struc-
tural similarity measure,” in Proc. Int. Conf. Inductive Logic Program-
ming, 2003, pp. 365–382.
[26] N. Todd, “The dynamics of dynamics: a model of musical expression,”
J. Acoust. Soc. Amer., vol. 91, pp. 3540–3550, 1992.
[27] G. Widmer, “Discovering strong principles of expressive music perfor-
mance with the PLCG rule learning strategy,” in Proc. 12th Eur. Conf.
Mach. Learning (ECML’01), 2001, pp. 552–563.
[28] I. H. Witten, Data Mining, Practical Machine Learning Tools and Tech-
niques With Java Implementation. San Francisco, CA: Morgan Kauf-
mann, 1999.
[29] A. D. Bernstein and E. D. Cooper, “The piecewise-linear technique
of electronic music synthesis,” J. Audio Eng. Soc., vol. 24, no. 6, pp.
446–454, 1976.
[30] P. N. Juslin and J. A. Sloboda, Musical Emotion: Theory and Re-
search. New York: Oxford Univ. Press, 2001.
[31] R. A. Kendall and E. C. Carterette, “The communication of musical
expression,” Music Perception, vol. 8, no. 2, pp. 129–164, 1990.
[32] W. Apel, Harvard Dictionary of Music. Cambridge, MA: Harvard
Univ. Press, 1972.
Rafael Ramirez received the B.Sc. degree in math-
ematics from the National University of Mexico,
Mexico City, and the M.Sc. degree in artificial in-
telligence and the Ph.D. degree in computer science
from The University of Bristol, Bristol, U.K.
He is currently an Assistant Professor at the
Technology Department, Pompeu Fabra University,
Barcelona, Spain. Prior to joining the Pompeu Fabra
University, he was a Lecturer at the Department
of Computer Science in the National University
of Singapore, Singapore, and a Researcher at the
National Research Institute in Computer Science (INRIA), France. His research
interests include artificial intelligence, music information retrieval, declarative
languages, and music perception and cognition.
Esteban Maestre received the M.A. degree in
electrical engineering from the Universitat Politèc-
nica de Catalunya, Catalunya, Spain, in 2003. He
is currently working towards the Ph.D. degree in
computer science and digital communication in the
Technology Department, Pompeu Fabra University,
Barcelona, Spain.
After a research internship at Philips Research
Laboratories, Aachen, Germany, he worked as a
Teaching Assistant in the Electronics Department at
the Universitat Politècnica de Catalunya until 2004.
Then, he joined the Music Technology Group, Pompeu Fabra University, as a
Research Assistant, working also as a Teaching Assistant in the Technology
Department. His research interests span from expressive audio analysis and
synthesis to musical gestures coding and rendering.
Antonio Pertusa received the B.Sc. degrees in com-
puter science and in computer systems, both in 2001,
and the M.Sc. degree in computer science from the
University of Alicante, Alicante, Spain, in 2003.
After receiving the M.Sc. degree, he joined the De-
partment of Software and Computing Systems (De-
partamento de Lenguajes y Sistemas Informáticos),
University of Alicante, where he is currently an As-
sistant Lecturer. He belongs to the Computer Music
Laboratory, which is part of the Pattern Recognition
and Artificial Intelligence Group, and he is member
of the Spanish Association of Pattern Recognition and Image Analysis. His re-
search interests include machine learning, music information retrieval, signal
processing, and music perception and modeling.
Emilia Gómez graduated as a Telecommunication
Engineer specialized in signal processing from the
Universidad de Sevilla, Sevilla, Spain. Then, she
received the DEA degree in acoustics, signal pro-
cessing and computer science applied to music, from
the IRCAM, Paris, France, and the Ph.D. degree in
computer science and digital communication from
the Pompeu Fabra University (UPF), Barcelona,
Spain, on the topic of tonal description of music
audio signals.
She is a researcher at the Music Technology
Group, UPF. During her doctoral studies, she was a Visiting Researcher at the
Signal and Image Processing (TSI) group of the École National Supérieure de
Télécommunications (ENST), Paris, France, and at the Music Acoustics Group
(TMH) , Stockholm Institute of Technology, Sweden.
Xavier Serra received the Ph.D. degree from Stan-
ford University, Stanford, CA, in 1989 with a disser-
tation focused on spectral modeling of musical sig-
nals.
He is currently an Associate Professor at the De-
partment of Technology, Pompeu Fabra University
(UPF), Barcelona, Spain. He is the Director of the
Music Technology Group , UPF, a group dedicated
to audio content analysis, description, and synthesis.
