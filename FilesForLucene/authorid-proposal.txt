CRI: Collaborative Research: Planning Proposal: CommunityResources for Research in Automated Authorship Attribution1 OverviewHomeland security and the criminal and civil justice systems increasingly require reliable and validmethods to identify the authors of anonymous documents. Further demand arises from elds asdiverse as computer forensics and literary studies. However, despite the clear and growing need formethodological research in this arena, there are as yet no standard test suites for authorship attri-bution, and hence no agreed upon ways to compare research results and validate techniques. Thissituation, combined with the highly interdisciplinary nature of the eld, has led to much redun-dant and sometimes unsound research. We therefore propose a one-year NSF-suppported projectto develop a detailed plan for a test corpus and associated resources for research on automatedauthorship attribution. We envisage enabling future proposals that will seek funding from multiplesources to implement the plan.Individuals have distinctive ways of speaking and writing, and there is a long history of linguis-tic and stylistic investigation into authorship attribution. In recent years, practical applicationsfor authorship attribution have grown in areas such as intelligence (linking intercepted messages toeach other and to known terrorists), criminal law (identifying writers of ransom notes and harrass-ing letters), civil law (copyright and estate disputes), and computer security (tracking authors ofcomputer virus source code). This activity is part of a broader growth within compute science ofidentication technologies, including biometrics (retinal scanning, speaker recognition, etc.), cryp-tographic signatures, intrusion detection systems, and others. As with these other technologies,there is increasing interest in the accuracy and scientic validity of determinations of authorshipby both human experts and automated methods.Automating authorship attribution promises more accurate results and objective measures ofreliability, both of which are critical for legal and security applications. Recent research has usedtechniques from machine learning [3, 16, 21, 41, 57], multivariate and cluster analysis [35, 36, 12, 30],and natural language processing [5, 56] in authorship attribution. These techniques have also beenapplied to related problems such as genre analysis [4, 1, 8, 23, 34, 56] and author proling (such asby gender [2, 20] or personality [52]). This work is scattered among the literatures of several areasof computer science (machine learning, computational linguistics, computer forensics), as well asstatistics, linguistics, and literary analysis. Methodological inconsistencies, unsettled controversies,and redundancy are rife.We propose to advance authorship attribution research by constructing a detailed plan fordeveloping community resources. The plan will be developed in concert with leading authorshipresearchers from a range of elds and reviewed at a workshop held towards the end of the projectperiod. The plan will form the basis of one or more funding proposals by ourselves and others toconstruct these community resources. The process of creating the plan will itself draw together theauthorship attribution community and aid research progress.Our focus will be the design of a collection or corpus of texts of known authorship, for use as aresearch/development testbed, along with the denition of standard attribution tasks for comparingtechniques. Use of a standardized corpus and standardized tasks will enable objective comparisonsbetween dierent techniques and provide for validated estimates of reliability (as required by legal
1
standards of evidence, for example).Designing a corpus adequate for testing hypotheses and drawing supportable conclusions inauthorship attribution is a complex and subtle task. How language is used in a text is aected bymany factors besides the identity of the author, including the text's topic, genre (news, businessreport, ction), purpose (information, persuasion, entertainment), time of composition, intendedaudience (broad/specic, young/old, etc.), medium (speech, book, email), and others. Past researchhas sometimes ignored these factors, leading to misleading conclusions. Properly controlling forthese factors, and drawing correct conclusions about their impact, requires a large and carefullyselected range of texts. Negotiating access to materials will be a major issue, as will formattingdata for ease of use and to omit unrealistic cues to authorship.Another important area which we will address is determining exactly which tasks should bedened and supported by the corpus. Dierent identication tasks include (i) identifying which ofa (small or large) set of authors wrote a text, (ii) determining whether two texts were written bydierent authors, and (iii) determining if a text was written by none of a set of dened authors. Avariety of `proling' tasks may also be considered, such as determining the age, dialect, or genderof an author. Some such tasks may require additional annotation of the corpus, and all will aectthe choice of what sorts of texts to include.Finally, we will consider what activities beyond corpus design and construction should be as-sociated with the project. Formal evaluations (with or without associated meetings), periodicreleases of new data, updating and improving content and annotations, and support of hosting andmaintenance are all issues that will be discussed.A good precedent for our proposal is Sparck Jones and van Rijsbergen's Report on the Need forand Provision of an `Ideal' Information Retrieval Test Collection, funded by the British Libraryin 1975. This report had a major inuence on the design of the DARPA/ARPA Tipster [19] andNIST TREC (Text REtrieval Conference) [27] test collections in the early 1990s. These collectionsand associated evaluations led to new text retrieval techniques usable on millions of heterogeneousdocuments. Later TREC and CLEF1 collections have unied and stimulated research communitiesworking on text ltering, speech and video retrieval, cross-language retrieval, and more. TheTREC question answering track has been particularly successful at bringing together a diverse setof researchers in natural language processing, information retrieval, databases, machine learning,and statistics, signicantly boosting research progress on the problem.Corpus and testbed building eorts in other elds have had similar positive impacts. Thedata sets produced by the Linguistic Data Consortium have been used in hundreds of studiesand have had a major impact in the explosion of high-quality empirical and statistical work incomputational linguistics [15]. The UCI repositories for machine learning and data mining havehad a similarly signicant impact on their elds [10, 7]. The Reuters collections [39] have helpedturn text categorization from a niche application into a thriving research and operational eldbringing together machine learning and information retrieval. Our goal is to similarly unify andgalvanize the authorship attribution research community and enable rapid research progress.Proposed Activities: To ensure that the proposed planning project addresses the needs ofthe authorship attribution community, we have recruited an advisory Working Group of eminentcomputer scientists, statisticians, linguists, and literary scholars who actively work on authorship1Cross-Language Evaluation Forum, sponsored by an international consortium of national organizations and in-stitutes in the US and Europe. 2
attribution and related problems. In consultation with this Working Group, we will undertakethe following activities during the project period to gather information and devise a draft planfor development of community resources. We will survey both what resources already exist andwhat practitioners feel they need. We will also assess the costs of obtaining, cleaning, annotating,distributing, and maintaining corpus data, as well as legal issues such as copyright. The plan wedevelop will be discussed and revised at a workshop held towards the end of the project period, withthe Working Group and other invited researchers. The revised plan will then be disseminated, andwill serve as the basis for funding proposals to develop the planned resources. The workshop willbe hosted by the Center for Discrete Mathematics and Theoretical Computer Science at RutgersUniversity (DIMACS), which has nearly two decades of experience organizing and hosting similarworkshops.Intellectual Merit and Broader Impacts: The proposed planning project will help unifyresearchers working on authorship attribution, encouraging greater communication between re-searchers coming from dierent elds. The corpus itself, once developed, will facilitate both morework in authorship attribution, and better work, with deeper understanding resulting from morerigorous and quantitative results. This work will both contribute to and benet from an increasinglysophisticated understanding of other identication technologies, such as biometrics, authenticationprotocols, and computer intrusion detection.More systematic research on authorship attribution is likely to produce results of broad interestin information retrieval, natural language processing, machine learning, statistics, and related elds.Authorship attribution work forces researchers to confront a range of challenges very dierent fromthose in, for instance, content-oriented text classication or information extraction. This synergywill be enhanced by the highly interdisciplinary nature of the eld, which already involves computerscientists, statisticians, linguists, and literary theorists. Creating a common testbed will encouragediscussion and cross-fertilization of ideas from these disparate elds.The community resources we propose to design will also have a substantial impacts on applica-tions of authorship attribution. Intelligence analysis, criminal investigations, commercial disputeresolution, and literary scholarship will all benet from more eective and more rigorously eval-uated authorship attribution technology. The resources may also stimulate the development ofentirely new technologies based on linguistic analysis of style (e.g. in marketing).2 Background2.1 Computational stylistics and forensic authorship analysisOur long-range goal is to develop community resources to support research on automated authorshipattribution and related problems. To place this proposal in context, we review briey the relevanthistory and methods of research on computational and forensic stylistics.Scientic investigation into measuring style and authorship of texts goes back to the late nine-teenth century, with the pioneering studies of Mendenhall [47] and Mascol [43, 44] on distributionsof sentence and word lengths in works of literature and the gospels of the New Testament.The underlying notion was that works by dierent authors are strongly distinguished by quan-tiable features of the text. By the mid-twentieth century, this line of research had grown into whatbecame known as \stylometrics", and a variety of textual statistics had been proposed to quantify3
textual style. The style of early work was characterized by a search for invariant properties oftextual statistics, such as Zipf's distribution [60] and Yule's K statistic [59].Modern work in authorship attribution (often referred to in the humanities as \nontraditionalauthorship attribution") was ushered in by Mosteller and Wallace in the 1960s, in their seminalstudy The Federalist Papers [49]. The study examined 146 political essays from the late eighteenthcentury, of which most are of acknowledged authorship by John Jay, Alexander Hamilton, andJames Madison, though twelve are claimed by both Hamilton and Madison. Mosteller and Wallaceshowed statistically signicant discrimination results by applying Bayesian statistical analysis tothe frequencies of a small set of `function words' (such as `the', `of', or `about'), as stylistic featuresof the text. Function words, and other similar classes of words, remain the most popular stylisticfeatures used for authorship discrimination.Other stylometric features that have been applied include various measures of vocabulary rich-ness and lexical repetition, based on Zipf's [60] studies on word frequency distributions. Most suchmeasures, however, are strongly dependent on the length of the text being studied, and so aredicult to apply reliably. Many other types of features have been applied, including word classfrequencies [2, 24], syntactic analysis [5, 56], word collocations [30, 55], grammatical errors [38], andword, sentence, clause, and paragraph lengths [3, 42]. Many studies combine features of dierenttypes using multivariate analysis techniques.One eective technique, pioneered for authorship studies by Burrows [12, 13], is to use principalcomponents analysis (PCA) to nd combinations of style markers that can discriminate betweena particular pair (or small set) of authors. This method has been used to good eect in severalstudies, including [29] and [5]. Another related class of techniques that have been applied aremachine learning algorithms (such as Winnow [40] or Support Vector Machines [17]) which canconstruct discrimination models over large numbers of documents and features. Such techniqueshave been applied widely in topic-based text categorization (see the excellent survey [54]) andother stylistic discrimination tasks (e.g. [2, 37, 56]), as well as for authorship discrimination [3, 21].Often, studies have relied on intuitive evaluation of results, based on visual inspection of scatter-plots and cluster-analysis trees, though recent work (e.g. [3, 20, 21]) has begun to apply somewhatmore rigorous tests of statistical signicance and cross-validation accuracy. The proposed projectwill encourage the development of more rigorous and widely-accepted evaluation standards forauthorship studies.A somewhat controversial technique is that of Cumulative Sums (CUSUM), developed by Mor-ton and Farringdon [48, 22]. This method seeks to analyze whether or not a given document is ofhomogeneous authorship, or was written by multiple authors (to do authorship attribution, then,the test document is simply concatenated with documents of known authorship). A graph is con-structed showing the cumulative deviations of dierent marker features (typically sentence length,and frequency of short words per sentence) from their overall means in the document. The claimis that if dierent authors wrote dierent parts of the document, the dierent curves will diverge.This is somewhat dependent, however, on the analyst's reading of the graph, and the method hasbeen criticized as unsound and ineective in the literature [6, 28], though it has been accepted asexpert evidence in British courts. A standardized corpus will help resolve such controversies.Indeed, in forensic text analysis, where quantiable eectiveness is now a sine qua non (due tothe Daubert decision|see below), disputes have become heated (see [14, 26, 45]). These disputesconcern the use of dierent feature sets, analysis techniques, and evaluation methodologies. Thestandard forensic approach (see [46]) for determining if a candidate author wrote a test document4
consists of (a) choosing an appropriate `confusion set' of comparison documents by other authors(controlling for dialect, genre, and register), (b) examining a variety of textual features to nd thosethat consistently dierentiate between the candidate and other authors, (c) determining which ofthose occur in the test document, and (d) performing statistical tests to see if the characteristics ofthe test document can be attributed to chance. There is a great deal of expert judgement involvedin this sort of procedure, which would be acceptable if there were generally accepted standards inthe eld. However, there is very little agreement in the eld over what level of control is necessaryin the confusion set, what sorts of features are most reliable, what statistical tests to use, andmost critically, how to determine the reliability of a proposed attribution. This state of aairsunderscores the necessity both for a standard corpus and tasks, as well as for forums that bringtogether researchers to share results.2.2 Need for reliable authorship attributionAccuracy in both attribution, and in estimating the reliability of attribution, are essential forautomated authorship attribution to meet its promise in real-world tasks. However, estimatingrelability will be possible only after large-scale systematic research on attribution ecacy in dif-ferent scenarios. The community resources we propose to design will enable such research to beundertaken.A key application is homeland security, where authorship attribution by human experts (sayof intercepted communications), can be prohibitively costly. Informal language-based authorshipanalysis has in the past proven important for identifying terrorists; for example, Ted Kaczinsky, theUnabomber, was found in part because a relative recognized his writing style. On a larger scale,automated authorship analysis might be combined with other data mining techniques in analyzingdocument collections such as those seized in Iraq and Afghanistan. To combine information fromauthorship analysis with other, perhaps conicting, data, it will be important for to have a clearmeasure of attribution reliability,Human authorship analysis has been important in a number of criminal cases (for example theransom note in the still unsolved JonBenet Ramsey case [31]). Such cases are currently addressedby expert forensic linguists, whose eld which has been growing in prominence [46]. A fundamentaldiculty in the forensic application of authorship attribution, however, is determining and de-scribing reliability of the evidence. In contrast to, for instance, DNA analysis, accurate statisticaldeterminations of attribution reliability are not currently possible for human linguistic analysis.This concern was deepened by the decision of the United States Supreme Court in Daubert v.Merrell Dow Pharmaceuticals, 509 U.S. 579 (1993), which established a test for expert testimonywhich includes (among others), \whether the theory or technique in question can be (and hasbeen) tested, ... its known or potential error rate, and the existence and maintenance of standardscontrolling its operation". A carefully designed corpous holds the promise that methods can bevalidated to satisfy this test.Potential commercial applications of automated authorship attribution include automated de-tection of copyright infringement and plagiarism (a problem that has grown with the Internet), andanalysis of customer communications. Sophisticated authorship analysis tools may help improvethe readability of collaborative texts, by `smoothing out' their style [25], and may also contributeto development of more sophisticated information retrieval systems.Finally, as the history of statistical authorship analysis shows, reliable techniques in this area
5
have the potential to contribute to scholarly debates about the authorship of historical documents,and in turn to improve our understanding of past events.3 Methodological IssuesAs we hope is now clear, authorship attribution is a complex interdisciplinary problem, to which agreat variety of approaches have been applied. The nature of the problem has led to fragmentationin the eld, as linguists, literary scholars, statisticians, computer scientists, and historians haveall worked on authorship attribution, using dierent methods, making dierent assumptions, andpublishing in a large number of dierent journals. As Rudman notes in his recent critique of theeld [53], such fragmentation has led to much methodological irregularity.There have been previous small-scale attempts to share data sets and encourage commonmethodology among authorship attribution researchers. Most notable is a recent authorship attri-bution \competition" organized by Patrick Juola and Stephen Ramsay [33, 32]. It followed goodpractice in dening separate training and test sets, and included several types of material (essays,novels, poetry, plays, and speech transcripts). However, the amount of data was tiny, with only98 test documents spread across 13 subtasks and 5 languages. Only one of the subtasks allowedfor textual controls (see below) and that one only for topic. While shared data sets and smallevaluations such as this have given some insight into the merits of dierent authorship attributionmethods, they have not led to signicant progress in authorship attribution technology, nor allowedreliability estimates to be developed suitable for legal testimony and critical applications.The main goal of this planning proposal is working towards developing research community re-sources to help remedy such irregularities. Methodological diculties in current authorship researchcan be roughly divided into four categories:Corpus design: Standard authorship attribution studies begin with a corpus of documents ofknown authorship. This is complicated, however, by the possibility of collaboration and edit-ing, which are not always controlled for. Indeed, Rudman [53] provides several examples ofpublished authorship attribution studies where the provenance of the documents is question-able. One issue concerns multiple editions of the same document. For example, the word\further" appears only once in the rst edition of Daniel Defoe's \Memoirs of an EnglishOcer" but appears 27 times in a later denition. Other issues include edited documentsand multi-authored documents. When dealing with historical documents, orthographic con-ventions can dier between dierent editions, such as the common interchange of `u' and `v'in Elizabethan English|standardizing all documents used in a study is time-consuming andexpensive, but crucial. Unfortunately, many studies have used just the most easily availableeditions, for convenience, making their results suspect.A large, publicly available corpus containing veriably single-authored documents of knownprovenance, standardized for formatting and orthography would advance the eld of author-ship attribution considerably, by enabling comparison of properly controlled studies.Proper and complete use of textual controls: Picking appropriate control documents presentsa signicant challenge in authorship attribution studies. For example consider documents bya sports journalist, say John, writing about the exploits of Babe Ruth in the 1920's. Supposewe have a challenge document, also about Babe Ruth in the 1920's, but actually written6
by a journalist other than John. If the training corpus contains no articles about baseballor about Babe Ruth or written in the 1920's, then a \John" attribution may be inevitable,no matter how sophisticated the attribution methodology. On the other hand, if the corpuscontains articles by many dierent sports journalists of the period, a correct identicationmay emerge. A substantial corpus with sub-corpora that control for period, topic, and genredoes not currently exist and represents a signicant obstacle.Feature selection: Traditional authorship attribution studies represent documents using so-calledfunction words (such as \and," \of," \the," etc.), as well as numerical statistics such as averageword, sentence, and paragraph length. The idea is that such features are topic independent;should an author write about a new topic in the future, the relative frequencies of the variousfunction words would remain approximately constant. However, dierent researchers haveused dierent function words and the notion of \topic-free" does not admit a simple denition.Argamon et al. [2], for example, have used a list of 450 function words that include \art"and \hopeless." Mosteller and Wallace [49] used a smaller list but included such words as\violence" and \rapid." Novak et al. [50] used all words in the document. Dening topic-free author attribution features is an open problem. In the context of disputed authorship,Rudman [53] warns against picking the features that best align a set of documents with aparticular author, referring to this as \cherry picking."One approach to better understanding the topic-dependence of textual features is systematicexperimentation examining statistics and discrimination utility of various kinds of features ina variety attribution tasks controlled for topic at dierent levels of specicity. Such experi-mentation will be enabled by a large and varied corpus, stratied for topic, as we propose towork towards here.Proper use of algorithms and statistics: According to Rudman, the authorship attributionliterature presents many examples of inappropriate statistical practice such as selecting thestatistical test that gives the desired result and, a more subtle problem, innappriate accountingfor and reporting of, uncertainty. The literature rarely provides estimates of predictive error.A large corpus with specic guidelines concerning test-training splits and cross-validationprocedures can help, as will the development of a more integrated research community.The issues we have just outlined have complex causes. First, construction of properly controlledand balanced corpus presents a signicant challenge. Development of a communal corpus andassociated set of tasks will directly help researchers overcome this diculty. Second, and moreprofoundly, authorship attribution research teams rarely include expertise in all the relevant areasof computer science, statistics, linguistics, and textual scholarship. Hence it is only too natural forshortcuts to be taken in one area or another. Furthermore, the fact that authorship attribution ispursued in several dierent research communities in parallel makes it far too easy for work thatis suspect in some way to get published and accepted by some subset of the eld. Here too webelieve that our proposed program for development of communal resources will help, by providing amechanism for researchers to compare and communicate their results based on a common testbed.In the following sections, we describe in detail the issues that we will explore in the proposedplanning project, and how development of such a corpus will aid research in authorship attribution.
7
4 Constructing an Author Identication CorpusAs discussed in the previous section, a variety of problems have plagued work in authorship at-tribution. We believe that many of these problems result largely from a lack of a critical pieceof research infrastructure, namely open-access, high quality corpora. Corpora are to text analysiswhat sequence databases are to genetics: resources that provide a multiplier eect far in excess ofthe initial investment.A well-designed corpus is a boon to any data intensive eld. Barriers to entering the eldsare reduced and a larger proportion of time and money can be spent on research questions. Ex-perimental results have improved comparability (since data has been prepared the same way) andquality (since the preparation can more easily be done to high standards). The methodologicalissues discussed in the previous section suggest authorship attribution will benet even more thanother elds.This section outlines the main issues that we will explore during the proposed activity, andhow we propose to resolve them, with the active involvement of the project Working Group. Theseissues include: The needs of authorship attribution researchers from dierent elds. What kinds of texts are needed to properly enable comprehensive and controlled experiments|while easily-available texts such as those downloadable from the Web are appropriate for somepurposes, they do not come near to covering the range of kinds of texts used in authorshipanalysis applications; How to preprocess and format texts in the corpus|raw texts are more useful for some pur-poses, while more standardized texts (for spelling, say) may be more appropriate for others; What sort of auxiliary annotations to include, such as word tagging, demographic authorinformation, and the like; Whether and what software should be included with the corpus; What (if any) auxiliary activities should be established in conjunction with the corpus (suchas public evaluations and other dissemination activities); How to deal with possible legal issues such as copyright and licensing; Design of a workable maintenance plan for managing the resources that are constructed overthe long term.4.1 Target user communitiesThere are many research communities whose needs must be considered in designing an authorshipattribution corpus. They include: Forensic linguistics: A primary concern here is improving and validating (ideally to courtroomstandards) the reliability of authorship judgments under conditions encountered in practice.A major question is how reliable can judgments be when the texts available on suspect authorsare few, short, or of dierent character than texts to be identied. These researchers will beparticularly interested in a wide diversity of material in a corpus. Literary stylistics, stylometrics, corpus linguistics, etc.: These are the researchers with themost direct interest in documents as writings. The pedigree, literary merit, and choice of
8
versions of texts are of interest. These researchers may prefer genres (e.g. ction, poetry,historical materials) dierent from those of interest to more applied elds. Machine learning, statistics, pattern recognition, etc.: Researchers in these elds usually viewauthor identication as just one of many benchmark tasks on which to test algorithms. theymay be unfamiliar with text processing, and so providing them corpus texts in the form ofextracted feature values will be of interest. Documentation that makes clear the particularconcerns of authorship attribution and presents standard experimental designs may encouragethem to produce results comparable with those of other authorship attribution researchers. Information retrieval, computational linguistics, etc.: Researchers here have similar intereststo those in machine learning, though they are more likely to do their own text processing.They may nd uses for corpus data besides author identication experiments, and so are morelikely to be interested in the particular genre and origins of texts included. Data mining, text mining, link analysis, collaborative ltering, etc.: These elds are similarto the previous two, but with more emphasis on combining multiple data sources. Researchhere is particular relevant to the application areas of business analytics, law enforcement, andintelligence. Researchers in these elds are particularly interested in data sources beyond theauthored document, such as demographic data on authors, texts they may have read, etc.Data sets that include the complete message trac within some community (e.g. a mailinglist) may be of great interest. Computer security, software forensics, etc.: The interest here is identifying authors of textsin computer-related security and law enforcement: electronic mail, chat, source code, internaldocuments from large organizations, and others. Researchers may desire to use nontextualdata such as message headers, web server logs, and packet traces, some of which may not bepractical to provide in a general authorship attribution corpus.The above descriptions are necessarily oversimplications, and many researchers in these elds haveoverlapping interests. However, the above indicates the range of user concerns that will be takeninto account in the proposed planning activity, as described in the following section.4.2 Choice of Material and Textual ControlAs discussed in Section 3, many factors besides authorship aect the character of a text. Theability to systematically control for these factors in experiments is the single greatest benet awell-designed corpus will provide. Only through such systematic experiments can the accuracy ofauthorship attribution techniques in complex, data poor situations be validated and improved.Genre is perhaps the most critical factor researchers would like to control, since it inuencesmany of the stylistic clues commonly used in determining authorship. Further, forensic authorshipattribution tasks often involves samples of authored material from one genre and material to beidentied from another. (This is almost axiomatic in some cases, e.g. anonymous threats, or suicidenotes.) The range of genres that might be includes is immense: technical articles, short stories,email discussion list postings, home pages, homework assignments, press releases, wills, diaries,interviews, phone messages, source code comments, and scores of others. Members of the working
9
group will draw on an extensive literature on taxonomies and characterizations of genres [9, 51, 18]in proposing genres to include in a corpus.Other factors to control include whether a text had multiple authors, whether the text has beenedited, the topic of the text, the year in which it was produced, and the medium of production(handwriting, typing, audio). The choice of particular languages or dialects to include is important,though for simplicity we will likely focus on English only. The number of authors, and number andlength of documents, will be a major factor in balancing issues of corpus expense, diversity, andusefulness.Demographic characteristics of authors such as age, country of origin, native language, edu-cational history, and profession are of interest for two reasons. First, they inuence style and socontrolling for them is desirable. Second, in some applications it may not be possible to identify aparticular person as author of a text, but determining the likely demographic characteristics of theauthor would still be of value. Producing corpus materials and metadata to allow studies of bothsorts is desirable.Finally, an author may alter their natural style in an attempt to foil identication. Anonymousharrassers are an important law enforcement example as are, to an unknown extent, communicationswithin criminal and terrorist groups. Author identication cases of popular interest (e.g. PrimaryColors) may have this characteristic as well. An intriguing possibility in corpus building would beto solicit deliberately deceptive texts from cooperative writers of various skill levels.4.3 Preprocessing and FormattingA surprisingly subtle question is how to preprocess and format the texts. Two issues predominatehere. First, researchers want formats that make explicit the information they care about, whileremoving extraneous information that complicates processing. However, one researcher's nuisancemay be another's vital clue. A computer forensics expert may be interested in formatting codes inraw word processor les, while a literary stylist may want normalized ASCII text. One researchermay want quoted, non-author material removed from e-mail messages, while another may want tospecically study it.Second, a corpus is constructed from available materials of known authorship, and used tosimulate situations of unknown, disputed, and concealed authorship. A realistic simulation is notalways straightforward, however. Using e-mail messages, for instance, may require the removalnot only of headers, but of signatures, signature blocks, and possibly other self-references by theauthor in running text. If material is taken from electronic conversations (mailing lists, chatrooms) then the name of an author may appear not only in their own texts, but in those of otherauthors. These issues are a particularly concern when applying machine learning techniques, whichmay implicitly use such cues without a researcher realizing it. Dierent researchers will also havedierent perspectives on what information should and shouldn't be available.During the proposed activity, we will obtain samples of material from a variety of sourcesand genres, and estimate what cleanup and annotation will be necessary for dierent purposes.Together with the Working Group, we will produce a plan for dealing with these issues, takinginto account the needs of the various research communities. Various within-le and auxiliary lemarkup strategies will be considered, as will providing several versions of the same data.
10
4.4 Auxiliary MaterialIn addition to texts in appropriate formats, an authorship attribution corpus might contain othermaterials. Some of these have been mentioned already, such as vectors of extracted features anddemographic information on authors. Good documentation on the structure, origin, and licensingrestrictions of texts will of course be important. Documentation on standard experimental designswill also be useful in increasing comparability of results between elds.Two types of software are also of possible interest. Programs that extract features from text ina standard way may be useful for improving comparability of results, particularly when researchershave access to additional texts of interest. In cases of linguistically complex features, providingsuch software could also greatly reduce the eort to produce meaningful results.Also of interest may be software for computing standard evaluation measures on authorshipattribution tasks. It can be surprisingly dicult to get two implementations of evaluation softwareto agree exactly on the value of a measure in all circumstances, particularly for ranking and clus-tering problems. Standard software, such as trec eval [58], that embodies agreed upon strategiesfor averaging across data, handling boundary cases, and so on can be helpful.We will investigate the relative expense and desirability of these various resources, in consulta-tion with the Working Group.4.5 Auxiliary ActivitiesThe are a variety of other activities that might or might not be associated with a corpus creationproject. The biggest decision here is whether to run one or more evaluations based on the corpus, asTREC and other programs have. It is important to note that TREC-style evaluations incorporateseveral dierent activities: denition of a standard task, time-delayed release of test materials,manual annotation of data, consistent computation of eectiveness measures, and a conferencewhere users of the corpus present results. The Working Group will consider whether none, all, orsome subset of these activities would be useful to associate with the corpus.2Other activities we will consider include creation of a website with various resources on au-thorship attribution, developed of software and documentation to aid others in building corpora,generation of baseline results on various authorship attribution tasks, and various outreach activi-ties.4.6 Legal IssuesEorts to produce corpora must contend with intellectual property law, particularly copyright law.In choosing texts to include, the ability to negotiate licensing terms with copyright owners, andthe impact of those terms on researchers using the data, are a major factor. Possible tax benetsto donors are another legal issue of importance.Some material of interest was originally distributed in a fashion that would have allowed aresearcher (or anyone) to access it without cost or license (e.g. web pages without robot exclusions,blogs, public mailing lists). Some previous corpus building eorts have used such material withoutlicensing it [11] while others have policies3 that forbid this.2The eventual funders of any corpus eort would be a major inuence on this choice. In the U.S., DoD andintelligence agency funders have had a particular interest in organized evaluations.3http://www.ldc.upenn.edu/Providing/ 11
If message trac is used, then privacy laws may be relevant as well, again potentially even forbroadcast material. A corpus intended for international use must take into privacy regulations in,for instance, the European Union.We will give close attention to these issues, and to the novel aspects that arise for authorshipattribution in particular. For instance, if demographic information on authors is desired, then thecooperation of those authors may be critical. Such cooperation could also lead to the availabilityof additional unpublished texts from authors that could be of great value.4.7 Resource ManagementCreating a corpus is a substantial eort involving many people, lengthy and unpredictable negoti-ations with content providers, problems with data formats, conicts over annotation and metadatachoice, and a variety of other issues. Members of the Working Group will draw on their ownexpertise, published literature on corpus building and revision eorts, and interviews with man-agers from past and ongoing corpora projects to help us develop appropriate strategies for projectmanagement, recruiting and training of personnel, and technical infrastructure.A major consideration will be whether to build the project from scratch, or work with an existingorganization with expertise in corpus creation, such as the Linguistic Data Consortium. There arearguments for both approaches, and inuence from funders is likely to be a major consideration aswell.5 Workplan and Project Management5.1 WorkplanWe have subdivided our proposed activities for the proposed one-year project into four quarters,listing the activities to be performed and goals to be reached in each.Quarter One. The rst quarter will focus on information gathering for corpus construction.Working with members of our Working Group, we will conduct a detailed survey of theauthor identication literature. We will examine existing corpora such as the various literarysources, the Enron e-mail data, Reuters RCV1 stories with identied journalists, and the re-cently acquired DIMACS newsgroup dataset. Via a mailing list, we will initiate a discussionwith the Working Group members about the key requirements for an author identicationcorpus and associated materials. We will also develop a list of candidate data sources.Quarter Two. The second quarter will focus on the author identication experimentation processand address the issues we discussed in Section 3. In particular the Working Group will developdesign standards for author attribution test corpora, develop a catalog of author identicationfeatures along with their advantages and disadvantages, discuss guidelines for appropriateuse of algorithms and statistical methods, and prescribe desiderata for appropriate controlsin author attribution experimentation. We will also examine the legal issues associated withthe various data sources under consideration.Quarter Three. In the third quarter we will prepare for the workshop. We will develop a detailedplan for corpus construction along with standards for author identication experimentation
12
and evaluation. We will also look at the processes and costs associated with data cleaning.This quarter will culminate in a draft design document ready for discussion by the Workshoppartipants.Quarter Four. We will hold a two-day workshop near the start of the fourth quarter, includingthe Working Group members and interested parties by them. We envisage providing nancialsupport to around 20 participants but the the total number of participants may be closer to30. Following the workshop, we will rene the existing design documents and summarize theworkshop proceedings in a comprehensive report.At the end of year we will publish our conclusions as a technical report. We will also submita version to a high-impact journal. This report will serve as the core of one or more proposalsto dierent funding sources for the development of high impact community resources for authoridentication.5.2 Project managementShlomo Argamon, the PI, will coordinate the overall planning eort, via frequent emails and phonemeetings with the coPI, David Madigan, and the consultant, David Lewis. An email list includingproject members and the Working Group will be set up, to enable continuous feedback on theplanning process. Two project meetings of all three principals will be held, one at IIT, and one atRutgers, in addition to the DIMACS-hosted workshop which will be held with the Working Grouptowards the end of the project year. Also, Argamon and Lewis will also meet on approximately amonthly basis at IIT; Lewis and Madigan have several ongoing collaborations and meet regularly.A research assistant at IIT will help with gathering and collating information about corpora andother resources, as well as working on any small-scale feasibility studies (for example, of softwareresources or annotation schemes) required in the course of the planning activity.5.3 Working groupThe Working Group will play a key role in the proposed project, providing ideas and feedback, aswell as, in some cases, resources that may be useable for those we intend to design. The followingindividuals have enthuasiastically agreed to participate:Dr. David Banks is a Professor of the Practice of Statistics at Duke University. Previouslyhe was on the faculty at Carnegie Mellon University. He has also worked for the FDA, NIST, andthe US Department of Transportation. He has broad research interests in Bayesian statistiscs andmultivariate analysis. He was co-editor of the recent Chance issue on author identication and haspublished a number of articles on information retrieval and text mining.Dr. W. Bruce Croft is a Distinguished Professor in the Department of Computer Scienceat the University of Massachusetts, Amherst. He is currently Director of the NSF CollaborativeResearch Center for Intelligent Information Retrieval (CIIR), as well as Chair of the Departmentof Computer Science. He has published more than 120 articles in information retrieval and relatedareas, has consulted for a wide range of companies and government agencies, and has developedtechnolgies used in a number of operational information retrieval systems. Dr. Croft was a memberof the National Research Council Computer Science and Telecommunications Board, 2000-2003,and was Editor-in-Chief of ACM Transactions on Information Systems, 1995-2002. He was elected
13
a Fellow of ACM in 1997, received the Research Award from the American Society for InformationScience and Technology in 2000, and received the Gerard Salton Award from the ACM SpecialInterest Group in Information Retrieval (SIGIR) in 2003.Dr. Haym Hirsh received his PhD in Computer Science from Stanford University in 1989.He is Professor and Department Chair of the Computer Science Department at Rutgers University.He has published more than 100 papers in articial intelligence, machine learning, informationretrieval, data mining, and related areas. Hirsh is a member of the Executive Council of theAmerican Association for Articial Intelligence, the Board of Directors of Institute for the Studyof Learning and Expertise, and the editorial board of various journals.Dr. David Holmes is Professor of Mathematics and Statistics at the College of New Jerseyand a prominent stylometrist. He has been involved in many disputed authorship identicationsand has a long publication record in this area. His most recent work has involved the civil war-eraPickett letters.Dr. Moshe Koppel is Associate Professor of Computer Science at Bar-Ilan University. He hasheld visiting faculty positions at the Institute for Advanced Study in Princeton, CUNY and CornellUniversity. His main area of research concerns machine learning, especially text categorization.In particular, he has published on demographic proling of text authors and on attribution ofanonymous and pseudonymous texts. Dr. Koppel's research has been presented in leading journalsand conferences and has been featured in Nature News, New Scientist, The New York Times andmany other publications. In addition to his academic work, Dr. Koppel consults for leading hi-techrms and has published two books on Talmudic Law.Dr. Gerald McMenamin received his doctorate in Spanish linguistics from El Colegio deMexico in 1978. He is currently Professor of Linguistics and Director of the Forensic LinguisticsInstitute at California State University, Fresno. He has taught several courses and training seminarson linguistic stylistics, and has worked on more than 250 civil and criminal cases of questionedauthorship. Dr. McMenamin is the author of a number of publications in forensic linguistics,including the book Forensic Stylistics (1993). He recently edited the book Forensic Linguistics:Advances in Forensic Stylistics, and authored 12 of its 15 chapters.Dr. Joseph Rudman has been working on non-traditional authorship attribution since themid 1970s. He has published widely on the topic and lectured throughout the United States andin seven foreign countries on the subject of non-traditional attribution. Dr. Rudman regularlyreferees papers on stylistics and non-traditional authorship studies for Literary and LinguisticComputing, Computers and the Humanities, Science, and other journals. He has acted as anAssessor for the Australian Research Council, and is a member of many professonal societies thatboth act as gatekeepers and publish attribution studies, such as The Association for Computers andHumanities, The Association for Literary and Linguistic Computing, The Classication Society ofNorth America, and the International Association of Forensic Linguists.Dr. Ian Soboro is a Computer Scientist with the Information Access Division of the Na-tional Institute of Standards and Technology. Each year, NIST hosts the Text REtrieval Conference(TREC), which is the world's largest coordinated research eort in information retrieval. Dr. Sobo-ro has organized TREC evaluations in web search, text ltering, novelty detection, and searchinglarge-scale collections. He has a number of research publications in text ltering, collaborativeltering, information retrieval evaluation, authorship attribution, and intelligent software agents.He received his Ph.D. in Computer Science from the University of Maryland, Baltimore County, in2001. 14
6 Signicance and Broader ImpactsAuthorship attribution has deep roots in the humanities, addressing sometimes long-standing dis-puted authorship issues. More recently, new applications focusing on intelligence and securityhave emerged and propelled an old eld to new prominence and importance. Researchers in au-thor identication currently face signicant challenges in terms of test corpora to evaluate novelmetholodology as well as evaluation methodologies. Our proposed project, if it proceeds beyondthe planning phase we propose here, could have a profound unifying eect on the now-dispersedauthor identication research community.The possible broader impacts of this work are legion. A well developed and rigorous method-ology for assessing the reliability of authorship attribution claims will have a signicant impact inthe criminal justice system. Homeland security could come to rely heavily on well-understood andvalidated author identication methodologies to nd and track known terrorists. With developmentof a standard test corpus and associated experimentation standards, research progress is likely tobe highly accelerated.
15
