                                        Knowledge-Based Systems 
                                  Manuscript Draft 
 
Manuscript Number: KNOSYS-D-11-00372 
 
Title: Cost-Sensitive Classifier Ensembles in Medical Diagnosis 
 
Article Type: Full Length Article 
 
Keywords: cost-sensitive classifier; Bagging; Boosting; Rotation Forest 
 
 
 
 
 
 
 
 
Cost-Sensitive Classifier Ensembles in Medical Diagnosis 
J. Novakovic 
 
Jasmina Novakovic 
‚ÄúFaculty of Computer Science‚Äù Megatrend University Belgrade 
Serbia, 11000 Belgrade, Bulevar Umetnosti 29 
E-mail: jnovakovic@megatrend.edu.rs 
 
Abstract 
In this research, cost-sensitive classifier ensembles that can incorporate unequal misclassification into 
classifier ensembles models for medical diagnosis are suggested. We chose classifier ensembles with 
decision trees, as the base classifiers because they are sensitive to rotation of the feature axes and still 
can be very accurate. For the purpose of this research, we use these performance measures: total 
misclassification cost, average misclassification cost, sensitivity, specificity and the Receiver Operating 
Characteristic (ROC) curve. All classifier ensembles: Bagging, Boosting and Rotation Forest, 
regardless of the decision tree that used as a basis, use Principal Component Analyze (PCA) for the 
selection and reduction of attribute data sets. In the cost-sensitive classification problem, experimental 
results demonstrate the effectiveness of the Rotation Forest with J.48 as base classifier compared to 
other methods: in all cases, values of total misclassification cost and average misclassification cost are 
smallest and values of ROC are the largest. 
Keywords: cost-sensitive classifier, Bagging, Boosting, Rotation Forest. 
 
1. Introduction 
 
In data mining, most classifiers assume equal weighting of the classes in terms of both the number of 
instances and the level of importance - misclassifying classes have the same importance. Standard data 
mining techniques are not successful, when trying to predict a minority class in an imbalanced data set 
or, when a false negative is deemed more important than a false positive. In real-world situations, 
unequal misclassification costs are common; especially for medical diagnosis, asymmetric 
misclassification costs have to be considered as an important factor.  
Cost-sensitive classifiers solved this type of problem where instances are predicted to have the class 
that has the lowest expected cost [1], [2]. These classifiers adapt models according to the 
misclassification error costs in the learning stage, with objectives to minimize the misclassification 
costs instead of maximizing the classification accuracy. Because many practical classification problems 
have different costs associated with different error types, various algorithms for cost-sensitive 
classification have been produced [3]. 
To overcome the shortcomings of stratification that assumed that all errors are equally costly, 
Domingo [4] proposed a procedure, called MetaCost. MetaCost is a cost sensitive classification-based 
bagging method by relabeling training examples with their estimated minimal-cost classes, and 
applying the error-based learner to the new training set. Elkan [5] presented the basic concepts behind 
optimal learning and decision-making when different misclassification errors cause different losses. To 
minimize the misclassification and test costs Ling et al. [6] suggest splitting criterion of minimal total 
cost on training data, instead of minimal entropy, to build decision trees. Greiner et al. [7] reviewed the 
theoretical aspects of active learning with test costs using a probably-approximately-correct learning 
framework. Zubek and Dietterich [8] introduced an algorithm for learning classifiers to minimize the 
total cost of attribute measurement plus misclassification error. This algorithm is based on formulating 
the classification process as a Markov Decision Process whose optimal policy gives the optimal 
Manuscript
Click here to view linked References
diagnostic procedure. The well-known decision tree learners incorporating asymmetric 
misclassification cost are C5.0 and the Classification and Regression Tree (CART). Decision tree 
learner C5.0 is an improved version of C4.5 that can produce a cost-sensitive tree when a cost matrix is 
given, while C4.5 treats all misclassification error costs as equal. CART as a binary decision tree 
learner, has exactly two branches at each internal node [9], [10]. Nanda and Pendharkar [11] propose 
linear models based on a genetic algorithm for minimizing misclassification costs in bankruptcy 
prediction. Ting [12] suggests an instance-weighting method to induce cost-sensitive trees, and Gama 
[13] proposes a cost-sensitive iterative Bayes. Majumder, Ghosh and Gupta [14] suggest the relevance 
vector machine for optical diagnosis of cancer that solves the problem of asymmetric misclassification 
costs. 
Zadrozny and Elkan have proposed a method called direct cost-sensitive decision making [15]. The 
main idea is that any learned classifier that can provide conditional probability estimates for training 
data can also estimate conditional probabilities for test data of the same domain. Zadrozny and Elkan 
claim that the optimal prediction labels of test examples can directly be computed. They have reported 
better results than MetaCost, which uses the same probability estimation methods on C4.5 with pruning 
and collapsing. This result is not surprising, since Ting [16] approve that MetaCost usually does not 
perform better than an internal cost-sensitive classifier. 
In this study, we suggest cost-sensitive classifier ensembles that can incorporate unequal 
misclassification into classifier ensembles models. The goal of this research is to present different 
algorithmic approaches for constructing and evaluating systems that learn from experience to make the 
decisions and predictions that not only minimize the expected number or proportion of mistakes, but 
also minimize the total costs associated with those decisions. 
For the purpose of this research, we choose a metaclassifier that makes its base classifier cost-
sensitive. Two methods can be used to introduce cost-sensitivity: reweighting training instances 
according to the total cost assigned to each class; or predicting the class with minimum expected 
misclassification cost (rather than the most likely class). The second method is very similar to studies of 
Zadrozny and Elkan [15].  
The rest of the paper is organized as follows: Section 2 explains cost-sensitive classifier. Section 3 
offers three approaches for constructing classifier ensembles. Section 4 illustrates methods for 
measuring the performance of cost-sensitive classifiers. Section 5 presents our experimental study 
comparing Bagging, Boosting and Rotation Forest as cost-sensitive classifier ensembles that can 
incorporate unequal misclassification into classifier ensembles models. Section 6 offers our conclusions 
and outlines directions of future work. 
 
2. The Cost-Sensitive Classification Problem 
 
Most of the classification algorithms ignore the asymmetric cost constraints of many real-world 
situations, although Breiman et al. [17] made preliminary studies as early as 1984. At the end of the last 
century, attention over this area has increased significantly, leading to an online bibliography [18] and a 
special workshop organization totally dedicated to cost-sensitive learning [19]. Elkan [20] and Turney 
[21] are depicting fundamentals of the subject. There is plenty of room for improvement in this most 
important topic, because being a recent research area cost-sensitive learning studies are at their infancy 
level.  
In this research, we address aspects of the supervised learning problem with asymmetric 
misclassification costs. Figure 1 presents general framework of supervised learning with asymmetric 
misclassification costs. A set of labeled examples , called training data is the input to a 
supervised learning algorithm, where a vector of continuous or discrete values called attributes is  and 
 is the label of . The training data is assumed to be generated according to an unknown probability 
distribution . There are two possible types of the attributes: symbolic and numeric. A symbolic 
attribute takes values from a finite, discrete set on which no ordering relation is assumed, and a numeric 
attribute takes values from a subset of some ordered set of values. The attribute is called a continuous 
attribute if this subset is continuous. In the case of classification, the labels can be elements of a discrete 
set of classes , or in the case of regression elements drawn from a continuous subset of a 
continuous set. 
 
Figure 1: General framework of supervised learning with asymmetric misclassification costs [22] 
 
Assume that all the examples that are presented to the learning algorithm are drawn from the same 
(unknown) distribution, supervised learning means finding a compact mapping  (hypothesis) that will 
be able to label correctly a high proportion of unlabeled examples drawn from the same distribution. 
The supervised learning algorithm describes a procedure for building or selecting a model and tuning 
the model's internal parameters, such that the model will implement a function  that best 
approximates f for any examples drawn according to the same probability distribution . 
In the cost-sensitive classification, the aim of the learning algorithm is to minimize the total expected 
loss  of the hypothesis: 
  (1) 
  
where  is the cost function describing the loss incurred by  on an instance . 
This paper focuses on classification tasks and assumes that for n-class problem, n-by-n cost matrix  
is available at learning time, and that the cost function is static, because none of the values in   
changes during the learning or the decision making process. The contents of cost matrix  specify 
the cost incurred when an example is predicted to be in class i when in fact it belongs to class j.  
Generally, the cost values of a cost matrix are represented in terms of a fixed measuring unit 
associated with the corresponding decisions. If all the cost values are scaled by a constant factor, the 
learning task is not altered. A careful analysis is needed prior to designing the matrix so that all 
potential costs that are incurred by a decision are captured. In the cost matrix, the individual cost values 
can represent either losses or benefits, or both. Positive values represent losses and negative values 
represent benefits. By discretizing equation (1) and using  as the cost function, the optimal hypothesis 
will need to minimize 
 
It means that the general objective of a classification system is to minimize the total cost of its 
hypothesis, computed as the sum of the costs of the individual decisions. The n-by-n cost matrix has the 
following structure: 
 
 
The rows of the cost matrix correspond to predicted classes and columns of the matrix correspond to 
actual classes;  represents the cost of classifying an instance of class j as class i. The elements 
 of the cost matrix that constitute up the main diagonal of the matrix are assumed to 
be all 0, representing the natural interpretation that correct classifications have no cost to the user. The 
non-diagonal elements of the cost matrix are assumed to be greater than zero, denoting loses of 
misclassification from a positive baseline. 
 
3. Classifier Ensembles 
 
In many fields, multiple classifier system is more accurate and robust than an excellent single 
classifier, because one single classification system cannot always provide high classification accuracy. 
In machine learning and pattern recognition, classifier combination is an active field of research [23], 
[24], [25], [26], [27], [28]. Many theoretical and practical studies present the advantages of the 
combination paradigm over the individual classifier models.  
A great deal of research has gone into designing multiple classifier systems that are commonly called 
classifier ensembles. We present three approaches for constructing classifier ensembles, which have 
been found to be accurate and computationally feasible across various data domains. 
 
 
 
3.1. Bagging 
 
In 1996, Breiman firstly introduced the Bagging predictor, which is similar to the Boosting algorithm. 
Bagging [29] takes bootstrap samples of objects and trains a classifier on each sample. The classifier 
votes are combined by majority voting. In some implementations of Bagging, classifiers produce 
estimates of the posterior probabilities for the classes. Posterior probabilities are averaged across the 
classifiers and the most probable class is assigned, called ‚Äúaverage‚Äù aggregation of the outputs. In this 
experiment, Bagging with average aggregation is used. Each individual classifier is trained on a 
bootstrap sample, which causes the data distribution seen during training is similar to the original 
distribution. As a result, the individual classifiers in a bagging ensemble have relatively high 
classification accuracy.  
Only the proportion of different objects in the training samples encourages diversity between these 
classifiers. The bootstrap sampling appears to lead to ensembles of low diversity compared to other 
ensemble creating methods, even though the classifier models used in Bagging are sensitive to small 
changes in data. Therefore, Bagging requires larger ensemble sizes to perform well. 
Breiman [29] shows in his work that the Bagging predictors can push an unstable weak classifier to a 
significant step towards optimality. This method has been used to various classification and prediction 
problems in biology and social sciences. 
A learning set consists of data  where   X is the input and   Y 
is the response from a predictor denoted by . Suppose we are given k learning sets  of N 
independent identical distribution observation from the same distribution of L.  An obvious procedure 
to use the average to get a better estimate of the classification: 
 
 
 
where EL denotes the expectation over L. Usually, we only have one learning set L, so, one resolvable 
way is to take bootstrap samples  of  cases, randomly, but with replacement from L, and form 
predictor set . We call this procedure ‚Äúbootstrap aggregating‚Äù and use the acronym 
Bagging. In that case, the predictor , which gets most voting score of test set, is the best classification. 
Bagging algorithm is showed in Figure 2. 
 
Figure 2: Bagging algorithm [30] 
 
3.2. AdaBoost 
 
Boosting is a family of methods for improving the performance of any learning algorithm, the most 
prominent member of which is AdaBoost. Boosting methods improve the performance of a ‚Äúweak‚Äù 
classifier by using it within an ensemble structure. Each subsequent classifier is trained on data that 
have been ‚Äúhard‚Äù for the previous ensemble members. In Boosting methods, a set of weights is 
maintained across the objects in the data set so that objects that have been difficult to classify acquire 
more weight, forcing subsequent classifiers to focus on them. Mechanism of these methods consist of 
repeatedly running a given weak learning algorithm on various distributions over the training data, and 
then combining the classifiers produced by the weak learner into a single composite classifier.  
The Boosting algorithm takes as input a training set of m examples  where 
xi is an instance drawn from some space X, and  is the class label associated with xi. In this 
research, we assume that the set of possible labels Y is of finite cardinality k. 
The Boosting algorithm has access to another unspecified learning algorithm, called the weak 
learning algorithm. The Boosting algorithm calls weak learning algorithm repeatedly in a series of 
rounds. On round t, the booster provides weak learning algorithm with a distribution Dt over the 
training set S. Weak learning algorithm computes a classifier or hypothesis , 
which should misclassify a non trivial fraction of the training examples, relative to Dt. The goal of weak 
learner is to find a hypothesis ht that minimizes the training error . This error 
is measured with respect to the distribution Dt that was provided to the weak learner. This process 
continues for T rounds. Finally, the booster combines the weak hypotheses into a single final 
hypothesis . In the Boosting algorithm the manner in which Dt is computed on each round, and how 
 is computed are unspecified. These questions solve different Boosting schemes in different ways.  
AdaBoost.M1 algorithm uses the simple rule present in Figure 3, where the initial distribution D1 is 
uniform over S so D1(i)=1/m for all i. In AdaBoost.M1 algorithm to update distribution, we multiply 
the weight of example i by some number  if ht classifies xi correctly, and otherwise the 
weight is left unchanged, and also divide by the normalization constant Zt. In this way, ‚Äúhard‚Äù 
examples, which tend often to be misclassified, get higher weight, and ‚Äúeasy‚Äù examples that are 
correctly classified by many of the previous weak hypotheses get lower weight. As a result, AdaBoost 
focuses the most weight on the examples that seem to be hardest for weak learning algorithm. 
 
Figure 3: AdaBoost.M1 algorithm [31] 
 
The number  is a function of and the final hypothesis  is a weighted vote of the weak 
hypotheses. In this algorithm, the weight of hypothesis ht is defined to be 1/Œ≤t   so that greater weight 
is given to hypotheses with lower error.  
The main weakness of this algorithm is that it is unable to handle weak hypotheses with error greater 
than 1/2. If k is the number of possible labels, then the expected error of a hypothesis that randomly 
guesses the label is . This means for binary classification problems, that the weak hypotheses 
need be only slightly better than random. Nevertheless, when the number of possible labels is more 
than two, the requirement of AdaBoost.M1 is much stronger than that, and might be hard to meet. 
The success of AdaBoost algorithm has been explained, among others, with its diversity creating 
ability, which is an important property of a classifier ensemble [32]. This algorithm creates inaccurate 
classifiers by forcing them to concentrate on difficult objects and ignore the rest of the data. This led to 
large diversity that boosted the ensemble performance, often beyond that of Bagging. This leads us to 
the famous accuracy-diversity dilemma, because it seems that classifiers cannot be both very accurate 
and have very diverse outputs. 
Bounds on the training and generalization errors of the ensemble construction through AdaBoost 
have been proven [33], [34], [35]. It appears that, AdaBoost is the better method than Bagging, 
although Bagging has application niches as well. However, for large ensemble sizes (in the order of 
thousand classifiers) the significant differences between the ensemble models almost disappear [36]. 
There is a search for a consistently good ensemble strategy for small ensemble sizes, in spite of the 
computational effort required for training; small ensembles will have the advantage of fast and in many 
cases, near optimal performance [37]. 
 
3.3. Rotation Forest 
 
Rotation Forest is an ensemble method that trains classifiers independently, using a different set of 
extracted features for each of classifier. Rotation Forest, as a newly proposed ensemble scheme, is 
accurate and feasible across various data domains [38]. Researchers compared Rotation Forest with 
classifier ensembles such as Bagging, AdaBoost, Random Forest and some experimental results 
showed that Rotation Forest outperformed all three methods by a large margin [38]. According to the 
researchers, the diversity-accuracy pattern of the generated ensembles is the reasons for the success of 
Rotation Forests. 
This algorithm has attracted many researchers‚Äô attention in recent years because it is more efficient 
and robust compared with many existing schemes. Rotation Forest randomly split the feature set into K 
subsets, which a base classifier uses as training data. Although the subsets may be disjoint or 
intersecting, Rotation Forest chose disjoint subsets to maximize the chance for high diversity. The filter 
used to project the data applied to each subset. Running the filter on a subset of classes instead of the 
whole set is done in a bid to avoid identical coefficients if the same feature subset is chosen for 
different classifiers. 
Explanation of Rotation Forest algorithm presents below. Assume that there are N training samples 
and n features in data set. Let X be the training sample set in a form of a  matrix, Y be the 
corresponding labels, where Y takes values from the set of class labels , and F be the feature 
set.  
Assuming there are L decision trees in Rotation Forest, denoted by D1,..., DL respectively, and the 
feature set split randomly into K subsets, we need to determine L and K in advance. Construction of the 
training set for an individual classifier Di is in three steps: 
Step1. Split F randomly into K disjointed subsets. Suppose that K is a factor of n so that each feature 
subset contains  features. 
Step2. Denote by Fij the jth subset of features for training set of classifier Di. Select randomly for 
each subset, a nonempty subset of classes, and then draw a bootstrap subset of objects with the size of 
75 percent of the data set to form a new training set. After that, PCA is applied to the M features in Fij 
and the selected subset of X. Denoted the coefficients of PCA by  each of size . 
Step3. Arrange a sparse rotation matrix Ri with the obtained coefficients in matrix as follows: 
(2) 
 
The columns of Ri should be rearranged according to the original features. The rearranged rotation 
matrix is denoted by . The transformed training set for classifier Di is . All classifiers will be 
trained in parallel. 
In the classification phase of Rotation Forest, for a given x, let  be the probability produced 
by the classifier Di to the hypothesis that x belongs to class . By the average combination method, 
calculate the confidence for each class as follows: 
 
 
The test sample x is easily assigned to the class with the largest confidence. The algorithm of 
Rotation Forest is showed below (Figure 4). 
 
Figure 4: Rotation Forest algorithm [38] 
 
4. Measures for Performance Evaluation 
There are several methods for measuring the performance of cost-sensitive classifiers [39]. For the 
purpose of this experiment, we use total misclassification cost, average misclassification cost, 
sensitivity, specificity and the ROC curve. In the text below, we describe these methods. 
Suppose a medical test that screens people for a disease, in which each person taking the test either 
has or does not have the disease. Outcome of the test can be positive (predicting that the person has the 
disease) or negative (predicting that the person does not have the disease); and may or may not match 
the subject's actual status. Denote by: 
ÔÇ∑ true positive (TP): sick people correctly diagnosed as sick, 
ÔÇ∑ false positive (FP): healthy people incorrectly identified as sick, 
ÔÇ∑ true negative (TN): healthy people correctly identified as healthy, 
ÔÇ∑ false negative (FN): sick people incorrectly identified as healthy. 
 
For cost-sensitive classifiers total misclassification cost is a widely used method and more 
appropriate than accuracy which is the proportion of correctly classified cases [40],  
 
 
 
Total misclassification cost measures overall cost of incorrectly classified cases in a given error cost: 
 
, 
 
where  denotes misclassification error cost of FP, and  denotes misclassification error 
cost of FN. 
Average misclassification costs are the ratio of the total misclassification costs and the total number 
of test instances: 
 
 
 
Sensitivity is the fraction of positive cases that are classified as positive, which is also widely used 
especially for the medical area because it can be viewed as a detection rate of diseases:  
 
 
 
Specificity is the fraction of negative cases classified as negative [39]: 
 
 
 
However, sensitivity and specificity are negatively correlated. For example, if the misclassification 
cost of a FN is higher than a FP then sensitivity tends to increase and specificity decrease.  
The ROC curve is a two-dimensional visualization of the FP rate (1-specificity, plotted on the X-axis) 
and the TP rate (sensitivity, plotted on the Y-axis). One point in the ROC curve is better if it is located 
more to the northwest, which means TP is higher and FP is lower on the ROC graph [41].  
 
5. Experimental Validation 
In this section, we will investigate the behavior of cost-sensitive classifier ensembles in medical 
domains. Consequences of employing different classifier ensembles with different base classifiers are 
monitored, together with the effects of reweighting training instances according to the total cost 
assigned to each class and predicting the class with minimum expected misclassification cost. Later on, 
we present comparisons of cost-sensitive classifier ensembles with different performance measures.  
In our experiment, we chose classifier ensembles with decision trees, as the base classifiers because 
they are sensitive to rotation of the feature axes and still can be very accurate. Preliminary results of 
medical data sets with classifiers ensembles and decision trees as the base classifiers showed good 
results compared with others classifier as the base classifiers. Good results are a consequence of the fact 
that a small rotation of axes may build a complete different tree; the transformation guaranteed the 
diversity of the ensemble system, and finally, the major vote rule fuses the outputs of all trees. 
Real data sets called ‚ÄúPima Indians diabetes‚Äù, ‚ÄúHepatitis‚Äù and ‚ÄúStatlog (Heart)‚Äù were used for tests, 
taken from the UCI repository of machine learning databases [42]. We used these data sets to compare 
different cost-sensitive classifier ensembles that can incorporate unequal misclassification into classifier 
ensembles models for medical diagnosis.  
In the following, we provide the details for the benchmark data sets we have used from UCI Machine 
Learning Repository.  
Pima Indians diabetes: In this data set the diagnostic is whether the patient shows signs of diabetes 
according to World Health Organization criteria (i.e., if the 2 hour post-load plasma glucose was at 
least 200 mg/dl at any survey examination or if found during routine medical care). Patients included in 
this investigation live near Phoenix, Arizona, USA. There are 768 instances and 8 features all of which 
are numeric valued. Negative (tested negative for diabetes) are 500 instances and remaining 268 are 
positive. There are missing values. 
Hepatitis: The main aim of this data set is to predict whether hepatitis patients will die or not. In this 
data set, there are two classes: live (123 instances) and die (32 instances). There are 155 instances and 
19 features, with missing values. 
Statlog (Heart): The task is to predict absence or presence of heart disease. This data set contains 12 
features (which have been extracted from a larger set of 74). There are 270 observations, with no 
missing values. 
Through discussions with domain experts, for the following data sets the cost matrices and cost 
weights are defined as follows [42], [43]: 
 
Statlog (Heart) 
(Absence) (Presence)  
0.0 1.0 (Absence) 
5.0 0.0 (Presence) 
 
Hepatitis   
(Live) (Die)  
0.0 0.9 (Live) 
0.4 0.0 (Die) 
 
Pima Indians diabetes 
(Diabetes) (No diabetes)  
0.0 0.9 (Diabetes) 
0.3 0.0 (No diabetes) 
 
Two methods are used to introduce cost-sensitivity - the reweighting of the training instances 
according to the total cost assigned to each class in the cost matrix or predicting the class with the 
minimum expected misclassification cost using the values in the cost matrix.  
For the first method of cost-sensitive classifier, we use pseudonym M1 and for the second M2, 
respectively. In M1, if the internal classifier has not the property of weight instances, then resampling is 
done to adjust weights of the training instances. On the other hand, if internal classifier supports 
weighting of instances, then the weights of the instances are simply updated to reflect the effect of 
benefit of classification. 
When M2 is wrapped around a distribution-based classifier, it uses the probability outputs of the 
internal classifier and puts these probabilities in the optimal decision equation to determine the optimal 
predictions that minimizes the cost of classification. 
In this study, classifier ensembles used following decision trees as base classifier: decision stump 
(DecisionStump), pruned or unpruned C4.5 (J48) and fast decision tree learner (REPTree). 
For the purpose of this research, we used PCA as the filter used to project the data. Pilot experiments 
on medical data sets with cost-sensitive classifier ensembles and others filter used to project the data 
showed that although the results were competitive, they were not as good as the results with PCA. 
In this study, we use performance measures with following abbreviations that are used in the tables: 
total misclassification cost (TCost), average misclassification cost (AvgCost), sensitivity (Sens), 
specificity (Spec) and ROC curve. 
Experimental results of several methods for measuring the performance of cost-sensitive classifiers 
that learn from experience to make the decisions and predictions that not only minimize the expected 
number or proportion of mistakes, but also minimize the total costs associated with those decisions are 
compared. Values of total misclassification cost and average misclassification cost for Hepatitis data set 
are smallest with Rotation Forest-J.48-M1. Value of ROC is the largest with Rotation Forest-J.48-M1 
(Table 1). Sensitivity, which can be viewed as a detection rate of diseases, for this data set has the 
highest values with ADABoostM1-J.48-M2, ADABoostM1-J.48-M1 and Rotation Forest-J.48-M1. 
Specificity as the fraction of negative cases classified as negative has the highest values with Bagging-
DecisionStump-M1, Bagging-RepTree-M1 and Bagging-RepTree-M2. 
 
Table 1: Performance evaluation of Hepatitis data set with cost-sensitive classifier 
 
Table 2: Performance evaluation of Pima Indians diabetes data set with cost-sensitive classifier 
 
Pima Indians diabetes data set values of total misclassification cost and average misclassification cost 
are smallest with Rotation Forest-J.48-M1. For this data set value of ROC is the largest with Rotation 
Forest-J.48-M1 (Table 2). Sensitivity has the highest values with Rotation Forest-DecisionStump-M1 
and Rotation Forest-DecisionStump-M2. Algorithms ADABoostM1-J.48-M1 and ADABoostM1-J.48-
M2 are the most successful in measuring specificity. 
 
Table 3: Performance evaluation of Statlog (Heart) data set with cost-sensitive classifier 
 
Values of total misclassification cost and average misclassification cost for Statlog (Heart) data set 
are smallest with Rotation Forest-J.48-M2. Value of ROC is the largest with Rotation Forest-J.48-M1 
(Table 3). Sensitivity has the highest values with ADABoostM1-J.48-M1 and ADABoostM1-J.48-M2, 
and specificity has the highest values with Bagging-RepTree-M1 and Bagging-RepTree-M2, Rotation 
Forest-J.48-M2. 
In all medical data sets, values of ROC are the largest with Rotation Forest-J.48-M1. Values of total 
misclassification cost and average misclassification cost, in all cases, are the smallest with Rotation 
Forest-J.48, and the biggest with ADABoostM1, especially with ADABoostM1-J.48-M1 and 
ADABoostM1-J.48-M2. The most successful algorithms for measure sensitivity are ADABoostM1-
J.48-M1 and ADABoostM1-J.48-M2, and the most unsuccessful algorithms are RotationForest-
DecisionStump-M2. The most successful algorithms for measure specificity are Bagging-RepTree-M1 
and Bagging-RepTree-M2, and the most unsuccessful algorithms are RotationForest-DecisionStump-
M1 and RotationForest-DecisionStump-M2.  
Figure 5 shows ROC space with different cost-sensitive classifiers and data sets. The point (0, 1) 
represents perfect classification. One point in ROC space is better than another if sensitivity is higher, 
1-specificity is lower, or both. Classifiers appearing near the X-axis, on the left-hand side of ROC 
space, make positive classifications only with strong evidence so they make few false positive errors, 
but they often have low true positive rates as well. Many classifiers, which work with Statlog (Heart) 
data set, appear near the X-axis. On the upper right-hand side of ROC space, classifiers make positive 
classifications with weak evidence so they classify nearly all positives correctly, but they often have 
high false positive rates. Many classifiers that work with Pima Indians diabetes data set appear on the 
upper right-hand side of ROC space. 
 
Figure 5: ROC space showing different cost-sensitive classifiers 
We can conclude that in experimental results the Rotation Forest with J.48 as base classifier 
demonstrate the effectiveness compared to other methods: in all cases, values of total misclassification 
cost and average misclassification cost are smallest and values of ROC are the largest. Comparing the 
two methods M1 and M2, we can conclude that both are equally successful, and that there is no 
dominance in the performance results. In some cases, one method is successful in other cases other 
method. 
 
6. Conclusion and Further Research 
In this study, we applied cost-sensitive classifier ensembles Bagging, Boosting and Rotation Forest 
with different base classifier to real life medical data sets to verify the effectiveness of these methods. 
These results evaluated and compared using different performance measures: total misclassification 
cost, average misclassification cost, sensitivity, specificity, and ROC curve. With the Rotation Forest 
with J.48 in all cases, values of total misclassification cost and average misclassification cost are 
smallest and values of ROC are the largest. Methods M1 and M2 are used to introduce cost-sensitivity, 
both are equally successful, and that there is no dominance in the performance results.  
Evaluation of classification accuracy is fast; compare with others classifiers, for instance MLP with 
many hidden layers. There are many questions and issues that remain to address and that we intend to 
investigate in future work. The choice of parameters for cost-sensitive classifier ensembles influenced 
on classification accuracy. In further research, we will use different values of these parameters to get 
better results. In addition, we will use other cost-sensitive classifier ensemble methods with the aim to 
get improvements of classification accuracy.  
 
References 
[1] Elkan C. The Foundations of Cost-Sensitive Learning. In: Proceedings of the Seventeenth 
International Conference on Artificial Intelligence, 973-978, 4-10 August 2001, Seattle 2001. 
[2] Drummond C, Holte RC. Cost Curves: An Improved Method for Visualizing Classifier 
Performance. Machine Learning, 65(1):95-130, 2006. 
[3] Zhu X, Wu X. Cost-Guided Class Noise Handling for Effective Cost-Sensitive Learning. In: 
Proceedings of the fourth IEEE international conference on data mining, 297‚Äì304, 2004. 
[4] Domingos P. MetaCost: A General Method for Making Classifiers Cost-Sensitive. In: Proceedings 
of the fifth ACM SIGKDD international conference on knowledge discovery and data mining, 155‚Äì
64, 1999. 
[5] Elkan C. The Foundations of Cost-Sensitive Learnin. In: Proceedings of the seventeenth 
international joint conference on artificial intelligence (IJCAI‚Äô01), 973‚Äì8, 2001. 
[6] Ling CX, Yang Q, Wang J, Zhang S. Decision Trees with Minimal Costs. In: The twentieth-first 
international conference on machine (ICML 2004), 69‚Äì77, 2004. 
[7] Greiner R, Grove A, Roth D. Learning Cost-Sensitive Active Classifiers. Artificial Intelligence 
Journal,139(2):137‚Äì74, 2002. 
[8] Zubek VB, Dietterich TG. Pruning Improves Heuristic Search for Cost-Sensitive Learning. In: 
Proceedings of the nineteenth international conference on machine learning, 27‚Äì34, 2002. 
[9] Breiman L, Friedman JH, Olshen RA, Stone CJ. Classification and Regression Trees, New York, 
NY: Chapman & Hall, 1984. 
[10] Steinberg D, Colla P. CART: Tree-Structured Non-Parametric Data Analysis. San Diego, CA, 
USA: Salford Systems, 1995. 
[11] Nanda S, Pendharkar P. Linear Models for Minimizing Misclassification Costs in Bankruptcy 
Prediction. In: International Journal of Intelligent Systems in Accounting, Finance & 
Management,10(3):155‚Äì68, 2001. 
[12] Ting KM. An Instance-Weighting Method to Induce Cost-Sensitive Trees. In:  IEEE Transactions 
on Knowledge and Data Engineering, 14(3):659‚Äì65, 2002. 
[13] Gama J. A Cost-Sensitive Iterative Bayes. In: Seventeenth international conference on machine 
learning, workshop on cost-sensitive learning, 2000. 
[14] Majumder SK, Ghosh N, Gupta PK. Relevance Vector Machine for Optical Diagnosis of Cancer. 
Lasers in Surgery and Medicine 36(4):323‚Äì33, 2005. 
[15] Zadrozny B, Elkan C, Learning and Making Decisions when Costs and Probabilities are Both 
Unknown. In: Proceedings of the Seventh International Conference on Knowledge Discovery and 
Data Mining, AAAI Press (distributed by MIT Press), 2001. 
[16] Ting KM. Cost-Sensitive Classification Using Decision Trees, Boosting and MetaCost. Book 
chapter in Heuristic and Optimization for Knowledge Discovery, Edited by Sarker, R., Abbass, H. & 
Newton, C. Idea Group Publishing, 2002. 
[17] Breiman L, J. H. Friedman, R. A. Olsen, C. J. Stone. Classification and Regression Trees, 
Wadsworth International Group, Belmont, Wadsworth, 1984.  
[18] Cost-sensitive learning bibliography. Online bibliography. P. Turney, O. Boz, editors, Institute 
for Information Technology of the National Research Council of Canada, Ottawa, 1997, 
http://home.ptd.net/~olcay/cost-sensitive.html. 
[19] ICML-2000 Workshop on Cost-Sensitive Learning ‚Äì Workshop Notes, 
http://www.dmargineantu.net/Workshops/Workshop-ICML2000/worknotes.html. 
[20] Elkan C. The Foundations of Cost-Sensitive Learning. In: Proceedings of the Seventeenth 
International Joint Conference on Artificial Intelligence, August, 2001. 
[21] Turney P. Types of Cost in Inductive Concept Learning. Workshop on Cost-Sensitive Learning at 
the Seventeenth International Conference on Machine Learning (WCSL at ICML-2000), 15-21, 
Stanford University, California, 2000. 
[22] Margineantu DD. Methods for Cost-Sensitive Learning. Doctoral thesis, Oregon State University, 
September 21, 2001. 
[23] Multiple Classifier Systems, Proc. Sixth Int‚Äôl Workshop, MCS 2005, Oza NC et al., eds., 2005. 
[24] Proc. First Int‚Äôl Workshop Multiple Classifier Systems (MCS 2000), Roli F, Kittler J, eds., 2000. 
[25] Proc. Second Int‚Äôl Workshop Multiple Classifier Systems (MCS 2001), Roli F, Kittler J, eds., 
2001. 
[26] Proc. Third Int‚Äôl Workshop Multiple Classifier Systems (MCS 2002), Roli F, Kittler J, eds., 2002. 
[27] Proc. Fifth Int‚Äôl Workshop Multiple Classifier Systems (MCS 2004), Roli F et al., eds., 2004. 
[28] Windeatt T, Roli F, Proc. Fourth Int‚Äôl Workshop Multiple Classifier Systems (MCS 2003), 2003. 
[29] Breiman L. Bagging Predictors. Machine Learning, vol. 24, no. 2, 123-140, 1996. 
[30] Dong L, Yuan Y, Cai Y. Using Bagging Classifier to Predict Protein Domain Structural Class. 
Journal of Biomolecular Structure & Dynamics, ISSN 0739-1102, Volume 24, Issue Number 3, 2006. 
[31] Freund Y, Schapire RE. Experiments with a New Boosting Algorithm. ICML, 1996. 
[32] Kuncheva LI. Diversity in Multiple Classifier Systems (editorial), Information Fusion, vol. 6, no. 
1, 3-4, 2004. 
[33] Freund Y, Schapire RE. A Decision-Theoretic Generalization of On-Line Learning and an 
Application to Boosting. J. Computer and System Sciences, vol. 55, no. 1, 119-139, 1997. 
[34] Schapire RE. Theoretical Views of Boosting. In: Proc. Fourth European Conf. Computational 
Learning Theory, 1-10, 1999. 
[35] Schapire RE, Y. Freund, P. Bartlett, W.S. Lee. Boosting the Margin: A New Explanation for the 
Effectiveness of Voting Methods. Annals of Statistics, vol. 26, no. 5, 1651-1686, 1998. 
[36] Banfield RE, Hall LO, Bowyer KW, Bhadoria D, Kegelmeyer WP, Eschrich S. A Comparison of 
Ensemble Creation Techniques. In: Proc Fifth Int‚Äôl Workshop Multiple Classifier Systems (MCS ‚Äô04), 
2004. 
[37] Margineantu DD, Dietterich TG. Pruning Adaptive Boosting. In: Proc. 14th Int‚Äôl Conf. Machine 
Learning, 211-218, 1997. 
[38] Rodriguez JJ, Kuncheva LI, Alonso CJ. Rotation Forest: A New Classifier Ensemble Method. In: 
IEEE Transactions on pattern analysis and machine intelligence, vol. 28, no. 10, October 2006. 
[39] Lavrac N. Selected Techniques for Data Mining in Medicine. Artificial Intelligence in Medicine, 
16(1):3‚Äì23, 1999. 
[40] Provost F, Fawcett T. Analysis and Visualization of Classifier Performance: Comparison under 
Imprecise Class and Cost Distributions. Knowledge Discovery and Data Mining, 43‚Äì48, 1997. 
[41] Bradley AP. The Use of the Area under the ROC Curve in the Evaluation of Machine Learning 
Algorithms. Pattern Recognition, 30(7):1145‚Äì59, 1997. 
[42] Frank A, Asuncion A. UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, 
CA: University of California, School of Information and Computer Science, 2010. 
[43] Cai J. Decision Tree Pruning Using Expert Knowledge. A Dissertation Present to the Graduate 
Faculty of the University of Akron, December 2006. 
 
 
 
Type equation here.
Training data ‚å© , ‚å™ 
Cost matrix   ,  
Learning algorithm ‚Ñé 
Output hypothesis 
Figure 1
______________________________________________________________________________ 
For  = 1, 2, ‚Ä¶ ,  Do 
Draw 	
 samples randomly and with replacement from the X. 
And form the learning set 	
. 
Train classification k (	
) from the learning set 	
. 
Make a plurality vote for every k of K classifications on the test set. 
The classification, which gets the most voting score, is the best one. 
 
 
Figure 2
______________________________________________________________________________ 
Input: sequence of m examples ‚å©, 	, ‚Ä¶ , , 	‚å™ with labels  ‚àà  = 1, ‚Ä¶ ,  
           weak learning algorithm 
           integer T specifying number of iterations 
Initialize D1(i)=1/m for all i. 
Do for  = 1,2, ‚Ä¶ ,  
1. Call weak learning algorithm, providing it with the distribution Dt. 
2. Get back ‚Ñé‚Ñé ‚Ñé:  ‚Üí . 
3. Calculate the error of ‚Ñé: " = ‚àë $:%&'(	)*( 	. If " > 1/2, then set  =  ‚àí 1 and 
abort loop. 
4. Set . = " 1 ‚àí 	‚ÅÑ . 
5. Update distribution $: $0	 = 1&
	
2&
√ó 4.    if ‚Ñé	 = 1         ‚Ñé78
 
where 9 is a normalization constant (chosen so that $0 will be a distribution). 
Output the final hypothesis: ‚Ñé:;	 = arg max ‚àë AB C&:%&'	D*  
 
Figure 3
 
Training Phase 
Given 
‚Ä¢ X: the object in the training data set (an  √ó  matrix) 
‚Ä¢ Y: the labels of the training set (an  √ó 1 matrix) 
‚Ä¢ L: the number of classifiers in the ensemble 
‚Ä¢ K: the number of subsets 
‚Ä¢ , ‚Ä¶ , 
: the set of class labels 
For  = 1 ‚Ä¶  
‚Ä¢ Prepare the rotation matrix : 
o Split F (the feature set) into K subsets: Fi,j (for  = 1 ‚Ä¶ ) 
o For  = 1 ‚Ä¶  
 Let Xi,j be the data set X for the features in Fi,j 
 Eliminate from Xi,j a random subset of classes 
 Select a bootstrap sample from Xi,j of size 75% of the number of objects 
in Xi,j. Denote the new set by ,‚Ä≤  
 Apply PCA on ,‚Ä≤  to obtain the coefficients in a matrix ,   
o Arrange the  ,  for  = 1 ‚Ä¶  in a rotation matrix   as in equation (2) 
o Construct  by rearranging the columns of    to match the order of features in 
F. 
‚Ä¢ Build classifier  $  using (, %) as the training set 
Classification Phase 
‚Ä¢ For a given x, let &,(') be the probability produced by the classifier Di to the 
hypothesis that x belongs to class . By the average combination method, calculate the 
confidence for each class as follows: 
                ((') =
1
 ) &,
*
+
('),  = 1, ‚Ä¶ , ,                 
 
‚Ä¢ The test sample x is easily assigned to the class with the largest confidence. 
 
 
Figure 4
 
Figure 5
Cost-sensitive classifier Hepatitis 
TCost AvgCost Sens Spec ROC 
Rotation Forest-J.48-M1 10.3000 0.0665 0.8633 0.8125 0.8570 
Rotation Forest-J.48-M2 11.0000 0.0710 0.8403 0.8182 0.6320 
Rotation Forest-DecisionStump-M1 13.3000 0.0858 0.7974 0.5000 0.8210 
Rotation Forest-DecisionStump-M2 12.8000 0.0826 0.7935 - 0.5000 
Rotation Forest-RepTree-M1 12.8000 0.0826 0.7935 - 0.8360 
Rotation Forest-RepTree-M2 12.8000 0.0826 0.7935 - 0.5000 
ADABoostM1-J.48-M1  15.0000 0.0968 0.8828 0.6296 0.8530 
ADABoostM1-J.48-M2  13.2000 0.0852 0.8846 0.6800 0.7330 
ADABoostM1-DecisionStump-M1 14.2000 0.0916 0.8417 0.6250 0.8380 
ADABoostM1-DecisionStump-M2 16.0000 0.1032 0.8394 0.5556 0.6240 
ADABoostM1-RepTree-M1 15.1000 0.0974 0.8406 0.5882 0.7620 
ADABoostM1-RepTree-M2 16.9000 0.1090 0.8382 0.5263 0.6200 
Bagging-J.48-M1 11.3000 0.0729 0.8243 0.8571 0.8170 
Bagging-J.48-M2  10.5000 0.0677 0.8356 0.8889 0.6210 
Bagging-DecisionStump-M1 12.4000 0.0800 0.7987 1.0000 0.7850 
Bagging-DecisionStump-M2 12.8000 0.0826 0.7935 - 0.5000 
Bagging-RepTree-M1 12.4000 0.0800 0.7987 1.0000 0.8320 
Bagging-RepTree-M2 12.4000 0.0800 0.7987 1.0000 0.5160 
 
 
Table 1
Cost-sensitive classifier Pima Indians diabetes 
TCost AvgCost Sens Spec ROC 
Rotation Forest-J.48-M1 90.3000 0.1176 0.8841 0.5667 0.8320 
Rotation Forest-J.48-M2 95.7000 0.1246 0.8753 0.5479 0.7320 
Rotation Forest-DecisionStump-M1 93.3000 0.1215 0.9029 0.5185 0.8090 
Rotation Forest-DecisionStump-M2 101.1000 0.1316 0.8982 0.4868 0.6950 
Rotation Forest-RepTree-M1 90.6000 0.1180 0.8838 0.5653 0.8310 
Rotation Forest-RepTree-M2 95.4000 0.1242 0.8800 0.5407 0.7300 
ADABoostM1-J.48-M1  124.2000 0.1617 0.7935 0.6058 0.7740 
ADABoostM1-J.48-M2  118.5000 0.1543 0.8055 0.5966 0.7090 
ADABoostM1-DecisionStump-M1 92.4000 0.1203 0.8750 0.5729 0.8190 
ADABoostM1-DecisionStump-M2 108.6000 0.1414 0.8333 0.5621 0.7160 
ADABoostM1-RepTree-M1 112.2000 0.1461 0.8246 0.5607 0.7700 
ADABoostM1-RepTree-M2 101.1000 0.1316 0.8472 0.5845 0.7360 
Bagging-J.48-M1 104.4000 0.1359 0.8469 0.5532 0.8030 
Bagging-J.48-M2  95.7000 0.1246 0.8700 0.5601 0.7370 
Bagging-DecisionStump-M1 117.3000 0.1527 0.8166 0.5376 0.7290 
Bagging-DecisionStump-M2 116.4000 0.1516 0.8173 0.5454 0.6980 
Bagging-RepTree-M1 90.9000 0.1184 0.8820 0.5671 0.8220 
Bagging-RepTree-M2 95.1000 0.1238 0.8694 0.5662 0.7400 
 
 
Table 2
 
Cost-sensitive classifier Statlog (Heart) 
TCost AvgCost Sens Spec ROC 
Rotation Forest-J.48-M1 82.0000 0.3037 0.7347 0.9189 0.8900 
Rotation Forest-J.48-M2 75.0000 0.2778 0.7101 0.9524 0.7400 
Rotation Forest-DecisionStump-M1 116.0000 0.4296 0.5827 0.8750 0.8700 
Rotation Forest-DecisionStump-M2 120.0000 0.4444 0.5556 - 0.5000 
Rotation Forest-RepTree-M1 101.0000 0.3741 0.6309 0.9189 0.8680 
Rotation Forest-RepTree-M2 94.0000 0.3481 0.6379 0.9474 0.6430 
ADABoostM1-J.48-M1  170.0000 0.6296 0.8026 0.7627 0.8550 
ADABoostM1-J.48-M2  149.0000 0.5519 0.8129 0.7913 0.7990 
ADABoostM1-DecisionStump-M1 95.0000 0.3519 0.7382 0.8861 0.8790 
ADABoostM1-DecisionStump-M2 96.0000 0.3556 0.7527 0.8809 0.7750 
ADABoostM1-RepTree-M1 101.0000 0.3741 0.7513 0.8706 0.8430 
ADABoostM1-RepTree-M2 100.0000 0.3704 0.7554 0.8721 0.7760 
Bagging-J.48-M1 98.0000 0.3630 0.7268 0.8816 0.8610 
Bagging-J.48-M2  80.0000 0.2963 0.7250 0.9286 0.7540 
Bagging-DecisionStump-M1 120.0000 0.4444 0.5556 - 0.8400 
Bagging-DecisionStump-M2 120.0000 0.4444 0.5556 - 0.5000 
Bagging-RepTree-M1 76.0000 0.2815 0.6772 0.9800 0.8750 
Bagging-RepTree-M2 86.0000 0.3185 0.6607 0.9565 0.6770 
 
Table 3
