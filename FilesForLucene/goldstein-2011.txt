 
AUTOMATED CLASSIFICATION OF THE NARRATIVE OF 
MEDICAL REPORTS USING NATURAL LANGUAGE 
PROCESSING 
 
by 
 
Ira Goldstein 
 
 
 
A Dissertation 
Submitted to the University at Albany, State University of New York 
in Partial Fulfillment of 
the Requirements for the Degree of 
Doctor of Philosophy 
 
 
College of Computing and Information 
Department of Informatics 
 
2011 
 
 
Automated Classification of the Narrative of Medical Reports 
using Natural Language Processing 
 
 
by 
 
 
Ira Goldstein  
 
 
 
 
 
 
 
 
COPYRIGHT 2011 
 
 iii 
Abstract 
In this dissertation we present three topics critical to the document level 
classification of the narrative in medical reports: the use of preferred terminology in light 
of the presence of synonymous terms, the less than optimal performance of classification 
systems when presented with a non-uniform distribution of classes, and the problems 
associated with scarcity of labeled data when presented with an imbalance of classes in 
the data sets.  
The literature is replete with instances of conflicting reports regarding the value of 
applying preferred terminology to improve system performance when presented with 
synonymous terms. Our study shows that the addition of preferred terms to the text of the 
medical reports helps to improve true positives for a hand-crafted rule-based system and 
that the addition did not consistently improve performance for the two machine learning 
systems. We show that the differences in the data, task, and approach can account for the 
variations in these results as well as the conflicting reports in the literature. 
The imbalance of classes in data sets can cause suboptimal classification 
performance by systems based on an exploration of statistics for representing attributes of 
data. To address this problem, we developed specializing, a panel of one-versus-all 
classifiers, which have been activated in a strict order, and apply it to an imbalanced data 
set. We show that specializing performs significantly better than voting and stacking 
panels of classifiers when used for multi-class classification on our data. 
Machine learning systems need labeled data in order to be trained, which is 
expensive to develop and may not always be readily available. We combine the semi-
supervised approach of co-training with specializing in order to address the issues 
 iv 
associated with a scarcity of labeled examples when presented with an imbalance of 
classes in the data sets. We show that by combining co-training and specializing, we are 
able to consistently improve recall on the less well-represented classes, even when trained 
on a small number of labeled samples. 
 v 
Acknowledgements 
The path that led to this dissertation was one that could not have been traveled 
without the support of many people. I was privileged to have an exceptional and 
encouraging dissertation committee. I would like to thank Dr. Özlem Uzuner, my 
committee chair, for sharing your professional world with me and for holding me to the 
highest standards. You will always be my mentor and friend. I want to thank Dr. Jagdish 
Gangolly and Dr. George Berg, the other members of my committee, for your valuable 
suggestions which helped in the writing of this dissertation, and for our conversations 
which expanded my horizons. While the department has a number of outstanding faculty 
members, I would like to offer a special thank you to Drs. David and Deborah Andersen 
for opening your home and your hearts to everyone in the Informatics program. 
Few paths are straight and level. My dissertation experience had many peaks and 
valleys. I would like to thank Fawzi and Dana Mulki, Meg Fryling, Jack Rivituso, and 
Wil Doane for sharing the high points and for raising me up when I was low.  
This dissertation would not have been possible were it not for my family. Thank you 
to my parents, Allen and Pearl Goldstein, for giving me the confidence in my ability to do 
anything that I set my mind to. Thank you to my sons, Matthew and Eric, for your love 
and support. Last, but nowhere near least, to my wife Marilyn. Thank you for everything. 
You cleared the path for me on this journey. I am very fortunate to be able to travel on all 
of life’s journeys with you. 
 
 
 vi 
Table of Contents 
Abstract   .............................................................................................................................. iii
Acknowledgements   ............................................................................................................. v
Table of Contents   ............................................................................................................... vi
Figures   ................................................................................................................................ xi
Tables   ............................................................................................................................... xiii
Chapter 1. Introduction   ................................................................................................. 1
1.1. Key Concepts   ...................................................................................................... 2
1.2. Research Questions   ............................................................................................. 3
1.2.1. Preferred Terminology   ................................................................................ 4
1.2.2. Imbalanced Data Sets   .................................................................................. 5
1.2.3. Availability of Labeled Data   ....................................................................... 6
1.3. Approach, Design, and Contribution   .................................................................. 7
1.4. Structure of the Dissertation   ............................................................................... 8
1.5. Summary   ........................................................................................................... 10
Chapter 2. Related Literature   ...................................................................................... 11
2.1. Introduction   ....................................................................................................... 11
2.2. Natural Language Processing   ............................................................................ 11
2.2.1. Morphology   ............................................................................................... 12
2.2.2. Syntactics   .................................................................................................. 13
2.2.3. Semantics   .................................................................................................. 15
2.3. Automated Classification   .................................................................................. 24
2.3.1. Hand-Crafted Rules   .................................................................................. 25
 vii 
2.3.2. Supervised Machine Learning  ................................................................... 27
2.3.3. Unsupervised Machine Learning   .............................................................. 31
2.3.4. Semi-supervised Machine Learning   .......................................................... 31
2.3.5. Combining Classifiers   ............................................................................... 32
2.4. Challenges and Limitations in the Literature   .................................................... 34
2.5. Summary   ........................................................................................................... 37
Chapter 3. Medical Records and Evaluation Metrics   ................................................. 38
3.1. Introduction   ....................................................................................................... 38
3.2. Data   ................................................................................................................... 38
3.2.1. CMC Data Set   ........................................................................................... 39
3.2.2. i2b2 Data Sets   ........................................................................................... 40
3.2.3. Institutional Review Board   ....................................................................... 44
3.3. Evaluation Metrics   ............................................................................................ 45
3.4. Summary   ........................................................................................................... 47
Chapter 4. Examining the Role of Preferred Terminology in the Document Level 
Classification of Medical Reports   ..................................................................................... 48
4.1. Introduction   ....................................................................................................... 48
4.2. Controlled Vocabulary   ...................................................................................... 50
4.3. Data   ................................................................................................................... 52
4.4. Methods  ............................................................................................................. 53
4.4.1. Hand-Crafted Rule-Based System   ............................................................ 54
4.4.2. Lucene Classifier   ....................................................................................... 58
4.4.3. BoosTexter Classifiers   .............................................................................. 60
 viii 
4.4.4. Noun Phrase Detection and Preferred Term Identification   ....................... 62
4.4.5. Surface Processing   .................................................................................... 70
4.5. Evaluation   ......................................................................................................... 71
4.6. Results and Discussion   ..................................................................................... 73
4.6.1. Hand-Crafted Rule-Based System   ............................................................ 73
4.6.2. NPDP Post-Hoc Coordination   .................................................................. 74
4.6.3. Lucene Classifier   ....................................................................................... 75
4.6.4. BoosTexter Classifiers   .............................................................................. 77
4.7. Conclusions   ....................................................................................................... 80
4.8. Summary   ........................................................................................................... 82
Chapter 5. Specializing for Predicting Obesity and its Co-morbidities   ...................... 83
5.1. Introduction   ....................................................................................................... 83
5.2. Related Work   .................................................................................................... 87
5.2.1. Combining Classifiers   ............................................................................... 87
5.2.2. Combined Classifiers   ................................................................................ 88
5.3. Materials and Methods   ...................................................................................... 90
5.3.1. Data   ........................................................................................................... 91
5.3.2. Training and Test Sets   .............................................................................. 92
5.3.3. Feature Extraction   ..................................................................................... 92
5.3.4. Weka   ......................................................................................................... 95
5.3.5. Specializing   ............................................................................................... 97
5.3.6. Evaluation Metrics and Methods   ............................................................ 105
5.4. Results and Discussion   ................................................................................... 113
 ix 
5.4.1. Aggregate Result Analysis   ...................................................................... 113
5.4.2. Disease-Level Results Analysis   .............................................................. 117
5.4.3. Specializing analysis and discussion   ....................................................... 119
5.5. Limitations and future work   ............................................................................ 120
5.6. Accessibility of Data   ....................................................................................... 122
5.7. Conclusions   ..................................................................................................... 123
5.8. Acknowledgements   ......................................................................................... 123
Chapter 6. Co-Specializing: Addressing the Scarcity of Labeled Data   .................... 125
6.1. Introduction   ..................................................................................................... 125
6.2. Co-Training   ..................................................................................................... 127
6.3. Co-Specializing Classifier   .............................................................................. 128
6.4. Data   ................................................................................................................. 131
6.4.1. Feature Extraction   ................................................................................... 132
6.4.2. Seed, Training, and Test Sets   .................................................................. 134
6.4.3. Data Views   .............................................................................................. 135
6.5. System Development   ...................................................................................... 137
6.6. Evaluation   ....................................................................................................... 138
6.7. Results and Discussion   ................................................................................... 140
6.7.1. Aggregate Obesity Results   ...................................................................... 140
6.7.2. Individual Obesity Disease Results   ......................................................... 143
6.7.3. Smoking Status Results   .......................................................................... 145
6.7.4. Co-Specializing Discussion   .................................................................... 153
6.8. Conclusions   ..................................................................................................... 154
 x 
6.9. Summary   ......................................................................................................... 155
Chapter 7. Conclusions   ............................................................................................. 156
7.1. Introduction   ..................................................................................................... 156
7.2. Major Findings   ................................................................................................ 156
7.2.1. Preferred Terminology   ............................................................................ 156
7.2.2. Imbalanced Data Sets   .............................................................................. 159
7.2.3. Availability of Labeled Data   ................................................................... 161
7.3. Strengths and Limitations   ............................................................................... 162
7.4. Issues for Additional Exploration   ................................................................... 164
References   ....................................................................................................................... 166
Appendix   ......................................................................................................................... 189
A. Instructions for Annotating Semantic Correctness  ................................................. 189
B. Abbreviations   ......................................................................................................... 191
 
 xi 
Figures 
Figure 1 - Sample radiology report from the XML ICD-9-CM data released as part of the 
2007 CMC challenge.   ....................................................................................................... 40
Figure 2 - Sample discharge summary from i2b2 Shared-Task and Workshop on 
Challenges in Natural Language Processing for Clinical Data: Obesity Challenge.   ........ 42
Figure 3 - Hand Crafted Rule-Based Coder - Developed from an examination of the test 
data set. Synonyms developed from ICD-9-CM code definitions and expanded with 
recurrent terms indentified in the text. Employs Negex’s pre-UMLS uncertainty and 
negation phrases. Given the task of assigning codes only for definitive diagnosis, 
uncertainty phrases are treated as negation. After processing with synonymy, uncertainty, 
and negation a series of simple rules are applied.   ............................................................. 57
Figure 4 - Lucene Based Coder – Training data are indexed (tf•idf) using the Apache 
Lucene libraries. Test samples are treated as queries to the index. The top three ranked 
indexed (training) documents are retrieved. Any codes used by a majority of the retrieved 
records are assigned to the test sample. If there is no code in common among the top three 
retrieved documents, the fourth highest ranked document is retrieved. Any codes in 
common between the four retrieved records are assigned to the sample. If no codes are in 
common, a NULL code is assigned to the test sample.   .................................................... 60
Figure 5 - Boosting Based Coder – Boosting is a machine learning algorithm for 
improving the performance of supervised learning systems. It performs several iterations 
of breaking the data into subsamples, and training “weak learners” (i.e., a classifier that 
performs slightly better than chance). The weak learners are combined to create the 
boosted classifier. The boosted classifier assigns codes to each sample in the test data.   . 62
 xii 
Figure 6 - General and Applied Processes for Selecting Specialist Classifiers   ................ 99
Figure 7 - General and Applied Processes for Selecting Catch-all Classifiers.   .............. 100
Figure 8 - Specializing Classifiers for a General Task and for the Disease Gallstones.   . 101
 xiii 
 
Tables 
Table 1- i2b2 Obesity Data. Y=Present, N=Absent, Q=Questionable, and 
U=Unmentioned Number of samples in each class in the training and test sets.   ............. 43
Table 2 - Smoking Status Training and Test Data. Number of samples per class   ............ 44
Table 3 - UMLS Semantic Types and Codes   .................................................................... 63
Table 4 - Number of times disease related noun phrases were detected by NPDP.   .......... 64
Table 5 - Consistency of the NPDP assigned preferred terms in the CMC data set.   ........ 67
Table 6 - Consistency of a sample of 4000 NPDP assigned preferred terms from the i2b2 
Obesity data set.   ................................................................................................................ 67
Table 7 - CMC Corrected Preferred Term Lucene Classifier runs at several levels of 
manually induced inconsistency.   ...................................................................................... 68
Table 8 - CMC Corrected Preferred Term Unigram BoosTexter Classifier runs at several 
levels of manually induced inconsistency.   ........................................................................ 68
Table 9 - CMC Corrected Preferred Term N-Gram BoosTexter Classifier runs at several 
levels of manually induced inconsistency.   ........................................................................ 68
Table 10 - Hand-Crafted Rule-Based System Performances.   ........................................... 74
Table 11 - Number of unique noun phrases and unique preferred terms by data set.   ....... 75
Table 12 - Lucene Classifier Performance.   ....................................................................... 77
Table 13 - BoosTexter Classifier Performance.   ................................................................ 79
Table 14 - Aggregate Results for Classifying Obesity and Fifteen of its Co-morbidities by 
Combined and Complementary Classifiers   ..................................................................... 109
 xiv 
Table 15 - Performance per Class, Aggregated Over All Diseases.   ............................... 110
Table 16 - Macro-averaged F-measure by Disease.   ........................................................ 112
Table 17 - Assignments by Specialists and Catch-all Classifiers by Disease.   ................ 113
Table 18 - Obesity Ground Truth.   ................................................................................... 135
Table 19 - Smoking Ground Truth.   ................................................................................. 135
Table 20 - Base Classifier F-Measure for each disease and smoking data set on each of 
the data views.   ................................................................................................................. 137
Table 21 - Co-Specializing Experiments   ........................................................................ 138
Table 22 - Performance per class, aggregated over six diseases.   .................................... 141
Table 23 - Asthma, performance per class.   ..................................................................... 146
Table 24 - Atherosclerotic CV Disease, performance per class.   ..................................... 147
Table 25 - Diabetes, performance per class.   ................................................................... 148
Table 26 - GERD, performance per class.   ...................................................................... 149
Table 27 - Hypercholesterolemia, performance per class.   .............................................. 150
Table 28 - Obesity, performance per class.   ..................................................................... 151
Table 29 - Smoking, performance per class.   ................................................................... 152
Table 30 - Abbreviations used.   ....................................................................................... 191
 
 1 
Chapter 1. Introduction 
The application of natural language processing (NLP) to the narrative of medical 
reports can inform many applications, including those that provide syndromic 
surveillance (Shapiro, 2004), develop patient problem lists (Bui, Taira, El-Saden, 
Dordoni, & Aberle, 2004), aid decision support (Fiszman, Chapman, Aronsky, & Evans, 
2000), and assign billing codes (Farkas & Szarvas, 2008). The objective of this 
dissertation was to develop methods that improve the automatic document level 
classification of the narrative of medical reports. Manual assignment of classes to 
documents is laborious and is subject to the inconsistencies associated with any labor 
intensive process. In addition to reducing the labor involved, automatic assignment of 
classes also has the benefit of eliminating irregularities in classification that can arise due 
to human error. This chapter describes key concepts, next develops research topics, then 
examines the approach, design, and contribution taken, and lastly presents a roadmap of 
the subsequent chapters.  
This dissertation presents three topics that are critical to the document level 
classification of the narrative of medical reports: the application of preferred terminology 
in light of the presence of synonymous terms, the less than optimal performance of 
classification systems when presented with a non-uniform distribution of classes, and the 
problems associated with scarcity of labeled data when presented with an imbalance of 
classes in the data sets. In order to insure a common understanding, we first define several 
key concepts before presenting research topics. 
 
 2 
1.1. Key Concepts 
Synonyms are words or phrases that have the same essential sense or meaning. For 
example, kidney stone and renal calculi both refer to the same medical condition. The 
presence of these semantically-equivalent but lexically-disparate terms can lead to errors 
in classification, question answering, and information retrieval. A controlled vocabulary 
attempts to address this problem by creating a predefined list of preferred terms, each of 
which describe a single concept. A preferred term gathers lexically-disparate but 
semantically-equivalent terms under one term and provides a consistent means of 
referring to a given concept. While the extensive use of controlled vocabularies has been 
reported in the medical informatics literature, the results have been mixed. As described 
in detail in section  2.2.3.3, some of the reports have shown that the application of 
controlled vocabularies to clinical records can aid many tasks, while others have shown 
no improvement. The conflict in the literature prompted this investigation into the 
applicability of preferred terms to the classification of narrative medical records. 
Real-world data is messy. The distribution of classes is typically skewed (e.g., in 
medical records, the presence of a disease in patients is less frequent than the absence of 
the disease), with few examples of the less well-represented classes. This non-uniform 
distribution of classes and sparsity of samples can pose challenges to methods, such as 
classification, that are based on an exploration of statistics for representing attributes of 
data. In statistical classification, the small number of examples offered by a sparse, less 
well-represented class may adversely affect the training of a classifier on that class. This 
suboptimal classification performance of the less well-represented classes occurs as a 
result of maximizing overall performance. While challenging to classify, the less well-
 3 
represented classes are often the more interesting classes (Kotsiantis & Pintelas, 2009) 
and it is important to classify them correctly. 
Training statistical machine learning classifiers to predict labels for pre-determined 
classes requires labeled data. The cost associated with the labor of manually labeling a 
data set is high and resources are often limited. This problem is compounded when trying 
to label a sufficient number of examples given the inherent skewed distribution of most 
data sets. Developing approaches that address the imbalance found in data and the limited 
availability of labeled data is both important and challenging. 
1.2. Research Questions 
This dissertation addresses three topics that are important to the classification of the 
narrative of medical reports. The first topic relates to the presence of synonymous terms 
in the narrative of the medical reports and the ability to achieve a semantic understanding 
of the text. Does classification performance improve when synonymous terms are 
brought together under a standardized preferred term? The second topic looks at the 
imbalance caused by the non-uniform distribution of classes in the data that can cause 
less than optimal performance in classification systems. When presented with a non-
uniform distribution of classes in the data, can the application of a panel of one-versus-
all classifiers improve performance on the less well-represented classes? The third topic 
concerns the limited amount of available labeled data for use in training classification 
systems. When presented with a limited amount of labeled data for use in training 
classification systems, can the application of a panel of one-versus-all classifiers improve 
performance on the less well-represented classes? In order to address these topics, this 
 4 
dissertation examined eight specific research questions that are detailed in the following 
three sections.  
1.2.1. Preferred Terminology 
The first set of questions relate to issues brought about by synonymy. A concept can 
be represented by semantically-equivalent but lexically-disparate terms (i.e., synonyms). 
The presence of synonyms in medical reports may mask the semantic similarity between 
the synonyms (Cimino, Hripcsak, & Johnson, 1994) and lead to errors in translation, 
question answering, and information retrieval. Bringing synonyms together under a 
standardized preferred term can obviate the lexical disparity between semantically-
equivalent terms, and can help reduce the errors caused by lexical disparity. 
As described fully below in the section on Synonymy (page 21), the literature is 
replete with examples of problems related to lexical disparity caused by synonyms. While 
some studies have reported improved system performance when resolving lexical 
disparity, cases have also been reported where resolving lexical disparity does not 
improve system performance. These conflicting reports in the literature inspired the 
following research questions: 
• Will the addition of preferred terms for diseases and symptoms improve system 
performance of a hand-crafted rule-based system in a multi-label document 
classification task?  
• Will the addition of preferred terms for diseases and symptoms, when enhanced 
with the polarity of assertions about diseases and symptoms, improve system 
 5 
performance of a hand-crafted rule-based system in a multi-label document 
classification task? 
• Will the addition of preferred terms for diseases and symptoms improve system 
performance of machine learning systems in a multi-label document 
classification task?  
• Will the addition of preferred terms for diseases and symptoms, when enhanced 
with the polarity of assertions about diseases and symptoms, improve system 
performance of machine learning systems in a multi-label document 
classification task? 
1.2.2. Imbalanced Data Sets 
The second set of research questions relate to imbalances found in data sets. 
Chawla, Japkowicz, and Kotcz (2004) note problems when presented with a non-uniform 
distribution of samples among the classes (e.g., there are very few cases of a particular 
disease when compared to the total number of patients seen by a physician). They believe 
that the imbalance caused by the non-uniform distribution of classes in data sets is 
pervasive and ubiquitous. This imbalance may cause suboptimal classification 
performance by systems based on an exploration of statistics for representing attributes of 
data. Given a non-uniform distribution of classes, machine learning classifiers may 
simply ignore the less-well represented classes in order to maximize overall performance 
(Tang & Liu, 2005). These observations motivated the following research questions: 
 6 
• Can the application of a panel of one-versus-all classifiers, when activated in a 
strict order, outperform J48, Naïve Bayes, and AdaBoost.M1 classifiers on the 
less well-represented classes in a multi-class, multi-label task? 
• Can the application of a panel of one-versus-all classifiers, when activated in a 
strict order, outperform the voting and stacking approaches to combining J48, 
Naïve Bayes, and AdaBoost.M1 classifiers on the less well-represented classes 
in a multi-class, multi-label task? 
1.2.3. Availability of Labeled Data 
The third set of research questions relate to the limited availability of labeled data. 
Despite initiatives such as the Semantic Web (Berners-Lee, 1998) and the HL7 for 
electronic health records (Dolin et al., 2006), the contents of many sources of data remain 
predominately an uncategorized and unclassified bastion of free text. Even though 
unclassified data may be obtainable, creating labeled data with which to train machine 
learning classification systems requires human expertise (Zhou, 2009) and can be tedious 
and expensive to develop (Zhong, 2005).  
Semi-supervised methods of machine learning, such as co-training (Blum & 
Mitchell, 1998), utilize unlabeled data to improve the results of supervised machine 
learning systems that have been trained with a small labeled training set. However, semi-
supervised methods of machine learning do not inherently address the issues associated 
with imbalanced data sets. These observations prompted the following research 
questions: 
 7 
• Does the application of co-training to a panel of one-versus-all classifiers, when 
activated in a strict order, outperform J48, Naïve Bayes, and SVM classifiers on 
the less well-represented classes in a multi-class classification task when 
presented with a scarcity of labeled data? 
• Does the application of co-training to a panel of one-versus-all classifiers, when 
activated in a strict order, outperform J48, Naïve Bayes, and SVM based 
co-training classifiers on the less well-represented classes in a multi-class 
classification task when presented with a scarcity of labeled data? 
1.3. Approach, Design, and Contribution 
The narrative of medical reports can inform many applications. However, in order 
to extract meaningful information, the medical reports need to be processed. The purpose 
of this dissertation was to improve the understanding and use of foundational natural 
language processing tools and of machine learning approaches in classifying the narrative 
of medical reports. This research focused on one domain in order to prevent an overly 
complex undertaking. This dissertation extended previous work in both NLP and machine 
learning. 
This dissertation used multiple data sets from multiple sources. These data sets 
represented two different types of documents, radiological reports and discharge 
summaries. As detailed in section  3.2, the characteristics of the radiological reports differ 
from those of the discharge summaries. 
 8 
This dissertation used multiple approaches to classification: hand-crafted rule-based 
and machine learning approaches. The machine learning approaches used individual 
classification algorithms as well as panels of individual classifiers. 
This dissertation used baselines and intermediate systems with which to answer the 
above stated research questions. Comparing full systems to both baseline and 
intermediate systems enabled analysis that determined which component, or combination 
of components, provided the improvement of system performance. 
1.4. Structure of the Dissertation  
This dissertation is presented in several parts. Chapters 1-3 provide context and 
background for the reader. Chapters 4-6 present essays that address each of the research 
questions. Chapter 7 reviews the findings, discusses limitations, and discusses issues for 
additional exploration. 
Chapter 1 presents an overview of the research, objectives, and research questions. 
This dissertation is informed by work in the domains of natural language processing and 
machine learning. Chapter 2 presents relevant literature in these two domains, focusing 
on their application to the narrative of medical reports, and explores the strengths and 
weaknesses found in the literature. Understanding the characteristics of the data provides 
context for the results. Chapter 3 describes the data sets employed in this dissertation and 
the evaluation metrics used in the experiments.  
Chapter 4 examines the issue of lexical disparity and the role preferred terminology 
plays as a means of post-hoc coordination in automatic classification of patient medical 
data. We apply natural language processing tools to the free text of the medical reports. 
 9 
We build and evaluate both a hand-crafted rule-based system and two machine learning 
systems: one based upon Lucene and the other based upon BoosTexter. We experiment 
with medical reports from two different corpora: one containing radiology reports and the 
other containing discharge summaries. We gather semantically-equivalent but lexically-
disparate medical terms used in these reports under preferred terms for multi-label 
classification of the medical reports. 
Chapter 5 reports on a new method for combining classifiers that helps to address 
the problems associated with imbalance of classes in the data sets. In this chapter we 
present specializing, a method for combining classifiers for multi-class classification. 
Specializing trains one specialist classifier per class and utilizes each specialist to 
distinguish that class from all others in a one-versus-all (OVA) manner. It then 
supplements the specialist classifiers with a catch-all classifier that performs multi-class 
classification across all classes. We refer to the resulting combined classifier as a 
specializing classifier. 
Chapter 6 reports on the application of specializing to the problem of imbalance 
given a scarcity of labeled data. In this chapter we present co-specializing, a method that 
combines the approaches of co-training and specializing to address the issues associated 
with a scarcity of labeled examples when presented with an imbalance of classes in the 
data sets. Co-specializing employs two panels of classifiers, each panel making use of a 
different view of the data.  
 This dissertation concludes with Chapter 7, which reviews the findings, discusses 
limitations, and proposes future research. This is followed by a list of references cited, the 
 10 
instructions used for annotating semantic correctness, and a list of abbreviations used in the 
dissertation. 
1.5. Summary 
This dissertation contributes to our understanding of the role preferred terminology 
plays in the classification of the narrative of medical reports. This dissertation also 
provides a new method for combining classifiers. This method addresses issues with 
statistical machine learning classifiers when they are presented with a non-uniform 
distribution of samples among the classes. Lastly, this dissertation examines the 
capability of this new method when presented with a small set of labeled samples and a 
large set of unlabeled samples. 
 
 11 
Chapter 2. Related Literature 
2.1. Introduction 
This dissertation is informed by work in the domains of natural language processing 
and machine learning. Both of these domains employ textual data. This chapter reports on 
the relevant literature in these two domains, focusing on their application to the narrative 
of medical reports, and explores the strengths and weaknesses in the literature.  
The literature review begins with a description of natural language processing and 
the morphologic, syntactic, and semantics tools employed. It continues by describing 
research into issues related to ambiguity, negation, and synonymy. This chapter then 
proceeds with a look at automated classification, including hand-crafted rules, supervised, 
unsupervised, and semi-supervised machine learning methods, as well as approaches for 
combining multiple classifiers. This section finishes with a discussion of the conflicts and 
questions of generalizability found in the literature. 
2.2. Natural Language Processing 
Natural language processing provides morphologic, syntactic, and semantic tools 
(Collier & Takeuchi, 2004; Manning & Schütze, 1999) to help transform stored text from 
raw data (e.g., dictated text) to useful information (e.g., a standardized diagnostic 
classification). Morphologic tools look at stored text as a sequence of linguistic units 
without regard to context or meaning. Syntactic tools provide grammar-related 
information about the text. Semantic tools, such as thesauri and ontologies, provide 
information about the meaning and sense of words. Taken together, these NLP tools can 
be used to resolve issues brought about by ambiguity, negation, and synonymy found in 
 12 
stored text. NLP has been successfully applied to stored text in a wide range of 
applications, including machine translation (Somers, 1999; Weaver, 1955), question 
answering (Q.-l. Guo & Zhang, 2009; Katz & Lin, 2002), information retrieval (Lewis & 
Spärck Jones, 1996; Sager, 1976; Tzoukermann, Klavans, & Strzalkowski, 2003), 
document summarization (Luhn, 1958; Spärck Jones, 2007), and author identification 
(Mosteller & Wallace, 1963; Stamatatos, 2009). 
2.2.1. Morphology 
The first NLP step in transforming stored text into useful information is to divide 
the text into linguistic units (Mikheev, 2003). Each linguistic unit corresponds to a word, 
number, or punctuation mark. These linguistic units are referred to as tokens. The process 
of dividing the text into linguistic units is called tokenization. 
Morphologic tools look at stored text as a sequence of tokens. Morphologic tools 
can provide information about token frequency (Zipf, 1949), document length, and 
vocabulary richness (Salton & McGill, 1983). To be more effective, syntactic and 
semantic tools can benefit from surface processing provided by morphological string 
manipulation. 
Two common types of surface processing are case conversion and stemming, both 
of which attempt to conflate variations of a given token. Case conversion transforms each 
token so that all of the alphabetic characters are the same case. For example, when 
converting to lower case, the tokens EXTRACT and Extract would both become extract. 
Stemming brings together variations of a word (Lovins, 1968; Porter, 1980) by 
manipulating word suffixes. For example, the tokens extraction and extracted would both 
 13 
be transformed into the token extract. It should be noted that in tasks such as information 
retrieval, stemming typically improves recall while reducing precision (Tzoukermann et 
al., 2003), performance metrics commonly used in natural language processing tasks (see 
section  3.3 for a full description of these metrics). Stemming algorithms do not always 
correctly handle the inconsistencies present in natural languages. Lovins (1971) describes 
under-stemming and over-stemming issues that occur in stemming algorithms. 
Under-stemming occurs when stemming fails to bring together two words that should 
otherwise be brought together (e.g., patience and patiently). Over-stemming occurs when 
stemming brings together words that should be kept separate (e.g., experiment and 
experience both stem to experi). 
2.2.2. Syntactics 
In addition to morphological processing, written text can be analyzed by looking at 
the rules for arranging words into phrases, sentences, and paragraphs (i.e., syntactics), as 
well as by looking at the meaning and sense of the words (i.e., semantics). Syntactic tools 
provide grammar-related information about the text. Syntactic tools can parse the text, 
examining each token in context (i.e., in relationship to the surrounding tokens), and can 
tag each token in the text with its grammatical part of speech (e.g., verb, noun, adjective) 
(Brill, 1992; Church, 1988; DeRose, 1988; Janas, 1977). For example, pool is a noun in 
They swam in the pool. but a verb in They decided to pool their resources. The 
information provided by part of speech (POS) tools is not limited to individual tokens. 
Shallow parsing divides the text into non-overlapping series of contiguous tokens 
(chunks) based upon their linguistic properties (Federici, Montemagni, & Pirrelli, 1996). 
 14 
Each chunk is then tagged with phrasal information. For example, the tokens urinary, 
tract, and infection when appearing together can be identified as the distinct noun phrase 
urinary tract infection. Approaches to tagging have been based upon both statistical (e.g., 
context-pattern rules and Hidden Markov Models) and non-statistical (e.g., 
transformation-based rules and context free grammars) methods. 
Grammatical parsing and POS tagging can be accomplished in many different ways. 
Greene & Rubin’s TAGGIT (1971) uses hand-crafted context-pattern rules to assign POS 
tags. These assignments are made based upon a word’s character patterns as well as upon 
the POS tags of the words to the right and to the left (i.e., its local context). This early 
system could disambiguate approximately 77 percent of the words in the Brown 
University million word corpus (Voutilainen, 2003). The University of Lancaster 
CLAWS system (Marshall, 1983, 1987) extends TAGGIT by applying probabilities of the 
occurrence of bigrams (pairs of consecutive words), derived from a subset of the Brown 
corpus. This use of statistical corpus evidence improves accuracy at the expense of 
needing a tagged corpus in order to determine bigram probabilities. 
A second statistical approach, Hidden Markov Models (HMM), has been 
successfully applied to POS tagging (Cutting, Kupiec, Pedersen, & Sibun, 1992). One 
advantage of an HMM approach is that it does not require previously tagged text for 
training. This approach is considered hidden since the POS categories are not observable. 
A second advantage is that HMMs operate on sentences and not on a word’s local 
context. While HMMs can be very accurate, since they do not build upon pre-tagged text, 
the results stray at times from correct tag assignments (Abney, 1997). 
 15 
Transformation-based tagging (Brill, 1992, 1995; Hindle, 1989) is a non-statistical 
syntactic NLP tool. Transformation-based tagging is an iterative process that generates an 
ordered list of rules. The process begins by tagging known words with their most-
frequently occurring tag, and tagging unknown words with an arbitrary tag such as noun. 
The process then compares the current tagging with a hand-tagged training text. For each 
tagging error identified, a candidate new rule is generated. If the new rule corrects more 
tags than it breaks, it is then added to the ordered list as an exception and the tagging is 
updated based upon the new rule. The process then repeats the comparison to the hand-
tagged training set and new rule creation steps until no new rules are added to the ordered 
list. 
Another non-statistical approach to parsing text utilizes context free grammars. A 
context free grammar looks at the syntax of a language “as having a small, possibly finite 
kernel of basic sentences … along with a set of transformations that can be applied to 
kernel sentences or to earlier transforms to produce new and more complicated sentences 
from elementary components” (Chomsky, 1956, p. 124). Context free grammars are 
useful in analyzing the syntax of languages and sub-languages and can be used to parse 
free text. 
2.2.3. Semantics 
While NLP tools can resolve issues associated with morphology and syntax, the 
semantic understanding (i.e., the meaning) of text remains a challenge. The nature of 
meaning is still an unsettled philosophical question (Manin, 2008). In fact, there is no 
universally agreed to meaning of the word meaning. Brodbeck (1968) describes four 
 16 
distinct meanings for the term meaning: referential, significance or lawfulness, 
intentional, and psychological. Even when referential, the articulated meaning of a 
concept can be generalized, indirect, or otherwise vague. S.C. Levinson (2000, p. 29) 
explains this by noting that, with respect to human effort, “inference is cheap, articulation 
expensive.” For example, an individual who says I am going to the city has avoided 
explicitly articulating their destination, and has left it up to the listener to infer the name 
of the city to which they are going. 
 Semantic tools, such as thesauri and ontologies, provide information about the 
meaning and sense of words. The semantic context determines dependencies (e.g., “part 
of” or “is a” types of relationships) and the sense intended for a particular word (Miller, 
1995; Miller, Beckwith, Fellbaum, Gross, & Miller, 1990), i.e., it is used to disambiguate 
the word. While POS taggers can determine when a given word is used as a noun or as a 
verb, they can not be used by themselves to disambiguate the sense of a word for a given 
part of speech. For example, it is necessary to disambiguate the noun bank in order to 
understand if we met at the bank refers to a gathering at a financial institution or at the 
sloping land adjacent to a body of water. 
2.2.3.1. Ambiguity 
Ambiguity is the inability to discern the sense of a polysemous word (i.e., a word 
with two or more meanings) in a specific context. Word sense disambiguation (WSD) 
attempts to determine the correct sense of a word (Schütze, 1998). Using a POS tagger by 
itself can help to disambiguate a word based upon the part of speech (e.g., see the 
noun/verb example for the word pool above in the section on Syntactics). However, the 
tagger can not disambiguate when the various senses have the same part of speech. For 
 17 
example, is Monty Hall the name of a person, or the name of an auditorium? One 
approach to performing WSD is to combine multiple methods. McRoy (1992) combines 
POS tagging with morphology, collocations, and word associations lexical knowledge 
sources. She assigns weights to the senses provided by each knowledge source to 
determine the correct sense. She tests the system on a 25,000 word corpus from the Wall 
Street Journal and was able to disambiguate 98% of all non-proper nouns and non-
abbreviated words. 
Word sense clustering (Pereira, Tishby, & Lee, 1993; Purandare & Pedersen, 2004; 
Schütze, 1998) has been used to perform WSD. Word sense clustering uses untagged text, 
and cluster instances of a word, based only on the instances’ mutual contextual 
similarities. Contextual similarity is based upon surface lexical features. Each word is 
represented by a context vector in a high dimensional word space. Disambiguation is 
accomplished by mapping the ambiguous word to the word space and then selecting the 
cluster whose centroid is closest. 
WSD may be aided by two observations about word senses. Gale, Church, and 
Yarowsky (1992, p. 236) observe that polysemous words that appear multiple times “in a 
well-written coherent discourse” will all share one sense per discourse. Yarowsky (1993) 
takes this further and observes that words have one sense per collocation (defining 
collocation as the “co-occurrence of two words in some defined relationship”). Krovetz 
(1998) and Martinez and Agirre (2000) have made contrary observations. They each 
report that multiple senses occur more than 30% of the time in a corpus. However, 
Martinez and Agirre also report that one sense per collocation does appear to hold within 
a given corpus, but that the collocations vary across corpora. 
 18 
2.2.3.2. Negation and Assertion 
Even when presented with unambiguous word senses, the text surrounding a word 
or phrase can alter the assertion being made about the word or phrase (e.g., compare the 
assertions in I am hungry. and I am not hungry.). Assertions can be divided into positive 
assertions, negative assertions, and uncertain assertions (Chapman, Bridewell, Hanbury, 
Cooper, & Buchanan, 2001). Understanding the type of assertion made in a statement is 
important in interpreting the meaning of the statement. 
According to Horn and Katō (2000, p. 1), “negative utterances are a core feature of 
every system of human communication.” However, negation is considered to be a 
troublesome aspect of medical NLP (Friedman, Alderson, Austin, Cimino, & Johnson, 
1994). The ability to decode negation is important in understanding medical narratives 
(Chapman et al., 2001). In the narrative of medical reports, physicians often state that a 
given condition is absent or that a disease can be ruled out (Mutalik, Deshpande, & 
Nadkarni, 2001). The simple detection that a condition or disease is mentioned in a 
medical report is not sufficient to state that the condition or disease is present in the 
patient. 
 In addition to simple negation phrases, such as patient denies pneumonia or 
unremarkable two views of the chest without focal pneumonia, where it is obvious that a 
given diagnoses has been eliminated, the negation phrase may sometimes be at a distance 
from the diagnosis which is being negated (Chapman et al., 2001). For example, in the 
sentence No fevers, chills, sweats, nausea, vomiting, diarrhea, chest pain and only mild 
shortness of breath. the word no applies to everything in the sentence except for mild 
shortness of breath. While the prior example contained hints to determine the scope, such 
 19 
hints do not appear in all instances. Therefore, the scope of a negation may not always be 
evident. For example, in the short sentence No cough, fever., is fever present or absent? 
Assertions made in medical reports are used to form a diagnosis, and are frequently 
divided into positive assertions, negative assertions, and uncertain assertions. Positive 
assertions are those that are believed to be true and, for example, indicate that a disease is 
present in the patient. Negative assertions are those diagnoses that have been eliminated 
as potential problems in the patient, while uncertain assertions are those that have not 
been eliminated, but are not definitively present in the patient. Medical professionals will 
often assert negative or uncertain diagnoses in patient records (Rao, Sandilya, Niculescu, 
Germond, & Rao, 2003). These negative and uncertain assertions are used to provide 
information that contrasts with the positively asserted diagnoses (Kim & Park, 2006), as 
well as to catalog all of the diagnoses that have been considered in order to maintain a 
record reflecting the thought process used to arrive at the final diagnosis. 
Negative and uncertain assertions in text can adversely affect the performance of 
NLP systems. For example, Xu and Croft (2000, p. 93), while investigating the impact of 
local context on information retrieval, referred to “negative indicators of relevance” as 
leading to the retrieval of incorrect concepts when a negative phrase was included in a 
query. Rao et al. (2003) note that the information contained in the negative phrases of a 
diagnosis is lost when using a bag-of-words approach to mine medical text, which then 
results in negated items being erroneously considered as if they were present in the 
patient. 
Several research efforts focus on identifying and making use of uncertain and 
negative assertions in text. Mutalik et al. (2001) use NegFinder (look-ahead, left-
 20 
recursive (1) grammar) to show that the Unified Medical Language System (UMLS) can 
be used to reliably detect negated concepts in medical narratives. They examine full 
negations and total absence of a concept, and find that negations generally occurred in 
close proximity to the target. They conclude (page 598) that “[n]egation of most concepts 
in medical narrative can be reliably detected by a simple strategy. The reliability of 
detection depends on several factors, the most important being the accuracy of concept 
matching.” Sibanda (2006) extends NegEx (Chapman et al., 2001), a series of regular 
expressions that both precede and follow target words, in order to identify not just 
positive, negative, and uncertain assertions, but also assertions made in reference to 
someone other than the patient. Patrick et al. (2007) use an atomic parse approach, 
finding that negations do not often cross sentence boundaries. Harkema, Dowling, 
Thornblade, and Chapman (2009) also extend NegEx to identify assertions that are 
historical (i.e., reports of prior conditions in the patient). 
The use of negation has relevance to real-world applications. This can be illustrated 
by examining the role of negation in the assignment of standardized diagnostic codes to 
medical reports. For example, the 9th Revision of the International Classification of 
Diseases (ICD-9) is a standardized list of diagnostic classifications (World Health 
Organization, 2009) and is used to code clinical information, billing charges (Puckett, 
1986), and epidemiological records (Tsui, Wagner, Dato, & Chang, 2002). ICD-9 codes 
can also assist in the development of patient problem lists (Bui et al., 2004). ICD-9-CM is 
the Clinical Modification to ICD-9 and is the standard used in the United States (National 
Center for Health Statistics, 2007). 
 21 
Creating ICD-9-CM coding systems requires recognizing the disease(s) and 
symptom(s) positively asserted in the medical narratives to be present in the patient 
(Lussier, Shagina, & Friedman, 2000). Over-coding reports, i.e., assignment of 
unnecessary ICD-9-CM codes to a report, is a discouraged practice (Pedersen, Pakhomov, 
Patwardhan, & Chute, 2007; Pestian et al., 2007). Therefore, while physicians often assert 
uncertain or negative diagnoses in patient records (Rao et al., 2003), either to provide 
information that contrasts with the positive diagnoses (Kim & Park, 2006) or to keep 
track of all potential diagnoses that have been considered or dismissed, these uncertain or 
negative diagnoses should not be coded (Tulane University, 2006). The presence of 
negative and uncertain assertions in patient records complicates automatic ICD-9-CM 
coding. 
2.2.3.3. Synonymy 
 “[T]he complexity, variability, and richness of natural language, rather ironically, 
leads to ambiguity” (Cleveland & Cleveland, 2001, p. 35) and can frustrate users’ efforts 
in information retrieval. In medical reports, synonymous concepts can be represented by 
semantically-equivalent but lexically-disparate terms (i.e., multiple words or phrases that 
have the same essential sense or meaning). For example, incontinence and enuresis both 
refer to the same medical condition, and when encountered need to be treated in the same 
manner. Informal terms, such as slang (e.g., positive reactor for positive reaction to PPD) 
and abbreviations (e.g., UTI or U. T. I. for urinary tract infection) also need to be 
considered as synonyms of the more formal terms. Determining all possible alternate 
terms for a concept can be challenging. 
 22 
The presence of synonymous terms in medical reports masks the semantic similarity 
between these terms (Cimino et al., 1994) and can hinder applications that rely on 
morphologic tools in order to make decisions about documents, e.g., can lead to errors in 
classification, question answering, and information retrieval. Bringing synonyms together 
under a standardized preferred term can obviate the lexical disparity between 
semantically-equivalent terms and help reduce the errors caused by lexical disparity. 
A common method of resolving lexical disparity in free text is the use of a 
controlled vocabulary (Brennan & Aronson, 2003). A controlled vocabulary is a 
predefined list of authorized terms, each of which describe a single concept (or when used 
in combination describe a more specific single concept), which reduces or, hopefully, 
eliminates the ambiguity of those concepts (National Information Standards Organization 
(U.S.) & American National Standards Institute, 2006). In a controlled vocabulary, one of 
the lexically-disparate but semantically-equivalent terms is designated as the preferred 
term. The preferred term gathers lexically-disparate but semantically-equivalent terms 
under one term and provides a consistent means of referring to a given concept. 
The literature is replete with examples of problems related to lexical disparity. 
While investigating the use of free text for syndromic surveillance, Shapiro (2004) 
explores issues relating to word variation in chief-complaint data. He reports challenges 
caused by lexical disparity. He observes (page 95) that “a single symptom can be 
described in multiple ways by using synonyms and paraphrases.” Similarly, Aronow, 
Fangfang, and Croft (1999) report issues of unpredictable data quality, including 
problems with inconsistent use of terminology, when attempting to classify radiological 
reports. 
 23 
The extensive use of controlled vocabularies has been reported in the medical 
informatics literature. Cimino et al. (1994) note that representing concepts through a 
controlled vocabulary is a “fundamental requirement” in medical informatics. Controlled 
vocabularies can either be hand-built for a specific task or employ standardized resources. 
Goldstein et al. (2007) develop their own controlled vocabulary. They gather 
semantically-equivalent disease names and descriptions under hand-selected preferred 
terms. They substitute instances of semantically-equivalent disease names and 
descriptions with their preferred terms. They show that this substitution improves the true 
positives in ICD-9-CM coding of narratives of radiological reports by a hand-crafted rule-
based system. The UMLS Metathesaurus (Aronson, 2001; Nadkarni, Chen, & Brandt, 
2001) is often used as a controlled vocabulary for medical reports. MetaMap (Aronson, 
2001) uses shallow parsing to identify phrases and maps them to concepts from the 
UMLS Metathesaurus. 
The application of controlled vocabularies to clinical records can aid many tasks. 
Delbecque et al. (2005) show promising results when mapping noun phrases to UMLS 
semantic types in a question-answering task. Brennan and Aronson (2003) successfully 
apply MetaMap to the task of identifying medical concepts in free text messages written 
by lay (non-medical) people. They use MetaMap to map the text of patient e-mails to 
UMLS concepts. They focus their investigation on the Nursing vocabularies within the 
UMLS Metathesaurus. They compare the ratio of the number of concepts identified to the 
number of phrases in each e-mail for each of the vocabularies. They find the best results 
when using a combination of UMLS Metathesaurus vocabularies. 
 24 
Cases have also been reported where controlled vocabularies do not improve system 
performance. Ruch (2006) combines a pattern matcher and a vector space retrieval engine 
to classify MEDLINE abstracts. Ruch found only modest (+0.2%) improvement in his 
system when using Medical Subject Headings (MeSH) for vocabulary control. In a review 
of the state of the art of term identification, Krauthammer and Nenadic (2004, p. 524) 
found that “[r]elying exclusively on existing controlled vocabularies to identify 
terminology in text suffers from both low recall and low precision, as such resources are 
insufficient for automatic terminology mining.” Passos and Wainer (2009) apply 
WordNet (Miller et al., 1990) as a controlled vocabulary to a document clustering task. 
They hypothesize that WordNet as a controlled vocabulary would mitigate issues of 
synonymy. They find that measures of word similarity computed using the relationship 
between words on WordNet do not improve document clustering on their data. 
2.3. Automated Classification 
The task of automated classification entails the training, or the creation, of a 
classifier that assigns class labels to instances in the data. Automated classification forms 
the basis for systems that can be used for diverse purposes including spam filtering 
(Blanzieri & Bryl, 2008), authorship attribution (Diederich, Kindermann, Leopold, & 
Paass, 2003), web page classification (Blum & Mitchell, 1998), and the assignment of 
medical billing codes to patient records (Farkas & Szarvas, 2008). Systems have been 
based on hand-crafted rules, on machine learning algorithms, and on a combination of 
both approaches (Guzella & Caminhas, 2009; Takahashi, Takamura, & Okumura, 2005). 
Hand-crafted rules and machine learning algorithms each have their own strengths and 
 25 
weaknesses. Systems based on hand-crafted rules benefit from expert knowledge of the 
domain but require considerable effort to retrain. However, while machine learning 
systems can usually be retrained with minimal effort, they may not perform as well as 
hand-crafted rule-based systems (Ben-David & Frank, 2009). 
Comparing automated classification with manual classification performed by 
humans we see that manual classification is typically based upon Aristotle’s hierarchical 
theory of categories that brings items with common characteristics together into the same 
grouping (Ludwig Wittgenstein, as cited by Lakoff, 1987; Taylor, 2004). This approach 
has sufficed when dealing with a relatively small number of items that could be placed 
into relatively broad and well-defined categories (e.g., books into U.S. Library of 
Congress Subject Headings). On the one hand, manual classification can be labor 
intensive, costly (Medelyan & Witten, 2008; Takahashi et al., 2005), and can suffer from 
issues of consistency in class assignments (Leininger, 2000; Olson & Wolfram, 2008). 
Automated classification, on the other hand, can reduce the labor and labor related costs 
associated with manual classification, while at the same time can provide consistent class 
assignments.  
2.3.1. Hand-Crafted Rules 
The use of expert knowledge to manually develop automated classification systems 
appears to be a fruitful approach to classifying medical free text. Wilcox and Hripcsak 
(2003, p. 330) report that “[f]or medical text report classification, expert knowledge 
acquisition is more significant to performance and more cost-effective to obtain than 
knowledge discovery. Building classifiers should therefore focus more on acquiring 
 26 
knowledge from experts than trying to learn this knowledge inductively.” In the same 
article, Wilcox and Hripcsak (2003, p. 336) report having also considered machine 
learning rule-based (decision trees and rule induction), instance-based (nearest neighbor), 
and probabilistic (naïve Bayes) algorithms; they conclude that “no inductive learning 
performed as well as physicians or expert-written rules.” 
Goldstein et al. (2007) compare three systems for automatically predicting the 
ICD-9-CM codes of radiological reports (which is expanded upon in Chapter 4) and 
report that their hand-crafted rule-based system outperforms the algorithmically more 
complex BoosTexter, a machine learning algorithm. They also report that the rule-based 
system outperforms a bag-of-words term frequency - inverse document frequency vector 
space model based upon Apache Lucene. A rule-based approach can also enhance 
machine learning approaches; Mendonca et al. (2005) successfully add rules to the 
MedLee system to find information about neonatal pneumonia.  
The primary deficiency of a hand-crafted rule-based system is its static nature 
(Hughes, Gose, & Roseman, 1990). Retraining a hand-crafted rule-based system is a 
manual, and potentially costly, process. As new information arises that requires an update 
to a natural language processing system, extensive human involvement (e.g., analysis and 
computer programming) is necessary in order to accommodate the new information (Ben-
David & Frank, 2009). Even when the effort is made to maintain such systems, it is not 
without long-term problems. Clancey (1983) reports on the MYCIN program and finds 
that updating the rule set is a difficult task for anyone other than the original rule authors. 
Studer et al. (1998), in a review of the field of Knowledge Engineering, observe that rule-
based systems become hard to maintain over long periods of time. 
 27 
2.3.2. Supervised Machine Learning 
Machine learning (ML) approaches to NLP are often preferred to hand-crafted rules 
for several reasons. First, while hand-crafted rules may be based on intuition and 
experience (Paek & Pieraccini, 2008), ML approaches benefit from being able to detect 
subtle interactions between features that are important to classifying data, but may not be 
discernable to humans who build hand-crafted rules. These patterns are useful in 
understanding classification only if the selected attributes and patterns are meaningful to 
humans (Michalski & Stepp, 1983). The literature also reports that ML approaches can 
excel when presented with high dimensional feature space in the data (Apté, Weiss, & 
Grout, 1994). Lastly, due to the reasons noted above, the cost of retraining a ML classifier 
will typically be lower than the costs associated with updating hand-crafted rules (Ben-
David & Frank, 2009). 
Approaches to machine learning include k-nearest neighbor (k-NN), vector space, 
support vector machine, and Bayesian algorithms. ML approaches can be used 
individually or as an ensemble. Depending upon the algorithm, machine learning 
classifiers are trained using labeled data, non-labeled data, or a mix of labeled and 
non-labeled data. 
2.3.2.1. K-Nearest Neighbor 
K-NN (Cover & Hart, 1967; Fix & Hodges, 1952) is a lazy machine learning 
algorithm in which attributes of the training data are mapped to an n-dimensional space. 
The algorithm is considered lazy since, for each sample in the training set, it simply stores 
a feature vector that represents that sample, and delays all computation until presented 
with a test sample to classify. To assign a class label to an unclassified test sample, the 
 28 
attributes of the test sample are mapped to the same n-dimensional space as used by the 
training data. The k closest samples (nearest neighbors) from the training set are 
retrieved. The unclassified test sample is then assigned a class label based upon a vote of 
the k retrieved training samples. The number of neighbors (k) is a positive integer, and is 
usually an odd number to avoid ties in the voting. Retraining simply entails mapping 
additional annotated training data to the n-dimensional space. 
An information retrieval bag of words approach can be considered a type of k-NN, 
using words as attributes (i.e., neither syntax nor semantics are considered). This 
approach is similar to k-NN in that it employs a vector space model (Salton & Buckley, 
1988). However, it differs from k-NN in that each word (attribute) is weighted by term 
frequency - inverse document frequency (tf•idf), which looks at the similarity between 
search terms and documents. The term frequency (tf) is measured by the frequency of a 
term in a given document, normalized for document length by dividing it by the total 
number of words in the document. Document frequency (df) is the count of those 
documents in a corpus that contain any occurrences of the search term. Inverse document 
frequency (idf) is simply 1/df. When combined, tf•idf expresses in a single measure how 
well the search term describes each of the documents (tf), and how common the search 
term is in the corpus (df). tf•idf also provides a means for ranking a set of retrieved 
documents. The highest ranked retrieved documents (i.e., those with the highest tf•idf) 
can be used to inform the coding of the target document (Tzoukermann et al., 2003). 
Retraining simply requires adding newly categorized documents to the stored corpus, and 
removing changed or superseded documents from the stored corpus. 
 29 
2.3.2.2. Support Vector Machines 
Support Vector Machines (SVM) (Burges, 1998; Vapnik, 1995) are supervised 
machine learning algorithms that allow the use of linear classification techniques to be 
applied to non-linear data that are not linearly separable. This is done by mapping the data 
into a multi-dimensional space such that a hyper-plane is able to separate one class of 
data from another. Targets are then classified based upon the side of the hyper-plane upon 
which they fall in the multi-dimensional space. The support vectors refer to those training 
samples that are used to define the hyper-plane. Interestingly, an SVM trained only on the 
support vector training samples would yield the same hyper-plane as an SVM that is 
trained on all of the training samples. Retraining simply requires a remapping and 
transformation of all of the training data into a (potentially) new multi-dimensional space 
in order to find the new separating hyper-plane. 
2.3.2.3. Decision trees 
Decision trees (Quinlan, 1986, 1993) construct directed acyclic graphs (i.e., there is 
only one path between any two nodes) to predict the label of an unclassified test sample. 
The branches of the tree (i.e., nodes of the graph) represent possible values of an attribute. 
The leaves represent the class labels. The path between two nodes follows a one-way 
direction from the parent node to the child node. In machine learning, a tree is constructed 
(i.e., a classification rule is generated) by recursively selecting an attribute and splitting 
the training data into subsets based upon the values of that attribute. The recursion ends 
when all of the members of the subset have the same class label or when the subset can 
no longer be divided. The structure of the tree will be determined by the algorithm used to 
split the training data. 
 30 
Decision tree construction varies based upon the algorithm used to determine the 
best split for each subgroup of the training data. Approaches such as Quinlan’s (1986; 
1993) ID3 and C4.5 use information gain to determine the best split. Information gain 
measures the change in information entropy (Shannon & Weaver, 1949) for a given split. 
Classification and Regression Trees (CART) (Breiman, Friedman, Olshen, & Stone, 
1984) uses the Gini index to determine the best split. The Gini index is calculated by 
taking the area between a Lorenz curve and the diagonal representing the line of perfect 
equality, and dividing it by the half-square area in which the Lorenz curve lies (Weiner & 
Solbrig, 1984).  
2.3.2.4. Naïve Bayes 
Bayes Theorem (Bayes 1763, as cited by Pearl, 1988): 
)(
)()|()|(
CP
APACPCAP =  
states that the conditional (posterior) probability of A, given knowledge of C, relates to 
the conditional probability of C given A, and the marginal probabilities of A and C. This 
results in a belief function that shows that once we have observed C, we then update our 
belief about observing the outcome of A. Bayesian classifiers (Duda & Hart, 1973) use 
the conditional probability of each attribute given the class, and the marginal probabilities 
of the attribute and the class, in order to predict the class label given a set of attributes for 
an unclassified test sample. Naïve Bayes classifiers make the assumption that the state of 
each attribute is independent of the state of all of the other attributes. Even though this 
assumption is often not true in real-world data, Naïve Bayes’ results tend to be 
competitive with other machine learning approaches (John & Langley, 1995). Since 
 31 
Bayesian classifiers use correlation, and not causation, their results are independent of the 
order in which training samples are provided.  
2.3.3. Unsupervised Machine Learning 
The above ML approaches all use labeled training data, and are referred to as 
supervised learning methods. The dilemma with training on labeled data is that it can be 
tedious and expensive to develop (Zhong, 2005) and may not always be readily available 
(Zhou, 2009). Unsupervised methods of machine learning, such as SenseClusters 
(Purandare & Pedersen, 2004), do not use labeled data, but develop categories from the 
content of the training set. As such, the resultant categories will not necessarily align with 
pre-defined classification schemes (e.g., ICD-9, Library of Congress Subject Headings). 
However, these categories may lead to the discovery of previously unknown relationships 
in the data. Unsupervised approaches (e.g., context free grammars) can be useful in 
creating the scaffolding for building automated computerized classification systems. 
2.3.4. Semi-supervised Machine Learning 
Semi-supervised methods of machine learning (Blum & Mitchell, 1998; Mitchell, 
1999; Nigam, McCallum, Thrun, & Mitchell, 2000) utilize unlabeled data to improve the 
results from supervised machine learning which has been trained with a small labeled 
training set. Semi-supervised machine learning predicts class labels based upon the set of 
classes found in the labeled data. Nigam et al. (2000) examine the effect of unlabeled data 
on supervised classification and find that the addition of unlabeled data can reduce 
classification errors by up to 30%. Co-training (Blum & Mitchell, 1998) is a semi-
supervised approach that uses two conditionally independent views of the data. 
 32 
Co-training iteratively trains classifiers, alternating between each of the two views of the 
data. At each iteration, the classifier adds the newly predicted sample for which it is most 
confident of its prediction to the set of labeled data. The classifier uses the samples 
labeled from one view in an attempt to improve the learned classifier for the other view.  
Both Nigam and Ghani (2000) and J. Chan et al. (2004) explore the requirements of 
redundant sufficiency and conditional independence. They both observe that co-training 
appears to benefit from redundancy within a feature set. They both examine co-training 
using a random split of features, rather than natural split of features. J. Chan et al. (2004, 
p. 589) find that “co-training using a random split of all the features was just as 
competitive as, and often outperformed, co-training with the natural feature sets.” 
2.3.5. Combining Classifiers 
Systems that combine classifiers typically outperform individual classifiers 
(Daskalakis et al., 2008; Duin & Tax, 2000; Eom, Kim, & Zhang, 2008). The selection of 
classifiers and the method for combining classifiers is dependent upon the characteristics 
of the data to be classified. Duin and Tax (2000, p. 28) report that “there is no overall 
winning combining rule” for classifiers. Classifiers can be combined as ensembles of 
different classification algorithms working with the same training set, or as multiple 
iterations of a given algorithm each of which works on different subset of the training 
data (Kittler, Hatef, Duin, & Matas, 1998). Voting is an example of the former, boosting 
an example of the latter, and stacking blends both approaches. 
Voting considers the predictions from several individual classifiers and assigns a 
class label based upon a combination rule. Majority rule is a consensus approach; the 
 33 
class label with the most votes is assigned to the test sample. Given a probability 
distribution of each of the individual classifiers, each classifier’s prediction can be 
weighted before voting. When dealing with assigning continuous data as the label, the 
weighted votes may be combined using a median rule that would avoid errors caused by 
outliers (Kittler et al., 1998). 
Boosting (Freund, 1995; Freund & Schapire, 1996; Schapire, 1990; Schapire & 
Singer, 2000) is a supervised machine learning algorithm for boosting the performance of 
supervised learning systems. A generic boosting algorithm performs multiple iterations of 
the following steps: 1) it divides the data into subsamples and 2) it trains a weak learner, 
i.e., a classifier that performs marginally better than chance, for a set of subsamples. At 
the end of each iteration the algorithm assigns more weight to the samples that were 
misclassified in the previous iterations, increasing the probability that those samples will 
be trained on by the next weak learner. At the end of a predetermined number of 
iterations, which is usually developed empirically, the weak learners are combined into 
the final boosted classifier, which usually performs better than any of the individual weak 
learners. Unclassified test samples are then classified by applying the set of rules 
developed by the classifier to the test samples. Retraining requires that the new training 
data are processed by the boosting algorithm, which then creates a new boosted classifier. 
Adaboost (Freund & Schapire, 1996) is an early implementation of the boosting 
algorithm that is able to boost numerous base learners. BoosTexter (Schapire & Singer, 
2000) boosts decision stumps (a single level decision tree) and can perform multi-label 
classification. MultiBoosting (Webb, 2000) extends Adaboost by using a C4.5 decision 
 34 
tree as the base learner and applies wagging to improve performance. Wagging assigns 
random weights to randomly drawn bootstrap replicas of the training set. 
Stacking (Wolpert, 1992) trains a meta-classifier that accepts as its input the 
predictions of the individual classifiers. The meta-classifier attempts to minimize the 
classification errors by working out the biases of the individual classifiers on subsets of 
the training set. Stacking uses k-fold rotate-validation (with replacement) training sets for 
the individual classifiers, but uses the validation set to train the meta-classifier. Stacking 
can be applied to multiple classification algorithms or to a single classification algorithm. 
Li and Zhou (2007) extend the co-training approach by applying Breiman’s (2001) 
random forest method of combining classifiers. They examine three medical diagnosis 
data sets in detail. They find that their approach is able to improve the performance of 
their system using a small number of diagnosed samples enhanced with undiagnosed 
samples. Hai-Tao, Xiao-Nan, Fei-Teng, ChunHui, & Jian-Min (2009) also extend 
co-training and employ Breiman’s (1996) bagging approach to combining classifiers. 
They apply their approach to a computer network traffic classification task. They find that 
their combined classifier approach achieves higher accuracy than single classifier 
approaches. 
2.4. Challenges and Limitations in the Literature 
Conflicts and questions of generalizability appear in the literature. There are 
conflicting reports in the literature regarding the value of applying a controlled 
vocabulary as a means to improve system performance in the presence of lexical disparity 
(i.e., synonyms). Passos and Wainer (2009) apply WordNet as a controlled vocabulary to 
 35 
a document clustering task. They assess the usefulness of WordNet to aid document 
clustering. They find that measures of word similarity computed using the relationship 
between words on WordNet do not improve document clustering on their data. Hersh, 
Price, and Donohoe (2000) apply the UMLS Metathesaurus as their controlled vocabulary 
to expand queries for an information retrieval task. They query MEDLINE using the 
SMART retrieval system. They find that the use of the UMLS Metathesaurus for query 
expansion generally causes a decline in retrieval performance. 
Compare the above reports with de Buenaga Rodríguez et al. (2000) and Brennan 
and Aronson (2003). de Buenaga Rodríguez et al. (2000) apply WordNet to a text 
categorization task. They explore the “semantic closeness” between terms, synonyms of a 
term, and categories, and then develop weights for the terms in the training set. They find 
improved results when the training data was enhanced with WordNet over approaches 
based only on training. Brennan and Aronson (2003) apply MetaMap to the task of 
identifying medical concepts in free text messages written by lay people. They use 
MetaMap to map the text of patient e-mails to UMLS concepts. They find the 
improvement occurs when using a combination of UMLS Metathesaurus vocabularies. 
While radiological reports are a common source of medical narratives investigated 
by NLP, the reports in the literature often show that the investigations drew upon a 
limited number of categories. Friedman et al. (1994) evaluate four diseases found in 
radiological reports. They parse, normalize terms, perform synonymy, and lastly map 
terms to a controlled vocabulary. Hripcsak and Friedman (1995) and Wilcox and 
Hripcsak (2003) both appear to use the same dataset of six clinical conditions found in 
200 admission chest radiograph reports. Thomas et al. (2005) develop their system with 
 36 
ankle radiological reports (“normal,” “fracture,” or “neither normal nor fracture”) and 
then test on spine and extremities reports. Aronow et al. (1999) look at classifying 
mammograms using POS tagging and noun phrases. They perform negation detection 
using NegExpander, and also examine uncertain assertions. Taria et al. (2001) focus their 
research on thoracic radiological reports. 
Most studies of medical narratives either focus upon one type (i.e., genre) of 
medical narrative (e.g., discharge summaries or radiological reports) or look at assigning 
a limited set of clinical conditions (i.e., categories) to free texts. To date, reports have not 
shown generalizability across multiple genres and categories. Additionally, neither 
machine learning approaches nor hand-crafted rules have definitively been shown to be 
the best solution to assigning categories to medical reports. In fact, some studies (Uzuner, 
Goldstein, Luo, & Kohane, 2008) have shown that rule-based and machine learning (e.g., 
SVMs) perform similarly on specific classification tasks, implying that the skill of those 
wielding the tools for the creation of a computerized classification system may be more 
important than the actual approach taken for implementation of those systems. 
As previously described, labeled data are expensive to develop (Zhong, 2005), and 
may not always be readily available (Zhou, 2009). In addition, Chawla, Japkowicz, and 
Kotcz (2004) note issues when presented with a non-uniform distribution of samples 
among the classes (e.g., the occurrence of an intrusion on a network when compared to 
the large number of valid transactions on the network). They believe that the imbalance 
caused by the non-uniform distribution of classes is pervasive and ubiquitous, and may 
cause suboptimal classification performance. Machine learning approaches to building 
classifiers may simply ignore the less-well represented classes in order to maximize 
 37 
overall performance. Proposed solutions to this imbalance problem include data 
re-sampling (Chawla, Bowyer, Hall, & Kegelmeyer, 2002) and one-class machine 
learning approaches (Raskutti & Kowalczyk, 2004). Re-sampling can suffer from the 
removal of examples if under-sampled, and from over-fitting if the data over-sampled. 
While the one-class machine learning approaches work well for binary classification, they 
do not address multi-labeled, multi-class classification tasks. 
2.5. Summary 
This dissertation addresses some of the gaps and issues noted in the literature. 
Specifically, given the unsettled question regarding preferred terminology, this 
dissertation investigates the role of preferred terminology in the classification of two 
different types of medical reports. Given the issues related to imbalance of classes found 
in data sets, this dissertation also reports on a new method of combining classifiers when 
presented with a non-uniform distribution of samples among the classes in patient 
discharge summaries. Lastly, we apply the results of these investigations to co-training in 
order to examine the performance of the combined classifier when presented with a small 
set of labeled samples and a large set of unlabeled samples. 
 38 
Chapter 3. Medical Records and Evaluation Metrics  
3.1. Introduction 
This chapter first presents the three data sets employed and then describes the 
evaluation metrics used in the experiments. The use of multiple data sets provides the 
ability to compare results based upon varying characteristics of those data sets, and 
improves the potential to generalize results. We employ Precision, Recall, and F-measure, 
metrics commonly used to evaluate NLP systems. 
3.2. Data 
The data for this dissertation were provided by the Cincinnati Computational 
Medicine Center (CMC) and the Informatics for Integrating Biology and the Bedside 
(i2b2). In 2007, the CMC organized a challenge (Pestian et al., 2007) and released data 
consisting of de-identified and classified radiological reports. The i2b2 released two data 
sets consisting of de-identified and classified discharge summaries. The first i2b2 data set 
came from the 2006 Shared-Task and Workshop on Challenges in Natural Language 
Processing for Clinical Data: Smoking Challenge (Uzuner et al., 2008). The second i2b2 
data set came from the 2008 Shared-Task and Workshop on Challenges in Natural 
Language Processing for Clinical Data: Obesity Challenge (Informatics for Integrating 
Biology and the Bedside, 2008; Uzuner, 2008). 
While the three data sets exhibit some differing characteristics, they do contain 
similarities. The records of the CMC data set (see Figure 1) can be described as being 
well formatted and written concisely and efficiently (i.e., it contains very little extraneous 
information). The records of the i2b2 data sets (see Figure 2) can be described as being 
 39 
semi-structured and verbose. Each of the data sets came from a relatively small 
community, and from a limited geographic area. The CMC data were collected from the 
Cincinnati Children’s Hospital Medical Center’s Department of Radiology. The i2b2 data 
were collected from Partners Healthcare, a system of hospitals in eastern Massachusetts. 
In each of these data, variation in vocabulary that might arise from the use of regional 
expressions would be limited. This would be especially true for the CMC data set since it 
came from a single medical department at a single hospital. Therefore, we would expect 
the terms used to be somewhat homogenous in nature. 
3.2.1. CMC Data Set 
The CMC data set consists of a training set of 978 radiological reports and a test set 
of 976 radiological reports. The reports cover a variety of diseases, including cystic 
fibrosis, kidney stones, and pneumonia. The report narratives consist of two fields marked 
CLINICAL HISTORY and IMPRESSION (see Figure 1). The CLINICAL HISTORY 
field usually states the complaints of the patient, while the IMPRESSION field notes the 
radiologic findings. Both fields appear to have been stripped of superfluous text. 
Typically, the text of each of the narrative fields of the reports consists of at most two 
sentences. In addition, each report is labeled with standard ICD-9-CM diagnostic 
classification codes. 
The CMC radiological reports have been fully de-identified and have been hand-
labeled by three independent coding companies. Each of the ICD-9-CM codes represents 
a distinct disease or condition in the patient. Codes assigned to a given report by two or 
more coding companies, referred to as the majority codes, are used as the ground truth. 
 40 
The codes used to label a given report reflect only the definite diagnoses mentioned in 
that report (i.e., that the disease or condition is present in the patient). At least one code is 
assigned to each report. Multiple codes per report are allowed. A total of 45 different 
ICD-9-CM majority codes were found in the testing data set, with 34 of the codes 
assigned to more than one report. All codes assigned to the reports in the testing data set 
had corresponding exemplars in the training data set. 218 of the 976 (22.3%) of the 
testing data set reports were assigned multiple codes, for a total of 1205 majority code 
assignments. 
 
Figure 1 - Sample radiology report from the XML ICD-9-CM data released as part of the 2007 CMC 
challenge. 
 
3.2.2. i2b2 Data Sets 
The i2b2 Obesity data set consists of a training set of 730 discharge summaries and 
a test set of 507 discharge summaries from Partners HealthCare (see sample: Figure 2). 
The discharge summaries range in size from 133 words to more than 3000 words. The 
<doc id="99702892" type="RADIOLOGY_REPORT"> 
  <codes> 
 <code origin="CMC_MAJORITY" type="ICD-9-CM">591</code> 
 <code origin="CMC_MAJORITY" type="ICD-9-CM">599.0</code> 
 <code origin="COMPANY3" type="ICD-9-CM">591</code> 
 <code origin="COMPANY1" type="ICD-9-CM">591</code> 
 <code origin="COMPANY1" type="ICD-9-CM">599.0</code> 
 <code origin="COMPANY1" type="ICD-9-CM">780.6</code> 
 <code origin="COMPANY2" type="ICD-9-CM">591</code> 
 <code origin="COMPANY2" type="ICD-9-CM">599.0</code> 
   </codes> 
   <texts> 
 <text origin="CCHMC_RADIOLOGY" type="CLINICAL_HISTORY">Febrile UTI evaluate 
for possible reflux.</text> 
 <text origin="CCHMC_RADIOLOGY" type="IMPRESSION">Mild hydronephrosis on the left 
with suggestion of minimal decrease following partial voiding.</text> 
   </texts> 
 </doc> 
 
 41 
discharge summaries have been fully de-identified and then hand-labeled by two obesity 
experts for information on obesity and fifteen of its most frequent co-morbidities. 
Two of the obesity experts coded each of the discharge summaries, determining 
whether each of the sixteen diseases was present (marked with a Y in the data set), absent 
(marked with an N in the data set), or questionable (marked with a Q in the data set) in 
the patient according to explicit statements in the text of the discharge summary, or were 
unmentioned (marked with a U in the data set) in the narrative. In cases where the two 
obesity experts disagreed, a third physician coded the discharge summaries. Majority 
decision among the experts determined the final judgment on each of the sixteen diseases 
for each discharge summary. The summaries that did not have a final judgment for a 
disease were omitted from the training and test data for that disease (i.e., some records 
contained final judgments only for a subset of the sixteen diseases).  
The data contains a non-uniform distribution of classes across the sixteen diseases. 
The present and unmentioned classes are well-represented across the diseases. The absent 
and questionable classes are less well-represented in all diseases.  
The i2b2 Smoking data set consists of a training set of 398 discharge summaries 
and a test set of 104 discharge summaries from Partners HealthCare. These discharge 
summaries follow the same format as the discharge summaries in the i2b2 Obesity data 
set. The discharge summaries have been fully de-identified and then annotated by two 
pulmonologists for the smoking status of the patient based upon the narrative of the 
patient’s discharge summary. 
 42 
 
Figure 2 - Sample discharge summary from i2b2 Shared-Task and Workshop on Challenges in 
Natural Language Processing for Clinical Data: Obesity Challenge. 
<doc id="330"> 
<text> 
435791816 | CULHS | 80771325 | | 428219 | 1/12/1994 12:00:00 AM | Discharge Summary | 
Unsigned | DIS | Admission Date: 1/12/1994 Report Status: Unsigned 
 
Discharge Date: 9/15/1994 
PRINCIPAL DIAGNOSIS: CORONARY ARTERY DISEASE. 
HISTORY OF PRESENT ILLNESS: Mr. Weddel is a 52-year-old man status 
post coronary artery bypass graft x1 , 
with saphenous vein graft to the left anterior descending in 1976. 
He presents with a 5-6 week history of unstable angina. He had an 
echocardiogram which revealed recurrent coronary artery disease. 
The patient was referred to Dr. Finstad for reoperative coronary 
artery bypass graft. 
PAST MEDICAL HISTORY: The patient has a history of coronary artery 
disease and peripheral vascular disease. 
PAST SURGICAL HISTORY: The patient is status post a radical 
prostatectomy complicated by osteitis pubis 
and a urethral colonic fistula. 
CURRENT MEDICATIONS: Aspirin , Lopressor , Procardia XL , intravenous 
heparin and intravenous nitroglycerin. 
ALLERGIES: No known drug allergies. 
LABORATORY DATA: BUN and creatinine 16/1.2 , white count 5500 , 
hematocrit 46. 
HOSPITAL COURSE: The patient was admitted to the Cardiology 
Service on September , 1994 , where he was 
stabilized and finally taken to the Operating Room on July , 
1994 , at which time he underwent a reoperative coronary artery 
bypass graft x3 , with a left interior mammary artery to the left 
anterior descending. The patient tolerated the procedure well and 
was hemodynamically stable in the unit. He was weaned overnight 
and extubated on postoperative day 1. He was started on aspirin 
and Lopressor , weaned off his O2 and started on diuretics. He was 
transferred to the unit on postoperative day 1. 
Once on the unit the patient was gradually advanced to a regular 
diet and weaned off his O2. He was diuresed down to his 
preoperative weight. He had a routine postoperative course. 
DISPOSITION: Home on August , 1994. 
CONDITION ON DISCHARGE: Good. 
DISCHARGE MEDICATIONS: Aspirin one tablet p.o. q. day; Lopressor 
25 mg p.o. t.i.d.; and Percocet p.r.n. 
FOLLOWUP: The patient will follow up with Dr. Mcgoff and will 
contact his office for an appointment after discharge. 
Dictated By: CLINTON H. BARRET , M.D. TM2 
Attending: DANIAL C. DELL , M.D. SY3  RZ941/4768 
Batch: 8363 Index No. K1QM0GBOY D: 1/13/94 
T: 6/15/94</text> 
</doc> 
 43 
Table 1- i2b2 Obesity Data. Y=Present, N=Absent, Q=Questionable, and U=Unmentioned Number of 
samples in each class in the training and test sets. 
 Number of Samples in the 
Training Data 
Number of Samples in the 
Test Data 
Disease Y N Q U Total Y N Q U Total 
Asthma 93 3 2 630 728 68 2 2 432 504 
Atherosclerotic CV disease 399 23 7 292 721 277 22 2 196 497 
Heart failure 310 11 0 399 720 205 11 0 280 496 
Depression 104 0 0 624 728 72 0 0 434 506 
Diabetes  485 15 7 219 726 338 12 3 150 503 
Gallstones/Cholecystectomy 109 4 1 615 729 87 2 0 418 507 
GERD 118 1 5 599 723 69 1 1 433 504 
Gout 90 0 4 634 728 52 0 0 453 505 
Hypercholesterolemia 304 13 1 408 726 213 6 4 279 502 
Hypertension 537 12 0 180 729 374 6 0 121 501 
Hypertriglyceridemia 18 0 0 711 729 10 0 0 497 507 
Osteoarthritis 115 0 0 613 728 86 0 0 416 502 
Obesity 298 4 4 424 730 198 3 3 289 493 
Obstructive sleep apnea 105 1 8 614 728 69 0 2 432 503 
Peripheral vascular disease 102 0 0 627 729 64 0 0 443 507 
Venous insufficiency 21 0 0 707 728 10 0 0 497 507 
Total 3208 87 39 8296 11630 2192 65 17 5770 8044 
 
 
Two pulmonologists annotated each of the discharge summaries, determining 
whether the tobacco smoking status of the patient is Past Smoker (i.e., a patient who was 
a smoker but has not smoked for at least one year), Current Smoker (i.e., a patient who 
has smoked sometime in the last year, or who was noted as being a “current smoker”), 
Smoker (i.e., a patient who is either a Past or Current Smoker, but the temporality cannot 
be determined based upon the narrative of the discharge summary), Non-Smoker (i.e., a 
patient who has never smoked), or Unknown (i.e., that there is no mention in the 
discharge summary regarding smoking by the patient). In cases where the two 
pulmonologists disagreed, two other pulmonologists annotated the discharge summaries. 
Majority decision among the pulmonologists determined the final judgment on the 
 44 
smoking status of the patient. Discharge summaries that failed to have a majority vote by 
the pulmonologists were omitted from the data set. 
As can be seen in Table 2, there is an imbalance across the classes caused by the 
non-uniform distribution of the smoking statuses. The Unknown class dominates, 
representing more than 60% of the records. The Smoker class is the least well represented 
class, comprising barely 3% of the records. 
Table 2 - Smoking Status Training and Test Data. Number of samples per class 
 Current 
Smoker 
Non-
Smoker 
Past 
Smoker Smoker Unknown Total 
Training 35 66 36 9 252 398 
Test 11 16 11 3 63 104 
3.2.3. Institutional Review Board 
One of the primary concerns when using patient medical reports for research is the 
risk of psychosocial harm (e.g., embarrassment and emotional distress) to the patients, 
which could arise due to a breach of confidentiality. This research used existing data sets 
that had been previously released to the research community. The developers of the CMC 
data set and the i2b2 data sets have fully de-identified and, using surrogates, artificially 
re-identified all of the records contained in their data sets. Both developers manually 
reviewed their data to insure that the records did not contain any Protected Health 
Information. 
This research did not involve any direct interaction with the patients. It was unlikely 
that the use of this data for this dissertation would increase the chance of causing 
psychosocial harm to the patients whose records were used in the development of the data 
sets. The institutional review board of SUNY Albany approved as exempt the use of the 
CMC data set, the i2b2 Obesity data set, and the i2b2 smoking data.  
 45 
3.3. Evaluation Metrics 
This dissertation evaluates and compares each of the systems to a baseline. For the 
task of classification, this dissertation presents results in terms of individual class, 
micro-averaged, and macro-averaged precision (π), recall (ρ), and F-measure (Özgür, 
Özgür, & Güngör, 2005; van Rijsbergen, 1979; Yang & Liu, 1999), as shown in 
equations 1-3. These performance metrics are commonly used in natural language 
processing tasks. These metrics are computed using the counts for true positives (TPi) 
i.e., the number of records correctly assigned to class i (out of M classes), false positives 
(FPi) i.e., the number of records erroneously assigned to class i, and false negatives (FNi) 
i.e., the number of records belonging to class i but were not assigned to class i. πi (Eq. 1a) 
is the sum of the number of reports correctly assigned to class i divided by the sum of the 
total number of reports assigned to class i. ρi (Eq. 1b) is the number of reports correctly 
assigned to class i divided by the total number of reports that should have been assigned 
to that class. The Fi-measure (Eq. 1c) is the harmonic mean of πi and ρi
Micro-averaged π, ρ, and F-measure (Eq. 2a-c) are computed over all samples in the 
data. Micro-averaging gives equal weight to each sample in the data. This is in contrast to 
macro-averaged π, ρ, and F-measure (Eq. 3a-c), which are the arithmetic mean of the π, ρ, 
and F-measures of all classes. By dividing the sum of the metric over all classes by the 
number of classes (M), equal weight is given to each class regardless of the class size. 
When observed together, micro- and macro-averaged π, ρ, and F-measures give a more 
complete picture of the strengths and weaknesses of systems under evaluation. 
. By varying ß, 
F-measure can favor either precision or recall. In this dissertation, we give equal weight 
to precision and recall by setting ß = 1. 
 46 
   
ii
i
i FPTP
TP
+
=π    (Eq. 1a – Precision) 
 
ii
i
i FNTP
TP
+
=ρ   (Eq. 1b – Recall) 
 
ii
ii
iF ρπβ
ρπβ
+
+
=
)(
)1(
2
2
  (Eq. 1c – F-measure) 
 
   
)(
1
1
∑
∑
=
=
+
= M
i
ii
M
i
i
micro
FPTP
TP
π   (Eq. 2a – Micro-Precision) 
 
∑
∑
=
=
+
= M
i
ii
M
i
i
micro
FNTP
TP
1
1
)(
ρ  (Eq. 2b – Micro-Recall) 
 
micromicro
micromicro
microF ρπβ
ρπβ
+
+
=
)(
)1(
1 2
2
  (Eq. 2c – Micro-F-Measure) 
 
   
M
M
i
i
macro
∑
== 1
π
π    (Eq. 3a – Macro-Precision) 
 
M
M
i
i
macro
∑
== 1
ρ
ρ   (Eq. 3b – Macro-Recall) 
 
M
F
F
M
i
i
macro
∑
== 1
1
1   (Eq. 3c – Macro-F-Measure) 
 
 47 
3.4. Summary 
The CMC and i2b2 data set exhibit differing characteristics. The content of the 
radiological reports of the CMC data set contains very little extraneous information. This 
can be compared to the discharge summaries of the i2b2 data sets that are semi-structured 
and more verbose. The use of multiple data sets provides the ability to compare results 
based upon varying characteristics and improves the potential to generalize results.  
This dissertation employs Precision, Recall, and F-measure, metrics commonly used 
to evaluate NLP systems. Micro-averaging gives equal weight to each sample in the data. 
Macro-averaging gives equal weight to each class regardless of the class size. When 
presented together, micro- and macro-averaged metrics help to provide a complete picture 
of the systems under evaluation. 
 48 
Chapter 4. Examining the Role of Preferred Terminology in the Document 
Level Classification of Medical Reports1
4.1. Introduction 
 
From grocery purchases to medical reports to social tagging, the rapid development 
of computer technologies has resulted in an explosion of stored electronic textual data 
(Sweeney, 2001). Unfortunately, these data are often in the form of unstructured free text 
(Friedman et al., 1994; Taira et al., 2001) and need to be processed in order to be 
converted to useful information (Ackoff, 1989). One method of processing tackles the 
issue that free text often contains semantically-equivalent but lexically-disparate terms for 
a given concept (i.e., synonyms). Processing free text by using a controlled vocabulary is 
a generally accepted means for resolving problems caused by semantically-equivalent but 
lexically-disparate terms. However, there are conflicting reports in the literature regarding 
the value of applying a controlled vocabulary as a means to improve system performance 
(e.g., compare Passos and Wainer (2009) with de Buenaga Rodríguez, et al. (2000)) in the 
presence of lexical disparity.  
This chapter begins with a review the literature related to controlled vocabulary. It 
then details the methods employed for the creation of our hand crafted rule-based and our 
machine learning classifiers. It then presents and discusses the results of applying these 
systems to two data sets. Lastly, we draw conclusions based upon those results. 
                                                 
1 Portions of this chapter were previously presented in: 
Goldstein, I., Arzumtsyan, A., & Uzuner, Ö. (2007). Three approaches to automatic assignment of 
ICD-9-CM codes to radiology reports. Proceedings of the AMIA symposium. 279–283. 
Goldstein, I., & Uzuner, Ö. (2009). Role of preferred terminology in the classification of medical reports. 
Proceedings of the International Symposium on Computer and Information Sciences - ISCIS 2009. 
260-264.  
Goldstein, I., & Uzuner, Ö. (2010). Does Negation Really Matter? Proceedings of the Workshop on 
Negation and Speculation in Natural Language Processing. 23-27. 
 49 
We examined the issue of lexical disparity as it applied to the classification of 
medical reports. A substantial amount of patient medical data is stored as free text in the 
narratives of patient reports (Spat et al., 2008), including radiology reports, admission 
notes, and discharge summaries (Wilcox & Hripcsak, 1999). The narratives of these 
reports contain a variety of useful information that can support syndromic surveillance 
(Shapiro, 2004), billing (Puckett, 1986), decision support (Fiszman et al., 2000), and 
problem list generation (Sibanda, He, Szolovits, & Uzuner, 2006). In this chapter, we 
report on the role of preferred terminology as a means of post-hoc coordination in 
automatic classification of patient medical data. We applied natural language processing 
tools to the free text of the medical reports; we built and evaluated both a hand-crafted 
rule-based system and two machine learning systems. 
We attempted to improve system performance in the automated document level 
classification of medical reports through the use of preferred terminology. We 
experimented with medical reports that came from two different corpora: one containing 
radiology reports and the other containing discharge summaries. We gathered 
semantically-equivalent but lexically-disparate medical terms used in these reports under 
preferred terms for multi-label classification of the medical reports. We showed that 
enriching these reports with preferred terms did not consistently improve document level 
classification on these corpora. However, the performance of preferred terminology can 
be enhanced when employed with asserting disease noun phrases. When taken in 
conjunction with the results from the literature, our findings indicate that the contribution 
of preferred terminology to a natural language processing task depends on the nature of 
the task, the data, and the approach applied. 
 50 
4.2. Controlled Vocabulary 
Cleveland and Cleveland noted that “the complexity, variability, and richness of 
natural language, rather ironically, leads to ambiguity” (Cleveland & Cleveland, 2001). In 
medical reports, a concept may be represented by several semantically-equivalent but 
lexically-disparate terms, e.g., incontinence may also be referred to as enuresis. The 
lexical disparity of semantically-equivalent terms obscures the semantic similarity 
between these terms (Cimino et al., 1994) and can hinder applications that rely on word 
counts in order to make decisions about documents, e.g., resulting in errors in 
classification. We can address this problem through post-hoc coordination by gathering 
semantically-equivalent but lexically-disparate terms under a standardized preferred term 
and help to reduce the errors that stem from lexical disparity.  
A common method of eliminating lexical disparity is through the use of a controlled 
vocabulary. A controlled vocabulary is a predefined list of authorized terms, each of 
which describes a single concept. In a controlled vocabulary one of the semantically-
equivalent but lexically-disparate terms is designated as the preferred term. The use of a 
preferred term gathers semantically-equivalent but lexically-disparate terms under one 
term, thereby providing a consistent means of referring to a given concept. 
The extensive use of controlled vocabularies has been reported in the medical 
informatics literature. While investigating the use of free text for syndromic surveillance, 
Shapiro (2004) explored issues relating to word variation in chief-complaint data. He 
observed that “a single symptom can be described in multiple ways by using synonyms 
and paraphrases.” Similarly, Aronow et al. (1999) classified radiology reports. They 
reported issues related to inconsistent use of terminology.  
 51 
Controlled vocabularies can either be hand-built for a specific task or employ 
standardized resources. Childs et al. (2009) developed their own controlled vocabulary. 
They data-mined online ICD-9-CM documentation to acquire medical terms. They 
grouped together synonymous text that pertained to medical evidence, patient history, 
signs, and symptoms. The Unified Medical Language System (UMLS) Metathesaurus 
(Aronson, 2001; Nadkarni et al., 2001) is often used as a standard controlled vocabulary 
for medical reports. Delbecque et al. (2005) showed promising results when mapping 
noun phrases to UMLS semantic types in a question-answering task. MetaMap (Aronson, 
2001) uses shallow parsing to identify phrases and maps them to concepts from the 
UMLS Metathesaurus.  
A review of the literature also revealed reports where controlled vocabularies did 
not improve system performance. Ruch (2006) combined a pattern matcher and a vector 
space retrieval engine to classify MEDLINE abstracts and found only modest 
improvement when using the U.S. National Library of Medicine Medical Subject 
Headings for vocabulary control. While investigating the uses of a controlled vocabulary 
for biomedical literature retrieval, IJzereef et al. (2005) noted that the use of controlled 
vocabulary was detrimental for the majority of the queries. This is consistent with 
Hersh et al. (2000), who found that the use of the UMLS Metathesaurus for query 
expansion generally caused a decline in retrieval performance. Scott and Matwin (1998) 
applied WordNet to a classification task, and concluded that their approach did not work 
well in the presence of text that was “written concisely and efficiently” or was written by 
a limited number of authors using consistent terminologies. 
 52 
Prompted by the conflicting reports in the literature, we explored the role of 
preferred terminology in the classification of medical reports. We mapped noun phrases 
in our medical reports to a controlled vocabulary. We applied a multi-label hand-crafted 
rule-based classifier and two multi-label machine learning classifiers, one based upon 
Lucene and the other based upon BoosTexter, to the text of the medical reports. The 
classifiers predicted the presence of a disease or condition in the patient based upon the 
free text of the medical reports. 
4.3. Data 
The data for this study were provided by the 2007 Computational Medicine Center 
(CMC) challenge (Pestian et al., 2007) (see section  3.2.1) and the i2b2 Shared-Task and 
Workshop on Challenges in Natural Language Processing for Clinical Data: Obesity 
Challenge (Informatics for Integrating Biology and the Bedside, 2008; Uzuner, 2008) (see 
section  3.2.2). 
The CMC data consisted of a training set of 978 radiology reports and a test set of 
976 radiology reports. The reports covered a variety of diseases, including hydronephrosis 
(swelling of the kidney), urinary tract infection, and pneumonia. The report narratives 
consisted of two fields marked “Clinical History” and “Impression.” The “Clinical 
History” field usually stated the complaints of the patient, while the “Impression” field 
noted the radiologic findings. Typically, each field of the report consisted of at most two 
sentences. 
The CMC radiology reports had been fully de-identified and hand-labeled with 45 
different ICD-9-CM codes by three independent coding companies. The codes used to 
 53 
label a given report reflect only the definite diagnoses mentioned in that report. At least 
one code was assigned to each report; multiple codes per report were allowed. Our task 
on this corpus was to label each report with one or more of the ICD-9-CM codes, a 
document level multi-label classification task. 
The i2b2 data consisted of a training set of 730 discharge summaries and a test set 
of 507 discharge summaries from Partners HealthCare. The discharge summaries ranged 
in size from 133 words to more than 3000 words. The discharge summaries had been 
de-identified and then hand-labeled by two obesity experts for information on obesity and 
fifteen of its most frequent co-morbidities. Multiple codes per discharge summary were 
allowed. Our task on this corpus was to label each discharge summary with those diseases 
that were present in the patient, a second document level multi-label classification task. 
On both the CMC and the i2b2 corpora, we focused on identifying the definite 
diagnoses for the patient. We developed and trained the classifiers on the training sets and 
evaluated these classifiers on the test sets. We reported results only on the test sets.  
4.4. Methods 
This study explored the impact of preferred terminology on three approaches to 
automatic classification of medical reports, a hand-crafted rule-based system and two 
machine learning approaches. Each of the approaches has strengths and weaknesses. 
Expert knowledge used to develop hand-crafted automated classification systems appears 
to be a fruitful approach to classifying medical free text. The primary deficiency of a 
hand-crafted rule-based system is its static nature (Hughes et al., 1990). Retraining a 
hand-crafted rule-based system is a manual, and potentially costly, process. 
 54 
On the other hand, Machine learning (ML) algorithms allow the development of 
automatic classification systems that are often able to detect subtle interactions between 
attributes. This study employed a diverse pair of ML algorithms. These algorithms vary in 
their approach to learning and classifying. For example, the Lucene classifiers treat each 
word independently while the BoosTexter classifiers have the ability to look at strings of 
words. However, ML approaches to classification do not inherently encode domain 
expertise.  
We experimented with gathering lexically-disparate but semantically-equivalent 
medical terms under a preferred term for multi-label classification of medical reports. The 
systems used preferred terms to perform post-hoc coordination of semantically-equivalent 
terms. The systems take advantage of the fact that preferred terms can represent a group 
of semantically-equivalent but lexically-disparate non-preferred terms without a loss in 
meaning. 
The systems expanded the text of the medical reports with preferred terminology for 
each of the non-preferred terms in the report. The systems pre-processed the text of the 
reports by removing punctuation, converting to lower case, and stemming. The systems 
applied labels to the expanded medical reports using a hand-crafted rule-based classifier 
and two machine learning classifiers. 
4.4.1. Hand-Crafted Rule-Based System 
The exploration of the role of preferred terminology in the classification of medical 
reports began by examining the impact that resolving synonymy played in a hand-crafted 
rule-based system. For this task the system assigned one or more ICD-9-CM codes to the 
 55 
CMC radiological reports, a multi-label classification task. The hand-crafted rule-based 
system implemented a set of simple rules, consisting of four subsets that were developed 
on the CMC training set. This system is referred to as the Full Rule-Based (FRB) system 
and is shown in Figure 3. 
The first three rule subsets constitute the semantic components, i.e., they captured 
shallow semantics. The first subset of rules marked uncertainty with respect to the 
diagnosis. Following the practice that discourages over-coding of reports, i.e., assignment 
of unnecessary ICD-9-CM codes to a report (Pestian et al., 2007), the system treated 
uncertainty phrases in the Impression field as negations. For example, the phrase “may 
represent atelectasis” implies that atelectasis (the full or partial collapse of a lung) could 
not be definitively diagnosed; therefore, the system should not code this report as 518.0 
(the ICD-9-CM code for atelectasis). These uncertainty phrases include most consistent 
with, likely, and even probable. The second subset of rules was based loosely upon 
NegEx’s pre-UMLS negation phrases (Chapman et al., 2001) and captured the explicitly 
negated information in the reports. The third subset of rules extended the terminology 
used in the system to include synonyms of disease names. We gathered semantically-
equivalent disease names and descriptions under hand-selected preferred terms for use as 
a controlled vocabulary. These synonyms were developed by manually examining the 
definitions of ICD-9-CM codes, the alternate terminology used for describing the codes, 
and the index entries containing the codes from www.icd9data.com. The sub-system 
substituted instances of semantically-equivalent disease names and descriptions with their 
preferred terms. Identifying synonyms and replacing them with preferred terms allowed 
the sub-system, for example, to extend the scope of 788.30 (the ICD-9-CM code for loss 
 56 
of bladder control) from incontinence to also include alternatives such as enuresis and 
wetting. 
The final subset of rules identified lexical elements based upon useful unigrams and 
n-grams, and expanded the resulting set of rules with additional lexical elements that 
helped identify ICD-9-CM codes that were not otherwise addressed. The system omitted 
17 ICD-9-CM codes from this process. 16 of these codes were assigned to six or fewer 
reports in the training set and generating rules for them would have potentially over fit the 
rules to the training data. No unique rule could be created for the remaining one 
ICD-9-CM code (V13.02).  
The FRB system applied, in sequence, uncertainty, negation, synonymy, and lexical 
rules. This sequential application prevented conflicts between rule sets. This full rule-
based ICD-9-CM coder was submitted to the 2007 Computational Medicine Center 
challenge and placed second in the challenge. 
This study compared the performance of the FRB system and four variations of the 
FRB system. The first variation was created using only lexical elements of the FRB, and 
is referred to as the base rule-based (BRB) system. The remaining three variations took 
the BRB and added one of the three semantic components. This resulted in a BRB + 
Uncertainty system, a BRB + Negation system, and a BRB + Synonymy system. This 
study compared all of the systems to the baseline of assigning the most frequent 
ICD-9-CM code (i.e., 786.2) to all of the reports. 
 57 
 
Figure 3 - Hand Crafted Rule-Based Coder - Developed from an examination of the test data set. 
Synonyms developed from ICD-9-CM code definitions and expanded with recurrent terms indentified in the 
text. Employs Negex’s pre-UMLS uncertainty and negation phrases. Given the task of assigning codes only 
for definitive diagnosis, uncertainty phrases are treated as negation. After processing with synonymy, 
uncertainty, and negation a series of simple rules are applied. 
 
Assign Code(s) 
Lexical Rules 
Negation 
Uncertainty 
Test Data 
Synonymy 
 58 
4.4.2. Lucene Classifier 
This study used the k-Nearest Neighbor (k-NN) (Cover & Hart, 1967; Fix & 
Hodges, 1952) process to build Lucene-based classifiers. k-NN is a lazy machine learning 
algorithm. Classification is based on the nearest training samples, as determined by the 
feature vectors. This approach assumes that similar training samples will cluster together 
in the feature vector space. The nearest training samples are considered to be those that 
are most similar to the data sample. Data samples are assigned to the class most common 
among its k nearest neighbors (i.e., nearest training samples). 
We developed the Lucene-based classifier using Apache Lucene (Gospodnetić & 
Hatcher, 2005), an open source search engine library. Lucene utilizes a vector space 
model (Salton & Buckley, 1988) weighted by term frequency–inverse document 
frequency (tf•idf) to study the similarity between search terms and documents. In this 
feature vector space, each dimension of the vector represents a word and each vector in 
the space represents a document. Similarity between two documents was determined by 
the angle between their vectors; the nearer the vectors (i.e., the smaller angle between 
them) the more similar the documents they represent. However, tf•idf is limited to string 
similarity and does not take into account the semantic similarity of semantically-
equivalent but lexically-disparate terms. 
System development began by removing the word no from the Lucene stop-word 
list and then filtering the rest of the stop-word list from the text of the reports. Training 
was performed by indexing the remaining text of the reports in the training set. For each 
report in the test set, the system used its narrative text to query the generated index. The 
Lucene classifier determined which training reports were similar to the target report based 
 59 
upon their narratives. As shown in Figure 4, the Lucene classifier performed the 
following steps:  
1. For each target report the system retrieved the three nearest (i.e., most 
similar) training reports. 
2. The system assigned any codes that were used by the majority of the 
retrieved reports. 
3. In cases where the retrieved training reports did not provide a majority code, 
the system retrieved the fourth nearest training report. 
4. If a majority code was still not found, the system assigned a NULL code to 
the target report. 
The initial runs of the Lucene Classifier were on the stemmed lowercase text of the 
medical reports. These runs are referred to as the Base Lucene Classifier runs. The 
Lucene Classifiers were then trained and tested on the text expanded with preferred 
terms. These runs are referred to as the Preferred Term Lucene Classifier runs.  
 
 
 60 
 
Figure 4 - Lucene Based Coder – Training data are indexed (tf•idf) using the Apache Lucene libraries. 
Test samples are treated as queries to the index. The top three ranked indexed (training) documents are 
retrieved. Any codes used by a majority of the retrieved records are assigned to the test sample. If there is 
no code in common among the top three retrieved documents, the fourth highest ranked document is 
retrieved. Any codes in common between the four retrieved records are assigned to the sample. If no codes 
are in common, a NULL code is assigned to the test sample. 
4.4.3. BoosTexter Classifiers 
A generic boosting algorithm, as depicted in Figure 5, performs multiple iterations 
of the following steps:  
1. It divides the data into subsamples.  
2. It trains a weak learner.  
Each weak learner is able to predict samples based upon the attributes in the data and 
associates a weight with its prediction. The weight indicates the strength of the 
confidence of the prediction made by the weak learner. Among these weak learners, 
boosting retained those that perform even marginally better than chance. At the end of a 
Training Data 
Top 3 Training 
Records 
Majority Code(s) 
Fourth Training 
Record 
Test Data 
Index 
 61 
predetermined number of iterations the weak learners were combined into the final 
boosted classifier, which usually performs better than any of the individual weak learners. 
The BoosTexter (Schapire & Singer, 2000) based classifier assigned labels to the 
narrative text of medical reports. BoosTexter has the ability to classify text using 
individual words (unigrams), strings of consecutive words (n-grams), or strings of non-
consecutive words (s-grams), without considering semantics. 
System development began by cross-validating BoosTexter (tenfold) on the CMC 
training set. Cross-validating established the optimal parameters for the CMC training set 
to be 1100 iterations, with n-grams of up to four words. An examination of the iteration 
and n-gram parameters on the i2b2 Obesity training set found the optimal parameters to 
be similar to the CMC training set optimal parameters. For consistency, the BoosTexter 
system used the parameters of 1100 iterations and n-grams of up to four words with both 
the CMC and i2b2 Obesity data sets. In addition to applying n-grams, the runs also 
applied unigrams to BoosTexter. The runs employed unigrams in order to provide 
BoosTexter classifier results that were comparable to those of the Lucene classifiers.  
The initial runs of the BoosTexter Classifier were on the stemmed lowercase text of 
the medical reports. These runs employed unigrams and n-grams, and are referred to as 
the Base BoosTexter Classifier runs. The BoosTexter Classifiers were then trained and 
tested on the text expanded with preferred terms. These runs are referred to as Preferred 
Term BoosTexter Classifier runs. 
 
 62 
 
Figure 5 - Boosting Based Coder – Boosting is a machine learning algorithm for improving the 
performance of supervised learning systems. It performs several iterations of breaking the data into 
subsamples, and training “weak learners” (i.e., a classifier that performs slightly better than chance). The 
weak learners are combined to create the boosted classifier. The boosted classifier assigns codes to each 
sample in the test data. 
 
4.4.4. Noun Phrase Detection and Preferred Term Identification  
The machine learning systems detected noun phrases (NP) in text via a Noun Phrase 
Detection Pre-processor (NPDP). NPDP was based on MetaMap Java APIs (MMTx) 
(Aronson, 2001). NPDP identified NPs that either directly or indirectly referenced 
diseases in medical reports. Inspired by prior work (Long, 2005; Sibanda et al., 2006), 
NPDP identified 17 UMLS semantic types (see Table 3) whose concepts could assist in 
the classification of diseases. First, NPDP mapped NPs in the text to UMLS semantic 
types. If the mapped semantic type was one of the 17 target semantic types, NPDP then 
tagged the NP. 
 
 
 
 
Training Data 
Assign Code(s) Weak Learners 
Test Data 
Boosted 
Classifier 
 63 
Table 3 - UMLS Semantic Types and Codes 
Code Description 
acab Acquired Abnormality 
anab Anatomical Abnormality 
bact Bacterium 
cgab Congenital Abnormality 
clna Clinical Attribute 
comd Cell or Molecular Dysfunction 
diap Diagnostic Procedure 
dsyn Disease or Syndrome 
fndg Finding 
inpo Injury or Poisoning 
lbpr Laboratory Procedure 
lbtr Laboratory or Test Result 
mobd Mental or Behavioral Dysfunction 
neop Neoplastic Process 
patf Pathologic Function 
sosy Sign or Symptom 
virs Virus 
 
NPDP examined each tagged NP for negation and uncertainty. Physicians often 
include negative or uncertain diagnoses in the narrative of medical reports (Rao et al., 
2003). Negation and uncertainty phrases mark negative and uncertain diagnoses, and help 
differentiate them from positive ones. NPDP made use of the pre-UMLS negation phrases 
of Extended NegEx (Sibanda et al., 2006) to identify adjectives that indicate the absence 
or uncertainty of a disease, symptom, test, finding, etc. in a tagged NP. It differentiated 
these adjectives from all other adjectives modifying the tagged NPs. For example, 
possible in possible reflux was differentiated from severe in severe reflux; possible as an 
uncertainty adjective was eliminated from the NP whereas severe was not. NPDP then 
mapped each NP to its UMLS preferred term. To insure that the preferred term was 
treated as a single token, spaces contained within multi-word preferred terms were 
replaced by underscores. For example, a report that stated:  
Febrile UTI evaluate for possible reflux. 
 64 
would first identify the disease related NPs and produce: 
[Febrile UTI] evaluate for [possible reflux]. 
where the identified NPs are offset by square brackets. It would then remove the absence 
and uncertainty adjectives from the NPs and would map the NPs to its UMLS preferred 
term, producing: 
[Urinary_tract_infection] evaluate for possible [Reflux]. 
where the resultant preferred terms are offset by square brackets. Given the output of 
NPDP, the systems expand the narratives of the patient reports with the NPDP assigned 
preferred terms. Table 4 lists the number of times disease related NPs were detected by 
NPDP in each of the data sets. 
 
Table 4 - Number of times disease related noun phrases were detected by NPDP. 
Data Set Training Noun Phrases 
Testing Noun 
Phrases 
CMC 2,831 2,705 
i2b2 57,235 40,420 
 
4.4.4.1. Semantic Correctness 
NPDP both identified noun phrases that either directly or indirectly reference 
diseases in the medical reports, and returned the UMLS preferred term for the NP. Given 
the variability in natural language, the preferred term returned by NPDP may not have 
been a synonym for the detected NP. We examined the impact that noise due to 
misidentified preferred terms played in the automatic classification of these medical 
reports. Specifically, while NPDP was able to assign preferred terms to noun phrases, we 
 65 
investigated both the semantic correctness of the assigned preferred terms and the 
consistency with which the preferred terms were assigned. 
From a semantic perspective, the preferred term could be the NP, contained in the 
NP, or a morph of the NP (e.g., Coughing and cough); be a synonym of the NP (e.g., 
Bedwetting and Nocturnal enuresis); be broader than the NP (e.g., Disease and Air space 
disease); be narrower than the NP (e.g., Abuse of steroids and steroids); or unrelated to 
the sense of the NP (e.g., Aggressive behavior and aggressive bronchodilators). 
A nurse librarian reviewed each of the detected NP/Preferred term pairs in the CMC 
data set, and 4000 of the detected NP/Preferred term pairs in the i2b2 Obesity data set, 
annotating each as: 
P – Preferred Term is the NP, a morph of the NP, or contained in the NP 
S - Preferred Term is a synonym for the NP, or a synonym for the noun contained in 
the NP 
B – Preferred term is a broader term than the NP 
N – Preferred term is a narrower term than the NP 
U – Preferred term is unrelated to the NP, or based upon context the wrong sense of 
the NP 
The complete annotation instructions can be found in Appendix A. 
Analyzing the annotations, we reported on the semantic correctness of the assigned 
preferred terms. We also reported on the consistency with which the preferred terms are 
assigned. Consistency was determined as follows: 
1. If an NP/Preferred term pair was annotated as a P or as an S, then the item 
was considered consistent, else; 
 66 
2. If an NP/Preferred term pair was annotated as a B or as an N, and the given 
preferred term was elsewhere annotated with a P or S, then the item was 
considered inconsistent, else; 
3. If an NP/Preferred term pair was annotated as a B or as an N, and only 
synonyms of the noun in the NP mapped to the preferred term of this pair, 
then the item was considered consistent, else; 
4. If an NP/Preferred term pair was annotated as a B or as an N, and there were 
semantically-disparate terms among nouns in the NPs which mapped to the 
preferred term of this pair, then the largest group of synonyms among the 
semantically-disparate terms reported as a B or as an N were considered 
consistent, else; 
5. If an NP/Preferred term pair was annotated as a U, and the given preferred 
term was elsewhere annotated with a P, S, B, or N, then the item was 
considered inconsistent, else; 
6. If an NP/Preferred term pair was annotated as a U, and only synonyms of the 
noun in the NP mapped to the preferred term of this pair, then the item was 
considered consistent, else; 
7. If an NP/Preferred term pair was annotated as a U, and there were 
semantically-disparate terms among nouns in the NPs which mapped to the 
preferred term of this pair, then the largest group of synonyms among the 
semantically-disparate terms was considered consistent. 
 67 
Tables 5 and 6 provide the breakdown of annotated NPs and of consistency. We 
determined that NPDP performs at about 95% consistency in its assignment of preferred 
terms to noun phrases. 
 
Table 5 - Consistency of the NPDP assigned preferred terms in the CMC data set. 
 
Training Data Testing Data 
Inconsistent Consistent Inconsistent Consistent 
Broader 111 15 107 13 
Narrower 0 4 0 6 
Preferred Term 0 1,934 0 1,885 
Synonym 0 734 0 670 
Unrelated 6 27 10 14 
Total 117 2,714 117 2,588 
Percent 4.13% 95.87% 4.33% 95.67% 
 
 
Table 6 - Consistency of a sample of 4000 NPDP assigned preferred terms from the i2b2 Obesity data 
set. 
 
Training Data Testing Data 
Inconsistent Consistent Inconsistent Consistent 
Broader 2 52 0 20 
Narrower 1 4 0 7 
Preferred Term 0 2,010 0 955 
Synonym 0 360 0 175 
Unrelated 117 173 71 53 
Total 120 2,599 71 1,210 
Percent 4.41% 95.59% 5.54% 94.46% 
 
Using these rules for determining consistency, all of the inconsistent NPDP 
assigned preferred terms in the CMC reports were manually corrected to produce a 100% 
consistent assigned preferred terms version of the reports. In order to determine the 
impact that inconsistent preferred terms have on classification, five additional versions of 
the manually corrected CMC reports were created with manually induced inconsistencies. 
The five versions consisted of 95%, 90%, 85%, 80%, and 75% of the NP/Preferred term 
pairs containing correct preferred terms. The manually induced inconsistencies were a 
 68 
random mix of unrelated preferred terms and preferred terms used by other noun phrases. 
The BoosTexter and Lucene classifiers were trained and tested on the reports expanded 
with the manually corrected 100% consistent assigned preferred terms and with the 
reports containing manually induced inconsistencies. These runs are referred to as the 
CMC Corrected Preferred Term Lucene Classifier runs (Table 7), the CMC Corrected 
Preferred Term Unigram BoosTexter Classifier runs (Table 8), and the CMC Corrected 
Preferred Term N-gram BoosTexter Classifier runs (Table 9).  
Table 7 - CMC Corrected Preferred Term Lucene Classifier runs at several levels of manually 
induced inconsistency. 
Consistency Precision Recall F-Measure 
100% 0.708 0.663 0.685 
95% 0.699 0.652 0.675 
90% 0.686 0.637 0.661 
85% 0.681 0.626 0.652 
80% 0.671 0.624 0.647 
75% 0.676 0.622 0.648 
Bold indicates significant difference from 100% Consistency.  
 
 
Table 8 - CMC Corrected Preferred Term Unigram BoosTexter Classifier runs at several levels of 
manually induced inconsistency. 
Consistency Precision Recall F-Measure 
100% 0.824 0.764 0.793 
95% 0.805 0.754 0.779 
90% 0.807 0.735 0.769 
85% 0.814 0.746 0.779 
80% 0.805 0.735 0.768 
75% 0.787 0.729 0.757 
Bold indicates significant difference from 100% Consistency. 
 
Table 9 - CMC Corrected Preferred Term N-Gram BoosTexter Classifier runs at several levels of 
manually induced inconsistency. 
Consistency Precision Recall F-Measure 
100% 0.863 0.818 0.840 
95% 0.860 0.814 0.836 
90% 0.850 0.793 0.820 
85% 0.841 0.788 0.814 
80% 0.838 0.783 0.810 
75% 0.836 0.774 0.804 
Bold indicates significant difference from 100% Consistency. 
 69 
 
These results indicated that the Lucene and BoosTexter classifiers would not be 
adversely impacted as long as the assigned preferred terms are at least 90% consistent. 
Given that the NPDP performs at approximately 95% consistency in its assignment of 
preferred terms to noun phrases, we concluded that the use of NPDP would not have a 
negative impact on classification. 
4.4.4.2. The Impact of Negation and Uncertainty on Preferred Terminology 
This study examined the impact that negation and uncertainty had on the 
effectiveness of applying preferred terminology to resolve synonymy on a document level 
multi-label classification task. Assertions made in medical reports are used to form 
diagnoses. Negative assertions are those diagnoses that have been eliminated as 
potentially present in the patient, while uncertain assertions are those that have not been 
eliminated, but are not definitively present in the patient. The simple detection that a 
condition or disease appears in a medical report is not sufficient to state that the condition 
or disease is present in the patient. 
The hand-crafted rule-based system contained negation and uncertainty subsystems 
that are described in  4.2. For the machine learning systems, this study employed the 
source code of ConText (Harkema et al., 2009) to determine the polarity of noun phrases, 
which either directly or indirectly referenced diseases in the medical reports. These noun 
phrases were identified by NPDP. 
In order to distinguish the positive, negative, and uncertain assertions from one 
another, once the polarity of an assertion had been determined, the positive assertions 
 70 
were unchanged, but the systems transformed the negative and uncertain assertions in the 
following manner:  
1. Sentences containing assertions identified as negative were repeated and 
modified with the noun phrase pre-pended with “abs” (e.g., “Patient denies 
fever.” was repeated as “Patient denies absfever.”). 
2. Sentences containing assertions identified as uncertain were repeated and 
modified with the noun phrase pre-pended with “poss” (e.g., “Possible 
pneumonia.” was repeated as “Possible posspneumonia.”).  
This study refers to these transformed terms as asserted noun phrases. 
This study asserted noun phrases for the text of the medical reports. The Lucene and 
BoosTexter classifiers were trained and tested on the reports with asserted noun phrases. 
These runs are referred to as the Asserted Lucene Classifier runs and the Asserted 
BoosTexter Classifier runs. This study then asserted noun phrases for the text of the 
medical reports expanded with preferred terms. The Lucene and BoosTexter classifiers 
were trained and tested on reports expanded with asserted preferred terms. These runs are 
referred to as the Asserted Preferred Term Lucene Classifier runs and the Asserted 
Preferred Term BoosTexter Classifier runs.  
4.4.5. Surface Processing 
Morphological processing can help reveal information contained in the narrative of 
medical reports. This research employed two common types of morphological analysis 
via surface processing of the narrative of medical reports: case conversion and stemming, 
both of which attempt to conflate variations of a given word. Case conversion 
 71 
transformed each word so that in every instance all of the alphabetic characters are the 
same case. For example, when converting to lower case, the strings EXTRACT and 
Extract would both became extract. 
Manipulating word suffixes through stemming brought together inflected variations 
of a word (Lovins, 1968; Porter, 1980) under the word’s morphological root. Stemming is 
based upon the idea that terms with a common stem will typically have similar meanings. 
For example, the words extraction and extracted would both have been transformed into 
extract. The machine learning systems employed the source code for Martin Porter’s 
stemmer (Porter, 2006). The Porter stemming algorithm treated the suffix of a word as 
being comprised of one or more simpler suffixes. The algorithm followed a series of 
ordered rules and modified or removed each of the simpler suffixes one at a time. For 
example, the word hopefulness would first have been transformed into hopeful and then 
finally into hope. Since this algorithm operated without the use of a lexicon, words with 
different meanings could have been erroneously conflated. However, in a comparison of 
stemming approaches (including the Porter Stemmer), Hull (1996, p. 83) noted that 
methods “based on a lexicon … cannot correctly stem words which are not contained in 
the lexicon.” He concluded that, excepting the simple approach of removing plurals, there 
are no performance differences between the use of a set of ordered rules and a lexicon. 
4.5. Evaluation 
This study evaluated each of the systems and compared them to a baseline using 
micro-averaged Precision, Recall, and F-measure (see section  3.3), metrics commonly 
used to evaluate NLP systems. Precision, Recall, and F-measure are derived from true 
 72 
positive (TP), false positive (FP), and false negative (FN) counts. For the CMC data set, 
we followed the scoring system used by the challenge organizers (Pestian et al., 2007) 
and counted each correctly assigned code as a TP, each missed code as an FN, and each 
erroneously assigned code as an FP. For the i2b2 Obesity data set, we counted each 
correctly predicted disease as present in the patient as a TP, each missed disease present 
in the patient as an FN, and each erroneously predicted disease present in the patient as an 
FP. 
Following the examples of Message Understanding Conferences (Grishman & 
Sundheim, 1996; Hirschman, 1998) and the i2b2 Smoking Challenge Shared-Task 
(Uzuner et al., 2008), this study checked and reported the significance of performance 
differences at α=0.10. This study tested the significance of performance differences with 
Z-scores. This study applied a two-tailed Z test, with Z = ±1.645. This study reported 
results only on the test sets. 
This study evaluated the impact that negation and uncertainty have on preferred 
terms when applied to a hand-crafted rule-based system and to two machine learning 
systems. The study evaluated a total of five hand-crafted rule-based coding systems: the 
FRB system, the BRB system, and the BRB system combined with each of the three 
semantic components. We compared the performance of the hand-crafted FRB system 
and the four variations of the FRB system. This study also provided a baseline by 
assigning the most frequent ICD-9-CM code (i.e., 786.2) to all of the reports with which 
to compare all five of hand-crafted rule-based coding systems. 
We then compared the performance of machine learning systems when enhanced 
with preferred terminology, asserted noun phrases, and both preferred terminology and 
 73 
asserted noun phrases. For both the CMC and i2b2 data sets, we compared each of the 
Preferred Term Lucene Classifiers and Preferred Term BoosTexter Classifiers to their 
respective Base Classifier. The results of this comparison helped to establish that 
preferred terminology, on its own, was able to improve system performance in the 
automated classification of medical reports. In order to help establish that asserted noun 
phrases, on its own, was able to improve system performance, each of the Asserted 
Lucene Classifiers and Asserted BoosTexter Classifiers were compared to their respective 
Base Classifier runs. Lastly, this study compared each of the Asserted Preferred Term 
Lucene Classifiers and Asserted Preferred Term BoosTexter Classifiers to their Base 
Classifier runs. The results of this set of comparisons helped to establish that preferred 
terminology when combined with asserted noun phrases was able to improve system 
performance in the automated classification of medical reports. 
4.6. Results and Discussion 
4.6.1. Hand-Crafted Rule-Based System 
The results of the Hand-Crafted Rule-Based systems (Table 10) showed significant 
differences in system F-measures between the BRB system and the systems enhanced 
with synonymy and uncertainty. We also found significant differences in Precision, 
Recall, and F-measure between the FRB system and each of the other systems. Given the 
guiding principle that ICD-9-CM codes should mark only the definite diagnoses, 
improvement of the FRB system over the BRB system shows that the lexical elements 
present in a report, on their own, did not definitively indicate the presence of a disease or 
symptom in the patient. Adding synonyms and asserting uncertainty about diseases and 
 74 
symptoms can each improve performance. An examination of the results revealed that 
negation and uncertainty helped to eliminate false positives, and that using synonyms of 
disease names and disease descriptions helped to improve true positives. 
Table 10 - Hand-Crafted Rule-Based System Performances. 
System Precision Recall F-Measure 
BRB 0.799 0.786 0.793 
BRB + Negation 0.825 0.799 0.812 
BRB + Synonymy 0.801 0.856 0.827 
BRB + Uncertainty 0.852 0.815 0.833 
FRB 0.876 0.895 0.886 
Baseline 0.270 0.218 0.241 
Bold indicates significant difference from the Base Rule-Based system. 
 
Analyzing the results, we found that adding negation to the BRB system corrected 
37 FPs and 19 FNs, while inducing additional incorrect codes in just one report. Rules 
encoding synonyms corrected 1 FP and 85 FNs (59 of which previously had no code 
assigned by the BRB system). The synonym rules also introduced 19 incorrect codes, 
although six of these incorrect codes were due to an inconsistency in the coding of the 
ground truth. The annotators used two different codes, 599.0 and V13.02, interchangeably 
to mark Urinary Tract Infections (UTI). This resulted in some otherwise indistinguishable 
reports to be marked with two different codes. It also caused the majority vote to fail to 
identify UTI as a diagnosis for some reports. Finally, we determined that the uncertainty 
rules corrected 78 FPs and 40 FNs while introducing coding errors into eight reports. 
4.6.2. NPDP Post-Hoc Coordination  
Through manual analysis of the unenhanced medical reports and the asserted noun 
phrases medical reports, we found that NPDP assigned preferred terms did in fact act as a 
 75 
form of post-hoc coordination and brought together semantically-equivalent but lexically-
disparate terms. As a result of post-hoc coordination, as shown in Table 11, the unique 
detected noun phrases in the data were represented by one third as many preferred terms 
in the CMC corpus and one fifth as many preferred terms in the i2b2 corpus. The ratio of 
noun phrases to preferred terms may be a factor in a system’s ability to benefit from the 
addition of preferred terms, and is discussed in detail in section  4.6.4 below. 
Table 11 - Number of unique noun phrases and unique preferred terms by data set. 
Data Set 
Original 
NPs 
Preferred 
Terms 
CMC Training Set 972 304 
CMC Test Set 901 316 
i2b2 Training Set 19,709 3,513 
i2b2 Test Set 14,745 2,987 
4.6.3. Lucene Classifier 
The contribution of asserted noun phrases and preferred terminology to overall 
classification performance was limited for the Lucene Classifiers. The Base Lucene 
Classifier (Table 12) showed no significant difference in performance when compared to 
the Preferred Term, Asserted, or Asserted Preferred Term Lucene Classifiers on either 
corpus. We attributed these results to the presence of information in the text beyond the 
direct references to the diseases and to the k-Nearest Neighbor approach to classification. 
Even in the presence of lexically-disparate terms and in the absence of text 
expansion with preferred terms or asserted noun phrases, some reports already contain 
enough information for correct classification of the reports in our task. For example, for 
CMC target report 99676143: 
1-year, 5-month - old female with first UTI. 
Normal kidney and bladder ultrasound. 
 
 76 
the Base Lucene Classifier retrieves report 99776226: 
1-year - old female with first time urinary tract 
infection. 
1. Debris in the urinary bladder. 2. Otherwise normal 
renal ultrasound. 
 
Despite the lexical disparity between the disease references “urinary tract infection” 
and “UTI,” Lucene classifiers can benefit from the terms female, first, normal, and 
ultrasound to label the report. While introduction of preferred terms to these reports 
improved the lexical overlap between the reports by bringing together “UTI” and “urinary 
tract infection,” and thus strengthening the evidence that would help classify these 
records as Urinary Tract Infection being present, it did not further improve the already 
correct document level classification. These lexically-disparate direct references to the 
disease and unenhanced assertions did not stand in the way of correct classification. 
The k-Nearest Neighbor approach was not able to take full advantage of the 
additional information provided by preferred terminology or by asserted noun phrases. 
The Lucene Classifiers utilize a vector space model, where each dimension of the vector 
represents a word and each vector in the space represents a document. Having used all of 
the words in a document as features, enhancing the text of a document with preferred 
terms or asserted noun phrases did not produce a major change in that document’s vector. 
The position of the enhanced documents in the vector space would be similar to the 
position of the original documents in the vector space, leaving the k-nearest documents 
relatively unchanged. 
 
 
 77 
Table 12 - Lucene Classifier Performance. 
 Precision Recall F-Measure 
CMC Base 0.704 0.657 0.680 
CMC Preferred 
Term 0.710 0.667 0.688 
CMC Asserted 0.704 0.663 0.683 
CMC Asserted 
Preferred Term 0.704 0.655 0.679 
i2b2 Base 0.614 0.664 0.638 
i2b2 Preferred 
Term 0.615 0.647 0.631 
i2b2 Asserted 0.618 0.644 0.631 
i2b2 Asserted 
Preferred Term 0.617 0.653 0.635 
Bold indicates significant difference from respective Base system. 
 
4.6.4. BoosTexter Classifiers 
Examining the BoosTexter Classifiers (Table 13), we found that the addition of 
preferred terminology did not consistently improve performance. However, the 
combination of both preferred terms and asserted noun phrases in the Asserted Preferred 
Term BoosTexter Classifiers showed significant improvement from the respective Base 
Classifiers on both corpora. 
We found that the post-hoc coordination provided by NPDP improved the 
BoosTexter Classifiers’ performance on the i2b2 corpus, but did not improve the 
BoosTexter Classifiers’ performance on the CMC corpus. We attribute the difference in 
results to the ratio of noun phrases to preferred terms in each of the two corpora. The 
CMC data came from a single medical department at a single hospital. Suchan (1995) 
found that individuals who work together for a period of time tend to employ consistent 
terminologies. The consistent use of terminology in the text of the medical records 
 78 
resulted in less of a need to resolve semantically-equivalent but lexically-disparate terms 
in the narrative of the CMC medical records.  
Focusing on the i2b2 corpus, we found, for example, the lexically-disparate terms 
cholesterol and hypercholesterolemia used as features in determining the presence of 
Hypercholesterolemia (abnormally high levels of cholesterol in the blood) in a patient by 
both the i2b2 Base and the i2b2 Preferred Term Unigram BoosTexter Classifier. Our 
approach to post-hoc coordination brought together cholesterol and hypercholesterolemia 
under the preferred term hypercholesterolemia, strengthening the semantic link between 
reports that used either of these two terms. For example, i2b2 report 487 listed “elevated 
cholesterol,” but failed to make any references to the preferred term hypercholesterolemia 
and was misclassified by the i2b2 Base Unigram BoosTexter Classifier, despite the 
evidence provided by the word cholesterol. Expanding the text of report 487 to include 
the preferred term hypercholesterolemia helped to correctly classify Hypercholesterolemia 
as being present in the patient in this report. 
BoosTexter’s weak learner weights played a role in this improvement. In the 
absence of the preferred term, the lexical disparity of semantically-equivalent terms 
limited their contribution to classification, e.g., the weight assigned to the word 
cholesterol was not sufficient for correct classification of the report 487. The addition of 
the preferred term hypercholesterolemia to enrich reports with lexically-disparate and 
semantically-equivalent terms resulted in an adjustment in the weak learner’s weight for 
the term hypercholesterolemia, i.e., while the non-preferred terms for 
hypercholesterolemia all contribute to the classification, the adjusted weight of this 
preferred term improved classification of Hypercholesterolemia. 
 79 
Table 13 - BoosTexter Classifier Performance. 
  Precision Recall F-Measure 
CMC Base 
unigram 0.812 0.747 0.778 
n-gram 0.865 0.812 0.838 
CMC Preferred 
Term 
unigram 0.812 0.761 0.785 
n-gram 0.865 0.815 0.839 
CMC Asserted  
unigram 0.837 0.767 0.800 
n-gram 0.866 0.813 0.839 
CMC Asserted 
Preferred Term 
unigram 0.848 0.801 0.824 
n-gram 0.883 0.815 0.848 
i2b2 Base 
unigram 0.902 0.889 0.895 
n-gram 0.911 0.895 0.903 
i2b2 Preferred 
Term 
unigram 0.915 0.911 0.913 
n-gram 0.916 0.916 0.916 
i2b2 Asserted 
unigram 0.908 0.891 0.899 
n-gram 0.914 0.903 0.908 
i2b2 Asserted 
Preferred Term 
unigram 0.928 0.905 0.916 
n-gram 0.934 0.913 0.924 
Bold indicates significant difference from respective Base system. 
 
Enhancing the BoosTexter systems with both preferred terminology and asserted 
noun phrases appeared to provide the most consistent improvement over the base 
systems. While the use of preferred terms helped to bring together semantically-
equivalent but lexically-disparate terms for a given concept, one could consider assertion 
as the ability to distinguish lexically-similar but semantically-disparate nouns. The 
sentences Patient has fever. and Patient denies fever., describe opposing statuses of the 
patient. By pre-pending abs to fever, the system essentially creates a distinct term, 
absfever, for the negative case. When combined with preferred terms, asserting the 
preferred terms creates a distinct preferred term for each of the positive, negative and 
 80 
uncertain diagnoses. While not powerful enough to improve performance on its own, the 
combination improved performance on the unigram CMC Asserted Preferred Term 
BoosTexter Classifier, and on both the unigram and n-gram i2b2 Asserted Preferred Term 
BoosTexter Classifiers. 
4.7. Conclusions 
We examined the characteristics of our data, task, and approach in order to put these 
results into context. We believe that differences in the data, task, and approach can 
account for the conflicting reports of the role of preferred terminology found in the 
literature. Furthermore, these specific characteristics can explain the limitations of the 
contribution of preferred terminology in our systems.  
We begin with the data. The CMC data set can be described as being terse, 
containing very little extraneous information. When expanded with preferred 
terminology, the machine learning classifiers showed no significant difference in 
performance from their Base runs on the CMC data. Scott and Matwin (1998) observe 
that their approach to addressing lexical disparity is not likely to work when the text is 
“written concisely and efficiently.” This description could be used to characterize the 
CMC data. When combined with the relative consistency of terminology use, the 
conciseness of the text in the CMC data set may help to explain why preferred 
terminology did not improve performance with either of the machine learning classifiers. 
The task for this study was document level classification. More specifically, to 
determine the presence of a disease or condition in the patient based upon the narrative of 
medical reports. The task of classification differs from other tasks. Classification entails 
 81 
assigning class labels to documents. Let us consider a vector space representation model 
of a set of documents. Given lexically-disparate terms, the vectors of the documents with 
a given class label may produce several clusters in the vector space. Assigning a class to a 
target report based upon the cluster nearest to the target report’s vector is sufficient for 
classification. However, compare this to the task of Information Retrieval (IR). IR 
attempts to retrieve all relevant documents based upon a query. If we consider the 
classification target report as the query for an IR task, then the IR system would need to 
retrieve all of the documents represented by the vectors in each of the several clusters. 
The use of preferred terms would help to conflate the several clusters, thereby improving 
IR recall. However, conflating the several clusters does not improve classification recall 
in those cases where the target already matches one of the several clusters. 
The approach used to build a classifier may impact preferred terminology’s 
contribution to classification. Both the Lucene and the BoosTexter based systems use 
machine learning approaches to classification. Machine learning approaches have the 
ability to adapt to variability and noise in the data (Mitchell et al., 1990). Given a 
sufficient number of training examples, machine learning systems are often able to 
successfully perform classification even in the presence of lexical disparity. Conversely, a 
hand-crafted rule-based approach does not inherently address the issues associated with 
lexical disparity. Hand-crafted rule-based systems often address lexical disparity with 
vocabulary control. The hand-crafted rule-based system described in this study 
successfully addressed lexical disparity via vocabulary control on the CMC data set. 
Similarly, Childs et al. (2009) applied vocabulary control to a hand-crafted system when 
predicting the labels on the i2b2 data set. Both systems conflated lexically-disparate terms 
 82 
for a given disease by developing a list of text strings that represented a disease or a 
concept associated with a disease. While it may be important for a hand-crafted approach 
to classification, the contribution of preferred terminology on its own may be limited for 
machine learning approaches to classification. However, the performance of preferred 
terminology can be enhanced when employed with asserting disease noun phrases. 
4.8. Summary 
This Chapter examined the role of preferred terminology as a means of post-hoc 
coordination in automatic classification of patient medical data. We applied natural 
language processing tools to the free text of the medical reports. We built and evaluated 
both a hand-crafted rule-based system and two machine learning systems. We 
experimented with medical reports that came from two different corpora, one containing 
radiology reports and the other containing discharge summaries. We gathered 
semantically-equivalent but lexically-disparate medical terms used in these reports under 
preferred terms for multi-label classification of the medical reports.  
Enriching medical reports with preferred terms failed to consistently improve 
classification system performance. These findings indicate that the contribution of 
preferred terminology to a natural language processing task depends on the nature of the 
task, the data, and the approach applied. 
 83 
Chapter 5. Specializing for Predicting Obesity and its Co-morbidities2
5.1. Introduction 
 
Narrative medical records can inform many applications, including creating patient 
problem lists (Sibanda et al., 2006), assigning billing codes (Goldstein et al., 2007), 
identifying co-morbidities (Informatics for Integrating Biology and the Bedside, 2008), 
and marking early warning signs for outbreaks of disease pandemics and epidemics 
(Krenzelok, Macpherson, & Mrvos, 2008). However, before they can be informative for 
computer-supported applications, the narrative medical records must be processed with 
tools that can extract meaningful facts from them.  
Classification provides a way of processing the content of narrative medical records. 
An ideal data set for tasks such as classification contains ample examples of all classes 
that are in question. However, data sets that include all pertinent categories, with 
sufficient samples from each category, are hard to obtain and even harder to create. 
Therefore, we are often limited to small data sets that try to represent reality. One 
possible characteristic of these representative data sets is the non-uniform distribution of 
samples among the classes. A second possible characteristic is the sparsity of the samples 
in some of the classes. 
Although the concept of sparsity depends on the task and the representation used for 
the data (Jain & Chandrasekaran, 1982), non-uniform distribution of classes and sparsity 
of samples can pose challenges to methods, such as classification, that are based on an  
                                                 
2 This chapter was previously published as Goldstein, I., & Uzuner, Ö. (2009). 
Specializing for predicting obesity and its co-morbidities. Journal of Biomedical 
Informatics, 42(5), 873-886. 
 84 
exploration of statistics for representing data. In statistical classification, the small 
number of examples offered by a sparse, less well-represented class may adversely affect 
the training of a classifier on that class. Given a non-uniform distribution of classes in the 
data, classifiers may simply predict the well-represented classes in order to obtain high 
overall accuracy (Tang & Liu, 2005). In binary classification it would be sufficient to 
have one of the classes be well represented in the data, e.g., exclusion from class A 
implies inclusion in class B. However, the same is not true in multi-class classification 
where exclusion from one class does not imply inclusion in any other specific class, e.g., 
exclusion from A implies inclusion in one of B, C, D… but does not specify which one.  
Despite the inclination of statistical classification techniques to focus on the well-
represented classes in data, the importance of the information contained in a class may 
not be reflected by how frequently or infrequently the class appears in the data. Even the 
small, sparse, less well-represented classes can contain valuable information which makes 
their classification worthwhile. 
Our aim in this paper is to improve the classification performance on the less well-
represented classes in multi-class classification of diseases based on information in 
medical discharge summaries. While improving performance on less well-represented 
classes, we also aim to maintain overall performance on the task. Given a discharge 
summary of a patient, our task is to predict the status of the patient with respect to obesity 
and 15 of its co-morbidities. We treat the prediction for each disease as an independent 
multi-class classification task. Each of the 16 diseases can be classified as being present, 
absent, or questionable in the patient, or unmentioned in the discharge summary of the 
patient. We observe that the data for these tasks exhibit both non-uniformity and sparsity. 
 85 
In other words, there is an imbalance in the distributions of present, absent, questionable, 
and unmentioned classes for each disease, e.g., there are orders of magnitude more 
examples of the unmentioned class than the questionable class for most diseases, and 
some classes contain a very small number of examples, e.g., the absent class contains 
only 152 of the 19,695 judgments across all diseases.  
To improve macro-averaged performance on multi-class classification in the 
presence of non-uniform distribution of data and in the presence of sparse, less well-
represented classes, we develop and present specializing. Specializing is a novel method 
for combining classifiers. In general, it works as follows: for a multi-class classification 
task, for each class in the data, specializing selects a specialist classifier from a set of 
available complementary classifiers that are trained in a one-versus-all (OVA)3 manner to 
predict that class. In other words, the specialist classifier for each class focuses on 
distinguishing that class from the rest. Specializing then combines the specialist classifier 
for each class with a catch-all classifier that is trained as a multi-class classifier4
                                                 
3 In a multi-class data set, a one-versus-all classifier recognizes only one class and learns 
to distinguish it from all others, e.g., class A versus not class A. 
 for the 
task and can distinguish all classes for that task from one another. In terms of 
performance metrics, the specialist classifier for each class is the highest F-measure 
OVA-trained classifier for that class whereas the catch-all classifier gives the highest 
micro-averaged F-measure across all classes for the task. The combination of the 
specialist classifiers with the catch-all classifier produces the specializing classifier for 
the task. 
4 A multi-class classifier learns to recognize multiple classes at the same time, i.e., 
distinguish all of classes A, B, C, D, etc., from each other. 
 86 
We combine the specialist and the catch-all classifiers by allowing the specialist 
classifiers to predict their classes before the catch-all classifier labels those samples that 
fail to receive a definitive class assignment from any of the specialist classifiers. This 
approach avoids having to handle contradicting assignments from competing specialist 
classifiers by running the specialist classifiers in a strict order, starting with the specialist 
for the least well-represented class, working its way up to the most well-represented class. 
Each specialist classifier only runs on those samples that do not receive a definitive class 
assignment from the specialists that run before it. This strict order ensures that the 
specialists for the less well-represented classes are given due consideration before the 
specialists for the better represented classes are run. For those samples that do not receive 
a definitive class assignment from any of the specialist classifiers, the catch-all classifier 
is run at the end, and always assigns a class.  
For predicting the status of patients with respect to obesity and 15 of its 
co-morbidities, we treat the classification of each disease as an independent multi-class 
classification task, i.e., we have 16 independent multi-class classification tasks and each 
disease can be present, absent, or questionable in the patient, or unmentioned in the 
discharge summary of the patient. In this paper, we refer to obesity and its 15 
co-morbidities as diseases. We refer to present, absent, questionable, and unmentioned as 
classes. We apply specializing to each disease separately. We create specialist classifiers 
for each of the present, absent, questionable, and unmentioned classes of each disease. 
We supplement the specialist classifiers for each disease with a catch-all classifier that 
can distinguish present, absent, questionable, and unmentioned classes from each other 
for that disease. We choose the specialist and the catch-all classifiers from C4.5 decision 
 87 
trees, Naïve Bayes classifiers, and AdaBoosted decision stumps. By combining the 
specialist and catch-all classifiers for a disease, we create the specializing classifier for 
that disease. 
5.2. Related Work 
Specializing trains classifiers with complementary strengths and combines these 
classifiers in a novel manner to create a specializing classifier per task. 
5.2.1. Combining Classifiers 
In the literature, various approaches to combining classifiers have been presented. 
Voting, stacking, and boosting are among these approaches. Voting combines classifiers 
in various ways, such as selecting the class assigned by the majority of the classifiers or 
taking the average or the product of the probabilities that the item is correctly classified 
(i.e., how strongly the classifier believes in the prediction). Stacking (Wolpert, 1992) 
applies cross-validation to combining individual classifiers while adjusting for the biases 
of these classifiers. More specifically, stacking attempts to figure out the bias of each of 
the classifiers with respect to the training data. It then adjusts for the determined bias. 
Stacking can be employed either to combine different classifiers or to improve the 
performance of a single classifier. Boosting iteratively combines many “weak learners” 
from a classifier (Freund, 1995; Schapire, 1990; Schapire & Singer, 2000): at each 
iteration, it trains a weak learner that tries to improve performance on the samples that 
have not been adequately learned by past weak learners. 
 88 
5.2.2. Combined Classifiers 
The literature has shown that ways of combining classifiers vary in their merits. 
Chan and Stolfo (1993; 1993; 1995) addressed the issue of scaling of data sets (i.e., the 
problems that arise as data sets grow larger than available computer memory) by dividing 
the training data set into subsets and by training various classifiers on these subsets. They 
presented combiners and arbiters as two approaches for combining classifiers. Combiners 
learn the relationship between the output of the individual classifiers and the correct 
classification in order to provide a prediction. Arbiters are classifiers which are combined 
with an arbitration rule, and can either arbitrate among the individual classifiers or 
provide their own predictions. Both combiners and arbiters are forms of conflict 
resolution among classifiers. Zenko et al. (2001) evaluated seven approaches for 
combining classifiers on 21 data sets, finding that stacking outperformed boosting. 
Liu et al.(2008) recognized that many classification methods discard all but the highest 
performing classifier and proposed a Combination Strategy for Multi-class Classification 
(CSMC). CSMC employed set theory and evidence weight, retaining multiple rules to 
combine into a classifier for multi-class multi-label classification. CSMC weighted rules 
based upon their frequency in the data, and saw improvement over individual classifiers 
including C4.5. Daskalakis et al. (2008) used a “panel of classifiers” (i.e., statistical 
quadratic Bayesian, k-nearest neighbor, and probabilistic neural network classifiers) to 
develop a multi-class classifier for biopsies of thyroid nodules. Each classifier in this 
panel approached the classification problem from a different perspective. This “panel of 
classifiers” used majority voting and resulted in a significant improvement over the use of 
the best single classifier. Similarly, Eom et al. (2008) compared individual classifiers to 
 89 
“ensemble models” and tested them against four data sets (i.e., cardiovascular disease, 
pulmonary complaints, tuberculosis, and cancer). The “ensemble models” combined 
individual classifiers whose errors differed from one another (i.e., there was minimal 
overlap in classification errors). Each of the “ensemble models” outperformed the best 
performing individual classifier on each of the four data sets. 
5.2.2.1. Characteristics of Combined Classifiers 
Duin et al. (2000), when analyzing the performance of various combined classifiers, 
concluded that “there is no overall winning combining rule” for classifiers. The selection 
of classifiers and of approaches to combining those classifiers depends upon the data, 
especially the number of classes to be classified. Tax et al.(2000) demonstrated this by 
comparing multiple rules for both binary and multi-class handwritten digit recognition. 
Surprisingly, they observed that when dealing with a binary problem, there was no 
difference in the average results between combining three classifiers and combining ten 
classifiers (i.e., on average, the combined classifiers performed equally well on binary 
classification). However, when there are more than two classes (i.e., multi-class 
classification), differences in the results appear with various classifier combinations.  
Hu and Damper (2008) took a theoretical approach to extend Kuncheva’s (2004) 
“no panacea” principle which stated that when combining two classifiers in binary 
classification problems, there is no perfect combination of algorithms for all situations 
(i.e., one size does not fit all). Hu and Damper showed that the principle applies when 
there are more than two classifiers and for multi-class classification.  
We extended the above studies by developing specializing, a method for combining 
statistical classifiers for multi-class classification of diseases based upon information in 
 90 
discharge summaries. Specializing differs from approaches in literature in its novel 
mixture of several performance enhancing ideas inasmuch as it: (1) takes advantage of 
classification algorithms that can learn samples that are hard to learn even when the 
attribute sets are relatively large and noisy, (2) trains these classifiers in a way, 
specifically in an OVA manner, that allows them to specialize on even the less well-
represented classes, (3) uses the best OVA-trained classifier for a class as the specialist 
for that class, (4) utilizes information about each class to impose a priority order on the 
specialists, (5) takes advantage of the difference in the focus of multi-class classification 
from one-versus-all classification in order to supplement the specialist classifiers with 
catch-all classifiers, and (6) makes use of the complete data set rather than subsets of data 
in order to maintain the most accurate representation of less well-represented classes 
throughout. Specializing allows a single classifier to make an assignment for a specific 
class. It combines classifiers in a sequential manner in order to arrive at a definitive class 
assignment for each of the samples in the data. Sequential activation of classifiers 
eliminates the need for an additional conflict resolution strategy. 
5.3. Materials and Methods 
The data set for the study presented in this paper was developed for the i2b2 
Shared-Task and Workshop on Challenges in Natural Language Processing for Clinical 
Data: Obesity Challenge (Informatics for Integrating Biology and the Bedside, 2008). 
This data set consisted of medical discharge summaries which had been annotated by 
doctors for obesity and 15 of its co-morbidities. We split this data set into training and 
test sets for each disease. We linguistically processed the discharge summaries in the data 
 91 
and extracted attributes with which to represent their text (see Section  5.3.3 for details). 
In order to automate the task of labeling obesity and co-morbidities as present, absent, or 
questionable in the patient, or unmentioned in the discharge summary of the patient, we 
set up one multi-class classification task per disease. We trained J48 decision trees, Naïve 
Bayes, and AdaBoost.M1 classifiers using 10-fold cross-validation on the training data 
for each disease. For each of the 16 diseases, we designated one specialist classifier per 
class and a catch-all classifier per disease based on cross-validation on the training set for 
that disease. We created the specializing classifier for each disease from the specialist 
classifiers and the catch-all classifier for that disease. 
5.3.1. Data 
The data for this study consisted of 1238 discharge summaries from Partners 
HealthCare. This data had been fully de-identified and then annotated by obesity experts 
for information on obesity and its co-morbidities. Two obesity experts annotated each 
discharge summary and determined whether obesity and its 15 most frequent 
co-morbidities were present (marked with a Y in the data set), absent (marked with an N 
in the data set), or questionable (marked with a Q in the data set) in the patient according 
to explicitly stated text in the discharge summary, or unmentioned (marked with a U in 
the data set) in the text. In cases where the two obesity experts disagreed, the discharge 
summary was annotated by a third expert. Majority decision among the experts 
determined the final judgment on each of the 16 diseases. In the absence of a majority 
vote, some diseases remained without a judgment. In other words, some records 
 92 
contained final judgments only for a subset of the 16 diseases. The Institutional Review 
Boards of SUNY, Albany and Partners HealthCare approved this study. 
5.3.2. Training and Test Sets 
As detailed by disease in Table 1, following the division of the data released as part 
of the i2b2 Shared-Task, we employed 59% of the discharge summaries as our training 
data and set aside 41% of the discharge summaries as our test data. The training data 
contained 11,630 class assignments across the 16 diseases. The test data contained 80655
Table 1
 
class assignments across the 16 diseases. The training data for each disease included a 
subset of the 59% of the discharge summaries set aside for training. The summaries that 
did not have a final judgment for a disease were omitted from the training data for that 
disease. Similarly, the test data for each disease included a subset of the 41% of the 
discharge summaries set aside for testing. The summaries that did not have a final 
judgment for a disease were omitted from the test data for that disease.  shows the 
non-uniform distribution of classes in the data across 16 diseases and indicates the absent 
and questionable classes as smaller, sparse, less well-represented classes in all diseases. 
5.3.3. Feature Extraction 
Linguistic processing of narratives can expose attributes that can contribute to more 
accurate representation of the narratives’ contents. The choice of linguistic attributes 
represented and utilized for a task depends on the task. These attributes can be based 
upon syntax, semantics, or even just surface processing. As the focus of this article is on 
                                                 
5 Following the studies in this manuscript, the i2b2 Shared-Task organizers eliminated 
some of the test samples from the data.  As a result, the final i2b2 Shared-Task test data 
included only 8044 class assignments 
 93 
specializing as a method for classifying diseases, we rely on only a basic set of attributes, 
namely stemmed lowercase words and the polarity (i.e., positive, uncertain, or negative) 
of the assertions made on terms corresponding to medical problems (i.e., diseases and 
symptoms). 
In order to extract these features from discharge summaries, before classification, 
we syntactically processed the discharge summaries to detect terms corresponding to 
medical problems. We then semantically processed the discharge summaries to determine 
the polarity of the assertions made on the identified terms. Once the polarity was 
determined, we transformed the text to distinguish the negative and uncertain assertions 
from positive assertions. Finally, we converted the transformed text to lower case and 
applied stemming (Porter, 1980) in order to conflate variations of words.  
We used a binary term vector space model (Salton & Buckley, 1988) to represent 
our data. In this vector space, we represented each discharge summary by an attribute 
vector. Each attribute in an attribute vector was mapped to a dimension of the vector 
space. We used attribute vectors as input to the classifiers in Weka (Witten & Frank, 
2005).  
5.3.3.1.  Syntactic Processing 
In order to determine the polarity of medical problem assertions (see the next 
section on Semantic Processing), we first identified terms corresponding to medical 
problems in our discharge summaries. We defined medical problems as diseases and 
symptoms. We based our medical problem terms upon a target list of Unified Medical 
Language System (UMLS) semantic types (see Table 3) and marked them using the 
MetaMap (Aronson, 2001) Java API (MMTx). More specifically, we identified the noun 
 94 
phrases (NP) in each report and studied the possible semantic types of the NPs based on 
UMLS. For each NP, we checked its possible semantic types for ones that are included in 
the target list in Table 3. Matching the semantic type of an NP and its headword to a 
member of the target list concluded the process and marked the whole NP as containing a 
term corresponding to a medical problem. Failure to achieve a match for a noun phrase 
required that we search its sub-phrases and the headwords of the sub-phrases for UMLS 
semantic types included in the target list. The sub-phrase that matched one of the target 
list semantic types, and whose headword also matched one of those semantic types, is 
marked as a term corresponding to a medical problem. 
5.3.3.2. Semantic Processing 
Physicians often assert uncertain or negative diagnoses in narrative medical 
records (Rao et al., 2003); for example, to provide information that contrasts with the 
positive diagnoses (Kim & Park, 2006) or to keep track of all potential diagnoses that 
have been considered. Unless correctly identified, the presence of negative and uncertain 
assertions in the narrative of medical records can be confused with positive assertions and 
adversely affect automated system performance. Several research efforts focused on 
identifying and making use of uncertain and negative assertions in text. Mutalik et 
al.(2001) showed that the UMLS can be used to reliably detect negated concepts in 
medical narratives. Sibanda (Sibanda, 2006) extended NegEx (Chapman et al., 2001), by 
taking a rule-based approach, to identify not just positive, negative, and uncertain 
assertions, but also assertions made in reference to someone other than the patient. 
Named Extended NegEx, this system was developed on medical discharge summaries.  
 95 
We employed the source code of Extended NegEx to study the nature of the 
assertions in discharge summaries. We applied Extended NegEx to terms corresponding 
to medical problems, as identified above in Syntactic Processing. In order to distinguish 
the positive, negative, and uncertain assertions from one another in the attribute vector, 
we left the positive assertions unchanged, but transformed the negative and uncertain 
assertions in the following manner: assertions that were identified as negative were 
repeated within the narrative, but the medical problem term was pre-pended with “abs” 
(e.g., “Patient denies fever” becomes “Patient denies fever absfever”); assertions that 
were identified as uncertain were repeated in the narrative, but the repetition included the 
medical problem term pre-pended with “poss” (e.g., “possible pneumonia” becomes 
“possible pneumonia posspneumonia”). We call these transformed terms asserted 
medical problems. 
5.3.3.3. Surface Processing 
Our last data preparation step involved morphological analysis. In order to ensure 
that morphologically similar words were brought together as a single attribute, we 
converted the text transformed via semantic processing to lower case and applied 
stemming (Porter, 1980) (e.g., both “CHRONIC” and “chronically” were converted to 
“chronic”).  
5.3.4. Weka 
Stemmed lowercase words and asserted medical problems provide a good start to 
predicting whether a disease is present, absent, or questionable in the patient, or 
unmentioned in the discharge summary of the patient. However, each discharge summary 
 96 
can make multiple references to a medical problem. The polarity of the assertions made 
on each mention of the medical problem can be different, requiring us to further process 
these attributes to determine the most likely meaning of their combination and the 
implications for the presence of a disease in a patient based on that discharge summary. 
Classification provides a means for further processing these attributes. 
We performed classification using Weka (Witten & Frank, 2005), an open source 
collection of machine learning algorithms. We employed Weka version 3.5.5’s 
classification algorithms with stemmed lowercase words (i.e., we discarded any non-
alphabetic content) and asserted medical problems. For each of our 16 diseases, we 
identified the stemmed lowercase words and asserted medical problems relevant for 
classifying a disease from the training data for that disease. In order to eliminate attribute 
selection as a factor in system performance, we chose to use all of the thus identified 
attributes, including stop words, for a disease in classifying that disease. We treated all 
attributes for a disease in the same manner, i.e., we did not differentiate between the 
stemmed lowercase words and the asserted medical problems. The number of attributes 
used for each disease was more than 2400. 
We employed the identified attributes for each disease to build a term vector space 
model for data representation for that disease. Each identified attribute matched to a 
dimension of the term vector space. The dimensions of the term vector space determined 
the dimensions of attribute vectors of medical discharge summaries. The attribute vector 
of each discharge summary noted the presence or absence of each attribute within that 
medical discharge summary. This gave equal weight to all of the attributes regardless of 
the number of times each attribute appeared within the given medical discharge summary. 
 97 
We trained and 10-fold cross-validated the classifiers for each disease on the 
training set and therefore on the term vector space for that disease. We evaluated the 
classifiers for each disease on the test set for that disease, using the term vector space 
created from the training set for that disease. 
5.3.5. Specializing 
Specializing aims to improve classification of less well-represented classes while 
maintaining overall performance in multi-class classification. It takes three major steps to 
achieve this goal:  
(1) For each class in a classification task, specializing trains complementary 
classifiers in an OVA manner and forces them to learn to distinguish the items in that 
class from all of the items in all of the other classes (Ryan & Aldebaro, 2004). It selects a 
specialist classifier from these OVA-trained classifiers (see Figure 6a). (2) Specializing 
supplements the specialist classifiers with a catch-all classifier per classification task. The 
catch-all classifier for a task is a multi-class classifier and aims to capture the distribution 
of samples in all of the classes in the task (see Figure 7a). (3) Specializing combines the 
specialists classifiers and the catch-all classifier for the task in a manner that ensures due 
consideration to all of the classes in the data (see Figure 8a).  
For classifying obesity and 15 of its co-morbidities, we treat each disease 
independently of the others and apply specializing to each disease separately. We treat the 
classification of each disease as a multi-class classification task in which the disease is 
classified as being present, absent, or questionable in the patient, or unmentioned in the 
discharge summary of the patient. For each of the present, absent, questionable, and 
 98 
unmentioned classes of a disease, we train J48, Naïve Bayes, and AdaBoost.M1 
classifiers in an OVA manner and select from them a specialist per class (see Figure 6b). 
For each disease, we train J48, Naïve Bayes, and AdaBoost.M1 classifiers in multi-class 
manner and select from the trained classifiers a catch-all classifier for the disease (see 
Figure 7b). The specializing classifier for each disease combines the specialist classifiers 
for the classes of that disease with the catch-all classifier for that disease (see Figure 8b).  
5.3.5.1. Complementary Classifiers 
Specializing relies upon classifiers with complementary strengths. Each of the 
employed classifiers can be an individual classifier or a combined classifier. Specializing 
treats each classifier as a “black box”, selecting a trained classifier based solely upon its 
F-measure (see Section  5.3.6).  
For predicting the status of patients with respect to obesity and 15 of its 
co-morbidities, we based our specializing classifier on three commonly used multi-class 
classifiers: Naïve Bayes, C4.5, and AdaBoost.M1. The choice of these classifiers was 
motivated by their complementary strengths given the focus on learning less well-
represented classes. Naïve Bayes classifiers have been shown to be effective when the 
number of attributes exceeds the number of observations (Bickel & Levina, 2004). 
However, given the possible noise in these attributes, we complemented Naïve Bayes 
with a widely used C4.5 classifier (Weka’s J48) which tends to be robust to noisy data 
(Polat & Güneş, 2008). Finally, we added AdaBoost.M1 (Freund & Schapire, 1997), a 
multi-class implementation of boosting for combining many “weak learners” to learn the 
samples that are harder to classify (Freund, 1995; Schapire, 1990; Schapire & Singer, 
2000).  
 99 
Complementary   
OVA-trained  
classifiers 
 
Specialist for absent class 
of Gallstones 
Train 
 
J48 
Train 
 
AdaBoost.M1 
Select the OVA-trained classifier with the 
highest F-measure on absent, i.e., ‘N’. 
 
Map the labels of all samples to two classes: 
‘N’ (for absent) and ‘not N’ (for not absent). 
Training data 
for Gallstones 
Train 
Naïve 
Bayes 
Complementary   
OVA-trained  
classifiers 
 
Specialist for class ‘C’ 
for a task 
Train 
Classifier 
#1 
Select the OVA-trained classifier with 
the highest F-measure on class ‘C’. 
Map the labels of all samples to two 
classes: ‘C’ and ‘not C’ 
Training data 
for a task 
Train 
Classifier 
#2 
… 
Figure 6a. General Process for Selecting the 
Specialist Classifier for Class ‘C’ for a Task. 
Figure 6b. Applied Process for Selecting the 
Specialist Classifier for Class ‘N’ (absent) for 
the Disease Gallstones. 
 
Train 
Classifier 
#n 
Figure 6 - General and Applied Processes for Selecting Specialist Classifiers 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 6a shows the general process for selecting a specialist classifier. This process begins by 
mapping the labels of all samples to two classes. The two classes consist of those samples that are in a given 
class which we refer to as ‘C’, and all samples not in the given class which we refer to as ‘not C’. The 
specialist for class ‘C’ is selected from a set of complementary classifiers trained in a one-versus-all (OVA) 
manner on class ‘C’ and has the highest non-zero F-measure on class ‘C’. 
Figure 6b demonstrates the selection of a specialist for the absent class, class ‘N’, of gallstones. For 
gallstones, class ‘N’ consists of those records where gallstones is absent in the patient according to 
explicitly stated text in the narrative of the discharge summary. The class ‘not N’ consists of those records 
where gallstones is either present (Y) or questionable (Q) in the patient, or unmentioned (U) in the 
discharge summary of the patient. Class ‘not N’ is created by consolidating classes Y, Q, and U and 
transforming their labels to ‘not N’. The specialist for class ‘N’ of gallstones is selected from among C4.5 
decision trees (J48), Naïve Bayes, and AdaBoost.M1 classifiers trained in an OVA manner to separate class 
‘N’ from ‘not N’. The OVA-trained classifier with the highest non-zero F-measure is selected as the 
specialist classifier for class ‘N’ of gallstones. 
 
 100 
Train 
complementary   
multi-class  
classifiers 
Catch-all classifier for Gallstones 
Train 
 
J48 
Train 
 
AdaBoost.M1 
Select the trained multi-class classifier with 
the highest micro-averaged F-measure. 
Training data 
for Gallstones 
Train 
Naïve 
Bayes 
Train 
complementary   
multi-class  
classifiers 
 
Catch-all classifier for a task 
Train 
Classifier 
#1 
Training data 
for a task 
Train 
Classifier 
#2 
… 
Figure 7a. General Process for Selecting 
a Catch-all Classifier for a Task. 
Figure 7b. Applied Process for Selecting 
the Catch-all Classifier for the Disease 
Gallstones. 
 
Train 
Classifier 
#n 
Select the trained multi-class classifier with 
the highest micro-averaged F-measure. 
Figure 7 - General and Applied Processes for Selecting Catch-all Classifiers. 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 7a demonstrates the general process for selecting a catch-all classifier for a task. This process 
begins by training complementary multi class classifiers that can distinguish all classes for a task from one 
another. The multi-class classifier with the highest micro-averaged F-measure across all classes is selected 
as the catch-all classifier for the task. 
Figure 7b applies the process in Figure 7a to selecting a catch-all classifier for the disease gallstones. 
We find that the disease can be present (Y), absent (N), or questionable (Q) in the patient, or unmentioned 
(U) in the discharge summary of the patient. The catch-all classifier for gallstones is selected from among 
C4.5 decision trees (J48), Naïve Bayes, and AdaBoost.M1 classifiers trained in multi-class manner to 
distinguish Y, N, Q, and U classes of gallstones from one another. It is the single multi-class classifier with 
the highest micro-averaged F-measure across all classes for gallstones. 
 101 
 
Figure 8 - Specializing Classifiers for a General Task and for the Disease Gallstones. 
 
Figure 8a shows that the Specializing classifier for a general task consists of one specialist classifier per 
class (C1 through Cn) and a catch-all classifier for the task. Each specialist classifier has an F-measure > 0 
and either predicts the class it specializes on or fails to make any prediction. The specialist classifiers are 
combined in a sequential manner starting with the specialist for the least well-represented class, working up 
to the most well-represented class. For each data sample, the process continues until the sample is labeled or 
until all specialist classifiers have attempted and failed to classify the sample. The specialist classifiers are 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Classification 
No  
Label  
No  
Label  
No  
Label  
U 
N 
Y 
 
Record is 
Labeled as 
N 
 
Record is 
Labeled as 
Y 
 
Record is 
Labeled as 
U 
Record 
Label 
selected 
from N, Y, 
and U 
Test Record 
for Gallstones 
 
Specialist 
N 
Catch-all  
classifier for 
Gallstones 
 
Specialist 
U 
 
Specialist 
Y 
or 
or 
or 
Specializing 
Classifier for 
Gallstones 
Figure 8b. Applied Specializing 
Classifier for the Disease Gallstones. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Classification 
No  
Label  
No  
Label  
No  
Label  
Cn 
C1 
 
C2 
 
Record is 
Labeled as 
C1 
 
Record is 
Labeled as 
C2 
 
Record is 
Labeled as 
Cn 
Record 
Label 
selected 
from C1, 
C2, … , Cn 
Test Record 
for a task 
Catch-all  
classifier for  
a task 
or 
or 
or 
Specializing 
Classifier for a  
Task 
Figure 8a. Specializing Classifier for a 
Task. 
 
Specialist 
C1 
 
 
Specialist 
C2 
 
Specialist 
Cn 
 102 
then supplemented with a multi-class catch-all classifier. The catch-all classifier ensures that a class 
assignment is made to each sample. 
Figure 8b shows the Specializing classifier for gallstones. For this disease, the sample sizes for the classes 
in the training data are Y=109, N=4, Q=1, and U=615. Specializing classifier for gallstones combines 
specialists in order N, Y, and U, going from the least well-represented to the most well-represented class. 
The specialist for Q for gallstones had an F-measure = 0; therefore, it is omitted from the Specializing 
classifier. Starting with the specialist classifier for class ‘N’, given a data sample, either the specialist labels 
the sample with an ‘N’ and the process stops, or the process moves on to the next specialist classifier. The 
specialist classifiers are supplemented by the catch-all classifier for gallstones. The catch-all classifier for 
gallstones was trained on the same data used to train the specialist classifiers for this disease and for 
gallstones it can assign a Y, N, Q, or U to the sample. 
 
5.3.5.1.1. C4.5 (J48) Decision Tree 
Weka’s J48 decision tree classifier is an implementation of C4.5, which is a greedy, 
divide and conquer, statistical learning algorithm. J48 decision trees use the attributes of 
the data to recursively split the data into smaller subgroups (Quinlan, 1986, 1993). At 
each step, the best split, as determined by the gain ratio (Quinlan, 1986), is selected from 
all possible splits across all attributes. Gain ratio is based upon both information gain and 
split information. Information gain measures the change in information entropy (Shannon 
& Weaver, 1949) due to a given split. Split information measures the level of 
homogeneity (in terms of class distributions) of data in a split. Gain ratio normalizes 
information gain by the split information. We included J48 in our specializing classifier 
due to its robustness to noisy data. 
5.3.5.1.2. Naïve Bayes 
Naïve Bayes classifiers apply Bayes’ theorem, assuming a naïve (i.e., strong) 
independence among the attributes, i.e., that the conditional probability of an attribute 
given a class is independent of other attributes given the class. Naïve Bayes uses joint 
probabilities to estimate the likelihood that an item belongs to a specific class (Yang & 
Liu, 1999). Naïve Bayes has been shown to be effective when the number of attributes 
exceed the number of items in the training set (Bickel & Levina, 2004). 
 103 
5.3.5.1.3. AdaBoost.M1 
Boosting improves the accuracy of a classifier by iteratively training classifiers on 
subsets of the training data (Freund & Schapire, 1996). Among these classifiers, it retains 
those that perform marginally better than chance. The retained classifiers are combined 
and often show improved performance. We employed the multi-class Adaptive Boosting 
(AdaBoost.M1) implementation of this algorithm. AdaBoost.M1 weights the results from 
many binary decision stumps to build a multi-class classifier whose results typically 
improve over any of the decision stumps. AdaBoost.M1 complements J48 and Naïve 
Bayes due to its ability to learn the samples that are harder to classify.  
5.3.5.2. Specialist Classifiers 
Specializing identifies a specialist classifier per class by training complementary 
classifiers in an OVA manner. For each class, it focuses only on those OVA-trained 
classifiers that could correctly assign samples to that class. It selects from these 
successfully trained OVA classifiers the one with the highest F-measure on that class (see 
Section  5.3.6 for a discussion on F-measure) as the specialist classifier for that class (see 
Figure 6a). The specialist for a class can only make a binary decision and either assigns 
that class or fails to assign any class, i.e., the specialist for class A can predict either A or 
not A, where an assignment of A implies a definitive assignment of A but an assignment 
of not A implies that the definitive class is not known by the specialist for class A.  
For the task presented in this paper, the specialist classifiers are chosen from OVA-
trained J48, Naïve Bayes, and AdaBoost.M1 classifiers (see Figure 6b). On our training 
data, the F-measures of the specialist classifiers for all classes of all diseases had a mean 
of .82 and a standard deviation of .10.  
 104 
5.3.5.3. Catch-all Classifiers 
Catch-all classifiers supplement specialist classifiers. The catch-all classifier for a 
task is the multi-class classifier with the highest micro-averaged F-measure across all of 
the classes in that task. Unlike the specialist classifiers, the catch-all classifier can assign 
any of the classes in the task (see Figure 7a). For the task in this paper, the catch-all 
classifiers are chosen from multi-class-trained J48, Naïve Bayes, and AdaBoost.M1 
classifiers (Figure 7b). On our training data, the F-measures of the catch-all classifiers for 
all diseases had a mean of .91 and a standard deviation of .06. 
5.3.5.4. Specializing Classifiers 
Specializing combines the specialist and catch-all classifiers for a classification task 
by employing these classifiers in a strict order. Given already trained specialist and catch-
all classifiers for a task, to classify an unknown data item, specializing first calls the 
specialist classifiers in increasing order of class size (see Figure 8a). It starts by invoking 
the specialist classifier for the least well-represented class in the data. This specialist 
attempts its classification before the specialist classifier for the second least well-
represented class is invoked. The specialist for the second least well-represented class is 
invoked only if the item fails to receive a definitive class assignment from the specialist 
classifier for the least well-represented class. The specialist for the second least well-
represented class is followed by the specialist for the third least well-represented class, 
but only if the item fails to receive a definitive class assignment from the specialist for the 
second least well-represented class, etc. This process continues until the item receives a 
definitive class assignment or until all of the specialist classifiers have attempted and 
failed to classify the item. If no classification has been made by any of the specialist 
 105 
classifiers, specializing invokes the catch-all classifier, which then assigns a class to the 
item. This method of combining classifiers helps to ensure that the specializing classifier 
gives due consideration to all of the classes. 
Note that having the specialist classifiers run in a strict order and allowing each 
specialist to label only those samples that have not been labeled by the earlier specialists 
is a conflict resolution strategy that allows the specialists for the smaller classes to 
override the specialists for the larger classes. Similarly, all of the specialist classifiers 
override the catch-all classifier. We expect that having all classifiers run at the same time, 
rather than in a strict order, but then imposing a conflict resolution strategy that achieves 
the same prioritization would give similar results. However, our choice of imposing a 
strict order on the classifiers eliminates the need for an additional conflict resolution 
strategy.  
5.3.6. Evaluation Metrics and Methods 
We evaluated specializing using F-measures. We compare the specializing classifier 
to the three complementary J48, Naïve Bayes, and AdaBoost.M1 classifiers and three 
combined classifiers.  
5.3.6.1. Evaluation Metrics 
We evaluate the specializing classifier using macro- and micro-averaged F-measure 
(Sebastiani, 2002). Macro-averaged F-measure weights small classes as heavily as larger 
classes and would more clearly show the change in performance on the small, sparse, less 
well-represented classes. Micro-averaged F-measure gives equal weight to each sample, 
rather than each class, and gives a sense of overall performance on the complete data.  
 106 
Macro- and micro-averaged F-measures are often used as performance metrics in 
natural language processing tasks (Salton & McGill, 1983; Yang & Liu, 1999). They are 
computed from precision, recall, and F-measure which require counts for true positives 
(TP), false positives (FP), and false negatives (FN). Precision measures the percentage of 
the correct assignments made to each class (Eq. 1a). Recall measures the percentage of all 
items in a class that can be assigned (Eq. 1b). F-measure is the harmonic mean of 
precision and recall (Eq. 1c). Using ß, it can favor either precision or recall. In this paper, 
we give equal weight to precision and recall by setting ß = 1. 
Macro-averaged precision, recall, and F-measure (Eq. (3a-c)) take the arithmetic 
mean of the precision, recall or F-measure metrics of all classes. By dividing the sum of 
the F-measure over all classes by the number of classes, M, it gives equal weight to each 
class, regardless of its size. In contrast, micro-averaged metrics (Eq. (2a-c)) are computed 
over all samples in the data (Yang & Liu, 1999). When observed together, macro- and 
micro-averaged metrics give a more complete account of the strengths and weaknesses of 
systems. Given the focus of this paper on the less well-represented classes, we use macro-
averaged F-measure as our primary evaluation metric and study micro-averaged 
F-measure only to check overall performance on all samples. 
5.3.6.2. Significance Testing 
We tested the significance of performance differences between classifiers with 
Z-scores. We used a two-tailed test and a Z-value of ±1.645. The results that are 
significantly different from the specializing classifier are marked in bold in the tables in 
this paper. Following the examples of the Message Understanding Conferences 
(Grishman & Sundheim, 1996; Hirschman, 1998) and of the i2b2 Smoking Challenge 
 107 
Shared-Task (Uzuner et al., 2008) Shared-Task, we performed all of the significance tests 
in this paper at α = .10. 
5.3.6.3. Evaluation Methods 
We evaluated specializing by comparing the specializing classifier to the three 
complementary J48, Naïve Bayes, and AdaBoost.M1 classifiers. These three 
complementary classifiers lie at the heart of our implementation of specializing for multi-
class classification of obesity and its co-morbidities. To utilize them as baselines, we 
trained these classifiers as multi-class classifiers on the training set for each disease, 
separately on each disease, with the same exact term vector space and attribute vectors 
that were available to the specializing classifier for each disease. We refer to the resulting 
classifiers as the complementary baseline classifiers. Note that the complementary 
baselines for a disease are the same set of classifiers from which specializing selects a 
catch-all classifier for that disease. 
We also compared the specializing classifier to four additional combined baseline 
classifiers: voting, stacking, a specialist-only classifier, and a catch-all-only classifier. 
The combined baseline classifiers were also trained on each disease separately. For each 
disease, they utilized the training data, term vector space, and attribute vectors available 
to the specializing classifier for that disease. Voting, as implemented in this paper, 
combined the three complementary baseline classifiers, J48, Naïve Bayes, and 
AdaBoost.M1, with majority voting. Stacking combined the three complementary 
baseline classifiers with the conjunctive rule. The specialist-only classifier for each 
disease was the combination of the specialist classifiers chosen by the specializing 
classifier of that disease. The specialist-only classifier ran these specialist classifiers in 
 108 
the same strict order as the specializing classifier. The catch-all-only classifier was 
selected from the three complementary baseline classifiers for each disease. It was the 
complementary baseline classifier with the highest micro-averaged F-measure on the 
training set for that disease, i.e., it is the same as the catch-all classifier selected by the 
specializing method for that disease. 
We ran the specializing classifiers and the combined and complementary baselines 
on each disease separately. We trained each of the specializing and baseline classifiers on 
the training set for each disease, cross-validated them on the training set of each disease, 
and finally evaluated them on the test set for each disease. We report results only on the 
test sets. 
We compare the specializing classifier to the combined and complementary 
baselines in several ways. Although each of the classifiers under study is trained on 16 
independent multi-class classification tasks, to understand the overall strengths of these 
classifiers, we aggregate the performance of each of the classifiers over all classes of all 
diseases and obtain a global performance measure for each classifier on the task of 
classifying 16 diseases as present, absent, questionable, or unmentioned (Table 14). In 
order to understand the strength of each of the classifiers on each of the present, absent, 
questionable, and unmentioned classes, we measure performance on each of these classes. 
For this, we aggregate for each of the classes across the 16 diseases, but keep the classes 
themselves separate (see Table 15). We measure macro-average F-measure on each 
disease by aggregating the results over the present, absent, questionable, and unmentioned 
classes of each disease but keeping the diseases themselves separate (Table 16). Finally, 
we determine the contribution that the specialists and the catch-all classifiers each make 
 109 
to specializing. For each disease, we measure the number of samples classified and the 
percent correct by the specialists and the catch-all classifiers (Table 17). 
 
Table 14 - Aggregate Results for Classifying Obesity and Fifteen of its Co-morbidities by Combined 
and Complementary Classifiers 
  Micro-averaged Macro-averaged 
 Classifier Precision Recall F-measure Precision Recall F-measure 
 
Specializing 0.9230 0.9230 0.9230 0.6369 0.4972 0.5229 
Specialist-only 0.9276 0.9162 0.9218 0.6348 0.4879 0.5160 
 
Catch-all-only 0.9159 0.9159 0.9159 0.5330 0.4738 0.4863 
Voting 0.9175 0.9147 0.9161 0.9505 0.4484 0.4494 
Stacking 0.9183 0.9183 0.9183 0.9519 0.4495 0.4505 
 
J48 0.9070 0.9070 0.9070 0.5193 0.4846 0.4959 
Naïve Bayes 0.7291 0.7291 0.7291 0.6618 0.3604 0.3616 
AdaBoost.M1 0.9046 0.9046 0.9046 0.9484 0.4345 0.4402 
Bold indicates statistically significant difference from the Specializing Classifier. Specializing combines 
complementary classifiers trained in one-versus-all and multi-class manners in order to create one 
specializing classifier per disease. Specialist-only classifier for a disease invokes only the specialist 
classifiers for the present, absent, questionable, and unmentioned classes of the disease; it employs these 
specialist classifiers in the same strict order used by the specializing classifier. Catch-all-only, voting, and 
stacking combine the complementary classifiers trained in multi-class manner in various ways in order to 
create one catch-all-only, one voting, and one stacking classifier per disease. The complementary classifiers 
J48, Naïve Bayes, and AdaBoost.M1 are also evaluated on their own, after being trained in multi-class 
classification of each disease separately. Aggregate results report performance on all classes on the 
complete set of obesity and fifteen of its co-morbidities. 
 110 
 
Table 15 - Performance per Class, Aggregated Over All Diseases. 
a: Specializing 
 Y N Q U Totals  Precision Recall F-Measure 
Y 1891 3 7 296 2197 Y 0.8694 0.8607 0.8651 
N 34 11 0 20 65 N 0.7333 0.1692 0.2750 
Q 15 0 0 7 22 Q 0.0000 0.0000 0.0000 
U 235 1 3 5542 5781 U 0.9449 0.9587 0.9517 
Totals 2175 15 10 5865      
 
b: Specialist-only 
 Y N Q U Totals  Precision Recall F-Measure 
Y 1848 3 0 291 2197 Y 0.8787 0.8411 0.8595 
N 26 10 0 17 65 N 0.7143 0.1538 0.2532 
Q 15 0 0 7 22 Q 0.0000 0.0000 0.0000 
U 214 1 3 5531 5781 U 0.9461 0.9568 0.9514 
Totals 2103 14 3 5846      
 
c: Catch-all-only 
 Y N Q U Totals  Precision Recall F-Measure 
Y 1865 7 8 317 2197 Y 0.8579 0.8489 0.8534 
N 36 6 0 23 65 N 0.3333 0.0923 0.1446 
Q 15 0 0 7 22 Q 0.0000 0.0000 0.0000 
U 258 5 2 5516 5781 U 0.9408 0.9542 0.9474 
Totals 2174 18 10 5863      
 
d: Voting 
 Y N Q U Totals  Precision Recall F-Measure 
Y 1834 0 0 350 2197 Y 0.8663 0.8348 0.8503 
N 33 0 0 22 65 N 1.0000 0.0000 0.0000 
Q 14 0 0 8 22 Q 1.0000 0.0000 0.0000 
U 236 0 0 5543 5781 U 0.9358 0.9588 0.9472 
Totals 2117 0 0 5923      
 
e: Stacking 
 Y N Q U Totals  Precision Recall F-Measure 
Y 1832 0 0 365 2197 Y 0.8736 0.8339 0.8533 
N 43 0 0 22 65 N 1.0000 0.0000 0.0000 
Q 15 0 0 7 22 Q 1.0000 0.0000 0.0000 
U 207 0 0 5574 5781 U 0.9340 0.9642 0.9488 
Totals 2097 0 0 5968      
 
f: J48 
 Y N Q U Totals  Precision Recall F-Measure 
Y 1846 17 8 326 2197 Y 0.8349 0.8402 0.8376 
N 36 10 0 19 65 N 0.3030 0.1538 0.2041 
Q 15 0 0 7 22 Q 0.0000 0.0000 0.0000 
U 314 6 2 5459 5781 U 0.9394 0.9443 0.9419 
Totals 2211 33 10 5811      
 
 111 
g: Naïve Bayes 
 Y N Q U Totals  Precision Recall F-Measure 
Y 1342 3 0 852 2197 Y 0.5142 0.6108 0.5584 
N 20 3 0 42 65 N 0.3000 0.0462 0.0800 
Q 6 0 0 16 22 Q 1.0000 0.0000 0.0000 
U 1242 4 0 4535 5781 U 0.8329 0.7845 0.8079 
Totals 2610 10 0 5445      
 
h: AdaBoost.M1 
 Y N Q U Totals  Precision Recall F-Measure 
Y 1686 0 0 511 2197 Y 0.8818 0.7674 0.8206 
N 41 0 0 24 65 N 1.0000 0.0000 0.0000 
Q 14 0 0 8 22 Q 1.0000 0.0000 0.0000 
U 171 0 0 5610 5781 U 0.9118 0.9704 0.9402 
Totals 1912 0 0 6153      
Y=Present, N=Absent, Q=Questionable, and U=Unmentioned. Bold indicates significant difference from 
the Specializing Classifier. Note that one specializing, one voting, one stacking, and one of each of multi-
class complementary classifiers J48, Naïve Bayes, and AdaBoost.M1 is generated per disease. Each 
generated classifier can predict any of the classes Y, N, Q, or U. The results in this table are aggregated 
over all diseases in the data, e.g., the table for specializing (Table 15a) presents the aggregate performance 
for classes Y, N, Q, and U of specializing classifiers over all diseases. 
 112 
 
Table 16 - Macro-averaged F-measure by Disease. 
 
C
la
ss
es
 w
ith
 >
 0
 
sa
m
pl
es
 p
er
 
di
se
as
e 
(O
bs
er
ve
d 
C
ls
se
s)
 
Sp
ec
ia
liz
in
g 
Sp
ec
ia
liz
in
g-
on
ly
 
C
at
ch
-a
ll-
on
ly
 
Vo
tin
g 
St
ac
ki
ng
 
J4
8 
N
aï
ve
 B
ay
es
 
Ad
aB
oo
st
.M
1 
Asthma 4 (Y,N,Q,U) 0.4586 0.4486 0.4661 0.4637 0.4661 0.4565 0.3007 0.4661 
Atherosclerotic 
CV disease 4 (Y,N,Q,U) 0.4168 0.4140 0.3900 0.4055 0.4027 0.3900 0.4135 0.3619 
Heart failure 4 (Y,N,Q,U) 0.4360 0.4353 0.4280 0.4329 0.4346 0.4092 0.3774 0.4280 
Depression 2 (Y, U) 0.7150 0.7150 0.7150 0.7035 0.4618 0.7387 0.5161 0.7150 
Diabetes mellitus 4 (Y,N,Q,U) 0.5853 0.5813 0.5708 0.4464 0.4384 0.5708 0.3645 0.4152 
Gallstones 
/Cholecystectomy  3 (Y, N, U) 0.6122 0.6192 0.6100 0.5930 0.6054 0.6100 0.3709 0.5859 
GERD 4 (Y,N,Q,U) 0.4427 0.4427 0.4355 0.4111 0.4435 0.4355 0.2705 0.4105 
Gout 2 (Y, U) 0.9096 0.9085 0.9096 0.9096 0.9160 0.9160 0.5267 0.9096 
Hypercholester-
olemia 4 (Y,N,Q,U) 0.5393 0.4885 0.4843 0.4239 0.4094 0.4843 0.3799 0.4094 
Hypertension 4 (Y,N,Q,U) 0.6115 0.6115 0.4288 0.4139 0.4288 0.5515 0.2513 0.4288 
Hypertriglycer-
idemia 2 (Y, U) 0.4940 0.4940 0.4940 0.5864 0.4950 0.5784 0.4940 0.6627 
Osteoarthritis 2 (Y, U) 0.8813 0.8813 0.8813 0.8874 0.8813 0.8813 0.5662 0.8145 
Obesity 4 (Y,N,Q,U) 0.4718 0.4718 0.4718 0.4655 0.4718 0.4644 0.3248 0.4718 
Obstructive sleep 
apnea 3 (Y, Q, U) 0.6014 0.6014 0.6009 0.6078 0.6170 0.6009 0.3447 0.5765 
Peripheral 
vascular disease 2 (Y, U) 0.8452 0.8452 0.8452 0.8264 0.8641 0.8452 0.6616 0.7951 
Venous 
insufficiency 2(Y, U) 0.4945 0.4945 0.4945 0.6607 0.4950 0.6514 0.4945 0.6607 
Macro-averaged F-measure of combined and complementary baseline classifiers for each of obesity and its 
fifteen most frequent co-morbidities. One of each kind of classifier is trained per disease. Macro-averages 
per disease are computed over the classes that are observed in the data for each disease, e.g., Y, N, Q, and 
U. The classes observed for each disease are shown in column 2. Bold indicates significant performance 
difference from the Specializing Classifier
 113 
 
Table 17 - Assignments by Specialists and Catch-all Classifiers by Disease. 
  Samples Assigned 
Disease 
Specialists Catch-all Total 
Number 
Assigned 
# (%) 
Correct 
Number 
Assigned 
# (%) 
Correct 
Number 
Samples 
Asthma 496 475 (96%) 9 9 (100%) 505 
Atherosclerotic CV disease 454 385 (85%) 44 22 (50%) 498 
Heart failure 494 428 (87%) 5 3 (60%) 499 
Depression 507 448 (88%) 0 N/A 507 
Diabetes mellitus 481 442 (92%) 23 16 (70%) 504 
Gallstones/Cholecystectomy 504 484 (96%) 4 0 (0%) 508 
GERD 505 480 (95%) 0 N/A 505 
Gout 503 489 (97%) 3 1 (33%) 506 
Hypercholesterolemia 499 425 (85%) 4 4 (100%) 503 
Hypertension 505 455 (90%) 0 N/A 505 
Hypertriglyceridemia 508 496 (98%) 0 N/A 508 
Osteoarthritis 503 467 (93%) 0 N/A 503 
Obesity 494 464 (94%) 0 N/A 494 
Obstructive sleep apnea 497 479 (96%) 7 0 (0%) 504 
Peripheral vascular disease 508 475 (94%) 0 N/A 508 
Venous insufficiency 508 497 (98%) 0 N/A 508 
Total 7966 7395 (93%) 99 55 (56%) 8065 
Number of samples assigned, and the number and percent assigned correctly, by the specialists and catch-all 
classifiers for each of obesity and its fifteen most frequent co-morbidities. 
5.4. Results and Discussion 
The results in Tables 14-16 show that specializing can perform significantly better 
than the baselines in classifying obesity and 15 of its co-morbidities. Table 17 shows that 
the specialist classifiers are responsible for majority of the success of specializing. The 
performance gain of specializing comes, in part, from its ability to perform better on the 
less well-represented absent class. 
5.4.1. Aggregate Result Analysis 
Overall, specializing improved macro-averaged F-measure without adversely 
affecting micro-averaged F-measure. The results in Table 14 show that the specializing 
 114 
classifier demonstrated significant improvement in macro-averaged F-measure over all of 
the complementary baselines and all but one of the combined baselines. The same table 
shows that the specializing classifier demonstrated significant improvement in micro-
averaged F-measure over one of the combined baselines and all of the complementary 
baselines. In both macro- and micro-averaged F-measures, the specializing classifier and 
the specialist-only baseline were not significantly different from each other. In other 
words, the specialists were responsible for most of the success of specializing and the 
gain provided by the catch-all classifiers was not statistically significant. On the other 
hand, the specializing classifier was significantly different from the catch-all-only 
baseline in both macro- and micro-averaged F-measure. In other words, the catch-all 
classifiers gained significantly from the addition of specialists.  
Comparing the combined baseline classifiers to each other shows that the specialist-
only classifier gives the best macro and micro-averaged results. In other words, specialist-
only classifier gets more of the less well-represented classes correct over all diseases (as 
reflected by macro-averaged metrics) and it also gets the highest raw number of correct 
assignments (as reflected by micro-averaged metrics). The catch-all-only classifier has the 
second best macro-averaged results while the second best micro-averaged results come 
from stacking. 
Comparing the specialist-only classifier with the complementary baselines shows 
that the specialist-only classifier outperforms all of the complementary baselines. In other 
words, picking and applying the specialist classifiers in a strict order for each disease 
gives better performance than applying any single complementary baseline to all of the 
diseases. 
 115 
Similarly, comparing the catch-all-only baseline with the complementary baselines 
shows that the catch-all-only classifier outperforms all of the complementary baselines. In 
other words, picking and applying the best complementary baseline classifier for each 
disease gives better performance than applying any single complementary baseline to all 
of the diseases. 
Finally, Table 14 also reveals that the complementary baselines differ from each 
other in their performance. J48 is the strongest and Naïve Bayes is the weakest among 
them. Table 15 shows the performance of specializing and the baselines on the present, 
absent, questionable, and unmentioned classes in the complete test data. On the present 
and unmentioned classes, which contained the bulk of the judgments, all of the 
complementary baseline classifiers correctly predicted a majority of the items. The ability 
to correctly assign test records to the present and unmentioned classes shows that the 
training set provided sufficient informative samples for all three complementary baseline 
classifiers to train on for these two classes. As a result, all of the combined baseline 
classifiers performed well and were able to make correct predictions for these two 
classes. The specializing classifiers’ F-measures did not differ significantly from the 
F-measures of the combined baselines on these classes. 
The less well-represented questionable class showed the opposite of the 
observations from the present and unmentioned classes. An examination of the results 
revealed that neither specializing nor any of the baselines was able to correctly classify 
discharge summaries judged as questionable. The questionable class was the least well-
represented class, with just 39 out of 11,630 instances in the training data (see Table 1). 
None of the complementary baseline classifiers were able to correctly classify any of the 
 116 
22 discharge summaries marked as questionable in the test data (see Table 15). Since 
each of the combined baseline classifiers relied upon the accuracy of the complementary 
classifiers, none of the combined baselines were able to correctly classify the questionable 
class. We attribute the results on the questionable class to the lack of a sufficient number 
of informative samples for this class in the training set. In Table 1, for example, we 
observe that for two of the diseases, Heart failure and Hypertension, there were no 
training samples for the questionable class even though those diseases were associated 
with five of the 22 discharge summaries judged as questionable in the test set. 
The second least well-represented class, the absent class, revealed the difference in 
the strengths of the various classifiers. The absent class consisted of just 87 of the 11,630 
judgments in the training data (see Table 1). Both the J48 and Naïve Bayes 
complementary baseline classifiers made some correct predictions (10 and 3 true 
positives, respectively) for this class. This, in turn, enabled the specializing, specialist-
only, and catch-all-only classifiers to correctly predict the judgments, at least some of the 
time, for this class. 
On the absent class, the specializing classifier showed significant improvement in 
F-measure over three of the combined baselines (see Table 15). Its difference from the 
specialist-only baseline was not statistically significant, i.e., most of specializing’s gain 
on this class comes from its specialists. The catch-all-only baseline was successful in 
recognizing the absent class part of the time. The fact that the catch-all-only classifier is 
able to get some of these samples correct implies that the complementary baseline 
classifiers that constitute the catch-all classifiers can capture this information. 
 117 
On the other hand, voting and stacking cannot directly benefit from the catch-all 
classifiers (i.e., the best complementary classifier per disease) in predicting the absent 
class. This is because the combination of the catch-all classifier for a disease with the rest 
of the complementary baseline classifiers for that disease, through either voting or 
stacking, overrides the informative catch-all classifier for the disease with the incorrect 
predictions provided by the remaining complementary baseline classifiers for that disease. 
The catch-all-only classifiers apply the catch-all classifier for each disease directly (rather 
than combining it with the rest of the complementary baseline classifiers) and accept the 
catch-all classifier as the authority on the items that the catch-all classifier gets to label. 
This contributes to the catch-all-only classifiers’ gain in performance over voting and 
stacking on the absent class.  
5.4.2. Disease-Level Results Analysis 
Given the performance of the classifiers on the overall task of classifying diseases, 
we compared the classifiers at the disease level for each of the 16 diseases (see Table 16). 
For this, we obtained aggregate results over the present, absent, questionable, and 
unmentioned classes of each disease. Note that the number of classes containing samples, 
i.e., the number of classes that were actually observed for each disease, varied from 
disease to disease in our data. For those diseases that had samples in more than two 
classes, i.e., were in fact multi-class classification problems, we found the specializing 
classifier performed no worse than, and at times showed significant improvement over, 
all of the baselines. 
 118 
For three of the diseases, Diabetes mellitus, Hypercholesterolemia, and 
Hypertension, the specializing classifier outperformed at least four of the seven baseline 
classifiers. These three diseases can be distinguished from the remaining diseases by 
virtue of the number of classes observed in their training data and by the rate of true 
positives predicted by the complementary classifiers. Specifically, these three diseases all 
have multi-class judgments and at least one of the complementary baseline classifiers’ 
recall was above .10 for all of the classes, excluding the questionable class. The 
remaining thirteen diseases fail to meet these criteria.  
In other words, the specializing classifier showed significant performance 
improvement over, or performed no worse than, the combined baselines on the diseases 
that contained at least three classes to predict, as long as those classes could reliably be 
predicted by at least one of the complementary classifiers. 
Most of the success of specializing at the disease level was accounted for by the 
specialists. As detailed by disease in Table 17, for eight of the 16 diseases, all of the 
samples in the test set were classified by the specialist classifiers, i.e., the catch-all 
classifier was not invoked. For six of the remaining eight diseases, less than 2% of the 
samples invoked the catch-all classifier. An examination of those records not classified by 
the specialists indicates that they may be difficult to classify. For example, for record 
1020 the ground truth listed both Asthma and Diabetes as being present in the patient, but 
the specialists for both Asthma and Diabetes did not classify this record. While the text 
‘‘asthma” was present in record 1020, other indicators of asthma as identified by the 
specialist classifier, such as ‘‘inh” (as in ‘‘2 puff inh”), were missing from this record. 
 119 
Similarly, while the text ‘‘diabetes” appeared in record 1020, other indicators of diabetes 
as identified by the specialist classifier, such as ‘‘insulin”, were missing from this record.  
5.4.3. Specializing analysis and discussion 
All of our results indicate that the specialists are responsible for most of the success 
of the specializing classifiers. Our analysis of the data samples in the test set showed that 
the contribution of the catch-all classifiers to specializing was limited by the number of 
samples passed on to the catch-alls by specializing. As a component of specializing, the 
catch-all classifiers predicted only those samples that failed to receive a definitive class 
assignment from any of the specialists for a disease, i.e., those samples that could not be 
classified by any of the specialist classifiers for a disease. Table 17 shows that only 99 of 
the 8065 test samples over all diseases were passed on to the catch-all classifiers by 
specializing.  
For five of the 16 diseases, the training data contained only two classes, present and 
unmentioned, with which to train specialist classifiers (see Table 1). The specialists 
trained under these conditions were complementary and exhaustive in the classification of 
their disease, i.e., for any given sample, exactly one of the two specialists predicted a 
class. This left no samples for the catch-all classifiers for these diseases. 
While the contribution of the catch-all classifiers in specializing is limited, the 
catch-all classifiers themselves are not weak (see Table 14); they can even recognize 
samples from the less well-represented absent class (see Table 15). What is more, the 
catch-all classifiers can complement the specialist classifiers on some data samples, e.g., 
in record 1020, they can correctly classify samples passed over by the specialists with 
 120 
respect to asthma and diabetes. A difference in the relative employment order of the 
specialists and the catch-all could change the number of samples classified by the 
catch-all. 
Extrapolating this hypothesis to the specialists, we checked if a change in the order 
of the specialists could affect overall performance. For each of the diseases, we examined 
the specialists in a pair-wise manner (e.g., present-absent, absent-unmentioned). We 
compared the predictions that would have been made by each of the specialists in the 
pair, had they each been asked to predict the sample. In the vast majority of the cases, no 
more than one of the specialists made a definitive prediction for a given sample. Across 
16 diseases, we found only 80 instances where both of the specialists would have made 
definitive, and therefore conflicting, predictions. In order to see the impact of those 80 
instances upon classification, we examined all of the combinations for ordering the 
specialists. We found that, for all 16 diseases, the order specified by specializing was 
either significantly better than or not significantly different from any other order of the 
specialists. Each case where a significant difference was found involved the placement of 
the specialist for the less well-represented absent class. For example, for the disease 
Hypertension, any combination that invoked the absent class before the present class 
performed significantly better than those combinations that invoked the present class 
before the absent class. 
5.5. Limitations and future work 
Specializing treats the classification of each disease as an independent multi-class 
classification task. For each disease, only one class is assigned to a record, i.e., the choice 
 121 
of one class excludes the remaining classes. Additional study will be necessary to apply 
specializing to tasks that require assignment of multiple labels to each data item. 
Specializing utilizes the best OVA-trained classifier for each class given the 
decisions already made by the specialists for the less well-represented classes. While we 
have found that specializing’s combination of specialists provides the best performance 
from among all combinations of specialists, there may exist other combinations of OVA-
trained classifiers that give suboptimal performance on some classes for the sake of 
achieving globally optimal results on the complete data. 
While our results are encouraging, future work should look into ways of improving 
the performances of the catch-all classifiers and of specializing. We hypothesize that the 
performance of catch-all classifiers can be improved by training the catch-all classifiers 
on the samples that could not have been correctly classified by the specialists (currently, 
specializing trains the catch-all classifiers on the complete training data). We expect that 
the samples that fail to be classified by the specialists are harder to classify and training 
catch-all classifiers on these samples could improve their contribution to the specialists. 
Alternatively, the catch-all classifiers could be replaced by more OVA-trained classifiers, 
e.g., the OVA-trained classifiers that performed second best and were not selected as 
specialists but demonstrated potential. To this end, including a threshold for minimum 
required performance level from the specialists and from any of the OVA-trained 
classifiers may further contribute to performance. These hypotheses remain untested 
mostly due to limitations of the data set. A review of the quantity and distribution of the 
samples that were left for the catch-all classifiers indicated the lack of a sufficient number 
 122 
of samples that could satisfactorily and reliably be used as training data for the 
experiments required to test these hypotheses. 
Finally, future work should study the generalizability of specializing, by applying 
specializing to corpora other than medical discharge summaries and to corpora with more 
than four classes, possibly with alternative sets of complementary classifiers.  
5.6. Accessibility of Data 
We have presented and studied specializing on a data set generated for the i2b2 
Shared-Task and Workshop on Challenges in Natural Language Processing for Clinical 
Data: Obesity Challenge (Informatics for Integrating Biology and the Bedside, 2008). 
This data has been made available to the research community from i2b2.org/NLP under a 
data use agreement, for studies that relate to the i2b2 Shared-Task. 
With few exceptions, such as the data released by the University of Cincinnati 
Computational Medicine Center (Pestian et al., 2007), the medical language processing 
community is haunted by the lack of a set of publicly available records that have been 
annotated for the gold standard, that can serve as a test bed for the development of 
competitive or complementary systems, and that can help replicate past work and advance 
the state of the art. This limitation stems from legal, proprietary, and privacy concerns 
("Standards for Privacy of Individually Identifiable Health Information," 2000) that make 
the patient records available only to limited audiences and under strict confidentiality 
agreements.  
Although currently it is not a substitute for a public domain data set that would offer 
greater benefits to the research community, the data set used in this study takes a step 
 123 
towards remedying this problem. While we appreciate the need for a public domain data 
set and aspire to help create it, until we obtain the institutional approvals that would make 
this aspiration a reality, we invite the research community to participate in future shared-
tasks that would grant them access to similar data. We also wholeheartedly support 
complementary activities that would bring about public domain medical record data sets 
for use by the research community. 
5.7. Conclusions 
We examine the performance of specializing based on its ability to predict whether 
obesity and each of 15 of its co-morbidities are present, absent, or questionable in a 
patient, or unmentioned in the patient discharge summary. For each disease, specializing 
combines multi-class classifiers trained in an OVA manner with a catch-all classifier that 
is trained as a multi-class classifier that can distinguish all classes from one another. The 
specializing approach to combining classifiers is an effective way to improve macro-
averaged performance in multi-class classification of diseases in the presence of less 
well-represented classes with a sufficient minimum number of samples to learn from 
based on medical discharge summaries. This improvement is accomplished without 
sacrificing overall micro-averaged performance. 
5.8. Acknowledgements 
This work was supported in part by the National Institutes of Health through 
research grants 1 RO1 EB001659 from the National Institute of Biomedical Imaging and 
Bioengineering and through the NIH Roadmap for Medical Research, Grant 
U54LM008748. 
 124 
We gratefully acknowledge Chen Song for his tireless programming assistance, and 
Peter Szolovits and the two anonymous reviewers of the Journal of Biomedical 
Informatics for their constructive and insightful comments.  
 125 
Chapter 6. Co-Specializing: Addressing the Scarcity of Labeled Data 
6.1. Introduction 
Given the explosion of computerized data, it is an almost trivial task to simply 
create data sets. However, annotating a data set is a manual task requiring specific 
knowledge about the information contained in the data set (Zhou, 2009). The cost 
associated with the labor of annotating a data set is high and resources are often limited 
(Medelyan & Witten, 2008; Takahashi et al., 2005; Zhong, 2005). This cost can be 
reduced through the use of semi-supervised machine learning. Semi-supervised methods 
of machine learning (Blum & Mitchell, 1998; Mitchell, 1999; Nigam et al., 2000) utilize 
unlabeled data to improve the results from supervised machine learning that has been 
trained with a small labeled training set. Semi-supervised machine learning predicts class 
labels based upon the set of classes found in the labeled data.  
Co-training is an iterative semi-supervised learning method of supplementing 
manual annotations formalized by Blum and Mitchell (1998). Blum and Mitchell applied 
co-training to classifying web pages. Co-training has also been successfully applied to a 
wide array of tasks, from textual data, classifying e-mail (J. Chan et al., 2004) and 
performing named entity recognition of biomedical text (Cai & Cheng, 2009), to 
non-textual data, such as CT images detection (M.-Z. Guo, Deng, Liu, & Li, 2011) and 
vehicle classification based upon visual and acoustic information (Godec, Leistner, 
Bischof, Starzacher, & Rinner, 2010).  
In addition to the problems related to acquiring labeled data, statistical classifiers 
also favor the more well-represented classes when trained on imbalanced data sets (Wang 
 126 
& Japkowicz, 2008). This suboptimal classification performance of the less well-
represented classes occurs as a result of maximizing overall performance. However, while 
difficult to classify, the less well-represented classes are often the more interesting classes 
(Kotsiantis & Pintelas, 2009) and it is important to classify them correctly. 
This chapter combines the approaches of co-training and specializing, a panel of 
one-versus-all (OVA) classifiers described in Chapter 5, to create Co-Specializing 
classifiers. The co-training approach helped to address the scarcity of labeled data. The 
specializing classification method helped to address the problems associated with 
imbalance of classes in the data sets. We show that the combination of the two methods 
addressed both the issue of limited annotation resources and the issue of data imbalance 
in a document level multi-class classification task based on information in medical 
discharge summaries. The attributes employed were the individual words and asserted 
standardized preferred terms of the disease noun phrases found in the narrative of patient 
medical records from both the i2b2 Obesity and Smoking data sets. The attributes were 
randomly split into two artificial views of the data. The system used the Weka (Witten & 
Frank, 2005) collection of machine learning algorithms to train the learners. We 
determined that co-specializing classifiers typically outperformed J48, Naïve Bayes, and 
Support Vector Machine (SVM) classifiers on the less well-represented classes in a 
document level multi-class classification task. In addition the co-specializing classifiers 
typically outperformed J48, Naïve Bayes, and SVM based co-training classifiers on the 
less well-represented classes. 
 127 
6.2. Co-Training 
As described by Blum and Mitchell (1998), co-training employs two classifiers, 
each making use of a different view of the data. The classifiers are incrementally cross 
trained on a small set of labeled samples and on the (initially) unlabeled samples 
predicted by the other classifier. Co-training assumes redundantly sufficient and 
independent views of the data (i.e., each set of features is sufficient for training a learner 
and is conditionally independent given the class). Each classifier is trained on one of the 
two views of the data. During the initial iteration, the classifiers are trained on the small 
labeled samples and predict labels for the unlabeled samples. The sample for which each 
classifier has the highest confidence in its prediction is added to the other classifier’s 
labeled samples. These additional samples are then treated in the same manner as the 
small set of manually labeled samples. Each classifier then retrains on the expanded set of 
labeled samples and makes predictions on the remaining unlabeled samples. 
Blum and Mitchell’s original experiments were with web pages. They pointed out 
that web pages have a naturally occurring pair of independent, non-overlapping views of 
the data. The first view was the text contained on the web page. The second view was the 
text associated with the hyperlinks pointing to the web page from other web pages. They 
maintained that if the two views of the data were redundantly sufficient (i.e., that either 
view of the data could be used to perform classification) and were conditionally 
independent given the class, then two learners could be trained separately on each view 
and the predictions from each trained learner could be used to expand the other learner’s 
training set. 
 128 
J. Chan et al. (2004), while examining the classification of e-mail, noted that most 
data sets do not have two naturally occurring views of the data. They examined 
co-training using the subject line and body as the pair of naturally occurring views of the 
data. They then combined the subject line and body views, and randomly split the 
combination into two artificial views of the data. They examined co-training using the 
two artificial views of the data and compared the results to the systems using the naturally 
occurring views of the data. They found that the performance of the systems using the 
random split were as good as, or better than, the performance of the systems using the 
naturally occurring views of the data. In this study, we also use an artificial split of the 
data. The individual words and asserted preferred terms in the narrative of medical 
records were randomly split, providing two artificial views of the data.  
6.3. Co-Specializing Classifier 
This chapter reports on a method that improved classification of less well-
represented classes in imbalanced multi-class data sets when presented with a small set of 
labeled samples and a large set of unlabeled samples. We combine the approaches of 
specializing (see Chapter 5) and co-training to create Co-Specializing classifiers. The 
co-specializing classifiers were trained and tested on both the i2b2 Obesity and Smoking 
data sets, two multi-class data sets. These data sets were used to predict the status of 
patients with respect to diseases and smoking. 
To predict the status of patients with respect to diseases and smoking, this study 
based the co-specializing classifier on three commonly used multi-class classifiers: C4.5, 
Naïve Bayes, and SVM (see pages 29-30). The choice of these classifiers was motivated 
 129 
by their complementary strengths given the focus on learning less well-represented 
classes. We refer to these three classifiers collectively as the set of complementary 
classifiers. Naïve Bayes classifiers have been shown to be effective when the number of 
attributes exceeds the number of observations (Bickel & Levina, 2004), as will be the 
case in early iterations of co-training. However, given the possible noise in these 
attributes, this study proposes to complement Naïve Bayes with a widely used C4.5 
classifier (Weka’s J48), which tends to be robust to noisy data (Polat & Güneş, 2009). 
Finally, this study added Support Vector Machines, implemented with libSVM (Chang & 
Lin, 2001), as the third component of the complementary classifiers. Given the high 
dimensionality of the feature sets, SVM complements Naïve Bayes and J48 by its ability 
to change nonlinearly separable data into linearly separable data by mapping the data to a 
higher dimensional space. 
Co-specializing employs two panels of classifiers, each panel making use of a 
different view of the data. The classifiers are incrementally cross trained on a small set of 
labeled samples and on the (initially) unlabeled samples predicted by the other panel of 
classifiers. For a given view of the data, for classes of each of the diseases and of 
smoking status in each of the data sets, the system trained J48, Naïve Bayes, and SVM 
classifiers in an OVA manner and, from them, selected one specialist per class. For a 
given view of the data, for each disease and for smoking status in each of the data sets, 
the system trained J48, Naïve Bayes, and SVM classifiers in a multi-class manner and 
selected from the trained classifiers a catch-all classifier for each disease and for smoking 
status.  
 130 
The specializing approach allowed a single classifier to make an assignment for a 
specific class. It combined classifiers in a sequential manner in order to arrive at a 
definitive class assignment for a sample. Sequential activation of classifiers eliminated 
the need for an additional conflict resolution strategy. Co-specializing began with the 
specializing method of activating a panel of specialist classifiers in a strict order. 
However, rather than activating the panel for a given unlabeled sample, predicting the 
label and moving to the next unlabeled sample, co-specializing activated the specialist for 
the least well-represented class, which attempted to predict labels for all of the unlabeled 
samples. The system selected the unlabeled sample for which the specialist classifier had 
the highest confidence. If the first specialist failed to predict a label for any of the 
unlabeled samples, co-specializing activated the specialist for the next least well-
represented class and used it to attempt to predict a label for all of the unlabeled samples. 
This strict order was followed for each of the specialist classifiers in succession and, if 
necessary, ultimately for the catch-all classifier, until a label was predicted for at least one 
of the unlabeled samples. This predicted sample was added to the other panel of 
classifiers’ labeled samples. The additional sample was then treated in the same manner 
as the small set of manually labeled samples. The process continued, alternating between 
each of the two panels of classifiers, until all of the unlabeled samples had been 
predicted. 
The specializing classification method helps to address the imbalance of classes in 
the data sets. When combined with co-training, we show that the resultant co-specializing 
classifiers will consistently improve recall on the less well-represented classes even when 
trained with a small number of labeled samples. 
 131 
6.4. Data  
This study employed the i2b2 Obesity and Smoking data sets. The i2b2 Obesity data 
set was developed for the 2006 Shared-Task and Workshop on Challenges in Natural 
Language Processing for Clinical Data: Smoking Challenge (Uzuner et al., 2008). The 
i2b2 Smoking data set was released as part of the 2008 Shared-Task and Workshop on 
Challenges in Natural Language Processing for Clinical Data: Obesity Challenge 
(Informatics for Integrating Biology and the Bedside, 2008; Uzuner, 2008). 
The task for this study was document level multi-class classification based on the 
narratives of patient medical records. However, exemplars did not exist for some of the 
classes for several of the diseases in the i2b2 Obesity training data set (see Table 1). 
Therefore, since this was not a binary classification task, this study used only those six 
diseases that had exemplars in all four of the classes (see Table 18). The system treated 
each disease independently of the others and applied the machine learning classifiers to 
each disease separately. For the Obesity data set, the system treated the classification of 
each disease as a document level multi-class classification task in which the disease was 
classified as being present, absent, or questionable in the patient, or unmentioned in the 
discharge summary of the patient. For the Smoking data set, the system treated the 
smoking status of the patient as a document level multi-class classification task in which 
the status was classified as current smoker, non-smoker, past smoker, smoker, or 
unknown. 
 132 
6.4.1. Feature Extraction 
The study classified diseases and smoking status based on the narratives of patient 
medical reports and relied only on a basic set of attributes. These attributes consisted of 
stemmed lowercase words, noun phrases that either directly or indirectly reference 
diseases (e.g., disease names, symptoms, tests, findings) in the medical reports, and the 
polarity (i.e., positive, uncertain, or negative) of the assertions made on standardized 
preferred terms of the disease noun phrases. 
In order to extract these attributes from the patient medical reports, before assigning 
a class, the co-specializing classification system syntactically processed the records with 
NPDP (see section  4.4.4) to detect noun phrases related to diseases and to provide 
standardized preferred terms for the noun phrases. The narrative of the medical reports 
was expanded with the preferred term for each detected noun phrase. The system then 
employed ConText (Harkema et al., 2009) to semantically process the medical reports to 
identify the polarity of the assertions made for each instance of each preferred term. The 
system then transformed the text to distinguish negative and uncertain assertions from 
positive assertions. Lastly, the system converted all of the text to lower case and applied 
stemming, in order to conflate variations of words.  
The system performed semantic processing of the narrative of the medical reports. 
NPDP processed each of the records and expanded the text of the narrative with preferred 
terms of noun phrases related to diseases by repeating the noun phrase and substituting 
the preferred term for the noun phrase. The system then marked the polarity of the 
assertions made on each of the preferred terms. Physicians frequently assert uncertain or 
negative diagnoses in the narrative of medical reports (Rao et al., 2003), either to provide 
 133 
contrasting information for the positive diagnoses (Kim & Park, 2006) or to monitor all 
potential diagnoses that have been taken into account. Unless properly identified, 
negative and uncertain assertions in the narrative of medical reports could have been 
confused with positive assertions and therefore adversely affect automated system 
performance. In order to identify the polarity of the assertion, the system applied ConText 
to sentences containing noun phrases that either directly or indirectly reference diseases. 
Following the process described in section  4.4.4.2, the system repeated each sentence 
containing an identified noun phrase, substituting the preferred term for the noun phrase 
in the sentence and, in order to distinguish the positive, negative, and uncertain assertions 
from one another, modified the assertions as follows:  
1. Repeated noun phrases containing assertions identified as positive assertions 
were unchanged. 
2. Repeated noun phrases containing assertions identified as negative were 
modified with the preferred term pre-pended with “abs” (e.g., “Patient 
denies coughing.” resulted in “Patient denies coughing abscoughing.”). 
3. Repeated noun phrases containing assertions identified as uncertain were 
modified with the preferred term pre-pended with “poss” (e.g., “Possible 
pneumonia.” resulted in “Possible pneumonia posspneumonia.”).  
We refer to these transformed terms as asserted preferred terms. 
The final pre-processing step involved morphological analysis via surface 
processing of the text. In order to ensure that morphologically similar words were brought 
together, the system converted all text (i.e., both the original text and the text transformed 
 134 
via semantic processing) to lower case and applied stemming (e.g., both 
“EXTRACTION” and “extracted” were converted to “extract”). 
6.4.2. Seed, Training, and Test Sets 
Each of the Obesity and Smoking data sets were released with both training and test 
sets. To simulate the scarcity of available labeled data, we created a subset of the training 
data set for each disease and for smoking status. We refer to these training subsets as the 
seed sets. The creation of each seed set was a two-step process. First, working up from 
the end of the training set, the last five records (when available) for each class were 
selected for inclusion in the seed set. This step insured at least a minimal representation 
in the seed set of each of the classes. Then, working circularly from the beginning of the 
training set, we selected every seventeenth record until the seed set contained a total of 65 
records. This step enabled the distribution of the classes in the seed set to approximate the 
distribution of the classes of the training set. Tables 18 and 19 show the number of 
samples in each class in the seed, training, and test sets in each of the Obesity and 
Smoking data sets. We developed on the training set, trained on the seed set, and tested 
on the test set.  
 
 
 
 
 
 
 135 
Table 18 - Obesity Ground Truth. 
Disease Data Set Y N Q U Total 
 Seed 7 3 2 53 65 
Asthma Training 93 3 2 630 728 
 Test 68 2 2 432 504 
Atherosclerotic CV 
Disease 
Seed 35 5 6 19 65 
Training 399 23 7 292 721 
Test 277 22 2 196 497 
 Seed 30 5 5 25 65 
Diabetes Training 485 15 7 219 726 
 Test 338 12 3 150 503 
 Seed 13 1 5 46 65 
GERD Training 118 1 5 599 723 
 Test 69 1 1 433 504 
 Seed 29 5 1 30 65 
Hypercholesterolemia Training 304 13 1 408 726 
 Test 213 6 4 279 502 
 Seed 22 4 4 35 65 
Obesity Training 298 4 4 424 730 
 Test 198 3 3 289 493 
Number of samples in each class in the seed, training, and test sets in six diseases from the i2b2 Shared-
Task and Workshop on Challenges in Natural Language Processing for Clinical Data: Obesity Challenge. 
Y, present; N, absent; Q, questionable; and U, unmentioned. 
 
Table 19 - Smoking Ground Truth. 
 Data Set Current 
Smoker 
Non-
Smoker 
Past 
Smoker Smoker Unknown Total 
 Seed 6 11 5 5 38 65 
Smoking Training 35 66 36 9 252 398 
 Test 11 16 11 3 63 104 
Number of samples in each class in the seed, training, and test sets in the data from the i2b2 Shared-Task 
and Workshop on Challenges in Natural Language Processing for Clinical Data: Smoking Challenge. 
 
6.4.3. Data Views 
The attributes for this study consisted of stemmed lowercase words and asserted 
standardized preferred terms of the disease noun phrases found in the narrative of patient 
medical records. The attributes were split to provide two views of the data, one view for 
each of a pair of co-training classifiers.  
 136 
The initial data views were derived from the attributes of the records in the seed 
sets. The system sequentially examined each of the attributes in each of the records in the 
seed set, and alternatingly assigned each attribute to one of the two data views. We 
employed the following strategy to maintain two independent views of the data. If the 
attribute was already a member of one of the two data views, the system moved on to the 
next attribute. If the attribute was an asserted preferred term, and if a related asserted 
preferred term was already a member of one of the two data views, the attribute was 
added to the data view containing the related asserted preferred term. Related asserted 
preferred terms differed from each other only by the presence or absence of the abs and 
poss prefixes. For example, pneumonia and posspneumonia would be related asserted 
preferred terms. This process continued until all of the records in the seed set were 
processed. 
As predicted samples were added to a classifier’s labeled samples, each attribute 
from the new sample was examined for inclusion in that classifier’s data view. If the 
attribute, or a related asserted preferred term of the attribute, was already a member of the 
other classifier’s data view, the system moved on to the next attribute. Otherwise, the 
attribute was added to its classifier’s data view. This strategy was used to maintain two 
independent views of the data. 
To insure that each view of the data was sufficient to train a learner, we trained each 
of the base classifiers as multi-class classifiers on the full training set, twice on each of 
the six Obesity diseases and Smoking training sets, once with each of the two views. We 
tested each of the trained classifiers on the associated test set. We reported each base 
classifier’s micro-averaged F-Measure in Table 20 and determined that for each disease 
 137 
and smoking status, each view was sufficient to successfully train at least two of the three 
learners. 
Table 20 - Base Classifier F-Measure for each disease and smoking data set on each of the data views. 
 J48 Naïve Bayes SVM 
 View 1 View 2 View 1 View 2 View 1 View 2 
Asthma 0.974 0.825 0.831 0.750 0.919 0.851 
Atherosclerotic CV Disease 0.869 0.734 0.751 0.726 0.831 0.795 
Diabetes 0.843 0.926 0.751 0.779 0.789 0.905 
GERD 0.968 0.980 0.680 0.664 0.911 0.915 
Hypercholesterolemia 0.769 0.707 0.663 0.633 0.751 0.649 
Obesity 0.966 0.645 0.724 0.625 0.937 0.615 
Smoking 0.798 0.567 0.577 0.548 0.673 0.096 
6.5. System Development 
We trained learners using Weka (Witten & Frank, 2005), an open source collection 
of machine learning algorithms. The systems employed Weka’s classification algorithms 
with stemmed lowercase words (i.e., the system discarded any non-alphabetic content) 
and asserted preferred terms as attributes. The system treated all attributes in the same 
manner, i.e., it did not differentiate between attributes containing stemmed lowercase 
words and those containing asserted preferred terms.  
The systems employed the attributes in each of the data sets to build a term vector 
space model for data representation of that disease and smoking status. Each identified 
attribute was matched to a dimension of the term vector space. The attribute vector of 
each medical record noted the presence or absence of each attribute within that record. 
This gave equal weight to all of the attributes regardless of the number of times an 
attribute appeared within a given medical record. 
We trained classifiers for each disease and for smoking status on the seed set. We 
evaluated the classifiers for each disease on the test set for that disease or for smoking 
status. 
 138 
6.6. Evaluation 
We evaluated our results in terms of micro- and macro-averaged precision, recall, 
and F-measure (Özgür et al., 2005; van Rijsbergen, 1979; Yang & Liu, 1999), 
performance metrics commonly used in natural language processing tasks. Section  3.3 
describes these metrics in detail. Micro-averaged metrics (Eq. (2a-c)) are computed over 
all samples in the data. In contrast, macro-averaged precision, recall, and F-measure 
(Eq. (3a-c)) take the arithmetic mean of the precision, recall or F-measure metrics of all 
classes, giving equal weight to each class, regardless of its size. When observed together, 
micro- and macro-averaged metrics give a more complete account of the strengths and 
weaknesses of a system. However, given the focus of this chapter on the less well-
represented classes, we used macro-averaged metrics as our primary evaluation 
measurement. 
As listed in Table 21, we compared co-specializing classifiers to three 
complementary classifiers (i.e., J48, Naïve Bayes, and SVM), to specializing 
implemented with the three complementary classifiers, to co-training classifiers 
implemented with each of the three complementary classifiers, and to co-training using 
specializing as its underlying classifier. 
Table 21 - Co-Specializing Experiments 
Complementary 
Classifiers 
Naïve Bayes 
SVM 
J48 
 Specializing 
Co-Trained 
Naïve Bayes 
SVM 
J48 
Specializing 
 Co-specializing 
 
 139 
We evaluated co-specializing classifiers by comparing them to the three 
complementary classifiers. To utilize the complementary classifiers as baselines, we 
trained these classifiers as multi-class classifiers on the seed data set for each disease and 
for smoking status, separately on each disease and on smoking status.We refer to the 
resulting classifiers as the complementary baseline classifiers.  
This study also compared the co-specializing classifier to three co-training baseline 
classifiers. Each of the co-training baseline classifiers was implemented with one of the 
three complementary classifiers. The co-training baseline classifiers were also trained on 
each disease and on smoking status separately. For each disease and for smoking status, 
the classifier utilized the seed data and attributes available to the co-specializing classifier 
for that disease or for smoking status. 
Lastly, to contrast our approach to combining co-training and specializing, this 
study compared the co-specializing classifier to both specializing and co-training with 
specializing as its underlying classifier. We trained the specializing classifier, using the 
three complementary classifiers, on the seed data of the training set for each disease and 
for smoking status, separately on each disease and on smoking status. We then 
implemented co-training with specializing as its underlying classifier, and trained it on 
each disease and on smoking status separately. We refer to this as the co-trained 
specializing classifier. 
This study ran the co-specializing classifiers, the complementary baseline 
classifiers, the specializing classifier, the co-training baseline classifiers, and the 
co-trained specializing classifier on each disease and on smoking status separately. This 
study trained each of the classifiers on the seed set for each disease and for smoking 
 140 
status, and evaluated them on the test set for each disease and for smoking status. We 
report results only on the test sets. 
Following the examples of Message Understanding Conferences (Grishman & 
Sundheim, 1996; Hirschman, 1998) and the i2b2 Smoking Challenge Shared-Task 
(Uzuner et al., 2008), this study reported the significance of performance differences at 
α=0.10. This study tested the significance of performance differences with Z-scores. This 
study applied a two-tailed Z test, with Z = ±1.645. 
6.7. Results and Discussion 
The results in Tables 22-29 show that co-specializing typically provided 
significantly higher recall than the complementary baseline classifiers, the co-training 
baseline classifiers, and the co-trained specializing classifiers in classifying the less well-
represented classes in a document level multi-class classification task when presented 
with a minimal number of labeled samples. The performance gain of co-specializing 
comes from combining co-training’s semi-supervised method for addressing the limited 
availability of annotation resources with specializing’s approach for handling the issue of 
data imbalance.  
We first report on the obesity results aggregated over all six diseases. We then 
report on each of the six diseases individually. Lastly, we report on the smoking status 
results. 
6.7.1. Aggregate Obesity Results 
For the six Obesity related diseases, we found the data to be skewed heavily towards 
the present (Y) and unmentioned (U) classes, with few exemplars in the absent (N) and 
 141 
questionable (Q) classes. The Q class appeared to be the most difficult to predict. Of the 
54 trained learners for the Obesity data (nine classifiers for each of six diseases), only 
four trained learners made correct predictions on the Q class, as compared to the 18 
trained learners that made correct predictions on the N class.  
Due to the small size of the N and Q classes for each disease, the differences 
between the performances of the classifiers for a given disease were often not statistically 
significant for these classes. Therefore, we examined the aggregated performance by class 
over all six diseases in the Obesity data set (see Table 22). 
 
Table 22 - Performance per class, aggregated over six diseases. 
 
Predicted 
Precision Recall F-Measure 
Y N Q U 
Co-Specializing 
Y 223 450 273 217 0.291 0.192 0.231 
N 4 20 9 13 0.020 0.435 0.039 
Q 0 8 5 2 0.010 0.333 0.020 
U 539 503 193 544 0.701 0.306 0.426 
Micro-Average 0.264 0.264 0.264 
Macro-Average 0.256 0.316 0.179 
Co-trained Specializing 
Y 570 62 1 530 0.688 0.490 0.573 
N 23 2 1 20 0.005 0.043 0.008 
Q 4 2 0 9 0.000 0.000 0.000 
U 231 376 0 1172 0.677 0.659 0.668 
Micro-Average 0.581 0.581 0.581 
Macro-Average 0.342 0.298 0.312 
Specializing 
Y 61 1030 15 57 0.709 0.052 0.098 
N 11 25 1 9 0.009 0.543 0.019 
Q 3 9 0 3 0.000 0.000 0.000 
U 11 1588 22 158 0.696 0.089 0.158 
Micro-Average 0.081 0.081 0.081 
Macro-Average 0.354 0.171 0.068 
J48 Co-training Baseline  
Y 789 11 67 296 0.680 0.678 0.679 
N 26 2 0 18 0.087 0.043 0.058 
Q 9 0 2 4 0.005 0.133 0.009 
U 336 10 368 1065 0.770 0.599 0.674 
Micro-Average 0.619 0.619 0.619 
Macro-Average 0.385 0.363 0.355 
 142 
NB Co-training Baseline 
Y 701 3 0 459 0.542 0.603 0.571 
N 36 0 0 10 0.000 0.000 0.000 
Q 4 0 0 11 0.000 0.000 0.000 
U 552 31 3 1193 0.713 0.671 0.691 
Micro-Average 0.631 0.631 0.631 
Macro-Average 0.314 0.318 0.316 
SVM Co-training Baseline 
Y 277 18 2 866 0.556 0.238 0.334 
N 22 0 0 24 0.000 0.000 0.000 
Q 2 0 0 13 0.000 0.000 0.000 
U 197 116 1 1465 0.619 0.823 0.707 
Micro-Average 0.580 0.580 0.580 
Macro-Average 0.294 0.265 0.260 
J48 Baseline 
Y 974 50 39 100 0.930 0.837 0.881 
N 13 14 3 16 0.096 0.304 0.146 
Q 10 2 0 3 0.000 0.000 0.000 
U 50 80 48 1601 0.931 0.900 0.915 
Micro-Average 0.862 0.862 0.862 
Macro-Average 0.489 0.510 0.486 
NB Baseline 
Y 682 0 0 481 0.626 0.586 0.606 
N 24 0 0 22 1.000 0.000 0.000 
Q 3 0 0 12 1.000 0.000 0.000 
U 380 0 0 1399 0.731 0.786 0.758 
Micro-Average 0.693 0.693 0.693 
Macro-Average 0.839 0.343 0.341 
SVM Baseline 
Y 738 39 15 371 0.609 0.635 0.621 
N 32 3 2 9 0.016 0.065 0.025 
Q 7 0 1 7 0.012 0.067 0.020 
U 435 148 67 1129 0.745 0.635 0.685 
Micro-Average 0.623 0.623 0.623 
Macro-Average 0.345 0.350 0.338 
Bold indicates significant difference from the Co-Specializing Classifier. Y, present; N, absent; Q, 
questionable; and U, unmentioned 
 
 
None of the classifiers had significantly higher recall then the co-specializing 
classifier on the less well-represented classes. In fact, the majority of the classifiers failed 
to make predictions for all four of the classes. Only co-specializing, the J48 co-training 
baseline classifier, and the SVM baseline classifier predicted all four of the classes. For 
the N class, co-specializing’s recall was significantly higher than either the J48 
 143 
co-training baseline classifier or the SVM baseline classifier. For the Q class, 
co-specializing’s recall was significantly higher than the SVM baseline classifier. There 
was no significant difference in the precision or the F-measure between the classifiers for 
either class. Co-specializing’s ability to correctly assign test records to the N and Q 
classes showed that the training set provided sufficient informative samples for this 
combined approach to classification. 
6.7.2. Individual Obesity Disease Results 
For each of the six Obesity diseases, we focus our discussion on the less well-
represented N and Q classes. Correct predictions for the N class were made in all six of 
the diseases. Correct predictions for the Q class were made in the diseases asthma, 
diabetes, and obesity. 
For the disease asthma (Table 23), there were only two unlabeled samples of the N 
class. The co-specializing classifier correctly predicted one of the two N class samples. 
However, due to the small class size, the differences between the performances of the 
classifiers on this class are not statistically significant 
Examining the results from the atherosclerotic CV disease runs (Table 24) we find 
that the co-specializing classifier correctly predicted six of the 22 N class unlabeled 
samples. The only other classifiers to correctly predict this class were the specializing 
classifier, which predicted 11 of the N class samples, and the J48 complementary baseline 
classifier that predicted three of the N class samples. Due to the small class size, the 
differences between the performances of these classifiers on this class are not statistically 
significant.  
 144 
 For the disease diabetes, co-specializing correctly predicted samples in all four of 
the classes, including ten of the twelve N class unlabeled samples. Co-specializing’s 
macro-average recall was significantly higher than the baseline co-training classifiers, and 
two of the three baseline classifiers. The three co-training baseline classifiers all failed to 
make correct predictions for the N class. The complementary baseline classifiers J48 and 
SVM correctly predicted, respectively, seven and two of the test samples for this class. 
The specializing classifier correctly predicted five of the test samples for this class. 
Comparing the performance of co-specializing to the three classifiers that correctly 
predicted at least one of the N class samples, we see (Table 25) that co-specializing’s 
recall for this class and its macro-average recall were significantly higher than for SVM 
and specializing, and was the same as the J48 classifier.  
For the disease GERD (Table 26), the test data set had only one sample for the N 
class. Only the specializing classifier correctly predicted this one sample, but it failed to 
correctly classify any other sample for this disease. We find the Y class made up less than 
14% of the test samples for this disease and could be considered a less well-represented 
class. We see that the co-specializing recall for this class was significantly higher than all 
of the co-trained classifiers, and significantly higher or the same as the complementary 
baseline classifiers. 
For the disease hypercholesterolemia (Table 27), the co-trained classifiers all failed 
to make a correct prediction for N class. Other than the co-specializing classifier, only the 
J48 complementary baseline classifier and the specializing classifier made correct 
predictions for this class. Comparing the results for these three classifiers, the differences 
in precision, recall, and F-measure for this class were not statistically significant.  
 145 
Examining the results for the disease obesity, only the specializing classifier 
correctly predicted the N class, but did so by incorrectly predicting over 96% of the 493 
unlabeled samples as N. We also noted that the co-specializing and the complementary 
baseline SVM classifiers were able to correctly predict samples from the Q class. From 
Table 28, we find that co-specializing’s recall for this class and macro-average precision 
and macro-average F-Measure were significantly higher when compared to the 
complementary baseline SVM classifier. In addition, the co-specializing classifier’s 
macro-averaged recall was significantly higher than the any of the three co-training 
baseline classifiers. 
6.7.3. Smoking Status Results 
For our evaluation of the smoking status, we focus our discussion on the less well-
represented classes: current smoker (C), non-smoker (N), past smoker (P), and smoker (S) 
(Table 29). The co-specializing classifier was the only classifier to correctly predict any 
of the S class samples. The co-specializing classifier correctly predicted samples for three 
of the four less well-represented classes (N, P, and S), failing to correctly predict any 
sample only for the C class. Of the co-training baseline classifiers, the co-trained 
specializing classifier, the specializing classifier, and the baseline classifiers, only the J48 
baseline classifier was also able to predict three of the four less well-represented classes 
(C, N, and P).  
 146 
 
Table 23 - Asthma, performance per class. 
 
Co-Specializing Co-trained Specializing Specializing 
P R F P R F P R F 
Y 0.800 0.059 0.110 0.833 0.074 0.135 1.000 0.000 0.000 
N 0.003 0.500 0.007 0.005 1.000 0.009 0.002 0.500 0.004 
Q 1.000 0.000 0.000 1.000 0.000 0.000 0.000 0.000 0.000 
U 0.877 0.396 0.545 0.982 0.127 0.225 0.071 0.005 0.009 
Micro Avg 0.349 0.349 0.349 0.123 0.123 0.123 0.006 0.006 0.006 
Macro Avg 0.670 0.239 0.165 0.705 0.300 0.092 0.268 0.126 0.003 
Co-training 
Baseline 
classifiers 
Co-train NB Co-train SVM Co-train J48 
P R F P R F P R F 
Y 1.000 0.000 0.000 1.000 0.000 0.000 0.053 0.015 0.023 
N 0.000 0.000 0.000 0.000 0.000 0.000 0.222 1.000 0.364 
Q 0.000 0.000 0.000 1.000 0.000 0.000 0.005 1.000 0.009 
U 0.852 0.921 0.885 0.854 0.731 0.788 0.887 0.109 0.194 
Micro Avg 0.790 0.790 0.790 0.627 0.627 0.627 0.103 0.103 0.103 
Macro Avg 0.463 0.230 0.221 0.714 0.183 0.197 0.292 0.531 0.147 
Complemen-
tary Baseline 
Classifiers 
NB SVM J48 
P R F P R F P R F 
Y 1.000 0.000 0.000 1.000 0.000 0.000 0.906 0.853 0.879 
N 1.000 0.000 0.000 0.006 0.500 0.012 0.286 1.000 0.444 
Q 1.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 
U 0.857 1.000 0.923 0.877 0.528 0.659 1.000 0.988 0.994 
Micro Avg 0.857 0.857 0.857 0.454 0.454 0.454 0.966 0.966 0.966 
Macro Avg 0.964 0.250 0.231 0.471 0.257 0.168 0.548 0.710 0.579 
Bold indicates significant difference from the Co-Specializing Classifier. Y, present; N, absent; Q, 
questionable; and U, unmentioned. P, precision; R, recall; and F, F-Measure. 
 147 
 
Table 24 - Atherosclerotic CV Disease, performance per class. 
 
Co-Specializing Co-trained Specializing Specializing 
P R F P R F P R F 
Y 0.875 0.076 0.140 0.559 0.993 0.715 0.742 0.083 0.149 
N 0.033 0.273 0.059 1.000 0.000 0.000 0.033 0.500 0.062 
Q 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 
U 0.444 0.444 0.444 0.750 0.015 0.030 0.832 0.531 0.648 
Micro Avg 0.229 0.229 0.229 0.559 0.559 0.559 0.278 0.278 0.278 
Macro Avg 0.338 0.198 0.161 0.577 0.252 0.186 0.377 0.278 0.215 
Co-training 
Baseline 
classifiers 
Co-train NB Co-train SVM Co-train J48 
P R F P R F P R F 
Y 0.560 1.000 0.718 0.558 1.000 0.717 0.781 0.733 0.756 
N 1.000 0.000 0.000 1.000 0.000 0.000 0.000 0.000 0.000 
Q 1.000 0.000 0.000 1.000 0.000 0.000 0.000 0.000 0.000 
U 1.000 0.010 0.020 1.000 0.005 0.010 0.645 0.760 0.698 
Micro Avg 0.559 0.559 0.559 0.559 0.559 0.559 0.708 0.708 0.708 
Macro Avg 0.890 0.253 0.184 0.890 0.251 0.182 0.356 0.373 0.363 
Complemen-
tary Baseline 
Classifiers 
NB SVM J48 
P R F P R F P R F 
Y 0.638 0.935 0.758 0.595 0.949 0.732 0.911 0.809 0.857 
N 1.000 0.000 0.000 0.000 0.000 0.000 0.081 0.136 0.102 
Q 1.000 0.000 0.000 1.000 0.000 0.000 0.000 0.000 0.000 
U 0.736 0.342 0.467 0.741 0.204 0.320 0.788 0.776 0.781 
Micro Avg 0.656 0.656 0.656 0.610 0.610 0.610 0.763 0.763 0.763 
Macro Avg 0.844 0.319 0.306 0.584 0.288 0.263 0.445 0.430 0.435 
Bold indicates significant difference from the Co-Specializing Classifier. Y, present; N, absent; Q, 
questionable; and U, unmentioned. P, precision; R, recall; and F, F-Measure. 
 
 148 
 
Table 25 - Diabetes, performance per class. 
 
Co-Specializing Co-trained Specializing Specializing 
P R F P R F P R F 
Y 0.857 0.018 0.035 0.918 0.497 0.645 0.655 0.056 0.104 
N 0.042 0.833 0.081 1.000 0.000 0.000 0.011 0.417 0.021 
Q 0.013 0.667 0.025 1.000 0.000 0.000 1.000 0.000 0.000 
U 0.933 0.653 0.769 0.431 0.920 0.587 0.200 0.020 0.036 
Micro Avg 0.231 0.231 0.231 0.608 0.608 0.608 0.054 0.054 0.054 
Macro Avg 0.461 0.543 0.227 0.837 0.354 0.308 0.384 0.123 0.040 
Co-training 
Baseline 
classifiers 
Co-train NB Co-train SVM Co-train J48 
P R F P R F P R F 
Y 0.674 0.678 0.676 1.000 0.000 0.000 0.693 0.947 0.800 
N 1.000 0.000 0.000 1.000 0.000 0.000 0.000 0.000 0.000 
Q 1.000 0.000 0.000 1.000 0.000 0.000 0.000 0.000 0.000 
U 0.319 0.347 0.332 0.298 1.000 0.459 0.750 0.140 0.236 
Micro Avg 0.559 0.559 0.559 0.298 0.298 0.298 0.678 0.678 0.678 
Macro Avg 0.748 0.256 0.252 0.825 0.250 0.115 0.361 0.272 0.259 
Complemen-
tary Baseline 
Classifiers 
NB SVM J48 
P R F P R F P R F 
Y 0.758 0.713 0.735 0.721 0.743 0.732 0.936 0.864 0.898 
N 1.000 0.000 0.000 0.154 0.167 0.160 0.467 0.583 0.519 
Q 1.000 0.000 0.000 1.000 0.000 0.000 0.000 0.000 0.000 
U 0.427 0.527 0.472 0.401 0.380 0.390 0.856 0.873 0.865 
Micro Avg 0.636 0.636 0.636 0.616 0.616 0.616 0.855 0.855 0.855 
Macro Avg 0.796 0.310 0.302 0.569 0.322 0.321 0.565 0.580 0.570 
Bold indicates significant difference from the Co-Specializing Classifier. Y, present; N, absent; Q, 
questionable; and U, unmentioned. P, precision; R, recall; and F, F-Measure. 
 
 149 
 
Table 26 - GERD, performance per class. 
 
Co-Specializing Co-trained Specializing Specializing 
P R F P R F P R F 
Y 0.127 0.768 0.219 1.000 0.145 0.253 1.000 0.000 0.000 
N 1.000 0.000 0.000 1.000 0.000 0.000 0.002 1.000 0.004 
Q 0.000 0.000 0.000 1.000 0.000 0.000 0.000 0.000 0.000 
U 0.805 0.162 0.269 0.877 1.000 0.934 1.000 0.000 0.000 
Micro Avg 0.244 0.244 0.244 0.879 0.879 0.879 0.002 0.002 0.002 
Macro Avg 0.483 0.232 0.122 0.969 0.286 0.297 0.501 0.250 0.001 
Co-training 
Baseline 
classifiers 
Co-train NB Co-train SVM Co-train J48 
P R F P R F P R F 
Y 1.000 0.000 0.000 1.000 0.000 0.000 1.000 0.319 0.484 
N 1.000 0.000 0.000 1.000 0.000 0.000 1.000 0.000 0.000 
Q 1.000 0.000 0.000 1.000 0.000 0.000 0.000 0.000 0.000 
U 0.859 1.000 0.924 0.859 1.000 0.924 0.900 0.995 0.945 
Micro Avg 0.859 0.859 0.859 0.859 0.859 0.859 0.899 0.899 0.899 
Macro Avg 0.965 0.250 0.231 0.965 0.250 0.231 0.725 0.329 0.357 
Complemen-
tary 
Baseline 
Classifiers 
NB SVM J48 
P R F P R F P R F 
Y 0.333 0.014 0.028 0.500 0.014 0.028 0.951 0.841 0.892 
N 1.000 0.000 0.000 1.000 0.000 0.000 0.000 0.000 0.000 
Q 1.000 0.000 0.000 1.000 0.000 0.000 0.000 0.000 0.000 
U 0.860 0.995 0.923 0.861 0.998 0.924 0.974 0.952 0.963 
Micro Avg 0.857 0.857 0.857 0.859 0.859 0.859 0.933 0.933 0.933 
Macro Avg 0.798 0.252 0.238 0.840 0.253 0.238 0.481 0.448 0.464 
Bold indicates significant difference from the Co-Specializing Classifier. Y, present; N, absent; Q, 
questionable; and U, unmentioned. P, precision; R, recall; and F, F-Measure. 
 
 
 
 
 150 
 
Table 27 - Hypercholesterolemia, performance per class. 
 
Co-Specializing Co-train Specializing Specializing 
P R F P R F P R F 
Y 0.430 0.244 0.311 0.689 0.239 0.355 0.731 0.089 0.159 
N 0.012 0.500 0.023 1.000 0.000 0.000 0.009 0.667 0.018 
Q 1.000 0.000 0.000 1.000 0.000 0.000 0.000 0.000 0.000 
U 0.541 0.237 0.329 0.598 0.918 0.724 0.773 0.122 0.211 
Micro Avg 0.241 0.241 0.241 0.612 0.612 0.612 0.114 0.114 0.114 
Macro Avg 0.496 0.245 0.166 0.822 0.289 0.270 0.378 0.219 0.097 
Co-training 
Baseline 
classifiers 
Co-train NB Co-train SVM Co-train J48 
P R F P R F P R F 
Y 0.468 0.380 0.420 0.000 0.000 0.000 0.535 0.831 0.651 
N 1.000 0.000 0.000 1.000 0.000 0.000 0.000 0.000 0.000 
Q 1.000 0.000 0.000 1.000 0.000 0.000 1.000 0.000 0.000 
U 0.571 0.674 0.618 0.554 0.993 0.711 0.768 0.462 0.577 
Micro Avg 0.536 0.536 0.536 0.552 0.552 0.552 0.610 0.610 0.610 
Macro Avg 0.760 0.264 0.260 0.639 0.248 0.178 0.576 0.323 0.307 
Complemen-
tary Baseline 
Classifiers 
NB SVM J48 
P R F P R F P R F 
Y 0.492 0.423 0.455 0.518 0.728 0.605 0.919 0.742 0.821 
N 1.000 0.000 0.000 0.000 0.000 0.000 0.057 0.333 0.098 
Q 1.000 0.000 0.000 1.000 0.000 0.000 0.000 0.000 0.000 
U 0.589 0.674 0.629 0.702 0.498 0.583 0.876 0.914 0.895 
Micro Avg 0.554 0.554 0.554 0.586 0.586 0.586 0.827 0.827 0.827 
Macro Avg 0.770 0.274 0.271 0.555 0.306 0.297 0.463 0.497 0.453 
Bold indicates significant difference from the Co-Specializing Classifier. Y, present; N, absent; Q, 
questionable; and U, unmentioned. P, precision; R, recall; and F, F-Measure. 
 
 
 151 
 
Table 28 - Obesity, performance per class. 
 
Co-Specializing Co-trained Specializing Specializing 
P R F P R F P R F 
Y 0.451 0.439 0.445 0.968 0.308 0.467 1.000 0.000 0.000 
N 1.000 0.000 0.000 1.000 0.000 0.000 0.006 1.000 0.013 
Q 0.013 1.000 0.026 0.000 0.000 0.000 0.000 0.000 0.000 
U 0.732 0.180 0.289 0.669 0.993 0.799 1.000 0.052 0.099 
Micro Avg 0.288 0.288 0.288 0.706 0.706 0.706 0.037 0.037 0.037 
Macro Avg 0.549 0.405 0.190 0.669 0.401 0.317 0.502 0.263 0.028 
Co-training 
Baseline 
classifiers 
Co-train NB Co-train SVM Co-train J48 
P R F P R F P R F 
Y 0.400 0.576 0.472 1.000 0.000 0.000 1.000 0.333 0.500 
N 1.000 0.000 0.000 1.000 0.000 0.000 0.000 0.000 0.000 
Q 1.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 
U 0.577 0.415 0.483 0.588 0.997 0.739 0.679 0.997 0.808 
Micro Avg 0.475 0.475 0.475 0.584 0.584 0.584 0.718 0.718 0.718 
Macro Avg 0.744 0.248 0.239 0.647 0.249 0.185 0.420 0.332 0.327 
Complemen-
tary Baseline 
Classifiers 
NB SVM J48 
P R F P R F P R F 
Y 0.508 0.460 0.483 0.562 0.343 0.426 0.958 0.929 0.944 
N 1.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 
Q 1.000 0.000 0.000 0.091 0.333 0.143 0.000 0.000 0.000 
U 0.643 0.699 0.670 0.647 0.806 0.718 0.961 0.775 0.858 
Micro Avg 0.594 0.594 0.594 0.613 0.613 0.613 0.828 0.828 0.828 
Macro Avg 0.788 0.290 0.288 0.325 0.371 0.322 0.480 0.426 0.450 
Bold indicates significant difference from the Co-Specializing Classifier. Y, present; N, absent; Q, 
questionable; and U, unmentioned. P, precision; R, recall; and F, F-Measure. 
 152 
 
Table 29 - Smoking, performance per class. 
 
Co-Specializing Co-trained Specializing Specializing 
P R F P R F P R F 
C 1.000 0.000 0.000 1.000 0.000 0.000 1.000 0.182 0.308 
N 0.267 0.250 0.258 0.333 0.375 0.353 0.141 0.688 0.234 
P 0.154 0.545 0.240 1.000 0.000 0.000 1.000 0.000 0.000 
S 0.050 0.333 0.087 1.000 0.000 0.000 1.000 0.000 0.000 
U 0.933 0.444 0.602 0.663 0.905 0.765 0.250 0.095 0.138 
Micro Avg 0.375 0.375 0.375 0.606 0.606 0.606 0.183 0.183 0.183 
Macro Avg 0.481 0.315 0.237 0.799 0.256 0.224 0.678 0.193 0.136 
Co-training 
Baseline 
classifiers 
Co-train NB Co-train SVM Co-train J48 
P R F P R F P R F 
C 1.000 0.182 0.308 1.000 0.182 0.308 0.000 0.000 0.000 
N 0.000 0.000 0.000 0.462 0.375 0.414 0.600 0.375 0.462 
P 1.000 0.000 0.000 1.000 0.000 0.000 0.333 0.091 0.143 
S 1.000 0.000 0.000 1.000 0.000 0.000 0.000 0.000 0.000 
U 0.620 0.984 0.761 0.674 0.952 0.789 0.639 0.841 0.726 
Micro Avg 0.615 0.615 0.615 0.654 0.654 0.654 0.577 0.577 0.577 
Macro Avg 0.724 0.233 0.214 0.827 0.302 0.302 0.314 0.261 0.266 
Complemen-
tary Baseline 
Classifiers 
NB SVM J48 
P R F P R F P R F 
C 1.000 0.182 0.308 1.000 0.182 0.308 0.167 0.091 0.118 
N 0.357 0.313 0.333 0.400 0.375 0.387 0.560 0.875 0.683 
P 1.000 0.000 0.000 1.000 0.000 0.000 0.250 0.091 0.133 
S 1.000 0.000 0.000 1.000 0.000 0.000 0.000 0.000 0.000 
U 0.693 0.968 0.808 0.724 1.000 0.840 0.906 0.921 0.913 
Micro Avg 0.654 0.654 0.654 0.683 0.683 0.683 0.712 0.712 0.712 
Macro Avg 0.810 0.293 0.290 0.825 0.311 0.307 0.377 0.395 0.369 
Bold indicates significant difference from the Co-Specializing Classifier. C, current smoker N, non-smoker; 
P, past smoker; S, smoker; or U, unknown. P, precision; R, recall; and F, F-Measure. 
 
 153 
6.7.4. Co-Specializing Discussion  
The co-specializing classifier was able to predict more samples from the less well-
represented classes than any of the baseline classifiers. Its recall was typically the same or 
higher than the other classifiers for these classes. In several instances, the co-specializing 
classifier was able to correctly predict classes that the underlying complementary baseline 
classifiers failed to predict. 
An increase in either the precision or the recall metric usually results in a decrease 
of the other metric. The implementation of co-specializing was not an exception to this 
rule. Co-specializing achieved its improved recall at the expenses of precision. However, 
co-specializing accomplished this while also correctly predicting some samples from the 
more well-represented classes for all of six of the diseases and for the smoking status.  
The co-specializing classifier was able to correctly predict the less well-represented 
classes for diabetes and for smoking status that the underlying complementary baseline 
classifiers failed to predict. This was accomplished by activating a panel of OVA 
classifiers in a strict order, thereby allowing co-specializing to focus on the less well-
represented classes. The complementary baseline classifiers and the co-training baseline 
classifiers were all trained in a multi-class manner, and therefore all tended to favor the 
more well-represented classes. The co-trained specializing classifier did not focus on the 
more well-represented classes since, during each round, it used the prediction for which it 
had the highest confidence. 
Co-specializing’s ability to correctly assign samples to the absent and questionable 
classes in the obesity data, and to the non-smoker, past smoker, and smoker classes in the 
 154 
smoking data showed that even in light of skewed data classes and scarcity of labeled 
data, the seed set provided sufficient informative samples for this combined approach to 
successfully perform classification. 
6.8. Conclusions 
This study combined the approaches of specializing and co-training to create 
co-specializing classifiers. This study showed that the combination of the two methods 
addressed both the issue of data imbalance and the issue of limited annotation resources 
in a document level multi-class classification task based on information in medical 
discharge summaries. 
Co-specializing began with the specializing method of activating a panel of OVA 
classifiers in a strict order. Co-specializing activated the first OVA classifier for all of the 
samples and selected the sample for which the classifier had the highest confidence. If the 
first OVA classifier failed to predict a label for any of the samples, co-specializing 
activated the next OVA classifier and used it to attempt to predict a label for all of the 
samples. This strict order was followed until a label was predicted for at least one of the 
samples. We then employed co-training’s two views of the data approach by adding the 
predicted sample to a second panel of classifiers’ labeled samples. The additional sample 
was treated in the same manner as the small set of manually labeled samples. The process 
continued, alternating between each of the two panels of classifiers, until all of the 
samples had been predicted. 
The specializing classification method helped to address the issues surrounding the 
imbalance of classes in the data sets. The co-training approach helped to address the 
 155 
problems associated with scarcity of labeled data. When combined, the resultant 
co-specializing classifiers were able to consistently improve recall on the less well-
represented classes despite the scarcity of labeled data. 
6.9. Summary 
This chapter described a method that combines the approaches of co-training and 
specializing to address the issues associated with a scarcity of labeled examples when 
presented with an imbalance of classes in the data sets. Co-specializing employs two 
panels of classifiers, each panel making use of a different view of the data.  
 We evaluated co-specializing on six obesity related diseases and on the smoking 
status of patients. We show that by combining co-training and specializing, 
co-specializing is able to improve recall on the less well-represented classes when 
presented with an imbalance of classes in the data sets, even when trained on a small 
number of labeled samples. 
 
 156 
Chapter 7. Conclusions 
7.1. Introduction 
This dissertation presented three topics that are critical to the document level 
classification of the narrative of medical reports. The first topic related to the application 
of preferred terminology in light of the presence of synonymous terms in the narrative of 
medical reports. The second topic looked at the imbalance caused by the non-uniform 
distribution of classes in the data that can cause less than optimal performance by 
classification systems. The last topic investigated an approach to resolving the problems 
associated with scarcity of labeled data when presented with an imbalance of classes in 
the data sets 
This chapter reviews the major findings for these three topics. It then describes the 
strengths and limitations of the dissertation. The chapter ends by providing several topics 
that should be considered for additional exploration. 
7.2. Major Findings 
7.2.1. Preferred Terminology 
The first set of questions relate to issues brought about by synonymy. The literature 
contains examples of problems related to lexical disparity caused by synonyms. While 
some studies have reported improved system performance when resolving lexical 
disparity, cases have also been reported where resolving lexical disparity does not 
improve system performance.  
 157 
This dissertation examined the role of preferred terminology as a means of post-hoc 
coordination in automatic classification of patient medical data. We applied natural 
language processing tools to the free text of the medical reports and we built and 
evaluated both a hand-crafted rule-based system and two machine learning systems. We 
experimented with medical reports that came from two different corpora, one containing 
radiology reports and the other containing discharge summaries. We gathered 
semantically-equivalent but lexically-disparate medical terms used in these reports under 
preferred terms for multi-label classification of the medical reports. 
The first two research questions asked: Will the addition of preferred terms for 
diseases and symptoms improve system performance of a hand-crafted rule-based system 
in a multi-label document classification task? Will the addition of preferred terms for 
diseases and symptoms when enhanced with the polarity of assertions about diseases and 
symptoms improve system performance of a hand-crafted rule-based system in a multi-
label document classification task?  
We determined that the addition of preferred terms helped to improve true positives, 
improving both recall and F-measure, for a hand-crafted rule-based system in a multi-
label document classification task. We also found that enhancing the preferred terms with 
the polarity of assertions decreased false positives, resulting in improved precision, recall, 
and F-measure in a hand-crafted rule-based system in a multi-label document 
classification task.  
Hand-crafted rule-based systems usually encode the most salient information that 
experts would find useful in classification. However, these systems do not have the 
 158 
intrinsic ability to identify unencoded patterns in data. Therefore, hand-crafted rule-based 
systems should benefit from the addition of preferred terminology. 
The next two questions related to the application of preferred terminology to 
machine learning: Will the addition of preferred terms for diseases and symptoms 
improve system performance of machine learning systems in a multi-label document 
classification task? Will the addition of preferred terms for diseases and symptoms when 
enhanced with the polarity of assertions about diseases and symptoms improve system 
performance of machine learning systems in a multi-label document classification task? 
Enhancing the text of the medical reports with preferred terms, assertion, or both 
preferred terms and assertion, failed to improve the Lucene classifier’s performance on 
either corpus in a multi-label document classification task. We attributed these results to 
the presence of information in the text beyond the direct references to the diseases and to 
the k-Nearest Neighbor approach to classification. While the introduction of preferred 
terms improved the lexical overlap between documents, since all of the words in the 
documents were used as attributes, enhancing the text with preferred terms or asserted 
noun phrases would leave a given document vector relatively unchanged. This resulted in 
consistent performance across variations of the Lucene classifier on both corpora. 
We showed that enriching the medical reports with preferred terms did not 
consistently improve the BoosTexter classifiers in a multi-label document classification 
task. The Preferred Term BoosTexter Classifier runs showed improvement in recall when 
compared to the base runs on the i2b2 Obesity corpus, but failed to show improvement on 
the CMC corpus. We attribute this difference to the underlying characteristics of the two 
corpora. However, the use of preferred terminology when enhanced with the polarity of 
 159 
assertions about diseases and symptoms was able to improve both of the i2b2 Asserted 
Preferred Term BoosTexter Classifier runs and the unigram CMC Asserted Preferred 
Term BoosTexter Classifier run. We attribute this improvement to the creation of a 
distinct preferred term for each of the positive, negative and uncertain diagnoses when 
combining preferred terms with assertion. 
Our task was one of multi-label document level classification. Working at the 
document level, each preferred term and assertion would play only a small role in 
predicting class labels. In other tasks, such as information retrieval, the use of preferred 
terms may provide a more consistent improvement in recall. When taken in conjunction 
with the results from the literature, our findings indicate that the contribution of preferred 
terminology to a natural language processing task depends on the nature of the task, the 
data, and the approach applied. 
7.2.2. Imbalanced Data Sets 
The second set of research questions relate to imbalances found in data sets. 
Imbalances caused by the non-uniform distribution of classes in the data can cause less 
than optimal performance by classification systems. Given a non-uniform distribution of 
classes, machine learning classifiers may simply ignore the less-well represented classes 
in order to maximize overall performance. 
This dissertation presented specializing, a method to improve the classification 
performance on the less well-represented classes in multi-class classification of diseases 
based on information in medical discharge summaries. Specializing combined a panel of 
classifiers for multi-class multi-label classification. Specializing trained a one-versus-all 
 160 
(OVA) classifier for each class, selected the highest performing OVA for each class, and 
activated each selected OVA classifier in a strict order to distinguish that class from all 
others. It then supplemented the OVA classifiers with a catch-all classifier that performed 
multi-class classification across all of the classes.  
The research questions related to imbalanced data sets asked: Can the application of 
a panel of one-versus-all classifiers, when activated in a strict order, outperform J48, 
Naïve Bayes, and AdaBoost.M1 classifiers on the less well-represented classes in a multi-
class, multi-label task? Can the application of a panel of one-versus-all classifiers, when 
activated in a strict order, outperform the voting and stacking approaches to combining 
J48, Naïve Bayes, and AdaBoost.M1 classifiers on the less well-represented classes in a 
multi-class, multi-label task? 
We tested specializing on the i2b2 Obesity corpus. For this multi-class, multi-label 
task, the specializing classifier demonstrated significant improvement in macro-averaged 
F-measure over J48, Naïve Bayes, and AdaBoost.M1 classifiers. Specializing also 
performed significantly better than voting and stacking approaches to combining J48, 
Naïve Bayes, and AdaBoost.M1 classifiers. Examining the results at the individually 
labeled diseases level, for those diseases that were in fact multi-class classification 
problems, i.e., had samples in more than two classes, we found that specializing 
performed no worse than, and at times showed significant improvement over, the 
complementary J48, Naïve Bayes, and AdaBoost.M1 baselines, and over the voting, and 
stacking combined baselines.  
Specializing’s approach to combining classifiers proved to be an effective way to 
improve macro-averaged performance in multi-class classification of diseases in the 
 161 
presence of less well-represented classes. This improvement was accomplished without 
sacrificing overall micro-averaged performance. We observed that specializing required a 
sufficient minimum number of samples from a given class in order to correctly predict 
that class.  
7.2.3. Availability of Labeled Data 
The third set of research questions relate to imbalanced data sets when presented 
with a limited availability of labeled data. Creating labeled data with which to train 
machine learning classification systems requires human expertise and can be expensive to 
develop. The semi-supervised method of co-training utilizes unlabeled data to improve 
the results from supervised machine learning systems that have been trained with a small 
labeled training set. However, co-training does not inherently address the issues 
associated with imbalanced data sets.  
This dissertation presented co-specializing, a method for addressing both the issue 
of data imbalance and the issue of limited annotation resources. Co-specializing 
combined the approaches of specializing and co-training. The specializing classification 
method helped to address the issues surrounding the imbalance of classes in the data sets. 
The co-training approach helped to address the problems associated with scarcity of 
labeled data.  
The research questions related to limited availability of labeled data asked: Does the 
application of co-training to a panel of one-versus-all classifiers, when activated in a strict 
order, outperform J48, Naïve Bayes, and SVM classifiers on the less well-represented 
classes in a multi-class classification task when presented with a scarcity of labeled data? 
 162 
Does the application of co-training to a panel of one-versus-all classifiers, when activated 
in a strict order, outperform J48, Naïve Bayes, and SVM based co-training classifiers on 
the less well-represented classes in a multi-class classification task when presented with a 
scarcity of labeled data? 
We examined the performance of co-specializing on the i2b2 Obesity and the i2b2 
Smoking data sets. We found that the co-specializing classifier was able to predict more 
samples from the less well-represented classes than any of the J48, Naïve Bayes, and 
SVM classifiers. Co-specializing was also able to predict more samples from the less 
well-represented classes than co-training, when employed with J48, Naïve Bayes, and 
SVM as the underlying classifiers. 
The recall for co-specializing was typically the same or higher than the other 
classifiers for the less well-represented classes. In several instances, the co-specializing 
classifier was able to correctly predict classes that the underlying baseline classifiers 
failed to predict.  
7.3. Strengths and Limitations 
All research studies have limitations, and it is important to explicitly acknowledge 
them both at research inception and when reporting results. The following describes the 
strengths and limitations of this dissertation. 
One limitation of this dissertation is that the data came from one domain. The 
purpose of this dissertation was to improve the understanding and use of foundational 
NLP and ML approaches to the classification of the narrative of medical reports. This 
research focused on one domain in order to prevent an overly complex undertaking. 
 163 
However, given that the data came from one domain, these results may not be 
generalizable and therefore may not apply to other domains. 
A second limitation of this dissertation is that the preferred terminology used by the 
NPDP all came from the UMLS Metathesaurus. While this limitation needs to be 
acknowledged, it is mitigated by the fact that the UMLS Metathesaurus incorporates a 
number of controlled vocabularies (e.g., MeSH, SNOMED CT). 
A third limitation relates to the specializing catch-all classifier. As implemented, 
specializing trained the catch-all classifiers on the complete training data. Other 
approaches to accomplishing catch-all classification may result in improved performance. 
A fourth limitation of this dissertation involves the selection of the specializing 
panel of classifiers. While we have shown that specializing out performs the underlying 
complementary classifiers, both individually and when used in combination via voting 
and stacking, there may be other panels of classifiers that would outperform those 
presented in this dissertation.  
One of the strengths of this dissertation is that it used multiple data sets from 
multiple sources. These data sets represented two different types of documents. As noted 
above in section  3.2, the characteristics of the radiological reports differ from those of the 
discharge summaries. 
A second strength of this dissertation is that it used multiple approaches to 
classification. While investigating the role of preferred terminology in the classification 
of medical reports, this dissertation investigated multiple machine learning systems and a 
hand-crafted rule-based system.  
 164 
A third strength of this dissertation is the extensive use of baselines and 
intermediate systems (e.g., the creation of both the Preferred Term BoosTexter Classifiers 
and the Asserted Preferred Term BoosTexter Classifiers) with which to compare system 
performance. The use of both baseline and intermediate systems allows for analysis that 
helped to determine which component, or combination of components, provided for the 
improvement in system performance. 
7.4. Issues for Additional Exploration 
This dissertation provides the foundation for a range of additional research. 
Beginning with preferred terminology, while we have shown that the contribution of 
preferred terminology to the narrative of medical records depends on the nature of the 
task, the data, and the approach applied, would the contribution of preferred terminology 
in other domains be affected by these factors? In addition, does the controlled vocabulary 
employed for preferred terminology alter system performance? 
Looking at specializing, can the specializing approach to addressing less well-
represented classes apply to other record types and other domains? Can the performance 
of the specializing catch-all classifiers be improved? For example, would an 
improvement be seen if the catch-all classifier trained only on the samples that were not 
correctly classified by the specialists? 
We employed specific panels of complementary classifiers. Must the specializing or 
co-specializing panels of classifiers contain complementary machine learning 
approaches? Could a panel of similar classifiers (e.g., a panel of decision tress), when 
activated in a strict order, perform as well or better than a panel of complementary 
 165 
classifiers? Would changing the number of complementary classifiers in the specializing 
or co-specializing panels of classifiers have an effect on performance? 
 166 
 
References 
Abney, S. (1997). Part-of-Speech Tagging and Partial Parsing. In S. Young & G. 
Bloothooft (Eds.), Corpus-based methods in language and speech processing (pp. 
118-136). Dordrecht Kluwer Academic  
Ackoff, R. L. (1989). From data to wisdom. Journal of Applied Systems Analysis, 16(1), 
3-9. 
Apté, C., Weiss, S., & Grout, G. (1994). Case studies in high-dimensional classification. 
Applied Intelligence: The International Journal of Artificial Intelligence, Neural 
Networks, and Complex Problem-Solving Technologies, 4(3), 269-281. 
Aronow, D. B., Fangfang, F., & Croft, W. B. (1999). Ad hoc classification of radiology 
reports. Journal of the American Medical Informatics Association, 6(5), 393-411. 
Aronson, A. R. (2001). Effective mapping of biomedical text to the UMLS 
Metathesaurus: the MetaMap program. Paper presented at the Proceedings of the 
AMIA symposium. 
Ben-David, A., & Frank, E. (2009). Accuracy of machine learning models versus “hand 
crafted” expert systems: A credit scoring case study Expert Systems with 
Applications, 36(3), 5264-5271. 
Berners-Lee, T. (1998). Semantic Web Road Map.   Retrieved 23 November, 2007, from 
http://www.w3.org/DesignIssues/Semantic.html 
Bickel, P. J., & Levina, E. (2004). Some theory for Fisher’s Linear Discriminant 
function,“naive Bayes”, and some alternatives when there are many more 
variables than observations. Bernoulli, 10(6), 989-1010. 
 167 
Blanzieri, E., & Bryl, A. (2008). A survey of learning-based techniques of email spam 
filtering (Technical Report No. DIT-06-056): University of Trento. 
Blum, A. L., & Mitchell, T. M. (1998). Combining labeled and unlabeled data with co-
training. Paper presented at the Eleventh Annual Conference on Computational 
Learning Theory, Madison, WI. 
Breiman, L. (1996). Bagging Predictors. Machine Learning, 24(2), 123-140. 
Breiman, L. (2001). Random Forests. Machine Learning, 45(1), 5-32. 
Breiman, L., Friedman, J. H., Olshen, R. A., & Stone, C. J. (1984). Classification and 
regression trees. Belmont, Calif.: Wadsworth International Group. 
Brennan, P. F., & Aronson, A. R. (2003). Towards linking patients and clinical 
information: detecting UMLS concepts in e-mail. Journal of Biomedical 
Informatics, 36(4-5), 334-341. 
Brill, E. (1992). A simple rule-based part of speech tagger. Paper presented at the Third 
Conference on Applied Natural Language Processing. Proceedings of the 
Conference, Trento, Italy. 
Brill, E. (1995). Transformation-based error-driven learning and natural language 
processing: A case study in part-of-speech tagging. Computational linguistics, 
21(4), 543-565. 
Brodbeck, M. (1968). Meaning and Action. In M. Brodbeck (Ed.), Readings in the 
Philosophy of the Social Sciences. New York, NY: Collier-Macmillan. 
de Buenaga Rodríguez, M., Gómez Hidalgo, J. M., & Díaz-Agudo, B. (2000). Using 
WordNet to Complement Training Information in Text Categorisation. In N. 
 168 
Nicolov & R. Mitkov (Eds.), Recent Advances in Natural Language Processing, 
II (pp. 353-364). Amsterdam, Netherlands: Benjamins. 
Bui, A. A. T., Taira, R. K., El-Saden, S., Dordoni, A., & Aberle, D. R. (2004). Automated 
medical problem list generation: towards a patient timeline. Medinfo. MEDINFO, 
11(Pt 1), 587-591. 
Burges, C. J. C. (1998). A tutorial on support vector machines for pattern recognition. 
Data Mining and Knowledge Discovery, 2(2), 121-167. 
Cai, Y., & Cheng, X. (2009). Biomedical named entity recognition with tri-training 
learning. Paper presented at the 2nd International Conference on Biomedical 
Engineering and Informatics (BMEI). 
Chan, J., Koprinska, I., & Poon, J. (2004). Co-training with a Single Natural Feature Set 
Applied to Email Classification. Paper presented at the 2004 IEEE/WIC/ACM 
International Conference on Web Intelligence. 
Chan, P. K., & Stolfo, S. J. (1993). Experiments on multistrategy learning by meta-
learning. Paper presented at the Proceedings of the second international 
conference on Information and knowledge management. 
Chan, P. K., & Stolfo, S. J. (1993). Toward parallel and distributed learning by meta-
learning. Paper presented at the AAAI Workshop in Knowledge Discovery in 
Databases. 
Chan, P. K., & Stolfo, S. J. (1995). A comparative evaluation of voting and meta-learning 
on partitioned data. Paper presented at the Proceedings of the Twelfth 
International Conference on Machine Learning, San Francisco. 
 169 
Chang, C.-C., & Lin, C.-J. (2001). LIBSVM: a library for support vector machines. 
Software available at http://www.csie.ntu.edu.tw/~cjlin/libsvm. 
Chapman, W. W., Bridewell, W., Hanbury, P., Cooper, G. F., & Buchanan, B. G. (2001). 
A Simple Algorithm for Identifying Negated Findings and Diseases in Discharge 
Summaries. Journal of Biomedical Informatics, 34(5), 301-310. 
Chawla, N. V., Bowyer, K. W., Hall, L. O., & Kegelmeyer, W. P. (2002). SMOTE: 
synthetic minority over-sampling technique. Journal of Artificial Intelligence 
Research, 16(3), 321-357. 
Chawla, N. V., Japkowicz, N., & Kotcz, A. (2004). Editorial: special issue on learning 
from imbalanced data sets. SIGKDD Explor. Newsl., 6(1), 1-6. 
Childs, L. C., Enelow, R., Simonsen, L., Heintzelman, N. H., Kowalski, K. M., & Taylor, 
R. J. (2009). Description of a rule-based system for the i2b2 challenge in natural 
language processing for clinical data. Journal of the American Medical 
Informatics Association, 16(4), 571-575. 
Chomsky, N. (1956). Three models for the description of language. IEEE Transactions 
on Information Theory, 2(3), 113-124. 
Church, K. W. (1988). A stochastic parts program and noun phrase parser for 
unrestricted text. Paper presented at the Second Conference on Applied Natural 
Language Processing, Austin, TX. 
Cimino, J. J., Hripcsak, G., & Johnson, S. B. (1994). Knowledge-based approaches to the 
maintenance of a large controlled medical terminology. Journal of the American 
Medical Informatics Association, 1(1), 35-50. 
 170 
Clancey, W. J. (1983). The epistemology of a rule-based expert system-a framework for 
explanation. Artificial Intelligence, 20(3), 215-251. 
Cleveland, D. B., & Cleveland, A. D. (2001). Introduction to indexing and abstracting 
(3rd ed.). Englewood, Colo.: Libraries Unlimited. 
Collier, N., & Takeuchi, K. (2004). Comparison of character-level and part of speech 
features for name recognition in biomedical texts. Journal Of Biomedical 
Informatics, 37(6), 423-435. 
Cover, T. M., & Hart, P. E. (1967). Nearest neighbor pattern classification. IEEE 
Transactions on Information Theory, 13(1), 21-27. 
Cutting, D., Kupiec, J., Pedersen, J., & Sibun, P. (1992). A practical part-of-speech 
tagger. Paper presented at the Third Conference on Applied Natural Language 
Processing. Proceedings of the Conference, Trento, Italy. 
Daskalakis, A., Kostopoulos, S., Spyridonos, P., Glotsos, D., Ravazoula, P., Kardari, M., 
et al. (2008). Design of a multi-classifier system for discriminating benign from 
malignant thyroid nodules using routinely H&E-stained cytological images. 
Computers in Biology and Medicine, 38(2), 196-203. 
Delbecque, T., Jacquemart, P., & Zweigenbaum, P. (2005). Indexing UMLS semantic 
types for medical question-answering. Studies In Health Technology And 
Informatics, 116, 805-810. 
DeRose, S. J. (1988). Grammatical category disambiguation by statistical optimization. 
Computational Linguistics, 14, 31-39. 
Diederich, J., Kindermann, J., Leopold, E., & Paass, G. (2003). Authorship attribution 
with support vector machines. Applied Intelligence: The International Journal of 
 171 
Artificial Intelligence, Neural Networks, and Complex Problem-Solving 
Technologies, 19(1), 109-123. 
Dolin, R. H., Alschuler, L., Boyer, S., Beebe, C., Behlen, F. M., Biron, P. V., et al. 
(2006). HL7 Clinical Document Architecture, Release 2. Journal of the American 
Medical Informatics Association, 13(1), 30-39. 
Duda, R. O., & Hart, P. E. (1973). Pattern classification and scene analysis. New York,: 
Wiley. 
Duin, R. P. W., & Tax, D. M. J. (2000). Experiments with classifier combining rules. 
Paper presented at the Multiple Classifier Systems. First International Workshop, 
MCS 2000., Berlin, Germany. 
Eom, J.-H., Kim, S.-C., & Zhang, B.-T. (2008). AptaCDSS-E: A classifier ensemble-
based clinical decision support system for cardiovascular disease level prediction. 
Expert Systems with Applications, 34(4), 2465-2479. 
Farkas, R., & Szarvas, G. (2008). Automatic construction of rule-based ICD-9-CM 
coding systems. BMC Bioinformatics, 9 Suppl 3, S10-S10. 
Federici, S., Montemagni, S., & Pirrelli, V. (1996). Shallow parsing and text chunking: a 
view on underspecification in syntax. Paper presented at the Eighth European 
Summer School in Logic, Language and Information. Workshop on Robust 
Parsing. ESSLLI'96, Prague, Czech Republic. 
Fiszman, M., Chapman, W. W., Aronsky, D., & Evans, R. S. (2000). Automatic detection 
of acute bacterial pneumonia from chest x-ray reports. Journal of the American 
Medical Informatics Association, 7, 593-604. 
 172 
Fix, E., & Hodges, J. L. (1952). Discriminatory analysis, nonparametric discrimination: 
Small sample performance (No. 11). Randolph Fields, TX: USAF School of 
Aviation Medicine. 
Freund, Y. (1995). Boosting a weak learning algorithm by majority. Information and 
Computation, 121(2), 256-285. 
Freund, Y., & Schapire, R. E. (1996). Experiments with a New Boosting Algorithm. Paper 
presented at the Machine Learning: Proceedings of the Thirteenth International 
Conference. 
Freund, Y., & Schapire, R. E. (1997). A decision-theoretic generalization of on-line 
learning and an application to boosting. Journal of Computer and System 
Sciences, 55(1), 119-139. 
Friedman, C., Alderson, P. O., Austin, J. H. M., Cimino, J. J., & Johnson, S. B. (1994). A 
general natural-language text processor for clinical radiology. Journal of the 
American Medical Informatics Association, 1(2), 161-174. 
Gale, W. A., Church, K. W., & Yarowsky, D. (1992). One sense per discourse. Paper 
presented at the DARPA Speech and Natural Language Workshop, Harriman, 
NY. 
Godec, M., Leistner, C., Bischof, H., Starzacher, A., & Rinner, B. (2010). Audio-Visual 
Co-Training for Vehicle Classification. Paper presented at the Proceedings 7th 
IEEE International Conference on Advanced Video and Signal Based Surveillance 
(AVSS 2010). 
 173 
Goldstein, I., Arzumtsyan, A., & Uzuner, Ö. (2007). Three approaches to automatic 
assignment of ICD-9-CM codes to radiology reports. Paper presented at the 
Proceedings of the AMIA symposium. 
Gospodnetić, O., & Hatcher, E. (2005). Lucene in action. Greenwich, CT: Manning. 
Greene, B. B., & Rubin, G. M. (1971). Automatic grammatical tagging of English. 
Providence, RI: Department of Linguistics, Brown University. 
Grishman, R., & Sundheim, B. (1996). Message Understanding Conference-6: a Brief 
History. Paper presented at the 16th Conference on Computational Linguistics, 
Copenhagen, Denmark. 
Guo, M.-Z., Deng, C., Liu, Y., & Li, P. (2011). Tri-training and MapReduce-based 
massive data learning. International Journal of General Systems, 40(4), 355-380. 
Guo, Q.-l., & Zhang, M. (2009). Semantic information integration and question 
answering based on pervasive agent ontology, Expert Systems with Applications 
(Vol. 36, pp. 10068-10077). 
Guzella, T. S., & Caminhas, W. M. (2009). A review of machine learning approaches to 
Spam filtering. Expert Systems with Applications, 36(7), 10206-10222. 
Hai-Tao, H., Xiao-Nan, L., Fei-Teng, M., ChunHui, C., & Jian-Min, W. (2009). Network 
traffic classification based on ensemble learning and co-training. Science in China 
Series F (Information Science), 52(2), 338-346. 
Harkema, H., Dowling, J. N., Thornblade, T., & Chapman, W. W. (2009). ConText: an 
algorithm for determining negation, experiencer, and temporal status from clinical 
reports. Journal Of Biomedical Informatics, 42(5), 839-851. 
 174 
Hersh, W. R., Price, S. L., & Donohoe, L. (2000). Assessing thesaurus-based query 
expansion using the UMLS Metathesaurus. Proceedings of the AMIA symposium, 
344-348. 
Hindle, D. (1989). Acquiring disambiguation rules from text. Paper presented at the 27th 
Meeting of the Association for Computational Linguistics, Vancouver, BC. 
Hirschman, L. (1998). The evolution of evaluation: lessons from the Message 
Understanding Conferences. Computer Speech and Language, 12(4), 281-305. 
Horn, L. R., & Katō, Y. (2000). Negation and polarity : syntactic and semantic 
perspectives. Oxford ; New York: Oxford University Press. 
Hripcsak, G., & Friedman, C. (1995). Unlocking clinical data from narrative reports: A 
study of natural language processing. Annals of Internal Medicine, 122(9), 681. 
Hu, R., & Damper, R. I. (2008). A 'No Panacea Theorem' for classifier combination. 
Pattern Recognition, 41(8), 2665-2673. 
Hughes, C. A., Gose, E. E., & Roseman, D. L. (1990). Overcoming deficiencies of the 
rule-based medical expert system. Computer Methods And Programs In 
Biomedicine, 32(1), 63-71. 
Hull, D. A. (1996). Stemming Algorithms: A Case Study for Detailed Evaluation. 
Journal of the American Society for Information Science, 47(1), 70-84. 
IJzereef, L., Kamps, J., & de Rijke, M. (2005). Biomedical retrieval: how can a thesaurus 
help? Paper presented at the On the Move to Meaningful Internet Systems 2005: 
CoopIS, DOA, and ODBASE., Agia Napa, Cyprus. 
 175 
Informatics for Integrating Biology and the Bedside. (2008). Second i2b2 shared-task and 
workshop: challenges in natural language processing for clinical data.   Retrieved 
May 30, 2008, from www.i2b2.org/NLP/ 
Jain, A. K., & Chandrasekaran, B. (1982). Dimensionality and sample size considerations 
in pattern recognition practice. In P. R. Krishnaia & L. N. Kanal (Eds.), Handbook 
of Statistics (Vol. 2, pp. 835-855): Amsterdam, The Netherlands. 
Janas, J. M. (1977). Automatic recognition of the part-of-speech for English texts. 
Information Processing & Management, 13(4), 205-213. 
John, G. H., & Langley, P. (1995). Estimating continuous distributions in Bayesian 
classifiers. Paper presented at the Eleventh Conference on Uncertainty in 
Artificial Intelligence, Montreal, Que., Canada. 
Katz, B., & Lin, J. J. (2002). START and beyond. Paper presented at the 6th World 
Multiconference on Systemics, Cybernetics and Informatics. Proceedings, 
Orlando, FL. 
Kim, J.-J., & Park, J. C. (2006). Extracting contrastive information from negation 
patterns in biomedical literature. ACM Transactions on Asian Language 
Information Processing (TALIP) 5(1), 44-60. 
Kittler, J., Hatef, M., Duin, R. P. W., & Matas, J. (1998). On combining classifiers. IEEE 
Transactions on Pattern Analysis & Machine Intelligence, 20(3), 226. 
Kotsiantis, S. B., & Pintelas, P. E. (2009). Selective costing ensemble for handling 
imbalanced data sets. International Journal of Hybrid Intelligent Systems, 6(3), 
123-133. 
 176 
Krauthammer, M., & Nenadic, G. (2004). Term identification in the biomedical literature. 
Journal Of Biomedical Informatics, 37(6), 512-526. 
Krenzelok, E., Macpherson, E., & Mrvos, R. (2008). Disease surveillance and 
nonprescription medication sales can predict increases in poison exposure. 
Journal Of Medical Toxicology: Official Journal Of The American College Of 
Medical Toxicology, 4(1), 7-10. 
Krovetz, R. (1998). More than one sense per discourse. Paper presented at the 
SENSEVAL Workshop, Herstmonceux Castle. 
Kuncheva, L. I. (2004). Combining pattern classifiers : Methods and algorithms. 
Hoboken, NJ: J. Wiley. 
Lakoff, G. (1987). Women, fire, and dangerous things : what categories reveal about the 
mind. Chicago: University of Chicago Press. 
Leininger, K. (2000). Interindexer consistency in PsycINFO. Journal of Librarianship 
and Information Science, 32(1), 4-8. 
Levinson, S. C. (2000). Presumptive meanings : the theory of generalized conversational 
implicature. Cambridge, Mass.: MIT Press. 
Lewis, D. D., & Spärck Jones, K. (1996). Natural language Processing for Information 
Retrieval. Communications of the ACM, 39(1), 92-101. 
Li, M., & Zhou, Z.-H. (2007). Improve Computer-Aided Diagnosis With Machine 
Learning Techniques Using Undiagnosed Samples. IEEE Transactions on 
Systems, Man & Cybernetics: Part A, 37(6), 1088-1098. 
 177 
Liu, Y.-Z., Jiang, Y.-C., Liu, X., & Yang, S.-L. (2008). CSMC: A combination strategy 
for multi-class classification based on multiple association rules. Knowledge-
Based Systems, 21(8), 786-793. 
Long, W. (2005). Extracting diagnoses from discharge summaries. Paper presented at the 
Proceedings of the AMIA symposium. 
Lovins, J. B. (1968). Development of a stemming algorithm. Mechanical Translations, 
11, 22-31. 
Lovins, J. B. (1971). Error Evaluation for Stemming Algorithms as Clustering 
Algorithms. Journal of the American Society for Information Science, 22(1), 28-
40. 
Luhn, H. P. (1958). The automatic creation of literature abstracts. IBM Journal of 
Research and Development(April 1958), 159-165. 
Lussier, Y. A., Shagina, L., & Friedman, C. (2000). Automating ICD-9-CM encoding 
using medical language processing: a feasibility study. Paper presented at the 
Proceedings of the AMIA symposium. 
Manin, D. Y. (2008). Zipf's Law and Avoidance of Excessive Synonymy. Cognitive 
Science, 32(7), 1075-1098. 
Manning, C. D., & Schütze, H. (1999). Foundations of statistical natural language 
processing. Cambridge, Mass.: MIT Press. 
Marshall, I. (1983). Choice of grammatical word-class without global syntactic analysis: 
tagging words in the LOB Corpus. Computers and the Humanities, 17(3), 139-
150. 
 178 
Marshall, I. (1987). Tag selection using probabilistic methods. In R. G. Garside, G. N. 
Leech & G. Sampson (Eds.), The Computational analysis of English : a corpus-
based approach (pp. 42-65). London ; New York: Longman. 
Martinez, D., & Agirre, E. (2000). One sense per collocation and genre/topic variations. 
Paper presented at the Joint SIGDAT conference on Empirical methods in natural 
language processing and very large corpora. 
McRoy, S. W. (1992). Using Multiple Knowledge Sources for Word Sense 
Discrimination. Computational Linguistics, 18(1), 1-30. 
Medelyan, O., & Witten, I. H. (2008). Domain-independent automatic keyphrase indexing 
with small training sets. Journal of the American Society for Information Science 
and Technology, 59(7), 1026-1040. 
Mendonca, E. A., Haas, J., Shagina, L., Larson, E., & Friedman, C. (2005). Extracting 
information on pneumonia in infants using natural language processing of 
radiology reports. Journal of Biomedical Informatics, 38(4), 314-321. 
Michalski, R. S., & Stepp, R. E. (1983). Learning from observation: conceptual 
clustering. In J. R. Anderson, R. S. Michalski, J. G. Carbonell & T. M. Mitchell 
(Eds.), Machine learning : an artificial intelligence approach. Los Altos, Calif.: 
M. Kaufmann. 
Mikheev, A. (2003). Text Segmentation. In R. Mitkov (Ed.), The Oxford Handbook of 
Computational Linguistics (pp. 201-218). Oxford ; New York: Oxford University 
Press. 
Miller, G. A. (1995). WordNet: a lexical database for English. Communications of the 
ACM, 38(11), 39-41. 
 179 
Miller, G. A., Beckwith, R., Fellbaum, C., Gross, D., & Miller, K. J. (1990). Introduction 
to wordnet: An on-line lexical database. International Journal of lexicography, 
3(4), 235-244. 
Mitchell, T. M. (1999). The role of unlabeled data in supervised learning. Paper 
presented at the Sixth International Colloquium on Cognitive Science (ICIS-99), 
San Sebastian, Spain. 
Mitchell, T. M., Buchanan, B., DeJong, G., Dietterich, T., Rosenbloom, P., & Waibel, A. 
(1990). Machine learning. In J. F. Traub, B. J. Grosz, B. W. Lampson & N. J. 
Nilsson (Eds.), Annual review of computer science. Vol.4. Palo Alto, CA: Annual 
Reviews. 
Mosteller, F., & Wallace, D. L. (1963). Inference in an authorship problem. Journal Of 
The American Statistical Association, 58(302), 275-309. 
Mutalik, P. G., Deshpande, A., & Nadkarni, P. M. (2001). Use of general-purpose 
negation detection to augment concept indexing of medical documents: a 
quantitative study using the UMLS. Journal of the American Medical Informatics 
Association, 8(6), 598-609. 
Nadkarni, P., Chen, R., & Brandt, C. (2001). UMLS concept indexing for production 
databases: a feasibility study. Journal of the American Medical Informatics 
Association, 8(1), 80-91. 
National Center for Health Statistics. (2007). International Classification of Diseases, 
Ninth Revision, Clinical Modification (ICD-9-CM).   Retrieved April 10, 2007, 
from http://cdc.gov/nchs/about/otheract/icd9/abticd9.htm 
 180 
National Information Standards Organization (U.S.), & American National Standards 
Institute. (2006). Guidelines for the construction, format, and management of 
monolingual controlled vocabularies : an American national standard. Bethesda, 
Md.: National Information Standards Organization. 
Nigam, K., & Ghani, R. (2000). Analyzing the effectiveness and applicability of co-
training. Paper presented at the Ninth international conference on Information and 
knowledge management, McLean, Virginia, United States. 
Nigam, K., McCallum, A. K., Thrun, S., & Mitchell, T. M. (2000). Text classification 
from labeled and unlabeled documents using EM. Machine learning, 39(2), 103-
134. 
Olson, H. A., & Wolfram, D. (2008). Syntagmatic relationships and indexing consistency 
on a larger scale. Journal of Documentation, 64(4), 602-615. 
Özgür, A., Özgür, L., & Güngör, T. (2005). Text Categorization with Class-Based and 
Corpus-Based Keyword Selection. Paper presented at the ISCIS 2005, Istanbul, 
Turkey. 
Paek, T., & Pieraccini, R. (2008). Automating spoken dialogue management design using 
machine learning: an industry perspective. Speech Communication, 50(8), 716-
729. 
Passos, A., & Wainer, J. (2009). Wordnet-based metrics do not seem to help document 
clustering. Paper presented at the International Workshop on Web and Text 
Intelligence (WTI - 2009). 
 181 
Patrick, J., Wang, Y., & Budd, P. (2007). An automated system for conversion of clinical 
notes into SNOMED clinical terminology. Paper presented at the The fifth 
Australasian symposium on ACSW frontiers. 
Pearl, J. (1988). Probabilistic reasoning in intelligent systems : networks of plausible 
inference. San Mateo, Calif.: Morgan Kaufmann Publishers. 
Pedersen, T., Pakhomov, S. V., Patwardhan, S., & Chute, C. G. (2007). Measures of 
semantic similarity and relatedness in the biomedical domain. Journal Of 
Biomedical Informatics, 40(3), 288-299. 
Pereira, F., Tishby, N., & Lee, L. (1993). Distributional clustering of English words. 
Paper presented at the 31st Annual Meeting of the ACL. 
Pestian, J. P., Brew, C., Matykiewicz, P., Hovermale, D. J., Johnson, N., Cohen, K. B., et 
al. (2007). A shared task involving multi-label classification of clinical free text. 
Paper presented at the ACL:BioNLP, Prague. 
Polat, K., & Güneş, S. (2008). A novel hybrid intelligent method based on C4.5 decision 
tree classifier and one-against-all approach for multi-class classification problems. 
Expert Systems with Applications, In Press, Corrected Proof. 
Polat, K., & Güneş, S. (2009). A novel hybrid intelligent method based on C4.5 decision 
tree classifier and one-against-all approach for multi-class classification problems. 
Expert Systems with Applications, 36(2), 1587-1592. 
Porter, M. F. (1980). An Algorithm for Suffix Stripping Program, 14(3), 130-137. 
Porter, M. F. (2006). The Porter Stemming Algorithm.   Retrieved May 1,, 2010, from 
tartarus.org/~martin/PorterStemmer/ 
 182 
Puckett, C. D. (1986). The Educational annotation of ICD-9-CM. Reno, Nev. (P.O. Box 
70723, Reno 89570): Channel Pub. 
Purandare, A., & Pedersen, T. (2004). SenseClusters: finding clusters that represent word 
senses. Paper presented at the Proceedings. Nineteenth National Conference on 
Artificial Intelligence (AAAI-04). Sixteenth Innovative Applications of Artificial 
Intelligence Conference (IAAI-04). 
Quinlan, J. R. (1986). Induction of decision trees. Machine Learning, 1(1), 81-106. 
Quinlan, J. R. (1993). C4.5 : Programs for Machine Learning. San Mateo, Calif.: 
Morgan Kaufmann Publishers. 
Rao, R. B., Sandilya, S., Niculescu, R. S., Germond, C., & Rao, H. (2003). Clinical and 
financial outcomes analysis with existing hospital patient records. Paper 
presented at the Proceedings of the Ninth ACM SIGKDD International 
Conference on Knowledge Discovery and Data Mining. 
Raskutti, B., & Kowalczyk, A. (2004). Extreme re-balancing for SVMs: a case study. 
SIGKDD Explor. Newsl., 6(1), 60-69. 
van Rijsbergen, C. J. (1979). Information retrieval (2d ed.). London ; Boston: 
Butterworths. 
Ruch, P. (2006). Automatic assignment of biomedical categories: toward a generic 
approach. Bioinformatics (Oxford, England), 22(6), 658-664. 
Ryan, R., & Aldebaro, K. (2004). In Defense of One-Vs-All Classification. The Journal 
of Machine Learning Research, 5, 101-141. 
 183 
Sager, N. (1976). Evaluation of Automated Natural Language Processing in the Further 
Development of Science Information Retrieval. String Program Reports No. 10: 
New York University, Linguistic String Project. 
Salton, G., & Buckley, C. (1988). Term-weighting approaches in automatic text retrieval. 
Information Processing & Management, 24(5), 513-523. 
Salton, G., & McGill, M. J. (1983). Introduction to modern information retrieval. New 
York: McGraw-Hill. 
Schapire, R. E. (1990). The Strength of Weak Learnability. Machine Learning, 5(2), 197-
227. 
Schapire, R. E., & Singer, Y. (2000). BoosTexter: a boosting-based system for text 
categorization. Machine Learning, 39(2), 135-168. 
Schütze, H. (1998). Automatic word sense discrimination. Computational Linguistics, 
24(1), 97-123. 
Scott, S., & Matwin, S. (1998). Text Classification Using WordNet Hypernyms. Paper 
presented at the Workshop on Usage of WordNet for Natural Language 
Processing Systems (COLING/ACL'98), Montreal. 
Sebastiani, F. (2002). Machine learning in automated text categorization. ACM 
Computing Surveys (CSUR), 34(1), 1-47. 
Shannon, C. E., & Weaver, W. (1949). The mathematical theory of communication. 
Urbana,: University of Illinois Press. 
Shapiro, A. R. (2004). Taming variability in free text: application to health surveillance. 
MMWR. Morbidity And Mortality Weekly Report, 53 Suppl, 95-100. 
 184 
Sibanda, T. C. (2006). Was the patient cured? Understanding semantic categories and 
their relationships in patient records. Unpublished Masters Thesis, MIT, 
Cambridge. 
Sibanda, T. C., He, T., Szolovits, P., & Uzuner, Ö. (2006). Syntactically-informed 
semantic category recognition in discharge summaries. Paper presented at the 
Proceedings of the AMIA symposium. 
Somers, H. (1999). Review Article: Example-Based Machine Translation. Machine 
Translation, 14(2), 113-157. 
Spärck Jones, K. (2007). Automatic summarising: The state of the art. Information 
Processing & Management, 43(6), 1449-1481. 
Spat, S., Cadonna, B., Rakovac, I., Gütl, C., Leitner, H., Stark, G., et al. (2008). Enhanced 
information retrieval from narrative German-language clinical text documents 
using automated document classification. Studies In Health Technology And 
Informatics, 136, 473-478. 
Stamatatos, E. (2009). A survey of modern authorship attribution methods. Journal of the 
American Society for Information Science & Technology, 60(3), 538-556. 
Standards for Privacy of Individually Identifiable Health Information. (2000). Department 
of Health & Human Services,, 45 CFR §160 and 164. 
Studer, R., Benjamins, V. R., & Fensel, D. (1998). Knowledge engineering: principles 
and methods. Data & Knowledge Engineering, 25(1), 161-197. 
Suchan, J. (1995). The Influence of Organizational Metaphors on Writers' 
Communication Roles and Stylistic Choices. Journal of Business Communication, 
32(1), 7-29. 
 185 
Sweeney, L. (2001). Information explosion. In L. Zayatz, P. Doyle, J. Theeuwes & J. 
Lane (Eds.), Confidentiality, disclosure, and data access: Theory and practical 
applications for statistical agencies (pp. 43-74). Washington, DC: Urban Institute. 
Taira, R. K., Soderland, S. G., & Jakobovits, R. M. (2001). Automatic structuring of 
radiology free-text reports. Radiographics: A Review Publication Of The 
Radiological Society Of North America, Inc, 21(1), 237-245. 
Takahashi, K., Takamura, H., & Okumura, A. (2005). Automatic occupation coding with 
combination of machine learning and hand-crafted rules. Paper presented at the 
Advances in Knowledge Discovery and Data Mining. 9th Pacific-Asia 
Conference, PAKDD 2005., Hanoi, Vietnam. 
Tang, L., & Liu, H. (2005). Bias Analysis in Text Classification for Highly Skewed Data. 
Paper presented at the Fifth IEEE International Conference on Data Mining 
(ICDM'05), Houston, TX. 
Tax, D. M. J., van Breukelen, M., Duin, R. P. W., & Josef, K. (2000). Combining 
multiple classifiers by averaging or by multiplying? Pattern Recognition, 33(9), 
1475-1485. 
Taylor, A. G. (2004). The organization of information (2nd ed.). Westport, Conn.: 
Libraries Unlimited. 
Thomas, B. J., Ouellette, H., Halpern, E. F., & Rosenthal, D. I. (2005). Automated 
computer-assisted categorization of radiology reports. AJR. American Journal Of 
Roentgenology, 184(2), 687-690. 
 186 
Tsui, F.-C., Wagner, M. M., Dato, V., & Chang, C.-C. H. (2002). Value of ICD-9-coded 
chief complaints for detection of epidemics. Journal of the American Medical 
Informatics Association, 9(6), S41-47. 
Tulane University. (2006). Guidelines for ICD-9 (Diagnosis Code) selection.   Retrieved 
25 August 2007, from 
http://www.tulane.edu/~contract/Rework_Sep06/PDF%20files/ICD-
9%20Codes%20Part%201.pdf 
Tzoukermann, E., Klavans, J. L., & Strzalkowski, T. (2003). Information Retrieval. In R. 
Mitkov (Ed.), The Oxford Handbook of Computational Linguistics (pp. 529-544). 
Oxford ; New York: Oxford University Press. 
Uzuner, Ö. (2008). Second i2b2 workshop on natural language processing challenges for 
clinical records. Proceedings of the AMIA symposium, 1252-1253. 
Uzuner, Ö., Goldstein, I., Luo, Y., & Kohane, I. (2008). Identifying patient smoking 
status from medical discharge records. Journal Of The American Medical 
Informatics Association: JAMIA, 15(1), 14-24. 
Vapnik, V. N. (1995). The nature of statistical learning theory. New York: Springer. 
Voutilainen, A. (2003). Part-of-speech tagging. In R. Mitkov (Ed.), The Oxford 
Handbook of Computational Linguistics (pp. 219-232). Oxford ; New York: 
Oxford University Press. 
Wang, B. X., & Japkowicz, N. (2008). Boosting support vector machines for imbalanced 
data sets. Knowledge & Information Systems, 25(1), 1-20. 
Weaver, W. (1955). Translation. In W. N. Locke (Ed.), Machine Translation of 
Languages (pp. 15-23). 
 187 
Webb, G. I. (2000). MultiBoosting: a technique for combining boosting and wagging. 
Machine Learning, 40(2), 159-196. 
Weiner, J., & Solbrig, O. T. (1984). The meaning and measurement of size hierarchies in 
plant populations. Oecologia, 61(3), 334-336. 
Wilcox, A. B., & Hripcsak, G. (1999). Classification algorithms applied to narrative 
reports. Paper presented at the Proceedings of the AMIA symposium. 
Wilcox, A. B., & Hripcsak, G. (2003). The Role of Domain Knowledge in Automating 
Medical Text Report Classification (Vol. 10, pp. 330-338). 
Witten, I. H., & Frank, E. (2005). Data mining : practical machine learning tools and 
techniques (2nd ed.). Amsterdam ; Boston, MA: Morgan Kaufman. 
Wolpert, D. H. (1992). Stacked generalization. Neural Networks, 5(2), 241-259. 
World Health Organization. (2009). International Classification of Diseases (ICD).   
Retrieved October 24, 2009, from 
http://www.who.int/classifications/icd/en/index.html 
Xu, J., & Croft, W. B. (2000). Improving the Effectiveness of Information Retrieval with 
Local Context Analysis. ACM Trans. Inf. Syst., 18(1), 79-112. 
Yang, Y., & Liu, X. (1999). A re-examination of text categorization methods. Paper 
presented at the Proceedings of SIGIR: International Conference on R&D in 
Information Retrieval. 
Yarowsky, D. (1993). One sense per collocation. Paper presented at the ARPA Human 
Language Tecnology Workshop, Princeton, NJ. 
Zenko, B., Todorovski, L., Dzeroski, S., Cercone, N., Lin, T. Y., Wu, X., et al. (2001). A 
comparison of stacking with meta decision trees to bagging, boosting, and 
 188 
stacking with other methods. Paper presented at the Proceedings 2001 IEEE 
International Conference on Data Mining, San Jose, CA. 
Zhong, S. (2005). Semi-supervised sequence classification with HMMs. International 
Journal of Pattern Recognition & Artificial Intelligence, 19(2), 165-182. 
Zhou, Z.-H. (2009). When Semi-supervised Learning Meets Ensemble Learning. In 
Multiple Classifier Systems (pp. 529-538): Springer. 
Zipf, G. K. (1949). Human behavior and the principle of least effort; an introduction to 
human ecology. Cambridge, Mass.,: Addison-Wesley Press. 
 
 
 189 
Appendix 
A. Instructions for Annotating Semantic Correctness  
   
We want to examine our system’s accuracy in assigning preferred terms to noun phrases.  
   
Each tagged noun phrase (NP) is in the format:  
<dis> Preferred Term :: NP :: CUI :: Semantic Code </dis>  
   
Each sentence to be annotated will contain one tagged noun phrase. Sentences with more 
than one noun phrase may be listed multiple times, once for each of the noun phrases. For 
example:  
5-year - old  with  <dis> Coughing :: cough  :: C0010200 :: sosy </dis>  and  chest pain  
5-year - old  with  cough  and  <dis> Chest Pain :: chest pain  :: C0008031 :: sosy </dis>  
   
Sentences may be as short as one word (for example “Cough.”).  
   
A given noun phrase will quite often occur in more than one sentence (for example 
“Cough and fever.” and “Wheezing cough.”). The annotation needs to be made given the 
context of the noun phrase.  
   
Each sentence needs to be annotated with a primary notation. If appropriate, a sentence 
may be assigned an optional secondary notation.  
   
Annotation codes:  
Primary Notation:  
P – Preferred Term is NP, morph of NP, or contained in NP. Examples:  
<dis> Focal pneumonia :: focal pneumonia  :: C1290339 :: dsyn </dis>  
<dis> Coughing :: Cough  :: C0010200 :: sosy </dis>  
<dis> Chest Pain :: Chest pain right side  :: C0008031 :: sosy </dis>  
   
S - Preferred Term is a synonym for the NP, or a synonym for the noun contained in the 
NP.  
In these examples, the preferred term is a synonym for the full NP:  
<dis> Urinary tract infection :: UTI  :: C0042029 :: dsyn </dis>  
<dis> Bedwetting :: Nocturnal enuresis  :: C0270327 :: sosy </dis>  
   
In this case the preferred term is a synonym for the noun part of the NP:  
<dis> Urinary tract infection :: first afebrile UTI  :: C0042029 :: dsyn </dis>  
   
B – Preferred term is a broader term than the NP. Example:  
<dis> Disease :: Air space disease  :: C0012634 :: dsyn </dis>  
Please note the narrower term if it is known.  
 190 
   
N – Preferred term is a narrower term than the NP Example:  
<dis> Abdominal Pain :: pain  :: C0000737 :: sosy </dis>  
<dis> Abuse of steroids :: steroids  :: C0338671 :: mobd </dis>    
   
U – Preferred term is unrelated to the NP, or based upon context the wrong sense of the 
NP. Examples:  
<dis> Diabetic Retinopathy :: Dr. Bruce Dorie  :: C0011884 :: dsyn </dis>  
<dis> Aggressive behavior :: aggressive bronchodilators  :: C0001807 :: mobd </dis>  
<dis> Bacterial stain, routine :: 2 grams IV q six hours  :: C0200966 :: lbpr </dis>  
 
   
   
Optional secondary notation:  
I – The NP is incorrectly parsed and contains a second NP. Examples:  
<dis> Pneumonia :: fever rule - out pneumonia  :: C0032285 :: dsyn </dis>  
<dis> Coughing :: Fever cough  :: C0010200 :: sosy </dis>  
   
M – The NP contains a misspelling which caused a first notation other than P or S. 
Example:  
<dis> Injury :: final cord injury  :: C0175677 :: inpo </dis>   (Would be a “B”)  
 
 191 
B. Abbreviations 
Abbreviation Term 
ANN Artificial Neural Network 
BRB Base Rule-Based 
CART Classification and Regression Trees 
df Document Frequency 
FN False Negatives 
FP False Positives 
FRB Full Rule-Based 
HMM Hidden Markov Model 
i2b2 Integrating Biology and the Bedside  
ICD International Classification of Diseases  
ICD-9 9th Revision of the International Classification of Diseases  
ICD-9-CM Clinical Modification to the 9th Revision of the International Classification of Diseases  
idf Inverse Document Frequency 
IRB Institutional Review Board 
k-NN k-nearest neighbor  
MeSH Medical Subject Headings  
ML Machine Learning 
MLP Multilayer Perceptron  
NLP Natural Language Processing 
NP Noun Phrase 
NPDP Noun Phrase Detection Pre-processor  
OVA One-Versus-All 
POS Part of speech 
SVM Support Vector Machine 
tf Term Frequency 
TP True Positives 
UMLS Unified Medical Language System  
WSD Word Sense Disambiguation 
Table 30 - Abbreviations used. 
