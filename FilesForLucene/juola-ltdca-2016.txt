Did Aunt Prunella Really Write That Will? A Simple and
Understandable Computational Assessment of Authorial
Likelihood
Patrick Juola
Juola & Associates
276 W Schwab Ave
Munhall, PA 15120 USA
juola@mathcs.duq.edu
ABSTRACT
Authorship attribution, the determination of the authorship
of a disputed document by analyzing the document itself, is
an important legal and technical question. By studying as-
pects of writing style such as word choice, punctuation, and
preferred syntax, statistical analysis can produce probabilis-
tic assessments of the likelihood of any specific author. We
have developed a program called Envelope that implements
a proposed protocol [23] using an ad hoc distractor set, five
separate analyses, and Fisher’s exact test on the rank order
sum as a data fusion method. We discuss testing results and
some suggested extensions and improvements.
CCS Concepts
•Security and privacy → Human and societal aspects
of security and privacy; •Computing methodologies
→ Information extraction; •Applied computing →
Investigation techniques; Evidence collection, stor-
age and analysis;
Keywords
authorship attribution, text classification, digital evidence
1. INTRODUCTION
The authorship of documents is a key question in many
legal cases, as a skim of many of Agatha Christie’s nov-
els will show. Electronic documents [6, 16, 17] bring their
own set of issues, as handwriting cannot be used to vali-
date the documents. Stylometry, the study of individual
writing style [15, 31], can. In the case of Ceglia v. Zucker-
berg, et al.[27], for example, ownership of a significant part
of Facebook depended in part on the validity of an emailed
agreement ostensibly between the two parties.
In this paper, we describe the authorship attribution prob-
lem, with some examples. We then describe a formal proto-
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full cita-
tion on the first page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
LTDCA ’16 San Diego, CA
c© 2016 ACM. ISBN 123-4567-24-567/08/06. . . $15.00
DOI: 10.475/123 4
col to address this kind of problem and present a software
system (Envelope) that implements this protocol in a simple
and easy-to-use way.
2. BACKGROUND
2.1 Authorship analysis
Language is among the most individualized activities peo-
ple engage in. For this reason, much can be learned about
a person by looking at his or her writings. An easy example
is distinguishing between different regional groups. A Com-
monwealth English speaker/writer can easily be spotted by
her use of “lorry” instead of “truck,” spelling “labor” with a
‘u,’ and less obviously by grammatical constructions such as
“could do” or “in hospital.” These insights can be extended
to questions of authorship without regard to handwriting.
The basic theory of traditional stylistics is fairly simple. As
McMenamin [27] describes it,
At any given moment, a writer picks and chooses
just those elements of language that will best
communicate what he/she wants to say. The
writer’s“choice”of available alternate forms is of-
ten determined by external conditions and then
becomes the unconscious result of habitually us-
ing one form instead of another. Individuality
in writing style results from a given writer’s own
unique set of habitual linguistic choices.[27]
Coulthard’s [8] description is also apt:
The underlying linguistic theory is that all
speaker/writers of a given language have their
own personal form of that language, technically
labeled an idiolect. A speaker/writer’s idiolect
will manifest itself in distinctive and cumulatively
unique rule-governed choices for encoding mean-
ing linguistically in the written and spoken com-
munications they produce. For example, in the
case of vocabulary, every speaker/writer has a
very large learned and stored set of words built
up over many years. Such sets may differ slightly
or considerably from the word sets that all other
speaker/writers have similarly built up, in terms
both of stored individual items in their passive
vocabulary and, more importantly, in terms of
their preferences for selecting and then combin-
 37 
ing these individual items in the production of
texts. [8]
These choices express themselves in a number of ways. In
an expert witness report, McMenamin [27] analyzed eleven
different and distinct “features” of the writing in both a
known (undisputed) email and a disputed email. One fea-
ture, for example, hinged on the spelling of the word cannot,
and in particular whether it was written as one word (can-
not) or as two (can not). Another feature was the use of
the single word “sorry” as a sentence opener (as opposed,
for example, to “I’m sorry”). Coulthard similarly discussed
(among other features) the use of the specific phrase “dis-
gruntled employees.” (Why “disgruntled” and not one of its
myriad synonyms?) In both cases, significant differences in
these features can be held as evidence of differences in au-
thorship.
2.2 Computational text analysis
Computer-based stylometry applies the same general the-
ory, but with a few major differences. The basic assump-
tion that people make individual choices about language still
holds, but instead of ad hoc features selected by examina-
tion of the specific documents, the analysts use more general
feature sets that apply across the spectrum of problems.[3,
1, 13, 32, 28, 24] Using these feature sets or others [30], the
features present in a document are automatically identified,
gathered into collections of feature representations (such as
vector spaces), and then classified using ordinary classifica-
tion methods [15, 29, 26, 14, 20] to establish the most likely
author.
Binongo [1] provides a clear example of this. In his study
of the authorship of the Oz books, he collected the frequen-
cies of the fifty most frequent words in English from the
books of undisputed authorship (his feature set). He applied
principle component analysis (his classification method) to
obtain a data visualization of the stylistic differences, then
showed that the disputed 13th book clearly lay in the stylis-
tic space corresponding only to one candidate author. This
would clearly be highly relevant evidence if the authorship
(perhaps for copyright reasons) were being disputed in court.
From a legal standpoint, there are three key issues with
this technology. The first, admissibility, has been addressed
in detail elsewhere [5, 8, 23], but is closely tied to the second
issue, the scientific validity of the technology itself. Numer-
ous surveys [15, 26, 14, 31] and TREC-style conferences [33]
have shown that authorship can be determined with high
accuracy (typically 80% or better) using realistically-sized
samples. Large-scale studies [20, 34] have confirmed that
there are often many different “best practices” that perform
well based on different features. This allows for ordinary
data fusion techniques such as mixture-of-experts [18, 10] to
boost accuracy rates to practical levels.
3. JUOLA’S PROPOSED PROTOCOL
The usefulness of the above technology has been demon-
strated in actual disputes. For example, Collins [7] used mix-
ture of experts to validate a newly discovered short story by
Edgar Allan Poe, and Juola [2, 21] used a similar method to
identify J.K. Rowling as the author of A Cuckoo’s Calling.
In a legal context, Juola [22] was able to validate authorship
of anonymous newspaper columns in support of an asylum
claim in US immigration court.
Partly to address admissibility needs and partly to address
the needs of a lawyer, judge, and jury to understand this
sort of evidence, Juola [23] proposed a formal protocol for
handling and developing this type of authorial evidence. Key
elements of this proposed protocol are:
• Suitable data for analysis, including an ad hoc set of
distractor authors not believed to be connected to the
case,
• A set of independent analysis methods that have been
validated to perform well on similar tasks
• A predefined data fusion framework amenable to for-
mal statistical analysis, so that the likelihood of error
can be assessed mathematically.
Juola [23] showed how this protocol could be applied sev-
eral separate authorship disputes. We have implemented
this protocol in a SaaS platform, named Envelope, to provide
low-cost, high-accuracy resolution of authorship disputes.
4. ENVELOPE: OUR IMPLEMENTATION
Envelope, in its current version, focuses on a specific (and
relatively common) type of disputed document, electronic
mail [6] written in English. The system is presented with
client-supplied copies of the disputed email(s) as well as
samples known to be by the purported author. These docu-
ments are compared against a set of distractor authors (cur-
rently a set of ten gender-balanced authors extracted from
the Enron corpus [25]) and rank-ordered for similarity along
five human-understandable features that have been shown
to work well in large-scale testing [20, 33].
The five measured dimensions are as follows:
• Authorial Vocabulary (Vocabulary overlap) : Words
are, of course, what a work is fundamentally all about.
A crime novel is usually about a dead body and how
people deal with the problem it poses? a romance
novel is about a small group of people and their feelings
for each other. Even email differs in word choices as
discussed above [27, 8]. Authorial vocabulary is also
one of the best ways to tell individual writers apart,
by looking at the choices they make, not only in the
concepts they try to express, but the specific words
they use to create their own individual expression. The
degree of shared vocabulary is thus a key authorial
indicator.
• Expressive Complexity (Word lengths) : One key at-
tribute of authors is, on the one hand, their complexity,
and on the other, their readability. A precise author
who uses exactly the specific word to every event —
“that’s not a car, that’s a Cadillac? that’s not a cat,
but a tabby” —will more or less be forced to use rarer
words. These rarer words, by their very nature, are
longer [35]. A large and complex vocabulary will nat-
urally be reflected in larger words, producing a very
distinctive style of writing. By tracking the distribu-
tion of word lengths (n.b., not just the average word
length, which is known not to perform well), we can
assess the expressive complexity of a given author.
• Character n-grams : In addition to comparing words
directly, scholarship has shown [29, 32, 28] that com-
38  
parison of character clusters (for example, four adja-
cent letters, whether as part of a word like “eXAM-
Ple” or across two words as in “iNTHe”). This allows
matching of similar but not identical words, such as
different forms of the same stem or words with similar
affixes, and even preferred combinations of words.
• Function words : One of the most telling and oft-
studied aspects of an individual writer is their use of
function words [3, 13, 1], the simple, short, common,
and almost meaningless words that form a substantial
fraction of English writing. (To understand how these
words lack meaning, consider that the three most com-
mon words in English are“the,”“a/an”, and“of.” What
would you offer as a dictionary definition of “the”?)
These words are called “function words” because they
do not carry meaning of their own, but instead de-
scribe the functions of the other words in the sentence
in relation to each other. These words thus provide a
good indication of the tone of the writing and the spe-
cific types of relationships expressed throughout the
manuscript.
• Punctuation : Although not necessarily linguistically
interesting, and often the choice of editor instead of
author, punctuation offers an insight into social con-
ventions that have little effect on the meaning of the
text. Because they have little effect, they are often
freely variable between authors. For example, an au-
thor’s decision to use an Oxford comma, their choice of
marking extraneous material (for example, with com-
mas, parentheses, or brackets], the way they split sen-
tences with semicolons, periods, or comma splices, and
even whether punctuation is put inside or outside quo-
tation marks, do not change the meaning. In unedited
documents (such as email), they therefore provide a
strongly topic-independent cue to authorship that is
not directly related to the other dimensions. (See [27]
for some non-computational examples.)
Along each document, the eleven possible authors (ten im-
plausible distractor authors plus one plausible suspect) are
thus ranked from #1 (most similar/likely) to #11. The rank
sum of the purported author across all dimensions is calcu-
lated and used to fuse the different analysss. For example,
if the purported author scored as the most similar author
on all five dimensions (the most compelling possible result),
the rank sum would be five. The system then uses Fisher’s
exact test [9] to determine a likelihood that the specific ex-
perimental result could have been obtained by chance.
In more detail, we consider the null hypothesis that the
disputed document was not written by the purported au-
thor, and that there is, in fact, no relation between them.
Under this assumption, the purported author would rank
anywhere from #1 to #11 (with equal probability), averag-
ing at the sixth slot. Consistently appearing closer than the
sixth slot, then, is evidence of systematic similarity between
the two authors across a variety of independent stylometric
variables. An unrelated person is unlikely to show this kind
of systematic similarity, and hence if the calculated rank
sum is small enough, we can reject the null hypothesis at
any specific alpha cutoff desired. The system as currently
developed uses standard cutoffs: if the p-value is 0.05 or
less, we consider this to be “strong indications of common
authorship,” while trend-level values (p-value of 0.10 or less)
are “indications of common authorship.” “Weak indications”
occur at p-values of 0.20 or less. Inconclusive or outright
contraindications are handled appropriately.
The system is therefore capable of delivering a sophisti-
cated stylometric analysis quickly, cheaply, and without hu-
man intervention (thereby minimizing analyst bias effects).
5. ACCURACY AND VALIDITY
To enhance validity, the system performs a certain amount
of data validation. Both the known and disputed documents
need to be of sufficient length (currently defined as =< 200
words), and cannot include header information (which can
be picked up, for example, by looking for From: lines). Fur-
thermore, the documents must be in English [4] (which we
currently approximate by confirming the existence of “the”
in the files). Violations of these conditions are documented
in the generated report but do not prevent analysis; more
sophisticated (and expensive) human-based analysis may be
necessary in these circumstances. For example, stylometric
analysis technology is known to transfer well between lan-
guages [19, 11, 12], but a new distractor corpus would be
necessary.
The accuracy of this system has been preliminarily tested
on a variety of other email samples drawn from 20 additional
authors in the Enron [25] corpus. Out of 375 trials, 179 pro-
duced“strong” indications of authorship, and all 179 (100%)
were correct. Similarly, “Weak” indications were correct in
21 of 23 cases (91%). Only 2 cases showed just “indications”,
and 1 of those 2 (150%) was correct. “Weak” indications
were correct in 22 of 23 cases, while the remaining 43 incon-
clusive cases could not be validated, but showed significant
numbers of both same (6) and different (37) author pairs.
Thus, as expected, this method does not return an answer
in all cases, but when an answer is returned, the accuracy is
very high.
6. DISCUSSION AND CONCLUSIONS
The Envelope system thus delivers a high-quality analysis
at low cost. Being fully automatic, the analysis is repro-
ducible and is not influenced by analyst bias in any specific
case. The probability of error has been confirmed empiri-
cally to be low, and the estimated error rate (in the form
of the calculated p−value) is realistic. It is easy to extend
the current system to additional languages, additional doc-
ument types, or even additional classification tasks such as
author profiling.
Interpreting an Envelope report can be fairly straightfor-
ward. In the event of a finding of similarity, this means that
the two documents were shown to be highly similar across
a very wide range of linguistic and stylistic characteristics.
If the author of the email was not Aunt Prunella, it was, at
a minimum, someone who used Aunt Prunella’s characteris-
tic vocabulary, her characteristic syntax, her characteristic
style of punctuation, and even used the little words (“func-
tion words”) in the same way that she did. The computer can
characterize the likelihood of this kind of match occurring
from a person off the street with high precision and high
reliability. As the old joke has it, “if it looks like a duck,
walks like a duck, and uses punctuation like a duck.. . . ”
There are a number of fairly obvious extensions and pos-
sible improvements. Extension to new genres and/or lan-
 39 
guages [19, 11, 12] can be as simple as the creation of a new
set of distractor documents. It may be possible to improve
the accuracy by the incorporation of other analysis methods
and feature sets (for example, the distribution of part-of-
speech tags), although high-level processing such as POS
tagging may limit its use in other languages. We continue
our preliminary testing and will be expanding our offerings
in terms of genre.
So, who did write Aunt Prunella’s will, or at least her
email setting out her last wishes? Who wrote the email os-
tensibly dividing up ownership of the startup, or revealing
confidential business information? Computational analysis,
as typified by Envelope, may not be able to provide definitive
answers, but the evidence it creates can provide valuable in-
formation to help guide investigations or suggest preliminary
conclusions. This system provides low-cost advice without
the time and cost of human analysis, while retaining high
accuracy.
7. REFERENCES
[1] J. N. G. Binongo. Who wrote the 15th book of Oz? an
application of multivariate analysis to authorship
attribution. Chance, 16(2):9–17, 2003.
[2] R. Brooks and C. Flyn. JK Rowling: The cuckoo in
crime novel nest. Sunday Times, 14 July, 2013.
[3] J. F. Burrows. ‘an ocean where each kind. . . ’ :
Statistical analysis and some major determinants of
literary style. Computers and the Humanities,
23(4-5):309–21, 1989.
[4] W. B. Cavnar and J. M. Trenkle. N-gram-based text
categorization. In In Proceedings of SDAIR-94, 3rd
Annual Symposium on Document Analysis and
Information Retrieval, pages 161–175, 1994.
[5] C. Chaski. Best practices and admissibility of forensics
author identification. Journal of Law and Policy,
XXI(2):333–376, 2013.
[6] C. E. Chaski. Who’s at the keyboard: Authorship
attribution in digital evidence investigations.
International Journal of Digital Evidence, 4(1):n/a,
2005. Electronic-only journal: http://www.ijde.org,
accessed 5.31.2007.
[7] P. Collins. Poe’s debut, hidden in plain sight. The
New Yorker, October, 2013.
[8] M. Coulthard. On admissible linguistic evidence.
Journal of Law and Policy, XXI(2):441–466, 2013.
[9] R. A. Fisher. The Design of Experiments. Macmillan,
9th edition, 1971. Original publication date 1935.
[10] A. Fridman, A. Stolerman, S. Acharya, P. Brennan,
P. Juola, R. Greenstadt, and M. Kam. Decision fusion
for multi-modal active authentication. IT Professional,
15:29–33, 2013.
[11] B. Hasanaj. Authorship attribution methods in
Albanian. In Duquesne University Graduate Student
Research Symposium, 2013.
[12] B. Hasanaj, E. Purnell, and P. Juola. Cross-linguistic
transference of authorship attribution. In Proceedings
of the International Quantitative Linguistic
Conference (QUALICO), 2014.
[13] D. L. Hoover. Delta prime? Literary and Linguistic
Computing, 19(4):477–495, 2004.
[14] M. L. Jockers and D. Witten. A comparative study of
machine learning methods for authorship attribution.
LLC, 25(2):215–23, 2010.
[15] P. Juola. Authorship attribution. Foundations and
Trends in Information Retrieval, 1(3), 2006.
[16] P. Juola. Authorship attribution for electronic
documents. In M. Olivier and S. Shenoi, editors,
Advances in Digital Forensics II, volume 222 of
International Federal for Information Processing,
pages 119–130. Springer, Boston, 2006.
[17] P. Juola. Future trends in authorship attribution. In
P. Craiger and S. Shenoi, editors, Advances in Digital
Forensics III, International Federal for Information
Processing, pages 119–132. Springer, Boston, 2007.
[18] P. Juola. Authorship attribution : What
mixture-of-experts says we don’t yet know. In
Proceedings of American Association for Corpus
Linguistics 2008, Provo, UT USA, March 2008.
[19] P. Juola. Cross-linguistic transference of authorship
attribution, or why english-only prototypes are
 40 
acceptable. In Proceedings of Digital Humanities 2009,
College Park, MD, 2009.
[20] P. Juola. Large-scale experiments in authorship
attribution. English Studies, 93(3):275–283, May 2012.
[21] P. Juola. How a computer program helped reveal
J. K. Rowling as author of A Cuckoo’s Calling.
Scientific American, August, 2013.
[22] P. Juola. Stylometry and immigration: A case study.
Journal of Law and Policy, XXI(2):287–298, 2013.
[23] P. Juola. The Rowling case: A proposed standard
protocol for authorship attribution. DSH (Digital
Scholarship in the Humanities), 2015.
[24] P. Juola, J. I. Noecker Jr, A. Stolerman, M. V. Ryan,
P. Brennan, and R. Greenstadt. Keyboard
behavior-based authentication for security. IT
Professional, 15:8–11, 2013.
[25] B. Klimt and Y. Yang. The Enron corpus: A new
dataset for email classification research. Machine
Learning: ECML 2004, pages 217–226, 2004.
[26] M. Koppel, J. Schler, and S. Argamon. Computational
methods in authorship attribution. Journal of the
American Society for Information Science and
Technology, 60(1):9–26, 2009.
[27] G. McMenamin. Declaration of Gerald McMenamin in
Ceglia v. Zuckerberg and Facebook,
WD 2012 WL 1392965 (W.D.N.Y)). Available online at
https://docs.justia.com/cases/federal/district-
courts/new-york/nywdce/1:2010cv00569/79861/50,
2011.
[28] G. K. Mikros and K. Perifanos. Authorship
attribution in greek tweets using multilevel author’s
n-gram profiles. In Papers from the 2013 AAAI Spring
Symposium ”Analyzing Microtext”, 25-27 March 2013,
Stanford, California, pages 17–23. AAAI Press, Palo
Alto, California, 2013.
[29] J. Noecker Jr. and P. Juola. Cosine distance
nearest-neighbor classification for authorship
attribution. In Proceedings of Digital Humanities
2009, College Park, MD, June 2009.
[30] J. Rudman. On determining a valid text for
non-traditional authorship attribution studies :
Editing, unediting, and de-editing. In Proc. 2003 Joint
International Conference of the Association for
Computers and the Humanities and the Association
for Literary and Linguistic Computing (ACH/ALLC
2003), Athens, GA, May 2003.
[31] E. Stamatatos. A survey of modern authorship
attribution methods. Journal of the American Society
for Information Science and Technology, 60(3):538–56,
2009.
[32] E. Stamatatos. On the robustness of authorship
attribution based on character n-gram features.
Journal of Law and Policy, XXI(2):420–440, 2013.
[33] E. Stamatatos, B. Stein, W. Daelemans, P. Juola,
A. Barrón-Cedeño, B. Verhoeven, and M. A.
Sanchez-Perez. Overview of the authorship
identification task at PAN 2014. In Proceedings of
PAN/CLEF 2014, Sheffield, UK, 2014.
[34] D. M. Vescovi. Best practices in authorship
attribution of English essays. Master’s thesis,
Duquesne University, 2011.
[35] G. K. Zipf. Human Behavior and the Principle of
Least Effort. Hafner Publishing Company, New York,
1949. Reprinted 1965.
 41 
