A Replicated Comparative Study of Source Code 
Authorship Attribution 
Matthew F. Tennyson 
Department of Computer Science & Information Systems 
Bradley University 
Peoria, IL, USA 
mtennyson@bradley.edu
Abstract— Source code authorship attribution is, simply, the 
task of deciding who wrote a piece of software given its source 
code. Applications include software forensics, plagiarism 
detection, and determining software ownership. Several methods 
of source code authorship attribution have been proposed in the
past. Based on the only known controlled, comprehensive 
comparative study of these methods, the two most effective 
methods are the Burrows method and the SCAP method. This 
paper presents a partial replication of that comparative study.
Specifically, it only compares the two most effective methods 
(Burrows and SCAP). This paper also includes a slight extension 
of that study: the original comparative study only considered 
anonymized data, while the replicated study considers both 
anonymized and non-anonymized data. The original comparative 
study indicated that the Burrows method outperformed all other 
methods – including the SCAP method – by a considerable 
margin. However, the results of the replicated study indicate that 
the SCAP method outperforms the Burrows method by a small 
margin when using anonymized data and by a large margin when 
using non-anonymized data. 
Keywords—authorship attribution; information retrieval; 
software forensics; plagiarism detection 
I. INTRODUCTION
The term "authorship attribution" refers simply to "the task 
of deciding who wrote a document" [1]. Authorship attribution 
of source code, then, is the task of deciding who wrote a source 
code document. Source code authorship attribution is a tenet of
software forensics, which is the process of analyzing software 
to identify characteristics of its authors for use in forensics 
activities [2]. The ultimate aim of software forensics is
typically author identification, and it is usually applied to
malicious code when analyzing software remnants left by an
attacker in order to identify the origin of the attack or
characteristics of the one who originated the attack.
Source code authorship attribution has many other 
applications outside of software forensics. In academia, the 
most obvious application is that of plagiarism detection on
programming assignments. In programming courses, students 
often plagiarize solutions to programming problems by
"borrowing" code from outside sources such as the Web, 
friends, or "Rent-A-Coder" services (without proper citations, 
of course) [14-16]. In industry, applications include activities 
related to configuration management and software ownership. 
In regards to configuration management, it could be important 
for author tracking or change control. In regards to software 
ownership, it could be important for the protection of trade 
secrets, patent claims, copyright infringement, or cases of
software theft [3]. 
The problem of source code authorship attribution is not 
equivalent to the problem of detecting copied programs or 
copied program segments, which is sometimes referred to as 
"code clone detection". Detecting cloned code can be reduced 
to the problem of determining whether any two segments of 
code are identical or significantly similar to each other, usually 
in a structural or functional sense. Source code authorship 
attribution, on the other hand, is determining whether a 
program is significantly similar (usually in a stylistic sense) to 
other programs written by the purported author, where the 
programs being compared are not structurally or functionally 
related whatsoever. 
An authorship attribution problem usually proceeds as 
follows: A document is encountered of unknown authorship. 
The document is compared to a corpus of documents of known 
authorship. The author from the corpus that is most similar to 
the unknown document is attributed to be its author. The 
measure of similarity is usually based in some way on style so 
as to answer the question, "Which author's style best matches 
the style in which the unknown document was written?" 
An authorship attribution experiment typically proceeds in 
a similar fashion: Several sample documents (of known 
authorship) are selected for experimental purposes. These 
documents are excluded from the corpus. An author is 
attributed to each of the samples using the experimental 
technique. The success is measured as a percentage of 
documents correctly attributed. In a closed form of the 
experiment, the actual author of the document is guaranteed to 
exist in the corpus. In an open form of the experiment, the 
actual author may not exist in the corpus. The open form of the 
experiment is obviously much harder, and no such studies of 
source code authorship attribution are known to have been 
published. Issues such as programming language, size of the 
samples, size of the corpus, etc., are variables that are either 
controlled or whose effects are measured experimentally. In 
most studies of source code authorship attribution, a "sample 
document" refers to a single source file. 
2013 Third International Workshop on Replication in Empirical Software Engineering Research
978-0-7695-5121-0/13 $26.00 © 2013 IEEE
DOI 10.1109/RESER.2013.12
76
This paper presents a comparative study of two of the most 
effective methods of source code authorship attribution. This 
comparative study is a partial replication of the Burrows 
comparative study [12], which is further discussed in the next 
section. 
II. LITERATURE REVIEW
A. Overview 
Several methods of source code authorship attribution have 
been proposed [3-11]. In 2010, Burrows performed a 
comparative study of these methods [12] and determined that 
the Burrows method [7] is the most effective. The Burrows 
study is the first known controlled, comprehensive comparative 
study of source code authorship attribution. An extended 
version of this comparative study was later published in [17].
The data set used in the study consisted of four collections,
referred to as Coll-A, Coll-T, Coll-P, and Coll-J. Coll-A and
Coll-T consisted of student-submitted solutions to
programming assignments written in C from RMIT University
in Melbourne, Australia. The two collections were
distinguished in that Coll-T contained relative timestamps
indicating when the programs were written, while Coll-A
contained no such timestamps. (The timestamps were used by 
Burrows in a separate experiment that investigated temporal 
effects on attribution results, but they had no bearing on the 
comparative study being discussed here.) Coll-P and Coll-J
both consisted of programs retrieved from the Planet Source
Code website, which proclaims to be "one of the largest public
source code databases on the internet" [13]. The two
collections were distinguished in that Coll-P contained
programs written in C/C++, while Coll-J contained programs
written in Java. The study itself consisted of a 10-class
experiment – that is, determining the author of a program from
a set of ten candidate authors. For each run of the experiment,
ten authors from the data set were randomly selected and all
programs written by those ten authors were used. A "leave-
one-out cross validation" experimental design was used – that
is, each program in the data set was selected, in turn, as a query
program while the remaining programs were used as the
training data.
 The results were measured in terms of accuracy – that is, 
as a percentage of programs correctly identified. The results of
the study showed that the Burrows method significantly 
outperformed all other methods when using three out of the 
four data collections. When using the fourth collection, the
Burrows method outperformed all the other methods except
one: the SCAP method [6].
The Burrows comparative study clearly indicated that the 
Burrows and SCAP methods are the two most effective 
methods, while the Burrows method is the single most 
effective. The comparative study presented herein is a partial 
replication of that comparative study that includes only those 
two most effective methods. 
B. The Burrows Method 
The Burrows method [7] of source code authorship 
attribution utilizes token-based n-grams and a similarity 
measure to determine authorship. Tokens include selected 
operators, keywords, and whitespace. Programs are scanned, 
and the token stream is broken into n-grams using a sliding 
window approach. Based on empirical results, the authors of
this method chose n=6 for the n-gram size. The similarity 
measure used is Okapi BM25 [14]. This measure was selected 
among five similarity measures that were evaluated: Okapi 
BM25, Cosine, Pivoted Cosine, language modeling with 
Dirichlet smoothing, and a metric developed specifically for 
source code authorship attribution called Author1. Through 
empirical testing, Okapi BM25 was found to be the most 
effective. To determine the author of a program, that program 
is considered to be a query. The query is compared using the
similarity measure to all of the programs in the data set. The
author of the most-similar program is considered the author of
the query program. So, in essence, it is the author who wrote
the program that is most similar to the query program that is
attributed to be the author. A more detailed overview of the
Burrows method can be found in [7] and [12].
C. The SCAP Method 
The SCAP method [6] of source code authorship attribution
also utilizes n-grams and a similarity measure to determine
authorship. However, the n-grams are based on bytes, rather
than tokens. That is, the n-gram representation retains every
byte contained in the source document. Again, n=6 is used for
the n-gram size. The similarity measure used is the Simplified
Profile Intersection (SPI), which was created by the authors
specifically for use in the SCAP method. All programs known
to have been written by each candidate author are concatenated
together, represented as n-grams, and the frequency of each n-
gram is stored in a table. This table of n-gram frequencies is
considered to be the profile of that author, or the Source Code
Author Profile (SCAP). Only the L-most frequently occurring
n-grams are retained, so that L is referred to as the profile
length. Based on empirical results, L=2000 was chosen for the
profile length. To determine the author of a program, the
program is compared using a similarity measure to all of the
profiles in the data set. The author of the most-similar profile is
considered the author of the query program. So, in essence, it is
the author whose profile is most similar to the query program
that is attributed to be the author. A more detailed overview of
the SCAP method can be found in [6] and [12].
III. METHODOLOGY
The comparative study presented herein is a partial 
replication of the Burrows comparative study. A general 
overview of the Burrows comparative study has already been 
provided in the Literature Review section. Details of the study 
can be found in [12]. This paper reiterates only those details 
that are specifically relevant to the replication or are otherwise 
interesting. The discussion focuses on two aspects of the 
methodology: the data set and experimental approach. 
A. The Data Set 
The data set used in the Burrows study consisted of four
collections: Coll-A, Coll-T, Coll-P, and Coll-J. Each collection
is described below:
77
 Coll-A. Consisted of student-submitted solutions to
programming assignments from RMIT University in
Melbourne, Australia, written in C.
 Coll-T. Consisted of student-submitted programs from
RMIT written in C. Distinguished from Coll-A in that
relative timestamps were included indicating when the
programs were written. There was some overlap in the
content of Coll-A and Coll-T. The timestamps were not 
used in the comparative study.
 Coll-P. Consisted of C/C++ programs retrieved from
the Planet Source Code website.
 Coll-J. Consisted of Java programs retrieved from the
Planet Source Code website.
The student-submitted data collections Coll-A and Coll-T
could not be shared due to privacy issues, and so they could not
be replicated. Student-submitted programs are problematic for
a number of reasons. Student-submitted programs cannot
generally be shared due to privacy restrictions. Minimally, the
programs would have to be stripped of all explicitly-identifying
information, and they could potentially be subject to even
further anonymization, such that all comments and string
literals would have to be stripped in their entirety. Student-
submitted programs are also not a good source of "perfect
ground truth" because a large portion of student programs are
copied and plagiarized [14-16]. Furthermore, the programs
should come from students with varied backgrounds and from
various institutions, but typically consist only of programs
from a single institution because such programs cannot be
shared. Also, programming assignments often contain program
segments that are provided by the instructor as part of the
assignment, so-called boiler-plate code. Such code is provided
with instructions for completing missing parts of the
assignment or instructions for modifying the provided code.
The inclusion of such boiler-plate code would obviously skew
experimental results.
In place of student-submitted programs, the replicated 
comparative study utilized sample programs that accompany 
programming textbooks. While these types of programs cannot 
be shared directly due to proprietary and copyright issues, they 
are generally freely available and easily accessible. Therefore, 
such data sets are easily replicable. The programs are academic 
in nature, varied according to the nature of the material being 
exemplified in each sample program, and they are reasonably 
close to "perfect ground truth". Copyright laws and reputations 
prohibit plagiarism. Consistency in approach and style is self-
enforced for reasons related to both pedagogy and software 
engineering. For these reasons, sample programs from 
textbooks are good candidates to be included in data sets for 
use in such experiments. 
Table 3 in Appendix A provides all the information 
necessary to access the sample programs that accompany the 
textbooks that were included in the data set used in this 
research. A direct link is provided to a downloadable file, if 
one exists. In case the direct link is not available or the URL 
expires, a link to the webpage that contains that link is also 
provided. Likewise, in case that link is not available or expires, 
the parent site is also provided. Finally, if all the provided 
URLs fail, then some combination of the author, ISBN, and 
publisher should be sufficient to track down the textbook. 
The Planet Source Code data collections Coll-P and Coll-J 
were created by Burrows with the intention of setting a 
precedent for use in source code authorship attribution 
experiments. While the data sets themselves could not be
shared directly, a process was well-documented in [12] to
replicate them. It was this process that was utilized to create the 
data sets used in the replicated comparative study. The Planet 
Source Code website provides searchable access to open-
source software, shared and uploaded by the authors 
themselves. 
To summarize, the Burrows comparative study consisted of
four collections: two with student-submitted programs in C, 
one with open-source programs in C/C++, and one with open-
source programs in Java. The replicated study consisted of four 
collections: two with textbook programs (one in C/C++ and 
one in Java) and two with open-source programs (one in
C/C++ and one in Java). Note that only the results related to
the open-source programs are presented here. The student-
submitted programs and textbook programs are inherently and 
altogether different, so the respective results are not to be
considered comparable. The main purpose of this paper is to
present a replication of the Burrows comparative study, so
results using the replicated collections are presented first. An
overview of the Burrows open-source collections and the 
replicated collections are shown in Table 1. Additional results, 
including the results using student programs and textbook 
programs, are discussed later. Note that a "sample" in this 
context refers to a source file. 
TABLE I. COMPARISON OF EXPERIMENTAL DATA SETS
Burrows Study Replicated Study
Coll-P Coll-J Coll-P Coll-J
Total Authors 100 76 15 15
Total Samples 1095 453 521 536
Min Samples 5 3 4 4
Mean Samples 11 6 35 36
Max Samples 57 36 101 172
Language C/C++ Java C/C++ Java
Min LOC 8 9 6 6
Median LOC 316 250 78 92
Mean LOC 984 667 149 184
Max LOC 58457 15057 3265 2724
One might note a discrepancy between the Burrows data set 
and the replicated data set in regards to the total number of
authors. The major reason for this is that the authors that were 
included across the two replicated collections are the same. In
other words, the same 15 authors are included in Coll-P as are 
also included in Coll-J. The intention is for the two collections 
to be as identical as possible, such that the only difference 
between them is the programming language. This would allow 
78
for controlled experimental studies wherein programming 
language is the experimental variable. The Burrows data set 
had no such delimitation. Note the variation between Coll-P 
and Coll-J in the replicated data set is less than the variation 
between the collections in the Burrows data set. It is not known 
whether there is any overlap between the authors selected for 
inclusion in the Burrows collections and the authors selected 
for inclusion in the replicated collections. A detailed and well-
documented process for selecting and obtaining open-source 
programs is provided in [12], and the same process was used to 
select and obtain open-source programs in the replicated 
collections. However, the authors that were specifically 
selected are left unknown. 
The effect of the discrepancy in the total number of authors 
included in the respective data sets is not as egregious as one
might suspect. The Burrows study only selected a small subset 
of the authors for inclusion in each sample run of the 
experiment. So, although 100 authors are included in the Coll-
P collection, all 100 authors were not used in any single run. 
Additional explanation is provided in the following section, 
which discusses the experimental approach. 
B. Experimental Approach 
The original Burrows study consisted of a 10-class
experiment – that is, determining the author of a program from
a set of ten candidate authors. For each run of the experiment,
ten authors from the data set were randomly selected and only
the programs written by those ten authors were used. All other
programs from all other authors were ignored for that particular
run. The results were reported as an average across several
runs. There was no check to ensure that the authors selected for
each run either did or did not overlap, but it was noted that the
probability was small that an exact duplicate set of authors
would be selected for more than one run. Nor was there a
check to ensure that all authors in the data set were selected
across all of the cumulative runs.
In the replicated study, all authors were selected for each
run of the experiment. This eliminates the need for averaging 
the results, and it maximizes the "size" of the problem. Because 
there are 15 authors included in the data set, the experiment 
was executed as a 15-class experiment. The results of the 
experiment were, therefore, deterministic. Each run of the 
experiment yielded identical results, enhancing the 
experiment's repeatability. 
A "leave-one-out cross validation" experimental design was 
used – that is, each program in the data set was selected, in
turn, as a query program while the remaining programs were 
used as the training data. The results were measured in terms of
accuracy – that is, as a percentage of programs correctly 
identified. This same approach was used in both the Burrows
study and the replicated study.
IV. RESULTS
The overall aggregate results of the Burrows and replicated 
studies are shown in Table 2. The results are measured as a 
percentage of programs correctly attributed. The original 
Burrows comparative study reported better results for the 
Burrows method (0.8% improved performance) while the 
replicated study indicated worse results (0.9% degraded 
performance). 
TABLE II. COMPARISON OF EXPERIMENTAL RESULTS
Burrows Study Replicated Study
Burrows
Method
SCAP
Method
Burrows
Method
SCAP
Method
Coll-P 89.3% 85.7% 86.0% 90.0%
Coll-J 80.8% 83.0% 93.8% 91.8%
TOTAL 85.2%a 84.4%a 90.0% 90.9%
a. These specific results were not reported in the original study. 
Recall that the Burrows study consisted of four collections, 
but only two are reported in Table 2 for reasons already cited. 
Thus, the Burrows study did not report on the percentage of
programs correctly attributed across only these two collections. 
Results were indeed reported for the two collections
individually and the number of queries used in each was also 
reported. Using that information, the percentages marked with 
footnote "a" in Table 2 were derived as a percentage of
documents correctly attributed across both of the two 
collections combined. 
A detailed comparison of the specific results reported from 
the original study and replicated study will not be provided 
here, due to the variations across the studies that have already 
been discussed (i.e., the differences in data sets and 
experimental approach). However, a few interesting, informal 
observations will be made. First, the Burrows study reported 
better results for the Burrows method, while the replicated 
study indicated better results for the SCAP method. Second, 
both methods performed better for Coll-P than Coll-J in the 
Burrows study, while they both conversely performed better 
for Coll-J in the replicated study. The discrepancies between 
Coll-P and Coll-J were much greater for the Burrows method 
than the SCAP method across both studies. These observations 
are apparent and can easily be seen in Figure 1.
Fig. 1. Comparison of experimental results across only the replicated 
collections of data 
79
The noted discrepancies cannot be fully and confidently 
explained. However, potential (i.e., hypothetical) explanations 
can be provided for some of the discrepancies. The large 
discrepancy in performance by the Burrows method between 
the different programming languages could be explained by the 
fact that the Burrows method utilizes an altogether different set 
of features across programming languages during the 
tokenization process, while the SCAP method is programming 
language-independent. It is not surprising that the performance 
of the Burrows method is vastly different across languages, 
while the difference for the SCAP method is much less 
pronounced. The fact that both methods perform better for 
Coll-P in the Burrows study, while they both perform better for 
Coll-P in the replicated study, could be explained by the 
differences in the data sets themselves. In the Burrows study, 
the Coll-P collection contains a much greater number of
authors, more samples per author, and the samples themselves 
are larger than Coll-J. In the replicated study, on the other 
hand, Coll-J contains slightly more samples per author and the 
samples are larger on average – although the discrepancies are 
not as pronounced – than Coll-P. Both methods performed 
better in the replicated study than the Burrows study, which is
somewhat confounding especially considering the replicated 
study used a 15-class problem as opposed to a 10-class 
problem. However, this could be explained by the fact that the 
number of samples per author is much greater in the replicated 
data set (across both collections) than the Burrows data set. 
V. ANALYSIS AND CONCLUSION
Source code authorship attribution is, simply, the task of
deciding who wrote a piece of software given its source code. 
Applications include software forensics, plagiarism detection, 
and determining software ownership. Several methods of
source code authorship attribution have been proposed. 
Burrows [12] presented the only known comprehensive 
comparative study of these methods. The comparative study 
presented herein is a partial replication of the Burrows 
comparative study, but it only includes the two methods 
identified by Burrows to be state of the art: the Burrows 
method and the SCAP method. 
The two major areas of concern when replicating the study 
were the creation of the data set and the experimental 
approach. The replicated data set was created using the same 
documented procedure as the original Burrows study with one
additional requirement: that the two collections contained the 
same authors and programs (to the extent that it was possible) –
the only difference being in the programming language used –
in order to support experiments meant to study programming 
language as an experimental variable. The replicated 
experimental approach was fundamentally the same as the 
Burrows method (i.e., closed n-class experiment, leave-one-out
cross validation, results measured as percentage of programs 
correctly attributed). However, the execution of the study was 
simplified to be more deterministic. All of the authors in the 
data set were included in the experiment, eliminating the need 
to execute multiple runs and randomly select authors to be
included in each run. 
A. Additional Results 
In the Results section of this paper, only the results that 
applied to the replicated open-source collections were 
presented. However, the original Burrows study also attributed 
authors to student-submitted programs, while the replicated 
study also attributed authors to textbook programs. Figure 2 
shows the aggregate results of both studies across all 
collections. 
Note that only Coll-P and Coll-J are comparable. The 
results shown in Figures 1 and 2 are identical for Coll-P and 
Coll-J. These were the collections that were replicated using 
identical procedures. Coll-A and Coll-T were only used in the 
Burrows study. These collections contain student-submitted 
solutions to programming assignments. They overlap in 
content and are differentiated in that Coll-T contains 
timestamps, while Coll-A doesn't. Coll-X and Coll-Y were 
only used in the replicated study. These collections contain 
textbook programs. They are differentiated in that Coll-X 
contains C/C++ programs, while Coll-Y contains Java 
programs. 
Fig. 2. Comparison of experimental results across all collections of data 
In Figure 2, the results of the Burrows study show that the 
Burrows method outperforms SCAP in three out of the four 
collections. The results of the replicated study, on the other 
hand, show that the SCAP method outperforms Burrows in
three out of the four collections. It is possible, and the data 
indeed indicates, that for student-submitted programs Burrows 
performs better than SCAP and that for textbook programs 
SCAP performs better than Burrows. 
One hypothetical explanation for this observation can be
deduced from an inherent difference in the Burrows and SCAP 
methods. Using the Burrows method, it is the author who wrote
the program that is most similar to the query program that is
attributed to be the author. Using the SCAP method, it is the
author whose profile is most similar to the query program that
is attributed to be the author. Student-submitted programs are
often copied and plagiarized, and students who copy and
plagiarize tend to do so repeatedly. So, a profile of such an
author would be inconsistent and the SCAP method would be
negatively impacted. The Burrows method, however, might be
80
immune to such an issue – if the students who copy and 
plagiarize copy and plagiarize from the same source 
repeatedly. 
Across all collections of data, the Burrows study showed 
that the Burrows method outperformed the SCAP method 
82.9% to 75.7%. The replicated study, however, showed that 
the SCAP method outperformed the Burrows method 91.0% to
88.9%. The results of both studies are statistically significant, 
based on a chi-square test for significance. 
Note that all data discussed and presented thus far has been 
anonymized, because the original Burrows study being 
replicated only used anonymized data. Anonymization, in this 
context, means that all comments and string literals were 
stripped from all programs in the data set. This was done for a 
couple of reasons. First, it was a requirement for Burrows to
strip all explicitly-identifying information due to privacy 
concerns associated with the student-submitted programs. 
Second, it mimics a real-world scenario more accurately in the 
context of software forensics where malicious code is being 
investigated. (Intelligent cybercriminals do not leave explicitly-
identifying information within their malicious code.) However, 
in the context of plagiarism detection and determining software 
ownership, comments and string literals indeed play a very 
important role in identifying the original author. 
Whether the programs are anonymized is not consequential 
when using the Burrows method. Comments and string literals 
are not among the features used during the tokenization 
process. So, such information would be lost intrinsically. When 
using the SCAP method, however, it has a drastic effect. The 
SCAP method is applied at the byte level. Bytes contained in 
comments, string literals, and elsewhere are treated equally. 
When comments and strings are stripped, significant 
information is potentially lost. So, the study was also 
performed on non-anonymized documents. The aggregate 
results across all data collections are shown in Figure 3. 
Fig. 3. Experimental results of replicated study across all data collections 
using non-anonymized data 
The results shown in Figure 3 for the Burrows method are 
identical to the results shown in Figure 2 because the 
anonymization of data is irrelevant in the Burrows method for 
reasons already discussed. In these results, the SCAP method
outperformed the Burrows method across all data collections. 
The SCAP method attributed 95.0% of all documents, 
compared to 88.9% attributed by the Burrows method. 
B. Final Remarks 
The Burrows comparative study indicated that the Burrows 
method of source code authorship attribution was clearly state 
of the art by a vast margin. It outperformed all other methods 
across nearly all data collections. However, the replicated study 
indicated otherwise. When using anonymized open-source 
documents (compiled using a procedure replicated from the 
Burrows study), the SCAP method outperformed the Burrows 
method by only a slight margin. When the data was not 
anonymized, however, the SCAP method was clearly state of 
the art by a wide margin. The student-submitted programs used 
in the Burrows study could not be replicated, but the results 
indicated that the Burrows method performed better on them. It 
is unfortunate that the study could not be replicated using those 
data collections. Textbook programs were instead used in the 
replicated study, and the SCAP method performed better on 
those. Based on the overall results of the replicated study, the 
SCAP method outperformed the Burrows method, which 
contradicts the results reported by the Burrows study. 
C. Future Work 
Student-submitted programs are often used in authorship 
attribution experiments due to the obvious application of 
plagiarism detection on programming assignments in an 
academic setting. However, student-submitted programs are 
problematic for reasons previously outlined. However, a study 
could perhaps be executed expressly for the purpose of 
collecting student-submitted programs to be shared and used 
by researchers. The study would need to be widespread, 
covering numerous and varied institutions. The programs 
would need to be developed in a controlled environment to 
prevent copying and plagiarism. Although it would be a large 
undertaking, such a project would be feasible and applicable to 
more than just authorship attribution experiments. Any 
research requiring the use of student-submitted programs 
would be benefitted.
Until such time that such a collection of student-submitted 
programs is created, textbook programs are an accessible 
analog. While textbook programs are certainly not equivalent 
to student-submitted programs, they are academic in nature and 
varied according to the nature of the material being 
exemplified in each sample program. They are reasonably 
close to "perfect ground truth". Copyright laws and reputations 
prohibit plagiarism. Consistency in approach and style is self-
enforced for reasons related to both pedagogy and software 
engineering. Moreover, sample programs from textbooks are 
generally feely available and easily accessible. 
A universal data set suitable for standard use in authorship 
attribution experiments (as well as other fields also requiring 
similar collections of programs) is a common vision among 
researchers. In the Burrows study, the open-source programs 
were collected using a procedure that was well-documented 
with the intention of setting a precedent. Perhaps the use of 
open-source programs collected using the Burrows approach, 
81
textbook programs that are freely available and accessible for 
download via the Web, and student-submitted programs 
collected in a controlled environment as suggested would 
together satisfy that vision. 
Other avenues of future work include individual and 
incremental improvements to both the Burrows and SCAP 
methods of source code authorship attribution. The Burrows 
method could potentially be improved by selecting different 
feature sets for program representation, by tweaking the Okapi 
parameters, or by selecting an altogether different similarity 
metric.  The SCAP method could potentially be improved by 
also selecting a different similarity metric or by intelligently 
selecting a value for the L parameter in the author profiles. 
The two methods could potentially be combined to create a 
new, more-effective method. Perhaps a "confidence level" 
could be applied to each document attribution. If the 
confidence is deemed low, alternative factors and/or methods 
could be utilized to assist in the final determination. Perhaps 
"identifying" data such as comments and string literals could 
be handled separately from the primary method of authorship 
attribution. If such information is unavailable, it won't affect 
the primary means of attribution. If it is available, it could be 
used in a secondary manner to supplement the primary method. 
This could make sense, considering the data contained in 
comments and string literals are mostly free from the confines 
of the programming language syntax and so, perhaps, should 
intrinsically be analyzed differently. Other factors such as 
identifiers and file names could potentially be analyzed in a 
similar way. 
Other areas of future work include general issues related to 
authorship attribution: code reuse, multiple authors, code 
obfuscation, pretty printers, the application of style rules, the 
application of conventions and guidelines, inconsistent 
programming style, and programmer background and 
education. Another major issue is in regards to the candidate 
authors. In this study (and all other known studies of source 
code authorship attribution), the author was attributed to one of 
a set of finite candidate authors. A variation would be a much 
harder problem: to attribute whether the author was one of a 
finite set or an unknown author from outside that set. 
REFERENCES
[1] Zhao, Y., Zobel, J., Effective and scalable authorship attribution using 
function words, Proceedings of the Second Asian Information Retrieval 
Symposium (AIRS), 174-189, 2005. 
[2] Spafford, E., Weeber, S., Software forensics: Can we track code to its 
authors?, Computers & Security (COMPSEC), 12, (6), 585-595, 1993. 
[3] MacDonell, S., Gray, A., MacLennan, G., Sallis, P., Software forensics 
for discriminating between program authors, Proceedings of the 6th 
International Conference on Neural Information Processing (ICONIP), 
66-71, 1999. 
[4] Krsul, I., Spafford, E., Authorship analysis: Identifying the author of a 
program, Computers & Security (COMPSEC), 16, (3), 233-257, 1997. 
[5] Ding, H., Samadzadeh, M., Extraction of java program fingerprints for 
software authorship identification, The Journal of Systems and 
Software, 72, 49-57, 2004. 
[6] Frantzeskou, G., Stamatatos, E., Gritzalis, S., Katsikas, S., Effective 
identification of source code authors using byte-level information, 
Proceedings of the 28th International Conference on Software 
Engineering (ICSE), 893-896, 2006. 
[7] Burrows, S., Tahaghoghi, S., Source code authorship attribution using n-
grams. Proceedings of the 12th Australasian Document Computing 
Symposium, 32-39, 2007. 
[8] Lange, R., Mancoridis, S., Using code metric histograms and genetic 
algorithms to perform author identification for software forensics. 
Proceedings of the 9th Annual Conference on Genetic and Evolutionary 
Computation (GECCO), 2082-2089, 2007. 
[9] Kothari, J., Shevertalov, M., Stehle, E., Mancoridis, S., A probabilistic 
approach to source code authorship identification, Proceedings of the 
Fourth International Conference on Information Technology, 243–248, 
2007. 
[10] Elenbogen, B., Seliya, N., Detecting outsourced student programming 
assignments. Journal of Computing Sciences in Colleges, 23, (3), 50-57, 
2008. 
[11] Shevertalov, M., Kothari, J.., Stehle, E., Mancoridis, S., On the use of 
discretized source code metrics for author identification. Proceedings of 
the 1st International Symposium on Search Based Software Engineering 
(SSBSE), 69-78, 2009. 
[12] Burrows, S., Source Code Authorship Attribution [Doctoral 
Dissertation], Melbourne, Australia: RMIT, 2010. 
[13] Planet Source Code, planet-source-code.com, retrieved May 10, 2013. 
[14] McCabe, D., Levels of cheating and plagiarism remain high, Center for 
Academic Integrity, Duke University, 2005, retrieved September 22, 
2010, from http://www.academicintegrity.org. 
[15] Bull, J., Collins, C., Coughlin, E., Sharp, D., Technical Review of 
Plagiarism Detection Software Report, Luton, England: Joint 
Information System Committee (JISC), 2001. 
[16] Culwin, F., MacLeod, A., Lancaster, T., Source Code Plagiarism in UK 
HE Computing Schools, Issues, Attitudes and Tools, London: South 
Bank University SCISM Technical Report SBU-CISM-01-01, 2001. 
[17] Burrows, S., Uitdenbogerd, A., Turpin, A., Comparing techniques for 
authorship attribution of source code, Software: Practice and 
Experience, 2012. 
82
APPENDIX A 
TABLE III. INFORMATION REGARDING PROCEDURE FOR COLLECTING TEXTBOOK PROGRAMS
Author ISBN Publisher Webpage Parent Site Direct Link Notes
Bronson 9780619159665 Cengage goo.gl/7MyBV cengage.com bit.ly/120tOwb b
Bronson 9780619217204 Cengage goo.gl/9d8Kh cengage.com bit.ly/YJZUfO b
Carrano 9780132923729 Pearson goo.gl/2YHC0 cssupport.pearsoncmg.com bit.ly/120ustG a
Carrano 9780132122306 Pearson goo.gl/59KwS cssupport.pearsoncmg.com bit.ly/X3LnuY a
Dale 9780763771560 Jones&Bartlett goo.gl/CwNBd jblearning.com bit.ly/11YRbtG a
Dale 9780763734022 Jones&Bartlett goo.gl/sDP5A jblearning.com bit.ly/11YRgxA a
Deitel 9780132662369 Pearson goo.gl/TF7qT deitel.com N/A c
Deitel 9780132575669 Pearson goo.gl/SY5pT deitel.com N/A c
Drozdek 9780534491826 Cengage goo.gl/dYJax cengage.com bit.ly/14vrHod b
Drozdek 9780534492526 Cengage goo.gl/slwLK cengage.com bit.ly/XOdZ8I b
Gaddis 9780132576253 Pearson goo.gl/pWX2X cssupport.pearsoncmg.com bit.ly/Xsjyhh a
Gaddis 9780136080206 Pearson goo.gl/pWX2X cssupport.pearsoncmg.com bit.ly/10iXP9c a
Horstmann 9780470927137 Wiley goo.gl/Zh3ge wiley.com bit.ly/Xskd2c a
Horstmann 9781118063316 Wiley goo.gl/ssCNh wiley.com bit.ly/YmkvY0 d
Horton 9781118368084 Wrox goo.gl/UTZdw wrox.com bit.ly/167JAFT a
Horton 9780470404140 Wrox goo.gl/sj7mp wrox.com bit.ly/10mDbDO a
Koffman 9780471467557 Wiley goo.gl/U9G5H wiley.com bit.ly/16o4PVX a
Koffman 9780470128701 Wiley goo.gl/LSmfN wiley.com bit.ly/X3MDOT a
Liang 9780136097204 Pearson goo.gl/oxoOu pearsonhighered.com/liang bit.ly/167JXAm a
Liang 9780132130790 Pearson goo.gl/RGuRr pearsonhighered.com/liang bit.ly/1756gdp a
Malik 9781133626381 Cengage goo.gl/qJjv4 cengage.com bit.ly/10iYFmo b
Malik 9781111530532 Cengage goo.gl/dTXM3 cengage.com bit.ly/113s9an b
Savitch 9780132830713 Pearson goo.gl/5gp2c cssupport.pearsoncmg.com bit.ly/Xslo1I a
Savitch 9780132830317 Pearson goo.gl/5gp2c cssupport.pearsoncmg.com bit.ly/YK1fDi a
Schildt 9780072232158 McGraw-Hill goo.gl/Z3xAJ mhprofessional.com bit.ly/Xsluq4 a
Schildt 9780071466509 McGraw-Hill goo.gl/dplrR mhprofessional.com bit.ly/10iYVBP a
Smiley 9780072225358 McGraw-Hill goo.gl/BeM1w johnsmiley.com bit.ly/14vtcCO a
Smiley 9780072131895 McGraw-Hill goo.gl/3cKBA johnsmiley.com bit.ly/Ymlaso a
Smith 9781133525806 Cengage goo.gl/sP6rz cengage.com bit.ly/ZtPCwm b
Smith 9781133526063 Cengage goo.gl/x4IVE cengage.com bit.ly/113sFoP b
a. zip file 
b. self-extracting zip exe 
c. direct link not available, website registration required 
d. single downloadable file not available, multiple zip files available 
83
