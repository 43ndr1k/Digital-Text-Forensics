Integrating Features from Different Sources for Music Information Retrieval
Tao Li
School of Computer Science
Florida International University
Miami, FL 33199
taoli@cs.fiu.edu
Mitsunori Ogihara
Department of Computer Science
University of Rochester
Rochester, NY 14620
ogihara@cs.rochester.edu
Shenghuo Zhu
NEC Laboratories America
Cupertino, CA 95014
zsh@sv.nec-labs.com
Abstract
Efficient and intelligent music information retrieval is
a very important topic of the 21st century. With the ulti-
mate goal of building personal music information retrieval
systems, this paper studies the problem of identifying “sim-
ilar” artists using both lyrics and acoustic data. In this pa-
per, we present a clustering algorithm that integrates fea-
tures from both sources to perform bimodal learning. The
algorithm is tested on a data set consisting of 570 songs
from 53 albums of 41 artists using artist similarity pro-
vided by All Music Guide. Experimental results show that
the accuracy of artist similarity classifiers can be signifi-
cantly improved and that artist similarity can be efficiently
identified.
1 Introduction
In multimedia information retrieval the data are natu-
rally multi-modal, in the sense that they are represented by
multiple sets of features. For example, the representation
of a movie has three modes: (i) the personnel (the producer,
the director, the editor, the scenario writer, the music com-
poser, the cast, etc.), (ii) the visual features (which sum-
marize the scenarios and the actions), and (iii) the acoustic
features (which summarize the voice and the background
audio). The representation of popular music is also tri-
modal in some sense, where the second feature set is re-
placed by the lyrics. The personnel feature set of the rep-
resentation of music, however, is significantly smaller than
that of movies, since many music artists produce, com-
pose, and perform themselves. This compels one to take
the standpoint that the representation of popular music is
bimodal, consisting of the acoustic features, which sum-
marize the sound, and the text features, which summarize
the words put into the music.
Two fundamental problems in dealing with multime-
dia data are classification and clustering. Classification
is the problem of assigning predefined class labels to the
data, while clustering is the problem of dividing the data
into classes based on their similarity without predefined
class labels. These concepts are interchangeably called su-
pervised learning and unsupervised learning, respectively.
Since the proportion of predefined class labels available
as part of input is 0% for clustering and 100% for clas-
sification, one naturally wonders about the special cases
of these two fundamental problems in which only a part
of the data has predefined labels. This problem is called
semi-supervised learning. The main question in semi-
supervised learning is whether it is possible to use the un-
labeled data to produce something better than the one pro-
duced using only the labeled data. In particular, for semi-
supervised learning of multi-modal data, i.e., data with het-
erogeneous sets of features, a natural question is whether
multi-modality can be effectively utilized in learning and,
if so, whether such multi-modal learning methods produce
better results than unimodal methods.
The celebrated paper of Blum and Mitchell [6] is the
first to address formally this question. In this paper, Blum
and Mitchell study the problem of incorporating unlabeled
data in building classifiers in the presence of two feature
sets. In particular, they propose a strategy for constructing
classifiers called co-training for the purpose of making use
of unlabeled data. The co-training algorithm proceeds in
rounds in the following way: In each round a classifier is
built on each of the two feature sets using the current train-
ing set, which is initially set to the set of data whose labels
are given as input. Then, for each feature set, the point
among the unlabeled data for which the classifier with re-
spect to the feature set provides the most confident asser-
tion is selected and is added to the training set of the other
feature set along with the assertion. (Note that the two
classifiers may select an identical point and disagree on its
class label). Blum and Mitchell show that under a certain
“independence” assumption about the joint distribution of
1
Proceedings of the Sixth International Conference on Data Mining (ICDM'06)
0-7695-2701-9/06 $20.00  © 2006
the feature sets their co-training algorithm converges in the
sense of PAC-learning. Many research efforts have been
done for the purpose of extending and generalizing the idea
of co-training [1, 9, 16, 25, 28].
It is also possible to design an interactive (or ensemble)1
learning algorithm (that exploits interactions among clas-
sifiers to improve accuracy) for supervised learning (that
is, all the data are already labeled). For example, the co-
boosting algorithm of Collins and Singer [8] uses the indi-
vidual boosting of the feature sets with the weight adjust-
ments influenced by the labeling of the other classifier(s).
The approach can be used not only for supervised learning
but for semi-supervised learning (indeed co-boosting algo-
rithm was originally conceived for semi-supervised learn-
ing). Although such algorithms may fall into pitfalls due
to the highly simple mutual boosting structure, Collins and
Singer point out, such multi-modal learning can be very
powerful and thus is worth while.
The work of Blum and Mitchell and that of Collins
and Singer study the design of effective algorithms multi-
modality through interaction for semi-supervised learning
and for supervised learning, respectively. This naturally
leads to the question of whether multi-modal interactive
methods can be more powerful than unimodal methods in
the case of unsupervised learning, namely, clustering. The
purpose of this paper is to study this question on bimodal
clustering (we of course anticipate that bimodal cluster-
ing techniques can be naturally extended to general multi-
modal clustering). We present a clustering framework for
integrating the features based on minimizing disagreement.
It is known that in bimodal learning minimizing disagree-
ment between two classifiers can improve the performance
of learning [3, 12].
In this paper we present a formalization of the prob-
lem of minimizing disagreement in bimodal learning in
the Bayesian framework. In the framework, minimizing
disagreement can be thought as a simple common theme
of multi-modal information retrieval: individual feature
sets interact to help each other by reducing disagreement
among their outputs. We then present a bimodal cluster-
ing algorithm based on the common theme —- initialize
the cluster layout using the output of the counterpart and
try to minimize the disagreement between two modes. We
apply the bimodal clustering algorithm to the problem of
clustering popular music songs.
The rest of the paper is organized as follows: Section 2
introduces the underlying principle of minimizing the dis-
agreement, Section 3 presents the clustering algorithm of
utilizing the general principle, Section 4 describes the two
1In the literature, the word “interactive” is used often in the case of
bimodal learning and “ensemble” in the case of learning with more than
two modes. Here we use “interactive” throughout, even to mean learning
of data with more than two modes.
heterogeneous feature sets extracted from the lyrics and
acoustics data, Section 5 presents the results of experi-
ments. Finally Section 6 concludes.
2 Minimizing the Disagreement
2.1 Theoretical Underpinnings
In this section, we introduce the basic principle of min-
imizing disagreement, i.e., minimizing the disagreement
between two individual models could lead to the improve-
ment of learning performance of individual models.
Our data are bimodal: let X1 and X2 be the space of the
first mode and the space of the second mode, respectively.
Let X = (X1, X2) be the product space of X1 and X2.
Let 0 and 1 be the class labels of these data, which we will
often denote by Y . For each u ∈ {0, 1}, we use ū to its
opposite class label, that is, 1− b. Suppose that the data in
X is subject to a distribution D. Let f be our class label
function and let f1 and f2 be our class label functions based
on the first mode and on the second mode, respectively.
The (x) in f and Y are often dropped — we will write
f = u to mean f(x) = u and Y = u to mean Y (x) = u,
etc.
Definition 1 We say that f is a nontrivial classifier if for
all u ∈ {0, 1},
Pr(f(x) = u|Y (x) = u) > Pr(f(x) = ū|Y (x) = u),
where the probability is subject to D.
Remark 1 The above nontrivial condition can be restated
as (∀u ∈ {0, 1})[Pr(f = u|Y = u) > 1/2] and as (∀u ∈
{0, 1})[Pr(f 6= Y ) ≤ Pr(f = u)].
In [6], it is assumed that x1 and x2 are conditionally
independent given the labels, i.e.,
Pr(x1 = x′1|x2 = x′2) = Pr(x1 = x′1|f2(x2) = f2(x′2)).
The independence assumption is rather strong, but has
been used by many successful applications. Suppose we
build hypotheses f ′1 on X1 and f
′
2 on X2. Thus, if x1 and
x2 are conditional independent given the labels, then f ′1
and f ′2 are also conditional independent. The conditional
independence of f ′1 and f
′
2 can be interpreted as follows:
Pr(f ′1(x1) = u|f ′2(x2) = v, Y = y) = Pr(f ′1(x1) = u|Y = y)
where u, v, y ∈ {0, 1}. In other words, The conditional
independence implies that (i) for all S1 ⊆ X1 such that
the probability of (S1, X2) is non-zero, the distribution of
X2 in which the first mode is restricted to S1 is identical
2
Proceedings of the Sixth International Conference on Data Mining (ICDM'06)
0-7695-2701-9/06 $20.00  © 2006
to the distribution of X2 with no restriction; and that (ii)
for all S2 ⊆ X2 such that the probability of (X1, S2) is
non-zero, the distribution of X1 in which the first mode is
restricted to S2 is identical to the distribution of X1 with
no restriction.
One can show the following (proof omitted):
Theorem 1 Under conditional independence assumption,
the disagreement upper bounds the misclassification error
for the nontrivial classifiers.
In essence, this indicates that, under certain conditions,
the disagreement upper bounds the misclassification error.
Thus, minimizing disagreement will ideally decrease the
upper bound on the misclassification error and could boot-
strap the learning algorithm. It should be pointed out that
although the principle was originally proved in the context
of supervised learning [12], it can be thought as a simple
common theme of multi-modal information retrieval: indi-
vidual feature sets interact to help each other by reducing
disagreement among their outputs.
2.2 A Bayesian Framework for Capturing Mini-
mizing Disagreement
Let x = (x1, x2) be an observation vector. Then the
Bayes decision rule for the first mode is:
Pr(Y = 1|x1) ≶01 Pr(Y = 0|x1).
This implies that if the posteriori probability of class 1 (re-
spectively, class 0) given x1 is larger than the probability
of class 0 (respectively, class 1), x1 is assigned to class
1. Using the Bayes theorem and eliminating the common
term Pr(x1), we get
Pr(Y = 1)Pr(x1|Y = 1) ≶01 Pr(Y = 0)Pr(x1|Y = 0).
The Bayes error can be computed as: 2
 =
Z
min{Pr(Y = 1)Pr(x1|1), Pr(Y = 0)Pr(x1|0)}dx1
= Pr(Y = 1)
Z
L10
Pr(x1|1)dx1 + Pr(Y = 0)
Z
L11
Pr(x1|0)dx1.
Here L11 is the area in which
Pr(Y = 1)Pr(x1|Y = 1) > Pr(Y = 0)Pr(x1|Y = 0)
and L10 is the area in which
Pr(Y = 1)Pr(x1|Y = 1) < Pr(Y = 0)Pr(x1|Y = 0).
2We use Pr(xi|j) to denote Pr(xi|Y = j) where i = 1, 2 and
j = 0, 1.
In other words, if an observation x1 ∈ L11, it will be clas-
sified as in class 1 and if x1 ∈ L10, it will be classified as in
class 0.
Under the conditional independence assumption, the
disagreement between two components can be computed
as
E(x1, x2)
=Pr{(Pr(Y = 1|x1) > Pr(Y = 0|x1))∧
(Pr(Y = 1|x2) < Pr(Y = 0|x2))}
+ Pr{(Pr(Y = 1|x1) < Pr(Y = 0|x1))∧
(Pr(Y = 1|x2) > Pr(Y = 0|x2))}
=
∫
L11
∫
L20
p0(x1, x2) + p1(x1, x2)dx1dx2
+
∫
L10
∫
L21
p0(x1, x2) + p1(x1, x2)dx1dx2,
where
p0(x1, x2) = Pr(Y = 0)Pr(x1|Y = 0)Pr(x2|Y = 0),
and
p1(x1, x2) = Pr(Y = 1)Pr(x1|Y = 1)Pr(x2|Y = 1).
Here L21 is the region where
Pr(Y = 1)Pr(x2|Y = 1) > Pr(Y = 0)Pr(x2|Y = 0)
and L20 is the region where
Pr(Y = 1)Pr(x2|Y = 1) < Pr(Y = 0)Pr(x2|Y = 0).
Similarly, if an observation x2 ∈ L21, it will be classified as
in class 1 and if x2 ∈ L20, it will be classified as in class 0.
Observe that
 =Pr(Y = 1)
∫
L10
Pr(x1|Y = 1)dx1
+ Pr(Y = 0)
∫
L11
Pr(x1|Y = 0)dx1
=Pr(Y = 1)
∫
L10
Pr(x1|Y = 1)
( ∫
Pr(x2|Y = 1)dx2
)
dx1
+ Pr(Y = 0)
∫
L11
Pr(x1|Y = 0)
( ∫
Pr(x2|Y = 1)dx2
)
dx1
=
∫
L10
∫
L20
p1(x1, x2)dx1dx2 +
∫
L10
∫
L21
p1(x1, x2)dx1dx2
+
∫
L11
∫
L20
p0(x1, x2)dx1dx2 +
∫
L11
∫
L21
p0(x1, x2)dx1dx2
Thus, to ensure that  ≤ E(x1, x2), it is sufficient that∫
L10
∫
L20
p1(x1, x2)dx1dx2 <
∫
L11
∫
L20
p1(x1, x2)dx1dx2,
3
Proceedings of the Sixth International Conference on Data Mining (ICDM'06)
0-7695-2701-9/06 $20.00  © 2006
and∫
L11
∫
L21
p0(x1, x2)dx1dx2 <
∫
L10
∫
L21
p0(x1, x2)dx1dx2.
The above formula can be reduced to
Pr(x1 ∈ L10|Y = 1) < Pr(x1 ∈ L11|Y = 1) (1)
Pr(x1 ∈ L11|Y = 0) < Pr(x1 ∈ L10|Y = 0) (2)
The formulas in Eq. (1) and (2) in the above are essentially
the same as those in Definition 1 of Section 2. Hence, the
disagreement upper bounds can also be derived from the
Bayes perspective.
Remark 2 When the conditional independence condition
(e.g., equation 1) doesn’t hold, to guarantee that disagree-
ment upper bounds the misclassification error, we need
Pr(f ′1 = 0|f ′2 = 0, Y = 1) ≤Pr(f ′1 = 1|f ′2 = 0, Y = 1)
Pr(f ′1 = 1|f ′2 = 1, Y = 0) ≤Pr(f ′1 = 0|f ′2 = 1, Y = 0)
In other words, if
Pr(f ′1 6= Y |f ′2 6= Y ) ≤ Pr(f ′1 = Y |f ′2 6= Y ),
then the disagreement still upper bounds the misclassifica-
tion error without the conditional independence condition.
3 Bimodal Clustering
In this section, we present a clustering algorithm that
integrates different features based on the principle of min-
imizing disagreements.
3.1 Measuring Agreements Between Clusterings
Let D = {d1, d2, · · · , dn} be a set of n data points.
Suppose we are given two clusterings P1 and P2 with each
consists of a set of clusters:
Pi = {C1i , C2i , · · · , C
ki
i }, i = 1, 2
where ki is the number of clusters for clustering Pi, and
D =
⋃ki
j=1 C
j
i . The first question is how to measure the
agreements between the two clusterings.
We use adjusted Rand index to compute the agreement
between clusterings. Adjusted Rand Index is a statistic
to assess the clustering quality compared against assigned
known classes. The Rand Index is defined as the number of
pairs of objects which are both located in the same cluster
and the same class, or both in different clusters and dif-
ferent classes, divided by the total number of objects [36].
Adjusted Rand Index which adjusts Rand Index is set be-
tween [0, 1] [17]. The higher the Adjusted Rand Index, the
more resemblance between the two clusterings.
Formally, the adjusted Rand index, ARI, is defined as
∑k1
i=1
∑k2
i=1
(
nij
2
)
−
∑k1
i=1
(
ni.
2
) ∑k2
j=1
(n.j2 )
(n2)Pk1
i=1 (ni.2 )+
Pk2
j=1 (
n.j
2 )
2 −
∑k1
i=1
(
ni.
2
) ∑k2
j=1
(n.j2 )
(n2)
.
Here nij denotes the number of objects belonging to both
C1i and C
2
j , ni. =
∑k2
j=1 nij , and n.j =
∑k1
i=1 nij .
3.2 Clustering Procedure
We present a bimodal clustering approach based on the
minimizing the disagreement principle. The algorithm is
an extension of the EM method [13]. In each iteration of
algorithm, an EM type procedure is employed to bootstrap
the model obtained from one data source by starting with
the cluster assignments obtained in the previous iteration
using the other data source. Upon convergence, the two
individual models are used to construct the final cluster as-
signment. Table 1 listed the notions used for the algorithm
and the algorithm procedure is presented in Figure 1.
n Number of Songs
si = (s1i , s
2
i ) A song si has two modes:
content s1i and lyrics s
2
i
S = (s1, · · · , sn) A collection of songs
K Number of clusters
Λ1 = (λ11, · · · , λ1K) Modal 1 model parameters
Λ2 = (λ21, · · · , λ2K) Modal 2 model parameters
Y = (y1, ..., · · · , yn) Cluster assignment vector
yn ∈ {1, · · · ,K}
s ∈ S s represents a song from S
ys = k Song s is in k-th cluster
Table 1. The list of notations
We assume parameterized models, one for each cluster.
Typically, all the models are from the same family, e.g.,
multivariate Gaussian. The algorithm described above is a
variant of the EM algorithm. It performs an iterative op-
timization process for each data source by using the clus-
ter assignments from the other sources. Note that in each
iteration, one data source is picked and every data point
is reassigned to one of the clusters based on information
from that data source and on its previous assignment. At
the end of each iteration, the algorithm explicitly checks
whether the agreement between two clusterings (one clus-
tering from each data source) has been improved. If it is
improved, the algorithm then continues to iterate. Other-
4
Proceedings of the Sixth International Conference on Data Mining (ICDM'06)
0-7695-2701-9/06 $20.00  © 2006
Algorithm 1 : Bimodal Clustering
Input: S, K
Output: Cluster assignment Y as well as the trained model
structure
1: Initialization: Initialize the model structure (Λ1,Λ2)
as well as the cluster assignment Y
2: while the stopping criterion does not meet do
3: Step I:
Randomly pick a different data source i ∈ {1, 2}
4: Step II:
Model Re-estimation for source i: for each cluster
k, the model parameters, λik, are re-estimated as
λik = argmax
λ
∑
s:s∈S,ys=k
log P (si|Λi)
5: Step III:
Sample re-assignment: for each data sample s ∈ S,
set
ys = argmax
k
log P (si|λik)
6: Step IV:
Measure the agreement between two sources. If the
agreement increases, goto Step I. Otherwise, goto
Step II.
7: end while
8: Return Y as well as the trained models (Λ1,Λ2)
wise, the algorithm will go back to the allocation step and
hopefully get a new clustering.
4 Two Heterogeneous Feature Sets
We addresses the issue of identifying the artist style us-
ing both content and lyrics. Ellis et al. [26] point out that
similarity between artists reflects personal tastes and sug-
gest that different measures have to be combined together
so as to achieve reasonable results in similar artist discov-
ery. Recently, [35] shows the usefulness of multi-modal
learning for music artist style classification. In this sec-
tion, we describe the feature sets extracted from the lyrics
and the acoustic content.
4.1 Text-Based Style Features
Recently, there has appeared some work that exploits
the use of non-sound information for music information
retrieval. Whitman and Smaragdis [35] study the use
of the descriptions (obtained from All Music Guide) and
the sounds of artists together to improve classification.
Whitman, Roy, and Vercoe [34] show that the mean-
ings the artists associate with words can be learned from
the sound signals. A number of researchers also pre-
sented probabilistic approaches to model music and text
jointly [5, 7, 22, 29]. From these results, it can be hypoth-
esized that by analyzing how words are used to generate
lyrics, artists can be distinguished from others and similar
artists can be identified.
Previous study on stylometric analysis has shown that
statistical analysis on text properties could be used for text
genre identification and authorship attribution [2, 18, 30]
and over one though stylometric features (style makers)
have been proposed in variety research disciplines [32]. To
choose features for analyzing lyrics, one should be aware
of the characteristics of popular song lyrics. For instance,
song lyrics are usually brief and are often built from a very
small vocabulary. In song lyrics, words are uttered with
melody, so the sound they make plays an important in de-
termination of words. The stemming technique, though
useful in reducing the number of words to be examined,
may have a negative effect. In song lyrics, word orders are
often different from those in conversational sentences and
song lyrics are often presented without punctuation.
To account for the characteristics of the lyrics, our text-
based feature extraction consists of four components: bag-
of-words features, Part-of-Speech statistics, lexical fea-
tures and orthographic features.
• Bag-of-words: We compute the TF-IDF measure for
each words and select top 200 words as our features.
We did not apply stemming operations.
• Part-of-Speech statistics: We also use the output of
Brill’s part-of-speech (POS) tagger [4] as the basis
for feature extraction. POS statistics usually reflect
the characteristics of writing. There are 36 POS fea-
tures extracted for each document, one for each POS
tag expressed as a percentage of the total number of
words for the document.
• Lexical Features: By lexical features, we mean fea-
tures of individual word-tokens in the text. The most
basic lexical features are lists of 303 generic func-
tion words taken from [23]3, which generally serve
as proxies for choice in syntactic (e.g., preposition
phrase modifiers vs. adjectives or adverbs), seman-
tic (e.g., usage of passive voice indicated by auxiliary
verbs), and pragmatic (e.g., first-person pronouns in-
dicating personalization of a text) planes. Function
words have been shown to be effective style markers.
• Orthographic features: We also use orthographic fea-
tures of lexical items, such as capitalization, word
3Available on line at
http://www.cse.unsw.edu.au/∼min/ILLDATA/Function.word.htm
5
Proceedings of the Sixth International Conference on Data Mining (ICDM'06)
0-7695-2701-9/06 $20.00  © 2006
placement, and word length distribution as our fea-
tures. Word orders and lengths are very useful since
the writing of lyrics usually follows certain melody.
4.2 Content-Based Features
There has been a considerable amount of work in ex-
tracting descriptive features from music signals for mu-
sic genre classification and artist identification [14, 19, 27,
33, 21]. In our study, we use timbral features along with
wavelet coefficient histograms. The feature set consists of
the following three parts and totals 35 features.
4.2.1 Mel-Frequency Cepstral Coefficients (MFCC)
MFCC is a feature set popular in speech processing and
is designed to capture short-term spectral-based features.
To obtain the feature, we first compute, for each frame,
the logarithm of the amplitude spectrum based on short-
term Fourier transform, where the frequencies are divided
into thirteen bins using the Mel-frequency scaling. (The
“cepstrum” is the name coined for this logarithm.) After
taking the logarithm of the amplitude spectrum, the fre-
quency bins are grouped and smoothed according to Mel-
frequency scaling, which is design to agree with percep-
tion. MFCC features are generated by decorrelating the
Mel-spectral vectors using discrete cosine transform. In
this study, we use the first five bins, and compute the mean
and variance of each over the frames.
4.2.2 Short-Term Fourier Transform Features (FFT)
This is a set of features related to timbral textures and is
not captured using MFCC. It consists of the following five
types. More detailed descriptions can be found in [33].
Spectral Centroid is the centroid of the magnitude spec-
trum of short-term Fourier transform and is a measure of
spectral brightness. Spectral Rolloff is the frequency be-
low which 85% of the magnitude distribution is concen-
trated. It measures the spectral shape. Spectral Flux is the
squared difference between the normalized magnitudes of
successive spectral distributions. It measures the amount of
local spectral change. Zero Crossings is the number of time
domain zero crossings of the signal. It measures noisiness
of the signal. Low Energy is the percentage of frames that
have energy less than the average energy over the whole
signal. It measures amplitude distribution of the signal.
We compute the mean for all five types and the variance
for all but zero crossings.
4.2.3 Daubechies Wavelet Coefficient Histograms
(DWCH)
Daubechies wavelet filters are ones that are popular in im-
age retrieval (see [10]). To extract DWCH features, the
db8 filter with seven levels of decomposition is applied
to thirty seconds of sound signals. After the decomposi-
tion, the histogram of the wavelet coefficients is computed
at each subband. Then the first three moments of a his-
togram, i.e., the average, the variance, and the skewness,
are used [11, 21] to approximate the probability distribu-
tion at each subband. In addition, the subband energy, de-
fined as the mean of the absolute value of the coefficients,
is also computed at each subband. A few trials reveal
that of the seven subbands of db8 (1: 11025–22050 Hz,
2: 5513–11025Hz, 3: 2756–5513Hz, 4: 1378–2756Hz, 5:
689–1378Hz, 6: 334–689Hz, 7: 0–334Hz), subbands 1, 2,
and 4 show little variation. We thus choose to use only
the remaining four subbands, 3, 5, 6, and 7, for our exper-
iments. In fact, The subbands match the models of sound
octave-division for perceptual scales [20].
5 Experiments
In this section, we perform experiments to evaluate
whether the clustering algorithms based on minimizing
disagreement can be more powerful than unimodal meth-
ods.
5.1 Data Description
Our experiments are performed on the dataset consist-
ing of 570 songs from 53 albums of a total of 41 artists.
The sound recordings and the lyrics from them are ob-
tained.
To obtain the ground truth of song styles, we choose
to use similarity information between artists available at
All Music Guide artist pages (http://www.allmusic.com),
assuming that this information is the reflection of multi-
ple individual users. By examining All Music Guide artist
pages, if the name of an artist X appears on the list of artists
similar to Y, it is considered that X is similar to Y. The clus-
ters are listed in Table 2. Our goal is to identify the song
styles using both content and lyrics, i.e., cluster the 570
songs into the four different clusters. We use the cluster
information of the artists as the labels for their songs.
5.2 Evaluation Measures
As discussed above, we use the cluster structures ob-
tained from All Music Guide as labels to evaluate the clus-
tering performance. We use Purity, Entropy and Accu-
racy [38] as our performance measures. We expect these
6
Proceedings of the Sixth International Conference on Data Mining (ICDM'06)
0-7695-2701-9/06 $20.00  © 2006
Clusters Members
No. 1 { Fleetwood Mac, Yes, Utopia, Elton John,
Genesis, Steely Dan, Peter Gabriel }
No. 2 { Carly Simon, Joni Mitchell, James Taylor,
Suzanne Vega, Ricky Lee Jones,
Simon & Garfunkel }
No. 3 { AC/DC, Black Sabbath, ZZ Top,
Led Zeppelin, Grand Funk Railroad,
Derek & The Dominos }
No. 4 All the remaining artists
Table 2. Cluster Memberships.
measures would provide us with good insights on how our
algorithm works.
Purity measures the extent to which each cluster con-
tained data points from primarily one class [38]. The pu-
rity of a clustering solution is obtained as a weighted sum
of individual cluster purity values and is given by
Purity =
K∑
i=1
ni
n
P (Si), P (Si) =
1
ni
maxj(n
j
i ),
where Si is a particular cluster of size ni, n
j
i is the number
of documents of the i-th input class that were assigned to
the j-th cluster, K is the number of clusters and n is the
total number of points 4. In general, the larger the values
of purity, the better the clustering solution is.
Entropy measures how classes distributed on various
clusters [38]. The entropy of the entire clustering solution
is computed as:
Entropy = − 1
n log2 m
K∑
i=1
m∑
j=1
nii log2
nji
ni
,
where m is the number of original labels, K is the number
of clusters. Generally, the smaller the entropy value, the
better the clustering quality is.
Accuracy discovers the one-to-one relationship between
clusters and classes, therefore to measure the extent to
which each cluster contained data points from the corre-
sponding class. It sums up the whole matching degree be-
tween all pair class-clusters. Accuracy of the clustering
can be represented as:
Accuracy =
max(
∑
Ck,Lm
T (Ck, Lm))
N
,
where Ck denotes the k-th cluster, and Lm is the m-th
class. T (Ck, Lm) is the number of entities which belong
to class m are assigned to cluster k. Accuracy computes
4P (Si) is also called the individual cluster purity.
the maximum sum of T (Ck, Lm) for all pairs of clusters
and classes, and these pairs have no overlaps. The larger
accuracy usually means the better clustering performance.
5.3 Experimental Comparisons
We compare the results of the bimodal clustering algo-
rithm with the results obtained when the clustering is ap-
plied on the two sources of data separately.
We also compare the bimodal clustering algorithm with
the following clustering strategies on integrating different
information sources:
• Feature-Level Integration: Feature-level integration
performs K-means clustering after simply concatenat-
ing the features obtained from the two data sources.
• Cluster Integration: Cluster integration refers to the
procedure of obtaining a combined clustering from
multiple clusterings of a dataset [31, 24, 15]. For-
mally, let C11 , ..., C
k1
1 denote the clusters obtained
from source 1, and C12 , ..., C
k2
2 denote the clusters
obtained from source 2. Each point di can be rep-
resented as a (k1 + k2)-dimensional vector
di =(di11, · · · , di1k1 , · · · , di21, · · · , di2k2)
dijl =
{
1 di ∈ C
kj
j
0 otherwise
, for 1 ≤ j ≤ 2.
A combined clustering can be found by applying the
K-means algorithm on the new representation.
• Sequential Integration: Sequential integration is an
intermediate approach of combining different infor-
mation sources. It first performs clustering on one
data source and obtains a clustering assignment, say,
C1, ..., Ck1 . We can represent each point di as a k1-
dimensional vector using the similar idea in cluster
integration. Then we can combine the new repre-
sentation with another data source using feature in-
tegration. Clustering can thus be performed on the
new concatenated vectors. Depending on the order of
the two sources, we have two sequential integration
strategies:
1. Sequential Integration I: first cluster based on
content, then integrate with lyrics;
2. Sequential Integration II: first cluster based on
lyrics, then integrate with content.
Figure 1 illustrates and summarizes various strategies
for integrating different information sources.
7
Proceedings of the Sixth International Conference on Data Mining (ICDM'06)
0-7695-2701-9/06 $20.00  © 2006
Feature Level Integration Cluster Integration
Sequential Integration I Sequential Integration II
Bi-modal Clustering
Clustering
Music Clusters
Music LyricsMusic Content
Music Content
Clustering
Music Content
Music Clusters
Music Clusters
Clustering
Clustering
Music Lyrics
Music Clusters
Music Clusters
Clustering
Music Lyrics
Clustering
Music Content
Music Clusters
Clustering
Music Lyrics
Music Clusters
Music Clusters
Clustering Combination
Clustering
Music Content
Clustering
Music Lyrics
Music Clusters
Figure 1. Various strategies for integrating different information sources.
5.4 Analysis of the Results
We compare the results of bimodal clustering with the
results obtained when clustering is applied on content and
lyrics separately, and with the results of other integration
strategies. Table 3 presents the experimental results.
Feature Set(s) Purity Entropy Accuracy
Content-only 0.436 0.731 0.438
Lyrics-only 0.444 0.728 0.402
Feature-Level Integration 0.425 0.729 0.380
Cluster Integration 0.465 0.725 0.423
Sequential Integration I 0.431 0.724 0.434
Sequential Integration II 0.438 0.734 0.407
bimodal Clustering 0.471 0.697 0.453
Table 3. Performance Comparison. The num-
bers are obtained by averaging over ten tri-
als.
From the table, we observe the following:
• The performance of purity, entropy, and accuracy rel-
ative to the other is not always consistent in our com-
parison, i.e., higher purity values do not necessarily
correspond to lower entropy values, or to higher ac-
curacy values. This is because different evaluation
measures consider different aspects of the clustering
results. For example, the entropy measure takes into
account the entire distribution of the data in a particu-
lar cluster and not just the largest class as in the com-
putation of the purity. The accuracy considers the re-
lationships among all pair class-clusters. We compare
these three different measures and hope they would
provide enough insights for our experiments.
• The purity and accuracy of feature-level integration
are worse than those of content-only and lyric-only
clustering methods, while their entropy values are
fairly close. This shows that even though the joint fea-
ture space is often more informative than that avail-
able from individual sources, naive feature integration
tends to generalize poorly [37].
• Cluster Integration: The cluster integration performs
better than content-only and lyrics-only: cluster in-
tegration have higher purity and accuracy values and
lower entropy values than those of content-only and
lyrics-only. This actually conforms to the results
in [15]: cluster aggregation would usually provide
better clustering results.
• Sequential Integration: the results of sequential inte-
gration are generally better than feature-level integra-
tion, and they are comparable with those of content-
only and lyrics-only.
• Our bimodal clustering outperforms all other meth-
ods in all three performance measures. The bimodal
8
Proceedings of the Sixth International Conference on Data Mining (ICDM'06)
0-7695-2701-9/06 $20.00  © 2006
clustering algorithm can be thought of as a kind of se-
mantic integration of data from different information
sources. The performance improvements show that
bimodal clustering has advantages over cluster inte-
gration. The bimodal clustering aims to minimize the
disagreements between different sources and it can
implicitly learn the correlation structure between dif-
ferent sets of features.
Experimental comparisons show that our bimodal clus-
tering can efficiently identify song styles. For example,
in our experiments, two songs from the album Utopia /
Anthology: Overture Mountain Top And Sunrise Commu-
nion With The Sun and The Very Last Time would be put
into two different clusters based on their contents or lyrics
only. However, using both the content and lyrics, our
bimodal clustering algorithm identifies them to be in the
same cluster with similar styles. Similarly, bimodal clus-
tering identifies two songs from the album Peter-Gabriel /
Peter Gabriel: Excuse Me and Solsbury hill to be in the
same cluster while other methods don’t. In our experi-
ments, we have identified around 50 such pairs and they
give good anecdotal evidence that our bi-modal clustering
algorithm can efficiently identify song styles.
To investigate the relationship between the clustering
performance and the agreement with respect to the two
sources, we take a closer look at our experiments. Figure 2
shows the cluster performance (entropy and purity values)
and the (dis-)agreements between two sources in a trial.
Each unit on the X-axis represents five iterations of the al-
gorithm and the Y-axis shows the performance value. We
can observe from Figure 2 that as the agreement between
the two sources increases, the clustering quality also tends
to increase (i.e., entropy is generally decreasing while pu-
rity is increasing).
6 Conclusion
In this paper, we study the problem on whether multi-
modal interactive methods can be more powerful than uni-
modal methods in the case of clustering. In particular, we
present a clustering framework for integrating the features
based on minimizing disagreement. Experimental results
on a data set consisting of 570 songs from 41 artists of 53
albums show the effectiveness of our approach.
There are two natural avenues for future research. The
first natural direction is on music annotation. How can we
automatically and efficiently generate music style or simi-
larity information? Note we did not agree completely with
the artist similarity obtained from All Music Guide, but
nonetheless used it as the ground truth to evaluate our algo-
rithms in the experiments. Can we incorporate the opinions
Figure 2. Relationships Between Clustering
Performance and Agreements. Each unit on
the X-axis represents 5 iterations of the algo-
rithm and the Y-axis shows the performance
value.
from music experts or take into account the views from in-
dividual users? Second, it would also be interesting to ex-
tend the bimodal algorithm by using statistical inference
techniques to adaptively weight different data sources dur-
ing the clustering process.
7 Acknowledgements
This work is supported in part by NSF Career Award
IIS-054680, NSF grants EIA-0080124, and EIA-0205061.
References
[1] Steven Abeny. Bootstrapping. In Proceedings of 40th An-
nual Meeting of the Association for Computational Lin-
guistics, pages 360–367. Morgan Kaufmann Publishers,
2002.
[2] Shlomo Argamon, Marin Saric, and Sterling S. Stein. Style
mining of electronic messages for multiple authorship dis-
crimination: first results. In Proceedings of the ninth ACM
SIGKDD international conference on Knowledge discovery
and data mining, pages 475–480. ACM Press, 2003.
[3] Suzanna Becker. Mutual information maximization: Mod-
els of cortical self-organization. Network: Computation in
Neural Systems,, 7(1):7–31, February 1996.
[4] Eric Bill. Some advances in transformation-based parts of
speech tagging. In Proceedings of the twelfth national con-
ference on Artificial intelligence (vol. 1), pages 722–727,
1994.
[5] David M. Blei, Andrew Y. Ng, and Michael I. Jordan. La-
tent dirichlet allocation. J. Mach. Learn. Res., 3:993–1022,
2003.
[6] Avrim Blum and Tom Mitchell. Combining labeled and
unlabeled data with co-training. In Proceedings of the
Eleventh Annual Conference on Computational Learning
Theory (COLT’98), pages 92–100. ACM Press, 1998.
9
Proceedings of the Sixth International Conference on Data Mining (ICDM'06)
0-7695-2701-9/06 $20.00  © 2006
[7] Eric Brochu and Nando de Freitas. Name that song!: A
probabilistic approach to querying on music and text. In
Neural Information Processing Systems: Natural and Syn-
thetic, 2002.
[8] Michael Collins and Yoram Singer. Unsupervised models
for named entity classification. In Proceedings of the Joint
SIGDAT Conference on Empirical Methods in Natural Lan-
guage Processing and Very Large Corpora, 1999.
[9] Sanjoy Dasgupta, Michael L. Littman, and David
McAllester. PAC generalization bounds for co-training. In
T. G. Dietterich, S. Becker, and Z. Ghahramani, editors, Ad-
vances in Neural Information Processing Systems 14, pages
375–382, Cambridge, MA, 2002.
[10] I. Daubechies. Ten lectures on wavelets. SIAM, Philadel-
phia, 1992.
[11] Afshin David and Sethurman Panchanathan. Wavelet-
histogram method for face recognition. Journal of Elec-
tronic Imaging, 9(2):217–225, 2000.
[12] Virginia R. De Sa and Dana Ballard. Category learn-
ing through multi-modality sensing. Neural Computation,
10(5):1097–1117, 1998.
[13] A.P. Dempster, N.M. Laird, and D.B. Rubin. Maximum
likelihood from incomplete data via the EM algorithm.
Journal of the Royal Statistical Society, 39(1):1, 38 1977.
[14] Jonathan Foote and Shingo Uchihashi. The beat spectrum:
a new approach to rhythm analysis. In IEEE International
Conference on Multimedia & Expo 2001, 2001.
[15] Aristides Gionis, Heikki Mannila, and Panayiotis Tsaparas.
Clustering aggregation. In ICDE, pages 341–352, 2005.
[16] Sally Goldman and Yan Zhou. Enhancing supervised learn-
ing with unlabeled data. In Proceedings of the 17th Interna-
tional Conference on Machine Learning (ICML’00), pages
327–334, 2000.
[17] Milligan GW and Cooper MC. A study of the compara-
bility of external criteria for hierarchical cluster analysis.
Multivar Behav Res, 21:846–850, 1986.
[18] Brett Kessler, Geoffrey Nunberg, and Hinrich Schütze. Au-
tomatic detection of text genre. In Philip R. Cohen and
Wolfgang Wahlster, editors, Proceedings of the Thirty-Fifth
Annual Meeting of the Association for Computational Lin-
guistics and Eighth Conference of the European Chapter of
the Association for Computational Linguistics, pages 32–
38, 1997.
[19] Jean Laroche. Estimating tempo, swing and beat locations
in audio recordings. In Workshop on Applications of Signal
Processing to Audio and Acoustics (WASPAA01), 2001.
[20] Guohui Li and Ashfaq A. Khokhar. Content-based indexing
and retrieval of audio data using wavelets. In IEEE Inter-
national Conference on Multimedia and Expo (II), pages
885–888, 2000.
[21] Tao Li, Mitsunori Ogihara, and Qi Li. A comparative study
on content-based music genre classification. In Proceed-
ings of 26th Annual ACM Conference on Research and De-
velopment in Information Retrieval (SIGIR 2003), pages
282–289. ACM Press, 2003.
[22] Beth Logan, Patrawadee Prasangsit, and Pedro Moreno.
Fusion of semantic and acoustic approaches for spoken
document retrieval. In Proceedings of ISCA Workshop on
Multilingual Spoken Document R etrieval, 2003.
[23] Roger Mitton. Spelling checkers, spelling correctors and
the misspellings of poor spellers. Information Processing
and Management, 23(5):103–209, 1987.
[24] Stefano Monti, Pablo Tamayo, Jill Mesirov, and Todd
Gloub. Consensus clustering: A resampling-based method
for class discovery and visualization of gene expression mi-
croarray data. Machine Learning Journal, 52(1-2):91–118,
2003.
[25] Kamal Nigam and Rayid Ghani. Analyzing the effective-
ness and applicability of co-training. In Proceedings of the
2000 ACM CIKM International Conference on Informa-
tion and Knowledge Management (CIKM’00), pages 86–
93. ACM Press, 2000.
[26] Daniel P.W.Ellis, Brian Whitman, Adam Berenzweig, and
Steve Lawrence. The quest for ground truth in musical
artist similarity. In Proceedings of 3rd International Con-
ference on Music Information Retrieval, pages 170–177,
2002.
[27] L. Rabiner and B.H. Juang. Fundamentals of Specch
Recognition. Prentice-Hall, NJ, 1993.
[28] Dan Roth and Dmitry Zelenko. Toward a theory of learn-
ing coherent concepts. In Proceedings of the Seventeenth
National Conference on Artificial Intelligence and Twelfth
Conference on on Innovative Applications of Artificial In-
telligence (AAAI/IAAAI’00), pages 639–644, 2000.
[29] Malcolm Slaney. Semantic-audio retrieval. In Proceedings
of the IEEE International Conference on Acoustics, Speech
and Signal Processing, 2002.
[30] Efstathios Stamatatos, Nikos Fakotakis, and George Kokki-
nakis. Automatic text categorization in terms of genre and
author. Computational Linguistics, 26(4):471–496, 2000.
[31] Alexander Strehl and Joydeep Ghosh. Cluster ensembles -
a knowledge reuse framework for combining multiple par-
titions. The Journal of Machine Learning Research, 3:583–
617, March 2003.
[32] Fiona J. Tweedie and R. Harald Baayen. How variable may
a constant be? Measure of lexical richness in perspective.
Computers and the Humanities, 32:323–352, 1998.
[33] George Tzanetakis and Perry Cook. Musical genre classifi-
cation of audio signals. IEEE Transactions on Speech and
Audio Processing, 10(5), July 2002.
[34] B. Whitman, D. Roy, and B. Vercoe. Learning word mean-
ings and descriptive parameter spaces from music. In
Proceedings of the HLT-NAACL03 workshop on learning
wording meaning from non-linguistic data, 2003.
[35] B. Whitman and P. Smaragdis. Combining musical and cul-
tural features for intelligent style detection. In Proceedings
of 3rd International Conference on Music Information Re-
trieval, pages 47–52, 2002.
[36] Rand WM. Objective criteria for the evaluation of cluster-
ing methods. J Am Stat Assoc, 66:846–850, 1971.
[37] Lizhong Wu, Sharon L. Oviatt, and Philip R. Cohen. Mul-
timodal integration - a statistical view. IEEE Transactions
on Multimedia, 1(4):334–341, 1999.
[38] Ying Zhao and George Karypis. Empirical and theoretical
comparisons of selected criterion functions for document
clustering. Machine Learning, 55(3):311–331, 2004.
10
Proceedings of the Sixth International Conference on Data Mining (ICDM'06)
0-7695-2701-9/06 $20.00  © 2006
