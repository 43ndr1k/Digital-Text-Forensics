Word-Level Segmentation in Printed and
Handwritten Documents
Lincoln Faria da Silva, Aura Conci
Instituto de Computação
Universidade Federal Fluminense - UFF
Niterói, Brazil
Email: {lsilva, aconci}@ic.uff.br
Angel Sanchez
Departamento de Ciencias de la Computacion
Universidad Rey Juan Carlos,
Madrid, Spain
Email: angel.sanchez@urjc.es
Abstract—Optical Character Recognition techniques for
printed and handwritten text are quite different. Therefore,
before any further document preprocessing, it is necessary to
separate these text types. A fundamental step for this separation
is the segmentation. In this paper we address the problem of
segmentation these documents into words. The proposed system
was tested in two public image databases. Many measures of
efficiency were computed achieving correct separation results
above 96% with respect to mean precisions and 97% for average
of the accuracies. Although it would be very important compare
our results with some other algorithm for the same purpose, on
this moment it is impossible because there is no work in the
same purpose were such comparison could be done.
Index Terms—segmentation; handwriting; printed text; text
identification; document analysis; Machine Vision.
I. INTRODUCTION
Nowadays, huge amount of information is produced and
stored using electronic technologies. Moreover, searching tools
could be easily accessed by anyone, anywhere in the world,
when such information is available on a network. However,
a lot of information (such as: forms, memos, letters, require-
ments and bank checks) is still created and stored on paper.
The digitization process of such information into an electronic
format is expensive and not very productive when performed
manually. The solution to this problem is to enable computers
to ”see” documents using automatic optical character recog-
nition (OCR).
OCR methodologies for machine printed text and hand-
written are totally different [1-7]. Therefore, in documents
that have both types, a preprocessing stage is required for
distinction between handwritten and printed text. In this stage
the text is segmented into smaller portions. These may be
characters, words, lines or even text regions. What will define
the type of segmentation is the objective of the application,
because its success depends largely thereof.
Previous works that perform the discrimination between
printed and handwritten text, segment the text at the word
level [8-11] at line level [12-16] or at text region level [17-
19]. In this work the text is segmented at the level of words.
The choice of the text segmentation at word level is based on
the conclusions of the Zheng et al.’s work [9], where the best
classification results of machine printed and handwritten are
achieved when the text is segmented into words.
II. PROPOSED TEXT SEGMENTATION APPROACH
This section presents the proposed system. It is composed
by four main stages: preprocessing, text segmentation, feature
extraction and classification.
The developed system considers application forms for var-
ious objectives, such as subscription forms, research ques-
tionnaires or preprinted memorandums. Blank regions, lines,
printed and handwritten words can be found all over these
documents. However, they do not present logos, figures, tables,
graphs or another types of component. Figure 1 shows an
example of possible images to be processed. Note that for
systems performing classification at line level it is not possible
the combination of written and printed in the same line.
Fig. 1. A sample of a image of the new developed database
A. Preprocessing stage
With the captured image, the preprocessing aims to prepare
the image for its segmentation into words. In this step, first
a median filtering is performed in order to reduce the image
noise which is originated from the capturing and digitizing
processes. Then the text is extracted from the background by
applying an automatic threshold technique. Next, morpholog-
ical operations are performed in order to smooth character
vertical edges. These edges are important in some extracted
features.
1) Noise reduction by spatial filtering: A 3×3 median filter
is applied to the image for noise reduction [20]. The median
filter eliminates small isolated ink components (i.e. noise) from
the image. Figure 2 shows a region of a document before and
after this operation.
Fig. 2. Application of the median filter: (left) before and (right) after filtering.
2) Automatic threshold: The text is separated from the
background by applying a threshold technique. In this case,
an optimal global threshold, represented by T, is automatically
determined by the Otsu’s method [21]. Figure 3 shows the
result this operation on a text sample.
Fig. 3. Automatic threshold of the image: (left) before and (right) after the
thresholding stage.
3) Removing horizontal lines: Even without figures, tables,
graphs and other elements, a form, almost always, has hori-
zontal lines. These lines are located and eliminated through a
simple algorithm implemented specifically for this. The aim is
to remove the lines that are the basis for manual form filling
(Figure 1), which reduce the performance of the system. Figure
4 shows this operation.
Fig. 4. Horizontal lines removal: (left) before and (right) after removing
lines.
4) Noise reduction and smoothing of vertical edges by mor-
phological opening: The morphological opening is applied
to remove some noise (which remain in the image after its
binarization) and small segments (which appear in border
region of the lines after they are eliminated). Figure 5 shows
an example of this.
Another effect of the morphological opening is the smooth-
ing of vertical edges of the characters in the text. This is
important because the vertical edges of the characters are
detected and subsequently used in extracted features. Figure
6 shows the smoothing of vertical edges by the structuring
element used by the system. Such structuring element is a
3x1 one, with (0,0) at central pixel.
Fig. 5. Noise reduction by morphological opening: (left) before and (right)
after this stage.
Fig. 6. Smoothed vertical edges: (left) before and (right) after smoothing.
B. Segmentation stage
After the preprocessing stage, the text is segmented. Firstly,
the extraction of connected components is performed and each
one is surrounded by a bounding box BB (Figure 7). The con-
nected component extraction is done using the neighborhood
N8(p). The algorithm implemented in this work is based on
the algorithm described by Pitas [22]. Next, close neighbours
to BB are joined by creating words (Figure 8). Two BB, that
are on same text line in the image, are grouped if the distance
between them is less than the following threshold value th:
th =
k∑
i=0
Li
2k
(1)
where k is the number of boxes and Li the lengths of all boxes
in the image.
Note that for printed text, the width of the BB of any
connected component is equal to the width of characters
and to the distance between words. The average of these
widths divided by two is a value that normally is less than
the distance between words and is greater than the distance
between characters. Thus, only characters of the same word
are united, justifying the use of Equation (1).
In this work, the distance between two BBs is the distance
between the parallel lines that pass through the right and left
side of two neighbors (see Figure 9).
Grouping of neighbour BB based on area intersection is also
applied. The system checks if there are any common portion,
that is same area for two given BB on the same text line
and, in positive case, it makes a fusion of such BBs. The area
intersection condition for two neighbor BB commonly occur
when one of them is enclosing a handwritten word while the
other one is enclosing the point of the character ”i”. Figure
10 illustrates how this occurs.
Fig. 7. Connected components surrounded by box candidates to words
Fig. 8. BB from boxes united forming words
Fig. 9. BB distance
Fig. 10. Union of overlapping boxes to form words: (up) before and (down)
after joining boxes.
C. Feature extraction and classification stages
After the segmentation of the text, structural features de-
scribed in [23] are extracted of the BB containing now words.
These features are used in classification rules mined during
the training phase of the system by the WEKA tool [24]. Such
rules are used to decide if a BB contains printed or handwritten
word. More details can be found in [20]. Table I shows some
rules mined based on the extracted features.
III. TESTS AND RESULTS
The tests on our method were performed on two different
document databases. One of these is the AIM Database 3.0
[25-26], in this we use k-fold cross validation method with
k = 10. Twenty images were chosen randomly and separated
in 10 subsets. Therefore each subset was formed for two of
these images (i.e. forms). In addition to the AIM database,
we build a new database to meet our specific benchmarking
requirements. For this case, we use a k-fold cross validation
procedure with k = 3. Twenty-four images of the database
were randomly chosen and separated into three disjoint sub-
sets. Therefore each subset was formed by eight of these
images (forms). Table II shows the result of the test using
the IAM Database 3.0. All the BB were classified correctly
(i.e. accuracy and precision equal 100%) for a 45% of the
chosen images of this database. Table III shows the result of
the test using our database. Accuracy and precision achieved
100% for 33% of the chosen images in such database.
In Tables II and III, the quality evaluator parameters Av-
erage of the accuracies (aa), Average of the precisions (ap),
Sensibility (sb) and Specificity (sf) are calculated by (2-5):
aa =
k∑
i=0
ai
k
(2)
ap =
k∑
i=0
pi
k
(3)
sb =
tp
tp + fn
(4)
sf =
tn
tn + fp
(5)
where, in equations (2) and (3), ai and pi are, respectively, the
accuracy and the precision calculated for each formed subset,
and k is the number of formed subsets; and, in equations (4)
and (5), tp, fn, tn and fp symbolize, respectively, the true
positive, false negative, true negative and false positive clas-
sification evaluators. For computation of all the values (true
positive, false negative, true negative and false positive), the
total number of printed and handwritten segmented words was
considered for each database. The segmentation of the chosen
images from AIM Database 3.0 has generated 1404 printed
words and 2029 handwritten words, whereas the segmentation
of the new database has generated 600 printed words and 1329
handwritten words.
TABLE I
SOME CLASSIFICATION RULES MINED OF THE NEW DEVELOPED DATABASE
Antecedent Class
IF{(Greater Vertical Edge by Height ≤ 392) THEN
and (Vertical Projection Variance ≤ 99)} {handwritten}
IF{(Major Horizontal Projection Difference > 20) THEN
and (Greater Vertical Edge by Height > 333) {printed}
and (Density Distribution ≤ 529)}
IF{(Height Deviation > 10)} THEN
{handwritten}
Although comparison with others is important, it is no
possible because there is no similar work where database and
results could be accessed. AIM Database 3.0 was established
for identification of authors and not of text types.
TABLE II
RESULTS USING THE IAM DATABASE 3.0 [25-26]
Quality evaluator parameter Printed Handwritten
Sensibility 97.51% 97.54%
Specificity 97.54% 97.51%
Average of the accuracies 97.55% 98.09%
Average of the precisions 96.70% 98.10%
TABLE III
RESULTS USING OUR DATABASE
Quality evaluator parameter Printed Handwritten
Sensibility 97.17% 99.47%
Specificity 99.47% 97.17%
Average of the accuracies 97.17% 99.46%
Average of the precisions 98.85% 98.75%
IV. CONCLUSIONS
In this paper, we proposed a technique for text segmentation
at word level in documents containing both printed and hand-
written text. This approach enables us to discriminate between
both text types when these are mixed in the same line because
the classification is now performed at the word-level and not
only at the line-level as done in other previous works. The
system was fully implemented and tested using two datasets.
For benchmarking purposes, a new classified database was
developed and published. In the future we consider to develop
a hybrid text segmentation method that contains top-down and
bottom-up parts. The top-down would segment the text into the
component lines, while the bottom-up part would extract the
words from the segmented text lines.
ACKNOWLEDGMENT
We acknowledge the grants provided by Brazilians agen-
cies CAPES and CNPq. This work has also been partially
supported by the Spanish project TIN2008-06890-C02-02
REFERENCES
[1] V. K. Govindan and A. P. Shivaprasad, ”Character Recognition - a
Review”, Pattern Recognition, v. 23, pp. 671-683, 1990.
[2] S.Impedovo and L. Ottaviano and S. Occhinegro, ”Optical Character
Recognition - a Survey”, Int. J. Pattern Recognition And Artificial
Intelligence, v. 5, pp. 1-24, 1992.
[3] M. C. Sampaio and G. Mongiovi and J. V. Carvalho, ”Utilizando Técnicas
de Data Mining para o Reconhecimento de Caracteres Manuscritos”, XIV
Simpósio Brasileiro de Banco de Dados, Florianópolis - SC., Annals, pp.
235-249, 1999.
[4] A. L. Koerich, ”Handwritten Word Recognition Using Markov Models”,
Latin America Transactions, IEEE, v. 2, n 2, pp. 132-141, 2004.
[5] E. L’Homer, ”Extraction of Strokes in Handwritten Characters”, Pattern
Recognitions, v. 33, n 7, pp. 1147-1160, 2000.
[6] M. N. Kapp, ”Reconhecimento de Palavras Manuscritas Utilizando Redes
Neurais Artificiais”, Pontifı́cia Universidade Católica do Paraná, M. Sc
Thesis, 2004.
[7] S. B. K. Aires, ”Reconhecimento de Caracteres Manuscritos Baseado em
Regiões Perceptivas”, Pontifı́cia Universidade Católica do Paraná, M. Sc
Thesis, 2005.
[8] J. K. Guo and M.Y. Ma, ”Separating Handwritten Material from Machine
Printed Text Using Hidden Markov Models”, Proceedings. Document
Analysis and Recognition, Sixth International Conference on, 10-13 sep.,
pp. 439-443, 2001.
[9] Y. Zheng and H. Li and D. Doermann, ”The Segmentation and Identifi-
cation of Handwriting in Noisy Document Images”, Document Analysis
Systems V, Lecture Notes in Computer Science, v. 2423, n 7, pp. 95-105,
2002.
[10] Y. Zheng and H. Li and D. Doermann, ”Text Identification in Noisy
Document Images Using Markov Random Field”, Proceedings. Document
Analysis and Recognition, Seventh International Conference on, 3-6 aug,
v. 1, pp. 599-603, 2003.
[11] F. Farooq and K. Sridharan and V. Govindaraju, ”Identifying Handwrit-
ten Text in Mixed Documents”, ICPR 2006. Pattern Recognition, 18th
International Conference on, v. 2, pp. 1142-1145, 2006.
[12] K. C. Fan and L. S. Wang and Y. T. Tu, ”Classification of Machine-
Printed and Handwritten Texts Using Character Block Layout Variance”,
Pattern Recognition, v. 31, n 9, pp. 1275-1284, 1998.
[13] U. Pal and B. B. Chaudhuri, ”Automatic Separation of Machine-Printed
and Hand-Written Text Lines”, ICDAR ’99, Proceedings. Document
Analysis and Recognition, Fifth International Conference on, pp. 645-
648, 1999.
[14] U. Pal and B. B. Chaudhuri, ”Machine-Printed and Hand-Written Text
Line Identification”, Pattern Recognition Letters, v. 22, n 3-4, pp. 431-
441, 2001.
[15] E. Kavallieratou and S. Stamatatos, ”Discrimination of Machine-Printed
from Handwritten Text Using Simple Structural Characteristics”, ICPR
2004, Proceedings. Pattern Recognition, 17th International Conference
on, v. 1, 23-26 aug., pp. 437-440, 2004.
[16] E. Kavallieratou and S. Stamatatos and H. Antonopoulou, ”Machine-
Printed from Handwritten Text Discrimination”, IWFHR-9 2004. Fron-
tiers in Handwriting Recognition, 9th Intern Workshop on, 26-29 oct.,
pp. 312-316, 2004.
[17] S. Imade and S. Tatsuta and T. Wada, ”Segmentation and Classification
for Mixed Text/Image Documents Using Neural Network”, Proceedings.
Document Analysis and Recognition, Second International Conference
on, 20-22 oct., pp. 930-934, 1993.
[18] S. Violante and R. Smith and M. Reiss, ”A Computationally Efficient
Technique for Discriminating Between Hand-Written and Printed Text”,
Document Image Processing and Multimedia Environments, IEEE Collo-
quium on, 2 nov., pp. 17/1-17/7, 1995.
[19] S. N. Srihari and others, ”A System to Read Names and Addresses on
Tax Forms”, Proceedings of the IEEE, v. 84, n 7, pp. 1038-1049, 1996.
[20] L. F. Silva, ”Distinção o Automática de Texto Impresso e Manuscrito
em uma Imagem de Documento”, Instituto de Computação, Universidade
Federal Fluminense - UFF, Brazil, M. Sc Thesis, 2009. available in:
http://www.ic.uff.br/PosGraduacao/Dissertacoes/411.pdf
[21] N. Otsu, ”A Threshold Selection Method from Gray-Level Histograms”,
Systems, Man and Cybernetics, IEEE Transactions on, v. 9, n 1, pp. 62-
66, 1979.
[22] I. Pitas, ”Digital Image Processing Algorithms”, ed. Prentice Hall, 1995.
[23] L. F. Silva and A. Conci, and A. Sanchez, ”Automatic Discrimination be-
tween Printed and Handwritten Text in Documents”, Computer Graphics
and Image Processing, Brazilian Symposium on, 11-15 oct., pp. 261-267,
2009.
[24] ”WEKA”, 1999, documentation available in:
http://www.cs.waikato.ac.nz/ml/weka/
[25] U. Marti and H. Bunke, ”The IAM-Database: an English Sentence
Database for Off-line Handwriting Recognition”, Int. Journal on Doc-
ument Analysis and Recognition, v. 5, n 1, pp. 39-46, 2002.
[26] U. Marti and H. Bunke, ”A Full English Sentence Database for Off-line
Handwriting Recognition”, ICDAR ’99, Proceedings. Document Analysis
and Recognition, Fifth International Conference on, pp. 705-708, 1999.
