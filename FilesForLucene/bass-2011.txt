Musician Identification with Tempo Performance Data
John Bass
and
David McAllister, PhD
Department of Computer Science
North Carolina State University
Raleigh, NC 27695, USA
ABSTRACT
Musician identification is explored by analyzing
MIDI note onset data using a human timing pro-
duction pattern discovered by Collyer, Broadbent,
and Church in 1992. Calculating the coefficient
of determination after removing common traits is
found to be an effective method to determine if a
musician is a member of a set of known musicians
and if so, identify the musician.
Keywords: Musician Identification, Performance
Data, Multimedia Analysis, Music Retrieval
1 INTRODUCTION
Research described here explores musician identifica-
tion by analyzing MIDI note onset data using a human
timing production pattern discovered by Collyer, Broad-
bent, and Church in 1992 [7]. Based on this pattern, the
experiment described by Collyer, et al. is modified and
the resulting data is used to test an identification method
using standard statistical methods. Calculating the coef-
ficient of determination after removing common traits is
found to be an effective method to determine if a musician
is a member of a set of known musicians and if so, identify
the musician.
Section 2 describes the pattern discovered by Collyer
et al. Section 3 describes an experiment based on the
experiment developed by Collyer et al. This section also
describes and tests a method for identifying a subject from
their tempo performance data as well as test the effective-
ness to determine if the subject is a member of a known
set of subjects. Section 4 summarizes the findings and
explores some follow-on research opportunities.
The ability to algorithmically identify a musician using
data describing their performances has many applications.
It can be used to search for musical content performed by
a particular musician, an artist with similar performance
characteristics, or for benchmarking music synthesized in
the style of a performer.
There is a dearth of performance data, but there are
lots of musical performances captured in the form of acous-
tical waveform data. It is difficult to extract performance
data from these waveforms, but over time this process will
become easier and more accurate. There have been many
advancements with onset detection and pitch detection,
and companies like Zenph Sound Innovations [14] are prov-
ing the economic viability of the conversion from acoustical
waveform data to performance data.
2 COLLYER TIMING PATTERN
The study of the characteristics of expression in mu-
sic began in 1932 by Hartmann [10] using player piano
rolls. Carl Seashore studied the psychology of music and
the mechanics of human musical expression in 1938 [24].
The study of musical content for characterizing human
expression has taken two major paths: analysis of audio
waveforms and analysis of performance data. Performance
data typically includes information such as note onset tim-
ing and the relative volume of notes. Bilmes [3] [4] and
Naveda [17] analyzed temporal changes in timbre to un-
derstand the concept of “groove” in ethnic rhythm based
music. The results of this study help characterize the in-
teractions of sounds in an ensemble, but not the timing
characteristics of each of the performers.
Repp [19] [22] [21] has done much work in the area of
empirical study of recorded works to extract and analyze
performers’ timing information. This research focuses on
investigating the concept of a “Beethoven pulse” proposed
by Manfred Clynes [5]. This work is related to a partic-
ular composer during a particular period who composed
a particular style of music. Beyond investigating Clynes’s
claims, Repp’s work is based on particular pieces of music,
and the findings are difficult to generalize to patterns that
can be applied to a wide range of musical material.
Others have studied more specific musical expression
features. Benedon [2] [1], Friberg [9], and Honing [13] an-
alyzed the characteristics of the complex concepts of jazz
swing feel and ensemble interaction. Windsor [27] explored
the timing characteristics of grace notes often used in clas-
sical music. These findings apply to individualistic timing
patterns which can be used for performer detection, but
only for musical styles which are characterized by a notion
of “swing” or which frequently use grace notes.
How listeners perceive tempo has been studied by De-
sain [8], Honing [12] and Repp [20]. Timing experiments
were devised by Repp [23] to determine subjects’ ability to
perceive differences in timing. Time perception obviously
has an impact on time production, but it appears that di-
rectly characterizing time production is the only feasible
approach to performer detection since characterizing time
perception would be very difficult to achieve by analyzing
musical content alone.
Researchers have developed models to characterize and
synthesize human rhythmic characteristics. Widmer [26]
provides an excellent overview of computational models
used to synthesize musical expression. Bilmes [3] devel-
oped a model to describe human expression of rhythm re-
gardless of the musical style. Honing [11] describes the
concept of time maps useful to transform metrical time to
follow empirical expression of time. The ability to create
human-like content is outside the scope of this research
since “universal” style and timing characteristics are the
focus.
Stamatatos and Widmer [25] have applied machine
learning techniques to achieve performer detection be-
tween 22 performers who play an identical composition.
This approach may prove complimentary to the research
presented here.
Palmer [18] compiled an excellent reference of the psy-
chonomics of music performance. Palmer also investigates
if movement determines timing or if timing determines
movement. Psychonomics researchers search for laws that
govern the operation of the mind and have investigated
the nature of a hypothetical internal clock that may gov-
ern movements that require precise timing with and with-
out external sources of synchronization. Regardless of the
cause, it appears that humans have similar timing char-
acteristics. Researchers have discovered that most people
generate arbitrary tempi near 100bpm, and the average
walking tempo is 111bpm.
Many psychological studies have hinted at universal
human timing production characteristics. A fundamental
principle in psychonomics states that the standard devi-
ation of response increases proportionally to the stimu-
lus target interval. This principle is known as Weber’s
Law [16].
Collyer, Broadbent, and Church (hereafter refered to
as Collyer) [7] explored this concept with a finger tapping
experiment to explore “categorical timing” which assumes
that humans may have several internal natural tempi.
Near these tempi, a musician conforms to Weber’s law.
Away from these tempi, a musician has characteristic re-
sponses that deviate slightly from Weber’s law. Collyer
et al. also explored this concept using the Musical In-
strument Digital Interface (MIDI) [6]. These experiments
show the potential use of MIDI as a measurement system
for human timing research.
Ivry and Hazletine [15] took a similar approach and
discovered that listener perception and performer produc-
tion timing processes are similar. They conclude that the
timing ability is due to an internal representation of the
target interval, not an internal oscillatory process. The
work by Collyer et al. appears to be the most applicable
method for musician identification. Also, Collyer’s find-
ings cover a range of tempi unlike the Ivry and Hazletine
research.
The Collyer categorical time experiment uses an inter-
stimulus interval (ISI) range from 175 ms to 875 ms. This
interval range corresponds to a tempo range of 342.9 bpm
to 68.6 bpm which is within a range often used in mu-
sic. ISIs were increased from 175 ms in intervals of 25 ms
for a total of 29 intervals. For a particular subject, the
tempi were picked at random. For each tempo, 50 clicks
at the tempo were played for the subject to hear and to
follow by tapping on an apparatus used to collect the sub-
ject’s inter-response intervals (IRIs). After the 50 clicks
stop, the subject’s task is to continue tapping at the same
tempo for a length of time equal to 50 clicks. This task is
repeated for a total of 5 runs. The subject would attempt
as many tempi as possible until fatigued. Sessions were
conducted until the subject experienced all tempi.
All inter-response intervals (IRIs) for all runs for a par-
ticular subject were averaged and the percent residual bias
(PRB) according to Equation 1.
PRB  pIRI  ISIq
ISI
 100 (1)
Collyer plotted PRB vs ISI and noted an interesting
and repeatable pattern centered about the horizontal axis.
PRBs above the horizontal axis imply “dragging” or tap-
ping slower than the associated tempo. PRBs below the
horizontal axis imply “rushing” or tapping faster than the
associated tempo. PRBs on or near the horizontal axis
imply that the subject is consistently able to produce a
particular tempo.
These results lead Collyer to conclude that the human
brain produces time based on more than one timing refer-
ence known as categorical timing in psychonomics. Collyer
continued with a conjecture that this timing pattern may
be individualistic. If this is true, this pattern may prove
to be a useful feature to identify a musician. The focus of
the research described below is to determine the degree of
usefulness of this pattern for musician identification.
3 EXPERIMENT
An experiment was designed to test the effectiveness
of Collyer’s timing pattern described in [7] for musician
identification. Data collected from subjects will be split
between creating a performance data library of known mu-
sicians and data to compare with the library to test the
detection method. Data collected from an additional sub-
ject will not be added to the library. This data will be
used to test the ability of the detection method to de-
termine that subject is not represented in the library of
known musicians.
Method
Experienced musicians were invited to participate in
this experiment. Six subjects participated. During data
collection, a tempo is randomly chosen and the subject
hears sixteen clicks generated by the MIDI sequencer at
that tempo. The clicks follow a quarter note pattern at
the given tempo. The subject strikes “middle C” in time
with the clicks and is allowed to play along with the 16
clicks at any point in their presentation but no data is
collected during this phase of the run.
When the 16 clicks end, data collection begins and the
subject attempts to continue striking the key at the same
tempo for fifty click times while no clicks are sounded by
the MIDI sequencer. The subject is signaled with an alert
sound at the end of the run and data collection stops.
Collyer et al. chose 50 clicks for the “synchronization”
phase of their experiment, but this is considered excessive
for the purposes of our experiment where the focus is en-
tirely on the subject’s ability to maintain the given tempo
without any external tempo information.
All tempi are chosen randomly from a list of ISIs rang-
ing from 175ms to 875ms in 25ms steps. Each tempo is
run three times for each subject. All tempi and runs are
mixed randomly. The subject is allowed to take a break
at the subject’s discretion. This experiment differs from
Collyer’s experiments in the following ways:
• Each tempo is run three times per subject vs. five
times.
• Each tempo is picked randomly and run once for each
pick. Collyer randomly picked the next tempo for a
subject, and all runs for a tempo were then executed.
• Collyer used a custom built tapping apparatus while
this experiment uses a MIDI keyboard and se-
quencer.
• Collyer used 50 synchronization clicks while this ex-
periment uses 16 synchronization clicks.
Measurements of the data collection apparatus reveal
a maximum timing precision of 0.228ms. Therefore all
time measurements will have a maximum time resolution
of 1ms.
Results
Figure 1 shows the averages of all IRIs for all runs for
each subject.
Note that all the subjects’ curves have a similar shape
that reflects Collyer’s findings. However, even with the
commonality of shape, there are distinctive differences be-
tween each subject.
Analysis
We analyzed the data using standard statistical meth-
ods. The data sets will be described in terms of a vector
of PRBs indexed by their respective ISIs.
Two different types of analyses will be done. One anal-
ysis will describe and test the accuracy of an identification
method by comparing a musician’s performance data with
a set of performance data of identified musicians. For this
analysis, the unidentified musician is a member of the set of
identified musicians. This style of identification is known
as “in-set” identification. A second analysis will deter-
mine if the identification method can properly determine
whether a subject is represented in the library of known
musicians.
In-set Identification For the in-set identification anal-
ysis, the data will be organized as follows: The three data
sets from each subject are split into two parts. Two data
sets are averaged to form the “known” data for each sub-
ject and the remaining data set is included in a group
“unknown” data. The “unknown” data will be compared
with each subject’s known data sets. All three combina-
tions of data sets acting as “known” and “unknown” data
are considered in the analysis. These combinations are as
follows:
• The first two data sets gathered form the known data
for each subject and the third data set is included in
the unknown group of data sets.
• The first and third data sets gathered form the
known data for each subject and the second data
set is included in the unknown group of data sets.
• The second and third data sets gathered form the
known data for each subject and the first data set is
included in the unknown group of data sets.
Each of these three combinations will be analyzed to
test the identification techniques.
One way to determine if an unknown data set “be-
longs” to a known data set is to find which known data
set produces the least Sum of Squares of the Error (SSerr)
with Equation 2.
SSerr  ņ
i0pyi  ŷiq2 (2)
The Coefficient of Determination (R2) provides a mea-
sure of confidence for a match or rejection of a particular
known/unknown data set pair. This aids in determining
if a subject is in the set of known subjects. R2 is shown
in Equation 3.
R
2  SSreg
SStot
(3)
The regression Sum of Squares (SSreg) is the sum of
the squares of the differences between the expected values
and the average of observations shown in Equation 4.
SSreg  ņ
i0pŷi  ȳq2 (4)
The total Sum of Squares is the sum of the squares of
the differences between the observations and the average
of observations shown in Equation 5.
SStot  SSreg   SSerr (5)
Therefore, R2 can be calculated by Equation 6.
R
2  1 SSerr
SSerr   SSreg (6)
R2 values range from 0 to 1 where the higher values
indicate a match and the lower values indicate a rejec-
tion. This gives an absolute measure of the confidence of
a match. Typical values of R2 for match rejection (sig-
nificance) are ¤ 0.05 and conversely values for acceptance
of a match (confidence) are ¥ 0.95. These thresholds cor-
respond to two standard deviations of the normal curve
which are arguably strict for the purposes of musician de-
tection. Relaxing to one standard deviation results in R2
thresholds of ¥ 0.7 for a match and ¤ 0.3 for a rejection.
We then remove the common traits from each data set
by averaging all the PRB values for all subjects at each
ISI and subtracting those values from all the data.
See Figure 2 for a plot showing the normalized known
data sets. Note that the characteristic shape of the data
is removed when compared to Figure 1.
This normalization has no effect on the SSerr values,
but the SSreg values do change which in turn has an effect
on the R2 values. Since the common traits add variability
to all the data and that variability is removed with nor-
malization, we expect the R2 values to more accurately
represent the matches and rejections.
Table 1 lists the R2 values with the third data set as
the “unknown” data for each subject. Table 2 lists the R2
values with the second data set as the “unknown” data for
each subject. Table 3 lists the R2 values with the first data
set as the “unknown” data for each subject. The values
listed in bold type represent matches between the test and
known data. If the identification method works, this value
should be the maximum on its row.
ISI (ms)
P
R
B
 (
%
)
0.
17
5
0.
20
0
0.
22
5
0.
25
0
0.
27
5
0.
30
0
0.
32
5
0.
35
0
0.
37
5
0.
40
0
0.
42
5
0.
45
0
0.
47
5
0.
50
0
0.
52
5
0.
55
0
0.
57
5
0.
60
0
0.
62
5
0.
65
0
0.
67
5
0.
70
0
0.
72
5
0.
75
0
0.
77
5
0.
80
0
0.
82
5
0.
85
0
0.
87
5
−7
−6
−5
−4
−3
−2
−1
0
1
2
3
4
5
6
7 subject 1
subject 2
subject 3
subject 4
subject 5
Figure 1: Average IRIs by Subject
ISI (ms)
P
R
B
 (
%
)
0.
17
5
0.
20
0
0.
22
5
0.
25
0
0.
27
5
0.
30
0
0.
32
5
0.
35
0
0.
37
5
0.
40
0
0.
42
5
0.
45
0
0.
47
5
0.
50
0
0.
52
5
0.
55
0
0.
57
5
0.
60
0
0.
62
5
0.
65
0
0.
67
5
0.
70
0
0.
72
5
0.
75
0
0.
77
5
0.
80
0
0.
82
5
0.
85
0
0.
87
5
−7
−6
−5
−4
−3
−2
−1
0
1
2
3
4
5
6
7
subject 1
subject 2
subject 3
subject 4
subject 5
Figure 2: Normalized Average IRIs by Subject
Known Data sets
s1 s2 s3 s4 s5
s1 0.77 0.20 0.29 0.20 0.08
s2 0.26 0.37 0.11 0.14 0.16
s3 0.56 0.19 0.46 0.32 0.09
s4 0.28 0.29 0.27 0.33 0.19
s5 0.21 0.28 0.11 0.14 0.29
Table 1: Normalized R2 Values with the Third Data Set
as Unknown Data
Known Data sets
s1 s2 s3 s4 s5
s1 0.83 0.17 0.28 0.14 0.07
s2 0.22 0.56 0.15 0.14 0.15
s3 0.62 0.17 0.71 0.20 0.08
s4 0.46 0.26 0.49 0.34 0.14
s5 0.20 0.27 0.14 0.13 0.27
Table 2: Normalized R2 Values with the Second Data Set
as Unknown Data
Known Data sets
s1 s2 s3 s4 s5
s1 0.93 0.17 0.27 0.13 0.09
s2 0.21 0.72 0.10 0.09 0.18
s3 0.56 0.19 0.72 0.16 0.10
s4 0.27 0.29 0.24 0.50 0.20
s5 0.15 0.27 0.11 0.09 0.61
Table 3: Normalized R2 Values with the First Data Set as
Unknown Data
Table 4 is a table of confusion showing the number of
correct and incorrect matches and rejections where p and n
represent actual matches or rejections and p’ and n’ repre-
sent predicted matches and rejections. A “False Positive”
or Type I error count is found in the upper right cell. A
“False Negative” or Type II error count is represented is
found in the lower left cell. The upper left cell shows the
number of correct matches and the lower right cell shows
the number of correct rejections. This table summarizes
all three combinations shown above. The number of possi-
ble acceptances is 15 and the number of possible rejections
is 60.
p n
p’ 6 0
n’ 2 54
Table 4: Table of Confusion for Normalized R2 Values
The rejection accuracy for this method is 90% and the
acceptance accuracy for this method is 40%. Finding and
removing the common traits from all the data and increas-
ing the rejection threshold for R2 to 0.3 results in an ef-
fective musician identification methodology using results
from the experiment. This accuracy may increase with
more data in the known data sets.
Out-of-set Determination The data for the out-of-set
determination analysis is organized as follows: All data is
normalized by removing common traits as described above.
Three data sets from each of five subjects are averaged to-
gether. The resulting five data sets are known data sets
for each of the five subjects. Three runs from a sixth sub-
ject acts as three separate sets of performance data from
an unidentified musician. Each of these runs is compared
with the identified musician data by using the identifica-
tion method described above.
p n
p’ 0 0
n’ 0 13
Table 5: Table of Confusion for Out-of-set Analysis
The results of this test are shown in Table 5. For this
test, we would expect 15 rejections since the unidentified
musician is not a member of the set of identified musicians
and there are three data sets for the unidentified musician.
We found no false matches and 13 rejections for an 87%
rejection accuracy.
4 CONCLUSIONS AND FUTURE WORK
A timing pattern discovered by Collyer et al. was used
to create a method for musician identification based on
characteristic responses to tempo. The data collection
method developed by Collyer et al. was modified includ-
ing the use of MIDI as the data collection apparatus. A
timing analysis of MIDI and the MIDI system used in the
experiment determined the maximum amount of precision
afforded by the collection apparatus. The data collection
method was used to collect three sets of performance data
from each subject. The collected data was transformed to
percent residual bias vs tempo according to Collyer’s anal-
ysis. The three data sets from each subject was split such
that two sets were averaged together to form a “known”
data set for the subject and the remaining set was placed
into a group of “unknown” or unidentified musician data
sets. The common traits of the data were removed from
all collected data sets and the coefficient of determination
was calculated. This method was proven effective for iden-
tifying musicians over a broad range of tempi using a very
simple measure as the basis for identification.
In future work, signal analysis techniques will be ex-
plored to develop more effective identification methods.
The ultimate goal of these identification techniques is their
application to actual musical content.
REFERENCES
[1] F. Benadon. Slicing the beat: Jazz eighth-notes as ex-
pressive microrhythm. Ethnomusicology, 50(1):73–98,
2006.
[2] F. Benadon. Time warps in early jazz. Music Theory
Spectrum, 31(1):1–25, 2009.
[3] J. Bilmes. A model for musical rhythm. In Proceed-
ings of the International Computer Music Conference,
pages 207–207. Citeseer, 1992.
[4] J. A. Bilmes. Timing is of the essence: Perceptual
and computational techniques for representing, learn-
ing, and reproducing expressive timing in percussive
rhythm. PhD thesis, Massachusetts Institute of Tech-
nology, 1993.
[5] M. Clynes. Expressive microstructure in music, linked
to living qualities. In Studies of music performance:
papers given at a seminar organized by the Music
Acoustics Committee of the Royal Swedish Academy
of Music, page 76. Academy, 1983.
[6] C. Collyer, S. S. Boatright-Horowitz, and S. Hooper.
A motor timing experiment implemented using a mu-
sical instrument digital interface (midi) approach. Be-
havior research methods, instruments & computers,
29(3):346–352, 1997.
[7] C. E. Collyer, H. A. Broadbent, and R. M. Church.
Categorical time production: Evidence for discrete
timing in motor control. Perception & psychophysics,
51(2):134–144, 1992.
[8] P. Desain and H. Honing. Does expressive timing in
music performance scale proportionally with tempo?
Psychological Research, 56(4):285–292, 1994.
[9] A. Friberg and A. Sundström. Swing ratios and
ensemble timing in jazz performance: Evidence for
a common rhythmic pattern. Music Perception,
19(3):333–349, 2002.
[10] A. Hartmann. Untersuchungen uber das metrische
verhalten in musikalischen interpretationsvarianten.
Archiv fur die gesamte Psychologie, 84:103–192, 1932.
[11] H. Honing. From time to time: The representa-
tion of timing and tempo. Computer Music Journal,
25(3):50–61, 2001.
[12] H. Honing. Evidence for tempo-specific timing in mu-
sic using a web-based experimental setup. Journal
Of Experimental Psychology Human Perception And
Performance, 32(3):780, 2006.
[13] H. Honing and W. B. de Haas. Swing once more:
Relating timing and tempo in expert jazz drumming.
Music Perception, 25(5):471–476, 2008.
[14] Zenph Sound Innovation. http://www.zenph.com.
[15] R. B. Ivry and R. E. Hazeltine. Perception and pro-
duction of temporal intervals across a range of du-
rations: Evidence for a common timing mechanism.
Journal of Experimental Psychology-Human Percep-
tion and Performance, 21(1):3–17, 1995.
[16] Peter R. Killeen and Neil A. Weiss. Optimal tim-
ing and the weber function. Psychological Review,
94(4):455 – 468, 1987.
[17] L. Naveda, F. Gouyon, C. Guedes, and M. Leman.
Multidimensional microtiming in samba music. In
Proceedings of the 12th Brazilian Symposium on Com-
puter Music, 2009.
[18] C. Palmer. Music performance. Annual Review of
Psychology, 48(1):115–138, 1997.
[19] B. H. Repp. Patterns of expressive timing in perfor-
mances of a beethoven minuet by nineteen famous pi-
anists. The Journal of the Acoustical Society of Amer-
ica, 88:622, 1990.
[20] B. H. Repp. Relational invariance of expressive mi-
crostructure across global tempo changes in music
performance: An exploratory study. Psychological
Research, 56(4):269–284, 1994.
[21] B. H. Repp. A microcosm of musical expression. i.
quantitative analysis of pianists’ timing in the initial
measures of chopin’s etude in e major. The Journal
of the Acoustical Society of America, 104:1085, 1998.
[22] B. H. Repp. Control of expressive and metro-
nomic timing in pianists. Journal of Motor Behavior,
31:145–164, 1999.
[23] B. H. Repp. Detecting deviations from metronomic
timing in music: Effects of perceptual structure on the
mental timekeeper. Perception and Psychophysics,
61(3):529–547, 1999.
[24] Carl E. Seashore. Psychology Of Music. Spencer
Press, 2008.
[25] E. Stamatatos and G. Widmer. Music performer
recognition using an ensemble of simple classifiers.
In ECAI 2002: 15th European Conference on Ar-
tificial Intelligence, July 21-26, 2002, Lyon France:
including Prestigious Applications of Intelligent Sys-
tems (PAIS 2002): proceedings, page 335. Ios Pr Inc,
2002.
[26] G. Widmer and W. Goebl. Computational models of
expressive music performance: The state of the art.
Journal of New Music Research, 33(3):203–216, 2004.
[27] L. Windsor, R. Aarts, P. Desain, H. Heijink, and
R. Timmers. The timing of grace notes in skilled mu-
sical performance at different tempi: A preliminary
case study. Psychology of Music, 29(2):149, 2001.
