A generic Architecture for a Social Network 
Monitoring and Analysis System 
 
Alexander Semenov*,  Jari Veijalainen  
Dept. of CS&IS  
University of Jyväskylä  
40014 Univ. of Jyväskylä, Finland  
{alexander.semenov, jari.veijalainen}@jyu.fi  
*Work was done when the author was the doctoral student of two 
universities: University ITMO and University of Jyväskylä 
Alexander Boukhanovsky 
e-Science Research Institute, 
The National Research University of Information 
Technologies, Mechanics and Optics (University ITMO) 
197101, Russia, Saint Petersburg, Kronverkskiy pr., 49 
avb_mail@mail.ru 
 
This paper describes the architecture and a partial 
implementation of a system designed for the monitoring and 
analysis of communities at social media sites. The main 
contribution of the paper is a novel system architecture that 
facilitates long-term monitoring of diverse social networks 
existing and emerging at various social media sites. It consists of 
three main modules, the crawler, the repository and the analyzer. 
The first module can be adapted to crawl different sites based on 
ontology describing the structure of the site. The repository 
stores the crawled and analyzed persistent data using efficient 
data structures. It can be implemented using special purpose 
graph databases and/or object-relational database. The analyzer 
hosts modules that can be used for various graph and multimedia 
contents analysis tasks. The results can be again stored to the 
repository, and so on. All modules can be run concurrently.  
Softwares architecture, web crawling, Social network 
monitoring, social network evolution, dynamic social network 
analysis 
I.  INTRODUCTION 
One of the most rapidly developing phenomena of modern 
age is the Internet and the simultaneous digitally encoded 
information accumulation on it. All digitally stored or produced 
data can be divided to public, semipublic and private or 
proprietary. The first category covers such data that can be 
accessed using a publicly available URI by anybody in the 
world. Often, the URI can be accessed through search engines. 
Semipublic data are those that one can access by knowing the 
URI but sometimes also a userid and password are needed. 
Typically, closed Internet forums and pages hidden from search 
engines belong to this category. Finally, the last category 
contains data that can only be accessed by a person, the 
government or a company owning the data. Digital voice 
streams in telecom networks and in the Internet (VoIP) also 
belong to this category.  In general, private data is not 
accessible to the public or search engines through the Internet. 
Extracted portions can be made accessible, though, to all or 
some Internet or mobile users in some cases (cf. vehicle 
register, various statistics, EU and local legislation, position of 
persons, etc.). In this context we are primarily interested in the 
public and semipublic data that is generated by individuals and 
can be accessed at social media(SM) sites, such as Facebook 
(FB) or LiveJournal (LJ). 
The above data carries information that is mostly about 
individuals and generated by individuals. A person can expose 
some information about him- or herself on SM sites, or about 
others. Most SM sites offer the possibility to record “friend” or 
other similar relationships, or group memberships between 
individuals. These form the basis for social group analysis. 
As a consequence of the fact that acquisition of the various 
kinds of information about  persons at SM sites and further 
WWW sites on the Internet is technically easy, such research 
methodologies as virtual ethnographies have appeared [22]. 
Their main principle is the qualitative description and analysis 
of the information collected from the web by some means. 
Virtual ethnography research may resort to qualitative 
description of the static aspects of the events at SM sites, or to 
analysis of the dynamics of their development.  
On the other hand, many methods and tools for quantitative 
analysis of the data have been developed, such as different 
methods of data-mining and knowledge discovery. These were 
mainly incorporated in such tools as OLAP and business 
intelligence software – tools for analyzing historical and 
current data in order to carry out predictive analysis [23]. Such 
tools have found their usage in such fields of the industry as 
marketing, business process analysis and so on.  One important 
direction of the research in this field is the application of 
computational data-analysis methods to various kinds of data 
available about the individuals, for the purpose of the detection 
of the potential perpetrators of serious crimes, such as school 
shootings, or terrorist acts [1, 21, 24] 
What makes social media [45] fascinating as a source of 
information is the fact, observed e.g. in [34], that some people 
discuss online topics that they would never have raised 
“outside” of the Internet, in their real lives. Such discussions 
can also give impulses to build e.g. hate-group communities 
and amplify these tendencies. Members of such communities 
could be influenced by these discussions and finally even 
commit some kinds of hate crimes as a consequence. Or, a 
determined individual could find such a hate-community and 
intentionally influence its members. Important example of 
similar processes is the phenomenon of school shootings. As 
described in [1], almost all of the major perpetrators were 
2011 International Conference on Network-Based Information Systems
978-0-7695-4458-8/11 $26.00 © 2011 IEEE
DOI 10.1109/NBiS.2011.52
178
familiar with the previous cases and evidently also knew 
discussion groups surrounding school shootings. 
In this paper we describe the research, devoted to building a 
decision support system that utilizes social networks analysis 
methods and other data analysis methods and automatically 
collects the data from the web. Only the public data is 
considered in this phases. The system design as such allows 
any kind of multimedia data to be used in the analysis, 
including voice and video streams, text messages, location 
data, web access logs, medical records, police reports, 
interrogation protocols, judgments, etc i.e. also this kind of 
private or semi-public data.   
II. RELATED WORK 
Because of the multi-faceted nature of the topic we will 
describe related work in several related fields, focused web-
crawlers, community detection algorithms, dynamic social 
network analysis, and similar existing systems. 
A. Focused web-crawlers 
A web-crawler is software that traverses the web relying on 
some algorithm to select nodes and links, and using appropriate 
stop policies. Generally, it is possible to divide web-crawlers 
into two categories: general purpose crawlers and focused 
crawlers. Goal of the general purpose crawlers is to collect the 
elements of the graph that are formed by viewing the web 
pages pointed by an URL as nodes and the URLs pointing 
inside the pages to other web pages and resources as links.  
Search engines use this approach. Because there is a path from 
the used seed nodes through URL-page-URL chains to almost 
any data in the public data portion (cf. above), a search engine 
can index the vast majority of the public data. Semi-public and 
proprietary data are usually not indexed.   The public data are 
thus accessible by key word searches to everybody. 
A typical example of social media site crawler is a crawler 
that tries to fetch the “friends”, friends of friends, etc. of a 
certain person X thus traversing the social network around X at 
a particular social media site. In this case the nodes of the 
network are persons (their profiles accessible through an URI) 
and links are the found “friend” relationships. These might be 
expressed by URIs pointing to the profiles, but other syntax 
might also be used.  The purpose of the focused crawlers is to 
collect nodes and links that suit to specific defined topic. Main 
research directions in the area of the crawlers are: 
parallelization of the crawler [5], node selection algorithms, 
crawling of AJAX enabled web-pages [4, 12], refreshing 
policies [3]. Paper [7] describes architecture of the crawler of 
twitter messages, which was deployed on two cloud platforms: 
Amazon AWS and Google AppEngine. 
Main difference between a “traditional” and focused 
crawler is the selection of the pages which suit to particular 
topic only. There are general algorithms which describe the 
way of traversing the nodes (in a sense – diffusion of the topic 
between the nodes), e.g. fish-search and shark-search [2, 6]. An 
important aspect of focused web-crawlers is the determination 
of the node topic. The authors of [8, 9, 10, 12, 14] describe the 
usage of the ontologies for focused crawlers' construction and 
detection of the topic of the page and description of its 
structure. The authors of [11] propose the usage of social 
network tags for topic discovery and describe automatic 
algorithms for social network website pages' detection (profile 
page, list page, detail page) based on DOM [30] structure 
classification. In addition, the body of the research in topic 
detection could be applied to focused crawlers [13]. 
Comparison of performance and other characteristics of several 
different types of focused crawlers is provided in [15].  
B. Community detection algorithms 
Existing algorithms for community detection are mostly 
based on graph theory [16, 18]. There are several types: graph 
partitioning methods, clustering methods, modularity based 
methods, divisive algorithms, spectral algorithms, and methods 
based on statistical inference. It is also possible to classify 
community detection algorithms into two categories based on 
the necessity of the presence/absence of the entire graph for 
detection. There are algorithms which need entire SN (social 
network) graph, but also algorithms that only operate on a 
partial graph (“local communities”) [17] and assume that graph 
will be collected during the process of community discovery. 
Paper [18] describes the design of the system intended for 
monitoring blog communities (for allowing “supervisors” to 
easily detect them) and experiments on Chinese web-sites and 
blogs. 
C. Dynamic social network analysis and existing software for 
online intelligence 
The authors of [20] describe the aspects of longitudinal 
studies of social networks and approach the visualization of the 
results. The paper contains descriptions of two cases: 
visualization of CSCL citation network and visualization of the 
data taken from mail-list of discussions on open source project 
“OpenSimulator”. Interesting from our point of view is the 
approach for the extraction and visualization of timeline data. 
The authors of [40] discuss the aspects of the analysis of 
historical data on social media site, where the role of edges is 
played by the data on the recent viewers of the page, 
represented as the list of identities. The shortcoming mentioned 
by the authors is mainly the special representation of these 
data: if the same user (A) is watching the profile page of user B 
at least two times during a time interval, and a number of other 
users are watching the page between the two visits of user A, 
then only the latest visit of A will be represented in the list of 
recent viewers. Thus it is necessary to query the page more 
often than users do in order to guarantee accurate analysis.  
Paper [21] describes system architecture for the early 
prediction of terrorist threats. It examines the problems with 
existing systems and presents a description of the new 
approach. The architecture of EWAS introduced in [21] 
contains the following components: 1) acquisition cluster -;–  
part of the system, responsible for getting the information from 
the internet and government databases; 2) extraction cluster – 
responsible for semantic analysis of acquired text and storing it 
in the internal database; 3) investigation system – system 
responsible for social network analysis based on the data 
produced  by the extraction cluster; 4) warning generation 
system – system which sends the warnings for subscribed 
users, based on the set of  rules. 
179
III. MODELING CONSIDERATIONS 
Nowadays, one of the most popular types of web-sites are 
online social media sites. The largest one today is Facebook 
[www.facebook.com]: it currently contains over 600 millions 
user profiles. In addition, many other web-sites maintain 
infrastructure intended for building virtual user communities. 
Virtual community in this sense is a social network of 
individuals who interact through specific media, potentially 
crossing geographical and political boundaries in order to 
pursue mutual interests or goals [26]. By individual we 
understand a physical person, but also application identity [25] 
- identity used by the person to access specific social media 
site. Relations in this sense could be explicitly technically 
described as links between the profiles, or associations inferred 
by implicitly defined attributes. There exist many different 
types of explicit relations between the users: “friends”, 
“subscribers” etc. All these relations enable the users to stay in 
touch and communicate with some members of the Social 
Media Site more often than with other individuals. Implicit 
relations could be expressed through, for instance, appearing in 
the same forum thread, or having the same interests [27]. There 
exist many different definitions of the term “community”, e.g. 
group of people sharing the same interests; group of people 
who communicate with members within the group more often 
than with people outside of the group [28]. The motivation of 
individuals to join online communities could be explained by 
several theories: anticipated reciprocity, increased recognition, 
sense of efficacy, and sense of community [33, 46, 47]. 
In the present paper we adopt the approach of modeling the 
social media site with the multidigraph. Nodes of the graph are 
the identities used at the social media, and edges the relations 
between them. Basically, social media site is modeled with 
multidigraph, because many different kinds of edges could 
exist simultaneously (e.g. “friends”, “subscribers” etc). Edges 
in multidigraph are directed, and in case that social media site 
contains undirected ties, edges in the multidigraph are directed 
mutually. We consider the node as the set of the attributes and 
their values. Different social media sites could provide 
different attributes, such as name, picture, set of the favorite 
videos or music. The values of these attributes could be located 
on several web-pages, which altogether form the complete 
profile of the person on the social media site, having a 
particular identifier. Usually these pages are hyperlinked with 
one “main” profile web-page. In addition, modern social 
networks often contain not only profiles of separate users, but 
for organizations as well. Introduction of such profiles doesn’t 
affect, generally, the possibility for social media site to be 
modeled by the multidigraph, but such kinds of nodes are not 
considered in the present architecture. 
One of the important processes, taking place in virtual and 
other communities is the community evolution [39, 42]: change 
of the members of the community and links between them as 
time goes by. A community may be built deliberately by 
someone [31] or created and developed by a group, depending 
on the change of the environment: sometimes people tend to 
unite in groups and act together (e.g. if they share interests and 
have the same goal). In addition, some individuals may 
influence other individuals of the community for some actions 
intentionally or unintentionally (social influence theory) [29]. 
Why we are devising this software: 
- In order to find out how the VCs develop over time in 
general (size growth etc.) 
- What are the main characteristics of different kinds of 
communities, like hate groups, political movements 
(cf. the development in Arabic countries[41]), hobby 
groups, fan groups 
- How do the contents submitted to a particular VC 
influence the opinions of the members and their 
possible behavior  
- Finding out faked identities in VCs 
- Detecting threat potential of individuals or VCs  
- Following the development of the contents  
- Etc. 
For all this we need a tool that stores the history of VCs so 
that longitudinal analysis of them becomes possible. 
Collection of the VCs is a multistage process. The first step 
is to choose the target site, such as Facebook (FB), LiveJournal 
(LJ), Twitter, etc. There are two main options to start the 
collection. Either, one can try to extract all the individuals and 
their relationships at that site and store them into a suitable 
multidigraph. The second option is to build “local” or partial 
communities applying incremental search among the 
individuals belonging to a particular VC at a particular site. 
This means that the multidigraph is incomplete at any point of 
time, but evolves towards a complete model of a particular VC 
at that site. It is important to mention, that the process of 
collecting of a complete graph is challenging, because the 
multidigraph could consist of several unconnected components 
and therefore, one should be able to guess “seed” nodes for 
each separate VC. Another inherent problem is that the VCs 
often evolve over the time, and thus the formed multidigraph 
might not accurately present the real state of the VC at the site.  
In this paper we assume that time spent in traversing the VC is 
significantly smaller than what is required for the VC to change 
substantially. Thus, each formed multidigraph version would 
rather adequately represent a VC in the reality. 
The first approach is computationally intensive and requires 
the traversal of all the VCs at a particular site.  It is hard to 
carry out in a short time. LJ, for example, has currently ca. 18 
million profiles; assuming a processing time of 100 ms/profile 
would require 1800 000 s, i.e. over 20 days for complete 
traversal and easily hundreds of terabytes storage to store the 
multidigraph. The second approach could be implemented as a 
part of the monitoring systems for online hatred detection, for 
instance.  
IV. SYSTEM REQUIREMENTS   
In this section we present  the requirements and architecture 
of the system which will focus not only on analysis of static 
snapshot  data, but will also store the history of changes in the 
network and supports methods for their analysis. We consider 
monitoring of the changes as more convenient than 
reconstruction of the timelines, because a) Often, profiles 
related to especially hate content are removed by web-site 
180
administrators rapidly b) Not all of the changes can be 
reconstructed from the timeline. For example, SM sites usually 
don’t keep history of changes for the information on edges 
(“friends”) and don’t record the time of the addition/removal.  
As development methodology for the system we have 
selected spiral method [38]. The development proceeds as 
follows: first initial requirements are elicited, then architecture 
is designed, after that a prototype of the software is developed 
and tested. This again leads to an extended architecture design 
and elaboration of the new requirements (and also since one of 
the main tasks of the software is knowledge discovery, new 
knowledge about the domain area). For example hate group 
evolution is planned to be analyzed by the semi-automatic tools 
and that are used by the experts in sociology and forensic 
science. Based on the experiences and emerging needs, new 
requirements will be elicited and added.  
The current version of the requirements (see below) was 
built based on the literature analysis of the related systems, 
experience of the authors with earlier version of the prototype, 
and detailed observation of school-shooters’ and related 
groups’ online behavior, partially documented in [1]. Such 
process could be seen also as agile [44] development, because 
of frequent communication between the involved researchers 
and quick evaluation of used technologies and methods of data 
analysis. 
The following key stakeholders were identified for this 
research prototype software and its versions: researchers and 
target persons.  Should this kind of system be really deployed 
in real use then one must think which actors would use it and 
possibly benefit or be harmed by its use. Because the use of 
the system raises privacy concerns, only authorities with 
corresponding responsibility and legal authorization should 
use such a system.  A more detailed stake holder analysis 
requires interviews with police and justice department, as well 
as with social and medical services. It is our intention that we 
can show what we can do with this kind of software and can 
then ask them (police etc.) what they think of it. 
 Based on the literature and on the use of the earlier 
versions of the prototype we arrived at the following 
requirements. 
A. Functional requirements 
o F1 Capability to let a human user describe 
which sites will be the target of data 
collection, when this will take place, what 
data is to be collected, and how they can be 
accessed (a metadata description of the 
structure, e.g.  ontology) 
o F2 Capability to access various SM sites and 
other WWW sources and retrieve any 
accessible raw data stored at them, including 
profile data, “friends” or similar 
relationships, multimedia contents attached 
to the profile, comments attached to the 
contents or profile by other users, etc. 
o F3 Capability to retrieve profiles or contents 
from SM sites based on  keywords  or on a 
larger ontology description given by a human 
user   
o F4 Capability to repeatedly retrieve a social  
network around a particular person (profile) 
on a particular site based on the “friends” or 
similar relationships recursively to a given 
distance (“friends”, “friends” of “friends”, 
etc)  
o F5 Capability to retrieve the entire social 
network at a particular SM site, i.e. all 
profiles, relationships and multimedia 
contents 
o F6 Capability to automatically create a set of  
(graph) models of the social networks at a 
SM site, based on the profiles and other data 
retrieved from them 
o F7 Capability to store the raw multimedia 
data and the above models persistently and in 
such a way that different instances of the 
social network model (e.g. around a certain 
person) and data can be distinguished and 
placed on a time line  
o F8 Capability to automatically or semi 
automatically analyze various properties of 
the stored models  (and raw data) and 
visualize the results to a human user and/or 
store them persistently for a later use  
B. Non-functional requirements 
 
o NF1 The system must run in parallel so that 
data collection (crawling) can happen 
simultaneously with analysis  
o NF2 The crawling performance should be  
such that all  changes at SM sites around the 
targeted persons can be captured and stored   
V.  DESCRIPTION OF THE ARCHITECTURE 
The system consists of three main components: 1) a crawler 
that continuously crawls the social media sites and other 
relevant web sites and the activities of their users specified by 
the user using a suitable ontology, 2) a persistent repository 
that stores the information gathered by the crawler, and 3) an 
analysis component that the user can ask to analyze the 
information gathered and stored into the persistent repository 
from the social media and other sites.  The analysis component 
also hosts the ontologies that are used in controlling the system. 
Figure 1 contains the diagram depicting the architecture of 
the system. 
A. Crawler 
Crawler – module of the system which continuously – or on 
request - crawls the web, according to the task (deep crawling, 
superficial crawling etc). Task in this sense is a combination of 
appropriate web-data extraction rules and traversal algorithm 
(cf. F1). 
181
Connectivity module – submodule which is responsible for 
maintenance of TCP/IP connection to target web-site and 
handling of necessary protocols (HTTP, HTTPS) (cf. F2) 
Web-Site Parsing Module – submodule responsible for 
extraction of meaningful information from the retrieved web-
page (DOM parsing with XPath) and possibility of usage of 
API provided by the web-site (e.g. Twitter API) is provided as 
well. Information can be as well extracted from Social Media 
site considering multi-page profile structure (e.g such that 
Facebook maintains: separate pages for user pictures, info 
etc).the functionality of this module is similar to  the “portlets” 
containing implemented interfaces for extracting the 
information from different kinds of social media sites (F2) 
Traversal algorithms module is responsible for storing the 
web-graph traversal program, irrelevant to a concrete web-site 
(BFS traversal, focused crawling etc. (cf. F3-F5). 
Parallelism module is responsible for parallelizing crawler 
traversals of the SM sites (cf. NF2). 
Web-site structure cache contains cache of the ontology of 
web-site structure, fetched from the repository in pre-compiled 
form (for fast processing) (cf. F2, NF2). 
Crawler is connected with repository server by means of 
ICrawl interface (crawled data transfer, web-site ontology data 
transfer, task data transfer) and it can also be controlled from 
UI (cf.F1).  
B. Repository 
Repository is the module storing the database and hosting 
the necessary applications that handle the graphs (models) and 
other contents data.  
User Profiles temporal storage – temporal graph database 
storing the structure of social media site at different intervals of 
time (according to the tasks). It also stores user-profiles data. In 
this sense structure of SM site at time interval is topological 
structure of the network and time of the collection. The data 
stored here are obtained by the crawler module (edges existing 
in multidigraph depend on the type of the task). During the 
analysis phase these edges are properly recognized. 
Projects storage – schema storing the information on tasks 
and projects, existing at the system. We assume that the system 
will be monitoring large number of the social media sites 
simultaneously, thus set of rules for monitoring will exist in the 
form of separate projects – user defined tasks for the system 
(cf. F3,F4). 
Analyser storage – storage for the information, produced by 
analyzer modules. (cf. F8) 
Web-site structure storage – metadata storage containing 
structure descriptions of different web-sites which are subject 
to be crawled and rules for extraction of the information from 
the pages (cf. F1) 
C. Analyser  
Analyser contains mainly modules, which are elaborated 
for analyzing and finding the patterns in the data from data 
storage by analyzing the crawled data: tools for social network 
analysis, dynamic social network analysis, and modules for 
recognition of the pictures and authorship attribution [35]. One 
of the key design principles of the analyzer is to use the 
interface IData towards the Repository and make various 
analyses on the data retrieved.  It is also able to create 
workflows using 3rd party software. Output data from workflow 
components should be, if configured, sent to input of other 
modules. That would lead the system to have complicated 
multi-stage analyzer tasks and it could analyze social networks 
with implicit ties which is computationally hard to obtain. E.g.: 
get all pictures from collected social media site -> recognize all 
the pictures with 3rd party software and extract images of the 
guns -> take the nodes having these pictures -> carry out SN 
analysis to find hidden patterns.  
Analyser can run continuously and in parallel with the 
Crawler.  
 
 
Figure 1.  Architecture diagram 
VI. IMPLEMENTATION DETAILS 
Currently, prototype is implemented using Python 
programming language [36]. Crawler is implemented as a 
separate module, which is able to carry out focused crawling, 
with page selection according to selected criteria (currently as 
indicator of the affiliation of the page to certain topic we use 
simple keywords search (bag of words approach). In addition, 
search of the content using 3rd party search engine is 
implemented. This allows one to find the given terms on a 
specific web-page, but the crawler is able to only access pages 
182
that are allowed it to access. Additionally, the crawler is able to 
traverse graph of social media site using breadth-first traversal, 
for a predefined depth (cf. F4). 
The crawler creates, while crawling the site, the mentioned 
models of social media sites producing multidigraphs. The 
graph structure is stored at a PostgreSQL database. Database  
also stores the time of accessing the profile page (time of the 
collection for node), thus providing the possibilities for 
analysis of the development history of the groups, based on the 
accumulation of sufficient quantities of the data (cf. F7). 
Currently, we store the graph structure in two tables of a 
relational database: the first contains nodes, and the second 
contains the representation of the (labeled) edges between the 
pairs of nodes. Currently, the types of the edges for 
multidigraph are explicitly defined. For example, in the current 
implementation for LiveJournal we use “friend” and “friendof” 
relations as the edges. Figure 2 depicts the table structure of the 
database: 3 tables in 3rd normal form. Table “nodes” is relation 
between internal variable “id”, “username” – SM username, 
“date” – date of the addition of this node to the database, 
“color” – column, representing whether the full information on 
the node (currently – interests) was stored in the DB, and “dist” 
– service variable, used for traversal of the graph (represents 
the depth of the traversal). “SM_type” represents the type of 
the social media, in current implementation it is set to 
“LiveJournal”. “Edges” table contains information on edges 
(“id_from” and “id_to” - incidence list used for storing the 
graph). “Interests” table - “id” – “interest” – one to many 
relation, storing user’s LJ interests. 
 
Figure 2.  Database structure 
As concerns identifying different objects in the models, we 
assume that social media sites have some symbolic or digital 
value, which acts as the identifier of the user at the site. The 
currently considered social media sites adopt the approach 
where URL for the profile of a user could be inferred based 
only on the unique identifier of the user at that site. Therefore, 
we use this identifier as identifier of the node within the 
digraph in the database, but internally these identities are 
represented as digital values. In the present version of the 
software we explicitly describe the type of the social media site 
in a separate column, and use different tables for representation 
of the content data in different social networks (e.g. interests 
and other profile data for LJ; and info and uploaded videos for 
YouTube). 
Analyzer module now consists of SNA tools, implemented 
using NetworkX [37] library for Python language (library 
which provides graph operations and algorithms).  We also 
partially export  data to Pajek and perform analysis using it (cf. 
F8). 
The structure of the social media site pages is being 
extracted by means of XPath, and also LiveJournal API is 
implemented. 
VII. AN EXAMPLE OF A RETRIEVED SOCIAL 
NETWORK  
As an example, we   collected the graph of such users of 
LiveJournal, who are interested in “mass murder”, “mass 
murders”, and “mass murderers” (key words). Figure 3 
represents the graph of the users retrieved from the site based 
on the above keywords (by the time of crawling, at LJ there 
were 533 users having interest “mass murder” – data extracted 
by LJ API. 220 such nodes were collected by crawler, and 59 
of them – who has at least one friend with such interest are 
represented at figure 3). Edges of the graphs on the pictures 
represent existence of at least one relation: “friend” or 
“friendof”, nodes – user profiles. Graph consists of several not 
connected components. Important from our point of view is at 
least one subgraph, depicted in figure 4. One of the nodes from 
this subgraph (“wekillemall”) is a mutual friend of “resistantx” 
– profile of school shooter Bastian Bosse [1]. So far, we’ve 
collected about 890 000 unique nodes from LJ network (cf. 
F5). 
 
Figure 3.  Results: graph formed out of “mass murder” keyword 
 
Figure 4.  Part of the graph 
183
VIII. CONCLUSIONS AND FURTHER RESEARCH 
This paper presents requirement analysis, overall 
architecture design, and the description of an implemented 
prototype with some results that have been obtained by running 
the prototype. The system goal of the system is to gather 
longitudinal data from social media sites and render it for 
analysis.  Further research is directed to improving the design 
of the architecture and the software, and analysis of the 
obtained networks with static and dynamic SNA methods. 
Important further directions in the architecture research are 
construction of crawler which would be more effective than the 
current one, and to extent the extraction of the data from the 
social media sites. As envisioned in the requirements, one 
should construct a more flexible system of the “rules” that 
could guide the process of data gathering from the social media 
sites. 
 We currently use only text search in the analysis, but 
modern social media sites contain large amounts of other 
multimedia data.  Thus, retrieval and analyzing methods for 
videos, images and audio should be implemented in order to 
enhance the capture of interesting data and persons.  
Another task  is the construction of the data repository, 
which could efficiently unite graph-databases and temporal 
databases: we collect history of changes for the social media 
site, and we foresee the following problems  emerge should we 
store a complete instances of a particular social network (i.e. 
multi-graph), and calculate the differences between them upon 
a request; or, should we only store the differences (i.e. “deltas”) 
found between two traversals of the social network  in the 
repository? The former makes easy to retrieve the entire graph 
at a particular moment (or interval) if needed, but difficult to 
calculate the changes, whereas the latter approach has opposite 
characteristics. Also, an important aspect is spatio-temporality 
of the data: the graph might contain also spatial data (city, 
country, or even address), so special methods should be used 
for storing this information.  
Architecture of the repository for efficient storing and 
querying structured data from different social media sites is a 
research problem as well. The current prototype runs even 
several days on an average laptop performing a single analysis 
task on the 890000 nodes data. 
Regarding the analysis part of the system, we are planning 
to analyze collected data with different methods, and adopt 
mathematical models to describe and forecast the processes 
taking place in complex networks. A further important aspect is 
that it is necessary to collect large amounts of data, and 
presumably only after substantial data reservoir it is possible to 
carry out interesting analyses. We anticipate that we have to 
store tens or hundreds of terabytes data. 
 Although the presented research is mainly software 
engineering, as concerns methods and outcomes, it has strong 
connections also to other disciplines. The analysis part of the 
system should especially draw on the knowledge of diverse 
areas of science such as graph theory, sociology, criminology, 
and psychology.  Currently, we have implemented a prototype 
that is able to collect data from LJ, store it in the database and 
carry out basic social network analysis. Earlier versions of the 
prototype consisted mainly of the crawler that produced data 
into files in a suitable format. These could then be analyzed by 
PAJEK [32] or other social network analysis software. As a 
result we present dataset, collected by the software, which 
represent the structure of particular online hate-groups related 
to real crimes. 
IX. ACKNOWLEDGEMENTS 
The research of the first author was supported by the 
University of Jyväskylä Graduate School in Computing and 
Mathematical Sciences (COMAS), as well as by National 
Research University of Information Technology, Mechanics 
and Optics (NRU ITMO). The authors gratefully acknowledge 
Jorma Kyppö from University of Jyväskylä for his valuable 
comments to the paper and Prof. Andrei Rybin from NRU 
ITMO for providing the expertise that supported the research. 
REFERENCES 
[1] A. Semenov, J. Veijalainen, and J. Kyppö, ”Analysing the presence of 
school-shooting related communities at social media sites”, International 
Journal of Multimedia Intelligence and Security (IJMIS), vol. 1 i. 3, 
2010, pp. 232-268 
[2] M. Hersovici, M. Jacovi, Y. S. Maarek, D. Pelleg, M. Shtalhaim, and S. 
Ur, “The shark-search algorithm. An application: tailored Web site 
mapping”, Comput. Netw. ISDN Syst. vol. 30, i. 1-7, Apr. 1998, pp. 
317-326.  
[3] Q. Tan, and P. Mitra, “Clustering-based incremental web crawling”, 
ACM Trans. Inf. Syst., Vol 28, i. 4, Article 17, Nov. 2010, pp. 1-27 
DOI=10.1145/1852102.1852103 
http://doi.acm.org/10.1145/1852102.1852103 
[4] C. Duda, G. Frey, D. Kossmann, and C. Zhou, “AJAXSearch: Crawling, 
Indexing and Searching Web 2.0 Applications”. Proc. VLDB Endow. 1, 
2, Aug. 2008, pp. 1440-1443. DOI=10.1145/1454159.1454195  
[5] D. H. Chau, Sh. Pandit, S. Wang, and Ch. Faloutsos, “Parallel Crawling 
for Online Social Networks”, Proceedings of the 16th international 
conference on World Wide Web, ACM, May 2007, pp 1283–1284. 
[6] Zh. Chen, Jun Ma, J. Lei, Bo Yuan, and Li Lian, ”An Improved Shark-
Search Algorithm Based on Multi-information”, Proceedings of the 
Fourth International Conference on Fuzzy Systems and Knowledge 
Discovery - Volume 04 (FSKD '07), vol. 4. IEEE Computer Society, 
Washington, DC, USA, pp. 659-658. 
[7] P. Noordhuis, M. Heijkoop, and A. Lazovik, ”Mining Twitter in the 
Cloud: A Case Study”. In Proceedings of the 2010 IEEE 3rd 
International Conference on Cloud Computing (CLOUD '10). IEEE 
Computer Society, Washington, DC, USA, pp. 107-114.  
[8] Sh-Y. Yang, ”Developing of an Ontological Focused-Crawler for 
Ubiquitous Services”, Proceedings of the 22nd International Conference 
on Advanced Information Networking and Applications - Workshops 
(AINAW '08). IEEE Computer Society, Washington, DC, USA, pp. 
1486-1491. 
[9] Wei Fang, Zhiming Cui, and Pengpeng Zhao, ”Ontology-based focused 
crawling of deep web sources”, Proceedings of the 2nd international 
conference on Knowledge science, engineering and management 
(KSEM'07), Springer-Verlag, Berlin, Heidelberg, pp. 514-519. 
[10] L. Kozanidis, ”An Ontology-Based Focused Crawler”, Proceedings of 
the 13th international conference on Natural Language and Information 
Systems: Applications of Natural Language to Information Systems 
(NLDB '08), Springer-Verlag, Berlin, Heidelberg, pp. 376-379. 
[11] Zhiyong Zhang, and Olfa Nasraoui.. ”Profile-based focused crawling for 
social media-sharing websites”. J. Image Video Process, a. 2, Jan. 2009, 
pp. 1-13 
[12] A. Juffinger et al, "Distributed Web2.0 crawling for ontology evolution," 
2nd International Conference on Digital Information Management. 
ICDIM '07, Oct. 2007, vol.2, no., pp.615-620. 
[13] M. Paul, and R. Girju, ”Cross-cultural analysis of blogs and forums with 
mixed-collection topic models”, Proceedings of the 2009 Conference on 
184
Empirical Methods in Natural Language Processing: Volume 3 - 
Volume 3 (EMNLP '09), Vol. 3. Association for Computational 
Linguistics, Stroudsburg, PA, USA, pp. 1408-1417. 
[14] Sheng-Yuan Yang; Chun-Liang Hsu, "An ontology-supported web 
focused-crawler for Java programs,"  3rd IEEE International Conference 
Ubi-media Computing (U-Media),  July 2010, pp.266-271  
[15] S. Batsakis, E. G. M. Petrakis, and E. Milios, "Improving the 
performance of focused web crawlers". Data Knowl. Eng., vol. 68, i. 10, 
Oct. 2009, pp. 1001-1013. 
[16] S. Fortunato. (2009, June)  "Community detection in graphs"., 103 p. 
[Online]. Available: 
http://arxiv.org/PS_cache/arxiv/pdf/0906/0906.0612v2.pdf 
[17] C. Jiyang, O. Zaian, R. Goebel , "Local Community Identification in 
Social Networks," Proceeding of International Conference on Advances 
in ,Social Network Analysis and Mining, 2009. ASONAM '09. July 
2009,  pp.237-242 
[18] J. Leskovec, K. J. Lang, and M. Mahoney, "Empirical comparison of 
algorithms for network community detection", Proceedings of the 19th 
international conference on World wide web (WWW '10). ACM, New 
York, NY, USA, pp. 631-640 
[19] Naizhou Zhang, Shijun Li, Wei Cao, "Applying a Multi-Attribute 
Metrics Approach to Detect Contents of Blog Communities", WiCOM 
'08. 4th International Conference on Wireless Communications, 
Networking and Mobile Computing, 2008. 
[20] A. Harrer, S. Zeini, and S. Ziebarth, "Visualisation of the Dynamics for 
Longitudinal Analysis of Computer-mediated Social Networks-concept 
and Exemplary Cases", From Sociology to Computing in Social 
Networks. Theory, Foundations and Applications, 2010, v.1, pp. 119 - 
134 
[21] R.A.Qureshi,, U.K.Wiil and  and N.Memon, "EWAS: Modeling 
Application for Early Detection of Terrorist Threats", From Sociology to 
Computing in Social Networks. Theory, Foundations and Applications, 
2010, pp. 135 – 155. 
[22] A. Ameripour, M. Newman, and B. Nicholson, "A Convivial Tool? The 
Case of the Internet in Iran." Journal of Information Technology v. 25, 
2010, pp. 244-257. 
[23] Wikipedia, (2011, March 13) "Businness intelligence tools", [Online]. 
Available: http://en.wikipedia.org/wiki/Business_intelligence_tools,  
[24] H. Chen, E. Reid, J. Sinai, A. Silke, and B. Ganor, "Terrorism 
Informatics: Knowledge Management and Data Mining for Homeland 
Security" (1st ed.). Springer Publishing Company, Incorporated. 2008 
[25] O. Mazhelis, J. Markkula, and J. Veijalainen, "An integrated identity 
verification system for mobile terminals", Information Management & 
Computer Security, vol. 13, i 5, 2005, pp. 367-378. 
[26] F.S.L.Lee, D. Vogel, and M. Limayem, “Virtual Community 
Informatics: A Review and Research Agenda”, The Journal of 
Information Technology Theory and Application (JITTA), vol. 5, i. 1, 
2003, pp. 47-61. 
[27] M. Smith, C. Giraud-Carrier, and N. Purser. 2009. "Implicit affinity 
networks and social capital". Inf. Technol. and Management vol. 10, i. 2-
3, Sept. 2009, pp. 123-134. DOI=10.1007/s10799-009-0057-2 
http://dx.doi.org/10.1007/s10799-009-0057-2 
[28] G.A. Hillery, Jr., "Definitions of Community: Areas of Agreement", 
Rural Sociology, vol. 20 i. 4, 1955, pp. 111-122  
[29] L. Rashotte, "Social Influence", The Blackwell Encyclopedia of Social 
Psychology, Malden: Blackwell Publishing, 2007, pp. 562-563 
[30] "Document Object Model", (2011, March 15)  [Online]. Available: 
http://www.w3.org/DOM/ 
[31] K. de la Pena McCook. A Place at the Table: Participating in 
Community Building. Chicago: American Library Association, 2000 
[32] Pajek, (2011, March 18) [Online]. Available:  http://vlado.fmf.uni-
lj.si/pub/networks/pajek/ 
[33] A. Java, X. Song, T. Finin, B. Tseng, "Why we twitter: understanding 
microblogging usage and communities", Proceedings of the 9th 
WebKDD and 1st SNA-KDD 2007 workshop on Web mining and social 
network analysis, 2007, pp. 56–65 
[34] E.S. Orr, M. Sisic, C. Ross, M. G. Simmering, J. M. Arseneault, and R. 
Orr, “The influence of shyness on the use of Facebook in an 
undergraduate sample”. CyberPsychology and Behaviour, vol. 12, i.3, 
2009. pp. 337-340 
[35] E. Stamatatos,  "A survey of modern authorship attribution methods". J. 
Am. Soc. Inf. Sci. Technol., vol. 60, i. 3, Mar 2009, pp. 538-556. 
DOI=10.1002/asi.v60:3 http://dx.doi.org/10.1002/asi.v60:3 
[36] Python Programming Language, (2011, March 10) [Online]. Available: 
http://www.python.org/ 
[37] NetworkX, (2011, March 13) [Online]. Available: 
http://networkx.lanl.gov/ 
[38] R.L.Nord and J.E. Tomayko, “Software architecture-centric methods and 
agile development” . IEEE Software, March-April 2006, pp. 47-53.  
[39] W. Richards, and N. Wormald. “Representing Small Group Evolution”, 
Proceedings of the 2009 International Conference on Computational 
Science and Engineering - Volume 04 (CSE '09), vol. 4, IEEE Computer 
Society, Washington, DC, USA, 2009, pp. 159-165. 
[40] Jing Jiang et al. "Understanding latent interactions in online social 
networks". In Proceedings of the 10th annual conference on Internet 
measurement (IMC '10). ACM, New York, NY, USA, 2010 pp. 369-
382. 
[41] "The Cascading Effects of the Arab Spring", (2011, February 23) 
[Online]. Available: http://www.miller-mccune.com/politics/the-
cascading-effects-of-the-arab-spring-28575/ 
[42] C. Aggarwal, Social Network Data Analytics,. (Ed.) 1st Edition., 2011 
[43] O. Preiss, and A. Wegman , "Stakeholder's Discovery and Classification 
based on System Science Principles", Proceedings of the Second Asia-
Pacific Conference on Quality Software, IEEE Computer Society 
Washington, DC, USA, 2001,  0-7695-1287-9/01  
[44] J. Shore, & S. Warden, "The Art of Agile Development". O’Reilly 
Media, Inc. 
[45] A. M. Kaplan, M. Haenlein, "Users of the world, unite! The challenges 
and opportunities of Social Media", Business Horizons, Vol. 53, i. 1, 
Jan.-Feb. 2010, pp. 59-68, ISSN 0007-6813 
[46] H. Krasnova, T. Hildebrand, O. Guenther, O. Kovrigin, and A. 
Nowobilska, "Why Participate in an Online Social Network? An 
Empirical Analysis". ECIS 2008 Proceedings, 2008 
[47] P. Kollock, "The Economies of Online Cooperation: Gifts and Public 
Goods in Cyberspace", 11 New Fetter Lane, London EC4P 4EE: 
Routledge, 1999, ch. 9, pp. 220–239. [Online]. Available: 
http://www.sscnet.ucla.edu/soc/faculty/kollock/papers/economies.htm 
 
 
 
185
