 
 
 
 
 
 
 
 
ISSN 1893-6563 
ISBN 978-82-321-0366-9
Norwegian Information Security Conference
Norsk Informasjonssikkerhetskonferanse
NISK 2013
Stavanger, 18th–20th November 2013
Redaktører: Chunming Rong og Vladimir Oleshchuk
NISK 2013             Norwegian Inform
ation Security Conference / Norsk inform
asjonssikkerhetskonferanse
Norwegian Information Security Conference
Norsk Informasjonssikkerhetskonferanse
NISK 2013
Stavanger,18th–20th November 2013
NIK-styret og redaksjonskomité
Lars Ailo Bongo Universitetet i Tromsø
Siri Fagernes Høgskolen i Oslo og Akershus
Dag Haugland Universitetet i Bergen
Erik Hjelmås Høgskolen i Gjøvik
Hein Meling Universitetet i Stavanger (redaktør og arrangør)
Tom Heine Nätt Høgskolen i Østfold
Andreas Prinz Universitetet i Agder
Erlend Tøssebro Universitetet i Stavanger (redaktør og arrangør)
Ingrid Wu Universitetet i Oslo
Trond Aalberg NTNU (styreleder)
© NISK-stiftelsen og Akademika forlag, Trondheim 2013
ISBN 978-82-321-0366-9
Det må ikke kopieres fra denne boka ut over det som er tillatt 
etter bestemmelser i lov om opphavsrett til åndsverk, og 
avtaler om kopiering inngått med Kopinor. Dette gjelder også filer, 
kode eller annen gjengivelse tilknyttet e-bok.
Redaktører: Chunming Rong (UiS) & Vladimir Oleshchuk (UiA)
Digital trykk og innbinding: AIT Oslo AS
Vi bruker miljøsertifiserte trykkerier
Akademika forlag
Oslo/Trondheim
www.akademikaforlag.no
Forlagsredaktør: Lasse Postmyr (lasse.postmyr@akademika.no)
Preface
Welcome to NISK 2013 - the 6th Norwegian Information Security Conference, 
in Stavanger, 18-20 November, 2013. Following long tradition, the conference is 
collocated with NIK and NOKOBIT. For this year, the NISK is sponsored by 
Forum for Research and Innovation in Security and Communications (FRISC), a 
value network in the VERDIKT program, and the PhD track/seminar is jointly 
sponsored  by  FRISC  and  the  COINS  Research  School  of  Computer  and 
Information Security, both funded by the Research Council of Norway (RCN).
This  year  NISK received  26  submissions  form both  Norwegian  and  foreign 
institutions.  At  least  three  Program  Committee  members  reviewed  each 
submission.  Based  on  the  strict  evaluations,  12  papers  were  selected  for 
presentation  and  publication  in  the  Proceedings.  In  addition  to  the  regular 
presentations, Professor Slobodan Petrovic from Gjøvik University College will 
give a tutorial on "Search Algorithms for Intrusion Detection"; and there is a 
new FRISC/COINS PhD track/seminar organized after the regular sessions. 
The  keynote  speech  this  year  is  “Supporting  cloud  research  with  secure  
infrastructures  -  bridging  the  gaps”,  given  by  Dr.  Jens  Jensen  from  the 
Rutherford Appleton Laboratory, Oxford, UK.
We would like to thank all members of the Program Committee for their efforts, 
valuable comments and detailed reviews.
Chunming Rong
NISK 2013 PC Chair
Vladimir Oleshchuk
PC Vice Chair
Stavanger, October 2013
Program Committee
Chunming Rong, University of Stavanger (Chair)
Vladimir Oleshchuk University of Agder (Vice Chair)
Aryan TaheriMonfared University of Stavanger (PhD Chair)
Members:
Antorweep Chakravorty University of Stavanger
Patrick Bours Gjøvik University College
KristianGjøsteen Norwegian Univ. of Science and Technology
Martin Gilje Jaatun SINTEF ICT
Hanno Langweg Gjøvik University College 
Stig Frode Mjølsnes Norwegian Univ. of Science and Technology 
Eli Winjum Norwegian Defence Research Establishment
Øyvind Ytrehus University of Bergen
Quan Zhou University of Stavanger
Innhold
Analysis of Norway’s Cyber and Information Security Strategy...........................................1
Kirsi Helkala1 and Nils Kalstad Svendsen
Adaptive Security for the Internet of Things Reference Model...........................................13
Kashif Habib and Wolfgang Leister
A Framework for Comparison and Analysis of Information Security Investment Models..25
Pankaj Pandey and Einar Arthur Snekkenes
Author Identification from Text-based Communications: Identifying generalized features
and computational methods..................................................................................................37
Ambika Shrestha Chitrakar and Katrin Franke
A Survey of Process Activity Tracking Systems..................................................................49
Yi-Ching Liao and Hanno Langweg
Modeling Adaptive Security in IoT Driven eHealth............................................................61
Waqas Aman
A MapReduce based K-Anonymization Solution for Sharing of Smart Home Data...........70
Antorweep Chakravorty, Tomasz Wlodarczyk, Chunming Rong
A Taxonomy of Challenges in Information Security Risk Management.............................76
Gaute Wangen and Einar Snekkenes
Security Analysis of the Terrestrial Trunked Radio (TETRA) Authentication Protocol......88
Shuwen Duan, Stig Mjolsnes and Joe-Kai Tsay
Automatic rule-mining for malware detection employing Neuro-Fuzzy Approach..........100
Andrii Shalaginov and Katrin Franke
A space-efficient multi-receiver public key algorithm.......................................................112
Sigurd Eskeland
Risk perception vs. usage control on Facebook.................................................................122
Åsmund Ahlmann Nyre and Martin Gilje Jaatun
Author Identification from Text-based
Communications: Identifying generalized features
and computational methods
Ambika Shrestha Chitrakar and Katrin Franke
ambika.chitrakar, katrin.franke@hig.no
Norwegian Information Security Laboratory
Gjøvik University College
Abstract
Author identification from text-based communication is a process of
determining authorship of an anonymous text based on writing styles
of an author. Writing styles (also known as textual features) are the
main elements for author identification. It is therefore very important
to analyze them and identify the most promising features. This paper
seeks to analyze and identify promising generalized features and better
methods for author identification that can be used for text in any domain
and are valid for even short texts. In order to achieve these objectives we
analyzed generalized features from four different feature groups by using
five feature-selection and five classification methods. On the basis of
experimental results, we found that the features of lexical feature group
were more appropriate, the feature-selection methods worked well with
a combination of classification methods. The combination of correlation
feature selection (CFS) with random forest and support vector machine,
information gain with näıve bayes, and symmetrical uncertainty with
bagging and decision tree (C4.5) proved to be more promising. After
analyzing overall classification accuracies, bagging, random forest, and
C4.5 classification methods proved to outperform others.
Keywords. Pattern recognition, Machine learning, Feature evaluation
and selection, Text processing, Document and text processing, Document
analysis
1 Introduction
Author identification is a part of authorship analysis studies and is a process
of determining or verifying the authorship of an anonymous text, based on the
documents written by that author [11,20]. The importance of author identification
increases with the increasing number of text related crimes. Nowadays, crime
through Internet and mobile has been increased. Some reasons behind it can be
quick communication, no physical interaction and possibility of hiding own identity.
This paper was presented at the NISK-2013 conference.
37
People with wrong intention can use online services like Skype, E-mail, Social Medias
(like facebook, twitter and hi5), and telephone services like SMS which are very
effective for communication. We also know many cases where such services have been
compromised to abuse innocent people. Investigators may identify the criminals by
investigating the artifacts collected from different tools that were used for crime but
what if a criminal is able to hide himself? There can be different solutions but one
solution can be by using author-identification process.
From prior studies, it has been seen that most of the researchers deal with long
texts, few authors and add content specific features like n-grams, keywords, and
phrases from text in order to achieve high accuracies. But in real scenario the
case can be different and the approaches that have been used before may not be
appropriate. Moreover, content-specific features are not generalized and cannot be
used in other unseen datasets from different domains. As author identification can
be applied to any text domain, analyzing and identifying generalized features and
methods that can better contribute in identifying the correct author of a text is very
important.
This paper focuses on analyzing generalized features for author-identification
problem, based on machine-learning approach and aims to solve four research
questions: i) which generalized features are more promising?, ii) which generalized
feature groups are more promising?, iii) which feature-selection methods are better
to remove redundant and irrelevant features?, and iv) which classification methods
are better to classify the questioned text? In order to solve these research questions,
this paper uses Enron dataset [6], constructs generalized features based on four
feature groups (lexical, syntactic, structural, and gender specific), uses five methods
(CFS, ReliefF, Chi Square, Information Gain, and Symmetrical Uncertainty) for
feature selection, and also uses five methods (Random Forest, Bagging, C4.5, Näıve
Bayes, and SVM) for classification.
As the findings of this paper are based on representative dataset with short as
well as long texts and with large number of authors, the results could be useful for
solving author-identification problem of short text documents like SMS, emails, chat
messages as well as for long text and even with large number of authors.
The remainder of this paper is structured as follows: Section 2 provides related
works about author identification, Section 3 provides the choice of methodology for
the experiment, Section 4 shows the experimental details and its results, Section
5 discusses the findings of the experiment, Section 6 concludes the findings of this
paper and Section 7 shows the future perspectives of this research.
2 Related works
According to Zheng et al. [20], the roots of authorship analysis are from the
linguistic research area called “stylometry” and defines authorship analysis as a
process of examining the characteristics of a piece of writing to draw conclusions
on its authorship. This paper [20] also shows that author identification is only
a part of authorship analysis where authorship analysis also contains authorship
characterization and similarity detection.
According to Zheng et al. [20], the origin of authorship identification is considered
to be in 18th century when English logician Augustus de Morgan suggested that
authorship might be settled by determining if one text contained more longer
words than another. His hypothesis was tested by Mendenhall in 1887 [12]
38
who subsequently published his results of authorship attribution among Bacon,
Marlowe, and Shakespeare. Another most popular study in the field of authorship
identification is by Mosteller and Wallace in 1964 [14] who studied the mystery of
authorship of the Federalist Papers. The conclusion of this study was accepted by
historical scholars and became milestone for this field [20]. These previous studies
are based on literary works but the features that were extracted for these kinds of
documents may not be appropriate for online documents as they are short in length
and writing style depends on the target recipient. Moreover, online messages can
be written in informal manner without caring the grammar and spelling mistakes.
Many researches have been done on online documents and one of them is a study
from Iqbal et al. [9]. They used novel approach, i.e. the use of “frequent pattern” for
mining write-prints for authorship attribution in e-mail forensics. They used Enron
email dataset corpus [6] for their research where they extracted features that were
more frequent for an author but less for other authors and they achieved accuracy
between 80% to 90%.
Many features have been extracted or categorised but none of them are
considered to be the best. One example is from the study of Rudman [15] who
summarized that almost 1000 different style features had been used in previous
authorship identification studies, but no best set of features is agreed for a wide
range of application domain. Neuforn and Franke [19] used qualitative text analysis
in their paper and analyzed features like outer appearance of the message, syntax,
lexix and empathical communication. Zheng et al. [20] used lexical, syntactical,
structural and content specific features in their research. In this paper, we focus on
features mostly from Zheng et al. [20].
Lexical features are the sequence of words grouped into sentences and they
statistically measure the lexical variation [2, 16]. Normally, lexical features include
all the features related to the construction of word, sentences like average character
length, average word length, average words in sentences, character n-grams, word
n-grams, word frequencies etc. Details about this feature can be found in [16, 20].
Syntactic features consider the relation between content words that form the
sentence. Main assumption behind it, is that authors have preferred syntax that
they commonly use without realizing it [16]. Abbasi and Chen [1] says that these
features represent the style or preference of an author because of which these features
perform well in authorship-identification problem. Function words, part-of-speech
(POS), punctuation marks are some of the examples of syntactic features [20].
Structural features consider the layout, fonts, images, greetings, paragraph
structure, HTML tags, comments etc. of any text document [2,16,18]. They are very
useful when dealing with online texts, e-mails and programming codes [1, 4, 18, 20]
but these features are specific for different applications and may not be in control
of authors [10]. Stamatatos [16] says that accurate tools are required to extract
structural features.
Content specific features are the features that are specific for a text; it may not be
useful for a wide range of topics but Stamatatos [16] says that these types of feature
might perform better if all the available texts are within the same topic and genre.
For example, Zheng et al. [20] used content specific features like “deal”, “sale”,
“paypal”, “check” keywords and Abbasi and Chen [2] used words like “laptop”,
“notepad” in their framework.
39
3 Methodology
It is very important to choose proper features and methods in order to derive
promising features and better computational methods. This section provides
information about selected feature groups and methods that have been used in the
experiment.
Choice of feature groups
One of the objectives of this paper is to identify promising generalized features for
author identification based on Enron [6] dataset. In order to find promising features
we deal with features from four feature groups which are mostly collected from the
research papers [7,13,20]. These four feature groups are: i) lexical, ii) syntactic, iii)
structural, and iv) gender specific feature groups.
Choice of feature selection methods
Feature selection is normally performed to select relevant, non-redundant and
informative features from a large set of features. It helps in limiting the storage and
increasing the speed of algorithm by reducing the feature set size, helps in improving
performance of classification accuracy, and also helps in understanding the data [8].
While using feature-selection method, relevant features should not be removed
otherwise it can provide less performance on classification. For this experiment,
we deal with filter-model-based feature-selection methods as they are faster than
other models (wrapper and ensemble) and useful in analyzing data [8]. We planned
to use five different feature-selection methods for the experiment: i) Correlation-
based feature selection (CFS), ii) ReliefF, iii) Chi Square, iv) Information Gain, and
v) Symmetrical Uncertainty.
Choice of learning algorithms
According to “No Free Lunch Theorem”, no classification method is considered to be
superior over other classification methods [5]. Therefore, we planned to use as much
classifiers as possible and identify the best one depending on their accuracies. We
chose five different supervised learning algorithms: i) Random Forest, ii) Bagging,
iii) Decision tree (C4.5), iv) Näıve Bayes, and v) Support Vector Machine(SVM). In
order to evaluate the performance of each classifier, we calculate accuracy on test
instances which is defined as:
Accuracy =
No. of correctly classified instances
Total number of instances
,
Choice of performance estimator
We planned to use 10-fold stratified cross-validation method to estimate the overall
performance of each classifier. According to [17], 10-fold cross-validation has
minimal effect of sample characteristics and it also has ability to investigate the
stability of items loading on multiple factors. The final accuracy of this method will
be the median average accuracy from each fold.
40
4 Experiment and results
This section provides detail information about experimental design, dataset,
extracted features, classification results and derivation of promising generalized
features based on classification results.
Experimental design
Figure 1 provides a sequential flow diagram showing important elements of the
experiment. It is mainly divided into two parts: i) feature-subset generation, and
ii) learning and classification. The idea is to generate different feature subsets that
contain instances with generalized features and create five different classification
models for each feature subset.
Figure 1: Experimental design
In figure 1, Enron dataset [6] is an input and we perform dataset cleaning,
feature extraction on it and generate different feature subsets. We create three
main feature sets (Full Feature Set, Feature Category Set, and Selected Feature Set)
and their subsets. The full feature set has only one subset which contains feature
vectors with all the extracted features. Feature category set has four subsets where
each subset contains feature vectors from specific feature group (lexical, syntactic,
structural, and gender specific), and selected feature set has 24 subsets which are
based on five feature-selection methods (CFS, ReliefF, Chi Square, Information
Gain, and Symmetrical Uncertainty). We show only five main subsets (CFS, ReliefF,
ChiSquare, InfoGain, and SymmetricUncert) in the figure but each subset has
multiple numbers of subsets with varying number of selected features making total
24 subsets.
41
By using stratified 10-fold cross-validation, instances in each feature subset are
divided into train and test patterns. Train patterns are used for feature selection
and training the classification models whereas test patterns are used for classification
and verification. After completing classification, classification outputs are generated
by each model for each feature subset. Output of these classification models are the
median average accuracies obtained from 10 iterations of 10-fold cross-validation
for each feature subset. For each classifiers, 10 outputs are shown in figure 1 but
actually there are 29 outputs; 24 for “Selected Feature Set”, 4 for “Feature Category
Set” and 1 for “Full Feature Set”. “<featuresubset>-<classifier>” is the naming
convention for output. First part before “-” stands for feature subsets where NoFS
is for full feature set, CFS, InfoGain, SymmetricUncert, ReliefF, and ChiSquare are
for selected feature set and remaining are for feature category set. Another part
after “-” stands for the classifiers. Final accuracies of these outputs are compared
to achieve the objectives of this paper.
Dataset
This experiment uses Enron dataset that contains real emails of Enron employees.
This dataset contains 255,636 email records and 87,474 authors. It is made
public and posted online by Federal Energy Regulatory Commission during its
investigation. Now, it is freely available for research purpose [6].
SN# Actions Messages People
Records before pre-processing 255,636 87,474
1 After deleting records that are linked to no address@enron.com email 254,963 87,473
2 After deleting records with message subject containing ‘re:’, ‘fw:’ and ‘fwd:’
texts
150,533 -
3 After deleting records with less than 21 words in message body 125,893 -
4 After deleting records that have number of emails less than 106 51,822 71,029
5 After deleting people who does not have email contents - 149
6 After deleting message records with ‘<html>’ and ‘<!doctype’ tags 51,446 -
7 After deleting message records that contain email body starting with ‘¿’ 51,414 -
8 After deleting message records that contain ‘[image]’ tag 50,307 -
9 After deleting duplicate emails (subject and contents are compared) 47,944 -
10 After removing messages that are blank while removing email contents below
‘—– Forwarded by’ text
42,182 -
11 After removing messages that are blank while removing email contents below
‘—Original Message’ text
42,182 -
12
❼ Replace =01, text by ’ (single code character) - 671 records affected
❼ Replace =20\n text by space character - 9 records affected
❼ Replace =20\r\n text by space character - 1148 records affected
❼ Replace =\n text by nothing - 858 records affected
❼ Replace =\r\ n text by nothing - 1418 records affected
❼ Replace =09 text by tab character - 424 records affected
- -
13 After deleting records with less than 21 words in message body 36,050 -
14 After deleting records that have number of emails less than 106 33,866 118
Table 1: Dataset cleaning steps performed in experiment
We kept Enron dataset intact so that we keep original dataset as a backup and
created a new dataset by copying its main tables. We named this new dataset as a
processed or experimental dataset. After generating this dataset, we performed 14
different cleaning steps on it so that only useful records will be available for analysis.
Table 1 shows step wise cleaning actions that were performed in the experiment.
After completing all the cleaning steps, we got total 33,866 email messages and
118 people. We performed feature extraction and analysis on these remaining records
only.
42
Extracted features
Figure 2: Extracted features for the experiment
Figure 2 shows the list of lexical, syntactic, structural, and gender specific
features that were extracted for the experiment. We extracted total 559 generalized
features from these four feature groups where 162 are from lexical, 381 from
syntactic, 7 from structural, and 9 are from gender specific group. Lexical features
were mostly collected from Zheng et al. [20] and Benjamin [7], syntactic features from
Zheng et al. [20], Benjamin [7], and Rohith and Yejin [13], structural features from
Zheng et al. [20] and Benjamin [7], and gender specific features from Benjamin [7]
respectively.
Accuracies of feature subsets
This subsection provides classification accuracies for each available feature sets and
helps in solving the research problems of this paper. Table 2 shows the accuracy
values in percentage for each feature subsets with five different classifiers and this
table is divided into three main feature sets i.e., i)Full feature set, ii)Feature category
set, and iii) Selected feature set.
Third main feature set contains 24 feature subsets based on the feature-selection
methods. The naming format of these subsets is “<feature-selection method>-
<number of top selected features>”. First part of the naming convention before “-”
indicates one of the feature-selection method and other part indicates the number
of highly ranked selected features in each subset. We chose multiple threshold for
43
selecting highly ranked features as selecting only one threshold may not give us
appropriate result for all the classifiers. It is because some classifiers work well with
less number of features and some need larger number of features. While comparing
the results of feature-selection methods, we only consider the highest accuracy for
each classifier from similar subsets (eg.: we consider the results of CFS-70 subset
instead of CFS-53 for CFS).
Features Subsets Features Random Forest Bagging C4.5 Näıve Bayes SVM
Full Feature Set
Full Feature 549 48.87% 58.04% 51.33% 38.06% 38.15%
Feature Category Set
Lexical 162 55.17% 59.58% 53.39% 44.74% 39.08%
Syntactic 381 43.61% 47.51% 41.87% 31.21% 16.70%
Structural 7 41.86% 42.75% 41.22% 10.09% 40.48%
Gender Specific 9 28.78% 28.58% 28.20% 15.69% 9.23%
Selected Feature Set
CFS-53 53 55.09% 56.41% 50.36% 41.38% 41.02%
CFS-70 70 55.42% 57.46% 50.66% 42.65% 42.34%
InfoGain-21 21 53.58% 53.32% 47.98% 34.70% 27.46%
InfoGain-35 35 54.04% 54.86% 49.11% 35.27% 29.38%
InfoGain-47 47 53.50% 55.07% 49.06% 36.54% 31.10%
InfoGain-79 79 53.80% 56.72% 49.62% 40.14% 33.57%
InfoGain-159 159 53.10% 57.92% 50.20% 44.97% 35.55%
InfoGain-420 420 49.76% 58.09% 51.05% 38.18% 37.71%
SymmetricUncert-40 40 53.76% 54.84% 49.45% 34.83% 29.77%
SymmetricUncert-64 64 54.13% 56.11% 49.19% 37.89% 32.34%
SymmetricUncert-78 78 53.62% 56.02% 49.18% 39.58% 33.55%
SymmetricUncert-118 118 53.03% 57.03% 49.79% 42.40% 34.45%
SymmetricUncert-419 419 49.15% 58.18% 51.23% 38.17% 37.71%
ChiSquare-30 30 53.03% 53.93% 49.48% 32.33% 29.01%
ChiSquare-45 45 53.54% 55.37% 50.28% 34.90% 31.22%
ChiSquare-70 70 53.52% 56.11% 50.33% 38.06% 33.04%
ChiSquare-149 149 52.86% 57.27% 51.04% 39.38% 35.29%
ChiSquare-422 422 49.31% 58.05% 51.07% 38.37% 37.72%
ReliefF-91 91 49.47% 52.62% 46.18% 41.29% 29.56%
ReliefF-111 111 49.92% 53.89% 47.16% 42.06% 30.82%
ReliefF-142 142 51.28% 56.03% 48.86% 42.77% 34.86%
ReliefF-194 194 50.74% 57.16% 49.69% 41.14% 35.63%
ReliefF-253 253 50.51% 57.80% 50.80% 40.62% 36.29%
ReliefF-306 306 50.51% 57.95% 50.95% 40.04% 36.73%
Table 2: Accuracies of feature subsets
First column of Table 2 lists all the feature subsets of three main sets, second
column lists the number of features (threshold) in each feature subset and remaining
five columns list the accuracies of each feature subset with random forest, bagging,
C4.5, näıve bayes, and SVM classifiers respectively. Some columns are highlighted
with yellow color in background and green color in border. These boxes appear only
once in each last five columns and represent the highest accuracy among all the
feature subsets with a particular classifier. We have also highlighted some columns
with green border in order to represent the accuracies that are highest within the
same type of subsets for each classifier.
44
SN# Features Lexical CFS-70 IG-159
1 Total number of characters (M) 1 1 1
2 Ratio of alphabetic characters to M 1 1 1
3 Ratio of upper-case characters to M 1 1 1
4 Ratio of digit characters to M 1 1 1
5 Ratio of white spaces to M 1 1 1
6 Ratio of tabs to M 1 1 1 1
7 Ratio of frequency of alphabetic characters (capital and small letters)
to M
26 8 26
8 Ratio of frequency of special characters to M 33 13 18
9 Total number of characters in subject (S) 1 1 1
10 Ratio of alphabetic characters in subject to S 1 1 1
11 Ratio of upper-case characters in subject to S 1 1 1
12 Ratio of digit characters in subject to S 1 0 1
13 Ratio of white spaces in subject to S 1 1 1
14 Ratio of tabs in subject to M 1 0 0
15 Ratio of frequency of alphabetic characters (capital and small letters)
to S
26 10 20
16 Ratio of frequency of special characters to S 33 4 5
17 Total number of words (W) 1 0 1
18 Ratio of short words (less than 4 characters) to W 1 1 1
19 Average word length 1 0 1
20 Average number of characters in sentences 1 1 1
21 Average number of words in sentences 1 1 1
22 Ratio of different words to W 1 0 1
23 Ratio of hapax legomena (words occurring only once) to W 1 0 1
24 Ratio of hapax dislegomena (words occurring only twice) to W 1 0 1
25 Ratio of frequency of a word (maximum occurring word) to W 1 0 1
26 Ratio of a frequency of a word (second maximum occurring word) to
W
1 0 1
27 Ratio of word length distribution (frequency of 1-20 length words) to
W
20 1 14
28 Total number of words in subject 1 1 1
29 Average word length in subject 1 1 1
30 Ratio of function word frequencies to W 0 14 37
31 Ratio of function words to W 0 0 1
32 Ratio of stop word frequencies to W (only those not listed in function
words)
0 0 5
33 Ratio of stop words (only those not listed in function words) to W 0 0 1
34 Total number of sentences 0 1 1
35 Total number of paragraphs 0 1 1
36 Total number of lines 0 1 1
37 Average characters per paragraph 0 1 1
38 Average words per paragraph 0 1 1
39 Ratio of words ending with “able”to W 0 0 1
40 Ratio of words ending with “al”to W 0 0 1
41 Ratio of words ending with “ive”to W 0 0 1
42 Ratio of words ending with “ly”to W 0 0 1
Total 162 70 159
Table 3: Features with highest accuracies
Features with highest accuracies
From Table 2, we can see that lexical, CFS-70 and InfoGain-159 are the feature
subsets that have contributed the most in getting highest accuracies with different
classifiers for processed dataset. Lexical feature set has achieved the highest
accuracies with bagging and C4.5, InfoGain-159 with näıve bayes, and CFS-70
with random forest and SVM. Therefore, we consider the features available in these
feature subsets most valuable than other features.
Table 3 presents the derived features that have contributed in achieving the
highest accuracies with different classifiers. We can see that out of 559 extracted
features, the combination of only 215 features is able to give better accuracy results
with five classifiers. This table also shows that most of the selected features are
lexical features. Lexical column of this table has only lexical type of features.
CFS-70 feature subset has selected 51 features from lexical feature group, 14 from
syntactic feature group, and 5 from structural feature group. Similarly, IG-159
feature subset has selected 106 features from lexical feature group, 44 from syntactic,
5 from structural, and 4 from gender specific feature groups.
5 Discussion
Analyzing content independent features are already done in many research papers
but they have used messages that have long text and few authors. For example, [20]
45
has analyzed features with average 169 words per message on English dataset and
average 807 words on Chinese dataset with only 20 authors on both. They have also
achieved good results on one of these datasets. In online texts like chat messages
and emails, the length of contents can vary depending on person and situation.
According to previous research papers, researchers have achieved high accuracies
on long texts but poor accuracies on short texts. An experiment in [3] shows that
the accuracy decreases with the number of authors examined and the length of the
content in dataset.
We have considered Enron email contents with more than 20 words which is
very small number of words compared to other research papers. The word count
of messages in our dataset ranges from 21 to 10,302 words, where out of 33,866
instances, 25,420 instances have word count less than 150 and 12,811 instances
have less than equal to 50 words. Total number of authors in our dataset is 118.
We performed experiment keeping in mind that we might get less classification
accuracies but until and unless we test we never know. We tried to solve our four
research problems by comparing the accuracies of 29 different feature subsets with 5
different classifiers. If we see Table 2, the highest accuracy is 59.58% which has been
achieved by the combination of lexical feature subset and bagging classifier. The
accuracies that we have achieved seem to be very less as compared to the accuracies
obtained by [20] and other research papers but the nature of dataset, number of
authors and records that we have considered in this experiment is different and we
have already seen that accuracy varies with such parameters. Therefore, we derive
the solutions for our research questions based on our experimental results and we
consider them valid for the used processed dataset, extracted generalized features
and used machine learning methods only.
By analyzing promising features from Table 3, we knew that features from
lexical, CFS-70, and InfoGain-159 have contributed the most in achieving the highest
accuracies with all classifiers which shows that there is still possibility of getting high
accuracies with their combination. We also found that features from lexical group are
selected the most than other groups. This group has also achieved high accuracies
when classified separately. From Table 2, it has been also observed that no single
feature-selection method independently has achieved highest accuracies with all the
classifiers. It means that the importance of feature-selection method depends on a
combination of classification method and feature-selection method. This result also
emphasizes in using ensemble type of feature-selection method. From the results, it
seems that the combination of CFS with random forest and SVM, information gain
with näıve bayes, and symmetrical uncertainty with bagging and C4.5 are more
promising than other feature-selection methods. When we compare the highest
accuracies obtained by all 5 classifiers, we can see that highest accuracy has been
obtained by bagging, random forest, C4.5, näıve bayes and SVM respectively. The
overall experimental result also shows that feature selection is not always necessary
when it is classified by good classifiers. For example, full feature subset has also
achieved better results when classified by bagging and C4.5 classifiers.
6 Conclusion
There were four goals of this paper related to author identification problem which
were to identify promising i) generalized features, ii) generalized feature groups,
iii) feature-selection methods and, iv) classification methods. According to the
46
experimental results, we derived 215 features that are listed in Table 3 as the most
promising generalized features. Among them 162 features are from lexical group, 44
from syntactic, 5 from structural and 4 from gender specific features. Therefore, we
consider lexical feature group as more promising feature group than others. We used
5 feature-selection methods to select relevant features but no single feature-selection
method performed well with all 5 classifiers. We derived that the combination of CFS
with random forest and SVM, information gain with näıve bayes, and symmetrical
uncertainty with bagging and decision tree (C4.5) classifiers proved to be better
than others. Therefore, we also consider these combinations more promising than
other combinations. After analyzing all accuracies of 29 feature subsets we consider
that bagging, random forest, and decision tree (C4.5) classifiers respectively are
more promising classification methods than others. These findings are valid for
the dataset, features, and methods that have been used in this experiment. In
conclusion, this paper has solved all the research questions on the basis of short as
well as long texts and the results that have been achieved might be beneficial and
valid for online short documents with even large number of authors.
7 Future work
There are lots of possibilities that can be added in this framework. It is always
better to play-around with large dataset with lot of features and methods. If we
look at dataset cleaning actions, we have removed some emails (with re:, fw:, or
fwd: texts in subject, [image] or HTML tags in email content etc.) which can be
included in future. We can also identify more features and use more methods for
analysis. This paper has derived 215 most promising generalized features but has
not tested their combination. In future, the accuracies of such combinations can be
tested against current results. Apart from improvements and extension of this work,
the result of this paper can be tested on unseen dataset from different text domain
and verified by other researchers.
References
[1] A. Abbasi and H. Chen. Applying authorship analysis to extremist-group web
forum messages. IEEE Intelligent Systems, 20:67–75, 2005.
[2] A. Abbasi and H. Chen. Writeprints: a stylometric approach to identity-level
identification and similarity detection in cyberspace. ACM Transactions on
Information Systems, 26(2):1–29, 2008.
[3] A. Abbasi, H. Chen, and J. Nunamaker. Stylometric identification in electronic
markets: scalability and robustness. Journal of Management Information
Systems, 5(1):49–78, 2008.
[4] M. W. Corney, A. M. Anderson, G. M. Mohay, and O. De Vel. Identifying the
authors of suspect email. Computers and Security, 2001.
[5] Richard O. Duda, Peter E. Hart, and David G. Stork. Pattern Classification.
Wiley-Interscience, 2000.
[6] Enron. Enron email dataset. http://www.cs.cmu.edu/~enron/,http://sgi.
nu/enron/corpora.php, 2012. [Accessed June, 2012].
47
[7] Benjamin C. M. Fung. Stylometric features. http://users.encs.concordia.
ca/~fung/pub/Stylometric.pdf, 2012. [Accessed June, 2012].
[8] Isabelle Guyon, Steve Gunn, Masoud Nikravesh, and Lotfi A. Zadeh.
Featureextraction: Foundations and applications. Series Studies in Fuzziness
and Soft Computing, Physica-Verlag, Springer, 2006.
[9] Farkhund Iqbal, Rachid Hadjidj, Benjamin C.M. Fung, and Mourad Debbabi.
A novel approach of mining write-prints for authorship attribution in e-mail
forensics. Original Research Article Digital Investigation, 5:S42–S51, 2008.
[10] P. Juola. Authorship attribution. Found. Trends Inf. Ret. 1, pages 233–334,
2006.
[11] Robert Layton, Paul Watters, and Richard Dazeley. Authorship attribution for
twitter in 140 characters or less. In Cybercrime and Trustworthy Computing
Workshop (CTC), pages 1–8, July 2010.
[12] T. C. Mendenhall. The characteristic curves of composition. Science, 11:237–
249, 1887.
[13] Rohith K. Menon and Yejin Choi. Domain independent authorship attribution
without domain adaptation. In Proc. of Recent Advances in Natural Language
Processing, pages 309–315, 2011.
[14] F. Mosteller and D. L. Wallace. Inference and disputed authorship: The
federalist. Addison-Wesley, Reading, Mass., 1964.
[15] J. Rudman. The state of authorship attribution studies: Some problems and
solutions. Computers and the Humanities, 31:351–365, 1997.
[16] E. Stamatatos. A survey of modern authorship attribution methods. Journal
of The Americal Society for Information Science and Technology, 60:538–556,
2009.
[17] Mark van der Gaag, Tonko Hoffman, Mila Remijsen, Ron Hijman, Lieuwe
de Haan, Berno van Meijel, Peter N. van Harten, Lucia Valmaggia, Marc
de Hert, Anke Cuijpers, and Durk Wiersma. The five-factor model of the
positive and negative syndrome scale ii: A ten-fold cross-validation of a revised
model. Schizophrenia Research, 85:280 – 287, 2006.
[18] O. Y. De Vel, A. Anderson, M. Corney, and G. M. Mohay. Mining email content
for author identification forensics. SIGMOID Record, 30(4):55–64, 2001.
[19] Daniela Stokar von Neuforn and Katrin Franke. Reading between the
lines: Human-centred classification of communication patterns and in-
tentions. http://www.public.asu.edu/~huanliu/sbp08/Presentations/
Papers/05_kyfranke_sbp2008.pdf. [Accessed May, 2013].
[20] Rong Zheng, Jiexun Li, Hsinchun Chen, and Zan Huang. A framework
for authorship identification of online messages: Writing-style features and
classification techniques. J. Am. Soc. Inf. Schi. Technol., 53(22):425–439, 2006.
48
 
 
 
 
 
 
 
 
ISSN 1893-6563 
ISBN 978-82-321-0366-9
Norwegian Information Security Conference
Norsk Informasjonssikkerhetskonferanse
NISK 2013
Stavanger, 18th–20th November 2013
Redaktører: Chunming Rong og Vladimir Oleshchuk
NISK 2013             Norwegian Inform
ation Security Conference / Norsk inform
asjonssikkerhetskonferanse
