Language Identification: Proposition of a New Optimized Variant for the Method of 
Cavenar and Trenkle 
 
Said Kadri 
Department of  ICST, Faculty of 
Mathematics and Computer Sciences 
University of M’sila 
M’sila, Algeria, 28000 
e-mail: kadri.said28@yahoo.fr 
Abdelouahab Moussaoui 
Department of Computer Sciences, 
Faculty of Sciences 
University Farhat Abbes of Setif, 
Setif, Algeria, 19000 
e-mail: moussaoui.abdel@gmail.com 
Linda Belabdelouahab-Fernini 
Department of  ICST, Faculty of 
Mathematics and Computer Sciences 
University of M’sila 
M’sila, Algeria, 28000 
e-mail: ferblinda@hotmail.com
 
 
Abstract--- Identifying the language of a text is a very 
important preliminary phase in the categorization of 
multilingual documents or even in information retrieval. This 
phase becomes difficult if we just consider the word as a basic 
unit of information in texts. Because It could be possible for 
some languages as French or English but very difficult for 
some other languages as German, Chinese  and Arabic.  
     In this paper, we present the most known identification 
methods, and we propose a new optimized and effective variant 
of the method of Cavenar and Trenkle based on n-grams of 
characters. We also evaluate the obtained results with other 
methods by adopting the two approaches of texts 
segmentation:  words approach, n-grams approach. 
 
Keywords-component; N-gram; language identification; text 
categorization; Text Mining; machine learning; 
I.  INTRODUCTION 
 Needs of users in information keep growing because of 
the massive expansion of Internet and the increasing 
availability of textual data on this network. The multilingual 
documentary research is one of the solutions proposed to 
satisfy these needs. The main objective of this research is to 
allow the user to formulate a query in a language and extract 
the corresponding documents in other languages. This 
process requires a very important and preliminary phase that 
consists in identifying the language of each document before 
extracting the required information from it. Language 
identification which is also called “a text language 
categorization” can be defined as the assignment of a 
document belonging to a multilingual corpus thematically 
heterogeneous to a given language. Several features of the 
language can be considered in this case, notably: the  
presence of some characters  [1], [2], presence of some 
words [2], [3], n-grams frequencies [4], [5], [6], [7], [8], [9]. 
II. STATE OF THE ART 
A. Language Identification Approaches 
In general, four approaches exist:   
 The linguistic approach: It is based on the presence 
of some strings in the text specific to the text 
language. This permits to recognize the text 
language easily [1], [2]. for example, the 
characteristic strings of English are (they, ery,…), 
for French (eux, aux, …), for German (die, der,…), 
and for Italian (cchi,…). Unfortunately, this 
approach comprises several problems [6], namely: 
these strings can be rare or absent in the text. A text 
in a language can also use other language strings. 
 The lexical approach : it is based on the use of a 
lexicon for every language to identify [2], [3]. E.g 
we compare words of the new text with a fixed list 
of words for every language (a language lexicon). 
The language whose lexicon contains all or most 
words of the text is the effective language of the text. 
Several problems can arise here: no language lexicon 
is complete (absence of scientific words), the 
presence of spelling mistakes, typing mistakes, and 
recognition mistakes for a text acquired by an OCR 
which can disrupt results gotten by this approach. 
 The grammatical approach : it is based on the 
presence of some grammatical features of words 
related to the language [7] such as : prepositions, 
conjunctions, determiners, names, adverbs, 
auxiliaries, etc.. representing approximately 50% of 
sentences and texts in most languages [3]. For 
example, in English we can find words like (the, 
they, are, he, she, them, ...) in French (le, la, les, des, 
leur, leurs, eux, est, sont, …). This approach is faster 
and more efficient than the two previous approaches, 
but it also embodies several weaknesses, such as : 
the texts should be segmented into words which is 
difficult for some languages, grammatical words are 
often removed during a preprocessing performed on 
the text (stopwords) [10], the approach gives poor 
results in the case of short texts because of the 
absence of these grammatical words, it is difficult to 
apply with other types of sequences (DNA 
sequences in biology). 
 The statistical and probabilistic approach: this 
approach does not require prior language skills but 
probability calculations. Its principle is to capture 
some formal regularities (words, n-grams) of 
languages using statistical and probabilistic model 
from a representative corpus for each language, to 
associate a frequency or probability of occurrence, 
and then calculate the probability that a text belongs 
to a language based on the appearance of these 
regularities. Probabilistic approach can be 
generalized from language recognition to thematic 
text classification [11]. Unfortunately, segmenting a 
text into words degrades the performance of this 
approach because a short text does not necessarily 
contain the most frequent words of the language. In 
addition, for some languages like Chinese or DNA 
sequences in biology, it is difficult to divide the text 
into words. For this purpose, the use of n-grams-
based approach [8], [9], [12] seems to be a 
widespread solution. 
B. Principle of Text Segmentation into Ngrams of Characters 
      An n-gram is a consecutive sequence of n characters [5] 
that can not be ordered. For a document, the set of n-grams 
that can be generated is a collection of photographs that we 
can obtain by moving a window of n characters with one 
character on the body of the text. For example, the 3-grams 
of the phrase "hello sir" are : hel, ell, llo, lo_, o_s, _si, sir 
[13], [14], [15]. The n-grams profile of a document is the 
list of adjacencies constituting of the most frequent n-grams 
in reverse order of their occurrence frequency in the 
document accompanied with their frequencies. 
C. Advantages of Text Segmentation into N-grams 
The approach of text segmentation into characteristic N-
grams has several advantages, in particular: 
- Avoid the use of lemmatization and stemming phase on 
the text. This phase requires considerable computational 
and linguistic effort. 
- Tolerant to spelling, typing, and OCR mistakes. [13] 
shows that retrieval systems based on n-grams keep their 
performance despite of an error rate of 30%, which is       
not the case for systems based on words approach. 
- This approach operates independently from languages. 
- Segmentation into words is difficult for some languages 
where it is difficult to find clear boundaries between 
words, e.g in Arabic the names and additional subjects 
are in some cases attached to verbs and the string is thus 
a sentence like : (katabtouhou) (I wrote it). The same 
can be said for German, Chinese and DNA      sequences 
in genetics [14], [15]. 
D. Methods Based on N-grams for Language Identification 
      Language identification systems using n-grams are 
based on almost the same pattern [15]: the phase of 
automatic acquisition of the language in which we select a 
representative corpus, we generate a characteristic profile 
that will be taken as a reference, after we calculate the 
frequencies of various n-grams (with n = 3, 4, 5, ...). The 
diagnostic phase in which we built for each text to identify 
their n-gram profile then we seek the reference profile 
which is the most similar. Several methods are proposed to 
measure this similarity. 
1) Nearest Neighbours Methods : Several algorithms of 
language identification of texts and thematic categorization 
are based on the concept of distance or similarity. The basic 
idea is to find the text of the training set that is nearest in 
distance to the new text that we want classify, and to assign 
to it the same language. You can increase the number of k 
texts nearest to the new text if necessary. In this case, the 
language of the new text is the same as the majority of the k 
nearest neighbors of this text (the majority language). The 
challenge for these approaches is how to define a distance 
measurement. Practically, we use several pseudo-distances 
measurement, including: 
a) The Distance of Beesly[4]: In this method, the 
identification consists in two phases: the learning phase that 
consists in segmenting texts of each language into words, 
then segmenting each word into bi-grams without repeating 
the letter more than once, in order to build a bi-grams 
profile and use it as a reference profile, then calculate the 
frequency or the probability of occurrence of each bi-gram 
in this profile. The diagnostic phase consists in establishing 
the bi-gram profile of the new text T, then find the nearest 
reference profile using the distance with the profile of the 
language L the product of bi-grams probabilities of the new 
text T found in the profile of the language L (naive Bayes 
method). This method assumes the possibility of segmenting 
texts into words which is not the case for some other  
languages (Arabic, Chinese, ...). In addition, it is based on 
bi-grams, then it is necessary to not neglect working with tri 
or quad-grams to maintain the specificity of each language. 
For example, the 4-gram "tion" characterizes French and 
English, if you segment into bi-grams as follows: "ti" and 
"on", the system finds some difficulties to differentiate with 
"ti" and " on " of Spanish and Portuguese [9], [15], [16]. 
b) The Distance of Cavenar and Trenkle[5]: This 
method consists in two phases: the acquisition phase that 
consists in establishing a tri-gram profile for each language 
L and use it as a reference profile. The diagnostic phase 
which involves building a tri-gram profile for the new text T 
to identify the language, and then calculate the distances 
between this profile and the reference profiles of different 
languages. The distance to be calculated is based on the sum 
of position errors (difference in ranks or “out-of-place” 
measurement [5]) between each tri-gram in the new text 
profile PT and the same tri-gram in the reference profile of 
each language PL if the tri-gram is present. Otherwise, e.g 
the tri-gram is absent from the language profile PL, it takes a 
maximum value of rank. The language of the new text is 
that for which the distance is minimal. Formally, the 
distance between the new text profile PT and the language 
profile PL is calculated as follows: 
           
         
 
                      
                                    
     
                                        
 
Where :  ng : a tri-gram 
PT , PL :  profile of the new text T , profile of the language L 
          ,          : position of the tri-gram «ng» in 
profiles PT , PL if « ng » belongs to the language profile.  
c) The Distance of Kullbach-Leibler (KL) [17] : based 
on the relative entropy of Kullbach and Leibler like distance 
measurement. Formally, this distance measurement is 
calculated by the following equation: 
                         
      
      
 
where:  T1, T2:  texts 
f1(ng) f2(ng): frequencies of n-grams "ng" in texts T1, T2 
successively. 
if the n-gram "ng" is absent from the text Ti a half-frequency 
is added to prevent the score from falling to -  
d) The Distance of Khi2 ( 2) [15]: this distance is 
formally presented as follows: 
 
             
               
 
      
            (3) 
with :                 : frequencies of n-grams « ng » in 
texts T1 , T2 successively   
       
                          
                        
      (4) 
2) Conventional Methods Used in Categorization: 
Several methods exist in the field of document 
categorization, their common difficulty is the very large 
dimension. Among these methods, we note: decision trees 
(ID3, C4.5, CART, ...) [18] requiring dimension reduction, 
neural networks with back propagation, the SVM and RBF 
methods [19], [20]. 
III. OUR PROPOSED METHOD 
Our method is inspired from the method of [5] with the 
following differences and improvements : 
 [5] in their method requires sorting profiles of different 
languages, as well as  the new text profile in reverse order of 
frequencies before any calculation, this is not necessary for 
our  method.  which permit to save a considerable time 
required by the sorting operation. The measurement of the 
distance used in [5] is based on the sum of position errors 
(out-of-place) between the new text tri-gram profile and the 
same tri-gram in the reference profile of each language, of 
course if the tri-gram is present. If this is not the case, it 
takes a maximum value of position error. Here, we can note 
two disadvantages for this distance: the first is that the 
calculation of the sum of the position error requires a huge 
computational effort, especially when we use corpus of 
large sizes. The second problem in the choice of the 
maximum position error is when the tri-gram is absent. 
Here, no method is specified to find the maximum position 
error. Thus to overcome the two disadvantages we propose 
the following method: we take each n-gram (n = 3, 4, 5) of 
the new text profile, and we look in the profile of each 
language.  If this n-gram exists, we assign to it the value 1, 
otherwise we assign the sum of the frequencies of all corpus 
n-grams, and then we calculate the sum which represents the 
distance. The text will be assigned to the language whose 
distance is minimal. Formally our distance measure is 
calculated as follows: 
          
    
   
     
                            
                                       
 
IV. EXPERIMENTATIONS 
A. Training Corpus 
     We used an heterogeneous multilingual corpus that  is 
constituted of 668 documents written in ten (10) languages 
(Ar, Fr, En, Gr, Sp, It, Gk, Ru, Tu, Ma). 
B. Test Corpus 
      The test corpus is constituted of a collection of 199 
documents in ten (10) languages mentioned above. 
      Both training and testing corpus were improved with a 
personal effort. Texts belong to the headlines of 
international newspapers (Le Monde, Newsweek, New York 
Times, Der Spiegle, Aljazeera.net, ...). We preferred to use 
reduced size documents (between 1 and 3kb) to facilitate 
their processing and segmentation into words and n-grams. 
Both corpus may be distributed as follows: 
TABLE I.  TRAINING AND TEST CORPUS USED IN 
EXPERIMENTS 
Languages 
Training Corpus Test Corpus 
Number of texts Number of texts 
Arabic 68 17 
French 80 19 
English 79 20 
German 74 20 
Spanish 73 19 
Italien 63 16 
Greek 65 23 
Russian 90 25 
Turkish 58 18 
Malysian 78 22 
Total 668 199 
C. Preprocessing Performed on Training and Testing Corpus 
      Before proceeding to the phase of language 
identification itself, another phase of pretreatment on the 
training and test corpus is essential. This phase comprises 
several  tasks, namely: 
- Elimination of unnecessary characters (punctuation, digits, 
special foreign characters, abbreviations and isolated 
characters, ...). 
    
- Conversion of uppercase to lowercase and vice versa. 
- Morphosyntactic Treatment on the text or text 
standardization (e.g in Arabic: Using a single hamza, ya and 
alif maksoura with and without hamza, vocalization signs 
such as shadda and attanwin, …). 
-Segmentation of the text into words and n-grams (n=3, 4, 5) 
- Setting a minimum threshold for the frequency and 
eliminate the hapax n-grams whose frequencies are less than 
the selected threshold. 
D. Performed Processing  
      After finishing the phase of preprocessing on the 
training and test corpus, we proceed to the following 
treatments: 
- For the segmentation of texts, we used two approaches: 
bag of words, n-grams  of characters. 
- The results were tested for varying frequencies thresholds 
(s = 2, 3, 4). 
- For text segmentation into n-grams we tested results for 
varying values of  n (n = 3, 4, 5). 
- For the applied learning algorithm, we chose the algorithm 
of k-nearest  neighbors with k = 1, as well as naive Bayes 
algorithm. 
- We applied the 1-NN algorithm with a variety of distances 
such as :  CT distance [5], Kullbach-Leibler distance KL 
[15], 2 (chi-2) distance [15], then our suggested method 
with the new associated distance. 
V.    EVALUATION OF OBTAINED RESULTS 
TABLE II.  SEGMENTATION OF THE TRAINING CORPUS INTO 
WORDS (S = 2) 
Languages # Texts 
# gross 
words 
# 
purified 
words 
# Most 
freq words 
Arabic 68 19010 4960 1933 
French 80 14698 5757 2646 
English 79 16672 5472 2761 
German 74 4160 2498 681 
Spanish 73 15249 5748 2498 
Italien 63 3827 2350 603 
Greek 65 6139 2693 688 
Russian 90 5857 3728 921 
Turkish 58 4292 2563 857 
Malysian 78 4205 2045 744 
 
 
 
TABLE III.  SEGMENTATION OF THE TRAINING CORPUS INTO 
N-GRAMS (N=3, S = 2) 
Languages # Texts 
# 3g 
gross 
# 3g 
purified  
#3g Most 
freq 
Arabic 68 53609 5276 3318 
French 80 63956 7271 4862 
English 79 61128 6796 4608 
German 74 12434 3412 2363 
Spanish 73 69255 3334 1429 
Italien 63 21314 1632 3313 
Greek 65 25315 3923 1356 
Russian 90 69414 1231 1313 
Turkish 58 21439 6463 2399 
Malysian 78 63114 1961 2493 
TABLE IV.  SEGMENTATION OF THE TRAINING CORPUS INTO 
N-GRAMS (N=4, S = 2) 
Languages # Texts 
# 4g 
gross 
# 4g 
purified  
#4g Most 
freq 
Arabic 68 52707 10397 5005 
French 80 63172 15998 6777 
English 79 59347 15828 8813 
German 74 14239 42264 6523 
Spanish 73 69333 41461 1361 
Italien 63 23559 43221 1533 
Greek 65 21115 42353 3333 
Russian 90 66523 41346 9233 
Turkish 58 22336 43133 6641 
Malysian 78 19235 43141 6399 
TABLE V.  SEGMENTATION OF THE TRAINING CORPUS INTO 
N-GRAMS (N=5, S = 2) 
Languages # Texts 
# 5g 
gross 
# 5g 
purified  
#3g Most 
freq 
Arabic 68 52490 16162 6755 
French 80 63048 24637 10946 
English 79 59104 25016 10706 
German 74 14339 20840 6684 
Spanish 73 58108 21791 9454 
Italien 63 32804 16958 5666 
Greek 65 37521 19895 6401 
Russian 90 55583 26971 9361 
Turkish 58 32853 15477 6532 
Malysian 78 48002 16939 6401 
 
TABLE VI.  CALCULATION OF : SUCCESS RATE, ERROR RATE 
FOR ALL LEARNING ALGORITHMS (APPROACH: BAG OF WORDS) 
Algorithm Suc_rate Err_rate 
N.Bayes 96,35 2,65 
1NN with CT 98,88 1,12 
1NN with KL 97,03 2,97 
1NN with 2 98,74 1,26 
The new method 99,49 0,50 
TABLE VII.  CALCULATION OF : SUCCESS RATE, ERROR RATE 
FOR ALL LEARNING ALGORITHMS (APPROACH: N-GRAMS) 
Algorithm 
N=3 N=4 N=5 
Suc_
rate 
Err_r
ate 
Suc_
rate 
Err_r
ate 
Suc_
rate 
Err_r
ate 
N.Bayes 97,04 2,96 98,72 1,28 98,93 1,07 
1NN with CT 98,91 1,09 99,20 0,80 98,75 1,25 
1NN with KL 97,89 2,11 98,14 1,86 96,37 3,63 
1NN with 2 98,86 1,14 100 0 97,33 2,67 
The new method 99,49 0,50 100 0 99,01 0,99 
 
     After the automatic preprocessing performed on both 
training and test corpus as it’s indicated in section III.C, we 
conducted the phase of texts segmentation into basic units 
(tokens), we have used for this, words segmentation, 3-
grams, 4-grams, and 5-grams segmentation. The tables II, 
III, IV, and V summarize the obtained results in the 
segmentation phase. We note here that learning is applied to 
the most frequent words and n-grams as it is shown in tables 
II, III, IV, V (column 5) because of their small number and 
because they always give the best success rate in the 
language recognition. For learning, we have also applied 
naive Bayes algorithm which is the best known algorithm in 
the field, and because it gives good results compared to 
other methods, plus a 1-NN algorithm referring to several 
pseudo-distances. And finally, we have applied our new 
method with its own pseudo-distance. The obtained results 
show that our method always gives results better compared 
to the results obtained using other methods (with a success 
rate > 99%) as it is shown in table VII (the last row). For 
segmentation into 4-grams the recognition rate (success 
rate) is 100%  (table VII, columns  4, 5). 
     Considering the used data, especially the corpus of texts 
used in experiments, and which we perfected arbitrarily. in 
addition to implementations that we made for all learning 
algorithms used in our language classifier, we believe that 
the results are very significant and can be improved in other 
works. 
VI.    CONCLUSION AND PERSPECTIVES 
In this paper we treated the problem of language 
identification of texts in a multilingual textual corpus. We 
explained the two most known approaches in the field to 
segment a text into basic units (tokens), including: “bag of 
words” approach and n-grams approach. The realized 
implementations show the limitation of the “bag of words” 
approach compared to the n-grams approach which is more 
general, independent of languages and always gives the best 
results. In language identification, we implemented several 
algorithms based on different pseudo-distances 
measurements, namely: Cavnar and Trenkle distance CT, 
Kullbach and Leibler (KL) distance , Khi-2 (2) distance. In 
the light of all these metric similarity measurements and 
results, we proposed a new method equipped with its own 
distance. This new method is inspired from the CT method 
with the advantage that this last one does not require sorting, 
and it's based on a simple and less expensive (in calculation) 
distance especially for corpus with large sizes. The obtained 
results by applying our new method are very significant, in 
term of computation time and in results accuracy when 
recognizing the language of the text. 
The perspective of our work is to propose to apply our new 
method on a corpus of semi-structured documents (XML 
documents), and generalize it not only in the language 
identification, but also in the field of text categorization 
itself, as well as any other task of document classification. 
REFERENCES 
 [1]   S. Mustonen, “Multiple Discriminant Analysis in Linguistic  
        Problems,”, Statistical Methods in Linguistics, (1981), pp   
       147, 195—197, Page visitée le 15 juin 2000 à l’adresse  
        http://www.nodali.sics.se/bibliotek/kval/smil, 1965. 
[2]   C. Souter, C. Churcher, G. Hayes, J. Hughes, J. Johnson, “    
        Natural Language Identification Using Corpus-Based Models,    
        Hermes Journal of Linguistics, vol. 13, 1994. 
[3]   E. Giguet, ’’ Méthode pour l’analyse automatique de structures  
        formelles sur documents multilingues __, PhD thesis, Université  
        de Caen, France, 1998. 
[4]   K. Beesley, “__ Language Identifier: A Computer Program for  
        Automatic Natural Language  Identification on On-Line Text   
        __, Proceedings of the 29th Annual Conference of the American  
        Translators Association, 1988, pp. 47–54. 
[5]   W. Cavnar, J. Trenkle, “Gram Based Text Categorization __,  
        Symposium on Document  Analysis and  Information  
        Retrieval, Las Vegas, 1994. 
[6]   T. Dunning, “Statistical Identification of Languages __, rapport   
                                                  MCCS, Computing Research Laboratory, New Mexico State  
        University, Las Cruces, New Mexico pp 94-273, 1994  
 [7]  G. Grefenstette, “ Comparing Two Language Identification  
        Schemes __, Proceedings of the  3rd International Conference  
        on the Statistical Analysis of Textual Data, Rome, Italy, 1995. 
[8]   R.M. Milne, R.A. O’Keefe, A. Trotmana, “A study in  
        language identification, ADCS December 05 - 06 2012,  
         Dunedin, New Zealand, 2012 
[9]   M. Zampieri, B.G. Gebre, “Automatic identification of  
        language varieties: the case of  portuguese, Proceedings of  
        KONVENS 2012 (Main track: poster presentations), Vienna,    
        September 20, 2012 
[10]   M. Sahami, “Using Machine Learning to Improve    
          Information Access _, PhD thesis, Computer Science  
          Department, Stanford University, 1999. 
[11]   W.M. Geiger, J. Rauch, K. Hornik, “Text categorization in  
          R:  A Reduced N-grams Approach, Springer-Verlag Berlin  
          Heidelberg,  2012 
 
[12]   C. Shannon, “The Mathematical Theory of Communication __,  
         Bell System Technical Journal, vol. 27, 1948, pp 379–423  
         and 623–656. 
[13]  E. Miller, D. Shen, J. Liu, C.Nicholas, “Performance and  
        Scalability of a Large-Scale N-gram Based Information  
        Retrieval System __, Journal of Digital Information, vol.1, n  5,  
        1999. 
[14]  I. Biskri, S. Delisle, ‘’les n-grammes de caractères pour l’aide  
         à l’extraction de connaissance dans des bases de données  
         textuelles multilingues, TALN 2001, Tours, 2-5 juillet 2001 
[15]   R. Jalam, O. Teytaud, ‘’Identification de la Langue et  
         Catégorisation de Textes basées sur les N-grams, Journées  
     Francophones d’extraction et de gestion de connaissances, 2002 
[16]   R.D. Brown, “Finding and identifying text in plus de 900‏  
          languages, Published by Elsevier Ltd. All rights reserved,  
         2012 
[17]   P. Sibun, J. Reynar, “Language Identification: Examining the  
          Issues , Symposium on Document Analysis and Information  
          Retrieval, Las Vegas, 1996, pp. 125–135. 
[18]   R. Rakotomalala, ‘’Arbres de décision,  Revue MODULAD,  
          2005, N°33 
[19]   T. Joachims, “Text Categorization with SVM: Learning with  
          Many Relevant Features, Machine Learning: ECML-98,  
         10th European Conference on Machine  Learning, 1998. 
[20]   A. Dimitrios, C. Pritsos, E. Stamatatos, “Open-Set  
         classification for Automated Genre Identification , ECIR 2013,  
          LNCS 7814, pp. 207–217, 2013. Springer-Verlag,  Berlin     
          Heidelberg,  2013 
 
