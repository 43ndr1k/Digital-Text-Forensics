An n-gram Based Approach to Multi-labeled Web Page Genre Classification 
 
 
Jane E. Mason Michael Shepherd Jack Duffy Vlado Kešelj Carolyn Watters 
Dalhousie University Dalhousie University Dalhousie University Dalhousie University Dalhousie University 
jmason@cs.dal.ca shepherd@cs.dal.ca jack.duffy@dal.ca vlado@cs.dal.ca watters@cs.dal.ca 
 
Abstract 
The extraordinary growth in both the size and 
popularity of the World Wide Web has created a 
growing interest not only in identifying Web page 
genres, but also in using these genres to classify Web 
pages. The hypothesis of this research is that an n-
gram representation of a Web page can be used 
effectively to automatically classify that Web page by 
genre, even when the Web page belongs to more than 
one genre. Experiments are run on a multi-labeled 
data set using both an SVM classifier and a distance 
function classification model. These n-gram based 
methods had very high precision results but 
somewhat lower recall results, indicating that the 
genre labels assigned by the classifiers are quite 
accurate, but that these machine learning classifiers 
are not assigning as many labels as did the human 
classifiers. The classification results compare 
favorably with those of other researchers on the same 
data set. 
 
 
1. Introduction  
 
Web page genre classification is a potentially 
powerful tool in filtering the results of online 
searches. As Shepherd and Watters [21] note, the use 
of genre for classification allows the recognition of 
items that are similar, even in the midst of great 
diversity. Categorizing Web pages by genre can 
allow a user to specify the genre that is of interest 
with regard to a particular query. For example, 
depending on the query, the user may be interested in 
retrieving only FAQ pages or only personal home 
pages. A user could also choose to filter online 
searches by excluding a particular genre from 
retrieved pages, such as retrieving all genres except 
shopping pages. 
The research reported in this paper investigates 
the automatic classification of Web pages by their 
genres, using n-gram representations of the Web 
pages. The hypothesis of this research is that an n-
gram representation of a Web page can be used 
effectively to automatically classify that Web page by  
 
 
genre, even when the Web page belongs to more than 
one genre, i.e., the n-gram representation may be a 
solution to the multi-label problem. 
The remainder of the paper proceeds as follows. 
Section 2 gives a brief overview of related work, 
while Section 3 reviews the methodology, including a 
description of the data set, the Web page 
representation, the classification methods, and the 
evaluation metrics. Sections 4 and 5 describe the 
experiments and discuss the results. Finally, Section 
6 presents the conclusions and the direction of future 
work; supplementary tables are found in the 
Appendix. 
 
2. Related work  
 
The idea of using Web page genres to filter online 
searches is not new. As early as 1997, Chekuri et al. 
[5] automatically classified Web pages into pre-
specified categories, with the goal of increasing the 
precision of Web searches, and Crowston and 
Williams [6] conducted a survey of 100 Web pages, 
looking for both reproduced and emergent genres.  
They found that analyzing Web usage through genres 
was very effective, and encouraged Web site 
designers to take users’ expectations about familiar 
genres into consideration. The following year, 
Roberts [16] demonstrated that personal homepages 
represent a distinct genre, and Shepherd and Watters 
[21] examined the emergence of what they termed 
cybergenres.  They noted that some genres seem to 
emerge spontaneously, such as hotlists and 
homepages, while others evolve from existing genres, 
such as online dictionaries and newspapers. 
Much of the research on the classification of Web 
pages by genre has focused on labeling each Web 
page as belonging to a single genre. However, 
researchers who have conducted surveys and user 
studies concerning Web page genres have 
acknowledged the difficulty of assigning a single 
genre label to a Web page [6, 14, 17]. In response to 
this problem, Santini has proposed a zero-to-multi 
genre classification scheme [18, 19]. Santini's user 
1
Proceedings of the 43rd Hawaii International Conference on System Sciences - 2010
978-0-7695-3869-3/10 $26.00  © 2010 IEEE
studies indicate that, at least from a user's 
perspective, a single-genre classification scheme is 
too narrow. Although some Web pages may fit neatly 
into a single genre, others do not fit into any genre, 
while still others may fit appropriately into several 
genres. Santini suggests designing genre 
classification models that not only allow Web pages 
to have more than one genre label, but that also allow 
a Web page to have a zero-genre label. This latter 
would be for the case in which a Web page does not 
match the conventions of any known genre. Labeling 
Web pages in this manner is not only a more flexible 
approach from the users’ perspective, but also better 
reflects the complex nature of the mixture of pages 
found on the World Wide Web. 
In order to classify Web pages by genre, it is 
necessary to identify features that effectively 
characterize each Web page and genre. Web pages 
can be represented much like documents that are used 
in text classification, however the representations 
may also include information such as URL 
information or HTML tags.  For example, Rehm [15] 
uses linguistic features combined with HTML meta 
data and presentation related tags, while Meyer zu 
Eissen and Stein [14] combine genre-specific 
vocabulary and closed-class word sets with text 
statistics, part-of-speech information, and HTML 
tags.  Boese and Howe [3] use a bag of words 
representation augmented with other information that 
includes text statistics, HTML tags, and URL 
information. Jebari [9] combines two centroid-based 
classifiers, one of which uses structural information 
from the document, while the other uses URL 
information.  Stein and Meyer zu Eissen [23] give a 
detailed chronological overview of the document 
representations used for genre classification on Web-
based corpora. 
The research discussed in this paper represents 
Web pages using n-grams. An n-gram can be thought 
of as the contents of a fixed-size sliding window 
moved through the text. Byte n-grams are raw 
character n-grams in which no bytes are ignored, 
including the whitespace characters, therefore byte n-
grams capture some of the structure of a document. 
The use of n-grams has been common in language 
modeling since at least 1948 when Claude Shannon, 
considered the father of information theory, 
investigated the question of determining the 
likelihood of the next letter in a given sequence of 
characters [20].  Since that time, n-grams have been 
widely used in natural language processing and 
statistical analysis. For example, Cavnar and Trenkle 
[4] use n-gram representations of documents for text 
classification, as do Kešelj et al. [11] in their work on 
authorship attribution. In the area of Web page genre 
identification, Kanaris and Stamatatos [10] use 
feature sets of variable-length character n-grams, 
combined with information about the most frequent 
HTML tags, to perform classification using a support 
vector machine (SVM).  
The term curse of dimensionality was coined by 
Richard Bellman [2] to describe the problem that 
occurs when searching in high dimensional spaces. 
As the dimensionality of the input data space 
increases, it becomes exponentially more difficult 
(more computationally complex) to fit models for the 
parameter space. In practice, it is often necessary to 
use feature selection techniques to select a subset of 
relevant features, because most standard machine 
learning techniques cannot be directly applied when 
the dimensionality is very high. Yang and Pedersen 
[26] provide a comparative study of the traditional 
feature selection techniques in text classification. 
Focusing specifically on the classification of Web 
pages by genre, Dong et al. [8] compared the 
performance of several feature selection measures. 
They found that when feature sets were as small as 5, 
Information Gain and the Chi-square statistic were 
able to successfully select features that gave good 
performance. Mason et al. [13], also working on Web 
page genre classification, compared the performance 
of frequency, Information Gain, and the Chi-square 
statistic as feature selection measures, and found that 
the use of the Chi-square statistic provided the best 
results. Based on this research, the experiments 
discussed in this paper use the Chi-square statistic as 
the feature selection measure. 
 
3. Methodology  
 
3.1. Data set 
 
The experiments discussed in this paper were run 
on the 20-Genre data set. The 20-Genre data set is a 
multi-labeled Web page collection constructed by 
Mitja Luštrek and Andrej Bratko, and described in 
Vidulin et al. [24, 25]. This data set contains 1539 
Web pages, each with one or more genre labels. Of 
the 1539 Web pages, 1059 have one genre label, 438 
have two genre labels, 39 have three labels, and 3 
have four labels. This gives a total of 2064 labels. 
The number of Web pages in each genre ranges from 
55 to 227. Because of the small size of some of the 
genres, we use 3-fold cross validation for the 
experiments. The results for all three iterations are 
2
Proceedings of the 43rd Hawaii International Conference on System Sciences - 2010
then averaged to give the final results. Tables 5 and 6 
include a listing of the genres and the number of Web 
pages in each genre. 
 
3.2. Web page representation 
  
In the experiments for this paper, no 
preprocessing of the Web pages is performed. Based 
on experiments performed by Mason et al. [12], the 
removal of HTML tags and JavaScript code from the 
Web pages is not necessary. The textual content of 
each Web page, including the HTML tags and 
JavaScript, is used to form n-gram representations of 
the Web pages. Previous research on the use of n-
gram representations of Web pages and Web page 
genres indicates that using the Chi-square statistic as 
a feature selection measure allows the use of a 
relatively small number of n-grams to represent each 
Web page [13]. For the experiments in this paper, we 
therefore use the Chi-square statistic as the feature 
selection method with which to choose the n-grams 
to represent each Web page.   
 
3.3. Classification methods 
 
This study looks at three methods of using n-gram 
profile representations of Web pages in the automatic 
identification of the genre of the Web pages. The first 
two methods use a distance function classification 
model, while the third method is an SVM classifier. 
For each of the three methods, the Web pages are 
represented by n-gram profiles. Initially, Web page 
profiles containing the n-grams and their associated 
normalized frequencies are produced using the Perl 
package Text:Ngrams1. The Chi-square statistic is 
then used as a feature selection measure; the n-grams 
are ranked according to the Chi-square statistics, and 
profiles are constructed for the Web pages. Each 
profile contains the L top ranked n-grams in the Web 
page, and the corresponding frequency for each of 
these n-grams. 
When using the distance function classification 
models, each Web page genre is represented by a 
profile that is constructed by combining the n-gram 
profiles for each Web page of that genre from the 
training set, forming a centroid profile for each Web 
page genre. These centroid profiles will contain a 
varying number of n-grams, therefore each of the 
centroid profiles is truncated to the size of the 
smallest centroid profile. Each Web page profile 
                                                 
1  http://users.cs.dal.ca/»vlado/srcperl/Ngrams/ 
from the test set is compared with each genre 
centroid profile from the training set. The distance 
between two n-gram profiles is computed using the 
formula suggested by Kešelj et al. [11] in their paper 
on the use of n-gram profiles for authorship 
attribution. The distance (dissimilarity) between two 
n-gram profiles is defined as 
 
where f1(m) and f2(m) are the frequencies of n-gram 
m in the two profiles, P1 and P2 respectively. 
In order to classify a Web page as belonging to 
more than one genre, or as not belonging to any 
known genre, we modify the distance function model 
to include thresholds that are computed for each 
genre. If the distance between the Web page profile 
and a genre profile is less than or equal to the 
threshold, then the Web page is labeled as belonging 
to that genre. If the distance is greater than the 
threshold, then the Web page is deemed to not belong 
to that genre. The addition of genre thresholds to the 
distance function classification model changes the 
architecture of the model, in this case, from that of 
one 20-way classifier to that of twenty 2-way 
classifiers. The following two methods of setting the 
genre thresholds are investigated. 
One method of determining the threshold for each 
Web page genre is to base the threshold on the 
distribution of the Web pages that belong to that 
genre, in the training set. For each genre in the data 
set, the distance is computed between the centroid 
profile for that genre and the profile of each Web 
page of that genre in the training set. If this set of 
distances has a normal distribution, that genre 
threshold is set at the 85th percentile (one standard 
deviation around the mean), such that 85% of the 
Web pages belonging to the genre (in the training set) 
fall within the threshold. If the set of distances is not 
normally distributed, the threshold is set at the 75th 
percentile. The 75th percentile was chosen as a cutoff 
based on the popular use of the semi-interquartile 
range statistic for non-normal data. This statistic is 
also appropriate for normal data, but is less powerful 
than the standard deviation statistic for such 
distributions [22]. Because each genre threshold is set 
based on the distribution of the Web pages belonging 
to the genre, we will refer to this as the distribution 
threshold method. 
Another method of setting each genre threshold is 
to first order all of the Web pages in the training set 
( ) ( ) ( )( )( ) ( ) ,
2
,
21
2
21
21
21 ∑
∪∈
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
+
−⋅
=
PPm mfmf
mfmf
PPd
3
Proceedings of the 43rd Hawaii International Conference on System Sciences - 2010
according to their distance from a particular genre 
profile, in ascending order. This ordered list of Web 
pages from the training set is then stepped through 
one Web page at a time, such that at each step, the 
current Web page is labeled as belonging to the genre 
in question, and the accuracy of the classification 
thus far is computed. In this manner, the optimal 
threshold for each genre, based on the training data, 
can be determined. This process is then repeated for 
each genre in the data set. We refer to this method of 
setting the genre thresholds as the optimal threshold 
method, because the method gives a set of fixed 
thresholds, one for each genre, that give the optimal 
classification accuracy on the training set.  
 
3.4. Evaluation metrics 
  
In order to compare our results with those of other 
researchers on the same corpus, we use macro-
precision and macro-recall as the evaluation metrics. 
Macro-precision, Pma, and macro-recall, Rma, are 
defined as follows: 
and 
 
 
where |G| is the number of genres, TPi is the number 
of true positives for genre i (pages correctly labeled 
genre i), FPi is the number of false positives for genre 
i (Web pages incorrectly labeled as belonging to 
genre i), and FNi is the number of false negatives for 
genre i (incorrectly labeled Web pages that actually 
do belong to genre i). 
 
3.5. Experiments 
 
The experiments reported in this paper are carried 
out on the 20-Genre data set, using each of the two 
threshold methods described in Section 3.3, as well 
as using an SVM classifier with which the sequential 
minimal optimization (SMO) method is used to find 
the separating hyperplane. In each case, the Web 
pages are represented by profiles composed of byte 
n-grams and their associated frequencies. With each 
method, 30 trials are performed, each with a different 
combination of n-gram length and Web page profile 
size. Based on previous experiments [13], the n-gram 
length ranges from 2 to 4 in increments of 1, and the 
number of n-grams selected from each Web page 
ranges from 5 to 50 in increments of 5.  
As discussed in Section 3.1, we use 3-fold cross 
validation for each experiment. The results for all of 
the iterations are averaged to give the final results. 
This helps provide robustness against overfitting and 
gives additional strength to the statistical analysis.  
 
4. Results  
 
The results of the experiments in this study are 
reported in Tables 1-6 in the Appendix. Analysis of 
variance (ANOVA) tests were performed to 
determine the effect, on the precision and recall, of 
the classification model, the n-gram length, the Web 
Page profile size, the genre, and the combination of 
these parameters. A summary of the experiments and 
results follows. 
 
4.1. Overall results 
 
Tables 1 and 3 in the Appendix give the results in 
terms of precision, averaged over Web page profile 
sizes of 5 to 50 and n-gram lengths of 2 to 4 
respectively; Tables 2 and 4 in the Appendix give the 
same results in terms of recall. Over n-gram lengths 
from 2 to 4 and Web page profile sizes from 5 to 50, 
both the optimal threshold method and the SVM 
method are significantly better than the distribution 
threshold method (p<0.001), in terms of precision. 
Based on precision, there is no statistically significant 
difference between the optimal threshold method and 
the SVM method. In terms of recall, both the optimal 
threshold method and the SVM method are 
significantly better than the distribution threshold 
method (p<0.001), and the optimal threshold method 
is significantly better than the SVM method 
(p<0.001).  
 
4.2. Effect of n-gram length 
 
In these experiments, the n-gram length ranges 
from 2 to 4 in increments of 1, and the effect of the n-
gram length on the precision and recall for each 
method is statistically significant (p<0.001). The 
partial Eta squared is the proportion of total 
variability attributable to a particular factor [1], 
which in this case is n-gram length.  With regard to 
precision, the partial Eta squared for the n-gram 
length is 0.012, and for recall it is 0.051. This 
( )⎟
⎟
⎠
⎞
⎜⎜
⎝
⎛
+
= ∑
=
G
i ii
i
ma FPTP
TP
G
P
1
1
( ) ,
1
1
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
+
= ∑
=
G
i ii
i
ma FNTP
TP
G
R
4
Proceedings of the 43rd Hawaii International Conference on System Sciences - 2010
indicates that the proportion of total variability in the 
precision and recall for each method is not highly 
influenced by the n-gram size. The results of a Sheffé 
post-hoc multiple comparison test [7] show that for 
each method, n-grams of length 2 are the best choice. 
Tables 1 and 2 in the Appendix give the mean 
precision and mean recall, respectively, for each n-
gram length, for each method. These results are 
averaged over Web page profile sizes from 5 to 50. 
Tables 1 and 2 show that as the n-gram length is 
increased, both the precision and recall decrease for 
each method. We conclude that using an n-gram 
length of 2 gives the best classification results in 
terms of precision and recall. The use of a small n-
gram size may also have the advantage of being less 
computationally intensive. 
 
4.3. Effect of Web page profile size 
 
In these experiments, the number of n-grams used 
to represent each Web page ranges from 5 to 50 in 
increments of 5. The effect of this Web page profile 
size was not statistically significant on the precision 
of the methods. The effect was found to be 
statistically significant on the recall of the methods 
(p<0.001), however the partial Eta squared was only 
0.038, indicating that the Web page profile size 
accounted for less than 4% of the overall variance of 
the dependent variables. The results of a Sheffé post-
hoc multiple comparison test [7] suggest that 
choosing Web page profile sizes in the range from 15 
to 50 is preferable. Tables 3 and 4 in the Appendix 
give the mean precision and mean recall, 
respectively, for each Web page profile size, for each 
method. These results are averaged over n-gram 
lengths from 2 to 4.  
 
4.4. Effect of genre 
 
The results in Tables 5 and 6 indicate, not 
surprisingly, that some genres are easier to classify 
than others. Genre is the leading factor (over method, 
n-gram length, and Web page profile size) in 
predicting the outcome for precision, and is second 
only to method in predicting the outcome for recall. 
In terms of precision, the partial Eta squared, which 
indicates the proportion of the total variability 
attributable to a factor [1], is 0.061 for precision, and 
0.416 for recall. This means that genre is responsible 
for approximately 6% of the overall variance in 
precision, and for more than 40% of the overall 
variance in recall. The lower partial Eta squared for 
precision could be the result of a ceiling effect, which 
can be expected as the mean precision approaches 
1.00, as it does in this case.  
Although genre is an influential factor in 
predicting the precision and recall outcomes, we have 
no specific hypotheses about which genres can be 
better classified than others. There is no observable 
trend that relates this influence on variability to the 
number of Web pages in each genre, and as shown in 
Tables 5 and 6, the results on a particular genre vary 
depending on the classification method; a genre that 
is well classified by one method may be very poorly 
classified by another method. 
 
5,  Discussion 
 
Tables 5 and 6 in the Appendix give a comparison 
of the best precision and corresponding recall, 
respectively, for each of the three methods tested in 
these experiments, as well as for those of Vidulin et 
al. [24, 25] using the same corpus. Vidulin et al. 
represent Web pages using a set of 502 genre features 
that are a combination of HTML-based, URL-based, 
and text-based features. They used J48, the Weka 
implementation of the C4.5 algorithm, for 
constructing the classifier and bagging ensembles. 
The results from Vidulin et al. that are shown in 
Tables 5 and 6 are for their use of the bagging 
algorithm, using 10-fold cross validation. 
Tables 5 and 6 show that each method has a much 
higher precision than recall, averaged over all 20 
genres. This means that the genre labels assigned by 
the classifiers are quite accurate, but these machine 
learning classifiers are not assigning as many labels 
as did the human classifiers when the data set was 
constructed. Although the distribution method for 
setting genre thresholds, used as part of the distance 
function classifier, achieves a best precision of 1.00, 
the corresponding recall is only 0.434, averaged over 
the 20 genres. The optimal threshold method, also 
used as part of the distance function model, achieves 
a best precision of 0.996, with a corresponding recall 
of 0.770. The SVM method has a best precision of 
0.999, with a recall of 0.705. The best precision and 
corresponding recall for these methods exceeds the 
best results reported by Vidulin et al. [24].  
With the distribution method of setting genre 
thresholds, the best precision, as given in Table 5, 
was achieved using n-grams of length 2 with a Web 
page profile size of 20. The best precision with the 
optimal threshold method was also found using n- 
grams of length 2, however in this case the Web page 
5
Proceedings of the 43rd Hawaii International Conference on System Sciences - 2010
profile size was 35. For the SVM method, an n-gram 
length of 3 and a Web page profile size of 30 proved 
to be the best combination.  
Of the three classification methods tested in these 
experiments, only one labeled any of the Web pages 
as not belonging to any recognized genre. The SVM 
and optimal threshold methods both supplied at least 
one genre label for each Web page, however the 
distribution threshold method did not. In this case, an 
average of 62% of the Web pages were labeled as not 
belonging to any of the 20 known genres in the data 
set. This indicates that the genre thresholds were 
being set too strictly, and helps account for the very 
low recall returned by this method. The Web pages 
that were unidentifiable by the distribution threshold 
method were of all genres, and the number of Web 
pages from each genre that were unidentifiable 
tended to be in proportion with the size of the genre. 
 
6. Conclusions and future work  
 
The research reported in this paper explored the 
classification of Web pages by genre using n-gram 
representations of the Web pages. Experiments were 
run on a multi-labeled data set using both an SVM 
classifier and a distance function classification 
model. Within the latter model, we investigated two 
methods of setting thresholds for each genre, in order 
to label Web pages as belonging to more than one 
genre, or as not belonging to any recognized genre. 
We compared the classification results obtained with 
the SVM classifier and the two versions of the 
distance function classification model with those of 
Vidulin et al. [24].  
Based on the results presented in this paper, we 
conclude that the n-gram representation of Web 
pages allows the effective classification of those Web 
pages by genre, even when the Web page belongs to 
more than one genre. We also conclude that the 
combination of the distance function classification 
model and the optimal threshold method, discussed in 
Section 3.3, is more successful than the SVM method 
in classifying this multi-labeled data set. 
The results of these experiments also showed that 
as the length of the n-grams used to represent the 
Web pages was increased, the precision and recall for 
each classification model decreased. Based on these 
results, we conclude that an n-gram length of 2 
should be used in future experiments. 
The experimental results also indicated that over 
the range of 5 to 50, the number of n-grams used to 
represent each Web page has only a slight impact on 
the classification results, but our statistical analysis 
suggests that using a Web page profile size in the 
range of 15 to 50 n-grams is best.  
The major contribution of this research is to show 
that byte n-gram Web page representations can be 
used effectively, with more than one classification 
model, to classify Web pages by genre, even when 
the Web pages belong to more than one genre. This 
research is ongoing. Future work will include testing 
the classification model on a highly unbalanced data 
set to which noise has been added. If necessary, the 
model will be refined to better deal with such real 
world conditions. For the purpose of this study, noise 
will be defined as any Web page belonging to a Web 
page genre that is not one of the genres in the data 
set. The optimal threshold method, discussed in 
Section 3.3, will be used to identify noise Web pages 
during the classification process. 
 
7. Acknowledgements  
 
This research has been supported by the Killam 
Trust and the Natural Sciences and Engineering 
Research Council of Canada (NSERC). 
 
8. References  
 
[1] L. Becker. (11/08/99). Measures of Effect Size 
(Strength of Association). In SPSS. Retrieved 06/15/09, 
http://web.uccs.edu/lbecker/SPSS/glm_effectsize.htm. 
 
[2] R.E. Bellman. Adaptive Control Processes: A Guided 
Tour. Princeton University Press, 1961. 
 
[3] E. Boese and A. Howe. Effects of Web Document 
Evolution on Genre Classification. Proc. 14th ACM Int. 
Conf. on Information and Knowledge Management, 
2005, pp. 632–639. 
 
[4] W. Cavnar and J. Trenkle. N-Gram Based Text 
Categorization. Proc. 3rd Annual Symposium on 
Document Analysis and Information Retrieval, 1994. 
 
[5] C. Chekuri, M. Goldwasser, P. Raghavan, and E. Upfal. 
Web Search using Automatic Classification. Proc. 6th 
Int. Conf. on the World Wide Web, 1997. 
 
[6] K. Crowston and M. Williams. Reproduced and 
Emergent Genres of Communication on the World-
Wide Web. Proc. 30th Hawaii Int. Conf. System 
Sciences, 1997. 
 
[7] R.I. Cue. (11/01/03). Multiple Comparisons.  
 In Statistics II. Retrieved 06/15/09, 
http://animsci.agrenv.mcgill.ca/servers/anbreed/statistic
sII/mcomp/index.html. 
 
[8] L. Dong, C. Watters, J. Duffy, and M. Shepherd. Binary 
Cybergenre Classification using Theoretic Feature 
Measures. Proc. Int. Conf. on Web Intelligence, 2006. 
 
6
Proceedings of the 43rd Hawaii International Conference on System Sciences - 2010
[9] C. Jebari. Refined and Incremental Centroid-based 
Approach for Genre Categorization of Web Pages. Proc. 
17th Int. World Wide Web Conference, 2008. 
 
[10] I. Kanaris and E. Stamatatos. Webpage Genre 
Identification using Variable-length Character n-grams. 
Proc.Int. Conf. on Tools with Artificial Intelligence, 
2007, pp. 3-10.  
 
[11] V. Kešelj, F. Peng, N. Cercone, and C. Thomas. N-
gram Based Author Profiles for Authorship Attribution. 
Proc. PACLING, 2003, pp. 255-264.  
 
[12] J. Mason, M. Shepherd, and J. Duffy. Classifying Web 
Pages by Genre: A Distance Function Approach. Proc. 
5th Int. Conf. on Web Information Systems and 
Technologies, 2009. 
 
[13] J. Mason, M. Shepherd, and J. Duffy. Feature 
Selection for an n-gram Approach to Web Page Genre 
Classification. Proc. Web Intelligence, 2009. To appear. 
 
[14] S. Meyer zu Eissen and B. Stein. Genre Classification 
of Web Pages. Proc. 27th German Conf. on Artificial 
Intelligence, 2004. 
 
[15] G. Rehm. Towards Automatic Web Genre 
Identification. Proc. 37th Hawaii Int. Conf. on System 
Sciences, 2002. 
 
[16] G. Roberts. The Home Page as Genre: A Narrative 
Approach. Proc. 31st Hawaii Int. Conf. on System 
Sciences, 1998, pp. 78-86. 
 
[17] M.A. Rosso. Using Genre to Improve Web Search. 
PhD Thesis, University of North Carolina, 2005. 
[18] M. Santini. Automatic Identification of Genre in 
Webpages, PhD Thesis, University of Brighton, 2007. 
 
[19] M. Santini. Zero, Single, or Multi? Genre of Web 
Pages Through the Users’ Perspective. Information 
Processing and Management, 2008, pp. 702-737. 
 
[20] C. Shannon. A Mathematical Theory of 
Communication. Bell System Technical Journal, 1948, 
Vol. 27, pp. 379-423 and 623-656. 
 
[21] M. Shepherd and C. Watters. The Evolution of 
Cybergenres. Proc. 31st Hawaii Int. Conf. on System 
Sciences, 1998, pp. 97-109.  
 
[22] M.R. Spiegel, J. Schiller, and R.A. Srinivasan. 
Schaum's Outline of Theory and Problems of Statistics. 
McGraw-Hill, 2000. 
 
[23] B. Stein and S. Meyer zu Eissen. Retrieval Models for 
Genre Classification. Scandinavian Journal of 
Information Systems, 2008, Vol. 20(1), pp. 93-119. 
 
[24] V. Vidulin, M. Luštrek, and M. Gams. Training the 
Genre Classifier for Automatic Classification of Web 
Pages. Proc. 29th International Conf. on Information 
Technology Interfaces, 2007, pp. 93-98. 
 
[25] V. Vidulin, M. Luštrek, and M. Gams. Using Genres 
to Improve Search Engines. Proc. Int. Workshop 
Towards Genre-Enabled Search Engines, 2007. 
 
[26] Y. Yang and J. Pedersen. A comparative study on 
feature selection in text categorization. Proc. 14th Int. 
Conf. on Machine Learning, 1997, pp. 412-420. 
 
 
Appendix 
 
Table 1. Mean precision over Web page profile sizes of 5 to 50. Standard error is in parenthesis. 
 
n-gram Length 
Mean Precision for 
n-gram Distribution 
Threshold Method 
Mean Precision for 
n-gram Optimal 
Threshold Method 
Mean Precision for 
n-gram 
SVM 
2 0.989 (0.005) 0.991 (0.005) 0.989 (0.005) 
3 0.959 (0.005) 0.986 (0.005) 0.998 (0.005) 
4 0.905 (0.005) 0.984 (0.005) 0.998 (0.005) 
Average 0.951 (0.003) 0.987 (0.003) 0.995 (0.003) 
 
Table 2. Mean recall over Web page profile sizes of 5 to 50.  Standard error is in parenthesis. 
 
n-gram Length 
Mean Recall for 
n-gram Distribution 
Threshold Method 
Mean Recall for 
n-gram Optimal 
Threshold Method 
Mean Recall for 
n-gram 
SVM 
2 0.411 (0.006) 0.766 (0.006) 0.719 (0.006) 
3 0.364 (0.006) 0.739 (0.006) 0.698 (0.006) 
4 0.337 (0.006) 0.700 (0.006) 0.661 (0.006) 
Average 0.371 (0.003) 0.735 (0.003) 0.692 (0.003) 
7
Proceedings of the 43rd Hawaii International Conference on System Sciences - 2010
Table 3. Mean precision over n-gram lengths of 2 to 4.  Standard error is in parenthesis. 
 
Web Page Profile Size 
Mean Precision for 
n-gram Distribution 
Threshold Method 
Mean Precision for 
n-gram Optimal 
Threshold Method 
Mean Precision for 
n-gram  
SVM 
5 0.932 (0.009) 0.972 (0.009) 0.995 (0.009) 
10 0.941 (0.009) 0.978 (0.009) 0.994 (0.009) 
15 0.926 (0.009) 0.987 (0.009) 0.995 (0.009) 
20 0.971 (0.009) 0.989 (0.009) 0.996 (0.009) 
25 0.950 (0.009) 0.991 (0.009) 0.995 (0.009) 
30 0.964 (0.009) 0.991 (0.009) 0.996 (0.009) 
35 0.962 (0.009) 0.994 (0.009) 0.994 (0.009) 
40 0.957 (0.009) 0.991 (0.009) 0.994 (0.009) 
45 0.951 (0.009) 0.991 (0.009) 0.996 (0.009) 
50 0.956 (0.009) 0.988 (0.009) 0.994 (0.009) 
Average 0.951 (0.003) 0.987 (0.003) 0.995 (0.003) 
 
 
 
Table 4. Mean recall over n-gram lengths of 2 to 4. Standard error is in parenthesis. 
 
Web Page Profile Size 
Mean Recall for 
n-gram Distribution 
Threshold Method 
Mean Recall for 
n-gram Optimal 
Threshold Method 
Mean Recall for 
n-gram 
SVM 
5 0.318 (0.018) 0.637 (0.018) 0.674 (0.018) 
10 0.347 (0.018) 0.680 (0.018) 0.690 (0.018) 
15 0.368 (0.018) 0.725 (0.018) 0.691 (0.018) 
20 0.371 (0.018) 0.725 (0.018) 0.695 (0.018) 
25 0.371 (0.018) 0.764 (0.018) 0.694 (0.018) 
30 0.383 (0.018) 0.765 (0.018) 0.695 (0.018) 
35 0.381 (0.018) 0.767 (0.018) 0.696 (0.018) 
40 0.388 (0.018) 0.765 (0.018) 0.697 (0.018) 
45 0.388 (0.018) 0.764 (0.018) 0.696 (0.018) 
50 0.391 (0.018) 0.761 (0.018) 0.696 (0.018) 
Average 0.371 (0.003) 0.735 (0.003) 0.692 (0.003) 
8
Proceedings of the 43rd Hawaii International Conference on System Sciences - 2010
Table 5. Best results for precision.  
 
Genre 
Number 
of Web 
Pages 
Best Precision 
Vidulin et al. [24],  
Bagging Algorithm 
Best Precision 
n-gram Distribution 
Threshold Method 
 
Best 
Precision 
n-gram 
Optimal 
Threshold  
Method 
Best 
Precision 
n-gram 
SVM 
Official 55 0.73 1.00 1.00 1.00 
Shopping 66 0.72 1.00 1.00 1.00 
Prose Fiction 67 0.69 1.00 1.00 1.00 
Adult 68 0.78 1.00 1.00 1.00 
FAQ 70 0.98 1.00 1.00 1.00 
Poetry 72 0.76 1.00 1.00 1.00 
Entertainment 76 0.69 1.00 0.978 1.00 
Scientific 76 0.85 1.00 1.00 1.00 
Blog 77 0.83 1.00 1.00 1.00 
Gateway 77 0.45 1.00 0.980 1.00 
Error Message 79 0.87 1.00 1.00 1.00 
Community 82 0.76 1.00 0.964 1.00 
User input 84 0.83 1.00 1.00 1.00 
Children’s 105 0.81 1.00 1.00 1.00 
Personal 113 0.72 1.00 1.00 0.987 
Commercial/promotional 121 0.40 1.00 1.00 1.00 
Content Delivery 138 0.64 1.00 1.00 1.00 
Journalistic 186 0.62 1.00 0.994 1.00 
Informative 225 0.30 1.00 1.00 1.00 
Index 227 0.63 1.00 1.00 1.00 
Average 103 0.70 1.00 0.996 0.999 
 
 
 
 
9
Proceedings of the 43rd Hawaii International Conference on System Sciences - 2010
Table 6. Corresponding recall for best precision results.  
 
Genre 
Number 
of Web 
Pages 
Recall 
Vidulin et al. [24],  
Bagging Algorithm 
Recall 
n-gram Distribution 
Threshold Method 
Recall 
n-gram 
Optimal 
Threshold 
Method 
Recall 
n-gram 
SVM 
Official 55 0.27 0.587 0.944 0.762 
Shopping 66 0.33 0.136 0.602 0.712 
Prose Fiction 67 0.30 0.255 0.744 0.866 
Adult 68 0.71 0.395 0.683 0.265 
FAQ 70 0.73 0.612 0.985 0.844 
Poetry 72 0.61 0.417 0.929 0.903 
Entertainment 76 0.27 0.378 0.677 0.723 
Scientific 76 0.51 0.631 0.972 0.869 
Blog 77 0.56 0.453 0.776 0.284 
Gateway 77 0.12 0.481 0.641 0.765 
Error Message 79 0.68 0.555 1.00 0.898 
Community 82 0.55 0.524 0.792 0.732 
User input 84 0.57 0.536 0.779 0.690 
Children’s 105 0.48 0.619 0.872 0.133 
Personal 113 0.16 0.382 0.596 0.734 
Commercial/promotional 121 0.04 0.540 0.798 0.669 
Content Delivery 138 0.23 0.210 0.456 0.797 
Journalistic 186 0.36 0.344 0.916 0.860 
Informative 225 0.09 0.209 0.714 0.818 
Index 227 0.37 0.419 0.534 0.771 
Average 103 0.40 0.434 0.770 0.705 
 
 
10
Proceedings of the 43rd Hawaii International Conference on System Sciences - 2010
