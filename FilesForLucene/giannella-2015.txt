An Improved Algorithm for Unsupervised Decomposition
of a Multi-Author Document
Chris Giannella
Human Language Technology Department, The MITRE Corporation, 7515 Colshire Drive, McLean, VA 22102.
E-mail: cgiannel@acm.org
This article addresses the problem of unsupervised
decomposition of a multi-author text document: identi-
fying the sentences written by each author assuming the
number of authors is unknown. An approach, BayesAD,
is developed for solving this problem: apply a Bayesian
segmentation algorithm, followed by a segment cluster-
ing algorithm. Results are presented from an empirical
comparison between BayesAD and AK, a modified
version of an approach published by Akiva and Koppel
in 2013. BayesAD exhibited greater accuracy than AK in
all experiments. However, BayesAD has a parameter that
needs to be set and which had a nontrivial impact on
accuracy. Developing an effective method for eliminat-
ing this need would be a fruitful direction for future work.
When controlling for topic, the accuracy levels of
BayesAD and AK were, in all but one case, worse than a
baseline approach wherein one author was assumed to
write all sentences in the input text document. Hence,
room for improved solutions exists.
Introduction
Authorship analysis is a field of study that aims to infer
authorship information from a document or corpus of docu-
ments. The field can be divided into several subfields
depending upon the type of authorship information to be
inferred. One subfield is authorship distinction (or similarity
detection), “the task of grouping documents by authorship
when the author of none of those documents is known. . . .”
(Layton, Watters, & Dazeley, 2011, p. 98). An interesting
variation on problems in this subfield has received relatively
little attention: unsupervised decomposition of a single
multi-author text document, that is, identifying the sentences
that were written by each author assuming the number of
authors is unknown. Akiva and Koppel (2012) argue that an
effective solution to this problem, when the number of
authors is assumed known, would be of practical interest in
a variety of contexts including “commercial or legal interest,
as in the case of contemporary documents, or of academic or
cultural interest, as in the case of important historical docu-
ments. . . .” (p. 205). This argument applies also to the more
general version of the problem where the number of authors
is unknown.
Formally stated, the unsupervised decomposition of a
single multi-author document problem is defined as follows.
Given D, a multi-author document consisting of |D|
sentences:d[1], . . . , d[|D|], produce a partition1 {C1, . . . ,
Cm} of the sentences matching authorship; d[i] and d[j] are
in the same part if and only if d[i] and d[j] were written by
the same author. The number of parts, m, is not specified and
must be automatically determined. Furthermore, neither
writing samples from the authors of D nor ground truth of
any kind is available. For brevity, in the remainder of this
article “unsupervised decomposition of a single multi-
author document” is shortened to “unsupervised authorship
decomposition” or “authorship decomposition.” The same
problem is addressed by Akiva and Koppel (2013), except
there the number of authors is assumed known.
This article develops an approach, BayesAD, for solving
the authorship decomposition problem. The approach is sum-
marized in the next section: First, divide D into subsequences
of consecutive sentences (segments); second, cluster the seg-
ments to produce the final authorship decomposition. The
segmentation2 algorithm is then described and adopts a
Bayesian approach combining ideas from Eisenstein and
Barzilay (2008) and Utiyama and Isahara (2001). The
segment clustering algorithm is described (with details in the
Appendix), using a modified version of the spectral clustering
algorithm by Zelnik-Manor and Perona (2004).
BayesAD was empirically compared with AK, a modified
version of the approach employed by Akiva and Koppel
(2013). The details of AK as well as the definition of the
accuracy metric used to quantify the comparison are con-
tained in the section titled Experiments: Decomposition
Approaches and Accuracy Quantification. The details of the
data used and the experimental procedure are then provided,
followed by a discussion of results obtained. Related work is
discussed in the concluding section.
Received February 27, 2014; revised June 19, 2014; accepted June 21, 2014
© 2015 ASIS&T • Published online in Wiley Online Library
(wileyonlinelibrary.com). DOI: 10.1002/asi.23375
JOURNAL OF THE ASSOCIATION FOR INFORMATION SCIENCE AND TECHNOLOGY, ••(••):••–••, 2015
Summary of BayesAD
In this approach, the following steps are used:
(1) Apply a segmentation algorithm to divide the sequence of
sentences in D into subsequences (segments) S1, . . . , Sq
each consisting of consecutive sentences. The number of
segments q is not specified and must be automatically
determined. The goal of the algorithm is to produce as
few segments as possible while respecting authorship: All
sentences in the same segment are written by the same
author.
(2) Apply a clustering algorithm to the segments with the goal
of grouping together those whose sentences were written by
the same author. Let SegC1, . . . , SegCm denote the resulting
segment clusters; the number of segment clusters m must be
automatically determined. The final partition {C1, . . . , Cm}
is formed in the natural way: Ci is the set of all sentences
that appear in a segment in SegCi.
The figure given here illustrates the overall approach. In
this example, step (1) produces q = 7 segments (top part of
the figure). Step (2) produces m = 3 segment clusters as
illustrated in solid red {S1, S3, S4}, shaded blue {S2, S7}, and
white {S5, S6}.
The Bayesian Segmentation Algorithm
D is assumed to arise according to a stochastic generative
model described next. The model is a combination of the
models described by Eisenstein and Barzilay (2008) and
Utiyama and Isahara (2001). An algorithm is developed for
choosing a segmentation of the sentences in D maximizing
the log-joint-likelihood.
The Stochastic Generative Model
Let |d[i]| denote the number of words3 in sentence d[i]
and W denote the set of all words that appear in any sentence
in D. For simplicity, the words in all sentences are denoted
by integers 1 to |W|. Let Z denote the set of all segmentations
of the sentences in D without restriction on the number of
segments present. Given a segmentation z in Z, |z| denotes
the number of segments in z. The segmentation is repre-
sented as a sequence of integers 0 = z(0) < z(1) < . . . <
z(|z|) = |D|. The jth segment is {d[z(j − 1) + 1], d[z(j − 1) + 2],
. . . , d[z(j)]}. Let θ0 ∈ ℝ+|W| denote a hyper-parameter vector
specifying a Dirichlet prior in the following generative
model.
First, a segmentation z ∈ Z is drawn from the prior dis-
tribution defined by Utiyama and Isahara (2001) which
assigns less probability to segmentations with more seg-
ments. Next, for each segment (the jth), the parameters θj for
the word distribution are drawn from a Dirichlet prior with
parameters θ0. The sentences in the jth segment are generated
independently. For each, the words are generated indepen-
dently by drawing each from a categorical distribution with
parameters θj. More precisely, the generative model pro-
ceeds as follows.
1. Choose z with probability |D|–|z|/σ(Z, |D|) where σ(Z, |D|)
is a normalization constant only depending on |D| and Z.
This prior distribution is based on the minimum descrip-
tion length principle as discussed by Utiyama and Isahara
(2001).
2. For j = 1 to |z| do
a. Choose θj according to4 Dir(.;θ0).
b. For i = z(j − 1) + 1 to z(j) do
I. For h = 1 to |d[i]| do
1. Generate the hth word of the ith sentence of the jth
segment according to5 Cat(.;θj).
The desired segmentation is z* = argmaxz∈Z[log(Pr(D,
z|θ0))]. An algorithm for computing z*, assuming fixed θ0, is
described next.
Maximizing the Log-Joint-Likelihood, logPr(D, z|θ0)
Let S(W) denote the set of all probability distributions
over W. It follows that
Pr D z Pr z Pr d i i z j
to z j
j S Wj
z
j
, | :
|
θ
θ
θ
0
1
1 1( ) = ( ) [ ] = −( ) +
( )}
{(
∈ ( )=
∫∏
) ( )Pr dj jθ θ θ| .0
Pr({d[i]:i = z(j − 1) + 1 to z(j)}|θj}) denotes the probability
of generating the jth segment {d[i]:i = z(j − 1) + 1 to z(j)}
given word categorical distribution parameters θj.
From the generative model priors, Pr(z) = |D|–k(z)/σ(Z, |D|)
and Pr(θj|θ0) = Dir(θj; θ0), it follows that (with C not
depending on z)
log , | log log :Pr D z D Pr d i
i
j S Wj
z
θ
θ
0
1
( )( ) = − ( ) + [ ]{(
⎛
⎝
⎜
⎡
⎣
⎢
⎢
=
∈ ( )=
∫∑
z j to z j d Cj j j−( ) + ( )} ) ( )
⎞
⎠
⎟
⎤
⎦
⎥
⎥
+1 1 0| ; .θ θ θ θDir
Since each term in the given sum corresponds to one and
only one segment in z, then argmaxz∈Z[log(Pr(D, z|θ0))] can
be computed using a weighted graph longest path approach
like that employed by Utiyama and Isahara (2001). To see
how, consider the directed graph with vertices {0, 1, . . . ,|D|}
and an edge from u to v for all 0 ≤ u < v ≤ |D|. Utiyama and
Isahara illustrated how each segmentation z in Z corresponds
to a path from vertex 0 to |D|, and vice versa. Let Term(u, v)
denote the weight on the edge from u to v and be defined as
the term in the above sum corresponding segment {d[u + 1],
. . . , d[v]}:
S1 S2 S3 S4 S5 S6 S7
S1 S2 S3 S4 S5 S6 S7
2 JOURNAL OF THE ASSOCIATION FOR INFORMATION SCIENCE AND TECHNOLOGY—•• 2015
DOI: 10.1002/asi
Term
Dir
def
u v D
d i i u to v dj j
, log
log Pr : | ;
( ) = − ( )
+ [ ] = +{ }( ) ( )1 0θ θ θ θ j
S Wjθ ∈ ( )
∫
⎛
⎝
⎜
⎞
⎠
⎟ .
Hence, argmaxz∈Z[log(Pr(D, z|θ0))] can be computed by
applying Dijkstra’s algorithm to find the largest weight path
from vertex 0 to |D|.
Computing Term(u, v)
Let c([u, v], w) denote the number of times word w
appears in the sentences in the segment defined by edge
[u, v]: {d[u + 1], d[u + 2], . . . , d[v]}. Let θ[u, v] denote the
parameters of the word categorical distribution for the
segment defined by edge [u, v]. From the generative model
independence assumptions, it follows that
Term
Dir
u v D
w du v
c u v w
u v u
, log
log ;,
, ,
, ,
( ) = − ( )
+ [ ] ( )[ ] [ ]( ) [ ]θ θ θ θ0 v
w
W
S Wu v
[ ]
=∈ ( )
∏∫
[ ]
⎛
⎝
⎜
⎞
⎠
⎟
1θ ,
.
Let s(c[u, v]) denote Σwc([u, v], w). From the definition of
the Dirichlet distribution and a property of the gamma func-
tion,6 it follows that
Term u v D w i
i
c u v w
w
W
, log log
log
, ,
( ) = − ( ) + [ ]+( )
−
=
[ ]( )−
=
∑∑ θ
σ θ
0
0
1
1
0( ) +( )
=
[ ]( )−
∑ i
i
s c u v
0
1,
.
Segment Clustering
In this section, an algorithm is described for clustering a
segmentation S1, . . . , Sq of the sentences in D. The cluster-
ing algorithm does not require the number of clusters to be
specified. The clustering algorithm is very similar to the one
by Zelnik-Manor and Perona (2004).
1. Compute q × q similarity matrix M, whose (i, j) entry is 0
if i = j; otherwise, one minus the Jensen–Shannon diver-
gence7 between the word frequency distributions over Si
and Sj.
2. Compute q × q normalized similarity matrix M′ =
Deg(M)−0.5MDeg(M)−0.5. Deg(M) is the degree matrix for
M: the diagonal matrix whose ith diagonal entry is the sum
of the entries in the ith row of M.
3. Compute K*, the desired number of clusters, using an
approach based on that of Zelnik-Manor and Perona
(2004, Section 3), but with nontrivial differences; see the
Appendix for details.
4. Compute the dominant K* eigenvectors of M′. Let V
denote the q × K* matrix whose ith column is the ith eigen-
vector. Normalize the rows of V to have Euclidean length
one.
5. Apply K*-means clustering to the normalized rows of V
with initial centroids chosen according to Arthur and
Vassilvitskii (2007). Si and Sj are put into a cluster if and
only if the ith and jth normalized rows of V end up in the
same K*-means cluster.
Experiments: Decomposition Approaches and
Accuracy Quantification
Authorship Decomposition Approaches
BayesAD was compared to a baseline approach, denoted
“1Author,” which assigns a single author to D. BayesAD
was also compared to a modified version of Akiva and
Koppel’s approach (2013), denoted AK and described
below.
1. Divide the sentences in D into subsequences (segments)
of a fixed number, AK_Segment_Size, of consecutive
sentences.8 The setting of AK_Segment_Size is explained
later.
2. For each word in D, count the number of different seg-
ments from step 1 in which the word appears. Sort the
words in D in decreasing order by this count and identify
the top 500 words in the sorted list; call these the 500
most common words. For each segment, build a length
500 binary vector whose ith entry is one (zero) if the ith
most common word is (is not) in the segment.
3. Build a segment similarity matrix, M, whose (i, j) entry is
zero if i = j, else is the cosine similarity between the
binary vectors for the ith and jth segments.
4. Cluster the segments as done in steps 2–5 in the previous
section, Segment Clustering. The clustering algorithm is
different than the one used by Akiva and Koppel. The
primary difference is that the number of clusters is
unknown and must be automatically determined (Akiva
and Koppel assumed the number of authors was known).
5. For each segment cluster, compute the “core” segments as
follows. Let Cent denote the cluster centroid. Given
segment S, let Row(S) denote the corresponding row
for S in the eigenvector matrix V in step 4 of the Segment
Clustering section. Compute Cent2(S), the second closest
centroid to Row(S) in terms of Euclidean dis-
tance.9 Compute CentDist(S) = EuclideanDistance[Cent,
Row(S)] and CentGap(S) = EuclideanDistance[Cent2(S),
Row(S)] − EuclideanDistance[Cent, Row(S)]. Compute
BestCentDist, the top 80% (rounding down) of the seg-
ments in the cluster according to smallest CentDist(.).
Compute BestCentGap, the top 80% (rounding down) of
the segments in the cluster according to largest
CentGap(.). Compute the core segments in the cluster as
the intersection between BestCentDist and BestCentGap.
If the intersection is empty, then the top segment by
CentDist(.) is assigned as the only core segment.
6. Compute FT, the set of all words that appear at least five
times in D. Assign a label to each segment, the number of
the cluster containing the segment. Represent each
segment as a length |FT| feature vector whose ith entry is
the number of times the ith word in FT appears in the
segment. Using version 2.0.7 of the MALLET open-
source library (McCallum, 2002), train a maximum
entropy classifier on the labeled feature vectors. Akiva
JOURNAL OF THE ASSOCIATION FOR INFORMATION SCIENCE AND TECHNOLOGY—•• 2015 3
DOI: 10.1002/asi
and Koppel used a support vector machine (SVM), but for
AK a maximum entropy classifier was used since,
unlike an SVM, it directly provides label probability
distributions.
7. For each sentence in D, compute its length |FT| feature
vector, then apply the maximum entropy classifier and
compute the largest label probability. For each sentence
whose largest label probability was among the top 25%,
assign the sentence its largest probability label.
8. For each sentence s in D which was not assigned a label
in step 7, find s′, the closest sentence10 to s in D which was
assigned a label in step 7, and assign that label to s. Here,
“closest” is based on the number of sentences in D
between s and s′. Akiva and Koppel use a different pro-
cedure to assign labels to sentences. Their procedure uti-
lizes properties of the boundary of the SVM produced
during their step 6.
Quantifying Authorship Decomposition Accuracy
An authorship decomposition, {C1, . . . , Cm}, is a parti-
tion of the sentences in D. Ground truth authorship forms
another partition, denoted {Γ1, . . . , Γt}. Let nij denote |Ci ∩
Γj|. Akiva and Koppel (2013) quantified the accuracy of {C1,
. . . , Cm} by using purity, a common extrinsic clustering
validation index:
D n
j t
ij
i
m
−
≤ ≤=
{ }∑1
1
1
max
A weakness of this index was pointed out by Manning,
Raghavan, and Schutze (2008, p. 357): “High purity is easy
to achieve when the number of clusters is large—in particu-
lar, purity is 1 if each document gets its own cluster.” This is
not a problem in Akiva and Koppel’s setting where the
number of authors (t) is assumed known. However, with this
assumption dropped, this weakness is a problem; hence
purity is not used in this article. Instead, Akiva and Koppel’s
(2012) extrinsic clustering validation index is used and is
referred to as matching accuracy.
To motivate the definition of matching accuracy, it is
useful to first consider a simpler index that quantifies the
accuracy of {C1, . . . , Cm} as follows. Assign each sen-
tence in D proposed label i and ground truth label j if the
sentence is in Ci and Γj. Compute standard classification
accuracy:
D nii
i
m t
−
=
{ }
∑1
1
min ,
.
This index is flawed, however, as a renumbering of the parts
in {C1, . . . , Cm} or {Γ1, . . . , Γt} could cause the standard
classification accuracy to change. The accuracy should not
be affected by any such renumbering. To remedy this flaw,
maximum classification accuracy is chosen over all one-to-
one mappings between part numbers.
If t ≥ m, then let Δ(t ≥ m) denote the set of all one-to-one
mappings from {1, . . . , m} into {1, . . . , t} and, given δ in
Δ(t ≥ m), let
MatchAcc
def
t m i i
i
m
D n≥
−
( )
=
( ) = ∑δ δ1
1
.
If m > t, then let Δ(m > t) denote the set of all one-to-one
mappings from {1, . . . , t} into {1, . . . , m} and, given δ in
Δ(m > t), let
MatchAcc
def
m t i i
i
t
D n>
−
( )
=
( ) = ∑δ δ1
1
.
The matching accuracy of {C1, . . . , Cm}, is defined as
max
max
δ
δ
δ
δ
∈ ≥( ) ≥
∈ >( ) >
( ){ } ≥
( ){
Δ
Δ
t m
t m
m t
m t
t mMatchAcc if
MatchAcc } >
⎧
⎨
⎪
⎩⎪
if m t.
Experiments: Data and Procedures
Data
Upon request, Navot Akiva provided two corpora, the
same as those used by Akiva and Koppel (2013): BP-Blog,
NYT-Columnists.
The BP-Blog corpus is a portion of the “Becker–Posner
Blog”11 which consists of blog posts by Gary Becker and
Richard Posner. This corpus was preprocessed as follows.
Blog posts pertaining to six topics were manually selected,
sentence segmented, their title lines (e.g., “Comment on Tort
Reform-BECKER”) removed, and concatenated to form six
multi-author documents. Each multi-author document per-
tains to one topic and has alternating authorship (see
Table 1).
Many of the posts are direct responses to previous posts
and contain sentences directly indicating the authorship of
the previous post, for example, “As Becker explains, a driver
does not consider the effect of his driving on the other users
of the road, but only on himself.” In these sentences, 14 in
total, “Becker” or “Posner” was replaced with “XXXX.”
Finally, the six multi-author documents were tokenized
using the Stanford English core natural language processing
(NLP) tokenizer (version 1.3.4) with the default settings.12
TABLE 1. The seven multi-author documents created from the BP-Blog
corpus.
Topic Author order and number of sentences per post
Tort Reform (TR) Posner (29), Becker (31), Posner (24)
Profiling (Pro) Becker (35), Posner (19), Becker (21)
Tenure (Ten) Posner (73), Becker (36), Posner (33), Becker
(19)
Traffic Congestion (TC) Becker (57), Posner (33), Becker (20)
Microfinance (Mic) Posner (51), Becker (37), Posner (44), Becker
(33)
Senate Filibuster (SF) Posner (39), Becker (26), Posner (28), Becker
(24)
4 JOURNAL OF THE ASSOCIATION FOR INFORMATION SCIENCE AND TECHNOLOGY—•• 2015
DOI: 10.1002/asi
Six experiments were performed, one for each multi-
author document. In each experiment: 1Author was run once
and its matching accuracy was computed; BayesAD and AK
were run 500 times and their mean matching accuracies and
0.95 confidence intervals13 were computed. BayesAD and
AK were run multiple times because they are non-
deterministic, unlike 1Author. The clustering algorithm used
by BayesAD and AK involves the nondeterministic setting
of initial K*-means centroids using the technique of Arthur
and Vassilvitskii (2007).
In all experiments, the AK_Segment_Size parameter was
set to 15 to be slightly smaller than all author run lengths (an
author run is a consecutive sequence of sentences written by
the same author).
The NYT-Columnists corpus is a collection of opinion
pieces written by four New York Times columnists (see
Table 2). The corpus appeared to be sentence-segmented
and tokenized. This was confirmed by N. Akiva via e-mail.
For each columnist, a sequence of sentences was formed
by concatenating the columnist’s pieces in the order they
appeared in the original corpus. Each of these sequences is
referred to as a “columnist’s sequence.” Unlike the BP-Blog
corpus, the experiments performed using the NYT-
Columnists corpus are designed to examine the impact of
author run length and number of runs per author on decom-
position approach accuracy. As such, a more complex
experimental procedure is used.
Experimental Procedure Using the
NYT-Columnists Corpus
The experiment involved two parameters: meanARL (the
mean author run length) and numRperA (number of runs per
author). The experiment consisted of numTrials trials:
During each, a multi-author document D is produced, the
authorship decomposition approaches are applied, and
matching accuracy is computed for each approach. The pro-
cedure for producing multi-author documents guarantees
that each contains exactly numRperA runs of consecutive
sentences from each columnist’s sequence, concatenated in
random order (possibly concatenating two runs from the
same columnist). In detail, each trial proceeds as follows.
For i in {0, 1, 2, 3}, let numSentences(i) denote the
number of sentences in the ith columnist’s sequence (the
“Total Number of Sentences” in Table 2).
1. For trial = 1 to numTrials, do set D to empty and:
a. For i = 0 to 3 do
A. Choose startSentence(i) uniformly from {0, 1, . . . ,
(numSentences(i)–4*numRperA*meanARL)}.
Discard the first startSentence(i) sentences from the
ith column’s sequence.
b. Choose a random permutation P of {0, 1, 2, . . . ,
(4*numRperA-1)}.14
c. For j = 0 to (4*numRperA-1) do
A. Choose a number from an exponential distribu-
tion15 with meanARL and round to the nearest
integer, denoted by ChunkSize(j).
B. Compute i = [P(j)mod4] and choose the first
ChunkSize(j) sentences (or as many as possible)
from the ith columnist’s sequence; append these to
the end of D; discard these from the columnist’s
sequence.
d. Apply BayesAD, AK, 1Author to D computing the
matching accuracy for each.
2. For each authorship decomposition approach, compute
the mean and 0.95 confidence interval16 over the
numTrials accuracies for the approach.
In all experiments and all trials, the AK_Segment_Size
parameter was set to min{40, ln(2)meanARL}, based on the
following statement by Akiva and Koppel (2012, p. 207):17
“Results aren’t very sensitive to chunk size, as long as
chunks are smaller than the median single-author run.”
The meanARL parameter controls the typical number of
consecutive sentences from the same author (run length) in
D. The numRperA parameter controls the number of runs
from each author appearing in D. A smaller value of
meanARL or larger value of numRperA tends to produce a
more difficult authorship decomposition problem.
Since the opinion pieces cover a wide variety of topics, D
tends to contain multiple topics with each author transition
tending to occur simultaneously with a topic transition. This
is in marked contrast with the six multi-author documents
produced from the BP-Blog corpus that are guaranteed to
each pertain to a single topic.
Except where indicated, numTrials was set to 500 in all
NYT-Columnists experiments.
Results
Setting θ0
A series of experiments were run on the NYT-Columnists
corpus with meanARL set to 25, numRperA set to 2, and θ0
varied from 0.025 to 1. The mean matching accuracy of
BayesAD ranged from 0.416 to 0.6, achieving its maximum
near θ0 = 0.1. An “EM-style” approach similar18 to that in
Section 3.4 of Eisenstein and Barzilay’s (2008) work was
implemented: alternately choose z* and θ0, each maximizing
the log-joint-likelihood while keeping the other fixed.
However, the accuracies produced were considerably lower
than those observed for θ0 = 0.1.
In all subsequent experiments, involving the NYT-
Columnists corpus and the BP-Blog corpus, θ0 was fixed
at 0.1.
TABLE 2. Statistics regarding the NYT-Columnists corpus.
Columnist’s name
Number of
opinion pieces
Total number
of sentences
Gail Collins 273 11,327
Maureen Dowd 299 11,660
Paul Krugman 331 12,634
Thomas Friedman 279 11,230
JOURNAL OF THE ASSOCIATION FOR INFORMATION SCIENCE AND TECHNOLOGY—•• 2015 5
DOI: 10.1002/asi
Controlling for Topic
Figure 1 depicts the results of the BP-Blog corpus experi-
ments. BayesAD, AK, and 1Author were compared on six
multi-author documents, each pertaining to a single topic.
Controlling for Author Run Length and Number of Runs
per Author
Figure 2 depicts the results of the NYT-Columnists
corpus experiments when the number of runs per author
(numRperA) was fixed at two and four while the mean
author run length (meanARL) was varied. Figure 3 depicts
the results when the meanARL was fixed at 15 and
numRperA was varied over a wider range than two and four.
To provide perspective, Table 3 shows the average number
of transitions between authors.
Comparison When the Number of Authors Is Known
To better understand the observed accuracy advantage of
BayesAD over AK, both approaches were modified to work
when the number of authors is assumed known. Step 3 was
modified in the segment clustering algorithm to set K* to the
known number of authors. Figure 4 depicts the results for
the BP-Blog corpus. Figures 5 and 6 depict the results for
the NYT-Columnists corpus.
Discussion
As seen in Figures 1–3, when the number of authors in
unknown, BayesAD had greater accuracy that AK for all
topics and all examined values of mean author run length
(meanARL) and number of runs per author (numRperA).
For example,
• In the experiment involving the “Tort Reform” document,
BayesAD produced a 39% larger mean accuracy than AK;
• In the NYT-Columnists corpus experiment with
meanARL = 35 and numRperA = 4, BayesAD produced a
126% larger mean accuracy than AK.
The overall difficulty of the authorship decomposition
problem, when topic is controlled, is evident. As seen in
Figure 1, the one-author baseline approach, 1Author, had
greater accuracy than BayesAD and AK on five of six topics.
To better understand the observed accuracy advantage of
BayesAD over AK, both approaches were modified to work
when the number of authors is assumed known. As seen in
Figures 4–6, the advantage of BayesAD over AK when the
number of authors was unknown largely went away or was
reversed entirely when the number of authors was assumed
known. This is because BayesAD starts by computing an
authorship segmentation, hence relies less heavily on the
clustering algorithm than AK. Therefore, choosing the
wrong number of clusters effects BayesAD less than AK.
Conclusions
The following conclusions can be drawn from the results
of the experiments.
• The authorship decomposition approach developed in this
article, BayesAD, exhibited greater accuracy than its leading
competitor, AK, in all experiments (when the number of
authors was unknown).
• However, the accuracy of BayesAD was sensitive to the
setting of its parameter, θ0. A method for eliminating the need
to manually set this parameter reported in the literature was
implemented and yielded no success. Developing an effective
method for eliminating this need would be a fruitful direction
for future work.
• The authorship decomposition problem is challenging and
much room for improved solutions exists. Indeed, when con-
trolling for topic, the accuracy of BayesAD (and AK) was, in
all but one case, worse than a simple, one-author baseline
approach.
• The accuracy advantage of BayesAD over AK when the
number of authors was unknown largely went away or was
reversed entirely when the number of authors was assumed
known.
Related Work
Unsupervised Text Segmentation by Topic
Many researchers have addressed the problem of divid-
ing, in unsupervised fashion, a text document into subse-
quences of consecutive sentences or paragraphs (segments)
with the goal of producing as few segments as possible
while respecting topic. Step (1) in the Summary of BayesAD
section addresses the analogous problem with authorship.
Some researchers have adopted a probabilistic viewpoint
and developed algorithms for choosing a maximum-
likelihood segmentation based on various modeling assump-
tions. The Bayesian segmentation algorithm laid out here
directly applies some of these modeling ideas to text
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
TR Pro Ten TC Mic SF
BayesAD AK 1Author
FIG. 1. Results of the experiments using the BP-Blog corpus. The x-axis
depicts topic (see Table 1), the y-axis depicts matching accuracy, the data
bars depict mean matching accuracies, and the error bars for BayesAD and
AK depict 0.95 confidence intervals. In many cases the confidence intervals
are quite small and are not easily seen in the figure. [Color figure can be
viewed in the online issue, which is available at wileyonlinelibrary.com.]
6 JOURNAL OF THE ASSOCIATION FOR INFORMATION SCIENCE AND TECHNOLOGY—•• 2015
DOI: 10.1002/asi
segmentation by authorship. Specifically, the algorithm
extends the approach of Eisenstein and Barzilay (2008)
allowing the number of segments to be unspecified. The
algorithm combines the segmentation probability model of
Eisenstein with the nonuniform prior on segmentations from
Utiyama and Isahara (2001). Misra, Yvon, Cappé, and Jose
(2011) adopt a similar approach and use a segment prior
similar to that of Utiyama, but consider segmentation prob-
abilities based on latent Dirichlet allocation and multinomial
mixture models. The Bayesian segmentation algorithm
could be replaced with Misra’s algorithm. Examining this
idea is left for future work.
Other researchers have adopted a variety of other
approaches, for example: peak finding in a lexical cohesion
curve (Hearst, 1997), minimization of an ad hoc segmenta-
tion cost function (Kehagias, Pavlina, & Petridis, 2003),
converting the text segmentation problem to one of image
segmentation then applying techniques from image process-
ing (Ji & Zha, 2003), and using affinity propagation in factor
graphs (Kazantseva & Szpakowicz, 2011).
numRperA = 2 numRperA = 4
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
5 15 25 35 5 15 25 35
FIG. 2. Results of experiments using the NYT-Columnists corpus. In both charts: the x-axes depict meanARL, the y-axes depict matching accuracy, the
graph data points depict mean matching accuracies, and the error bars depict 0.95 confidence intervals. [Color figure can be viewed in the online issue, which
is available at wileyonlinelibrary.com.]
-0.1
0.1
0.3
0.5
0.7
4 8 12 16
meanARL=15
BayesAD AK 1Author
FIG. 3. Results of experiments using the NYT-Columnists corpus. The
x-axis depicts numRperA, the y-axis depicts matching accuracy, the graph
data points depict mean matching accuracies, and the error bars depict 0.95
confidence intervals. For the experiments involving numRperA = 4, 8, 12,
numTrials was set to 100. For the experiment involving numRperA =16,
numTrials was set to 8 (hence the wider confidence intervals). [Color
figure can be viewed in the online issue, which is available at
wileyonlinelibrary.com.]
TABLE 3. The average number of author transitions in the experiments
the results of which are depicted in Figure 3.*
numRperA Average number of author transitions
4 11.19
8 23.23
12 35.19
16 47.19
Note. *On average, each author had 20 sentences before a transition
occurred.
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
TR Pro Ten TC Mic SF
BayesAD AK 1Author
FIG. 4. Results of the experiments using the BP-Blog corpus when the
number of authors, two, was assumed known. The x-axis depicts topic (see
Table 1), the y-axis depicts matching accuracy, the data bars depict mean
matching accuracies, and the error bars for BayesAD and AK depict 0.95
confidence intervals. In many cases the confidence intervals are quite small
and are not easily seen in the figure. [Color figure can be viewed in the
online issue, which is available at wileyonlinelibrary.com.]
JOURNAL OF THE ASSOCIATION FOR INFORMATION SCIENCE AND TECHNOLOGY—•• 2015 7
DOI: 10.1002/asi
Intrinsic Plagiarism Detection
Some researchers have focused on developing compu-
tational approaches to detect plagiarism in a given text
document when no reference documents are provided.
Their key supposition was that changes in writing style are
indicative of a change in authorship. Stamatatos (2009)
developed an approach centered on a “style change func-
tion” based on character n-grams which quantified the sty-
listic difference between the writing in a window of fixed
size to the left and right of a fixed point in a document.
The approach built a style curve by sliding the style
function across the document, then, based on variance and
the presence of peaks in the curve, determined if the docu-
ment contained plagiarized passages and identified them. A
modest modification of this approach could be used in
place of the Bayesian segmentation algorithm. Examining
this idea is left to future work.
Stein, Lipka, and Prettenhofer (2011) developed an
approach which started by decomposing the given document
in sections of uniform length and identifying outliers based
on a variety of stylometric features. The non-outlying sec-
tions were presumed to have been written by a single author
and “unmasking” (Koppel, Schler, & Bonchek-Dokow,
2007) was applied to decide if that author also wrote all the
outlying sections.
Authorship Analysis
The application of statistical and computational methods
to problems in authorship analysis has been the focus of
much study. Koppel, Schler, and Argamon (2009) surveyed
this line of work,19 focused on three specific types of prob-
lems, and discussed how machine learning methods can be
applied to those problems.
Layton et al. (2011) addressed the problem of authorship
distinction: cluster a batch of documents (the number of
clusters is not specified) with the goal that for any pair of
documents, the documents are in the same cluster if and only
if the documents were written by the same author. The
document clustering algorithm Layton developed could be
used in place of the segment clustering algorithm in step (2)
in the Summary of BayesAD section. Doing so is left to
future work.
Graham, Hirst, and Marthi (2005) developed an approach
for segmenting text documents by identifying paragraph
numRperA = 2 numRperA = 4
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
5 15 25 35 5 15 25 35
FIG. 5. Results of the experiments using the NYT-Columnists corpus when the number of authors, four, was assumed known. In both charts: the
x-axes depict meanARL, the y-axes depict matching accuracy, the graph data points depict mean matching accuracies, and the error bars depict 0.95
confidence intervals. In all of these experiments, numTrials was set to 100. [Color figure can be viewed in the online issue, which is available at
wileyonlinelibrary.com.]
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
4 8 12 16
meanARL = 15
BayesAD AK 1Author
FIG. 6. Results of experiments using the NYT-Columnists corpus when
the number of authors, four, was assumed known. The x-axis depicts
numRperA, the y-axis depicts matching accuracy, the graph data
points depict mean matching accuracies, and the error bars depict 0.95
confidence intervals. In all experiments, numTrials was set to 10. [Color
figure can be viewed in the online issue, which is available at
wileyonlinelibrary.com.]
8 JOURNAL OF THE ASSOCIATION FOR INFORMATION SCIENCE AND TECHNOLOGY—•• 2015
DOI: 10.1002/asi
breaks where the writing style changes significantly. Their
approach is supervised in that it used a training set of docu-
ments in which the significant style change points are
known. A neural network was trained to classify consecutive
paragraphs in terms of whether or not the first is significantly
different in style than the second. Non-training documents
were segmented by applying the classifier to each pair of
consecutive paragraphs in the documents. Due to its funda-
mentally supervised nature, Graham et al.’s approach is not
applicable to the unsupervised author segmentation problem
addressed in this article.
Brooke and colleagues (Brooke, Hirst, & Hammond,
2013; Brooke, Hammond, & Hirst, 2012) devised an
approach for clustering voices in poetry20 assuming the
number of voices is known. They developed an algorithm
whose overall structure is similar to BayesAD: First apply a
voice segmentation algorithm, then apply a segment cluster-
ing algorithm. However, they used a different segmentation
and clustering algorithm and, most significantly, assumed
the number of voices is known. Nonetheless, the Bayesian
segmentation algorithm laid out in this article could be
replaced with Brooke’s segmentation algorithm. Examining
this idea is left for future work.
Akiva and Koppel (2013) defined a close variant of unsu-
pervised author segmentation problem. In their definition,
the number of authors is assumed known, but that assump-
tion is dropped in this article. Akiva and Koppel’s approach
was modified to work after dropping this assumption, as
described in this article. The modified approach was com-
pared to BayesAD, as discussed in the Results section.
Akiva and Koppel’s (2013) work is the most closely related
to this article.
Endnotes
1. A partition {C1, . . . , Cm} of the sentences in D is defined as
follows. Each part Ci is a non-empty subset of sentences in D. Each pair of
parts Ci and Cj (i ≠ j) is disjoint. Every sentence in D is contained in some
part.
2. Throughout this article, the term “segmentation” is used to mean
“linear segmentation,” as opposed to “hierarchical segmentation.”
3. A word is simply any consecutive sequence of non-whitespace
characters as defined by the Java character class “\s”: space, tab, newline,
form feed, carriage-return, and vertical tab.
4. Dir(.;θ0) denotes a Dirichlet distribution with parameters θ0. See
http://en.wikipedia.org/wiki/Dirichlet_distribution for details.
5. Cat(.;θj) denotes a categorical distribution with parameters θj. See
http://en.wikipedia.org/wiki/Categorical_distribution for details.
6. For real x > 0 and integer k ≥ 0, log Γ Γx x k( ) +( )( )
log x i
i
k
= − +( )
=
−
∑
0
1
and log logΓ Γx k x x i
i
k
+( ) ( )( ) = +( )
=
−
∑
0
1
where Γ(.)
denotes the standard gamma function, http://en.wikipedia.org/wiki/
Gamma_function
7. http://en.wikipedia.org/wiki/Jensen%E2%80%93Shannon
_divergence
8. The last segment may have fewer sentences—whatever remains
in D.
9. By virtue of the way K-means clustering works, Cent is the closest
centroid to Row(S).
10. If there are two sentences each assigned a label in step 5, each
equally close to s, and with no closer sentence with an assigned label, then
the label assigned to s is randomly chosen between the labels of the two
sentences.
11. http://www.becker-posner-blog.com
12. http://nlp.stanford.edu/software/corenlp.shtml
13. The t test was used to compute the 0.95 confidence intervals.
14. The “Knuth Shuffle” was used:
http://en.wikipedia.org/wiki/Fisher%E2%80%93Yates_shuffle
15. http://en.wikipedia.org/wiki/Exponential_distribution
16. The t test was used to compute the 0.95 confidence interval.
17. AK_Segment_Size was set to 40 in all experiments reported by
Akiva and Koppel (2012). Each author chunk was drawn from an exponen-
tial distribution with median ln(2)meanARL.
18. As implemented for this article, all components of θ0 were forced to
be the same; Eisenstein and Barzilay’s approach does not require this.
19. Koppel et al. refer to authorship analysis as “authorship
attribution.”
20. Changes of voice are often purposely made by a single author
to give characters a distinctive style. We conjecture that voice clustering
is easier than author clustering since voices are often purposely made
distinct.
21. http://en.wikipedia.org/wiki/Matrix_norm
22. More specifically: segment Sj is in the ith cluster if and only if the jth
row of X contains all zeros except in the entry for column i.
23. The Simpson Index is defined here: http://en.wikipedia.org/wiki/
Diversity_index
Appendix
Computing K*, the Desired Number of Clusters
Some notation is needed before discussing the compu-
tation of K*. Given an arbitrary matrix B, Bij denotes the (i,
j) entry in B. Bi. denotes the ith row of B and ||Bi.|| denotes
the two-norm of that row. BT denotes the transpose matrix
of B, that is, (BT)ij = Bji for all i, j. B is orthogonal if BTB
is the identity matrix. ||B||F denotes the Frobenius norm.21
Assume B is square, with the same number of rows as
columns. Then B−1 denotes the inverse matrix of B, that is,
B−1 is the square matrix such that B−1B is the identity
matrix. The reader is referred to Strang (2005) for a dis-
cussion of the fundamental definitions and properties of
the eigenvectors and eigenvalues.
To understand how K* can be computed from the q × q
normalized similarity matrix M, it is useful to consider the
following hypothetical assumption.
Hypothetical Assumption
Assume the segments cluster cleanly with respect
to their similarities in M: Mij = 0 if Si and Sj are in a dif-
ferent cluster, otherwise Mij = 1. The columns and their
corresponding rows of M can be reordered to produce
a block-diagonal matrix where each block corresponds to a
cluster. The same column and row reordering of the nor-
malized matrix M′ produces the same block-diagonal
structure.
Example: K* = 3 and each cluster contains three seg-
ments (so q = 9). M, after column and row reordering, is
JOURNAL OF THE ASSOCIATION FOR INFORMATION SCIENCE AND TECHNOLOGY—•• 2015 9
DOI: 10.1002/asi
M M M
M M M
M M M
M
′ ′ ′
′ ′ ′
′ ′ ′
11 12 13
12 22 23
13 23 33
0 0 0 0 0 0
0 0 0 0 0 0
0 0 0 0 0 0
0 0 0 ′ ′ ′
′ ′ ′
′ ′ ′
44 45 46
45 55 56
46 56 66
0 0 0
0 0 0 0 0 0
0 0 0 0 0 0
0 0 0 0 0 0
M M
M M M
M M M
M ′ ′ ′
′ ′ ′
′ ′ ′
77 78 79
78 88 89
79 89 99
0 0 0 0 0 0
0 0 0 0 0 0
M M
M M M
M M M
Key to recovering K* is the following fact, similar to
Proposition 2 in von Luxburg’s (2007) work. There exists a
q × K* orthogonal matrix X whose columns form a basis
for the eigenspace associated with the largest eigenvalue of
M, and each row of X contains all zeros except a single
entry.22
For a given K′, let X′ denote a q × K′ matrix X′ produced
by a standard eigensolver: X′ is orthogonal and its columns
are the K′ dominant eigenvectors of M′. If K′ = K*, then, like
X, the columns of X′ form a basis for the eigenspace asso-
ciated with the largest eigenvalue of M′. However, there is no
guarantee that X′ will equal X. Nonetheless, some K′ × K′
orthogonal matrix O must exist such that X′O equals X,
hence all rows of X′O contain all zeros except a single entry.
Moreover, if K′ > K*, then for any O, X′ will contain a row
with more than one non-zero entry.
Let Θ denote an error function mapping K′ × K′ orthogo-
nal matrices, O, to a number quantifying the extent to which
all rows of X′O contain all zeros except a single entry.
Formally23
Θ O
X O
X O
X O
X
i
ijj
K
iK( ) =
−
′( )
′( )
…
′( )
′
=
′
′
∑
def
SimpsonIndex1
1
2
2
1
2
, ,
O
X O
ijj
K
ij
j
K
i ( )
⎡
⎣
⎢
⎢
⎤
⎦
⎥
⎥
′( ) ≠
⎧
⎨
⎪
⎪
⎩
⎪
⎪ =
′ =
′
∑
∑
2
1
2
1
0
0
if
otherwise
=
=
′
=
′
∑
∑∑= − ′
⎡
⎣
⎢
⎤
⎦
⎥ ′
⎛
⎝⎜
⎞
⎠⎟
′ ≠
1
4
1
4
1
41
1
0
0
q
i
ij jh
j
K
h
K
i
X
X O X
.
.if
otherwise.
⎧
⎨
⎪
⎩
⎪=
∑
i
q
1
The last equality follows from the definition of the Simpson
Index and the fact that O is orthogonal. Θ(O) equals its
minimum of zero exactly when all rows of X′O contain all
zeros except at most a single entry. Let minK′(Θ) denote the
minimum of Θ over all K′ × K′ orthogonal matrices. Rea-
soning in the previous paragraph implies the approach to
recovering K*, namely, find the largest value of K′ such that
minimizes minK′(Θ).
Computing minK′(Θ) amounts to solving a constrained
optimization problem.
minimize any real-valued matrix
subject to is ortho
O O K K
O
( ) ′ × ′{ }:
: gonal.
Constrained optimization problems with such constraints
have been addressed in the literature, in particular by Wen
and Yin (2013). The gradient-descent search therein will
find a critical point of Θ and guarantee that the matrix
produced at each step is orthogonal. However, a critical
point of Θ is not guaranteed to be a local minimum, let alone
a global minimum. A simple way to deal with this problem
is by repeating the gradient-descent search 10 times from
randomly chosen starting points and picking the search ter-
mination matrix, O(terminate), which minimizes Θ over all
termination matrices.
The General Case
If the hypothetical assumption is dropped that the seg-
ments cluster cleanly with respect to their similarities, then
M′ will enjoy only approximately the block structure
described earlier. Nonetheless, the approach described in the
hypothetical case is still applied. To be clear, the algorithm
for finding K* in the general case is as follows.
1. For K′ = 2 to q − 1, do
a. Use a standard eigensolver to produce q × K′ orthogonal
matrix X′ whose columns are the K′ dominant eigenvectors
of M′.
b. Generate 10 matrices randomly from the space of all K′ × K′
orthogonal matrices. Each of these will serve as a starting
point to a gradient-descent search as described next. Each
search will find an orthogonal matrix that is approximately a
critical point of Θ. Let err(K′) denote the minimum value of
Θ over all these 10 approximate critical points.
2. Return the largest 2 ≤ K* ≤ q − 1 for which err(K*) =
min{err(K′): 2 ≤ K′ ≤ q − 1}.
Gradient-Descent on Θ Under Orthogonality Constraints
Let B denote a K′ × K′ matrix and G[B] denote a K′ × K′
matrix whose (a, b) entry is the partial derivative of Θ with
respect to Oab evaluated at B:
∂
∂
[ ] = − ′
⎡
⎣
⎢
⎤
⎦
⎥ ′
⎛
⎝⎜
⎞
⎠⎟
′
∂
∂=
′
=
′
∑∑
O
B X
X O X
O
O
ab
i
ij jh
j
K
h
K
ij
jh4
4
1
3
1. abj
K
i
i
q
ia
i
B X
X
X
[ ] ′ ≠
⎧
⎨
⎪
⎩
⎪
= −
′
′
⎡
⎣
⎢
⎤
⎦
=
′
=
∑∑ 1
4
1
4
0
0
4
if
otherwise
.
.
⎥ ′
⎛
⎝⎜
⎞
⎠⎟
′ ≠
⎧
⎨
⎪
⎩
⎪
=
′
=
∑∑ X B Xij jbj
K
i
i
q
1
3
4
1
0
0
if
otherwise
.
.
A gradient-descent approach is described in Algorithm 1 in
Wen and Yin’s (2013) work. This algorithm takes the fol-
lowing inputs: X′ the matrix of orthogonal eigenvectors of
M′, X(0)′ an orthogonal K′ × K′ matrix that serves as the
starting point of the search and parameters 0 < c, ρ < 1,
0 < ε, 0 < maxNumSteps. The ith step of the search will
produce a new K′ × K′ orthogonal matrix X(i + 1)′.
10 JOURNAL OF THE ASSOCIATION FOR INFORMATION SCIENCE AND TECHNOLOGY—•• 2015
DOI: 10.1002/asi
1. Set the step number i to 0 and compute gradient matrix G[X(0)′]
and matrix A(0) = G[X(0)′]X(0)′T − X(0)′G[X(0)′]T.
2. For any τ > 0, let Y(τ) denote [I + A(i)(τ/2)]−1[I − A(i)(τ/2)]X(i)′.
[Lemma 3 in Wen and Yin (2013) shows that Y(τ) is defined,
orthogonal, and represents a descent path for Θ from X(i)′.] Use
a line search algorithm to determine τ′ the step size taken along
path Y(τ). The line search algorithm uses parameters ρ and c and
is described later.
3. Set I to i + 1 and X(i)′ to Y(τ′). Compute gradient matrix G[X(i)′]
and matrix A(i) = G[X(i)′]X(i)′T − X(i)′G[X(i)′]T.
4. If i ≥ maxNumSteps or ||A(i)X(i)′||F < ε, then return X(i)′ and
terminate, otherwise go to step II. The second condition checks
whether the first-order Lagrange optimality condition is close
enough to being satisfied at X(i)′, as discussed in Lemma 1 of
Wen and Yin (2013). If so, then X(i)′ is regarded as an approxi-
mate critical point of Θ and the algorithm terminates.
For the stopping parameters ε and maxNumSteps, values
of 0.00001 and 1000 were used, the same as used by Wen
and Yin (2013) in their experiments. The line search algo-
rithm is called “backtracking-Armijo” and is described by
Procedure 3.1 in Nocedal and Wright’s (1999) work. The
algorithm takes these inputs: X(i)′, a K′ × K′ orthogonal
matrix which is the starting point of the line search, that is,
Y(0) = X(i)′; A(i), the matrix G[X(i)′]X(i)′T − X(i)′G[X(i)′]T;
and parameters 0 < ρ, 0 < c.
1. Set τ′ to 1.
2. Repeat until Θ([I + A(i)(τ′/2)]−1[I − A(i)(τ′/2)]X(i)′) ≤ Θ(X(i) ′)
− 0.5τ′c(||A(i)||F)2
a. Set τ′ = ρτ′.
The “Repeat-until” condition is drawn from (26a) and
Lemma 3 part 3 in Wen and Yin (2013). It is based on the
Armijo condition which, if true, implies that the step length
yields a sufficient decrease in Θ. The geometric reduction in
the step length ensures that the step length will not be too
small. Common settings were used for ρ and c, 0.5 and
0.0001, respectively. The setting for c is based on Nocedal
and Wright (1999, p. 38).
Acknowledgments
I would like to thank the following people for their assis-
tance in conducting this research: Richard MacMillan
produced the Java implementation of the Bayesian segmen-
tation algorithm; Navot Akiva provided the data used in all
experiments.
References
Akiva, N., & Koppel, M. (2012). Identifying distinct components of a
multi-author document. In Proceedings of the European Intelligence and
Security Informatics Conference (pp. 205–209). Washington, DC: IEEE
Computer Society.
Akiva, N., & Koppel, M. (2013). A generic unsupervised method for
decomposing multi-author documents. Journal of the American Society
for Information Science and Technology, 64(11), 2256–2264.
Arthur, D., & Vassilvitskii, S. (2007). k-means++: The advantages of
careful seeding. In Proceedings of the ACM-SIAM Symposium on
Discrete Algorithms (SODA) (pp. 1027–1035). Philadelphia, PA:
Society for Industrial and Applied Mathematics.
Brooke, J., Hammond, A., & Hirst, G. (2012). Unsupervised stylistic seg-
mentation of poetry with change curves and extrinsic features. In First
Workshop on Computational Linguistics for Literature, Co-located with
the Conference of the North American Chapter of the Association for
Computational Linguistics: Human Language Technologies (NAACL-
HLT) (pp. 26–35). Montreal, Canada: Association for Computational
Linguistics. Available at: http://aclweb.org/anthology/W12-2504
Brooke, J., Hirst, G., & Hammond, A. (2013). Clustering voices in the
waste land. In Second Workshop on Computational Linguistics for Lit-
erature, Co-located with the Conference of the North American Chapter
of the Association for Computational Linguistics: Human Language
Technologies (NAACL-HLT) (pp. 41–46). Atlanta, GA: Association for
Computational Linguistics. Available at: http://aclweb.org/anthology/
W13-1406
Eisenstein, J., & Barzilay, R. (2008). Bayesian unsupervised topic segmen-
tation. In Proceedings of the Conference on Empirical Methods in
Natural Language Processing (EMNLP) (pp. 334–343). Stroudsburg,
PA: Association for Computational Linguistics.
Graham, N., Hirst, G., & Marthi, B. (2005). Segmenting documents by
stylistic character. Natural Language Engineering, 11(4), 397–415.
Hearst, M. (1997). TextTiling: Segmenting text in multi-paragraph subtopic
passages. Computational Linguistics, 23(1), 33–64.
Ji, X., & Zha, H. (2003). Domain-independent text segmentation using
anisotropic diffusion and dynamic programming. In Proceedings of the
26th ACM SIGIR Conference on Research and Development in Infor-
mation Retrieval (pp. 322–329). New York: ACM Press.
Kazantseva, A., & Szpakowicz, S. (2011). Linear text segmentation using
affinity propagation. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing (EMNLP) (pp. 284–293).
Stroudsburg, PA: Association for Computational Linguistics.
Kehagias, A., Pavlina, F., & Petridis, V. (2003). Linear text segmentation
using a dynamic programming algorithm. In Proceedings of the 10th
Conference of the European Association for Computational Linguistics
(EACL) (pp. 171–178). Stroudsburg, PA: Association for Computational
Linguistics.
Koppel, M., Schler, J., & Argamon, S. (2009). Computational methods in
authorship attribution. Journal of the American Society for Information
Science and Technology, 60(1), 9–26.
Koppel, M., Schler, J., & Bonchek-Dokow, E. (2007). Measuring differen-
tiability: Unmasking pseudonymous authors. The Journal of Machine
Learning Research, 8, 1261–1276.
Layton, R., Watters, P., & Dazeley, R. (2011). Automated unsupervised
authorship analysis using evidence accumulation clustering. Natural
Language Engineering, 19(1), 95–120.
Manning, C., Raghavan, P., & Schutze, H. (2008). Introduction to informa-
tion retrieval. New York: Cambridge University Press.
McCallum, A. (2002). MALLET: MAchine Learning for LanguagE
Toolkit. Retrieved from http://mallet.cs.umass.edu
Misra, H., Yvon, F., Cappé, O., & Jose, J. (2011). Text segmentation: A
topic modeling perspective. Information Processing and Management,
47(4), 528–544.
Nocedal, J., & Wright, S. (1999). Numerical optimization. New York:
Springer Science+Business Media Inc.
Stamatatos, E. (2009). Intrinsic plagiarism detection using character
n-gram profiles. In Proceedings of the PAN Workshop as Part of the 25th
Annual Conference of the Spanish Society for Natural Language
Processing (SEPLN) (pp. 38–46). San Sebastian (Donostia), Spain:
CEUR-WS.org. Available at: http://ceur-ws.org/Vol-502/paper8.pdf
Stein, B., Lipka, N., & Prettenhofer, P. (2011). Intrinsic plagiarism analysis.
Language Resources and Evaluation, 45(1), 63–82.
Strang, G. (2005). Linear algebra and its application (4th ed.). Belmont,
CA: Brooks/Cole Publishing Co.
Utiyama, M., & Isahara, H. (2001). A statistical model for domain-
independent text segmentation. In Proceedings of the Conference of the
Association for Computational Linguistics (ACL) (pp. 491–498).
Stroudsburg, PA: Association for Computational Linguistics.
JOURNAL OF THE ASSOCIATION FOR INFORMATION SCIENCE AND TECHNOLOGY—•• 2015 11
DOI: 10.1002/asi
von Luxburg, U. (2007). A tutorial on spectral clustering. Statistics and
Computing, 17(4), 395–416.
Wen, Z., & Yin, W. (2013). A feasible method for optimization with
orthogonality constraints. Mathematical Programming, 142(1–2), 397–
434. doi: 10.1007/s10107-012-0584-1
Zelnik-Manor, L., & Perona, P. (2004). Self-tuning spectral clustering. In
Proceedings of the 18th Conference on Neural Information Processing
Systems (NIPS) (pp. 1601–1608). Vancouver, British Columbia, Canada:
Neural Information Processing Systems Foundation, Inc. Available at:
http://papers.nips.cc/paper/2619-self-tuning-spectral-clustering.pdf
12 JOURNAL OF THE ASSOCIATION FOR INFORMATION SCIENCE AND TECHNOLOGY—•• 2015
DOI: 10.1002/asi
