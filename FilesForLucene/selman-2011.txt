Multilayered Feedforward Neural Networks as a
Tool for Distinction of the Authors of Texts
Suvad Selman
Faculty of Engineering and Natural Sciences
International University of Sarajevo
Bosnia and Herzegovina
Email: sselman@ius.edu.ba
Alma Husagic-Selman
Faculty of Engineering and Natural Sciences
International University of Sarajevo
Bosnia and Herzegovina
Email: asselman@ius.edu.ba
Abstract—This paper proposes a means of using a multilayered
feedforward neural network to identify the author of a text. The
network has to be trained where multilayer feedforward neural
network as a powerful scheme for learning complex input-output
mapping have been used in learning of the textual descriptors
in a paragraphs of an author. The resulting training information
we get will be used to identify the texts written by authors. The
computational complexity is solved by dividing it into a number
of computationally simple tasks where the input space is divided
into a set of subspaces and then combining the solutions to those
tasks. By this, we have been able to successfully distinguish the
books authored by Leo Tolstoy, from the ones authored by Fyodor
Dostoyevsky.
Keywords - Machine learning, author identification, artificial
neural networks
I. INTRODUCTION
Author identification is the task of identifying the author of
a given text. It can be considered as a typical classification
problem, where a set of documents with known authors are
used for training and the aim is to automatically determine
the corresponding author of an anonymous text. In contrast
to other classification tasks, it is not clear which features of
a text should be used to classify an author. Consequently,
the main concern of computer-based author identification is
to define an appropriate characterization of documents that
captures the writing style [6, 7] of authors. Recent research
has used techniques from machine learning [9, 10] and natural
language processing author identification.
Author identification can be used in a broad range of applica-
tions, to analyze anonymous or disputed documents/books. In
Plagiarism detection which can be used to establish whether
claimed authorship is valid. In criminal investigation as Ted
Kaczynski [19] was targeted as a primary suspect in the
Unabomber case, because author identification methods deter-
mined that he could have written the Unabombers manifesto.
In forensic investigations where verifying the authorship of e-
mails and newsgroup messages, or identifying the source of a
piece of intelligence.
In this paper an application to artificial neural networks is pre-
sented to authorship attribution is considered as a classification
task [5]. Texts studied are literary works of worldwide known
writers, Leo Tolstoy and Fyodor Dostoyevsky. Feature selected
to describe texts are lexical and syntactical components that
show promising results when used as writer invariants because
they are used rather subconsciously and reflect the individual
writing style which is difficult to be copied. Properly trained
neural networks possess generalisation properties that allow
for the required high accuracy of classification[3,4].
II. OBJECTIVES
The primary aim of author distinction is to remove uncer-
tainty about the author of some text, which can be used in
literary tasks of textual analysis for works edited, translated,
with disputed authorship or anonymous, but also with forensic
aspect in view to detect plagiarism, forgery of the whole
document or its constituent parts, verify ransom notes, etc.
Analysts claim that each writer possesses some unique char-
acteristic, called the authorial or writer invariant, which keeps
constant for all texts written by this author and perceivably
different for texts of other authors [20]. To find writer invari-
ants there are used style markers which are based on textual
properties belonging to either of four categories: lexical,
syntactic, structural, and content-specific.
Lexical descriptors provide statistics of total number of words
or characters, average number of words per sentence, charac-
ters per sentence or characters per word, frequency of usage
for individual letters or distribution of word length[1, 2].
Syntactic features reflect the structure of sentences, which can
be simple or complex, or conditional, built with punctuation
marks. Structural attributes express the organization of text
into paragraphs, headings, signatures, embedded drawings or
pictures, and also special font types or its formatting that go
with layout.
Content-specific properties recognise some keywords: words
of special meaning or significant importance for the given
context.
Unfortunately, the convenience of using contemporary word
editors and processors works against preserving individual
author styles due to its available options of ”copy and paste”. It
makes imitation of somebody elses style much easier and that
is why modern stylometric techniques aim at exploiting the
computational powers of computers to analyse patterns within
subconsciously used common parts of speech, as opposed to
historical approaches that emphasised some rare standing out
978-1-4577-0746-9/11/$26.00 ©2011 IEEE
elements of a text which could be noticed by virtually anybody
and thus likely to be faked.
III. NEURAL NETWORKS
There are a number of different answers possible to the
question of how to define neural networks. At one extreme,
the answer could be that neural networks are simply a class
of mathematical algorithms, since a network can be regarded
essentially as a graphic notation for a large class of algorithms.
Such algorithms produce solutions to a number of specific
problems. At the other end, the reply may be that these are
synthetic networks that emulate the biological neural networks
found in living organisms [18]. In light of today’s limited
knowledge of biological neural networks and organisms, the
more plausible answer seems to be closer to the algorithmic
one.
In search of better solutions for engineering and computing
tasks, many avenues have been pursued. There has been a
long history of interest in the biological sciences on the part
of engineers, mathematicians, and physicists endeavoring to
gain new ideas, inspirations, and designs. Artificial neural
networks have undoubtedly been biologically inspired, but the
close correspondence between them and real neural systems
is still rather weak [18]. Vast discrepancies exist between
both the architectures and capabilities of artificial and natural
neural networks. Knowledge about actual brain functions are
so limited, however, that there is little to guide those who
would try to emulate them. No models have been successful in
duplicating the performance of the human brain. Therefore, the
brain has been and still is only a metaphor for a wide variety of
neural network configurations that have been developed [18].
A. Topology
From topology point of view neural networks can be divided
into two categories: feed-forward and recurrent networks.
In feed-forward networks the flow of data is strictly from
input to output cells that can be grouped into layers but
no feedback interconnections can exist. On the other hand,
recurrent networks contain feedback loops and their dynamical
properties are very important. The most popularly used type
of neural networks employed in pattern classification tasks
is the feedforward network which is constructed from layers
and possesses unidirectional weighted connections between
neurons [22]. The common examples of this category are
Multilayer Perceptron or Radial Basis Function networks, out
of which the former will be addressed in more detail.
Multilayer Perceptron (MLP) type is more closely defined by
establishing the number of neurons from which it is built, and
this process can be divided into three parts, the two of which,
finding the number of input and output units, are quite simple,
whereas the third, specification of the number of hidden neu-
rons can become crucial to accuracy of obtained classification
results [22]. The number of input and output neurons can
be actually seen as external specification of the network and
these parameters are rather found in a task specification. For
classification purposes as many distinct features are defined for
objects which are analysed that many input nodes are required.
The only way to better adapt the network to the problem is
in consideration of chosen data types for each of selected
features. For example instead of using the absolute value of
some feature for each sample it can be more advantageous to
calculate its change as this relative value should be smaller
than the whole range of possible values and thus variations
could be more easily picked up by Artificial Neural Network
[22]. The number of network outputs typically reflects the
number of classification classes.
The third factor in specification of the Multilayer Perceptron
is the number of hidden neurons and layers and it is essential
to classification ability and accuracy. With no hidden layer
the network is able to properly solve only linearly separable
problems with the output neuron dividing the input space by a
hyperplane. Since not many problems to be solved are within
this category, usually some hidden layer is necessary.
With a single hidden layer the network can classify objects
in the input space that are sometimes and not quite formally
referred to as simplexes (single convex objects that can be
created by partitioning out from the space by some number of
hyperplanes) whereas with two hidden layers the network can
classify any objects since they can always be represented as
a sum or difference of some such simplexes classified by the
second hidden layer.
Apart from the number of layers there is another issue of
the number of neurons in these layers. When the number
of neurons is unnecessarily high the network easily learns
but poorly generalises on new data. This situation reminds
autoassociative property: too many neurons keep too much
information about training set rather ”remembering” than
”learning” its characteristics. This is not enough to ensure
good generalization that is needed [23].
On the other hand, when there are too few hidden neurons the
network may never learn the relationships amongst the input
data. Since there is no precise indicator how many neurons
should be used in the construction of a network, it is a common
practice to built a network with some initial number of units
and when it trains poorly this number is either increased or
decreased as required. Obtained solutions are usually task-
dependant.
For the purposes of this research a neural network with twenty
input terminals, eight hidden neurons in one hidden layer and
an output layer with one neuron is chosen as shown in Fig.1.
At first instance we feed input layer of neurons with the
text characteristics that we have taken from the particular
texts. These characteristics, represented by numbers, will be
transformed with the sigmoid function and its results will
be transfered to te hiden layer of neurons. Another type of
sigmoid function is used as processing function for the input
data at the hidden layers. Output layer of neurons, which
consists of one neuron, is not using a function to further
process the data. At the output processed data would be
compared where error would br calculated. If the error is 0
that means that the given result is same as the original text
and we take it as correct classification. On the other hand
Fig. 1. Signal flow graph of the chosen neural network
classification is incorrect. As a result of error calculations
sinoptic weights have been adjusted. Adjusting the wheights
is done in opposite dirrection, from the output to the input. At
fitst instance sinoptic wights of the output layer are adjusted,
than those of hidden layer and tha last ones to be corrected
are ones between input and hidden layer. After the adjustment
has been done another data ser is inserted to the input of the
network to check for the output. Network is trained like this
for a quite long time where error treshold has been chosen and
it will go until that point. When the learning process reaches
the treshold value it stops because it is considered as good
one.
B. Activation Functions
All neural networks take numeric input and produce nu-
meric output. The transfer function of a unit is typically chosen
so that it can accept input in any range, and produces output
in a strictly limited range (it has a squashing effect). Although
the input can be in any range, there is a saturation effect so that
the unit is only sensitive to inputs within a fairly limited range.
The illustration below shows one of the most common transfer
functions, the logistic function (also sometimes referred to as
the sigmoid function, although strictly speaking it is only one
example of a sigmoid, S-shaped, function). In this case, the
output is in the range (0, 1), and the input is sensitive in a range
not much larger than (-1, +1). The function is also smooth
and easily differentiable, facts that are critical in allowing the
network training algorithms to operate (this is the reason why
the step function is not used in practice) [24].
The limited numeric response range, together with the fact
that information has to be in numeric form, implies that neural
solutions require pre-processing and post-processing stages to
be used in real applications [19].
C. Learning Algorithms
In order to produce the desired set of output states whenever
a set of inputs is presented to a neural network it has to be
configured by setting the strengths of the interconnections and
this step corresponds to the network learning procedure [24].
Learning rules are roughly divided into three categories of
supervised, unsupervised and reinforcement learning methods.
Fig. 2. Sigmoid activation function
In supervised learning, we are given a set of example pairs and
the aim is to find a function in the allowed class of functions
that matches the examples. In other words, we wish to infer
the mapping implied by the data; the cost function is related
to the mismatch between our mapping and the data and it
implicitly contains prior knowledge about the problem domain.
A commonly used cost is the mean-squared error, which tries
to minimize the average squared error between the network’s
output, f(x), and the target value y over all the example pairs
[13]. When one tries to minimize this cost using gradient
descent for the class of neural networks called multilayer
perceptrons, one obtains the common and well-known back-
propagation algorithm for training neural networks [33]. Tasks
that fall within the paradigm of supervised learning are pattern
recognition (also known as classification) and regression (also
known as function approximation). The supervised learning
paradigm is also applicable to sequential data (e.g., for speech
and gesture recognition). This can be thought of as learning
with a ”teacher”, in the form of a function that provides
continuous feedback on the quality of solutions obtained thus
far.
In unsupervised learning, some data is given and the cost
function to be minimized, that can be any function of the
data and the network’s output. The cost function is dependent
on the task (what we are trying to model) and our a priori
assumptions (the implicit properties of our model, its param-
eters and the observed variables). Tasks that fall within the
paradigm of unsupervised learning are in general estimation
problems; the applications include clustering, the estimation
of statistical distributions, compression and filtering [22].
In reinforcement learning, data are usually not given, but
generated by an agent’s interactions with the environment. At
each point in time, the agent performs an action and the envi-
ronment generates an observation and an instantaneous cost,
according to some (usually unknown) dynamics. The aim is
to discover a policy for selecting actions that minimizes some
measure of a long-term cost; i.e., the expected cumulative cost.
The environment’s dynamics and the long-term cost for each
policy are usually unknown, but can be estimated [13].
D. Committee Machines
As the base topology of artificial neural network committee
machines [25] with the feed-forward multilayer perceptron
with sigmoid activation function trained by back-propagation
algorithm is used. In committee machines approach, a complex
computational task is solved by dividing it into a number of
computationally simple tasks and then combining the solutions
to those tasks. In supervised learning, computational simplicity
is achieved by distributing the learning task among a number
of experts, which in turn divides the input space into a set
of subspaces. The combination of experts is said to constitute
a committee machine. Basically, it fuses knowledge acquired
by experts to arrive at an overall decision that is supposedly
superior to that attainable by anyone of them acting alone. The
idea of a committee machine may be traced back to Nilsson
[23]; the network structure considered therein consisted of
a layer of elementary perceptrons followed by a vote-taking
perceptron in the second layer[26]. Committee machines are
universal approximators. They may be classified into two
major categories: 1. Static structures. In this class of committee
machines, the responses of several predictors (experts) are
combined by means of a mechanism that does not involve
the input signal, hence the designation ”static.” This category
includes the following methods: Ensemble averaging, where
the outputs of different predictors are linearly combined to
produce an overall output. Boosting, where a weak learning
algorithm is converted into one that achieves arbitrarily high
accuracy. 2. Dynamic structures. In this second class of
committee machines, the input signal is directly involved in
actuating the mechanism that integrates the outputs of the
individual experts into an overall output, hence the designation
”dynamic.”
IV. DATA
A. Texts Used
In this research texts of famous Russian writers, Leo Tol-
stoy[28, 29] and Fyodor Dostoyevsky [30, 31, 32] were used.
Their novels provide the corpus which is wide enough to make
sure that characteristic features found based on the training
data can be treated as representative of other texts and this
generalized knowledge can be used to confirm or discount the
possibility of either of considered writers being recognised as
the author of a text of unknown origin.
Obviously literary texts can greatly vary in length and all
stylistic features can be influenced not only by different
timelines within which the text is written but also by its genre.
The first of these issues is easily dealt with by dividing long
texts, such as novels, into some number of smaller parts of
approximately the same size [20].
Described approach gives additional advantage in classification
tasks as even in case of some incorrect classification results
of these parts the whole text can still be properly attributed
to some author by based the final decision on the majority of
outcomes instead of all individual decisions for all samples.
Whether the genre of a novel is reflected in lexical and syn-
tactic characteristics of it is the question yet to be answered. If
the influence is significant, then lexical and syntactic features
cannot be used as the writer invariant as unreliable [8, 11]. On
the other hand, this can be rectified by including within the
training data set fragments of texts being representatives of not
only one but several genres. For intended implementation of
the classifier with Artificial Neural Networks, which efficiently
deal with large amount of data, adding samples to the training
set simply means better coverage of the input space that is
important in continuous case [27].
Hence in the training set there were included samples com-
ing from ”War and Peace” [28] by Leo Tolstoy and Crime
and Punishment [30] from Dostoyevsky. Altogether we have
selected over 3000 paragraphs coming from different four
books from these authors, two books for each. For the testing
purposes we have used same books as well as the ones that
were not used for training.
B. Features Extraction
Establishing features that work as effective discriminators
of texts under study is one of critical issues in research on
authorship analysis which are both lexical.
In this research twenty textual descriptors are used, namely:
characters without space, characters with spaces count, number
of words, number of sentences, commas, dots, and count, or
count, a count, an count, the count, in count, on count, to
count, of count, it count, that count, as count, is count and so
count in paragraphs. The descriptive statistics for these textual
descriptors are as in Table I.
TABLE I
PARAGRAPH AVERAGES AND VARIANCES OF THE TEXTUAL DESCRIPTORS
USED IN THIS RESEARCH
Fyodor Dostoyevsky Leo Tolstoy
Textual descriptors Mean Variance Mean Variance
Char 260.509 478101 198.281 860499
Char with spaces 317.511 722496 240.967 89307.5
Word count 58.0018 25164.4 42.7109 2810.61
Sentence count 5.58909 99.5908 3.48273 15.6949
Comma count 4.64182 157.413 3.24182 16.2927
dots count 0.34545 1.69311 0.13545 0.36835
and count 2.20636 52.4951 1.54545 6.03706
or count 0.13363 0.23963 0.08545 0.13645
a count 1.43091 14.8060 0.83454 1.84157
an count 0.16181 0.46514 0.13272 0.17345
the count 2.64182 36.8652 2.29091 14.1373
in count 0.98181 8.61295 0.62090 1.46216
on count 0.49636 1.81163 0.28272 0.51598
to count 1.33364 26.1352 1.22091 3.78282
of count 1.09636 14.6185 0.86909 2.06383
it count 0.85454 6.28092 0.37636 0.62619
that count 0.68636 8.65404 0.47545 0.96846
as count 0.36545 1.69708 0.32454 0.54152
is count 0.41090 4.59533 0.34000 0.70322
so count 0.23545 0.98272 0.16363 0.24617
As it can be seen, there is statistical difference between the
use of textual descriptors, for instance, Dostoyevsky prefers
longer paragraphs. In average Dostoyevskys paragraphs con-
tain 58 words with variance 25.166, while Tolstoys average is
43 with variance of 2.810. Our neural networks will capture
this pattern during the training phase, and use this information
to classify the paragraphs in the test data.
V. RESULTS AND DISCUSSION
For training purposes 500 samples are used from some other
parts of the same works of both writers Set 1 of data consists of
lexical descriptors from 250 paragraphs are randomly selected
from both novels of each author. N1=250 is the number of data
to train the first machine of the committee which has two input
terminals, eight hidden neurons in one hidden layer. Once
training stage is finished 250 data points from the different
texts, same authors, are selected and used for testing purposes
that will give us the date on correct classification. The results
of classification performed at the end of testing process by
this network machine with the books that were not used in
training are given in the Table II.
TABLE II
FIRST COMMITTEE MACHINE CORRECT CLASSIFICATIONS
Data Points Correctly Classified Classification %
Dostoyevsky 250 228 91.2
Tolstoy 250 234 93.6
Total 500 462 92.4
As for the second committee machine another set N2 of
250 paragraph descriptors are randomly taken from the sample
of 500 data originally taken. Than training process is done
for the second machine and weights are obtained. Same as
with the committee machine one, 250 data points are taken
from the different texts of the same authors and prepared
for testing. Weights that are obtained in the training process
are than applied on to the testing data to obtain classification
percentages. The results of classification performed at the end
of testing by this second network machine with the books that
were not used in training are given in the Table III.
TABLE III
SECOND COMMITTEE MACHINE CORRECT CLASSIFICATIONS
Data Points Correctly Classified Classification %
Dostoyevsky 250 216 86.4
Tolstoy 250 235 95.6
Total 500 455 91
For the third committee machine another set, N3, of 250
paragraph descriptors are randomly chosen from the sample
of 500 data originally taken. Than training process is done
for the second machine and weights are obtained. Same as
with the committee machine one, 250 data points are taken
from the different texts of the same authors and prepared for
testing. Weights obtained in the training process are used on
to the testing data to obtain classification percentages. The
results of classification performed at the end of testing by this
second network machine with the books that were not used in
training are given in the Table IV.
Although personal success rate for Tolstoy is low in the
TABLE IV
THIRD COMMITTEE MACHINE CORRECT CLASSIFICATIONS
Data Points Correctly Classified Classification %
Dostoyevsky 250 248 99.2
Tolstoy 250 234 65.2
Total 500 163 82.2
third machine, seemingly the second machine is an expert for
Tolstoy, and the third machine is Dostoyevsky expert compared
to other machines in the committee.
A. Combining Results
To combine the results, we ensemble decisions of each
machine simply taking the average of the decisions of the
three experts. Committee performance in classifying the test
data is given at the last column of Table V.
TABLE V
PARAGRAPH CORRECT CLASSIFICATIONS AFTER COMBINING RESULTS OF
ALL COMMITTEE MACHINES
Data Points Correctly Classified Classification %
Dostoyevsky 250 233 93.2
Tolstoy 250 233 93.2
Total 500 466 93.2
As it is seen from Table 5, the committee success is
satisfactory, 93.2% of the paragraphs in the test data authored
by Dostoyevsky are correctly identified. The same percentage
93.2% of the paragraphs authored by Tolstoy is identified
correctly as well. Overall correct classification probability is
high enough, 93.2%. There is 6.8% of misclassification, 17
out of 250 paragraphs of Dostoyevsky and 17 out of 250
paragraphs of Tolstoy are identified incorrectly.
VI. CONCLUSION
This paper concerning author identification analysis shows
how efficient a Artificial Neural Networks can be when applied
in classification tasks. Yet conclusions as to the choice of
textual descriptors used as features for recognition process,
based only on results presented in the previous section and
leading to some arbitrary statement that syntactic attributes are
more effective in authorship attribution, would be much too
hasty and premature. Undeniably true in the studied example,
it would have to be verified against much wider corpora as for
other writers other features could give better results.
Thus a series of future experiments should include applica-
tion of the presented here artificial neural networks -based
methodology to wider range of authors, definition of new sets
of textual descriptors, and test for other types and structures
of neural networks, and search the possibility of inheritance
through translation into other languages.
REFERENCES
[1] A. Genkin, D. D.Lewis, and D. Madigan, Large-scale bayesian logistic
regression for text categorization, 2004.
[2] B. Diri and M. F. Amasyal, Automatic Author Detection for Turkish
Text, ICANN/ICONIP’03 13th International Conference on Artificial
Neural Network and 10th International Conference on Neural Information
Processing, 2003.
[3] B. Kessler, G. Nunberg and H. Schutze, Automatic Detection of Text
Genre, Proc. of 35th Annual Meeting of the Association for Computa-
tional Linguistics (ACL/EACL’97), 32-38 1997
[4] C. Callison-Burch, Co-training for Statistical Machine Translation,
Master’s thesis, University of Edinburgh, 2002.
[5] C. D. Manning and H. Schutze, Foundations of Statistical Natural
Language Processing, The MIT Press, 1999.
[6] D. Biber, Variations Across Speech and Writing, Cambridge University
Press, 1988.
[7] D. I. Holmes, Stylometry: Its Origins, Development and Aspirations,
presented to the Joint International Conference of the Association for
Computers and the Humanities and the Association for Literary and
Linguistic Computing, Queens University, Kingston, Ontario, 1997.
[8] D. Khmelev, Disputed authorship resolution using relative entropy for
markov chain of letters in a text, In R. Baayen, editor, 4th Conference
Int. Quantitative Linguistics Association, Prague, 2000.
[9] E. Stamatatos, N. Fakotakis, G. Kokkinakis, Automatic Text Catego-
rization in Terms of Genre and Author, Computational Linguistics, pages
471-495, 2000.
[10] F. J. Tweedie, S. Singh, D. I. Holmes, Neural Network Applications
in Stylometry: The Federalist Paper, Computers and the Humanities, Vol.
30, pages 1-10, 1996.
[11] H. Baayen, H. van Halteren, and F. Tweedie, Outside the cave of
shadows: Using syntactic annotation to enhance authorship attribution.
Literary and Linguistic Computing, Vol.11(3), pages 121-131, 1996.
[12] J. Allen, Natural Language Understanding, Benjamin/Cummings Pub.
Co., Redwood City, California, 1995.
[13] J. Goldsmith, Unsupervised learning of the morphology of a natural
language, Computational Linguistics, Vol. 27(2), pages 153198, 2001.
[14] J. Karlgren, and D. Cutting, Recognizing Text Genres with Simple Met-
rics using Discriminant Analysis, Proceedings of the 15th. International
Conference on Computational Linguistics, Kyoto, 1994.
[15] J. M. Farringdon, Analyzing for Authorship: A Guide to the Cusum
Technique. University of Wales Press, 1996.
[16] M. Creutz, Unsupervised segmentation of words using prior distribu-
tions of morph length and frequency, In Proceedings of the 41st Annual
Meeting of the Association for Computational Linguistics (ACL), pages
280287, Sapporo, Japan, 2003.
[17] R. A. Bosch, J. A. Smith, Separating Hyperplanes and the Authorship
of the Disputed Federalist Papers, American Mathematical Monthly,
Volume 105, pages 601-608, 1998.
[18] R. Durbin, C. Miall, G. Mitchison, On the Correspondence Between
Network Models and the Nervous System, in The Computing Neuron,
Addison-Wesley Publishing Co. 1989
[19] C. Bishop. Neural Networks for Pattern Recognition. Oxford: Univer-
sity Press. 1995.
[20] S. Argamon-Engelson, M. Koppel, and G. Avneri, Style-based text
categorization: What newspaper am I reading?, In Proc. AAAI Workshop
on Learning for Text Categorization, pages 1-4, 1998.
[21] T. Mendenhall, The characteristic curves of composition, Science,
214:237249, 1887.
[22] S. Haykin, Neural Networks A Comprehensive Foundation, Second
Edition, Prentice-Hall, Inc. Simon & Schuster,A Viacom Company Upper
Saddle River, New Jersey 07458, 1999.
[23] N. J. Nilsson, Learning Machines: Foundations of Trainable Pattern-
Classifying Systems, New York: Mcffraw-Hill, 1965.
[24] R. E. Schapire,The strength of weak learnability, Machine Learning, vol.
5, pp.197-227, 1990.
[25] R. E. Schapire, Using output codes to boost multiclass learning
problems, Machine Learning: Proceedings of the Fourteenth International
Conference, 1997, Nashville, TN.
[26] H. Drucker, C. Cortes, L.D. Jackel, and Y. LeCun, Boosting and other
ensemble methods. Neural Computation, vol. 6, pp.1289-1301, 1994
[27] H. Drucker, R.E. Schapire, and P. Simard, Improving performance
in neural networks using a boosting algorithm, Advances in Neural
Information Processing Systems, vol. 5, pp. 42–49, 1993, Cambridge,
MA: MIT Press.
[28] L. Tolstoy, War and Crime
[29] L. Tolstoy, The Death of Ivan Ilyich
[30] F. Dostoyevsky, Crime and Punishment
[31] F. Dostoyevsky, The Possessed
[32] F. Dostoyevsky, The Brothers Karamazov
