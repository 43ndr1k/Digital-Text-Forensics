International Symposium on Performance Science © The Author 2011, Published by the AEC 
ISBN 978-94-90306-02-1 All rights reserved 
Perceiving individuality in musical 
performance: Recognizing harpsichordists 
playing different pieces 
 
 
Réka Koren1 and Bruno Gingras2,3 
 
1 Department of Psychology, Goldsmiths, University of London, UK 
2 Department of Computing, Goldsmiths, University of London, UK 
3 Schulich School of Music, McGill University, Canada 
 
 
The present study aimed to test whether listeners are able to distinguish 
between unfamiliar performers playing two different, unfamiliar pieces 
on the harpsichord. Recordings of two different Baroque pieces by six 
professional harpsichordists were used in this test. Twenty musicians 
and twenty non-musicians, with ten men and ten women in each group, 
participated in the experiment. Most of the participants performed sig-
nificantly better than chance, demonstrating that there was sufficient 
information in the excerpts to recognize the performance characteristics 
of any given performer. The grouping accuracy of musicians was signifi-
cantly higher than that of non-musicians. Moreover, grouping accuracy 
was significantly different between both pieces, suggesting that their 
features differed in a way which rendered one of them more easily recog-
nizable. 
 
Keywords: musical performance; characteristics; recognition; categori-
zation; musical expertise 
 
 
Identity can be recognized in many aspects of human life. One of these as-
pects is facial recognition. People seem to be capable of perceiving the unique 
identity of a virtually unlimited number of faces (Haxby et al. 2000). They are 
also able to recognize themselves and others in a dynamic display of their 
movements (Johansson 1973, Loula et al. 2005). In the auditory domain, 
various studies have been conducted on clapping recognition. Repp (1987) 
found that people recognized clappers significantly better than chance and 
that they had an even better rate for self-recognition in clapping. Voice recog-
nition studies showed that recognizing the voices of famous people is a rela-
474 WWW.PERFORMANCESCIENCE.ORG 
tively easy task and that even backward voice samples can be identified (Van 
Lancker et al. 1985). These studies provide good evidence of our ability to 
perceive identity cues across different perceptual modalities. 
Musical performance can also convey cues about the performer’s identity. 
Indeed, playing a well-known repertoire piece in a recognizably different way 
from another performance of the same piece can show a musician’s unique 
personality and character (Lehmann et al. 2007). Although a handful of 
studies have investigated whether humans are able to process identity cues in 
music performance, none of them veritably focused on whether people can 
recognize a performer playing two different pieces. The present study aims to 
make this question its point of focus. Stamatatos and Widmer (2005) pre-
sented a computational approach to this issue and found that after having 
been trained on a set of Chopin pieces, learning ensembles were able to iden-
tify the same performers playing a different piece from Chopin. Furthermore, 
a previous study by Gingras et al. (2008) reported that most listeners were 
able to distinguish between unfamiliar performers playing excerpts from a 
single unfamiliar piece on the organ. The present study differs from the study 
by Gingras et al. mainly in its use of two different pieces instead of a single 
one. 
 
METHOD 
Participants 
Forty participants completed the experiment. In order to investigate a poten-
tial relationship between the listeners’ musical background and their per-
formance in the task, 20 musicians (defined as people who have completed at 
least one year of musical training at university level), and 20 non-musicians 
(defined as people who have had less than two years of musical training of 
any kind) were selected to participate in the experiment. Both groups were 
equally balanced for gender, giving 10 males and 10 females in each group. 
Participants received a gift for their contribution and the chance to win one 
out of four prizes worth £20 GBP each. 
 
Materials 
Two musical pieces were used for the experiment: Les Bergeries (rondeau; 
from Pièces de clavecin, Book II, 6ème ordre) by Couperin (1583-1643), and 
Partita No. 12 sopra l’aria di Ruggiero (Variation III) by Frescobaldi (1668-
1733). Both were performed by three prize-winning and three non-prize-
winning harpsichordists on the same harpsichord which was equipped with a 
INTERNATIONAL SYMPOSIUM ON PERFORMANCE SCIENCE 475 
MIDI console. The pieces were recorded (each of them at least twice) in 
Montreal, Canada, in 2008. For each piece, an excerpt of 10-14 s (depending 
on the performance) and corresponding to a syntactically coherent musical 
unit was chosen. Excerpts were edited to retain a sense of closure. Audacity 
was used for cutting out and editing the pieces. 
 
Procedure 
Before the actual task, the participants’ hearing was tested in a soundproof 
booth, using a manually operated Amplivox 2160 pure tone diagnostic audi-
ometer and following a standardized procedure for the measurement of 
hearing thresholds. Participants who passed the hearing test were allowed to 
continue the experiment. The main experiment was a computer-based task, 
carried out using a software interface within the Matlab environment. Before 
the actual experiment, participants had the opportunity to practice the task. 
In the main experiment, participants were presented with six colored boxes 
on the left side of the screen, each of them representing a performer. On the 
right side of the screen 24 icons represented the musical excerpts, each of 
them identified by random numbers. Participants were asked to group to-
gether the excerpts to which they believed to have been played by the same 
performer. They were able to listen to the excerpts by double-clicking on the 
icons. They had to listen to the excerpts at least once before being able to drag 
them into the boxes on the left side. At the end of the experiment they were 
required to listen to the content of each box before finishing the experiment. 
There was no time limit for the categorization but the time taken to arrange 
the selections was recorded. After finishing the computer-based main task, 
participants were asked to complete a questionnaire in order to ascertain 
information about their musical background, as well as the strategies they 
had used for completing the task. 
 
RESULTS 
The listeners’ categorization was coded as a co-occurrence matrix. Their 
grouping accuracy was obtained by comparing their actual grouping to the 
correct grouping of the excerpts (when all the ones that are played by the 
same performer are grouped together). This number was evaluated by com-
puting adjusted Rand index values which is a widely used statistical tool to 
measure the similarity between two data clusterings. The number obtained 
this way is the score. The overall (grand) mean score for all the participants 
was M=0.17 (see Table 1). 
476 WWW.PERFORMANCESCIENCE.ORG 
Table 1. Participants’ mean scores and standard deviations. 
 
 Musicians Non-musicians 
Men 0.24 (SD=0.14) 0.11 (SD=0.07) 
Women 0.23 (SD=0.12) 0.10 (SD=0.09) 
 
Of 40 participants, 39 managed to perform better than chance (had a 
score better than 0). Overall, 26 participants (65%) performed significantly 
better on the task than chance level (p<0.05). There was no significant differ-
ence in scores between men and women (t38=0.28, p>0.05), but musicians 
(M=0.23, SE=0.03) performed significantly better on the task than did non-
musicians (M=0.10, SE=0.02). The results also showed no significant differ-
ence between the numbers of correctly grouped pairs in the case of prize-
winner and non-prize-winner performers (F1,36=0.88, p>0.05), indicating 
that there was no effect of the performers’ expertise on the grouping accuracy. 
To examine whether there was a difference in participants’ grouping accu-
racy between the two pieces, we looked at the number of the pairs of excerpts 
which they matched correctly. The repeated-measures analysis of variance 
(ANOVA) showed a significant difference between the two pieces 
(F1,36=23.82, p<0.01). Participants showed better accuracy for grouping the 
excerpts of the Couperin than for the Frescobaldi. In order to explain this 
difference in accuracy, we looked at the tempo and found that there were 
significantly greater tempo differences between the two recordings of the 
Couperin (r=0.46, p>0.05) than in the case of the Frescobaldi (r=-0.38, 
p>0.05), negating the possibility that smaller tempo differences were respon-
sible for the greater accuracy in the Couperin piece. 
Finally, we also compared the grouping accuracy for pairs of excerpts 
from the same piece versus pairs containing one excerpt from each piece. A 
significant difference was found (p<0.05) between the grouping accuracy for 
pairs comprising two excerpts from the Couperin (M=53.33, SE=3.07) and 
pairs comprising one excerpt from each piece (M=20.42, SE=3.91), although 
no significant difference was found for the Frescobaldi. There was also a sig-
nificant difference between musicians’ and non-musicians’ correct grouping 
of the excerpts from the same piece and the excerpts from different pieces. In 
the case of musicians there was also a significant difference (p<0.05) between 
the grouping accuracy for pairs with both excerpts from the Couperin 
(M=65.83, SE=2.39) and pairs with one excerpt from each piece (M=22.29, 
SE=5.34). This difference was not significant for non-musicians. 
 
INTERNATIONAL SYMPOSIUM ON PERFORMANCE SCIENCE 477 
DISCUSSION 
There have been very few studies which have investigated the ability of hu-
mans to process identity cues in music performance. To our knowledge this is 
the first study that examines whether people are able to recognize the same 
performer playing two different pieces. Although most people reported the 
task as being very difficult, everyone managed to complete the task at a level 
better than chance with the exception of one participant. In both this study 
and in Gingras et al. (2008), musicians performed better than non-musicians 
on the given task, but in the present case the difference between the two 
groups was significant, suggesting that musical training has an important role 
in recognizing personal characteristics in a short musical excerpt. 
Gingras et al. (2008) found a significant effect of performers’ expertise on 
the grouping accuracy, something which we also expected to find. However, 
our results did not show any significant effect of performers’ expertise. Since 
participants’ performance was significantly better for one piece over the 
other, it would appear that the musical features of a piece are important in 
picking up on its characteristics and thus in recognizing the identity of the 
performer. These results let us suggest that the performer’s expertise is not as 
relevant in grouping the excerpts as are the characteristics of the pieces. 
To investigate why the Couperin piece was sorted more accurately, we ran 
several analyses on the MIDI data. As one possibility we examined tempo 
differences within the same piece (between the two recordings) and between 
the different pieces and found that the tempo differences in the Couperin 
were significantly greater than in the Frescobaldi. Although this should make 
recognizing the matching pairs more difficult in the Couperin piece, that is 
not what we observed. Another explanation for the better grouping accuracy 
for the Couperin may be related to the duration of the excerpts, which were 
on average a few seconds longer than those for the Frescobaldi. 
We were also interested in examining whether listeners fared better in 
matching excerpts from the same piece (either the Couperin or the 
Frescobaldi piece) or excerpts from different pieces but played by the same 
performer. A significant difference was found between grouping excerpts 
from the Couperin piece and grouping excerpts from different pieces by the 
same performer. Listeners showed notably better grouping accuracy in the 
first case. This result indicates that participants were not as successful in 
matching the excerpts from the different pieces played by the same per-
former, although they still performed better than chance. 
In conclusion, this experiment yielded intriguing results in a domain of 
identity perception which has yet to be fully understood. Gingras et al. (2008) 
478 WWW.PERFORMANCESCIENCE.ORG 
provided a useful reference point as both studies share some common fea-
tures. Although some findings differed between the two studies, such as the 
lack of effect of performer’s expertise in the present study, they show globally 
similar results. Both of them showed that people are able to recognize a per-
former’s individual characteristics in musical excerpts and that musically 
trained listeners generally showed a better grouping accuracy. 
 
Acknowledgments 
We wish to thank Alain Poirier, Director of the Conservatoire National Supérieur de 
Musique et de Danse de Paris, for the loan of the MIDI harpsichord. 
 
Address for correspondence 
Réka Koren, Hunyadi János utca 29/B, Budapest 1028, Hungary; Email: koren.reka@ 
gmail.com 
 
References 
Gingras B., Lagrandeur-Ponce T., Giordano B. L., and McAdams S. (2008). Effect of 
expressive intent, performer expertise, and listener expertise on the perception of 
artistic individuality in organ performance. In K. Miyazaki, M. Adachi, Y. Hiraga et 
al. (eds.), Proceedings of the 10th International Conference on Music Perception 
and Cognition (pp. 10-14). Sapporo, Japan: Japanese Society for Music Perception 
and Cognition. 
Haxby J. V., Hoffman E. A., and Gobbini M. I. (2000). The distributed human neural 
system for face perception. Trends in Cognitive Science, 4, pp. 223-233. 
Johansson G. (1973). Visual perception of biological motion and a model for its analysis. 
Perception and Psychophysics, 14, pp. 201-211. 
Lehmann A. C., Sloboda J. A., and Woody R. H. (2007). Psychology for Musicians. 
Oxford: Oxford University Press.  
Loula F., Prasad S., Harber K., and Shiffrar M. (2005). Recognizing people from their 
movement. Journal of Experimental Psychology: Human Perception and Per-
formance, 31, pp. 210-220. 
Repp B. H. (1987). The sound of two hands clapping: An exploratory study. Journal of 
the Acoustical Society of America, 81, pp. 1100-1109. 
Stamatatos E. and Widmer G. (2005). Automatic identification of musical performers 
with learning ensembles. Artificial Intelligence, 165, pp. 37-56.  
Van Lancker D., Kreiman J., and Emmorey K. (1985). Familiar voice recognition: Pat-
terns and parameters. Part I: Recognition of backward voices. Journal of Phonetics, 
13, pp. 19‐38. 
