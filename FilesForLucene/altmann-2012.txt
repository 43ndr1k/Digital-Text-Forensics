On the origin of long-range correlations in texts
Eduardo G. Altmann,1 Giampaolo Cristadoro,2 and Mirko Degli Esposti2
1Max Planck Institute for the Physics of Complex Systems, 01187 Dresden, Germany
2Dipartimento di Matematica, Università di Bologna, 40126 Bologna, Italy
The complexity of human interactions with social and natural phenomena is mirrored in the way
we describe our experiences through natural language. In order to retain and convey such a high
dimensional information, the statistical properties of our linguistic output has to be highly correlated
in time. An example are the robust observations, still largely not understood, of correlations on
arbitrary long scales in literary texts. In this paper we explain how long-range correlations flow from
highly structured linguistic levels down to the building blocks of a text (words, letters, etc..). By
combining calculations and data analysis we show that correlations take form of a bursty sequence
of events once we approach the semantically relevant topics of the text. The mechanisms we identify
are fairly general and can be equally applied to other hierarchical settings.
Published as: Proc. Nat. Acad. Sci. USA (2012) doi: 10.1073/pnas.1117723109
Literary texts are an expression of the natural language
ability to project complex and high-dimensional phenom-
ena into a one-dimensional, semantically meaningful se-
quence of symbols. For this projection to be successful,
such sequences have to encode the information in form
of structured patterns, such as correlations on arbitrarily
long scales [1, 2]. Understanding how language processes
long-range correlations, an ubiquitous signature of com-
plexity present in human activities [3–7] and in the nat-
ural world [8–11], is an important task towards compre-
hending how natural language works and evolves. This
understanding is also crucial to improve the increasingly
important applications of information theory and statis-
tical natural language processing, which are mostly based
on short-range-correlations methods [12–15].
Take your favorite novel and consider the binary se-
quence obtained by mapping each vowel into a 1 and all
other symbols into a 0. One can easily detect structures
on neighboring bits, and we certainly expect some repe-
tition patterns on the size of words. But one should cer-
tainly be surprised and intrigued when discovering that
there are structures (or memory) after several pages or
even on arbitrary large scales of this binary sequence.
In the last twenty years, similar observations of long-
range correlations in texts have been related to large
scales characteristics of the novels such as the story be-
ing told, the style of the book, the author, and the lan-
guage [1, 2, 6, 8, 16, 19, 20, 22]. However, the mechanisms
explaining these connections are still missing (see Ref. [2]
for a recent proposal). Without such mechanisms, many
fundamental questions cannot be answered. For instance,
why all previous investigations observed long-range corre-
lations despite their radically different approaches? How
and which correlations can flow from the high-level se-
mantic structures down to the crude symbolic sequence
in the presence of so many arbitrary influences? What
information is gained on the large structures by looking
at smaller ones? Finally, what is the origin of the long-
range correlations?
In this paper we provide answers to these questions
by approaching the problem through a novel theoretical
framework. This framework uses the hierarchical organi-
zation of natural language to identify a mechanism that
links the correlations at different linguistic levels. As
schematically depicted in Fig. 1, a topic is linked to sev-
eral words that are used to describe it in the novel. At
the lower level, words are connected to the letters they
are formed, and so on. We calculate how correlations are
transported through these different levels and compare
the results with a detailed statistical analysis in ten dif-
ferent novels. Our results reveal that while approaching
semantically relevant high-level structures, correlations
unfold in form of a bursty signal. Moving down in lev-
els, we show that correlations (but not burstiness) are
preserved, explaining the ubiquitous appearance of long-
range correlations in texts.
e    t   .. i   ..    p ..  r ..
the  ...    prince ... 
V         C
TOPIC 1 TOPIC 2
...
...
...
...
...
le
ve
l
up
(high)
down
(low)
Monday, May 14, 2012
FIG. 1: Hierarchy of levels at which literary texts can be
analyzed. Depicted are the levels vowels/consonants (V/C),
letters (a-z), words, and topics.
ar
X
iv
:1
20
7.
06
58
v1
  [
ph
ys
ic
s.
da
ta
-a
n]
  3
 J
ul
 2
01
2
2
I. THEORETICAL FRAMEWORK
A. The importance of the observable
In line with information theory, we treat a literary text
as the output of a stationary and ergodic source that
takes values in a finite alphabet and we look for infor-
mation about the source through a statistical analysis
of the text [23]. Here we focus on correlations func-
tions, which are defined after specifying an observable
and a product over functions. In particular, given a sym-
bolic sequence s (the text), we denote by sk the symbol
in the k-th position and by smn (m ≥ n) the substring
(sn, sn+1, . . . , sm). As observables, we consider functions
f that map symbolic sequences s into a sequence x of
numbers (e.g., 0’s and 1’s). We restrict to local map-
pings, namely xk = f(s
k+r
k ) for any k and a finite con-
stant r ≥ 0. Its autocorrelation function is defined as:
Cf (t) := 〈f(si+ri )f(s
i+t+r
i+t )〉 − 〈f(s
i+r
i )〉〈f(s
i+t+r
i+t )〉, (1)
where t plays the role of time (counted in number of
symbols) and 〈·〉 denotes an average over sliding windows,
see Supporting Information (SI) Sec. I for details.
The choice of the observable f is crucial in determin-
ing whether and which “memory” of the source is being
quantified. Only once a class of observables sharing the
same properties is shown to have the same asymptotic
autocorrelation, it is possible to think about long-range
correlations of the text as a whole. In the past, differ-
ent kinds of observables and encodings (which also cor-
respond to particular choices of f) were used, from the
Huffmann code [25], to attributing to each symbol an
arbitrary binary sequence (ASCII, unicode, 6-bit tables,
dividing letters in groups, etc.) [1, 16, 20, 26, 27], to the
use of the frequency-rank [7] or parts of speech [19] on the
level of words. While the observation of long-range corre-
lations in all cases points towards a fundamental source,
it remains unclear which common properties these ob-
servables share. This is essential to determine whether
they share a common root (conjectured in Ref. [1]) and
to understand the meaning of quantitative changes in the
correlations for different encodings (reported in Ref. [16]).
In order to clarify these points we use mappings f that
avoid the introduction of spurious correlations. Inspired
by Voss [11] and Ebeling et al. [6, 8][40] we use fα’s that
transform the text into binary sequences x by assigning
xk = 1 if and only if a local matching condition α is
satisfied at the k-th symbol, and xk = 0 otherwise (e.g.,
α = k-th symbol is a vowel). See SI-Sec. II for specific
examples.
B. Correlations and burstiness
Once equipped with the binary sequence x associ-
ated with the chosen condition α we can investigate the
asymptotic trend of its Cx(t). We are particularly inter-
ested in the long-range correlated case
Cx(t) := 〈xjxj+t〉 − 〈xj〉〈xj+t〉 ' t−β , 0 < β < 1,
(2)
for which
∑∞
t=0 C(t) diverges. In this case the asso-
ciated random walker X(t) :=
∑t
j=0 xj spreads super-
diffusively as [11, 29]
σ2X(t) := 〈X(t)2〉 − 〈X(t)〉2 ' tγ , γ = 2− β. (3)
In the following we investigate correlations of the bi-
nary sequence x using Eq. (3) because integrated indica-
tors lead to more robust numerical estimations of asymp-
totic quantities [1, 8, 10, 11]. We are mostly interested in
the distinction between short- (β > 1, γ = 1) and long-
(0 < β < 1, 1 < γ < 2) range correlations. We use
normal (anomalous) diffusion of X interchangeably with
short- (long-) range correlations of x.
An insightful view on the possible origins of the long-
range correlations can be achieved by exploring the rela-
tion between the power spectrum S(ω) at ω = 0 and the
statistics of the sequence of inter-event times τi’s (i.e.,
one plus the lengths of the cluster of 0’s between consec-
utive 1’s). For the short-range correlated case, S(0) is
finite and given by [30, 31]:
S(0) =
σ2τ
〈τ〉3
(
1 + 2
∑
k
Cτ (k)
)
. (4)
For the long-range correlated case, S(0)→∞ and Eq. (4)
identifies two different origins: (i) burstiness measured as
the broad tail of the distribution of inter-event times p(τ)
(divergent στ ); or (ii) long-range correlations of the se-
quence of τi’s (not summable Cτ (k)). In the next section
we show how these two terms give different contributions
at different linguistic levels of the hierarchy.
C. Hierarchy of levels
Building blocks of the hierarchy depicted in Fig. 1
are binary sequences (organized in levels) and links be-
tween them. Levels are established from sets of seman-
tically or syntactically similar conditions α’s (e.g., vow-
els/consonants, different letters, different words, different
topics)[41]. Each binary sequence x is obtained by map-
ping the text using a given fα, and will be denoted by
the relevant condition in α. For instance, prince de-
notes the sequence x obtained from the matching con-
dition α : sk+7k = “ prince ”. A sequence z is linked to
x if for all j’s such that xj = 1 we have zj+r′ = 1, for
a fixed constant r′. If this condition is fulfilled we say
that x is on top of z and that x belongs to a higher level
than z. By definition, there are no direct links between
sequences at the same level. A sequence at a given level
is on top of all the sequences in lower levels to which
there is a direct path. For instance, prince is on top of e
which is on top of vowel. As will be clear later from our
3
results, the definition of link can be extended to have a
probabilistic meaning, suited for generalizations to high
levels (e.g., “ prince ” is more probable to appear while
writing about a topic connected to war).
D. Moving in the hierarchy
We now show how correlations flow through two linked
binary sequences. Without loss of generality we denote x
a sequence on top of z and y the unique sequence on top
of z such that z = x + y (sum and other operations are
performed on each symbol: zi = xi + yi for all i). The
spreading of the walker Z associated with z is given by
σ2Z(t) = σ
2
X(t) + σ
2
Y (t) + 2C(X(t), Y (t)), (5)
where C(A,B) = 〈AB〉−〈A〉〈B〉 is the cross-correlation.
Using the Cauchy-Schwarz inequality |C(X(t), Y (t))| ≤
σX(t)σY (t) we obtain
σZ(t) ≤ σX(t) + σY (t). (6)
Define x̄, as the sequence obtained reverting 0 ↔ 1 on
each of its elements x̄i = 1 − xi. It is easy to see
that if z = x + y then x̄ = z̄ + y. Applying the
same arguments above, and using that σX = σX̄ for
any x, we obtain σX(t) ≤ σZ(t) + σY (t) and similarly
σY (t) ≤ σZ(t) + σX(t). Suppose now that σ2i ' tγi with
i ∈ {X,Y, Z}. In order to satisfy simultaneously the
three inequalities above, at least two out of the three γi
have to be equal to the largest value maxi{γi}. Next we
discuss the implications of this restriction to the flow of
correlations up and down in our hierarchy of levels.
Up. Suppose that at a given level we have a binary
sequence z with long-range correlations γZ > 1. From
our restriction we know that at least one sequence x on
top of z, has long-range correlations with γX ≥ γZ . This
implies, in particular, that if we observe long-range cor-
relations in the binary sequence associated with a given
letter then we can argue that its anomaly originates from
the anomaly of at least one word where this letter ap-
pears, higher in the hierarchy[42].
Down. Suppose x is long-range correlated γX > 1.
From Eq. (10) we see that a fine tuning cancellation with
cross-correlation must appear in order for their lower-
level sequence z (down in the hierarchy) to have γZ < γX .
From the restriction derived above we know that this is
possible only if γX = γY , which is unlikely in the typical
case of sequences z receiving contributions from different
sources (e.g., a letter receives contribution from different
words). Typically, z is composed by n sequences x(j),
with γX(1) 6= γX(2) 6= . . . 6= γX(n) , in which case γZ =
maxj{γX(j)}. Correlations typically flow down in our
hierarchy of levels.
Finite-time effects. While the results above are valid
asymptotically (infinitely long sequences), in the case of
any real text we can only have a finite-time estimate γ̂ of
the correlations γ. Already from Eq. (10) we see that the
addition of sequences with different γX(j) , the mechanism
for moving down in the hierarchy, leads to γ̂Z < γZ if γ̂Z
is computed at a time when the asymptotic regime is
still not dominating. This will play a crucial role in our
understanding of long-range correlations in real books.
In order to give quantitative estimates, we consider the
case of z being the sum of the most long-range correlated
sequence x (the one with γX = maxj{γX(j)}) and many
other independent non-overlapping[43] sequences whose
combined contribution is written as y = ξ(1 − x), with
ξi an independent identically distributed binary random
variable. This corresponds to the random addition of 1’s
with probability 〈ξ〉 to the 0’s of x. In this case σ2Z shows
a transition from normal γ̂Z = 1 to anomalous γ̂Z = γX
diffusion. The asymptotic regime of z starts after a time
tT ≥
(
〈ξ〉
1− 〈ξ〉
1
g〈x〉
)1/(γX−1)
, (7)
where 0 < g ≤ 1 and γX > 1 are obtained from σ2X
which asymptotically goes as g〈x〉〈1 − x〉tγX . Note that
the power-law sets at t = 1 only if g = 1. A similar
relation is obtained moving up in the hierarchy, in which
case a sequence x in a higher level is built by random
subtracting 1’s from the lower-level sequence z as x = ξz
(see SI-Sec. III-A for all calculations).
Burstiness. In contrast to correlations, burstiness
due to the tails of the inter-event time distribution p(τ)
is not always preserved when moving up and down in the
hierarchy of levels. Consider first going down by adding
sequences with different tails of p(τ). The tail of the com-
bined sequence will be constrained to the shortest tail of
the individual sequences. In the random addition exam-
ple, z = x + ξ(1− x) with x having a broad tail in p(τ),
the large τ asymptotic of z has short-tails because the
cluster of zeros in x is cut randomly by ξ [32]. Going up
in the hierarchy, we take a sequence on top of a given
bursty binary sequence, e.g., using the random subtrac-
tion x = ξz mentioned above. The probability of finding
a large inter-event time τ in z is enhanced by the number
of times the random deletion merges two or more clusters
of 0’s in x, and diminished by the number of times the
deletion destroys a previously existent inter-event time τ .
Even accounting for the change in 〈τ〉, this moves cannot
lead to a short-ranged p(τ) for x if p(τ) of z has a long
tail (see SI-Sec. III-B). Altogether, we expect burstiness
to be preserved moving up, and destroyed moving down
in the hierarchy of levels.
Summary. From Eq. (4) the origin of long-range
correlations γ > 1 can be traced back to two differ-
ent sources: the tail of p(τ) (burstiness) and the tail of
Cτ (k). The computations above reveal their different role
at different levels in the hierarchy: γ is preserved moving
down, but there is a transfer of information from p(τ)
to Cτ (k). This is better understood by considering the
following simplified set-up: suppose at a given level we
observe a sequence x coming from a renewal process with
4
0 25 50 75τ
10
-4
10
-2
p(τ)
x
A1
 shuffled {0,1}
10
1
10
2
10
3
10
4
t
10
0
10
2
10
4
σ2
x
(t) 
x
10
1
10
2
10
3
10
4
10
5
τ
10
-4
10
-3
10
-2
10
-1
10
0
P(τ)
x
A2 
 shuffled τ
i
's
10
1
10
2
10
3
10
4
10
5
τ
10
-3
10
-2
10
-1
10
0
P(τ)
10
1
10
2
10
3
10
4
t
10
-2
10
0
10
2
σ2
x
(t) 
(a) (b)
t
1.
39
t
(d)(c)
t1
.6
8
tW
or
d 
  "
pr
in
ce
"
L
et
te
r 
  "
e"
 
Burstiness Correlation
στ/〈τ〉=3.86
στ/〈τ〉=0.83 γ=1.39
γ=1.68
^
^
FIG. 2: Burstiness and long-range correlation on different
linguistic levels. The binary sequences of the letter “e” (a,b)
and of the word “ prince ” (c,d) in the book “War and Peace”
are shown. (a,c) The cumulative inter-event time distribution
P (τ) ≡
∫ τ
0
p(t′)dt′. (b,d) Transport σ2X(t) defined in Eq. (3).
The numerical results show: (a) exponential decay of P (τ)
with στ/〈τ〉 = 0.83 Inset: p(τ) in log-linear scales; (b) γ̂ =
1.39± 0.05; (c) non-exponential decay of P (τ) with στ/〈τ〉 =
3.86; and (d) γ̂ = 1.68± 0.05. All panels show results for the
the original and A1, A2-shuffled sequences, see legend.
broad tails in the inter-event times
p(τ) ∼ τ−µ and Cτ (k) = δ(k), (8)
with 2 < µ < 3 leading to γX = 4 − µ [19]. Let us now
consider what is observed in z, at a level below, obtained
by adding to x other independent sequences. The long τ ’s
(a long sequence of 0’s) in Eq. (8) will be split in two
long sequences introducing at the same time a cut-off τc
in p(τ) and non-trivial correlations Cτ (k) 6= 0 for large k.
In this case, asymptotically the long-range correlations
(γZ = max{γX , γY } > 1) is solely due to Cτ (k) 6= 0.
Burstiness affects only γ̂ estimated for times t < τc. A
similar picture is expected in the generic case of a starting
sequence x with broad tails in both p(τ) and Cτ (k).
II. DATA ANALYSIS OF LITERARY TEXTS
Equipped with previous section’s theoretical frame-
work, here we interpret observations in real texts. We
use ten English versions of international novels (see SI-
Sec. IV for the list and for the pre-processing applied to
the texts). For each book 41 binary sequences were ana-
lyzed separately: vowel/consonants, 20 at the letter level
FIG. 3: Burstiness-correlation diagram for sequences at dif-
ferent levels. στ/〈τ〉 is an indicator of the burstiness of the
distribution p(τ). γ̂ is a finite time estimator of the global in-
dicator of long-range correlation γ. A Poisson process has
(στ/〈τ〉, γ) = (1, 1). The twenty most frequent symbols
(white circles) and twenty frequent words (black circles) of
wrnpc are shown (see SI-Tables for all books). V indicates
the case of vowels and B of blank space. The red dashed-
line is a lower-bound estimate of γ̂ due to burstiness (see
SI-Sec. VI). This diagram is a generalization for long-range
correlated sequences of the diagrams in Ref. [33].
(blank space and the 19 most frequent letters), and 20 at
the word level (6 most frequent words, 7 most frequent
nouns, and 7 words with frequency matched to the fre-
quency of the nouns). The finite-time estimator of the
long-range correlations γ̂ was computed fitting Eq. (3)
in a broad range of large t ∈ [ts′ , ts] (time lag of cor-
relations) up to ts = 1% of the book size. This range
was obtained using a conservative procedure designed to
robustly distinguish between short and long-range corre-
lations (see SI-Sec. V). We illustrate the results in our
longest novel, “War and Peace” by L. Tolstoy (wrnpc, in
short, see SI-Tables for the results in all books).
A. Data analysis of correlations and burstiness
One of the main goals of our measurements is to dis-
tinguish, at different hierarchy levels, between the two
possible sources of long-range correlations in Eq. (4) –
5
burstiness corresponding to p(τ) with diverging στ or di-
verging
∑
Cτ (k). To this end we compare the results
with two null-model binary sequences xA1,xA2 obtained
by applying to x the following procedures:
A1: shuffle the sequence of {0, 1}’s. Destroys all corre-
lations.
A2: shuffle the sequence of inter-event times τi’s. De-
stroys correlations due to Cτ (k) but preserves those
due to p(τ).
Starting from the lowest level of the hierarchy depicted
in Fig. 1, we obtain γ̂ = 1.55 ± 0.05 for the sequence
of vowels in wrnpc and γ̂ between 1.18 and 1.61 in the
other 9 books (see SI-Fig. S1). The values for xA1 and
xA2 were compatible (two error bars) with the expected
value γ = 1.0 in all books. Figures 2ab show the compu-
tations for the case of the letter “e”: while p(τ) decays ex-
ponentially in all cases (Fig. 2a), long-range correlations
are present in the original sequence e but absent from the
A2 shuffled version of e (Fig. 2b). This means that bursti-
ness is absent from e and does not contribute to its long-
range correlations. In contrast, for the word “ prince ”
Fig. 2c shows a non-exponential p(τ) and Fig. 2d shows
that the original sequence prince and the A2 shuffled
sequence show similar long-range correlations (black and
red curves, respectively). This means that the origin of
the long-range correlations of prince are mainly due to
burstiness – tails of p(τ) – and not to correlations in the
sequence of τi’s – Cτ (k).
In Fig. 3 we plot for different sequences the summary
quantities γ̂ and στ/〈τ〉 – a measure of the burstiness
proportional to the relative width of p(τ) [33, 34]. A
Poisson process has γ = στ/〈τ〉 = 1. All letters have
στ/〈τ〉 ≈ 1, but clear long-range correlations γ̂ > 1.1
(left box magnified in Fig. 3). This means that corre-
lations come from Cτ (k) and not from p(τ), as shown
in Fig. 2(a,b) for the letter “e”. The situation is more
interesting in the higher-level case of words. The most
frequent words and the words selected to match the nouns
mostly show στ/〈τ〉 ≈ 1 so that the same conclusions we
drew about letters apply to these words. In contrast to
this group of function words are the most frequent nouns
that have large στ/〈τ〉 [19, 21, 34, 35] and large γ̂, appear-
ing as outliers at the upper right corner of Fig. 3. The
case of “ prince ” shown in Fig. 2(c,d) is representative of
these words, for which burstiness contributes to the long-
range correlations. In order to confirm the generality of
Fig. 3 in the 10 books of our database, we performed a
pairwise comparison of γ̂ and στ/〈τ〉 between the 7 nouns
and their frequency matched words. Overall, the nouns
had a larger γ̂ in 56 and a larger στ/〈τ〉 in 55 out of the
70 cases (P-value < 10−6, assuming equal probability).
In every single book at least 4 out of 7 comparisons show
larger values of γ̂ and στ/〈τ〉 for the nouns.
We now explain a striking feature of the data shown
in Fig. 3: the absence of sequences with low γ̂ and high
στ/〈τ〉 (lower-right corner). This is an evidence of cor-
relation between these two indicators and motivates us
to estimate a στ/〈τ〉-dependent lower bound for γ̂, as
shown in Fig. 3. Note that high values of burstiness are
responsible for long-range correlations estimate γ̂ > 1,
as discussed after Eq. (8). For instance, the slow de-
cay of p(τ) for intermediate τ in prince (Fig. 2c) leads
to στ/〈τ〉  1 and an estimate γ̂ > 1 at intermediate
times. Burstiness contribution to γ̂ (which gets also con-
tributions from long-range correlations in the τi’s) is mea-
sured by γ̂A2, which is usually a lower bound for the total
long-range correlations: γ̂ ≥ γ̂A2. More quantitatively,
consider an A2-shuffled sequence with power-law p(τ) –
as in Eq. (8) – with an exponential cut-off for τ > τc.
By increasing τc we have that στ/〈τ〉 monotonously in-
creases [it can be computed directly from p(τ)]. In terms
of γ̂A2, if the fitting interval t ∈ [ts′ , ts] used to compute
the finite time γ̂A2 is all below τc (i.e. ts < τc) we have
γ̂A2 = 4− µ > 1 (see Eq. (8)) while if the fitting interval
is all beyond the cutoff (i.e. τc < ts′ ) we have γ̂A2 = 1.
Interpolating linearly between these two values and us-
ing µ = 2.4 we obtain the lower bound for γ̂ in Fig. 3.
It strongly restricts the range of possible (στ/〈τ〉, γ̂) in
agreement with the observations and also with γ̂ obtained
for the A2-shuffled sequences (see SI-Sec. VI for further
details).
B. Data analysis of finite-time effects
The pre-asymptotic normal diffusion – anticipated in
Sec. Finite-time effects – is clearly seen in Fig. 4.
Our theoretical model explains also other specific obser-
vations:
1. Key-words reach higher values of γ̂ than letters
(γ̂e < γ̂prince). This observation contradicts our expecta-
tion for asymptotic long times: prince is on top of e and
the reasoning after Eq. (10) implies γe ≥ γprince. This
seeming contradiction is solved by our estimate (17) of
the transition time tT needed for the finite-time estimate
γ̂ to reach the asymptotic γ. This is done imagining a
surrogate sequence with the same frequency of “e” com-
posed by prince and randomly added 1’s. Using the
fitting values of g, γ for prince in Eq. (17) we obtain
tT ≥ 6 105, which is larger than the maximum time ts
used to obtain γ̂. Conversely, for a sequence with the
same frequency of “ prince ” built as a random sequence
on top of e we obtain tT ≥ 7 108. These calculations
not only explain γ̂e < γ̂prince, they show that prince
is a particularly meaningful (not random) sequence on
top of e, and that e is necessarily composed by other se-
quences with 1 < γ < γ̂prince that dominate for shorter
times. More generally, the observation of long-range cor-
relations at low levels is due to widespread correlations
on higher levels.
2. The sharper transition for keywords. The addition of
many sequences with γ > 1 explains the slow increase in
γ̂(t) for letters because sequences with increasingly larger
γ dominate for increasingly longer times. The same rea-
soning explains the positive correlation between γ̂e and
6
the length of the book (Pearson Correlation r = 0.44,
similar results for other letters). The sequence so also
shows slow transition and small γ̂, consistent with the in-
terpretation that it is connected to many topics on upper
levels. In contrast, the sharp transition for prince indi-
cates the existence of fewer independent contributions on
higher levels, consistent with the observation of the onset
of burstiness στ/〈τ〉 > 1. Altogether, this strongly sup-
ports our model of hierarchy of levels with keywords (but
not function words) strongly connected to specific topics
which are the actual correlation carriers. The sharp tran-
sition for the keywords appears systematically roughly at
the scale of a paragraph (102 − 103 symbols), in agree-
ment with similar observation in Refs. [2, 20, 22, 36].
C. Data analysis of shuffled texts
Additional insights on long-range correlations are ob-
tained by investigating whether they are robust under
different manipulations of the text [2, 6]. Here we fo-
cus on two non-trivial shuffling methods (see SI-Sec. VII
for simpler cases for which our theory leads to analytic
results). Consider generating new same-length texts by
applying to the original texts the following procedures
M1 Keep the position of all blank spaces fixed and place
each word-token randomly in a gap of the size of
the word.
M2 Recode each word-type by an equal length random
sequence of letters and replace consistently all its
tokens.
Note that M1 preserve structures (e.g., words and letter
frequencies) destroyed by M2. In terms of our hierarchy,
M1 destroys the links to levels above word level while
M2 shuffles the links from word- to letter-levels. Since
according to our picture correlations originate from high
level structures, we predict that M1 destroys and M2
preserves long-range correlations. Indeed simulations un-
equivocally show that long-range correlations present in
the original texts (average γ̂ of letters in wrnpc 1.40±0.09
and in all books 1.26 ± 0.11) are mostly destroyed by
M1 (1.10 ± 0.08 and 1.07 ± 0.08) and preserved by M2
(1.33± 0.08 and 1.20± 0.09 (see SI-Tables for all data).
At this point it is interesting to draw a connection to
the principle of the arbitrariness of the sign, according
to which the association between a given sign (e.g., a
word) and the referent (e.g., the object in the real world)
is arbitrary [37]. As confirmed by the M2 shuffling, the
long-range correlations of literary texts are invariant un-
der this principle because they are connected to the se-
mantic of the text. Our theory is consistent with this
principle.
10
1
10
2
10
3
10
4
t
1
1.2
1.4
1.6
1.8
γ (t) 
10
1
10
2
10
3
10
4
t
1.2
1.6
γ (t)
"e" γ = 1.39 ± 0.04
" so " γ = 1.14 ± 0.03
" prince " γ = 1.68 ± 0.06
^
^
^
^
^
FIG. 4: Transition from normal to anomalous behav-
ior. The time dependent exponent is computed as γ̂(t) ≡
∆ log σ2X(t)/∆ log t (local derivative of the transport curve in
Fig. 2bd). Results for three sequences in wrnpc are shown
(from top to bottom): the noun “ prince ”, the most frequent
letter “e”, and the word “ so ” (same frequency of“ prince
”). The horizontal lines indicate the γ̂, the error bars, and
the fitting range. Inset (from top to bottom): the 4 other
nouns appearing as outliers in Fig. 3, the 4 most frequent let-
ters after “e”, and the 4 words matching the frequency of the
outlier-nouns.
III. DISCUSSION
From an information theory viewpoint, long-range cor-
relations in a symbolic sequence have two different and
concurrent sources: the broad distribution of the dis-
tances between successive occurrences of the same sym-
bol (burstiness) and the correlations of these distances.
We found that the contribution of these two sources is
very different for observables of a literary text at different
linguist levels. In particular, our theoretical framework
provides a robust mechanism explaining our extensive
observations that on relevant semantic levels the text is
high-dimensional and bursty while on lower levels succes-
sive projections destroy burstiness while preserving the
long-range correlations of the encoded text via a flow of
information from burstiness to correlations.
The mechanism explaining how correlations cascade
from high- to low-levels is generic and extends to lev-
els higher than word-level in the hierarchy in Fig. 1.
The construction of such levels could be based, e.g., on
techniques devised to extract information on a “concept
space” [2, 22, 36]. While long-range correlations have
been observed at the concept level [2], further studies are
required to connect to observations made at lower levels
and to distinguish between the two sources of correla-
tions. Our results showing that correlation is preserved
after random additions/subtractions of 1’s help this con-
nection because they show that words can be linked to
concepts even if they are not used every single time the
concept appears (a high probability suffices). For in-
stance, in Ref. [2] a topic can be associated to an axis
7
of the concept space and be linked to the words used
to build it. In this case, when the text is referring to
a topic there is a higher probability of using the words
linked to it and therefore our results show that corre-
lations will flow from the topic to the word level. In
further higher levels, it is insightful to consider as a limit
picture the renewal case – Eq. (8) – for which long-range
correlations originate only due to burstiness. This limit
case is the simplest toy model compatible with our re-
sults. Our theory predicts that correlations take form of
a bursty sequence of events once we approach the seman-
tically relevant topics of the text. Our observations show
that some highly topical words already show long-range
correlations mostly due to burstiness, as expected by ob-
serving that topical words are connected to less concepts
than function words [35]. This renewal limit case is the
desired outcome of successful analysis of anomalous dif-
fusion in dynamical systems and has been speculated to
appear in various fields [19, 32]. Using this limit case as
a guideline we can think of an algorithm able to auto-
matically detect the relevant structures in the hierarchy
by pushing recursively the long-range correlations into a
renewal sequence.
Next we discuss how our results improve previous anal-
yses and open new possibilities of applications. Previous
methods either worked below the letter level [1, 25–27]
or combined the correlations of different letters in such a
way that asymptotically the most long-range correlated
sequence dominates [6, 8, 11]. Only through our results
it is possible to understand that indeed a single asymp-
totic exponent γ should be expected in all these cases.
However, and more importantly, γ is usually beyond ob-
servational range and an interesting range of finite-time γ̂
is obtained depending on the observable or encoding. On
the letter level, our analysis (Figs. 2 and 3) revealed that
all of them are long-range correlated with no burstiness
(exponentially distributed inter-event times). This lack
of burstiness can be wrongly interpreted as an indication
that letters [33] and most parts of speech [38] are well de-
scribed by a Poisson processes. Our results explain that
the non-Poissonian (and thus information rich) charac-
ter of the text is preserved in the form of long-range
correlations (γ > 1), which is observed also for all fre-
quent words (even in the most frequent word “ the ”).
These observations violate not only the strict assump-
tion of a Poisson process, they are incompatible with
any finite-state Markov chain model. These models are
the basis for numerous applications of automatic seman-
tic information extraction, such as keywords extraction,
authorship attribution, plagiarism detection, and auto-
matic summarization [12–15]. All these applications can
potentially benefit from our deeper understanding of the
mechanisms leading to long-range correlations in texts.
Apart from these applications, more fundamental ex-
tensions of our results should: (i) consider the mutual in-
formation and similar entropy-related quantities, which
have been widely used to quantify long-range correla-
tions [6, 9] (see [24] for a comparison to correlations);
(ii) go beyond the simplest case of the two point au-
tocorrelation function and consider multi-point correla-
tions or higher order entropies [6], which are necessary
for the complete characterization of the correlations of a
sequence; and (iii) consider the effect of non-stationarity
on higher levels, which could cascade to lower levels and
affect correlations properties. Finally, we believe that our
approach may help to understand long-range correlations
in any complex system for which an hierarchy of levels
can be identified, such as human activities [6] and DNA
sequences [9–11, 39].
Acknowledgments
We thank B. Lindner for insightful suggestions and
S. Graffi for the careful reading of the manuscript.
G.C. acknowledges partial support by the FIRB-project
RBFR08UH60 (MIUR, Italy). M. D. E. acknowledges
partial support by the PRIN project 2008Y4W3CY
(MIUR, Italy).
[1] Schenkel A, Zhang J, Zhang Y (1993) Long range correla-
tion in human writings. Fractals 1:47-55.
[2] Alvarez-Lacalle E, Dorow B, Eckmann JP, Moses E, (2006)
Hierarchical structures induce long-range dynamical corre-
lations in written texts. Proc Natl Acad Sci USA 103:7956-
7961.
[3] Voss R, Clarke J (1975) ‘1/f noise’ in music and speech.
Nature 258:317-318.
[4] Gilden D, Thornton T, Mallon M (1995) 1/f noise in hu-
man cognition. Science 267:1837-1839.
[5] Muchnik L, Havlin S, Bunde A, Stanley HE (2005) Scal-
ing and memory in volatility return intervals in financial
markets. Proc Natl Acad Sci USA 102:9424-9428.
[6] Rybski D, Buldyrev SV, Havlin S, Liljeros F, Makse HA
(2009) Scaling laws of human interaction activity. Proc
Natl Acad Sci 106:12640-12645.
[7] Kello CT, Brown GDA, Ferrer-i-Cancho R, Holden JG,
Linkenkaer-Hansen K, Rhodes T, Van Orden GC (2010)
Scaling laws in cognitive sciences. Trends Cogn Sci 14:223-
232
[8] Press WH (1978) Flicker Noises in Astronomy and Else-
where. Comments on Astrophysics 7:103
[9] Li W, Kaneko K (1992) Long-range correlation and partial
1/fα spectrum in a noncoding DNA sequence. Europhys
Lett 17:655-660.
[10] Peng CK, Buldyrev S, Goldberger A, Havlin S, Sciortino
F, Simons M, and Stanley HE (1992) Long-Range Corre-
lations in Nucleotide Sequences. Nature 356: 168-171.
[11] Voss RF (1992) Evolution of long-range fractal correla-
tions and 1/f noise in DNA base sequences. Phys Rev Lett
68:3805-3808.
[12] C.D. Manning, H. Schütze (1999) Foundations of Statis-
8
tical Natural Language Processing, (The MIT Press, Cam-
bridge, Massachusetts, USA).
[13] Stamatatos E (2009) A survey of modern authorship at-
tribution methods. Journal of the American Society for
Information Science and Technology 60:538-556.
[14] Oberlander J and Brew C (2000) Stochastic text gener-
ation. Phil Trans R Soc Lond A 358:1373-1387.
[15] O Usatenko, V Yampolskii (2003) Binary N-Step Markov
Chains and Long-Range Correlated Systems. Phys Rev
Lett 90:110601.
[16] Amit M, Shmerler Y, Eisenberg E, Abraham M, Shnerb
N (1994) Language and codification dependence of long-
range correlations in texts. Fractals 2:7-13
[17] Ebeling W, Neiman A (1995) Long-range correlations be-
tween letters and sentences in texts. Physica A 215:233-
241.
[18] Ebeling W, Pöschel T (1994) Entropy and long-range
correlations in literary English. Europhys Lett 26:241-246.
[19] Allegrini P, Grigolini P, Palatella L (2004) Intermittency
and scale-free networks: a dynamical model for human
language complexity. Chaos, Solitons and Fractals 20:95-
105.
[20] Melnyk SS, Usatenko OV, and Yampolskii VA (2005)
Competition between two kinds of correlations in literary
texts. Phys Rev E 72:026140.
[21] Herrera JP, Pury PA (2008) Statistical keyword detection
in literary corpora. Eur Phys J B 63:135-146.
[22] Montemurro MA, Zanette D (2010) Towards the quan-
tification of the semantic information encoded in written
language. Adv Comp Syst 13:135-153.
[23] Cover TM, Thomas JA (2006) Elements of Information
Theory (Wiley Series in Telecommunications and Signal
Processing)
[24] Herzel H, Große I (1995) Measuring correlations in sym-
bol sequences. Physica A: Statistical Mechanics and its
Applications, 216:518-542
[25] Grassberger P (1989) Estimating the information content
of symbol sequences and efficient codes. IEEE Transac-
tions on Information Theory, 35:669-675.
[26] Kokol P, Podgorelec V (2000) Complexity And Human
Writings . Complexity 7:1-6.
[27] Kanter I, Kessler DA (1995) Markov processes: linguis-
tics and Zipf’s Law. Phys Rev Lett 74:4559-4562.
[28] Montemurro MA, Pury PA (2002) Long-range fractal cor-
relations in literary corpora. Fractals 10:451-461.
[29] Trefán G, Floriani E, West BJ and Grigolini P, (1994)
Dynamical approach to anomalous diffusion: response of
Levy processes to a perturbation. Phys Rev E 50:2564-
2579.
[30] Cox DR, Lewis PAW (1978) The statistical analysis of
series of events (Chapman and Hall, London).
[31] B. Lindner (2006) Superposition of many independent
spike trains is generally not a Poisson process. Phys Rev
E 73:022901.
[32] Allegrini P, Menicucci D, Bedini R, Gemignani A, Par-
adisi P (2010) Complex intermittency blurred by noise:
Theory and application to neural dynamics. Phys Rev E
82:015103.
[33] Goh K-I, Barabasi A-L, Burstiness and memory in com-
plex systems. Europhys Lett 81: 48002.
[34] Ortuno M, Carpena P, Bernaola-Galvan P, Munoz E, So-
moza AM (2002) Keyword detection in natural languages
and DNA. Europhys Lett 57:759-764.
[35] Altmann EG, Pierrehumbert JB, Motter AE (2009) Be-
yond word frequency: Bursts, lulls, and scaling in the tem-
poral distributions of words. PLoS ONE 4:e7678.
[36] Doxas I, Dennis S, Oliver WL (2009) The dimensionality
of discourse. Proc Natl Acad Science USA 107:4866-4871.
[37] Saussure F de (1983) Course in General Linguistics, Eds.
Charles Bally and Albert Sechehaye. (Trans. Roy Harris.
La Salle, Illinois)
[38] Badalamenti AF (2001) Speech Parts as Poisson Pro-
cesses. Journal of Psycholinguistic Research 30:31.
[39] Schmitt AO, Ebeling W, Herzel H (1996) The mod-
ular structure of informational sequences. Biosystems
37:199210.
[40] Our approach is slightly different from Refs. [6, 8, 11]
because instead of performing an average over different
symbols we investigate each symbol separately.
[41] Note that our hierarchy of levels is different from the
one used in Ref. [2], which is based on increasingly large
adjacent pieces of texts.
[42] A sequence x of a word containing the given letter is
on top of the sequence z of that letter. If z is long range
correlated (lrc) then either x is lrc or y is lrc. Being finite
the number of words with a given letter, we can recursively
apply the argument to y and identify at least one lrc word.
[43] Sequences x and y are non-overlapping if for all i for
which xi = 1 we have yi = 0.
9
Supporting Information
I. AVERAGE PROCEDURE IN BINARY
SEQUENCES
Given an ergodic and stationary stochastic process,
correlation functions are defined as
Corr(j, t) := E (xjxj+t)− E(xj)E(xj+t). (9)
where E(·) denotes an average over different realiza-
tions x of the process. Stationarity guarantees that
Corr(j, t) depends on the time lag t only. In practice,
one typically has no access to different realizations of the
process but only to a single finite sequence. In our case,
any binary sequence x is obtained from a single text of
length N through a given mapping. In such cases it is
possible to use the assumption of ergodicity to approxi-
mate the correlation function (9) by
Cx(t) := 〈xjxj+t〉 − 〈xj〉〈xj+t〉,
where 〈·〉 means averaging, for each fixed t, over all pairs
xj and xj+t for j = 1, 2, ..., (N − t) as
〈·〉 ≡ 1
N − t
N−t∑
j=1
·.
II. MAPPING EXAMPLES
Consider the sentence “This paper is a paper of
mine”. By choosing the condition α to be the k-th sym-
bol is a vowel the projection fα maps the sentence
into the sequence {00100010100100100101001000101}.
If α is the k-th symbol is equal to ‘e’ than we get:
{0000000010000000100001000000001}. Generally, we
can treat any n-gram of letters in the same way, as
for example by choosing the condition α to be the
2-gram starting at the k-th symbol is equal to ‘er’,
that projects using a sliding window the sentence
to:{00000000100000000001000000000}. Words are en-
coded using their corresponding n-gram, for example
α could be the 7-gram starting at the k-th symbol is
equal to ‘ paper ’ (blank spaces included) that gives:
{0000100000000000010000000}. It is possible to general-
ize these procedures to more semantic conditions α that
associate 1 to either all or part of the symbols that ap-
pears in a sentence that is attached to a specified topic.
These topics can be quantitatively constructed from the
frequency of words using methods such as latent semantic
analysis [1] or the procedures to determine the so-called
concept space [2].
III. SIMPLE OPERATIONS ON BINARY
SEQUENCE AND THEIR EFFECTS ON
LONG-RANGE CORRELATIONS AND
BURSTINESS
We describe two simple procedures to construct two
binary sequences x and z such that x is on top of z.
These procedures will be based either on the “addition”
of 1’s to x or on the “subtraction” of 1’s of z. In the
simplest cases of random addition and subtraction, we
explicitly compute how long-range correlations flow from
x to z (corresponding to a flow from upper to lower levels
of the hierarchy) and how burstiness is preserved when
extracting x from z ( moving from lower to upper levels
in the hierarchy).
Recall that a sequence x is on top of z if for all j such
that xj = 1 we have zj+r = 1, for a fixed constant r.
Without loss of generality in the following calculations
we fix for simplicity r = 0. We now define simple opera-
tions that map two binary sequences into a third binary
sequence:
• Given two generic binary sequences z and ξ we de-
fine their multiplication y=ξ z as yi = ξizi, ∀i. By
construction y is on top of z.
• Given two non-overlapping sequences x and y we
define their sum z = x+y as zi = xi + yi, ∀i. By
construction x and y are on top of z. We say that
sequences x and y are non-overlapping if for all i
for which xi = 1 we have yi = 0.
In general, two independent binary sequences x and
ξ will overlap. A sequence y which is non-overlapping
with x can be constructed from ξ as y= ξ(1-x), where 1
denotes the trivial sequence with all 1’s. In this case, we
say that z = x+y, with y =ξ(1-x) is a sequence lower
than x in the hierarchy that is constructed by a random
addition (of 1’s) to x. Similarly, if ζ is independent of z,
the sequence ζz is a random subtraction (of 1’s) of z
A. Transition time from normal to anomalous
diffusion
Consider a sequence z constructed as a random addi-
tion of 1’s to a given long-range correlated sequence x:
z = x + y, with y =ξ(1-x) and ξ a sequence of i.i.d.
binary random variables. The associated random walker
Z spreads anomalously with the same exponent of X.
This asymptotic regime is masked at short times by a
pre-asymptotic normal behavior. Here we first compute
explicitly the spreading of Z in terms of that of X and
Y and then we compute a bound for the transition time
tT to the asymptotic anomalous diffusion of Z.
10
As written in Eq. (5) of the main text we have
σ2Z(t) = σ
2
X(t) + σ
2
Y (t) + 2C(X(t), Y (t)). (10)
For our particular case we obtain
〈Y (t)〉 = 〈ξ〉t (1− 〈x〉) . (11)
and
〈Y (t)2〉 =
〈
t∑
i,j=1
(x̄iξi)(x̄jξj)
〉
=
〈
t∑
i=1
(x̄2i ξ
2
i )
〉
+
〈
t∑
i,j=1,i6=j
(x̄iξi)(x̄jξj)
〉
=
t∑
i=1
〈
x̄2i
〉 〈
ξ2
〉
+
t∑
i,j=1,i6=j
〈x̄ix̄j〉 〈ξ〉2
=
t∑
i=1
〈
x̄2i
〉 〈
ξ2
〉
−
t∑
i=1
〈
x̄2i
〉
〈ξ〉2 +
t∑
i=1
〈
x̄2i
〉
〈ξ〉2 +
t∑
i,j=1,i6=j
〈x̄ix̄j〉 〈ξ〉2
= 〈ξ〉2〈X(t)2〉+ σ2(ξ)
t∑
i=1
〈x̄2i 〉. (12)
From Eqs. (11) and (12) – and noting that
∑t
i=1〈x̄2i 〉 =
∑t
i=1〈x̄i〉 = t(1− 〈x〉) and σ2X̄(t) = σ
2
X(t) – we obtain
σ2Y (t) ≡ 〈Y 2(t)〉 − 〈Y (t)〉2 = 〈ξ〉2 σ2X(t) + t σ2ξ (1− 〈x〉) (13)
The correlation term in Eq. (10) can also be obtained through direct calculations:
C(X(t), Y (t)) = 〈X(t)Y (t)〉 − 〈X(t)〉 〈Y (t)〉
=
〈
t∑
i,j=1
xi(1− xj)ξj
〉
− 〈X〉
〈
t∑
j=1
(1− xj)ξj
〉
= 〈X(t)〉 〈ξ〉 t−
〈
X2(t)
〉
〈ξ〉 − 〈X(t)〉
[
〈ξ〉 t− 〈X(t)〉 〈ξ〉
]
= −〈ξ〉σ2X(t). (14)
Finally, inserting Eqs. (13) and (14) into Eq. (10) we have
σ2Z(t) = σ
2
X(t) + σ
2
Y (t) + 2C(X(t), Y (t))
= σ2X(t) + 〈ξ〉2σ2X(t) + tσ2ξ (1− 〈x〉)− 2〈ξ〉σ2X(t)
= tσ2ξ (1− 〈x〉) + σ2X(t)(1− 〈ξ〉)2
= 〈ξ〉(1− 〈ξ〉)(1− 〈x〉)t + (1− 〈ξ〉)2σ2X(t) (15)
As X superdiffuses so it will Z and they both have
the same asymptotic behavior. On the other hand
the asymptotic regime is masked at short times by a
pre-asymptotic normal behavior, given by the linear
term in t. We stress that, even if the non-overlapping
condition for y forces both σ2Y (t) and C(X(t), Y (t))
to have the same asymptotic behavior of σ2X(t), their
cumulative contributions does not cancel out unless we
trivially have 〈ξ〉 = 1.
We now give a bound on the transition time tT to the
asymptotic anomalous diffusion of Eq. (15). Without loss
of generality consider the case in which even the asymp-
totic anomalous behavior of X is masked by generic pre-
asymptotic A(t) such that
σ2X(t) = 〈x〉(1− 〈x〉)
[
(1− g)A(t) + gtγX
]
with 0 < g ≤ 1 and A(t) increasing and such that
A(t)/tγX → 0 for t → ∞ (to guarantee that the asymp-
totic behavior is dominated by tγX ) and A(1) = 1
(as σ2X(1) = 〈x〉(1 − 〈x〉)). The asymptotic behavior
σ2Z(t) ∼ tγX in Eq.(15) dominates only after a time tT
11
such that:
〈ξ〉tT + (1− g)〈x〉(1− 〈ξ〉)A(tT )
g(1− 〈ξ〉)〈x〉
= tγXT (16)
Using the fact that the term (1 − g)〈x〉(1 − 〈ξ〉)A(t) is
positive and that tγ is monotonically increasing we finally
have
tT ≥ t∗T =
(
〈ξ〉
1− 〈ξ〉
1
g〈x〉
)1/(γX−1)
, (17)
which corresponds to Eq. (7) of the main text. In
practice, any finite-time estimate γ̂X is close to the
asymptotic γX only if the estimate is performed for
t tT , otherwise γ̂X < γX (γ̂X = 1 if t tT ).
As noted in the main text, if z = x+y then x̄ = z̄ +y.
Applying to this relation the same arguments above, sim-
ilar pre-asymptotic normal diffusion and transition time
appear in the case of random subtraction, moving up in
the hierarchy. More specifically, starting from a sequence
z such that asymptotically σ2Z(t) ' g〈z〉(1− 〈z〉)tγZ and
constructing x = ζz, with ζ independent of z, we obtain
a transition time tT for x given by:
tT ≥ t∗T =
(
1− 〈ζ〉
〈ζ〉
1
g(1− 〈z〉)
)1/(γZ−1)
, (18)
which corresponds to Eq. (17) above after properly re-
placing 〈x〉 → (1− 〈z〉), 〈ξ〉 → (1− 〈ζ〉) and γX → γZ .
B. Random subtraction preserves burstiness
We consider the case of sequences as in Eq. (8) of
the main text: z is a sequence emerging from a renewal
process with algebraically decaying inter-event times, i.e.
p(τ) = τ−µ and Cτ (k) = δ(k). Given now a fixed
0 ≤ 〈ξ〉 ≤ 1, we consider the random subtraction x = ξz
where each zj = 1 is eventually set to zj = 0 with prob-
ability 〈ξ〉. It is easy to see that the inter-event times of
the new process will be distributed as:
p̃(τ) = (1− 〈ξ〉)p(τ) +
∞∑
k≥1
(〈ξ〉)k
∑
t1+t2+···+tk=τ
k∏
j=1
p(tj).
Asymptotically p̃(τ) is dominated by the long tails of
(1 − 〈ξ〉)p(τ): given a large τ , fix k̄ > 0 eventually di-
verging with τ →∞ and split accordingly the sum over k
in the second term of the right hand side. The term cor-
responding to the sum k > k̄ is exponentially dominated
by ξk̄ and arbitrary small, while the remaining finite sum
over k ≤ k̄ is controlled again by the tail of p(τ).
IV. DATA
In our investigations we considered the English ver-
sion of the 10 popular novels listed in SI-Tab. Books.
The texts were obtained through the Gutenberg project
(http://www.gutenberg.org). We implement a very
mild pre-processing of the text that reduces the number
of different symbols and simplifies our analysis: we con-
sider as valid symbols the letters “a-z”, numbers “0-9”,
the apostrophe “ ’ ” and the blank space “ ”. Capitaliza-
tion, punctuations and other markers were removed. A
string of symbols between two consecutive blank spaces is
considered to be a word. No lemmatization was applied
to them so that plurals and singular forms are considered
to be different words.
V. CONFIDENCE INTERVAL FOR
DETERMINING LONG-RANGE CORRELATION
As described in the main text, the distinction be-
tween long-range and short-range correlation requires a
finite-time estimate γ̂ of the asymptotic diffusion expo-
nent γ of the random-walkers associated to a binary se-
quence. In practice, this corresponds to estimate the
tails of the σ2 ' tγ relation and it is therefore essen-
tial to estimate the upper limit in t, denoted as ts, for
which we have enough accuracy to provide a reasonable
estimate γ̂. We adopt the following procedure to esti-
mate ts. We consider a surrogate binary sequence with
the same length N and fraction of symbols (1’s), but
with the symbols randomly placed in the sequence. For
this sequence we know that γ = 1. We then consider
instants of time ti equally spaced in a logarithmic scale
of t (in practice we consider ti+1/ti = 1.2, with i integer
and t0 = 1). We then estimate the local exponent as
γ̂local(ti) = [log10 ∆σ
2(ti+1) − log10 ∆σ2(ti)]/ log10(1.2).
For small t, γ̂local = 1 but for larger t statistical fluc-
tuations arise due to the finiteness of N , as illustrated
in Fig. S2(a). We choose ts as the smallest ti for
which {γ̂local(ti+1), γ̂local(ti+2), γ̂local(ti+3)} are all out-
side [0.9, 1.1] (see Fig. S2a). We recall that our primary
interest in the distinction between γ = 1 and γ 6= 1.
The procedure described above is particularly suited for
this distinction and an exponent γ̂ > 1.1 obtained for
large t / ts can be confidently regarded as a signature
of super diffusion (long-range correlation). In Fig. S2 we
verify that ts show no strong dependence on the fraction
of 1’s in the binary sequence (inset) and that it scales
linearly with N . Based on these results, a good estimate
of ts is ts = N/100, i.e. the safe interval for determining
long-range correlation ends two decades before the size
of the text. This phenomenological rule was adopted in
the estimate of γ̂ for all cases. The ts is only the upper
limit and the estimate γ̂ is performed through a least-
squared fit in the time interval ts′ < t < ts = N/100,
where ts′ ≈ ts/100. In practice, we select 10 different
values of ti around ts/100 and report the mean and vari-
12
ance over the different fittings as γ̂ and its uncertainty,
respectively.
VI. LOWER BOUND FOR γ̂ DUE TO
BURSTINESS
We start clarifying the validity of the inequality
γ̂ ≥ γ̂A2, (19)
where γ̂ is the finite-time estimate of the total long-range
correlation γ of a binary sequence x and γA2 is the es-
timate for the correlation due to the burstiness (which
can be quantified by shuffling x using the procedure A2
of the main text). Equation (4) of the main text shows
that both burstiness στ/〈τ〉 → ∞ and long-range correla-
tions in the sequence of τi’s contribute to the long-range
correlations of a binary sequence x. While the στ/〈τ〉
contribution is always positive, the contribution from the
correlation in τi’s can be positive or negative. In prin-
ciple, a negative contribution could precisely cancel the
contribution of στ/〈τ〉 and violates the inequality (19).
Conversely, this inequality is guaranteed to hold if the
asymptotic contribution of the correlation in τi’s of x to
σ2X is positive. We now show that this is the case for
the sequences we have argued to provide a good account
of our observations. Consider high in the hierarchy a
renewal sequence x with a given γ > 1 and broad tail
in p(τ) (diverging στ/〈τ〉). Adding many independent
non-overlapping sequences, we construct a lower level se-
quence that still has long range correlation, with the same
exponent γ (see Sec. III above). For this sequence we
know that the broad tail in p(τ) has a cutoff τc and thus
burstiness gives no contribution to γ. Instead, γ > 1
results solely from the correlations in the τ ’s, which are
therefore necessarily positive. It is natural to expect that
this positiveness of the asymptotic correlation extends to
finite times, in which case the (finite time) inequality (19)
holds. Indeed, for small τ < τc, the distribution p(τ) is
not strongly affected by the independent additions and
thus for t < τc a finite time estimate γ̂ will receive con-
tributions from both burstiness and τ ′s correlations. Fi-
nally, we have directly tested the validity of Eq. (19) by
comparing γ̂ of different sequences x to the γ̂A2 obtained
from the corresponding xA2 (A2-shuffled sequences of x,
see main text). The inequality (19) was confirmed for
every single sequence we have analyzed, as shown by the
fact that γ̂A2 (red symbols) in Fig. S3 are systematically
below their corresponding γ̂X (black circles).
We now obtain a quantitative lower bound for γ̂ using
Eq. (19). We consider a renewal sequence (in which case
γ̂ = γ̂A2) with an inter-event time distribution given by
p(τ) = Cτ−(4−γA2)e−
τ
τc , τ > τmin, (20)
where τc is the cut-off time, γA2 is the anomalous dif-
fusion exponent for a renewal sequence with no cutoff
τc →∞, τmin is a lower cut-off (we fixed it at τmin = 10),
and C is a normalization constant. We obtain the lower
bound for γ̂ as a function of στ/〈τ〉 by considering how
γ̂A2 and στ/〈τ〉 change with τc in the model above. For
short times (t << τc) the corresponding walkers have not
seen the cutoff and their diffusion will be anomalous with
exponent γ̂A2 = γA2. At longer time (t >> τc [3–5]) the
diffusion becomes normal γ̂A2 = 1. Correspondingly, if
the fitting interval t ∈ [t′s, ts] used to compute the finite
time γ̂A2 (see Sec. V) is all below τc (i.e. ts < τc) we
have γ̂A2 = γA2 while if the fitting interval is all beyond
the cutoff (i.e. τc < ts′) we have γ̂A2 = 1. When τc is
inside the fitting interval we approximate γ̂A2 by linearly
interpolating between γA2 and 1. Finally, we can com-
pute στ/〈τ〉 by directly calculating the first and second
moments of the distribution (20). Particularly impor-
tant are the values s1 and s2 obtained evaluating στ/〈τ〉
at the critical values of the cutoff τc = ts′ and τc = ts,
respectively. Using the fact that στ/〈τ〉 is a monotonic
increasing function of τc we can obtain explicitely the γ̂
dependency on στ/〈τ〉. The γ̂A2 for the case of a binary
sequence with distribution (20) is given by
γ̂A2 = 1 if στ/〈τ〉 < s1,
γ̂A2 = (στ/〈τ〉 − s1) (γA2−1)(s2−s1) + 1 if στ/〈τ〉 ∈ [s1, s2],
γ̂A2 = γA2 if στ/〈τ〉 > s2.
The red dashed line in Fig. S3 (Fig. 3 of the main text)
was computed using the fitting range corresponding to
the book wrnpc ts′ = 3 10
2, ts = 3 10
4 (see Sec. V), and
γA2 = 1.6 (compatible with γ̂ observed for words with
large στ/〈τ〉).
VII. ADDITIONAL SHUFFLING METHODS
In addition to the shuffling methods presented in the
main text, we discuss here briefly two cases:
• Shuffle words
Mixing words order kills correlations for scales
larger than the maximum word length [6, 7]. Even
the blank space sequence B becomes uncorrelated
because its original correlations originate (as in the
case of all letters) from the correlation in τi and not
from tails in p(τ).
• Keep all blank spaces in their original positions
and fill the empty space between them with:
1- two letters a, b, placed randomly with proba-
bilities pa = p and pb = 1− p.
2- the same letters of the book, placed in random
positions.
By construction, correlation for blank space is
trivially preserved. What do we expect for the
13
other letters? The following simple reasoning in-
dicates that long-range correlation should be ex-
pected asymptotically in both cases: any letter se-
quence x is on top of the reverted blank space se-
quence B̄; the results in Sec. III above show that
either the selected sequence x or its complement y
(such that x + y = B̄) has γ = γB ; and Eq. (15)
above shows that any randomly chosen x on top
of B̄ has γ = γB . In practice these exponents are
relevant only if the subsequence is dense enough in
order for tT in Eq. (18) above to be inside the ob-
servation range. For the first shuffling method and
for our longest book (wrnpc), we obtain that only
if p > 95.8% one finds tT < ts = 1% book size.
Since the most frequent letter in a book has much
smaller frequency (around 10%), we conclude that
in practice all sequences obtained using the sec-
ond shuffling mehthod have γ̂ = 1 for all books of
size smaller than 100 × tT ≈ 1011 symbols (≈ 107
pages).
These simple calculations show that γB > 1 does
not explain the correlations observed in the let-
ters of the original text, as has been speculated
in Ref. [8]. Their origin are the long-range correla-
tions on higher levels.
[1] Landauer TK, Foltz P, Laham D (1998) Introduction to
latent semantic analysis. Discourse Process 25:259-284.
[2] Alvarez-Lacalle E, Dorow B, Eckmann JP, Moses E, (2006)
Hierarchical structures induce long-range dynamical corre-
lations in written texts. Proc Natl Acad Sci USA 103:7956-
7961.
[3] Mantegna RN and Stanley EH (1994) Stochastic Process
with Ultraslow Convergence to a Gaussian: The Trun-
cated Lévy Flight Phys. Rev. Lett 73:29462949.
[4] Shlesinger MF (1995) Comment on Stochastic Process
with Ultraslow Convergence to a Gaussian:The Truncated
Lévy Flight Phys. Rev. Lett. 74: 49594959
[5] del-Castillo-Negrete D (2009) Truncation effects in su-
perdiffusive front propagation with Lévy flights Phys. Rev.
E 79: 031120.
[6] Ebeling W, Pöschel T (1994) Entropy and long-range cor-
relations in literary English. Europhys Lett 26:241-246.
[7] Montemurro MA, Pury PA (2002) Long-range fractal cor-
relations in literary corpora. Fractals 10:451-461.
[8] Ebeling W, Neiman A (1995) Long-range correlations be-
tween letters and sentences in texts. Physica A 215:233-
241.
14
100 101 102 103 104 105
t
100
101
102
103
104
105
!
X
2 (
t)
vowels - wrnpc
shuffled "i's
shuffled {0,1}
~t1.55, #=1.55
~t     , #=1.00
100 101 102 103 104
t
100
101
102
103
104
!
X
2 (
t)
vowels
shuffled {0,1}
shuffled "i'salice sawyer pride
missisipi jungle beagle
moby ulysses quixote
Fig. S1. Long-range correlation in texts encoded as vowels. Upper plot: detailed analysis in the book wrnpc with exponent γ̂ = 1.55 ± 0.05 (wrnpc). Lower plots:
analysis of the remaining 9 books with the following exponents γ̂: 1.55 ± 0.05 (wrnpc), 1.18 ± 0.05 (alice), 1.23 ± 0.04 (sawyer), 1.20 ± 0.03 (pride) 1.48 ± 0.05
(missisipi), 1.26 ± 0.05 (jungle), 1.25 ± 0.04 (beagle), 1.45 ± 0.05 (moby), 1.61 ± 0.06 (ulysses), 1.26 ± 0.04 (quixote)
Footline Author PNAS Issue Date Volume Issue Number 7
Fig. S1: Long-range correlation in texts encoded as vowels. Upper plot: detailed analysis in the book wrnpc with exponent γ̂ =
1.55 ± 0.05 (wrnpc). Lowe plots: analysis of the remaining 9 books with the following exponents γ̂: 1.55 ± 0.05 (w npc),
1.18 ± 0.05 (alice), 1.23 ± 0.04 (sawyer), 1.20 ± 0.03 (pride) 1.48 ± 0.05 (missisipi), 1.26 ± 0.05 (jungle), 1.25 ± 0.04 (beagle),
1.45± 0.05 (moby), 1.61± 0.06 (ulysses), 1.26± 0.04 (quixote)
15
10
0
10
1
10
2
10
3
10
4  
10
5
t
0.4
0.6
0.8
1.0
1.2
1.4
1.6
 γ l
oc
al
(t
)
(a)
t
s
(a)
(b)
^
σ2(t) ~ tγ [left axis]
Local γ [right axis]
γ=1
10
3
10
4
10
5
10
6
10
7
N
10
1
10
2
10
3
10
4
10
5
10
6
t s
Boxplot
t
s
=N/100
10
-1
10
0
10
1
10
2
10
3
10
4
σ2
X
(t
)
    0.1%  1%   10%   50%   0.01%
frequency
3
4
5
t s
Fig. S2: (Color online) Determination of the time interval for the estimate of the long-range correlation exponent γ̂. (a)
The dispersion ∆σ2 as a function of time t is shown as  for a random binary sequence of size N = 106 and 10% of 1’s. The
local derivative is shown as  and agrees with the theoretical exponent γ = 1 until fluctuations start for long t (axis on the
right). The time ts denotes the end of the interval of safe determination of γ, as explained in the text. (b) Dependence of ts
on the size of the binary sequence N . The boxplots show the 5%, 25%, 50% (median),75%, and 95% quantiles over M different
realizations of a random binary sequence. Black boxplots: M = 300 (M = 44 for N = 107) realizations equally divided between
frequency= 1%, 10%, 50%. Blue boxplots: M = 35 realizations equally divided between the frequencies of the three most
frequent letters (“ ”,“e”, “t”) and two most frequent words (“ the ”,“ and ”) of the shortest (Alice, N = 143, 488) and longest
(War and Peace, N = 3, 147, 284) books Inset: boxplots of ts for different frequencies and fixed sequence length N = 10
6 and
M = 100, showing no strong dependence on frequency.
16
Fig. S3: This figure corresponds to Fig. 3 of the main paper with the addition (red squares) of the estimated γ̂ for sequences
xA2 obtained shuffling each one of the original sequences. The shuffling does not change the στ/〈τ〉 and therefore the original
and shuffled sequences appear always on the same vertical line. The fact that the results for xA2 are systematically below their
corresponding x is a strong evidence of the validity of the inequality (19).
17
S
h
or
t
n
am
e
N
-
N
u
m
b
er
of
S
y
m
b
ol
s
T
it
le
A
u
th
o
r
T
ra
n
sl
a
to
r
a
n
d
In
fo
rm
a
ti
o
n
fr
o
m
P
ro
je
ct
G
u
te
n
b
er
g
al
ic
e
13
4,
84
7
A
li
ce
’s
A
d
v
en
tu
re
s
in
W
on
d
er
la
n
d
L
ew
is
C
a
rr
o
ll
R
el
ea
se
d
:
2
0
0
9
-0
5
-1
9
sa
w
ye
r
36
9,
22
2
T
h
e
A
d
v
en
tu
re
s
of
T
om
S
aw
ye
r
M
a
rk
T
w
a
in
R
el
ea
se
d
:
2
0
0
6
-0
8
-2
0
p
ri
d
e
65
9,
40
8
P
ri
d
e
an
d
P
re
ju
d
ic
e
J
a
n
e
A
u
st
en
U
p
d
a
te
d
:
2
0
1
0
-0
9
-0
5
;
R
el
ea
se
d
:
J
u
n
e,
1
9
9
8
m
is
si
si
p
i
77
2,
39
1
L
if
e
O
n
T
h
e
M
is
si
ss
ip
p
i
M
a
rk
T
w
a
in
R
el
ea
se
d
:
2
0
0
4
-0
8
-2
0
ju
n
gl
e
78
3,
01
4
T
h
e
J
u
n
gl
e
U
p
to
n
S
in
cl
a
ir
R
el
ea
se
D
a
te
:
2
0
0
6
-0
3
-1
1
b
ea
gl
e
1,
15
3,
63
8
T
h
e
V
oy
ag
e
O
f
T
h
e
B
ea
gl
e
C
h
a
rl
es
D
a
rw
in
R
el
ea
se
d
:
F
eb
ru
a
ry
2
0
0
3
;
R
ep
ri
n
t
fr
o
m
:
J
u
n
e
1
9
1
3
m
ob
y
1,
16
9,
85
0
M
ob
y
D
ic
k
;
or
T
h
e
W
h
al
e
H
er
m
a
n
M
el
v
il
le
U
p
d
a
te
d
:
2
0
0
9
-0
1
-0
3
;
R
el
ea
se
d
:
J
u
n
e
2
0
0
3
u
ly
ss
es
1,
45
3,
58
6
U
ly
ss
es
J
a
m
es
J
oy
ce
R
el
ea
se
d
:
J
u
ly
,
2
0
0
3
q
u
ix
ot
e
2,
08
0,
43
1
D
on
Q
u
ix
ot
e
M
ig
u
el
d
e
C
er
va
n
te
s
S
a
av
ed
ra
T
ra
n
sl
a
to
r:
J
o
h
n
O
rm
sb
y
R
el
ea
se
d
:
2
0
0
4
-0
7
-2
7
w
rn
p
c
3,
08
2,
07
9
W
ar
an
d
P
ea
ce
L
eo
T
o
ls
to
y
T
ra
n
sl
a
to
r:
L
o
u
is
e
a
n
d
A
y
lm
er
M
a
u
d
e
U
p
d
a
te
d
:
2
0
0
7
-0
5
-0
7
;
R
el
ea
se
d
:
A
p
ri
l
2
0
0
1
T
a
b
le
S
1
:
L
is
t
o
f
b
o
o
k
s
co
n
si
d
er
ed
in
o
u
r
in
v
es
ti
g
a
ti
o
n
s.
T
h
e
te
x
ts
w
er
e
re
tr
ie
v
ed
fr
o
m
th
e
P
ro
je
ct
G
u
te
n
b
er
g
w
w
w
.g
u
te
n
be
rg
.o
rg
o
n
2
1
-0
9
-2
0
1
0
18
Table S2: Correlation γ̂ and burstiness στ/〈τ〉 obtained for
the diferrent binary sequences in the indicated book.
Book: alice; N=134,847
Original data Shuffling M1 Shuffling M2
sequence Ni στ/〈τ〉 error γ̂ error γ̂ error γ̂ error
vowels 41414 0.440 0.020 1.18 0.05
26666 0.379 0.011 1.13 0.06 1.13 0.06 1.13 0.06
e 13545 0.812 0.003 1.20 0.04 1.11 0.04 1.01 0.04
t 10667 0.858 0.003 1.17 0.05 1.05 0.03 1.05 0.03
a 8772 0.838 0.003 1.14 0.05 1.07 0.03 0.98 0.04
o 8128 0.920 0.002 1.25 0.05 1.13 0.04 0.99 0.04
i 7500 0.887 0.002 1.20 0.04 1.10 0.03 1.03 0.03
h 7379 0.848 0.003 1.15 0.04 1.11 0.04 1.04 0.03
n 7001 0.895 0.002 1.09 0.03 1.13 0.04 1.02 0.03
s 6497 0.925 0.002 1.11 0.04 1.09 0.03 1.07 0.03
r 5418 0.905 0.002 1.15 0.04 1.15 0.04 1.04 0.03
d 4928 0.878 0.003 1.06 0.03 1.10 0.04 0.97 0.04
l 4704 1.081 0.003 1.20 0.06 1.12 0.04 1.00 0.03
u 3469 0.901 0.004 1.15 0.04 1.15 0.04 1.07 0.03
w 2681 0.966 0.003 1.11 0.04 1.23 0.05 0.99 0.04
g 2529 0.986 0.003 1.13 0.04 1.16 0.05 0.97 0.05
c 2397 0.980 0.005 1.15 0.05 1.11 0.04 1.00 0.03
y 2259 1.070 0.004 1.23 0.04 1.05 0.03 1.00 0.04
m 2103 1.030 0.005 1.16 0.04 1.24 0.05 0.98 0.03
f 1988 1.089 0.006 1.17 0.05 1.14 0.04 1.02 0.04
p 1514 1.143 0.006 1.17 0.04 1.13 0.04 1.05 0.03
the 1635 0.971 0.005 1.29 0.07
and 868 0.973 0.008 1.08 0.04
to 734 1.013 0.006 1.08 0.04
a 624 1.057 0.007 1.08 0.03
she 542 1.548 0.012 1.34 0.06
it 530 1.172 0.008 1.22 0.04
alice 386 0.885 0.008 1.01 0.05
in 367 0.959 0.008 1.04 0.03
way 57 1.098 0.021 1.21 0.04
turtle 57 4.066 0.039 1.47 0.12
hatter 55 4.978 0.050 1.46 0.12
gryphon 55 3.541 0.038 1.43 0.10
quite 55 1.290 0.025 1.16 0.04
mock 55 3.919 0.045 1.45 0.12
are 54 1.250 0.024 1.10 0.03
think 52 1.268 0.039 1.09 0.04
more 49 1.040 0.023 1.11 0.04
head 49 1.230 0.024 1.07 0.03
never 48 1.083 0.049 1.09 0.05
voice 47 1.359 0.054 1.07 0.03
19
Table S3: Correlation γ̂ and burstiness στ/〈τ〉 obtained for
the diferrent binary sequences in the indicated book.
Book: beagle; N=1,153,638
Original data Shuffling M1 Shuffling M2
sequence Ni στ/〈τ〉 error γ̂ error γ̂ error γ̂ error
vowels 358397 0.454 0.020 1.25 0.04
208375 0.434 0.009 1.34 0.04 1.34 0.04 1.34 0.04
e 123056 0.817 0.002 1.18 0.04 1.22 0.05 0.96 0.05
t 86576 0.836 0.002 1.18 0.04 1.15 0.04 0.98 0.04
a 78914 0.847 0.002 1.11 0.04 1.11 0.04 1.03 0.03
o 67896 0.885 0.001 1.20 0.04 1.24 0.04 0.96 0.05
n 64597 0.865 0.002 1.13 0.04 1.18 0.04 1.02 0.04
i 63755 0.889 0.001 1.22 0.04 1.15 0.04 1.07 0.03
s 62383 0.909 0.001 1.21 0.04 1.20 0.04 1.05 0.03
r 59027 0.870 0.001 1.17 0.04 1.09 0.04 1.05 0.03
h 54880 0.861 0.002 1.23 0.04 1.07 0.05 1.09 0.03
l 38467 1.033 0.001 1.28 0.05 1.10 0.04 1.03 0.03
d 37051 0.923 0.001 1.24 0.04 1.19 0.04 1.01 0.03
c 27687 0.978 0.001 1.28 0.04 1.17 0.04 1.11 0.03
u 24776 0.957 0.001 1.15 0.04 1.12 0.04 1.04 0.03
f 24052 0.955 0.001 1.18 0.04 1.12 0.04 1.08 0.03
m 21509 0.987 0.001 1.20 0.04 1.20 0.04 1.03 0.04
w 19172 1.010 0.001 1.36 0.05 1.21 0.04 1.07 0.03
g 18284 0.962 0.001 1.20 0.04 1.09 0.05 0.99 0.03
p 16742 1.040 0.001 1.24 0.04 1.10 0.03 1.04 0.03
y 15700 0.993 0.002 1.26 0.04 1.15 0.04 1.03 0.04
the 16882 0.924 0.002 1.21 0.04
of 9414 0.970 0.002 1.25 0.04
and 5765 0.897 0.003 1.10 0.04
a 5326 1.097 0.003 1.19 0.04
in 4287 1.022 0.003 1.14 0.04
to 4080 1.051 0.003 1.20 0.04
water 417 1.509 0.011 1.26 0.04
little 412 1.117 0.011 1.08 0.03
where 349 1.086 0.011 1.06 0.03
sea 348 1.534 0.015 1.29 0.04
much 338 1.112 0.010 1.08 0.04
country 337 1.519 0.011 1.28 0.05
land 318 1.387 0.012 1.25 0.04
must 317 1.290 0.009 1.12 0.04
feet 312 1.391 0.013 1.23 0.04
may 311 1.118 0.010 1.09 0.03
species 303 2.459 0.022 1.55 0.05
found 303 1.217 0.010 1.16 0.04
me 301 1.206 0.012 1.07 0.04
day 301 1.375 0.007 1.12 0.03
20
Table S4: Correlation γ̂ and burstiness στ/〈τ〉 obtained for
the diferrent binary sequences in the indicated book.
Book: jungle; N=783,014
Original data Shuffling M1 Shuffling M2
sequence Ni στ/〈τ〉 error γ̂ error γ̂ error γ̂ error
vowels 237050 0.420 0.020 1.26 0.05
151300 0.404 0.010 1.53 0.05 1.53 0.05 1.53 0.05
e 78161 0.843 0.002 1.16 0.04 1.09 0.04 1.04 0.04
t 58475 0.873 0.002 1.32 0.05 1.19 0.04 1.04 0.03
a 53663 0.854 0.002 1.21 0.04 1.17 0.04 1.08 0.03
o 47726 0.896 0.001 1.18 0.04 1.24 0.04 0.97 0.04
n 44497 0.874 0.002 1.14 0.04 1.18 0.04 1.12 0.03
h 44473 0.832 0.002 1.37 0.05 1.27 0.04 1.17 0.05
i 40025 0.906 0.001 1.34 0.05 1.23 0.05 1.13 0.04
s 37500 0.941 0.001 1.32 0.05 1.37 0.04 1.07 0.03
r 34514 0.888 0.002 1.19 0.04 1.19 0.04 1.09 0.04
d 30491 0.929 0.001 1.31 0.06 1.22 0.04 1.00 0.04
l 24876 1.059 0.001 1.20 0.04 1.10 0.04 1.07 0.03
u 17475 0.943 0.001 1.16 0.04 1.22 0.04 0.99 0.05
w 17213 0.948 0.002 1.36 0.06 1.30 0.05 1.06 0.03
m 14754 0.977 0.001 1.18 0.04 1.24 0.04 1.04 0.03
c 14148 1.006 0.001 1.37 0.05 1.18 0.04 1.10 0.04
g 14069 0.994 0.002 1.28 0.06 1.27 0.04 1.06 0.03
f 13862 1.016 0.002 1.25 0.04 1.21 0.04 1.09 0.04
y 10868 1.068 0.002 1.29 0.05 1.16 0.04 0.95 0.05
p 9940 1.074 0.002 1.24 0.05 1.24 0.04 1.06 0.04
the 8930 1.018 0.003 1.34 0.04
and 7280 0.958 0.002 1.27 0.04
of 4365 1.113 0.003 1.42 0.07
to 4190 1.077 0.003 1.20 0.04
a 4158 1.152 0.004 1.22 0.04
he 3311 2.158 0.011 1.60 0.05
him 1184 2.009 0.013 1.42 0.05
jurgis 1098 2.077 0.010 1.48 0.07
i 485 6.141 0.275 1.54 0.06
man 463 1.301 0.013 1.27 0.04
said 367 1.975 0.019 1.38 0.04
time 356 1.209 0.013 1.15 0.04
men 329 1.768 0.011 1.33 0.05
now 325 1.077 0.009 1.11 0.03
day 280 1.378 0.021 1.15 0.04
other 279 1.244 0.014 1.16 0.04
place 263 1.227 0.013 1.17 0.04
only 261 1.042 0.010 1.03 0.04
before 235 1.117 0.010 1.09 0.03
home 229 1.759 0.012 1.23 0.04
21
Table S5: Correlation γ̂ and burstiness στ/〈τ〉 obtained for
the diferrent binary sequences in the indicated book.
Book: missisipi; N=772,391
Original data Shuffling M1 Shuffling M2
sequence Ni στ/〈τ〉 error γ̂ error γ̂ error γ̂ error
vowels 235370 0.445 0.020 1.48 0.05
146786 0.429 0.009 1.65 0.05 1.65 0.05 1.65 0.05
e 76483 0.850 0.002 1.40 0.05 1.11 0.04 1.10 0.03
t 59660 0.858 0.002 1.24 0.04 1.18 0.04 1.02 0.04
a 51642 0.859 0.002 1.23 0.04 1.18 0.04 1.11 0.04
o 47123 0.890 0.002 1.23 0.04 1.22 0.04 1.05 0.04
n 44064 0.869 0.002 1.24 0.04 1.24 0.04 1.08 0.03
i 42750 0.920 0.001 1.30 0.04 1.19 0.04 1.11 0.03
s 38995 0.940 0.001 1.34 0.06 1.23 0.04 1.20 0.04
h 36904 0.859 0.002 1.40 0.05 1.13 0.04 1.20 0.04
r 35465 0.912 0.001 1.34 0.05 1.19 0.04 1.16 0.04
d 27682 0.974 0.001 1.40 0.06 1.24 0.04 1.05 0.03
l 24910 1.055 0.001 1.20 0.04 1.12 0.04 1.06 0.03
u 17372 0.947 0.002 1.20 0.04 1.17 0.04 1.07 0.03
w 15554 0.996 0.002 1.30 0.04 1.25 0.04 1.10 0.03
m 14940 1.006 0.002 1.29 0.04 1.27 0.04 1.09 0.04
c 14884 1.042 0.001 1.35 0.05 1.21 0.04 1.21 0.06
f 14234 1.006 0.001 1.24 0.05 1.14 0.04 1.04 0.03
g 12890 1.044 0.001 1.26 0.04 1.17 0.04 1.09 0.03
y 11994 1.022 0.002 1.34 0.04 1.14 0.04 1.02 0.03
p 11087 1.093 0.002 1.30 0.05 1.17 0.04 1.16 0.05
the 9091 1.043 0.003 1.38 0.04
and 5898 0.995 0.003 1.34 0.05
of 4380 1.033 0.003 1.32 0.05
a 4057 1.098 0.003 1.22 0.04
to 3545 1.095 0.004 1.24 0.04
in 2555 1.031 0.004 1.14 0.04
would 480 1.552 0.012 1.26 0.04
river 478 2.176 0.014 1.43 0.06
water 242 1.899 0.015 1.38 0.05
she 239 2.055 0.022 1.44 0.06
boat 212 1.921 0.028 1.32 0.05
here 210 1.508 0.015 1.24 0.04
night 177 1.609 0.012 1.30 0.05
can 177 1.392 0.015 1.13 0.04
go 176 1.275 0.010 1.16 0.04
head 175 1.612 0.017 1.41 0.06
pilot 172 2.652 0.047 1.40 0.05
long 172 1.246 0.013 1.06 0.03
first 164 1.132 0.018 1.11 0.04
miles 162 1.816 0.030 1.49 0.05
22
Table S6: Correlation γ̂ and burstiness στ/〈τ〉 obtained for
the diferrent binary sequences in the indicated book.
Book: moby; N=1,169,850
Original data Shuffling M1 Shuffling M2
sequence Ni στ/〈τ〉 error γ̂ error γ̂ error γ̂ error
vowels 356037 0.441 0.020 1.45 0.05
215939 0.424 0.009 1.54 0.05 1.54 0.05 1.54 0.05
e 116938 0.859 0.002 1.29 0.04 1.12 0.04 1.05 0.03
t 87882 0.860 0.002 1.23 0.04 1.25 0.04 1.00 0.03
a 77820 0.851 0.002 1.24 0.04 1.22 0.05 1.05 0.03
o 69258 0.900 0.001 1.27 0.04 1.16 0.04 1.09 0.03
n 65552 0.886 0.001 1.20 0.04 1.20 0.04 1.07 0.03
i 65349 0.905 0.001 1.28 0.04 1.11 0.04 1.09 0.03
s 64148 0.917 0.001 1.34 0.05 1.31 0.04 1.15 0.04
h 62824 0.856 0.002 1.32 0.04 1.38 0.06 1.21 0.04
r 52073 0.900 0.002 1.32 0.04 1.19 0.04 1.14 0.04
l 42733 1.051 0.001 1.22 0.04 1.20 0.04 0.99 0.03
d 38192 0.969 0.001 1.42 0.05 1.19 0.04 1.05 0.03
u 26672 0.968 0.001 1.23 0.04 1.09 0.03 1.02 0.03
m 23243 0.998 0.001 1.22 0.04 1.16 0.04 0.96 0.04
c 22482 1.031 0.001 1.32 0.04 1.30 0.04 1.15 0.04
w 22193 0.957 0.001 1.24 0.04 1.23 0.04 1.06 0.03
f 20812 0.997 0.001 1.33 0.05 1.20 0.04 1.01 0.04
g 20801 1.009 0.001 1.32 0.04 1.11 0.03 1.07 0.04
p 17233 1.057 0.001 1.23 0.04 1.13 0.04 1.12 0.03
y 16852 1.037 0.001 1.25 0.05 1.22 0.04 0.98 0.04
the 14404 1.033 0.002 1.40 0.04
of 6600 1.073 0.003 1.47 0.06
and 6428 0.962 0.002 1.23 0.04
a 4722 1.137 0.003 1.34 0.05
to 4619 1.023 0.003 1.15 0.04
in 4166 1.021 0.003 1.30 0.05
whale 1096 2.162 0.018 1.57 0.07
from 1085 1.143 0.006 1.15 0.04
man 476 1.252 0.007 1.21 0.04
them 474 1.214 0.012 1.16 0.04
sea 453 1.311 0.009 1.24 0.04
old 450 1.507 0.012 1.33 0.04
we 445 1.646 0.011 1.28 0.05
ship 438 1.522 0.012 1.31 0.04
ahab 436 3.056 0.021 1.53 0.06
ye 431 2.680 0.018 1.43 0.04
who 344 1.136 0.012 1.22 0.04
head 342 1.346 0.012 1.35 0.05
time 333 1.086 0.014 1.08 0.04
long 333 1.092 0.009 1.07 0.03
23
Table S7: Correlation γ̂ and burstiness στ/〈τ〉 obtained for
the diferrent binary sequences in the indicated book.
Book: pride; N=659,408
Original data Shuffling M1 Shuffling M2
sequence Ni στ/〈τ〉 error γ̂ error γ̂ error γ̂ error
vowels 203916 0.437 0.020 1.20 0.04
122194 0.450 0.008 1.41 0.05 1.41 0.05 1.41 0.05
e 69370 0.828 0.002 1.19 0.04 1.12 0.04 1.08 0.04
t 46645 0.872 0.002 1.10 0.05 1.09 0.04 1.02 0.03
a 41688 0.849 0.002 1.11 0.03 1.18 0.04 1.04 0.04
o 40041 0.891 0.001 1.18 0.04 1.09 0.03 1.00 0.04
i 37830 0.870 0.002 1.16 0.04 1.31 0.04 1.09 0.04
n 37689 0.884 0.001 1.13 0.04 1.16 0.04 1.09 0.03
h 34067 0.869 0.002 1.31 0.04 1.04 0.04 1.10 0.03
s 33114 0.956 0.001 1.06 0.03 1.11 0.03 1.06 0.03
r 32299 0.882 0.001 1.18 0.04 1.09 0.04 1.06 0.04
d 22303 0.917 0.002 1.15 0.04 1.11 0.03 1.05 0.03
l 21594 1.036 0.001 1.19 0.05 1.10 0.04 1.03 0.04
u 14987 0.971 0.002 1.26 0.04 1.17 0.04 1.03 0.03
m 14764 0.963 0.002 1.17 0.04 1.17 0.04 1.02 0.03
c 13461 1.005 0.002 1.27 0.06 1.20 0.04 1.04 0.04
y 12706 0.992 0.002 1.37 0.05 1.20 0.05 1.04 0.03
w 12305 0.949 0.002 1.23 0.04 1.14 0.04 1.06 0.04
f 11998 0.988 0.002 1.23 0.04 1.05 0.03 1.05 0.03
g 10031 0.949 0.002 1.06 0.04 1.17 0.04 1.03 0.03
b 9088 0.943 0.002 1.19 0.05 1.08 0.03 0.99 0.04
the 4331 1.083 0.003 1.24 0.04
to 4163 0.945 0.003 1.11 0.03
of 3609 0.974 0.003 1.21 0.04
and 3585 0.859 0.003 1.18 0.04
her 2225 1.592 0.015 1.31 0.04
i 2068 2.915 0.014 1.46 0.05
at 788 1.071 0.006 1.10 0.04
mr 786 1.218 0.007 1.32 0.06
they 601 1.459 0.010 1.26 0.04
elizabeth 597 1.192 0.027 1.17 0.06
or 300 1.026 0.010 1.00 0.03
bennet 294 2.047 0.034 1.37 0.07
who 284 1.148 0.010 1.06 0.03
miss 283 1.536 0.015 1.35 0.07
one 268 1.066 0.009 1.06 0.04
jane 264 1.741 0.016 1.29 0.06
bingley 257 3.166 0.019 1.45 0.08
we 253 1.546 0.013 1.26 0.04
own 183 1.078 0.015 1.06 0.04
lady 183 1.924 0.023 1.38 0.06
24
Table S8: Correlation γ̂ and burstiness στ/〈τ〉 obtained for
the diferrent binary sequences in the indicated book.
Book: quixote; N=2,080,431
Original data Shuffling M1 Shuffling M2
sequence Ni στ/〈τ〉 error γ̂ error γ̂ error γ̂ error
vowels 638882 0.430 0.020 1.26 0.04
402964 0.415 0.009 1.34 0.05 1.34 0.05 1.34 0.05
e 204300 0.840 0.002 1.25 0.04 1.25 0.04 1.03 0.03
t 157193 0.867 0.002 1.26 0.04 1.25 0.04 1.03 0.03
a 138706 0.841 0.002 1.25 0.04 1.28 0.04 1.09 0.03
o 136541 0.881 0.001 1.24 0.04 1.29 0.04 1.03 0.03
h 117821 0.852 0.002 1.23 0.04 1.10 0.04 1.06 0.03
n 115898 0.866 0.002 1.23 0.04 1.17 0.04 1.05 0.04
i 112746 0.881 0.001 1.26 0.04 1.19 0.04 1.04 0.03
s 106979 0.935 0.001 1.28 0.05 1.27 0.04 1.06 0.03
r 92501 0.910 0.001 1.27 0.04 1.28 0.04 1.06 0.03
d 76655 0.929 0.001 1.34 0.04 1.22 0.04 1.03 0.03
l 62107 1.108 0.002 1.24 0.04 1.28 0.05 1.01 0.03
u 46589 0.949 0.001 1.23 0.04 1.17 0.04 1.00 0.04
m 42945 0.992 0.001 1.21 0.04 1.25 0.04 1.04 0.05
f 38552 0.977 0.001 1.24 0.04 1.24 0.04 1.05 0.03
w 38209 0.986 0.001 1.29 0.04 1.21 0.04 1.05 0.03
c 37602 0.984 0.001 1.21 0.04 1.26 0.04 1.08 0.03
g 31927 0.988 0.001 1.22 0.04 1.23 0.04 1.03 0.03
y 31053 1.048 0.001 1.25 0.04 1.26 0.04 1.05 0.04
p 23880 1.069 0.001 1.23 0.04 1.26 0.04 1.11 0.03
the 20652 1.050 0.002 1.36 0.04
and 16835 0.908 0.002 1.22 0.05
to 13184 1.031 0.002 1.26 0.04
of 12173 1.033 0.002 1.26 0.04
that 7515 1.023 0.002 1.21 0.04
in 6716 1.023 0.002 1.11 0.03
by 2069 1.042 0.004 1.09 0.03
sancho 2063 3.762 0.025 1.63 0.05
or 2048 1.154 0.004 1.15 0.04
quixote 2002 3.214 0.016 1.55 0.06
other 609 1.072 0.008 1.11 0.04
knight 606 2.175 0.016 1.43 0.05
take 546 1.195 0.008 1.14 0.04
master 545 1.720 0.013 1.38 0.04
thy 510 2.252 0.017 1.35 0.05
senor 509 1.632 0.009 1.28 0.04
worship 470 2.337 0.012 1.37 0.04
here 467 1.237 0.007 1.14 0.04
god 467 1.169 0.011 1.12 0.04
way 466 1.056 0.006 1.07 0.04
25
Table S9: Correlation γ̂ and burstiness στ/〈τ〉 obtained for
the diferrent binary sequences in the indicated book.
Book: sawyer; N=369,222
Original data Shuffling M1 Shuffling M2
sequence Ni στ/〈τ〉 error γ̂ error γ̂ error γ̂ error
vowels 110026 0.432 0.020 1.23 0.04
71180 0.402 0.010 1.50 0.05 1.50 0.05 1.50 0.05
e 35603 0.864 0.002 1.30 0.04 1.05 0.05 0.99 0.04
t 28825 0.858 0.002 1.16 0.04 1.23 0.04 0.99 0.04
a 23478 0.858 0.002 1.17 0.04 1.03 0.03 1.13 0.04
o 23192 0.898 0.001 1.22 0.04 1.06 0.03 0.98 0.04
n 20146 0.866 0.002 1.12 0.04 1.23 0.04 1.07 0.03
h 19565 0.861 0.002 1.18 0.04 1.11 0.04 1.07 0.04
i 18811 0.910 0.002 1.16 0.05 1.23 0.04 1.05 0.03
s 17716 0.951 0.001 1.19 0.04 1.13 0.04 1.08 0.03
r 15247 0.917 0.002 1.30 0.05 1.13 0.04 1.10 0.05
d 14850 0.950 0.002 1.20 0.04 1.20 0.04 1.01 0.03
l 12136 1.086 0.002 1.17 0.04 1.14 0.04 1.06 0.03
u 8942 0.949 0.002 1.18 0.04 1.07 0.04 1.06 0.03
w 8042 0.949 0.002 1.13 0.03 1.18 0.04 1.12 0.04
m 7135 0.977 0.002 1.22 0.04 1.18 0.04 1.02 0.03
y 6725 1.043 0.002 1.36 0.04 1.04 0.03 1.00 0.04
g 6606 1.041 0.002 1.16 0.04 1.15 0.05 1.06 0.03
c 6497 1.030 0.003 1.23 0.05 1.09 0.05 1.16 0.04
f 6004 1.047 0.003 1.22 0.04 1.11 0.03 1.02 0.03
b 4958 0.959 0.003 1.10 0.04 1.25 0.04 1.02 0.03
the 3703 1.154 0.004 1.35 0.04
and 3105 1.008 0.003 1.21 0.04
a 1863 1.085 0.005 1.20 0.04
to 1727 1.054 0.004 1.14 0.03
of 1436 1.127 0.005 1.21 0.04
he 1197 1.770 0.015 1.40 0.04
tom 689 1.740 0.014 1.39 0.06
with 647 1.068 0.008 1.15 0.04
if 237 1.404 0.011 1.23 0.04
huck 223 3.228 0.024 1.46 0.07
boys 155 1.767 0.019 1.24 0.06
did 150 1.336 0.018 1.22 0.04
joe 133 2.248 0.051 1.38 0.06
never 131 1.185 0.017 1.14 0.04
boy 122 1.788 0.054 1.29 0.06
back 121 0.968 0.015 1.04 0.03
off 99 1.335 0.019 1.12 0.04
night 98 2.025 0.057 1.29 0.04
other 96 1.145 0.019 1.12 0.03
becky 96 2.701 0.036 1.55 0.10
26
Table S10: Correlation γ̂ and burstiness στ/〈τ〉 obtained for
the diferrent binary sequences in the indicated book.
Book: ulysses; N=1,453,586
Original data Shuffling M1 Shuffling M2
sequence Ni στ/〈τ〉 error γ̂ error γ̂ error γ̂ error
vowels 440676 0.456 0.020 1.61 0.06
265304 0.436 0.009 1.78 0.06 1.78 0.06 1.78 0.06
e 141465 0.855 0.002 1.28 0.04 1.30 0.06 1.11 0.03
t 100183 0.904 0.001 1.54 0.07 1.37 0.06 1.05 0.03
a 93129 0.877 0.001 1.32 0.05 1.25 0.06 1.06 0.03
o 91403 0.930 0.001 1.19 0.04 1.35 0.05 1.10 0.04
i 81407 0.914 0.001 1.44 0.07 1.33 0.06 1.21 0.05
n 80138 0.897 0.001 1.34 0.05 1.27 0.04 1.26 0.06
s 76915 0.950 0.001 1.40 0.06 1.31 0.06 1.23 0.06
h 72550 0.906 0.002 1.61 0.06 1.23 0.05 1.44 0.08
r 69852 0.918 0.001 1.49 0.06 1.22 0.04 1.30 0.06
l 55052 1.074 0.001 1.41 0.06 1.39 0.06 1.19 0.05
d 49093 0.980 0.001 1.44 0.05 1.17 0.04 1.04 0.04
u 33272 0.982 0.001 1.25 0.04 1.36 0.06 1.16 0.04
m 31535 1.025 0.001 1.29 0.05 1.35 0.05 1.05 0.03
c 29894 1.072 0.001 1.62 0.07 1.31 0.04 1.36 0.07
g 27791 1.031 0.001 1.36 0.06 1.24 0.05 1.19 0.04
f 26638 1.025 0.001 1.30 0.05 1.22 0.04 1.12 0.03
w 26164 1.056 0.001 1.53 0.07 1.32 0.05 1.31 0.06
y 24251 1.032 0.001 1.36 0.05 1.15 0.03 1.01 0.03
p 22440 1.124 0.002 1.46 0.06 1.27 0.05 1.25 0.05
the 14952 1.071 0.002 1.44 0.06
of 8141 1.121 0.003 1.63 0.07
and 7217 1.167 0.003 1.53 0.05
a 6518 1.144 0.003 1.24 0.04
to 4963 1.157 0.003 1.38 0.07
in 4946 1.002 0.002 1.16 0.04
were 510 1.461 0.013 1.27 0.04
stephen 505 4.955 0.099 1.64 0.06
we 425 2.427 0.085 1.25 0.04
man 415 1.388 0.019 1.16 0.04
into 330 1.179 0.011 1.14 0.04
eyes 329 1.921 0.013 1.21 0.04
where 310 1.214 0.014 1.11 0.03
hand 308 1.295 0.017 1.18 0.04
street 293 1.394 0.013 1.21 0.04
our 291 1.556 0.018 1.23 0.04
first 278 1.306 0.011 1.19 0.04
father 277 1.631 0.013 1.62 0.05
day 250 1.131 0.012 1.10 0.03
just 249 2.014 0.012 1.20 0.04
27
Table S11: Correlation γ̂ and burstiness στ/〈τ〉 obtained for
the diferrent binary sequences in the indicated book.
Book: wrnpc; N=3,082,079
Original data Shuffling M1 Shuffling M2
sequence Ni στ/〈τ〉 error γ̂ error γ̂ error γ̂ error
vowels 945097 0.430 0.020 1.55 0.05
565161 0.426 0.009 1.50 0.05 1.50 0.05 1.50 0.05
e 312626 0.834 0.002 1.39 0.05 1.35 0.04 1.05 0.03
t 224180 0.886 0.001 1.40 0.04 1.27 0.04 1.09 0.03
a 204154 0.869 0.002 1.42 0.05 1.18 0.04 1.05 0.03
o 191126 0.904 0.001 1.45 0.05 1.40 0.04 1.04 0.04
n 182910 0.860 0.002 1.28 0.04 1.43 0.05 1.17 0.04
i 172403 0.894 0.001 1.48 0.05 1.46 0.05 1.21 0.04
h 166290 0.852 0.002 1.50 0.05 1.26 0.04 1.28 0.05
s 161889 0.955 0.001 1.32 0.04 1.47 0.05 1.04 0.03
r 146667 0.919 0.001 1.38 0.04 1.34 0.04 1.10 0.03
d 117632 0.923 0.001 1.48 0.05 1.33 0.04 1.05 0.03
l 95888 1.064 0.001 1.26 0.04 1.30 0.04 1.04 0.04
u 64788 0.971 0.001 1.26 0.04 1.26 0.04 1.04 0.03
m 61162 1.018 0.001 1.30 0.04 1.26 0.04 0.98 0.04
c 60576 1.009 0.001 1.54 0.05 1.39 0.04 1.17 0.04
w 58852 0.978 0.001 1.28 0.04 1.29 0.04 1.24 0.05
f 54419 1.064 0.001 1.49 0.05 1.23 0.04 1.05 0.03
g 50819 1.014 0.001 1.49 0.05 1.37 0.04 1.08 0.04
y 45847 1.035 0.001 1.36 0.04 1.34 0.04 1.01 0.04
p 44680 1.080 0.001 1.48 0.05 1.34 0.04 1.12 0.03
the 34495 1.128 0.002 1.59 0.05
and 22217 0.874 0.002 1.22 0.04
to 16640 1.056 0.001 1.24 0.04
of 14864 1.168 0.002 1.59 0.05
a 10525 1.119 0.002 1.21 0.04
he 9860 1.893 0.006 1.44 0.06
so 1900 1.180 0.005 1.14 0.03
prince 1890 3.862 0.026 1.68 0.06
pierre 1796 6.563 0.042 1.78 0.06
an 1625 1.131 0.004 1.17 0.04
could 1115 1.285 0.005 1.15 0.05
natasha 1098 6.334 0.036 1.74 0.06
man 1081 1.407 0.007 1.33 0.04
will 1066 1.530 0.011 1.36 0.04
andrew 1047 4.321 0.021 1.71 0.06
do 1037 1.273 0.010 1.16 0.04
time 926 1.100 0.008 1.13 0.03
princess 915 5.431 0.033 1.72 0.06
face 893 1.445 0.007 1.25 0.04
french 879 2.287 0.029 1.53 0.05
