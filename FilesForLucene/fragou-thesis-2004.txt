 
 
ΑΡΙΣΤΟΤΕΛΕΙΟ ΠΑΝΕΠΙΣΤΗΜΙΟ ΘΕΣΣΑΛΟΝΙΚΗΣ 
 
 
ΠΟΛΥΤΕΧΝΙΚΗ ΣΧΟΛΗ 
 
ΤΜΗΜΑ ΗΛΕΚΤΡΟΛΟΓΩΝ ΜΗΧΑΝΙΚΩΝ & ΜΗΧΑΝΙΚΩΝ 
ΥΠΟΛΟΓΙΣΤΩΝ 
 
∆Ι∆ΑΚΤΟΡΙΚΗ ∆ΙΑΤΡΙΒΗ 
 
 
Κατηγοριοποίηση και Τµηµατοποίηση Κειµένων µε χρήση µεθόδων 
Υπολογιστικής Νοηµοσύνης 
 
 
 
Παυλίνα  Α. Φράγκου 
 
 
Επιβλέπων: καθ. Βασίλειος Πετρίδης 
 
 
ΘΕΣΣΑΛΟΝΙΚΗ 2004 
 
 
 
ARISTOTLE UNIVERSITY OF THESSALONIKI 
 
 
SCHOOL OF ENGINEERING 
 
DEPARTMENT OF ELECTRICAL AND COMPUTER ENGINEERING  
 
 
DOCTORAL DISSERTATION 
 
 
Classification and Segmentation of Texts using methods of Computational 
Linguistics 
 
 
 
Pavlina A. Fragkou 
 
 
Supervisor : prof. Vassilios Petridis 
 
THESSALONIKI 2004 
 
 
 
Κατηγοριοποίηση και Τµηµατοποίηση Κειµένων µε χρήση µεθόδων 
Υπολογιστικής Νοηµοσύνης 
 
∆ιδακτορική ∆ιατριβή   Παυλίνας Α. Φράγκου  
 
Επιβλέπων: 
Πετρίδης Βασίλειος, καθηγητής, Τµήµα Ηλεκτρολόγων Μηχανικών & Μηχανικών 
Υπολογιστών, Πολυτεχνική Σχολή, Αριστοτέλειο Πανεπιστήµιο Θεσσαλονίκης.  
Στην Τριµελή Συµβουλευτική Επιτροπή, εκτός από τον επιβλέποντα καθηγητή, 
συµµετείχαν: 
Χασάπης Γεώργιος, καθηγητής, Τµήµα Ηλεκτρολόγων Μηχανικών & Μηχανικών 
Υπολογιστών, Πολυτεχνική Σχολή, Αριστοτέλειο Πανεπιστήµιο Θεσσαλονίκης. 
Πέτρου Λουκάς, αναπλ.καθηγητής, Τµήµα Ηλεκτρολόγων Μηχανικών & Μηχανικών 
Υπολογιστών, Πολυτεχνική Σχολή, Αριστοτέλειο Πανεπιστήµιο Θεσσαλονίκης.  
 
 
Στην Επταµελή Εξεταστική Επιτροπή, εκτός από τα µέλη της Τριµελούς Συµβουλευτικής 
Επιτροπής, συµµετείχαν: 
Στρίντζης Μιχάλης – Γεράσιµος, καθηγητής, Τµήµα Ηλεκτρολόγων Μηχανικών & 
Μηχανικών Υπολογιστών, Πολυτεχνική Σχολή, Αριστοτέλειο Πανεπιστήµιο Θεσσαλονίκης. 
Θεοχάρης Ιωάννης, αναπλ.καθηγητής, Τµήµα Ηλεκτρολόγων Μηχανικών & Μηχανικών 
Υπολογιστών, Πολυτεχνική Σχολή, Αριστοτέλειο Πανεπιστήµιο Θεσσαλονίκης.  
Μήτκας Περικλής, αναπλ.καθηγητής, Τµήµα Ηλεκτρολόγων Μηχανικών & Μηχανικών 
Υπολογιστών, Πολυτεχνική Σχολή, Αριστοτέλειο Πανεπιστήµιο Θεσσαλονίκης.  
Κεχαγίας Αθανάσιος, Λέκτορας, Γενικό Τµήµα, Πολυτεχνική Σχολή, Αριστοτέλειο 
Πανεπιστήµιο Θεσσαλονίκης. 
 
 
Περίληψη 
 
Η παρούσα διδακτορική διατριβή πραγµατεύεται την ανάπτυξη υπολογιστικών µεθόδων για 
την εµβάθυνση στο περιεχόµενο των κειµένων και την ανάδειξη του τρόπου δόµησής τους (µε την 
εύρεση των υποθεµάτων από τα οποία αποτελούνται) άρα και κατ’ επέκταση τη βελτίωση πρόσβασης 
σε πληροφορία µε τη βοήθεια γλωσσικής τεχνολογίας. Η εν λόγω διατριβή αφορά την χρήση του 
θησαυρού όρων Wordnet για την ακριβέστερη απόδοση της έννοιας των λέξεων µέσα στο περιεχόµενο 
στο οποίο απαντώνται, αλλά και την αποτελεσµατικότερη πρόσβαση στην πληροφορία µε την 
τµηµατοποίηση µεγάλης έκτασης κειµένων σε µικρότερα τµήµατα καθένα από τα οποία αναφέρεται 
σε ένα συγκεκριµένο θέµα. Μια τέτοιου είδους βελτιωµένη πρόσβαση είναι χρήσιµη στην ολοένα 
αυξανόµενη πληροφορία που απαντάται στις µέρες µας κυρίως στο ∆ιαδίκτυο. 
Μετά από επισκόπηση των µοντέλων και µεθόδων για την εύρεση της εννοιολογικής δοµής 
των κειµένων και τη βελτίωση πρόσβασης σε πληροφορία προτείνονται τρία µοντέλα. Το πρώτο από 
αυτά ακολουθεί την προσέγγιση της Mηχανικής Mάθησης και πραγµατοποιεί κατηγοριοποίηση 
κειµένων µε την βοήθεια της έννοιας της κάθε λέξης -όπως αυτή προσδιορίζεται από το περιεχόµενο 
µέσα στο οποίο αυτή απαντάται και όπως αυτή δίνεται από τον θησαυρού όρων Wordnet - και όχι των 
αυτούσιων λέξεων του κειµένου. Το δεύτερο µοντέλο πραγµατεύεται την τµηµατοποίηση κειµένων µε 
την βοήθεια τεχνικών κατηγοριοποίησης κειµένων. Τέλος, το τελευταίο προτεινόµενο µοντέλο 
προτείνει και υλοποιεί ένα µοντέλο τµηµατοποίησης κειµένων µεγάλης έκτασης σε µικρότερα 
τµήµατα καθένα από τα οποία παρουσιάζει ισχυρή συνάφεια και συνοχή σε τοπικό επίπεδο. Η εν 
λόγω τµηµατοποίηση πραγµατοποιείται ως συνδυασµός τεχνικών εύρεσης της οµοιότητας µεταξύ των 
διαφόρων µερών του κειµένου και αυτόµατου καθορισµού των ορίων µεταξύ των τµηµάτων.  
Η επιτυχία και των τριών µοντέλων επιβεβαιώνεται από την εφαρµογή τους σε αντίστοιχα 
σώµατα κειµένων µε πιο σηµαντικό το σώµα κειµένων το οποίο απαρτίζεται από ελληνικά κείµενα. Η 
σπουδαιότητα των εν λόγω µοντέλων έγκειται στο γεγονός ότι αποτελούν ισχυρά βοηθήµατα σε ένα 
ευρύ πεδίο εφαρµογών όπως η ακριβέστερη ανάκτηση και εξόρυξη πληροφορίας, η εξαγωγή 
περιλήψεων, η θεµατική κατηγοριοποίηση κειµένων κλπ, τόσο σε αγγλικά όσο και σε ελληνικά 
κείµενα.  
 
 i
 
Abstract 
 
This dissertation deals with the development of computational methods which penetrate into the 
content of texts and reveal their structure, as the result of finding their topics and subtopics. The 
benefit of such methods is the improvement in accessing in information using a language technology. 
The dissertation uses Wordnet’s thesaurus in order to attribute -in a more accurate way- the sense of a 
word taking under consideration the content in which this word appears in the text. The dissertation 
achieves a more effective access in information using the result of the segmentation of large texts into 
smaller segments each of which refers to a specific topic. Such an improved access is extremely useful 
while searching the Web. 
After an overview of the models and methods that have been  proposed in the literature for the 
problem of finding the semantic structure of a text, we propose and implement three models. The first 
of those follows the Machine Learning approach and classifies texts using the appropriate sense of the 
words appearing in a text – taken from Wordnet’s thesaurus-, according to the content in which each 
word appears. This classification is compared to the one using the original words of the texts. The 
second model implements text segmentation using the outcome of classification. Finally, the third 
model suggests a method for segmenting large texts into smaller segments, each of which exhibiting 
strong cohesion and coherence in a topical level. The aforementioned segmentation is realized by the 
combination of a technique which calculates the similarity between parts of a text and a technique 
which automatically determines segment boundaries.  
The success of the aforementioned models was validated after their evaluation in an important 
number of datasets, the most important of which consists of greek texts. The importance of those 
models lies in the fact that they can play a key role in a wide area of  language processing applications 
such as effective and accurate Web search, information retrieval, information extraction, 
summarization, thematic text classification etc., both to greek and english copora.  
 
 
 ii
 
Ευχαριστίες 
 
Όπως και κάθε είδους ερευνητική εργασία, η εν λόγω διατριβή δεν θα µπορούσε να 
πραγµατοποιηθεί δίχως την πολύτιµη βοήθεια ενός σηµαντικού αριθµού ατόµων τα οποία συνέβαλαν 
µε τον δικό τους τρόπο στην ολοκλήρωσή της. Σε όλους αυτούς τους ανθρώπους οι οποίοι µε 
στήριξαν είτε σε επιστηµονικό, είτε σε τεχνικό είτε σε συναισθηµατικό αλλά είτε και σε οικονοµικό 
επίπεδο είναι αφιερωµένη από καρδιάς η εν λόγω διατριβή. 
Πρώτα από όλα θέλω να εκφράσω την ευγνωµοσύνη µου στον επιβλέποντα καθηγητή µου κο 
Βασίλειο Πετρίδη, ο οποίος µε τίµησε µε την εµπιστοσύνη του για αυτό το εγχείρηµα καθώς και για 
την ανεκτίµητη καθοδήγηση και υποστήριξη καθόλη τη διάρκεια εκπόνησης της διατριβής.  
Η παρούσα διατριβή δεν θα είχε ποτέ ολοκληρωθεί δίχως τη συµβολή του Λέκτορα του Γενικού 
Τµήµατος της Πολυτεχνικής Σχολής του Αριστοτελείου Πανεπιστηµίου κου Αθανάσιου Κεχαγιά. Το 
ευχαριστώ δεν επαρκεί για τις ατελείωτες ώρες που σπατάλησε από τον πολύτιµο χρόνο του αλλά και 
την ηθική συµπαράσταση στις δύσκολες στιγµές.  
Οφείλω να ευχαριστήσω όλα τα µέλη της Τριµελούς Επιτροπής, τους καθηγητές του Τµήµατος, 
τον καθηγητή του Τµήµατος Πληροφορικής κο Ιωάννη Μανωλόπουλο καθώς και τους συνεργάτες 
Βασίλειο Καµπουρλάζο και Σπύρο Καζαρλή. Πολύτιµη αποτελεί και η συνεισφορά του Γεώργιου 
Ορφανού και του καθηγητή Ευστάθιου Σταµατάτου για την ευγενική παραχώρηση της διδακτορικής 
διατριβής και του σώµατος ελληνικών κειµένων αντίστοιχα.  
Θεωρώ τον εαυτό µου ιδιαίτερα τυχερό µιας και καθόλη την προσπάθειά µου είχα τη 
συµπαράσταση και ενθάρρυνση πολλών και πιστών φίλων. Οφείλω να ευχαριστήσω ιδιαίτερα τους 
Αντώνη Σιδηρόπουλο, Αρτέµη Βογιατζή, Μιχάλη Χριστοδούλου, Λέανδρο Μαγλαρά καθώς και τις 
επιστήθιες φίλες µου Μαίρη Φιλιππάκου, Χριστίνα Πάχου, Βιολέτα Κοτσώτα, Άννα Παπαδοπούλου 
και Εβίτα Φωστήρα. Ελπίζω ειλικρινά να µην ξέχασα κάποιον. 
 Τέλος, οφείλω να εκφράσω την ευγνωµοσύνη µου στους γονείς µου και την αδελφή µου που µε 
την αγάπη τους µε ενθάρρυναν και µε στήριξαν από κάθε πλευρά σε αυτή την προσπάθεια όλα αυτά 
τα χρόνια, καθώς και τον σύντροφο της ζωής µου Κωνσταντίνο Τσαρχά για την αγάπη του και την 
απεριόριστη κατανόησή του κάθε φορά που του έλεγα «Κώστα, έχω διάβασµα». 
 iii
 
Περιεχόµενα 
Κεφάλαιο 1 Eισαγωγή ........................................................................................................... 1 
1.1 Εισαγωγή......................................................................................................................... 1 
1.2 Αντικείµενο της διατριβής ............................................................................................. 4 
1.3  Συνεισφορά της διατριβής ............................................................................................ 6 
1.4  ∆οµή της διατριβής........................................................................................................ 8 
1.5 ∆ηµοσιεύσεις ................................................................................................................. 10 
Κεφάλαιο 2  Κατηγοριοποίηση Κειµένων µε χρήση λέξεων και εννοιών.......................... 12 
2.1 Εισαγωγή....................................................................................................................... 12 
2.1.1 Κατηγοριοποίηση κειµένων .................................................................................... 12 
2.1.1.1 Κατηγοριοποιητής Bayesian ∆ικτύου .............................................................. 14 
2.1.1.2 ∆έντρα απόφασης ............................................................................................. 14 
2.1.1.3 Κανόνες Απόφασης .......................................................................................... 15 
2.1.1.4 Τεχνητά Νευρωνικά ∆ίκτυα.............................................................................. 15 
2.1.1.5 Γενετικοί Αλγόριθµοι ....................................................................................... 16 
2.1.1.6 Support Vector Machines ................................................................................. 17 
2.1.1.7 Κατηγοριοποιητές βασιζόµενοι σε στιγµιότυπα– Rocchio .............................. 17 
2.1.2 Λέξεις και Έννοιες................................................................................................... 18 
2.1.3 Το πρόβληµα που εξετάζουµε ................................................................................. 19 
2.2  Περιγραφή του θησαυρού όρων Wordnet................................................................. 20 
2.2.1 Ο θησαυρός όρων Wordnet ..................................................................................... 20 
2.2.2 Το Αλφαβητικό Ευρετήριο Σηµασιών του Brown Corpus ...................................... 21 
2.3 Σχετικές Εργασίες Κατηγοριοποίησης µε χρήση του Wordnet ............................... 22 
2.4 Παράσταση του κειµένου - Η προσέγγισή µας .......................................................... 25 
2.5 Αλγόριθµοι κατηγοριοποίησης .................................................................................... 27 
2.5.1  Κατηγοριοποίηση Μέγιστης Ύστερης Πιθανότητας .............................................. 27 
2.5.1.1 Αλγόριθµος Μέγιστης Ύστερης  Πιθανότητας (Μαζική Παραλλαγή)............. 28 
2.5.1.2 Αλγόριθµος Μέγιστης Ύστερης  Πιθανότητας (Aναδροµική Παραλλαγή) ..... 29 
2.5.2 Κατηγοριοποίηση Μέγιστης Πιθανοφάνειας .......................................................... 30 
2.5.3 Αλγόριθµος Κ-Πλησιέστερων Γειτόνων ................................................................. 31 
2.5.3.1 Πρώτη Παραλλαγή Αλγορίθµου Κ-Πλησιέστερων Γειτόνων.......................... 31 
2.5.3.2 ∆εύτερη Παραλλαγή Αλγορίθµου Κ-Πλησιέστερων Γειτόνων ....................... 32 
2.5.3.3 Τρίτη Παραλλαγή Αλγορίθµου Κ-Πλησιέστερων Γειτόνων............................ 33 
2.5.3.4 Τέταρτη Παραλλαγή Αλγορίθµου Κ-Πλησιέστερων Γειτόνων........................ 34 
2.5.4 σ -FLN µε Ψηφοφορία............................................................................................ 34 
2.6 Πειράµατα ..................................................................................................................... 38 
2.6.1 Πρώτη σειρά πειραµάτων......................................................................................... 38 
2.6.1.1 Το σώµα κειµένων προς κατηγοριοποίηση ...................................................... 38 
2.6.1.2 Τα δεδοµένα προς εκπαίδευση και προς επαλήθευση ...................................... 39 
2.6.1.3 Παραστάσεις των κειµένων.............................................................................. 40 
2.6.1.4 Μέθοδοι Κατηγοριοποίησης ............................................................................ 41 
2.6.1.5 Αποτελέσµατα Κατηγοριοποίησης ................................................................... 43 
2.6.1.6 Συµπεράσµατα.................................................................................................. 50 
iv 
   
2.6.2 ∆εύτερη σειρά πειραµάτων ..................................................................................... 51 
2.6.2.1  Το σώµα κειµένων προς κατηγοριοποίηση ..................................................... 51 
2.6.2.2 Τα δεδοµένα προς εκπαίδευση και επαλήθευση............................................... 53 
2.6.2.3 Παραστάσεις των κειµένων.............................................................................. 54 
2.6.2.4 Μέθοδοι Κατηγοριοποίησης ............................................................................ 54 
2.6.2.5 Αποτελέσµατα Κατηγοριοποίησης ................................................................... 56 
2.6.2.6 Συµπεράσµατα.................................................................................................. 61 
2.7 Αξιολόγηση αποτελεσµάτων ........................................................................................ 70 
Κεφάλαιο 3     Τµηµατοποίηση Κειµένων- Βιβλιογραφικές Αναφορές............................... 72 
3.1  Εισαγωγή...................................................................................................................... 72 
3.1.1 Ορισµός του Προβλήµατος ..................................................................................... 73 
3.1.2 Επίλυση του προβλήµατος ...................................................................................... 73 
3.2 Θεωρία των Halliday και Hasan................................................................................. 74 
3.3 Γλωσσολογική προσέγγιση εύρεσης της δοµής ενός κειµένου ................................. 75 
3.4 Στατιστική προσέγγιση εύρεσης της δοµής ενός κειµένου ....................................... 76 
3.4.1 Γραµµική Τµηµατοποίηση Κειµένου ...................................................................... 76 
3.4.1.1 Εξέταση  της Εµφάνισης των Λέξεων µέσα σε ένα κείµενο ............................ 77 
3.4.1.2 Μηχανισµοί Εύρεσης της Οµοιότητας µεταξύ γειτονικών τµηµάτων ενός 
κειµένου........................................................................................................................ 78 
3.4.1.3 Μηχανισµοί Εύρεσης της Οµοιότητας µεταξύ όλων των τµηµάτων ενός 
κειµένου........................................................................................................................ 79 
3.4.2 Ιεραρχική Τµηµατοποίηση Κειµένου ...................................................................... 81 
3.4.3 Εννοιολογικό ∆ίκτυο ............................................................................................... 82 
3.4.4  Τεχνικές Εύρεσης των ορίων µεταξύ των τµηµάτων ενός κειµένου ...................... 83 
3.4.4.1 Γραφική Παράσταση ........................................................................................ 83 
3.4.4.2 ∆ιαιρετική Οµαδοποίηση ................................................................................. 84 
3.4.4.3 Γράφος.............................................................................................................. 84 
3.4.4.4 Αλγόριθµοι Μηχανικής Μάθησης .................................................................... 84 
3.4.4.5 ∆υναµικός Προγραµµατισµός .......................................................................... 85 
3.4.4.6 Απόδοση Συµπίεσης ......................................................................................... 86 
3.4.4.7 Hidden Markov Models.................................................................................... 87 
3.5 Μέτρα αξιολόγησης της τµηµατοποίησης ................................................................. 88 
3.6 Η δική µας προσέγγιση ................................................................................................ 91 
Κεφάλαιο 4   Τµηµατοποίηση Κειµένων βάσει του Αποτελέσµατος Κατηγοριοποίησης . 94 
4.1 Εισαγωγή....................................................................................................................... 94 
4.2 Τµηµατοποίηση µε χρήση του αποτελέσµατος της κατηγοριοποίησης................... 95 
4.2.1  Παράσταση του κειµένου ....................................................................................... 95 
4.2.2 Ο αλγόριθµος PREMONN ...................................................................................... 97 
4.2.2.1 Πρώτη παραλλαγή του αλγορίθµου PREMONN............................................. 98 
4.2.2.1.1  Πρώτη εναλλακτική µορφή του αλγορίθµου PREMONN....................... 99 
4.2.2.1.2 ∆εύτερη εναλλακτική µορφή του αλγορίθµου PREMONN.................... 101 
4.2.2.2 ∆εύτερη παραλλαγή του αλγορίθµου PREMONN......................................... 102 
4.2.3 Τεχνική «Ψηφοφορίας» ......................................................................................... 103 
4.3 Πειράµατα ................................................................................................................... 104 
4.3.1 Το σώµα κειµένων ................................................................................................. 104 
 v
   
4.3.2 ∆ιαδικασία τµηµατοποίησης µε χρήση του αποτελέσµατος κατηγοριοποίησης... 108 
4.3.3 Πρώτη σειρά πειραµάτων...................................................................................... 109 
4.3.4 ∆εύτερη σειρά πειραµάτων ................................................................................... 112 
4.3.5 Τρίτη σειρά πειραµάτων........................................................................................ 114 
4.4 Συµπεράσµατα ............................................................................................................ 123 
Κεφάλαιο 5  Αλγόριθµος Τµηµατοποίησης µε ∆υναµικό Προγραµµατισµό................... 126 
5.1  Εισαγωγή.................................................................................................................... 126 
5.2 Ο Αλγόριθµος Τµηµατοποίησης µε ∆υναµικό Προγραµµατισµό ........................... 127 
5.2.1 Η Παράσταση ........................................................................................................ 127 
5.2.2 Η Συνάρτηση Κόστους.......................................................................................... 129 
5.2.3 ∆υναµικός Προγραµµατισµός ............................................................................... 132 
∆υναµικός Προγραµµατισµός του Αλγορίθµου Τµηµατοποίησης Κειµένου µε Ρήτρα 
Μήκους ....................................................................................................................... 132 
5.3 Πειράµατα ................................................................................................................... 135 
5.3.1 Κριτήρια µέτρησης της ακρίβειας τµηµατοποίησης ............................................. 136 
5.3.2 Πρώτη Οµάδα Πειραµάτων................................................................................... 136 
5.3.2.1 Πρώτο Γκρουπ Πειραµάτων........................................................................... 137 
5.3.2.3 Τρίτο Γκρουπ Πειραµάτων............................................................................. 156 
5.3.2.3.1 Ορισµός του προβλήµατος ...................................................................... 156 
5.3.2.3.2 Μοντέλα ∆ιαχωρισµού Γινοµένου .......................................................... 157 
5.3.2.3.3 Αποτελέσµατα Τµηµατοποίησης µε χρήση «Μοντέλων ∆ιαχωρισµού 
Γινοµένου» ............................................................................................................. 160 
5.3.3 ∆εύτερη Οµάδα Πειραµάτων ................................................................................ 163 
5.3.4 Παρατηρήσεις........................................................................................................ 170 
5.4 Συµπεράσµατα ............................................................................................................ 172 
Κεφάλαιο 6   Τµηµατοποίηση Ελληνικών Κειµένων........................................................ 175 
6.1 Εισαγωγή..................................................................................................................... 175 
6.2 Το Σώµα Ελληνικών Κειµένων της εφηµερίδας «Το Βήµα» .................................. 176 
6.3 Ο Μορφοσυντακτικός Αναλυτής της Νέας Ελληνικής............................................ 178 
6.4 Πειράµατα ................................................................................................................... 181 
6.4.1 Προεπεξεργασία .................................................................................................... 181 
6.4.2 Πρώτη Σειρά Πειραµάτων..................................................................................... 182 
6.4.3 ∆εύτερη Σειρά Πειραµάτων .................................................................................. 190 
6.5 Συµπεράσµατα ............................................................................................................ 193 
Kεφαλαιο 7     Συµπεράσµατα............................................................................................ 196 
7.1 Αποτίµηση του έργου ................................................................................................. 196 
7.2 Ανοιχτά θέµατα ........................................................................................................... 200 
7.3 Μελλοντικές Κατευθύνσεις ....................................................................................... 201 
Βιβλιογραφικές Αναφορές ..................................................................................................... 204 
Γλωσσάρι ............................................................................................................................... 219 
Παράρτηµα ............................................................................................................................ 225 
 vi
   
Α1 H stop list για τα αγγλικά κείµενα....................................................................... 225 
Α2 Το Σώµα Κειµένων του Freddy Choi.................................................................. 226 
A3       Το Σώµα Κειµένων της Εφηµερίδας «Το Βήµα» ............................................... 236 
 
Ευρετήριο Σχηµάτων 
 
Σχήµα 2.1: Μέσος όρος του ποσοστού επιτυχίας κατηγοριοποίησης ως προς το 
κριτήριο του µέσου όρου απόδοσης για τις Οµάδα πειραµάτων Α και C της 1ης 
σειράς µε χρήση των 3 κατηγοριών ..................................................................... 49 
Σχήµα 2.2: Μέσος όρος του ποσοστού επιτυχίας κατηγοριοποίησης ως προς το 
κριτήριο του µέσου όρου απόδοσης για τις Οµάδα πειραµάτων B και D της 1ης 
σειράς µε χρήση των 3 κατηγοριών ..................................................................... 49 
Σχήµα 2.3:  Ποσοστό επιτυχίας κατηγοριοποίησης ως προς το κριτήριο του µέσου όρου 
απόδοσης (για τους αλγορίθµους κατηγοριοποίησης που χρησιµοποιούνται) στην 2η 
σειρά πειραµάτων και των παραστάσεων κειµένου που χρησιµοποιεί καθένας από 
αυτούς. 61 
Σχήµα 2.4:  Απόδοση του αλγορίθµου κατηγοριοποίησης  Μέγιστης Ύστερης 
Πιθανότητας (Μαζική παραλλαγή) στο σύνολο κειµένων του Brown Corpus. Στο 
σχήµα φαίνεται το ποσοστό των σωστά κατηγοριοποιηµένων κειµένων προς 
επαλήθευση και για τις δυο παραστάσεις (µε λέξεις και µε έννοιες) συναρτήσει της 
παραµέτρου α . Η παράσταση βασιζόµενη στις έννοιες υπερισχύει οριακά αυτής 
που βασίζεται στις λέξεις. .................................................................................... 64 
Σχήµα 2.5: Απόδοση του αλγορίθµου κατηγοριοποίησης Μέγιστης Ύστερης 
Πιθανότητας (αναδροµική παραλλαγή) στο σύνολο κειµένων του Brown Corpus. 
Στο σχήµα φαίνεται το ποσοστό των σωστά κατηγοριοποιηµένων κειµένων προς 
επαλήθευση και για τις δυο παραστάσεις (µε λέξεις και µε έννοιες) συναρτήσει της 
παραµέτρου α  και της παραµέτρου h η οποία λαµβάνει τη σταθερή τιµή 10 5− . Η 
παράσταση βασιζόµενη στις έννοιες υπερισχύει οριακά αυτής που βασίζεται στις 
λέξεις. Το σχήµα δεν περιλαµβάνει την απόδοση για διαφορετικές τιµές της 
παραµέτρου h. ...................................................................................................... 65 
Σχήµα 2.6: Απόδοση του αλγορίθµου Κατηγοριοποίησης Μέγιστης Πιθανοφάνειας στο 
σώµα κειµένων του Brown Corpus. Στο σχήµα φαίνεται το ποσοστό των σωστά 
κατηγοριοποιηµένων κειµένων προς επαλήθευση και για τις δυο παραστάσεις (µε 
λέξεις και µε έννοιες) συναρτήσει της παραµέτρουα . Η παράσταση που βασίζεται 
στις έννοιες υπερισχύει οριακά αυτής βάσει των  λέξεων. .................................. 65 
Σχήµα 2.7: Απόδοση του αλγορίθµου κατηγοριοποίησης Κ- Πλησιέστερων Γειτόνων 
(Τρίτη παραλλαγή) στο σώµα κειµένων του Brown Corpus. Στο σχήµα φαίνεται το 
ποσοστό των σωστά κατηγοριοποιηµένων κειµένων προς επαλήθευση και για τις 
τέσσερις δυνατές παραστάσεις (βασιζόµενες στις λέξεις, στις έννοιες, στην δυαδική 
παράσταση και στην σχετική συχνότητα) συναρτήσει της παραµέτρου Ρ. Οι 
δυαδικές παραστάσεις παρουσιάζουν σηµαντικά καλύτερη απόδοση έναντι των 
αντιστοίχων που βασίζονται στη σχετική συχνότητα, ενώ η παράσταση που 
βασίζεται στις έννοιες υπερισχύει οριακά αυτής που βασίζεται στις λέξεις........ 66 
Σχήµα 2.8:  Απόδοση του αλγορίθµου κατηγοριοποίησης  Κ-Πλησιέστερων Γειτόνων 
(Τέταρτη παραλλαγή) στο σύνολο κειµένων του Brown Corpus. Στο σχήµα φαίνεται 
το ποσοστό των σωστά κατηγοριοποιηµένων κειµένων προς επαλήθευση και για τις 
 vii
   
τέσσερις δυνατές παραστάσεις συναρτήσει της παραµέτρου Ρ. Οι δυαδικές 
παραστάσεις παρουσιάζουν σηµαντικά καλύτερη απόδοση έναντι των αντιστοίχων 
που βασίζονται στη σχετική συχνότητα, ενώ η παράσταση που βασίζεται στις 
έννοιες υπερισχύει οριακά αυτής που βασίζεται στις λέξεις................................ 67 
Σχήµα 2.9:  Απόδοση του αλγορίθµου κατηγοριοποίησης  σ-FLN-MAP µε ψηφοφορία 
στο σύνολο κειµένων του Brown Corpus. Στο σχήµα φαίνεται το ποσοστό των 
σωστά κατηγοριοποιηµένων κειµένων προς επαλήθευση συναρτήσει του αριθµού 
των ψηφοφόρων Vn  και της παραµέτρου «επαγρύπνισης» ρ=0.94. Και οι τέσσερις 
δυνατές παραστάσεις (βάσει των λέξεων, των εννοιών, της δυαδικής παρ. και της 
σχετικής συχνότητας) πρακτικά  παρουσίασαν την ίδια  περίπου απόδοση. ....... 67 
Σχήµα 2.10:  Απόδοση του αλγορίθµου κατηγοριοποίησης  σ-FLN-MAP µε ψηφοφορία 
στο σύνολο κειµένων του Brown Corpus. Στο σχήµα φαίνεται το ποσοστό των 
σωστά κατηγοριοποιηµένων κειµένων προς επαλήθευση συναρτήσει του αριθµού 
των ψηφοφόρων Vn =13 και της παραµέτρου «επαγρύπνισης» ρ. Οι παραστάσεις 
που βασίζονται στη σχετική συχνότητα παρουσιάζουν σηµαντικά καλύτερη 
απόδοση έναντι των αντιστοίχων δυαδικών, ενώ η παράσταση που βασίζεται στις 
έννοιες υπερισχύει οριακά αυτής βασιζόµενη στις λέξεις.................................... 68 
Σχήµα 2.11:  Απόδοση του αλγορίθµου κατηγοριοποίησης  σ-FLN-MAP µε ψηφοφορία 
στο σύνολο κειµένων του Brown Corpus. Στο σχήµα φαίνεται το ποσοστό των 
σωστά κατηγοριοποιηµένων κειµένων προς επαλήθευση συναρτήσει του αριθµού 
των ψηφοφόρων Vn και της παραµέτρου «επαγρύπνισης» ρ για τις παραστάσεις που 
βασίζονται τόσο στις έννοιες όσο και στις λέξεις. Η απόδοση κατηγοριοποίησης 
παραµένει πρακτικά σταθερή για ρ=0.94 και Vn =13.......................................... 69 
Σχήµα 2.12:  Ποσοστό ακρίβειας κατηγοριοποίησης όλων των ανεξάρτητων µονάδων 
σ-FLN-MAP και του σχήµατος  «σ-FLN-MAP µε ψηφοφορία» για την 
κατηγοριοποίηση του συνόλου κειµένων του Brown Corpus συναρτήσει του 
αριθµού των ψηφοφόρων Vn  χρησιµοποιώντας τις παραστάσεις που βασίζονται 
τόσο στις έννοιες όσο και στις λέξεις. Το σχήµα  «σ-FLN- µε ψηφοφορία» 
υποδηλώνει µια σταθερή βελτίωση έναντι των ανεξάρτητων µονάδων σ-FLN-MAP 
των οποίων η απόδοση κυµαίνεται σηµαντικά. ................................................... 69 
Σχήµα 4.1: Οι µέσοι όροι των τιµών του κριτηρίου kP  του Beeferman για τα σύνολα 
Set0, …, Set9 όπως αυτές ελήφθησαν µετά από την εφαρµογή της 1ης και της 2ης 
εναλλακτικής µορφής του αλγορίθµου του PREMONN µε και χωρίς την εφαρµογή 
της τεχνικής της ψηφοφορίας τόσο για την περίπτωση όπου χρησιµοποιούνται οι 
λέξεις όσο και για την περίπτωση όπου χρησιµοποιούνται οι έννοιες των κειµένων, 
όπως αυτά προκύπτουν και από τις τρεις σειρές πειραµάτων............................ 123 
Σχήµα 5.1: Γραφική παράσταση του πίνακα οµοιότητας D µεταξύ προτάσεων, ενός 
κειµένου το οποίο αποτελείται από 91 προτάσεις. Οι µονάδες παρίστανται µε µαύρα 
τετράγωνα ενώ τα µηδενικά µε άσπρα τετράγωνα............................................. 128 
Σχήµα 5.2 : Γραφική παράσταση του kP  σαν συνάρτηση των γ και r για τα κείµενα του 
Set0 για την πρώτη σειρά του πρώτου γκρουπ πειραµάτων. ............................. 141 
Σχήµα 5.3 : Γραφική παράσταση του kP  σαν συνάρτηση των γ και r για τα κείµενα του 
Set1 για την πρώτη σειρά του πρώτου γκρουπ πειραµάτων. ............................. 141 
Σχήµα 5.4: Γραφική παράσταση του kP  σαν συνάρτηση των γ και r για τα κείµενα του 
Set2 για την πρώτη σειρά του πρώτου γκρουπ πειραµάτων. ............................. 142 
Σχήµα 5.5: Γραφική παράσταση του kP  σαν συνάρτηση των γ και r για τα κείµενα του 
Set3 για την πρώτη σειρά του πρώτου γκρουπ πειραµάτων. ............................. 142 
 viii
   
Σχήµα 5.6: Γραφική παράσταση του kP  σαν συνάρτηση για τα υποσύνολα  Set0, Set1, 
Set2 και Set3 για το σύνολο των κειµένων του Choi οι οποίες προκύπτουν από τις 
καλύτερες τιµές των παραµέτρων γ και r της δεύτερης σειράς του πρώτου γκρουπ 
πειραµάτων (µετά την εφαρµογή της τεχνικής της επαλήθευσης) µε τη χρήση και 
των τεσσάρων συναρτήσεων κόστους τµηµατοποίησης.................................... 146 
Σχήµα 5.7 Οι τιµές των Precision, Recall και kP  για τα υποσύνολα  Set0, Set1, Set2 και 
Set3 για το σύνολο των κειµένων του Choi οι οποίες προκύπτουν από τις καλύτερες 
τιµές των παραµέτρων γ και r της δεύτερης σειράς του πρώτου γκρουπ πειραµάτων 
(µετά την εφαρµογή της τεχνικής της επαλήθευσης) µε τη χρήση και των τεσσάρων 
συναρτήσεων κόστους τµηµατοποίησης. ........................................................... 146 
Σχήµα 5.8: Γραφική παράσταση του kP  σαν συνάρτηση των γ και r για τα κείµενα του 
Set0 για την πρώτη σειρά του δεύτερου γκρουπ πειραµάτων............................ 149 
Σχήµα 5.9: Γραφική παράσταση του kP  σαν συνάρτηση των γ και r για τα κείµενα του 
Set1 για την πρώτη σειρά του δεύτερου γκρουπ πειραµάτων............................ 150 
Σχήµα 5.10: Γραφική παράσταση του kP  σαν συνάρτηση των γ και r για τα κείµενα 
του Set2 για την πρώτη σειρά του δεύτερου γκρουπ πειραµάτων. .................... 150 
Σχήµα 5.11: Γραφική παράσταση του kP  σαν συνάρτηση των γ και r για τα κείµενα 
του Set3 για την πρώτη σειρά του δεύτερου γκρουπ πειραµάτων. .................... 151 
Σχήµα 5.12: Γραφική παράσταση του kP  για τα υποσύνολα  Set0, Set1, Set2 και Set3 
για το σύνολο των κειµένων του Choi οι οποίες προκύπτουν από τις καλύτερες τιµές 
των παραµέτρων γ και r της δεύτερης σειράς του δεύτερου γκρουπ πειραµάτων 
(µετά την εφαρµογή της τεχνικής της επαλήθευσης) µε τη χρήση και των τεσσάτων 
συναρτήσεων κόστους τµηµατοποίησης. ........................................................... 154 
Σχήµα 5.13: Γραφική παράσταση των Precision, Recall και kP  για τα υποσύνολα  Set0, 
Set1, Set2 και Set3 για το σύνολο των κειµένων του Choi οι οποίες προκύπτουν από 
τις καλύτερες τιµές των παραµέτρων γ και r της δεύτερης σειράς του δεύτερου 
γκρουπ πειραµάτων (µετά την εφαρµογή της τεχνικής της επαλήθευσης) µε τη 
χρήση και των τεσσάτων συναρτήσεων κόστους τµηµατοποίησης. .................. 154 
Σχήµα 5.14 Σύγκριση των λαµβανόµενων τιµών του κριτηρίου kP  από διάφορους 
αλγορίθµους που εµφανίζονται στην βιβλιογραφία µε τα αντίστοιχα του τρίτου 
γκρουπ πειραµάτων του δικού µας αλγορίθµου, και της δεύτερης σειράς των 
πρώτων δυο γκρουπ εφαρµοζόµενοι πάνω σε καθένα από τα σύνολα Set0, Set1, Set2 
και Set3 των δεδοµένων του Choi...................................................................... 163 
Σχήµα 5.15: Γραφική παράσταση του kP  σαν συνάρτηση των γ και r για τα κείµενα 
του Set4 (δική µας συλλογή κειµένων). ............................................................. 166 
Σχήµα 5.16: Γραφική παράσταση του kP  σαν συνάρτηση των γ και r για τα κείµενα 
του Set7 (δική µας συλλογή κειµένων). ............................................................. 166 
Σχήµα 5.17: Γραφική παράσταση του kP  σαν συνάρτηση των γ και r για τα κείµενα 
του Set8 (δική µας συλλογή κειµένων). ............................................................. 167 
Σχήµα 5.18: Γραφική παράσταση του kP  σαν συνάρτηση των γ και r για τα κείµενα 
του Set9 (δική µας συλλογή κειµένων). ............................................................. 167 
Σχήµα 5.19: Γραφική παράσταση του kP  για τα σύνολα κειµένων Set0, Set1, …, Set9 
(της δικής µας συλλογής κειµένων) χρησιµοποιώντας τις επαληθευµένες τιµές των 
παραµέτρων. ....................................................................................................... 169 
 ix
   
Σχήµα 5.20: Γραφική παράσταση Τιµές των Precision, Recall και kP  για τα σύνολα 
κειµένων Set0, Set1, …, Set9 (της δικής µας συλλογής κειµένων) χρησιµοποιώντας 
τις επαληθευµένες τιµές των παραµέτρων. ........................................................ 169 
Σχήµα 6.1: Αρχιτεκτονική του Μορφοσυντακτικού Αναλυτή................................. 180 
Σχήµα 6.2: Η απόδοση του κριτηρίου kP  του Beeferman για το υποσύνολο Subset0 του 
συνόλου κειµένων Set5 για τις διάφορες τιµές των παραµέτρων γ  και r. ......... 185 
Σχήµα 6.3: Η απόδοση του κριτηρίου kP  του Beeferman για το υποσύνολο Subset1 του 
συνόλου κειµένων Set5 για τις διάφορες τιµές των παραµέτρων γ  και r. ......... 186 
Σχήµα 6.4: Η απόδοση του κριτηρίου kP  του Beeferman για το υποσύνολο Subset2 του 
συνόλου κειµένων Set5 για τις διάφορες τιµές των παραµέτρων γ  και r. ......... 186 
Σχήµα 6.5: Η απόδοση του κριτηρίου kP  του Beeferman για το υποσύνολο Subset3 του 
συνόλου κειµένων Set5 για τις διάφορες τιµές των παραµέτρων γ  και r. ......... 187 
Σχήµα 6.6: Η απόδοση του κριτηρίου kP  του Beeferman για τα σύνολα Set0, Set1, 
Set2, Set3, Set4 και Set5 χρησιµοποιώντας προτάσεις ως µονάδα τµήµατος, µετά 
από τη διαδικασία εκπαίδευσης και επαλήθευσης ............................................. 189 
Σχήµα 6.7: Οι µέσοι όροι των τιµών των κριτηρίων Precision, Recall και kP  του 
Beeferman για τις συλλογές Set0, Set1, Set2, Set3, Set4 και Set5 χρησιµοποιώντας 
προτάσεις ως µονάδα τµήµατος, όπως αυτές ελήφθησαν µετά από τη διαδικασία 
εκπαίδευσης και επαλήθευσης και οι αντίστοιχες της συλλογής κειµένων του Choi......190 
 
 
Ευρετήριο Πινάκων 
 
Πίνακας 2.1: Κατανοµή κειµένων προς εκπαίδευση και επαλήθευση σε κάθε µια από 
τις δεκαπέντε κατηγορίες και στα δυο προβλήµατα της 1ης σειράς πειραµάτων. 40 
Πίνακας 2.2 Λίστα των αλγορίθµων κατηγοριοποίησης που χρησιµοποιούνται και των 
παραστάσεων κειµένου που χρησιµοποιεί καθένας από αυτούς (∆Λ = ∆υαδικό 
Άνυσµα Λέξεων, ΣΛ= Άνυσµα Συχνότητας Λέξεων, ∆Ε = ∆υαδικό Άνυσµα Εννοιών, 
ΣΕ = Άνυσµα Συχνότητας Εννοιών) στην 1η σειρά πειραµάτων. ........................ 42 
Πίνακας 2.3 Ποσοστό επιτυχίας κατηγοριοποίησης ως προς το κριτήριο του µέσου 
όρου απόδοσης για την Οµάδα πειραµάτων Α και τους αλγορίθµους 
κατηγοριοποίησης που χρησιµοποιούνται στην 1η σειρά πειραµάτων και των 
παραστάσεων κειµένου που χρησιµοποιεί καθένας από αυτούς.......................... 45 
Πίνακας 2.4 Ποσοστό επιτυχίας κατηγοριοποίησης ως προς το κριτήριο του µέσου 
όρου απόδοσης για την Οµάδα πειραµάτων Β και τους αλγορίθµους 
κατηγοριοποίησης που χρησιµοποιούνται στην 1η σειρά πειραµάτων και των 
παραστάσεων κειµένου που χρησιµοποιεί καθένας από αυτούς.......................... 46 
Πίνακας 2.5 Ποσοστό επιτυχίας κατηγοριοποίησης ως προς το κριτήριο του µέσου 
όρου απόδοσης για την Οµάδα πειραµάτων C και τους αλγορίθµους 
κατηγοριοποίησης που χρησιµοποιούνται στην 1η σειρά πειραµάτων και των 
παραστάσεων κειµένου που χρησιµοποιεί καθένας από αυτούς.......................... 47 
Πίνακας 2.6 Ποσοστό επιτυχίας κατηγοριοποίησης ως προς το κριτήριο του µέσου 
όρου απόδοσης για την Οµάδα πειραµάτων D και τους αλγορίθµους 
 x
   
κατηγοριοποίησης που χρησιµοποιούνται στην 1η σειρά πειραµάτων και των 
παραστάσεων κειµένου που χρησιµοποιεί καθένας από αυτούς.......................... 48 
Πίνακας 2.7: Οι κατηγορίες της συλλογής κειµένων και το πλήθος των κειµένων ανά 
κατηγορία τόσο για τις αρχικές όσο και για τις καινούργιες κατηγορίες............. 52 
Πίνακας 2.8: Η κατανοµή των κειµένων για κάθε µια κατηγορία, σε κείµενα προς 
εκπαίδευση και κείµενα προς επαλήθευση. ......................................................... 53 
Πίνακας 2.9: Τα µεγέθη του λεξιλογίου τόσο των λέξεων όσο και των εννοιών για κάθε 
ένα από τα σύνολα δεδοµένων της 2ης σειράς πειραµάτων, όπως αυτά προκύπτουν 
από τον τρόπο αναπαράστασης των κειµένων ο οποίος περιγράφεται στην 
παράγραφο 2.4...................................................................................................... 55 
Πίνακας 2.10 Λίστα των αλγορίθµων κατηγοριοποίησης που χρησιµοποιούνται και των 
παραστάσεων κειµένου που χρησιµοποιεί καθένας από αυτούς (∆Λ = ∆υαδικό 
Άνυσµα Λέξεων, ΣΛ= Άνυσµα Συχνότητας Λέξεων, ∆Ε = ∆υαδικό Άνυσµα Εννοιών, 
ΣΕ = Άνυσµα Συχνότητας Εννοιών) στην 2η σειρά πειραµάτων. ........................ 55 
Πίνακας 2.11 Ποσοστό επιτυχίας κατηγοριοποίησης ως προς το κριτήριο του µέσου 
όρου απόδοσης (για τους αλγορίθµους κατηγοριοποίησης που χρησιµοποιούνται) 
στην 2η σειρά πειραµάτων και των παραστάσεων κειµένου που χρησιµοποιεί 
καθένας από αυτούς. ............................................................................................ 58 
Πίνακας 2.12: Ποσοστό επιτυχίας κατηγοριοποίησης του αλγόριθµου 
Κατηγοριοποίησης Μέγιστης Ύστερης Πιθανότητας (µαζικής παραλλαγής) της 2ης 
σειράς πειραµάτων.
 
.............................................................................................. 59 
Πίνακας 2.13: Ποσοστό επιτυχίας κατηγοριοποίησης του αλγόριθµου 
Κατηγοριοποίησης Μέγιστης Ύστερης Πιθανότητας (αναδροµικής παραλλαγής) της 
2ης σειράς πειραµάτων. ........................................................................................ 59 
Πίνακας 2.14: Ποσοστό επιτυχίας κατηγοριοποίησης του αλγόριθµου 
Κατηγοριοποίησης Μέγιστης Πιθανοφάνειας της 2ης σειράς πειραµάτων . ....... 59 
Πίνακας 2.15: Ποσοστό επιτυχίας κατηγοριοποίησης του αλγόριθµου 
κατηγοριοποίησης Κ Πλησιέστερων Γειτόνων  - Τρίτη παραλλαγή της 2ης σειράς 
πειραµάτων .
 
......................................................................................................... 60 
Πίνακας 2.16: Ποσοστό επιτυχίας κατηγοριοποίησης του αλγόριθµου 
κατηγοριοποίησης Κ Πλησιέστερων Γειτόνων  – Τέταρτη  παραλλαγή της 2ης 
σειράς πειραµάτων.
 
.............................................................................................. 60 
Πίνακας 2.17: Ποσοστό επιτυχίας κατηγοριοποίησης του αλγόριθµου 
κατηγοριοποίησης σ-FLNMAP µε ψηφοφορία της 2ης σειράς πειραµάτων. ...... 60 
Πίνακας 4.1: Στατιστικά στοιχεία των 76 κειµένων της πρώτης και έβδοµης κατηγορίας 
του σώµατος κειµένων του Brown Corpus. ....................................................... 106 
Πίνακας 4.2: Οι τιµές του κριτηρίου kP  του Beeferman για τα σύνολα Set0, …, Set9 
όπως αυτές ελήφθησαν µετά από την εφαρµογή της 1ης και της 2ης εναλλακτικής 
µορφής του αλγορίθµου του PREMONN, µε και χωρίς την εφαρµογή της τεχνικής 
της «ψηφοφορίας» τόσο για την περίπτωση όπου χρησιµοποιούνται οι λέξεις όσο 
και για την περίπτωση όπου χρησιµοποιούνται οι έννοιες των κειµένων.......... 111 
Πίνακας 4.3: Οι τιµές του κριτηρίου kP  του Beeferman για τα σύνολα Set0, …, Set9 
όπως αυτές ελήφθησαν µετά από την εφαρµογή της 1ης και της 2ης εναλλακτικής 
µορφής του αλγορίθµου του PREMONN σε συνδυασµό µε τον 2ο παράγοντα 
εξοµάλυνσης, µε και χωρίς την εφαρµογή της τεχνικής της ψηφοφορίας τόσο για την 
περίπτωση όπου χρησιµοποιούνται οι λέξεις όσο και για την περίπτωση όπου 
χρησιµοποιούνται οι έννοιες των κειµένων. ...................................................... 113 
Πίνακας 4.4: Οι τιµές του κριτηρίου kP  του Beeferman για όλα τα σύνολα όπως αυτές 
ελήφθησαν από την 1η και την 2η εναλλακτική µορφή του αλγορίθµου PREMONN 
 xi
   
σε συνδυασµό µε τον 1ο παράγοντα εξοµάλυνσης µε πλάτος παραθύρου ίσο µε 5, µε 
και χωρίς την εφαρµογή της τεχνικής της ψηφοφορίας τόσο για την περίπτωση όπου 
χρησιµοποιούνται οι λέξεις όσο και για την περίπτωση όπου χρησιµοποιούνται οι 
έννοιες των κειµένων. ........................................................................................ 117 
Πίνακας 4.5: Οι τιµές του κριτηρίου kP  του Beeferman για όλα τα σύνολα όπως αυτές 
ελήφθησαν από την 1η και την 2η εναλλακτική µορφή του αλγορίθµου PREMONN 
σε συνδυασµό µε τον 1ο παράγοντα εξοµάλυνσης µε πλάτος παραθύρου ίσο µε 10, 
µε και χωρίς την εφαρµογή της τεχνικής της ψηφοφορίας τόσο για την περίπτωση 
όπου χρησιµοποιούνται οι λέξεις όσο και για την περίπτωση όπου χρησιµοποιούνται 
οι έννοιες των κειµένων. .................................................................................... 118 
Πίνακας 4.6: Οι τιµές του κριτηρίου kP  του Beeferman για όλα τα σύνολα όπως αυτές 
ελήφθησαν από την 1η και την 2η εναλλακτική µορφή του αλγορίθµου PREMONN 
σε συνδυασµό µε τον 1ο παράγοντα εξοµάλυνσης µε πλάτος παραθύρου ίσο µε 15, 
µε και χωρίς την εφαρµογή της τεχνικής της ψηφοφορίας τόσο για την περίπτωση 
όπου χρησιµοποιούνται οι λέξεις όσο και για την περίπτωση όπου χρησιµοποιούνται 
οι έννοιες των κειµένων. .................................................................................... 119 
Πίνακας 4.7: Οι τιµές του κριτηρίου kP  του Beeferman για όλα τα σύνολα όπως αυτές 
ελήφθησαν από την 1η και την 2η εναλλακτική µορφή του αλγορίθµου PREMONN 
σε συνδυασµό µε τον 1ο παράγοντα εξοµάλυνσης µε πλάτος παραθύρου ίσο µε 20, 
µε και χωρίς την εφαρµογή της τεχνικής της ψηφοφορίας τόσο για την περίπτωση 
όπου χρησιµοποιούνται οι λέξεις όσο και για την περίπτωση όπου χρησιµοποιούνται 
οι έννοιες των κειµένων. .................................................................................... 120 
Πίνακας 4.8: Οι τιµές του κριτηρίου kP  του Beeferman για όλα τα σύνολα όπως αυτές 
ελήφθησαν από την 1η και την 2η εναλλακτική µορφή του αλγορίθµου PREMONN 
σε συνδυασµό µε τον 1ο παράγοντα εξοµάλυνσης µε πλάτος παραθύρου ίσο µε 25, 
µε και χωρίς την εφαρµογή της τεχνικής της ψηφοφορίας τόσο για την περίπτωση 
όπου χρησιµοποιούνται οι λέξεις όσο και για την περίπτωση όπου χρησιµοποιούνται 
οι έννοιες των κειµένων. .................................................................................... 121 
Πίνακας 4.9: Οι µέσοι όροι των τιµών του κριτηρίου kP  του Beeferman για τα σύνολα 
Set0, …, Set9 όπως αυτές ελήφθησαν µετά από την εφαρµογή της 1ης και της 2ης 
εναλλακτικής µορφής του αλγορίθµου του PREMONN µε και χωρίς την εφαρµογή 
της τεχνικής της ψηφοφορίας τόσο για την περίπτωση όπου χρησιµοποιούνται οι 
λέξεις όσο και για την περίπτωση όπου χρησιµοποιούνται οι έννοιες των κειµένων, 
όπως αυτά προκύπτουν και από τις τρεις σειρές πειραµάτων............................ 122 
Πίνακας 5.1: Εύρος του n (αριθµού των προτάσεων) και αριθµός των κειµένων για τα 
σύνολα Set0, Set1, Set2 και Set3 της συλλογής κειµένων του Choi. ................ 137 
Πίνακας 5.2a: Οι καλύτερες τιµές των Precision, Recall και kP  για τα υποσύνολα  
Set0, Set1, Set2 και Set3 για το σύνολο των κειµένων του Choi οι οποίες 
προκύπτουν από τις καλύτερες τιµές των παραµέτρων γ και r για την πρώτη σειρά 
του πρώτου γκρουπ πειραµάτων (δίχως την τεχνική της επαλήθευσης) µε τη χρήση 
της συνάρτησης “U-shape”. ............................................................................... 139 
Πίνακας 5.2b: Οι καλύτερες τιµές των Precision, Recall και kP  για τα υποσύνολα  
Set0, Set1, Set2 και Set3 για το σύνολο των κειµένων του Choi οι οποίες 
προκύπτουν από τις καλύτερες τιµές των παραµέτρων γ και r για την πρώτη σειρά 
του πρώτου γκρουπ πειραµάτων (δίχως την τεχνική της επαλήθευσης) µε τη χρήση 
της συνάρτησης “ κορεσµένη U-shape ” (“saturated”, [Gloss(00074)])............ 139 
Πίνακας 5.2c: Οι καλύτερες τιµές των Precision, Recall και kP  για τα υποσύνολα  
Set0, Set1, Set2 και Set3 για το σύνολο των κειµένων του Choi οι οποίες 
 xii
   
προκύπτουν από τις καλύτερες τιµές των παραµέτρων γ και r για την πρώτη σειρά 
του πρώτου γκρουπ πειραµάτων (δίχως την τεχνική της επαλήθευσης) µε τη χρήση 
της συνάρτησης “V-shape”................................................................................. 140 
Πίνακας 5.2d: Οι καλύτερες τιµές των Precision, Recall και kP  για τα υποσύνολα  
Set0, Set1, Set2 και Set3 για το σύνολο των κειµένων του Choi οι οποίες 
προκύπτουν από τις καλύτερες τιµές των παραµέτρων γ και r για την πρώτη σειρά 
του πρώτου γκρουπ πειραµάτων (δίχως την τεχνική της επαλήθευσης) µε τη χρήση 
της συνάρτησης “κορεσµένη V -shape” (“saturated”, [Gloss(00074)])............. 140 
Πίνακας 5.3a: Οι τιµές των Precision, Recall και kP  για τα υποσύνολα  Set0, Set1, 
Set2 και Set3 για το σύνολο των κειµένων του Choi οι οποίες προκύπτουν από τις 
καλύτερες τιµές των παραµέτρων γ και r της δεύτερης σειράς του πρώτου γκρουπ 
πειραµάτων (µετά την εφαρµογή της τεχνικής της επαλήθευσης) µε τη χρήση της 
συνάρτησης “U-shape”....................................................................................... 144 
Πίνακας 5.3b: Οι τιµές των Precision, Recall και kP  για τα υποσύνολα  Set0, Set1, 
Set2 και Set3 για το σύνολο των κειµένων του Choi οι οποίες προκύπτουν από τις 
καλύτερες τιµές των παραµέτρων γ και r της δεύτερης σειράς του πρώτου γκρουπ 
πειραµάτων (µετά την εφαρµογή της τεχνικής της επαλήθευσης) µε τη χρήση της 
συνάρτησης “κορεσµένη U-shape” (“saturated”, [Gloss(00074)]). ................... 144 
Πίνακας 5.3c: Οι τιµές των Precision, Recall και kP  για τα υποσύνολα  Set0, Set1, 
Set2 και Set3 για το σύνολο των κειµένων του Choi οι οποίες προκύπτουν από τις 
καλύτερες τιµές των παραµέτρων γ και r της δεύτερης σειράς του πρώτου γκρουπ 
πειραµάτων (µετά την εφαρµογή της τεχνικής της επαλήθευσης) µε τη χρήση της 
συνάρτησης “V-shape”. ...................................................................................... 145 
Πίνακας 5.3d: Οι τιµές των Precision, Recall και kP  για τα υποσύνολα  Set0, Set1, 
Set2 και Set3 για το σύνολο των κειµένων του Choi οι οποίες προκύπτουν από τις 
καλύτερες τιµές των παραµέτρων γ και r της δεύτερης σειράς του πρώτου γκρουπ 
πειραµάτων (µετά την εφαρµογή της τεχνικής της επαλήθευσης) µε τη χρήση της 
συνάρτησης “κορεσµένη V-shape” (“saturated”, [Gloss(00074)]). ................... 145 
Πίνακας 5.4a: Οι τιµές των Precision, Recall και kP  για τα υποσύνολα  Set0, Set1, 
Set2 και Set3 για το σύνολο των κειµένων του Choi οι οποίες προκύπτουν από τις 
καλύτερες τιµές των παραµέτρων γ και r της πρώτης σειράς του δεύτερου γκρουπ 
πειραµάτων (δίχως την εφαρµογή της τεχνικής της επαλήθευσης) µε τη χρήση της 
συνάρτησης “U-shape”....................................................................................... 147 
Πίνακας 5.4b: Οι τιµές των Precision, Recall και kP  για τα υποσύνολα  Set0, Set1, 
Set2 και Set3 για το σύνολο των κειµένων του Choi οι οποίες προκύπτουν από τις 
καλύτερες τιµές των παραµέτρων γ και r της πρώτης σειράς του δεύτερου γκρουπ 
πειραµάτων (δίχως την εφαρµογή της τεχνικής της επαλήθευσης) µε τη χρήση της 
συνάρτησης “κορεσµένη U-shape” (“saturated”, [Gloss(00074)]). ................... 148 
Πίνακας 5.4c: Οι τιµές των Precision, Recall και kP  για τα υποσύνολα  Set0, Set1, 
Set2 και Set3 για το σύνολο των κειµένων του Choi οι οποίες προκύπτουν από τις 
καλύτερες τιµές των παραµέτρων γ και r της πρώτης σειράς του δεύτερου γκρουπ 
πειραµάτων (δίχως την εφαρµογή της τεχνικής της επαλήθευσης) µε τη χρήση της 
συνάρτησης “V-shape”. ...................................................................................... 148 
Πίνακας 5.4d: Οι τιµές των Precision, Recall και kP  για τα υποσύνολα  Set0, Set1, 
Set2 και Set3 για το σύνολο των κειµένων του Choi οι οποίες προκύπτουν από τις 
καλύτερες τιµές των παραµέτρων γ και r της πρώτης σειράς του δεύτερου γκρουπ 
 xiii
   
πειραµάτων (δίχως την εφαρµογή της τεχνικής της επαλήθευσης) µε τη χρήση της 
συνάρτησης “κορεσµένη V-shape” (“saturated”, [Gloss(00074)]). ................... 149 
Πίνακας 5.5a: Οι τιµές των Precision, Recall και kP  για τα υποσύνολα  Set0, Set1, 
Set2 και Set3 για το σύνολο των κειµένων του Choi οι οποίες προκύπτουν από τις 
καλύτερες τιµές των παραµέτρων γ και r της δεύτερης σειράς του δεύτερου γκρουπ 
πειραµάτων (µετά την εφαρµογή της τεχνικής της επαλήθευσης) µε τη χρήση της 
συνάρτησης “U-shape”....................................................................................... 152 
Πίνακας 5.5b: Οι τιµές των Precision, Recall και kP  για τα υποσύνολα  Set0, Set1, 
Set2 και Set3 για το σύνολο των κειµένων του Choi οι οποίες προκύπτουν από τις 
καλύτερες τιµές των παραµέτρων γ και r της δεύτερης σειράς του δεύτερου γκρουπ 
πειραµάτων (µετά την εφαρµογή της τεχνικής της επαλήθευσης) µε τη χρήση της 
συνάρτησης “κορεσµένη U-shape” (“saturated”, [Gloss(00074)]). ................... 152 
Πίνακας 5.5c: Οι τιµές των Precision, Recall και kP  για τα υποσύνολα  Set0, Set1, 
Set2 και Set3 για το σύνολο των κειµένων του Choi οι οποίες προκύπτουν από τις 
καλύτερες τιµές των παραµέτρων γ και r της δεύτερης σειράς του δεύτερου γκρουπ 
πειραµάτων (µετά την εφαρµογή της τεχνικής της επαλήθευσης) µε τη χρήση της 
συνάρτησης “V-shape”. ...................................................................................... 153 
Πίνακας 5.5d: Οι τιµές των Precision, Recall και kP  για τα υποσύνολα  Set0, Set1, 
Set2 και Set3 για το σύνολο των κειµένων του Choi οι οποίες προκύπτουν από τις 
καλύτερες τιµές των παραµέτρων γ και r της δεύτερης σειράς του δεύτερου γκρουπ 
πειραµάτων (µετά την εφαρµογή της τεχνικής της επαλήθευσης) µε τη χρήση της 
συνάρτησης “κορεσµένη V -shape” (“saturated”, [Gloss(00074)]). .................. 153 
Πίνακας 5.6: Σύγκριση των λαµβανόµενων τιµών του κριτηρίου kP  από διάφορους 
αλγορίθµους που εµφανίζονται στην βιβλιογραφία µε τα αντίστοιχα της δεύτερης 
σειράς και των δυο γκρουπ πειραµάτων του δικού µας αλγορίθµου εφαρµοζόµενοι 
πάνω σε καθένα από τα σύνολα Set0, Set1, Set2 και Set3 των δεδοµένων του Choi......156 
Πίνακας 5.7 Οι τιµές των Precision, Recall και kP  για τα datasets Set0, Set1, Set2, Set3 
και για την συνολική συλλογή (συλλογή κειµένων του Choi) που προκύπτουν από 
τις καλύτερες τιµές των παραµέτρων γ και r του τρίτου γκρουπ πειραµάτων (µετά 
την εφαρµογή της τεχνικής της επαλήθευσης)................................................... 161 
Πίνακας 5.8 Σύγκριση των λαµβανόµενων τιµών του κριτηρίου kP  από διάφορους 
αλγορίθµους που εµφανίζονται στην βιβλιογραφία µε τα αντίστοιχα του τρίτου 
γκρουπ πειραµάτων του δικού µας αλγορίθµου, και της δεύτερης σειράς των 
πρώτων δυο γκρουπ εφαρµοζόµενοι πάνω σε καθένα από τα σύνολα Set0, Set1, Set2 
και Set3 των δεδοµένων του Choi...................................................................... 162 
Πίνακας 5.9: Τιµές των minL και maxL (ελάχιστος και µέγιστος αριθµός γραµµών) για 
τα σύνολα κειµένων Set0, Set1, …, Set9 (της δικής µας συλλογής κειµένων). 164 
Πίνακας 5.10: Τιµές των Precision, Recall και kP  για τα σύνολα κειµένων Set0, Set1, 
…, Set9 (της δικής µας συλλογής κειµένων) οι οποίες προκύπτουν από τις 
καλύτερες τιµές των παραµέτρων γ και r της πρώτης σειράς της  δεύτερης οµάδας 
πειραµάτων (δίχως την εφαρµογή της τεχνικής της επαλήθευσης). .................. 165 
Πίνακας 5.11: Τιµές των Precision, Recall και kP  για τα σύνολα κειµένων Set0, Set1, 
…, Set9 (της δικής µας συλλογής κειµένων) χρησιµοποιώντας τις επαληθευµένες 
τιµές των παραµέτρων........................................................................................ 168 
Πίνακας 5.12: Σύγκριση του αλγορίθµου µας µε τους αλγορίθµους των ([Choi,2000], 
[Choi et al., 2001], [Utiyama & Isahara, 2001]) σε ότι αφορά τον µέσο χρόνο 
εκτέλεσης της τµηµατοποίησης ενός κειµένου. ................................................. 170 
 xiv
   
Πίνακας 6.1. Η δοµή της εφηµερίδας  «Το ΒΗΜΑ»............................................... 177 
Πίνακας 6.2. Περιγραφή των κειµένων που προέρχονται από την εφηµερίδα «ΤΟ 
ΒΗΜΑ» και τα οποία συνιστούν το σώµα κειµένων που κατασκευάστηκε από τους  
Σταµατάτο, Φακοτάκη και Κοκκινάκη............................................................... 178 
Πίνακας 6.3: Λίστα των συλλογών (Set0, … , Set5) που δηµιουργήθηκαν στη πρώτη 
σειρά πειραµάτων και τα κείµενα των συγγραφέων που χρησιµοποιήθηκαν σε κάθε 
µια από αυτές...................................................................................................... 183 
Πίνακας 6.4: Εύρος του n (αριθµού προτάσεων) για τα τέσσερα υποσύνολα Subset0, 
Subset1, Subset2 και Subset3 καθενός εκ των έξη συνόλων των κειµένων της 
πρώτης σειράς πειραµάτων. ............................................................................... 183 
Πίνακας 6.5: Οι τιµές των κριτηρίων Precision, Recall και kP  του Beeferman για τα 
σύνολα Set0, Set1, Set2, Set3, Set4 και Set5 χρησιµοποιώντας προτάσεις ως µονάδα 
τµήµατος, όπως αυτές ελήφθησαν µετά από τη διαδικασία εκπαίδευσης και 
επαλήθευσης....................................................................................................... 188 
Πίνακας 6.6: Οι µέσοι όροι των τιµών των κριτηρίων Precision, Recall και kP  του 
Beeferman για τις συλλογές Set0, Set1, Set2, Set3, Set4 και Set5 χρησιµοποιώντας 
προτάσεις ως µονάδα τµήµατος, όπως αυτές ελήφθησαν µετά από τη διαδικασία 
εκπαίδευσης και επαλήθευσης και οι αντίστοιχες της συλλογής κειµένων του Choi......189 
Πίνακας 6.7: Οι µέσοι όροι παραγράφων ανά κείµενο, προτάσεων ανά κείµενο και 
προτάσεων ανά παράγραφο των κειµένων καθενός από τους δέκα συγγραφείς της 
συλλογής άρθρων της εφηµερίδας «Το Βήµα» καθώς και οι αντίστοιχοι µέσοι όροι 
για το σύνολο αυτών. ......................................................................................... 192 
Πίνακας 6.8: Οι µέσοι όροι των τιµών των κριτηρίων Precision, Recall και kP  του 
Beeferman για τη µοναδική συλλογής της δεύτερης σειρά πειραµάτων 
χρησιµοποιώντας παραγράφους ως µονάδα τµήµατος, όπως αυτές ελήφθησαν µετά 
από τη διαδικασία εκπαίδευσης και επαλήθευσης. ............................................ 193 
 xv
   
 xvi
Κεφάλαιο 1 Εισαγωγή   
ΚΕΦΑΛΑΙΟ 1 ΕΙΣΑΓΩΓΗ 
 
1.1 Εισαγωγή 
 
 Τα τελευταία χρόνια, παρατηρούµε µια τεράστια αύξηση της διαθέσιµης πληροφορίας η οποία 
βρίσκεται σε ηλεκτρονική µορφή. Το γεγονός αυτό, ιδιαίτερα για την ελληνική κοινωνία όπου τα 
τελευταία επιτεύγµατα της τεχνολογίας δεν είναι ευρέως γνωστά και κατανοητά στον µέσο πολίτη, 
αποτέλεσε ένα είδος επανάστασης. Ιδιαίτερα η «εισβολή» του ∆ιαδικτύου (“Internet”, [Gloss(00043)]) 
στην ζωή µας αποτέλεσε παράγοντα ανατρεπτικό. Και αυτό γιατί, µέσα από το ∆ιαδίκτυο (“Internet”, 
[Gloss(00043)]), οι χρήστες έχουν την δυνατότητα πρόσβασης σε εκατοντάδες πηγές πληροφορίας 
(“information sources”, [Gloss(00038)]) και σε εκατοµµύρια κειµένων, των οποίων ο αριθµός 
αυξάνεται από λεπτό σε λεπτό, σε κάθε γωνιά του πλανήτη µας.  
 Το τεράστιο αυτό πλήθος της διαθέσιµης πληροφορίας δηµιούργησε την ανάγκη εύρεσης 
έξυπνων και αποτελεσµατικών µεθόδων για την εύκολη και γρήγορη πρόσβαση των πληροφοριών 
αυτών από τα εκατοµµύρια των χρηστών. Αυτό αποτέλεσε το αντικείµενο ενασχόλησης και έρευνας 
του τοµέα των Ψηφιακών Βιβλιοθηκών (“Digital Libraries”, [Gloss(00023)]), του οποίου κύριος 
στόχος είναι η εύρεση λύσεων στο πρόβληµα «Πώς µπορεί να διευκολυνθεί η πρόσβαση στον 
τεράστιο όγκο πληροφοριών». Πιο συγκεκριµένα, ο τεράστιος αυτός όγκος πληροφοριών δηµιουργεί 
την ανάγκη ανάπτυξης εργαλείων που θα βοηθούν τους χρήστες να βρουν τα κείµενα της αρεσκείας 
τους, αλλά και που θα είναι σε θέση να ικανοποιήσουν τις «ανάγκες πληροφορίας» (“information 
needs”, [Gloss(00036)]) του χρήστη. Για την ανάπτυξη αυτών των εργαλείων, οι επιστήµονες 
στράφηκαν προς τους τοµείς της Μηχανικής Μάθησης (“Machine Learning ”, [Gloss(00054)]) 
([Mitchell, 1997], [Duda et al., 2001], [Manning& Schuetze 1999]) και της Ανάκτησης ∆εδοµένων 
(“Information Retrieval”, [Gloss(00037)]), [Lewis, 1992], [Lewis et al., 1996],  [Lewis & Jones, 
1996]), αναζητώντας σε αυτούς µεθόδους και τεχνικές για την ανάπτυξη των ζητούµενων εργαλείων.  
 Πιο συγκεκριµένα, οι υπάρχουσες µέθοδοι που επινοήθηκαν µε σκοπό την πρόσβαση σε 
µεγάλες συλλογές κειµένων (“large text collections”, [Gloss(00048)]) όπως είναι το ∆ιαδίκτυο 
(“Internet”, [Gloss(00043)]), µπορούν να καταταχθούν σε δύο µεγάλες κατηγορίες. Η πρώτη 
 1
Κεφάλαιο 1 Εισαγωγή   
κατηγορία περιλαµβάνει εκείνες τις µεθόδους ανάκτησης κειµένων (“document retrieval”, 
[Gloss(00027)]) που ανήκουν σε µία συλλογή ούτως ώστε να ικανοποιήσουν ένα ερώτηµα ή αλλιώς 
άντληση πληροφοριών (“query”, [Gloss(00070)]) του χρήστη. Τέτοιου είδους ανακτήσεις γίνονται µε 
την βοήθεια των διαφόρων Μηχανών Αναζήτησης (“Search Engines”, [Gloss(00077)]) οι οποίες είναι 
ευρύτατα γνωστές και διαδεδοµένες στο ∆ιαδίκτυο (“Internet”, [Gloss(00043)]), όπως αυτή του Alta 
Vista, τoυ Google, του WebCrawler και της Infoseek. Σε αυτές, ο χρήστης καλείται να διατυπώσει το 
ερώτηµά του µε την µορφή άντλησης πληροφοριών (“query”, [Gloss(00070)]). Η µορφή αυτή 
περιλαµβάνει την διατύπωση µιας ή περισσοτέρων λέξεων – κλειδιών (“keywords”, [Gloss(00047)]).  
Η διαδικασία της εύρεσης σχετικών µε την ερώτηση του χρήστη κειµένων περιλαµβάνει την σύγκριση 
αυτών των λέξεων – κλειδιών µε τις αντίστοιχες που υπάρχουν στα κείµενα της υπό εξέτασης 
συλλογής.  
 Σε αντιδιαστολή µε την πρώτη κατηγορία αλλά σε παρόµοια φιλοσοφία µε αυτή των 
παραδοσιακών µεθόδων που αφορούν τις βιβλιοθήκες, η δεύτερη κατηγορία περιλαµβάνει οργανωτικά 
σχήµατα τα οποία δίνουν τη δυνατότητα στον χρήστη να ψάξει χειρωνακτικά (“manually”, 
[Gloss(00055)]) για κείµενα τα οποία ικανοποιούν κάποια επιθυµία του µέσα σε συλλογές οι οποίες 
είναι οργανωµένες υπό την µορφή ταξινοµίας (“taxomomies” ή αλλιώς “directories”, [Gloss(00089)]). 
Η εύρεση από τον χρήστη των σχετικών κειµένων γίνεται µε την εγκάρσια διάβαση (“traversal”, 
[Gloss(00096)]) στις εκάστοτε τοπικές ιεραρχίες οι οποίες συνιστούν τη συνολική ταξινοµία. Αυτός ο 
τρόπος εύρεσης είναι ιδιαίτερα εύκολος για τον χρήστη, γιατί κατά την διάρκεια της εύρεσης ο 
χρήστης αποκτά εποπτικότερη εικόνα του χώρου στον οποίο κινείται, δηλαδή της θεµατικής 
κατηγορίας στην οποία ψάχνει. Χαρακτηριστικό παράδειγµα τέτοιου είδους ταξινοµίας είναι το 
Yahoo.  
 Το βασικό µειονέκτηµα των µεθόδων εύρεσης κειµένων που βασίζονται στις ταξινοµίες 
αποτελεί το γεγονός ότι, όσο αυξάνει το πλήθος των κειµένων τα οποία περιέχει η ταξινοµία τόσο πιο 
αργή γίνεται η διαδικασία εύρεσης των ζητούµενων κειµένων. Αυτό είναι ιδιαίτερα σηµαντικό γιατί, 
όπως αναφέραµε παραπάνω, το πλήθος των διαθέσιµων κειµένων αυξάνεται εκθετικά µε την πάροδο 
του χρόνου.  
 Ταυτόχρονα, µέθοδοι και τεχνικές οι οποίες ικανοποιούν ένα ερώτηµα του χρήστη µε το να του 
παρέχουν µια απλή λίστα από σχετικά κείµενα γίνονται συνεχώς λιγότερο ελκυστικές, µιας και ο 
αριθµός των κειµένων που ικανοποιούν το ερώτηµα µπορεί να είναι πολύ µεγάλος. Αυτό αναγκάζει 
τον ίδιο τον χρήστη, να κάνει µια πιο εξονυχιστική επιλογή στα κείµενα που του επιστράφηκαν, 
γεγονός που είναι εξαιρετικά χρονοβόρο και επίπονο. 
Λαµβάνοντας υπόψη τα παραπάνω µειονεκτήµατα των µεθόδων αυτών, σκοπός του τοµέα της 
Μηχανικής Μάθησης είναι η εύρεση και η ανάπτυξη νέων, πιο εξελιγµένων, µεθόδων οι οποίες θα 
 2
Κεφάλαιο 1 Εισαγωγή   
δίνουν λύση στο πρόβληµα Ανάκτησης ∆εδοµένων (“Information Retrieval”, [Gloss(00037)]), [Lewis, 
1992], [Lewis et al., 1996],  [Lewis & Jones, 1996]). Για τον σκοπό αυτό πραγµατοποιείται µια 
στροφή προς την κατεύθυνση ενός πιο εξελιγµένου τρόπου συσχέτισης της πληροφορίας και των 
εννοιών οι οποίες περιέχονται σε καθένα από τα κείµενα. Και αυτό γιατί, ο ανθρώπινος νους για να 
αντιλαµβάνεται τον κόσµο που τον περιβάλει δηµιουργεί λογικές και εννοιολογικές συσχετίσεις των 
διαφόρων πραγµάτων, χωρίζοντας τα σε διάφορες νοητικές κατηγορίες. Αυτή η εννοιολογική δοµή και 
συσχέτιση συνήθως παρίσταται και στα κείµενα. Για παράδειγµα, ένα κείµενο το οποίο αναφέρεται 
στον τίτλο του σε µια ασθένεια π.χ. την οστρακιά, εικάζουµε ότι θα είναι ιατρικού περιεχοµένου οπότε 
και το σώµα του κειµένου, θα είναι και αυτό ιατρικής φύσης, όπου µπορεί να αναφέρονται 
παρεµφερείς ασθένειες. Παρόλα αυτά µια τέτοιου είδους συσχέτιση της πληροφορίας και των εννοιών  
οι οποίες περιέχονται σε καθένα από τα κείµενα καθίσταται δύσκολη µιας και η φυσική γλώσσα είναι 
ασαφής δεδοµένου ότι σχεδόν κάθε λέξη έχει πολλαπλές ερµηνείες και χρήσεις. Κατ' επέκταση, στα 
πλαίσια ενός κειµένου κάθε µια λέξη που εµφανίζεται µέσα σε αυτό είναι δυνατό να απαντάται µε 
περισσότερες από µια έννοιες γεγονός που καθιστά την εύρεση αυτού του είδους της συσχέτισης 
δύσκολη.  
Αντικείµενο έρευνας των επιστηµόνων τα τελευταία χρόνια αποτελεί η εύρεση και ανάπτυξη 
µεθόδων που προσεγγίζουν τη συσχέτιση των όρων-εννοιών που εµφανίζονται σε κάθε κείµενο και 
πλησιάζουν τον τρόπο µε τον οποίο ο ανθρώπινος νους πραγµατοποιεί αυτή την συσχέτιση. Η 
συσχέτιση αυτή θα αποτελεί ένα είδος «δακτυλικού αποτυπώµατος» για το κείµενο, και θα µας 
βοηθάει στην αποτελεσµατική εύρεση κειµένων, όταν αυτά ζητούνται από τον χρήστη. Ένα από τα 
προβλήµατα τα οποία καλείται να αντιµετωπίσει ο κλάδος της Υπολογιστικής Γλωσσολογίας 
(“Computational Linguistics”, [Gloss(00016)]) αποτελεί η ανάπτυξη τέτοιων µεθόδων. 
 ∆οθείσης της ανάγκης ανάπτυξης εργαλείων που θα βοηθούν τους χρήστες να βρουν τα κείµενα 
της αρεσκείας τους µέσα από έναν τεράστιο όγκο πληροφοριών αλλά και που θα είναι σε θέση να 
ικανοποιήσουν τις «ανάγκες πληροφορίας» (“information needs”, [Gloss(00036)]) τους, 
πραγµατοποιείται επίσης µια άλλη στροφή της επιστηµονικής κοινότητας προς την κατεύθυνση ενός 
πιο εξελιγµένου τρόπου εύρεσης της ζητούµενης πληροφορίας σε τµήµατα ενός κειµένου, καθένα από 
τα οποία θα αναφέρεται σε ένα ξεχωριστό θέµα, και όχι στο συνολικό κείµενο. Ένα τέτοιο τµήµα, 
όταν αυτό θα επιστρέφεται σε έναν χρήστη σαν απάντηση σε ένα ερώτηµα αυτού αντί για το συνολικό 
κείµενο, θα είναι ιδιαίτερα ελκυστικό µιας και θα τον "απαλλάσσει" από την ανάγνωση του συνολικού 
κειµένου η ολότητα του οποίου ίσως να µην είναι σχετική µε το διατυπωθέν ερώτηµα. Ένας τέτοιος 
εξελιγµένος τρόπος εύρεσης θα παρουσιάζει µεγάλο βαθµό ακρίβειας, ορθότητας και αξιοπιστίας της 
δοθείσας απάντησης απαλείφοντας το "παράπονο" µιας µεγάλης µερίδας χρηστών οι οποίοι 
επισηµαίνουν ότι “το αποτέλεσµα µιας αναζήτησης περιέχει σε πολύ µεγάλο βαθµό µη σχετικά προς 
το ζητούµενο κείµενα”. 
 3
Κεφάλαιο 1 Εισαγωγή   
 Στο ίδιο πλαίσιο κατατάσσονται και προσπάθειες οι οποίες πραγµατοποιούν συνδυασµό 
διαφόρων τεχνικών που παρουσιάζονται στην βιβλιογραφία µε τον ίδιο µε τον προαναφερθέντα 
σκοπό. Μεταξύ αυτών αξίζει να σηµειωθούν προσεγγίσεις που συνδυάζουν τεχνικές όπως η 
τµηµατοποίηση κειµένων και αντιστοίχων που κατατάσσονται στη θεµατική περιοχή της 
κατηγοριοποίησης κειµένων. 
1.2 Αντικείµενο της διατριβής 
 
Αντικείµενο της παρούσας διατριβής αποτελεί τόσο η κατηγοριοποίηση κειµένων µε τη βοήθεια 
των λέξεων και των εννοιών αυτών όσο και η τµηµατοποίηση µεγάλης έκτασης κειµένων µε χρήση 
µεθόδων υπολογιστικής νοηµοσύνης, στόχος των οποίων αποτελεί η εµβάθυνση στο περιεχόµενο των 
κειµένων και η ανάδειξη του τρόπου δόµησής τους (µε την εύρεση των υποθεµάτων από τα οποία 
αποτελούνται) άρα και του τρόπου µε τον οποίο συσχετίζεται η πληροφορία η οποία περιέχεται µέσα 
σε αυτά. Οι εν λόγω µέθοδοι βασίζονται στο ότι, η συγγραφή ενός κειµένου αποτελεί την καταγραφή 
των ιδεών µε την βοήθεια γλωσσολογικών µέσων. Η αρχική ιδέα µπορεί να περιγραφεί ως ένα πλαίσιο 
το οποίο αποτελείται από λογικά αλληλοσυνδεόµενες προτάσεις οι οποίες αποτελούν την βάση του 
κειµένου. Ο συγγραφέας «χτίζει» το πλαίσιο σύµφωνα µε γνωστούς ρητορικούς κανόνες και τοποθετεί 
«θεµελιώδεις λίθους» σε κάθε σηµείο του κειµένου διαιρώντας το κείµενο σε παραγράφους καθεµία 
από τις οποίες παρουσιάζει συνοχή, τοποθετώντας προτάσεις που αναφέρονται στο υπό συζήτηση 
θέµα στην αρχή των παραγράφων, τοποθετώντας φράσεις – κλειδιά όπως «Σκοπός αυτής της 
διατριβής είναι», επιλέγοντας συνειδητά ή όχι τις λέξεις για να εκφράσει µια συγκεκριµένη έννοια ή 
χρησιµοποιώντας την ίδια λέξη µέσα στο ίδιο κείµενο αναφερόµενος κάθε φορά σε κάποια από τις 
διαφορετικές έννοιες αυτής. Κατά αυτόν τον τρόπο ένα κείµενο µπορεί να θεωρηθεί ως µια «άγνωστη 
γη» από την οποία µπορούµε να εξάγουµε πολύτιµη πληροφορία µε τη βοήθεια των µεθόδων που 
προτείνονται στα πλαίσια της παρούσας διατριβής.  
Έναυσµα της παρούσας διατριβής είναι η εύρεση υπολογιστικών µεθόδων µε την βοήθεια των 
οποίων θα επιτυγχάνεται ακριβέστερη και αποτελεσµατικότερη πρόσβαση στην ζητούµενη ή αλλιώς 
ακριβή πληροφορία και όχι στην συνολική (συνολικά διαθέσιµη) πληροφορία. Ο όρος ζητούµενη 
/ακριβής πληροφορία ικανοποιεί τις ακόλουθες δυο συνθήκες: 
 Η επιστρεφόµενη πληροφορία αναφέρεται στο «ακριβές θέµα» του ερωτήµατος το οποίο 
διατυπώθηκε. Ο όρος «ακριβές θέµα» εµπεριέχει την έννοια ότι πραγµατοποιείται αποσαφήνιση 
της έννοιας ([Agirre & Martinez, 2000], [Escudero et al., 2000]) ή του συνόλου των εννοιών οι 
οποίες εµπεριέχονται στο ερώτηµα αλλά και σύγκριση αυτής/ αυτών µε εκείνες που εµφανίζονται 
 4
Κεφάλαιο 1 Εισαγωγή   
σε κάθε ένα κείµενο. Η δυσκολία του καθορισµού εκείνων των κειµένων ή τµηµάτων κειµένων 
που σχετίζονται µε την ζητούµενη έννοια ή συνόλου εννοιών καθίσταται επίπονη αν ληφθούν 
υπόψη τα φαινόµενα της πολυσηµίας και του πλήθους των υπό εξέταση κειµένων.  
 Η έκταση της επιστρεφόµενης πληροφορίας είναι ακριβώς η ζητούµενη. Παρά το γεγονός ότι ένα 
κείµενο αναφέρεται σε ένα θέµα, πολλά από τα τµήµατα που το αποτελούν είναι δυνατό να θίγουν 
διαφορετικά υποθέµατα τα οποία σχετίζονται σε µικρότερο βαθµό µε το συνολικό θέµα. Όταν το 
επιστρεφόµενο κείµενο περιέχει µόνο την ακριβώς ζητούµενη πληροφορία τότε ο χρήστης 
απαλλάσσεται από τον φόρτο ανάγνωσης του συνολικού κειµένου που µπορεί να περιέχει για 
αυτόν άχρηστη πληροφορία.  
Μια «εις βάθος» επεξεργασία (και όχι επιφανειακή, όπως αυτή λαµβάνει χώρα στις διάφορες 
µηχανές αναζήτησης) η οποία θα αναδείκνυε σε όλο το εύρος τους το πλήθος και την έκταση όλων 
των υποθεµάτων που εµφανίζονται στα κείµενα θα επιτύγχανε ακριβέστερη ανάκτηση και εξόρυξη 
πληροφορίας µέσα σε αυτά. Το αποτέλεσµα µιας τέτοιας επεξεργασίας θα ήταν ευεργετικό σε πλήθος 
εφαρµογών όπως στις µηχανές αναζήτησης, στη θεµατική κατηγοριοποίηση κειµένων, στην εξαγωγή 
περιλήψεων κ.λ.π.  
Μετά από επισκόπηση των µοντέλων και µεθόδων για την εύρεση της εννοιολογικής δοµής 
των κειµένων και τη βελτίωση πρόσβασης σε πληροφορία προτείνονται στην παρούσα διατριβή τρία 
µοντέλα. Το πρώτο από αυτά ακολουθεί την προσέγγιση της Μηχανικής Μάθησης (“Machine 
Learning ”, [Gloss(00054)]) ([Mitchell, 1997], [Duda et al., 2001], [Manning& Schuetze 1999]) και 
πραγµατοποιεί κατηγοριοποίηση κειµένων (“Text Classification”, [Gloss(00091)]) µε την βοήθεια της 
έννοιας της κάθε λέξης -όπως αυτή προσδιορίζεται από το περιεχόµενο µέσα στο οποίο αυτή 
απαντάται και όπως αυτή δίνεται από τον θησαυρό όρων Wordnet - και όχι των αυτούσιων λέξεων του 
κειµένου. Το δεύτερο µοντέλο πραγµατεύεται την τµηµατοποίηση κειµένων (“Text Segmentation”, 
[Gloss(00092)]), ([Halliday & Hasan, 1976], [Hirschberg & Litman, 1993], [Reynar, 1994], [Reynar, 
1998], [Yaari, 1997], [Yaari, 1999], [Hearst, 1993],  [Beeferman et al., 1999], ([Choi et al., 2001]) µε 
την βοήθεια τεχνικών κατηγοριοποίησης κειµένων. Τέλος, το τελευταίο προτεινόµενο µοντέλο 
προτείνει και υλοποιεί ένα µοντέλο τµηµατοποίησης µεγάλης έκτασης κειµένων σε µικρότερα 
τµήµατα καθένα από τα οποία παρουσιάζει ισχυρή συνάφεια και συνοχή σε τοπικό επίπεδο. Η εν λόγω 
τµηµατοποίηση πραγµατοποιείται ως συνδυασµός τεχνικών εύρεσης της οµοιότητας µεταξύ των 
διαφόρων µερών του κειµένου και αυτόµατου καθορισµού των ορίων µεταξύ των τµηµάτων. Τα δυο 
τελευταία µοντέλα οδήγησαν στην υλοποίηση αντίστοιχων αλγορίθµων. 
Για την εύρεση µεθόδων µε την βοήθεια των οποίων θα επιτυγχάνεται ο συσχετισµός της 
πληροφορίας και των εννοιών οι οποίες περιέχονται σε καθένα από τα κείµενα στραφήκαµε στην 
θεµατική περιοχή των θησαυρών όρων (“Thesaurus”, [Gloss(00093)]) ([Benkhalifa et al., 2001], 
 5
Κεφάλαιο 1 Εισαγωγή   
[Petridis et al., 2001]) και των αλγορίθµων του τοµέα της Μηχανικής Μάθησης οι οποίοι σχετίζονται 
µε την «Κατηγοριοποίηση Κειµένων» (“Text Classification”, [Gloss(00091)]) ([Duda et al., 2001], 
[Kaburlazos &  Petridis, 2000], [Manning & Schuetze, 1999], [Mitchell, 1997], [Petridis & Kehagias, 
1996], [Petridis & Kaburlazos, 1999], [Petridis & Kaburlazos, 2000], [Petridis & Kaburlazos, 2001], 
[Yang, 1995]). Πιο συγκεκριµένα, εξετάσαµε τον τρόπο µε τον οποίο συσχετίζει την πληροφορία την 
οποία περιέχει ο θησαυρός όρων Wordnet ([Miller et al., 1990]) µε την µορφή συνωνύµων και της 
δηµιουργούµενης ιεραρχίας. Χρησιµοποιήσαµε την εν λόγω πληροφορία αντικαθιστώντας κάθε λέξη 
που εµφανίζεται σε ένα κείµενο µε την έννοια που αντιστοιχεί στο περιεχόµενο στο οποίο εµφανίζεται 
η λέξη. Χρησιµοποιήσαµε ένα πλήθος αλγορίθµων του τοµέα  Μηχανικής Μάθησης για την επίτευξη 
της κατηγοριοποίησης τόσο µε την χρήση των εννοιών όσο και των αυτούσιων λέξεων των κειµένων. 
Εντελώς αντίστοιχα, για την ανάπτυξη µεθόδων µε την βοήθεια των οποίων θα επιτυγχάνεται ο 
προσδιορισµός της δοµής του κειµένου (δηλαδή των υποθεµάτων από τα οποία αυτό αποτελείται) και 
του αυτόµατου καθορισµού εκείνων των τµηµάτων του κειµένου καθένα από τα οποία αντιστοιχεί και 
σε ένα διαφορετικό θέµα, στραφήκαµε στη θεµατική περιοχή της “Τµηµατοποίησης Κειµένων” 
(“Text Segmentation”, [Gloss(00092)]), ([Halliday & Hasan, 1976], [Hirschberg & Litman, 1993], 
[Reynar, 1994], [Reynar, 1998], [Yaari, 1997], [Yaari, 1999], [Hearst, 1993],  [Beeferman et al., 1999], 
([Choi et al., 2001]). Πιο συγκεκριµένα εξετάσαµε τρόπους εύρεσης της οµοιότητας µεταξύ των 
διαφόρων τµηµάτων καθώς και τρόπους αυτόµατου καθορισµού των ορίων µεταξύ των τµηµάτων 
µέσα σε ένα κείµενο. Επίσης εξετάσαµε το συνδυασµό τεχνικών Μηχανικής Μάθησης και 
Τµηµατοποίησης Κειµένων. Τα πειράµατα τα οποία παρουσιάζονται πραγµατοποιήθηκαν σε κείµενα 
τα οποία δεν έφεραν κανένα σχολιασµό και καµία πληροφορία σε ότι αφορά τη δοµή τους. Αξίζει να 
σηµειωθεί ότι στα πλαίσια της παρούσας διατριβής εξετάστηκαν τόσο αγγλικά όσο και ελληνικά 
κείµενα.  
1.3  Συνεισφορά της διατριβής 
 
Το απόσταγµα των µοντέλων τα οποία αναπτύχθηκαν στα πλαίσια της παρούσας διατριβής και 
των πειραµάτων τα οποία πραγµατοποιήθηκαν πάνω σε αυτά σε συνδυασµό µε την επισκόπηση της 
διεθνούς βιβλιογραφίας πάνω στο θέµα της Κατηγοριοποίησης κειµένων (“Text Classification”, 
[Gloss(00091)]) κάνοντας χρήση των εννοιών των λέξεων, της Τµηµατοποίησης κειµένων (“Text 
Segmentation”, [Gloss(00092)]) µε την βοήθεια τεχνικών κατηγοριοποίησης κειµένων και µε την 
βοήθεια τεχνικών αυτόµατου υπολογισµού των ορίων µεταξύ των τµηµάτων, µπορούν να 
συνoψισθούν στα ακόλουθα συµπεράσµατα τα οποία εκφράζονται µε την µορφή ισχυρισµών: 
 6
Κεφάλαιο 1 Εισαγωγή   
Ισχυρισµός 1ος: Η κατηγοριοποίηση κειµένων κάνοντας χρήση των εννοιών των λέξεων  -σύµφωνα µε 
το περιεχόµενο στο οποίο εµφανίζονται όπως αυτό προέκυψε από την συλλογή κειµένων του Brown 
Corpus ([Francis & Kucera, 1982]) η οποία λαµβάνει την έννοια κάθε λέξης κάνοντας χρήση του 
θησαυρού όρων Wordnet ([Miller et al., 1990]) - προκύπτει οριακά καλύτερη από την 
κατηγοριοποίηση των κειµένων χρησιµοποιώντας τις αυτούσιες λέξεις. Το παραπάνω συµπέρασµα 
προέκυψε ως απόσταγµα εφαρµογής γνωστών αλγορίθµων κατηγοριοποίησης στο εν λόγω σώµα 
κειµένων. Μια πιθανή ερµηνεία του αποτελέσµατος θα πρέπει να λάβει υπόψη της τον χειρωνακτικό 
και υποκειµενικό τρόπο απόδοσης της έννοιας κάθε λέξης η οποία εµφανίζεται στα εν λόγω κείµενα 
Ισχυρισµός 2ος: Η τµηµατοποίηση κειµένων κάνοντας χρήση τεχνικών κατηγοριοποίησης  κειµένων 
αποτελεί µια καινοτόµο προσέγγιση του προβλήµατος τµηµατοποίησης κειµένων. Παρόλα αυτά 
απαιτεί για την αποτελεσµατική εφαρµογή της την πληροφορία των υπαρχουσών κατηγοριών στις οποίες 
ανήκουν τα κείµενα. ∆υστυχώς στην βιβλιογραφία δεν εµφανίζονται άλλες παρόµοιες προσεγγίσεις για 
το ίδιο πρόβληµα ούτε σώµατα κειµένων για τα οποία είναι γνωστή η πληροφορία των υπαρχουσών 
κατηγοριών στις οποίες ανήκουν τα κείµενα και των ορίων µεταξύ των τµηµάτων, γεγονός που µας 
οδήγησε στην κατασκευή ενός τεχνητού σώµατος κειµένων. Παρόλα αυτά, η εν λόγω προσέγγιση 
είναι πρωτότυπη και αξίζει να εξεταστεί σε µεγαλύτερο βάθος.  
Ισχυρισµός 3ος: Η τµηµατοποίηση κειµένων κάνοντας χρήση συνδυασµού τεχνικών εύρεσης της 
οµοιότητας µεταξύ όλων των µερών ενός κειµένου και τεχνικών αυτόµατου υπολογισµού των ορίων 
µεταξύ τµηµάτων, όπως αυτής του δυναµικού προγραµµατισµού, αποτελεί µια προσέγγιση η οποία 
δεν είχε εξεταστεί µέχρι τώρα στην βιβλιογραφία. Ο προτεινόµενος σε αυτή τη διατριβή τρόπος 
υπολογισµού της συνάρτησης κόστους για τον υπολογισµό των ορίων µεταξύ των τµηµάτων αποδείχθηκε 
ιδιαίτερα καινοτόµος και αποτελεσµατικός συγκριτικά µε τους προτεινόµενους στη βιβλιογραφία 
αλγορίθµους. Ο εν λόγω αλγόριθµος απέδειξε την ευρωστία του τόσο σε γνωστά στην βιβλιογραφία 
σώµατα κειµένων όσο και σε µια πρωτο-χρησιµοποιούµενη για αυτό το πρόβληµα συλλογή που 
περιείχε ελληνικά κείµενα. Ο συγκεκριµένος τρόπος υπολογισµού της συνάρτησης κόστους για τον 
υπολογισµό των ορίων µεταξύ των τµηµάτων αποτελεί και τη µεγαλύτερη συνεισφορά της εν λόγω 
διατριβής.  
 
Η παρούσα διατριβή χαίρει πλήθους νεωτεριστικών στοιχείων. Το πρώτο νεωτεριστικό στοιχείο 
αποτελεί η προσέγγιση του εξεταζόµενου προβλήµατος από τρεις διαφορετικές σκοπιές µε την 
ανάπτυξη τριών διαφορετικών µοντέλων. Σηµαντική καινοτοµία του πρώτου µοντέλου αποτελεί η 
χρήση των εννοιών σε όλο το εύρος του κειµένου και η εξέταση σηµαντικού πλήθους αλγορίθµων 
κατηγοριοποίησης για τη σύγκριση του αποτελέσµατος της κατηγοριοποίησης µε βάση τις έννοιες και 
τις αυτούσιες λέξεις των κειµένων. Από όσο είµαστε σε θέση να γνωρίζουµε, µια τέτοιου είδους 
συγκριτική παράθεση πραγµατοποιείται για πρώτη φορά στη βιβλιογραφία, γεγονός που αναδεικνύει 
 7
Κεφάλαιο 1 Εισαγωγή   
τη βαρύτητά της. Ένας από τους σηµαντικότερους νεωτερισµούς της διατριβής αποτελεί η προσέγγιση 
της τµηµατοποίησης κειµένων µε τη χρήση τεχνικών κατηγοριοποίησης – στοιχείο το οποίο λαµβάνει 
χώρα στο δεύτερο µοντέλο- καθώς και η συγκριτική παράθεση της εν λόγω προσέγγισης µε τη χρήση 
τόσο των εννοιών όσο και των αυτούσιων λέξεων των κειµένων. Ο συνδυασµός µεθόδων 
κατηγοριοποίησης (µε εναλλακτική χρήση εννοιών και λέξεων) και τµηµατοποίησης απαντάται για 
πρώτη φορά στη βιβλιογραφία και πλεονεκτεί στο γεγονός ότι «εκµαιεύει» την πληροφορία που 
περιέχεται σε ένα κείµενο µε δυο διαφορετικούς τρόπους.  
 
Ιδιαίτερα πρωτότυπη αποτελεί η προσέγγιση που ακολουθείται στο τρίτο µοντέλο το οποίο 
αντιµετωπίζει µε ολικό τρόπο τόσο τον υπολογισµό της οµοιότητας ανάµεσα στα διάφορα µέρη ενός 
κειµένου όσο και τον τρόπο εύρεσης των ορίων µεταξύ των τµηµάτων. Οι ως τώρα προτεινόµενες στη 
βιβλιογραφία εργασίες αντιµετωπίζουν τον ένα µόνο από τους παραπάνω δυο υπολογισµούς µε ολικό 
τρόπο αλλά ποτέ και τους δυο ταυτόχρονα. Η επιτυχίας της δικής µας προσέγγισης διαφαίνεται από τα 
ληφθέντα αποτελέσµατα σε όλες τις εξεταζόµενες συλλογές κειµένων.  
 
Τέλος ένα από τα πλέον νεωτεριστικά στοιχεία της παρούσας διατριβής αποτελεί η κατασκευή 
του σώµατος κειµένων προς τµηµατοποίηση γραµµένων στην ελληνική γλώσσα (τα οποία 
προέρχονται από άρθρα της εφηµερίδας «Το Βήµα») και η εφαρµογή του αλγορίθµου µας πάνω σε 
αυτό. Το πρόβληµα της τµηµατοποίησης ελληνικών κειµένων αντιµετωπίζεται για πρώτη φορά. 
1.4  ∆οµή της διατριβής 
 
Η παρούσα διατριβή δοµείται σε 7 κεφάλαια. Στο παρόν κεφάλαιο γίνεται σύντοµη παρουσίαση 
του αντικειµένου και των στόχων της διατριβής.  
 
Το Κεφάλαιο 2 παρουσιάζει το πρόβληµα της κατηγοριοποίησης κειµένων κάνοντας χρήση της 
έννοιας της κάθε λέξης -όπως αυτή προσδιορίζεται από το περιεχόµενο µέσα στο οποίο αυτή 
απαντάται και όπως αυτή δίνεται από τον θησαυρού όρων Wordnet – και σύγκριση µε την 
κατηγοριοποίηση µε χρήση των αυτούσιων λέξεων του κειµένου. Στο εν λόγω κεφάλαιο µετά τη 
συνοπτική παρουσίαση των βιβλιογραφικών αναφορών και προσεγγίσεων πάνω στο θέµα, 
παρουσιάζεται το προτεινόµενο µοντέλο µας και όλες οι χρησιµοποιούµενες µέθοδοι 
κατηγοριοποίησης καθώς και τα αποτελέσµατα τα οποία λάβαµε και τα συµπεράσµατα στα οποία 
καταλήξαµε. 
 
 8
Κεφάλαιο 1 Εισαγωγή   
Το Κεφάλαιο 3 εξετάζει το πρόβληµα της τµηµατοποίησης κειµένων δίνοντας ιδιαίτερη έµφαση 
στις προσεγγίσεις οι οποίες παρουσιάζονται στη διεθνή βιβλιογραφία. Επίσης καταγράφει µερικά από 
τα προβλήµατα τµηµατοποίησης κειµένων τα οποία έχουν διατυπωθεί και τα οποία εξετάστηκαν µέχρι 
εκείνη την στιγµή και οριοθετεί το πρόβληµα το οποίο καλούµαστε να επιλύσουµε. 
Το Κεφάλαιο 4 παρουσιάζει το πρόβληµα της τµηµατοποίησης κειµένων κάνοντας χρήση 
µεθόδων κατηγοριοποίησης. ∆εδοµένου ότι το εν λόγω πρόβληµα -από όσο είµαστε σε θέση να 
γνωρίζουµε- απαντάται για πρώτη φορά στην βιβλιογραφία δεν παραθέτονται βιβλιογραφικές 
αναφορές. Στην συνέχεια παρουσιάζεται το µοντέλο και ο αλγόριθµος τµηµατοποίησης κειµένων µε 
την βοήθεια τεχνικών κατηγοριοποίησης κειµένων (“Text Classification”, [Gloss(00091)]). Γίνεται 
αναλυτική αναφορά στις µεθόδους που χρησιµοποιήθηκαν τόσο για την τµηµατοποίηση όσο και για 
την κατηγοριοποίηση, στα πειράµατα τα οποία πραγµατοποιήθηκαν και στα συµπεράσµατα στα οποία 
καταλήξαµε.   
Το Κεφάλαιο 5 παρουσιάζει το µοντέλο τµηµατοποίησης κειµένων το οποίο υλοποιείται ως 
συνδυασµός µιας τεχνικής εύρεσης της οµοιότητας µεταξύ όλων των µερών του κειµένου και µιας 
τεχνικής αυτόµατου καθορισµού των ορίων µεταξύ των τµηµάτων η οποία βασίζεται στην λογική του 
δυναµικού προγραµµατισµού. Στην συνέχεια στο κεφάλαιο παραθέτονται συγκριτικά αποτελέσµατα 
εφαρµογής του προτεινόµενου αλγορίθµου µας τόσο πάνω σε γνωστό στην βιβλιογραφία σώµα 
κειµένων όσο και σε σώµα κειµένων το οποίο κατασκευάστηκε από εµάς για τον σκοπό της διατριβής, 
Στο τέλος του κεφαλαίου παραθέτονται τα συµπεράσµατα στα οποία καταλήξαµε και τα 
πλεονεκτήµατα του µοντέλου µας έναντι των άλλων που παρουσιάζονται στην βιβλιογραφία.  
 
Το Κεφάλαιο 6 παρουσιάζει εφαρµογές του µοντέλου τµηµατοποίησης κειµένων που 
παρουσιάστηκε στο Κεφάλαιο 5 σε σώµα ελληνικών κειµένων το οποίο προήλθε από άρθρα της 
εφηµερίδας «Το Βήµα». Για την πραγµατοποίηση των πειραµάτων έγινε στο αρχικό στάδιο χρήση του 
Μορφοσυντακτικού Αναλυτή της Νέας Ελληνικής ο οποίος υλοποιήθηκε στα πλαίσια της 
διδακτορικής διατριβής του Γεώργιου Ορφανού ([Orphanos, 2000]). Τα πειράµατα τα οποία 
παραθέτονται στο κεφάλαιο αυτό αφορούν την τµηµατοποίηση των παραπάνω κειµένων κάνοντας 
χρήση ενός µεταβλητού αριθµού συγγραφέων, αλλά και την τµηµατοποίηση µεγάλης έκτασης 
κείµενων σε ότι αφορά το µέγεθος του τµήµατος. Τέλος παραθέτονται τα συµπεράσµατα στα οποία 
καταλήξαµε. 
 
Το Κεφάλαιο 7 αποτελεί τον επίλογο της διατριβής. Γίνεται αποτίµηση του έργου που 
παρουσιάστηκε. Αναφέρονται τα θέµατα που αφορούν την τµηµατοποίηση κειµένων τα οποία δεν 
καλύφθηκαν από το έργο αυτό. Τέλος σκιαγραφούνται τα ανοιχτά θέµατα που προκύπτουν και οι 
 9
Κεφάλαιο 1 Εισαγωγή   
µελλοντικές ερευνητικές κατευθύνσεις που µπορούν να ακολουθηθούν χρησιµοποιώντας  τους 
αλγορίθµους που αναπτύχθηκαν στην παρούσα διατριβή. 
1.5 ∆ηµοσιεύσεις  
 
Η παρούσα διατριβή βασίζεται στις παρακάτω δηµοσιεύσεις. Ευχαριστώ τους συν-συγγραφείς για 
την πολύτιµη συνεισφορά τους. 
1. [Petridis et al., 2001] Petridis, V., Kaburlazos, V., Fragkou, P. and  Kehagias, A. “Text 
Classification using the σ-FLNMAP Neural Network”. In Proceedings of the 2001 International 
Joint Conference on Neural Networks (IJCNN’2001), Washington D.C., 14-19 July 2001, vol. 2, 
pp. 1362-1367. 
2. [Kehagias et al., 2003 (a)]  Kehagias, A., Fragkou, P. and Petridis, V. (2003). “Linear Text 
Segmentation using a Dynamic Programming Algorithm”. In Proceedings of the Eacl’03, April 12-
17, 2003, Budapest, Hungary, pp. 171-178. 
3. [Kehagias et al., 2003 (b)] Kehagias, A., Petridis, V., Kaburlasos, V.G. and Fragkou, P. (2003). “A 
Comparison of Word- and Sense-based Text Categorization Using Several Classification 
Algorithms”. Journal of Intelligent Information Systems, Kluwer Academic Publishers. Vol 21. 
No.3 November 2003, pp. 227-248. 
4. [Kehagias et al., 2004]  Kehagias, A., Nicolaou, A., Fragkou, P. and Petridis, V. (2004). “Text 
Segmentation by Product Partition Models and Dynamic Programming”. Mathematical and 
Computer Modelling, vol. 39, pp. 209-217. 
5. [Fragkou, 2003] Fragkou Pavlina. (2003). “A Dynamic Programming Algorithm for the 
Segmentation of Greek Texts”. In Proceedings of CONSOLE XII (International Conference of 
Graduate Students), December 12-14, 2003, Patra Greece. 
6. [Fragkou et al., 2004 (a)] Fragkou, P., Petridis, V. and Kehagias, A. (2004). “A Dynamic 
Programming Algorithm for Linear Text Segmentation”. Το appear in Journal of Intelligent 
Information Systems, Kluwer Academic Publishers. 
7. [Fragkou et al., 2004 (b)] Fragkou, P., Petridis, V. and Kehagias, A. (2004). “Linear Text 
Segmentation of Greek Texts using a Dynamic Programming Algorithm”. (submitted). 
 10
 
 
11 
Κεφάλαιο 2 Κατηγοριοποίηση κειµένων µε χρήση λέξεων και  εννοιών 
ΚΕΦΑΛΑΙΟ 2 
ΚΑΤΗΓΟΡΙΟΠΟΙΗΣΗ ΚΕΙΜΕΝΩΝ ΜΕ ΧΡΗΣΗ 
ΛΕΞΕΩΝ ΚΑΙ ΕΝΝΟΙΩΝ 
2.1 Εισαγωγή 
 
Όπως αναφέρθηκε στο κεφάλαιο 1, βασικός στόχος της παρούσας διατριβής είναι τόσο η 
κατηγοριοποίηση κειµένων µε τη βοήθεια των λέξεων και των εννοιών αυτών, όσο και η 
τµηµατοποίηση µεγάλης έκτασης κειµένων µε χρήση µεθόδων υπολογιστικής νοηµοσύνης, στόχος 
των οποίων αποτελεί η εµβάθυνση στο περιεχόµενο των κειµένων και η ανάδειξη του τρόπου δόµησής 
τους.  Στην ουσία, το πρόβληµα το οποίo καλείται να επιλύσει είναι η βελτίωση της πρόσβασης στην 
πληροφορία (όπως π.χ. αυτή λαµβάνει χώρα στις διάφορες µηχανές αναζήτησης) ούτως ώστε η 
επιστρεφόµενη σε ένα ερώτηµα (“query”, [Gloss(00070)]) του χρήστη πληροφορία να είναι αξιόπιστη 
τόσο σε επίπεδο ακρίβειας ως προς το ζητούµενο θέµα όσο και σε επίπεδο έκτασης. Για τον σκοπό 
αυτό στραφήκαµε στις θεµατικές περιοχές των θησαυρών όρων και της κατηγοριοποίησης κειµένων. 
2.1.1 Κατηγοριοποίηση κειµένων 
 
Κατηγοριοποίηση κειµένων (“Text Classification”, [Gloss (00091)]) ([Duda et al., 2001], 
[Cohen, 1995(a)], [Cohen, 1995(b)], [Cohen, 1995(c)], [Cohen & Hirsh, 1998], [Lewis, 1992], [Lewis 
& Jones, 1996], [Lewis et al., 1996], [Manning & Schuetze, 1999], [Mitchell, 1997], [Sahami et al, 
1996], [Wiener & al., 1995], [Yang & Liu, 1999], [Yang, 1995], [Yang, 1998]) είναι η διαδικασία που 
µας επιτρέπει να βρούµε τις κοινές ιδιότητες που εµφανίζονται σε ένα σύνολο κειµένων - το οποίο 
εναλλακτικά ονοµάζεται «σύνολο κειµένων προς εκπαίδευση» - και να τις οµαδοποιούµε σύµφωνα µε 
έναν αλγόριθµο κατηγοριοποίησης. Στόχος της κατηγοριοποίησης είναι η ανάλυση των κειµένων 
(προς εκπαίδευση) και ο καθορισµός µιας εµπεριστατωµένης περιγραφής ή αλλιώς ενός ακριβούς 
µοντέλου για κάθε µια από τις δοθείσες κατηγορίες στις οποίες ανήκουν τα κείµενα χρησιµοποιώντας 
τα διαθέσιµα χαρακτηριστικά καθενός από αυτά. Η διαδικασία κατασκευής του µοντέλου ονοµάζεται 
 12
Κεφάλαιο 2 Κατηγοριοποίηση κειµένων µε χρήση λέξεων και  εννοιών 
εναλλακτικά διαδικασία «εκµάθησης» του συνόλου των κειµένων. Χαρακτηριστικό των δοθέντων 
κειµένων αποτελεί το γεγονός ότι µπορούν να ανήκουν σε περισσότερες από µια κατηγορίες.  
Στόχος της κατηγοριοποίησης είναι η χρησιµοποίηση του µοντέλου που προέκυψε από τη 
διαδικασία εκµάθησης για τον προσδιορισµό της κατηγορίας στην οποία ανήκει ένα οποιοδήποτε 
κείµενο. Για την ανάπτυξη ενός µοντέλου κατηγοριοποίησης από ένα σύνολο κειµένων χρειαζόµαστε: 
1. Mια µέθοδο προεπεξεργασίας του εκάστοτε κειµένου µε σκοπό την αφαίρεση εκείνου του είδους 
της πληροφορίας η οποία δε µπορεί να συνεισφέρει στον προσδιορισµό της κατηγορίας στην 
οποία ανήκει το εν λόγω κείµενο. Οι πιο συνηθισµένες µορφές προεπεξεργασίας είναι η αφαίρεση 
εκείνων των λέξεων οι οποίες ανήκουν στην stop-list (στην οποία περιέχονται όλες οι προθέσεις 
τα άρθρα, κοινότυπα ρήµατα,και γενικότερα οι λέξεις που δεν προσδίδουν ιδιαίτερη πληροφορία 
µέσα σε ένα κείµενο και η οποία παρατίθεται στο Παράρτηµα Α1) και η αντικατάσταση των 
υπολοίπων στη ρίζα τους (“Stemmed words”, [Gloss (00084)]).  
2. Έναν τρόπο παράστασης του εκάστοτε κειµένου ο οποίος στην ουσία θα αποτελεί ένα είδος 
συσχέτισης ανάµεσα στις κατηγορίες και τα χαρακτηριστικά που διέπουν τα κείµενα στις οποίες 
αυτά ανήκουν. Ο επικρατέστερος στην βιβλιογραφία τρόπος είναι αυτός του «µοντέλου 
διανυσµατικού χώρου» (“vector space model”, [Gloss(00100)]). Σύµφωνα µε αυτή την 
παράσταση, κάθε κείµενο χαρακτηρίζεται είτε από ένα δυαδικό είτε από ένα αριθµητικό άνυσµα. 
Η διάσταση του ανύσµατος συνήθως αντιστοιχεί στο πλήθος των µοναδικών και «ευδιάκριτων» 
όρων που εµφανίζονται στο σύνολο των κειµένων προς επεξεργασία. Ένα δοσµένο άνυσµα 
κειµένου περιέχει σε κάθε συνιστώσα µια αριθµητική τιµή η οποία υπακούει σε µια συνάρτηση 
και η οποία δηλώνει το πόσο συχνά εµφανίζεται ο όρος ο οποίος αντιστοιχεί στη συγκεκριµένη 
διάσταση στο εν λόγω κείµενο. Η συνάρτηση αυτή περιγράφει τη συµπεριφορά των κειµένων και 
των όρων που περιέχονται σε αυτά σε συνάρτηση µε την ή τις κατηγορίες που ανήκουν.  
3. Το σύνολο των κατηγοριών στις οποίες ανήκουν τα κείµενα.  
4. Έναν ικανοποιητικό αριθµό κειµένων ο οποίος µας επιτρέπει να περιγράψουµε διεξοδικά και 
αποτελεσµατικά κάθε µια από τις υπάρχουσες κατηγορίες.  
5. Έναν αλγόριθµο κατηγοριοποίησης ο οποίος θα περιγράφει κάθε κατηγορία µε την βοήθεια των 
χαρακτηριστικών όρων που την χαρακτηρίζουν. Στην περίπτωση της κατηγοριοποίησης κειµένων 
ένας χαρακτηριστικός όρος µπορεί να είναι µια µεµονωµένη λέξη ή µια οµάδα από λέξεις.  
6. Ένα σύνολο κειµένων τα οποία θα δοθούν προς επαλήθευση και τα οποία δεν θα πρέπει να έχουν 
χρησιµοποιηθεί κατά τη διάρκεια της διαδικασίας εκµάθησης. Ένα καλό τεστ του µοντέλου 
εκµάθησης είναι η αξιολόγηση ενός αντιπροσωπευτικού δείγµατος πρωτοεµφανιζόµενων  
 13
Κεφάλαιο 2 Κατηγοριοποίηση κειµένων µε χρήση λέξεων και  εννοιών 
κειµένων, στοιχείο το οποίο αποτελεί το µοναδικό κριτήριο εξακρίβωσης της πραγµατικής 
απόδοσης κατηγοριοποίησης.  
Για την κατασκευή του αλγορίθµου εκµάθησης έχουν προταθεί στην βιβλιογραφία διάφορες 
τεχνικές οι οποίες εκφράζουν µε διαφορετικό τρόπο τη συσχέτιση ανάµεσα στις κατηγορίες και τα 
χαρακτηριστικά που διέπουν τα κείµενα στις οποίες ανήκουν. Στα επόµενα υποκεφάλαια, 
περιγράφονται περιληπτικά, ορισµένοι από τις πιο αντιπροσωπευτικούς αλγορίθµους εκµάθησης της 
θεµατικής περιοχής της Μηχανικής Μάθησης (“Machine Learning”, [Gloss(00054)]) οι οποίοι 
χρησιµοποιούνται ευρέως για την επίλυση προβληµάτων κατηγοριοποίησης.  
2.1.1.1 Κατηγοριοποιητής Bayesian ∆ικτύου 
 
Ένας κατηγοριοποιητής Bayesian δικτύου (“Bayesian network classifier”, [Gloss(00009)]) 
([Domingos & Pazzani, 1997], [Sahami et al., 1998]) είναι ένας κατευθυνόµενος γράφος (“direct 
acyclic graph”, [Gloss(00029)]) στον οποίο κάθε κόµβος αντιστοιχεί είτε σε ένα χαρακτηριστικό όρο 
(“feature”, [Gloss(00024)]) είτε σε µια από τις υπάρχουσες κατηγορίες. Οι συσχετίσεις µεταξύ των 
όρων και των κατηγοριών δηλώνονται µε την βοήθεια τόξων τα οποία συνδέουν τους κόµβους µεταξύ 
τους.  Η έλλειψη συσχέτισης µεταξύ δυο κόµβων δηλώνεται µε την έλλειψη σύνδεσης µέσω ενός 
τόξου ανάµεσα σε δυο κόµβους. ∆οθέντος ενός ανύσµατος χ , το Bayesian δίκτυο µας επιτρέπει να 
υπολογίσουµε την πιθανότητα το εξεταζόµενο άνυσµα να ανήκει σε κάποια από τις δοθείσες 
κατηγορίες. Η απλούστερη αλλά και πιο εφαρµοσµένη έκδοση αυτού του είδους κατηγοριοποιητή 
είναι ο κατηγοριοποιητής Naïve Bayes ([Baker & McCallum, 1998], [Craven et al., 1998], [McCallum 
& Nigam, 1998(a)], [McCallum & Nigam, 1998(b)], [Nigam et al., 1998], [Nigam et al., 1999]).   
2.1.1.2 ∆έντρα απόφασης  
 
Ένα δέντρο απόφασης (“decision tree”, [Gloss(00021)]) ([Breiman, 1994], [Quinlan, 1986], 
[Quinlan, 1993]) είναι µια δοµή η οποία αποτελείται α) από φύλλα καθένα από τα οποία αντιστοιχεί 
σε µια κατηγορία και β) από κόµβους απόφασης (“decision nodes”, [Gloss(00020)]) καθένας από τους 
οποίους προσδιορίζει κάποια εξέταση η οποία θα διεξαχθεί πάνω στην τιµή ενός χαρακτηριστικού 
όρου. Κάθε κόµβος απόφασης διαθέτει µια διακλάδωση και ένα υποδέντρο για κάθε ένα από τα 
πιθανά αποτελέσµατα της εξέτασης. Ένα δέντρο απόφασης µπορεί να χρησιµοποιηθεί για την 
κατηγοριοποίηση µιας περίπτωσης ξεκινώντας από την ρίζα του δέντρου και διατρέχοντας το δέντρο 
µε σκοπό να καταλήξουµε σε κάποιο από τα φύλλα αυτού. Σε κάθε κόµβο απόφασης του δέντρου – ο 
 14
Κεφάλαιο 2 Κατηγοριοποίηση κειµένων µε χρήση λέξεων και  εννοιών 
οποίος δεν αποτελεί φύλλο - το αποτέλεσµα της εξέτασης της περίπτωσης σε αυτό τον κόµβο 
καθορίζει και το υποδέντρο το οποίο θα ακολουθήσουµε. Όταν τελικά η παραπάνω διαδικασία 
οδηγήσει σε κάποιο φύλλο, η περίπτωση που εξετάζουµε κατατάσσεται στην κατηγορία στην οποία 
ανήκει το φύλλο. 
2.1.1.3 Κανόνες Απόφασης  
 
Ένας κανόνας απόφασης (“Production Rule”, [Gloss(00068)]) έχει τη µορφή L->R όπου κάθε 
αριστερή πλευρά L είναι µια σύνδεση από τεστ βασιζόµενα στους χαρακτηριστικούς όρους, ενώ η 
δεξιά πλευρά R αποτελεί µια κατηγορία. Για την κατηγοριοποίηση µιας περίπτωσης χρησιµοποιώντας 
το µοντέλο κανόνων απόφασης, εξετάζεται η διατεταγµένη λίστα των κανόνων απόφασης για την 
εύρεση εκείνου που ικανοποιεί την αριστερή πλευρά για την εξεταζόµενη περίπτωση. Στην 
εξεταζόµενη περίπτωση αναθέτεται η κατηγορία που αντιστοιχεί στη δεξιά πλευρά του κανόνα που 
ικανοποιήθηκε. Αν δεν ικανοποιηθεί κανένας από τους κανόνες τότε η εξεταζόµενη περίπτωση 
κατατάσσεται στην προεπιλεγµένη (“default”, [Gloss(00022)]) κατηγορία. Σε αυτή την οικογένεια 
κατηγοριοποιητών ανήκουν και οι γνωστοί αλγόριθµοι IREP (Incremental Reduced Error) και 
RIPPER ([Cohen, 1995(a)], [Cohen, 1995(b)], [Cohen, 1995(c)] [Cohen & Hirsh, 1998], [Cohen & 
Singer, 1996], [Cohen, 1996]).  
2.1.1.4 Τεχνητά Νευρωνικά ∆ίκτυα  
 
Οι τεχνικές των Νευρωνικών ∆ικτύων (“Artificial Neural Networks”, [Gloss(00002)]) 
εξετάστηκαν εκτεταµένα από τους Ng, Goh και Low, Wiener, Pedersen και Weigend, αλλά και τους 
Yang και Liu ([Ng et al., 1997], [Wiener & al., 1995], [Yang & Liu, 1999], [Yang, 1995], [Yang, 
1998]). 
  
Ένα νευρωνικό δίκτυο αποτελείται από µονάδες που συνδέονται µεταξύ τους µε συνδέσµους. 
Υπάρχουν τρεις κατηγορίες µονάδων: οι µονάδες εισόδου οι οποίες εισάγουν πληροφορία στο δίκτυο 
από το περιβάλλον, οι µονάδες εξόδου οι οποίες παρέχουν τα αποτελέσµατα, και άλλες µονάδες οι 
οποίες είναι «κρυφές» από το περιβάλλον. Κάθε σύνδεσµος έχει ένα συσχετιζόµενο βάρος ενώ 
ορισµένες µονάδες έχουν µια πόλωση (“bias”, [Gloss(00010)]). Για την εξέταση µιας περίπτωσης, οι 
µονάδες εισόδου λαµβάνουν τιµές µεταξύ 0 και 1 οι οποίες αντιστοιχούν στις τιµές των 
χαρακτηριστικών όρων. Η είσοδος κάθε µονάδας καθορίζεται από το άθροισµα: α) της σταθµισµένης 
(“weighted”, [Gloss(00101)]) εξόδου των µονάδων που συνδέονται µε αυτή, β) της πόλωσης της 
 15
Κεφάλαιο 2 Κατηγοριοποίηση κειµένων µε χρήση λέξεων και  εννοιών 
µονάδας και γ) της εξόδου της µονάδας. Οι τιµές των βαρών των δικτύων και των πολώσεων 
«µαθαίνονται» ύστερα από επαναλαµβανόµενες εξετάσεις των περιπτώσεων εκµάθησης.  
Η απόκλιση της εξόδου κάθε µονάδας από τη σωστή τιµή για κάθε περίπτωση διαδίδεται 
(“Back propagated”, [Gloss(00003)]) δια µέσω του δικτύου. Έτσι, προσαρµόζονται όλα τα σχετικά 
βάρη των συνδέσεων και οι πολώσεις των µονάδων χρησιµοποιώντας τον αλγόριθµο µέγιστης κλίσης 
(“gradient descent”, [Gloss(00033)]) µε στόχο να προσεγγίσει η έξοδος τον επιθυµητό στόχο – τιµή. Η 
εκπαίδευση του νευρωνικού δικτύου συνεχίζεται µέχρις ότου σταθεροποιηθούν τα βάρη και οι 
πολώσεις.  
2.1.1.5 Γενετικοί Αλγόριθµοι 
 
Οι γενετικοί αλγόριθµοι ([Dowis, 1991], [Holland, 1975]) βασίζονται σε έναν πληθυσµό από 
άτοµα -τα οποία περιέχουν κάποια χαρακτηριστικά- τα οποία ανταγωνίζονται µεταξύ τους για τη 
δηµιουργία µιας πρόβλεψης. Τα άτοµα εκείνα τα οποία παρουσιάζουν χαµηλή απόδοση 
αποµακρύνονται από τον πληθυσµό, ενώ εκείνα που παρουσιάζουν υψηλή απόδοση 
πολλαπλασιάζονται και παράγουν παραλλαγές του εαυτού τους.  
 
Σε µια απλή µορφή γενετικού αλγορίθµου για διακριτούς χαρακτηριστικούς όρους, κάθε 
άτοµο αποτελείται από: 
• Μια µονάδα ταξινόµησης (“taxon”, [Gloss(00090)]), η οποία καθορίζει για κάθε 
χαρακτηριστικό όρο είτε µια συγκεκριµένη τιµή η οποία θα πρέπει να συνταιριαστεί µε µια 
περίπτωση είτε µια εξ’ορισµού τιµή (“don’t care”, [Gloss(00028)]).  
• Μια προβλεπόµενη κατηγορία (“predicted class”, [Gloss(00065)]). 
• Μια δύναµη (“strength”, [Gloss(00085)]). 
Κάθε φορά που θέλουµε να κατηγοριοποιήσουµε µια περίπτωση, εξετάζουµε κάθε άτοµο για 
να εξακριβώσουµε κατά πόσο η περίπτωση ταιριάζει µε το άτοµο (δηλαδή έχει τις απαιτούµενες τιµές 
για όλους τους χαρακτηριστικούς όρους). Από εκείνα τα άτοµα που ταιριάζουν µε την περίπτωση, 
επιλέγεται τυχαία ένα το οποίο όµως έχει πιθανότητα ανάλογη µε το άτοµο το οποίο προσδιορίζει την 
προβλεπόµενη κατηγορία. Κατά την διάρκεια της εκµάθησης, οι δυνάµεις των ατόµων 
προσαρµόζονται κατά τέτοιο τρόπο ούτως ώστε να επιβραβεύουν σωστές προβλέψεις και/ή να 
«τιµωρούν» λάθη. Ο συνολικός πληθυσµός περιοδικά υφίσταται ραγδαία αναπροσαρµογή στην οποία 
ασθενή στοιχεία πεθαίνουν ενώ νέα άτοµα δηµιουργούνται. Η τελευταία διαδικασία προκύπτει ως 
 16
Κεφάλαιο 2 Κατηγοριοποίηση κειµένων µε χρήση λέξεων και  εννοιών 
αποτέλεσµα της «µετάλλαξης» (“mutation”, [Gloss(00059)]) στην οποία πραγµατοποιούνται τυχαίες 
αλλαγές στην µονάδα ταξινόµησης και στην αναπαραγωγή (“breeding”, [Gloss(00011)]) ενός ατόµου, 
όπου δύο άτοµα ενώνονται για να δώσουν ένα νέο άτοµο του οποίου η µονάδα ταξινόµησης παρέχεται 
µερικώς από τον κάθε γονιό.  
2.1.1.6 Support Vector Machines 
 
Τα Support Vector Machines ([Drucker et al., 1999], [Joachims, 1998], [Vapnik, 1995]) 
αποτελούν µια σχετικά καινούργια µέθοδο την οποία εισήγαγε ο Vapniκ το 1995 για την επίλυση 
προβληµάτων αναγνώρισης προτύπων. Η µέθοδος ορίζεται πάνω σε ένα χώρο ανυσµάτων, όπου το 
πρόβληµα έγκειται στην εύρεση µιας επιφάνειας απόφασης η οποία χωρίζει κατά βέλτιστο τρόπο τα 
σηµεία τα οποία αντιστοιχούν στα δεδοµένα, στις υπάρχουσες κατηγορίες. Για να µπορέσουµε να 
ορίσουµε τον βέλτιστο διαχωρισµό είναι αναγκαία η εισαγωγή της έννοιας του «περιθωρίου» 
(“margin”, [Gloss(00056)]) το οποίο είναι η απόσταση, στο διανυσµατικό χώρο, ανάµεσα σε δυο 
κατηγορίες και εξαρτάται από το συνολικό αριθµό των κατηγοριών (εδώ το πλήθος των κατηγοριών 
ισούται µε δυο). Μια «επιφάνεια απόφασης» σε ένα γραµµικώς διαχωρίσιµο χώρο είναι ένα 
υπερεπίπεδο (“hyperplane”, [Gloss(00035)]). Το πρόβληµα των Support Vector Machines είναι η 
εύρεση εκείνης της «επιφάνειας απόφασης» η οποία µεγιστοποιεί το «περιθώριο» ανάµεσα στα 
σηµεία που αντιστοιχούν στα δεδοµένα του σώµατος κειµένων προς εκπαίδευση. 
2.1.1.7 Κατηγοριοποιητές βασιζόµενοι σε στιγµιότυπα– Rocchio 
 
Ένας τρόπος κατηγοριοποίησης µιας περίπτωσης είναι µε την ανάκληση µιας παρόµοιας 
περίπτωσης της οποίας η κατηγορία είναι γνωστή και η πρόβλεψη του ότι η νέα περίπτωση θα ανήκει 
στην ίδια κατηγορία. Η φιλοσοφία αυτή διέπει τα συστήµατα  που βασίζονται σε στιγµιότυπα 
(“Instance Based System”, [Gloss(00040)], “Instance Based Classifiers”, [Gloss (00039)]) ([Joachims, 
1997], [Joachims et al., 1997]) τα οποία κατηγοριοποιούν νεοεµφανιζόµενες περιπτώσεις 
ανατρέχοντας σε παρόµοιες ήδη εξεταζόµενες περιπτώσεις.  
Σε αυτή την οικογένεια κατηγοριοποιητών ανήκει και ο αλγόριθµος Rocchio ([Rocchio, 
1971]) ο οποίος βασίζεται στην παράσταση ενός κειµένου µε την βοήθεια ενός ανύσµατος 
χαρακτηριστικών όρων. Ο αλγόριθµος αυτός πρακτικά δηµιουργεί ένα δυαδικό κατηγοριοποιητή ο 
οποίος αθροίζει διανυσµατικά τα ανύσµατα των κειµένων που αντιστοιχούν στα θετικά παραδείγµατα 
για την εκάστοτε κατηγορία και αφαιρεί διανυσµατικά εκείνα τα ανύσµατα των κειµένων που 
 17
Κεφάλαιο 2 Κατηγοριοποίηση κειµένων µε χρήση λέξεων και  εννοιών 
αντιστοιχούν στα αρνητικά παραδείγµατα για την ίδια κατηγορία. Αυτή η διαδικασία παράγει το 
τελικό άνυσµα βαρών το οποίο στην συνέχεια χρησιµοποιείται για την κατηγοριοποίηση ενός νέου 
κειµένου το οποίο δίνεται µε την µορφή ανύσµατος. Το τελευταίο επιτυγχάνεται µε τον υπολογισµό 
του εσωτερικού γινοµένου των βαρών µε αυτό του υπό εξέταση κειµένου εφόσον αυτό ξεπερνάει µια 
προκαθορισµένη τιµή.  
Πιο συγκεκριµένα, αν D είναι το σύνολο των κειµένων προς εκπαίδευση οπότε D  είναι το 
πλήθος των στοιχείων που περιέχονται στο εν λόγω σύνολο, τότε για την κατηγορία  ο αλγόριθµος 
Rocchio υπολογίζει το αρχέτυπο άνυσµα µε τη βοήθεια της εξίσωσης:  
jC
→
jC
                    ∑∑
−∈
→
→
∈
→
→
→
→→
⋅
−
⋅−⋅⋅=
jj CDdjCdj
j
d
d
CD
b
d
d
C
aC
||||
1
||||
1
 
όπου τα  και  είναι παράµετροι οι οποίες ρυθµίζουν τον βαθµό επηρεασµού των θετικών και των 
αρνητικών παραδειγµάτων,  είναι το σύνολο των κειµένων προς εκπαίδευση που ανήκουν στην 
κατηγορία 
a b
jC
j ,  είναι το σύνολο των κειµένων που δεν ανήκουν στην κατηγορία || jC−D j  και  
είναι το µέτρο του ανύσµατος . Για την κατηγοριοποίηση ενός καινούργιου κειµένου το οποίο 
παρίσταται µε την βοήθεια του ανύσµατος υπολογίζεται το συνηµίτονο αυτού µε όλα τα αρχέτυπα 
ανύσµατα όλων των κατηγοριών. Το κείµενο κατατάσσεται σε εκείνη την κατηγορία η οποία 
επιτυγχάνει τη µέγιστη τιµή συνηµιτόνου. 
||||
→
d
→
d 'd
→
'd
→
C j
'd
2.1.2 Λέξεις και Έννοιες  
 
Όπως αναφέρθηκε στο πρώτο κεφάλαιο, στόχος της παρούσας διατριβής είναι η εύρεση 
µεθόδων µε την βοήθεια των οποίων θα πραγµατοποιείται µια ανάλυση σε βάθος στο περιεχόµενο των 
κειµένων και ιδιαίτερα στον τρόπο µε τον οποίο αυτό παρουσιάζεται και δοµείται. ∆εδοµένου ότι ένα 
κείµενο αναφέρεται σε κάποιο θέµα, οι λέξεις που χρησιµοποιήθηκαν από τον συγγραφέα για την 
απόδοση του θέµατος, επιλέχθηκαν – συνειδητά ή όχι - µε τέτοιο τρόπο ούτως ώστε να το 
προσεγγίζουν και να το αποδίδουν πιστά. Έτσι, θα µπορούσε κανείς να βασιστεί στην υπόθεση ότι 
µέσα σε ένα κείµενο, οι λέξεις που το απαρτίζουν θα αναφέρονται στην ίδια ή σε συγγενικές ως προς 
το θέµα «έννοιες». 
 18
Κεφάλαιο 2 Κατηγοριοποίηση κειµένων µε χρήση λέξεων και  εννοιών 
 Με τον όρο «έννοια» µιας λέξης ορίζουµε το σύνολο όλων των διαφορετικών σηµασιών και 
νοηµάτων της συγκεκριµένης λέξης. Η πιο συνηθισµένη µεθοδολογία για την απόδοση µιας έννοιας 
είναι κάνοντας χρήση ενός λεξικού ή θησαυρού όρων (“Thesaurus”, [Gloss (00093)]) όπου σε κάθε 
λέξη αποδίδεται ένα πεπερασµένο πλήθος σηµασιών.  
Για να µπορέσουµε να εξάγουµε µια σωστή παράσταση του νοήµατος µιας πρότασης ή ενός 
κειµένου, θα πρέπει να προσδιορίσουµε σε ποια έννοια αναφέρεται η εκάστοτε λέξη του κειµένου 
([Agirre & Martinez, 2000], [Escudero et al., 2000]), στην συνέχεια να συνδυάσουµε τη σηµασία των 
µεµονωµένων λέξεων σε επίπεδο πρότασης και τέλος σε επίπεδο κειµένου. Μιας τέτοιας φύσης 
διαδικασία είναι εξαιρετικά επίπονη δεδοµένου ότι µια λέξη είναι δυνατό να έχει περισσότερες από 
µια σηµασίες. Π.χ. η αγγλική λέξη “base” µπορεί να σηµαίνει είτε στρατιωτική βάση είτε να 
αναφέρεται στο άθληµα του baseball. Στην ελληνική γλώσσα, η λέξη «εργολάβος» µπορεί να έχει την 
έννοια του προσώπου που αναλαµβάνει την εκτέλεση ενός έργου έναντι αµοιβής αλλά σε άλλες 
περιπτώσεις µπορεί να έχει την έννοια ενός είδους γλυκίσµατος ενώ η λέξη «οίστρος» έχει την έννοια 
είτε του υπερβάλλοντα ζήλου είτε του εντόµου της βοιδοµύγας. 
Παρόλα αυτά, το αποτέλεσµα µιας τέτοιας διαδικασίας προσδιορισµού ([Agirre & Martinez, 
2000], [Escudero et al., 2000]) η οποία αναδεικνύει την χρησιµοποιηµένη έννοια µιας λέξης σύµφωνα 
µε το περιεχόµενο (θέµα) του κειµένου είναι ιδιαίτερα ενδιαφέρον και σηµαντικό γιατί αναδεικνύει το 
«κρυµµένο» νόηµα των λέξεων. 
2.1.3 Το πρόβληµα που εξετάζουµε 
 
Το πρόβληµα που εξετάζουµε δεν στοχεύει στην αξιολόγηση των µεθόδων προσδιορισµού της 
έννοιας κάθε λέξης, αλλά στην µελέτη του κατά πόσο οι έννοιες που προκύπτουν από µια τέτοια 
µέθοδο µπορούν να χρησιµοποιηθούν αντί των αρχικών λέξεων του κειµένου µε σκοπό τη βελτίωση 
του αποτελέσµατος της κατηγοριοποίησης και κατ’ επέκταση του προσδιοριµού της εννοιολογικής 
δοµής που εµφανίζεται στα κείµενα. Με άλλα λόγια, αναζητούµε να δούµε κατά πόσο η απόδοση της 
έννοιας σε κάθε λέξη και η εξέταση αυτής πρώτα σε επίπεδο πρότασης και έπειτα σε επίπεδο κειµένου 
επιτυγχάνει εµβάθυνση στο περιεχόµενο του συνολικού κειµένου µε το να παρέχει περισσότερη 
πληροφορία σχετικά µε το θέµα (άρα και την κατηγορία στην οποία ανήκει) από ότι οι αυτούσιες 
λέξεις του κειµένου.  
Για να µπορέσουµε να εξετάσουµε το παραπάνω ενδεχόµενο συγκρίνουµε την απόδοση 
κατηγοριοποίησης κειµένων χρησιµοποιώντας τις παραστάσεις οµάδων λέξεων (“bag-of-words”, 
[Gloss(00007)]) και οµάδων εννοιών (“bag-of-meanings”, [Gloss(00006)]). Αυτού του είδους η 
 19
Κεφάλαιο 2 Κατηγοριοποίηση κειµένων µε χρήση λέξεων και  εννοιών 
παράσταση αγνοεί σηµαντικά στοιχεία της δοµής του κειµένου, όπως τη σειρά µε την οποία 
εµφανίζονται οι λέξεις µέσα στο κείµενο και το συντακτικό αυτού ([Bengio et al., 2000]).* Σύµφωνα 
µε την παράσταση οµάδων λέξεων ([Manning & Schuetze, 1999], [McCallum & Nigam, 1998(a)]) 
(εναλλακτικά οµάδων εννοιών), ένα κείµενο παρίσταται από το σύνολο λέξεων (εννοιών) που 
εµφανίζονται σε αυτό. Αυτό το σύνολο λέξεων (εννοιών) αποτελεί ένα υποσύνολο του «λεξιλογίου» 
δηλαδή όλων των διαφορετικών λέξεων (εννοιών) οι οποίες εµφανίζονται στα κείµενα που 
εξετάζουµε. Για το πρόβληµα το οποίο εξετάζουµε, χρησιµοποιούµε τον θησαυρό όρων Wordnet 
([Miller et al., 1990]) ο οποίος παρέχει για κάθε λέξη µια λίστα µε όλες τις πιθανές σηµασίες αυτής. Ο 
θησαυρός όρων Wordnet συνοδεύεται από ένα σώµα κειµένων, το Brown Corpus ([Francis & Kucera, 
1982]) το οποίο περιέχει σχολιασµούς. Το Brown Corpus αποτελείται από εκατοντάδες κείµενα, όπου 
κάθε κείµενο ανήκει σε µια από τις 15 υπάρχουσες κατηγορίες, ενώ για κάθε λέξη του κειµένου 
υπάρχει η πληροφορία της έννοιας την οποία αυτή έχει, λαµβανοµένου υπόψη του συνολικού 
περιεχοµένου. Με αυτό τον τρόπο αποφορτιζόµαστε από την δυσκολία εύρεσης των πιθανών εννοιών 
κάθε λέξης µιας και αυτές παρέχονται από τον θησαυρό όρων Wordnet και της εφαρµογής ενός 
αλγορίθµου αποσαφήνισης της έννοιας κάθε λέξης, µιας και το Brown Corpus παρέχει έτοιµο το 
αποτέλεσµα µιας τέτοιας διαδικασίας .  
2.2  Περιγραφή του θησαυρού όρων Wordnet  
2.2.1 Ο θησαυρός όρων Wordnet 
 
To Wordnet αποτελεί µια on-line βάση δεδοµένων λέξεων η οποία δηµιουργήθηκε στο 
Cognitive Science Laboratory του πανεπιστηµίου του Princeton υπό την επίβλεψη του G.A. Miller 
([Miller et al., 1990]). To Wordnet µοιάζει µε έναν θησαυρό όρων και έχει οργανωθεί ώστε να 
διακρίνει την διαφορά µεταξύ λέξεων και εννοιών (δηλαδή δίνει έµφαση στις διαφορετικές έννοιες 
                                                 
* Στην βιβλιογραφία συναντά κανείς «πλουσιότερες» παραστάσεις όπως για παράδειγµα στο [Mladenic, 1998] 
όπου κατασκευάζονται ιεραρχίες από δέντρα. Επίσης άλλοι ερευνητές χρησιµοποιούν για τη βελτίωση απόδοσης 
της κατηγοριοποίησης «εξωτερική» λεκτική πληροφορία ([Benkhalifa et al., 2001], [Petridis et al., 2001]). Για 
τους πιο συνηθισµένους τρόπους παράστασης ενός κειµένου, δηλαδή επιλογής των πιο αντιπροσωπευτικών 
όρων βλέπε ([Berger et al., 1996], [Caruana & Freitag, 1994], [Della Pietra et al., 1997], [Duda et al., 2001], 
[Huynh et al., 1998], [Joachims, 1998], [Koller & Sahami, 1997], [Manning & Schuetze 1999], [Wiener et al., 
1995]).  
 20
Κεφάλαιο 2 Κατηγοριοποίηση κειµένων µε χρήση λέξεων και  εννοιών 
που λαµβάνει κάθε λέξη). Το Wordnet περιέχει ένα µεγάλο αριθµό από ουσιαστικά, ρήµατα, 
επιρρήµατα και επίθετα της αγγλικής γλώσσας, φτάνοντας συνολικά τις 130.000 λέξεις. Οι λέξεις 
είναι οργανωµένες σε οµάδες συνωνύµων (“synsets”, [Gloss(00088)]). Κάθε οµάδα συνωνύµων 
παριστά µια θεµελιώδη έννοια η οποία καθορίζεται από όλες τις συνώνυµες µεταξύ τους λέξεις. Μια 
λέξη είναι δυνατό να ανήκει σε περισσότερες από µια οµάδες συνωνύµων. Κάθε οµάδα συνωνύµων 
σχετίζεται µε µια έννοια δηλαδή µε την σηµασία µιας λέξης. Το Wordnet περιέχει συνολικά περίπου 
100.000 διαφορετικές έννοιες. Ένα πολύ σηµαντικό χαρακτηριστικό του Wordnet αποτελεί το γεγονός 
ότι οι οµάδες συνωνύµων συνδέονται µεταξύ τους µέσω γλωσσολογικών σχέσεων  (“lexical 
relations”, [Gloss(00051)]). Παρόλα αυτά, για τους σκοπούς της σύγκρισης την οποία εδώ 
πραγµατοποιούµε, µας ενδιαφέρει µόνο η διάκριση µεταξύ λέξεων και εννοιών (των λέξεων) καθώς 
και το γεγονός ότι το Wordnet περιέχει ένα προσεκτικά επιλεγµένο λεξιλόγιο λέξεων και εννοιών της 
αγγλικής γλώσσας µαζί µε την πληροφορία για το ποιες έννοιες έχει η κάθε λέξη.  
2.2.2 Το Αλφαβητικό Ευρετήριο Σηµασιών του Brown Corpus  
 
Το σώµα κειµένων Brown Corpus ([Francis & Kucera, 1982]) αποτελείται από 500 κείµενα 
καθένα από τα οποία ανήκει σε µια από τις 15 κατηγορίες οι οποίες είναι οι παρακάτω: 
1. Press: Reportage 
2. Press: Editorial 
3. Press: Reviews 
4. Religion 
5. Skills and Hobbies 
6. Popular Lore 
7. Belles Lettres / Biography / Memoirs 
8. Miscellaneous  
9. Learned  
10. General Fiction 
11. Mystery and Detective Fiction 
 21
Κεφάλαιο 2 Κατηγοριοποίηση κειµένων µε χρήση λέξεων και  εννοιών 
12. Science Fiction 
13. Adventure and Western Fiction 
14. Romance and Love Story 
15. Humor. 
To Αλφαβητικό Ευρετήριο Σηµασιών του Brown Corpus διατίθεται µαζί µε το Wordnet. Ένα 
Αλφαβητικό Ευρετήριο Σηµασιών (“Semantic Concordance”, [Gloss(00078)]) είναι ο συνδυασµός 
µιας συλλογής κειµένων και ενός θησαυρού όρων κατά τέτοιο τρόπο ούτως ώστε κάθε σηµαντική 
λέξη κάθε κειµένου να συνδέεται µε την κατάλληλη έννοια όπως αυτή δίνεται από τον θησαυρό όρων. 
Κατά αυτόν τον τρόπο το Αλφαβητικό Ευρετήριο Σηµασιών µπορεί να θεωρηθεί είτε ως συλλογή 
κειµένων στην οποία οι λέξεις φέρουν µια συντακτική και σηµασιολογική πληροφορία είτε ως ένας 
θησαυρός όρων στον οποίο για τους διάφορους ορισµούς υπάρχουν πολλά παραδείγµατα προτάσεων. 
Το Αλφαβητικό Ευρετήριο Σηµασιών χρησιµοποιεί τα 352 από τα 500 κείµενα του Brown Corpus. 
Κάθε ουσιαστικό, ρήµα, επίθετο ή επίρρηµα το οποίο εµφανίζεται σε αυτά τα 352 κείµενα 
σχολιάστηκε µε την τοποθέτηση µιας ετικέτας η οποία δηλώνει τον αναγνωριστικό αριθµό της λέξης 
αυτής στο Wordnet και την έννοια µε την οποία η λέξη αυτή χρησιµοποιείται την δεδοµένη εκείνη 
στιγµή (στην πραγµατικότητα, από τα 352 συνολικά κείµενα µόνο στα 186 υπάρχουν ετικέτες που 
προσδιορίζουν το µέρος του λόγου και την αντίστοιχη έννοια για την κάθε λέξη. Στα υπόλοιπα 166 
κείµενα υπάρχει η πληροφορία για το µέρος του λόγου και την αντίστοιχη έννοια µόνο για τα 
ρήµατα). Αυτή η τοποθέτηση των ετικετών που προσδιορίζουν την εκάστοτε έννοια έγινε 
«χειρωνακτικά» κάνοντας χρήση διαφόρων εργαλείων.  
 Έχοντας τη συλλογή κειµένων του Brown Corpus στην διάθεσή µας, µπορούµε να 
παραστήσουµε κάθε κείµενο της συλλογής είτε µε την παράσταση οµάδων λέξεων είτε µε αυτή των 
οµάδων εννοιών και να εφαρµόσουµε σε αυτά διάφορες µεθόδους κατηγοριοποίησης. Οι µέθοδοι 
κατηγοριοποίησης οι οποίες χρησιµοποιήθηκαν για τον σκοπό αυτό περιγράφονται παρακάτω. 
2.3 Σχετικές Εργασίες Κατηγοριοποίησης µε χρήση του Wordnet  
 
Από τις εµφανιζόµενες στη βιβλιογραφία εργασίες που κάνουν χρήση του θησαυρού όρων 
Wordnet εξετάζουµε τις ακόλουθες δυο οι οποίες είναι πιο αντιπροσωπευτικές και προσεγγίζουν σε 
µεγαλύτερο βαθµό το πρόβληµα το οποίο εξετάζουµε.  
 
 22
Κεφάλαιο 2 Κατηγοριοποίηση κειµένων µε χρήση λέξεων και  εννοιών 
Πιο συγκεκριµένα, η πρώτη από αυτές [Rodriguez et al., 1997] χρησιµοποίησε το Wordnet σε 
συνδυασµό µε αλγορίθµους εκµάθησης γραµµικού διαχωρισµού (“Linear discriminant learning 
algorithms”, [Gloss(00052)]) στο στάδιο της εκµάθησης, µε σκοπό την βελτίωση της απόδοσης της 
κατηγοριοποίησης πάνω στο σώµα κειµένων Reuters–22173. Στην εν λόγω εργασία, οι ερευνητές 
εκµεταλλεύτηκαν το γεγονός ότι οι θεµατικές επικεφαλίδες (“topic headings”, [Gloss(00094)]) των 
ιστοριών συχνά αντιστοιχούσαν σε λέξεις οι οποίες εµφανίζονταν στο κείµενο της αντίστοιχης 
ιστορίας. Βασισµένοι στο φαινόµενο αυτό, κατασκεύασαν µια λίστα η οποία περιείχε τις θεµατικές 
επικεφαλίδες όλων των ιστοριών και εξέτασαν το Wordnet για να βρουν τις συνώνυµες λέξεις αυτών 
που περιέχονταν στην λίστα. Η λίστα των συνωνύµων η οποία σχηµατίστηκε, έπειτα από την 
εφαρµογή µιας τεχνικής αποσαφήνισης - προσδιορισµού της έννοιας µιας λέξης (“word–sense 
disambiguation”, [Gloss(00105)]) µε σκοπό την επιλογή των κατάλληλων συνωνύµων, 
χρησιµοποιήθηκε για να πολώσει τους αλγορίθµους εκµάθησης Rocchio και Widrow-Hoff προς 
κάποια από τις θεµατικές κατηγορίες (“topics”, [Gloss(00095)]), µε το να αυξάνει τα αρχικά βάρη των 
χαρακτηριστικών όρων οι οποίοι αντιστοιχούσαν σε κάποιο από τα συνώνυµα της εν λόγω θεµατικής 
κατηγορίας. Η συνολική διαδικασία πραγµατοποιήθηκε χειρωνακτικά και εκµεταλλεύτηκε την εκ των 
προτέρων γνώση που είχαν οι ερευνητές σε ότι αφορά τη συλλογή κειµένων Reuters. Αξίζει να 
σηµειωθεί ότι δεν πραγµατοποιήθηκε καµία αλλαγή στην παράσταση των κειµένων και η χρήση του 
Wordnet περιορίστηκε στην χρήση της σχέσης των συνωνύµων. Τα αποτελέσµατα της εν λόγω χρήσης 
του θησαυρού όρων Wordnet οδήγησαν σε βελτίωση των αποτελεσµάτων κατηγοριοποίησης στο 
σώµα κειµένων του Reuters–22173.  
Η δεύτερη σχετική εργασία [Scott & Matwin, 1999], αφορά µια συγκριτική εργασία η οποία 
χρησιµοποιεί εναλλακτικούς τρόπους παράστασης κειµένων οι οποίοι βασίζονται σε συντακτικές και 
σηµασιολογικές σχέσεις µεταξύ των λέξεων («φράσεις, συνώνυµα και υπερώνυµα*»). Πιο 
συγκεκριµένα, στην εν λόγω εργασία εξετάζεται η αποτελεσµατικότητα οκτώ διαφορετικών τρόπων 
παράστασης (οµάδες λέξεων (“bag – of words”, [Gloss(00007)]), Λέξεις που προκύπτουν µετά από την 
αφαίρεση της κατάληξης (“Stemmed words”, [Gloss(00084)]), Ονοµατικές φράσεις (“Noun phrases”, 
[Gloss(00061)]), Ονοµατικές φράσεις που προκύπτουν µετά από την αφαίρεση της κατάληξης των 
λέξεων (“Stemmed Noun phrases”, [Gloss(00083)]), Φράσεις-Κλειδιά (“Key phrases”, 
[Gloss(00046)]), Φράσεις-Κλειδιά που προκύπτουν µετά από την αφαίρεση της κατάληξης των λέξεων 
(“Stemmed Key phrases”, [Gloss(00082)]) ενός κειµένου. Η τρίτη από αυτές τις µεθόδους παράστασης 
επιλέγει εκείνες τις ακολουθίες λέξεων οι οποίες αναγνωρίζονται ως ονοµατικές. Η πέµπτη παράσταση 
βασίζεται στην εξαγωγή φράσεων – κλειδιών (“key phrase extraction”, [Gloss(00045)]).  Για κάθε 
έναν από τους τρόπους παράστασης, οµάδες λέξεων, Ονοµατικές φράσεις και Φράσεις-Κλειδιά 
                                                 
* Υπερώνυµο = λέξη της οποίας η σηµασία είναι ευρύτερη της σηµασίας άλλων λέξεων. π.χ η λέξη «άνθος» 
είναι υπερώνυµο των λέξεων «τριαντάφυλλο» και «γαρίφαλο». 
 23
Κεφάλαιο 2 Κατηγοριοποίηση κειµένων µε χρήση λέξεων και  εννοιών 
εξετάστηκε τόσο η περίπτωση κατά την οποία χρησιµοποιείται η τεχνική της αφαίρεσης της 
κατάληξης των λέξεων δηλαδή του stemming ([Porter, 1980]) όσο και η περίπτωση όπου η τεχνική 
αυτή δεν χρησιµοποιείται. Επίσης εξετάστηκε και η παράσταση µε χρήση υπερώνυµων (“hypernym”, 
[Gloss(00034)]) σε δυο διαφορετικές παραλλαγές. Στην εν λόγω εργασία δεν πραγµατοποιήθηκε 
αποσαφήνιση δηλαδή προσδιορισµός της έννοιας της εκάστοτε λέξης (“word–sense disambiguation”, 
[Gloss(00105)]). Αντίθετα όλες οι έννοιες τις οποίες επιστρέφει το Wordnet θεωρήθηκαν ότι ήταν 
κατά ισοδύναµο τρόπο σχετικές, και όλα αυτά συµπεριλήφθηκαν στο σύνολο των χαρακτηριστικών 
όρων. 
∆εδοµένου ότι ο σκοπός των πειραµάτων ήταν η εύρεση µιας καλύτερης παράστασης και 
λαµβανοντας υπόψη τα αποτελέσµατα τα οποία παραθέτονται στην βιβλιογραφία για την συλλογή 
κειµένων Reuters–21578, όπως αυτά τα οποία επιτυγχάνονται από την τεχνική των Support Vector 
Machines ([Drucker et al., 1999], [Joachims, 1998], [Vapnik, 1995]), και τους αλγορίθµους C4.5 
([Quinlan, 1993]) και Κ-Πλησιέστερων Γειτόνων (“Κ-Nearest Neighbors”, [Gloss(00044)]) ([Manning 
& Schuetze, 1999], [Masand et al., 1992], [Mitchell, 1997], [Yang & Liu, 1999], [Yang, 1995], [Yang, 
1998]), οι εν λόγω ερευνητές αξιολόγησαν τις παραπάνω παραστάσεις κάνοντας χρήση του 
αλγορίθµου εκµάθησης RIPPER ([Cohen, 1995(a)], [Cohen, 1995(b)], [Cohen, 1995(c)], [Cohen & 
Singer, 1996], [Cohen, 1996]). Από τα ληφθέντα αποτελέσµατα κατέληξαν στο συµπέρασµα ότι καµία 
από τις παραπάνω παραστάσεις δεν επιτύγχανε σηµαντικά καλύτερα αποτελέσµατα σε σύγκριση µε 
την παράσταση των οµάδων λέξεων. Επιπρόσθετα, η απόδοση των παραστάσεων βασιζόµενες σε 
φράσεις επιβεβαιώνουν τα συµπεράσµατα παλαιοτέρων ερευνών ([Lewis, 1992]) ότι οι προτάσεις δεν 
ενέχουν µεγάλη «δύναµη» σε ότι αφορά την κατηγοριοποίηση.  
Τα αποτελέσµατα τα οποία προέκυψαν από την χρήση των παραστάσεων χρησιµοποιώντας τα 
υπερώνυµα ήταν τα πιο απογοητευτικά. Η µοναδική βελτίωση σε ότι αφορά την συλλογή Reuters 
21578 προέκυψε από τις Ονοµατικές Φράσεις συναρτήσει των οµάδων λέξεων. Η προφανής λύση που 
προέκυψε ήταν ο συνδυασµός αυτών των δυο παραστάσεων (ο οποίος φέρει τον συµβολισµό NPw). 
Όπως είναι προφανές, κατά τον σχηµατισµό της παράστασης των Ονοµατικών Φράσεων, και πιο 
συγκεκριµένα στο στάδιο της επιλογής των χαρακτηριστικών όρων, ορισµένες πολύτιµες λέξεις οι 
οποίες δεν ήταν ουσιαστικά αφαιρέθηκαν, ενώ κατά τον σχηµατισµό της παράστασης NPw 
συµπεριλήφθηκαν και αυτές.  
Στην συνέχεια εξετάστηκε η τεχνική «ψηφοφορίας» σε κατηγοριοποιητές για την βελτίωση 
του αποτελέσµατος της κατηγοριοποίησης. Πειράµατα πραγµατοποιήθηκαν χρησιµοποιώντας µια, 
τρεις και πέντε διαφορετικές παραστάσεις και εφαρµόζοντας σε αυτές την τεχνική της ψηφοφορίας. 
Πιο συγκεκριµένα πραγµατοποιήθηκαν πειράµατα µε το συνδυασµό τριών και πέντε 
κατηγοριοποιητών σε καθένα από τους οποίους αποδόθηκε η ετικέτα µιας κατηγορίας. Τα καλύτερα 
αποτελέσµατα ελήφθησαν όταν χρησιµοποιήθηκαν οι τρεις και πέντε κατηγοριοποιητές σε αντίθεση 
 24
Κεφάλαιο 2 Κατηγοριοποίηση κειµένων µε χρήση λέξεων και  εννοιών 
µε τη χρήση µόνο του ενός. Είναι αξιοσηµείωτο ότι η απόδοση η οποία βασιζόταν στην πλειοψηφία 
του αποτελέσµατος κατηγοριοποίησης καθενός εκ των κατηγοριοποιητών βελτιώθηκε σε όλες τις 
περιπτώσεις και ότι η απόδοση στο σώµα κειµένων του Reuters είναι συγκρίσιµη αλλά χειρότερη από 
αυτή των Support Vector Machines. 
2.4 Παράσταση του κειµένου - Η προσέγγισή µας 
 
Στα πειράµατα τα οποία πραγµατοποιήθηκαν, χρησιµοποιήθηκαν για τα κείµενα τέσσερις 
διαφορετικές παραστάσεις. Οι δυο από αυτές βασίζονται στις λέξεις ενώ οι άλλες δυο στις έννοιες. 
Παρουσιάζουµε µε µεγαλύτερη λεπτοµέρεια τις δυο παραστάσεις οι οποίες βασίζονται στις λέξεις (οι 
αντίστοιχες οι οποίες βασίζονται στις έννοιες είναι εντελώς ανάλογες). Ορίζουµε τα ακόλουθα 
στοιχεία:  
1. Μια συλλογή κειµένων προς εκπαίδευση Mxxx ...,, 21 , 
2. Μια συλλογή από Κ κατηγορίες Kccc ...,, 21 , 
3. Η πληροφορία της κατηγορίας στην οποία ανήκει το κείµενο  (για m=1,2, …., M, ∈ mx mx
}...,,{ 21 Kccc ). 
Καθένα από τα κείµενα είναι ένα άνυσµα λέξεων, δηλαδή για m =1,2, …., M, έχουµε  
[ ]mmJmjmmm xxxxx ,....,,...,, 21=  
Όπου  m=1,2, …., M, και  j=1, 2, …, .  mJ
 Αξίζει να σηµειωθεί ότι  είναι ο συνολικός αριθµός των λέξεων οι οποίες εµφανίζονται στο 
m-στό κείµενο, όπου  είναι η j-στή λέξη η οποία εµφανίζεται στο m-στό κείµενο. Επιπρόσθετα , 
το  λαµβάνει τιµές από το λεξιλόγιο το οποίο ορίζεται από το άνυσµα: 
mJ
mjx
mjx
     [ ]wNn wwww ,....,,...,, 21W =  
Όπου  είναι ο συνολικός αριθµός των λέξεων που εµφανίζονται σε όλα τα κείµενα προς 
εκπαίδευση.  
wN
 25
Κεφάλαιο 2 Κατηγοριοποίηση κειµένων µε χρήση λέξεων και  εννοιών 
 Αξίζει να σηµειωθεί ότι, παρά το γεγονός ότι τα περιεχόµενα του m-στού κειµένου 
τοποθετούνται στο άνυσµα , µε µεταβλητό µήκος , η παράσταση του κειµένου είναι ένα 
άνυσµα σταθερού µήκους . Χρησιµοποιούµε δυο τέτοιου είδους παραστάσεις: 
mx
w
mJ
N
1. Η πρώτη παράσταση είναι το ∆υαδικό Άνυσµα Κειµένου Λέξεων (“Word Boolean document 
vector”, [Gloss(00103)]). Το ∆υαδικό άνυσµα κειµένου Λέξεων (∆Λ) για το m-στό κείµενο έχει 
την µορφή: 
     [ ]wmNmnmmm bbbbb ,....,,...,, 21=  
όπου (για n=1,2,…, ) έχουµε wN 1=mnb  αν η  εµφανίζεται στο , αλλιώς 0. nw mx
2. Η δεύτερη παράσταση είναι το Άνυσµα Κειµένου Συχνοτήτων Λέξεων (“Word Frequency  
document vector”, [Gloss(00104)]): 
   [ ]wmNmnmmm fffff ,....,,...,, 21=  
Όπου (για  n=1,2,…, ) έχουµε  wN
mnf = το πλήθος των φορών που η n-λέξη εµφανίζεται στο m-στο κείµενο.   
 Το ∆υαδικό Άνυσµα Κειµένου Λέξεων και το Άνυσµα Κειµένου Συχνοτήτων Λέξεων αποτελούν 
τις βασικές παραστάσεις των κειµένων σε ότι αφορά τις λέξεις. ∆υο επιπρόσθετες παραστάσεις 
κειµένων υπολογίζονται µε εντελώς ανάλογο τρόπο χρησιµοποιώντας τις έννοιες δηλαδή το ∆υαδικό 
Άνυσµα Κειµένου Εννοιών (∆Ε) και το Άνυσµα Κειµένου Συχνοτήτων Εννοιών (ΣΕ). Οι τελευταίες 
παραστάσεις κάνουν χρήση του λεξιλογίου των εννοιών, το οποίο έχει τη µορφή  
       [ ]sNn ssssS ,....,,...,, 21=  
Όπου  είναι ο συνολικός αριθµός των εννοιών που εµφανίζονται σε όλα τα κείµενα προς 
εκπαίδευση. Όλες οι υπόλοιπες λεπτοµέρειες είναι εντελώς ανάλογες µε αυτές των αντίστοιχων 
παραστάσεων λέξεων.  
sN
 26
Κεφάλαιο 2 Κατηγοριοποίηση κειµένων µε χρήση λέξεων και  εννοιών 
2.5 Αλγόριθµοι κατηγοριοποίησης 
2.5.1  Κατηγοριοποίηση Μέγιστης Ύστερης Πιθανότητας 
 
Ο αλγόριθµος Κατηγοριοποίησης Μέγιστης Ύστερης Πιθανότητας (“Maximun a Posteriori 
(MAP) Classification”, [Gloss(00058)]) ή αλλιώς Naïve Bayes ([Baker & McCallum ,1998], [Craven 
et al., 1998], [McCallum & Nigam, 1998 (a)], [McCallum & Nigam, 1998 (b)], [Nigam et al., 1998], 
[Nigam et al., 1999]) βασίζεται στην µεγιστοποίηση πάνω στα , , …, 1c 2c Kc  της ύστερης 
(“posterior”, [ Gloss(00063)]) πιθανότητας  
                    (2.1) ),...,,|()|(
21 mmJmmkrmkr
xxxcPxcP =
γεγονός που σηµαίνει ότι το κείµενο κατηγοριοποιείται στην -στή κατηγορία όπου   
^
k
  .          (2.2) ),...,,|(maxarg
21,...,1
^
mmJmmkrKk
xxxcPk
=
=
Το όνοµα “Naïve Bayes” προκύπτει από την ακόλουθη παραδοχή: υποτίθεται ότι η 
πιθανότητα της j-στης λέξης (η οποία περιέχεται µέσα στο m-στο κείµενο) εξαρτάται αποκλειστικά και 
µόνο από την κατηγορία και όχι από τις υπόλοιπες λέξεις που εµφανίζονται µέσα στο κείµενο. Αυτό 
επιτρέπει τον εύκολο υπολογισµό της παράστασης (2.1) σε ότι αφορά τις υπό συνθήκη πιθανότητες 
των λέξεων δοθείσης της κατηγορίας. Στην συνέχεια θα δούµε πως µπορούµε να λάβουµε 
εκτιµήσεις των  καθώς και δυο εναλλακτικούς τρόπους υπολογισµού της παράστασης 
(2.1). 
)|( knr cwP
)|( knr cwP
 Χρησιµοποιώντας το θεώρηµα του Bayes έχουµε: 
 
∑
=
==
wN
n
knr
knr
kr
knr
knr
cwP
cwP
cP
cwP
cwP
1
),(
),(
)(
),(
)|(           (2.3) 
Ο τρόπος υπολογισµού µιας προσέγγισης της παράστασης (2.3) σε αναλογία µε τον ορισµό 
του αλογρίθµου Naïve Bayes (για n = 1,2,…,  και k = 1,2…,K) δίνεται παρακάτω: wN
 27
Κεφάλαιο 2 Κατηγοριοποίηση κειµένων µε χρήση λέξεων και  εννοιών 
    
∑ ∑
∑
= =
=
+
+
=
wN
n
M
m
mkmn
M
m
mkmn
knr
fa
fa
cwP
1 1
1
^
)1(
)1(
)|(            (2.4) 
όπου = 1 αν το κείµενο ανήκει στην κατηγορία  αλλιώς 0 και mk1 kc α είναι µια ρυθµιστική 
παράµετρος του αλγορίθµου (την οποία εισάγουµε) τέτοια ώστε µεγάλες τιµές αυτής να οδηγούν σε 
µια πιο κανονικοποιηµένη κατανοµή πιθανοτήτων. Αξίζει να σηµειωθεί ότι µια εκτίµηση της 
ποσότητας δίνεται από την ακόλουθη σχέση: )( kr cP
  
M
cP
M
m
mk
kr
∑
== 1
^
1
)(             (2.5) 
Στην συνέχεια παρουσιάζουµε δύο παραλλαγές του αλγορίθµου Κατηγοριοποίησης Μέγιστης 
Ύστερης Πιθανότητας για τον υπολογισµό της ποσότητας σε σχέση µε 
την ποσότητα . Παρουσιάζουµε τις δύο εναλλακτικές παραλλαγές του αλγορίθµου οι 
οποίες βασίζονται στις λέξεις. Εντελώς ανάλογα ορίζονται και οι αντίστοιχες που βασίζονται στις 
έννοιες.  
),...,,|( 21 mmJmmkr xxxcP
)|( knr cwP
2.5.1.1 Αλγόριθµος Μέγιστης Ύστερης  Πιθανότητας (Μαζική Παραλλαγή)  
 
∆οθέντος ενός κειµένου  (για το οποίο δεν υπάρχει η πληροφορία σε 
ποια κατηγορία ανήκει), ο αλγόριθµος Κατηγοριοποίησης Μέγιστης Ύστερης Πιθανότητας υπολογίζει 
(σε σχέση µε τις πιθανότητες ) τις πιθανότητες για όλα τα k=1,…,K και 
κατηγοριοποιεί το κείµενο d σε εκείνη την κατηγορία  η οποία µεγιστοποιεί την πιθανότητα 
. Ο υπολογισµός της πιθανότητας  γίνεται µε τον ακόλουθο 
τρόπο: 
),...,,(
21 mmJmm
xxxd =
)|( knr cwP
|( kr cP
)|( dcP kr
),...,
mmJ
x
kc
,
1m
x)|( dcP kr 2mx
 
),...,,(
)()|,...,,(
),...,,|()|(
21
21
21
m
m
m
mJmmr
krkmJmmr
mJmmkrkr xxxP
cPcxxxP
xxxcPdcP
∗
==    (2.6) 
 28
Κεφάλαιο 2 Κατηγοριοποίηση κειµένων µε χρήση λέξεων και  εννοιών 
               
∑
=
∗
∗
= K
i
irimJmmr
krkmJmmr
cPcxxxP
cPcxxxP
m
m
1
)()|,...,,(
)()|,...,,(
21
21  
                                          
∑
=
∗
∗
= K
i
irimrimrimr
krkmrkmrkmr
cPcxPcxPcxP
cPcxPcxPcxP
mJ
mJ
1
2
2
)(*)|(*...*)|()|(
)(*)|(*...*)|()|(
1
1  
 Αφού για j =1,….,  έχουµε mJ Wxmj ∈  προκύπτει ότι η εξίσωση (2.6) µπορεί να υπολογιστεί 
σε σχέση µε τις προσεγγίσεις  (για k =1,2,…, Κ και για n = 1,2….,  οι οποίες 
δίνονται από την εξίσωση (2.4) και τις εκτιµήσεις (για k =1,2,…, Κ)  οι οποίες δίνονται από 
την εξίσωση (2.5). Έτσι, η Μαζική παραλλαγή του αλγόριθµου Κατηγοριοποίησης Μέγιστης Ύστερης 
Πιθανότητας έχει µια µόνο παράµετρο την 
)| kn c(
^
r wP wN
)(
^
kr cP
α . 
2.5.1.2 Αλγόριθµος Μέγιστης Ύστερης  Πιθανότητας (Aναδροµική Παραλλαγή)  
 
Σε αυτήν την ενότητα παρουσιάζουµε τον αναδροµικό τρόπο υπολογισµού της εξίσωσης (2.1). 
Ο συγκεκριµένος υπολογισµός σχετίζεται µε τον αλγόριθµο κατηγοριοποίησης για χρονοσειρές ο 
οποίος περιγράφεται στις ([Petridis & Kehagias, 1996] [Petridis & Kehagias, 1998], [Petridis & 
Kaburlazos, 1998]).  
 Πιο συγκεκριµένα,  για m = 1,2,…, M, k = 1,2,…, K και j = 1,2,…,  ορίζουµε την ποσότητα: mJ
           (2.7) ),...,,|(),(
21
,,
0 mmJmmkr
km
jkr
km xxxcPpcPp ==
Με άλλα λόγια η ποσότητα παριστά την πιθανότητα το m-στο κείµενο να ανήκει στην 
k-στη κατηγορία έχοντας εξετάσει µέχρι και την j-στη λέξη. Παρακάτω δίνουµε την αναδροµική σχέση 
η οποία υπολογίζει τα ,  , …, από τα ,  , …, .  Κατά αυτό τον 
τρόπο µπορούµε να υπολογίσουµε την κατηγορία στην οποία είναι πιο πιθανό να ανήκει το m-στο 
κείµενο, έχοντας εξετάσει µέχρι και την j-στη λέξη του κειµένου αυτού. Ο υπολογισµός αυτός ξεκινά 
µε την έκφραση του κανόνα του Bayes κατά τον ακόλουθο τρόπο: 
km
jp
,
k
jp
,1 k
jp
,2 km
jp
, k
jp
,1
1−
k
jp
,2
1−
km
jp
,
1−
 29
Κεφάλαιο 2 Κατηγοριοποίηση κειµένων µε χρήση λέξεων και  εννοιών 
∑
∑
=
−−
−−
=
−
−
−
− ===
K
i
jmmmirijmmmmjr
jmmmkrkjmmmmjr
K
i
jmmmmjir
jmmmmjkr
jmmmmjr
jmmmmjkr
mjmmkr
xxxcPcxxxxP
xxxcPcxxxxP
xxxxcP
xxxxcP
xxxxP
xxxxcP
xxxcP
1
1,1,
1,1,
1
1,
1,
1,
1,
),...,,|(*),,...,,|(
),...,,|(*),,...,,|(
),...,,|,(
),...,,|,(
),...,,|(
),...,,|,(
),...,,|(
2121
2121
21
21
21
21
21
 
Χρησιµοποιώντας τον ορισµό του  και την παραδοχή του Naïve Bayes προκύπτει ότι:  kmjp
,
 
∑
=
−
−== K
i
imjr
im
j
kmjr
km
j
mjmmkr
km
j
cxPp
cxPp
xxxcPp
1
,
1
,
1,
)|(
)|(
),...,,|(
21
         (2.8) 
Ο παραπάνω υπολογισµός µπορεί να πραγµατοποιηθεί σε σχέση µε τις εκτιµήσεις  
(οι οποίες δίνονται από την εξίσωση (2.5)) και τις  (οι οποίες δίνονται από την εξίσωση 
(2.4)). Στην πραγµατικότητα, ο υπολογισµός της παράστασης (2.8) τροποποιείται κατά τον ακόλουθο 
τρόπο: ορίζεται από τον χρήστη µια παράµετρος «κατώφλι» h  και σε κάθε επανάληψη ελέγχεται το 
κατά πόσο η ποσότητα , k = 1,2, …,K είναι µικρότερη από την παράµετρο h. Αν αυτό συµβαίνει 
τότε η ποσότητα  λαµβάνει τιµή ίση µε το h όπως εξηγείται στα (([Petridis & Kehagias, 1996] 
[Petridis & Kehagias, 1998], [Petridis & Kaburlazos, 1998]). Έτσι η αναδροµική εκδοχή του 
αλγόριθµου Κατηγοριοποίησης Μέγιστης Ύστερης Πιθανότητας έχει δύο ρυθµιστικές παραµέτρους 
τις a και h.  
)(
^
kr cP
)|(
^
knr cwP
km
jp
,
km
jp
,
2.5.2 Κατηγοριοποίηση Μέγιστης Πιθανοφάνειας   
 
Σύµφωνα µε τον αλγόριθµο Κατηγοριοποίησης Μέγιστης Πιθανοφάνειας (“Maximum 
Likelihood Classification”, [Gloss(00057)]), η κατηγοριοποίηση πραγµατοποιείται µε την 
µεγιστοποίηση της πιθανότητας:  
∏
=
==
m
m
J
j
kmjrkmJmmrkmr cxPcxxxPcxP
1
)|()|,...,,()|(
21
         (2.9) 
 30
Κεφάλαιο 2 Κατηγοριοποίηση κειµένων µε χρήση λέξεων και  εννοιών 
Η Κατηγοριοποίηση Μέγιστης Πιθανοφάνειας µπορεί να εφαρµοστεί µε εντελώς ανάλογο 
τρόπο µε την Κατηγοριοποίηση Μέγιστης Ύστερης Πιθανότητας, όπου ο παράγοντας 
στην εξίσωση (2.9) για την j-στη λέξη του m-στου κειµένου αντικαθίσταται από την 
εκτίµηση (η οποία δίνεται από την εξίσωση (2.4)) για την πιθανότητα της αντίστοιχης 
λέξης του λεξιλογίου W = [ ]. Όπως προηγούµενα, η ρυθµιστική παράµετρος 
του αλγορίθµου µας είναι η a.  
)|( kmjr cxP
(
^
rP )| kn cw
wNn wwww ,...,,...,, 21
2.5.3 Αλγόριθµος Κ-Πλησιέστερων Γειτόνων 
 
Παρουσιάζουµε τώρα έναν αλγόριθµο κατηγοριοποίησης ο οποίος είναι εµπνευσµένος από 
τον γνωστό αλγόριθµο κατηγοριοποίησης Κ-Πλησιέστερων Γειτόνων (“Κ-Nearest Neighbors”, 
[Gloss(00044)]) ([Manning & Schuetze, 1999], [Masand et al., 1992], [Mitchell, 1997], [Yang & Liu, 
1999], [Yang, 1995], [Yang, 1998]). Όπως και παραπάνω, παρουσιάζουµε εκείνη την έκδοση του 
αλγορίθµου η οποία βασίζεται στις λέξεις. Εντελώς αντίστοιχα ορίζεται και η έκδοση του αλγορίθµου 
που ασχολείται µε τις έννοιες. Παρουσιάζουµε τέσσερις παραλλαγές του αλγορίθµου, καθεµία από τις 
οποίες µπορεί να εφαρµοστεί τόσο σε δυαδικά ανύσµατα όσο και σε ανύσµατα τα οποία παριστάνουν 
την σχετική συχνότητα.  
2.5.3.1 Πρώτη Παραλλαγή Αλγορίθµου Κ-Πλησιέστερων Γειτόνων 
 
Ο αλγόριθµος Κ-Πλησιέστερων Γειτόνων περιγράφεται µε λεπτοµέρεια στο  [Duda et al., 
2001]. ∆οσµένου ενός ανύσµατος d για το οποίο δεν γνωρίζουµε την κατηγορία στην οποία ανήκει, 
µπορούµε να βρούµε , , …,  τα οποία είναι k γνωστά µέλη του συνόλου εκµάθησης - για τα 
οποία είναι γνωστή η κατηγορία στην οποία ανήκουν – και τα οποία είναι τα πιο κοντινά προς το 
άνυσµα d σύµφωνα µε κάποια συνάρτηση υπολογισµού της απόστασης D(…, …). Τότε η κατηγορία 
στην οποία ανήκει το άνυσµα d καθορίζεται λαµβάνοντας µια ψήφο ανάµεσα στα , , …,  
δηλαδή µε την εύρεση της κατηγορίας στην οποία ανήκουν τα περισσότερα από τα , , …, . Η 
ρυθµιστική εδώ παράµετρος είναι ο αριθµός Κ δηλαδή ο αριθµός των γειτόνων τους οποίους 
λαµβάνουµε υπόψη µας. Στα πειράµατα τα οποία πραγµατοποιήσαµε επιλέξαµε την απόσταση  η 
οποία δεν είναι άλλη από την: 
1a 2a Ka
1a
1 a
2a
2
Ka
1l
a Ka
 31
Κεφάλαιο 2 Κατηγοριοποίηση κειµένων µε χρήση λέξεων και  εννοιών 
        ∑
=
−=
N
n
nn yxyx
1
),(D            (2.10) 
 Η εν λόγω παραλλαγή του αλγορίθµου Κ-Πλησιέστερων Γειτόνων φέρει την ονοµασία 
Καθορισµένοι Γείτονες (“Fixed Neighbors”, [Gloss(00030)]) όπου οι κοντινότεροι γείτονες 
επιλέχθηκαν αποκλειστικά και µόνο από στοιχεία του σώµατος κειµένων προς εκπαίδευση. 
 Όσον αφορά στο είδος της παράστασης η οποία χρησιµοποιήθηκε για τον αλγόριθµο αυτό, 
εξετάστηκαν δυο δυνατές παραστάσεις. Στην πρώτη από αυτές που ήταν η ∆υαδική Παράσταση, ένα 
κείµενο θεωρείται ισοδύναµο µε ένα σύνολο λέξεων ή εννοιών οι οποίες εµφανίζονται µέσα σε αυτό 
και το οποίο είναι υποσύνολο του συνολικού λεξιλογίου (λέξεων ή εννοιών) του συνολικού σώµατος 
κειµένων. Αυτό παρίσταται ως ένα άνυσµα d=[ , , …, ] (Ν είναι ο αριθµός των λέξεων – 
εννοιών του λεξιλογίου) όπου το  ισούται µε την µονάδα όταν η n-οστή λέξη εµφανίζεται στο 
κείµενο, στην αντίθετη περίπτωση ισούται µε το µηδέν. Σύµφωνα µε την δεύτερη παράσταση, ένα 
κείµενο παρίσταται µε την µορφή ενός ανύσµατος d=[ , , …, ] όπου όµως εδώ το  
αντιστοιχεί στην σχετική συχνότητα µιας λέξης µέσα σε ένα συγκεκριµένο κείµενο η οποία ορίζεται 
ως o λόγος του πλήθους των φορών που η n-στή λέξη εµφανίζεται µέσα σε ένα κείµενο δια το 
συνολικό αριθµό των λέξεων οι οποίες εµφανίζονται µέσα σε αυτό κείµενο. Και οι δυο αυτές 
παραστάσεις µπορούν να χρησιµοποιηθούν τόσο για την περίπτωση των λέξεων όσο και για την 
περίπτωση των εννοιών. 
1d 2d Nd
2d
nd
1d Nd nd
2.5.3.2 ∆εύτερη Παραλλαγή Αλγορίθµου Κ-Πλησιέστερων Γειτόνων 
 
Στην δεύτερη παραλλαγή η οποία φέρει την ονοµασία Μεταβλητοί Γείτονες (“Variable 
Neighbors”, [Gloss(00099)]), το πλήθος  k  των κειµένων - για τα οποία είναι γνωστή η κατηγορία 
στην οποία ανήκουν – και τα οποία είναι τα πιο κοντινά ως προς ένα νέο-αφιχθέν άνυσµα d είναι 
δυνατό να µεταβάλλεται. Πιο συγκεκριµένα ο αριθµός k  είναι δυνατό να αυξάνεται ως αποτελέσµα 
της προσθήκης ενός νέο-αφιχθέντος κειµένου δηλαδή ενός κειµένου το οποίο δεν ανήκει στο σύνολο 
των κειµένων προς εκπαίδευση αλλά στο σύνολο των κειµένων προς επαλήθευση. Έτσι, κάθε νεο 
κείµενο του οποίου δεν γνωρίζουµε την κατηγορία, αποτελεί υποψήφιο κοντινότερο γείτονα για τα 
επόµενα στάδια του αλγορίθµου. Αν κάποιο από αυτά τα νεο – αφιχθέντα κείµενα για τα οποία δεν 
γνωρίζουµε την κατηγορία στην οποία ανήκουν συµπεριληφθεί µεταξύ των k κοντινότερων γειτόνων 
τότε ψηφίζει για την κατηγορία στην οποία βρέθηκε ότι ανήκει δηλαδή συµπεριλαµβάνεται στους ήδη 
γνωστούς – από το στάδιο της εκµάθησης – «γείτονες» που προσδιορίζουν την εν λόγω κατηγορία. Η 
παράσταση κειµένων είναι ίδια µε αυτή της πρώτης  παραλλαγής.  
 32
Κεφάλαιο 2 Κατηγοριοποίηση κειµένων µε χρήση λέξεων και  εννοιών 
2.5.3.3 Τρίτη Παραλλαγή Αλγορίθµου Κ-Πλησιέστερων Γειτόνων 
 
Ο αλγόριθµος αυτός µπορεί να εφαρµοστεί τόσο σε δυαδικά ανύσµατα όσο και σε ανύσµατα 
σχετικής συχνότητας. Στην περίπτωση που χρησιµοποιούνται ανύσµατα συχνοτήτων  τότε το 
πρώτο βήµα αποτελεί η κατασκευή από αυτά των ανυσµάτων σχετικής συχνότητας . Τα τελευταία 
ορίζονται κατά τον ακόλουθο τρόπο: 
mf
mnr
∑
=
=
wN
n
mn
mn
mn
f
f
r
1
                    (2.11) 
Παρουσιάζουµε τον αλγόριθµο σε σχέση µε τα δυαδικά ανύσµατα . Η περίπτωση των 
ανυσµάτων  είναι εντελώς ανάλογη. Το σώµα κειµένων προς εκπαίδευση αποτελείται από τα 
ανύσµατα κειµένων , , …, 
mb
mr
1b 2b Mb . Καθένα από αυτά τα ανύσµατα αποθηκεύεται στην µνήµη µαζί 
µε την αντίστοιχη πληροφορία που αφορά την κατηγορία στην οποία ανήκει καθένα από αυτά , 
, …, 
1q
2q Mq .  
Ας θεωρήσουµε τώρα ένα εισερχόµενο κείµενο για το οποίο δεν διαθέτουµε την πληροφορία 
σχετικά µε το σε ποια κατηγορία ανήκει, και το οποίο παριστάνεται από ένα άνυσµα b το οποίο 
βασίζεται στις λέξεις του κειµένου. Για m =1,2, …, M υπολογίζουµε τις ποσότητες: 
bb
bb
D
m
m
m *
2−
=              (2.12) 
(όπου * είναι η Ευκλείδεια απόσταση) και για k = 1,2,…, K υπολογίζουµε τις ποσότητες 
mk
M
m
P
m
k D
C 11
1
∑
=






=            (2.13) 
όπου Ρ είναι µια ρυθµιστική παράµετρος ενώ η ποσότητα  έχει οριστεί στην περιγραφή του 
αλγορίθµου Κατηγοριοποίησης Μέγιστης Ύστερης Πιθανότητας. Η απόδοση του εισερχόµενου 
κειµένου στην -στη κατηγορία γίνεται ως εξής: 
mk1
^
k
 33
Κεφάλαιο 2 Κατηγοριοποίηση κειµένων µε χρήση λέξεων και  εννοιών 
Kk
kCk
,...,2,1
^
maxarg
=
=               (2.14) 
Η λογική του αλγορίθµου είναι προφανής: Για m=1,2,…, M το m-στο κείµενο προς 
εκπαίδευση «ψηφίζει» για το κατά πόσο το µη κατηγοριοποιηµένο κείµενο θα πρέπει να 
κατηγοριοποιηθεί στην κατηγορία (δηλαδή στην κατηγορία του m-στου κειµένου προς 
εκπαίδευση). Παρόλα αυτά, οι «ψήφοι» λαµβάνουν βάρος κατά τρόπο αντιστρόφως ανάλογο ως προς 
την απόσταση των «ψηφοφόρων» από το µη κατηγοριοποιηµένο κείµενο (δείτε τη διαβάθµιση 
(“scaling”, [Gloss(00075)]) στην εξίσωση (2.10)). Αυτές οι «ψήφοι» µε το βάρος που έχουν λάβει 
αξιολογούνται και αναθέτονται στις  µεταβλητές και τελικά το εν λόγω κείµενο κατηγοριοποιείται 
σε εκείνη η οποία εµφανίζει την µέγιστη τιµή .  
mq
kC
kC
2.5.3.4 Τέταρτη Παραλλαγή Αλγορίθµου Κ-Πλησιέστερων Γειτόνων 
 
Η τέταρτη παραλλαγή είναι παρόµοια µε την τρίτη µε την µόνη διαφορά στον ορισµό της 
παράστασης   η οποία ορίζεται τώρα κατά τον ακόλουθο τρόπο: mD
bb
bb
D
m
m
m +
−
=            (2.15) 
τόσο για την περίπτωση του ∆υαδικού Ανύσµατος Κειµένου Λέξεων όσο και για την περίπτωση του 
Ανύσµατος Κειµένου Συχνοτήτων Λέξεων, του ∆υαδικού Ανύσµατος Κειµένου Εννοιών και του 
Ανύσµατος Κειµένου Συχνοτήτων Εννοιών. Oι εξισώσεις (2.11) και (2.12) παραµένουν οι ίδιες. 
2.5.4 σ -FLN µε Ψηφοφορία 
 
Ο αλγόριθµος σ -FLN MAP (όπου FLN είναι τα αρχικά του Fuzzy Lattice Neurocomputing) 
παρουσιάστηκε αρχικά στο [Petridis & Kaburlazos, 2000] ως ένας αλγόριθµος νευρωνικών δικτύων 
για κατηγοριοποίηση µε την βοήθεια της Συσσωµάτωσης µε επίβλεψη (“supervised clustering”, 
[Gloss(00086)]). Με άλλα λόγια, το πρώτο βήµα της κατηγοριοποίησης είναι η συσσωµάτωση 
(“clustering”, [Gloss(00013)]) των δεδοµένων προς εκπαίδευση σε οµοιογενείς οµάδες (“clusters”, 
[Gloss(00012)]), δηλαδή στόχος είναι κάθε στοιχείο – δεδοµένο σε µια οµάδα να ανήκει στην ίδια 
 34
Κεφάλαιο 2 Κατηγοριοποίηση κειµένων µε χρήση λέξεων και  εννοιών 
κατηγορία. Στο δεύτερο βήµα, τα κείµενα προς επαλήθευση κατηγοριοποιούνται στην κατηγορία της 
οµάδας στην οποία περιέχονται κατά το µέγιστο βαθµό (το οποίο θα εξηγηθεί στην συνέχεια).  
  O αλγόριθµος σ-FLNΜAP ([Kaburlazos &  Petridis, 2000], [Petridis et al., 2001] [Petridis & 
Kaburlazos, 2001]) αποτελεί επέκταση του αλγορίθµου fuzzy-ARTMAP νευρωνικού αλγορίθµου για 
εκµάθηση [Carpenter et al., 1992]. To σ-FLNMAP βασίζεται στον συνεργατικό (“synergetic”, 
[Gloss(00087)]) συνδυασµό δύο σ-FLN µονάδων νευρωνικών δικτύων για συσσωµάτωση 
([Kaburlazos &  Petridis, 2000]). Όπως εξηγείται στο [Petridis & Kaburlazos, 1999], ένα µοντέλο FLN 
εφαρµόζεται σε οποιαδήποτε θεµατική περιοχή που µπορεί να εκφραστεί µαθηµατικά σαν ένα 
δικτυωτό (“lattice”, [Gloss(00049)]) ([Birkhoff, 1967]). Για παράδειγµα στα  [Petridis et al., 2001] και 
[Petridis & Kaburlazos, 2001] ένα FLN µοντέλο για συσσωµάτωση εφαρµόζεται σε ένα δικτυωτό από 
γράφους οι οποίοι προκύπτουν από έναν θησαυρό όρων συνωνύµων µε σκοπό τον υπολογισµό 
δικτυωτών τα οποία περιέχουν σηµασιολογικά συσχετιζόµενες λέξεις. Σε αυτήν την εργασία το σ-
FLNMAP εφαρµόζεται σε ένα Ν-διάστατο υπερπαραλληλεπίπεδο.  
 Η είσοδος του σ-FLNMAP µπορεί να είναι είτε δυαδικά ανύσµατα , , …, 1b 2b Mb  είτε 
κανονικοποιηµένες παραλλαγές των ανυσµάτων συχνότητας . Στην δεύτερη περίπτωση, 
για m=1,2…, M, έχουµε όπου 
Mfff ...,,2,1
],...,,[ 21 wmNmmm gggg =
M
mjf
m
mjg max
=
= mj
f
,...,1
. Επιπρόσθετα, τα 
ανύσµατα µπορεί να είναι είτε ανύσµατα λέξεων είτε ανύσµατα εννοιών.  
 Τόσο οι φάσεις της συσσωµάτωσης όσο και οι φάσεις της κατηγοριοποίησης κάνουν χρήση της 
έννοιας του ασαφούς εγκλεισµού (“fuzzy inclusion”, [Gloss(00032)]). Τα δικτυωτά τα οποία 
υπολογίζονται από το σ-FLNMAP αποτελούν υπερπαραλληλεπίπεδα τα οποία περιέχονται στην 
µονάδα τoυ  – διάστατου υπερπαραλληλεπιπέδου. Ένα υπερπαραλληλεπίπεδο καθορίζεται από τις 
συντεταγµένες των διαγωνίως τοποθετηµένων άνω και κάτω γωνιών. Με άλλα λόγια ένα 
υπερπαραλληλεπίπεδο καθορίζεται από ένα ζευγάρι ανυσµάτων συντεταγµένων: . Έτσι, τα 
σηµεία δεδοµένων αντιστοιχούν σε «τετριµµένα» υπερπαραλληλεπίπεδα όπου η πάνω και η κάτω 
γωνία συµπίπτουν. Μια συνάρτηση θετικής αξιολόγησης υ(.) (“positive valuation function”, 
[Gloss(00062)]) αντιστοιχίζει σε κάθε παραλληλεπίπεδο Α έναν πραγµατικό αριθµό υ(Α), ο οποίος 
µπορεί να σχετίζεται µε το µέγεθος του Α. Ο βαθµός του ασαφούς εγκλεισµού του παραλληλεπιπέδου 
Α στο παραλληλεπίπεδο Β καθορίζεται σε σχέση µε την συνάρτηση εγκλεισµού 
WN
),( hgA =
)
)()(
B
BBA
∨
=≤
(Aυ
υσ  όπου Α Β είναι το µικρότερο παραλληλεπίπεδο το οποίο περιέχει τόσο το Α 
όσο και το Β. ∆οθέντος ενός υπερ παραλληλεπιπέδου 
∨
==A ),( hg  
 35
Κεφάλαιο 2 Κατηγοριοποίηση κειµένων µε χρήση λέξεων και  εννοιών 
[ ] [ ]( )wNwN hhhggg ,...,,,,...,, 2121  υπολογίζουµε το  όπου [ ]∑
=
+=
wN
i
ii hgA
1
)()( θυ ℜ→ℜ:θ  
είναι µια συνάρτηση τέτοια ώστε )( 2x)( 121 xxx θθ ≥⇒≤ . Αξίζει να σηµειωθεί ότι στην 
περίπτωση που περιγράφουµε έχουµε χρησιµοποιήσει την συνάρτηση xx −=1)(θ . Όπως εξηγείται 
λεπτοµερέστερα στο [Kaburlazos &  Petridis, 2000], η υ(.) ορίζει µια συνάρτηση θετικής αξιολόγησης 
και κατά συνέπεια ορίζει ένα µέτρο εγκλεισµού στο δικτυωτό των υπερπαραλληλεπιπέδων.  
JB
mg
]1,0[∈
JB JB mg∨
ρ
1gBL =
)( im Bg ≤
iB
σ
iB
 Έχοντας καθορίσει το µέτρο εγκλεισµού  µπορεί κανείς εύκολα να υλοποιήσει τους αλγορίθµους 
συσσωµάτωσης και κατηγοριοποίησης. Οι όροι «προσδιόρισε (ένα παραλληλεπίπεδο)» και 
«επαναπροσδιόρισε (ένα παραλληλεπίπεδο)» στον ακόλουθο αλγόριθµο σηµαίνουν ότι ένα 
παραλληλεπίπεδο  είναι «διαθέσιµο» ή «µη διαθέσιµο» αντίστοιχα για να δεχθεί ένα εισερχόµενο 
στοιχείο – κείµενο  όπου η «αποδοχή» στο  ορίζεται από την εξίσωση  =  όπως 
αναφέρεται λεπτοµερέστερα παρακάτω: 
JB
 
1. Η διαδικασία της συσσωµάτωσης πραγµατοποιείται κατά τον ακόλουθο τρόπο, όπου η  
παράµετρος ρ είναι µια παράµετρος «επαγρύπνησης» οριζόµενη από τον χρήστη µε 
.  
L: =1 
 ( το πρώτο κείµενο εισόδου προς εκπαίδευση φυλάσσεται στην µνήµη) 
For m =1,2, …, M 
   «θέσε σε διαθεσιµότητα» όλα τα παραλληλεπίπεδα ,  i =1,…, L 
   For i =1,…, L 
      Υπολόγισε το  
  EndFor 
 While (υπάρχει ένα «σύνολο» από παραλληλεπίπεδα  που φυλάσσονται στην µνήµη i 
=1,…, L) 
 36
Κεφάλαιο 2 Κατηγοριοποίηση κειµένων µε χρήση λέξεων και  εννοιών 
     =  το παραλληλεπίπεδο µε την JB LiBg im ,...1)},(max{ =≤σ  µεταξύ των 
παραλληλεπιπέδων που φυλάσσονται στην µνήµη  
  If  ρσ ≥≥ )( Jm Bg  then 
   =  («συνυπολόγισε» το στο ) JB JB mg∨ mg JB
    Εxit the while loop 
 Else 
    «όρισε το παραλληλεπίπεδο  µη διαθέσιµο» JB
 EndIf 
 EndWhile 
         If (όλα τα παραλληλεπίπεδα  που φυλάσσονται στην µνήµη i =1,…, L iB  έχουν 
χαρακτηριστεί ως    «µη διαθέσιµα») then 
              L:= L+1 
               = (αποθήκευσε στην µνήµη το κείµενο εισόδου ) LB mg mg
EndIf 
       EndFor 
 
2. Κατά την διάρκεια της κατηγοριοποίησης, µια συλλογή από παραλληλεπίπεδα (τα οποία 
έχουν παραχθεί από την διαδικασία της συσσωµάτωσης) είναι διαθέσιµη: . 
Ένα εισερχόµενο στοιχείο (κείµενο) g κατηγοριοποιείται στην κατηγορία του 
παραλληλεπιπέδου  το οποίο µεγιστοποιεί την συνάρτηση εγκλεισµού: 
RBB ,...,1
^
r
B
                                                   
Rr
rBgr
...,2,1
^
)(maxarg
=
≤= σ
 
 37
Κεφάλαιο 2 Κατηγοριοποίηση κειµένων µε χρήση λέξεων και  εννοιών 
Όπως είναι γνωστό από το [Petridis & Kaburlazos, 2000], τα παραλληλεπίπεδα τα οποία 
«µαθαίνονται» κατά την διάρκεια της διαδικασίας εκπαίδευσης από τον αλγόριθµο σ-FLNMAP 
εξαρτώνται από την σειρά µε την οποία παρέχονται τα δεδοµένα στον αλγόριθµο ως είσοδος. Ο 
αλγόριθµος «σ-FLNMAP µε ψηφοφορία» εµφανίζεται ως ένα σχήµα το οποίο εκπαιδεύει ένα σύνολο 
από µονάδες σ-FLNMAP, µε διαφορετικές εναλλαγές των δεδοµένων προς εκπαίδευση και στην 
συνέχεια κατηγοριοποιεί ένα κείµενο σύµφωνα µε την πλειοψηφία των ψήφων του παραπάνω 
συνόλου. Για ορισµένες τιµές της παραµέτρου «επαγρύπνησης» ρ και για ένα ορισµένο αριθµό 
ψηφοφόρων , η απόδοση της κατηγοριοποίησης του συνόλου είναι καλύτερη και πιο σταθερή 
συγκριτικά µε αυτή ενός µεµονωµένου σ-FLNMAP του συνόλου. Η ιδέα αυτή σχετίζεται µε την 
τεχνική “συσσώρευση και προώθηση” (“bagging and boosting”, [Gloss(00005)]) [Breiman et al., 
1984]. Οι παράµετροι του αλγορίθµου «σ-FLNMAP µε ψηφοφορία» είναι η παραµέτρος 
«επαγρύπνησης» ρ  και το πλήθος των ψηφοφόρων .  
Vn
Vn
2.6 Πειράµατα 
2.6.1 Πρώτη σειρά πειραµάτων  
2.6.1.1 Το σώµα κειµένων προς κατηγοριοποίηση  
 
Τα πειράµατα τα οποία πραγµατοποιήθηκαν και παρουσιάζονται στα [Petridis et al., 2001] και 
[Petridis & Kaburlazos, 2001], οµαδοποιούνται σε τέσσερις διαφορετικές οµάδες καθεµία από τις 
οποίες χρησιµοποιεί έναν διαφορετικό συνδυασµό κατηγοριών και µερών του λόγου. Σε καθεµία από 
τις παραπάνω οµάδες, πειραµατιστήκαµε χρησιµοποιώντας ως χαρακτηριστικούς όρους τόσο τις 
λέξεις όσο και τις έννοιες καθώς και διάφορες µεθόδους κατηγοριοποίησης. 
Πιο συγκεκριµένα, το πρόβληµα της κατηγοριοποίησης διαµερίστηκε σε δυο προβλήµατα. Το 
πρώτο από αυτά αφορούσε την κατηγοριοποίηση 100 κειµένων τα οποία ανήκαν συνολικά σε τρεις 
κατηγορίες του σώµατος κειµένων Brown Corpus, οι οποίες είναι οι Press: Reportage, Press: Editorial 
και General Fiction που αντιστοιχούν δηλαδή στην πρώτη, την δεύτερη και την δέκατη κατηγορία. To 
δεύτερο πρόβληµα αφορούσε την κατηγοριοποίηση 352 κειµένων τα οποία ανήκαν συνολικά στις 
δεκαπέντε κατηγορίες του σώµατος κειµένων Brown Corpus. Επιπρόσθετα, ως χαρακτηριστικοί όροι 
χρησιµοποιήθηκαν είτε όλα τα µέρη του λόγου, είτε µόνο τα ουσιαστικά και τα ρήµατα. Αυτό έχει ως 
αποτέλεσµα τις ακόλουθες τέσσερις οµάδες πειραµάτων: 
 38
Κεφάλαιο 2 Κατηγοριοποίηση κειµένων µε χρήση λέξεων και  εννοιών 
1. Η οµάδα πειραµάτων Α, η οποία χρησιµοποιεί τα 100 κείµενα τα οποία ανήκαν συνολικά σε τρεις 
κατηγορίες του σώµατος κειµένων Brown Corpus και ως χαρακτηριστικούς όρους όλα τα µέρη 
του λόγου (τόσο σε επίπεδο λέξεων όσο και σε επίπεδο εννοιών). 
2. Η οµάδα πειραµάτων Β, η οποία χρησιµοποιεί τα 352 κείµενα τα οποία ανήκαν συνολικά στις 
δεκαπέντε κατηγορίες του σώµατος κειµένων Brown Corpus και ως χαρακτηριστικούς όρους όλα 
τα µέρη του λόγου (τόσο σε επίπεδο λέξεων όσο και σε επίπεδο εννοιών). 
3. Η οµάδα πειραµάτων C, η οποία χρησιµοποιεί τα 100 κείµενα τα οποία ανήκαν συνολικά σε τρεις 
κατηγορίες του σώµατος κειµένων Brown Corpus και ως χαρακτηριστικούς όρους µόνο τα 
ουσιαστικά και τα ρήµατα (τόσο σε επίπεδο λέξεων όσο και σε επίπεδο εννοιών). 
4. Η οµάδα πειραµάτων D, η οποία χρησιµοποιεί τα 352 κείµενα τα οποία ανήκαν συνολικά στις 
δεκαπέντε κατηγορίες του σώµατος κειµένων Brown Corpus και ως χαρακτηριστικούς όρους µόνο 
τα ουσιαστικά και τα ρήµατα (τόσο σε επίπεδο λέξεων όσο και σε επίπεδο εννοιών). 
Οι τρόποι παράστασης ενός κειµένου οι οποίοι χρησιµοποιήθηκαν εδώ ήταν οι ακόλουθοι: 
• ∆υαδικό Άνυσµα Λέξεων. 
• Άνυσµα Συχνότητας Λέξεων. 
• ∆υαδικό Άνυσµα Εννοιών. 
• Άνυσµα Συχνότητας Εννοιών. 
2.6.1.2 Τα δεδοµένα προς εκπαίδευση και προς επαλήθευση 
 
Όπως αναφέρθηκε προηγούµενα,, το σύνολο των κειµένων µας στο πρώτο πρόβληµα 
αποτελείται από 100 κείµενα, ενώ στο δεύτερο από 352 κείµενα. Παρόλα αυτά, και στα δυο 
προβλήµατα, ορισµένα από τα κείµενα χρησιµοποιήθηκαν για την εκπαίδευση (το σύνολο κειµένων 
προς εκπαίδευση) των αλγορίθµων, ενώ τα υπόλοιπα (το σύνολο των κειµένων προς επαλήθευση) 
χρησιµοποιήθηκαν για την αξιολόγηση της απόδοσης των αλγορίθµων. Πιο συγκεκριµένα, σε κάθε 
οµάδα πειραµάτων και για κάθε σύνολο χαρακτηριστικών όρων, πραγµατοποιούµε δέκα 
διαφορετικούς διαχωρισµούς των κειµένων σε σύνολο κειµένων προς εκπαίδευση και σύνολο 
κειµένων προς επαλήθευση. Κάθε διαχωρισµός πραγµατοποιείται κατά τυχαίο τρόπο, παρόλα αυτά σε 
κάθε περίπτωση το σώµα των κειµένων προς εκπαίδευση αποτελείται από τα 2/3 των συνολικά 
διαθεσίµων κειµένων και το σώµα των κειµένων προς επαλήθευση αποτελείται από το υπόλοιπο 1/3 
 39
Κεφάλαιο 2 Κατηγοριοποίηση κειµένων µε χρήση λέξεων και  εννοιών 
των συνολικά διαθεσίµων κειµένων. Παρά το γεγονός ότι ο διαχωρισµός των κειµένων ήταν τυχαία, 
µεριµνήσαµε έτσι ώστε σε κάθε περίπτωση, κάθε µια από τις δεκαπέντε κατηγορίες να εκπροσωπείται 
από τον ίδιο προκαθορισµένο αριθµό κειµένων τόσο στο σύνολο των κειµένων προς εκπαίδευση όσο 
και στο σύνολο των κειµένων προς επαλήθευση. Η κατανοµή των κειµένων για κάθε κατηγορία σε 
σύνολα κειµένων προς εκπαίδευση και σε σύνολο κειµένων προς επαλήθευση δίνεται στον Πίνακα 
2.1.  
 
Πλήθος Κειµένων ανά 
Πρόβληµα & Κατηγορία 
Cat1 Cat2 Cat3 Cat4 Cat5 Cat6 Cat7 Cat8 
Κείµενα προς εκπαίδευση 30 18 12 12 21 19 19 14 
Κείµενα προς επαλήθευση 14 9 5 5 10 9 9 6 
 Cat9 Cat10 Cat11 Cat12 Cat13 Cat14 Cat15  
Κείµενα προς εκπαίδευση 33 20 12 4 12 8 6  
Κείµενα προς επαλήθευση 16 9 6 2 5 4 3  
Πίνακας 2.1: Κατανοµή κειµένων προς εκπαίδευση και επαλήθευση σε κάθε µια από τις δεκαπέντε κατηγορίες 
και στα δυο προβλήµατα της 1ης σειράς πειραµάτων. 
2.6.1.3 Παραστάσεις των κειµένων 
 
Το πρώτο βήµα σε κάθε ακολουθία πειραµάτων κατηγοριοποίησης είναι η κατασκευή του 
λεξιλογίου των λέξεων και των εννοιών.  Αυτό αποτελεί ένα αναγκαίο βήµα για την κατασκευή της 
παράστασης του κειµένου η οποία θα παρουσιαστεί παρακάτω. Το λεξιλόγιο των λέξεων 
κατασκευάζεται µε γνώσεις οι οποίες προέρχονται αποκλειστικά και µόνο από τα δεδοµένα προς 
εκπαίδευση. Αυτό σηµαίνει ότι τα κείµενα προς επαλήθευση είναι δυνατό να περιέχουν λέξεις οι 
οποίες δεν συµπεριλαµβάνονται στο παραπάνω λεξιλόγιο των λέξεων. Επιπρόσθετα, αυτό σηµαίνει 
επίσης ότι το µέγεθος του λεξιλογίου είναι δυνατό να µεταβάλλεται σε κάθε µια οµάδα πειραµάτων 
και σε κάθε µια από τις δυνατές παραστάσεις. Το ίδιο ακριβώς ισχύει και για τις έννοιες που 
εµφανίζονται στα κείµενα. Επιπρόσθετα στα πειράµατα τα οποία πραγµατοποιούνται εδώ, σε καθεµία 
από τις τέσσερις οµάδες πειραµάτων χρησιµοποιήθηκαν τέσσερις διαφορετικές παραδοχές σύµφωνα 
µε τις οποίες κατασκευάστηκε το λεξιλόγιο των λέξεων και των εννοιών. Αυτές περιγράφονται 
παρακάτω: 
 40
Κεφάλαιο 2 Κατηγοριοποίηση κειµένων µε χρήση λέξεων και  εννοιών 
1. Όλες οι λέξεις οι οποίες εµφανίζονται τουλάχιστον µια φορά σε τουλάχιστον ένα κείµενο. Στην 
οµάδα πειραµάτων Α αυτό οδήγησε στον σχηµατισµό ενός λεξιλογίου το οποίο αποτελούνταν από 
9348 λέξεις, στην οµάδα πειραµάτων Β σε ένα λεξιλόγιο των 22101 λέξεων, στην οµάδα 
πειραµάτων C σε ένα λεξιλόγιο των 7068 λέξεων ενώ στην οµάδα πειραµάτων D σε ένα λεξιλόγιο 
των 15728 λέξεων. 
2. Όλες οι λέξεις οι οποίες εµφανίζονται τουλάχιστον δυο φορές σε τουλάχιστον ένα κείµενο. Στην 
οµάδα πειραµάτων Α αυτό οδήγησε στον σχηµατισµό ενός λεξιλογίου το οποίο αποτελούνταν από 
2252 λέξεις, στην οµάδα πειραµάτων Β σε ένα λεξιλόγιο των 7495 λέξεων, στην οµάδα 
πειραµάτων C σε ένα λεξιλόγιο των 1898 λέξεων ενώ στην οµάδα πειραµάτων D σε ένα λεξιλόγιο 
των 5747 λέξεων. 
3. Όλες οι έννοιες οι οποίες εµφανίζονται τουλάχιστον µια φορά σε τουλάχιστον ένα κείµενο. Στην 
οµάδα πειραµάτων Α αυτό οδήγησε στον σχηµατισµό ενός λεξιλογίου το οποίο αποτελούνταν από 
10890 έννοιες, στην οµάδα πειραµάτων Β σε ένα λεξιλόγιο των 25683 εννοιών, στην οµάδα 
πειραµάτων C σε ένα λεξιλόγιο των 8448 εννοιών ενώ στην οµάδα πειραµάτων D σε ένα 
λεξιλόγιο των 18806 εννοιών. 
4. Όλες οι έννοιες οι οποίες εµφανίζονται τουλάχιστον δυο φορές σε τουλάχιστον ένα κείµενο. Στην 
οµάδα πειραµάτων Α αυτό οδήγησε στον σχηµατισµό ενός λεξιλογίου το οποίο αποτελούνταν από 
2580 έννοιες, στην οµάδα πειραµάτων Β σε ένα λεξιλόγιο των 9057 εννοιών, στην οµάδα 
πειραµάτων C σε ένα λεξιλόγιο των 2214 εννοιών ενώ στην οµάδα πειραµάτων D σε ένα 
λεξιλόγιο των 7144 εννοιών.  
Έχοντας κατασκευάσει το λεξιλόγιο των λέξεων, προχωρούµε ούτως ώστε να 
κατασκευάσουµε τις παραστάσεις για τα 100 συνολικά κείµενα του πρώτου προβλήµατος και για τα 
352 κείµενα του δεύτερου. Η διαδικασία αυτή πραγµατοποιείται και για τα δέκα σύνολα καθεµίας εκ 
των τεσσάρων οµάδων πειραµάτων. Για καθένα από αυτά κατασκευάζουµε τα ∆υαδικά Ανύσµατα 
Λέξεων, τα Ανύσµατα Συχνότητας Λέξεων, τα ∆υαδικά Ανύσµατα Εννοιών και τα Ανύσµατα 
Συχνότητας Εννοιών. 
2.6.1.4 Μέθοδοι Κατηγοριοποίησης 
 
Ξεκινώντας από τα δέκα διαφορετικά σύνολα (για εκπαίδευση και επαλήθευση) για κάθε 
οµάδα πειραµάτων και για κάθε σύνολο χαρακτηριστικών όρων  παράγουµε για καθένα από αυτά, 
τέσσερις διαφορετικές παραστάσεις: ∆υαδικό Άνυσµα Λέξεων (∆Λ), Άνυσµα Συχνότητας Λέξεων 
(ΣΛ),  ∆υαδικό Άνυσµα Εννοιών (∆Ε) και Άνυσµα Συχνότητας Εννοιών (ΣΕ). Στην συνέχεια 
 41
Κεφάλαιο 2 Κατηγοριοποίηση κειµένων µε χρήση λέξεων και  εννοιών 
παραθέτουµε τις παραµέτρους που λαµβάνει καθένας από τους αλγόριθµους κατηγοριοποίησης που 
εξετάζουµε στην εν λόγω σειρά πειραµάτων καθώς και το σε ποιες από τις παραπάνω παραστάσεις 
κειµένου καθένας από αυτούς εφαρµόζεται. Οι πληροφορίες αυτές δίνονται στον Πίνακα 2.2. Όπως 
µπορεί κανείς να παρατηρήσει από τον Πίνακα 2.2, προκύπτουν επτά διαφορετικά σχήµατα 
κατηγοριοποίησης όπου καθένας από τους παραπάνω αλγορίθµους περιέχει µια ή περισσότερες 
παραµέτρους οι οποίες είναι δυνατό να επηρεάσουν την απόδοση κατηγοριοποίησης. Πιο 
συγκεκριµένα, στον αλγόριθµο Κ-Πλησιέστερων Γειτόνων πειραµατιστήκαµε µε τον αριθµό Κ των 
κοντινότερων γειτόνων και µε την τιµή της ρυθµιστικής παραµέτρου Ρ, ενώ στον αλγόριθµο σ-
FLNMAP µε ψηφοφορία  πειραµατιστήκαµε τόσο µε το πλήθος  των ψηφοφόρων όσο και µε την 
τιµή της παραµέτρου ρ.  Για καθένα από τους παραπάνω αλγορίθµους, για κάθε ένα από τα δέκα 
σύνολα κειµένων, για κάθε µια επιλογή χαρακτηριστικών όρων και για κάθε µια από τις τέσσερις 
οµάδες πειραµάτων πραγµατοποιούµε πειράµατα κατηγοριοποίησης αρκετές φορές (χρησιµοποιώντας 
διάφορες τιµές παραµέτρων) και καταγράφουµε την απόδοση της κατηγοριοποίησης η οποία 
αντιστοιχεί σε κάθε µια από τις τιµές των παραµέτρων αυτών. Ο πειραµατισµός µε διάφορες τιµές των 
παραµέτρων οδήγησε στο συµπέρασµα ότι τα αποτελέσµατα δεν επιδεικνύουν ιδιαίτερη 
µεταβλητότητα σε µεταβολές των παραµέτρων µέσα σε ένα ευρύ φάσµα.   
Vn
 
Αλγόριθµος Παράµετροι Παραστάσεις Κειµένου 
Μέγιστης Ύστερης Πιθανότητας - Μαζική παραλλαγή 
(παράγραφος 2.5.1.1) 
1=α  ΣΛ, ΣΕ 
Κ-Πλησιέστερων Γειτόνων – Πρώτη παραλλαγή 
(παράγραφος 2.5.3.1) 
Ρ, Κ ∆Λ, ΣΛ, ∆Ε, ΣΕ 
Κ-Πλησιέστερων Γειτόνων-  ∆έυτερη παραλλαγή 
(παράγραφος 2.5.3.2) 
Ρ ∆Λ, ΣΛ, ∆Ε, ΣΕ 
σ-FLNMAP µε ψηφοφορία (παράγραφος 2.5.4) ρ ,  Vn ∆Λ, ΣΛ, ∆Ε, ΣΕ 
Πίνακας 2.2 Λίστα των αλγορίθµων κατηγοριοποίησης που χρησιµοποιούνται και των παραστάσεων κειµένου 
που χρησιµοποιεί καθένας από αυτούς (∆Λ = ∆υαδικό Άνυσµα Λέξεων, ΣΛ= Άνυσµα Συχνότητας Λέξεων, ∆Ε = 
∆υαδικό Άνυσµα Εννοιών, ΣΕ = Άνυσµα Συχνότητας Εννοιών) στην 1η σειρά πειραµάτων. 
 
 
 
 42
Κεφάλαιο 2 Κατηγοριοποίηση κειµένων µε χρήση λέξεων και  εννοιών 
2.6.1.5 Αποτελέσµατα Κατηγοριοποίησης  
 
Στην συνέχεια, επεξηγούµε την διάταξη µε την οποία παρουσιάζονται τα αποτελέσµατα 
κατηγοριοποίησης. Υπενθυµίζεται στον αναγνώστη ότι το βασικό πείραµα κατηγοριοποίησης 
επαναλαµβάνεται κάθε φορά πολλές φορές για κάθε µια από τις τέσσερις οµάδες πειραµάτων: 
1) Χρησιµοποιώντας κάθε µια από τις τέσσερις διαφορετικές παραστάσεις 
2) Για κάθε µια από αυτές τις παραστάσεις εξετάζουµε τους τέσσερις διαφορετικούς αλγορίθµους. 
3) Για κάθε µια παράσταση και για κάθε ένα από τους τέσσερις αλγορίθµους χρησιµοποιούµε 
διαφορετικές τιµές παραµέτρων. 
4) Για κάθε µια παράσταση, για κάθε έναν αλγόριθµο και για κάθε µια τιµή παραµέτρου, 
χρησιµοποιούµε και τα δέκα διαφορετικά σύνολα. 
Στόχος µας δεν είναι η αξιολόγηση των αλγορίθµων αλλά η σύγκριση της αξίας και 
συµπεριφοράς των λέξεων και των εννοιών υπό ορισµένες συνθήκες. Στον όρο συνθήκες 
συµπεριλαµβάνονται οι διάφοροι αλγόριθµοι κατηγοριοποίησης καθώς επίσης και οι τιµές των 
παραµέτρων που καθένας από αυτούς χρησιµοποιεί.  
Τα αποτελέσµατα παρουσιάζονται υπό την µορφή πινάκων µε σκοπό την διευκόλυνση της 
σύγκρισης ανάµεσα στις λέξεις και τις έννοιες. Στους Πίνακες 2.3 ως 2.6 (οι οποίοι αντιστοιχούν στις 
οµάδες πειραµάτων Α ως D) παρατίθεται ο µέσος όρος απόδοσης κατηγοριοποίησης . Οι τιµές 
οι οποίες παρουσιάζονται, αντιστοιχούν σε µια συγκεκριµένη επιλογή σχήµατος 
κατηγοριοποίησης η οποία εφαρµόζεται είτε στις λέξεις είτε στις έννοιες  και υπογραµµίζεται το 
καλύτερο αποτέλεσµα, υποδεικνύοντας αν ήταν οι λέξεις ή οι έννοιες οι οποίες παρουσίασαν την 
υψηλότερη απόδοση κατηγοριοποίησης.  
lic
−
lic
Ας θεωρήσουµε προς στιγµήν ότι έχουν καθοριστεί τόσο ο αλγόριθµος όσο και η παράσταση 
του κειµένου. Για µια συγκεκριµένη τιµή των παραµέτρων του εκάστοτε αλγορίθµου, ένα πείραµα 
κατηγοριοποίησης επαναλαµβάνεται δέκα φορές µια για κάθε ένα από τα δέκα σύνολα. Έτσι, για την 
l-στη παράµετρο και το i-στο σύνολο λαµβάνουµε την ακρίβεια κατηγοριοποίησης η οποία 
ορίζεται κατά τον ακόλουθο τρόπο: 
lic
=lic Πλήθος σωστά κατηγοριοποιηµένων κειµένων στο i-στο σύνολο κειµένων προς επαλήθευση / 
συνολικό πλήθος των κειµένων τα οποία περιέχονται στο i-στο σύνολο κειµένων προς επαλήθευση 
 43
Κεφάλαιο 2 Κατηγοριοποίηση κειµένων µε χρήση λέξεων και  εννοιών 
Στην συνέχεια υπολογίζουµε τους ακόλουθους δείκτες κατηγοριοποίησης λαµβάνοντας τους 
µέσους όρους πάνω σε όλα τα σύνολα κειµένων. Τα εν λόγω αποτελέσµατα παρουσιάζονται στους  
Πίνακες 2.3 ως 2.6. Μια εποπτικότερή τους εικόνα παρατίθεται στα Σχήµατα 2.1 και 2.2. 
Από τους Πίνακες 2.3 ως 2.6 διαφαίνεται ότι, στα αποτελέσµατα τα οποία προέκυψαν στην 
Οµάδα Α, όπου χρησιµοποιούνται όλα τα µέρη του λόγου των κειµένων που ανήκουν σε τρεις 
κατηγορίες, οι λέξεις πέτυχαν καλύτερη απόδοση κατηγοριοποίησης σε πέντε περιπτώσεις ενώ οι 
έννοιες σε εννέα περιπτώσεις ενώ η υψηλότερη συνολική απόδοση κατηγοριοποίησης επιτεύχθηκε 
από τον αλγόριθµο σ-FLNΜAP όταν αυτός εφαρµόστηκε στις έννοιες (82.18%). Εντελώς αντίστοιχα, 
στα αποτελέσµατα τα οποία προέκυψαν στην Οµάδα Β, όπου χρησιµοποιούνται όλα τα µέρη του 
λόγου των κειµένων που ανήκουν στις δεκαπέντε κατηγορίες, οι λέξεις πέτυχαν καλύτερη απόδοση 
κατηγοριοποίησης σε έξη περιπτώσεις ενώ οι έννοιες σε επτά περιπτώσεις, ενώ σε µια περίπτωση 
είχαµε ισοπαλία. Η υψηλότερη συνολική απόδοση κατηγοριοποίησης επιτεύχθηκε από τον αλγόριθµο 
σ-FLNΜAP όταν αυτός εφαρµόστηκε στις έννοιες (48.39%). Σε ότι αφορά τα αποτελέσµατα τα οποία 
προέκυψαν στην Οµάδα C, όπου χρησιµοποιούνται µόνο τα ουσιαστικά και τα ρήµατα των κειµένων 
που ανήκουν σε τρεις κατηγορίες, οι λέξεις πέτυχαν καλύτερη απόδοση κατηγοριοποίησης σε τέσσερις  
περιπτώσεις ενώ οι έννοιες σε  οκτώ περιπτώσεις, ενώ σε δυο περιπτώσεις είχαµε ισοπαλία. Η 
υψηλότερη συνολική απόδοση κατηγοριοποίησης επιτεύχθηκε από τον αλγόριθµο σ-FLNΜAP όταν 
αυτός εφαρµόστηκε στις έννοιες (82.50%). Τέλος σε ότι αφορά τα αποτελέσµατα τα οποία προέκυψαν 
στην Οµάδα D, όπου χρησιµοποιούνται µόνο τα ουσιαστικά και τα ρήµατα των κειµένων που ανήκουν 
στις δεκαπέντε κατηγορίες, οι λέξεις πέτυχαν καλύτερη απόδοση κατηγοριοποίησης σε έξη 
περιπτώσεις ενώ οι έννοιες σε οκτώ περιπτώσεις. Η υψηλότερη συνολική απόδοση κατηγοριοποίησης 
επιτεύχθηκε από τον αλγόριθµο σ-FLNΜAP όταν αυτός εφαρµόστηκε στις έννοιες (47.85%). 
 44
Κεφάλαιο 2 Κατηγοριοποίηση κειµένων µε χρήση λέξεων και  εννοιών 
 
Αλγόριθµος Λέξεις Άνυσµα 
Συχνότητας 
Έννοιες Άνυσµα 
Συχνότητας 
Λέξεις ∆υαδική 
Παράσταση 
Έννοιες ∆υαδική 
Παράσταση 
Μέγιστης Ύστερης 
Πιθανότητας - Μαζική 
παραλλαγή Συχνότητα 
2 ≥
55.62% 58.86%   
Μέγιστης Ύστερης 
Πιθανότητας - Μαζική 
παραλλαγή Συχνότητα 
1 ≥
53.12% 56.87%   
Κ-Πλησιέστ. Γειτόν- 1η 
παρ. Συχνότητα 2 ≥
74.00% 75.00% 74.00% 72.00% 
Κ-Πλησιέστ. Γειτόν- 1η 
παρ. Συχνότητα 1 ≥
76.00% 80.00% 80.00% 76.00% 
Κ-Πλησιέστ. Γειτόν- 2η 
παρ. Συχνότητα 2 ≥
72.00% 77.00% 80.00% 74.00% 
Κ-Πλησιέστ. Γειτόν- 2η 
παρ. Συχνότητα 1 ≥
77.00% 78.00% 80.00% 76.00% 
σ-FLNMAP µε 
ψηφοφορία Συχνότητα 
2 ≥
79.06% 78.43% 77.18% 79.06% 
σ-FLNMAP µε 
ψηφοφορία Συχνότητα 
1 ≥
80.00% 82.18% 79.68% 81.25% 
Πίνακας 2.3 Ποσοστό επιτυχίας κατηγοριοποίησης ως προς το κριτήριο του µέσου όρου απόδοσης για την 
Οµάδα πειραµάτων Α και τους αλγορίθµους κατηγοριοποίησης που χρησιµοποιούνται στην 1η σειρά πειραµάτων 
και των παραστάσεων κειµένου που χρησιµοποιεί καθένας από αυτούς. 
 
 
 
 
 45
Κεφάλαιο 2 Κατηγοριοποίηση κειµένων µε χρήση λέξεων και  εννοιών 
Αλγόριθµος Λέξεις Άνυσµα 
Συχνότητας 
Έννοιες Άνυσµα 
Συχνότητας 
Λέξεις ∆υαδική 
Παράσταση 
Έννοιες ∆υαδική 
Παράσταση 
Μέγιστης Ύστερης 
Πιθανότητας - Μαζική 
παραλλαγή Συχνότητα 
2 ≥
43.23% 39.10%   
Μέγιστης Ύστερης 
Πιθανότητας - Μαζική 
παραλλαγή Συχνότητα 
1 ≥
39.60% 38.21%   
Κ-Πλησιέστ. Γειτόν- 1η 
παρ. Συχνότητα 2 ≥
35.00% 39.00% 42.00% 39.00% 
Κ-Πλησιέστ. Γειτόν- 1η 
παρ. Συχνότητα 1 ≥
42.00% 46.00% 45.00% 46.00% 
Κ-Πλησιέστ. Γειτόν- 2η 
παρ. Συχνότητα 2 ≥
36.00% 38.00% 46.00% 40.00% 
Κ-Πλησιέστ. Γειτόν- 2η 
παρ. Συχνότητα 1 ≥
40.00% 45.00% 47.00% 47.00% 
σ-FLNMAP µε 
ψηφοφορία Συχνότητα 
2 ≥
44.10% 42.23% 43.12% 40.71% 
σ-FLNMAP µε 
ψηφοφορία Συχνότητα 
1 ≥
44.91% 48.39% 43.83% 46.60% 
Πίνακας 2.4 Ποσοστό επιτυχίας κατηγοριοποίησης ως προς το κριτήριο του µέσου όρου απόδοσης για την 
Οµάδα πειραµάτων Β και τους αλγορίθµους κατηγοριοποίησης που χρησιµοποιούνται στην 1η σειρά πειραµάτων 
και των παραστάσεων κειµένου που χρησιµοποιεί καθένας από αυτούς. 
 
 
 
 
 
 
 
 46
Κεφάλαιο 2 Κατηγοριοποίηση κειµένων µε χρήση λέξεων και  εννοιών 
Αλγόριθµος Λέξεις Άνυσµα 
Συχνότητας 
Έννοιες Άνυσµα 
Συχνότητας 
Λέξεις ∆υαδική 
Παράσταση 
Έννοιες ∆υαδική 
Παράσταση 
Μέγιστης Ύστερης 
Πιθανότητας - Μαζική 
παραλλαγή Συχνότητα 
2 ≥
55.93% 59.37%   
Μέγιστης Ύστερης 
Πιθανότητας - Μαζική 
παραλλαγή Συχνότητα 
1 ≥
55.31% 61.24%   
Κ-Πλησιέστ. Γειτόν- 1η 
παρ. Συχνότητα 2 ≥
73.44% 75.31% 75.31% 72.81% 
Κ-Πλησιέστ. Γειτόν- 1η 
παρ. Συχνότητα 1 ≥
75.94% 80.31% 80.31% 75.63% 
Κ-Πλησιέστ. Γειτόν- 2η 
παρ. Συχνότητα 2 ≥
72.50% 76.60% 79.70% 74.70% 
Κ-Πλησιέστ. Γειτόν- 2η 
παρ. Συχνότητα 1 ≥
76.60% 79.10% 80.00% 76.90% 
σ-FLNMAP µε 
ψηφοφορία Συχνότητα 
2 ≥
79.06% 79.06% 78.12% 78.12% 
σ-FLNMAP µε 
ψηφοφορία Συχνότητα 
1 ≥
80.62% 82.50% 80.62% 81.25% 
Πίνακας 2.5 Ποσοστό επιτυχίας κατηγοριοποίησης ως προς το κριτήριο του µέσου όρου απόδοσης για την 
Οµάδα πειραµάτων C και τους αλγορίθµους κατηγοριοποίησης που χρησιµοποιούνται στην 1η σειρά πειραµάτων 
και των παραστάσεων κειµένου που χρησιµοποιεί καθένας από αυτούς. 
 
 
 
 
 
 
 
 47
Κεφάλαιο 2 Κατηγοριοποίηση κειµένων µε χρήση λέξεων και  εννοιών 
Αλγόριθµος Λέξεις Άνυσµα 
Συχνότητας 
Έννοιες Άνυσµα 
Συχνότητας 
Λέξεις ∆υαδική 
Παράσταση 
Έννοιες ∆υαδική 
Παράσταση 
Μέγιστης Ύστερης 
Πιθανότητας - Μαζική 
παραλλαγή Συχνότητα 
2 ≥
44.47% 39.82%   
Μέγιστης Ύστερης 
Πιθανότητας - Μαζική 
παραλλαγή Συχνότητα 
1 ≥
41.00% 38.57%   
Κ-Πλησιέστ. Γειτόν- 1η 
παρ. Συχνότητα 2 ≥
34.29% 34.29% 41.25% 39.46% 
Κ-Πλησιέστ. Γειτόν- 1η 
παρ. Συχνότητα 1 ≥
41.16% 46.16% 45.71% 45.71% 
Κ-Πλησιέστ. Γειτόν- 2η 
παρ. Συχνότητα 2 ≥
36.70% 39.60% 43.80% 40.40% 
Κ-Πλησιέστ. Γειτόν- 2η 
παρ. Συχνότητα 1 ≥
42.50% 47.30% 47.20% 48.10% 
σ-FLNMAP µε 
ψηφοφορία Συχνότητα 
2 ≥
45.00% 44.01% 43.30% 41.51% 
σ-FLNMAP µε 
ψηφοφορία Συχνότητα 
1 ≥
44.01% 47.85% 41.60% 47.23% 
Πίνακας 2.6 Ποσοστό επιτυχίας κατηγοριοποίησης ως προς το κριτήριο του µέσου όρου απόδοσης για την 
Οµάδα πειραµάτων D και τους αλγορίθµους κατηγοριοποίησης που χρησιµοποιούνται στην 1η σειρά πειραµάτων 
και των παραστάσεων κειµένου που χρησιµοποιεί καθένας από αυτούς. 
 48
Κεφάλαιο 2 Κατηγοριοποίηση κειµένων µε χρήση λέξεων και  εννοιών 
0
10
20
30
40
50
60
70
80
90
Λέξεις Άνυσµα Συχνότητας, Έννοιες Άνυσµα Συχνότητας, Λέξεις ∆υαδική Παράσταση, 
Έννοιες ∆υαδική Παράσταση
Μέγιστ. Ύστερ. Πιθανότητας Συχνότητα >=2 Μέγιστ. Ύστερ. Πιθανότητας Συχνότητα >=1
Κ-Πλησιέστ, Γειτόν- 1η παρ, Συχνότητα >=2 Κ-Πλησιέστ, Γειτόν- 1η παρ, Συχνότητα >=1
Κ-Πλησιέστ, Γειτόν- 2η παρ, Συχνότητα >=2 Κ-Πλησιέστ, Γειτόν- 2η παρ, Συχνότητα >=1
σ-FLMMAP µε ψηφοφορία Συχνότητα >=2 σ-FLMMAP µε ψηφοφορία Συχνότητα >=1
 
Σχήµα 2.1: Μέσος όρος του ποσοστού επιτυχίας κατηγοριοποίησης ως προς το κριτήριο του µέσου 
όρου απόδοσης για τις Οµάδα πειραµάτων Α και C της 1ης σειράς µε χρήση των 3 κατηγοριών 
0
10
20
30
40
50
Λέξεις Άνυσµα Συχνότητας, Έννοιες Άνυσµα Συχνότητας, Λέξεις ∆υαδική 
Παράσταση, Έννοιες ∆υαδική Παράσταση
Μέγιστ. Ύστερ. Πιθανότητας Συχνότητα >=2 Μέγιστ. Ύστερ. Πιθανότητας Συχνότητα >=1
Κ-Πλησιέστ, Γειτόν- 1η παρ, Συχνότητα >=2 Κ-Πλησιέστ, Γειτόν- 1η παρ, Συχνότητα >=1
Κ-Πλησιέστ, Γειτόν- 2η παρ, Συχνότητα >=2 Κ-Πλησιέστ, Γειτόν- 2η παρ, Συχνότητα >=1
σ-FLMMAP µε ψηφοφορία Συχνότητα >=2 σ-FLMMAP µε ψηφοφορία Συχνότητα >=1
 
Σχήµα 2.2: Μέσος όρος του ποσοστού επιτυχίας κατηγοριοποίησης ως προς το κριτήριο του µέσου 
όρου απόδοσης για τις Οµάδα πειραµάτων B και D της 1ης σειράς µε χρήση των 3 κατηγοριών 
 
 49
Κεφάλαιο 2 Κατηγοριοποίηση κειµένων µε χρήση λέξεων και  εννοιών 
2.6.1.6 Συµπεράσµατα  
 
Στόχος των παραπάνω πειραµάτων ήταν η εξέταση του οφέλους κατηγοριοποίησης 
χρησιµοποιώντας τις έννοιες έναντι της κατηγοριοποίησης χρησιµοποιώντας τις λέξεις. Από τα 
πειράµατα τα οποία πραγµατοποιήσαµε καταλήξαµε στα ακόλουθα συµπεράσµατα. 
1. Οι κατηγοριοποιητές οι οποίοι χρησιµοποιούσαν τις έννοιες υπερίσχυσαν σε 32 περιπτώσεις έναντι 
αυτών των λέξεων, αποδείχθηκαν χειρότεροι σε 21 περιπτώσεις ενώ σε 3 περιπτώσεις οι δύο αυτοί 
τύποι κατηγοριοποιητών ήρθαν ισόπαλοι. Επιπρόσθετα, η καλύτερη συνολική απόδοση επιτεύχθηκε 
από τον αλγόριθµο σ-FLNΜAP µε ψηφοφορία ο οποίος εφαρµόστηκε στις έννοιες. 
2. Η διαφορά στην απόδοση των κατηγοριοποιητών οι οποίοι χρησιµοποιούν τις έννοιες και σε αυτούς 
που χρησιµοποιούν τις λέξεις σε ορισµένες περιπτώσεις είναι σηµαντική (κυµαινόµενη από 1 ως 6%), 
ενώ στις υπόλοιπες περιπτώσεις είναι οριακή (λιγότερη από 1%).  
3. Τα συνολικά καλύτερα αποτελέσµατα τόσο για την κατηγοριοποίηση χρησιµοποιώντας τις έννοιες 
όσο και για την κατηγοριοποίηση χρησιµοποιώντας τις λέξεις ελήφθησαν από τον αλγόριθµο σ-
FLNΜAP µε ψηφοφορία.  
4. O αλγόριθµος Naïve Bayes ή αλλιώς ο αλγόριθµος Κατηγοριοποίησης Μέγιστης Ύστερης 
Πιθανότητας µε τιµή παραµέτρου  ίση µε 1 εµφανίζει τα χειρότερα αποτελέσµατα συνολικά εκτός 
από τις περιπτώσεις εκείνες των οµάδων Α και D µε Συχνότητα Εµφάνισης >= 2 τόσο στη περίπτωση 
των λέξεων όσο και στην περίπτωση των εννοιών.  
a
5. Σε όλες τις περιπτώσεις εκτός από µια (Οµάδα A, Έννοιες Συχνότητα Εµφάνισης >= 2) η 
παράσταση µε την βοήθεια του ανύσµατος συχνότητας υπερισχύει σε απόδοση έναντι της ∆υαδικής 
παράστασης στον αλγόριθµο σ-FLNΜAP µε ψηφοφορία ενώ στον αλγόριθµο K-Πλησιέστερων 
Γειτόνων δεν µπορεί να εξαχθεί ένα τέτοιο συµπέρασµα. 
6. Η απόδοση η οποία επιτεύχθηκε αποδεικνύει ότι, το πρόβληµα της κατηγοριοποίησης των κειµένων 
τα οποία ανήκουν στις δεκαπέντε κατηγορίες είναι δύσκολο. Αυτό µπορεί να οφείλεται στο ότι 
ορισµένες κατηγορίες µπορεί να µοιάζουν ως προς το θέµα τους (όπως για παράδειγµα οι κατηγορίες 
Press: Reportage και Press: Editorial) καθώς επίσης και στο γεγονός ότι σε κάθε κατηγορία 
περικλείεται µεγάλη γκάµα κειµένων. 
Η επιτυχία του αλγορίθµου σ-FLNΜAP µε ψηφοφορία φαίνεται να οφείλεται στην 
αποτελεσµατικότητα του «µέτρου εγκλεισµού»σ  και στην ικανότητα γενίκευσης του µοντέλου η 
 50
Κεφάλαιο 2 Κατηγοριοποίηση κειµένων µε χρήση λέξεων και  εννοιών 
οποία βασίζεται στον υπολογισµό των οµοιόµορφων υπερπαραλληλεπιπέδων στα δεδοµένα προς 
εκπαίδευση. Η απόδοση του αλγορίθµου σ-FLNΜAP µε ψηφοφορία παραµένει σχετικά σταθερή για 
ένα µεγάλο εύρος τιµών της παραµέτρου «επαγρύπνησης» ρ και µειώνεται απότοµα όταν η εν λόγω 
παράµετρος πλησιάζει την τιµή 1. 
2.6.2 ∆εύτερη σειρά πειραµάτων  
 
Λαµβάνοντας υπόψη τα αποτελέσµατα των παραπάνω πειραµάτων θεωρήσαµε σκόπιµο να 
πραγµατοποιήσουµε συµπληρωµατικά πειράµατα µε σκοπό να αποκοµίσουµε µια πιο 
εµπεριστατωµένη και βαθύτερη εικόνα σε ότι αφορά την αποτελεσµατικότητα της κατηγοριοποίησης 
χρησιµοποιώντας τις έννοιες αντί για τις λέξεις. 
2.6.2.1  Το σώµα κειµένων προς κατηγοριοποίηση   
 
Στα συγκεκριµένα πειράµατα δεν ελήφθησαν υπόψη τα κείµενα της κατηγορίας Religion τα 
οποία είναι συνολικά τέσσερα αλλά ούτε και τα 166 κείµενα τα οποία φέρουν ετικέτα µόνο στα 
ουσιαστικά και τα ρήµατα. Τα καινούργια πειράµατα πραγµατοποιήθηκαν στα 182 κείµενα τα οποία 
ανήκουν σε συνολικά 14 κατηγορίες. Επιπρόσθετα, ορισµένες από τις 14 αυτές κατηγορίες 
συγχωνεύτηκαν σε καινούργιες. Στόχος αυτού του είδους της προεπεξεργασίας ήταν η λήψη ενός 
σεβαστού αριθµού κειµένων τα οποία φέρουν την πληροφορία της έννοιας της κάθε λέξης για κάθε 
µια από τις κατηγορίες. Το τελικό σύνολο κειµένων αποτελούνταν από 182 κείµενα τα οποία 
θεωρήθηκε ότι ανήκαν σε επτά  κατηγορίες όπως διακρίνεται στον Πίνακα 2.7. 
Έτσι, µε την εν λόγω «ανακατανοµή» των κειµένων και των κατηγοριών, το πρόβληµα της 
κατηγοριοποίησης περιορίστηκε σε ένα και µοναδικό το οποίο περιλάµβανε συνολικά 182 κείµενα τα 
οποία ανήκαν σε επτά κατηγορίες. Στο εν λόγω πρόβληµα χρησιµοποιήθηκαν όλα τα µέρη του λόγου 
που περιέχονται στα κείµενα, ενώ πειραµατιστήκαµε χρησιµοποιώντας τόσο τις λέξεις όσο και τις 
έννοιες των κειµένων καθώς και διάφορες µεθόδους κατηγοριοποίησης. 
 Οι τρόποι παράστασης ενός κειµένου οι οποίοι χρησιµοποιήθηκαν εδώ ήταν οι ίδιοι µε αυτούς 
που χρησιµοποιήθηκαν στα προηγούµενα πειράµατα, δηλαδή: 
• ∆υαδικό Άνυσµα Λέξεων. 
• Άνυσµα Συχνότητας Λέξεων. 
 51
Κεφάλαιο 2 Κατηγοριοποίηση κειµένων µε χρήση λέξεων και  εννοιών 
• ∆υαδικό Άνυσµα Εννοιών. 
• Άνυσµα Συχνότητας Εννοιών. 
 
Αρχικές Κατηγορίες Πλήθος 
κειµένων στην 
αρχική 
κατηγορία 
Νέα 
Κατηγορία 
Πλήθος 
κειµένων στη 
νέα κατηγορία 
1 Press: Reportage (A) 
2 Press: Editorial (B) 
3 Press: Reviews ( C ) 
7 
2 
3 
Press 12 
4 Skills and Hobbies (E) 14 Skills and 
Hobbies 
14 
5 Popular Lore (F) 19 Popular Lore 19 
6 Belles Lettres/ Biography/ Memoirs 
(G) 
18 Belles Lettres 
etc 
18 
7 Miscellaneous (H) 12 Miscellaneous 12 
8 Learned (J) 43 Learned 43 
9 General Fiction (K) 
10 Mystery and Detective Fiction (L) 
11  Science Fiction (M) 
12 Adventure and Western Fiction (N) 
13  Romance and Love Story (P) 
14 Humor (R) 
29 
11 
2 
 
10 
6 
6 
Fiction 64 
Πίνακας 2.7: Οι κατηγορίες της συλλογής κειµένων και το πλήθος των κειµένων ανά κατηγορία τόσο για τις 
αρχικές όσο και για τις καινούργιες κατηγορίες. 
 52
Κεφάλαιο 2 Κατηγοριοποίηση κειµένων µε χρήση λέξεων και  εννοιών 
2.6.2.2 Τα δεδοµένα προς εκπαίδευση και επαλήθευση  
 
Όπως επεξηγήθηκε και προηγούµενα, το αρχικό µας σύνολο δεδοµένων αποτελείται από 182 
κείµενα που προέρχονται από το σχολιασµένο σύνολο κειµένων Brown Corpus. Μερικά από αυτά τα 
κείµενα (το σύνολο κειµένων προς εκπαίδευση) χρησιµοποιήθηκαν για την εκπαίδευση των 
αλγορίθµων που παρουσιάστηκαν παραπάνω ενώ τα υπόλοιπα (το σύνολο κειµένων προς 
επαλήθευση) χρησιµοποιήθηκαν για την αξιολόγηση της απόδοσης των αλγορίθµων. Τα 182 κείµενα 
χωρίστηκαν µε τυχαίο τρόπο σε ένα σύνολο το οποίο αποτελούνταν από τα 2/3 αυτών δηλαδή από 123 
κείµενα (το οποίο αποτέλεσε το σύνολο κειµένων προς εκπαίδευση) και σε ένα δεύτερο σύνολο το 
οποίο περιείχε το υπόλοιπο 1/3 των κειµένων δηλαδή τα υπόλοιπα 59 κείµενα. Η παραπάνω 
διαδικασία τυχαίου διαχωρισµού κειµένων σε σύνολα µε 2/3 και 1/3 των συνολικών κειµένων 
πραγµατοποιήθηκε δέκα φορές. Κατά αυτόν τον τρόπο παρήχθησαν δέκα ζεύγη συνόλων προς 
εκπαίδευση και προς επαλήθευση. Αναφερόµαστε σε καθένα από αυτά ως Set0, Set1, …., Set9. Παρά 
το γεγονός ότι η τµηµατοποίηση των κειµένων ήταν τυχαία, µεριµνήσαµε έτσι ώστε σε κάθε 
περίπτωση κάθε µια από τις επτά κατηγορίες να εκπροσωπείται από τον ίδιο προκαθορισµένο αριθµό 
κειµένων τόσο στο σύνολο κειµένων προς εκπαίδευση όσο και στο σύνολο κειµένων προς 
επαλήθευση. Η κατανοµή των κειµένων για κάθε κατηγορία σε σύνολα κειµένων προς εκπαίδευση και 
σε σύνολα κειµένων προς επαλήθευση δίνονται στο Πίνακα 2.8. 
 
Κατηγορία  CAT1 CAT2 CAT3 CAT4 CAT5  CAT6  CAT7 
Πλήθος Κειµένων στο σώµα 
κειµένων προς εκπαίδευση  
8 10  13 12 8 29 43 
Πλήθος Κειµένων στο σώµα 
κειµένων προς επαλήθευση  
4 4 6 6 4 14 21 
Πίνακας 2.8: Η κατανοµή των κειµένων για κάθε µια κατηγορία, σε κείµενα προς εκπαίδευση και κείµενα προς 
επαλήθευση. 
 
Σε κάθε ένα από αυτά τα δέκα σύνολα δεδοµένων πραγµατοποιήσαµε πειράµατα 
εφαρµόζοντας τα σε κάθε έναν από τους αλγορίθµους κατηγοριοποίησης (όπου σε κάθε έναν από 
αυτούς οι παράµετροι λαµβάνουν διάφορες τιµές από ένα κατάλληλο εύρος τιµών). 
 53
Κεφάλαιο 2 Κατηγοριοποίηση κειµένων µε χρήση λέξεων και  εννοιών 
2.6.2.3 Παραστάσεις των κειµένων 
 
Όπως αναφέρθηκε και στην πρώτη σειρά πειραµάτων, το πρώτο βήµα σε κάθε ακολουθία 
πειραµάτων κατηγοριοποίησης είναι η κατασκευή του λεξιλογίου των λέξεων και των εννοιών.  Αυτό 
αποτελεί ένα αναγκαίο βήµα για την κατασκευή της παράστασης του κειµένου. Το λεξιλόγιο των 
λέξεων κατασκευάζεται µε γνώσεις οι οποίες προέρχονται αποκλειστικά και µόνο από τα δεδοµένα 
προς εκπαίδευση. Αυτό σηµαίνει ότι τα κείµενα προς επαλήθευση είναι δυνατό να περιέχουν λέξεις οι 
οποίες δεν συµπεριλαµβάνονται στο παραπάνω λεξιλόγιο των λέξεων. Επιπρόσθετα, αυτό σηµαίνει 
επίσης ότι το µέγεθος του λεξιλογίου είναι δυνατό να µεταβάλλεται σε καθένα από τα δέκα 
διαφορετικά σύνολα κειµένων. Οι ίδιες παρατηρήσεις ισχύουν και για την περίπτωση του λεξιλογίου 
του εννοιών. Στο Πίνακα 2.9 παραθέτουµε το µέγεθος των λεξιλογίων των λέξεων και των εννοιών για 
καθένα από τα δέκα διαφορετικά σύνολα κειµένων. 
Έχοντας κατασκευάσει το λεξιλόγιο των λέξεων, προχωρούµε ούτως ώστε να κατασκευάσουµε 
τις παραστάσεις για τα 182 κείµενα. Η διαδικασία αυτή πραγµατοποιείται και για τα δέκα σύνολα. Για 
καθένα από αυτά κατασκευάζουµε τα ∆υαδικά Ανύσµατα Λέξεων, τα Ανύσµατα Συχνότητας Λέξεων, 
τα ∆υαδικά Ανύσµατα Εννοιών και τα Ανύσµατα Συχνότητας Εννοιών. 
2.6.2.4 Μέθοδοι Κατηγοριοποίησης 
 
Ξεκινώντας από τα δέκα διαφορετικά σύνολα (για εκπαίδευση και επαλήθευση) παράγουµε για 
καθένα από αυτά τέσσερις διαφορετικές παραστάσεις: ∆υαδικό Άνυσµα Λέξεων (∆Λ),  Άνυσµα 
Συχνότητας Λέξεων (ΣΛ),  ∆υαδικό Άνυσµα Εννοιών (∆Ε) και Άνυσµα Συχνότητας Εννοιών (ΣΕ). 
Στην συνέχεια παραθέτουµε τις παραµέτρους που λαµβάνει καθένας από τους αλγόριθµους 
κατηγοριοποίησης που εξετάζουµε στην εν λόγω σειρά πειραµάτων καθώς και το σε ποιες από τις 
παραπάνω παραστάσεις κειµένου καθένας από αυτούς εφαρµόζεται. Οι πληροφορίες αυτές δίνονται 
στον Πίνακα 2.10. 
 Όπως µπορεί κανείς να παρατηρήσει από τον Πίνακα 2.10, καθένας από τους παραπάνω 
αλγορίθµους περιέχει µια ή περισσότερες παραµέτρους οι οποίες είναι δυνατό να επηρεάσουν την 
απόδοση κατηγοριοποίησης. Αυτές οι παράµετροι συγκεντρώνονται σε ένα άνυσµα παραµέτρων το 
οποίο συµβολίζεται µε το γράµµα π. Για καθένα από τους παραπάνω αλγορίθµους και για κάθε ένα 
από τα δέκα σύνολα κειµένων πραγµατοποιούµε πειράµατα κατηγοριοποίησης αρκετές φορές 
(χρησιµοποιώντας διάφορες τιµές παραµέτρων Lπππ ,...,, 21 ) και καταγράφουµε την απόδοση της 
κατηγοριοποίησης η οποία αντιστοιχεί σε κάθε µια από τις τιµές των παραµέτρων αυτών. 
 54
Κεφάλαιο 2 Κατηγοριοποίηση κειµένων µε χρήση λέξεων και  εννοιών 
Dataset Μέγεθος του λεξιλογίου των λέξεων Μέγεθος του λεξιλογίου των εννοιών  
Set0 15860 20134 
Set1 15698 19976 
Set2 15684 20094 
Set3 15872 20246 
Set4 15579 19943 
Set5 15833 20138 
Set6 15789 20147 
Set7 15683 19940 
Set8 15796 20128 
Set9  15705 19994 
Πίνακας 2.9: Τα µεγέθη του λεξιλογίου τόσο των λέξεων όσο και των εννοιών για κάθε ένα από τα σύνολα 
δεδοµένων της 2ης σειράς πειραµάτων, όπως αυτά προκύπτουν από τον τρόπο αναπαράστασης των κειµένων ο 
οποίος περιγράφεται στην παράγραφο 2.4.  
 
Αλγόριθµος Παράµετροι  Παραστάσεις Κειµένου 
Μέγιστης Ύστερης Πιθανότητας - Μαζική παραλλαγή 
(παράγραφος 2.5.1.1) 
α  ΣΛ, ΣΕ 
Μέγιστης Ύστερης Πιθανότητας – Αναδροµική 
παραλλαγή (παράγραφος 2.5.1.2) 
α , h ΣΛ, ΣΕ 
Μέγιστης Πιθανοφάνειας (παράγραφος 2.5.2)  α  ΣΛ, ΣΕ 
Κ-Πλησιέστερων Γειτόνων - Τρίτη παραλλαγή 
(παράγραφος 2.5.3.3) 
Ρ ∆Λ, ΣΛ, ∆Ε, ΣΕ 
Κ-Πλησιέστερων Γειτόνων-  Τέταρτη παραλλαγή 
(παράγραφος 2.5.3.4) 
Ρ ∆Λ, ΣΛ, ∆Ε, ΣΕ 
σ-FLNMAP µε ψηφοφορία (παράγραφος 2.5.4) ρ ,  Vn ∆Λ, ΣΛ, ∆Ε, ΣΕ 
Πίνακας 2.10 Λίστα των αλγορίθµων κατηγοριοποίησης που χρησιµοποιούνται και των παραστάσεων κειµένου 
που χρησιµοποιεί καθένας από αυτούς (∆Λ = ∆υαδικό Άνυσµα Λέξεων, ΣΛ= Άνυσµα Συχνότητας Λέξεων, ∆Ε = 
∆υαδικό Άνυσµα Εννοιών, ΣΕ = Άνυσµα Συχνότητας Εννοιών) στην 2η σειρά πειραµάτων. 
 55
Κεφάλαιο 2 Κατηγοριοποίηση κειµένων µε χρήση λέξεων και  εννοιών 
2.6.2.5 Αποτελέσµατα Κατηγοριοποίησης  
 
Σε απόλυτη αναλογία µε την 1η σειρά πειραµάτων, επεξηγούµε την διάταξη µε την οποία 
παρουσιάζονται τα αποτελέσµατα κατηγοριοποίησης. Υπενθυµίζεται στον αναγνώστη ότι το βασικό 
πείραµα κατηγοριοποίησης επαναλαµβάνεται κάθε φορά πολλές φορές: 
1) Χρησιµοποιώντας κάθε µια από τις τέσσερις διαφορετικές παραστάσεις. 
2) Για κάθε µια από αυτές τις παραστάσεις εξετάζουµε τους έξη διαφορετικούς αλγορίθµους. 
3) Για κάθε µια παράσταση και για κάθε ένα από τους έξη αλγορίθµους χρησιµοποιούµε 
διαφορετικές τιµές στο άνυσµα παραµέτρων. 
4) Για κάθε µια παράσταση, για κάθε έναν αλγόριθµο και για κάθε µια τιµή παραµέτρου, 
χρησιµοποιούµε και τα δέκα διαφορετικά σύνολα. 
Όπως τονίσαµε και στην 1η σειρά πειραµάτων, στόχος µας δεν αποτελεί η αξιολόγηση των 
αλγορίθµων αλλά η σύγκριση της αξίας και συµπεριφοράς των λέξεων και των εννοιών υπό ορισµένες 
συνθήκες. Στον όρο συνθήκες συµπεριλαµβάνονται οι διάφοροι αλγόριθµοι κατηγοριοποίησης καθώς 
επίσης και οι τιµές των παραµέτρων που καθένας από αυτούς χρησιµοποιεί.  
 Λαµβανοµένων υπόψη των παραπάνω παρουσιάζουµε συνοπτικά τα αποτελέσµατα των 
αλγορίθµων µας στους Πίνακες 2.11-2.17. Ο Πίνακας 2.11 είναι εντελώς ανάλογος µε τους Πίνακες 
2.3-2.6 και παραθέτει τον µέσο όρο απόδοσης κατηγοριοποίησης για κάθε έναν από τους έξη 
αλγορίθµους κατηγοριοποίησης τόσο στην περίπτωση όπου χρησιµοποιούνται οι λέξεις όσο και στην 
περίπτωση όπου χρησιµοποιούνται οι έννοιες και για τις τέσσερις διαφορετικές παραστάσεις. Κάθε 
ένας από τους Πίνακες 2.12-217 αντιστοιχεί και σε ένα από τους διαφορετικούς αλγορίθµους 
κατηγοριοποίησης, ενώ κάθε στήλη σε καθέναν από αυτούς αντιστοιχεί σε κάθε µια από τις τέσσερις 
διαφορετικές παραστάσεις. Επιπρόσθετα, σε κάθε στήλη παρουσιάζονται διάφοροι δείκτες ακρίβειας 
της κατηγοριοποίησης: ελάχιστο, συγκεκριµένης τιµής παραµέτρου (“fixed parameter”, 
[Gloss(00031)]), µέγιστο, µέσος όρος, και περιστασιακά επαλήθευσης (“validated”, [Gloss(00098)]). 
Στην συνέχεια επεξηγούµε κάθε έναν από αυτούς τους δείκτες. Μια εποπτικότερη εικόνα του Πίνακα 
2.11 αποτελεί το σχήµα 2.3. 
Ας θεωρήσουµε προς στιγµήν ότι έχουν καθοριστεί τόσο ο αλγόριθµος όσο και η παράσταση 
του κειµένου. Για µια συγκεκριµένη τιµή του ανύσµατος κειµένου παραµέτρων έστω lπ , ένα πείραµα 
κατηγοριοποίησης επαναλαµβάνεται δέκα φορές µια για κάθε ένα από τα δέκα σύνολα. Έτσι, για την 
 56
Κεφάλαιο 2 Κατηγοριοποίηση κειµένων µε χρήση λέξεων και  εννοιών 
l-στη παράµετρο και το i-στο σύνολο λαµβάνουµε την ακρίβεια κατηγοριοποίησης η οποία 
ορίζεται κατά τον ακόλουθο τρόπο: 
lic
 
=lic Πλήθος σωστά κατηγοριοποιηµένων κειµένων στο i-στο σύνολο κειµένων προς επαλήθευση /  
συνολικό πλήθος των κειµένων τα οποία περιέχονται στο i-στο σύνολο κειµένων προς επαλήθευση 
Στην συνέχεια υπολογίζουµε τους ακόλουθους δείκτες κατηγοριοποίησης λαµβάνοντας τους 
µέσους όρους πάνω σε όλα τα σύνολα κειµένων.  
1) Η καλύτερη απόδοση ως προς ένα σύνολο κειµένων η οποία λαµβάνεται από την καλύτερη τιµή 
της παραµέτρου υπολογίζεται από τον ακόλουθο τύπο: 
∑
= =
=
10
1 ,...,2,1
max max10
1
i Ll
licc  
2) Η χειρότερη απόδοση ως προς ένα σύνολο κειµένων η οποία λαµβάνεται από την χειρότερη τιµή 
της παραµέτρου υπολογίζεται από τον ακόλουθο τύπο: 
∑
= =
=
10
1 ...,2,1
min min10
1
i Ll
licc  
3)  Η  µέση απόδοση ως προς ένα σύνολο κειµένων η οποία λαµβάνεται από τον µέσο όρο όλων των 
τιµών των παραµέτρων υπολογίζεται από τον ακόλουθο τύπο: 
∑
∑
=
==
10
1
1
10
1
i
L
l
li
ave L
c
c  
4) Η συγκεκριµένης τιµής παραµέτρου απόδοση ως προς ένα σύνολο κειµένων η οποία λαµβάνεται 
από µια προκαθορισµένη τιµή παραµέτρων 1lπ η οποία ξέρουµε εµπειρικά ότι επιτυγχάνει καλή 
απόδοση υπολογίζεται από τον ακόλουθο τύπο: 
 ∑
=
=
10
1
110
1
i
ilfix cc  
 57
Κεφάλαιο 2 Κατηγοριοποίηση κειµένων µε χρήση λέξεων και  εννοιών 
5) Η απόδοση επαλήθευσης ως προς ένα σύνολο κειµένων η οποία λαµβάνεται από µια 
επαληθευµένη (“validated”, [Gloss(00098)]) τιµή παραµέτρων 2lπ  υπολογίζεται από τον ακόλουθο 
τύπο: 
∑
=
=
10
1
210
1
i
ilval cc  
 
Οι παραπάνω δείκτες δίνουν µια σχετικά ευρεία αποτίµηση της συµπεριφοράς τόσο των 
λέξεων όσο και των εννοιών ως ανύσµατα κειµένων. Τα αποτελέσµατα της κατηγοριοποίησης 
δίνονται στους πίνακες 2.12-2.17. Τα αποτελέσµατα του αλγορίθµου Κατηγοριοποίησης Μέγιστης 
Ύστερης Πιθανότητας µε την Μαζική και την Αναδροµική παραλλαγή δίδονται στους πίνακες 2.12 
και 2.13 αντίστοιχα. Ο πίνακας 2.14 περιέχει τα αποτελέσµατα του αλγορίθµου Κατηγοριοποίησης 
Μέγιστης Πιθανοφάνειας. Tα αποτελέσµατα του αλγορίθµου K-Πλησιέστερων Γειτόνων για την τρίτη 
και την τέταρτη παραλλαγή δίνονται αντίστοιχα στους πίνακες 2.15 και 2.16 αντίστοιχα. Τέλος ο 
Πίνακας 2.17 παρουσιάζονται τα αποτελέσµατα του αλγορίθµου «σ-FLNMAP µε ψηφοφορία». 
 
Αλγόριθµος Λέξεις Άνυσµα 
Συχνότητας 
Έννοιες Άνυσµα 
Συχνότητας 
Λέξεις ∆υαδική 
Παράσταση 
Έννοιες ∆υαδική 
Παράσταση 
Μέγιστης Ύστερης 
Πιθανότητας(Μαζική 
παραλλ.) 
67.10% 68.80%   
Μέγιστης Ύστερης 
Πιθανότητας(αναδ. Παραλλ.) 
65.40% 65.80%   
Μέγιστης Πιθανοφάνειας 67.19% 68.94%   
Κ-Πλησ.Γειτόνων-3η παραλ. 62.70% 64.60% 71.20% 72.00% 
Κ-Πλησ. Γειτόνων-4η παραλ. 62.40% 65.50% 70.00% 71.60% 
σ-FLΝMAP µε ψηφοφορία 70.65% 70.97% 70.40% 71.51% 
Πίνακας 2.11 Ποσοστό επιτυχίας κατηγοριοποίησης ως προς το κριτήριο του µέσου όρου απόδοσης (για τους 
αλγορίθµους κατηγοριοποίησης που χρησιµοποιούνται) στην 2η σειρά πειραµάτων και των παραστάσεων 
κειµένου που χρησιµοποιεί καθένας από αυτούς. 
 
 58
Κεφάλαιο 2 Κατηγοριοποίηση κειµένων µε χρήση λέξεων και  εννοιών 
 Λέξεις Άνυσµα Συχνότητας Έννοιες Άνυσµα Συχνότητας 
Cmin 59,50% 64.40% 
Cfix 72.20% 71.50% 
Cmax 72.20% 71.90% 
Cave 67.10% 68.80% 
Cval 71.70% 70.30% 
Πίνακας 2.12: Ποσοστό επιτυχίας κατηγοριοποίησης του αλγόριθµου Κατηγοριοποίησης Μέγιστης Ύστερης 
Πιθανότητας (µαζικής παραλλαγής) της 2ης  σειράς πειραµάτων. 
 
 Λέξεις Άνυσµα Συχνότητας Έννοιες Άνυσµα Συχνότητας 
Cmin 60,30% 60.70% 
Cfix 65.90% 67.30% 
Cmax 70.20% 70.00% 
Cave 65.40% 65.80% 
Cval 66.40% 67.10% 
Πίνακας 2.13: Ποσοστό επιτυχίας κατηγοριοποίησης του αλγόριθµου Κατηγοριοποίησης Μέγιστης Ύστερης 
Πιθανότητας (αναδροµικής παραλλαγής) της 2ης  σειράς πειραµάτων. 
 
 Λέξεις Άνυσµα Συχνότητας Έννοιες Άνυσµα Συχνότητας 
Cmin 59,66% 64.57% 
Cfix 67.18% 69.02% 
Cmax 72.20% 72.20% 
Cave 67.19% 68.94% 
Cval 70.66% 70.67% 
Πίνακας 2.14: Ποσοστό επιτυχίας κατηγοριοποίησης του αλγόριθµου Κατηγοριοποίησης Μέγιστης 
Πιθανοφάνειας της 2ης  σειράς πειραµάτων . 
 
 59
Κεφάλαιο 2 Κατηγοριοποίηση κειµένων µε χρήση λέξεων και  εννοιών 
 Λέξεις Άνυσµα 
Συχνότητας  
Έννοιες Άνυσµα 
Συχνότητας 
Λέξεις ∆υαδική 
Παράσταση 
Έννοιες ∆υαδική 
Παράσταση 
Cmin 60.00% 62.40% 68.80% 70.70% 
Cfix 63.70% 65.30% 72.50% 72.40% 
Cmax 65.30% 66.80% 74.10% 73.20% 
Cave 62.70% 64.60% 71.20% 72.00% 
Πίνακας 2.15: Ποσοστό επιτυχίας κατηγοριοποίησης του αλγόριθµου κατηγοριοποίησης Κ Πλησιέστερων 
Γειτόνων  - Τρίτη παραλλαγή της 2ης  σειράς πειραµάτων . 
 
 Λέξεις Άνυσµα 
Συχνότητας 
Έννοιες Άνυσµα 
Συχνότητας 
Λέξεις ∆υαδική 
Παράσταση 
Έννοιες ∆υαδική 
Παράσταση 
Cmin 61.00% 63.90% 69.20% 71.20% 
Cfix 61.90% 64.70% 70.20% 71.50% 
Cmax 63.90% 66.60% 71.20% 72.40% 
Cave 62.40% 65.50% 70.00% 71.60% 
Πίνακας 2.16: Ποσοστό επιτυχίας κατηγοριοποίησης του αλγόριθµου κατηγοριοποίησης Κ Πλησιέστερων 
Γειτόνων  – Τέταρτη  παραλλαγή της 2ης  σειράς πειραµάτων. 
 
 Λέξεις Άνυσµα 
Συχνότητας 
Έννοιες Άνυσµα 
Συχνότητας 
Λέξεις ∆υαδική 
Παράσταση 
Έννοιες ∆υαδική 
Παράσταση 
Cmin 61.35% 61.69% 49.15% 57.62% 
Cfix 74.03% 74.06% 73.38% 74.06% 
Cmax 79.49% 78.81% 78.64% 80.16% 
Cave 70.65% 70.97% 70.40% 71.51% 
Πίνακας 2.17: Ποσοστό επιτυχίας κατηγοριοποίησης του αλγόριθµου κατηγοριοποίησης σ-FLNMAP µε 
ψηφοφορία της 2ης  σειράς πειραµάτων. 
 
Κάθε γραµµή των πινάκων αντιστοιχεί σε µια διαφορετική σκοπιά της απόδοσης των 
κατηγοριοποιητών οι οποίοι βασίζονται στις λέξεις και των κατηγοριοποιητών που βασίζονται στις 
έννοιες.  
 60
Κεφάλαιο 2 Κατηγοριοποίηση κειµένων µε χρήση λέξεων και  εννοιών 
0
20
40
60
80
Λέξεις Άνυσµα Συχνότητας, Έννοιες Άνυσµα Συχνότητας, Λέξεις ∆υαδική Παράσταση, Έννοιες 
∆υαδική Παράσταση
Cmin = χειρότερη απόδοση που λαµβάνεται από τη χειρότερη τιµή παραµέτρου
Cfix = συγκεκριµένης τιµής παραµέτρου απόδοση που λαµβάνεται από µια
προκαθορισµένη τιµή παραµέτρων
Cmax = καλύτερη απόδοση που λαµβάνεται από τη καλύτερη τιµή παραµέτρου
Cave = µέση απόδοση που λαµβάνεται από τον µέσο όρο των τιµών των παραµέτρων
 
Σχήµα 2.3:  Ποσοστό επιτυχίας κατηγοριοποίησης ως προς το κριτήριο του µέσου όρου απόδοσης 
(για τους αλγορίθµους κατηγοριοποίησης που χρησιµοποιούνται) στην 2η σειρά πειραµάτων και των 
παραστάσεων κειµένου που χρησιµοποιεί καθένας από αυτούς. 
 
2.6.2.6 Συµπεράσµατα 
 
Όπως τονίστηκε και στην 1η σειρά πειραµάτων, στόχος των παραπάνω πειραµάτων ήταν η 
εξέταση του οφέλους κατηγοριοποίησης χρησιµοποιώντας τις έννοιες έναντι της κατηγοριοποίησης 
χρησιµοποιώντας τις λέξεις. Από τα πειράµατα τα οποία πραγµατοποιήσαµε καταλήξαµε στα 
ακόλουθα συµπεράσµατα: 
1. Εξετάζοντας τον µέσο όσο απόδοσης κατηγοριοποίησης  παρατηρούµε ότι, οι 
κατηγοριοποιητές οι οποίοι χρησιµοποιούν τις έννοιες υπερισχύουν σε όλες τις περιπτώσεις έναντι 
αυτών που χρησιµοποιούν τις λέξεις. Επιπρόσθετα, η συνολικά καλύτερη απόδοση (ως προς το ίδιο 
πάντα κριτήριο) επιτεύχθηκε από τον αλγόριθµο σ- FLΝMAP µε ψηφοφορία ο οποίος εφαρµόστηκε 
στις έννοιες. Το ίδιο γεγονός παρατηρήθηκε και στην 1
avec
η σειρά πειραµάτων. 
2. Παρά το γεγονός ότι στις περισσότερες φορές (όχι όµως σε όλες) οι έννοιες επιτυγχάνουν καλύτερη 
απόδοση κατηγοριοποίησης συγκριτικά µε τις λέξεις, η διαφορά στην απόδοση είναι οριακή (της 
τάξης του 0.50% εως 2%). Επιπρόσθετα, σε ορισµένες περιπτώσεις είναι χειρότερη. Aυτό συµβαίνει 
αν εξετάσουµε την απόδοση των αλγορίθµων Κατηγοριοποίησης Μέγιστης Ύστερης Πιθανότητας 
τόσο στην Μαζική όσο και στην Αναδροµική Παραλλαγή και σ-FLΝMAP µε ψηφοφορία όταν 
 61
Κεφάλαιο 2 Κατηγοριοποίηση κειµένων µε χρήση λέξεων και  εννοιών 
κάνουµε χρήση του Ανύσµατος Συχνότητας ως προς τον παράγοντα  και την απόδοση του 
αλγορίθµου Κατηγοριοποίησης Μέγιστης Ύστερης Πιθανότητας στην Μαζική Αναδροµική 
Παραλλαγή ως προς τον παράγοντα .  
fixc
valc
3. Εξετάζοντας τα αποτελέσµατα  µπορούµε να δούµε ότι σε µια µόνο από τις εννέα περιπτώσεις 
η διαφορά στην απόδοση κατηγοριοποίησης µεταξύ εννοιών και λέξεων ήταν µεγαλύτερη από 2%. 
Εντελώς αντίστοιχα, εξετάζοντας τα αποτελέσµατα (η οποία είναι ίσως η πιο αντιπροσωπευτική 
πτυχή της απόδοσης κατηγοριοποίησης) µπορούµε να δούµε ότι σε δυο µόνο από τις εννέα 
περιπτώσεις οι λέξεις επιτυγχάνουν καλύτερη απόδοση σε σχέση µε τις έννοιες, στις πέντε από τις 
εννέα περιπτώσεις οι έννοιες επιτυγχάνουν καλύτερη απόδοση σε σχέση µε τις λέξεις αλλά η διαφορά 
στην απόδοση είναι µικρότερη από 2% και µόνο σε µια περίπτωση η διαφορά στην απόδοση µεταξύ 
των εννοιών και των λέξεων είναι µεγαλύτερη από 2% (πιο συγκεκριµένα είναι 2.5%).  
avec
fixc
4. Το συνολικά καλύτερο αποτέλεσµα κατηγοριοποίησης χρησιµοποιώντας τις λέξεις είναι 79.49% 
ενώ χρησιµοποιώντας τις έννοιες 80.16% δηλαδή η διαφορά είναι 0.67% και ελήφθη και στις δυο 
περιπτώσεις από τον αλγόριθµο σ-FLΝMAP µε ψηφοφορία.  
5. Τα συνολικά χειρότερα αποτελέσµατα (ως προς το κριτήριο ) ελήφθησαν από τον αλγόριθµο 
σ- FLΝMAP µε ψηφοφορία χρησιµοποιώντας τη ∆υαδική Παράσταση στις λέξεις των κειµένων. Τα 
συνολικά καλύτερα αποτελέσµατα (ως προς το κριτήριο ) ελήφθησαν από τον ίδιο αλγόριθµο 
χρησιµοποιώντας τη ∆υαδική Παράσταση στις έννοιες  των κειµένων. 
minc
maxc
6. Σε ότι αφορά τους αλγορίθµους Κ-Πλησιέστερων Γειτόνων και σ-FLΝMAP µε ψηφοφορία όπου 
µπορούµε να χρησιµοποιήσουµε τόσο τη ∆υαδική Παράσταση όσο και την παράσταση που βασίζεται 
στη Συχνότητα των λέξεων – εννοιών, από τα παραπάνω πειράµατα διαφαίνεται ότι η ∆υαδική 
Παράσταση υπερισχύει έναντι της παράστασης που βασίζεται στη Συχνότητα των λέξεων – εννοιών  
τόσο για τις λέξεις όσο και για τις έννοιες και στις δυο παραλλαγές του αλγορίθµου Κ-Πλησιέστερων 
Γειτόνων. ∆εν ισχύει όµως το ίδιο και στην περίπτωση του αλγορίθµου σ- FLΝMAP µε ψηφοφορία, 
ιδιαίτερα στην περίπτωση που χρησιµοποιούνται αυτούσιες οι λέξεις των κειµένων. 
7.  Όπως ακριβώς συµβαίνει και στην 1η σειρά πειραµάτων, η χρήση των εννοιών οδηγεί σε οριακή 
βελτίωση στην ακρίβεια της κατηγοριοποίησης. 
 
 Μερικά από τα αποτελέσµατα των πειραµάτων µας παρουσιάζονται επίσης και στα σχήµατα 2.4 
ως 2.11. Σε κάθε ένα από αυτά τα σχήµατα παραθέτουµε την µέση απόδοση κατηγοριοποίησης του 
εκάστοτε αλγορίθµου σαν συνάρτηση της τιµής παραµέτρου δηλαδή παριστούµε γραφικά το 
∑
=
=
10
110
1
i
licc σε συνάρτηση µε την παράµετρο l. Στα σχήµατα 2.4, 2.6, 2.7, 2.8 (που αντιστοιχούν 
στους αλγορίθµους Κατηγοριοποίησης Μέγιστης Ύστερης Πιθανότητας (Μαζική παραλλαγή), 
 62
Κεφάλαιο 2 Κατηγοριοποίηση κειµένων µε χρήση λέξεων και  εννοιών 
Κατηγοριοποίησης Μέγιστης Πιθανοφάνειας, Κ-Πλησιέστερων Γειτόνων - Τρίτη Παραλλαγή και Κ-
Πλησιέστερων Γειτόνων – Τέταρτη Παραλλαγή), η παράµετρος lπ  είναι µονοδιάστατη εποµένως τα 
σχήµατα είναι διδιάστατα. Το σχήµα 2.5 αντιστοιχεί στην αναδροµική παραλλαγή του αλγορίθµου 
Κατηγοριοποίησης Μέγιστης Ύστερης Πιθανότητας η οποία χρησιµοποιεί δυο παραµέτρους a και h. 
Σε αυτήν την περίπτωση παραθέτουµε επίσης ένα διδιάστατο γράφηµα γιατί βρήκαµε ότι η 
παράµετρος h δεν επηρεάζει σηµαντικά την απόδοση της κατηγοριοποίησης. Έτσι δώσαµε µια 
σταθερή τιµή στην παράµετρο h  ίση µε  και παραστήσαµε γραφικά το  συναρτήσει της 
παραµέτρου a. Τα σχήµατα 2.9 - 2.12 αντιστοιχούν στα πειράµατα του αλγορίθµου «σ-FLNMAP µε 
ψηφοφορία». Αξίζει να επαναληφθεί ότι ο εν λόγω αλγόριθµος χαρακτηρίζεται από δυο παραµέτρους 
ρ και . Το σχήµα 2.9 παριστά το  συναρτήσει της παραµέτρου  θεωρώντας σταθερή τιµή για 
την παράµετρο ρ ίση µε 0.94. Το σχήµα 2.10 παριστά το συναρτήσει της παραµέτρου ρ θεωρώντας 
σταθερή τιµή για την παράµετρο  ίση µε 13. Το σχήµα 2.11 είναι ένα τρισδιάστατο σχήµα του  
συναρτήσει και των δυο παραµέτρων ρ και . Το σχήµα 2.12 δείχνει το αποτέλεσµα της εισαγωγής 
των διαφόρων «σ-FLNMAP ψηφοφόρων». Πιο συγκεκριµένα, εκτός από το γεγονός ότι επιτυγχάνεται 
βελτιστοποίηση της απόδοσης κατηγοριοποίησης, ένα σηµαντικό κέρδος από την χρήση των «σ-
FLNMAP ψηφοφόρων» αποτελεί το γεγονός ότι η απόδοση της κατηγοριοποίησης γίνεται πιο 
σταθερή από την αντίστοιχη ενός µεµονωµένου κατηγοριοποιητή η οποία µπορεί να µεταβάλλεται 
σηµαντικά όπως φαίνεται και από το σχήµα 2.9. Επιπρόσθετα, από το σχήµα 2.9 φαίνεται επίσης ότι 
για επιλεγµένες τιµές της παραµέτρου «επαγρύπνησης» ρ, η απόδοση του αλγορίθµου «σ-FLNMAP 
µε ψηφοφορία» µπορεί να είναι λίγο υψηλότερη από την απόδοση κατηγοριοποίησης των 
ανεξάρτητων σ-FLNMAP µονάδων που τον αποτελούν.  
510−
Vn
lc
Vn lc Vn
lc
Vn lc
 
  Από τα σχήµατα 2.4-2.12 καθίσταται ευδιάκριτο ότι για κάθε αλγόριθµο, η καµπύλη η οποία 
αντιστοιχεί στην απόδοση η οποία επιτυγχάνεται µε την χρήση των εννοιών είναι παρόµοια µε την 
αντίστοιχη των λέξεων, µε την πρώτη να ανεβαίνει ελαφρώς αλλά σταθερά πάνω από την δεύτερη.  
 
  Η ακρίβεια κατηγοριοποίησης των έξη αλγορίθµων στα κείµενα προς επαλήθευση κυµαίνεται 
στο εύρος 65-75% ή και λίγο καλύτερα. Η χαµηλή απόδοση κατηγοριοποίησης α) των δύο 
παραλλαγών του αλγορίθµου Κατηγοριοποίησης Μέγιστης Ύστερης Πιθανότητας και β) του 
αλγορίθµου Μέγιστης Πιθανοφάνειας µπορούν να αποδοθούν στην υπόθεση του “Naïve Bayes” που 
αφορά την στατιστική ανεξαρτησία των λέξεων. Οι δύο παραλλαγές του αλγορίθµου K-Πλησιέστερων 
Γειτόνων έδωσαν καλύτερα αποτελέσµατα από τους προηγούµενους αλγορίθµους. Παρόλα αυτά, η 
απόδοσή τους περιορίζεται από το σταθµισµένο άθροισµα (“weighted summation”, [Gloss(00102)]) το 
οποίο πραγµατοποιούν και το οποίο είναι δυνατό να «εξοµαλύνει αρνητικά» (“smooth out”, 
[Gloss(00080)]) χρήσιµους διευκρινιστικούς χαρακτηριστικούς όρους.  Τα συνολικά καλύτερα 
 63
Κεφάλαιο 2 Κατηγοριοποίηση κειµένων µε χρήση λέξεων και  εννοιών 
αποτελέσµατα επιτεύχθηκαν από τον αλγόριθµο «σ-FLNMAP µε ψηφοφορία». Η καλή γενίκευση των 
κειµένων προς επαλήθευση του τελευταίου αλγορίθµου οφείλεται στον υπολογισµό των µέγιστων 
«οµοιόµορφων παραλληλεπιπέδων» από τα δεδοµένα προς εκπαίδευση. Επιπρόσθετα, η 
χρησιµοποίηση περισσότερων από µια µονάδων σ-FLNMAP οδήγησε σε σταθερότητα και σε υψηλή 
απόδοση κατηγοριοποίησης. Αυτό οφείλεται στην εξουδετέρωση του θορύβου που προέρχονται από 
τα δεδοµένα λόγω των διαφόρων αντιµεταθέσεων στα δεδοµένα προς εκπαίδευση τα οποία 
χρησιµοποιήθηκαν για να εκπαιδεύσουν τις διάφορες µονάδες σ-FLNMAP.  
 
 
Αλγόριθµος Μέγιστης Ύστερης Πιθανότητας (Μαζική Παραλλαγή) 
0.00% 
10.00% 
20.00% 
30.00% 
40.00% 
50.00% 
60.00% 
70.00% 
80.00% 
90.00% 
100.00% 
0.25 0.50 0.75 1.00 2.00
α
% 
Aπ
όδ
οσ
η 
Κα
τηγ
ορι
οπ
οίη
ση
ς 
Λέξεις 
Έννοιες 
 
Σχήµα 2.4:  Απόδοση του αλγορίθµου κατηγοριοποίησης  Μέγιστης Ύστερης Πιθανότητας (Μαζική 
παραλλαγή) στο σύνολο κειµένων του Brown Corpus. Στο σχήµα φαίνεται το ποσοστό των σωστά 
κατηγοριοποιηµένων κειµένων προς επαλήθευση και για τις δυο παραστάσεις (µε λέξεις και µε 
έννοιες) συναρτήσει της παραµέτρου α . Η παράσταση βασιζόµενη στις έννοιες υπερισχύει οριακά 
αυτής που βασίζεται στις λέξεις. 
 
 
 
 
 
 
 64
Κεφάλαιο 2 Κατηγοριοποίηση κειµένων µε χρήση λέξεων και  εννοιών 
 
Αλγόριθµος  Μέγιστης  Ύστερης  Πιθανότητας  (Αναδροµ. Παραλ) 
0.00
%
10.00
%
20.00
%
30.00
%
40.00
%
50.00
%
60.00
%
70.00
%
80.00
%
90.00
%
100.00
%
0.25 0.50 0.75 1.00 2.00
α
% 
Απ
όδο
ση  
Κατ
ηγο
ριο
ποί
ηση
ς  
Λέξεις  
Έννοιες 
 
Σχήµα 2.5: Απόδοση του αλγορίθµου κατηγοριοποίησης Μέγιστης Ύστερης Πιθανότητας 
(αναδροµική παραλλαγή) στο σύνολο κειµένων του Brown Corpus. Στο σχήµα φαίνεται το ποσοστό 
των σωστά κατηγοριοποιηµένων κειµένων προς επαλήθευση και για τις δυο παραστάσεις (µε λέξεις 
και µε έννοιες) συναρτήσει της παραµέτρου α  και της παραµέτρου h η οποία λαµβάνει τη σταθερή 
τιµή 10 . Η παράσταση βασιζόµενη στις έννοιες υπερισχύει οριακά αυτής που βασίζεται στις λέξεις. 
Το σχήµα δεν περιλαµβάνει την απόδοση για διαφορετικές τιµές της παραµέτρου h.  
5−
Αλγόριθµος Μέγιστης Πιθανοφάνειας 
0.00
10.00
20.00
30.00
40.00
50.00
60.00
70.00
80.00
90.00
100.00
0.25 0.50 0.75 1.00 2.00
α
% 
Α
π
ό
δ
ο
η
σ
η 
Κ
ατ
ηγ
ο
ρι
ο
π
οί
σ
ης 
Λέξεις 
Έννοιες 
Σχήµα 2.6: Απόδοση του αλγορίθµου Κατηγοριοποίησης Μέγιστης Πιθανοφάνειας στο σώµα 
κειµένων του Brown Corpus. Στο σχήµα φαίνεται το ποσοστό των σωστά κατηγοριοποιηµένων 
 65
Κεφάλαιο 2 Κατηγοριοποίηση κειµένων µε χρήση λέξεων και  εννοιών 
κειµένων προς επαλήθευση και για τις δυο παραστάσεις (µε λέξεις και µε έννοιες) συναρτήσει της 
παραµέτρουα . Η παράσταση που βασίζεται στις έννοιες υπερισχύει οριακά αυτής βάσει των  λέξεων. 
 
Κ-ΠΛΗΣΙΕΣΤΕΡΟΙ ΓΕΙΤΟΝΕΣ – ΤΡΙΤΗ ΠΑΡΑΛΛΑΓΗ 
0.00
10.00
20.00
30.00
40.00
50.00
60.00
70.00
80.00
90.00
100.00
0.25 0.50 1.00 1.50 2.00 3.00 4.00 5.00 10.00 11.00 12.00 13.00 14.00 15.00 
P
% 
Π
ο
σ
ο
στ
ό 
Κ
ατ
ηγ
ο
ρι
ο
π
οί
η
σ
ης 
ΣΛ 
ΣΕ 
∆Λ 
∆Ε 
 
Σχήµα 2.7: Απόδοση του αλγορίθµου κατηγοριοποίησης Κ- Πλησιέστερων Γειτόνων (Τρίτη 
παραλλαγή) στο σώµα κειµένων του Brown Corpus. Στο σχήµα φαίνεται το ποσοστό των σωστά 
κατηγοριοποιηµένων κειµένων προς επαλήθευση και για τις τέσσερις δυνατές παραστάσεις 
(βασιζόµενες στις λέξεις, στις έννοιες, στην δυαδική παράσταση και στην σχετική συχνότητα) 
συναρτήσει της παραµέτρου Ρ. Οι δυαδικές παραστάσεις παρουσιάζουν σηµαντικά καλύτερη απόδοση 
έναντι των αντιστοίχων που βασίζονται στη σχετική συχνότητα, ενώ η παράσταση που βασίζεται στις 
έννοιες υπερισχύει οριακά αυτής που βασίζεται στις λέξεις. 
 
 
 
 66
Κεφάλαιο 2 Κατηγοριοποίηση κειµένων µε χρήση λέξεων και  εννοιών 
 
Κ-Πλησιέστεροι Γείτονες (Τέταρτη Παραλλαγή) 
0.00
10.00
20.00
30.00
40.00
50.00
60.00
70.00
80.00
90.00
100.00
0.250.50 1.00 1.50 2.003.004.005.0010.011.012.013.014.015.0
P
% 
Π
ο
ο
σ
στ
ό 
κα
τη
γο
ρι
ο
π
οί
η
σ
ης 
ΣΛ 
ΣΕ 
∆Λ 
∆Ε 
Σχήµα 2.8:  Απόδοση του αλγορίθµου κατηγοριοποίησης  Κ-Πλησιέστερων Γειτόνων (Τέταρτη 
παραλλαγή) στο σύνολο κειµένων του Brown Corpus. Στο σχήµα φαίνεται το ποσοστό των σωστά 
κατηγοριοποιηµένων κειµένων προς επαλήθευση και για τις τέσσερις δυνατές παραστάσεις 
συναρτήσει της παραµέτρου Ρ. Οι δυαδικές παραστάσεις παρουσιάζουν σηµαντικά καλύτερη απόδοση 
έναντι των αντιστοίχων που βασίζονται στη σχετική συχνότητα, ενώ η παράσταση που βασίζεται στις 
έννοιες υπερισχύει οριακά αυτής που βασίζεται στις λέξεις 
0 
10 
20 
30 
40 
50 
60 
70 
80 
90 
100 
1 2 3 4 5 6 7 8 9 10111213141516171819202122 
nV
 
ΣΛ 
ΣΕ 
∆Λ 
∆Ε 
Αλγόριθµος σ-FLN MAP µε ψηφοφορία 
% 
Πο
σο
στ
ό 
Κα
τη
γο
ρι
ο
π
οί
ησ
ης 
Σχήµα 2.9:  Απόδοση του αλγορίθµου κατηγοριοποίησης  σ-FLN-MAP µε ψηφοφορία στο σύνολο 
κειµένων του Brown Corpus. Στο σχήµα φαίνεται το ποσοστό των σωστά κατηγοριοποιηµένων 
κειµένων προς επαλήθευση συναρτήσει του αριθµού των ψηφοφόρων  και της παραµέτρου 
«επαγρύπνισης» ρ=0.94. Και οι τέσσερις δυνατές παραστάσεις (βάσει των λέξεων, των εννοιών, της 
δυαδικής παρ. και της σχετικής συχνότητας) πρακτικά  παρουσίασαν την ίδια  περίπου απόδοση. 
Vn
 67
Κεφάλαιο 2 Κατηγοριοποίηση κειµένων µε χρήση λέξεων και  εννοιών 
0 
10 
20 
30 
40 
50 
60 
70 
80 
90 
100 
0.8 0.8
1 
0.8
2 
0.8
3 
0.8
4 
0.8
5 
0.8
6 
0.8
7 
0.8
8 
0.8
9 
0.9 0.9
1 
0.9
2 
0.9
3 
0.9
4 
0.9
5 
0.9
6 
0.9
7 
0.9
8 
0.9
9 
1 
ρ
% 
Πο
σο
στ
ό 
Κα
τη
γο
ρι
ο
π
οί
ησ
ης 
ΣΛ 
ΣΕ 
∆Λ 
∆Ε 
Αλγόριθµος σ-FLN MAP µε ψηφοφορία 
Σχήµα 2.10:  Απόδοση του αλγορίθµου κατηγοριοποίησης  σ-FLN-MAP µε ψηφοφορία στο σύνολο 
κειµένων του Brown Corpus. Στο σχήµα φαίνεται το ποσοστό των σωστά κατηγοριοποιηµένων 
κειµένων προς επαλήθευση συναρτήσει του αριθµού των ψηφοφόρων =13 και της παραµέτρου 
«επαγρύπνισης» ρ. Οι παραστάσεις που βασίζονται στη σχετική συχνότητα παρουσιάζουν σηµαντικά 
καλύτερη απόδοση έναντι των αντιστοίχων δυαδικών, ενώ η παράσταση που βασίζεται στις έννοιες 
υπερισχύει οριακά αυτής βασιζόµενη στις λέξεις. 
Vn
 
 
 
 
 
 
 
 
 
 
 
 68
Κεφάλαιο 2 Κατηγοριοποίηση κειµένων µε χρήση λέξεων και  εννοιών 
  
 
 
 
 
 
 
 
 
 
 
 
 
Σχήµα 2.11:  Απόδοση του αλγορίθµου κατηγοριοποίησης  σ-FLN-MAP µε ψηφοφορία στο σύνολο 
κειµένων του Brown Corpus. Στο σχήµα φαίνεται το ποσοστό των σωστά κατηγοριοποιηµένων 
κειµένων προς επαλήθευση συναρτήσει του αριθµού των ψηφοφόρων και της παραµέτρου 
«επαγρύπνισης» ρ για τις παραστάσεις που βασίζονται τόσο στις έννοιες όσο και στις λέξεις. Η 
απόδοση κατηγοριοποίησης παραµένει πρακτικά σταθερή για ρ=0.94 και =13. 
Vn
Vn
0 
5 
10 
15 
20 
25 
30 
35 
40 
45 
50 
55 
60 
65 
70 
75 
80 
85 
90 
95 
100 
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22
Πλήθος Ψηφοφόρων Nv
 
Ανεξ. µον. 
Ψηφοφορί
α 
Αλγόριθµος σ-FLN MAP µε ψηφοφορία % 
Πο
σο
στ
ό 
Κα
τη
γο
ρι
ο
π
οί
ησ
ης 
 
Σχήµα 2.12:  Ποσοστό ακρίβειας κατηγοριοποίησης όλων των ανεξάρτητων µονάδων σ-FLN-MAP 
και του σχήµατος  «σ-FLN-MAP µε ψηφοφορία» για την κατηγοριοποίηση του συνόλου κειµένων του 
Brown Corpus συναρτήσει του αριθµού των ψηφοφόρων  χρησιµοποιώντας τις παραστάσεις που Vn
 69
Κεφάλαιο 2 Κατηγοριοποίηση κειµένων µε χρήση λέξεων και  εννοιών 
βασίζονται τόσο στις έννοιες όσο και στις λέξεις. Το σχήµα  «σ-FLN- µε ψηφοφορία» υποδηλώνει µια 
σταθερή βελτίωση έναντι των ανεξάρτητων µονάδων σ-FLN-MAP των οποίων η απόδοση κυµαίνεται 
σηµαντικά.  
2.7 Αξιολόγηση αποτελεσµάτων 
 
Στο σύνολο των πειραµάτων τα οποία πραγµατοποιήθηκαν και πιο συγκεκριµένα σε ότι 
αφορά τη σύγκριση της απόδοσης µε την χρήση των λέξεων και των εννοιών υποθέσαµε ότι έχουµε 
πλήρη γνώση των εννοιών. Παρόλα αυτά, σε ένα πρακτικό πρόβληµα κατηγοριοποίησης οι έννοιες 
µπορούν να προκύψουν ως αποτέλεσµα της εφαρµογής µιας τεχνικής αποσαφήνισης η οποία, το πιο 
πιθανό, είναι να εισάγει σηµαντικό λάθος. Είναι πιθανό ότι το πλεονέκτηµα στην απόδοση 
κατηγοριοποίησης της τάξεως του 1-2%, το οποίο προκύπτει από τη χρήση των εννοιών όπως 
διαφαίνεται από τα παραπάνω πειράµατα, µπορεί να οφείλεται σε λανθασµένη αποσαφήνιση των 
εννοιών. Παρά το γεγονός ότι τα συµπεράσµατα τα οποία παρουσιάζονται εδώ δεν µπορούν να 
θεωρηθούν οριστικά, η χρήση κατηγοριοποιητών οι οποίοι χρησιµοποιούν τις έννοιες δεν δείχνουν να 
είναι ιδιαίτερα ελκυστικοί έναντι εκείνων που χρησιµοποιούν τις λέξεις σε βαθµό που να καθιστά τη 
χρήση των εννοιών µια ενδιαφέρουσα εναλλακτική λύση έναντι της κατηγοριοποίησης 
χρησιµοποιώντας τις λέξεις. Επιπρόσθετα, τα πειράµατα τα οποία πραγµατοποιήθηκαν κάνοντας 
χρήση των εννοιών των λέξεων δεν αναδεικνύουν σε όλο το εύρος τους τις δυνατότητες των εννοιών. 
Για το λόγο αυτό, οι δυνατότητες των εννοιών εξετάζονται και στο πρόβληµα της τµηµατοποίησης 
κειµένων µε τη βοήθεια του αποτελέσµατος της κατηγοριοποίησης (όπου η κατηγοριοποίηση 
πραγµατοποιείται κάνοντας χρήση τόσο των λέξεων όσο και των εννοιών των κειµένων). Το εν λόγω 
πρόβληµα παρουσιάζεται διεξοδικά στο Κεφάλαιο 4.  Το κεφάλαιο 3 που ακολουθεί περιγράφει 
διεξοδικά το πρόβληµα της τµηµατοποίησης κειµένων και περιγράφει συνοπτικά τους 
σηµαντικότερους αλγορίθµους που εµφανίζονται στην τρέχουσα βιβλιογραφία.  
 
 
 
 
 
 
 70
 
 
 
 
 
 
 
 
 
 
 
71 
Κεφάλαιο 3 Τµηµατοποίηση κειµένων - Βιβλιογραφικές αναφορές 
ΚΕΦΑΛΑΙΟ 3     
ΤΜΗΜΑΤΟΠΟΙΗΣΗ ΚΕΙΜΕΝΩΝ- ΒΙΒΛΙΟΓΡΑΦΙΚΕΣ 
ΑΝΑΦΟΡΕΣ 
3.1  Εισαγωγή 
 
Όπως αναφέρθηκε στα προηγούµενα κεφάλαια, στόχος της παρούσας διατριβής είναι τόσο η 
κατηγοριοποίηση κειµένων µε τη βοήθεια των λέξεων και των εννοιών αυτών, όσο και η 
τµηµατοποίηση µεγάλης έκτασης κειµένων µε χρήση µεθόδων υπολογιστικής νοηµοσύνης, στόχος 
των οποίων αποτελεί η εµβάθυνση στο περιεχόµενο των κειµένων και η ανάδειξη του τρόπου δόµησής 
τους.  Στην ουσία, το πρόβληµα το οποίο καλείται να επιλύσει είναι η βελτίωση της πρόσβασης στην 
πληροφορία (όπως π.χ. αυτή λαµβάνει χώρα στις διάφορες µηχανές αναζήτησης) ούτως ώστε η 
επιστρεφόµενη σε ένα ερώτηµα (“query”, [Gloss(00070)]) του χρήστη πληροφορία να είναι αξιόπιστη 
τόσο σε επίπεδο ακρίβειας ως προς το ζητούµενο θέµα όσο και σε επίπεδο έκτασης. Μιας τέτοιας 
φύσης τεχνολογία είναι απαραίτητη γιατί η διαθέσιµη πληροφορία αυξάνεται εκθετικά µέρα µε τη 
µέρα, δυσχεραίνοντας την προσπάθεια των χρηστών για αναζήτηση µέσα στον τεράστιο όγκο των 
διαθέσιµων κειµένων. Μια τέτοια τεχνολογία θα επέτρεπε στον χρήστη να θέτει ένα ερώτηµα και σαν 
απάντηση να του επιστρέφεται το τµήµα ή τα τµήµατα του κειµένου τα οποία θα απαντούν επακριβώς 
το ερώτηµά του και θα τον αποφόρτιζε από την ανάγνωση του συνολικού κειµένου. Απαραίτητες 
προϋποθέσεις για µια τέτοιου είδους προσέγγιση αποτελούν τόσο η διαίρεση κάθε κειµένου σε 
τµήµατα, όπου τα όρια καθενός από αυτά θα περιορίζονται στο εύρος αναφοράς ενός συγκεκριµένου 
θέµατος, όσο και η εύρεση των ορίων µεταξύ των διαφόρων τµηµάτων τα οποία θα είναι 
ευθυγραµµισµένα σε επίπεδο προτάσεων για µεγαλύτερη ευκρίνεια. Ο κλάδος της Ανάκτησης 
Πληροφορίας (“Information Retrieval”, [Gloss(00037)]) και της Επεξεργασίας ∆εδοµένων (“Data 
Processing”, [Gloss(00019)]) ο οποίος ασχολείται µε την επίλυση τέτοιων προβληµάτων είναι ο 
κλάδος της Τµηµατοποίησης Κειµένου (“Text Segmentation”, [Gloss(00092)]).  
 72
Κεφάλαιο 3 Τµηµατοποίηση κειµένων - Βιβλιογραφικές αναφορές 
3.1.1 Ορισµός του Προβλήµατος   
 
Το πρόβληµα που εξετάζουµε είναι η ανάπτυξη υπολογιστικών µεθόδων για τη βελτίωση στην 
πρόσβαση και στην αναζήτηση πληροφορίας όχι στα αυτούσια κείµενα αλλά σε κείµενα ή τµήµατα 
κειµένων µικρότερης έκτασης τα οποία απαντούν µε ακρίβεια στο ερώτηµα του χρήστη. Και αυτό 
γιατί, ένα κείµενο είναι δυνατό να θίγει περισσότερα από ένα θέµατα ή να θίγει σε µικρότερη ή 
µεγαλύτερη έκταση διάφορα άλλα υποθέµατα τα οποία θα µπορούν να αποτελούν στοιχείο 
αναζήτησης ενός οποιουδήποτε χρήστη.  
 Η αναγκαιότητα τέτοιας φύσης µεθόδων είναι έντονη στις διάφορες Μηχανές Αναζήτησης, όπου 
παρά τον βαθµό εξέλιξής τους σε ότι αφορά τη διατύπωση του ερωτήµατος που θέτει ο χρήστης (µε τη 
µορφή περιορισµών), οι επιστρεφόµενες απαντήσεις, εκτός από το γεγονός ότι είναι πολλές σε 
πλήθος, δεν απαντούν πάντα ευθέως στο ερώτηµα του χρήστη. Αυτό οφείλεται στο γεγονός ότι οι 
υπάρχουσες τεχνικές εξόρυξης πληροφορίας εν δυνάµει και καταχρηστικά θεωρούν ότι ένα κείµενο 
αναφέρεται αποκλειστικά και µόνο σε ένα θέµα, ενώ στην πραγµατικότητα η ζητούµενη απάντηση 
µπορεί να βρίσκεται µέσα σε µια ή δυο παραγράφους ενός µεγάλης έκτασης κειµένου, αναγκάζοντας 
έτσι το χρήστη να προβεί στην ανάγνωση του συνολικού κειµένου. Το ίδιο πρόβληµα εµφανίζεται 
επίσης και σε ακολουθίες ανεξάρτητων τµηµάτων τα οποία εµφανίζονται το ένα µετά το άλλο, όπως 
συµβαίνει π.χ. στις διάφορες ακολουθίες ειδήσεων, όπου το ζητούµενο είναι η εύρεση εκείνων των 
ιστοριών οι οποίες θίγουν το ίδιο θέµα.  
3.1.2 Επίλυση του προβλήµατος  
 
Για την ανάπτυξη µεθόδων όπως αυτή που περιγράφεται στην προηγούµενη παράγραφο, 
στρεφόµαστε στον κλάδο της Τµηµατοποίησης Κειµένου (“Text Segmentation”, [Gloss (00092)]). Με 
το όρο Τµηµατοποίηση ενός Κειµένου εννοούµε την διαίρεση αυτού σε οµοιογενή τµήµατα, καθένα 
από τα οποία αναφέρεται σε ένα συγκεκριµένο θέµα, ενώ συνεχόµενα τµήµατα αντιστοιχούν σε 
διαφορετικά θέµατα. Η βασική θεωρία πάνω στην οποία βασίζονται όλες οι µέθοδοι που συναντώνται 
στη βιβλιογραφία και οι οποίες υπολογίζουν την οµοιογένεια ή ετερογένεια µεταξύ των διαφόρων 
τµηµάτων ενός κειµένου, είναι αυτή των Halliday και Hasan [Halliday & Hasan, 1976], η οποία 
περιγράφεται µε λεπτοµέρεια στην επόµενη ενότητα. 
 Για την πραγµατοποίηση της τµηµατοποίησης ενός κειµένου απαιτείται η επίλυση των 
ακόλουθων προβληµάτων: πρώτα από όλα, απαιτούνται τρόποι εύρεσης και υπολογισµού της 
 73
Κεφάλαιο 3 Τµηµατοποίηση κειµένων - Βιβλιογραφικές αναφορές 
οµοιογένειας ή εναλλακτικά της ετερογένειας µεταξύ των τµηµάτων άρα της δοµής του κειµένου. Το 
στάδιο της εύρεσης της οµοιογένειας µεταξύ τµηµάτων ακολουθεί εκείνο της ανάπτυξης 
υπολογιστικών µεθόδων µε τη βοήθεια των οποίων θα πραγµατοποιείται αυτόµατα η εύρεση των 
ορίων µεταξύ των τµηµάτων. Για την αξιολόγηση του αποτελέσµατος της τµηµατοποίησης 
απαιτούνται, τέλος, τρόποι υπολογισµού του αποτελέσµατος των µεθόδων τµηµατοποίησης, εφόσον 
φυσικά, είναι γνωστές εξαρχής, οι θέσεις των ορίων µεταξύ των διαφόρων τµηµάτων.  
3.2 Θεωρία των Halliday και Hasan 
 
Οι Halliday και Hasan στο βιβλίο τους "Cohesion in English" [Halliday & Hasan,1976], 
υποστηρίζουν ότι κάθε κείµενο διέπεται από δυο αλληλοσυµπληρούµενα στοιχεία: τη συνάφεια και τη 
συνοχή. Οι Halliday και Hasan ορίζουν τη συνοχή ως µια ποιοτική ιδιότητα του κειµένου η οποία 
συνεισφέρει στον συνολικό χαρακτήρα αυτού, ενώ τη συνάφεια ως µια κατάσταση στην οποία όλα τα 
τµήµατα ή ιδέες που αναφέρονται µέσα στο κείµενο ταιριάζουν τόσο καλά µεταξύ τους ώστε να 
σχηµατίζουν µια ολότητα. Η συνάφεια (“coherence ”, [Gloss(00014)]) σχετίζεται µε τη συνέπεια 
στόχου, φωνής, περιεχοµένου ή στυλ και µορφής του λόγου όπως αυτά είχε την πρόθεση να τα 
παραθέσει ο συγγραφέας. Η συνοχή (“cohesion ”, [Gloss(00015)]) αποτελεί µια ποιοτική ιδιότητα του 
κειµένου που προκύπτει π.χ. από την ταυτόχρονη εµφάνιση σηµασιολογικά πανοµοιότυπων λέξεων 
και είναι παρούσα όταν ένα στοιχείο ενός κειµένου ερµηνεύεται καλύτερα υπό το φως ενός 
προηγούµενου (ή σπανιότερα, ενός επόµενου) στοιχείου µέσα στο ίδιο κείµενο. 
 Οι Halliday και Hasan υποστήριξαν πέντε σηµασιολογικές σχέσεις οι οποίες υποδεικνύουν 
γλωσσολογική συνοχή: την επανάληψη µε οµοιότητα, την επανάληψη χωρίς οµοιότητα, την 
επανάληψη µέσω αναφοράς σε ανώτερη κατηγορία στην οποία η προαναφερθείσα οντότητα ανήκει, 
τη συστηµατική σηµασιολογική σχέση - η οποία εµφανίζεται όταν µια λέξη ή µια οµάδα λέξεων έχει 
µια ευκρινώς ορισµένη σχέση µε µια προηγούµενα αναφερόµενη λέξη ή πρόταση-, και τέλος τη µη 
συστηµατική σηµασιολογική σχέση η οποία εµφανίζεται µεταξύ δυο λέξεων ή προτάσεων όταν αυτές 
ανήκουν στο ίδιο θέµα, η φύση όµως της µεταξύ τους σχέσης είναι δύσκολο να οριστεί. 
 Οι παραπάνω σχέσεις αποτελούν ενδείξεις ή αλλιώς κριτήρια συσχέτισης (µεγαλύτερης ή 
µικρότερης) των διαφόρων τµηµάτων ενός κειµένου ενώ ταυτόχρονα σκιαγραφούν τη δοµή αυτού. 
Άλλοι ερευνητές, ακολουθώντας το πρότυπο των Halliday και Hasan, όρισαν διαφορετικά κριτήρια 
για την εύρεση των σηµασιολογικών σχέσεων που διέπουν ένα κείµενο. 'Έτσι π.χ. οι Raskin και 
Weiser [Raskin & Weiser, 1987]  όρισαν ως κριτήρια την επανάληψη και την συγκριτική παράθεση, 
 74
Κεφάλαιο 3 Τµηµατοποίηση κειµένων - Βιβλιογραφικές αναφορές 
όπου το πρώτο σχετίζεται µε την επανάληψη λέξεων ή συνωνύµων αυτών, ενώ το δεύτερο αναφέρεται 
σε λέξεις που έχουν την τάση να εµφανίζονται µαζί µέσα σε ένα κείµενο. 
 Βασιζόµενοι στην παραπάνω θεωρία, οι µετέπειτα ερευνητές ανέπτυξαν διάφορες µεθόδους για 
την εύρεση της δοµής ενός κειµένου οι οποίες διαχωρίζονται σε δυο µεγάλες οικογένειες: τη 
γλωσσολογική και τη στατιστική. Και οι δυο αυτές οικογένειες παρουσιάζονται µε λεπτοµέρειες στις 
ενότητες που ακολουθούν.  
3.3 Γλωσσολογική προσέγγιση εύρεσης της δοµής ενός κειµένου 
 
Η προσέγγιση αυτή επιδιώκει τη χρήση γλωσσολογικών ενδείξεων ή αλλιώς κριτηρίων για την 
εύρεση της δοµής ενός κειµένου. Ως τέτοιες ενδείξεις χρησιµοποιήθηκαν από τους Hirschberg και 
Litman [Hirschberg & Litman, 1993] οι λέξεις και οι προτάσεις «σινιάλο» (“cue words and phrases”, 
[Gloss(00018)]), oι οποίες είναι λέξεις και προτάσεις που χρησιµοποιούνται για να σηµατοδοτήσουν 
τη δοµή του λόγου όπως π.χ. τα επιρρήµατα ή τα ρήµατα ή άλλες λέξεις που υποδηλώνουν την έναρξη 
ενός παραδείγµατος µέσα σε µια πρόταση ή ενός συµπεράσµατος. Λέξεις «σινιάλο» (“cue words”, 
[Gloss(00018)]) θα µπορούσαν να είναι σύνδεσµοι ή και αντωνυµίες (προσωπικές ή αναφορικές). Οι 
εν λόγω λέξεις και οι προτάσεις «σινιάλο» (“cue words and phrases”, [Gloss(00018)]) συλλέχθηκαν 
από διάφορες πηγές οι οποίες ήταν ανεξάρτητες θεµατικής κατηγορίας. Αντίθετα, από τον Reynar 
([Reynar, 1994], [Reynar, 1998], [Reynar & Ratnaparkhi, 1997]) χρησιµοποιήθηκαν συνώνυµα, λέξεις 
και οι προτάσεις «σινιάλο» (“cue words and phrases”, [Gloss(00018)]), οι οποίες εξαρτώνται 
σηµαντικά από τη θεµατική περιοχή ή περιείχαν ακολουθίες λέξεων συγκεκριµένου τύπου όπως για 
παράδειγµα ονόµατα περιοχών και ατόµων.  
Ο Yaari ([Yaari, 1997], [Yaari, 1999]) στην διδακτορική του διατριβή επιτυγχάνει την αναγνώριση 
των σηµαντικότερων θεµάτων που εµφανίζονται σε ένα κείµενο µε τον συνδυασµό δυο 
αλληλοσυµπληρούµενων µεθόδων οι οποίες στοχεύουν στην αναγνώριση: 
1. Επαναλαµβανόµενων ονοµατικών λέξεων οι οποίες εµφανίζονται µαζί κάνοντας χρήση της 
ανάλυσης των ν-γραµµάτων (“n-grams”, [Gloss (00060)]) . 
2. Οριστικής αναφοράς και αλυσίδας κυρίων ονοµάτων χρησιµοποιώντας µεθόδους ανάλυσης 
αναφοράς. 
Ο Yaari χρησιµοποίησε επίσης και την πληροφορία του µέρους του λόγου κάθε λέξης. Παρόµοια 
µε τον Yaari, οι Passonneau και Litman ([Passoneau & Litman, 1993], [Passoneau & Litman, 1994]) 
 75
Κεφάλαιο 3 Τµηµατοποίηση κειµένων - Βιβλιογραφικές αναφορές 
πρότειναν αλγόριθµους τµηµατοποίησης βασιζόµενους σε λέξεις «σινιάλο» (“cue words”, 
[Gloss(00018)]), αναφορικές ονοµατικές προτάσεις και την ύπαρξη παύσεων.  
Οι Kan, Klavans και McKeown [Kan et al., 1998] από την άλλη πλευρά, για την αναγνώριση 
της δοµής του κειµένου, χρησιµοποιούν τις εµφανίσεις των ονοµατικών προτάσεων και των 
αντωνυµιών που εµφανίζονται σε ένα κείµενο. Πιο συγκεκριµένα, θεωρούν ότι οι κύριες ονοµατικές 
προτάσεις, οι κοινές ονοµατικές προτάσεις και οι προσωπικές και κτητικές αντωνυµίες αντανακλούν 
το θεµατικό περιεχόµενο του κειµένου. Ο Kan [Kan, 2001] αργότερα κατασκεύασε ένα σύστηµα, το 
οποίο έκανε χρήση των ακολούθων χαρακτηριστικών: απόσταση µεταξύ αντικειµένων, ειδικά σηµάδια 
όπως επιλογή γραµµατοσειράς, στίξη και ειδικές λέξεις οι οποίες υποδεικνύουν ένα συγκεκριµένο 
στυλ. Από άλλους, τέλος, ερευνητές χρησιµοποιήθηκε η προσωδική πληροφορία, για την περίπτωση 
κυρίως του προφορικού λόγου.  
3.4 Στατιστική προσέγγιση εύρεσης της δοµής ενός κειµένου 
 
Η προσέγγιση αυτή επιδιώκει την εύρεση της δοµής ενός κειµένου µε τη χρήση στατιστικών 
κριτηρίων. Για την εν λόγω εύρεση, οι ερευνητές βασίστηκαν στην παραδοχή ότι η δοµή ενός 
κειµένου είναι είτε γραµµική είτε ιεραρχική. Βασιζόµενοι στη γραµµική ή ιεραρχική παραδοχή της 
δοµής του κειµένου, ανέπτυξαν στη συνέχεια διάφορους αλγόριθµους (οι οποίοι βασίζονταν κυρίως 
στη συχνότητα εµφάνισης των λέξεων µέσα στο κείµενο) και διάφορες τεχνικές για τον υπολογισµό 
της οµοιότητας είτε µεταξύ γειτονικών είτε µεταξύ όλων των τµηµάτων των κειµένων. Οι εν λόγω 
αλγόριθµοι και τεχνικές περιγράφονται στις υποενότητες που ακολουθούν. 
3.4.1 Γραµµική Τµηµατοποίηση Κειµένου  
 
Οι αλγόριθµοι οι οποίοι πραγµατοποιούν γραµµική τµηµατοποίηση βασίζονται στην παραδοχή 
ότι η δοµή του λόγου έχει γραµµική µορφή. Σαν αποτέλεσµα αυτής της παραδοχής, τόσο ο τρόπος µε 
τον οποίο υπολογίζεται η οµοιότητα µεταξύ των διαφόρων τµηµάτων ενός κειµένου όσο και οι 
τεχνικές εύρεσης µεταξύ των ορίων των τµηµάτων είναι γραµµικοί. Στις επόµενες υποενότητες 
περιγράφουµε µε λεπτοµέρεια τόσο τις τεχνικές που χρησιµοποιήθηκαν για την εύρεση των στοιχείων 
εκείνων που υποδηλώνουν συνάφεια µέσα στο κείµενο, όσο και τις τεχνικές βάσει των οποίων 
υπολογίζεται η οµοιότητα µεταξύ µερικών µερών ή όλων των µερών του κειµένου.  
 76
Κεφάλαιο 3 Τµηµατοποίηση κειµένων - Βιβλιογραφικές αναφορές 
3.4.1.1 Εξέταση  της Εµφάνισης των Λέξεων µέσα σε ένα κείµενο 
 
 Για την πλειοψηφία  των ερευνητών, εναρκτήριο σηµείο για την εύρεση της συνάφειας κάθε 
κειµένου αποτέλεσε ο υπολογισµός και η µελέτη της εµφάνισης των λέξεων µέσα στο εξεταζόµενο 
κείµενο. Η σχέση ανάµεσα στην συνάφεια και στην εµφάνιση των λέξεων µέσα σε ένα κείµενο 
µοντελοποιήθηκε από τον κάθε ερευνητή µε διαφορετικό τρόπο. Έτσι, ο Youmans ([Youmans, 1990], 
[Youmans, 1991]) υποστήριξε ότι η πρώτη χρήση λέξεων µέσα σε ένα κείµενο συνήθως συνοδεύει 
αλλαγές στο θέµα δεδοµένου ότι γίνεται αναφορά σε καινούργια πρόσωπα, µέρη και γεγονότα 
οδηγώντας στη χρήση λέξεων οι οποίες δεν εµφανίζονται προηγούµενα µέσα σε αυτό. Εντελώς 
αντίστοιχα, λέξεις οι οποίες επαναλαµβάνονται συχνά αποτελούν ισχυρή ένδειξη για την παρουσία 
του ίδιου θέµατος στο εύρος του κειµένου που εξετάζουµε. Από την άλλη πλευρά ο Philips [Philips, 
1985] εξέτασε τη σχέση µεταξύ των λέξεων που έχουν την τάση να εµφανίζονται µαζί και 
χρησιµοποίησε τα στατιστικά συχνότητας εµφάνισης τέτοιων λέξεων µαζί µε την cluster analysis για 
να αναγνωρίσει δίκτυα λέξεων σε κεφάλαια επιστηµονικών βιβλίων. Η Hearst ([Hearst, 1992], [Hearst 
& Plaunt, 1993],  [Hearst, 1993], [Hearst, 1994(a)], [Hearst,1994(b)]) στη διδακτορική της διατριβή, 
χρησιµοποίησε το παραδοσιακό µοντέλο ανύσµατος χώρου σε επίπεδο προτάσεων, χρησιµοποιώντας 
την συχνότητα εµφάνισης των λέξεων που εµφανίζονται σε καθεµία από αυτές. Ο Yaari ([Yaari, 1997], 
[Yaari, 1999]) στην διδακτορική του διατριβή, εξέτασε την επανάληψη λέξεων (και πιο συγκεκριµένα 
ονοµατικών λέξεων) ή αλυσίδων λέξεων που αναφέρονταν στο ίδιο θέµα.  
Οι Beeferman, Berger και Lafferty από την άλλη πλευρά ([Beeferman et al., 1997(a)], 
[Beeferman et al., 1997(b)], [Beeferman et al., 1999]), κατασκεύασαν ένα στατιστικό µοντέλο το 
οποίο ονόµασαν "µοντέλο επαγωγής χαρακτηριστικών όρων από τυχαία πεδία και εκθετικά µοντέλα". Η 
βασική ιδέα πίσω από την προσέγγιση αυτή ήταν η ανάθεση µιας πιθανοτικής κατανοµής σε µια 
συγκεκριµένη θέση στη ροή των δεδοµένων συνδυάζοντας διαφορετικούς χαρακτηριστικούς όρους και 
αναθέτοντας σε καθένα από αυτούς ένα βάρος σε ένα εκθετικό µοντέλο. Το µοντέλο χρησιµοποίησε 
δυο κατηγορίες χαρακτηριστικών όρων: τους τοπικούς, οι οποίοι χρησιµοποιούσαν προσαρµοσµένα 
γλωσσικά µοντέλα και λέξεις «σινιάλο» (“cue words”, [Gloss(00018)]) χαρακτηριστικούς όρους για 
την ανίχνευση ειδικών λέξεων οι οποίες µπορεί να εξαρτώνται από τη θεµατική περιοχή. 
 Οι Ponte και Croft ([Ponte & Croft, 1997], [Xu & Croft, 1996]) χρησιµοποίησαν την τεχνική 
της Τοπικής Ανάλυσης Περιεχοµένου (“Local Content Analysis ”, [Gloss(00053)]) για την αναγνώριση 
των βασικών αρχών που διέπουν τα κείµενα ή τµήµατα αυτού και τη χρησιµοποίηση των αρχών αυτών 
ως υποκατάστατα των προτάσεων του κειµένου. Υπολόγιζαν στη συνέχεια, την οµοιότητα µεταξύ 
γειτονικών προτάσεων µετρώντας τον αριθµό των κοινών αρχών ανάµεσα στα σύνολα των αρχών που 
εµφανίζονταν σε κάθε πρόταση. Οι Ahonen et al. [Ahonen et al, 1997] πρότειναν την επιλογή µόνο 
 77
Κεφάλαιο 3 Τµηµατοποίηση κειµένων - Βιβλιογραφικές αναφορές 
ενός προκαθορισµένου αριθµού από λέξεις οι οποίες εµφάνιζαν τη µεγαλύτερη συχνότητα (π.χ. σε 
επίπεδο παραγράφου επιλογή περί των 50 λέξεων) και χρησιµοποιούσαν µόνο αυτές για τον 
υπολογισµό της οµοιότητας µεταξύ των παραγράφων (µε τη βοήθεια του συνηµιτόνου).  
 Όλες οι παραπάνω µέθοδοι χρησιµοποιούνταν συχνά στον υπολογισµό της οµοιότητας µεταξύ 
γειτονικών ή όλων των τµηµάτων ενός κειµένου. Οι µηχανισµοί που κατά καιρούς προτάθηκαν στη 
βιβλιογραφία για τον εν λόγω υπολογισµό περιγράφονται στις ενότητες που ακολουθούν.  
3.4.1.2 Μηχανισµοί Εύρεσης της Οµοιότητας µεταξύ γειτονικών τµηµάτων ενός 
κειµένου  
 
Η εν λόγω οικογένεια αλγορίθµων βασίζεται στην παραδοχή ότι οι αλλαγές στην οµοιότητα οι 
οποίες εµφανίζονται εξετάζοντας κάθε φορά ένα µικρό τµήµα του κειµένου µε τα γειτονικά του, 
σηµατοδοτούν την ύπαρξη ορίων άρα και την εµφάνιση διαφορετικών θεµάτων µέσα στο κείµενο. Με 
άλλα λόγια, η εν λόγω οικογένεια αλγορίθµων δεν εξετάζει πιθανές εµφανίσεις του ιδίου θέµατος σε 
διαφορετικά σηµεία του κειµένου.  
 Η προσέγγιση αυτή ακολουθήθηκε από την Hearst ([Hearst, 1992], [Hearst & Plaunt, 1993] 
[Hearst, 1993], [Hearst, 1994(a)], [Hearst, 1994(b)]) η οποία χρησιµοποίησε την τεχνική του 
κινούµενου παραθύρου (“sliding window”, [Gloss(00079)]) και υπολόγιζε την οµοιότητα ενός µπλοκ 
µε τα γειτονικά του. Εδώ ένας σηµαντικός παράγοντας του αλγορίθµου ήταν το µέγεθος του µπλοκ, 
δηλαδή το πλήθος των ψευδοπροτάσεων (οι οποίες αποτελούνταν κατά µέσο όρο από 20 λέξεις) οι 
οποίες το συνιστούσαν. Ως µέγεθος του µπλοκ Κ ορίστηκε ο µέσος όρος των παραγράφων ενός 
κειµένου (µετρούµενο ως προς τις λέξεις). Στα πειράµατα τα οποία πραγµατοποίησε, η οµοιότητα 
µεταξύ µπλοκ υπολογίστηκε µε τη βοήθεια του συνηµιτόνου. 
 Από την άλλη πλευρά οι Richmond, Smith και Amitay [Richmond et al., 1997], αφού µέτρησαν 
τη σηµαντικότητα των λέξεων µε βάση τη συχνότητα εµφάνισής τους µέσα στο κείµενο και την 
απόσταση των επαναλήψεων των λέξεων, καθόρισαν την οµοιότητα µεταξύ γειτονικών περιοχών του 
κειµένου αθροίζοντας τα βάρη των λέξεων οι οποίες εµφανίζονταν και στις δυο περιοχές και στη 
συνέχεια αφαιρώντας τα αθροισµένα βάρη των λέξεων οι οποίες εµφανίζονταν µόνο σε ένα από τα 
δυο τµήµατα. Στη συνέχεια κανονικοποιούσαν διαιρώντας µε τον αριθµό των λέξεων σε κάθε τµήµα. 
 Τέλος ο Choi [Choi, 2000], υπολόγισε την οµοιότητα µεταξύ γειτονικών τµηµάτων κάνοντας 
χρήση της τεχνικής "Σειρά σε Τοπικό Επίπεδο" (“Rank in Local Context”, [Gloss(00071)]), η οποία 
αποτελεί ένα σχήµα ταξινόµησης σε σειρά που βασίζεται στην οµοιότητα που εµφανίζεται σε τοπική 
περιοχή. Πιο συγκεκριµένα, κάθε τιµή του πίνακα οµοιότητας µεταξύ των προτάσεων του κειµένου 
 78
Κεφάλαιο 3 Τµηµατοποίηση κειµένων - Βιβλιογραφικές αναφορές 
(όπου κάθε πρόταση έχει παρασταθεί µε τη βοήθεια του µοντέλου ανύσµατος χώρου) αντικαθίστανται 
από τη σειρά της σε τοπική περιοχή. Η τιµή που λαµβάνει η σειρά είναι ο αριθµός των γειτονικών 
περιοχών οι οποίες εµφανίζουν µικρότερη τιµή οµοιότητας. Για την αποφυγή προβληµάτων 
κανονικοποίησης, κάθε τιµή αντικαθίσταται από το λόγο του πλήθους των στοιχείων τα οποία 
παρουσιάζουν µικρότερη τιµή οµοιότητας δια το πλήθος των εξεταζόµενων στοιχείων.  
3.4.1.3 Μηχανισµοί Εύρεσης της Οµοιότητας µεταξύ όλων των τµηµάτων ενός 
κειµένου  
 
 Η εν λόγω οικογένεια αλγορίθµων, από την άλλη πλευρά, εξετάζει πιθανές εµφανίσεις του ιδίου 
θέµατος σε όλη την έκταση του κειµένου, δηλαδή βασίζεται στην παραδοχή ότι περισσότερα από ένα 
τµήµατα του κειµένου τα οποία απαντώνται σε τυχαία – δηλαδή όχι κατ’ ανάγκη σε συνεχόµενα - 
σηµεία  αυτού, είναι δυνατό να αναφέρονται στο ίδιο θέµα. Έτσι υπολογίζουν την οµοιότητα όλων 
των µερών του κειµένου µεταξύ τους µε σκοπό την εύρεση και τέτοιων επαναλήψεων.  
 Σε αυτή την οικογένεια αλγορίθµων ανήκει και η µέθοδος που ανέπτυξε ο Philips [Philips, 
1985] µε σκοπό την εύρεση της συνολικής δοµής των θεµάτων, χρησιµοποιώντας την cluster analysis 
για να αναγνωρίσει δίκτυα λέξεων σε κεφάλαια επιστηµονικών βιβλίων. Η εν λόγω µέθοδος αρχικά 
εξήγαγε από τα συµπλέγµατα λέξεων που παρήχθησαν ως αποτέλεσµα της cluster analysis, 
"πυρηνικές" λέξεις, τις οποίες θεωρούσε ότι ήταν οι πιο κεντρικές στο κείµενο µε σκοπό την 
αναγνώριση της δοµής των υποθεµάτων. Στη συνέχεια συνέκρινε σύνολα από "πυρηνικές" λέξεις από 
διαφορετικά κεφάλαια και στην περίπτωση που ήταν επαρκώς όµοια σηµείωνε οµοιότητα µεταξύ των 
κεφαλαίων. 
 Ο Choi ([Choi et al., 2001],[Deerwster et al., 1990]) πρότεινε ένα καινούργιο µέτρο για τον 
υπολογισµό της οµοιότητας µεταξύ των προτάσεων το Latent Semantic Analysis (LSA), το οποίο 
αποτελεί µια προσέγγιση κατηγοριοποίησης για την επέκταση του κειµένου. Σε αυτό, η σηµασία µιας 
λέξης παρίσταται µε βάση τη συσχέτισή της µε τις άλλες λέξεις. Η τεχνική LSA εφαρµόζει την 
τεχνική ανάλυσης σε πρωτεύουσες συνιστώσες στον πίνακα οµοιότητας µεταξύ των λέξεων µε σκοπό 
τον προσδιορισµό των καλύτερων χαρακτηριστικών όρων για τη διάκριση ανοµοιογενών λέξεων. Η 
σηµασία κάθε λέξης υπολογίζεται ως το άθροισµα των ανυσµάτων χαρακτηριστικών όρων λέξεων. Η 
οµοιότητα των κειµένων στη συνέχεια, υπολογίζεται µε τη βοήθεια του συνηµιτόνου της γωνίας των 
αντίστοιχων ανυσµάτων χαρακτηριστικών όρων. 
 Οι Utiyama και Isahara [Utiyama & Isahara, 2001] κατασκεύασαν ένα πιθανοτικό µοντέλο το 
οποίο υπολόγιζε την µέγιστη πιθανότητα τµηµατοποίησης για ένα δοσµένο κείµενο. Ο αλγόριθµος 
 79
Κεφάλαιο 3 Τµηµατοποίηση κειµένων - Βιβλιογραφικές αναφορές 
αυτός επέλεγε τη βέλτιστη τµηµατοποίηση βασιζόµενη στην πιθανότητα που οριζόταν από ένα 
στατιστικό µοντέλο. Το στατιστικό µοντέλο υπολόγιζε τις πιθανότητες των λέξεων να ανήκουν σε ένα 
θέµα. Έκαναν την παραδοχή ότι ένα θέµα καθορίζεται από την κατανοµή των λέξεων οι οποίες 
περιέχονται σε αυτό, µε διαφορετικά θέµατα να εµφανίζουν διαφορετική κατανοµή λέξεων.  
 Ο Tony Berber Sardinha ([Sardinha, 1993], [Sardinha, 1999]) προσπάθησε να υπολογίσει την 
οµοιότητα µεταξύ συνδέσµων οι οποίοι εµφανίζονται ανάµεσα στις προτάσεις του κειµένου. Θεώρησε 
ότι ένας σύνδεσµος εµφανίζεται όταν λαµβάνει χώρα επανάληψη µιας λέξης σε δυο διαφορετικές 
προτάσεις. Έτσι εισήγαγε την έννοια του συνόλου συνδέσµων, όπου θεώρησε ότι το σύνολο των 
προτάσεων µε το οποίο συνδέεται µια πρόταση µέσω συνδέσµου (δηλ. έχει κοινές λέξεις) σχηµατίζει 
το σύνολο των συνδέσµων για αυτή. Κατά αυτόν τον τρόπο, η οµοιότητα µέσα στο κείµενο µπορεί να 
καθοριστεί εξετάζοντας το κοινό λεξιλόγιο µεταξύ των προτάσεων του κειµένου. 
 Ο Reynar ([Reynar, 1994], [Reynar, 1998], [Reynar & Ratnaparkhi, 1997]) για τον υπολογισµό 
της οµοιότητας µεταξύ όλων των µερών του κειµένου βασίστηκε στο µοντέλο ανύσµατος χώρου 
κατασκευάζοντας τα αντίστοιχα ανύσµατα για κάθε πρόταση του κειµένου. Στη συνέχεια  υπολόγιζε 
την οµοιότητα κάθε πρότασης µε όλες τις υπόλοιπες, µε τη βοήθεια του συνηµιτόνου. Με αυτόν τον 
τρόπο κατασκεύαζε κάθε φορά έναν τετραγωνικό πίνακα µε τις εν λόγω οµοιότητες τον οποίο 
ονόµασε dotplot. Παρατήρησε ότι η απόδοση όλων των παραµετρικοποιήσεων της εν λόγω τεχνικής 
βελτιωνόταν όταν οι λέξεις αντικαθίστανται από το αντίστοιχό τους  λήµµα και αγνοούνται αυτές που 
ανήκαν στην stop list (στην οποία περιέχονται όλες οι προθέσεις τα άρθρα, κοινότυπα ρήµατα,και 
γενικότερα οι λέξεις που δεν προσδίδουν ιδιαίτερη πληροφορία µέσα σε ένα κείµενο και η οποία 
παρατίθεται στο Παράρτηµα Α1).  
 Οι Bloedorn και Mani [Mani & Bloedorn, 1997] παρουσίασαν µια µέθοδο για τη περίληψη των 
οµοιοτήτων και των διαφορών οι οποίες εµφανίζονταν σε ένα ζεύγος από σχετικά µεταξύ τους κείµενα 
παριστώντας καθένα από αυτά µε τη µορφή γράφου. Οι έννοιες οι οποίες διαφαίνονταν από τις λέξεις, 
τις προτάσεις και τα κύρια ονόµατα µέσα σε ένα κείµενο παριστάνονταν, καθοριζόµενες από τη θέση 
τους, ως κόµβοι του γράφου, ενώ οι ακµές του γράφου αντιστοιχούσαν στις σηµασιολογικές σχέσεις 
µεταξύ εννοιών. Ανάλογα για το ποιο ζεύγος κειµένων επιθυµούσαν να κάνουν την περίληψη, ο 
αλγόριθµός τους στην αρχή χρησιµοποιούσε µια τεχνική ενεργοποίησης για την εύρεση, σε κάθε ένα 
κείµενο, εκείνων των κόµβων οι οποίοι σχετίζονταν µε το ζητούµενο θέµα. Οι ενεργοποιηµένοι κόµβοι 
κάθε κειµένου, στη συνέχεια, συνταιριάζονταν για να οδηγήσουν σε ένα γράφο ο οποίος 
αντιστοιχούσε στις οµοιότητες και διαφορές ανάµεσα στο ζεύγος των κειµένων.  
 
 
 80
Κεφάλαιο 3 Τµηµατοποίηση κειµένων - Βιβλιογραφικές αναφορές 
3.4.2 Ιεραρχική Τµηµατοποίηση Κειµένου 
 
Η ιεραρχική τµηµατοποίηση βασίζεται στην υπόθεση ότι ένα κείµενο έχει ιεραρχική δοµή. Η 
πιο γνωστή ιεραρχική θεωρία του λόγου είναι, αυτή της attentional / intentional δοµής η οποία 
διατυπώθηκε από τους Grosz και Sidner [Grosz & Sidner, 1986] και σύµφωνα µε την οποία η δοµή 
του λόγου συνίσταται από τµήµατα αυτού και µια σχέση εµπέδωσης η οποία υπάρχει µεταξύ τους. Πιο 
συγκεκριµένα, η εν λόγω θεωρία συνίσταται από τρία τµήµατα: την attentional state, τη γλωσσολογική 
δοµή και τη δοµή πρόθεσης (“intentional state”, [Gloss(00041)]). Η attentional state επικεντρώνεται 
στην εστίαση προσοχής των συνοµιλητών και στην πρόσβαση των οντοτήτων του λόγου που 
βρίσκονται σε «περίοπτη θέση» (“salience”, [Gloss(00073)]). Η γλωσσολογική δοµή «συλλαµβάνει» 
τις σχέσεις ανάµεσα σε διαδοχικές λέξεις ή εκφράσεις (“utterances”, [Gloss(00097)]) και διαιρεί το 
κείµενο σε τµήµατα του λόγου. Αυτά τα τµήµατα σχηµατίζουν µια ιεραρχική δοµή. Η γλωσσολογική 
δοµή περιορίζει αλλαγές στην attentional state. Τέλος, η δοµή πρόθεσης (“intentional state”, 
[Gloss(00041)]) µοντελοποιεί τους στόχους και τους υποστόχους των συνοµιλητών. Ο στόχος των 
τµηµάτων του λόγου είναι η πρόθεση η οποία σχετίζεται µε ένα τµήµα του λόγου. Οι Grosz και Sidner 
κατηγοριοποιούν τις συσχετίσεις µεταξύ των προθέσεων οι οποίες είναι δυνατό να αναγνωριστούν 
από τη γλωσσολογική δοµή. Έτσι, όταν µια πρόθεση κυριαρχεί έναντι κάποιας άλλης στη δοµή 
πρόθεσης, τότε η κυριαρχούµενη πρόθεση αντιστοιχεί σε ένα τµήµα του λόγου στη γλωσσολογική 
δοµή η οποία είναι απόγονος του τµήµατος λόγου που σχετίζεται µε την κυριαρχούσα δοµή, 
σχηµατίζοντας έτσι µια ιεραρχική περιγραφή του λόγου.  
 Μια άλλη θεωρία πάνω στην οποία βασίζονται οι τεχνικές που πραγµατοποιούν ιεραρχική 
τµηµατοποίηση είναι η Ρητορική ∆οµή η οποία διατυπώθηκε από τους Mann και Thomson [Mann & 
Thοmson, 1987]. 
 Και οι δυο θεωρίες χρησιµοποιούν προτασιακές µονάδες ως δοµικά µπλοκ. Η θεωρία της 
Ρητορικής ∆οµής είναι µια συναρτησιακή ανάλυση βασιζόµενη σε ένα περιγραφικό µοντέλο, της 
ρητορικής δοµής του κειµένου, σύµφωνα µε την οποία ένα κείµενο χωρίζεται σε προτασιακές 
ενότητες καθεµία από τις οποίες συµµετέχει σε µια σχέση pairwise nucleus satellite. Τα ζεύγη 
συµµετέχουν ως συστατικά στοιχεία µεγαλύτερων ενοτήτων ζευγών, σχηµατίζοντας µια ιεραρχική 
περιγραφή του λόγου.  
 Βασιζόµενοι στις παραπάνω θεωρίες οι Morris και Hirst ([Morris & Hirst, 1991], [Morris, 
1998]) κατασκεύασαν έναν αλγόριθµο τµηµατοποίησης λόγου ο οποίος χώριζε το κείµενο σε τµήµατα 
τα οποία σχηµάτιζαν ιεραρχική δοµή. Το πρώτο βήµα στον αλγόριθµο αποτελούσε η σύνδεση 
ακολουθιών λέξεων - σχετικών µεταξύ τους - µε σκοπό τον σχηµατισµό αλυσίδων. Για την εύρεση 
 81
Κεφάλαιο 3 Τµηµατοποίηση κειµένων - Βιβλιογραφικές αναφορές 
των συσχετίσεων που δήλωναν  λεκτική συνάφεια µεταξύ των λέξεων, χρησιµοποιήθηκε ο θησαυρός 
όρων Roget ([Roget, 1911], [Roget, 1977]). Μετά την αναγνώριση και σχηµατισµό αλυσίδων λέξεων 
στο κείµενο, προέβαιναν στην σύγκριση των στοιχείων των αλυσίδων λέξεων για τον καθορισµό του 
κατά πόσο µια αλυσίδα αποτελεί συνέχεια µιας άλλης η οποία εµφανίστηκε προηγούµενα. Σε τέτοιες 
αλυσίδες έδιναν την ετικέτα "επιστρεφόµενες αλυσίδες" επειδή "επανα-επισκέπτονταν" το θέµα το 
οποίο είχε αναφερθεί από µια άλλη αλυσίδα λέξεων, κατασκευάζοντας µε αυτόν τον τρόπο µια 
ιεραρχία από αλυσίδες λέξεων. 
 Ιεραρχική τµηµατοποίηση πραγµατοποίησε και ο Yaakov Yaari ([Yaari, 1997], [Yaari, 1999]) 
πάνω σε περιγραφικά κείµενα µε τη βοήθεια της τεχνικής του Hierarchical Agglomerative Clustering. 
Αφού αφαίρεσε λέξεις οι οποίες ανήκαν σε µια κλειστή κατηγορία, στη συνέχεια χρησιµοποίησε τον 
αλγόριθµο του Porter [Porter, 1980] για τη µείωση των λέξεων που υπολείπονταν, στη ρίζα τους. Στη 
συνέχεια υπολόγισε την οµοιότητα µεταξύ παραγράφων χρησιµοποιώντας το µέτρο του συνηµιτόνου 
µε το αντίστροφο βάρος συχνότητας εµφάνισης, το οποίο δίνει µεγαλύτερη βαρύτητα σε σπάνιες 
λέξεις του κειµένου παρά σε πολύ συχνές. Η τεχνική του Hierarchical Agglomerative Clustering 
χρησιµοποίησε τις τιµές αυτές της οµοιότητας για την οµαδοποίηση γειτονικών τµηµάτων και πιο 
συγκεκριµένα παραγράφων. Μετά την οµαδοποίηση παραγράφων, ο Yaari δηµιούργησε ένα 
δενδρόγραµµα δείχνοντας τη σειρά µε την οποία πραγµατοποιήθηκαν οι συγχωνεύσεις και κατόπιν, 
εφάρµοσε κανόνες για την εύρεση των ορίων µεταξύ των διαφόρων τµηµάτων µε σκοπό τη µετατροπή 
της ιεραρχικής δοµής σε µια γραµµική τµηµατοποίηση ούτως ώστε να γίνεται ευκολότερη η σύγκριση 
του αποτελέσµατος της τµηµατοποίησης.  
3.4.3 Εννοιολογικό ∆ίκτυο  
 
 Οι Kozima και Furugori ([Kozima, 1993], [Kozima & Furugori, 1993], [Kozima & Furugori, 
1994]) κατασκεύασαν ένα εννοιολογικό δίκτυο χρησιµοποιώντας το αγγλικό λεξικό Longman 
Dictionary of Contemporary English για την παρακολούθηση της συνάφειας µεταξύ των λέξεων. Ο 
Kozima και Furugori χρησιµοποίησαν το εννοιολογικό δίκτυο για να παρέχουν γνώση για εκείνα τα 
ζευγάρια λέξεων τα οποία συνδέονταν εννοιολογικά. Η λεκτική συνάφεια µεταξύ δυο λέξεων 
καθορίστηκε µε τη βοήθεια µιας συνάρτησης η οποία είχε υλοποιηθεί στο εννοιολογικό δίκτυο µε την 
"ενεργοποίηση" ενός κόµβου, ο οποίος αντιστοιχούσε σε µια από τις δυο λέξεις και στη συνέχεια την 
"παρατήρηση της ενεργοποιηµένης τιµής" της άλλης λέξης µετά από έναν ορισµένο αριθµό 
επαναλήψεων "ενεργοποίησης διάδοσης" (“spreading activation”, [Gloss(00081)]) µεταξύ κόµβων.  
 82
Κεφάλαιο 3 Τµηµατοποίηση κειµένων - Βιβλιογραφικές αναφορές 
3.4.4  Τεχνικές Εύρεσης των ορίων µεταξύ των τµηµάτων ενός κειµένου 
 
Το τελικό στάδιο της τµηµατοποίησης αποτελεί ο αυτόµατος τρόπος υπολογισµού της θέσης 
µέσα στο κείµενο των ορίων των διαφόρων τµηµάτων. Αυτός ο τρόπος υπολογισµού είναι συχνά 
ανεξάρτητος από τον τρόπο µε τον οποίο υπολογίστηκε, σε προηγούµενο στάδιο, η οµοιότητα µεταξύ 
των διαφόρων τµηµάτων του κειµένου. Στη διεθνή βιβλιογραφία έχουν προταθεί κατά καιρούς 
διάφοροι τρόποι αυτόµατου υπολογισµού των ορίων µεταξύ των τµηµάτων. Οι σπουδαιότεροι από 
αυτούς περιγράφονται στις υποενότητες που ακολουθούν.  
3.4.4.1 Γραφική Παράσταση  
 
 Η µέθοδος αυτή χρησιµοποιήθηκε από τη Hearst  ([Hearst, 1992], [Hearst & Plaunt, 1993] 
[Hearst, 1993], [Hearst, 1994(a)], [Hearst, 1994(b)]) η οποία κατασκεύαζε µια γραφική παράσταση 
στην οποία παρίστανται οι δείκτες οµοιότητας των διαφόρων τµηµάτων µεταξύ τους. Βάσει αυτής της 
γραφικής παράστασης, η Hearst υπολόγιζε τον "δείκτη βάθους" το οποίο θεωρούσε ως το άθροισµα 
των διαφορών ανάµεσα στην ακµή της κορυφής η οποία εµφανίζεται ακριβώς στα αριστερά και δεξιά 
µιας "κοιλάδας". Ο υπολογισµός του µεγέθους του "δείκτη βάθους" είχε ως εξής: Εκκίνηση από ένα 
συγκεκριµένο διάκενο ανάµεσα σε δυο µπλοκ κειµένου και καταχώρηση του βαθµού οµοιότητας ο 
οποίος σχετιζόταν µε τα µπλοκ τα οποία βρίσκονταν δεξιά και αριστερά του διάκενου αυτού. Στην 
συνέχεια πραγµατοποιούνταν έλεγχος του βαθµού οµοιότητας στο αµέσως επόµενο διάκενο. Αν ήταν 
µεγαλύτερος, συνέχιζε εξετάζοντας το αµέσως επόµενο διάκενο. Ο αλγόριθµος συνέχιζε κατά αυτόν 
τον τρόπο µέχρις ότου βρισκόταν ένας δείκτης οµοιότητας µικρότερος από έναν δείκτη τον οποίο είχε 
προηγούµενα εξετασεί. Στην συνέχεια γινόταν αφαίρεση του δείκτη οµοιότητας του αρχικού διάκενου 
από τον µέγιστο δείκτη οµοιότητας ο οποίος βρέθηκε. Η διαδικασία αυτή επαναλαµβανόταν για τα 
διάκενα µεταξύ µπλοκ τα οποία ήταν επόµενα του πρώτου διάκενου. Οι δείκτες οµοιότητας 
χρειάζονταν να υπολογιστούν µόνο για διάκενα τα οποία ήταν τοπικά ελάχιστα της συνάρτησης 
οµοιότητας.  
 Ο Youmans ([Youmans, 1990], [Youmans, 1991]) χρησιµοποίησε την ίδια λογική. Πιο 
συγκεκριµένα, υποστήριξε ότι τα όρια στη δοµή του λόγου µπορούν να ανιχνευτούν εξετάζοντας τον 
γράφο "∆ιαχείρισης Προφίλ Λεξιλογίου", ο οποίος ήταν ένα γράφηµα του αριθµού των εµφανίσεων 
των καινούργιων λέξεων συναρτήσει της θέσης της εκάστοτε λέξης µέσα στο κείµενο, για απότοµες 
αλλαγές µετά από βαθιές κοιλάδες. Στόχος του, σε ότι αφορά την αναγνώριση ορίων, ήταν η 
 83
Κεφάλαιο 3 Τµηµατοποίηση κειµένων - Βιβλιογραφικές αναφορές 
τοποθέτηση αυτών σε σηµεία στα οποία εκπαιδευόµενοι αναγνώστες της αγγλικής λογοτεχνίας 
πιθανότατα θα τα τοποθετούσαν.  
3.4.4.2 ∆ιαιρετική Οµαδοποίηση   
 
Ο Reynar ([Reynar, 1994], [Reynar, 1998], [Reynar & Ratnaparkhi, 1997]) πρότεινε δυο 
σχετικούς αλγορίθµους – οι οποίοι έφεραν την κοινή ονοµασία «διαιρετική οµαδοποίηση » (“divisive 
clustering”, [Gloss(00026)])  - για την αναγνώριση των ορίων µεταξύ των διαφόρων τµηµάτων µε τη 
µέτρηση της πυκνότητας πάνω στον πίνακα οµοιότητας µεταξύ των προτάσεων του κειµένου 
(dotplot). Η πρώτη τεχνική αναγνώριζε όρια τα οποία µεγιστοποιούσαν την πυκνότητα µέσα σε 
τµήµατα το οποία κείτονταν κατά µήκος της κύριας διαγωνίου του dotplot. Η δεύτερη τεχνική 
αναγνώριζε όρια τα οποία ελαχιστοποιούσαν την πυκνότητα µέσα σε τµήµατα τα οποία κείτονταν 
εκτός της κύριας διαγωνίου του dotplot. ∆ιαισθητικά, η πρώτη τεχνική έβρισκε τµήµατα (όρια 
τµηµάτων) τα οποία µεγιστοποιούσαν την οµοιότητα µεταξύ τους, ενώ η δεύτερη τεχνική έβρισκε 
τµήµατα τα οποία ελαχιστοποιούσαν την οµοιότητα µεταξύ τους.  
3.4.4.3 Γράφος 
 
Οι Utiyama και Isahara [Utiyama & Isahara, 2001], για την εύρεση των ορίων µεταξύ των 
τµηµάτων, κατασκεύασαν ένα γράφο στο οποίο οι κόµβοι παρίσταναν τις λέξεις του κειµένου, ενώ οι 
ακµές αντιστοιχούσαν στις συνδέσεις µεταξύ των λέξεων. Έχοντας την πληροφορία των πιθανοτήτων 
των λέξεων να ανήκουν σε ένα θέµα, προσπαθούσαν να ελαχιστοποιήσουν το κόστος 
τµηµατοποίησης, δηλαδή να βρουν το µικρότερο µονοπάτι στο οποίο οι ακµές αντιστοιχούσαν στα 
τµήµατα. Το µικρότερο µονοπάτι ήταν αυτό το οποίο ελαχιστοποιούσε το κόστος τµηµατοποίησης. Ο 
εν λόγω αλγόριθµος καθόριζε επίσης αυτόµατα και τον αριθµό των τµηµάτων. 
3.4.4.4 Αλγόριθµοι Μηχανικής Μάθησης  
 
Ο Yaari ([Yaari, 1997],[Yaari, 1999]), στη διδακτορική του διατριβή, εφάρµοσε έναν 
αλγόριθµο Μηχανικής Μάθησης για τον καθορισµό των ορίων του λόγου. Συµπληρωµατικά µε τη 
λεκτική εγγύτητα, η εν λόγω µέθοδος χρησιµοποιούσε τόσο κατά τη διάρκεια της εκπαίδευσης όσο 
και κατά την διάρκεια της εξέτασης, ως χαρακτηριστικά γνωρίσµατα την πληροφορία των αλυσίδων 
 84
Κεφάλαιο 3 Τµηµατοποίηση κειµένων - Βιβλιογραφικές αναφορές 
λέξεων, ξεχωριστές λέξεις (λέξεις «σινιάλο» (“cue words”, [Gloss(00018)]) καθώς και διάφορα 
συντακτικά γνωρίσµατα. Τα παραπάνω χαρακτηριστικά γνωρίσµατα δίνονταν ως είσοδος στο 
σύστηµα µηχανικής µάθησης RIPPER το οποίο αναγνώριζε τα όρια των µερών του λόγου, άρα και 
των διαφόρων τµηµάτων. 
 Εντελώς αντίστοιχα, οι Beeferman, Berger και Lafferty [Beeferman et al., 1999] πρότειναν ένα 
µοντέλο το οποίο δεχόταν ως είσοδο γλωσσικά µοντέλα και ανίχνευε πανοµοιότυπες θέσεις αλλαγής 
θέµατος µέσα σε ένα κείµενο. Για να βοηθηθεί σε µια τέτοιου είδους ανίχνευση, το µοντέλο 
συµβουλευόταν ένα σύνολο από λεκτικές ενδείξεις τις οποίες είχε µάθει να συσχετίζει µε την 
παρουσία ενός ορίου µεταξύ τµηµάτων σαν αποτέλεσµα της µελέτης ενός µεγάλου σώµατος κειµένων 
τα οποία αποτελούνταν από δεδοµένα που περιείχαν σχολιασµό.  
3.4.4.5 ∆υναµικός Προγραµµατισµός  
  
Ο αλγόριθµος δυναµικού προγραµµατισµού χρησιµοποιήθηκε τόσο από τον Heinonen 
([Heinonen, 1998]), όσο και από τους  Ponte και Croft ([Ponte & Croft, 1997], [Xu & Croft, 1996]) 
για την εύρεση των ορίων µεταξύ των τµηµάτων. Πιο συγκεκριµένα, στον αλγόριθµο τους οι Ponte 
Croft, όταν λάµβαναν µια ταξινοµηµένη σειρά τµηµάτων µεγέθους Ν, σε κάθε σηµείο του κειµένου το 
οποίο είχε δοθεί ως είσοδος, δηλαδή για κάθε µπλοκ µεγέθους Ν, υπολόγιζαν  έναν δείκτη για κάθε 
µπλοκ από το Ρ ως το Ρ+Ν για κάθε θέση Ρ της ακολουθίας εισόδου. Η αριθµηµένη θέση (“rank 
position”, [Gloss(00072)]) καθενός τµήµατος χρησιµοποιήθηκε ως ένδειξη ότι ένα τµήµα σε µια 
συγκεκριµένη θέση είναι µεγέθους Ν. Σε αυτό το σηµείο χρησιµοποιήθηκε δυναµικός 
προγραµµατισµός για να ληφθεί υπόψη κάθε δυνατή τµηµατοποίηση. Στον εν λόγω αλγόριθµο, 
χρησιµοποιήθηκε το Gaussian µοντέλο µήκους για την απόδοση βάρους σε κάθε πιθανό τµήµα σε 
συνάρτηση µε την προγενέστερη πιθανότητα του µήκους τµήµατος. Tο µοντέλο µήκους (“length 
model”, [Gloss(00050)]) ορίστηκε ως ακολούθως: 
2))(
2
1
(
*
2
σ
µ
σπ
−
−
x
ek           (3.1) 
όπου µ ήταν το εκτιµούµενο µέσο µήκος τµήµατος, σ ήταν η εκτιµούµενη τυπική απόκλιση και k ήταν 
µια σταθερά η οποία χρησιµοποιούνταν για σκοπούς κλιµάκωσης. Οι παράµετροι εκτιµήθηκαν από 
ένα  σώµα κειµένων προς εκπαίδευση το οποίο αποτελούνταν από 85 τµήµατα.  
Από την άλλη πλευρά ο Heinonen [Heinonen, 1998] υπολόγιζε την οµοιότητα µεταξύ όλων 
των τµηµάτων του κειµένου. ∆οθείσης αυτής της πληροφορίας και του µήκους των παραγράφων 
 85
Κεφάλαιο 3 Τµηµατοποίηση κειµένων - Βιβλιογραφικές αναφορές 
προσπαθούσε να βρει τοπικά ελάχιστα στην καµπύλη οµοιότητας και θεωρούσε ότι αυτά αποτελούν 
και υποψήφια όρια. Ο αλγόριθµος ο οποίος ανέπτυξε λειτουργεί ως εξής: Ξεκινάµε µε το πρώτο όριο 
και υπολογίζουµε το κόστος για αυτό σαν να ήταν η πρώτη παράγραφος ένα µοναδικό τµήµα. Στην 
συνέχεια παίρνουµε το δεύτερο όριο και επισυνάπτουµε σε αυτό το ελάχιστο των παρακάτω δυο 
διαθέσιµων δυνατοτήτων: το κόστος το οποίο προκύπτει αν θεωρήσουµε ότι οι δυο πρώτες παράγραφοι 
αποτελούν ένα και µοναδικό τµήµα και το κόστος του να αποτελεί η δεύτερη παράγραφος ένα ξεχωριστό 
τµήµα. Στα βήµατα τα οποία ακολουθούν, η εκτίµηση προχωράει κατά µια παράγραφο κάθε φορά και 
λαµβάνονται υπόψη όλες οι πιθανές θέσεις των προηγούµενων σηµείων. H διαδικασία αυτή συνεχίζεται 
µέχρι το τέλος του κειµένου και στο τέλος δηµιουργείται µια λίστα από σηµεία τα οποία υποδεικνύουν 
την τµηµατοποίηση. Το κόστος για κάθε ένα όριο αποτελεί συνδυασµό τριών στοιχείων: του κόστους του 
µήκους του τµήµατος , του κόστους cost[.] και την οµοιότητας sim[.] κάποιου προηγούµενου ορίου. 
Η συνάρτηση κόστους δίνει την χαµηλότερη τιµή κόστους για το επιλεγόµενο µήκος τµήµατος το 
οποίο δίνεται από τον χρήστη. Ένα τµήµα κειµένου το οποίο είναι είτε µεγαλύτερο σε µήκος είτε 
µικρότερο λαµβάνει µεγαλύτερο κόστος δηλαδή «τιµωρείται»  για το µήκος του.  
lenc
lenc
Ο Heinonen πειραµατίστηκε µε δυο οικογένειες συναρτήσεων κόστους. Η πρώτη ήταν η 
οικογένεια συναρτήσεων δευτέρου βαθµού (παραβολικές συναρτήσεις):  
  





+−= 111*),,( 22 xp
x
p
hhpxclen            (3.2) 
και η δεύτερη αυτή των V-shape γραµµικών συναρτήσεων: 
  





−∗= 1),,(
p
xhhpxclen              (3.3) 
όπου x είναι το πραγµατικό µήκος ενός τµήµατος, p είναι το επιλεγόµενο µήκος τµήµατος το οποίο 
δίνεται από τον χρήστη και h είναι µια παράµετρος κλιµάκωσης (“scaling parameter”, 
[Gloss(00076)]) η οποία µας επιτρέπει να ρυθµίζουµε το βάρος µε βάση το µήκος του τµήµατος. Όσο 
µικρότερη είναι η τιµή του h τόσο λιγότερο βάρος δίνεται στο επιλεγόµενο από τον χρήστη µήκος 
τµήµατος συγκριτικά µε το µέτρο της οµοιότητας.  
3.4.4.6 Απόδοση Συµπίεσης  
 
 Ο Reynar ([Reynar, 1994], [Reynar, 1998], [Reynar & Ratnaparkhi, 1997]) στη διδακτορική του 
διατριβή ανέπτυξε έναν αλγόριθµο ο οποίος παρακολουθούσε την απόδοση συµπίεσης ως ένδειξη της 
 86
Κεφάλαιο 3 Τµηµατοποίηση κειµένων - Βιβλιογραφικές αναφορές 
αλλαγής θέµατος, δεδοµένου ότι η οµοιότητα µεταξύ τµηµάτων κειµένου τα οποία αναφέρονται στο 
ίδιο θέµα πρέπει να είναι µεγαλύτερη. Η εν λόγω απόδοση συµπίεσης µετρήθηκε ως ο λόγος του 
µεγέθους του αρχικού κειµένου διαιρούµενο µε το µέγεθος του συµπιεσµένου κειµένου. Η 
αναγνώριση των ορίων µεταξύ των τµηµάτων έγινε µε την εύρεση των σηµείων εκείνων όπου 
ελαχιστοποιούνταν ο λόγος συµπίεσης, δεδοµένου ότι σε εκείνα τα σηµεία το συµπιεζόµενο κείµενο 
είναι στον ελάχιστο βαθµό όµοιο µε το προηγούµενο κείµενο. Για τη συµπίεση του κειµένου 
χρησιµοποιήθηκε ο αλγόριθµος LZ77.  
3.4.4.7 Hidden Markov Models  
 
Οι Mittendorf και Schuble [Mittendorf & Schuble, 1996] και αργότερα οι Blei και Moreno [Blei 
& Moreno, 2001] για την τµηµατοποίηση κατασκεύασαν ένα segmenting aspect Hidden Markov 
Model (AHMM). Το ΑΗΜΜ για τµηµατοποίηση είναι ένα ΗΜΜ στο οποίο η κάθε κρυφή κατάσταση 
αντιστοιχεί στο θέµα και η z τυχαία µεταβλητή αντιστοιχεί στο µοντέλο προς εκπαίδευση. Γενικότερα, 
το ΑΗΜΜ λειτουργεί ακριβώς µε τον ίδιο τρόπο µε το ΗΜΜ µε τη µόνη διαφορά ότι οι λέξεις από τον 
επιλεγόµενο κρυφό παράγοντα, παράγονται µε τη βοήθεια του aspect µοντέλου αντί µε ανεξάρτητο 
τρόπο. Για την εκπαίδευση του ΑΗΜΜ, εκπαιδεύεται ένα µοντέλο πάνω σε ένα σύνολο τµηµάτων 
προς εκπαίδευση και στη συνέχεια πραγµατοποιείται συσσωµάτωση (“clustering”, [Gloss(00013)]) 
των εκπαιδευόµενων τµηµάτων. Τέλος υπολογίζονται οι πιθανότητες µετάβασης ανάµεσα στις οµάδες 
(“clusters”, [Gloss(00012)]) και στις αρχικές πιθανότητες κάθε cluster. Το ΑΗΜΜ τµηµατοποιεί ένα 
νέο κείµενο διαιρώντας τις λέξεις του σε  παράθυρα παρατήρησης µεγέθους L λέξεων και 
εφαρµόζοντας τον αλγόριθµο Viterbi για την εύρεση της πιο πιθανής ακολουθίας κρυφών θεµάτων τα 
οποία παράγουν το δοσµένο κείµενο. Η αλλαγή θέµατος µέσα σε ένα κείµενο γίνεται ορατή όταν η 
τιµή µιας µεταβλητής θέµατος αλλάζει κατά τη µετάβαση από ένα παράθυρο σε ένα άλλο.  
Ο Yamron ([Yamron et al., 1998]) χρησιµοποίσε επίσης την τεχνική των ΗΜΜ για την 
τµηµατοποίηση ακολουθιών ειδήσεων βασιζόµενος στην παραδοχή ότι κάθε ιστορία αποτελεί ένα 
στιγµιότυπο ενός συνεπαγόµενου θέµατος. Στο µοντέλο που ανέπτυξε, η εύρεση των ορίων µεταξύ 
των ιστοριών ήταν ισοδύναµη µε την εύρεση της αλλαγής του θέµατος. Το εν λόγω µοντέλο 
υλοποιήθηκε µε τη βοήθεια τεχνικών ΗΜΜ, στις οποίες οι κρυφές καταστάσεις αντιστοιχούσαν στα 
θέµατα ενώ οι παρατηρήσεις αντιστοιχούσαν είτε σε λέξεις είτε σε προτάσεις, µε σκοπό την εύρεση 
της καλύτερης υπόθεσης άρα και της αντίστοιχης βέλτιστης τµηµατοποίησης. Πιο συγκεκριµένα, ο 
Yamron κατασκεύασε ένα γράφο όπου κάθε κόµβος αντιστοιχούσε σε ένα unigram µοντέλο γλώσσας 
και σε κάθε λέξη ή πρόταση του κειµένου, και έβρισκε το καλύτερο µονοπάτι κατά µήκος του δικτύου. 
Ο δείκτης του κάθε κόµβου ήταν ο δείκτης της πρότασης του ως προς το γλωσσολογικό µοντέλο 
λαµβάνοντας υπόψη ένα κόστος για τις µεταβάσεις µεταξύ κόµβων από ένα θέµα σε ένα άλλο.  
 87
Κεφάλαιο 3 Τµηµατοποίηση κειµένων - Βιβλιογραφικές αναφορές 
3.5 Μέτρα αξιολόγησης της τµηµατοποίησης 
 
Για τη µέτρηση του αποτελέσµατος της τµηµατοποίησης προτάθηκαν κατά καιρούς στη 
βιβλιογραφία διάφορα κριτήρια. Πριν την ανάπτυξη αυτοµατοποιηµένων υπολογιστικών µεθόδων 
χρησιµοποιήθηκαν κριτές οι οποίοι µετά την ανάγνωση του κειµένου προς τµηµατοποίηση 
προσδιόριζαν, κατά την κρίση τους, τα όρια µεταξύ των τµηµάτων. ∆εδοµένου ότι αυτός το τρόπος 
ήταν καθαρά υποκειµενικός, έπρεπε να βρεθούν τρόποι µέτρησης της συµφωνίας µεταξύ των 
σχολιαστών. O Reynar ([Reynar, 1994], [Reynar, 1998], [Reynar & Ratnaparkhi, 1997]) στην 
διδακτορική διατριβή υποστήριξε ότι, είναι δύσκολο και χρονοβόρο να σχολιάζει κανείς σώµατα 
κειµένων καθορίζοντας σε αυτά τα τµήµατα τα οποία αντιστοιχούν σε διαφορετική θεµατική 
κατηγορία. Ακόµα και όταν έχουν δοθεί πολύ συγκεκριµένες οδηγίες στους σχολιαστές, είναι απίθανο 
αυτοί να συµφωνήσουν οµόφωνα σχετικά µε την θέση των ορίων των διαφόρων θεµάτων µέσα στο 
κείµενο ([Hirschberg & Litman,1993], [Passonneau & Litman,1993]). Μετά τον σχολιασµό µιας 
συλλογής κειµένων, το πρόβληµα εξακολουθεί να υφίσταται στον καθορισµό του κατά πόσο οι 
σχολιασµοί είναι σύµφωνοι µεταξύ τους ούτως ώστε να χρησιµοποιηθούν.  
Η Carletta [Carletta,1994] εισήγαγε το µέτρο kappa-statistic, το οποίο εφαρµόζεται ευρέως 
στον τοµέα της ανάλυσης περιεχοµένου (“content analysis”, [Gloss(00017)]) για την µέτρηση της 
συµφωνίας µεταξύ των σχολιαστών (“inter-annotator agreement ”, [Gloss(00042)]). To µέτρο kappa-
statistic ορίζεται από την σχέση: 
   
)(1
)()(
EP
EPAPK
−
−
=            (3.4) 
Όπου Ρ(Α) είναι κλάσµα των φορών που οι σχολιαστές συµφωνούν µεταξύ τους και Ρ(Ε) είναι 
κλάσµα των φορών που οι σχολιαστές αναµένεται να συµφωνήσουν µεταξύ τους τυχαία. Στον τοµέα 
της ανάλυσης περιεχοµένου, όταν η τιµή του µέτρου kappa-statistic είναι µεγαλύτερη από 0.80 τότε 
θεωρούµε ότι έχουµε υψηλή αξιοπιστία ενώ όταν η τιµή του µέτρου kappa-statistic κυµαίνεται µεταξύ 
0.67 και 0.80 τότε θεωρούµε ότι έχουµε αβέβαιη αξιοπιστία. Τέλος όταν η τιµή του µέτρου kappa-
statistic είναι µικρότερη από 0.67 θεωρούµε ότι ο σχολιασµός που έγινε από τους διάφορους 
σχολιαστές δεν είναι αξιόπιστος.  
 Αργότερα οι ερευνητές στράφηκαν προς τα γνωστά από τη θεµατική περιοχή της Ανάκτησης 
Πληροφορίας (“Information Retrieval ”, [Gloss(00037)]) Precision και Recall. Για το πρόβληµα της 
τµηµατοποίησης κειµένων, το µέτρο του Precision ορίστηκε ως «το πλήθος των εκτιµώµενων ορίων 
που είναι πραγµατικά όρια, διαιρούµενο προς το συνολικό πλήθος των εκτιµώµενων ορίων». Εντελώς 
 88
Κεφάλαιο 3 Τµηµατοποίηση κειµένων - Βιβλιογραφικές αναφορές 
αντίστοιχα, το µέτρο του Recall ορίστηκε ως «το πλήθος των εκτιµώµενων ορίων που είναι 
πραγµατικά όρια διαιρούµενα δια του συνολικού πλήθους των πραγµατικών ορίων». Οι τιµές των 
Precision και Recall µπορούν να διαµορφώνονται η µια σε βάρος της άλλης. Αυτό σηµαίνει ότι οι 
αλγόριθµοι µπορούν να ρυθµιστούν κατά τέτοιο τρόπο ούτως ώστε να επιτυγχάνουν υψηλή απόδοση 
στο Recall αλλά ταυτόχρονα χαµηλή στο Precision. Ένα επιπρόσθετο µειονέκτηµα των κριτηρίων 
Precision και Recall σε ότι αφορά το πρόβληµα της τµηµατοποίησης κειµένων είναι το ότι, κάθε 
λανθασµένα εκτιµώµενο όριο µεταξύ τµηµάτων «τιµωρείται ισάξια» ανεξάρτητα αν αυτό είναι κοντά 
ή όχι στο πραγµατικό όριο. Αυτό σηµαίνει ότι για την πλήρη γνώση της απόδοσης ενός αλγορίθµου 
χρειαζόµαστε ένα γράφο των Precision και Recall ή εναλλακτικά ένα µέτρο που να συνδυάζει τις δύο 
αυτές ποσότητες. Στην βιβλιογραφία του τοµέα Ανάκτησης Πληροφορίας εµφανίζονται διάφορες 
εναλλακτικές λύσεις όπως αυτές του γινοµένου των Precision και Recall και το F-measure. 
 Οι Βeeferman, Berger και Lafferty ([Βeeferman et al., 1999]) πρότειναν ένα καινούργιο µέτρο 
υπολογισµού της απόδοσης τµηµατοποίησης το οποίο επιλύει τα παραπάνω προβλήµατα και το οποίο 
µετρά την ανακρίβεια  της τµηµατοποίησης. Το εν λόγω µέτρο δίνει περισσότερο βάρος στις 
περιπτώσεις όπου ένα υποτιθέµενο όριο είναι πραγµατικά σωστό από ότι σε αυτές που είναι λάθος, 
ενώ στο τέλος αποδίδει µια και µοναδική τιµή. ∆ιαισθητικά το εν λόγω µέτρο υπολογίζει το ποσοστό 
των προτάσεων οι οποίες έχουν λανθασµένα προβλεφθεί ότι ανήκουν στο ίδιο τµήµα (ενώ στην 
πραγµατικότητα ανήκουν σε διαφορετικά τµήµατα) ή των προτάσεων οι οποίες λανθασµένα 
προβλέφθηκαν ότι ανήκουν σε διαφορετικά τµήµατα (ενώ στην πραγµατικότητα ανήκουν στο ίδιο 
τµήµα). Πιο συγκεκριµένα, δοθέντος ενός κειµένου το οποίο αποτελείται από Τ προτάσεις για κάθε s,t 
= 1,2, ….T, οι ποσότητες ),( tstruδ και ),( tshypδ  ορίζονται κατά τον ακόλουθο τρόπο: 
 
=),( tstruδ  1, αν και µόνο αν οι προτάσεις s, t ανήκουν στο  
ίδιο τµήµα στην πραγµατική τµηµατοποίηση, 
0, αλλιώς 
(3.5α)  
=),( tshypδ  1, αν και µόνο αν οι προτάσεις s, t ανήκουν στο  
ίδιο τµήµα στην υποθετική τµηµατοποίηση, 
0, αλλιώς 
(3.5β) 
  
 89
Κεφάλαιο 3 Τµηµατοποίηση κειµένων - Βιβλιογραφικές αναφορές 
Οι Βeeferman, Berger και Lafferty εισήγαγαν µια συνάρτηση (s,t = 1,2,…, T) η οποία 
είναι µια πιθανοτική κατανοµή πάνω στο σύνολο των τυχαία επιλεγµένων ζευγών προτάσεων s,t του 
κειµένου. Στη συνέχεια όρισαν την ποσότητα     
t)D(s,
)),(),((1),(
1
tststsDP hyptru
Tts
k δδ ≠⋅= ∑
≤≤≤
         (3.6) 
Όπου 1 αποτελεί µια συνάρτηση η οποία ισούται µε 1 εφόσον )( ba ≠ ba = και 0 αλλιώς. 
 Υποθέτουµε τώρα ότι µας δίνεται ένα κείµενο, η πραγµατική καθώς και η υποθετική 
τµηµατοποίηση αυτού. Από τον παραπάνω ορισµό προκύπτει ότι το  είναι η πιθανότητα δυο τυχαία 
επιλεγόµενων προτάσεων να είναι «λανθασµένα τµηµατοποιηµένες» δηλαδή είτε (α) η υποθετική-
εκτιµώµενη τµηµατοποίηση υποστηρίζει λανθασµένα ότι οι δυο προτάσεις ανήκουν στο ίδιο τµήµα 
είτε (β) η υποθετική- εκτιµώµενη τµηµατοποίηση υποστηρίζει λανθασµένα ότι ανήκουν σε 
διαφορετικά τµήµατα. Η πιθανότητα  εξαρτάται από την πιθανότητα D(s,t) επιλογής δυο 
συγκεκριµένων προτάσεων και ισούται µε µηδέν µόνο όταν η πραγµατική και η υποθετική- 
εκτιµώµενη κατανοµή είναι ταυτόσηµες.  
kP
kP
 Ιδιαίτερα ενδιαφέρουσα αποτελεί η περίπτωση όπου η D(s,t) εξαρτάται αποκλειστικά από την 
απόσταση µεταξύ δυο προτάσεων (οι οποίες επιλέγονται τυχαία από το κείµενο προς τµηµατοποίηση) 
δηλαδή έχει τη µορφή:  
            (3.7) ||),( tsetsD −−∗= µµµ γ
Η  είναι µια εκθετική κατανοµή µε µέση τιµή 1/µ η οποία εκφράζει την πιθανοτική 
κατανοµή απόστασης (“distance probability distribution”, [Gloss(00025)]) πάνω στο σύνολο όλων 
των δυνατών αποστάσεων µεταξύ προτάσεων οι οποίες επιλέγονται τυχαία από το κείµενο. Οι 
Βeeferman, Berger και Lafferty υπολόγισαν την τιµή του µ βασιζόµενοι στο µέσο µήκος τµήµατος που 
αντιστοιχεί σε µια θεµατική κατηγορία της συλλογής την οποία εξέτασαν. Η 
),( tsDµ
µγ  είναι µια σταθερά 
κανονικοποίησης η οποία κανονικοποιεί το  ούτως ώστε να είναι µια εκθετική κατανοµή. Η 
 αποδίδει µε αποτελεσµατικό τρόπο µεγαλύτερο βάρος σε συγκρίσεις µεταξύ κοντινότερων 
προτάσεων στον υπολογισµό το . Εν συντοµία, το  είναι η πιθανότητα ένα τυχαίο ζεύγος 
προτάσεων οι οποίες απέχουν απόσταση µ  µεταξύ τους να κατηγοριοποιούνται διαφορετικά δηλαδή 
να προκύπτει από την υποθετική – εκτιµώµενη τµηµατοποίηση ότι ανήκουν σε διαφορετικά τµήµατα.  
),( tsDµ
),( tsDµ
kP kP
 90
Κεφάλαιο 3 Τµηµατοποίηση κειµένων - Βιβλιογραφικές αναφορές 
Μπορεί να δειχθεί, ότι για κατάλληλες τιµές του , το παίρνει τιµές από 0% ως 100% 
και είναι ένα κριτήριο του κατά πόσο η πραγµατική και η υποθετική-εκτιµώµενη τµηµατοποίηση ενός 
κειµένου συµφωνούν (µε µικρές τιµές του να υποδεικνύουν υψηλό ποσοστό ακρίβειας 
τµηµατοποίησης). Το  «τιµωρεί» λάθη στις θέσεις των ορίων τα οποία βρίσκονται κοντά στις 
πραγµατικές θέσεις των πραγµατικών ορίων λιγότερο από εκείνα τα οποία εµφανίζουν µεγάλη 
απόσταση από τα πραγµατικά όρια. Κατά αυτόν τον τρόπο, το  αξιολογεί ορθότερα την ακρίβεια 
τµηµατοποίησης από ότι τα κριτήρια  Precision και Recall.  
µD kP
k
kP
kP
P
 Οι Βeeferman, Berger και Lafferty παρατήρησαν ότι ο υπολογισµός του εν λόγω µέτρου απαιτεί 
κάποια γνώση της συλλογής κειµένων δεδοµένου ότι η τιµή του µ πρέπει να επιλεχθεί. Η παρουσία 
µιας σταθεράς η οποία εξαρτάται από την εκάστοτε συλλογή σε ότι αφορά τον υπολογισµό της  
σηµαίνει ότι, οι τιµές της απόδοσης οι οποίες λαµβάνονται από διαφορετικές συλλογές κειµένων 
πιθανόν να µην είναι συγκρίσιµες. Για παράδειγµα, δεν είναι αξιόπιστο το να πούµε ότι ένας 
αλγόριθµος ο οποίος επιτυγχάνει απόδοση 0.90 σε µια συλλογή είναι καλύτερος από ότι ένας άλλος ο 
οποίος επιτυγχάνει απόδοση 0.85 σε µια άλλη συλλογή κειµένων.  
µP
 Ένα δεύτερο µειονέκτηµα του συγκεκριµένου µέτρου εµφανίζεται λόγω του ότι το µέτρο αυτό 
σχεδιάστηκε για την αξιολόγηση αλγορίθµων τµηµατοποίησης κειµένων τα οποία αποτελούνται από 
ιστορίες οι οποίες συνδέονται αλυσιδωτά η µια µετά την άλλη. Ο υπολογισµός της συν-αθροιστικής 
τιµής των Precision και Recall είναι απλός όταν τµηµατοποιεί κανείς συλλογές κειµένων σε 
διαφορετικά αρχεία, δεν είναι όµως προφανές πώς να συνδυάσει τις λαµβανόµενες τιµές σε ένα 
πιθανοτικό µέτρο. Τα διάφορα κείµενα είναι δυνατό να έχουν συνδεθεί αλυσιδωτά πριν από τον 
υπολογισµό του δείκτη, το γεγονός όµως προκαλεί – σύµφωνα µε τον Reynar - διαστρέβλωση της 
αξιολόγησης γιατί στον εκάστοτε αλγόριθµο θα δίνεται «πίστωση» επειδή «µαντεύει» τα όρια µεταξύ 
των κειµένων τα οποία είναι όµως ήδη γνωστά. Παρόλα αυτά, ο Reynar πιστεύει ότι αυτή η εν λόγω 
µέθοδος αξιολόγησης είναι η καλύτερη από όλες τις ως τώρα προτεινόµενες όταν χρησιµοποιείται σε 
κατάλληλο σώµα κειµένων. 
3.6 Η δική µας προσέγγιση  
 
Μετά από µελέτη της διεθνούς βιβλιογραφίας και των προτεινόµενων σε αυτή µεθόδων, 
κατασκευάσαµε δυο εναλλακτικά µοντέλα καθένα από τα οποία προσπαθεί να δώσει µια 
αποτελεσµατικότερη εναλλακτική λύση στο πρόβληµα της τµηµατοποίησης κειµένων. Και τα δυο 
προτεινόµενα µοντέλα ακολουθούν την παραδοχή ότι η δοµή ενός κειµένου είναι γραµµική και έτσι 
 91
Κεφάλαιο 3 Τµηµατοποίηση κειµένων - Βιβλιογραφικές αναφορές 
πραγµατοποιούν γραµµική τµηµατοποίηση. Το πρώτο µοντέλο ανιχνεύει τη συνοχή µέσα στο κείµενο 
υπολογίζοντας την κατηγορία στην οποία προκύπτει ότι ανήκει κάθε µια λέξη που εµφανίζεται στο 
κείµενο ενώ η εύρεση των ορίων µεταξύ των τµηµάτων ανάγεται στην εύρεση εκείνων των σηµείων 
του κειµένου στα οποία παρατηρείται αλλαγή κατηγορίας. Με αυτόν τον τρόπο το εν λόγω µοντέλο 
αναδεικνύει και τη συνεισφορά της χρήσης των εννοιών έναντι αυτής των αυτούσιων λέξεων ενός 
κειµένου. Το δεύτερο µοντέλο εστιάζεται περισσότερο στην τµηµατοποίηση µεγάλων κειµένων . Πιο 
συγκεκριµένα, ανιχνεύει τη συνοχή µέσα στο κείµενο εξετάζοντας την οµοιότητα µεταξύ όλων των 
µερών του κειµένου και κατασκευάζοντας έναν πίνακα που περιείχε την οµοιότητα όλων των 
προτάσεων του κειµένου µεταξύ τους, ενώ για την εύρεση των ορίων µεταξύ των τµηµάτων κάνει 
χρήση της τεχνικής του δυναµικού προγραµµατισµού, εµπνευσµένο από τη δουλειά του Oskari 
Heinonen. Το πρώτο µοντέλο περιγράφεται διεξοδικά στο Κεφάλαιο 4 που ακολουθεί ενώ το δεύτερο 
µοντέλο περιγράφεται αναλυτικά στο Κεφάλαιο 5. 
 
 
 
 
 92
 
 
93 
Κεφάλαιο 4 Τµηµατοποίηση Κειµένων βάσει του αποτελέσµατος κατηγοριοποίησης 
ΚΕΦΑΛΑΙΟ 4  
ΤΜΗΜΑΤΟΠΟΙΗΣΗ ΚΕΙΜΕΝΩΝ ΒΑΣΕΙ ΤΟΥ 
ΑΠΟΤΕΛΕΣΜΑΤΟΣ ΚΑΤΗΓΟΡΙΟΠΟΙΗΣΗΣ 
 
4.1 Εισαγωγή 
 
Στόχος του παρόντος κεφαλαίου είναι η προσπάθεια επίλυσης της εύρεσης της δοµής των 
κειµένων άρα και του τρόπου µε τον οποίο συσχετίζεται η πληροφορία που περιέχεται µέσα σε αυτά 
µε την ανάπτυξη ενός µοντέλου το οποίο πραγµατοποιεί τµηµατοποίηση κειµένων µε τη βοήθεια του 
αποτελέσµατος της κατηγοριοποίησης. Το αποτέλεσµα της κατηγοριοποίησης προκύπτει από την 
εναλλακτική χρήση των λέξεων και των εννοιών αυτών. Με αυτόν τον τρόπο µελετάται και 
αξιολογείται ταυτόχρονα κάτω από ένα διαφορετικό πρίσµα (από αυτό του Κεφαλαίου 2) η επιρροή 
και η συνεισφορά των εννοιών στην απόδοση κατηγοριοποίησης όταν αυτές χρησιµοποιούνται έναντι 
των αυτούσιων λέξεων. 
Για την πραγµατοποίηση της κατηγοριοποίησης κειµένων χρησιµοποιήθηκε ο αλγόριθµος 
PREMMON -ο οποίος µοιάζει σε µεγάλο βαθµό µε τον αλγόριθµο Κατηγοριοποίησης Μέγιστης 
Πιθανότητας (Μaximum Α Ρosteriori Classification) ο οποίος παρουσιάστηκε στο Κεφάλαιο 2- σε 
εναλλακτικές µορφές, ενώ η τµηµατοποίηση κειµένων µοντελοποιήθηκε µε τρόπο άρρηκτα 
συνδεδεµένο µε το αποτέλεσµα της κατηγοριοποίησης. Για την εξέταση της απόδοσης εφαρµογής της 
κατηγοριοποίησης (βασιζόµενοι είτε στις λέξεις είτε στις αντίστοιχες έννοιες ενός κειµένου) στο 
πρόβληµα της τµηµατοποίησης κατασκευάσαµε ένα καινούργιο σώµα κειµένων προερχόµενο από τα 
κείµενα του Brown Corpus. 
Τόσο ο ορισµός του προβλήµατος της τµηµατοποίησης µε τη βοήθεια του αποτελέσµατος της 
κατηγοριοποίησης όσο και τα πειράµατα τα οποία πραγµατοποιήθηκαν παρουσιάζονται διεξοδικά στις 
ενότητες που ακολουθούν. Τα αποτελέσµατα τα οποία προέκυψαν από την εφαρµογή του αλγορίθµου 
PREMMON µε τις διάφορες εναλλακτικές µορφές του, όπου χρησιµοποιήθηκε το αποτέλεσµα της 
κατηγοριοποίησης για την τµηµατοποίηση κειµένων, έδειξαν ότι η χρήση των εννοιών εξακολουθεί να 
 94
Κεφάλαιο 4 Τµηµατοποίηση Κειµένων βάσει του αποτελέσµατος κατηγοριοποίησης 
πλεονεκτεί έναντι της χρήσης των λέξεων και ότι το πρόβληµα της τµηµατοποίησης αποτελεί ένα 
ιδιαίτερα ενδιαφέρον και επίκαιρο πρόβληµα της θεµατικής περιοχής «Ανάκτησης Πληροφορίας» 
(“Information Retrieval”, [Gloss(00037)]). 
4.2 Τµηµατοποίηση µε χρήση του αποτελέσµατος της 
κατηγοριοποίησης 
 
Σε αυτό το κεφάλαιο επιθυµούµε να εµβαθύνουµε στον τρόπο δόµησης των κειµένων. Για το 
σκοπό αυτό προτείνουµε ένα µοντέλο το οποίο προβαίνει στον συνδυασµό της τεχνικής της 
κατηγοριοποίησης – χρησιµοποιώντας τόσο τις λέξεις όσο και τις έννοιες ενός κειµένου - µε αυτή της 
τµηµατοποίησης κειµένων (“Text Segmentation”, [Gloss(00092)]).  
 Πιο συγκεκριµένα, στόχος της εν λόγω σειράς πειραµάτων είναι η τµηµατοποίηση ενός 
κειµένου µε την βοήθεια της κατηγοριοποίησης των διαφόρων τµηµάτων του. Το πρόβληµα που 
καλούµαστε να επιλύσουµε ορίζεται ως εξής: Θεωρούµε ένα κείµενο το οποίο θέλουµε να 
τµηµατοποιήσουµε σε τµήµατα καθένα από το οποίο εµφανίζει σε όλο το εύρος του ισχυρή οµοιότητα (ή 
αλλιώς «λεκτική συνάφεια») όπως αυτή υποδεικνύεται από τις λέξεις – έννοιες που περιέχονται µέσα σε 
αυτό. Για την τµηµατοποίηση αυτή, για κάθε µια λέξη του κειµένου υπολογίζουµε την πιθανότητα της εν 
λόγω λέξης να ανήκει σε µια από τις δοθείσες κατηγορίες στις οποίες ανήκουν τα κείµενα τα οποία 
εξετάζουµε (ο εν λόγω υπολογισµός των πιθανοτήτων γίνεται µε διάφορους τρόπους οι οποίοι 
παρουσιάζονται στην συνέχεια). Η εκάστοτε λέξη προκύπτει ότι ανήκει σε εκείνη την κατηγορία η οποία 
επιτυγχάνει τη µέγιστη τιµή πιθανότητας. Με βάση την πληροφορία αυτή και παρατηρώντας τον τρόπο  
µεταβολής των κατηγοριών όλων των λέξεων του εκάστοτε κειµένου, προσπαθούµε να βρούµε εκείνες 
τις λέξεις στις οποίες παρουσιάζεται αλλαγή κατηγορίας. Αυτές οι λέξεις θεωρούµε ότι αποτελούν τα 
όρια µεταξύ των διαφόρων τµηµάτων. Η παραδοχή δηλαδή η οποία γίνεται εδώ είναι ότι, αλλαγή στην 
κατηγορία στην οποία ανήκουν οι λέξεις του κειµένου συνεπάγεται και αλλαγή τµήµατος. Στόχος µας 
είναι η εύρεση τόσο του αριθµού των τµηµάτων όσο και της θέσης των ορίων µεταξύ αυτών, όπως 
υποδεικνύουν οι λέξεις στις οποίες πραγµατοποιείται αλλαγή κατηγορίας. 
 4.2.1  Παράσταση του κειµένου  
 
Στα πειράµατα τα οποία πραγµατοποιήθηκαν, χρησιµοποιήθηκαν για τα κείµενα δυο 
διαφορετικές παραστάσεις, οι οποίες είναι οι ίδιες µε αυτές του Κεφαλαίου 2. Η  πρώτη από αυτές 
 95
Κεφάλαιο 4 Τµηµατοποίηση Κειµένων βάσει του αποτελέσµατος κατηγοριοποίησης 
βασίζεται στις λέξεις ενώ η δεύτερη στις έννοιες. Παρουσιάζουµε µε µεγαλύτερη λεπτοµέρεια την 
παράσταση η οποία βασίζεται στις λέξεις (η αντίστοιχη η οποία βασίζεται στις έννοιες είναι εντελώς 
ανάλογη). Ορίζουµε τα ακόλουθα στοιχεία:  
1. Μια συλλογή κειµένων προς εκπαίδευση Mxxx ...,, 21 . 
2. Μια συλλογή από Κ κατηγορίες Kccc ...,, 21 . 
3. Η πληροφορία της κατηγορίας στην οποία ανήκει το κείµενο  (για m=1,2, …., M, 
∈ {
mx
mx }...,, 21 Kccc ). 
Καθένα από τα κείµενα είναι ένα άνυσµα λέξεων, δηλαδή για m=1,2, …., M, έχουµε  
  [ ]
mmJmjmmm
xxxxx ,....,,...,, 21=  
Όπου  m=1,2, …., M, και  j=1, 2, …, .  mJ
Αξίζει να σηµειωθεί ότι  είναι ο συνολικός αριθµός των λέξεων οι οποίες εµφανίζονται στο 
m-στο κείµενο, όπου  είναι η j-στη λέξη η οποία εµφανίζεται στο m-στο κείµενο. Επιπρόσθετα , 
το  λαµβάνει τιµές από το λεξιλόγιο το οποίο ορίζεται από το άνυσµα: 
mJ
mjx
mjx
     [ ]
wNn
wwww ,....,,...,, 21=W  
Όπου  είναι ο συνολικός αριθµός των λέξεων που εµφανίζονται σε όλα τα κείµενα προς 
εκπαίδευση.  
wN
Αξίζει να σηµειωθεί ότι, παρά το γεγονός ότι τα περιεχόµενα του m-του κειµένου 
τοποθετούνται στο άνυσµα , µε µεταβλητό µήκος , η παράσταση του κειµένου είναι ένα 
άνυσµα σταθερού µήκους . Χρησιµοποιούµε για την παράσταση µε βάση τις λέξεις το Άνυσµα 
Κειµένου Συχνοτήτων Λέξεων (“Word Frequency document vector”, [Gloss(00104)]): 
mx
w
mJ
N
[ ]
wmNmnmmm
fffff ,....,,...,, 21=  
Όπου (για  n=1,2,…, ) έχουµε = το πλήθος των φορών που η n-λέξη εµφανίζεται στο m-στο 
κείµενο.   
wN mnf
 96
Κεφάλαιο 4 Τµηµατοποίηση Κειµένων βάσει του αποτελέσµατος κατηγοριοποίησης 
Το Άνυσµα Κειµένου Συχνοτήτων Λέξεων αποτελεί τη βασική παράσταση των κειµένων σε ότι 
αφορά τις λέξεις. Μια επιπρόσθετη παράσταση κειµένων υπολογίζεται µε εντελώς ανάλογο τρόπο 
χρησιµοποιώντας τις έννοιες δηλαδή το Άνυσµα Κειµένου Συχνοτήτων Εννοιών (ΣΕ). Η τελευταία 
παράσταση κάνει χρήση του λεξιλογίου των εννοιών, το οποίο έχει τη µορφή:  
     [ ]
sNn
ssssS ,....,,...,, 21=  
Όπου  είναι ο συνολικός αριθµός των εννοιών που εµφανίζονται σε όλα τα κείµενα προς 
εκπαίδευση. Όλες οι υπόλοιπες λεπτοµέρειες είναι εντελώς ανάλογες µε αυτές της αντίστοιχης 
παράστασης για τις λέξεις.  
sN
 4.2.2 Ο αλγόριθµος PREMONN  
 
Το PREMMON ([Petridis & Kehagias, 1996], [Petridis & Kehagias, 1998]) αποτελεί ένα 
νευρωνικό δίκτυο το οποίο συνίσταται από υπο-µονάδες και χρησιµοποιείται για πρόβλεψη. Το 
PREMMON έχει ιεραρχική δοµή. Το κατώτερο επίπεδο αποτελείται από ένα σύνολο (γραµµικών ή µη 
γραµµικών) µονάδων πρόβλεψης  ενώ το ανώτερο επίπεδο αποτελεί µια µονάδα απόφασης (η οποία 
χρησιµοποιεί Bayesian ή µη πιθανοτικούς κανόνες απόφασης). Η σύγκλιση των µονάδων αυτών προς 
τη σωστή κατηγοριοποίηση αποδεικνύεται για διάφορες επιλογές µονάδων πρόβλεψης και απόφασης 
ενώ η γενική αρχιτεκτονική είναι δυνατό να τροποποιηθεί τόσο στο επίπεδο της πρόβλεψης όσο και 
στο επίπεδο της απόφασης. Για παράδειγµα στο επίπεδο πρόβλεψης µπορούν να χρησιµοποιηθούν 
διάφορες µονάδες όπως γραµµικές, σιγµοειδείς, πολυωνυµικές κλπ. Στο επίπεδο απόφασης είναι 
δυνατό να χρησιµοποιηθεί µια ποικιλία από συναρτήσεις εκτίµησης όπως π.χ. οι Bayesian.  
Στο Κεφάλαιο 2 παρουσιάστηκε ο αλγόριθµος Κατηγοριοποίησης Μέγιστης Ύστερης 
Πιθανότητας (“Μaximum Α Ρosteriori Classification”, MAP [Gloss(00058)]). Η οµοιότητα του 
αλγορίθµου Κατηγοριοποίησης Μέγιστης Πιθανότητας µε τον αλγόριθµο PREMMON απαντάται στο 
γεγονός ότι ο αλγόριθµος PREMMON χρησιµοποιεί ως συνάρτηση απόφασης την Bayesian. Οι 
παραλλαγές του αλγορίθµου PREMMON που χρησιµοποιούνται στα πειράµατα του εν λόγω 
κεφαλαίου δεν διαφέρουν σηµαντικά από τον αλγόριθµο Κατηγοριοποίησης Μέγιστης Ύστερης 
Πιθανότητας που παρουσιάστηκε στο Κεφάλαιο 2. 
 
 
 97
Κεφάλαιο 4 Τµηµατοποίηση Κειµένων βάσει του αποτελέσµατος κατηγοριοποίησης 
4.2.2.1 Πρώτη παραλλαγή του αλγορίθµου PREMONN  
 
Ο αλγόριθµος PREMMON, σε απόλυτη αναλογία µε τον αλγόριθµο Κατηγοριοποίησης 
Μέγιστης Πιθανότητας, βασίζεται στην µεγιστοποίηση πάνω στα , , …, 1c 2c Kc  της ύστερης 
πιθανότητας  
),...,,|()|(
21 mmJmmkrmkr
xxxcPxcP =          (4.1) 
γεγονός που σηµαίνει ότι το κείµενο κατηγοριοποιείται στην -στη κατηγορία όπου   
^
k
),...,,|(maxarg
21,...,1
^
mmJmmkrKk
xxxcPk
=
= .         (4.2) 
Ο αλγόριθµος PREMMON ακριβώς όπως και ο αλγόριθµος Κατηγοριοποίησης Ύστερης 
Μέγιστης Πιθανότητας βασίζεται στην υπόθεση ότι η πιθανότητα της j-στης λέξης (η οποία περιέχεται 
µέσα στο m-στο κείµενο) εξαρτάται αποκλειστικά και µόνο από την κατηγορία και όχι από τις 
υπόλοιπες λέξεις που εµφανίζονται µέσα στο κείµενο. Αυτό επιτρέπει τον εύκολο υπολογισµό της 
παράστασης (4.1) σε ότι αφορά τις υπό συνθήκη πιθανότητες  των λέξεων δοθείσης της 
κατηγορίας. Στην συνέχεια θα δούµε πως µπορούµε να λάβουµε εκτιµήσεις των  καθώς 
και δυο εναλλακτικούς τρόπους υπολογισµού της παράστασης (4.1). 
)|( knr cwP
)|( knr cwP
 
Χρησιµοποιώντας το θεώρηµα του Bayes έχουµε: 
    
∑
=
==
wN
n
knr
knr
kr
knr
knr
cwP
cwP
cP
cwP
cwP
1
),(
),(
)(
),(
)|(          (4.3) 
Ο υπολογισµός µιας προσέγγισης της παράστασης (4.3) σε αναλογία  µε τον ορισµό του 
αλγορίθµου Naïve Bayes (για n = 1,2,…,  και k = 1,2…,K) δίνεται παρακάτω: wN
            
∑ ∑
∑
= =
=
+
+
=
wN
n
M
m
mkmn
M
m
mkmn
knr
fa
fa
cwP
1 1
1
^
)1(
)1(
)|(            (4.4) 
 98
Κεφάλαιο 4 Τµηµατοποίηση Κειµένων βάσει του αποτελέσµατος κατηγοριοποίησης 
όπου = 1 αν αν το κείµενο ανήκει στην κατηγορία  αλλιώς 0, α είναι µια ρυθµιστική 
παράµετρος του αλγορίθµου (την οποία εισάγουµε)  τέτοια ώστε, µεγάλες τιµές της παραµέτρου α να 
οδηγούν σε µια πιο κανονικοποιηµένη κατανοµή πιθανοτήτων. Αξίζει να σηµειωθεί ότι, µια εκτίµηση 
της ποσότητας δίνεται από την ακόλουθη σχέση: 
mk1 kc
)( kr cP
      
M
cP
M
m
mk
kr
∑
== 1
^
1
)(             (4.5) 
Στην συνέχεια παρουσιάζουµε δύο εναλλακτικές µορφές του αλγορίθµου PREMMON για τον 
υπολογισµό της ποσότητας συναρτήσει µε την ποσότητα . 
Οι εν λόγω εναλλακτικές µορφές του αλγορίθµου βασίζονται στις λέξεις. Εντελώς ανάλογα ορίζονται 
και οι αντίστοιχες που βασίζονται στις έννοιες.  
),...,,|( 21 mmJmmkr xxxcP )|( knr cwP
 
4.2.2.1.1  Πρώτη εναλλακτική µορφή του αλγορίθµου PREMONN  
 
∆οθέντος ενός κειµένου d (για το οποίο δεν υπάρχει η πληροφορία  σχετικά µε την κατηγορία 
στην οποία ανήκει), ο αλγόριθµος κατηγοριοποίησης PREMMON υπολογίζει (µε την βοήθεια των 
πιθανοτήτων ) τις πιθανότητες για όλα τα  k=1,…,K και κατηγοριοποιεί το 
κείµενο d σε εκείνη την κατηγορία  η οποία µεγιστοποιεί την πιθανότητα . Ο 
υπολογισµός της εξίσωσης (4.1) δηλαδή της πιθανότητας  γίνεται µε 
τον ακόλουθο αναδροµικό τρόπο όπως αυτός περιγράφεται στα [Petridis & Kehagias, 1996]και 
[Petridis & Kehagias, 1998] :  
)|( knr cwP )|( dcP kr
kc )|( dcP kr
)m,...,,|( 21 mJmmkr xxxcP
Για m = 1,2,…, M,  k = 1,2,…, K  και j = 1,2,…,  ορίζουµε τις ποσότητες: mJ
                          (4.6) ),...,,|(),(
21
,,
0 mjmmkr
km
jkr
km xxxcPpcPp ==
Με άλλα λόγια η ποσότητα παριστά την πιθανότητα το m-στο κείµενο να ανήκει στην 
k-στη κατηγορία έχοντας εξετάσει µέχρι και την j-στη λέξη. Παρακάτω δίνουµε την αναδροµική σχέση 
η οποία υπολογίζει τα ,  , …, από τα ,  , …,  ενώ το  
απεικονίζει την προγενέστερη γνώση σχετικά µε την τιµή του j. Ελλείψει τέτοιου είδους πληροφορίας 
km
jp
,
kk
jp
,1
jp
,2 km
jp
, k
jp
,1
1−
k
jp
,2
1−
km
jp
,
1−
kmp ,0
 99
Κεφάλαιο 4 Τµηµατοποίηση Κειµένων βάσει του αποτελέσµατος κατηγοριοποίησης 
µπορούµε να υποθέσουµε ότι όλα τα µοντέλα είναι ισοπίθανα δηλαδή 
kc
cPp
k
kr
km 11)(,0 === για  
k = 1, …, Κ και για m = 1,2,…, M.  Κατά αυτό τον τρόπο µπορούµε να υπολογίσουµε µε αναδροµικό 
τρόπο την κατηγορία στην οποία είναι πιο πιθανό να ανήκει το m-στο κείµενο, έχοντας εξετάσει µέχρι 
και την j-στη λέξη του κειµένου αυτού. Ο υπολογισµός αυτός ξεκινά µε την έκφραση του κανόνα του 
Bayes κατά τον ακόλουθο τρόπο: 
 
∑
∑
=
−−
−−
=
−
−
−
−
=
==
==
K
i
jmmmirijmmmmjr
jmmmkrkjmmmmjr
K
i
jmmmmjir
jmmmmjkr
jmmmmjr
jmmmmjkr
mjmmkr
xxxcPcxxxxP
xxxcPcxxxxP
xxxxcP
xxxxcP
xxxxP
xxxxcP
xxxcP
1
1,211,21
1,211,21
1
1,21
1,21
1,21
1,21
21
),...,,|(*),,...,,|(
),...,,|(*),,...,,|(
),...,,|,(
),...,,|,(
),...,,|(
),...,,|,(
),...,,|(
 
Χρησιµοποιώντας τον ορισµό του και την παραδοχή του Naïve Bayes προκύπτει ότι:  kmjp
,
  
∑
=
−
−
+⋅
+⋅
== K
i
imjr
im
j
kmjr
km
j
mjmmkr
km
j
hcxPp
hcxPp
xxxcPp
1
,
1
,
1,
)|(
)|(
),...,,|(
21
                  (4.7) 
Η πιθανότητα  υπολογίζεται εµπειρικά από τα δεδοµένα προς εκπαίδευση. Για 
την αποφυγή λανθασµένης κατηγοριοποίησης λόγω του γεγονότος ότι η  είναι δυνατό να 
παραµένει σταθερή ως προς την τιµή της για µεγάλο χρονικό διάστηµα και πολύ κοντά στην τιµή 1 
(για µια συγκεκριµένη τιµή της παραµέτρου k ενώ για τις υπόλοιπες τιµές της παραµέτρου k η εν  
λόγω πιθανότητα να λαµβάνει τιµές πολύ κοντά στο µηδέν), δηλαδή για πολλές τιµές της παραµέτρου 
j η εν λόγω πιθανότητα να λαµβάνει την ίδια τιµή, εισάγουµε ένα προκαθορισµένο κατώφλι h. Στην 
ουσία, η χρήση αυτού του κατωφλιού είναι ανάλογη µε την εισαγωγή ενός παράγοντα «λησµονιάς» 
 δίνοντας ταυτόχρονα σε κάθε τιµή της εξίσωσης (4.7) µια µικρή ύστερη πιθανότητα ίση µε h, 
ανεξάρτητα από όλες τις προηγούµενες τιµές των πιθανοτήτων . Η εξίσωση (4.7) αποτελεί την 
πρώτη εναλλακτική µορφή του αλγορίθµου PREMMON.  
)|( kmjr cxP
km
jp
,
km
jp
,
1−
 
 
 100
Κεφάλαιο 4 Τµηµατοποίηση Κειµένων βάσει του αποτελέσµατος κατηγοριοποίησης 
4.2.2.1.2 ∆εύτερη εναλλακτική µορφή του αλγορίθµου PREMONN  
 
Ο υπολογισµός της εξίσωσης (4.7) είναι δυνατό να τροποποιηθεί κατά τον ακόλουθο τρόπο: 
ορίζουµε µια παράµετρο «κατώφλι» l  και σε κάθε επανάληψη ελέγχουµε το κατά πόσο η τιµή της 
ποσότητας , k = 1,2, …, K είναι µικρότερη από την τιµή της παραµέτρου l. Αν αυτό συµβαίνει, 
τότε η ποσότητα  λαµβάνει τιµή ίση µε το l (περισσότερες διευκρινήσεις παρέχονται στα 
[Petridis & Kehagias, 1996] και [Petridis & Kehagias, 1998]).  
km
jp
,
km
jp
,
Ο αναδροµικός τρόπος υπολογισµού του ο οποίος κάνει επίσης χρήση του 
προκαθορισµένου κατωφλιού l  υπολογίζεται σύµφωνα  µε την παρακάτω εξίσωση: 
km
jp
,
   












⋅
⋅
=
∑
=
−
− l
qp
qp
p K
i
im
j
im
j
km
j
km
jkm
j ,max
1
,,
1
,,
1,           (4.8) 
όπου  
( )
ςαλλι
αν
ώe
fcxP
q
r
mj
r
kmjrkm
j
,)2010(
0,)|(,
−
>
=  
Ο παράγοντας δρα ως ρυθµιστικός παράγοντας και εξαρτάται από την συχνότητα 
εµφάνισης της λέξης . Για την αποφυγή απόδοσης µηδενικής πιθανότητας στην περίπτωση που 
µια λέξη δεν εµφανίζεται σε ένα κείµενο, ο παράγοντας λαµβάνει µια πολύ µικρή, αλλά όχι 
µηδενική τιµή, ίση µε , όπου η παράµετρος r λειτουργεί ως παράγοντας πόλωσης των 
αποτελεσµάτων. Πιο συγκεκριµένα, µικρές τιµές της παραµέτρου r αποδυναµώνουν την επιρροή του 
παράγοντα  στον υπολογισµό της ποσότητας  , ενώ µεγάλες τιµές τον ενδυναµώνουν. 
km
jq
,
mj
e10( −
x
km
jq
,
r)20
km
jq
, km
jp
,
Η εξίσωση (4.8) αποτελεί τη δεύτερη εναλλακτική µορφή του αλγορίθµου PREMMON  και 
περιέχει δύο ρυθµιστικές παραµέτρους τις r και l. Στην εν λόγω µορφή  η παράµετρος l παίζει τον 
ρόλο του «κατωφλιού» αποδίδοντας στη χειρότερη περίπτωση στην εξίσωση (4.8) τιµή ίση µε l.  
 101
Κεφάλαιο 4 Τµηµατοποίηση Κειµένων βάσει του αποτελέσµατος κατηγοριοποίησης 
4.2.2.2 ∆εύτερη παραλλαγή του αλγορίθµου PREMONN  
 
 Βασιζόµενοι στον αλγόριθµο PREMMON κατασκευάσαµε δυο συµπληρωµατικές  υλοποιήσεις 
αυτού οι οποίες εισάγουν στον υπολογισµό της ποσότητας  και έναν παράγοντα εξοµάλυνσης, 
διαφορετικό σε κάθε µια από αυτές.  
km
jp
,
Πιο συγκεκριµένα η πρώτη συµπληρωµατική υλοποίηση µε προσθήκη παράγοντα 
εξοµάλυνσης κάνει χρήση της τεχνικής του «Κινούµενου Παραθύρου» (“sliding window”, 
[Gloss(00079)]) όπου κάθε φορά, στον υπολογισµό της ποσότητας  λαµβάνονται υπόψη και οι 
αντίστοιχες τιµές των n προηγούµενων και n επόµενων λέξεων του κειµένου. Ο λόγος χρήσης της 
τεχνικής του «Κινούµενου Παραθύρου» έγκειται στην αποφυγή τυχαίων αποτελεσµάτων τα οποία 
είναι δυνατό να προκύψουν από την τεχνική του PREMMON. Έστω π.χ. ότι από τους υπολογισµούς 
που πραγµατοποιεί το PREMMON προκύπτει ότι µια λέξη  ανήκει στην κατηγορία , ενώ οι n 
προηγούµενες και οι n επόµενες λέξεις της  προκύπτει ότι ανήκουν στην κατηγορία . Ένα 
τέτοιο αποτέλεσµα θεωρείται ως τυχαίο αποτέλεσµα και είναι δυνατό να οδηγήσει σε λανθασµένα 
συµπεράσµατα. Για το λόγο αυτό, αντί της χρησιµοποιείται µια εξοµαλυµένη µορφή της 
οποίας ο υπολογισµός συµπεριλαµβάνει και την τεχνική του «Κινούµενου Παραθύρου» και 
πραγµατοποιείται σύµφωνα µε την παρακάτω εξίσωση: 
km
jp
,
mjx kc
jcmjx
jp
kc≠
~
,m
jp
km, k
( )
12
)|(,
~
,
+⋅
⋅
=
∑
+
−=
n
cxPp
p
nj
njs
kmsr
km
s
km
j           (4.9) 
όπου 2n+1 είναι το συνολικό εύρος του κινούµενου παραθύρου. Η παραπάνω εξίσωση δηλώνει ότι για 
τον υπολογισµό της ποσότητας  δεν λαµβάνεται υπόψη µόνο ο παράγοντας  που 
αντιστοιχεί στην αµέσως προηγούµενη λέξη του κειµένου η οποία εξετάστηκε, αλλά ο µέσος όρος των 
n προηγούµενων αλλά και των n επόµενων λέξεων εκτός της λέξης . Με αυτόν τον τρόπο 
διασφαλίζεται το γεγονός ότι δεν θα λαµβάνονται τυχαία και πιθανώς λανθασµένα αποτελέσµατα σε 
ότι αφορά την απόδοση της βέλτιστης κατηγορίας στην οποία ανήκει µια λέξη δεδοµένου ότι 
λαµβάνουµε επιπρόσθετα υπόψη την αντίστοιχη των γειτονικών αυτής.  
~
,km
jp
km
jp
,
1−
mjx
Η δεύτερη συµπληρωµατική υλοποίηση του αλγορίθµου PREMMON µε χρήση παράγοντα 
εξοµάλυνσης δίνεται από την παρακάτω εξίσωση: 
 102
Κεφάλαιο 4 Τµηµατοποίηση Κειµένων βάσει του αποτελέσµατος κατηγοριοποίησης 
                          (4.10) )|()1(,1
,
kmjr
km
j
km
j cxPpp ∗−+∗= − γγ
 Όπως διαφαίνεται και από την εξίσωση (4.10) , η χρησιµοποιούµενη παράµετρος  γ σταθµίζει 
την επιρροή του παράγοντα  ως συµπλήρωµα του παράγοντα  στον υπολογισµό του 
παράγοντα .  Μεγάλες τιµές της παραµέτρου γ (όπου γ ∈[0,1]) αυξάνουν τον βαθµό 
σηµαντικότητας των προηγούµενων προβλέψεων ενώ µικρές τιµές αυτής τον αποδυναµώνουν, 
ρίχνοντας την βαρύτητα, σε ότι αφορά τον υπολογισµό της ποσότητας , στην τιµή της ποσότητας 
. Στα πειράµατα τα οποία πραγµατοποιήσαµε αναθέσαµε στην παράµετρο  γ την τιµή 0.9.  
km
jp
,
1− )|( kmjr cxP
km
jp
,
km
jp
,
)|( kmjr cxP
4.2.3 Τεχνική «Ψηφοφορίας» 
 
Ως συµπληρωµατικό στοιχείο του αλγορίθµου PREMMON (για όλες τις εναλλακτικές 
υλοποιήσεις αυτού) εισάγουµε µια τεχνική ψηφοφορίας η οποία εφαρµόζεται στο αποτέλεσµα που 
προκύπτει ύστερα από την εφαρµογή του αλγορίθµου PREMMON. Έναυσµα για την εισαγωγή της εν 
λόγω τεχνικής αποτελεί το ενδεχόµενο ότι για µια λέξη, η διαφορά των τιµών πιθανοτήτων να ανήκει 
σε κάθε µια από τις υπάρχουσες κατηγορίες (στις οποίες ανήκουν τα κείµενα της συλλογής) είναι 
δυνατό να προκύψει µικρή, πράγµα το οποίο µπορεί να αποτελέσει παράγοντα σύγχυσης. Αυτό 
προκύπτει από το γεγονός ότι η εν λόγω λέξη – έννοια δεν βοηθά στην αποσαφήνιση σχετικά µε το 
κατά πόσο αυτή αποτελεί όριο µεταξύ τµηµάτων, στοιχείο το οποίο είναι δυνατό να οδηγήσει σε 
λανθασµένες αποφάσεις. Η τεχνική της ψηφοφορίας καλείται να αποµονώσει τέτοιου είδους 
συµπεριφορές και ορίζεται ως ακολούθως: 
Για κάθε µια λέξη – έννοια που περιέχεται στα κείµενα προς τµηµατοποίηση, εξετάζουµε την 
διαφορά των τιµών των πιθανοτήτων να ανήκει σε κάθε µια από τις κατηγορίες στις οποίες ανήκουν τα 
κείµενα της συλλογής. Αν η εν λόγω διαφορά στην τιµή των πιθανοτήτων είναι µικρότερη της τιµής 0.5 
αποδίδεται στη λέξη – έννοια αυτή µικρότερος βαθµός σηµαντικότητας, ούτως ώστε αυτή να 
διαδραµατίσει µικρότερο ρόλο σε ότι αφορά την απόφαση της θέσης των ορίων µεταξύ των διαφόρων 
τµηµάτων έναντι άλλων λέξεων-εννοιών για τις οποίες είναι ευκρινέστερη η κατηγορία στην οποία 
ανήκουν. 
 
 
 103
Κεφάλαιο 4 Τµηµατοποίηση Κειµένων βάσει του αποτελέσµατος κατηγοριοποίησης 
4.3 Πειράµατα  
 
Για την αξιολόγηση της απόδοσης του αλγορίθµου µας χρησιµοποιήσαµε ακόµα µια φορά το 
σώµα κειµένων του Brown Corpus. Πιο συγκεκριµένα, χρησιµοποιήσαµε τη δική µας «συγχώνευση» 
των κειµένων σε 7 κατηγορίες όπως αυτή παρουσιάστηκε στην παράγραφο 2.6.2.1. Τα πειράµατα τα 
οποία πραγµατοποιήθηκαν έκαναν χρήση της παράστασης των κειµένων τόσο µε βάση τις λέξεις όσο 
και µε βάση τις έννοιες του εκάστοτε κειµένου και αξιολογήθηκαν µε τη βοήθεια του κριτηρίου του 
Beeferman το οποίο περιγράφεται στην παράγραφο 3.3.3. Στις ενότητες που ακολουθούν, 
περιγράφουµε το σώµα από το οποίο αντλούµε κείµενα, τον τρόπο δηµιουργίας των κειµένων προς 
τµηµατοποίηση, καθώς και τις τρεις σειρές πειραµάτων που αφορούν τη χρήση των εναλλακτικών 
υλοποιήσεων του PREMMON στην τµηµατοποίηση των εν λόγω κειµένων.  
4.3.1 Το σώµα κειµένων 
 
Όπως αναφέρθηκε προηγούµενα, για τα πειράµατα τα οποία πραγµατοποιήσαµε 
χρησιµοποιήσαµε κείµενα από το σώµα κειµένων του Brown Corpus. Πιο συγκεκριµένα 
χρησιµοποιήσαµε κείµενα τα οποία ανήκαν στην πρώτη κατηγορία που φέρει το όνοµα Press 
(Reportage, Editorial και Reviews, συνολικά 12 κείµενα), και στην έβδοµη κατηγορία µε την 
ονοµασία Fiction η οποία περιλαµβάνει τις θεµατικές περιοχές: a) General Fiction, b) Mystery and 
Detective Fiction, c) Science Fiction, d) Adventure and Western Fiction, e) Romance and love story, f) 
Humor, και περιλαµβάνει συνολικά 64 κείµενα. Όπως αναφέρεται στην παράγραφο 2.6.2.2, τα 
συνολικά 182 κείµενα και των 7 κατηγοριών διαχωρίστηκαν σε 123 κείµενα προς εκπαίδευση και σε 
59 κείµενα προς επαλήθευση. Πιο συγκεκριµένα, στην πρώτη κατηγορία (σύµφωνα µε τον παραπάνω 
διαχωρισµό) αντιστοιχούν 8 κείµενα προς εκπαίδευση και 4 κείµενα προς επαλήθευση, ενώ στην 
έβδοµη κατηγορία αντιστοιχούν 43 κείµενα προς εκπαίδευση και 23 κείµενα προς επαλήθευση. Τα 
κείµενα προς εκπαίδευση καθεµίας από τις κατηγορίες χρησιµοποιήθηκαν για τον υπολογισµό των 
ποσοτήτων που δίνονται στην εξίσωση (4.4) για όλες τις λέξεις και όλες τις έννοιες που εµφανίζονται 
σε αυτά. Οι εν λόγω πιθανότητες χρησιµοποιούνται στη συνέχεια για τις λέξεις-έννοιες που 
εµφανίζονται στα κείµενα προς τµηµατοποίηση, τα οποία προέρχονται από τµήµατα των κειµένων 
προς επαλήθευση.  
Ο λόγος για τον οποίο επιλέξαµε την 1η και την 7η κατηγορία αποτελεί το γεγονός ότι η 
θεµατολογία της µιας διαφέρει σηµαντικά από την αντίστοιχη της άλλης. Για τον λόγο αυτό 
 104
Κεφάλαιο 4 Τµηµατοποίηση Κειµένων βάσει του αποτελέσµατος κατηγοριοποίησης 
υποθέτουµε ότι τόσο το λεξιλόγιο που εµφανίζεται σε κάθε µια από αυτές όσο και οι κατανοµές 
πιθανοτήτων των λέξεων-εννοιών που περιέχονται στα κείµενα στις κατηγορίες αυτές θα διαφέρουν 
σηµαντικά. Κατά συνέπεια, τµήµατα κειµένων που ανήκουν στις κατηγορίες αυτές, θα είναι ευκρινώς 
διαχωρίσιµα έπειτα από την εφαρµογή οποιασδήποτε από τις παραλλαγές του αλγορίθµου 
PREMMON όπως αυτές παρουσιάστηκαν διεξοδικά σε αυτό το κεφάλαιο. Η εµπλοκή περισσοτέρων 
κατηγοριών θα οδηγούσε πιθανά σε λανθασµένα αποτελέσµατα εξαιτίας της επικάλυψης των 
θεµατικών περιοχών αυτών.  
 Τα συνολικά 76 κείµενα της 1ης και της 7ης κατηγορίας χρησιµοποιήθηκαν για την κατασκευή 
10 συνόλων κειµένων καθένα από τα οποία περιλαµβάνει 25 κείµενα προς τµηµατοποίηση. 
Αναφερόµαστε σε καθένα από αυτά ως Set0, Set1, …, Set9. Η διαφορά µεταξύ αυτών των 10 συνόλων 
έγκειται στα κείµενα που χρησιµοποιούνται για να υπολογιστεί η πιθανότητα µιας λέξης-έννοιας να 
ανήκει σε κάθε µια από τις δυο κατηγορίες και στα κείµενα που χρησιµοποιούνται για να 
κατασκευαστούν τα δεδοµένα προς τµηµατοποίηση. Έτσι, σε κάθε ένα από τα 10 σύνολα προς 
τµηµατοποίηση επιλέγονται κάθε φορά µε τυχαίο τρόπο 4 κείµενα από την πρώτη κατηγορία και 21 
κείµενα από τη έβδοµη. Τα εν λόγω 25 το πλήθος κείµενα χρησιµοποιούνται για τη δηµιουργία 25 
κείµενων προς τµηµατοποίηση ενώ τα υπόλοιπα για τον υπολογισµό των κατανοµών πιθανοτήτων των 
λέξεων – εννοιών. Ο λόγος για τον οποίο πραγµατοποιήθηκε η εν λόγω επιλογή αποτελεί η κατασκευή 
όσο το δυνατό ανοµοιογενών µεταξύ τους κειµένων προς τµηµατοποίηση. Τα 76 συνολικά κείµενα και 
των δυο κατηγοριών παρουσιάζουν τα χαρακτηριστικά που παραθέτονται στον Πίνακα 4.1.  
 Όπως αναφέρθηκε προηγούµενα, καθένα από τα δέκα σύνολα αποτελείται από 25 κείµενα προς 
τµηµατοποίηση. Καθένα από αυτά τα κείµενα συνίσταται από 10 τµήµατα. Από τα αρχικά κείµενα του 
Brown Corpus κρατήθηκε η πληροφορία των ορίων µεταξύ των παραγράφων που εµφανίζονται σε 
καθένα από αυτά. Κατά τη διάρκεια παραγωγής των κειµένων προς τµηµατοποίηση ακολουθήσαµε τις 
παρακάτω παραδοχές: 
 Κάθε τµήµα αντιστοιχεί σε ένα κοµµάτι ενός κειµένου µιας εκ των δυο κατηγοριών ούτως ώστε 
δυο διαδοχικά τµήµατα να µην αντιστοιχούν σε κείµενα της ίδιας κατηγορίας, δηλαδή οι δυο 
κατηγορίες εναλλάσσονται.  
 Αλλαγή κατηγορίας, όπως αυτή υπολογίζεται από τον τρόπο µεταβολής των κατηγοριών στις 
οποίες ανήκουν οι λέξεις (ή εναλλακτικά οι έννοιες) του κειµένου συνεπάγεται και αλλαγή 
τµήµατος. 
 105
Κεφάλαιο 4 Τµηµατοποίηση Κειµένων βάσει του αποτελέσµατος κατηγοριοποίησης 
 Τα όρια µεταξύ των τµηµάτων απαντώνται στο τέλος παραγράφων.* Όπου αυτό δεν είναι εφικτό 
κατά τη διαδικασία παραγωγής των τµηµάτων για τα κείµενα προς τµηµατοποίηση, προστίθεται η 
πληροφορία του τέλους παραγράφου. Για κάθε ένα από τα σχηµατιζόµενα κείµενα  προς 
τµηµατοποίηση φυλάσσεται η πληροφορία της θέσης των ορίων µεταξύ των τµηµάτων καθώς και 
της κατηγορίας στην οποία ανήκει κάθε ένα τµήµα. Η εν λόγω πληροφορία θα χρησιµοποιηθεί 
αργότερα για την αξιολόγηση της απόδοσης του αλγορίθµου µας, για κάθε µια από τις 
διαφορετικές παραλλαγές αυτού σε συνδυασµό µε όλες τις δυνατές τιµές των παραµέτρων.  
 
Στατιστικά Στοιχεία των κειµένων της 1ης και της 7ης κατηγορίας του Brown Corpus 
Μέσος όρος παραγράφων ανά κείµενο 42.10 
Μέσος όρος λέξεων ανά κείµενο 480.68 
Μέσος όρος εννοιών ανά κείµενο 441.32 
Μέσος όρος λέξεων ανά παράγραφο 11.4157 
Μέσος όρος εννοιών ανά παράγραφο 10.481 
Ελάχιστος αριθµός λέξεων µέσα σε κείµενο 286 
Ελάχιστος αριθµός εννοιών µέσα σε κείµενο 261 
Μέγιστος αριθµός λέξεων µέσα σε κείµενο 705 
Μέγιστος αριθµός εννοιών µέσα σε κείµενο 702 
Ελάχιστος αριθµός παραγράφων µέσα σε κείµενο 9 
Μέγιστος αριθµός παραγράφων µέσα σε κείµενο 96 
Πίνακας 4.1: Στατιστικά στοιχεία των 76 κειµένων της πρώτης και έβδοµης κατηγορίας του σώµατος κειµένων 
του Brown Corpus. 
Για τη δηµιουργία καθενός από τα 25 κείµενα προς τµηµατοποίηση, για κάθε ένα από τα δέκα 
σύνολα κειµένων ακολουθείται η παρακάτω διαδικασία:  
                                                 
* Στα εν λόγω πειράµατα δεν εξετάζουµε το τέλος προτάσεων δηλαδή δεν χρησιµοποιούµε ως µέτρο αναφοράς 
τις προτάσεις των κειµένων. Ο λόγος για τον οποίο συµβαίνει αυτό, έγκειται στον τρόπο δηµιουργίας των 
κειµένων προς τµηµατοποίηση, όπου για την κατασκευή όσο το δυνατό µεγαλύτερων τµηµάτων εξάγουµε 
παραγράφους και όχι προτάσεις από τα αρχικά κείµενα.  
 106
Κεφάλαιο 4 Τµηµατοποίηση Κειµένων βάσει του αποτελέσµατος κατηγοριοποίησης 
Για κάθε ένα από τα δέκα τµήµατα από τα οποία αποτελείται κάθε ένα κείµενο προς τµηµατοποίηση 
πραγµατοποιούµε τα ακόλουθα: 
Βήµα 1ο: Επιλέγουµε µε τυχαίο τρόπο την κατηγορία (µια εκ των δύο) στην οποία επιθυµούµε να 
ανήκει το εν λόγω τµήµα. Έστω Ι η µεταβλητή η οποία δηλώνει την επιλεγόµενη κατηγορία.  
Βήµα 2ο: ∆οθείσης της πληροφορίας της κατηγορίας στην οποία επιθυµούµε να ανήκει το εν λόγω 
τµήµα, επιλέγουµε µε τυχαίο τρόπο (µε τη βοήθεια µιας γεννήτριας τυχαίων αριθµών) ένα από τα 
κείµενα προς επαλήθευση του Brown Corpus που ανήκει στην εν λόγω κατηγορία. Έστω Κ η 
µεταβλητή η οποία δηλώνει το επιλεγόµενο κείµενο της κατηγορίας Ι.  
Βήµα 3ο: Επιλέγουµε µε τυχαίο τρόπο (µε τη βοήθεια µιας γεννήτριας τυχαίων αριθµών) έναν αριθµό 
 όπου τα α και b αντιστοιχούν στον ελάχιστο και στον µέγιστο αριθµό λέξεων – εννοιών 
που δόθηκαν ως παράµετροι του προβλήµατος. Ύστερα από εξέταση του σώµατος κειµένων του 
Brown Corpus, θέσαµε ως ελάχιστο αριθµό λέξεων – εννοιών που συνιστούν ένα τµήµα την τιµή α = 
54 και ως µέγιστο την τιµή b = 130. 
{ bal ,...,∈ }
                                                
* 
Βήµα 4ο: Εξάγουµε από το κείµενο Κ συνεχόµενες παραγράφους ξεκινώντας από την πρώτη λέξη – 
έννοια του κειµένου, όσων το άθροισµα των λέξεων – εννοιών που περιλαµβάνουν συνιστά τον τυχαία 
επιλεγόµενο αριθµό l. Το τέλος του τµήµατος ενισχύεται µε την πληροφορία του τέλους παραγράφου. 
Οι εν λόγω παράγραφοι των οποίων το συνολικό άθροισµα των λέξεων – εννοιών που περιέχουν 
ισούται µε l, συνιστούν το παραγόµενο τµήµα. 
 Η εν λόγω διαδικασία οδηγεί στη δηµιουργία 25 κειµένων για κάθε ένα από τα δέκα σύνολα 
Set0, Set1, …, Set9. 
 
* Αυτό σηµαίνει ότι, κατά µέσο όρο ένα τµήµα θα αποτελείται από 92 λέξεις – έννοιες οι οποίες σύµφωνα µε τις 
πληροφορίες που παραθέτονται στον Πίνακα 4.1 θα αντιστοιχούν σε περίπου 8 µε 9 παραγράφους ανά κείµενο, 
άρα συνολικά, λαµβανοµένων υπόψη και των 10 τµηµάτων σε 80 µε 90 παραγράφους κατά µέσο όρο. Θεωρούµε 
ότι κείµενα προς τµηµατοποίηση τέτοιου µήκους – διπλάσιου περίπου των αρχικών – αποτελούν ένα αρκετά 
δύσκολο αλλά συνάµα ενδιαφέρον πρόβληµα, αν ληφθεί υπόψη τόσο ο σεβαστός αριθµός των λέξεων – εννοιών 
που εµφανίζονται σε αυτά όσο και ο τρόπος επηρεασµού των κατανοµών πιθανοτήτων αυτών στις δυο 
κατηγορίες.  
 
 107
Κεφάλαιο 4 Τµηµατοποίηση Κειµένων βάσει του αποτελέσµατος κατηγοριοποίησης 
4.3.2 ∆ιαδικασία τµηµατοποίησης µε χρήση του αποτελέσµατος 
κατηγοριοποίησης.  
 
Η διαδικασία τµηµατοποίησης καθενός από τα 25 κείµενα για κάθε ένα από τα δέκα σύνολα 
περιλαµβάνει τα ακόλουθα βήµατα: 
Βήµα 1ο: Υπολογισµός, για το σύνολο κειµένων που εξετάζουµε, των ποσοτήτων  και 
 σύµφωνα µε την εξίσωση (4.4) για τις λέξεις και τις έννοιες που εµφανίζονται στα 
δεδοµένα προς εκπαίδευση του εν λόγω συνόλου και για τις δυο κατηγορίες. 
)|(
^
kmjr cxP
)|(
^
kmjr csP
Βήµα 2ο: Εφαρµογή µιας από τις τέσσερις συνολικά παραλλαγές του αλγορίθµου PREMMON - µε 
εναλλακτική χρήση της τεχνικής ψηφοφορίας - οι οποίες κάνουν χρήση των υπολογιζόµενων 
πιθανοτήτων του προηγούµενου βήµατος για τις λέξεις-έννοιες οι οποίες εµφανίζονται στα κείµενα 
προς τµηµατοποίηση του εν λόγω συνόλου. Το αποτέλεσµα αυτής της εφαρµογής οδηγεί στην 
εύρευση του τρόπου µεταβολής των κατηγοριών για κάθε µια από τις λέξεις – έννοιες του εκάστοτε 
κειµένου. 
Βήµα 3ο: Εξέταση για κάθε µια από τις παραγράφους του κειµένου της υπερισχύουσας κατηγορίας 
αυτής όπως αυτή προκύπτει από την εξέταση της κατηγορίας στην οποία ανήκει κάθε µια λέξη που 
εµφανίζεται στην εν λόγω παράγραφο και την εύρεση της συνολικά επικρατέστερης. 
Βήµα 4ο: Εξέταση του τρόπου µεταβολής των κατηγοριών των παραγράφων οι οποίες συνιστούν το 
εκάστοτε κείµενο προς τµηµατοποίηση. Τα σηµεία εκείνα στα οποία παρατηρείται µεταβολή στην 
υπερισχύουσα κατηγορία αποτελούν την έναρξη ενός νέου τµήµατος.  
Για την αξιολόγηση της απόδοσης των µεθόδων που παρουσιάστηκαν προηγούµενα στο 
πρόβληµα της τµηµατοποίησης το οποίο ορίσαµε αλλά κυρίως της περαιτέρω εξέτασης του βαθµού 
συνεισφοράς των εννοιών έναντι των λέξεων, πραγµατοποιήσαµε τρεις σειρές πειραµάτων. Και στις 
τρεις σειρές πειραµάτων χρησιµοποιούνται τα 250 κείµενα προς τµηµατοποίηση που περιέχονται στα 
δέκα σύνολα κειµένων Set0, Set1, …, Set9.  Η διαφορά  µεταξύ των τριών αυτών σειρών έγκειται στο 
ποιες µέθοδοι εξετάστηκαν ως προς την απόδοση τµηµατοποίησης και κατά πόσο αυτές έκαναν χρήση 
ή όχι της τεχνικής «ψηφοφορίας». Στις ενότητες που ακολουθούν παρουσιάζονται διεξοδικά και οι 
τρεις σειρές πειραµάτων καθώς επίσης και το κριτήριο σύµφωνα µε το οποίο υπολογίζεται η απόδοση 
τµηµατοποίησης. 
 108
Κεφάλαιο 4 Τµηµατοποίηση Κειµένων βάσει του αποτελέσµατος κατηγοριοποίησης 
4.3.3 Πρώτη σειρά πειραµάτων 
 
Η πρώτη σειρά πειραµάτων εξετάζει την απόδοση τµηµατοποίησης όταν χρησιµοποιούνται η 
πρώτη και η δεύτερη εναλλακτική µορφή του αλγορίθµου PREMONN µε και χωρίς την εφαρµογή της 
τεχνικής της ψηφοφορίας. Στην παράµετρο α της εξίσωσης (4.4) αποδόθηκε τιµή ίση µε 0.25. Η 
επιλογή της εν λόγω τιµής προέκυψε ύστερα από αξιολόγηση της απόδοσης µε χρήση διαφόρων τιµών 
της παραµέτρου όπως 0.25, 0.5, 0.75 και 1. Στην πρώτη εναλλακτική µορφή του αλγορίθµου 
PREMONN η παράµετρος h έλαβε την τιµή 0.000001, ενώ στην δεύτερη εναλλακτική µορφή η 
παράµετρος l έλαβε τιµή ίση µε 0.01 ενώ η παράµετρος r τιµές από το διάστηµα [0.25, 0.75]. Από τα 
πειράµατα τα οποία πραγµατοποιήθηκαν προέκυψε ότι η τιµή της παραµέτρου r η οποία επιτυγχάνει 
τα καλύτερα αποτελέσµατα είναι η 0.5. Ο Πίνακας 4.2 παραθέτει τα αποτελέσµατα τµηµατοποίησης, 
µετρούµενα µε την βοήθεια του κριτηρίου του Beeferman, τόσο για την περίπτωση όπου 
χρησιµοποιούνται οι λέξεις όσο και όταν αυτές αντικαθίστανται από τις αντίστοιχές τους έννοιες. 
Από τον Πίνακα 4.2 προκύπτουν τα ακόλουθα συµπεράσµατα: 
1.  Η τεχνική της ψηφοφορίας αποδεικνύεται και στις 2 εναλλακτικές µορφές του PREMONN -τόσο 
στην περίπτωση που χρησιµοποιούνται οι λέξεις όσο και στην περίπτωση που χρησιµοποιούνται οι 
έννοιες - ότι δεν βελτιώνει τα αποτελέσµατα συνολικά (δηλαδή και στα 10 σύνολα) αλλά µόνο σε 
µεµονωµένες περιπτώσεις (στην 1η εναλλακτική µορφή στις 3 από τις 10 περιπτώσεις τόσο στην 
περίπτωση των λέξεων όσο και στην περίπτωση των εννοιών, ενώ στην 2η εναλλακτική µορφή του 
PREMONN βελτιώνει το αποτέλεσµα στις 4 από τις 10 περιπτώσεις τόσο στην περίπτωση των λέξεων 
όσο και σε αυτή των εννοιών). 
2.  Συνολικά στα 7 από τα 10 σύνολα η 1η εναλλακτική µορφή του PREMONN – τόσο όταν 
χρησιµοποιούνται οι λέξεις όσο και όταν χρησιµοποιούνται οι έννοιες - δίχως την τεχνική της 
ψηφοφορίας επιτυγχάνει καλύτερη απόδοση από ότι επιτυγχάνει η 2η εναλλακτική µορφή του 
PREMONN δίχως την τεχνική της ψηφοφορίας και συνολικά σε όλα τα σύνολα την καλύτερη 
απόδοση. 
3.  Τα αποτελέσµατα τα οποία προκύπτουν από την 1η εναλλακτική µορφή του PREMONN µε και 
χωρίς την τεχνική της ψηφοφορίας είναι συνολικά (µέσος όρος) καλύτερα από τα αντίστοιχα της 2ης 
εναλλακτικής µορφής του PREMONN και για την περίπτωση των λέξεων όσο και για την περίπτωση 
των εννοιών.  
4.  Το εύρος των τιµών απόδοσης του κριτηρίου του Beeferman είναι αναµενόµενο, λόγω της 
δυσκολίας του προβλήµατος που εξετάζουµε καθώς και της φύσης του αλγορίθµου PREMONN, η 
 109
Κεφάλαιο 4 Τµηµατοποίηση Κειµένων βάσει του αποτελέσµατος κατηγοριοποίησης 
οποία παρουσιάζει ένα είδους «χρονικής καθυστέρησης» στην εύρεση της κατηγορίας στην οποία 
ανήκει κάθε λέξη του κειµένου άρα και κατ’ επέκταση στον διαχωρισµό των κατηγοριών και στην 
τοποθέτηση των ορίων µεταξύ των τµηµάτων. 
Αξίζει να σηµειωθεί ότι, κατά την εφαρµογή της 1ης εναλλακτικής µορφής του αλγορίθµου 
PREMONN δίχως την εφαρµογή της τεχνικής της ψηφοφορίας, η απόδοση τµηµατοποίησης µε χρήση 
των εννοιών υπερίσχυσε στα 131 κείµενα, η αντίστοιχη µε χρήση των λέξεων στα 111 κείµενα ενώ σε 
8 από τα συνολικά 250 κείµενα και των δέκα συνόλων η απόδοση τµηµατοποίησης προέκυψε η ίδια. 
Κατά την εφαρµογή της 1ης εναλλακτικής µορφής του PREMONN σε συνδυασµό αυτή τη φορά µε 
την τεχνική της ψηφοφορίας προέκυψε ότι, η απόδοση τµηµατοποίησης µε χρήση των λέξεων 
υπερίσχυσε σε 128 κείµενα, η αντίστοιχη µε χρήση των εννοιών σε 117 κείµενα ενώ σε 5 από τα 
συνολικά 250 κείµενα  και των δέκα συνόλων η απόδοση τµηµατοποίησης προέκυψε η ίδια.  Εντελώς 
αντίστοιχα, κατά την εφαρµογή της 2ης εναλλακτικής µορφής του αλγορίθµου PREMONN δίχως την 
εφαρµογή της τεχνικής της ψηφοφορίας, η απόδοση τµηµατοποίησης µε χρήση των εννοιών 
υπερίσχυσε στα 167 κείµενα, η αντίστοιχη µε χρήση των λέξεων στα 76 κείµενα ενώ σε 7 από τα 
συνολικά 250 κείµενα και των δέκα συνόλων η απόδοση τµηµατοποίησης προέκυψε η ίδια. Κατά την 
εφαρµογή της 2ης εναλλακτικής µορφής του PREMONN σε συνδυασµό αυτή τη φορά µε την τεχνική 
της ψηφοφορίας προέκυψε ότι, η απόδοση τµηµατοποίησης µε χρήση των λέξεων υπερίσχυσε σε 172 
κείµενα, η αντίστοιχη µε χρήση των εννοιών σε 70 κείµενα ενώ σε 8 από τα συνολικά 250 κείµενα και 
των δέκα συνόλων η απόδοση τµηµατοποίησης προέκυψε η ίδια. 
 
 
 
 
 
 
 
 
 
 
 110
Κεφάλαιο 4 Τµηµατοποίηση Κειµένων βάσει του αποτελέσµατος κατηγοριοποίησης 
Sets 2η εναλ. Μορ. 
PREMONN µε 
Ψηφοφορία  
2η εναλ. Μορ. 
PREMONN χωρίς 
Ψηφοφορία 
1η εναλ. Μορ. 
PREMONN µε 
Ψηφοφορία 
1η εναλ. Μορ. 
PREMONN χωρίς  
Ψηφοφορία 
Set0 Λέξεις 20.15% 20.80% 25.67% 16.46% 
Set0 Έννοιες 27.03% 26.10% 22.71% 19.46% 
Set1 Λέξεις 37.25% 37.25% 30.80% 30.15% 
Set1 Έννοιες 32.29% 32.66% 36.13% 30.74% 
Set2 Λέξεις 42.85% 43.09% 40.12% 39.42% 
Set2 Έννοιες 35.82% 37.00% 44.14% 40.34% 
Set3 Λέξεις 28.75% 26.03% 28.52% 29.29% 
Set3 Έννοιες 41.68% 44.09% 40.94% 49.03% 
Set4 Λέξεις 35.08% 34.71% 30.63% 28.95% 
Set4 Έννοιες 28.15% 27.81% 35.77% 22.43% 
Set5 Λέξεις 22.80% 21.32% 4.60% 3.65% 
Set5 Έννοιες 21.37% 21.31% 12.18% 12.46% 
Set6 Λέξεις 28.51% 28.47% 14.76% 11.54% 
Set6 Έννοιες 26.65% 27.89% 9.87% 9.98% 
Set7 Λέξεις 34.74% 35.57% 40.39% 41.01% 
Set7 Έννοιες 31.14% 29.40% 29.08% 25.17% 
Set8 Λέξεις 31.08% 29.89% 24.68% 25.90% 
Set8 Έννοιες 23.10% 22.39% 11.49% 4.82% 
Set9 Λέξεις 26.59% 25.23% 22.46% 13.93% 
Set9 Έννοιες 26.20% 26.11% 26.66% 17.35% 
AVG Λέξεις 30.58% 30.24% 26.26% 24.03% 
AVG Έννοιες 29.34% 29.48% 26.90% 23.18% 
Πίνακας 4.2: Οι τιµές του κριτηρίου  του Beeferman για τα σύνολα Set0, …, Set9 όπως αυτές ελήφθησαν 
µετά από την εφαρµογή της 1
kP
ης και της 2ης εναλλακτικής µορφής του αλγορίθµου του PREMONN, µε και χωρίς 
την εφαρµογή της τεχνικής της «ψηφοφορίας» τόσο για την περίπτωση όπου χρησιµοποιούνται οι λέξεις όσο και 
για την περίπτωση όπου χρησιµοποιούνται οι έννοιες των κειµένων. 
 
 111
Κεφάλαιο 4 Τµηµατοποίηση Κειµένων βάσει του αποτελέσµατος κατηγοριοποίησης 
4.3.4 ∆εύτερη σειρά πειραµάτων 
 
Η δεύτερη σειρά πειραµάτων εξετάζει την απόδοση τµηµατοποίησης όταν χρησιµοποιούνται 
η 1η και η 2η εναλλακτική µορφή του PREMONN, µε και χωρίς την εφαρµογή της τεχνικής 
ψηφοφορίας, σε συνδυασµό αυτή τη φορά µε τον 2ο παράγοντα εξοµάλυνσης ο οποίος παρουσιάστηκε 
στην παράγραφο 4.2.2.2 και πιο συγκεκριµένα στην εξίσωση (4.10). Οι τιµές των παραµέτρων που 
χρησιµοποιήθηκαν για τις δυο εναλλακτικές µορφές του PREMONN αλλά και για την παράµετρο της 
εξίσωσης (4.4) είναι ίδιες µε αυτές της 1ης σειράς πειραµάτων. Τέλος για την παράµετρο της εξίσωσης 
(4.10) χρησιµοποιήθηκε η τιµή 0.9. Ο Πίνακας 4.3 παραθέτει τα αποτελέσµατα της απόδοσης 
τµηµατοποίησης – µετρούµενα µε τη βοήθεια του κριτηρίου  του Beeferman- τόσο για την 
περίπτωση όπου χρησιµοποιούνται οι αυτούσιες λέξεις των κειµένων όσο και όταν αυτές 
αντικαθίστανται από τις αντίστοιχές τους έννοιες.  
kP
 Από τον Πίνακα 4.3 προκύπτουν τα ακόλουθα συµπεράσµατα: Πρώτα από όλα, η τεχνική της 
ψηφοφορίας σε συνδυασµό µε τον παράγοντα εξοµάλυνσης αποδεικνύεται ευεργετική µόνο στην 2η 
εναλλακτική µορφή του αλγορίθµου PREMONN έναντι της αντίστοιχής της δίχως την εφαρµογή της 
τεχνικής ψηφοφορίας. Πιο συγκεκριµένα, ο εν λόγω συνδυασµός επιτυγχάνει καλύτερη απόδοση 
τµηµατοποίησης στα οκτώ από τα δέκα σύνολα κειµένων όταν χρησιµοποιούνται οι λέξεις των 
κειµένων και στα έξη από τα δέκα σύνολα όταν χρησιµοποιούνται οι έννοιες αντί οι λέξεις των 
κειµένων. Παρόλα αυτά, ο συνδυασµός της τεχνικής της ψηφοφορίας µε τον παράγοντα εξοµάλυνσης, 
όπως αυτός δίνεται από την εξίσωση (4.10) στην 1η εναλλακτική µορφή του αλγορίθµου δεν 
αποδεικνύεται ιδιαίτερα αποτελεσµατικός έναντι εκείνου που δεν κάνει χρήση της τεχνικής 
ψηφοφορίας. Το γεγονός αυτό διαφαίνεται από το ότι επιτυγχάνει καλύτερη απόδοση τµηµατοποίησης 
σε δυο µόνο από τα δέκα σύνολα όταν χρησιµοποιούνται οι λέξεις του κειµένου και σε ένα µόνο από 
τα δέκα  σύνολα όταν χρησιµοποιούνται οι έννοιες του κειµένου. 
 
 
 
 
 
 
 112
Κεφάλαιο 4 Τµηµατοποίηση Κειµένων βάσει του αποτελέσµατος κατηγοριοποίησης 
Sets 2  εναλ. Μορ. 
PREMONN µε 
Ψηφοφορία και 
2  παραγ. Εξοµ 
η
ος
η
ος
1  εναλ. Μορ. 
PREMONN µε 
Ψηφοφορία και 2  
παραγ. Εξοµ. 
η
ος
2  εναλ. Μορ. 
PREMONN 
χωρίς 
Ψηφοφορία και 
2  παραγ. Εξοµ 
1 εναλ. Μορ. 
PREMONN χωρίς  
Ψηφοφορία και 2  
παραγ. Εξοµ. 
η 
ος
Set0 Λέξεις 20.51% 21.44% 29.22% 26.51% 
Set0 Έννοιες 28.28% 27.39% 29.98% 27.44% 
Set1 Λέξεις 36.66% 37.40% 34.78% 31.24% 
Set1 Έννοιες 32.11% 33.16% 35.93% 31.82% 
Set2 Λέξεις 43.39% 44.21% 44.20% 35.74% 
Set2 Έννοιες 34.66% 36.32% 39.88% 
Set3 Λέξεις 45.61% 46.48% 42.61% 45.34% 
Set3 Έννοιες 44.46% 43.36% 44.53% 45.90% 
Set4 Λέξεις 31.92% 34.02% 31.05% 30.73% 
Set4 Έννοιες 26.48% 26.88% 33.71% 25.45% 
Set5 Λέξεις 20.51% 21.44% 29.22% 26.51% 
Set5 Έννοιες 28.28% 27.39% 29.98% 27.44% 
Set6 Λέξεις 28.14% 27.65% 15.99% 12.64% 
Set6 Έννοιες 25.42% 26.50% 12.23% 10.87% 
Set7 Λέξεις 39.13% 32.61% 38.86% 39.87% 
Set7 Έννοιες 28.77% 31.52% 32.48% 19.92% 
Set8 Λέξεις 32.51% 33.54% 37.70% 28.34% 
Set8 Έννοιες 22.47% 23.88% 12.02% 3.95% 
Set9 Λέξεις 24.13% 25.89% 26.27% 20.62% 
Set9 Έννοιες 25.69% 26.43% 24.21% 21.56% 
AVG Λέξεις 32.25% 32.47% 32.99% 29.75% 
AVG Έννοιες 29.66% 30.28% 30.31% 25.42% 
48.08% 
Πίνακας 4.3: Οι τιµές του κριτηρίου  του Beeferman για τα σύνολα Set0, …, Set9 όπως αυτές ελήφθησαν 
µετά από την εφαρµογή της 1
kP
ης και της 2ης εναλλακτικής µορφής του αλγορίθµου του PREMONN σε συνδυασµό 
µε τον 2ο παράγοντα εξοµάλυνσης, µε και χωρίς την εφαρµογή της τεχνικής της ψηφοφορίας τόσο για την 
 113
Κεφάλαιο 4 Τµηµατοποίηση Κειµένων βάσει του αποτελέσµατος κατηγοριοποίησης 
περίπτωση όπου χρησιµοποιούνται οι λέξεις όσο και για την περίπτωση όπου χρησιµοποιούνται οι έννοιες των 
κειµένων. 
 
Επιπρόσθετα, τα αποτελέσµατα τα οποία προέκυψαν από την 1η εναλλακτική µορφή του 
PREMONN δίχως την εφαρµογή της τεχνικής της ψηφοφορίας είναι κατά µέσο όρο καλύτερα από 
αυτά της δεύτερης εναλλακτικής µορφής δίχως την εφαρµογή της τεχνικής ψηφοφορίας, τόσο για την 
περίπτωση όπου χρησιµοποιούνται οι λέξεις όσο και οι έννοιες του κειµένου. ∆εν ισχύει όµως το ίδιο 
όπου εφαρµόζεται η τεχνική της ψηφοφορίας στην 1η εναλλακτική µορφή του PREMONN τόσο για 
την περίπτωση όπου χρησιµοποιούνται οι λέξεις όσο και οι έννοιες των κειµένων συγκριτικά µε την 2η 
εναλλακτική µορφή και τον συνδυασµό αυτής µε την τεχνική της ψηφοφορίας. Αξίζει να σηµειωθεί 
ότι, η συνολικά βέλτιστη – κατά µέσο όρο – απόδοση τµηµατοποίησης επιτυγχάνεται τόσο για τις 
λέξεις όσο και για τις έννοιες των κειµένων από την 1η εναλλακτική µορφή του PREMONN  δίχως την 
εφαρµογή της τεχνικής ψηφοφορίας, όπως ακριβώς και στην 1η σειρά πειραµάτων.  
Τέλος, από τους Πίνακες 4.2 και 4.3 παρατηρούµε ότι, η εφαρµογή της τεχνικής του 2ου 
παράγοντα εξοµάλυνσης επιτυγχάνει συνολικά χειρότερη απόδοση τµηµατοποίησης από αυτή που 
λαµβάνουµε στην 1η σειρά πειραµάτων. Αυτό οφείλεται στο γεγονός ότι, παρά το ότι ο 2ος παράγοντας 
εξοµάλυνσης εξοµαλύνει τις διαφορές των πιθανοτήτων καθεµίας λέξης – έννοιας του εκάστοτε 
κειµένου, αποδεικνύεται ότι δρα ως παράγοντας καθυστέρησης σε ότι αφορά την «διάδοση» της 
αλλαγής κατηγορίας. 
4.3.5 Τρίτη σειρά πειραµάτων 
 
Η τελευταία σειρά πειραµάτων εξετάζει την απόδοση τµηµατοποίησης όταν χρησιµοποιούνται 
η 1η και η 2η εναλλακτική µορφή του PREMONN µε και χωρίς την εφαρµογή της τεχνικής της 
ψηφοφορίας σε συνδυασµό µε τον 1ο παράγοντα εξοµάλυνσης που υλοποιεί την τεχνική του 
«Κινούµενου Παραθύρου» (“sliding window”, [Gloss(00079)]) όπως αυτή παρουσιάστηκε στην 
παράγραφο 4.2.2.2 και πιο συγκεκριµένα στην εξίσωση (4.9). Οι τιµές των παραµέτρων που 
χρησιµοποιήθηκαν για την παράµετρο της εξίσωσης (4.4) καθώς και αυτές που αφορούν τις δυο 
εναλλακτικές µορφές του PREMONN είναι ίδιες µε αυτές της 1ης σειράς πειραµάτων. Επιπρόσθετα, 
για την παράµετρο που εµφανίζεται στην εξίσωση (4.10) και αφορά το εύρος του παραθύρου για τα 
πειράµατά µας χρησιµοποιήσαµε τις τιµές 5, 10, 15, 20 και 25.  
 114
Κεφάλαιο 4 Τµηµατοποίηση Κειµένων βάσει του αποτελέσµατος κατηγοριοποίησης 
 Από τα πειράµατα τα οποία πραγµατοποιήθηκαν στην 3η σειρά πειραµάτων κάνοντας χρήση 
του 1ου παράγοντα εξοµάλυνσης, µε όλες τις δυνατές τιµές των παραµέτρων του και στις δυο 
εναλλακτικές µορφές του PREMONN µε και χωρίς την εφαρµογή της τεχνικής της ψηφοφορίας 
καταλήξαµε στα ακόλουθα συµπεράσµατα: 
1.  Ο µέσος όρος της απόδοσης τµηµατοποίησης των δέκα συνόλων τόσο για την περίπτωση όπου 
χρησιµοποιούνται οι λέξεις όσο και για την περίπτωση όπου χρησιµοποιούνται οι έννοιες είναι 
καλύτερος όταν δεν χρησιµοποιείται ο 1ος παράγοντας εξοµάλυνσης. Μοναδική εξαίρεση αποτελεί η 
απόδοση τµηµατοποίησης η οποία επιτυγχάνεται από την 2η εναλλακτική µορφή του PREMONN σε 
συνδυασµό µε την τεχνική της ψηφοφορίας αλλά και του 1ου παράγοντα εξοµάλυνσης, όταν το πλάτος 
παραθύρου ισούται µε 15, 20 και 25. Πιο συγκεκριµένα, όσο αυξάνεται το εύρος του παραθύρου – 
όπως διαφαίνεται από τους Πίνακες 4.4 – 4.8 αλλά και από τον Πίνακα 4.9 που περιέχει τα 
συγκεντρωτικά αποτελέσµατα και των τριών σειρών πειραµάτων – τόσο µειώνεται η απόδοση 
τµηµατοποίησης. Η εν λόγω µείωση σε ορισµένες περιπτώσεις είναι µικρή και ισχύει και για την 
περίπτωση όπου χρησιµοποιούνται οι έννοιες του κειµένου εκτός από την περίπτωση της 2ης 
εναλλακτικής µορφής του PREMONN σε συνδυασµό µε την τεχνική της ψηφοφορίας.  
2. Η διαφορά στην απόδοση µε την εφαρµογή του 1ου παράγοντα εξοµάλυνσης (ανεξαρτήτου τιµής 
πλάτους Κινούµενου Παραθύρου) σε επίπεδο συνόλου κειµένων είναι τις περισσότερες φορές 
µικρότερη του 3% (µετρούµενη ως προς το κριτήριο  του Beeferman, και ισχύει τόσο για την 
περίπτωση των λέξεων όσο και για την περίπτωση των εννοιών του κειµένου). Η εν λόγω διαφορά 
γίνεται περισσότερο αισθητή στην περίπτωση όπου χρησιµοποιείται η 2
kP
η εναλλακτική µορφή του 
αλγορίθµου PREMONN – ιδιαίτερα σε συνδυασµό µε την τεχνική της ψηφοφορίας – και λιγότερο 
στην αντίστοιχη της 1η εναλλακτικής µορφής. 
3. Η βέλτιστη απόδοση τµηµατοποίησης επιτυγχάνεται, κατά µέσο όρο, χρησιµοποιώντας την 1η 
εναλλακτική µορφή του αλγορίθµου PREMONN δίχως την τεχνική της ψηφοφορίας τόσο για την 
περίπτωση όπου χρησιµοποιούνται οι λέξεις όσο και για την περίπτωση όπου χρησιµοποιούνται οι 
έννοιες του κειµένου και για πλάτος παραθύρου ίσο µε 5. Η χείριστη κατά µέσο όρο απόδοση 
τµηµατοποίησης, από την άλλη πλευρά, προκύπτει από την εφαρµογή της 1ης εναλλακτικής µορφής 
του PREMONN σε συνδυασµό µε την τεχνική της ψηφοφορίας µε εφαρµογή του 1ου παράγοντα 
εξοµάλυνσης για πλάτος παραθύρου ίσο µε 25, τόσο για την περίπτωση όπου χρησιµοποιούνται οι 
λέξεις όσο και για την περίπτωση όπου χρησιµοποιούνται οι έννοιες ενός κειµένου προς 
τµηµατοποίηση. 
Στην συνέχεια παραθέτονται οι Πίνακες 4.4 ως και 4.8 οι οποίοι περιέχουν τα αποτελέσµατα 
που προκύπτουν από την εφαρµογή της 1ης και της 2ης εναλλακτικής µορφής του αλγορίθµου 
PREMONN σε συνδυασµό µε τον 1ο παράγοντα εξοµάλυνσης µε και χωρίς την εφαρµογή της τεχνικής 
 115
Κεφάλαιο 4 Τµηµατοποίηση Κειµένων βάσει του αποτελέσµατος κατηγοριοποίησης 
της ψηφοφορίας τόσο για την περίπτωση όπου χρησιµοποιούνται οι λέξεις όσο και για την περίπτωση 
όπου χρησιµοποιούνται οι έννοιες των κειµένων, για τιµές πλάτους παραθύρου ίσες µε 5, 10, 15, 20 
και 25. Ο Πίνακας 4.9 περιέχει τους µέσους όρους των αποτελεσµάτων εκφρασµένες ως προς το 
κριτήριο  του Beeferman για τα σύνολα Set0, …, Set9 όπως αυτές ελήφθησαν µετά από την 
εφαρµογή της 1
kP
ης και της 2ης εναλλακτικής µορφής του αλγορίθµου του PREMONN µε και χωρίς την 
εφαρµογή της τεχνικής της ψηφοφορίας τόσο για την περίπτωση όπου χρησιµοποιούνται οι λέξεις όσο 
και για την περίπτωση όπου χρησιµοποιούνται οι έννοιες των κειµένων, όπως αυτά προκύπτουν και 
από τις τρεις σειρές πειραµάτων. Το Σχήµα 4.1 παριστά την πληροφορία η οποία περιέχεται στον 
Πίνακα 4.9. Από τον Πίνακα 4.9 (που παραθέτεται στην σελίδα 119) προκύπτει ότι η καλύτερη 
απόδοση τµηµατοποίησης (η οποία προκύπτει από την µικρότερη τιµή ως προς το κριτήριο  του 
Beeferman, δες σελίδα 87) επιτυγχάνεται µε την εφαρµογή της 1
kP
ης εναλλακτικής µορφής του 
αλγορίθµου του PREMONN χωρίς την εφαρµογή της τεχνικής της ψηφοφορίας τόσο για την 
περίπτωση όπου χρησιµοποιούνται οι λέξεις όσο και για την περίπτωση όπου χρησιµοποιούνται οι 
έννοιες των κειµένων στην 1η σειρά πειραµάτων.  
 
 
 
 
 
 
 
 
 
 
 
 
 
 116
Κεφάλαιο 4 Τµηµατοποίηση Κειµένων βάσει του αποτελέσµατος κατηγοριοποίησης 
Sets 2η εναλ. Μορ. 
PREMONN µε 
Ψηφοφορία & 1ος 
παρ. Εξοµ. W=5 
2η εναλ. Μορ. 
PREMONN χωρίς 
Ψηφοφορία & 1ος 
παρ. Εξοµ. W=5 
1η εναλ. Μορ. 
PREMONN µε 
Ψηφοφορία & 1ος 
παρ. Εξοµ. W=5 
1η εναλ. Μορ. 
PREMONN χωρίς  
Ψηφοφορία & 1ος 
παρ. Εξοµ. W=5 
Set0 Λέξεις 21.94% 21.93% 30.12% 28.95% 
Set0 Έννοιες 26.48% 27.17% 30.48% 27.49% 
Set1 Λέξεις 35.57% 36.83% 33.73% 31.27% 
Set1 Έννοιες 32.32% 32.76% 36.98% 32.41% 
Set2 Λέξεις 42.16% 43.54% 42.42% 41.28% 
Set2 Έννοιες 35.67% 37.24% 47.76% 40.17% 
Set3 Λέξεις 45.48% 45.97% 40.14% 41.69% 
Set3 Έννοιες 39.26% 44.47% 45.45% 46.79% 
Set4 Λέξεις 33.50% 34.23% 31.12% 36.15% 
Set4 Έννοιες 27.43% 26.38% 34.58% 24.85% 
Set5 Λέξεις 21.94% 21.93% 30.12% 28.95% 
Set5 Έννοιες 26.48% 27.17% 30.48% 27.49% 
Set6 Λέξεις 28.59% 28.57% 16.27% 12.07% 
Set6 Έννοιες 27.29% 26.57% 15.64% 9.67% 
Set7 Λέξεις 35.56% 35.63% 43.63% 34.36% 
Set7 Έννοιες 31.41% 31.95% 29.78% 20.28% 
Set8 Λέξεις 32.26% 29.79% 28.72% 24.98% 
Set8 Έννοιες 24.10% 20.66% 16.57% 5.61% 
Set9 Λέξεις 24.22% 24.94% 20.38% 17.47% 
Set9 Έννοιες 25.93% 26.54% 24.57% 19.14% 
AVG Λέξεις 32.12% 32.34% 31.66% 29.72% 
AVG Έννοιες 29.64% 30.09% 31.23% 25.39% 
Πίνακας 4.4: Οι τιµές του κριτηρίου  του Beeferman για όλα τα σύνολα όπως αυτές ελήφθησαν από την 1kP η 
και την 2η εναλλακτική µορφή του αλγορίθµου PREMONN σε συνδυασµό µε τον 1ο παράγοντα εξοµάλυνσης µε 
πλάτος παραθύρου ίσο µε 5, µε και χωρίς την εφαρµογή της τεχνικής της ψηφοφορίας τόσο για την περίπτωση 
όπου χρησιµοποιούνται οι λέξεις όσο και για την περίπτωση όπου χρησιµοποιούνται οι έννοιες των κειµένων. 
 
 117
Κεφάλαιο 4 Τµηµατοποίηση Κειµένων βάσει του αποτελέσµατος κατηγοριοποίησης 
Sets 2η εναλ. Μορ. 
PREMONN µε 
Ψηφοφορία & 1ος 
παρ. Εξοµ. W=10 
2η εναλ. Μορ. 
PREMONN χωρίς 
Ψηφοφορία & 1ος παρ. 
Εξοµ. W=10 
1η εναλ. Μορ. 
PREMONN µε 
Ψηφοφορία & 1ος 
παρ. Εξοµ. W=10 
1η εναλ. Μορ. 
PREMONN χωρίς  
Ψηφοφορία & 1ος 
παρ. Εξοµ. W=10 
Set0 Λέξεις 20.51% 21.44% 29.22% 26.51% 
Set0 Έννοιες 28.28% 27.39% 29.98% 27.44% 
Set1 Λέξεις 36.65% 37.40% 34.78% 31.24% 
Set1 Έννοιες 32.11% 33.16% 35.93% 31.82% 
Set2 Λέξεις 43.39% 44.21% 44.20% 35.74% 
Set2 Έννοιες 34.66% 36.32% 48.08% 39.88% 
Set3 Λέξεις 45.61% 46.48% 42.61% 45.34% 
Set3 Έννοιες 44.46% 43.36% 44.53% 45.90% 
Set4 Λέξεις 31.92% 34.02% 31.05% 30.73% 
Set4 Έννοιες 26.48% 26.88% 33.71% 25.45% 
Set5 Λέξεις 20.51% 21.44% 29.22% 26.51% 
Set5 Έννοιες 28.28% 27.39% 29.98% 27.44% 
Set6 Λέξεις 28.14% 27.65% 15.99% 12.64% 
Set6 Έννοιες 25.42% 26.50% 12.23% 10.87% 
Set7 Λέξεις 39.13% 32.61% 38.86% 39.87% 
Set7 Έννοιες 28.77% 31.52% 32.48% 19.92% 
Set8 Λέξεις 32.51% 33.54% 37.70% 28.34% 
Set8 Έννοιες 22.47% 23.88% 12.02% 3.95% 
Set9 Λέξεις 24.13% 25.89% 26.27% 20.62% 
Set9 Έννοιες 25.69% 26.43% 24.21% 21.56% 
AVG Λέξεις 32.25% 32.47% 32.99% 29.75% 
AVGΈννοιες 29.66% 30.28% 30.31% 25.42% 
Πίνακας 4.5: Οι τιµές του κριτηρίου  του Beeferman για όλα τα σύνολα όπως αυτές ελήφθησαν από την 1kP η 
και την 2η εναλλακτική µορφή του αλγορίθµου PREMONN σε συνδυασµό µε τον 1ο παράγοντα εξοµάλυνσης µε 
πλάτος παραθύρου ίσο µε 10, µε και χωρίς την εφαρµογή της τεχνικής της ψηφοφορίας τόσο για την περίπτωση 
όπου χρησιµοποιούνται οι λέξεις όσο και για την περίπτωση όπου χρησιµοποιούνται οι έννοιες των κειµένων. 
 
 118
Κεφάλαιο 4 Τµηµατοποίηση Κειµένων βάσει του αποτελέσµατος κατηγοριοποίησης 
Sets 2η εναλ. Μορ. 
PREMONN µε 
Ψηφοφορία & 1ος 
παρ. Εξοµ. W=15 
2η εναλ. Μορ. 
PREMONN χωρίς 
Ψηφοφορία & 1ος 
παρ. Εξοµ. W=15 
1η εναλ. Μορ. 
PREMONN µε 
Ψηφοφορία & 1ος 
παρ. Εξοµ. W=15 
1η εναλ. Μορ. 
PREMONN χωρίς  
Ψηφοφορία & 1ος 
παρ. Εξοµ. W=15 
Set0 Λέξεις 20.96% 21.22% 29.24% 28.76% 
Set0 Έννοιες 29.56% 27.91% 28.15% 27.32% 
Set1 Λέξεις 33.05% 37.40% 34.33% 31.80% 
Set1 Έννοιες 30.80% 33.26% 35.61% 32.20% 
Set2 Λέξεις 43.14% 44.97% 40.41% 37.69% 
Set2 Έννοιες 35.41% 36.20% 46.43% 36.58% 
Set3 Λέξεις 46.36% 46.37% 45.55% 45.61% 
Set3 Έννοιες 47.99% 42.91% 47.58% 46.65% 
Set4 Λέξεις 30.83% 33.85% 27.60% 30.87% 
Set4 Έννοιες 25.53% 26.64% 32.80% 26.24% 
Set5 Λέξεις 20.96% 21.22% 29.24% 28.76% 
Set5 Έννοιες 29.56% 27.91% 28.15% 27.32% 
Set6 Λέξεις 25.75% 28.78% 17.87% 15.32% 
Set6 Έννοιες 20.02% 27.62% 15.22% 16.29% 
Set7 Λέξεις 43.88% 33.21% 37.41% 39.23% 
Set7 Έννοιες 25.28% 31.27% 32.65% 20.64% 
Set8 Λέξεις 33.12% 33.81% 35.82% 34.91% 
Set8 Έννοιες 20.39% 24.03% 17.51% 11.18% 
Set9 Λέξεις 23.94% 25.88% 27.46% 19.93% 
Set9 Έννοιες 25.24% 26.36% 22.84% 21.02% 
AVG Λέξεις 32.20% 32.67% 32.49% 31.29% 
AVGΈννοιες 28.98% 30.41% 30.69% 26.54% 
Πίνακας 4.6: Οι τιµές του κριτηρίου  του Beeferman για όλα τα σύνολα όπως αυτές ελήφθησαν από την 1kP η 
και την 2η εναλλακτική µορφή του αλγορίθµου PREMONN σε συνδυασµό µε τον 1ο παράγοντα εξοµάλυνσης µε 
πλάτος παραθύρου ίσο µε 15, µε και χωρίς την εφαρµογή της τεχνικής της ψηφοφορίας τόσο για την περίπτωση 
όπου χρησιµοποιούνται οι λέξεις όσο και για την περίπτωση όπου χρησιµοποιούνται οι έννοιες των κειµένων. 
 
 119
Κεφάλαιο 4 Τµηµατοποίηση Κειµένων βάσει του αποτελέσµατος κατηγοριοποίησης 
Sets 2η εναλ. Μορ. 
PREMONN µε 
Ψηφοφορία & 1ος 
παρ. Εξοµ. W=20 
2η εναλ. Μορ. 
PREMONN χωρίς 
Ψηφοφορία & 1ος 
παρ. Εξοµ. W=20 
1η εναλ. Μορ. 
PREMONN µε 
Ψηφοφορία & 1ος 
παρ. Εξοµ. W=20 
1η εναλ. Μορ. 
PREMONN χωρίς  
Ψηφοφορία & 1ος 
παραγ. Εξοµ. W=20 
Set0 Λέξεις 24.65% 29.16% 30.46% 25.41% 
Set0 Έννοιες 28.03% 28.29% 29.35% 28.95% 
Set1 Λέξεις 34.05% 37.34% 34.82% 31.43% 
Set1 Έννοιες 31.14% 33.74% 36.43% 32.93% 
Set2 Λέξεις 43.96% 44.93% 40.58% 41.02% 
Set2 Έννοιες 36.56% 35.75% 41.34% 38.76% 
Set3 Λέξεις 46.81% 46.17% 44.71% 45.63% 
Set3 Έννοιες 43.02% 46.09% 49.17% 49.33% 
Set4 Λέξεις 30.08% 33.68% 30.59% 31.32% 
Set4 Έννοιες 25.76% 26.80% 33.66% 28.12% 
Set5 Λέξεις 24.65% 29.16% 30.46% 25.41% 
Set5 Έννοιες 28.03% 28.29% 29.35% 28.95% 
Set6 Λέξεις 16.84% 28.67% 22.56% 21.85% 
Set6 Έννοιες 21.59% 27.66% 19.74% 18.62% 
Set7 Λέξεις 42.98% 32.87% 40.54% 40.92% 
Set7 Έννοιες 25.34% 30.94% 38.73% 22.39% 
Set8 Λέξεις 31.67% 32.36% 37.79% 37.79% 
Set8 Έννοιες 16.15% 24.30% 24.12% 12.13% 
Set9 Λέξεις 21.70% 23.70% 26.87% 19.06% 
Set9 Έννοιες 23.92% 26.54% 25.94% 19.64% 
AVG Λέξεις 31.74% 33.80% 33.94% 32.28% 
AVG Έννοιες 27.95% 30.84% 32.78% 27.98% 
Πίνακας 4.7: Οι τιµές του κριτηρίου  του Beeferman για όλα τα σύνολα όπως αυτές ελήφθησαν από την 1kP η 
και την 2η εναλλακτική µορφή του αλγορίθµου PREMONN σε συνδυασµό µε τον 1ο παράγοντα εξοµάλυνσης µε 
πλάτος παραθύρου ίσο µε 20, µε και χωρίς την εφαρµογή της τεχνικής της ψηφοφορίας τόσο για την περίπτωση 
όπου χρησιµοποιούνται οι λέξεις όσο και για την περίπτωση όπου χρησιµοποιούνται οι έννοιες των κειµένων. 
 
 120
Κεφάλαιο 4 Τµηµατοποίηση Κειµένων βάσει του αποτελέσµατος κατηγοριοποίησης 
Sets 2η εναλ. Μορ. 
PREMONN µε 
Ψηφοφορία & 1ος 
παραγ. Εξοµ. W=25 
2η εναλ. Μορ. 
PREMONN χωρίς 
Ψηφοφορία και 1ος 
παραγ. Εξοµ. W=25 
1η εναλ. Μορ. 
PREMONN µε 
Ψηφοφορία & 1ος 
παρ. Εξοµ. W=25 
1η εναλ. Μορ. 
PREMONN χωρίς  
Ψηφοφορία & 1ος 
παρ. Εξοµ. W=25 
Set0 Λέξεις 26.16% 31.60% 31.78% 30.27% 
Set0 Έννοιες 28.40% 30.09% 29.87% 33.58% 
Set1 Λέξεις 33.62% 37.43% 36.98% 31.50% 
Set1 Έννοιες 31.90% 33.67% 38.28% 33.78% 
Set2 Λέξεις 44.52% 45.03% 40.09% 43.09% 
Set2 Έννοιες 38.31% 36.05% 42.42% 42.26% 
Set3 Λέξεις 47.96% 46.32% 43.61% 46.73% 
Set3 Έννοιες 42.94% 45.64% 50.00% 49.56% 
Set4 Λέξεις 25.64% 33.80% 31.77% 31.49% 
Set4 Έννοιες 25.29% 27.09% 34.72% 29.49% 
Set5 Λέξεις 26.16% 31.60% 31.78% 30.27% 
Set5 Έννοιες 28.40% 30.09% 29.87% 33.58% 
Set6 Λέξεις 17.47% 29.07% 26.86% 23.31% 
Set6 Έννοιες 21.77% 27.50% 24.70% 19.43% 
Set7 Λέξεις 41.90% 33.08% 39.79% 39.79% 
Set7 Έννοιες 26.03% 30.90% 41.76% 24.74% 
Set8 Λέξεις 36.27% 30.06% 35.68% 42.52% 
Set8 Έννοιες 17.66% 24.38% 27.84% 13.98% 
Set9 Λέξεις 22.94% 24.97% 26.66% 17.56% 
Set9 Έννοιες 24.78% 26.27% 26.85% 21.56% 
AVG Λέξεις 32.26% 34.30% 34.50% 33.87% 
AVGΈννοιες 28.55% 31.17% 31.17% 30.20% 
Πίνακας 4.8: Οι τιµές του κριτηρίου  του Beeferman για όλα τα σύνολα όπως αυτές ελήφθησαν από την 1kP η 
και την 2η εναλλακτική µορφή του αλγορίθµου PREMONN σε συνδυασµό µε τον 1ο παράγοντα εξοµάλυνσης µε 
πλάτος παραθύρου ίσο µε 25, µε και χωρίς την εφαρµογή της τεχνικής της ψηφοφορίας τόσο για την περίπτωση 
όπου χρησιµοποιούνται οι λέξεις όσο και για την περίπτωση όπου χρησιµοποιούνται οι έννοιες των κειµένων. 
 
 121
Κεφάλαιο 4 Τµηµατοποίηση Κειµένων βάσει του αποτελέσµατος κατηγοριοποίησης 
Μέσος Όρος Απόδοσης 
Τµηµατοποίησης όλων 
των συνόλων 
2η εναλ. Μορ. 
PREMONN 
µε Ψηφοφορία 
2η εναλ. Μορ. 
PREMONN 
χωρίς 
Ψηφοφορία 
1η εναλ. Μορ. 
PREMONN 
µε Ψηφοφορία 
1η εναλ. Μορ. 
PREMONN 
χωρίς  
Ψηφοφορία 
1η σειρ. πειρ. (Λέξεις) 30.58% 30.24% 26.26% 24.03% 
1η σειρ. πειρ.  (Έννοιες) 29.34% 29.48% 26.90% 23.18% 
2η σειρ. πειρ. (Λέξεις) 32.25% 32.47% 32.99% 29.75% 
2η σειρ. πειρ.  (Έννοιες) 29.66% 30.28% 30.31% 25.42% 
3η σειρ. πειρ. (Λέξεις) 
Μέγεθος Παραθ = 5 
32.12% 32.34% 31.66% 29.72% 
3η σειρ. πειρ. (Έννοιες) 
Μέγεθος Παραθ = 5 
29.64% 30.09% 31.23% 25.39% 
3η σειρ. πειρ. (Λέξεις) 
Μέγεθος Παραθ = 10 
32.25% 32.47% 32.99% 29.75% 
3η σειρ. πειρ. (Έννοιες) 
Μέγεθος Παραθ = 10 
29.66% 30.28% 30.31% 25.42% 
3η σειρ. πειρ. (Λέξεις) 
Μέγεθος Παραθ = 15 
32.20% 32.67% 32.49% 31.29% 
3η σειρ. πειρ. (Έννοιες) 
Μέγεθος Παραθ = 15 
28.98% 30.41% 30.69% 26.54% 
3η σειρ. πειρ. (Λέξεις) 
Μέγεθος Παραθ = 20 
31.74% 33.80% 33.94% 32.28% 
3η σειρ. πειρ. (Έννοιες) 
Μέγεθος Παραθ = 20 
27.95% 30.84% 32.78% 27.98% 
3η σειρ. πειρ. (Λέξεις) 
Μέγεθος Παραθ = 25 
32.26% 34.30% 34.50% 33.87% 
3η σειρ. πειρ. (Έννοιες) 
Μέγεθος Παραθ = 25 
28.55% 31.17% 34.63% 30.20% 
Πίνακας 4.9: Οι µέσοι όροι των τιµών του κριτηρίου  του Beeferman για τα σύνολα Set0, …, Set9 όπως 
αυτές ελήφθησαν µετά από την εφαρµογή της 1
kP
ης και της 2ης εναλλακτικής µορφής του αλγορίθµου του 
PREMONN µε και χωρίς την εφαρµογή της τεχνικής της ψηφοφορίας τόσο για την περίπτωση όπου 
χρησιµοποιούνται οι λέξεις όσο και για την περίπτωση όπου χρησιµοποιούνται οι έννοιες των κειµένων, όπως 
αυτά προκύπτουν και από τις τρεις σειρές πειραµάτων. 
 
 122
Κεφάλαιο 4 Τµηµατοποίηση Κειµένων βάσει του αποτελέσµατος κατηγοριοποίησης 
0
5
10
15
20
25
30
35
2η εναλ, Μορ, PREMONN µε Ψηφοφορία, 2η εναλ, Μορ, PREMONN χωρίς Ψηφοφορία, 1η 
εναλ, Μορ, PREMONN µε Ψηφοφορία, 1η εναλ, Μορ, PREMONN χωρίς Ψηφοφορία
1η σειρ, πειρ, (Λέξεις) 1η σειρ, πειρ,  (Έννοιες)
2η σειρ, πειρ, (Λέξεις) 2η σειρ, πειρ,  (Έννοιες)
3η σειρ, πειρ, (Λέξεις) ΜΟ Μέγεθος Παραθ 3η σειρ, πειρ, (Έννοιες)ΜΟ Μέγεθος Παραθ
 
Σχήµα 4.1: Οι µέσοι όροι των τιµών του κριτηρίου  του Beeferman για τα σύνολα Set0, …, Set9 
όπως αυτές ελήφθησαν µετά από την εφαρµογή της 1
kP
ης και της 2ης εναλλακτικής µορφής του 
αλγορίθµου του PREMONN µε και χωρίς την εφαρµογή της τεχνικής της ψηφοφορίας τόσο για την 
περίπτωση όπου χρησιµοποιούνται οι λέξεις όσο και για την περίπτωση όπου χρησιµοποιούνται οι 
έννοιες των κειµένων, όπως αυτά προκύπτουν και από τις τρεις σειρές πειραµάτων. 
 
4.4 Συµπεράσµατα 
 
Το παρόν κεφάλαιο, εξετάζει το πρόβληµα της τµηµατοποίησης κειµένων κάνοντας χρήση 
µεθόδων κατηγοριοποίησης  αλλά ταυτόχρονα και το όφελος χρήσης των εννοιών που αντιστοιχούν 
στις λέξεις που εµφανίζονται σε ένα κείµενο, όπως αυτές παρέχονται από τον θησαυρό όρων του 
Wordnet, έναντι των αυτούσιων λέξεων. Η κατηγοριοποίηση κειµένων πραγµατοποιήθηκε τόσο 
χρησιµοποιώντας τις λέξεις όσο και τις έννοιες των κειµένων και το αποτέλεσµά της χρησιµοποιήθηκε 
στην τµηµατοποίηση. Για την τµηµατοποίηση των κειµένων χρησιµοποιήθηκε ο αλγόριθµος του 
PREMONN σε δυο εναλλακτικές µορφές, ενισχυµένος συµπληρωµατικά από δυο διαφορετικές 
µορφές εξοµάλυνσης και µια τεχνική ψηφοφορίας. Από τα πειράµατα τα οποία πραγµατοποιήθηκαν 
προέκυψε ότι, η βέλτιστη τµηµατοποίηση επιτυγχάνεται χρησιµοποιώντας την 1η εναλλακτική µορφή 
του PREMONN δίχως την τεχνική της ψηφοφορίας και δίχως την εφαρµογή κάποιων εκ των 
 123
Κεφάλαιο 4 Τµηµατοποίηση Κειµένων βάσει του αποτελέσµατος κατηγοριοποίησης 
παραγόντων εξοµάλυνσης. Σε όλα όµως τα πειράµατα τα οποία πραγµατοποιήθηκαν, και όπως 
διαφαίνεται και από τον Πίνακα 4.9, η απόδοση τµηµατοποίησης µε χρήση των εννοιών είναι 
καλύτερη από την αντίστοιχη µε χρήση των λέξεων. Παρόλα αυτά, η διαφορά των εν λόγω επιδόσεων 
δεν ξεπερνά, στην πλειοψηφία τους, το 2.5% ως προς το κριτήριο  του Beeferman, ενώ σε τέσσερις 
µόνο περιπτώσεις ξεπερνά το 4% (στην 1
kP
η εναλλακτική µορφή του PREMONN (α) χωρίς την τεχνική 
της ψηφοφορίας, (β) χωρίς τον παράγοντα εξοµάλυνσης, (γ) µε τον 2ο παράγοντα εξοµάλυνσης, (δ) µε 
τον 1ο παράγοντα εξοµάλυνσης για πλάτος παραθύρου 5,10, 15 και 20). Το γεγονός αυτό σε 
συνδυασµό µε τα αποτελέσµατα της κατηγοριοποίησης µε χρήση των εννοιών έναντι εκείνων µε 
χρήση των λέξεων τα οποία παρουσιάστηκαν στο 2ο κεφάλαιο, µας οδηγεί στο συµπέρασµα ότι, η 
χρήση των εννοιών επιτυγχάνει οριακή βελτίωση στην απόδοση κατηγοριοποίησης και στην απόδοση 
τµηµατοποίησης. Το γεγονός αυτό δεν την καθιστά ιδιαίτερα ελκυστική αν συλλογιστεί κανείς τον 
βαθµό δυσκολίας αποσαφήνισης της έννοιας µιας λέξης ακόµα και µε τη χρήση του θησαυρού όρων 
του Wordnet.  
Από την άλλη πλευρά, τα πειράµατα τα οποία πραγµατοποιήθηκαν σε αυτό το κεφάλαιο 
αναδεικνύουν το πρόβληµα της τµηµατοποίησης ως ένα πρόβληµα ιδιαίτερα ενδιαφέρον και επίκαιρο, 
για το οποίο αξίζει να εξεταστούν και άλλες παραλλαγές και παράµετροι πέραν από αυτή του 
PREMONN καθώς επίσης και άλλα σώµατα κειµένων. Το Κεφάλαιο 5 εξετάζει το πρόβληµα της 
τµηµατοποίησης από διαφορετική σκοπιά περιγράφοντας το δεύτερο προτεινόµενο µοντέλο µας και 
παραθέτοντας σηµαντικό πλήθος πειραµάτων τα οποία πραγµατοποιήθηκαν πάνω σε αυτό.  
 124
 
 
125 
Κεφάλαιο 5 Αλγόριθµος Τµηµατοποίησης µε ∆υναµικό Προγραµµατισµό 
ΚΕΦΑΛΑΙΟ 5 
ΑΛΓΟΡΙΘΜΟΣ ΤΜHMATΟΠΟΙΗΣΗΣ ΜΕ ∆ΥΝΑΜΙΚΟ 
ΠΡΟΓΡΑΜΜΑΤΙΣΜΟ 
 
5.1  Εισαγωγή 
 
Σε αυτό το κεφάλαιο παρουσιάζουµε διεξοδικά το δεύτερο προτεινόµενο µοντέλο 
τµηµατοποίησης το οποίο προσπαθεί να επιλύσει το πρόβληµα της εύρεσης της εννοιολογικής δοµής 
ενός κειµένου, άρα και του τρόπου µε τον οποίο συσχετίζεται η πληροφορία η οποία περιέχεται µέσα 
σε αυτό, µε τον προσδιορισµό των υποθεµάτων τα οποία το συνιστούν. Ο αλγόριθµός µας ο οποίος 
φέρει την ονοµασία «αλγόριθµος τµηµατοποίησης µε δυναµικό προγραµµατισµό» πραγµατοποιεί 
γραµµική τµηµατοποίηση των κειµένων µε τον ολικό τρόπο υπολογισµού της οµοιότητας όλων των 
µερών ενός κειµένου, ενώ για την εύρεση των ορίων µεταξύ των τµηµάτων που υποδηλώνουν αλλαγή 
θέµατος κάνει χρήση της τεχνικής του δυναµικού προγραµµατισµού. Ο προτεινόµενος αλγόριθµος 
δυναµικού προγραµµατισµού ελαχιστοποιεί ολικά µια συνάρτηση κόστους τµηµατοποίησης η οποία 
αποτελείται από δυο παράγοντες: (α) την εντός – τµήµατος οµοιότητα βασιζόµενη στις λέξεις του 
κειµένου και (β) προηγούµενη πληροφορία σχετικά µε το µήκος του τµήµατος. 
 Ο αλγόριθµός τµηµατοποίησης µε δυναµικό προγραµµατισµό παρουσιάζει το πλεονέκτηµα ότι 
µπορεί να εφαρµοστεί είτε σε µεγάλα κείµενα - για την τµηµατοποίηση αυτών στα τµήµατα από τα 
οποία αποτελούνται (δηλαδή ένα άρθρο στα κεφάλαιά του) - είτε σε µια ακολουθία ανεξάρτητων, 
συνεχόµενων το ένα µετά το άλλο κειµένων (π.χ. για την τµηµατοποίηση µιας εγγραφής ενός 
τηλεοπτικού προγράµµατος ειδήσεων, στις ειδήσεις από τις οποίες αποτελείται). Αρχικά 
επικεντρωθήκαµε σε προβλήµατα του δεύτερου τύπου. 
 Πιο συγκεκριµένα, εφαρµόσαµε τον αλγόριθµό τµηµατοποίησης µε δυναµικό προγραµµατισµό 
στην συλλογή κειµένων του Choi ([Choi, 2000], [Choi et al., 2001]) η οποία είναι η µοναδική συλλογή 
κειµένων για την οποία παραθέτονται στην βιβλιογραφία αποτελέσµατα, αλλά και σε µια συλλογή 
κειµένων την οποία κατασκευάσαµε εµείς. Το πρόβληµα της τµηµατοποίησης µοντελοποιήθηκε, στη 
 126
Κεφάλαιο 5 Αλγόριθµος Τµηµατοποίησης µε ∆υναµικό Προγραµµατισµό 
συνέχεια, κάνοντας χρήση µιας παραλλαγής των «Μοντέλων ∆ιαχωρισµού Γινοµένου» (“Product 
Partition Models”, [Gloss(00067)]) ως ένα πρόβληµα βελτιστοποίησης κάνοντας χρήση του 
προτεινόµενου αλγορίθµου µας. Τόσο ο αλγόριθµός τµηµατοποίησης µε δυναµικό προγραµµατισµό 
όσο και τα εν λόγω πειράµατα περιγράφονται µε λεπτοµέρειες στις ενότητες που ακολουθούν.  
5.2 Ο Αλγόριθµος Τµηµατοποίησης µε ∆υναµικό Προγραµµατισµό 
5.2.1 Η Παράσταση 
 
Έστω ότι ένα κείµενο αποτελείται από Τ προτάσεις και το λεξιλόγιό του αποτελείται από L 
συνολικά διαφορετικές λέξεις. Το κείµενο αυτό είναι δυνατό να παρασταθεί µε έναν πίνακα F 
διαστάσεων Τ x L ο οποίος ορίζεται ως ακολούθως: για t=1,2…, T και για l = 1,2… L θέτουµε: 
                  1, αν η l-στη λέξη εµφανίζεται στην t-στη πρόταση  =ltF ,
                              0, αλλιώς 
(5.1) 
 
Ο πίνακας οµοιότητας µεταξύ των προτάσεων του κειµένου -όπως αυτός ορίστηκε από τον 
Reynar ([Reynar, 1994], [Reynar, 1998], [Reynar & Ratnaparkhi, 1997]) - είναι ένας Τ x Τ πίνακας D 
όπου για  δυο τυχαίες προτάσεις s,t = 1,2…, Τ θέτουµε: 












=
>
=
∑
∑
=
=
0,0
0,1
1
,,
1
,,
, L
l
ltls
L
l
ltls
ts
FF
FF
D
αν
αν
               (5.2) 
Αυτό σηµαίνει ότι =1 αν η s-στή και η t-στή πρόταση περιέχουν τουλάχιστον µια κοινή 
λέξη. Κάθε κοµµάτι του αρχικού κειµένου αντιστοιχεί σε έναν υποπίνακα του D. Είναι αναµενόµενο 
ότι, οι υποπίνακες οι οποίοι αντιστοιχούν σε πραγµατικά τµήµατα θα περιέχουν πολλές προτάσεις µε 
κοινές λέξεις και έτσι θα περιέχουν πολλούς άσσους.  Στο Σχήµα 5.1 δίνουµε το dotplot του πίνακα D 
(ο συγκεκριµένος πίνακας αντιστοιχεί σε ένα κείµενο το οποίο αποτελείται από 91 προτάσεις το οποίο 
ανήκει στην συλλογή κειµένων του υποκεφαλαίου 5.3.2). Οι µονάδες παρίστανται ως µαύρα 
τετράγωνα ενώ τα µηδενικά µε  άσπρα τετράγωνα.  
tsD ,
 127
Κεφάλαιο 5 Αλγόριθµος Τµηµατοποίησης µε ∆υναµικό Προγραµµατισµό 
Κάνουµε την παραδοχή ότι τα όρια µεταξύ των τµηµάτων εµφανίζονται στο τέλος των 
προτάσεων. Η τµηµατοποίηση ενός κειµένου αποτελεί την τµηµατοποίηση του συνόλου {1,2…, Τ } 
σε Κ υποσύνολα (δηλαδή τµήµατα) της µορφής {1,2…, }, {1t 11 +t , 21 +t , …, }, …, {2t 11 +−Kt , 
21 +−Kt ,…, Τ}. Μια συντοµότερη παράσταση της τµηµατοποίησης µπορεί να γίνει µε την µορφή 
ενός ανύσµατος , όπου τα  είναι τα όρια µεταξύ των τµηµάτων (που 
αντιστοιχούν στην τελευταία πρόταση κάθε υποσυνόλου) και πρέπει να ικανοποιούν την συνθήκη: 
),...,,( 10 Ktttt = Kttt ,...,, 10
Ttttt KK =<<<= −110 ...0           (5.3) 
Σχήµα 5.1: Γραφική παράσταση του πίνακα οµοιότητας D µεταξύ προτάσεων, ενός κειµένου το οποίο 
αποτελείται από 91 προτάσεις. Οι µονάδες παρίστανται µε µαύρα τετράγωνα ενώ τα µηδενικά µε 
άσπρα τετράγωνα.  
 
Αξίζει να σηµειωθεί ότι το Κ, δηλαδή το µήκος του ανύσµατος, είναι µια µεταβλητή. Έτσι, το t 
είναι δυνατό να περιγράφει οποιοδήποτε αριθµό τµηµάτων (παρόλα αυτά θα πρέπει να ισχύει Κ ≤ Τ, 
όπου Τ το πλήθος των προτάσεων).  
 128
Κεφάλαιο 5 Αλγόριθµος Τµηµατοποίησης µε ∆υναµικό Προγραµµατισµό 
5.2.2 Η Συνάρτηση Κόστους 
 
Ο δυναµικός προγραµµατισµός ως µέθοδος εγγυάται το ευνοϊκότερο αποτέλεσµα σε 
συνάρτηση µε την είσοδο και τις παραµέτρους. Ακολουθώντας την προσέγγιση του Heinonen 
[Heinonen, 1998], χρησιµοποιούµε δυναµικό προγραµµατισµό ο οποίος αποφασίζει τη θέση των 
ορίων µεταξύ των τµηµάτων υπολογίζοντας τον ολικά βέλτιστο διαµερισµό µε βάση την καµπύλη 
οµοιότητας, ένα προτιµώµενο µήκος κείµενου και µια οριζόµενη συνάρτηση κόστους 
τµηµατοποίησης. 
Όπως προκύπτει από την προηγούµενη ενότητα, κάθε τµήµα του αρχικού κειµένου 
αντιστοιχεί σε έναν υποπίνακα του D. Είναι αναµενόµενο ότι οι υποπίνακες οι οποίοι αντιστοιχούν 
στα πραγµατικά τµήµατα θα περιέχουν πολλούς άσσους, αφού τα αντίστοιχα τµήµατα θα περιέχουν 
πολλές προτάσεις µε κοινές λέξεις (πράγµατι, στο σχήµα 5.1 µπορούµε να δούµε αρκετές περιοχές που 
παρουσιάζουν «υψηλή πυκνότητα» οι οποίες προσδοκούµε ότι αντιστοιχούν στα πραγµατικά 
τµήµατα). Έτσι, µια «καλή» τµηµατοποίηση θα πρέπει να µεγιστοποιεί την πυκνότητα των µονάδων 
στους υποπίνακες του D που αντιστοιχούν στα τµήµατα. Από την άλλη πλευρά, σε πολλές περιπτώσεις 
διαθέτουµε πληροφορία σχετικά µε το µήκος των τµηµάτων. Για παράδειγµα, είναι δυνατό να 
γνωρίζουµε ότι το µέσο µήκος τµήµατος ισούται µε µ. Αυτή η πληροφορία είναι δυνατό να 
χρησιµοποιηθεί για την βελτιστοποίηση της απόδοσης τµηµατοποίησης.  
Οι παραπάνω συλλογισµοί είναι δυνατό να τυποποιηθούν ορίζοντας µια συνάρτηση κόστους 
τµηµατοποίησης ),,,;( γσµ rtJ παρόµοια µε αυτή που ορίστηκε από τον Heinonen [Heinonen, 1998] 
(όπου t είναι η ανεξάρτητη µεταβλητή και µ, σ, γ και r είναι παράµετροι όπως φαίνεται παρακάτω): 
( )rkk
t
tt
t
tt
tsK
k
kk
tt
D
tt
GrtJ
k
k
k
k
1
1 1
,
1
1 1 1)1(),,,;(
−
+= +=
=
−
−
⋅−−




 −−⋅=
∑ ∑
∑ − −γσ
µ
γγσµ      (5.4)  
Το 




 −− −
σ
µ1kk ttG  είναι η συνάρτηση µήκους κόστους, όπου το µ είναι το µέσο µήκος και 
το σ η τυπική απόκλιση από αυτό. Η συνάρτηση 




 −− −
σ
µ1kk ttG  είναι δυνατό να πάρει πολλές 
µορφές. Ίσως οι απλούστερες από αυτές είναι οι γνωστές από τη βιβλιογραφία συναρτήσεις V-shape  
όπου )5.5(11
σ
µ
σ
µ −−
=




 −− −− kkkk ttttG  και U-shape όπου 
 129
Κεφάλαιο 5 Αλγόριθµος Τµηµατοποίησης µε ∆υναµικό Προγραµµατισµό 
)6.5(
2
)(
2
2
11
σ
µ
σ
µ
⋅
−−
=




 −− −− kkkk ttttG . Ορίζουµε παραλλαγές των παραπάνω συναρτήσεων οι 
τις οποίες µπορούν να χρησσιµοποιηθούν στη συνάρτηση κόστους: τη συνάρτηση «κορεσµένη V-
shape» (“saturated”, [Gloss(00074)]) που ορίζεται από τη σχέση:  












>
−−
≤
−−−−
=




 −−
−
−−
−
Α    όταν A,
Α  όταν,
1
11
1
σ
µ
σ
µ
σ
µ
σ
µ
kk
kkkk
kk
tt
tttt
tt
G      (5.7) 
και τη συνάρτηση «κορεσµένη U-shape» (“saturated”, [Gloss(00074)]) που ορίζεται από τη σχέση: 
  
( ) ( )
( )












>
⋅
−−
≤
⋅
−−
⋅
−−
=




 −−
−
−−
−
Α   
2
 όταν ,A 
Α 
2
 όταν,
2
2
2
1
2
2
1
2
2
1
1
σ
µ
σ
µ
σ
µ
σ
µ
kk
kkkk
kk
tt
tttt
tt
G     (5.8) 
H παράµετρος Α αποτελεί µια παράµετρο «κατώφλι». Έτσι, το συνολικό κόστος 
τµηµατοποίησης είναι το άθροισµα του κόστους των Κ τµηµάτων. Το κόστος κάθε τµήµατος είναι το 
άθροισµα των ακόλουθων δυο όρων (µε την σχετική τους σηµασία σταθµισµένη από την παράµετρο 
γ): 
1. Τον όρο 




 −− −
σ
µ1kk ttG  ο οποίος αντιστοιχεί στην πληροφορία µήκους. Πιο συγκεκριµένα ο 
όρος αυτός µετρά την απόκλιση από το µέσο µήκος τµήµατος. Υπό αυτήν την έννοια, οι 
παράµετροι µ και σ µπορούν να θεωρηθούν ως ο µέσος όρος και η τυπική απόκλιση του µήκους 
τµήµατος - που µετρώνται είτε βάσει του αριθµού των προτάσεων οι οποίες εµφανίζονται στα 
τµήµατα του κειµένου είτε βάσει του αριθµού των λέξεων οι οποίες εµφανίζονται στα τµήµατα 
του κειµένου- και µπορούν να εκτιµηθούν από δεδοµένα προς εκπαίδευση. Μικρές τιµές αυτού 
του όρου υποδεικνύουν συµφωνία µε το αναµενόµενο µήκος τµήµατος. Αξίζει να σηµειωθεί ότι ο 
αριθµός των λέξεων οι οποίες εµφανίζονται στα τµήµατα ενός κειµένου εµφανίζει µεγαλύτερη 
µεταβλητότητα από ότι ο αριθµός των προτάσεων µέσα σε αυτά. 
 130
Κεφάλαιο 5 Αλγόριθµος Τµηµατοποίησης µε ∆υναµικό Προγραµµατισµό 
2. Τον όρο 
( )rkk
kt
ktt
kt
ktt
ts
tt
D
1
11 11
,
−
+−= −=
−
∑ ∑
+
, ο οποίος αντιστοιχεί στην οµοιότητα (βάσει των λέξεων) µεταξύ 
των προτάσεων. Αξίζει να σηµειωθεί ότι ο παράγοντας  είναι ο συνολικός 
αριθµός των µονάδων στο υποπίνακα D ο οποίος αντιστοιχεί σε k-στό τµήµα. Στην περίπτωση 
όπου η παράµετρος r ισούται µε 2, ο παράγοντας αντιστοιχεί στην περιοχή του 
υποπίνακα. Έτσι όταν r  = 2, ο όρος 
∑ ∑
+−= +−=
kt
ktt
kt
ktt
tsD
11 11
,
( )rkt 1−−
( )
kt
r
kt
k
tsD
1
11
,
−
+−
∑ ∑
∑
+−= +−=
kt
ktt
kt
ktt
D
11 1
k
tt
t
=
k
kt
ktt
t
11+−=
−
 αντιστοιχεί στην «πυκνότητα 
τµήµατος». Όταν η τιµή της παραµέτρου r είναι διαφορετική από την τιµή 2 λαµβάνουµε µια 
«γενικευµένη πυκνότητα», η οποία µας επιτρέπει να ελέγχουµε το βαθµό επηρεασµού της 
επιφάνειας σε συνάρτηση µε την πληροφορία (δηλαδή το πλήθος των «µονάδων») που περιέχεται 
σε αυτήν. Ανεξάρτητα από την τιµή του r, µεγάλες τιµές του  υποδεικνύουν 
ισχυρή ενδο-τµηµατική οµοιότητα (η οποία µετράται από τον αριθµό των κοινών λέξεων µεταξύ 
των προτάσεων οι οποίες περιέχονται σε ένα τµήµα) .  
∑ ts
1
,
 
Ένα «καλό» άνυσµα τµηµατοποίησης t δίνει τµήµατα τα οποία εµφανίζουν µεγάλη πυκνότητα 
και µικρή απόκλιση από το µέσο µήκος τµήµατος (δηλαδή, µικρή τιµή του ),,,;( γσµ rtJ , µικρή µε 
την αλγεβρική έννοια µιας και ο παράγοντας ),,,;( γσµ rtJ
∧
t
µπορεί να πάρει τόσο θετικές όσο και 
αρνητικές τιµές). Η βέλτιστη τµηµατοποίηση  παρέχει το ολικό ελάχιστο του ),,,;( γσµ rtJ
Kttt ,...,, 10
. 
Αξίζει να σηµειωθεί ότι βέλτιστη τµηµατοποίηση  καθορίζει τόσο τον βέλτιστο αριθµό των 
τµηµάτων Κ όσο και τις βέλτιστες θέσεις των ορίων µεταξύ των τµηµάτων .  
∧
t
 
 
 
 131
Κεφάλαιο 5 Αλγόριθµος Τµηµατοποίησης µε ∆υναµικό Προγραµµατισµό 
5.2.3 ∆υναµικός Προγραµµατισµός 
 
 Στόχος µας είναι να λάβουµε το άνυσµα τµηµατοποίησης t το οποίο ελαχιστοποιεί το 
),,,;( γσµ rtJ . Ακολουθώντας την προσέγγιση του [Heinonen, 1998], χρησιµοποιούµε έναν 
αλγόριθµο δυναµικού προγραµµατισµού ο οποίος βρίσκει το ολικά βέλτιστο . Η είσοδος του 
αλγορίθµου είναι ο πίνακας οµοιότητας D και οι παράµετροι µ, σ, γ και r ενώ η έξοδος είναι το  που 
υπολογίζεται σε χρόνο , όπου Τ είναι το πλήθος των προτάσεων. Παρακάτω παρατίθεται ο 
αλγόριθµος τµηµατοποίησης µε δυναµικό προγραµµατισµό µε την µορφή ψευδοκώδικα. 
∧
t
∧
t
)( 2TΟ
 
__________________________________________________________ 
∆υναµικός Προγραµµατισµός του Αλγορίθµου Τµηµατοποίησης Κειµένου µε Ρήτρα Μήκους 
Είσοδος: Ο πίνακας οµοιότητας D διαστάσεων Tx T  (όπου Τ το πλήθος των προτάσεων του 
κειµένου) και οι παράµετροι µ, σ, γ και r 
Αρχικοποίηση  
For t = 1, 2, .. T 
 Sum = 0; 
 For s = 1,2,…, t-1  
   Sum = Sum +  tsD ,
  = tsS ,1+ rst
Sum
)( −
 
      End 
End  
 
 132
Κεφάλαιο 5 Αλγόριθµος Τµηµατοποίησης µε ∆υναµικό Προγραµµατισµό 
Ελαχιστοποίηση  
0C = 0 , = 0 0Z
For t = 1,2, …, T  
 = ∞  tC
 For s = 1, 2, ….t-1 
 If  C  + s tsS
stG ,1)1( +⋅−−




 −−⋅ γ
σ
µγ tC≤  then 
  C  = + t sC tsS
stG ,1)1( +⋅−−




 −−⋅ γ
σ
µγ  
   Z = s t
 EndIf 
     End 
End  
Οπισθοδρόµηση 
K = 0 
KL  = T 
While > 0 
KL
Z
K = K+1 
KL =  1−KLZ
End  
 133
Κεφάλαιο 5 Αλγόριθµος Τµηµατοποίησης µε ∆υναµικό Προγραµµατισµό 
K = K+1 
kL  = 0 
0
^
=ot  
For k = 1,2,… K 
=
^
kt kKL −  
End 
Έξοδος: Το βέλτιστο άνυσµα  ),...,,(
^^
1
^
0
^
Ktttt =
 
Τα βασικά στοιχεία του αλγορίθµου είναι η ελαχιστοποίηση και η «οπισθοδρόµηση»  
(“backtracking”, [Gloss(00004)]). Κατά τη διάρκεια της ελαχιστοποίησης υπολογίζουµε (για 
t=0,1,2,…, T) το  το οποίο είναι το βέλτιστο (ελάχιστο) κόστος τµηµατοποίησης του «υπο-
κειµένου» το οποίο ξεκινά από την πρόταση 1 και τελειώνει στην πρόταση t. Το κόστος αυτό 
υπολογίζεται αναδροµικά χρησιµοποιώντας το όρισµα του δυναµικού προγραµµατισµού. Πιο 
συγκεκριµένα, το  (δηλαδή το βέλτιστο κόστος για την τµηµατοποίηση των προτάσεων από 1 ως t) 
είναι η ελάχιστη τιµή  (συναρτήσει του s)  του + + 
tC
tC
sC tsS ,1+ 




 −− −
σ
µ1kk ttG
0S 1S
.  Το άθροισµα 
αποτελεί το βέλτιστο κόστος για την τµηµατοποίηση των προτάσεων 1 ως s συν το κόστος 
δηµιουργίας ενός τµήµατος το οποίο θα συµπεριλαµβάνει τις προτάσεις από την s+1 ως την t. Το  
είναι το όριο τµήµατος το οποίο προηγείται του t στη βέλτιστη τµηµατοποίηση. Σύµφωνα µε αυτά, 
στο τµήµα ελαχιστοποίησης του αλγορίθµου υπολογίζουµε το βέλτιστο κόστος τµηµατοποίησης για 
τις προτάσεις 1 ως Τ δηλαδή για το συνολικό κείµενο. Το κοµµάτι της «οπισθοδρόµησης»  
(“backtracking”, [Gloss(00004)]) αρχικά δηµιουργεί την ακολουθία , , …,  η οποία 
αποτελεί τα βέλτιστα όρια σε αντίστροφη σειρά και στην συνέχεια αντιστρέφει την ακολουθία για την 
παραγωγή της βέλτιστης τµηµατοποίησης . Αξίζει να σηµειωθεί ότι το Κ 
αντιπροσωπεύει το βέλτιστο αριθµό τµηµάτων και υπολογιζεται αυτόµατα.  
tZ
KS
)(
^
t = ,....,
^
kt,
^
1
^
0 tt
 134
Κεφάλαιο 5 Αλγόριθµος Τµηµατοποίησης µε ∆υναµικό Προγραµµατισµό 
5.3 Πειράµατα 
 
Το πρόβληµα της τµηµατοποίησης κειµένων αποτελεί ένα πρόβληµα το οποίο απασχόλησε τους 
ερευνητές τα τελευταία δεκαπέντε περίπου χρόνια  και διατυπώθηκε από τον καθένα από αυτούς µε 
διαφορετικό τρόπο. Θα µπορούσε να ισχυριστεί κανείς ότι ο αλγόριθµος τµηµατοποίησης όπως 
διατυπωνόταν από τον κάθε ερευνητή ήταν ισχυρά συνυφασµένος µε το/α κείµενο/α τα οποία είχε ως 
στόχο να τµηµατοποιήσει. Έτσι τα κείµενα τα οποία χρησιµοποιήθηκαν από τον κάθε ερευνητή, 
ιδιαίτερα στους πρώτους αλγόριθµους που παρουσιάστηκαν στη βιβλιογραφία, διέφεραν, γεγονός το 
οποίο εµπόδιζε τη σύγκριση και την αξιολόγηση των αλγορίθµων µεταξύ τους. 
 Ενδεικτικά αναφέρουµε ότι κατά καιρούς χρησιµοποιήθηκαν από τους ερευνητές διάφορα 
κείµενα όπως π.χ. από την Hearst ([Hearst, 1993], [Hearst, 1994(a)], [Hearst, 1994(b)], [Hearst & 
Plaunt, 1993], [Hearst, 1992]) το κείµενο Stargazers, από τον Youmans ([Youmans, 1990], [Youmans, 
1991]) η νουβέλα του James Joyce “The Dead” και το πρώτο τµήµα από το βιβλίο του Hemingway  
“Big-Two Hearted River”, από τον Yaari ([Yaari, 1997], [Yaari, 1999]) εκτός από το κείµενο 
Stargazers τα κείµενα Sidonious, Genetics και desert, ενώ από τον Heinonen ([Heinonen, 1998]) το 
κείµενο Mars του Percival Lowell και από τον Kozima ([Kozima, 1993], [Kozima & Furugori, 1993], 
[Kozima & Furugori, 1994] ) µια απλοποιηµένη έκδοση του κειµένου του O’Henry “Spring time a la 
carte”.  
 Αργότερα οι ερευνητές χρησιµοποίησαν τόσο κείµενα από επιστηµονικά άρθρα και βιβλία ή 
εγκυκλοπαίδειες (όπως π.χ. ο Sardinha ([Sardinha, 1999], [Sardinha,1993]), ο Philips [Philips, 1985]  
και ο Yaari  ([Yaari, 1997], [Yaari, 1999])), αλλά και λογοτεχνικά κείµενα (όπως ο Reynar  
[Reynar,1994], [Reynar,1998], [Reynar & Ratnaparkhi, 1997]) ή κείµενα ψυχολογίας (όπως οι 
Richmond, Smith και Amitay [Richmond et al., 1997]). Μεταγενέστερα χρησιµοποιήθηκαν συλλογές 
από ακολουθίες ειδήσεων µε πιο δηµοφιλείς αυτές του Wall Street Journal (Reynar [Reynar,1994], 
[Reynar,1998], [Reynar & Ratnaparkhi, 1997] , Ponte & Croft ([Ponte & Croft, 1997], [Xu & Croft, 
1996]), Kan, Klavans & McKeown ([Kan et al., 1998]), του TDT (Richmond, Smith και Amitay 
[Richmond et al., 1997]), του Economist (Kan, Klavans & McKeown ([Kan et al., 1998]) και του 
σώµατος κειµένων του New York Times (Blei & Moreno [Blei & Moreno, 2001]).  
 Η πρώτη συλλογή κειµένων η οποία αποτέλεσε σηµείο αναφοράς είναι αυτή που 
κατασκευάστηκε από τον Choi το 2000 [Choi, 2000] και η οποία περιέχει κείµενα, τα οποία 
αποτελούνται από συνηµµένα τµήµατα παρµένα από κείµενα του Brown Corpus [Francis & Kucera, 
1982]. Λεπτοµέρειες για την εν λόγω συλλογή παραθέτονται στην επόµενη ενότητα. Η συλλογή 
κειµένων του Choi είναι η µόνη η οποία χρησιµοποιήθηκε από τους µεταγενέστερους ερευνητές και 
 135
Κεφάλαιο 5 Αλγόριθµος Τµηµατοποίησης µε ∆υναµικό Προγραµµατισµό 
για την οποία είναι δυνατό να πραγµατοποιηθεί συγκριτική παράθεση των αποτελεσµάτων διαφόρων 
αλγορίθµων. Για το λόγο αυτό την επιλέξαµε για την αξιολόγηση της απόδοσης του αλγορίθµου µας. 
Ταυτόχρονα εξετάσαµε την απόδοση του αλγορίθµου µας και σε άλλα σώµατα κειµένων τα οποία 
κατασκευάσαµε ή τροποποιήσαµε για το πρόβληµα της τµηµατοποίησης. Τόσο τα σώµατα κειµένων 
που χρησιµοποιήθηκαν για την εξέταση του αλγορίθµου µας όσο και τα αποτελέσµατα τα οποία 
ελήφθησαν περιγράφονται στις ενότητες που ακολουθούν.  
5.3.1 Κριτήρια µέτρησης της ακρίβειας τµηµατοποίησης  
 
Στα παραδείγµατα που ακολουθούν αξιολογούµε την απόδοση του αλγορίθµου µας µε την 
χρήση τριών κριτηρίων: των Precision και Recall, τα οποία µετρούν την ακρίβεια τµηµατοποίησης και 
το κριτήριο του Beeferman , το οποίο µετρά την ανακρίβεια τµηµατοποίησης. ∆ιαισθητικά, το 
κριτήριο  µετρά το ποσοστό των “προτάσεων που εκτιµήθηκαν λάθος ότι ανήκουν στο ίδιο τµήµα 
(ενώ στην πραγµατικότητα ανήκουν σε διαφορετικά τµήµατα)” ή το ποσοστό των “προτάσεων που 
εκτιµήθηκαν λάθος ότι ανήκουν σε διαφορετικά τµήµατα  (ενώ στην πραγµατικότητα ανήκουν στο ίδιο 
τµήµα)”. Τα εν λόγω κριτήρια ορίστηκαν στο Κεφάλαιο 3. 
kP
kP
 Τα κριτήρια των Precision και  Recall  λαµβάνουν τιµές από 0% ως 100%, όπου υψηλές τιµές 
τόσο της Precision όσο και της Recall υποδεικνύουν υψηλή απόδοση τµηµατοποίησης.  Το κριτήριο 
του Beeferman  λαµβάνει τιµές από 0% ως 100%. όπου χαµηλές τιµές αυτού υποδεικνύουν 
ποσοστό ακρίβειας τµηµατοποίησης 
kP
5.3.2 Πρώτη Οµάδα Πειραµάτων 
 
Για την εξέταση του αλγορίθµου µας χρησιµοποιήσαµε το σώµα κειµένων του Choi ([Choi, 
2000] , [Choi et al., 2001]). H εν λόγω συλλογή αποτελείται από 700 κείµενα, καθένα από τα οποία 
περιλαµβάνει δέκα ιστορίες – τµήµατα το ένα µετά το άλλο. Κάθε ιστορία – τµήµα αποτελεί «τις 
πρώτες n προτάσεις ενός τυχαία επιλεγόµενου κειµένου από το σώµα κειµένων του Brown Corpus 
[Francis & Kucera, 1982]» (τα άρθρα ειδήσεων ca**.pos και το πληροφοριακό κείµενο cj**.pos, όπου 
σε καθένα από αυτά το τέλος ενός τµήµατος απαντάται πάντα στο τέλος µιας πρότασης). Τα 700 αυτά 
κείµενα µπορούν να υποδιαιρεθούν σε 4 σύνολα, Set0, Set1, Set2 και Set3, σύµφωνα µε το εύρος του 
n (δηλαδή το εύρος των προτάσεων µέσα σε ένα κείµενο) όπως αναγράφεται στον Πίνακα 5.1.  
 136
Κεφάλαιο 5 Αλγόριθµος Τµηµατοποίησης µε ∆υναµικό Προγραµµατισµό 
 Tα εν λόγω κείµενα υφίστανται προ-επεξεργασία δηλαδή από αυτά αφαιρούνται τα σηµεία 
στίξεως και οι λέξεις οι οποίες ανήκουν στην stop – list (στην οποία περιέχονται όλες οι προθέσεις τα 
άρθρα, κοινότυπα ρήµατα,και γενικότερα οι λέξεις που δεν προσδίδουν ιδιαίτερη πληροφορία µέσα σε 
ένα κείµενο και η οποία παρατίθεται στο Παράρτηµα Α1) ενώ οι υπόλοιπες λέξεις αντικαθίστανται 
από τη ρίζα τους [Gloss (00084)] µετά την εφαρµογή του αλγορίθµου του Porter [Porter, 1980].  Ένα 
παράδειγµα προεπεξεργασίας ενός κειµένου της εν λόγω συλλογής παραθέτεται στο Παράρτηµα Α2. 
 Set0 Set1 Set2 Set3 
Εύρος του n 3-11 3-5 6-8 9-11 
Πλήθος κειµένων 400 100 100 100 
Πίνακας 5.1: Εύρος του n (αριθµού των προτάσεων) και αριθµός των κειµένων για τα σύνολα Set0, Set1, Set2 
και Set3 της συλλογής κειµένων του Choi.  
Η εν λόγω συλλογή χρησιµοποιήθηκε για την πραγµατοποίηση τριών γκρουπ πειραµάτων για 
την εξέταση της απόδοσης του αλγορίθµου τµηµατοποίησης. Όπως αναφέρθηκε προηγούµενα στην 
παράγραφο 5.2.2, ο αλγόριθµος τµηµατοποίησης µε δυναµικό προγραµµατισµό χρησιµοποιεί τέσσερις 
παραµέτρους : τις µ, σ, r και γ, όπου οι παράµετροι µ και σ µπορούν να ερµηνευτούν ως ο µέσος όρος 
και η τυπική απόκλιση από το µήκος τµήµατος. Ο υπολογισµός των µ και σ δεν είναι προφανής. Μια 
δυνατότητα είναι να υπολογίσουµε το µέσο όρο και την τυπική απόκλιση από το µήκος τµήµατος 
βασιζόµενοι στον αριθµό των λέξεων που εµφανίζονται στα τµήµατα ενός κειµένου (γεγονός το οποίο 
λαµβάνει χώρα στο πρώτο γκρουπ πειραµάτων) ενώ µια δεύτερη δυνατότητα είναι να υπολογίσουµε 
το µέσο όρο και την τυπική απόκλιση από το µήκος τµήµατος βασιζόµενοι στον αριθµό των 
προτάσεων που εµφανίζονται στα τµήµατα ενός κειµένου (γεγονός το οποίο λαµβάνει χώρα στο 
δεύτερο γκρουπ πειραµάτων). Το τρίτο γκρουπ πειραµάτων χρησιµοποιεί µια παραλλαγή των 
«Μοντέλων ∆ιαχωρισµού Γινοµένου» (“Product Partition Models ”, [Gloss(00067)])  και ορίζει ένα 
πρόβληµα βελτιστοποίησης κάνοντας χρήση του προτεινόµενου αλγορίθµου µας. Τα εν λόγω 
πειράµατα παρουσιάζονται στις ενότητες που ακολουθούν. 
5.3.2.1 Πρώτο Γκρουπ Πειραµάτων 
 
Στην συνέχεια παρουσιάζουµε το πρώτο γκρουπ πειραµάτων στο οποίο εξετάζουµε τη συλλογή 
κειµένων του Choi και σύµφωνα µε τον τρόπο ορισµού των τµηµάτων µέσα σε αυτή,  υπολογίζουµε 
τον µέσο όρο και την τυπική απόκλιση του µήκους ενός τµήµατος βασιζόµενοι στον αριθµό των 
λέξεων που εµφανίζονται στα τµήµατα ενός κειµένου. Το εν λόγω γκρουπ αποτελείται από δυο σειρές 
 137
Κεφάλαιο 5 Αλγόριθµος Τµηµατοποίησης µε ∆υναµικό Προγραµµατισµό 
πειραµάτων. Η διαφορά µεταξύ των δυο αυτών σειρών έγκειται στην επιλογή των τιµών των 
παραµέτρων. Όπως αναφέρθηκε προηγούµενα, ο αλγόριθµος τµηµατοποίησης µε δυναµικό 
προγραµµατισµό χρησιµοποιεί τις τέσσερις παραµέτρους: µ, σ, r και γ (ο ρόλος καθεµιάς από αυτές 
περιγράφεται στην παράγραφο 5.2.2). Και στις δυο σειρές πειραµάτων επιθυµούµε να εξετάσουµε την 
επιρροή της µονάδας ως προς την οποία πραγµατοποιείται ο υπολογισµός του µέσου µήκους και της 
τυπικής απόκλισης στο µοντέλο µήκους καθώς επίσης και την επιρροή των παραµέτρων r και γ στην 
απόδοση τµηµατοποίησης (όπως αυτή µετράται σύµφωνα µε το κριτήριο του Beeferman), κάθε 
φορά µε διαφορετικό τρόπο.  
kP
 Στο πρώτο γκρουπ πειραµάτων και πιο συγκεκριµένα στην πρώτη σειρά πειραµάτων στόχος 
είναι ο καθορισµός του βαθµού επηρεασµού των παραµέτρων γ και r στην απόδοση τµηµατοποίησης. 
Η ακόλουθη διαδικασία επαναλαµβάνεται για τα Set0, Set1, Set2 και Set3: 
1. Καθορίζονται κατάλληλες τιµές για τις παραµέτρους µ και σ οι οποίες προσδιορίζονται 
χρησιµοποιώντας όλα τα κείµενα του εκάστοτε συνόλου (χρησιµοποιώντας τις στατιστικές 
προσεγγίσεις οι οποίες βασίζονται στον αριθµό των λέξεων).  
2. Η παράµετρος γ ορίζεται να παίρνει τις τιµές 0.00, 0.01, 0.02, …, 0.09, 0.1,0.2, 0.3, …, 1.0 ενώ η 
παράµετρος r να παίρνει στις τιµές 0.33, 0.5, 0.66 και 1. Αυτό οδηγεί  σε 20x4= 80 δυνατούς 
συνδυασµούς για τις τιµές των γ και r.  
3. Ο αλγόριθµος µας εκτελείται για κάθε έναν από τους δυνατούς συνδυασµούς των (γ, r).  
Επαναλαµβάνουµε τη διαδικασία για τις ακόλουθες συναρτήσεις “U-shape”, “V-shape”, 
“κορεσµένη U-shape” και “κορεσµένη V-shape” (“saturated”, [Gloss(00074)]). Οι Πίνακες 5.2 a-d  
περιέχουν τη βέλτιστη τιµή του  (και την αντίστοιχη τιµή των κριτηρίων Precision και Recall) η 
οποία επιτεύχθηκε σε κάθε µια από αυτές τις περιπτώσεις. Όπως είναι φανερό τα καλύτερα 
αποτελέσµατα είναι αυτά τα οποία προκύπτουν µε τη χρήση της συνάρτησης “U-shape”.  
kP
 
 
 
 
 
 
 138
Κεφάλαιο 5 Αλγόριθµος Τµηµατοποίησης µε ∆υναµικό Προγραµµατισµό 
 
Group Precision Recall kP  
Set0 (3-11) 81.47% 80.66% 8.43% 
Set1 (3-5) 86.47% 82.00% 6.82% 
Set2 (6-8) 83.03% 81.78% 5.97% 
Set3 (9-11) 83.99% 85.22% 5.02% 
All Sets 82.77% 81.66% 7.36% 
Πίνακας 5.2a: Οι καλύτερες τιµές των Precision, Recall και  για τα υποσύνολα  Set0, Set1, Set2 και Set3 για 
το σύνολο των κειµένων του Choi οι οποίες προκύπτουν από τις καλύτερες τιµές των παραµέτρων γ και r για την 
πρώτη σειρά του πρώτου γκρουπ πειραµάτων (δίχως την τεχνική της επαλήθευσης) µε τη χρήση της συνάρτησης 
“U-shape”. 
kP
 
Group Precision Recall kP  
Set0 (3-11) 70.91% 82.97% 10.94% 
Set1 (3-5) 79.57% 81.67% 9.27% 
Set2 (6-8) 76.18% 81.22% 7.49% 
Set3 (9-11) 73.17% 73.17% 7.12% 
All Sets 73.22% 81.73% 9.66% 
Πίνακας 5.2b: Οι καλύτερες τιµές των Precision, Recall και  για τα υποσύνολα  Set0, Set1, Set2 και Set3 για 
το σύνολο των κειµένων του Choi οι οποίες προκύπτουν από τις καλύτερες τιµές των παραµέτρων γ και r για την 
πρώτη σειρά του πρώτου γκρουπ πειραµάτων (δίχως την τεχνική της επαλήθευσης) µε τη χρήση της συνάρτησης 
“ κορεσµένη U-shape ” (“saturated”, [Gloss(00074)]). 
kP
 139
Κεφάλαιο 5 Αλγόριθµος Τµηµατοποίησης µε ∆υναµικό Προγραµµατισµό 
 
Group Precision Recall kP  
Set0 (3-11) 78.14% 80.75% 9.52% 
Set1 (3-5) 85.25% 82.33% 8.06% 
Set2 (6-8) 81.96% 83% 6.35% 
Set3 (9-11) 83.26% 86.22% 5.10% 
All Sets 80.43% 82.08% 8.3% 
Πίνακας 5.2c: Οι καλύτερες τιµές των Precision, Recall και  για τα υποσύνολα  Set0, Set1, Set2 και Set3 για 
το σύνολο των κειµένων του Choi οι οποίες προκύπτουν από τις καλύτερες τιµές των παραµέτρων γ και r για την 
πρώτη σειρά του πρώτου γκρουπ πειραµάτων (δίχως την τεχνική της επαλήθευσης) µε τη χρήση της συνάρτησης 
“V-shape”. 
kP
 
Group Precision Recall kP  
Set0 (3-11) 72.99% 81.64% 10.62% 
Set1 (3-5) 79.6% 82.22% 9.18% 
Set2 (6-8) 76.50% 79.78% 8.49% 
Set3 (9-11) 74.32% 81.45% 8.26% 
All Sets 74.63% 81.43% 9.77% 
Πίνακας 5.2d: Οι καλύτερες τιµές των Precision, Recall και  για τα υποσύνολα  Set0, Set1, Set2 και Set3 για 
το σύνολο των κειµένων του Choi οι οποίες προκύπτουν από τις καλύτερες τιµές των παραµέτρων γ και r για την 
πρώτη σειρά του πρώτου γκρουπ πειραµάτων (δίχως την τεχνική της επαλήθευσης) µε τη χρήση της συνάρτησης 
“κορεσµένη V -shape” (“saturated”, [Gloss(00074)]). 
kP
Μια ιδέα του τρόπου επηρεασµού των παραµέτρων γ και r στη απόδοση του κριτηρίου  της 
πρώτης σειράς του πρώτου γκρουπ πειραµάτων µπορεί να παρατηρηθεί στα σχήµατα 5.2-5.5 για την 
περίπτωση που χρησιµοποιείται η συνάρτηση U-shape η οποία επιτυγχάνει για τα καλύτερα 
αποτελέσµατα.  
kP
 
 140
Κεφάλαιο 5 Αλγόριθµος Τµηµατοποίησης µε ∆υναµικό Προγραµµατισµό 
0,00
0,05
0,10
0,15
0,20
0,25
0,30
0,35
0,40
0,45
0,50
0 0,1 0,2 0,3 0,4 0,5 0,6 0,7 0,8 0,9 1
γ
Pk
r = 1 r = 2/3 r = 1/2 r = 1/3
Σχήµα 5.2 : Γραφική παράσταση του  σαν συνάρτηση των γ και r για τα κείµενα του Set0 για την 
πρώτη σειρά του πρώτου γκρουπ πειραµάτων. 
kP
 
0
0,05
0,1
0,15
0,2
0,25
0,3
0,35
0,4
0,45
0 0,1 0,2 0,3 0,4 0,5 0,6 0,7 0,8 0,9 1
γ
Pk
 r = 1  r = 2/3  r = 1/2  r = 1/3
Σχήµα 5.3 : Γραφική παράσταση του  σαν συνάρτηση των γ και r για τα κείµενα του Set1 για την 
πρώτη σειρά του πρώτου γκρουπ πειραµάτων. 
kP
 
 
 
 141
Κεφάλαιο 5 Αλγόριθµος Τµηµατοποίησης µε ∆υναµικό Προγραµµατισµό 
0
0,1
0,2
0,3
0,4
0,5
0,6
0 0,1 0,2 0,3 0,4 0,5 0,6 0,7 0,8 0,9 1
γ
Pk
 r = 1  r = 2/3  r = 1/2 r = 1/3
 
Σχήµα 5.4: Γραφική παράσταση του  σαν συνάρτηση των γ και r για τα κείµενα του Set2 για την 
πρώτη σειρά του πρώτου γκρουπ πειραµάτων. 
kP
0
0,05
0,1
0,15
0,2
0,25
0,3
0,35
0,4
0,45
0,5
0 0,1 0,2 0,3 0,4 0,5 0,6 0,7 0,8 0,9 1
γ
Pk
 r = 1 r = 2/3  r = 1/2  r = 1/3
 
Σχήµα 5.5: Γραφική παράσταση του  σαν συνάρτηση των γ και r για τα κείµενα του Set3 για την 
πρώτη σειρά του πρώτου γκρουπ πειραµάτων. 
kP
 
 142
Κεφάλαιο 5 Αλγόριθµος Τµηµατοποίησης µε ∆υναµικό Προγραµµατισµό 
Παρόλα αυτά, µόνο όταν οι βέλτιστες τιµές των παραµέτρων γ και r καθώς και οι τιµές των 
παραµέτρων µ και σ είναι γνωστές εκ των προτέρων τότε µπορούµε να λάβουµε τις τιµές του Πίνακα 
5.2. Σε µια πραγµατική εφαρµογή καµία από αυτές τις τιµές δεν θα είναι εκ των προτέρων διαθέσιµη, 
γεγονός που καθιστά αναγκαία την εφαρµογή µιας διαδικασίας για τον καθορισµό των κατάλληλων 
τιµών των παραµέτρων γ, r, µ και σ η οποία θα οδηγεί σε µια ρεαλιστικότερη αξιολόγηση του 
αλγορίθµου µας.  
 Στη δεύτερη σειρά του πρώτου γκρουπ πειραµάτων για τον καθορισµό των κατάλληλων τιµών 
των παραµέτρων γ, r, µ και σ χρησιµοποιούµε δεδοµένα προς εκπαίδευση και µια διαδικασία 
επαλήθευσης των παραµέτρων. Στην συνέχεια ο αλγόριθµός µας αξιολογείται σε (προηγούµενα 
άγνωστα) κείµενα προς εξέταση. Πιο συγκεκριµένα, για κάθε ένα από τα υποσύνολα  Set0, Set1, Set2 
και Set3 πραγµατοποιούµε την ακόλουθη διαδικασία: 
1. Επιλέγονται µε τυχαίο τρόπο τα µισά από τα κείµενα του κάθε συνόλου και χρησιµοποιούνται ως 
δεδοµένα προς εκπαίδευση. Τα υπόλοιπα από αυτά χρησιµοποιούνται ως δεδοµένα προς εξέταση. 
2. Καθορίζονται οι κατάλληλες τιµές των παραµέτρων µ και σ χρησιµοποιώντας όλα τα δεδοµένα 
προς εκπαίδευση και τους στάνταρτ στατιστικούς εκτιµητές.  
3. Καθορίζονται κατάλληλες τιµές για τις παραµέτρους γ και r µετά την εκτέλεση του αλγορίθµου 
µας χρησιµοποιώντας όλα τα δεδοµένα προς εκπαίδευση και µε τους 80 δυνατούς συνδυασµούς 
των τιµών των παραµέτρων γ και r. Εκείνος ο συνδυασµός τιµών ο οποίος επιτυγχάνει την 
µικρότερη τιµή του θεωρείται ως ο βέλτιστος συνδυασµός τιµών. kP
4. Εκτελείται ο αλγόριθµός µας πάνω στα δεδοµένα προς εξέταση χρησιµοποιώντας τις παραπάνω 
υπολογισµένες τιµές των παραµέτρων γ , r, µ και σ.  
 
Η παραπάνω διαδικασία επαναλαµβάνεται πέντε φορές για κάθε ένα από τα τέσσερα σύνολα της 
δεύτερης σειράς του πρώτου γκρουπ πειραµάτων ενώ υπολογίζεται ο µέσος όρος των λαµβανόµενων 
τιµών των Precision, Recall και . Τα αποτελέσµατα των παραπάνω πειραµάτων παραθέτονται στον 
Πίνακα 5.3 αλλά και στα σχήµατα 5.6 και 5.7 παρέχοντας µια εποπτικότερη εικόνα των ληφθέντων 
αποτελεσµάτων. Ο Πίνακας 5.3 είναι ακριβώς ο ίδιος µε τον Πίνακα 5.2 µε την µόνη διαφορά ότι 
τώρα περιέχει τα αποτελέσµατα της δεύτερης σειράς του πρώτου γκρουπ πειραµάτων.  
kP
 
 
 143
Κεφάλαιο 5 Αλγόριθµος Τµηµατοποίησης µε ∆υναµικό Προγραµµατισµό 
 
Group Precision Recall kP  
Set0 (3-11) 83.89% 81.41% 7.16% 
Set1 (3-5) 84.69% 84.00% 7.54% 
Set2 (6-8) 84.50% 83.33% 5.51% 
Set3 (9-11) 88.30% 88.09% 3.08% 
All Sets 84.73% 84.73% 6.40% 
Πίνακας 5.3a: Οι τιµές των Precision, Recall και  για τα υποσύνολα  Set0, Set1, Set2 και Set3 για το σύνολο 
των κειµένων του Choi οι οποίες προκύπτουν από τις καλύτερες τιµές των παραµέτρων γ και r της δεύτερης 
σειράς του πρώτου γκρουπ πειραµάτων (µετά την εφαρµογή της τεχνικής της επαλήθευσης) µε τη χρήση της 
συνάρτησης “U-shape”.  
kP
 
Group Precision Recall kP  
Set0 (3-11) 76.79% 83.53% 8.85% 
Set1 (3-5) 77.86% 80.98% 10.01% 
Set2 (6-8) 79.84% 82.13% 6.93% 
Set3 (9-11) 78.85% 81.34% 6.48% 
All Sets 77.67% 82.65% 8.40% 
Πίνακας 5.3b: Οι τιµές των Precision, Recall και  για τα υποσύνολα  Set0, Set1, Set2 και Set3 για το σύνολο 
των κειµένων του Choi οι οποίες προκύπτουν από τις καλύτερες τιµές των παραµέτρων γ και r της δεύτερης 
σειράς του πρώτου γκρουπ πειραµάτων (µετά την εφαρµογή της τεχνικής της επαλήθευσης) µε τη χρήση της 
συνάρτησης “κορεσµένη U-shape” (“saturated”, [Gloss(00074)]). 
kP
 144
Κεφάλαιο 5 Αλγόριθµος Τµηµατοποίησης µε ∆υναµικό Προγραµµατισµό 
 
Group Precision Recall kP  
Set0 (3-11) 81.81% 82.93% 7.90% 
Set1 (3-5) 83.69% 82.36% 8.26% 
Set2 (6-8) 83.92% 82.88% 6.02% 
Set3 (9-11) 87.56% 86.4% 3.77% 
All Sets 83.20% 83.34% 7.09% 
Πίνακας 5.3c: Οι τιµές των Precision, Recall και  για τα υποσύνολα  Set0, Set1, Set2 και Set3 για το σύνολο 
των κειµένων του Choi οι οποίες προκύπτουν από τις καλύτερες τιµές των παραµέτρων γ και r της δεύτερης 
σειράς του πρώτου γκρουπ πειραµάτων (µετά την εφαρµογή της τεχνικής της επαλήθευσης) µε τη χρήση της 
συνάρτησης “V-shape”. 
kP
 
Group Precision Recall kP  
Set0 (3-11) 78.71% 81.88% 8.63% 
Set1 (3-5) 77.78% 79.64% 10.62% 
Set2 (6-8) 79.39% 83.15% 6.80% 
Set3 (9-11) 81.05% 84.31% 5.76% 
All Sets 79.01% 82.09% 8.24% 
Πίνακας 5.3d: Οι τιµές των Precision, Recall και  για τα υποσύνολα  Set0, Set1, Set2 και Set3 για το σύνολο 
των κειµένων του Choi οι οποίες προκύπτουν από τις καλύτερες τιµές των παραµέτρων γ και r της δεύτερης 
σειράς του πρώτου γκρουπ πειραµάτων (µετά την εφαρµογή της τεχνικής της επαλήθευσης) µε τη χρήση της 
συνάρτησης “κορεσµένη V-shape” (“saturated”, [Gloss(00074)]). 
kP
 145
Κεφάλαιο 5 Αλγόριθµος Τµηµατοποίησης µε ∆υναµικό Προγραµµατισµό 
0
3
6
9
12
Set0 (3-11), Set1 (3-5), Set2 (6-8), Set3 (9-11), All Sets
U-shape Pk κορεσµένη U-shape Pk
V-shape Pk κορεσµένη V-shape Pk
Σχήµα 5.6: Γραφική παράσταση του  σαν συνάρτηση για τα υποσύνολα  Set0, Set1, Set2 και Set3 
για το σύνολο των κειµένων του Choi οι οποίες προκύπτουν από τις καλύτερες τιµές των παραµέτρων 
γ και r της δεύτερης σειράς του πρώτου γκρουπ πειραµάτων (µετά την εφαρµογή της τεχνικής της 
επαλήθευσης) µε τη χρήση και των τεσσάρων συναρτήσεων κόστους τµηµατοποίησης. 
kP
0
15
30
45
60
75
90
Set0 (3-11), Set1 (3-5), Set2 (6-8), Set3 (9-11), All Sets
U-shape Precision U-shape Recall U-shape Pk
κορεσµ U-shape Precision κορεσµένη U-shape Recall κορεσµένη U-shape Pk
V-shape Precision V-shape Recall V-shape Pk
κορεσµένη V-shape Precision κορεσµένη V-shape Recall κορεσµένη V-shape Pk
 
Σχήµα 5.7 Οι τιµές των Precision, Recall και  για τα υποσύνολα  Set0, Set1, Set2 και Set3 για το 
σύνολο των κειµένων του Choi οι οποίες προκύπτουν από τις καλύτερες τιµές των παραµέτρων γ και r 
της δεύτερης σειράς του πρώτου γκρουπ πειραµάτων (µετά την εφαρµογή της τεχνικής της 
επαλήθευσης) µε τη χρήση και των τεσσάρων συναρτήσεων κόστους τµηµατοποίησης.
kP
 146
Κεφάλαιο 5 Αλγόριθµος Τµηµατοποίησης µε ∆υναµικό Προγραµµατισµό 
5.3.2.2 ∆εύτερο Γκρουπ Πειραµάτων 
 
Στην συνέχεια παρουσιάζουµε το δεύτερο γκρουπ πειραµάτων το οποίο υπολογίζει τον µέσο 
όρο και την τυπική απόκλιση του µήκους ενός τµήµατος µε βάση τον αριθµό των προτάσεων που 
εµφανίζονται στα τµήµατα ενός κειµένου σύµφωνα µε τον τρόπο ορισµού ο οποίος περιγράφτηκε 
στην παράγραφο 5.2.2. Το εν λόγω γκρουπ, σε απόλυτη αναλογία µε το πρώτο γκρουπ, αποτελείται 
από δυο σειρές πειραµάτων. Η διαφορά µεταξύ των δυο αυτών σειρών έγκειται στην επιλογή των 
τιµών των παραµέτρων. Και στις δυο σειρές πειραµάτων επιθυµούµε να εξετάσουµε την επιρροή της 
µονάδας ως προς την οποία πραγµατοποιείται ο υπολογισµός του µέσου µήκους και της τυπικής 
απόκλισης στο µοντέλο µήκους καθώς επίσης και την επιρροή των παραµέτρων r και γ στην απόδοση 
τµηµατοποίησης (όπως αυτή µετράται σύµφωνα µε το κριτήριο του Beeferman). kP
Στην πρώτη σειρά πειραµάτων στόχος είναι ο καθορισµός του βαθµού επηρεασµού των 
παραµέτρων γ και r στην απόδοση τµηµατοποίησης (όπως αυτή µετράται σύµφωνα µε το 
κριτήριο του Beeferman). Η διαδικασία η οποία ακολουθείται είναι εντελώς αντίστοιχη µε αυτή 
που ακολουθείται στην πρώτη σειρά πειραµάτων του πρώτου γκρουπ. Η εν λόγω διαδικασία 
πραγµατοποιείται χρησιµοποιώντας τις συναρτήσεις “U-shape”, “V-shape”, “κορεσµένη U-shape” και 
“κορεσµένη V-shape” (“saturated”, [Gloss(00074)]). Οι Πίνακες 5.4
kP
 a-d  περιέχουν τη βέλτιστη τιµή 
του  (και την αντίστοιχη τιµή των κριτηρίων Precision και Recall) η οποία επιτεύχθηκε σε κάθε 
µια από αυτές τις περιπτώσεις. Όπως είναι φανερό τα καλύτερα αποτελέσµατα είναι αυτά τα οποία 
προκύπτουν µε τη χρήση της συνάρτησης “U-shape”.  
kP
 
Group Precision Recall kP  
Set0 (3-11) 81.27% 84.20% 7.00% 
Set1 (3-5) 89.54% 89.55% 4.75% 
Set2 (6-8) 89.82% 90.00% 2.40% 
Set3 (9-11) 94.22% 94.22% 1.00% 
All Sets 85.53% 87.24% 5.16% 
Πίνακας 5.4a: Οι τιµές των Precision, Recall και  για τα υποσύνολα  Set0, Set1, Set2 και Set3 για το σύνολο 
των κειµένων του Choi οι οποίες προκύπτουν από τις καλύτερες τιµές των παραµέτρων γ και r της πρώτης 
kP
 147
Κεφάλαιο 5 Αλγόριθµος Τµηµατοποίησης µε ∆υναµικό Προγραµµατισµό 
σειράς του δεύτερου γκρουπ πειραµάτων (δίχως την εφαρµογή της τεχνικής της επαλήθευσης) µε τη χρήση της 
συνάρτησης “U-shape”.   
 
Group Precision Recall kP  
Set0 (3-11) 69.38% 81.97% 12.10% 
Set1 (3-5) 78.16% 84.11% 8.626% 
Set2 (6-8) 75.75% 77.78% 7.078% 
Set3 (9-11) 72.02% 73.55% 5.92% 
All Sets 71.93% 80.47% 10.00% 
Πίνακας 5.4b: Οι τιµές των Precision, Recall και  για τα υποσύνολα  Set0, Set1, Set2 και Set3 για το σύνολο 
των κειµένων του Choi οι οποίες προκύπτουν από τις καλύτερες τιµές των παραµέτρων γ και r της πρώτης 
σειράς του δεύτερου γκρουπ πειραµάτων (δίχως την εφαρµογή της τεχνικής της επαλήθευσης) µε τη χρήση της 
συνάρτησης “κορεσµένη U-shape” (“saturated”, [Gloss(00074)]).  
kP
 
Group Precision Recall kP  
Set0 (3-11) 73.53% 80.77% 11.00% 
Set1 (3-5) 85.28% 86.11% 6.51% 
Set2 (6-8) 86.48% 86.55% 3.61% 
Set3 (9-11) 88.58% 88.77% 2.03% 
All Sets 79.21% 83.50% 8.02% 
Πίνακας 5.4c: Οι τιµές των Precision, Recall και  για τα υποσύνολα  Set0, Set1, Set2 και Set3 για το σύνολο 
των κειµένων του Choi οι οποίες προκύπτουν από τις καλύτερες τιµές των παραµέτρων γ και r της πρώτης 
σειράς του δεύτερου γκρουπ πειραµάτων (δίχως την εφαρµογή της τεχνικής της επαλήθευσης) µε τη χρήση της 
συνάρτησης “V-shape”.  
kP
 
 
 
 148
Κεφάλαιο 5 Αλγόριθµος Τµηµατοποίησης µε ∆υναµικό Προγραµµατισµό 
Group Precision Recall kP  
Set0 (3-11) 71.63% 80.52% 10.88% 
Set1 (3-5) 78.83% 83.44% 8.71% 
Set2 (6-8) 76.95% 76.44% 7.21% 
Set3 (9-11) 73.45% 74.89% 5.84% 
All Sets 73.68% 79.55% 9.33% 
Πίνακας 5.4d: Οι τιµές των Precision, Recall και  για τα υποσύνολα  Set0, Set1, Set2 και Set3 για το σύνολο 
των κειµένων του Choi οι οποίες προκύπτουν από τις καλύτερες τιµές των παραµέτρων γ και r της πρώτης 
σειράς του δεύτερου γκρουπ πειραµάτων (δίχως την εφαρµογή της τεχνικής της επαλήθευσης) µε τη χρήση της 
συνάρτησης “κορεσµένη V-shape” (“saturated”, [Gloss(00074)]).  
kP
 Μια ιδέα του τρόπου επηρεασµού των παραµέτρων γ και r στη απόδοση του κριτηρίου  µε 
τη χρήση της συνάρτησης “ U-shape” µπορεί να παρατηρηθεί στα σχήµατα 5.8-5.11. 
kP
 
0
0,05
0,1
0,15
0,2
0,25
0,3
0,35
0,4
0,45
0,5
0 0,1 0,2 0,3 0,4 0,5 0,6 0,7 0,8 0,9 1
γ
Pk
r = 1
r =2/3
r =1/2
r =1/3
 
Σχήµα 5.8: Γραφική παράσταση του  σαν συνάρτηση των γ και r για τα κείµενα του Set0 για την 
πρώτη σειρά του δεύτερου γκρουπ πειραµάτων. 
kP
  
 149
Κεφάλαιο 5 Αλγόριθµος Τµηµατοποίησης µε ∆υναµικό Προγραµµατισµό 
0
0,05
0,1
0,15
0,2
0,25
0,3
0,35
0,4
0 0,1 0,2 0,3 0,4 0,5 0,6 0,7 0,8 0,9 1
γ
Pk
r = 1
r =2/3
r =1/2
r =1/3
 
Σχήµα 5.9: Γραφική παράσταση του  σαν συνάρτηση των γ και r για τα κείµενα του Set1 για την 
πρώτη σειρά του δεύτερου γκρουπ πειραµάτων. 
kP
 
0
0,05
0,1
0,15
0,2
0,25
0,3
0,35
0 0,1 0,2 0,3 0,4 0,5 0,6 0,7 0,8 0,9 1
γ
Pk
r = 1
r =2/3
r =1/2
r =1/3
 
Σχήµα 5.10: Γραφική παράσταση του  σαν συνάρτηση των γ και r για τα κείµενα του Set2 για την 
πρώτη σειρά του δεύτερου γκρουπ πειραµάτων. 
kP
 
 
 150
Κεφάλαιο 5 Αλγόριθµος Τµηµατοποίησης µε ∆υναµικό Προγραµµατισµό 
0
0,05
0,1
0,15
0,2
0,25
0,3
0,35
0 0,1 0,2 0,3 0,4 0,5 0,6 0,7 0,8 0,9 1
γ
Pk
r = 1
r =2/3
r =1/2
r =1/3
 
Σχήµα 5.11: Γραφική παράσταση του  σαν συνάρτηση των γ και r για τα κείµενα του Set3 για την 
πρώτη σειρά του δεύτερου γκρουπ πειραµάτων. 
kP
 
Όπως µπορεί κανείς να παρατηρήσει από τα σχήµατα 5.8-5.11, οι καλύτερες τιµές των 
Precision, Recall και  είναι αυτές που παραθέτονται στον Πίνακα 5.4. Τα αποτελέσµατα αυτά είναι 
καλύτερα από όλα όσα έχουν µέχρι τώρα αναφερθεί στην βιβλιογραφία ([Choi, 2000], [Choi et al., 
2001], [Utiyama & Isahara, 2001]) σε ότι αφορά την συλλογή κειµένων του Choi.  
kP
Παρόλα αυτά, τα αποτελέσµατα του Πίνακα 5.4 µπορούν να ληφθούν µόνο όταν είναι 
γνωστές εκ των προτέρων οι βέλτιστες τιµές των παραµέτρων γ και r καθώς και οι τιµές των 
παραµέτρων µ και σ. Σε µια πραγµατική εφαρµογή καµία από αυτές τις τιµές δεν θα είναι εκ των 
προτέρων διαθέσιµη, γεγονός που καθιστά αναγκαία την εφαρµογή µιας διαδικασίας για τον 
καθορισµό των κατάλληλων τιµών των παραµέτρων γ, r, µ και σ η οποία θα οδηγεί σε µια 
ρεαλιστικότερη αξιολόγηση του αλγορίθµου µας.  
Στην δεύτερη σειρά πειραµάτων χρησιµοποιούµε αρχικά δεδοµένα προς εκπαίδευση και την 
ίδια διαδικασία επαλήθευσης των παραµέτρων όπως ακριβώς και στο πρώτο γκρουπ πειραµάτων, και 
πιο συγκεκριµένα στην δεύτερη σειρά, για τον καθορισµό των κατάλληλων τιµών των παραµέτρων γ, 
r, µ και σ. Στην συνέχεια ο αλγόριθµός µας αξιολογείται σε (προηγούµενα άγνωστα) κείµενα προς 
εξέταση.  
Η παραπάνω διαδικασία επαναλαµβάνεται πέντε φορές, όπως ακριβώς και στη δεύτερη σειρά 
του πρώτου γκρουπ πειραµάτων,  για κάθε ένα από τα τέσσερα σύνολα της δεύτερης σειράς του 
 151
Κεφάλαιο 5 Αλγόριθµος Τµηµατοποίησης µε ∆υναµικό Προγραµµατισµό 
δεύτερου γκρουπ πειραµάτων, ενώ υπολογίζεται ο µέσος όρος των λαµβανόµενων τιµών των 
Precision, Recall και . Η εν λόγω διαδικασία πραγµατοποιείται για τις συναρτήσεις “U-shape”, “V-
shape”, “κορεσµένη U-shape” και “κορεσµένη V-shape ” (“saturated”, [Gloss(00074)]). Οι Πίνακες 
5.5
kP
 a-d  περιέχουν τη βέλτιστη τιµή του  (και την αντίστοιχη τιµή των κριτηρίων Precision και 
Recall) η οποία επιτεύχθηκε σε κάθε µια από αυτές τις περιπτώσεις. Όπως είναι φανερό τα καλύτερα 
αποτελέσµατα είναι αυτά τα οποία προκύπτουν µε τη χρήση της συνάρτησης “U-shape”. Τα 
αποτελέσµατα των παραπάνω πειραµάτων παραθέτονται στους Πίνακες 5.5a-d αλλά και στα Σχήµατα 
5.12 και 5.13.  
kP
 
Group Precision Recall kP  
Set0 (3-11) 82.66% 82.78% 7.00% 
Set1 (3-5) 88.17% 87.70% 5.45% 
Set2 (6-8) 88.68% 88.71% 3.00% 
Set3 (9-11) 92.37% 92.44% 1.33% 
All Sets 85.70% 85.73% 5.39% 
Πίνακας 5.5a: Οι τιµές των Precision, Recall και  για τα υποσύνολα  Set0, Set1, Set2 και Set3 για το σύνολο 
των κειµένων του Choi οι οποίες προκύπτουν από τις καλύτερες τιµές των παραµέτρων γ και r της δεύτερης 
σειράς του δεύτερου γκρουπ πειραµάτων (µετά την εφαρµογή της τεχνικής της επαλήθευσης) µε τη χρήση της 
συνάρτησης “U-shape”.  
kP
 
Group Precision Recall kP  
Set0 (3-11) 76.87% 81.54% 8.74% 
Set1 (3-5) 78.06% 82.62% 9.05% 
Set2 (6-8) 79.41% 80.62% 6.31% 
Set3 (9-11) 76.80% 78.53% 4.69% 
All Sets 77.40% 81.13% 7.86% 
Πίνακας 5.5b: Οι τιµές των Precision, Recall και  για τα υποσύνολα  Set0, Set1, Set2 και Set3 για το σύνολο 
των κειµένων του Choi οι οποίες προκύπτουν από τις καλύτερες τιµές των παραµέτρων γ και r της δεύτερης 
kP
 152
Κεφάλαιο 5 Αλγόριθµος Τµηµατοποίησης µε ∆υναµικό Προγραµµατισµό 
σειράς του δεύτερου γκρουπ πειραµάτων (µετά την εφαρµογή της τεχνικής της επαλήθευσης) µε τη χρήση της 
συνάρτησης “κορεσµένη U-shape” (“saturated”, [Gloss(00074)]).  
 
Group Precision Recall kP  
Set0 (3-11) 81.80% 82.67% 7.35% 
Set1 (3-5) 86.11% 85.06% 6.79% 
Set2 (6-8) 86.81% 86.08% 3.70% 
Set3 (9-11) 91.14% 90.75% 1.68% 
All Sets 84.46% 84.66% 5.94% 
Πίνακας 5.5c: Οι τιµές των Precision, Recall και  για τα υποσύνολα  Set0, Set1, Set2 και Set3 για το σύνολο 
των κειµένων του Choi οι οποίες προκύπτουν από τις καλύτερες τιµές των παραµέτρων γ και r της δεύτερης 
σειράς του δεύτερου γκρουπ πειραµάτων (µετά την εφαρµογή της τεχνικής της επαλήθευσης) µε τη χρήση της 
συνάρτησης “V-shape”.  
kP
 
Group Precision Recall kP  
Set0 (3-11) 77.32% 81.65% 8.70% 
Set1 (3-5) 78.01% 82.79% 9.07% 
Set2 (6-8) 78.77% 78.57% 6.74% 
Set3 (9-11) 78.24% 79.60% 4.75% 
All Sets 77.75% 81.08% 7.90% 
Πίνακας 5.5d: Οι τιµές των Precision, Recall και  για τα υποσύνολα  Set0, Set1, Set2 και Set3 για το σύνολο 
των κειµένων του Choi οι οποίες προκύπτουν από τις καλύτερες τιµές των παραµέτρων γ και r της δεύτερης 
σειράς του δεύτερου γκρουπ πειραµάτων (µετά την εφαρµογή της τεχνικής της επαλήθευσης) µε τη χρήση της 
συνάρτησης “κορεσµένη V -shape” (“saturated”, [Gloss(00074)]).  
kP
 153
Κεφάλαιο 5 Αλγόριθµος Τµηµατοποίησης µε ∆υναµικό Προγραµµατισµό 
0
2
4
6
8
Set0 (3-11), Set1 (3-5), Set2 (6-8), Set3 (9-11), All Sets
U-shape Pk κορεσµένη U-shape Pk
V-shape Pk κορεσµένη V-shape Pk
 
Σχήµα 5.12: Γραφική παράσταση του  για τα υποσύνολα  Set0, Set1, Set2 και Set3 για το σύνολο 
των κειµένων του Choi οι οποίες προκύπτουν από τις καλύτερες τιµές των παραµέτρων γ και r της 
δεύτερης σειράς του δεύτερου γκρουπ πειραµάτων (µετά την εφαρµογή της τεχνικής της 
επαλήθευσης) µε τη χρήση και των τεσσάτων συναρτήσεων κόστους τµηµατοποίησης.  
kP
0
15
30
45
60
75
90
Set0 (3-11), Set1 (3-5), Set2 (6-8), Set3 (9-11), All Sets
U-shape Precision U-shape Recall U-shape Pk
κορεσµ U-shape Precision κορεσµένη U-shape Recall κορεσµένη U-shape Pk
V-shape Precision V-shape Recall V-shape Pk
κορεσµένη V-shape Precision κορεσµένη V-shape Recall κορεσµένη V-shape Pk
Σχήµα 5.13: Γραφική παράσταση των Precision, Recall και  για τα υποσύνολα  Set0, Set1, Set2 
και Set3 για το σύνολο των κειµένων του Choi οι οποίες προκύπτουν από τις καλύτερες τιµές των 
παραµέτρων γ και r της δεύτερης σειράς του δεύτερου γκρουπ πειραµάτων (µετά την εφαρµογή της 
τεχνικής της επαλήθευσης) µε τη χρήση και των τεσσάτων συναρτήσεων κόστους τµηµατοποίησης.  
kP
 
 154
Κεφάλαιο 5 Αλγόριθµος Τµηµατοποίησης µε ∆υναµικό Προγραµµατισµό 
Παρά το γεγονός ότι οι τιµές του Πίνακα 5.5 είναι ελαφρώς χειρότερες από αυτές του Πίνακα 
5.4, εξακολουθούν να είναι καλύτερες από όλες τις προηγούµενα αναφερθείσες στην βιβλιογραφία 
πάνω στην συλλογή κειµένων του Choi. Στον Πίνακα 5.6 παραθέτουµε τις τιµές του  οι οποίες 
επιτεύχθηκαν από προηγούµενους αλγορίθµους τµηµατοποίησης πάνω στην συλλογή κειµένων του 
Choi ([Choi, 2000], [Choi et al., 2001], [Utiyama & Isahara, 2001]). Στην πρώτη στήλη του πίνακα 
αναγράφεται ο χρησιµοποιούµενος αλγόριθµος, στην δεύτερη η επιστηµονική δηµοσίευση στην οποία 
εµφανίζεται το εν λόγω αποτέλεσµα. Στις επόµενες τέσσερις στήλες αναγράφονται οι τιµές του  για 
τα σύνολα κειµένων Set0, Set1, Set2, Set3 ενώ στην τελευταία στήλη αναγράφονται οι µέσοι όροι της 
απόδοσης των συνόλων (όπου δίνονται µόνο τα αποτελέσµατα του  µιας και δεν παρέχονται αυτά 
των Precision και Recall στις  [Choi, 2000], [Choi et al., 2001], [Utiyama & Isahara, 2001]).  Τα 
αποτελέσµατα του δικού µας αλγορίθµου µε τη χρήση της συνάρτησης “U-shape” παρέχονται στις 
τελευταίες δυο γραµµές. Είναι φανερό ότι, ο αλγόριθµός µας επιτυγχάνει καλύτερη απόδοση από 
όλους τους υπόλοιπους. Αξίζει να σηµειωθεί ότι, η καλύτερη απόδοση επιτυγχάνεται όταν το γ παίρνει 
τιµές από το διάστηµα [0.08,0.4] και το r ισούται είτε µε 0.66 είτε µε 0.5. 
kP
kP
kP
 155
Κεφάλαιο 5 Αλγόριθµος Τµηµατοποίησης µε ∆υναµικό Προγραµµατισµό 
 
 Επιστηµον. ∆ηµοσίευση T3,11 T3,5 T6,8 T9,11 T 
CWM1 [Choi et al., 2001] 9% 10% 7% 5% 8% 
U00b [Utiyama&Isahara, 2001] 10% 9% 7% 5% 9% 
CWM3 [Choi et al., 2001] 12% 10% 9% 8% 11% 
C99b [Choi, 2000] 12% 11% 10% 9% 11% 
U00 [Utiyama&Isahara, 2001] 11% 13% 6% 6% 10%  
C99 [Choi, 2000] 13% 18% 10% 10% 13%   
CWM2 [Choi et al., 2001] 14% 10% 11% 12% 13% 
C99b,-r [Choi, 2000] 23% 19% 21% 20% 22% 
1ο γκρουπ(2η σειρά)  [Kehagias et al.,2003(a)]  7.16% 7.54% 5.51% 3.08% 6.40% 
2ο γκρουπ(2η σειρά) [Kehagias et al.,2003(a)]  7.00% 5.45% 3% 1.33% 5.39% 
Πίνακας 5.6: Σύγκριση των λαµβανόµενων τιµών του κριτηρίου  από διάφορους αλγορίθµους που 
εµφανίζονται στην βιβλιογραφία µε τα αντίστοιχα της δεύτερης σειράς και των δυο γκρουπ πειραµάτων του 
δικού µας αλγορίθµου εφαρµοζόµενοι πάνω σε καθένα από τα σύνολα Set0, Set1, Set2 και Set3 των δεδοµένων 
του Choi.  
kP
5.3.2.3 Τρίτο Γκρουπ Πειραµάτων 
 
5.3.2.3.1 Ορισµός του προβλήµατος 
 
Όπως αναφέρθηκε και προηγούµενα, προσεγγίσαµε το πρόβληµα της τµηµατοποίησης από µια 
διαφορετική σκοπιά µε στόχο τη λήψη µιας πιο εµπεριστατωµένης άποψης της απόδοσης του 
αλγορίθµου µας. Πιο συγκεκριµένα, χρησιµοποιήσαµε τα «Μοντέλα ∆ιαχωρισµού Γινοµένου» για τη 
διατύπωση της τµηµατοποίησης ως ένα πρόβληµα βελτιστοποίησης το οποίο επιλύουµε µε τον 
προτεινόµενο µας αλγόριθµο τµηµατοποίησης µε δυναµικό προγραµµατισµό. 
Ένα Μοντέλο ∆ιαχωρισµού Γινοµένου (“Product Partition Model (ΡΡΜ)”, [Gloss(00067)]) 
αποτελεί µια Bayesian διαδικασία εξαγωγής συµπεράσµατος (“Bayesian inference procedure”, 
[Gloss(00008)]) για την τµηµατοποίηση µιας ακολουθίας τυχαίων µεταβλητών βασιζόµενη στην 
ετερογένεια αυτής. Τα «Μοντέλα ∆ιαχωρισµού Γινοµένου» παρουσιάστηκαν από τους Barry και 
 156
Κεφάλαιο 5 Αλγόριθµος Τµηµατοποίησης µε ∆υναµικό Προγραµµατισµό 
Hartigan ([Barry & Hartigan, 1992], [Barry & Hartigan, 1993]) (δες επίσης ([Crowley, 1997], [Loschi 
& Cruz, 2002]) για την αναγνώριση πολλών σηµείων αλλαγής µέσα στην ακολουθία τυχαίων 
µεταβλητών, βασιζόµενοι στον µέσο όρο και την τυπική απόκλιση αυτής από την κανονική κατανοµή. 
Το µοντέλο υποθέτει ότι η τυχαία τµηµατοποίηση, η οποία προκύπτει από τα σηµεία αλλαγής 
,εµφανίζει µια κατανοµή πιθανοτήτων - ξεχωριστή για κάθε ένα τµήµα - η οποία είναι ανάλογη του 
γινοµένου των προηγούµενων συναφειών – συνοχών (“prior cohesions”, [Gloss(00066)]). ∆οθέντων 
των παρατηρήσεων, ένα νέο µοντέλο διαχωρισµού γινοµένου εξακολουθεί να ισχύει µε µεταγενέστερες 
συνάφειες – συνοχές (“posterior cohesions”, [Gloss(00064)]) για τα τµήµατα.  
 Σε αυτό το γκρουπ πειραµάτων, χρησιµοποιούµε το πλαίσιο εργασίας των «Μοντέλων 
∆ιαχωρισµού Γινοµένου» για την αναγνώριση των τµηµάτων των κειµένων. Προς αυτή την 
κατεύθυνση λαµβάνουµε την µεταγενέστερη από κοινού πιθανότητα ενός εξεταζόµενου κειµένου και 
της τµηµατοποίησης αυτού, ως γινόµενο των ακόλουθων δυο παραγόντων: (α) της πιθανότητας 
τµηµατοποίησης, η οποία περιγράφεται από κατάλληλες προηγούµενες συνάφειες – συνοχές και (β) 
της υπό συνθήκη (δοθείσης της τµηµατοποίησης) πιθανότητας του πίνακα οµοιότητας µεταξύ 
προτάσεων, ο οποίος εκφράζεται από µια κατάλληλη συνάρτηση οµοιογένειας. Αξίζει να σηµειωθεί 
ότι, χρησιµοποιούµε τα «Μοντέλα ∆ιαχωρισµού Γινοµένου» για την ανάθεση πιθανοτήτων σε 
διδιάστατες δοµές (τους πίνακες οµοιότητας µεταξύ των προτάσεων) αντί σε µονοδιάστατες 
ακολουθίες. Ο αρνητικός λογάριθµος της από κοινού πιθανότητας αποτελεί το κόστος τµηµατοποίησης, 
το οποίο ελαχιστοποιείται από τον προτεινόµενο µας αλγόριθµο τµηµατοποίησης µε δυναµικό 
προγραµµατισµό (συγκρινόµενος µε τη χρήση των υπολογιστικά «δαπανηρών» αλγορίθµων Markov 
Chain Monte Carlo, [Barry & Hartigan, 1992], [Barry & Hartigan, 1993]). Από όσο µας είναι γνωστό, 
η προσέγγιση αυτή δεν έχει προηγούµενα χρησιµοποιηθεί σε συνδυασµό µε τα «Μοντέλα 
∆ιαχωρισµού Γινοµένου».  
5.3.2.3.2 Μοντέλα ∆ιαχωρισµού Γινοµένου 
 
Στην συνέχεια θα θεωρήσουµε ότι ο αριθµός των προτάσεων του κειµένου προς τµηµατοποίηση 
Τ είναι γνωστός. Ορίζουµε δυο τυχαίες µεταβλητές: Η µεταβλητή τµηµατοποίησης Τ λαµβάνει τιµές 
από το  (όπου µε το  συµβολίζουµε το σύνολο όλων των πιθανών τµηµατοποιήσεων) και η 
µεταβλητή οµοιότητας προτάσεων D λαµβάνει τιµές από το 
TΦ TΦ
TΨ  (όπου µε το  συµβολίζουµε το 
σύνολο όλων των πιθανών πινάκων οµοιότητας προτάσεων διάστασης ΤxΤ ).  Κάθε µια από αυτές τις 
µεταβλητές χαρακτηρίζεται από την δική της (διακριτή) συνάρτηση πιθανότητας. Στην συνέχεια, θα 
συµβολίζουµε τις συναρτήσεις πιθανότητας µε το γράµµα f (και κατάλληλους κάτω δείκτες). Για 
παράδειγµα,  
TΨ
 157
Κεφάλαιο 5 Αλγόριθµος Τµηµατοποίησης µε ∆υναµικό Προγραµµατισµό 
           (5.9) )],,...,,(ÔPr[),...,,( 1010 KKT ttttttf ==
)],...,,(Ô|dDPr[),...,,|d( 1010| KKTD ttttttf ===          (5.10) 
Στα [Barry & Hartigan, 1992] και [Barry & Hartigan, 1993] τα «Μοντέλα ∆ιαχωρισµού 
Γινοµένου» εισήχθησαν για να περιγράψουν πιθανοτικά τη δηµιουργία των ανοµοιογενών 
χρονοσειρών. Ο ίδιος φορµαλισµός είναι δυνατό να εφαρµοστεί (µε µερικές τροποποιήσεις) στην 
τµηµατοποίηση κειµένου. Τροποποιώντας τον ορισµό των [Barry & Hartigan, 1992] και [Barry & 
Hartigan, 1993] ορίζουµε ένα «Μοντέλο ∆ιαχωρισµού Γινοµένου» ως ένα ζεύγος τυχαίων µεταβλητών 
(D,Τ) το οποίο έχει ένα ξεχωριστό τύπο για την από κοινού κατανοµή πιθανότητας . Πιο 
συγκεκριµένα, ένα ζεύγος (D,Τ) είναι ένα «Μοντέλο ∆ιαχωρισµού Γινοµένου» αν ικανοποιούνται οι 
παρακάτω συνθήκες: 
TDf ,
1. Η πιθανότητα µιας συγκεκριµένης τµηµατοποίησης έχει την µορφή: ),...,,( 10 Kttt
),(...),(),(),...,,( 12110110Ô KKK ttcttcttcGtttf −⋅⋅⋅⋅=         (5.11) 
όπου η συνάρτηση συνάφειας-συνοχής c(s,t) (η οποία σχετίζεται µε το τµήµα { s , , …, 
..t} ορίζεται για όλους τους ακέραιους s,t
1+ 2+s
∈{1,2…, Τ}. Το είναι µια κανονικοποιηµένη σταθερά 
και το Κ στην (5.11) µπορεί να λάβει τιµές από 1 ως Τ (αξίζει να σηµειωθεί ότι η εξίσωση (5.11) 
δεν συνεπάγεται ότι τα µήκη των τµηµάτων t - t , - t …, - t  είναι ανεξάρτητα) .  
1G
2 1 3t 2 Kt 1−K
 
 
2. Κάνοντας την υπόθεση στο Τ=( t ), η πυκνότητα πιθανότητας του υποπίνακα 
D( t έχει την µορφή (για k=1,2…, K) 
Ktt ,...,, 10
),1 kk t−
*:.  
))t,t(d()t,t(),...,,|)t,t(d( k1-kk1-k210k1-kÔ|)t, tD( k-1k gGtttf K ⋅=    (5.12) 
όπου είναι µια συνάρτηση οµοιογένειας και G  είναι µια σταθερά κανονικοποίησης.  )(⋅g )t,t( k1-k2
                                                 
* Χρησιµοποιούµε τον συµβολισµό d(s, t) για να αναφερθούµε στον τετραγωνικό υποπίνακα του d ο οποίος 
ορίζεται από τις θέσεις (s+1, s+1) και (t, t). Για κάθε s = 0,1,2…, Τ-1 και  για t = 1,2…, Τ λαµβάνουµε ένα 
υποπίνακα d(s, t) ο οποίος αντιστοιχεί στο τµήµα (s+1, s+2,…, t). 
 158
Κεφάλαιο 5 Αλγόριθµος Τµηµατοποίησης µε ∆υναµικό Προγραµµατισµό 
Στην συνέχεια, η από κοινού πιθανότητα των D και Τ έχει την µορφή  
∏
=
⋅⋅⋅=
K
k
K gGcGtttf
1
k1-kk1-k2k1-k110Ô|D ))]t,t(d()t,t()t,t([),...,,d,(       (5.13) 
Είναι φανερό ότι ένα «Μοντέλο ∆ιαχωρισµού Γινοµένου» χαρακτηρίζεται από την συνάρτηση 
συνάφειας – συνοχής (η οποία αναθέτει πιθανότητες στις τµηµατοποιήσεις , ανεξάρτητα 
από τις οµοιότητες µεταξύ προτάσεων d) και τη συνάρτηση οµοιογένειας (η οποία αναθέτει µια 
πιθανότητα σε κάθε τµήµα δοθείσης µιας τµηµατοποίησης). 
),...,,( 10 Kttt
 Παρουσιάζουµε τώρα τις συγκεκριµένες µορφές συνάφειας – συνοχής και οµοιογένειας τις 
οποίες χρησιµοποιούµε σε αυτό το γκρουπ πειραµάτων. Για την συνάφεια χρησιµοποιούµε (µε 
) Tts ≤≤ ≺0
  













 −−
⋅−=
2
2
exp),(
σ
µγ sttsc          (5.14) 
(όπου οι παράµετροι µ, t, s είναι ρυθµιστικοί παράµετροι). Για την οµοιογένεια χρησιµοποιούµε την 
µορφή: 
























−
⋅−=
∑ ∑
+= +=
r
t
si
t
sj
ji
st
d
tsg
)(
)1(exp)],(d[ 1 1
,
γ          (5.15) 
(όπου τα γ και r είναι παράµετροι). Η συνάφεια – συνοχή της (5.14) χρησιµοποιείται για να 
συµπεριλάβει κάποια πληροφορία σχετικά µε το µήκος τµήµατος (για παράδειγµα, αν είναι διαθέσιµα 
τµηµατοποιηµένα δεδοµένα εκπαίδευσης, τότε από αυτά µπορούν να εκτιµηθούν η µέση τιµή µ και η 
τυπική απόκλιση σ του µήκους τµήµατος). Η οµοιογένεια της (5.15) αναθέτει υψηλή πιθανότητα σε 
τµήµατα µε µεγάλες τιµές του r
t
si
t
sj
ji
st
d
)(
1 1
,
−
∑ ∑
+= += . 
∆οθέντων των επιλογών που παρουσιάστηκαν παραπάνω, ορίζουµε το κόστος τµηµατοποίησης 
να είναι ο αρνητικός λογάριθµος της από κοινού πιθανότητας: 
 159
Κεφάλαιο 5 Αλγόριθµος Τµηµατοποίησης µε ∆υναµικό Προγραµµατισµό 
( )
( )rkk
t
tt
t
tt
ts
kk
K
k
kk
tt
D
ttG
tt
rtJ
k
k
k
k
1
1 1
,
12
1
2
2
1 1 1)1()(log
2
),,,;(
−
+= +=
−
=
−
−
⋅−−−−
⋅
−−
⋅=
∑ ∑
∑ − −γσ
µ
γγσµ  (5.16) 
Ο όρος  µπορεί να συµπεριληφθεί στο τµήµα συνάφειας – συνοχής του κόστους. )(log 12 −− kk ttG
Αξίζει να επιστήσουµε την προσοχή στο ότι, παρά την επίσηµη οµοιότητα της εξίσωσης (5.14) µε την 
κανονική κατανοµή, το µήκος τµήµατος δεν αποτελεί Gaussian τυχαία µεταβλητή. Το σηµείο κλειδί 
αποτελεί το ότι η (5.14) ορίζει µια πιθανοτική κατανοµή των τµηµατοποιήσεων και όχι των µηκών 
τµηµάτων*.  
5.3.2.3.3 Αποτελέσµατα Τµηµατοποίησης µε χρήση «Μοντέλων ∆ιαχωρισµού Γινοµένου» 
 
Σε αυτή την παράγραφο αξιολογούµε τον αλγόριθµό µας χρησιµοποιώντας ακόµα µια φορά το 
σώµα κειµένων το οποίο έχει κατασκευάσει ο Choi. Για την εκτίµηση των παραµέτρων µ, σ, r και γ (ο 
ρόλος των οποίων περιγράφεται στην παράγραφο 5.2.2), χρησιµοποιήσαµε 10-πλή επαλήθευση («10-
fold cross validation», [Gloss(00001)]). Κάθε ένα από τα τέσσερα σύνολα δεδοµένων εξετάστηκε 
ξεχωριστά κατά τον ακόλουθο τρόπο: Το εκάστοτε σύνολο δεδοµένων χωρίστηκε σε δέκα υποσύνολα 
και η ακόλουθη διαδικασία  πραγµατοποιήθηκε δέκα φορές: 
1. Τυχαία επιλογή εννέα υποσυνόλων για εκπαίδευση. Χρησιµοποίηση του εναποµείναντος προς 
εξέταση. 
2. Καθορισµός των βέλτιστων τιµών των µ και σ κάνοντας χρήση των δεδοµένων προς εκπαίδευση 
και  των στάνταρ στατιστικών εκτιµητών. 
3. Καθορισµός κατάλληλων τιµών για τις παραµέτρους γ και r έπειτα από εκτέλεση του αλγορίθµου 
τµηµατοποίησης πάνω σε όλα τα δεδοµένα προς εκπαίδευση για όλους τους 80 δυνατούς 
συνδυασµούς της παραµέτρου γ η οποία λάµβανε τιµές από το σύνολο {0.00, 0.01, 0.02, …, 0.09, 
0.1,0.2, 0.3, …, 1.0} και της παραµέτρου r η οποία λάµβανε τιµές από το σύνολο {0.33, 0.5, 0.66, 
                                                 
* Μπορούµε να λάβουµε µια πιθανοτική κατανοµή του µήκους τµήµατος από την (5.14) µετά από κατάλληλες 
αθροίσεις πάνω στις τµηµατοποιήσεις. Η προκύπτουσα πιθανότητα θα πρέπει να είναι υπό συνθήκη πάνω στο 
συνολικό µήκος τµήµατος το οποίο θα πρέπει να ίσο µε τον (δοθέντα αριθµό) Τ. Αν κάποιος πραγµατοποιήσει 
αυτόν τον υπολογισµό θα καταλήξει στο συµπέρασµα ότι το µήκος τµήµατος δεν ακολουθεί την κανονική 
κατανοµή.  
 160
Κεφάλαιο 5 Αλγόριθµος Τµηµατοποίησης µε ∆υναµικό Προγραµµατισµό 
1}. Ο βέλτιστος συνδυασµός τιµών των παραµέτρων γ και r είναι εκείνος ο οποίος επιτυγχάνει την 
µικρότερη* τιµή του . kP
4. Εκτέλεση του αλγορίθµου τµηµατοποίησης πάνω στα δεδοµένα προς εξέταση χρησιµοποιώντας 
τις προαναφερθείσες βέλτιστες τιµές.  
Ο Πίνακας 5.7 περιέχει τα αποτελέσµατα του πειράµατος (δηλαδή τις τιµές των κριτηρίων 
Precision, Recall και ) για κάθε ένα από τα τέσσερα σύνολα δεδοµένων, καθώς επίσης και 
συγκεντρωτικά αποτελέσµατα τα οποία παραθέτονται στο [Kehagias et al., 2004]. 
kP
Στον Πίνακα 5.8 αλλά και στο σχήµα 5.14 συγκρίνουµε τα αποτελέσµατα του αλγορίθµου µας ως 
προς το κριτήριο  µε τα αντίστοιχα που προέκυψαν από άλλους αλγορίθµους τµηµατοποίησης που 
εφαρµόστηκαν στη συλλογή κειµένων του Choi. Τα αποτελέσµατα του αλγορίθµου τµηµατοποίησης 
µε δυναµικό προγραµµατισµό παρέχονται στις τελευταίες τρεις γραµµές. 
kP
 
 
Group Precision Recall kP  
Set0 (3-11) 81.21% 83.68% 7.13% 
Set1 (3-5) 88.00% 87.90% 5.21% 
Set2 (6-8) 89.42% 89.34% 3.04% 
Set3 (9-11) 91.76% 91.45% 1.47% 
All Sets 84.86% 86.20% 5.46% 
Πίνακας 5.7 Οι τιµές των Precision, Recall και  για τα datasets Set0, Set1, Set2, Set3 και για την συνολική 
συλλογή (συλλογή κειµένων του Choi) που προκύπτουν από τις καλύτερες τιµές των παραµέτρων γ και r του 
τρίτου γκρουπ πειραµάτων (µετά την εφαρµογή της τεχνικής της επαλήθευσης).  
kP
 
 
                                                 
* Σε αυτό το βήµα και το επόµενο παραλείπουµε τον υπολογισµό του όρου G2(s,t) από τον αλγόριθµό µας. Αυτό 
απλοποιεί τον αλγόριθµο και όπως φαίνεται δεν µειώνει την απόδοση.  
 161
Κεφάλαιο 5 Αλγόριθµος Τµηµατοποίησης µε ∆υναµικό Προγραµµατισµό 
 
 
 Επιστηµονική ∆ηµοσίευση Set0 Set1 Set2 Set3 All Sets 
CWM1 [Choi et al., 2001] 9% 10% 7% 5% 8% 
U00b [Utiyama&Isahara, 2001] 10% 9% 7% 5% 9% 
CWM3 [Choi et al., 2001] 12% 10% 9% 8% 11% 
C99b [Choi, 2000] 12% 11% 10% 9% 11% 
U00 [Utiyama&Isahara, 2001] 11% 13% 6% 6% 10%  
C99 [Choi, 2000] 13% 18% 10% 10% 13%   
CWM2 [Choi et al., 2001] 14% 10% 11% 12% 13% 
C99b,-r [Choi, 2000] 23% 19% 21% 20% 22% 
1ο γκρουπ(2η σειρά)  [Kehagias et al.,2003 (a)] 7.16% 7.54% 5.51% 3.08% 6.40% 
2ο γκρουπ(2η σειρά) [Kehagias et al.,2003(a)] 7.00% 5.45% 3% 1.33% 5.39% 
Μοντέλα ∆ιαχωρισ. 
Γινοµένου 
[Kehagias et al., 2004] 7.13% 5.21% 3.04% 1.47% 5.46% 
Πίνακας 5.8 Σύγκριση των λαµβανόµενων τιµών του κριτηρίου  από διάφορους αλγορίθµους που 
εµφανίζονται στην βιβλιογραφία µε τα αντίστοιχα του τρίτου γκρουπ πειραµάτων του δικού µας αλγορίθµου, και 
της δεύτερης σειράς των πρώτων δυο γκρουπ εφαρµοζόµενοι πάνω σε καθένα από τα σύνολα Set0, Set1, Set2 
και Set3 των δεδοµένων του Choi.  
kP
 
 
 
 
 162
Κεφάλαιο 5 Αλγόριθµος Τµηµατοποίησης µε ∆υναµικό Προγραµµατισµό 
i
0
5
10
15
20
25
Set0 (3-11), Set1 (3-5), Set2 (6-8), Set3 (9-11), All Sets
CWM1 (Choi) U00b(Utiyama & Isahara)
CWM3 (Choi) C99b (Choi)
U00 (Utiyama & Isahara) C99 (Choi)
CWM2 (Choi) C99b,-r (Choi)
1ο γκρουπ 2ο γκρουπ
Σχήµα 5.14 Σύγκριση των λαµβανόµενων τιµών του κριτηρίου  από διάφορους αλγορίθµους που 
εµφανίζονται στην βιβλιογραφία µε τα αντίστοιχα του τρίτου γκρουπ πειραµάτων του δικού µας 
αλγορίθµου, και της δεύτερης σειράς των πρώτων δυο γκρουπ εφαρµοζόµενοι πάνω σε καθένα από τα 
σύνολα Set0, Set1, Set2 και Set3 των δεδοµένων του Choi.  
kP
 
5.3.3 ∆εύτερη Οµάδα Πειραµάτων 
 
Στη δεύτερη οµάδα πειραµάτων χρησιµοποιούµε τη συλλογή κειµένων “Press: Reporting” των 
κειµένων του Brown Corpus [Francis & Kucera, 1982]. Η συλλογή µας αποτελείται από δέκα σύνολα 
κειµένων: Set0, Set1, …, Set9. Κάθε σύνολο κειµένων περιέχει δέκα κείµενα και κάθε κείµενο 
παράγεται σύµφωνα µε την ακόλουθη διαδικασία (η οποία εγγυάται ότι κάθε κείµενο θα περιέχει δέκα 
τµήµατα): 
1. Επιλέγονται ακέραιες τιµές για τα minL  και . Όπως θα φανεί στο επόµενο βήµα το maxL minL  
είναι ο ελάχιστος και το  είναι ο µέγιστος αριθµός γραµµών µέσα σε ένα τµήµα. maxL
2. Για i=1,2,…, 10, παράγεται τυχαία ένας ακέραιος αριθµός  ο οποίος ακολουθεί την 
οµοιόµορφη κατανοµή στο {
iL
minL , 1min +L , …, }. maxL
 163
Κεφάλαιο 5 Αλγόριθµος Τµηµατοποίησης µε ∆υναµικό Προγραµµατισµό 
3. To i -στό τµήµα δηµιουργείται αποσπώντας  συνεχόµενες γραµµές από ένα τυχαία επιλεγόµενο 
κείµενο του Brown Corpus (ξεκινώντας από την πρώτη γραµµή του κειµένου). 
iL
4. Τα δέκα παραγόµενα τµήµατα ενώνονται το ένα µετά το άλλο για την δηµιουργία ενός κειµένου. 
Κάθε ένα από τα δέκα σύνολα κειµένων χρησιµοποιεί διαφορετικές τιµές των minL  και . 
Τα δέκα διαφορετικά ζεύγη των 
maxL
minL  και  παραθέτονται στον πίνακα 5.9.  maxL
Εντελώς όµοια µε την πρώτη οµάδα πειραµάτων, πραγµατοποιούµε δυο ακολουθίες 
πειραµάτων. Στην πρώτη ακολουθία πειραµάτων στόχος µας είναι η εύρεση της καλύτερης δυνατής 
απόδοσης τµηµατοποίησης (δηλ. χρησιµοποιώντας τις βέλτιστες τιµές των γ και r).  Η διαδικασία η 
οποία ακολουθείται είναι εντελώς αντίστοιχη µε αυτή που ακολουθείται στην πρώτη σειρά 
πειραµάτων της πρώτης οµάδας πειραµάτων. Ο Πίνακας 5.10  περιέχει τη βέλτιστη τιµή του  (και 
η αντίστοιχη τιµή των κριτηρίων Precision και Recall) η οποία επιτεύχθηκε σε κάθε µια από αυτές τις 
περιπτώσεις. 
kP
 
 Set0 Set1 Set2 Set3 Set4 Set5 Set6 Set7 Set8 Set9 
Lmin 15 15 15 15 20 20 20 25 25 30 
Lmax 20 25 30 35 25 30 35 30 35 35 
Πίνακας 5.9: Τιµές των minL και (ελάχιστος και µέγιστος αριθµός γραµµών) για τα σύνολα κειµένων 
Set0, Set1, …, Set9 (της δικής µας συλλογής κειµένων).  
maxL
  
 
 
 
 
 
 
 164
Κεφάλαιο 5 Αλγόριθµος Τµηµατοποίησης µε ∆υναµικό Προγραµµατισµό 
 
Group Precision Recall kP  
Set0 (15-20) 76.44% 77.78% 5.16% 
Set1 (15-25) 75.33% 80% 6.32% 
Set2 (15-30) 73.30%  80% 6.07% 
Set3 (15-35) 71.33% 73.33% 6.68% 
Set4 (20-25) 79.5455% 83.33% 3.22% 
Set5 (20-30) 68.19% 70% 4.92% 
Set6 (20-35) 79.64% 76.67% 5.87% 
Set7 (25-30) 72.25% 72.22% 4.93% 
Set8 (25-35) 77.84% 74.44% 4.26% 
Set9 (30-35) 77.81% 81.11% 3.37% 
All Sets 75.17% 76.89% 5.08% 
Πίνακας 5.10: Τιµές των Precision, Recall και  για τα σύνολα κειµένων Set0, Set1, …, Set9 (της δικής µας 
συλλογής κειµένων) οι οποίες προκύπτουν από τις καλύτερες τιµές των παραµέτρων γ και r της πρώτης σειράς 
της  δεύτερης οµάδας πειραµάτων (δίχως την εφαρµογή της τεχνικής της επαλήθευσης). 
kP
Στα σχήµατα 5.15-5.18 παρίσταται η τιµή του  σαν συνάρτηση των γ και r (για τα σύνολα 
κειµένων Set4, Set7, Set8 και Set9).  
kP
 
 
 165
Κεφάλαιο 5 Αλγόριθµος Τµηµατοποίησης µε ∆υναµικό Προγραµµατισµό 
0
0,05
0,1
0,15
0,2
0,25
0,3
0,35
0,4
0,45
0 0,1 0,2 0,3 0,4 0,5 0,6 0,7 0,8 0,9 1
γ
Pk
r = 1
r =2/3
r =1/2
r =1/3
γ
 
Σχήµα 5.15: Γραφική παράσταση του  σαν συνάρτηση των γ και r για τα κείµενα του Set4 (δική 
µας συλλογή κειµένων).  
kP
 
0
0,05
0,1
0,15
0,2
0,25
0,3
0,35
0,4
0,45
0 0,1 0,2 0,3 0,4 0,5 0,6 0,7 0,8 0,9 1
γ
Pk
r = 1
r =2/3
r =1/2
r =1/3
γ
 
Σχήµα 5.16: Γραφική παράσταση του  σαν συνάρτηση των γ και r για τα κείµενα του Set7 (δική 
µας συλλογή κειµένων).  
kP
 
 166
Κεφάλαιο 5 Αλγόριθµος Τµηµατοποίησης µε ∆υναµικό Προγραµµατισµό 
0
0,05
0,1
0,15
0,2
0,25
0,3
0,35
0,4
0,45
0 0,1 0,2 0,3 0,4 0,5 0,6 0,7 0,8 0,9 1
γ
Pk
r = 1
r =2/3
r =1/2
r =1/3
 
Σχήµα 5.17: Γραφική παράσταση του  σαν συνάρτηση των γ και r για τα κείµενα του Set8 (δική 
µας συλλογή κειµένων).  
kP
 
0,00
0,05
0,10
0,15
0,20
0,25
0,30
0,35
0,40
0,45
0,50
0,00 0,10 0,20 0,30 0,40 0,50 0,60 0,70 0,80 0,90 1,00
γ
Pk
r = 1
r =2/3
r =1/2
r =1/3
 
Σχήµα 5.18: Γραφική παράσταση του  σαν συνάρτηση των γ και r για τα κείµενα του Set9 (δική 
µας συλλογή κειµένων).  
kP
  
 
 167
Κεφάλαιο 5 Αλγόριθµος Τµηµατοποίησης µε ∆υναµικό Προγραµµατισµό 
Στην δεύτερη σειρά πειραµάτων υπολογίζουµε τις τιµές των µ, σ και τις βέλτιστες τιµές των γ 
και r (ο ρόλος των οποίων περιγράφεται στην παράγραφο 5.2.2) µε την µέθοδο επαλήθευσης που 
περιγράφτηκε στη δεύτερη σειρά των δυο πρώτων γκρουπ της πρώτης οµάδας πειραµάτων. Στον 
πίνακα 5.11 παραθέτουµε τις τιµές της απόδοσης τµηµατοποίησης για κάθε ένα από τα δέκα σύνολα 
κειµένων (όπως λαµβάνονται από τις επαληθευµένες τιµές των παραµέτρων). Οι ίδια πληροφορία 
παρατίθεται µε γραφική µορφή στα Σχήµατα 5.19 και 5.20. 
∆εδοµένου ότι η παραπάνω συλλογή κειµένων δεν έχει χρησιµοποιηθεί πρωτύτερα στην 
βιβλιογραφία δεν είναι δυνατή ουδεµία σύγκριση. Παρόλα αυτά, αξίζει να σηµειωθεί ότι η συλλογή 
αυτή αποτελεί δυσκολότερο πρόβληµα τµηµατοποίησης από ότι αυτό της πρώτης οµάδας πειραµάτων 
δεδοµένου ότι τόσο το λεξιλόγιο όσο και οι αριθµός των προτάσεων  που περιέχονται σε ένα τµήµα 
είναι µεγαλύτερα.  
 
Group Precision Recall kP  
Set0 (15-20) 67.22% 68.89% 9.92% 
Set1 (15-25) 59.47% 60.00% 8.93% 
Set2 (15-30) 66.64% 67.78% 13.59% 
Set3 (15-35) 63.63% 68.89% 11.50% 
Set4 (20-25) 79.72% 78.89% 5.13% 
Set5 (20-30) 71.84% 75.56% 7.56% 
Set6 (20-35) 68.06% 70.00% 8.63% 
Set7 (25-30) 72.33% 75.56% 6.84% 
Set8 (25-35) 63.13% 70.00% 8.62% 
Set9 (30-35) 70.89% 74.45% 5.87% 
All Sets 68.30% 71.00% 8.60% 
Πίνακας 5.11: Τιµές των Precision, Recall και  για τα σύνολα κειµένων Set0, Set1, …, Set9 (της δικής µας 
συλλογής κειµένων) χρησιµοποιώντας τις επαληθευµένες τιµές των παραµέτρων.  
kP
 168
Κεφάλαιο 5 Αλγόριθµος Τµηµατοποίησης µε ∆υναµικό Προγραµµατισµό 
0
2
4
6
8
10
12
14
Set0 (15-20), Set1 (15-25), Set2 (15-30), Set3 (15-35), Set4 (20-25), 
Set5 (20-30), Set6 (20-35), Set7 (25-30), Set08(25-35), Set9 (30-35), 
AllSets
 
Σχήµα 5.19: Γραφική παράσταση του  για τα σύνολα κειµένων Set0, Set1, …, Set9 (της δικής µας 
συλλογής κειµένων) χρησιµοποιώντας τις επαληθευµένες τιµές των παραµέτρων.  
kP
0
10
20
30
40
50
60
70
80
Set0 (15-20), Set1 (15-25), Set2 (15-30), Set3 (15-35), Set4 (20-25), Set5 (20-
30), Set6 (20-35), Set7 (25-30), Set08(25-35), Set9 (30-35), AllSets
Precision Recall Beeferman Pk
 
Σχήµα 5.20: Γραφική παράσταση Τιµές των Precision, Recall και  για τα σύνολα κειµένων Set0, 
Set1, …, Set9 (της δικής µας συλλογής κειµένων) χρησιµοποιώντας τις επαληθευµένες τιµές των 
παραµέτρων.  
kP
 169
Κεφάλαιο 5 Αλγόριθµος Τµηµατοποίησης µε ∆υναµικό Προγραµµατισµό 
5.3.4 Παρατηρήσεις 
 
H εφαρµογή του αλγορίθµου τµηµατοποίησης που αναπτύξαµε, πάνω στην συλλογή κειµένων 
του Choi οδηγεί σε σηµαντικά καλύτερα αποτελέσµατα από όλα τα προαναφερθέντα στην 
βιβλιογραφία ([Choi,2000], [Choi et al., 2001], [Utiyama & Isahara, 2001]). Ο αλγόριθµός µας επίσης 
επιτυγχάνει καλή απόδοση και στην πρωτοεµφανιζόµενη και πρωτο-εξεταζόµενη συλλογή κειµένων 
της δεύτερης οµάδας πειραµάτων. Η υπολογιστική πολυπλοκότητα του αλγορίθµου µας είναι 
)( 2TΟ *, όπου Τ είναι το πλήθος των προτάσεων. Στον Πίνακα 5.12 παρέχονται οι χρόνοι εκτέλεσης 
(που αντιστοιχούν στην τµηµατοποίηση ενός κειµένου) του αλγορίθµου µας και ορισµένων άλλων 
αλγορίθµων (([Choi,2000], [Choi et al., 2001], [Utiyama & Isahara, 2001])).  Ο χρόνος εκπάιδευσης 
καθενός καθενός υποσυνόλου δεν ξεπερνά τα ένα µε δυο λεπτά της ώρας. Αξίζει να σηµειωθεί ότι ο 
αλγόριθµός µας εκτελέστηκε σε έναν υπολογιστή Pentium III 600 MHz µε 256 Mbyte RAM. Είναι 
πιθανό οι υπόλοιποι αλγόριθµοι να εκτελέστηκαν σε πιο αργούς υπολογιστές.  
 
Αλγόριθµος U00b U00 C99b C99 Αλγόριθµος Τµηµατοποίησης 
µε ∆υναµικό Προγραµµατισµό 
Μέσος Χρόνος 
εκτέλεσης σε sec 
1.37 1.36 1.45 1.49 0.91 
Πίνακας 5.12: Σύγκριση του αλγορίθµου µας µε τους αλγορίθµους των ([Choi,2000], [Choi et al., 2001], 
[Utiyama & Isahara, 2001]) σε ότι αφορά τον µέσο χρόνο εκτέλεσης της τµηµατοποίησης ενός κειµένου.  
Επιπρόσθετα, ένα ελκυστικό χαρακτηριστικό του αλγορίθµου µας είναι ο αυτόµατος 
υπολογισµός του βέλτιστου αριθµού των τµηµάτων.  
Αξίζει να αναφέρουµε τους λόγους στους οποίους πιστεύουµε πως οφείλεται η καλή απόδοση 
του αλγορίθµου µας: 
                                                 
* * Μια µορφή pruning είναι δυνατό να εφαρµοστεί επιλέγοντας έναν ακέραιο και περιορίζοντας τον 
εσωτερικό βρόχο αν τρέχει για s=T +1, T +2, …, T. Σε αυτή την περίπτωση χρόνος εκτέλεσης είναι 
Ο(T).  
 
lowS
 ο - lowS - lowS
 170
Κεφάλαιο 5 Αλγόριθµος Τµηµατοποίησης µε ∆υναµικό Προγραµµατισµό 
1. Η χρήση του παράγοντα µήκους τµήµατος στην συνάρτηση κόστους δείχνει να βελτιώνει 
σηµαντικά την απόδοση της τµηµατοποίησης όπως µπορεί να δει κανείς και από τις γραφικές 
παραστάσεις 5.2-5.9.  Σε αυτές τις γραφικές παραστάσεις η τιµή του  η οποία αντιστοιχεί στην 
τιµή γ = 0 αντιστοιχεί στην τµηµατοποίηση η οποία πραγµατοποιείται δίχως την χρήση της 
πληροφορίας του µήκους τµήµατος. Είναι φανερό ότι σε αυτή την περίπτωση οι τιµές του  
είναι σηµαντικά χειρότερες από τις αντίστοιχες βέλτιστες.  
kP
kP
2. Επιπρόσθετα, η χρήση του dotplot και της «γενικευµένης πυκνότητας» (r≠2) δείχνει να βελτιώνει 
την απόδοση. Παρά το γεγονός ότι η χρήση της «πραγµατικής πυκνότητας» φαίνεται πιο φυσική, 
από τις γραφικές παραστάσεις 5.2-5.9 αποδεικνύεται ότι η βέλτιστη απόδοση τµηµατοποίησης 
(ελάχιστη τιµή της τιµής ) επιτυγχάνεται για µικρότερες τιµές του r. Η χρήση της 
«γενικευµένης πυκνότητας» (r≠2) µας επιτρέπει να ελέγχουµε καλύτερα τον βαθµό επηρεασµού 
της επιφάνειας ενός τµήµατος σε αναλογία µε την «πληροφορία που περιέχεται µέσα σε αυτό».  
Επιπρόσθετα, όπως είναι κατανοητό, σε ένα τµήµα κειµένου, οι γειτονικές προτάσεις είναι δυνατό 
να µην είναι ιδιαίτερα όµοιες µεταξύ τους ενώ µεταξύ όµοιων προτάσεων είναι δυνατό να 
παρεµβάλλονται άλλες ανόµοιες. Οι οµοιότητες όµως αυτές αποκαλύπτονται και αξιοποιούνται µε 
τη βοήθεια του dotplot. 
kP
3. Παρόλα αυτά, η πληροφορία του µήκους τµήµατος και η «γενικευµένη πυκνότητα» µπορούν να 
οδηγήσουν σε βελτίωση της απόδοσης µόνο όταν χρησιµοποιούνται µε κατάλληλες τιµές των γ, r, 
µ και σ οι οποίες προκύπτουν από τα δεδοµένα προς εκπαίδευση και την επαλήθευση των 
παραµέτρων. 
4. Επιπρόσθετα, η χρήση του αλγορίθµου δυναµικού προγραµµατισµού αποκαλύπτεται καθοριστική 
για την υψηλή απόδοση τµηµατοποίησης. Αξίζει να σηµειωθεί ότι, ο αλγόριθµός είναι «ολικός» 
από δυο απόψεις. Πρώτα από όλα, η οµοιότητα µεταξύ των προτάσεων υπολογίζεται ολικά µε την 
χρήση του πίνακα D και του dotplot. ∆εύτερον, αυτή η ολική πληροφορία της οµοιότητας 
βελτιστοποιείται επιπρόσθετα ολικά µε την χρήση του αλγορίθµου του δυναµικού 
προγραµµατισµού. Το γεγονός αυτό έρχεται σε αντίθεση µε την τοπική βελτιστοποίηση της ολικής 
πληροφορίας (η οποία πραγµατοποιείται από τον Choi µε την χρήση της διαιρετικής οµαδοποίησης 
(“divisive clustering”, [Gloss(00026)]) την οποία πραγµατοποιεί τοπική βελτιστοποίηση για την 
τµηµατοποίηση ενός ολικού πίνακα οµοιότητας) και τη ολική βελτιστοποίηση της τοπικής 
πληροφορίας (η οποία πραγµατοποιείται από τον Heinonen [Heinonen, 1998] ο οποίος 
χρησιµοποιεί δυναµικό προγραµµατισµό για να βελτιστοποιήσει ολικά την οµοιότητα µεταξύ 
γειτονικών προτάσεων).  
 171
Κεφάλαιο 5 Αλγόριθµος Τµηµατοποίησης µε ∆υναµικό Προγραµµατισµό 
5. Τέλος, η µέτρηση του µήκους τµήµατος µε βάση τις προτάσεις αντί µε βάση τις λέξεις από τις 
οποίες αυτό αποτελείται οδηγεί σε βελτίωση της απόδοσης τµηµατοποίησης όπως αυτό 
διαφαίνεται από την σύγκριση των αποτελεσµάτων µεταξύ του πρώτου και του δεύτερου γκρουπ 
πειραµάτων. Αυτό οφείλεται στο γεγονός ότι ο βαθµός µεταβλητότητας των προτάσεων που 
εµφανίζονται σε ένα τµήµα – άρα και η αντίστοιχη τυπική απόκλιση – είναι µικρότερος από ότι ο 
αντίστοιχος του πλήθους των λέξεων που εµφανίζονται µέσα σε αυτό, γεγονός που επηρεάζει 
σηµαντικά τον αντίστοιχο παράγοντα στη συνάρτηση κόστους τµηµατοποίησης.  
Οι τέσσερις πρώτοι παράγοντες χρησιµοποιήθηκαν στο παρελθόν από άλλους ερευνητές για 
την τµηµατοποίηση κειµένων, αλλά, από όσο είµαστε σε θέση να γνωρίζουµε, ο συνδυασµός και των 
τεσσάρων αυτών µε την προσθήκη της επιλογής µέτρησης του µήκους τµήµατος τόσο, µε βάση τις 
προτάσεις όσο και µε βάση τις λέξεις που εµφανίζονται σε ένα τµήµα, είναι καινούργιος και δεν έχει 
χρησιµοποιηθεί στο παρελθόν. Πιστεύουµε ότι είναι αυτός ο συνδυασµός καθώς και η εξέταση των 
τεσσάρων διαφορετικών συναρτήσεων µήκους τµήµατος  (“U-shape, κορεσµένη U-shape, V-shape 
και κορεσµένη V-shape”) στον οποίο οφείλεται η βελτιωµένη απόδοση η οποία επιτεύχθηκε.   
5.4 Συµπεράσµατα 
 
Σε αυτό το κεφάλαιο παρουσιάσαµε έναν αλγόριθµο αλγόριθµο τµηµατοποίησης µε δυναµικό 
προγραµµατισµό ο οποίος πραγµατοποιεί τµηµατοποίηση κειµένου µε ολική ελαχιστοποίηση µιας 
συνάρτησης κόστους τµηµατοποίησης η οποία αποτελείται από δυο µέρη: (α) εντός – τµήµατος 
οµοιότητα βασιζόµενη στις λέξεις του κειµένου και (β) προηγούµενη πληροφορία σχετικά µε το µήκος 
του τµήµατος. Η απόδοση του αλγορίθµου µας είναι ιδιαίτερα ικανοποιητική λαµβανοµένου υπόψη 
ότι πέτυχε τα καλύτερα αποτελέσµατα που αναφέρθηκαν µέχρι τώρα στην βιβλιογραφία στην συλλογή 
των κειµένων του Choi, αλλά και τις πολύ καλές επιδόσεις στο σώµα κειµένων το οποίο εµείς 
κατασκευάσαµε. Αξίζει επίσης να σηµειωθεί ότι, στο βαθµό που είµαστε σε θέση να γνωρίζουµε, τα 
«Μοντέλα ∆ιαχωρισµού Γινοµένου» δεν έχουν προηγούµενα εφαρµοστεί σε διδιάστατες δοµές (όπως 
είναι ο πίνακας οµοιότητας µεταξύ των προτάσεων ενός κειµένου). 
 Σε ότι αφορά το σύνολο των πειραµάτων στα οποία έγινε χρήση των «Μοντέλων ∆ιαχωρισµού 
Γινοµένου» µπορεί να παρατηρήσει κανείς ότι είναι ενδιαφέρουσα η σύγκριση αυτών µε τα Hidden 
Markov Models (HMM’s) τα οποία έχουν ευρέως εφαρµοστεί σε προβλήµατα τµηµατοποίησης 
χρονοσειρών ([Rabiner, 1989], [Bengio, 1998]). Παρά το γεγονός ότι ο αλγόριθµός µας σχετίζεται µε 
τον αλγόριθµο Viterbi (ο οποίος χρησιµοποιείται στην τµηµατοποίηση µε την βοήθεια των HMM’s), 
υπάρχει µια σηµαντική διαφορά ανάµεσα στα HMM’s και τα «Μοντέλα ∆ιαχωρισµού Γινοµένου». 
 172
Κεφάλαιο 5 Αλγόριθµος Τµηµατοποίησης µε ∆υναµικό Προγραµµατισµό 
Πιο συγκεκριµένα, στα HMM’s κάθε παρατήρηση µέσα σε ένα τµήµα εξαρτάται αποκλειστικά και 
µόνο από την τρέχουσα κατάσταση. Αντίθετα στα «Μοντέλα ∆ιαχωρισµού Γινοµένου» υπολογίζεται η 
πιθανοτική κατανοµή για το συνολικό πλήθος των παρατηρήσεων µέσα σε ένα τµήµα. Κατά αυτήν 
την έννοια, τα «Μοντέλα ∆ιαχωρισµού Γινοµένου» δείχνουν να είναι ένα πιο φυσικό µοντέλο 
δηµιουργίας ενός κειµένου.  
Σε αυτό το  κεφάλαιο εξετάσαµε ένα offline πρόβληµα τµηµατοποίησης κειµένου. Με άλλα 
λόγια, υποθέτουµε ότι το συνολικό κείµενο είναι διαθέσιµο προς επεξεργασία από έναν αλγόριθµο 
τµηµατοποίησης. Ένα online πρόβληµα θα απαιτούσε την τµηµατοποίηση µιας συνεχόµενα 
εισερχόµενης ακολουθίας κειµένων. Ο αλγόριθµος ο οποίος παρουσιάστηκε εδώ δεν είναι σε θέση να 
επιλύσει τέτοιου είδους προβλήµατα. Παρόλα αυτά, δεν είναι τόσο δύσκολο να προσαρµόσουµε έναν 
δυναµικό αλγόριθµο προγραµµατισµού σε µια online λειτουργία. Στη συνέχεια, στοχεύουµε να 
επικεντρώσουµε την προσοχή µας στον υπολογισµό του µοντέλου µήκους βασιζόµενο στο µέσο 
αριθµό προτάσεων που εµφανίζονται στα τµήµατα ενός κειµένου καθώς επίσης και σε ένα ευρύ φάσµα 
προβληµάτων τµηµατοποίησης.  
∆οθείσης της υψηλής απόδοσης του αλγορίθµου µας, θεωρούµε ως ενδιαφέροντα προβλήµατα 
την τµηµατοποίηση µεγάλων σε µήκος κειµένων, την αναγνώριση των σηµείων εκείνων όπου 
παρουσιάζεται αλλαγή θέµατος σε ακολουθίες ειδήσεων καθώς και την τµηµατοποίηση ελληνικών 
κειµένων (το οποίο αποτελεί και το αντικείµενο του επόµενου κεφαλαίου το οποίο από όσο είµαστε σε 
θέση να γνωρίζουµε δεν έχει πρωτύτερα εξεταστεί από τους ερευνητές). Και αυτό γιατί, όπως 
παρατήρησαν οι Utiyama και Isahara ([Utiyama & Isahara, 2001]): «Είναι σηµαντικό να 
αποτιµήσουµε την απόδοση των συστηµάτων χρησιµοποιώντας πραγµατικά κείµενα. Αυτά τα κείµενα 
θα πρέπει να είναι επίσης ανεξάρτητα θεµατικής κατηγορίας. Θα πρέπει επίσης να ανήκουν σε 
περισσότερες από µια γλώσσες αν θέλουµε να εξετάσουµε την πολυγλωσσικότητα των συστηµάτων».  
 173
 
 
174 
Κεφάλαιο 6  Τµηµατοποίηση Ελληνικών  Κειµένων  
ΚΕΦΑΛΑΙΟ 6  TMHMAΤΟΠΟΙΗΣΗ ΕΛΛΗΝΙΚΩΝ 
ΚΕΙΜΕΝΩΝ 
6.1 Εισαγωγή 
 
Το παρόν κεφάλαιο πραγµατεύεται την εφαρµογή του αλγορίθµου τµηµατοποίησης σε ένα 
σώµα το οποίο αποτελείται από ελληνικά κείµενα. Στόχος της εν λόγω εφαρµογής είναι τόσο η 
εξέταση της πολυγλωσσικότητας του αλγορίθµου µας όσο και η απόδοση αυτού κατά την 
τµηµατοποίηση µεγάλων σε µήκος κειµένων, µεγαλύτερων από αυτά τα οποία κλήθηκε να 
τµηµατοποιήσει στο προηγούµενο κεφάλαιο. 
Πιο συγκεκριµένα, τα κείµενα τα οποία χρησιµοποιούνται προς τµηµατοποίηση προέρχονται 
από άρθρα της εφηµερίδας «Το Βήµα». Τα εν λόγω κείµενα επεξεργάστηκαν µε τη βοήθεια του 
Μορφοσυντακτικού Αναλυτή ο οποίος αναπτύχθηκε από τον Γεώργιο Ορφανό ([Gakis et al., 1999], 
[Noussia & Orphanos, 1999], [Orphanos & Christodoulakis, 1999], [Orphanos & Tsalidis, 1999 ], 
[Orphanos et  al., 1999 (α)],  [Orphanos et  al.,  1999 (β)], [Tsalidis & Orphanos, 1995], [Orphanos, 
2000]) κατά τη διάρκεια της διδακτορικής του διατριβής. Ο εν λόγω Μορφοσυντακτικός Αναλυτής 
έχει τη δυνατότητα να αναλύει οποιοδήποτε κείµενο της Νέας Ελληνικής ανεξαρτήτως περιεχοµένου. 
Η εν λόγω ανάλυση συνίσταται στην απόδοση µονοσήµαντης µορφοσυντακτικής πληροφορίας (δηλ. 
µορφολογικής προέλευσης και µορφοσυντακτικών ιδιοτήτων όπως π.χ. πτώση, αριθµός, γένος, µέρος 
του λόγου) και λήµµατος στις λέξεις του κειµένου.  
Το παρόν κεφάλαιο περιγράφει από δυο σειρές πειραµάτων. Και στις δυο σειρές, τα κείµενα 
προς τµηµατοποίηση κατασκευάστηκαν από τµήµατα των άρθρων της εφηµερίδας «Το Βήµα». Η 
πρώτη σειρά µελετά την τµηµατοποίηση κειµένων των οποίων το µήκος τµήµατος είναι το ίδιο µε 
αυτό της συλλογής κειµένων του Choi. Η δεύτερη σειρά πειραµάτων πραγµατεύεται την 
τµηµατοποίηση µεγαλύτερων σε έκταση κειµένων, όπου εδώ το εκάστοτε τµήµα αποτελείται από µία 
ή περισσότερες παραγράφους. 
Τα αποτελέσµατα τα οποία προέκυψαν από την εφαρµογή του αλγορίθµου µας στις δύο 
παραπάνω σειρές πειραµάτων ήταν ιδιαίτερα ικανοποιητικά ιδιαίτερα στη δεύτερη σειρά πειραµάτων 
όπου το πρόβληµα το οποίο κλήθηκε να αντιµετωπίσει ο αλγόριθµός µας ήταν δυσκολότερο λόγω της 
 175
Κεφάλαιο 6  Τµηµατοποίηση Ελληνικών  Κειµένων  
µεγάλης έκτασης των κειµένων σε ότι αφορά το µήκος των τµηµάτων από τα οποία αποτελούνται. Τα 
εν λόγω αποτελέσµατα ενισχύουν την πεποίθησή µας σε ότι αφορά  την ευρωστία του αλγορίθµου µας 
και περιγράφονται διεξοδικά στις ενότητες που ακολουθούν.  
6.2 Το Σώµα Ελληνικών Κειµένων της εφηµερίδας «Το Βήµα» 
 
Τα κείµενα τα οποία χρησιµοποιήσαµε για τα πειράµατά µας προέρχονται από το σώµα 
κειµένων που κατασκεύασαν οι Σταµατάτος, Φακοτάκης και Κοκκινάκης ([Stamatatos et  al.,  1999]) 
και τα οποία ελήφθησαν από την ιστοσελίδα της εφηµερίδας «Το Βήµα» (http://tovima.dolnet.gr). 
Τα κείµενα της εν λόγω εφηµερίδας επιλέχθηκαν επειδή η ιστοσελίδα της περιέχει πλήθος άρθρων 
µεγάλης έκτασης τα οποία χωρίζονται σε εννέα διαφορετικές θεµατικές κατηγορίες, καθεµία από τις 
οποίες πραγµατεύεται ένα συγκεκριµένο θέµα. Οι εν λόγω θεµατικές κατηγορίες παραθέτονται στον 
Πίνακα 6.1. 
Οι Σταµατάτος, Φακοτάκης και Κοκκινάκης ([Stamatatos et  al., 1999]) επέλεξαν κείµενα τα 
οποία ανήκουν στο τµήµα Β το οποίο περιλαµβάνει δοκίµια που αναφέρονται στην επιστήµη, τον 
πολιτισµό, την ιστορία κλπ για τρεις λόγους: 
1. Σε τέτοιου είδους κείµενα, η ιδιοσυγκρασία στο στυλ γραφής του συγγραφέα δεν είναι πιθανό 
να επισκιαστεί από τα χαρακτηριστικά της θεµατικής κατηγορίας στην οποία καθένα από αυτά 
ανήκει. 
2. Γενικά, τα κείµενα του παραρτήµατος Β είναι γραµµένα από καθηγητές πανεπιστηµίων, 
λογοτέχνες κλπ αντί από δηµοσιογράφους. 
3. Τέλος, υπάρχει ένα κλειστό σύνολο από συγγραφείς οι οποίοι τακτικά συνεισφέρουν σε αυτό 
το παράρτηµα. Για αυτό τον λόγο, καθίσταται εύκολη η συλλογή ενός σηµαντικού πλήθους 
κειµένων από τον ίδιο  συγγραφέα. 
Οι Σταµατάτος, Φακοτάκης και Κοκκινάκης κατασκεύασαν το εν λόγω σώµα κειµένων 
επιλέγοντας αρχικά δέκα συγγραφείς από το τµήµα Β δίχως να λάβουν υπόψη τους ειδικά κριτήρια, 
και στη συνέχεια τριάντα κείµενα από τον κάθε ένα συγγραφέα. Αξίζει να σηµειωθεί ότι δεν 
πραγµατοποίησαν καµία χειρωνακτική επεξεργασία ή δειγµατοληψία στα εν λόγω κείµενα εκτός από 
την αφαίρεση των περιττών τίτλων που δεν σχετίζονταν µε το αυτούσιο κείµενο. Για την αποφυγή 
ενδεχόµενης αλλαγής στο προσωπικό στυλ του εκάστοτε συγγραφέα µε την πάροδο του χρόνου, όλα 
τα επιλεγόµενα (τριακόσια συνολικά) κείµενα ανήκουν σε φύλλα της χρονικής περιόδου 1997 ως 
 176
Κεφάλαιο 6  Τµηµατοποίηση Ελληνικών  Κειµένων  
1999. Ο Πίνακας 6.2 περιέχει τα ονόµατα των δέκα συγγραφέων που επιλέχθηκαν καθώς επίσης και 
πληροφορίες σχετικά µε το συνολικό πλήθος και το µέσο µήκος των κειµένων τους σε λέξεις. Στην 
τελευταία στήλη αναγράφεται η θεµατική κατηγορία στην οποία ανήκει η πλειοψηφία των κειµένων 
του εκάστοτε συγγραφέα. Αξίζει να σηµειωθεί ότι η εν λόγω πληροφορία δεν έχει ληφθεί υπόψη κατά 
τη διάρκεια της δηµιουργίας του εν λόγω  σώµατος κειµένων.  
 
Κωδικός 
Θεµατικής 
Κατηγορίας  
Τίτλος (µετάφραση) Περιγραφή 
Α ΤΟ ΒΗΜΑ Κύρια άρθρα, Ηµερολόγια, Ρεπορτάζ, 
Πολιτική, ∆ιεθνή συµβάντα, Αθλητική 
Ανασκόπηση  
Β ΝΕΕΣ ΕΠΟΧΕΣ Πολιτιστικά 
C ΤΟ ΑΛΛΟ ΒΗΜΑ Περιοδικό ανασκοπήσεων 
D ΑΝΑΠΤΥΞΗ Επιχειρήσεις, οικονοµία 
E Η ∆ΡΑΧΜΗ ΣΑΣ Οικονοµία επενδυτών 
I ΕΙ∆ΙΚΗ ΕΚ∆ΟΣΗ Θέµα της εβδοµάδας 
S ΒΙΒΛΙΑ Παράρτηµα κριτικής βιβλίων 
Z ΤΕΧΝΕΣ ΚΑΙ ΚΑΛΛΙΤΕΧΝΕΣ Παράρτηµα κριτικής τέχνης 
T ΤΑΞΙ∆ΙΑ Παράρτηµα – ταξιδιωτικός οδηγός. 
Πίνακας 6.1. Η δοµή της εφηµερίδας  «Το ΒΗΜΑ». 
 
Όπως φαίνεται και από τον Πίνακα 6.2, το µήκος των κειµένων ποικίλλει ανάλογα µε τον 
συγγραφέα. Υπάρχουν τρεις συγγραφείς των οποίων το µέσο µήκος κειµένων µετρούµενο σε λέξεις 
είναι µικρότερο των χιλίων λέξεων (οι συγγραφείς ∆ερτιλής, Μαρωνίτης και Βώκος). Το µεγαλύτερο 
µέσο µήκος κειµένων (της συγγραφέας Κιοσσέ) είναι τρεις φορές  µεγαλύτερο από αυτό µε το 
µικρότερο µέσο µήκος (του συγγραφέα Μαρωνίτη). Περίπου το 50% των κειµένων (δηλαδή τα 146 
από τα 300) έχουν µήκος µικρότερο των χιλίων λέξεων.  
 
 
 177
Κεφάλαιο 6  Τµηµατοποίηση Ελληνικών  Κειµένων  
 
ΟΝΟΜΑ 
ΣΥΓΓΡΑΦΕΑ 
ΠΛΗΘΟΣ 
ΚΕΙΜΕΝΩΝ 
ΣΥΝΟΛΙΚΕΣ 
ΛΕΞΕΙΣ 
ΜΕΣΟ ΜΗΚΟΣ 
ΚΕΙΜΕΝΟΥ 
ΣΕ ΛΕΞΕΙΣ 
ΘΕΜΑΤΙΚΗ 
ΠΕΡΙΟΧΗ 
Αλαχιώτης 30 30,137 30,137 Βιολογία 
Μπαµπινιώτης 30 34,747 1,158 Γλωσσολογία 
∆ερτιλής 30 26,823 894 Ιστορία, Κοινωνία 
Κιοσσέ 30 50,670 1,689 Αρχαιολογία 
Λιάκος 30 37,692 1,256 Ιστορία, Κοινωνία 
Μαρωνίτης 30 17,166 572 Πολιτισµός, Κοινωνία 
Πλωρίτης 30 34,980 1,166 Πολιτισµός, Κοινωνία 
Τάσσιος 30 30,587 1,020 Τεχνολογία, Κοινωνία 
Τσουκαλάς 30 41,389 1,380 Εσωτερικές υποθέσεις 
Βώκος 30 29,553 985 Φιλοσοφία 
ΣΥΝΟΛΟ 300 333,744 1,112  
Πίνακας 6.2. Περιγραφή των κειµένων που προέρχονται από την εφηµερίδα «ΤΟ ΒΗΜΑ» και τα οποία 
συνιστούν το σώµα κειµένων που κατασκευάστηκε από τους  Σταµατάτο, Φακοτάκη και Κοκκινάκη.  
6.3 Ο Μορφοσυντακτικός Αναλυτής της Νέας Ελληνικής  
 
  
Η προεπεξεργασία του παραπάνω σώµατος κειµένων πραγµατοποιήθηκε µε τη βοήθεια του 
λογισµικού που αναπτύχθηκε στα πλαίσια της διδακτορικής διατριβής του Γεώργιου Ορφανού ([Gakis 
et al., 1999], [Noussia & Orphanos, 1999], [Orphanos & Christodoulakis, 1999], [Orphanos & 
Tsalidis, 1999 ], [Orphanos et  al., 1999 (α)],  [Orphanos et  al.,  1999(β)], [Tsalidis & Orphanos, 
1995], [Orphanos, 2000]) µε θέµα «Υπολογιστική Μορφοσυντακτική Ανάλυση της Νέας Ελληνικής». 
Στην εν λόγω διατριβή αναπτύχθηκε ένας Μορφοσυντακτικός Αναλυτής βασική προδιαγραφή του 
οποίου ήταν η ευρωστία, η ικανότητά του δηλαδή να αναλύει οποιοδήποτε νεοελληνικό κείµενο 
ανεξαρτήτως περιεχοµένου. Η µορφοσυντακτική ανάλυση είναι µια γνωστική διεργασία που κινείται 
µεταξύ µορφολογίας και σύνταξης µε στόχο την απόδοση µονοσήµαντης «µορφοσυντακτικής 
πληροφορίας» στις λέξεις του κειµένου. Με τον όρο «µορφοσυντακτική πληροφορία» για µια λέξη 
 178
Κεφάλαιο 6  Τµηµατοποίηση Ελληνικών  Κειµένων  
εννοούµε τη µορφολογική της προέλευση και τις µορφοσυντακτικές της ιδιότητες π.χ. η λέξη 
ανθρώπου είναι γενική ενικού του αρσενικού ουσιαστικού [άνθρωπος]. Από τη µορφοσυντακτική 
πληροφορία προκύπτει το λήµµα της εκάστοτε λέξης.  
Ο εν λόγω Μορφοσυντακτικός Αναλυτής υλοποιήθηκε µε τη βοήθεια του Μορφολογικού 
Λεξικού που αναπτύχθηκε από τα µέλη της Ερευνητικής Μονάδας 2 του ΙΤΥ ([Τsalidis & Orphanos, 
1995], [Orphanos & Tsalidis, 1999]). Το εν λόγω λεξικό παρέχει τη δυνατότητα απόδοσης σε κάθε 
λέξη όλων των µοφοσυντακτικών χαρακτηριστικών αυτής δηλαδή του Μέρους του Λόγου (ΜτΛ), του 
Αριθµού, του Γένους, της Πτώσης, της Φωνής, της Έγκλισης και του Λήµµατος, και χρησιµοποιήθηκε 
για τη συγκρότηση και το µορφοσυντακτικό σχολιασµό, µε ηµιαυτόµατο τρόπο, ενός σώµατος 7.624 
προτάσεων. Οι εν λόγω προτάσεις (οι οποίες αποτελούνται από 137.764 λέξεις, σηµεία στίξης, 
αριθµούς κ.λ.π.) προέρχονται από αποµαγνητοφωνηµένο προφορικό λόγο, λογοτεχνικά, 
δηµοσιογραφικά και επιστηµονικά κείµενα ως χαρακτηριστικά δείγµατα του σύγχρονου νεοελληνικού 
λόγου. Χαρακτηριστικό του εν λόγω σώµατος προτάσεων αποτελεί η ικανότητα αναγνώρισης της 
συµπεριφοράς όλων των τύπων µορφοσυντακτικής ασάφειας οι οποίοι εµφανίζονται στην Νέα 
Ελληνική (δηλ. ασάφεια του τύπου Αντωνυµία/Κλιτικού/Άρθρου, Αντωνυµία/Κλιτικού, 
Επιθέτου/Επιρρήµατος, κλπ.). 
Το εν λόγω σώµα προτάσεων χρησιµοποιήθηκε για την κατασκευή ενός µοντέλου σκοπός του 
οποίου ήταν η συνολική επίλυση της µορφοσυντακτικής ασάφειας που εισάγεται από το 
Μορφοσυντακτικό Λεξικό. Για την κατασκευή του µοντέλου ακολουθήθηκε µια υβριδική προσέγγιση. 
Το πρώτο επίπεδο αυτής ακολουθεί την προσέγγιση της Μηχανικής Μάθησης (“Machine Learning”, 
[Gloss(00054)]) και επιλύει την ασάφεια του Μέρους του Λόγου (ΜτΛ). Η εν λόγω επίλυση 
πραγµατοποιήθηκε µε τη βοήθεια δέντρων απόφασης (“decision trees”, [Gloss(00021)]) τα οποία 
κατασκευάστηκαν από δείγµατα εκπαίδευσης. Τα δείγµατα εκπαίδευσης προήλθαν από προτάσεις του 
σώµατος των 7.624 προτάσεων. Πιο συγκεκριµένα κατασκευάστηκαν δέντρα απόφασης για κάθε ένα 
είδος ασάφειας ως προς το ΜτΛ καθώς επίσης και δέντρα απόφασης για να «µαντεύουν» το ΜτΛ των 
αγνώστων λέξεων. Το δεύτερο επίπεδο ακολουθεί τη γλωσσολογική προσέγγιση και επιλύει την 
υπόλοιπη ασάφεια µε τη βοήθεια χειρωνακτικά κωδικοποιηµένων συντακτικών κανόνων και ενός 
επιφανειακού συντακτικού αναλυτή µε την προϋπόθεση ότι έχει ήδη επιλυθεί η ασάφεια ως προς το 
ΜτΛ.  
Το Σχήµα 6.1 παρουσιάζει την αρχιτεκτονική του Μορφοσυντακτικού Αναλυτή ο οποίος 
αποτελείται από τρία τµήµατα: τον Αναγνωριστή συµβόλων και προτάσεων, το Μορφολογικό Λεξικό 
και τον Αποσαφηνιστή.  
 179
Κεφάλαιο 6  Τµηµατοποίηση Ελληνικών  Κειµένων  
 
λέξεις, σηµεία στίξης,
αριθµοί, κτλ. 
λέξεις µε 
ασάφεια
µορφοσυντακτική 
πληροφορία
αναγνωριστής 
συµβόλων και 
προτάσεων
ASCII κείµενο
άγνωστες
λέξεις
αποσαφηνισµένη
µορφοσυντακτική
πληροφορία
µορφολογικό
λεξικό
αποσαφηνιστής
συµφρα-
ζόµενα
mark-up
φορµαλισµός
(XML)
ακολουθία συµβόλων και προτάσεων
κείµενο µε
µορφοσυντακτικό
σχολιασµό
λέξεις
 
Σχήµα 6.1: Αρχιτεκτονική του Μορφοσυντακτικού Αναλυτή. 
Ο Αναγνωριστής συµβόλων και προτάσεων σε πρώτη φάση αναγνωρίζει τα σύµβολα που 
εµφανίζονται στο πηγαίο κείµενο (λέξεις, σηµεία στίξης, αριθµοί, συντµήσεις κλπ). Σε δεύτερη φάση 
αναγνωρίζει τα όρια των προτάσεων, µετασχηµατίζοντας το πηγαίο κείµενο σε µια ακολουθία 
συµβόλων µέσα στην οποία έχουν µαρκαριστεί τα όρια των προτάσεων. Όσα σύµβολα αποτελούνται 
αποκλειστικά από αλφαβητικούς χαρακτήρες (δηλ. οι λέξεις) αναζητούνται στο Μορφολογικό Λεξικό, 
µε τρία πιθανά ενδεχόµενα: α) να τους αποδοθεί από το Λεξικό σαφής µορφοσυντακτική πληροφορία, 
συµπεριλαµβανοµένης και της πληροφορίας του λήµµατος της λέξης β) να τους αποδοθεί από το 
λεξικό ασαφής µορφοσυντακτική πληροφορία και γ) να µην τους αποδοθεί καθόλου µορφοσυντακτική 
πληροφορία, επειδή δεν υπάρχουν στο λεξικό. Ο Αποσαφηνιστής αναλαµβάνει να αντιµετωπίσει τις 
περιπτώσεις (β) και (γ). Στην περίπτωση (β) ανατρέχει στο πρώτο επίπεδο του µοντέλου που 
κατασκευάστηκε και πιο συγκεκριµένα στο δέντρο απόφασης που επιλύει την ασάφεια αυτού του 
τύπου. Ως αποτέλεσµα επιστρέφεται η σαφής µορφοσυντακτική πληροφορία της λέξης 
ακολουθούµενη από το λήµµα της. Στην περίπτωση (γ) ο Αποσαφηνιστής ανατρέχει στο δεύτερο 
επίπεδο του µοντέλου και πιο συγκεκριµένα στο δέντρο απόφασης που αντιστοιχεί στις άγνωστες 
λέξεις. Το εν λόγω δέντρο απόφασης διασχίζεται λαµβάνοντας υπόψη του συντακτικά χαρακτηριστικά 
όπως π.χ. το τέλος της λέξης και πιθανά κεφαλαία γράµµατα, και επιστρέφει µε τη σειρά του το λήµµα 
και τη µορφοσυντακτική πληροφορία της εξεταζόµενης λέξης.  
Αξίζει να σηµειωθεί ότ, τα δέντρα απόφασης αποσαφηνίζουν όλες τις λέξεις που είναι 
ασαφείς ως προς το ΜτΛ, κάνοντας όµως λάθος στο ~5% των λέξεων, ενώ  το δέντρο απόφασης σε 
συνδυασµό µε τους κανόνες που αποσαφηνίζουν επακριβώς τις άγνωστες λέξεις - που είναι ασαφείς 
ως προς τα λοιπά µορφοσυντακτικά χαρακτηριστικά - επιτυγχάνουν στο ~80% των περιπτώσεων. Το 
 180
Κεφάλαιο 6  Τµηµατοποίηση Ελληνικών  Κειµένων  
υπόλοιπο ~20% των λέξεων εξακολουθεί να παρουσιάζει (µειωµένη) ασάφεια  ως προς τα λοιπά 
µορφοσυντακτικά χαρακτηριστικά.  
6.4 Πειράµατα   
 
∆οθείσης της υψηλής απόδοσης του αλγορίθµου τµηµατοποίησης µας στη συλλογή κειµένων 
του Choi θεωρήσαµε ενδιαφέρουσα την εφαρµογή του σε ένα ελληνικό σώµα κειµένων. Στις 
παραγράφους που ακολουθούν παρουσιάζουµε δυο σειρές πειραµάτων. Στην πρώτη σειρά 
εφαρµόζουµε τον αλγόριθµό µας πάνω σε ελληνικά κείµενα αλλά το πρόβληµα το οποίο καλούµαστε 
να επιλύσουµε είναι πανοµοιότυπο µε αυτό της συλλογής κειµένων του Choi σε ότι αφορά το µήκος 
των τµηµάτων. Στη δεύτερη σειρά πειραµάτων ο αλγόριθµός µας καλείται να τµηµατοποιήσει 
µεγαλύτερα σε µέγεθος κείµενα όπου τώρα το κάθε τµήµα αποτελείται από ένα µεταβλητό αριθµό 
παραγράφων και όχι προτάσεων όπως συµβαίνει στην πρώτη σειρά. Τα αποτελέσµατα τα οποία 
προέκυψαν και από τις δυο σειρές πειραµάτων - χρησιµοποιώντας για άλλη µια φορά τα κριτήρια 
Precision, Recall και το κριτήριο  του Beeferman - ήταν ιδιαίτερα καλά γεγονός που καθιστά τον 
αλγόριθµό µας ιδιαίτερα εύρωστο και αξιόπιστο ακόµα και για την επίλυση προβληµάτων 
τµηµατοποίησης όπου η τυπική απόκλιση από το µέσο µήκος τµήµατος είναι µεγάλη. Τα εν λόγω 
πειράµατα παρουσιάζονται διεξοδικά στις ενότητες που ακολουθούν. 
kP
6.4.1 Προεπεξεργασία  
 
Τόσο στην πρώτη όσο και στην δεύτερη σειρά πειραµάτων χρησιµοποιήσαµε κοµµάτια 
κειµένων προερχόµενα από τη συλλογή άρθρων της εφηµερίδας «Το Βήµα» η οποία κατασκευάστηκε 
από τους Σταµατάτο, Φακοτάκη και Κοκκινάκη. Σε κάθε ένα από τα τριακόσια κείµενα της εν λόγω 
συλλογής εφαρµόστηκε ο Μορφοσυντακτικός Αναλυτής της Νέας Ελληνικής. Ως αποτέλεσµα της εν 
λόγω εφαρµογής αρχικά αφαιρέθηκαν όλα τα σηµεία στίξης, αριθµοί, ηµεροµηνίες, διάφορα µέρη του 
λόγου όπως αριθµητικά, σύνδεσµοι αντωνυµίες κλπ. τα οποία θεωρήσαµε ότι δεν προσδίδουν 
ιδιαίτερη πληροφορία σχετικά µε το ύφος και τη θεµατική κατηγορία στην οποία ανήκει κάθε κείµενο, 
ενώ αντίθετα κρατήθηκαν όλες οι λέξεις οι οποίες ήταν είτε ουσιαστικά είτε ρήµατα είτε επίθετα είτε 
επιρρήµατα εφόσον τα επιρρήµατα δεν ήταν κοινότυπα δηλαδή προσέδιδαν ιδιαίτερη πληροφορία 
σχετικά µε την θεµατική κατηγορία στην οποία ανήκε το κείµενο. Στη συνέχεια, κάθε µια από τις 
εναποµείναντες λέξεις του κειµένου αντικαταστάθηκε από το αντίστοιχό της λήµµα ως αποτέλεσµα 
 181
Κεφάλαιο 6  Τµηµατοποίηση Ελληνικών  Κειµένων  
της εφαρµογής του Μορφοσυντακτικού Αναλυτή. Στην περίπτωση που το λήµµα µιας λέξης δεν 
υπήρχε στο Λεξικό και ο Μορφοσυντακτικός Αναλυτής απέτυχε να αποδώσει µορφοσυντακτική 
πληροφορία µέσω του Αποσαφηνιστή του, η εν λόγω λέξη χρησιµοποιούνταν αυτούσια. Η µοναδική 
πληροφορία η οποία κρατήθηκε για το κάθε κείµενο ήταν τα όρια µεταξύ των προτάσεων και των 
παραγράφων του. Τα κείµενα µε την καινούργια τους πλέον µορφή χρησιµοποιήθηκαν για την 
κατασκευή της εκάστοτε συλλογής κειµένων προς τµηµατοποίηση σε κάθε µια από τις σειρές 
πειραµάτων. Ένα παράδειγµα εφαρµογής του Μορφοσυντακτικού Αναλυτή σε ένα άρθρο της 
παραπάνω συλλογής καθώς και το αποτέλεσµα της αντικατάστασης των λέξεων που είναι είτε 
ρήµµατα είτε ουσιαστικά είτε επίθετα είτε µη κοινότυπα επιρρήµατα από το αντίστοιχό τους λήµµα 
δίνεται στο Παράρτηµα Α3.  
 6.4.2 Πρώτη Σειρά Πειραµάτων 
 
Όπως αναφέρθηκε και προηγούµενα, στην εν λόγω σειρά πειραµάτων καλούµαστε να 
εξετάσουµε την απόδοση τµηµατοποίησης του αλγορίθµου µας σε ένα πρόβληµα πανοµοιότυπο µε 
αυτό της συλλογής κειµένων του Choi. Και στις δυο σειρές πειραµάτων, για τη µοντελοποίηση του 
παράγοντα που αντιστοιχεί στην πληροφορία του µήκους τµήµατος χρησιµοποιήθηκε η συνάρτηση U-
shape δεδοµένου ότι στα πειράµατα τα οποία πραγµατοποιήθηκαν στο προηγούµενο κεφάλαιο αυτή 
πέτυχε την µεγαλύτερη ακρίβεια τµηµατοποίησης. Με άλλα λόγια το κόστος τµηµατοποίησης 
υπολογίστηκε συνδυάζοντας τις εξισώσεις 5.4 και 5.5 του προηγούµενου κεφαλαίου.  
Πιο συγκεκριµένα, για τα πειράµατα της πρώτης σειράς κατασκευάσαµε έξη σύνολα 
δεδοµένων τα Set0, …., Set5. Κάθε ένα από αυτά τα σύνολα διαφέρει από τα υπόλοιπα ως προς τον 
αριθµό των συγγραφέων που χρησιµοποιεί για να κατασκευάσει κάθε ένα από τα κείµενα προς 
τµηµατοποίηση και κατά συνέπεια ως προς τον συνολικό αριθµό επιλεγόµενων κειµένων από τη 
συλλογή των άρθρων της εφηµερίδας «Το Βήµα». Η εν λόγω πληροφορία  συνοψίζεται στον Πίνακα 
6.3.  
Κάθε ένα από τα έξη σύνολα αποτελείται από τέσσερα υποσύνολα κειµένων. Η διαφορά 
µεταξύ αυτών των υποσυνόλων έγκειται στο εύρος n των προτάσεων που συνιστούν κάθε ένα τµήµα 
των κειµένων προς τµηµατοποίηση. Ο Πίνακας 6.4 περιέχει την πληροφορία του ελάχιστου και του 
µέγιστου αριθµού προτάσεων οι οποίες εµφανίζονται στα τµήµατα των κειµένων καθενός εκ των 
τεσσάρων υποσυνόλων.  
 
 182
Κεφάλαιο 6  Τµηµατοποίηση Ελληνικών  Κειµένων  
 
Set Συγγραφέας Συνολικό Πλήθος Κειµένων ανά 
Set 
0 Κιοσσέ, Αλαχιώτης 60 
1 Κιοσσέ, Μαρωνίτης 60 
2 Κιοσσέ, Μαρωνίτης, Αλαχιώτης 90 
3 Κιοσσέ, Μαρωνίτης, Αλαχιώτης, Πλωρίτης 120 
4 Κιοσσέ, Μαρωνίτης, Αλαχιώτης, Πλωρίτης, Βώκος 150 
5 Όλοι οι συγγραφείς 300 
Πίνακας 6.3: Λίστα των συλλογών (Set0, … , Set5) που δηµιουργήθηκαν στη πρώτη σειρά πειραµάτων και τα 
κείµενα των συγγραφέων που χρησιµοποιήθηκαν σε κάθε µια από αυτές. 
 
 Subset0 Subset1 Subset2 Subset3 
Έυρος n  (προτάσεων ανά τµήµα)  3-11 3-5 6-8 9-11 
Πίνακας 6.4: Εύρος του n (αριθµού προτάσεων) για τα τέσσερα υποσύνολα Subset0, Subset1, Subset2 και 
Subset3 καθενός εκ των έξη συνόλων των κειµένων της πρώτης σειράς πειραµάτων. 
 
Πιο συγκεκριµένα, για κάθε ένα από τα έξη σύνολα, αρχικά επιλέγουµε το πλήθος και την 
ταυτότητα των συγγραφέων από τους οποίους επιθυµούµε να αντλήσουµε κείµενα, τµήµατα των 
οποίων θα χρησιµοποιηθούν για την δηµιουργία των κειµένων προς τµηµατοποίηση (το αποτέλεσµα 
της εν λόγω επιλογής παραθέτεται στον Πίνακα 6.3). Κάθε υποσύνολο καθενός συνόλου περιέχει 
πενήντα κείµενα, εικοσιπέντε εκ των οποίων χρησιµοποιούνται για εκπαίδευση ενώ τα υπόλοιπα για 
επαλήθευση. Κάθε ένα από τα πενήντα αυτά κείµενα αποτελείται από δέκα τµήµατα. Κάθε τµήµα 
αποτελεί ένα κοµµάτι κειµένου – εκ των τριάντα διαθέσιµων - ενός συγγραφέα εξ αυτών που 
εµπλέκονται στην δηµιουργία των κειµένων του συνόλου, ο οποίος επιλέγεται κάθε φορά µε τυχαίο 
τρόπο. Η διαδικασία παραγωγής των κειµένων προς τµηµατοποίηση πραγµατοποιήθηκε 
ακολουθώντας τις παρακάτω παραδοχές:  
 Κάθε τµήµα αντιστοιχεί σε ένα κοµµάτι κειµένου διαφορετικού συγγραφέα ούτως ώστε δυο 
διαδοχικά τµήµατα να µην αντιστοιχούν σε κείµενα του ιδίου συγγραφέα.  
 183
Κεφάλαιο 6  Τµηµατοποίηση Ελληνικών  Κειµένων  
 Τα όρια µεταξύ τµηµάτων απαντώνται πάντα στο τέλος των προτάσεων και όχι σε 
οποιοδήποτε σηµείο µέσα σε αυτές. Για κάθε ένα από τα σχηµατιζόµενα κείµενα προς 
τµηµατοποίηση φυλάσσεται η πληροφορία της θέσης των ορίων µεταξύ των τµηµάτων. Η εν 
λόγω πληροφορία θα χρησιµοποιηθεί αργότερα για την αξιολόγηση της απόδοσης του 
αλγορίθµου µας. 
Για τη δηµιουργία καθενός από τα παραπάνω πενήντα κείµενα προς τµηµατοποίηση καθενός 
υποσυνόλου για κάθε ένα από τα έξη σύνολα κειµένων ακολουθείται η παρακάτω διαδικασία: 
Έστω το πλήθος των συγγραφέων οι οποίοι συνεισφέρουν στη δηµιουργία του εκάστοτε συνόλου. 
Για κάθε ένα από τα δέκα τµήµατα του κειµένου προς τµηµατοποίηση ακολουθούµε τα παρακάτω 
βήµατα: 
Βήµα 1ο
Βήµα 2ο: Επιλέγουµε µε τυχαίο τρόπο (µε τη βοήθεια µιας γεννήτριας τυχαίων αριθµών) ένα από τα 
τριάντα διαθέσιµα κείµενα του συγγραφέα Ι. Έστω Κ  η µεταβλητή η οποία δηλώνει το επιλεγόµενο 
κείµενο του συγγραφέα Ι. 
Βήµα 3ο
Βήµα 4ο: Εξάγουµε από το κείµενο Κ, συνεχόµενες προτάσεις ξεκινώντας από την πρώτη πρόταση του 
κειµένου. Οι εν λόγω προτάσεις συνιστούν το παραγόµενο τµήµα. 
 
Όπως αναφέρθηκε στο προηγούµενο κεφάλαιο και πιο συγκεκριµένα στην παράγραφο 5.2.2, ο 
αλγόριθµός µας χρησιµοποιεί τέσσερις παραµέτρους τις µ, γ, σ και r. Όπως επίσης επισηµάνθηκε, οι 
παράµετροι µ, και σ  παίζουν τον ρόλο του µέσου µήκους και της τυπικής απόκλισης από αυτό. Για να 
υπολογίσουµε τις βέλτιστες τιµές των παραµέτρων αυτών χρησιµοποιούµε την ίδια διαδικασία 
εκπαίδευσης και επαλήθευσης των παραµέτρων που χρησιµοποιήθηκε στην παράγραφο 5.3.2.1 και πιο 
συγκεκριµένα στη δεύτερη σειρά του πρώτου γκρουπ πειραµάτων για κάθε ένα από τα τέσσερα 
υποσύνολα καθενός από τα έξη σύνολα κειµένων προς τµηµατοποίηση.  
Χ
: Επιλέγουµε µε τυχαίο τρόπο (µε τη βοήθεια µιας γεννήτριας τυχαίων αριθµών) έναν συγγραφέα 
µεταξύ των  οι οποίοι συνεισφέρουν στη δηµιουργία του εν λόγω συνόλου. Έστω Ι  η µεταβλητή η 
οποία δηλώνει τον εν λόγω συγγραφέα.  
Χ
: Επιλέγουµε µε τυχαίο τρόπο (µε τη βοήθεια µιας γεννήτριας τυχαίων αριθµών) έναν αριθµό 
 όπου τα και b αντιστοιχούν στο ελάχιστο και το µέγιστο πλήθος προτάσεων που µπορεί 
να περιέχει ένα τµήµα στο εκάστοτε υποσύνολο κειµένων προς τµηµατοποίηση.   
{ bal ,...,∈ } α
l
Η εν λόγω διαδικασία οδηγεί στη δηµιουργία πενήντα κειµένων για κάθε ένα υποσύνολο 
καθενός από τα έξη σύνολα κειµένων.  
 184
Κεφάλαιο 6  Τµηµατοποίηση Ελληνικών  Κειµένων  
Τα σχήµατα 6.2 - 6.5 (τα οποία αντιστοιχούν στα υποσύνολα Subset0, Subset1, Subset2 και 
Subset3 του συνόλου Set5) παρέχουν µια ιδέα του βαθµού επηρεασµού των παραµέτρων γ  και r στο 
κριτήριο  του Beeferman για την πρώτη σειρά πειραµάτων.  
Η διαδικασία επαλήθευσης επαναλαµβάνεται πέντε φορές για κάθε ένα από τα έξη σύνολα και 
για τις ληφθείσες τιµές των κριτηρίων Precision, Recall και  του Beeferman υπολογίζεται ο µέσος 
όρος. Ο Πίνακας 6.6 περιέχει τους µέσους όρους των τιµών και για τα έξη σύνολα κειµένων της 
πρώτης σειράς πειραµάτων σε συγκριτική παράθεση µε τα αντίστοιχα ληφθέντα αποτελέσµατα από τη 
συλλογή κειµένων του Choi. Ο Πίνακας 6.5 παραθέτει τις τιµές των Precision, Recall και  του 
Beeferman αναλυτικά για κάθε ένα σύνολο και για κάθε ένα υποσύνολο.  Το Σχήµα 6.6 παρέχει µια 
εποπτικότερη εικόνα της πληροφορίας που περιέχεται στον Πίνακα 6.5 ως προς το κριτήριο  του 
Beeferman. 
 
Σχήµα 6.2: Η απόδοση του κριτηρίου  του Beeferman για το υποσύνολο Subset0 του συνόλου 
κειµένων Set5 για τις διάφορες τιµές των παραµέτρων γ  και r.  
 
 
kP
kP
kP
kP
0
0,05
0,1
0,15
0,2
0,25
0,3
0,35
0,4
0,45
0,5
0 0,1 0,2 0,3 0,4 0,5 0,6 0,7 0,8 0,9 1
γ
Pk
r = 1
r =2/3
r =1/2
r =1/3
 
kP
 185
Κεφάλαιο 6  Τµηµατοποίηση Ελληνικών  Κειµένων  
  
Σχήµα 6.3: Η απόδοση του κριτηρίου  του Beeferman για το υποσύνολο Subset1 του συνόλου 
κειµένων Set5 για τις διάφορες τιµές των παραµέτρων γ  και r.  
 
Σχήµα 6.4: Η απόδοση του κριτηρίου  του Beeferman για το υποσύνολο Subset2 του συνόλου 
κειµένων Set5 για τις διάφορες τιµές των παραµέτρων γ  και r.  
 
0
0,05
0,1
0,15
0,2
0,25
0,3
0,35
0,4
0,45
0,5
0 0,1 0,2 0,3 0,4 0,5 0,6 0,7 0,8 0,9 1
γ
Pk
r = 1
r =2/3
r =1/2
r =1/3
kP
 
0
0,02
0,04
0,06
0,08
0,1
0,12
0,14
0,16
0,18
0 0,1 0,2 0,3 0,4 0,5 0,6 0,7 0,8 0,9 1
γ
Pk
r = 1
r =2/3
r =1/2
r =1/3
kP
 186
Κεφάλαιο 6  Τµηµατοποίηση Ελληνικών  Κειµένων  
0
0,05
0,1
0,15
0,2
0,25
0,3
0,35
0,4
0,45
0,5
0 0,1 0,2 0,3 0,4 0,5 0,6 0,7 0,8 0,9 1
γ
Pk
r = 1
r =2/3
r =1/2
r =1/3
 
Σχήµα 6.5: Η απόδοση του κριτηρίου  του Beeferman για το υποσύνολο Subset3 του συνόλου 
κειµένων Set5 για τις διάφορες τιµές των παραµέτρων γ  και r.  
kP
 
Παρατηρώντας τα αποτελέσµατα που παραθέτονται στον Πίνακα 6.5 συµπεραίνουµε ότι όσο 
αυξάνεται το πλήθος των συγγραφέων που συνεισφέρουν στη δηµιουργία των κειµένων προς 
τµηµατοποίηση τόσο µειώνεται η απόδοση τµηµατοποίησης του αλγορίθµου µας. Πραγµατοποιώντας 
συγκριτική παράθεση της βέλτιστης και της χείριστης απόδοσης τµηµατοποίησης καθενός 
υποσυνόλου και για τα έξη σύνολα και υπολογίζοντας τον µέσο όρο των διαφορών αυτών και για τα 
τέσσερα υποσύνολα προκύπτει ότι η εν λόγω µείωση είναι κατά µέσο όρο 3.17% ως προς το κριτήριο 
 του Βeeferman. Η εν λόγω µείωση είναι µικρή δεδοµένου ότι οι θεµατικές κατηγορίες στις οποίες 
ανήκουν τα κείµενα των δέκα συγγραφέων είναι πολλές φορές επικαλυπτόµενες ή σε υψηλό βαθµό 
παρεµφερείς. Το γεγονός αυτό σε συνδυασµό µε τη σηµαντική οµοιότητα σε ότι αφορά την ακρίβεια 
τµηµατοποίησης του αλγορίθµου µας τόσο στη συλλογή του Choi όσο και στη συλλογή που περιέχει 
τµήµατα άρθρων της εφηµερίδας «Το Βήµα» όπου στην ουσία το πρόβληµα το οποίο καλείται να 
αντιµετωπίσει είναι πανοµοιότυπο, όπως αυτή διαφαίνεται στον Πίνακα 6.6 αλλά και στο σχήµα 6.7 
καθιστά τον αλγόριθµό µας ιδιαίτερα αξιόπιστο και αποδοτικό. 
kP
 
 
 
 187
Κεφάλαιο 6  Τµηµατοποίηση Ελληνικών  Κειµένων  
 
1η σειρά πειραµάτων  Subset0 Subset1 Subset2 Subset3 
Set0 Precision 70.65% 86.82% 96.44% 93.33% 
Set0 Recall  71.11% 87.11% 96.44% 93.33% 
Set0 Beeferman’s  kP 14.04% 6.20% 0.82% 0.84% 
Set1 Precision 63.86% 82.98% 91.11% 94.67% 
Set1 Recall  67.11% 83.56% 91.11% 94.67% 
Set1 Beeferman’s  kP 15.82% 8.47% 2.80% 0.98% 
Set2 Precision 71.14% 90% 91.11% 92.44% 
Set2 Recall  60.89% 89.78% 91.11% 92.44% 
Set2 Beeferman’s  kP 14.42% 3.45% 2.15% 1.247% 
Set3 Precision 59.99% 84.44% 86.22% 91.11% 
Set3 Recall  58.67% 83.56% 86.22% 91.11% 
Set3 Beeferman’s  kP 17.93% 7.36% 3.28% 1.45% 
Set4 Precision 57.99% 85% 88.89% 91.11% 
Set4 Recall  51.11% 84.89% 88.89%  91.11% 
Set4 Beeferman’s  kP 17.38% 6.76% 2.65% 1.39% 
Set5 Precision 65.75% 81.56% 89.33% 88.89% 
Set5 Recall  61.78% 81.78% 89.33% 88.89% 
Set5 Beeferman’s  kP 14.54% 6.49% 3.57% 1.86% 
Πίνακας 6.5: Οι τιµές των κριτηρίων Precision, Recall και  του Beeferman για τα σύνολα Set0, Set1, Set2, 
Set3, Set4 και Set5 χρησιµοποιώντας προτάσεις ως µονάδα τµήµατος, όπως αυτές ελήφθησαν µετά από τη 
διαδικασία εκπαίδευσης και επαλήθευσης.  
kP
 
 
 188
Κεφάλαιο 6  Τµηµατοποίηση Ελληνικών  Κειµένων  
0
3
6
9
12
15
18
Subset0(3-11), Subset1(3-5),Subset2( 6-8),Subset3( 9-11)
Set0 Set1 Set2 Set3 Set4 Set5
 
Σχήµα 6.6: Η απόδοση του κριτηρίου  του Beeferman για τα σύνολα Set0, Set1, Set2, Set3, Set4 
και Set5 χρησιµοποιώντας προτάσεις ως µονάδα τµήµατος, µετά από τη διαδικασία εκπαίδευσης και 
επαλήθευσης 
kP
 
 Subset0 Subset1 Subset2 Subset3 
Precision (1η σειρά πειραµάτων) 64.90% 85.13% 90.51% 91.92% 
Recall(1η σειρά πειραµάτων) 61.77% 85.11% 90.51% 91.92% 
Beeferman’s    (1kP
η σειρά πειραµάτων) 15.69% 6.45% 2.54% 1.29% 
Precision (Choi) 82.66% 82.66% 88.68% 92.37% 
Recall(Choi) 82.78% 87.70% 88.71% 92.44% 
Beeferman’s   (Choi) kP 7.00% 5.45% 3.00% 1.33% 
Πίνακας 6.6: Οι µέσοι όροι των τιµών των κριτηρίων Precision, Recall και  του Beeferman για τις συλλογές 
Set0, Set1, Set2, Set3, Set4 και Set5 χρησιµοποιώντας προτάσεις ως µονάδα τµήµατος, όπως αυτές ελήφθησαν 
µετά από τη διαδικασία εκπαίδευσης και επαλήθευσης και οι αντίστοιχες της συλλογής κειµένων του Choi. 
kP
 
 
 189
Κεφάλαιο 6  Τµηµατοποίηση Ελληνικών  Κειµένων  
0
15
30
45
60
75
90
Subset0(3-11) Subset1(3-5) Subset2(6-8) Subset3(9-11)
Precision (1η σειρά πειραµάτων) Precision (Choi)
Recall(1η σειρά πειραµάτων) Recall(Choi)
Beeferman’s  Pk  (1η σειρά πειραµάτων) Beeferman’s  Pk (Choi)
 
 
Σχήµα 6.7: Οι µέσοι όροι των τιµών των κριτηρίων Precision, Recall και  του Beeferman για τις 
συλλογές Set0, Set1, Set2, Set3, Set4 και Set5 χρησιµοποιώντας προτάσεις ως µονάδα τµήµατος, όπως 
αυτές ελήφθησαν µετά από τη διαδικασία εκπαίδευσης και επαλήθευσης και οι αντίστοιχες της 
συλλογής κειµένων του Choi. 
kP
 
6.4.3 ∆εύτερη Σειρά Πειραµάτων 
 
Στη δεύτερη σειρά πειραµάτων χρησιµοποιούµε ξανά τα τριακόσια άρθρα που προέρχονται από 
την εφηµερίδα «Το Βήµα». Η διαφορά ανάµεσα στις δυο σειρές πειραµάτων έγκειται στον τρόπο 
δηµιουργίας των δεδοµένων προς τµηµατοποίηση και κατ’ επέκταση του µεγέθους των τµηµάτων από 
τα οποία αυτά αποτελούνται. ∆εδοµένου ότι επιθυµούµε να εξετάσουµε την απόδοση του αλγορίθµου 
µας σε δυσκολότερα προβλήµατα σε ότι αφορά το µέσο µήκος τµήµατος και την τυπική απόκλιση από 
αυτό, στην εν λόγω σειρά επιλέγουµε τα τµήµατα των κειµένων προς τµηµατοποίηση να αποτελούνται 
από µια ή περισσότερες παραγράφους. Όπως και στην πρώτη σειρά πειραµάτων το κόστος 
τµηµατοποίησης προκύπτει από τον συνδυασµό των εξισώσεων 5.4 και 5.5 του προηγούµενου 
κεφαλαίου δηλαδή χρησιµοποιώντας τη συνάρτηση U-shape στον παράγοντα που αντιστοιχεί στην 
πληροφορία του µήκους τµήµατος.  
 Πιο συγκεκριµένα, η εν λόγω σειρά πειραµάτων περιλαµβάνει µια και µοναδική συλλογή η 
οποία αποτελείται από διακόσια κείµενα τα οποία κατασκευάζονται χρησιµοποιώντας και τα 
 190
Κεφάλαιο 6  Τµηµατοποίηση Ελληνικών  Κειµένων  
τριακόσια άρθρα της εφηµερίδας «Το Βήµα» δηλαδή και τους δέκα διαθέσιµους συγγραφείς. Από τα 
διακόσια κείµενα προς τµηµατοποίηση τα οποία κατασκευάστηκαν, τα µισά χρησιµοποιήθηκαν για 
εκπαίδευση (µε σκοπό την εύρεση της βέλτιστης τιµής των παραµέτρων µ, γ, σ  και r) ενώ τα 
υπόλοιπα για επαλήθευση. Και στην εν λόγω σειρά ακολουθήθηκαν οι παρακάτω παραδοχές: 
 Κάθε τµήµα αντιστοιχεί σε ένα κοµµάτι κειµένου διαφορετικού συγγραφέα ούτως ώστε δυο 
διαδοχικά τµήµατα να µην αντιστοιχούν σε κείµενα του ιδίου συγγραφέα.  
 Τα όρια µεταξύ τµηµάτων απαντώνται στο τέλος των παραγράφων και όχι σε οποιοδήποτε 
σηµείων µέσα σε αυτές. Για καθένα από τα σχηµατιζόµενα κείµενα φυλάσσεται η πληροφορία 
των ορίων µεταξύ των τµηµάτων. Η εν λόγω πληροφορία θα χρησιµοποιηθεί αργότερα για την 
αξιολόγηση της απόδοσης τµηµατοποίησης του αλγορίθµου µας.  
Τα εν λόγω κείµενα κατασκευάστηκαν σύµφωνα µε την ακόλουθη διαδικασία η οποία 
εγγυάται ότι κάθε κείµενο προς τµηµατοποίηση θα αποτελείται από δέκα τµήµατα όπου τα όρια 
µεταξύ των διαφόρων τµηµάτων θα απαντώνται στο τέλος µιας πρότασης η οποία θα συµπίπτει και µε 
το τέλος παραγράφου.  
Για κάθε ένα από τα δέκα τµήµατα του κειµένου προς τµηµατοποίηση ακολουθούµε τα παρακάτω 
βήµατα: 
Βήµα 1ο: Επιλέγουµε µε τυχαίο τρόπο (µε τη βοήθεια µιας γεννήτριας τυχαίων αριθµών) έναν συγγραφέα 
µεταξύ των δέκα  οι οποίοι συνεισφέρουν στη δηµιουργία της εν λόγω συλλογής. Έστω Ι  η µεταβλητή η 
οποία δηλώνει τον εν λόγω συγγραφέα.  
Βήµα 2ο:  Επιλέγουµε µε τυχαίο τρόπο (µε τη βοήθεια µιας γεννήτριας τυχαίων αριθµών) ένα από τα 
τριάντα διαθέσιµα κείµενα του συγγραφέα Ι. Έστω Κ  η µεταβλητή η οποία δηλώνει το επιλεγόµενο 
κείµενο του συγγραφέα Ι.. Το εν λόγω κείµενο διαβάζεται ούτως ώστε να καθοριστεί το πλήθος των 
παραγράφων από τις οποίες αποτελείται. Έστω ότι το κείµενο Κ περιέχει Ζ το πλήθος παραγράφους.  
Βήµα 3ο: Επιλέγουµε µε τυχαίο τρόπο (µε τη βοήθεια µιας γεννήτριας τυχαίων αριθµών) έναν αριθµό 
 ο οποίος αντιστοιχεί στο πλήθος των παραγράφων από τις οποίες θα αποτελείται το 
παραγόµενο τµήµα. 
{ Zl ,...,1∈ }
Βήµα 4ο: Επιλέγουµε µε τυχαίο τρόπο (µε τη βοήθεια µιας γεννήτριας τυχαίων αριθµών) έναν αριθµό 
 ο οποίος αντιστοιχεί στην «εναρκτήρια παράγραφο». Έτσι το παραγόµενο τµήµα θα 
περιέχει όλες τις παραγράφους του Κ-στου κειµένου ξεκινώντας από την παράγραφο ως την 
παράγραφο .  
{ lZm −∈ ,...,1
lm +
}
m
 191
Κεφάλαιο 6  Τµηµατοποίηση Ελληνικών  Κειµένων  
Όπως είναι φανερό, τα διακόσια παραγόµενα κείµενα προς τµηµατοποίηση σε αυτήν τη 
συλλογή είναι µεγαλύτερα σε µέγεθος από τα αντίστοιχά της πρώτης σειράς πειραµάτων. Εποµένως ο 
αλγόριθµός µας καλείται να αντιµετωπίσει ένα δυσκολότερο πρόβληµα. Το εν λόγω γεγονός 
αποδεικνύεται και από τα στατιστικά στοιχεία των κειµένων κάθε συγγραφέα όπως αυτά 
παραθέτονται στον Πίνακα 6.7.  
 
Συγγραφέας  Μ.Ο παραγράφων / 
κείµενο  
Μ.Ο προτάσεων / 
κείµενο 
Μ.Ο προτάσεων / 
παράγραφο 
1. Αλαχιώτης 9.6 42 4.375 
2. Βώκος  8.07 34.87 4.323 
3. ∆ερτιλής 10.94 45.2 4.1341 
4. Κιοσσέ 12.14 64.64 5.16 
5. Λιάκος 11.9 70.14 5.90 
6. Μαρωνίτης 6.37 23.74 3.73 
7. Μπαµπινίωτης  7.26 37.67 5.1834 
8. Πλωρίτης  14.27 57.57 4.035 
9. Τάσσιος 12.87 58.57 4.55 
10. Τσουκαλάς 10.4 56.5 5.432 
Σύνολο συγγραφέων 10.38 48.89 4.681789 
Πίνακας 6.7: Οι µέσοι όροι παραγράφων ανά κείµενο, προτάσεων ανά κείµενο και προτάσεων ανά παράγραφο 
των κειµένων καθενός από τους δέκα συγγραφείς της συλλογής άρθρων της εφηµερίδας «Το Βήµα» καθώς και 
οι αντίστοιχοι µέσοι όροι για το σύνολο αυτών.  
 
Κατόπιν υπολογισµών προέκυψε ότι τα διακόσια κείµενα τα οποία παρήχθησαν προς 
τµηµατοποίηση κατά µέσο όρο περιέχουν 27.5 προτάσεις ανά τµήµα. Εξετάζοντας διεξοδικά τα εν 
λόγω κείµενα υπολογίστηκε ότι ο µικρότερος αριθµός προτάσεων ανά τµήµα ανέρχεται στις 12.7 
προτάσεις ενώ ο µεγαλύτερος στις 44 προτάσεις ανά τµήµα, γεγονός το οποίο καθιστά φανερή την 
αύξηση του βαθµού δυσκολίας του προβλήµατος το οποίο κλήθηκε να αντιµετωπίσει ο αλγόριθµος 
τµηµατοποίησης µε δυναµικό προγραµµατισµό συγκρινόµενο µε αυτό της πρώτης σειράς πειραµάτων. 
Ο Πίνακας 6.8 περιέχει τα αποτελέσµατα του αλγορίθµου τµηµατοποίησης έπειτα από την εφαρµογή 
της τεχνικής εκπαίδευσης και στη συνέχεια της τεχνικής επαλήθευσης µε σκοπό τον υπολογισµό των 
βέλτιστων τιµών των παραµέτρων µ, γ, σ  και r. Όπως διαφαίνεται από τον παρακάτω πίνακα, η 
απόδοση τµηµατοποίησης εξακολουθεί να είναι ιδιαίτερα υψηλή παρά το γεγονός ότι  η τυπική 
απόκλιση από το µέσο µήκος τµήµατος είναι µεγάλη.  
 192
Κεφάλαιο 6  Τµηµατοποίηση Ελληνικών  Κειµένων  
 
2η σειρά πειραµάτων  
Precision 60.60% 
Recall 57.00% 
Beeferman’s  kP 11.07% 
Πίνακας 6.8: Οι µέσοι όροι των τιµών των κριτηρίων Precision, Recall και  του Beeferman για τη µοναδική 
συλλογής της δεύτερης σειρά πειραµάτων χρησιµοποιώντας παραγράφους ως µονάδα τµήµατος, όπως αυτές 
ελήφθησαν µετά από τη διαδικασία εκπαίδευσης και επαλήθευσης. 
kP
6.5 Συµπεράσµατα 
 
Σε αυτό το κεφάλαιο παρουσιάσαµε τα αποτελέσµατα εφαρµογής του αλγορίθµου 
τµηµατοποίησης σε συλλογές κειµένων που περιέχουν τµήµατα άρθρων της εφηµερίδας «Το Βήµα» 
γραµµένα στην ελληνική γλώσσα. Πιο συγκεκριµένα, παρουσιάσαµε δυο σειρές πειραµάτων κάθε µια 
από τις οποίες αντιµετώπιζε ένα διαφορετικής φύσης πρόβληµα.  
 Το πρόβληµα το οποίο κλήθηκε ο αλγόριθµός µας να αντιµετωπίσει στην πρώτη σειρά 
πειραµάτων ήταν παρεµφερές µε αυτό της συλλογής κειµένων του Choi. Σε αυτή τη σειρά πειραµάτων 
ο αλγόριθµός µας παρουσίασε ιδιαίτερα υψηλή ακρίβεια τµηµατοποίησης συγκρίσιµη µε αυτή που 
επέδειξε στη συλλογή κειµένων του Choi παρά το γεγονός ότι οι θεµατικές κατηγορίες των άρθρων 
της εφηµερίδας «Το Βήµα» είναι τις περισσότερες φορές παρεµφερείς και επικαλυπτόµενες. Η εν 
λόγω ληφθείσα απόδοση αποτελεί ισχυρή ένδειξη της σταθερότητας και της αποτελεσµατικότητας του 
αλγορίθµου µας σε τέτοιας φύσης προβλήµατα.  
 Η αποτελεσµατικότητα του αλγορίθµου µας όµως έγινε φανερή στην δεύτερη σειρά πειραµάτων 
όπου κλήθηκε να αντιµετωπίσει ένα σηµαντικά δυσκολότερο πρόβληµα το οποίο µοιάζει πολύ µε 
προβλήµατα που αφορούν πραγµατικά δεδοµένα. Έχοντας ως µονάδα τµήµατος αυτή τη φορά τις 
παραγράφους και όχι τις προτάσεις των άρθρων της εφηµερίδας «Το Βήµα», όπου τώρα το µέσο 
µήκος τµήµατος αλλά κυρίως η τυπική απόκλιση από αυτό ήταν ιδιαίτερα υψηλή, ο αλγόριθµός µας 
µε την ακρίβεια τµηµατοποίησης την οποία πέτυχε απέδειξε την υπεροχή και την ευρωστία του.  
 Αξίζει να σηµειωθεί ότι, επειδή η συλλογή άρθρων της εφηµερίδας «Το Βήµα» χρησιµοποιείται 
για πρώτη φορά για την επίλυση του προβλήµατος τµηµατοποίησης δεν µπορούµε να παραθέσουµε 
 193
Κεφάλαιο 6  Τµηµατοποίηση Ελληνικών  Κειµένων  
συγκριτικά αποτελέσµατα. Έµµεση σύγκριση πραγµατοποιείται µε τα πειράµατα της συλλογής 
κειµένων του Choi έναντι αυτών της πρώτης σειράς πειραµάτων. 
 Τέλος, αξίζει να δοθεί έµφαση στο γεγονός ότι η υψηλή απόδοση τµηµατοποίησης οφείλεται 
κατά ένα µεγάλο µέρος στην αποτελεσµατικότητα του Μορφοσυντακτικού Αναλυτή της Νέας 
Ελληνικής. Κατά κανόνα, ο εν λόγω Αναλυτής αποτυγχάνει να βρει το ΜτΛ και το αντίστοιχο λήµµα 
ιδιαίτερα τεχνικών λέξεων και όρων που δεν εµφανίζονται στο Λεξικό και ο Αποσαφηνιστής  δεν 
µπορεί να προσδιορίσει. Η χρήση αυτών όπως εµφανίζονται στο αυτούσιο άρθρο αποδείχθηκε ότι δεν 
αποτελεί παράγοντα µείωσης της ακρίβειας τµηµατοποίησης.  
Το Κεφάλαιο 7 που ακολουθεί συνοψίζει τα πλεονεκτήµατα των µοντέλων που 
παρουσιάστηκαν, αποτιµά το προσφερόµενο έργο της εν λόγω διατριβής και σκιαγραφεί τις 
µελλοντικές εφαρµογές και βελτιώσεις που είναι δυνατό να πραγµατοποιηθούν.  
 194
 
 
195 
Κεφάλαιο 7 Συµπεράσµατα 
KΕΦΑΛΑΙΟ 7     ΣΥΜΠΕΡΑΣΜΑΤΑ 
7.1 Αποτίµηση του έργου 
 
Η παρούσα διατριβή πραγµατεύεται τόσο την κατηγοριοποίηση κειµένων µε τη βοήθεια των 
λέξεων και των εννοιών αυτών, όσο και την τµηµατοποίηση µεγάλης έκτασης κειµένων µε χρήση 
µεθόδων υπολογιστικής νοηµοσύνης, στόχος των οποίων αποτελεί η εµβάθυνση στο περιεχόµενο των 
κειµένων και η ανάδειξη του τρόπου δόµησής τους. Στην ουσία, το πρόβληµα το οποίο καλείται να 
επιλύσει είναι η βελτίωση της πρόσβασης στη διαθέσιµη πληροφορία (όπως π.χ. αυτή λαµβάνει χώρα 
στις διάφορες µηχανές αναζήτησης) ούτως ώστε η επιστρεφόµενη σε ένα ερώτηµα του χρήστη 
πληροφορία να είναι αξιόπιστη τόσο σε επίπεδο ακρίβειας ως προς το ζητούµενο θέµα όσο και σε 
επίπεδο έκτασης. 
 Η λύση που προτείνεται στην παρούσα διατριβή είναι η ανάπτυξη υπολογιστικών µεθόδων για 
τον προσδιορισµό της εννοιολογικής δοµής η οποία περιέχεται στα κείµενα. Ο εν λόγω προσδιορισµός 
προσεγγίζεται σε πρώτη φάση µε την εύρεση του τρόπου συσχετισµού της πληροφορίας που 
εµφανίζεται µέσα στα κείµενα µε τη βοήθεια των εννοιών που περιέχονται σε αυτά και τη χρήση της 
εν λόγω πληροφορίας σε πλήθος αλγορίθµων Μηχανικής Μάθησης για την επίτευξη της 
κατηγοριοποίησης τόσο µε βάση τις έννοιες όσο και µε τις αυτούσιες λέξεις του κειµένου. Ο 
προσδιορισµός αυτός εµπλουτίζεται µε την ανάπτυξη µεθόδων προσδιορισµού της δοµής του 
εκάστοτε κειµένου δηλαδή των υποθεµάτων από τα οποία αυτό αποτελείται, µε τον αυτόµατο 
καθορισµό των τµηµάτων του κειµένου όπου καθένα από αυτά αντιστοιχεί σε διαφορετικό θέµα.  
 Οι προσπάθειες επίλυσης του εν λόγω προβλήµατος οδήγησαν στην κατασκευή τριών 
µοντέλων. Το πρώτο µοντέλο ασχολείται µε το πρόβληµα της κατηγοριοποίησης κειµένων κάνοντας 
χρήση της έννοιας κάθε λέξης – όπως αυτή καθορίζεται από το περιεχόµενο µέσα στο οποίο 
απαντάται και όπως αυτή προσδιορίζεται από τον θησαυρό όρων του Wordnet- χρησιµοποιώντας 
πλήθος µεθόδων κατηγοριοποίησης. Το δεύτερο µοντέλο ασχολείται µε το πρόβληµα της 
τµηµατοποίησης κειµένων κάνοντας χρήση µεθόδων κατηγοριοποίησης. Τέλος το τρίτο µοντέλο 
αποτελεί ένα µοντέλο τµηµατοποίησης κειµένων το οποίο υλοποιείται ως συνδυασµός µιας τεχνικής 
εύρεσης της οµοιότητας µεταξύ όλων των µερών του κειµένου και µιας τεχνικής αυτόµατου 
 196
Κεφάλαιο 7 Συµπεράσµατα 
καθορισµού των ορίων µεταξύ των τµηµάτων η οποία βασίζεται στη λογική του δυναµικού 
προγραµµατισµού.  
 Σε καθένα από αυτά τα µοντέλα ακολουθήθηκαν διαφορετικές προσεγγίσεις, 
πραγµατοποιήθηκαν διαφορετικές παραδοχές και χρησιµοποιήθηκαν ξεχωριστά σώµατα κειµένων 
προς εκπαίδευση και επαλήθευση. Πιο συγκεκριµένα, το πρώτο µοντέλο βασίστηκε στην παραδοχή 
ότι οι έννοιες οι οποίες προσδιορίζονται από το θησαυρό όρων του Wordnet για τις λέξεις που 
εµφανίζονται στο σώµα κειµένων του Brown Corpus είναι ορθά προσδιορισµένες δίχως να 
πραγµατοποιείται κάποιου άλλου είδους προεπεξεργασίας στα κείµενα. Στη συνέχεια 
χρησιµοποιήθηκαν ευρέως αποδεκτές µέθοδοι κατηγοριοποίησης οι οποίες εφαρµόστηκαν τόσο στις 
έννοιες όσο και στις αυτούσιες λέξεις των παραπάνω κειµένων. Η εγκυρότητα των ληφθέντων 
αποτελεσµάτων πραγµατοποιήθηκε µε την εισαγωγή µιας ευρείας γκάµας κριτηρίων αξιολόγησης.  
 Εντελώς αντίστοιχα, το δεύτερο µοντέλο βασίστηκε στην παραδοχή ότι η κατανοµή των 
κειµένων του Brown Corpus στις υπάρχουσες κατηγορίες είναι ορθή και χρησιµοποίησε την εν λόγω 
πληροφορία καθώς και αυτή των εννοιών των λέξεων που περιέχονται σε αυτά, για την 
πραγµατοποίηση της τµηµατοποίησης. Για την τµηµατοποίηση βασίστηκε στην παραδοχή ότι η 
διαίρεση ενός κειµένου σε τµήµατα πρέπει να βασιστεί στην οµοιογένεια τα οποία αυτά παρουσιάζουν 
δηλαδή στο κατά πόσο αυτά αναφέρονται στο ίδιο θέµα άρα ανήκουν στην ίδια κατηγορία. Στη 
συνέχεια, το εν λόγω µοντέλο χρησιµοποιεί ένα µεγάλο πλήθος παραλλαγών του αλγορίθµου 
Premmon σε συνδυασµό µε µια τεχνική ψηφοφορίας, οι οποίες εφαρµόζονται στο νεοσύστατο σώµα 
κειµένων το οποίο προέρχεται από τµήµατα κειµένων του Brown Corpus. Η εγκυρότητα των 
ληφθέντων αποτελεσµάτων επιτυγχάνεται µε την πραγµατοποίηση σηµαντικού πλήθους πειραµάτων 
για διαφορετικές τιµές των παραµέτρων των παραλλαγών του αλγορίθµου και τον συνδυασµό αυτών 
των παραλλαγών µε την τεχνική της ψηφοφορίας.  
 Τέλος, το τρίτο και τελευταίο µοντέλο βασίστηκε στην παραδοχή ότι, ένα κείµενο έχει 
γραµµική δοµή, οπότε ο αλγόριθµος που αναπτύχθηκε πραγµατοποιεί γραµµική τµηµατοποίηση των 
κειµένων µε ολικό τρόπο υπολογισµού της οµοιότητας όλων των µερών ενός κειµένου, ενώ για την 
εύρεση των ορίων µεταξύ των τµηµάτων που υποδηλώνουν αλλαγή θέµατος κάνει χρήση της τεχνικής 
του δυναµικού προγραµµατισµού. Ο προτεινόµενος αλγόριθµος αλγόριθµο τµηµατοποίησης µε 
δυναµικό προγραµµατισµό ελαχιστοποιεί ολικά µια συνάρτηση κόστους τµηµατοποίησης η οποία 
αποτελείται από: (α) την εντός- τµήµατος οµοιότητα, η οποία υπολογίζεται µε την βοήθεια του πίνακα 
οµοιότητας µεταξύ των προτάσεων οι οποίες συνιστούν το κείµενο, βασιζόµενη στις λέξεις του 
κειµένου και (β) προηγούµενη πληροφορία σχετικά µε το µήκος τµήµατος, δηλαδή το µέσο µήκος και 
την τυπική απόκλιση από αυτό. Η αποτελεσµατικότητα του εν λόγω µοντέλου εξετάστηκε πάνω σε 
τρία διαφορετικά σώµατα κειµένων, εκ των οποίων τα δυο πρώτα συνίστανται από αγγλικά κείµενα 
ενώ το τρίτο από ελληνικά κείµενα. Το πρώτο σώµα κειµένων αποτελεί η συλλογή κειµένων του Choi, 
 197
Κεφάλαιο 7 Συµπεράσµατα 
το δεύτερο σώµα κατασκευάστηκε από εµάς χρησιµοποιώντας κείµενα του Brown Corpus ενώ το 
τρίτο κατασκευάστηκε επίσης από εµάς χρησιµοποιώντας τµήµατα που προέρχονται από άρθρα της 
εφηµερίδας «Το Βήµα» γραµµένα στην ελληνική γλώσσα. Στα αγγλικά κείµενα πραγµατοποιήθηκε 
προεπεξεργασία η οποία περιλάµβανε την αφαίρεση εκείνων των λέξεων οι οποίες ανήκαν στην stop 
list (στην οποία περιέχονται όλες οι προθέσεις τα άρθρα, κοινότυπα ρήµατα,και γενικότερα οι λέξεις 
που δεν προσδίδουν ιδιαίτερη πληροφορία µέσα σε ένα κείµενο και η οποία παρατίθεται στο 
Παράρτηµα Α1) και την αντικατάσταση των εναποµεινάντων από τις λέξεις που προκύπτουν µετά από 
την αφαίρεση της κατάληξης [Gloss(00084)]. Στα ελληνικά κείµενα η προεπεξεργασία 
πραγµατοποιήθηκε µε τη βοήθεια της χρήσης του Μορφοσυντακτικού Αναλυτή της Νέας Ελληνικής 
[Ορφανός, 2000] η οποία οδήγησε στην αφαίρεση εκείνων των λέξεων που δεν προσέδιδαν σηµαντική 
πληροφορία στο κείµενο, και στην αντικατάσταση των υπολοίπων από το αντίστοιχό τους λήµµα. 
Απαραίτητη προϋπόθεση για την επαλήθευση της αποτελεσµατικότητας του αλγορίθµου µας 
αποτέλεσε η εκ των προτέρων γνώση της θέσης των ορίων µεταξύ των διαφόρων τµηµάτων. Η 
εγκυρότητα των ληφθέντων αποτελεσµάτων επικυρώθηκε τόσο από το πλήθος των 
πραγµατοποιηθέντων πειραµάτων, χρησιµοποιώντας εναλλακτικές µορφές της συνάρτησης κόστους 
τµηµατοποίησης, όσο και από τη χρήση τεχνικών επαλήθευσης των αποτελεσµάτων µε την κατασκευή 
δεδοµένων προς εκπαίδευση και αξιολόγηση του αλγορίθµου σε προηγούµενα άγνωστα κείµενα προς 
εξέταση. 
 Καθένα από τα παραπάνω µοντέλα συνεισφέρει µε τον δικό του τρόπο και βαθµό στο πρόβληµα 
το οποίο καλείται να επιλύσει η εν λόγω διατριβή. Πιο συγκεκριµένα, σε ότι αφορά το πρώτο µοντέλο, 
το απόσταγµα το οποίο προκύπτει από την εφαρµογή γνωστών αλγορίθµων κατηγοριοποίησης στο 
σώµα κειµένων του Brown Corpus, το οποίο αποδίδει την έννοια κάθε λέξης που εµφανίζεται στα 
κείµενα µε τη βοήθεια του θησαυρού όρων Wordnet, αποτελεί το γεγονός ότι η κατηγοριοποίηση των 
κειµένων χρησιµοποιώντας τις έννοιες προκύπτει ισοδύναµη και οριακά καλύτερη από την αντίστοιχη 
χρησιµοποιώντας τις αυτούσιες λέξεις. Το εν λόγω απόσταγµα αποδυναµώνει την εντύπωση ότι οι 
έννοιες των λέξεων αποκαλύπτουν σε σηµαντικά µεγαλύτερο βαθµό την εννοιολογική δοµή ενός 
κειµένου και εµβαθύνουν στο περιεχόµενο του συνολικού κειµένου σε αντιδιαστολή µε τις αυτούσιες 
λέξεις αυτού. Το νεωτεριστικό στοιχείο του εν λόγω µοντέλου συνίσταται στη χρήση των εννοιών σε 
όλο το εύρος του κειµένου και την εξέταση σηµαντικού πλήθους αλγορίθµων κατηγοριοποίησης για 
τη σύγκριση του αποτελέσµατος της κατηγοριοποίησης µε βάση τις έννοιες και τις αυτούσιες λέξεις 
των κειµένων. Από όσο είµαστε σε θέση να γνωρίζουµε, µια τέτοιου είδους συγκριτική παράθεση 
πραγµατοποιείται για πρώτη φορά στη βιβλιογραφία γεγονός που αναδεικνύει τη βαρύτητά της. 
Το δεύτερο µοντέλο που αναπτύχθηκε επιλύει το πρόβληµα της εύρεσης της δοµής ενός 
κειµένου, άρα και των υποθεµάτων από τα οποία αποτελείται, µε την εφαρµογή µιας τεχνικής 
τµηµατοποίησης η οποία αξιοποιεί την πληροφορία της κατηγορίας στην οποία καθένα από αυτά 
 198
Κεφάλαιο 7 Συµπεράσµατα 
ανήκει. Η γνώση της κατηγορίας των τµηµάτων ενός κειµένου αναδεικνύει µοναδικά τον τρόπο µε τον 
οποίο συσχετίζεται η πληροφορία η οποία περιέχεται µέσα σε αυτό. Ο εν λόγω συσχετισµός είναι 
δυνατό να ενισχυθεί και από την παρουσία των εννοιών των κειµένων εφόσον αυτές είναι διαθέσιµες. 
Το νεωτεριστικό στοιχείο του δεύτερου µοντέλου αποτελεί η προσέγγιση της τµηµατοποίησης µε τη 
χρήση τεχνικών κατηγοριοποίησης καθώς και η συγκριτική παράθεση της εν λόγω προσέγγισης µε τη 
χρήση τόσο των εννοιών όσο και των αυτούσιων λέξεων των κειµένων. Από όσο είµαστε σε θέση να 
γνωρίζουµε, η εν λόγω προσέγγιση παρατίθεται για πρώτη φορά στη  βιβλιογραφία. 
 Το τρίτο µοντέλο που αναπτύχθηκε επιλύει µε διαφορετικό τρόπο το πρόβληµα της εύρεσης της 
εννοιολογικής δοµής ενός κειµένου, άρα και του συσχετισµού της πληροφορίας η οποία περιέχεται 
µέσα σε αυτό, µε τον προσδιορισµό των υποθεµάτων τα οποία το συνιστούν εµβαθύνοντας µε αυτόν 
τον τρόπο στο περιεχόµενο του εκάστοτε κειµένου. Το εν λόγω µοντέλο µπορεί να θεωρηθεί ως η 
µεγαλύτερη συνεισφορά της διατριβής. Το νεωτεριστικό στοιχείο που περιέχει αποτελεί το γεγονός ότι 
αντιµετωπίζει µε ολικό τρόπο τόσο τον υπολογισµό της οµοιότητας ανάµεσα στα διάφορα µέρη ενός 
κειµένου όσο και τον τρόπο εύρεσης των ορίων µεταξύ των τµηµάτων. Οι ως τώρα προτεινόµενες στη 
βιβλιογραφία εργασίες αντιµετωπίζουν τον έναν µόνο από τους παραπάνω δυο υπολογισµούς µε ολικό 
τρόπο αλλά ποτέ και τους δυο ταυτόχρονα. Η επιτυχία της δικής µας προσέγγισης διαφαίνεται από τα 
ληφθέντα αποτελέσµατα τόσο στη συλλογή κειµένων του Choi η οποία χρησιµοποιήθηκε και από 
άλλους ερευνητές όσο και στις συλλογές που εµείς κατασκευάσαµε, καθιστώντας την µε αυτόν τον 
τρόπο ιδιαίτερα ποιοτική. 
 Μια από τις σηµαντικότερες συνεισφορές και νεωτερισµούς της διατριβής αποτελεί η 
κατασκευή του σώµατος κειµένων προς τµηµατοποίηση γραµµένα στην ελληνική γλώσσα τα οποία 
προέρχονται από άρθρα της εφηµερίδας «Το Βήµα» και η εφαρµογή του αλγορίθµου µας 
τµηµατοποίησης πάνω σε αυτό. Το πρόβληµα της τµηµατοποίησης ελληνικών κειµένων 
αντιµετωπίζεται για πρώτη φορά και αποτελεί ιδιαίτερη καινοτοµία, αν αναλογιστούµε την έντονη 
µορφολογία της ελληνικής γλώσσας (ιδιαίτερα χρήσιµος σε αυτή την προσπάθεια αποδείχθηκε ο 
Μορφοσυντακτικός Αναλυτής της Νέας Ελληνικής [Ορφανός, 2000]) και το γεγονός ότι εφαρµόζεται 
για πρώτη φορά σε ελληνικά κείµενα µια µέθοδος υπολογιστικής νοηµοσύνης µε σκοπό τη 
σκιαγράφηση της εννοιολογικής δοµής ελληνικών κειµένων. 
 Τα µοντέλα που αναπτύχθηκαν παραπάνω δεν έχουν προς το παρόν ενσωµατωθεί σε κάποια 
εφαρµογή. Η σπουδαιότητά τους, παρόλα αυτά, καθίσταται προφανής στην ενδεχόµενη ενσωµάτωσή 
τους σε µηχανές αναζήτησης ως ένα στάδιο προεπεξεργασίας, για τη βελτίωση πρόσβασης σε 
πληροφορία τόσο σε ότι αφορά την πιστότητα της επιστρεφόµενης πληροφορίας  ως προς το 
διατυπωθέν ερώτηµα όσο και στην έκταση αυτής. Οι θεµατικές περιοχές στις οποίες βρίσκουν 
απήχηση τα παραπάνω µοντέλα είναι οι τοµείς της Τµηµατοποίησης Κειµένων, της Κατηγοριοποίησης 
 199
Κεφάλαιο 7 Συµπεράσµατα 
Κειµένων, της Υπολογιστικής Γλωσσολογίας, της Ανάκτησης ∆εδοµένων και της Εξόρυξης 
Πληροφορίας. 
7.2 Ανοιχτά θέµατα  
 
Παρά το γεγονός ότι τα παραπάνω µοντέλα επιλύουν µε το δικό τους τρόπο και στο δικό τους 
βαθµό το πρόβληµα το οποίο εξετάζει η εν λόγω διατριβή, καθένα από αυτά παρουσιάζει διαφορετικά 
σηµεία τα οποία πιθανώς δυσχεραίνουν την απόδοση του καθώς και άλλα τα οποία επιδέχονται 
εµπλουτισµό ή διόρθωση. 
 Πιο συγκεκριµένα, σε καθένα από τα κείµενα τα οποία χρησιµοποιήθηκαν για τη συγκριτική 
παράθεση της κατηγοριοποίησης µε χρήση εννοιών σε αντιδιαστολή µε αυτή που κάνει χρήση των 
αυτούσιων λέξεων του κειµένου, η έννοια που αντιστοιχεί σε κάθε λέξη αποδόθηκε µε τρόπο 
χειρωνακτικό, επιλέγοντας ανάµεσα στις πιθανές έννοιες όπως αυτές προκύπτουν από τον θησαυρό 
όρων του Wordnet. Το γεγονός αυτό είναι δυνατό να οφείλεται για τη χαµηλή απόδοση 
κατηγοριοποίησης των εννοιών έναντι των λέξεων. Ένας πιθανός εµπλουτισµός θα µπορούσε να 
αποτελέσει η εφαρµογή µιας µεθόδου αποσαφήνισης της έννοιας κάθε λέξης για τον έλεγχο της 
ορθότητας απόδοσης της έννοιας η οποία προέκυψε µε τον χειρωνακτικό τρόπο. Ένα άλλο ακόµα 
στοιχείο το οποίο πιθανώς ευθύνεται για την χαµηλή απόδοση κατηγοριοποίησης τόσο µε τις έννοιες 
όσο και µε τις λέξεις είναι η φύση των κειµένων και η κατανοµή τους σε κατηγορίες δεδοµένου ότι 
παρατηρείται µεγάλη οµοιότητα ανάµεσα στις διάφορες κατηγορίες. ∆υστυχώς, το εν λόγω σώµα 
κειµένων είναι το µοναδικό στη βιβλιογραφία στο οποίο παρέχεται η πληροφορία της έννοιας κάθε 
λέξης. 
 Η προσέγγιση η οποία αναπτύχθηκε στο δεύτερο µοντέλο, πιθανώς να οφείλει την απόδοσή της 
στο γεγονός ότι χρησιµοποίησε το σώµα κειµένων του Brown Corpus. Επιπρόσθετα, ένα στοιχείο το 
οποίο πιθανώς να επηρέασε την ληφθείσα απόδοση είναι η χρήση της µεθόδου κατηγοριοποίησης η 
οποία επιλέχθηκε. Ένας πιθανός εµπλουτισµός για την περαιτέρω αξιολόγηση της µεθόδου θα 
µπορούσε να αποτελέσει η κατασκευή περισσότερων κειµένων προς τµηµατοποίηση.  
 Το τρίτο και τελευταίο µας µοντέλο είναι αυτό το οποίο παρουσίασε την υψηλότερη απόδοση 
και εξετάστηκε διεξοδικότερα. Πάραυτα, ένα στοιχείο το οποίο περιορίζει την απόδοση 
τµηµατοποίησης αποτελεί το γεγονός ότι η συνάρτηση κόστους εξαρτάται από την πληροφορία του 
µέσου µήκους και της τυπικής απόκλισης από αυτό. 
 
 200
Κεφάλαιο 7 Συµπεράσµατα 
7.3 Μελλοντικές Κατευθύνσεις 
 
Οι µελλοντικές κατευθύνσεις της έρευνας αφορούν σε γενικές γραµµές: α) τη βελτίωση των 
ληφθέντων αποτελεσµάτων των διαφόρων µοντέλων, β) τον εµπλουτισµό των µοντέλων καθώς και 
την εξέταση εναλλακτικών προσεγγίσεων και γ) την ενσωµάτωση των εν λόγω µοντέλων σε άλλα για 
την επίλυση παρεµφερών προβληµάτων. 
 Πιο συγκεκριµένα, µια εναλλακτική προσέγγιση σε ότι αφορά το πρώτο µοντέλο µπορεί να 
αποτελέσει η απόδοση της έννοιας µε βάση των θησαυρό όρων του Wordnet αλλά και της εξέταση 
µιας ευρείας γκάµας µεθόδων αποσαφήνισης της έννοιας µιας λέξης. Η παραπάνω προσέγγιση σε 
συνδυασµό µε την εξέταση του εν λόγω µοντέλου και σε άλλα σώµατα κειµένων θα αποφαινόταν 
σχετικά  µε  την «δύναµη» των εννοιών αλλά και θα αναδείκνυε την ισχύ του αλγορίθµου µας. 
 Σε ότι αφορά το δεύτερο µοντέλο, θεωρούµε ότι οι δυνατότητές του δεν εξετάστηκαν διεξοδικά 
σε βαθµό τέτοιο ώστε να έχουµε πλήρη επίγνωση των δυνατοτήτων του αλλά και το βαθµό 
συνεισφοράς του. Η βαρύτητα του µοντέλου θα µπορούσε να αναδειχθεί µε την εξέταση αυτού σε 
διαφορετικά σώµατα κειµένων αλλά και την εναλλακτική εξέταση διαφορετικών µεθόδων 
κατηγοριοποίησης.  
 Τέλος, το τρίτο µοντέλο είναι δυνατό να βελτιωθεί και να εµπλουτιστεί µε διάφορους τρόπους. 
Ένας τρόπος είναι να µειώσουµε το βαθµό εξάρτησης της συνάρτησης κόστους τµηµατοποίησης από 
την πληροφορία του µέσου µήκους τµήµατος και της τυπικής απόκλισης από αυτό, γιατί ένας από τους 
µελλοντικούς µας στόχους είναι η αξιοποίηση του αλγορίθµου µας για την τµηµατοποίηση µεγάλων 
σε έκταση κειµένων. Μια άλλη δυνατότητα είναι να εισαγάγουµε στην κατασκευή του πίνακα 
οµοιότητας µεταξύ των προτάσεων κριτήρια αξιολόγησης των χαρακτηριστικών όρων όπως η 
Εντροπία για κείµενο ή να εφαρµόσουµε τεχνικές µείωσης της διάστασης των χαρακτηριστικών όρων 
µε σκοπό να κάνουµε τον εν λόγω πίνακα πιο «πυκνό». Μια ενδιαφέρουσα εναλλακτική λύση θα 
µπορούσε να είναι ο συνδυασµός κριτηρίων που εµφανίζονται στη γλωσσολογική προσέγγιση εύρεσης 
της δοµής ενός κειµένου όπως π.χ. cue words, παύσεις και άλλα, ως ένα συµπληρωµατικό βήµα 
επεξεργασίας των κειµένων πριν ή και κατά τη διάρκεια της κατασκευής του πίνακα οµοιότητας. 
Τέλος το εν λόγω µοντέλο είναι δυνατό να ενσωµατωθεί σε άλλα µοντέλα τα οποία επεξεργάζονται 
και τµηµατοποιούν video και ήχο σε πολυµεσικά κείµενα.  
Ένα από τα µελλοντικά σχέδιά µας αποτελεί η ενσωµάτωση µερικών ή και όλων των 
παραπάνω µοντέλων σε µηχανές αναζήτησης οι οποίες περιέχουν τόσο αγγλικά όσο και ελληνικά 
κείµενα, µε σκοπό µια εξελιγµένη προεπεξεργασία των κειµένων που περιέχονται σε αυτές και κατ’ 
επέκταση τη βελτίωση της ποιότητας της επιστρεφόµενης πληροφορίας.  
 201
Κεφάλαιο 7 Συµπεράσµατα 
 Τέλος, στα µελλοντικά µας σχέδια αποτελεί η εφαρµογή των µοντέλων µας για την εξαγωγή 
περιλήψεων και τη θεµατική κατηγοριοποίηση κειµένων καθώς και η περαιτέρω ενασχόληση µε την 
επεξεργασία ελληνικών κειµένων και την βελτίωση της απόδοσης τµηµατοποίησης σε αυτά, τόσο µε 
τη βοήθεια των µοντέλων που αναπτύξαµε όσο και µε τον συνδυασµό αυτών µε βελτιωµένες εκδόσεις 
αλγορίθµων που πραγµατοποιούν προεπεξεργασία των ελληνικών κειµένων όπως π.χ. ο 
Μορφοσυντακτικός Αναλυτής της Νέας Ελληνικής. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 202
 
 
 
 
 
 
203 
Βιβλιογραφικές Αναφορές 
ΒΙΒΛΙΟΓΡΑΦΙΚΕΣ ΑΝΑΦΟΡΕΣ 
A 
[Ahonen et al., 1997] Ahonen, H., Heikkinen, B., Heinonen, O. and Klemettinen, M. (1997). 
“Discovery of Reasonably-sized Fragments Using Inter-paragraph Similarities”. Technical Report C-
1997-67. University of Helsinki, Department of Computer Science.  
[Agirre & Martinez, 2000] Agirre, E. and Martinez, D. (2000). “Exploring automatic word sense 
disambiguation with decision lists and the Web”. In Procedings of the COLING 2000 Workshop on 
Semantic Annotation and Intelligent Content. 
B 
 [Baker & McCallum, 1998] Baker, D. and  McCallum, A. (1998) “Distributional Clustering of Words 
for Text Classification”. In Procedings of the SIGIR-98.  
[Barry & Hartigan, 1992]  Barry, D. and Hartigan J.A. (1992). “Product partition models for change 
point problems”. Annual  Of State, vol.20, pp.260-279. 
[Barry & Hartigan, 1993] Barry D. and Hartigan J.A. (1993). “A Bayesian analysis for change point 
problems”. JASA., vol.88, pp.309-319. 
[Beeferman et al., 1997(a)] Βeeferman, D., Berger, A. and Lafferty, J. (1997). “A model of lexical 
attraction and repulsion”. In Proceedings of the 35th Annual Meeting of the Association of 
Computational Linguistics, pp. 373-380, Madrid. 
[Beeferman et al., 1997(b)] Βeeferman, D., Berger, A. and Lafferty, J. (1997). “Text Segmentation 
using exponential models”. In Proceedings of the Second Conference on Empirical Methods in Natural 
Language Processing, pp. 35-46, Providence, Rhode Island. 
[Beeferman et al., 1999] Beeferman, D., Berger, A. and  Lafferty, J. (1999). “Statistical Models for 
Text Segmentation”. Machine Learning, Special Issue on Natural Language Processing, vol. 34(1-3), 
pp. 177-210, C. Cardie and R. Mooney (editors).  
 204
Βιβλιογραφικές Αναφορές 
[Bengio, 1998] Bengio, Y. (1998). “Markovian models for sequential data”. Neural Computaion  
Surveys, vol.2, pp.129-162.  
 [Bengio et al., 2000] Bengio, Y., Ducharme, R. and Vincent P. (2000). “A Neural Probabilistic 
Language Model”. Technical Report 1178, Universite de Montreal, Montreal, Quebec, Canada. 
 [Benkhalifa et al., 2001] Benkhalifa, M., Mouradi, A. and Bouyakhf, H. (2001). “Intergating External 
Knowledge to Supplement Training Data in Semi-Supervised Learning for Text Categorization”. 
Information Retrieval, vol. 4 (2), pp. 91-113. 
 [Berger et al., 1996] Berger, A.L., Della Pietra, S.A. and Della Pietra, V.J. (1996). “A maximum 
entropy approach to natural language processing”. Computational Linguistics, vol. 22, pp. 39-71. 
 [Birkhoff, 1967] Birkhoff, G. (1967). “Lattice Theory”. Providence, RI: American Mathematical 
Society, Colloquium Publications, vol. 25. 
[Blei & Moreno, 2001] Blei, D.M. and Moreno, P.J. (2001). “Topic segmentation with an aspect 
hidden Markov model”.  Technical Report. CRL 2001-07, COMPAQ Cambridge Research Lab. 
 [Breiman, 1994] Breiman, L. (1994). “Bagging predictors”. Technical Report 421, Dept. of Statistics, 
Univ. of California at Berkeley. 
 [Breiman et al., 1984] Breiman, L., Freidman, J.H., Olsen, R.A. and Stone, C.J. (1984). 
“Classification and Regression Trees”. Wadsworth International Group. 
C 
[Carletta,1994] Carletta, J. (1994). “ Assessing agreement on classification tasks: The kappa stastistic”. 
Computational Linguistics, vol. 22(2), pp. 249-254.  
[Carpenter et al., 1992] Carpenter, G.A., Grossberg, S., Markuzon, N., Reynolds, J.H. and Rosen., D.B. 
(1992). “Fuzzy ARTMAP: a neural network architecture for incremental supervised learning of analog 
multidimensional maps”. IEEE Transactions on Neural Networks,vol. 3, pp. 698-713. 
 [Caruana & Freitag, 1994] Caruana, R. and Freitag, D. (1994). “Greedy Attribute Selection.” In 
Proceedings of the 11th International Conference on Machine Learning , pp.26-28. 
[Choi, 2000] Choi, F.Y.Y. (2000). “Advances in domain independent linear text segmentation”. In 
Proceedings of the North American Chapter of the Association for Computational Linguistics, pp. 26-
33, Seattle, USA, May. ACL. 
 205
Βιβλιογραφικές Αναφορές 
[Choi et al., 2001] Choi, F.Y.Y., Wiemer-Hastings, P. and Moore, J. (2001). “Latent Semantic Analysis 
for Text Segmentation”. In Proceedings of the 6th Empirical Methods of Natural Language Processing, 
pp 109-117.  
 [Cohen, 1995(a)] Cohen, W.W. (1995). “Learning to Classify English Text with ILP Methods”.  In 
Proceedings of the Workshop on Inductive Logic Programming. Leuven. 
 [Cohen, 1995(b)] Cohen, W.W. (1995). “Text Categorization and Relational Learning”. In 
Proceedings of the 12th International Conference in Machine Learning , pp.124-132. 
 [Cohen & Singer, 1996] Cohen, W.W. and  Singer, Y. (1996). “Context-sensitive learning methods for 
text categorization”. In Proceedings of the SIGIR-96. 
 [Cohen, 1995(c)] Cohen, W.W. (1995). “Fast effective rule induction”. In Machine Learning 
Proceedings of the Twelfth international Conference, Lake Taho, California, Morgan Kaufman.  
 [Cohen, 1996] Cohen, W.W. (1996). “Learning Rules that Classify E-Mail”. In the 1996 AAAI Spring 
Symposium on Machine Learning in Information Access.  
 [Cohen & Hirsh, 1998] Cohen, W.W. and Hirsh, H. (1998). “Joins that generalize: Text classification 
using WHIRL”. In Proceedings of the Fourth Int. Conference on Knowledge Discovery and Data 
Mining. 
 [Craven et al., 1998] Craven, M.., DiPasquo, D., Freitag, D., McCallum, A., Mitchell, T., Nigam, K. 
and  Slattery, S. (1998). “Learning to Extract Knowledge from the World Wide Web”. In AAAI-98: 
Proceedings of the Fifteenth National Conference on Artificial Intelligence. 
  [Crowley, 1997] Crowley, E.M. (1997). "Product partition  models for normal means". JASA, vol.92, 
pp.192-198. 
D 
[Deerwster et al., 1990] Deerwster, S., Dumais, S.T., Furnas, G.W., Landawer, T.K. and Harshman R. 
(1990). “Indexing by Latent Sematic Analysis”. In Jamer Soc. Inf. Sci. vol.1,6, pp. 391-407. 
 [Della Pietra et al., 1997] Della Pietra, S.A., Della Pietra, V.J. and  Laferty, J. (1997). “Inducing 
features of random fields”. IEEE Transactions on Pattern Analysis and Machine Intellligence, vol. 19. 
 [Domingos & Pazzani, 1997] Domingos, P. and Pazzani, M. (1997). “On the optimality of the simple 
Bayesian classifier under zero-one loss”. Machine Learning, vol. 29, pp. 103-130. 
 206
Βιβλιογραφικές Αναφορές 
 [Dowis, 1991] Dowis. L. (1991). “Handbook of genetic Algorithms”, Van Nostrand, N.York. 
 [Drucker et al., 1999] Drucker, H., Wu, D. and  Vapnik, V. (1999). “Support Vector machines for Span 
Categorization”. IEEE Transactions on Neural Networks, Vol.10, No.5, September 1999. 
[Duda et al., 2001] Duda, R.O., Hart, P.E., and Stork, D.G. (2001).  “Pattern classification”. New York, 
NY: John Wiley & Sons. 
E  
 [Escudero et al., 2000] Escudero, G., Marquez, L., and Rigau, G. (2000). “A comparison between 
supervised learning algorithms for word sense disambiguation”. In Proceedings of the 4th Conference 
on Computational Natural Language Learning, CoNLL'2000, pp. 31-36. 
F 
[Fragkou, 2003] Fragkou Pavlina. (2003). “A Dynamic Programming Algorithm for the Segmentation 
of Greek Texts”. In Proceedings of CONSOLE XII (International Conference of Graduate Students), 
December 12-14, 2003, Patra Greece. 
[Fragkou et al., 2004 (a)] Fragkou, P., Petridis, V. and Kehagias, A. (2004). “A Dynamic Programming 
Algorithm for Linear Text Segmentation”. Το appear in Journal of Intelligent Information Systems, 
Kluwer Academic Publishers. 
[Fragkou et al., 2004 (b)] Fragkou, P., Petridis, V. and Kehagias, A. (2004). “Linear Text Segmentation 
of Greek Texts using a Dynamic Programming Algorithm”. (submitted). 
[Francis & Kucera, 1982] Francis, W.N. and Kucera, H. (1982). “Frequency Analysis of English 
Usage: Lexicon and Grammar”. Boston, MA: Houghton Mifflin Company. 
G 
[Gakis et al., 1999] Γάκης, Π., Ορφανός, Γ. και Ιορδανίδου, A. (1999). “Υπολογιστική Επεξεργασία 
της Νέας Ελληνικής: Καταγραφή της Μορφοσυντακτικής Ασάφειας”. Γλώσσα, τεύχος 49, Εκδόσεις 
Νέας Παιδείας. 
[Grosz & Sidner, 1986] Grosz, B.J. and Sidner, C.L. (1986). “Attention, Intentions and the structure of 
discourse”. Computational Linguistics, vol. 12(3), pp. 174-204. 
 207
Βιβλιογραφικές Αναφορές 
H 
[Halliday & Hasan, 1976] Halliday, M. and Hasan, R. (1976). “Cohesion in  English”. Longman 
Group, New York. 
 [Hearst, 1993] Hearst, M..A. (1993). “TextTiling: A quantitative approach to discourse segmentation”. 
Technical Report 93/24, University of California, Berkeley. 
 [Hearst, 1994(a)] Hearst, M.A. (1994). “Context and Structure in Automated Full-Text Information 
Access”. Phd Thesis, University of California, Berkeley. 
[Hearst, 1994(b)] Hearst, M.A. (1994). “Multi-paragraph segmentation of expository texts”. In 
Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistic, pp. 9-16, 
Las Cruses, New Mexico. 
[Hearst & Plaunt, 1993] Hearst, M.A. and Plaunt, C. (1993). “Subtopic structuring for full-length 
document access”. In Proceedings of the Special Interest Group on Information Retrieval, pp. 59-68. 
 [Hearst, 1992] Hearst, M.A. (1992). “Automatic acquisition of hyponyms from large text corpora”. In 
Proceedings of the Fourteenth International Conference on Computational Linguistics, pp. 539-545, 
Nantes, France. 
[Heinonen, 1998] Heinonen,O. (1998). “Optimal Multi-Paragraph Text Segmentation by Dynamic 
Programming”. In Proceedings of COLING-ACL’98.  
[Hirschberg & Litman, 1993] Hirschberg, J. and Litman, D. (1993). “Empirical studies on the 
disambiguation and cue phrases”. Computational Linguistics, vol. 19(3), pp. 501-530. 
[Holland, 1975] Holland, J.H. (1975). “Adaptation in Natural and Artificial Systems”. Ann Arbor, 
Michigan: The University of Michigan Press. 
 [Huynh et al., 1998] Huynh, Q.Q., Cooper, L.N., Intrator, N., and Shouval, H. (1998). “Classification 
of underwater mammals using feature extraction based on time frequency analysis and BCM theory”. 
IEEE Transactions on Signal Processing, vol. 46, pp. 1202-1207. 
 208
Βιβλιογραφικές Αναφορές 
I 
J 
[Joachims, 1997] Joachims, T. (1997). “A Probabilistic Analysis of the Rocchio Algorithm  with 
TFIDF for Text Categorization”. In Proceedings of the 14th International Conference on Machine 
Learning ICML97, pp. 143-151. 
 [Joachims et al., 1997] Joachims, T., Freitag, D. and Mitchell, T. (1997). “Webwatcher: a tour guide 
for the World Wide Web”. In Proceedings of the International Joint Conference on Artificial 
Intelligence. 
 [Joachims, 1998] Joachims, T. (1998). “Text categorization with support vector machines: learning 
with many relevant features”. In Proceedings of European Conference on Machine Learning. 
K 
[Kaburlazos &  Petridis, 2000] Kaburlazos, V.G. and Petridis, V. (2000). “Fuzzy Lattice 
Neurocomputing (FLN) models”, Neural Networks, vol. 13, pp. 1145-1170. 
[Kan et al., 1998] Kan, M., Klavans, J.L. and McKeown, K. R. (1998). “Linear Segmentation and 
Segment Significance”. In Proceedings of the 6-th International Workshop of Very Large Corpora 
(WVLC-6), pp. 197-205, Montreal, Quebec, Canada. 
[Kan, 2001] Kan, Min-Yen. (2001). “Combining visual layout and lexical cohesion features for text 
segmentation”.Technical Report CUCS-002-01, Department of Computer Science, Columbia 
University. 
[Kehagias et al., 2003 (a)] Kehagias, A., Fragkou, P., Petridis, V. (2003). “Linear Text Segmentation 
using a Dynamic Programming Algorithm”. In Proceedings of the Eacl’03, April 12-17, 2003, 
Budapest, Hungary, pp. 171-178. 
[Kehagias et al., 2003 (b)] Kehagias, A., Petridis, V., Kaburlasos, V.G. and Fragkou, P. (2003). “A 
Comparison of Word- and Sense-based Text Categorization Using Several Classification Algorithms”. 
Journal of Intelligent Information Systems, Kluwer Academic Publishers. Vol 21. No.3, November 
2003, pp. 227-248. 
 209
Βιβλιογραφικές Αναφορές 
[Kehagias et al., 2004]  Kehagias, A., Nicolaou, A., Fragkou, P. and Petridis, V. (2004). “Text 
Segmentation by Product Partition Models and Dynamic Programming”. Mathematical and Computer 
Modelling, vol. 39, pp. 209-217. 
[Koller & Sahami, 1997] Koller, D. and Sahami, M. (1997). “Hierarchically Classifying Documents 
Using Very Few Words”. In Proceedings of the 14th Int. Conference on Machine Learning, pp. 170-
178. 
[Kozima, 1993] Kozima, H. (1993). “Text Segmention based on similarity between words”. In 
Proceedings of the 31st Annual Meeting of the Association for Computational Liguistics, Student 
Session,  pp. 286-288. 
[Kozima & Furugori, 1993] Kozima, H. and Furugori, T. (1993). “Similarity between words computed 
by spreading activation on an English dictionary”. In Proceedings of the European Association for 
Computational Linguistics, pp. 232-239. 
[Kozima & Furugori, 1994] Kozima, H. and Furugori, T. (1994). “Segmenting narrative text into 
coherent scenes”. Literary and Linguistic Computing, vol. 9(1), pp. 13-19. 
L 
 [Lewis, 1992] Lewis, D.D. (1992). “Representation and Learning in Information Retrieval”. Ph.D. 
Thesis, Dept. of Computer Science, Univ. Of Massachussets. 
[Lewis et al., 1996] Lewis, D.D., Schapire, R.E., Callan, J.P., and Papka, R. (1996). “Training 
Algorithms for Linear Text Classifiers”. Research and Development in Information Retrieval, pp. 298-
306. 
[Lewis & Jones, 1996] Lewis, D.D. and Jones, K.S. (1996). “Natural Language Processing for 
Information Retrieval”. Communications of the ACM, vol. 39, pp. 92-101. 
[Loschi & Cruz, 2002] Loschi, R.H. and Cruz, F.R.B. (2002). “An analysis of the influence of some 
prior specifications in the identification of change points via product partition model”. Comput. Statist. 
Data Anal., vol.39, pp. 477-501. 
 210
Βιβλιογραφικές Αναφορές 
M 
[McCallum & Nigam, 1998 (a)] McCallum, A. and  Nigam, K. (1998). “A Comparison of Event 
Models for Naive Bayes Text Classification”. In AAAI-98 Workshop on "Learning for Text 
Categorization".  
[McCallum & Nigam, 1998 (b)] McCallum, A. and  Nigam, K. (1998). “Employing EM in Pool-Based 
Active Learning for Text Classification”. In Proceedings of ICML-98.  
[Mani & Bloedorn, 1997] Mani, I. and Bloedorn, E. (1997). “Multi-Document Summarization by 
Graph Search and Matching”. In Proceedings of the Fourteenth National Conference on Artificial 
Intelligence and Ninth Innovative Applications of Artificial Intelligence Conference, AAAI ’97, 
IAAI’97, Providence, Rhode Island. 
[Mann & Thimson, 1987] Mann, W.  and Thomson, S.A. (1987). “Rhetorical structure theory: A theory 
of text organization”. Technical Report ISI/RS 87-190, ISI.  
[Manning & Schuetze, 1999] Manning, C.D. and Schuetze, H. (1999). “Foundations of Statistical 
Natural Language Processing”.  MIT Press. 
[Masand et al., 1992] Masand, B., Linoff, G. and Waktz, D. (1992). “Classifying news stories using 
memory based reasoning”. In SIGIR 92, pp. 59-65. 
[Miller et al., 1990] Miller, G.A., Beckwith, R.,  Fellbaum, C., Gross, D., and Miller, K.J. (1990). 
“Introduction to WordNet: an on-line lexical database”. International Journal of Lexicography, vol. 3, 
pp. 235 - 244. 
[Mitchell, 1997] Mitchell, T.M. (1997). “Machine Learning”. The McGraw-Hill Companies, Inc. 
[Mittendorf & Schuble, 1996] Mittendorf, E. and Schuble, P. (1996). “Document and passage retrieval 
based on hidden Markov models”. In Proceedings of the 19th Annual International of Association of 
Computer Machinery - Special Interest Group on Information Retrieval (ACM / SIGIR) Conference on 
Research and Development in Information Retrieval, pp. 318-327. 
[Mladenic, 1998] Mladenic, D. (1998). “Machine learning of non-homogeneous distributed text data”. 
Ph.D. dissertation, Dept. of Computer and Information Science, Univ. of Ljubljana. 
[Morris, 1998] Morris, J. (1988). “Lexical cohesion, the thesaurus and the structure of the text”. 
Technical Report CSRI-219, Computer Systems Research Institute, Univerity of Toronto. 
 211
Βιβλιογραφικές Αναφορές 
[Morris & Hirst, 1991] Morris, J. and Hirst, G. (1991). “Lexical cohesion computed by thesaural 
relations as an indicator of the structure of text”. Computational Linguistics, vol. 17(1), pp. 21-42. 
N 
[Ng et al., 1997] Ng, H.T., Goh, W.B. and Low, K.L. (1997). “Feature Selection Perceptron Learning 
and a Usability Case Study for Text Categorization”. In 20th Annual International ACM SIGIR 
Conference on Research and Development in Information Retrieval (SIGIR’97), pp. 67-73. 
[Nigam et al., 1999] Nigam, K., Lafferty, J. and  McCallum, A. (1990). “Using Maximum Entropy for 
Text Classification”. In IJCAI'99 Workshop on Information Filtering.  
[Nigam et al., 1998] Nigam, K., McCallum, A. and Trun. (1998). “Learning to Classify Text from 
Labeled and Unlabeled Documents”. In AAAI-98. (Longer version draft accepted to the Machine 
Learning Journal.)  
[Noussia & Orphanos, 1999] Noussia T. and Orphanos G. (1999). “Detection and Correction of Ill – 
Formed  Text in a Highly Inflectional Language”. Accepted in ACM Transactions on Information 
Systems.  
O 
[Orphanos & Christodoulakis, 1999] Orphanos G. and Christodoulakis D. (1999). “Part-of-speech 
Disambiguation and Unknown Word Guessing with Decision Trees”. In the Proceedings of EACL’99, 
Bergen, Norway. 
[Orphanos & Tsalidis, 1999] Orphanos G. and Tsalidis C. (1999). “Combining handcrafted and coprus 
– acquired Lexical Knowledge into a Morphosyntactic Tagger”. In the Proceedings of the 2nd Research 
Colloquium for Computational Linguistics in United Kingdom (CLUK), Essex, UK.  
[Orphanos et  al., 1999 (a)] Orphanos G., Kalles D., Papagelis A. and Christodoulakis D. (1999). 
“Decision Trees and Natural Language Processing: A case study in Part-of-Speech Tagging”. In the 
Proceedings of ACAI99 Workshop on Machine Learning in Human Language Technology, Chania, 
Greece.  
[Orphanos et  al., 1999(b)] Ορφανός Γ., Γάκης, Π. και Ιορδανίδου Α. (1999). “Μορφοσυντακτική 
ασάφεια στη νέα ελληνική: η περίπτωση επιθέτου - ουσιαστικού – επιρρήµατος”. Πρακτικά της 20ης 
Συνάντησης του Τοµέα Γλωσσολογίας του Τµήµατος Φιλολογίας, Θεσσαλονίκη, Ελλάδα. 
 212
Βιβλιογραφικές Αναφορές 
[Orphanos, 2000] Ορφανός, Γ. (2000). ∆ιδακτορική ∆ιατριβή µε τίτλο “Υπολογιστική 
Μορφοσυντακτική Ανάλυση της Νέας Ελληνικής”. Τµήµα Μηχανικών Ηλεκτρονικών Υπολογιστών 
και Πληροφορικής,  Πολυτεχνική Σχολή Πανεπιστηµίου Πατρών.  
P 
[Passoneau & Litman, 1993] Passoneau, R. and Litman, D.J. (1993). “Intention – based segmentation: 
Human reliability and correlation with linguistic cues”. In Proceedings of the 31st Meeting of the 
Association for Computational Liguistics, pp. 148-155. 
[Passoneau & Litman, 1994] Passoneau, R. and Litman, D. J. (1996). “Empirical analysis of three 
dimensions of spoken discourse: Segmentation, coherence and linguistic devices”.  In Hovy, E.H. and 
Scott, D., R., editors, Computational and Conversational Discourse: Burning Issues- An 
Interdisciplinary Account,  chapter 7, pp. 161-194. Springer Verlag, Berlin. 
[Petridis et al., 2001] Petridis, V., Kaburlazos, V.G., Fragkou, P. and Kehagias, A. (2001). “Text 
Classification Using the σ-FLNMAP Neural Network”. In Proceedings of the 2001 International Joint 
Conference on Neural Networks (IJCNN'2001), Washington D.C. 
[Petridis & Kaburlazos, 1999] Petridis, V. and Kaburlazos, V.G. (1999). “Learning in the Framework of 
Fuzzy Lattices”. IEEE Transactions on Fuzzy Systems, vol. 7, pp. 422-440. Errata in (2000) IEEE 
Transactions on Fuzzy Systems, vo. 8, pp. 236. 
[Petridis & Kaburlazos, 2000] Petridis, V. and Kaburlazos, V.G. (2000). “An Intelligent Mechatronics 
Solution for Automated Tool Guidance in the Epidural Surgical Procedure”. In Proceedings 7th 
Conference on Mechatronics and Machine Vision in Practice (M2VIP'00), pp. 201-206,  Hervey Bay, 
Queensland, Australia. 
[Petridis & Kaburlazos, 2001] Petridis, V. and Kaburlazos, V.G. (2001). “Clustering and Classification 
in Structured Data Domains Using Fuzzy Lattice Neurocomputing (FLN)”. IEEE Transactions on 
Knowledge and Data Engineering, 13(2), pp. 245-260. 
[Petridis & Kehagias, 1996] Petridis, V.  and Kehagias, A. (1996). “Modular Neural Networks for 
Bayesian Classification of Time Series and the Partition Algorithm”. IEEE Trans. on Neural Networks, 
vol. 7, pp. 73-86. 
[Petridis & Kehagias, 1998] Petridis, V. and Kehagias, A. (1998). “Predictive Modular Neural 
Networks.” Time Series Applications, Kluwer. 
 213
Βιβλιογραφικές Αναφορές 
[Petridis & Kaburlazos, 1998] Petridis V. and Kaburlazos, V.G. (1998). “Fuzzy lattice neural network 
(FLNN): a hybrid model for learning”. IEEE Transactions on Neural Networks, vol. 9, pp. 877-890. 
[Philips, 1985] Philips, M. (1985). “Aspects of Text Structure: An Investigation of the Lexcal 
Organisation of Text”. North Holland Linguistic Series. North Holland, Amsterdam. 
[Ponte & Croft, 1997] Ponte, J.M. and Croft, W.B. (1997). “Text Segmentation by topic”. In European 
Conference on Digital Libraries, pp. 113-125, Pisa, Italy. 
[Porter, 1980] Porter, M.F. (1980). “An algorithm for suffix stripping”. Program 14,3, pp. 130-137. 
Q 
[Quinlan, 1986] Quinlan, J.R. (1986). “Induction of Decision Trees”. Machine Learning, vol.1, pp. 81-
86. Reprinted in Shavlik and Dietterich(eds). Readings in Machine Learning.  
[Quinlan, 1993] Quinlan, J.R. (1993). “C4.5: Programs for Machine Learning”. Morgan Kaufman 
Publishers. 
R 
[Rabiner, 1989] Rabiner, L.R. (1989). “A tutorial on Hidden Markov Models and selected applications 
in speech recognition”. Proc of the IEEE, vol.77, pp. 257-285. 
 [Raskin & Weiser, 1987]  Raskin, V. and Weiser, I. (1987). “Language and writing : Applications of 
linguistics to rhetoric and composition”. Norwood, New Jersey: ABLEX: Publishing Corporation.  
[Reynar, 1994] Reynar, J.C. (1994). “An automatic method of finding topic boundaries”. In 
Proceedings of the 32nd Annuan Meeting of the Association for Computational Linguistics, Student 
Session, pp. 331-333, Las Cruces, New Mexico. 
[Reynar, 1998] Reynar, J.C. (1998). “Topic Segmentation: Algorithms and Applications”. Phd Thesis, 
Philadelphia.  
[Reynar & Ratnaparkhi, 1997] Reynar, J.C. and Ratnaparkhi, A. (1997). “A maximum entropy 
approach to identifying sentence boundaries”. In Proceedings of the Fifth Conference on Applied 
Natural Language Processing, pp. 16-19, Washington, D.C. 
 214
Βιβλιογραφικές Αναφορές 
[Richmond et al., 1997] Richmond, K., Smith, A., and Amitay, E. (1997). “Detecting subject 
boundaries within text: A language independent statistical approach”. In Exploratory Methods in 
Natural Language Processing, pp. 47-54, Providence, Rhode Island. 
[Rocchio, 1971] Rocchio, J. (1971). “Relevance Feedback in Information Retrieval”. In Salton, G., 
editor, the SMART Retrieval System: Experiments in Automatic Document Processing, pp. 313-323. 
Prentice Hall, Inc., Englewood Cliffs, New Jersey.  
[Rodriguez et al., 1997] Rodriguez, Manuel de Buenaga, Gomez – Hidalgo, Jose Maria and Diaz-
Agudo, Belen. (1997). “Using Wordnet to complement training information in text categorization”. In 
RANLP-97, pp. 150-157. 
[Roget, 1911] Roget, P.M. (1911). “Roget’s International Thesaurus”. Cromwell, New York, first 
edition.  
[Roget, 1977] Roget, P.M. (1977). “Roget’s International Thesaurus”. Harper and Row, New York, 
fourth edition. 
S 
 [Sahami et al., 1998] Sahami, M., Dumais, S., Heckerman, D. and Horvitz, E. (1998). “A Bayesian 
approach to filtering junk e-mail”. In AAAI'98 Workshop on Learning for Text Categorization, July 27, 
1998, Madison, Wisconsin. 
 [Sahami et al., 1996] Sahami, M., Hearst, M. and Saund, E. (1996). “Applying the multiple cause 
mixture model to text categorization”. In Proceedings of the 13th International Conference in Machine 
Learning , pp. 435-443. 
[Sardinha, 1993] Sardinha, B. (1993). “Lexis in annual reports: Text Segmentation and lexical 
threads”. Technical Report 8, Development of International Research in English for Commerce and 
Technology. 
[Sardinha, 1999] Sardinha, B. “Looking at discourse in a corpus: The role of lexical cohesion”. In 
Proceedings of AILA ’99, Tokyo, Japan.   
[Scott & Matwin, 1999] Scott, S. and Matwin, S. (1999). “Feature Engineering for Text 
Classification”. In Proceedings of ICML-99, 16th International Conference on Machine Learning. 
 215
Βιβλιογραφικές Αναφορές 
[Stamatatos et  al., 1999] Stamatatos, E., Fakotakis, N. and Kokkinakis G. (1999). “Automatic 
Extraction of Rules for Sentence Boundary Disambiguation”. In Proceedings of ACAI99 Workshop on 
Machine Learning in Human Language Technology, Chania, Greece. 
T 
[Tsalidis & Orphanos, 1995] Tsalidis, C. and Orphanos, G. (1995). “Word Description Languages”. In 
Proceedings of the 1st Workshop in Natural Language Processing. Athens Greece. 
U 
[Utiyama & Isahara, 2001] Utiyama, M. and Isahara, H.(2001). “A statistical model for domain – 
independent text segmentation”. In Proceedings of the ACL’2001, Toulouse, France. 
V 
 [Vapnik, 1995] Vapnik, V. (1995). “The nature of Statistical Learning Theory”. Springer, New York.. 
W 
[Wiener et al., 1995] Wiener, E., Pedersen, J.O. and Weigend, A.S. (1995). “A Neural Network 
approach to Topic Spotting”. In Proceedings of the Fourth Annual Symposium on Document Analysis 
and Information Retrieval (SDAIR’95). 
X 
[Xu & Croft, 1996] Xu, J. and Croft, W.B. (1996). “Query expansion using local and global document 
analysis”. In Proceedings of the Nineteenth Annual International ACM SIGIR Conference on Research 
and Development in Information Retrieval, pp. 4-11, Zurich, Switzerland. 
Y 
[Yaari, 1997] Yaari, Y. (1997). “Segmentation of expository texts by hierarchical agglomerative 
clustering”. In Proceedings of Recent Advances in Natural Language Processing, Bulgaria.   
 216
Βιβλιογραφικές Αναφορές 
[Yaari, 1999] Yaari, Y. (1999). “Intelligent exploration of expository texts”. Phd thesis. Bar-Ilan 
University, Ramat-Gan, Israel.  
[Yamron et al., 1998] Yamron, J.P., Carp, I., Lowe, S. and  van Mulbregt, P. “A hidden Markov model 
approach to text segmentation and event tracking”. In Proceedings. of ICASSP-98. 
[Yang & Liu, 1999] Yang, Y. and Liu, X. (1999). “A re-examination of text categorization methods”. In 
Proceedings of the 22nd ACM SIGIR Conference on Research and Development in Information 
Retrieval, pp. 42-49. 
[Yang, 1995] Yang, Y. (1995). “Noise reduction in a statistical approach to text categorization”. In 
SIGIR 95, pp. 256-263. 
[Yang, 1998] Yang, Y. (1998). “An evaluation of statistical approaches to text categorization”. 
Information Retrieval.  
[Youmans, 1990] Youmans, G. (1990). “Measuring lexical style and competence: The type-token 
vocabulary curve”. Style, vol. 24, pp. 584-599. 
[Youmans, 1991] Youmans, G. (1991). “A new tool for discourse analysis: The vocabulary 
management profile”. Language, 67(4):763-789. 
 
 
 
 
 
 
 
 
 
 
 
 217
 
 
 
 
 
 
 
 
218 
Γλωσσάρι 
ΓΛΩΣΣΑΡΙ 
[Gloss(00001)] 10-ford cross validation 10-πλή  επαλήθευση 
[Gloss(00002)] Artificial Neural Networks Τεχνητά Νευρωνικά ∆ίκτυα 
[Gloss(00003)] Back propagation ∆ιάδοση προς τα πίσω  
[Gloss (00004)] Backtracking Οπισθοδρόµηση 
[Gloss (00005)] Bagging and boosting Συσσώρευση και προώθηση 
[Gloss (00006)] Bag-of-meanings Οµάδα εννοιών  
[Gloss (00007)] Bag-of-words Οµάδα λέξεων  
[Gloss (00008)] Bayesian inference procedure Bayesian διαδικασία εξαγωγής συµπεράσµατος 
[Gloss (00009)]  Bayesian network classifier Κατηγοριοποιητής Bayesian δικτύου   
[Gloss (00010)] Bias Πόλωση 
[Gloss (00011)] Breeding Αναπαραγωγή 
[Gloss (00012)] Cluster Οµάδα 
[Gloss (00013)] Clustering Συσσωµάτωση  
[Gloss (00014)] Coherence Συνάφεια 
[Gloss (00015)] Cohesion Συνοχή 
[Gloss (00016)] Computational Linguistics Υπολογιστική Γλωσσολογία 
[Gloss (00017)] Content analysis Ανάλυση περιεχοµένου 
[Gloss (00018)] Cue word or phrase Λέξη ή πρόταση «σινιάλο» 
[Gloss (00019)] Data Processing Επεξεργασία ∆εδοµένων 
[Gloss (00020)]  Decision node Κόµβος απόφασης 
[Gloss (00021)] Decision tree ∆έντρο απόφασης 
[Gloss (00022)] Default Προεπιλεγµένη  
[Gloss (00023)] Digital Libraries Ψηφιακές Βιβλιοθήκες 
 219
Γλωσσάρι 
[Gloss (00024)]  Direct acyclic graph  Ακυκλικός γράφος 
[Gloss (00025)] Distance probability distribution Πιθανοτική κατανοµή απόστασης 
[Gloss (00026)] Divisive clustering ∆ιαιρετική οµαδοποίηση 
[Gloss (00027)] Document retrieval Ανάκτηση κειµένων 
[Gloss (00028)] Don’t care Εξ’ ορισµού τιµή 
[Gloss (00029)] Feature Χαρακτηριστικός όρος 
[Gloss (00030)] Fixed Neighbors Καθορισµένοι Γείτονες 
[Gloss (00031)] Fixed parameter Συγκεκριµένης τιµής παραµέτρου  
[Gloss (00032)] Fuzzy inclusion Ασαφής εγκλεισµός  
[Gloss (00033)] Gradient descent Μέγιστη κλίση  
[Gloss (00034)] Hypernym Υπερώνυµο 
[Gloss (00035)] Hyperplane Υπερεπίπεδο 
[Gloss (00036)] Information needs Ανάγκες πληροφορίας 
[Gloss (00037)] Information Retrieval Ανάκτηση Πληροφορίας 
[Gloss (00038)] Information sources Πηγές πληροφορίας 
[Gloss (00039)] Instance Based Classifier Κατηγοριοποιητής βασισµένος σε στιγµιότυπα 
[Gloss (00040)] Instance Based System Σύστηµα βασισµένο σε παραδείγµατα 
[Gloss (00041)] Intentional state ∆οµή πρόθεσης 
[Gloss (00042)] Inter-annotator agreement Συµφωνία µεταξύ των σχολιαστών 
[Gloss (00043)] Internet ∆ιαδίκτυο 
[Gloss (00044)] Κ-Nearest Neighbors Κ-Πλησιέστερων Γειτόνων  
[Gloss (00045)] Key phrase extraction Εξαγωγή φράσεων - κλειδιών 
[Gloss (00046)] Key phrases Φράσεις-Κλειδιά 
[Gloss (00047)] Keyword Λέξη – Κλειδί  
[Gloss (00048)] Large text collections Μεγάλες συλλογές κειµένων 
[Gloss (00049)] Lattice ∆ικτυωτό 
[Gloss (00050)] Length model Μοντέλο µήκους 
 220
Γλωσσάρι 
[Gloss (00051)] Lexical relations Γλωσσολογικές  σχέσεις  
[Gloss (00052)] Linear discriminant learning
algorithms 
 Αλγόριθµος εκµάθησης γραµµικού διαχωρισµού
[Gloss (00053)] Local Content Analysis Τοπικής Ανάλυσης Περιεχοµένου 
[Gloss (00054)] Machine Learning Μηχανική Μάθηση 
[Gloss (00055)] Manually Χειρωνακτικά  
[Gloss (00056)] Margin Περιθώριο 
[Gloss (00057)] Maximum Likelihood
Classification  
 Κατηγοριοποίηση Μέγιστης Πιθανοφάνειας   
[Gloss (00058)] Maximun a Posteriori (MAP)
Classification 
 Κατηγοριοποίηση Μέγιστης Ύστερης 
Πιθανότητας 
[Gloss (00059)] Mutation Μετάλλαξη 
[Gloss (00060)] N-gram Ν-γραµµα 
[Gloss (00061)] Noun phrases Ονοµατικές φράσεις 
[Gloss (00062)] Positive valuation function Συνάρτηση θετικής αξιολόγησης 
[Gloss (00063)] Posterior Ύστερη 
[Gloss (00064)] Posterior cohesions Μεταγενέστερη συνάφεια – συνοχή 
[Gloss (00065)] Predicted class Προβλεπόµενη κατηγορία 
[Gloss (00066)] Prior cohesion Προγενέστερη συνάφεια – συνοχή 
[Gloss (00067)] Product Partition Models Μοντέλα ∆ιαχωρισµού Γινοµένου 
[Gloss (00068)] Production Rule Κανόνας παραγωγής-απόφασης 
[Gloss (00069)] Pruning   «Κλάδεµα», αποκοπή 
[Gloss (00070)] Query Ερώτηµα , άντληση πληροφοριών 
[Gloss (00071)] Rank in Local Context Σειρά σε Τοπικό Επίπεδο 
[Gloss (00072)] Rank position Αριθµηµένη θέση 
[Gloss (00073)] Salience Περίοπτη θέση 
[Gloss (00074)] Saturated Κορεσµένος 
[Gloss (00075)] Scaling ∆ιαβάθµιση  
 221
Γλωσσάρι 
[Gloss (00076)] Scaling parameter Παράµετρος κλιµάκωσης 
[Gloss (00077)] Search Engine Μηχανή Αναζήτησης  
[Gloss (00078)] Semantic Concordance Αλφαβητικό Ευρετήριο Σηµασιών 
[Gloss (00079)] Sliding window Κινούµενο παράθυρο 
[Gloss (00080)] Smooth out Εξοµαλύνω αρνητικά  
[Gloss (00081)] Spreading activation Ενεργοποίηση διάδοσης 
[Gloss (00082)] Stemmed Key phrases Φράσεις-Κλειδιά που προκύπτουν µετά από την 
αφαίρεση της κατάληξης των λέξεων 
[Gloss (00083)] Stemmed Noun phrases Ονοµατικές φράσεις που προκύπτουν µετά από 
την αφαίρεση της κατάληξης των λέξεων  
[Gloss (00084)] Stemmed words Λέξεις που προκύπτουν µετά από την αφαίρεση 
της κατάληξης 
[Gloss (00085)] Strength ∆ύναµη 
[Gloss (00086)] Supervised clustering Συσσωµάτωση µε επίβλεψη  
[Gloss (00087)] Synergetic Συνεργαστικό 
[Gloss (00088)] Synsets Οµάδες συνωνύµων 
[Gloss (00089)] Taxomomy, directory Ταξινοµία 
[Gloss (00090)] Taxon Μονάδα ταξινόµησης 
[Gloss (00091)] Text Classification Κατηγοριοποίηση Κειµένων 
[Gloss (00092)] Text Segmentation Τµηµατοποίηση κειµένων 
[Gloss (00093)] Τhesaurus Θησαυρός όρων 
[Gloss (00094)] Topic headings Θεµατικές επικεφαλίδες 
[Gloss (00095)] Topics Θεµατική κατηγορία 
[Gloss (00096)] Traversal Εγκάρσια διάβαση 
[Gloss (00097)] Utterances Λέξη ή έκφραση 
[Gloss (00098)] Validated Επικύρωσης– επιβεβαίωσης, επικυρωµένη-
επαληθευµένη 
[Gloss (00099)] Variable Neighbors Μεταβλητοί Γείτονες 
[Gloss (00100)]  Vector space model Μοντέλο διανυσµατικού χώρου 
 222
Γλωσσάρι 
[Gloss (00101)] Weighted Σταθµισµένος  
[Gloss (00102)] Weighted summation Σταθµισµένο άθροισµα 
[Gloss (00103)] Word Boolean document vector ∆υαδικό Άνυσµα Κειµένου Λέξεων 
[Gloss (00104)] Word Frequency  document vectorΆνυσµα Κειµένου Συχνοτήτων Λέξεων 
[Gloss (00105)] Word–sense disambiguation Αποσαφήνιση - προσδιορισµός της έννοιας µιας 
λέξης 
 
 
 
 
 
 
 
 
 223
 
 
224 
Παράρτηµα 
ΠΑΡΑΡΤΗΜΑ 
 
Α1 H stop list για τα αγγλικά κείµενα 
a c r y h o w e v e r o t h e r t h i s
a b o u t d e h u n d r e d o t h e r s t h o s e
a b o v e d e s c r i b e i o t h e r w is e t h o u g h
a c r o s s d e t a i l i e o u r t h r e e
a f t e r d o i f o u r s t h r o u g h
a f t e r w a r d s d o n e in o u r s e l v e s t h r o u g h o u t
a g a in d o w n in c o u t t h r u
a g a in s t d u e in d e e d o v e r t h u s
a l l d u r i n g i n t e r e s t o w n t o
a lm o s t e a c h in t o p a r t t o g e t h e r
a lo n e e g i s p e r t o o
a lo n g e ig h t i t p e r h a p s t o p
a l r e a d y e i t h e r i t s p l e a s e t o w a r d
a l s o e le v e n i t s e l f p u t t o w a r d s
a l t h o u g h e l s e k e e p r a t h e r t w e l v e
a lw a y s e l s e w h e r e l a s t r e t w e n t y
a m e m p t y l a t t e r s a id t w o
a m o n g e n o u g h la t t e r l y s a m e u n
a m o n g s t e t c l e a s t s e e u n d e r
a m o u n g s t e v e n le s s s e e m u n t i l
a m o u n t e v e r l t d s e e m e d u p
a n e v e r y m a d e s e e m in g u p o n
a n d e v e r y o n e m a n y s e e m s u s
a n o t h e r e v e r y t h in g m a y s e r i o u s v e r s a
a n y e v e r y w h e r e m e s e v e r a l v e r y
a n y h o w e x c e p t m e a n w h i l e s h e v ia
a n y o n e f e w m ig h t s h o u ld v i c e
a n y t h i n g f i f t e e n m i l l s h o w w a s
a n y w a y f i f y m in e s i d e w e
a n y w h e r e f i l l m o r e s i n c e w e l l
a r e f i n d m o r e o v e r s i n c e r e w e r e
a r o u n d f i r e m o s t s i x w h a t
a s f i r s t m o s t l y s i x t y w h a t e v e r
 225
Παράρτηµα 
a t f i v e m o v e s o w h e n
b a c k f o r m u c h s o m e w h e n c e
b e f o r m e r m u s t s o m e h o w w h e n e v e r
b e c a m e f o r m e r l y m y s o m e o n e w h e r e
b e c a u s e f o r t y m y s e l f s o m e t h i n g w h e r e a f t e r
b e c o m e f o u n d n ' t s o m e t i m e w h e r e a s
b e c o m e s f o u r n a m e s o m e t i m e s w h e r e b y
b e c o m i n g f r o m n a m e l y s o m e w h e r e w h e r e i n
b e e n f r o n t n e i t h e r s t i l l w h e r e u p o n
b e f o r e f u l l n e v e r s u c h w h e r e v e r
b e f o r e h a n d f u r t h e r n e v e r t h e l e s s y s t e m w h e t h e r
b e h i n d g e t n e x t t a k e w h i c h
b e i n g g i v e n i n e t e n w h i l e
b e l o w g o n o t h a n w h i t h e r
b e s i d e h a d n o t h a t w h o
b e s i d e s h a s n o b o d y t h e w h o e v e r
b e t w e e n h a s n t n o n e t h e i r w h o l e
b e y o n d h a v e n o o n e t h e m w h o m
b i l l h e n o r t h e m s e l v e s w h o s e
b o t h h e n c e n o t t h e n w h y
b o t t o m h e r n o t h i n g t h e n c e w i l l
b u t h e r e n o w t h e r e w i t h
b y h e r e a f t e r n o w h e r e t h e r e a f t e r w i t h i n
c a l l h e r e b y o f t h e r e b y w i t h o u t
c a n h e r e i n o f f t h e r e f o r e w o u l d
c a n n o t h e r e u p o n o f t e n t h e r e i n y e s
c a n t h e r s o n t h e r e u p o n y e t
c o h e r s e l f o n c e t h e s e y o u
c o m p u t e r h i m o n e t h e y y o u r
c o n h i m s e l f o n l y t h i c k y o u r s
c o u l d h i s o n t o t h i n y o u r s e l f
c o u l d n t h o w o r t h i r d y o u r s e l v e s
 
 226
Παράρτηµα 
Α2 Το Σώµα Κειµένων του Freddy Choi 
Ένα σώµα προς τµηµατοποίηση της συλλογής κειµένων του Freddy Choi έχει τη µορφή: 
 
========== 
The Sane Society is an ambitious work .  
Its scope is as broad as the question : What does it mean to live in modern society ? ?  
A work so broad , even when it is directed by a leading idea and informed by a moral vision , must necessarily `` fail '' .  
Even a hasty reader will easily find in it numerous blind spots , errors of fact and argument , important exclusions , areas 
of ignorance and prejudice , undue emphases on trivia , examples of broad positions supported by flimsy evidence , and 
the like .  
Such books are easy prey for critics .  
Nor need the critic be captious .  
A careful and orderly man , who values precision and a kind of tough intellectual responsibility , might easily be put off by 
such a book .  
It is a simple matter , for one so disposed , to take a work like The Sane Society and shred it into odds and ends .  
The thing can be made to look like the cluttered attic of a large and vigorous family -- a motley jumble of discarded 
objects , some outworn and some that were never useful , some once whole and bright but now chipped and tarnished , 
some odd pieces whose history no one remembers , here and there a gem , everything fascinating because it suggests some 
part of the human condition -- the whole adding up to nothing more than a glimpse into the disorderly history of the 
makers and users .  
That could be easily done , but there is little reason in it .  
It would come down to saying that Fromm paints with a broad brush , and that , after all , is not a conclusion one must 
work toward but an impression he has from the outset .  
========== 
the effect of the digitalis glycosides is inhibited by a high concentration of potassium in the incubation medium and is 
enhanced by the absence of potassium ( Wolff , 1960 ) .  
B. Organification of iodine The precise mechanism for organification of iodine in the thyroid is not as yet completely 
understood .  
However , the formation of organically bound iodine , mainly mono-iodotyrosine , can be accomplished in cell-free 
systems .  
In the absence of additions to the homogenate , the product formed is an iodinated particulate protein ( Fawcett and 
Kirkwood , 1953 ; ; Taurog , Potter and Chaikoff , 1955 ; ; Taurog , Potter , Tong , and Chaikoff , 1956 ; ; Serif and 
Kirkwood , 1958 ; ; De Groot and Carvalho , 1960 ) .  
This iodoprotein does not appear to be the same as what is normally present in the thyroid , and there is no evidence so far 
that thyroglobulin can be iodinated in vitro by cell-free systems .  
In addition , the iodoamino acid formed in largest quantity in the intact thyroid is di-iodotyrosine .  
If tyrosine and a system generating hydrogen peroxide are added to a cell-free homogenate of the thyroid , large quantities 
of free mono-iodotyrosine can be formed ( Alexander , 1959 ) .  
It is not clear whether this system bears any resemblance to the in vivo iodinating mechanism , and a system generating 
 227
Παράρτηµα 
peroxide has not been identified in thyroid tissue .  
On chemical grounds it seems most likely that iodide is first converted to Afj and then to Afj as the active iodinating 
species .  
========== 
the statement empirical , for goodness was not a quality like red or squeaky that could be seen or heard .  
What were they to do , then , with these awkward judgments of value ? ?  
To find a place for them in their theory of knowledge would require them to revise the theory radically , and yet that 
theory was what they regarded as their most important discovery .  
It appeared that the theory could be saved in one way only .  
If it could be shown that judgments of good and bad were not judgments at all , that they asserted nothing true or false , but 
merely expressed emotions like `` Hurrah '' or `` Fiddlesticks '' , then these wayward judgments would cease from 
troubling and weary heads could be at rest .  
This is the course the positivists took .  
They explained value judgments by explaining them away .  
Now I do not think their view will do .  
But before discussing it , I should like to record one vote of thanks to them for the clarity with which they have stated their 
case .  
It has been said of John Stuart Mill that he wrote so clearly that he could be found out .  
========== 
Greer Garson , world-famous star of stage , screen and television , will be honored for the high standard in tasteful 
sophisticated fashion with which she has created a high standard in her profession .  
As a Neiman-Marcus award winner the titian-haired Miss Garson is a personification of the individual look so important to 
fashion this season .  
She will receive the 1961 `` Oscar '' at the 24th annual Neiman-Marcus Exposition , Tuesday and Wednesday in the Grand 
Ballroom of the Sheraton-Dallas Hotel .  
The only woman recipient , Miss Garson will receive the award with Ferdinando Sarmi , creator of chic , beautiful women 
's fashions ; ; Harry Rolnick , president of the Byer-Rolnick Hat Corporation and designer of men 's hats ; ; Sydney 
Wragge , creator of sophisticated casuals for women and Roger Vivier , designer of Christian Dior shoes Paris , France , 
whose squared toes and lowered heels have revolutionized the shoe industry .  
The silver and ebony plaques will be presented at noon luncheons by Stanley Marcus , president of Neiman-Marcus , 
Beneficiary of the proceeds from the two showings will be the Dallas Society for Crippled Children Cerebral Palsy 
Treatment Center .  
The attractive Greer Garson , who loves beautiful clothes and selects them as carefully as she does her professional roles , 
prefers timeless classical designs .  
Occasionally she deserts the simple and elegant for a fun piece simply because `` It 's unlike me '' .  
In private life , Miss Garson is Mrs. E.E. Fogelson and on the go most of the time commuting from Dallas , where they 
maintain an apartment , to their California home in Los Angeles ' suburban Bel-Air to their ranch in Pecos , New Mexico .  
Therefore , her wardrobe is largely mobile , to be packed at a moment 's notice and to shake out without a wrinkle .  
Her creations in fashion are from many designers because she does n't want a complete wardrobe from any one designer 
any more than she wants `` all of her pictures by one painter '' .  
 228
Παράρτηµα 
========== 
Wage-price policies of industry are the result of a complex of forces -- no single explanation has been found which applies 
to all cases .  
The purpose of this paper is to analyze one possible force which has not been treated in the literature , but which we 
believe makes a significant contribution to explaining the wage-price behavior of a few very important industries .  
While there may be several such industries to which the model of this paper is applicable , the authors make particular 
claim of relevance to the explanation of the course of wages and prices in the steel industry of the United States since 
World War 2 .  
 
Indeed , the apparent stiffening of the industry 's attitude in the recent steel strike has a direct explanation in terms of the 
model here presented .  
The model of this paper considers an industry which is not characterized by vigorous price competition , but which is so 
basic that its wage-price policies are held in check by continuous critical public scrutiny .  
Where the industry 's product price has been kept below the `` profit-maximizing '' and `` entry-limiting '' prices due to 
fears of public reaction , the profit seeking producers have an interest in offering little real resistance to wage demands .  
The contribution of this paper is a demonstration of this proposition , and an exploration of some of its implications .  
In order to focus clearly upon the operation of this one force , which we may call the effect of `` public-limit pricing '' on `` 
key '' wage bargains , we deliberately simplify the model by abstracting from other forces , such as union power , which 
may be relevant in an actual situation .  
For expository purposes , this is best treated as a model which spells out the conditions under which an important industry 
affected with the public interest would find it profitable to raise wages even in the absence of union pressures for higher 
wages .  
========== 
The vast Central Valley of California is one of the most productive agricultural areas in the world .  
During the summer of 1960 , it became the setting for a bitter and basic labor-management struggle .  
The contestants in this economic struggle are the Agricultural Workers Organizing Committee ( AWOC ) of the AFL-CIO 
and the agricultural employers of the State .  
By virtue of the legal responsibilities of the Department of Employment in the farm placement program , we necessarily 
found ourselves in the middle between these two forces .  
It is not a pleasant or easy position , but one we have endeavored to maintain .  
We have sought to be strictly neutral as between the parties , but at the same time we have been required frequently to rule 
on specific issues or situations as they arose .  
Inevitably , one side was pleased and the other displeased , regardless of how we ruled .  
Often the displeased parties interpreted our decision as implying favoritism toward the other .  
We have consoled ourselves with the thought that this is a normal human reaction and is one of the consequences of any 
decision in an adversary proceeding.  
It is disconcerting , nevertheless , to read in a labor weekly , `` Perluss knuckles down to growers '' , and then to be 
confronted with a growers ' publication which states , `` Perluss recognizes obviously phony and trumped-up strikes as 
bona fide '' .  
========== 
Rookie Ron Nischwitz continued his pinpoint pitching Monday night as the Bears made it two straight over Indianapolis , 
 229
Παράρτηµα 
5-3 .  
The husky 6-3 , 205-pound lefthander , was in command all the way before an on-the-scene audience of only 949 and 
countless of television viewers in the Denver area .  
It was Nischwitz ' third straight victory of the new season and ran the Grizzlies ' winning streak to four straight .  
They now lead Louisville by a full game on top of the American Association pack .  
Nischwitz fanned six and walked only Charley Hinton in the third inning .  
He has given only the one pass in his 27 innings , an unusual characteristic for a southpaw .  
The Bears took the lead in the first inning , as they did in Sunday 's opener , and never lagged .  
Dick McAuliffe cracked the first of his two doubles against Lefty Don Rudolph to open the Bear 's attack .  
After Al Paschal gruonded out , Jay Cooke walked and Jim McDaniel singled home McAuliffe .  
Alusik then moved Cooke across with a line drive to left .  
========== 
Unemployed older workers who have no expectation of securing employment in the occupation in which they are skilled 
should be able to secure counseling and retraining in an occupation with a future .  
Current programs The present Federal program of vocational education began in 1917 with the passage of the Smith-
Hughes Act , which provided a continuing annual appropriation of $ 7 million to support , on a matching basis , state-
administered programs of vocational education in agriculture , trades , industrial skills and home economics .  
Even the states remain primarily in an assisting role , providing leadership and teacher training .  
Some vocational training schools provide such training , but the current need exceeds the facilities .  
Since 1917 some thirteen supplementary and related acts have extended this Federal program .  
The George-Barden Act of 1946 raised the previous increases in annual authorizations to $ 29 million in addition to the $ 7 
million under the Smith Act .  
The Health Amendment Act of 1956 added $ 5 million for practical nurse training .  
The latest major change in this program was introduced by the National Defense Education Act of 1958, Title 8, of which 
amended the George-Barden Act .  
Annual authorizations of $ 15 million were added for area vocational education programs that meet national defense needs 
for highly skilled technicians .  
The Federal program of vocational education merely provides financial aid to encourage the establishment of vocational 
education programs in public schools .  
The initiative , administration and control remain primarily with the local school districts .  
========== 
briefly , the topping configuration must be examined for its inferences .  
Then the fact that the lower channel line was pierced had further forecasting significance .  
And then the application of the count rules to the width ( horizontally ) of the configuration gives us an intial estimate of 
the probable depth of the decline .  
The very idea of there being `` count rules '' implies that there is some sort of proportion to be expected between the 
 230
Παράρτηµα 
amount of congestive activity and the extent of the breakaway ( run up or run down ) movement .  
This expectation is what really `` sold '' point and figure .  
But there is no positive and consistently demonstrable relationship in the strictest sense .  
Experience will show that only the vaguest generalities apply , and in fine , these merely dwell upon a relationship 
between the durations and intensities of events .  
After all , too much does not happen too suddenly , nor does very little take long .  
The advantages and disadvantages of these two types of charting , bar charting and point and figure charting , remain the 
subject of fairly good-natured litigation among their respective professional advocates , with both methods enjoying in 
common , one irrevocable merit .  
They are both trend-following methods .  
========== 
Miami , Fla. , March 17 -- The Orioles tonight retained the distinction of being the only winless team among the eighteen 
Major-League clubs as they dropped their sixth straight spring exhibition decision , this one to the Kansas City Athletics 
by a score of 5 to 3 .  
Indications as late as the top of the sixth were that the Birds were to end their victory draught as they coasted along with a 
3-to-o advantage .  
Siebern hits homer Over the first five frames , Jack Fisher , the big righthander who figures to be in the middle of Oriole 
plans for a drive on the 1961 American League pennant , held the A 's scoreless while yielding three scattered hits .  
Then Dick Hyde , submarine-ball hurler , entered the contest and only five batters needed to face him before there existed 
a 3-to-3 deadlock .  
A two-run homer by Norm Siebern and a solo blast by Bill Tuttle tied the game , and single runs in the eighth and ninth 
gave the Athletics their fifth victory in eight starts .  
House throws wild With one down in the eighth , Marv Throneberry drew a walk and stole second as Hyde fanned Tuttle .  
Catcher Frank House 's throw in an effort to nab Throneberry was wide and in the dirt .  
Then Heywood Sullivan , Kansas City catcher , singled up the middle and Throneberry was across with what proved to be 
the winning run .  
Rookie southpaw George Stepanovich relieved Hyde at the start of the ninth and gave up the A 's fifth tally on a walk to 
second baseman Dick Howser , a wild pitch , and Frank Cipriani 's single under Shortstop Jerry Adair 's glove into center .  
========== 
 
Τα όρια µεταξύ των τµηµάτων σηµατοδοτούνται µε τη βοήθεια του συµβόλου «==========». 
 
Το εν λόγω κείµενο µετά την αφαίρεση των λέξεων που ανήκουν την stop list όπως αυτή 
παραθέτεται στο Παράρτηµα Α1 και την εφαρµογή της τεχνικής του stemming θα λάβει η µορφή: 
 
 
 231
Παράρτηµα 
========== 
 sane societi ambiti work . 
 scope broad question live modern societi . 
 work broad direct lead idea inform moral vision necessarili fail . 
 ty reader easili find numer blind spot error fact argument import exclus area ignor prejudic undu emphas trivia broad posit 
support flimsi evid . 
 book easi prei critic . 
 critic captiou . 
 care orderli man precis kind tough intellectu respons easili put book . 
 simpl matter dispos work sane societi shred odd end . 
 thing made clutter attic larg vigor famili motlei jumbl discard object outworn bright chip tarnish odd piec histori rememb 
gem everyth fascin suggest part human condit glimps disorderli histori maker user . 
 easili reason . 
 paint broad brush conclus work impress outset . 
 ========== 
 effect digitali glycosid inhibit high concentr potassium incub medium enhanc absenc potassium wolff . 
 organif iodin precis mechan organif iodin thyroid complet understood . 
 format organ bound iodin monoiodotyrosin accomplish cellfre system . 
 absenc addit homogen product form iodin particul protein fawcett kirkwood taurog potter chaikoff taurog potter tong 
chaikoff serif kirkwood de groot carvalho . 
 iodoprotein present thyroid evid thyroglobulin iodin vitro cellfre system . 
 addit iodoamino acid form largest quantiti intact thyroid diiodotyrosin . 
 tyrosin system gener hydrogen peroxid cellfre homogen thyroid larg quantiti free monoiodotyrosin form alexand . 
 clear system bear resembl vivo iodin mechan system gener peroxid identifi thyroid tissu . 
 chemic ground iodid convert afj afj activ iodin speci . 
 ========== 
 statement empir good qualiti red squeaki heard . 
 awkward judgments. 
 find place theori knowledg requir revis theori radic theori regard import discoveri . 
 theori save . 
 shown judgment good bad judgment assert true fals express emot hurrah fiddlestick wayward judgment ceas troubl weari 
head rest . 
 positivist . 
 232
Παράρτηµα 
 explain judgment explain . 
 view . 
 discuss record vote clariti state case . 
 john stuart mill wrote found . 
 ========== 
 greer garson worldfam star stage screen televis honor high standard tast sophist fashion creat high standard profess . 
 neimanmarcu award winner titianhair miss garson personif individu import fashion season . 
 receiv oscar annual neimanmarcu exposit tuesdai wednesdai grand ballroom sheratondalla hotel . 
 woman recipi miss garson receiv award ferdinando sarmi creator chic beauti women fashion harri rolnick presid 
byerrolnick hat corpor design men hat sydnei wragg creator sophist casual women roger vivier design christian dior shoe 
pari franc squar toe lower heel revolution shoe industri . 
 silver eboni plaqu present noon luncheon stanlei marcu presid neimanmarcu beneficiari proce show dalla societi crippl 
children cerebr palsi treatment center . 
 attract greer garson love beauti cloth select carefulli profession role prefer timeless classic design . 
 occasion desert simpl eleg fun piec simpli unlike. 
 privat life miss garson fogelson time commut dalla maintain apart california home angel suburban belair ranch peco 
mexico . 
 wardrob larg mobil pack moment notic shake wrinkl . 
 creation fashion design complet wardrob design pictur painter . 
 ========== 
 wagepric polici industri result complex forc singl explan found appli case . 
 purpos paper analyz forc treat literatur make signific contribut explain wagepric behavior import industri . 
 industri model paper applic author make claim relev explan wage price steel industri unit state world war . 
 appar stiffen industri attitud recent steel strike direct explan term model present . 
 model paper industri character vigor price competit basic wagepric polici held check continu critic public scrutini . 
 industri product price profitmaxim entrylimit price due fear public reaction profit seek produc interest offer real resist 
wage demand . 
 contribut paper demonstr proposit explor implic . 
 order focu oper forc call effect publiclimit price kei wage bargain deliber simplifi model abstract forc union power relev 
actual situat . 
 expositori purpos treat model spell condit import industri affect public interest find profit rais wage absenc union pressur 
higher wage . 
 ========== 
 vast central vallei california product agricultur area world . 
 233
Παράρτηµα 
 summer set bitter basic labormanag struggl . 
 contest econom struggl agricultur worker organ committe awoc aflcio agricultur employ state . 
 ========== 
 unemploi worker expect secur employ occup skill secur counsel retrain occup futur . 
 virtu legal respons depart employ farm placement program necessarili found middl forc . 
 pleasant easi posit endeavor maintain . 
 sought strictli neutral parti time requir frequent rule specif issu situat aros . 
 inevit side displeas rule . 
 displeas parti interpret decis impli favorit . 
 consol normal human reaction consequ decis adversari proceed . 
 disconcert read labor weekli perluss knuckl grower confront grower public state perluss recogn phoni trumpedup strike 
bona fide . 
 rooki ron nischwitz continu pinpoint pitch mondai night bear made straight indianapoli . 
 huski pound lefthand command onthescen audienc countless televis viewer denver area . 
 nischwitz straight victori season ran grizzli win streak straight . 
 lead louisvil full game top american associ pack . 
 nischwitz fan walk charlei hinton . 
 pass unusu characterist southpaw . 
 bear lead sundai open lag . 
 dick mcauliff crack doubl lefti don rudolph open bear attack . 
 al paschal gruond jai cook walk jim mcdaniel singl home mcauliff . 
 alusik move cook line drive left . 
 ========== 
 vocat train school train current exce facil . 
 current program present feder program vocat educ began passag smithhugh act continu annual appropri million support 
match basi stateadminist program vocat educ agricultur trade industri skill home econom . 
 thirteen supplementari relat act extend feder program . 
 georgebarden act rais previou increas annual author million addit million smith act . 
 health amend act million practic nurs train . 
 latest major chang program introduc nation defens educ act titl amend georgebarden act . 
 annual author million area vocat educ program meet nation defens highli skill technician . 
 feder program vocat educ financi aid encourag establish vocat educ program public school . 
 234
Παράρτηµα 
 initi administr control remain primarili local school district . 
 state remain primarili assist role leadership teacher train . 
 ========== 
 top configur examin infer . 
 fact lower channel line pierc forecast signific . 
 applic count rule width horizont configur intial estim probabl depth declin . 
 idea count rule impli sort proport expect amount congest activ extent breakawai run run movement . 
 expect sold point figur . 
 miami fla march oriol tonight retain distinct winless team majorleagu club drop straight spring exhibit decis kansa citi 
athlet score . 
 posit consist demonstr relationship strictest sens . 
 experi show vaguest gener appli fine relationship durat intens event . 
 happen suddenli long . 
 advantag disadvantag type chart bar chart point figur chart remain subject fairli goodnatur litig respect profession advoc 
method enjoi common irrevoc merit . 
 trend method . 
 ========== 
 indic late top bird end victori draught coast advantag . 
siebern hit homer frame jack fisher big righthand figur middl oriol plan drive american leagu pennant held scoreless yield 
scatter hit . 
 dick hyde submarineball hurler enter contest batter face exist deadlock . 
 tworun homer norm siebern solo blast bill tuttl game singl run ninth gave athlet victori start . 
 hous throw wild marv throneberri drew walk stole hyde fan tuttl . 
 catcher frank hous throw effort nab throneberri wide dirt . 
 heywood sullivan kansa citi catcher singl middl throneberri prove win run . 
 rooki southpaw georg stepanovich reliev hyde start ninth gave talli walk baseman dick howser wild pitch frank cipriani 
singl shortstop jerri adair glove center . 
 ========== 
 
 
 
 
 235
Παράρτηµα 
A3 Το Σώµα Κειµένων της Εφηµερίδας «Το Βήµα» 
Ένα κείµενο του συγγραφέα κου ∆ερτιλή που περιέχεται στη συλλογή κειµένων των 
Σταµατάτος, Φακοτάκης και Κοκκινάκης αποτελούµενη από άρθρα της εφηµερίδας «Το Βήµα» είναι 
το παρακάτω: 
 
<CC> 
Γ. ∆ΕΡΤΙΛΗΣ ΤΟ ΒΗΜΑ, 23-03-1997 Κωδικός άρθρου: B12421B062 </CC> 
<TITLE> 
                          Σαφήνεια και αµφιβολία 
</TITLE> 
<TEXT> 
Προϋπόθεση του καλού ύφους, η σαφήνεια είναι αναγκαία τόσο στη λογοτεχνία όσο και στην επιστηµονική γραφή. 
Αλλά πρόκειται για δύο διαφορετικές σαφήνειες. Η µία είναι ποιητική, η άλλη εξηγηµατική. 
Με τη σαφήνεια του ύφους του, ο λογοτέχνης «ποιεί» την πολυσηµία. Ετσι ανοίγει µπροστά στον αναγνώστη ένα 
ριπίδιο αναγνώσεων: τον ευκολύνει να διαβάσει και να ερµηνεύσει το πολύσηµο κείµενο µε πολλαπλούς τρόπους. 
Αλλά ο συγγραφέας ενός επιστηµονικού έργου (αυτός που κυρίως θα µας απασχολήσει σήµερα) εξαφανίζει µε τη 
σαφήνεια του ύφους του όλες τις αµφισηµίες και πολυσηµίες του κειµένου. Αποκλείει έτσι τις αµφιβολίες του 
αναγνώστη για τα όσα ο συγγραφέας ισχυρίζεται και διευκολύνει τον ανα-γνωστικό, επιστηµονικό έλεγχο. 
Η πολυσηµία που προσπαθεί να εκφράσει ο λογοτέχνης µοιάζει, εξάλλου, αλλά δεν ταυτίζεται µε την αµφιβολία που 
κάποτε εκφράζει στο κείµενό του ένας επιστήµονας. Την εκφράζει επειδή συναισθάνεται τα όρια του εαυτού του, του 
συγκεκριµένου έργου του, των προσωπικών του θεωριών, ακόµη και της επιστήµης του. Αλλά παραµένει η ανάγκη να 
είναι σαφείς οι θεωρίες του, σαφές και το κείµενό του. Ετσι, ο συγγραφέας από τη µια καταγράφει την αµφιβολία, από 
την άλλη όµως υποστηρίζει µε σαφήνεια τη συλλογιστική του, τις απόψεις και τις ερµηνείες του: επειδή ο 
επιστηµονικός λόγος, εξ ορισµού, δεν επιδέχεται αντιφάσεις. 
Οπως είναι φυσικό, ο κανόνας της σαφήνειας δεν έχει ενιαία εφαρµογή. Υπάρχουν οι διαφοροποιήσεις που εξαρτώνται 
από την προσωπικότητα και τις ικανότητες του κάθε συγγραφέα. Ενας επιστήµονας µε καλό συγγραφικό ταλέντο 
µπορεί ίσως να βρει ελευθεριότερους τρόπους παρουσίασης των ιδεών του, να επεκταθεί σε υπαινιγµούς, σε 
αµφισηµίες και σε αποσιωπήσεις που έχουν τη δική τους λειτουργία και αισθητική. Αλλά αυτό δεν αναιρεί την 
επιστηµονική του υποχρέωση να δείξει µε σαφήνεια, σε άλλα σηµεία του κειµένου, τις απόψεις και τις ερµηνείες του. 
Υπάρχουν έπειτα διαφοροποιήσεις ανάλογες µε τα γνωστικά αντικείµενα και τα είδη του γραπτού επιστηµονικού 
λόγου. Η Ιστορία, π.χ., αφήνει περισσότερες υφολογικές δυνατότητες στον συγγραφέα από οποιαδήποτε άλλη 
επιστήµη. Του επιτρέπει, κάποτε του επιβάλλει κιόλας, να αναδείξει τις εσωτερικές αντιφάσεις του ανθρώπου και των 
ανθρωπίνων κοινωνιών· τον ρόλο των ανθρωπίνων παθών· τη σηµασία των συµπτώσεων και της τύχης· το βάρος των 
µαζικών κοινωνικών δυνάµεων· τους αναπόδραστους φραγµούς της φύσης. 
Ωστόσο, ο ιστορικός δεν δείχνει τις αντιφάσεις ουσίας µε αντιφάσεις ύφους, αλλά µε σαφήνεια. Τα πάθη δεν τα δείχνει 
µε ψευδοροµαντική ασάφεια, αλλά µε τη σαφήνεια εκείνη που θα αναδείξει την αιχµηρότητά τους. Τονίζει τις 
συµπτώσεις και την τυχαιότητα µε ύφος σαφές και όχι τυχάρπαστο. Τη «µοίρα» δεν την αποδίδει σε µεταφυσικές 
δυνάµεις - εφόσον κάνει επιστήµη. Μπορεί να την ταυτίζει µε δυνάµεις που θεωρούσαν ανεξήγητες και µεταφυσικές οι 
άνθρωποι που µελετά· αλλά ο ίδιος δίνει όνοµα στις δυνάµεις αυτές· και τις εντάσσει, µε σαφήνεια, σε έναν αιτιακό 
συλλογισµό, σε ένα ερµηνευτικό σχήµα. 
Ενα ευτυχές ιστοριογραφικό έργο απαιτεί έναν καλό συγκερασµό της επιστήµης µε την τέχνη του ύφους. Από εκεί και 
πέρα, υπάρχει µόνο η υπέρβαση και της επιστήµης και του ύφους. Στον υπερβατικό αυτό χώρο, εκεί όπου ο 
συγκερασµός γίνεται ταύτιση γνώσης και τέχνης, οδηγεί ένας δρόµος σχεδόν άβατος. Τόπος που ονειρεύονται πολλοί, 
επιστήµονες και τεχνίτες, τόπος άφθαστος για µας τους πολλούς - όχι, όµως, ουτοπία. Μας τον έχουν δείξει οι 
 236
Παράρτηµα 
ελάχιστοι που έφτασαν εκεί, οι δάσκαλοί µας, ο καθένας µε τη µεγάλη και τη µικρή του ιστορία, όντα διόλου 
µεταφυσικά, πολύ ανθρώπινα. Ενας απλός άνθρωπος δεν ήταν άραγε ο δάσκαλος που, πριν από δυόµισι αιώνες, 
σκάρωνε κάθε µέρα τη φυγή του προς τα εκεί, µε ένα απλό, αλλά καλώς συγκερασµένο κλειδοκύµβαλο; 
Ο κ. Γ. Β. ∆ερτιλής είναι καθηγητής της Ιστορίας στο Πανεπιστήµιο Αθηνών. 
</TEXT> 
 
 Η εφαρµογή του Μορφοσυντακτικού Αναλυτή στο παραπάνω κείµενο έχει ως αποτέλεσµα την 
µετατροπή του στην ακόλουθη µορφή: 
 
  <fw>TITLE</fw> 
<d> 
<s> 
  <abbr>Γ.</abbr> 
  <gw cat="N" lemma="?">∆ΕΡΤΙΛΗΣ</gw> 
  <gw cat="Art" attrs="MscSngAcc + NtrSngNomAcc" lemma="ο">ΤΟ</gw> 
  <gw cat="N" attrs="NtrSngNomAccVoc" lemma="βήµα">ΒΗΜΑ</gw> 
  <punc>,</punc> 
  <num>23-03-1997</num> 
  <gw cat="N" attrs="MscSngNom" lemma="κωδικός">Κωδικός</gw> 
  <gw cat="N" attrs="NtrSngGen" lemma="άρθρο">άρθρου</gw> 
  <punc>:</punc> 
  <num>B12421B062</num> 
  <symb><</symb> 
  <symb>/</symb> 
  <fw>CC</fw> 
  <symb>></symb> 
</s> 
<s> 
  <symb><</symb> 
  <symb>></symb> 
</s> 
 237
Παράρτηµα 
<s> 
  <gw cat="N" attrs="FemSngNomAccVoc" lemma="σαφήνεια">Σαφήνεια</gw> 
  <gw cat="Cnj" lemma="και">και</gw> 
  <gw cat="N" attrs="FemSngNomAccVoc" lemma="αµφιβολία">αµφιβολία</gw> 
</s> 
<s> 
  <symb><</symb> 
  <symb>/</symb> 
  <fw>TITLE</fw> 
  <symb>></symb> 
</s> 
<s> 
  <symb><</symb> 
  <fw>TEXT</fw> 
  <symb>></symb> 
</s> 
<p> 
  <s> 
    <gw cat="N" attrs="FemSngNomAccVoc" lemma="προϋπόθεση">Προϋπόθεση</gw> 
    <gw cat="Art" attrs="MscNtrSngGen" lemma="ο">του</gw> 
    <gw cat="Adj" attrs="MscNtrSngGen" lemma="καλός">καλού</gw> 
    <gw cat="N" attrs="NtrSngGen" lemma="ύφος">ύφους</gw> 
    <punc>,</punc> 
    <gw cat="Art" attrs="FemSngNom" lemma="ο">η</gw> 
    <gw cat="N" attrs="FemSngNomAccVoc" lemma="σαφήνεια">σαφήνεια</gw> 
    <gw cat="V" attrs="PsvPrsFucIndSjvSngPlr_C_" lemma="είµαι">είναι</gw> 
    <gw cat="Adv" lemma="αναγκαία">αναγκαία</gw> 
    <gw cat="Adv" lemma="τόσο">τόσο</gw> 
    <gw cat="PrpArt" attrs="FemSngAcc" lemma="στο">στη</gw> 
    <gw cat="N" attrs="FemSngNomAccVoc" lemma="λογοτεχνία">λογοτεχνία</gw> 
    <gw cat="Adv" lemma="όσο">όσο</gw> 
 238
Παράρτηµα 
    <gw cat="Cnj" lemma="και">και</gw> 
    <gw cat="PrpArt" attrs="FemSngAcc" lemma="στο">στην</gw> 
    <gw cat="Adj" attrs="FemSngNomAccVoc" lemma="επιστηµονικός">επιστηµονική</gw> 
    <gw cat="N" attrs="FemSngNomAccVoc" lemma="γραφή">γραφή</gw> 
    <punc>.</punc> 
  </s> 
  <s> 
    <gw cat="Cnj" lemma="αλλά">Αλλά</gw> 
    <gw cat="V" attrs="ActPrsIndSng_C_" lemma="προτίθεµαι">πρόκειται</gw> 
    <gw cat="Prp" lemma="για">για</gw> 
    <gw cat="Arith" attrs="MscFemNtrPlrNomGenAccVoc" lemma="δύο">δύο</gw> 
    <gw cat="Adj" attrs="FemPlrNomAccVoc" lemma="διαφορετικός">διαφορετικές</gw> 
    <gw cat="N" attrs="FemPlrNomAccVoc" lemma="σαφήνεια">σαφήνειες</gw> 
    <punc>.</punc> 
  </s> 
  <s> 
    <gw cat="Art" attrs="FemSngNom" lemma="ο">Η</gw> 
    <gw cat="Arith" attrs="FemSngNomAcc" lemma="ένας">µία</gw> 
    <gw cat="V" attrs="PsvPrsFucIndSjvSngPlr_C_" lemma="είµαι">είναι</gw> 
    <gw cat="Adj" attrs="FemSngNomAccVoc" lemma="ποιητικός">ποιητική</gw> 
    <punc>,</punc> 
    <gw cat="Art" attrs="FemSngNom" lemma="ο">η</gw> 
    <gw cat="Prn" attrs="FemSngNomAcc" lemma="άλλος">άλλη</gw> 
    <gw cat="Adj" lemma="?">εξηγηµατική</gw> 
    <punc>.</punc> 
  </s> 
</p> 
<p> 
  <s> 
    <gw cat="Prp" lemma="µε">Με</gw> 
    <gw cat="Art" attrs="FemSngAcc" lemma="ο">τη</gw> 
 239
Παράρτηµα 
    <gw cat="N" attrs="FemSngNomAccVoc" lemma="σαφήνεια">σαφήνεια</gw> 
    <gw cat="Art" attrs="MscNtrSngGen" lemma="ο">του</gw> 
    <gw cat="N" attrs="NtrSngGen" lemma="ύφος">ύφους</gw> 
    <gw cat="Clt" attrs="MscNtrSng_C_" lemma="δικός">του</gw> 
    <punc>,</punc> 
    <gw cat="Art" attrs="MscSngNom" lemma="ο">ο</gw> 
    <gw cat="N" attrs="MscFemSngNom" lemma="λογοτέχνης">λογοτέχνης</gw> 
    <symb>«</symb> 
    <gw cat="V" attrs="ActPrsFucIndSng_C_ + ActPrsSjvSng_C_" lemma="ποιώ">ποιεί</gw> 
    <symb>»</symb> 
    <gw cat="Art" attrs="FemSngAcc" lemma="ο">την</gw> 
    <gw cat="N" attrs="FemSngNomAccVoc" lemma="πολυσηµία">πολυσηµία</gw> 
    <punc>.</punc> 
  </s> 
  <s> 
    <gw cat="Adv" lemma="έτσι">Έτσι</gw> 
    <gw cat="V" attrs="ActPrsFucIndSjvSng_C_" lemma="ανοίγω">ανοίγει</gw> 
    <gw cat="Adv" lemma="µπροστά">µπροστά</gw> 
    <gw cat="PrpArt" attrs="MscSngAcc" lemma="στο">στον</gw> 
    <gw cat="N" attrs="MscSngGenAccVoc" lemma="αναγνώστης">αναγνώστη</gw> 
    <gw cat="Arith" attrs="MscSngAcc + NtrSngNomAcc" lemma="ένας">ένα</gw> 
    <gw cat="N" attrs="FemPlrGen" lemma="ανάγνωση">αναγνώσεων</gw> 
    <punc>:</punc> 
    <gw cat="Pcl" lemma="να">να</gw> 
    <gw cat="N" attrs="NtrSngNomAccVoc" lemma="ριπίδι">ριπίδιο</gw> 
    <gw cat="Clt" attrs="MscSngAcc_C_" lemma="εγώ">τον</gw> 
    <gw cat="V" attrs="ActPrsFucIndSjvSng_C_ + ActPstSjvSng_C_ + ActPstInf + ActFutIndSng_C_" 
lemma="ευκολύνω">ευκολύνει</gw> 
    <gw cat="Pcl" lemma="να">να</gw> 
    <gw cat="V" attrs="ActPstSjvSng_C_ + ActPstInf + ActFutIndSng_C_" lemma="διαβάζω">διαβάσει</gw> 
    <gw cat="Cnj" lemma="και">και</gw> 
 240
Παράρτηµα 
    <gw cat="V" attrs="ActPstSjvSng_C_ + ActPstInf + ActFutIndSng_C_" lemma="ερµηνεύω">ερµηνεύσει</gw> 
    <gw cat="Art" attrs="MscSngAcc + NtrSngNomAcc" lemma="ο">το</gw> 
    <gw cat="Adj" attrs="MscSngAcc + NtrSngNomAccVoc" lemma="πολύσηµος">πολύσηµο</gw> 
    <gw cat="Pcp" attrs="PsvPfcMscSngAcc + PsvPfcNtrSngNomAccVoc" lemma="κείµαι">κείµενο</gw> 
    <gw cat="Prp" lemma="µε">µε</gw> 
    <gw cat="Adj" attrs="MscPlrAcc" lemma="πολλαπλός">πολλαπλούς</gw> 
    <gw cat="N" attrs="MscPlrAcc" lemma="τρόπος">τρόπους</gw> 
    <punc>.</punc> 
  </s> 
</p> 
<p> 
  <s> 
    <gw cat="Cnj" lemma="αλλά">Αλλά</gw> 
    <gw cat="Art" attrs="MscSngNom" lemma="ο">ο</gw> 
    <gw cat="N" attrs="MscSngNom" lemma="συγγραφέας">συγγραφέας</gw> 
    <gw cat="Arith" attrs="MscNtrSngGen" lemma="ένας">ενός</gw> 
    <gw cat="Adj" attrs="MscNtrSngGen" lemma="επιστηµονικός">επιστηµονικού</gw> 
    <gw cat="N" attrs="NtrSngGen" lemma="έργο">έργου</gw> 
    <symb>(</symb> 
    <gw cat="Prn" attrs="MscSngNom" lemma="αυτός">αυτός</gw> 
    <gw cat="Adv" lemma="που">που</gw> 
    <gw cat="Adv" lemma="κύρια">κυρίως</gw> 
    <gw cat="Pcl" lemma="θα">θα</gw> 
    <gw cat="Clt" attrs="PlrGenAcc_A_" lemma="εγώ">µας</gw> 
    <gw cat="V" attrs="ActPstSjvSng_C_ + ActPstInf + ActFutIndSng_C_" lemma="απασχολώ">απασχολήσει</gw> 
    <gw cat="Adv" lemma="σήµερα">σήµερα</gw> 
    <symb>)</symb> 
    <gw cat="V" attrs="ActPrsFucIndSjvSng_C_" lemma="εξαφανίζω">εξαφανίζει</gw> 
    <gw cat="Prp" lemma="µε">µε</gw> 
    <gw cat="Art" attrs="FemSngAcc" lemma="ο">τη</gw> 
    <gw cat="N" attrs="FemSngNomAccVoc" lemma="σαφήνεια">σαφήνεια</gw> 
 241
Παράρτηµα 
    <gw cat="Art" attrs="MscNtrSngGen" lemma="ο">του</gw> 
    <gw cat="N" attrs="NtrSngGen" lemma="ύφος">ύφους</gw> 
    <gw cat="Clt" attrs="MscNtrSng_C_" lemma="δικός">του</gw> 
    <gw cat="Adj" attrs="FemPlrNomAccVoc" lemma="όλος">όλες</gw> 
    <gw cat="Art" attrs="FemPlrAcc" lemma="ο">τις</gw> 
    <gw cat="N" attrs="FemPlrNomAccVoc" lemma="αµφισηµία">αµφισηµίες</gw> 
    <gw cat="Cnj" lemma="και">και</gw> 
    <gw cat="N" attrs="FemPlrNomAccVoc" lemma="πολυσηµία">πολυσηµίες</gw> 
    <gw cat="Art" attrs="MscNtrSngGen" lemma="ο">του</gw> 
    <gw cat="Pcp" attrs="PsvPfcMscNtrSngGen" lemma="κείµαι">κειµένου</gw> 
    <punc>.</punc> 
  </s> 
  <s> 
    <gw cat="V" attrs="ActPrsFucIndSng_C_ + ActPrsSjvSng_C_" lemma="αποκλείω">Αποκλείει</gw> 
    <gw cat="Adv" lemma="έτσι">έτσι</gw> 
    <gw cat="Art" attrs="FemPlrAcc" lemma="ο">τις</gw> 
    <gw cat="N" attrs="FemPlrNomAccVoc" lemma="αµφιβολία">αµφιβολίες</gw> 
    <gw cat="Art" attrs="MscNtrSngGen" lemma="ο">του</gw> 
    <gw cat="N" attrs="MscSngGenAccVoc" lemma="αναγνώστης">αναγνώστη</gw> 
    <gw cat="Prp" lemma="για">για</gw> 
    <gw cat="Art" attrs="NtrPlrNomAcc" lemma="ο">τα</gw> 
    <gw cat="Prn" attrs="NtrPlrNomAcc" lemma="όσος">όσα</gw> 
    <gw cat="Art" attrs="MscSngNom" lemma="ο">ο</gw> 
    <gw cat="N" attrs="MscSngNom" lemma="συγγραφέας">συγγραφέας</gw> 
    <gw cat="V" attrs="PsvPrsFucIndSjvSng_C_" lemma="ισχυρίζοµαι">ισχυρίζεται</gw> 
    <gw cat="Cnj" lemma="και">και</gw> 
    <gw cat="V" attrs="ActPrsFucIndSjvSng_C_ + ActPstSjvSng_C_ + ActPstInf + ActFutIndSng_C_" 
lemma="διευκολύνω">διευκολύνει</gw> 
    <gw cat="Art" attrs="MscSngAcc" lemma="ο">τον</gw> 
    <unk>ανα-γνωστικό</unk> 
    <punc>,</punc> 
 242
Παράρτηµα 
    <gw cat="Adj" attrs="MscSngAcc + NtrSngNomAccVoc" lemma="επιστηµονικός">επιστηµονικό</gw> 
    <gw cat="N" attrs="MscSngAcc" lemma="έλεγχος">έλεγχο</gw> 
    <punc>.</punc> 
  </s> 
</p> 
<p> 
  <s> 
    <gw cat="Art" attrs="FemSngNom" lemma="ο">Η</gw> 
    <gw cat="N" attrs="FemSngNomAccVoc" lemma="πολυσηµία">πολυσηµία</gw> 
    <gw cat="Adv" lemma="που">που</gw> 
    <gw cat="V" attrs="ActPrsFucIndSng_C_ + ActPrsSjvSng_C_" lemma="προσπαθώ">προσπαθεί</gw> 
    <gw cat="Pcl" lemma="να">να</gw> 
    <gw cat="V" attrs="ActPstSjvSng_C_ + ActPstInf + ActFutIndSng_C_" lemma="εκφράζω">εκφράσει</gw> 
    <gw cat="Prp" lemma="µε">µε</gw> 
    <gw cat="Art" attrs="MscSngNom" lemma="ο">ο</gw> 
    <gw cat="N" attrs="MscFemSngNom" lemma="λογοτέχνης">λογοτέχνης</gw> 
    <gw cat="V" attrs="ActPrsFucIndSjvSng_C_" lemma="µοιάζω">µοιάζει</gw> 
    <punc>,</punc> 
    <gw cat="Adv" lemma="εξάλλου">εξάλλου</gw> 
    <punc>,</punc> 
    <gw cat="Cnj" lemma="αλλά">αλλά</gw> 
    <gw cat="Pcl" lemma="δε">δεν</gw> 
    <gw cat="V" attrs="PsvPrsFucIndSjvSng_C_" lemma="ταυτίζω">ταυτίζεται</gw> 
    <gw cat="Art" attrs="FemSngAcc" lemma="ο">την</gw> 
    <gw cat="N" attrs="FemSngNomAccVoc" lemma="αµφιβολία">αµφιβολία</gw> 
    <gw cat="Adv" lemma="που">που</gw> 
    <gw cat="Adv" lemma="κάποτε">κάποτε</gw> 
    <gw cat="V" attrs="ActPrsFucIndSjvSng_C_" lemma="εκφράζω">εκφράζει</gw> 
    <gw cat="PrpArt" attrs="MscNtrSngAcc" lemma="στο">στο</gw> 
    <gw cat="Pcp" attrs="PsvPfcMscSngAcc + PsvPfcNtrSngNomAccVoc" lemma="κείµαι">κείµενό</gw> 
    <gw cat="Art" attrs="MscNtrSngGen" lemma="ο">του</gw> 
 243
Παράρτηµα 
    <gw cat="Arith" attrs="MscSngNom" lemma="ένας">ένας</gw> 
    <gw cat="N" attrs="MscSngNom" lemma="επιστήµονας">επιστήµονας</gw> 
    <punc>.</punc> 
  </s> 
  <s> 
    <gw cat="Clt" attrs="FemSngAcc_C_" lemma="εγώ">Την</gw> 
    <gw cat="V" attrs="ActPrsFucIndSjvSng_C_" lemma="εκφράζω">εκφράζει</gw> 
    <gw cat="Cnj" lemma="επειδή">επειδή</gw> 
    <gw cat="V" attrs="PsvPrsFucIndSng_C_" lemma="συναισθάνοµαι">συναισθάνεται</gw> 
    <gw cat="Art" attrs="NtrPlrNomAcc" lemma="ο">τα</gw> 
    <gw cat="N" attrs="NtrPlrNomAccVoc" lemma="όριο">όρια</gw> 
    <gw cat="Art" attrs="MscNtrSngGen" lemma="ο">του</gw> 
    <gw cat="Prn" attrs="MscSngGen" lemma="εαυτό">εαυτού</gw> 
    <gw cat="Clt" attrs="MscNtrSng_C_" lemma="δικός">του</gw> 
    <punc>,</punc> 
    <gw cat="Art" attrs="MscNtrSngGen" lemma="ο">του</gw> 
    <gw cat="Adj" attrs="MscNtrSngGen" lemma="συγκεκριµένος">συγκεκριµένου</gw> 
    <gw cat="N" attrs="NtrSngGen" lemma="έργο">έργου</gw> 
    <gw cat="Clt" attrs="MscNtrSng_C_" lemma="δικός">του</gw> 
    <punc>,</punc> 
    <gw cat="Art" attrs="MscFemNtrPlrGen" lemma="ο">των</gw> 
    <gw cat="N" attrs="NtrPlrGen" lemma="προσωπικό">προσωπικών</gw> 
    <gw cat="Art" attrs="MscNtrSngGen" lemma="ο">του</gw> 
    <gw cat="N" attrs="FemPlrGen" lemma="θεωρία">θεωριών</gw> 
    <punc>,</punc> 
    <gw cat="Adv" lemma="ακόµα">ακόµη</gw> 
    <gw cat="Cnj" lemma="και">και</gw> 
    <gw cat="Art" attrs="FemSngGen" lemma="ο">της</gw> 
    <gw cat="N" attrs="FemSngGen" lemma="επιστήµη">επιστήµης</gw> 
    <gw cat="Clt" attrs="MscNtrSng_C_" lemma="δικός">του</gw> 
    <punc>.</punc> 
 244
Παράρτηµα 
  </s> 
    <gw cat="Art" attrs="FemSngAcc" lemma="ο">τη</gw> 
  <s> 
    <gw cat="Cnj" lemma="αλλά">Αλλά</gw> 
    <gw cat="V" attrs="ActPrsFucIndSjvSng_C_" lemma="παραµένω">παραµένει</gw> 
    <gw cat="Art" attrs="FemSngNom" lemma="ο">η</gw> 
    <gw cat="N" attrs="FemSngNomAccVoc" lemma="ανάγκη">ανάγκη</gw> 
    <gw cat="Pcl" lemma="να">να</gw> 
    <gw cat="V" attrs="PsvPrsFucIndSjvSngPlr_C_" lemma="είµαι">είναι</gw> 
    <gw cat="Adj" attrs="MscFemPlrNomAccVoc" lemma="σαφής">σαφείς</gw> 
    <gw cat="Art" attrs="MscFemPlrNom" lemma="ο">οι</gw> 
    <gw cat="N" attrs="FemPlrNomAccVoc" lemma="θεωρία">θεωρίες</gw> 
    <gw cat="Clt" attrs="MscNtrSng_C_" lemma="δικός">του</gw> 
    <punc>,</punc> 
    <gw cat="Adj" attrs="NtrSngNomAccVoc" lemma="σαφής">σαφές</gw> 
    <gw cat="Cnj" lemma="και">και</gw> 
    <gw cat="Art" attrs="MscSngAcc + NtrSngNomAcc" lemma="ο">το</gw> 
    <gw cat="Pcp" attrs="PsvPfcMscSngAcc + PsvPfcNtrSngNomAccVoc" lemma="κείµαι">κείµενό</gw> 
    <gw cat="Clt" attrs="MscNtrSng_C_" lemma="δικός">του</gw> 
    <punc>.</punc> 
  </s> 
  <s> 
    <gw cat="Adv" lemma="έτσι">Έτσι</gw> 
    <punc>,</punc> 
    <gw cat="Art" attrs="MscSngNom" lemma="ο">ο</gw> 
    <gw cat="N" attrs="MscSngNom" lemma="συγγραφέας">συγγραφέας</gw> 
    <gw cat="Prp" lemma="από">από</gw> 
    <gw cat="Arith" attrs="FemSngNomAcc" lemma="ένας">µια</gw> 
    <gw cat="V" attrs="ActPrsFucIndSng_C_ + ActPrsSjvSng_C_" lemma="καταγράφω">καταγράφει</gw> 
    <gw cat="Art" attrs="FemSngAcc" lemma="ο">την</gw> 
    <gw cat="N" attrs="FemSngNomAccVoc" lemma="αµφιβολία">αµφιβολία</gw> 
 245
Παράρτηµα 
    <punc>,</punc> 
    <gw cat="Pcl" lemma="δε">δεν</gw> 
    <gw cat="Prp" lemma="από">από</gw> 
    <gw cat="Art" attrs="FemSngAcc" lemma="ο">την</gw> 
    <gw cat="Prn" attrs="FemSngNomAcc" lemma="άλλος">άλλη</gw> 
    <gw cat="Cnj" lemma="όµως">όµως</gw> 
    <gw cat="V" attrs="ActPrsFucIndSjvSng_C_" lemma="υποστηρίζω">υποστηρίζει</gw> 
    <gw cat="Prp" lemma="µε">µε</gw> 
    <gw cat="N" attrs="FemSngNomAccVoc" lemma="σαφήνεια">σαφήνεια</gw> 
    <gw cat="Art" attrs="FemSngAcc" lemma="ο">τη</gw> 
    <gw cat="Adj" attrs="FemSngNomAccVoc" lemma="συλλογιστικός">συλλογιστική</gw> 
    <gw cat="Clt" attrs="MscNtrSng_C_" lemma="δικός">του</gw> 
    <punc>,</punc> 
    <gw cat="Art" attrs="FemPlrAcc" lemma="ο">τις</gw> 
    <gw cat="N" attrs="FemPlrNomAccVoc" lemma="άποψη">απόψεις</gw> 
    <gw cat="Cnj" lemma="και">και</gw> 
    <gw cat="Art" attrs="FemPlrAcc" lemma="ο">τις</gw> 
    <gw cat="N" attrs="FemPlrNomAccVoc" lemma="ερµηνεία">ερµηνείες</gw> 
    <gw cat="Clt" attrs="MscNtrSng_C_" lemma="δικός">του</gw> 
    <punc>:</punc> 
    <gw cat="Cnj" lemma="επειδή">επειδή</gw> 
    <gw cat="Art" attrs="MscSngNom" lemma="ο">ο</gw> 
    <gw cat="Adj" attrs="MscSngNom" lemma="επιστηµονικός">επιστηµονικός</gw> 
    <gw cat="N" attrs="MscSngNom" lemma="λόγος">λόγος</gw> 
    <punc>,</punc> 
    <gw cat="Prp" lemma="εξ">εξ</gw> 
    <gw cat="N" attrs="MscSngGen" lemma="ορισµός">ορισµού</gw> 
    <punc>,</punc> 
    <gw cat="V" attrs="PsvPrsFucIndSjvSng_C_" lemma="επιδέχοµαι">επιδέχεται</gw> 
    <gw cat="N" attrs="FemPlrNomAccVoc" lemma="αντίφαση">αντιφάσεις</gw> 
    <punc>.</punc> 
 246
Παράρτηµα 
  </s> 
</p> 
    <gw cat="V" attrs="PsvPrsFucIndSjvPlr_C_" lemma="εξαρτώ">εξαρτώνται</gw> 
<p> 
  <s> 
    <gw cat="Adv" lemma="όπως">Όπως</gw> 
    <gw cat="V" attrs="PsvPrsFucIndSjvSngPlr_C_" lemma="είµαι">είναι</gw> 
    <gw cat="Adj" attrs="MscSngAcc + NtrSngNomAccVoc" lemma="φυσικός">φυσικό</gw> 
    <punc>,</punc> 
    <gw cat="Art" attrs="MscSngNom" lemma="ο">ο</gw> 
    <gw cat="N" attrs="MscSngNom" lemma="κανόνας">κανόνας</gw> 
    <gw cat="Art" attrs="FemSngGen" lemma="ο">της</gw> 
    <gw cat="N" attrs="FemSngGen" lemma="σαφήνεια">σαφήνειας</gw> 
    <gw cat="Pcl" lemma="δε">δεν</gw> 
    <gw cat="V" attrs="ActPrsFucIndSng_C_ + ActPrsSjvSng_C_" lemma="έχω">έχει</gw> 
    <gw cat="Adv" lemma="ενιαία">ενιαία</gw> 
    <gw cat="N" attrs="FemSngNomAccVoc" lemma="εφαρµογή">εφαρµογή</gw> 
    <punc>.</punc> 
  </s> 
  <s> 
    <gw cat="V" attrs="ActPrsFucIndSjvPlr_C_" lemma="υπάρχω">Υπάρχουν</gw> 
    <gw cat="Art" attrs="MscFemPlrNom" lemma="ο">οι</gw> 
    <gw cat="N" attrs="FemPlrNomAccVoc" lemma="διαφοροποίηση">διαφοροποιήσεις</gw> 
    <gw cat="Adv" lemma="που">που</gw> 
    <gw cat="Prp" lemma="από">από</gw> 
    <gw cat="Art" attrs="FemSngAcc" lemma="ο">την</gw> 
    <gw cat="N" attrs="FemSngNomAccVoc" lemma="προσωπικότητα">προσωπικότητα</gw> 
    <gw cat="Cnj" lemma="και">και</gw> 
    <gw cat="Art" attrs="FemPlrAcc" lemma="ο">τις</gw> 
    <gw cat="N" attrs="FemPlrNomAccVoc" lemma="ικανότητα">ικανότητες</gw> 
    <gw cat="Art" attrs="MscNtrSngGen" lemma="ο">του</gw> 
 247
Παράρτηµα 
    <gw cat="Prn" attrs="NtrSngPlrNomGenAcc" lemma="καθετί">κάθε</gw> 
    <gw cat="N" attrs="MscSngGenAccVoc" lemma="συγγραφέας">συγγραφέα</gw> 
    <punc>.</punc> 
  </s> 
  <s> 
    <gw cat="Arith" attrs="MscSngNom" lemma="ένας">Ένας</gw> 
    <gw cat="N" attrs="MscSngNom" lemma="επιστήµονας">επιστήµονας</gw> 
    <gw cat="Prp" lemma="µε">µε</gw> 
    <gw cat="Adj" attrs="MscSngAcc + NtrSngNomAccVoc" lemma="καλός">καλό</gw> 
    <gw cat="Adj" attrs="MscSngAcc + NtrSngNomAccVoc" lemma="συγγραφικός">συγγραφικό</gw> 
    <gw cat="N" attrs="NtrSngNomAccVoc" lemma="ταλέντο">ταλέντο</gw> 
    <gw cat="V" attrs="ActPrsFucIndSjvSng_C_" lemma="µπορώ">µπορεί</gw> 
    <gw cat="Pcl" lemma="ίσως">ίσως</gw> 
    <gw cat="Pcl" lemma="να">να</gw> 
    <gw cat="V" attrs="ActPstSjvSng_C_ + ActPstInf + ActFutIndSng_C_" lemma="βρίσκω">βρει</gw> 
    <gw cat="N" lemma="?">ελευθεριότερους</gw> 
    <gw cat="N" attrs="MscPlrAcc" lemma="τρόπος">τρόπους</gw> 
    <gw cat="N" attrs="FemSngGen" lemma="παρουσίαση">παρουσίασης</gw> 
    <gw cat="Art" attrs="MscFemNtrPlrGen" lemma="ο">των</gw> 
    <gw cat="N" attrs="FemPlrGen" lemma="ιδέα">ιδεών</gw> 
    <gw cat="Clt" attrs="MscNtrSng_C_" lemma="δικός">του</gw> 
    <punc>,</punc> 
    <gw cat="Pcl" lemma="να">να</gw> 
    <gw cat="V" attrs="PsvPstSjvSng_C_ + PsvPstInf + PsvFutIndSng_C_" lemma="επεκτείνω">επεκταθεί</gw> 
    <gw cat="Prp" lemma="σε">σε</gw> 
    <gw cat="N" attrs="MscPlrAcc" lemma="υπαινιγµός">υπαινιγµούς</gw> 
    <punc>,</punc> 
    <gw cat="Prp" lemma="σε">σε</gw> 
    <gw cat="N" attrs="FemPlrNomAccVoc" lemma="αµφισηµία">αµφισηµίες</gw> 
    <gw cat="Cnj" lemma="και">και</gw> 
    <gw cat="Prp" lemma="σε">σε</gw> 
 248
Παράρτηµα 
    <gw cat="N" attrs="FemPlrNomAccVoc" lemma="αποσιώπηση">αποσιωπήσεις</gw> 
    <gw cat="Adv" lemma="που">που</gw> 
    <gw cat="V" attrs="ActPrsFucIndPlr_C_ + ActPrsSjvPlr_C_" lemma="έχω">έχουν</gw> 
    <gw cat="Art" attrs="FemSngAcc" lemma="ο">τη</gw> 
    <gw cat="Prn" attrs="FemSngNomAcc" lemma="δικός">δική</gw> 
    <gw cat="Clt" attrs="Plr_C_" lemma="δικός">τους</gw> 
    <gw cat="N" attrs="FemSngNomAccVoc" lemma="λειτουργία">λειτουργία</gw> 
    <gw cat="Cnj" lemma="και">και</gw> 
    <gw cat="Adj" attrs="FemSngNomAccVoc" lemma="αισθητικός">αισθητική</gw> 
    <punc>.</punc> 
  </s> 
  <s> 
    <gw cat="Cnj" lemma="αλλά">Αλλά</gw> 
    <gw cat="Prn" attrs="NtrSngNomAcc" lemma="αυτός">αυτό</gw> 
    <gw cat="Pcl" lemma="δε">δεν</gw> 
    <gw cat="V" attrs="ActPrsFucIndSjvSng_C_" lemma="αναιρώ">αναιρεί</gw> 
    <gw cat="Art" attrs="FemSngAcc" lemma="ο">την</gw> 
    <gw cat="Adj" attrs="FemSngNomAccVoc" lemma="επιστηµονικός">επιστηµονική</gw> 
    <gw cat="Clt" attrs="MscNtrSng_C_" lemma="δικός">του</gw> 
    <gw cat="N" attrs="FemSngNomAccVoc" lemma="υποχρέωση">υποχρέωση</gw> 
    <gw cat="Pcl" lemma="να">να</gw> 
    <gw cat="V" attrs="ActPstSjvSng_C_ + ActPstInf + ActFutIndSng_C_" lemma="δείχνω">δείξει</gw> 
    <gw cat="Prp" lemma="µε">µε</gw> 
    <gw cat="N" attrs="FemSngNomAccVoc" lemma="σαφήνεια">σαφήνεια</gw> 
    <punc>,</punc> 
    <gw cat="Prp" lemma="σε">σε</gw> 
    <gw cat="Prn" attrs="NtrPlrNomAcc" lemma="άλλος">άλλα</gw> 
    <gw cat="N" attrs="NtrPlrNomAccVoc" lemma="σηµείο">σηµεία</gw> 
    <gw cat="Art" attrs="MscNtrSngGen" lemma="ο">του</gw> 
    <gw cat="Pcp" attrs="PsvPfcMscNtrSngGen" lemma="κείµαι">κειµένου</gw> 
    <punc>,</punc> 
 249
Παράρτηµα 
    <gw cat="Art" attrs="FemPlrAcc" lemma="ο">τις</gw> 
    <gw cat="N" attrs="FemPlrNomAccVoc" lemma="άποψη">απόψεις</gw> 
    <gw cat="Cnj" lemma="και">και</gw> 
    <gw cat="Art" attrs="FemPlrAcc" lemma="ο">τις</gw> 
    <gw cat="N" attrs="FemPlrNomAccVoc" lemma="ερµηνεία">ερµηνείες</gw> 
    <gw cat="Clt" attrs="MscNtrSng_C_" lemma="δικός">του</gw> 
    <punc>.</punc> 
  </s> 
</p> 
<p> 
  <s> 
    <gw cat="V" attrs="ActPrsFucIndSjvPlr_C_" lemma="υπάρχω">Υπάρχουν</gw> 
    <gw cat="Adv" lemma="έπειτα">έπειτα</gw> 
    <gw cat="N" attrs="FemPlrNomAccVoc" lemma="διαφοροποίηση">διαφοροποιήσεις</gw> 
    <gw cat="Adj" attrs="FemPlrNomAccVoc" lemma="ανάλογος">ανάλογες</gw> 
    <gw cat="Prp" lemma="µε">µε</gw> 
    <gw cat="Art" attrs="NtrPlrNomAcc" lemma="ο">τα</gw> 
    <gw cat="Adj" attrs="NtrPlrNomAccVoc" lemma="γνωστικός">γνωστικά</gw> 
    <gw cat="N" attrs="NtrPlrNomAccVoc" lemma="αντικείµενο">αντικείµενα</gw> 
    <gw cat="Cnj" lemma="και">και</gw> 
    <gw cat="Art" attrs="NtrPlrNomAcc" lemma="ο">τα</gw> 
    <gw cat="N" attrs="NtrPlrNomAccVoc" lemma="είδος">είδη</gw> 
    <gw cat="Art" attrs="MscNtrSngGen" lemma="ο">του</gw> 
    <gw cat="Adj" attrs="MscNtrSngGen" lemma="γραπτός">γραπτού</gw> 
    <gw cat="Adj" attrs="MscNtrSngGen" lemma="επιστηµονικός">επιστηµονικού</gw> 
    <gw cat="N" attrs="MscSngGen" lemma="λόγος">λόγου</gw> 
    <punc>.</punc> 
  </s> 
  <s> 
    <gw cat="Art" attrs="FemSngNom" lemma="ο">Η</gw> 
    <gw cat="N" attrs="FemSngNomAccVoc" lemma="ιστορία">Ιστορία</gw> 
 250
Παράρτηµα 
    <punc>,</punc> 
    <gw cat="PrpArt" attrs="MscSngAcc" lemma="στο">στον</gw> 
    <gw cat="N" attrs="FemSngNomAccVoc" lemma="επιστήµη">επιστήµη</gw> 
    <gw cat="Adj" attrs="FemPlrNomAccVoc" lemma="εσωτερικός">εσωτερικές</gw> 
    <abbr>π.χ.</abbr> 
    <punc>,</punc> 
    <gw cat="V" attrs="ActPrsFucIndSng_C_ + ActPrsSjvSng_C_" lemma="αφήνω">αφήνει</gw> 
    <gw cat="Adj" attrs="FemPlrNomAccVoc" lemma="πολύς">περισσότερες</gw> 
    <gw cat="Adj" attrs="FemPlrNomAccVoc" lemma="υφολογικός">υφολογικές</gw> 
    <gw cat="N" attrs="FemPlrNomAccVoc" lemma="δυνατότητα">δυνατότητες</gw> 
    <gw cat="N" attrs="MscSngGenAccVoc" lemma="συγγραφέας">συγγραφέα</gw> 
    <gw cat="Prp" lemma="από">από</gw> 
    <gw cat="Prn" attrs="FemSngNomAcc + NtrPlrNomAcc" lemma="οποιοσδήποτε">οποιαδήποτε</gw> 
    <gw cat="Prn" attrs="FemSngNomAcc" lemma="άλλος">άλλη</gw> 
    <punc>.</punc> 
  </s> 
  <s> 
    <gw cat="Clt" attrs="MscNtrSngGen_C_" lemma="εγώ">Του</gw> 
    <gw cat="V" attrs="ActPrsFucIndSng_C_ + ActPrsSjvSng_C_" lemma="επιτρέπω">επιτρέπει</gw> 
    <punc>,</punc> 
    <gw cat="Adv" lemma="κάποτε">κάποτε</gw> 
    <gw cat="Clt" attrs="MscNtrSng_C_" lemma="δικός">του</gw> 
    <gw cat="V" attrs="ActPrsFucIndSjvSng_C_" lemma="επιβάλλω">επιβάλλει</gw> 
    <gw cat="Adv" lemma="κιόλας">κιόλας</gw> 
    <punc>,</punc> 
    <gw cat="Pcl" lemma="να">να</gw> 
    <gw cat="V" attrs="ActPstSjvSng_C_ + ActPstInf + ActFutIndSng_C_" lemma="αναδεικνύω">αναδείξει</gw> 
    <gw cat="Art" attrs="FemPlrAcc" lemma="ο">τις</gw> 
    <gw cat="N" attrs="FemPlrNomAccVoc" lemma="αντίφαση">αντιφάσεις</gw> 
    <gw cat="Art" attrs="MscNtrSngGen" lemma="ο">του</gw> 
    <gw cat="N" attrs="MscSngGen" lemma="άνθρωπος">ανθρώπου</gw> 
 251
Παράρτηµα 
    <gw cat="Cnj" lemma="και">και</gw> 
    <gw cat="Art" attrs="MscFemNtrPlrGen" lemma="ο">των</gw> 
    <gw cat="Adj" attrs="MscNtrPlrGen" lemma="ανθρώπινος">ανθρωπίνων</gw> 
    <gw cat="N" attrs="FemPlrGen" lemma="κοινωνία">κοινωνιών</gw> 
    <punc>·</punc> 
    <gw cat="Art" attrs="MscSngAcc" lemma="ο">τον</gw> 
    <gw cat="N" attrs="MscSngAcc" lemma="ρόλος">ρόλο</gw> 
    <gw cat="Art" attrs="MscFemNtrPlrGen" lemma="ο">των</gw> 
    <gw cat="Adj" attrs="MscNtrPlrGen" lemma="ανθρώπινος">ανθρωπίνων</gw> 
    <gw cat="N" attrs="NtrPlrGen" lemma="πάθος">παθών</gw> 
    <punc>·</punc> 
    <gw cat="Art" attrs="FemSngAcc" lemma="ο">τη</gw> 
    <gw cat="N" attrs="FemSngNomAccVoc" lemma="σηµασία">σηµασία</gw> 
    <gw cat="Art" attrs="MscFemNtrPlrGen" lemma="ο">των</gw> 
    <gw cat="N" attrs="FemPlrGen" lemma="σύµπτωση">συµπτώσεων</gw> 
    <gw cat="Cnj" lemma="και">και</gw> 
    <gw cat="Art" attrs="FemSngGen" lemma="ο">της</gw> 
    <gw cat="N" attrs="FemSngGen" lemma="τύχη">τύχης</gw> 
    <punc>·</punc> 
    <gw cat="Art" attrs="MscSngAcc + NtrSngNomAcc" lemma="ο">το</gw> 
    <gw cat="N" attrs="NtrSngNomAccVoc" lemma="βάρος">βάρος</gw> 
    <gw cat="Art" attrs="MscFemNtrPlrGen" lemma="ο">των</gw> 
    <gw cat="Adj" attrs="MscFemNtrPlrGen" lemma="µαζικός">µαζικών</gw> 
    <gw cat="Adj" attrs="MscFemNtrPlrGen" lemma="κοινωνικός">κοινωνικών</gw> 
    <gw cat="N" attrs="FemPlrGen" lemma="δύναµη">δυνάµεων</gw> 
    <punc>·</punc> 
    <gw cat="Art" attrs="MscPlrAcc" lemma="ο">τους</gw> 
    <gw cat="Adj" attrs="MscPlrAcc" lemma="αναπόδραστος">αναπόδραστους</gw> 
    <gw cat="N" attrs="MscPlrAcc" lemma="φραγµός">φραγµούς</gw> 
    <gw cat="Art" attrs="FemSngGen" lemma="ο">της</gw> 
    <gw cat="N" attrs="FemSngGen" lemma="φύση">φύσης</gw> 
 252
Παράρτηµα 
    <punc>.</punc> 
  </s> 
</p> 
    <gw cat="Pcl" lemma="δε">δεν</gw> 
    <gw cat="Prp" lemma="µε">µε</gw> 
<p> 
  <s> 
    <gw cat="Cnj" lemma="ωστόσο">Ωστόσο</gw> 
    <punc>,</punc> 
    <gw cat="Art" attrs="MscSngNom" lemma="ο">ο</gw> 
    <gw cat="Adj" attrs="MscSngNom" lemma="ιστορικός">ιστορικός</gw> 
    <gw cat="V" attrs="ActPrsFucIndSjvSng_C_" lemma="δείχνω">δείχνει</gw> 
    <gw cat="Art" attrs="FemPlrAcc" lemma="ο">τις</gw> 
    <gw cat="N" attrs="FemPlrNomAccVoc" lemma="αντίφαση">αντιφάσεις</gw> 
    <gw cat="N" attrs="FemSngGen" lemma="ουσία">ουσίας</gw> 
    <gw cat="Prp" lemma="µε">µε</gw> 
    <gw cat="N" attrs="FemPlrNomAccVoc" lemma="αντίφαση">αντιφάσεις</gw> 
    <gw cat="N" attrs="NtrSngGen" lemma="ύφος">ύφους</gw> 
    <punc>,</punc> 
    <gw cat="Cnj" lemma="αλλά">αλλά</gw> 
    <gw cat="Prp" lemma="µε">µε</gw> 
    <gw cat="N" attrs="FemSngNomAccVoc" lemma="σαφήνεια">σαφήνεια</gw> 
    <punc>.</punc> 
  </s> 
  <s> 
    <gw cat="Art" attrs="NtrPlrNomAcc" lemma="ο">Τα</gw> 
    <gw cat="N" attrs="NtrPlrNomAccVoc" lemma="πάθος">πάθη</gw> 
    <gw cat="Pcl" lemma="δε">δεν</gw> 
    <gw cat="Clt" attrs="NtrPlrNomAcc_C_" lemma="εγώ">τα</gw> 
    <gw cat="V" attrs="ActPrsFucIndSjvSng_C_" lemma="δείχνω">δείχνει</gw> 
    <gw cat="Adj" lemma="?">ψευδοροµαντική</gw> 
 253
Παράρτηµα 
    <gw cat="N" attrs="FemSngNomAccVoc" lemma="ασάφεια">ασάφεια</gw> 
    <punc>,</punc> 
    <gw cat="Cnj" lemma="αλλά">αλλά</gw> 
    <gw cat="Prp" lemma="µε">µε</gw> 
    <gw cat="Art" attrs="FemSngAcc" lemma="ο">τη</gw> 
    <gw cat="N" attrs="FemSngNomAccVoc" lemma="σαφήνεια">σαφήνεια</gw> 
    <gw cat="Prn" attrs="FemSngNomAcc" lemma="εκείνος">εκείνη</gw> 
    <gw cat="Adv" lemma="που">που</gw> 
    <gw cat="Pcl" lemma="θα">θα</gw> 
    <gw cat="V" attrs="ActPstSjvSng_C_ + ActPstInf + ActFutIndSng_C_" lemma="αναδεικνύω">αναδείξει</gw> 
    <gw cat="Art" attrs="FemSngAcc" lemma="ο">την</gw> 
    <gw cat="N" attrs="FemSngNomAccVoc" lemma="αιχµηρότητα">αιχµηρότητά</gw> 
    <gw cat="Clt" attrs="Plr_C_" lemma="δικός">τους</gw> 
    <punc>.</punc> 
  </s> 
  <s> 
    <gw cat="V" attrs="ActPrsFucIndSjvSng_C_" lemma="τονίζω">Τονίζει</gw> 
    <gw cat="Art" attrs="FemPlrAcc" lemma="ο">τις</gw> 
    <gw cat="N" attrs="FemPlrNomAccVoc" lemma="σύµπτωση">συµπτώσεις</gw> 
    <gw cat="Cnj" lemma="και">και</gw> 
    <gw cat="Art" attrs="FemSngAcc" lemma="ο">την</gw> 
    <gw cat="N" lemma="?">τυχαιότητα</gw> 
    <gw cat="Prp" lemma="µε">µε</gw> 
    <gw cat="N" attrs="NtrSngNomAccVoc" lemma="ύφος">ύφος</gw> 
    <gw cat="Adj" attrs="NtrSngNomAccVoc" lemma="σαφής">σαφές</gw> 
    <gw cat="Cnj" lemma="και">και</gw> 
    <gw cat="Pcl" lemma="όχι">όχι</gw> 
    <gw cat="Adj" attrs="MscSngAcc + NtrSngNomAccVoc" lemma="τυχάρπαστος">τυχάρπαστο</gw> 
    <punc>.</punc> 
  </s> 
  <s> 
 254
Παράρτηµα 
    <gw cat="Art" attrs="FemSngAcc" lemma="ο">Τη</gw> 
    <symb>«</symb> 
    <gw cat="N" attrs="FemSngNomAccVoc" lemma="µοίρα">µοίρα</gw> 
    <symb>»</symb> 
    <gw cat="Pcl" lemma="δε">δεν</gw> 
    <gw cat="Clt" attrs="FemSngAcc_C_" lemma="εγώ">την</gw> 
    <gw cat="V" attrs="ActPrsFucIndSjvSng_C_" lemma="αποδίδω">αποδίδει</gw> 
    <gw cat="Prp" lemma="σε">σε</gw> 
    <gw cat="Adj" attrs="FemPlrNomAccVoc" lemma="µεταφυσικός">µεταφυσικές</gw> 
    <gw cat="N" attrs="FemPlrNomAccVoc" lemma="δύναµη">δυνάµεις</gw> 
    <unk>-</unk> 
    <gw cat="Cnj" lemma="εφόσον">εφόσον</gw> 
    <gw cat="N" attrs="FemPlrNomAccVoc" lemma="µεταφυσική">µεταφυσικές</gw> 
    <gw cat="V" attrs="ActPrsFucIndSjvSng_C_ + ActPstInf + ActFutIndSng_C_ + ActPstSjvSng_C_" 
lemma="κάνω">κάνει</gw> 
    <gw cat="N" attrs="FemSngNomAccVoc" lemma="επιστήµη">επιστήµη</gw> 
    <punc>.</punc> 
  </s> 
  <s> 
    <gw cat="V" attrs="ActPrsFucIndSjvSng_C_" lemma="µπορώ">Μπορεί</gw> 
    <gw cat="Pcl" lemma="να">να</gw> 
    <gw cat="Clt" attrs="FemSngAcc_C_" lemma="εγώ">την</gw> 
    <gw cat="V" attrs="ActPrsFucIndSjvSng_C_" lemma="ταυτίζω">ταυτίζει</gw> 
    <gw cat="Prp" lemma="µε">µε</gw> 
    <gw cat="N" attrs="FemPlrNomAccVoc" lemma="δύναµη">δυνάµεις</gw> 
    <gw cat="Adv" lemma="που">που</gw> 
    <gw cat="V" attrs="ActPscIndPlr_C_" lemma="θεωρώ">θεωρούσαν</gw> 
    <gw cat="Adj" attrs="FemPlrNomAccVoc" lemma="ανεξήγητος">ανεξήγητες</gw> 
    <gw cat="Cnj" lemma="και">και</gw> 
    <gw cat="Art" attrs="MscFemPlrNom" lemma="ο">οι</gw> 
    <gw cat="N" attrs="MscPlrNomVoc" lemma="άνθρωπος">άνθρωποι</gw> 
 255
Παράρτηµα 
    <gw cat="Adv" lemma="που">που</gw> 
    <gw cat="V" attrs="ActPrsFucIndSjvSng_C_" lemma="µελετώ">µελετά</gw> 
    <punc>·</punc> 
    <gw cat="Cnj" lemma="αλλά">αλλά</gw> 
    <gw cat="Art" attrs="MscSngNom" lemma="ο">ο</gw> 
    <gw cat="Prn" attrs="MscSngNom" lemma="ίδιος">ίδιος</gw> 
    <gw cat="V" attrs="ActPrsFucIndSjvSng_C_" lemma="δίνω">δίνει</gw> 
    <gw cat="N" attrs="NtrSngNomAccVoc" lemma="όνοµα">όνοµα</gw> 
    <gw cat="PrpArt" attrs="FemPlrAcc" lemma="στο">στις</gw> 
    <gw cat="N" attrs="FemPlrNomAccVoc" lemma="δύναµη">δυνάµεις</gw> 
    <gw cat="Prn" attrs="FemPlrNomAcc" lemma="αυτός">αυτές</gw> 
    <punc>·</punc> 
    <gw cat="Cnj" lemma="και">και</gw> 
    <gw cat="Art" attrs="FemPlrAcc" lemma="ο">τις</gw> 
    <gw cat="V" attrs="ActPrsFucIndSjvSng_C_" lemma="εντάσσω">εντάσσει</gw> 
    <punc>,</punc> 
    <gw cat="Prp" lemma="µε">µε</gw> 
    <gw cat="N" attrs="FemSngNomAccVoc" lemma="σαφήνεια">σαφήνεια</gw> 
    <punc>,</punc> 
    <gw cat="Prp" lemma="σε">σε</gw> 
    <gw cat="Arith" attrs="MscSngAcc" lemma="ένας">έναν</gw> 
    <gw cat="N" lemma="?">αιτιακό</gw> 
    <gw cat="N" attrs="MscSngAcc" lemma="συλλογισµός">συλλογισµό</gw> 
    <punc>,</punc> 
    <gw cat="Prp" lemma="σε">σε</gw> 
    <gw cat="Arith" attrs="MscSngAcc + NtrSngNomAcc" lemma="ένας">ένα</gw> 
    <gw cat="Adj" attrs="MscSngAcc + NtrSngNomAccVoc" lemma="ερµηνευτικός">ερµηνευτικό</gw> 
    <gw cat="N" attrs="NtrSngNomAccVoc" lemma="σχήµα">σχήµα</gw> 
    <punc>.</punc> 
  </s> 
</p> 
 256
Παράρτηµα 
<p> 
  <s> 
    <gw cat="Arith" attrs="MscSngAcc + NtrSngNomAcc" lemma="ένας">Ένα</gw> 
    <gw cat="Adj" attrs="NtrSngNomAccVoc" lemma="ευτυχής">ευτυχές</gw> 
    <gw cat="Adj" attrs="MscSngAcc + NtrSngNomAccVoc" lemma="ιστοριογραφικός">ιστοριογραφικό</gw> 
    <gw cat="N" attrs="NtrSngNomAccVoc" lemma="έργο">έργο</gw> 
    <gw cat="V" attrs="ActPrsFucIndSjvSng_C_" lemma="απαιτώ">απαιτεί</gw> 
    <gw cat="Arith" attrs="MscSngAcc" lemma="ένας">έναν</gw> 
    <gw cat="Adj" attrs="MscSngAcc + NtrSngNomAccVoc" lemma="καλός">καλό</gw> 
    <gw cat="N" attrs="MscSngAcc" lemma="συγκερασµός">συγκερασµό</gw> 
    <gw cat="Art" attrs="FemSngGen" lemma="ο">της</gw> 
    <gw cat="N" attrs="FemSngNomAccVoc" lemma="υπέρβαση">υπέρβαση</gw> 
    <gw cat="N" attrs="FemSngGen" lemma="επιστήµη">επιστήµης</gw> 
    <gw cat="Prp" lemma="µε">µε</gw> 
    <gw cat="Art" attrs="FemSngAcc" lemma="ο">την</gw> 
    <gw cat="N" attrs="FemSngNomAccVoc" lemma="τέχνη">τέχνη</gw> 
    <gw cat="Art" attrs="MscNtrSngGen" lemma="ο">του</gw> 
    <gw cat="N" attrs="NtrSngGen" lemma="ύφος">ύφους</gw> 
    <punc>.</punc> 
  </s> 
  <s> 
    <gw cat="Prp" lemma="από">Από</gw> 
    <gw cat="Adv" lemma="εκεί">εκεί</gw> 
    <gw cat="Cnj" lemma="και">και</gw> 
    <gw cat="Adv" lemma="πέρα">πέρα</gw> 
    <punc>,</punc> 
    <gw cat="V" attrs="ActPrsFucIndSjvSng_C_" lemma="υπάρχω">υπάρχει</gw> 
    <gw cat="Adv" lemma="µόνο">µόνο</gw> 
    <gw cat="Art" attrs="FemSngNom" lemma="ο">η</gw> 
    <gw cat="Cnj" lemma="και">και</gw> 
    <gw cat="Art" attrs="FemSngGen" lemma="ο">της</gw> 
 257
Παράρτηµα 
    <gw cat="N" attrs="FemSngGen" lemma="επιστήµη">επιστήµης</gw> 
    <gw cat="Cnj" lemma="και">και</gw> 
    <gw cat="Art" attrs="MscNtrSngGen" lemma="ο">του</gw> 
    <gw cat="N" attrs="NtrSngGen" lemma="ύφος">ύφους</gw> 
    <punc>.</punc> 
  </s> 
  <s> 
    <gw cat="PrpArt" attrs="MscSngAcc" lemma="στο">Στον</gw> 
    <gw cat="Adj" attrs="MscSngAcc + NtrSngNomAccVoc" lemma="υπερβατικός">υπερβατικό</gw> 
    <gw cat="Prn" attrs="NtrSngNomAcc" lemma="αυτός">αυτό</gw> 
    <gw cat="N" attrs="MscSngAcc" lemma="χώρος">χώρο</gw> 
    <punc>,</punc> 
    <gw cat="Adv" lemma="εκεί">εκεί</gw> 
    <gw cat="Adv" lemma="όπου">όπου</gw> 
    <gw cat="Art" attrs="MscSngNom" lemma="ο">ο</gw> 
    <gw cat="N" attrs="MscSngNom" lemma="συγκερασµός">συγκερασµός</gw> 
    <gw cat="V" attrs="PsvPrsFucIndSjvSng_C_" lemma="γίνοµαι">γίνεται</gw> 
    <gw cat="N" attrs="FemSngNomAccVoc" lemma="ταύτιση">ταύτιση</gw> 
    <gw cat="N" attrs="FemSngGen" lemma="γνώση">γνώσης</gw> 
    <gw cat="Cnj" lemma="και">και</gw> 
    <gw cat="N" attrs="FemSngGen" lemma="τέχνη">τέχνης</gw> 
    <punc>,</punc> 
    <gw cat="V" attrs="ActPrsFucIndSjvSng_C_" lemma="οδηγώ">οδηγεί</gw> 
    <gw cat="Arith" attrs="MscSngNom" lemma="ένας">ένας</gw> 
    <gw cat="N" attrs="MscSngNom" lemma="δρόµος">δρόµος</gw> 
    <gw cat="Adv" lemma="σχεδόν">σχεδόν</gw> 
    <gw cat="Adj" attrs="MscSngNom" lemma="άβατος">άβατος</gw> 
    <punc>.</punc> 
  </s> 
  <s> 
    <gw cat="N" attrs="MscSngNom" lemma="τόπος">Τόπος</gw> 
 258
Παράρτηµα 
    <gw cat="Adv" lemma="που">που</gw> 
    <gw cat="V" attrs="PsvPrsFucIndPlr_C_ + PsvPrsSjvPlr_C_" lemma="ονειρεύοµαι">ονειρεύονται</gw> 
    <punc>,</punc> 
  </s> 
    <gw cat="V" attrs="ActPstSjvSng_C_ + ActPstInf + ActFutIndSng_C_" lemma="δείχνω">δείξει</gw> 
    <gw cat="Adj" attrs="MscPlrNomVoc" lemma="λίγος">ελάχιστοι</gw> 
    <gw cat="Adv" lemma="που">που</gw> 
    <gw cat="Adj" attrs="MscPlrNomVoc" lemma="πολύς">πολλοί</gw> 
    <punc>,</punc> 
    <gw cat="N" attrs="MscPlrNomAccVoc" lemma="επιστήµονας">επιστήµονες</gw> 
    <gw cat="Cnj" lemma="και">και</gw> 
    <gw cat="N" attrs="MscPlrNomAccVoc" lemma="τεχνίτης">τεχνίτες</gw> 
    <punc>,</punc> 
    <gw cat="N" attrs="MscSngNom" lemma="τόπος">τόπος</gw> 
    <gw cat="Adj" attrs="MscSngNom" lemma="άφθαστος">άφθαστος</gw> 
    <gw cat="Prp" lemma="για">για</gw> 
    <gw cat="Clt" attrs="PlrGenAcc_A_" lemma="εγώ">µας</gw> 
    <gw cat="Art" attrs="MscPlrAcc" lemma="ο">τους</gw> 
    <gw cat="Adj" attrs="MscPlrAcc" lemma="πολύς">πολλούς</gw> 
    <unk>-</unk> 
    <gw cat="Pcl" lemma="όχι">όχι</gw> 
    <gw cat="Cnj" lemma="όµως">όµως</gw> 
    <punc>,</punc> 
    <gw cat="N" attrs="FemSngNomAccVoc" lemma="ουτοπία">ουτοπία</gw> 
    <punc>.</punc> 
  <s> 
    <gw cat="Clt" attrs="Plr_A_" lemma="δικός">Μας</gw> 
    <gw cat="Clt" attrs="MscSngAcc_C_" lemma="εγώ">τον</gw> 
    <gw cat="V" attrs="ActPrsFucIndPlr_C_ + ActPrsSjvPlr_C_" lemma="έχω">έχουν</gw> 
    <gw cat="Art" attrs="MscFemPlrNom" lemma="ο">οι</gw> 
    <gw cat="V" attrs="ActPstIndPlr_C_" lemma="φτάνω">έφτασαν</gw> 
 259
Παράρτηµα 
    <gw cat="Adv" lemma="εκεί">εκεί</gw> 
    <punc>,</punc> 
    <gw cat="Prp" lemma="µε">µε</gw> 
    <punc>,</punc> 
  </s> 
    <gw cat="Art" attrs="MscFemPlrNom" lemma="ο">οι</gw> 
    <gw cat="N" attrs="MscPlrNomVoc" lemma="δάσκαλος">δάσκαλοί</gw> 
    <gw cat="Clt" attrs="Plr_A_" lemma="δικός">µας</gw> 
    <punc>,</punc> 
    <gw cat="Art" attrs="MscSngNom" lemma="ο">ο</gw> 
    <gw cat="Prn" attrs="MscSngNom" lemma="καθένας">καθένας</gw> 
    <gw cat="Art" attrs="FemSngAcc" lemma="ο">τη</gw> 
    <gw cat="Adj" attrs="FemSngNomAccVoc" lemma="µεγάλος">µεγάλη</gw> 
    <gw cat="Cnj" lemma="και">και</gw> 
    <gw cat="Art" attrs="FemSngAcc" lemma="ο">τη</gw> 
    <gw cat="Adj" attrs="FemSngNomAccVoc" lemma="µικρός">µικρή</gw> 
    <gw cat="Clt" attrs="MscNtrSng_C_" lemma="δικός">του</gw> 
    <gw cat="N" attrs="FemSngNomAccVoc" lemma="ιστορία">ιστορία</gw> 
    <gw cat="Pcp" attrs="ActPrsMscSngAcc + ActPrsNtrPlrNomAccVoc" lemma="είµαι">όντα</gw> 
    <gw cat="Adv" lemma="διόλου">διόλου</gw> 
    <gw cat="Adj" attrs="NtrPlrNomAccVoc" lemma="µεταφυσικός">µεταφυσικά</gw> 
    <punc>,</punc> 
    <gw cat="Adv" lemma="πολύ">πολύ</gw> 
    <gw cat="Adv" lemma="ανθρώπινα">ανθρώπινα</gw> 
    <punc>.</punc> 
  <s> 
    <gw cat="Arith" attrs="MscSngNom" lemma="ένας">Ένας</gw> 
    <gw cat="Adj" attrs="MscSngNom" lemma="απλός">απλός</gw> 
    <gw cat="N" attrs="MscSngNom" lemma="άνθρωπος">άνθρωπος</gw> 
    <gw cat="Pcl" lemma="δε">δεν</gw> 
    <gw cat="V" attrs="PsvPscIndSngPlr_C_" lemma="είµαι">ήταν</gw> 
 260
Παράρτηµα 
    <gw cat="Adv" lemma="άραγε">άραγε</gw> 
    <gw cat="Art" attrs="MscSngNom" lemma="ο">ο</gw> 
    <gw cat="N" attrs="MscSngNom" lemma="δάσκαλος">δάσκαλος</gw> 
    <gw cat="Adv" lemma="που">που</gw> 
    <punc>,</punc> 
    <gw cat="Adv" lemma="πριν">πριν</gw> 
    <gw cat="Prp" lemma="από">από</gw> 
    <gw cat="Arith" attrs="MscFemNtrPlrNomGenAccVoc" lemma="δυόµισι">δυόµισι</gw> 
    <gw cat="N" attrs="MscPlrNomAccVoc" lemma="αιώνας">αιώνες</gw> 
    <punc>,</punc> 
    <gw cat="V" attrs="ActPscIndSng_C_ + ActPrsImpSng_B_" lemma="σκαρώνω">σκάρωνε</gw> 
    <gw cat="Prn" attrs="NtrSngPlrNomGenAcc" lemma="καθετί">κάθε</gw> 
    <gw cat="N" attrs="FemSngNomAccVoc" lemma="ηµέρα">µέρα</gw> 
    <gw cat="Art" attrs="FemSngAcc" lemma="ο">τη</gw> 
    <gw cat="N" attrs="FemSngNomAccVoc" lemma="φυγή">φυγή</gw> 
    <gw cat="Clt" attrs="MscNtrSng_C_" lemma="δικός">του</gw> 
    <gw cat="Prp" lemma="προς">προς</gw> 
    <gw cat="Art" attrs="NtrPlrNomAcc" lemma="ο">τα</gw> 
    <gw cat="Adv" lemma="εκεί">εκεί</gw> 
    <punc>,</punc> 
    <gw cat="Prp" lemma="µε">µε</gw> 
    <gw cat="Arith" attrs="MscSngAcc + NtrSngNomAcc" lemma="ένας">ένα</gw> 
    <gw cat="Adj" attrs="MscSngAcc + NtrSngNomAccVoc" lemma="απλός">απλό</gw> 
    <punc>,</punc> 
    <gw cat="Cnj" lemma="αλλά">αλλά</gw> 
    <gw cat="Adv" lemma="καλά">καλώς</gw> 
    <gw cat="Pcp" lemma="?">συγκερασµένο</gw> 
    <gw cat="N" attrs="NtrSngNomAccVoc" lemma="κλειδοκύµβαλο">κλειδοκύµβαλο</gw> 
    <punc>;</punc> 
  </s> 
</p> 
 261
Παράρτηµα 
<s> 
  <gw cat="Art" attrs="MscSngNom" lemma="ο">Ο</gw> 
  <abbr>κ.</abbr> 
  <abbr>Γ.</abbr> 
  <abbr>Β.</abbr> 
  <gw cat="N" lemma="?">∆ερτιλής</gw> 
  <gw cat="V" attrs="PsvPrsFucIndSjvSngPlr_C_" lemma="είµαι">είναι</gw> 
  <gw cat="N" attrs="MscSngNom" lemma="καθηγητής">καθηγητής</gw> 
  <gw cat="Art" attrs="FemSngGen" lemma="ο">της</gw> 
  <gw cat="N" attrs="FemSngGen" lemma="ιστορία">Ιστορίας</gw> 
  <gw cat="PrpArt" attrs="MscNtrSngAcc" lemma="στο">στο</gw> 
  <gw cat="N" attrs="NtrSngNomAccVoc" lemma="πανεπιστήµιο">Πανεπιστήµιο</gw> 
  <gw cat="N" attrs="FemPlrGen" lemma="Αθήνα">Αθηνών</gw> 
  <punc>.</punc> 
</s> 
<s> 
  <symb><</symb> 
  <symb>/</symb> 
  <fw>TEXT</fw> 
  <symb>></symb> 
</s> 
</d> 
 
∆εδοµένου ότι από κάθε κείµενο κρατιούνται µόνο τα ουσιαστικά, τα επίθετα, τα ρήµατα καθώς 
και τα µη κοινότυπα επιρρήµατα., από το παραπάνω κείµενο οι λέξεις οι οποίες κρατήθηκαν για κάθε 
µια από τις προτάσεις και τις παραγράφους του κειµένου είναι οι ακόλουθες: 
 
 
 
 262
Παράρτηµα 
<p> 
 <s> προϋπόθεση καλός ύφος σαφήνεια αναγκαία λογοτεχνία επιστηµονικός γραφή </s> 
 <s> προτίθεµαι διαφορετικός σαφήνεια </s> 
 <s> ποιητικός εξηγηµατική </s> 
 </p> 
<p> 
 <s> σαφήνεια ύφος λογοτέχνης ποιώ πολυσηµία </s> 
 <s> ανοίγω µπροστά αναγνώστης ριπίδι ανάγνωση ευκολύνω διαβάζω ερµηνεύω πολύσηµος κείµαι πολλαπλός τρόπος 
</s> 
 </p> 
<p> 
 <s> συγγραφέας επιστηµονικός έργο κύρια απασχολώ εξαφανίζω σαφήνεια ύφος όλος αµφισηµία πολυσηµία κείµαι 
</s> 
 <s> αποκλείω αµφιβολία αναγνώστης συγγραφέας ισχυρίζοµαι διευκολύνω επιστηµονικός έλεγχος </s> 
 </p> 
<p> 
 <s> πολυσηµία προσπαθώ εκφράζω λογοτέχνης µοιάζω εξάλλου ταυτίζω αµφιβολία εκφράζω κείµαι επιστήµονας 
</s> 
 <s> εκφράζω συναισθάνοµαι όριο συγκεκριµένος έργο προσωπικό θεωρία επιστήµη </s> 
<p> 
 <s> παραµένω ανάγκη σαφής θεωρία σαφής κείµαι </s> 
 <s> συγγραφέας καταγράφω αµφιβολία υποστηρίζω σαφήνεια συλλογιστικός άποψη ερµηνεία επιστηµονικός λόγος 
ορισµός επιδέχοµαι αντίφαση </s> 
 </p> 
 <s> φυσικός κανόνας σαφήνεια ενιαία εφαρµογή </s> 
 <s> υπάρχω διαφοροποίηση εξαρτώ προσωπικότητα ικανότητα συγγραφέας </s> 
 <s> επιστήµονας καλός συγγραφικός ταλέντο µπορώ βρίσκω ελευθεριότερους τρόπος παρουσίαση ιδέα επεκτείνω 
υπαινιγµός αµφισηµία αποσιώπηση λειτουργία αισθητικός </s> 
 <s> αναιρώ επιστηµονικός υποχρέωση δείχνω σαφήνεια σηµείο κείµαι άποψη ερµηνεία </s> 
 </p> 
<p> 
 <s> υπάρχω διαφοροποίηση ανάλογος γνωστικός αντικείµενο είδος γραπτός επιστηµονικός λόγος </s> 
 <s> ιστορία αφήνω πολύς υφολογικός δυνατότητα συγγραφέας επιστήµη </s> 
 <s> επιτρέπω επιβάλλω αναδεικνύω εσωτερικός αντίφαση άνθρωπος ανθρώπινος κοινωνία ρόλος ανθρώπινος πάθος 
 263
Παράρτηµα 
σηµασία σύµπτωση τύχη βάρος µαζικός κοινωνικός δύναµη αναπόδραστος φραγµός φύση </s> 
 </p> 
<p> 
 <s> ιστορικός δείχνω αντίφαση ουσία αντίφαση ύφος σαφήνεια </s> 
 <s> πάθος δείχνω ψευδοροµαντική ασάφεια σαφήνεια αναδεικνύω αιχµηρότητα </s> 
 <s> τονίζω σύµπτωση τυχαιότητα ύφος σαφής τυχάρπαστος </s> 
 <s> µοίρα αποδίδω µεταφυσικός δύναµη κάνω επιστήµη </s> 
 <s> µπορώ ταυτίζω δύναµη θεωρώ ανεξήγητος µεταφυσική άνθρωπος µελετώ δίνω όνοµα δύναµη εντάσσω σαφήνεια 
αιτιακό συλλογισµός ερµηνευτικός σχήµα </s> 
 </p> 
<p> 
 <s> ευτυχής ιστοριογραφικός έργο απαιτώ καλός συγκερασµός επιστήµη τέχνη ύφος </s> 
 <s> υπάρχω υπέρβαση επιστήµη ύφος </s> 
 <s> υπερβατικός χώρος συγκερασµός γίνοµαι ταύτιση γνώση τέχνη οδηγώ δρόµος άβατος </s> 
 <s> τόπος ονειρεύοµαι πολύς επιστήµονας τεχνίτης τόπος άφθαστος πολύς ουτοπία </s> 
 <s> δείχνω λίγος φτάνω δάσκαλος µεγάλος µικρός ιστορία είµαι µεταφυσικός ανθρώπινα </s> 
 <s> απλός άνθρωπος δάσκαλος αιώνας σκαρώνω ηµέρα φυγή απλός συγκερασµένο κλειδοκύµβαλο </s> 
 <s> ∆ερτιλής καθηγητής ιστορία πανεπιστήµιο Αθήνα </s> 
 </p> 
 
 264
