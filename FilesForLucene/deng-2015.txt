ar
X
iv
:1
51
0.
01
31
5v
1 
 [
cs
.C
L
] 
 5
 O
ct
 2
01
5
Rank-frequency relations of phonemes uncover an author-dependency of their usage
Weibing Deng1) and Armen E. Allahverdyan2), ∗
1)Murray Gell-Mann Institute of Complexity Science,
Central China Normal University, Wuhan 430079, China
2)Yerevan Physics Institute, Alikhanian Brothers Street 2, Yerevan 375036, Armenia
We study rank-frequency relations for phonemes in texts written by different authors. We show
that they can be described by generating phonemes via random probabilities governed by the (one-
parameter) Dirichlet density, the simplest density for random probabilities. This description allows
us to demonstrate that the rank-frequency relations for phonemes of a text do depend on the author.
The author-dependency effect is not caused by common words used in different texts. This suggests
that it is directly related to phonemes or/and syllables. These features contrast to rank-frequency
relations for words, which are both author and text independent and are governed by the Zipf’s law.
PACS numbers: 89.75.Fb, 05.10.Gg, 05.65.+b
I. INTRODUCTION
Language can be viewed as a hierarchic construction: phoneme, syllable, morpheme, word ... Each of these objects
(i) expresses meaning or participates in its formation; (ii) is extractable and reproducible in its ready form; (iii)
consists of elements of the previous level, i.e. syllable consists of phonemes [1–3].
The lowest hierarchic level is phoneme, which is defined to be a representative for a group of sounds that are not
distinguishable with respect to their meaning-formation function in a concrete language. For instance /r/ and /l/ are
different phonemes in English, e.g. because row and low which differ only by these phonemes are different words; see
Appendix I for a list of English phonemes. But they are the same phoneme in Japanese, since in that language there
is no danger of meaning-ambiguity upon mixing /r/ with /l/ 1. Thus the meaning is crucial for the definition of the
phoneme, although a single phoneme does not express a separate meaning [1–3]. The next hierarchic level (syllable)
indirectly participates in the definition of the phoneme, since the syllable bounds phonemes 2.
The history of phoneme is a rich and complex one. It appeared in Greek and Indian linguistic traditions simulta-
neously with atomistic ideas in natural philosophy [4–6]. Analogies between atom and phoneme are still potent in
describing complex systems [7]. Within the Western linguistic tradition the development of phoneme was for a while
overshadowed by related (but different) concepts of letter and sound [1, 2]. The modern definition of phoneme goes
back to late XIX century [2]. While it is agreed that the phoneme is a unit of linguistic analysis [3], its psychological
status is a convoluted issue [8–12]. Different schools of phonology and psychology argue differently about it, and there
is a spectrum of opinions concerning the issue (e.g. perception of phonemes, their identification, reproduction etc)
[11, 12]; see [8-10] for recent reviews.
For defining a rank-frequency relation, one calculates the frequencies fr of certain constituents (e.g. words or
phonemes) entering into a given text, lists them in a decreasing order
f1 ≥ f2 ≥ ... ≥ fn, (1)
and studies the dependence of the frequency fr on the rank r (its position in (1), 1 ≤ r ≤ n). This provides a coarse-
grained description, because not the frequencies of specific phonemes are described, but rather the order relation
between them, e.g. the same form of the rank-frequency relation in two different texts is consistent with the same
phoneme having different frequencies in those texts. The main point of employing rank-frequency relations is that
they (in contrast to the full set of frequencies) can be described via simple statistical models with very few parameters.
Rank-frequency relations are well-known for words, where they comply to the Zipf’s law; see [13,14] for reviews.
This law is universal in the sense that for all sufficiently long texts (and their mixtures, i.e. corpora) it predicts the
same power law shape fr ∝ r−1 for the dependence of the word frequency on its rank. It was shown recently that
the representation of the word frequencies via hidden frequencies—the same idea as employed in the present work—is
∗ wdeng@mail.ccnu.edu.cn, armen.allahverdyan@gmail.com
1 Different speech sounds that are realizations of the same phoneme are known as allophones.
2 There cannot be a phoneme which belongs to (i.e. is separated between) two different syllables. For instance, diftongs belong to the
same syllable [1, 2].
2
capable of reproducing both the Zipf’s law and its generalizations to low-frequency words (hapax legomena) [15] 3.
There are also several works devoted to the rank-frequency relations of phonemes and letters [16–22]. One of first
works is that by Sigurd, who has shown that the phoneme rank-frequency relations are not described by the Zipf’s law
[16]. He also noted that a geometric distribution gives a better fit than the Zipf’s law. Other works studied various
few-parameter functions—e.g. the Yule’s distribution—and fitted it to the rank-frequency relations for phonemes of
various languages; see [22] for a recent review of that activity.
Due to its universality, the Zipf’s law for words cannot relate the text to its author. Our main motivation for
studying rank-frequency relation for phonemes is whether they can provide information on the author of the text, and
thereby attempt at clarifying the psychological aspect of phonemes. For a theoretical description we postulate that
phoneme frequencies are random variables with a given density. The ranked frequencies are then recovered via the
order statistics of this density. This postulate allows to restrict the freedom of choosing various (theoretical) forms of
rank-frequency relations, since—as developed in mathematical statistics [27, 28]—the idea of the simplest density for
probability of probability allows to come up with the unique family of Dirichlet densities. This family is characterized
by a positive parameter β, which allows quantitative comparison between phoneme frequencies for different authors.
In several respects the Dirichlet density is similar to the ideal gas model in statistical mechanics that provides a simple
(yet accurate) decription of atomic and molecular gases [29]. As seen below, the Dirichlet density leads to an accurate
description of phoneme rank-frequency relations and allows to establish that the frequencies of phonemes do depend
on the author of the text.
The closest to the present approach is the study by Good [17] which was developed in Refs. [18-20]. These authors
applied the same idea on hidden probabilities as here, but they restricted themselves by the flat density, which is a
particular case β = 1 of the Dirichlet density [17–19]. Superficially, this case seems to be special, because it incorporates
the idea of non-informative (unknown) probabilities (in the Bayesian sense) [31]. However, the development of the
Bayesian statistics has shown that the β = 1 case of the Dirichlet density is by no means special with respect the
prior information [31]. Rather, the whole family of Dirichlet densities (with β > 0 being a free parameter) qualifies
for this role [32].
This paper is organized as follows. Next section discusses the Dirichlet density and its features. There we also
deduce explicit formulas for the probabilities ordered according to the Dirichlet density. Then, we analyze the data
obtained from nine English texts [see Table I] and show that it can be described via the Dirichlet density. There we
also demonstrate (in different ways) that rank-frequency relations for phonemes are author-dependent. This result
is corroborated by a non-parametric method. Next, we show that the author-dependency effect is not caused by
common words used in different texts. We summarize in the last section. Here we also discuss several roads for future
research.
II. DIRICHLET DENSITY
A. Definition and main features
The Dirichlet density D(θ1, ..., θn) is a probability density over continuous variables (θ1, ..., θn) which by themselves
have the meaning of probabilities, i.e. D(θ1, ..., θn) is non-zero only for θk ≥ 0 and
∑n
k=1 θk = 1:
D(θ1, ..., θn|β1, ..., βn) =
Γ[
∑n
k=1 βk]∏n
k=1 Γ[βk]
n∏
k=1
θβk−1k δ(
n∑
k=1
θk − 1), (2)
where βk > 0 are the parameters of the Dirichlet density, δ(x) is the delta-function, Γ[x] =
∫∞
0
dθ θx−1e−θ is the
Euler’s Γ-function, and (2) is properly normalized:
∫∞
0
∏n
k=1 dθkD(θ1, ..., θn|β1, ..., βn) = 1.
The random variables Θ1, ...,Θn (with realizations θ1, ..., θn) are independent modulo the constraint that they sum
to 1; see (2). In this sense (2) is the simplest density for probabilities. Now (2) for a particular case βk = β (which is
most relevant for our purposes) can be given the following statistical-physics interpretation: if ln( 1
θk
) is interpreted
3 The rank-frequency relation for morphemes and syllables was so far not studied systematically. Ref. [23] comes close to this potentially
interesting problem, since it studies the rank-frequency relations of Chinese characters, which are known to represent both morpheme
and syllable (in this context see also [24-26]). This study demonstrated that the Zipf’s law still holds for a restricted range of ranks.
For long texts this range is relatively small, but the frequencies in this range are important, since they carry out ≃ 40 % of the overall
text frequency. It was argued that the characters in this range refer to the most polisemic morphemes [23].
3
as the energy 4 of k, then β − 1 becomes the inverse temperature for an ideal gas. It is useful to keep this analogy in
mind, when discussing further features of the Dirichlet density.
Consider the subset (θ1, ..., θm) (m < n) of probabilities (θ1, ..., θn). If (θ1, ..., θm) should serve as new probabilities,
they should be properly normalized. Hence we define new random variables as follows:
(θ̃1, ..., θ̃n) = (θ̂1, ..., θ̂m, θm+1, ...θn), θ̂k =
θk∑m
i=1 θi
, k = 1, ...,m. (3)
The joint probability P(θ̃1, ..., θ̃n) now reads from (2):
P(θ̃1, ..., θ̃n) = D(θ̂1, ..., θ̂m|β1, ..., βm)X (θm+1, ..., θn), (4)
where the precise form of X is not relevant for the message of (4): if we disregard some probabilities and properly
re-normalize the remaining ones, the kept probabilities follow the same Dirichlet density and are independent from
the disregarded ones [27]. This means that we do not need to know the number of constituents before applying the
Dirichlet density. This feature is relevant for phonemes, because their exact number is to a large extent a matter of
convention, e.g. should English diftongs [see Appendix I] be regarded as separate phonemes, or as combinations of a
vowel and a semivowel.
Condition (4) (called sometimes neutrality), together with few smoothness conditions, determines the shape (2) of
the Dirichlet density [28].
Assuming n free parameters βk for n phoneme frequencies does not amount to any effective description. Hence
below we employ (2) with
βk = β, (5)
for describing the ranked phoneme frequencies. This implies that the full vector (β1, ..., βn) is replaced by a certain
characteristic value β, which is to be determined from comparing with data. To provide some intuition on β, let us
note from (2) that a larger value of β leads to more homogeneous density (many events have approximately equal
probabilities). For βk → 0 the region θk ≃ 0 is the most probable one.
B. Distribution of ordered probabilities (order statistics)
The random variables Θ1, ...,Θn (whose realizations are θ1, ..., θn in (2)) are now put in a non-increasing order:
Θ(1) ≥ ... ≥ Θ(n). (6)
This procedure defines new random variables, so called order statistics of the original ones [30]. We are interested by
the marginal probability density of Θ(r). It is difficult to obtain this object explicitly, because the initial Θ1, ...,Θn
are correlated random variables. However, we can explicitly obtain from (2) a two-argument function that suffices for
calculating the moments of Θ(r) [see Appendix II]:
χr(y;m) =
Γ[nβ]
Γ[nβ +m]
n!
(n− r)!(r − 1)!
yβ−1e−y
Γ[β]
ϕn−r(y)[1− ϕ(y)]r−1, (7)
where Γ[x] is the Γ-function and where
ϕ(y) =
1
Γ[β]
∫ y
0
dxxβ−1e−x (8)
is the regularized incomplete Γ-function. Now the moments of Θ(r) are obtained as
〈θm(r)〉 =
∫ ∞
0
dy ymχr(y;m). (9)
In the next section we shall see that the sequence of ordered probabilities fr [cf. (1)] can be generated via (7). To
this end, the empiric quantities fr will be compared to f̂r = 〈θ(r)〉; cf. (9). The rationale for using the average is that
4 This interpretation can be motivated via the information theory; see [33] for further details.
4
for parameters we are interested in—where n ≃ 40− 50 (for English phonemes n = 44) and 0.5 ≤ β ≤ 1—we get from
(7–9) that relative fluctuations around the average f̂r ≡ 〈θ(r)〉 are small. Namely, εr ≡ (〈θ2(r)〉−〈θ(r)〉2)/〈θ(r)〉2 . 0.02
for all values of r, excluding r ≈ n, i.e. very low frequency phonemes. This is shown in Fig. 1 for a particular value
β = 0.8. Note that εr is not a monotonic function of r: it is smallest for middle ranks. (Even for those values of
r, where εr ≃ 1, f̂r = 〈θ(r)〉 can still describe the empiric frequencies fr, as seen below.) Now there is a simpler
approximate formula for f̂r = 〈θ(r)〉 that is deduced from (9) [see Appendix II]
r
n
= 1− ϕ(f̂rnβ). (10)
0 10 20 30 40
0.00
0.05
0.10
0.15
0.20
0.25
Rank
Fr
eq
ue
nc
y,
va
ri
an
ce
FIG. 1: Rank-frequency curves and error generated by the Dirichlet density with β = 0.8 and n = 44. Blue curve: 〈θ(r)〉 (as
a function of r) calculated according to (7–9). Black curve: f̂r calculated via the approximate formula (10); cf. Appendix II.
Red points: the normalized variance (〈θ2(r)〉 − 〈θ(r)〉
2)/〈θ(r)〉
2 for r = 1, ..., 44 calculated according to (7–9). This expression is
well approximated by (38).
Fig. 1 shows that f̂r obtained from (10) indeed approximates well 〈θ(r)〉 for almost all ranks r.
III. RESULTS
A. Fitting rank-frequency relations to the Dirichlet distribution
We studied nine English texts written by three different, native-English authors; see Table I. For each text we
extracted the phoneme frequencies {fr}nr=1 and ordered them as in (1); the list of English phonemes is given in
Appendix I. It is important to specify from which set of words (of a text) one extracts the phoneme frequencies. Two
natural choices are possible here: either one employs all words of the text, or different words of the text (i.e. multiple
occurrences of the same word are neglected). We shall study both cases.
The ordered set {fr}nr=1 of phoneme frequencies for each text was compared with the prediction {f̂r = 〈θ(r)〉}nr=1
of the Dirichlet density [see (9)]. Here the parameter β [cf. (2, 5)] is found from minimizing the error:
SSerr =
n∑
k=1
(fk − f̂k)2. (11)
For each studied case we also monitored the coefficient of correlation between {fr}nr=1 and {f̂r}nr=1:
R2 =
[∑n
k=1(fk − f̄)(f̂k − f̂)
]2
∑n
k=1(fk − f̄)2
∑n
k=1(f̂k − f̂)2
, (12)
where
f̄ ≡ 1
n
∑n
k=1
fk, f̂ ≡
1
n
∑n
k=1
f̂k. (13)
5
TABLE I: Nine texts and their parameters. Texts are abbreviated and numbered. Ntw, Npht, Ndw and Nphd are, respectively,
the total number of words, the number of phonemes of the total words, the number of different words and the number of
phonemes of different words.
J. Austen: Mansfield Park (MP or 1) 1814; Pride and Prejudice (PP or 2) 1813; Sense and Sensibility (SS or 3) 1811.
C. Dickens: A Tail of Two Cities (TC or 4) 1859; Great Expectations (GE or 5) 1861; Adventures of Oliver Twist (OT or 6)
1838.
J. Tolkien: The Fellowship of the Ring (FR or 7) 1954; The Return of the King (RK or 8) 1955; The Two Towers (TT or 9)
1954.
Texts Ntw Npht Ndw Nphd
MP (1) 160473 567750 7854 48747
PP (2) 121763 435322 6385 39767
SS (3) 119394 425822 6264 38668
TC (4) 135420 468642 9841 58760
GE (5) 186683 623079 10933 65364
OT (6) 159103 555372 10359 61072
FR (7) 177227 617106 8644 46509
TT (8) 143436 502303 7676 39823
RK (9) 134462 431141 7087 36494
A good fitting means that R2 is close to 1. Importantly, we found that (as functions of β) SSerr and 1−R2 minimize
simultaneously.
0 10 20 30 40
0.00
0.02
0.04
0.06
0.08
0.10
0.12
0.14
0 10 20 30 40
0.00
0.02
0.04
0.06
0.08
0.10
0.12
 
 
Fr
eq
ue
nc
y
Rank
 TC (all words)
  = 0.67
(a)
 
 
Fr
eq
ue
nc
y
Rank
PP (different words)
  = 0.69
(b)
FIG. 2: Rank-frequency relation (black circles) and the fitting with Dirichlet distribution (red line). (a) Text TC. (b) Text PP;
see Table I.
Examples of fitting curves for phoneme rank-frequency relations are presented in Fig. 2. The fitting parameters are
given in Tables II and III in Appendix III. Note that the fitting values of R2 are good. The group of most frequent
eight phonemes reads [see Appendix I]: /ı/, /@/, /n/, /s/, /t/, /l/, /d/, /r/. The concrete ranking between them
depends on the text, but the most frequent one is normally /ı/.
Tables II and III in Appendix III show that the texts by the same author have closer values of β than those written
by different authors; see also Figs. 3 and 4. This can be quantified via the following three inequalities
0 < b(A) ≡ min {|βi − βk|}i=1,2,3;k=4,5,6,7,8,9 − max {|βi − βj |}i<j;i,j=1,2,3, (14)
0 < b(D) ≡ min {|βi − βk|}i=4,5,6;k=1,2,3,7,8,9 − max {|βi − βj |}i<j;i,j=4,5,6, (15)
0 < b(T) ≡ min {|βi − βk|}i=7,8,9;k=1,2,3,4,5,6 − max {|βi − βj |}i<j;i,j=7,8,9, (16)
where A, D and T refer, respectively to Austen, Dickens and Tolkien [see Table I]. The indices i and j run over the
texts by the same author, while k refer to different authors, e.g. i, j = {1, 2, 3} (Austen) and k = {4, 5, 6, 7, 8, 9} (not
Austen). The minimization (or maximization) in (14-16) goes over indicated indices.
Eqs. (14-16) hold both for phoneme frequencies extracted from different words and from all words of a text; cf.
6
Tables II and III in Appendix III. For instance, b[all words](A) = 0.02, b[all words](D) = 0.02, b[all words](T) = 0. The
latter is the only minor exclusion from (14-16).
Thus the set {βi}9i=1 fragments into three clusters that refer to different authors. Note that
b[diff. words](a) > b[all words](a), a = A, D, T. (17)
Hence different words display the author-dependency in a stronger form; this is confirmed below by other methods.
The author-dependency of phoneme rank-frequency relation is unexpected, because the rank-frequency relation for
words (which consists of phonemes) follows the Zipf’s law whose shape is independent of the author [13–15]. Note
that the few most frequent phonemes and the least frequent ones appear to fit best the theoretical prediction; cf.
Fig. 2. This feature again contrasts the rank-frequency relation for words, where it is known that high-frequency
words—these are mostly the functional words, e.g. and, or—do hold the Zipf’s law worst than other words [15].
On the other hand, the moderate-frequency phonemes deviate most from the prediction of the Dirichlet curve; cf.
Fig. 2. This effect is not statistical, since fluctuations around the average are most suppressed for moderate-frequency
phonemes; see after (9) and Figs. 1 and 2.
Another pertinent result is that [see Tables II and III in Appendix III]
β
[diff. words]
i > β
[all words]
i , i = 1, ..., 9, (18)
i.e. the phoneme distribution obtained from different words is more homogeneous [see our discussion after (5)], because
for all words the frequency of high-rank phonemes is amplified due to multiple usage of frequent words.
B. Distance between phoneme frequencies
The author-dependency of phoneme rank-frequency relation is corroborated by looking directly at suitable distances
between the ranked phoneme frequencies in different texts. We choose to work with the variational distance 5
ρ1(ij) =
1
2
n∑
k=1
| fk[i]− fk[j] |, (19)
where {fk[i]}nk=1 are the ordered phoneme frequencies in the text i. We shall also employ a more fine-grained (detail-
specific) distance. Let f [α|i] be the frequency of phoneme α in text i (α = 1, ..., n, i = 1, .., 9); see Appendix I and
Table I. We can now define [cf. (19)]
ρ0(ij) =
1
2
n∑
α=1
| f [α|i]− f [α|j] |. (20)
Now ρ0(ij) = 0 only if f [α|i] = f [α|j]. It is seen from Tables IV-VI in Appendix III that ρ0(ij) > ρ1(ij), as it should
be, because ρ1(ij) is less sensitive to details (i.e. it is more coarse-grained).
Tables IV and V in Appendix III also show that phoneme rank-frequency relations between the texts written by
the same author are closer to each other—in the sense of distances ρ0 and ρ1—than the ones written by different
authors. This is also seen on Figs. 3 and 4.
To quantify these differences, consider the following inequalities that define clustering with respect to authors (see
Table I for numbering of texts, and note that ρλ(ij) = ρλ(ji) for the distance between the texts i and j):
0 < zλ(A) ≡ min {ρλ(ik)}i=1,2,3;k=4,5,6,7,8,9 − max {ρλ(ij)}i<j;i,j=1,2,3, λ = 0, 1, (21)
0 < zλ(D) ≡ min {ρλ(ik)}i=4,5,6;k=1,2,3,7,8,9 − max {ρλ(ij)}i<j;i,j=4,5,6, λ = 0, 1, (22)
0 < zλ(T) ≡ min {ρλ(ik)}i=7,8,9;k=1,2,3,4,5,6 − max {ρλ(ij)}i<j;i,j=7,8,9, λ = 0, 1, (23)
where A, D and T refer, respectively to Austen, Dickens and Tolkien; cf. (21–23) with (14-16). For example,
the maximal distance (20) between texts by Austen (see Table I) is denoted by max {ρ0(ij)}i<j;i,j=1,2,3, while
5 To motivate the choice of the variational distance ρ0 =
1
2
∑n
α=1 | pα − qα | between two sets of probabilities {pα}
n
α=1 and {qα}
n
α=1, let
us recall an important feature of this distance [34]: ρ0 = maxΩ
∣
∣
∑
α∈Ω(pα − qα)
∣
∣, where the maximization goes over all sub-sets Ω of
{1, ..., n}. Thus ρ0 refers to the (composite) event that gives the largest probability difference between {pα}nα=1 and {qα}
n
α=1.
7
0 10 20 30 40
0.00
0.02
0.04
0.06
0.08
0.10
0.12
0 10 20 30 40
0.00
0.02
0.04
0.06
0.08
0.10
0.12
 
 
 TC (all words)
 GE (all words)
         Same author: Dickens
Fr
eq
ue
nc
y
Rank
(a)
 
 
 PP (different words)
 SS (different words)
         Same author: Austen
Fr
eq
ue
nc
y
Rank
(b)
FIG. 3: Rank-frequency relation (black and red circles) for two texts written by the same author. (a) TC and GE written by
Dickens. (b) PP and SS written by Austen (see Table I).
0 10 20 30 40
0.00
0.02
0.04
0.06
0.08
0.10
0.12
0.14
0 10 20 30 40
0.00
0.02
0.04
0.06
0.08
0.10
0.12
 
 
 MP (all words)
 TC (all words)
Fr
eq
ue
nc
y
Rank
(a)
 
 
 SS (different words)
 RK (different words)
Fr
eq
ue
nc
y
Rank
(b)
FIG. 4: Rank-frequency relation (black and red circles) for two texts written by different authors. (a) TC by Dickens versus
MP by Austen. (b) SS by Austen versus RK by Tolkien; see Table I for parameters of these texts.
min {ρ0(kl)}k=1,2,3;l=4,5,6,7,8,9 is the minimal distance between texts written by Austen and those written by Dickens
and Tolkien.
The meaning of (21–23) can be clarified by looking at an authorship attribution task: let several texts i = 1, 2, 3
by (for example) Austen are at hands, and one is given an unknown text α. The question is whether α could also be
written by Austen. If now maxi[ρλ(iα)] ≤ maxi<j [ρλ(ij)], we have an evidence that α is written by Austen.
We stress that there are no fitting parameters in (20–23). Our data (cf. Tables IV and V in Appendix III) holds
eleven (out of twelve) inequalities (21–23) for phoneme frequencies extracted both from different and from all words
of the text. There is only one exclusion: z
[all words]
1 (T) = −0.00207, which is by an order of magnitude smaller than
the respective frequencies [cf. (23)]. Apart of this minor exclusion, we confirm the above prediction (obtained via the
fitted values of β) on the author-dependency for phoneme frequencies.
Data shown in Tables IV and V in Appendix III also imply the following inequalities [confirming (17)]
z
[diff. words]
λ (a) > z
[all words]
λ (a), λ = 0, 1, a = A, D, T. (24)
Another pertinent feature is that the distances ρ0 and ρ1 between texts written by the same author hold
ρ
[all words]
λ (ij) > ρ
[diff. words]
λ (ij), (25)
λ = 0, 1, (ij) = { (12), (13), (23), (45), (46), (78), (79), (89) }.
8
Seventeen out of eighteen relations (25) hold for our data; see Tables IV and V in Appendix III. The only exclusion
is ρ
[diff. words]
0 (78)− ρ
[all words]
0 (78) = 0.02853− 0.02584 = 0.00269. No definite relations exist between ρ
[all words]
λ and
ρ
[diff. words]
λ for texts written by different authors. One can interpret (25) as follows. When going from different words
to all words of the text, the majority of frequent words are not author-specific: they are mostly key-words (that are
specific to the text, but not necessarily to the author) and functional words (e.g. and, or, of, but) that are again not
author-specific.
Taken together, (24) and (25) imply that the clustering with respect to authors is better visible for frequencies
extracted from different words of the texts (the inter-cluster distance increases, whereas the intra-cluster distance
decreases). The same effect was obtained above via fitted values of β’s; see (17).
C. The origin of the author-dependency effect is not in common words
One possible reason for the author-dependency of phoneme frequencies is that the effect is due to the vocabulary
of the author. In this scenario the similarity between phoneme frequencies in text written by the same author would
be caused by the fact that these texts have sufficiently many common words that carry out the same phonemes.
Texts written by the same author do have a sizable number of common words, as was already noted within the
authorship attribution research [35, 36]. We confirm this result in Table VII of Appendix III, where it is seen that
the fraction of common words holds the analogues of (21–23); see Table VII in Appendix III for details. Hence this
fraction also shows the author-dependency effect.
In order to understand whether the author-dependency of phoneme frequencies can be explained via common
words, we excluded from different words of texts i and k the common words of those texts [i, k = 1, ..., 9, see Table
I], re-calculated phoneme frequencies, and only then determined the respective distances ρ
[no comm. words]
0 (ik) and
ρ
[no comm. words]
1 (ik). If the explanation via common words holds, they will not show author-dependency. This is
however not the case: the effect is there because relations (21–23) do hold for them [see Table VI in Appendix III]:
z
[no comm. words]
λ (a) > 0, λ = 0, 1, a = A, D, T. (26)
After excluding the common words the author-dependency did not get stronger in the sense of (25), because the data
of Tables V and VI in Appendix III imply for texts written by the same author
ρ
[no comm. words]
λ (ij) > ρ
[diff. words]
λ (ij), λ = 0, 1, (27)
(ij) = { (12), (13), (23), (45), (46), (78), (79), (89) }.
In this context recall (24, 25). But it also did not get weaker [cf. (24) and (21–23)], because
z
[no comm. words]
λ (a) > z
[diff. words]
λ (a), λ = 0, 1, a = A, D, T, (28)
as seen from Tables V and VI in Appendix III.
IV. SUMMARY
Phonemes are the minimal building blocks of the linguistic hierarchy that still relate to meaning. Rank-frequency
relations provide a coarse-grained description of phoneme frequencies. We have shown that these relations are de-
scribed by generating phonemes via random probabilities governed by the (one-parameter) Dirichlet density. This
is the simplest density for random probabilities that corresponds to the ideal gas model in statistical physics. It
appears that the most frequent phonemes fit the Dirichlet distribution much better than others. This contrasts the
rank-frequency relations for words, where the Zipf’s law holds worst for the most frequent words.
The fitting to the Dirichlet density uncovers an important aspect of phoneme frequencies: they depend on the
author of the text. We confirmed this result via a parameter-free method that is based on calculating distances
between phoneme frequencies of different texts. Again, this contrasts the Zipf’s law for rank-frequency relations of
words whose shape is author-independent.
It is well-known that certain aspects of text-statistics display author-dependency, and this is applied in various
author attribution tasks; see e.g. [36-39] for recent reviews. In particular, this concerns frequencies of functional
words. The fact that author-dependency is seen on such a coarse-grained level as rank-frequency relations may mean
that phoneme frequencies can be useful for existing methods of authorship attribution [36–39]. This should be clarified
in future.
9
A straightforward reason for explaining the author-dependency effect of phoneme frequencies would be that it is
due to the author’s vocabulary, as reflected by common words in texts written by the same author. The previous
section has shown that such an explanation is ruled out.
Then we are left with options that the effect is due to storing (with different frequencies) syllables or/and phonemes.
If syllable frequencies have author-dependency, this could result to author-dependent phoneme frequencies, because
there are specific rules that (at least probabilistically) determine the phoneme composition of syllables [41]. But note
that syllables are in several respect similar to words (and not phonemes): (i) there are many of them; e.g. English
has more than 12000 syllables. (ii) There is large gap between frequent and infrequent syllables [40] (cf. with the
hapax legomena for words). (iii) There are indications that syllables are stored in a syllabic lexicon that in several
ways is similar to the mental lexicon that stores words [40].
The second possibility would mean that the authors store phonemes [10], and this will provide a statistical argument
for psychological reality of phonemes. Note that the issue of psychological reality of a phoneme is not settled in modern
phonology and psychology, various schools arguing pro and contra of it; see [8-12] for discussions. And then both these
options might be present together. Thus further research—also involving rank-frequency relations for syllables—is
needed for clarifying the situation.
In conclusion, we recall another interesting argument that indicates on a deeper role of phonemes as elements of
communication systems [42]. The number of phonemes in languages roughly varies between 20 and ∼ 50 6. By its
order of magnitude this number coincides [42] with the number of ritualized (i.e. sufficiently abstract) signals of
animal communication, which is also stable across different species [43] (an example of this are gestures of apes).
V. APPENDIX
I. English phonemes
Here we recall 44 English phonemes according to the International Phonetic Alphabet.
I. 20 vowels (7 short phonemes, 5 long and 8 diftongs):
2 (but), æ(cat), @ (about), e (men), ı(sit), 6(not), U(book),
A: (part), 3: (word, learn), i: (read), O: (sort), u: (too),
aı (my), aU (how), oU (go), eı (day), ı@ (here), oı (boy), U@ (tour, pure), e@ (wear, fair)
II. 24 consonants:
b (born), d (do), f (five), g (get), h (house), j (yes), k (cat), l (lion), m (mouse), n (nouse), ï (sing), p
(put), r (room), s (saw), S (shall), t (time), Ù (church), T (think), ð (the), v (very), w (window), z (zoo), Z
(casual), dZ (judge)
II. Order statistics for Dirichlet density
Let us introduce the following notation for the order integration
I(dθ1, ..., dθn) ≡
∫ ∞
0
dθ1
∫ θ1
0
dθ2...
∫ θn−1
0
dθn. (29)
Now the average over the order statistics of the Dirichlet density (2) is defined as [30]
〈θm(r)〉 =
I(dθ1, ..., dθn) θmr δ(
∑n
k=1 θk − 1)
∏n
k=1 θ
β−1
k
I(dθ1, ..., dθn) δ(
∑n
k=1 θk − 1)
∏n
k=1 θ
β−1
k
(30)
In the numerator of (30) we change variables as θ̂k = rθk (r > 0), multiply both sides by e
−r, and then integrate both
sides over r ∈ [0,∞):
I(dθ1, ..., dθn) θmr δ(
n∑
k=1
θk − 1)
n∏
k=1
θβ−1k × Γ[nβ +m] = I(dθ̂1, ..., dθ̂n) θ̂mr
n∏
k=1
θ̂β−1k e
−θ̂k (31)
6 The average number of phonemes in European languages is ≃ 37. Tonal languages (e.g. Chinese) have much more phonemes, but they
evolved from a smaller set of basic phonemes that complies with the above number [44].
10
The denominator of (30) is worked out analogously.
Let us now define
χr(y;m) =
Γ[nβ]
Γ[nβ +m]
I(dθ̂1, ..., dθ̂n) δ(y − θ̂r)
∏n
k=1 θ̂
β−1
k e
−θ̂k
I(dθ̂1, ..., dθ̂n)
∏n
k=1 θ̂
β−1
k e
−θ̂k
(32)
so that (9) holds. Working out I(dθ̂1, ..., dθ̂n)
∏n
k=1 θ̂
β−1
k e
−θ̂k and I(dθ̂1, ..., dθ̂n) δ(y− θ̂r)
∏n
k=1 θ̂
β−1
k e
−θ̂k in (32) via
integration by parts (starting from the last integration in I(dθ̂1, ..., dθ̂n)) we obtain (7–9).
If (n − r) ≫ 1 and r ≫ 1 the behavior of χr(y;m) in (7) is determined by the exponential factor
e(n−r) lnϕ(y)+r ln(1−ϕ(y)). Working it out via the saddle-point method we conclude that asymptotically:
χr(y;m) ≃
Γ[nβ]
Γ[nβ +m]
1√
2πσ
e−
1
2σ (y−y0)
2
, (33)
where y0 and σ are defined as follows
n− r
n
= ϕ(y0), σ =
(n− r)r
n3
1
[ϕ′(y0) ]2
, (34)
where φ′(y) = dϕ(y)/dy.
Hence we get from (9) and (33, 34):
〈θ(r)〉 =
y0
nβ
, (35)
〈θ2(r)〉 − 〈θ(r)〉2 =
nβσ − y20
[nβ]2(nβ + 1)
=
1
[nβ]2(nβ + 1)
(
β(n− r)r
n2
1
[ϕ′(y0)]2
− y20
)
. (36)
The importance of fluctuations is characterized by
〈θ2(r)〉 − 〈θ(r)〉2
〈θ(r)〉2
=
1
nβ + 1
(
β(n− r)r
n2
1
y20 [ϕ
′(y0)]2
− 1
)
(37)
=
1
nβ + 1
(
β(n− r)r
n2
Γ2[β] y−2β0 e
2y0 − 1
)
, (38)
where we employed (8). Eq. (38) is a good approximation of
〈θ2(r)〉−〈θ(r)〉
2
〈θ(r)〉2
calculated (exactly) from (7–9); see Fig. 1.
III. Tables of fitting parameters, distances and fraction of common words between texts
TABLE II: Fitting parameters for texts numbered as 1-9; see (11, 12) and Table I for text numbers. The phoneme frequencies
are extracted from all words of the text.
1 2 3 4 5 6 7 8 9
β 0.61 0.63 0.61 0.67 0.69 0.69 0.75 0.74 0.79
SSerr × 10
−7 7696 7574 6151 4317 5287 3993 4196 4337 3580
R2 0.9768 0.9765 0.9816 0.9859 0.9820 0.9867 0.9844 0.9842 0.9860
Acknowledgements
This work was supported by National Natural Science Foundation of China (Grant No. 11505071), the Programme
of Introducing Talents of Discipline to Universities under Grant NO. B08033.
[1] L.V. Scherba, Memoires de la Societe de Linguistique de Paris, 16, 1 (1910).
11
TABLE III: Fitting parameters for texts numbered as 1-9; see (11, 12) and Table I for text numbers. The phoneme frequencies
are extracted from different words of the text.
1 2 3 4 5 6 7 8 9
β 0.72 0.69 0.69 0.77 0.78 0.79 0.968 0.979 0.975
SSerr × 10
−7 5150 4495 5003 6107 5265 5220 11296 12943 10366
R2 0.9818 0.9847 0.9829 0.9771 0.9800 0.9800 0.9501 0.9403 0.9525
TABLE IV: Distances ρ0 and ρ1 between texts; see Table I and (20–19). The phoneme frequencies are extracted from all words
of the text.
12 13 23 45 46 56 78 79 89
ρ0 × 10
−5 3045 2062 2549 3423 2382 3448 2584 2066 2464
ρ1 × 10
−5 2227 1602 2103 2100 1978 2753 1808 1809 2037
14 15 16 17 18 19 24 25 26 27 28 29
ρ0 × 10
−5 3583 4690 4000 7372 7402 7322 3645 4762 4064 7653 7629 7650
ρ1 × 10
−5 2784 3044 3260 5149 5227 5599 2712 3059 3110 4978 5052 5449
34 35 36 37 38 39 47 48 49 57 58 59 67 68 69
ρ0 × 10
−5 3562 4924 4358 7737 6950 7447 5174 5327 5061 6113 6436 6217 5074 5706 5202
ρ1 × 10
−5 2546 3022 3181 5266 5085 5654 3950 3568 3935 3894 4014 4325 3727 3934 3770
TABLE V: Distances ρ0 and ρ1 between texts; see Table I and (20–19). The phoneme frequencies are extracted from different
words of the text.
12 13 23 45 46 56 78 79 89
ρ0 × 10
−5 1563 1317 1413 1568 1380 1100 2853 1946 2025
ρ1 × 10
−5 1346 1205 1346 1266 1126 1052 1635 1476 1569
14 15 16 17 18 19 24 25 26 27 28 29
ρ0 × 10
−5 2296 2703 2868 7430 9535 8434 2839 3318 3458 8141 9999 9167
ρ1 × 10
−5 1967 2110 2470 6103 7200 6775 2252 2436 2709 6587 7544 7136
34 35 36 37 38 39 47 48 49 57 58 59 67 68 69
ρ0 × 10
−5 2718 3264 3257 7943 9998 8997 5918 7875 6899 5521 7842 6646 5595 7785 6786
ρ1 × 10
−5 2193 2486 2636 6539 7447 7022 4795 5971 5368 4631 5566 5222 4486 5645 5201
TABLE VI: Distances ρ0 and ρ1 between texts; see Table I and (20–19). The phoneme frequencies are extracted after excluding
the common words of the texts.
12 13 23 45 46 56 78 79 89
ρ0 × 10
−5 3792 3217 3734 3146 2930 2329 5918 4421 4770
ρ1 × 10
−5 2832 2463 2502 2190 2215 1610 3317 2773 2809
14 15 16 17 18 19 24 25 26 27 28 29
ρ0 × 10
−5 4758 5742 6087 12574 15119 13490 5708 6385 6880 13323 15733 14113
ρ1 × 10
−5 3912 4276 4830 8800 9576 8895 4529 4991 5495 9469 10387 9621
34 35 36 37 38 39 47 48 49 57 58 59 67 68 69
ρ0 × 10
−5 5188 5887 6476 13391 15842 14244 10980 13905 12109 10346 13003 11673 10413 13288 11911
ρ1 × 10
−5 4344 4917 5285 9835 10637 9891 7025 7371 6928 6537 7021 6673 6580 6667 6433
12
TABLE VII: The fraction p of common words between texts given in Table I. Now p is defined as follows. Let n(i) and
n(ij) be, respectively, the number of different words in text i and the number of common words in texts i and j. We define:
p(ij) = n(ij)/(n(i) + n(j) − n(ij)), where 0 ≤ p(ij) ≤ 1. This is the number of common words divided over the number of all
different words in texts i and j. As seen from the data below, analogues of (21–23) hold with 1− p(ij) instead of ρλ(ij).
12 13 23 45 46 56 78 79 89
p× 10−5 47554 47786 50655 41146 42454 41822 45010 46948 48173
14 15 16 17 18 19 24 25 26 27 28 29
p× 10−5 35592 35819 36660 28978 25870 26730 32902 32499 33877 26549 24180 24643
34 35 36 37 38 39 47 48 49 57 58 59 67 68 69
p× 10−5 33463 32813 34643 27572 25340 25733 33901 30387 32005 32069 27963 29994 32002 28649 30518
[2] W.F. Twaddell, Language, 11, 5-62 (1935).
[3] E. Sapir, The psychological reality of phonemes. In D. Mandelbaum (ed.) Selected Writings of E. Sapir, pp. 46-60 (University
of California Press, Berkeley and Los Angeles, CA, 1949).
[4] J.R. Skoyles, J. Social Biol. Struc. 13, 321 (1990).
[5] F. Staal, Journal of Indian Philosophy, 34, 89 (2006).
[6] V.G. Lysenko, Voprosy Filosofii, n.6, 9 (2014) (In Russian).
[7] W.L. Abler, J. Social Biol. Struc. 12, 1 (1989).
[8] R. Port, New Ideas in Psychology 25, 143 (2007).
[9] R. Valimaa-Blum, The phoneme in cognitive phonology: episodic memories of both meaningful and meaningless units?
CogniTextes: Revue de l’Association francaise de linguistique cognitive, 2 (2009).
[10] G. Nathan, Phonology, in The Oxford Handbook of Cognitive Linguistics, ed. by D. Geeraerts and H. Cuyckens (Oxford
University Press, Oxford).
[11] H.B. Savin and T.G. Bever, Journal of Verbal Learning and Verbal Behavior, 9, 295 (1970).
[12] D.J. Foss and D.A. Swinney, Journal of Verbal Learning and Verbal Behavior, 12, 246 (1973).
[13] L E. Wyllis, Library Trends, 30, 53 (1981).
[14] H. Baayen, Word frequency distribution (Kluwer Academic Publishers, 2001).
[15] A. E. Allahverdyan, W. Deng, and Q. A. Wang, Physical Review E 88, 062804 (2013).
[16] B. Sigurd, Phonetica, 18, 1 (1968).
[17] I.J. Good, Statistics of Language, in A. R. Meetham and R. A. Hudson (Eds.), Encyclopaedia of Linguistics, Information
and Control, pp. 567-581 (Pergamon Press, New York, 1969).
[18] S.M. Gusein-Zade, Prob. Inform. Trans. 24, 338 (1988).
[19] C. Martindale, S. M. Gusein-Zade, D. McKenzie and M. Yu. Borodovsky, Journal of Quantitative Linguistics, 3, 106
(1996).
[20] I.H. Witten and T.C. Bell, International Journal of Man-Machine Studies, 32, 545 (1990).
[21] Y. Tambovtsev and C. Martindale, SKASE Journal of Theoretical Linguistics 4, 1 (2007)
[22] H. Pande and H. S. Dhami, International Journal of Mathematics and Scientific Computing, 3, 19 (2013).
[23] W. Deng, A. E. Allahverdyan, Bo Li and Q. A. Wang, European Physical Journal B, 87, 47 (2014).
[24] K.H. Zhao, American Journal of Physics, 58, 449 (1990).
[25] S. Shtrikman, Journal of Information Science, 20, 142 (1994).
[26] Q. Chen, J. Guo and Y. Liu, Journal of Quantitative Linguistics, 19, 232 (2012).
[27] B. A. Frigyik, A. Kapila and M. R. Gupta, Introduction to the Dirichlet Distribution and Related Processes, University of
Washington technical report, UWEETR-2010-0006.
[28] J.N. Darroch and D. Ratcliff, Journal of the American Statistical Association 66, 641 (1971).
[29] R. Balian, From Microphysics to Macrophysics, volume I (Springer, 1992).
[30] H.A. David, Order Statistics (Wiley & Sons, New York, 1981).
[31] E.T. Jaynes, IEEE Trans. Syst. Science & Cyb. 4, 227 (1968).
[32] J.L. Schafer, Analysis of Incomplete Multivariate Data (Chapman & Hall/CRC, Boca Raton, USA, 1997)
[33] Yu.A. Shrejder, Problems of Information Transmission, 3, 57 (1967).
Y. Dover, Physica A 334, 591 (2004).
E.V. Vakarin and J. P. Badiali, Physical Review E 74, 036120 (2006).
[34] A.L. Gibbs and F.E. Su, International Statistical Review, 70, 419-435 (2002).
[35] L. Ule, Association for Literary and Linguistic Computing Bulletin, 10, 73 (1982).
[36] P. Joula, Foundations and Trends in Information Retrieval, 1, 233 (2006).
[37] M. Koppel, J. Schler and S. Argamon, Journal of the American Society for information Science and Technology, 60, 9-26
(2009).
[38] E. Stamatatos, Journal of the American Society for Information Science and Technology, 60, 538-556 (2009).
[39] O. V. Kukushkina, A. A. Polikarpov and D. V. Khmelev, Problems of Information Transmission, 37, 172-184 (2001).
13
[40] W.J.M. Levelt, A. Roelofs and A.S. Meyer, Behavioral Brain Sciences, 22, 1 (1999).
W.J.M. Levelt and A.S. Meyer, European Journal of Cognitive Psychology, 12, 433 (2000).
[41] B. Kessler and R. Treiman, Journal of Memory and Language, 37, 295 (1997).
[42] V.V. Ivanov, Even and Odd: Asymmetry of the Brain and of Semiotic Systems (Soviet Radio, Moscow, 1978) (In Russian).
[43] M. Moynihan, Journal of Theoretical Biology, 29, 85 (1970).
[44] G. Sampson, Linguistics, 32, 117 (1994).
