Contents lists available at ScienceDirect
Journal of Statistical Planning and Inference
Journal of Statistical Planning and Inference 141 (2011) 3665–36740378-37
doi:10.1
 Cor
E-mjournal homepage: www.elsevier.com/locate/jspiStylometric analyses using Dirichlet process mixture modelsParamjit S. Gill a,, Tim B. Swartz b
a I.K. Barber School of Arts and Sciences, University of British Columbia Okanagan, Kelowna, BC, Canada V1V 1V7
b Department of Statistics and Actuarial Science, Simon Fraser University, Burnaby, BC, Canada V5A 1S6a r t i c l e i n f o
Article history:
Received 27 September 2010
Received in revised form
27 April 2011
Accepted 30 May 2011
Available online 6 June 2011
Keywords:
Bayesian methods
Clustering
Computational linguistics
Dirichlet process priors
Disputed authorship
Federalist papers
Multinomial distribution58/$ - see front matter & 2011 Elsevier B.V. A
016/j.jspi.2011.05.020
responding author.
ail addresses: paramjit.gill@ubc.ca (P.S. Gill),a b s t r a c t
Stylometry refers to the statistical analysis of literary style of authors based on the
characteristics of expression in their writings. We propose an approach to stylometry
based on a Bayesian Dirichlet process mixture model using multinomial word
frequency data. The parameters of the multinomial distribution of word frequency
data are the ‘‘word prints’’ of the author. Our approach is based on model-based
clustering of the vectors of probability values of the multinomial distribution. The
resultant clusters identify different writing styles that assist in author attribution for
disputed works in a corpus. As a test case, the methodology is applied to the problem of
authorship attribution involving the Federalist papers. Our results are consistent with
previous stylometric analyses of these papers.
& 2011 Elsevier B.V. All rights reserved.1. Introduction
Stylometry deals with the statistical and computer analysis of literary style using the characteristics of expression. For
more than 150 years, a wide variety of statistical techniques have been proposed to compare literary styles and address
the problem of authorship attribution. The methods range from goodness-of-fit tests, principal component analysis,
discriminant analysis and classical clustering methodologies to sophisticated artificial intelligence and neural networks
models. Specific stylometric applications have involved religious texts (Old and New Testaments, Book of Mormon), Old
English manuscripts, political essays (Federalist papers), popular literature (The Royal Book of Oz), Shakespearean plays,
American Civil War letters, plagiarism in students’ essays and the authenticity of legal documents. Holmes (1999) provides
a good review and history of research in this field.
Pioneering work on stylometry was based on word-length and sentence-length distributions. However, it was often the
case that these variables were unable to discriminate between authors (Mosteller and Wallace, 1984). Consequently, many
stylometric analyses include other variables such as vocabulary richness, the proportion of nouns, the proportion of
adjectives, the number of one-letter and two-letter words, etc. (Holmes, 1999). For the purpose of author attribution,
successful methods rely on identifying and exploiting an author’s ‘‘word prints’’. The methodologies involve words, called
function words, which are non-contextual (i.e. topic-independent) but serve as useful indicators of an author’s unconscious
stylistic preferences while writing on any topic. Commonly used function words are conjunctions, prepositions and articles
that have little meaning by themselves but are used to define relationships between content words in a sentence.
Data extracted from texts consist of the frequencies of occurrence of various function words.ll rights reserved.
tim@stat.sfu.ca (T.B. Swartz).
P.S. Gill, T.B. Swartz / Journal of Statistical Planning and Inference 141 (2011) 3665–36743666The question of authorship falls into the general statistical problem of classification where objects that are ‘‘similar’’ are
grouped together in clusters. In stylometry, the objects are texts and the clusters consists of texts that are deemed to have
been written by the same author. Sometimes prior knowledge is available of the form where the authorship is known for
some of the texts under consideration.
In our approach, we view the frequencies of function words as samples from an underlying multinomial distribution.
The use of the multinomial distribution is an important aspect of our methodology and differs from analyses based on the
multivariate normal distribution (e.g. Holmes and Forsyth, 1995). When dealing with frequency data, percentages sum to
100% and induce negative correlations between the frequencies of function words. In the normal setting, the negative
correlations are not quite handled correctly even when a non-diagonal variance matrix is permitted. Furthermore, the
discrete nature of the data is not taken into consideration when analyses are based on the normal distribution.
Our approach also differs from most stylometric analyses in that it is Bayesian. Unlike deterministic clustering
algorithms, this implies that there is a quantifiable uncertainty in the resultant clusters of texts. We account for the
uncertainty by expressing probabilities associated with clusters. Although methods of fuzzy analysis (Dunn, 1977) provide
membership coefficients for individual objects, classical (as opposed to model-based) agglomerative approaches do not
provide probability assessments for clustering. Clustering probabilities are sometimes available using Bayesian mixture
models (Liu et al., 2003). However, these methods typically rely on Markov chains which often require fine tuning to
promote mixing in the Markov chain. A mixture model-based on generalized Dirichlet multinomial distributions to cluster
count data with applications to digit recognition is discussed in Bouguila (2008).
In Section 2, we describe the underlying Bayesian model used in our approach. The data on function word frequencies are
assumed to arise from multinomial distributions. The parameters of the multinomial distributions are therefore characteristics
of an author’s writing style, and clustering is carried out on these parameters. As there exists uncertainty in the parameters, it
follows that there is uncertainty in the clustering which leads to the calculation of posterior clustering probabilities. In Gill et al.
(2007), stylometric clustering is done in a two-stage process whereby parameter output is generated from a Markov chain, and
the output is then fed into a standard clustering algorithm. In the proposed approach, the clustering is more natural as the
clustering is an inherent part of the statistical model. This is accomplished via the Dirichlet process whose support is restricted
to discrete distributions (Ferguson, 1974) and is therefore well-suited for clustering. Using Dirichlet process priors also allows
weaker prior assumptions by going from a parametric to a semiparametric framework. Our approach makes various prior
assumptions that are appropriate to the stylometric context. We also suggest a pragmatic approach to the selection of function
words. Our methodology and analysis is computational and we discuss aspects of the computation in Section 3. Some theory
and discussion is provided regarding the effect of increasing the number of function words. In Section 4, a detailed simulation
study provides insights regarding the interplay of the various input parameters of the algorithm. The simulation study suggests
conditions when the approach may be effective. In Section 5, the methods are applied to the well-studied problem concerning
the authorship of the Federalist papers. In the analyses, we indicate how prior knowledge can be utilized by grouping texts in
appropriate ways and by varying the concentration parameter used in the Dirichlet process. A concluding discussion is
provided in Section 6.
2. Bayesian model
A principle of stylometric analysis is that authors use high-frequency function words unreflectively in their writings.
These function words occur regardless of context, and hence, differential rates of usage form a basis for distinguishing
authorship. Function words are typically prepositions, conjunctions, articles and common verbs. The choice of function
words is typically determined by a subject matter expert. Given the selection of K function words, we assume that
frequency counts are obtained for each of N texts. Let Xik denote the frequency of function word k in the ith text,
k¼ 1, . . . ,K , i¼ 1, . . . ,N: This gives rise to the model
Xi ¼ ðXi1, . . . ,XiK ÞT Multinomialðni; pi1, . . . ,piK Þ ð1Þ
where ni ¼
P
kXik is the total number of function words in the ith text, the multinomial vectors are assumed independent
over i¼ 1, . . . ,N and
P
kpik ¼ 1. The multinomial distribution is a natural distribution for use in this problem. However, in a
clustering context, we are not aware of any model-based mixture approaches that use the multinomial distribution.
Perhaps this is due to the fact that there is no clear choice for a dissimilarity measure between the multinomial vectors
X1, . . . ,XN in classical partitioning and hierarchical clustering algorithms. Traditional stylometric methods are often based
on large sample multivariate normal approximations to the multinomial distribution for word frequency data (Holmes and
Forsyth, 1995).
The unknown parameters in model (1) are the pik’s which represent the word prints of the author of the ith text. In a
standard Bayesian analysis based on (1), it is typical to assign independent flat priors to pi ¼ ðpi1, . . . ,piK ÞT for i¼ 1, . . . ,N. In
the given application to stylometry, we suggest that it is more appropriate to assume that the pi’s are grouped in latent
clusters representing the underlying authors. We proceed by using a Dirichlet process mixture model whereby
pijG
iid
G ð2Þ
Gja,G0 DPða,G0Þ
P.S. Gill, T.B. Swartz / Journal of Statistical Planning and Inference 141 (2011) 3665–3674 3667for i¼ 1, . . . ,N. In non-technical language, (2) states that the parameters pi are independent and are distributed according
to G. However, the distribution G is unknown and this is the semiparametric aspect of the Dirichlet process. Furthermore,
(2) states that the distribution G arises from a distribution of distributions where E(G)¼G0 and the concentration
parameter a40 is such that larger (smaller) values of a imply that G is closer (further) in distributional distance to G0.
Again, an important but not so obvious feature of (2) is that the support of G is limited to discrete distributions, and this is
fundamental to the clustering of the pi’s. We also note that the Dirichlet process does not require the specification of the
number of clusters. Once a theoretical curiosity, the Dirichlet process and its extensions are finding increasing application
areas in nonparametric statistics. For a clear exposition of a Dirichlet process mixture model in an applied random-effects
setting together with references to the technical literature concerning the Dirichlet process, see Ohlssen et al. (2007).
To complete the Bayesian model, we require the specification of the baseline distribution G0 and the remaining prior
distributions. In our problem, we suggest the baseline distribution
G0 Dirichletða1, . . . ,aK Þ ð3Þ
where the Dirichlet is appropriately defined on the (K1)-dimensional simplex. Although setting a1 ¼    ¼ aK ¼ 1 in (3)
may be interpreted as noninformative, we take the view that the individual writing styles pi arise from a superpopulation
of writing styles. We therefore take an empirical Bayes approach and set ak ¼mqk where qk is the proportion of times that
function word k appears across all manuscripts (i.e. qk ¼
P
iXik=
P
ini ) for k¼ 1, . . . ,K. The specification implies that the kth
component of the Dirichlet distribution has expectation qk, k¼ 1, . . . ,K . The determination of m is based on variability
considerations where we equate the ‘‘empirical variance’’ s2 ¼
P
ðqiqÞ2=K with the theoretical variance of the kth
component of the Dirichlet distribution. This leads to ðmþ1Þs2 ¼ qkð1qkÞ, and summing over k¼ 1, . . . ,K gives
m¼
P
qkð1qkÞP
ðqkqÞ2
1
Finally, in various applications, the treatment of the concentration parameter a has sometimes proved problematic as
inferences may be sensitive to a (Dorazio, 2009). Accordingly, we treat the concentration parameter as a tuning parameter
and investigate its effect under various fixed settings. In our examples, we set values of a ranging from 1 to 100. This is
demonstrated in the simulation study of Section 4.2.1. Choice of function words
Earlier, we glossed over the problem of determining function words. In fact, this is a serious issue which has a direct
impact on the success of any stylometric approach.
A temptation and standard strategy is to look at the texts in question, and choose non-contextual function words that
have good discriminating power in differentiating texts between authors who are known to be distinct. In classical
analyses, such an approach typically violates the statistical procedures used in the analyses. The reason is that statistical
procedures such as t-tests are unconditional tests, and unconditional tests do not allow the user to first look at the data,
and then use the information to determine aspects of the test (i.e. determine the function words). This standard
stylometric strategy therefore argues for a Bayesian approach. In Bayesian statistics, all inferences proceed from the
posterior distribution, and the posterior describes parameter uncertainty conditional on the observed data. Bayesian
procedures are therefore conditional procedures which allow a user to make decisions on how to proceed based on an
initial inspection of the data.
In our Bayesian approach to stylometry, we determine function words based on data inspection. Suppose that we have
K non-contextual function words initially chosen by a subject matter expert. Often the subject matter expert may do
something as simple as list the most frequent words and eliminate contextual words from the list. As discussed at the end
of Section 3, we do not want K large, and therefore, we attempt to reduce the list of non-contextual function words.
Suppose further that we have N texts in the stylometric analysis. Given the kth function word under consideration and
texts i and j, we calculate the statistic
z¼
p̂ikp̂jkffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi
1
ni
þ 1
nj
 
~pð1 ~pÞ
s ð4Þ
where p̂lk ¼ Xlk=nl is the proportion of occurrence of function word k in text l, ~p ¼ ðXikþXjkÞ=ðniþnjÞ is the pooled
proportion of the given function word in text i and text j, and nl is the total number of function words in text l, l¼ i, j. The
statistic z in (4) is calculated for all KðN2Þ combinations of function words and pairs of texts. We sort jzj and use the largest
values to determine the reduced set of function words. This is a simple and effective method of determining function
words, and it is a legitimate procedure when implemented as part of a Bayesian analysis.
P.S. Gill, T.B. Swartz / Journal of Statistical Planning and Inference 141 (2011) 3665–367436683. Computational algorithm
In Bayesian analyses, the posterior distribution provides the full description of uncertainty for all that is unknown.
Accordingly, it is often convenient to describe the posterior using summaries such as posterior means and posterior
standard deviations. However, in most Bayesian applications, the complexity and dimensionality of the posterior prevents
the analytic calculation of summary measures which take the form of integrals. In these problems, it is common to take a
sampling approach to approximate the integrals, and Markov chain methods are often the sampling method of choice.
The Dirichlet process mixture model presented in Section 2 is a model whose complexity seems to demand an analysis
via Markov chain methods. Our first attempt at computation involved an implementation via the software package
WinBUGS (Spiegelhalter et al., 2003). WinBUGS analyses are appealing as the programmer need only specify the statistical
model, the prior and the data. In WinBUGS, the Markov chain calculations are done in the background whereby the user is
supplied with Markov chain output. As the Dirichlet process prior is not one of the available WinBUGS distributions,
an approximation of the prior was considered as carried out in Ghosh et al. (2010). The approximation is based on a
truncation of the ‘‘stick-breaking’’ sum as given in the Sethuraman (1994) construction of the Dirichlet process. We use
this approach as a confirmatory procedure to ensure that we have qualitatively correct results. Our preferred procedure
which we describe next does not require the tuning parameter involved in the stick-breaking truncation.
An alternative approach involves the direct programming of a Markov chain for the Dirichlet process mixture model of
Section 2. Neal (2000) describes various Markov chain algorithms for the analysis of Dirichlet process mixture models. Our
model has a conjugate structure (i.e. Dirichlet baseline distribution with multinomial data) and this facilitates the
development of algorithms. We implement Algorithm 2 given by Neal (2000) which involves the introduction of latent
variables describing the cluster membership of the writing styles p1, . . . ,pN . Compared to various alternative MCMC
algorithms, Algorithm 2 is thought to mix well and lead to practical convergence in realistic computing times for a variety
of problems (Neal, 2000). Specifically, let ci denote the latent class of the ith text and let nic be the size of latent class
c¼ 1, . . . ,G when text i is excluded from consideration. Then Algorithm 2 from Neal (2000) translates into iterations of the
following two steps: For i¼ 1, . . . ,N: generate ci according to Pr½ci ¼ c ¼ rc where
rcp
0 if nic ¼ 0
a
N1þa
 
Gða1þ    þaK Þ
Gða1Þ   GðaK Þ
Gða1þXi1Þ   GðaKþXiK Þ
Gða1þ    þaKþXi1þ    þXiK Þ
if c¼ Gþ1
nic
N1þa
 
pXi1c1   p
XiK
cK otherwise
8>>><
>>>:
ð5Þ
If ci belongs to an existing class cj, then set pi¼pj. If ci does not belong to an existing class, then generate
pi Dirichletða1þXi1, . . . ,aKþXiK Þ. P For each latent class c¼ 1, . . . ,G: sample the corresponding pc Dirichletðac1, . . . ,acK Þ where ack ¼ akþ i:ci ¼ cXik.The primary inferential question for stylometry concerns the attribution of authorship. To address this question, consider a
single iteration of the Markov chain which yields parameter output p1, . . . ,pN . For each pair of texts (i,j), we check whether
pi¼pj and increment its counter accordingly. After many iterations, we obtain the proportion of time that pi and pj are
equal, and this provides an estimate of the probability that authors of text i and j are the same.
In (5), the concentration parameter a is fixed and we view it as a tuning parameter for the algorithm. However, it may
be more satisfying to consider a fully Bayes procedure where a prior distribution is assigned to a. Neal (2000) states that
the random a case can be handled. Gill and Casella (2009) consider the case where a is discrete. In the continuous case, let
pðaÞ denote the prior density of a. Then it is not difficult to show that the full conditional distribution for a is
½ajp GðaÞ
GðaþNÞ
aGpðaÞ ð6Þ
where G is the number of latent classes. We recognize that (6) is a nonstandard density which does not readily admit
variate generation. We therefore imbed a Metropolis step in the Gibbs sampling algorithm (5) where qðaÞ is chosen as the
proposal density for a. The Metropolis step then proceeds by generating uUniformð0,1Þ and generating a according to
the proposal distribution. We denote the previous value of a in the Markov chain as a. We use the generated a from the
proposal as the next iterate in the Markov chain unless
u4
GðaÞ
GðaÞ
GðaþNÞ
GðaþNÞ
a
a
 G pðaÞ
pðaÞ
qðaÞ
qðaÞ
ð7Þ
in which case, we set a¼ a. As a particular case, we choose the prior density pðaÞ as Uniformð0,y0Þ for some prescribed y0.
Then noting that GðaÞaG=GðaþNÞ is increasing in a, we choose the proposal density qðaÞ ¼ 2a=y20 on ð0,y0Þ. Generating from
P.S. Gill, T.B. Swartz / Journal of Statistical Planning and Inference 141 (2011) 3665–3674 3669this proposal is straightforward via a¼ y0
ffiffiffi
v
p
where vUniformð0,1Þ and inequality (7) reduces to
u4
GðaÞ
GðaÞ
GðaþNÞ
GðaþNÞ
a
a
 G1
Initially, our intuition suggested that by increasing the number K of function words, the discriminating power of the
methodology would improve, and better stylometric inferences would be obtained. This is what is generally observed in
multivariate analyses where increasing the number of variables tends to increase the opportunity to discriminate.
However, our investigations suggest that this is not the case, and in fact, very large values of K lead to unreasonably small
numbers of clusters. Therefore, the user needs to strike a balance between adding function words that are truly
discriminatory versus adding function words that create noise in the analysis. To get a sense why this happens, consider
for simplicity the noninformative prior a1 ¼    ¼ aK ¼ 1. Then using the probabilities in (5), we investigate the probability
that a new cluster is formed in the Markov chain. Letting Q denote a constant and using Stirling’s approximation to the log-
gamma function, we have
log
rGþ1
rc
 
¼ logaþ logGðKÞþ
X
k
logGðXikþ1ÞlogG Kþ
X
k
Xik
 !
lognic
X
k
Xiklogpck
 logða=nicÞþðK1ÞlogðK1ÞðK1Þþ
X
k
XiklogXik
X
k
Xik
 K1þ
X
k
Xik
 !
log K1þ
X
k
Xik
 !
þ K1þ
X
k
Xik
 !

X
k
Xiklogpck
QþðK1ÞlogðK1Þ K1þ
X
k
Xik
 !
log K1þ
X
k
Xik
 !
rQþðK1ÞlogðK1Þ K1þ
X
k
Xik
 !
logðK1Þ
¼Q
X
k
XiklogðK1Þ-1
as K-1. This implies that new clusters do not form in the Markov chain when the number of function words becomes
excessively large.4. Simulation study
We now provide a comprehensive simulation study on the effect of various input parameters to the stylometric
clustering algorithm.4.1. The baseline data, model and analysis
We consider a stylometric problem involving N¼10 texts and three clusters. We generated data such that texts
i¼ 1, . . . ,5 belong to the first cluster, texts i¼6, 7, 8 belong to the second cluster and texts i¼9, 10 belong to the third
cluster. Specifically, for texts i¼ 1, . . . ,10, we generated data
ðXi1, . . . ,XiJÞT Multinomialð ~ni; pi1, . . . ,piJÞ
where J¼21 and we initially considered a total text size of ~ni ¼ 5000 words. Note that piJ ¼ 1
PJ1
j ¼ 1 pij corresponds to the
frequency of non-function words such that the number of function words that can be used in model (1) is K ¼ 2, . . . ,J1
leading to ni ¼
PK
k ¼ 1 Xik.
The multinomial parameters for texts i¼ 1, . . . ,5 in the first cluster were set as follows:pi1 pi2 pi3 , . . . ,pi5 pi6 , . . . ,pi8 pi9 pi10 pi11 , . . . ,pi200.060 0.030 0.025 0.020 0.010 0.005 0.002For texts i¼6, 7, 8 in the second cluster, we set
pij ¼
a2p1j j odd
b2p1j j even
(
ð8Þ
Table 1
The frequency counts Xi1 , . . . ,Xi10 corresponding to the first 10 function words of the baseline data from Section 4.1.
Text i Xi1 Xi2 Xi3 Xi4 Xi5 Xi6 Xi7 Xi8 Xi9 Xi10
1 316 142 116 131 124 95 111 82 41 20
2 271 145 118 154 132 99 105 101 56 17
3 307 161 114 123 124 96 115 110 43 22
4 322 137 124 127 132 106 94 86 41 35
5 298 150 127 125 134 95 91 95 57 33
6 268 161 127 129 117 111 96 102 56 31
7 245 188 112 131 116 116 90 106 41 24
8 268 158 112 151 129 109 85 116 39 26
9 347 139 123 121 130 73 105 85 52 27
10 356 142 114 107 126 66 108 60 58 23
Table 2
The posterior probability of clustering between texts i and j for the baseline data of Section 4.1 where K¼2 and aUniformð0,3Þ.
Text i Text j
1 2 3 4 5 6 7 8 9 10
1 0.00 0.43 0.46 0.67 0.53 0.23 0.01 0.26 0.64 0.64
2 0.43 0.00 0.64 0.35 0.60 0.55 0.11 0.58 0.29 0.29
3 0.46 0.64 0.00 0.39 0.63 0.53 0.09 0.55 0.32 0.31
4 0.67 0.35 0.39 0.00 0.47 0.16 0.00 0.19 0.71 0.71
5 0.53 0.60 0.63 0.47 0.00 0.44 0.06 0.47 0.41 0.40
6 0.23 0.55 0.53 0.16 0.44 0.00 0.26 0.66 0.11 0.11
7 0.01 0.11 0.09 0.00 0.06 0.26 0.00 0.23 0.00 0.00
8 0.26 0.58 0.55 0.19 0.47 0.66 0.23 0.00 0.14 0.14
9 0.64 0.29 0.32 0.71 0.41 0.11 0.00 0.14 0.00 0.75
10 0.64 0.29 0.31 0.71 0.40 0.11 0.00 0.14 0.75 0.00
P.S. Gill, T.B. Swartz / Journal of Statistical Planning and Inference 141 (2011) 3665–36743670where a2¼0.9 and b2¼1.1 were chosen to differentiate the second cluster from the first cluster at the 10% level. For texts
i¼9, 10 in the third cluster, we set
pij ¼
a3p1j j odd
b3p1j j even
(
ð9Þ
where a3¼1.1 and b3¼0.9 were chosen to differentiate the third cluster from the first cluster at the 10% level. With these
settings, the first cluster ‘‘lies’’ somewhere between the second and third clusters.
The generated data are intended to portray a realistic stylometric problem. We have considered texts which are of
standard essay length (5000 words) and the function word frequencies pik correspond to the usage of common non-
contextual words such as ‘‘the’’ and ‘‘as’’. Note that the frequencies decrease as we increase the number of function words;
this is realistic as most English function words have frequencies less than 2%. Table 1 provides the data counts Xi1, . . . ,Xi10
for the first 10 function words for each of the texts i¼ 1, . . . ,10.
Having generated the baseline data, we completed the specification of the baseline stylometric model by using the
continuous prior aUniformð0,3Þ and using the first K¼2 function words.
Table 2 provides the resultant posterior probabilities of pairwise clustering between texts. Using a probability threshold
of 0.5 for clustering, Table 2 suggests three clusters with text memberships f1,4,9,10g, f2,3,5,6,8g and f7g. At first glance,
the clustering results may appear poor. However, upon close inspection of the first two columns of Table 1 (recall K¼2),
the clustering appears sensible and reflects the variation of the data generation mechanism. The posterior probabilities in
Table 2 are reasonable in the sense that they are not too close to either 0 or 1. For example, texts 2, 3 and 5 are nearly
clustered with texts 1 and 4. We repeated the exercise by generating several datasets under the same input conditions, and
in each of the cases, the clustering was ‘‘incorrect’’ with a number of misclassified texts.4.2. The effect of the function word probabilities pik
Hopefully, the methodology is better able to distinguish differences between texts when there is in fact greater
differentiation between texts. To explore this, we changed the differentiation parameters in (8) and (9) to a2¼0.8, b2¼1.2,
Table 3
The posterior probability of clustering between texts i and j for the baseline data of Section 4.1 where K¼2 and aDiscrete Uniformð1, . . . ,20Þ.
Text i Text j
1 2 3 4 5 6 7 8 9 10
1 0.00 0.32 0.35 0.52 0.41 0.16 0.00 0.19 0.48 0.48
2 0.32 0.00 0.51 0.26 0.47 0.43 0.06 0.45 0.20 0.19
3 0.35 0.51 0.00 0.28 0.49 0.41 0.05 0.43 0.21 0.21
4 0.52 0.26 0.28 0.00 0.35 0.11 0.00 0.13 0.54 0.54
5 0.41 0.47 0.49 0.35 0.00 0.34 0.03 0.36 0.28 0.27
6 0.16 0.43 0.41 0.11 0.34 0.00 0.17 0.52 0.07 0.07
7 0.00 0.06 0.05 0.00 0.03 0.17 0.00 0.15 0.00 0.00
8 0.19 0.45 0.43 0.13 0.36 0.52 0.15 0.00 0.08 0.08
9 0.48 0.20 0.21 0.54 0.28 0.07 0.00 0.08 0.00 0.57
10 0.48 0.19 0.21 0.54 0.27 0.07 0.00 0.08 0.57 0.00
P.S. Gill, T.B. Swartz / Journal of Statistical Planning and Inference 141 (2011) 3665–3674 3671a3¼1.2 and b3¼0.8 corresponding to an increased 20% level of differentiation. We generated four datasets and the
clustering results were as follows: f1,2,3,4,5g, f6,7,8g, f9,10g
 f1,2,3,4,5,8g, f6g, f7g, f9,10g
 f1,2,3,4,5,6g, f7,8g, f9,10g
 f1,3,4,5g, f2g, f6,7,8g, f9,10gAlthough the clustering is ‘‘perfect’’ only in the first case, the overall clustering performance here is better than in the case
of 10% differentiation. This provides evidence that our stylometric methodology makes better decisions when authors are
more heterogeneous in their writing styles.
4.3. The effect of the DP concentration parameter a
One of the contributions in the paper is the specification and implementation of a continuous prior for a. In the analysis
of Section 4.1, the posterior mean of a was 1.6. If we instead use a discrete Uniform prior for a (Gill and Casella, 2009), we
observe that the posterior means can differ substantially. For example, using a discrete Uniformð1, . . . ,20Þ prior, the
posterior mean of a is 5.1. The clustering results also differ with texts apportioned to six clusters according to f1,4g, f2,3g,
f5g, f6,8g, f7g and f9,10g. This is expected as expanding the limit of support for the concentration parameter a is known to
increase the numbers of clusters. However, the good news is that changing the prior for the nuisance parameter a did not
greatly change the pairwise clustering probabilities. In Table 3, we provide the posterior pairwise clustering probabilities
using the baseline data given in Section 4.1 with K¼2 and aDiscrete Uniformð1, . . . ,20Þ. The entries in Tables 2 and 3 are
qualitatively similar in the sense that for every text we have the same rank order for the row probabilities.
We also consider the effect of Gammaða,bÞ priors which appear well-suited to the stylometric application. Unlike
Uniform distributions with truncated support that needs to be specified, Gamma distributions are defined on ð0,1Þ.
Another appealing feature of the Gamma family is that the Metropolis step (7) can be avoided. Instead, Escobar and West
(1995) provide a direct expression for the posterior distribution of a as a mixture of two Gamma’s involving an auxiliary
parameter Z. Dorazio (2009) provides a method of selecting the Gamma parameters a and b such that the number of
possible clusters 1, . . . ,N is apriori equiprobable. In our simulated baseline data with N¼10, Dorazio’s (2009) method
provides a¼ 0:525 and b¼0.046. In this case, the clustering methodology yields the partition f1g, f2g, f3g, f4,9,10g, f5g, f6g,
f7g and f8g, which as anticipated, contains more clusters than when using the Uniformð0,3Þ prior. However, if we were
apriori more inclined toward fewer clusters, we might consider changing the clustering threshold from 0.5 to 0.4. With the
lower clustering threshold, the posterior pairwise clustering probabilities yield the partition f1,4,9,10g, f2,3,5,6,8g and f7g
which is the same partition when using the Uniformð0,3Þ prior.
Further, we note that using various fixed values of a (e.g. a¼ 5:0) does not result in markedly different clustering.
4.4. The effect of the text sizes ~ni
Our intuition is that increasing text sizes leads to more information which in turn leads to improved clustering. To
explore this conjecture, we generated a new dataset according to the specifications of Section 4.1 except that each of the
text sizes is increased 10-fold from ~ni ¼ 5000 words to ~ni ¼ 50,000 words.
The pairwise clustering results corresponding to the new dataset are presented in Table 4. From Table 4, we observe
‘‘perfect’’ clustering (i.e. f1,2,3,4,5g, f6,7,8g, f9,10g) and we note that the probability assessments are more definitive than
in Table 2. These observations suggest that our stylometric methodology may provide better results when we study
larger texts.
Table 4
The posterior probability of clustering between texts i and j based on larger text sizes of ~ni ¼ 50,000 words where K¼2 and aUniformð0,3Þ.
Text i Text j
1 2 3 4 5 6 7 8 9 10
1 0.00 0.90 0.88 0.88 0.92 0.01 0.00 0.00 0.00 0.00
2 0.90 0.00 0.86 0.85 0.90 0.01 0.00 0.00 0.00 0.00
3 0.88 0.86 0.00 0.90 0.89 0.02 0.00 0.00 0.00 0.00
4 0.88 0.85 0.90 0.00 0.88 0.02 0.00 0.00 0.00 0.00
5 0.92 0.90 0.89 0.88 0.00 0.01 0.00 0.00 0.00 0.00
6 0.01 0.01 0.02 0.02 0.01 0.00 0.84 0.77 0.00 0.00
7 0.00 0.00 0.00 0.00 0.00 0.84 0.00 0.86 0.00 0.00
8 0.00 0.00 0.00 0.00 0.00 0.77 0.86 0.00 0.00 0.00
9 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.92
10 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.92 0.00
Table 5
The posterior probability of clustering between texts i and j for the baseline data of Section 4.1 where K¼6 and aUniformð0,3Þ.
Text i Text j
1 2 3 4 5 6 7 8 9 10
1 0.00 0.76 0.95 0.97 0.95 0.75 0.68 0.75 0.22 0.21
2 0.76 0.00 0.80 0.77 0.81 0.97 0.91 0.98 0.01 0.01
3 0.95 0.80 0.00 0.96 0.97 0.79 0.72 0.79 0.18 0.18
4 0.97 0.77 0.96 0.00 0.96 0.75 0.68 0.75 0.21 0.21
5 0.95 0.81 0.97 0.96 0.00 0.79 0.72 0.79 0.18 0.17
6 0.75 0.97 0.79 0.75 0.79 0.00 0.93 0.98 0.00 0.00
7 0.68 0.91 0.72 0.68 0.72 0.93 0.00 0.93 0.00 0.00
8 0.75 0.98 0.79 0.75 0.79 0.98 0.93 0.00 0.00 0.00
9 0.22 0.01 0.18 0.21 0.18 0.00 0.00 0.00 0.00 0.99
10 0.21 0.01 0.18 0.21 0.17 0.00 0.00 0.00 0.99 0.00
P.S. Gill, T.B. Swartz / Journal of Statistical Planning and Inference 141 (2011) 3665–367436724.5. The effect of the number of function words K
In this section, we consider the effect of the number of function words K on clustering. Although our intuition suggests
that larger K leads to more information and therefore improved clustering, we will see that the effect of K is less
straightforward.
To investigate the effect of K, we ran the stylometric clustering algorithm on the baseline data of Section 4.1 using K¼6.
The posterior pairwise probabilities are reported in Table 5 and lead to the two clusters f1,2,3,4,5,6,7,8g and f9,10g. This is
an improvement over the analysis of Section 4.1 where K¼2 was used and clusters f1,4,9,10g, f2,3,5,6,8g and f7g were
obtained. We note that the pairwise probabilities in Table 5 are more affirmative than those in Table 2. This reflects one of
the idiosyncrasies regarding the effect of increasing K. We remark that when K¼10, we obtain the same clustering results
(f1,2,3,4,5,6,7,8g and f9,10g) except that the pairwise probabilities become even more affirmative. For example, the
posterior probability of clustering between texts 1 and 2 increases from 0.76 with K¼6 to 0.81 with K¼10.
When we set K¼20 and use the baseline dataset from Section 4.1, all of the 10 texts clustered together with posterior
pairwise probabilities of 1.0. This is clearly problematic, and confirms the theoretical derivation from Section 3 that
suggests that increasing K leads to the formation of fewer clusters. Although we have reported results only in the case of
the dataset from Section 4.1, we have seen these patterns in all of the datasets that we have generated.
The complex role of K in our clustering algorithm does not appear to be generally well-known. The reason for this may
be that most applications of the DP in hierarchical models (2) involve a parameter (pi in our case) that is univariate. For
example, often an experimenter has a univariate parameter that represents means of sub-populations. In our case, the
parameter pi has dimension K1, and it is the increased dimensionality which impedes the formation of additional
clusters.
When a new function word is added, will it assist in discovering new clusters? If the function word is truly
discriminatory, it may. However, if the new function word is not discriminatory (i.e. pi,Kþ1  pj,Kþ1 ), then because the pik’s
lie on a simplex (i.e. pi1þ    þpiK ¼ 1), a new function word reduces the magnitude of JpikpjkJ for k¼ 1, . . . ,K.
It therefore appears that we need to strike a balance between the additional discriminatory information provided by
increasing the number K of function words versus the tendency of forming fewer clusters. This highlights the importance
of choosing a ‘‘good’’ but small set of function words in stylometric analyses.
Table 6
The 10 function words used in the analysis of the Federalist papers.
ANOTHER ALSO ANY AND AS ON ARE VOICE AN ALL
P.S. Gill, T.B. Swartz / Journal of Statistical Planning and Inference 141 (2011) 3665–3674 36735. The Federalist papers
In 1788, 85 articles were compiled and published as The Federalist. The main purpose of these essays (most of which
had appeared earlier in newspapers) was to persuade citizens of the State of New York to ratify the new Constitution of the
United States. It is widely assumed that the authors of these papers were limited to Alexander Hamilton, John Jay and
James Madison. The overall consensus amongst scholars is that Hamilton was the author of 51 of the papers, Madison
wrote 14, Jay wrote 5, and 3 were jointly written by Hamilton and Madison. The authorship of the remaining 12 papers
(referred to as the disputed papers) is unknown, but each of these disputed papers is widely thought to be the work of
either Hamilton or Madison. Mosteller and Wallace (1963, 1984) analyzed the distribution of function words extracted
from the Federalist papers and concluded that Madison was the author of all 12 disputed papers. The Federalist papers are
now considered a test case for new methods of authorship attribution and the papers are freely available from both the
Project Gutenberg and the Library of Congress websites. An interesting account of earlier, though unsuccessful, efforts for
determining authorship of the Federalist papers is given by Mosteller and Wallace (1984).
To investigate our methodology with respect to the Federalist papers, we amalgamate all of the 51 Hamilton papers
into a single text. We also amalgamate the 14 Madison papers into a single text. We eliminate the five Jay texts from the
analysis as the disputed texts are believed to be written by either Hamilton or Madison. We also eliminate the three joint
papers as it is unclear that the writing styles of these texts ought to exhibit the style of Hamilton, the style of Madison or
some intermediate style. Thus we have N¼1þ1þ12¼14 texts under consideration where the text sizes of the disputed
papers are much smaller than the text sizes of the two amalgamated texts.
The word frequency data for each of the papers were extracted using WordSmith tools (Scott, 1998). We started with a
list of 125 function words gathered from Tables 2.5, 2.6 and 2.7 of Mosteller and Wallace (1963). We emphasize that the
starting list of function words was not based on any screening. As observed in the simulation study in Section 4, selection
of a small number of function words is very crucial for stylometric analysis. In the case of Federalist papers we are
interested in identifying the authorship of disputed papers between Hamilton and Madison. Therefore, it seems sensible
that the function words used be such that they differ in the frequency of occurrence in the known works of Hamilton and
Madison. Using the Z-test methodology of Section 3 on the amalgamated Hamilton and Madison federal papers, we ended
up with 10 words with the largest jZj values. These words, ordered according to decreasing jZj value, are listed in Table 6.
We ran the Dirichlet process mixture algorithm using the selected 10 function and the continuous prior
aUniformð0,3Þ and obtained the same conclusions as Mosteller and Wallace (1963, 1984). That is, two clusters were
obtained with all 12 of the disputed papers clustering with Madison and the second cluster consisting of the amalgamated
Hamilton text. The strength of membership of the disputed papers with Madison was strong. The smallest posterior
probability of membership amongst the 12 disputed texts into the ‘‘Madison cluster’’ was 0.70.6. Discussion
In this paper, we have developed an algorithm for the stylometric clustering of texts. Some of the features of the
approach include suggestions on the selection of function words, the incorporation of the multinomial distribution and a
semiparametric Bayesian framework based on the Dirichlet process. The Dirichlet process is well suited to the stylometric
problem as clustering is a by-product of model development and posterior probability assessments can be obtained with
respect to clusters. The approach also enables an user to incorporate prior knowledge concerning authorship by
amalgamating texts which are believed to have the same author.
The prior for the function word probabilities pik and prior for the DP concentration parameter a have been proposed for
the stylometric problem in hand. However, we note that apart from these priors, the proposed algorithm has much greater
applicability as a general clustering tool for categorical (i.e. multinomial) data.
In a honest appraisal concerning the suitability of the algorithm for stylometric problems, one keeps in mind the adage
that ‘‘the more that is assumed, the more that can be inferred’’. In the case of the DP mixture model, very little is assumed.
It is therefore comforting that our approach replicates long-standing views concerning the authorship of the Federalist
papers. In stylometric problems where there is controversy concerning authorship, it remains to be seen whether the
proposed approach can differentiate texts. As investigated in Section 4, this is partly a function of text size and the
‘‘differentiability’’ between texts.
With the need to keep the number of function words K at a reasonable level, the approach may be best suited to
stylometric problems where the potential number of authors (i.e. clusters) is relatively small. Ultimately, as discussed in
some detail in the paper, the existence of a small set of good discriminating function words provides the best chance for
success.
P.S. Gill, T.B. Swartz / Journal of Statistical Planning and Inference 141 (2011) 3665–36743674Acknowledgments
The authors were partially supported by research grants from the Natural Sciences and Engineering Research Council of
Canada. The authors are appreciative of the comments made by an anonymous referee. The comments have lead to an
improvement in the manuscript.
References
Bouguila, N., 2008. Clustering of count data using generalized Dirichlet multinomial distributions. IEEE Transactions on Knowledge and Data Engineering
20, 462–474.
Dorazio, R.M., 2009. On selecting a prior for the precision parameter of Dirichlet process mixture models. Journal of Statistical Planning and Inference 139,
3384–3390.
Dunn, J.C., 2002. Indices of partition fuzziness and the detection of clusters in large data sets. In: Gupta, M. (Ed.), Fuzzy Automata and Decision Processes.
Elsevier, New York, pp. 271–284.
Escobar, M.D., West, M., 1995. Bayesian density estimation and inference using mixtures. Journal of the American Statistical Association 90, 577–588.
Ferguson, T.S., 1974. Prior distributions on spaces of probability measures. Annals of Statistics 2, 615–629.
Gill, J., Casella, G., 2009. Nonparametric priors for ordinal Bayesian social science models: specification and estimation. Journal of the American Statistical
Association 104, 453–464.
Gill, P.S., Swartz, T.B., Treschow, M., 2007. A stylometric analysis of King Alfred’s literary works. Journal of Applied Statistics 34, 1251–1258.
Ghosh, P., Gill, P.S., Muthukumarana, S., Swartz, T.B., 2010. A semiparametric Bayesian approach to network models using Dirichlet process priors.
Australian and New Zealand Journal of Statistics 52, 289–302.
Holmes, D.I., Forsyth, R.S., 1995. The Federalist revised: new directions in author attribution. Literary and Linguistic Computing 10, 111–127.
Holmes, D.I., 1999. Encyclopedia of Statistical Sciences: Update, vol. 3. Wiley, New York, pp. 721–727.
Liu, J.S., Zhang, J.L., Palumbo, M.L., Lawrence, C.E., 2003. Bayesian clustering with variable and transformation selections. In: Bernardo, J.M. (Ed.), Bayesian
Statistics, vol. 7. Oxford University Press, Oxford, pp. 249–276.
Mosteller, F., Wallace, D.L., 1963. Inference in an authorship problem. A comparative study of discrimination methods applied to the authorship of the
disputed Federalist papers. Journal of the American Statistical Association 58, 275–309.
Mosteller, F., Wallace, D.L., 1984. Applied Bayesian and Classical Inference: The Case of the Federalist Papers. Springer-Verlag, New York.
Neal, R.M., 2000. Markov chain sampling methods for Dirichlet process mixture models. Journal of Computational and Graphical Statistics 9, 249–265.
Ohlssen, D.I., Sharples, L.D., Spiegelhalter, D.J., 2007. Flexible random-effects models using semi-parametric models: applications to institutional
comparisons. Statistics in Medicine 26, 2088–2112.
Scott, M., 1998. WordSmith Tools Manual Version 3.0. Oxford University Press, Oxford.
Sethuraman, J., 1994. A constructive definition of Dirichlet priors. Statistica Sinica 4, 639–650.
Spiegelhalter, D., Thomas, A., Best, N., 2003. WinBUGS (Version 1.4) User Manual. MRC Biostatistics Unit, Cambridge.
