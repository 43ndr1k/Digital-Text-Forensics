A Framework for Performer Identification in
Audio Recordings
Rafael Ramirez and Esteban Maestre
Music Technology Group
Pompeu Fabra University
Tanger 38, Barcelona, Spain
Tel:+34 935421365, Fax:+34 935422202
rafael.ramirez@upf.edu, esteban.maestre@upf.edu
Abstract. We present a general framework for the task of identifying
performers from their playing styles. We investigate how musicians ex-
press and communicate their view of the musical content in pieces and
how to use this information in order to automatically identify perform-
ers. We study note-level deviations of parameters such as timing and
amplitude. Our approach to performer identification consists of inducing
an expressive performance model for each of the interpreters (essentially
establishing a performer dependent mapping of inter-note features to a
timing and amplitude expressive transformations). We outline two suc-
cessful performer identification case studies.
1 Introduction
Music performance plays a central role in our musical culture today. Concert
attendance and recording sales often reflect people’s preferences for particular
performers. The manipulation of sound properties such as pitch, timing, ampli-
tude and timbre by different performers is clearly distinguishable by the listeners.
Expressive music performance studies the manipulation of these sound properties
in an attempt to understand expression in performances. There has been much
speculation as to why performances contain expression. Hypothesis include that
musical expression communicates emotions and that it clarifies musical struc-
ture, i.e. the performer shapes the music according to her own intentions.
In this paper we describe a general framework for the task of identifying
performers from their playing style using high-level descriptors extracted from
single-instrument audio recordings. The identification of performers by using the
expressive content in their performances raises particularly interesting questions
but has nevertheless received relatively little attention in the past.
The data used in our investigations are monophonic audio recordings of music
performances. We use sound analysis techniques based on spectral models [11]
for extracting high-level symbolic features from the recordings. Once the rele-
vant high-level information is extracted we apply machine learning techniques to
automatically discover regularities and expressive patterns for each performer.
We use these regularities and patterns in order to identify a particular performer
in a given audio recording.
The rest of the paper is organized as follows: Section 2 describes related
work. Section 3 describes how we process the audio recordings in order to extract
inter-note information. Section 4 presents our approach to performance-driven
performer identification. Section 5 briefly describes two case studies on identify-
ing performers based on their playing style, and finally, Section 6 presents some
conclusions.
2 Related work
Understanding and formalizing expressive music performance is an extremely
challenging problem which in the past has been studied from different perspec-
tives. The main approaches to empirically studying expressive performance have
been based on statistical analysis (e.g. [9]), mathematical modeling, and analysis-
by-synthesis (e.g. [2]). In all these approaches, it is a person who is responsible
for devising a theory or mathematical model which captures different aspects of
musical expressive performance. The theory or model is later tested on real per-
formance data in order to determine its accuracy. The majority of the research
on expressive music performance has focused on the performance of musical ma-
terial for which notation (i.e. a score) is available, thus providing unambiguous
performance goals. Expressive performance studies have also been very much fo-
cused on (classical) piano performance in which pitch and timing measurements
are simplified.
Previous research addressing expressive music performance using machine
learning techniques has included a number of approaches, e.g. [4], [8]. Neverthe-
less, the use of expressive performance models, either automatically induced or
manually generated, for identifying musicians has received little attention in the
past. This is mainly due to two factors: (a) the high complexity of the feature
extraction process that is required to characterize expressive performance, and
(b) the question of how to use the information provided by an expressive per-
formance model for the task of performance-based performer identification. To
the best of our knowledge, the only group working on performance-based auto-
matic performer identification is the group led by Gerhard Widmer. Saunders et
al [10] apply string kernels to the problem of recognizing famous pianists from
their playing style. The characteristics of performers playing the same piece are
obtained from changes in beat-level tempo and beat-level loudness. From such
characteristics, general performance alphabets can be derived, and pianists’ per-
formances can then be represented as strings. They apply both kernel partial
least squares and Support Vector Machines to this data.
Stamatatos and Widmer [12] address the problem of identifying the most
likely music performer, given a set of performances of the same piece by a num-
ber of skilled candidate pianists. They propose a set of very simple features for
representing stylistic characteristics of a music performer that relate to a kind of
’average’ performance. A database of piano performances of 22 pianists playing
two pieces by Frdric Chopin is used. They propose an ensemble of simple clas-
sifiers derived by both subsampling the training set and subsampling the input
features. Experiments show that the proposed features are able to quantify the
differences between music performers.
3 Feature extraction
We perform a spectral analysis of a portion of sound, called analysis frame,
whose size is a parameter of the algorithm. This spectral analysis consists of
multiplying the audio frame with an appropriate analysis window and performing
a Discrete Fourier Transform (DFT) to obtain its spectrum. In this case, we use
a frame width of 46 ms, an overlap factor of 50%, and a Keiser-Bessel 25dB
window. Then, we compute a set of low-level descriptors for each spectrum:
energy and an estimation of the fundamental frequency. From these low-level
descriptors we perform a note segmentation procedure. Once the note boundaries
are known, the note descriptors are computed from the low-level values. the main
low-level descriptors used to characterize note-level expressive performance are
instantaneous energy and fundamental frequency.
Energy computation. The energy descriptor is computed on the spectral do-
main, using the values of the amplitude spectrum at each analysis frame. In
addition, energy is computed in different frequency bands as defined in [3], and
these values are used by the algorithm for note segmentation.
Fundamental frequency estimation. For the estimation of the instantaneous
fundamental frequency we use a harmonic matching model derived from the
Two-Way Mismatch procedure (TWM) [5].
Note segmentation. Note segmentation is performed using a set of frame
descriptors, which are energy computation in different frequency bands and fun-
damental frequency. Energy onsets are first detected following a band-wise algo-
rithm that uses some psycho-acoustical knowledge [3]. In a second step, funda-
mental frequency transitions are also detected. Finally, both results are merged
to find the note boundaries (onset and offset information).
Note descriptors. We compute note descriptors using the note boundaries and
the low-level descriptors values. The low-level descriptors associated to a note
segment are computed by averaging the frame values within this note segment.
Pitch histograms have been used to compute the pitch note and the fundamental
frequency that represents each note segment, as found in [6]. This is done to
avoid taking into account mistaken frames in the fundamental frequency mean
computation.
Musical Analysis. In order to provide an abstract structure for the record-
ings, we use Narmour’s theory of perception and cognition of melodies [7] to
analyze the performances. Narmour’s theory states that a melodic musical line
continuously causes listeners to generate expectations of how the melody should
continue. According to Narmour, there are two principles (registral direction and
intervallic difference) that determine the listener expectations. Based on these
two principles, melodic patterns or groups can be identified that either satisfy or
violate the implication as predicted by the principles. Such patterns are called
structures and are labeled to denote characteristics in terms of registral direction
and intervallic difference. A note in a melody often belongs to more than one
structure. Thus, a description of a melody as a sequence of Narmour structures
consists of a list of overlapping structures. We parse each melody in the training
data in order to automatically generate an implication/realization analysis of
the pieces.
4 Performance-driven Performer Identification
Our general approach to performer identification consists of inducing an expres-
sive performance model for each performer considered and given a new perfor-
mance determine which of the induced models fits better the new performance.
In other words, the approach can be stated as follows:
For each performer Pi, induce a performance model Mi
Given a new melody
Initialize Scorei=0
for each note in melody
for each interpreter Pi
update Scorei according to Mi
return Pm such that Scorem=max(Score1,,Scoren)
Clearly, the expressive models Mi play a central role in the output of classi-
fier. In this paper, the expressive models have been induced by applying Tildes
top-down decision tree induction algorithm ([1]). Tilde can be considered as a
first order logic extension of the C4.5 decision tree algorithm: instead of test-
ing attribute values at the nodes of the tree, Tilde tests logical predicates. This
provides the advantages of both propositional decision trees (i.e. efficiency and
pruning techniques) and the use of first order logic (i.e. increased expressiveness).
Next, we describe the note features we use to train the algorithms to obtain the
models.
4.1 Note features
The note features represent both properties of the note itself and aspects of the
musical context in which the note appears. Information about the note includes
note pitch and note duration, while information about its melodic context in-
cludes the relative pitch and duration of the neighboring notes (i.e. previous and
following notes) as well as the Narmour structures to which the note belongs.
The note’s Narmour structures are computed by performing the musical analysis
described before. Thus, each performed note is characterized by the tuple
(Pitch, Dur, PrevPitch, PrevDur, NextPitch, NextDur, Nar1, Nar2, Nar3)
5 Case studies
5.1 Celic jigs
In this study we are focused on Celtic jigs, fast tunes but slower that reels, that
usually consist of eighth notes in a ternary time signature, with strong accents at
each beat. The training data used in our experimental investigations are mono-
phonic recordings of nine Celtic jigs performed by two professional violinists.
Apart from the tempo (they played following a metronome), the musicians were
not given any particular instructions on how to perform the pieces.
For evaluation purposes, We held out approximately 30% of the data as test
data while the remaining 70% was used as training data (we held out 3 pieces
for each violinist). When selecting the test data, we left out the same number
of melody fragments per class. In order to avoid optimistic estimates of the
classifier performance, we explicitly removed from the training set all melody
fragment repetitions of the hold out fragments. This is motivated by the fact
that musicians are likely to perform a melody fragment and its repetition in
a similar way. We tested our algorithm in each of the six test pieces (three
pieces of each class) and obtained 100% accuracy (correctly classified instances
percentage). This is, the six pieces in the test set were classified correctly.
5.2 Jazz standards
The training data used in this case study are monophonic recordings of four Jazz
standards (Body and Soul, Once I loved, Like Someone in Love and Up Jumped
Spring) performed by three different professional saxophonists in a controlled
studio environment. Each piece was performed at two different tempos. For each
note in the training data, in addition to the note featues described, we also
computed several intra-note features such as notes attack level, sustain duration,
sustain slope, amount of legato with the previous note, amount of legato with
the following note, mean energy, spectral centroid and spectral tilt.
There were a total of aproximately 800 notes available for each performer.
We segmented each of the performed pieces in phases and obtain a total of
120 short phrases and 32 long phrases for each performer. The length of the
obtained phrases and long phrases ranged from 5 to 12 notes and 40 to 62 notes,
respectively. In the short phrase case, the average accuracy of the classifier was
96.64%. In the short phrase case, the average accuracy was 95.77%.
6 Conclusion
We presented a general framework for the task of identifying performers from
their playing style using note descriptors extracted from audio recordings. We
characterized performances by representing each note in the performance by a
set of note features representing the context in which the note appears. We
then induced an expressive performance model for each of the performers and
briefly presented two successful performer identification case studies. The results
obtained seem to indicate that the note features selected contain sufficient infor-
mation to identify the studied set of performers, and that the machine learning
method explored is capable of learning performance patterns that distinguish
these performers.
Acknowledgments. This work is supported by project TIN2006-14932-C01-01.
References
1. H. Blockeel, L. D. Raedt, and J. Ramon. Top-down induction of clustering trees.
In Proceedings of the 15th International Conference on Machine Learning, 1998.
2. Friberg, A.; Bresin, R.; Fryden, L.; 2000. Music from Motion: Sound Level En-
velopes of Tones Expressing Human Locomotion. Journal of New Music Research
29(3): 199-210.
3. Klapuri, A. (1999). Sound Onset Detection by Applying Psychoacoustic Knowl-
edge, Proceedings of the IEEE International Conference on Acoustics, Speech and
Signal Processing, ICASSP.
4. Lopez de Mantaras, R. and Arcos, J.L. (2002). AI and music, from composition to
expressive performance, AI Magazine, 23-3.
5. Maher, R.C. and Beauchamp, J.W. (1994). Fundamental frequency estimation of
musical signals using a two-way mismatch procedure, Journal of the Acoustic So-
ciety of America, vol. 95 pp. 2254-2263.
6. McNab, R.J., Smith Ll. A. and Witten I.H., (1996). Signal Processing for Melody
Transcription, SIG working paper, vol. 95-22.
7. Narmour, E. (1990). The Analysis and Cognition of Basic Melodic Structures: The
Implication Realization Model. University of Chicago Press.
8. Rafael Ramirez, Amaury Hazan, Esteban Maestre, Xavier Serra, A Data Mining
Approach to Expressive Music Performance Modeling, in Multimedia Data mining
and Knowledge Discovery, Springer.
9. Repp, B.H. (1992). Diversity and Commonality in Music Performance: an Analysis
of Timing Microstructure in Schumann’s ‘Traumerei’. Journal of the Acoustical
Society of America 104.
10. Saunders C., Hardoon D., Shawe-Taylor J., and Widmer G. (2004). Using String
Kernels to Identify Famous Performers from their Playing Style, Proceedings of
the 15th European Conference on Machine Learning (ECML’2004), Pisa, Italy.
11. Serra, X. and Smith, S. (1990). ”Spectral Modeling Synthesis: A Sound Analy-
sis/Synthesis System Based on a Deterministic plus Stochastic Decomposition”,
Computer Music Journal, Vol. 14, No. 4.
12. Stamatatos, E. and Widmer, G. (2005). Automatic Identification of Music Per-
formers with Learning Ensembles. Artificial Intelligence 165(1), 37-56.
