 
 
 
 
 
Boosting in Textual Source Attribution 
 
 
 
 
 
 
CS 290C Final Project 
UCSC Department of Computer Science 
 
 
 
 
 
 
By 
Foaad Khosmood 
Winter 2006 
 
 
Project Description 
 
Source attribution refers to the ability of an automated process to identify the source of a 
piece of text. Source attribution is closely related to authorship attribution, but it differs 
from AA precisely because it does not restrict its attribution target to a single author. 
Since the goal is recognize writing style, groups of authors or multiple subcategories of 
one author could be compared and attributed.[1] 
 
In this project we explore the question of comedy detection in Shakespearian literature. Is 
it possible to write a program that can tell if a given play is a comedy? We build a 
boosting-based binary classifier and apply it to a testing set, given a training set 
consisting of a mix of comedies and non-comedies.  
 
Data set and feature selection 
 
The plays were downloaded in HTML form from the “Complete Works of William 
Shakespeare Online” at MIT Tech website.[2] The site uses the standard classification of 
Shakespearian writings: Comedies, Tragedies, Historical Plays, Sonnets and Poems. For 
this project we downloaded 17 comedies and 10 tragedies which are listed at the MIT 
website. Our binary classifier is really categorizing “comedy” and “non-comedy.” We 
attempted to stay away from controversial categories such as “dark comedies” and other 
points of literary intersection. Figure 1 describes the plays that form our working corpus. 
 
 
Table1 (Shakespearian Plays Used) 
Play (file) 
size 
(KB) 0=comedy 
allswell.html 253959 0 
antonyandcleopatra.html 305501 1 
asyoulikeit.html 231240 0 
comedyoferrors.html 168527 0 
coriolanus.html 315368 1 
cymbeline.html 298437 0 
hamlet.html 338528 1 
julius.html 221794 1 
kinglear.html 297595 1 
loveslabourslost.html 254983 0 
macbeth.html 195747 1 
marryWives.html 248389 0 
measureformeasure.html 245752 0 
merchant.html 218648 0 
midsummer.html 174414 0 
muchado.html 236971 0 
othello.html 304791 1 
pericles.html 202895 0 
romeoandjuliet.html 261489 1 
taming.html 234416 0 
tempest.html 187795 0 
timon.html 212944 1 
titus.html 214056 1 
troilus.html 302393 0 
twelfth.html 225447 0 
twogentlemen.html 200495 0 
winterstale.html 264370 0 
 
 
Lexical tokens are the most commonly used and easily accessible features in text 
categorization problems.[4] Words are certainly not the only candidate for features. 
Sentence structures, punctuation distributions and other measurements have been the 
subject of much research in this area. But rarely are they utilized without also taking 
advantage of word occurrences and frequencies in classification. The 1998 Schapire 
paper that we used as guide also experimented with multi-word combinations. 
 
We rely on word occurrence as features. To begin with, we need a common base of 
words by which we can compare all the plays. We derive this base by examining the set 
of unique words that exist in the entire corpus. We calculate the frequency of every 
unique, non-trivial word in both of the classes (Comedy/Non-comedy.) Then we rate each 
word by how common it is in other plays. Using these two measures, we rank all the 
unique words and select the top 2500, an arbitrary number striking a balance between 
performance and feasibility. Thus each play is reduced to a vector of 2500 Boolean 
values each denoting presence of one word. 
 
For each run, we sample the 27 plays we have in our pool. We randomly select a training 
set of 7 non-comedies and 7 comedies, with 3 plays from each category serving as test 
cases. Thus all our statistics, as to word frequencies and commonalities are re-derived 
every time from the new training set.  
 
Preprocessing 
 
Preprocessing consisted of several phases as described below. 
1. HTML tag removal: The data was in HTML. Each play could be downloaded as 
a single web page. We first used the tags to remove miscellaneous stage 
directions and the play titles. Other non-dialogue material such as passage 
descriptions, setting commentary, as well as act and scene titles were left in. We 
felt that these texts are part of the author’s expression and could contribute to 
classification. Next used a self developed GNU flex based parser to remove all 
HTML tags and HTML verbiage used inside of tags.  
2. Trivial words removal: We used an online source for the top 300 most frequently 
used English words [5] and removed them from the set of words representing 
each play. The trivial words include all grammatical and conjugative operators, 
as well as a number of frequent nouns verbs. Stemming was not employed thus, 
multiple forms of each word are considered separate tokens.  
3. Removing multiple instances: At this point, we constructed a master file of only 
unique words that we had extracted along with the frequency of their occurrence 
within their class. We then discarded the extra occurrences of these words.  
4. Using the process described above, we ranked all the words universally based on 
commonality and frequency in that order. For example, if word X was present in 
10 of the 15 plays, and Y was present in 8 of the 15, we ranked X ahead of Y. We 
used overall frequency of words to break ties which were frequent. 
 
At this point, we chose the top 2500 ranked words as the official feature set. The same 
2500 words are used to encode test documents. None of the test documents were 
obviously examined when deriving the base words. 
 
Boosting Algorithm 
 
Boosting is a powerful ensemble meta-classifier technique with considerable success 
recent years. The algorithm depends on the existence of a large number of “weak 
learners,” what Schapire calls “rules of thumb.” The technique then combines the power 
of all the weak learners to produce a powerful classifier. Each weak learner is weighed 
according to its performance during the training phase. For the purposes of this project, 
we use a generalized boosting algorithm described by Schapire and Singer in 1998 
(Figure 2). 
 
We chose the generalized AdaBoost because the literature research revealed some 
specific instances of binary text classification, notably in the Schapire and Singer paper, 
but also in Freund’s 2000 paper, “Introduction to Boosting Classification.” Schapire and 
Singer have a tool called “BoosTexer,” whose object code is available from AT&T. This 
is a software tool based originally on AdaBoost which does end-to-end binary 
classification of text. We had no access to the source code of this tool, but we have 
attempted to mimic it by writing our own end-to-end classifier in this project. 
 
Figure 1, Generalized AdaBoost from Schapire ‘98 
 
 
AdaBoost, accepts a set of labeled documents. A procedure called weakLearn is called in 
order to produce the best hypothesis possible given the current state of knowledge as 
represented in the D distribution over the documents. 
 
During the training itself, distribution D is maintained and updated over the example sets 
of the training data, in this case, a vector of 2500 words for each play. This distribution is 
updated according to the performance of each weak learner. In general, a wrongly 
classified exampled will increase in weight while a correctly classified one decreases in 
weight. In the next iteration, the new error rate reflects this new distribution, and thus if 
what was misclassified before is misclassified again, the error rate increases, making that 
particular h(x) contribute less the final classifier solution.  
 
A diagram of dependencies illustrates the internal functioning of the AdaBoost algorithm. 
In the diagram, a circle represents a variable or vector, and arrows represent data flow in 
form of inputs into other variables that are necessary for updates. A cycle is formed 
whereby the system tries to minimize error as each iteration is performed. 
 
Figure 2, AdaBoost update diagram 
 
 
Weak Classifiers 
 
Since a large number weak classifiers are required, we turn to the presence of word 
features themselves as hypotheses. That is, a hypothesis, h(x) is associated with one 
feature word and could be a simple function that checks weather or not the x vector 
contains that word. The other requirement is that the weak classifier be better than 50% 
correct.  
 
In order to accomplish this, we need a set of hypotheses that can make a -1 or 1 
attribution. The words associated with these hypotheses must have strong likelihood of 
indicating either comedy or non-comedy. If a word is very strongly associated with 
“comedy,” for instance, than it could label every example that contains it with 1, and 
every other example with -1. 
 
In the original inception of this paper, we had externally classified a smaller subset of the 
words accordance to the ir likelihood of occurrence within the general body of Comedy or 
non-Comedy, drawing upon training data only to make this determination. In the current 
version, we obviate the need to for this pre-classification. The weakLearn procedure 
essentially subsumes this process. Any word, regardless of prior affiliation will be tested 
with the current distribution D, and will be evaluated by comparing its error value to 
other words. The weak classifier thus is selected on the basis of merit.  
 
However, in order to maximize the utility of each word, it would help if the original set 
of 2500, could contain words that are likely to be present in many of the plays. We thus, 
used 2500 words that are most common within each training set. 
 
Termination 
 
While Schapire and Singer did not specifically address “termination” in their algorithm in 
the 1998 paper, they did discuss “choosing the number of rounds.” As with their analysis, 
we found that using AdaBoost on this problem with the broader set of words, the training 
error will fairly quickly reach 0. We implemented this criterion as our termination policy. 
Thus, whenever the training error became 0, as judged by the c-programming “double” 
data type precision, we set final T= current t, and thus broke out of the main loop and 
ended training.  
 
The termination policy has pros and cons. In almost every case, the termination policy 
yields a smaller set of words that are actually necessary to classify all the training data. 
Using only these features will be more efficient if they can accomplish the same 
classification (error=0) as a largest set of words. However, termination also yields over 
fitting. As a particularly small set of words may be all that is needed to classify the 
training corpora, but may not be enough for that testing data. This is one possible 
explanation for our failure to classify test data consistently at a high rating success. Other 
reasons are discussed under conclusions.  
 
Tools and Platform 
 
Most of the preprocessing activities were done using programs developed with GNU-flex 
parser and lexer with the help of BASH/Unix scripting on Linux. The main boosting 
algorithm was implemented with a C++ program which outputs the alpha vector as 
weights for the f(x). Testing classifier was done with GNU Bash and GNU awk. Some of 
the key C++ code is attached in Appendix II: Code. The rest of the code is available upon 
request. 
 
Results and Verification 
 
The following results were obtained after about 100 separate runs for each category with 
random permutations of training test data. At all times, 7 documents from each class were 
used in training, 3 for testing. The top part of the chart below notes a Shakespearian 
comedy / non-comedy classification. The bottom part, is a totally arbitrary binary 
classification over the same plays. A graph is also provided based on the same data. The 
key number are 66.8% as the average performance of the AdaBoost algorithm when 
applied to a Comedy/Non-comedy classification; And 51.8% as the average performance 
of an arbitrary classification. 
 
The data clearly indicates a stronger similarity in a cluster of data when genre is 
considered over an arbitrarily drawn set of categories. 
 
Table 2, Results 
num 
correct/6 % # runs total 
2 0.333333 5 1.666665 
3 0.5 28 14 
4 0.66667 36 24.00012 
5 0.833333 23 19.16666 
6 1 8 8 
Total Comedy/NC 100 66.83344 
        
       
2 0.333333 38 12.66665 
3 0.5 34 17 
4 0.66667 22 14.66674 
5 0.833333 9 7.499997 
6 1 0 0 
Total Arbitrary Class 103 51.83339 
   
 
 
Figure 3, Results 
AdaBoost Binary Text Classification Performance 
on Shakespear Data Set
0
5
10
15
20
25
30
2 3 4 5 6
Number correctly lassified (out of 
6)
%
 o
f 
ti
m
e
Total Comedy/NC
Total Arbitrary Class
 
 
Conclusions 
 
Using boosting for a text classification problem, even one as difficult as classifying 
comedy, shows definite promise. One of the challenging aspects of boosting algorithms is 
their dependence on an infinite or “black box” supply of weak learners, guaranteed to 
perform at least slightly better than a 50/50 coin toss. These classifiers may not be readily 
available. Constructing them may require more resources than it’s worth. Fortunately for 
text classification, this problem can be solved by using word features (or any other 
features) where the feature can be guaranteed to classify at least one single example, thus 
providing a slight edge over the 50% mark. Intuitively, that should not be enough for a 
robust and powerful ensemble classifier. Indeed there are many words that can classify 
more than one example successfully. These words seem more valuable and their 
corresponding hypothesis will have less error as a result. The word “coffer” for example 
appears in 4 separate comedies and in no tragedies. Such a phenomenon suggests that 
there’s a higher probability of this word occurring in comedy samples in the tests.  
 
Although 100% correct classified instances were achieved, they were rather rare. The 
average in our 100 trial set was a little over 2/3. A variety of explanations can be offered 
for investigation in the future. One is that our termination policy may have contributed to 
over- fitting, that is, weak hypothesis that did not get used due to the fact that our training 
error was already 0, could have been instrumental in classifying unexamined test 
documents. Another explanation is the nature of the data itself. Source attribution is 
usually done between two authors or sets of writers. In this case, everything was from the 
same source, thus it is understandable why they are difficult to classify. Many of the 
same words are used in comedies as in tragedies and the sophistication required to detect 
a piece of comedy is probably beyond simple word frequencies.   
 
Another avenue for experiments in the future is testing example size. That is, how small 
can a piece of text get and still be able to be classified correctly by the system? It is 
certainly preferable and less resource intensive to evaluate not an entire play, but perhaps 
an act or a scene. Other methods in source attribution have suggested that smaller texts 
are just as classifiable [1]. 
 
As for the problem set itself, of course there has not been enough testing to verify 
anything universally. But if we consider that the boosting algorithm will be about 66% 
accurate on the average, this tells us that there is in fact a strong correlation between 
Shakespeare’s vocabulary usage and the type of plays he was writing. As someone else 
suggested at the presentation, the algorithm could be picking up a deeper grouping hidden 
to us at the moment (for example, time frame, late Shakespeare vs. early Shakespeare), 
but that theory doesn’t seem plausible with the results we have obtained. 
 
Much more than sheer presence of words is normally considered by human scholars of 
Shakespeare when critically examining his plays. Sentence structures, character analysis, 
subject matter and approach are also integral in forming a Shakespearian comedy. It is 
possible for example to write a Shakespearian-style tragedy and a comedy using identical 
word and this would easily fool our system. Our tool did not take any of these harder-to-
define rules into consideration and thus the only claim we can possibly make is about the 
correlation of vocabulary usage and Shakespearian genre.  
 
 
 
References 
 
 [1] Khosmood, Foaad and Kurfess, Franz, “Automatic Source Attribution of 
  Text: A Neural Networks Approach,” In IJCNN-05, Montreal, Canada, June 2005. 
 
[2] Complete Works of William Shakespeare Online  
http://www-tech.mit.edu/Shakespeare/ 
 
[3] Shakespeare text statistics: OpenShakespeare.org 
 http://www.opensourceshakespeare.org/stats/ 
 
[4] N. Fakotakis E. Stamatatos and G. Kokkinakis. 2000. Automatic Text Categorization  
in Terms of Genre and Author. Computational Linguistics, 26(4), 471-495. 
 
[5] 300 most common words in the English language 
http://www.cssmi.qc.ca/carrefour_educatif/ressources_pedagogiques/robertt/300_
Most_Common_Words.htm 
 
[6] Freund and Schapire, A short Introduction to Boosting, Journal of Japanese Society 
            forArtificial Intelligence, September 1999. 
 
[7] Warmuth and Liao, Totally Corrective Boosting Algorithms that Maximize the  
Margin, ICML 06? 
[8] Schapire, Singer, Singhal, Boosting and Rocchio Applied to Text 
Filtering, Proceedings of SIGIR-98, 21st ACM International Conference on 
Research and Development in Information Retrieval, 1998. 
 
[9] Freund and Schapire, Experiments with New Boosting Algorithm, Proceedings of the 
            Thirteenth International Conference on Machine Learning, 1996. 
 
 
 
 
Appendix I: Output  
 
Sample Program Output (annotated) 
GROUP 1: 0,3,9,12,13,14,15, //Categories and documents that  belong to each (random or Comedy/non-comdey) 
GROUP 2: 1,6,7,16,18,21,22,  
GROUP TEST: 2(-1),4(1),5(-1),8(1),10(1),11(-1), // Set of testing documents and labels that belong to each 
sizeD=14 
 
[REACHED! 14]    
[horn: a[0]=1.282475]  //Top words used for categorization, ranked by minimizing error 
[officer: a[1]=2.030221]   
[hose: a[2]=2.965510] 
[shrewd: a[3]=5.600490] 
[blunt: a[4]=5.793436] 
[jew: a[5]=9.371020] 
[able: a[6]=10.223621] 
[troth: a[7]=8.645583] 
[access: a[8]=17.241394] 
[gift: a[9]=34.482788] 
[accord: a[10]=68.513298] 
[according: a[11]=68.513298] 
[ability: a[12]=127.633179] 
[ace: a[13]=225.858887] 
[achilles: a[14]=0.000000] 
 
]]DOC #2: (sum=-441.471742)  // This is the result of running each test doc through the final weighted classifier 
]]DOC #4: (sum=390.858680) 
]]DOC #5: (sum=-209.633001) 
]]DOC #8: (sum=-355.314734) 
]]DOC #10: (sum=-350.128913) 
]]DOC #11: (sum=-496.401773) 
TOTAL SCORE: 0.666667  // hit-rate, or performance measure for this set of test documents. 
 
----------------------------------------------------------------------------- 
Example of a script-produced input file for the above program, describes the presence of each word within each document 
 
[root@localhost shak]# more wordHits.txt 
abate 1 1 1 1 1 0 0 1 1 0 1 1 1 0 0 1 1 1 0 0 1 1 0 1 1 1 1 
abhorr 1 1 1 1 0 0 1 1 0 1 1 1 0 1 1 1 1 1 1 1 0 0 1 0 1 1 0 
ability 0 1 1 1 1 1 0 1 1 1 1 1 0 1 1 0 0 1 1 1 1 1 1 0 0 1 0 
able 0 0 0 0 0 1 0 1 0 1 1 0 0 0 0 1 1 0 0 0 1 0 0 0 1 0 0 
above 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 
accept 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 0 1 0 1 0 0 0 1 1 0 
access 1 1 0 1 0 1 0 1 1 1 0 1 0 1 1 1 0 0 0 0 1 1 1 1 0 0 0 
accident 1 1 1 1 1 0 0 1 1 1 1 1 0 0 1 0 0 1 0 1 0 1 1 0 0 1 0 
accord 1 1 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 0 
according 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 1 0 0 1 1 1 1 0 0 0 
account 1 1 1 1 0 0 0 1 0 1 0 1 1 0 1 0 0 0 0 0 1 0 0 1 1 0 0 
ace 0 1 1 1 1 0 1 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 
aches 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 0 0 1 1 1 1 1 
achieve 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 0 1 1 
achilles 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 
aching 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1 
acquaint 0 1 0 1 1 1 0 1 0 1 0 1 0 1 1 0 1 1 0 1 1 1 0 1 1 1 0 
acre 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 0 
act 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
 
Appendix II: Code 
 
 
Classify.sh (BASH script to perform classification tasks for test subjects) 
[root@localhost shak_boosting2]# more classify.sh 
#!/bin/bash 
 
for file in `ls samples/output/*ratio.txt`;do 
 
let totalweight=0; 
 echo -n $file; 
 
 for line in `cat $1 | tr ' ' '_'`;do 
  wrd=`echo $line | awk -F "_" '{print $2}'`; 
  wei=`echo $line | awk -F "_" '{print $1}'`; 
 
  c=`grep " $wrd$" $file | grep -c -v "^0 "`; 
 
  if [ $c -gt 0 ];then 
        totalweight=`echo $totalweight $wei | awk '{print $1+$2}'`; 
  fi; 
  done; 
 
  echo ": "$totalweight; 
 
done; 
 
 
 
 
  
Main Boosting Program 
 
//AdaBoost classifier and random class groupings generator 
//Foaad Khosmood, UCSC, Winter 2006 
 
#include <stdio.h> 
#include <stdlib.h> 
#include <math.h> 
#include <string.h> 
#include <time.h> 
 
#define FEATURES 3000 
#define NUM_DOCS 27 
#define NUM_TRIALS 30 
#define NUM_TESTS 3     //number of test cases per class 
 
int main(int argc,char *argv[]) 
{ 
 
        int lables[NUM_DOCS];           //original labels 
        int docs[FEATURES+1][NUM_DOCS]; //docs[words][number]= presence(1) 
        int grouping[NUM_DOCS][2];      //test/training [0|1], class[1,-1] 
 
        char name[FEATURES][20]; 
        char tempName[20]; 
        double a[FEATURES+1]; 
        double D[FEATURES+1][NUM_DOCS]; 
        double e[FEATURES+1]; 
        double tempF,Z; 
        double sum; 
        double score; 
        int minE; 
        int maxE; 
        float negE,posE; 
        int sizeD; 
        char junk; 
        FILE *diffFile; 
        int limit=0; 
        int test=0; 
        int trial=0; 
        int f=0; 
        int d=0; 
        int n=0; 
        int w,t,k,T; 
 
 
lables[n++]=1;  //allswell      0 
lables[n++]=1;  //antonyandcleopatra 
lables[n++]=0;  //asyoulikeit 
lables[n++]=0;  //comedyoferrors 
lables[n++]=1;  //coriolanus 
lables[n++]=1;  //cymbeline     5 
lables[n++]=0;  //hamlet 
lables[n++]=0;  //julius 
lables[n++]=0;  //kinglear 
lables[n++]=0;  //loveslabourslost 
lables[n++]=1;  //macbeth       10 
lables[n++]=0;  //marryWives 
lables[n++]=0;  //measureformeasure 
lables[n++]=0;  //merchant 
lables[n++]=0;  //midsummer 
lables[n++]=0;  //muchado       15 
lables[n++]=0;  //othello 
lables[n++]=1;  //pericles 
lables[n++]=1;  //romeoandjuliet 
lables[n++]=0;  //taming 
lables[n++]=1;  //tempest       20 
lables[n++]=1;  //timon 
lables[n++]=1;  //titus 
lables[n++]=0;  //troilus 
lables[n++]=1;  //twelfth 
lables[n++]=0;  //twogentlemen  25 
lables[n]=0;    //winterstale 
 
//printf("main starts\n"); 
        if ((diffFile = fopen(argv[1],"r"))==NULL) 
        { 
                printf("Cannot Open file\n"); 
                exit(1); 
        } 
 
//printf("here\n"); 
 
        for (f=0;f<FEATURES;f++) 
        { 
//printf("start loop\n"); 
 
                fscanf(diffFile,"%s",&name[f]); 
//printf("done\n"); 
printf("\n%s:",name[f]); 
                for (d=0;d<NUM_DOCS;d++) 
                { 
                        docs[f][d]=0; 
                        fscanf(diffFile,"%i",&docs[f][d]); 
                        printf("%i,",docs[f][d]); 
        //              doc2[f][d]=docs[f][d]; 
                } 
 
 
        } 
        fclose(diffFile); 
printf("***************\n"); 
        for (f=0;f<FEATURES;f++) 
        { 
                printf("\n%i-%s:",f,name[f]); 
//              for (n=1;n<=(20-strlen(name[f]));n++) 
//                      printf(" "); 
                for (d=0;d<NUM_DOCS;d++) 
                { 
//              if (doc2[f][d] > 1) 
                        printf("%i,",docs[f][d]); 
                } 
        } 
 
 
        // construct the classes 
        // First, the comedy/tragedy classs 
        for(n=0;n<NUM_DOCS;n++) 
        { 
                grouping[n][0]=lables[n]; 
                grouping[n][1]=0; 
printf("(%i:%i)",n,grouping[n][0]); 
        } 
 
printf("\npassed grouping1\nCurrent Set:"); 
 
 
 
        limit=10; 
        for (n=0;n<NUM_DOCS;n++) 
        { 
                if(limit && !grouping[n][0]) 
                { 
                        limit--; 
                        grouping[n][0]=-1; 
                } 
                printf("(%i:%i)",n,grouping[n][0]); 
        } 
        printf("\n"); 
 
        for(trial=0;trial<=NUM_TRIALS;trial++) 
        { 
                for(n=0;n<NUM_DOCS;n++) 
                        grouping[n][1]=0; 
 
 
                // construct test/training split 
                test=NUM_DOCS/3;        //total test cases 
                test=NUM_TESTS;         //test cases for each class 
 
                srand((unsigned)time(NULL)); 
 
                f=0; 
                while(test >=1) 
                { 
                        srand((unsigned)time(NULL)); 
                        n=rand()%(NUM_DOCS-f); 
                        if (!grouping[n][1] && (grouping[n][0]==-1)) 
                        { 
//                              
printf("n=%i,test=%i,n1=%i,n0=%i\n",n,test,grouping[n][1],grouping[n][0]); 
                                grouping[n][1]=1; 
                                test--; 
 
 
                        } 
                } 
 
                test=NUM_TESTS; 
                while(test >=1) 
                { 
                        srand((unsigned)time(NULL)); 
                        n=rand()%(NUM_DOCS); 
                        if (!grouping[n][1] && (grouping[n][0]==1)) 
                        { 
//                              
printf("n=%i,test=%i,n1=%i,n0=%i\n",n,test,grouping[n][1],grouping[n][0]); 
                                grouping[n][1]=1; 
                                test--; 
                        } 
                } 
 
// initialize D vector 
 
                sizeD=0; 
                for(n=0;n<NUM_DOCS;n++) 
                        if(!grouping[n][1]) 
                                if(grouping[n][0]==-1 || grouping[n][0]==1) 
                                        sizeD++; 
// initialize D 
                for(n=0;n<NUM_DOCS;n++) 
                { 
                        D[0][n]=1.0/sizeD; 
//                      printf("\tD(%i)=%f",n,D[0][n]); 
                } 
 
                T=FEATURES; 
                t=-1; 
 
                while(t++<T) 
                { 
                        minE=t; 
                        for(w=t;w<FEATURES;w++) 
                        { 
                                e[w]=0.0; 
                                for(n=0;n<NUM_DOCS;n++) 
                                { 
                                        if(!grouping[n][1] && (grouping[n][0])) 
                                        { 
                                                if(lables[n]!=docs[w][n]) 
                                                           e[w]+=D[t][n]; 
                                        } 
                                } 
                                if (e[w] > 0.5) // reverse the hypothesis 
                                { 
                                        //printf("**switch**"); 
                                        for(n=0;n<NUM_DOCS;n++) 
                                                if(docs[w][n]) 
                                                        docs[w][n]=0; 
                                                else 
                                                        docs[w][n]=1; 
                                        e[w]=0.0; 
                                        for(n=0;n<NUM_DOCS;n++) // recalculate the error; 
                                        { 
                                                if(!grouping[n][1] && (grouping[n][0])) 
                                                { 
                                                        if(lables[n]!=docs[w][n]) 
                                                                e[w]+=D[t][n]; 
                                                } 
                                        } 
                                } 
 
//find new lowest 
                                if(e[w] < e[minE]) 
                                { 
                                        minE=w; 
                                } 
 
%s(%i)=%f",name[minE],e[minE],name[w],w,e[w]); 
                        } // weakLearn procedure 
 
                        if(minE!=t) // current t becomes the lowest error t 
                        { 
                                strcpy(tempName,name[t]); 
                                strcpy(name[t],name[minE]); 
                                strcpy(name[minE],tempName); 
                                e[t]=e[minE]; 
                        } 
 
                        if(e[t]<=0.0) 
                        { 
                                T=t; 
                                printf("\n %i",t); 
                                for (w=0;w<=T;w++) 
                                { 
                        //              printf("[%s: a[%i]=%f]\n",name[w],w,a[w]); 
                                } 
                                break; 
                        } 
 
                        a[t]=float(.5*log((1.0-e[t])/e[t])); 
 
                        //updating D vector 
                        Z=0.0; 
                        for(n=0;n<NUM_DOCS;n++) 
                        { 
                                if(!grouping[n][1]) 
                                { 
                                        if(lables[n]>0) 
                                                tempF=1.0; 
                                        else 
                                                tempF=-1.0; 
 
                                        if(docs[t][n]<=0) 
                                                tempF*=-1.0; 
 
                                        D[t+1][n]=D[t][n]*exp(-a[t]*tempF); 
                                        Z+=D[t+1][n]; 
                                } 
                        } 
 
                        tempF=0.0; 
                        for(n=0;n<NUM_DOCS;n++) 
                        { 
                                if(!grouping[n][1]) 
                                { 
                                        D[t+1][n]/=Z; 
                                        tempF+=D[t+1][n]; 
                                } 
                        } 
 
                }// main for loop (t) 
 
                score=0.0; 
                for(n=0;n<NUM_DOCS;n++) //Final Classification (this Trial) 
                { 
                        tempF=0.0; 
                        sum=0.0; 
                        if(grouping[n][1]) 
                        { 
                                for(t=0;t<FEATURES;t++) 
                                { 
                                        if(t==0) 
                        //                      printf("\n]]DOC #%i: ",n); 
                                        if(docs[t][n]>0) 
                                        { 
                                                tempF=-a[t]; 
                                        } 
                                        else 
                                        { 
                                                tempF=a[t]; 
                                        } 
                                        sum+=tempF; 
                                } 
                        //      printf("(sum=%f)",sum); 
                                if((sum*grouping[n][0])>0) 
                                        score+=1.0; 
 
                        } 
                } // final classification for loop 
                printf("%f \n",(score/6.0)); 
 
        } // Trial loop, for multiple sets and splits 
} // main() 
 
 
