 
 
PLEASE SCROLL DOWN FOR ARTICLE
This article was downloaded by: [HEAL-Link Consortium]
On: 28 August 2009
Access details: Access Details: [subscription number 793285000]
Publisher Routledge
Informa Ltd Registered in England and Wales Registered Number: 1072954 Registered office: Mortimer House,
37-41 Mortimer Street, London W1T 3JH, UK
Journal of Quantitative Linguistics
Publication details, including instructions for authors and subscription information:
http://www.informaworld.com/smpp/title~content=t716100702
Neural Computation in Authorship Attribution: The Case of Selected Tamil
Articles*
M. Bagavandas a; Abdul Hameed b; G. Manimannan c
a SRM University, Tamil Nadu, India b Department of Mathematics, C.A. Hakeem College, India c Department
of Statistics, Madras Christian College, India
Online Publication Date: 01 May 2009
To cite this Article Bagavandas, M., Hameed, Abdul and Manimannan, G.(2009)'Neural Computation in Authorship Attribution: The
Case of Selected Tamil Articles*',Journal of Quantitative Linguistics,16:2,115 — 131
To link to this Article: DOI: 10.1080/09296170902734156
URL: http://dx.doi.org/10.1080/09296170902734156
Full terms and conditions of use: http://www.informaworld.com/terms-and-conditions-of-access.pdf
This article may be used for research, teaching and private study purposes. Any substantial or
systematic reproduction, re-distribution, re-selling, loan or sub-licensing, systematic supply or
distribution in any form to anyone is expressly forbidden.
The publisher does not give any warranty express or implied or make any representation that the contents
will be complete or accurate or up to date. The accuracy of any instructions, formulae and drug doses
should be independently verified with primary sources. The publisher shall not be liable for any loss,
actions, claims, proceedings, demand or costs or damages whatsoever or howsoever caused arising directly
or indirectly in connection with or arising out of the use of this material.
Neural Computation in Authorship Attribution: The
Case of Selected Tamil Articles*
M. Bagavandas1, Abdul Hameed2 and G. Manimannan3
1SRM University, Tamil Nadu, India; 2Department of Mathematics, C.A. Hakeem
College, Melvisharam, India; 3Department of Statistics, Madras Christian College,
Tambaram, India
ABSTRACT
Neural networks regard author attribution as a problem of pattern recognition and the
proven results of their applications make them promising techniques for the future.
Several neural networks are being applied for authorship determination. Learning vector
quantization (LVQ) is a neural network technique that develops a codebook of
quantization vectors and makes use of these vectors to encode any input vector. In this
article an attempt is made to attribute authorship to disputed articles using LVQ and
verify them with the results obtained by traditional canonical discriminant analysis. This
study demonstrates that statistical methods of attributing authorship can be paired
effectively with neural networks to produce a powerful classification tool. Comparisons
are made using means of 24 function words identified from the 32 articles written in the
Tamil language by three contemporary scholars of great repute to determine the
authorship of 23 unattributed articles pertaining to the same period. This study
establishes the fact that LVQ is a powerful technique for computational stylistics.
1. INTRODUCTION
The ever-growing computer power and the availability of computer-
readable literary texts have been increasing the uses of neural network
techniques in solving outstanding authorship disputes. Neural network
authorship studies not only complement the traditional statistical literary
studies but also provide an alternative method for investigating the works
of doubtful provenance (Holmes, 1998). Neural network computations
*Address correspondence to: M. Bagavandas, School of Public Health, SRM University,
Tamil Nadu-603203, India. E-mail: mbdas49@yahoo.com
Journal of Quantitative Linguistics
2009, Volume 16, Number 2, pp. 115–131
DOI: 10.1080/09296170902734156
0929-6174/09/16020115  2009 Taylor & Francis
D
o
w
n
l
o
a
d
e
d
 
B
y
:
 
[
H
E
A
L
-
L
i
n
k
 
C
o
n
s
o
r
t
i
u
m
]
 
A
t
:
 
1
8
:
4
1
 
2
8
 
A
u
g
u
s
t
 
2
0
0
9
have several advantages over traditional statistical methods in solving
disputed problems. Neural networks are excellent classifiers and require
fewer input variables than standard statistical techniques in making such
classification (Holmes, 1998). They have the ability to recognize the
underlying organization of data, which is of vital importance for any
pattern recognition problem. They have a high level of fault tolerance
and can deal with noise in the data while capturing the nuizances of high-
level interactions between input measures (Tweedie, Singh & Holmes,
1996a).
Several comparison studies have been made on neural network and
statistical classifiers. Many of these comparative studies have treated
these two techniques as alternatives and this approach has led, on the one
hand, to a fruitful analysis of existing neural networks, and on the other,
it has enriched the currently available statistical techniques with newly
traced viewpoints, and has sometimes pioneered a useful synthesis of
these two fields (Michie, Spiegelhalter & Taylor, 1994). In this study, an
attempt is made to compare the performance of neural networks and
statistical techniques with the classification of the disputed articles and
these two techniques have been found matching optimally in the design
of authorship attribution.
Attributing authorship to an article on the basis of statistical analysis
of stylistic features is termed ‘‘non-traditional’’ authorship attribution in
linguistic research. In the modern era authorship attribution studies
make use of the computer, statistics, and stylistic features and these
methods assume that an author has a unique and identifiable style which
can be identified in all his/her works. The computer has now taken an
important place in these literary studies and is the main reason why non-
traditional authorship studies have scientific flavour. Holmes (1998)
provided a comprehensive review on authorship attribution studies. In
this article, an attempt is made to compare the performance of neural
network and statistical techniques with the classification of the disputed
articles and these two techniques have been found to match optimally.
2. DATA AND METHODS
The present study deals with the literary works of three contemporary
Tamil scholars: namely, Mahakavi Bharathiar (MB), Subramanya Iyer (SI)
116 M. BAGAVANDAS ET AL.
D
o
w
n
l
o
a
d
e
d
 
B
y
:
 
[
H
E
A
L
-
L
i
n
k
 
C
o
n
s
o
r
t
i
u
m
]
 
A
t
:
 
1
8
:
4
1
 
2
8
 
A
u
g
u
s
t
 
2
0
0
9
and T.V. Kalyanasundaram (TVK). During the pre-independence
period, these three scholars had written a number of articles on India’s
freedom movement in a magazine called India. Initially, all the three
scholars had written their articles by attributing their names in this
magazine. Because of the opposition of the then British regime, the
articles written on the same topic then appeared in the same magazine
but without the authors’ names. Ilasai Maniyan (1975) has compiled all
these attributed and unattributed articles and brought out a book called
Bharathi’s Dharizanam. In this book, this scholar also made a statement
that the writing styles of these unattributed articles indicate that
Mahakavi Bharathiar may plausibly be the author of these articles. This
pioneering study makes an attempt to verify his statement using neural
networks and statistical techniques. For attribution purposes the
articles should belong to the same genre and time. Hence, all the
attributed and unattributed articles written on India’s freedom move-
ment published in the year 1906 in the same magazine are considered
for this quantitative study. Accordingly, there are 19 articles of MB,
seven of SI, and six of TVK and 23 unattributed articles. Twenty-four
function words are considered as stylistic features for this study (see
Table 2 later).
For a comparative analysis the frequency counts of the stylistic
features must be normalized to the text length in an article. In this study
since each sentence is considered as a sample, to normalize the stylistic
features, the raw frequency counts of each stylistic feature are divided by
the number of words in each sentence and then multiplied by 100 to
express it as a percentage. The frequency of occurrence of each of the 24
function words is identified from each sentence and they are expressed as
percentages.
If we represent each article by mean values of p stylistic features and if
we have n such articles then we have a data matrix of size n6 p for each
author. Thus the entire information is converted to a data matrix and
these data matrices form the basis for this quantitative study. The data
matrix of MB is of size 196 24, of SI is of size 76 24 and of TVK is of
size 66 24. The main objectives of this study are to establish authorial
consistency and to identify the authorship of 23 unattributed articles
by analysing these data matrices using the first version of the learning
vector quantization neural network technique and statistical canonical
discriminant analysis.
NEURAL COMPUTATION IN AUTHORSHIP ATTRIBUTION 117
D
o
w
n
l
o
a
d
e
d
 
B
y
:
 
[
H
E
A
L
-
L
i
n
k
 
C
o
n
s
o
r
t
i
u
m
]
 
A
t
:
 
1
8
:
4
1
 
2
8
 
A
u
g
u
s
t
 
2
0
0
9
3. NEURAL NETWORK APPROACH
Neural network techniques are used to attribute authorship to disputed
texts. One such network was built with the links having random
strengths. The network was presented with training texts of known
authorship. Any time the network guessed incorrectly, it adjusted the
strengths of its links until the network could properly identify known
texts. Once the training period is complete, the network can properly
determine authorship of texts by authors that it has been trained on
previously. Tweedie, Singh, and Holmes (1996b) have provided a
comprehensive review of the applications of neural network techniques
in computational linguistics
A neural network, mainly based on ideas drawn from neuro-
physiology, is made up of ‘‘connections’’ and ‘‘processing elements’’ or
‘‘nodes’’. A neural network needs to be trained on patterns known as a
‘‘training set’’ so that it can learn to differentiate between, for example,
two candidate authors before it attempts to classify unattributed articles.
The input on which to train the network may be the frequencies of
function words or ratios of combinations of word occurrences.
Merriam and Matthews (1993, 1994) are the pioneers who introduced
neural networks into author attribution studies. They employed a
neural network method known as a ‘‘multi-layer perception’’, which
discriminated articles by identifying a boundary made up of linear
segments. Lowe and Matthews (1995) have used an architecture known
as ‘‘radial basis function’’ to re-examine the collaboration between
Shakespeare and Fletcher. Tweedie et al. (1996b) have obtained results
consistent with those of Mosteller and Wallace (1964) by using eleven
marker words of the Federalist papers as input to a neural network.
This paper examines the use of the Cascade-Correlation algorithm for
the construction of minimal networks. Waugh, Adams and Tweedie
(2000), have found that a number of problems in computational
stylistics with a large number of variables, but a limited number of
training examples, may be solved successfully without resorting to large
networks.
3.1 Learning Vector Quantization
There are many different types of self-organizing neural networks.
Learning vector quantization technique (LVQ) is identified as one of
the efficient self-organizing neural networks classifiers. This supervised
118 M. BAGAVANDAS ET AL.
D
o
w
n
l
o
a
d
e
d
 
B
y
:
 
[
H
E
A
L
-
L
i
n
k
 
C
o
n
s
o
r
t
i
u
m
]
 
A
t
:
 
1
8
:
4
1
 
2
8
 
A
u
g
u
s
t
 
2
0
0
9
learning technique classifies input vectors based on vector quantization.
Kohonen (1986) has developed all the three versions of this technique. This
study proposes to use the first version (LVQ1) of this technique. The LVQ1
training process proceeds with an input vector being randomly selected
from the ‘‘labelled’’ training set. Given an input vector ui to the network,
the output neuron in LVQ 1 is deemed to be a winner according to
min d ui;wj
 
¼ min ui  wj
 
8j 8j
ð1Þ
3.2 Algorithm
Let {ui}, for i¼ 1, 2, . . . , N be the set of input vectors, and the network
synaptic weight vectors (Voronoi vector) are denoted as {wi} for j¼ 1,
2, . . . , m. We also let Cwj be the class (or category) that is associated with
the weight vector wj and Cui is the class label of the input vector ui to the
network. The weight vector wj is adjusted in the following manner:
(i) If the class associated with the weight vector and the class label of
the input are the same, that is, Cwj¼Cui, then
wj kþ 1ð Þ ¼ wj kð Þ þ mðkÞ ui  wj kð Þ
 
ð2Þ
where 05 m(k)51 (the learning rate parameter)
(ii) But if Cwj 6¼ Cuj, then
wj kþ 1ð Þ ¼ wj kð Þ  mðkÞ ui  wj kð Þ
 
ð3Þ
and the other weight vectors are not adapted.
Therefore, the update rule for modifying a weight vector in (2) is the
standard one if the class is correct. In other words, according to
the learning rule in (2), the weight vector wj is moved in the direction of
the input ui if the class labels of the input vector and the weight vector
agree. However, if the class is not correct, the weight vector is moved in
the opposite direction away from the input vector according to the
above-mentioned Equation (3). The learning rate parameter m(k) is a
function of k and is monotonically decreased in accordance with the
discrete-time index k (e.g. linearly decreased in time, starting at 0.01 or
0.02. However, many times 0.1 is used as the initial value).
NEURAL COMPUTATION IN AUTHORSHIP ATTRIBUTION 119
D
o
w
n
l
o
a
d
e
d
 
B
y
:
 
[
H
E
A
L
-
L
i
n
k
 
C
o
n
s
o
r
t
i
u
m
]
 
A
t
:
 
1
8
:
4
1
 
2
8
 
A
u
g
u
s
t
 
2
0
0
9
The convergence properties of LVQ have been studied by Baras and
LaVigna (1990), and their approach is based on the stochastic
approximation theory. The weights can be initialized using several
methods. In this article the first m (total number of classes) vectors from
the set of training vectors are used to initialize the weight vectors, that is,
wj(0) for j¼ 1, 2, . . . , m. Another approach is to randomly initialize the
weight vectors (within the dynamic range of the input vectors).
The stopping condition can be based on the total number of desired
training epochs, or on monitoring the convergence of the weight vectors.
Another stopping condition can be based on monitoring the learning
rate parameter directly, and when it is sufficiently small, training can be
terminated. In this article, we establish the stopping condition to be the
total number (predefined) of iterations. The basic LVQ algorithm can
be summarized as follows:
Step 1. Initialize all weight vectors wj(0), initialize the learning rate
parameter m(0), and set k¼ 0.
Step 2. Check the stopping condition. If false, continue; if true, quit.
Step 3. For each training vectors ui, perform steps 4 and 5:
Step 4. Determine the weight vector index (j¼ q) such that min
kui7wj(k)k2 and the weight vector wq(k) that minimizes the
square of the norm.
Step 5. Update the appropriate weight vector wq(k) as follows:
If Cwq¼Cui, then wq(kþ 1)¼wq(k)þ m(k) [ui7wq(k)]
If Cwq 6¼ Cui, then wq(kþ 1)¼wq(k)7m(k)[ui7wq(k)]
Step 6. Set k ! kþ 1, reduce the learning rate parameter, then go to
Step 2.
The learning rate parameter m can be reduced in accordance with k
(discrete-time index) using m(k)¼ m(kþ 1)/(kþ 1) for k4 0.
3.3 Analysis
We have associated the 19 articles of MB to class 1, seven of SI to class 2
and six of VK to class 3, and thereby we have a total of 32 articles
(vectors) associated with three classes. One method to initialize the
codebook vectors w1, w2, and w3 is to use the first three articles, one from
each group of articles of each author. The associated classes for w1, w2,
and w3 are then 1, 2, and 3 respectively. The remaining 29 articles of the
three scholars can be used for training.
120 M. BAGAVANDAS ET AL.
D
o
w
n
l
o
a
d
e
d
 
B
y
:
 
[
H
E
A
L
-
L
i
n
k
 
C
o
n
s
o
r
t
i
u
m
]
 
A
t
:
 
1
8
:
4
1
 
2
8
 
A
u
g
u
s
t
 
2
0
0
9
We initialized the learning rate parameter to u(k¼ 1)¼ u(1)¼ 0.1 and
decrease it by k every training epoch, for example u(2)¼ u(1)/2, u(3)¼ u(2)/
3, etc. According to the algorithm given above, we set k¼ 2, and u(2)¼ u(1)/
2¼ 0.05, then go to step 2 and check the stopping condition. We start the
next training epoch with the first training article among the remaining
twenty-nine articles of the three scholars. TheMATLAB coding is usedwith
the number of training epochs set to 20,000 to find the codebook vectors.
After 20,000 training epochs, the final weights give the following codebook
vectors and these are given in Table 1.
The convergence profile for the elements of the training epoch is shown
in Figures 1 to 4. Finally, if we calculate the minimum distance between
each of the unattributed articles and the computed weight vectors w1, w2,
and w3 of the codebook shown above, it will give the class to which each
unattributed article belongs, and it is computed using the MATLAB
code.
At first, the attributed articles were presented to the LVQ1 neural net-
work technique to verify its efficiency. This neural network should then be
able to identify an article ofMBas article ofMB and so on. The end result of
this intra-analysis is given in Figure 5. The convergence result is given in
Table 2. Thus LVQ1 with MATLAB code has identified correctly all the
attributed articles. All the 23 disputed articles are then presented to the
LVQ1 neural network. This experiment attributes all the 23 unattributed
articles to the Tamil scholar Mahakavi Bharathi and this final result is
given in figure 6. The convergence result is given in Table 3 later.
Table 1. Codebook vectors after 20,000 training epochs.
S. No. MB SI TVK S. No. MB SI TVK
1 8.7361 14.1736 10.8238 13 12.9489 16.5957 16.5527
2 1.4750 9.5798 7.1318 14 0.2667 7.9110 7.0963
3 9.0911 13.3470 14.3877 15 2.4500 7.7434 5.7505
4 2.7344 7.4534 7.0850 16 5.3701 6.8349 4.1750
5 0.3583 5.6531 4.8220 17 5.7489 7.4827 9.1878
6 0.6017 1.7607 6.1187 18 4.7295 5.0091 6.6000
7 2.6955 6.2396 4.9368 19 5.4394 6.0197 7.0635
8 5.4777 6.6415 6.2482 20 0.2489 4.9429 4.9349
9 0.8828 7.4377 4.9511 21 0.2483 7.1509 4.6086
10 0.4939 8.2528 6.9066 22 0.3133 7.4499 6.7399
11 3.9695 8.1289 6.2955 23 0.0178 5.5320 4.5143
12 1.4239 8.5051 4.2802 24 13.2117 14.4504 13.3227
NEURAL COMPUTATION IN AUTHORSHIP ATTRIBUTION 121
D
o
w
n
l
o
a
d
e
d
 
B
y
:
 
[
H
E
A
L
-
L
i
n
k
 
C
o
n
s
o
r
t
i
u
m
]
 
A
t
:
 
1
8
:
4
1
 
2
8
 
A
u
g
u
s
t
 
2
0
0
9
Fig. 1. Convergence profiles for the elements of weight vector w1.
Fig. 2. Convergence profiles for the elements of weight vector w2.
122 M. BAGAVANDAS ET AL.
D
o
w
n
l
o
a
d
e
d
 
B
y
:
 
[
H
E
A
L
-
L
i
n
k
 
C
o
n
s
o
r
t
i
u
m
]
 
A
t
:
 
1
8
:
4
1
 
2
8
 
A
u
g
u
s
t
 
2
0
0
9
Fig. 3. Convergence profiles for the elements of weight vector w3.
Fig. 4. LVQ network after training.
NEURAL COMPUTATION IN AUTHORSHIP ATTRIBUTION 123
D
o
w
n
l
o
a
d
e
d
 
B
y
:
 
[
H
E
A
L
-
L
i
n
k
 
C
o
n
s
o
r
t
i
u
m
]
 
A
t
:
 
1
8
:
4
1
 
2
8
 
A
u
g
u
s
t
 
2
0
0
9
4. STATISTICAL CANONICAL DISCRIMINANT ANALYSIS
This study proposes to re-analyse the same dataset using statistical
canonical discriminant analysis to verify the results obtained through
LVQ1 neural network computation. Canonical discriminant analysis is
a multivariate method developed for testing the significance of two or
more pre-defined groups of objects. The main objectives of this analysis
are:
(1) to determine a set of linear discriminant functions with ordered
discrimination power between groups identified a priori;
(2) to test whether the means of these groups are significantly different;
and
(3) to assign individual objects of unknown origin to the given known
groups.
The main assumptions of this analysis are that there are multiple groups
that can be unambiguously defined in advance and all the individuals of
unknown origin can be assigned to one and only one such group (Klecka,
1980).
Fig. 5. Classification of MB, SI and TVK.
124 M. BAGAVANDAS ET AL.
D
o
w
n
l
o
a
d
e
d
 
B
y
:
 
[
H
E
A
L
-
L
i
n
k
 
C
o
n
s
o
r
t
i
u
m
]
 
A
t
:
 
1
8
:
4
1
 
2
8
 
A
u
g
u
s
t
 
2
0
0
9
In this analysis, the three authors, namely Mahakavi Bharathiar (MB),
Subramaniya Iyer (SI) and T.V. Kalyansundaranar (TVK), are
designated as author 1, author 2 and author 3 respectively. As there
are three authors to be differentiated, we get two canonical discriminant
functions. The first canonical discriminant function accounts for 96% of
the between-authors variance. The second canonical discriminant
Table 2. Canonical discriminant functions of function words.
Function words Meaning in English
Functions
1 2
Um Also 0.284* 70.001
Aakiyaal As 0.252* 0.029
Entraal For 0.222* 0.132
Aavatu1 For 0.201* 70.029
Aaka As 0.195* 0.088
Mikavum Very much 0.174* 0.138
Pola Like 0.159* 0.020
Entru For 0.152* 0.125
Pearil On 0.149* 0.131
Irrunthu From 0.119* 0.044
Kooda Also 0.072* 0.071
Ul Inside 0.048* 0.008
Ai Unmarked 0.027* 70.012
Nodu With 0.024* 0.013
Lall With 0.020* 0.005
Aall Unmarked 0.085 0.235*
Ukku To 0.033 70.141*
Atu My 0.072 0.119*
Ill In 0.002 0.117*
Utan With 0.025 0.092*
Um1 At every 0.047 70.074*
Enum At least 0.054 0.073*
Utaiya Of 0.014 70.053*
Pattri About 0.013 0.017*
Eigenvalue 269.434 12.539
Percentage variance 95.6 4.4
Cumulative percentage 95.6 100.0
Canonical correlation 0.998 0.962
Wilk’s lambda 0.000 0.074
Chi-square 143.598 45.598
Degrees of freedom 48 13
Significance 0.000 0.003
NEURAL COMPUTATION IN AUTHORSHIP ATTRIBUTION 125
D
o
w
n
l
o
a
d
e
d
 
B
y
:
 
[
H
E
A
L
-
L
i
n
k
 
C
o
n
s
o
r
t
i
u
m
]
 
A
t
:
 
1
8
:
4
1
 
2
8
 
A
u
g
u
s
t
 
2
0
0
9
function accounts for the remaining 4% of total between-authors
variance. Each canonical discriminant function is a linear combination
of the twenty-four function words and is orthogonal to the other. The
Fig. 6. Classification of MB, SI, TVK and unattributed articles.
Table 3. Convergence results after training LVQ technique for MB, SI and VK.
Paper Group 1 Group 2 Group 3
MB 1111111111111111111 0000000 000000
SI 0000000000000000000 2222222 000000
VK 0000000000000000000 0000000 333333
Table 4. Convergence results after training LVQ technique for MB, SI, VK and
unattributed articles.
Paper Group 1 Group 2 Group 3
MB 1111111111111111111 0000000 000000
SI 0000000000000000000 2222222 000000
VK 0000000000000000000 0000000 333333
Unattributed article 1111111111111111111111 0000000 000000
126 M. BAGAVANDAS ET AL.
D
o
w
n
l
o
a
d
e
d
 
B
y
:
 
[
H
E
A
L
-
L
i
n
k
 
C
o
n
s
o
r
t
i
u
m
]
 
A
t
:
 
1
8
:
4
1
 
2
8
 
A
u
g
u
s
t
 
2
0
0
9
significant canonical correlation between authors and the first canonical
discriminant function (r¼ 0.998) and authors and the second canonical
discriminant function (r¼ 0.962) indicates that both canonical discrimi-
nant functions can explain the differentiation of the authors (Table 2).
The classification matrix (Table 5) provides the summary of the
classification results of this study. The percentages of cases classified
correctly are often considered as an index of the effectiveness of the
derived discriminant functions. The diagonal elements of this matrix are
the number of cases classified correctly into groups and the non-diagonal
Table 5. Classification results for MB, SI and VK.
Classification Resultsa
Predicted Group Membership
GRO ART 1 2 3 Total
Original
Count 1 19 0 0 19
2 0 7 0 7
3 0 0 6 6
% 1 100.0 .0 .0 100.0
2 .0 100.0 .0 100.0
3 .0 .0 100.0 100.0
a100.0% of original grouped cases correctly classified.
Table 6. Classification results for MB, SI, VK and unattributed articles.
Classification Resultsa
Predicted Group Membership
GRO ART 1 2 3 Total
Original
Count 1 19 0 0 19
2 0 7 0 7
3 0 0 6 6
Ungrouped cases 23 0 0 23
% 1 100.0 .0 .0 100.0
2 .0 100.0 .0 100.0
3 .0 .0 100.0 100.0
Ungrouped cases 100.0 .0 .0 100.0
a100.0% of original grouped cases correctly classified.
NEURAL COMPUTATION IN AUTHORSHIP ATTRIBUTION 127
D
o
w
n
l
o
a
d
e
d
 
B
y
:
 
[
H
E
A
L
-
L
i
n
k
 
C
o
n
s
o
r
t
i
u
m
]
 
A
t
:
 
1
8
:
4
1
 
2
8
 
A
u
g
u
s
t
 
2
0
0
9
Fig. 7. Classification map after training LVQ1 technique for MB, SI and VK.
Fig. 8. Classification map after training LVQ1 technique for MB, SI,VK and unattributed
articles.
128 M. BAGAVANDAS ET AL.
D
o
w
n
l
o
a
d
e
d
 
B
y
:
 
[
H
E
A
L
-
L
i
n
k
 
C
o
n
s
o
r
t
i
u
m
]
 
A
t
:
 
1
8
:
4
1
 
2
8
 
A
u
g
u
s
t
 
2
0
0
9
elements are the misclassified cases. The articles of all three authors are
classified into three groups correctly. The overall percentages of cases
classified correctly are 100%. This result indicates that all the nineteen
articles of author 1, seven articles of author 2 and six articles of author 3
are correctly classified into three different groups. It shows that all the
nineteen articles of MB form a group and this result establishes that the
percentages of occurrences of different stylistic features in all the 19
articles of MB are the same. Hence it can be concluded that MB had
maintained the same style in writing all the 19 articles, and this is true
also in the cases of the other two scholars. Thus the consistency of the
writing styles of these three scholars is established. Also all the
Fig. 9. Classification of MB (author 1), SI (author 2) and TVK (author 3).
NEURAL COMPUTATION IN AUTHORSHIP ATTRIBUTION 129
D
o
w
n
l
o
a
d
e
d
 
B
y
:
 
[
H
E
A
L
-
L
i
n
k
 
C
o
n
s
o
r
t
i
u
m
]
 
A
t
:
 
1
8
:
4
1
 
2
8
 
A
u
g
u
s
t
 
2
0
0
9
23 unattributed articles are attributed to author 1 (Table 6). This shows
that Mahakavi Bharathiar may have written all these articles. This
important result endorses the statement of Ilasai Maniyan (1975).
5. CONCLUSION
Computational literary analysis provides a unique opportunity for
researchers to experiment with high dimensional data. This comparative
study made use of neural network and multivariate statistical techniques
to distinguish the writing styles of three Tamil language scholars, namely,
Mahakavi Bharathi, V. Kalyanasundaram and Srinivasa Iyer, and also
to attribute authorship for disputed articles. The materials for this study
were 19 articles of Bharathi, six articles of V. Kalyanasundaram, seven
articles of Srinivasa Iyer and 23 disputed articles. All these articles were
written on India’s freedom movement in same period. The stylistic
features of this study were 24 function words. Application of the first
version of learning vector quantization neural network technique and
Fig. 10. Classification of MB (author 1), SI (author 2), TVK (author 3) and unattributed
articles.
130 M. BAGAVANDAS ET AL.
D
o
w
n
l
o
a
d
e
d
 
B
y
:
 
[
H
E
A
L
-
L
i
n
k
 
C
o
n
s
o
r
t
i
u
m
]
 
A
t
:
 
1
8
:
4
1
 
2
8
 
A
u
g
u
s
t
 
2
0
0
9
canonical discriminant analysis have given the same results and say
that the writing styles of three Tamil scholars – namely, Bharathi,
Kalyanasundaram and Srinivasa Iyer – are distinct and consistent and
that the 23 disputed articles may have been written by Bharathi.
REFERENCES
Baras, J. S., & LaVigna, A. (1990). Convergence of Kohonen’s learning vector
quantization. Paper presented at the International Joint Conference on Neural
Networks, San Diego, CA, 3, pp. 17–20.
Holmes, D. I. (1998). The evolution of stylometry in humanities scholarship. Literary and
Linguistic Computing, 13(3), 111–117.
Illasai Manian, T. (1975). Bharathi Dharizanam. Chennai, India: Maraimalai Adigal
Padippagam.
Klecka, W. R. (1980). Discriminant Analysis. California: Sage Publications.
Kohonen, T. (1986). Learning vector quantization for pattern recognition. Technical
Report TKK-F-A601. Finland: Helsinki University of Technology.
Lowe, D., & Mathews, R. (1995). A stylometric analysis by radial basis functions.
Computers and the Humanities, 29, 449–461.
Mathews, R., & Merriam, T. (1993). Neural computation in stylometry I: An application
to the works of Shakespeare and Fletcher. Literary and Linguistic Computing, 8,
203–209.
Merriam, T., & Mathews, R. (1994). Neural computation in stylometry II: An application
to the works of Shakespeare and Morlowe. Literary and Linguistic Computing, 9,
01–06.
Michie, D., Spiegelhalter, D. T., & Taylor, C. C. (1994). Machine Learning, Neural and
Statistical Classification. London: Ellis Harwood Ltd.
Mosteller, F., & Wallace, D. (1964). Inference and Disputed Authorship: The Federalist.
Reading, MA: Addison-Wesley.
Tweedie, F. J., Singh, S., & Holmes, D. I. (1996a). Neural network applications in
stylometry: The Federalist papers. Computers and the Humanities, 30, 1–30.
Tweedie, F. J., Singh, S., & Holmes, D. I. (1996b). An introduction to neural networks in
stylometry. Research in Humanities Computing, 5, 249–263.
Waugh, A., Adams, A., & Tweedie, F. (2000). Computational stylistics using artificial
neural networks. Literary and Linguistic Computing, 15(2), 187–198.
NEURAL COMPUTATION IN AUTHORSHIP ATTRIBUTION 131
D
o
w
n
l
o
a
d
e
d
 
B
y
:
 
[
H
E
A
L
-
L
i
n
k
 
C
o
n
s
o
r
t
i
u
m
]
 
A
t
:
 
1
8
:
4
1
 
2
8
 
A
u
g
u
s
t
 
2
0
0
9
