See	discussions,	stats,	and	author	profiles	for	this	publication	at:	http://www.researchgate.net/publication/283452953
Emotion	Analysis	of	Arabic	Articles	and	Its
Impact	on	Identifying	the	Author's	Gender
CONFERENCE	PAPER	·	NOVEMBER	2015
READS
24
5	AUTHORS,	INCLUDING:
Mohammed	A.	Shehab
Jordan	University	of	Science	and	Technology
7	PUBLICATIONS			13	CITATIONS			
SEE	PROFILE
Mahmoud	Al-Ayyoub
Jordan	University	of	Science	and	Technology
68	PUBLICATIONS			232	CITATIONS			
SEE	PROFILE
Available	from:	Mahmoud	Al-Ayyoub
Retrieved	on:	24	November	2015
Emotion Analysis of Arabic Articles and Its Impact
on Identifying the Author’s Gender
Kholoud Alsmearat∗, Mohammed Shehab∗ Mahmoud Al-Ayyoub∗ Riyad Al-Shalabi† and Ghassan Kanaan†
∗Jordan University of Science and Technology
Irbid, Jordan
Emails: {kaalsmearat12,mashehab12}@cit.just.edu.jo, maalshbool@just.edu.jo
†Amman Arab University
Amman, Jordan
Email: {rshalabi64,ghassan.kanaan}@yahoo.com
Abstract—The Gender Identification (GI) problem is con-
cerned with determining the gender of the author of a given text
based on its contents. The GI problem is one of the authorship
profiling problems which have a wide range of applications
in various fields such as marketing and security. Due to its
importance, extensive research efforts have been invested in the
GI problem for different languages. Unfortunately, the same
cannot be said about the Arabic language despite its strategic
importance and widespread. In this work, we explore the GI
problem for Arabic text as a supervised learning problem.
Specifically, we consider and compare two approaches for feature
extraction. The first one is the Bag-Of-Words (BOW) approach
while the second one is based on computing features related to
sentiments and emotions. One goal of this work is to confirm the
validity of the common stereotype that female authors tend to
write in a more emotional way than male authors. Our results
show that there is no conclusive evidence that this is true for our
dataset.
Index Terms—Arabic text analysis, gender identification, bag-
of-words, sentiments lexicons, emotions lexicons.
I. INTRODUCTION
During the early years of the Web, users’ interactions with
contents posted on websites were limited and one directional.
In the past 10-15 years, this behavior began to change as users
began to contribute more to contents of the websites and post
their own material. Some of features of the Internet such as
the inherent anonymity and the lack of accountability make
it difficult to determine the real author of any text posted
online or any significant characteristic of him/her. Anyone can
post anything online and falsely claim to be another person or
falsely claim to have certain characteristics (such as gender,
age, etc.). This problem is further amplified with the wide-
spread of the social networks and chat services. Thus, there is
a growing need for an automated tool that can accurately and
efficiently detect such false claims.
The authorship analysis (AA) problem deals with analyzing
a certain text to either determine the identity (authorship
authentication or attribution problem) or the characteristics
(authorship characterization or profiling problem) of the au-
thor based on the content of the text [1]. In this sense, the AA
problems can be thought as variations of the well-known Text
Classification (TC) problem where different classes represent
different authors (for the authorship attribution problem) or
different characteristics of the authors (for the authorship
profiling problem) instead of the typically assumed topic (or
domain) classes [2]. Each type of the AA problems has its own
applications. For example, authorship profiling is very useful
for security-related applications such as deception and fraud
detection whereas authorship authentication has its historical
roots with the existence of many classical texts with doubtful
attribution to well-known authors such as Shakespeare [3].
Since these are historical problems with continuously renewed
interest, it is useful to look into the classical approaches of
addressing them before discussing what is new. The classical
approach followed by linguists is to identify unique stylometric
features of written texts. The fundamental assumption here is
that each author (or group of authors with common character-
istics such as age group) has certain writing styles (measured
using stylometric as well as statistical features) that do not
change with time and thus can be used to uniquely determine
the author (or the characteristics of the author) [4].
In this work, we are interested in an authorship profiling
problem in which we wish to determine the gender of the
author. This Gender Identification (GI) problem has its obvious
applications in various fields from marketing to security. It
is so important that the high profile Conference and Labs of
the Evaluation Forum (formerly known as the Cross-Language
Evaluation Forum, or CLEF) has dedicated one of its challenge
tasks for it. To be specific, the evaluation lab on uncovering
plagiarism, authorship, and social software misuse (PAN)
has been featuring author profiling tasks (for English and
Spanish) since its 13th version in 2013.1 We are interested
in addressing the GI problem for Arabic articles. To the best
of our knowledge, only one paper has previously addressed
this problem.
Being the mother tongue of hundreds of millions of people
in the Arab world and the main language for the religious
practices and rituals of more than 1.5 billion Muslims all
over the world, the Arabic language is considered one of the
main live languages in the world [5], [6]. However, this is
not the only reason that makes Arabic so interesting to study.
1http://www.uni-weimar.de/medien/webis/events/pan-13/pan13-web/
author-profiling.html
The challenges associated with the Arabic language and the
relatively young and under-developed field of Arabic Natural
Language Processing (NLP) makes it more appealing to study
such problems for Arabic. Arabic has many characteristics
that have significant effect on the general authorship analysis
problems such as its complexity, its “triglossic” nature,2 being
a derivational and inflectional language, etc. [2], [7].
The general approach we follow to address the problem at
hand is as follows. We make use of a dataset collected in a
previous work on GI of Arabic articles [8]. The dataset consists
of articles written in MSA by various authors of both genders.
The authors are homogenous in the sense that they all hail from
the same geographical region with similar levels of education
and decent experiences in professional/semi-professional writ-
ing. The dataset is subjected to several preprocessing steps
before going into the main step of interest in this work, which
is feature extraction. We consider and compare two approaches
for feature extraction. The first one is the Bag-Of-Words
(BOW) approach while the second one is based on computing
features related to sentiments and emotions. One goal of this
work is to confirm the validity of the common stereotype that
female authors tend to write in a more emotional way than
male authors [9], [10]. We conduct various experiments and
perform a t-test on the returned results in order to confirm or
disconfirm this claim.
The rest of this paper is organized as follows. The following
section (Section II) gives a general overview of the current
literature on TC and AA with a focus on works considering the
Arabic language. After that, the methodology followed in this
work and the experimental results are discussed in Sections III
and IV, respectively, before concluding the paper in Section V
with a discussion of the obtained observations and the future
directions of this work.
II. RELATED WORKS
The general text classification (TC) problem is concerned
with automatically categorizing text documents based on their
contents. It is one of the fundamental problems in many
fields such as text mining, machine learning, natural language
processing, information retrieval, etc., with a vast range of
applications such as spam filtering [11], sentiment analysis
[12], [13], [14], [15], determining author’s characteristics such
as identity [16], [3], [17], [18], gender [10], [19], dialect
[20], native language [21], political orientation [22], etc. In
following paragraphs, we briefly discuss some of recent works
on general Arabic TC. Unfortunately, the field of AA is still
largely understudied for the Arabic language, to the best of
our knowledge.
2Three variation of Arabic are commonly used: (i) the classical Arabic
used in the Holy Quran as well as classical and theological texts, (ii) the
Modern Standard Arabic (MSA) used in the media, education, and formal
communication and (iii) colloquial Arabic (each region has its own dialect).
The first two types are generally well-understood across all Arabic speakers
even if they have a mediocre level of education. On the other hand, each
dialect is usually understood only by its native speakers with the exception
of a few popular dialects such as the Egyptian one.
Several papers on Arabic TC have been published in the
past two decades. For a recent and broad coverage of these
papers, the interested reader is referred to [23], [24], [25], [26],
[27], [28]. An important thing to note here is that the majority
of the current works consider datasets collected from online
sources such as news websites and follow the general approach
as follows. They start by preprocessing the text to remove
unwanted contents such as punctuation marks, numbers, non-
Arabic letters and stop words. Then, feature vectors are
computed for each document using the bag-of-words (BOW)
approach (with different weighting techniques) or any other
approach such as n-gram words/characters. Part-of-speech
(POS) tagging, stemming,3 χ2 statistics and other feature
reduction techniques are sometimes necessary to handle the
large number of extracted features. Finally, a classification
algorithm is trained on part of the dataset and tested on the
rest of the dataset.
Recently, several interesting works appeared addressing the
Arabic TC problem in interesting ways that are relevant to
authorship analysis problems. One example is [29] in which
the authors focus on the emergence of new writing styles due
to the prevalence of online social networks and the effect
of these styles on typical methods of text categorization.
Other works focused on character-based features. One such
example is the use of compression based techniques [30],
which were only recently applied to Arabic text [31]. The
results showed some advantages for such approaches over
typical “word features” based approaches. These advantages
are worth investigating for problems like authorship analysis.
Compared to TC, AA is largely understudied in the Arabic
language, to the best of our knowledge. Below, we discuss
some of the works on this field starting with the more prevalent
problem, which is authorship attribution, before going into
the more relevant works on authorship profiling. For both
problems, computing stylometric features is very common as
it is regarded as a more intuitive approach than the BOW
approach.
To the best of our knowledge the authorship attribution
problem for the Arabic language has not been studied well as
the number of published works is very small [1], [32], [33],
[34]. One of the most notable works is that of Abbasi and Chen
[1] in which the authors used different sets of features includ-
ing lexical, syntactic, structural and content-specific features
for the authorship attribution problem under both the English
and the Arabic languages. The classification techniques used
in their study were SVM and decision trees. In [32], the author
focused on the problem of small and imbalanced datasets,
which is a common problem with AA datasets. He represented
each document using the bag of character n-gram approach
which he claims to be better than the bag of words approach
as it can capture stylistic as well as thematic information more
accurately. Shaker and Corne [33] applied the popular author-
ship authentication approach of relying on the usage pattern
of function words to the Arabic language. They exploited a set
3Reducing each word to its stem or root.
of 104 function words reflecting the semantics of the English
function words. As for the classification approach, they used a
hybrid of evolutionary algorithms (EA) and linear-discriminant
analysis (LDA), which is known to have excellent performance
in authorship authentication in English based on a function
word approach. They tested their approach on a dataset of 14
Arabic novels by six different writers. Ouamour and Sayoud
[34] considered a dataset of 30 historic texts written by 10
different authors. What is special about this dataset is that the
authors are all famous well-educated Arab explorers describing
their travels and expeditions to different regions of the world.
The consistency in the topic and the educational background
of the authors adds to the difficulty of distinguishing between
their works. The authors applied the n-gram technique on the
character-level as well as the word-level to extract features
in addition to considering rare words. Finally, the authors
experimented with several classifiers such as SVM, ANN, etc.
In the authorship characterization (profiling) problem, the
goal is to determine certain traits or characteristics of the
author. Identifying the gender of the author is one example
of such characteristics. The only two papers we know of on
GI of Arabic articles are the one by Estival et al. [35] and the
one by Alsmearat et al. [8]. In [35], the authors considered a
deeper level of authorship profiling by considering two types
of traits: demographic and psychometric. The demographic
traits included the author’s age, gender, and level of education
whereas the psychometric traits are: extraversion, lie (or social
desirability), neuroticism (or emotionality), and psychoticism
(or tough mindedness). They considered two datasets: English
and Arabic. The Arabic dataset consisted of 8,028 emails
(including 761 emails written using Latin script). The emails
were written by 1,030 authors. To collect information about
their traits, the authors were asked to fill out a questionnaire.
The best accuracy for the GI problem was 81.15% and it was
obtained by SVM.
The authors of [8] collected a dataset of 500 articles
distributed equally among the two genders. They performed
an extensive study of the BOW approach to GI by consid-
ering several feature selection/reduction reduction techniques
coupled with different classifiers. They reached an unlikely
conclusion that the BOW approach can yield very encouraging
results. In fact, the best accuracy was 94.1% and it was
obtained by the Stochastic Gradient Descent (SGD) classifier.
The conclusion of [8] is considered unlikely since most
existing works focus solely on computing stylometric finger-
prints of the authors to be used to uniquely identify them. In
our work, we extend [8]’s work by considering more features
related to sentiments and emotions and show whether such
features improves the classification accuracy or not. This is
one step towards bridging the gap between [8]’s work and
existing works in the literature that rely on stylometric features
since it is widely believed that female authors tend to employ
emotions and sentiments in a way different from male authors,
which make them the most intuitive stylometric features to be
considered.
Finally, another relevant characteristic for authorship pro-
filing is related to the school of thought or ideology of the
author. This is considered by [22], [36].
III. METHODOLOGY
This section is dedicated to discussing the details of the
two features extraction techniques considered in this work:
the BOW approach and the Sentimemt/Emotion (SE) features
approach. As with any supervised approach, the first step is to
provide a labeled dataset for training and testing purposes. The
next step is feature extraction. The BOW splits each document
into a set of terms (words) and computes the feature vectors
based on their frequencies. Since there are many terms in
any natural language, the BOW approach usually generates
high dimensional feature vectors making the problem too large
for most classifiers to handle. Thus, dimensionality reduction
techniques are usually applied. As for the other approach
for feature extraction, it relies on computing SE features by
summing the numbers of occurrences of SE terms in each
article. Once the features are computed, the dataset is split
into a training set and a testing set. The former is fed into
a classification algorithm which builds a model of it. Then,
the testing set is used to evaluate the suitability of the build
model. In the following subsection, we discuss each one of
these phases in details.
A. The Dataset
Like any other TC problem, the first step is to build a
large-enough dataset. The dataset was collected manually by
[8] from several Arabic news websites such as http://www.
khaberni.com/ and http://www.sawaleif.com/. It is a collection
of articles written mainly in Modern Standard Arabic (MSA)
with the occasional use of phrases of colloquial nature. The
dataset consists of 500 articles distributed evenly across the
two genders. It enjoys some balance between the two classes
not just by having equal numbers of authors (15 authors) from
each gender, but also by having almost identical distributions
of articles across the different authors for the two classes
under consideration. Moreover, the articles in the dataset were
chosen so as to provide consistency in certain aspects while
providing some diversity in other aspects. For example, all
authors are well-educated authors from the same region (Jor-
dan/Palestine) with some experience in journalistic writing.
They all had their own daily contributions published either
in known traditional newspapers or on popular online news
websites. On the other hand, there is some diversity in the
writing style in the sense that some of them tend to use
sarcasm more frequently. These choices will provide more
meaningful results and allow us to gain some insights into
this problem.
Before going further into the methodology we follow and
the results we obtain, a simple statistical analysis of the dataset
is presented in Table I as it will help in explaining some of
the results obtained. The table shows that while male authors
tend to write much longer articles in terms of the numbers
of words and characters, the number of sentences they use in
an article is not much larger than that of the female authors.
TABLE I
STATISTICAL FEATURES OF THE COLLECTED DATASET.
Female Male
Number of articles 250 250
Number of authors 15 15
Avg. number of sentences/article 32.6 49.7
Avg. number of words/article 326.6 647.5
Avg. number of characters/article 1925.7 3750.8
Avg. number of characters/sentence 59.1 75.4
Avg. number of characters/word 5.9 5.8
Avg. number of words/sentence 10 13
This means that their sentences are considerably longer than
female authors. These observations do not fit very well with
the misconception that women “tend to talk more.” As for
using “big words,” both classes use, on average, words of
the same character-length. This is actually predictable since
in Arabic, words tend to be smaller than English and the
difference in word length distribution is of smaller significance
when it comes to writing styles [1], [2].
B. Feature Extraction
The first feature extraction approach is the BOW approach.
It relies on the occurrences of terms within document. To
provide more meaningful features, the term frequency-inverse
document frequency (tf-idf) weighting technique is applied
which gives higher weights to terms appearing frequently
in the given document and rarely in other document. Since
the BOW approach usually generates high-dimensional feature
vectors, dimensionality reduction techniques must be applied.
Several measures can be taken to address this issue. The first
one is to reduce the number of terms taken from each docu-
ment. In our experiments, we set this number to 1,000. Another
technique is to use stemming. For this project, the widely
used Khoja root-based stemming technique is employed. It
is worth mentioning that for our dataset, the resulting number
of features is 1,211.
As for SE features, we use a lexicon-based approach to com-
pute them similar to the ones used in [37], [38]. Specifically,
we exploit the NRC Emotion Lexicons (EmoLex) provided
by Mohammad and Turney [39]. Basically, EmoLex contains
ten unweighted lexicons. Two of which are for sentiments
(positive and negative) while the remaining eight are for
the eight emotional classes commonly used in the literature
(anger, anticipation, disgust, fear, joy, sadness, surprise and
trust). It should be noted here that EmoLex were originally
collected for the English language and then later translated
(using Google Translate service) into several other languages
including Arabic. The use of machine translation resulted
many duplicates, flaws and inconsistencies in the translated
lexicons. Moreover, EmoLex contained many terms conveying
no sentiment or emotion. All of these issues were cleared by
manually inspecting the lexicons. This reduced the number
of terms in the lexicons from 14,182 to only 4,279. Table II
shows the number of terms remaining in each lexicon.
TABLE II
THE NUMBER OF TERMS REMAINING IN EACH LEXICON.
Lexicon Number of Terms
Positive 1541
Negative 2169
Anger 809
Anticipation 571
Disgust 702
Fear 987
Joy 471
Sadness 795
Surprise 341
Trust 827
The SE features we compute are very simple. We compute
a feature for each sentiment/emotion in each each article. This
is achieved by taking the sum of the numbers of occurrences
of all terms the lexicon associated with the sentiment/emotion
under consideration. Since there are a large number of terms in
each article and even a larger number of terms in the lexicons,
simple sequential search would take hours to compute the
required feature values. So, we resort to multi-threading and
build a very efficient tool capable of computing these feature
values within few minutes.
The tool starts by reading the lexicon terms and applying
light stemming on each term. The light stemmer removes the
stop words, suffixes and prefixes. Also it normalizes some
characters at the end of words the after stemming process.
After that, the tool reads the dataset files and initializes the
number of threads to run the code in parallel. Each thread takes
input file and runs light stemming and tokenization on the
input file. It then starts searching for keywords in the lexicon
and writes the final results inside one variable. Reading from
a single file does not create any conflict between the threads;
however, writing on the same memory locations requires some
method to synchronize the writing operations. The master
thread controls the worker threads and checks the writing
process. It maintains the order of the input files by sending
each file to a single worker thread.
C. Classification
Foe each of the two feature extraction approaches under
consideration, we experimented with several classifiers. Some
of the considered classifiers are known to perform well for
the BOW approach of TC such as Naive Bayes while others
are known to be unsuitable for this purposes such as K-
Nearest Neighbor. To avoid cluttering, we only report the
most interesting results we obtain. Following is a list of the
classifiers discussed in this work.
• BNaive Bayes (NB).
• Decision Tree (DT).
• Support Vector Machines (SVM).
• K-Nearest Neighbor (KNN).
Generally speaking, Bayesian classifiers and SVM are
known to perform very well for the general TC problem [2],
TABLE III
RESULTS FOR THE BOW FEATURES ALONE.
Acc Pre Rec F AUC
DT 80% 80.1% 80% 80% 83.2%
NB 75.4% 78.7% 75.4% 74.9% 83.7%
SVM 86.4% 86.7% 86.3% 86.4% 86.6%
KNN 56.1% 65% 56.1% 51.9% 57.6%
[30]. On the other hand, tree-based classifiers can sometimes
be suitable for certain text mining problems while KNN is
probably unsuitable for the problem at hand despite being the
most efficient classifier under consideration in terms of the
running time. It is included mainly to provide some sort of
benchmarking.
IV. RESULTS AND EVALUATION
An extensive set of experiments is conducted to evaluate
the efficiency of different feature selection techniques and
different classifiers on the collected dataset. Moreover, these
tests are conducted on one of the most popular data mining
tools, Weka [40]. Below, we discuss some of the details related
to the experiments before going into the results.
For the testing option, we choose to use the hold-out method
in which two thirds of the dataset is used for training and the
remaining third is used for testing. Each experiment is repeated
five times with five different seeds and the average accuracy
measures are reported. The measures we report include the
accuracy in addition to the precision, recall (sensitivity), F-
measure and Area Under the ROC Curve (AUC).4
The experiments are divided into three parts. In the first
part, we experiment with the BOW features alone, while, in
the second part, we experiment with the SE features alone.
In the last part, we combine both feature sets and experiment
with them. Table III shows the results for the BOW features
alone. As expected, the table shows the superiority of SVM
over the other classifiers. The table also shows that DT and NB
perform well whereas KNN is performing at a level close to
the random baseline. A final observation about this table is the
close values of precision and recall. In a binary classification
problem such as the one under consideration, close values of
precision and recall suggest that the classifier has the same
ability to classify each class. The only exception here is the
KNN classifier.
Table IV shows the results for the SE features alone. The
table shows a surprising trend that relying on such features
does not produce results much better than the random baseline.
This means that these features posses low discriminative power
suggesting that there might be little or no difference between
male and female authors in terms of the use of sentiment-
baring or emotional terms. Another, less important, observa-
tion from this table is that the relative order of classifiers in
terms of accuracy is the same in Table III.
4http://en.wikipedia.org/wiki/Precision and recall
TABLE IV
RESULTS FOR THE SE FEATURES ALONE.
Acc Pre Rec F AUC
DT 58.7% 60.8% 58.7% 57.7% 61.1%
NB 60.1% 65.7% 60.1% 57% 62.2%
SVM 61.9% 70.3% 61.9% 58.1% 62.5%
KNN 58.7% 59.3% 58.7% 58.6% 58.9%
TABLE V
RESULTS FOR THE BOW+SE FEATURES.
Acc Pre Rec F AUC
DT 80.1% 80.2% 80.1% 80.1% 82.3%
NB 74.8% 78.8% 74.8% 74.1% 82.2%
SVM 86.4% 86.7% 86.3% 86.4% 86.6%
KNN 55.3% 62.4% 55.3% 51.5% 57.3%
Table V shows the results for the combined BOW and SE
features. These results are very close to the ones in Table III
supporting the hypothesis that SE features play little role in
distinguishing male authors from female ones. Here again, the
superiority of SVM as well as the inferiority of KNN are
evident.
The results discussed in the previous paragraphs elude to
the fact that SE features play little to no role in distinguishing
between male and female authors. To prove or disprove this
claim, we use the t-test. We consider the accuracies of each
classifier with and without the addition of the SE features.
The average accuracy (µ = 0.0032, σ = 0.0026 and N = 20)
was not significantly greater than zero, t(7) = 1.237, two-tail
p = 0.231, refuting the hypothesis that the accuracy improved
with the addition of the SE features. A similar conclusion is
reached if the same test was applied on the f-measure instead
of the accuracy.
V. CONCLUSION AND FUTURE WORK
In this paper, we addressed the problem of automatically
determining the gender of the author of an Arabic article
based on its contents. This important yet largely understudied
problem has great benefits in many areas from marketing
to security. In this work, we explored the GI problem for
Arabic text as a supervised learning problem. Specifically,
we considered and compared two approaches for feature
extraction. The first one is the Bag-Of-Words (BOW) approach
while the second one is based on computing features related to
sentiments and emotions. One goal of this work was to confirm
the validity of the common stereotype that female authors tend
to write in a more emotional way than male authors. Our
results showed that there is no conclusive evidence that this is
true for our dataset.
REFERENCES
[1] A. Abbasi and H. Chen, “Applying authorship analysis to extremist-
group web forum messages,” Intelligent Systems, IEEE, vol. 20, no. 5,
pp. 67–75, 2005.
[2] A. Alwajeeh et al., “On authorship authentication of arabic articles,” in
The fifth International Conference on Information and Communication
Systems (ICICS 2014), 2014.
[3] E. Stamatatos, “A survey of modern authorship attribution methods,”
Journal of the American Society for Information Science and Technol-
ogy, vol. 60, no. 3, pp. 538–556, 2009.
[4] P. Juola, “Large-scale experiments in authorship attribution,” English
Studies, vol. 93, no. 3, pp. 275–283, 2012.
[5] N. A. Abdulla et al., “An extended analytical study of arabic sentiments,”
International Journal of Big Data Intelligence, vol. 1, no. 1, pp. 103–
113, 2014.
[6] N. Abdulla et al., “Automatic lexicon construction for arabic sentiment
analysis,” in The 2nd International Conference on Future Internet of
Things and Cloud (FiCloud), 2014.
[7] M. Al-Ayyoub, S. Bani Essa, and I. Alsmadi, “Lexicon-based sentiment
analysis of arabic tweets,” International Journal of Social Network
Mining (IJSNM), vol. 2, no. 2, pp. 101–114, 2015.
[8] K. Alsmearat et al., “An extensive study of the bag-of-words approach
for gender identification of arabic articles,” in Computer Systems and
Applications (AICCSA), 2014 IEEE/ACS 11th International Conference
on. IEEE, 2014, pp. 601–608.
[9] J. M. Jaffe, Y. Lee, L. Huang, and H. Oshagan, “Gender, pseudonyms,
and cmc: Masking identities and baring souls,” in 45th Annual Con-
ference of the International Communication Association, Albuquerque,
New Mexico, 1995.
[10] N. Cheng, R. Chandramouli, and K. Subbalakshmi, “Author gender
identification from text,” Digital Investigation, vol. 8, no. 1, pp. 78–
88, 2011.
[11] C. C. Aggarwal and C. Zhai, “A survey of text classification algorithms,”
in Mining text data. Springer, 2012, pp. 163–222.
[12] A. Abbasi, H. Chen, and A. Salem, “Sentiment analysis in multiple
languages: Feature selection for opinion classification in web forums,”
ACM Transactions on Information Systems (TOIS), vol. 26, no. 3, p. 12,
2008.
[13] N. Abdulla et al., “Arabic sentiment analysis: Corpus-based and lexicon-
based,” in Proceedings of The IEEE conference on Applied Electrical
Engineering and Computing Technologies (AEECT), 2013.
[14] M. N. Al-Kabi et al., “An analytical study of arabic sentiments: Maktoob
case study,” in The 8th International Conference for Internet Technology
and Secured Transactions (ICITST). IEEE, 2013, pp. 89–94.
[15] B. Al Shboul et al., “Multi-way sentiment classification of arabic
reviews,” in ICICS. IEEE, 2015.
[16] P. Juola, “Authorship attribution,” Foundations and Trends in information
Retrieval, vol. 1, no. 3, pp. 233–334, 2006.
[17] M. Al-Smadi et al., “Using aspect-based sentiment analysis to evaluate
arabic news affect on readers,” in 8th IEEE/ACM International Confer-
ence on Utility and Cloud Computing, 2015.
[18] J. Albadarneh et al., “Using big data analytics for authorship authenti-
cation of arabic tweets,” in 8th IEEE/ACM International Conference on
Utility and Cloud Computing, 2015.
[19] F. Rangel et al., “Overview of the author profiling task at pan 2013,” in
CLEF Conference on Multilingual and Multimodal Information Access
Evaluation. CELCT, 2013, pp. 352–365.
[20] O. F. Zaidan and C. Callison-Burch, “Arabic dialect identification,”
Computational Linguistics, vol. 40, no. 1, 2013.
[21] J. Tetreault, J. Burstein, and C. Leacock, Eds., Proceedings of the Eighth
Workshop on Innovative Use of NLP for Building Educational Applica-
tions. Atlanta, Georgia: Association for Computational Linguistics,
June 2013.
[22] M. Koppel, N. Akiva, E. Alshech, and K. Bar, “Automatically classifying
documents by ideological and organizational affiliation,” in Intelligence
and Security Informatics, 2009. ISI’09. IEEE International Conference
on. IEEE, 2009, pp. 176–178.
[23] D. Said, N. M. Wanas, N. M. Darwish, and N. Hegazy, “A study of
text preprocessing tools for arabic text categorization,” in The Second
International Conference on Arabic Language, 2009.
[24] M. K. Saad, “The impact of text preprocessing and term weighting on
arabic text classification,” Master’s thesis, Computer Engineering, The
Islamic University-Gaza, 2010.
[25] M. Khorsheed and A. Al-Thubaity, “Comparative evaluation of text
classification techniques using a large diverse arabic dataset,” Language
resources and evaluation, vol. 47, no. 2, 2013.
[26] I. Hmeidi et al., “Automatic arabic text categorization: A comprehensive
comparative study,” Journal of Information Science, vol. 41, no. 1, pp.
114–124, 2015.
[27] ——, “A comparative study of automatic text categorization methods
using arabic text,” in ITMC, 2015.
[28] N. Ahmed et al., “Scalable multi-label arabic text classification,” in
ICICS. IEEE, 2015.
[29] M. Faqeeh et al., “Cross-lingual short-text document classification for
facebook comments,” in The 2nd International Conference on Future
Internet of Things and Cloud (FiCloud), 2014.
[30] Y. Marton, N. Wu, and L. Hellerstein, “On compression-based text
classification,” in Advances in Information Retrieval. Springer, 2005,
pp. 300–314.
[31] H. Ta’amneh et al., “Compression-based arabic text classification,” in
The ACS/IEEE International Conference on Computer Systems and
Applications (AICCSA), 2014.
[32] E. Stamatatos, “Author identification: Using text sampling to handle
the class imbalance problem,” Information Processing & Management,
vol. 44, no. 2, pp. 790–799, 2008.
[33] K. Shaker and D. Corne, “Authorship attribution in arabic using a hybrid
of evolutionary search and linear discriminant analysis,” in 2010 UK
Workshop on Computational Intelligence (UKCI). IEEE, 2010, pp.
1–6.
[34] S. Ouamour and H. Sayoud, “Authorship attribution of short historical
arabic texts based on lexical features,” in 2013 International Conference
on Cyber-Enabled Distributed Computing and Knowledge Discovery
(CyberC). IEEE, 2013, pp. 144–147.
[35] D. Estival et al., “Tat: an author profiling tool with application to
arabic emails,” in Proceedings of the Australasian Language Technology
Workshop, 2007, pp. 21–30.
[36] R. Abooraig et al., “On the automatic categorization of arabic articles
based on their political orientation,” in 3rd International Conference on
Informatics Engineering and Information Science (ICIEIS), 2014.
[37] N. Abdulla et al., “Towards improving the lexicon-based approach for
arabic sentiment analysis,” IJITWE, vol. 9, no. 3, pp. 55–71, 2014.
[38] I. Obaidat et al., “Enhancing the determination of aspect categories and
their polarities in arabic reviews using lexicon-based approaches,” in
AEECT. IEEE, 2015.
[39] S. M. Mohammad and P. D. Turney, “Crowdsourcing a word-emotion
association lexicon,” Computational Intelligence, vol. 29, no. 3, pp. 436–
465, 2013.
[40] M. Hall et al., “The weka data mining software: an update,” ACM
SIGKDD explorations newsletter, vol. 11, no. 1, 2009.
