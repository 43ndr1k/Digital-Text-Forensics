A Machine Learning Approach for Analyzing Musical Expressions of Piano 
Performance 
 
 
Kuo-Liang Ou, Pao-Te Tsai, Wern-huar Tarng 
Graduate Instute of Computer Science, 
National Hsin-Chu Uinversity of Education 
kloutw@gmail.com 
 
Abstract 
This paper proposed a machine learning approach for 
analyzing teachers’ expert knowledge of classifying 
students’ piano performance into approximate expression 
categories. Students are usually confused when learning 
the expressive performance because of teachers’ 
subjective intention difference on the same performance. 
In this paper, teacher models will be built by analyzing 
teachers’ classification rules. By replaying their 
performances and read teachers’ suggestions in 
graphical and textual modes which are generated 
automatically by teacher model, students could 
understand the nuance of performance features on each 
expression. Three teachers and ten students joined this 
experiment. Sixty piano performances were recorded for 
constructing the teacher models. The average accuracy 
of teacher models for classifying performance expression 
is 70.8%. Questionnaires reflect both teachers and 
students are satisfied with the user interface, generated 
suggestions, and classification rules. 
1. Introduction 
The expressive marks on music staves support the 
communications between composers and the performers 
about the performance expression with the limitations of 
the scores [1]. For example, a music scores with dolce 
expressive mark represents the composers’ implication 
for a sweet and tender music. Therefore, a dolce music 
usually trend to be performed with lower volume and 
graceful rhythm; on the other hand, a music scores with 
agitato expressive make represents the implication for a 
agitate music. Consequently, an agitato music often trend 
to be performed with higher volume and sharp rhythm. 
In fact, there is neither formal definition nor detail 
description of musical expressive marks; the performers 
need to describe the expressive intention subjectively, 
which is the part of art and make music live. However, 
students who are the piano beginners have difficulties to 
learn about the performance expression with different 
teachers who have different expressive intentions on the 
same music scores.  
To understand the nuance of performance expression, 
some researches [2-5] successfully captured and 
analyzed the performers’ tradeoff between the 
performance features, for example, tempo, duration, 
loudness, precision of pitch, and timbre. Computer 
techniques  are beneficially in improving the precision of 
recognizing the performance features [6] [7] [8] . These 
researches indicate that the computer based 
methodologies support the possibility of scientific 
approaches on criticizing music art, and on assisting 
students to learn the performance expression via 
analyzing the nuance of performance features. 
This paper proposed a novel system of employing 
the machine learning [9] approach for students to analyze 
their performance features when learning the 
performance expression. Students’ performances on 
digital pianos were recorded in standard MIDI files at 
first. Then teachers listened students’ performances and 
classified each them into one of the approximate 
expression. The teacher models were constructed by 
analyzing the relationships between students’ 
performance features and teachers’ classification rules. 
The teacher models could generate graphical and textual 
suggestions for students about the nuance of their 
performance for specific expression intention. Thus, 
students could realize the place on music staves need to 
be noticed when playing, and modify their performance 
by practice repeatedly to approach teachers’ expressive 
intention.  
2. Relative works 
Expressive intention of a music performance 
involves various aspects of human sensory perception. 
Some researches propose systematical methods to 
analysis the factors of expressive intention. For example, 
Canazza, De Poli, and Vidolin [7] proposed a 
multidimensional scaling and cluster methodology for 
recognized the performers’ intention. The dimensions 
include soft-hard, dark-bright, and heavy-light. Every 
expressive intention could be mapped into the 
multidimensional space. In this paper, similar with 
Canazza et al.’s research, music feature of students’ 
performance will be extract, however, instead of cluster 
methodology; the classification methodology will be 
employed for constructing teacher models and 
classifying students’ performance into approximate 
expression. 
Gestalt theory[10] was founded by German thinkers 
Max Wertheimer, Wolfgang Kohler, and Kurt Koffka 
and focused on how people interpret the world. It 
addresses on human’s perceptive and cognitive processes 
based on the basic principle that the whole structure 
carries a different and greater meaning than its individual 
elements. Besides that, human have the abilities to 
realize the whole structure with sensing the patterns and 
the correlations between the individual elements[11]. 
Thus, the stimulation of individual items is grouped 
together to produce an organized structure according to 
four grouping laws: 
2009 Ninth International Conference on Hybrid Intelligent Systems
978-0-7695-3745-0/09 $25.00 © 2009 IEEE
DOI 10.1109/HIS.2009.16
42
 Vicinity: The cluster, which means objects are close 
enough to each other and apart enough from the rest 
to build a group. 
 Similarity: The similarity of objects’ attributes. 
Things which are similar in some way appear to be 
grouped together. Grouping can occur in both visual 
and auditory stimulation.   
 Continuity of direction: Points that are connected by 
straight or curving lines are seen in a way that 
follows the smoothest path. 
 Closure: Things are grouped together if they seem to 
complete some entity. The contradictory 
information and gaps in information are often 
ignored. 
 
When listening and perceive music, human also tend 
to gestalt the expressive intention based on above laws 
with sensing the features of each music elements. Some 
researchers have addressed how Gestalt Theory applied 
on music analysis. Cohen and Dubnov [12] discussed the 
Gestalt phenomena in musical texture, that the tonal 
music could be represented by different curves or 
contours. Godøy [13] addressed the possible relation 
between musical features and various shape-images, that 
humans could recognize musical objects as holistic 
entities. In this paper, each student’s performance is 
decomposed into phrases, which are the elements of 
melody. The performance features of each phrase are the 
keys of teacher models, and represent how teachers to 
classify the performance into approximate expressive 
category. The machine learning technique is employed 
for constructing the teacher models. 
Machine learning applied on music analysis, 
recently, becomes an interesting and challenging 
research topic. It contributes classification, clustering, 
sequential pattern analysis for modeling listeners’ 
intention. Widmer et. al [8, 14-16] presented artificial 
intelligence approaches to the problem of discrimination 
between music performers playing the same piece of 
music, and successfully analyzed the expressive music 
performance. However, there is no discussion about how 
teachers classify student’s performance expression. In 
this paper, teacher models of performance expression 
will be constructed by decision tree technique. By 
analyzing students’ performance via teacher models, 
students could realize the nuance of their performance 
expression on different teachers’ expressive intention. 
3. System Implementation 
There are three primary components in this system: 
(1) performance feature module (PF), (2) expression 
learning module (EL), and (3) expression analyzing 
module (EA). Figure 1 illustrates the experiment process 
and system architecture. 
 
Figure 1. System architecture 
Student’s performances will be recorded in MIDI 
file format directly by digital piano. The PF module will 
capture students’ performance features and divided these 
performance features into two groups: the performance 
feature for training and the performance feature for 
testing. All teachers need to do is to listen students’ 
performance and classify them into approximate 
expression class. The EL module then constructs teacher 
models of each teacher. The teacher model represents 
each teacher’s classification rules of expressive intention 
in the form of a decision tree. Finally, when a new 
performance is recorded for testing, the EA module will 
classify this performance by different teacher models 
into approximate expression category and give 
suggestions for students’ performance at the same time. 
The three primary modules are described as below: 
 
 Performance Feature Module 
 
In PF module, student’s performances recorded in  
standard MIDI data format are composted of Note, 
Controller, and Message. To transform student’s 
performance into digital format is for the purpose of 
computational analyzing. Any MIDI players could replay 
the performance with acceptable distortion. The 
following table illustrates the features captured by this 
module. 
 
Feature  Name Description 
Syncopation_ 
ratio 
The ratio of Syncopation notes to all 
notes. 
Bass_Ratio The ratio of notes below C3 key to all notes. 
Alto_Ratio The ratio of notes between C3 key and C5 key to all notes. 
Tenor_Ratio The ratio of note above C5 key to all notes. 
AT_Ratio The ratio of notes above middle C (C4) to all notes. 
Pitch_ 
Deflection 
The average distance between each 
notes to the core pitch 
Bass_Time_ 
Ratio 
The ratio of duration accumulated below 
C3 key to all notes. 
Pitch_ The standard deviation of note pitch. 
43
variation 
Chord The deviation of music chord . 
Legato_ 
Ratio_Fix 
The adjusted ratio of legato notes to all 
notes. 
Legato_Ratio The original ratio of legato notes to all notes. 
Volume_ 
variation 
The variation of volume 
Volume The average of volume 
1/16note_ 
Volume_ADV 
The sixteenth-notes average volume 
1/8note_ 
Volume_ADV 
The eighth-notes average volume 
Syncopation_ 
Volume_Ratio 
The volume ratio of Syncopation 
Note_Length_ 
ADV 
 
Crowd_Note_ 
Length 
The average length of crowd notes 
Crowd_Note_ 
Length_sub 
The ratio between the length of crowd 
notes and core notes 
Staccato_Ratio The ratio of Staccato notes hit by both hands to all notes 
Staccato_ 
Deflection_ABS 
The ratio of Staccato notes by left hand 
to right hand. 
Staccato_Ratio The ratio Staccato notes hit by any one hand to all note. 
Table 1. The captured music features 
 Expression Learning Module 
 
The EL model will construct teacher models for 
recognizing six performance expressions for each score. 
The performance expression include: Agitato(agitated), 
Animato(animated) ,Dolce(sweetly, tenderly), 
Dolente(sadly), Maestoso (majestically), and 
Pesante(heavy, lumpy). Generally speaking, the Agitato 
is relative to the Dolce; the Animato is relative to the 
Pesante; and the Maestoso is relative to the Dolente. 
However, different teachers may have different 
classification method in mind to evaluate students’ 
performance by considering the features they captured. 
The decision tree technique of machine learning could 
construct each teacher’s classification rules in the form 
of trees like the following figure: 
 
 
Figure 2. The teacher model of teacher A’s expressive 
performance classification rules 
Figure 2. illustrates teacher A’s teacher model of the 
performance features for training on Mozart’s Turkish 
March. If students’ performance with music features of 
Note_Length_ADV > 1715 and Chord >0, then teacher 
A will classify this performance as Maestoso 
expression (nine of ten training cases conformed with 
this rule). If the performance with music features of 
Note_Length_ADV > 1715, Chord <=0, and 
Bass_Time_Raion <=0.72255, then teacher A will 
classify this performance as Dolente (all ten cases 
conformed with this rule). Therefore, a Maestoso and 
Dolente performance both with the same music feature 
of note length; however, the differences between these 
two expressions are the Chord and Bass time ration.  
 
 Expression Analyzing Module 
 
After classifying each student’s performance into 
expression with specific teacher model, a graphic user 
interface is supported for realizing the place where 
should be noticed. Figure 3. shows that a student’s 
performance is intended to be played as a Dolce 
expression. However, the teacher model A classified it 
as a Maestoso expression. The teacher’s suggestions 
are labeled in blue lines (for the staccato) and red 
circles on the scores (for the syncoporations). The 
detail suggestions are also listed in textual mode. 
Therefore, students could read these suggestions and 
verify their performance by replay (Figure 4).  
 
 
Figure 3. The suggestions in graphical and textual mode 
 
44
 
Figure 4. The verified performance of specific expression 
4.  Results 
Ten students and three teachers joined this 
experiment. Each student performed six of fourteen 
phrases extracted from famous piano music on 
YAMAHA DGX-630 digital piano. The music phrases 
are listed below: 
  
Music name Composer Type 
Chopsticks Beyer op.101 etude 
Serenade F.P.Schubert Classical 
Bridal March R. Wagner Classical 
Dolly’s Dreaming 
and Awakening 
T. Oesten Classical 
Lu xiao yu Jay Chou modern 
Turkish March W.A.Mozart Classical 
For Elise L.v.Beethoven Classical 
National 
anthem(R.O.C) 
Cheng,Mao-yun anthem 
Präludium J.S. Bach Classical 
Hunter J.F. Burgmüller etude 
Harmonie des 
anges 
J.F. Burgmüller etude 
Pastorale J.F. Burgmüller etude 
Courant limpide J.F. Burgmüller etude 
Donna Donna Greece Folk rhyme 
Table 2. The list of music to be performed  
The performances were recorded in MIDI files 
immediately and transferred into the feature capture 
module online. Twenty-two features (Table 1) were 
captured successfully for each student’s performance. 
After listened these MIDI files of replay, teachers 
classified each performance into one of six expressions 
by their expert knowledge and personal subjective 
intention. The performance expression learning module 
then constructed a teacher model by analyzing each 
teacher’s classification rules. Figure 2, Figure 5, and 
Figure 6 illustrate the teacher models of teacher A, 
teacher B, and teacher C in decision trees.  
 
 
Figure 5 The teacher model of teacher B 
 
Figure 6 The teacher model of teacher C 
The teacher model A and teacher model B are 
similar, however, the teacher model C are quite different 
with the other two teacher models. For example, in 
teacher model A,  
The accuracy of teacher models for classifying 
expression are 82.35%, 70% and 60% (average 70.8%), 
which are estimated by cross validation method. The 
questionnaires reflect teachers’ average satisfaction on 
their classification is 78.3%. 
5. Conclusion 
In this paper, a computational methodology for 
capturing and analyzing the part of the art of music is 
proposed. The machine learning based approach for 
constructing teacher models of classifying students’ 
piano performance expression is successfully examined. 
The teacher models were composed by decision trees, 
each tree represents a teacher’s expressive intention of a 
music phrase. Thus, students could understand the 
nuance of their performance expression by reading the 
suggestions which are generated automatically by 
different teacher models. Ten students and three teachers 
joined this experiment for six specific expressions. The 
high accuracy and satisfactions indicate that a computer 
assisted methodology for humans to analyze and learn 
the art performance is beneficial.      
 
45
Acknowledgement 
The authors thank the National Science Council of 
Republic of China for financially supporting this 
research under Contract Nos. NSC-96-2520-S-134-001. 
 
References 
 
1] Levi, D.S., Expressive Qualities in Music Perception 
and Music Education. Journal of Research in Music 
Education, 1978. 26(4): p. 425-435. 
[2] Kendall, R.A. and E.C. Carterette, Communicating 
performer intent through musical performance. I. 
Mapping intent to perception. The Journal of the 
Acoustical Society of America, 1996. 100(4): p. 
2777-2777. 
[3] Palmer, C., Mapping musical thought to musical 
performance. Journal of experimental psychology. 
Human perception and performance, 1989. 15. 
[4] Minetti, A.E., L.P. Ardigo, and T. McKee, Keystroke 
dynamics and timing: Accuracy, precision and 
difference between hands in pianist&apos;s 
performance. Journal of Biomechanics, 2007. 40(16): 
p. 3738-3743. 
[5] Honing, H., Structure and Interpretation of Rhythm 
and Timing. Dutch Journal of Music Theory, 2002. 
7(3): p. 227-232. 
[6] Grindlay, G. and D. Helmbold, Modeling, analyzing, 
and synthesizing expressive piano performance with 
graphical models. Machine Learning, 2006. 65(2): p. 
361-387. 
[7] Canazza, S., G. De Poli, and A. Vidolin, Perceptual 
analysis of the musical expressive intention in a 
clarinet performance, in Music, Gestalt, and 
Computing. 1997. p. 441-450. 
[8] Widmer, G., Using AI and machine learning to study 
expressive music performance: project survey and 
first report AI Communications, 2001. 14(Volume 14, 
Number 3/2001): p. 149-162. 
[9] Mitchell, T.M., Machine Learning. 2006. 
[10] Köhler, W., Gestalt psychology. 1963, New York, 
NY: Liveright. 
[11] Peifeng, X., Y. Xin, and S. Yuanchun. Web Page 
Segmentation Based on Gestalt Theory. in 
Multimedia and Expo, 2007 IEEE International 
Conference on. 2007. 
[12] Cohen, D. and S. Dubnov, Gestalt phenomena in 
musical texture, in Music, Gestalt, and Computing. 
1997. p. 386-405. 
[13] Godøy, R., Knowledge in music theory by shapes of 
musical objects and sound-producing actions, in 
Music, Gestalt, and Computing. 1997. p. 89-102. 
[14] Stamatatos, E. and G. Widmer, Automatic 
identification of music performers with learning 
ensembles. Artificial Intelligence, 2005. 165(1): p. 
37-56. 
[15] Widmer, G., Discovering simple rules in complex 
data: A eta-learning algorithm and some surprising 
musical discoveries. Artificial Intelligence, 2003. 
146(2): p. 129-148. 
[16] Widmer, G., Machine Discoveries: A Few Simple, 
Robust Local Expression Principles, in Journal of 
New Music Research. 2002, Routledge. p. 37. 
 
46
