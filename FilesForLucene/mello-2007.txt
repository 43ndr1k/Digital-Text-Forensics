 
 
  
Abstract— It is presented in this paper an algorithm for 
thresholding images of historical documents. The main objective 
is to generate high quality monochromatic images in order to 
make them easily accessible thru Internet and achieve high 
recognition rates by Optical Character Recognition algorithms. 
There are several well-known thresholding algorithms which 
are not suitable for this kind of image. Our new algorithm is 
based on the classical entropy concept and a variation defined 
by the Tsallis Entropy and it proved to be more efficient than 
classical thresholding algorithms. 
I. INTRODUCTION 
 
HIS research takes place in the Image Processing of 
Historical Documents Project (DocHist) [16][17][18] for 
preserving and broadcasting of a file of thousands of 
historical documents. This file is composed of more than 
6,500 letters and documents which amounts more than 30,000 
pages from the end of the 19th century and beginning of the 
20th century. 
To preserve the file, the documents are digitized in 200 dpi 
resolution in true color and stored in JPEG file format with 
1% loss for better quality/space storage rate. Even in this 
format each image of a document reaches, in average, 400 
Kb. In spite of the common use of broadband Internet access 
nowadays, the visualization of a bequest of thousand of files 
is not a simple task. Even in JPEG file format all the archive 
consumes Giga bytes of space. A possible solution to this 
problem is to convert the images to black-and-white. 
Image thresholding or binarization [20] is subject of 
several research efforts. It is the first step in some image 
processing applications as optical character recognition 
(OCR). Threshold algorithms search for a point that separates 
object and background in an image: the threshold or cut-off 
value. It defines which colors belong to one or another class. 
In the case of images of documents these two classes are the 
paper (the background) and the ink (the foreground). A 
correct threshold value for this application is one that 
generates a final bi-level image with all the colors that belong 
to the ink turned to black and all the colors that belong to the 
paper turned to white. This is quite a simple task when one 
deals with recent documents where the paper is almost 
completely clear. This process however is not so easily done 
in images with low contrast. In these cases, image 
 
C.A.B.Mello is with the Department of Computing Systems, Polytechnic 
School, University of Pernambuco, Rua Benfica, 455, Madalena, Recife PE, 
Brazil, 50.720-001 (email:carlos@dsc.upe.br). 
L.A.Schuler: e-mail: laschuler@yahoo.com.br 
 
enhancement techniques could be used first to improve the 
visual appearance of the image for further thresholding. 
Another major problem is the definition of the features that 
are going to be analyzed in the search for the perfect threshold 
value. 
Images of historical documents present some unique features that 
make a binarization process very hard: 1) some documents are 
written on both sides of the paper and the ink from one side passes to 
the other side, creating a back-to-front interference (also known as 
ink-bleeding); 2) some paper sheets are very consumed and the 
paper has darkened over the time (making its color similar to the 
ink); 3) in some documents the ink has faded so much that it has 
almost the same color as the paper. Examples of these classes of 
documents are shown in Figure 1. This Figure also presents the 
results of their binarization using a commercial tool. 
   
   
Fig. 1. (top) Sample documents and (bottom) their bi-level versions: (left) a 
document with back-to-front interference; (center) a document with darkened 
paper and (right) a very faded document. 
 
Next Section discusses some of the researches being developed 
for image processing of historical documents. Classical thresholding 
algorithms are detailed in Section 3 of this paper. The new proposed 
method is explained in Section 4 and its results are analyzed in 
Section 5 which is followed by the Conclusions of the paper. 
II. IMAGE PROCESSING OF HISTORICAL DOCUMENTS 
Previous works related to image processing of historical 
documents can be found in literature. The problem of 
back-to-front interference is dealt in [28] where a canny edge 
detector is used to detect and to suppress undesired 
background patterns. The method is based on the direction of 
the writing considering that the angle of the writing in the 
foreground opposes the angle of the writing in the 
Tsallis Entropy-Based Thresholding Algorithm for Images of 
Historical Documents 
Carlos A.B.Mello, and Luciana A.Schuler
T 
11121-4244-0991-8/07/$25.00/©2007 IEEE
 
 
background. This approach, however, does not deal with 
horizontal and vertical lines as can be found in a letter "H" for 
example (even if it is handwritten). The same authors also 
propose a new method to work with ink bleeding using the 
matching of the images from both sides of the paper. A 
wavelet reconstruction process then iteratively enhances the 
foreground strokes and smears the interfering strokes [29]. 
In [14], the authors propose the use of a multi-stage 
thresholding where different algorithms are used in different 
stages of the complete process in order to create the best 
image possible. 
An algorithm for background normalization is proposed in 
[27] to decrease the background influence for further 
binarization. Unfortunately, the method is not adjusted for 
documents written on both sides of the paper. 
Chen and Leedham in [3] propose the use of a quadtree 
decomposition to break the image into sub-regions and to 
apply different thresholding algorithms. The choice for the 
best algorithm is done after a training process based on a set 
of documents from the Library of Congress, USA. 
Background removal is also considered in [12] and [1] as a 
first step for text segmentation of historical documents. 
A combination of global and local thresholding algorithms 
is presented in [10] where the first step is the binarization of 
the document using Iterative Global Thresholding (IGT). 
This method decreases the contrast of the image and it 
darkens the image for a further histogram stretch. After this, 
sub-areas n by n of the image are analyzed to verify if they 
have more black pixels than they should have. These areas are 
then processed again using IGT. The authors do not explain 
how the size of the sub-areas must be defined as this clearly 
changes the results of the algorithm. 
A new method is detailed in [4] which consists of five 
distinct steps: a pre-processing procedure using a low-pass 
Wiener filter, a rough estimation of foreground regions, a 
background surface calculation by interpolating neighboring 
background intensities, a thresholding by combining the 
calculated background surface with the original image and 
finally a post-processing step that improves the quality of text 
regions and preserves stroke connectivity. This algorithm is 
not set to documents with back-to-front interference. 
Several other projects are being developed world around 
with historical documents as object of study: 
• HUMI Project: The Humanities Media Interface (HUMI) 
Project is an initiative to launch a digital library, 
presenting a digital version of Gutenberg Bible. More 
information at its web site: http://www.humi.keio.ac.jp. 
• Glasgow University Emblem: The Glasgow University 
owns the Stirling Maxwell Collection of Emblems Books 
which is the largest emblem collection of the world. 
Additional information can be found at: 
http://www.emblems.arts.gla.ac.uk. 
• ARTFL Project: The Project for American and French 
Research on the Treasury of the French Language 
(ARTFL) has several collections of French texts and 
encyclopedias from the 16th century: 
http://humanities.uchicago.edu/orgs/ARTFL 
• Koninklijke Bibliotheek: Books and journals from the 
Netherlands (http://www.konbib.nl/index-en.html). 
• The Digital Scriptorium: As defined in its web site “The 
Digital Scriptorium is an image database of medieval and 
renaissance manuscripts, intended to unite scattered 
resources from many institutions into an international 
tool for teaching and scholarly research”: 
http://sunsite.berkeley.edu/Scriptorium 
• Cervantes Digital Library: A digital library dedicated to 
the work of Miguel de Cervantes: 
http://www.csdl.tamu.edu/cervantes/english/index.html 
• Madonne Project: A French initiative to use document 
image analysis techniques for the purpose of preserving 
and exploiting heritage documents: 
http://l3iexp.univ-lr.fr/madonne/ 
• George Washington’s Manuscripts: Digital database at 
the US Library of Congress containing historical 
manuscripts that were scanned from microfilm: 
http://memory.loc.gov/ammem/gwhtml/gwhome.html 
Classical thresholding algorithms were tested as it is presented in 
the next Section. None of them achieved satisfactory results when 
applied to our images. Because of this, a new algorithm is presented 
using the classical definition of Entropy and a variation proposed by 
C. Tsallis [30] adjusted for the images in analysis. 
III. GRAYSCALE THRESHOLDING ALGORITHMS 
Thresholding [20] is an important part of several image 
processing or pattern recognition applications. In our case, we 
search for means to classify the foreground (the ink) and the 
background (the paper). The main problem comes when the 
images have low contrast. This means that there exists a gray 
level interval of pixel intensities where it is hard to determine 
whether any pixel belongs to the foreground or the 
background region. To obtain a perfect separation, an 
appropriate threshold value must be defined. The gray levels 
below this value are classified as ink and the gray levels 
above are part of the paper. The threshold value is considered 
correct if all the essential information of the image is 
preserved.  
The oldest thresholding algorithms were based on simple 
features of the images or their histograms. The average value 
of the grayscale histogram is used as cut-off value in the 
thresholding by mean gray level algorithm [20]. Another 
algorithm is based on the percentage of black pixels desired 
[20]. For documents, in general, it is expected that 10% of the 
image belongs to the ink, the rest being part of the paper. So a 
percentage of black set to 10% achieves good results for the 
major part of the common documents. 
In two peaks algorithm, the threshold is found at the lower 
point between two peaks of the histogram [20].  
The authors in [25] classify thresholding algorithms based 
on histogram, entropy, maximization or minimization 
functions or fuzzy theory. 
Entropy [26] is a measure of information content. In 
Information Theory, it is assumed that there are n possible 
1113
 
 
symbols, s, which occur with probability p(s). The entropy 
associated with the source S of symbols is: 
∑
=
−=
n
i
ii spspSH
0
])[log(][)(         (1) 
where the entropy can be measured in bits/symbols.  
Five entropy-based segmentation algorithms are briefly 
described: Pun [21], Kapur et al [9], Li-Lee [15], Wu-Lu [31] 
and Renyi [24]. 
Pun’s algorithm [21] analyses the entropy of black pixels, 
Hb, and the entropy of the white pixels, Hw, bounded by the 
threshold t: 
∑
=
−=
t
i
ii spspHb
0
])[log(][  
∑
+=
−=
255
1
])[log(][
ti
ii spspHw         (2) 
The algorithm suggests that t is such that maximizes the 
function H = Hb + Hw.  
In [9], Kapur et al. defines a probability distribution A for 
the object and a distribution B for the background of the 
document image, such that: 
A: p0/Pt, p1/Pt, ..., pt/ Pt 
B: (pt+1)/(1 – Pt), (pt + 2)/(1 - Pt),..., p255/(1 – Pt) 
The entropy values Hw and Hb are as before with p[i] 
defined by these new distributions. The function Hw + Hb is 
maximized to find the threshold value t. 
Li-Lee algorithm [15] uses the minimum cross entropy 
thresholding, where the threshold selection is solved by 
minimizing the cross entropy between the image and its 
segmented version. 
The main idea of the Wu-Lu algorithm [31] is the use of the 
lower difference between the minimum entropy of the objects 
and the entropy of the background. This method is very useful 
in ultra-sound images which have few different contrast 
values. 
Renyi method [24] uses two probability distribution 
functions (one for the object and the other for the 
background), the derivatives of the distributions and the 
methods of Maximum Sum Entropy and Entropic 
Correlation. 
Other algorithms are based on the maximization or 
minimization of functions. Brink method [11] identifies two 
threshold values (T1 and T2), using Brink’s maximization 
algorithm. The colors below T1 are turned to black and the 
colors above T2 are turned to white. The values between T1 
and T2 are colorized analyzing the neighbors of the pixel. A 
25 by 25 area is analyzed and, if there is a pixel in this area 
which color is greater than T2, the pixel is converted to white. 
In Kittler and Illingworth algorithm [13], based on Yan’s 
unified algorithm [34], the foreground and background class 
conditional probability density functions are assumed to be 
Gaussian, but, in contrast to the previous method, the equal 
variance assumption is removed. The error expression can be 
interpreted also as a fitting expression to be minimized. 
Fisher method [2] consists in the localization of the 
threshold values between the gray level classes. These 
threshold values are found using a minimization of the sum of 
the inertia associated to the two classes. 
Otsu [19] suggests minimizing the weighted sum of 
within-class variances of the foreground and background 
pixels to establish an optimum threshold. The algorithm has 
its basis in the linear discriminant analysis. 
In a fuzzy set, an element x belongs to a set S with a grade 
membership us(x). This definition of fuzzy sets can be easily 
applied to the segmentation problem. Most of the algorithms 
use a measure of fuzziness which is a distance between the 
original graylevel image and the segmented one. The 
minimization of the fuzziness produces a most accurate 
binarized version of the image. We can cite three binarization 
algorithms that use fuzzy theory: C Means [7], Huang [6] and 
Yager [32]. 
As an adaptive algorithm, iterative selection [22] makes an 
initial guess of a threshold value which is refined by 
improving this value. The initial guess is the mean graylevel 
which separates two areas and the mean values of these areas 
are evaluated (Tb and To). A new estimative of the threshold is 
evaluated as (Tb + To)/2. The process is repeated using this 
new value of the threshold until no change is found in it in 
two consecutives steps. Another adaptive algorithm is the 
Ye-Danielsson [5] which is also implemented as an iterative 
thresholding. 
Figure 2 shows the results of the application of some of 
these algorithms on the sample documents presented in 
Figures 1-left and center. 
    
    
Fig. 2. Application of Brink, Huang, Pun and Otsu thresholding algorithms in 
(top) document of Figure 1-left and (bottom) document of Figure 1-center. 
IV. NEW THRESHOLDING ALGORITHM 
The first step is the evaluation of the image entropy, H. 
This is done using Equation 1 but with the logarithmic basis 
taken as the product of the image dimensions. As defined in 
[8], changes in the logarithmic basis do not alter the definition 
of the entropy. H is used to classify the documents. Its value 
defines three classes of documents: 
• H ≥ 0.28 (class 1): darkened documents or documents 
with more ink elements than expected (as the ones with 
back-to-front interference); 
• H ≤ 0.23 (class 2): documents with few parts of text 
where the ink has faded; 
1114
 
 
• 0.23 < H < 0.28 (class 3): common documents with none 
of the previous features. 
These classes were defined by observation over a set of 200 
documents representatives of the complete file. 
Figure 1-left and center present sample documents from 
class 1 (H = 0.32 and H = 0.29, respectively), while Figure 
1-right shows a document that belongs to the class 2 (H = 
0.22). Class 1 documents are going to be detailed further. 
These classes are adjustments from previous studies 
presented in [17] and they were tested in a set of more than 
500 documents from the three classes. This classical 
definition of entropy can be divided into parts as proposed in 
Pun’s algorithm. This division and a binarization algorithm 
based on both Hb and Hw is explored in [16]. However, this 
approach presented some problems when applied to darkened 
documents. To solve this problem for this class of documents 
and the others as well, another measure is used instead of 
Shannon’s entropy [26]. 
Tsallis entropy [30] has been considered a new information 
measure. It has been used in several image processing 
applications as Content Based Image Retrieval (CBIR) [23] 
and even thresholding [33][34]. According to Tsallis, an 
universal definition of entropy can be given by: 
1
)(1
)(
−
−
= ∑
α
α
α
i
ip
SH            (3) 
where p(i) is a probability as in the classical definition of 
entropy and α is a real parameter. When α tends to 1, Tsallis 
entropy reduces to Boltzmann-Gibbs entropy: 
∑−= i ipipSH ))(ln()()(  
Let t be the most frequent color in the grayscale image to be 
binarized and p(i) is the a priori probability of the gray value i 
to be present in the image. If the gray value of a pixel is below 
t, then it belongs to the foreground; otherwise, it belongs to 
the background. With this, the distribution of the probabilities 
of the foreground and background is p(i)/Pb(t) and p(i)/Pw(t), 
respectively, where: 
∑
=
=
t
i
b iptP
0
)()(   and  ∑
+=
=
255
1
)()(
ti
w iptP  
The a prioi Tsallis entropy for each distribution is: 
1
)/)((1
)( 0
−
−
=
∑
=
α
α
α
t
i
b
b
Pip
tH   
and                (4) 
1
)/)((1
)(
255
1
−
−
=
∑
+=
α
α
α
ti
w
w
Pip
tH  
The value of the α parameter is not defined by Tsallis. The 
authors in [33] present a study of how this parameter can 
affect the ideal threshold for an image. Based on the three 
classes of documents that characterize our file, we have 
defined empirically the following fixed values for the α 
parameter: 
• Class 1 (H ≥ 0.28): α = 0.3; 
• Class 2 (H ≤ 0.23): α = 0.04; 
• Class 3 (0.23 < H < 0.28): α = 0.05. 
 
With these values of α, it can be evaluated the values of Hαb 
and Hαw as presented in Equations 4. The threshold value is 
then th = Hαb + Hαw. The influence of this parameter in the 
specification of th is analyzed next. 
Consider the sample document of Figure 1-center (the 
darkened one). That document has Shannon’s entropy (H) 
equal to 0.29645, making it a member of the class 1. The most 
frequent color of that document (excluding the white tones as 
they do not belong to the document itself) is the gray value of 
129. So Hαb = 33.9826 and Hαw = 21.1915, and th = 55.1741 
which is the cut-off value. If α=0.5, for example, in this 
document, the threshold value would be 26 which would 
generate a very clear bi-level image. A α value of 0.1, as 
another example, produces th = 143 and the final image is 
almost completely black. 
Document of Figure 1-left (with back-to-front 
interference) has H = 0.32521 (class 1), most frequent color 
equals to 148, Hαb = 49.9015, Hαw =17.461 and th = 67.3625. 
For the document of Figure 1-right (where the ink has faded), 
H = 0.22886 (class 2), the most frequent color is 203, Hαb = 
143.6392, Hαw =40.7954 and th = 184.4346. The binarization 
of these three sample documents is shown in Figure 3. 
Although noise can still be seen in the image of Figure 3-right 
(the binarization of the document presented in Figure 1-right), 
this is the best bi-level image ever achieved for this document 
with an automatic algorithm. 
   
Fig. 3. Bi-level versions of sample documents of Figure 1 generated by the 
new algorithm. 
 
Documents of class 3 need a different approach. Before the 
binarization, they need to be preprocessed. Figure 4-left 
presents one of these documents. In general, typewritten 
letters have a great amount of characters but the ink is not as 
strong as in handwritten letters. In fact, part of the ink is 
always faded making harder the thresholding process. In 
order to binarize these images, we must at first use a square 
root filter to change the color distribution of the image. To 
apply the filter, the colors are normalized so that they go from 
0 to 1, instead of 0 to 255. The square root of each normalized 
pixel is evaluated and denormalized back to the normal color 
range from 0 to 255. In Figure 5, one can see a graph that 
represents the changes in the color distribution with the 
evaluation of the square root. It shows that the colors increase 
to clear tones more rapidly, making the image brighter. Figure 
4-center presents the result of the application of this 
non-linear filter to the image of Figure 4-left. 
1115
 
 
After the use of the square root filter, the main 
characteristics of the image changes and it must be analyzed 
again in the search of the correct α value. The process is the 
same as before with the search for the most frequent color, the 
evaluation of Shannon and Tsallis entropy. As the images are 
brighter than before, the only change is the value defined for 
the α parameter for class 2 documents: α decreases to 0.02 
now. Figure 4-right presents the binarization of a sample 
document from this class. 
 
   
Fig. 4. (left) Sample document from Class 3, (right) its filtered version using 
a non-linear square root filter and (right) its final binarized version. 
 
 
Fig. 5. The effect of the square root filter in the color distribution: both axis 
represents the colors of a grayscale palette; the dashed line represents a direct 
mapping of one color into itself; the continuous curve is the function y = 
square_root(x) evaluated over the normalized value of the colors. 
V. RESULTS 
The proposed algorithm was tested in a set of 500 images 
that are considered representative of the complete file. The 
results were considered very satisfactory. The only cases 
where it did not present good results were in documents with 
large figures and text. The presence of figures changes the 
probabilistic features of the documents and the defined 
settings do not work well. For these cases, the ThRoc 
algorithm [16] achieves better quality images. Figure 6 
presents some documents and the results after the 
thresholding by the new algorithm. 
In order to make a quantitative evaluation of the 
performance of the new algorithm, a “clean” image (with no 
background) was created for a set of 200 representative 
documents and compared to the images generated by the 
algorithm; the threshold value used to create this clean image 
is defined by visual inspection. This comparison is made 
using the concepts of: precision, recall, accuracy and 
specificity, defined based on the values of True Positive (TP), 
False Positive (FP), True Negative (TN) and False Negative 
(FN):  
• Precision = TP/(TP + FP); 
• Recall = TP/(TP + FN) 
• Accuracy = (TP + TN)/(TP + TN + FP + FN) 
• Specificity = TN/(FP + TN) 
 
   
   
Fig. 6. (top) More examples of documents and (bottom) their bi-level 
versions thresholded by the proposed algorithm. 
 
Based on these measures, a good algorithm must have: 
• Precision→1: which means FP→0, i.e., there were few 
mistakes in the classification of the paper elements; 
• Recall→1: meaning that FN→0 or there were few 
mistakes in the classification of the ink elements; 
• Accuracy→1:  (FP + FN)→0; there was no 
misclassification at all; 
• Specificity→1: indicating that FP→0 and every pixel 
that belongs to the paper were classified as that. 
 
A good algorithm must have all these measures tending to 
1 at the same time. Table 1 presents the average result for 
these four measures in a comparison between a set of 200 
documents binarized by the new proposed algorithm and their 
“clean” images generated manually. The average values of 
these measures for some classical algorithms are also shown. 
As one can see, the algorithm achieved very good results for 
the four measures. These results are better than the ones 
achieved by every other classical algorithm running over the 
same base of documents as can be seen in Table 1. Figure 6 
presents some more examples of the application of the 
algorithm. 
VI. CONCLUSIONS 
This paper presents a new entropy-based thresholding 
algorithm for images of historical documents. The algorithm 
uses both Shannon and Tsallis definition of entropy to find 
the best cut-off value. Some documents need to be 
pre-processed before the binarizing step. For this, a square 
root filter is used in order to change the features of the 
document and further thresholding. The algorithm was 
applied in a set of 50 representative images and its results 
1116
 
 
were analyzed by visual inspection and by comparison with 
perfect bi-level images. The values of precision, recall, 
accuracy and specificity were evaluated and the algorithm 
achieved satisfactory results. 
 
 
TABLE 1. AVERAGE VALUES OF PRECISION (P), RECALL (R), ACCURACY (A) 
AND SPECIFICITY (S) IN A SET OF 200 DOCUMENTS GENERATED BY THE NEW 
ALGORITHM AND CLASSICAL ALGORITHMS COMPARED WITH THEIR “CLEAN” 
VERSION GENERATED MANUALLY 
Algorithm P R A S 
New Algorithm 0.9206 0.7311 0.968 0.993 
Brink 0.9125 0.692 0.949 0.985 
C Means 0.878 0.792 0.935 0.986 
Fisher 0.953 0.513 0.729 0.990 
Huang 0.883 0.80 0.936 0.986 
Kittler 0.939 0.733 0.955 0.992 
Li-Lee 0 0.573 0.889 0.889 
Mean Grey Level 0.951 0.714 0.959 0.994 
Otsu 0.882 0.81 0.967 0.982 
Pun 0.945 0.693 0.931 0.994 
Renyi 0.879 0.766 0.935 0.987 
Two Peaks 0.871 0.824 0.953 0.986 
Wu-Lu 0.944 0.711 0.948 0.993 
Yager 0.99 0.168 0.384 0.908 
 
The new algorithm fails just in documents with big figures 
and text mixed (as post cards). This is acceptable as it is 
defined to work in letters and documents with just text (hand 
or typewritten). 
REFERENCES 
[1] A.Antonacopoulos, C.C.Castilla: “Flexible Text Recovery from 
Degraded Typewritten Historical Documents”, International 
Conference on Pattern Recognition, 2006, pp. 1062-1065, Japan. 
[2] M.S.Chang et al.: “Improved binarization algorithm for document 
image by histogram and edge detection”, ICDAR, 1995, Canada. 
[3] Y.Chen, G.Leedham: “Decompose algorithm for thresholding 
degraded historical document images”, Vision, Image and Signal 
Processing, 2005, vol. 152, no. 6, pp. 702-714. 
[4] B.Gatos et al.: “An adaptive binarisation technique for low quality 
historical documents”, IARP Workshop on Document Analysis 
Systems, 2004, Lecture Notes in Computer Science(3163), pp. 
102-113. 
[5] C.A.Glasbye: “An Analysis of Histogram-Based Thresholding 
Algorithm”, Graph.Models and Image Processing, 1993, vol. 55, no. 6. 
[6] L.K.Huang, M.J.Wang: “Image Thresholding by Minimizing the 
Measures of Fuzziness”, Pat.Recognition, 1995, vol 28, no 1. 
[7] C.Jawahar et al.: “Investigations on Fuzzy Thresholding Based on 
Fuzzy Clustering”, Pat.Recognition, 1997, vol 30, no 10. 
[8] J.N.Kapur: Measures of Information and their Applications, J.Wiley & 
Sons, 1994. 
[9] J.N.Kapur et al.: “A New Method for Gray-Level Picture Thresholding 
using the Entropy of the Histogram”, Comp Vision, Graphics and 
Image Proc., 1985, Vol 29, no 3. 
[10] E.Kavallieratou, E.Stamatatos: “Improving the Quality of Degraded 
Document Images”, International Conference on Document Image 
Analysis for Libraries, 2006, pp. 340-349, France. 
[11] S.W.Katz, A.D.Brink: “Segmentation of Chromosome Images”, IEEE 
COMSIG, 1993, pp. 85-90. 
[12] D.J.Kennard, W.A.Barrett: “Separating Lines of Text in Free-Form 
Handwritten Historical Documents”, International Conference on 
Document Image Analysis for Libraries, 2006, pp. 12-23, France. 
[13] J.Kittler, J.Illingworth: “Minimum Error Thresholding”, Pattern 
Recognition, 1986, vol. 19, no. 1, pp. 41-47. 
[14] G.Leedham et al.: “Separating Text and Background in Degraded 
Document Images - A Comparison of Global Thresholding Techniques 
for Multi-Stage Thresholding”, International Workshop on Frontiers in 
Handwriting Recognition, 2002, pp. 244-249, Canada. 
[15] C.H.Li, C.K.Lee: “Minimum Cross Entropy Thresholding”, Pattern 
Recognition, 1993, vol. 26, no 4. 
[16] C.A.B.Mello, A.L.I.Oliveira, A.Sanchez: “Image Thresholding of 
Historical Documents: Application to the Joaquim Nabuco's File”, 
Digital Cultural Heritage Conference - Eva Vienna, 2006, p. 115-122, 
Austria. 
[17] C.A.B.Mello, A.H.M.Costa: “Image Thresholding of Historical 
Documents Using Entropy and ROC Curves”, Lecture Notes in 
Computer Science, 2005, v. 3773, p. 905-916. 
[18] A.L.I.Oliveira et al.: “Optical Digit Recognition for Images of 
Handwritten Historical Documents”, Brazilian Symposium of Neural 
Networks, 2006,  p.29, Brazil. 
[19] N.Otsu: “A threshold selection method from gray-level histogram,” 
Transactions on.System,  Man, and Cybernetics, 1978, vol 8. 
[20] J.R.Parker: Algorithms for Image Processing and Computer Vision. 
John Wiley & Sons, 1997. 
[21] T.Pun: “Entropic Thresholding, A New Approach,” Computer 
Graphics and Image Processing, 1981, vol. 16. 
[22] T.W.Ridler, S.Calvard: “Picture Thresholding Using an Iterative 
Selection Method”, IEEE Transactions on System, Man and 
Cybernetics, 1978, Vol.SMC-8, 8. 
[23] P.S.Rodrigues et al.: “Using Tsallis Entropy into a Bayesian Network 
for CBIR”, International Conference on Image Processing, 2005, pp. 
1028-1031, Genova. 
[24] P.Sahoo et al.: “Threshold Selection using Renyi’s Entropy”, Pattern 
Recognition, 1997, vol. 30, no 1. 
[25] M.Sezgin et al.: “Survey over image thresholding techniques and 
quantitative performance evaluation”, Journal of Electronic Imaging, 
2004, vol. 13, no. 1. 
[26] C.Shannon: “A Mathematical Theory of Communication”. Bell System 
Technology Journal, 1948, vol. 27, pp. 370-423, 623-656. 
[27] Z.Shi, V.Govindaraju: “Historical Document Image Enhancement 
Using Background Light Intensity Normalization”, International 
Conference on Pattern Recognition, 2004, pp. 473-476, UK. 
[28] C.L.Tan et al.: “Removal of Interfering Strokes in Double-Sided 
Document Images”, Workshop on Applications of Computer Vision, 
2000, pp. 16-21, USA. 
[29] C.L.Tan et al.: “Restoration of Archival Documents Using a Wavelet 
Technique”, IEEE Transactions on Pattern Analysis and Machine 
Intelligence, 2002, vol. 24, no. 10, pp. 1399-1404. 
[30] C.Tsallis: “Possible Generalization of Boltzmann-Gibbs statistics”, 
Journal of Statistical Physics, 1988, vol. 52, nos. 1-2, pp. 479-487. 
[31] L.Wu et al.: “An Effective Entropic thresholding for Ultrasonic 
Images”, International Conference on Pattern Recognition, 1998, pp 
1552-1554, Australia. 
[32] R.R.Yager: “On the Measures of Fuzziness and Negation Part.1: 
Membership in the Unit Interval”, International Journal of General 
Systems, 1979, Vol. 5. 
[33] L.Yan et al.: “An Application of Tsallis Entropy Minimum Difference 
on Image Segmentation”, World Congress on Intelligent Control and 
Automation, 2006, pp. 9557-9561, China. 
[34] L.Yan et al.: “Image Segmentation based on Tsallis-entropy and Renyi 
entropy and Their Comparison”, International Conference on Industrial 
Informatics, 2006, pp. 943-948, Singapore. 
[35] H. Yan: “Unified Formulation of a Class of Image Thresholding 
Techniques”, Pattern Recognition, 1996, vol. 29. 
1117
