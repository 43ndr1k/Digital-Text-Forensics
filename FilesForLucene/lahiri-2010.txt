Measuring The Informality Of Web Documents
Shibamouli Lahiri
Computer Science and
Engineering
The Pennsylvania State
University
University Park, PA 16802
shibamouli@cse.psu.edu
Sumit Bhatia
Computer Science and
Engineering
The Pennsylvania State
University
University Park, PA 16802
sumit@cse.psu.edu
Prasenjit Mitra
College of Information
Sciences and Technology
The Pennsylvania State
University
University Park, PA 16802
pmitra@ist.psu.edu
ABSTRACT
Web 2.0 content is mostly created by the users and for the
users. Writing style variation is an important categorizer
of web documents. In this paper we propose a quantitative
measure of informal writing style. This measure applies to
a document and is based on the number of informal sen-
tences the document contains. An informality measure of
this kind helps language and style analysis, tonality detec-
tion and opinion mining. It can be used as part of verti-
cal search engines and focused crawlers. We follow a bi-
nary classification approach that selects informal sentences
from a document and computes its informality measure. We
have used supervised learning techniques to train our clas-
sifier and compared the proposed informality measure with
F-score and readability scores of a document. Finally, we
studied the effect of ranking documents based on its infor-
mality (colloquial-ness).
Categories and Subject Descriptors
I.2.7 [Artificial Intelligence]: Natural Language Process-
ing—Text analysis; H.3.1 [Information Storage and Re-
trieval]: Content Analysis and Indexing—Linguistic Pro-
cessing ; I.5.4 [Pattern Recognition]: Applications—Text
processing
General Terms
Algorithms, Experimentation, Human Factors, Languages,
Measurement, Performance
Keywords
Informality Measure, Colloquial Expressions, F-Score, Read-
ability Metric, Supervised Learning
1. INTRODUCTION
The last decade has been witness to an explosion of web
technologies. Billions of webpages are created, maintained
and searched daily by millions of users across the world.
With time, web has evolved from a mere collection of hy-
perlinked pages to content created by the users and for the
users. Social networks like Facebook, Twitter, LinkedIn and
Orkut, personal blogs created by users, photo-sharing web-
sites like Flickr and Photobucket are hallmarks of Web 2.0.
Today’s web search engines crawl and index all this diverse
Copyright is held by the author/owner(s).
WWW2010, April 26-30, 2010, Raleigh, North Carolina.
.
information and present search results to the end user in a
coherent form. The ranking of search results depends on the
query as well as the indexed documents. The documents can
be academic papers, news articles, web forum threads, stock
information, pages of digitized books, multimedia and so on.
Since they are of different types and characteristics, main-
taining consistency in ranking is an important challenge.
The problem is compounded by the fact that documents
use different styles and expressions. Academic documents
are written in formal style. On the other hand, forum posts
often use informal expressions. This heterogeneity of ex-
pressions indicates that classes of documents pertaining to
similar styles should come from similar sources. Since aca-
demic papers and forum posts are two different categories
of documents, it makes sense to index them separately. One
way to achieve this separation is to analyze the document
content and style. If we can quantify the style of a particular
document, then we will be able to place the document in a
separate class. Moreover, if the style measure is query and
topic-independent, then we can rank each document on an
absolute scale.
Just like there are style variations in documents, there is
variation of choice among users. Two users may present the
same query to a search engine and yet they might be look-
ing for totally different class of search results. For example,
one user may try the query “Friday the 13th” to see movie
reviews and another user may try the same query to see the
origin of this expression. Since the two types of information
need are very different, so should be the search results. In
an ideal world, the first user will be presented with movie
reviews only and the second user will be given articles re-
lated to etymology of the term. Although in reality this
separation is not possible without knowing the context and
background of a particular query, we can better the situa-
tion by providing both users with two subsets of results -
a relatively informal subset and a relatively formal subset.
We can expect that movie reviews should be more informal
than academic articles, and should be placed in the first cat-
egory. On the other hand, the etymological articles should
be placed in the second category. This categorization re-
quires a style-based ranking, and even when the binary cat-
egorization is not desired, we can show the style measure to
the user along with search results to enable him/her pick up
the particular choice of style.
The same argument holds for crawling as well. Mod-
ern web search engines often employ focused crawlers [8]
for topic-oriented retrieval of webpages. Focused crawling
can be achieved using context graphs, online relevance feed-
back, ontologies of specific domains, etc [12, 7, 13]. We can
think of expanding the current notion of focused crawling
with the idea of style-based retrieval, where documents of a
particular class of writing style will be the crawler’s target.
A potential consumer of this “style crawler” can be vertical
and domain-specific search engines. Since web 2.0 content
is created by the users and searched by the users, the style
variation reflected in different kinds of documents is com-
plemented by the fact that people may want to look for
different writing styles at different time. In order to quan-
tify the style of a document in terms of formality, we need
to be able to automatically distinguish informal expressions
from formal ones. This problem naturally boils down to a
problem of binary classification, where we want to classify
expressions as formal or informal (colloquial). However, as
we describe later in this paper, whether an expression is in-
formal or not depends on human judgment, time, place and
context. Since language understanding is a hard issue, we do
not always expect a precise boundary between informal and
formal expressions. This leads us back to the idea of ranking
documents in terms of informality, where instead of a hard
boundary between formal and informal documents, we are
trying to place them on a uniform scale of measurement.
Informal style detection helps in some other areas as well.
A first step in sentiment analysis and opinion mining is look-
ing for discriminative words and expressions concerning a
particular topic or view. We can restrict this search within
informal expressions, if we can ensure that the document is
mostly written in an informal style. This will expedite the
detection of positive, negative, neutral and other kinds of
opinions by only focusing on informal words and expressions.
Similar arguments hold for sentiment analysis and sarcasm
mining, where the goal is to pinpoint the author’s sentiment
behind writing a particular piece of text. We can also think
of the measure of informality as a first step towards estimat-
ing the tonality of a document, e.g. whether an email is rude
or nice. Finally, there is a more general and harder prob-
lem - conversion between formal and informal expressions.
Formal to colloquial conversion can be used in designing a
customized chatbot or an intelligent query-response system,
where the replies are given informally according to the con-
text. This helps achieve a form of personalization of the sys-
tem. On the other hand, colloquial to formal conversion is
very useful in speech recognition and automatic transcript
generation, especially when we are dealing with imprecise
and possibly incomplete utterances. Detection of informal
style in documents is the starting point for this problem.
This paper is organized as follows. In Section 2, we ar-
rive at a quantitative measure of informality of a particular
document. Next we show the need for a supervised learning
approach to derive this measure automatically from a given
document. We also formulate a precise problem statement
in this section. In Section 3, we describe the dataset used
in training the classifiers and then we discuss the features in
some detail and challenges encountered in selecting a suit-
able feature space. Section 4 gives the experimental results
and evaluation. We provide the related work in Section 5
and conclude in Section 6.
2. PROBLEM FORMULATION
2.1 An Informality Measure
We want to define a measure of informality as a starting
point. This quantitative criterion will guide us through the
whole study. Theoretically it is possible to have a measure of
informality for every word, sentence, paragraph, document
or document collection. That is also a good idea because
not only will such a measure allow us to rank each semantic
unit (word, sentence, paragraph, etc) on a uniform scale,
but they will also enable us to combine the scores of smaller
units into that of a larger unit. This type of measure can
be obtained using soft classification or ranking of the corre-
sponding unit. However, as we observed in this study, words
without context information often do not provide sufficient
cues as to detect whether they are informal or not. In this
regard, words act as chemical atoms that do not have a sep-
arate identity outside molecules.
On the other side of the spectrum, paragraphs and doc-
uments are composed of sentences. Sentences act as stand-
alone units (much like chemical molecules). Their informal-
ity is readily apparent even without looking at the context.
In fact, we failed to observe any significant performance im-
provement when we took context information into account
in sentence classification. Also, informality of a paragraph
or a whole document can be readily derived if we know the
style of each sentence. This leads us to a much simpler char-
acterization of informality measure. We define “Informality
Measure” (IM) of a document as
IM =
#C
#T
(1)
where #C denotes total number of informal sentences in
the document and #T denotes total number of sentences in
the document. Note that this measure can vary between 0
and 1, and can be applied to any piece of text, provided
it is composed of sentences. We can therefore think of an
informality measure for a book, an article, a movie review,
an academic paper, etc. This measure effectively allows us
to gauge the style of any kind of text documents.
2.2 Fundamental Challenges
The main problem in assessing the informality measure
of a particular document is that automatic language under-
standing is a very hard issue. It is difficult to say (sometimes
even for a human being) accurately whether a sentence is
informal or not. Akmajian et al [3] pointed out, “a precise
definition of slang seems extremely difficult”. It depends on
at least four factors - human judgment, time, place and con-
text. The sentence “For all the stars in the sky, I don’t care”
seems informal, but “For all the stars in the sky, Doppler
Shift is an important physical phenomenon” is a formal sen-
tence. The word “ace” in U.K. slang means “brilliant”, but
there is no such adjective usage in the U.S.1.
Time plays an important role in determining whether an
expression is informal or not. With time, a colloquial ex-
pression may either fall into disuse (e.g. “cabbage” mean-
ing “currency”) or become part of standard language (e.g.
“bootleg” and “crush”)[2, 1]. The effect of context is best
explained with the example “Black beetles”. It may either
refer to a particular species of beetle (attagenus unicolor)
in some biology document, or one of Captain Haddock’s fa-
vorite curses. Here we need to know the context (biology
or Tintin) before deciding whether or not the expression
“Black beetles” is colloquial. Finally, depending on human
judgment, some expressions might be informal while others
1http://www.effingpot.com/slang.shtml
are not. For example, the sentence “I think the test wasn’t
hard” may be considered informal by some people and stan-
dard by others. These difficulties are so fundamental that
we are forced to negotiate a workable path of informality
detection. First of all, we had to admit that there is no way
that a system composed of hard-coded rules will always be
able to detect all informal sentences, simply because all the
possible variations and exceptions are almost impossible to
capture as rules. In other words, we were not able to come
up with a colloquialism grammar that will work for all sit-
uations. This aspect is no different than trying to design a
formal grammar for the English language. With this lesson,
we resorted to supervised learning approach for detection of
informal expressions.
2.3 Problem Statement
Under supervised learning setting, our problem is to clas-
sify each sentence in a document as informal (colloquial) or
formal (non-colloquial). Then we assign the document an
informality measure according to (1).
This approach is more tractable in the sense that we do
not have to deal with all possible idiosyncrasies of the lan-
guage. At the same time, the inherent ambiguity and un-
certainty associated with any informality-related decision is
quantified on a statistical background. This decision is im-
portant, because superficially it might seem that detecting
colloquialisms can be much faster and more accurate using
an ad hoc rule-based system. We may assume that collo-
quial expressions can be detected using a grammar checker
(for grammatical inconsistencies), a slang dictionary and a
few rules. However, as we observed, this approach fails to
give good performance. On the other hand, machine learn-
ing based techniques are more robust. They do not impose a
hard boundary between formal and informal sentences based
on a few rules. The classification boundary can also be tuned
according to what performance measure we want to give
more importance to.
3. DATASET AND FEATURE EXTRACTION
3.1 The Training Dataset
Since our problem is cast in a supervised learning setting,
we need a suitable dataset to train our classifiers. For this
study, we have selected the Splog Blog Dataset2. We took 50
documents from this dataset, pre-processed them and man-
ually annotated each document. Note that the goal of our
annotation is to label each sentence as informal or formal.
Although this annotation depends heavily on the four fac-
tors mentioned in Section 1 (human judgment, time, place
and context), we strived to reduce ambiguity and maintain
consistency in the annotation.
An important decision in this regard is to exclude meta-
data like feed links, related post links, syndication infor-
mation and post titles from consideration. Part of a blog
page with metadata marked in red ellipses, is shown in Fig-
ure 1. We do not really want to annotate this information,
because most of the times they have no connection with
the author’s writing style. Moreover, the metadata of this
kind remains invariant across pages from the same blog. So
taking it into consideration blurs the discriminative aspects
2http://ebiquity.umbc.edu/resource/html/id/212/Splog-
Blog-Dataset
Figure 1: Blog Metadata.
of a document with respect to informality. Similar ideas
apply when dealing with other kinds of documents. For ex-
ample, academic papers contain metadata like title, author
information, date and venue of publication. They also con-
tain document-elements like figures, tables, algorithms, etc.
These are not relevant to informality detection and must be
removed before sentences are classified as formal or informal.
After the pre-processing step, our training dataset con-
sisted of 7488 sentences. Among these sentences, we anno-
tated 3303 as informal and 4185 as formal. All the docu-
ments were annotated by the same observer to ensure con-
sistency. The annotator labelled each sentence as formal
or informal, depending on sentence structure, visual cues,
grammatical aspects, presence of slang terms and the con-
text in which the sentence is placed. The main difficulty
our annotator faced while determining whether a sentence
is informal or not, is mostly due to grammatical aspects and
context information.
3.2 Feature Extraction
As we observed in the annotation phase, labeling a sen-
tence as formal or informal is hard because it depends on
many factors. It is out of question to design an ad hoc rule-
based system, because rules may have exceptions and they
can not effectively capture context information. Even if we
consider only grammatical rules, it becomes readily appar-
ent that we need at least an exponentially large number of
rules to model all possible grammatical inconsistencies. A
much cleaner approach is given by machine learning tech-
niques where we extract features from the documents and
train a classifier based on labelled documents. We extracted
15 binary features based on four categories - Grammatical,
Visual, Constructional and Miscellaneous. The unit of clas-
Figure 2: Frequency of sentences where a feature
has value 1.
sification is a sentence, so all these features are sentence-level
features. The following is an enumeration of these features
along with example sentences for which the feature value is
1.
1. Exclamation mark at the end of a sentence
If a sentence ends with an exclamation mark, then this
feature is given value 1. Otherwise, it is 0.
Example sentence: “It’s a really sunny morning!”
2. Uncapitalized beginning of a sentence
If a sentence begins with lowercase character, this fea-
ture is given value 1. Otherwise, it is 0.
Example sentence: “i wanna have my breakfast.”
3. Improper beginning of interrogative sentence
Formal interrogative sentences start with wh-words or
verbs like “do”, “is”, “are”, “could”, “should”, “would”,
etc (Table 1). This feature is given value 1 for inter-
rogative sentences starting with anything other than
these words.
Example sentence: “Mind to take this seat?”
4. Smiley
Smileys or emoticons are important visual cues regard-
ing the informality of a sentence. This feature is given
value 1 if a sentence contains a smiley. Otherwise, it
is 0.
Example sentence: “Cool job, dude :)”
5. Asterisk
If a sentence contains a word with an asterisk in it,
this feature is given value 1. Otherwise, it is 0.
Example sentence: “Holy s**t.”
6. Apostrophed contractions
Contractions like “it’s”, “we’ve”, “can’t”, “there’s”,
etc are not appropriate in formal writing. If a sentence
contains such words, this feature is given value 1.
Example sentence: “You can’t do this.”
7. A word with all letters separated by hyphens
If a sentence contains words where letters are sepa-
rated by hyphens, this feature is given value 1.
Example sentence: “We are going to enjoy another h-
o-l-i-d-a-y.”
8. Sentence in uppercase
Fully capitalized sentences denote loud outcry. They
may come from advertisements, or as an expression
(long shout or wail). For fully capitalized sentences,
this feature is given value 1. Otherwise, it is 0.
Example sentence: “LONG LIVE, FABIO.”
9. Words in uppercase
Fully capitalized words denote stress or emphasis by
the author. For sentences containing such words, this
feature is given value 1.
Example sentence: “Can YOU hear me?”
10. Repetition of letters in a word
Words with a particular letter heavily repeated, denote
emphasis. For sentences containing such words, this
feature is given value 1.
Example sentence: “It was a baaaaaaaad day.”
11. Repetition of words in a sentence
Sentences with a particular word heavily repeated, de-
note stress or emphasis. For such sentences, this fea-
ture is given value 1.
Example sentence: “It was really really really fun.”
12. Word equations
Equations like “Black + Gold = Pittsburgh” are in-
formal expressions. This feature is given value 1 for
sentences containing such equations.
Example sentence: “Duke + UNC + NCSU = Re-
search Triangle.”
13. Short sentences
This feature is given value 1 for sentences with 3 or
less words.
Example sentence: “There you go.”
14. Placeholder characters
Characters acting as placeholders for conventional apos-
trophes give rise to contractions like “don-t”, “he:s”,
etc. If a sentence contains such placeholders, this fea-
ture is given value 1.
Example sentence: “It?s a waste of time.”
15. Grammar checker
If a sentence is flagged by the grammar checker as syn-
tactically inconsistent, then this feature is given value
1. Otherwise, it is 0. We used the LanguageTool open
source grammar checker3 by Daniel Naber.
Example sentence: “Before rushing into a purchase,
make sure that it does not get repossessed if somehow
your marketing is fruitless at first.”
The categorization of the above 15 features is presented
in Table 2. Grammatical features capture syntactic incon-
sistencies of a sentence. Visual features are visual cues re-
garding informality of a sentence. Constructional features
depend on sentence and word structure. Finally, Miscella-
neous category contains all the remaining features. Since
features are binary, we can plot the number of sentences
for which a feature has value 1. Figure 2 shows this plot.
3http://www.languagetool.org/
Table 1: Interrogative Words
Word Type Examples
Wh-words what, why, where, when, how, who, whom, whose, which
Do-verbs do, does, did
Have-verbs have, has, had
Be-verbs am, is, are, was, were
Modal verbs shall, will, can, may, should, would, could, might, must
Table 2: Categorization of Features
Feature Category Feature Number
Grammatical 2, 3, 15
Visual 1, 4, 5, 6, 7, 14
Constructional 8, 9, 10, 11, 13
Miscellaneous 12
The blue bars indicate the number of informal sentences
where a feature has value 1, and the red bars indicate the
number of formal sentences where the feature has value 1.
This plot is important for several reasons. Note that the
features 1, 2, 6, 9, 13, 14 and 15 are given value 1 for a
larger number of sentences than other features. This indi-
cates that those features are more common across different
types of sentences. The second observation concerns the
height difference between blue and red bars. Most of the
features have higher blue bars, which signifies the fact that
these features indeed possess discriminative aspects regard-
ing formality/informality of sentences. However, since most
features do have red bars, an ad hoc rule-based system (con-
sidering the features as rules) is unlikely to work very well.
In the following section, we briefly describe the supervised
learning algorithms explored in this study.
3.3 Learning Algorithms
We explored five different supervised learning algorithms
and compared their performance across the training dataset.
In this section we briefly describe Linear and Quadratic Dis-
criminant Analysis, Logistic Regression, Naive Bayes Clas-
sifier and Support Vector Machine (SVM).
3.3.1 Linear Discriminant Analysis
In Bayesian approach, classification is carried out by MAP
(Maximum A Posteriori) rule, given by
Ĝ(x) = argmaxkPr(G = k|X = x)
By Bayes’ Theorem, Pr(G = k|X = x) ∝ fk(x)πk, where
πk denotes prior probability for the class k, and fk(x) de-
notes likelihood for the class k. In Linear Discriminant Anal-
ysis (LDA), likelihood functions fk(x) are assumed to be
Gaussian:
fk(x) =
1
(2π)p/2|Σk|1/2 e
− 12 (x−µk)T Σ
−1
k
(x−µk)
with Σk = Σ ∀k. The function
δk(x) = [x
T Σ−1µk − 1
2
µTk Σ
−1µk + log(πk)]
is called “linear discriminant function” (linear in x) and
the classification rule is given by Ĝ(x) = argmaxk δk(x).
The estimated parameters are as follows:
π̂k = Nk/N
µ̂k = Σgi=kx
(i)/Nk
Σ̂ = ΣKk=1Σgi=k(x
(i) − µ̂k)(x(i) − µ̂k)T /(N −K)
3.3.2 Quadratic Discriminant Analysis
Like LDA, Quadratic Discriminant Analysis (QDA) also
deals with Gaussian likelihood functions, but here we have
different covariance matrix Σk for each class.
The discriminant function is given by
δk(x) = [−1
2
log|Σk| − 1
2
(x− µk)T Σ−1(x− µk) + log(πk)]
This function is quadratic in x. It requires estimation of
k covariance matrices, where k is the number of classes.
3.3.3 Logistic Regression
The Bayesian boundary between classes k and l is given
by the equation
Pr(G = k|X = x) = Pr(G = l|X = x)
Rearranging and taking log of both sides,
log
Pr(G = k|X = x)
Pr(G = l|X = x) = 0
In logistic regression, the right hand side of the above
equation is taken to be a linear function of x, so the poste-
rior probabilities are
Pr(G = k|X = x) = e
βk0+β
T
k x
1 + ΣK−1l=1 e
βl0+β
T
l
x
for k = 1, ..., K − 1, and
Pr(G = K|X = x) = 1
1 + ΣK−1l=1 e
βl0+β
T
l
x
We estimate β using iteratively reweighted least squares(IRLS)
algorithm.
3.3.4 Naive Bayes Classifier
Naive Bayes (NB) Classifier is a probabilistic classification
model based on Bayes’ Theorem. Mathematically, a naive
Bayes model can be expressed as
P (C|F1, . . . , Fn) = 1
Z
P (C)
n∏
i=1
P (Fi|C)
where F1 to Fn are features, C represents class labels,
and Z is a constant dependent on F1 to Fn. Intuitively, this
Table 3: Classifier Performance
Classifier Precision Recall F1 Accuracy
LDA 0.764 0.642 0.698 0.754
QDA 0.441 1.0 0.612 0.441
Logistic Regression 0.765 0.64 0.697 0.755
Naive Bayes 0.724 0.652 0.686 0.737
SVM 0.765 0.639 0.696 0.754
model gives us the posterior probability of a class label con-
ditioned on feature values as the product of two functions -
the prior probability of the class label and the conditional
probability of features given a class label. The implicit as-
sumption is independence of features. So this model has
been known as “independent feature model”. It has been
successfully used in document classification, spam detection,
Bayesian inference and predictive analytics.
3.3.5 Support Vector Machine
Support Vector Machines (SVM) constitute a family of
sparse kernel classifiers. The essence of kernel machines
lies in the fact that they project data onto a higher dimen-
sional space. The goal of this projection is to find out a
linear boundary between two classes. SVM tries to iden-
tify a boundary of maximum margin. The kernel function
is essentially a dot product of two vectors. By taking the
dot product, lower dimensional vectors are projected onto
higher dimensions. This is known as the “Kernel Trick”.
SVM gives state-of-the-art performance for many data min-
ing problems. For our implementation, we used the SVM-
Light package by T. Joachims [20].
4. EXPERIMENTAL EVALUATION
This section is divided into three broad subsections. In
the first subsection, we describe the results on the train-
ing dataset and evaluate the performance of sentence clas-
sification. In the second subsection, we detail the results
of comparison between our informality measure and other
readability metrics. The final subsection deals with ranking
documents according to informality measure.
4.1 Classification Performance
As mentioned previously, we experimented with five differ-
ent supervised learning algorithms and observed their per-
formance on the training dataset. For all these experiments,
we did 5-fold cross validation. Table 3 shows the results.
Here precision denotes the ratio of informal sentences classi-
fied as informal to the total number of sentences classified as
informal. Similarly, recall denotes the ratio of informal sen-
tences classified as informal to the total number of informal
sentences. F1 is the harmonic mean of precision and recall.
Accuracy is given by the ratio of total number of correct
classifications to total number of sentences in the dataset.
To make notations cleaner, from now on we will adopt the
following convention:
True Positive (TP) = Number of informal sentences clas-
sified as informal, False Positive (FP) = Number of formal
sentences classified as informal, True Negative (TN) = Num-
ber of formal sentences classified as formal, False Negative
(FN) = Number of informal sentences classified as formal,
so that
Precision (P) =
TP
TP + FP
Recall (R) =
TP
TP + FN
F1 =
2 ∗ Precision ∗Recall
Precision + Recall
Accuracy (A) =
TP + TN
TP + TN + FP + FN
In terms of accuracy and F1, we observe that Logistic
Regression, LDA and SVM performed better than others.
For QDA, we obtained a high recall at the cost of very low
precision.
In our next experiment we tuned the cost factor parame-
ter of SVM to observe its impact on performance. The cost
factor is a parameter by which training errors on positive
examples outweigh errors on negative examples. Its default
value in SVM-Light is 1. We varied cost factor from 0.1
to 4 in steps of 0.1, and measured precision, recall, F1 and
accuracy at each step with 5 fold cross validation. Figure 3
shows the corresponding variation in performance. Preci-
sion decreases as cost factor increases, and recall shows the
opposite trend. When cost factor is too low, precision is
almost 1. On the other hand, when cost factor is too high,
recall is about 1. Figure 3 shows a balance between precision
and recall in the middle range, where values of F1 and accu-
racy are also the highest. In our case, F1 is more important
than accuracy because we are not much interested in cor-
rectly classifying formal sentences (true negatives). Rather,
we want to maximize the combination of precision and re-
call, which are important metrics for detection of informal
sentences. F1 serves as combined precision and recall, and
we want to find out an optimal F1. In this case, the optimal
F1 (0.7) was found at a cost factor of 2.2. Corresponding
precision and recall values are 0.763 and 0.645. Since we
obtained the best precision-recall combination at this con-
figuration, for all succeeding experiments we chose to use
this setting.
Our feature set comprises 15 dimensions. It is of inter-
est to observe which dimensions give the best performance
when taken alone. In other words, we wanted to know the
individual feature performance under the same conditions.
For this experiment, SVM cost factor was taken as 2.2 and
5-fold cross validation was performed separately on each of
the 15 dimensions. Figure 4 shows the results. For each fea-
ture number there are four columns representing precision,
recall, F1 and accuracy, respectively. The first observation is
high accuracy bars for all the features. Features with good
discriminative aspects especially stand out. The explana-
tion of these features are given in Section 3.2 and Table 2.
However, precision and recall differ only by a small amount,
with recall slightly exceeding precision in most of the cases.
4.2 Informality Measure
In the previous section, we described the performance of
sentence classification. Classifying sentences as formal or
informal is the first step towards calculating the informality
measure of a document. Since we defined informality mea-
sure as a ratio of two non-negative integers (Section 2.1),
its value can vary from 0 to 1. When there are no informal
sentences in a given document, this measure is 0. When
all sentences in a document are informal, this measure is 1.
Figure 3: Variation of performance with cost factor.
Figure 4: Impact of individual features on perfor-
mance.
This notion of quantifying formality draws a parallel to the
approach given by Heylighen et al [19]. The contextuality
score (F-score) proposed in their work attempts to measure
the formality and contextuality of a document using part-of-
speech information. Although this approach is simpler, we
believe that quantifying informality is harder than empirical
measurements, because it involves semantics.
Our proposed informality measure can also be used as a
readability metric. Since we are concerned only with for-
mality/informality, a relatively informal document may re-
flect the need for improving language. To assess whether or
not standard readability tests and our informality measure
are correlated, we performed a series of experiments involv-
ing popular readability tests like Gunning fog Index, SMOG
test, etc. For these experiments, we took 100 blog posts, 100
news articles, 100 academic papers and 100 newsgroup doc-
uments. For each document, we computed the informality
measure, the F-score (Heylighen et al) and six readability
metrics (given below). Note that the computation of in-
formality measure required us to pre-process all these doc-
uments (Section 3.1), represent each sentence as a feature
vector (Section 3.2) and finally classify the sentences. After
classification, we follow Equation (1) to determine informal-
ity measure. Below we give a short description of F-score
along with the readability metrics. Most of the readability
metrics give an approximate idea of U.S. school grade level
required to understand a piece of text. For the following
discussion, wordCount, sentenceCount, characterCount and
syllCount refer to the number of words, sentences, charac-
ters and syllables in a document. ComplexCount denotes
the number of words with 3 or more syllables.
• F-score is defined as (freq of nouns + freq of adjec-
tives + freq of prepositions + freq of articles - freq of
pronouns - freq of verbs - freq of adverbs - freq of in-
terjections + 100)/2. Each frequency is a percentage
with respect to all the words in a given document. This
score can vary from 0 to 100, with higher scores asso-
ciated with more formal documents. The score was
derived by taking into account the relative frequencies
of different parts-of-speech present in formal and con-
textual documents. Computing this score requires a
part-of-speech tagger. We have used CRFTagger[27]
for our experiments.
• Flesch Reading Ease Score (FRES) [14] is defined
as
206.835− 1.015 ∗ wordCount
sentenceCount
− 84.6 ∗ syllCount
wordCount
Lower values indicate documents that are more diffi-
cult to read.
• Automated Readability Index (ARI) is defined as
4.71 ∗ characterCount
wordCount
+
0.5 ∗ wordCount
sentenceCount
− 21.43
• Flesch-Kincaid Readability Test (FKRT) is de-
fined as
0.39 ∗ wordCount
sentenceCount
+
11.8 ∗ syllCount
wordCount
− 15.59
• Coleman-Liau Index (CLI) [10] is defined as
5.89 ∗ characterCount
wordCount
− 30.0 ∗ sentenceCount
wordCount
−15.8
• Gunning fog Index (GFI) [16] is defined as
0.4 ∗ ( wordCount
sentenceCount
+
100.0 ∗ complexCount
wordCount
)
• SMOG (Simple Measure of Gobbledygook)[25] is de-
fined as
√
complexCount ∗ 30.0
sentenceCount
+ 3.0
To ensure a good mix of documents across which the above
scores make sense, we collected 100 blog posts, 100 news ar-
ticles, 100 academic papers and 100 newsgroup documents.
These datasets are described below:
• Blog Dataset
We collected most recent posts from the top 100 blogs
listed by Technorati4, a leading blog search engine.
Since Technorati changes its ratings every day, we col-
lected all documents on a single day (October 31, 2009).
The posts encompass several different topics. The
dataset consists of 2110 sentences.
• News Article Dataset
We collected 100 news articles from 20 news sites (5
from each). These articles are mostly from “Breaking
News”, “Recent News” and “Local News” categories.
There is no specific preference to any of the categories.
The news sites we used are CNN, CBS News, ABC
News, Reuters, BBC News Online, New York Times,
Los Angeles Times, The Guardian (U.K.), Voice of
America, Boston Globe, Chicago Tribune, San Fran-
cisco Chronicle, Times Online (UK), news.com.au, Xin-
hua, The Times of India, Seattle Post Intelligencer,
Daily Mail and Bloomberg L.P. The dataset consists
of 3009 sentences.
• Academic Paper Dataset
We collected 100 papers from the CiteSeerX5 digital
library. The papers encompass different topics. This
dataset consists of 161406 sentences.
• Newsgroup Document Dataset
For the newsgroup documents, we used Twenty News-
group Dataset6. It is a sizeable collection of newsgroup
documents categorized under 20 topics. We took 5
documents from each topic to build our dataset. The
dataset consists of 9179 sentences.
Table 4 shows the average value of the above metrics
across four datasets. Note that newsgroup documents have
the highest average informality, followed by blog posts. News
4http://www.technorati.com
5http://citeseerx.ist.psu.edu
6http://people.csail.mit.edu/jrennie/20Newsgroups/
Figure 5: Coleman-Liau Index vs Informality Mea-
sure.
articles have a much lower informality, and the lowest in-
formality comes with academic papers. These results intu-
itively make sense, because people are more likely to use
formal expressions and sentences in academic papers than
in blogs. Another interesting observation is how F-score and
the other readability scores maintained a consistent relation-
ship across different datasets. For example, F-score is high-
est for papers and lowest for news articles. For newsgroup
documents, however, we obtained a quite high F-score. This
aspect is explained by the fact that F-score depends cru-
cially on part-of-speech information, while informality mea-
sure does not suffer much from this problem.
We investigated the correlation between informality mea-
sure, F-score and other metrics. In Figures 5 and 6, we show
two scatter plots of the 400 documents (blog posts, news ar-
ticles, newsgroup documents and academic papers). X-axis
denotes Informality Measure and Y-axis denotes Coleman-
Liau Index (Figure 5) and SMOG score (Figure 6). As
observed from these two plots, there does not seem to be
good correlation between readability measures and infor-
mality measure. For the Coleman-Liau case, as informality
increases, CLI does not show a corresponding decrease. It
remains fairly stable. On the other hand, Figure 6 shows no
correlation at all. Other readability measures as well as F-
score showed a trend similar to Figure 5. We cannot include
all these plots due to space constraints, but in general we
did not observe significant correlation between informality
measure and other scores.
This observation leads us to conclude that informality
measure picks up data traits unobserved in other types of
metrics. Especially because the readability scores as well as
F-score are hard (static) formulae, they are unlikely to gain
significant advantage when visual features and context fea-
tures play an important role. It is even more apparent that
due to the machine learning approach adopted in estimat-
ing informality score, we are not placing a hard boundary
between a formal and an informal sentence. This view is
crucial because a sentence might appear to be informal in
one context and formal in another. The problem of gra-
dient well-formedness [18] dictates that a sentence may fall
somewhere in between “completely grammatical” and “com-
Table 4: Average Informality Measure, F-score and Readability Metrics
Dataset Informality F-score FRES ARI FKRT CLI GFI SMOG
Blog 0.4370 65.2452 61.0410 11.6291 9.4748 11.4261 13.8266 12.0286
News 0.3451 66.5103 56.2142 13.1261 10.7821 12.4727 15.4975 13.4615
Academic Paper 0.3393 68.6186 48.4125 15.8608 12.6177 14.2031 18.0052 15.1517
Newsgroup Document 0.4601 62.2790 82.8180 5.6799 3.9193 8.2483 8.9297 8.4035
Figure 6: SMOG Score vs Informality Measure.
pletely ungrammatical”. The hard scores are practically of
no use when dealing with cases like this. We do not claim to
have fully solved this problem either, but the lack of correla-
tion between informality measure and other scores indicates
that there is room for further investigation. Si and Callan
[30] have drawn similar conclusions in their work.
4.3 Informality Ranking
In this experiment we took top 10 documents returned by
Google as a result of the query “Thanksgiving Holiday” and
re-ranked them according to informality measure. These
documents come from websites as diverse as Wikipedia and
The History Channel. Table 5 shows the results. Note that
the informality rank is assigned in the descending order of
informality measure. Table 5 gives two ideas. First of all,
it conclusively shows the ranking of documents by informal-
ity. Second, it also shows that document ranking in search
engines may actually be helped by informality measure. If
search engines provide this ranking (or the measure itself)
along with each document, users will be better served to find
their style of choice. Again, the informality rank may even
be more useful. Observe that the top informal document
here comes from holidays.net. Somebody looking for differ-
ent ways to celebrate Thanksgiving holidays will actually be
better served by going to this page than by looking at the
Wikipedia article (the top result returned by Google).
5. RELATED WORK
The area of genre and author identification deals with
writing styles. Santini [29] gives a comprehensive account.
Rosso in [28] and Kanaris et al in [21] give user-centric and
ngram-based approaches for web genre identification. An
important recent study by De Assis et al [11] shows a genre-
aware approach to focused crawling. In this paper, we have
proposed a similar idea: focused crawling based on infor-
mality. Writing style identification is another area where
we need to take care of many challenging semantic aspects.
McCarthy et al [24] have shown a tool (Coh-Metrix) that
analyzes text on over 200 measures of cohesion and diffi-
culty.
Emotion analysis and opinion mining are two other ar-
eas that deal with writing styles. Mao et al [22] show how
tricky it is to track local sentiment flow in documents, and
devise a new CRF model. We also faced many difficulties
when working with words and expressions rather than sen-
tences. Yang et al [32] classify blog sentiments using CRF
and SVM. Godbole et al [15] show ways of doing large scale
sentiment analysis on news and blogs. [4] is a good refer-
ence for emotion detection. Style-based text categorization
[5] is another related area. Work on spam detection and
noisy post detection also make use of visual cues like smi-
leys, shouts (capitalized words), etc [23, 31]. These cues are
informal expressions and they represent an important part
of the feature set used in our experiments. But our goal is
different in the sense that we are trying to classify sentences
using these features, whereas spam filtering techniques filter
out posts containing these expressions.
Identification of spoken language variations has been ex-
plored in different languages. Among these, Haraty et al
describes a speech recognition system that identifies varia-
tions in spoken Arabic [17]. Chi-Shun et al [9] gives an unsu-
pervised Markovian approach for classifying Chinese expres-
sions into content and filler phrases. Pereltsvaig and Asya
discuss techniques for handling split phrases in Russian col-
loquial expressions [26]. Gupta in [6] has given contact fea-
tures of Singapore colloquial English (SCE) and commented
that many scholars have taken SCE as “erroneous”. This
again confirms our point that whether an expression is col-
loquial or not, depends on location.
6. CONCLUSION
In this paper we have described a novel method of mea-
suring informality of web documents based on informal sen-
tence detection using machine learning. We have compared
the proposed informality measure with readability metrics
and F-score. Finally we have shown an example ranking
of web documents according to this measure. This ranking
is query-independent, but it can give superior results when
users are looking for less formal documents.
7. REFERENCES
[1] Slang of the 1920.
http://local.aaca.org/bntc/slang/slang.htm.
[2] Wikisaurus:money - Wiktionary.
http://en.wiktionary.org/wiki/Wikisaurus:money.
[3] A. Akmajian, A. K. Farmer, R. M. Harnish, and R. A.
Table 5: Ranking According to Informality
Google Rank Page Source Informality Informality Rank
1 Wikipedia 0.143 7
2 Wikipedia 0.225 4
3 history.com 0.167 5
4 theholidayspot.com 0.154 6
5 theholidayspot.com 0.067 8
6 gonewengland.about.com 0.238 3
7 holidays.net 0.269 1
8 sfgate.com 0 9 = 10
9 theholidayzone.com 0.256 2
10 holidays.kaboose.com 0 9 = 10
Demers. Linguistics: An Introduction to Language and
Communication. MIT Press, Cambridge, 2001.
[4] C. O. Alm, D. Roth, and R. Sproat. Emotions from
text: machine learning for text-based emotion
prediction. In Proceedings of the Human Language
Technology Conference and the Conference on
Empirical Methods in Natural Language Processing
(HLT/EMNLP), 2005.
[5] S. Argamon-Engelson, M. Koppel, and G. Avneri.
Style-based text categorization: What Newspaper Am
I Reading? In Proceedings of the AAAI Workshop on
Text Categorization, pages 1–4, 1998.
[6] K. Bolton and H. Kwok, editors. Sociolinguistics today
: international perspectives. Routledge, London, New
York, 1992.
[7] S. Chakrabarti, K. Punera, and M. Subramanyam.
Accelerated Focused Crawling through Online
Relevance Feedback, 2002.
[8] S. Chakrabarti, M. van den Berg, and B. Dom.
Focused crawling: a new approach to topic-specific
Web resource discovery, 1999.
[9] C. Chi-Shun and P. Fung. Unsupervised learning of a
chinese spontaneous and colloquial speech lexicon
with content and filler phrase classification: Special
double issue on chinese spoken language technology.
International Journal of Speech Technology,
7(2-3):173+.
[10] M. Coleman and T. L. Liau. A computer readability
formula designed for machine scoring. Journal of
Applied Psychology, 1975.
[11] G. T. De Assis, A. H. Laender, M. A. Gonçalves, and
A. S. Da Silva. A Genre-Aware Approach to Focused
Crawling. World Wide Web, 12(3):285–319, 2009.
[12] M. Diligenti, F. Coetzee, S. Lawrence, C. Giles, and
M. Gori. Focused Crawling Using Context Graphs. In
In 26th International Conference on Very Large
Databases, VLDB 2000, pages 527–534, 2000.
[13] M. Ehrig and A. Maedche. Ontology-Focused
Crawling of Web Documents, 2003.
[14] R. Flesch. A new readability yardstick. Journal of
Applied Psychology, 32(3):p221 – 233, June 1948.
[15] N. Godbole, M. Srinivasaiah, and S. Skiena.
Large-Scale Sentiment Analysis for News and Blogs.
In Proceedings of the International Conference on
Weblogs and Social Media (ICWSM), 2007.
[16] R. Gunning. The technique of clear writing.
McGraw-Hill, 1968.
[17] R. A. Haraty and O. E. Ariss. CASRA+: A colloquial
arabic speech recognition application, 2007.
[18] B. Hayes. Gradient well-formedness in Optimality
Theory, 1997.
[19] F. Heylighen and J. marc Dewaele. Variation in the
Contextuality of Language: An Empirical Measure. In
Context in Context, Special issue of Foundations of
Science, pages 293–340, 1998.
[20] T. Joachims. Making Large-scale SVM Learning
Practical. Advances in Kernel Methods - Support
Vector Learning, pages 169–184, 1999.
[21] I. Kanaris and E. Stamatatos. Webpage Genre
Identification Using Variable-Length Character
n-Grams. Tools with Artificial Intelligence, IEEE
International Conference on, 2:3–10, 2007.
[22] Y. Mao and G. Lebanon. Isotonic Conditional Random
Fields and Local Sentiment Flow. In Advances in
Neural Information Processing Systems, 2007.
[23] L.-E. Marvin. Spoof, Spam, Lurk and Lag: the
Aesthetics of Text-based Virtual Realities. J.
Computer-Mediated Communication, 1(2), 1995.
[24] P. M. McCarthy, G. A. Lewis, D. F. Dufty, and D. S.
McNamara. Analyzing Writing Styles with
Coh-Metrix. In FLAIRS Conference, pages 764–769,
2006.
[25] H. G. McLaughlin. SMOG grading - a new readability
formula. Journal of Reading, pages 639–646, May
1969.
[26] Pereltsvaig and Asya. Split phrases in colloquial
Russian. Studia Linguistica, 62(1):5–38, April 2008.
[27] X.-H. Phan. CRFTagger: CRF English POS Tagger,
2006.
[28] M. A. Rosso. User-based identification of Web genres.
J. Am. Soc. Inf. Sci. Technol., 59(7):1053–1072, 2008.
[29] M. Santini. State-of-the-Art on Automatic Genre
Identification, 2004.
[30] L. Si and J. Callan. A statistical model for scientific
readability. In In Proc. of CIKM, pages 574–576, 2001.
[31] W. Weerkamp and M. de Rijke. Credibility Improves
Topical Blog Post Retrieval. In Proceedings of
ACL-08: HLT, pages 923–931, Columbus, Ohio, June
2008. Association for Computational Linguistics.
[32] C. Yang, K. H.-Y. Lin, and H.-H. Chen. Emotion
Classification Using Web Blog Corpora. In WI ’07:
Proceedings of the IEEE/WIC/ACM International
Conference on Web Intelligence, pages 275–278,
Washington, DC, USA, 2007. IEEE Computer Society.
