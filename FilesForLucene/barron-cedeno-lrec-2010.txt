Corpus and Evaluation Measures for Automatic Plagiarism Detection
Alberto Barrón-Cedeño1, Martin Potthast2, Paolo Rosso1, Benno Stein2
1Natural Language Engineering Lab — ELiRF
Department of Information Systems and Computation
Universidad Politécnica de Valencia, Spain
http://users.dsic.upv.es/grupos/nle
{lbarron, prosso}@dsic.upv.es
2 Web Technology and Information Systems
Faculty of Media / Media Systems
Bauhaus-Universität Weimar, Germany
http://www.webis.de
{martin.potthast, benno.stein}@uni-weimar.de
Abstract
The simple access to texts on digital libraries and the WWW has led to an increased number of plagiarism cases in recent years, which
renders manual plagiarism detection infeasible at large. Various methods for automatic plagiarism detection have been developed
whose objective is to assist human experts to analyze documents for plagiarism. Unlike other tasks in natural language processing
and information retrieval, it is not possible to publish a collection of real plagiarism cases for evaluation purposes since they cannot
be properly anonymized. Therefore, current evaluations found in the literature are incomparable and often not even reproducible. Our
contribution in this respect is a newly developed large-scale corpus of artificial plagiarism and new detection performance measures
tailored to the evaluation of plagiarism detection algorithms.
1. Introduction
Plagiarism is the use of text written by a third party
in one’s own writing without permission or acknowledg-
ment (Clough, 2000). The goal of automatic plagiarism de-
tection is to identify the plagiarized sections in a suspicious
document dq . Two approaches exist to tackle this task: in-
trinsic plagiarism detection and external plagiarism detec-
tion.
In intrinsic plagiarism detection, features that indicate writ-
ing style are used to detect style irregularities caused by the
insertion of text from a different author into dq . The writing
style of a text can be quantified, by measuring a text’s read-
ability, vocabulary richness, or by the use of basic statis-
tics, such as the average sentence length and the average
word length (Meyer zu Eißen and Stein, 2006). Other ap-
proaches apply character n-grams profiles to characterize
an author’s style and search for irregularities across dq (Sta-
matatos, 2009).
External plagiarism detection has attracted more attention
because of its close relation to information retrieval. A doc-
ument dq and a collection of potential source documents D
are given, and the task is to identify the plagiarized sections
in dq (if there are any), and their respective source sections
in D (Potthast et al., 2009). Two issues render this task dif-
ficult: the number of potential source documents, |D|, and
the fact that plagiarizing a text often includes paraphrasing,
summarizing, and sometimes even translation.
To deal with these problems it has been proposed to com-
pile D into a fingerprint index using text fingerprinting
This work has been partially supported by the TEXT-
ENTERPRISE 2.0 TIN2009-13391-C04-03 research project as
well as the CONACyT-Mexico 192021 grant.
schemes, such as SPEX (Bernstein and Zobel, 2004) and
Winnowing (Schleimer et al., 2003). The index then can be
queried with dq’s fingerprint in order to retrieve documents
from D with overlapping or near-duplicate contents. How-
ever, since D is often considered to be the whole Web, a
more practical solution is the automatic retrieval of a small
number of candidate source documents using a Web search
engine, and to compare them with dq on the basis of the
vector space model (Broder, 1997; Maurer et al., 2006).
We observe in this connection that the evaluation of plagia-
rism detection algorithms is not standardized, i.e., most of
the time the algorithms are evaluated on homemade corpora
using various different performance measures.1 This situa-
tion renders the existing research almost incomparable. The
contributions of the paper in hand address this problem. We
propose:
1. a plagiarism corpus, called PAN-PC-09 (Section 2.)
2. tailored performance measures based on the idea of
precision, recall, and granularity (Section 3.)
The corpus and the measures form the first controlled eval-
uation environment dedicated to plagiarism detection. They
were used, among others, in the First International Compe-
tition on Plagiarism Detection (Section 4.). Final remarks
are provided in Section 5.
1To our knowledge, the only corpus which can be used to eval-
uate plagiarism detection is the METER corpus (Clough et al.,
2002); it consists of a small number of annotated cases of text
reuse in news articles but is not specifically designed to support
plagiarism detection evaluation.
771
2. The PAN-PC-09 Plagiarism Corpus
The PAN Plagiarism Corpus (PAN-PC-09) is a collection
of 41 223 documents in which 94 202 artificial plagiarism
cases have been inserted.2 It is the first corpus which al-
lows for large-scale evaluations of both intrinsic and exter-
nal plagiarism detection methods. During its construction
a number of parameters have been varied so that the corpus
features a wide cross-section of different plagiarism cases:
• Document Length. 50% of the documents are small
(1-10 pages), 35% medium (10-100 pages), and
15% large (100-1,000 pages).
• Suspicious-to-Source Ratio. 50% of the documents
are designated as suspicious documents Dq , and
50% as source documents D.
• Plagiarism Percentage. The percentage of plagiarism
per suspicious document dq ∈ Dq ranges from 0%
to 100% (cf. Figure 1). In order to compose a realistic
framework, 50% of the suspicious documents contain
no plagiarism at all.
• Plagiarism Length. The length of a plagiarism case is
uniformly distributed between 50 and 5,000 words.
• Plagiarism Languages. 90% of the cases are mono-
lingual English plagiarism. The remaining 10% are
cross-language plagiarism, i.e., the source document
is written either in German or in Spanish, and the pla-
giarism is translated into English.
• Plagiarism Obfuscation. The monolingual portion of
the plagiarism in the external test corpus is obfuscated.
The degree of obfuscation ranges uniformly from none
to high.
With respect to plagiarism obfuscation further explanations
are necessary. Plagiarists often paraphrase or summarize
the text they plagiarize in order to obfuscate it, i.e., to hide
their offense. A synthesizer, that simulates the obfuscation
of a section of text sx in order to generate a different text
section sq to be inserted into dq , has been designed on the
basis of the following basic operations:
• Random Text Operations. Given sx, sq is created by
shuffling, removing, inserting, or replacing words or
short phrases at random.
• Semantic Word Variation. Given sx, sq is created
by replacing each word by one of its synonyms, hy-
ponyms, hypernyms, or even antonyms.
• POS-preserving Word Shuffling. sq is created by shuf-
fling words while maintaining the original sequence of
parts of speech in sx.
These operations do not guarantee the generation of human-
readable text. However, automatic text generation is still
a largely unsolved problem which is why we have ap-
proached the task from the basic understanding of content
similarity in information retrieval, namely the bag-of-words
model.
2http://www.webis.de/research/corpora
5 755025 100%
15%
7%
Percentage of Plagiarism per Document
P
ct
. o
f D
oc
um
en
ts

Figure 1: Distribution of plagiarism per document.
As mentioned before, a weakness of corpora containing ac-
tual cases of plagiarism is that they cannot be published
due to ethical reasons. One of the aims of the PAN-PC-09
corpus was to avoid such weaknesses. Therefore, suspi-
cious and source documents were generated on the basis of
22 874 documents from Project Gutenberg.3 To the best of
our knowledge, such documents are public domain. Hence,
the PAN-PC-09 corpus is available free of charge for re-
search purposes.
3. Evaluation Measures
In order to evaluate the performance of a plagiarism detec-
tion algorithm, precision and recall cannot be applied di-
rectly. For intrinsic plagiarism detection it is necessary to
evaluate if a plagiarized section has been properly identi-
fied as such. Additionally, for external plagiarism detection
it is also necessary to evaluate if the source section has been
accurately retrieved.
Let dq be a document including plagiarized sections; dq de-
fines a sequence of characters labeled as plagiarized or non-
plagiarized. A plagiarized section s forms a contiguous se-
quence of characters in dq . The set of all plagiarized sec-
tions in dq is denoted by S; the plagiarized sections do not
intersect, i.e., ∀si, sj ∈ S : i 6= j → (si ∩ sj = ∅). Like-
wise, the set of all sections r ⊂ dq found by a plagiarism
detection algorithm is denoted by R. See Figure 2 for an
illustration.
original characters
plagiarized characters
detected characters  yydocument as character sequenceSR  yr1 r3r2   yyr5r4s1 s3s2Figure 2: A document as character sequence, including pla-giarized sections S and detections R returned by a plagia-rism detection algorithm.
The plagiarized sections are treated as basic retrieval units.
In this sense, each si ∈ S defines a query for which a pla-
giarism detection algorithm returns a result set Ri ⊆ R.
The recall of a plagiarism detection algorithm, recPDA, is
3http://www.gutenberg.org
772
Reference
collection D
Heuristic
retrieval
Detailed
analysis
Plagiarism Detection
Suspicious
sections
Candidate
documents
Knowledge-based
post-processing
dq
Figure 3: Retrieval process of external plagiarism detection, derived from (Stein et al., 2007).
then defined as the mean of the returned fractions of the
plagiarized sections, averaged over all sections in S:
recPDA(S, R) =
1
|S|
∑
s∈S
|s ⊓
⋃
r∈R
r|
|s|
,
where ⊓ computes the positionally overlapping characters.
However, the precision of a plagiarism detection algorithm
is not defined under this view, which is rooted in the fact
that a detection algorithm does not return a unique result
set for each plagiarized section s ∈ S but for the whole
of S. This deficit can be resolved by switching the refer-
ence basis. Instead of the plagiarized sections S, the al-
gorithmically determined detections R become the targets:
the precision with which the queries in S are answered is
then measured as the recall of R under S. By computing the
mean average over the r ∈ R one obtains a definite com-
putation rule that captures the concept of retrieval precision
for S:
precPDA(S, R) =
1
|R|
∑
r∈R
|r ⊓
⋃
s∈S
s|
|r|
.
recPDA and precPDA are insensitive to the number of
times an s ∈ S is detected in a detection result R, i.e.,
the granularity of R. We define the granularity of R for a
set of plagiarized sections S by the average size of the ex-
isting covers: a detection r ∈ R belongs to the cover Cs
of an s ∈ S iff s and r overlap. Let SR ⊆ S denote the
set of cases so that for each s ∈ S : |Cs| > 0. Then, the
granularity of R given S is defined as:
granPDA(S, R) =
1
|SR|
∑
s∈SR
|Cs| ,
where Cs = {r | r ∈ R ∧ s ∩ r 6= ∅} and SR = {s |
s ∈ S ∧ ∃r ∈ R : s ∩ r 6= ∅}. The domain of the
granularity is [1, |R|], where 1 marks the desirable one-to-
one correspondence between R and S, and |R| marks the
worst case, when a single s ∈ S is detected over an over
again.
In order to allow for an absolute ranking among plagiarism
detection algorithms, the three measures are combined to
an overall score:
overallPDA(S, R) =
F
log
2
(1 + granPDA)
,
where F denotes the F -Measure, i.e., the harmonic mean
of the precision prec
PDA
and the recall recPDA. We take
the logarithm of the granularity to smooth its influence on
the overall score.
4. Competition on Plagiarism Detection
The PAN-PC-09 corpus was formerly constructed for em-
ployment in the First International Competition on Plagia-
rism Detection (Potthast et al., 2009).4 Here, 13 research
teams from all over the world submitted detection results
obtained with a variety of different plagiarism detection al-
gorithms. 10 teams attempted to solve the external task,
4 teams competed in the intrinsic task, and one of them in
both tasks.
In the intrinsic plagiarism detection task unexpected vari-
ations through a text were measured in order to determine
whether a document contained plagiarized fragments. Such
variations were analysed on the basis of character n-grams
profiles (Stamatatos, 2009), word frequency class and text
frequencies (Zechner et al., 2009), and Kolmogorov com-
plexity measures (Seaward and Matwin, 2009). The former
strategy was applied by the winner in this subtask.
For the external plagiarism detection task, all systems were
based on common approaches, following the three-stage
plagiarism detection process illustrated in Figure 3. The
teams carried out the heuristic retrieval on the basis of the
vector space model using character-16-grams (Grozea et
al., 2009) or word-1-grams (Kasprzak et al., 2009), word-
5-grams (Basile et al., 2009), or word-8-grams (Zechner et
al., 2009). Only one team approached the task on the ba-
sis of fingerprinting techniques (Scherbinin and Butakov,
2009). The detailed analysis was carried out by search-
ing for exact matches of character-n-grams (Grozea et al.,
2009; Kasprzak et al., 2009), or word-n-grams (Basile et
al., 2009), and sentences (Zechner et al., 2009). The win-
ning approach of this task, as well as the overall winner of
the competition was the team of Grozea et al. (2009).
Interestingly, no team tried to detect cross-language pla-
giarism; presumably because this type of plagiarism detec-
tion is still in its infancy and has attracted attention only
recently (Potthast et al., 2010 in press).
4For further reading on the competition and the results ob-
tained by the participants see http://www.webis.de/research/
workshopseries/pan-09/competition.html
773
5. Conclusions
The PAN-PC-09 corpus is the first standardized corpus ded-
icated to the evaluation of automatic plagiarism detection
and was successfully employed in the First International
Competition on Plagiarism Detection. We believe that our
corpus and the performance measures will become an effec-
tive means to evaluate future plagiarism detection research.
Currently, an improved version of the corpus is being con-
structed. This corpus will be used in the Second Interna-
tional Competition on Plagiarism Detection, held in con-
junction with the evaluation conference CLEF 2010.5
6. References
Chiara Basile, Dario Benedetto, Giampaolo Caglioti, and
Mirko Degli Esposti. 2009. A Plagiarism Detection Pro-
cedure in Three Steps: Selection, Matches and Squares.
In SEPLN 2009 Workshop on Uncovering Plagiarism,
Authorship, and Social Software Misuse (PAN 09) (pan,
2009), pages 19–23.
Yaniv Bernstein and Justin Zobel. 2004. A Scalable Sys-
tem for Identifying Co-Derivative Documents. In Pro-
ceedings of the Symposium on String Processing and In-
formation Retrieval, pages 55–67. Springer.
Andrei Z. Broder. 1997. On the Resemblance and Contain-
ment of Documents. In Compression and Complexity of
Sequences (SEQUENCES’97), pages 21–29. IEEE Com-
puter Society.
Paul Clough, Robert Gaizauskas, and Scott Piao. 2002.
Building and Annotating a Corpus for the Study of Jour-
nalistic Text Reuse. In Proceedings of the 3rd Interna-
tional Conference on Language Resources and Evalua-
tion (LREC 2002), volume V, pages 1678–1691, Las Pal-
mas, Spain.
Paul Clough. 2000. Plagiarism in Natural and Program-
ming Languages: an Overview of Current Tools and
Technologies. Research Memoranda: CS-00-05, De-
partment of Computer Science. University of Sheffield,
UK.
Cristian Grozea, Christian Gehl, and Marius Popescu.
2009. ENCOPLOT: Pairwise Sequence Matching in Lin-
ear Time Applied to Plagiarism Detection. In SEPLN
2009 Workshop on Uncovering Plagiarism, Authorship,
and Social Software Misuse (PAN 09) (pan, 2009), pages
10–18.
Jan Kasprzak, Michal Brandejs, and Miroslav Krip̌ač.
2009. Finding Plagiarism by Evaluating Document Sim-
ilarities. In SEPLN 2009 Workshop on Uncovering Pla-
giarism, Authorship, and Social Software Misuse (PAN
09) (pan, 2009), pages 24–28.
Hermann Maurer, Frank Kappe, and Bilal Zaka. 2006. Pla-
giarism - A Survey. Journal of Universal Computer Sci-
ence, 12(8):1050–1084.
Sven Meyer zu Eißen and Benno Stein. 2006. Intrinsic pla-
giarism detection. Advances in Information Retrieval:
Proceedings of the 28th European Conference on IR Re-
search (ECIR 2006), LNCS (3936):565–569.
2009. San Sebastian, Spain. CEUS-WS.org.
5For further details, confer http://pan.webis.de as
well as http://www.clef2010.org
Martin Potthast, Benno Stein, Andreas Eiselt, Alberto
Barrón-Cedeño, and Paolo Rosso. 2009. Overview of
the 1st International Competition on Plagiarism Detec-
tion. In SEPLN 2009 Workshop on Uncovering Plagia-
rism, Authorship, and Social Software Misuse (PAN 09)
(pan, 2009), pages 1–9.
Martin Potthast, Alberto Barrón-Cedeño, Benno Stein, and
Paolo Rosso. 2010 (in press). Cross-Language Plagia-
rism Detection.
Vladislav Scherbinin and Sergey Butakov. 2009. Using
Microsoft SQL Server Platform for Plagiarism Detec-
tion. In SEPLN 2009 Workshop on Uncovering Plagia-
rism, Authorship, and Social Software Misuse (PAN 09)
(pan, 2009), pages 36–37.
Saul Schleimer, Daniel S. Wilkerson, and Alex Aiken.
2003. Winnowing: Local Algorithms for Document Fin-
gerprinting. In Proceedings of the 2003 ACM SIGMOD
International Conference on Management of Data, New
York, NY. ACM.
Leanne Seaward and Stan Matwin. 2009. Intrinsic Plagia-
rism Detection Using Complexity Analysis. In SEPLN
2009 Workshop on Uncovering Plagiarism, Authorship,
and Social Software Misuse (PAN 09) (pan, 2009), pages
56–61.
Efstathios Stamatatos. 2009. Intrinsic Plagiarism Detec-
tion Using Character n-gram Profiles. In SEPLN 2009
Workshop on Uncovering Plagiarism, Authorship, and
Social Software Misuse (PAN 09) (pan, 2009), pages 38–
46.
Benno Stein, Sven Meyer zu Eissen, and Martin Potthast.
2007. Strategies for Retrieving Plagiarized Documents.
In Charles Clarke, Norbert Fuhr, Noriko Kando, Wessel
Kraaij, and Arjen de Vries, editors, 30th Annual Interna-
tional ACM SIGIR Conference, pages 825–826. ACM.
Michael Zechner, Markus Muhr, Roman Kern, and Michael
Granitzer. 2009. External and Intrinsic Plagiarism De-
tection Using Vector Space Models. In SEPLN 2009
Workshop on Uncovering Plagiarism, Authorship, and
Social Software Misuse (PAN 09) (pan, 2009), pages 47–
55.
774
