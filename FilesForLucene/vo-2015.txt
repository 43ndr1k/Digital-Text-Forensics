Expert Systems with Applications 42 (2015) 1684–1698Contents lists available at ScienceDirect
Expert Systems with Applications
journal homepage: www.elsevier .com/locate /eswaLearning to classify short text from scientific documents using topic
models with various types of knowledgehttp://dx.doi.org/10.1016/j.eswa.2014.09.031
0957-4174/ 2014 Elsevier Ltd. All rights reserved.
⇑ Corresponding author.
E-mail addresses: thuanvd@gmail.com (D.-T. Vo), okcy@ulsan.ac.kr (C.-Y. Ock).Duc-Thuan Vo, Cheol-Young Ock ⇑
School of Electrical Engineering, University of Ulsan, 93 Daehak-ro, Nam-gu, Ulsan 680-749, Republic of Korea
a r t i c l e i n f o a b s t r a c tArticle history:
Available online 28 September 2014
Keywords:
Data sparseness
Information retrieval
Latent Dirichlet Allocation
Short text classification
Topic modelClassification of short text is challenging due to data sparseness, which is a typical characteristic of short
text. In this paper, we propose methods for enhancing features using topic models, which make short text
seem less sparse and more topic-oriented for classification. We exploited topic model analysis based on
Latent Dirichlet Allocation for enriched datasets, and then we presented new methods for enhancing
features by combining external texts from topic models that make documents more effective for
classification. In experiments, we utilized the title contents of scientific articles as short text documents,
and then enriched these documents using topic models from various types of universal datasets for clas-
sification in order to show that our approach performs efficiently.
 2014 Elsevier Ltd. All rights reserved.1. Introduction
Text classification has been used for the important task of auto-
matically sorting a set of documents into predefined categories
where each document belongs to one or multiple classes. The task
has several applications such as indexing documents, clustering
web searches, filtering spam, and classifying products. With regard
to long documents, several researches (Cancedda, Gaussier, Goutte,
& Renders, 2003; Joachims, 2002; Ma, Shepherd, & Nguyen, 2003;
Nigam, Lafferty, & McCallum, 1999; Sebastiani, 2002) based on
bag-of-words features combining with methods of clustering,
matching and ranking. Nigam et al. (1999) applied Max-Entropy
to measure similarities between documents, while Joachims
(2002) successfully demonstrated classifying documents by term
frequency-inverse document frequency (tf-idf) using Support Vec-
tor Machine (SVM). These approaches focused on feature enrich-
ment regarding the correlation of words occurring in documents.
Recently, there has been an increase of short text documents which
are different from long documents in terms of their length and data
sparseness, which hinders the application of conventional machine
learning and text mining algorithms. In fact, short text documents
are limited in both word occurrences and context shares that cause
failures in classifying the sparse data accurately.
With the rapid increase in the number of scientific documents
such as proceedings papers and journals, classification of these
documents is a constant task for assigning them to one or morecategories. In particular, titles of scientific documents are often
brief. They contain succinct attributes about specialty in statistical
reports, references, and citations. Short length and poor informa-
tive content lead to weak linkage to certain topics in these docu-
ments for classifying. Additionally, due to the diversification of
language, the same topic can be expressed in different ways; this
reduces the possibility of a feature term occurrence in several dif-
ferent short texts. As a result, the accuracy of short text classifica-
tion based on a feature term co-occurrence is often reduced due to
data sparseness. Classifying scientific documents based on their
titles as short text is challenging due to the data sparseness of
the occurrence of words. This discourages the determining or clas-
sifying of such documents.
Recently, research on the classification of short text has focused
on two methods, combining external text into target documents
and utilizing user-defined topics. First, combining external text
into target short text for classification (Banerjee, Ramanathan, &
Gupta, 2007; Long, Chen, Zhu, & Zhang, 2012; Metzler et al.,
2007; Sahami & Heilman, 2006) that enhance features in docu-
ments impacting classifying. The limitations of these researches
are that they are difficult to guarantee external text definitions
effectively. Second, the methods utilize user-defined topics to
adapt them into short text documents that make the relevant
among sparse words. These researches have achieved high accu-
racy in using a universal dataset for user-defined topics. For
instance, Phan et al. (2011) and Chen, Jin, and Shen (2011) pro-
posed a successful topic model analysis (Blei, Andrew, & Jordan,
2003) from a large web corpus of Wikipedia in order to cluster
short text from web searching and advertising.
D.-T. Vo, C.-Y. Ock / Expert Systems with Applications 42 (2015) 1684–1698 1685Using topic models from universal datasets, this paper proposes
a method to classify scientific documents based on titles. We con-
sider the enhancement of features in short text documents using
topic models from various universal datasets, and then to apply
machine learning techniques for classification. The main contribu-
tions of this work are as follows:
- We propose a framework to classify short text documents; we
focus on classifying scientific documents based on their titles,
which appear as short text.
- We use three types of universal datasets as Computer Science
Bibliography1 (DBLP), Lecture Notes in Computer Science2 book
series (LNCS), and Wikipedia3 to analyze topic models for enrich-
ing features in short text documents.
- We propose two ways to enhance features in short text by (1)
assigning topics from topic models as external features, and
(2) combining external texts of adapted topics as external
features.
- We apply machine learning algorithms as SVM, Naive bayes and
kNN for classification. We consider comparisons of the classifi-
cation performance of short text documents using three types
of universal datasets.
In experiments, we carefully use datasets with six types of
sub-categories related to Computer Science domain from scientific
documents. Those documents are enriched from three types of uni-
versal datasets based on topic models to gain efficiency in the clas-
sification performance. The rest of this paper is organized as
follows. Section 2 presents related work on text classification. Sec-
tion 3 describes a probabilistic topic model in which we focus on
LDA. In Section 4, we present a general framework for short text
classification. Section 5 discusses about the topic model analysis
using three types of universal datasets. Section 6 describes meth-
ods for enhancing features in short text documents. In Section 7,
experiments are applied to short text documents with six sub-
categories related to the Computer Science domain for classifica-
tion. The last section ends with conclusions and future work.
2. Related work
Research on text classification of long documents has com-
monly focused on feature extraction and feature reduction. Most
traditional techniques concentrate on measuring the similarity
between two documents based on the co-occurrences of words.
Hunnisett and Teahan (2004), Joachims (2002) and Stamatatos,
Kokkinakis, and Fakotakis (2000) proposed a method to extract
features from texts to construct a vector classifier. And, some other
works in reducing the feature vector are through the use of genetic
computing (Zhang et al., 2005). These researches obtain relatively
high accuracy in the case of long documents based on sharing com-
mon words, but it appears to not be accurate for short text snippets
because of its sparseness in feature extraction. Thus, researchers
could possibly use new ways of incorporating linguistic knowledge
into presentations of feature extraction, feature reduction and even
classifying parts of a text classification engine.
The classification of short text will always incur challenges if
the same methods used in long text are used to analyze data
sparseness with respect to exploring the feature correlation. Sev-
eral methods of classifying, clustering and matching rely on short
text snippets from search engines. Metzler, Dumais, and Meek
(2007) evaluated a wide range of similarity measures for short
queries from Web search logs. Yih and Meek (2007) explored1 http://dblp.uni-trier.de/xml
2 http://www.springer.com/computer/lncs
3 http://www.wikipedia.orgWeb-relevance similarity, and Sahami and Heilman (2006) inte-
grated external text into target short text to enhance features in
documents. Long et al. (2012) proposed a transfer classification
method to exploit external data to tackle the data sparseness issue
from short text in which classifiers will be learned in the original
feature space. Bollegala, Matsuo, and Ishizuka (2007) used search
engines to determine the semantic relatedness of words. Efron,
Organisciak, and Fenlon (2012) improved information retrieval
for short text in Twitter based on aggressive document expansion,
which helps in this content because short text yields little in the
way of term frequency information. Meanwhile, Gabrilovich and
Markovitch (2007) computed semantic relatedness using Wikipe-
dia concepts for matching between short texts from the Web and
search engines.
Based on external topic-oriented, Cai and Hofmann (2003),
Letsche and Berry (1997) and Liu, Chen, Zhang, Ma, and Wu
(2004) attempted to analyze topics using probabilistic Latent
Semantic Analysis, and they used both original data and generated
topics in order to train and boost two different weak classifiers. The
disadvantages of these researches are to extract topics only from
the training and testing data. Sun (2012) selected the most repre-
sentative and topical-indicative words from a given short text as
query words, and then searched for a small set of labeled short
texts best matching the query words. The predicted category label
voted for the research results.
Phan et al. (2011), Chen et al. (2011) and Quan, Liu, Lu, Ni, and
Liu (2010) discovered hidden topics from external large-scale data
collections which can make short text documents less sparse and
more topic-oriented. They processed short text and Web segments
rather than normal text documents. Chen et al. (2011) applied
datasets from Phan et al. (2011) that are short text snippets from
searching to leverage topics at multiple granularities. Le,
Bernardi, and Vald (2011) carried out the short text classification
of queries gathered from library searching. Wikipedia, as universal
knowledge, was used as universal dataset for topic models in these
researches. Experimental result indicated that their methods using
the discovered latent topics have achieved the state-of-the-art per-
formance. However, the proposed approaches of these researches
have some limitations. First, the authors just focused in adapted
topics adding to short text document as external features without
using knowledge which belongs to topics. Second, they lacked
comparison between Wikipedia and other knowledge.
In this paper, we propose methods to classify scientific docu-
ments based on titles, which can be seen as short text documents,
using topic models. The difference between this and previous
methods is that we will explore the topic models from various
external large-scale data collections not only Wikipedia but also
DBLP and LNCS for solving data sparseness in short texts. In addi-
tion, to enhance features, we exploit external texts from topic
models including topics and knowledge inside such topics and
present extended methods to optimize external features that
enhance relatedness in short text documents for classification.
Finally, we apply the proposed method to scientific documents, a
special domain, for classification.3. Probabilistic topic model
In documents, words are related to each other in terms of syn-
onymy, hypernymy and hyponymy. Their co-occurrence in a docu-
ment can be reasoned existence of certain relations definitely
(Salton, Wong, & Yang, 1975). Deerwester, Furnas, and Landauer
(1990) presented Latent Semantic Indexing (LSI) as an indexing
and retrieval method that uses a mathematical technique called
singular value decomposition (SVD) to identify patterns in the
relationships between the terms and concepts in an unstructured
1686 D.-T. Vo, C.-Y. Ock / Expert Systems with Applications 42 (2015) 1684–1698collection of text. Hofmann (1999) introduced a new probabilistic
topic approach extended from LSI, known as probabilistic Latent
Semantic Indexing (pLSI). Although pLSI is a generative model of
the documents, it is not clear how to assign the probability to a
document outside of the training set. Blei et al. (2003) proposed
Latent Dirichlet Allocation (LDA) based on the pLSI model, the
authors integrated into a Dirichlet prior; this has been pointed
out to be more attractive than pLSI. We briefly discuss the princi-
ples of LDA in following section.
3.1. Latent Dirichlet Allocation
Blei et al. (2003) proposed LDA as a generative probability
model of a corpus that can be used to estimate multinomial
observations using unsupervised learning. Given M documents
D = {d1,d2, . . . ,dM} with N unique words W = {w1,w2, . . . ,wN}, we
assume the collection contains Z hidden topics T = {t1, t2, . . . , tZ}.
For each word i in the document, we use W(i) to represent the
corresponding word i, D(i) represents its document, and T(i) repre-
sents its topic. In LDA, a document d is generated by picking a dis-
tribution of topics T(i) from a Dirichlet distribution Dir(a), which
determines the topic assignment for words in that document.
Then, the topic assignment for each word a placeholder is per-
formed by sampling a particular topic T(i) from the multinomial
distribution (Mult (T(i))). Finally, a particular word W(i) is generated
for placeholder by sampling from the multinomial distribution
ðMultð~uTÞ. This process is repeated until all K topics have been gen-
erated for the entire collection. The algorithm for LDA is as follows.
1. Choose the number of words n according to Poisson Distribu-
tion Poisson(n).
2. Choose the distribution over topics h by Dirichlet Distribution
Dir(a).
3. To characterize each of the n words W(i)
a. Choose a topic T(i) Multinomial(h).
b. Choose a word W(i) from P(W(i)|T(i),b), a multinomial proba-
bility conditioned on the topic T(i).
Taking the product of the marginal probabilities of a document
to obtain a new document d with n words is described as follows:
Pðdja;bÞ ¼
Z
PðhjaÞ
Yn
i¼1
X
TðiÞ
P W ðiÞjT ðiÞ;b
 
PðT ðiÞjhÞ
 !
dh ð1Þ
where P(h|a) is derived by the Dirichlet Distribution parameterized
by a, and P(W(i)|T(i),b) is the probability of W(i) under topic T(i)
parameterized by b. Parameter a can be viewed as a prior observa-
tion counting the number of times each topic is sampled in a docu-
ment before we have seen any words from that document. The
parameter b is a hyper parameter determining the number of times
words are sampled from a topic before any words from the corpus
are observed. Once the distribution is known, we can find the prob-
ability of the whole corpus D by multiplying all of the above prob-
abilities according to:
PðDja; bÞ ¼
YM
i¼1
PðdiÞ ð2Þ3.2. Gibbs Sampling for an LDA estimation parameter
The topic distribution P(W(i)|T(i),b) in Eq. (1) for estimation in
LDA was explored by Gibbs Sampling (Griffiths & Steyver, 2004;
Heinrich, 2008). Gibbs Sampling is a method for extracting topics
from a corpus, which is also used for distributing topics. Let ~d and
~t be the vectors of all words and their topic assignment for the wholedata collection D. The current topic assignment of all other word
positions is used to define which topic assignment for a particular
word. Specifically, the topic assignment of a particular word w is
sampled from the following multinomial distribution as follows:
p ti ¼ kj~t:i;~d
 
¼
CðwÞk;:i þ bwPV
v¼1C
ðvÞ
k þ bv
h i
 1
CðkÞm;:i þ akPK
j¼1C
ðjÞ
m þ aj
h i
 1
ð3Þ
where CðwÞk;:i represents the number of times the word w is assigned
to topic k, not including the current assignment.
PV
v¼1C
ðvÞ
k  1 is the
total number of words assigned to topic k , not including the current
assignment. CðkÞm;:i is the number of words in set m assigned to topic
k, not including the current assignment.
PK
j¼1C
ðjÞ
m  1 is the total
number of words in set m, not including the current word w. In nor-
mal cases, Dirichlet parameters ~a and ~b are symmetric. This mean
that all ak (k = 1, . . . ,K) are the same, and bv (v = 1, . . . ,V) is similar
to this. Two matrices are computed after finishing the Gibbs
Sampling as follows:
uk;w ¼
CðwÞk þ bwPV
v¼1C
ðvÞ
k þ bv
ð4Þ
and
vm;k ¼
CðkÞm þ akPK
j¼1C
ðjÞ
m þ aj
ð5Þ4. General framework
In this section, the proposed framework will be described as a
classification system of short text documents. The framework is
depicted in Fig. 1 and consists of four major steps: short text pre-
processing, topic model analysis, feature enrichment and classifier.
It is summarized as follows.
4.1. Short text preprocessing
Titles of scientific documents as short texts have been removed
stop-words, function words and useless punctuation. We then
extract features based on the bag-of-words of such documents that
will be served for feature enrichment in Step 3.
4.2. Topic model analysis
We collect raw documents from DBLP, LNCS and Wikipedia then
perform preprocessing for topic model analysis. The preprocessing
steps also include the removal of HTML tags, normalization, punc-
tuation and stop words. We estimate topic model analysis based on
LDA through Gibbs Sampling on such knowledge datasets. This will
be discussed in more detail in Section 5.
4.3. Feature enrichment
We exploit topic model analysis from Step 2 to enhance fea-
tures in short text documents by (1) assigning topics from topic
models as external features in documents, and (2) combining
external texts of adapted topics as external features in documents.
The detailed discussion on feature enrichment is in Section 6.
4.4. Classifier
Enriched data of short text is performed to construct vector
classifier. We use machine learning algorithms as SVM, Naive
bayes and kNN for classification. The results of classification will
be discussed in Section 7.
Fig. 1. Proposed framework for short text classification.
Table 1
Universal knowledge datasets for topic model analysis.
DBLP LNCS Wikipedia
Raw
documents
2 Million records,
Size: 932 MB
1200 book series
with 43,600 articles
(abstract only),
Size: 250 MB
50,000 HTML
documents,
Size: 920 MB
Final
documents
1.3 Million
records,
Size: 88 MB
43,600 articles,
Size: 25 MB
42,000 articles,
Size: 282 MB
D.-T. Vo, C.-Y. Ock / Expert Systems with Applications 42 (2015) 1684–1698 16875. Topic model analysis
This section provides a detailed description of topic model analysis
from three types of universal datasets. We collect raw documents from
DBLP, LNCS and Wikipedia then perform preprocessing as shown in
Table 1. The preprocessing steps include the removal of HTML tags,
normalization, punctuations and stop word. The details for the three
types of universal datasets are as follows.5.1. DBLP dataset. A computer bibliography lists more than two
million of tracked articles on computer science in Dec. 2012.
Tracked articles are in most journals and conference proceedings.
We collected documents from the DBLP XML dump dataset then
extracted title records. We filtered records in order to guarantee
that the lengths of documents are appropriate for LDA.4 http://gibbslda.sourceforge.net5.2. LNCS dataset. LNCS is a series of computer science books that
has been published by Springer Media since 1973. LNCS reports
research results in computer science, especially in the form of pro-
ceedings, post-proceedings and research monographs. We choseapproximately 1200 International Standard Serial Number (ISSN)
of LNCS and then collected abstract contents of 43,600 articles.5.3. Wikipedia dataset. Wikipedia, a collaborative Wiki-based ency-
clopedia, has become a huge phenomenon among Internet users. It
covers a large number of concepts in a variety of fields and contains
more than four million articles. We only filtered Wikipedia docu-
ments related to the objective domain of document classification
by using a Wiki crawler with keyword searching. For the Computer
Science domain, we crawled about 50,000 Wikipedia documents,
and then gathered 42,000 articles for topic model analysis.
We estimate topic models based on LDA through Gibbs Sam-
pling using GibbsLDA++4 on universal datasets. We optimize three
important parameters a, b and number of topics T in the LDA. It is
based on the topic number and the size of the vocabulary in the
document collection, which are a = 50/T and b = 0.01 respectively
(Heinrich, 2008; Phan et al., 2011; Quan et al., 2010), with T being
number of topics in each model. Then we arrange topic models
from 20,30, . . . ,100,120 and 150. Analyzing topic models from
three universal datasets took about 180 h. Several sample hidden
topics of model 150 from three universal datasets are shown in
Fig. 2(a)–(c). Universal datasets are very important for topic
models analysis. We check the number of some sample words
existing in the domain ‘‘Natural Language Processing’’ in topic
model 150 from three universal datasets in Fig. 2(d). The distribu-
tion of such words varies considerably in term of the quantity and
topic position. For examples, the word ‘‘grammar’’ is in Topic66 of
DBLP, in Topic52 of LNCS and in Topic93 of Wikipedia. However
Fig. 2. (a) Sample topics analyzed from topic model 150 of DBLP; (b) sample topics analyzed from topic model 150 of LNCS; (c) sample topics analyzed from topic model 150
of Wikipedia; and (d) distribution of sample words in topic model 150 of three types of universal datasets.
1688 D.-T. Vo, C.-Y. Ock / Expert Systems with Applications 42 (2015) 1684–1698the words ‘‘extraction’’ is in two topics Topic51 and Topic141 of
DBLP, and one topic Topic139 of LNCS, but does not occur in any
topic of Wikipedia. Thus, exploiting such universal datasets for
topic model analysis will cause differences in the improvement
of features. We discuss the performance comparison in the exper-
imentation section.6. Classification of short text document
6.1. Enhancing features of short text documents
To enhance the features of short text documents, we match top-
ics from topic models to all words of the documents. We consider
Fig. 3. Enhancing features in short text documents. (A) Matching topic models into short texts; (B) selecting the optimal number of adapted topics; (C) combining external
texts from adapted topics to short text documents.
Table 2
Experiment datasets.
Categories Positive Negative Total
Bioinformatics (Bio) 650 650 1300
Computer architecture (CA) 700 700 1400
Database 700 700 1400
Geographic information system (GIS) 650 650 1300
Network 650 650 1300
Natural language processing (NLP) 700 700 1400
Table 3
Contingency table for performance evaluation.
Positive texts Negative texts
Positive texts TP FP
Negative texts FN TN
D.-T. Vo, C.-Y. Ock / Expert Systems with Applications 42 (2015) 1684–1698 1689two types of external features for enrichment by (1) selecting the
optimal number of adapted topics5 and (2) selecting optimal
number of adapted topics and then combining external words6 from
adapted topics as depicted in Fig. 3.5 Topics assigned to short text documents are called adapted topics.
6 A topic model contains a list of topics, and each topic contains a list of
relationship words.Let ~t ¼ ft1; t2; . . . ; tig and d
!
¼ fw1;w2; . . . ;wjg be the vector of
the topic model generated by LDA and the vector of the short
text document. We first integrate the topic into the short text
document, which is the process of merging topic models into
short text documents described by ~t [~d. It is performed by
matching words in the short text document and with words in
the topic model. A sample correspondence of topics from topic
model 150 assigned to short text documents with a certain prob-
ability value is shown in Fig. 3(A). Particularly, the word ‘‘estima-
tion’’ of document d1 exists in topics 47, 48, 92 and 98 with
probability values 0.034, 0.014, 0.133 and 0.055 respectively.
Therefore, these topics will be merged into the target word ‘‘esti-
mation’’ in document d1.
Fig. 4. Evaluating optimal number of adapted topics.
1690 D.-T. Vo, C.-Y. Ock / Expert Systems with Applications 42 (2015) 1684–1698We enrich features in short text using adapted topics as exter-
nal words. After assigning topics, the words in short texts adapted
by topics can be represented as vector ~wjðt1; t2; . . . ; tiÞ with ti as
the adapted topic. However, the probabilities of different topics
being assigned in a target word are different, which causes an
increase in spaces in short text documents. Hence, evaluating
and selecting the quantity of adapted topics are necessary for
enhancing features and also for avoiding diluted features. As in
Fig. 3(A), the word ‘‘nonlinear’’ in document d1 is assigned by 5Fig. 5. System performance in Bio category; (A) F1 and Accutopics: 105, 54, 142, 32 and 20. Some adapted topics are quite
homogeneous in their semantic relationship, but some others
are different. Selecting the optimal number of assigned topics in
the target word is based on the probability value of topics and
how these topics are assigned to other words in the same docu-
ment. Note, the word ‘‘nonlinear’’ in document d1 is in Topic32
with distribution probability equal to 0.018, while in Topic54, it
has probability value equal to 0.020. Furthermore, the word ‘‘non-
linear’’ is associated with the words ‘‘instantaneous’’ and ‘‘expo-
nent’’ because they are in the same topic Topic32. Thus,
assigning item ‘‘Topic32’’ to the target word ‘‘nonlinear’’ in docu-
ment d1 will enhance features better than item ‘‘Topic54’’. To do
this, in each target word we add the probability value of topics
assigned to this word and the probability value of such topics
assigned to other words in the same document according to the
following formula:
PwðtiÞ ¼ PwðtiÞ þ
X
bPðtiÞ ð6Þ
We continually determine the optimal number of topics for the tar-
get word:
RanknðPwðtiÞÞ ð7Þ
The distribution value PwðtiÞ of topic ti assigned to each target
word w is calculated from the probability value Pw(ti) added toP
bP(ti), where, Pw(ti) is the probability value of adapted topic tiracy of method M1; (B) F1 and Accuracy of method M2.
Fig. 6. System performance in CA category; (A) F1 and Accuracy of method M1; (B) F1 and Accuracy of method M2.
D.-T. Vo, C.-Y. Ock / Expert Systems with Applications 42 (2015) 1684–1698 1691directly assigned to target word w and
P
bP(ti) is the sum of the
probability values of this topic ti assigned to other words in the
same document. Topics assigned to the target word directly are
more effective than such topics assigned to other words in the doc-
ument. Thus, we set 0 6 b 6 1.0 for controlling the effect of the
adapted topics. RanknðPwðtiÞÞ is the order number of n topics
ranked from the highest PwðtiÞ value to the lowest PwðtiÞ value.
The process of selecting the optimal adapted topics is summarized
in Algorithm 1. Fig. 3(B) shows the results of selecting the three
best topics. For example, the word ‘‘nonlinear’’ in document d1 is
assigned by 5 topics (105, 54, 142, 32 and 20), but the three best
adapted topics are 105, 32 and 142, which will be selected with
n = 3. In another example, the word ‘‘dynamic’’ in document d3 is
assigned by 4 topics (130, 46, 126 and 86), with n = 3 the three best
topics (130, 126 and 46) will be selected.
We enrich features in short text using external words from
adapted topics. After selecting the number of best topics, we
continue to combine external words from such adapted topics to
short text. Note that, each topic in the topic model is considered
to be a placeholder containing 20 topic-related words (see
Fig. 2). We will base word’s ranking on the probability values
and their existence in all short text documents. Fig. 3(C) displays
three words belonging to the adapted topics with the highest prob-
ability values, which are combined into short text documents. For
example, in document d1, the target word ‘‘estimation’’ will be
combined with external words from adapted topics: Topic92 with
group words {motion, reduction, parameter}, Topic98 with group
words {frequency, phase, signal}, and Topic47 with group words
{distribution, regression, probability}.6.2. Classification algorithm
There is a wide variety of machine learning algorithms for data
classification that can be applied to text classification. Many classi-
fication methods, such as SVM (Joachims, 2002; Liu, 2011; Zhang,
Yoshida, & Tang, 2008;), k-NN (Yang & Chute, 1994; Yang & Liu,
1999), Naive bayes (Lewis, 1998; McCallum & Nigam, 1998) and
so on can be used in our framework. We convert documents into
vector ~D ¼ fv1;v2; . . . ;v ig after enhancing features, where vi is
Fig. 7. System performance in Database category; (A) F1 and Accuracy of method M1; (B) F1 and Accuracy of method M2.
1692 D.-T. Vo, C.-Y. Ock / Expert Systems with Applications 42 (2015) 1684–1698weight of the ith feature in vector ~D; the weight of tf-idf is calcu-
lated as follows:
v i ¼ tfij  log
M
dfj
 
ð8Þ
where M is the total number of documents in the collection, dfj is
the document frequency, and tfij is simply the number of occur-
rences of feature vi in document dfj. Enriched short text documents
of the framework will increase of the number of tf-idf presentations
that are necessary for classification. We select a range of classifica-
tion algorithms based on the diversity of the based on the underly-
ing machine learning methods that support them. The types of
algorithms considered for included linear (SVM), probabilistic
(Naive bayes), and example-based (kNN) for our experiments.
7. Experimentation
7.1. Experimental datasets and evaluation measures
We manually collected titles of scientific documents in journals
and conference proceedings related to the Computer Science
domain via online publishing systems such as IEEEXplore7, Sprin-
gerLink8, and ACM Digital Library9. We chose six sub-categories that
cover most fields in Computer Science including Bioinformatics
(Bio), Computer Architecture (CA), Database, Geographic Information
System (GIS), Network, and Natural Language Processing (NLP)7 http://ieeexplore.ieee.org
8 http://link.springer.com
9 http://dl.acm.orgshown in Table 2. These sub-categories do not overlap with each
other. The numbers of documents are from 1300 to 1400 that guar-
antee the training/test datasets.
Next, we annotated labels of classes by determining which clas-
ses belong to which categories for the training phrase. Note that
this task needs to be performed by experts. In this study, we care-
fully annotated each document based on the indexing of papers
from journals or conference proceedings by publishers with an
appropriate label. For example, in the NLP category, collected doc-
uments of proceedings at the NLP conference, ‘‘Association for
Computational Linguistics10’’, indexed in the NLP field will be anno-
tated with a positive label, while collected documents that were
indexed in other fields as Network in this category will be annotated
with negative label.
To evaluate the effectiveness of the system performance, four
kinds of classical evaluation measures generally used in text
classification, Precision, Recall, F-measure and Accuracy are
adopted in this study. These evaluation measures can be calculated
according to Table 3 and following formulas respectively:
P ðprecisionÞ ¼ TP
TP þ FP ð9Þ
R ðrecallÞ ¼ TP
TP þ FN ð10Þ
F1 ¼
2 P  R
P þ R ð11Þ
Acc ðAccuracyÞ ¼ TP þ TN
TP þ TN þ FP þ FN ð12Þ10 http://www.aclweb.org
Fig. 8. System performance in GIS category; (A) F1 and Accuracy of method M1; (B) F1 and Accuracy of method M2.
D.-T. Vo, C.-Y. Ock / Expert Systems with Applications 42 (2015) 1684–1698 16937.2. Experiments
The machine learning algorithms selected were the 3 different
instances explained in Section 6.2 as SVM, kNN and Naive bayes.
We parameterized kNN by choosing the value k = 30, which is
known to perform well (Yang & Liu, 1999). For SVM, a linear kernel
was preferred due to its good balance between execution time and
accuracy. In the case of Naive bayes, there are two common used
probabilistic models as multinomial model and multivariate Ber-
noulli (McCallum & Nigam, 1998) for text classification under
Naive bayes framework. The multinomial model performance sig-
nificantly better than the multivariate Bernoulli, hence we choose
the first one for this work.
We evaluate the impact of feature improvement in short text
documents using topic models. Each category of short text docu-
ments is enriched with topic models from DBLP, LNCS and Wikipe-
dia; the tf-idf weight is then performed to serve for classifiers.
Following Eqs. (6) and (7) we evaluate number of the adapted top-
ics using topic models 50, 100, and 150 from DBLP in Bio category
as shown in Fig. 4. Thus, we set n = 3 to define the number of opti-
mal adapted topics for experiments. Experiments are applied to the
use of two types of external features for enrichment as (1) external
features are adapted topics, which are topics assigned into short
text documents (called M1), and (2) external features are external
words from adapted topics (called M2). The evaluation follows
5-fold cross validation schema. Experiments are applied in six
cases in two methods as follows:
 M1-DBLP: using external features as adapted topics from topic
models of DBLP. M1-LNCS: using external features as adapted topics from topic
models of LNCS.
 M1-Wikipedia: using external feature as adapted topics from
topic models of Wikipedia.
 M2-DBLP: using external features as external words in adapted
topics from topic models of DBLP.
 M2-LNCS: using external features as external words in adapted
topics from topic models of LNCS.
 M2-Wikipedia: using external features as external words in
adapted topics from topic models of Wikipedia.
7.2.1. Results
After enhancing feature using external features from three
types of knowledge by two methods M1 and M2, each category
was selected to form the documents used by the different classifi-
ers. We chose the F1 and Accuracy in order to provide a general
measure of performance by the classifiers. Fig. 5 depicts perfor-
mance results on Bio categories. Fig. 5.A presents performance
results of method M1 while Fig. 5.B presents the performance
results of method M2. Likewise, Figs. 6–11 present the perfor-
mance results on categories CA, Database, GIS, Network, NLP and
Overall respectively. SVM classifier is better than Naive bayes
and kNN in classification performance. We highlight the perfor-
mances points of each category as below:
7.2.1.1. Bio category. The best result is with 87.27% in F1 and
86.92% in Accuracy using topic model 50 from M2-Wikipedia by
SVM. Regarding method M1, system performance obtained the
highest results using Wikipedia with topic models 50, 70 and 80.
Using LNCS the system performances were better than using DBLP
Fig. 9. System performance in Network category; (A) F1 and Accuracy of method M1; (B) F1 and Accuracy of method M2.
1694 D.-T. Vo, C.-Y. Ock / Expert Systems with Applications 42 (2015) 1684–1698in most topic models. In M2, the system performance continued
with good results in using Wikipedia at most topic models except
topic model 30, and using LNCS was also better than using DBLP.
7.2.1.2. CA category. In this category, the highest result is 87.80% of
F1 and 86.79% of Accuracy in topic model 150 using M2-DBLP by
SVM. For M1, the system performances were good at topics 60
and 80 using M1-DBLP and topics 120 and 150 using MB-LNCS.
But using Wikipedia, the system performances were just effective
in topic models 20 and 70, while its results were lower than other
knowledge in other topic models. In M2, the system performance
archived good results at topic models 70 and 150 from M2-DBLP;
when using LNCS the outcome of system performance was rather
stable in all topic models; and the system performance just got
good results in using topic models 20 and 70 from Wikipedia.
7.2.1.3. Database category. The highest result is with 84.83% of F1
and 83.21% of Accuracy at topic model 100 using M1-DBLP by
SVM. With regard to M1, using DBLP obtained good results in most
of topic models. The difference between classifying through using
LNCS and Wikipedia was not much different at topic models 20
and 60, but the performance from using LNCS was better than
using Wikipedia in topic models from 70 to 150. In M2, employing
DBLP obtained good results in most of topic models, but in topic
models as 50, 80, 90, using LNCS shows priority over using DBLP
in classifying. The performance from employing Wikipedia was
low as comparing with other knowledge.
7.2.1.4. GIS category. The highest result is with 84.80% of F1 and
84.60% of Accuracy at topic model 150 using M1-Wikipedia bySVM. In M1, the system performance through using Wikipedia
was better than other knowledge in most of topic models; DBLP
was better than LNCS but not much. Regarding M2, using Wikipe-
dia yielded good outcomes at topic models 100, 120, and 150.
Meanwhile, using DBLP brought good results at topic models 40,
50, 70, 80, and 90.
7.2.1.5. Network category. The system resulted best performance
with 74.82% of F1 and 74.62% of Accuracy at topic model 40 using
M2-DBLP by SVM. Regarding M1, it is easy to recognize that the
system output equivalently at topic models of every types of
knowledge, but it seemed a little better at topic models 70, 120,
and 150 if using LNCS. In M2, the system performance achieved dif-
ferently towards each types of knowledge. Particularly, DBLP
results were better than LNCS’s and Wikipedia’s in most of topic
models. If comparing LNCS and Wikipedia, we can see their perfor-
mances were almost equivalent, but in some cases of using Wiki-
pedia were better than LNCS such as topic models 50, 60, 90, and
120. In short, the SVM classifier is still better than Naive bayes
and kNN in performance.
7.2.1.6. NLP category. The highest result is with 91.23% of F1 and
91.07% of Accuracy at topic model 50 using M1-DBLP by SVM. In
M1, the system performance obtained good results at topic models
possessing with small topics, e.g., topic models from 20 to 50.
Regarding M2, the performance of system with using LNCS was
fairly better than with other knowledge. Meanwhile, employing
DBLP resulted in limitation if comparing with others. The system
also obtained quite good performance towards using Wikipedia
at topic models 40, 100, and 120.
Fig. 10. System performance in NLP category; (A) F1 and Accuracy of method M1; (B) F1 and Accuracy of method M2.
D.-T. Vo, C.-Y. Ock / Expert Systems with Applications 42 (2015) 1684–1698 16957.2.1.7. Overall. In M1, the classification performance was achieved
from 71.76% to 79.48% in F1 and Accuracy. With regard to the
employment of universal knowledge, DBLP brought good results
at topic models 90, 100, and 150; and LNCS performed well at topic
models 50, 60 and 70; Wikipedia resulted in low at most of topic
models. In M2, the system performance achieved from 67.65% to
79.87% in F1 and Accuracy. DBLP brought good results in all 6 cases
of topic models, but it is the best at topic models 120 and 150.
LNCS was better than Wikipedia in most topic models except topic
models 20, 40 and 70. In general, the system had got the best per-
formance corresponding to the universal knowledge as follows
DBLP at topic models 150 with M1 and M2, LNCS at topic model
130 with M1 and topic model 70 with M2, Wikipedia at topic
model 50 with both M1 and M2.
In sum, from Figs. 5–11, we can conclude that.
The classifier of SVM is superior over the classifier of Naive
bayes, and kNN. Depending on the congruousness of category with
the types of knowledge, that knowledge affects topic models with
respect to enrich features in short text documents. For example,
DBLP is congruous with categories Database, CA and GIS; Wikipe-
dia is with categories Bio and GIS; while using LNCS performs well
with category CA. Regarding overall performance of M1 and M2
(Fig. 11), the best result is at topic model 150 from DBLP-M2.
Method M2 is more successful than method M1 in most of topic
models using DBLP. Different in using DBLP, the methods M1 is
better than the method M2 in using most of topic models towards
LNCS and Wikipedia.
7.2.2. Comparison
In this section, we discuss on overall performance (the best
cases) using three types of knowledge based on the methods M1and M2 and compare those with the Baseline and method of
Phan et al. (2011). The Baseline is overall performance of system
without using topic model. For method of Phan et al. (2011), we
have applied topic models followed the cases in M1 and M2 to
enrich features in documents with cutoff of 0.05 (Phan et al.,
2011) for overall performance. Note that, Phan et al. (2011) only
used adapted topics as external features. Table 4 displays overall
performances of 11 best cases from our proposed methods M1
and M2, method of Phan et al. (2011), and Baseline. The bold num-
bers show the best performance of proposed methods in each type
of knowledge in comparison. SVM is better than kNN and Naive
bayes in most cases. The method M1 obtained the best cases in
topic model 150 from DBLP, topic models 70 from LNCS, and topic
model 50 from Wikipedia. And, the method M2 obtained the best
cases in topic model 150 from DBLP, topic models 130 from LNCS,
and topic model 50 from Wikipedia. With respect to the Baseline,
the system performance achieved 72.2% of F1 and 71.61% of Accu-
racy in SVM, 71.76% of F1 and 70.94% of Accuracy in Naive bayes,
and achieved 71.23% of F1 and 69.64% of Accuracy in kNN. The
method of Phan et al. (2011) obtained better results than the Base-
line in using three types of knowledge. For example, the system
obtained 76.45% of F1 and 76.2% of Accuracy in SVM, 75.93% of
F1 and 75.5% of Accuracy in Naive bayes, and 75.34% of F1 and
74.62% of Accuracy in kNN using DBLP. Regarding our proposed
methods M1 and M2, the results show that our methods are better
than the method of Phan et al. (2011) and the Baseline. In particu-
lar, the methods M1 and M2 are better than Phan et al. (2011)
about 2.5% to 2.8% in F1 and Accuracy using DBLP, about 1.7% to
2.3% in F1 and Accuracy using LNCS, and about 2.6% to 2.8% in F1
and Accuracy using Wikipedia. Thus, selecting the optimized
adapted topics which are suitable with text documents, then
Fig. 11. Overall performance; (A) F1 and Accuracy of method M1; (B) F1 and Accuracy of method M2.
Table 4
Overall comparison between proposed method, the baseline and related work.
kNN Naive bayes SVM
P R F1 Acc P R F1 Acc P R F1 Acc
Baseline 73.78 68.86 71.23 69.64 72.55 70.98 71.76 70.94 72.95 71.52 72.2 71.61
DBLP
Phan et al.(2011) (topic150) 78.07 72.90 75.34 74.62 78.61 73.44 75.93 75.5 78.34 74.64 76.45 76.2
M1 (topic150) 79.23 74.77 76.94 76.5 79.51 77.1 78.29 77.87 81.68 77.39 79.48 78.76
M2 (topic150) 80.85 75.51 78.08 77.34 81.29 77.16 79.17 78.38 82.98 77.0 79.87 79.04
LNCS
Phan et al.(2011) (topic70) 75.89 72.61 74.21 73.38 76.29 73.05 74.64 74.31 77.68 73.69 75.52 75.65
M1 (topic70) 78.25 75.66 75.56 74.59 80.93 74.25 77.44 76.78 81.97 74.8 78.22 77.39
Phan et al.(2011) (topic130) 74.71 71.54 73.09 72.66 76.62 72.15 74.31 73.62 76.62 74.05 75.31 74.20
M2 (topic130) 76.91 73.69 75.27 73.36 79.32 74.48 76.82 75.25 82.49 74.16 78.11 76.50
Wikipedia
Phan et al.(2011) (topic50) 73.52 71.69 72.6 72.84 74.4 71.95 73.15 73.62 75.1 72.23 73.63 73.89
M1 (topic50) 71.1 78.02 74.39 74.54 73.01 78.03 75.43 75.81 73.95 78.76 76.28 76.56
M2 (topic50) 76.02 75.68 75.85 75.1 77.07 76.94 77 76.11 77.68 77.12 77.39 76.76
1696 D.-T. Vo, C.-Y. Ock / Expert Systems with Applications 42 (2015) 1684–1698external features from such adapted topics are continually
enhanced features the documents. By this way, our proposed
methods lead the documents are being abundant in features which
facilitate classification effectively.
7.3. Discussion
In experiments, our approach obtained quite an improvement
in the accuracy of short text classification. The experiments show
that enhancing features will intensify the similarity of short text
documents in categories. Applying LDA is an efficient method formaking short text documents less sparse and more topic-oriented.
The universal datasets affect analysis of the topic models with
respected to enrich features in short text documents. The
differences in the three types of universal datasets and word distri-
butions in such datasets cause a slight difference in the classifica-
tion results.
Our approach makes the features of short text documents much
more strength due to external words that make the correlation of
word relationships affecting to classification. By experiments,
selecting the optimal topics then exploiting external texts from
adapted topics in our proposed methods is suitable to enrich
D.-T. Vo, C.-Y. Ock / Expert Systems with Applications 42 (2015) 1684–1698 1697features in short text. By this way, the short text documents are
being abundant in features which facilitate classification more
effective than the Baseline and Phan et al. (2011). Moreover, we
understand that the characteristics of word distribution in univer-
sal datasets of DBLP and LNCS are quite significant for tasks of text
classification in domains related to scientific documents (e.g., Com-
puter Science domain).
Finally, we examine the effect of topic models from various uni-
versal datasets. Our approach deals with the problem of data
sparseness in short text document for classification. The following
are main advantages of our approach: (1) it exploits various univer-
sal datasets as DBLP, LNCS and Wikipedia for enhancing features in
short texts, which is a signification method for classification; (2) it
pays attention to the consistency of the merger between the univer-
sal datasets and short texts by ranking method for optimizing fea-
tures in short text documents; and (3) it is easy to implement and
is not expensive due to the availability of knowledge resources such
as DBLP, LNCS and Wikipedia. In experiments, the proposed method
obtained impressive results than the Baseline and related work,
which indicates that it is a high quality contextual classification
for short text, especially in scientific documents.
8. Conclusion and future work
We have presented a method for classifying short text from sci-
entific documents based on titles. The method involves enhancing
the features of short text documents with topic models from three
types of universal datasets as DBLP, LNCS and Wikipedia then
applying machine learning algorithms for classification. We
exploited the recent successful topic model, Latent Dirichlet Alloca-
tion, on enriched datasets; when used, it can make documents less
sparse and more topic-oriented. Furthermore, we presented meth-
ods for combining external texts from adapted topics to improve
features in short text documents which are then used as an
enriched presentation of snippets to be classified. In experiments,
we carefully conducted experiments short text datasets from six
types of sub-categories related to Computer Science domain. The
results show that our approach obtained more impressive perfor-
mance. Based on this, we assert that our approach is useful for clas-
sifying scientific documents focusing on short text.
Regarding future work, the research has been suggested with
attractive aspects to improve as follows. First, this study will be
considered on how to estimate and adjust the number of external
features of topic models towards target documents automatically.
Additionally, the consistency between the set of knowledge
applied and the object documents need classifying should be also
explored. Second, the features enrichment in documents could be
improved in analyzing the relationship of features by exploiting
topic models which have conditional references as LDA-SP
(Ritter, Mausam & Etzioni, 2010) or ConceptNet (Speer & Havasi,
2013). Those could make documents generating significant fea-
tures which will enhance the similarity between documents in
order to facilitate for classifying. Finally, this work can be applied
for queries classification in scientific documents searching. In par-
ticular, queries are often briefed as short texts, so they can be
applied by our proposed method. Hence, the framework of this
study could be considered to extend into Question Answering Sys-
tem applying to digital library.
Acknowledgments
This research was supported by Basic Science Research Program
through the National Research Foundation of Korea (NRF) funded
by the Ministry of Education, Science and Technology
(2012R1A1A2006906).References
Banerjee, S., Ramanathan, K., & Gupta, A. (2007). Clustering short texts using
wikipedia. In Proceedings of ACM SIGIR 2007.
Blei, D., Andrew, Y. Ng., & Jordan, M. I. (2003). Latent Dirichlet allocation. Journal of
Machine Learning Research, 3, 993–1022.
Bollegala, B., Matsuo, Y., & Ishizuka, M. (2007). Measuring semantic similarity
between words using web search engines. In Proceeding of 16th international
conference world wide web (WWW’07).
Cai, L., & Hofmann, T. (2003). Text categorization by boosting automatically
extracted concepts. In Proceedings of ACM SIGIR 2003.
Cancedda, N., Gaussier, E., Goutte, C., & Renders, J. M. (2003). Word sequence
kernels. Journal of Machine Learning Research, 3, 1059–1082.
Chen, M., Jin, X., & Shen, D. (2011). Short text classification improved by learning
multi-granularity topics. In Proceedings of the 22nd international joint conference
on artificial intelligence (pp. 1776–1781). July 16–22, 2011, Barcelona, Catalonia,
Spain.
Deerwester, S., Furnas, G., & Landauer, T. (1990). Indexing by latent semantic
analysis. Journal of the American Society for Information Science, 41(6), 391–407.
Efron, M., Organisciak, P., & Fenlon, K. (2012). Improving retrieval of short texts
through document expansion. In Proceedings of SIGIR ‘12, 2012.
Gabrilovich, E., & Markovitch, S., (2007). Computing semantic relatedness using
wikipedia-based explicit semantic analysis. In Proceedings of 20th international
joint conference artificial intelligence (IJCAI’07).
Griffiths, T., & Steyver, M. (2004). Finding scientific topics. In Proceedings of national
academy of sciences of the United States of America (Vol. 101, pp. 5228–5235).
Heinrich, G. (2008). Parameter estimation for text analysis. Rapport technique,
University of Leipzig.
Hofmann, T. (1999). Probabilistic latent semantic indexing. In Proceedings of the
22nd annual international ACM SIGIR conference on research and development in
information retrieval (SIGIR’99) (pp. 50–57).
Hunnisett, D. S., & Teahan, W. J. (2004). Context-based methods for text
categorization. In Proceedings of the 27th annual international ACM SIGIR
conference on research and development in information retrieval (pp. 578–579).
Joachims, T. (2002). Learning to classify text using support vector machines. Kluwer
(Dissertation).
Le, D. T., Bernardi, R., & Vald, E. (2011). Query classification via topic models for an
art image archive. In Proceedings of Recent Advances in Natural Language
Processing (RANLP2011). Bulgaria.
Letsche, T. A., & Berry, M. W. (1997). Large-scale information retrieval with latent
semantic indexing. Information Science, 100(1–4), 105–137.
Lewis, David D. (1998). Naive (bayes) at forty: The independence assumption in
information retrieval. In Proceedings of ECML-98, 10th European conference on
machine learning, Chemnitz, DE (pp. 4–15). Heidelberg, DE: Springer Verlag.
Liu, B. (2011). Web data mining: Exploring hyperlinks, contents, and usage data.
Data centric systems and applications. 9783642194597. Springer.
Liu, T., Chen, Z., Zhang, B., Ma, W., & Wu, G. (2004). Improving text classification
using local latent semantic indexing. In Proceedings of IEEE international
conference on data mining (ICDM 2004).
Long, G., Chen, L., Zhu, X., & Zhang, C. (2012). TCSST: Transfer classification of short
& sparse text using external data. In Proceedings of ACM CIKM’12, 2012.
Ma, L., Shepherd, J., & Nguyen, A., (2003). Document classification via structure
synopses. In Proceedings of the 14th Australasian database conference on database
technologies 2003 (Vol. 17, pp 59–65). Adelaide, Australia.
McCallum, A. & Nigam, K. (1998). A comparison of event models for naive bayes text
classification. In Proceedings of AAAI-98 workshop on learning for text
categorization (pp. 137–142). Madison, Wisconsin.
Metzler, D., Dumais, S., & Meek, C. (2007). Similarity measures for short segments of
text. In Proceedings of 29th European conference IR research (ECIR’07).
Nigam, K., Lafferty, J., & McCallum, A. (1999). Using maximum entropy for text
classification. In Workshop on machine learning for information filtering IJCAI-99
(pp. 61–67).
Phan, X. H., Nguyen, C. T., Le, D. T., Nguyen, L. M., Horiguchi, S., & Ha, Q. T.
(2011). A hidden topic-based framework toward building applications with
short web documents. IEEE Transactions on Knowledge and Data Engineering,
23, 961–976.
Quan, X., Liu, G., Lu, Z., Ni, X., & Liu, W. (2010). Short text similarity based on
probabilistic topics. Knowledge and Information Systems, 25(3), 473–491.
Ritter, A., Mausam, & Etzioni, O., (2010). A latent Dirichlet allocation method for
selectional preferences. In Proceedings of the 48th annual meeting of the
association for computational linguistics (pp. 424–434). July 11–16, 2010.
Sahami, M., & Heilman, T. (2006). A web-based kernel function for measuring the
similarity of short text snippets. In Proceedings of the 15th international
conference on world wide web (WWW2006) (pp. 377–386). New York: ACM
Press.
Salton, G., Wong, A., & Yang, C. S. (1975). A vector space model for automatic
indexing. Communications of the ACM, 18(11), 613–620.
Sebastiani, F. (2002). Machine learning in automated text categorization. ACM
Computing Surveys, 34, 1–47.
Speer, R., & Havasi, C. (2013). ConceptNet 5: A large semantic network for relational
knowledge. In The People’s Web Meets NLP. Berlin, Heidelberg: Springer.
Stamatatos, E., Kokkinakis, G., & Fakotakis, N. (2000). Automatic text categorization
in terms of genre and author. Computational Linguistics, 26, 471–495.
Sun, A. (2012). Short text classification using very few words. In Proceedings of ACM
SIGIR’12, 2012.
1698 D.-T. Vo, C.-Y. Ock / Expert Systems with Applications 42 (2015) 1684–1698Yang, Y. & Liu, X. (1999). A re-examination of text categorization methods. In
Proceedings of 22nd annual international SIGIR. (pp. 42–49).
Yang, Y., & Chute, C. G. (1994). An example-based mapping method for text
categorization and retrieval. ACM Transactions on Information Systems, 12(3),
252–277.
Yih, W., & Meek, C. (2007). Improving similarity measures for short segments of
text. In Proceedings of 22nd national conference artificial intelligence (AAAI2007).Zhang, B., Chen, Y., Fan, W., Fox, E.A., Goncalves, M., Cristo, M. et al. (2005).
Intelligent GP fusion from multiple sources for text classification. In Proceedings
of the 14th ACM international conference on Information and knowledge
management (pp 477–484).
Zhang, W., Yoshida, T., & Tang, X. (2008). Text classification based on multi-word
with support vector machine. Knowledge-Based Systems, 21(8), 879–886.
