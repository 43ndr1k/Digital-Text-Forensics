ARTICLE IN PRESS
JID: ESWA [m5G;August 4, 2015;21:54]
Expert Systems With Applications xxx (2015) xxx–xxx
Contents lists available at ScienceDirect
Expert Systems With Applications
journal homepage: www.elsevier.com/locate/eswa
PDLK: Plagiarism detection using linguistic knowledge
Asad Abdi a,∗, Norisma Idris a, Rasim M. Alguliyev b, Ramiz M. Aliguliyev bQ1
a Department of Artificial Intelligence Faculty of Computer Science and Information Technology, University of Malaya, 50603 Kuala Lumpur, Malaysia
b Institute of Information Technology, Azerbaijan National Academy of Sciences, 9, B. Vahabzade Street, AZ1141 Baku, Azerbaijan
a r t i c l e i n f o
Keywords:
Automatic plagiarism detection
Text reuse
String matching
Semantic analysis
a b s t r a c t
Plagiarism is described as the reuse of someone else’s previous ideas, work or even words without sufficient
attribution to the source. This paper presents a method to detect external plagiarism using the integration of
semantic relations between words and their syntactic composition. The problem with the available methods
is that they fail to capture the meaning in comparison between a source document sentence and a suspi-
cious document sentence, when two sentences have same surface text (the words are the same) or they are
a paraphrase of each other. Therefore it causes inaccurate or unnecessary matching results. However, this
method can improve the performance of plagiarism detection because it is able to avoid selecting the source
text sentence whose similarity with suspicious text sentence is high but its meaning is different. It is exe-
cuted by computing the semantic and syntactic similarity of the sentence-to-sentence. Besides, the proposed
method expands the words in sentences to tackle the problem of information limit. It bridges the lexical gaps
for semantically similar contexts that are expressed in a different wording. This method is also capable to
identify various kinds of plagiarism such as the exact copied text, paraphrasing, transformation of sentences
and changing of word structure in the sentences. As a result, the experimental results have displayed that the
proposed method is able to improve the performance compared with the participating systems in PAN-PC-
11. The experimental results also displayed that the proposed method demonstrates better performance as
compared to other existing techniques on PAN-PC-10 and PAN-PC-11 datasets.
© 2015 Published by Elsevier Ltd.
1. Introduction1
With the increasing information in World Wide Web, it makes2
easy to represent someone else’s thought as own work without pro-3
viding the appropriate credit for the first owner or original source.4
Avoiding plagiarism is essential with regard to ethics. Recently, it5
made a significant issue in both academic and non-academic worlds.6
In plagiarism, the plagiarists attempt to change the contribution,7
the idea or the words of others as their own work (Geravand8
& Ahmadi, 2014; Osman, Salim, Binwahlan, Alteeb, & Abuobieda,9
2012). The plagiarism can be performed by exact copy, cutting sen-10
tences, combining sentences, paraphrasing, replacing the original11
words with the similar words or synonym words (El-Alfy, Abdel-Aal,12
Al-Khatib, & Alvi, 2015; Sánchez-Vega, Villatoro-Tello, Montes-y-13
Gomez, Villaseñor-Pineda, & Rosso, 2013). The challenge involving14
plagiarism can be obtained from several areas and therefore has ef-15
fects on us in several ways. Most of these areas contain: academia,16
∗ Corresponding author. Tel.: +60 104223242.Q2
E-mail addresses: alborzabdi85@gmail.com, asadabdi55@gmail.com
(A. Abdi), norisma@um.edu.my (N. Idris), r.alguliev@gmail.com (R.M. Alguliyev),
r.aliguliyev@gmail.com (R.M. Aliguliyev).
scientific research, journalism, patents and literature (Oberreuter & 17
VeláSquez, 2013). 18
Anti-plagiarism tool can have key role to prevent people perform- 19
ing plagiarism inadvertently or intentionally, so that the people pro- 20
vide much attempt to contribute new thoughts or even methods 21
based on their investigation to the academic world. Plagiarism iden- 22
tification can be done in two main ways: manually and automatically. 23
While the automatic plagiarism recognition is performed by the com- 24
puter system, the manual plagiarism recognition is carried out by 25
human. The plagiarism identification is also divided into the exter- 26
nal plagiarism identification and internal plagiarism identification. In 27
internal plagiarism identification method an unknown document is 28
compared with a set of known documents by the same author, in or- 29
der to determine whether the unknown document has been written 30
by the same author published the known documents (Mahdavi, Sia- 31
dati, & Yaghmaee, 2014; Oberreuter & VeláSquez, 2013). In external 32
plagiarism identification a suspicious document is compared with a 33
set of source documents to find plagiarized text between them (Rao, 34
Gupta, Singhal, & Majumder, 2011; Wang, Qi, Kong, & Nu, 2013). 35
Nowadays there are several methods to identify the plagiarized 36
content. Usually, these methods compare two documents on word, 37
phrase or sentence level (Sarkar, Marjit, & Biswas, 2014) . This paper 38
also aims to propose a method that considers both the semantic and 39
http://dx.doi.org/10.1016/j.eswa.2015.07.048
0957-4174/© 2015 Published by Elsevier Ltd.
Please cite this article as: A. Abdi et al., PDLK: Plagiarism detection using linguistic knowledge, Expert Systems With Applications (2015),
http://dx.doi.org/10.1016/j.eswa.2015.07.048
2 A. Abdi et al. / Expert Systems With Applications xxx (2015) xxx–xxx
ARTICLE IN PRESS
JID: ESWA [m5G;August 4, 2015;21:54]
syntactic information in comparison between a source document and40
a suspicious document.41
In text relevance context, linguistic information for instance se-42
mantic relations between words and their syntactic structure, have43
key role in sentence comprehension. Syntactic information, like44
word-order, can prepare beneficial information to distinguish the45
meaning of two sentences, when two sentences share the similar46
bag-of-words. For example, “Alex calls John” and “John calls Alex” will47
be judged as similar sentences because they have the same surface48
text. However, their meaning is different. Therefore, to compare two49
documents, the source document and a suspicious document, the50
proposed method should contribute syntactic information to deter-51
mine suspicious similarity between two documents; otherwise, it52
fails to capture the meaning in comparison and often there is a con-53
flict to identify suspicious similarity between documents. However, it54
leads to incorrect or even unnecessary matching results.55
On other hand, in comparison between two sentences, two sen-56
tences are considered to be similar if most of the words are the same57
or if they are a paraphrase of each other. However, it is not always58
the case that sentences with similar meaning necessarily share many59
similar words. Hence, semantic information such as semantic simi-60
larity between words and synonym words can provide useful infor-61
mation when two sentences have similar meaning, but they used62
different words in the sentences. This is because people can express63
the same meaning using various sentences in terms of word content.64
However, the more similar sentence may be represented with similar65
words, rather than the original words expressed in the source docu-66
ment sentences; hence the semantic information will help to identify67
the similar ideas, when an author presents someone else’s idea as his68
or her own words by text manipulation approach, paraphrase or syn-69
onym words.70
The proposed method is used to detect the plagiarized text. The71
method includes three important points. First, it is a comprehen-72
sive plagiarism method, which can detect different types of plagia-73
rism such as the exact copied text, paraphrasing (similar or synonym74
words replacing), transformation of sentences and changing of word75
structure in the sentences. The second point is related to the com-76
parison mechanism, where our method considers both the seman-77
tic and syntactic information to compute the similarity measure be-78
tween two sentences. The third point indicates that the method can79
capture the meaning of sentences using the combination of semantic80
and syntactic information.81
The method is called Plagiarism Detection using Linguistic Knowl-82
edge (PDLK), since the suspicious documents are identified using83
semantic information obtained from a lexical database and syntac-84
tic information is given by analysing the structure of the sentence.85
The structure of this paper is as follows. Section 2 provides a short86
overview of the previous methods that are used to produce sum-87
maries. Section 3 introduces the proposed method. Section 4 dis-88
cusses the performance analysis and presents the results of the anal-89
ysis. Finally, in Section 5, we summarize the works discussed and the90
progress of the project.91
2. Brief review of literature92
The plagiarism detection process contains three main steps93
(Barrón-Cedeño, Gupta, & Rosso, 2013; Potthast et al., 2012): can-94
didate retrieval, detailed comparison, and heuristic post-processing.95
Given a suspicious document (denoted by docsuspicious) and a large96
corpus of documents (denoted by cordocument). First, the candi-97
date retrieval selects a set of document from the corpus (denoted98
by docsource; docsource ⊆ cordocument) that are more similar to the99
docsuspicious and can be a source of plagiarized content. Second, each100
source document, docsource, is compared with the suspicious doc-101
ument, docsuspicious, then each pair of sections of both document102
are extracted and are considered as a plagiarism, if they have high103
similarity. Third, heuristic post-processing presents all extracted sec- 104
tions pairs. Below a set of methods that have been proposed to detect 105
plagiarism are introduced. 106
Paul and Jamal (2015) proposed a method to for plagiarism detec- 107
tion. The method comprises five main steps: pre-processing; candi- 108
date retrieval; sentence ranking; semantic role labeling and similar- 109
ity detection. Pre-processing includes two sub-steps that are text seg- 110
mentation and stop word removal. The text segmentation splits the 111
text into several sentences. The stop word removal, eliminate some of 112
the English words that are most frequently used. Candidate retrieval 113
determines a subset of source documents for a suspicious document. 114
It uses n-gram and Jaccard coefficient similarity for text comparison. 115
Sentence ranking is employed to rank sentences in the suspicious and 116
original document to retrieve original and suspicious sentence pairs. 117
The similarity between sentences is calculated using the cosine sim- 118
ilarity. Semantic Role Labeling (SRL) aims to determine the semantic 119
roles of each term of a sentence based on the semantic relationship 120
between their terms. It determines the object and subject of a sen- 121
tence for identifying the semantic roles of each term. Plagiarism de- 122
tection using SRL aims to detect the semantic similarity between the 123
ranked sentences. Similarity detection, in this stage, sentence simi- 124
larity between ranked suspected and original sentences is performed. 125
Sentences in suspected documents are compared with each sentence 126
in the candidate. The experimental displayed that the application of 127
sentence ranking in current method decreases the time of checking. 128
Oktoveri, Wibowo, and Barmawi (2014) proposed a method to 129
reduce non-relevant documents, which have no similar topic with 130
query document. The proposed method used several algorithm 131
and approach: winnowing algorithm; AVL Tree for indexing doc- 132
uments; Longest Common Subsequence and term frequency. AVL 133
Tree algorithm is a data structure algorithm based on Binary Tree 134
(AdelsonVelskii & Landis, 1963; Foster, 1965; Irving & Love, 2003). It 135
is used a data structure including of terms from each document. Each 136
indexed documents is omitted based on the similarity measure calcu- 137
lated using asymmetric similarity equation (Raphael, 2002) based on 138
term frequency. Term frequency is the number of term occurrences 139
in a document. It is used to compute the similarity measure between 140
two documents (Raphael, 2002; Stein & Zu Eissen, 2006). Winnowing 141
algorithm (Schleimer, Wilkerson, & Aiken, 2003) is employed reduce 142
the number of terms in order to accelerate the detection process. LCS 143
is used to compare two strings and to find out the longest overlap- 144
ping path (Campos & Martinez, 2012; Iliopoulos & Rahman, 2009). 145
The result shows that reducing non-relevant document shortens the 146
processing time compared to non-reduced process. It also provides 147
good result in terms of speed and accuracy. 148
Mahdavi et al. (2014)) proposed an external plagiarism detection 149
method based on the vector space model (VSM) (Raghavan & Wong, 150
1986). The proposed method contains three main phases: data prepa- 151
ration, relevant documents retrieval and detailed string matching. 152
The main task of data preparation is to convert both source and sus- 153
picious documents into vectors of the corpus terms. The terms are 154
high frequent ones of the corpus. The data preparation phase includes 155
six sub-steps: Text normalisation, stop words removal, stemming, 156
synonyms replacement, tokenisation and feature selection. In rele- 157
vant document retrieval phase a suspicious document is compared 158
to all source documents using vector cosine similarity in order to re- 159
trieve the most relevant source documents. In detailed string match- 160
ing phase the suspicious document and all retrieved documents from 161
previous phase are converted into tri-grams. Then, using the over- 162
lap coefficient, the most similar documents to the suspicious docu- 163
ment are determined as plagiarized source documents. The exper- 164
imental result demonstrated that the accuracy of the method was 165
encouraging. 166
Soleman and Purwarianti (2014) proposed a method for plagia- 167
rism detection based on the Latent Semantic Analysis (LSA) (Franzke 168
& Streeter, 2006). The method includes three main components: 169
Please cite this article as: A. Abdi et al., PDLK: Plagiarism detection using linguistic knowledge, Expert Systems With Applications (2015),
http://dx.doi.org/10.1016/j.eswa.2015.07.048
A. Abdi et al. / Expert Systems With Applications xxx (2015) xxx–xxx 3
ARTICLE IN PRESS
JID: ESWA [m5G;August 4, 2015;21:54]
pre-processing (PRE) component, heuristic retrieval (HR) component170
and detailed analysis (DA) component. LSA is used in both heuris-171
tic retrieval and detailed analysis components. The pre-processing is172
performed for both source and suspicious documents. This compo-173
nent contains three processes such as stop word removal, stemming174
and tokenisation. The aim of heuristic retrieval component to reduce175
the number of documents that must be analysed detailed analysis176
component. It used LSA method to find the most relevant source177
documents. Detailed analysis component identifies most suspected178
plagiarism section in the source document. To determine this, the179
source document and suspected document are split into section such180
as paragraph or sentences then each section in both documents is181
compared. In this component, LSA is also used as the document com-182
parison method.183
Hussein (2015) proposed a method for document Similarity184
Estimation. It includes pre-processing, Phrase Extraction, Building185
Document Model and Similarity Estimation. In pre-processing step186
the main linguistic functions such as tokenisation and stop word re-187
moval is performed. In the next step, Phrase Extraction, all documents188
are split into n-gram, e.g. unigram, bigram and trigram. Building Doc-189
ument Model represents documents using a matrix, where columns190
represent documents and rows represent phrases. Each element of191
matrix represents the weighted occurrence frequency of phrase ex-192
tracted from previous step. The last stage, Similarity Estimation, com-193
putes the mutual pairwise document similarity. For this purpose, the194
Singular Value Decomposition (SVD) is applied to decompose the ma-195
trix A into three independent matrices (Ceska, Toman, & Jezek, 2008).196
Finally, if a query vector q, represents a suspicious document the sim-197
ilarity measure between the query vector q and the m document vec-198
tors is computed using the cosine similarity. The experimental results199
displayed that the method could generate better results than other200
methods.201
Wang et al. (2013) proposed a method based on the VSM (Bao,202
Shen, Liu, & Song, 2003) and Jaccard coefficient (Kong et al. ) to detect203
the plagiarism. VSM is based on TF-IDF scheme. In the VSM the source204
document and the suspicious document are divided into sentences,205
and then the sentences are represented in term of vector. The weight206
of each term in vector is calculated by the TF-IDF method. Finally the207
similarity between two sentences is computed using the cosine dis-208
t209
I210
c211
t212
t213
t214
a215
t216
c217
t218
219
r220
s221
s222
c223
s224
a225
d226
b227
t228
a229
l230
s231
fi232
f233
s234
g235
then using the similarity coefficient method (Abdi, Idris, Alguliev, & 236
Aliguliyev, 2015; Nawab, Stevenson, & Clough, 2010) the similarity is 237
calculated. If the similarity value exceeds the threshold value, the pla- 238
giarized texts are selected using the graph-based approach and the 239
depth first search algorithm. Finally, in fourth step the plagiarized 240
text are presented. 241
Ceska (2008) proposed a method based on Singular Value Decom- 242
position (SVD) for plagiarism detection. It called SVDPlag. The pro- 243
posed method includes the following steps. First, pre-processing per- 244
forms several tasks such as, stop-word removed, Lemmatisation to 245
identify the root of each word (Toman, Tesar, & Jezek, 2006) and part- 246
of-speech of each word. Second, in Phrase Extraction, the documents 247
are split into phrases in term of n-gram. Third, the phrase reduction 248
aims to reduce the number of phrases. It used document frequency 249
to identify whether a phrase is important or not. If a phrase appears 250
only in one document, then the phrase will be removed, otherwise it 251
is considered as an important phrase. Fourth, creating a matrix, in this 252
step the documents are represented in form of matrix, where rows in- 253
dicates documents and the column indicates the phrases. The weight 254
of each cell, aij, in matrix is equal to the number of times that a phrase 255
i was appeared in document j. Finally in the last step, the SVD is ap- 256
plied to the matrix, obtained from previous step, and the similarity 257
measure between each pair of documents is computed. If the similar- 258
ity score between two documents exceeds the pre-defined threshold, 259
both documents are considered as plagiarized. 260
3. Proposed method 261
In this section we describe our plagiarism detection method. 262
The general architecture of our proposed method is presented in 263
Fig. 1. Our method includes three main steps. In the first step, pre- 264
processing the basic natural language processing tasks is done. At de- 265
tailed comparison step, the docsuspicious and the docsource are decom- 266
posed into several sentences in order to identify pairs of sentences 267
(Sq, Sx), where Sq ∈ docsuspicious and Sx ∈ docsource, which are more 268
similar. Finally, in the last step, the results of the previous step are 269
considered as input for the post-processing step in order to present 270
the plagiarized sentences. 271
We describe each of the aforementioned steps in the subsequent 272
s 273
3 274
275
d 276
c 277
s 278
279
t 280
a 281
d 282
s 283
g 284
285
q 286
c 287ance. The VSM focuses on computing the global similarity measure.
t is good to detect plagiarism when the passages or sentences in-
lude plagiarized phrases. However, the similarity measure based on
he VSM is not able to identify all kinds of plagiarized content; hence
he Jaccard coefficient based on the term-matching also used to de-
ect the plagiarism passages. The proposed method applies both VSM
nd Jaccard coefficient to compute the similarity measure between
wo sentences. The experimental results displayed that the method
ould generate better results than the methods which employs only
he VSM or Jaccard coefficient.
Ekbal, Saha, and Choudhary (2012) proposed a method for plagia-
ism detection. It includes three major steps. First, in pre-processing
tep the basic tasks of natural language processing are done. This
tep contains the following functions. The generate tokens, the POS-
lasses and stop-word removal. In the second step, a number of
ource documents that are more similar to the suspicious document
re selected. To identify the source documents for each suspiciousocument, it used Vector Space Model (VSM). In this model the
oth source document and suspicious document are represented in
erm of vector. Each cell of vector is weighted using term-frequency
nd inverse document frequency (TF-IDF) scheme. Finally the simi-
arity measure between two documents is calculated using the co-
ine similarity. If the obtained similarity measure exceeds the prede-
ned threshold, the document is considered as a source document
or the suspicious document. In third step, the similar text in both
ource document and suspicious document are found using the n-
ram method. Both documents are split into n-gram (i.e. n = 3),
i 288
s 289
w 290
o 291
w 292
e 293
e
i
q
c
Please cite this article as: A. Abdi et al., PDLK: Plagiarism detection usin
http://dx.doi.org/10.1016/j.eswa.2015.07.048ections.
.1. Pre-processing
The main task of this step is to prepare the source document
ocsource and the suspicious documents docsuspicious for further pro-
essing. This step consists of three main functions, such as sentence
egmentation, stop word removal and stemming.
Sentence segmentation — in this process, the source document and
he suspicious documents are split into individual sentences, which
re the textual units considered for comparison between the source
ocument and the suspicious documents. A sentence ends with full
top (.) whereas a paragraph is ended by new line. Therefore, a para-
raph consists of a group of sentences.
Stop word removal— stop words, are words which occurred fre-
uently in a document and are meaningless words, such as arti-
les, propositions and conjunctions (van Rijsbergen, 1986). Accord-
ng to the results of research (Tomasic & Garcia-Molina, 1993), the
top words include 50% of documents text words. Removing such
ords speeds the system processing and improves the performance
f the method (Baeza-Yates, 1992). Using stop word removal, the
ords that are very common within a text and are also consid-
red as noisy terms are removed. Obviously, their removal can beffective before the accomplishment of a natural language process- 294
ng task. Removal of such words can improve accuracy and time re- 295
uirements for comparisons by saving memory space and thus by in- 296
reasing the speed of processing (Paul & Jamal, 2015). Such removal 297
g linguistic knowledge, Expert Systems With Applications (2015),
4 A. Abdi et al. / Expert Systems With Applications xxx (2015) xxx–xxx
ARTICLE IN PRESS
JID: ESWA [m5G;August 4, 2015;21:54]
itectu
298
299
300
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
s 323
T 324
t 325
c 326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
e 348
t 349
f 350
351
352
d 353
d 354
s 355
s 356Fig. 1. The Arch
Table 1
Examples of stop words.
Stop words
‘About, above, across, after, afterwards, again, against, all, almost, alone,’
‘already, also, although, always, am, among, amongst, amongst, amount, an,’
‘another, any, anyhow, anyone, anything, anyway, anywhere, are, around, as,’
‘along, and, in, the, of’ …
is usually performed by word filtering with the aid of a list of stop
words. In our work, the stop words extracted from the English stop
word list (http://jmlr.csail.mit.edu/papers/volume5/lewis04a/a11-
smart-stop-list/english.stop). Table 1 shows some of these stop words
that may appear in a sentence.
Stemming—it is used to reduce word to its stem form. It is useful to
identify words that belong to the same stem (e.g. went and gone, both
come from the verb go). This process obtains the root of each word
using the lexical database, Word Net. Our method used stemming in
comparison between two documents.
Word Net is a lexical database for English which was developed
at Princeton University (Miller & Charles, 1991). It includes 121,962
unique words, 99,642 synsets (each synset is a lexical concept repre-
sented by a set of synonymous words) and 173,941 senses of words.
3.2. Detailed comparison
The detailed comparison step includes the identification of all
plagiarized sentences from the suspicious document and their cor-
responding source sentences from the source document. For this,
given a suspicious document, docsuspicious, and a source document,
docsource. Both documents the docsource and the docsuspicious are split
into several sentences. After that a pair of sentences (Sq, Sx), where
Sq ∈ docsuspicious and Sx ∈ docsource, are considered as pairs of plagia-
rism sentences if their similarity measures exceeded the threshold
value. We apply our method to detect all plagiarism sentences. The
proposed method combined the semantic similarity and syntacticPlease cite this article as: A. Abdi et al., PDLK: Plagiarism detection usin
http://dx.doi.org/10.1016/j.eswa.2015.07.048re of the PDLK.
imilarity to calculate the similarity measure between two sentences.
he overall process of applying the semantic and syntactic informa-
ion to calculate the similarity measure is shown in Fig. 2. These pro-
esses are as follows:
1. Two sentences Sq, Sx are considered as its inputs.
2. The word-set is created using the two sentences.
3. The semantic-vector is created for each of two sentences.
4. The word-order vector is created for each of the two sentences.
5. The semantic word similarity approach is used to find the sim-
ilar words. The steps 3 and 4 employ this method to create the
semantic-vector and word-order vector.
6. It measures the semantic similarity measure between two sen-
tences. The semantic similarity measure is determined by the
cosine between the two corresponding semantic vectors.
7. It computes the word-order similarity measure between two
sentences. The similarity score is determined by the syntactic-
vector approach (Li, McLean, Bandar, O’shea, & Crockett, 2006).
This approach will be explained in the next section.
8. Finally, it calculates the similarity measure between two sen-
tences (Sq and Sx) using a linear equation that combines the
obtained similarity measures from steps 6 and 7.
9. The final score obtained from the previous step is checked. If
the similarity score exceed the threshold, a pair of sentences
(Sq, Sx) is considered as plagiarized sentences.
Fig. 2 includes several components such as word-set, context word
xpansion, semantic similarity and syntactic similarity between sen-
ences. The tasks of each component are explained in detail in the
ollowing sections.
(a) The Word Set
Given two sentences Sq and Sx, a “word-set” is produced using
istinct words from the pair of sentences. Let WS = {W1, W2 . . . WN}
enote word set, where N is the number of distinct words in the word
et. The word set between two sentences is obtained through certain
teps as follows:g linguistic knowledge, Expert Systems With Applications (2015),
A. Abdi et al. / Expert Systems With Applications xxx (2015) xxx–xxx 5
ARTICLE IN PRESS
JID: ESWA [m5G;August 4, 2015;21:54]
rd 
ord
in
ord
rity
mil
 bet
ity 
imilar
357
358
359
360
361
362
363
364
365
366
367
368
369
370
Z371
i372
o373
w374
t375
376
377
378
379
380
381
382
383
384
385
i 386
S 387
I
388
S
389
n 390
o 391
t 392Sentence 1
Word order vector 1 
Word order vector 2
Wo
Content w
Stemm
Word order similarity score
W
Simila
Sentence si
 Semantic similarity
 Word order similar
Fig. 2. Sentence s
1. It takes two sentences as its input.
2. By a loop for each word, w, from Sq, it undertakes certain tasks,
which include:
(i) It determines the root of the w (denoted by the RW ) using
the Word Net.
(ii) It checks if the RW appears in the WS, it jumps to the step
2 and continues the loop by the next word from Sq, other-
wise, it jumps to step iii.
(iii) If the RW does not appear in the WS, then the RW is as-
signed to the WS and then it jumps to the step 2 to continue
the loop by the next word from Sq.
(iv) It conducts the same process for Sx.
(b) Semantic similarity between words (SSW)
Semantic similarity between words (Lin, 1998; Tian, Li, Cai, &
hao, 2010) plays an important role in our method. It provides useful
nformation to detect the plagiarism when an author presents some-
ne else’s idea as his or her own words using paraphrasing or re-
ording. The semantic similarity between two words is determined
hrough these steps:
1. It takes two words, W and W , as its input.1 2
Please cite this article as: A. Abdi et al., PDLK: Plagiarism detection usin
http://dx.doi.org/10.1016/j.eswa.2015.07.048Sentence 2
Semantic vector 1 
Semantic vector 2
Set
s expansion
g(word) 
Semantic similarity score
 Net
 score
arity score=
ween sentences +
between sentences 
ity computation.
2. It gets the root of each word using the lexical database, Word
Net.
3. It gets the synonym of each word using the Word Net.
4. It determines the number of synonyms of each word.
5. It determines Least Common Subsume (LCS) of two words and
their length.
6. It computes the similarity score between words using Eqs. (1)
and (2).
We use the following equations to calculate the semantic similar-
ty between two words (Aytar, Shah, & Luo, 2008; Mihalcea, Corley, &
trapparava, 2006; Warin, 2004):
C (w) = 1 − Log(synset(w) + 1)
log(max _w)
(1)
im(w1, w2) =
{
2∗IC(LCS(w1, w2))
IC(w1) + IC(w2)
if w1 = w2
1 if w1 = w2
(2)
Where LCS stands for the least common subsume, max_w is the
umber of words in Word Net, Synset (w) is the number of synonyms
f word w, and IC (w) is the information content of word w based on
he lexical database Word Net.g linguistic knowledge, Expert Systems With Applications (2015),
6 A. Abdi et al. / Expert Systems With Applications xxx (2015) xxx–xxx
ARTICLE IN PRESS
JID: ESWA [m5G;August 4, 2015;21:54]
(c) Semantic similarity between sentences393
We use the semantic-vector approach (Alguliev, Aliguliyev, &394
Mehdiyev, 2011; Li et al., 2006) to measure the semantic similarity395
between sentences. The following tasks are performed to measure396
the semantic similarity between two sentences.397
1. To create the semantic-vector.398
The semantic-vector is created using the word set and cor-399
responding sentence. Each cell of the semantic-vector corre-400
sponds to a word in the word set, so the dimension equals the401
number of words in the word set.402
2. To weight each cell of the semantic-vector.403
Each cell of the semantic-vector is weighted using the calcu-404
lated semantic similarity between words from the word set405
and corresponding sentence. As an example:406
(i) If the word, w, from the word set appears in the sentence407
Sq, the weight of the w in the semantic vector is set to 1.408
Otherwise, go to the next step.409
(ii) If the sentence Sq does not contain the w, then compute the410
similarity score between the w and the words from sen-411
tence Sq using the SSW approach.412
(iii) If exist similarity values, then the weight of the w in the413
semantic-vector is set to the highest similarity value. Oth-414
erwise, go to the next step.415
(iv) If there is no similarity value, then the weight of the w in416
the semantic-vector is set to 0.417
3. The semantic-vector is created for each of the two sentences.418
The semantic similarity measure is computed based on the two419
semantic-vectors. The following equation is used to calculate420
the semantic similarity between sentences:421
Simsemantic(Sq, Sx) =
∑m
j=1
(
w1 j × w2 j
)
√∑m
j=1 w
2
1 j
×
√∑m
j=1 w
2
2 j
(3)
where Sq = (w11, w12, . . . , w1m) and Sx = (w21, w22, . . . , w2m) are the422
semantic vectors of sentences Sq and Sx, respectively; wpj is the423
weight of the jth word in vector Sp, m is the number of words.424
(d) Word-order similarity between sentences425
426
427
428
429
430
431
432
433
434
435
436
437
438
439
440
441
442
443
444
445
446
447
448
449
450
451
452
3. For both sentences the syntactic-vector is created. Then, the 453
syntactic similarity measure is computed based on the two 454
syntactic-vectors. The following equation is used to calculate 455
word-order similarity between sentences: 456
Simword order(Sq, Sx) = 1 −
||O1 − O2||
||O1 + O2|| (4)
where O1 = (d11, d12, · · · , d1m) and O2 = (d21, d22, · · · , d2m) are the 457
syntactic-vectors of sentences Sq and Sx, respectively; dpj is the 458
weight of the jth cell in vector Op. 459
(e) Sentence similarity measurement 460
The similarity measure between two sentences is calculated using 461
a linear equation that combines the semantic and word-order simi- 462
larity. The similarity measure is computed as follows: 463
Simsentences(Sq, Sx) = α · simsemantic(Sq, Sx)
+ (1 − α) · simwordorder(Sq, Sx) (5)
where 0 << 1 is the weighting parameter, specifying the relative 464
contributions to the overall similarity measure from the semantic 465
and syntactic similarity measures. The larger the α, the heavier the 466
weight for the semantic similarity. If = 0.5 the semantic and syntac- 467
tic similarity measures are assumed to be equally important. 468
3.3. Post-processing 469
Given a set of pairs sentences (Sq, Sx), extracted from previous 470
Section 3.2. The post-processing step selects all pair sentences as pla- 471
giarized sentences using the following steps. 472
473
t 474
s 475
i 476
P
w 477
r 478
479
a 480
S 481
L 482
( 483
d 484
o 485
m 486
s 487
i 488
s 489
o 490
s 491
j 492
4 493
494
t 495
P 496
4 497
498
m 499
w 500We use the syntactic-vector approach (Li et al., 2006) to measure
the word-order similarity between sentences. The following tasks
are performed to measure the word-order similarity between two
sentences.
1. To create the syntactic-vector.
The syntactic-vector is created using the word set and corre-
sponding sentence. The dimension of current vector is equal to
the number of words in the word set.
2. To weight each cell of the syntactic-vector.
Unlike the semantic-vector, each cell of the syntactic-vector is
weighted using a unique index. The unique index can be the
index position of the words that appear in the corresponding
sentence. However, the weight of each cell in syntactic-vector
is determined by the following steps:
(i) For each word, w, from the word set. If the w appears in
the sentence Sq the cell in the syntactic-vector is set to the
index position of the corresponding word in the sentence
Sq. Otherwise, go to the next step.
(ii) If the word w does not appear in the sentence Sq, then com-
pute the similarity score between the w and the words from
sentence Sq using the SSW approach.
(iii) If exist similarity values, then the value of the cell is set to
the index position of the word from the sentence Sq with
the highest similarity measure.
(iv) If there is not a similar value between the w and the words
in the sentence Sq, the weight of the cell in the syntactic-
vector is set to 0.Please cite this article as: A. Abdi et al., PDLK: Plagiarism detection usin
http://dx.doi.org/10.1016/j.eswa.2015.07.048Step 1. It removes all pair sentences that do not meet certain cri-
eria. This step removes a pair of sentences whose the similarity mea-
ure under Eq. (5) is below a threshold value. The following formula
s used to judge whether one sentence exists plagiarism or not:
(Sq, Sx) =
{
1, Sim(Sq, Sx) ≥ t1
0, others
(6)
here Sim(Sq, Sx) is Eq. (5) described above, and 1 indicates plagia-
isms, 0 indicates none-plagiarism. The t1 is the threshold value.
Step 2. Let dsrc = {Sq1, Sq2 . . . SqN} represent all sentences from
source document, docsource, where N is the number of sentences.
x indicates a sentence of a suspicious document, docsuspicious.
et ArrPla = {(Sq1, Sx,Valuesim(Sq1,Sx)), (Sq2, Sx,Valuesim(Sq2,Sx)) . . .
SqM, Sx,Valuesim(SqM ,Sx))} represent all the sentences from the
ocsource that their similarity measure with Sx exceeded the thresh-
ld t1, where M ≤ N and Valuesim(SqM ,Sx) indicates the similarity
easure between two sentences SqM and Sx . Based on the previous
tep, a sentence from a suspicious document can have several match-
ng sentences from a source document, and thus generate several
entence pairs. In such case, we select a pair (SqM, Sx,Valuesim(SqM ,Sx))
f sentences from the ArrPla which have a greatest similarity mea-
ure, Valuesim(SqM ,Sx). Finally, the selecteted pair of sentence which
udged as plagiarized sentences is screened.
. Experiments
Our proposed method, PDLK, has been applied for plagiarism de-
ection. We conducted the experiments on the data sets provided by
AN-PC-10 and PAN-PC-11 (http://www.pan.webis.de/).
.1. Data set
In this section, we describe the data used throughout our experi-
ents. For assessment of the performance of the proposed method
e used the datasets provided in PAN-PC-10 and PAN-PC-11. Eachg linguistic knowledge, Expert Systems With Applications (2015),
A. Abdi et al. / Expert Systems With Applications xxx (2015) xxx–xxx 7
ARTICLE IN PRESS
JID: ESWA [m5G;August 4, 2015;21:54]
dataset includes suspicious and corresponding source document sets.501
The PAN-PC-10 corpus comprises 27,073 documents split into a set of502
15,925 suspicious documents and a set of 11,148 source documents.503
The PAN-PC-11corpus also includes 22,186 documents split into a set504
of 11,093 suspicious documents and same number of source docu-505
ments. Both corpuses are based on the 22,730 books from the Project506
Gutenberg (www.gutenberg.org). There are several plagiarism cases507
in the PAN-10 and PAN-11 corpus. The plagiarism cases have been508
generated by human (simulated) or by a computer program (arti-509
ficial) able to obfuscate a text by removing, inserting, or replacing510
words or short phrases by one of its synonyms and antonyms.511
In both corpuses, the main task is to find all plagiarized pas-512
sages in the suspicious documents and, if available, the correspond-513
ing source passages. In order to evaluate the performance of our514
method, we conducted two experiments. In the first experiment, we515
used plagiarized documents plus the original documents from PAN-516
PC-10 corpus for parameter tuning (the threshold and the α). In the517
second experiment, we used the data provided in PAN-PC-11 to com-518
pare our method with the other method and with the systems that519
participated in PAN-PC-11.520
4.2. Evaluation metrics521
In order to evaluate and compare the performance of our pro-522
posed method, we used four various standard measures, macro-523
average Precision (Prec), Recall (Rec), F-measure and granularity524
(gran) (Potthast, Barrón-Cedeño, Eiselt, Stein, & Rosso, 2010; Sta-525
matatos, 2011). In more detail, we define S is the set of plagiarism526
cases and R is the set of detections that found by method. Let r and527
s are the elements in R and S respectively, so the macro-average pre-528
cision and recall are computed using the Eqs. (7) and (8). Precision529
denotes what portion of the detection cases identified by system are530
plagiarism cases. Recall denotes what portion of the plagiarism cases531
are identified by the system.532
prec (S, R) = 1|R|
∑
r∈R
|Us∈S(s ∩ r)|
|r| (7)
533
Rec (S, R) = 1|S|
∑
s∈S
|Ur∈R(s ∩ r)|
|s| (8)
where,534
s ∩ r =
{
s∩r, if r detects s,
∅, otherwise.
535
(
Q3
536
r537
g538
r539
m540
m541
F
w542
a543
v544
o545
p546
l547
F
w548
549
r550
Granularity is described as the ratio of number of identified plagia- 551
rized source section to given plagiarized source section. The granu- 552
larity measure is determined as follows: 553
Gran (S, R) = 1
SR
∑
s∈SR
|RS| (11)
where SR  S cases are identified by detections in R and RS  R are 554
the dictions of a given s: 555
SR = {s|s ∈ S ∧  r ∈ R : r detects s} (12)
556RS = {r|r ∈ R ∧ r detects s} (13)
The domain of gran(S, R) is [1, |R|]. The minimum and ideal gran- 557
ularity value is 1 and |R| indicates the worst case. 558
Moreover, in order to makes a unique ranking among detection 559
methods, the above measures are combined into a single overall score 560
as follows: 561
plagdet (S, R) = F − measure
log2(1 + gran(S, R))
(14)
4.3. Parameter setting 562
The proposed method requires two parameters to be determined 563
before use: a weighting parameter (α) for weighting the signifi- 564
cance between semantic information and syntactic information and a 565
threshold (t1). Both parameters in the current experiment were found 566
using 300 suspicious documents and source documents. All docu- 567
ments are decomposed into some sentences. The stop words from 568
sentences are removed. We ran our proposed method on the cur- 569
rent data set. We used Eqs. (5), (7), (8), (10), (11) and (14). Eq. (5) 570
is used to calculate the similarity measure. Eqs. (7), (8) and (10) are 571
used to calculate the value of precision, recall and F-measure respec- 572
tively. Eq. (11) is used to calculate the granularity. We evaluate our 573
method for each peer (α) between 0.1 and 0.9 with a step of 0.1 574
and (t1) between 0.1 and 1 with a step of 0.1, (e.g. = 0.4, β = 0.7). 575
Table 1 presents our experimental results achieved by using the vari- 576
ous α and the t1 values. We evaluate the results in terms of recall, 577
precision, F-measure and granularity. By analysing the results, we 578
find that the best performance is achieved by a = 0.8 and t1 = 0.6. 579
This α and the t1 produced the recall, precision, F-measure, plagdet 580
and granularity values as follows: 0.685 (recall), 0.802 (precision), 581
0.739 (F-measure), 0.733 (plagdet) and 1.010 (granularity). The best 582
values of Table 2 have been marked in boldface. As a result, using the 583
current data set, we obtained the best result when we use 0.8 as the 584
α 585
α 586
4 587
588
m 589
m 590
s 591
t 592
e 593
2 594
e 595
596
d 597
t 598
o 599
t 600
t 601
c 602
i 603
P 604There is an anti–correlation between precision and recall
Manning et al., 2008). It means the recall drops when the precision
ises and vice versa. In other words, a system attempts for recall will
et lower precision and a system attempts for precision will get lower
ecall. To take into consideration the two metrics together, a single
easure, called F-score, is used. F-score is a statistical measure that
erges both precision and recall. It is calculated as follows:
− measure = 1
α × 1
P
+ (1 − α) 1
R
= (β
2 + 1)P × R
β2 × P + R (9)
here β2 = 1−αα , α ∈ [0, 1], and β2 ∈ [0, ∞]. If a large value (β > 1)
ssigns to the β , it indicates that precision has more priority. If a small
alue (β < 1) assigns to the β , it indicates that recall has more pri-
rity. If β = 1 the precision and recall are assumed to have equally
riority in computing F-score. F-score for β = 1 is computed as fol-
ows:
− measure = 2 × P × R
P + R (10)
here P is precision and R is recall.
Besides precision and recall we used another evaluation met-
ic, granularity, to determine the efficiency of our detection method.Please cite this article as: A. Abdi et al., PDLK: Plagiarism detection usin
http://dx.doi.org/10.1016/j.eswa.2015.07.048value and 0.6 as the t1 value. Therefore, we can recommend this the
and the t1 values for use on the rest of the data set.
.4. Comparison with PAN-PC-11 systems
To confirm the aforementioned results, we validate our proposed
ethod, PDLK, using a comparison of the overall recall, precision, F-
easure and plagdet value obtained by PDLK and the participating
ystems in PAN-PC-11(Overview of the 3rd International Competi-
ion on Plagiarism Detection): (a) the top four systems with the high-
st plagdet value: Sys-1 (Cooke, Gillam, Wrobel, Cooke, & Al-Obaidli,
011), Sys-2 (R. M. A. Nawab, Stevenson, & Clough, 2011), Sys-3 (Rao,
t al., 2011) and Sys-4 (Grman & Ravas, 2011).
We apply our method to the 200 previously unused suspicious
ocuments and source documents only with the α value 0.8 and the
hreshold value 0.6. Table 3 and Fig. 3 present the obtained results
f recall, precision, F-measure and plagdet with the α of 0.8 and the
hreshold of 0.6. The obtained results prove that PDLK outperforms
he other examined methods and that our method produces very
ompetitive results. PDLK is also able to obtain the Plagdet of (0.789)
n comparison with the best existing method, Sys-4, which has the
lagdet of (0.615).g linguistic knowledge, Expert Systems With Applications (2015),
8 A. Abdi et al. / Expert Systems With Applications xxx (2015) xxx–xxx
ARTICLE IN PRESS
JID: ESWA [m5G;August 4, 2015;21:54]
hold a Q4
ults ar
Reca
–
–
–
0.50
0.52
0.55
0.63
0.65
0.68
0.614
0.55
0.46
0.35
–
–
–
605
606
607
608
609
610
611
612
613
614
615
616
617
618
i 619
f 620
a 621
m 622
6 623
a 624
t 625
3 626
m 627
( 628
629
t 630
a 631
t 632
2 633
betweTable 2
Performance of the PDLK against various thres
to space limitations of this paper, a sample res
Weighting (α) Threshold Precision
α =(0.1–0.7) 0.1 –
– –
1 –
α = 0.8 0.1 0.625
0.2 0.659
0.3 0.693
0.4 0.722
0.5 0.775
0.6 0.802
0.7 0.701
0.8 0.612
0.9 0.522
1 0.478
α =(0.9) 0.1 –
– –
1 –
Table 3
Performance comparison between PDLK and PAN-PC-11
systems.
Comparative performance results
System Precision Recall F-measure Plagdet
PDLK 0.902 0.702 0.790 0.789
Sys-1 0.711 0.150 0.248 0.247
Sys-2 0.278 0.089 0.134 0.267
Sys-3 0.454 0.162 0.239 0.199
Sys-4 0.893 0.473 0.618 0.615
4.5. Comparison with related methods (Crm)
In this section, the performance of our method is compared with
other well-known or recently proposed methods. In particular, to
evaluate our methods on PAN-PC-11 data set, we select the follow-
ing methods: Crm-1 (Ekbal et al., 2012), Crm-2 (Wang et al., 2013),
Crm-3 (Grozea, Gehl, & Popescu, 2009), Crm-4 (Kasprzak, Brandejs,
& Kripac, 2009), Crm-5 (Oberreuter, Ríos, & Velásquez, 2010), Crm-
6 (Rodríguez-Torrejón & Martín-Ramos, 2010) and Crm-7 (Suchomel,
Kasprzak, & Brandejs ). These methods have been chosen for compar-
ison because they have achieved the best results on the PAN-PC data
set. The evaluation metrics values are reported in Table 4 and Fig. 4.
4.6. Detailed comparison
From the comparison of the evaluation metrics values for PAN-
PC-11 systems and other methods, PDLK obtains a considerable
Fig. 3. Performance comparisonPlease cite this article as: A. Abdi et al., PDLK: Plagiarism detection usin
http://dx.doi.org/10.1016/j.eswa.2015.07.048nd the α values on PAN-PC-10 data set. (Due
e shown.)
ll F-measure Plagdet Granularity
– – –
– – –
– – –
0 0.556 0.509 1.131
8 0.586 0.543 1.114
2 0.615 0.594 1.050
9 0.678 0.666 1.023
5 0.710 0.695 1.031
5 0.739 0.733 1.010
0.655 0.641 1.029
5 0.582 0.567 1.038
8 0.494 0.309 2.025
0 0.404 0.245 2.133
– – –
– – –
– –
Table 4
Performance comparison between PDLK and other methods.
Comparative performance results
System Precision Recall F-measure Plagdet
PDLK 0.902 0.702 0.790 0.789
Crm-1 0.659 0.190 0.295 0.289
Crm-2 0.858 0.685 0.762 0.757
Crm-3 0.742 0.659 0.698 0.696
Crm-4 0.557 0.697 0.619 0.609
Crm-5 0.867 0.555 0.677 0.674
Crm-6 0.834 0.500 0.626 0.625
Crm-7 0.893 0.552 0.683 0.683
mprovement. Tables 5 and 6 show the improvement of PDLK for all
our evaluation metrics. It is clear that PDLK obtains the high Plagdet
nd outperforms all the other methods. We use the relative improve-
ent ( Our method−Other method
Other method
) × 100, for comparison. In Tables 5 and
‘‘+’’ means the proposed method improves the PAN-PC-11 systems
nd existing methods. Table 5 shows among the PAN-PC-11 systems
he Sys-4 displays the best results compared to Sys-1, Sys-2 and Sys-
. In comparison with the method Sys-4, PDLK improves the perfor-
ance of the Sys-4 method as follows: 1.070% (precision), 48.419%
recall), 27.697% (F-measure) and 28.153% (plagdet).
Table 6 displays among the existing methods the Crm-2 shows
he best results compared to Crm-1, Crm-3, Crm-4, Crm-5, Crm-6
nd Crm-7. In comparison with the method Crm-2, PDLK improves
he performance of the Crm-2 method as follows: 5.168% (precision),
.541% (recall), 3.690% (F-measure) and 4.209% (plagdet).
en PDLK and PAN-PC-11 systems.g linguistic knowledge, Expert Systems With Applications (2015),
A. Abdi et al. / Expert Systems With Applications xxx (2015) xxx–xxx 9
ARTICLE IN PRESS
JID: ESWA [m5G;August 4, 2015;21:54]
n betw
4634
635
t636
T637
s638
I639
o640
t641
A642
g643
m644
p645
m646
o647
r648
a649
t650
p651
4652
s653
654
t655
s 656
p 657
t 658
659
660
661
662
663
664
665
666
667
668
669
670
671
s 672
t 673
s 674
r 675
a 676
s 677
e 678
t 679
s 680
s 681
682
f 683
t 684
t 685
o 686
w 687
m 688
h 689Fig. 4. Performance compariso
Table 5
Performance evaluation compared between the PDLK and PAN-PC-
11 systems.
PDLK improvement (%)
System Precision Recall F-measure Plagdet
Sys-1 +26.968 +367.990 +218.743 +219.634
Sys-2 +224.568 +693.458 +488.249 +195.127
Sys-3 +98.694 +333.981 +231.008 +296.125
Sys-4 +1.070 +48.419 +27.697 +28.153
Table 6
Performance evaluation compared between the PDLK and other
methods.
PDLK improvement (%)
System Precision Recall F-measure Plagdet
Crm-1 +36.857 +268.808 +167.295 +172.792
Crm-2 +5.168 +2.541 +3.690 +4.209
Crm-3 +21.637 +6.638 +13.202 +13.349
Crm-4 +61.906 +0.791 +27.538 +29.437
Crm-5 +4.034 +26.453 +16.642 +17.086
Crm-6 +8.134 +40.324 +26.236 +26.142
Crm-7 +1.023 +27.104 +15.689 +15.514
.7. Discussion
The current section presents the following main findings that ob-
ained from Tables 2– 5. The obtained results validate our method.
his is due to the fact that, (a) it is able to identify the synonym or
imilar words among all sentences using a lexical database, WordNet.
t is very important to consider this aspect (identifying the synonym
r similar words) when measuring the similarity score of sentence-
o-sentence. (b) Given two sentences (i.e., S1 : Alex likes Allen; S2 :
llen likes Alex), unlike other method, our method is able to distin-
uish the meaning of two sentences by using the combination of se-
antic and syntactic information. Moreover, the main feature of the
roposed method is its ability to carry out the sentence matching se-
antically and syntactically. (c) Tables 4 and 6 show that our method
btained good result in precision, recall and F-measure scores. The
esults confirm that our method outperforms the other methods. In
ddition, the results show that the combination of semantic and syn-
actic information; and the semantic word similarity can improve the
erformance.
.8. Influence of semantic similarity between sentences, word-order
imilarity between sentences and semantic similarity between words
To examine the efficiency of semantic similarity between sen-
ences, word-order similarity between sentences and semanticPlease cite this article as: A. Abdi et al., PDLK: Plagiarism detection usin
http://dx.doi.org/10.1016/j.eswa.2015.07.048een PDLK and other methods.
Table 7
Performance of the PDLK against various tests (SOW, SSBW,
SSW).
Various tests
Test 1 (SOW) Test 2 (SSBW) Test 3 (SSW)
Method Precision Recall F-measure
PDLK 0.887 0.823 0.854
PDLK 0.652 0.571 0.609
PDLK 0.239 0.452 0.313
imilarity between words on our proposed method, PDLK, we ap-
lied our method to current dataset (PAN-PC-11) using three different
ests:
1. Test 1 – SOW, to calculate sentence similarity measurement us-
ing semantic similarity between sentences, word-order sim-
ilarity between sentences and semantic similarity between
words.
2. Test 3 – SSBW, to calculate sentence similarity measurement
using semantic similarity between sentences and semantic
similarity between words, without word-order similarity be-
tween sentences.
3. Test 2 – SSW, to calculate sentence similarity measurement
using semantic similarity between sentences and word-order
similarity between sentences, without semantic similarity be-
tween words.
We aim to determine what combination (SOW, SSBW and SSW)
hould be chosen to calculate similarity measure between two sen-
ences. Table 7 and Fig. 5 show the results obtained with recall, preci-
ion and F-measure for different tests. This table shows that the best
esult is obtained with SOW; the mean result is obtained by SSBW;
nd the worst result is obtained by SSW. Based on the evaluation re-
ults using the test cases 1, 2 and 3, the SSBW and SSW have not much
ffect on improving system performance. This is because of the fact
hat (a) SSW is not able to identify the synonym words among all
entences; (b) SSBW does not take into account the word order or
yntactic information to compute text similarity.
The experimental results indicated that the SOW gave higher per-
ormance than the SSBW and SSBW. SOW is capable to improve
he system performance. It calculates similarity measure between
wo sentences using semantic similarity between sentences, word-
rder similarity between sentences and semantic similarity between
ords. Therefore, we employed the SOW to compute the similarity
easure between two sentences in our proposed method to obtain
igh performance.g linguistic knowledge, Expert Systems With Applications (2015),
10 A. Abdi et al. / Expert Systems With Applications xxx (2015) xxx–xxx
ARTICLE IN PRESS
JID: ESWA [m5G;August 4, 2015;21:54]
0.000
0.100
0.200
0.300
0.400
0.500
0.600
0.700
0.800
0.900
1.000
Precision Recall F-measure
SOW SSBW SSW
Fig. 5. Performance of the PDLK against various tests (SOW, SSBW, SSW).
4.9. Runtime complexity analysis690
In this section, we analyse the time complexity of the proposed691
method. The time complexity of the proposed method usually de-692
pends on the number of source documents and their total sentences.693
In order to estimate the amount of computation during the compari-694
son of the suspected document and source documents, we make the695
following assumptions:696
1. Let m be the total number of sentence in suspected document.697
Let n be the total number of sentences in source documents.698
The n is calculated as follows:699
n =
N∑
Countsentence(Doci) (15)
700
701
702
703
704
705
706
707
708
709
Q5
710
711
712
713
714
715
716
717
718
719
720
721
722
723
724
725
726
and Fig. 5, we selected the best combination of them. The main fea- 727
ture of the proposed method is its ability to capture the meaning the 728
meaning in comparison between a source text passage and suspicious 729
text passage, when two passages have same surface text or different 730
words have been used in the passages. This method is also able to 731
detect common actions performed by plagiarists such as the exact 732
copied text, paraphrasing, transformation of sentences and changing 733
of word structure in the sentences. 734
The plagiarism detection evaluation of PDLK is conducted over 735
PAN-PC plagiarism dataset that comprises a wide variety of text 736
lengths. The proposed method is very easy to follow and requires 737
minimal text processing cost. Initially, parameters of PDLK are op- 738
timized over the PAN-PC-10 dataset. Later the actual plagiarism de- 739
tection evaluation is done over PAN-PC-11 dataset. PDLK is compared 740
with the participating system in PAN-PC-11 and the current meth- 741
o 742
g 743
m 744
w 745
t 746
c 747
o 748
1 749
( 750
751
c 752
t 753
m 754
t 755
‘ 756i=1
where
N is the total number of source documents. Count sentence
(Doci) is the total number of sentences in a source document.
Therefore, the time complexity for the comparison between
source documents and suspected one is O(n × m).
2. According to proposed method, after the comparison opera-
tion, we need take one more step which is the post-processing.
In this step the pairs sentences extracted from the previous
step are stored in an array. This step stores a pair of sentences
whose similarity measure is above a threshold value. After
storing pairs sentences in the array, we also need to sort the
elements of the array based on the similarity values from large
to small. The elements of array are sorted using the quick sort.
The total amount of computation to do quick sort in array is
calculated as follows.O(k log2 k) (16)
where k is the length of array.
Thus summing up the above complexities, total time complexity
becomes,
O(n × m × k log2 k) (17)
5. Conclusion and future work
Plagiarism detection in large document collections should be both
efficient and effective. In this paper, we propose a method based
on the linguistic knowledge to detect the plagiarized text. It em-
ploys three similarity metrics to calculate similarity measure be-
tween two sentences: (a) semantic similarity between sentences; (b)
word-order similarity between sentences; and (c) semantic similarity
between words. We analysed the influence of three similarity met-
rics on our proposed method. Due to the results as shown in Table 7
h
m
w
C
a
a
i
m
k
i
w
f
t
o
w
t
Please cite this article as: A. Abdi et al., PDLK: Plagiarism detection usin
http://dx.doi.org/10.1016/j.eswa.2015.07.048ds which are well-known existing methods that are used in pla-
iarism detection. The experimental results display that the perfor-
ance of the proposed method is very competitive when compared
ith other methods. The results also displayed that PDLK improved
he performance of the participating system in PAN-PC-11 and the
urrent methods. We observed that PDLK is able to obtain the plagdet
f 0.789 in comparison with the best participating system in PAN-PC-
1, (Sys-4), which had plagdet of 0.615 and the best existing system,
Crm-2), which had plagdet of 0.757.
As future work we plan to improve the proposed method by
onsidering identifying passive and active sentence, and expanding
he semantic knowledge base, which are limitations of the current
ethod. (a) The method is not able to distinguish between an ac-
ive sentence and a passive sentence. Given a suspicious sentence (A:
Teacher likes his student.’) and two source sentences (B: ‘student likesis teacher.’; C: ‘student is liked by his teacher.’), although the similarity 757
easure between sentences (A and B) and (A and C) is same, but as 758
e can see the meaning of sentence A is more similar to the sentence 759
. hence, it is important to know what passive and active sentences 760
re before comparisons can be drawn. (b) The method used WordNet 761
s the main semantic knowledge base to calculate the semantic sim- 762
larity between words. The comprehensiveness of Word Net is deter- 763
ined by the proportion of words in the text that are covered by its 764
nowledge base. However, the main criticism of WordNet concerns 765
ts limited word coverage to calculate semantic similarity between 766
ords. Obviously, this disadvantage has a negative effect on the per- 767
ormance of our proposed algorithm. To tackle this problem, in addi- 768
ion to WordNet, other knowledge resources, such as Wikipedia and 769
ther large corpus should be used. 770
In addition to the aforementioned future works, the following 771
orks are also considered as future works. In future, we aim to ex- 772
end our method to detect intrinsic plagiarism where there is no 773
g linguistic knowledge, Expert Systems With Applications (2015),
A. Abdi et al. / Expert Systems With Applications xxx (2015) xxx–xxx 11
ARTICLE IN PRESS
JID: ESWA [m5G;August 4, 2015;21:54]
reference collection. Further enhance proposed method by reduction774
the runtime complexity with a parallel programming and adding ad-775
ditional functionality to the method. Also, we aim to add a select-776
ing candidate documents step to method, to select a set of document777
from the corpus that are more similar to the doc suspicious. This step778
can be useful for increasing the overall performance of the proposed779
method.780
References781
Abdi, A., Idris, N., Alguliev, R. M., & Aliguliyev, R. M. (2015). Automatic summarization782
assessment through a combination of semantic and syntactic information for in-783
telligent educational systems. Information Processing & Management, 51, 340–358.784
AdelsonVelskii, M., & Landis, E.M. (1963). An algorithm for the organization of infor-785
mation. DTIC document.786
Alguliev, R. M., Aliguliyev, R. M., & Mehdiyev, C. A. (2011). Sentence selection for787
generic document summarization using an adaptive differential evolution algo-788
rithm. Swarm and Evolutionary Computation, 1, 213–222.789
Aytar, Y., Shah, M., & Luo, J. (2008). Utilizing semantic word similarity measures for790
video retrieval. In Proceedings of the IEEE conference on computer vision and pattern791
recognition, 2008. CVPR 2008. (pp. 1–8). IEEE.792
Baeza-Yates, R.A. (1992). Introduction to data structures and algorithms related to in-793
formation retrieval.794
Bao, J.-P., Shen, J.-Y., Liu, X.-D., & Song, Q.-B. (2003). A survey on natural language text795
copy detection. Journal of Software, 14, 1753–1760.796
Barrón-Cedeño, A., Gupta, P., & Rosso, P. (2013). Methods for cross-language plagiarism797
detection. Knowledge-Based Systems, 50, 211–217.798
Campos, R. A. C., & Martinez, F. J. Z. (2012). Batch source-code plagiarism detection799
using an algorithm for the bounded longest common subsequence problem. In800
Proceedings of the 9th international conference on electrical engineering, computing801
science and automatic control (CCE), 2012 (pp. 1–4). IEEE.802
Ceska, Z. (2008). Plagiarism detection based on singular value decomposition. In Ad-803
vances in natural language processing (pp. 108–119). Springer.804
Ceska, Z., Toman, M., & Jezek, K. (2008). Multilingual plagiarism detection. In Artificial805
intelligence: Methodology, systems, and applications (pp. 83–92). Springer.806
Cooke, N., Gillam, L., Wrobel, P., Cooke, H., & Al-Obaidli, F. (2011). A High-performance807
plagiarism detection system-notebook for PAN at CLEF 2011. In Proceedings of the808
CLEF (notebook papers/labs/workshop).809
Ekbal, A., Saha, S., & Choudhary, G. (2012). Plagiarism detection in text using vec-810
tor space model. In Proceedings of the conference on hybrid intelligent systems, HIS811
(pp. 366–371).812
El-Alfy, E.-S. M., Abdel-Aal, R. E., Al-Khatib, W. G., & Alvi, F. (2015). Boosting paraphrase813
detection through textual similarity metrics with abductive networks. Applied Soft814
Computing, 26, 444–453.815
Foster, C. C. (1965). Information retrieval: Information storage and retrieval using AVL816
trees. In Proceedings of the 20th national ACM conference. (pp. 192–205). ACM.817
Franzke, M., & Streeter, L.A. (2006). Building student summarization, writing and read-818
ing comprehension skills with guided practice and automated feedback. Highlights819
from research at the University of Colorado, a white paper from Pearson Knowl-820
edge Technologies.821
G822
823
G824
825
G826
827
828
H829
830
831
I832
833
I834
835
K836
837
K
Q6
838
839
840
841
L842
843
844
L845
846
M847
848
849
Mihalcea, R., Corley, C., & Strapparava, C. (2006). Corpus-based and knowledge-based 850
measures of text semantic similarity. In Proceedings of the conference on AAAI: Vol. 851
6 (pp. 775–780). 852
Miller, G. A., & Charles, W. G. (1991). Contextual correlates of semantic similarity. Lan- 853
guage and Cognitive Processes, 6, 1–28. 854
Nawab, R., Stevenson, M., & Clough, P. (2010). University of sheffield: Lab report for 855
PAN at CLEF 2010. In Proceedings of the conference on CLEF 2010 LABs and workshops, 856
notebook papers. CLEF. 857
Nawab, R. M. A., Stevenson, M., & Clough, P. (2011). External plagiarism for PAN at CLEF 858
2011. In Proceedings of the 5th international workshop on uncovering plagiarism, au- 859
thorship, and social software misuse. Sheffield. 860
Oberreuter, G., Ríos, S.A., & Velásquez, J.D. (2010). FASTDOCODE: Finding approximated 861
segments of n-grams for document copy detection lab report for PAN at CLEF 2010. 862
Detection using information retrieval and sequence alignment-notebook. 863
Oberreuter, G., & VeláSquez, J. D. (2013). Text mining applied to plagiarism detection: 864
The use of words for detecting deviations in the writing style. Expert Systems with 865
Applications, 40, 3756–3763. 866
Oktoveri, A., Wibowo, A. T., & Barmawi, A. M. (2014). Non-relevant document reduction 867
in anti-plagiarism using asymmetric similarity and AVL tree index. In Proceedings 868
of the 5th International Conference on Intelligent and Advanced Systems (ICIAS), 2014 869
(pp. 1–5). IEEE. 870
Osman, A. H., Salim, N., Binwahlan, M. S., Alteeb, R., & Abuobieda, A. (2012). An im- 871
proved plagiarism detection scheme based on semantic role labeling. Applied Soft 872
Computing, 12, 1493–1502. 873
Paul, M., & Jamal, S. (2015). An improved SRL based plagiarism detection technique 874
using sentence ranking. Procedia Computer Science, 46, 223–230. 875
Potthast, M., Barrón-Cedeño, A., Eiselt, A., Stein, B., & Rosso, P. (2010). Overview of the 876
2nd international competition on plagiarism detection. In Proceedings of the con- 877
ference on CLEF (notebook papers/labs/workshops). 878
Potthast, M., Gollub, T., Hagen, M., Kiesel, J., Michel, M., Oberländer, A., Tippmann, M., 879
Barrón-Cedeno, A., Gupta, P., & Rosso, P. (2012). Overview of the 4th international 880
competition on plagiarism detection. In Proceedings of the conference on CLEF (on- 881
line working notes/labs/workshop). 882
Raghavan, V. V., & Wong, S. M. (1986). A critical analysis of vector space model for 883
information retrieval. Journal of the American Society for information Science, 37, 884
279–287. 885
Rao, S., Gupta, P., Singhal, K., & Majumder, P. (2011). External & intrinsic plagiarism 886
887
888
R 889
890
R 891
892
893
S 894
895
896
S 897
898
899
900
S 901
902
903
S 904
905
906
907
S 908
909
S 910
911
S 912
913
914
T 915
916
917
T 918
Q7919
T 920
921
922
v 923
924
925
W 926
927
928
W 929
930eravand, S., & Ahmadi, M. (2014). An efficient and scalable plagiarism checking system
using Bloom filters. Computers & Electrical Engineering, 40, 1789–1800.
rman, J., & Ravas, R. (2011). Improved implementation for finding text similarities in
large collections of data: Notebook for PAN at CLEF 2011. In Notebook Papers of CLEF.
rozea, C., Gehl, C., & Popescu, M. (2009). ENCOPLOT: Pairwise sequence matching in
linear time applied to plagiarism detection. In Proceedings of the 3rd PAN workshop
on uncovering plagiarism, authorship and social software misuse (p. 10).
ussein, A. S. (2015). Arabic document similarity analysis using n-grams and singu-
lar value decomposition. In Proceedings of the IEEE 9th International Conference on
Research Challenges in Information Science (RCIS), 2015 (pp. 445–455). IEEE.
liopoulos, C. S., & Rahman, M. S. (2009). A new efficient algorithm for computing the
longest common subsequence. Theory of Computing Systems, 45, 355–371.
rving, R. W., & Love, L. (2003). The suffix binary search tree and suffix AVL tree. Journal
of Discrete Algorithms, 1, 387–408.
asprzak, J., Brandejs, M., & Kripac, M. (2009). Finding plagiarism by evaluating docu-
ment similarities. In Proceedings of the conference on SEPLN: Vol. 9 (pp. 24–28).
ong, L., Qi, H., Wang, S., Du, C., Wang, S., & Han, Y. Approaches for candidate docu-
ment retrieval and detailed comparison of plagiarism detection—notebook for PAN
at CLEF 2012. In Proceedings of the conference on CLEF 2012 evaluation labs and
workshop–working notes papers (pp. 17–20).
i, Y., McLean, D., Bandar, Z. A., O’shea, J. D., & Crockett, K. (2006). Sentence similarity
based on semantic nets and corpus statistics. Knowledge and Data Engineering, IEEE
Transactions on, 18, 1138–1150.
in, D. (1998). An information-theoretic definition of similarity. In Proceedings of the
conference on ICML: Vol. 98 (pp. 296–304).
ahdavi, P., Siadati, Z., & Yaghmaee, F. (2014). Automatic external Persian plagiarism
detection using vector space model. In Proceedings of the 4th international confer-
ence on computer and knowledge engineering (ICCKE), 2014 (pp. 697–702). IEEE.Please cite this article as: A. Abdi et al., PDLK: Plagiarism detection usin
http://dx.doi.org/10.1016/j.eswa.2015.07.048detection: VSM & discourse markers based approach-notebook for PAN at CLEF
2011. In Proceedings of the conference on CLEF (notebook papers/labs/workshop).
aphael, A. (2002). Signature extraction for overlap detection in documents. In Pro-
ceedings of the twenty-fifth Australasian conference on computer science: Vol. 4.
odríguez-Torrejón, D., & Martín-Ramos, J. (2010). CoReMo system (contextual refer-
ence monotony) a fast, low cost and high performance plagiarism analyzer system:
Lab report for PAN at CLEF 2010. In Notebook Papers of CLEF.
ánchez-Vega, F., Villatoro-Tello, E., Montes-y-Gomez, M., Villaseñor-Pineda, L., &
Rosso, P. (2013). Determining and characterizing the reused text for plagiarism de-
tection. Expert Systems with Applications, 40, 1804–1813.
arkar, A., Marjit, U., & Biswas, U. (2014). A conceptual model to develop an advanced
plagiarism checking tool based on semantic matching. In Proceedings of the 2nd
International Conference on Business and Information Management (ICBIM), 2014
(pp. 104–108). IEEE.
chleimer, S., Wilkerson, D. S., & Aiken, A. (2003). Winnowing: Local algorithms for
document fingerprinting. In Proceedings of the 2003 ACM SIGMOD international con-
ference on Management of data (pp. 76–85). ACM.
oleman, S., & Purwarianti, A. (2014). Experiments on the Indonesian plagiarism de-
tection using latent semantic analysis. In Proceedings of the 2nd international con-
ference on information and communication technology (ICoICT), 2014 (pp. 413–418).
IEEE.
tamatatos, E. (2011). Plagiarism detection using stopword n-grams. Journal of the
American Society for Information Science and Technology, 62, 2512–2527.
tein, B., & Zu Eissen, S. M. (2006). Near similarity search and plagiarism analysis. From
data and information analysis to knowledge engineering (pp. 430–437). Springer.
uchomel, Š., Kasprzak, J., & Brandejs, M. Three way search engine queries with multi-
feature document comparison for plagiarism detection—notebook for PAN at CLEF
2012. Forner et al (Eds.) ISBN, 978–988.
ian, Y., Li, H., Cai, Q., & Zhao, S. (2010). Measuring the similarity of short texts by word
similarity and tree kernels. In Proceedings of the IEEE youth conference on informa-
tion computing and telecommunications (YC-ICT), 2010 (pp. 363–366). IEEE.
oman, M., Tesar, R., & Jezek, K. (2006). Influence of word normalization on text classi-
fication. Proceedings of InSciT, 354–358.
omasic, A., & Garcia-Molina, H. (1993). Query processing and inverted indices in
shared: Nothing text document information retrieval systems. The VLDB Journal—
The International Journal on Very Large Data Bases, 2, 243–276.
an Rijsbergen, C. J. (1986). (invited paper) A new theoretical framework for informa-
tion retrieval. In Proceedings of the 9th annual international ACM SIGIR conference on
research and development in information retrieval (pp. 194–200). ACM.
ang, S., Qi, H., Kong, L., & Nu, C. (2013). Combination of VSM and Jaccard coefficient
for external plagiarism detection. In Proceedings of the international conference on
machine learning and cybernetics (ICMLC), 2013 : Vol. 4 (pp. 1880–1885). IEEE.
arin, M. (2004). Using wordnet and semantic similarity to disambiguate an ontology.
Retrieved January, 25, 2008.g linguistic knowledge, Expert Systems With Applications (2015),
