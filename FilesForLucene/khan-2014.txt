 
 
 
 
MS (CS) THESIS 
 
 
 
  
 
 
 
 
 
 
 
Use of LDA and POS Tags for Efficient 
Searching of Plagiarized Passages   
 
 
 
By: 
Jamal Ahmad Khan 
Reg#: 613-FBAS/MSCS/F10 
 
 
 
Supervisor: 
Dr. Ali Daud    
Assistant Professor 
    
 
 
Department of Computer Science & Software Engineering, 
Faculty of Basic & Applied Sciences, International Islamic 
University, 
Sector H-10, Islamabad 
 
 
 
 
 
 
 
 
  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
A thesis submitted to the 
Department of Computer Science 
and Software Engineering, 
International Islamic University, Islamabad 
As a partial fulfillment of the requirements 
for the award of the degree of 
MS Computer Science 
 
 
 
 
 
 
 
 
 
 
DECLARATION 
 
I, hereby declare that “Use of LDA and POS Tags for Efficient Searching of 
Plagiarized Passages” neither as a whole nor as a part thereof has been 
copied out from any source. I have developed this project and the 
accompanied report entirely on the basis of my personal efforts made under 
the sincere guidance of my supervisor. No portion of the work presented in 
this report has been submitted in support of any application for any other 
degree or qualification of this or any other university or institution of learning. 
 
 
 
 
Jamal Ahmad Khan 
613-FBAS/MSCS/F-10 
 
 
 
 
ACKNOWLEDGEMENTS 
 
All praises and much gratitude to Almighty Allah, the most merciful and 
glorious, who granted me the potential to work hard and perseverance to 
accomplish this research work. 
 
I would like to sprinkle special thanks on my supervisor Dr. Ali Daud, 
Assistant Professor, who always provided me greatest support and help 
whenever I needed throughout my research work. He was readily available for 
consultation, shared his knowledge with me as well as motivated me for 
successful completion of this research work. 
 
I can’t forget successful support of my affectionate parents and my wife, who 
always have shown desire and prayed for my success as well as provided me 
financial and moral support throughout my life. 
 
I would like to thank all my respectable teachers, sincere friends and all those 
people in the faculty who helped me during this research project. 
 
 
 
 
Jamal Ahmad Khan 
613-FBAS/MSCS/F-10 
 
 
 
 
     PROJECT IN BRIEF 
 
PROJECT TITLE : Use of LDA and POS Tags for Efficient        
Searching of   Plagiarized Passages 
 
UNIVERSITY : Department of Computer Science & Software  
Engineering International Islamic University, 
Islamabad. 
 
 
UNDERTAKEN BY : Jamal Ahmad Khan 
613-FBAS/MSCS/F10 
 
SUPERVISED BY : Dr. Ali Daud 
Assistant Professor Department of Computer 
Science & Software Engineering International 
Islamic University, Islamabad. 
 
 
TOOLS USED : Microsoft C#.Net 2008(for development purpose) 
Hunspellx86.dll (for word stemming and 
synonyms generation) 
OpenNLP.dll (for sentence Tokenization) 
XML Linq (for querying xml documents) 
MS Office 2007 for documentation & presentation 
  
OPERATING  : 
SYSTEM 
 
Windows 7 (32-bit.) 
HP CANON 
Intel (R) Core (TM) 2 Duo CPU T2390 @ 1.87       
GHz, RAM 1 GB 
 
 
START DATE          : January, 2013 
COMPLETION : 
DATE  
 
May, 2014 
 
          
  
  
 
 
 
 
TABLE OF CONTENTS 
ABSTRACT 13 
CHAPTER 1: INTRODUCTION 14 
     1.  Plagiarism 14 
    1.1 Plagiarism Detection 15 
                1.1.1 External Plagiarism Detection 16 
                1.1.2 Intrinsic Plagiarism Detection 17 
CHAPTER 2: LITERATURE REVIEW 18 
    2.1  Literature Review 18 
         2.1.1 Syntactic Features Analysis 18 
         2.1.2 Term Occurrence Analysis 19 
         2.1.3 Word N-Gram Based Analysis 22 
         2.1.4 Using Word-Net based semantic similarity 25 
    2.2 Problem Statement  26 
    2.3 Research Objectives 26 
    2.4 Research Delimitations 26 
CHAPTER 3: RESEARCH METHODOLOGY 27 
    3.1  POS Tagging 27 
         3.1.1 Adjectives 28 
         3.1.2 Adverbs 28 
         3.1.3 Stop Words 28 
         3.1.3 Pronouns 29 
          3.1.4 Nouns 30 
3.1.5 Verbs 30 
 
 
 
 
  3.2 LDA Model 30 
     3.3 Steps Performed for Plagiarism Detection 31 
 3.3.1 Pre Processing 32 
          3.3.2  Post Processing 34 
CHAPTER 4: EXPERIMENTS 39 
 4.1  Data Set 39 
      4.2  Performance Evaluation 40 
          4.2.1 Micro Averaged Precision 41 
          4.2.2 Micro Averaged Recall 41 
          4.2.3 Macro Averaged Precision and Recall 42 
          4.2.4  F - Measures 43 
          4.2.5 Granularity 43 
          4.2.6 Overall 43 
      4.3 Results and Discussion 43 
          4.3.1 Parameter Adjustment Tests 44 
         4.3.2 Baseline Methods 51 
                  4.3.2.1 Stop-Word N Gram based method 51 
                  4.3.2.2 Word N-Gram based method 52 
         4.3.3 Qualitative Comparison 
53 
 
4.3.3.1 Example 1 53 
4.3.3.2 Example 2 56 
4.3.4 Quantitative Comparison 59 
 
 
   
 
 
 
 
 
CHAPTER 5: CANCLUSIONS   62 
5.1  Future Work 63 
REFERENCES 64 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
LIST OF FIGURES 
Figure 1: Shows the way a suspicious document is compared to external 
resources 
Figure 2: Shows a Cluster with ten associated sentence vectors as well as a 
query sentence vector 
Figure 3: A view of both Source and Suspicious XML files 
Figure 4: Showing the Overall Methodology of Proposed Framework 
Figure 5: A document D as a sequence of characters with plagiarized sections S 
and detected cases R 
Figure 6 A: Micro-averaged F-Measure for results of detections with a = 1 
Figure 7 A: Micro-averaged F-Measure for results of detections with a = 5 
Figure 8 A: Micro-averaged F-Measure for results of detections with a = 6 
Figure 9 A: Micro-averaged F-Measure for results of detections with a = 7 
Figure 10 A: Micro-averaged F-Measure for results of detections with a = 8 
Figure 11 A: Overall results for all five values of a 
Figure 12 B; Micro-averaged F-Measure for results of detections with a = 1 
Figure 13 B: Micro-averaged F-Measure for results of detections with a = 5 
Figure 14 B: Micro-averaged F-Measure for results of detections with a = 6 
Figure 15 B: Micro-averaged F-Measure for results of detections with a = 7 
Figure 16 B: Micro-averaged F-Measure for results of detections with a = 8 
Figure 17 B: Overall results for all five values of a 
Figure 18 C: Micro-averaged F-Measure for results of detections with a = 1 
Figure 19 C: Micro-averaged F-Measure for results of detections with a = 5 
Figure 20 C: Micro-averaged F-Measure for results of detections with a = 6 
 
 
 
 
Figure 21 C: Micro-averaged F-Measure for results of detections with a = 7 
Figure 22 C: Micro-averaged F-Measure for results of detections with a = 8 
Figure 23 C: Overall results for all five values of a 
Figure 24: Graphical representation of F-measure score for SWNG Method 
Figure 25: Graphical representation of F-measure score for WNG Method 
Figure 26: Graphical representation of F-measure score for POS-LDA Method 
Figure 27: Graphical representation of Overall score for all three Methods, from 
Left to Right we have SWNG, WNG and POS-LDA 
  
  
  
  
  
  
 
 
 
 
 
 
 
 
 
 
 
 
LIST OF TABLES 
Table 1: Sample syntactic formulae 
Table 2: List of stop-word n-grams 
Table 3: Gibb’s LDA Parameters Settings 
Table 4: Showing Sentences with their respective Tags 
Table 5: List of Stop-Words removed for sentence topic production 
Table 6: Methods Used to for Simulated Plagiarism Cases 
Table 7: Parameters with range of values and the relevant description 
Table 8: The Parameter Values used for Comparison 
Table 9: Example of Text Paraphrasing 
Table 10: A More Complex Example of Text Paraphrasing 
Use of LDA and POS Tags for Efficient Search of Plagiarized Passages 
Department of Computer Science & Software Engineering, International Islamic University, Islamabad 
 
13 
 
ABSTRACT 
In this research thesis a new more efficient method is presented, to address the 
detection of external textual plagiarism between the suspicious and a number of 
source documents. In recent years a number of techniques have been deployed in 
order to cater for the wide spread plagiarism of text documents; but every technique 
has some limitations as the plagiarists keep on discovering new methods to defeat 
these techniques. We compared our method with the classical and most widely used 
word n-gram based method and a recently proposed technique, the stop-word n-gram 
based plagiarism detection methods. Our method is based on the syntactic and 
semantic analysis of text using “Part of Speech” tags and Latent Dirichlet Allocation 
model for topic extraction of text windows i.e. sentences. Our main aim is to find 
framework for the fast search of plagiarized areas of text within the suspicious 
documents which is different from conventional and state of the art methods deployed 
so far. 
Keywords: Plagiarism detection, Extrinsic, LDA, Part of Speech, Syntactic analysis, 
n-gram 
 
 
  
Use of LDA and POS Tags for Efficient Search of Plagiarized Passages 
Department of Computer Science & Software Engineering, International Islamic University, Islamabad 
 
14 
 
   CHAPTER 1 
 
 
 
 
INTRODUCTION 
 
 
 
1.  Plagiarism 
With the Advent of and popularity of search engines like Google, Yahoo, AltaVista 
etc. and online information sources like Wikipedia, the availability of documents on 
different topics has increased. People from different departments of education and 
research download such available documents as a supporting material in their related 
work. However, there is a drawback of such easy availability and access of these 
documents as more and more people either try to copy the parts of whole document or 
the whole document itself to show others that the copied work is related to them 
without giving a reference to the original work. So, basically this copying of other’s 
work, statements or ideas in one’s own document is called plagiarism. In Latin the 
word “plagiarius” means kidnapper from which word “plagiarism” is derived. It is 
defined as “the passing off of another person's work as if it were one's own, by 
claiming credit for something that was actually done by someone else” [1].  
Use of LDA and POS Tags for Efficient Search of Plagiarized Passages 
Department of Computer Science & Software Engineering, International Islamic University, Islamabad 
 
15 
The Impact of plagiarism is huge at educational level as every year more and more 
graduating and under graduate students try to get involved in online plagiarism for the 
submission of their assignments as this is an easier and shorter way for them; but as a 
result of this new thoughts and ideas never get revealed as students copy and paste 
answers to questions or the solutions to the problems as is in their own assignments. 
Same behavior is seen at higher research level where some researchers try to publish 
someone else’s work as their own with very minor changes. Moreover, the credit to 
original author is denied. As the cases of plagiarism increased with time; universities 
and other research institutes bothered to find a solution to this problem. Over the 
years different algorithms and solutions have been proposed to find plagiarism within 
the documents written in different languages, but still there are different 
complications for this task of plagiarism detection because the task itself gets 
complicated when people have to reference some previous work in their scholarly 
articles like equations, quotes, definitions or even algorithms etc. so the false positive 
results may increase. 
Hence there is always a need of some standard approach to detect plagiarized 
passages within a document. In the next section we will have a look at the plagiarism 
detection techniques. 
1.1  Plagiarism Detection 
Plagiarism detection task can be divided into two broader categories one is called as 
External Plagiarism Detection and the other is called as Intrinsic Plagiarism detection. 
Both methods differ from each other by definition and methodology and both have 
different complications. As our main focus in this paper will be mainly over Intrinsic 
Plagiarism Detection but in this section we will have a look at both of these methods. 
Use of LDA and POS Tags for Efficient Search of Plagiarized Passages 
Department of Computer Science & Software Engineering, International Islamic University, Islamabad 
 
16 
1.1.1 External Plagiarism Detection 
This approach deals with the detection methods applied when we have some external 
reference to the suspicious document available from which text may have been 
copied. Also this type of reference oriented search is highly dependent on topic of 
document or the set of keywords used in suspicious document.  This method can be 
divided into two additional classes; one which works at the local level of user’s PC 
and analyzes the local archives of source documents or carry out internet exploring, 
the second approach is the one which facilitates the client to upload the suspicious 
documents at remote server and the plagiarism detection processes takes place 
remotely at other machine [2].  
Figure 1 show how the suspicious documents are queried and compared in the form of 
segments from both local and online available resources that are referenced in the 
suspicious document.  Also the two very basic techniques for plagiarism detection are 
shown. However, there are also a number of other plagiarism techniques that can be 
applied in this specific case.  
  
 
 
 
 
            
      Figure 1: Shows the way a suspicious document is compared to external resources [2] 
Use of LDA and POS Tags for Efficient Search of Plagiarized Passages 
Department of Computer Science & Software Engineering, International Islamic University, Islamabad 
 
17 
In case of online plagiarism detection one may identify  several  suspicious  sentences  from  
the write-up and feed them one by one as a query to a search  engine  to  obtain  a  set  of  
documents.  Then human  reviewers  can  manually  examine  whether these  documents  are  
truly  the  sources  of  the suspicious  sentences.  While it is quite straightforward and 
effective technique, but there are some limitations to this technique, both in case of query 
length and the methodology used.  
1.1.2 Intrinsic Plagiarism Detection 
Intrinsic plagiarism detection is a very recent technique used now days in order to detect the 
plagiarism within a document when there is no reference corpus is available. Different 
Stylometric features are used to detect plagiarized text pieces or sentences in a suspicious 
document. Some of the other intrinsic types where the Stylometric methods can be applied are 
Authorship attribution and Self Plagiarism; another field where Stylometric features can be 
used is forgery in legal documents.  
 
 
 
 
 
 
 
 
 
 
 
 
Use of LDA and POS Tags for Efficient Search of Plagiarized Passages 
Department of Computer Science & Software Engineering, International Islamic University, Islamabad 
 
18 
 
 
CHAPTER 2 
 
 
 
 
LITERATURE REVIEW 
 
 
2.1   LITERATURE REVIEW 
This chapter will focus on some of the latest and classical techniques and methods 
used in the field of extrinsic plagiarism detection; here we have reviewed a total of ten 
research papers which have been divided into different plagiarism detection 
categories.  
2.1.1 Syntactic Features Analysis 
The authors Ozlem Uzuner et al. [4] offered a new approach to explore set of 
syntactic arrangement of text that contains information different ways of writing and 
showed how this information can help to find similarities between two texts. They 
observed how different authors try to express same content in a translated and 
paraphrased passages using different syntactic arrangements of words. Following is 
the methodology adopted by the authors. 
 The authors chose one book from each title and used this book  
Use of LDA and POS Tags for Efficient Search of Plagiarized Passages 
Department of Computer Science & Software Engineering, International Islamic University, Islamabad 
 
19 
 They trained a model that captured the syntactic elements of expression used in this 
title. 
  Then the remaining books paraphrasing the title were used as the test set.  
Following is an example of how the authors got syntactic information from different 
sentences. 
Table 1: Sample syntactic formulae [4] 
 
 
2.1.2 Term Occurrence Analysis 
Mario Zechner et al. [5] used the vector space model for the detection of both external 
and intrinsic plagiarism detection. Their aim was to speed up the search in high 
dimensional vector spaces at the cost of precision. They used the following three step 
process to achieve the task. 
 First each passage of each document in the reference corpus was vectorized and the 
corpus was partitioned into a vector space where each exclusive word in the reference 
corpus was signified as a single dimension and each sentence was a vector instead of 
representing a whole document as a vector,  
 Second was the term vectorization of the sentences of a suspicious document and 
finding each passage’s nearest neighbor(s) in the reference corpus vector space where 
as the detection of plagiarism for each suspicious case depends upon its nearest 
neighbor catalog by cosine similarity index.  
Use of LDA and POS Tags for Efficient Search of Plagiarized Passages 
Department of Computer Science & Software Engineering, International Islamic University, Islamabad 
 
20 
 Third one was based on post processing of plagiarized passages into single block.  
 
 
    (1) 
 
 
 
 
Figure 2: Shows a group of 10 related sentence vectors and query sentence vector [5] 
 
Efstathios Stamatatos [6] showed how the stop-words n-grams can play a vital role for 
plagiarism detection since these stop-words expose syntactic resemblance among 
suspicious and source documents. The list of used 50 stop words is mentioned below. 
Table 2. List of stop-word n-grams[5] 
 
 
 
 
Following is the 
methodology of proposed framework. 
 Given a document and a list of stop-words, the passage is condensed to the 
appearances of the stop-words in the document.  All other words are removed.  
Use of LDA and POS Tags for Efficient Search of Plagiarized Passages 
Department of Computer Science & Software Engineering, International Islamic University, Islamabad 
 
21 
 Find common n-grams of stop-words between the suspicious and the source 
documents where the length of matched n-grams be n1 and any n< n1 will not 
considered plagiarized. 
 A step to escape from any coincidental match of different passages with same stop-
words. Where the most frequent list of n-grams is sorted out from documents in the 
form of list named C. Following is the equation for this task.  
           g 2-n1<C)maxseq(g, 1-n1<C)member(g, :ds)}P(n1,dx)P(n1,{     (2) 
Where P(n1,  dx)  and  P(n1,  ds)  are  the  related  documents profiles  which contains 
stop-words n-grams  of  length  n1 and the two methods member(g,C) and 
maxseq(g,C) return the stop-words of the n-gram  g  that  exist in C  and  the  
maximum  sequence  of  terms  of  g  that  are in  C, respectively. Method to calculate 
the plagiarized passage boundaries in both the suspicious and the source documents 
using the following equation. 
                               (3) 
Where M1 is the index set of related n-grams say {(1,1),(2,4)….} in both documents 
profiles and mi is the member index. The methods abs and diff gives the fixed value 
and the variation (derivative) and θg is a threshold that allows relatively tiny spaces to 
be integrated in the identified suspicious text. 
In the end detected passage similarity depends on the following equation             
                                         (4) 
 
 
 
 
Use of LDA and POS Tags for Efficient Search of Plagiarized Passages 
Department of Computer Science & Software Engineering, International Islamic University, Islamabad 
 
22 
2.1.3  Word N-Gram Based Analysis 
            Markus Muhr [7] et al. presented a mixture system for the PAN test at CLEF 2010. 
Their system executes plagiarism detection for both translated, non-translated 
plagiarized archives of texts.  
 As the first step for non English documents like the ones in German and Spanish; the 
authors translated these using Europarl aligned corpus  
 Next the source documents were transformed into overlapping blocks of 40 tokens 
and these blocks along with offset, length and document id were indexed by using an 
indexing and query software called “Lucene”.  
 After that suspicious documents were tokenized and overlapping blocks of tokens 
were transformed to boolean queries and terms of the Boolean query were sorted by 
their corpus frequency in increasing order.  
 Next they merged all sequence matches in the suspicious document that point to the 
same source document. And as the blocks were already indexed, they escaped from 
the post processing step. 
            Sobha Lalitha Devi et al. [8] introduced method for detection of external plagiarism in 
PAN-10 competition. The simple algorithm steps are as follows. 
 First the  identification of similar  plagiarized segments for a suspicious and source 
documents using  Vector  Space  Model  (VSM)  and  cosine  likeness measure where 
the  documents are arranged as vector of terms and a term is a sequence  of  four 
successive  terms,  called  chunks.   
 The value of the chunk in the vector space is determined by term frequency and 
inverse document frequency. For similarity measure the cosine similarity formula was 
used as shown in the equation 1. 
Use of LDA and POS Tags for Efficient Search of Plagiarized Passages 
Department of Computer Science & Software Engineering, International Islamic University, Islamabad 
 
23 
 Finding out the plagiarized texts in the suspicious document using Chunk ratio (R). 
Whose formula is R = square(C)/(cosine  score).  C = frequency of common used 
words in suspicious  and source documents. 
           Gupta Parth et al. [9] used the conventional n-gram approach for the task of external 
plagiarism detection. The methodology they adopted consists of following steps. 
 The suspicious documents are tagged with Lingpipe NE Tagger which is java based 
software for getting Name-Entity relationship tagging. Then,  
 Then non-overlaping n-grams where n=9 which enclose minimum one  Named Entity 
(NE) are queried into indexed source documents using an indexing and query 
software. And top 5 returned documents were retrieved. 
  Overlapping n-grams (n=7) of suspicious documents were compared with those of 
source documents. If these terms match, were found as plagiarized texts.  
 Then all those chunks less than 500 chars apart were merged as a single plagiarized 
area. 
           Clara Vania and Mirna Adriani [10] proposed a model for external plagiarism 
detection in both English and Non- English sources. The plagiarism cases are 
identified by the  number  of  overlapped  words (n-grams)  between suspicious and 
source passages. The salient features of proposed methodology are as follows. 
 Identification  the  language  used  in  the  documents  using  an  automatic language  
identifier. Then translated  all  non-English  documents  into  English  using  an  
online  language translator. 
 Indexing the overall source documents and use suspicious documents as queries using 
Lucene open source software to index and retrieve the corpus. 
Use of LDA and POS Tags for Efficient Search of Plagiarized Passages 
Department of Computer Science & Software Engineering, International Islamic University, Islamabad 
 
24 
 They  divided  the  top  10  source  documents  and  suspicious  documents  into  
small passages where  each  passage  contained  20  sentences and indexed and  
retrieved passages  that  are  similar  to  the  sections  found  in  the  source  
documents. 
 Computed the overlapping n-grams between two passages and chose pair passages 
that have at least three overlapping 6-grams. 
            Sameer Rao et al. [11] proposed a plagiarism detection system which can detect 
External as well as Intrinsic Plagiarism using VSM & Discourse Markers based 
Approach. They also used the document translation method for non English 
documents. The methodology proposed by them is as follows. 
 Convert all the non-English documents using Google Language Identifier and then 
translate all non-English documents into English using Google Translator API. 
 All the source documents are listed and each suspicious document is given as query to 
this index and analyzed top 250 source documents in the ranked list as candidate or 
those with Similarity Score greater than 0.01. 
 Then they took a 7-gram from the suspicious document and searched for it in source 
document. If it matches then from that matching point they took 25 words window in 
both suspicious and source documents and compute the similarity index.  
 Removed stop-words from that word window. 
 In case eight successive windows have similarity measure under 0.50 which is 50% of 
words matched in the window, the algorithm stops. 
 Merged the successive plagiarism cases if they are 500 characters distant. 
Gabriel Oberreuter et al. [12] proposed two different strategies for both external and 
intrinsic plagiarism detection using outlier detection approach for detecting changes in 
Use of LDA and POS Tags for Efficient Search of Plagiarized Passages 
Department of Computer Science & Software Engineering, International Islamic University, Islamabad 
 
25 
the author’s style for intrinsic plagiarism and for external plagiarism they used the 
following steps. 
 They first executed a plagiarism search space reduction method by removing stop-
words, and considered word 4-grams.  
 If two documents have at least two word 4-grams coincidences close enough as to be 
in the same paragraph, the documents are given to the next phase. Otherwise the pair 
is discarded. 
 For the exhaustive search, word tri-grams were used and stop-words were not 
removed.  
2.1.4 Using Word-Net based semantic similarity  
Yurii Palkovskii et al. [13] used Word-Net to counter for the paraphrased cases in 
external plagiarism detection. They performed the semantic text comparison in 
following steps for plagiarism detection. Given   two sentences X and Y, they denoted 
m and n as the lengths of X and Y and the   approach   to   find-out   the  semantic   
resemblance   between   two   sentences is given below.    
 Tokenization and word stemming. 
 Perform part of speech tagging. 
 Word sense disambiguation. 
 Constructing   a matrix   R[m,   n]  which shows semantic similarity of   among each 
word pairs. 
 The   related   results   from   the   previous   step   are   merged  into   a   single 
similarity value for two sentences using the following equation. 
                                         2  x  Match(X,Y) / X + Y                        (4) 
Use of LDA and POS Tags for Efficient Search of Plagiarized Passages 
Department of Computer Science & Software Engineering, International Islamic University, Islamabad 
 
26 
2.2 Problem Statement 
Now a days new plagiarism methods are discovered for academic purposes and the 
use of word n-grams and stop-words n-grams for external plagiarism detection can 
become unfruitful as the n-grams do not take into account semantic similarity and will 
fail in case someone change the words with synonyms. Stop-words n-grams only 
could prove worthless in case someone remove or alter stop-words from plagiarized 
passages. This motivated us to propose a new improved technique for finding external 
plagiarism detection.  
 
2.3 Research Objectives 
 Finding external plagiarism by using part of speech tags and querying the suspicious 
documents passages using generated topics through Latent Dirichilet Allocation 
model. 
 Improving detection through multiple experiments. 
 Comparing our results with other detection models over the self-built data set. 
 Show both qualitative and quantitative results in form of graphs and tables. 
 Finding limitations of our model through obtained results. 
 
2.4 Research Delimitations 
This research only focuses on finding plagiarized passages from pre-nominated source 
and suspicious documents present in local directories. We have not considered 
searching for related source documents from large text documents archives first and 
then finding plagiarism in suspicious document. 
 
 
Use of LDA and POS Tags for Efficient Search of Plagiarized Passages 
Department of Computer Science & Software Engineering, International Islamic University, Islamabad 
 
27 
 
CHAPTER 3 
 
 
 
 
RESEARCH METHODOLOGY 
 
In this section we will present our model “Use of LDA and PAS Tags for Efficient 
Search of Plagiarized Passages”. Our system will syntactically analyze the word 
windows in order to address the above problems that may occur while using n-grams 
profile and in case of changing the order of words in plagiarized passages. Replacing 
words with their respective POS tags will allow us to not to be dependent on the 
software like Word-Net and hence attaining the target of rapid searching of 
plagiarized chunks within a matrix space. We will also use Latent Dirichlet  
Allocation model along with POS tags (of text windows) to get the actual text context. 
Let’s discuss these two components of proposed framework in detail.  
3.1  POS Tagging 
POS tagging which is, also known as grammatical tagging, by this method we mark a 
word or term in a text document as its related part of speech. This approach is based 
on the term classification and the context in which it’s used i.e. association with 
neighboring words in the same phrase, sentence, or article. POS tagging is a difficult 
task than merely having a catalog of terms and their related parts of speech, because 
Use of LDA and POS Tags for Efficient Search of Plagiarized Passages 
Department of Computer Science & Software Engineering, International Islamic University, Islamabad 
 
28 
some terms can express more than one part of speech when written in different 
situations, and also because some are difficult to express. However the most common 
parts of speech are 9 which are noun, verb, article, adjective, preposition, adverb, 
pronoun, conjunction and interjection [14].  
Here we have divided each type of POS in different groups and allocated a decimal 
number for each one in order to make the post processing and comparison more easier 
and faster. Let’s discuss some of the important POS that we have used in our project 
in details. 
3.1.1 Adjectives  
An adjective is used to change, qualify, describe or quantify a noun or a pronoun. 
Tags assigned to different forms are JJ, JJR and JJS. 
3.1.2 Adverbs   
Like the adjective for nouns and pronouns; an adverb modifies a verb, an adjective, 
another adverb, a sentence, or a condition. An adverb specify mode, time, place, 
reason, or degree and answers questions such as "how," "when," "where," "how 
much". A class of adverbs can be recognized by their "ly" suffix. . Tags assigned to 
different forms are RB, RBR and RBS. 
3.1.3 Stop Words 
The following different POS cotegoris are identified as main stop word category by 
our algorithm. 
 Conjuction: You can use a conjunction to join words or sentences e.g. I ate 
the apple "and" the mango. Tags assigned to different forms are CC, IN and 
TO. 
 
Use of LDA and POS Tags for Efficient Search of Plagiarized Passages 
Department of Computer Science & Software Engineering, International Islamic University, Islamabad 
 
29 
 Determiner: This category express terms “a(n),  every, no and  the, another, 
any, and, some,  each,  either,  neither, that,  these,  this and  those. Tags 
assigned to different forms are DT and EX. 
 
 Cardinals: Numerals and digits are included in this category.  Tag assigned to 
this category is CD.   
 
 Foreign: Words which are non-english but are included from other languages 
e.g. noir, beta, gama; Tag assigned to this category is FW. 
 
 Posessive Ending: When we have  possessive ending at the end of nouns  in  
's  which is usually split from noun or pronoun by tagging algorithm. Tag 
assigned to this category is POS 
 
3.1.3 Pronouns 
The following types of pronouns are taken into account. 
 Personal Pronoun: Personal  pronouns  are represented  without the  taking 
into account for  case difference e.g. I, me,  you,  he, him, etc. Tag assigned to 
this category is PRP. 
 Posessive Pronoun: The adjectival possessive forms my,  your, his, her, its, 
our and  their, on the other hand, are tagged  PRP$.  
 
 
 
 
Use of LDA and POS Tags for Efficient Search of Plagiarized Passages 
Department of Computer Science & Software Engineering, International Islamic University, Islamabad 
 
30 
3.1.4 Nouns 
Indicates the name of any thing, person, personality and relation e.g. parents, tables, 
jamal etc. Tags assigned to this category according to types are NN, NNP, NNS and 
NNPS. 
3.1.5 Verbs  
This tag subsumes any work performed by any thing or person; this includes 
Imperative it has following forms e.g. Do it; present  participle-VBG I am working; 
past participle e.g. gone, done; 3rd person singular e.g. goes, cares. Tags assigned to 
this category according to types are VB, VBD, VBG, VBN, VBP and VBZ. 
 Modal verb: This type  takes into account all verbs that  do not have an -s 
closing in the third person  singular  present e.g. can, could, (dare),  may,  
might,  must,  ought, shall,  should,  will,  would. Tags assigned to this 
category according to types are WP, WP$ and WDT. 
3.2 LDA Model 
Latent Dirichlet Allocation is a prevailing algorithm that can learn by itself by 
clustering group of words into "topics" and documents into fusion of  “topics”. This 
model is applied successfully in different scientific areas. We can describe a topic 
model as a Bayesian model that links probability allocation over the topics in each 
document. Topics are in fact distributions over words [15]. An example of how LDA 
works is presented below by a number of sentences. 
1. I like to eat mangoes and apples. 
2. I ate an apple and bread for my breakfast. 
3. Tigers and puppies are pretty. 
4. My brother bought a cat today. 
5. Look at this pretty cat chewing on a piece of bread. 
 
So LDA is simply the model to find topics hidden in these sentences. 
 
 Sentences 1 and 2: 100% Topic A 
Use of LDA and POS Tags for Efficient Search of Plagiarized Passages 
Department of Computer Science & Software Engineering, International Islamic University, Islamabad 
 
31 
 Sentences 3 and 4: 100% Topic B 
 Sentence 5: 60% Topic A, 40% Topic B 
 Topic A: 30% apples, 15% mangoes, 10% breakfast, 10% chewing … (here we can 
guess that first is related to food) 
 Topic B: 30% cats, 20% tigers, 20% pretty, 15% puppies, (here we can guess that 
first is related to animals) 
 
We have used Gibb’s LDA written in C++ in our software for generating topics 
ranging from 1 to 10 for each sentence depending on the type of configuration we 
choose. The parameters that are set for generation of topics by Gibb’s LDA are 
following. 
 
Table 3. Gibb’s LDA Parameters Settings 
 
Parameter Description  Values 
-est Estimate the LDA model from scratch -est 
-ntopics 
The number of topics to produce for each 
sentence 
Values from 1 to 
10 per sentence 
-niters The number of Gibbs sampling iterations 100 to 150 
-twords The number of most likely words for each topic 1 
-dfile File name to write topics Data.txt 
 
 
3.3 Steps Performed for Plagiarism Detection  
 
Here we will discuss the methodology steps of proposed framework that we have 
proposed. The following are the steps that the framework will use for plagiarism 
detection. The task is divided into two steps i.e. Pre-Processing of source and 
suspicious documents and Post-Processing to find out the plagiarism cases. 
 
 
Use of LDA and POS Tags for Efficient Search of Plagiarized Passages 
Department of Computer Science & Software Engineering, International Islamic University, Islamabad 
 
32 
3.3.1 Pre Processing 
1. We will first assign POS tags to every sentence present in both source and 
suspicious documents as shown in following table. This step of tokenization 
and tagging will be done using Open-NLP library for windows. Sentences 
with length shorter than five words are not included. 
Table 4. Showing Sentences with their respective Tags  
No. Sentence  POS Tags 
1 Maria can lastly put some cash in the 
bank 
NNP MD RB VB DT NN 
IN NN 
2 Finally, Martha can lay some money in 
bank 
RB NNP MD VB DT NN 
IN  NN 
3 Nora sent the book to Richard NNP VBD NN TO NNP 
4 Noor  lead the volume to Rashid NNP VBD NN TO NNP 
5 That’s her mistake DT PRP NN 
6 That’s her fault DT PRP NN 
2. Each tag will be represented by a one unique number e.g. NN will be 
represented by ‘1’ and VB will be represented by ‘2’ and so on. Hence we will 
get an integer string or array of variable length for each sentence’s POS tags 
window.  This step will fasten up the post-processing.    
3. To find out the topic of each sentence we first remove stop-words from it. For 
this purpose we use the regular expression which returns us a sentence free of 
stop-words. The regular expression used is given below. 
 
 
 
Use of LDA and POS Tags for Efficient Search of Plagiarized Passages 
Department of Computer Science & Software Engineering, International Islamic University, Islamabad 
 
33 
Table 5. List of Stop-Words removed for sentence topic production 
the many can't will on my his was be 
your more isn't would how which she were there 
mine too couldn't shall very with they are do 
you've haven't shalln't should much where them of did 
I've shouldn't don't not we her their an done 
into hasn't there's nor I him is also so 
being hadn't that's no you a this in all 
must wasn't n't yes he on these from to 
that as by had has if at may might 
but ought n't have who it it's what and 
been or for „s how can could if else 
 
4. Then the sentence will be assigned a topic tag using the LDA which will be 
used while querying the suspicious sentences. The number of topics produced 
for each sentence depends on the threshold value α chosen by user; where α is 
the number of words chosen for each topic assignment. By default its value 
will be 1 (topic per sentence) for suspicious document and 2 (topics per 
sentence) for source documents. 
5. The synonyms for each topic will also be generated in case of suspicious 
documents pre-processing. 
6. Each sentence along with topics and POS tags will be written to an xml file 
along with its index number. This index number is used in case to back track 
the text from documents. 
 
 
Use of LDA and POS Tags for Efficient Search of Plagiarized Passages 
Department of Computer Science & Software Engineering, International Islamic University, Islamabad 
 
34 
 
 
 
 
 
 
Figure 3: A view of both Source and Suspicious XML files 
Once the xml profiles are complete for each document, we are ready for the next step 
to query the xml documents for plagiarized sentences. 
3.3.2  Post Processing  
Each sentence topic Ts in source XML file x ϵ Xs is queried for the matching topic 
(this also includes topic synonyms) Tu in the Suspicious XML file Xu using LINQ. 
Where Xs is the set of all source files. 
1. Each matched result r is 3-tuple <i, POS-Su, POS-Sr> which includes index i 
of sentence Si in suspicious document and the array of POS Tags decimal 
values POS-Su for suspicious sentence and POS-Sr for source sentence. 
Where r ϵ R and R = (Ts, Xs) ∩ (Tu, Xu)        (5) 
2. Now the decision for the sentence being plagiarized or not depends on the 
following factors. 
 Suspicious XML 
<Txt2> 
    <syn>duty-bound</syn> 
    <syn>obligated</syn> 
    <syn>obliged</syn> 
    <syn>religion</syn> 
    <syn>religious belief</syn> 
    <syn>belief</syn> 
    <syn>theological virtue</syn> 
    <syn>supernatural virtue</syn> 
    <syn>state</syn> 
    <syn>honesty</syn> 
    <syn>honestness</syn> 
    <syn>integrity</syn> 
    <TAGS>6 54 8 4 22 0 33 45 44 7 4 
35 5 35 </TAGS> 
       Source XML 
<Txt2> 
    <syn>duty-bound</syn> 
    <syn>Brave</syn> 
    <TAGS>6 54 8 4 22 0 33 45 44 7 4 
35 5 35 </TAGS> 
  </Txt2> 
<Txt3> 
    <syn>light</syn> 
    <syn>dangers</syn> 
    <TAGS>35 45 4 9 21 0 6 33 8 45 
21 0 5 22 36 45 44 36 0 6 44 33 
</TAGS> 
  </Txt3> 
   
Use of LDA and POS Tags for Efficient Search of Plagiarized Passages 
Department of Computer Science & Software Engineering, International Islamic University, Islamabad 
 
35 
a. Length  L longest sequence of matched indices Ls in POS-Su and POS-Sr; and 
the Length ratio Lr of POS-Su and POS-Sr   
b. Indices of stop-word sequence matched PS in POS-Su and POS-Sr   
c. Indices of nouns sequence matched PN in POS-Su and POS-Sr   
d. Indices of verbs sequence matched PV in POS-Su and POS-Sr   
e. Indices adverbs sequence matched PA in POS-Su and POS-Sr   
f. Indices of pronouns sequence matched PP in POS-Su and POS-Sr   
g. Indices of adjective sequence matched PD in POS-Su and POS-Sr   
The formulas for deciding whether the sentence Si is plagiarized or not are as follows.  
                   Lr =  abs( (| |,| |)
 ((| |,| |)  
 )               (6) 
                                               
                   L= (|POS-Su ∩ POS-Sr|)                                               (7)              
  
The following sub-equation (8) will be used in the main equation (9) 
 
tf = ((L ≥ 2βl) ˄ ((|PA| ≥ βa)  ˅ (|PP| ≥ βp) ˅ (|PD| ≥ βd) )) (8) 
 
TF = ((L ≥ βl) ˄ (Lr ≤ 2)) ˄ ( ((|PS| ≥ βs) ˄ (|PN| ≥ βn)) ˅ ((|PS| ≥ βs) ˄ (|PV| ≥ βv)) ˅ 
((|PN| ≥ βn) ˄ (|PV| ≥ βv)) ˅ (tf = true) )                                                      (9) 
Where βl, βs, βn, βv, βa, βp and βd are the threshold values for matching POS index 
sequence Length, percentage of matched stop-words sequence,  percentage of 
matched nouns sequence, percentage of matched verbs sequence, percentage of 
Use of LDA and POS Tags for Efficient Search of Plagiarized Passages 
Department of Computer Science & Software Engineering, International Islamic University, Islamabad 
 
36 
matched adverbs sequence, percentage of matched pronouns sequence and percentage 
of matched adjectives sequence respectively. tf and TF are both Boolean values. 
An example of how indices are matched in two suspicious and source arrays is as 
follows. 
POS-Sr = { 35 45 27 4 36 45 35 0 5 21 35 0 27 46 21 7 41 4 21 6 35 } 
POS-Su = { 35 46 6 4 0 27 4 36 45 35 0 5 21 35 0 27 46 21 7 41 4 35 21 6 35 36 0 5 21 
36 6 36 } 
From the above two arrays we have 
Longest sequence match Ls = {27 4 36 45 35 0 5 21 35 0 27 46 21 7 41 4} 
Length of Ls is L =  16  
Length Ratio among POS-Sr and POS-Su is  Lr = abs(32 / 20)  
Total nouns in POS-Su = 9 
Matching Noun indices in POS-Sr and POS-Su is PN = {35 36 35 35 35} 
Length of PN = 5 
The matching percentage for nouns |PN| in POS-Su = (5 / 9) x 100 
In this way we will calculate all the other parameters.  
3. Once it’s decided that Si is a plagiarized sentence; the plagiarized indices Pi 
are retrieved where Pi ⊆ POS-Su.  
Use of LDA and POS Tags for Efficient Search of Plagiarized Passages 
Department of Computer Science & Software Engineering, International Islamic University, Islamabad 
 
37 
 Pi = Ls ∪ PN ∪ PS ∪ PV ∪ PA ∪ PD ∪ PP (11) 
Once we find the set of plagiarized indices in POS-Su; we calculate the lp which is 
the percentage between the Length of Pi and Length of POS-Su. 
lp  =   
| |
| |
 × 100  (12) 
if  lp  ≥ βlp then all the indices of POS-Su are set as plagiarized. This means the 
whole sentence Si will be considered plagiarized. 
4. The set of plagiarized sentences <S1, S2,…. Si….. Sn > is written in another 
XML file for evaluation purpose. 
 
 
 
 
 
 
 
 
 
 
 
Use of LDA and POS Tags for Efficient Search of Plagiarized Passages 
Department of Computer Science & Software Engineering, International Islamic University, Islamabad 
 
38 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 4. Showing the Overall Methodology of Proposed Framework 
 
 
 
Topic 
based 
query              
Source XML Files              
Query Results Post Processing              
Corpus               Suspicious  Document              
TOPIC ASSIGNMENT   + Topic Synonyms 
Gen 
TOPIC ASSIGNMENT  
Suspicious XML Files              
 XML Files with suspicious Sentences              
POS TAGGING + TAG CONVERSION             
XML File Generation             
Use of LDA and POS Tags for Efficient Search of Plagiarized Passages 
Department of Computer Science & Software Engineering, International Islamic University, Islamabad 
 
39 
 
CHAPTER 4 
 
 
 
EXPERIMENTS 
 
 
4.1 DATASET 
To get some real plagiarism cases, Ideally, we may have to study and monitor a large 
number of people who plagiarize and use their plagiarized text for verification and 
evaluation of proposed models; but there are certain aspects against this approach, one 
of which is distributing or using actual cases of plagiarism involve the permission 
from the plagiarist and real owner of text and a free text archive with real cases is 
questionable from an moral and lawful point of view. So this is more sensible for us to 
generate plagiarism cases by decided alteration, which is also called “simulated plagiarism”. 
We will use this strategy to make a testing corpus for our model. 
The dataset that we will use will be of two types. For testing purpose we will use 
about 300 different documents over different topics like scientific, English literature, 
Political columns from the web [17][18]. As our model detects only unilingual 
plagiarism, so all the documents will be in English.  
We prepared 52 suspicious documents by simulating different plagiarism cases. For 
this purpose we used passage summarization and compaction tools like Ginger, words 
Use of LDA and POS Tags for Efficient Search of Plagiarized Passages 
Department of Computer Science & Software Engineering, International Islamic University, Islamabad 
 
40 
to synonym replacer tools over the web [19]. Along with this different types of 
challenging plagiarized passages over the web were also used. The types of cases 
created to test our algorithm are mentioned in following table. Moreover a 
combination of following methods was used while creating the plagiarized passages. 
Table 6. Methods Used to for Simulated Plagiarism Cases 
Type Description 
CUT  Different passages were copied from source and pasted into 
suspicious files 
SYNONYMS Words in copied passages were replaced with respective 
synonyms using softwares like Ginger 
STOP WORDS REMOVED Stop-words were removed or altered from the copied passages 
REWRITTEN Copied passages were rewritten in different words 
ONLINE TEXT 
PARAPHRASING 
Online tools were used to paraphrased copied passages 
ONLINE TEXT 
SUMMARIZATION 
Online tools were used for to summarize the passages 
PASSAGE SLICING Copied passages were sliced into smaller sentences and were 
paraphrased then pasted into suspicious documents  
SENTENCE SLICING Different sentences were paraphrased and combined to make 
a single passage  
ONLINE  
EXAMPLES[20][21] 
Online plagiarism examples were used 
 
4.2 PERFORMANCE EVALUATION  
In order to define the following quality measures we need to introduce some notation 
[16]. Let s represent a copied text from the set S of all plagiarized texts. Let a 
detection is represented by r from the universal detection set R. Let SR is a subset of S 
for which detected cases exists in R. Let |s|, |r| indicate the lengths of characters in s, r 
and let |S|, |R|, |SR | be the sizes of the relevant sets. 
Use of LDA and POS Tags for Efficient Search of Plagiarized Passages 
Department of Computer Science & Software Engineering, International Islamic University, Islamabad 
 
41 
4.2.1 Micro Averaged Precision 
Precision measures the proportion between the correctly detected plagiarized passages 
and the total amount of detected plagiarized passages including the ones that were 
detected as plagiarism but in fact were not plagiarism. 
Precision =  
 
     
 =  
|  ∩ |
| |
  = ∑ #     
| |
| |       (13) 
4.2.2 Micro Averaged Recall 
Recall measures the proportion between the correctly detected plagiarized passages 
and the total amount of plagiarized passages including the ones that were not detected. 
Recall =  
 
     
 =  
|  ∩ |
| |
  = ∑ #     
| |
| |       (14) 
In Micro-average method, you sum up the individual true positives, false positives, 
and false negatives of the system for different sets and the apply them to get the 
statistics. Suppose there are n number of true positive TP, false positive FP and false 
negative FN cases then equation 13 and 14 can be expressed as 
Micr-Average Precision =  
⋯…
( ⋯ )  ( ⋯ ) 
                     (15) 
Micr-Average Recall       =  
⋯…
( ⋯ )  ( ⋯ ) 
                     (16) 
 
 
Use of LDA and POS Tags for Efficient Search of Plagiarized Passages 
Department of Computer Science & Software Engineering, International Islamic University, Islamabad 
 
42 
 
 
 
 
 
Figure 5: A document D as a sequence of characters with plagiarized sections S 
and detected cases R 
 
For the situation shown in Figure 5 the micro-averaged precision is 8/16, likewise, the 
micro-averaged recall is 8/13.  
4.2.3 Macro Averaged Precision and Recall 
Macro-averaged Precision and Recall are simply the average of all precisions P and 
recalls R, and is useful when you want to know how the system performs overall 
across the sets of data. You should not come up with any specific decision with this 
average.   
Macro-Avg Precision =  
⋯…
 
                     (17) 
Macro-Avg Recall =  
⋯…
 
                        (18) 
 
Use of LDA and POS Tags for Efficient Search of Plagiarized Passages 
Department of Computer Science & Software Engineering, International Islamic University, Islamabad 
 
43 
4.2.4 F-measure 
The F-measure is a composite measurement that tries to capture both Precision and 
Recall, and is defined as the harmonic mean between them. 
F −measure = 
. .                                       (19) 
4.2.5 Granularity 
The granularity measures the number of times a part of the text is detected as 
plagiarism and introduces a way to penalize overlapping plagiarism detections. If n is 
the number of true positive detections then. 
Granularity = ∑ # Of times a true positive case is reportedPrecision+Recall      (20) 
4.2.6 Overall 
The overall measurement is, as the F-measure, a way to combine the other 
measurements; it combines precision, recall, and granularity. 
Overall = 
( )
                                     (21) 
 
4.3 RESULTS AND DISCUSSION 
This section provides the detail comparison of different parameter settings that we 
have tested in order to adjust our method settings for final comparison with baseline 
Use of LDA and POS Tags for Efficient Search of Plagiarized Passages 
Department of Computer Science & Software Engineering, International Islamic University, Islamabad 
 
44 
methods. Once the parameter values are adjusted, we will compare the results of our 
method with the base line methods. 
4.3.1 Parameter Adjustment Tests 
As discussed in section 3.3.1 and 3.3.2 different parameters play a vital role in 
detection of plagiarized passages. Hence comprehensive tests are conducted over the 
dataset in order to find out the best values of these parameters for which the detection 
results (precision, recall, F-measure) are best. These parameters are shown in 
following table. 
Table 7. Parameters with range of values and the relevant description 
Name  Description Range 
α  This is the maximum number of words chosen for each 
topic assignment. Where α = 1 means one topic for each 
sentence 
1 - 10 
βl  Max Sequence match for source and suspicious 
sentences.  
5 - 15 
βs  Percentage of stop-words sequence match.  30% - 100% 
βn Percentage of nouns sequence match 30% - 100% 
βv Percentage of Verbs sequence match 30% - 100% 
βa Percentage of adverbs sequence match. Tests were done 
with constant value of βa = 30% 
 Fixed 
βp Percentage of pronouns sequence match. Tests were done 
with constant value of βp = 50% 
Fixed 
βd Percentage of adjectives sequence match. Tests were 
done with constant value of βa = 70% 
Fixed 
lp Maximum percentage of matched indices for a whole 
sentence to be marked as plagiarized. 
40% - 100% 
 
The values of adjectives, adverbs and pronouns were fixed and never changed 
throughout the experiments. The tests were conducted for a number of 33 suspicious 
documents. The following graphical result shows micro-averaged precision and recall. 
Use of LDA and POS Tags for Efficient Search of Plagiarized Passages 
Department of Computer Science & Software Engineering, International Islamic University, Islamabad 
 
45 
(a)  Configuration βl = 5, βs = 40%,  βn = 60%,  βv = 60%, lp= 60% 
 
Figure 6. Micro-averaged F-Measure for results of detections with α = 1 
 
Figure 7. Micro-averaged F-Measure for results of detections with α = 5 
 
Figure 8. Micro-averaged F-Measure for results of detections with α = 6 
0
20
40
60
80
100
1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33
Series1
0
20
40
60
80
100
1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33
Series1
0
20
40
60
80
100
1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33
Series1
Use of LDA and POS Tags for Efficient Search of Plagiarized Passages 
Department of Computer Science & Software Engineering, International Islamic University, Islamabad 
 
46 
 
Figure 9. Micro-averaged F-Measure for results of detections with α = 7 
 
Figure 10. Micro-averaged F-Measure for results of detections with α = 8 
 
Figure 11. Overall results for all five values of α 
0
20
40
60
80
100
1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33
Series1
0
20
40
60
80
100
1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33
Series1
61.3
81.32 80.56 80.83 83.22
0
10
20
30
40
50
60
70
80
90
1 2 3 4 5
Use of LDA and POS Tags for Efficient Search of Plagiarized Passages 
Department of Computer Science & Software Engineering, International Islamic University, Islamabad 
 
47 
(b)  Configuration βl = 7, βs = 30%,  βn = 50%,  βv = 40%, lp= 45% 
 
Figure 12. Micro-averaged F-Measure for results of detections with α = 1 
 
Figure 13. Micro-averaged F-Measure for results of detections with α = 5 
 
Figure 14. Micro-averaged F-Measure for results of detections with α = 6 
0
20
40
60
80
100
1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33
Series1
0
20
40
60
80
100
1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33
Series1
0
20
40
60
80
100
1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33
Series1
Use of LDA and POS Tags for Efficient Search of Plagiarized Passages 
Department of Computer Science & Software Engineering, International Islamic University, Islamabad 
 
48 
 
Figure 15. Micro-averaged F-Measure for results of detections with α = 7 
 
Figure 16. Micro-averaged F-Measure for results of detections with α = 8 
 
 
.  Figure 17. Overall results for all five values of α 
0
20
40
60
80
100
1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33
Series1
0
20
40
60
80
100
1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33
Series1
62.37
85.84 84.81 82.44 84.61
0
10
20
30
40
50
60
70
80
90
100
1 2 3 4 5
Use of LDA and POS Tags for Efficient Search of Plagiarized Passages 
Department of Computer Science & Software Engineering, International Islamic University, Islamabad 
 
49 
(b)  Configuration βl = 8, βs = 30%,  βn = 50%,  βv = 50%, lp= 50% 
 
Figure 18. Micro-averaged F-Measure for results of detections with α = 1 
 
Figure 19. Micro-averaged F-Measure for results of detections with α = 5 
 
 
 
Figure 20. Micro-averaged F-Measure for results of detections with α = 6 
0
20
40
60
80
100
1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33
Series1
0
20
40
60
80
100
1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33
Series1
0
20
40
60
80
100
1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33
Series1
Use of LDA and POS Tags for Efficient Search of Plagiarized Passages 
Department of Computer Science & Software Engineering, International Islamic University, Islamabad 
 
50 
 
Figure 21. Micro-averaged F-Measure for results of detections with α = 7 
 
 
Figure 22. Micro-averaged F-Measure for results of detections with α = 8 
 
 
Figure 23. Overall results for all five values of α 
0
20
40
60
80
100
1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33
Series1
0
20
40
60
80
100
1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33
Series1
60.78
85.44 83.97 81.78 84.09
0
10
20
30
40
50
60
70
80
90
1 2 3 4 5
Series1
Use of LDA and POS Tags for Efficient Search of Plagiarized Passages 
Department of Computer Science & Software Engineering, International Islamic University, Islamabad 
 
51 
From the above results we observe that the second parameter configuration with α = 5 
gave the highest overall results of 85.84. The reason for this is obvious because 5 is 
the least number of words for which a topic will be produced; the more number of 
topics in each sentence makes it difficult even for heavily paraphrased sentences to 
escape from matching query which depends on topic equivalence of sentences.  
4.3.2 Baseline Methods  
We implemented two baseline methods in order to compare our results with each of 
them over the same dataset. These two methods use stop-words n-gram and word n-
grams based string similarity matching. We implemented these methods in the same 
way as we implemented our method i.e. the detected plagiarized passages were 
written to an XML file for further evaluation. 
4.3.2.1 Stop-Word N Gram based method 
We skipped the initial steps of document retrieval as this was not our aim in this 
project. We implemented the Efstathios Stamatatos’s  method [6] (discussed earlier) 
into two steps i.e. first finding the suspicious passages from pre-nominated suspicious 
files and then applying the final criteria (equation 4) for identifying a passage as 
suspicious or not. We used the same threshold parameter values (except θL) as were 
mentioned by the Stamatatos in paper that gave best results. 
Table 8. The Parameter Values used for Comparison 
Parameter Value Description 
θc 0.5 Lower threshold of the similarity measure to keep a detection 
θL 20 Lower limit (in characters) of the detected passage length 
nc 3 Character n-gram length to measure similarity between 
passages 
n 5 The stop words 5-grams profiles will be generated 
  
The reason for which we set θL = 20 is because the dataset we chose to run our 
method had smaller length passages. Thus by setting threshold θL to a smaller value 
Use of LDA and POS Tags for Efficient Search of Plagiarized Passages 
Department of Computer Science & Software Engineering, International Islamic University, Islamabad 
 
52 
will help us to include the smaller passages in suspicious documents for further 
comparison. It is also important to mention here that in the end the method to declare 
a passage as plagiarized is same to the conventional methods of word n-gram based 
string matching. 
4.3.2.2 Word N-Gram based method 
The methods that we discussed in section 2.1.3 are based on word n-gram based 
matching of strings in vector spaces. Hence we implemented a general n-gram based 
method that suits the purpose of external plagiarism detection. Following steps are 
applied for this purpose.  
 
1. Removal of all stop-words (depicted in table 5) from a suspicious document D 
and stemming all the words to basic form. 
2.  Constructing the grams vocabulary V of all unique stemmed words in D of 
size n. 
3. Creating n sized binary inverted index array for each sentence s in D, where 
each non-zero entry in array depicts the presence of same unique word present 
in both s and D. 
4. Suppose there are k numbers of sentences present in D, hence a binary matrix 
M of k rows and n columns is prepared. 
5. Each sentence in the source documents S is passed through the steps 1 and 3 
and a binary array Sa of length n is created. 
6. The array Sa is queried with each row of matrix M in order to find the 
similarity measure. The type of similarity measure we used is Jaccard 
similarity. The Jaccard’s similarity is shown in following example 
Suppose we have two binary arrays A and B of size 13 
Use of LDA and POS Tags for Efficient Search of Plagiarized Passages 
Department of Computer Science & Software Engineering, International Islamic University, Islamabad 
 
53 
A =  [ 1 1 0 0 1 0 1 0 1 0 0 0 1 ]  
B =  [ 1 0 0 1 1 1 0 0 1 0 0 0 1 ] 
 
Total number of 00 in two arrays = 5 
Total number of 11 in two arrays = 4 
Other indices = 13 – 5 (Total number of 00 in two arrays) 
Jaccard’s Similarity = 
       
 
=                      (22) 
We have set the lower threshold value for Jaccard’s similarity = 0.5 and n =1 for our 
comparison purpose. 
4.3.3 QUALITATIVE COMPARISON 
In this section we will present a few examples to from our dataset that will show how 
our method has achieved good results. 
4.3.3.1 Example 1 
Table 9.  Example of Text Paraphrasing 
 
Source Sentence Suspicious Sentence 
First then, it is 
thought that every name has, or ought to 
have, one only precise and 
settled signification, which inclines men 
to think there are certain 
abstract 
initial then, it is 
notion that each name has, or must to 
have, one only exact and 
established meaning, which persuade guy 
to believe there are sure 
summary 
 
a) Stop-Words N-Gram Method 
The 5-gram stop-word profile for source passage  
{ it is that has or |  is that has or to | that has or to have | has or to have and |  
Use of LDA and POS Tags for Efficient Search of Plagiarized Passages 
Department of Computer Science & Software Engineering, International Islamic University, Islamabad 
 
54 
  or to have and which | to have and which to |  have and which to there |  
  and which to there are }  
 
The 5-gram stop-word profile for suspicious passage  
{it is that has or |  is that has or to | that has or to have | has or to have and | 
 or to have and which | to have and which to | have and which to there | 
 and which to there are} 
These profiles match each other, now we apply the final condition (eq. 4) for deciding 
whether or not two texts are related. 
Number of distinct words 3-grams in suspicious sentence = 144 
Number of distinct words 3-grams in suspicious sentence = 144 
Number of matching 3-grams in both sentences = 68 
PlagDet = 
 
( , )
= 0.47 <  θc                     
Hence this method failed to detect plagiarism. 
b) Words N-Gram Method 
Unique n-gram vocabulary of suspicious sentence with stop-words removed 
A = { initial notion each name one only exact established meaning persuade guy 
believe sure summary } 
As this is the only sentence, thus the binary array for this will be 
a = {1 1 1 1 1 1 1 1 1 1 1 1 1 1}  
Unique n-gram vocabulary of source sentence with stop-words removed 
Use of LDA and POS Tags for Efficient Search of Plagiarized Passages 
Department of Computer Science & Software Engineering, International Islamic University, Islamabad 
 
55 
B = { first thought every name one only precise settle signification incline men think 
certain abstract}   
The binary array that we get as result of applying binary query is over matching 
words in A and B 
b = {0 0 0 1 1 1 0 0 0 0 0 0 0 0} 
Total number of 00 in two arrays = 0 
Total number of 11 in two arrays = 3 
Other indices (10 and 01) = 11  
Jaccard’s Similarity = 
       
 
= = 0.27 < 0.5    
Hence this method also failed to detect plagiarism. 
c) POS-LDA Method 
Topics and synonyms produced for suspicious sentence with POS tags 
{initial, one, lone, sole, first, single…….. } 
A = { 21 8 0 33 46 35 6 4 35 46 0 5 48 7 41 0 2 8 21 5 44 35 0 27 42 35 7 41 3 45 21 
35 }  
Topics produced for source sentence with POS tags 
{certain, abstract, precise, men, first} 
B = { 8 8 0 33 46 44 6 4 35 46 0 5 48 7 41 0 2 8 21 5 42 21 35 0 27 46 36 7 41 3 45 
21 35} 
The best adjusted parameters values from section 4.3.1 b are 
βl = 7, βs = 30%,  βn = 50%,  βv = 40%, lp= 45%, βa = 30%, βd = 50%, βp = 70% 
  
After applying the matching criteria we get the following results 
Use of LDA and POS Tags for Efficient Search of Plagiarized Passages 
Department of Computer Science & Software Engineering, International Islamic University, Islamabad 
 
56 
Max Sequence Match Length =  14  > βl 
Stop-Words Sequence Match =  100%  > βs 
Nouns Sequence Match =   40 %  < βn 
Verbs Sequence Match =   37%  < βv 
Adverbs Sequence Match =   100%  > βa 
Adjectives Sequence Match =  43%  < βd 
Pronouns Sequence Match =   43%  < βp 
Length Ratio  Lr =    33 / 32 < 2 
By inserting these values in equations 8 and 9   
tf = ((14 ≥ 2βl) ˄ ((100 ≥ βa)  ˅ (43 ≥ βp) ˅ (43 ≥ βd) )) = true  
TF = ((14 ≥ βl) ˄ (Lr ≤ 2)) ˄ ( ((100 ≥ βs) ˄ (40 ≥ βn)) ˅ ((100 ≥ βs) ˄ (37 ≥ βv)) ˅ 
(40 ≥ βn) ˄ (37 ≥ βv)) ˅ (tf = true) )  = true 
 Hence this method managed to detect plagiarism. 
4.3.3.2 Example 2 
Table 10. A More Complex Example of Text Paraphrasing 
 
Source Sentence Suspicious Sentence 
A drink driver who ran into the Queen 
Mother’s official Daimler was fined 
£seven and banned from driving for 2 
years.  
A DRUNK driver who crashed into the 
back of the Queen Mum’s limo was 
forbidden for two years yesterday.  
 
a)  Stop-Words N-Gram Method 
The 5-gram stop-word profile for both source passages  
{ a who the was and | who the was and from | the was and from for}  
The 5-gram stop-word profile for both suspicious passages  
{a who the of the | who the of the was | the of the was for }  
Use of LDA and POS Tags for Efficient Search of Plagiarized Passages 
Department of Computer Science & Software Engineering, International Islamic University, Islamabad 
 
57 
We get no stop-word 5-gram match for suspicious passage 
Hence this method failed to detect plagiarism. 
b) Words N-Gram Method 
Unique n-gram vocabulary of suspicious sentence with stop-words removed 
A = { drunk driver crash back queen Mum limo forbidden two year yesterday} 
As this is the only sentence, thus the binary array for this will be 
a = {1 1 1 1 1 1 1 1 1 1 1} 
Unique n-gram vocabulary of source sentence with stop-words removed 
B = { drink driver ran queen Mother official Daimler fine seven banned drive year} 
The binary array that we get as result of applying binary query is over matching 
words in A and B 
b = {0 1 0 0 1 0 0 0 0 1 0 } 
Total number of 00 in two arrays = 0 
Total number of 11 in two arrays = 3 
Other indices (10 and 01) = 8  
Jaccard’s Similarity = 
       
 
= = 0.37 < 0.5    
Hence this method also failed to detect plagiarism. 
 
 
Use of LDA and POS Tags for Efficient Search of Plagiarized Passages 
Department of Computer Science & Software Engineering, International Islamic University, Islamabad 
 
58 
c) POS-LDA Method 
Topics and its synonyms produced for suspicious sentence with POS tags 
{ period, class, twelvemonth, year…….. } 
A = { 4 35 35 25 42 6 4 35 35 35 35 42 44 8 5 44 6 43 6 2 36 }  
Topics produced for source sentence with POS tags 
{ daimler, drink, seven, year, banned } 
B = { 4 35 35 25 42 6 4 35 6 4 35 35 35 42 44 6 2 36 35 } 
The best adjusted parameters values from section 4.3.1 b are 
βl = 7, βs = 30%,  βn = 50%,  βv = 40%, lp= 45%, βa = 30%, βd = 50%, βp = 70% 
  
After applying the matching criteria we get the following results 
Max Sequence Match Length =  8   > βl 
Stop-Words Sequence Match =  42%   > βs 
Nouns Sequence Match =   87 %   > βn 
Verbs Sequence Match =   100%   > βv 
Adverbs Sequence Match =   0%   < βa 
Adjectives Sequence Match =  42%   < βd 
Pronouns Sequence Match =   42%   < βp 
Length Ratio  Lr =    21 / 19              < 2 
By inserting these values in equations 8 and 9   
tf = ((8 ≥ 2βl) ˄ ((0 ≥ βa)  ˅ (42 ≥ βp) ˅ (42 ≥ βd) )) = false  
TF = ((8 ≥ βl) ˄ (Lr ≤ 2)) ˄ ( ((42 ≥ βs) ˄ (87 ≥ βn)) ˅ ((42 ≥ βs) ˄ (100 ≥ βv)) ˅ (87 ≥ 
βn) ˄ (1-- ≥ βv)) ˅ (tf = true) )  = true 
 Hence this method managed to detect plagiarism. 
Use of LDA and POS Tags for Efficient Search of Plagiarized Passages 
Department of Computer Science & Software Engineering, International Islamic University, Islamabad 
 
59 
4.3.4 QUANTITATIVE COMPARISON 
The overall quantitative results of precision, recall and F-measure for all three 
methods are presented here in graphical form. These tests were carried out over 52 
suspicious files all divided into different sections. Here the x-axis is the number of 
suspicious files for which plagiarism is detected and y-axis is the F-measure. 
 
a)  Stop-Words N-Gram Method 
 
 
Figure 24. Graphical representation of F-measure score for SWNG Method 
 
 
 
 
 
 
 
0
10
20
30
40
50
60
70
80
90
100
1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41 43 45 47 49 51
Series1
Use of LDA and POS Tags for Efficient Search of Plagiarized Passages 
Department of Computer Science & Software Engineering, International Islamic University, Islamabad 
 
60 
b) Words N-Gram Method 
 
 
Figure 25. Graphical representation of F-measure score for WNG Method 
c) POS-LDA Method 
 
 
Figure 26. Graphical representation of F-measure score for POS-LDA Method 
Here we can clearly see the difference of how all the three methods performed in case 
of detecting the plagiarism in each suspicious file. Also the better performance of our 
method is evident. This will be more clear from the overall score for macro averaged 
0
10
20
30
40
50
60
70
80
90
100
1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41 43 45 47 49 51
Series1
0
10
20
30
40
50
60
70
80
90
100
1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41 43 45 47 49 51
Series1
Use of LDA and POS Tags for Efficient Search of Plagiarized Passages 
Department of Computer Science & Software Engineering, International Islamic University, Islamabad 
 
61 
values in the following graphical representation of all the three methods. Here the x-
axis is the method number and y-axis is the overall score of each. 
 
 
Figure 27. Graphical representation of Overall score for all three Methods, from 
Left to Right we have SWNG, WNG and POS-LDA 
 
 
 
 
 
 
 
 
 
 
42.33
52.69
79.11
0
10
20
30
40
50
60
70
80
90
1 2 3
Use of LDA and POS Tags for Efficient Search of Plagiarized Passages 
Department of Computer Science & Software Engineering, International Islamic University, Islamabad 
 
62 
 
CHAPTER 5 
 
 
 
 
CONCLUSIONS 
 
The most plagiarism detection frameworks used now days are applying n-gram based 
string matching approach which has its defects and proves unworthy in case someone 
heavily replace the words in phrase with respective synonyms, as we have seen in the 
above mentioned examples. We have applied our method over almost all types of 
simulated plagiarism cases that a plagiarist could try out in order to rephrase a 
sentence and deceive the n-gram based string matching plagiarism detection tools. We 
have successfully detected plagiarism over almost all cases. The reason that some of 
the plagiarism cases that escaped from our POS-LDA method was that our method 
failed to match topics and their synonyms among source and suspicious passages. The 
reason why these topics did not matched is obvious e.g. we have a topic word 
“skilled” in source passage which is copied in suspicious passage as “skilled”; Now if 
the word “skilled” is also chosen topic of suspicious phrase then it may have its 
synonym list like “capable, able, trained, skillful, talented…”; none of which would 
match the source topic. Another main reason for a failure in detection would be in 
case the POS patterns of source and suspicious passages in case a suspicious passage 
is heavily revised and the order of words is changed.    
Use of LDA and POS Tags for Efficient Search of Plagiarized Passages 
Department of Computer Science & Software Engineering, International Islamic University, Islamabad 
 
63 
 
5.1 FUTURE WORK 
We compared pre-nominated source documents for each suspicious document in local 
archive and have used topic matching based approach in order to select suspicious 
passages for further processing and evaluation; however this approach can be further 
utilized to choose the related source documents from an archive of documents over 
the network. Also the mean for choosing the suspicious topics and synonyms could be 
improved further. 
Another observation is the time taken by our method in order to produce suspicious 
topics and related synonyms, which should be reduced.      
 
 
 
 
 
 
 
 
 
 
 
 
Use of LDA and POS Tags for Efficient Search of Plagiarized Passages 
Department of Computer Science & Software Engineering, International Islamic University, Islamabad 
 
64 
 
References: 
1. [Online] http://en.wikipedia.org/w/index.php?title=Plagiarism&oldid=65284248, 
2012.     
2. Hermann Maurer, Frank Kappe and Bilal Zaka, “Plagiarism - A Survey”, Journal 
of Universal Computer Science, vol. 12, no. 8, pp. 1050-1084, 2006. 
3. [Online] http://en.wikipedia.org/wiki/Plagiarism_detection#Detection_methods, 
2012. 
4. Ozlem Uzuner, Boris Katz and Thade Nahnsen, “Using Syntactic Information to 
Identify Plagiarism”, Proceedings of the 2nd Workshop on Building Educational 
Applications Using NLP, pp. 37–44, 2005. 
5. Mario Zechner, Markus Muhr and Roman Kern, “External and Intrinsic 
Plagiarism DetectionUsing Vector Space Models”, Stein, Rosso, Stamatatos, 
Koppel, Agirre (Eds.): PAN'09, pp. 47-55, 2009. 
6. Efstathios Stamatatos, “Plagiarism Detection Using Stopword n-grams”, Journal 
of the American Society for Information Science and Technology, pp. 2512-2527, 
2011. 
7. Markus Muhr, Roman Kern, Mario Zechner and Michael Granitzer, “External 
and Intrinsic Plagiarism Detection using a Cross-Lingual Retrieval and 
Segmentation System”, In Proceedings of the 4th International Workshop on 
Uncovering Plagiarism, Authorship, and Social Software Misuse. Notebook 
Papers of CLEF 11 Labs and Workshops, 2010. 
8. Sobha Lalitha Devi, Pattabhi R K Rao, Vijay Sundar Ram and A Akilandeswari, 
“External Plagiarism Detection”, In Proceedings of the 4th International 
Workshop on Uncovering Plagiarism, Authorship, and Social Software Misuse. 
Notebook Papers of CLEF 11 Labs and Workshops, 2010. 
9. Gupta Parth, Rao Sameeer, and Prasenjit Majumdar, “External Plagiarism 
Detection: N-Gram Approach using Named Entity Recognizer”, In Proceedings 
of the 4th International Workshop on Uncovering Plagiarism, Authorship, and 
Social Software Misuse. Notebook Papers of CLEF 11 Labs and Workshops, 
2010. 
10. Clara Vania and Mirna Adriani, “Automatic External Plagiarism Detection Using 
Passage Similarities”, In Proceedings of the 4th International Workshop on 
Use of LDA and POS Tags for Efficient Search of Plagiarized Passages 
Department of Computer Science & Software Engineering, International Islamic University, Islamabad 
 
65 
Uncovering Plagiarism, Authorship, and Social Software Misuse. Notebook 
Papers of CLEF 11 Labs and Workshops, 2010. 
11. Sameer Rao, Parth Gupta, Khushboo Singhal and Prasenjit Majumder, “External 
& Intrinsic Plagiarism Detection: VSM & Discourse Markers based Approach”, 
In Proceedings of the 4th International Workshop on Uncovering Plagiarism, 
Authorship, and Social Software Misuse. Notebook Papers of CLEF 11 Labs and 
Workshops, 2010. 
12. Gabriel Oberreuter, Gaston L’Huillier, Sebastián A. Ríos and Juan D. Velásquez 
“Approaches for Intrinsic and External Plagiarism Detection”, In Proceedings of 
the 4th International Workshop on Uncovering Plagiarism, Authorship, and 
Social Software Misuse. Notebook Papers of CLEF 11 Labs and Workshops, 
2010.  
13. Yurii Palkovskii, Alexei Belov and Iryna MuzykaUsing, “WordNet-based 
semantic similarity measurement in External Plagiarism Detection”, In 
Proceedings of the 5th International Workshop on Uncovering Plagiarism, 
Authorship, and Social Software Misuse. Notebook Papers of CLEF 11 Labs and 
Workshops, 2011. 
14. Beatrice Santorini, “Part-of-Speech Tagging Guidelines for the Penn Treebank 
Project (3rd Revision)”, repository.upenn.edu/cis reports/570, 1990. 
15. David M. Blei, Andrew Y. Ng and Michael I. Jordan, “Latent Dirichlet 
Allocation”, Berkely, CA 94720, 2003. 
16. Martin Potthast, Benno Stein, Alberto Barron-Cedeno and Paolo Rosso “An 
Evaluation Framework for Plagiarism Detection”, Poster Volume, pp. 997–1005, 
Beijing, August 2010. 
17. [Online]  http://www.shortstoryarchive.com/, 2013. 
18. [Online]  https://archive.org/details/textfiles-dot-com, 2011. 
19. [Online]  http://articlerewritertool.com/, 2014. 
20. [Online]  http://articlerewritertool.com/, 2014. 
21. [Online]  http://www.princeton.edu/pr/pub/integrity/pages/plagiarism/, 2013. 
22. [Online] http://examples.yourdictionary.com/examples/examples-of-
plagiarism.html, 2013. 
 
 
