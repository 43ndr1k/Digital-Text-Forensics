ARTICLE IN PRESS
Pattern Recognition ( ) –
www.elsevier.com/locate/pr
EROS: Ensemble rough subspaces
Qinghua Hu∗, Daren Yu, Zongxia Xie, Xiaodong Li
Harbin Institute of Technology, Harbin, China
Received 24 May 2006; received in revised form 28 March 2007; accepted 29 April 2007
Abstract
Ensemble learning is attracting much attention from pattern recognition and machine learning domains for good generalization. Both
theoretical and experimental researches show that combining a set of accurate and diverse classifiers will lead to a powerful classification
system. An algorithm, called FS-PP-EROS, for selective ensemble of rough subspaces is proposed in this paper. Rough set-based attribute
reduction is introduced to generate a set of reducts, and then each reduct is used to train a base classifier. We introduce an accuracy-guided
forward search and post-pruning strategy to select part of the base classifiers for constructing an efficient and effective ensemble system. The
experiments show that classification accuracies of ensemble systems with accuracy-guided forward search strategy will increase at first, arrive
at a maximal value, then decrease in sequentially adding the base classifiers. We delete the base classifiers added after the maximal accuracy.
The experimental results show that the proposed ensemble systems outperform bagging and random subspace methods in terms of accuracy
and size of ensemble systems. FS-PP-EROS can keep or improve the classification accuracy with very few base classifiers, which leads to a
powerful and compact classification system.
 2007 Pattern Recognition Society. Published by Elsevier Ltd. All rights reserved.
Keywords: Attribute reduction; Ensemble learning; Multiple classifier system; Rough set; Selective ensemble
1. Introduction
Ensemble learning refers to training a set of base predictors
for a given classification or regression task and then combining
their outputs with a fusion strategy. This is also called multi-
ple classifier systems [1], expert committee [2], decision forest
[3,4], etc. Great improvement in generalization performance
has been observed from ensemble learning in a wide range of
numerical experiments and practical applications [5–7,40].
Intuitively, combining a set of the same classifiers will not
yield any improvement. The improvement comes from the
diversity among the combined classifiers. It has been observed
that although one of the classifiers will get a good perfor-
mance, the samples misclassified by different classifiers would
not necessarily overlap. This suggests that different classifiers
potentially offer complementary information about the samples
to be classified, which could be harnessed to improve the perfor-
mance of the selected classifiers [8]. The previous work shows
∗ Corresponding author. Tel.: +86 45186413241 252;
fax: +86 45186413241 221.
E-mail address: huqinghua@hcms.hit.edu.cn (Q. Hu).
0031-3203/$30.00  2007 Pattern Recognition Society. Published by Elsevier Ltd. All rights reserved.
doi:10.1016/j.patcog.2007.04.022
the base classifiers should be both accurate and diverse for
good ensemble for constructing a good ensemble system
[9–12,42,48]. This poses the challenge of generating a set of
classifiers with good individual performances and indepen-
dently distributed predictions for unseen data.
A number of techniques for training multiple diverse clas-
sifiers have been devised so far. Roughly speaking, they can
be divided into two classes: classifier perturbation and sample
perturbation. Classifier perturbation refers to making use of
the instability of learning algorithms. As we know, the deci-
sion tree learning algorithms and neural network are subject to
the influence of the initialization. Namely, the learned predic-
tors will converge at different local minimal points if different
initializations are given; therefore, there will be some difference
in the trained neural networks or decision trees. Random forests,
proposed by Breiman in Ref. [13], are the representatives of the
classifier perturbation techniques, where the base decision tree
is constructed with random split selection. Hampshire et al.
designed a neural network ensemble, in which the base neural
networks were trained with distinct objective functions. The
number of hidden neurons [14] and the initialization weight
[15] were used to generate diverse classifiers. What is more,
Please cite this article as: Q. Hu, et al., EROS: Ensemble rough subspaces, Pattern Recognition (2007), doi: 10.1016/j.patcog.2007.04.022
2 Q. Hu et al. / Pattern Recognition ( ) –
ARTICLE IN PRESS
different classes of learning algorithms were employed in
some ensembles [16]. As to sample perturbation, there are two
aspects which can be used to construct diverse classifiers. One
is to train classifiers with different sample subsets and the
other is to construct classifiers in distinct feature subspaces.
The Bagging technique generates a number of training sam-
ple sets from the original data with bootstrap sampling and
then trains a base predictor with each of these sets of samples
[11]. Here, the sample sets are independently extracted and
the base predictor can be constructed simultaneously. How-
ever, boosting, as another representative of resampling, must
sequentially train the base classifier [17] because the training
samples wrongly predicted by a component learner will play
a more important role in the training of its subsequent learner.
After improvements by Freund [18], expanded in Ref. [19],
AdaBoost (Adaptive Boosting) was introduced by Freund and
Schapire [20,47]. From another point of view, Ho proposed a
novel method for constructing decision forests with random
subspace techniques [3]. The classifier system consists of
multiple trees constructed systematically by pseudo-randomly
selecting subsets of components of the feature vector, that
is, trees are constructed in randomly chosen subspaces; the
subspace method was shown to perform better when the
data set has a large number of features and not too few
samples. Furthermore, Bryll et al. presented an approach
to construct multiple classifier system, named attribute bag-
ging [21]. It establishes an appropriate size of the attribute
subset with a wrapper method and then randomly selects
subsets of features, creating projections of the training set
on which the ensemble classifiers are built. This method
was used for hand configuration recognition, and it was
found that bagging the attributes of strong generalization
power improved the performance of the resulting ensem-
ble.
In Ref. [22], Zhou and Yu reviewed some results from the
literatures and pointed out that the bagging algorithm had
achieved great success in building ensembles of decision trees
and neural networks. Few of them significantly outperform
bagging C4.5 decision trees although many ensemble learning
algorithms which component learners can be trained in parallel
have been developed. However, although bagging can work
well on unstable base learners such as decision trees and neu-
ral networks, it can hardly work on stable base learners such
as nearest-neighbor classifiers [11]. In order to deal with this
problem, a new ensemble algorithm named filtered attribute
subspace-based bagging with injected randomness (FASBIR)
was proposed, which utilizes multimodal perturbation to help
generate accurate and diverse base classifiers. FASBIR em-
ploys the perturbation on the training data with bootstrap
sampling, the perturbation on the input attributes with attribute
filtering and attribute subspace selection, and the perturbation
on the learning parameters with randomly configured distance
metrics.
Recent researches showed that a well-devised feature
selection algorithm would significantly improve the effi-
ciency and accuracies of subspace ensembles because attribute
reduction lessens the impact of the “curse of dimensionality”
and speeds up the training and test process [21,23–25]. The
key problem of this ensemble method is how to get a set
of attribute subset with good predicting power. Rough set
theory, which was introduced by Pawlak [26], has attracted
much attention from AI society. This methodology proves to
be a powerful tool to reason with uncertainty [27], in partic-
ular to attribute reduction [28–32]. In the rough set frame-
work, reducts are the minimal attribute subsets which keep
the discernibility of the original data and have no redundant
attributes. It is worth noting that there are usually multiple
reducts for a given data set. Sometimes, tens or hundreds of
reducts can be found. All the reducts can be employed for
constructing multiple classifiers [4,33]. However, most of the
applications select the reduct with the fewest attributes to
construct classifier at present [34–36]. The information hid-
den in other reducts is wasted in this case. In this paper, we
will present a systematic technique for selectively combining
parts of reducts with an accuracy-guided forward search and
post-pruning strategy, rather than combining all of reducts
[33]. In detail, we introduce attribute reduction algorithms
to find a set of reducts and train a base classifier with each
reduct. Then we present an accuracy-guided Forward Search
and Post-Pruning strategy (FS-PP) to select parts of base
classifiers for ensemble systems. As the ensemble system is
to Ensemble multiple ROugh Subspaces, we call our method
FS-PP-EROS. Compared with sample bagging and random
subspace methods, properties of FS-PP-EROS are discussed in
this work.
The rest of the paper is structured as follows. Basic defini-
tions on rough sets are given in Section 2; Section 3 presents the
EROS algorithm. The empirical results and analysis are shown
in Section 4. The conclusion comes in Section 5.
2. Preliminary knowledge on rough sets
Rough set theory, which was introduced to deal with
imperfect and vague concepts by Pawlak [26], has attracted a
lot of attention from machine learning and data mining soci-
eties. Data sets are usually given in the form of tables; we call
a data table as an information system, formulated as a four-
tuple IS= 〈U, A, V, f 〉, where U = {x1, x2, . . . , xn} is a set of
finite and nonempty objects, called the universe, A is the set of
attributes characterizing the objects, V is the domain of
attribute value and f is the information function f : U × A→
V . If the attribute set is divided into condition attribute C and
decision attribute D, the information system is also called a
decision table.
With arbitrary attributes B ⊆ A, there is an associated indis-
cernibility relation IND(B) :
IND(B)= {〈x, y〉 ∈ U × U |∀a ∈ B, f (x, a)= f (y, a)}.
〈x, y〉 ∈ IND(B) means objects x and y are indiscernible with
respect to B. Obviously, an indiscernibility relation is an equiv-
alent relation which satisfies reflexivity, symmetry and transi-
tivity. The equivalent class induced by B is denoted by
[xi]B = {x|〈xi, x〉 ∈ IND(B), x ∈ U}.
Please cite this article as: Q. Hu, et al., EROS: Ensemble rough subspaces, Pattern Recognition (2007), doi: 10.1016/j.patcog.2007.04.022
ARTICLE IN PRESS
Q. Hu et al. / Pattern Recognition ( ) – 3
Equivalence classes generated by B are also called B-elemental
granules, B-information granules. The set of elemental granules
forms a concept system, which is used to characterize arbitrary
subsets in the information system. Given an arbitrary subset X
in the information system, two unions of elemental granules
are associated with{
BX = {[x]B |[x]B ⊆ X, x ∈ U},
BX = {[x]B |[x]B ∩X 
= ∅, x ∈ U}.
The concept X is approximated by the two sets of elemen-
tal granules. BX and BX are called lower and upper approx-
imations of X in terms of attributes B. BX is also called pos-
itive region. X is a definable set if BX = BX. This means
the concept X can be perfectly characterized with the knowl-
edge B, otherwise, X is indefinable. An indefinable set is called
a rough set. BND(X) = BX − BX is called the boundary of
the approximations, and as a definable set, the boundary is
empty. In this context, attributes are the knowledge to discern
the objects and form concept systems for classification and
reasoning.
Assume C is the set of condition attribute set and D is the
decision in a given nonempty and finite universe U, then C and
D will generate two partitions of the universe. Mach. Learn. is
usually involved in using condition knowledge to approximate
the decision or searching the mapping from the conditions to
decisions. Approximating U/D with U/C, the positive and
boundary regions are defined as
POSC(D)=
⋃
X∈U/D
CX,
BNDC(D)=
⋃
X∈U/D
CX −
⋃
X∈U/D
CX.
The boundary region is the set of elemental granules which
cannot be perfectly described by the knowledge C, and the posi-
tive region is the set of C-elemental granules which completely
belong to one of the decision concepts. The size of positive or
boundary regions reflects the approximation power of the con-
dition attributes. Given a decision table, for any B ⊆ C, it is
said the decision attribute set D depends on B with degree k,
denoted by B⇒kD, where
k = B(D)=
|POSB(D)|
|U | .
The dependency coefficient k measures the approximation
power of a set of condition attributes. In data mining, espe-
cially in feature selection, it is important to find the dependence
relations between attribute sets.
Given a decision table DT =〈U, C ∪D, V, f 〉, if P ⊆ Q ⊆
C, we have
Q(D)P (D).
Given a decision table DT = 〈U, C ∪ D, V, f 〉, B ⊆ C,
a ∈ B, we say condition attribute a is indispensable in B if
(B−a)(D) < B(D); otherwise, we say a is redundant. We say
K1
K2
Kj
Core
K3Kr
I
Fig. 1. Structure of attributes in an information system
B ⊆ C is independent if any a in B is indispensable. Attribute
subset B is a reduct of the decision table if
(1) B(D)= C(D);
(2) ∀a ∈ B : B(D) > B−a(D).
A reduct of a decision table is a subset of condition attributes,
which keeps the approximation power of the whole condition
attributes, and has no redundant attribute. The definition of
“reduct” is perfect for feature selection, where most of the aims
are to find a most compact representation of the problem for
computation complexity and generalization.
Usually, there exists a number of reducts for a given decision
table. Let 〈U, C ∪D, V, f 〉 be a decision table, and {Bj |j r}
be the set of reducts, we denote the following attribute subset:
Core=
⋂
j  r
Bj , K =
⋃
j  r
Bj − Core,
Kj = Bj − Core, I = A−
⋃
j  r
Bj .
Core is the attribute subset which cannot be deleted from any
reduct, otherwise the discernibility of the system will decrease.
Therefore, the core attributes will be in all of the reducts. I is
called the completely irrelevant attribute set. The attribute in I
will not be included by any reduct, which means I is completely
useless in the system. Kj is a weak relevant attribute set. The
union of Core and Kj forms a reduct of the information system.
The structure of attributes is shown in Fig. 1.
Rough set theory discloses the fact that there exist multiple
subsets of attributes which can keep the distinguishability of
the original data. They characterize the recognition problem in
distinct subspaces and, therefore, capture different information
of classification tasks. They are complementary to each other.
The generalization power may be improved via combining a
set of rough-set-based reducts.
3. EROS: construct ensembles with reducts
3.1. Basic idea of EROS
EROS is a multiple classifier system, in which base classi-
fiers are trained with rough-set-based reducts. The structure of
EROS is shown in Fig. 2. Given a decision table, there are a
set of attributes {a1, a2, . . . , an}. These attributes are grouped
into a number of reducts {R1, R2, . . . , RN } with a reduction
algorithm. Then each reduct is used to train a base classifier
Please cite this article as: Q. Hu, et al., EROS: Ensemble rough subspaces, Pattern Recognition (2007), doi: 10.1016/j.patcog.2007.04.022
4 Q. Hu et al. / Pattern Recognition ( ) –
ARTICLE IN PRESS
DT = <U, C ∪ D, V, f >
a1 a2 ai an
R1 R2 Rj RN
T1 T2 Tj TN
w1 w2 wj wN
Sample set
Attribusette 
reduct set
Classifier set
multiple classifier system
weight set
Voting
Fig. 2. Structure of EROS.
with some learning algorithm. Given a test sample, the deci-
sions from all the base classifiers are combined with a weighted
voting strategy. In this work wj takes values in {0, 1}. That
is, a base classifier either takes part in the final decision or is
excluded in the ensemble system.
There is some work to do for constructing such an ensem-
ble system. First, an algorithm for searching multiple reducts
should be proposed. Second, one should select learning
algorithms to train base classifiers with the resulting reducts.
Finally, a strategy of selecting base classifiers is required for
constructing an efficient and powerful ensemble system.
3.2. Search reducts in feature spaces
Several algorithms for finding a good reduct have been
proposed based on heuristic strategies, such as discernibil-
ity matrix [37], dependency function [38], and information
entropy [39]. Here we give the dependency-based algorithm.
Let C and D be the condition attribute set and decision
attribute, respectively. B ⊆ C, ∀a ∈ B, we define a coefficient
SIG(a, B, D)= |POSB(D)||U | −
|POSB−a(D)|
|U |
as the significance of attribute a in B relative to decision D.
SIG(a, B) reflects the changes of the positive region if attribute
a is eliminated from B. Accordingly, we also can define
SIG1(a, B, D)= |POSB+a(D)||U | −
|POSB(D)|
|U | ,
∀a ∈ C − B.
SIG1(a, B) measures the increment of the positive region
if attribute a is introduced in B. SIG1(a, B) can be used in a
forward attribute reduction, while SIG(a, B) is applicable to
the backward algorithm.
Since the core attributes belong to any reducts, the reduction
process can be started with the core. The core can be defined as
Core= {a‖POSC−a(D)|/|U |< |POSC(D)|/|U |, a ∈ C}.
Then the algorithm for search core attributes can be formu-
lated as follows.
Algorithm 1. (Search core in information systems).
Input: IS = 〈U, C ∪D, V, f 〉
Output: core
1. Core← ∅
2. for i= 1 to n//n is the number of attributes
3. compute
SIG(ai, C, D)= |POSC(D)|/|U | − |POSC−ai (D)|/|U |
4. if SIG(ai, C) > 0
5. Core← Core ∪ ai
6. end if
7. end for
8. returnCore
Computing significance of an attribute SIG(ai, C, D) is the
key step in algorithm 1. Xu et al gave a quick algorithm with
complexity O(|U |) in Ref. [49]. So the complexity of comput-
ing core is O(|U ||C|).
Starting with the core, we take the attribute with the maximal
significance into the reduct set in each loop until the maximal
significance equals zero, and then we can get a reduct. For-
mally, a forward greedy reduction algorithm can be written as
follows.
Algorithm 2. (Forward reduction for searching a single
reduct).
Input: IS = 〈U, A= C ∪D, V, f 〉
Output: a reduct B of C.
1. compute the core with algorithm 1
2. B ← Core
3. do
4. T ← B
5. ∀a ∈ {C − B}, compute SIG({B ∪ a}, T )
6. If SIG({B ∪ aj }, T , D)=
max
i
{SIG({B ∪ ai}, T , D)}
7. T ← B ∪ {aj }
8. B ← T
9. until ∀ai , SIG({B ∪ ai}, T )= 0
10. return B
In Algorithm 2, we begin with an empty set and add an at-
tribute with the maximal significance into set B in each stage
until the maximal significance is zero. In fact, we can also start
with the whole attribute set and delete an attribute whose sig-
nificance equals zero in each loop until there is no attribute
with significance zero. This process is called a backward reduc-
tion algorithm. Their time complexities are O(|U ||C| + |C −
core|2|U |).
In order to generate multiple reducts from data, we loosen
the condition of maximal significance in each loop. We can
select the algorithm with the second maximal significance. An
algorithm named WADF was proposed to get multiple reducts
[33] with this strategy, as shown below.
Please cite this article as: Q. Hu, et al., EROS: Ensemble rough subspaces, Pattern Recognition (2007), doi: 10.1016/j.patcog.2007.04.022
ARTICLE IN PRESS
Q. Hu et al. / Pattern Recognition ( ) – 5
Algorithm 3. (Backward reduction for searching multiple
reducts).
Input: an information system IS = 〈U, C ∪D, V, f 〉
Output: a set of reducts
1. compute the CORE
2. B ← C − CORE
3. B∗ ← sorted B in the ascending order in terms of
g(a) //g(a)= |POSa(D)|/|U | + |U/a|/|U |.
4. P ∗ ← B∗ ∪ CORE
5. find a reduct RED0
6. K ← RED0 − CORE
7. RED← 
8. for i = 1 to |K|
9. P ∗ ← P ∗ − {ai}
10. Find a new reduct REDi
11. If REDi /∈RED then RED← RED+ REDi
12. P ∗ ← P ∗ + {ai}
13. end for
14. return RED= {RED0, RED1, . . . , REDN }.
Algorithm 3 yields a set of reducts via perturbing the
selection of attributes in reduction. As heuristic algorithms
are sensitive to the perturbation, there may be great differ-
ence in the yielded reducts via perturbation. This difference
lays a foundation for constructing complementary classifiers.
Assume the algorithm yields N reducts, then time complexity
is O(|U ||C| + |C − core|2|U ||N |). Please see the detailed in-
formation about fast reduction algorithms and time complexity
in Refs. [49,50].
3.3. Ensemble reducts with forward search and post-pruning
Reducts are attribute subsets which have the same discern-
ing power as the original data but the number of attributes
are reduced. Therefore, reduction can be looked as a feature
subset selection process. In the view of rough set theory,
each reduct does not lose any information of the raw data.
So the classifiers trained with reducts will have a good gen-
eralization power. What is more, the classifiers trained with
different reducts should be diverse because they are trained
in different attribute subspaces. It seems a good solution to
construct learning ensemble with rough-set-based reducts. As
reduction is independent of specific learning algorithms, any
inducing algorithm can be employed, and we denote learning
algorithm by T, which can be CART, ID3, C4.5, Bayes network
or SVM, etc.
With experimental analysis, we find that there are usu-
ally hundreds of reducts for a given data. This puts forward
a problem: whether we require using all the reducts or not.
Literatures showed that selecting part of base classifiers for
ensemble is preferred [43,45]. However, how to select base
classifiers is still an open problem [47]. Although a number
of diversity measures and search strategies were proposed for
selective ensembles, no technique consistently gets better re-
sults than others in experiments and applications [43–46]. Here
we present a simple and yet effective method for selecting
classifiers.
In the method, we select base classifiers via directly evalu-
ating prediction accuracy of ensemble systems. At first all the
base classifiers are evaluated with 10-fold cross validation and
sorted with descending order according to average accuracy.
We select two classifiers with the highest accuracies into the
ensemble system. Then we select one classifier from the rest in
each stage and add it into the ensemble system. The selected
classifier maximizes the classification performance of the
ensemble system. With this processing, the base classifiers are
sequentially added into the ensemble system. During this pro-
cess, we record the classification accuracies of the ensemble
system. Generally speaking, accuracy of the ensemble system
will steeply go up at first, and arrive at a peak value, then
decrease. We call this step as an accuracy-guided forward
search. After the forward search, we get a curve of accuracy.
It is easy to see where the ensemble system gets the maximal
accuracy. The classifiers added after the peak value should
be deleted from the system. This step is call post-pruning.
Formally, the algorithm can be written as follows.
Algorithm 4. Selective ensemble rough subspaces with for-
ward search and post-pruning (FS-PP-EROS)
Input: IS = 〈U, C ∪D, V, f 〉, base inducer T .
Output: Ensemble system C∗.
1. compute a set of reducts {Bi |iN} with algorithm
3//N is the number of reducts;
2. for i = 1 to N
3. train a classifier C={Ci}i=1,...,N with reducts
B = {Bi}i=1,...,N ;
4. evaluate classification accuracy of C with 10-fold
cross validation;
5. end for
6. Sort C = {Ci}i=1,...,N with descending order
according to average accuracy.
7. C =
N⋃
3=1
Ci, C
∗ ← {C1, C2}, f ← ∅, i ← 1;
8. While i < N − 1 Do
9. ∀Ck ∈ C : Compute the accuracy fk of ensemble
system C′k = C∗ ∪ Ck;
10. select Cm such that fm=max
Ck∈C
(fk): C∗ ← C∗∪Cm,
f i ← fm;
11. i ← i + 1;
12. C ← C − C∗;
13. end while
14. find l where fl =max
i
(fi);
15. C∗ ← C∗ − N∪
i=l Ci ;
16. combine C∗ with plurality voting.
Algorithm 4 trains a set of classifiers with all the reducts,
and then sequentially adds them into the ensemble system. We
record the classification performance of the ensemble system as
the evaluating measure. Post-pruning is conducted by eliminat-
ing the base classifiers which are added after the peak accuracy.
In this paper, we denote the forward search ensemble of rough
subspaces by FS-EROS, and forward search and post-pruning
Please cite this article as: Q. Hu, et al., EROS: Ensemble rough subspaces, Pattern Recognition (2007), doi: 10.1016/j.patcog.2007.04.022
6 Q. Hu et al. / Pattern Recognition ( ) –
ARTICLE IN PRESS
ensemble of rough subspaces by FS-PP-EROS, respectively.
Algorithm 4 can be divided into four main steps: (1) searching
N reducts with time complexity O(|U ||C|+|C−core|2|U ||N |),
(2) training and evaluating N base classifiers, (3) combining
base classifiers with time complexity O(|N |2|U |), and (4) delet-
ing the base classifiers after the maximal accuracy. Time com-
plexity of the second step depends on inducing algorithms one
use. Fortunately, the base classifiers can be parallel trained and
evaluated.
Prepruning is preferred if accuracy monotonously increases
with the number of ensembled classifiers. In this case, we can
stop forward selection if the classification performance of the
ensemble system decreases. However, we find it is not the case
in experiments and practical applications. That is, accuracy
does not monotonously increase with the number of ensembled
classifiers and prepruning may lead to under-fitting.
FS-PP-EROS is essentially a subspace algorithm for con-
structing classifier ensemble. Different from bagging and
boosting, subspace methods try to produce diversity via input
feature perturbation. Each classifier is built with part of the
feature set. It will lead to some advantages for constructing an
ensemble.
Lessening dimensionality curse: In real-world situations, the
relevant features for classification tasks are often unknown a
prior. Therefore, many candidate features are given for well
presenting the recognition task. Directly building classifier with
many redundant and irrelevant features will significantly dete-
riorate the generalization power of learned classifiers. The sub-
space method can help to avoid this problem via reducing fea-
tures in constructing a base classifier, accordingly improves the
generalization of the base classifiers as well as the ensemble
systems [21,23].
Stabling learning algorithms: In Ref. [11] Breiman pointed
out, for unstable learning algorithms, bagging works well; how-
ever, it degrades the performance of stable procedures. With
a number of experiments, Zhou and Yu showed that perturb-
ing input attributes worked well for stable learning algorithms
[22]. Since the base classifiers work in the different subspaces,
EROS will lead to the performance improvement for unstable
classifiers and stable algorithms.
Guaranteeing good feature subsets: Compared with ran-
dom subspace method and attribute bagging, EROS presents
a systematic method to get a set of attribute subsets which do
not loss the distinguishing information in the original data.
However, the former two methods randomly select attributes;
thus, the quality of the base classifiers is uncertain. Therefore,
rough subspace ensembles have more opportunity to get good
Table 1
Data description
Data Samples Features Classes Reducts
1 Dermatology (Derm) 366 34 6 160
2 Ionosphere (Iono) 351 34 2 195
3 Sonar, Mines vs. Rocks (Sonar) 208 60 2 248
4 Wisconsin Diagnostic Breast Cancer (WDBC) 569 30 2 212
5 Wine recognition (Wine) 118 13 3 136
generalization than the random subspace method and the at-
tribute bagging method.
We can see the trend of variation of classification accuracy
when adding new base classifiers into ensemble systems as to
FS-PP-EROS. In practice, we do not require stopping ensemble
until no base classifier is rested. We can stop ensemble if no
improvement is observed after several reducts are added, which
will reduce the training time.
What is more, as outlined in Ref. [21], compared with sample
subsets, two advantages of attribute subspace methods can be
added. Projections are smaller in size than the original training
set. As a result, the amount of data transferred and duplicated
during the creation of the training sets is smaller than in bag-
ging. The reduced size of the data set results in faster induction
of classifiers.
The class distribution of the original data does not change
in subspace methods. All classes appearing in the original data
are still represented in the training sets created by projection,
which guarantees that each predictor used in voting is built with
full class representation. However, some of the classes present
in the original training set may be underrepresented or even
absent from some of the derived training sets in bagging, which
weaken predictors constructed from those sets.
Certainly, there are some disadvantages of EROS. Attribute
reduction increases the computational complexity of construct-
ing ensemble. The mail time in EROS is spent on attribute
reduction. The time complexity of the algorithm to search
multiple reducts is O(|N‖U‖C|2). In experiments, we can get
reducts within several minutes for data sets with ten thousands
of samples and tens of attributes. Searching multiple reducts is
time-consuming if there are a great number of attributes and
samples [49,50]. What is more, some of the data sets just have
a few reducts; accordingly, we cannot get enough reducts for
constructing ensemble systems.
4. Empirical analysis
Five data sets from the University of California at Irvine
(UCI) Machine Learning Repository [41] are used in the
empirical study. The information about the data is shown in
Table 1. The last column is the number of reducts for each
data. We can find that more than one hundred reducts are found
by conducting reduction on the data.
Two popular learning algorithms CART and SVM are intro-
duced to induce base classifiers in the experiments. Here, we
select Gini index as split criterion for CART and use linear
SVM. The statistics of classification accuracies based on
Please cite this article as: Q. Hu, et al., EROS: Ensemble rough subspaces, Pattern Recognition (2007), doi: 10.1016/j.patcog.2007.04.022
ARTICLE IN PRESS
Q. Hu et al. / Pattern Recognition ( ) – 7
6 7 8 9 34
0.6
0.7
0.8
0.9
Derm: CART
6 7 8 9 34
0.7
0.8
0.9
Derm: SVM
9 10 11 12 13 14 15 34
0.82
0.84
0.86
0.88
0.9
0.92
Iono: CART
9 10 11 12 13 14 15 34
0.75
0.8
0.85
Iono: SVM
6 7 60
0.5
0.6
0.7
0.8
Sonar: CART
6 7 60
0.55
0.6
0.65
0.7
0.75
Sonar: SVM
8 9 10 11 12 13 14 30
0.9
0.92
0.94
0.96
WDBC: CART
8 9 10 11 12 13 14 30
0.92
0.94
0.96
WDBC: SVM
4 5 6 13
0.75
0.8
0.85
0.9
0.95
Wine: CART
4 5 6 13
0.8
0.85
0.9
0.95
Wine: SVM
(1) (2)
(4)(3)
(5) (6)
(8)(7)
(9) (10)
Fig. 3. Box plots of the classification accuracies of reducts grouped by attribute number in the reducts.
10-fold cross validation are shown in box plots in Fig. 3. The
x-axis is the number of features in the reducts used to train
classifiers and y-axis is the statistical values of accuracies. Note
that the last column in each plot is the accuracy of the original
data, which provides a base line to evaluate the performance of
reduction.
Please cite this article as: Q. Hu, et al., EROS: Ensemble rough subspaces, Pattern Recognition (2007), doi: 10.1016/j.patcog.2007.04.022
8 Q. Hu et al. / Pattern Recognition ( ) –
ARTICLE IN PRESS
0 50 100 150
0.6
0.7
0.8
0.9
1
Number of fused classifiers Number of fused classifiers
Number of fused classifiers Number of fused classifiers
Number of fused classifiers Number of fused classifiers
Number of fused classifiers Number of fused classifiers
Number of fused classifiers Number of fused classifiers
A
c
c
u
ra
c
y
A
c
c
u
ra
c
y
A
c
c
u
ra
c
y
A
c
c
u
ra
c
y
A
c
c
u
ra
c
y
A
c
c
u
ra
c
y
A
c
c
u
ra
c
y
A
c
c
u
ra
c
y
A
c
c
u
ra
c
y
A
c
c
u
ra
c
y
Derm (CART)
0 50 100 150
0.7
0.8
0.9
1
Derm (SVM)
0 50 100 150 200
0.8
0.85
0.9
0.95
Iono (CART)
0 50 100 150 200
0.7
0.75
0.8
0.85
Iono (SVM)
0 50 100 150 200 250
0.5
0.6
0.7
0.8
Sonar (CART)
0 50 100 150 200 250
0.5
0.6
0.7
0.8
Sonar (SVM)
0 50 100 150 200
0.85
0.9
0.95
1
Wdbc (CART)
0 50 100 150 200
0.92
0.94
0.96
0.98
1
Wdbc (SVM)
0 20 40 60 80 100 120
0.7
0.8
0.9
1
Wine (CART)
0 20 40 60 80 100 120 140
0.8
0.9
1
Wine (SVM)
Random
Ascend
Decsend
FS
Random
Ascend
Decsend
FS
Random
Ascend
Decsend
FS
Random
Ascend
Decsend
FS
Random
Ascend
Decsend
FS
Random
Ascend
Decsend
FS
Random
Ascend
Decsend
FS
Random
Ascend
Decsend
FS
Random
Ascend
Decsend
FS
Fig. 4. Variation of classification accuracies with number of base classifiers.
Box plot produces a box and whisker plot with one box for
each vector. The boxes have lines at the lower quartile, median,
and upper quartile values. The whiskers are lines extending
from each end of the boxes to show the extent of the rest
of the data. Outliers are data with values beyond the ends of
the whiskers. It is easy to find that not all of the reducts get
Please cite this article as: Q. Hu, et al., EROS: Ensemble rough subspaces, Pattern Recognition (2007), doi: 10.1016/j.patcog.2007.04.022
ARTICLE IN PRESS
Q. Hu et al. / Pattern Recognition ( ) – 9
Table 2
Comparison of classification accuracies of different classification systems with CART
Data Raw Max-Red Max-Des Max-As Max-Rand Max-FS
1 Derm 0.9198 0.9258 0.9536 0.9317 0.9344 0.9836
2 Iono 0.8804 0.9172 0.9316 0.9174 0.9316 0.9573
3 Sonar 0.6550 0.7950 0.8221 0.7163 0.7596 0.8462
4 Wdbc 0.9209 0.9625 0.9736 0.9578 0.9613 0.9824
5 Wine 0.8909 0.9727 0.9831 0.9661 0.9746 0.9831
Table 3
Comparison of classification accuracies of different classification systems with SVM
Data Raw Reducts Max-Des Max-As Max-Rand Max-FS
1 Derm 0.9714 0.9349 0.9699 0.9262 0.9290 0.9781
2 Iono 0.8429 0.8744 0.8860 0.8291 0.8319 0.8974
3 Sonar 0.6421 0.7779 0.8029 0.7740 0.7981 0.8269
4 Wdbc 0.9699 0.9740 0.9740 0.9578 0.9613 0.9877
5 Wine 0.9857 0.9818 0.9915 0.9915 0.9831 1.0000
good performance. In particular, all classifiers trained with the
reducts are poorer than the classifier built with the original data
as to Derm data. As to Sonar and SVM, the reducts outperform
the original data.
Fig. 4 shows the accuracy curves of ensemble systems
varying with the number of fused base classifiers. Four fusion
strategies are compared in the experiments. FS are accuracies
of ensemble systems with accuracy-guided forward search
strategy. Descend are the classification accuracies of ensemble
systems, where the base classifiers are combined into the en-
sembles with the descending order of accuracies, which means
that the better classifiers are firstly included in the ensembles.
Accordingly, the curves marked with Random and Ascend are
the accuracies of ensembles with random and ascending order
strategies.
We can find that the forward Search strategy is consistently
better than the other three as to the five data sets. What is more,
as to Derm-CART, Derm-SVM, Iono-SVM, Sonar-CART and
WDBC-SVM, the descend strategy is far better than random
and ascending strategies, which shows that the performance
of the base classifiers greatly influences the ensemble. With-
out considering the diversity, selecting the better classifier for
ensemble will be a good solution. However, forward search
strategy is consistently better than descend ensembles in the
experiments, which shows that combining the best base classi-
fiers cannot get the optimal performance; diversity among base
classifiers also plays an important role in constructing multiple
classifier systems.
Interestingly, in the forward search and descend curves, we
can find that the classification accuracies of ensemble systems
do not monotonously increase with the number of the fused
base classifiers. At first the accuracies increase, reach a peak
value, and then decrease. It shows that combining a part of
base classifiers will be better than combining all of them. In
these algorithms, we should conduct a pruning method on the
ensemble systems; otherwise, over-fitting will appear in these
cases. However, it is not clear as to random and ascending order
fusions.
Tables 2 and 3 show the comparisons of classification accu-
racies with CART and SVM, respectively, where Raw denotes
the accuracies with the original data. Max-Red, Max-Des,
Max-As, Max-Rand and Max-FS mean the maximal accuracies
of the base classifiers, descending-order fusion, ascending-
order fusion, random-order fusion and forward search strate-
gies, respectively. We can see that Max-FS fusion gets the best
performance in all of the techniques. Especially, classification
performance is greatly improved if the average accuracies of
base classifiers are rather low, such as data sonar.
Furthermore, in order to show the properties of FS-EROS,
some comparison experiments are conducted. We build the
same number of base classifiers with bagging and random sub-
space methods. Fig. 5 shows the accuracy curves of the three al-
gorithms, where Random, Bagging and FS mean accuracies of
the random subspace method, Bagging methods and FS-EROS,
respectively.
In Fig. 5, FS-EROS gets the best or near best classification
performance among all the cases. As to Iono, Sonar and Wdbc,
FS-EROS is far better than the other ensemble methods. From
Figs. 3(1), 3(2), we can see most of the base classifiers trained
with reducts of Derm are worse than the classifiers trained with
the original data. However, their ensembles with FS strategy are
still comparable. What is more, it is easy to see there are evident
peak values in the classification accuracy of FS-EROS, which
brings convenience in pruning the superfluous base classifiers.
FS-ROS arrives at the peak values of classification accuracies
with very few base classifiers in all of the cases. This shows
most of the base classifiers can be deleted from the ensemble
systems. Therefore, efficient and effective ensemble systems
can be yielded with FS-PP-EROS.
Genetic algorithm is used to select base classifiers for con-
structing a compact ensemble [4,43]. Here we compare FS-
PP-EROS strategy with naive GA-based search proposed in
Please cite this article as: Q. Hu, et al., EROS: Ensemble rough subspaces, Pattern Recognition (2007), doi: 10.1016/j.patcog.2007.04.022
10 Q. Hu et al. / Pattern Recognition ( ) –
ARTICLE IN PRESS
0 50 100 150
0.9
0.95
1
0.9
0.95
0.85
1
Number of fused classifiers Number of fused classifiers
Number of fused classifiers Number of fused classifiers
Number of fused classifiers Number of fused classifiers
Number of fused classifiers Number of fused classifiers
Number of fused classifiers Number of fused classifiers
A
c
c
u
ra
c
y
A
c
c
u
ra
c
y
A
c
c
u
ra
c
y
A
c
c
u
ra
c
y
A
c
c
u
ra
c
y
A
c
c
u
ra
c
y
A
c
c
u
ra
c
y
A
c
c
u
ra
c
y
A
c
c
u
ra
c
y
A
c
c
u
ra
c
y
Derm (CART)
0 50 100 150
Derm (SVM)
0 50 100 150 200
0.85
0.9
0.95
Iono (CART)
0 50 100 150 200
0.8
0.85
0.9
Iono (SVM)
0 50 100 150 200 250
0.65
0.7
0.75
0.8
0.85
Sonar (CART)
0 50 100 150 200 250
0.65
0.75
0.7
0.8
0.85
Sonar (SVM)
0 50 100 150 200
0.92
0.96
0.94
0.98
Wdbc (CART)
0 50 100 150 200
0.94
0.95
0.96
0.97
0.98
Wdbc (SVM)
0 50 100
0.85
0.9
0.95
1
Wine (CART)
0 50 100
0.9
0.95
1
Wine (SVM)
Bagging
Random
FS
Bagging
Random
FS
Bagging
Random
FS
Bagging
Random
FS
Bagging
Random
FS
Bagging
Random
FS
Bagging
Random
FS
Bagging
Random
FS
Bagging
Random
FS
Bagging
Random
FS
Fig. 5. Comparison of accuracies of FS-EROS, bagging and random subspace method.
Ref. [4], where a binary vector is used as code of Popula-
tion, 0 means the base classifier is excluded and 1 means the
base classifier is included. The fitness function is the accuracy
of ensemble systems. We stop the optimization after 100
iterations or if incremental of fitness is less than 0.001 after 10
iterations.
Please cite this article as: Q. Hu, et al., EROS: Ensemble rough subspaces, Pattern Recognition (2007), doi: 10.1016/j.patcog.2007.04.022
ARTICLE IN PRESS
Q. Hu et al. / Pattern Recognition ( ) – 11
Table 4
Comparison of three ensemble strategies with CART learning algorithm
All GA search FS-PP-EROS
Size Accuracy Size Accuracy Size Accuracy
Derm 160 0.9290 85 0.9627 15 0.9836
Iono 195 0.9174 93 0.9486 11 0.9573
Sonar 248 0.7115 116 0.7779 12 0.8462
Wdbc 212 0.9561 115 0.9632 7 0.9824
Wine 136 0.9576 73 0.9909 31 0.9831
Average 190.2 0.8943 96.40 0.9287 15.2 0.9505
Table 5
Comparison of three ensemble strategies with SVM learning algorithm
All GA search FS-PP-EROS
Size Accuracy Size Accuracy Size Accuracy
Derm 160 0.9262 80 0.9631 4 0.9781
Iono 195 0.8291 95 0.8601 10 0.8974
Sonar 248 0.7740 124 0.7929 9 0.8269
Wdbc 212 0.9666 104 0.9755 7 0.9877
Wine 136 0.9831 70 0.9909 3 1.0000
Average 190.2 0.8958 94.6 0.9165 6.6 0.9380
The sizes of ensembles and classification accuracies with
three strategies are shown in Tables 4 and 5. Comparing the
classification accuracies, we can find FS-PP-EROS is bet-
ter than or comparable with GA based ensembles and the
ensembles with all of base classifiers. In particular, the accu-
racy is greatly improved as to data sonar. In the same time, the
sizes of ensemble systems are significantly reduced. Therefore,
FS-PP-EROS is able to construct a more compact and effective
ensemble systems.
5. Conclusion and future work
Ensemble learning is a simple and yet effective technique
to improve generalization power of a recognition system. A
number of solutions for constructing classifier ensembles were
proposed in the literatures. However, how to construct multi-
ple base classifiers with enough accuracies and diversity is still
an open problem. In this paper, we present a novel ensemble
learning system, named FS-PP-EROS, which introduces rough-
set-based attribute reduction algorithm to get a set of reducts of
the original data and train base classifiers with reducts. What
is more, we employ an accuracy-guided forward search strat-
egy to sequentially add base classifiers into the ensemble sys-
tem. Experiments show the accuracy of the ensemble system
increases at first, and arrive at a peak value, then decrease. It
shows the base classifiers added after the peak value can be
eliminated from the final ensemble system. Accordingly, the
size of the ensemble systems is reduced. Theoretically speak-
ing, reducts are the optimal attribute subsets of the original
data because they do not lose any indistinguishing information
and have the least redundancy. Therefore, the base classifiers
trained with reducts will get good generalization. At the same
time, the base classifiers in FF-PP-EROS are constructed in dif-
ferent subspaces; there is a great opportunity for them to get
diversity. In experiments, we also find that accuracy-guided for-
ward search and post-pruning strategy can delete most of the
base classifiers and improve classification. Via comparing some
learning systems with experiments the following conclusions
are attained. (1) The performance of ensemble systems greatly
depends on the fusion order. Accuracy-guided forward search
strategy outperforms the descending-order ensemble and the
ascending-order ensemble. (2) The classification accuracies of
ensemble systems do not monotonously increase with the num-
ber of fused base classifiers. The accuracies of ensemble first
go up to a peak value, then decrease. This shows selectively
combining part of the reducts is better than combining all of
the reducts. (3) The performance of FS-PP-EROS ensembles is
better than bagging and random subspace methods in most of
the cases in terms of accuracy and size of ensemble systems.
What is more, FS-PP-EROS can improve or keep classification
accuracy with very few base classifiers relative to GA-based
selective ensembles.
There are several research directions in ensemble learning. In
this work, we discuss the strategies of generating diverse base
classifiers and selecting base classifiers. And we combine the
selected base classifiers with voting techniques. In fact, there
are a number of ways to select and combine base classifiers.
We will incorporate them to improve this work in future.
References
[1] J. Czyz, J. Kittler, L. Vandendorpe, Multiple classifier combination
for face-based identity verification, Pattern Recognition 37 (2004)
1459–1469.
[2] M. Aksela, J. Laaksonen, Using diversity of errors for selecting members
of a committee classifier, Pattern Recognition 39 (2006) 608–623.
[3] T.K. Ho, The random subspace method for constructing decision forests,
IEEE Trans. Pattern Anal. Mach. Intell. 20 (1998) 832–844.
[4] Q.H. Hu, D.R. Yu, M.Y. Wang, in: Constructing Rough Decision Forests,
D. Slezak et al. (Eds.), RSFDGrC 2005, Lecture Notes in Artificial
Intelligence, vol. 3642, 2005, pp. 147–156.
[5] J. Cao, M. Ahmodi, M. Shridhar, Recognition of handwritten numerals
with multiple feature and multistage classifier, Pattern Recognition 28
(1995) 153–160.
[6] E. Stamatatos a, Gerhard Widmer, Automatic identification of music
performers with learning ensembles, Artif. Intell. 165 (2005) 37–56.
[7] T.G. Dietterich, An experimental comparison of three methods for
constructing ensembles of decision trees: bagging, boosting, and
randomization, Mach. Learn. 40 (2000) 139–157.
[8] J. Kittler, M. Hatef, R.P.W. Duin, J. Matas, On combining classifiers,
IEEE Trans. Pattern Anal. Mach. Intell. 20 (1998) 226–239.
[9] S. Geman, E. Bienenstock, R. Doursat, Neural networks and the
bias/variance dilemma, Neural Comput. 4 (1992) 1–58.
[10] A. Krogh, J. Vedelsby, Neural network ensembles, in: G. Tesauro, D.S.
Touretzky, T.K. Leen (Eds.), Advances in Neural Information Processing
Systems, MIT Press, Cambridge, MA, 1995, pp. 231–238.
[11] L. Breiman, Bagging predictors, Mach. Learn. 24 (1996) 123–140.
[12] P.M. Granitto, P.F. Verdes, H.A. Ceccatto, Neural network ensembles:
evaluation of aggregation algorithms, Artif. Intell. 16 (2005) 3139–3162.
[13] L. Breiman, Random forests, Mach. Learn. 45 (2001) 5–32.
[14] K.J. Cherkauer, Human expert level performance on a scientific image
analysis task by a system using combined artificial neural networks, in:
Proceedings of 13th AAAI workshop on integrating multiple learned
models for improving and scaling machine algorithms, 1996, pp. 15–21.
Please cite this article as: Q. Hu, et al., EROS: Ensemble rough subspaces, Pattern Recognition (2007), doi: 10.1016/j.patcog.2007.04.022
12 Q. Hu et al. / Pattern Recognition ( ) –
ARTICLE IN PRESS
[15] R. Maclin, J.W. Shavlik, Combining the predictions of multiple
classifiers: using competitive learning to initialize neural networks,
in: Proceedings of the 14th international joint conference on artificial
intelligence, Morgan Kaufmann, San Mateo, CA, 1995, pp. 524–530.
[16] Z.H. Zhou, Y. Jiang, NeC4.5: Neural Ensemble Based C4.5, IEEE Trans.
Knowledge Data Eng. 16 (2004) 770–773.
[17] R.E. Schapire, The strength of weak learnability, Mach. Learn. 5 (1990)
197–227.
[18] Y. Freund, Boosting a weak learning algorithm by majority, in:
Proceedings of the Third Annual Workshop on Computational Learning
Theory, 1990, pp. 202–216.
[19] Y. Freund, Boosting a weak learning algorithm by majority, Inform.
Comput. 121 (1996) 256–285.
[20] Y. Freund, R.E. Schapire, A decision-theoretic generalization of on-line
learning and an application to boosting, in: Proceedings of the Second
European Conference on Computational Learning Theory, Springer,
1995, pp. 23–37.
[21] R. Bryll, R. Gutierrez-Osuna, F. Quek, Attribute bagging: improving
accuracy of classifier ensembles by using random feature subsets, Pattern
Recognition 36 (2003) 1291–1302.
[22] Z.H. Zhou, Y. Yu, Ensembling local learners through multimodal
perturbation, IEEE Trans. SMC—Part B: Cybernetics 35 (2005)
725–735.
[23] K. Tumer, N.C. Oza, Input decimated ensembles, Pattern Anal. Appl. 6
(2003) 65–77.
[24] S. Gunter, H. Bunke, Feature selection algorithms for the generation
of multiple classifier systems and their application to handwritten word
recognition, Pattern Recognition Lett. 25 (2004) 1323–1336.
[25] Y. Kim, Toward a successful CRM: variable selection, sampling, and
ensemble, Decision Support Systems 41 (2006) 542–553.
[26] Z. Pawlak, Rough Sets—Theoretical Aspects of Reasoning about Data,
Kluwer Academic, Dordrecht, 1991.
[27] L. Polkowski, T.Y. Lin, S. Tsumoto (Eds.), in: Rough Set Methods
and Applications: New Developments in Knowledge Discovery in
Information Systems (Studies in Fuzziness and Soft Computing), vol.
56, Physica-Verlag, 2000.
[28] J. Wang, J. Wang, Reduction algorithms based on discernibility matrix:
the ordered attributes method, J. Comput. Sci. Technol. 16 (6) (2001)
489–504.
[29] W. Roman, S. Skowron, Rough set methods in feature selection and
recognition, Pattern Recognition Lett. 24 (2003) 833–849.
[30] A.E. Hassanien, Rough set approach for attribute reduction and rule
generation: a case of patients with suspected breast cancer, J. Am. Soc.
Inform. Sci. Technol. 55 (2004) 954–962.
[31] R. Jensen, Q. Shen, Semantics-preserving dimensionality reduction:
rough and fuzzy-rough-based approaches, IEEE Trans. Knowledge Data
Eng. 16 (2004) 1547–1571.
[32] Q. Hu, D. Yu, Z. Xie, Information-preserving hybrid data reduction
based on fuzzy rough techniques, Pattern Recognition Lett. 27 (2006)
414–423.
[33] Q. Wu, D. Bell, M. Mcginnity, Multiknowledge for decision making,
Knowledge Inform. Systems 7 (2005) 246–266.
[34] Q. Shen, R. Jensen, Selecting informative features with fuzzy-rough sets
and its application for complex systems monitoring, Pattern Recognition
37 (2004) 1351–1363.
[35] Q. Hu, D. Yu, Z. Xie, Hybrid data reduction for classification with a
fuzzy-rough set technique, in: Fifth SIAM Conference on Data Mining,
2005.
[36] R.B. Bhatt, M. Gopal, On fuzzy-rough sets approach to feature selection,
Pattern Recognition Lett. 26 (2005) 965–975.
[37] A. Skowron, C. Rauszer, The discernibility matrices and functions
in information systems. Intelligent Decision Support: Handbook of
Applications and Advances of Rough Set Theory, 1991 pp. 331–362.
[38] J. Jelonek, K. Krawiec, R. Slowinski, Rough set reduction of attributes
and their domains for neural networks, Comput. Intell. 11 (1995)
339–347.
[39] G.Y. Wang, Rough reduction in algebra view and information view, Int.
J. Intell. Systems 18 (2003) 679–688.
[40] Y.M. Sun, M.S. Kamel, A.K.C. Wong, Empirical study on weighted
voting multiple classifiers, Lecture Notes in Computer Science, vol.
3686, 2005, pp. 335–344.
[41] C. Blake, E. Keogh, C.J. Merz, UCI Repository of Machine Learning
Databases. Dept. Inf. Comput. Sci., Univ. California, Irvine, CA, 1998,
〈http://www.ics.uci.edu/∼mlearn/MLRepository.html〉.
[42] H.W. Shin, S.Y. Sohn, Selected tree classifier combination based on both
accuracy and error diversity, Pattern Recognition 38 (2005) 191–197.
[43] Z.H. Zhou, J.X. Wu, W. Tang, Ensembling neural networks: many could
be better than all, Artif. Intell. 137 (2002) 239–263.
[44] T. Windeatt, Diversity measures for multiple classifier system analysis
and design, Information Fusion 6 (2005) 21–36.
[45] M. Aksela, J. Laaksonen, Using diversity of errors for selecting members
of a committee classifier, Pattern Recognition 39 (2006) 608–623.
[46] M.H. Nguyen, H.A. Abbass, R.I. McKay, Stopping criteria for ensemble
of evolutionary artificial neural networks, Appl. Soft Comput. 6 (2005)
100–107.
[47] R.E. Schapire, Y. Singer, Improved boosting algorithms using confidence-
rated predictions, Mach. Learn. 37 (1999) 297–336.
[48] L.I. Kuncheva, C.J. Whitaker, Measures of diversity in classifier
ensembles, Mach. Learn. 51 (2003) 181–207.
[49] Z.Y. Xu, Z.P. Liu, B.Y. Yang, W. Song, A Quick attribute reduction with
complexity of max(O(|C‖U|), O(|C|2|U/C|)), Chinese J. Comput. 29
(2006) 391–399.
[50] S. Liu, Q. Sheng, B. Wu, Z. Shi, F. Hu, Research on efficient algorithms
for rough set methods, Chinese J. Comput. 26 (5) (2003) 1–6.
About the Author—QINGHUA HU received the master degree in power engineering from Harbin Institute of Technology, Harbin, China in 2002. Now he is
a Ph.D. student with Harbin Institute of Technology. His research interests are focused on data mining, knowledge discovery in historical record database of
power plants with fuzzy and rough techniques. He has authored or coauthored more than 40 journal and conference papers in the areas of machine learning,
data mining and rough set theory.
About the Author—DAREN YU was born in Datong, China, in 1966. He received the M.Sc. and D.Sc. degrees from Harbin Institute of Technology, Harbin,
China, in 1988 and 1996, respectively. Since 1988, he has been working at the School of Energy Science and Engineering, Harbin Institute of Technology.
His main research interests are in modeling, simulation and control of power systems. He has published more than one hundred conference and journal papers
on power control and fault diagnosis.
Please cite this article as: Q. Hu, et al., EROS: Ensemble rough subspaces, Pattern Recognition (2007), doi: 10.1016/j.patcog.2007.04.022
