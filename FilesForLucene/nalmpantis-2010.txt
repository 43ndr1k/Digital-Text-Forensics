Low Resources Prepositional Phrase Attachment 
 
 
Pavlos Nalmpantis, Romanos Kalamatianos, Konstantinos Kordas and Katia Kermanidis  
Department of Informatics, Ionian University 
Corfu, Greece  
{cs200664, cs200611, cs200539, kerman}@ionio.gr
 
 
 
Abstract‚ÄîPrepositional phrase attachment is a major 
disambiguation problem when it‚Äôs about parsing natural 
language, for many languages. In this paper a low resources 
policy is proposed using supervised machine learning 
algorithms in order to resolve the disambiguation problem of 
Prepositional phrase attachment in Modern Greek. It is a first 
attempt to resolve Prepositional phrase attachment in Modern 
Greek, without using sophisticated syntactic annotation and 
semantic resources. Also there are no restrictions regarding 
the prepositions addressed, as is common in previous 
approaches.  
Decision Trees, Modern Greek, PP attachment, Supervised 
learning 
I.  INTRODUCTION 
The correct attachment of prepositional phrases (PPs) to 
another constituent in a sentence is a significant 
disambiguation problem for parsing natural languages. For 
example, take the following two sentences: 
1 She eats soup with a spoon. 
2 She eats soup with tomatoes. 
 
In sentence 1, the PP "with a spoon" is attached to the 
verb phrase (VP) "eats" denoting the instrument utilized for 
the eating action, thus making it the anchor phrase of the PP. 
Sentence 2 seems to differ only minimally from the first 
sentence, but, as can be seen from their syntax trees in Fig. 1 
and Fig. 2 respectively, their syntactic structure is quite 
different. The PP "with tomatoes" does not attach to the verb 
but to the noun phrase (NP) "soup" denoting the type of 
soup. 
 
 
 
Figure 1. Syntax tree of sentence 1 
 
 
Figure 2. Syntax tree of sentence 2 
PP attachment has many significant uses. It can be used, 
as referred above, for improving the performance of 
syntactic parsers, as it is a major source of ambiguity in 
natural language. It facilitates further semantic processing, 
and also constitutes an important pre-processing step in 
many information extraction systems. It has also been 
employed in speech processing as a filter in prosodic 
phrasing [1]. 
Over the years, many solutions have been proposed to 
resolve the disambiguation problem of PP attachment. 
Solutions include the use of machine learning algorithms [2]-
[3], statistical analysis using corpus-based pattern 
distributions and lexical signatures [4], as well as the back-
off model [5], the maximum entropy model [6] etc. 
However, these methods usually require many resources (i.e. 
syntactic annotation and often even semantic 
disambiguation) which are often unavailable for many 
languages. 
Regarding machine learning techniques, previous 
approaches have experimented with various learning 
schemata. Memory-based learning has been proposed [2], 
employing the 1-NN algorithm (IB1) and its tree-variation 
(IB1-IG). These results were compared to results of other 
methods and correct attachment performance turned out 
much better. Also, a nearest-neighbor algorithm has been 
proposed, employing a cosine similarity measure of 
pointwise mutual information [3]. The work described in [7] 
recreates the EDTBL (Error-Driven Transformation-Based 
Learning) experiment and compares the results with three 
machine learning algorithms, namely: Na√Øve Bayes, ID3 IG 
(Information Gain) and ID3 GR (Gain Ratio). The latter two 
are a tree variation of the k-NN algorithm. The experiment 
was conducted in two phases. In phase 1 the training set was 
2010 14th Panhellenic Conference on Informatics
978-0-7695-4172-3/10 $26.00 ¬© 2010 IEEE
DOI 10.1109/PCI.2010.34
78
gradually increased until all training examples were used. In 
phase 2 10-fold cross validation was used, and the summary 
of results for all the aforementioned approaches is shown in 
Table 1. 
In this paper we propose a methodology to resolve the 
disambiguation problem of PP attachment in Modern Greek 
using supervised machine learning algorithms given a 
dataset of feature-vectors extracted from a morphologically 
annotated corpus. The presented methodology is, to the 
authors‚Äô knowledge, a first attempt to resolve the PP 
attachment problem in Modern Greek using minimal 
linguistic resources, like grammars, tools, treebanks and 
semantic thesauri. Finally we do not put some restriction on 
the type of prepositional phrases, thus taking into account 
all the prepositions. 
TABLE I. SUMMARY OF RESULTS OF RELATED PREVIOUS WORK 
Algorithm  Results
IB1(1-NN) [2] 83.7% 
IB-IG(1-NN tree) [2] 84.1% 
Cosine similarity (NN) [3] 86.5% 
ID3 IG(k-NN) [7] 79% (phase 1)  79% (phase 2) 
ID3 GR(k-NN) [7] 78% (phase 1) 79% (phase 2) 
Na√Øve Bayes [7] 74% (phase 1) 76% (phase 2) 
 
The rest of this paper is organized as follows. Section 2 
introduces the properties of the Modern Greek language. The 
corpus used in the presented experiments, the feature 
extraction process, as well as an example of the creation of 
the learning vector is discussed in Section 3. Section 4 
describes in detail the experimental process and shows the 
results that were achieved in the experiments. The results are 
discussed quantitatively and qualitatively in Section 5, and 
future research prospects are proposed. Finally, the paper 
concludes with some interesting comments and remarks. 
II. MODERN GREEK PROPERTIES 
Modern Greek is a relatively free-word-order language. 
The ordering of the phrases within a sentence may vary 
without affecting its correct syntax or its meaning. Internal 
phrase structure is stricter. Within the noun phrase, 
adjectives usually precede the noun (e.g. ‚Äú  	
‚Äù, 
[to megalo spiti], 'the big house'), while possessors follow it 
(e.g. ‚Äú 	
 ‚Äù, [to spiti mu], 'my house'). Regarding 
VPs, certain grammatical elements attach to the verb as 
clitics and form a rigidly ordered group together with it. This 
applies particularly to unstressed object pronouns, negation 
particles, the tense particle ‚Äú‚Äù [a], and the subjunctive 
particle ‚Äú‚Äù [na] [8]. 
In Modern Greek, prepositions normally require the 
accusative case: ‚Äú
‚Äù (from), ‚Äú‚Äù (for), ‚Äú‚Äù (with), 
‚Äú‚Äù (after), ‚Äú‚Äù (without), ‚Äú‚Äù (as) and ‚Äú	‚Äù (to, in 
or at). The preposition ‚Äú	‚Äù, when followed by a definite 
article, fuses with it into forms like ‚Äú	‚Äù (	 + ) and 
‚Äú	‚Äù (	 + ). For this reason, lemmatization of the 
preposition is required [8]. 
III. MODERN GREEK PP-ATTACHMENT 
A. Corpus and Pre-processing 
The text corpus used in the experiments is the 
ILSP/ELEFTHEROTYPIA corpus [9]. It consists of 5244 
sentences; it is balanced in domain and genre, and manually 
annotated with complete morphological information. Further 
(phrase structure) information is obtained automatically by a 
multi-pass chunker [10]. 
During chunking, NPs, VPs, PPs, adverbial phrases 
(ADP) and conjunctions (CON) are detected via multi-pass 
parsing. The chunker exploits minimal linguistic resources: a 
keyword lexicon containing 450 keywords (i.e. closed-class 
words such as articles, prepositions etc.) and a suffix lexicon 
of 300 of the most common word suffixes in Modern Greek. 
The chunked phrases are non-overlapping. Embedded 
phrases are flatly split into distinct phrases. Nominal 
modifiers in the genitive case are included in the same phrase 
with the noun they modify; base nouns joined by a 
coordinating conjunction are grouped into one phrase. The 
chunker identifies basic phrase constructions during the first 
passes (e.g. adjective-nouns, article-nouns), and combines 
smaller phrases into longer ones in later passes (e.g. 
coordination, inclusion of genitive modifiers, compound 
phrases). 
B. Feature Selection 
The feature vector is necessary to enable the 
classification algorithms to classify the PP attachment. The 
attributes which can be used to form the feature vector need 
to be related to the PP attachment ambiguity resolution task, 
and they vary in the bibliography. The ones encountered 
most commonly in previous work (e.g. [11]) are: the number 
of commas between the PP and the anchor candidate, the 
number of other punctuation marks between the PP and the 
anchor candidate, the number of words between the PP and 
the anchor candidate, the POS-tag of the last token of the 
anchor candidate, the lemma of the PP, the number of PPs 
between the PP and the anchor candidate, the label of the 
phrase immediately before the PP, the anchor candidate, etc. 
In the present work, a feature vector is formed for every 
anchor candidate and every preposition in a given corpus 
sentence. Thus, the syntactic freedom of Modern Greek is 
taken into account. Of the features we mentioned earlier, 
after a number of experiments conducted using the tools of 
the WEKA machine learning workbench (Waikato 
Environment for Knowledge Analysis) [12], which enables 
the experimentation with various classification algorithms, 
the following features were selected to form our feature 
vector: 
 The lemma of the preposition introducing the PP.  
 The type of the phrase immediately before the PP. 
 The anchor candidate. 
 The POS-tag of the last token of the anchor 
candidate. 
 The number of words between the PP and the 
anchor candidate. 
 The number of PPs between the PP and the anchor 
79
candidate.  
 The number of commas between the PP and the 
anchor candidate.  
 The number of other punctuation marks between 
the PP and the anchor candidate. 
 
The feature ‚ÄúCorrect attachment‚Äù was used as the 
classification class of the vector in the experiments 
conducted. The reason we selected the feature ‚ÄúPOS-tag of 
the last token of the anchor candidate‚Äù is that, in most cases, 
the headword of NPs and the main verb in VPs is the last 
token of the phrase. The anchor candidate may be preceding 
or following the PP, as the syntactic freedom of the language 
does not impose restrictions on the ordering. Therefore, no 
such restrictions have been imposed on the described feature 
set.  
C. Feature Vector Extraction 
The process of exporting values for each feature vector is 
automated. We created a program that is written in C 
language that automatically identifies the first eight features 
of our vector and stores the results in an Excel file. This 
program gives all possible attachments (anchor candidates) 
of a PP in a sentence, that are NPs and VPs, as these phrase 
types constitute the most significant error source for PP 
attachment. However it is fairly easy, in future research, to 
include other anchor candidate phrases types. The correct 
class label was assigned to every extracted feature vector by 
three language experts, manually. This feature takes the 
values of TRUE or FALSE and indicates whether the 
attachment example represented in the given vector is correct 
or not. Inter-annotation agreement between the experts was 
90%. For the remaining 10%, where the experts didn‚Äôt 
initially agree, a discussion among them followed, resulting 
to a common decision about correct attachment.  
An example of the feature vector extraction process 
follows. Take the following annotated sentence (firstly 
presented in the Modern Greek language and then translated 
in English). 
 
Modern Greek: 
NP[<N*	> 
<A*
> 
<N*>] PP[<S*> <T*> 
*
<N*
>] ADP[<R*>] NP[<T*> 
*
<N*
>.<F>] 
 
English: 
NP[The dialogue of a contemporary worker] PP[with 
his wife] ADP[yesterday] NP[afternoon.] 
 
Each word in the sentence is annotated with its POS-tag 
and lemma. Table 2 shows the value of each symbol that 
represents the POS-tag of a word. 
The program will extract two feature vectors, which are 
presented in Table 3, giving us two possible attachments: 
The PP ‚Äúwith his wife‚Äù could be attached to the NP ‚ÄúThe 
dialogue of a contemporary worker‚Äù or to the NP 
‚Äúafternoon‚Äù. These two possible attachments will be 
examined by the three language experts mentioned earlier 
and they will decide which the correct attachment is. In this 
case the PP ‚Äúwith his wife‚Äù (‚Äú  	‚Äù in Modern 
Greek) is correctly attached to the NP ‚ÄúThe dialogue of a 
contemporary worker‚Äù because it denotes the person with 
whom the dialogue took place. 
TABLE II. POS-TAG SYMBOLS 
Symbol Value
N Noun 
A Adjective 
S Preposition 
T Article 
R Adverb 
F Punctuation mark 
TABLE III. FEATURE VECTORS 
L Pre-P AC POS-tag #W #PPs #C #PM 
 NP NP N 0 0 0 0 
 NP NP F 1 0 0 0 
Extracted feature vectors from example sentence; L=Lemma, Pre-
P=Previous Phrase, AC=Anchor Candidate, POS-tag=Anchor Candidate 
last word, #W=word distance between PP and anchor candidate, 
#PPs=number of PP‚Äôs between PP and anchor candidate, #C=number of 
commas, #PM=other punctuation marks. 
IV. EXPERIMENTAL PROCESS 
Machine learning is used in order to train the system to 
be able to make ‚Äúsmart‚Äù decisions regarding the anchor 
candidate phrase to which the PP phrase is to be attached. 
The produced dataset consisted of 8500 vectors 
corresponding to 500 corpus sentences. Consequently, the 
type of machine learning we used is supervised machine 
learning and, having already observed the right results-
outputs which have been given by the experts, the system 
will be able to predict results automatically. 
Of the 8500 vectors, only 7.9% of them indicated a 
correct attachment (positive examples) and the remaining 
82.1% indicated an erroneous attachment (negative 
examples), so we had an imbalanced dataset and biased 
results (recall and precision were much higher for the 
negative class than for the positive class). To balance the 
dataset random undersampling [13] was performed, i.e. the 
random removal of negative instances in order for the 
remaining to reach the number of positive instances. The 
final number of instances was 1350. 
We used the following algorithms: IBk (an 
implementation of the K-NN classifier), J48 (an 
implementation of the C4.5 decision tree classifier [14]) and 
Na√Øve Bayes [15]. IBk is a lazy supervised learning 
algorithm where a new unseen instance vector is classified, 
according to the class label majority of the k-nearest training 
vectors (k is user-defined). The classification uses majority 
voting to classify the unseen instance. J48 is a decision tree 
supervised learning algorithm. It first needs to create a 
decision tree based on attribute values of the available 
training data. Whenever it encounters a set of items (training 
instances) it identifies the attribute that discriminates them 
most clearly according to their class label. We used the 
pruned version which means that, once the tree is created, 
80
C4.5 attempts to remove branches that do not help 
classification by replacing them with leaf nodes. Na√Øve 
Bayes is a supervised learning algorithm based on the 
assumption of conditional independence [15], i.e. each 
attribute is independent from the others, given the class label. 
 
0,6
0,62
0,64
0,66
0,68
0,7
0,72
0,74
0,76
0,78
0,8
0,82
0,84
0,86
k=1 k=3 k=5 k=7
Recall (TRUE)
Precision (TRUE)
Recall (FALSE)
Precision (FALSE)
IBK
Number of neighbors  
Figure 3. Precision & Recall for both class labels as a function of the 
number of nearest neighbors (value of k) using IBk for 1350 vectors. 
In order to validate our results we used 10-fold cross- 
validation. Through this method, the original sample is 
randomly partitioned into ten subsamples. One out of ten 
subsamples is retained as validation data for testing the 
model, and the remaining nine subsamples are used as 
training data. The cross-validation process is then repeated 
ten times, with each of the ten subsamples being used only 
once as validation data. 
0,6
0,62
0,64
0,66
0,68
0,7
0,72
0,74
0,76
0,78
0,8
0,82
0,84
0,86
350 680 1000 1350
Recall (TRUE)
Precision (TRUE)
Recall (FALSE)
Precision (FALSE)
IB5
Size of Dataset
Figure 4. Precision & Recall for both class labels as a function of the 
dataset size using IB5.
The results of the conducted experiments, i.e. precision 
and recall for both class labels, are shown in Fig. 3 to 6. IBk 
was run for various values of k (Fig. 3). The results with 
k=5, being the highest, are then depicted for various data set 
sizes in Fig. 4. Fig. 5 and Fig. 6 show the results with J48 
and Na√Øve Bayes respectively. 
V. EVALUATION 
Using the J48 algorithm one may notice that the most 
significant feature is the ‚ÄúCandidate and PPs word distance‚Äù. 
It is the feature that is placed at the root of the tree 
constructed by the C4.5 classifier. In other words, it is the 
first feature to be checked in order to classify a new unseen 
attachment example. The constructed tree is represented in 
Fig. 7. Table 4 explains the integers on the decision tree 
nodes in Fig.7. 
0,6
0,62
0,64
0,66
0,68
0,7
0,72
0,74
0,76
0,78
0,8
0,82
0,84
0,86
0,88
350 680 1000 1350
Recall (TRUE)
Precision (TRUE)
Recall (FALSE)
Precision (FALSE)
J48
Size of Dataset
  
 
Figure 5. Precision & Recall for both class labels as a function of the 
dataset size using J48. 
0,440,46
0,480,5
0,520,54
0,560,58
0,60,62
0,640,66
0,680,7
0,720,74
0,760,78
0,80,82
0,840,86
0,880,9
0,92
350 680 1000 1350
Recall (TRUE)
Precision (TRUE)
Recall (FALSE)
Precision (FALSE)
NaiveBayes
Size of Dataset
Figure 6. Precision & Recall for both class labels as a function of the 
dataset size using Na√Øve Bayes. 
It is clear from the tree structure that the features 
‚ÄúPreposition Lemma‚Äù, ‚ÄúPos-Tag Candidates last token‚Äù and 
‚ÄúPPs between candidate and PP‚Äù are leaves of the tree 
constructed by C4.5 classifier. This means that these features 
are not so significant. From the representation of the tree 
(Fig. 7) it is evident that when the feature ‚Äúcandidates and 
PPs word distance‚Äù has a value greater than 3, the example is 
usually classified as false. Therefore, many examples that 
should be classified as true, are classified as false, because 
their value in this feature is great. From the results produced 
by the J48 algorithm in Fig. 5 one may notice that as the size 
of the dataset increases, recall and precision of both class 
labels start to converge with each other. Using the IBk 
algorithm we can see in Fig. 3 that as the number of the 
nearest neighbors increases until getting the value 5 (IB5), 
recall and precision of both class labels rise. However, at this 
point, when the number of nearest neighbors takes values 
greater than 5, the performance of the learner is affected due 
to noise introduced by the 'distant' closest neighbors. Also, 
81
from the results that were produced by the IBk algorithm 
with 5 nearest neighbors (Fig. 4), recall and precision of both 
class labels rise as the size of the dataset increases. 
Due to the conditional independence assumption of 
Na√Øve Bayes, the results produced by this algorithm are poor 
compared to other algorithms. From Fig. 6, it is evident that, 
as the size of the dataset increases, recall and precision of 
both class labels decrease. Recall of class label TRUE and 
precision of class label FALSE increase slightly when the 
size of the dataset consists of 1350 vectors. 
The best performance (comparing all learning algorithms 
and dataset sizes) was achieved with J48 with the 1350 
dataset (recall and precision of approximately 80%). 
Compared to the previous work results in Table 2, the results 
are comparable to previous approaches that utilize more 
sophisticated external resources. 
Figure 7. This figure depicts the tree constructed by the C4.5 classifier. T stands for class label TRUE and F for FALSE.
 
VI. CONCLUSION 
Supervised machine learning was used in order to solve 
the PP attachment problem in Modern Greek. The proposed 
policy was a low resources solution employing basic 
syntactic pre-processing, and no use of sophisticated parsing 
tools or semantic thesauri was made. Also, no restrictions 
were placed upon the prepositions that were addressed, i.e. 
all prepositions were taken into account. 
Future research could take into account other anchor 
candidate phrase types. Furthermore, the above experiments 
could be repeated with different learning algorithms and/or 
a different set of features which could lead to even better 
results. The low resource policy allows for the easy 
portability of the presented methodology to other languages 
that lack in resources, another interesting research directive 
that could be explored in the future. 
REFERENCES 
 
[1] O. Van Herwijnen, J. Terken, A. van den Bosch and Erwin Marsi, 
‚ÄúLearning PP attachment for filtering prosodic phrasing,‚Äù in 
Proceedings of the European Chapter of the Association for 
Computational Linguistics, Budapest, Hungary,  2003. Available: 
http://ilk.uvt.nl/downloads/pub/papers/paper_EACL03.pdf. 
[2] J. Zavrel, W. Daelemans and J. Veenstra, ‚ÄúResolving PP attachment 
ambiguities with memory-based learning,‚Äù in Proceedings of the 
Conference on Computational Natural Language Learning, pp. 136-
144, 1997.  
[3] S. Zhao and D. Lin, ‚ÄúA nearest-neighbor method for resolving PP-
attachment ambiguity,‚Äù in 1st International Joint Conference on 
Natural Language Processing, 2004. 
[4] N. Gala and M. Lafourcade. ‚ÄúCombining corpus based pattern 
distributions with lexical signatures for PP attachment ambiguity 
resolution,‚Äù in Proceedings of the 6th Symposium on Natural 
Language Processing, Chiang Rai, Thailand, 2005. 
[5] M. Collins and J. Brooks, ‚ÄúPrepositional phrase attachment through a 
backed-off model,‚Äù in Proceedings of the Third Workshop on Very 
Large Corpora, pp 27-38, 1995, Cambridge. 
[6] A. Ratnaparkhi, J. Reyna.r and S. Roukos, ‚ÄúA maximum entropy 
model for prepositional phrase attachment,‚Äù in Proceedings of the 
ARPA Workshop on Human Language Technology, pp. 250-255, 
Plainsboro, N J, March 1994. 
[7] B. Mitchell and R. Gaizauskas, ‚ÄúA comparison of machine learning 
algorithms for prepositional phrase attachment,‚Äù in 3rd International 
Conference on Language Resources and Evaluation, pp 2082-2087, 
Las Palmas, Canary Islands, Spain, 2002. 
[8] D. Holton, P. Mackridge and I. Philippaki-Warburton, Greek: a 
comprehensive grammar of the modern language, Routledge, London, 
1997. 
[9] N. Hatzigeorgiu, M. Gavrilido, S. Piperidis, G. Carayannis, A. 
Papakostopoulou, A. Spiliotopoulou, A. Vacalopoulou, P. 
Labropoulou, E. Mantzari, H. Papageorgiou and I. Demiros, ‚ÄúDesign 
and implementation of the online ILSP Greek corpus,‚Äù in 2nd 
International Conference on Language Resources and Evaluation, pp. 
1737-1742. Athens, Greece (2000). Available: 
http://www.elda.fr/catalogue/en/text/W0022.html. 
[10] E. Stamatatos, N. Fakotakis and G. Kokkinakis, ‚ÄúA practical chunker 
for unrestricted text,‚Äù in Conference on Natural Language Processing, 
pp. 139-150. Patras, Greece, 2000, submitted for publication. 
[11] V. Van Asch and W. Daelemans, ‚ÄúPrepositional phrase attachment in 
shallow parsing,‚Äù in 7th International Conference on Recent 
Advances in Natural Language Processing, Borovets, Bulgaria, 2009. 
[12] http://www.cs.waikato.ac.nz/ml/weka/ . 
[13] N. V. Chawla, ‚ÄúData mining for imbalanced datasets: an 
overview(Periodical style)‚Äù, Dept. of Computer Science and 
Engineering, Notre Dame Univ., U.S, 2005. 
[14] J. R. Quinlan, ‚ÄúBagging, boosting and C4.5,‚Äù in 13th National 
Conference on Artificial Intelligence, Portland, Oregon, 1996. 
[15] S. Russell and P. Norvig, Artificial Intelligence: A Modern Approach, 
3rd edition, Prentice Hall, 2009. 
 
82
