Digital Philology 1.1 (Spring): 42–72 © 2012 Johns Hopkins University Press  42
Mike Kestemont
University of Antwerp
Stylometry for Medieval 
Authorship Studies
An Application to Rhyme Words1
4In the digital humanities much research has been done concerning stylometry, the computational study of style. Literary authorship at-
tribution, especially, has been a central topic. After a brief introduction, I 
will discuss the enormous potential of this paradigm for medieval philology, 
a field that studies so many texts of unknown or disputed origin. At the 
same time, it will be stressed that stylometry’s application to medieval texts 
is currently not without problems: many attribution techniques are still con-
troversial and do not account for the specific nature of medieval text pro-
duction. Throughout this paper, I will tentatively apply two well-established 
attribution techniques (principal components analysis and Burrows’s Delta) 
to a number of case studies in Middle Dutch studies. These analyses shall 
be restricted to rhyme words, since these words are less likely to have been 
altered by scribes.
1. The quest for the human stylome
It has become a platitude to note that given the ever increasing corpus of 
electronic data available, philologists can now engage with texts on an 
unprecedented scale. It is indeed hard to overestimate the impact of the 
information revolution upon academia, especially in recent decades. The 
humanities’ increased interest in information technologies has been es-
pecially reflected in the emergence of the digital humanities. This paper 
will focus on one particular line of research which is currently being 
pursued within this paradigm: stylometry. Stylometry generally refers to 
the computational study of writing style, often by means of advanced 
quantification and statistics (Holmes). Although stylometry has had 
various applications in the past, authorship attribution (AA) has undeni-
ably attracted most of the attention.
At the heart of present-day studies in computational AA lies the 
hypothesis “that by measuring some textual features we can distinguish 
Kestemont 4 Stylometry and Rhyme Words 43
between texts written by different authors” (Stamatatos 538). This 
would imply that each author has so personal a writing style that it can 
function as stylistic DNA or a “stylome” (Van Halteren et al.), being 
stable throughout an author’s oeuvre and readily distinguishable from 
every other author’s writing style. In its purest formulation, the “indi-
vidualistic conviction” assumes that this is true for every human author, 
regardless of time, place, language or genre (Love). Needless to say, 
however fascinating these views are from a theoretical perspective, they 
largely remain to be proven in practice. Nevertheless, there have been 
numerous empirical studies in which several techniques were reported 
to perform well in AA (Stamatatos; Juola, “Authorship”; Koppel, Schler 
and Argamon). Although many of these studies have been carried out 
during controlled experiments using trustworthy test procedures, only 
a wide-scale application to other case studies will show how well these 
techniques apply to other authors and texts, written in different lan-
guages and time periods (Rudman).
AA is relevant in various applications, ranging from plagiarism de-
tection (Stein, Lipka and Prettenhofer) to forensics, for example iden-
tifying the author of an explosive letter (Juola, “Authorship” 307ff). 
Nevertheless, one of the most popular employments of AA is in histori-
cal literary scholarship (Holmes). Our literary heritage is full of cases 
of pseudonymously or anonymously published texts or works whose 
authorship is heavily debated (such as Shakespeare vs. Marlowe). In ver-
nacular medieval studies, however, computational AA has not revealed 
many applications to date. Rare examples include García and Martín 
for Old English, Van Dalen-Oskam and Van Zundert for Middle Dutch 
or Dimpel for Middle German. Medieval literature naturally offers 
many case studies of unresolved authorship. First of all, medieval au-
thors were themselves not always keen on revealing their identity. Both 
the humilitas ideal in religious literature and the traditionally-formulaic 
nature of many secular epics are but two examples of factors which im-
pede our understanding of medieval authorship. Far more problematic 
is the poor survival of the manuscript witnesses—let alone autographs—
of medieval texts. Manuscripts have often survived in a state of severe 
damage, lacking an informative prologue or epilogue identifying a text’s 
author. Moreover, this kind of meta-information was easily excluded by 
later scribes, who were no longer interested in a text’s origin. Techniques 
to automatically help determine the authorship of medieval texts would 
be most welcome. I intend to explore this issue further below.
The structure of this paper is as follows. In section 2, I will offer a 
concise introduction to the prevailing methodology in computational 
44 Digital Philology: A Journal of Medieval Cultures
AA for modern texts. This section will remain somewhat abstract but 
the introduction of some terminology related to text classification is 
needed in order to appreciate the rest of this paper. In section 3 I will 
discuss how such AA could be operationalized for use in medieval phi-
lology. Some issues linked to the peculiar nature of medieval text pro-
duction pose interesting problems, which is why my analysis will be 
restricted to rhyme words (section 4). I will apply two well-established 
attribution techniques to a test case in Middle Dutch studies (sections 
5 and 6). In section 7, I will discuss how these methodologies could 
contribute to a real-world example of unresolved authorship in Middle 
Dutch literature, with a critical emphasis on the shortcomings of cur-
rent methods. This contribution hopes to offer a critical discussion of 
stylometry’s potential for medieval philology, as well as its limitations 
in practice.
2. Text classification in terms of authorship
AA is often (implicitly) considered to be a text classification issue: a pre-
viously unlabeled piece of text automatically needs to be assigned a label 
that indicates its class, in this case, the author (Sebastiani). While not all 
scholars have presented their attribution work within this framework, 
most studies can easily be re-expressed using the terminology of such an 
approach (Luyckx and Daelemans 35–37; Argamon 132). Text classifi-
cation is an important subfield of information science, using techniques 
from computational linguistics and artificial intelligence. Well-known 
applications include spam filtering for email messages (classes “spam” 
and “no spam”) or topic classification for newspaper articles (classes 
“sports,” “finance,” “lifestyle,” etc.). The generic goal in these studies is 
to automatically derive a function that correctly maps a sample of text 
to some sort of class label (Sebastiani 7).
For modern texts, a typical AA experiment could take the following 
form: we select a representative writing sample by a number of authors. 
The amount of text is preferably roughly equal per author and we want 
to ensure that the texts are as similar as possible, in topic, dialect or 
genre, for example. Should our texts display stylistic (dis)similarities, we 
want to ensure that these are maximally linked to authorship and not 
to other factors. Next, we sample each text into a set of consecutive, 
equal-sized parts. These samples need to be represented in a consistent 
fashion. In text classification, samples are often represented under a 
“term-frequency vector space model” (see Stein, Lipka and Prettenhoffer 
77). This means that our dataset is represented as a table, in which each 
Kestemont 4 Stylometry and Rhyme Words 45
row corresponds to one sample; the columns represent all words en-
countered in the data. Each cell in the table holds the relative frequency 
of one particular word (“term”) in one sample. The list of values in one 
row is commonly referred to as a feature “vector.” Together, these vec-
tors constitute a high-dimensional “space” in which each text occupies 
a position, hence the derivation of the “term-frequency vector space 
model.”
Note that the initial data table will contain many superfluous col-
umns, such as the column for a hapax legomenon (a word encountered 
only once). Obviously, removing such a word will hardly affect our ex-
periments. Removing less relevant columns is called “feature selection” 
and serves to decrease the complexity of the original space. This is ef-
ficient (smaller vectors are required to describe a sample) and efficacious 
(we can discard less useful features). When distinguishing between sports 
and finance news items, the function word “the” will obviously be less 
informative than the words “soccer” or “stocks.” Various algorithms 
have been successfully applied to feature selection for AA (Stamatatos 
544–545). Nevertheless, one technique stands out, because it is both 
conceptually simple and theoretically well-grounded.
This technique (introduced in the seminal work by Mosteller and 
Wallace) only considers the n words that are most frequent in the entire 
data set. The restriction to a fairly small n (typically n < 300) has several 
advantages (Binongo 11). The words which are typically most frequent 
in a corpus are “function words,” the small linguistic category of short 
words such as particles (also, anyway), determiners (the, that) or con-
junctions (and, or). These “stop words” occur frequently throughout 
corpora and offer a reliable base of comparison because they are fairly 
content-independent: their frequencies are hardly affected by a text’s 
topic or genre. Whether an author writes about ballet or wildlife, he or 
she will always need to use definite articles. Moreover, these features are 
rarely under an author’s conscious control, which makes them robust to 
imitation or forgery. Because they occur so often in texts, they are at-
tractive from a statistical perspective.
The next step is to implement the actual attribution technique. 
Consider a new piece of text that was written by one of the authors in 
our dataset. How could we find out which one? In terms of classifica-
tion, we would like to assign to this sample the correct author label. 
This classification is often implemented via machine learning, a part 
of artificial intelligence in which scholars computationally simulate the 
ability of human beings to learn, that is to optimize their behavior based 
on previous experiences (Daelemans and Van den Bosch). A possible il-
46 Digital Philology: A Journal of Medieval Cultures
lustration is a soccer-playing robot. We program the robot to shoot the 
ball at a goal a couple of times, each time from a different angle or with 
a different foot position. In some cases, the robot will score, in other 
cases it won’t. Based on this simulation, we have the robot learn under 
which circumstances it was best able to score. If this training has been 
successful, the robot will now optimize its performance in future games. 
In text classification, a similar “supervised” strategy is adopted. During 
training, we first confront an algorithm with labeled examples, for in-
stance, news items that were previously labeled by a human expert as 
“sports” or “lifestyle.” Based on this training, the learner can optimize 
its knowledge so that in the future it will be able to independently clas-
sify similar news items into the correct category.
In AA, one simple intuitive learning technique is particularly 
popular: nearest neighbor learning (Argamon 132, 144; Luyckx and 
Daelemans; Daelemans and Van den Bosch). During training, this learn-
ing algorithm will simply memorize the examples. When confronted 
with a new unlabelled test sample, the learner will scan its memory and 
calculate a distance between the test item and each training item. The 
algorithm will then assign a “nearest neighbor” to the test item: the 
training instance at the smallest distance from the test item. Finally, the 
classification takes place through extrapolating the class label from the 
nearest neighbor to the test item. Naturally, the key to a successful clas-
sification is calculating a good distance between samples. If two news 
items belong to the sports category, the distance between them is pref-
erably short, whereas the comparison of items from different sections 
should result in a larger distance. Below, I will discuss in greater detail a 
concrete implementation of such a distance function for AA.
Suppose that the learner classified the unlabelled test item. At this 
point, we have no way of assuring the quality of the classification. In 
AA, our “anonymous” sample will have an author label but we have 
no idea of the accuracy of our methodology. To assess the performance 
of a learning system, scholars have come up with an evaluation scheme 
called “leave-one-out validation” (Daelemans and Van den Bosch 47–
48). Consider 100 writing samples by two authors (50 samples of each 
author). We set up an experiment in which each one of the 100 samples 
is left out, iteratively, from the data set and put aside as a test instance. 
During each iteration, we train our algorithm on the remaining 99 sam-
ples and have the trained classifier attribute the test instance to one of 
our two authors. Since we know the correct author of the test sample 
but the classifier did not, we can now check the accuracy of the classi-
fier. After 100 tests we have a pragmatic approximation of how well our 
Kestemont 4 Stylometry and Rhyme Words 47
learning technique would work for other samples, even if we might not 
know the correct author for some. If the learner made correct attribu-
tions in 96% of cases, we would have a fairly reliable learner. Scholars 
often consider a “baseline,” or the accuracy of a naive attribution based 
on chance. If a classifier would always predict “author A” in our ex-
ample, it would still have an accuracy of 50%. A good classifier should 
naturally outperform the baseline.
3. Authorship attribution for medieval texts
The generic methodology for AA presented above has been widely ap-
plied to post-medieval texts, such as novels, plays or newspaper articles. 
High accuracy figures (> 95%) have been reported. Would it be possible 
to apply this methodology to medieval texts with comparable results? 
The first condition for such a study is, of course, the availability of texts 
in a machine-readable format. Luckily, the number of electronic text 
editions has been steadily growing in recent years, not in the least within 
the Text Encoding Initiative, a framework in which many medievalist 
editors have been active. Secondly, in order to establish the authority of 
a given methodology, we would first like to try it out on a “gold stan-
dard” data set: a test set of works whose authorship is established in the 
scholarly literature—or is at least not subject to serious doubt. Never-
theless, even if we have constructed such a data set, there is one major 
obstacle: scribal variance.
Before the introduction of printing technologies in Europe, texts 
were manually copied by scribes (Kestemont, Daelemans and De Pauw 
287–288). Languages were not yet standardized and a spelling standard 
was lacking in most areas. Medieval spelling was therefore very phono-
logical, reflecting a scribe’s personal orthographical predilections or dia-
lect. Even within a single scribe’s work, we find various spellings for the 
same word token. Naturally, we would like to uniformize such words 
before the computational analysis. It is also common in modern texts to 
have a linguistic analysis precede the classification of texts (Stamatatos 
539ff). In English, it is not unusual to tokenize and lemmatize a text. 
A contraction such as don’t could be restored to do not. Likewise, the 
words walked and roses could be replaced by their respective dictionary 
lemmas: to walk and rose. Such linguistic analyses are often automated 
by tools from computational linguistics, which for a steadily increasing 
number of medieval vernaculars are now available (e.g. Souvay and 
Pierrel; see Kestemont, Daelemans and De Pauw for further references).
Nevertheless, any student in medieval philology will acknowledge 
that Cerquiglini’s variance (111) in manuscripts extends beyond spelling 
48 Digital Philology: A Journal of Medieval Cultures
differences or genuine copying errors. Once a copy of the authorial ex-
emplar came into circulation, there were no guarantees that subsequent 
scribes would leave the authorial text intact. Parallel manuscripts clearly 
show that scribes enjoyed relatively extensive freedom in adapting texts 
according to their own wishes. Material philologists have stressed that 
with each copy the opportunity arose for scribes to create a relatively 
new text (Nichols). But if an author’s style risked further fading with 
each subsequent copy, then how much remains of the original author’s 
stylome if so many scribes could have had a hand in the surviving copies?
Scribal interference with authorial style is a troublesome issue, and 
is currently difficult to assess. Current studies show that the impact 
of scribes should not be underestimated (Van Dalen-Oskam and Van 
Zundert). In some case studies, the high-frequency words in texts ap-
peared particularly sensitive to scribal adaptations (Kestemont and Van 
Dalen-Oskam). Small inconspicuous words were apparently easily in-
serted or deleted while copying texts. This seems worrying for medieval 
AA, since it is precisely such high-frequency features which are used for 
AA in modern texts. Although the issue is insufficiently investigated, it 
seems somewhat reckless to apply the same stylometric methods to high-
frequency items in medieval texts as is done for modern texts. For texts 
written by distinct medieval authors, but copied by the same scribe, 
this could result in artificially close intertextual distances (or the other 
way round for texts written by the same author but copied by different 
scribes). The analyses in this paper will therefore be restricted to words 
that seem to have been less sensitive to scribal adaptation: rhyme words.
One characteristic feature of much medieval literature is its aural 
dimension: many texts were primarily intended for oral recitation—
especially vernacular texts, often targeted at less literate audiences. 
Therefore, we find many rhymed texts, since rhyme could be agree-
able in oral reception as well as offer a useful aid in memorization. In 
the first centuries of the second millennium, this stylistic device had 
been introduced from Latin poetry to the vernacular (Gasparov 117). By 
the twelfth century, end rhyme had become a popular way to structure 
vernacular epics of larger size, for instance, in the canonized works of 
Chrétien de Troyes. Although rhyme had to compete rapidly with prose 
forms, it remained popular in many European literatures throughout the 
medieval period. In Middle Dutch literature, pair-wise end rhyme has 
long been the dominant form for narrative genres (Lie).
In the international literature, it has been stressed that one inter-
esting property of rhyme words is their stability in text transmission 
(Benskin and Laing). Since the chain of rhyme words provided the base 
Kestemont 4 Stylometry and Rhyme Words 49
skeleton texts were hung up on, it must indeed have been quite cumber-
some to try to change a text’s rhyme words, which would likely require 
the recomposition of large portions of the text (Besamusca 21). Even if 
scribes were able to easily adapt words within verse lines, parallel manu-
scripts demonstrate that scribes generally left the underlying lexeme of 
rhyme words intact, even if they changed their spelling. If we were to 
lemmatize the rhyme words of a medieval text—to abstract away from 
spelling variations—these rhyme words could be an interesting category 
for AA in medieval texts. I shall discuss this idea further in the rest of 
this paper.
At the outset, it should be noted that the stability of rhyme words in 
medieval text transmission is a provisional assumption, not an absolute 
given. Passages that were deleted by scribes obviously pose a smaller 
problem than do cases in which scribes interpolated new passages in 
texts, something which is often difficult to assess in the absence of paral-
lel witnesses. The impact of scribal interpolations should therefore not 
be underestimated. In the rest of this paper I will nevertheless assume 
that such interpolations generally do not quantitatively outweigh the 
original author’s style and can be discarded as less relevant “noise” 
through the use of robust attribution algorithms. Moreover, it deserves 
emphasis that the restriction to rhyme words limits the scope of this 
paper. Obviously, this methodology is only language-independent inso-
far as it can be applied to other rhymed texts. Moreover, the restriction 
to rhyme words in texts waste many other potentially informative re-
sources, namely all the other words. However, given the methodological 
reasons discussed above, I find it safer to only use rhyme words in this 
exploratory paper. It should be clear that the stylometric use of rhyme 
words in medieval texts is presented here as a useful but temporary 
bypass for the problem of medieval scribal interference, rather than a 
permanent solution.
4. A pilot study
As emphasized, any stylometric investigation should depart from a reli-
able test case. It only makes sense to apply a technique to an anonymous 
medieval text, if it has been proven to work for texts whose authorship 
is certain. I will now introduce a case study that approximates such a 
test case. In the last decades of the thirteenth century, Jacob van Maer-
lant started to translate the Speculum historiale by Vincent of Beauvais 
into Middle Dutch rhymes.2 Maerlant must have been aware of the 
magnitude of the undertaking the translation of such an enormous text, 
50 Digital Philology: A Journal of Medieval Cultures
covering the history of the Universe from Genesis to the Crusades. Fol-
lowing his source text, Maerlant divided the text into four major partien 
‘parts.’ Around Damme, a vibrant suburb of medieval Bruges, Maerlant 
sought the assistance of a younger apprentice, Filip Utenbroeke. Mae-
rlant “outsourced” the second part of the Speculum, while he himself 
covered the first and third part. Maerlant and Utenbroeke must have 
worked closely together and researchers have stressed the stylistic af-
finities between their work. Maerlant started the fourth part but died 
before he was able to finish it. The translation was picked up some years 
later (ca. 1316) and finished by Lodewijk van Velthem, a priest who was 
a great admirer of Maerlant. He also added a fifth part to the Spiegel 
historiael, covering the most recent history (without a Latin source text).
Another author included in the pilot experiment is Jan van Boendale, 
an early fourteenth-century poet and city administrator in Antwerp 
(Claassens). His authorship can be ascertained for at least two literary 
works. In one smaller didactic work, Jans Teesteye (see appendix, hence-
forth JT), Boendale describes an eloquent dialogue between his alter-ego 
and a conversation partner, named Wouter, about the vices of medieval 
society. In the prologue he dedicates the text to Rogier van Leefdaal, 
one of the Duke of Brabant’s most influential counselors at that time. 
He mentions that his patron has reprimanded him for some strongly 
misogynistic remarks in earlier work: “Want hi mi sulc stont heeft der 
af / Berespt van dien dat ik hem gaf” ‘Because he [Rogier van Leefdaal] 
has reprimanded me back then because of what I gave him’ (JT, lines 
100–101). This earlier work is traditionally identified as Der leken spie-
ghel (see appendix, henceforth LS). The author of this extensive rhymed 
moralistic text also introduces himself as Jan u arme clerk ‘John, your 
poor clerk’ (LS, line 24) and dedicates it to Rogier.
The combination of these four authors appears to be a good pilot 
case, because it should be difficult to differentiate between them sty-
listically. Maerlant, Utenbroeke and Velthem worked on the same co-
herent text and Maerlant is said to have had a large influence on his 
followers, especially Utenbroeke, with whom he worked so closely to-
gether. Boendale is also said to have been a great admirer of Maerlant’s 
oeuvre. He read and reworked many of his works, calling Maerlant 
the founding father of all poets writing in Dutch. Boendale moreover 
originated from the same area in which Lodewijk van Velthem worked, 
and scholars have argued that Boendale must have read Velthem’s work 
carefully. Boendale should not be easily distinguishable from his pre-
decessors either. Note, however, that JT and LS are relatively less his-
toric in content than the Spiegel-samples, which, furthermore, belong to 
Kestemont 4 Stylometry and Rhyme Words 51
the same text. This compromises our test case to some extent, since it 
violates the above-mentioned requirement that one preferably needs to 
“ensure that the texts are as similar as possible, for instance, in topic, 
dialect or genre.” Theoretically speaking, only the experiments that ex-
clude Boendale below are therefore entirely valid, since they control 
the texts for genre. Nevertheless, Boendale’s works are generally close 
enough to Maerlant’s works to be interesting for inclusion in these pilot 
experiments.
Large samples were taken from the second part (Utenbroeke, P2: 
32,312 rhyme words), third part (Maerlant, P3: 31,080 rhyme words), 
and from the fourth and fifth parts (Velthem, P4_P5: 14,237 rhyme 
words) of the Spiegel historiael. These texts were complemented with 
the entire JT (4,101 rhyme words) and LS (21,669 rhyme words) by 
Boendale. The rhyme words in these texts were automatically lemma-
tized: each medieval token was tagged with its lemma. The Middle 
Dutch tokens paart, paert and peerd are all replaced by the lemma 
PAARD (‘horse’) in order to eliminate the scribal influence on the spell-
ing of these words. All further calculations were restricted to these lem-
mas, instead of the original tokens.
The use of rhyme words has already been analyzed from a method-
ological perspective, but as of now, there is no indication that they would 
work for AA. Recall that it is currently popular to use high-frequency 
words for AA, because these items are well-spread in texts and relatively 
content-independent. The standard procedure for selecting these words 
is simply to make a word list of all words in the corpus and count how 
often each word occurs. Next, we rank the words for their frequency 
and select the n most frequent ones, assuming that they are relatively 
content-independent. We could apply the same procedure to the rhyme 
words in our corpus, selecting only those rhyme words that are most 
frequent in our texts. Strictly speaking, however, this is not without 
problems. Suppose that our corpus contains one very large text in which 
one particular rhyme word is extremely frequent (but absent in all other 
texts). In the overall word ranking, this word might pop up within the 
set of most frequent items, without actually being well-spread.
To assess whether this issue might compromise our methodology, it 
is necessary to perform a test. We first sample the texts in our current 
corpus (P2, P3, P4_P5, LS, and JT) based on a single sampling param-
eter, namely s (sample size) or the amount of rhyme words required per 
sample. If we would sample JT (counting 4,101 rhyme words in total) 
with s = 500, the result would be 8 non-overlapping, consecutive sam-
ples of 500 rhyme words. The text’s final 101 rhyme words would not 
52 Digital Philology: A Journal of Medieval Cultures
be included so that all samples are of an equal size. After sampling all 
of our texts in this fashion, we can loop through our samples, construct 
a list of all words in these samples, and rank them according to their 
cumulative frequency. The assumption is that if we select the n highest 
items in this ranking, we are selecting a “crest” of rhyme words that are 
well-spread and relatively content-independent throughout the corpus.
One way to test this hypothesis is to go through our frequency-
ranked word list and collect for each rhyme word its relative frequency 
in the individual samples. For each rhyme word r, this yields an array 
of values, namely r’s relative frequency in each of the samples. Over this 
array we can calculate r’s mean frequency and the standard deviation, 
expressing how much dispersion there is with respect to r’s mean. If we 
now take the ratio of r’s mean frequency over the standard deviation, 
we obtain the coefficient of variation, or CV (Lewontin). A low CV indi-
cates a good spread, since there are little deviations in the array’s values 
(all of them are close to the mean). A high CV, on the other hand, would 
be indicative of large frequency fluctuations, indicating that a rhyme 
word is frequent in some samples only.
Below is a plot in which each little circle corresponds to one rhyme 
word (Fig. 1; for s = 2000). We have plotted these rhyme words’ rank in 
the overall frequency list against their frequencies’ CV in the individual 
samples. The plot clearly reveals a strong correlation between a word’s 
position in the overall ranking and its “dispersion.” The higher a word’s 
rank, the more stable its spread over the samples and the more suitable 
it is for use in AA. The dashed vertical line indicates which rhyme words 
would be included in the case of n = 300. The rhyme words on the left 
side of this line are generally well-spread throughout the samples.
A qualitative analysis of this “crest” of high-frequency rhyme words 
reveals that they—just as modern function words—are generally char-
acterized by very Spartan semantics, such as: “know this,” “now,” 
“never,” and “for sure.” This explains why they are well-spread in the 
corpus. From the point of view of poetics, this result corresponds with 
the traditional view that many epic poets were fishing from the same 
pool of rhyme words (Van Driel 37–39). The rhyme word combinations 
in a language are limited in number so that poets were more or less 
bound to recycle them. Some rhyme words even seem to have attained 
a formulaic status, being used as nifty mnemonics by several authors, 
once they had proved useful in a variety of contexts. Moreover, it is not 
inconceivable that authors would display individual predilections for 
specific rhyme words and perhaps gave away their identity in this pro-
cess of repetition. This analysis adds a justification of the use of rhyme 
words for AA in terms of poetics.
Kestemont 4 Stylometry and Rhyme Words 53
5. Principal components analysis
Let us turn to an actual technique from AA. Recall that after sampling, 
we are left with a sample table that still has many columns. Even after 
a feature selection with, for example, n = 300, our vector space still 
counts 300 dimensions. It is cumbersome to inspect, let alone visualize, 
our samples’ position in all these dimensions simultaneously. For data 
exploration, stylometrists therefore turn to techniques for dimension 
reduction from multivariate statistics, such as Principal Components 
Analysis, or PCA (Binongo). The idea is to reduce the original n vari-
ables into a far smaller number of p new (uncorrelated) variables. We 
attempt to summarize our data, while retaining as much of the original 
information as possible (Binongo; Binongo and Smith). The resulting p 
new variables are the Principal Components (PCs) and can be ranked 
for informativity (PC1, . . . , PCP) in terms of how much of the original 
variability they retain. In stylometry, it is often the case that only the 
Fig. 1: Result of an experiment on 50 samples (s = 2000) from JT, LS, P2, P3 and 
P4_P5. Each circle represents a rhyme word in the samples. Each word’s rank in the 
overall frequency-list has been plotted on the horizontal axis against its CV over the 
individual samples. The dashed vertical line intersects the horizontal axis at rank 
300. The corresponding linear model (diagonal line) reveals a strong correlation be-
tween rank and CV (F(1, 4313) = 49960, p < .0001, adjusted R2 = 0.92).
54 Digital Philology: A Journal of Medieval Cultures
two (sometimes three) most informative PCs are studied. All of our 
samples are assigned scores on each of the PCs, which can be used for 
visualization. These scores are interpreted as a sample’s coordinates in a 
lower-dimensional space.
The standard outcome of a PCA is thus a two-dimensional scat-
terplot of data points representing the original samples. The effect of 
the “summary” is often that samples fall apart into well-distinguished 
sample clouds. In authorship studies it has been noted that this cluster-
ing, when applied to high-frequency items, corresponds to the authorial 
structure in the data (Binongo and Smith). Samples written by the same 
author tend to cluster in the scatterplot, whereas samples by different 
authors tend to lie far apart. This is especially remarkable since PCA is 
an unsupervised procedure: it does not have access to information about 
the samples’ origin, apart from the word frequencies. The fact that this 
method is considered easy to implement and largely language-indepen-
dent adds to its reputation (Stamatatos 545).
The application of a PCA to our test data yields the scatterplots in 
Figure 2. This Figure contains four subpanels: one for each experiment 
in which we have compared three of the four authors with s = 3000, n 
= 300. Authorial samples from the various texts are represented as little 
symbols in these plots (see the legends). These are positioned according 
to their coordinates in the first two dimensions (or principal compo-
nents) that result from the PCA. In general, the first two PCs seem to 
succeed well in clustering samples by the same author in all four plots. 
Samples from different texts never “mingle” but instead form well-dis-
tinguished and dense clusters in different “quadrants” of the scatterplots 
in Figure 2. The clearest instance of this clustering can be found in 
Figure 2d: Maerlant’s P3 samples (triangles) form a tight cluster in the 
lower-left part of the plane, whereas Utenbroeke’s P2 sample (circles) 
form a fairly dense cloud in the upper-left corner. Velthem’s samples 
(plus signs) are, stylistically speaking, clearly different from the samples 
by his predecessors and cluster in the upper-right quadrant of the scat-
terplot. Perhaps more importantly, the scatterplots generally have ample 
margins between the authorial clusters, which are indicative of the large 
differences the procedure discloses between these authors’ styles. Note 
that the single JT sample (in panels 2a to 3c) is always very close to the 
LS samples. Should anyone still doubt that JT and LS were written by 
the same author, this outcome could help to counter this. Therefore, the 
PCA yields the expected result regarding these medieval texts.
Figure 2 demonstrates that a PCA’s main advantage is that it can be 
used for visualization, allowing the researcher to intuitively inspect the 
Fig. 2a: Boendale, Maerlant, Velthem
Fig. 2b: Boendale, Utenbroeke, Maerlant
Fig. 2: Four subpanels (2a to 2d) showing PCA scatterplots (first two components) of 
all four combinations of three authors in the test data set, in each experiment with s 
= 3000, n = 300. The PCA generally seems apt at clustering the samples written by 
the same author.
Fig. 2c: Boendale, Utenbroeke, Velthem
Fig. 2d: Maerlant, Utenbroeke, Velthem
Kestemont 4 Stylometry and Rhyme Words 57
overall structure in his data on the fly. Nevertheless, it also has numer-
ous drawbacks. Note that the interpretation of the scatterplot can be 
subjective. An attribution based on a PC-scatterplot often takes the form 
of a nearest neighbor-reasoning (Argamon 144): an anonymous text 
gets attributed to a sample cloud by a candidate author, because the test 
sample seems relatively close to this cloud. This is what one is inclined 
to do for JT in these figures. Exactly how “close” the test item should 
be, however, is often open to interpretation. Note, for instance, that 
panel 2b does not provide too ample a margin between samples from P2 
and Boendale’s work: is the difference still significant? Some sort of sta-
tistical testing could be applied to the samples’ coordinates (cf. Holmes 
and Smith 192; Juola, “Jack London”) in the scatterplot but this does 
not always happen. Moreover, such procedures often merely reproduce 
the outcome of a simple visual inspection of a scatterplot. Another prob-
lem is that only the first (two) PCs are often discussed, while in fact, 
all scatterplots that account for more than 5% of the original variation 
should be discussed. This seems mainly due to pragmatic reasons since 
it could be cumbersome to reproduce so many figures in the average 
scholarly publication.
Another issue is that scatterplots can be volatile: adding samples 
that are extremely different from the rest might absorb much of the 
variation in the first PCs. This might produce a radically different vi-
sual outcome than if these samples were not included. The other, less 
extreme distinctions between samples might, in that case, not show up 
in the scatterplot and might be pushed back to the lower PCs. Binongo 
and Smith have also argued against the use of PCA for comparing more 
than three authors, since two dimensions can only describe so much 
variation (464). Therefore, it would be unadvisable to perform a PCA 
of the four authors simultaneously. A final consideration is that PCA 
scatterplots only offer a “snapshot,” which can be very dependent on 
specific parameters. In our case, different values for n and r might yield 
radically different scatterplots with a less clear-cut authorial distinction. 
We worked with rhetorical parameters (s = 3000 and n = 300) and 
might be overestimating the efficiency of our analysis. We now turn to 
a rather different attribution technique that can remedy some of these 
shortcomings.
6. Burrows’s Delta
In 2002, John Burrows first published a new method for AA, called 
Delta. This technique has been very influential and many scholars in 
the humanities have tested and refined it since its introduction. Re-
58 Digital Philology: A Journal of Medieval Cultures
cently, Argamon proved that Burrows’s original implementation can be 
mathematically simplified. In what follows, I will stick to Argamon’s 
reformulation, showing that Delta is essentially a variant of the near-
est neighbor classification paradigm already mentioned. When presented 
with an anonymous writing sample, Burrows’s Delta will compare the 
test sample to a number of training samples by candidate authors in its 
memory. It will calculate the test sample’s distance (its Delta) to each 
training item and extrapolate the authorship from the test sample’s near-
est neighbor. The novelty of Burrows’s approach lies in his definition of 
the intuitive distance metric.
Burrows’s Delta also operates on the n most frequent words encoun-
tered in a corpus. Originally Burrows had set n at 30 but larger values 
up to 300 have been proposed in the literature (Stamatatos 540–541). 
Subsequently, each sample is represented as a feature vector containing 
the normalized frequencies of these n words in the sample. The distance 
between two samples X and Y is calculated as in Equation 1 (Argamon 
132). For each word i of the n high-frequency words, the absolute dif-
ference is calculated between i’s frequency in both samples (|xi-yi|). The 
result is weighted by taking the ratio of this difference over the standard 
deviation of word i’s frequency in all available samples (si). The final 
Delta (D) between X and Y is obtained through the simple summation of 
the n differences (see Formula 1).
The advantage of using Delta is that we can use it for experiments 
in leave-one-out validation. Recall the experiments with s = 3000 and n 
= 300. Instead of producing a scatterplot, we could perform a leave-one-
out attribution experiment using Delta. The accuracy in such an experi-
ment (the number of correct attributions divided by the total number 
of attributions) can be expressed as a single number, which is especially 
useful for comparing experiments with different parameters. Smaller 
values for s and n are likely to result in lower accuracy figures: the com-
parison of leave-one-out accuracies allows us to test this.
I have performed leave-one-out experiments for various parameters. 
For s, values from 250 to 4,000 (with a step size of 250) were iteratively 
combined with n-values from 10 to 300 (with a step size of 10). The 
comparison of the Delta-accuracies under each of these settings allows 
us to study the algorithm’s behavior under different conditions. The 
Kestemont 4 Stylometry and Rhyme Words 59
panels under Figure 3 visualize these results in a three-dimensional plot. 
The attribution accuracy (vertical axis) for each combination of an s 
and n value (horizontal axes) is plotted as a point in a three-dimensional 
surface. Figure 3 has three panels: in the first one, only Maerlant and 
Velthem were included. In the second and third one we respectively 
added Utenbroeke and Boendale to study the effect of the number of 
candidate authors included in an attribution experiment.
All three panels clearly show that the quality of our attribution is 
generally good and significantly above the chance level or baseline. The 
accuracy generally increases with higher values for n and s. This is no 
surprise since the algorithm can in both cases benefit from more infor-
mation (higher n) from more representative writing samples (higher s). 
Interestingly, the Delta-algorithm only becomes flawless with relatively 
higher sample sizes. From this we can already deduce that we should 
not overestimate Delta’s performance: we need a writing sample of a 
Fig. 3a: Maerlant, Velthem
Fig. 3: Three panels showing accuracy figures resulting from leave-one-out Delta-ex-
periments with different parameters. The experiments in the first panel only included 
Maerlant and Utenbroeke; Velthem was added in the second panel’s experiments; 
Boendale in the third panel. The plots show that the quality of the attribution (verti-
cal axis) benefits from higher values for n as well as s (horizontal axes). The difficulty 
of the task seems to increase with the number of candidate authors in these panels.
Fig. 3b: Maerlant, Velthem, Utenbroeke
Fig. 3c: Maerlant, Velthem, Utenbroeke, Boendale
Kestemont 4 Stylometry and Rhyme Words 61
realistic size to make reliable style-based inferences about the sample’s 
authorship. The comparison between the various subpanels from Figure 
4 suggests that with a larger number of authors included, it becomes in-
creasingly difficult to reach a flawless performance. These observations 
are in line with AA studies for modern texts, reporting similar effects for 
data size and the number of candidate authors in similar experiments 
(Luyckx and Daelemans). The Delta-results in this section again show 
that stylometric techniques can be applied to medieval texts, but that the 
same limitations to techniques hold as for modern texts, for example, 
regarding sample sizes. Samples of at least 2,000 words seem necessary 
for a trustworthy analysis.
7. A real-world case: the Antwerp School
Previous sections above demonstrated that stylometric methods developed 
for modern texts can also yield valid results for medieval texts. However, 
the results did little more than reproduce the traditional views on the 
authorship of texts in our test case. This is certainly interesting from a 
theoretical perspective, but the true challenge inevitably lies in the ap-
plication of such techniques to a real-world case of unresolved medieval 
authorship. One such generic case from Middle Dutch studies is that of 
the Antwerp School. As mentioned, the authorship of the Antwerp city 
clerk, Jan van Boendale (ca. 1280–1350), can be ascertained for JT and 
LS. Nevertheless, there were more works written in Antwerp in the period 
from 1316 to 1351, but these texts unfortunately do not identify their 
author. This corpus forms a stylistically coherent body of (ten or so) di-
dactic, ethical and historiographical texts, mostly written in rhyming cou-
plets. Many scholars have argued that Boendale must have written many 
more of these texts but to date, it remains unclear how many of these are 
actually his. This corpus is often denoted by the neutral designation “An-
twerp School” (Antwerpse School), allowing for the possibility that more 
than one poet could have been involved in writing these texts.
As regards three rhymed poems in particular, many scholars have 
argued in favor of Boendale’s authorship (see appendix).4 Midway April 
1342, an anonymous author in Antwerp, finished the translation of 
Melibeus (henceforth Mel) from the thirteenth century Latin Liber con-
solationis et consilii by Albertanus of Brescia. The affinities of this text 
with other texts from the Antwerp School in terms of dating, origin, style, 
and didactic content lead many scholars to assume that Boendale could 
have also been the author of Mel. The same is true for Dietsche doctri-
nale (Doc), another adaptation inspired by one of Albertanus’s tractates. 
The anonymous author mentions that the adaptation was completed in 
62 Digital Philology: A Journal of Medieval Cultures
1345 in Antwerp. Seven years after Doc, in 1351, another author from 
Antwerp completed the Boec vander Wraken (Wrak), which is an exten-
sive moralistic text dealing with mankind’s handling of the day of reckon-
ing. Wrak is also often included in Boendale’s bibliography, because the 
text bears important resemblances to other Antwerp texts.
In an influential and innovative article, J. Reynaert has argued that 
Mel and Doc were indeed written by Jan van Boendale. Reynaert ex-
tracted stylistic whimsicalities from these texts that particularly struck 
him as an expert reader. After listing these peculiarities, he turned to the 
Cd-rom Middelnederlands ‘CD-ROM Middle Dutch,’ then a recently 
published electronic disc containing searchable versions of a large por-
tion of the surviving corpus of Middle Dutch literature. Exploiting the 
disc’s search possibilities, Reynaert was able to collect an impressive list 
of stylistic features that Mel and Doc had in common with other texts 
attributed to Boendale but which were largely absent in other texts. 
Based on these observations Reynaert convincingly argued that both 
Mel and Doc were written by Boendale.
From the point of view of stylometry, one could assume the rhetori-
cal role of devil’s advocate and attempt to critique Reynaert’s perspec-
tive. It can be noted that such studies do not start with work that is 
certainly written by Boendale and assign equal weight to similarities 
between, for example, Mel and JT (certainly Boendale’s) and Mel and 
Wrak (maybe Boendale’s). Furthermore, the selection of stylistic traits 
in these studies remains somewhat subjective. Apart from freely mixing 
syntactic with lexical characteristics, we hardly find an objective justifi-
cation as to why these specific stylistic phenomena were analyzed whilst 
others were not. Most notably, scholars have assigned considerable 
weight to low-frequency phenomena, such as conspicuous word choices 
which are rare, even inside the School itself. Note that these words 
seem whimsicalities for human readers, precisely because of their low 
frequency. This is problematic in the context of a School of authors: if a 
group of poets have indeed tried to produce a stylistically coherent body 
of literature, they will have imitated each other’s style. Low-frequency 
items will have struck the medieval imitator as strongly as present-day 
researchers. It would therefore be interesting to explore whether stylo-
metric techniques could invalidate Reynaert’s research.
Figure 4 contains three panels with the scatterplots of PCAs in 
which we have contrasted Boendale’s work with that of the three other 
authors already discussed. In all three cases (each time s = 2000, n = 
300), the three texts of disputed origin were included (Mel, Doc and 
Wrak). Since it is not advisable to compare more than three authors 
Fig. 4a: Boendale, Maerlant + Antwerp School (Mel, 
Doc, and Wrak)
Fig. 4b: Boendale, Utenbroeke + Antwerp School (Mel, 
Doc, and Wrak)
Fig. 4: Four subpanels with the scatterplots resulting from a PCA (each time s = 
2000, n = 300). In the first three panels we combined Boendale’s JT and LS with the 
work from one control author: Maerlant (4a), Utenbroeke (4b) and Velthem (4c). 
The Antwerp moralistic works (Mel, Doc and Wrak) are included in each panel. In 
4d no control author was included.
Fig. 4c: Boendale, Velthem + Antwerp School (Mel, 
Doc, and Wrak)
Fig. 4d: Boendale + Antwerp School (Mel, Doc, and 
Wrak)
Kestemont 4 Stylometry and Rhyme Words 65
in a PCA, and we need to account for the possibility that an Antwerp 
author different from Boendale had written Mel, Doc, or Wrak, note 
that we restrict those PCAs to a single control author in the subpanels 
(Maerlant, Velthem or Utenbroeke).
One similarity shared between the first three subpanels in Figure 4 
is that the samples by the three control authors tend to cluster in a very 
tight sample cloud that generally seems restricted to one of the four 
“quadrants” in the scatterplots. In these scatterplots, the samples from 
the Antwerp School are restricted to the other three quadrants. Together 
with the samples from LS and JT, they are thrice separated from the 
control author by a relatively wide diagonal margin. This therefore con-
firms the traditional view that these Antwerp texts somehow form a sty-
listically coherent body of literature. In the first three panels, LS clearly 
forms the most central text: its samples invariably follow a diagonal 
trail in the scatterplots. Interestingly, this trail seems to attract the other 
samples from the Antwerp School, as well from JT. Both LS and Wrak 
thrice occupy a central position in LS’s trail. From the perspective of 
nearest neighbor reasoning, these scatterplots do not offer an argument 
to contradict the claim that Wrak was written by Boendale.
In all subpanels of Fig. 4 Mel and Doc occupy similar positions, 
but generally not as close to samples from LS or JT as Wrak. This 
seems especially true in Figure 4d where no external control author was 
used and Doc and Mel are isolated in their own quadrant. In 4a to 4c, 
however, following a nearest neighbor approach, they are still much 
closer to other Antwerp samples than to the control author’s samples. 
The problem here is that if a PCA is restricted to a single author’s oeu-
vre—as could be the case in 4d—it is known to occasionally pick up 
other factors than authorship, such as the chronology of works (Juola, 
“Becoming Jack”). This makes it difficult to assess the authorship of Mel 
and Doc. They do not seem all too different from the other Antwerp 
texts but stylometry does not offer a straightforward methodology for 
such a case of authorship verification (Stamatatos 554).5
One ultimate “attempt to reject” Boendale’s authorship for these 
texts could be a leave-one-out Delta-experiment in which we compare 
the samples from the Antwerp School to the writings of Boendale and 
the three control authors. In order to determine the accuracy of an attri-
bution in the case of Wrak, Doc, or Mel we could assume that an attri-
bution is correct if the sample’s nearest neighbor belongs to another text 
from the Antwerp School, JT or LS. This does not really help to verify 
the authorship of these texts, but it might provide an insight into the ef-
fect of the experimental parameters. When comparing the results in Fig. 
66 Digital Philology: A Journal of Medieval Cultures
5 to those in Fig. 3, we see that the Delta-procedure does not experience 
real difficulties in attributing the anonymous samples to Boendale in 
this set-up. The learning curves do not display any anomalies, suggest-
ing that Boendale did not write the Antwerp School texts. At the same 
time, it should be stressed that this does not offer any conclusive proof 
for his authorship either. Perhaps the most salient conclusion would be 
that stylometry does not seem able to convincingly invalidate state of 
the art techniques about the authorship of these texts. Indirectly—ex 
falso sequitur quodlibet—one could see in this an argument in favor of 
Boendale’s authorship.
Fig. 5: A leave-one-out Delta-experiment, similar to the one in Fig. 3 but with the 
inclusion of Wrak, Doc, and Mel. Note that s is restricted to 3,750 (instead of 4,000) 
because of Mel’s limited length (3,771 rhyme words).
Kestemont 4 Stylometry and Rhyme Words 67
8. Conclusion: new methodologies come with baggage
This exploratory paper offered an introduction to computational au-
thorship attribution as currently practiced in stylometry. The application 
of this paradigm in medieval philology, a field concerned with so many 
cases of unresolved authorship, is naturally fascinating. Nevertheless, 
a few obstacles, such as scribal stylistic interference, need to be taken 
into consideration and require further investigation. In this paper, two 
well-established techniques were applied to a test case in Middle Dutch 
studies with promising results. Nevertheless, one should not have unre-
alistic expectations: as with modern authors, a text’s size, for instance, 
should clearly be large enough (> 2,000 words) to infer anything sen-
sible about its author. Although these techniques are likely to yield in-
teresting results for longer, rhymed texts (also in other languages), the 
future development of other methodologies is desirable, especially for 
the late-medieval period, in which texts often become shorter and are 
less frequently rhymed. The application of these techniques to a real 
case of unresolved authorship in Middle Dutch studies showed that sty-
lometry might be an important scholarly aid, but will not always offer 
conclusive proof in and of itself. In our case study, stylometry could 
not invalidate the current state-of-the-art techniques—which is notewor-
thy—but it was not able to prove them either.
A
pp
en
di
x:
 A
n 
ov
er
vi
ew
 o
f 
th
e 
(r
hy
m
ed
, 
no
n-
hi
st
or
io
gr
ap
hi
ca
l)
 w
or
ks
 f
ro
m
 t
he
 A
nt
w
er
p 
Sc
ho
ol
 i
n 
th
is
 s
tu
dy
. 
T
he
 e
nt
ir
e 
co
rp
us
 
fo
r 
th
is
 s
tu
dy
 w
as
 d
ig
it
al
ly
 h
ar
ve
st
ed
 f
ro
m
 t
he
 C
d-
ro
m
 M
id
de
ln
ed
er
la
nd
s.
M
id
dl
e 
   
   
   
   
   
   
   
A
bb
re
vi
at
io
n 
   
   
E
ng
lis
h 
   
   
   
   
   
   
C
on
te
nt
   
   
   
   
   
   
   
   
   
   
  N
um
be
r 
of
   
   
   
   
   
   
 D
at
e 
of
  
D
ut
ch
  t
it
le
   
   
   
   
   
   
   
   
   
   
   
   
   
 t
ra
ns
la
ti
on
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
  l
in
es
 in
 c
or
pu
s 
   
   
   
   
co
m
po
si
ti
on
 
   
   
   
   
   
  
 
 
 
 
 
 
 
 
 
 
   
   
   
   
   
 (
ap
pr
ox
.)
Ja
ns
 T
ee
st
ey
e 
JT
 
Jo
hn
’s
 o
pi
ni
on
 
D
ia
lo
gu
e 
be
tw
ee
n 
 
 
 
th
e 
“a
ut
ho
r”
 a
nd
  
 
 
 
“W
ou
te
r”
 a
bo
ut
 t
he
  
 
 
 
fa
te
 o
f 
m
an
ki
nd
 
4,
10
1 
13
30
–1
33
4
L
ek
en
sp
ie
ge
l  
L
S 
L
ay
m
an
’s
 m
ir
ro
r 
E
th
ic
al
 
 
 
 
re
co
m
m
en
da
ti
on
s 
 
 
 
 
in
sp
ir
ed
 b
y 
hi
st
or
ic
al
  
 
 
 
ev
en
ts
 
21
,6
69
 
C
a.
 1
32
5–
13
30
M
el
ib
eu
s 
M
el
 
M
el
ib
eu
s 
A
lle
go
ri
ca
l l
ov
e 
st
or
y 
 
 
 
of
 M
el
ib
eu
s 
an
d 
 
 
 
 
Pr
ud
en
ti
a 
3,
77
1 
13
42
D
ie
ts
ch
e 
do
ct
ri
na
le
 
D
oc
 
T
he
 d
oc
tr
in
e 
in
  
D
id
ac
ti
c 
po
em
 a
bo
ut
 
 
M
id
dl
e 
D
ut
ch
 
m
or
al
 d
oc
tr
in
es
 
6,
66
7 
13
45
B
oe
c 
va
nd
er
 W
ra
ke
n 
W
ra
k 
T
he
 b
oo
k 
of
 R
ev
en
ge
 
H
is
to
ry
 t
el
lin
g 
fr
om
 
 
 
 
th
e 
m
or
al
is
ti
c 
pe
rs
pe
ct
iv
e 
 
 
 
 
of
 t
he
 L
as
t 
Ju
dg
m
en
t 
5,
86
9 
13
46
–1
35
1
Kestemont 4 Stylometry and Rhyme Words 69
Notes
1. The author is a predoctoral researcher (“aspirant”) with the Research 
Foundation of Flanders (FWO – Vlaanderen) and gratefully acknowledges the 
Foundation’s support. The author would like to thank Frank Willaert, Walter 
Daelemans and the anonymous reviewers of Digital Philology for their valuable 
comments on earlier versions of this article.
2. The background of the Spiegel historiael as well as the corpus extracted 
from it is further discussed (in English) by Kestemont. Due to a lack of space, 
I refer to this paper for further references. Note that a number of passages 
for which Velthem’s authorship cannot be ascertained have been removed from 
P4_P5. Velthem was often eager to include passages by other (historiographic) 
authors in his works, without proper reference. This practice seems reminiscent 
of my earlier remarks on scribal interpolations but it is of course different in 
the sense that the author here introduces “polluted sections” in the text himself. 
Such interpolations can be detected using techniques from the field of (so-called 
“intrinsic”) plagiarism detection (Stein, Lipka and Prettenhoffer).
3. Originally, Burrows’s Delta depended on an external reference corpus 
for these standard deviations. Nowadays the standard deviations are often cal-
culated on the basis of the data set that is used for the experiment (Holmes and 
Crofts 185).
4. The Antwerp School also holds a number of historiographical texts 
which are not further considered, mainly because of their complex interrelations 
and manuscript traditions. Furthermore, it is currently unclear how genre-effects 
interfere with authorial style (Stamatatos 553). The predominantly historio-
graphical texts from the School might seem (artificially) closer to the Spiegel-
samples than the non-historiographical texts, because they belong to the same 
genre.
5. One technique for this problem (“unmasking”) was originally proposed 
by Koppel and Schler. Unfortunately this technique was shown to be unreliable 
for texts shorter than 5,000 words (Sanderson and Guenter).
Works Cited
Argamon, Shlomo. “Interpreting Burrows’s Delta: Geometric and Probabilistic 
Foundations.” Literary and Linguistic Computing 23.2 (2008): 131–147. 
Print.
Benskin, Michael, and Margaret Laing. “Translations and Mischsprachen in 
Middle English Manuscripts.” So meny people longages and tonges. Philo-
logical Essays in Scots and Mediaeval English Presented to Angus McIn-
tosh. Ed. Michael Benskin and M. L. Samuels, Edinburgh: The Editors, 
1981. Print.
Besamusca, Bart. The Book of Lancelot: The Middle Dutch Lancelot Compila-
tion and the Medieval Tradition of Narrative Cycles. Cambridge: Brewer, 
2003. Print.
70 Digital Philology: A Journal of Medieval Cultures
Binongo, José Nilo G. “Who wrote the 15th Book of Oz? An application of 
multivariate analysis to authorship attribution.” Chance 16.2 (2003): 9–17. 
Print.
Binongo, José Nilo G, and W. M. A Smith. “The application of principal com-
ponents analysis to stylometry.” Literary and Linguistic Computing 14.4 
(1999): 445–466. Print.
Burrows, John. “ ‘Delta’: A Measure of Stylistic Difference and a Guide to 
Likely Authorship.” Literary and Linguistic Computing 17.3 (2002): 267–
287. Print.
Cd-rom Middelnederlands. Den Haag/Antwerp: Sdu, 1998. CD-ROM. 
Cerquiglini, Bernard. Eloge de la variante: Histoire critique de la philologie. 
Paris: Seuil, 1989. Print.
Claassens, Geert H. M. “Jan van Boendale (ca. 1280–1351).” Medieval Ger-
many: An Encyclopaedia. Ed. John M. Jeep. New York: Routledge, 2001. 
Print.
Daelemans, Walter, and Antal van den Bosch. Memory-Based Language Pro-
cessing. Cambridge: Cambridge UP, 2005. Print.
Dimpel, Friedrich Michael. “Der Verlust der ‘Eneas’-Handschrift als Fiktion. 
Eine computergestützte, textstatistische Untersuchung.” Amsterdamer Be-
iträge zur älteren Germanistik 61.1 (2006): 87–102. Print.
García, Antonio Martina, and Martín, Javier Calle. “Function Words in Author-
ship Attribution Studies.” Literary and Linguistic Computing 22.1 (2007): 
49–66. Print.
Gasparov, M. L. A History of European Versification. Trans. J. S. Smith and 
Marina Tarlinskaja. Oxford: Clarendon, 1996. Print.
Holmes, David I. “The Evolution of Stylometry in Humanities Scholarship.” 
Literary and Linguistic Computing 13.3 (1998): 111–117. Print.
Holmes, David I. and Daniel W. Crofts. “The diary of a public mad man: a case 
study in traditional and non-traditional authorship attribution.” Literary 
and Linguistic Computing 25.2 (2010): 179–197. Print.
Juola, Patrick. “Authorship Attribution.” Foundations and Trends in Informa-
tion Retrieval 1.3 (2006): 233–334. Print.
———. “Becoming Jack London.” Journal of Quantitative Linguistics 14.2 
(2007): 145–147. Print.
Kestemont, Mike, and Karina van Dalen-Oskam. “Predicting the Past: Memory-
Based Copyist and Author Discrimination in Medieval Epics.” Proceedings 
of the Twenty-First Benelux Conference on Artificial Intelligence. Ed. T. 
Calders, K. Tuyls, and M. Pechinizkyi, Eindhoven: BNVKI-AIABN, 2009. 
Print.
Kestemont, Mike, Walter Daelemans, and Guy de Pauw. “Weigh your words – 
Memory-Based Lemmatization for Middle Dutch.” Literary and Linguistic 
Computing 25.3 (2010): 287–301. Print.
Kestemont, Mike. “Velthem et al. A stylometric analysis of the rhyme words in 
the acconut of the Battle of the Golden Spurs in the fifth part of the Spiegel 
historiael.” Queeste 17.1 (2010): 1–34. Print.
Kestemont 4 Stylometry and Rhyme Words 71
Koppel, Mosche, Jonathan Schler, and Shlomo Argamon. “Computational 
methods in authorship attribution.” Journal of the American Society for 
Information Science and Technology 60.1 (2009): 9–26. Print.
Lewontin, R.C. “On the Measurement of Relative Variability.” Systematic Biol-
ogy 15.2 (1966): 141–142. Print.
Lie, Orlanda S.H. “What is truth? The Verse-Prose Debate in Medieval Dutch 
Literature.” Queeste 1.1 (1994): 34–65. Print.
Love, Harold. Attributing Authorship: An Introduction. Cambridge: Cambridge 
UP, 2002. Print.
Luyckx, Kim, and Walter Daelemans. “The effect of author set size and data size 
in authorship attribution.” Literary and Linguistic Computing 26.1 (2011): 
35–55. Print.
Mosteller, Frederick, and David L. Wallace. Inference and Disputed Authorship: 
The Federalist. Reading, MA: Addison-Wesley, 1964. Print.
Nichols, Stephen. “Why Material Philology? Some Thoughts.” Zeitschrift für 
deutsche Philologie 116 (1997): 10–30. Print.
Reynaert, Joris. “Boendale of ‘Antwerpse School’? Over het auteurschap van 
‘Melibeus’ en ‘Dietsche doctrinale’.” Al t’Antwerpen in die stad. Jan van 
Boendale en de literaire cultuur van zijn tijd. Ed. Wim van Anrooij et al. 
Amsterdam: Prometheus, 2002. Print.
Rudman, Joseph. “The State of Authorship Attribution Studies: Some Problems 
and Solutions.” Computers and the Humanities 31.4 (1998): 351–365. 
Print.
Sanderson, Conrad, and Simon Guenter. “Short Text Authorship Attribution 
Via Sequence Kernels, Markov Chains and Author Unmasking: An Inves-
tigation.” Proceedings of the 2006 Conference on Empirical Methods in 
Natural Language Processing. Ed. Dan Jurasky and Eric Gaussier. Strouds-
burg: ACL, 2006. Print.
Sebastiani, Fabrizio. “Machine Learning in Automated Text Categorization.” 
ACM Computing Surveys 34.1 (2002): 1–47. Print.
Souvay, Gilles, and Jean-Marie Pierrel. “LGeRM. Lemmatisation des Mots en 
Moyen Français.” Traitement Automatique des Langues 50.2 (2009): 149–
172. Print.
Stamatatos, Efstathios. “A Survey of Modern Authorship Attribution Meth-
ods.” Journal of the American Society for Information Science and Tech-
nology 60.3 (2009): 538–556. Print.
Stein, Benno, Nedim Lipka, and Peter Prettenhofer. “Intrinsic plagiarism analy-
sis.” Language Resources and Evaluation 45.1 (2011): 63–82. Print.
Text Encoding Initiative. The Text Encoding Initiative Consortium. Web. 24 
Jan. 2012. <http://www.tei-c.org>.
Van Dalen-Oskam, Karina, and Joris van Zundert. “Delta for Middle Dutch—
Author and Copyist Distinction in Walewein.” Literary and Linguistic 
Computing 22.3 (2007): 345–362. Print.
72 Digital Philology: A Journal of Medieval Cultures
Van Driel, Joost. Prikkeling der zinnen. De stilistische diversiteit van Middelned-
erlandse epische poëzie. Zutphen: Walburg, 2007. Print.
Van Halteren, Hans, Harald R. Baayen, Fiona J. Tweedie, Marco Haverkort, 
and Anneke Neijt. “New Machine Learning Methods Demonstrate the Ex-
istence of a Human Stylome.” Journal of Quantitative Linguistics 12.1 
(2005): 65–77. Print.
