ORIGINAL ARTICLE
Deception detection: dependable or defective?
Anna Vartapetiance • Lee Gillam
Received: 12 July 2013 / Revised: 14 November 2013 / Accepted: 16 December 2013 / Published online: 6 March 2014
 Springer-Verlag Wien 2014
Abstract How do human beings tell the difference
between truths and lies, and avoid being deceived? And is
it possible for a machine to determine the veracity of any
given statement or set of statements prior to incorporating
such statements in a knowledge base, or to determine
whether the deception even exists at the statement level?
This paper reviews past research in deception and its
detection to explore such questions. We focus in on various
inconsistencies, contradictions, and other difficulties in
recent deception research, and show how the nature of the
deception largely dictates the methods that can be deployed
effectively in detection by reference to several experiments
on materials which can have a strongly deceptive framing.
Keywords Deception detection  Authorship
identification  Authorship attribution  Authorship
verification  Intrinsic plagiarism detection  Sexual
predator detection  Social media
1 Introduction
‘‘Almost every time another social tragedy comes to
light now, it seems to have been internet-assisted;
from selling babies like commodities, to fascism, and
most disturbingly paedophilia. The news stories
invariable lead some people blame the internet as a
haven for vice, abuse and illegal activities… [And]
they may be right’’
(Left 2002)
Communication over the Internet, including email and
various social media, offers much autonomy and also
potential anonymity. The former relates to the demo-
cratisation of publication, although many are still to
realise fully the legal implications—in particular libel,
but also relating to defamation and related offences—of
this. The latter means that people can also choose to be
whoever, whatever and wherever they wish, presenting
themselves and re-inventing themselves howsoever they
see fit. There are likely many fictional identities like that
of Robin Sage,1 fake facebook accounts with many
friends, and fantastic CVs2 and few will necessarily have
the expertise, time and energy to discover and address
such deceptions and to inform others with regard to all
such deceptions. The extent to which such deceptions are
harmful often leads people to distinguish ‘‘white lies’’,
typically geared towards personal gain (self-deception) or
protection from harm, from ‘‘black lies’’ which are more
likely to be of a more harmful nature. This distinction is
both more limited than we would prefer, and acts to
conflate deceptions and lies, which we believe are
important to distinguish as we will go on to explain.
Suppose, however, that we apply this distinction. Can we
consider the misrepresentation of science and the mis-
representation as science as either ‘‘black’’ or ‘‘white’’?
Misrepresentation as science is prevalent in, for example,
This article is part of the Topical Collection on Uncovering Deception
in Social Media.
A. Vartapetiance (&)  L. Gillam
Department of Computing, Faculty of Engineering and Physical
Sciences, University of Surrey, Surrey, UK
e-mail: a.vartapetiance@surrey.ac.uk
L. Gillam
e-mail: l.gillam@surrey.ac.uk
1 Robin Sage (2013).
2 Vijayan (2010).
123
Soc. Netw. Anal. Min. (2014) 4:166
DOI 10.1007/s13278-014-0166-8
the advertising of cosmetic products. We may assume
that the products are crafted to make people feel good
about themselves, and so any lack of rigour involved
with claimed outcomes of such products could be con-
sidered merely as ‘‘white’’. It remains difficult to discern,
however, which of these is appropriate to the portrayal
by an apparently trusted media outlet of a reported
survey of 200 students’ responses to a question of
whether they thought they had hallucinated after drinking
coffee fits the former or the latter when characterised by
the BBC as ‘‘‘Visions link’ to coffee intake’’ (BBC
2009).
To construct a ‘machine’ that might act appropriately
when faced with the fictional identities, fake facebook
accounts, fantastic CVs, and so on, we must assume first
that it is possible, given time, to distinguish at least fake
from real, fact from fiction, and that both frauds and
fraudsters have machine-identifiable features. So, could we
rely on existing deception research in the production of
such a machine?
In this paper, we present a critical review of deception
research, exploring whether it is possible to answer such a
question. In Sect. 23, we first explain our preferred defi-
nition of deception, to disentangle deceptions from lies,
and then clarify the impact that selection of a specific
medium (text) has on the likely nature of deception. We
then review the features that researchers tend to focus on as
‘‘cues’’ that might be used for detecting deception, focus
these down to a set as may be detectable in text, and then
demonstrate that in treatments of such cues by leading
deception researchers that there are various inconsistencies,
contradictions, and other difficulties. We next explore the
role of readability for deception detection which helps
suggest yet more inconsistencies. This section concludes
with additional critiques on approaches taken in deception
research. Section 3 then discusses possible approaches that
could help in detecting other types of real-world deception.
This offers an overview of our approaches in the Interna-
tional Workshop on Uncovering Plagiarism, Authorship,
and Social Software Misuse in both 2012 and 2013 (which
we will refer to later as PAN 2012, 2013) in three tasks of
authorship identification, plagiarism and sexual predator
detection. We conclude with a brief discussion of our
findings to date on both detection systems and how dif-
ferent types of deceptions might require different types of
detections.
2 Defining deception
From various definitions for deception, (e.g. Masip et al.
2004; Hall and Pritchard 1996; Russow 1986), we settle on
Mahon (2007) for the purposes of this paper:
‘‘To intentionally cause another person to have or
continue to have a false belief that is truly believed to
be false by the person intentionally causing the false
belief by bringing about evidence on the basis of
which the other person has or continues to have that
false belief.’’
In contrast to others, this definition leads with intent.
Whilst unintended actions might lead to deception, we
should be careful to distinguish intended deceptions from
those merely reflecting on ill-formed opinions or other
badly held beliefs. We must also distinguish from those
who simply have differing opinions, providing that these
are suitably well grounded. So, different opinions on cli-
mate change may well be formed from analysis of the same
data and application of individual hypotheses to such
analysis. One could perhaps be misled into believing that
the moon is made of cheese and happy to share such an
opinion, but the deceiver would be the person who had
instilled such a belief in the first place. This definition also
covers a deception occurring through inaction—for exam-
ple, through not correcting somebody who is ill informed
regarding the cheese-ness of the moon so as to allow others
to be similarly amused by such opinions. We would con-
trast this also with lies as a specialised subgroup of
deception, and again highlight Mahon (2008):
‘‘… to make a believed-false statement with the
intention that that statement be believed to be true’’.
Hence, lies take the essentially narrow scope to specific
false statements. For example, deliberately pointing the
wrong way without saying which way to go would be a
deception, but only becomes a lie, as we assume through
this definition, through a specific spoken or written act
which could be labelled as false. Being ‘‘very economical
in his information’’ and hence concealing the truth leads to
a deception but not a lie. However, some researchers
equate lies with deceptions and have a tendency to use both
terms interchangeably (Vrij 2000; Ekman 1985).
Further, in contrast to the simple distinction of ‘‘black’’
and ‘‘white’’ as mentioned in the introduction, it can be
useful to take a broader view of outcomes from Erat and
Gneezy (2009) (Fig. 1). The authors define four types of
lies, which we consider can readily generalise to deception
with similar payoffs but without the requirement of specific
verbalisation based on their consequences, depending on
who makes profit or loss as either sender (of the lie) or
receiver:
3 Section 2 is adapted from our publication in the 13th Conference of
the European Chapter of the Association for Computational Linguis-
tics specialised workshop on Computational Approaches to Deception
Detection (Vartapetiance and Gillam 2012a). The EACL copyright
agreement allows for publication of this material in other outlets.
166 Page 2 of 14 Soc. Netw. Anal. Min. (2014) 4:166
123
1. ‘‘Selfish black lies’’ increase the receiver’s payoff but
decrease the sender’s payoff.
2. ‘‘Spite black lies’’ decrease both sender’s and recei-
ver’s payoffs.
3. ‘‘Pareto white lies’’ increase both sender’s and
receiver’s payoffs.
4. ‘‘Altruistic white lies’’ increase the receiver’s payoff
but costly to the sender.
Whilst we have suggested lying as a specialisation of
deception:
Usefully, the above figure may also be related to the
degree to which deceptive behaviour is planned. The most
harmful deceits—those that can in lower half of the fig-
ure—are those where prior thinking is more likely to be
required, potentially involving some level of preparation.
By contrast, actions with little or no negative consequences
for receivers—white lies—are more likely spontaneous.
Researchers are not necessarily in agreement over such a
characterisation (see, for example, Camden et al. 1984;
Lindskold and Walters 1983; Hample 1980), and there are
many other characteristics relating to the formulation of
deceptions/lies that become of interest as we outline in the
following.
2.1 Deception in media
Like any other human interaction, deceptive behaviour can
be divided into planned and unplanned. In planned inter-
actions, people have time to think, reflect and compare
situations with past experiences. They know or have time
to consider knowing, the person with whom they interact
(DePaulo et al. 2003). In unplanned interactions, people are
not necessarily aware of actions that will happen which
might need to be controlled. We should expect planned
deceits to be harder to detect simply because the deceivers
have time to rehearse their words and behaviours to present
the impression of being truthful, or at least being more
compelling.
In addition, choice of medium for communication can
force the type of interaction. Based on Hancock et al.
(2004a, b), deceptiveness in media relates to:
• Synchronicity: To what extent the medium provides
real-time communication.
• Distribution: Whether the people who are communi-
cating are in the same physical location or not.
• Recordability: To what extent the medium is automat-
ically recordable.
By knowing these, it is possible to argue that synchro-
nicity and unplanned interactions are directly related, so
media that are synchronous should be avoided for planned
deceits as they give opportunity to discuss, whilst deceivers
might need time to rehearse their answers so will prefer
asynchronous communication—and, indeed preferring
such mechanisms for communication as email, Facebook,
LinkedIn or Twitter.
If we focus, then, on running text as the medium for
deception, synchronicity and distribution may be variable,
but recordability is certain—even if users have a tendency
to forget this. This will mean that deceits could be planned
well in advance, which could well make their detection
somewhat more challenging. On the other hand, users of
social media may assume greater degrees of synchronicity
and a notionally lower distribution, so deceptions in social
media may be prevalent as planning is not promoted. The
next question, then, is what might be detectable, and this
brings us to the notion of deception ‘‘cues’’.
2.2 Deception detection cues
The ability to model human deception processes has
encouraged experts in many fields such as psychology,
sociology, criminology, philosophy and anthropology to
study such behaviour and look for cues as might indicate it.
Researchers have shown that telling a lie or being engaged
in deceptive behaviour is mentally, emotionally and
physically more challenging than being truthful (Vrij et al.
2001; Miller and Stiff 1993; Zuckerman et al. 1981). It is
emotionally challenging because deceivers might experi-
ence fear and threat (of being caught), guilt and shame (of
deceiving someone and of having their trust questioned) or
even duping delight (joy of deceiving someone). It is
mentally challenging as deceivers need to create a story
that is believable and consistent and try to remember what
they are saying just in case they are questioned later (Vrij
2000; Vrij et al. 2001; Miller and Stiff 1993; Cody et al.
1984; Zuckerman et al. 1981). It is physically challenging
as deceivers usually attempt to control the physical signs of
their deceptive behaviour (Vrij and Mann 2004; Buller and
Burgoon 1996). These attempts can give away a deception
or a lie as it is not easy to hide nervousness and fear/guilt,
remember lies in detail, and try to manage all of these to
make an honest impression at the same time. These will
result in behaviours which would be different from truthful
Fig. 1 Categories of lies based on change in payoffs
Soc. Netw. Anal. Min. (2014) 4:166 Page 3 of 14 166
123
actions, giving potentially detectable cues. This may
include eye movement, choice of words, arm positions or
motions, and more. One or many of these may be involved
in a single communication, but some will be more specific
to certain types of communication than others. For exam-
ple, body language and eye movements are mainly con-
sidered in synchronous, non-distributed communication,
while the structure of the sentence will be more apparent in
recordable, distributed communication such as chats and
Facebook messages. These kinds of cues can be grouped by
the 3Vs:
• Visual (Non-verbal): Any physical behaviour; reac-
tions, movements, etc. in three main groups of body
acts, postures and face.
• Vocal: Elements that accompany verbal communica-
tion with two main features involved: Nature of voice
(e.g. tone/tension, pitch) and Rhythm (e.g. number and
the length of pauses).
• Verbal: Anything said or written (e.g. wording and
structure).
Due to the nature of the medium, we are only interested
here in verbal deception, of which there are three types:
• Spoken (e.g. face-to-face, audio and video recordings).
• Written (e.g. blogs, emails, testimonies, academic
articles).
• Transcripts of spoken (e.g. phonetic transcription,
orthographic transcription).
However, recordings of speech will retain vocal elements
which may offer cues, and transcripts may offer surrogates
for pauses and retain the speech disfluencies (‘‘ums’’,
‘‘ahs’’, ‘‘like’’, and so on). Written text, then, is possibly
hardest to treat as the visual and vocal cues are missing in
contrast to spoken and transcripts (Gupta and Skillicorn
2006a, b). Interestingly, this suggests that Web and social
media content could offer ready source material but with the
significant challenge in terms of detecting deception as the
deceiving authors will have the opportunity to plan.
Many researchers have investigated the lexical, syntac-
tic, and meta-content features of verbal deception, classi-
fying pattern changes into three main dimensions: (1)
quantity; (2) quality; and (3) overall impression. Quantity
changes relate to the number of words being used. Quality
change focusses on the difference between the word
choices but still on a quantitative basis. However, overall
impression is based on human judgment from deceivers’
verbalisations including such elements as friendliness,
sounding helpful, serious, uncertain, and so on (DePaulo
et al. 2003). We discard these cues due to reasons of
subjective interpretation––judges (detectors) would need to
be trained, and while something seems believable and
helpful to one, it may not appear the same to others, and
exploring inter-annotator agreement would become a dis-
traction. We focus only on existing measurable cues that
should be independent of a judge’s training and so could be
used by both humans and machines.
For quantity and quality measurements, there are vari-
ous hypotheses, different lists of cues, and even different
expected changes. We have focussed more on studies
where ideas have gained traction through adoption (citation
and derivative exploration) by others. For example,
Pennebaker’s research has been adapted based on its style
(word-by-word), accuracy and flexibility for both written
and spoken text (e.g. Toma and Hancock 2010; Little and
Skillicorn 2008; Gupta and Skillicorn 2006a, b; Newman
et al. 2003).
2.2.1 Generalised cues
DePaulo et al. (2003) developed a list of 158 visual, verbal
and vocal deception cues, extracted from an analysis of 116
research papers between 1920 and 2001. From this list, we
consider just 25 cues to relate to verbal and to be mea-
surable, and these relate to just 10 research papers over that
period. The cues include: response length, talking time,
cognitive complexity, unique words, generalising terms,
self-references, mutual and group and other references,
word and phrase repetitions, negative statements and
complaints, and extreme descriptions. As we will show,
research since 2001 picks up on several of these cues, and
we have been able to use DePaulo’s coding system to
cross-reference subsequent papers for our own purposes.
2.2.2 Frequency-based cues
A number of researchers appear to make use of Penne-
baker’s linguistic inquiry and word count (LIWC) system
to support their experiments and claims (e.g. Gupta and
Skillicorn 2006a, b; Keila and Skillicorn 2005a, b, c;
Hancock et al. 2004a, b). They mention that the cues
defining deception according to Pennebaker involve:
Self-references: Using first person singular (e.g. me, I
and my) shows speaker ownership of a specific statement
or event. This offers a link between the reality and the
speaker, and as deceivers have not experienced that link
they will reduce the use of self-references.
Negative words: Emotions such as guilt, shame and fear
may be attributed to the deceivers’ discomfort (DePaulo
et al. 2003) and the effect of negative emotions on the
pattern of language is believed to lead to an increase in the
use of negative words.
Cognitive complexity: As suggested earlier, cognitive
complexity increases while deceiving. These effects
become apparent in statements in various ways, which
directly affects the structure of the text by changes in two
166 Page 4 of 14 Soc. Netw. Anal. Min. (2014) 4:166
123
main categories. (a) Exclusive words: statements grounded
in reality are more likely to highlight the details, including
what happened and related reactions. Deceivers, lacking
these details, use fewer exclusive words such as except,
but, without and exclude. (b) Motion/action verbs: a
decrease in exclusive words can result in an increase in
action verbs (e.g. go, lead, walk) while trying to sound
more assuring and convince others to take actions based on
their words. Moreover, cognitively, it is easier to use
simple and concrete actions in stating false stories com-
pared to fake evaluations and retaining details.
However, we have so far found little evidence that
Pennebaker has proposed cues for deception except for in
one research paper by (Newman et al. 2003). In that paper,
the authors discuss cues previously offered by others (that
relate to categories in LIWC) along with the reduction in
the number of third person pronouns, which contradicts
previous studies such as Knapp et al. (1974). Subsequent
authors have referenced such articles ambiguously, which
may give the impression that LIWC itself offers the
answer, for example, Hancock et al. (2004a, b):
‘‘[LIWC] was used to create empirically derived
statistical profiles of deceptive and truthful commu-
nications (Pennebaker et al. 2003),…’’.
and Gupta and Skillicorn (2006a, b):
‘‘Pennebaker et al. have constructed a model (LIWC)
(Newman et al. 2003; Pennebaker and Booth 2001)
for deception based on the frequencies of various
classes of words.’’
Whilst LIWC can offer analysis of individual pieces of
data, when it comes to understanding the behaviours of
cues as might indicate deception by ‘‘increase’’ or
‘‘decrease’’ in frequency, there is no clear baseline. So, to
be able to detect any deception, work would first need to be
done to (1) establish the frequency ranges for different
elements within a specific collection, (2) set thresholds of
deception per-collection and per cue, and then (3) manually
verify those above and below the deception threshold.
Relationship to some collection-specific average, or across
sources, is unlikely to readily produce appropriate results.
2.2.3 Category-based cues
Burgoon and colleagues have categorised deception cues.
However, Burgoon and other researchers have, without
much explanation that we can find, varied the number of
categories and also reported cues in different categories in
different research papers (Burgoon and Qin 2006; Qin et al.
2004, 2005; Zhou, Burgoon and Twitchell 2003a; Zhou
et al. 2003b, 2004; Burgoon et al. 2003). Indeed, they
appear to add, delete, or otherwise emphasise different cues
throughout their work. Neither the cues nor the threshold
related to their deceptiveness appears stable. A set of cues
that have been moved around categories is represented by
Black cells in Table 1. Table 1 also shows, in gray, certain
inconsistencies amongst these researchers: in Zhou et al.
(2004), the number of words, sentences and the emotive-
ness index show an increase in cases of deception, but in
Burgoon et al. (2003) and Zhou et al. (2003b) all three are
shown to decrease.
Burgoon and colleagues are not alone in offering a
categorisation; Pennebakers’ LIWC categories would be
related, modulo terminological and category variation.
However, indications of expected values for such cues
remain elusive and we only have information that some
may rise whilst others may fall.
2.3 Evaluating cues
Despite commonalities in what can be and is being studied
amongst DePaulo, Pennebaker and Burgoon, it is apparent
that there is not yet a clear set of cues with predefined
expected values that could be used for detecting verbal
deception. However, without clear descriptions of how to
interpret results, it is also possible that results could have
been misreported. To address this, we undertook a number
of small experiments—mainly geared around repetition of
previous reported experiments—to try to understand the
behaviour of deception cues.
Our example experiment here involves analysis of the
BBC article ‘‘‘Visions link’ to coffee intake’’ mentioned
previously (BBC 2009) with cues identified by Pennebaker
et al. 2003), tests on academic work (we used 100 scientific
abstracts, which we have no reason to believe are decep-
tive), and attempting to repeat an analysis of the Enron
email corpus including the emails of the executives (Keila
and Skillicorn 2005a, b, c). The latter of these is made all
the more difficult by offering three differing numbers of
emails for the analysis without details of how to obtain
such a number from the full collection. Unfortunately,
experiments all tended to support the idea that it would be
hard to detect deception ‘‘in the wild’’ reliably, in part
because deceptive texts may be hidden in amongst non-
deceptive when using such measures. It is possible to see
how this might happen with a simple experiment using the
online version of LIWC. In an experiment shown in Fig. 2,
we use the seven LIWC categories presented in the online
version, scaled by the maximum of each, for the 100 texts
from the MuchMore Springer corpus.4 We then select the
closest matching text (Nearest) from the first 10 to the
coffee article (BBC Article: presented with Circle sight).
4 MuchMore Springer Bilingual Corpus, Available at: http://much
more.dfki.de/resources1.htm.
Soc. Netw. Anal. Min. (2014) 4:166 Page 5 of 14 166
123
Figure 2 shows that values for five of these seven catego-
ries are close together, with differences for social words
and cognitive words more marked, but still well within the
ranges of truthful text (unless, of course, the scientific
articles are deceptive). A broad grain such as this is unli-
kely to be revealing, as at this grain the article ‘looks like’
science. And, indeed, similar is likely to be observed for
other pseudoscientific texts.
Historically, readability measures have been used to
indicate the proportion of the population that would be able
to understand a given text, but it has become apparent that
word familiarity, cognitive load/complexity, cohesion, and
other features of text contribute to its readability (Newbold
and Gillam 2010; Gray and Leary 1935) and are also fea-
tures considered in deception research.
Given the apparent overlap between deception and
readability, we consider whether we might use readability
measures to point more reliably to deceptive texts. Table 2
shows the cues covered by Gray and Leary (1935) for
readability which are also studied as verbal cues for
deception, along with the expectation over whether more or
fewer would occur in relation to readability and to decep-
tion (direction for the latter as suggested in e.g. Qin et al.
2004; Burgoon et al. 2003). There are some clear
Table 1 A sample of
contradictions in cues and
expectation
Italic inconsistency in expected
results; in Bold inconsistency in
categories
(**) included, (***) mentioned
but not highlighted
Q quantity, C complexity;
S specificity, A affect,
E activation/expressiveness,
D diversity, V verbal non-
immediacy, I informality,
U uncertainty, VC vocabulary
complexity, GC grammatical
complexity. (1) Qin et al. (2005)
(2) Zhou et al. (2004) (3) Zhou,
Burgoon and Twitchell (2003a)
(4) Zhou et al. (2003b) (5)
Burgoon et al. (2003)
Cues (1) (2) (3) (4) (5)
Word ** Q ?** Q ?** Q -** Q -** Q
Sentence ** Q ?** Q ?** Q -** Q -** Q
Modifiers -** U ?** U ** Q ** Q – –
First person singular – – -** V ?** V -** V – –
2nd person pronouns – – -** U ** V – – – –
3rd person pronouns – – ** V – –
Temporal details ** S ?** S – – -** S – –
Spatial details ** S – – – –
Perceptual information – – ?** S – – -** S – –
Affective terms ** A – – – – – – -** S
Positive – – ?** S ?** A -** S – –
Negative – – ?** S ?** A ?** S – –
Emotiveness index – – ?** E – – ?** E -** S
Lexical diversity -** D -** D -** D -** D – –
Redundancy -** D -** D ** D ?** D – –
Passive voice ** V ? ** V ** V ? ** V – –
Modal verbs -** U ?** U ** V ?** V – –
Uncertainty ?*** – -** U ** V ?** V – –
Objectification – – -** V ?** V ** V – –
Typo errors -*** – ?** I ?** I ?** I – –
0
0.2
0.4
0.6
0.8
1
S
co
re
d 
B
as
ed
 L
IW
C
 O
nl
in
e 
V
er
si
on
 
MAX MIN AVG BBC article Nearest (first 10)Fig. 2 How a potentially
deceptive article might hide
amongst scientific articles
166 Page 6 of 14 Soc. Netw. Anal. Min. (2014) 4:166
123
suggestions that more difficult texts (those that are less
readable) are more likely to be deceptive, but the two are
clearly not interchangeable.
Could such a clear relationship hold in practice? What
would happen with articles such as ‘‘‘Visions link’ to coffee
intake’’ or the 100 scientific abstracts? Scientific texts, and
texts offering a misrepresentation of or as science, will
probably both contain Big words, likely Nouns, may contain
rare words in contrast to general language, and possibly have
relatively complex sentences. The writing style is also likely
to impact on pronoun count. So systematic differences
amongst such values might offer an indication of deception.
Also, the online version of LIWC has a category for Big
words (those with more than six letters). The values from this
follow a similar pattern to that of Grade level for readability.
Figure 3 shows that for Coffee against 10 Springer articles,
dividing Big word by Grade level provides the lowest value
for the Coffee article. So, it is possible—indicatively, but not
conclusively—that the ratio of Big words to Grade level
could be useful.
To explore how the relationship mentioned above might
hold in practice, we consider a small experiment compar-
ing, essentially, the same core content, but which results in
different readability scores. A document that has been
(supposedly equivalently) translated several times, albeit
with particular variations, offers such a basis, and a good
example of this is the Bible. We selected the Gospel of
John (because it contains first person pronouns) from the
following four Bibles;
• New International Version (NIV)
• New King James Bible (NKJB)
• King James Bible (KJM) and
• New America Standard Bible (NASB)
We are not suggesting here that the Gospel of John is a
general representative for the English language, nor that it
should be seen to be deceptive per se, but since these
versions are translations from a single source it should help
to demonstrate any effect.
We choose four Pennebaker categories for our com-
parison. Since we have yet to find complete lists in
Pennebaker’s research, and since Newman et al. (2003) do
not offer up full lists of words, we make use of the list of 86
words cited by Little and Skillicorn (2008) as being from
Pennebaker. If all versions of the Gospel of John essen-
tially contain the same content, and if we can use these
categories for ranking purposes, we might expect to see
either equal ranks for all four cases or to have the old
versions (KJM and NASB) flagged as more deceptive
based on the criteria mentioned in Table 2.
Table 3 shows the scores for First Person (FP), Negative
Words (NW), Exclusive Words (EW) and Motion Verbs
(MV) as well as Grade Level and Reading ease score which
shows, in terms of readability, NIV and NKJB are the
better.
Based on the results presented in the table above, we
associated criteria with versions of the Bible to see which
Table 2 Readability features and their relationship to deception
Characteristics/cues More readable
text
Deceptive
text
Big words Fewer More
Nouns Fewera More
Verbs a More
Rare words Fewer More
Sentence complexity Fewer Fewer
Number of first person pronouns More Fewer
Number of second person pronouns More Fewer
Number of third person pronouns More Fewer
Average syllables per word and
sentence
Fewer More
a May vary depending of the structure of the sentence and the words
before and after them
0
5
10
15
20
25
30
35
40
45
50
S
co
re
 f
o
r 
B
ig
 W
o
rd
s 
an
d
 G
ra
d
e 
L
ev
el Grade Level  Big words (> 6 letters)Fig. 3 How big words and
grade level tend towards
indicating each other
Soc. Netw. Anal. Min. (2014) 4:166 Page 7 of 14 166
123
will satisfy more. For Newman et al. (2003), the ones,
which fit the individual criteria, are:
• Decreased frequency of first person singular
pronouns ? NIV
• Increased frequency of negative emotion
words ? KJB
• Decreased frequency of exclusive words ? NIV
• Increased frequency of action verbs ? NKJB, NIV
• However, for Little and Skillicorn (2008) they change
too.
• Increased frequency of first person singular
pronouns ? NKJB
• Increased frequency of negative emotion words ? KJB
• Increased frequency of exclusive words ? NKJB
• Increased frequency of action verbs ? NKJB, NIV
In each case, one version satisfies three out of four
criteria. Results with respect to Newman et al. (2003)
suggest the New International Version (NIV), whilst Little
and Skillicorn (2008) suggest the NKJB. Interestingly,
referring back to the Table 3, both approaches have
selected versions with lower Grade Level (more readable).
These results seem to contradict Table 2 which sug-
gested—based on a review of other researches—that more
readable should be equated to less deceptive.
This brief experiment further underlines the difficulty in
relying entirely on ideas and findings from the literature,
and leads us to question whether readability offers any gain
at this grain.
2.4 Further critique
Analysis and experiments presented above suggest that
difficulties emerge from present considerations of cues of
deception—at least in relation to verbal deception. How-
ever, it is unclear whether this is a consequence of how the
cues are being treated, or whether there are other biases
which have a telling effect. In much of this research,
conclusions have tended to be drawn on specific datasets,
many of which are not readily available for inspection or
use in repeat experiments by others.
The datasets were usually collected in one of three
ways:
Role Playing: Some are asked to deceive others (e.g.
Hancock et al. 2005; Qin et al. 2004, 2005; Burgoon et al.
2003).
Diary Keeping: Individuals are asked to document their
own interactions (e.g. Hancock et al. 2004a, b; Hancock
et al. 2009; DePaulo et al. 1996). In this type of study,
participants take time to document, once per day, their lies
and to self score them based on dimensions such as seri-
ousness, feelings while lying, and fear of getting caught.
Obtained as-is: (e.g. Keila and Skillicorn 2005a, b, c).
Most such studies adopt Pennebaker’s approach. This
means that any classificatory thresholds have to be manu-
ally set and evaluated, via the means of the Human
Eyeball.
All three approaches suffer from potential experimental
effects. For the first two, it would be important to control
for the Hawthorne effect which highlights that ‘‘observa-
tion and studies can change the behaviour of the partici-
pants’’ regardless of whether they should have really
changed anything specifically in diary-based studies (Jones
1992; Franke and Kaul 1978). We believe during such
studies peoples’ behaviours might change, intentionally or
otherwise. This might be because they become more cau-
tious about perceptions of them by the researchers, want to
avoid fear, shame, and so on, or feel uncomfortable with
undertaking or documenting such an act. Indeed, the
researchers may even be being deceived about the decep-
tions by the subjects. The third approach leaves the deci-
sion regarding actual deceptiveness of the text or statement
open to the possibility that the researcher has been ‘‘primed
by expectations’’ (Doyen et al. 2012).
3 Tests for deception in the PAN experiments
In the previous section, we looked to elaborate the notion
of deception and the relationship of social, psychological
and technical views, and to try to highlight problematic
areas and research gaps. We might conclude from this
treatment that current systems and approaches for auto-
mated and semi-automated deception detection will not be
readily effective for the majority of deceptive situations,
both online and offline, without significant efforts in
attempting to ascertain the magic numbers appropriate to
making particular techniques act as classifiers. We have
seen deception detection attempted with the Enron email
dataset, Court testimonies from Quebec, Diaries of College
students, Online Dating profiles, Chat Conversations,
Facebook Messages, SMS Communications and many
Table 3 Variables for deception and readability for gospel of john in
four bibles
FP NW EW MV Grade level Reading easea
NIV 1.96 0.24 0.53 0.47 6.48 78.11
NKJB 3.44 0.12 1.00 0.48 7.24 77.21
KJB 2.29 0.28 0.69 0.37 7.78 74.48
NASB 2.04 0.23 0.65 0.44 8.38 73.88
Newman et al. (2003): Italic
Little and Skillicorn (2008): Bold
a Readability values from: http://www.online-utility.org/english/read
ability_test_and_improve.jsp
166 Page 8 of 14 Soc. Netw. Anal. Min. (2014) 4:166
123
others. And we have seen unexplained variation in
approaches to these, in particular in treatment of datasets in
isolation. Rather than trying to tune techniques towards
these, our approach instead is to look for techniques which
might more readily generalise across such data. However,
as we will show, what we might detect will vary inherently
with the nature of the deception itself. We explore decep-
tion here in relation to a variety of tasks that can be
characterised as deceptive in some way or other in PAN
(2012) and (2013).
3.1 Authorship identification
Authorship identification is the task of identifying the
original author of a piece and can help in solving ambig-
uous cases mentioned above. The relationship between
authorship identification and deception is not clear-cut
since it depends on the context. For example, identifying an
author of a book may be merely for curiosity. It may be to
determine whether there are potentially multiple authors
for example in partially ghost-written works such as Fed-
eral Papers (Mosteller and Wallace 1964) and autobiog-
raphies by iconic individuals (e.g. David Beckham’s My
Side Walker 2004). However, it may also relate more
directly to acts of plagiarism. Authorship identification is
readily relevant to social media: consider, for example,
testing an anonymous email where the author is known to
be within the organisation vs. an anonymous email that
could come from any source, or where the medium is
Twitter, Facebook or LinkedIn, or even where the pur-
ported author may be the victim of hacking.
Since interested readers apparently gain from such an
‘autobiography’, and may knowingly purchase such an
item, and the supposed author gains from sales, we can
consider this ‘‘Pareto-white’’. Is such a work deceptive?
Only in so far as it does not necessarily deserve the label
‘autobiography’. Does it contain deceptions or lies? It may,
depending on what the author(s) want us to believe about
the individual. In contrast, it might be possible to claim
‘‘Spite black’’ for incidents such as the infamous Dodgy
Dossier5 which was claimed to have been produced by
British Intelligence but apparently omitted the typical
hedges on veracity that are core to such a document—and
those who produced the document must have gambled on
negative consequences to their own careers—although they
seem to have suffered markedly little.
A study on authorship identification tends to require
original work by authors. The number of authors consid-
ered is important, as this leads to separation between
attribution and verification:
• Verification: Given an unknown document and a set of
documents from a single author, decide if the unknown
document was written by the same author.
• Attribution: Given an unknown document and sets of
known documents from different authors,
– Denote an author for each unknown document
(closed class problem).
– An extension to the above where the author may not
be represented by these sets (open class problem).
Attribution is, of course, a kind of interdependent veri-
fication in which only one verification is possible (or there
is no verification at all). There is, of course, an extension to
these above in that such tasks should be considered as not
necessarily applicable to a document in its entirety, but to
variously sized chunks of the document up to its full size to
address the problem of multiple authors.
Recent research in authorship focusses on usage of
numerous techniques that operate as contrasts over bags
of words, N-grams, and parts of speech, with varying
degrees of success. Such approaches are prevalent in the
PAN competitions, which focus only on whole-text
identification, particularly in 2012 and 2013. Consider our
own approach to this task, based on Church and Hanks
(1991), using a mean–variance framework for detecting
strong associations between co-occurring stopwords—in
contrast to the kinds of fixed phrases such as ‘‘bread and
butter’’, which demonstrate a clear preference over
‘‘butter and bread’’, as they were exploring. Our approach
essentially investigates grammatical preference, with
mean and variance indicative of separation due to selec-
tion of lexical words without needing to address lexical
words specifically (Vartapetiance and Gillam 2012b). For
PAN (2013), using 10 or fewer stopwords for each of
three languages—English, Greek, Spanish, of which the
authors are only fluent in English—this approach attains
an f1 value of 0.54 across the three languages (0.5, 0.53
and 0.6 for the respective languages), with an f1 of 0.56
in a trial submission showing consistency of results, and
also quite a reasonable consistency across three
languages.
The kinds of contrasts being used for such a task are not
geared towards detecting a deception or a lie as the back-
ground literature might suggest. We are in written, verbal
but with disfluencies, quantity, quality and overall
impression are irrelevant—mere counts of unique words,
self-references, negative words, and so on, do not identify a
deception for such a task they merely characterise a text. A
contrastive approach requires a specific difference to be
identifiable and it is unclear that contrasts of such features
across entire sets of texts are readily suited for such a task:
authors write different things and so scores will vary per
work per author.5 Dossier D (2013).
Soc. Netw. Anal. Min. (2014) 4:166 Page 9 of 14 166
123
3.2 Plagiarism: intrinsic detection and authorship
clustering
Plagiarism is a specific kind of deception in which the
purported author attempts to deceive others to believe that
the expression of an idea, either as a fragment of a work or
as an entire work, is theirs. Common types of plagiarism
involved are (Maurer et al. 2006):
• Copy-paste: Copying word to word textual contents.
• Idea plagiarism: using similar concept or opinion which
is not common knowledge.
• Paraphrasing: changing grammar, similar meaning
words, re-ordering sentences in original work. Or
restating same contents in different words.
• Artistic plagiarism: presenting someone else’s work
using different media, such as text, images, voice or
video.
• Code plagiarism: using program code, algorithms,
classes, or functions without permission or reference.
• Forgotten or expired links to resources: addition of
quotations or reference marks but failing to provide
information or up-to-date links to sources.
• No proper use of quotation marks: Failing to identify
exact parts of borrowed contents.
• Misinformation of references: adding references to
incorrect or non existing original sources.
• Translated plagiarism: cross language content transla-
tion and use without reference to original work.
Here, addressing Erat and Gneezy (2009) taxonomy of
payoffs would seem to involve considering whether the
receiver is aware and whether the sender is caught and
punished. If not caught, both sender and receiver have a
positive payoff. And, in fact, a positive payoff for the
sender should be true of any really successful deception—a
failed deception should have negative payoff, unless the
intention of the deception was to expose the truth. Perhaps
this taxonomy is also insufficiently granular to fully
address the notion of payoff?
Plagiarism as a task for detection has been divided, in
PAN, into two categories:
• External: Comparing suspicious documents against a
set of potential original documents (Stein et al. 2007).
These are the reference corpus and the task is to
identify pairs of identical or very similar passages
where passage length is very variable (Gipp et al. 2011;
Stamatatos 2009; Seaward and Matwin 2009).
• Intrinsic: Identifying potentially plagiarised content in a
document only by analysing the document in isolation
and attempting to distinguish changes in topic or style
that indicate suspicious passages (Gipp et al. 2011;
Seaward and Matwin 2009; Stein et al. 2007).
External detection is a closer fit to academic rigour
around student assessment where it is assumed that all
source materials from which material could be lifted are
available to the detection system. But to detect deceptions
in fast-moving social media, it is likely that intrinsic
detection is more efficient.
Intrinsic plagiarism detection systems must attempt to
imitate humans in the sense that they can sometimes
identify suspicious passages without checking external
sources, for example, in being able to identify sudden
changes in the grammar or narrative (Stein et al. 2010;
Clough and Stevenson 2010; Eissen and Stein 2006).
Clough and Stevenson (2010) state that the most extreme
version of intrinsic plagiarism is when an entire document
is copied without any changes. If sources can be found that
validate this, it would represent an external detection, but it
is also an authorship attribution/verification task (Stein
et al. 2007, 2010) with key differences as shown in the
table below (Table 4):
Intrinsic plagiarism detection has to rely on contrasts of
features (Seaward and Matwin 2009; Eissen and Stein
2006) of chunks of the text, which then makes the task
similar to authorship verification for each chunk in com-
parison to the others. Many intrinsic plagiarism detection
methods have used ‘‘Stylistic features’’—a.k.a author fin-
gerprinting or stylometry (Stamatatos 2009; Seaward and
Matwin 2009; Stein et al. 2007). Stamatatos (2009) iden-
tifies five categories of features:
1. Lexical features: Word frequencies, word n-grams,
vocabulary richness, etc.
2. Character features: Character types, character n-grams
3. Syntactic features: Part-of-speech frequencies, types of
phrases, etc.
4. Semantic features: Synonyms, semantic dependencies,
etc.
5. Application-specific features: Structural, content-spe-
cific, language-specific.
Table 4 Key contrasts between authorship tasks and intrinsic pla-
giarism detection
Authorship attribution/
verification
Intrinsic plagiarism detection
Long text: usually
document length
Short text: usually paragraph length
Comparison with other
documents
Comparison with other paragraphs within
the same documents
Boundaries for
comparison are
known
Boundaries for comparison are unknown—
as plagiarism might be a sentence with a
paragraph
Single author per
document
Unknown number of authors per document
166 Page 10 of 14 Soc. Netw. Anal. Min. (2014) 4:166
123
Intrinsic plagiarism detection task has been divided into
two classes in PAN to,
– Cluster/identify paragraphs by each author–number of
authors known (closed class clustering)
– Cluster/identify paragraphs by each author–number of
authors unknown (open class clustering)
Our only efforts in intrinsic plagiarism detection were in
PAN (2012) and as this was the last time this task was run.
For closed class clustering, we remove a fixed list of
stopwords to use the first five most frequent remaining
words and clustered paragraphs depending on the usage of
those words in each paragraph; for open class clustering,
we used the five most frequent nouns(Vartapetiance and
Gillam 2012b). We achieved accuracy of 82.2 % for closed
and 100 % for open class clustering tasks. Neither of these
approaches bears much relation to the extensive sets of
features claimed to be cues for detection, and only lexical
features seem pertinent from the Stamatatos categories.
Again the kinds of contrasts being used for such a task
are not geared towards detecting a deception or a lie as the
background literature might suggest. We are in written,
verbal with disfluencies, quality; overall impression,
unique words, self-references, negative words, and so on
do little for us. Only quantity has a relevance here for
carefully selected subsets of words. Again, this is a highly
contrastive approach that requires specific differences to be
identifiable across passages.
3.3 Sexual predator detection
Sexual predators apparently inhabit chat rooms, aiming to
dupe children into meeting them in the real world. This
online world offers many advantages to them over the
physical world:
• They can remain hidden away from the eyes of others
during the duping.
• They can target more than one child readily.
• They can bypass parental supervision.
• They can assume any persona—e.g. age, gender and
image—and craft elaborate and apparently exciting
stories as they wish.
A difficulty for detection is that, yet again, most of the
cues for deception detection do not seem to be particularly
relevant, not least because well-formed sentences may not
readily occur during such situations, but also because
conversations will readily include or exclude self-refer-
ences depending on topic, and negative emotions may be
readily expressed without entailing anything of a suspi-
cious nature. Worst of all, in terms of deception cues, some
predators are certainly not deceptive about what they
would like to do.
PAN (2012) introduced the challenge of detecting sex-
ual predators based on chat-logs with two tasks on datasets
of chat lines:
– Identify whether the chat indicates a predator.
– Identify the predatory elements of the chat.
We are aware of previous research in this direction that
has involved use of support vector machines, neural net-
work classifiers, lexical features (in particular unigrams
and bigrams, their weighting using TF-IDF). However,
these do not seem to fit with ‘‘behavioural features’’ -‘‘the
number of times a user starts a dialogue, the response time
after a message of the partner in the conversation, the
number of questions asked, the frequency of turn-taking,
intention (grooming, hooking,…), etc. (Inches & Crestani,
2012)’’––relating to requests for personal information of
certain kinds but variation in the wording of the request.
We looked at samples of the training data and identified
four principal classes of information request:
• Address: Most ask for the address of the house or
somewhere nearby to travel to.
• Parents: Questions about whether they are aware of the
communication or whether the potential prey might be
alone.
• Age: Predators can be quite open about their age. They
would usually highlight the fact that they are older, and
wishing the child was older.
• Activities: References to sexual activities—what they
would like to do; this can be quite explicit.
Based on sets of examples for each category, we were
able to detect 113 out of 142 predators with recall of 80 %
(based on the existence of two or more occurrences of all
four categories). Moreover, we tested each indicator indi-
vidually—varying the number of occurrences and in vari-
ous combinations—to analyse the significance of each
class. Interestingly, although all four classes play an
important role, the combination of two or more occurrences
of parents and address classes together flagged 105 pre-
dators correctly.
For this task, contrasts are not particularly relevant.
However, various supposed cues for deception are also not
necessarily of relevance given the importance of the four
categories above. And, if the predator is being honest
regarding what they say, it is going to be difficult to sug-
gest that any deception cues of a syntactic, lexical, quality
or quantity are going to perform well.
It is important, also, to mention that one of the diffi-
culties with sexual predator detection relates to the dataset.
There are many datasets that could be used for authorship
identification and intrinsic plagiarism detection, and it is
readily possible to craft such a dataset. However, for sexual
predator detection, a variety of legal and privacy issues
Soc. Netw. Anal. Min. (2014) 4:166 Page 11 of 14 166
123
restrict availability, and it is unlikely that researchers will
be entirely comfortable attempting to emulate a sexual
predator to create a test collection. The only readily
available dataset is provided by Perverted Justice Foun-
dation.6 This is a non-profit organisation where the vol-
unteers act as honey pots—by imitating children—to
attract the predators. Their conversation is only published
when the predator is convicted. This does mean that this
collection can be used as a single point of comparison for
those involved with deception detection to contrast their
approaches—and in doing so to improve the potential for
detection of such a monstrous activity—but it would be
difficult to determine whether the approach extends well to
other datasets, or to data that become available once such
predators learn about what is being detected.
4 Conclusion
This paper has reviewed previous research in deception
detection to assess whether deception detection is readily
possible across a variety of deceptions. On present evi-
dence, whilst there may be various important findings,
there are too many areas open to question to believe that a
general purpose deception detection system could readily
be constructed.
We still believe that previous deception detection
research has a significant role to play, but many of the
difficulties outlined in this paper need to be addressed first.
This begins from considering:
1. What kinds of deceptions exist and how they are
embodied.
2. Whether simple counts or contrasts are relevant and if
so of what:
(a) Categories themselves (e.g. for sexual
predators).
(b) Categories between documents or chunks of
documents (authorship verification/attribution.
(c) Categories between chunks of an individual
document (intrinsic plagiarism detection).
Systematic approaches to datasets would then identify
specific approaches that are suited to specific kinds of
deception detection. For example, the categories as might
indicate predators are not suited for authorship. Deception
as distancing is not necessarily relevant to identifying fraud
in financial statements since people inherently distance
themselves from bad news (preferring to blame, for
example, ‘‘the economy’’ or other external factors rather
than mere bad management). Rigour in identifying cues
tested, following DePaulo, would be highly beneficial.
From this, it may then be possible to identify specific
cues as worth studying in certain genres, whilst of little
interest in others—irrespective of their frequency of use.
Systematic exploration grounded in psychological theories
are more likely to achieve useful results for the future, in
contrast to what will be achieved by a multiplicity of
theory-independent approaches that pull every available
feature into a machine learning toolbox and running them
against a given dataset. The latter approach may lead to the
development of a useful system that can work for a specific
dataset, but the researchers will likely to struggle to explain
why the approach worksOr, as Skillicorn and Little have
carefully hedged their own use of Pennebaker’s deception
‘model’ against election speeches:
‘‘We do not yet completely understand these models
of word use, so the results here should be taken with a
grain of salt’’.7
Acknowledgments The authors gratefully acknowledge prior from
EPSRC/JISC (EP/I034408/1), the UK’s Technology Strategy Board
(TSB, 169201), and also the efforts of the PAN13 organizers in
crafting and managing the tasks.
References
BBC (2009) ‘Visions Link’ to coffee intake. BBC News. Retrieved
10.07.2013 from http://news.bbc.co.uk/1/hi/health/7827761.stm
Buller DB, Burgoon JK (1996) Interpersonal deception theory.
Commun Theory 6:203–242
Burgoon JK, Qin T (2006) The dynamic nature of deceptive verbal
communication. J Lang Soc Psychol 25(1):76–96
Burgoon JK, Blair JP, Qin T, Nunamaker JF Jr (2003) Detecting
deception through linguistic analysis. In: Proceedings of first
NSF/NIJ symposium on intelligence and security informatics
(ISI), Tucson, pp 91–101
Camden C, Motley MM, Wilson A (1984) White lies in interpersonal
communication: a taxonomy and preliminary investigation of
social motivations. West J Speech Commun 48:309–325
Church K, Hanks P (1991) Word association norms, mutual
information and lexicography. J Comput Linguist 16(1):22–29
Clough P, Stevenson M (2010) Developing a corpus of plagiarised
short answers. J Lang Resour Eval 45(1):5–24
Cody MJ, Marston PJ, Foster M (1984) Deception: paralinguistic and
verbal leakage. In: Bostrom RN, Westley BH (eds) Communi-
cation Yearbook 8. Sage, Beverly Hills, pp 464–490
DePaulo BM, Kashy DA, Kirkendol SE, Wyer MM, Epstein JA
(1996) Lying in everyday life. J Pers Soc Psychol 70:979–995
DePaulo BM, Lindsay JJ, Malone BE, Muhlenbruck L, Charlton K,
Cooper H (2003) Cues to deception. Psychol Bull 129(1):74–118
Dossier D (2013). Wikipedia: The Free Encyclopaedia. Wikimedia
Foundation Inc. Encyclopaedia on-line. Retrieved 10/07/2013
from http://en.wikipedia.org/wiki/Iraq_Dossier
6 http://www.perverted-justice.com/.
7 Spinning the Election (Skillicorn and Little) http://research.cs.
queensu.ca/home/skill/election/election.html.
166 Page 12 of 14 Soc. Netw. Anal. Min. (2014) 4:166
123
Doyen S, Klein O, Pichon CL, Cleeremans A (2012) Behavioral
priming: it’s all in the mind, but whose mind? PLoS One 7(1):
e29081. doi:10.1371/journal.pone.0029081
Eissen SM zu, Stein B (2006) Intrinsic plagiarism detection. In:
Advances in information retrieval, Lecture notes in computer
science, vol 3936, pp 565–569
Ekman P (1985) Telling lies, clues to deceit in the marketplace,
politics, and marriage. W.W. Norton & Company, New York
Erat S, Gneezy U (2009) White Lies, Rady Working paper, Rady
School of Management, University of California, San Diego
Pennebaker JW, Francis, ME, Booth RJ (2001) Linguistic inquiry and
word count (LIWC). Erlbaum Publishers
Franke RC, Kaul JD (1978) The Hawthorne experiments: first
statistical interpretation. Am Sociol Rev 43(5):623–643
Gipp B, Meuschke N, Beel J (2011) Comparative evaluation of text-
and citation-based plagiarism detection approaches using Gut-
tenPlag. In: Proceedings of 11th ACM/IEEE-CS joint conference
on digital libraries (JCDL’11), pp 225–258
Gray WS, Leary BE (1935) What Makes a Book Readable. Chicago
University Press, Chicago
Gupta S, Skillicorn D (2006) Improving a textual deception detection
model. In: Proceedings of the 2006 conference of the center for
advanced studies on collaborative research, Toronto, pp 1–4
Gupta S, Skillicorn D (2006) Improving a textual deception detection
model. In: Proceedings of the 2006 conference of the center for
advanced studies on collaborative research, Canada, pp 1–4
Hall HV, Pritchard DA (1996) Detecting Malingering and Deception.
Forensic Distortion Analysis (FDA). St. Lucie Press, Boca Raton
Hample D (1980) Purposes and effects of lying. South Speech
Commun J 46:33–47
Hancock JT, Curry L, Goorha S, Woodworth MT (2004) Lies in
conversation: an examination of deception using automated
linguistic analysis. In: Proceedings of annual conference of the
cognitive science society, 26, pp 534–540
Hancock JT, Thom-Santelli J, Ritchie T (2004) Deception and design:
the impact of communication technology on lying behaviour. In:
Proceedings of the conference on human factors in computing
systems (ACM SIGCHI), pp 129–134
Hancock JT, Curry L, Goorha S, Woodworth MT (2005) Automated
linguistic analysis of deceptive and truthful synchronous com-
puter-mediated communication. In: Proceedings of the 38th
annual Hawaii international conference on system sciences
(HICSS-38), IEEE Press, Los Alamitos
Hancock JT, Birnholtz J, Bazarova N, Guillory J, Amos B, Perlin J
(2009) Butler lies: awareness, deception and design. In:
Proceedings of the ACM conference on human factors in
computing systems (CHI 09), Boston, pp 517–526
Inches G, Crestani F (2012) Overview of the international sexual
predator identification competition at PAN-2012. In: Proceedings
of the 6th PAN workshop at CLEF2012 on uncovering plagiarism,
authorship, and social software misuse (PAN2012), Rome
JonesSR (1992) Was there aHawthorne effect? Am JSociol 98(3):451–468
Keila PS, Skillicorn DB (2005a) Detecting unusual and deceptive
communication in email. In: Proceedings of the 2005 conference
of the centre for advanced studies on collaborative research,
Toronto, Ontario, Canada, IBM Press
Keila PS, Skillicorn DB (2005b) Structure in the Enron email dataset.
Comput Math Organ Theory 11(3):183–199
Keila PS, Skillicorn DB (2005b) Detecting unusual email communi-
cation. In: Proceedings of the 2005 conference of the centre for
advanced studies on collaborative research, pp 117–125
Knapp ML, Hart RP, Dennis HS (1974) An exploration of deception
as a communication construct. Human Commun Res 1:15–29
Left S (2002) Casting the Net for Paedophiles. Guardian Unlimited.
Retrieved 10.07.2013 from http://www.guardian.co.uk/technol
ogy/2002/apr/24/internetnews.childprotection1
Lindskold S, Walters PS (1983) Categories for acceptability of lies.
J Soc Psychol 120:129–136
Little A, Skillicorn B (2008) Detecting deception in testimony. In:
Proceeding of IEEE international conference of intelligence and
security informatics (ISI 2008), Taipei, pp 13–18
Mahon JE (2007) A definition of deceiving. Int J Appl Philos
21:181–194
Mahon JE (2008) Two definitions of lying. Int J Appl Philos
22(2):211–230
Masip J, Garrido E, Herrero C (2004) Defining deception. Anales de
Psicologia 20(1):147–171
Maurer H, Kappe F, Zaka B (2006) Plagiarism: a survey. J Univ
Comput Sci 12(8):1050–1084
Miller GR, Stiff JB (1993) Deceptive communication. Sage, Newbury
Park
Mosteller F, Wallace DL (1964) Inference and disputed authorship:
the federalist. Addison Wesley
Newbold N, Gillam L (2010) The linguistics of readability: the next
step for word. In: Processing. Workshop on computational
linguistics and writing: writing processes and authoring aids
(CLandW 2010), Los Angles, pp 65–72
Newman ML, Pennebaker JW, Berry DS, Richards JM (2003) Lying
words: predicting deception from linguistic styles. Pers Soc
Psychol Bull 29(5):665–675
PAN (2012) In: 6th PAN workshop at CLEF2012 on uncovering
plagiarism, authorship, and social software misuse (PAN2012), Rome
PAN (2013) In: 7th PAN workshop at CLEF2013 on uncovering
plagiarism, authorship, and social software misuse (PAN2013),
Valencia
Pennebaker JW, Mehl M, Niederhoffer K (2003) Psychological
aspects of natural language use: our words, our selves. Annu Rev
Psychol 54(1):547–577
Qin T, Burgoon JK, Nunamaker JF Jr (2004) An exploratory study on
promising cues in deception detection and application of
decision trees. In: Proceedings of the 37th Hawaii international
conference on system sciences, Waikoloa, pp 23–32
Qin T, Burgoon JK, Blair JP, Nunamaker JF (2005) Modality effects
in deception detection and applications in automatic-deception-
detection. In: Proceedings of the 38th annual Hawaii interna-
tional conference on system sciences, pp 1–10
Robin Sage (2013). Wikipedia: The Free Encyclopaedia. Wikimedia
Foundation Inc. Encyclopaedia on-line. Retrieved 10/07/2013
from http://en.wikipedia.org/wiki/Robin_Sage
Russow LM (1986) Deception: a philosophical perspective. In:
Mitchell RW, Thompson NS (eds) Deception: perspective on
human and nonhuman deceit. State University of New York
Press, Albany, pp 41–52
Seaward L, Matwin S (2009) Intrinsic plagiarism detection using
complexity analysis. In: Proceedings of the 3rd international
workshop on uncovering plagiarism, authorship, and social
software misuse (PAN09), pp 56–61
Stamatatos E (2009) Intrinsic plagiarism detection using character
n-gram profiles. In: Proceedings of the 3rd international work-
shop on uncovering plagiarism, authorship, and social software
misuse (PAN09), pp 38–46
Stein B, Koppel M, Stamatatos E (2007) Plagiarism analysis,
authorship identification, and near-duplicate detection PAN’07.
ACM SIGIR Forum pp 68–71
Stein B, Lipka N, Prettenhofer P (2010) Intrinsic plagiarism analysis.
Lang Resour Eval 45(1):63–82
Toma CL, Hancock JT (2010) Reading between the lines: linguistic
cues to deception in online dating profiles. In: Proceedings of the
2010 ACM conference on Computer-Supported Cooperative
Work (CSCW 2010), Savannah, pp 5–8
Vartapetiance A, Gillam L (2012) I don’t know where he’s not’: does
deception research yet offer a basis for deception detectives? In:
Soc. Netw. Anal. Min. (2014) 4:166 Page 13 of 14 166
123
Proceeding of EACL 2012 workshop on computational
approaches to deception detection, Avignon, pp 5–15
Vartapetiance A, Gillam L (2012) Quite simple approaches for
authorship attribution, intrinsic plagiarism detection and sexual
predator identification. In: Proceedings of the 6th PAN workshop
at CLEF2012 on uncovering plagiarism, authorship, and social
software misuse (PAN2012), Rome
Vijayan J (2010) Fake femme fatale shows social network risks.
Computerworld Magazine. Retrieved 10.07.2013 from http://
www.computerworld.com/s/article/9179507/Fake_i_femme_
fatale_i_shows_social_network_risks
Vrij A (2000) Detecting lies and deceit: the psychology of lying and
its implications for professional practice. Wiley, Chichester
Vrij A, Mann S (2004) Detecting deception: the benefit of looking at a
combination of behavioral, auditory and speech content related
cues in a systematic manner. Group Decision and Negotiation
(special deception issue) 13:61–79
Vrij A, Edward K, Bull R (2001) Stereotypical verbal and nonverbal
responses while deceiving others. Pers Soc Psychol Bull
27:899–909
Walker D (2004) I’m a celebrity, get me a ghost writer. BBC News.
Retrieved 10.07.2013 from http://news.bbc.co.uk/1/hi/magazine/
3757275.stm
Zhou L, Burgoon JK, Twitchell DP (2003) A longitudinal analysis of
language behavior of deception in E-mail. In: Proceedings of
Intelligence and Security Informatics, 2665, pp 102–110
Zhou L, Twitchell DP, Tiantian Q, Burgoon JK, Nunamaker JF Jr
(2003) An exploratory study into deception detection in text-
based computer-mediated communication. In: Proceedings of the
36th annual Hawaii international conference on system sciences,
Waikoloa, pp 10–19
Zhou L, Burgoon JK, Zhang D, Nunamaker JF Jr (2004) Language
dominance in interpersonal deception in computer-mediated
communication. Comput Hum Behav 20(3):381–402
Zuckerman M, DePaulo BM, Rosenthal R (1981) Verbal and
nonverbal communication of deception. In: Berkowitz L (ed)
Advances in Experimental Social Psychology, 14:1–59
166 Page 14 of 14 Soc. Netw. Anal. Min. (2014) 4:166
123
