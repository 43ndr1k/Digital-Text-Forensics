See	discussions,	stats,	and	author	profiles	for	this	publication	at:	http://www.researchgate.net/publication/283442899
Enhancing	the	Determination	of	Aspect
Categories	and	Their	Polarities	in	Arabic
Reviews	Using	Lexicon-Based	Approaches
CONFERENCE	PAPER	·	NOVEMBER	2015
READS
83
5	AUTHORS,	INCLUDING:
Rami	Mohawesh
Jordan	University	of	Science	and	Technology
2	PUBLICATIONS			0	CITATIONS			
SEE	PROFILE
Mahmoud	Al-Ayyoub
Jordan	University	of	Science	and	Technology
68	PUBLICATIONS			232	CITATIONS			
SEE	PROFILE
Mohammad	AL-Smadi
Jordan	University	of	Science	and	Technology
44	PUBLICATIONS			90	CITATIONS			
SEE	PROFILE
Yaser	Jararweh
Jordan	University	of	Science	and	Technology
81	PUBLICATIONS			232	CITATIONS			
SEE	PROFILE
Available	from:	Mahmoud	Al-Ayyoub
Retrieved	on:	25	November	2015
Enhancing the Determination of Aspect Categories
and Their Polarities in Arabic Reviews Using
Lexicon-Based Approaches
Islam Obaidat, Rami Mohawesh, Mahmoud Al-Ayyoub, Mohammad AL-Smadi and Yaser Jararweh
Jordan University of Science and Technology, Irbid, Jordan
Emails: {iamobaidat, rami5227, malayyoub, smadi.mohammad, yaser.amd}@gmail.com
Abstract—Sentiment Analysis (SA) is the process of determin-
ing the sentiment of a text written in a natural language to be
positive, negative or neutral. It is one of the most interesting
subfields of natural language processing (NLP) and Web mining
due to its diverse applications and the challenges associated with
applying it on the massive amounts of textual data available
online (especially, on social networks). Most of the current
works on SA focus on the English language and work on the
sentence-level or the document-level. This work focuses on the
less studied version of SA, which is aspect-based SA (ABSA)
for the Arabic language. Specifically, this work considers two
ABSA tasks: aspect category determination and aspect category
polarity determination, and makes use of the publicly available
human annotated Arabic dataset (HAAD) along with its baseline
experiments conducted by HAAD providers. In this work, several
lexicon-based approaches are presented for the two tasks at hand
and show that some of the presented approaches significantly
outperforms the best known result on the given dataset.
Index Terms—aspect-based sentiment analysis; aspect category
determination; aspect category polarity determination; sentiment
lexicon; category lexicon.
I. INTRODUCTION
The field of Arabic Natural Language Processing (NLP)
is a growing field with many interesting and challenging
problems. Two types of Arabic are usually considered in
Arabic NLP papers: Modern Standard Arabic (MSA) and
dialects (vernaculars). MSA is derived from Classical Arabic.
It is the official Arabic language used in media, education,
culture, literature, official documents, old books and most of
new books throughout the Arab world, which spans regions of
the Middle East and North Africa (MENA) in addition to parts
of East Africa (Horn of Africa). It is one of the six official
languages used in the United Nations. It is the native language
of 420 million people [1].
Classical Arabic and MSA remained the only documented
versions of Arabic till mid 1990s when the dawn of In-
ternet services and mobile communication pushed for the
documentation of different Arabic dialects (vernaculars). The
widespread use of emails, SMS, blogs and later social media
helped in documenting these Arabic vernaculars in addition
to giving birth to a new version of Arabic called Arabizi in
which the Arabic words are transliterated using the Roman
alphabet [2].
A number of specialists like Habash [2] consider the Arabic
vernaculars as the true native language forms, since they are
used in daily informal communications between people who
live in the Arab world. Although these Arabic vernaculars lack
standardization, are not generally found in written form and
are not officially taught, they can be found in TV shows,
movies, songs, theaters, etc. Arabic vernaculars are classi-
fied by linguists into seven main regional language groups:
Maghrebi, Egyptian, Mesopotamian, Arabian Peninsula, Su-
danese, Levantine, and Andalusian (now extinct) [3], [4].
Sentiment analysis (SA) and opinion mining (OM) is a
growing field of study that automatically determines people’s
opinions, sentiments, attitudes, and emotions from written text
or speech excerpts [5]. It is the focus of a large number of
research projects and the reasons for this are: availability of
a number of good machine learning methods, the availability
of huge corpora and, most importantly, the realization of the
intellectual challenges and commercial applications of SA [6].
This field of study is active in many research areas such as
NLP, data mining, Web mining, and text mining [5]. Due to its
vast applications, SA has spread from computer science to the
management sciences, political science, economics, and social
sciences [5].
Most works on SA focus on sentence-based or document-
based SA. A very interesting version of SA known as aspect-
based SA (ABSA) is less studied in the literature despite its
grave importance. This might be due to the several challenges
it poses. This is the case for the well-studied English language.
The situation is even worse for other languages such as Arabic
where dozens of paper have been published in the few years
on SA with only two papers [7], [8] (as far as we know)
published on ABSA.
Researchers in the field of SA usually depend on lexicons
as essential resources in their studies to identify the polarity
of different sentiments. Lexicons used in SA comprise of a list
of sentiment words (opinion words, polar words, or opinion-
bearing words) and sentiment phrases that used to express
positive or negative sentiments. Liu presents in his book the
major challenges facing the use of such lexicons [5].
Many studies presented different algorithms to compile
lexicons of sentiment-bearing words from the English lan-
guage. On the other hand, few studies presented algorithms
to construct lexicons for Arabic words such as [9], [10],
[11]. A number of SA studies of Arabic sentiments are based
on manually constructed lexicons such as [12]. Manually
constructed lexicons are characterized by their quality, but they
are limited in size.
This study is distinguished by applying different methods
proposed in literature for lexicon creation relative to the
studies conducted by [9], [10], [11]. A comparative study and
analysis is conducted to evaluate and test the different lexicon
construction approaches’ flexibility and effectiveness for the
Arabic language.
The rest of this paper is organized as follows. Section II
surveys related studies. Then, the adopted methodology is pre-
sented in Section III along with a discussion of the experimen-
tation setup and results analysis. Finally, the main conclusions
are stated and future work guidelines are addressed.
II. BACKGROUND AND RELATED WORKS
In this section, we present some background information
important to understand and appreciate this work. We then
discuss existing works related to ABSA of Arabic text.
A. Aspect-Based Sentiment Analysis Tasks
Going through a reference like Liu’s book on SA [5], one
would be overwhelmed with the various challenges posed by
ABSA and the complexities associated with them. However,
the community has approached ABSA in a slow and careful
manner. Instead of trying to address all aspects related to
ABSA at once, the community started with the simplest
versions of ABSA and is gradually expanding to address more
issues related to ABSA. This is most evident in the SemEval
workshops,1 which are a very prestigious series of workshops
posing different tasks every year. ABSA is one of the recurring
tasks in SemEval over the past few years. Each year, the
requirements and test for the ABSA task get more difficult
signifying the rapid progress in this field. Another example of
this trend is present in [8] where the authors focused on only
some of the basic tasks of ABSA.
Following the guidelines of SemEval2014, ABSA is divided
into multiple tasks. We briefly discuss some of these tasks in
the following few paragraphs. The discussion here pertains
to the dataset we consider in this work which is the human
annotated Arabic dataset (HAAD) of book reviews [7].
Aspect Term Extraction (Task T1). Given a review sentence,
this task deals with extracting all the possible aspect terms
with respect to the review domain (i.e. Book reviews in our
case) reviewed by the sentence. The extraction of aspects is
done regardless to their polarity. For instance, conflicting and
neutral aspect terms should be extracted as well.
Aspect Term Polarity (Task T2). Depending on previous
task (T1), this task focuses on assigning polarity class (pos-
itive, negative, conflict, and neutral) to extracted aspects The
conflicting case happens when both positive and negative
sentiment is expressed by the same aspect term or category
(e.g., “An interesting novel but a bit complicated”).
1http://alt.qcri.org/semeval2014/task4/
Aspect Category Identification (Task T3). Having a prede-
fined aspect categories and a collection of review sentences
(without any annotations), this task investigate the ability
of assigning one or more aspect categories to each review
sentence. The difference between this task and T1 is that
the aspect terms are more fine-grained and should appear in
the review sentence, whereas the aspect category is coarser
category of the sentence and do not appear in the review
sentence. Moreover, the aspect categories are not identified
using aspect terms in the sentence, but rather inferred using
sense words, adjectives, or context of the sentence meaning.
Aspect Category Polarity (Task T4). Having that the aspect
categories of the review sentences are given, this task investi-
gates the possibilities of assigning a specific polarity (positive,
negative, conflict, and neutral) to each aspect category.
B. Related Works
SA is viewed as a variation of the classical text cate-
gorization (TC) problem where the classes are the simply
the different polarities. This is not odd as many problems
are approached as TC problems such as spam filtering [13],
determining author’s characteristics such as identity [14], [15],
[16], [17], demographic and psychometric traits [18], gender
[19], [20], [21], dialect [22], native language [23], political
orientation [24], [25], etc.
The literature proposes two main approaches for TC:
corpus-based [26] and lexicon-based [27]. Works on SA basi-
cally followed these two approaches or a combination of them.
The first approach depends on collecting and annotating a huge
corpus for training and testing purposes. Then, a machine
learning classifier is trained on this corpus and constructs a
statistical model. This model is later applied on part of the
corpus (testing set) and if an acceptable accuracy is achieved,
the model is used for future classification. Many studies [28],
[29], [30], [31], [32], [33], [34], [35] employed the corpus-
based approach for English as well as other languages like
Arabic. Discussing this approach is beyond the scope of this
work.
On the other hand, the lexicon-based method necessitates
both a lexicon construction and a SA model design and im-
plementation. Numerous studies have addressed this approach
and discussed its challenges. In the following paragraphs, we
mention some of them.
Although the literature is full of studies to analyze English
sentiments, we emphasize the studies dedicated to the Arabic
language. One of the pioneering studies in Arabic SA was by
Ahmad et al. [36], who used a local grammar approach to
extract sentiments from Arabic and Chinese texts. In [37], the
authors extended [36] to address Arabic, English and Urdu.
Another study relied on a grammar approach was Farra et
al. [38], who also worked on a lexicon-based technique. It
is worth mentioning that grammar approaches need manual
exhaustive Part-Of-Speech (POS) tagging.
Some studies adopted lexicon-based approach using differ-
ent mechanisms such as Elhawary and Elfeky [39] who also
TABLE I
ASPECT TERMS DISTRIBUTION ACROSS DIFFERENT POLARITY CLASSES.
Dataset Positive Negative Conflicting Neutral Total
Train 1,252 855 26 126 2,259
Test 124 432 1 22 579
Overall Dataset 1,376 1,287 27 148 2,838
used MapReduce technique to improve the SA system per-
formance. Whilst, El-Halees [40] combined both the corpus-
based and lexicon-based approaches to enhance the accuracy
(attained 80% accuracy). Unlike [40], Itani et al. [41] con-
ducted a comparative study on both SA proposed approaches
(corpus-based and lexicon-based) where each approach is run
independently. The results showed that lexicon-based could
yield high accuracy (83.4%) since it is better equipped to han-
dle patterns (e.g., “not good”). Abdulla et al. [12] carried out
an analogous study to [41]. The main difference between them
was that they augmented intensification besides negation and
tackled dialects as well. Also, Elarnaoty et al. [42] attempted
to integrate three lexicon-based models to determine which
combination yields the best results. Finally, some works [43],
[44], [45] focused on the different techniques of constructing
the lexicons.
III. METHODOLOGY
An outline of the general methodology followed in this
paper is as follows. First, a training set is preprocessed to
extract lexicon terms for each label. Some of the construction
techniques under consideration require that each lexicon term
be assigned a weight. This is done within this step. The
built lexicons are then employed by our classification tool
to generate a decision about each term. After generating the
decisions, the quality of the classification tool is evaluated
using the Java tool provided by SemEval2014 organizers.
Hence, the outputs of our classification tool must be XML
files adhering to the SemEval2014 format. The evaluation tool
takes the XML files generated by our tool and compares them
with the gold standard. It generates accuracy measures suitable
for each task.
A. Dataset
The dataset we consider here is the human annotated Arabic
dataset (HAAD) [7]. HAAD is based on the large scale Arabic
book reviews (LABR) dataset originally prepared by Aly and
Atiya [46] in 2013. Although LABR is a very large dataset
containing more than 63,000 book reviews (mainly in MSA),
it lacks information relevant to ABSA work. LABR was
prepared for the purposes of review-based multi-way sentiment
analysis where each review is given a rating on a scale of 1-5.
With respect to the four tasks discussed in Section II-A, this
information is only relevant to the aspect category polarity task
(T4) where a review is considered: (i) positive if its overall
polarity rating is 4 or 5, (ii) negative if its overall polarity
rating is 1 or 2, and (iii) neutral if its overall polarity rating is
3. No further information is provided relevant to the remaining
TABLE II
CATEGORIES DISTRIBUTION ACROSS DIFFERENT POLARITY CLASSES.
Dataset Positive Negative Conflicting Neutral Total
Train 691 483 19 15 1,210
Test 32 268 1 2 303
Overall Dataset 723 751 20 17 1,513
tasks. Hence, a massive effort by human experts needed to be
invested if the LABR dataset is to be used for ABSA. This is
where HAAD comes in.
In [7], the authors described their effort to re-annotate the
LABR dataset for ABSA purposes. Stating that not all reviews
in the LABR dataset are useful for ABSA, the authors started
by selecting ABSA-compliant reviews in a group-based effort.
They formed seven groups each consisting of three graduate
students. Each group was responsible for selecting 400 diverse
reviews covering different books and ratings and annotate them
for ABSA purposes. The resulting dataset consisted of 2,389
reviews. These reviews were subjected to another filtering step
that left out 1,513 Arabic book reviews annotated for the four
tasks at hand.
The annotation process of HAAD was lengthy and tedious.
It involved training sessions that gradually honed the annota-
tors abilities. It also involved several validation steps including
internal validation (group-mates validating each others’ anno-
tations) and a final round of validation for the overall dataset.
Finally, the formatting scheme of SemEval2014 was followed
with its use of XML files and the BRAT annotation tool.2
Since our focus is on tasks T3 and T4, we limit our
discussion to the information related to these tasks and refer
the interested reader to [7] for more information related to
the other ABSA tasks. The annotated dataset of 1,513 reviews
contained 1,296 distinct aspect terms. The total number of
aspect terms occurrences was 2,838. Table I shows the aspect
terms distribution across different polarity classes [7]. As for
the aspect categories, there were 14 categories including Plot,
Feelings, Authors, Epilogue, etc. Table II shows the categories
distribution across different polarity classes [7].
B. Lexicons Construction and Evaluation
In this section, the proposed approach is discussed in depth.
We start by discussing the different lexicon construction tech-
niques before discussing the proposed lexicon-based algorithm
for determining the polarities and categories of aspect terms.
It is worth mentioning that we made use of the AraNLP tool
[47] for preprocessing purposes.
The first part of this study involves lexicons creation. The
literature suggests two main approaches for building a lexicon:
either manually or automatically based on a seed of words
or an annotated corpus [48], [49]. According to [50], [5],
automatic construction approaches suffer from many flaws
such as low accuracy and poor robustness. Nonetheless, they
do possess several appealing characteristics such as requiring
2http://brat.nlplab.org/
Fig. 1. The first approach for T4.
minimal human involvement which means that the effort, time
and cost invested in them can be significantly lower than
the manual approaches. Moreover, they are easier to maintain
update and even expand to cover new topics/fields. On the
other hand, manually created lexicon requires massive efforts
from human experts such as native speakers and linguists,
which means that their construction is slow and costly. Such
efforts are repeated every time the lexicons are expanded. In
this study, we explore several automatic lexicon construction
techniques for both tasks under consideration, determination
aspect categories and their polarities (Tasks T3 and T4). Our
discussion is divided across the following two subsection
where each subsection is dedicated to one of the two tasks
at hand. We start with Task 4 before going into Task T3.
1) Aspect Category Polarity Determination (Task T4): For
this task, we gradually enhance the lexicon construction over
three phases. Before discussing these phases, we present the
baseline experiments of [7] so that the reader would appreciate
the improvements our approaches have over the best known
results for the dataset at hand.
Baseline for Task T4 [7]. Each aspect category c in the test
sentence s is assigned the most frequent polarity label l that
have the d most similar training sentences to s. The Dice
coefficient similarity measure is used to compute the distance
between sentences s and d. If c is not seen in the training
sentences, then the most frequent aspect category polarity label
l in the whole training set is assigned to aspect category c. The
evaluation metric used for this task is the accuracy, which is
defined as the number of correctly predicted polarities divided
by the total number of aspect term polarity or aspect category
polarity annotations. The reported accuracy for T4 is 42.6%.
First Approach. The first lexicon construction approach is
a very simple one. It consists of a table with five columns;
one for the words/tokens and the remaining four are for fre-
quencies in positive, negative, neutral and conflicting reviews,
respectively. Each of these four columns can be viewed as a
separate lexicon by itself. However, they are stored in a single
file to facilitate the search procedure discussed next.
To determine the polarity of a certain review r, the review
is divided into words. Each word is looked up in the lexicon
to determine its frequencies in the positive, negative, neutral
and conflicting lexicons, respectively. The occurrences of all
words in r are summed up to produce a vector of four values
Fig. 2. The second approach for T4.
representing the total frequencies of all of r’s words in each of
the four lexicons under consideration. The decision is simply
made by taking the polarity corresponding to the maximum
value of this vector. Figure 1 shows a graphical depiction
of this approach. The obtained accuracy of this approach is
17.4%. This is a significantly lower accuracy than the reported
baseline experiments, and thus needs major improvement.
Second Approach. We start by giving an example to illustrate
the weakness of the first approach that motivated us to devise
this approach. Suppose that we have a negative review that
is negative and consist of five words. Suppose that the first
approach for the first four words was negative (the total
sentiment of the sentence so far) but the last word has a
very high frequency of occurrence in a positive sentiment. So,
adding this word’s frequencies to the total frequencies of the
sentence would changed the review’s sentiment from negative
(which is the correct choice) to positive. So, this approach
is devised to deal with cases where words with very high
frequencies dominate the computed sums leading to possibly
wrong decisions.
The second lexicon construction approach is an extension
of the first approach where four separate lexicons are created
and placed in different files each representing one of the four
polarities under consideration (positive, negative, neutral and
conflicting). The difference between this approach and the first
approach is that, in this approach, each word is placed in the
lexicon in which it has the highest occurrence frequency.
The test for this approach is conducted in a similar manner
to the first approach where the existence of a review word in
a certain lexicon of label l would increment the l’s counter in
four-element vector by one. The decision is simply made by
taking the polarity corresponding to the maximum value of this
vector. Figure 2 shows a graphical depiction of this approach.
The obtained accuracy of this approach is 29%. This a much
better result compared with the first approach; however, it is
much lower than the reported baseline experiments.
Third Approach (Dynamic Approach). By carefully inspect-
ing the cases that caused the poor performance of the first two
approaches, one can easily see that one of the major problems
with these two approaches is that the constructed lexicons are
limited and they have weights assigned to the words in the lexi-
con in a careful manner. Many of the lexicon-based approaches
for SA place so much emphasis on the comprehensiveness of
Fig. 3. The third (dynamic) approach for T4.
the constructed lexicon and the weights assigned to each word
in it. One example is SentiWordNet 3.0 [51], which is a well-
known tool with a comprehensive weighted lexicon of English
words.
This approach improves over the past two approaches in
two ways. First, it has a more carefully thought of weighting
scheme for the words in the constructed lexicons. Second, it
leverages the use of existing tools like Stanford Part-Of-Speech
(POS) tagger3 and SentiWordNet and its weighted lexicon
as follows. If a review word is not found in the constructed
lexicon, then it is translated to English using Bing Translator.4
The returned English terms along with their POS tags are then
fed into SentiWordNet and their polarity values are returned
to our decision making tool. To speed up future searches for
the same word, each time a word is translated and queried on
SentiWordNet, the word and its polarity values are added to
the constructed lexicon. Figure 3 shows a graphical depiction
of this approach. The obtained accuracy of this approach is
71%. This is a much better result compared with the reported
baseline experiments.
2) Aspect Category Determination (Task T3): The work on
the previous task benefitted from the rich literature on lexicon-
based SA as it made use of the existing sentiment lexicon of
SentiWordNet. Such an advantage does not exist for Task T3.
So, the room for improvement and creativity is wider for this
task. Like we did for Task T4, before discussing our work on
this task, we present the baseline experiments of [7].
Baseline for Task T3 [7]. The baseline for Task T3 is very
simple. For every test sentence s, the baseline retrievers the d
most similar sentences in the training set where the similarity
is computed in a similar to what is done for Task T4. The c
most frequent aspect category label of d is then assigned to
s. The evaluation metric used for this task is the F1 measure,
which is computed as follows.
F1 =
2× P ×R
P +R
,
where P and R are the precision and recall values, respec-
3http://nlp.stanford.edu/software/tagger.shtml
4https://www.bing.com/translator/
tively. They are computed as follows.
P =
|S ∩G|
|S|
, R =
|S ∩G|
|G|
,
where S is the set of aspect categories annotations out of the
test sentences in T3 and G is the set of the gold (correct)
aspect categories for the same test set. The reported accuracy
for T3 is 15.2%.
First Approach. The first approach for building category
lexicons is similar to the second approach of Task T4 discussed
in Section III-B1 (see Figure 2). The main difference is
that we here create 14 lexicons for each of the 14 aspect
categories under consideration. The decision is made in a
similar way except that resulting vector would now contain 14
values and the maximum would be selected to determine the
aspect category. The obtained F1 measure for this approach is
22.8%. This a better result compared with the reported baseline
experiments.
Second Approach. The result of the previous paragraph is
actually encouraging considering the fact that such a simple
approach failed to produce better results for Task T4. Nonethe-
less, we still look for further enhancements of this result.
Obviously, we cannot use existing lexicons since no such
lexicons exist for the aspect categories under consideration.
So, we resort to improving our lexicon construction approach
by employing better weighting techniques for the words in
the lexicons. Specifically, we follow the suggestion of Turney
[52], [53] and employ pointwise mutual information (PMI) as
a measure of similarity (synonymity) between different words.
The basic idea is that if we can have a set of “seed” words
for each lexicon, then we can compute the PMI values of each
non-seed word w in the training set with these seed words and
place w in the lexicon whose seed words produced the highest
PMI values with w. The seed words for lexicon c are selected
by simply extracting the words that appear only in the training
reviews of category c.
After computing the seed words for each lexicon. We
compute the PMI values of each non-seed word with each
seed word and find out that the range of the PMI values is
[0,10.04]. These values are normalized by dividing them by
10.04 to obtain weights in the range [0,1]. Note that seed
words are given a weight of 1 in the normalized lexicon.
Determining the aspect category of a review in the test set
is done in a way similar to what is done for the first approach
of Task T4 discussed in Section III-B1. Each word in the
review is looked up in the 14 lexicons and its weights are
summed to the weighted for the other words in the same
review. This gives a vector of 14 values and the algorithm
selects the aspect category whose corresponding value is the
highest in the vector. Note here that if a word w is not found
in the lexicons (i.e., w does not appear at all in the training
set), then the algorithm takes the word before w and the word
after w. It then computes the average PMI values of these
two words with all seed words. The obtained F1 measure for
this approach is 23.4%, which is slightly better than the first
approach.
IV. CONCLUSIONS AND FUTURE WORK
This work focused on lexicon-based approaches for ABSA
of Arabic reviews. Specifically, the addressed ABSA tasks are
aspect category determination (Task T3) and aspect category
polarity determination (Task T4). We presented our work on
the two main challenges faced with lexicon-based approaches:
lexicon construction and decision tool design. For Task T4,
this work explored the gradual enhancement of the lexicon
construction techniques and showed that the accuracy grows
to be much better than the best known result on the give dataset
(42.6% versus 71%). A similar approach was devised for Task
T3; however, the improvements over the best known result on
the given dataset are not as impressive as Task T4 (15.2%
versus 23.4%).
Future goals involve considering and comparing other au-
tomatic techniques for lexicon creation for both colloquial
Arabic and MSA. Improving the weighting techniques and
the decision making algorithm is among the most important
future directions of this work.
V. ACKNOWLEDGMENTS
This research is partially supported by Jordan University of
Science and Technology, Research Grant Number: 20150164.
REFERENCES
[1] I. Hmeidi et al., “A comparative study of automatic text categorization
methods using arabic text,” in ITMC, 2015.
[2] N. Y. Habash, “Introduction to arabic natural language processing,”
Synthesis Lectures on Human Language Technologies, vol. 3, no. 1,
pp. 1–187, 2010.
[3] H. Ta’amneh et al., “Compression-based arabic text classification,” in
ACS/IEEE AICCSA, 2014.
[4] M. Faqeeh et al., “Cross-lingual short-text document classification for
facebook comments,” in FiCloud. IEEE, 2014.
[5] B. Liu, “Sentiment analysis and opinion mining,” Synthesis Lectures on
Human Language Technologies, vol. 5, no. 1, pp. 1–167, 2012.
[6] B. Pang and L. Lee, “Opinion mining and sentiment analysis,” Founda-
tions and trends in information retrieval, vol. 2, pp. 1–135, 2008.
[7] M. Al-Smadi et al., “Human annotated arabic dataset of book reviews
for aspect based sentiment analysis,” in FiCloud. IEEE, 2015.
[8] ——, “Using aspect-based sentiment analysis to evaluate arabic news
affect on readers,” in IEEE/ACM UCC, 2015.
[9] A. Abbasi, H. Chen, and A. Salem, “Sentiment analysis in multiple
languages: Feature selection for opinion classification in web forums,”
ACM Transactions on Information Systems, vol. 26, no. 3, p. 12, 2008.
[10] M. Abdul-Mageed, M. T. Diab, and M. Korayem, “Subjectivity and
sentiment analysis of modern standard arabic.” in ACL, 2011.
[11] M. Abdul-Mageed and M. Diab, “Toward building a large-scale arabic
sentiment lexicon,” in GWC, 2012.
[12] N. Abdulla et al., “Arabic sentiment analysis: Lexicon-based and corpus-
based,” in AEECT. IEEE, 2013.
[13] C. C. Aggarwal and C. Zhai, “A survey of text classification algorithms,”
in Mining text data, 2012, pp. 163–222.
[14] P. Juola, “Authorship attribution,” Foundations and Trends in Informa-
tion Retrieval, vol. 1, no. 3, pp. 233–334, 2006.
[15] E. Stamatatos, “A survey of modern authorship attribution methods,”
JASIST, vol. 60, no. 3, pp. 538–556, 2009.
[16] A. Alwajeeh et al., “On authorship authentication of arabic articles,” in
ICICS. IEEE, 2014.
[17] J. Albadarneh et al., “Using big data analytics for authorship authenti-
cation of arabic tweets,” in IEEE/ACM UCC, 2015.
[18] D. Estival et al., “Tat: an author profiling tool with application to arabic
emails,” in ALTA Workshop, 2007.
[19] N. Cheng, R. Chandramouli, and K. Subbalakshmi, “Author gender
identification from text,” Digital Investigation, vol. 8, no. 1, 2011.
[20] K. Alsmearat et al., “An extensive study of the bag-of-words approach
for gender identification of arabic articles,” in IEEE/ACS AICCSA, 2014.
[21] ——, “Emotion analysis of arabic articles and its impact on identifying
the author’s gender,” in IEEE/ACS AICCSA, 2015.
[22] O. F. Zaidan and C. Callison-Burch, “Arabic dialect identification,”
Computational Linguistics, vol. 40, no. 1, pp. 171–202, 2013.
[23] J. Tetreault, J. Burstein, and C. Leacock, Eds., NAACL-HLT BEA8.
ACL, 2013. [Online]. Available: http://www.aclweb.org/anthology/
W13-17.pdf
[24] M. Koppel, N. Akiva, E. Alshech, and K. Bar, “Automatically classifying
documents by ideological and organizational affiliation,” in ISI, 2009.
[25] R. Abooraig et al., “On the automatic categorization of arabic articles
based on their political orientation,” in ICIEIS, 2014.
[26] I. Hmeidi et al., “Automatic arabic text categorization: A comprehensive
comparative study,” JIS, vol. 41, no. 1, pp. 114–124, 2015.
[27] N. Ahmed et al., “Scalable multi-label arabic text classification,” in
ICICS. IEEE, 2015.
[28] B. Pang, L. Lee, and S. Vaithyanathan, “Thumbs up?: sentiment classi-
fication using machine learning techniques,” in ACL, 2002.
[29] M. Rushdi-Saleh et al., “Bilingual experiments with an arabic-english
corpus for opinion mining,” in RANLP, 2011.
[30] ——, “Oca: Opinion corpus for arabic,” JASIST, vol. 62, no. 10, pp.
2045–2054, 2011.
[31] A. Shoukry and A. Rafea, “Sentence-level arabic sentiment analysis,”
in CTS, 2012.
[32] A. Mountassir et al., “An empirical study to address the problem of
unbalanced data sets in sentiment classification,” in SMC. IEEE, 2012.
[33] S. Ahmed, M. Pasquier, and G. Qadah, “Key issues in conducting
sentiment analysis on arabic social media text,” in IIT, 2013.
[34] N. Abdulla et al., “An extended analytical study of arabic sentiments,”
IJBDI, vol. 1, no. 1-2, pp. 103–113, 2014.
[35] B. Al Shboul et al., “Multi-way sentiment classification of arabic
reviews,” in ICICS. IEEE, 2015.
[36] K. Ahmad et al., “Multi-lingual sentiment analysis of financial news
streams,” in Proc. of the 1st Intl. Conf. on Grid in Finance, 2006.
[37] Y. Almas and K. Ahmad, “A note on extracting sentiments in financial
news in english, arabic & urdu,” in CAASL, 2007.
[38] N. Farra et al., “Sentence-level and document-level sentiment mining
for arabic texts,” in ICDMW. IEEE, 2010.
[39] M. Elhawary and M. Elfeky, “Mining arabic business reviews,” in
ICDMW. IEEE, 2010.
[40] A. El-Halees, “Arabic opinion mining using combined classification
approach,” in ACIT, 2011.
[41] M. M. Itani et al., “Classifying sentiment in arabic social networks:
Naive search versus naive bayes,” in ACTEA, 2012.
[42] M. Elarnaoty, S. AbdelRahman, and A. Fahmy, “A machine learning
approach for opinion holder extraction in arabic language,” arXiv, 2012.
[43] M. Al-Ayyoub, S. Bani Essa, and I. Alsmadi, “Lexicon-based sentiment
analysis of arabic tweets,” IJSNM, vol. 2, no. 2, pp. 101–114, 2015.
[44] N. Abdulla et al., “Towards improving the lexicon-based approach for
arabic sentiment analysis,” IJITWE, vol. 9, no. 3, pp. 55–71, 2014.
[45] ——, “Automatic lexicon construction for arabic sentiment analysis,” in
FiCloud. IEEE, 2014.
[46] M. A. Aly and A. F. Atiya, “Labr: A large scale arabic book reviews
dataset,” in ACL, 2013.
[47] M. Althobaiti, U. Kruschwitz, and M. Poesio, “Aranlp: A java-based
library for the processing of arabic text,” in LREC 2014, 2014.
[48] P. D. Turney, “Thumbs up or thumbs down?: semantic orientation
applied to unsupervised classification of reviews,” in ACL, 2002.
[49] P. D. Turney and M. L. Littman, “Measuring praise and criticism:
Inference of semantic orientation from association,” ACM Transactions
on Information Systems, vol. 21, no. 4, pp. 315–346, 2003.
[50] M. Taboada et al., “Lexicon-based methods for sentiment analysis,”
Computational linguistics, vol. 37, no. 2, pp. 267–307, 2011.
[51] S. Baccianella, A. Esuli, and F. Sebastiani, “Sentiwordnet 3.0: An
enhanced lexical resource for sentiment analysis and opinion mining.”
in LREC, vol. 10, 2010.
[52] P. Turney, “Mining the web for synonyms: Pmi-ir versus lsa on toefl,”
in ECML, 2001.
[53] P. D. Turney, P. Pantel et al., “From frequency to meaning: Vector space
models of semantics,” JAIR, vol. 37, no. 1, pp. 141–188, 2010.
