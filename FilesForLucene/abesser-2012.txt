This article was downloaded by: [University of Aegean]
On: 09 July 2012, At: 03:31
Publisher: Routledge
Informa Ltd Registered in England and Wales Registered Number: 1072954 Registered office: Mortimer House,
37-41 Mortimer Street, London W1T 3JH, UK
Journal of New Music Research
Publication details, including instructions for authors and subscription information:
http://www.tandfonline.com/loi/nnmr20
Classification of Music Genres Based on Repetitive
Basslines
Jakob Abeßer a , Hanna Lukashevich a & Paul Bräuer b
a Fraunhofer IDMT, Germany
b piranha womex AG, Germany
Version of record first published: 05 Jul 2012
To cite this article: Jakob Abeßer, Hanna Lukashevich & Paul Bräuer (2012): Classification of Music Genres Based on
Repetitive Basslines, Journal of New Music Research, DOI:10.1080/09298215.2011.641571
To link to this article:  http://dx.doi.org/10.1080/09298215.2011.641571
PLEASE SCROLL DOWN FOR ARTICLE
Full terms and conditions of use: http://www.tandfonline.com/page/terms-and-conditions
This article may be used for research, teaching, and private study purposes. Any substantial or systematic
reproduction, redistribution, reselling, loan, sub-licensing, systematic supply, or distribution in any form to
anyone is expressly forbidden.
The publisher does not give any warranty express or implied or make any representation that the contents
will be complete or accurate or up to date. The accuracy of any instructions, formulae, and drug doses should
be independently verified with primary sources. The publisher shall not be liable for any loss, actions, claims,
proceedings, demand, or costs or damages whatsoever or howsoever caused arising directly or indirectly in
connection with or arising out of the use of this material.
Classification of Music Genres Based on Repetitive Basslines
Jakob Abeßer1, Hanna Lukashevich1 and Paul Bräuer2
1Fraunhofer IDMT, Germany; 2piranha womex AG, Germany
Abstract
In this paper, we present a comparative study of three
different classification paradigms for genre classification
based on repetitive basslines. In spite of a large variety in
terms of instrumentation, a bass instrument can be found
in most music genres. Thus, the bass track can be
analysed to explore stylistic similarities between music
genres. We present an extensive set of transcription-
based high-level features related to rhythm and tonality
that allows one to characterize basslines on a symbolic
level. Traditional classification techniques based on
pattern recognition techniques and audio features are
compared with rule-based classification and classification
based on the similarity between basslines. We use a novel
dataset that consists of typical basslines of 13 music
genres from different cultural backgrounds for evalua-
tion purposes. Finally, the genre confusion results
obtained in the experiments are examined by musicolo-
gists. Our study shows that several known stylistic
relationships between music genres could be verified that
way by classifying typical basslines. We could achieve a
highest accuracy value of 64.8% for the genre classifica-
tion solely based on repetitive basslines of a song.
1. Introduction
In contrast to melody lines and harmonic progressions,
basslines have rarely been investigated in Music Infor-
mation Retrieval (MIR). The bass track plays an
important role in music genres of different historical
epochs from Western classical Baroque music to
contemporary genres such as heavy metal, drum and
bass, or rhythm’n’blues, as well as genres from various
regional traditions from Western European, American,
and African countries. From military marching bands to
modern dance music, the sonic qualities of basslines very
often play a key role in social affiliation and hence in the
perception and discrimination of musical styles (Wicke,
2007). Typical bass playing styles have evolved in a lot of
genres.
Bass frequencies provide fundamental orientation in
auditory experience due to the physics of sound and the
human biology. They resonate strongly in the listeners
body and head and are subconsciously perceived as
powerful. In music, these qualities of bass frequencies are
very often used to give a basic structure such as
orientation in a complex composition or performance
(Bruhn, Oerter, & Rösing, 2005). The bassline carries the
main rhythmic and structural information of a song and
provides insight into its underlying harmonic progres-
sion. Hence, the automatic classification of extracted
basslines allows for describing a music piece in terms of
harmony, rhythm, and style.
In this paper, we investigate 520 basslines that
represent 13 different music genres from various historical
and regional backgrounds (see Table 5, Section 5.1). We
perform a comparative study of three different classifica-
tion approaches to automatically classify the genre of a
song solely based on tonal and rhythmic properties of the
repetitive bassline in various evaluation experiments.
This paper is organized as follows. After providing an
overview of related work in Section 2, we illustrate
different symbolic audio features for the tonal and
rhythmic description of a bassline in Section 3. Then,
we explain the three different classification approaches
we compare in this publication in Section 4. In Section 5,
we introduce the applied data set, describe the evaluation
experiments that we performed, and discuss their results.
Finally, we discuss the results, conclude our work in
Section 6, and give an outlook in Section 7.
Correspondence: Jakob Abeßer, Fraunhofer IDMT, Metadata, Ehrenbergstrasse 31, Ilmenau, 98693 Germany.
E-mail: abr@idmt.fraunhofer.de
Journal of New Music Research
2012, iFirst article, pp. 1–19
http://dx.doi.org/10.1080/09298215.2011.641571  2012 Fraunhofer IDMT and Paul Bräuer
The work of Jakob Abeßer and Hanna Lukashevich was authored as part of their employment, and copyright in their work is hereby
asserted by Fraunhofer IDMT.
The work of Paul Bräuer was authored in his own right and copyright is asserted by him.
D
ow
nl
oa
de
d 
by
 [
U
ni
ve
rs
ity
 o
f 
A
eg
ea
n]
 a
t 0
3:
31
 0
9 
Ju
ly
 2
01
2 
2. Related work
In this section, we first provide an overview of music
score representation. Then, we present a short summary
of extraction methods for score-based audio features and
classification techniques that were previously applied in
related research fields.
2.1 Score representations and high-level features
In a lot of music styles treated in this publication,
compositions are commonly not written out at all. On
the other hand, some of its musical predecessors are
score-based, especially regarding the basslines. There-
fore, it does make sense to look at basslines in terms of a
score. The score parameters, note pitch, note onset time,
note duration and note loudness, are commonly applied
to provide a simple yet meaningful symbolic representa-
tion of each note event. While a lot of information is
necessarily left out in any form of transcription, these
four parameters provide a scalable and comprehensive
representation of the original bassline.
Basic score parameters can be extracted from the audio
data via music transcription (Hainsworth & Macleod,
2001; Goto, 2004; Dittmar, Dressler, & Rosenbauer,
2007; Ryynänen & Klapuri, 2008; Tsunoo, Ono, &
Sagayama, 2009) or directly from symbolic audio data.
We refer to audio features derived from score para-
meters as high-level features. In general, these features can
capture different properties of music related to melody,
harmony, and rhythm (McKay & Fujinaga, 2004, 2006; de
Léon & Iñesta, 2007; Abeßer, Lukashevich, Dittmar, &
Schuller, 2009; Abeßer, Lukashevich, Bräuer, & Schuller,
2010a). Most of these features correspond to the terminol-
ogy of music theory. Usually, different statistical methods
were used to derive high-level features from note onsets,
pitches, and intervals (Pérez-Sancho, de León, & Iñesta,
2006; de Léon & Iñesta, 2007). For instance, high-level
features based on entropy, compression, and prediction
were presented inMadsen andWidmer (2007).Rhythmical
aspects like swing or syncopation were investigated in
various publications as for instance in Friberg and
Sundström (2002), Gouyon (2005) and Flanagan (2008).
Since inaccurate transcription results directly impair
the accuracy of these high-level features, symbolic audio
formats such as MIDI or Humdrum are preferably used
for feature extraction. In contrast to real audio files,
symbolic audio formats offer a direct access to the
relevant score parameters. In the MIDI standard, each
note is considered as a single note event that is
characterized by its absolute pitch, its onset time, its
offset time and its loudness. We use the MIDI Toolbox
for Matlab (Eerola & Toiviainen, 2004) in this work to
extract the basic score parameters fromMIDI files. In this
publication we do not analyse real audio data but manual
transcriptions of audio recordings. Our goal is to
investigate to what extent transcription-based audio
features derived from the bass track are generally
applicable for music genre classification.
So far, only a few publications focussed solely on the
description of the bass track. Tsuchihashi, Kitahara, and
Katayose (2008) confined themselves to the progression
of the note pitch values and distinguished between
features characterizing the pitch variability and the pitch
motion. In general, high-level features extracted from
different instruments were successfully applied for genre
classification tasks (de Léon & Iñesta, 2007; Abeßer,
Dittmar, & Großmann, 2008; Tsuchihashi et al., 2008).
Many publications approached related aspects of
expressive music performance. They aimed at characteriz-
ing the performance of a musician in terms of rhythmic
and tonal play. For example, Stamatatos and Widmer
(2005) used features derived from the onset values, inter-
onset-interval and loudness values of note progression to
quantify the performance style of piano players in terms
of their timing, articulation and dynamics. An excellent
overview of existing approaches for expressive perfor-
mance analysis was provided in Widmer, Dixon, Goebl,
Pampalk, and Tobudic (2003) and Widmer and Goebl
(2004).
2.2 Classification approaches
2.2.1 Classification based on pattern similarity
The computation of similarity between different melodies
is useful for both music retrieval and analysis. The most
common application of music similarity is query by
humming (QbH) (Unal, Chew, Georgiou, & Narayanan,
2008), which allows one to identify a melody that was
hummed without or with minor tonal or rhythmic errors
before. Usually, each note of a melody is represented by
its absolute pitch and onset value in order to represent a
melody as a character sequence. Therefore, different
distance measures are applied to measure the similarity
between melodies. Commonly applied techniques are n-
grams, the edit distance (Levenshtein distance) (Müllen-
siefen & Frieler, 2004), the EarthMovers Dicance (EMD)
and the derived Proportional Transportation Distance
(PTD) (Typke, Giannopoulos, Veltkamp, Wiering, & van
Oostrum, 2002). Other similarity measures were derived
from the perception-based Implication/Realization (I/R)
model (Grachten, Arcos, & de Mantaras, 2004) or from a
graph-based representation of musical structure (Orio &
Rodá, 2009).
2.2.2 Classification based on statistical pattern recognition
Statistical pattern recognition methods were widely
applied for classification of musical styles or genres based
on the extracted acoustical features. In order to classify
the musical content based on bass-related features the
2 Jakob Abeßer et al.
D
ow
nl
oa
de
d 
by
 [
U
ni
ve
rs
ity
 o
f 
A
eg
ea
n]
 a
t 0
3:
31
 0
9 
Ju
ly
 2
01
2 
following techniques were used. The Mahalanobis dis-
tance of the feature vector to the feature distribution of
each genre was utilized by Tsuchihashi et al. (2008). The
genre minimizing the Mahalanobis distance was chosen
as the classification result. Support Vector Machines
(SVM), Gaussian Mixture Models (GMM), Naive Bayes
(NB) and k-Nearest Neighbour (kNN) classifiers were
utilized in Abeßer et al. (2009) in a combination with
various feature selection and feature space transforma-
tion methods. Audio genre classification based on bass-
line patterns in Tsunoo et al. (2009) was performed by
means of a SVM classifier. Bass playing style detection in
Abeßer et al. (2010a) was done by applying the SVM
classifier with a preceding feature selection algorithm.
Related work regarding general (not only bass-
related) high-level features accounts for the following
techniques. Automatic genre classification using large
high-level musical feature sets in McKay and Fujinaga
(2004) was performed via a novel ensemble of feedfor-
ward neural networks and kNN classifiers. The features
were selected with the aid of genetic algorithms. Decision
trees, decision stumps and kNN were utilized in Gouyon
(2005) to classify audio genres based on high-level
rhythmical features. A broad palette of classification
paradigms including kNN, NB, multilayer perceptron
(MLP) and SVM were used for symbolic genre recogni-
tion in Pérez-Sancho et al. (2006). Fully supervised
methods NB and kNN and unsupervised Self-Organizing
Maps (SOM) were applied to statistical descriptors in de
Léon and Iñesta (2007) for music style identification.
2.2.3 Rule-based classification and expert systems
In contrast to abstract genre models, which are trained
using supervised classifiers such as of SVM, the modelling
of different genres or styles using a list of rules is more
intuitive and comprehensible for humans (Abeßer, Luka-
shevich,Dittmar, Bräuer,&Krause, 2010b; deLeón,Rizo,
& Iñesta, 2007). These rules correspond to a set of distinct
properties of a music genre. Each rule can be expressed
using a simple feature-value relation. The automatic
learning of these rules was previously presented for
harmony progressions (Anglade, Ramirez, & Dixon,
2009) and melody characterization (de León et al., 2007),
but also for the purpose of automatic music generation
(Bresin, 2001; Buzzanca, 2001). Bresin (2001) investigated
the influence of different articulation rules for the
improvement of automatically generated piano perfor-
mances.
3. Proposed method—feature extraction
3.1 Score parameters
As mentioned in the previous chapter, we focus on the
processing of symbolic audio data in this publication.
The absolute pitch hP, the note onset time uo, and the
note duration uD provide a description of the tonal and
the rhythmic context of each note. The note duration is
derived by subtracting the onset time from the offset time
for each note. Both the note onset and duration are
commonly measured in seconds. The absolute pitch is
assigned to an integer value between 0 and 127 according
to the MIDI standard. In this work, we do not use the
note loudness to derive features.
Table 1 provides an overview of all score parameters
used in this work as well as their notation, dimension-
ality, and value range. Some of the score parameters have
discrete values, some have continuous values. All score
parameters are stored as vectors, which we denote in
bold print. In the following subsections, we explain
further score parameters related to rhythm and tonality
that are computed before features are extracted. An
example that contains an excerpt of a funk bassline and
the derived score parameters is given in Table 2.
3.1.1 Score parameters (tonality)
We assume that a bassline consists of N notes and has a
length of M measures. The chromatic representation hP;12
of the absolute pitch hP is derived as
yP;12;k ¼ yP;k mod 12; ð1Þ
hP;12 2 ZN:
Table 1. Score parameters.
Score parameter Notation
Vector
dimension
Value
range
Tonality
Absolute pitch hP Z
N [0, 127]
(Chroma) hP,12 Z
N [0, 11]
Relative pitch DhP Z
N71 [7127, 127]
(Mapped to one
octave)
DhP,12 Z
N71 [711, 11]
Interval direction DhP,D Z
N71 {71, 0, 1}
Functional intervals DhP,F Z
N71 {77, . . . ,
72, 1, . . . ,7}
Rhythm
Onset (in seconds) j½sO
RN [0,?)
Onset (in measures) j½mO
RN [0,?)
Simplified onset j½mO;1
RN [0, 1)
Duration (in seconds) j½sD R
N (0,?)
Duration (in
measures)
j½mD R
N (0,?)
Inter-onset interval
(in measures)
Du½mO
RN71 (0,?)
Note-duration ratio jDR R
N71 (0,?)
Classification of music genres based on repetitive basslines 3
D
ow
nl
oa
de
d 
by
 [
U
ni
ve
rs
ity
 o
f 
A
eg
ea
n]
 a
t 0
3:
31
 0
9 
Ju
ly
 2
01
2 
The chromatic values represent all absolute pitch values
mapped to one octave (yP,12,k 2 [0, 11]). By definition,
yP,12,k¼ 0 corresponds to the note name C.
The relative pitch DhP describes the interval size
between two consecutive notes as
DyP;k ¼ yP;kþ1  yP;k; ð2Þ
DhP 2 ZN1:
We use two additional score parameters: DhP;D repre-
sents the interval direction and DhP;F provides a functional
interval representation. The interval direction DyP,D,k
between two notes is 1 if the interval between both notes
is ascending, 71 if the interval is descending, or 0 in case
both notes have the same absolute pitch value. It is
computed as
DyP;D;k ¼ sgn DyP;k; ð3Þ
DhP;D 2 ZN1; DyP;D;k 2 1; 0; 1f g:
The functional interval representation DhP;F is set
according to the musical interval name that corresponds
to each interval. Therefore, we map each interval to a
range between one octave downwards and one octave
upwards as
D̂yP;k ¼ DyP;D;k DyP;k
 mod 12 : ð4Þ
The assignment between D̂yP;k and the function interval
representation DyP,F,k is illustrated in Table 3.
3.1.2 Score parameters (rhythm)
The onset times and durations of note events can be
expressed in both a physical and a musical time
representation. We use the superscript indices s and m
to indicate that both the onset uo and the duration uD of
a note can be measured in seconds as well as in fractions
of measure lengths. Thus, u
½m
O and u
½m
D provide a tempo-
independent representation of the note onset times and
note lengths. This allows to compute rhythmic features
independent of the large range of tempo values in
different music genres we investigate in this publication
(see Figure 7, see Section 5.1). Both u
½m
O and u
½m
D can be
extracted from the MIDI files using the MIDI toolbox
(see Eerola & Toiviainen, 2004).
Table 2. Score parameters for an excerpt of a funk bassline.
Note number k 1 2 3 4 5 6 7 8 9 10 11 12
Note name C3 G3 A3 A#3 B2 C3 G3 A#3 A3 G3 A#2 B2
Absolute pitch hP 48 55 57 58 47 48 55 58 57 55 46 47
(Chroma) hP,12 0 7 9 10 11 0 7 10 9 7 10 11
Relative pitch DhP 7 2 1 711 1 7 3 71 72 79 1 –
(Mapped to octave) DhP,12 7 2 1 711 1 7 3 71 72 79 1 –
(Interval direction) DhP,D 1 1 1 71 1 1 1 71 71 71 1 –
(Functional intervals) DhP,F 5 2 2 77 2 5 3 72 72 76 2 –
Onset j½mO
0 1
4
3
8
7
16
7
8
1 1 14 1
7
16 1
10
16 1
3
4
1 78 1
15
16
Simplified onset j½mO;1
0 1
4
3
8
7
16
7
8
0 1
4
7
16
10
16
3
4
7
8
15
16
Duration j½mD
1
8
1
16
1
16
1
16
1
8
1
8
3
32
3
16
1
8
1
16
1
16
1
16
Inter-Onset-Interval Du½mO
1
4
1
8
1
16
7
16
1
8
1
4
3
16
3
16
1
8
1
8
1
16
–
Note-duration-ratio jDR 12 1 1 2 1
3
4
2 2
3
1
2
1 1 –
Table 3. Functional representation of intervals.
D̂yP;k DyP,F,k Interval name
711, 710 77 Descending seventh
79, 78 76 Descending sixth
77, 76 75 Descending fifth
75 74 Descending fourth
74, 73 73 Descending third
72, 71 72 Descending second
0 1 Prime
1,2 2 Ascending second
3,4 3 Ascending third
5 4 Ascending fourth
6,7 5 Ascending fifth
8,9 6 Ascending sixth
10,11 7 Ascending seventh
4 Jakob Abeßer et al.
D
ow
nl
oa
de
d 
by
 [
U
ni
ve
rs
ity
 o
f 
A
eg
ea
n]
 a
t 0
3:
31
 0
9 
Ju
ly
 2
01
2 
The note onset values are converted into a simplified
onset representation u
½m
O;1 by neglecting the current
measure number as
j½mO;1;k ¼ j
½m
O;k mod 1; ð5Þ
u
½m
O;1 2 R
N:
The inter-onset interval (IOI) Du½mO is derived from the
onset difference between two consecutive notes, it is
defined by
Dj½mO;k ¼ j
½m
O;kþ1  j
½m
O;k; ð6Þ
Du½mO 2 R
N1:
The note-duration ratio (NDR) uDR is computed as
the ratio between the duration values of a consecutive
note as
jDR;k ¼
j½mD;kþ1
j½mD;k
; ð7Þ
uDR 2 RN1:
3.2 Feature extraction
In the following subsections, we explain the computation
of various high-level features based on the previously
extracted score parameters. Features are denoted as a
with different subscripts.
Furthermore, we define two auxiliary functions n(x, v)
and p(x, v). Consider x to be a vector that represents an
arbitrary score parameter and v to be a vector with all
different values taken by the elements xi. Then ni denotes
the number of elements in x with xj¼ vi. Each element pi
is the (relative) frequency of its corresponding value vi in
the vector x:
pi ¼
ni
jxj jj
: ð8Þ
3.2.1 Tonality
3.2.1.1 Basic tonal features. To capture the overall tonal
range of a bassline, we compute feature as
aTR ¼ maxkyP;k minkyP;k; ð9Þ
k 2 1;N½ :
By computing the frequencies of occurrence for absolute
pitch values in hP, we detect the most frequent pitch value
yP,dom and the number of notes n(hP, yP,dom) having this
pitch value. If multiple pitch values appear equally
often, we take the lowest of these pitch values as yP,dom.
We use
adom ¼ p hP; yP;dom
 
ð10Þ
as a feature to measure the dominance of the most fre-
quent pitch value.
Some basslines are based on a so-called pedal-tone.
This term is used if the dominant pitch value is the lowest
pitch value in a pattern. Furthermore, this pitch value is
constantly repeated with only a few variations into
higher pitches. We compute a feature as
aPedal ¼
yP;dom  ymin
ymax  ymin
: ð11Þ
The use of a pedal-tone is most likely if the value of apedal
is very small. We do not take the duration into account
here although some bass notes might be held and thus act
as a pedal-tone without being repeated.
3.2.1.2 Chroma-based features. Based on the chromatic
pitch representation hP;12, we compute the frequencies of
occurrence p(hP;12, c) with
c 2 Z12; ci 2 0; 11½  ð12Þ
containing all possible chroma values between 0 and 11.
We compute the zero-order entropy H0 over p(hP,12, c)
as a tonal feature to measure whether only a few or a lot
of chromatic values are present in a bassline. The number
of different chromatic pitch values in a bassline can
provide evidence about its tonal complexity.
3.2.1.3 Scale. The choice of tonal scales is a distinctive
feature of different music genres. A scale is a set of pitch
values that are related by a unique interval structure.
Usually, these pitch values provide the melodic material
for a musical composition. We compute a measure-of-fit
between the analysed bassline and 10 different scales
listed in Table 4 to detect the most likely scale, which a
Table 4. Investigated scales.
Scale index s Scale name Scale template ts
0 Natural minor 1 0 1 1 0 1 0 1 1 0 1 0
1 Harmonic minor 1 0 1 1 0 1 0 1 1 0 0 1
2 Melodic minor 1 0 1 1 0 1 0 1 0 1 0 1
3 Pentatonic minor 1 0 0 1 0 1 0 1 0 0 1 0
4 Blues minor 1 0 0 1 0 1 1 1 0 0 1 1
5 Whole tone 1 0 1 0 1 0 1 0 1 0 1 0
6 Whole tone half tone 1 0 1 1 0 1 1 0 1 1 0 1
7 Arabian 1 1 0 0 1 1 0 1 1 0 0 1
8 Minor gypsy 1 0 1 1 0 0 1 1 1 0 1 0
9 Hungarian gypsy 1 0 1 1 0 0 1 1 1 0 0 1
Classification of music genres based on repetitive basslines 5
D
ow
nl
oa
de
d 
by
 [
U
ni
ve
rs
ity
 o
f 
A
eg
ea
n]
 a
t 0
3:
31
 0
9 
Ju
ly
 2
01
2 
bassline is based on. In this work, we do not take the
local information on whether a bassline is ascending or
descending in pitch into account to determine the
prominent scale.
In this work, each scale is represented by a scale
template ts 2 R12. We use a simplified representation
ts,i 2 {0, 1} to clearly discern between outside-scale tones
(ts,i¼ 0) and inside-scale tones (ts,i¼ 1). The first value ts,1
represents the lowest note of a scale, which is referred to
as its root note. The vector ts can be rotated by r
semitones using a cyclic shift operation to derive ts,r with
r 2 [0, 11]. This operation does not affect the interval
structure of a scale, it only changes the chromatic value
of its root-note.
To compute a measure-of-fit for a scale s, we use a
template-matching approach. First, we compute the
frequencies of occurrence P(hP,12, c) of all chromatic
values c 2 [0, 11] in a bassline. For each scale index s and
each rotation r 2 [0,11], we compute
gs;r ¼
P
ts;r  p hP;12; c
  P
ts;r
ð13Þ
to measure the likelihood of scale s rotated by r
semitones. * denotes the element-wise product between
two vectors. The summation is performed over all vector
elements in each case. Finally, we compute the maximum
ratio
aSC;s ¼ max
8s;r
gs;r ð14Þ
over all rotations r 2 [0, 11] to measure the likelihood of
each scale s 2 [0, 9] for a given bassline and use all aSC,s
as features.
3.2.1.4 Interval types and progression. To characterize
the interval progression, we use the frequencies of
different typical note sequences in a bassline as features.
Hence, we seek for sequences of consecutive notes with
constant pitch (DyP,K¼ 0) as well as chromatic transitions
between notes and measures. A chromatic transition
between notes is characterized by two equal consecutive
intervals of plus or minus one semitone:
DyP;k ¼ DyP;kþ1; DyP;k 2 1; 1f g; ð15Þ
k 2 1;N 1½ :
If the pitch difference between the last note of a measure
and the first note of the consecutive measure is one
semitone, we call the transition between both measures
chromatic. In music genres related to Jazz, the walking
bass playing style is often used. Here, chromatic transi-
tions between measures can be found in the bassline if the
last note in a measure is the leading note towards the first
note of the next measure. For both types of transitions,
we take the ratio between the number of note sequences
and the overall number of notes as features.
The mean and the variance over the absolute interval
size jDhPj are computed to characterize whether a
bassline comprises mainly small or large intervals.
Furthermore, we compute the number of intervals with
ascending pitch n(DhP,D,1) and the number of intervals
with descending pitch n(DhP,D,71). The feature
aDD ¼
n DhP;D; 1
 
n DhP;D; 1
 
þ n DhP;D;1
  ð16Þ
measures whether an ascending or a descending interval
direction is dominant in the bassline. In the case
(DhP,D,1)þ n(DhP,D,71)¼ 0, aDD is set to 0.
A bassline is perceived as smooth and fluent if con-
secutive intervals often have the same direction. There-
fore, we compute the frequencies of consecutive note
transitions with the interval direction value DyP,D,k as a
measure of constant direction aOD.
aCD ¼
1
N 2
XN2
k¼1
bk; ð17Þ
bk ¼
1 for DyP;D;k ¼ DyP;D;kþ1
0 otherwise:

ð18Þ
To measure the variety of applied interval types, we
compute the frequencies of each possible value of DhPF
(see Table 1).
As additional features, we compute the zero-order
entropy H(DhPF) over the vector containing the func-
tional pitch representation DhPF, the mean and the
variance of the absolute size of all intervals jDyP,kj as well
as the absolute number of different intervals appearing in
a bassline.
3.2.2 Rhythm
3.2.2.1 Rhythmic subdivisions. In this work, we investi-
gate different rhythmic subdivisions of basslines. There-
fore, each measure is divided into Q equidistant
subdivisions with the indices b 2 [1,Q]. The value of Q
relates to the corresponding musical note length values
such as crotchets (Q¼ 4) or eight-notes (Q¼ 8). Each
beat is associated with a musical onset time
j½mQ;b ¼ b=Q; b 2 1;Q½ : ð19Þ
For each subdivision Q, each note k of a bassline can be
quantized to an adjacent beat with the index b̂k as
b̂k ¼ arg min
b2 1;Q½ 
j½mO;1;k  j
½m
Q;b
 : ð20Þ
6 Jakob Abeßer et al.
D
ow
nl
oa
de
d 
by
 [
U
ni
ve
rs
ity
 o
f 
A
eg
ea
n]
 a
t 0
3:
31
 0
9 
Ju
ly
 2
01
2 
If the note is exactly located between two beats, it is
quantized to the earlier beat. All beats with odd indices
bOn;i ¼ 2i  1; i > 0
are referred to as on-beats, all others as off-beats.
3.2.2.2 On-beat accentuation. Since on-beats are usually
emphasized in most Western music genres, we aim to
measure the accentuation of on-beats in a given
bassline. We compute the relative frequency of notes
nOn,Q,m that are quantized to on-beats of a given
subdivision Q as
aOn;Q;m ¼
nOn;Q;m
N
ð21Þ
is computed for all measures.
We use the mean and the variance over all aOn,Q,m
with m 2 [1, M] as rhythmic features. Both features are
calculated for the subdivisions Q 2 {2, 4, 8, 16, 32}.
These features for instance allow one to distinguish
between walking bass basslines in swing and basslines in
reggae. Walking basslines mainly comprise of crotches
that correspond to on-beat notes for Q¼ 8. The frequent
use of crotchets result in high mean values over aOn,Q,m.
In contrast, reggae basslines often contain accentuations
shifted from on-beats to following off-beats and thus lead
to lower mean values over aOn,Q,m.
3.2.2.3 Dominant rhythmic feeling. Binary and ternary
rhythms are dominating in the music genres investigated
in this publication. These rhythms are based on rhythmic
subdivisions Q that are multiples of 2 or 3. We derive
features to capture the dominant rhythmic feeling of the
notes in a given bassline. Therefore, we investigate all
rhythmic subdivisions in the matrix
QF ¼
2 4 8 16 32 64
3 6 12 24 48 96
 
:
The binary subdivisions are placed in the first row and
the corresponding ternary subdivision in the second row
of QF.
For each rhythmic subdivision QF,i,j, we compute a
simplified onset representation similar to the approach
previously shown in Equation 6. In contrast, we do not
use a length of one measure as reference but the inter-
onset-intervals between two adjacent beats for a subdivi-
sion QF,i,j as
j½mO;Q;k ¼ j
½m
O;k mod
1
QF;i;j
; ð22Þ
k 2 1;N½ ;
The values j½mO;Q;k correspond to the onset distances
between the kth note and its adjacent preceding beat for
a given subdivision QF,i,j.
To evaluate if a note has an onset close to one of the
beats of a given subdivision, we have to consider that it
can be located either closely before or after a beat.
Therefore we modify u
½m
O;Q as
d
j½mO;Q;k ¼ min j
½m
O;Q;k;
1
QF;i;j
 j½mO;Q;k
 
ð23Þ
to obtain a two-side distance measure.
To detect the dominant rhythmic feeling, we investi-
gate the percentage of notes that can be associated to
each of the subdivisions in QF. Thus, we use the distance
criterion
d
j½mO;Q;k  0:05
1
QF;i;j
ð24Þ
to decide if the the kth note is mapped to the subdivision
QF,i,j. For each subdivision QF,i,j, we store the percentage
of notes assigned to this subdivision in a matrix
cF 2 R26:
The value cF,i,j can be thought of as a measure-of-fit
for the subdivision to characterize a bassline. To inves-
tigate whether binary or ternary subdivisions are
dominant, we compare the values in CF column-wise.
We compute the number of binary subdivisions nB that
have a higher value cF,i,j than their corresponding ternary
subdivision as
nB ¼
X6
j¼1
gj; gj ¼
1 for cF;1;j  cF;2;j;
0 otherwise;

ð25Þ
and the number of dominant ternary subdivisions nB for
cF,1,j  cF,2,j accordingly. Finally, we obtain two features
aBS ¼
nB
nB þ nT
ð26Þ
and
aTS ¼
nT
nB þ nT
ð27Þ
to measure the dominance of binary and ternary sub-
divisions in a given bassline.
3.2.2.4 Dominant rhythmic subdivision. As described in
the previous subsection, we can investigate the percen-
tage of notes in a bassline that are associated with
different rhythmic subdivisions. We define the dominant
Classification of music genres based on repetitive basslines 7
D
ow
nl
oa
de
d 
by
 [
U
ni
ve
rs
ity
 o
f 
A
eg
ea
n]
 a
t 0
3:
31
 0
9 
Ju
ly
 2
01
2 
rhythmic subdivision as the subdivision to which at least
80% of all notes are assigned to. Therefore, we seek the
lowest indices ı̂ and ĵ that fulfill cF,i,j 0.8. We store the
dominant rhythmic subdivision
aRS ¼ QF;î;k̂ ð28Þ
as a feature. In the case no value in cF is bigger than 0.8,
we use
aRS ¼
QF;1;6 for nB  nT;
QF;2;6 otherwise:

ð29Þ
3.2.2.5 Note density. In order to describe the note
density distribution in a particular bassline, we take the
mean and variance of the number of notes per measure
over all measures as features.
3.2.2.6 Syncopation. Syncopation is frequently used
especially in Latin American music genres such as bossa
nova or salsa. Instead of playing notes on on-beat
positions, the accentuation is often shifted to the
adjacent off-beat positions. To detect syncopated note
sequences within a bassline, we again investigate multiple
temporal subdivisions Q 2 {4, 8, 16, 32}. For each sub-
division Q, we map all notes inside a measure to the
closest beat inside this measure as described in Equation
20. If at least one note is mapped to a particular beat, the
beat is associated with the value 1, otherwise with 0.
In Figure 1, we illustrate four examples of syncopated
note sequences for an eight-note subdivisions (Q¼ 8).
Each sequence of notes corresponds to a sequence of
alternating on-beat and off-beat accentuations that
musicologist refer to as syncopations. After quantizing
the notes to a subdivision Q, we compute the number of
the beat sequences—[1 0 0 1], [0 1 1 0], [1 1 0 1], and
[0 1 1 1], in a bassline. The ratio between the number of
syncopation sequences and the overall number of beats
per measure is computed as features.
3.2.2.7 Sub-pattern similarity. The basslines used in this
work have different lengths between 4 and 16 measures.
They often contain one ore two measures that are
repeated within the complete pattern with only slight
variation. These sub-patterns can be characteristic for a
bassline. To compute features, that capture whether sub-
patterns of a bassline are similar to each other, we
proceed as follows.
First, if necessary, the pattern is elongated to a length
of 16 measures by simply repeatedly appending the
complete pattern to itself. This approach is valid since
we assume that the basslines appear as repeating patterns
in real songs. Then, we compute the similarity between
consecutive sub-patterns of a length of NSP measures
within the bassline. For this feature, we use the
Levensthein distance, which will be explained in Section
4.3.1 based on the onset values u
½m
O as a distance measure.
Finally, all similarity values are averaged to compute the
feature aSub,N for the resolutions NSP 2 {2, 4, 8}. Figure 2
illustrates this procedure for a pattern of a length of l¼ 7
measures and NSP¼ 4 measures.
3.2.2.8 Basic feature related to the note duration. In
addition to the features derived from the note onset
values, we obtain various characteristics regarding the
note duration vector u
½m
D and the inter-onset interval
vector Du½mO . We calculate the mean and the variance
over u
½m
D as basic statistical features. To describe the
variety in terms of note lengths and distances, we derive
the number of different values in u
½m
D and Du
½m
O .
Furthermore, we compute the frequencies of occur-
rence pðu½mD ;
d
u
½m
D Þ of 15 different note duration values
d
j½mD;i ¼
a
b
; a 2 1; 2
3
;
3
2
 
; b 2 2; 4; 8; 16; 32f g ð30Þ
and use all pi as features. The values of a represent the
regular note (a¼ 1), the triplet note (a¼ 2/3), and the
dotted note (a¼ 3/2) and the values of b correspond to
Fig. 1. Note sequences with and without syncopation for an
quaver subdivision (Q¼ 8) and corresponding beat sequence
shown above.
Fig. 2. Computation of the sub-pattern similarity for a pattern
of a length of l¼ 7 measures and a sub-pattern length of
NSP¼ 4 measures. First, the pattern is elongated to a length of
16 measures by repeatedly appending the pattern to itself. Then,
the similarity between adjacent sub-patterns is computed and
averaged to compute the sub-pattern similarity values.
8 Jakob Abeßer et al.
D
ow
nl
oa
de
d 
by
 [
U
ni
ve
rs
ity
 o
f 
A
eg
ea
n]
 a
t 0
3:
31
 0
9 
Ju
ly
 2
01
2 
the note length of a second note (b¼ 2), a crotchet
(b¼ 4), and so forth. Each note duration value j½mD;k is
mapped to the ı̂ th element of
d
u
½m
D as
î ¼ arg min
i
j½mD;k 
d
j½mD;i

: ð31Þ
3.2.2.9 Features related to the note-duration ratio
(NDR). The NDR uDR provides a tempo-independent
representation that characterizes the progression of the
note duration values in a bassline. Basslines from the
music genres minimal techno, motown, hip-hop, and
blues often contain note sequences of constant duration
as for instance simple quaver octave patterns. Hence, the
majority of the NDR values will be most likely equal to
one. We compute the relative frequencies of p(uDR5 1),
p(uDR¼ 1) and p(uDR4 1) as features.
3.2.2.10 Rhythmic articulation. To characterize the
dominant rhythmic articulation which is commonly
referred to by the terms staccato and legato, we use the
ratio
aSL ¼
1
N 1
XN1
i¼1
u
½m
D ðiÞ
Du½mO ðiÞ
ð32Þ
as a feature to measure the mean ratio between the length
of a note and its distance to its successor. A high value
indicates very short or no breaks between consecutive
notes which refers to legato play, low values of aSL point
to staccato play.
Overall, we obtain a 101-dimensional feature vector
representing each bassline.
4. Proposed method—classification
In this publication, we compare three different ap-
proaches for the classification of the bass playing style.
Figure 3 provides an overview.
4.1 Classification based on statistical pattern recognition
We apply a state-of-the-art supervised classification
technique, namely Support Vector Machine (SVM).
SVM is a binary discriminative classifier, attempting to
generate an optimal decision plane between feature
vectors of the training classes (Vapnik, 1998). Commonly
for real-world applications, classification with linear
separation planes is not possible in the original feature
space. To overcome this problem, the so-called kernel
trick is applied. The key idea of the kernel trick is to
replace the dot product in a high-dimensional space with
a kernel function in the original feature space. Trans-
formed into a high-dimensional space, non-linear classi-
fication problems can become linearly solvable. In the
experiments in this work, we use the most common type
of kernel, namely Radial Basis Function (RBF).
4.2 Classification based on rules
The classification rules in this work are inducted using a
Classification and Regression Tree (CART) recursive
partitioning technique with subsequent optimal pruning
strategy as proposed in Breiman, Friedman, Stone, and
Olshen (1993). CART is a nonparametric data mining
algorithm for generating decision trees, that does not
make any assumption about the specific distribution of
input feature vectors. Decision tree splits are made by
using the features that possess the best discriminative
properties with respect to the target classes. In CART
each node can be split into two child nodes. This
recursive partitioning is continued until the certain
stopping rules are fulfilled. The optimal parameters for
the stopping rules—e.g. a minimum number of items per
node to be still considered for splitting—are determined
experimentally. The generalization properties of the
decision tree are controlled in a cross-validation scenario,
where the tree is pruned to a certain level in order to
prevent overfitting the training data.
4.3 Classification based on pattern similarity
In many genres, typical basslines have evolved that are
frequently used with only some variation. We investigate
two different pattern similarity measures to compare
basslines with regard to their tonal and rhythmic
similarity. Therefore, we use either the vector u
½m
O
containing the note onset values or the vector hP
Fig. 3. Three different classification approaches.
Classification of music genres based on repetitive basslines 9
D
ow
nl
oa
de
d 
by
 [
U
ni
ve
rs
ity
 o
f 
A
eg
ea
n]
 a
t 0
3:
31
 0
9 
Ju
ly
 2
01
2 
containing the absolute pitch values to represent a
bassline P as depicted in Table 2. Hence, P can be
thought of as a character sequence that allows one to use
edit distances such as Levenshtein distance to compare
two patterns. For example, the funk bassline illustrated
as an example in Table 2 is represented by
P ¼ 48; 55; 57; 58; . . .½  ð33Þ
for all tonal similarity measures and by
P ¼ 0; 1
4
;
3
8
;
7
16
; . . .
 
ð34Þ
for all rhythmic similarity measures.
Since two basslines may be very similar but notated in
different keys, their absolute pitch values have to be
aligned to avoid erroneous similarity values. Instead of
using the lowest pitch value, we take the most frequent
pitch value yP,dom (see Section 3.2.1) as the reference
value. Again, if multiple absolute pitch appears equally
often, we select the lowest candidate as the most frequent
pitch value yP,dom.
Hence, if two arbitrary patterns P1 and P2 are to be
compared using a pitch-related similarity measure, we
modify all absolute pitch values yP,2k of P2 as
ŷP;2;k ¼ yP;2;k þ yP;1;dom  yP;2;dom; k 2 1;N2½ : ð35Þ
We compare two similarity measures in this publica-
tion, one based on the Levenshtein distance and
another one based on a pairwise pattern similarity
measure.
4.3.1 Levensthein distance
The Levensthein distance DL allows one to compute the
similarity between character strings (Gusfield, 1997).
Therefore, the minimum number of edits in terms of
insertions, deletions, and substitutions of characters is
determined, which is necessary to convert one string into
the other. We apply the Wagner–Fischer algorithm as
described in Wagner and Fischer (1974) to compute DL.
The similarity measure SL is derived as
SL ¼ 1
DL
DL;max
; ð36Þ
where DL,max equals the maximum value between the
lengths l1 and l2 of both patterns:
DL;max ¼
l1; l1  l2;
l2; l2 > l1:

ð37Þ
In the experiments, we use the rhythmic similarity
measure SL,R derived from the Levenshtein distance
between the onset vectors u
½m
O . A tonal similarity
measure SL,T is derived accordingly from the Levenshtein
distance between absolute pitch vectors hP. Furthermore,
we investigate the similarity measures
SL;RT;Max ¼
SL;R; SL;R  SL;T;
SL;T; SL;T > SL;R

ð38Þ
and
SL;RT;Mean ¼
1
2
ðSL;R þ SL;TÞ ð39Þ
by using the maximum and the arithmetic mean between
of SL,R and SL,T as aggregated similarity measures.
4.3.2 Pairwise pattern similarity measure
To derive a pairwise similarity measure, we compute the
number of notes N1,2 in pattern P1, for which at least one
note in pattern P2 exists that has the same absolute pitch
value hP (for the similarity measure SP,T) or onset value
u
½m
O (for the similarity measure SP,R). We derive N2,1 vice
versa for pattern P2. The pairwise similarity measure is
computed as
SP ¼
1
2
N1;2
N1
þN2;1
N2
	 

: ð40Þ
In addition to the similarity measures SP,R and SP,T, we
compute three similarity measures that incorporate both
the rhythmic and tonal similarity. Therefore, we use the
constraint that both the onset value u
½m
O and the absolute
pitch value hP have to be equal in Equation 40 to obtain
the measure SP,R,T. In addition, we compute SP,R,T,Max
and SP,R,T,Mean from SP,R and SP,T analogous to
Equations 38 and 39.
4.3.3 Aggregation strategies
In addition to the two pattern similarity approaches
explained in the previous subsections, we compare two
different strategies to aggregate the similarity results. We
use cross-validation scenarios for the classification
experiments. All folds contain the same number of
basslines and the same percentage of basslines from all
genres.
In each fold, we denote the set of patterns in the
training set by Pt. According to their genre labels
g 2 [1, 13] (see Section 5.1), all patterns in Pt can be
further subdivided into sub-sets Pt,g of patterns of each
genre g.
4.3.3.1 Pattern-wise classification. In order to classify
the genre of an unknown bassline P, we compute a
likelihood value lg for each genre g. Therefore we seek the
10 Jakob Abeßer et al.
D
ow
nl
oa
de
d 
by
 [
U
ni
ve
rs
ity
 o
f 
A
eg
ea
n]
 a
t 0
3:
31
 0
9 
Ju
ly
 2
01
2 
most similar pattern in each sub-set Pt,g in comparison to
P. The corresponding similarity measures are taken as
genre likelihood values
lg ¼ Smax;g: ð41Þ
Smax,g denotes the highest similarity detected between P
and all patterns in Pt,g assigned to genre g. The genre that
maximizes lg is the classified genre:
ĝ ¼ arg max
g2 1;13½ 
lg: ð42Þ
This approach is illustrated in Figure 4. In case two or
more genres are associated with the same likelihood-
value, the classified genre is randomly selected from the
most likely candidates.
In the dataset we used in this publication, the basslines
have different lengths between 4 and 16 measures. In
order to compute the similarity between two patterns of
arbitrary length, we proceed as follows. First, if
necessary, all patterns are elongated by adding their first
measures to achieve that all pattern lengths are powers of
two. If one pattern is shorter than the other, we shift it
with a step-size of two measures and compute a similarity
measure between the shorter pattern and the correspond-
ing sub-pattern of the longer pattern at the current
position. Finally, we average all similarity values to
compute the similarity between both patterns. This is
illustrated for an example of two patterns of 4 and 7
measures length in Figure 5.
4.3.3.2 Measure-wise classification. Each pattern
P 2 Pt can be subdivided into sub-patterns of one
measure length. The set of all sub-patterns in the training
set is denoted as St. As described in the previous
subsection, the set St can be subdivided into sub-sets
St,g according to the genre labels of the corresponding
sub-patterns.
To classify an unknown pattern P, we subdivide it
into N¼ jPj sub-patterns PSn(n 2 [1,N]). For each sub-
pattern PSn, we seek the most similar sub-pattern from
the training set for each genre. The genre likelihood ln,g
for each sub-pattern and each genre is computed as
ln;g ¼ Smax;n;g: ð43Þ
In order to obtain a genre likelihood value that cor-
responds to the whole pattern P, we average the
likelihood values over all sub-patterns for each genre as
lg ¼
1
N
XN
n¼1
ln;g: ð44Þ
Finally, the genre classification is performed as explained
in Equation 42 in the previous subsection. This approach
is depicted in Figure 6.
5. Evaluation
5.1 Data set
The data set used in this publication is an extended
version of the data set used in (Abeßer et al., 2010a). The
former collection comprised basslines from eight
Fig. 4. Pattern-wise classification strategy. For each genre m,
highest similarity Smax,m between an unknown pattern P and all
patterns assigned to the genre m is computed. These similarity
values are used as likelihood values lg for the genre classification.
Fig. 5. Computation of pattern-wise similarity between two
patterns PA and PB of arbitrary length. In this example, the
patterns have the length of 4 and 7 measures. The longer
pattern (PB) is elongated to a length of l(PB)¼ 8 measures by
adding its first measure. To compute the similarity between PA
and PB, the shorter pattern (PA) is shifted with a hop-size of
two measures. For each shift, the similarity between PA and the
corresponding sub-pattern of PB is computed. Finally, all
similarity values are averaged.
Classification of music genres based on repetitive basslines 11
D
ow
nl
oa
de
d 
by
 [
U
ni
ve
rs
ity
 o
f 
A
eg
ea
n]
 a
t 0
3:
31
 0
9 
Ju
ly
 2
01
2 
different genres that have been entirely taken from
instructional bass literature (Westwood, 1997; Reznicek,
2001). This collection has been extensively revised by
musicologists. All genre-sets were modified and five new
sets were added. Table 5 provides an overview of all
genres used in this work as well as their regional origin
and approximate beginning period.
Since instructional literature is not available for all of
the genres, a lot of the basslines have been manually
transcribed from real audio recordings by a semi-
professional bass player. Musicologists selected the songs
and the segments to be transcribed and checked the
transcriptions afterwards. These basslines enrich the ones
taken from instructional literature, which is not available
for all of the genres.
The current set consists of 520 basslines with 40
basslines for each of the 13 genres listed in Table 5. The
tempo distribution over all genres is depicted in Figure 7.
5.2 Experiments and results
5.2.1 Classification based on pattern similarity
We used 10-fold cross-validation to compute the mean
classification accuracy for all similarity measures and
both classification approaches introduced in Section 4.3.
As illustrated in Table 6, the pattern-wise classification
strategy clearly outperforms the measure-wise strategy
for all investigated similarity measures. Apparently,
sub-patterns of one measure length do not contain
sufficient stylistic information and thus can not represent
a genre as well as patterns of a length of four or more
measures.
A second finding is that the combined similarity
measures that take both the tonal and rhythmic
similarity into account performed better than the
similarity measures related to only one of both simila-
rities. The highest mean accuracy of 38.9% was achieved
using the pattern-wise classification and the similarity
measure SP,R,T,Mean based on the mean rhythmic and
tonal pairwise-similarity between patterns.
Third, we consistently found a large variance of the
classification accuracy results over all 13 genres. For
instance, the upper matrix in Table 7 depicts all genre
confusions in percent for the best configuration (similar-
ity measure SP,R,T,Mean, pattern-wise classification). The
mean accuracy values strongly vary from 97.5% for
swing down to 7.5% for seventies rock.
The classification approach based on pattern similar-
ity only provides satisfying results over 60% accuracy for
the genres swing, blues, and bossa nova. We assume that
for those genres, prototypic patterns exist that are often
used with small rhythmic and tonal variation. The
presented classification approach strongly depends on
this assumption. The basslines associated with other
genres seem to be too diverse in terms of rhythmic and
tonal similarity in order to be classified appropriately.
5.2.2 Classification based on rules
The evaluation was performed according to the following
scenario. First we experimentally chose the optimal
minimum number of items per node for CART.
Fig. 6. Measure-wise classification strategy. Each unknown
pattern P is divided into sub-patterns of one measure length.
For the each sub-pattern PSn, the highest similarity Smax,n,m is
detected for each genre m. Again, these similarity values are
used as likelihood values ln,g for the genres.
Table 5. Investigated genres.
g Abbr. Genre Origin
First
recordings
1 BLU blues USA 1912
(transcription)
2 BOS bossa nova Brasil 1958
3 FOR forró Brasil 1900
(transcription)
4 FUN funk USA 1960s
5 HIP hip-hop USA 1970s
6 MIN minimal techno USA,
Germany
1994
7 MOT motown USA 1960
8 REG reggae Jamaica 1960s
9 RON nineties rock USA, GB 1990s
10 ROS seventies rock USA, GB 1970s
11 SAL salsa & mambo Cuba 1930s
12 SWI swing USA 1920s
13 ZOU zouglou Cote
d’Ivoire
1995
12 Jakob Abeßer et al.
D
ow
nl
oa
de
d 
by
 [
U
ni
ve
rs
ity
 o
f 
A
eg
ea
n]
 a
t 0
3:
31
 0
9 
Ju
ly
 2
01
2 
This parameter was varied from 1 to 19 in a step of two.
For each parameter value the experiment was repeated
100 times. The box plots of accuracy values are presented
in Figure 9. The best results were achieved by setting
the minimum number of items per node to 9. The
generalization properties of the decision tree were
controlled in a 52-fold cross-validation scenario, where
the tree was pruned to an optimal level as proposed in
Breiman et al. (1993) in order to prevent overfitting to
the train data.
We assume this classifier to be slightly overtrained, as
the final results are achieved without clear separation of
the training and test data. This scenario was chosen to
allow better interpretation of one inducted set of rules
instead of multiple sets of rules for various validation
folds. All in all the final pruned decision tree is composed
of 57 nodes and make use of 14 features. Some highly
discriminative features—e.g. tempo in beats per minute
(see Figure 7)—appear multiple times in the decision
nodes. The resulting pruned decision tree and decision
tree rules are shown in Figure 10 and Table 8,
respectively.
Classification with a pruned decision tree achieves a
mean accuracy of 64.8%. Best accuracy of 92.5% is
obtained for the genre swing. In comparison to the pattern
similarity approach, there are no classes with the
classification accuracy close to a random one. The poorest
performance of 35% can be observed for Ninties Rock
genre, which is often confused with blues and reggae.
5.2.3 Classification based on statistical pattern
recognition
The evaluation of SVM classification was performed in a
52-fold cross-validation scenario, i.e. for each fold 10 items
were excluded from training and used solely as test material.
Additional cross-validation was applied to the training data
of each fold in order to estimate best kernel and regulation
parameters for the model. As SVM is a binary classifier, the
one-against-one method with subsequent voting was utilized
to enable multi-class classification.
The results of the SVM classification are presented in
Table 7. The mean accuracy over all classes comprises
55.4%. Same as for the pruned decision tree, the best
result of 92.5% is achieved for genre swing. The lowest
genre accuracy can be observed for motown due to the
multiple confusions with blues, funk and seventies rock.
More details on the confusions and their nature can be
found in the next section.
Fig. 7. Boxplots of tempo values for all genres. All tempo values are given in beats per minute (BPM). Genre abbreviations are
explained in Table 5.
Table 6. Classification based on pattern similarity—mean
classification accuracy (MN), standard deviation (SD), lowest
genre accuracy (MIN), and highest genre accuracy (MAX). All
values given in percent.
Classification strategy
Similarity
Pattern-wise Measure-wise
measure MN SD MIN MAX MN SD MIN MAX
SP,R 33.5 22.9 12.4 93.3 16.1 19.7 0 75.3
SP,T 21.7 8.5 7.9 35.4 7.8 12.5 0 44.5
SP,RT 29.9 17.6 5.4 57.5 12 21 0 75.8
SP,RT,Max 38.6 23.6 10 97.5 12.1 23.1 0 85
SP,RT,Mean 38.9 23.9 7.5 97.5 14.4 25.5 0 92.5
SL,R 33.5 22.3 10.7 93.3 16.3 18.9 1.7 73.3
SL,T 26 15.5 3.8 51 9 12.6 0 40
SL,RT,Max 35.8 22 13.2 93.3 15.7 17.9 1 67.8
SL,RT,Mean 37.9 24.6 7.5 97.5 13.1 17.2 0 58.3
Classification of music genres based on repetitive basslines 13
D
ow
nl
oa
de
d 
by
 [
U
ni
ve
rs
ity
 o
f 
A
eg
ea
n]
 a
t 0
3:
31
 0
9 
Ju
ly
 2
01
2 
6. Discussions and conclusions
In this paper, we compared three different classification
approaches for genre classification based on repetitive
basslines. We found that the rule-based approach
achieved a mean accuracy of 64.8% and thus out-
performed classification based on pattern similarity and
statistic pattern recognition by 25.9% and 9.4%. The
random baseline accuracy for 13 classes is approx. 7.7%.
The field of genre in music is complex and ambiguous.
The data set chosen for this paper is not designed to
simplify this reality but is meant to reflect it. From a
musicological point of view, this paper confirms and
specifies known findings: low classification rates strongly
correspond with what the authors would like to call
eclectic styles, i.e. music styles, which by their own
history and logic are a camouflage of older styles. In
bossa nova (BOS) for example, Jazz and Latin
Table 7. Confusion matrices—all values given in percent.
BLU BOS FOR FUN HIP MIN MOT REG RON ROS SAL SWI ZOU
Classification (pattern similarity - best configuration): m¼ 38.9%
BLU 60 2.5 0 0 2.5 5 7.5 2.5 10 2.5 0 7.5 0
BOS 5 67.5 15 0 0 0 2.5 0 2.5 7.5 0 0 0
FOR 5 12.5 40 2.5 2.5 12.5 5 0 5 2.5 2.5 10 0
FUN 25 0 2.5 32.5 2.5 2.5 5 2.5 10 5 7.5 0 5
HIP 12.5 5 5 12.5 15 12.5 7.5 10 12.5 0 0 2.5 5
MIN 6.3 0 1.3 7.5 10 36.3 6.3 5 17.5 2.5 0 5 2.5
MOT 26.7 0 0 5 2.5 0 27.5 6.7 8.3 18.3 2.5 0 2.5
REG 12.5 0 5 2.5 7.5 2.5 17.5 25 5 7.5 2.5 2.5 10
RON 27.5 4.2 0 1.7 2.5 5 2.5 5 24.2 20 2.5 5 0
ROS 5 7.5 5 7.5 0 7.5 22.5 10 17.5 7.5 0 7.5 2.5
SAL 10 0 4.2 12.5 2.5 6.7 10 2.5 2.5 7.5 36.7 0 5
SWI 2.5 0 0 0 0 0 0 0 0 0 0 97.5 0
ZOU 5 0 2.5 6.3 11.3 8.8 5 5 15 0 5 0 36.3
Classification (pruned tree): m¼ 64.8%
BLU 70 0 0 5 0 0 10 7.5 5 0 2.5 0 0
BOS 0 67.5 12.5 0 0 5 2.5 0 0 2.5 0 0 10
FOR 2.5 10 77.5 0 0 0 2.5 0 0 0 5 0 2.5
FUN 2.5 0 0 77.5 0 2.5 10 0 0 2.5 2.5 0 2.5
HIP 7.5 2.5 0 5 50 0 0 10 0 2.5 12.5 2.5 7.5
MIN 5 0 0 0 0 60 7.5 0 2.5 5 10 0 10
MOT 5 0 0 0 0 0 72.5 2.5 2.5 10 2.5 0 5
REG 10 2.5 0 2.5 2.5 0 2.5 55 0 0 7.5 0 17.5
RON 17.5 7.5 0 0 0 0 5 17.5 35 5 10 0 2.5
ROS 10 0 0 5 0 0 5 7.5 12.5 45 10 0 5
SAL 5 0 0 5 0 0 0 2.5 2.5 2.5 65 0 17.5
SWI 0 0 0 0 0 0 2.5 0 2.5 0 2.5 92.5 0
ZOU 2.5 0 0 2.5 2.5 0 5 2.5 0 5 5 0 75
Classification (SVM): m¼ 55.4%
BLU 60 0 0 2.5 2.5 2.5 10 2.5 7.5 7.5 0 5 0
BOS 0 77.5 17.5 0 0 0 0 0 0 2.5 2.5 0 0
FOR 0 25 57.5 0 0 5 5 0 0 2.5 2.5 0 2.5
FUN 5 0 0 67.5 2.5 0 5 7.5 7.5 2.5 2.5 0 0
HIP 2.5 0 2.5 0 45 17.5 5 7.5 0 10 7.5 0 2.5
MIN 0 0 2.5 0 12.5 67.5 0 2.5 5 5 5 0 0
MOT 17.5 0 2.5 12.5 2.5 7.5 20 2.5 7.5 17.5 5 0 5
REG 7.5 2.5 0 7.5 10 0 7.5 40 10 2.5 0 0 12.5
RON 5 2.5 2.5 12.5 0 10 12.5 10 37.5 5 0 2.5 0
ROS 10 2.5 0 5 10 2.5 15 10 7.5 27.5 2.5 2.5 5
SAL 0 2.5 5 2.5 12.5 2.5 2.5 2.5 5 0 55 0 10
SWI 2.5 0 0 0 0 2.5 0 0 0 2.5 0 92.5 0
ZOU 0 0 0 0 2.5 7.5 0 7.5 0 2.5 7.5 0 72.5
14 Jakob Abeßer et al.
D
ow
nl
oa
de
d 
by
 [
U
ni
ve
rs
ity
 o
f 
A
eg
ea
n]
 a
t 0
3:
31
 0
9 
Ju
ly
 2
01
2 
styles—especially samba—are blended. However, the
two styles are still recognizable. The high rate of
confusion of bossa nova with other Latin styles therefore
is a meaningful one.
This is even stronger in the case of hip-hop (HIP).
Originally, its music is comprised of samples from tracks
of other styles. This idea of a non-altering collage of
elements that genuinely derive from other styles (funk,
soul, jazz or rock) even persists, when the music of a hip-
hop track is self-made (i.e. played by a hip-hop combo
instead of sampled from other recordings).
Some genres in the data set therefore are quite similar
(e.g. seventies rock and nineties rock, salsa and bossa
nova). The observed genre confusions in this paper as in
all serious musical classification therefore must be
separated into corresponding confusions and non-corre-
sponding confusions.
As regards the non-corresponding confusions, the
classification is simply mistaken. Some confusions on the
other hand correspond to, i.e. reflect, ambiguities in
the musical reality. For example basslines of blues music
have been mistaken as rock (ROS, RON) or motown
(MOT), but almost never as one of the four different
Latin or Afro-Carribean music styles (SAL, ZOU, BOS,
FOR). Other than mistakes or misclassification, these
highly corresponding confusions are valuable informa-
Fig. 8. Boxplots of frequency of crotchets p(j½mD ¼ 1=4) for all genres. Genre abbreviations are explained in Table 5.
Fig. 9. Box plots of accuracy values achieved by pruned decision tree with various settings for the minimum number of items per node
in CART.
Classification of music genres based on repetitive basslines 15
D
ow
nl
oa
de
d 
by
 [
U
ni
ve
rs
ity
 o
f 
A
eg
ea
n]
 a
t 0
3:
31
 0
9 
Ju
ly
 2
01
2 
tion in a lot of use cases such as musicological research
on genre similarity or distinguished classification of
fusion styles for online shops and distributors.
Adding to the promising results, many of the genre-
wise confusions are corresponding confusions (see
above). Most of them are intra-regional: blues is confused
with rock and motown. The two rock styles are confused
with one another as are the urban electronic music styles
of hip-hop and minimal techno. Motown, as an overlap
genre, also belongs to another stylistic region or cluster—
self-consciously Afro-American or African music styles
motown, reggae, salsa and hip-hop as well as African
zouglou are confused with each other significantly.
Reggae does stand in the middle of this stylistic map:
corresponding to the history of this music style it is
confused with the urban hip-hop, African zouglou and
American motown. Zouglou on the other hand is further
at the edge of this cluster, being confused only with salsa
and reggae, which corresponds to zouglou being inter-
twined with Carribean styles that also influenced reggae
and salsa but not so much with hip-hop, funk and soul.
The confusions between seventies rock and zouglou in
the statistical pattern recognition approach on the other
hand simply are mistakes.
As the figures above show, SVM classification is in a
lot of cases but not always worse than the pruned-tree
approach. Regarding the harmonically more complex
genre of bossa nova the SVM approach even beats the
pruned tree by 10% of accuracy. While the pruned tree
approach has the best classification rates, it is worst
regarding the rate of non-corresponding confusions.
Remarkably, the most information of inter-stylistic
musical relations could be drawn out of the pattern
similarity.
7. Future directions
It is not surprising, that the observation of only one
musical partition (i.e. the bassline) does not overrule
Fig. 10. Pruned decision tree, see Table 8 for details.
16 Jakob Abeßer et al.
D
ow
nl
oa
de
d 
by
 [
U
ni
ve
rs
ity
 o
f 
A
eg
ea
n]
 a
t 0
3:
31
 0
9 
Ju
ly
 2
01
2 
Table 8. Decision tree rules for Figure 10.
Node IF condition Yes No
N 1 p(j½mD ¼ 1=4) (crotchets)50.944 N 2 N 3
N 2 Tempo (bpm)5202.5 N 4 N 5
N 3 genre¼ swing
N 4 p(j½mD ¼ 1=2) (half notes)50.027 N 6 N 7
N 5 genre¼ forró
N 6 aSL (see Equation 32)50.673 N 8 N 9
N 7 Variance over number of notes per second51.119 N 10 N 11
N 8 Tempo (bpm)5105.5 N 12 N 13
N 9 aSL (see Equation 32)50.946 N 14 N 15
N 10 genre¼ bossa nova
N 11 genre¼ zouglou
N 12 genre¼ hip-hop
N 13 First-order entropy over DhP,D570.470 N 16 N 17
N 14 p(j½mD ¼ 3=8) (dotted quavers)50.061 N 18 N 19
N 15 Variance over aOn,32,m (see Equation 21)50.014 N 20 N 21
N 16 genre¼ zouglou
N 17 genre¼minimal techno
N 18 p(pentatonic minor scale) (see Table 4)50.165 N 22 N 23
N 19 p(DhP,F¼ 2) (ascending second)50.127 N 24 N 25
N 20 p(j½mD ¼ 1=12) (triplet quavers)50.383 N 26 N 27
N 21 genre¼ swing
N 22 Mean absolute intervall size53.175 N 28 N 29
N 23 p(jDhP,Fj 2 {1,2,3}) – p(Primes, seconds, thirds)50.442 N 30 N 31
N 24 genre¼ salsa & mambo
N 25 genre¼ funk
N 26 NN/ttotal (number of notes / length (s))52.656 N 32 N 33
N 27 genre¼ blues
N 28 Tempo (bpm)5108.5 N 34 N 35
N 29 p(whole tone half tone scale) (see Table 4)50.093 N 36 N 37
N 30 genre¼ zouglou
N 31 genre¼ reggae
N 32 Scale index s of most likely scale52.5 (see Table 4) N 38 N 39
N 33 Mean over aOn,2,m (see Equation 21)50.563 N 40 N 41
N 34 p(j½mD ¼ 1=16) (semiquavers)50.617 N 42 N 43
N 35 p(DyP,k)50.161 (measure of constant pitch) N 44 N 45
N 36 genre¼ salsa & mambo
N 37 genre¼ funk
N 38 genre¼ seventies rock
N 39 genre¼ forró
N 40 genre¼ nineties rock
N 41 Tempo (bpm)5113.5 N 46 N 47
N 42 Mean number of notes per second52.375 N 48 N 49
N 43 genre¼ reggae
N 44 genre¼ seventies rock
N 45 p(chromatic note sequence)50.218 N 50 N 51
N 46 genre¼ blues
N 47 p(minor gypsy scale) (see Table 4)50.097 N 52 N 53
N 48 genre¼ hip-hop
N 49 genre¼ blues
N 50 Variance over aOn,4,m50.007 N 54 N 55
N 51 genre¼ funk
N 52 genre¼ nineties rock
N 53 genre¼motown
N 54 genre¼motown
N 55 p(jDR 41) (see Equation 7)50.123 N 56 N 57
N 56 genre¼ reggae
N 57 genre¼ seventies rock
Classification of music genres based on repetitive basslines 17
D
ow
nl
oa
de
d 
by
 [
U
ni
ve
rs
ity
 o
f 
A
eg
ea
n]
 a
t 0
3:
31
 0
9 
Ju
ly
 2
01
2 
more general approaches. However in several cases
the bassline is a very successful discriminator. In
combination with a corresponding rule system, the
approach will produce valuable information for genre
classification.
Given the ambiguity of the field of music genres,
the authors are satisfied to see that most of the
observed confusions correspond to the musical reality
as e.g. Latin styles are confused with each other as are
urban styles or the two rock genres. Additionally the
results reflect the autonomy of a style, eclectic styles
like hip-hop being more prone to confusions than
swing.
The limited approach of using symbolic MIDI files
does not allow a detailed investigation of micro-timing
deviations, which usually have a smaller order of
magnitude than the smallest rhythmic unit, say a
demisemiquaver. Further aspects of performance such
as rhythmic micro timing, dynamic progression in note
sequences and the interaction between the bass player
and the drums and the harmony instrument need to be
incorporated to derive a complete view on musical
performance.
The automatic description of musical style by
basslines must not stand alone. We assume that it can
be a very useful part of a multi-layered approach that
incorporates a stylistic analysis of all instrument tracks
of a song. The results of the three classification
approaches indicate that a combined hybrid classification
framework would benefit from the strengths of the
individual components. This hybrid framework could for
instance be comprised of a majority-based voting
including all three classifiers.
Alternatively to classification algorithms one could
apply clustering methods to group and explore musical
items. Constrained clustering has been developed to
improve clustering methods through pairwise constraints
between items in the feature space (Mercado & Luka-
shevich, 2010). Based on the results of our work, these
constraints could be the relations between cultural
regions or music styles. Applying these constraints could
help to avoid non-corresponding confusions, see Sec-
tion 6.
Acknowledgements
The authors would like to thank Christian Dittmar for
valuable comments. This work has been partly supported
by the German research project GlobalMusic2One1
funded by the Federal Ministry of Education and
Research (BMBF-FKZ: 01/S08039B). Additionally, the
Thuringian Ministry of Economy, Employment and
Technology supported this research by granting funds
of the European Fund for Regional Development to the
project Songs2See2, enabling transnational cooperation
between Thuringian companies and their partners from
other European regions.
References
Abeßer, J., Dittmar, C., & Großmann, H. (2008). Automatic
genre and artist classification by analyzing improvised solo
parts frommusical recordings. In Proceedings of the Audio
Mostly Conference, Piteå, Sweden, pp. 127–131.
Abeßer, J., Lukashevich, H., Bräuer, P., & Schuller, G.
(2010a). Bass playing style detection based on high-level
features and pattern similarity. In Proceedings of the
International Society of Music Information Retrieval
(ISMIR), Utrecht, Netherlands, pp. 94–98.
Abeßer, J., Lukashevich, H., Dittmar, C., Bräuer, P., &
Krause, F. (2010b). Rule-based classification of musical
genres from a global cultural background. In Proceedings
of the International Symposium on Computer Music
Modeling and Retrieval, Málaga, Spain, pp. 317–336.
Abeßer, J., Lukashevich, H., Dittmar, C., & Schuller, G.
(2009). Genre classification using bass-related high-level
features and playing styles. In Proceedings of the Interna-
tional Society of Music Information Retrieval (ISMIR
Conference), Kobe, Japan, pp. 453–458.
Anglade, A., Ramirez, R., & Dixon, S. (2009). Genre classi-
fication using harmony rules induced from automatic
chord transcriptions. InProceedings of the 10thConference
on Music Information Retrieval (ISMIR), Kobe, Japan,
pp. 669–674.
Breiman, L., Friedman, J., Stone, C., & Olshen, R. (1993).
Classification and Regression Trees. London: Chapman &
Hall.
Bresin, R. (2001). Articulation rules for automatic music
performance. InProceedings of the International Computer
Music Conference (ICMC), Havana, Cuba, pp. 294–297.
Bruhn, H., Oerter, R., & Rösing, H. (Eds.). (2005). Musik-
Psychologie – Ein Handbuch. Reinbek: Rowohlt.
Buzzanca, G. (2001). A rule-based expert system for music
style recognition. In Proceedings of the 1st International
Conference onUnderstanding and CreatingMusic (UCM),
Caserta, Italy, pp. 1–8.
De Léon, P.J.P., & Iñesta, J.M. (2007). Pattern recognition
approach for music style identification using shallow
statistical descriptors. IEEE Transactions on System,
Man and Cybernetics – Part C: Applications and Reviews,
37(2), 248–257.
De León, P.J.P., Rizo, D., & Iñesta, J.M. (2007). Towards a
human-friendly melody characterization by automatically
induced rules. In Proceedings of the 8th International
Conference on Music Information Retrieval (ISMIR),
Vienna, Austria, pp. 437–440.
1See http://www.globalmusic2one.net 2See http://www.songs2see.net
18 Jakob Abeßer et al.
D
ow
nl
oa
de
d 
by
 [
U
ni
ve
rs
ity
 o
f 
A
eg
ea
n]
 a
t 0
3:
31
 0
9 
Ju
ly
 2
01
2 
Dittmar, C., Dressler, K., & Rosenbauer, K. (2007). A tool-
box for automatic transcription of polyphonic music. In
Proceedings of the Audio Mostly Conference, Ilmenau,
Germany, pp. 58–65.
Eerola, T., & Toiviainen, P. (2004). MIDI Toolbox: MA-
TLAB Tools for Music Research. Jyväskylä, Finland:
University of Jyväskylä.
Flanagan, P. (2008). Quantifying metrical ambiguity. In
Proceedings of the International Conference on Music
Information Retrieval (ISMIR), Philadelphia, USA, pp.
635–640.
Friberg, A., & Sundström, A. (2002). Swing ratios and
ensemble timing in jazz performance: Evidence for a
common rhythmic pattern. Music Perception, 19(3), 333–
349.
Goto,M. (2004). A real-timemusic-scene-description system:
Predominant-F0 estimation for detectingmelody and bass
lines in real-world audio signals. Speech Communication,
43, 311–329.
Gouyon, F. (2005). A computational approach to rhythm
description – audio features for the computation of rhythm
periodicity functions and their use in tempo induction and
music content processing (PhD thesis).Universitat Pompeu
Fabra, Barcelona, Spain.
Grachten, M., Arcos, J.-L., & de Mantaras, R.L. (2004).
Melodic similarity: Looking for a good abstraction level.
InProceedings of the 5th International Conference inMusic
Information Retrieval (ISMIR), Barcelona, Spain, pp.
210–215.
Gusfield, D. (1997). Algorithms on Strings, Trees, and
Sequences: Computer Science and Computational Biology.
Cambridge: Cambridge University Press.
Hainsworth, S.W., &Macleod, M.D. (2001). Automatic bass
line transcription from polyphonic audio. In Proceedings
of the International Computer Music Conference (ICMC),
La Habana, Cuba, pp. 431–434.
Madsen, S.T., & Widmer, G. (2007). A complexity-
based approach to melody track identification in MIDI
files. In Proceedings of the International Workshop on
Artificial Intelligence and Music (MUSIC-AI), Hydera-
bad, India.
McKay, C., &Fujinaga, I. (2004). Automatic genre classifica-
tion using large high-level musical feature sets. In
Proceedings of the 5th International Conference in Music
Information Retrieval (ISMIR), Barcelona, Spain, pp.
525–530.
McKay, C., & Fujinaga, I. (2006). jSymbolic: A feature
extractor for MIDI files. In International Computer
Music Conference (ICMC), New Orleans, USA, pp.
302–305.
Mercado, P., & Lukashevich, H. (2010). Feature selection in
clustering with constraints: Application to active explora-
tion of music collections. In Proceedings of the Interna-
tional Conference on Machine Learning and Applications,
Washington, USA, pp. 649–654.
Müllensiefen, D., & Frieler, K. (2004). Optimizing measures
of melodic similarity for the exploration of a large folk
song database. In Proceedings of the 5th International
Conference in Music Information Retrieval (ISMIR),
Barcelona, Spain, pp. 274–280.
Orio, N., & Rodá, A. (2009). A measure of melodic similarity
based on a graph representation of the music structure. In
Proceedings of the 10th International Society for Music
Information Retrieval Conference (ISMIR), Kobe, Japan,
pp. 543–548.
Pérez-Sancho, C., De León, P.J.P., & Iñesta, J.M. (2006). A
comparison of statistical approaches to symbolic genre
recognition. In Proceedings of the International Computer
Music Conference (ICMC), New Orleans, USA, pp. 545–
550.
Reznicek, H.-J. (2001). I’m Walking – Jazz Bass. Brühl:
AMA-Verlag.
Ryynänen, M.P., & Klapuri, A.P. (2008). Automatic tran-
scription of melody, bass line, and chords in polyphonic
music. Computer Music Journal, 32, 72–86.
Stamatatos, E., & Widmer, G. (2005). Automatic identifica-
tion of music performers with learning ensembles.
Artificial Intelligence, 165, 37–56.
Tsuchihashi, Y., Kitahara, T., & Katayose, H. (2008). Using
bass-line features for content-based MIR. In Proceedings
of the International Conference on Music Information
Retrieval (ISMIR), Philadelphia, USA, pp. 620–625.
Tsunoo, E., Ono, N., & Sagayama, S. (2009). Musical bass-
line pattern clustering and its application to audio genre
classification. In Proceedings of the International Society
of Music Information Retrieval (ISMIR), Kobe, Japan,
pp. 219–224.
Typke, R., Giannopoulos, P., Veltkamp,R.C.,Wiering, F., &
van Oostrum, R. (2002). Using transportation distances
for measuring melodic similarity. In Proceedings of the 3rd
International Conference on Music Information Retrieval
(ISMIR), Paris, France.
Unal, E., Chew, E., Georgiou, P.G., & Narayanan, S.S.
(2008). Challenging uncertainty in query by humming
systems: a fingerprinting approach. IEEE Transactions on
Audi, Speech, and Language Processing, 16, 359–371.
Vapnik, V.N. (1998). Statistical Learning Theory.NewYork:
Wiley.
Wagner, R.A., & Fischer, M.J. (1974). The string-to-string
correction problem. Journal of the ACM (JACM), 21(1),
168–173.
Westwood, P. (1997). Bass Bible. Brühl: AMA-Verlag.
Wicke, P. (2007).Handbuch der PopulärenMusik: Geschichte,
Stile, Praxis, Industrie. Mainz: Schott Music.
Widmer, G., Dixon, S., Goebl, W., Pampalk, E., & Tobudic,
A. (2003). In search of the Horowitz factor. AI Magazine,
24, 111–130.
Widmer, G., & Goebl, W. (2004). Computational models of
expressivemusic performance: The state of the art. Journal
of New Music Research, 33(3), 203–216.
Classification of music genres based on repetitive basslines 19
D
ow
nl
oa
de
d 
by
 [
U
ni
ve
rs
ity
 o
f 
A
eg
ea
n]
 a
t 0
3:
31
 0
9 
Ju
ly
 2
01
2 
