Neural Process Lett (2016) 43:295–307
DOI 10.1007/s11063-015-9422-9
Locality Alignment Discriminant Analysis for Visualizing
Regional English
Peng Tang1 · Mingbo Zhao1 · Tommy W. S. Chow1
Published online: 2 April 2015
© Springer Science+Business Media New York 2015
Abstract In this paper, a novel dimensionality reduction algorithm named locality align-
ment discriminant analysis (LADA) for visualizing regional English is proposed. In the
LADA algorithm, the proposed intrinsic graph or penalty graph measures the similarities
between each pairwise textual slices, which can better characterize the intra-class compact-
ness and inter-class separability; the projection matrix obtained by the proposed method
is orthogonal, which can eliminate the redundancy between different projection directions,
and is more effective for preserving the intrinsic geometry and improving the discriminat-
ing ability. To evaluate the performance of the algorithm, a regional written English corpus
is designed and collected. Consequently, articles are split into slices and then transformed
into 140-dimensional data points by 140 text style markers. Finally, variations existing in
the regional written English are attempted to be recognized with our proposed LADA. The
similarity among different types of English can be observed by the data plots. The results of
visualization and numerical comparison indicate that LADAoutperforms other existing algo-
rithms in handling regional English data, as the proposed LADA can better preserve the local
discriminative information embedded in the data, which is suitable for pattern classification.
Keywords Locality alignment discriminant analysis · Regional English ·
Dimensionality reduction
1 Introduction
English is the most commonly used language which is often the first language or the second
language throughout the world. But Englishalso varies regionally in both written and spoken.
B Mingbo Zhao
mbzhao4@gmail.com
Peng Tang
ptang@ee.cityu.edu.hk
Tommy W. S. Chow
eetchow@cityu.edu.hk
1 Department of Electronic Engineering, City University of Hong Kong, Kowloon, Hong Kong
123
296 P. Tang et al.
Early in 1999, a system for analyzing regional accents has been developed by using accent-
independent keyword lexicons [4]. A survey of regional differences in word usage, grammar,
and pronunciation in the United States was conducted, indicating English dialects in the
US are unevenly distributed [26]. Some may even argue that “English” should be a noun
with plural form, “Englishes”, because English is globally used with linguistic diversities
regionally [23]. The differences among regional English exist not only in accents of English
speakers such as the strong Scottish accent, but also in spelling, vocabulary, grammar and
some other aspects. Such linguistic diversities may cause communication problems, misun-
derstanding to listeners, and even inefficient working performance. Many previous research
studies have found distinct characteristics among different regional English stem from the
aspects of word using [19], pronunciation [10,25,29]. But most of them use manually statis-
tics on the basis of pre-defined rules, e.g. count the frequencies of a group of specific words
for making comparison of articles with different sources. In this study, we attempt to use
computational approaches to analyze the variations of modern regional written English. It is
also the intent of this study to show analyzing and discriminating regional written English
can be performed by using an effective computational method.
During the past decade, huge amount of text documents have been createdwith rapid devel-
opment of Internet. Computational text analysis approaches, which are the core of rapid data
processing tools used for large-scare text datasets, have thus become increasingly important.
Text mining is an essential technique for effective manpower allocation from simple reading
and sorting to advanced analysis and solution implementation in government, business cor-
porate, industry and organizations. A recent study indicates that over 80 % of information of
is contained in textual documents [31]. In Korea, text and data mining technique is proposed
to read and classify large amount of online petition [22]. Text classification is one of the
most important text mining techniques that automatically assign unseen digital documents to
suitable predefined categories [2,16]. By using machine learning approaches, user-friendly
results can be delivered to meet users’ requirements. Existing machine learning techniques,
k-nearest neighbor [7], support vector machine (SVM) [12,28] and neural networks [17],
deliver promising results of text classification. Moreover, text document clustering provides
advanced intuitive navigation by organizing large sets of documents into a small number of
meaningful clusters. In [9], Ontology lexicons are used to enhance the clustering efficiency
of documents with similar topics which results in providing interesting combinations of doc-
uments for users. To date, most researches still mainly focus on exploring topic and generic
information in textual materials and few of them are designed for studying regional English
articles.
In this paper, a novel dimensionality reduction algorithm named locality alignment dis-
criminant analysis (LADA) for visualizing regional English is proposed. First, a regional
written English corpus is designed and collected. Consequently, articles are split into slices
and then transformed into 140-dimensional data points by 140 text style markers. Finally,
variations existing in the regional written English are attempted to be recognized with our
proposed LADA. The similarity among different types of English can be observed by the
data plots. The results of visualization and numerical comparison indicate that LADA outper-
forms other existing algorithms in handling regional English data. The results of visualization
are highly consistent with common sense and, thus, validates the feasibility of our proposed
method. We also compare our proposed LADA with the unsupervised PCA algorithm, the
supervised DLA, TR-LDA, MFA and LDA algorithms. Our obtained results indicate the
LADA algorithm outperforms the other studied approaches because (1) the proposed intrin-
sic graph or penalty graphmeasures the similarities between each pairwise textual slices in the
same class or in different classes, which can better characterize the intra-class compactness
123
Locality Alignment Discriminant Analysis… 297
and inter-class separability; and (2) the projection matrix obtained by the proposed method
is orthogonal, which can eliminate the redundancy between different projection directions,
and is more effective for preserving the intrinsic geometry and improving the discriminating
ability. This is important because these characteristics ensure the LADA algorithm is able to
handle the case when density of slices varies dramatically.
2 The Proposed Locality Alignment Discriminant Analysis
2.1 Review of Linear Discriminant Analysis
The goal of LDA is to find a linear projectionmatrixW ∈ RD×d formaximizing the between-
class scatter while minimizing the within-class scatter. Let X = {x1, x2, . . . xl} ∈ RD×l be
the training set, each xi belongs to a class ci = {1, 2, . . . c}. Let li be the number of data
points in the i th class and l be the number of data points in all classes, μi is the mean of
the data points in the i th class, and μ is the mean of the data points in all classes. We also
define two graph similarity matrixes Wb ∈ Rl×l and Ww ∈ Rl×l as Wb|mn = 1/ l − 1/li ,
Ww|mn = 1/li , if x jm and x jn are both in the i th class;Wb|mn = 1/ l,Ww|mn = 0, otherwise,
and Lb and Lw are their corresponding Laplacian matrixes. Then, the between-class scatter
matrix Sb and within-class scatter matrix Sw are defined as follows:
Sw =
∑c
i=1
∑
x∈ci
(x − μi ) (x − μi )T = XLwXT
Sb =
∑c
i=1 li (μi − μ) (μi − μ)
T = XLbXT
(1)
The original formulation of LDA, called Fisher LDA [5], can only deal with binary classi-
fication. Two optimization criteria can be used to extend Fisher LDA to solve the multi-class
classification problem. The first one is in the ratio trace form (we refer it as LDA):
W ∗ = argmaxWTr
{(
WT SwW
)−1
WT SbW
}
(2)
and the second one is in the trace ratio form (we refer it as TR-LDA):
W ∗ = argmaxWTW=I T r
(
WT SbW
)
/Tr
(
WT SwW
)
(3)
The optimal solution of LDA can be formed by the top eigenvectors of S−1w Sb. On the
other hand, the optimization problem of TR-LDA in Eq. (3) has no close-form solution
and has to calculate it by an iterative trace ratio method (ITR) [27]. Specifically, if Wt
denotes the solution at the t th iteration, then at the (t + 1) th solution, Wt+1 can be formed
by the top eigenvectors of Sb − λt Sw, where λt = Tr
(
WTt SbWt
)
/Tr
(
WTt SwWt
)
. This
procedure can be proved to converge to the globally optimal solution given any initialization
W0 [11,27].
2.2 A More Efficient Algorithm for Solving the TR Problem
Though the ITR algorithm works well for solving the TR problem, it has its own drawback.
The ITR algorithm method has chosen d eigenvectors corresponding to the d largest eigen-
values of Sb −λ∗Sw to formW ∗. These eigenvectors can only maximize the trace difference
value Tr
(
WT (Sb − λ∗Sw)W
)
, but these eigenvectors cannot maximize the trace ratio value
123
298 P. Tang et al.
Tr
(
WTt SbWt
)
/Tr
(
WTt SwWt
)
. Thus, how to find d eigenvectors to maximize the trace ratio
value is an important question. Motivated by this issue, we in the previous work [35], pro-
pose a more efficient algorithm, called improved ITR algorithm (iITR), which can solve this
problem.
Given any initial λt , by performing the eigen-decomposition of Sb − λt Sw , we can
obtain the D eigenvectors of Sb − λt Sw. The problem is then to choose the d eigen-
vectors Wt =
{
wi1 , wi2 , . . . , wiD
}
maximizing
∑d
k=1 wTik Sbwik /
∑d
k=1 wTik Swwik , where
i = {i1, i2, . . . , id} is a certain permutation chosen from {1, 2, . . . , D}. Here, if we define
f = { f1, f2, . . . , fD} ∈ R1×D , g = {g1, g2, . . . , gD} ∈ R1×D with each element satisfying
fi = wTi Sbwi and gi = wTi Swwi , the above problem can be converted to find the optimal
selection vector b = {b1, b2, . . . , bD} ∈ R1×D as:
b∗ = argmaxb f1b1+ f2b2+···+ fDbDg1b1+g2b2+···+gDbD
subject to bi ∈ {0, 1} , b1T = d . (4)
Note that the above problem is a linear fractional programming (LFP) problem [20,30,32].
It can be solved by Dinkelbachs algorithm which is a general algorithm for optimizing
γ = Φ (b)/Ψ (b) with Ψ (b) > 0. In Dinkelbachs algorithm, it converts the problem to a
sequence of sub-problems for optimizing Φ (b) − γΨ (b). Hence in our case, by initializing
γ0 = λt and let f , g be defined as above, the optimal selection vector b∗ can then be obtained
by iteratively solving the following sub-problem:
{
γ0 = λt
f, g
}
→
⎧
⎨
⎩
bk = argmaxbb( f − γkg)T
subject to bki ∈ {0, 1} , bk1T = d
γk+1 = bk f T /bkgT
⎫
⎬
⎭ →
{
b∗ : bk = bk−1
γ ∗ = b∗ f T /b∗gT
}
(5)
After b∗ is obtained, we can output Wt by choosing the d eigenvectors with b∗i = 1. The
basic steps of the algorithm are listed in Table 1.
2.3 The Proposed Algorithm
The above algorithm in Table 1 is developed with the assumption that the samples in each
class follow a Gaussian distribution. However, in many text classification problems, sam-
Table 1 iITR algorithm for solving the trace ratio problem
1. Initialize λ0 = 0.
2. Compute the eigen-decomposition of Sb − λt Sw as (Sb − λt Sw)wi = τiwi , where
wi (i = 1, 2, . . . D) is the eigenvector of Sb − λt Sw .
3. Calculate fi = wTi Sbwi and gi = wTi Swwi for i ∈ {1, 2, . . . , D} and initialize γ0 = λt and
b0 =
[
b01, b
0
2, . . . b
0
D
]
be a zero vector, iteratively solving the sub-problem of Eq. (5) until
convergence:
- Sort fi − γk gi and set bki = 1 corresponding to the d largest value of fi − γk gi , bki = 0
otherwise.
- Update γi+1 = bk f T /bkgT .
- If bk = bk−1, output b∗ = bk and γ ∗ = b∗ f T /b∗gT .
4. Form Wt by choosing the d eigenvectors of wi with b
∗
i = 1 and Update λt+1 = γ ∗.
5. Iterate the steps (2–4) until
∣∣λt+1 − λt
∣∣ < ε. Output W∗.
123
Locality Alignment Discriminant Analysis… 299
ples in a data set may follow a non-Gaussian distribution that cannot satisfy the above
assumption. Without this assumption, the separation of different classes may not be well
characterized by the scatter matrices, causing the classification results to be degraded [5].
To solve this problem, some researchers have demonstrated that it is beneficial to exploit
local geometric properties of data to characterize intra-class compactness and inter-class
separability [14,15,30,32]. These methods usually start from the local structure of training
samples and then arm to preserve the geometric information provided by both data sam-
ple and label information. In this paper, considering the local structure is approximately
linear, we develop a new method for supervised dimensionality reduction to handle non-
Gaussian distributed data. Our approach is motivated from the patch alignment framework
[32] and recent process on trace ratio criterion model which has been extensively used for
semi-supervised clustering [34], semi-supervised dimensionality reduction [36] and medical
diagnosis [35]. Specifically, let Nk
(
x j
)
be the k neighborhood set of x j including itself,
we denote X j =
{
x j0 , x j1 , . . . , x jk
} ∈ RD×k as the local data matrix formed by all sam-
ples in Nk
(
x j
)
, where { j1, j2, . . . , jk} is the index set of Nk
(
x j
)
and j1 = j , x j1 = x j .
We also denote the low-dimensional presentations of x j and its local patch X j as f j and
Fj =
{
f j1 , f j2 , . . . , f jk
} ∈ Rd×k , respectively. Then, Fj can be viewed as a selection from
the global low-dimensional presentations F = { f1, f2, . . . , fl+u} ∈ Rd×(l+u), i.e.
Fj = FSj , (6)
where S j ∈ R(l+u)×k is the selected matrix with each element satisfying S j |pq = 1, if
p = iq ; S j |pq = 0. In addition, we also define k ji as the number of data points of X j
belonging to the i th class,W jb ∈ Rk×k andW jw ∈ Rk×k are two similarity matrixes defined as
W jb |mn = 1/k−1/k ji ,W jw|mn = 1/k ji , if x jm and x jn are both in the i th class;W jb |mn = 1/k,
W jw|mn = 0, otherwise. Then the local between-class and with-class scatter matrixes can be
defined as:
S jb = Fj L jb FTj , S jw = Fj L jwFTj , (7)
where L jb , L
j
w are the corresponding graph Laplacian matrix of the local scatter matrixes of
W jb , W
j
w . In practice, we hope that the local between-class scatter can be maximized while
the local within-class scatter can be minimized (shown in Fig. 1). Then, it is reasonable to
maximize the following objective function, which is based on a trace ratio criterion as in
Eq. (3):
maxFj T r
(
Fj L
j
b F
T
j
)
/Tr
(
Fj L
j
wF
T
j
)
. (8)
To obtain the global F , we sum the traces of local between-class and within-class scatter
matrixes in Eq. (8) over all local patches, which can be formulated as:
maxFj
∑l
j=1 Tr
(
Fj L
j
b F
T
j
)
∑l
j=1 Tr
(
Fj L
j
wFTj
) = maxF
∑l
j=1 Tr
(
FSj L
j
b S
T
j F
T
)
∑l
j=1 Tr
(
FSj L
j
wSTj F
T
)
= maxF
Tr
{
F
(∑l
j=1 S j L
j
b S
T
j
)
F
}
Tr
{
F
(∑l
j=1 S j L
j
wSTj
)
F
}
(9)
123
300 P. Tang et al.
Fig. 1 Local between-class scatter and local within-class scatter. In this figure, different shapes represents
different classes. We hope that after dimensionality reduction, the data points of the same class move close to
each other, while the points of different classes move away from each other
In order to calculate the projection matrix, we constrain the low-dimensional represen-
tation F lies in the linear subspace formed by the data matrix X , i.e. F = WT X , then by
replacing F into Eq. (9), we can reformulate the objective function of Eq. (9) as:
maxF
Tr
{
F
(∑l
j=1 S j L
j
b S
T
j
)
F
}
Tr
{
F
(∑l
j=1 S j L
j
wSTj
)
F
} = maxWTW=I
T r
{
WT X
(∑l
j=1 S j L
j
b S
T
j
)
XTW
}
Tr
{
WT X
(∑l
j=1 S j L
j
wSTj
)
XTW
}
= maxWTW=I
T r
{
WT X L̃bXTW
}
Tr
{
WT X L̃wXTW
} , (10)
where
S̃b = X L̃bXT , S̃w = X L̃wXT . (11)
It should be noted that L jb and L
j
w are graph Laplacian matrixes. Based on the property of
matrix, i.e. given a positive semi-definite matrix C , DCDT is a positive semi-definite matrix
for any matrix D, we have S j L
j
b S
T
j and S j L
j
wSTj are graph Laplaican matrixes. Based on
the property again, we have L̃b = ∑lj=1 S j L jb STj and L̃w =
∑l
j=1 S j L
j
wSTj are also graph
Laplacian matrixes.
Since both L̃b and L̃w has guaranteed as graph laplacian matrix, we can simply perform
notation substitutions in Table 1, i.e. S̃b → Sb, S̃w → Sw, and solve the problem in Eq. (10)
by using the algorithm of Table 1. Finally, the basic steps for solving Eq. (10) can be listed
in Table 2.
123
Locality Alignment Discriminant Analysis… 301
Table 2 The proposed algorithm
Input: Data matrix X , parameter k1 and k2
Output: Optimal projection matrix W∗
Algorithms:
(1) Form the scatter matrix S̃b and S̃w .
(2) Calculate the projection matrix W using Table 1 with S̃b and S̃w .
(3) Output the optimal projection matrix W∗.
Table 3 Components of our
corpus
Component names Source Count
1. American English nytimes.com 2000
2. British English guardian.com 2000
3. Chinese English xinhuanet.com 2000
4. Australian English news.com.au 2000
5. French English france24.com/en/ 2000
6. Indian English timesofindia.indiatimes.com 2000
7. Japanese English japantimes.co.jp 2000
8. South Korean English koreatimes.co.kr 1000
3 Data Preparation
A corpus consisting of English articles designed and collected from the World Wide Web
in eight geographical regions is used to validate our method. The collected articles include
topics from global news, politics, science and technology from each region. All articles were
published in 2012 and 2013. Table 3 lists the details of the regional components, sources
and the number of articles in the corpus. Note that the numbers of English Korean English
articles are only half of other components. This aims to test the performance when dealing
with the discriminative analysis on imbalanced data.
3.1 Pre-processing of Articles
In order to ensure a fair test, pre-processing procedures are applied to all articles of the corpus.
All spellings are converted to American style to avoid the bias caused by different spellings
of the same word. For example, colour/color are united to color; dialogue/dialog are united
to dialog; programme/program are united to program. Variations of word caused by letter
decorations, e.g. accent marks, are ignored. For example, cafe and café are considered as the
same word. Currency symbols are replaced by a unified symbol.
Hapax legomena are unified to a special symbol to avoid bias caused by topic shifts. For
example, an article containing the term “tempura” tends to be composed by Japanese authors;
“Bouillabaisse” usually means the corresponding articles are from France.
All articles of each components are first randomly joined and then split into pieces con-
taining constant n sentences, referred as “slices”, which can avoid the bias caused by the
variation of article lengths. The value of n should be large enough to ensure recognizable
difference can be captured. In this study, n is empirically set to 200 .
123
302 P. Tang et al.
Table 4 Style markers used in our study
Type Description
Symbol Numbers of the following symbols: ’ ” ( ) , – . :
Vocabulary Numbers of the following POS taggers: CC, CD, DT, EX, FW, IN, JJ, JJR, JJS, LS, MD,
NN, NNP, NNPS, NNS, PDT, POS, PRP, PRP$, RB, RBR, RBS, RP, SYM, TO, UH,
VB, VBD, VBG, VBN, VBP, VBZ, WDT, WP, WP$, WRB; FRR
Syntax Numbers of the following POS tagger combinations: CD|NN, CD|NNS, CD|NN|NP,
CD|RB, DT|NN, DT|RB, IN|JJ, IN|PP, IN|RB, IN|RP, JJR|IN, JJR|RBR, JJ|CC, JJ|IN,
JJ|JJR, JJ|NN, JJ|NP, JJ|RB, JJ|VBG, JJ|VBN, LS|EX, LS|JJ, LS|NN, LS|NNS, MD|VB,
NNPS|NNS, NNPS|VBZ, NNP|CC|NP, NNP|JJ, NNP|NN, NNP|NP, NNP|NPS,
NNP|POS, NNP|VB, NNP|VBN, NNP|VBZ, NNS|DT, NNS|LS, NNS|NN, NNS|NPS,
NNS|VBZ, NN|CD, NN|DT, NN|IN, NN|JJ, NN|JJ|RB, NN|NN, NN|NNS, NN|POS,
NN|RB, NN|SYM, NN|VB, NN|VBG, NN|VBP, NN|WRB, PRP|JJ, PRP|MD, PRP|VBP,
RBR|JJR, RBR|NN, RBS|JJ, RBS|JJS, RB|CC, RB|DT, RB|IN, RB|JJ, RB|NN|JJ,
RB|RBR, RB|RP, RB|VBG, RB|VBZ, RP|IN, RP|RB, VBD|RB, VBD|VBN, VBD|VBP,
VBG|JJ, VBG|NN, VBG|NN|JJ, VBN|JJ, VBN|TO, VBN|VBD, VBP|IN, VBP|PP,
VBP|TO, VBP|VB, VBP|VBD, VB|IN, VB|NN, WP|IN, WP|MD|NP
Derivative ASL: Average Length of sentence; ALP: Average Length of paragraph; INTI: Intimacy;
NSW: Number of stop words
3.2 Style Markers
A style marker refers to a textual feature or an attribute. In our study, 140 popular style
markers [1,6,13,21,24] are employed for extracting different types of textual characteristics.
All 140 style markers used in our study are listed in Table 4. We adopt the part-of-speech
(POS) tags used in the Penn Treebank Project [18]. The stylemarkers, i.e. the textual features,
are categorized into four types, namely symbol features, vocabulary features, syntax features
and derivative features. Symbol features are delimiters and punctuations. Vocabularies are the
POS-tagged terms frequently representing words using in original articles. Syntax features
comprise a series of POS tagger combinations which implies the syntactic and grammatical
structures of sentences. Derivative features are some general textual statistics within each
slice. Thus, each slice is transformed into a 140-dimensional data point and each dimension
is labeled by one of the 140 style markers.
4 Simulations and Results
4.1 Visualization Comparison
To intuitively compare our proposed LADA to other algorithms, we conduct three groups
of simulations. Six dimensionality reduction algorithms, including LADA, DLA, TR-LDA,
MFA, PCA and LDA [30], are employed to compress high dimensional data into 2D vectors
for visualization. The three sets of study include:
– (a) Discrimination of Chinese, Japanese and Korean English from the US, British and
Australian English;
– (b) Discrimination of the US and British English from Chinese, Japanese, Korean, Indian
and French English;
– (c) Discrimination of the Japanese English from Chinese English.
123
Locality Alignment Discriminant Analysis… 303
All visualization results are displayed in Fig. 2. Figure 2a1–a6 illustrate the 2-D discrim-
ination results of Asian English from native speakers English. We can observe that the two
testing components do not have a clear demarcation, but Fig. 2 a1 shows our proposed LADA
can deliver a relatively clear separation with the smallest overlapping between the two testing
components. Figure 1b1–b6 study the difference between native English and ESL English.
It can be observed that in Fig. 2b2–b5 the color points, represent the non-native English,
congregate with the black points in different ways, which indicate the TR-LDA, DLA, MFA
and LDA are not performing well in discriminating native English from ESL English. In
this case, despite the 6 algorithms all being unable to provide a clear demarcation, the pro-
posed LADA and PCA can still provide a relatively promising discrimination results with
less overlapping. Figure 2c1–c6 display the discrimination results between Japanese English
and Chinese English. It is interesting to observe that the two components can be separated by
the six methods in general. But it can be observed that LADA and DLA are able to provide a
clear demarcation for the two testing components. It shows Japanese English is significantly
different from Chinese Englsish dispite both of them being ESL English. In all, the above
results show our proposed LADA can deliver promising results under different comparative
scenarios.
4.2 Numerical Comparison
Numerical comparison is also conducted in our study.We use normalizedmutual information
(NMI) and clustering accuracy (ACC) to evaluate the performance of visualization [3,8,33,
34]. The larger the values of NMI and ACC obtains, the more efficient the method is. The
results are shown in Fig. 3. With clusters number increases, the ACC value decreases, but
LADA is still able to deliver the largest ACC value in most cases. Meanwhile, the NMI
values delivered by our proposed LADA are significantly larger than those by the other five
methods. The comparative results indicate that our proposed method can capture language
characteristics more effectively than the others methods. The clustering results are listed in
Table 5. From Table 5, it can also be observed that for most cases, our proposed LADA
outperforms the other methods by delivering higher AC and NMI values. We thus choose
LADA for the visualization study hereinafter.
5 Discussion and Conclusion
In this paper, a new corpus consisting of English articles from eight geographically different
regions were collected for thorough study using computational methods. In order to improve
the discrimination results in low dimensional space, a new dimensionality reduction algo-
rithm, LADA, was derived for analyzing text slices. Compared to other existing algorithms,
the LADA algorithm improves the following aspects. First, the proposed intrinsic graph Ws
or penalty graphW p measures the similarities between each pair-wise text slices in the same
class or in different classes, which can better characterize the intra-class compactness and
inter-class separability. Second, the projection matrix obtained by the proposed method is
orthogonal, which can eliminate the redundancy between different projection directions, and
is more effective for preserving the intrinsic geometry and improving the discriminating
ability. This is important because these characteristics ensure the LADA algorithm is able to
handle the casewhen density of slices varies dramatically, which is of particular useful for text
application. Numerical analysis indicates that our proposed method is able to deliver a higher
ACC and NMI compared with other dimensionality reduction methods studied in this study.
123
304 P. Tang et al.
ALDADL-RTADAL
 
Asian English
Western English
 
Asian English
Western English
 
Asian English
Western English
ACPADLAFM
 
Asian English
Western English
 
Asian English
Western English
 
Asian English
Western English
ALDADL-RTADAL
 
Native English
ESL English
 
Native English
ESL English
 
Native English
ESL English
ACPADLAFM
 
Native English
ESL English
 
Native English
ESL English
 
Native English
ESL English
ALDADL-RTADAL
 
Chinese English
Japanese English
 
Chinese English
Japanese English
 
Chinese English
Japanese English
ACPADLAFM
 
Chinese English
Japanese English
 
Chinese English
Japanese English
 
Chinese English
Japanese English
(a)
(b)
(c)
Fig. 2 Two-dimensional visualization of the discrimination results using LADA, TR-LDA, DLA,MFA, LDA
and PCA. a Discrimination of Chinese, Japanese and Korean English from the US, British and Australian
English. Asterisk stands for Chinese, Japanese and Korean English; Plus stands for the US, British and
Australian English b Discrimination of the US and British English from Chinese, Japanese, Korean, Indian
and French English. Asterisk stands for the US and British English; Plus stands for Chinese, Japanese, Korean,
Indian and French English c Discrimination of Japanese from Chinese English. Asterisk stands for Chinese
English; Plus stands for Japanese English
123
Locality Alignment Discriminant Analysis… 305
ACC NMI
2 3 4 5
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
A
cc
ur
ac
y
LADA
TR−LDA
DLA
MFA
LDA
PCA
2 3 4 5
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
N
M
I
LNDA
TR−LDA
DLA
MFA
LDA
PCA
2 3 4 5
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
A
cc
ur
ac
y
LADA
TR−LDA
DLA
MFA
LDA
PCA
2 3 4 5
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
N
M
I
LNDA
TR−LDA
DLA
MFA
LDA
PCA
2 3 4 5
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
A
cc
ur
ac
y
LADA
TR−LDA
DLA
MFA
LDA
PCA
2 3 4 5
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
N
M
I
LNDA
TR−LDA
DLA
MFA
LDA
PCA
(a)
(b)
(c)
Fig. 3 Clustering based comparative analysis of LADA, TR-LDA, DLA,MFA, LDA and PCA. aDiscrimina-
tion of theChinese, Japanese andKoreanEnglish from theUS,British andAustralianEnglish.bDiscrimination
of the US and British English from Chinese, Japanese, Korean, Indian and French English. c Discrimination
of the Japanese English from Chinese English
In all, this paper presented a thorough study on regional written English using our devel-
oped LADA algorithm with other computational methods. This study showed that it is
possible to use advanced computational methods to classify different region written Eng-
lish, although it may not be highly accurate. The major implications of this study showed
that computational methods can be used to analyze written English in a rather sophisticated
way. And more interesting findings can be obtained with further study using different com-
putational methods.
123
306 P. Tang et al.
Table 5 Performance comparisons on the three sets of discrimination study
Simulation setting
Method Set a Set b Set c
ACC NMI ACC NMI ACC NMI
LADA 0.7877 0.7140 0.8435 0.7166 0.8921 0.6539
TR-LDA 0.6442 0.4764 0.4455 0.4784 0.6604 0.6113
DLA 0.7381 0.6535 0.5759 0.5506 0.8919 0.6645
MFA 0.5282 0.5519 0.5208 0.4721 0.8495 0.7335
LDA 0.3951 0.3312 0.6097 0.4902 0.8172 0.7654
PCA 0.4172 0.3058 0.542 0.4101 0.8180 0.6705
Set a discrimination of Chinese, Japanese and Korean English from the US, British and Australian English, Set
b discrimination of the US and British English from Chinese, Japanese, Korean, Indian and French English,
Set c discrimination of Japanese from Chinese English
Acknowledgments This work was partly supported by the National Natural Science Foundation of China
under Grant No. 61300209.
References
1. Biber D (1995) Dimensions of register variation: a cross-linguistic comparison. Cambridge Univesity
Press, Cambridge
2. Branavan SRK, Chen H, Eisenstein J, Barzilay R (2009) Learning document-level semantic properties
from free-text annotations. J Artif Intell Res 34:569–603. doi:10.1613/jair.2633
3. Cai D, He X, Han J (2005) Document clustering using locality preserving indexing. IEEE Trans Knowl
Data Eng 17(12):1624–1637
4. Fitt S, Isard S (1999) Synthesis of regional english using a keyword lexicon. In: Proceedings Eurospeech
99, 823–826
5. Fukunaga K (1990) Introduction to statistical pattern recognition. Academic Press, Massachusetts
6. van Halteren H, Tweedie F, Baayen H (1996) Outside the cave of shadows: using syntactic annotation to
enhance authorship attribution. Comput Humanit 28(2):87–106
7. Han E, Karypis G, Kumar V (2001) Text categorization using weight adjusted k-nearest neighbor classi-
fication. Conference on advances in knowledge discovery and data mining, pp 53–65
8. He X, Cai D, Niyogi P (2006) Laplacian score for feature selection. Adv Neural Inf Process Syst 18:507
9. Hotho A, Staab S, Stumme G (2003) Ontologies improve text document clustering. In: Third IEEE inter-
national conference on data mining 2003, ICDM 2003. pp. 541–544. doi:10.1109/ICDM.2003.1250972
10. Hughes A, Trudgill P, Watt D (2012) English accents and dialects: an introduction to social and regional
varieties of English in the British Isles. Routledge, London
11. Jia Y, Nie F, Zhang C (2009) Trace ratio problem revisited. IEEE Trans Neural Netw 20(4):729–735
12. Joachims T (1999) Transductive inference for text classification using support vector machines. In:
Machine learning-international workshop then conference, Morgan Kaufmann Publishers Inc., pp. 200–
209
13. Kessler B, Numberg G, Schütze H (1997) Automatic detection of text genre. In: Proceedings of the
35th annual meeting of the association for computational linguistics and eighth conference of the euro-
pean chapter of the association for computational linguistics, ACL ’98, Association for Computational
Linguistics, Stroudsburg, PA, pp. 32–38. doi:10.3115/976909.979622
14. Lai Z, Wong WK, Xu Y, Zhao C, Sun M (2013) Sparse alignment for robust tensor learning. IEEE Trans
Neural Netw Learn Syst 25(10):1779–1792
15. Lai Z, Xu Y, Yang J, Jinhui T, David Z (2013) Sparse tensor discriminant analysis. IEEE Trans Image
Process 22(10):3904–3915
16. Mairesse F, Walker MA, Mehl MR, Moore RK (2007) Using linguistic cues for the automatic recognition
of personality in conversation and text. J Artif Intell Res 30:457–500. doi:10.1613/jair.2349
123
Locality Alignment Discriminant Analysis… 307
17. Manevitz L, Yousef M (2007) One-class document classification via neural networks. Neurocomputing
70(7):1466–1481
18. Marcus MP, Marcinkiewicz MA, Santorini B (1993) Building a large annotated corpus of english: the
penn treebank. Comput Linguist 19(2):313–330
19. Metcalf AA (2000) How we talk: American regional english today;[a talking tour of American english,
region by region]. Houghton Mifflin Harcourt, Boston
20. Nie F, Xiang S, Jia Y, Zhang C, Yan S (2008) Trace ratio criterion for feature selection. In: AAAI, vol. 2,
671–676
21. Stamatatos E, Fakotakis N, Kokkinakis G (1999) Automatic authorship attribution. In: Proceedings of
the ninth conference on European chapter of the Association for Computational Linguistics, EACL ’99,
Association for Computational Linguistics, Stroudsburg, PA, pp. 158–164. doi:10.3115/977035.977057
22. Suh JH, Park CH, Jeon SH (2010) Applying text and data mining techniques to forecasting the trend of
petitions filed to e-people. Expert Syst Appl 37(10):7255–7268. doi:10.1016/j.eswa.2010.04.002. http://
www.sciencedirect.com/science/article/pii/S0957417410002733
23. Tanaka S (2006) English and multiculturalism—from the language user’s perspective. RELC J 37(1):47–
66
24. Tang P, Chow TWS (2013) Recognition of word collocation habits using frequency rank ratio and inter-
term intimacy. Expert Syst Appl 40(11):4301–4314
25. ThompsonRM(1975)Mexican-American english: social correlates of regional pronunciation.AmSpeech
50(1/2):18–24
26. Vaux B, et al. (2003) Harvard survey of North American dialects
27. Wang H, Yan S, Xu D, Tang X, Huang T (2007) Trace ratio vs. ratio trace for dimensionality reduction.
In: IEEE conference on computer vision and pattern recognition 2007, CVPR’07. pp 1–8
28. Wang TY, Chiang HM (2011) Solving multi-label text categorization problem using support vec-
tor machine approach with membership function. Neurocomputing 74(17):3682–3689. doi:10.1016/j.
neucom.2011.07.001
29. Wolfram W, Schilling-Estes N (1998) American English: dialects and variation. Blackwell Malden,
Malden
30. Yan S, Xu D, Zhang B, Zhang HJ, Yang Q, Lin S (2007) Graph embedding and extensions: a general
framework for dimensionality reduction. IEEE Trans Pattern Anal Mach Intell 29(1):40–51
31. Yu L, Wang S, Lai K (2005) A rough-set-refined text mining approach for crude oil market tendency
forecasting. Int J Knowl Syst Sci 2(1):33–46
32. Zhang T, Tao D, Li X, Yang J (2009) Patch alignment for dimensionality reduction. IEEE Trans Knowl
Data Eng 21(9):1299–1313
33. Zhang Z, Chow T, Zhao M (2013) M-isomap: orthogonal constrained marginal isomap for nonlinear
dimensionality reduction. IEEE Trans Cybern 43(1):180–191
34. Zhang Z, Chow TW, Zhao M (2013) Trace ratio optimization-based semi-supervised nonlinear dimen-
sionality reduction for marginal manifold visualization. IEEE Trans Knowl Data Eng 25(5):1148–1161.
doi:10.1109/TKDE.2012.47
35. Zhao M, Chan RH, Tang P, Chow TW, Wong SW (2013) Trace ratio linear discriminant analysis for
medical diagnosis: a case study of dementia. IEEE Signal Process Lett 20(5):431–434
36. Zhao M, Zhang Z, Chow TW (2012) Trace ratio criterion based generalized discriminative learning for
semi-supervised dimensionality reduction. Pattern Recognit 45(4):1482–1499
123
