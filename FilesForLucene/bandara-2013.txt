Pattern Recognition Letters 34 (2013) 330–334Contents lists available at SciVerse ScienceDirect
Pattern Recognition Letters
journal homepage: www.elsevier .com/locate /patrecSource code author identification with unsupervised feature learning
Upul Bandara ⇑, Gamini Wijayarathna
Department of Industrial Management, University of Kelaniya, Sri Lanka
a r t i c l e i n f o a b s t r a c tArticle history:
Received 26 April 2012
Available online 16 November 2012
Communicated by S. Sarkar
Keywords:
Source code author identification
Unsupervised feature learning
Code metrics
Auto-encoder
Logistic regression0167-8655/$ - see front matter  2012 Elsevier B.V. A
http://dx.doi.org/10.1016/j.patrec.2012.10.027
⇑ Corresponding author. Tel./fax: +94 715 468 345.
E-mail address: upulbandara@gmail.com (U. BandAutomatic identification of source code authors has many applications in different fields such as source
code plagiarism detection, and law suit prosecution. This paper presents a new source code author iden-
tification system based on an unsupervised feature learning technique. As a method of extracting features
from high dimensional data, unsupervised feature learning has obtained a great success in many fields
such as character recognition and image classification. However, according to our knowledge it has not
been applied for source code author identification systems. Therefore, we investigated an unsupervised
feature learning technique called sparse auto-encoder as a method of extracting features from source
code files. Our system was evaluated with several datasets and results have shown that performance
is very close to the state of art techniques in the source code identification field.
 2012 Elsevier B.V. All rights reserved.1. Introduction
Automatic identification of source code authors is an important
activity in many fields in computing. Among those fields, source
code plagiarism detection is one of the main fields which can be
easily benefited by the source code author identification. According
to Zobel (2004), ‘‘students may plagiarize by copying code from
friends, the Web or so called ‘‘private tutors’’. Therefore, detection
and prevention of source code plagiarism is essential for program-
ming assignments.
Chao et al. (2006) have mentioned that ‘‘A quality plagiarism
detector has a strong impact to law suit prosecution’’. Furthermore,
Lange and Mancoridis (2007) have pointed out that, source code
author identification is useful in criminal justice and corporate lit-
igation as well.
In this research we investigated a novel technique in machine
learning called unsupervised feature learning (Raina et al., 2007)
in order to improve the performance of source code author identi-
fication systems. Recently unsupervised feature learning algo-
rithms have obtained a huge attention in the machine learning
community (Raina et al., 2007) and have been successfully used
for many applications. However, according to our knowledge it
has not been tried for source code author identification. Therefore,
we investigated one such learning technique called sparse auto-en-
coder (Coates et al., 2010).
This paper is organized as follows. In Section 2, we present pre-
vious works related to source code author identification. The archi-
tecture of our system is described in Section 3. In Section 4, well rights reserved.
ara).describe training, and testing of the system. Finally, we conclude
this paper by presenting our conclusions and discussing further
improvements.
2. Related work
A few search papers have been written on source author identi-
fication using machine learning techniques. Lange and Mancoridis
(2007) have proposed a source code author identification method,
which uses source code metric histograms and genetic algorithm.
Firstly, source code metrics were generated from software source
code files. Then the normalized metrics were used as the input
for the nearest neighbor classifier. The system is capable of identi-
fying the true author of each source code file with 55% accuracy.
Burrows and Tahaghoghi (2007) described a system which uses
information retrieval approach for source code attribution. Source
codes are tokenized and n-gram tokens are indexed in a search en-
gine. In order to determine the author of a source code file, they
use it as a search engine query on indexed documents of known
authors. Similarity between querying code file and indexed files
is measured by using several similarity detection functions and
system is capable for identifying true authors with 76.78% of
accuracy.
Frantzeskou et al. (2006) described a technique called, source
code author profiles (SCAP) for authorship attribution. SCAP is
based on byte level n-grams author profiles. Firstly, for each author
a set of L most frequent n-grams are calculated and which repre-
sents the profile of the author. Similarly, profile of each testing
source code file is calculated. In order to classify a test source code
file, its profile is compared with precalculated author profiles and
most likely author is the one who has least dissimilar profile
Table 2
Source code metric and token frequencies generated by LineLengthCalculator metric.
Length of the line Number of occurrences Token Token frequency
5 12 LLC_5 12
8 20 LLC_8 20
15 13 LLC_15 13
20 9 LLC_20 9
32 11 LLC_32 11
38 3 LLC_38 3
40 2 LLC_40 2
U. Bandara, G. Wijayarathna / Pattern Recognition Letters 34 (2013) 330–334 331according to a simplified version of the similarity measure intro-
duced by Keselj et al. (2003). According the paper, accuracy of
the SCAP method is very close to 100%.
Shevertalov et al. (2009) described a novel method for generat-
ing source code author profiles for author attribution. Their meth-
od is based on source code discretization. An optimum set of
discretized metrics are identified with the help of genetic algo-
rithms. Application of decision tree techniques and style metrics
for source code author identification are described by Elenbogen
and Seliya (2008).3. Source code author identification system
Following section describe the architecture of our source code
author identification system. The system consists of four pipeline
stages as enumerated below.
1. Source code files are fed as the input for the system.
2. Code metrics are extracted from the input source code files.
Lange and Mancoridis (2007) have conducted an extensive
research on selecting an optimum set of source code metrics
from a huge collection of metrics and have identified 8 such
metrics. Since our main objective of this project was to investi-
gate unsupervised feature learning for source code author iden-
tification, initially we selected these eight metrics. However,
some of these metrics were not fully independent from each
other. For example, trail-space, and trail-tab (measure the trail-
ing whitespaces and tabs at the end of a line) are very similar to
each other. Therefore, we represent these two as a single metric
in our system. Similarly, inline-space, and inline-tab (measure
the whitespace that occurs on the interior areas of each non-
whitespace line) are represent as a single metric in our system.
In addition to that, we introduced three new code metrics.
These metrics and their descriptions are shown in Table 1.
3. Featured are extracted using an unsupervised technique called,
sparse auto-encoder (Bengio, 2009).
4. Train a supervised learning algorithm called, logistic regression
(Bishop, 2007). Output of the logistic regression gives the pre-
dicted authors of the input source code files.
Following sections discuss above pipeline stages in detail.3.1. Source code metrics generation
Nine code metrics used in this research are depicted in Table 1.
Moreover, a three letter unique code is assigned to each source
code metric.
As the next step each source code metric is turn into a set of to-
kens. This can be better explained with an example. Consider a
metric generated by ‘‘LineLengthCalculator’’ for a particular source
code file as shown in first two columns of Table 2.Table 1
Source code metrics used for source code author identification.
Metric name Code Description
LineLengthCalculator LLC This metric measures the number of charac
LineWordsCalculator LWC This metric measures the number of words
AccessCalculator ACL Java uses the four levels of access control: p
of these access levels used by the programm
CommentsFrequencyCalculator CFC Java uses three types of comments. This me
programmers
IndentifiersLengthCalculator ILC This metric calculates the length of each id
InLineSpaceInlineTabCalculator INT This metric calculates the whitespaces that
TrailTabSpaceCalculator TTS This metric measures the whitespace and t
UnderscoresCalculator USC This metric measures the number of unders
IndentSpaceTabCalculator IST This metric calculates the indentation whitThis metric generates a set of tokens and token frequencies as
shown in last two columns of Table 2. Following the same method,
we converted each source code file into a sequence of tokens. These
sequences of tokens are used as the feature vector for the sparse
auto-encoder learning algorithm.
3.2. Unsupervised feature learning using sparse auto-encoder
Unsupervised feature learning has been successfully used for
various real-world applications. Coates et al. (2011) described text
detection and character recognition in scene images using unsuper-
vised feature learning. Lee et al. (2009) described application of
unsupervised feature learning algorithms for audio classification.
Auto-encoder is one of the unsupervised feature leaning algo-
rithms and it was first introduced by Rumelhart et al. (1986).
According to Bengio and Delalleau (2011) ‘‘auto-encoders can be
used as one-layer unsupervised learning algorithms that give rise
to a new representation of the input or of the previous layer’’.
Further, auto-encoder has been used for building and training mul-
ti-layer neural networks (Bengio, 2009). However, auto-encoder
performs badly because it could learn useless features when num-
ber of features is larger than the input size (Bengio and Delalleau,
2011). Sparse auto-encoder is a variation of auto-encoder which
addresses that limitation of auto-encoder. It was introduced by
Ranzato et al. (2006).
Auto-encoder is a neural network and it can be trained to en-
code input x into different kind of representation such a way that
input can be reconstructed from that representation. Fig. 1 shows
the pictorial representation of an auto-encoder. In Fig. 1, L1 layer
represents the input layer and L3 layer represents the output layer.
L2 is the middle layer and number of computational unites in the
middle layer is represented by K and it is one of the hyper-param-
eters of the auto-encoder. If mean squared error creation is used to
learn the auto-encoder, the K unites in the hidden layer of the auto-
encoder learn to project the input in the span of the first K principal
components of the data (Bengio, 2009). Therefore, auto-encoder
can be considered as a nonlinear representation of Principle Com-
ponent Analysis (PCA) with the ability to capture multimodal as-
pect of the input distribution (Hinton, 2006).
Let xð1Þ; yð1Þ
 
; xð2Þ; yð2Þ
 
; . . . ; xðmÞ; yðmÞ
  
denote m training
examples and in our research input layer of the auto-encoderters in one source code line
in one source code line
ublic, protected, default and private. This metric calculates the relative frequency
ers
tric calculates the relative frequency of those comment types used by the
entifier of Java programs
occurs on the interior areas of non-whitespace lines
ab occurrence at the end of each non-whitespace line
core characters used in identifiers
espaces used at the beginning of each non-whitespace line
Fig. 1. An auto-encoder.
Table 3
Detailed information for datasets used in this study. Program lengths are measured by
means of lines of codes (LOC).
Dataset
I
Dataset
II
Dataset
III
Dataset
IV
Dataset
V
Number of authors 10 10 8 5 9
Number of source
code files
1644 780 475 131 520
Training set 850 550 300 100 400
Cross validation set 325 – – – –
Testing set 469 230 175 31 120
Minimum source code
files per author
61 28 33 20 35
Maximum source
code files per
author
377 128 118 36 118
Size of smallest
source code file
(LOC)
7 28 91 8 20
Size of largest source
code file (LOC)
3691 15052 4880 654 1135
Average LOC per
author
29857 44620 26690 2452 6268
332 U. Bandara, G. Wijayarathna / Pattern Recognition Letters 34 (2013) 330–334consist of 642 features, i.e. x 2 R1642. hW,b(x) denotes the output of
auto-encoder. Using squared error, the cost function for single in-
put example can be written as:
JðW; b; x; yÞ ¼ 1
2
jjhW;bðxÞ  yjj2 ð1Þ
where W represents weights associated with nodes of the neural
network and b represents bias terms of the neural network. We
can extend Eq. (1) for m training examples as follows:
JðW; bÞ ¼ 1
m
Xm
i¼1
1
2
jjhW;bðxÞ  yjj2
 
ð2Þ
Further, we introduce a regularization term for the Eq. (2). Regular-
ization term helps over fitting and reduces the magnitude of
weights. Eq. (3) shows the cost function with regularization term.
JðW; bÞ ¼ 1
m
Xm
i¼1
1
2
jjhW;bðxÞ  yjj2
 
þ k
2
X2
i¼1
Xsl
i¼1
Xslþ1
j¼1
W ðlÞji
 	2
ð3Þ
where, k represents weight decay parameter of the auto-encoder, sl
represents number of processing units in layer l, and Wji(l) repre-
sents the weight associated with the unit i in the layer l and unit j
in the layer l + 1.
Let að2Þj denotes the activation of hidden unit j in the auto-enco-
der. Further, let að2Þj ðxÞ to denote the activation of hidden unit a
ð2Þ
j ,
when the network is given a specific input x. Further, let q̂ to
denote the average activation of hidden unit j in the network. Let
m represents number of training examples, and then q̂ can be
represented in the Eq. (4).
q̂j ¼
1
m
Xm
j¼1
að2Þj ðx
ðiÞÞ
h i
ð4Þ
We would like to enforce the sparsity constraint and let q to de-
note sparsity constant and sparsity constraint for the auto-encoder
will be q̂j ¼ q. Without sparse cording, auto-encoder learns a low
dimensional representation similar to PAC. In order to account
sparsity we add an extra penalty term to the objective function
that penalizes q̂j deviating from q. For this purpose we use Kull-
back–Leibler (Bishop, 2007) divergence functions and it can be
written as follows:
KLðqjjq̂jÞ ¼ q log
q
q̂j
 
þ ð1 qÞ log 1 q
1 q̂j
 
ð5ÞTherefore complete objective function of the sparse auto-enco-
der can be written as:
JðW;bÞ¼ 1
m
Xm
i¼1
1
2
jjhW;bðxðiÞÞxðiÞjj2
 " #
þ k
2
X2
i¼1
Xsl
i¼1
Xslþ1
j¼1
ðW ðlÞji Þ
2þb
Xs2
j¼1
KLðqjjbqÞ
ð6Þ
where, b represents weight of the sparsity penalty term and s2 rep-
resents the number of processing units in the hidden layer of the
auto-encoder.
We train the sparse auto-encoder using the Backpropagation
algorithm (Bishop, 2007) to minimize the squared reconstruction
error of the cost function given in the Eq. (6). Sparse auto-encoder
consists of several hyper-parameters and optimum values for these
parameters were selected by using a cross-validation data set.
3.3. Author identification using logistic regression
For source code author classification, we trained the logistic
regression classifier (Bishop, 2007). Features learned from the
sparse auto-encoder are used as inputs for the logistic regression
and it returns whether a given source code file belongs to a partic-
ular author or not. Since logistic regression is a binary classifier, we
trained k number of classifiers where k represents number of
authors in a given data set.
Let xð1Þ; yð1Þ
 
; xð2Þ; yð2Þ
 
; . . . ; xðmÞ; yðmÞ
  
represents training
examples and yðiÞ 2 f0;1g. Hypothesis of logistic regression is given
by the Eq. (7).
hhðxÞ ¼
1
ð1þ expðhT xÞÞ
ð7Þ
where h 2 Rn represents parameters of the logistic regression and n
is the number of features we feed into the logistic regressions. In
our research n represents number of features extracted from the
sparse auto-encoder. Cost function of the logistic regression is given
by the Eq. (8).
JðhÞ¼ 1
m
Xm
i¼1
yðiÞ log hh xðiÞ
  
 1yðiÞ
 
log 1hh xðiÞ
   " #
þ k
2m
Xn
j¼1
h2j
ð8Þ
where m represents the number examples in the training set and k
represent the regularization parameter of the cost function. We
minimized J(h) with respect to h in order to get the parameters of
the logistic regression.
Table 5
Classification accuracy (%) of SCAP method on five datasets described in Table 3.
Profile
size
n-Gram size Profile
size
n-Gram size
3 4 5 3 4 5
Dataset I 500 87.26 85.59 85.32 Dataset
IV
500 80.65 83.87 80.65
1000 89.75 91.41 91.68 1000 80.65 83.87 83.87
1500 88.09 88.08 87.25 1500 87.10 83.87 83.87
2000 90.58 88.36 89.47 2000 90.32 83.87 83.87
2500 92.52 90.58 90.02 2500 87.10 87.10 87.10
3000 90.31 90.30 90.30 3000 87.09 87.10 87.09
Dataset
II
500 90.68 91.10 91.10 Dataset
V
500 89.63 89.63 91.11
1000 91.10 91.10 91.10 1000 91.11 91.11 91.85
1500 91.10 94.07 94.91 1500 93.33 92.59 93.33
2000 91.53 94.07 94.92 2000 93.33 92.59 94.07
2500 91.52 93.64 94.06 2500 94.07 94.07 93.33
3000 91.51 93.64 94.06 3000 94.07 93.33 93.33
Dataset
III
500 82.27 89.36 88.65
1000 83.69 85.81 87.23
1500 75.18 73.76 82.26
2000 88.65 87.94 89.36
2500 90.78 88.65 90.07
3000 90.78 85.81 90.78
Fig. 2. Classification accuracy of cross validation dataset as a function of hyper-parameters of the system. (a)–(q) cross validation accuracy vs. number of iterations for
different training batch sizes (r) cross validation accuracy vs. training batch size.
Table 4
Source code author identification accuracy on five datasets mentioned in Table 3.
System Dataset I Dataset II Dataset III Dataset IV Dataset V
Lange and
Mancoridis
(2007)
55.00% – – – –
Bandara and
Wijayarathna
(2011)
86.64% – – – –
This paper 92.82% 93.64% 90.78% 77.42% 89.62%
U. Bandara, G. Wijayarathna / Pattern Recognition Letters 34 (2013) 330–334 3334. Experiments
In order to gauge the performance of our system, experiments
were conducted by using five datasets and details about these
datasets are given below.
Dataset I is the same dataset used by Lange and Mancoridis
(2007) and Bandara and Wijayarathna (2011). It consists of source
code files belonging to 10 authors extracted from Sourceforge1
website. We created Dataset II by extracting Java source code files
from free and open source project in the Internet.2,3,4,5,6,7,8,9,10,11 It
consists of source code files belonging to 10 different authors. We
created Dataset III by using the source code files shipped with Java
Development Kit12 Dataset IV was created by collecting Java source
code files downloaded from Plant Source Code web site.13 These
source code files belonging to five authors and each source code1 http://jfm.sourceforge.net [Date of Access: 2012, March 12].
2 http://jfm.sourceforge.net [Date of Access: 2012, March 12].
3 http://www.jbasic.net/JBasic.net [Date of Access: 2012, March 15].
4 http://jackcess.sourceforge.net [Date of Access: 2012, March 12].
5 http://java-ml.sourceforge.net [Date of Access: 2012, April 07].
6 http://www.ee.ucl.ac.uk/~mflanaga/java/ [Date of Access: 2012, April 10].
7 http://sisc-scheme.org/ [Date of Access: 2012, March 12].
8 http://www.mcmanis.com/chuck/java/cocoa/index.html [Date of Access: 2012
March 27].
9 http://www.galagosearch.org/ [Date of Access: 2012, March 22].
10 http://jaskell.codehaus.org/ [Date of Access: 2012, March 14].
11 http://jatha.sourceforge.net/ [Date of Access: 2012, March 18].
12 http://www.oracle.com/technetwork/java/javase/downloads/index.html [Date o
Access: 2012, April 02].
13 http://www.planet-source-code.com/ [Date of Access: 2012, August 01]. 14 http://www.apress.com/ [Date of Access: 2012, August 03].,
f
was written by exactly one author. We created Dataset V by collect-
ing source codes, which are freely available with Java related pro-
gramming books published by Apress Inc.14 Each book in the
dataset is authored only by a single author. Detailed information
of datasets is provided in Table 3.
First dataset was divided into three subsets called training,
cross validation, and testing. Other datasets were divided into
training and testing subsets. The system consists of several hy-
per-parameters as enumerated below and these parameters were
estimated by cross validation dataset.
1. Number of features learned using the auto-encoder.
2. Weight decaying parameter of the auto-encoder.
3. Sparsity parameter of the auto-encoder.
4. Weight decaying parameter of sparsity penalty term of the
auto-encoder.
334 U. Bandara, G. Wijayarathna / Pattern Recognition Letters 34 (2013) 330–3345. Weight decaying parameter of the logistic regression.
Bergstra and Bengio (2012) have shown that random search for
hyper-parameter optimization is more effective than manual and
grid search. Therefore, we adopted to random search as the method
of optimizing above hyper-parameter.
Initial training sample size was 50 files, where each author has
five source code files. Training sample size was increased up to 850
files. For each training sample, we ran our system 50 times and
tested accuracy by using cross validation dataset. Cross validation
accuracies are plotted as scatter graphs per training size as shown
in graphs (a)–(q) in the Fig. 2. In each training size, we identified
best cross validation accuracy and relevant hyper-parameter val-
ues and plotted the best cross validation accuracy vs training size
as detected in graph (r) in the Fig. 2. We used graph (r) in Fig. 2
as a tool for selecting training subset size for datasets.
Finally, we evaluated our system with testing subset of each
dataset and predicting accuracies are shown in Table 4. In order
to compare performance of our system with previously publised
systems, we implemented source code author profiles (SCAP) sys-
tem as described by Frantzeskou et al. (2006). The selection of
SCAP method was due to, its declared high accuracy. Classification
accuracies of the SCAP method on five datasets are given in the
Table 5.
According to Tables 4 and 5, performance of our system is very
close to the SCAP method specially, when training samples are rea-
sonably large. However, when training samples are small SCAP
outperforms our system.
5. Conclusion and future works
This paper investigated applicability of sparse auto-encoder as
an unsupervised feature learning technique for source code author
identification. We tested our system with five datasets. Further, we
implemented SCAP system and tested with same datasets. Our sys-
tem’s performance was very close to SCAP’s performance for large
datasets. However, SCAP outperformed our system for small
datasets.
We used sparsed auto-encoder for feature extraction. However
it is interesting to investigate performance of other unsupervised
feature learning algorithms in source code author identification
context. Our system assumes that, a source code file is written
by a single author. If it is written by more than one authors, system
is not capable of classifying it. In the future we will improve our
system in order to handle source code files written by more than
one author.References
Bandara, Upul, Wijayarathna, Gamini, 2011. A machine learning based tool for
source code plagiarism detection. Internat. J. Machine Learn. Comput. 1 (4),
337–343.
Bengio, Y., 2009. Learning deep architectures for AI. Found. Trends Machine Learn.
2 (1), 1–127.
Bengio, Yoshua, Delalleau, Olivier, 2011. On the expressive power of deep
architectures. In: 14th Internat. Conf. on Discovery Science. Springer-Verlag.
Bergstra, James, Bengio, Yoshua, 2012. Random search for hyper-parameter
optimization. J. Machine Learn. Res., 281–305.
Bishop, Christopher M., 2007. Pattern Recognition and Machine Learning. Springer.
Burrows, Steven, Tahaghoghi, Seyed M.M., 2007. Source code authorship attribution
using N-grams. In: Spink, Amanda, Turpin, Andrew, Wu, Mingfang (Eds.), Source
Code Authorship Attribution using N-Grams. Melbourne, MelbouAustraliarne,
pp. 32–39.
Chao, Liu, Chen, Chen, Jiawei, Han, Philip, Yu, 2006. GPLAG: Detection of software
plagiarism by program dependence graph analysis. In: 2nd ACM SIGKDD
Internat. Conf. on Knowledge Discovery and Data Mining. ACM, Philadelphia,
pp. 872–888.
Coates, A. et al., 2011. Text detection and character recognition in scene images
with unsupervised feature learning. In: 11th Internat. Conf. on Document
Analysis and Recognition, Beijing, pp. 440–445.
Coates, Adam, Lee, Honglak, Ng, Andrew, 2010. An analysis of single-layer networks
in unsupervised feature learning. In: 14th Internat. Conf. on Artificial
Intelligence and, Statistics.
Elenbogen, Bruce S., Seliya, Naeem, 2008. Detecting outsourced student
programming assignments. J. Comput. Sci. Coll. 23 (3), 50–57.
Frantzeskou, Georgia, Gritzalis, Efstathios, Stamatatos, Efstathios, Katsikas, Sokratis,
2006. Source Code Author Identification Based on N-Gram Author Profiles.
Artificial Intelligence Applications and Innovations, vol. 204. Springer, Boston,
pp. 508–515.
Hinton, G.H., Salakhutdinov, R.R., 2006. Reducing the dimensionality of data with
neural networks. Science, 504–507.
Keselj, V., Peng, F., Cercone, N., Thomas, C., 2003. N-gram-based author profiles
for authorship attribution. In: Pacific Association for Computational
Linguistics.
Lange, Robert Charles, Mancoridis, Spiros, 2007. Using code metric histograms and
genetic algorithms to perform author identification for software forensics. In:
GECCO ’07. ACM, New York, NY, USA, pp. 2082–2089.
Lee, H., Largman, Y., Pham, P., Ng, A., 2009. Unsupervised feature learning for audio
classification using convolutional deep belief networks. Adv. Neural Inform.
Process. Systems, 1096–1104.
Raina, R., Battle, A., Lee, H., Packer, B., Ng, A., 2007. Self-taught learning: Transfer
learning from unlabeled data. In: 24th Internat. Conf. on Machine Learning.
ACM, pp. 759–766.
Ranzato, Marc’Aurelio, Poultney, Christopher, Chopra, Sumit, Lecun, Yann, 2006.
Efficient learning of sparse representations with an energy-based model. MIT
Press, pp. 1137–1144.
Rumelhart, E., Hinton, G., Williams, R., 1986. Learning internal representations by
error propagation. In: Parallel Distributed Processing: Explorations in the
Microstructure of Cognition, pp. 318–362.
Shevertalov, Maxim, Kothari, Jay, Stehle, Edward, Mancoridis, Spiros, 2009. On the
use of discretized source code metrics for author identification. In: Proc. 2009
1st Internat. Symposium on Search Based Software Engineering. IEEE Computer
Society, Washington, DC, USA, pp. 69–78.
Zobel, Justin, 2004. Uni cheats racket: A case study in plagiarism investigation.
In: ACE ‘04 Proc. 6th Australasian Conf. on Computing, Education, pp. 357–
365.
