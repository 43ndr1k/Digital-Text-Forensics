1
3
4 Q1
5 Q2
6
7
8
9
10
11
13 Q4
14
15
16
17
18
19
Q3
Journal of King Saud University – Computer and Information Sciences (2014) xxx, xxx–xxx
JKSUCI 133 No. of Pages 12
29 September 2014
Q1King Saud University
Journal of King Saud University –
Computer and Information Sciences
www.ksu.edu.sa
www.sciencedirect.comNaı̈ve Bayes classifiers for authorship attribution
of Arabic texts* Corresponding author. Tel.: +966 504 882499.
E-mail addresses: atheneyan@ksu.edu.sa (A.S. Altheneyan),
menai@ksu.edu.sa (M.E.B. Menai).
1 Tel.: +966 1 4670687.
Peer review under responsibility of King Saud University.
Production and hosting by Elsevier
http://dx.doi.org/10.1016/j.jksuci.2014.06.006
1319-1578  2014 Production and hosting by Elsevier B.V. on behalf of King Saud University.
Please cite this article in press as: Altheneyan, A.S., Menai, M.E.B. Naı̈ve Bayes classifiers for authorship attribution of Arabic texts. Journal of King Saud Un
– Computer and Information Sciences (2014), http://dx.doi.org/10.1016/j.jksuci.2014.06.006Alaa Saleh Altheneyan a,1, Mohamed El Bachir Menai b,*aDepartment of Information Technology, College of Computer and Information Sciences, King Saud University,
P.O. Box 89638, Riyadh 11692, Saudi Arabia
bDepartment of Computer Science, College of Computer and Information Sciences, King Saud University,
P.O. Box 51178,
Riyadh 11543, Saudi ArabiaKEYWORDS
Authorship attribution;
Arabic language;
Naı̈ve Bayes classifier;
Event modelAbstract Authorship attribution is the process of assigning an author to an anonymous text based
on writing characteristics. Several authorship attribution methods were developed for natural lan-
guages, such as English, Chinese and Dutch. However, the number of related works for Arabic is
limited. Naı̈ve Bayes classifiers have been widely used for various natural language processing tasks.
However, there is generally no mention of the event model used, which can have a considerable
impact on the performance of the classifier. To the best of our knowledge, naı̈ve Bayes classifiers
have not yet been considered for authorship attribution in Arabic. Therefore, we propose to study
their use for this problem, taking into account different event models, namely, simple naı̈ve Bayes
(NB), multinomial naı̈ve Bayes (MNB), multi-variant Bernoulli naı̈ve Bayes (MBNB) and multi-
variant Poisson naı̈ve Bayes (MPNB). We evaluate these models’ performances on a large Arabic
dataset extracted from books of 10 different authors and compare them with other existing meth-
ods. The experimental results show that MBNB provides the best results and could attribute the
author of a text with an accuracy of 97.43%. Comparison results with related methods indicate that
MBNB and MNB are appropriate for authorship attribution.
 2014 Production and hosting by Elsevier B.V. on behalf of King Saud University.20
21
22
23
24
25
26
27
28
291. Introduction
Authorship attribution is a subfield of authorship analysis. It is
the process of attributing the author of an anonymous text
based on its characteristics (Juola et al., 2006). This problem
has a long history; studies of authorship attribution can be
traced back to the 19th century. The early traditional
approaches were human expert-based, then from 1964 up until
the 1990s, the non-traditional authorship attribution studies
were performed. The focus of research at that time was on defin-
ing features that measure the writing style of authors. In recentiversity
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
2 A.S. Altheneyan, M.E.B. Menai
JKSUCI 133 No. of Pages 12
29 September 2014
Q1years, the development in fields such as information retrieval,
machine learning and natural language processing has had a
great impact on authorship attribution studies (Stamatatos,
2009). Authorship attribution can be used in a broad range of
applications in diverse areas, including intelligence, criminal
and civil law, computer forensics, and cybercrime investigation
as well as in the traditional application to literary research.
A large number of methods have been developed to tackle
the authorship attribution problem. These methods can be
divided into three classes based on their approach: the unitary
invariant approach, multivariate analysis and machine learn-
ing approach (Koppel et al., 2009). These methods rely on
the linguistic devices used unconsciously by authors, such as
semantic, syntactic, lexicographic, orthographic and morpho-
logical devices. Although the Arabic language is one of the
official languages of the United Nations and is widely used
by hundreds of millions of people, only a very small number
of authorship attribution studies have been published for Ara-
bic texts so far (Shaker and Corne, 2010).
Naı̈ve Bayes classifiers have been used for authorship attri-
bution in many languages, including English (Hoorn et al.,
1999; Zhao and Zobel, 2005; Tan and Tsai, 2010; Pillay and
Solorio, 2010), Turkish (Türkoğlu et al., 2007), and Mexican
(Coyotl-Morales et al., 2006)). However, there is generally
no mention of the event model used. Naı̈ve Bayes classifiers
have also been used for Arabic text classification (El Kourdi
et al., 2004; Al-Salemi, 2011; Al-Shammari, 2010; Alsaleem,
2011; Noaman et al., 2010). The results provided by the classi-
fier were encouraging.
In this paper, we propose to investigate the naı̈ve Bayes event
models for Arabic authorship attribution because they have not
been considered for this problem before. Four naı̈ve Bayes event
models are examined in this study, namely, the simple naı̈ve
Bayes (NB), multinomial naı̈ve Bayes (MNB), multi-variant
Bernoulli naı̈ve Bayes (MBNB) andmulti-variant Poisson naı̈ve
Bayes (MPNB). The rest of this paper is organized as follows.
Section 2 presents a general overview of authorship attribution
and writing style features. In Section 3, characteristics of the
Arabic language are discussed. In Section 4, an extensive study
of the different authorship attribution methods is provided
along with a study of the available feature selection methods.
In Section 5, the naı̈ve Bayes event models are described. In Sec-
tion 6, the Arabic authorship attribution system is detailed. Sec-
tion 7 presents and discusses experimental results. Finally, a
general conclusion of this work is presented in Section 8.
2. Background
2.1. Authorship attribution
Authorship attribution addresses the problem of determining
the author of an anonymous text from a set of candidate
authors based only on internal characteristics of the text. It fits
a typical text classification problem where each author repre-
sents a class (Koppel et al., 2009). The main key research top-
ics in authorship attribution are feature selection and
attribution techniques.
2.2. Writing style features
Writing style features are extracted text characteristics that assist
the attribution of texts (Abbasi and Chen, 2005a). According toPlease cite this article in press as: Altheneyan, A.S., Menai, M.E.B. Naı̈ve Bayes class
– Computer and Information Sciences (2014), http://dx.doi.org/10.1016/j.jksuci.2014authorship attribution studies, taxonomies of many feature sets
exist, which can be categorized into: lexical, character, syntactic,
semantic, content-specific, structural and language-specific
(Abbasi and Chen, 2005a,b; Stamatatos, 2009).
 Lexical: Lexical features are one of the earliest and most tra-
ditional features used for attributing authorship. Examples
of these features are word length, sentence length, word fre-
quencies, and vocabulary richness. One main problem with
lexical features is that in some oriental languages (e.g., Chi-
nese), there are no boundaries separating words, making it
hard to apply these measures without requiring special tools.
 Character: Character-based measures treat texts as a
sequence of characters. There are several measures, such
as character type, letter frequencies and character n-grams.
The significance of character n-gram measures is that they
can capture lexical information and contextual information.
They can be applied to any language easily without requiring
any special tools. However, the dimensionality of the repre-
sentation is very high compared to the lexical approach
because of redundant information (e.g., |or_|, |_or|).
 Syntactic: Syntactic features are used by authors uncon-
sciously, which makes them more reliable than lexical fea-
tures. Different syntactic measures were used in
attribution studies, including part-of-speech (POS) frequen-
cies, rewrite rule frequencies, syntactic errors and function
words. These features require accurate language dependent
tools to extract them.
 Semantic: Current natural language processing (NLP) tools
for handling semantic analysis are not sufficient. As a result,
only a few attempts to exploit semantic features have been
performed. These features include semantic dependencies,
synonyms and the most significant systemic functional lin-
guistics (SFL), which define functional words summed with
POS features.
 Content-specific: Content-specific features are used when
the available texts for all authors are of the same content.
Content-specific words are key words of a particular topic
that can be used to aid other stylistic features.
 Structural: These features capture the habits of an author
when organizing a text. They were defined as a result of
applying authorship attribution to emails and online forum
messages. Examples of these measures include paragraph
length, use of signature, font color and font size. The struc-
tural features are significant when attributing short texts
because it is hard to capture stylistic properties of the text.
 Language-specific: These features are specific for a particu-
lar language. Measures for these features have to be defined
manually.
Lexical, character, syntactic and semantic features can be
extracted fromany text independentof the applicationor text lan-
guage by using the appropriate tools. According to attribution
studies, lexical and syntactic features are the most used features
for attribution (Abbasi and Chen, 2005a; Stamatatos, 2009).
3. Arabic characteristics
The complex linguistic structure of the Arabic language intro-
duces several challenges: inflection, diacritics, word length, and
elongation.ifiers for authorship attribution of Arabic texts. Journal of King Saud University
.06.006
145
146
147
148
149 Q5
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
Naı̈ve Bayes classifiers for authorship attribution of Arabic texts 3
JKSUCI 133 No. of Pages 12
29 September 2014
Q1 Inflection
Arabic is a highly inflectional language. Stems are derived
from roots by adding affixes (prefixes, infixes and suffixes).
Words are a result of adding affixes to stems (e.g., root:
بتك stem: بتكم word: بتاكملا ) (de Roeck and Al-Fares, 2000).
Inflection increases the number of words, which might cause
particular problems when extracting lexical features, e.g., some
vocabulary richness measures will not be that effective (Abbasi
and Chen, 2005a).
 Diacritics
Diacritics are special marks placed above or below letters to
represent short vowels. The use of diacritics changes both the
pronunciation and meaning of words. However, diacritics are
rarely used in writings because the readers are expected to infer
the missing short vowels using their semantic knowledge of the
language. However, for computers, it is not possible for feature
extraction programs to infer this knowledge.
This might reduce the effectiveness of using function words
as a feature. For example, without using diacritics, the func-
tion words (man) and (men) are identical, and comput-
ers cannot distinguish between them (Abbasi and Chen, 2005a;
Farghaly and Shaalan, 2009).
 Word length
Arabic words tend to be short. This might reduce the effec-
tiveness of lexical features, such as word length distribution
(Abbasi and Chen, 2005a).
 Elongation
Elongation is the use of a special dash between two letters
in Arabic writing for purely stylistic reasons. Although elonga-
tion can be used as a significant attribution feature, it causes a
problem when extracting lexical features, especially the word
length feature because some word lengths double after using
elongation. For example, the word is a four-letter word.
After the addition of four dashes, the elongated word is
eight letters (Abbasi and Chen, 2005a).
4. Literature review
4.1. Feature selection
One of the major issues that need to be considered when tack-
ling an authorship attribution problem is the high dimension-
ality of the feature set, especially when using lexical and
character features because every word and phrase represents
a feature. Feature selection is essential to reduce the feature
set, speed up the computation and improve the classification
process (Yang and Pedersen, 1997; Forman, 2003). Feature
selection methods can be classified into two main approaches:
wrappers and filters.
Wrappers select feature subsets using classical search
methods in artificial intelligence (e.g., hill-climbing and beam
search) that explore the search space for appropriate feature
subsets. Each subset is evaluated using the induction algo-
rithm, which is a time-consuming operation. Therefore,255
Please cite this article in press as: Altheneyan, A.S., Menai, M.E.B. Naı̈ve Bayes classi
– Computer and Information Sciences (2014), http://dx.doi.org/10.1016/j.jksuci.2014wrappers are unpractical for large-scale problems (Forman,
2003).
Filters methods use feature scoring measures to score each
feature independently. The feature subset is then formed by
choosing a predefined number of the best features. A number
of effective feature scoring measures are used with texts such
as: chi-squared v2 (CHI), document frequency (DF), informa-
tion gain (IG), term strength (TS), mutual information (MI),
odd ratio (OR), cross entropy (CE), Weight Of Evidence
(WOE), random, Ng-Goh-Low (NGL) coefficient, Gala-
votti–Sebastiani–Simi (GSS) coefficient, and term frequency–
inverse document frequency (TF–IDF) (Forman, 2003).
4.2. Authorship attribution approaches
Authorship attribution methods fall into three main classes:
unitary invariant, multivariate analysis and machine learning
classes.
4.2.1. Unitary invariant
Unitary invariant is the oldest approach used to attribute the
author of a text. It uses a single textual feature to discriminate
between authors, such as sentence length and word length
(Koppel et al., 2009). Mendenhall (1887) used curves that repre-
sent word length frequencies to attribute text toMarlowe, Bacon
or Shakespeare. Yule (1939) examined the authorship of De
Imitatione Christi (a published religious treatise in 1418) and
Observations upon the Bills of Morality (an economic writing
believed to have been written by either John Graunt or Sir
William Petty) using sentence length. Brinegar (1963) also used
word length frequencies for the attribution of the Quintus Cur-
tius Snodgrass Letters (10 letters published in the New Orleans
Daily Crescent in 1861). None of these methods provided reli-
able results, which gave way to multivariate analysis methods.
4.2.2. Multivariate analysis
The multivariate analysis method uses a set of features to sta-
tistically attribute texts. Mosteller and Wallace (1964) initiated
the use of this method by proposing the use of Bayesian statis-
tical analysis to attribute the Federalist Papers (a number of
political newspaper essays written by John Jay, Alexander
Hamilton, and James Madison; both Hamilton and Madison
claimed that they wrote 12 of these essays). Their method
based on the most frequent function words provided reliable
results, which encouraged scholars to explore other types of
features and techniques.
Principal component analysis (PCA) (Pearson, 1901) is a
statistical analysis method that uses as few features as possible
to examine the variation in texts. It was used for the author-
ship attribution of many disputed documents (Binongo and
Smith, 1999; Holmes et al., 2001a,b; Baayen et al., 2002;
Binongo, 2003). Linear discriminant analysis (LDA) (Fisher,
1936) is another statistical method used for attribution
(Baayen et al., 1996; Stamatatos et al., 2000; Baayen et al.,
2002; Chaski, 2005).
Distance-based methods attribute the author of an anony-
mous text by measuring the distance between the anonymous
text and the available documents written by the candidate
authors using some distance measures (Burrows, 2002; Keselj
et al., 2003; Hoover, 2004; Juola, 2005; Zhao et al., 2006; Zhao
and Zobel, 2007; Zhao and Vines, 2007; Koppel et al., 2010).fiers for authorship attribution of Arabic texts. Journal of King Saud University
.06.006
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281 Q6
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
370
4 A.S. Altheneyan, M.E.B. Menai
JKSUCI 133 No. of Pages 12
29 September 2014
Q1Other statistical techniques based on Markov chains were
used for authorship attribution (Khmelev and Tweedie, 2001;
Kukushkina et al., 2001). Data compression techniques were
also considered, including the Lempel and Ziv (LZ77) com-
pression method used by Benedetto et al. (2002) for authorship
attribution, the prediction by partial matching (PPM) text
compression scheme used by Teahan and Harper (2003) for
text categorization, and the R-measure based method pro-
posed by Khmelev and Teahan (2003) for plagiarism detection
and text categorization.
4.2.3. Machine learning methods
Supervised machine learning methods are applied on training
documents represented as vectors of features to build classifiers
that attribute anonymous documents. Various machine learn-
ing methods have been used for authorship attribution such as
naı̈ve Bayes (Hoorn et al., 1999; Zhao and Zobel, 2005;
Coyotl-Morales et al., 2006; Türkoğlu et al., 2007; Tan and
Tsai, 2010; Pillay and Solorio, 2010), Bayesian classifiers
(Kjell, 1994; Zhao and Zobel, 2005; Zhao et al., 2006; Pillay
and Solorio, 2010), K-nearest neighbor (Hoorn et al., 1999;
Zhao and Zobel, 2005; Türkoğlu et al., 2007), decision trees
(Zheng et al., 2003; Zhao and Zobel, 2005; Zheng et al.,
2006; Türkoğlu et al., 2007; Pillay and Solorio, 2010), neural
networks (Hoorn et al., 1999; Zheng et al., 2003; Zhao and
Zobel, 2005; Zheng et al., 2006; Türkoğlu et al., 2007) and sup-
port vector machines (SVM) (Diederich et al., 2003; Zheng
et al., 2003; Argamon and Levitan, 2005; Sanderson and
Guenter, 2006; Zhao et al., 2006; Zheng et al., 2006;
Türkoğlu et al., 2007; Pavelec et al., 2007).
4.3. Authorship attribution of Arabic text
Abbasi and Chen (2005a) used support vector machine (SVM)
and C4.5 decision trees on political and social Arabic web
forum messages from Yahoo groups for authorship analysis.
They preprocessed the texts before extracting the features to
remove elongation using an elongation filter; however, the
number of elongation characters and elongated words were
calculated to be used later as features. The feature set used
by Abbasi and Chen (2005a) consists of 410 features including
lexical features such as frequent roots and sentence length, syn-
tactic features such as function words and structural features
such as the number of attachments and content-specific fea-
tures. These features were partitioned into different sets for
testing as follows:
set1: Lexical features
set2: Lexical + syntactic features
set3: Lexical + syntactic + structural features
set4: Lexical + syntactic + structural + content-specific
features
A cluster algorithm by de Roeck and Al-Fares (2000) was
used to extract the roots and to use them as features. In each
experiment, five authors were selected and for each author,
20 texts were used. The best results were achieved when
SVM and set4 features were used.
Abbasi and Chen (2005b) also used SVM and C4.5 with
lexical, syntactic, structural and content-specific features. They
also used a filter to remove elongations and the clusterPlease cite this article in press as: Altheneyan, A.S., Menai, M.E.B. Naı̈ve Bayes class
– Computer and Information Sciences (2014), http://dx.doi.org/10.1016/j.jksuci.2014algorithm of de Roeck and Al-Fares (2000) to extract root
words. They tested their method on English and Arabic web
forum messages. The Arabic set was extracted from a Yahoo
group forum for the Al-Aqsa Martyrs group using four differ-
ent sets as in Abbasi and Chen (2005a). For each experiment,
five authors with 20 texts for each were used. The best average
precision obtained was 94.83% for Arabic and 97.00% for
English, when all four sets of features were used with SVM.
Abbasi and Chen (2006) used SVM and writeprint, an
authorship visualization, which creates patterns for different
author writing styles using a number of documents written
by them. They tested their method on the same data set used
by Abbasi and Chen (2005b), which consists of a group of
10 messages for each author. Writeprint outperformed SVM
when testing the attribution of a group of messages written
by one author. However, SVM performed better when testing
the attribution of a single message.
Stamatatos (2008) tested the use of SVM on Arabic news-
paper reports from an Alhayat newspaper. The aim of the
study was to propose a solution for the class imbalance prob-
lem: some authors have long and diverse training documents,
while others have only a few short documents. He concluded
that the best results are obtained when the method uses many
short texts for some authors and a few long texts for the
others.
Shaker and Corne (2010) used linear discriminant analysis
(LDA) for the attribution of 12 Arabic books. They used func-
tion words as a feature and started with 104 words of common
conjunctions and prepositions. Then, they built their data set
based on the English set used by Mosteller and Wallace
(1964); however, only 64 words were used because they omit-
ted the forty most frequently used words from the set. For
the selection of function words, an evolutionary search was
used to choose subsets of function words. Two authors were
selected for each of the experiments conducted. For each
author, two books were selected: one for testing and the other
for training. The books were divided into 1000-word chunks
for the first experiment and 2000-word chunks for the second
experiment with both 65 and 54 function words. The best per-
formance obtained was 87.63% accuracy, when 2000-word
chunks and 54 function words were used.
5. Naı̈ve Bayes models for arabic authorship attribution
Let a;A; f; and n denote an author, the total number of
authors, a feature, and the total number of features, respec-
tively. For the naı̈ve Bayes classifier, a set of training docu-
ments is provided for each author a 2 A. Each document is
represented by a set of features ff1; f2; . . . ; fng. A new docu-
ment is described by the same set of features ff1; f2; . . . ; fng,
and the learner is asked to predict the author of the new doc-
ument, assuming that the occurrences of the features are mutu-
ally independent (Mitchell, 1997).
5.1. Simple naı̈ve Bayes
The simple naı̈ve Bayes classifier (NB) attributes a new docu-
ment with a set of features ff1; f2; . . . ; fng to the most probable
target author a according to Eq. (1).
a ¼ argmaxa2APðajf1; f2; . . . ; fnÞ ð1Þifiers for authorship attribution of Arabic texts. Journal of King Saud University
.06.006
371
372
373
375
376
377
378
379
381
382
383
385
386
387
389
390
391
392
394
395
396
397
399
400
401
402
403
405
406
407
408
409
410
411
412
413
414
415
416
417
418
419
420
421
422
424
425
426
427
429
430
431
432
433
434
435
436
437
438
439
440
441
442
444
445
446
447
448
449
450
451
452
453
454
455
456
458
459
460
461
462
463
464
466
467
468
469
470
471
472
473
474
475
476
477
478
479
480
481
482
483
Naı̈ve Bayes classifiers for authorship attribution of Arabic texts 5
JKSUCI 133 No. of Pages 12
29 September 2014
Q1The probability Pðajf1; f2; . . . ; fnÞ needs to be calculated for
each a 2 A using the following Bayes formula:
Pðajf1; f2; . . . ; fnÞ ¼
Pðf1; f2; . . . ; fnjaÞ  PðaÞ
Pðf1; f2; . . . ; fnÞ
ð2Þ
where Pðf1; f2; . . . ; fnÞ–0:
Assuming the uniformity of ðf1; f2; . . . ; fnÞ, Eq. (2) can be
simplified into Eq. (3).
Pðajf1; f2; . . . ; fnÞ ¼ Pðf1; f2; . . . ; fnjaÞ  PðaÞ ð3Þ
By using the chain rule, we obtain:
Pðf1; f2; . . . ; fnjaÞ  PðaÞ ¼ PðaÞ:
Yn
i¼1
PðfijaÞ ð4Þ
Therefore, an author a is attributed according to Eq. (5)
a ¼ argmaxa2APðaÞ
Yn
i¼1PðfijaÞ ð5Þ
where the probability PðaÞ is estimated by the frequency of a in
the training data.
PðaÞ ¼ number of documents written by a
total number of documents
ð6Þ
PðfijaÞ can be estimated using a Gaussian distribution
(Zhao and Zobel, 2005) or Laplacian prior (Al-Salemi, 2011):
PðfijaÞ ¼ gðfi; li; riÞ
gðfi; li; riÞ ¼ 1ffiffiffiffiffiffi
2pri
p e
ðfiliÞ2
2r2 ðr > 0Þ ð7Þ
where li is the mean value of feature fi in documents written by
author a and ri is its standard deviation.
The Laplacian prior is given by Eq. (8)
PðfijaÞ ¼
1þDai
AþDa ð8Þ
where Dai is the total number of documents written by a and
containing fi and Da is the total number of documents written
by a Absent features can cause zero probabilities, which mis-
lead the classifier. To overcome this problem, the number of
documents Dai is primed with a count of one using a Laplacian
prior. Continuous features such as word length, vocabulary
richness and sentence length can only be calculated using a
Gaussian distribution.
5.2. Multinomial naı̈ve Bayes
The multinomial model captures feature frequency informa-
tion (Yang and Liu, 1999). So, instead of representing a docu-
ment as a set of features ff1; f2; . . . ; fng, such as in the simple
model, the document is represented as a vector
v ¼ v1; v2; . . . ; vn; where vi is the frequency of fi in the docu-
ment. So, the new document is attributed to the most probable
target author a according to Eq. (9).
a ¼ argmaxa2APðaÞ
Yn
i¼1
PðvijaÞ ð9Þ
The probability PðvijaÞ is calculated using Eq. (10)
(Manning et al., 2008),
PðvijaÞ ¼ 1þ via
nþ na ð10Þ
where mia is the frequency of feature fi in documents written by
author a, na, is the total number of features in documents writ-Please cite this article in press as: Altheneyan, A.S., Menai, M.E.B. Naı̈ve Bayes classi
– Computer and Information Sciences (2014), http://dx.doi.org/10.1016/j.jksuci.2014ten by author a and a Laplacian prior is used to prime feature
frequency with one to avoid the zero probability problem.
5.3. Multi-variant Bernoulli naı̈ve Bayes
The multi-variant Bernoulli naı̈ve Bayes model is similar to the
multinomial model, but instead of representing the document
as a frequency vector, it is represented as a binary vector
(Al-Salemi, 2011) b ¼< b1; b2; . . . ; bn >. If fi occurs in the doc-
ument, then bi ¼ 1; otherwise, bi ¼ 0. A new document is
attributed to the most probable target author a according to
Eq. (11).
a ¼ argmaxa2APðaÞ
Yn
i¼1ðbiPðfijaÞ þ ð1 biÞð1 PðfijaÞÞÞ
ð11Þ
The probability PðfijaÞ is calculated using Eq. (8).
5.4. Multi-variant Poisson naı̈ve Bayes
The Poisson statistical distribution is commonly used for mod-
eling random events in a fixed unit of time. Poisson distribu-
tion has been used for text classification in English (Kim
et al., 2006; Huang and Li, 2011). A document is represented
as a random vector x ¼ x1; x2; . . . ; xn; where xi is a Poisson
random variable assigned the value vi from within the term-fre-
quency of feature fi (Kim et al., 2006). The attribution of a new
document to the most probable target author a is given by Eq.
(12).
a ¼ argmaxa2APðaÞ
Yn
i¼1e
kiakviia ð12Þ
When using MNB, MPNB and MBNB, some features such
as word length are not suitable. Ideal features are ‘‘frequency-
based” features because documents are represented as fre-
quency and binary vectors.
The probability kai is calculated by Eq. (13),
kai ¼ c1þfai
c2 þDa ð13Þ
where c1; c2 2 ½0; 1.
6. Arabic authorship attribution system
In this section, we describe the main components of the system
that we implemented to test the four naı̈ve Bayes event models
for Arabic authorship attribution. The four main phases of the
authorship attribution process consist of preprocessing of the
texts, extraction of the features, selection of a sub-set of fea-
tures, and then training and attributing. Fig. 1 illustrates this
process.
6.1. Preprocessing
For preprocessing, the following steps were taken:
 Normalization: Normalization is used to help overcome the
variation in Arabic text representation. We chose the fol-
lowing normalization steps:
o Use CP1256 for text encoding.
o Replace إ,آ or أ with bare alif .ا
o Replace the sequence ءى with .ىfiers for authorship attribution of Arabic texts. Journal of King Saud University
.06.006
484
485
486
487
488
489
490
491
492
493
494
495
496
497
498
499
500
501
502
503
504
505
506
507
508
509
510
511
512
513
514
515
516
517
518
519
520
521
522
523
524
525
526
527
528
529
530
531
532
533
534
535
536
537
538
539
540
541
542
543
544
Figure 1 Authorship attribution of Arabic texts.
6 A.S. Altheneyan, M.E.B. Menai
JKSUCI 133 No. of Pages 12
29 September 2014
Q1o Replace final ى with .ي
o Replace final ة with .ه
We implemented our own preprocessing tool. Each docu-
ment in the training and test sets is preprocessed before
extracting its features.
 Function words, punctuation, diacritics and non-letter
removal: Non-letters, diacritics, punctuation and function
words (stop words) are kept because they can provide
authorial evidence.
 Elongation: Elongation can be used as a significant attribu-
tion feature, but it introduces a problem when extracting
lexical features, particularly the word length. To overcome
this problem, we implemented an elongation filter to extract
the number of elongations and the number of elongated
words. They are then used as features before removing
elongation.
 Stemming: Stemming is the process of finding roots for Ara-
bic words. Stemming methods are divided into root-based
and stem-based classes. Abbasi and Chen, 2005a,b used
the most common roots as features. They used the cluster-
ing algorithm of de Roeck and Al-Fares (2000) and a root
dictionary to extract the roots, while other features were
extracted from the original texts. Stamatatos (2008),
Shaker and Corne (2010) did not use any stemming prepro-
cessing. In our work, we used Khojah’s stemmer (http://
zeus.cs.pacificu.edu/shereen/research.htm) to extract the
roots of words.
6.2. Feature extraction
Documents are represented as numerical vectors of features.
We used a feature set similar to the one used by Abbasi and
Chen, 2005a,b, 2006, which has a total of 408 features forPlease cite this article in press as: Altheneyan, A.S., Menai, M.E.B. Naı̈ve Bayes class
– Computer and Information Sciences (2014), http://dx.doi.org/10.1016/j.jksuci.2014the simple naı̈ve Bayes model and 374 features for the other
models. Two hundred distinct words are used as features. A
complete description of the feature set is presented in Table 1.
The extraction of these features is performed in two steps: first,
all of the distinct words are extracted and then, the features
and 200 words are selected based on some feature selection
method.
6.3. Feature selection
For feature selection, we used term frequency feature selection
with the NB model because the calculation of the probability
for this model depends on the mean and standard deviation
of the features. Chi-squared is used for MNB, MBNB, and
MPNB because this measure provided good results when used
in Arabic text classification. Indeed, Al-Salemi (2011) used a
naı̈ve Bayes classifier with different feature selection methods
and showed that chi-squared provided the best result. Chi-
Squared also provided the best result among other feature
selection methods when tested by Mesleh (2008) with his
SVM classifier for Arabic text. Chi-squared was also used by
Al-Harbi et al. (2008) and Mesleh (2007).
6.4. Training and attributing
The four models NB, MNB, MBNB, and MPNB are trained
and tested on a large Arabic corpus for authorship attribution.
Their performance evaluation and comparison are detailed in
the next section.
7. Experimental evaluation
The authorship attribution system was implemented using the
JAVA programing language under the NetBeans IDE 6.9.1ifiers for authorship attribution of Arabic texts. Journal of King Saud University
.06.006
545
546
547
548
549
550
551
552
553
554
555
556
557
558
559
560
561
562
563
564
565
566
567
568
569
570
571
572
573
574
575
576
577
578
579
580
581
582
583
584
585
586
587
588
589
590
591
592
593
594
595
596
597
598
599
600
601
602
603
604
605
606
607
608
609
Table 1 Extracted features. Note that M = total number of words, V = total number of distinct words, and C = total number of
characters in a document.
Feature Number Feature extraction tool
Total number of characters (C) 0 Tokenizer
Total number of words (M) 1 Tokenizer
Frequency of elongation 2 Elongation filter
Frequency of elongated words 3 Elongation filter
Total number of punctuations 4 Character dictionary
Total number of whitespace characters fnt n n n f n r spaceg 5 Character dictionary
Total number of space characters/C 6 Character dictionary
Total number of space characters/number white-space characters 7 Character dictionary
Total number of tab spaces/C 8 Character dictionary
Total number of tab spaces/number white-space characters 9 Character dictionary
Total number of punctuations/C 10 Character dictionary
Number of blank lines/total number of lines 11 Character dictionary
Average sentence length 12–13 Sentence splitter
Frequency of punctuation (8 features)
‘‘,”,‘‘?”,‘‘.”, ‘‘:”, ‘‘!”,‘‘ ’ ”,‘‘ ” ”,”; ”
14–21 Character dictionary
Average word length (number of characters) 22 Tokenizer
Word length frequency distribution/M 23–37 Tokenizer
Total number of function words/M (from Kojah stemmer) 38 Tokenizer
Frequency of function word 39–202 Tokenizer
Total number of hapax legomena/V 203 Tokenizer
Total number of hapax legomena/M 204 Tokenizer
Vocabulary richness i.e., V/M 205–208 Tokenizer
200 distinct words 209–408 Tokenizer
Naı̈ve Bayes classifiers for authorship attribution of Arabic texts 7
JKSUCI 133 No. of Pages 12
29 September 2014
Q1environment on a personal computer with an Intel Core 2 Duo
CPU P8700 @2.53 GHz CPU, a 4-Gbyte RAM and a 32-bit
Windows Vista operating system.
Different performance metrics are used to measure the
effectiveness of the attribution methods. Abbasi and Chen
(2005a, 2006), Stamatatos (2008), and Shaker and Corne
(2010) used the accuracy measure to evaluate the performance
of their proposed methods, while Abbasi and Chen (2005b)
used the precision measure. To be able to compare our results
with the results of the other Arabic authorship attribution sys-
tems, we used both accuracy and precision. We also used the
recall and F1-measure as additional metrics.
7.1. Arabic corpus
Different Arabic corpora have been used by scholars to
tackle the Arabic authorship attribution problem. Abbasi
and Chen (2005a, 2006) used Arabic web forum messages
from a Yahoo group forum for the Al-Aqsa Martyrs group.
Abbasi and Chen (2005a) used political and social Arabic
web forum messages from Yahoo groups. Stamatatos
(2008) used Arabic newspaper reports from an Alhayat news-
paper. Finally, Shaker and Corne (2010) used 12 Arabic
books from the Arab Writers Union’s website to test their
method. We planned to use the same corpus used by
Abbasi and Chen (2005a,b, 2006) to effectively compare
our results with theirs, but unfortunately, we were not able
to obtain it from the authors. We then built our own dataset,
which consists of 30 Arabic books written by 10 different
authors collected from the Alwaraq website (http://www.alw-
araq.net): Alfarabi, Alghazali, Aljahedh, Almas3ody, Almeq-
rezi, Altabary, Altow7edy, Ibnaljawzy, Ibnrshd, and Ibnsena.
Each book is partitioned into chunks of 1980 to 2020 words
so that each author has 60 book chunks.Please cite this article in press as: Altheneyan, A.S., Menai, M.E.B. Naı̈ve Bayes classi
– Computer and Information Sciences (2014), http://dx.doi.org/10.1016/j.jksuci.20147.2. Sensitivity to stemming and/or normalization
We applied stemming and normalization to our corpus and
divided it into four different datasets. The first dataset contains
the original documents that have been neither stemmed nor
normalized (denoted R  N). The second dataset contains
the documents that have not been stemmed, but normalized
(denoted R +N).
The third dataset contains the documents that have been
stemmed, but not normalized (denoted +R  N).
Finally, the fourth dataset contains the documents that
have been both stemmed and normalized (denoted +R
+N). A ten-fold cross validation was used for training and
testing of the four naı̈ve Bayes models (NB, MNB, MBNB,
and MPNB).
Tables 2–5 summarize the results obtained by the four clas-
sifiers on different datasets in terms of the mean and standard
deviation of the recall, precision, accuracy, and F1-measure.
Note that l and r represent the mean value and its standard
deviation, respectively. Fig. 2 shows the variation of the mean
accuracy for the four classifiers on different datasets. It is clear
from the results that the MBNB model achieves the best
results, regardless of the dataset used. The MBNB model
achieves the best results with 97.43% accuracy and 86.07%
F1-measure when applied on data that have been neither
stemmed nor normalized (R  N). The MNB model results
are worse than MBNB, but it outperforms the MPNB and
NB models. Its best performance is obtained when it is applied
on data that have been neither stemmed nor normalized
(R  N) with an accuracy of 92.03% and an F1-measure
of 56.26%. The NB model performance is the worst; its overall
accuracy is 82.30%. Normalizing the data prior to its classifi-
cation has a small impact on the results of its classification.
Indeed, the overall results obtained on normalized data showfiers for authorship attribution of Arabic texts. Journal of King Saud University
.06.006
610
611
612
613
614
615
616
617
618
619
620
621
622
623
624
625
626
627
628
629
630
631
632
633
Table 2 Results of NB, MNB, MBNB, and MPNB classifiers on the dataset R+N.Q8
l(Recall)
(%)
r(Recall)
(%)
l(Precision)
(%)
r(Precision)
(%)
l(Accuracy)
(%)
r(Accuracy)
(%)
l(F1-measure)
(%)
r(F1-measure)
(%)
NB 10.50 18.53 8.93 18.11 82.10 15.24 7.12 12.43
MNB 53.50 38.87 54.29 36.60 89.10 8.31 48.10 31.07
MBNB 87.00 22.34 89.14 17.49 97.40 2.66 85.48 17.32
MPNB 36.00 37.00 35.00 35.00 87.00 8.00 31.00 30.00
Table 3 Results of NB, MNB, MBNB, and MPNB classifiers on the dataset R  N.
l(Recall)
(%)
r(Recall)
(%)
l(Precision)
(%)
r(Precision)
(%)
l(Accuracy)
(%)
r(Accuracy)
(%)
l(F1-measure)
(%)
r(F1-measure)
(%)
NB 11.50 19.02 9.05 17.59 82.30 12.78 7.95 12.93
MNB 60.17 35.24 63.64 29.24 92.03 4.30 56.26 26.16
MBNB 87.17 21.16 89.44 16.31 97.43 2.73 86.07 16.26
MPNB 37.00 39.26 33.62 32.21 87.40 6.79 30.19 28.56
Table 4 Results of NB, MNB, MBNB, and MPNB classifiers on the dataset +R+ N.
l(Recall)
(%)
r(Recall)
(%)
l(Precision)
(%)
r(Precision)
(%)
l(Accuracy)
(%)
r(Accuracy)
(%)
l(F1-measure)
(%)
r(F1-measure)
(%)
NB 2.00 5.49 0.65 1.91 80.40 19.67 0.89 2.53
MNB 30.83 39.34 29.47 35.38 86.17 10.53 22.49 24.46
MBNB 53.67 41.18 53.11 36.62 90.73 5.47 46.60 31.66
MPNB 24.67 34.68 20.16 28.38 84.93 9.14 16.99 20.93
Table 5 Results of NB, MNB, MBNB, and MPNB classifiers on the dataset +R  N.
l(Recall)
(%)
r(Recall)
(%)
l(Precision)
(%)
r(Precision)
(%)
l(Accuracy)
(%)
r(Accuracy)
(%)
l(F1-measure)
(%)
r(F1-measure)
(%)
NB 0.83 2.64 1.63 5.17 80.17 15.98 0.75 2.36
MNB 31.33 40.10 27.38 36.06 86.27 11.23 22.53 25.87
MBNB 61.83 40.24 61.26 36.26 92.37 5.14 55.62 32.81
MPNB 28.33 39.09 19.35 26.74 85.67 9.86 19.30 25.20
75%
80%
85%
90%
95%
100%
(-R+N) (-R-N) (+R+N) (+R-N)
µ 
(A
cc
ur
ac
y)
NB MNB MBNB MPNB
Figure 2 Variation of the mean accuracy for the four classifiers
on different datasets.
8 A.S. Altheneyan, M.E.B. Menai
JKSUCI 133 No. of Pages 12
29 September 2014
Q1that the accuracy of the different naı̈ve Bayes models increases
maximally up to 2.93% and decreases up to 1.64%, in the case
of non-stemmed and stemmed data, respectively. However,
stemming the data has a larger impact on the results because
the accuracy of different models decreases to 7.1%.Please cite this article in press as: Altheneyan, A.S., Menai, M.E.B. Naı̈ve Bayes class
– Computer and Information Sciences (2014), http://dx.doi.org/10.1016/j.jksuci.20147.3. Performance comparison
Figs. 3–6 show the average results of applying the different
naı̈ve Bayes models on data that have been neither stemmed
nor normalized for 10 different authors. The NB model attrib-
uted texts with an accuracy of 90% and above to 5/10 authors:
Alfarabi, Alghazali, Altabary, Ibnrshd, and Ibnsena. Its lowest
performance (approximately 60% accuracy) was given on texts
of Ibnaljawzy. The MNB model attributed texts with an accu-
racy of 90% and above to 7/10 authors: Alfarabi, Alghazali,
Almeqrezi, Altabary, Ibnaljawzy, Ibnrshd, and Ibnsena. Its
accuracy on texts of Alghazali and Almeqrezi is greater than
96%. The accuracy of the MBNB model is greater than 95%
for all authors and exceeds 98% for Alfarabi, Alghazali,
Almeqrezi, and Altabary. The MPNB model attributed texts
with an accuracy of 90% and above to 2/10 authors: Alghazali
and Almeqrezi. However, its lowest accuracy is approximately
82% (Alfarabi).
The confusion matrix, shown in Table 6 for the MBNB
model on a single run, demonstrates its high performance inifiers for authorship attribution of Arabic texts. Journal of King Saud University
.06.006
634
635
636
637
638
639
640
642
643
645
646
647
648
649
650
651
84%
86%
88%
90%
92%
94%
96%
98%
Multinomial Naïve Bayes 
µ 
(A
cc
ur
ac
y)
alfarabi alghzali aljahedh almas3ody almeqrezi
altabary altow7edy ibnaljawzy ibnrshd ibnsena
Figure 4 Variation of the mean accuracy for the MNB model for
different authors.
94%
95%
96%
97%
98%
99%
Multi-variant Bernoulli Naïve Bayes 
µ 
(A
cc
ur
ac
y)
alfarabi alghzali aljahedh almas3ody almeqrezi
altabary altow7edy ibnaljawzy ibnrshd ibnsena
Figure 5 Variation of the mean accuracy for the MBNB model
for different authors.
40%
50%
60%
70%
80%
90%
100%
Simple Naïve Bayes
µ 
(A
cc
ur
ac
y)
alfarabi alghzali aljahedh almas3ody almeqrezi
altabary altow7edy ibnaljawzy ibnrshd ibnsena
Figure 3 Variation of the mean accuracy for the NB model for
different authors.
76%
78%
80%
82%
84%
86%
88%
90%
92%
94%
Multi-variant Poisson Naïve Bayes 
µ 
(A
cc
ur
ac
y)
alfarabi alghzali aljahedh almas3ody almeqrezi
altabary altow7edy ibnaljawzy ibnrshd ibnsena
Figure 6 Variation of the mean accuracy for the MPNB model
for different authors.
Naı̈ve Bayes classifiers for authorship attribution of Arabic texts 9
JKSUCI 133 No. of Pages 12
29 September 2014
Q1attributing particular texts of Alfarabi, Alghazali, Almas3ody,
and Ibnaljawzy. Additionally, it can be used to underline some
similarities between authors’ styles. For example, it shows that
nine texts of Almeqrezi are attributed to Almas3ody because
of some common features, including the average word length
and frequency of function words. The following is a fragment
of a misclassified text:
An example of Almas3ody’s text is:Please cite this article in press as: Altheneyan, A.S., Menai, M.E.B. Naı̈ve Bayes classi
– Computer and Information Sciences (2014), http://dx.doi.org/10.1016/j.jksuci.20147.4. Comparison with other methods
For comparative purposes, we considered all of the works con-
ducted to tackle the Arabic authorship attribution problem to
the best of our knowledge. Table 7 presents our results and
those reported in other references in terms of recall, precision,
and accuracy. The results are not in fact directly comparablefiers for authorship attribution of Arabic texts. Journal of King Saud University
.06.006
652
653
654
655
656
657
658
659
660
661
662
663
664
665
666
667
668
669
670
671
672
673
674
675
676
677
678
679
680
681
682
683
684
685
686
687
688
689
690
691
692
693
694
695Q7
696
697
698
699
700
701
702
703
704
Table 6 Confusion matrix for the MBNB classifier on the dataset R  N.
Alfarabi Alghzali Aljahedh Almas3ody Almeqrezi Altabary Altow7edy Ibnaljawzy Ibnrshd Ibnsena
Alfarabi 57 0 0 0 0 0 0 0 2 1
Alghzali 0 56 0 0 0 0 0 0 1 3
Aljahedh 0 0 50 0 0 0 1 9 0 0
Almas3ody 0 0 0 60 0 0 0 0 0 0
Almeqrezi 1 0 0 9 49 0 0 1 0 0
Altabary 0 0 0 7 0 52 0 1 0 0
Altow7edy 1 0 2 2 0 1 40 11 0 3
Ibnaljawzy 0 0 1 0 0 2 0 57 0 0
Ibnrshd 6 1 0 0 0 0 0 0 47 6
Ibnsena 0 1 0 0 0 0 0 0 4 55
Table 7 Comparison of the four naı̈ve Bayes models with other methods. Note that ‘NR’ means ‘Not Reported’.
Reference Attribution method Recall Precision Accuracy Data
This paper NB 11.50% 9.05% 82.30% Arabic books collected from Alwaraq website
MNB 60.17% 63.64% 92.03%
MBNB 87.17% 89.44% 97.43%
MPNB 37.00% 33.62% 87.40%
Abbasi and Chen (2005a) Decision trees (C4.5) NR NR 81.03% Arabic web forum messages from Yahoo groups
SVM NR NR 85.43%
Abbasi and Chen (2005b) Decision trees (C4.5) NR 71.93% NR Arabic web forum messages from Yahoo group forum
for Al-Aqsa martyrsSVM NR 94.83% NR
Abbasi and Chen (2006) SVM NR NR 87.00% Arabic web forum messages from Yahoo group forum
for Al-Aqsa martyrsWriteprint NR NR 68.92%
Stamatatos (2008) SVM NR NR 93.60% Arabic newspaper report of Alhayat
Shaker and Corne (2010) LDA NR NR 87.63% Arabic books obtained from the website of the Arab
Writers Union
10 A.S. Altheneyan, M.E.B. Menai
JKSUCI 133 No. of Pages 12
29 September 2014
Q1because they were not obtained on the same datasets. More-
over, the granularities of the tasks vary. However, they can
give an indication of the performance of the different methods.
It shows that MBNB achieved the best accuracy (97.43%),
while the second best accuracy was obtained by an SVM
method used by Stamatatos (2008) on Arabic newspaper
reports of Alhayat (93.60%). The best precision was obtained
by another SVM method used by Abbasi and Chen (2005b) on
Arabic web forum messages from a Yahoo group forum for
Al-Aqsa martyrs (94.83%), while MBNB achieved the second
best accuracy (89.44%).
8. Conclusions and future work
We investigated the applicability of naı̈ve Bayes classifiers and
their influence on event models for authorship attribution of
Arabic texts. We implemented an authorship attribution sys-
tem to test and compare four different models of naı̈ve Bayes:
NB, MNB, MBNB, and MPNB. MBNB probability estima-
tion depends on the existence or absence of a feature, while
MPNB and MNB probability estimation depend on the fea-
ture frequency. Probability estimation in the NB model is
based on the mean and standard deviation of the features.
We evaluated their performance on a large corpus of four dif-
ferent datasets and examined the effect of stemming and nor-
malization on the attribution process. The overall results
show that the MBNB model provides the best results among
all naı̈ve Bayes models; it was able to attribute the author ofPlease cite this article in press as: Altheneyan, A.S., Menai, M.E.B. Naı̈ve Bayes class
– Computer and Information Sciences (2014), http://dx.doi.org/10.1016/j.jksuci.2014a text with an average accuracy of 97.43%. They also show
that normalization does not have a large impact on the attribu-
tion results, while stemming decreases the efficiency of the clas-
sifier because roots provide less authorial evidence than words.
The results were compared with those of available methods for
Arabic authorship attribution to give an indication on the per-
formance of the naı̈ve Bayes models. These results indicate
that MBNB outperforms all of the other methods in terms
of accuracy.
As future work, we intend to extend the experiments to lar-
ger datasets of more than ten authors. We also plan to inves-
tigate the impact of other feature selection methods on the
performance of the naı̈ve Bayes models.
References
Abbasi, A., Chen, H., 2005a. Applying authorship analysis to Arabic
web content. In: Kantor, P., Muresan, G., Roberts, F., Zeng, D.D.,
Wang, F.-Y., Chen, H., Merkle, R.C. (Eds.), Intelligence and
Security Informatics, vol. 3495, Berlin, Heidelberg, pp. 183–197.
Abbasi, A., Chen, H., 2005b. Applying authorship analysis to
extremist-group Web forum messages. IEEE Intell Syst 20 (5),
67–75. http://dx.doi.org/10.1109/MIS.2005.81.
Abbasi, A., Chen, H., 2006. Visualizing Authorship for Identification.
IN ISI, pp. 60–71.
Al-Harbi, S., Almuhareb, A., Al-Thubaity, A., Khorsheed, M. S., Al-
Rajeh, A., 2008. Automatic Arabic Text Classification. 9th
International journal of statistical analysis of textual data, pp.
77–83.ifiers for authorship attribution of Arabic texts. Journal of King Saud University
.06.006
705
706
707
708
709
710
711
712
713
714
715
716
717
718
719
720
721
722
723
724
725
726
727
728
729
730
731
732
733
734
735
736
737
738
739
740
741
742
743
744
745
746
747
748
749
750
751
752
753
754
755
756
757
758
759
760
761
762
763
764
765
766
767
768
769
770
771
772
773
774
775
776
777
778
779
780
781
782
783
784
785
786
787
788
789
790
791
792
793
794
795
796
797
798
799
800
801
802
803
804
805
806
807
808
809
810
811
812
813
814
815
816
817
818
819
820
821
822
823
824
825
826
827
828
829
830
831
832
833
834
835
836
837
838
839
840
Naı̈ve Bayes classifiers for authorship attribution of Arabic texts 11
JKSUCI 133 No. of Pages 12
29 September 2014
Q1Alsaleem, S., 2011. Automated Arabic Text categorization Using SVM
and NB. Int. Arab J. e-Technol. 2 (2), 124–128.
Al-Salemi, 2011. Statistical Bayesian learning for automatic Arabic
text categorization. J. Comput. Sci. 7 (1), 39–45. http://dx.doi.org/
10.3844/jcssp.2011.39.45.
Al-Shammari, E.T., 2010. Improving Arabic document categorization:
Introducing local stem. 2010 10th International Conference on
Intelligent Systems Design and Applications (ISDA), IEEE, pp.
385–390. doi:10.1109/ISDA.2010.5687235.
Argamon, S., Levitan, S., 2005. Measuring the usefulness of function
words for authorship attribution. Proc. Joint Conf. Assoc. Com-
put. Humanities Assoc. Literary Linguist. Comput., 1–3.
Baayen, H., van Halteren, H., Tweedie, F., 1996. Outside the cave of
shadows: using syntactic annotation to enhance authorship attri-
bution. Literary Linguist. Comput. 11 (3), 121–132. http://dx.doi.
org/10.1093/llc/11.3.121.
Baayen, H., Halteren, H.V., Neijt, A., Tweedie, F., 2002. An
experiment in authorship attribution. 6th JADT, pp. 69–75.
Benedetto, D., Caglioti, E., Loreto, V., 2002. Language trees and
zipping. Phys. Rev. Lett. 88 (4), 048702. http://dx.doi.org/10.1103/
PhysRevLett. 88.048702.
Binongo, J., Smith, M., 1999. The application of principal component
analysis to stylometry. Literary Linguist. Comput. 14 (4), 445–466.
http://dx.doi.org/10.1093/llc/14.4.445.
Binongo, J., 2003. WhoWrote the 15th Book of Oz? An Application of
Multivariate Analysis to Authorship Attribution. Chance 16 (2), 9–
17.
Brinegar, C.S., 1963. Mark Twain and the Quintus Curtius Snodgrass
letters: a statistical test of authorship. J. Am. Stat. Assoc. 58 (301),
85–96. http://dx.doi.org/10.2307/2282956.
Burrows, J., 2002. ‘‘Delta”: a measure of stylistic difference and a
guide to likely authorship. Literary Linguist. Comput. 17 (3), 267–
287. http://dx.doi.org/10.1093/llc/17.3.267.
Chaski, C.E., 2005. Who’s at the keyboard: authorship attribution in
digital evidence investigations. Int. J. Digital Evidence 4 (1).
Coyotl-Morales, R.M., Villaseñor-Pineda, L., Montes-y-Gómez, M.,
Rosso, P., 2006. Authorship attribution using word sequences. In:
Martı́nez-Trinidad, J.F., Carrasco Ochoa, J.A., Kittler, J. (Eds.),
Progress in Pattern Recognition, Image Analysis and Applications,
Berlin, Heidelberg, vol. 4225, pp. 844–853.
de Roeck, A.N., Al-Fares, W., 2000. A morphologically sensitive
clustering algorithm for identifying Arabic roots. In: Proceedings of
the 38th Annual Meeting on Association for Computational
Linguistics, ACL’00, Stroudsburg, PA, USA, pp. 199–206. Doi:
http://dx.doi.org/10.3115/1075218.1075244.
Diederich, J., Kindermann, J., Leopold, E., Paass, G., Informations-
technik, G.F., Augustin, D.-S., 2003. Authorship attribution with
support vector machines. Appl. Intell. 19, 109–123.
El Kourdi, M., Bensaid, A., Rachidi, T., 2004. Automatic Arabic
document categorization based on the naı̈ve Bayes algorithm. In:
Proceedings of the Workshop on Computational Approaches to
Arabic Script-based Languages, Semitic’04, Stroudsburg, PA,
USA, pp. 51–58.
Farghaly, A., Shaalan, K., 2009. Arabic natural language processing:
challenges and solutions, 8(4), 14:1–14:22. doi:10.1145/
1644879.1644881.
Fisher, R.A., 1936. The use of multiple measurements in taxonomic
problems. Ann. Eugenics 7 (2), 179–188.
Forman, G., 2003. An extensive empirical study of feature selection
metrics for text classification. J. Mach. Learn. Res. 3, 1289–1305.
Holmes, D.I., Gordon, L.J., Wilson, C., 2001a. A widow and her
soldier: stylometry and the American Civil War. Literary
Linguist. Comput. 16 (4), 403–420. http://dx.doi.org/10.1093/llc/
16.4.403.
Holmes, D., Robertson, M., Paez, R., 2001b. Stephen crane and the
New-York Tribune: a case study in traditional and non-traditional
authorship attribution. Comput. Humanities 35 (3), 315–331.
http://dx.doi.org/10.1023/A:1017549100097.Please cite this article in press as: Altheneyan, A.S., Menai, M.E.B. Naı̈ve Bayes classi
– Computer and Information Sciences (2014), http://dx.doi.org/10.1016/j.jksuci.2014Hoorn, J., Frank, S., Kowalczyk, W., van der Ham, F., 1999. Neural
network identification of poets using letter sequences. Literary
Linguist. Comput. 14 (3), 311–338. http://dx.doi.org/10.1093/llc/
14.3.311.
Hoover, D.L., 2004. Testing Burrows’s delta. Literary Linguist.
Comput. 19 (4), 453–475. http://dx.doi.org/10.1093/llc/19.4.453.
Huang, Y., Li, L., 2011. Naive Bayes classification algorithm based on
small sample set. In: 2011 IEEE International Conference on Cloud
Computing and Intelligence Systems (CCIS), pp. 34–39.
doi:10.1109/CCIS.2011.6045027.
Juola, P., 2005. A controlled-corpus experiment in authorship iden-
tification by cross-entropy. Literary Linguist. Comput. 20 (Suppl.
1), 59–67. http://dx.doi.org/10.1093/llc/fqi024.
Juola, P., Sofko, J., Brennan, P., 2006. A Prototype for authorship
attribution studies. Literary Linguist. Comput. 21 (2), 169–178.
http://dx.doi.org/10.1093/llc/fql019.
Keselj, V., Peng, F., Cercone, N., Thomas, C., 2003. N-gram-based
author profiles for authorship attribution. Computat. Linguist. 3,
255–264, Doi: 10.1.1.9.7388.
Khmelev, D.V., Tweedie, F.J., 2001. Using Markov Chains for
identification of writer. Literary Linguist. Comput. 16 (3), 299–
307. http://dx.doi.org/10.1093/llc/16.3.299.
Khmelev, D.V., Teahan, W.J., 2003. A repetition based measure for
verification of text collections and for text categorization. In:
Proceedings of the 26th annual international ACM SIGIR confer-
ence on Research and development in information retrieval, pp.
104–110.
Kim, S.-B., Han, K.-S., Rim, H.-C., Myaeng, S.-H., 2006. Some
effective techniques for naive Bayes text classification. IEEE Trans.
Knowledge Data Eng. 18 (11), 1457–1466. http://dx.doi.org/
10.1109/TKDE.2006.180.
Kjell, B., 1994. Authorship attribution of text samples using neural
networks and Bayesian classifiers. In: 1994 IEEE International
Conference on Systems, Man, and Cybernetics, 1994. Humans,
Information and Technology, vol. 2, pp. 1660–1664. Doi:10.1109/
ICSMC.1994.400086.
Koppel, M., Schler, J., Argamon, S., 2009. Computational methods in
authorship attribution. J. Am. Soc. Inf. Sci. Technol. 60 (1), 9–26.
http://dx.doi.org/10.1002/asi.v60:1.
Koppel, M., Schler, J., Argamon, S., 2010. Authorship attribution in
the wild. Lang. Resour Evaluat. 45 (1), 83–94. http://dx.doi.org/
10.1007/s10579-009-9111-2.
Kukushkina, O.V., Polikarpov, A.A., Khmelev, D.V., 2001. Using
literal and grammatical statistics for authorship attribution. Probl.
Inf. Transm. 37 (2), 172–184. http://dx.doi.org/10.1023/
A:1010478226705.
Manning, C.D., Raghavan, P., Schütze, H., 2008. Introduction to
Information Retrieval, 1st ed. Cambridge University Press.
Mendenhall, T.C., 1887. The characteristic curves of composition.
Science ns-9, 237–246. http://dx.doi.org/10.1126/science.ns-
9.214S.237.
Mesleh, A., 2007. Chi square feature extraction based SVMS Arabic
language text categorization system. J. Comput. Sci. 3 (6), 430–435.
Mesleh, A.M., 2008. Support vector machines based Arabic language
text classification system: feature selection comparative study. In:
Sobh, T. (Ed.), Advances in Computer and Information Sciences
and Engineering. Springer, Dordrecht, Netherlands, pp. 11–16.
Mitchell, T.M., 1997. Machine Learning, 1st ed. McGraw-Hill.
Mosteller, F., Wallace, D.L., 1964. Inference and disputed authorship:
the federalist. The David Hume series of philosophy and cognitive
science reissues. Addison-Wesley.
Noaman, H. M., Elmougy, S., Ghoneim, A., Hamza, T. 2010. Naive
Bayes Classifier based Arabic document categorization. In: 2010
The 7th International Conference on Informatics and Systems
(INFOS), IEEE, pp. 1–5.
Pavelec, D., Justino, E., Oliveira, L.S., 2007. Author identification
using stylometric features. Inteligencia artificial: Revista Ibero-
americana de Inteligencia Artificial 11 (36), 59–66.fiers for authorship attribution of Arabic texts. Journal of King Saud University
.06.006
841
842
843
844
845
846
847
848
849
850
851
852
853
854
855
856
857
858
859
860
861
862
863
864
865
866
867
868
869
870
871
872
873
874
875
876
877
878
879
880
881
882
883
884
885
886
887
888
889
890
891
892
893
894
895
896
897
898
899
900
901
902
903
904
905
906
907
908
909
910
12 A.S. Altheneyan, M.E.B. Menai
JKSUCI 133 No. of Pages 12
29 September 2014
Q1Pearson, K., 1901. On lines and planes of closest fit to systems of
points in space. Philos. Mag. 2 (11), 559–572.
Pillay, S.R., Solorio, T., 2010. Authorship attribution of web forum
posts. eCrime Researchers Summit (eCrime), IEEE, pp. 1–7.
doi:10.1109/ecrime.2010.5706693.
Sanderson, C., Guenter, S., 2006. On authorship attribution via
markov chains and sequence kernels. In: Pattern Recognition 2006
ICPR 2006 18th International Conference on, 3(X), pp. 437–440.
Shaker, K., Corne, D., 2010. Authorship Attribution in Arabic using a
hybrid of evolutionary search and linear discriminant analysis. In:
2010 UKWorkshop on Computational Intelligence (UKCI), pp. 1–
6. Doi:10.1109/UKCI.2010.5625580.
Stamatatos, E., 2008. Author identification: using text sampling to
handle the class imbalance problem. Inf. Process. Manage. 44 (2),
790–799. http://dx.doi.org/10.1016/j.ipm.2007.05.012.
Stamatatos, E., 2009. A survey of modern authorship attribution
methods. J. Am. Soc. Inf. Sci. Technol. 60 (3), 538–556. http://dx.
doi.org/10.1002/asi.21001.
Stamatatos, E., Kokkinakis, G., Fakotakis, N., 2000. Automatic text
categorization in terms of genre and author. Comput. Linguist. 26
(4), 471–495. http://dx.doi.org/10.1162/089120100750105920.
Tan, R.H.R., Tsai, F.S., 2010. Authorship identification for online
text. In: Proceedings of the 2010 International Conference on
Cyberworlds, CW’10, Washington, DC, USA, pp. 155–162.
Doi:10.1109/CW.2010.50.
Teahan, W.J., Harper, D.J., 2003. Using compression-based language
models for text categorization. In: Croft, W.B., Lafferty, J. (Eds.),
Language modeling for information retrieval. Springer, Nether-
lands, pp. 141–165.
Türkoğlu, F., Diri, B., Amasyal±, M.F., 2007. Author attribution of
Turkish texts by feature mining. Corpus 1086–1093. http://dx.doi.
org/10.1007/978-3-540-74171-8_110.
Yang, Y., Liu, X., 1999. A re-examination of text categorization
methods. In: Proceedings of the 22nd annual international ACM
SIGIR conference on Research and development in informationPlease cite this article in press as: Altheneyan, A.S., Menai, M.E.B. Naı̈ve Bayes class
– Computer and Information Sciences (2014), http://dx.doi.org/10.1016/j.jksuci.2014retrieval, SIGIR’99, New York, NY, USA, pp. 42–49. doi:10.1145/
312624.312647.
Yang, Y., Pedersen, J.O., 1997. A comparative study on feature
selection in text categorization. In: Proceedings of the Fourteenth
International Conference on Machine Learning, ICML’97, San
Francisco, CA, USA, pp. 412–420.
Yule, G.U., 1939. On sentence-length as a statistical characteristic of
style in prose: with application to two cases of disputed authorship.
Biometrika 30 (3/4), 363–390. http://dx.doi.org/10.2307/2332655.
Zhao, Y., Vines, P., 2007. Authorship Attribution Via Combination of
Evidence. In: Amati, G., Carpineto, C., Romano, G. (Eds.), . In:
Advances in Information Retrieval, LNCS, Vol. 4425. Springer,
Berlin, Heidelberg, pp. 661–669.
Zhao, Y., Zobel, J., 2005. Effective and scalable authorship attribution
using function words. Proc. Second Asian Inf. Retr. Symp. 3689,
174–189.
Zhao, Y., Zobel, J., 2007. Searching with style: authorship attribution
in classic literature. In: Proceedings of the thirtieth Australasian
conference on Computer science –ACSC’07, Darlinghurst, Austra-
lia, Australia, vol. 62, pp. 59–68.
Zhao, Y., Zobel, J., Vines, P., 2006. Using relative entropy for
authorship attribution. In: Ng, H.T., Leong, M.-K., Kan, M.-Y.,
Ji, D. (Eds.), . In: Information Retrieval Technology, vol. 4182.
Springer, Berlin, Heidelberg, pp. 92–105.
Zheng, R., Li, J., Chen, H., Huang, Z., 2006. A framework for
authorship identification of online messages: writing-style features
and classification techniques. J. Am. Soc. Inf. Sci. Technol. 57 (3),
378–393. http://dx.doi.org/10.1002/asi.v57:3.
Zheng, R., Qin, Y., Huang, Z., Chen, H., 2003. Authorship analysis in
cybercrime investigation. In: Chen, H., Miranda, R., Zeng, D.,
Demchak, C., Schroeder, J., Madhusudan, T. (Eds.), . In: Intelli-
gence and Security Informatics, LNCS, vol. 2665. Springer, Berlin,
Heidelberg, p. 959.ifiers for authorship attribution of Arabic texts. Journal of King Saud University
.06.006
