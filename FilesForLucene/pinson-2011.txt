Connected Component Level Discrimination of 
Handwritten and Machine-Printed Text Using Eigenfaces 
Samuel J. Pinson 
Pinson Linguistic Service 
1013 E. Kessler Dr. 
Sultan, WA  98294 
sjpinson@hotmail.com 
William A. Barrett 
Department of Computer Science 
Brigham Young University 
Provo, Utah 84602  USA 
barrett@cs.byu.edu
 
 
 
Abstract—We employ Eigenfaces to discriminate between 
handwritten and machine-printed text at the connected component 
(CC) level. Normalized images of machine print CCs are treated as 
points in a high-dimensional space. PCA yields a reduced-
dimensional character space. Representative machine print CCs are 
projected into character space and a local distance threshold for 
each representative is automatically determined. CCs are classified 
as machine print if they are within the local distance threshold of 
their closest machine print representative. Otherwise, they are 
classified as handwriting. Recursive character segmentation using 
min graph cut is used to address the problem of touching 
characters. Validation over a large NIST handwriting and machine 
print database demonstrates precision of 93.98% and 89.1% for 
machine print and handwriting respectively.  
 
Keywords-Eigenfaces; Handwriting/machine print discrimin-
ation; touching character segmentation; min graph cut; NIST 
1. INTRODUCTION 
Many applications in document analysis and recognition 
require discrimination between machine print and hand-
writing. Annotations may need to be recognized, recorded 
and removed. Handwriting may need to be extracted from 
forms and processed independently while machine print 
connected components (CCs) are passed to an OCR engine. 
Other documents may need to be indexed or searched based 
on machine print or annotated entries. The goal and 
contribution of this paper is to correctly label input CCs 
(Figure 1, left) as machine print (green) or handwriting 
(red), as illustrated in Figure 1, right.  
Early work for discriminating between machine print 
and handwriting relies exclusively on stroke orientation [1]. 
Franke et. al train an ensemble of statistical classifiers on 
CC features [2]. Line straightness and symmetry features 
from CCs have also been used to train a neural network for 
character level discrimination [3].  
An application for mail address blocks [4] uses a variety 
of features to train a neural network. Character block layout 
variance is used for Kanji at the text line level [5]. 
Discriminant functions have been applied to vertical project-
ion profiles [6] at the line, not the CC level. In [7] several 
CC features extracted are used for word-level classification. 
Guo and Ma use a HMM based on linguistic context [8] 
in conjunction with the differences between vertical 
projection profiles of machine print and handwriting. Zheng  
 
 
Figure 1. Discrimination between handwritten (red) and machine-printed 
(green) text using eigenfaces. Note touching character false positives. 
 
et al. address machine print and handwriting identification 
in noisy images [9] using three two-way Fischer linear 
classifiers. A SVM is used to isolate signatures from 
machine print in [12] and to identify sparse handwritten 
annotations occurring at arbitrary orientations in [13]. K-
means clustering followed by MRF relabeling is used in 
[14] for segmentation of handwriting, machine print and 
noise with an overall recall of 96.33%. In [18] a novel 
approach to automatically discover features pushes error 
rates of handwriting and machine print to 13.8%. 
Color annotation is extracted from color documents in 
[15] using robust feature alignment and background 
subtraction. Work in [16] builds on this, making use of color 
clustering and a decision tree to identify handwritten anno-
tation in marked up documents containing machine print.  
Muller and Herbst [11] apply Eigenfaces [10] to 
character identification by treating characters as points in a 
high-dimensional space that is reduced to a Character Space 
of eigenvectors using PCA. Characters are classified based 
on the minimum weighted Euclidean distance. The weights 
are proportional to the standard deviation in the direction of 
the eigenvector. Solli also makes use of eigenfaces for 
looking up fonts in a large font library [18].  
We seek to overcome the limitations of previous 
techniques (reliance on color, manually-derived features, 
lengthy training or incremental learning cycles) by using 
Eigenfaces to exploit all relevant character components 
while automatically deriving CC classification thresholds.  
2011 International Conference on Document Analysis and Recognition
1520-5363/11 $26.00 © 2011 IEEE
DOI 10.1109/ICDAR.2011.280
1394
2. MACHINE PRINT AND HANDWRITING DISCRIMINATION 
Similar to [11], we project CCs into a hyperdimensional 
character space for classification. However, we discriminate 
between machine print and handwritten characters, rather 
than classifying machine print characters.  
2.1 Character Space  
Let ! = !1,!2,...,!M be a set of M vectors of length N2 
derived from N ! N character images (N=64). The mean 
character is defined  as                       ,  and the difference 
image of the ith vector as                     (Figure 2). 
 
            
=-
 
Figure 2. Difference image,                  , where 
Samuel James Pinson, Sr. and William Barrett: Connected Component Level Method Identification using Eigenfaces 7
/users/sjpinson/thesis/thesis/graphics/faces.pdf
(a) Facial images
us rs/sjpinson/thesi /thesis/graphics/faceMean.png
(b) Mean of faces
/users/sjpinson/thesis/thesis/graphics/faceDifferenceImageEquation.pdf
(c) Face Difference Image
Fig. 11. Mean and diff erence images. The mean face is sub-
tracted from each of the M faces to create diff erence images.
Images couresty of the Olivetti Research Laboratory [?].
tor as Γ̂i = Γi−Ψ (Figure 11(c)). Let A =
￿
Γ̂1 . . . Γ̂M
￿
.
That is, the columns of A are the diff erence images of the
M vectors in Γ . The covariance matrix C for the set Γ
is the N2 by N2 matrix where C =
￿
1
M
￿M
k=1 Γ̂iΓ̂
T
j
￿
=
1
M AA
T . The eigenvectors of the covariance matrix C
are found via singular value decomposition [? ]. The sin-
gular value decomposition of a matrix A is given by
A = USV T . The eigenvectors of C = 1M AA
T are the
columns of U . We refer the interested reader to [? ] for a
detailed derivation.
Each eigenvector e￿i has a corresponding eigenvalue.
The eigenvector corresponding to the largest eigenvalue
points in the direction of greatest variance in face space.
Eigenvectors corresponding to successively smaller eigen-
values are mutually othogonal and point in the direction
of greatest remaining variance. Because pixel intensities
in images of faces are not statistically independent, rel-
atively few of the eigenvectors span most of the vari-
ance in the set Γ . The remaining eigenvectors may be
dropped, thus reducing the dimensionality of the space
and making the rest of the algorithm computationally
practical. The M ￿ eigenvectors retained form the basis
of a vector space, referred to as face space . In Figure 12
we treat these N2-dimensional eigenvectors as N × N
/users/sjpinson/thesis/thesis/graphics/faceEigenvectorsAsImages.pdf
Fig. 12. Eigenfaces. The N2-dimensional eigenvectors of face
space, or eigenfaces, resemble ghostly faces when viewed as
N ×N images. Here we see the three most significant eigen-
faces, i.e. the directions that correspond to the three largest
variances in the distribution of faces. These eigenfaces were
derived from the images in the Olivetti Research Laboratory
facial image database.
images. Note that they resemble ghostly faces, hence the
name eigenfaces.
Known Faces Once face space has been defined, we de-
termine which faces we would like to recognize. These
faces are called known faces (Figure 9d). Let K ￿ be the
projection of the set of known faces K into face space,
and let K￿i be the i
th element of K￿ . The jth component
of K￿i is K
￿
ij = eTj (Ki −Ψ) = eTj K̂i, where ej is the jth
eigenvector of face space. That is, the jth component of
the projection of the ith known face is the dot product
of the jth eigenvector with the ith difference image.
Note that projecting into face space makes N2-dimensional
vectors become M ￿-dimensional (M ￿ ￿ N2). For Turk
and Pentland this meant moving from a 65,536-dimensional
space to a 7-dimensional space.
Face Recognition Recognition of a face with unknown
identity, U, (Figure 9e) proceeds as follows: We subtract
the mean face from U to get the difference image Û =
U−Ψ and project into face space to get U￿. A distance
threshold, θ is established empirically, and the unknown
face is classified based on its distance from the known
faces in face space. Specifically,
class (U) =
￿
class i if ￿U￿ −K￿i￿ ≤ θ
unknown otherwise
(1)
where i = arg minj ￿U￿ −K￿j￿. In other words, an un-
known face is assigned to the class of the known face
that minimizes the Euclidean distance from the unknown
face. If the minimum Euclidean distance across all known
faces is greater than the threshold, the face is not recog-
nized (Figure 9e).
3.2 Method Identification by Eigenfaces
Figure 13 outlines our approach to connected component
level method identification (Compare with Figure 9).
Because machine print is consistent, connected com-
ponents that correspond to machine-printed characters
 = mean image. 
 
 
 
Figure 3. First and last 9 eigenvectors capture character structure, detail. 
 
Let                             .  That is,  the columns of A are  the   
difference images of the M vectors i  !. Th  covariance 
matrix  C  for  the   set   !   is   th    N2 ! N2   matrix   where  
Samuel James Pinson, Sr. and William Barrett: Connected Component Level Method Identification using Eigenfaces 7
/users/sjpinson/thesis/thesis/graphics/faces.pdf
(a) Facial images
us rs/sjpinson/thesi /thesis/graphics/faceMean.png
(b) Mean of faces
/users/sjpinson/thesis/thesis/graphics/faceDifferenceImageEquation.pdf
(c) Face Difference Image
Fig. 11. Mean and difference images. The mean face is sub-
tracted from each of the M faces to create difference images.
Images couresty of the Olivetti Research Laboratory [?].
tor as Γ̂i = Γi−Ψ (Figure 11(c)). Let A =
￿
Γ̂1 . . . Γ̂M
￿
.
That is, the columns of A are the difference images of the
M vectors in Γ . The covariance matrix C for the set Γ
is the N2 by N2 matrix where C =
￿
1
M
￿M
k=1 Γ̂iΓ̂
T
j
￿
=
1
M AA
T . The eigenvectors of the covariance matrix C
are found via singular value decomposition [?]. The sin-
gular value decomposition of a matrix A is given by
A = USV T . The eigenvectors of C = 1M AA
T are the
columns of U . We refer the interested reader to [?] for a
detailed derivation.
Each eigenvector e￿i has a corresponding eigenvalue.
The eigenvector corresponding to the largest eigenvalue
points in the direction of greatest variance in face space.
Eigenvectors corresponding to successively smaller eigen-
values are mutually othogonal and point in the direction
of greatest remaining variance. Because pixel intensities
in images of faces are not statistically independent, rel-
atively few of the eigenvectors span most of the vari-
ance in the set Γ . The remaining eigenvectors may be
dropped, thus reducing the dimensionality of the space
and making the rest of the algorithm computationally
practical. The M ￿ eigenvectors retained form the basis
of a vector space, referred to as face space. In Figure 12
we treat these N2-dimensional eigenvectors as N × N
/users/sjpinson/thesis/thesis/graphics/faceEigenvectorsAsImages.pdf
Fig. 12. Eigenfaces. The N2-dim nsional eigenvectors of face
space, or eigenfaces, resemble ghostly faces when viewed as
N ×N images. Here we see the three most significant eigen-
faces, i.e. the directions that correspond to the three largest
variances in the distribution of faces. These eigenfaces were
derived from the images in the Olivetti Research Laboratory
facial image database.
images. Note that they resemble ghostly faces, hence the
name eigenface .
Known Faces Once face space has been defined, we de-
termine which faces we would like to recognize. he e
faces are called known faces (Figure 9d). Let K ￿ be the
projection of the set of known faces K int face sp ce,
and let K￿i b th i
th elem nt of K ￿. The jth component
of K￿i is K
￿
ij =
T
j (Ki −Ψ) = eTj K̂i, where ej is the jth
eigenvector of face space. That is, the jth component of
the projection of the ith known face is the dot oduct
f the jth eigenvector wi the ith difference image.
Note that projecting into face space mak s N2- i ensio al
vectors become M ￿ -dimensional (M ￿ ￿ N2). For Turk
and Pentland this meant moving from a 65,536-dimensional
pace to a 7-dimensional space.
Face Recognition Recognition of fac with unknown
identity, U , (Figure 9e) proceeds as follows: We subtract
the m an face from U to get the difference image Û =
U−Ψ and project i to face space to get U￿. A distance
threshold, θ is established empirically, and the unknown
face is classified based on its distance from the known
faces in face space. Specifically,
class (U) =
￿
class i if ￿U￿ −K￿i￿ ≤ θ
unknown otherwise
(1)
where i = arg minj ￿U￿ −K￿j￿. In other words, an un-
known face is assigned to the class of the known face
that minimizes the Euclidean distance from the unknown
face. If the minimum Euclidean distance across all known
faces is greater than the threshold, the face is not recog-
nized (Figure 9e).
3.2 Method Identification by Eigenfaces
Figure 13 outlines our approach to connected component
level method identification (Compare with Figure 9).
Because machine print is consistent, connected com-
ponents that correspond to machine-printed characters
                  
C are found via SVD. That is, A = USVT, wh re the 
eigenvec ors of C ar  the columns of U. The eigenvector 
corresponding to the largest eigenvalue points in the 
direction of greatest variance in character space. Because 
relatively few of the eigenvectors span most of the variance 
in the set !, most of the remaining eigenvectors may be 
dropped, reducing the dimensionality of character space to 
100 and making the algorithm computationally practical. 
The first and last nine basis vectors (eigenvectors – Figure 
3) capture the high-level and detailed structure, respectively, 
of the characters along their respective axis. The complete 
algorithm for discrimination between machine print and 
handwriting is outlined in Figure 6. 
2.2 Automati  Selection of Local Distance Threshold, "i 
Eigenfaces uses a single global threshold for face 
recognition. However, because machine-printed characters 
cluster much more tightly than handwriting, we introduce an 
algorithm to automatically select local distance thresholds, "i 
for each representative machine print template, !"i. We do 
this by computing the relative density of machine print and 
handwriting surrounding each !"I (Figure 4). We used Adobe 
Illustrator to create 5,957 representative templates of fonts 
and styles of characters, numbers, punctuation and symbols.  
To discriminate between an unknown CC, U, we project 
its difference image, 
10 Samuel James Pinson, Sr. and William Barrett: Connected Component Level Method Identification using Eigenfaces
/users/sjpinson/thesis/thesis/graphics/top100evals.pdf
Fig. 17. 100 largest eigenvalues. We retained 100 eigenvec-
tors (out of 4,096) to form the basis of character space. These
eigenvectors correspond to the 100 largest eigenvalues, de-
picted here. The vertical axis r presents the ma nitude of
the eigenvalues.
/users/sjpinson/thes s/th is/gr phics/illustratorEigenv ctors.png
Fig. 18. Eigenvectors viewed as images. Here are the first
nine eigenvectors of the character space defined by Γ . Note
the ghostly appearance of characters
/users/sjpinson/thesis/thesis/graphics/projectedR pres ntatives.pdf
Fig. 19. Projection of machine print representatives. Here
we see the projection of the 5,957 machine print templates
onto the two most significant eigenvectors, i.e. those with the
greatest eigenvalues.
First, we create its t mplate, U. Then we project its
difference i Û = U − Ψ, into character space to
get U￿. We determine the Euclidean distance from U￿ to
the projection of each representative machine print tem-
plate, Γ￿i. If the projection of U
￿ lies sufficiently close
to the projection of the closest machine print template
in Γ , then we conclude that the connected component
from which we created the template U is machine print.
Otherwise we conclude that it is handwriting.
To define suffici ntly clos we determine a threshold,
θi, for each Γi. If these local thresholds are too high,
then we will frequently mistake handwriting for machine
print. On the other hand, if the thresholds are too low,
then we wi l hav poor generalization. Clearly, the ob-
jective in select ng a threshold for a particular Γi is to
make it as large as possible without having too many
false positives.
Thus, our method identification rule, Equation 2,
is analogous to the Eigenface classification rule (Equa-
tion 1). However, in thi case, rather than using a sin-
gle global distance threshold, θ, we use a local distance
threshold, θi for each representative machine print tem-
plate.
method(U) =
￿
machine print if ￿U￿ − Γ￿i￿ ≤ θi
handwriting otherwise (2)
where i = arg minj ￿U￿ − Γ￿j￿.
Automatic Local Distance Threshold Selection The large
umber of Γi prohibit manual threshold selection. We
automatically determine local thr sholds for each Γi given
a us r-supplied global target for machine print precision,
P . Machine print precision refers to the fraction of con-
nected components classified as machine print that were
actually machine print.
Our data-driven approach begins with a large set of
machine print connected components, A, and a large set
of handwritten connected components, B. We generated
our own images of 273,286 machine print connected com-
ponents in 3 styles (normal, bold, italic) and 26 differ-
ent fonts for A. The first four partitions from the NIST
SD19 database [?] provided 599,724 handwriting con-
nected components from 2,100 writers for B. Each set
was meant to be representative of the distribution of
its method in character space. Their projection onto the
first two eigenvectors of character space is shown in Fig-
ure 20. We use the connected components in these sets to
determi e the connected component radial density (Fig-
ure 21) of machine print and handwriting with respect
to each Γ￿i.
Let us explain what we mean by connected compo-
nent radial density. Density is a measure of mass per
volume. The units of connected component radial den-
sity are connected components per spherical volume in
character space. Just as air density is a function of dis-
tance above sea level, connected component density is a
function of radial distance in character space from a par-
ticular machine print representative. Since each machine
print representative has a unique location in character
space, each machine print representative has a unique
connected component density.
We distinguish between connected component den-
sity for machine print and handwriting connected com-
ponents. If we let v(r) be the volume of a hypersphere
of radius r in character space, then the machine print
connected component density for the representative Γi
at a radial distance r is:
  U " #, into charac  s   get 
U". We determine the Euclidean distance from U" to the 
projection of each !"i. If the projection of U" lies sufficiently  
well as images of faces to be recognized are projected into face space.  The minimum 
Euclidean distance from an unknown face to the known faces determines how the 
unknown face will be classified.  If the minimum distance is above some threshold, then 
the face is rejected, i.e. not recognized.  
The basis of face space consists of the most significant eigenvectors of the 
covaria ce matrix of a training set of face images.  The covariance matrix is given by 
M
i
T
iiM
C
1
))((1 , 
where  is the ii
th training image and is the average of all the training images.  The 
projection of a face, , into face pace is a vector of weights, one for each dimension of 
face space.  The kth component of the projection is , where  is the k)(Tkk u ku
th 
most significant eigenvector of the covariance matrix C. 
 Muller and Herbst [3] employ this idea in character recognition.  Images of faces 
are replaced with images of characters.  We build on their work and apply the principle to 
connected component level method identification.  That is, for each connected 
component in a  image, we determine whether the connected component is machine print 
 handwriting. 
 We accomplish this by determining the face 
space for a large set of machine print characters.  
Representative machine print characters are then 
projected into this space.  We determine a local distance 
threshold for each representative machine print 
character based on a user-supplied global requirement 
for machine print precision and the radial density (See 
Figure 2) of machine print and handwriting surrounding 
the connected component.  This algorithm is outlined 
below. 
 
 
 
Figure 2. Rad al density 
 
 
global
: user-supplied global requirement for machine print precision. 
local
i : local distance threshold for the i
th machine print representative. 
dr mp : number of machine print connected components within distance d of the ith machine print representative. 
dr hw : number of handwriting connected components within distance d of the ith machine print representative. 
 
1.   0locali
2. while ( ) 
globallocal
i
hw
i
local
i
mp
i
local
i
mp
i rrr )]()(/[)(
Increment  
local
i
3. if (  > 0) 
local
i
 Decrement 
local
. i
 
 
 
Figure 4. Radial Density (inset) for determining threshold "i (blue circle). 
 
close, U is considered mac ine print, o herwise handwriting. 
 A user-supplied  global  targ t  for machine print 
precision, P, is ne ded to determine "i. We begin with a 
large set of machine print connected components, A, and a 
large set of handwritten connected components, B, meant to 
represent the distribution f its machine print or handwriting 
representative in character space.  
 
A = 273,286 machine print CCs (26 fonts, normal, bold, italic) 
B = 599,724 handwriti g CCs, 2100 writers (NIST SD19 DB, 1st 4 parts) 
 
The CCs in these sets are used to determine the CC radial 
density (number of proximate CCs) of machine print and 
handwriting with respect to each !"i. 
If we let v(r) be the volume of a hypersphere of radius r 
in character space, then the machine print CC density for the 
representative !"i at a radial distance r is: 
 
  
 
where !A! is the cardinality of the set A. Similarly for the 
handwriting CCs. The summation in Equation 1 yields the 
number of machine print connected components within 
distance r of  !"i divided by the volume, v(r), of character 
space under consideration.  
We use P = 98%, to automatically select "i. We quantize 
character space abo t e ch !"i into b concentric hyper-
spheres (Figure 4). The radius of the outermost hypersphere, 
R, which defines the local neighborhood of each !"i, is 
empirically d termined. "i is the last (quantized) radial 
distance, r, before which P drops below 98%. In other 
words, it is the greatest distance for which it, and all smaller 
distances, satisfy the target machine print precision (98%).  
2.3 Calculating Local Machine Print Precision 
Local machine print precision, Pi(r), for a given !"i and 
"i yields the fraction of CCs classified as machine print that 
were act ally machine print (Figure 5). 
 
 
 
Figure 5. Local Machine Print precision, Pi(r) for given !"i, "i. 
6 Samuel James Pinson, Sr. and William Barrett: Connected Component Level Method Identification using Eigenfaces
/users/sjpinson/thesis/thesis/graphics/mp_a.pdf
(a) Machine print
/users/sjpinson/thesis/thesis/graphics/hw_a.pdf
(b) Handwriting
Fig. 8. Different instances of the character, ’a’. Machine print
connected components exhibit great consistency, while hand-
writing is consistently different.
to method identification, we present a detailed explana-
tion of Eigenfaces in Section 2.5.
The explanation of Eigenfaces will lay the ground-
work for the presentation of our connected component
level method identification technique in Section ??. Anal-
ogous to Eigenfaces, we use principal component analysis
over a set of images of machine print connected com-
ponents to find a suitable basis for a lower-dimensional
vector space (∼100 dimensions), which we shall call char-
acter space. Euclidean distance in character space, from
a set of representative machine print connected compo-
nents, determines the method, e.g. whether a connected
component is machine print or handwriting.
Finally, in Section ?? we will address the issue of
touching characters, which was discussed in Section ??
and illustrated in Figure 5. We give a novel algorithm
based on min graph cut [?] for separating touching char-
acters
3.1 Eigenfaces
We mentioned in the last chapter that the Eigenfaces
technique uses principal component analysis over a set
of facial images (∼ 16 images), which are treated as
points in a hyper-dimensional space (65, 536 dimensions
for 256 × 256 images), to find a suitable basis for a
lower-dimensional vector space. Euclidean distance in
the lower-dimensional space (∼ 7 dimensions), which is
called face space, is used to determine whether or not a
face is recognized. Figure 9 overviews the process.
Face recog ition using Eigenfaces begins by treating
N × N images of faces (Figure 9a) as points in an N2
hyper-dimensional space (Figure 9b) s shown in Figure
/users/sjpinson/thesis/thesis/graphics/eigenFacesInANutshell.pdf
Fig. 9. Eigenfaces overview. (a) About 16 facial images. (b)
Faces as points in hyper-dimensional space (65,536 dimen-
sions for 256 × 256 images) where each pixel is a mutually
orthogonal direction. (c) Principal component analysis pro-
duces a reduced-dimensional space (∼7 dimensions), the ba-
sis of which are the eigenvectors corresponding to the largest
eigenvalues. (d) Known faces projected into face space. (e)
An unknown face is recognized if it is within a user-defined
Euclidean distance of a known face, otherwise it is not rec-
ognized.
/users/sjpinson/thesis/thesis/graphics/templateAsFeatureVector.pdf
Fig. 10. A face template viewed as a feature vector. The
feature vector consists of the image pixel intensities, concate-
nated in row major order.
10, where the vector elements represent pixel intensi-
ties in row major order from the upper left to the lower
right-hand corner of the image. In other words, the pixels
are mutually orthogonal directions in an N2-dimensional
space. Typically, N is around 256, which means the N2
hyper-dimensional space has 65,536 dimensions. After
observing that the distribution of faces in this space
will not be random, Turk and Pentland, apply princi-
ple component analysis to reduce the dimensionality of
the space. The reduced-dimension space, which h s ∼7
dimensions, is called face space. We now describe the
formation of face space.
Face Space Let Γ = Γ1,Γ2, . . . ,ΓM (Figure 11(a)) be a
set of M vectors of length N2 derived from N × N fa-
ci l images. The mean face (Figure 11(b)) is defined as
Ψ = 1M
￿M
i=1 Γi, and the difference image of the i
th vec-
Samuel James Pinson, Sr. and William Barrett: Connected Component Level Method Identification using Eigenfaces 7
/users/sjpinson/thesis/thesis/graphics/faces.pdf
(a) Facial images
us rs/sjpinson/thesi /thesis/graphics/faceMean.png
(b) Mean of faces
/users/sjpinson/thesis/thesis/graphics/faceDifferenceImageEquation.pdf
(c) Fac Difference Image
Fig. 11. Mean and difference images. The mean face is sub-
tracted from each of the M faces to create diffe nce images.
Image couresty of the Olivetti Research Laboratory [?].
Γ̂i = Γi−Ψ 11(c)). Let A =
￿
Γ̂1 . . . Γ̂M
￿
.
That is, the columns of A are the difference images of the
M vectors in Γ . The covariance matrix C for the set Γ
is the N2 by N2 ma rix where C =
￿
1
M
￿M
k=1 Γ̂iΓ̂
T
j
￿
=
1
M AA
T . The eigenvectors of the covariance m trix C
r found via singular value decomposition [?]. The sin-
gular value decomposition of a matrix A is given by
A = USV T . The eigenvectors of C = 1M AA
T are the
columns of U . We refer the interested reader to [?] for a
detailed deri ation.
Each eigenvector e￿i has a cor esponding eigenvalue.
The eigenvector corresponding to the largest eigenvalue
points in th irection of greatest variance in face space.
Ei ctors corresp nding to succ ssively smaller eigen-
values are mutually othogonal and point in the direction
of greatest remaining variance. Because pix l intensities
in images of faces re n t st tistically independent, rel-
atively few of the eigenvectors span most of the vari-
anc in the set Γ . The r ini g eigenvectors may be
dropped, thus reducing the dimensionality of the space
and making the rest of the algorithm co putationally
practical. The M ￿ eigen cto s retained form th basis
of a vector space, referred to as face space . In Figure 12
we treat these N2 -dimensional eigenvectors s N × N
/users/sjpinson/thesis/thesis/graphics/faceEigenvectorsAsImages.pdf
Fig. 12. Eigenfaces. The N2-dimensional eigenvectors of face
space, or eigenfaces, resemble ghostly faces when viewed as
N ×N images. Here we see the three most significant eigen-
faces, i.e. the directions that correspond to the three largest
variances in the distribution f faces. These eigenfaces were
derived from t e images in the Olive ti Research Laboratory
facial image database.
images. Note that they resemble ghostly faces, hence the
name eigenfaces.
Known Faces Once face space has been defined, we de-
termine which faces would like o recognize. Thes
faces are called k own faces (Figure 9d). Let K ￿ be the
projection of the set of known faces K into face space,
and let K￿i be the i
th element of K ￿. The jth compo ent
of K￿i is K
￿
ij = eTj (Ki −Ψ) = eTj K̂i, where ej is the jth
eigenvector of face space. That is, the jth component of
the projection of the ith known face is the dot ro uct
of the jth ig nvector with t e ith difference image.
N te that pr jecting into face space makes N2-dimensional
vectors become M ￿-dim nsional (M ￿ ￿ N2). For Turk
and P ntland this meant moving from a 65,536-dimensional
space to a 7-dimensional space.
Face Recognition Re ognition of a fac with unknown
identity, U, (Figure 9e) proceeds as follows: We subtract
the mea face fro U to et he difference image Û =
U−Ψ and project into face space to get U￿. A distance
threshold, θ is established empirically, and the unknown
face is classified b sed on i s distance from the known
faces in face space. Specifically,
class (U) =
￿
las i if ￿U￿ −K￿i￿ ≤ θ
unknown otherwise
(1)
where i = arg minj ￿U￿ −K￿j￿. In other words, an un-
known face is assigned to the class of the known face
that minimizes the Euclidean distance from the unknown
face. If the minimum Euclidean distance across all known
faces is greater than the threshold, the face is not recog-
nized (Figure 9e).
3.2 Method Identification by Eigenfaces
Figure 13 outlines our approach to connected component
level method ident fication (Compare with Figure 9).
B cause machine print is consistent, connected com-
ponents that correspond to machine-printed characters
Samuel James Pinson, Sr. and William Barrett: Connected Component Level Method Identification using Eigenfaces 7
/users/sjpinson/thesis/thesis/graphics/faces.pdf
(a) Facial imag s
us rs/sjpinso / hesi /thesis/graphics/faceMean.png
(b) Mean of faces
/users/sjpinson/thesis/thesis/graphics/faceDifferenceImageEquation.pdf
(c) Face Diff rence Image
Fig. 11. Mean and diff rence images. The mean face is sub-
tracted from e h of the M faces to cre t diffe nce images.
Images couresty of the Olivetti Research Laboratory [?].
tor as Γ̂i = Γi−Ψ (Figu 11 c)). L t A =
￿
Γ̂1 . . . Γ̂M
￿
That is, the columns of A re the difference images of the
M vectors in Γ . The covariance m trix C for the set Γ
is the N2 by N2 matrix where C =
￿
1
M
￿M
k=1 Γ̂iΓ̂
T
j
￿
=
1
M AA
T . The eigenvectors of the covariance matrix C
are found vi singular value decomposition [?]. The sin-
gular value decomposition of a matrix A is give by
A = USV T . The eigenvectors of C = 1M AA
T are the
columns of U . We refer the interested re d r to [?] for a
detailed derivation.
Each eigenvector e￿i has a corresponding eigenvalu .
The eigenvector corresponding to the largest eigenvalue
points in the direction of greatest variance in face space.
Eigenvectors corresponding to successively smaller eigen-
values are mutually othogonal a d point in th dir ction
of greatest remaining variance. Because pixel intensities
in images of faces are not st tist c lly independent, rel-
atively few of the eigenvectors span most of the vari-
ance in the se Γ . Th remai ing eigenvectors may be
dropped, thus reducing t e dimensionality of the space
and making the re t of th algorithm computationally
practical. The M ￿ eigenvectors retained form the basis
of a vector space, referred to as face space. In Figure 12
we treat these N2-dimensional eigenvectors as N × N
/users/sjpinson/thesis/thesis/graphics/faceEigenvectorsAsImages.pdf
Fig. 12. Eigenfaces. The N2-dimensional eigenvectors of face
space, or eigenfaces, resemble ghostly faces when viewed as
N ×N images. Here we see th three os significant eigen-
faces, i.e. the directions that correspond to the three largest
variances in the distribution of faces. These eigenfaces were
derived from the images in the Olivetti Research Laboratory
facial image database.
images. Note that they resemble ghostly faces, hence the
name eigenfaces.
K own Faces Once face space h s been defined, we de-
termine which faces we would li e to recognize. These
faces are called known faces (Figure 9d). Let K ￿ be the
projection of the se of kn wn faces K into face space,
a d let K￿i be the i
th element of K￿ . The jth component
of K￿i is K
￿
ij = eTj (Ki −Ψ) = eTj K̂i, where j is the jth
eigenvector f face sp ce. That is, he jth component of
the projection of the ith known face is the dot product
of the jth eigenv ctor with the ith d fference image.
Note that projecting into face space makes N2-dimensional
vectors become M ￿-dimensional (M ￿ ￿ N2). For Turk
nd Pentland this meant moving from a 65,536-dimensional
space to 7-dimensional space.
Face Recognition Recognition of a face with unknown
ide tity, U, (Figure 9e) proceeds as follows: We subtract
th mean face from U to ge th difference image Û =
U−Ψ nd proj ct into face space to get U￿. A distance
threshold, θ is est blished empir cally, and the unknown
face is classified based on its distance from the known
faces in face sp . Specifically,
class (U) =
￿
class i if ￿U￿ −K￿i￿ ≤ θ
unknown otherwise
(1)
where i = a g inj ￿U￿ −K￿j￿. In other words, an u -
known face is assigned to the class of the known face
that minimizes the Euclidean distance from the unknown
face. If the minimum Euclidean distance across all known
faces is greater than the threshold, the face is not recog-
nized (Figure 9e).
3.2 Method Identification by Eigenfaces
Figure 13 outlines our approach to connected component
level method identification (Compare with Figure 9).
Beca se machine print is consistent, connected com-
ponents that correspond to machine-printed characters
Samuel James Pinson, Sr. and William Barrett: Connected Component Level Method Identification using Eigenfaces 7
/users/sjpinson/thesis/thesi /graphi s/faces.pdf
(a) Facial images
us rs/ jp son/thesi /thesis/graphics/faceMean.png
(b) ean of faces
/users/sjpinson/thesis/thesis/graphics/faceDifferenceI ageEquation.pd
(c) Face Difference Image
Fig. 11. Mean an difference images. The e is sub-
tracted from each of the M faces to create i ages.
Images couresty of the Olive ti Research [?].
tor as Γ̂i = Γi−Ψ (Figure 11(c)).
￿ ˆ
1 . . . ˆ
￿
.
That is, the columns of A are the difference i ages of he
M vectors in Γ . The covariance matrix C for the set Γ
is the N2 by N2 matrix where C =
￿
1
M
M
k=1 Γ̂iΓ̂
T
j
￿
=
1
M AA
T . The eigenvectors of the covariance m trix C
are found via singular value decomposition [?]. The sin-
gular value decomposition of a matrix A is given by
A = USV T . The eigenvectors of C = 1M AA
T are the
columns of U . We refer the interested reader to [?] for a
detailed derivation.
Each eigenvector e￿i has a corresponding eigenvalue.
The eigenvector corresponding to the largest eigenvalue
points in the direction of greatest v riance in face space.
Eigenvectors corresponding to successively smaller eigen-
values are mutually othogonal and oint in t e dir ction
of greatest remaining variance. Because pixel intensities
in images of faces are not statisti lly independent, rel-
atively few of the eigenvectors span most of the vari-
ance in the set Γ . The remaining eigenvect rs may be
dropped, thus reducing the dimensionality of the space
and making the rest of the algorithm computationally
practical. T M ￿ eigenvectors retained form the basis
of a vector space, referred to as face space. In Figure 12
we treat these N2-dimensional eigenvectors as N × N
/users/sjpinson/thesis/thesis/graphics/faceEigenvectorsAsImages.pdf
Fig. 12. Eigenfaces. The N2-dimensi nal eigenvectors of face
space, or eigenfaces, semble ghostly faces when viewed as
N ×N images. Here we see the three m st significant eigen-
faces, i.e. the directions tha correspond to the three larg st
variances in the d stributi n of fac s. The e igenfaces were
derived from the images in the Olivetti Research Laboratory
facial image database.
images. Note that they resemble ghostly faces, hence the
name eigenfaces.
Known Faces Once face space has been defi ed, we de-
termine which faces we would like o reco nize. Th s
faces are call d known faces (Figure 9d). L t K ￿ be t e
projection of the set of known faces K into face space,
and let K￿i be the i
th ele e t of K ￿. The jth component
of K￿i s K
￿
ij = eTj (Ki −Ψ) = eTj K̂i, wh re ej is t e jt
eigenvector of face space. That is, th jth mp n nt of
the projection of the ith known face is the dot product
of the jth igenvect r with t e i th difference image.
Note that projecting into fac space makes N2- i ensional
vec ors beco e M ￿ -dimensional (M ￿ ￿ N2). For Tu k
a d Pe tland this meant moving from a 65,536-dimensional
space to a 7-dimen ional space.
Face Recognition Recognition of a face with unknown
identi y, U , (Figure 9e) proceeds as follows: We sub ract
the mean face from U to get the difference image Û =
U−Ψ and roject into face space to get U￿. A distance
threshol , θ is established empiri ally, and the unknown
f ce is cl ssified bas d its distance from the known
face in face space. Spe ifically,
class (U) =
￿
class i if ￿U￿ −K￿i￿ ≤ θ
unknown otherwise
(1)
where i = arg minj ￿U￿ −K￿j￿ . In other words, an un-
k own face is ssigned to he class of the known face
that inimizes the Euclidean distance from the unknown
face. If the minimum Euclidean distance across all known
faces is greater than the threshold, the fa e is not recog-
nized (Figure 9e).
3.2 Method Identification by Eigenfaces
Figure 13 outlines our approach to connected component
level m thod ide tification (C mpare with Figure 9).
Because machine print is consistent, connected com-
ponents that correspond to machine-printed characters
Samuel James Pinson, Sr. and William Barrett: Connected Component Level Method Identification using Eigenfaces 7
/users/s esis/thesis/graphics/faces.pdf
(a) Facial images
us r sjpinson thes /th sis/graphic /faceMean.png
(b) Mean f faces
/users/sjpinson/thesis/thesis/graphics/fac DifferenceImageEquation.pdf
(c) Face Difference I ge
Fig. 11. M an and difference images. The mean face is sub-
tracted from ach of the M faces to create differ nce im ges.
Images couresty of t Olivetti Research Laboratory [?].
tor as Γ̂i = Γi−Ψ (Figure 11(c)). Let A =
￿
Γ̂1 . . . Γ̂M
￿
.
hat is, the columns of A ar the difference i ages of the
vectors i Γ . The covaria ce matrix C fo the set Γ
i the N2 by 2 atrix re C =
￿
1
M
￿M
k=1 Γ̂iΓ̂
T
j
￿
=
1
M AA
T . The eig nvectors f the covariance matrix C
are found via singul r value decompos tion [?]. The si -
gul r value d omposition of a matrix A is giv n by
A = USV T . The eigenvectors of C = 1M AA
T are the
columns of U . We refer the interested reade to [?] f r a
detailed derivation.
Each eigenvector e￿i has a corresponding eigenvalue.
The eigenvector corresponding to the largest eigenvalue
points in the direction of greatest variance in face space.
Eigenvectors corresponding to successively smaller eigen-
values are mutually othogonal and point in the direction
of greatest remaining variance. Because pixel intensities
in images of faces are not statistically independent, rel-
atively few of the eigenvectors span most of the vari-
ance in the set Γ . The remaining eigenvectors may be
dropped, thus reducing the dimensionality of the space
and making the rest of the algorithm computationally
practical. The M ￿ eigenvectors retained form the basis
of a vector space, referred to as face spa e. In Figure 12
we treat these N2 -dimensional eigenv ctors as N × N
/users/sjpinson/thesis/thesis/graphics/faceEigenvectorsAsImages.pdf
Fig. 12. Eigenfaces. The N2-dimensional eigenvectors of face
space, or eigenfaces, resemble ghostly faces when viewed as
N ×N images. Here we see the three m st si nific nt eigen-
fac s, i.e. the directions that correspond to the three largest
variances in the distribution of faces. These eigenfaces were
erived from the images in the Olivetti Research Laboratory
fa ial image database.
images. Note that they resemble ghostly faces, hence the
name eigenfaces.
Known Fa s Once face space has been defined, we de-
termine which faces we would like to recognize. These
faces are called known faces (Figure 9d). Let K ￿ be the
projec ion f t e set of k own faces K into face space,
n l t K￿i be h i
th element of K ￿. The jth component
of K￿ is K ￿j = eTj (Ki −Ψ) = eTj K̂i, where ej is the jth
eigenvector of face space. That is, the jth component of
the project on of the th known face is the dot product
of the j h e genvector with the ith difference image.
Note that projecting into face space makes N2-dimensional
vect r become M ￿-dimensional (M ￿ ￿ N2). For Turk
and Pe tland this meant moving from a 65,536-dimensional
space to a 7-dimensional space.
Face Recognition Recognition of a face with unknown
identity, U, (Figure 9e) proceeds as follows: We subtract
the mean face from U to get the difference image Û =
U−Ψ and project into face space to get U￿. A distance
threshold, θ is established empirically, and the unknown
face is classified based on its distance from the known
faces in face space. Specifically,
class (U) =
￿
lass i if ￿U￿ −K￿i￿ ≤ θ
unknown otherwise
(1)
where i = arg minj ￿U￿ −K￿j￿. In other words, an un-
known face is assigned to the class of the known face
that minimizes the Euclidean distance from the unknown
face. If the minimum Euclidean distance across all known
faces is greater than the threshold, the face is not recog-
nized (Figure 9e).
3.2 Method Identification by Eigenfaces
Figure 13 outlines our approach to connected component
level method identification (Compare with Figure 9).
Because machine print is consistent, connected com-
ponents th t correspond to machine-printed characters
  The  eigenvectors  of (1) 
1395
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
  
Figure 6. Discrimination between machine print and handwritten text: (a) machine print connected components (CCs) 
(b) projection of CCs into N2 hyper-dimensional space (c) PCA to create reduced dimensional character space            
(d) projection of machine print representatives in character space (e) automatic local distance threshold selection from 
large sets of machine print and handwriting templates and user-specified precision (f) classification as machine print or 
handwriting (g) min graph cut recursive touching character segmentation by seeding source/sink with outside pixels. 
1396
3. RESULTS 
The NIST SD19 database consists of 3,699 handwriting 
forms such as in Figure 7. A certain partition of NIST 
SD19, hsf 4, with samples from 500 different writers has 
been designated by NIST for the purposes of reporting OCR 
results. We use hsf 4 to evaluate our method. 
After registering the images and removing the form 
boxes and name field, each connected component is 
classified as machine print (green) or handwriting (red) 
(Figure 7). Extremely small connected components, which 
do not provide enough information to make a confident 
classification, are not considered.  
The NIST SD08 database includes 360 binary images of 
machine print characters. Three styles (normal, bold, and 
italic), six fonts (Courier, Helvetica, New Century 
Schoolbook, Optima, Palatino, and Times Roman), and ten 
point sizes (4, 5, 6, 8 10, 11, 12, 15, 17, 20) are represented. 
Two images of randomly selected characters for each style, 
font, and size, were used to test our classifier. 
The confusion matrix in Table 1 summarizes our results. 
The upper left entry means that when we predicted a 
connected component to be machine print, we were right 
98.21% of the time. Similarly, the lower right entry means 
when we classified a connected component as handwriting, 
we were right 71.05% of the time. 
For a connected component to be classified as machine 
print it must lie within the local distance threshold of the 
nearest representative machine print template (Figure 4). 
High machine print precision implies that local distance 
thresholds are not too loose, else handwriting connected 
components would be mistaken for machine print. 
In general, there is a tradeoff between machine print 
precision and handwriting precision. However, in this case 
the low handwriting precision, 71.052%, has a specific 
cause: small machine print characters get mistaken for 
handwritten characters. The average height of machine print 
characters is 19.72 pixels with 40% of them < 20 pixels 
high, and over 93% < 23 pixels in height. 
As the size of connected components diminishes, so do 
many of the distinguishing features between machine print 
and handwriting. To test this assertion we replaced the 
machine print with a similar but larger font (about 40 pixels 
high) from NIST SD08. The results, shown in Table 2, show 
18% improvement in the precision of handwritten characters 
while maintaining ~94% precision with machine print.   
 
 Predicted Machine Print Predicted Handwriting 
Actually Machine Print 98.21% 1.8% 
Actually Handwriting 28.95% 71.05% 
 
Table 1. NIST SD19 hsf 4 confusion matrix. Over 98% machine print 
precision, but lower handwriting precision due to font size. 
 
 Predicted Machine Print Predicted Handwriting 
Actually Machine Print 93.98% 6.02% 
Actually Handwriting 10.9% 89.1% 
 
Table 2. NIST SD19 hsf 4 adjusted confusion matrix from replacing 
machine print in NIST SD19 with a larger font (~40 pixels high) from 
NIST SD08.  
 
 
 
 
 
 
 
 
 
 
 
 
Figures 1 and 8 are representative of results achieved for 
discriminating between machine print and handwriting. 
Most handwriting false positives result from touching 
characters or annotations that artificially connect 
components. Others result from too much similarity 
between machine print and handwriting representatives or 
insufficient discrimination of the eigenfaces algorithm.  
Figure 7. NIST SD19 handwriting sample form. Left: original.  Right:  
labeled – machine print (green), handwriting (red). Note false positives from 
touching machine print characters. 
 
Figure 8. Handwritten annotation and machine print discrimination 
captures annotations although some annotations cause false positives. 
. 
1397
3.1 Handwriting Accuracy by Writer 
           
 
Figure 9. Handwriting accuracy by writer. Each bin in the histogram shows 
the number of writers for which the labeled accuracy was achieved. 
 
Figure 9 shows the fraction of handwriting CCs correctly 
classified as handwriting for 500 writers. The worst 
accuracy for any single writer was 88.24%, the best 100%, 
with an average accuracy of 96-97%. 
3.2 Touching Character Segmentation 
About 13% of the handwriting false positives are touch-
ing machine print characters. To address this we created a 
recursive touching character segmentation algorithm using 
min graph cut to split touching characters by seeding 
source/sink with outside pixels (Figure 6). Application of 
the algorithm to 1,056 touching characters consisting of 
every pairwise combination of letters, in all combinations of 
upper and lower case, improved classification from 4.07% 
(without segmentation) to 89.19% (Table 3). 
 
 Without Segmentation With Segmentation 
Machine print precision 4.07% 89.19% 
 
Table 3. 4.07% machine print precision over 1,056 touching character 
CCs. Recursive segmentation increased precision to 89.19%. 
 
4. CONCLUSION AND FUTURE WORK 
Based on a user-supplied global target precision, and 
automated local threshold detection, discrimination of 
machine print from handwriting is performed with high 
precision (94%) over a large data set while maintaining 
handwriting precision at 89%. Automated selection of 
principle component features [18] could possibly increase 
precision while greatly reducing the dimensionality of the 
character space. Training on handwritten CCs and improved 
segmentation of touching characters and annotations (Fig. 8) 
would also likely improve precision/recall to match or 
exceed levels reported in [14] while competing with error 
rates reported in state-of-the-art approaches [18]. 
Handwritten annotations are effectively identified 
without the aid of color, although annotations that touch 
machine print cause false positives (Figure 8). Automated 
seeding of strokes in the character segmentation algorithm 
(Figure 6) could be used to decouple machine print from 
annotations. Grayscale, lexical and typographic context 
might also improve segmentation of touching characters.  
In general, machine print that is falsely labeled as 
handwriting occurs infrequently in the midst of correctly 
labeled machine print. (See Figures 1, 7 and 8.) These errors 
could be corrected contextually by inspecting local 
thresholds of the nearby machine print CCs to discover a 
better global fit at the word or sentence level. Elimination of 
font and style templates that are globally distant from the 
connected components in a document might also improve 
results. 
REFERENCES 
 
[1] T. Umeda and S. Kasuya, “Discriminator between handwritten and 
machine-printed characters,” http://patft.uspto.gov, 1990. 
[2] J. Franke and M. Oberlander, “Writing style detection by statistical 
combination of classifiers in form reader applications,” in Document 
Analysis and Recognition, 2nd Intl. Conference, pp. 581–584, 1993.  
[3] K. Kuhnke, L. Simoncini, and Z. Kovacs-V, “A system for machine-
written and hand-written character distinction,” 3rd Intl. Conf on 
Document Analysis and Recognition (Vol. 2), pp. 811–814, 1995.  
[4] S. Violante, R. Smith, and M. Reiss, “A computationally efficient 
technique for discriminating between hand-written and printed text,” 
pp. 1–7, 1995.  
[5] K. Fan, L. Wang, and Y. Tu, “Classification of machine printed and 
handwritten texts using character block layout variance,” Pattern 
Recognition, vol. 31, no. 9, pp. 1275–1284, 1998.  
[6] E. Kavallieratou, S. Stamatatos, and H. Antonopoulou, “Machine-
printed from handwritten text discrimination,” Intl. Workshop on 
Fontiers in Handwriting Recognition, pp. 312-316, 2004.   
[7] F. M. Wahl, K. Y. Wong, and R. G. Casey, “Block segmentation and 
text extraction in mixed text/image documents,” in Computer 
Graphics and Image Processing, vol. 20, pp. 375–390, 1982.  
[8] J. Guo and M. Ma, “Separating handwritten material from machine 
printed text using hidden markov models,” in Document Analysis and 
Recognition, 6th Intl. Conference, pp. 439–443, 2001.  
[9] Y. Zheng, H. Li, and D. Doermann, “Machine printed text and 
handwriting identification in noisy document images,” Pattern 
Analysis and Machine Intelligence, IEEE Transactions, vol. 26, no. 3, 
pp. 337–353, 2004. 
[10] M. Turk and A. Pentland, “Face recognition using eigenfaces,” in 
Computer Vision and Pattern Recognition, IEEE Computer Society 
Conference, pp. 586–591, 1991. 
[11] N. Muller and B. Herbst, “The use of eigenpictures for optical 
character recognition,” in Pattern Recognition, 14th Intl Conf, pp. 
1124–1126, 1998. 
[12] M.S. Shirdhonkar and Manesh B. Kokare, “Discrimination between 
Printed and Handwritten Text in Documents,” in IJCA Special Issue 
on “Recent Trends in Image Processing and Pattern Recognition” 
RTIPPR, pp. 131-134, 2010. 
[13] S. Chanda, K. Franke and U. Pal, “Structural handwritten and 
machine print classification for sparse content and arbitrary oriented 
document fragments,” in SAC’10, pp. 18-122, March 22-26, 2010. 
[14] X. Peng, S. Setlur, V. Govindaraju, R. Sitaram and K. Bhuvanagiri, 
“Markov Random Field Based Text Identification from Annotated 
Machine Printed Documents,” 10th Intl. Conference on Document 
Analysis and Recognition (ICDAR 2009), pp. 431-435, 2009. 
[15] T. Nakai, K. Kise, and M. Iwamura, “A Method of Annotation 
Extraction from Paper Documents Using Alignment Based on Local 
Arrangements of Feature Points,” 9th Intl. Conference on Document 
Analysis and Recognition (ICDAR 2007), pp. 23-27, 2007. 
[16] A. Mazzei, F. Kaplan and P. Dillenbourg, “Extraction and 
Classification of Handwritten Annotations,” in ACM 978-1-60558-
843-8/10/09, UbiComp, pp. 1-7, 2010. 
[17] M. Solli and R. Lenz, “FyFont: Find-your-Font in Large Font 
Databases,” in SCIA 2007, LNCS 4522, pp. 432–441, 2007. 
[18] S. Wang, H. Baird, and C. An, “Document Content Extraction Using 
Automatically Discovered Features,” in 10th Intl Conf on Document 
Analysis and Recognition, pp. 1076-1080, 2009.  
Handwriting accuracy by 
writer for 500 writers 
1398
