Automatic Genre Classification by Using Co-training 
 
 
Rui Liu, Minghu Jiang, Zheng Tie 
Lab. of Computational Linguistics, School of Humanities and Social Sciences, 
Tsinghua University, Beijing 100084, China 
jiang.mh@tsinghua.edu.cn 
 
Abstract 
 
Researchers have concentrated on topic-based text 
classification while the genre of a document is rarely 
considered. In this article, we discuss the automatic 
genre classification and its application. We argue that 
word level features and sentence level features are two 
important measures which vary in number among 
different genres. Word level features include word 
frequency and POS (Part of Speech) tag statistics. 
Sentence level features include grammar rules, which 
have strong relations between different genres. Based 
on the two aspects of view, we explore a robust 
approach where the Co-training method is employed to 
obtain high effectiveness for genre classification. 
 
1. Introduction 
 
The rapid increase in digital resources makes it 
difficult for people to distinguish the valuable 
knowledge they need from the overwhelm information. 
Automated text classification, which is a particularly 
challenging task, can help solve the problem. In the 
past decades, researchers have concentrated on topic-
based text classification, while the genre–based 
classification is seldom touched.  
Genre considerations are one of the most important 
factors in determining what a person will see or read. 
Many genres have their own audiences and 
corresponding publications that support them, such as 
poems and fictions. Classifying web pages by genre 
would make it easier for us to have access to important 
information. For example, when a user expect to find 
poems about “summer”, she may feel troublesome 
facing the mixed results of poems and other genres of 
documents returned by search engine. Considering 
users’ difficulty in searching process, it would be 
worthwhile to determine the genre of a document.  
However, defining genres is far from simple 
because different perspectives can be defined into 
different genres. Not only documents can be divided 
into spoken and written language, but also can be 
divided into three basic types, which are the classic 
genres of Ancient Greece: poetry, drama, and prose. 
Different from the issue of identifying topics, genre 
definition is controversy. Longacre [1] suggested that 
there were four genres of texts: narration, procedural 
discourse, behavioral discourse and expository 
discourse. Swales’s description [2] sounds more 
reasonable for web pages: genres are collections of 
communicative events with shared communicative 
purposes which can vary in their prototypicality. These 
perspectives make it possible to understand genre in a 
direct and useful manner. Based on these perspectives, 
a rough taxonomy (Section 3) of ten classes is 
collected in our experiment. 
Challenges associated with automated genre 
classification come from two main fronts: the selection 
of appropriate features to represent the genres and the 
learning algorithms to train and classify the text data. 
For selecting the effective features, in the topic-based 
text classification, the word frequency information 
which is also called the bag-of-words approach, is the 
most widely used one. However, topic-based 
techniques which have been successful for text 
classification may not be sufficient when classifying 
the genre of a document. In fact, our result shows that 
using word frequency information only can hardly 
achieve a good result.  
When it comes to early works, two basic types of 
features are introduced: 
1). The exterior information of documents that uses 
hyperlinks, URL and HTML tags [3] relating to 
documents. 
2). The inner information such as function word 
frequencies, and POS tags etc [4] [5] [6].  
Although previous work has tried various features, 
little in-depth analysis relating to sentence level 
features is taken into account. Also, the exterior 
information of texts such as hyperlinks, URL and 
HTML tags are not always valuable and obtainable. 
For the learning algorithms, Kessler [5] chose 
logistic regression as the basic numerical method, 
Karlgren and Cutting’s algorithm [4] used discriminate 
analysis to predict the new individuals, Boese and 
Howe [3] used LogitBoost with a list of features. These 
2009 Sixth International Conference on Fuzzy Systems and Knowledge Discovery
978-0-7695-3735-1/09 $25.00 © 2009 IEEE
DOI 10.1109/FSKD.2009.609
129
Authorized licensed use limited to: Aegean University. Downloaded on July 27,2010 at 15:36:34 UTC from IEEE Xplore.  Restrictions apply. 
learning methods require a large amount of training 
instances, which is time consuming and difficult to 
obtain. Moreover, multiple aspects of features are not 
well organized in the previous work. 
Analysis of these problems in current web search 
engines and previous work leads us to explore a robust 
system for the genre classification. In this paper, the 
knowledge of texts is represented from two aspects of 
view: their lexical level and syntactic level. Classifiers 
are trained by Co-training [7]. Co-training is a semi-
supervised algorithm in which two learners are 
iteratively combining their outputs to increase the 
training set used to re-train each other and generate 
more labeled data automatically. This approach trains 
two classifiers corresponding to lexical and syntactic 
perspectives of documents. 
This paper is organized as follows: Section 2 
presents genre corpus, section 3 gives out the method 
about genres features, section 4 puts out the method of 
co-training, section 5 is the experimental results, and 
Section 6 concludes the paper. 
 
2. Corpus based on genre 
 
In general, the already existing data sets were rarely 
built for the text genre detection. The topic based data 
sets are not suitable for genre classification, and thus a 
genre corpus is composed. 
Text can be divided into four types according to its 
structural feature or its writing purpose: narrative, 
procedural, behavioral, and expositive. We have 
collected these four main categories containing 3309 
(1500 documents are used for training set, and others 
for test set.) documents with ten subcategories as listed 
in Table 1.  
 
 3. Features 
 
Before we can design features to classify texts 
relating to their genres, we must determine the factors 
that people use to judge the text genre. We surveyed 
many readers, and asked them to list the differences 
between various genres. Based on the survey, we found 
three distinctive factors: words, POS tags, and 
syntactic characteristics.  
Words and POS tags are seen as lexical level 
features. Different from topic based classification, the 
function words, which have no lexical meanings, can 
be used to determine the genre of text. Examples of 
function words include pronouns, conjunctions, and 
auxiliary verbs etc. POS tags are used to express 
grammatical relationships and they are good indicators 
for the type of genres. For example, pronouns such as 
he (him) and she (her) are more likely to appear in 
dialogs rather than in Repair manuals. To represent these 
features we employ the most widely-used bag-of-
words approach: each document d is represented by a k 
dimensions vector of word w1...k and an m dimensions 
POS-tag vectors p1...m. 
For syntactic level, a total of 24 features are used in 
our experiments. Examples of these elements are the 
average length of sentences, quotations (She writes, “I 
see.”), passives (Java can be found at 
http://java.sun.com.), subjunctives (Without heat and 
sunlight, plants on the earth would not grow well.), 
cues (such as Warning:/Notice:), and a set of 16 verb 
sub-categorizations. A part of list of verb sub-
categorizations features analyzed are shown as the 
following: 
1. [NP]: Make sure that you download and install 
[the SDK]. 
2. [NP][NP]: An accident that taught [me] [a 
valuable lesson]. 
3. [NP][PP]: America is [your source] [to the 
source] for travel throughout the United States. 
4. [that-S]: …, it is suggested [that you install all 
cygwin packages]. 
5. [wh-S]: Find out [what you can do in the UI 
package by typing]. 
6. [inf-S]: Sam told me [to go]. 
7. [verb-ing]: She keeps [laughing] at me. 
8. [NP][that-S]: You can try running [an extraction 
experiment with the data] [that you have 
created]. 
9. [NP][wh-S]: They asked [me] [why I believe in 
you]. 
10. [PP]: Sarah Murray follows the journey our 
dinner makes [from field] to plate. 
… 
Genres Number of 
documents 
Total 
Narrative A. Reportage 138 1077 
B. Fiction 279 
C. News 360 
D. Essay 300 
Procedural  E. Online help 592 1204 
F. Repair manuals 120 
Behavioral  G. Speech 292 613 
H. Dialog 321 
Expository  J. Guidebook 153 451 
K. Adv. 298 
Table 1. Text corpora for genre. 
130
Authorized licensed use limited to: Aegean University. Downloaded on July 27,2010 at 15:36:34 UTC from IEEE Xplore.  Restrictions apply. 
In our experiments the syntactic level features 
perform better than lexical level’s especially when the 
genres are topic free.  
 
4. Using Co-training system 
 
Co-training [7] is introduced by Avrim Blum and 
Tom Mitchell, which has been applied in many 
applications. Co-training is able to use very few 
manually labeled examples to learn two independent 
classifiers based on different feature subsets. The 
classifiers then can provide each other with labels for 
the unlabeled data.  
In our algorithm, learners of lexical and syntactic 
levels are iteratively combining their outputs to 
increase the training set which is used to retrain each 
other and generate more labeled data automatically. 
We use two views to solve the problem and the results 
illustrate the good performance of the syntactic level 
features. Co-training model can be described as the 
following: 
1). Let (x, y) be a labeled instance where the instance 
Xx ∈  and the label { 1, 1}.y ∈ − + Assume that the 
feature space can be viewed from two sets: 1 2,X X X= ×  
where 1X and 2X respectively represent two different 
views of a document, and thus in the two-view 
problem a labeled instance (x, y) can be represented as 
(x1, x2, y), where 1 1 2 2,x X x X∈ ∈ ; 
2). 1 1 2 2 1 2 1 2, : ( , | ) ( | ) ( | ),x X x X p x x y p x y p x y∀ ∈ ∈ =  
i.e., the two sets are conditionally independent; 
3). 1X and 2X are compatible: when there are 
hypotheses 1 1: { 1, 1},h X → − + 2 2: { 1, 1},h X → − + let 
f(x) be a decision function, then for any example 
1 2( , )x x x= labeled as l, we have f1(x1) =  f2(x2)=l. 
In the application of algorithms, we select two 
mutually independent attribute sets and use SVM 
(Support Vector Machine) learner for each of the two 
classes. An extension for SVM is TSVM [8], which 
can use unseen data. Given labeled data 
1 1( , ),....,( , )n nx y x y , unlabeled data * *1,..., kx x , parameters C, 
C*, and penalizing variables *,ξ ξ  , to find the 
labels * *1 ,..., ky y of test data, we need to obtain the 
hyperplane ,w b< > to separate both training and test 
data with maximum margin. The problem is: over 
* * * *
1 1 1( , ..., , , , , ..., , , ..., )k n ky y w b ξ ξ ξ ξ , 
2 * *
1 1
1min
2
n k
i j
i j
w C Cξ ξ
= =
+ +   
Subject to: 
1
* *
1
1
*
1
: [ ] 1
: [ ] 1
: 0
: 0
n
i i i i
k
j j j j
n
i i
k
j j
y w x b
y w x b
ξ
ξ
ξ
ξ
=
=
=
=
∀ ⋅ + ≥ −
∀ ⋅ + ≥ −
∀ ≥
∀ ≥
 
An optimization algorithm is necessary for co-
training because the learners run repeatedly in a dataset 
with labeled and unlabeled data. The algorithm of our 
co-training is shown as follows: 
Input: Dataset D, labeled data Dl, unlabeled data Du, 
the number of iterations T. 
1. Initialize: Use the attributes in X1, X2 to train 
initial transductive support vector machine f1, f2 
on labeled data Dl respectively.  
2. while t <  T  
             For v = 1 to 2 
lD
+ ← Predict the most confident 
unlabeled ones as positive via decision 
function ( )tv if x ; 
'u u lD D D
+← − ; 
lD
− ←  Predict the most confident 
unlabeled ones as negative via decision 
function ( )tv if x ; 
'u u lD D D
−← − ; 
Update the parameter for TSVM learner; 
Training fv; 
'u uD D← ; 
              End 
End 
 
5. Experimental results 
 
For syntactic level, we use some templates to 
generate cue features such as “Warning:/whether 
/if…/should”. Accurate NLP tool is very important 
since we use a lot of POS-tagging and chunks. Our 
implementation attempts to exploit OpenNLP tools [9] 
for the extraction of POS-tagging information.  
Before applying a learning algorithm, we use 
Sequential Forward Selection method [10] in order to 
deduce the run time of the algorithm and find the best 
set of features to improve the accuracy. Sequential 
Forward Selection method is a bottom-up search 
procedure, where one feature at each stage is added to 
the current feature set. 
The Co-training method is operated on the text 
corpus by using two level features. To solve multi- 
class learning problems, we use the well known “one-
vs-all” scheme, which classifies one class against all 
others. Three experimental results are evaluated: 
lexical level, syntactic level by using TSVM, and the 
131
Authorized licensed use limited to: Aegean University. Downloaded on July 27,2010 at 15:36:34 UTC from IEEE Xplore.  Restrictions apply. 
Co-training. And the classification errors are 32.6%, 
30.5% and 24.2% on average respectively. Details are 
shown in Table II. It is not surprisingly to note genre 
classification is a difficult task especially when the 
classes are homogeneous. We suppose this is the 
reason why the performance of “fiction” (44%) and 
“essay” (72%) is rather poor. We also find that the 
error of “online help” in lexical level is 33%, the 
syntactic achieves is 14% and there’s a significant 
improvement in Co-training (3%). We conclude that 
the syntactic information used in training significantly 
affected our experimental results.  
 
6. Conclusion 
 
Fortunately current NLP tools are able to produce 
accurate results for POS-tagging, and thus it is possible 
to investigate the grammar rules about text genre. We 
find there are significant differences among different 
genres related to syntactic information, which can be 
used to predict genre labels. Our experiment has shown 
the positive effect of Co-training method. It is worth to 
note that the syntactic information affected 
classification significantly, although the effect varies 
among different classes. We also find that texts are 
difficult to classify especially when the classes are 
homogeneous. In general, the exterior information does 
not always work well, and this motivates us to propose 
a more robust Co-training method. But there’s still a 
long way to go. In the future we will try to increase the 
accuracy of the task and to reduce the time-consuming 
of features extraction. 
 
Acknowledgement 
 
This work was supported by the National Natural 
Science Foundation in China (No. 60673109 & 
60871100), State Key Lab of Pattern Recognition 
Open Foundation, Chinese Academy of Sciences, and 
the Teaching and Scientific Research Foundation for 
the Returned Overseas Chinese Scholars, State 
Education Ministry. 
 
References 
 
[1] R. E. Longacre, The Grammar of Discourse, 
Plenum Press, New York, 1983.  
[2] J. Swales, Genre Analysis, Cambridge University 
Press, Cambridge, 1990. 
[3] E. Boese and A. Howe, Effects of Web Document 
Evolution on Genre Classification, ACM 
Fourteenth Conference on Information and 
Knowledge Management, Germany, 2005. pp. 
632-639. 
[4] J. Karlgren and D. Cutting, Recognizing text genres 
with simple metrics using discriminant analysis, 
Proceedings of the 15th conference on 
Computational linguistics, 1994. pp. 1071–1075.  
[5] B. Kessler, G. Nunberg, and H. Sch tze, Automatic 
detection of text genre, Proceedings of the Thirty-
Fifth Annual Meeting of the Association for 
Computational Linguistics and Eighth Conference 
of the European Chapter of the Association for 
Computational Linguistics, New Jersey, 1997. pp. 
32-38. 
[6] E. Stamatatos, N. Fakotakis and G. Kokkinakis, 
Automatic text categorization in terms of genre 
and author, Computational Linguistics, 2001, 
26(4):471–495. 
[7] A. Blum and T. Mitchell, Combining Labeled and 
Unlabeled Data with Co-training, Proceedings of 
the 11th Annual Conference on Computational 
Learning Theory, 1998. pp. 92-100. 
[8] G. X. Zhang, W. D. Jin and L. Z. Hu, Quantum 
evolutionary algorithm for multi-objective 
optimization problem, Proceedings of the IEEE Int 
Sym on Intelligent Control. 2003. pp. 703 – 7081. 
[9] OpenNLP, "OpenNLP." Website. [Online]. 
Available: http://opennlp.sourceforge.net/, 2008. 
[10] J. Kittler, Feature set search algorithms, In C. H. 
Chen, editor, Pattern Recognition and Signal 
Processing, The Netherlands, 1978. pp. 41–60.  
 
Genres 
Errors (%) 
Lexica
l level 
Syntactic 
level 
Co-
training 
Narrative A. 
Reportage 28 24 16 
B. Fiction 45 57 44 
C. News 26 44 23 
D. Essay 39 52 72 
Procedural E. Online 
help 33 14 3 
F. Repair 
manuals 19 10 7 
Behavioral G. Speech 35 22 20 
H. Dialog 21 16 10 
Expository J. Guide-
book 18 36 18 
K. Adv. 62 30 29 
Table 2. Overall performance. 
132
Authorized licensed use limited to: Aegean University. Downloaded on July 27,2010 at 15:36:34 UTC from IEEE Xplore.  Restrictions apply. 
