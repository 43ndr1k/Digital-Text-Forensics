 
 
PLEASE SCROLL DOWN FOR ARTICLE
This article was downloaded by: [HEAL-Link Consortium]
On: 3 March 2009
Access details: Access Details: [subscription number 786636557]
Publisher Taylor & Francis
Informa Ltd Registered in England and Wales Registered Number: 1072954 Registered office: Mortimer House,
37-41 Mortimer Street, London W1T 3JH, UK
Journal of Experimental & Theoretical Artificial Intelligence
Publication details, including instructions for authors and subscription information:
http://www.informaworld.com/smpp/title~content=t713723652
The effect of borderline examples on language learning
Katia Lida Kermanidis a
a Department of Informatics, Ionian University, Corfu, Greece
Online Publication Date: 01 March 2009
To cite this Article Kermanidis, Katia Lida(2009)'The effect of borderline examples on language learning',Journal of Experimental &
Theoretical Artificial Intelligence,21:1,19 — 42
To link to this Article: DOI: 10.1080/09528130802113406
URL: http://dx.doi.org/10.1080/09528130802113406
Full terms and conditions of use: http://www.informaworld.com/terms-and-conditions-of-access.pdf
This article may be used for research, teaching and private study purposes. Any substantial or
systematic reproduction, re-distribution, re-selling, loan or sub-licensing, systematic supply or
distribution in any form to anyone is expressly forbidden.
The publisher does not give any warranty express or implied or make any representation that the contents
will be complete or accurate or up to date. The accuracy of any instructions, formulae and drug doses
should be independently verified with primary sources. The publisher shall not be liable for any loss,
actions, claims, proceedings, demand or costs or damages whatsoever or howsoever caused arising directly
or indirectly in connection with or arising out of the use of this material.
Journal of Experimental & Theoretical Artificial Intelligence
Vol. 21, No. 1, March 2009, 19–42
The effect of borderline examples on language learning
Katia Lida Kermanidis*
Department of Informatics, Ionian University, Corfu, Greece
(Received 31 July 2007; final version received 16 March 2008)
Imbalanced training sets, where one or more classes are heavily underrepresented
in the training data compared to the others, prove to be problematic when trying
to classify new instances that belong to a rare class. In the present article, class
imbalance occurs in the datasets of three language learning applications:
automatic identification of verb complements, automatic recognition of semantic
entities and learning taxonomic relations. All three applications are tested on
Modern Greek text corpora; verb complement identification is applied to English
corpora also, and comparative results are presented. The difference in statistical
behaviour between the classes may be attributed to the low pre-processing level of
the corpora, and/or the automatic nature of the pre-processing phase, and/or the
exceptional occurrence of the linguistic information of interest in the data. Two
different approaches are experimented with in order to deal with the problem:
one-sided sampling (OSS) of the dataset and classification using support vector
machines (SVMs). OSS removes redundant, noisy and misleading instances of the
majority class, reducing thereby the training set size, while SVMs, without
removing any instances, take into account only examples that appear close to
the borderline region between the classes. The better results achieved by OSS in
all test cases lead to some very interesting observations regarding the impact of
the various training instances on classification, depending on their position in the
feature-vector space.
Keywords: support vector machines; one-sided sampling; class imbalance;
natural language learning; Modern Greek
1. Introduction
Linguistic labelling of textual elements, i.e. assigning a syntactic or semantic tag to
sentence constituents (e.g. tokens, words, phrases, etc.), faces very often the problem of
disproportion when applied to free natural language text. In other words, the targeted
linguistic information is often hidden among large quantities of irrelevant data that is in
most cases useless and/or misleading.
The reasons behind this disproportion between the entities of interest and the rest of
the entities can vary. Sometimes the interesting information is the linguistic exception
rather than the rule, and is therefore sparse. Sometimes, low-level pre-processing of the
text does not permit sophisticated filtering of the useless elements, allowing thereby a large
amount of noise to appear in the final dataset.
*Email: kerman@ionio.gr
ISSN 0952–813X print/ISSN 1362–3079 online
 2009 Taylor & Francis
DOI: 10.1080/09528130802113406
http://www.informaworld.com
D
o
w
n
l
o
a
d
e
d
 
B
y
:
 
[
H
E
A
L
-
L
i
n
k
 
C
o
n
s
o
r
t
i
u
m
]
 
A
t
:
 
1
7
:
5
0
 
3
 
M
a
r
c
h
 
2
0
0
9
Given a two-class classification problem (interesting/not interesting examples), this
noise appears in the form of a high disproportion between the number of positive
(interesting) and negative (not interesting) examples, the former being heavily under-
represented in the data compared to the latter. This disproportion causes a significant drop
in accuracy when trying to predict the class of a new minority example, as the nearest
neighbour of any example is more likely to be negative.
In previous research, the class imbalance problem has been dealt with in different
ways (Domingos 1999). One approach is oversampling the minority class, i.e. repeating
the minority examples until the dataset consists of an equal number of positive and
negative examples (Japkowicz 2000). Another is synthetic oversampling by interpolating
between several minority class examples that lie together (Chawla et al. 2002).
Undersampling the majority class (either random or focused) reduces the number of
negative examples, while the positive ones remain untouched. One-sided sampling (OSS;
of particular interest in the present work) is a focused undersampling method that leads
to a more balanced subset of the initial training set by pruning out noisy, borderline
and redundant instances of the majority class. This approach has been used in the past
in several domains such as image processing (Kubat and Matwin 1997), medicine
(Laurikkala 2001), text categorisation (Lewis and Gale 1994). In general, under-
sampling the majority class leads to better classifier performances than oversampling
the minority class (Chawla et al. 2002), which is the reason for choosing this sampling
method in the present work.
Another way to avoid the bad effects of class imbalance is the use of sophisticated
machine-learning algorithms that do not rely on all the training instances in order to create
a decision function. Support vector machines (SVMs) (Cristianini et al. 1998; Vapnik
1998), for example, take into account only borderline examples (examples that are close to
the boundary region between the two classes) in order to make a classification decision.
Therefore, it is expected that their performance is not influenced by the large number of
redundant negative examples.
In the present work, these two methodologies, i.e. the removal of noisy, borderline and
redundant negative examples versus the inclusion of only borderline examples in the
classification process, are experimented with and evaluated on three distinct linguistic
tagging applications: verb argument identification, named entity recognition and
taxonomy learning. The results are bound to show the effect of the various categories
of the negative examples on language learning. All three applications are developed for
and tested on Modern Greek corpora, with the exception of verb argument identification,
where comparative results with English are also presented.
2. One-sided sampling
Instances of the majority class can be categorised into four groups (dots in Figure 1):
noisy are instances that appear within a cluster of examples of the opposite class
(crosses in Figure 1), borderline are instances close to the boundary region between the
two classes, redundant are instances that can be already described by other examples of
the same class and safe are instances crucial for determining the class. The dashed line
in Figure 1 shows the decision boundary between the two classes. Instances belonging
to one of the three first groups need to be eliminated as they do not contribute to class
prediction.
20 K.L. Kermanidis
D
o
w
n
l
o
a
d
e
d
 
B
y
:
 
[
H
E
A
L
-
L
i
n
k
 
C
o
n
s
o
r
t
i
u
m
]
 
A
t
:
 
1
7
:
5
0
 
3
 
M
a
r
c
h
 
2
0
0
9
Noisy and borderline examples can be detected using Tomek links (Tomek 1976). Two
examples, x and y, of opposite classes have a Euclidean distance of (x, y). This pair of
instances constitutes a Tomek link if no other example exists at a smaller distance to x or y
than (x, y).
Redundant instances may be removed by creating a consistent subset of the initial
training set. A subset C of training set T is consistent with T, if, when using the nearest
neighbour (1-NN) algorithm, it correctly classifies all the instances in T. To this end, the
algorithm starts with a subset C consisting of all complement examples and one non-
complement example. A learner is trained with C and tries to classify the rest of the
instances of the initial training set. All misclassified instances are added to C, which is the
final reduced dataset. The exact process of the proposed algorithm is:
(1) Let T be the original training set, where the size of the negative examples
outnumbers that of the positive examples.
(2) Construct a dataset C, containing all positive instances plus one randomly selected
negative instance.
(3) Classify T with 1-NN using the training examples of C and move all misclassified
items to C. C is consistent with T, only smaller.
(4) Remove all negative examples participating in Tomek links. The resulting set Topt
is used for classification instead of T.
3. Support vector machines
Another solution to the class imbalance problem is the use of a machine-learning
algorithm whose classification performance is not affected by superfluous examples of one
class. SVMs take into account only examples that are close to the boundary region
between the two classes, i.e. borderline examples.
Figure 1. The four groups of the majority class instances.
Journal of Experimental & Theoretical Artificial Intelligence 21
D
o
w
n
l
o
a
d
e
d
 
B
y
:
 
[
H
E
A
L
-
L
i
n
k
 
C
o
n
s
o
r
t
i
u
m
]
 
A
t
:
 
1
7
:
5
0
 
3
 
M
a
r
c
h
 
2
0
0
9
Given a two-class classification problem, a training set D is a pair {(xi, yi)}, i¼ 1 to n,
with each m-dimensional input vector xi 2 Rm and each binary label vector yi2 {1,þ1}
corresponding to the two classes. n is the number of given training examples. SVMs
separate the positive and negative examples by a hyperplane (continuous line in Figure 2)
expressed as (w  x)þ b¼ 0 (w2Rm, b2R). SVMs find the optimal hyperplane, i.e. the
hyperplane that maximises the margin (distance between the two parallel dashed lines)
between the two classes. The two dashed lines can be expressed as (w  x)þ b¼1. Margin
(M) can be expressed as M¼ 2/kwk.
The problem of maximising the margin becomes equivalent to solving the following
optimisation problem: LðwÞ ¼ ð1=2Þkwk2 needs to be minimised, subject to
yi½ðw  xiÞ þ bi  1(i¼ 1, 2, . . . ,m).
The examples that lie on either of the two dashed lines are called support vectors, and
are the only examples that play a role in classification. Even if all the remaining examples
(except for the support vectors) are removed, the same decision function is obtained.
In other words, according to the terminology of the previous section, redundant and safe
examples are ignored.
In practice, the examples of the two classes are usually not linearly separable due to
noise. However, a linear hyperplane may be built if some misclassifications are allowed.
The above problem may be solved by substituting every dot product of xi, xj with a kernel
function K(xi, xj) (Vapnik 1998). The most commonly used kernel function is the
polynomial kernel, Kðxi, xjÞ ¼ ðxi  xj þ 1Þd.
4. Test case 1: identifying verb arguments
Among the dependents of a verb, i.e. the syntactic terms that are dominated by the verb,
arguments are key participants in the event described by the verb (they play leading roles
Figure 2. The hyperplane (continuous line) separating the instances of the two classes.
22 K.L. Kermanidis
D
o
w
n
l
o
a
d
e
d
 
B
y
:
 
[
H
E
A
L
-
L
i
n
k
 
C
o
n
s
o
r
t
i
u
m
]
 
A
t
:
 
1
7
:
5
0
 
3
 
M
a
r
c
h
 
2
0
0
9
in the event), while adjuncts comprise secondary information concerning merely the
‘setting’ of the event (its context, location, etc.). Complements constitute important
information for most natural language processing applications (parsing, thematic role
assignment, identification of selectional restrictions, etc.)
We spoke to John. (complement)
We spoke in the garden. (non-complement)
The morphosyntactic properties of the arguments of a verb are strongly linked to the verb
sense. Different verbs take different syntactic arguments. Even different senses of the same
verb may take different syntactic arguments.
The present test case addresses the issue of the automatic distinction of verb
complements (i.e. all arguments except for the subject) from the rest of the sentence terms,
in Greek and English text corpora. Pre-processing of the corpora reaches the stage of
intrasentential, non-embedded chunking (Section 4.2), due to the unavailability of
sophisticated, wide-coverage tools and resources for Modern Greek (MG) and for the
majority of languages. The low-level pre-processing allows for a significant amount of
noise to appear in the data because of its inability to distinguish between verb dependents
and the rest of the sentence constituents, leading to a large number of irrelevant terms
being included in the list of candidate complements of a verb.
In previous work in automatic complement-adjunct distinction, Buchholz (1998)
uses memory-based learning on the part-of-speech (pos) tagged and phrase structured
part of the Wall Street Journal with a generalisation accuracy of 91.6% and she
includes verb subcategorisation information in her data. Merlo and Leybold (2001) use
decision trees to distinguish prepositional arguments from prepositional modifiers. They
incorporate semantic verb class, preposition and noun cluster information and reach an
accuracy of 86.5% with a training set of 3692 and a test set of 400 instances. Aldezabal
et al. (2002) work on Basque. They apply mutual information and Fisher’s Exact Test
to verb-case pairs (a case is any type of argument), which were obtained from a
partially parsed newspaper corpus of 1.3 million words. Evaluation was performed by
human tagging of the dependents of 10 test verbs outside (55% f-measure) and inside
(95% f-measure) the context of the sentence. Many researchers have attempted to
distinguish complements from adjuncts as a prerequisite for identifying verb
subcategorisation frames: Sarkar and Zeman (2000) use a treebank and iteratively
reduce the size of the candidate frame to filter out adjuncts. Briscoe and Carroll (1997)
and Korhonen et al. (2000) use a grammar and a sophisticated parsing tool for
argument-adjunct distinction.
Unlike previous approaches that distinguish complements from adjuncts (having
already distinguished these two categories from the rest of the sentence terms during
pre-processing), in the present work complements are distinguished directly from all the
remaining sentence constituents (including adjuncts), as adjuncts have not been identified
in some previous phase. It is therefore a binary classification problem (complements/
non-complements).
4.1. Complements in MG and English
Concerning morphology, MG is highly inflectional. The pos, the grammatical case and the
verb voice are key morphological features for complement detection. Concerning sentence
Journal of Experimental & Theoretical Artificial Intelligence 23
D
o
w
n
l
o
a
d
e
d
 
B
y
:
 
[
H
E
A
L
-
L
i
n
k
 
C
o
n
s
o
r
t
i
u
m
]
 
A
t
:
 
1
7
:
5
0
 
3
 
M
a
r
c
h
 
2
0
0
9
structure, MG is a ‘semi-free’ word-order language. The arguments of a verb do not have
fixed positions with respect to the verb and are therefore determined primarily by their
morphology rather than their position.
Certain semantic verb attributes are also very significant in MG: the verb’s copularity,
its mode and whether it is (im)personal. A verb is copular when it assigns a quality to its
subject. Mode is the property that determines the semantic relation between the verb and
its subject (whether the latter affects or is affected by the verb action). Although all of these
features are normally context-dependent, there are verbs with a priori known values for
them. This a priori information is taken into account in our final dataset, as context-
dependent semantic information could not be provided automatically, and we tried to keep
manual intervention to a minimum.
The following examples show some characteristic cases of verb complements in MG
(Klairis and Babiniotis 1999). Complements are underlined. A complement may be a noun
phrase (NP) in the accusative or the genitive case (a, b, f), a prepositional phrase (PP) (d)
or a secondary clause (e). Often the complements appear within the verb phrase itself in
the form of weak personal pronouns (c). Copular verbs can only take as an argument a
noun or adjective in the nominative (predicative) (b). In example f, although the verb is
morphologically in the passive voice, it has an active mode, i.e. the complement is affected
by the subject via the verb’s action.
(a) Zste to wtapódi se mZlZ ’ot #.
Grill the octopus in low heat.
(b) O Þrgo& ’iet aó1.
The George seems nice.
(George seems nice)
(c)  	o
rise& to oói o
;
Will me give the watch your?
(Will you give me your watch?)
(d) ersseue e a"a "a 	oui;
Is one too many to anyone a pencil?
(Does anyone have one too many pencils?)
(e) O  #slo& l"e óti o H	o1 Zta o 	eauteo1 poitZ1.
The teacher says that the Homer was the greatest poet.
(The teacher says that Homer was the greatest poet.)
(f) O 
ologstZ& eeerg #et ta dedo	"a.
The computer processes the data.
Each of the above features is important but not definitive on its own for complement
detection. When combined, however, and including context information of the
candidate complement, many cases of ambiguity are correctly resolved. The biggest
sources of ambiguity are the accusative NP, which is very often adverbial denoting
usually time (g, h), and the PP introduced by se (to), also often adverbial, denoting
usually place (i, j).
(g)  ’ #o to  !	. (complement)
Will eat the bread.
(I will eat the bread)
24 K.L. Kermanidis
D
o
w
n
l
o
a
d
e
d
 
B
y
:
 
[
H
E
A
L
-
L
i
n
k
 
C
o
n
s
o
r
t
i
u
m
]
 
A
t
:
 
1
7
:
5
0
 
3
 
M
a
r
c
h
 
2
0
0
9
(h)  ’ #o to r #
. (non-complement)
Will eat the evening.
(I will eat in the evening)
(i) MlZsme to ia #. (complement)
Talked to John.
(We talked to John)
(j) MlZsme sto Zo. (non-complement)
Talked in garden.
(We talked in the garden)
In English, a complement may be an NP (k), a PP (k), an infinitive (m, n), a gerund (o),
a past participle (p), a secondary clause (l).
(k) Kate put the pillow on the chair.
(l) He knows that he is losing.
(m) Doug likes to surf.
(n) His mother helped him walk.
(o) He started singing when he was twelve.
(p) I want him gone.
4.2. Corpora and pre-processing
The MG corpora used in the present experiments were a collection of the
ILSP/ELEFTHEROTYPIA (Hatzigeorgiu et al. 2000), ESPRIT 860 (Partners of
ESPRIT-291/860 1986) and part of DELOS (Kermanidis et al. 2002) corpora (a total of
800,000 words). These corpora are balanced in genre and annotated with morphological
information.
Further (phrase structure) information was obtained automatically by the chunker
described in detail in Stamatatos et al. (2000). Noun (NP), verb (VP), prepositional (PP),
adverbial phrases (ADP) and conjunctions (CON) are detected via multi-pass parsing.
Precision and recall reach 94.5% and 89.5%, respectively. Phrases are non-overlapping.
Concerning phrase structure, complements (except for weak personal pronouns) are not
included in the verb phrase, nominal modifiers in the genitive case are included within the
NP they modify, coordinated simple noun and ADP are grouped into one phrase.
The next step is empirical headword identification. NP headwords are determined
based on the pos and case of the phrase constituents. For VPs, the headword is the main
verb or the conjunction if they are introduced by one. For PPs, it is the preposition
introducing them.
The Penn Treebank 3 version of the WSJ corpus (Marcus et al. 1993) of about 1 million
words, annotated with complete syntactic information, was used for English. Headwords
are determined empirically for the English phrases also, and they are the last words in the
phrase.
4.3. Feature selection
For the Greek data, in order to take into account the freedom of the language
structure, context information of every verb in the corpus focuses on the two phrases
Journal of Experimental & Theoretical Artificial Intelligence 25
D
o
w
n
l
o
a
d
e
d
 
B
y
:
 
[
H
E
A
L
-
L
i
n
k
 
C
o
n
s
o
r
t
i
u
m
]
 
A
t
:
 
1
7
:
5
0
 
3
 
M
a
r
c
h
 
2
0
0
9
preceding and the three phrases following it. Only one out of 200 complements in the
corpus appears outside this window. Each of these phrases is in turn the focus phrase
(the candidate complement) and an instance of 29 features (28 features plus the class
label) is formed for every focus phrase (fp). So a maximum of five instances per verb
occurrence are formed. Forming of these instances for the following corpus sentence is
shown in Table 1. The example sentence has two verbs: e	 (to be) and eu! (to
believe). Words starting with the asterisk (*) are phrase headwords.
VP[*"i] NP1[ló *i] NP2[o * #mro&] CON[] VP[*steue] PP[*sto eó.]
VP[Is] NP1[good boy] NP2[the Labros] CON[and] VP[believes] PP[in God.](Labros is a
good boy and believes in God.)
The first five features are the verb lemma (VERB), its mode (F1), whether it is
(im)personal (F2), its copularity (F3), and its voice (F4). Two features encode the
presence of a personal pronoun in the accusative (F5) or genitive (F6) within the VP.
For every fp (fps are in bold), apart from the seven features described above, a context
window of three phrases preceding the fp and three phrases following it is taken into
account. Morphological information on each phrase (the fp and the context phrases) is
encoded into a set of three features (shown in triples in the example) resulting in a
total of 21 features. A detailed description of the dataset can be found in
Kermanidis et al. (2004).
The formatting described in the previous section was applied to the Greek corpora.
The class of each fp for every created instance was hand-labelled by two linguists by
looking up the verb in its context, based on the detailed descriptions for complements
by Klairis and Babiniotis (1999). The dataset consisted of 63,000 instances. The
imbalance ratio is 1 : 5.9 (one complement instance for every 5.9 non-complement
instances).
In order for the pre-processing to be equivalent for the two languages, the
multi-layered treebank structure of the WSJ was ‘flattened’ into a format of
non-overlapping chunks. A sentence may consist of NP, VP, PP, ADP adjectival
phrases. Apart from the morphosyntactic bracketing, sentence terms in the WSJ are
also followed by a function tag (e.g. TMP for temporal, MNR for manner, etc.). Each
sentence is again transformed into a set of instance vectors, but as the notions of
copularity and mode do not apply in English, only the voice of the verb is taken into
account as a verb feature. Also, due to the more restricted nature of the English
sentence structure, only the three phrases following the verb are considered as
candidate complements. It was calculated that only one out of 415 complements
appears outside this context window. Table 2 shows the instances for the following
corpus sentence:
PP[*For six years,] NP[T. Marshall *Hahn Jr.] VP[has *made] NP[corporate *acquisitions]
PP[*in the George Bush mode:] NP[kind and *gentle.]
The syntactic category that a term belongs to (type of phrase, pos) and its function tag
determine whether a term in a WSJ sentence is a complement or not of a given verb.
Complements are NPs with no function tag, clauses and secondary clauses with no
adverbial tag, logical subject terms, verb phrases and all the terms with beneficiary, closely
related, predicative, put (locative complements of the verb put) and nom (nominal relative
clauses and gerunds) tags. The imbalance ratio for English is 1 : 5 (one complement
instance for every five non-complement instances).
26 K.L. Kermanidis
D
o
w
n
l
o
a
d
e
d
 
B
y
:
 
[
H
E
A
L
-
L
i
n
k
 
C
o
n
s
o
r
t
i
u
m
]
 
A
t
:
 
1
7
:
5
0
 
3
 
M
a
r
c
h
 
2
0
0
9
T
a
b
le
1
.
T
h
e
fi
v
e
la
b
el
le
d
in
st
a
n
ce
s
fo
r
th
e
se
n
te
n
ce
E
i



ló



i



#m
ro
&



s
te
u e

s
to

eó
.
V
er
b
F
1
F
2
F
3
F
4
F
5
F
6
fp

3

2

1
þ
1
þ
2
þ
3
L
a
b
el
ei
m

O
P
C
P
F
F
N
P
,N
,n

,
,

,
,
V
P
,V
,

N
P
,N
,n
V
P
,V
,-
P
P
,
,s
e
C
ei
m

O
P
C
P
F
F
N
P
,N
,n

,
,
V
P
,V
,
N
P
,N
,n
V
P
,V
,
P
P
,
,s
e

,
,
N
C

s
te
u o
E
P
N
C
A
F
F
N
P
,N
,n

,
,

,
,
V
P
,V
,
N
P
,N
,n
V
P
,V
,
P
P
,
,s
e
N
C

s
te
u o
E
P
N
C
A
F
F
N
P
,N
,n

,
,
V
P
,V
,
N
P
,N
,n
V
P
,V
,
P
P
,
,s
e

,
,
N
C

s
te
u o
E
P
N
C
A
F
F
P
P
,
,s
e
N
P
,N
,n
N
P
,N
,n
V
P
,V
,

,
,

,
,

,
,
C
Journal of Experimental & Theoretical Artificial Intelligence 27
D
o
w
n
l
o
a
d
e
d
 
B
y
:
 
[
H
E
A
L
-
L
i
n
k
 
C
o
n
s
o
r
t
i
u
m
]
 
A
t
:
 
1
7
:
5
0
 
3
 
M
a
r
c
h
 
2
0
0
9
4.4. Experimental results and discussion
The C4.5 decision tree algorithm (Quinlan 1993) was chosen for classification. Decision
trees were chosen because of their high representational power, which is very significant
for understanding the impact of each feature on the classification accuracy, and because
of the knowledge that can be extracted from the resulting tree itself.
Ten fold cross-validation was performed on all the data: the initial dataset was divided
into 10 sets of equal size. Ten rounds of experiments were performed using each time,
nine sets for training and one set for testing. OSS was applied to the training set only.
For guiding the C4.5 pruning process, one of the ten subsets was used as the held-out
validation set.
Regarding the SVM parameters, after extended experimentation, a polynomial kernel
of second degree (d¼ 2) was chosen, a complexity factor of value c¼ 0.1, and the SVMs
were trained using the Sequential Minimal Optimization (SMO) algorithm (Platt 1998).
Classification performance was evaluated using precision and recall metrics for every
class. a and d are the correctly identified non-complements and complements respectively,
b are the non-complements which have been misclassified as complements and c are the
misclassified complements.
prNC ¼
a
aþ c reNC ¼
a
aþ b prC ¼
d
bþ d reC ¼
d
cþ d
The f-measure for each class combines the previous two metrics into one:
f-measure ¼ 2 precision recall
precision recall
Table 3 shows the results for MG and English using the initial dataset before any attempt
is made to reduce its size (left column), after applying OSS to the training set (middle
column) and finally using SVMs (right column).
Regarding the results for the initial dataset, the difference between the performance
scores of the two classes is obvious. After applying OSS, the improvement in the
performance when using the reduced dataset is very interesting to observe and
exceeds 14%.
Apart from the positive impact of OSS on predicting positive examples,
the tables show its positive (or at least non-negative) impact on predicting negative
instances. Non-complement accuracy either increases or remains the same after balancing.
Regarding the results with SVMs, by not taking into account examples that are not
close to the boundary region, SVMs perform better than the traditional-learning
algorithms, showing the significance of not taking into account all the superfluous
Table 2. The three labelled instances for the sentence For six years, T. Marshall Hahn Jr has made
corporate acquisitions in the George Bush mode: kind and gentle.
Verb Voice fp 3 2 1 þ1 þ2 þ3 Label
make A NP,N,– PP,TMP,for NP,N,– VP,P,– PP,MNR,in NP,J,– –,–,– C
make A PP,MNR,in NP,N,– VP,P,– NP,N,– NP,J,– –,–,– –,–,– NC
make A NP,J,– VP,P,– NP,N,– PP,MNR,in –,–,– –,–,– –,–,– NC
28 K.L. Kermanidis
D
o
w
n
l
o
a
d
e
d
 
B
y
:
 
[
H
E
A
L
-
L
i
n
k
 
C
o
n
s
o
r
t
i
u
m
]
 
A
t
:
 
1
7
:
5
0
 
3
 
M
a
r
c
h
 
2
0
0
9
negative examples for classification. On the other hand, OSS leads to better results than
SVMs. In other words, several borderline examples can be misleading and therefore their
removal leads to better performance. The better performance of OSS shows also the
importance of the safe negative examples, examples that are necessary for the creation of
an efficient decision function, and which are disregarded by SVMs.
5. Test case 2: identifying economic entities
The tagging of semantic entities in written text is an important subtask for information
retrieval and data mining and refers to the task of identifying the entities within free text
and assigning them to the appropriate semantic category.
One major subclass of semantic entities are named entities (such as names of persons,
organisations, locations, etc). Automatic named entity recognition (NER) has been
attracting the interest of numerous researchers during the last years. Hendrickx and Van
den Bosch (2003) employ manually tagged and chunked English and German datasets, and
use memory-based learning to learn new named entities that belong to four categories.
They perform iterative deepening to optimise their algorithmic parameter and feature
selection, and extend the learning strategy by adding seed list (gazetteer) information,
by performing stacking and by making use of unannotated data. They report an average
f-score on all four categories of 78.20% on the English test set. Anther approach that
makes use of external gazetteers is described in Ciaramita and Altun (2005), where a
Hidden Markov Model and Semi-Markov Model is applied to the CoNNL 2003 dataset.
The authors report a mean f-score of 90%. Multiple stacking is also employed in
Tsukamoto et al. (2002) on Spanish and Dutch data, and the authors report 71.49 and
60.93% mean f-score, respectively. Sporleder et al. (2006) focus on the natural history
domain. They employ a Dutch zoological database to learn three different named-entity
classes, and use the contents of specific fields of the database to bootstrap the named entity
tagger. In order to learn new entities, they too train a memory-based learner. Their
reported average f-measure reaches 68.65% for all three entity classes. Other approaches
(Radu et al. 2003; Wu et al. 2006) utilise combinations of classifiers in order to tag
new-named entities by ensemble learning.
This second test case describes the automatic recognition of semantic entities related to
the economic domain in MG free text. Unlike previous approaches to NER, the semantic
entities in the present work are not limited to named entities only, such as names of
organisations, persons and locations. First, they also cover names of stocks and bonds,
Table 3. Results for both languages using the initial dataset, the reduced dataset with OSS and
SVMs.
MG English
Initial OSS SVM Initial OSS SVM
prNC 91.5 93.0 90.1 89.9 96.1 90.0
reNC 95.2 94.8 90.3 93.4 96.3 90.2
prC 68.9 73.2 59.5 60.0 70.4 64.6
reC 54.7 68.9 58.9 49.6 63.7 59.8
Journal of Experimental & Theoretical Artificial Intelligence 29
D
o
w
n
l
o
a
d
e
d
 
B
y
:
 
[
H
E
A
L
-
L
i
n
k
 
C
o
n
s
o
r
t
i
u
m
]
 
A
t
:
 
1
7
:
5
0
 
3
 
M
a
r
c
h
 
2
0
0
9
as well as names of newspapers (due to the newswire genre of the corpus). Furthermore,
there are other semantic types that are important for economic information retrieval, like
quantitative units (e.g. denoting stock and fund quantities, monetary amounts, stock
values), percentages, etc. Temporal words and expressions are also identified due to their
importance for data mining tasks.
This information appears in free text in either one-word or multi-word expressions.
The present work views semantic entity recognition as a two-task experiment: the first task
is to detect the boundaries (the beginning and the end) of these expressions. The second is
to assign a semantic label to each of them.
The text data used in the entity recognition experiments were formed by a part of the
DELOS corpus. DELOS is a collection of newspaper and journal articles. The documents
are of varying genres, like press reportage, news, articles, interviews and scientific studies,
and cover all the basic areas of the economic domain, i.e. microeconomics, macro-
economics, international economics, finance, business administration, economic history,
economic law, public economics, etc. Therefore, it presents richness in vocabulary, in
linguistic structure, in the use of idiomatic expressions and colloquialisms, which is not
encountered in the highly domain- and language-restricted texts used normally for NER
(e.g. medical records, technical articles, tourist site descriptions). The corpus is
automatically tagged with pos, basic token type information (whether a token is a
number, a symbol, an abbreviation, an acronym, etc.) and elementary morphological
information (case, number and gender) (Sgarbas et al. 2000).
For this particular test case, learning is performed in two stages: the learner is first
trained on the training data and used to classify new, unseen instances. In the second stage,
the classification predictions of the first stage are added to the instance vector as extra
features to force the classifier to learn from its mistakes.
5.1. Modern Greek
The grammatical case of nouns, adjectives or articles affects semantic labelling. For
example, the genitive case may denote possession, quantity, quality, origin, division, etc.,
as is shown in the following examples:
G tmZZle sto osó to 12.33 E.
The[NOM] price[NOM] reached the[ACC] value[ACC] the[GEN] 12.33 E.
The price reached the value of 12.33 E.
G Tr #e tZ& Ell #o&
The[NOM] Bank[NOM] the[GEN] Greece[GEN]
The Bank of Greece
As can be deduced from these examples, another important property is the agreement of
morphological features (case, person, gender and number values) between consecutive
words. The borders of the agreement define the borders of basic nominal chunks.
Context information is often decisive when trying to detect a semantic entity. In the
following example, the verb "o	 (to reach) is a strong indicator that the entity next
to it is an amount/value, because this verb is typically used in MG to express ‘reaching
a value’.
O meto"& Zl st& 500.
The stocks reached the 500.
The number of stocks reached 500.
30 K.L. Kermanidis
D
o
w
n
l
o
a
d
e
d
 
B
y
:
 
[
H
E
A
L
-
L
i
n
k
 
C
o
n
s
o
r
t
i
u
m
]
 
A
t
:
 
1
7
:
5
0
 
3
 
M
a
r
c
h
 
2
0
0
9
5.2. Feature set
Each token in the corpus constitutes a candidate semantic entity. Each candidate entity is
represented by a feature-value vector. The features forming the vector are:
(1) The token lemma. In the case where automatic lemmatisation was not able to
produce the token lemma, the token itself is the value of this feature.
(2) The pos category of the token.
(3) The morphological tag of the token. The morphological tag is a string of three
characters encoding the case, number and gender of the token, if it is nominal
(noun, adjective or article).
(4) The case tag of the token. The case tag is one of three characters denoting the
token case.
(5) Capitalisation. A Boolean feature encoding whether the first letter of the token is
capitalised or not.
For each candidate entity, context information was included in the feature-value vector, by
taking into account the two tokens preceding and the two tokens following it. Each of
these tokens was represented in the vector by the five features described above. As a result,
a total of 25 (55) features are used to form the instance vectors.
The class label assigns a semantic tag to each candidate token. These tags represent the
entity boundaries (whether the candidate token is the start, the end or inside an entity) as
well as the semantic identity of the token. A total of 40,000 tokens were manually tagged
with their class value. Table 4 shows the various values of the class feature, as well as their
frequency among the total number of tokens.
Unlike most previous approaches that focus on labelling three or four semantic
categories of named entities, the present work deals with a total of 30 class values plus the
non-entity (NULL) value, as can be seen in Table 3.
Another important piece of information disclosed by Table 3 is the imbalance between
the populations of the positive instances (entities) in the dataset, that form only 15% of the
total number of instances and the negative instances (non-entities).
5.3. Experimental setup and results
Instance-based learning, and the 1-NN algorithm in particular, were selected to classify the
candidate semantic entities. 1-NN was chosen because, due to storing all examples in
memory, it is able to deal competently with exceptions and low-frequency events, which
are important in language learning tasks (Daelemans et al. 1999), and are ignored by other
learning algorithms.
After experimenting with several parameters for the SVM classifier, a first-degree
polynomial kernel function was chosen and a complexity factor of value c¼ 0.1.
As mentioned earlier, learning is performed in two stages. In the second stage, the
predictions of the first stage are added to the set of features described in the previous
section. The total number of features in the second stage is 30. This learning scheme proves
very beneficial for this particular test case due to the sequential nature of the class labels in
the data. A report of extensive experiments regarding the importance of learning in two
stages for the present task as well as the impact of the candidate entities’ context window
size on classification can be found in Kermanidis (2007). The mentioned work, however,
deals with other classification schemata.
Journal of Experimental & Theoretical Artificial Intelligence 31
D
o
w
n
l
o
a
d
e
d
 
B
y
:
 
[
H
E
A
L
-
L
i
n
k
 
C
o
n
s
o
r
t
i
u
m
]
 
A
t
:
 
1
7
:
5
0
 
3
 
M
a
r
c
h
 
2
0
0
9
Testing of the algorithm was performed using tenfold cross-validation, following the
procedure described in Section 4.4. Table 5 shows f-score results for every class label with
the initial dataset (left column), after applying OSS (middle column) and with SVMs
(right column). The few cases where a drop in f-score is noted after applying OSS are
indicated in bold.
5.4. Discussion
As can be seen in Table 5, classification for certain types reaches a poor score. Looking
more closely at Table 4, this can be attributed to the sparseness that characterises these
types (multi-word person names, multi-word stock/bond names, multi-word locations).
An interesting exception to this rule is newspaper/journal names, that reach very high
scores, despite their low frequency, because they are normally introduced by a small set of
specific words like ‘eZmeri’ (newspaper) or ‘eroó’ (journal).
Table 5 also shows the high f-score achieved for the negative (NULL) class compared
to that of the positive classes, due to its high overrepresentation in the dataset.
Table 4. Values and distribution of the class label.
Tag Description Percentage (%)
AE Start of company/organisation/bank name 1.4
ME Middle of company/organisation/bank name 0.74
TE End of company/organisation/bank name 1.4
E Company/organisation/bank one-word name 1.1
AP Start of monetary amount/price/value 0.88
MP Middle of monetary amount/price/value 0.63
TP End of monetary amount/price/value 0.88
AAM Start of number of stocks/bonds 0.3
MAM Middle of number of stocks/bonds 0.42
TAM End of number of stocks/bonds 0.3
AT Start of percentage value 0.73
MT Middle of percentage value 0.08
TT End of percentage value 0.73
AX Start of temporal expression 1
MX Middle of temporal expression 0.75
TX End of temporal expression 1
X One-word temporal expression 0.55
AO Start of stock/bond name 0.16
MO Middle of stock/bond name 0.17
TO End of stock/bond name 0.16
ON One-word stock/bond name 0.05
AL Start of location name 0.21
ML Middle of location name 0.48
TL End of location name 0.21
L One-word location name 0.33
F One-word newspaper/journal name 0.14
AN Start of person name 0.18
MN Middle of person name 0.02
TN End of person name 0.18
N One-word person name 0.06
32 K.L. Kermanidis
D
o
w
n
l
o
a
d
e
d
 
B
y
:
 
[
H
E
A
L
-
L
i
n
k
 
C
o
n
s
o
r
t
i
u
m
]
 
A
t
:
 
1
7
:
5
0
 
3
 
M
a
r
c
h
 
2
0
0
9
One-sided sampling proved highly beneficial for the majority of the entity types.
It forces the learner to pay more attention to the minority classes. Combining it
with instance-based learning, which seems to cope well with the large number of classes
and the sparse data, the best results are achieved. The large number of classes, as well as
the wide range of nominal values appearing in the dataset because of the features that
represent the token lemmata, do not permit SVMs as well as OSS to perform in this test
case also.
Looking at Table 4, one-word stock/bond names (ON) occur extremely rarely in the
corpus. Person names consisting of more than two words (MN) are even more rare.
No classification scheme has been able to detect these classes due to the sparseness.
Given the nature and complexity of the corpus, the low level of pre-processing
(compared to previous approaches that use phrase-chunked input), and the large number
of class labels, the results of Table 5 are very promising when compared to the ones
reported in the literature.
Table 5. Comparative classification results (f-score) for all learning schemata and class labels.
Class Initial (1-NN) OSS (1-NN) SVMs
NULL 0.976 0.922 0.956
AAM 0.865 0.868 0.889
AE 0.808 0.839 0.832
AL 0.542 0.545 0.462
AN 0.541 0.556 0.55
AO 0.529 0.649 0.583
AP 0.897 0.939 0.843
AT 0.928 0.97 0.976
AX 0.716 0.79 0.709
E 0.645 0.756 0.652
F 1 0.923 0.8
L 0.557 0.588 0.574
MAM 0.913 0.876 0.867
ME 0.833 0.794 0.667
ML 0.667 0.706 0.688
MN 0 0 0
MO 0.619 0.571 0.714
MP 0.926 0.957 0.967
MT 0.952 0.87 0.967
MX 0.743 0.775 0.716
N 0.533 0.533 0.727
ON 0 0 0
TAM 0.838 0.845 0.844
TE 0.83 0.882 0.895
TL 0.489 0.512 0.496
TN 0.718 0.667 0.688
TO 0.474 0.541 0.532
TP 0.872 0.893 0.914
TT 0.926 0.944 0.88
TX 0.686 0.767 0.695
X 0.419 0.6 0.54
Mean f-score 0.692 0.712 0.698
Journal of Experimental & Theoretical Artificial Intelligence 33
D
o
w
n
l
o
a
d
e
d
 
B
y
:
 
[
H
E
A
L
-
L
i
n
k
 
C
o
n
s
o
r
t
i
u
m
]
 
A
t
:
 
1
7
:
5
0
 
3
 
M
a
r
c
h
 
2
0
0
9
6. Test case 3: identifying taxonomic relations
A domain ontology is the tool that enables information retrieval, data mining and
intelligent search. Ontologies consist of concepts that are important for communicating
domain knowledge. These concepts are structured hierarchically through taxonomic
relations. A taxonomy usually includes hyperonymy–hyponymy (is-a) and meronymy
(part-of) relations.
A complete ontology, however, may also include further information regarding each
concept. The economic domain, especially, is governed by more ‘abstract’ relations that
capture concept attributes (e.g. rise and drop are two attributes of the concept value; a
stockholder is an attribute of the concept company). Henceforth, this type of relation will
be referred to as attribute relation.
The present approach towards taxonomy learning proposes a methodology for
automatically detecting taxonomic relations between the terms that have been extracted
from MG collections of economic texts. Unlike most previous work that focuses basically
on hyponymy, in this work, meronymy as well as attribute relations are also detected.
A term pair is governed by an attribute relation if it does not match the typical profile of
an is-a or a part-of relation. All the aforementioned types of relations are henceforth called
taxonomic.
One of the main ideas that this work has been based on is the ability of the proposed
methodology to be easily applied to other languages and other domains. For this reason,
no use of external resources (e.g. semantic networks, grammars or pre-existing ontologies)
is made.
6.1. Related work
Previous approaches to taxonomy learning have varied from supervised to unsupervised
clustering techniques, and from methodologies that make use of external taxonomic
thesauri, to those that rely on no external resources.
Regarding previous approaches that employ clustering techniques,
Cimiano et al. (2004) describe a conceptual clustering method that is based on the
Formal Concept Analysis for automatic taxonomy construction from text and
compares it to similarity-based clustering (agglomerative and Bi-Section-KMeans
clustering). The automatically generated ontology is compared against a hand-crafted
gold standard ontology for the tourism domain and reports a maximum lexical recall
of 44.6%.
Other clustering approaches are described in Faure and Nedellec (1998) and Pereira
et al. (1993). The former use a syntactically parsed text (verb-subcategorisation examples)
and utilise iterative clustering to form new concept graphs. The latter also make use
of verb-object dependencies, and relative frequencies and relative entropy as similarity
metrics for clustering.
Pekar and Staab (2002) take advantage of a taxonomic thesaurus (a tourism-domain
ontology) to improve the accuracy of classifying new words into its classes. Their
classification algorithm is an extension of the kNN method, which takes into account the
taxonomic similarity between nearest neighbours. They report a maximum overall
accuracy of 43.2%.
Lendvai (2005) identifies taxonomic relations between two sections of a medical
document using memory-based learning. Binary vectors represent overlap between the two
34 K.L. Kermanidis
D
o
w
n
l
o
a
d
e
d
 
B
y
:
 
[
H
E
A
L
-
L
i
n
k
 
C
o
n
s
o
r
t
i
u
m
]
 
A
t
:
 
1
7
:
5
0
 
3
 
M
a
r
c
h
 
2
0
0
9
sections, and the tests are run on parts of two Dutch medical encyclopedias. A best overall
accuracy value of 88% is reported.
Witschel (2005) proposes a methodology for extending lexical taxonomies by first
identifying domain-specific concepts, then calculating semantic similarities between
concepts, and finally using decision trees to insert new concepts to the right position in
the taxonomy tree. The classifier is evaluated against two subtrees from GermaNet.
Navigli and Velardi (2004) interpret semantically the set of complex terms that they
extract, based on simple string inclusion. They make use of a variety of external resources
in order to generate a semantic graph of senses.
Another approach that makes use of external hierarchically structured textual
resources is Makagonov et al. (2005). The authors map an already existing hierarchical
structure of technical documents to the structure of a domain-specific technical ontology.
Words are clustered into concepts, and concepts into topics. They evaluate their ontology
against the structure of existing textbooks in the given domain.
Maedche and Volz (2001) make use of clustering, as well as pattern-based (regular
expressions) approaches in order to extract taxonomies from domain-specific German
texts.
Degeratu and Hatzivassiloglou (2004) also make use of syntactic patterns to extract
hierarchical relations, and measure the dissimilarity between the attributes of the terms
using the Lance and Williams coefficient. They evaluate their methodology on a collection
of forms provided by the state agencies and report a precision value of 73 and 85% for is-a
and attributive relations, respectively.
6.2. Term extraction
Corpora comparison was employed for the extraction of economic terms (Thanopoulos
et al. 2006). Corpora comparison detects the difference in statistical behaviour that a term
presents in a balanced and in a domain-specific corpus. DELOS is the domain-specific
corpus used for the experiments in this application, while the ESPRIT 860 and ILSP/
ELEFTHEROTYPIA collections comprised the balanced corpus.
Noun and prepositional phrases of the two corpora are selected to constitute candidate
terms, as only these phrase types are likely to contain terms. Coordination schemes are
detected within the phrases, and the latter are split into smaller phrases, respectively. The
occurrences of words and multi-word units (n-grams), pure as well as nested, are counted.
Longer candidate terms are split into smaller units (tri-grams into bi-grams and uni-grams,
bi-grams into uni-grams).
Due to the relative freedom in the word ordering in MG sentences, bi-gram A B (A and
B being the two lemmata forming the bi-gram) is considered to be identical to bi-gram B
A, if the bi-gram is not a named entity. Their joint count in the corpora is calculated and
taken into account. The resulting uni-grams and bi-grams are the candidate terms. The
candidate term counts in the corpora are then used in the statistical filters.
The domain-specific corpus is quite large compared to the balanced corpus. As a
result, several terms that appear in it do not appear in the balanced corpus, making it
impossible for them to be detected by corpora comparison. In order to overcome this
problem, Lidstone’s law (Manning and Schuetze 1999) was applied to the candidate
terms, i.e. each candidate term count was augmented by a value of l¼ 0.5 in both
corpora.
Journal of Experimental & Theoretical Artificial Intelligence 35
D
o
w
n
l
o
a
d
e
d
 
B
y
:
 
[
H
E
A
L
-
L
i
n
k
 
C
o
n
s
o
r
t
i
u
m
]
 
A
t
:
 
1
7
:
5
0
 
3
 
M
a
r
c
h
 
2
0
0
9
Filtering was then performed in two stages: first, the relative frequencies
are calculated for each candidate term. Then, for those candidate terms that
present a relative frequency value 41, the Log Likelihood ratio (LLR) is calculated.
The LLR metric detects how surprising (or not) it is for a candidate term to appear
in the domain-specific or in the balanced corpus (compared to its expected
appearance count), and therefore constitute an economic domain term (or not).
The term extraction methodology reaches a precision of 82% for the 200 N-best
candidate terms.
6.3. Learning taxonomic relations
The sense of a term is strongly linked to the context the term appears in. To this end, for
each extracted term semantic context vectors have been constructed that are comprised
of the 10 most frequent words the term co-occurs with in the domain-specific corpus.
A context window of two words preceding and two words following the term for every
occurrence of the term in the corpus is formed. All non-content words (prepositions,
articles, pronouns, particles, conjunctions) are disregarded, while acronyms, abbreviations
and certain symbols (e.g.%, E) are taken into account because of their importance for
determining the semantic profile of the term in the given domain. Bi-grams (pairs of the
term with each word within the context window) are generated and their frequency is
recorded. The 10 words that present the highest bi-gram frequency scores are chosen to
form the context vector of the term.
For each pair of terms, their semantic similarity is calculated, based on their
semantic context vectors. The smaller the distance between the context vectors, the
more similar the terms’ semantics. The value of semantic similarity is an integer with a
value ranging from 0 to 10, which denotes the number of common words two context
vectors share.
Another important semantic feature that is taken into account is how ‘diverse’ the
semantic properties of a term are, i.e. the number of other terms that a term shares
semantic properties with. This property is important when creating taxonomic hierarchies,
because the more ‘shared’ the semantic behaviour of a term is, the more likely it is for the
term to have a higher place in the hierarchy. We include the notion of ‘semantic diversity’
in our feature set by calculating the percentage of the total number of terms whose
semantic similarity with the focus term (one of the two terms whose taxonomic relation is
to be determined) is at least one.
Syntactic information regarding the linguistic patterns that govern the
co-occurrence of two terms is significant for extracting taxonomic information. For
languages with a relatively strict sentence structure, like English, such patterns are
easier to detect (Hearst 1992), and their impact on taxonomy learning is more
straightforward. As mentioned earlier, MG presents a larger degree of freedom in the
ordering of the constituents of a sentence, due to its rich morphology and its complex
declination system. This freedom makes it difficult to detect syntactic patterns, and,
even if they are detected, their contribution to our task is not that easily observable.
However, two MG syntactic schemata prove very useful for learning taxonomies. They
are the attributive modification schema and the genitive modification schema. The first,
known in many languages, is the pattern where (usually) an adjective modifies the
following noun. The second is typical for MG, and it is formed by two nominal
36 K.L. Kermanidis
D
o
w
n
l
o
a
d
e
d
 
B
y
:
 
[
H
E
A
L
-
L
i
n
k
 
C
o
n
s
o
r
t
i
u
m
]
 
A
t
:
 
1
7
:
5
0
 
3
 
M
a
r
c
h
 
2
0
0
9
expressions, one of which (usually following the other) appears in the genitive case and
modifies the preceding nominal, denoting possession, property, origin, quantity, quality.
The following phrases show examples of the first (example 1) and the second (examples
2, 3 and 4) schemata, respectively.
(1) to metoó[ADJ] e’ #lo[NOUN]
the stock capital
(2) Z t #esZ[NOUN] etgZ&[NOUN-GEN]
the deposit cheque
(the deposit of the cheque)
(3) róero&[NOUN] to
 s
mo
lio
[NOUN-GEN]
head the council
(head of the council)
(4) uZsZ[NOUN] to
 e’lio
[NOUN-GEN]
increase the capital
(capital increase)
Both these schemata enclose the notion of taxonomic relations: hyponymy relations (a
cheque deposit is a type of deposit, a stock capital is a type of capital), as well as
meronymy relations (the head is part of a council). The fourth example incorporates an
attribute relation. The distinction among the types of relations is not always clear. In the
cheque deposit example, the deposit may also be considered an attribute of cheque,
constituting thereby an attribute relation. For each pair of terms, we calculated the
number of times they occur in one of the two schemata in the domain-specific corpus.
6.4. Experimental setup
After the process described in Section 6.2, the 250 most highly ranked terms (according to
the LLR metric) were selected, and each one was paired with the rest. The syntactic and
semantic information described in the previous section has been encoded in a set of
attributes that form a feature-value vector for each pair of terms. In more detail, the
features are:
. the lemmata of the two terms
. the semantic similarity between the two terms
. the semantic diversity metrics of the two terms
. the frequency counts of the two terms
. the pos tags of the two terms
. eight features that show the number of times the two terms co-occur in the
attributive modification schema and in the genitive modification schema for four
relative positions of the two terms.
The four relative positions are (the underscore denotes a third intervening word):
. term1 term2
. term2 term1
. term1_term2
. term2_term1
Journal of Experimental & Theoretical Artificial Intelligence 37
D
o
w
n
l
o
a
d
e
d
 
B
y
:
 
[
H
E
A
L
-
L
i
n
k
 
C
o
n
s
o
r
t
i
u
m
]
 
A
t
:
 
1
7
:
5
0
 
3
 
M
a
r
c
h
 
2
0
0
9
The proposed features are empirical. The semantic similarity metric is inspired by
the Dice coefficient (Manning and Schuetze 1999). A detailed report on the
importance of each individual feature for the task may be found in Kermanidis
and Fakotakis (2007).
The semantic relations of a total of 6000 term pairs were manually annotated by
two economy and finance experts with one of the four class label values: is-a, part-of,
attribute relation and NULL. NULL indicates no direct taxonomic/semantic relation
between the two terms. In order to ensure inter-annotator agreement, experts were
given detailed and specific guidelines, and they were frequently in contact to resolve
cases of ambiguity.
However, 73% of the term pairs turned out to belong to the NULL (majority) class.
This imbalance leads to a drop in accuracy when trying to predict the class of instances
that belong to one of the remaining classes: where 9% of the term pairs belong to the is-a
class, 17% belong to the attribute class and only 1% belongs to the part-of class. The is-a
and part-of classes are significantly underrepresented and constitute the difficult cases.
The 6000 examples were split into a training (80%) and a test set (20%). OSS was then
applied to the training set only. Approximately 9% of the negative examples were removed
(37.5% of which were noisy or borderline and the remaining 62.5% were redundant).
6.5. Classification algorithms
The basic 1-NN instance-based learner is chosen for referring to a baseline classification
performance. The C4.5 decision tree classification algorithm (Quinlan 1993) was also
experimented with. When no pruning on the decision tree is performed, overlooking tree
paths that might be important for classification is avoided. This explains the significant
increase in f-score that is noted after running comparative experiments with and without
pruning. Therefore, the results reported in the next section are derived from the unpruned
version.
Regarding the SVM classification scheme, after several experiments for parameter
selection, a first-degree polynomial kernel function was selected and a complexity factor
of c¼ 0.05. The SMO algorithm was chosen to train the support vector classifier.
6.6. Results and discussion
Table 6 shows the classification results (f-score) for all the taxonomic relation types and all
the algorithms when they are applied to the initial, as well as the reduced, dataset. As can
be seen, prediction of the NULL class achieves the highest scores, due to its over-
representation in the dataset.
One-sided sampling leads again to better classification results than SVMs. This implies
again that the inclusion of safe negative examples, as well as the removal of borderline
negative instances, helps predict the class for new instances. The lack of clarity in the
boundary region, i.e. the large number of misleading, borderline instances, makes it
difficult for classification to rely satisfactorily only on support vectors. This large number
arises inevitably due to the lack of more sophisticated resources (external ontologies,
semantic networks, etc.) that would most likely have led to a more appropriate feature set.
However, as mentioned earlier, more sophisticated resources are not always available for
several languages.
38 K.L. Kermanidis
D
o
w
n
l
o
a
d
e
d
 
B
y
:
 
[
H
E
A
L
-
L
i
n
k
 
C
o
n
s
o
r
t
i
u
m
]
 
A
t
:
 
1
7
:
5
0
 
3
 
M
a
r
c
h
 
2
0
0
9
The inability to detect part-of relations is attributed mainly to their extremely rare
occurrence in the data. The economic domain is more ‘abstract’ and is governed to a large
extent by other relation types (amounts, buy/sell transactions, business relations, etc.).
Attempting to detect a more fine-grained set of relations, that are suitable to the specific
domain, is one of the author’s priority future research directions.
7. Conclusion
In this article, the positive effect of OSS and SVMs is described on three distinct
imbalanced linguistic datasets, derived from Greek and English text corpora. Comparative
experiments show that SVMs lead to worse performance, showing thereby that borderline
negative examples alone can be misleading, while safe examples (not taken into account by
SVMs) are important for correct classification of minority class instances. This is
especially the case when the numbers of borderline instances are large, and the decision
boundary region is unclear, a phenomenon inevitable when the available resources are
limited (as is the case for many languages) and text pre-processing is to a large extent
automatic in nature. The removal of some of the borderline instances by OSS helps clarify
the decision boundary region, and takes into account more instances when creating a
decision function, leading to better classification performance.
References
Aldezabal, I., Aranzabe, M., Atutxa, A., Gojenola, K., and Sarasola, K. (2002), ‘Learning
Argument/Adjunct Distinction for BASQUE,’ in SIGLEX Workshop of the ACL,
Philadelphia, pp. 42–50.
Briscoe, T., and Carroll, J. (1997), ‘Automatic Extraction of Subcategorization From Corpora,’
in Proceedings of ANLP 1997, Washington DC, pp. 356–363.
Buchholz, S. (1998), ‘Distinguishing Complements from Adjuncts Using Memory-based Learning,’
in Proceedings of the Workshop on Automated Acquisition of Syntax and Parsing, ESSLLI-98,
Saarbruecken, Germany, pp. 41–48.
Chawla, N., Bowyer, K., Hall, L., and Kegelmeyer, W.P. (2002), ‘SMOTE: Synthetic Minority Over-
sampling Technique,’ Journal of Artificial Intelligence Research, 16, 321–357.
Ciaramita, M., and Altun, Y. (2005) ‘Named Entity Recognition in Novel Domains with External
Lexical Knowledge,’ in Workshop on Advances in Structured Learning for Text and Speech
Processing (NIPS).
Cimiano, P., Hotho, A., and Staab, S. (2004), ‘Comparing Conceptual, Divisive and Agglomerative
Clustering for Learning Taxonomies from Text,’ in Proceedings of the European Conference on
Artificial Intelligence (ECAI) Valencia, Spain.
Table 6. f-scores for all relation types and all classifiers using the initial and the reduced dataset.
1-NN C4.5 OSS (1-NN) OSS (C4.5) SVM
Is-a 0.694 0.704 0.805 0.776 0.728
Part-of 0 0 0 0 0
Attribute 0.765 0.69 0.805 0.71 0.788
NULL 0.904 0.919 0.931 0.913 0.907
Journal of Experimental & Theoretical Artificial Intelligence 39
D
o
w
n
l
o
a
d
e
d
 
B
y
:
 
[
H
E
A
L
-
L
i
n
k
 
C
o
n
s
o
r
t
i
u
m
]
 
A
t
:
 
1
7
:
5
0
 
3
 
M
a
r
c
h
 
2
0
0
9
Cristianini, N., Shawe-Taylor, J., and Campbell, C. (1998), ‘Dynamically Adapting Kernels in
Support Vector Machines,’ in Advances in Neural Information Processing Systems (vol. 11) eds.
M.S. Kearns, S.A. Solla, and D.A. Cohn, Cambridge, MA: MIT Press.
Daelemans, W., van den Bosch, A., and Zavrel, J. (1999), ‘Forgetting Exceptions is Harmful in
Language Learning,’ Machine Learning, 34, 11–41.
Degeratu, M., and Hatzivassiloglou, V. (2004), ‘An Automatic Model for Constructing Domain-
Specific Ontology Resources,’ in Proceedings of the International Conference on Language
Resources and Evaluation (LREC), Lisbon, Portugal, pp. 2001–2004.
Domingos, P. (1999), ‘Metacost: A General Method for Making Classifiers Cost-sensitive,’ in
Proceedings of the International Conference on Knowledge Discovery and Data Mining,
San Diego, CA, pp. 155–164.
Faure, D., and Nedellec, C. (1998), ‘A Corpus-based Conceptual Clustering Method for Verb
Frames and Ontology,’ in Proceedings of the LRECWorkshop on Adapting Lexical and Corpus
Resources to Sublanguages and Applications, Granada, Spain.
Hatzigeorgiu, N., Gavrilidou, M., Piperidis, S., Carayannis, G., Papakostopoulou, A.,
Spiliotopoulou, A., Vacalopoulou, A., Labropoulou, P., Mantzari, E., Papageorgiou, H.,
and Demiros, I. (2000), ‘Design and Implementation of the Online ILSP Greek Corpus,’
in Proceedings of the 2nd International Conference on Language Resources and Evaluation
(LREC), Athens, Greece, pp 1737–1742.
Hearst, M.A. (1992), ‘Automatic Acquisition of Hyponyms from Large Text Corpora,’ in
Proceedings of the International Conference on Computational Linguistics (COLING),
Nantes, France, pp. 539–545.
Hendrickx, I., and van den Bosch, A. (2003), ‘Memory-based One-step Named-entity Recognition:
Effects of Seed List Features, Classifier Stacking and Unannotated Data,’ in Proceedings
of the 7th Conference on Computational Natural Language Learning (CoNNL). Edmonton,
Canada.
Japkowicz, N. (2000), ‘The Class Imbalance Problem: Significance and Strategies,’ in Proceedings of
the International Conference on Artificial Intelligence, Las Vegas, Nevada.
Kermanidis, K., Fakotakis, N., and Kokkinakis, G. (2002), ‘DELOS: An Automatically Tagged
Economic Corpus for Modern Greek,’ in Proceedings of the 3rd International
Conference on Language Resources and Evaluation (LREC 2002), Las Palmas de Gran
Canaria, Spain, pp. 93–100.
Kermanidis, K., Maragoudakis, M., Fakotakis, N., and Kokkinakis, G. (2004), ‘Learning
Greek Verb Complements: Addressing the Class Imbalance,’ in Proceedings of
the International Conference on Computational Linguistics (COLING), Geneva, Switzerland,
pp. 1065–1071.
Kermanidis, K.L. (2007), ‘Identifying Boundaries and Semantic Labels of Economic Entities Using
Stacking and Re-sampling,’ in Proceedings of the 4th International Workshop on Natural
Language Processing and Cognitive Science (NLPCS), Funchal, Portugal.
Kermanidis, K., and Fakotakis, N. (2007), ‘One-sided Sampling for Learning Taxonomic Relations
in the Modern Greek Economic Domain,’ in Proceedings of the 19th IEEE International
Conference on Tools with Artificial Intelligence (ICTAI), Patras, Greece, pp. 354–361.
Klairis, C., and Babiniotis, G. (1999), Grammar of Modern Greek. II. The Verb (in Greek), Athens:
Greek Letters Publications.
Korhonen, A., Gorrell, G., and McCarthy, D. (2000), ‘Statistical Filtering and Subcategorization
Frame Acquisition,’ in Proceedings of the Joint SIGDAT EMNLP Conference, Hong Kong,
pp. 199–205.
Kubat, M., and Matwin, S. (1997) ‘Addressing the Curse of Imbalanced Training Sets,’
in Proceedings of the International Conference on Machine Learning, pp. 179–186.
Laurikkala, J. (2001), ‘Improving Identification of Difficult Small Classes by Balancing Class
Distribution,’ in Proceedings of the Conference on Artificial Intelligence in Medicine in Europe,
Portugal, pp. 63–66.
40 K.L. Kermanidis
D
o
w
n
l
o
a
d
e
d
 
B
y
:
 
[
H
E
A
L
-
L
i
n
k
 
C
o
n
s
o
r
t
i
u
m
]
 
A
t
:
 
1
7
:
5
0
 
3
 
M
a
r
c
h
 
2
0
0
9
Lendvai, P. (2005), ‘Conceptual Taxonomy Identification in Medical Documents,’ in Proceedings of
the 2nd International Workshop on Knowledge Discovery and Ontologies (KDO), Porto,
Portugal, pp. 31–38.
Lewis, D., and Gale, W. (1994), ‘Training Text Classifiers by Uncertainty Sampling,’ in Proceedings
of the International ACM SIGIR Conference on Research and Development in Information
Retrieval, Dublin, pp. 3–12.
Maedche, A., and Volz, R. (2001), ‘The Ontology Extraction and Maintenance Framework Text-To-
Onto,’ in Proceedings of the Workshop on Integrating Data Mining and Knowledge Mining,
San Jose, CA.
Makagonov, P., Figueroa, A.R., Sboychakov, K, and Gelbukh, A. (2005), ‘Learning a Domain
Ontology from Hierarchically Structured Texts,’ in Proceedings of the 22nd International
Conference on Machine Learning (ICML), Bonn, Germany.
Manning, C., and Schuetze, H. (1999), Foundations of Statistical Natural Language Processing,
Cambridge, MA: MIT Press.
Marcus, M., Santorini, B., and Marcinkiewicz, M.A. (1993), ‘Building a Large Annotated Corpus of
ENGLISH: The Penn Treebank,’ Computational Linguistics, 19, 313–330.
Merlo, P., and Leybold, M. (2001), ‘Automatic Distinction of Arguments and Modifiers: The Case
of Prepositional Phrases,’ in Proceedings of the Workshop on Computational Language
Learning, Toulouse, France.
Navigli, R., and Velardi, P. (2004), ‘Learning Domain Ontologies from Document Warehouses
and Dedicated WebSites,’ Computational Linguistics, 50.
Partners of ESPRIT-291/860. (1986), ‘Unification of the Word Classes of the ESPRIT Project 860,’
Internal Report BU-WKL-0376.
Pekar, V., and Staab, S. (2002), ‘Taxonomy Learning –Factoring the Structure of a Taxonomy into a
Semantic Classification Decision,’ in Proceedings of the International Conference on
Computational Linguistics (COLING), Taipei, Taiwan.
Pereira, F., Tishby, N., and Lee, L. (1993), ‘Distributional Clustering of English Words,’ in
Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics
(ACL).
Platt, J. (1998), ‘Fast Training of Support Vector Machines using Sequential Minimal Optimization,’
in Advances in Kernel Methods – Support Vector Learning, eds. B. Schoelkopf, C. Burges, and
A. Smola, Cambridge, MA: MIT Press.
Quinlan, R. (1993), C4.5: Programs for Machine Learning, San Mateo, CA: Morgan Kaufmann
Publishers.
Radu, F., Ittycheriah, A., Jing, H., and Zhang, T. (2003), ‘Named Entity Recognition through
Classifier Combination,’ in Proceedings of the 7th Conference on Computational Natural
Language Learning (CoNNL), Edmonton, Canada, pp. 168–171.
Sarkar, A., and Zeman, D. (2000), ‘Automatic extraction of subcategorization frames for Czech,’ in
Proceedings of the International Conference on Computational Linguistics (COLING 2000),
Saarbruecken, Germany, pp. 691–697.
Sgarbas, K., Fakotakis, N., and Kokkinakis, G. (2000), ‘A Straightforward
Approach to Morphological Analysis and Synthesis,’ in Proceedings of the Workshop on
Computational Lexicography and Multimedia Dictionaries (COMLEX), Kato Achaia, Greece,
pp. 3134.
Sporleder, C., van Erp, M., Porcelijn, T., van den Bosch, A., and Arntzen, P. (2006), ‘Identifying
Named Entities in Text Databases from the Natural History Domain,’ in Proceedings of the
5th International Conference on Language Resources and Evaluation (LREC), Genoa, Italy.
Stamatatos, E., Fakotakis, N., and Kokkinakis, G. (2000), ‘A Practical Chunker for Unrestricted
Text,’ in Proceedings of the Conference on Natural Language Processing, Patras, Greece,
pp. 139–150.
Thanopoulos, A., Kermanidis, K., and Fakotakis, N. (2006), ‘Challenges in Extracting Terminology
from Modern Greek Texts,’ in Proceedings of the Workshop on Text-based Information
Journal of Experimental & Theoretical Artificial Intelligence 41
D
o
w
n
l
o
a
d
e
d
 
B
y
:
 
[
H
E
A
L
-
L
i
n
k
 
C
o
n
s
o
r
t
i
u
m
]
 
A
t
:
 
1
7
:
5
0
 
3
 
M
a
r
c
h
 
2
0
0
9
Retrieval (TIR), European Conference on Artificial Intelligence (ECAI), Riva del Garda,
Italy.
Tomek, I. (1976), ‘Two modifications of CNN,’ IEEE Transactions on Systems, Man and
Communications, SMC, 6, 769–772.
Tsukamoto, K., Mitsuishi, Y., and Sassano, M. (2002), ‘Learning with Multiple Stacking for Named
Entity Recognition,’ in Proceedings of the 6th Conference on Natural Language Learning,
Taipei, Taiwan, pp. 1–4.
Vapnik, V. (1998), Statistical Learning Theory, Hoboken, NJ: Wiley-Interscience.
Witschel, H.F. (2005), ‘Using Decision Trees and Text Mining Techniques for Extending
Taxonomies,’ in Proceedings of the Workshop on Learning and Extending Lexical Ontologies
by Using Machine Learning Methods.
Wu, C., Jan, S., Tsai, T., Hsu, W. (2006), ‘On Using Ensemble Methods for Chinese Named Entity
Recognition,’ in Proceedings of the 5th SIGHAN Workshop on Chinese Language Processing,
Sydney, Australia, pp. 142–145.
42 K.L. Kermanidis
D
o
w
n
l
o
a
d
e
d
 
B
y
:
 
[
H
E
A
L
-
L
i
n
k
 
C
o
n
s
o
r
t
i
u
m
]
 
A
t
:
 
1
7
:
5
0
 
3
 
M
a
r
c
h
 
2
0
0
9
