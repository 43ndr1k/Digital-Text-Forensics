Author Attribution Evaluation with Novel Topic Cross-Validation
Andrew I. Schein, Johnnie F. Caver and Craig H. Martell
Computer Science Department
Naval Postgraduate School
1411 Cunningham Road
Monterey, CA 93943
USA
{aischein,jfcaver,cmartell}@nps.edu
Abstract
The practice of using statistical models in predicting au-
thorship (so-called author attribution models) is long es-
tablished. Several recent authorship attribution studies
have indicated that topic-specific cues impact author at-
tribution machine learning models. The arrival of new
topics should be anticipated rather than ignored in an
author attribution evaluation methodology; a model that
relies heavily on topic cues will be problematic in de-
ployment settings where novel topics are common. We
develop a protocol and test bed for measuring sensi-
tivity to topic cues using a methodology called novel
topic cross-validation. Our methodology performs a
cross-validation where only topics unseen in training
data are used in the test portion. Analysis of the test-
ing framework suggests that corpora with large num-
bers of topics are beneficial for novel topic evaluation
studies. In order to implement the evaluation metric,
we developed two subsets of the New York Times An-
notated Corpus including one with 15 authors and 23
topics. Evaluation with a maximum entropy classifier
on our multi-category data set revealed a 33% drop in
accuracy when performing author attribution over new
topics. Our evaluation framework supports automatic
learning of stylometric cues that are topic neutral, and
our test bed is reproducible using document identifiers
available from the authors.
Introduction
Authorship attribution researchers build machine learning
classification models or rule-based systems identifying the
author of an anonymous text given undisputed knowledge
of various communications written by that particular author.
The earliest (as well as continuing) efforts in the field looked
at the authorship of historically interesting documents. To-
day, interest in the field is additionally motivated by fairness
and public welfare concerns: plagiarism detection and iden-
tifying authors in a criminal investigation or intelligence set-
ting.
Several authorship attribution studies have speculated
about the existence of a link between topic cues and author
style features (Mikros and Argiri 2007; Corney 2003; Kop-
pel, Schler, and Bonchek-Dokow 2008; Madigan et al. 2005;
Copyright c© 2010, Association for the Advancement of Artificial
Intelligence (www.aaai.org). All rights reserved.
Gehrke 2008). We present a novel experimental protocol for
measuring the impact of topic features on author attribution
predictive models. Our technique, called novel topic cross-
validation, consists of isolating a single topic in a test set,
generating training models from the remaining topics, then
iterating over choices of held-out topic to compute an aver-
age performance score. The result is a method for measuring
the impact topic cues have on the ability of the classifier to
predict the author of a particular communication.
Novel topic cross-validation simulates a scenario where
we are trying to perform the author attribution task when
novel topics appear. However, our work developing an eval-
uation method that can separate topic cue influence in a
classifier is additionally motivated by a desire to build ro-
bust models using stylometric features. Stylometric features
are those that capture author specific word and grammar
choices. Author style may vary depending on the topic, and
quantifying this phenomenon is highly desirable. Many as-
pects of author style are likely to be relatively topic neutral,
and understanding these features of style would be highly
beneficial to the author attribution community. On the other
hand, learning features that are topic-specific but have little
to do with style should be less important to the author attri-
bution researcher; when deploying the model we are likely
to discover both new authors writing on the same topics as
a target author as well as new topics that our target authors
may write about. These two categories of novel entities (au-
thors and topics) will damage performance of a method re-
lying strictly on topic cues.
We quickly realized that existing metrics and data re-
sources are not well developed to implement a research pro-
gram that attempts to isolate stylometric cues from topic.
Our development of an evaluation methodology and test bed
allows the community as a whole to understand, measure,
and isolate topic influence in author attribution studies. For
this reason, we are making the author, topic and document
IDs from our test bed available on a web site1. The origi-
nal unfiltered data is the New York Times Annotated Cor-
pus (Sandhaus 2008) which is distributed by the Linguistic
Data Consortium. Thus, this work introduces the methodol-
ogy of novel cross validation, as well as a testbed for imple-
1
http://faculty.nps.edu/cmartell/
NPSCrossValidationSet/nps nyt novel topic cv.tar.gz
menting the procedure. With our test bed and methodology
we demonstrate that author attribution on novel topics can
be harder than on known topics. With techniques for mea-
surement established, researchers may begin to tackle this
problem in a scientific fashion.
Using the New York Times Annotated Corpus, we gener-
ated two sub-corpora of data: one consisting of 3,000 doc-
uments cross-tabulated with 2 authors and 4 topics (the bi-
nary data set), and the other consisting of 18,862 documents
cross-tabulated with 15 authors and 23 topics (the multi-
category data set). From these sub-corpora, we perform
a novel topic cross-validation comparing the results with a
standard cross-validation. Our data set differs from previ-
ous test beds used to model topic/author influence in scope,
balance, and classification; previous studies were limited
to three or fewer topics or authors, using equally balanced
data sets2 and binary classifications. The document count
of previous studies was frequently limited to several hun-
dred. Having a larger set of documents, topics and authors
combined with our innovative approach to controlling topic
should provide researchers with a greater opportunity to ex-
plore the variability of style cues represented in sets of au-
thors, as well as the confounding influence of topic. More-
over, our experiments and analysis demonstrate that hav-
ing a larger number of topics (with documents distributed
as evenly as possible among them) has important and ben-
eficial ramifications for hypothesis testing in a novel topic
evaluation.
Related Work
Recent review articles describe the state of the art in au-
thor attribution algorithms, similarity statistics, feature sets,
and evaluation methods (Stamatatos 2009; Malyutov 2006).
Since our own work focuses primarily on the influence of
topic and evaluation methodology, we focus our review on
these areas.
Studies of Topic Influence
Several previous efforts have made an attempt to quantify a
relationship between topic and author. The first study, con-
ducted by Mikros and Argiri, tested topic-neutrality of stylo-
metric features used in authorship attribution by performing
a two-way ANOVA test to determine the interaction between
authors and topics (Mikros and Argiri 2007). They tested the
impact of topic on authorship attribution using the following
stylometric features:
• Vocabulary richness
• Sentence length
• Function words
• Average word length
• Character frequency
The corpus they used consisted of 200 modern Greek
electronic newswire articles written by two authors about
2A balanced data set is one where category labels are equally
represented. In this case, the balance refers to the number of arti-
cles written by each author.
two topics. The data set was completely balanced, with each
author writing 100 articles, half of which were written about
one of two topics. From the results of the two-way ANOVA
test, they concluded that there is a significant correlation be-
tween the stylometric features and topic text, and that use of
such features in authorship attribution over multi-topic cor-
pora should be done with caution.
The second study, conducted by Koppell, Schler, and
Bonchek-Dokow, explored the depth of difference between
topic variability in authorship attribution using an unmask-
ing technique (Koppel, Schler, and Bonchek-Dokow 2008).
The intuition behind this technique is to gauge how fast
the cross-validation accuracy degrades during the process
of iteratively removing the most distinguishable features be-
tween two classes. They used a corpus of 1,139 Hebrew-
Aramaic legal query response letters written by three distinct
authors about three distinct topics. They concluded that it
is more difficult to distinguish writings by the same author
on different topics than writings by different authors on the
same topic.
The third study, conducted by Corney (Corney 2003),
showed that the topic did not adversely affect the identifi-
cation of the author in e-mail messages. In order to support
this claim, Corney used a corpus of 156 e-mail messages
from three distinct authors about three distinct topics. He
then developed a model for each of the three authors, us-
ing one of the three topics. Next, he used a support vector
machine to test for authorship on e-mails from the remain-
ing two topics. He reported a success rate of approximately
85% when training on one topic and testing on the others,
which was consistent with the rate of success for authorship
attribution across all topics. We attribute Corney’s results to
the length and structure of e-mail communications. Often,
the most discriminatory words associated with topic are in
the subject of an e-mail and, therefore, if only the body of
the e-mail text is evaluated, the impact of content-specific
words could easily be negligible.
In contrast to results obtained by Corney (Corney 2003),
the fourth study, by Madigan et al. (Madigan et al. 2005),
tested the effect of topic on authorship attribution with 59
Usenet postings by two distinct authors and three distinct
topics. Just as with Corney, they constructed a model of each
author on one of the three topics and tested for authorship
on postings written about the remaining two topics. Their
results demonstrated poor performance by a unigram model;
however, their bi-gram parts-of-speech model proved to be
one of the best among the tested possibilities.
Finally, the fifth study, conducted by Baayen et
al. (Baayen, van Halteren, and Tweedie 1996), used princi-
pal components analysis (PCA) and linear discriminant anal-
ysis (LDA) to evaluate the effectiveness of grouping text by
author, using stylometric features. Their data set consisted
of 72 documents written by eight students. Each student
wrote a total of 24 documents in three different genres about
three different topics. They found that compensating for the
topic imbalance coverage led to increased performance in a
cross-validation.
The recent review by Stamatatos (Stamatatos 2009) points
to a small number of additional similar studies. A key dif-
ference between our data set and those used by previous re-
searchers is size. The number of observations in our multi-
category data set is much larger than any of the previous
examples. In addition, our multi-category data set has many
more topics, which we will later argue is advantageous in a
novel topic evaluation. The nature of our evaluation is also a
bit different in that it simulates what happens when a author
attribution classifier encounters a new topic. Many of the
previous studies are “in-sample” analysis or examine other
questions pertaining to topics.
Evaluation Methodology
Typical evaluations of author attribution divide a corpus into
a train/test split. In some cases standardized train/test splits
have been developed for reproducibility (Stamatatos, Kokki-
nakis, and Fakotakis 2000). When developing an evaluation,
typically researchers have attempted to control for factors
that can influence outcome. In addition to topic (the focus
of the current work), age, sex, or other attributes of the au-
thor may have predictive ability that need to be controlled.
In our opinion, within the literature a consensus has formed
that an evaluation will ideally have a balanced number of
documents per author in a test set; this greatly simplifies the
interpretability of a test set accuracy. In practice, requiring
data set balance limits the qualified data sets available to the
author attribution researcher. In particular, it is challenging
to locate a data set with many authors writing very many
documents if we require these authors to write on the same
topics and with the same frequency.
In his recent review of the author attribution methods, Sta-
matatos comments on evaluation: “Ideally, all the texts of
the training corpus should be on exactly the same topic for
all the candidate author” (Stamatatos 2009). This advice is
important in aspects of algorithm evaluation. However, we
see the field of author attribution progressing by embrac-
ing topic and social distinctions as a source of complex-
ity with scientifically (and functionally) interesting conse-
quences. We believe there is an important place for evalua-
tion methodologies that focus on exploring factors that have
real consequences for building deployable systems rather
than neutralizing them for algorithm evaluation purposes.
Data Set Preparation
The New York Times Annotated Corpus is a collection of
1, 855, 658 XML documents representing nearly all arti-
cles published in the NYT between January 1987 and June
2007 (Sandhaus 2008). Each XML document contains one
New York Times article along with meta-data identifying in-
formation pertaining to the document to include the docu-
ments title, author, and topic. Although 99.95% of the doc-
uments contain tags for the topic, only 48.18% of the docu-
ments contain tags for the author. Therefore, we filtered the
data to a subset of 871, 050 documents that are tagged with
author, topic, and title.
From this corpus, we selected documents with a single
topic and a single author. After our filtering steps on the
NYT Annotated Corpus, we were left with a subcorpus with
this property. Documents written by a single author about
a single topic were selected from a relational database in
order to generate the following two subsets of data used to
conduct these experiments: a binary data set and a multi-
category data set. The binary data set was balanced across
two authors (e.g. the two authors wrote the same number of
documents) and unbalanced across four topics. The multi-
category data set was unbalanced across 15 authors and un-
balanced across 23 topics. Due to the independent proce-
dures used to generate the two subsets, the binary data set is
not a strict subset of the multi-category data set.
Binary Data Set
In the binary data subset, a total of 3, 000 documents were
selected from the 224, 308 that were written by a single au-
thor about a single topic. This subset consisted of documents
written by two distinct authors who wrote an equal number
of documents. These documents were about four distinct
topics that appeared in at least 500 of the 3, 000 documents.
Table 1 is a list of authors along with the corresponding total
number of documents in the subset written by each author.
The average vocabulary size over all documents was 282.57
with a minimum vocabulary size of 2 and a maximum of
1, 304.
Multi-Category Data Set
In the multi-category data set, a total of 18, 862 documents
were selected from the 224, 308 documents that were writ-
ten by a single author about a single topic. This subset con-
sisted of documents written by a total of 15 distinct authors
and about 23 distinct topics. Table 3 lists the topic cate-
gories along with their corresponding topic identifications.
Table 2 shows the how counts are distributed amongst au-
thors and topics. The minimum number of documents writ-
ten by a particular author was 730 and the maximum num-
ber was 2, 912. The minimum number of documents written
about a particular topic was 35 and the maximum number
was 2, 907. The average vocabulary size over all documents
was 306.12 with a minimum vocabulary size of 25 and a
maximum of 2, 889.
Experimental Design
Feature Extraction
We extracted the article text from each of the documents in
to a separate file for processing. Certain authors were dupli-
cated in the source data with differing IDs: Stephen Hold-
ing and Stuart Elliott. These author IDs were merged in
the experiments that follow. Punctuation was removed from
the text of the documents by replacing all non-alphanumeric
characters with the empty string. In addition, all letters were
converted to lower case to reduce the dimensionality of the
feature space. Finally, to facilitate use of unigram word fea-
tures, data was processed into word grams by tokenizing
words on whitespace.
The regular expression that extracted text from the file did
not always capture the lead paragraph; we discovered that
Author ID Author Author Total Docs Topic ID Topic Topic Total Docs
A100024 Dunning, Jennifer 1500
T50031 Music 1
T50048 Motion Pictures 6
T50050 Dancing 1,467
T50128 Theatre 26
A100078 and A105328 Holden, Stephen 1500
T50031 Music 500
T50048 Motion Pictures 494
T50050 Dancing 6
T50128 Theatre 500
Table 1: Author/Topic Data Tabulation
Prediction Ground Truth
A100024 A100078
A105328
A100024 True Positive False Positive
A100078/A105328 False Negative True Negative
Table 4: Matrix Used to Construct Precision, Recall, and
F-score
some XML documents in the NYT Annotated Corpus con-
tained an XML tag for a lead paragraph then repeated the
lead paragraph twice in the XML tag for the full text whereas
other documents did not.
Scenario 1: 10-Fold Cross-Validation
Our baseline testing scenario is a 10-fold cross-validation of
the sort that is usually applied in author attribution as well
as other machine learning tasks. A maximum entropy clas-
sifier (Berger, Pietra, and Pietra 1996; Daumé III 2004) was
trained on 90% of the documents, and then tested on the re-
maining 10% for each fold using a binary classification for
the data set with two authors and using a multi-category clas-
sification for the data set with 15 authors. The 10% of test
documents in each fold consisted of 10% of the documents
written by each author with the last fold also including any
remaining documents not tested in folds one through nine.
The 10-fold cross-validation provides a baseline for com-
paring degradation when performing author attribution on a
novel topic task.
Scenario 2: Novel Topic Cross-Validation
In the second scenario, we conducted a leave-one-topic-out
n-fold cross-validation where n represented the total num-
ber of topics in the data set. In each fold of the experiments,
the maximum entropy classifier was tested on all documents
pertaining to one topic and trained on all other documents
pertaining to the remaining n − 1 topics. There were a total
of four topics in the binary data set, and a total of 23 topics
in the multi-category data set.
Performance Measures
Accuracy, precision, recall, and F-score were the perfor-
mance measures used to evaluate the results of the exper-
iments. Precision, recall and F-score are standard evalu-
ation metrics in the natural language processing commu-
nity (Manning and Schtze 1999). All metrics were com-
puted for each cross-validation fold of the binary data set.
Only accuracy was computed for the multi-category data set
since the other metrics are designed for binary classifica-
tion. Table 4 depicts the confusion matrix used to compute
the precision, recall, and F-score for the two authors in the
binary data set.
In a standard cross validation, each fold of the cross-
validation has equal sizes (modulo a small factor for unequal
division), and one can compute average and standard devia-
tions for the results of the cross-validation naively. Within a
novel topic cross-validation, each division of data will have
different sizes since generally there will be different num-
bers of documents in each topic category. For novel topic
cross-validation the appropriate aggregation technique for
the test statistics is to use a weighted average and standard
deviation: the weights are computed as the fraction of the
total document count represented within the cross-validation
fold. The size of the sample for computing an average ac-
curacy (or other score) in a novel topic cross-validation is
equal to the number of topics. From this, we determine that
having a greater number of topics is beneficial. From the
mean and variance computations for the weighted average:
µ̂ =
n∑
i=1
wixi (1)
V2 =
n∑
i=1
w2
i
(2)
σ2 =
1
1 − V2
n∑
i=1
wi(xu − µ̂)
2, (3)
we see that having an equal proportion of documents across
topics (or as close as possible) is also beneficial in producing
low variance estimates of performance. This close-to-equal
proportion property is something we strived to accomplish
in developing the document subsets for our experiments.
We report standard average for n-fold cross-validation and
weighted averages for novel topic cross-validation. When
comparing such averages in a hypothesis testing setting, bear
in mind the differing sample sizes which result from the dif-
ferent procedures.
AUTHORS
A100024 A100078
A105328
A111554 A111915
A104872
A100046 A100042 A113159 A102480 . . .
TOPICS T50014 3 4 0 4 0 1 0 0
T50031 1 1149 0 0 0 0 0 0
T50013 0 0 491 0 12 55 1022 729
T50128 26 509 0 0 0 0 0 0
T50012 0 0 6 0 21 867 135 13
T50048 6 1602 0 1 0 0 0 0
T50015 0 1 0 0 0 0 0 0
T50097 0 0 179 0 25 10 3 6
T50050 1536 6 0 0 0 0 0 0
T50006 9 6 0 12 0 0 0 0
T50115 0 0 781 0 780 19 0 357
T50136 0 0 0 0 0 0 0 0
T50187 0 0 0 290 0 0 0 0
T51556 0 16 0 1 0 0 0 0
T50172 0 0 0 1487 0 0 0 0
T50383 0 0 4 0 157 5 0 0
T50368 0 0 6 0 0 155 0 1
T50273 0 0 25 0 33 17 0 0
T50222 0 0 0 0 0 0 0 0
T50338 0 0 1 0 154 0 0 1
T50049 1 0 0 63 0 0 0 0
T50214 0 0 0 0 0 0 0 0
T50077 0 0 0 0 0 0 0 0
TOTALS 1582 3293 1493 1858 1182 1129 1160 1107
A100512 A111487 A100023 A101068 A100006 A111661 A111723 TOTALS
T50014 0 3 1 354 18 1 1 390
T50031 0 0 0 0 1 0 783 1934
T50013 560 0 0 0 0 0 0 2869
T50128 0 0 145 1 842 0 1 1524
T50012 2 0 0 0 0 0 0 1044
T50048 0 2 752 539 5 0 0 2907
T50015 0 764 1 0 0 1 0 767
T50097 2 0 0 0 0 0 0 225
T50050 0 0 0 0 1 0 0 1543
T50006 0 0 3 0 2 1 2 35
T50115 0 0 0 0 0 0 0 1937
T50136 0 0 0 0 0 394 0 394
T50187 0 0 0 0 0 0 0 290
T51556 0 5 0 0 0 0 33 55
T50172 0 0 0 0 0 0 0 1487
T50383 0 0 0 0 0 0 0 166
T50368 0 0 0 0 0 0 0 162
T50273 490 0 0 0 0 0 0 565
T50222 0 121 0 0 0 0 0 121
T50338 0 0 0 0 0 0 0 156
T50049 0 0 0 0 0 0 0 64
T50214 0 0 0 0 0 163 0 163
T50077 0 0 0 0 0 64 0 64
TOTALS 1054 895 902 894 869 624 820 18862
Table 2: Topic/Author Data Tabulation for the Multi-Category Data Set
T50014 Books and Literature T50187 Appointments and Executive Changes
T50031 Music T51556 Deaths (Obituaries)
T50013 Baseball T50172 Advertising and Marketing
T50128 Theatre T50383 Golf
T50012 Football T50368 Boxing
T50048 Motion Pictures T50273 Horse Racing
T50015 Art T50222 Photography
T50097 Basketball T50338 Soccer
T50050 Dancing T50049 Suspensions, Dismissals and Resignations
T50006 Television T50214 Cooking and Cookbooks
T50115 Hockey, Ice T50077 Food
T50136 Restaurants
Table 3: Multi-Category Topic Categories
Top 3 Accuracy Categories
Topic Accuracy
Suspensions, Dismissals, and Resignations 1.0000
Appointments and Executive Changes 0.9966
Advertising and Marketing 0.9059
Bottom Three Accuracy Categories
Topic Accuracy
Dancing 0.0026
Football 0.0057
Soccer 0.0064
Table 7: Topics with Highest/Lowest Accuracy in Novel
Topic Cross-Validation
Results
For both binary and multi-category data sets, substantial
performance degradation occurs when evaluating on novel
topics. Table 5 shows the comparison of performance be-
tween a standard 10-fold cross-validation and a novel topic
cross-validation for the binary data set, while Table 6 shows
the same effect for the multi-category data set. The results
are recorded as averages (over the cross-validation folds) as
well as the corresponding standard deviations. We report
different performance metrics between binary and multi-
category data sets for the reasons outlined in the Perfor-
mance Measures section. The performance degradation of
average scores (such as accuracy) is accompanied in all
cases by an increase in the standard deviation. Examination
of the data shows that novel topic cross-validation leads to
a much broader range of accuracies (or other metrics) when
compared against a standard cross validation.
For the binary data set, the results included a 36.60%
decline in F-score (statistically significant with p-value <
0.05) and a 22.59% decline in accuracy (not statistically sig-
nificant). For the multi-category data set, the outcome was
a 32.51% decline in accuracy, with a much more significant
p-value: ≪ 0.001. We examined the size of the vocabulary–
both in the training and the test sets and found no correlation
with accuracy in the novel topic cross-validation. Table 7
shows the topics that had the highest and lowest accuracies
in the novel topic cross-validation.
Discussion
Our evaluation simulates an important use case for an author
attribution model; predicting author identity as new topics
are discovered. The results show that the novel topic sce-
nario is much harder than the simpler known-topic one: the
accuracies are lower with much larger standard deviation.
The scenario we have constructed is a reproducible evalu-
ation for comparing different author attribution techniques
on the novel topic problem. Ideally, we would like to have a
data set where each author has written on each topic in equal
numbers. If this were the case then each author would have
the same amount of training data on each fold of the novel
topic cross-validation and the comparison of means becomes
easier. Finding such a corpus with many authors, topics, and
documents is challenging, and remains a subject for future
work. We have done the next best thing: taken steps to en-
sure that each author has at least several hundred examples
in each fold of the cross-validation.
We examined individual folds of the novel topic cross-
validation to see which topics were particularly hard or easy
to classify. The top three “easiest” and “hardest” topics are
listed in Table 7. At first it appears that broader topics (such
as Suspensions, Dismissals, and Resignations) are easiest,
while very focused topics such as football were hard. How-
ever, those three “easy” topics are almost entirely written
by a single author, and so it is likely that the particular au-
thor is easy to predict–independent of the topic. The ease
of accurately classifying this particular author is embed-
ded in the average accuracy of the standard 10-fold cross-
validation, but does not stick out noticeably within individ-
ual folds the way it does with a novel topic cross-validation.
Nonetheless, when we perform the document-weighted av-
erage of the performance metrics (e.g. accuracy) in the
novel topic cross-validation, each document and topic has
the same weight it would under an ordinary cross-validation
regime.
We developed the novel topic scenario using two data sets
in order to have one that is balanced in author writings (the
binary data set) and another that is larger in documents, au-
10-Fold Cross-Validation Novel Topic Cross-Validation (N=4)
Average Std. Dev. Average Std. Dev.
Accuracy 0.9957 0.0039 0.7697 0.2593
Precision 0.9993 0.0023 0.7319 0.4063
Recall 0.9923 0.0075 0.7385 0.2316
F-Score 0.9956 0.0039 0.6296 0.2768
Table 5: Accuracy, Precision, Recall and F-score Results for the Binary Data Set
10-Fold Cross-Validation Novel Topic Cross-Validation (N=23)
Average Std. Dev. Average Std. Dev.
Accuracy 0.5839 0.0737 0.2587 0.2904
Table 6: Accuracy for the Multi-Category Data Set
thors, and topics (the multi-category data set). In hindsight
we can conclude that having a larger data set is more im-
portant for novel topic cross-validation than having balance.
The reason is that we would eventually like to perform ma-
chine learning model comparison with these data sets, and
model comparison requires as many repeated samples as
possible. In the novel topic cross-validation for the binary
data set there are only 4 topics, and therefore 4 samples to
compute a weighted mean and standard deviation. With such
small counts, we were unable to call a 22.59% decline in ac-
curacy significant. However, for the multi-category data set
the number of folds is 23, and this provides opportunity for a
much more statistically powerful hypothesis test. From this
line of reasoning we additionally conclude that our multi-
category test bed is superior for novel topic cross-validation
than the data sets described in the Related Work section (we
included the number of topics in each of the data sets in part
to make this point). The increase in number of authors and
topics presents other advantages by providing a richer sam-
pling of the complexity of variation that exists “in the wild.”
Conclusions
Author attribution evaluation with novel topic cross-
validation demonstrates that predicting authorship can be a
harder task when the document includes a previously unseen
topic. Our evaluation puts a statistically significant p-value
on this conclusion, and paves the way for reproducible stud-
ies that attempt to overcome the novel topic problem. More-
over, we have established a testing protocol researchers can
use to identify style features that are unaffected or less af-
fected by change in topic. In order to perform a statistically
powerful hypothesis test, our experiments and analysis lead
us to recommend large data sets with many documents and
topics, and with as close-to-equal document membership in
topics as possible.
Acknowledgments
We would like to thank Constantine Perepelitsa for help tun-
ing our evaluation scripts. Members of the Naval Postgrad-
uate School Natural Language Processing Laboratory gave
valuable feedback. We would like to thank the National Re-
connaissance Office for funding that supported this work.
References
Baayen, H.; van Halteren, H.; and Tweedie, F. 1996. Out-
side the cave of shadows: using syntactic annotation to en-
hance authorship attribution. Literary and Linguistic Com-
puting 11(3):121–132.
Berger, A. L.; Pietra, V. J. D.; and Pietra, S. A. D. 1996.
A maximum entropy approach to natural language process-
ing. Comput.Linguist. 22(1):39–71.
Corney, M. W. 2003. Analysing E-mail Text Authorship
for Forensic Purposes. Master’s thesis, Queensland Uni-
versity of Technology.
Daumé III, H. 2004. Notes on CG and LM-BFGS opti-
mization of logistic regression. Paper available at http:
//pub.hal3.name#daume04cg-bfgs, implemen-
tation available at http://hal3.name/megam/.
Gehrke, G. T. 2008. Authorship Discovery in Blogs Using
Bayesian Classification with Corrective Scaling. Master’s
thesis, Naval Postgraduate School.
Koppel, M.; Schler, J.; and Bonchek-Dokow, E. 2008.
Measuring Differentiability: Unmasking Pseudonymous
Authors. Journal of machine learning research : JMLR.
8(1):1261–1276.
Madigan, D.; Genkin, A.; Lewis, D. D.; Argamon, S.;
Fradkin, D.; and Ye, L. 2005. Author Identification on the
Large Scale. In Proc. of the Meeting of the Classification
Society of North America.
Malyutov, M. 2006. Authorship attribution of texts: A
review. Ahlswede, Rudolf (ed.) et al., General theory of
information transfer and combinatorics. Berlin: Springer.
Lecture Notes in Computer Science 4123, 362-380 (2006).
Manning, C. D., and Schtze, H. 1999. Foundations of Sta-
tistical Natural Language Processing. Cambridge, Mass.:
MIT Press. ID: 40848647.
Mikros, G., and Argiri, E. K. 2007. Investigating Topic
Influence in Authorship Attribution. In Proceedings of the
SIGIR ’07 Workshop on Plagiarism Analysis, Authorship
Identification, and Near-Duplicate Detection, PAN 2007,
Amsterdam, Netherlands, July 27, 2007.
Sandhaus, E. 2008. The New York Times Annotated Cor-
pus Overview. Linguistic Data Consortium, Philadelphia.
Stamatatos, E.; Kokkinakis, G.; and Fakotakis, N. 2000.
Automatic text categorization in terms of genre and author.
Comput. Linguist. 26(4):471–495.
Stamatatos, E. 2009. A Survey of Modern Authorship
Attribution Methods. Journal of the American Society for
Information Science and Technology 60(3):538–556.
