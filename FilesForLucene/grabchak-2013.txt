This article was downloaded by: [University of Aegean]
On: 18 November 2013, At: 07:02
Publisher: Routledge
Informa Ltd Registered in England and Wales Registered Number: 1072954
Registered office: Mortimer House, 37-41 Mortimer Street, London W1T 3JH,
UK
Journal of Quantitative
Linguistics
Publication details, including instructions for authors
and subscription information:
http://www.tandfonline.com/loi/njql20
Authorship Attribution Using
Entropy
M. Grabchak a , Z. Zhang a & D. T. Zhang b
a University of North Carolina at Charlotte , Charlotte ,
USA
b North Carolina School of Mathematics and Science ,
Durham , USA
Published online: 12 Nov 2013.
To cite this article: M. Grabchak , Z. Zhang & D. T. Zhang (2013) Authorship
Attribution Using Entropy, Journal of Quantitative Linguistics, 20:4, 301-313, DOI:
10.1080/09296174.2013.830551
To link to this article:  http://dx.doi.org/10.1080/09296174.2013.830551
PLEASE SCROLL DOWN FOR ARTICLE
Taylor & Francis makes every effort to ensure the accuracy of all the
information (the “Content”) contained in the publications on our platform.
However, Taylor & Francis, our agents, and our licensors make no
representations or warranties whatsoever as to the accuracy, completeness, or
suitability for any purpose of the Content. Any opinions and views expressed
in this publication are the opinions and views of the authors, and are not the
views of or endorsed by Taylor & Francis. The accuracy of the Content should
not be relied upon and should be independently verified with primary sources
of information. Taylor and Francis shall not be liable for any losses, actions,
claims, proceedings, demands, costs, expenses, damages, and other liabilities
whatsoever or howsoever caused arising directly or indirectly in connection
with, in relation to or arising out of the use of the Content.
This article may be used for research, teaching, and private study purposes.
Any substantial or systematic reproduction, redistribution, reselling, loan, sub-
licensing, systematic supply, or distribution in any form to anyone is expressly
forbidden. Terms & Conditions of access and use can be found at http://
www.tandfonline.com/page/terms-and-conditions
D
ow
nl
oa
de
d 
by
 [
U
ni
ve
rs
ity
 o
f 
A
eg
ea
n]
 a
t 0
7:
02
 1
8 
N
ov
em
be
r 
20
13
 
Authorship Attribution Using Entropy*
M. Grabchak1, Z. Zhang1 and D. T. Zhang2
1University of North Carolina at Charlotte, Charlotte, USA; 2North Carolina School of
Mathematics and Science, Durham, USA
ABSTRACT
We propose a new methodology for testing the authorship of a relatively small work
compared with the large body of an author’s cannon. Our approach is based on comparing
the entropy of the two samples. The difficulty lies in the fact that known estimators of
entropy tend to have a large bias even when the sample size is fairly large. To deal with this,
we suggest splitting the larger sample into several parts of length equal to the length of the
smaller work. We then propose using these new sub-samples in a simple non-parametric test.
We apply our methodology to test whether the poem “Shall I Die?” which is sometimes
attributed to William Shakespeare was, in fact, written by him.
1. INTRODUCTION
Entropy was introduced by Shannon (1949) as a measure of randomness. In
quantitative linguistics it is a common index used to quantify an author’s
lexical richness. While Thoiron (1986) argued that this index misses many
important properties that a measure of lexical richness should possess,
Greive (2007), in a study of many such measures, showed that it can never-
theless be useful in author attribution studies. Further applications of
entropy and other measures of lexical richness to author attribution are
given in K. T. Zhang and Z. Zhang (2010). In this paper, we introduce a
new methodology for testing the authorship of a particular piece of writing
using entropy.
Our objective is to perform a statistical test to see if two writing samples
come from the same author. The first sample, which we call the “corpus”,
*Address correspondence to: M. Grabchak, Department of Mathematics and Statistics, UNC
Charlotte, 340H Fretwell Hall, Charlotte, NC. Email: mgrabcha@uncc.edu
Journal of Quantitative Linguistics, 2013
Vol. 20, No. 4, 301–313, http://dx.doi.org/10.1080/09296174.2013.830551
 2013 Taylor & Francis
D
ow
nl
oa
de
d 
by
 [
U
ni
ve
rs
ity
 o
f 
A
eg
ea
n]
 a
t 0
7:
02
 1
8 
N
ov
em
be
r 
20
13
 
is assumed to be much larger than the second. It represents the totality of
the author’s work (or at least the totality of the author’s work of a particular
type). The second, much shorter sample, which we call the “test sample”, is
the work whose authorship is being tested. In order to perform such a test,
we must mathematically summarize the information in the two writing
samples. Before discussing our approach, we formulate our mathematical
framework.
Consider all of the possible word types that an author can use, although
not necessarily those that have actually been used by the author. We call
this the “vocabulary”. Of course, it is an impossible task to actually write
down every word in the English (or any other) language; even the best dic-
tionaries are not exhaustive. Never-the-less, it is convenient to be able to
talk about all of them, even though we will never have to work them all at
once. We assume that the vocabulary consists of some large, but finite,
number of word types S. For our purposes this value need not be known.
Our setup implies that all authors share the same vocabulary but with differ-
ent relative frequency (probability) distributions. Let us denote the common
vocabulary by the sequence fcsg ¼ fcs; 1  s  Sg, where cs, for each s,
stands for a distinct word type, and denote an author’s probability distribu-
tion over {cs} by {ps} = fps; 1  s  Sg. We call this the author’s “word
type distribution”. For a given author, we think of ps as the probability that
the author will use the word cs. This probability is unobserved. Even if we
have the totality of an author’s writings, we can only find the frequency
with which the author has used the word, and not the probability with
which the author would use the word if he/she were to write something
new.
If there are two authors, 1 and 2, then their word type distributions are
denoted by fpð1Þs g ¼ fpð1Þs ; 1  s  Sg and fpð2Þs g ¼ fpð2Þs ; 1  s  Sg. It
is clear that fpð1Þs g–fpð2Þs g if and only if pð1Þs –pð2Þs for some 1  s  S. We
define a lexical “difference” between two authors as a difference in their
word type distributions. It is, of course, possible, although intuitively not
very likely, that two distinct authors have the exact same word type distri-
bution. In this case the problem is outside of our realm of consideration.
Next, let us describe how an author samples words from his/her word
type distribution, i.e., how an author chooses which words to write. We
assume that the words in a writing sample are an iid sample from the
underlying word type distribution. This assumption is implicitly made in
many, if not all, authorship attribution studies based on lexical richness. Of
302 M. GRABCHAK ET AL.
D
ow
nl
oa
de
d 
by
 [
U
ni
ve
rs
ity
 o
f 
A
eg
ea
n]
 a
t 0
7:
02
 1
8 
N
ov
em
be
r 
20
13
 
course when an author writes a text, the words are written down one after
another in sequence. Grammatical rules prevent certain words from immedi-
ately following certain other words. This fact clearly suggests that the
words, as they are written down, are not a sequence of independent trials.
To model such a complex process, one must use a stochastic process to
absorb the dynamically changing dependence between consecutive words.
This modelling perspective is, however, outside the realm of a purely lexi-
cal approach. Interested readers may refer to Thisted and Efron (1987) for
an example. Our assumption of the sample being iid can be partially justi-
fied from a different perspective. It is assumed that an iid sample of n
words is taken from the distribution {ps} and the actual text is a particular
arrangement of the n randomly selected words. The arrangement process is
not a part of the sampling scheme. While this is a useful way of thinking
about the situation, it nevertheless misses some important local structure
and we will discuss an ad hoc way to correct for this below.
Note that an author’s word type distribution {ps} is difficult to work
with directly since it is an S-dimensional object, where S is, in general, very
large. Instead we would like to summarize the information in {ps} using a
measure of lexical richness. In this paper we use the entropy of the underly-
ing distribution. Recall that the entropy of {ps} is given by
H ¼ HðfpsgÞ ¼ 
XS
s¼1
ps log ps:
This allows us to work with just one number H instead of the S-dimen-
sional word type distribution {ps}. We pay a small price for this simplifica-
tion because it is possible for two distinct distributions to have the same
entropy. This is demonstrated by a situation discussed in Zhang and Zhang
(2010) where two authors have the same word type distributions except that
when one writes “yes” the other writes “no”. In general, if one word type
distribution is just a permutation of another word type distribution then they
will have the same entropy. There are, also, other situations that can lead to
the same entropy coming from different word type distributions. Neverthe-
less, we do not expect any of these situations to be common in practice.
Given a writing sample, we need a way to estimate the entropy of the
underlying distribution. Let n be the number of words in the writing sam-
ple, let r̂s be the number of times that word cs appears in the sample, and
let p̂s ¼ r̂s=n. The entropy of the sample is given by 
PS
s¼1 p̂s log p̂s.
AUTHORSHIP ATTRIBUTION USING ENTROPY 303
D
ow
nl
oa
de
d 
by
 [
U
ni
ve
rs
ity
 o
f 
A
eg
ea
n]
 a
t 0
7:
02
 1
8 
N
ov
em
be
r 
20
13
 
While this can be used as an estimator of the entropy of the underlying
distribution, it is well known that the bias of this estimator decays very
slowly. For details about this so called “plug-in” estimator see the
references in Zhang and Zhang (2012). Instead, we will use the estimator
first proposed in Zhang (2012), which has exponentially decaying bias. This
estimator is given by
Ĥz ¼
Xn1
v¼1
1
v
nvþ1½n ðvþ 1Þ!
n!
XS
s¼1
p̂s
Yv1
j¼0
1 p̂s  jn
 " #( )
ð1Þ
We have found that the following equivalent formulation is more
tractable for computational purposes
Ĥz ¼
XS
s¼1
p̂s
Xnr̂s
v¼1
1
v
Yv1
j¼0
1þ 1 r̂s
n 1 j
 " #
ð2Þ
Note that, although the first sum ranges over all of the word types in
our vocabulary, p̂s ¼ 0 for any word type, cs, that is not in our sample. For
this reason, we never have to worry about any words that were not
observed. The estimator Ĥz always underestimates the true entropy and the
bias is given by
H  E½Ĥz ¼
X1
v¼n
1
v
XS
s¼1
psð1 psÞv
where H is the true entropy and E½Ĥz is the expected value of the estimator
Ĥz. Note that the bias depends on both the underlying distribution and the
sample size. This means that estimates with different sample sizes will have
very different amounts of bias and are thus not easily comparable. However,
this is exactly the situation where we are interested in making a
comparison.
To deal with the fact that our two samples have vastly different sizes,
and thus vastly different amounts of bias, we propose chopping the larger
(corpus) sample into several sub-samples of the same size as the smaller
(testing) sample. Specifically, assume that the corpus is made up of n1
words (that is the sample is of size n1) and the testing sample is made up
of n2 words, where n2 is much smaller than n1. Divide the corpus into
m ¼ ½n1=n2 sub-samples, each of size n2 (here b:c refers to the integer
304 M. GRABCHAK ET AL.
D
ow
nl
oa
de
d 
by
 [
U
ni
ve
rs
ity
 o
f 
A
eg
ea
n]
 a
t 0
7:
02
 1
8 
N
ov
em
be
r 
20
13
 
part). The remaining n1  mn2 words in the corpus can be discarded. We
now have m + 1 independent samples of size n2, one of these is the testing
sample, while the remaining m are sub-samples of the corpus.
Our null hypothesis is that the original two samples come from the same
word type distribution. This means that our m + 1 samples of size n2 are
iid under the null hypothesis. We calculate the entropy estimator Ĥz for
each of the m + 1 iid sub-samples of size n2, obtaining Ĥz;1; Ĥz;2; :::; Ĥz;mþ1.
Ordering these from smallest to largest gives the order statistics of the esti-
mates. Under the null hypothesis, every permutation of the order statistics
is equally likely. Thus the test sample is equally likely to be in any one of
the m + 1 ordered positions. Let ‘ refer to its position. Clearly, if ‘ is far
from the centre, that is far from (m + 1)/2, then this is evidence against the
null hypothesis. The p-value for this test is the number of permutations in
which the location of the test sample is as far or further from the centre of
the distribution as it is observed to be. Specifically, if ‘\ðmþ 1Þ=2 then
the two-sided p-value is 2‘=ðmþ 1Þ and if ‘[ðmþ 1Þ=2 then the two-sided
p-value is 2ðmþ 2 ‘Þ=ðmþ 1Þ: Note that, no matter what happens, the p-
value can never be less than 2=ðmþ 1Þ: We can also consider a one-sided
test; in this case we divide the given p-value by 2, i.e. either ‘=ðmþ 1Þ for
a left-sided hypothesis or ðmþ 2 ‘Þ=ðmþ 1Þ for a right-sided hypothesis.
We end this section with a discussion of how to chop up the corpus into
m sub-samples. According to our framework, any random division into sub-
samples should work equally well. However, as we have discussed, a piece
of writing is not really a random sample. There is a lot of structure
involved. For instance an author may use repetition of certain phrases, and
certain words like “the” must be used with some amount of regularity. For
this reason, we suggest that, to preserve the local structure of the writing,
the corpus should be chopped up as follows. Let the first sub-sample be the
first n2 words of the corpus, let the second sub-sample be the next n2 words
of the corpus, and so forth until the mth sub-sample is obtained.
2. DATA ANALYSIS
We now apply our methodology to the poetry of William Shakespeare. In
1985, a poem was discovered with an attribution to Shakespeare. See, for
example, Foster (1987) or Taylor (1985). This poem starts with the line
“Shall I die? Shall I fly?” For this reason the poem is often referred to as
AUTHORSHIP ATTRIBUTION USING ENTROPY 305
D
ow
nl
oa
de
d 
by
 [
U
ni
ve
rs
ity
 o
f 
A
eg
ea
n]
 a
t 0
7:
02
 1
8 
N
ov
em
be
r 
20
13
 
“Shall I Die?” Several statistical analyses have been performed to test if this
poem was, in fact, written by Shakespeare. Thisted and Efron (1987) were
unable to reject the assumption that this poem has the same distribution as
all of Shakespeare’s other writings. On the other hand, Zhang and Huang
(2007) rejected this assumption. In both cases “Shall I Die?” was compared
with the totality of Shakespeare’s writing. However, as pointed out in
Greive (2007), “When attributing an anonymous text, it is unnecessary and
unsound to compile an author-based corpus that attempts to represent the
variety that encompasses all that author’s written utterances ... [instead it
should be based on] texts produced in the most similar register, for the most
similar audience, and around the same point of time as the anonymous
text.” To deal with these issues, Zhang and Zhang (2010) compare “Shall I
Die?” with 14 of Shakespeare’s sonnets. They find no significant difference.
However, they worry about the power of their test. We will use our method-
ology to test the authorship of this poem. To further analyse our methodol-
ogy, we also test a poem known to be written by Shakespeare and a group
of poems known to be written by Philip Sidney, a contemporary of
Shakespeare.
The corpus, representing Shakespeare’s (short) poetry, consists of all 154
of Shakespeare’s sonnets. The sonnets were downloaded from http://www.
opensourceshakespeare.org. The sample is simply the sonnets written in order
from first to last with no titles or numbers. This sample has n1 = 17,539
words in total, of these there are 3199 unique word types. By word type we
mean a particular word, we ignore punctuation and capitalization. Thus if a
word sometimes appears capitalized and other times not capitalized, we still
treat it as the same word type.
First, we compare the corpus to the test sample containing the short
poem “The Pheonix and the Turtle”. This poem was downloaded from
http://www.opensourceshakespeare.org. The poem is known to have been
written by Shakespeare, but it is not a sonnet and hence not part of the
Table 1. The corpus was divided into 49 sub-samples, each with 351 words. Here we give
the estimated entropy for each sub-sample. They are ordered from smallest to largest.
4.904 5.040 5.125 5.131 5.136 5.147 5.152 5.179 5.213 5.224
5.227 5.258 5.265 5.267 5.273 5.279 5.283 5.286 5.296 5.302
5.304 5.311 5.316 5.316 5.324 5.326 5.327 5.336 5.349 5.357
5.361 5.364 5.378 5.379 5.395 5.396 5.397 5.400 5.402 5.405
5.407 5.409 5.426 5.429 5.436 5.438 5.443 5.444 5.524
306 M. GRABCHAK ET AL.
D
ow
nl
oa
de
d 
by
 [
U
ni
ve
rs
ity
 o
f 
A
eg
ea
n]
 a
t 0
7:
02
 1
8 
N
ov
em
be
r 
20
13
 
corpus. The sample size is n2 = 351 with 217 unique word types. Its
entropy is estimated to be Ĥz ¼ 5:370: The ordered entropy estimates for
the m = 49 sub-samples of the corpus are given in Table 1. Of these 17 are
larger than the estimated entropy for the test sample, thus the one-sided p-
value is 18/50 = 0.36 and the two sided p-value is 36/50 = 0.72. This
means that we cannot reject the null hypothesis that the poem was written
by Shakespeare. This is as we would expect since we know that the poem
was, in fact, written by Shakespeare.
Next, we compare the corpus with four randomly chosen sonnets from
Philip Sidney’s Astrophel and Stella sequence. Specifically, our test sample
consists of sonnets 5, 6, 10, and 29. These sonnets were downloaded from
http://www.theotherpages.org/poems/. In this case n2 = 490 with 291 unique
words types. The entropy is estimated to be Ĥz ¼ 5:639. The ordered
entropy estimates for the m = 35 sub-sample of the corpus are given in
Table 2 below. Of these all are smaller than the estimated entropy for the
test sample, thus the one-sided p-value is 1/36 ≈ 0.0278 and two-sided
p-value is 2/36 ≈ 0.0556. Here we reject the hypothesis that these sonnets
were written by Shakespeare. This is as we would expect since these
sonnets are known to have not been written by Shakespeare.
Finally, we compare the corpus with “Shall I Die?” This poem has n2 =
428 words and 260 unique word types. The entropy estimate for “Shall I
Die?” is Ĥz ¼ 5:544. The ordered entropy estimates for the m = 40
sub-sample of the corpus are given in Table 3 below. There are three entries
here that are larger than the value obtained for “Shall I Die?” Thus a
Table 3. The corpus was divided into 40 sub-samples, each with 428 words. Here we give
the estimated entropy for each sub-sample. They are ordered from smallest to largest.
5.005 5.142 5.214 5.249 5.286 5.298 5.312 5.315
5.349 5.349 5.353 5.362 5.372 5.374 5.386 5.396
5.402 5.403 5.406 5.406 5.427 5.429 5.433 5.438
5.443 5.448 5.453 5.458 5.462 5.479 5.480 5.497
5.497 5.503 5.515 5.515 5.528 5.608 5.641 5.644
Table 2. The corpus was divided into 35 sub-samples, each with 490 words. Here we give
the estimated entropy for each sub-sample. They are ordered from smallest to largest.
5.131 5.147 5.152 5.213 5.224 5.227 5.258 5.265 5.267
5.273 5.283 5.302 5.304 5.311 5.316 5.324 5.326 5.327
5.336 5.349 5.357 5.364 5.378 5.379 5.395 5.397 5.402
5.405 5.409 5.426 5.429 5.436 5.438 5.443 5.444
AUTHORSHIP ATTRIBUTION USING ENTROPY 307
D
ow
nl
oa
de
d 
by
 [
U
ni
ve
rs
ity
 o
f 
A
eg
ea
n]
 a
t 0
7:
02
 1
8 
N
ov
em
be
r 
20
13
 
one-sided p-value is 4/41 ≈ 0.0976 and a two-sided p-value is 8/41 ≈
0.1951. This offers very marginal statistical significance, if any, to support
the alternative hypothesis that the poem was written by Shakespeare.
3. CONCLUDING REMARKS
Lexical richness can be reasonably measured by many indices. Our paper is
concerned with Shannon’s entropy. Another popular measure of lexical rich-
ness is Simpson’s index, n1;1 ¼
PS
s¼1 psð1 psÞ. This index can be thought
of as a weighted average of the relative frequencies of all word types in an
author’s vocabulary, particularly with higher weights on word types that
have lower relative frequencies. The generalized Simpson’s indices,
nu;v ¼
PS
s¼1 p
u
s ð1 psÞv where u  1and v  0 are prefixed integers, intro-
duced in Zhang and Zhou (2010), are also of this type. Author attribution
using these indices is discussed is Zhang and Zhang (2010).
Simpson’s index and the generalized Simpson’s indices put an emphasis
on the “rare” word types used by an author. However, in all of these cases
the weights are bounded by one. On the other hand, Shannon’s entropy
weights a word type cs by  log ps. This weight can be arbitrarily large, as
ps is arbitrarily small. Since it is commonly thought that the usage patterns
of rare word types is where the writing style lies, entropy is particularly
useful for author attribution. We choose Shannon’s entropy for this reason,
and for the fact it is an essential measurement of information in modern
information theory, see, for example, Cover and Thomas (2006).
The estimator Ĥz proposed by Zhang (2012) is a significant improve-
ment over the standard plug-in estimator of entropy since it has an expo-
nentially decaying bias. The asymptotic normality of Ĥz established by
Zhang (2013) would seem to have resolved the two-sample test problem for
the equality of two entropies. Unfortunately, the bias of Ĥz remains stub-
bornly substantial for smaller samples. This may not seem like a major
issue at first since, when performing a two-sample z-test, we consider the
difference between two estimators. If they have comparable bias, then this
difference should have a very small amount of bias. However, since, in the
situation at hand, we are comparing samples with very different sizes,
the two estimators will have vastly different amounts of bias and thus the
difference between the two may still have an unacceptable amount of bias.
The methodology proposed in this paper, however, forces the bias to be
308 M. GRABCHAK ET AL.
D
ow
nl
oa
de
d 
by
 [
U
ni
ve
rs
ity
 o
f 
A
eg
ea
n]
 a
t 0
7:
02
 1
8 
N
ov
em
be
r 
20
13
 
comparable by making the samples have equal sizes. Thus, if a difference
is observed it is likely due to distributional differences rather than an
artefact caused by the fact that estimates based on different sample sizes
have different amounts of bias. This, in turn, lends more power to answer
the question of interest.
The proposed methodology can be easily extended to cover the case
where the two samples have sizes that are not vastly different (but different
enough that the bias issues discussed above are still present). Suppose n1 =
500, n2 = 200, and therefore m = 2. The proposed methodology essentially
fails due to the fact that m is too small. Even if the estimated entropies are
very different, the one-sided p-value bottoms out at 1/3, which would offer
no chance of rejecting the null hypothesis. However, the spirit of the
proposed methodology would remain intact if we chop both samples into
sub-samples of size n where n ≠ n2 but a fraction of n2. For the given
example one may take n = 200/4 = 50. Thus the larger sample is chopped
into m1 = 10 sub-samples of size n = 50 and the smaller sample is chopped
into m2 = 4 sub-samples also of size n = 50. These 14 sub-samples would
in turn give 14 independent estimates of the entropy. We can then test the
Table 4. The corpus was divided into 163 sub-samples, each with 107 words. Here we give
the estimated entropy for each sub-sample. They are ordered from smallest to largest.
4.295 4.318 4.356 4.372 4.375 4.388 4.404 4.415 4.415 4.421 4.422 4.423
4.424 4.430 4.431 4.443 4.449 4.449 4.458 4.458 4.459 4.471 4.477 4.487
4.502 4.505 4.520 4.523 4.525 4.534 4.538 4.538 4.540 4.541 4.546 4.550
4.555 4.556 4.558 4.561 4.561 4.562 4.571 4.572 4.574 4.582 4.583 4.587
4.590 4.590 4.590 4.591 4.595 4.595 4.595 4.597 4.601 4.602 4.602 4.603
4.605 4.605 4.606 4.608 4.608 4.610 4.611 4.614 4.616 4.618 4.620 4.623
4.623 4.625 4.626 4.626 4.627 4.628 4.628 4.629 4.629 4.634 4.636 4.641
4.642 4.644 4.646 4.646 4.650 4.650 4.654 4.655 4.662 4.667 4.668 4.669
4.670 4.672 4.674 4.677 4.678 4.679 4.680 4.681 4.682 4.687 4.687 4.689
4.691 4.691 4.693 4.694 4.700 4.702 4.703 4.704 4.705 4.713 4.716 4.717
4.718 4.718 4.720 4.724 4.728 4.728 4.731 4.731 4.733 4.736 4.741 4.742
4.742 4.742 4.749 4.749 4.749 4.751 4.756 4.759 4.764 4.765 4.767 4.768
4.781 4.784 4.785 4.786 4.790 4.794 4.795 4.800 4.800 4.801 4.803 4.809
4.811 4.812 4.818 4.827 4.843 4.845 4.846
Table 5. “Shall I Die?” was divided into four sub-samples, each with 107 words. Here we
give the estimated entropy for each sample. They are ordered from smallest to largest.
4.393 4.721 4.762 4.798
AUTHORSHIP ATTRIBUTION USING ENTROPY 309
D
ow
nl
oa
de
d 
by
 [
U
ni
ve
rs
ity
 o
f 
A
eg
ea
n]
 a
t 0
7:
02
 1
8 
N
ov
em
be
r 
20
13
 
Ta
bl
e
6.
W
or
d
fr
eq
ue
nc
ie
s
fo
r
“S
ha
ll
I
D
ie
?”
.
F
re
qu
en
cy
1
2
3
4
5
6
7
8
12
13
14
20
N
o.
of
w
or
ds
20
1
33
6
6
5
1
3
1
1
1
1
1
Ĥ
ðsÞ z
0.
01
6
0.
02
6
0.
03
6
0.
04
5
0.
05
3
0.
06
1
0.
06
8
0.
07
6
0.
10
1
0.
10
7
0.
11
3
0.
14
4
310 M. GRABCHAK ET AL.
D
ow
nl
oa
de
d 
by
 [
U
ni
ve
rs
ity
 o
f 
A
eg
ea
n]
 a
t 0
7:
02
 1
8 
N
ov
em
be
r 
20
13
 
null hypothesis with a Wilcoxon rank-sum test, or the like, based on the
two samples of sizes m1 = 10 and m2 = 4.
For the example where “Shall I Die?” is compared to the corpus
composed of 154 sonnets written by Shakespeare we have n1 = 17,539 and
n2 = 428. If we take n = 107, then the corpus is chopped into m1 = 163
sub-samples and “Shall I Die?” is chopped into m2 = 4 sub-samples. The
ordered estimated values of the entropy are given in Tables 4 and 5. A
Wilcoxon rank-sum test (performed using the Wilcoxon Test function in R)
gives a one-sided p-value of 0.1626 and a two-sided p-value of 0.3252.
This is comparable to (although slightly larger than) what we saw in the
previous section. Note that when we order all 167 samples from Tables 4
and 5 together, the estimates from parts of “Shall I Die?” are in positions 7,
125, 143, and 155. The relatively small entropy corresponds to the first
fourth of “Shall I Die?” It appears to be caused by the unusually large
amount of repetition in that part of the poem. Although the estimated entro-
pies for all four parts are in the tails of the distribution, the first is in the
left tail, while the rest are in the right tail. This helps to explain why
working with this poem leads to only marginal results.
Finally, we note two distinct advantages of our methodology: (1)
Simplicity. The calculation is simple and straightforward. A computer
program is available from the authors to compute the value of Ĥz. After
obtaining these estimated entropies, the p-value can be easily calculated by
the simple formulas given in Section 1. (2) Generality. The proposed meth-
odology is not limited to Ĥz. Any reasonable statistic, of entropy or other-
wise, can be used in place of Ĥz, albeit argument for using entropy is afore
given. In fact, as illustrated by Mosteller and Wallace (1963), there is often
quite bit of information available, a priori, outside of the samples. In such
cases a more effective metric based on empirical information may be found
and used.
REFERENCES
Cover, T. A. M., & Thomas, J. A. (2006). Elements of Information Theory, 2nd edition.
Hoboken: John Wiley & Sons, Inc.
Foster, D. W. (1987). “Shall I Die” post mortem: defining Shakespeare. Shakespeare
Quarterly, 38(1), 58–77.
Greieve, J. W. (2007). Quantitative authorship attribution: an evaluation of techniques.
Literary and Linguistic Computing, 22(3), 251–270.
AUTHORSHIP ATTRIBUTION USING ENTROPY 311
D
ow
nl
oa
de
d 
by
 [
U
ni
ve
rs
ity
 o
f 
A
eg
ea
n]
 a
t 0
7:
02
 1
8 
N
ov
em
be
r 
20
13
 
Mosteller, F., & Wallace, D. L. (1963). Inference in an authorship problem. Journal of the
American Statistical Association, 58(302), 275–309.
Shannon, C. E., & Weaver, W. (1949). The Mathematical Theory of Communication.
Urbana, IL: University of Illinois Press.
Taylor, G. (1985). Shakespeare’s new poem: A scholar’s clue and conclusion. New York
Times Book Review (15 December), 11–14.
Thisted, R., & Efron, B. (1987). Did Shakespeare write a newly-discovered poem?
Biometrika, 74(3), 445–455.
Thoiron, P. (1986). Diversity index and entropy as measures of lexical richness. Computers
and the Humanities, 20, 197–202.
Zhang, K. T., & Zhang, Z. (2010). Shakespearean Sonnets versus Shakespearean Canon.
Journal of Quantitative Linguistics, 17(2), 81–93.
Zhang, Z. (2012). Entropy estimation in Turing’s perspective. Neural Computation, 24(5),
1368–1389.
Zhang, Z. (2013). Asymptotic normality of an entropy estimator with exponentially decaying
bias. IEEE Transactions on Information Theory, 59(1), 504–508.
Zhang, Z., & Huang, H. W. (2007). Turing’s formula revisited. Journal of Quantitative
Linguistics, 14(2–3), 222–241.
Zhang, Z., & Zhou, J. (2010). Re-parameterization of multinomial distribution and diversity
indices. Journal of Statistical Planning and Inference, 140(7), 1731–1738.
Zhang, Z., & Zhang, X. (2012). A normal law for the plug-in estimator of entropy. IEEE
Transactions on Information Theory, 58(5), 2745–2747.
APPENDIX
In this appendix we give a detailed explanation of how to evaluate Formula (2) in the
case of the poem “Shall I Die?” The approach is similar for the other poems. “Shall I
Die?” is made up of 428 words and 260 unique word types. The word type frequencies
for this poem are given in the first two lines of Table 6. This table can be interpreted as
follows: there are 201 word types that appear exactly once, 33 word types that appear
exactly twice, six word types that appear exactly three times, etc. Let
Ĥ ðsÞz ¼ p̂s
Xnr̂s
v¼1
1
v
Yv1
j¼0
1þ 1 r̂s
n 1 j
 " #
be the contribution of word type cs to Ĥz. We need to evaluate Ĥ ðsÞz for each word type
that has been observed and then to add all of these together. If a word type cs1 appears
only once then
r̂s1 ¼ 1; p̂s1 ¼ r̂s1n 1=428;
and
312 M. GRABCHAK ET AL.
D
ow
nl
oa
de
d 
by
 [
U
ni
ve
rs
ity
 o
f 
A
eg
ea
n]
 a
t 0
7:
02
 1
8 
N
ov
em
be
r 
20
13
 
Ĥ ðs1Þz ¼
1
428
X427
v¼0
1
v
Yv1
j¼0
1þ 1 1
427 j
 " #
 :016
If a word type cs2 appears exactly twice then r̂s2 ¼ 1; p̂s2 ¼ 2428, and
Ĥ ðs2Þz ¼
2
428
X426
v¼1
1
v
Yv1
j¼0
1 1
427 j
 " #
 :026
We continue evaluating Ĥ ðsÞz for all of the word types that appear in the poem. This
is summarized in the third line of Table 6. This can be interpreted as follows: every one
of the 201 word types with frequency 1 have Ĥ ðsÞz ≈ 0.016, every one of the 33 word
types with frequency 2 have Ĥ ðsÞz ≈ 0.026, every one of the six word types with
frequency 3 have Ĥ ðsÞz ≈ 0.036, etc.
To estimate the entropy we need to add the Ĥ ðsÞz over all of the word types. Since
there are 201 word types with frequency 1, the contribution of all of these words is
(201)(0.016), similarly the contribution of all 33 word types of frequency 2 is (33)
(0.026). In this way we see that
Ĥz ¼ ð201Þð0:016Þ þ ð33Þð0:026Þ þ ð6Þð0:036Þ þ ð6Þð0:045Þ þ ð5Þð0:053Þ
þ ð1Þð0:061Þ þ ð3Þð0:068Þ þ ð1Þð0:076Þ þ ð1Þð0:101Þ þ ð1Þð0:107Þ
þ ð1Þð0:113Þ þ ð1Þð0:144Þ:
This sum does not give us exactly Ĥz = 5.544, which was reported in Section 2,
because of rounding error. However, if we evaluate everything at the level of machine
precision, we would get this estimate.
AUTHORSHIP ATTRIBUTION USING ENTROPY 313
D
ow
nl
oa
de
d 
by
 [
U
ni
ve
rs
ity
 o
f 
A
eg
ea
n]
 a
t 0
7:
02
 1
8 
N
ov
em
be
r 
20
13
 
