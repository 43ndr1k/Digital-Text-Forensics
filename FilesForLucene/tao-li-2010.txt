Appl Intell (2010) 33: 207–219
DOI 10.1007/s10489-009-0160-4
On combining multiple clusterings: an overview and a new
perspective
Tao Li · Mitsunori Ogihara · Sheng Ma
Published online: 12 February 2009
© Springer Science+Business Media, LLC 2009
Abstract Many problems can be reduced to the problem of
combining multiple clusterings. In this paper, we first sum-
marize different application scenarios of combining multi-
ple clusterings and provide a new perspective of viewing the
problem as a categorical clustering problem. We then show
the connections between various consensus and clustering
criteria and discuss the complexity results of the problem.
Finally we propose a newmethod to determine the final clus-
tering. Experiments on kinship terms and clustering popular
music from heterogeneous feature sets show the effective-
ness of combining multiple clusterings.
Keywords Multiple clusterings · Combining · Categorical
1 Introduction
1.1 Problem overview
Generally the problem of combining multiple clusterings is:
given multiple clusterings of the dataset, find a combined
clustering which would provide better cluster results. Many
problems can be reduced to the problem of combining mul-
tiple clusterings:
T. Li ()
School of Computer Science, Florida International University,
11200 SW 8th Street, Miami, FL 33199, USA
e-mail: taoli@cs.fiu.edu
M. Ogihara
Department of Computer Science, University of Miami,
1365 Memorial Drive, Coral Gables, FL 33146, USA
S. Ma
Machine Learning for Systems, IBM T.J. Watson Research
Center, 17 Skyline Drive, Hawthorne, NY 10532, USA
• Ensemble clustering: Clustering is an inherently ill-posed
problem due to the lack of label information. Different
clustering algorithms and evenmultiple replications of the
same algorithm result in different solutions due to ran-
dom initializations and stochastic learning methods. In
the area of supervised learning, ensemble methods, by
combining multiple classifiers, have shown to be a pop-
ular way to overcome instability in classification prob-
lems [6]. The success of ensemble methods in classifi-
cation provides the main motivation for applying ensem-
ble methods in clustering. The problem of ensemble clus-
tering is to find a combined clustering result based on
multiple clusterings of the dataset. There are many ways
to obtain multiple clusterings such as applying different
clustering algorithms; using resampling to get subsam-
ples of the dataset [53], utilizing feature selection meth-
ods such as random projection to get different feature
spaces [18], and exploiting the randomness of the clus-
tering algorithm.
• Clustering with Multiple Criteria: In many applications,
especially in the social sciences, clustering problems of-
ten require optimization over more than one criterion [15]
or clustering needs to meet additional constraints which
are not included in the clustering criterion [16]. A typical
example is second world war politicians problem [14],
where the data were obtained by asking many subjects to
rate the dissimilarities of second world war politicians.
Each subject corresponds to a optimization criterion and
hence clustering the politicians needs to optimize multiple
criteria. For clustering with multiple criteria, solutions op-
timal according to each particular criterion are not identi-
cal. The core problem is then how to find the best solution
so as to satisfy as much as possible all the criteria con-
sidered. A typical approach is to combine multiple clus-
terings obtained via single criterion clustering algorithms
208 T. Li et al.
based on each criterion [11]. There are also several recent
proposals on multi-objective data clustering [2, 26, 37,
48, 55, 56].
• Distributed clustering: Over the years, data set sizes have
grown rapidly with the advances in technology and the in-
creasingly automated business processes. Many data sets
are, in nature, geographically distributed across multiple
sites. In a distributed environment, data sites may be ho-
mogeneous, i.e., different sites containing data for exactly
the same set of features, or heterogeneous, i.e., different
sites storing data for different set of features, possibly
with some common features among sites. To cluster the
distributed datasets, one way is to first cluster them lo-
cally and then combine the clustering obtained at each
site [34, 47].
• Three-way clustering: When a multivariate phenomenon
is observed on different occasions, the datasets collected
are identified according to three modes: units (rows), vari-
ables (columns) and occasions (layers) and then arranged
into a three-way data matrix X = (xijh) [63]. Typical ex-
amples include macroeconomics performance data of dif-
ferent regions over different time period or medical exam-
inations of different plants over different locations. One
way to perform clustering on three-way datasets is to clus-
ter the units based on the variables at each occasion and
then combine the clustering results [4].
In addition, combining multiple clusterings also provides a
framework for knowledge reuse and can be used to exploit
existing knowledge implicit in legacy clusterings [60]. It can
also be applied to privacy-preserving data mining and miss-
ing value imputation [20]. Moreover, many different algo-
rithms have also been developed recently for ensemble clus-
tering [1, 3, 12, 17, 27, 30, 40, 41].
1.2 Content of the paper
This paper provides a unified view on combining multiple
clusterings by studying connections among various consen-
sus and clustering criteria. The contribution of this paper is
four-fold: we first provide a new perspective of viewing the
problem of combining multiple clustering as a categorical
clustering problem, we then show the connections between
various consensus and clustering criteria. Third, we discuss
the complexity results of the problem, and finally we pro-
pose a new method to determine the final clustering. Our
experimental results on kinship terms and clustering popu-
lar music from heterogeneous feature sets show the effec-
tiveness of combining multiple clusterings. A preliminary
version of the paper is appeared in [46].
The rest of the paper is organized as follows: Sect. 2 for-
mally defines the problem of combining multiple clusterings
and presents two different perspectives: consensus partition
and categorical clustering. Section 3 introduces various cri-
teria for combining multiple clusterings. Section 4 illustrates
the connections among them. In particular, the equivalence
relation between consensus partition and categorical cluster-
ing is established. Section 5 discusses the complexity results
of the problem and Sect. 5.3 presents a method for deter-
mining final clusterings. Section 6 presents our experiments
on kinship terms and clustering popular music, and finally
Sect. 7 concludes.
2 Problem definition
Formally the problem of combining multiple clusterings
can be described as follows: let D = {d1, d2, . . . , dn} be
a set of n data points. Suppose we are given a set of T
partitions1 P = {P1,P2, . . . ,PT } of the data points in D.
Each partition Pi, i = 1, . . . , T consists of a set of clusters
Pi = {C1i ,C2i , . . . ,Ckii } where ki is the number of clusters
for partition Pi and D = ⋃kij=1Cji . Our goal is to find a fi-
nal clustering P = {C1, . . . ,CK } of D such that the points
inside each Ci are “similar” to each other.
Basically the problem of combining multiple clusterings
can be solved from two perspectives: one is using consensus
classification methods to find a target partition for optimiz-
ing consensus functions [11, 31], and the other one, as we
will show later, is performing categorical (or binary) clus-
tering in the space induced by the multiple partitions.
A consensus function maps a given set of partitions P =
{P1,P2, . . . ,PT } to a final partition P . There are many ways
to define the consensus function [11]. On the other hand,
it is convenient to characterize the consensus problem as a
binary clustering problem where the attributes are induced
by the partitions. Let p = ∑Ti=1 ki , then each point di ∈ D
can be represented as a p-dimensional vector
di = (di11, . . . , di1k1, . . . , dij1, . . . , dijkj , . . . , diT 1,
. . . , diT kT ) (1)
dijl =
{
1 di ∈ Clj ,
0 otherwise,
1≤ j ≤ T ,1≤ l ≤ kj
After inducing new binary representation, finding the final
clustering can be achieved with various categorical cluster-
ing algorithms. Data clustering is a data reduction (data dis-
cretization) process, from similarity or distance to eventu-
ally the discrete the decision that if two data points belong
to a cluster or not. It has the advantage of effectively remov-
ing data noise.
1In this paper, we interchangeably use partition and clustering. By par-
tition, we mean a set of mutually exclusive and collectively exhaustive
classes such that each point is in one and only one cluster. Note that
partition is an equivalence relation.
On Combining multiple clusterings: an overview and a new perspective 209
Table 1 Notations for problem definition
D = {d1, d2, . . . , dn} The set of n data points
P = {P1,P2, . . . ,PT } The set of partitions
Pt = {C1t ,C2t , . . . ,Cktt } The clusters in a partition
kt Number of clusters for partition Pt
P = {C1, . . . ,CK } Final partition
K Number of clusters for final partition P
di = (dij l ) Induced vector representation for di
The notations used for problem definition are listed in the
Table 1.
3 Various criteria
There are many different ways to define the consensus func-
tion such as co-associations between data points or based on
pairwise agreements between partitions. Some of the criteria
are based on the similarity between data points and some of
them are based on the estimates of similarity between par-
titions. In what follows, we survey various criteria for com-
bining multiple clusterings. The connections among various
criteria are elaborated in Sect. 4.
Note that each partition Pt defines an associated n × n
matrix that stores the information, for each pair of points,
whether they are in the same cluster. The entries of the ma-
trix are defined as follows:
Mt(i, j) =
{
1 i, j belongs to the same cluster
0 otherwise
(2)
1. Partition difference: Given two partitions A and B ,2 a
common measure of their difference is:3
(A,B) =
∑
i,j
(A(i, j) − B(i, j))2
Intuitively, (A,B) computes the number of pairs of
points that belong to different clusters in A and B . A tar-
get partition P thus should minimize
∑T
t=1(P,Pt ).
2. Consensus matrix: Another measure based on the associ-
ated matrices is the normalized consensus matrix, defined
by
S(i, j) = 1
T
T∑
t=1
Mt(i, j)
2We also use A and B to denote the matrices associated with the parti-
tions.
3Note that since A(i, j) and B(i, j) are either 0 or 1, thus (A,B) =∑
i,j (A(i, j) − B(i, j))2 =
∑
i,j |A(i, j) − B(i, j)|.
Basically, S indicates, for each pair of points, the propor-
tion of times in which they are clustered together. S(i, j)
is a measure of co-association indicating the “similarity”
between i and j .
3. Katz & Powell index: Consensus measures can also be
defined based on the indexes of pairwise agreements
among partitions. The first index for comparing two par-
titions was constructed by Katz and Powell [35] as fol-
lows: given two partition matrices A,B , the index of
agreement between them is
(A,B) = n
2NAB − NANB√
NA(n2 − NA)NB(n2 − NB)
where NA and NB are respectively the number of 1’s in
A and B , and NAB is the number of entries in A and B
both defined by a 1. Using the classical contingency table
notation, we have
NAB =
n∑
i=1
n∑
j=1
AijBij =
p∑
u=1
q∑
v=1
n2uv
NA =
∑
ij
Aij =
p∑
u=1
n2u., NB =
∑
ij
Bij =
q∑
v=1
n2.v
where nuv is the number of points in the cluster u of A
and the cluster v ofB; nu. and n.v refer respectively to the
number of points in clusters u and v. It has been shown
that the above index is equivalent to the ordinary product
moment correlation coefficient between the off-diagonal
cells of A and B ordered identically [32]. The best parti-
tion should maximize the value of the overall agreement
Pbest = argmax
p
T∑
t=1
(P,Pt ) (3)
4. Chance-corrected measures: A well-known approach to
define chance-corrected measures of an association or
agreement is
lN = l − τ
lmax − τ
where lN is the chance-corrected measure, τ represents a
structure in which lN = 0 and lmax is the maximum value
of the coefficients regardless of the fixed margins [50].
Cohen’s Kappa [9], defined by
K(A,B) = NAB − NANB
n2 − NANB
is an example of chance-corrected measures. A family of
chance-corrected rand index measures were introduced
in [31]. Similarly, the best partition should maximize the
overall agreement defined by the chance-corrected mea-
sure.
210 T. Li et al.
5. Chi-squared measure: Chi-squared measure for compar-
ing two partitions A and B is defined as follows:
χ2(A,B) =
p∑
u=1
q∑
v=1
(nuv − Euv)2
Euv
(4)
where Euv = nu.n.vn . Assuming the sizes of clusters in
each clustering are fixed, we have
χ2(A,B) =
p∑
i=1
q∑
j=1
(nuv − Euv)2
Euv
=
p∑
i=1
q∑
j=1
n2uv
Euv
− n
Similarly, the best partition should maximize the overall
agreement defined by the chi-squared measure.
6. Category utility functions: Given the partition P =
{C1, . . . ,CK}, the category utility function U(P,Pt ) is
defined as follows [22, 51]:
U(P,Pt ) =
K∑
r=1
p(Cr)
kt∑
j=1
P(C
j
t |Cr)2 −
kt∑
j=1
p(C
j
t )
2 (5)
where p(Cr) = |Cr |n ,p(Cjt |Cr) = |C
j
t ∩Cr ||Cr | ,p(C
j
t ) = |C
j
t |
n
.
In other words, the function U(P,Pt ) assesses the agree-
ment between two partitions as the difference between
the expected number of classes of partition Pt that can
be correctly predicted both with the knowledge of clus-
tering P or without it. The overall utility of the partition
P with respect to P = {P1,P2, . . . ,PT } can then be de-
fined as
U(P, P ) =
T∑
t=1
U(P,Pt )
and thus the best partition should maximize the value of
the overall utility.
7. Information-theoretic measures: Strehl and Ghosh [60]
suggested an information theoretic function based on
mutual information
Pbest = argmax
P
I ′(P, P )
where I ′(P, P ) = 1
T
∑T
t=1 I ′(P,Pt ). I ′(P,Pt ) is the
normalized mutual information between P and Pt and
is calculated by
I ′(P,Pt ) = I (P,Pt )√
H(P )H(Pt )
where
p(Cr,C
j
t ) =
|Cjt ∩ Cr |
n
I (P,Pt ) =
K∑
r=1
kt∑
j=1
p(Cr,C
j
t ) log
(
p(Cr,C
j
t )
p(Cr)p(C
j
t )
)
H(P ) = −
K∑
r=1
p(Cr) logp(Cr),
H(Pi) = −
kt∑
j=1
p(C
j
t ) logp(C
j
t )
It should be pointed out that another information-
theoretic measure VI (Variation of Information), which
measures the amount of information lost and gained in
changing from clustering to clustering, is also proposed
to compare clusterings [49].
8. Within/between cluster distances: Another collection of
criteria is based on clustering criteria for categorical data.
For clustering, a well-known criterion is to find the final
partition minimizing either the intra-cluster distance∑
k
∑
x,y∈Ck
dist(x, y) or
∑
k
∑
x∈Ck
dist(x, C̄k)
or maximizing the between-cluster distance∑
k dist(C̄k, C̄) where C̄k is the centroid for cluster Ck
and C̄ is the overall centroid. dist can be defined by Lp
distances or other dissimilarity measures.
4 Relations among different consensus functions
We have derived the connections among various criteria and
they can be briefly summarized in Fig. 1. In particular, the
equivalence relation between the two perspectives: consen-
sus partition and categorical clustering, is established. The
mathematical details of derivation on the connections are
presented in the rest of this section.
First, the consensus matrix defines similarity between
data points and the similarity is defined by the fraction of the
number of clusters shared by them across all the partitions.
Define dist(i, j) = αS(i, j) + β for some α < 0, β , then
maximizing
∑K
r=1
∑
i,j∈Cr S(i, j) is equivalent to minimize∑K
r=1
∑
i,j∈Cr dist(i, j). A popular choice of α = −1, β = 1
defines dist(i, j) = 1− S(i, j) = 1
T
|di − dj |, where di is the
induced vector representation for point i as in (1).
For consensus criteria based on Katz & Powell index, the
chance-corrected and chi-squared measures, if the sizes of
clusters in each clustering are fixed, they all reduce to maxi-
mizing
∑
t NPPt . In particular, given two clusterings A and
B , if the sizes of each cluster within each clustering are the
same, Katz & Powell index, Kappa and chi-squared mea-
sures are linearly related to each other. Mathematically, we
have
nu. = n
p
, n.v = n
q
, 1≤ u ≤ p,1≤ v ≤ q.
On Combining multiple clusterings: an overview and a new perspective 211
Fig. 1 Summary of Relations for Various Consensus Criteria. The
words beside the links describe connections between the criteria
Then
χ2(A,B) =
p∑
i=1
q∑
j=1
n2uv
Euv
− n = pq
n
p∑
i=1
q∑
j=1
n2uv − n
(A,B) = n
2NAB − NANB√
NA(n2 − NA)NB(n2 − NB)
= pq
∑p
i=1
∑q
j=1 n2uv
n2
√
(p − 1)(q − 1) −
1√
(p − 1)(q − 1)
= 1
n
√
(p − 1)(q − 1)χ
2(A,B)
K(A,B) = NAB − NANB
n2 − NANB =
pq
∑p
i=1
∑q
j=1 n2uv − n4
pqn2 − n4
= 1
n(pq − n2)χ
2(A,B) − 1− n
2
pq − n2
Moreover, observe that
K∑
r=1
∑
i,j∈Cr
S(i, j) =
∑
k
∑
i,j∈Cr
1
T
T∑
t=1
Mt(i, j)
= 1
T
T∑
t=1
K∑
u=1
kt∑
v=1
n2uv =
1
T
T∑
t=1
NPPt
Hence maximizing
∑
k
∑
i,j∈Ck S(i, j) is equivalent to max-
imizing
∑
t NPPt .
Next we show the relations between the partition differ-
ence and other measures. DenoteM as the associated matrix
for partition P ,
T∑
t=1
(P,Pt ) =
T∑
t=1
∑
i,j
(M(i, j) − Mt(i, j))2
=
T∑
t=1
K∑
u=1
kt∑
v=1
n2 −
T∑
t=1
K∑
u=1
kt∑
v=1
n2uv
= Constant− T
K∑
r=1
∑
i,j∈Cr
S(i, j)
So minimizing
∑T
t=1(P,Pt ) is consistent with maximiz-
ing
∑
k
∑
i,j∈Ck S(i, j) and
∑
t NPPt . On the other hand,
T∑
t=1
(P,Pt )
=
T∑
t=1
∑
i,j
(M(i, j) − Mt(i, j))2
= Constant−
∑
i,j
M(i, j)
[
2
T∑
t=1
Mt(i, j) − T
]
= Constant+ T
∑
i,j
M(i, j)[−2S(i, j) + 1] (6)
Define dist(i, j) = −2S(i, j) + 1, hence minimizing∑T
t=1(P,Pt ) is equivalent to minimizing
∑
i,j M(i, j) ×
dist(i, j) = ∑Kr=1∑i,j∈Cr dist(i, j), i.e., the popular clus-
tering criterion.
Now let’s turn to the category utility function. We can
show that the category utility function is equivalent to the
square-error criterion defined for the induced categorical
clustering problem. As we mentioned above, we can trans-
form the partition Pt assuming kt values by kt binary fea-
tures and the solution of the partition problem can be ap-
proached by the clustering algorithm operating in the in-
duced space. Note that
U(P,Pt ) =
K∑
r=1
p(Cr)
kt∑
j=1
p(C
j
t |Cr)2 −
kt∑
j=1
p(C
j
t )
2
=
K∑
r=1
kt∑
j=1
[p(Cr)p(Cjt |Cr) − p(Cjt )p(Cr)]2
p(Cr)
=
K∑
r=1
p(Cr)
kt∑
j=1
(
p(Cr)p(C
j
t |Cr)
p(Cr)
− p(Cjt )
)2
212 T. Li et al.
The overall mean for the tj -th attribute is p(Cjt ) and the
mean for the cluster k in the final partition is p(Cr )p(C
j
t |Cr)
p(Cr )
.
T∑
t=1
U(P,Pt )
=
K∑
r=1
p(Cr)
T∑
t=1
kt∑
j=1
(
p(Cr)p(C
j
t |Cr)
p(Cr)
− p(Cjt )
)2
=
K∑
r=1
p(Cr)dist(C̄r , C̄)
Where C̄r and C̄ are the cluster representative for cluster r
and the overall cluster center respectively. Hence maximiz-
ing the category utility function is equivalent to maximizing
the within-cluster distance weighted by cluster sizes. This
suggests the relations between the category utility function
(using the between-attribute (partition) similarity measures)
and the square-error clustering criterion (using distances be-
tween points and prototypes).
Finally let’s take a look at the information theoretical
measures.
I (P,Pt ) =
K∑
r=1
kt∑
j=1
p(Cr,C
j
t ) log
(
p(Cr,C
j
t )
p(Cr)p(C
j
t )
)
=
K∑
r=1
p(Cr)
kt∑
j=1
p(C
j
t |Cr) logp(Cjt |Cr)
−
kt∑
j=1
p(C
j
t ) logp(C
j
t ) (7)
In fact, if we use x − 1 to approximate logx, then we
have
I (P,Pt ) =
K∑
r=1
p(Cr)
kt∑
j=1
p(C
j
t |Cr) logp(Cjt |Cr)
−
kt∑
j=1
p(C
j
t ) logp(C
j
t )
=
K∑
r=1
p(Cr)
kt∑
j=1
p(C
j
t |Cr)(p(Cjt |Cr) − 1)
−
kt∑
j=1
p(C
j
t )(p(C
j
t ) − 1)
=
K∑
r=1
p(Cr)
kt∑
j=1
p(C
j
t |Cr)2 −
kt∑
j=1
p(C
j
t )
2
−
K∑
r=1
p(Cr)
kt∑
j=1
p(C
j
t |Cr) +
kt∑
j=1
p(C
j
t )
= U(P,Pt ) −
K∑
r=1
kt∑
j=1
p(Cr,C
j
t ) +
kt∑
j=1
p(C
j
t )
= U(P,Pt )
Thus, we can see that (7) and (5) are equivalent.
5 Complexity results on combining multiple clusterings
In this section, we summarize complexity results on the
problem of combining multiple clusterings. There are two
perspectives on the problem: consensus partition and cate-
gorical clustering.
5.1 Consensus partition
From the consensus partition perspective, the problem of
combining multiple clusterings is to find a target partition
which optimizes the consensus criteria. To investigate the
complexity of the problem, let’s look at the partition dif-
ference measure. From (6), maximizing
∑T
t=1(P,Pt ) is
equivalent to minimizing
∑
i,j M(i, j)dist(i, j). Note that
M is the associated matrix for partition P , so it should sat-
isfy the equivalence conditions in Table 2.
The reflexivity and symmetry conditions reduce the size
of the problem and we only need to compute M(i, j), i =
1, . . . , n; j = i + 1, . . . , n. M2(i, j) ≤ M(i, j) can be ex-
pressed as M(i, k) + M(j, k) − M(i, j) ≤ 1,∀i, j, k. With
these conditions, the problem of finding the consensus par-
tition is equivalent to an integer programming problem⎧⎪⎨
⎪⎩
Minimize
∑n
i=1
∑n
j=i+1M(i, j)dist(i, j) where
M(i, j) ∈ {0,1}
M(i, k) + M(j, k) − M(i, j) ≤ 1, ∀i, j, k
(8)
It has been shown in general that the 0− 1 integer program-
ming problem is NP-hard [54].
5.2 Clustering complexity
Another way to look at the problem is from the clustering
perspective. Generally when k ≥ 3 and the number of fea-
tures is greater than 1, the clustering problem is proved to
Table 2 Conditions on the associated matrix
M(i, j) ∈ {0,1} Boolean matrix
M(i, i) = 1, i = 1, . . . , n Reflexivity
M(i, j) = M(j, i) Symmetry
M2(i, j) ≤ M(i, j) Transitivity
On Combining multiple clusterings: an overview and a new perspective 213
be NP-complete [8, 19, 54]. There are few cases, when the
number of clusters is less than 3 or the data only contain one
feature, the clustering problem is polynomial solvable [8].
Here we offer a different view by counting the number
of clusterings. The number of the points to be clustered is
n, and we need to find a clustering into k-classes. Then the
multinomial number
( n
n1,...,nk
)
is the number of clusterings
whose cluster sizes are n1, . . . , nk . Thus the number of all
the clusterings of k clusters is
∑
n1+···+nk=n
( n
n1,...,nk
)
. Tak-
ing the ordering of clusters into consideration, the number of
ways to partition the set of n points into k-disjoint clusters is
N(n, k) = 1
k!
∑k
i=0(−1)k−i
(
k
i
)
in [13]. It is easily seen that
this number is growing exponentially.
5.3 Finding the final clustering
In previous section, we have shown the relations between
various criteria and hence the problem of combining multi-
ple clusterings can be solved via binary clustering. In this
section, we present a method to determine the number of
clusters for final clustering. The similar idea has been ap-
peared in our previous work on document clustering [43].
A critical problem in clustering is to determine the num-
ber of clusters. In this section, we present a method to deter-
mine the number of clusters for final clustering. The similar
idea has been appeared in our previous work on document
clustering [43].
Lemma 1 Let
X =
⎛
⎜⎝
1 · · · 1
...
...
...
1 · · · 1
⎞
⎟⎠
i.e., all entries in the matrix X ∈ Rn×n are 1. Then the only
nonzero eigenvalue of X is n.
Lemma 2 Let
L =
⎛
⎜⎝
X1 0 · · · 0
...
...
...
...
0 · · · 0 Xk
⎞
⎟⎠
That is, L is a block diagonal matrix, with each block matrix
formed as in Lemma 1. Let ni be the size of the matrix Xi ,
for i = 1, . . . , k. Then the only nonzero eigenvalues of L are
n1, n2, . . . , nk .
Lemma 3 Let A and E be two symmetric matrices with the
same dimensions. Then
|λi(A) − λi(A + E)| ≤‖ E ‖2, for i = 1, . . . , n
where λi(A) denotes the i-th largest eigenvalue of the matrix
A, similarly, λi(A + E) for matrix (A + E).
Proofs for Lemmas 1 and 2 are immediate and Lemma 3
follows from the standard results in matrix perturbation the-
ory [21].
Theorem 4 Let M = L + E where L has the form as in
Lemma 2 and E is a matrix with a small value in each en-
try. Then M has k dominant eigenvalues, which are close to
n1, n2, . . . , nk .
Theorem 4 follows directly from Lemmas 2 and 3. Given
a binary dataset D, DDT is a n × n matrix and can be
thought as a similarity matrix between data points. The sim-
ilarity of data point i and j is simply the inner product of
the i-th row and the j -th row of D. If D is normalized, then
each entry ofDDT shows the cosine similarity between cor-
responding data points. Since the permutation of the matri-
ces does not change the spectral properties, we can order
the points in D according to which cluster they are in and
henceDDT can be regarded as the addition of two matrices:
DDT = L + E as described above. Hence the number of
clusters can then be decided from the eigenvalues of DDT .
6 Experiments
In this section, we describe our experiments on kinship
terms and clustering popular music from heterogeneous fea-
ture sets. In both experiments, we first characterize the con-
sensus problem as a binary clustering problem using (1) and
we then solve the problem using clustering approach.
6.1 Experimental setup
Binary data clustering has been widely studied in litera-
ture [25, 29, 33, 42]. A unified view of binary data clus-
tering has been provided by examining the connections
among various methods including entropy-based methods,
distance-based methods (e.g., K-means), mixture models,
and matrix decomposition [38, 39]. In addition, it also shows
the equivalence between K-means clustering methods with
many other methods on binary data clustering using empiri-
cal studies [38, 39]. In our experiments, we use K-means as
the clustering methods.
Generally there are three approaches to evaluate the clus-
tering results (e.g., cluster validity): (a) external criteria in
which the external knowledge about the structure in the data
is used for evaluation; (b) internal criteria in which the in-
herent characteristics of the data is used for evaluation; and
(c) relative criteria in which the evaluation is performed by
comparing with other clustering schemes [28]. In our exper-
iments, we generally have the domain knowledge about the
structure of the data and thus use external criteria for perfor-
mance evaluation. Specifically, we use purity [64], which
214 T. Li et al.
measures the extent to which each cluster contained data
points from primarily one class, as our metric for cluster
validity. The purity of a clustering solution is obtained as a
weighted sum of individual cluster purities and is given by
Purity = ∑Ki=1 nin P (Si),P (Si) = 1ni maxj (nji ) where Si is a
particular cluster of size ni , n
j
i is the number of documents
of the i-th input class that were assigned to the j -th clus-
ter, K is the number of clusters and n is the total number of
points.4 In general, the larger the values of purity, the better
the clustering solution is.
6.2 Kinship terms
The method described in previous sections is first applied
to the analysis of 15 kinship terms data5 provided by
85 female undergraduates at Rutgers University [58]. The
15 terms were Grandfather (GrF), Grandmother (GrM),
Grandson (Grs), Granddaughter (GrD), Brother (Bro), Sis-
ter (Sis), Father (Fat), Mother (Mot), Son, Daughter (Dau),
Nephew (Nep), Niece (Nie), Uncle (Unc), Aunt (Aun) and
Cousin (Cou). Each student was instructed to provide a
grouping of the terms on the basis of some aspect of mean-
ing. The number of groups that the students pick is from 2
to 8. We first characterize the consensus problem as a binary
clustering problem using (1) and we then solve the problem
using clustering approach. After transformation, each term
is then represented as a 421-dimension binary vector. De-
note the transformed dataset as D, then using the approach
described in previous section, we can decide the number of
final clusters. Figure 2 shows the top eigenvalues of the nor-
malized DDT . Note that the eigenvalues are close to the
cluster sizes and there is a big drop from the third largest
eigenvalue to the fourth largest eigenvalue. Hence we choose
the final number of clusters as 3. Figure 3 lists the clustering
results. These results are consistent with previous analysis
of these data reported in [23, 24].
6.3 Clustering popular music using both lyrics and
acoustic data
This section addresses the issue of clustering popular music,
i.e., clustering the music songs into groups denoted by the
artists. As a fundamental and effective tool for efficient or-
ganization, summarization, navigation and retrieval of large
amount of music data, clustering has been very active and
enjoying a growing amount of attention. Ellis et al. [57]
point out that similarity between music songs reflects per-
sonal tastes and suggest that different measures have to be
4P (Si) is also called the individual cluster purity.
5The dataset can be downloaded from http://www-solar.mcs.st-and.ac.
uk/~allan/KinshipTerms.data.
Table 3 Clustering results
Index Members
I {GrF, GrM, GrS, GrD}
II {Sis, Fat,Mot, Son, Dau}
III {Nep, Nie, Unc, Aun, Cou}
Fig. 2 Top eigenvalues of DDT . The Y -axis indicates the eigenvalues
and the X-axis indicates the order of the eigenvalues
combined together so as to achieve reasonable results in sim-
ilarity retrieval. Our previous work has explored the idea of
music artist style identification by semi-supervised learning
from both lyrics and content [44]. In this section, we take the
approach of combining multiple clusterings: first perform
clustering on each separate feature sets and then combine
the clustering results.
6.3.1 Heterogeneous feature sets
In this section, we describe the feature sets extracted from
the lyrics and the acoustic content.
Text-based style features Previous study on stylometric
analysis has shown that statistical analysis on text properties
could be used for text genre identification and authorship at-
tribution [5, 59] and over one thousand stylometric features
(style makers) have been proposed in variety research dis-
ciplines [61]. To choose features for analyzing lyrics, one
should be aware of some characteristics of popular song
lyrics. For example, song lyrics are usually brief and are of-
ten built from a very small vocabulary. In song lyrics, words
are uttered with melody, so the sound they make plays an
important role in the determination of words.
We divided the text-based style features into three dif-
ferent feature sets: Bag-of-words, Part-of-speech statistics
On Combining multiple clusterings: an overview and a new perspective 215
Fig. 3 Artist similarity graph
and Lexical/orthographic features. Each of these three fea-
ture sets has been applied in previous study on stylometric
analysis [61]. The text-based features are summarized in Ta-
ble 4.
• Bag-of-words: We compute the TF-IDF measure for each
words and select the top 200 words as our features. We
remove stop words and do not apply stemming operations.
• Part-of-speech statistics: We use the output of Brill’s part-
of-speech (POS) tagger [7] as the basis for feature extrac-
tion. POS statistics usually reflect the characteristics of
writing. There are 36 POS features extracted for each doc-
ument, one for each POS tag expressed as a percentage of
the total number of words for the document.
• Lexical/orthographic features: By lexical features, we
mean features of individual word-tokens in the text. The
most basic lexical features are lists of 303 generic func-
tion words taken from [52],6 which generally serve as
proxies for choice in syntactic (e.g., preposition phrase
modifiers vs. adjectives or adverbs), semantic (e.g., usage
6Available on line at http://www.cse.unsw.edu.au/~min/ILLDATA/
Function.word.htm.
Table 4 Summary of feature sets for lyric styles
Type Number
Function words (FW) 303
Token place 5
Capitalization 10
Start of . . . 9
Word length 6
Line length 6
Average word length 1
Average sentence length 1
POS features 36
Bag-of-words 200
of passive voice indicated by axillary verbs), and prag-
matic (e.g., first-person pronouns indicating personaliza-
tion of a text) planes. We also use orthographic features
of lexical items, such as capitalization, word placement,
word length distribution. Word orders and lengths are
very useful since the writing of lyrics usually follows a
certain rhythm.
216 T. Li et al.
Content-based features There has been a considerable
amount of work in extracting descriptive features from mu-
sic signals for music genre classification and artist identifi-
cation [45, 62]. In our study, we use timbral features along
with wavelet coefficient histograms. The feature set consists
of the following three parts and totals 35 features.
• Mel-Frequency Cepstral Coefficients (MFCC): MFCC is
designed to capture short-term spectral-based features.
After taking the logarithm of the amplitude spectrum
based on short-term Fourier transform for each frame, the
frequency bins are grouped and smoothed according to
Mel-frequency scaling, which is designed to agree with
perception. MFCC features are generated by decorrelat-
ing the Mel-spectral vectors using discrete cosine trans-
form.
• Other Timbral Features: Timbral features consist of Spec-
tral Centroid, Spectral Rolloff, Spectral Flux, Zero Cross-
ings and Low Energy. Spectral Centroid is the centroid of
the magnitude spectrum of short-term Fourier transform
and is a measure of spectral brightness. Spectral Rolloff
is the frequency below which 85% of the magnitude dis-
tribution is concentrated. It measures the spectral shape.
Spectral Flux is the squared difference between the nor-
malized magnitudes of successive spectral distributions. It
measures the amount of local spectral change. Zero Cross-
ings is the number of time domain zero crossings of the
signal. It measures noisiness of the signal. Low Energy is
the percentage of frames that have energy less than the av-
erage energy over the whole signal. It measures amplitude
distribution of the signal.
• DWCH (DaubechiesWavelet Coefficients Histogram): To
extract DWCH features, the Db8 filter with seven levels
of decomposition is applied to three seconds of sound
signals. After the decomposition, the histogram of the
wavelet coefficients is computed at each subband. Then
the first three moments of a histogram is used [10] to ap-
proximate the probability distribution at each subband. In
addition, the subband energy is computed at each sub-
band, which is defined as the mean of the absolute value
of coefficients, for each subband. More details on DWCH
feature extraction can be found in [45].
6.3.2 Experimental results on three artists
The first experiment is performed on a dataset consisting
of 106 songs from 11 albums by three artists (4 albums
from Elton John, 4 albums from Joni Mitchell and 3 al-
bums from Led Zeppelin). The sound recording and the
lyrics from them are obtained and we use the method de-
scribed in Sect. 6.3.1 to extract the text-based and content-
based features. Four different feature sets: three text-based
feature sets and one content-based feature set are obtained,
Table 5 Experimental results
on three artists. The clustering
performance was tested using
various combinations of feature
sets.Multiple clusterings row
shows the purity result of
combining multiple clusterings
and all the other rows show the
results of clustering on the
corresponding feature
combinations. The results were
obtained by averaging ten trials
Feature set(s) Purity
1 0.53
2 0.48
3 0.52
4 0.54
1+ 2 0.64
1+ 3 0.61
1+ 4 0.56
2+ 3 0.58
2+ 4 0.55
3+ 4 0.58
1+ 2+ 3 0.68
1+ 2+ 4 0.66
2+ 3+ 4 0.60
1+ 2+ 3+ 4 0.688
Multiple clusterings 0.698
and we label them as Feature set 1 (Bag-of-words), 2 (Part-
of-speech statistics), 3 ( Lexical/orthographic features) and
4 (Content-based features) respectively. The experiment is
to cluster the music. The artists’ names are used as labels to
evaluate the performance of clustering.
Table 5 presents the experimental results on three artists
including both the clustering performances on different fea-
ture combinations and the result of combining multiple clus-
terings. For multiple clusterings, we first performed cluster-
ing on each of the four feature sets and then combine the
clusterings results. The clustering algorithms were imple-
mented using K-means approaches [36]. It can be observed
that the clustering performance increases as more features
are added in. On the combination of all the four feature
sets, the clustering purity is the highest among all the feature
combinations. This indicates that all the feature sets seem to
be useful for identifying the music styles. The result of com-
bining clusterings is slightly better than that on the combina-
tion of all four feature sets. The improvement is partially due
to the ability of the multiple clusterings algorithm to learn
the correlation structure among different feature sets. In ad-
dition, combining multiple clustering can also overcome the
instability inherent in the clustering problem.
6.4 Experiments on artist similarity
The second experiment is performed on the dataset consist-
ing of 570 songs from 56 albums of a total of 43 artists. The
sound recording and the lyrics from them were obtained. In
this experiment, we try to identify the artist similarities.
6.4.1 Similarity ground truth
Although we believe that the degree at which a listener finds
a piece of music similar to another is influenced by the lis-
On Combining multiple clusterings: an overview and a new perspective 217
Fig. 4 Artist similarity
dendrogram
tener’s cultural and music backgrounds and by the listener’s
state of mind, to make our investigation more plausible we
choose to use similarity information available at All Music
Guide (www.allmusic.com) as the ground truth, assuming
that this information is reflection of multiple individual lis-
teners. By examining All Music Guide artist pages, if the
name of an artist X appears on the list of artists similar to Y,
it is considered that X is similar to Y. The similarity graph of
these nodes are shown in Fig. 3. We did not agree completely
with the artist similarity thus obtained but nonetheless used
it as the ground truth.
6.4.2 Results analysis
We first group songs into different clusters using both lyrics
and content data via the approach of combining multiple
clusterings (as described in Sect. 6.3.2). Then we define the
similarity between two artists as the ratio of the number of
songs in the same cluster to the total number of songs. A hi-
erarchical clustering scheme is then used to generate a sim-
ilarity dendrogram of all the artists. Figure 4 shows the gen-
erated dendrogram from our experiment.
We can derive three clusters from the dendrogram as
listed in Table 6. In the similarity graph of Fig. 3, if the
name of an artist X appears on the list of artists similar to Y,
it is considered that X is similar to Y. The cluster structures
listed in Table 6 indicate that our approach of combining
multiple clusterings correctly learned relationships revealed
in the artist similarity graph. For example, Fleetwood Mac
and Hootie (as well as Sheryl Crow, Hootie) were correctly
clustered into the same class. AC/DC and Deep Purple were
classified as the same class with Elton John. However, there
were inconsistencies between the cluster structures gener-
ated by the dendrogram and the similarity graph. Using an-
alytical similarity measures to obtain the ground truth about
artist similarity, thereby improving upon the data provided
by web information resources, will be our future goal. In
conclusion, we can conclude from these experiments that
artist similarity can be efficiently learned using multiple data
sources.
218 T. Li et al.
Table 6 Cluster memberships
Clusters Members
No.1 {Jackson Browne, Genesis, Suzanne Vega,
Melsissa Etheridgem The Rolling Stones,
Carly Simon, Utopia, Hootie}
No. 2 {ZZ Top, Elton John, James Taylor,
Led Zepplin, Sheryl Crow, Fleetwood Mac,
AC/DC, Ricky Lee Jones, Black Sabbath,
Sting, Deep Purple}
No. 3 {Jim Hendrix, Yes, Joni Mitchell
Eagles, Steely Dan, Peter Garbiel
Derek & The Dominos}
7 Conclusions
Many application problems can be reduced to the problem of
combining multiple clusterings. This paper provides a uni-
fied view of the problem of combining multiple clusterings
by exploring the connections among various criteria. In ad-
dition, it shows the equivalence between the two different
perspectives for combining multiple clusterings: consensus
partition and categorical clustering. A novel method based
on the matrix theory for determining the final clustering is
also presented. Experiment results show the effectiveness of
combining multiple clusterings.
Acknowledgements The authors want to thank Mr. Chengliang
Zhang for helping with the experiments in Sect. 6.4. We are also grate-
ful to the conference reviewers for their helpful comments and sug-
gestions. T. Li is partially supported by NSF grants IIS-0546280 and
DMS-0844513. This work is also partly supported by the Open Re-
search Fund of Lab. of Spatial Data Mining and Information Sharing
of Ministry of Education of China at Fuzhou University.
References
1. Al-Razgan M, Domeniconi C (2006) Weighted clustering ensem-
bles. In: Proceedings of 2006 SIAM international conference on
data mining (SDM 2006)
2. Alhajj R, KayaM (2008) Multi-objective genetic algorithms based
automated clustering for fuzzy association rules mining. J Intell
Inf Syst 31:243–264
3. Fred ALN, Jain A (2003) Robust data clustering. In: Proceedings
of IEEE computer society conference on computer vision and pat-
tern recognition
4. Arabie P, Carroll JD, Desarbo W (1987) Three-way scaling and
clustering. Sage, Thousand Oaks
5. Argamon S, Saric M, Stein SS (2003) Style mining of electronic
messages for multiple authorship discrimination: first results. In:
Proceedings of the ninth ACM SIGKDD international conference
on Knowledge discovery and data mining. ACM, New York, pp
475–480
6. Bauer E, Kohavi R (1999) An empirical comparison of voting
classification algorithms: bagging, boosting, and variants. Mach
Learn 36:105–139
7. Bill E (1994) Some advances in transformation-based parts of
speech tagging. In: Proceedings of the twelfth national conference
on artificial intelligence, vol. 1. American Association for Artifi-
cial Intelligence, Menlo Park, pp 722–727
8. Brucker P (1977) On the complexity of clustering problems. In:
Optimization and operations research. Springer, NewYork, pp 45–
54
9. Cohen J (1960) A coefficient of agreement for nominal scales.
Educ Psychol Meas 20:37–46
10. David A, Panchanathan S (2000) Wavelet-histogram method for
face recognition. J Electron Imaging 9:217–225
11. Day WHE (1986) Foreword: Comparison and consensus of clas-
sifications. J Classif 3:183–185
12. de Souto MCP, de Araujo DSA, da Silva BL (2006) Cluster en-
semble for gene expression microarray data: accuracy and diver-
sity. In: Proceedings of the 2006 international joint conference on
neural networks
13. Duran BS, Odell PL (1974) Cluster analysis: a survey. Springer,
New York
14. Everitt BS (1987) Introduction to optimization methods and their
application in statistics. Chapman and Hall, London
15. Ferligoj A (1992) Direct multicriteria clustering algorithm. J Clas-
sif 9:43–61
16. Ferligoj A, Batagelj V (1983) Some types of clustering with rela-
tional constraints. Psychometrika 48:541–552
17. Fern X, Lin W (2008) Cluster ensemble selection. In: Proceed-
ings of 2008 SIAM international conference on data mining (SDM
2008)
18. Fern XZ, Brodley CE (2003) Random projection for high dimen-
sional data clustering: a cluster ensemble approach. In: Proceed-
ings of the twentieth international conference on machine learning
(ICML 2003). Morgan Kaufmann, San Mateo, pp 186–193
19. Filkov V, Skiena S (2004) Integrating microarray data by consen-
sus clustering. Int J Artif Intell Tools, pp 863–880
20. Gionis A, Mannila H, Tsaparas P (2005) Clustering aggregation.
In: ICDE, pp 341–352
21. Golub GH, Loan CFV (1991) Matrix computations. The Johns
Hopkins University Press, Baltimore
22. Goodman LA, Kruskal WH (1954) Measures of associations for
cross classification. J Am Stat Assoc 49:732–764
23. Gordan AD, Vichi M (1998) Partitions of partitions. J Classif
15:265–285
24. Gordan AD, Vichi M (2002) Obtaining partitions of a set of hard
or fuzzy partitions. Classification, clustering and data analysis: re-
cent advances and applications. Springer, Berlin, pp 75–79
25. Gyllenberg M, Koski T, Verlaan M (1997) Classification of binary
vectors by stochastic complexity. J Multivar Anal 63:47–72
26. H J, Knowles J (2004) Evolutionary multiobjective clustering.
In: Proceedings of the eighth international conference on parallel
problem solving from nature. Springer, New York, pp 1081–1091
27. Hadjitodorov ST, Kuncheva LI, Todorova LP (2006) Moderate di-
versity for better cluster ensembles. Inform Fus 7:264–275
28. Halkidi M, Batistakis Y, Vazirgiannis M (2001) On clustering val-
idation techniques. J Intell Inf Syst 17:107–145
29. Hartigan JA (1975) Clustering algorithms. Wiley, New York
30. Hu X, Yoo I, Zhang X, Nanavati P, Das D (2006) Wavelet trans-
formation and cluster ensemble for gene expression analysis. Int J
Bioinform Res Appl 1:447–460
31. Hubert LJ, Arabie P (1985) Comparing partitions. J Classif 2:193–
218
32. Hubert LJ, Baker FB (1978) Evaluating the conformity of socio-
metric measurements. Psychometrika 43:31–41
33. Jain AK, Dubes RC (1988) Algorithms for clustering data. Pren-
tice Hall, New York
34. Kargupta H, Huang W, Sivakumar K, Johnson EL (2001) Dis-
tributed clustering using collective principal component analysis.
Knowl Inf Syst 3:422–448
On Combining multiple clusterings: an overview and a new perspective 219
35. Katz L, Powell JH (1953) A proposed index of the conformity of
one sociometric measurement to another. Psychometrika 18:249–
256
36. Kaufman L, Rousseeuw PJ (1990) Finding groups in data: an in-
troduction to cluster analysis. Wiley, New York
37. Law MHC, Topchy AP, Jain AK (2004) Multiobjective data clus-
tering. In: Proceedings of the IEEE computer society conference
on computer vision and pattern recognition, pp 424–430
38. Li T (2005) A general model for clustering binary data. In:
KDD’05: Proceeding of the eleventh ACM SIGKDD international
conference on knowledge discovery in data mining, pp 188–197
39. Li T (2006) A unified view on clustering binary data. Mach Learn
62:199–215
40. Li T, Ding C (2008) Weighted consensus clustering. In: Proceed-
ings of 2008 SIAM international conference on data mining (SDM
2008)
41. Li T, Ding C, Jordan MI (2007) Solving consensus and semi-
supervised clustering problems using nonnegative matrix factor-
ization. In: Proceedings of 2007 IEEE international conference on
data mining (ICDM 2007)
42. Li T, Ma S (2004) IFD: iterative feature and data clustering. In:
Proceedings of the 2004 SIAM international conference on data
mining (SDM 2004). SIAM, Philadelphia
43. Li T, Ma S, Ogihara M (2004a) Document clustering via adaptive
subspace iteration. In: Proceedings of twenty-seventh annual in-
ternational ACM SIGIR conference on research and development
in information retrieval (SIGIR 2004), pp 218–225
44. Li T, Ogihara M (2004) Music artist style identification by semi-
supervised learning from both lyrics and content. In: Proceedings
of the ACM conference on multimedia
45. Li T, Ogihara M, Li Q (2003a) A comparative study on content-
based music genre classification. In: SIGIR’03. ACM, New York,
pp 282–289
46. Li T, Ogihara M, Ma S (2004b) On combining multiple cluster-
ings. In: CIKM, pp 294–303
47. Li T, Zhu S, Ogihara M (2003b) Algorithms for clustering high
dimensional and distributed data. Intell Data Anal J 7:305–326
48. Matake N, Hiroyasu T, Miki M, Senda T (2007) Multiobjective
clustering with automatic k-determination for large-scale data. In:
GECCO’07: Proceedings of the 9th annual conference on genetic
and evolutionary computation. ACM, New York, pp 861–868
49. Meila M (2003) Comparing clusterings by the variation of infor-
mation. In: Proceedings of learning theory and kernel machines:
16th annual conference on learning theory and 7th kernel work-
shop, COLT/Kernel 2003. Springer, Berlin, pp 173–187
50. Messatfa H (1992) An algorithm to maximize the agreement.
J Classif 9:5–15
51. Mirkin B (20001) Reinterpreting the category utility function.
Mach Learn 45:219–228
52. Mitton R (1987) Spelling checkers, spelling correctors and the
misspellings of poor spellers. Inf Process Manag 23:103–209
53. Monti S, Tamayo P, Mesirov J, Gloub T (2003) Consensus cluster-
ing: a resampling-based method for class discovery and visualiza-
tion of gene expression microarray data. Mach Learn J 52:91–118
54. Moret BM (1998) The theory of computation. Addison-Wesley,
Reading
55. Ozyer T, Alhajj R (2008a) Deciding on number of clusters by
multi-objective optimization and validity analysis. J Multi-Valued
Log Soft Comput 14:457–474
56. Ozyer T, Alhajj R (2008b) Parallel clustering of high dimensional
data by integrating multi-objective genetic algorithm with divide
and conquer. Appl Intell, to appear, 2009
57. Ellis PWD, Whitman B, Berenzweig A, Lawrence S (2002) The
quest for ground truth in musical artist similarity. In: Proceedings
of 3rd international conference on music information retrieval,
pp 170–177
58. Rosenberg S, Kim MP (1975) The method of sorting as a data
gathering procedure in multivariate research. Multivar Behav Res
10:489–502
59. Stamatatos E, Fakotakis N, Kokkinakis G (2000) Automatic text
categorization in terms of genre and author. Comput Linguist
26:471–496
60. Strehl A, Ghosh J (2003) Cluster ensembles—a knowledge reuse
framework for combining multiple partitions. J Mach Lear Res
3:583–617
61. Tweedie FJ, Baayen RH (1998) How variable may a constant
be? Measure of lexical richness in perspective. Comput Humanit
32:323–352
62. Tzanetakis G, Cook P (2002) Musical genre classification of audio
signals. IEEE Trans Speech Audio Process 10
63. Vichi M (1999) One-mode classification of a three-way data ma-
trix. J Classif 16:27–44
64. Zhao Y, Karypis G (2001) Criterion functions for document clus-
tering: Experiments and analysis. Technical Report, Department
of Computer Science, University of Minnesota
