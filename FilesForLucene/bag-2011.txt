2011 International Conference on Image Information Processing (ICIIP 2011)
Robust Binarization of Degraded Documents using Adaptive-cum-Interpolative
Thresholding in a Multi-scale Framework
Soumen Bag, Partha Bhowmick
Computer Sc. & Engg. Department
IIT Kharagpur, India
{soumen, pb}@cse.iitkgp.ernet.in
Priyaranjan Behera
Computer Sc. & Engg. Department
NIT Rourkela, India
priyaranjanb-cs21@nitrkl.ac.in
Gaurav Harit
Computer Sc. & Engg. Department
IIT Rajasthan, India
gharit@iitj.ac.in
Abstract—A novel technique for binarization of degraded
documents is proposed. It works in a multi-scale framework
with an adaptive-cum-interpolative thresholding as a modifica-
tion of Otsu’s method. Instead of computing a global threshold
value for an input document image, it computes the local
threshold values for a small set of grid points by observing the
intensity pattern of the pixels lying in the concerned grid cells.
Thresholds estimated for these grid points are used, in turn, to
compute the threshold values of all the remaining pixels using
a fast-yet-efficient interpolation procedure. To handle noises
in degraded images, this grid-based adaptive thresholding is
applied in successively reducing scales to obtain the near-
optimal binarization as a set of connected components. After
a post-processing with these connected components, we get
the final output. Exhaustive experimentation has been carried
out with benchmark datasets including George Washington
corpus of handwritten documents, and also with our own
datasets. When compared to other methods, the proposed
method is found to be robust and appreciably better, as tested
by conventional evaluation schemes.
Keywords-Adaptive thresholding; Degraded documents; Doc-
ument image binarization; Grid-based approach; Multi-scale
framework.
I. INTRODUCTION
Document image binarization is often performed in the
preprocessing stage of different document image processing
related applications such as optical character recognition
(OCR), document image retrieval, word spotting, document
image indexing, etc. Binarization is always a great challenge
in all image process fields, especially in the process of
document images where binarization result can directly
affect the optical character recognition or word spotting
rates directly. It converts a gray-scale document image
into a binary document image and accordingly facilitates
the ensuing tasks such as document skew estimation and
document layout analysis. As more and more text documents
are scanned, fast and accurate document image binarization
is becoming increasingly important.
Though document image binarization has been studied for
many years [1, 2], the strategy for thresholding in case of
degraded document images is not yet a fully solved problem.
This is due to the difficulty in modeling different types of
document degradation such as uneven illumination, image
contrast variation, smear, marginal noise and differential
fading that exist within many document images, as illustrated
in Figure 1.
Image binarization techniques, in principle, can be broadly
classified into two categories: global thresholding and lo-
cal thresholding. Global thresholding is based on a single
threshold value for the binarization of an entire image [6–8].
This technique works well for good quality images but fails
for degraded images containing different types of noises.
To handle these degraded documents, local thresholding
is used [9, 10]. In this technique, threshold values are
computed in an adaptive manner by inspecting each local
neighborhood.
As far as the problem in degraded documents is con-
cerned, Leedham et al. [11] have compared some traditional
methods on degraded document images. Gatos et al. [12]
have used different well-known strategies to design an adap-
tive method for low-quality historical documents. Kavalliera-
tou and Stamatatos [13] have proposed a hybrid binarization
approach for improving the quality of old documents using
a combination of global and local thresholding. To restore
weak connections and strokes of character images in a
degraded document, Moghaddam et al. [14] have proposed a
multi-scale binarization framework. But most of these global
and local thresholding methods suffer from the problem of
discontinuity in character strokes in the document, which
pose serious problems during recognition.
To address the above problem, we propose an adaptive-
cum-interpolative image binarization method for degraded
documents. In this method, a multi-scale framework is added
to an adaptive version of Otsu’s method [6]. To convert
Otsu’s method to an adaptive model, instead of computing
the global threshold value for the whole image, we compute
the local threshold value for each pixel by observing the
intensity behavior of its neighbor pixels. But this adaptive
method is computationally very expensive. Hence, to reduce
the computational time, we use a grid-based approach with
a particular scale size. To handle noises in different scales,
we use this grid-based adaptive approach for successively
reducing scales.
The rest of this paper is organized as follows. Sec-
978-1-61284-861-7/11/$26.00 c©2011 IEEE
Proceedings of the 2011 International Conference on Image Information Processing (ICIIP 2011)
2011 International Conference on Image Information Processing (ICIIP 2011)
(a) (b) (c) (d)
Figure 1. Examples of degraded document images on which our algorithm has been tested. (a) George Washington’s writing [3]; (b) Bengali literature;
(c) DIBCO-2009 dataset [4]; (d) DIBCO-2010 dataset [5].
tion II describes the proposed document image binarization
methodology. Section III contains some experimental results
and their comparison with results of other existing binariza-
tion methods. Concluding remarks and future work are given
in Section IV.
II. PROPOSED METHOD
The proposed document image binarization has four-stage
improvements over Otsu’s method, as explained below.
A. Adaptive Model of Otsu’s Method
There exist several adaptive thresholding models of bina-
rization [9, 10,14]. However, they are not specially designed
for degraded documents. In our work, to convert Otsu’s
method [6] to an adaptive model, we compute the local
threshold value for each pixel in a particular scale size by
observing the intensity behavior of its neighbor pixels. The
adaptive threshold used in our method is given by
Ta(p) = f
(
|TI − To(p)|
δ
− 1
)
(T ′o(p)− To(p)) + To(p)
(1)
where, I is the input image; f(·) is the unit step function;
TI = arg
(
max
(
σ2W
σ2
))
; To(p) = k arg
(
max
(
σ2W (p)
σ2(p)
))
;
T ′o(p) = k
′ arg
(
max
(
σ′2W (p)
σ′2(p)
))
; σ2W (σ
′2
W ) and σ
2 (σ′2)
represent the respective variances for the computational
window W in I (I ′) and the entire image, I (I ′); k and
k′ are the threshold multipliers; and T ′o(p) denotes To(p) at
p for the inverse image I ′ = 1 − I . The unit step function
is defined as
f(t− t0) = 1, if t ≥ t0;
= 0, otherwise.
The parameter δ is set to 0.1 and is a measure of the
deviation of the local threshold, To(p), from the global one,
TI . Like other adaptive methods, we use two parameters,
namely k and k′, as threshold multipliers for To(p) and
T ′o(p) respectively, which makes the algorithm versatile for
different types of images. When two bands (i.e., both fore-
ground and background) are present in the region of interest
defined by W , To(p) serves as the threshold value; and when
only background is present, T ′o(p) is considered. We use
a higher value of k (= 2) to discard noises and a lower
value of k′ (= 0.6) to capture foreground with low contrast.
This adaptive approach is computationally expensive, but it
performs better than the original Otsu’s method for degraded
images, as found in our experimentation (Sec. III).
B. Grid-based Interpolation
To reduce the computational time, we use a grid-based
approach with a scale size, s, and grid size, gs, where gs =
(s−1)/2. The underlying grid G is defined as a set of equi-
spaced rows and equi-spaced columns so that the distance
between two consecutive rows (columns) is gs. A grid point
q(i, j) ∈ G is the point of intersection between ith row and
jth column of G. The point q of G corresponds to (gsi, gsj)
in I . The square region of I with the four grid points, namely
q1 := q(i, j), q2 := q(i + 1, j), q3 := q(i + 1, j + 1), and
q4 := q(i, j+1), as vertices, is defined as the cell c(i, j), as
shown in Figure 2. We first compute local threshold value
Ta(qk) of each grid point qk(1 ≤ k ≤ 4) by considering
the computational window W of size gs × gs centered at
qk, using Eqn. 1. Then the threshold value for each other
pixel p ∈ c(i, j) is computed by interpolating the threshold
values of {qk : 1 ≤ k ≤ 4}. The interpolated threshold at p
is estimated as the weighted mean of {Ta(qk) : 1 ≤ k ≤ 4},
where the weights {wk : 1 ≤ k ≤ 4} are assigned as the
reciprocals of corresponding Manhattan distances {dk : 1 ≤
k ≤ 4} of {qk : 1 ≤ k ≤ 4} from p. The threshold value
T̃ (p) is given by
T̃ (p) =
4∑
k=1
1
dk
Ta(qk)
4∑
k=1
1
dk
. (2)
For each grid point q, the threshold T̃ (q) is assigned as
Ta(q). Subsequently, the binarized image Ib is obtained as
follows (1 = foreground, 0 = background).
Ib(p) =

1 if

p ∈ G and I(p) ≥ Ta(p)
or
p ∈ I rG and I(p) ≥ T̃ (p)
0 if

p ∈ G and I(p) < Ta(p)
or
p ∈ I rG and I(p) < T̃ (p)
(3)
C. Multi-scale Framework
To make our algorithm more effective and to handle noises
at different scales, we use the grid-based interpolation in a
multi-scale framework. The steps are as follows.
1) We start with a high scale value s(= min{wI , hI}),
wI and hI being the respective width and height
Proceedings of the 2011 International Conference on Image Information Processing (ICIIP 2011)
2011 International Conference on Image Information Processing (ICIIP 2011)
p
q1
q2 q3
q4i
i+ 1
j j + 1
Figure 2. Interpolation of threshold at p in a cell ci based on thresholds
at the four grid points defining ci.
of I; and continue up to s = s (set as s24 in our
experimentation). For each successive iteration, the
scale is reduced by a factor of 2, and the input of the
new iteration is the output of the previous iteration.
2) For each iteration, we adopt the threshold computation
as used in the grid-based approach and compute the
threshold T̃ with the current scale value.
3) For any pixel p,
• if I(p) > T̃ (p) + 0.2, then I(p) = 1.
• if I(p) < T̃ (p), then I(p) = 0.
• if T̃ (p) ≤ I(p) ≤ T̃ (p) + 0.2, then I(p) remains
unchanged (i.e, p remains unclassified).
4) Repeat until the scale value reaches s.
Finally, we get a binarized image Ib. If some pixel p re-
mains unclassified through all iterations, then it is classified
as a background pixel (i.e., Ib(p) = 0).
D. Post-processing
We observed that due to very low contrast variation in be-
tween the background and the foreground of a degraded doc-
ument, the binarized output often suffers from unexpected
discontinuity in the strokes of character images (Figure 3).
Such discontinuities affect seriously on subsequent character
recognition. To overcome this, we apply a post-processing
scheme on the binarized output. The major steps are as
follows.
1) Detect all the connected components in the binarized
image Ib. Now, sort them based on their sizes in
descending order and take the first two-third for fur-
ther processing. Let C = {C1, C2, . . . , Cn} be this
(ordered) set. Our objective is to maintain the desired
connectivity for large components. We neglect the last
one-third components having smaller size in order
to avoid unwanted connectivity among the isolated
components, which are possibly placed singly in the
document (e.g., the dot above the character ‘i’ or ‘j’
in the English alphabet).
Figure 3. Improvement of test result using post-processing. Top: Input
sample (cropped). Mid: Disconnected strokes (encircled in red). Bot-
tom: Improved result after post-processing.
2) Take each component Ci from the ordered set C and
derive its boundary pixels.
3) For each boundary pixel pi ∈ Ci, take a 3×3 window
Wi centered at pi on the original image I . If there
lies a pixel pj in Wi such that pj /∈ Ci and I(pj) >
1
2 T̃ (pi), then pj is included in Ci; and the process is
repeated with a new window Wj centered at pj on I
to include more pixels, if any, in Ci.
Figure 3 shows a portion of improved output after post-
processing.
III. EXPERIMENTAL RESULTS AND DISCUSSION
We have implemented the proposed method and the four
other methods in C (Intel Core 2 Duo CPU E4500 2.20
GHz, 1GB RAM, Fedora 10 Linux 2008). We have tested
the proposed method with different images taken from
George Washington corpus [3] of handwritten documents
and scanned documents of Bengali literature. Figure 4 shows
few samples of experimental results. We observe that our
method performs well for these types of degraded historical
images and maintain the proper connectivity for different
strokes of alphabets in the manuscripts.
A. Performance Evaluation
The performance evaluation is done using different eval-
uation measurement techniques, such as Recall, Precision,
F-measure (FM), Peak Signal-to-Noise Ratio (PSNR), Neg-
ative Rate Metric (NRM), and Misclassification Penalty
Metric (MPM). These are the measures used in various
evaluation models of document image binarization, e.g.,
DIBCO [15]. These measures are defined as follows.
FM =
2× Recall × Precision
Recall + Precision
(4)
Proceedings of the 2011 International Conference on Image Information Processing (ICIIP 2011)
2011 International Conference on Image Information Processing (ICIIP 2011)
Figure 4. Test results by the proposed method on different degraded document images. Top: Input image; Bottom: Binarized image.
where, Recall =
TP
TP + FN
and Precision =
TP
TP + FP
.
TP, FP, and FN denote the respective counts of true positive,
false positive, and false negative pixels required to compute
Recall and Precision metrics. Recall and Precision metrics
have values lying in [0, 1]. As these metrics approach 1,
the results get better. A higher value of FM indicates the
efficiency of correct binarization.
PSNR = 10 log
(
c2
MSE
)
(5)
where, MSE is the mean square error and c (= 1) is a
constant.
NRM =
NFN
NFN+NTP
+ NFPNFP+NTN
2
(6)
where, NTP, NFP, NTN, and NFN represent the number
of true positives, false positives, true negatives, and false
negatives respectively. NRM measures pixel mismatch rate
between the ground-truth image and result image.
MPM =
∑NFN
i=1 d
i
FN +
∑NFP
j=1 d
j
FP
2D
(7)
Proceedings of the 2011 International Conference on Image Information Processing (ICIIP 2011)
2011 International Conference on Image Information Processing (ICIIP 2011)
where, diFN and d
j
FP represent the distance of the ith false
negative and the jth false positive pixel from the contour
of the ground-truth segmentation. The normalization factor
D is the sum over all the pixel-to-contour distances of the
ground truth object. The metric MPM indicates how well the
result image represents the contour of ground truth image.
B. Comparison among Different Methods
The proposed method has been compared with the follow-
ing four well-known methods: Otsu’s global thresholding
method [6], Niblack’s and Sauvola’s adaptive methods [9,
10], and Moghaddam’s adaptive multi-scale method [14].
Figure 5 shows a output-wise comparison of these methods
vis-a-vis our proposed method on George Washington’s
handwritten document (a sample shown in Figure 1(a)).
We see that the overall performance of our method is
usually better than those of other methods, since our method
maintains proper connectivities in the strokes of character
images in a badly degraded document.
The performance evaluation of these four methods and our
proposed method has been done using DIBCO-2009 [4] and
DIBCO-2010 [5] datasets, and the average evaluation results
are shown in Table I. As evident from this table, our pro-
posed method achieves highest scores in F-Measure, PSNR,
NRM, and MPM for DIBCO-2009 dataset. For DIBCO-
2010 dataset, its PSNR is slightly lower than Moghaddam’s
method. Clearly, our proposed method guarantees higher
precision and better stroke connectivity for degraded doc-
ument images.
A comparison of average computational time taken by
these methods is shown in Table II. For the experimental
purpose, we have used DIBCO-2009 and 2010 datasets and
two document images from George Washington’s writing
(G-W-1.jpg) and Bengali literature (B-L-1.jpg). Al-
though Otsu’s method is computationally inexpensive, it
shows a poor performance due to global thresholding.
IV. CONCLUSION
We have proposed a novel adaptive-cum-interpolative bi-
narization method for degraded document images. The pro-
posed method is an adaptive version of Otsu’s method and is
used in different scales to handle different type of noises. Ex-
periments are done on different degraded document images
taken from George Washington corpus of handwritten doc-
uments and Bengali literature. An evaluation study among
different well-known binarization methods on DIBCO-2009
and DIBCO-2010 datasets with their ground truths shows
that our method is at par or better with the existing methods.
In future, we would use this binarization as a part of an
optical character recognition system for degraded documents
of historical importance.
Table I
EVALUATION MEASUREMENTS FOR DIFFERENT BINARIZATION
METHODS.
Methods Measurement DIBCO-2009 DIBCO-2010
techniques
Recall 0.94 0.30
Precision 0.74 0.86
Otsu’s F-Measure (%) 69.77 38.29
PSNR 15.34 13.45
NRM 0.09 0.70
MPM 0.50 0.18
Recall 0.98 0.95
Precision 0.36 0.21
Niblack’s F-Measure (%) 48.50 32.22
PSNR 7.20 5.82
NRM 0.15 0.19
MPM 0.49 0.68
Recall 0.61 0.31
Precision 0.88 0.81
Sauvola’s F-Measure (%) 68.84 40.35
PSNR 14.11 13.70
NRM 0.39 0.69
MPM 0.28 0.46
Recall 0.71 0.77
Precision 0.83 0.74
Moghaddam’s F-Measure (%) 76.53 69.39
PSNR 13.79 15.39
NRM 0.25 0.25
MPM 0.14 0.14
Recall 0.70 0.75
Precision 0.95 0.83
Proposed method F-Measure (%) 81.61 78.79
PSNR 15.87 14.49
NRM 0.08 0.12
MPM 0.09 0.11
Table II
AVERAGE COMPUTATIONAL TIMES (IN SECONDS) TAKEN BY DIFFERENT
BINARIZATION METHODS.
Methods DIBCO- DIBCO- G-W-1.jpg B-L-1.jpg
2009 2010
Otsu’s 0.04 0.07 0.39 0.31
Niblack’s 88.90 113.29 910.70 595.09
Sauvola’s 17.76 22.01 21.39 15.46
Moghaddam’s 4.95 7.60 28.95 19.65
Proposed method 1.75 1.99 18.53 12.46
REFERENCES
[1] O. Trier and T. Taxt, “Evaluation of binarization methods for
document images,” IEEE Trans. PAMI, vol. 17, pp. 312–315,
1995.
[2] M. Sezgin and B. Sankur, “Survey over image thresholding
techniques and quantitative performance evaluation,” Journal
of Electronic Imaging, vol. 13, no. 1, pp. 146–165, 2004.
[3] The Library of Congress (http://memory.loc.gov/)
[4] http://www.iit.demokritos.gr/∼bgat/DIBCO2009/benchmark/
[5] http://users.iit.demokritos.gr/∼bgat/H-DIBCO2010/
benchmark/
[6] N. Otsu, “A threshold selection method from gray-level
histogram,” IEEE Trans. SMC, vol. 9, pp. 62–66, 1979.
[7] A. Brink, “Thresholding of digital images using two-
dimensional entropies,” Patt. Rec., vol. 25, no. 8, pp. 803–
808, 1992.
[8] Y. Solihin and C. Leedham, “Integral ratio: A new class of
global thresholding techniques for handwriting images,” IEEE
Trans. PAMI, vol. 21, pp. 761–768, 1999.
[9] W. Niblack, An Introduction to Image Processing. Engle-
wood Cliffs, NJ: Prentice-Hall, 1986.
Proceedings of the 2011 International Conference on Image Information Processing (ICIIP 2011)
2011 International Conference on Image Information Processing (ICIIP 2011)
(a) (b) (c)
(d) (e) (f)
Figure 5. Comparison among different binarization methods. (a) Input image; (b) Otsu’s method; (c) Niblack’s method; (d) Sauvola’s method;
(e) Moghaddam’s method; (f) Proposed method.
[10] J. Sauvola and M. Pietikainen, “Adaptive document image
binarization,” Patt. Rec., vol. 33, pp. 225–236, 2000.
[11] G. Leedham, S. Verma, A. Patankar, and V. Govindaraju,
“Separating text and background in degraded document im-
ages,” in IWFHR, 2002, pp. 244–249.
[12] B. Gatos, I. Pratikakis, and S. J. Perantonis, “An adaptive
binarization technique for low quality historical documents,”
in IAPR DAS, 2004, pp. 102–113.
[13] E. Kavallieratou and E. Stamatatos, “Improving the quality
of degraded document images,” in DIAL, 2006, pp. 340–349.
[14] R. F. Moghaddam and M. Cheriet, “A multi-scale framework
for adaptive binarization of degraded document images,” Patt.
Rec., vol. 42, pp. 2186–2198, 2010.
[15] B. Gatos, K. Ntirogiannis, and I. Pratikakis, “ICDAR 2009
document image binarization contest (DIBCO 2009),” in
ICDAR, 2009, pp. 1375–1382.
Acknowledgement
This work has been partly funded by the Project DRD
(Sponsor: Ministry of Communication & Technology, GOI)
at CSE Deptt., IIT Kharagpur.
Proceedings of the 2011 International Conference on Image Information Processing (ICIIP 2011)
