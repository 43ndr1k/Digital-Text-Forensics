Stylometry in Information Retrieval Systems 
Leyla Bilge 
Department of Computer Engineering 
Bilkent University 
Ankara, Turkey 
 
lbilge@cs.bilkent.edu.tr 
E. Büşra Çelikkaya 
Department of Computer Engineering 
Bilkent University 
Ankara, Turkey 
 
busra@cs.bilkent.edu.tr 
Kardelen Hatun 
Department of Computer Engineering 
Bilkent University 
Ankara, Turkey 
 
kardelen@cs.bilkent.edu.tr 
 
 
ABSTRACT 
Stylometry is the statistical analysis of literary and linguistic style. 
Style refers to the linguistic choices of authors that persist over 
their works, independently of content.  Authorship attribution 
using stylometric features is employed in many different areas 
such as identifying the author of an anonymous text, forensic 
author identification. Various measures of lexical richness are 
used in this process. However, there is no defined subset for 
discriminatory stylometric features.  In this paper we explore 
authorship identification in original texts and translations of these 
texts. We experimented on the works of five authors by using 
linguistic features like word length, function words, and 
punctuation marks. We used stylometry to analyze and classify 
the texts translated by the same translator but written by different 
authors and also texts translated by different translators but 
written by the same authors. 
Keywords 
stylometry, authorship attribution, stylometric features, author 
classification, translation classification. 
1. INTRODUCTION 
Authorship identification is the process of predicting the most 
likely author of a given text among a set of candidate authors by 
using stylometric features. This task is similar to a text 
categorization problem with multiple classes where the authors 
form the classes. However, text categorization is based on content 
related features whereas authorship identification uses stylistic 
and content free features. There are a growing number of studies 
in this field which indicate authorship attribution with a large 
number of candidate authors and limited training data is possible. 
Applications of authorship attribution include organization and 
retrieval of documents based on their writing style, plagiarism 
detection (e.g. college essays), deducing the writer of 
inappropriate communications that were sent anonymously or 
under a pseudonym (e.g. threatening or harassing e-mails), as well 
as resolving historical questions of unclear or disputed authorship.  
Stylistic variation depends on author preferences and competence, 
familiarity, genre, communicative context and expected 
characteristics of the intended audience. Modeling, representing 
and utilizing this variation is the business of stylistic analysis.  
Although there is a large collection of stylometric features (over 
1000), no subset of them is defined as highly discriminative and 
there is no consensus on methodology. Currently, authorship 
attribution studies are dominated by the use of lexical measures. 
Lexically based methods include vocabulary richness of the 
author, frequencies of occurrence of individual words. 
We focus on authorship identification for English texts and their 
Turkish translations. We employ lexically based features and try 
to associate the original works with their translations by using 
supervised and unsupervised classification. Our data set is 
composed of novels, short stories of five authors which are Jack 
London, Charles Dickens, J.K. Rowling, Mark Twain and Herman 
Melville. 
The rest of the paper is organized as follows:  Section 2 mentions 
about the related work on this area. Section 3 gives stylometry 
information and feature extraction. In Section 4 classifiers that are 
employed are discussed and Section 5 gives the results of 
experiments. 
2. RELATED WORK 
Currently, authorship attribution studies are dominated by the use 
of lexical measures. A subset of structural and stylometric features 
are used on a set of authors without consideration of author 
characteristics. Some of the generally used statistics are word 
length, syllables per word, sentence-length, sentence count, text 
length in words, and use of punctuation marks. 
A study to measure intra and inter-authorial faithfulness for 
forensic applications uses function words, high frequency words 
and percentage of hapax legomana which presents the percentage 
of words with frequency 1. The results of this study show that 
when all the style characteristics are combined texts by the same 
author and texts by different authors are easily differentiated. [1] 
According to Stamatatos, using sub-word units like character n-
grams can be very effective for capturing the nuances of an 
author’s style.  The most frequent n-grams of a text provide 
crucial information about the author’s stylistic choices on the 
lexical, syntactical, and structural level. In his paper about 
ensemble-based author identification 3-grams, 4-grams and 5-
grams are used as features. Performance results are ranging from 
70% to 90% when support vector classification is employed. [2] 
Another study about computer-based authorship attribution by 
Stamatatos, Fakotakis, and Kokkinakis does not employ any 
distributional lexical measure. Instead utilize analysis of the text 
by using an existing natural language processing tool. They 
analyzed unrestricted Modern Greek text by an NLP tool 
implementing Multiple Regression and Discriminant Analysis and 
achieved a success rate of 65% and 72%.[3] 
This paper differs from the ones mentioned above since we 
compare the original texts and their translations and see if the 
stylistic features of the authors are preserved.   
 
3. Feature Extraction 
Features over a document space are meant to reveal what could be 
called an author’s fingerprint, or an aspect of the text that an 
author cannot help but including due to their very nature. An open 
problem in the stylometry domain is a theoretical basis for feature 
selection, or even an explanation of why some features produce 
better results in discriminating than others. Until this problem is 
solved any feature extraction method must be based upon 
speculations on what might provide an insightful analysis. 
 
What is given though is that a document as a whole is too much 
for any classification algorithm to analyze. Documents can be an 
arbitrarily long set of words, and in some cases may contain 
things that are not words whatsoever. The purpose of feature 
extraction is to reduce the dimensionality of the problem into 
something tractable. Overall, there are three areas into which 
feature extraction is divided; lexical, semantic and syntactic. 
 
A lexical analysis of the text deals with information about the raw 
text - statistical information about character, word or sentence 
occurrences. A distinct advantage to the lexical approach is that it 
is largely language independent. Mean word length, mean 
sentence length, or n-gram frequencies are common instances of 
lexical features, none of which depend on the language of 
authorship. Lexical N-gram frequencies are subdivided into word 
or character n-grams. An n-gram over either of these categories is 
a grouping of N sequential tokens. For example, “the” is a 
character trigram, and “the book” is a word bi-gram. 
 
There is a particularly interesting subset of word unigram 
frequency analysis known as function word frequency analysis. 
Function words are words such as “for,” “into,” “has,” etc; in 
particular, words that are common in any English literary piece. 
An analysis of these words hence provides a context independent 
metric of vocabulary richness. An appealing aspect to function 
word frequencies is that the dimensionality is much reduced over 
an unsupervised word unigram frequency analysis. 
 
The second class of features that exists in texts is semantic, or 
relating to meaning in the language. Although this is the 
theoretically deepest of the three areas, it is the least used for 
several reasons. The main reason it is not used in automated 
classification is that semantic extraction from raw text is still 
altogether an unsolved problem. Although now there exists 
several very useful resources that can help with semantic 
extraction, a full semantic model will most likely not be available 
until significant progress is made in the field of grounding, or 
associating word meanings with a world model. Another problem 
with semantic features is that they are often context dependent, 
which is something that must be avoided in order to obtain an 
accurate fingerprint of an author independent of context. 
 
The third and final class of features is syntactic, or those relating 
to the grammatical structure of the sentence. Grammar of 
sentences is frequently reduced to terms called ‘parts of speech’ 
when analyzing syntax. Parts of speech are what most of us 
learned in remedial English classes as “nouns,” “verbs”, 
“conjunctions” and the like. There also exist more complicated 
methods for decomposing sentences into grammatical tokens, but 
for reasons similar to those above such decompositions are 
unfavorable since they would produce a feature space with raw 
dimensionality too high to be usable. Fortunately simple 
decompositions into parts of speech are tractable if the token set is 
chosen well. Even more fortunate is that there exist a wide variety 
of tools for automatically annotating raw text into parts of speech 
that have been published on the internet for free usage. 
 
We have used lexical analysis for feature extraction.  The 
elements of the feature sets are mean word length, mean sentence 
length, function words, punctuation characters and n-grams.  
Since we try to analyze the style of a text and its translation to 
Turkish, we have different function words and n-grams for each 
of the languages.   
• The function words used for English text analysis are; 
"and", "but", “a",  "i", "for",  "you", "who", "what",  
"because", "not", "the",  "of", "an",  "in", "to", "it",  "is", 
"was",  "that",  "he", "where", "this", "that", "at", "how", 
"yes", "no", "many", "much", "sister", "lady", "reply",  
"captain", "dear", "uncle", "miss", "mr", "aunt" and 
"till". 
• The function words used for Turkish is;  “ve", "ama", 
"bir",  "ben", "için",  "sen", "kim", "ne", "cünkü",  
“degil", "bu", "evet", "hayir", "cok", "o", "su", "nerede".  
• N-gram’s used for English are;  "tion", "ed", "ing", 
"ment", "'ll", "'s", "'ve", 
• N-gram’s used for Turkish are; "imsi", "da", "de", 
"mis", 
• Punctuation characters are, ",", ";", ":", "-", "?", "!", 
 
We have four feature set which are; 
1. [(function words)-(punctuation characters)-(word 
length)-(sentence length)] 
2. [(n-grams)-(punctuation characters)-(word length)-
(sentence length)] 
3. [(function words)-(n-grams)-(punctuation 
characters)-(word length)-(sentence length)] 
4. [(function words)] 
The first three feature sets are used for English and Turkish data 
when there are examined independently. The last feature set is a 
mix of English and Turkish data so the function words used are 
translation of each. The function words used for the forth type of 
analysis are in the table below. 
Table 1 – Mixed Feature Set Function Words 
English Turkish 
And Ve 
But Ama 
A Bir 
I Ben 
For İçin 
You Sen 
Who Kim 
What Ne 
Because  Çünkü 
Not Degil 
 
Before feature extraction, data set has to be collected. The data 
consists of some texts written in English and translated to Turkish.  
Examined authors and their books are listed in the table below: 
 
Table 2 - Books 
Authors Books in English Books in Turkish 
Charles 
Dickens 
Oliver Twist 
Cristmast Carol 
Tale of Two Cities 
Hard Times 
Oliver Twist 
Noel Şarkısı 
İki Şehrin Hikayesi 
Zor Zamanlar 
Jack 
London 
Martin Eden 
Iron Heel 
Martin Eden 
Demir Ökçe 
Mark 
Twain 
Eve’s Diary 
The Capitoline Venus 
McWilliams and 
Lightning 
Adventures of Tom 
Sawyer 
Havva’nın Günlüğü 
Kapitolün Venüsü 
McWilliams ve 
Yıldırım 
Tom Sawyer 
J.K. 
Rowling 
Harry Potter and the 
Order of the Phoenix 
Harry Potter and the 
Philosopher's Stone 
Harry Potter ve 
Zümrüdüanka 
Yoldaşlığı 
Harry Potter ve Felsefe 
Taşı 
Herman 
Melville 
Bartleby 
Benito Cereno 
Bartleby 
Benito Cereno 
 
We have implemented a program in C for the feature extraction. 
The feature extraction algorithm works as follows: 
1. Parse the English data and prepare different files for first 
three feature sets mentioned above. 
2. Parse the Turkish data and prepare different files for the first 
feature sets. 
3. Parse the English and Turkish data and examine it together.  
Prepare a file for each of the authors with respect to the last 
feature set. 
 
4. CLASSIFIERS 
4.1 Gaussian Classifier with Arbitrary 
Covariance 
Gaussian can be considered as a model where the feature vectors 
for a given class are continuous valued randomly corrupted 
versions of a single typical or prototype vector. To determine the 
discriminant function we apply the following formula: 
 
  
Where; 
  Σ  
 Σ  
 Σ ln|Σ | lnP w  
  
In this case of Gaussian classifier the decision boundaries are 
hyperquadrics.  
4.2 K-nearest Neighbourhood 
KNN works as follow, for pattern p we find the k nearest 
neighbors of p, and then we look at the classes of these neighbors. 
Then we assign a class number to pattern p according to the class 
which most of the neighbors belong to. Although this method of 
classification works best with features free of noise. Stylometric 
feature sets may contain a lot of noise such as redundant function 
words. Still it remains to b a useful classification method for many 
datasets. 
4.3 M-perceptron 
This is the extended case of the perceptron neural network 
algorithm for multiple class purposes. Perceptron is a very widely-
used basic algorithm which works as follows: 
Learning: 
• x(j) denotes the j-th item in the input vector  
• w(j) denotes the j-th item in the weight vector  
• y denotes the output from the neuron  
• δ denotes the expected output  
• α is a constant and 0 < α < 1  
 
The weights are updated as follows: 
    α δ    
• xi denotes the input vector for the i-th iteration  
• wi denotes the weight vector for the i-th iteration  
• yi denotes the output for the i-th iteration  
• denotes a 
training set of m iterations  
 
For each iteration the weight vector is updated as follows 
• For each (x,y) pair in 
 
• Pass (xi,yi,wi) to the update rule w(j)' = w(j) + α(δ − 
y)x(j)  
If the Dm is not linearly separable, then it is not guaranteed that 
perceptron algorithm will converge. 
. 
4.4 K-means 
K-means is an iterative squared error partitioning.K-means is 
computationally efficient and gives good results if the clusters are 
compact, hyperspherical in shape, and well-separated in the 
feature space. However, choosing k and choosing the initial 
partition are the main drawbacks of this algorithm. The value of k 
is often chosen empirically or by prior knowledge about the data. 
The initial partition is often chosen by generating k random points 
uniformly distributed within the range of the data, or by randomly 
selecting k points from the data. 
 
 
5. RESULTS AND DISCUSSION 
In the results section we only display the results we obtained with 
the most compact feature space. We tried with as many function 
words as possible and then reduce the number of words.  
The abbreviations for the authors are: 
Charles Dickens: CD 
Herman Melville: HM 
Mark Twain: MT 
J.K. Rowling: HP 
Jack London: JL 
5.1 Results 
5.1.1 Gaussian 
Table 3 – Feature Set 1 
Original CD HM MT HP JL 
CD 11 0 0 3 207 
HM 8 0 0 2 34 
MT 4 0 1 0 45 
HP 77 0 0 62 65 
JL 12 0 0 0 214 
Success 
rates 
0.0498 0 0.02 0.3039 0.9469 
 
Table 4 – Feature Set 1 
Translation CD HM MT HP JL 
CD 32 6 19 41 1 
HM 1 11 0 0 2 
MT 24 0 0 2 0 
HP 24 3 6 191 9 
JL 24 2 0 90 26 
Success 
rates 
0.3232 0.7857 0 0.8197 0.18
31 
 
 
 
Table 5 – Feature Set 2 
Original CD HM MT HP JL 
CD 11 0 0 3 207 
HM 8 0 0 2 34 
MT 4 0 1 0 45 
HP 77 0 0 62 65 
JL 12 0 0 0 214 
Success rates 0.0498 0 0.02 0.3039 0.9469 
 
 
Table 6 – Feature Set 2 
Translation CD HM MT HP JL 
CD 34 6 16 42 1 
HM 3 6 2 0 3 
MT 16 0 0 9 1 
HP 31 3 19 178 2 
JL 16 15 14 55 42 
Success rates 0.3434 0.4286 0 0.7639 0.2958 
 
 
Table 7 – Feature Set 3 
Original CD HM MT HP JL 
CD 11 0 0 3 207 
HM 8 0 0 2 34 
MT 4 0 1 0 45 
HP 77 0 0 62 65 
JL 12 0 0 0 214 
Success rates 0.0498 0 0.02 0.3039 0.9469 
 
 
Table 8 – Feature Set 3 
Translation CD HM MT HP JL 
CD 31 7 19 41 1 
HM 1 11 0 0 2 
MT 24 0 0 2 0 
HP 24 3 6 191 9 
JL 24 2 0 90 26 
Success rates 0.3131 0.7857 0 0.8197 0.1831 
 
 
 
Table 9 – Mixed Features Set 
Mixed CD HM MT HP JL 
CD 4 28 0 41 26 
HM 1 0 0 8 5 
MT 3 4 0 8 11 
HP 1 33 0 556 100 
JL 1 9 0 354 166 
Success rates 0.0404 0 0 0.8058 0.3132 
 
 
For the Gaussian case we see that there’s a very dramatic 
confusion between Jack London and Charles Dickens. Also 
Melville and Mark Twain are classified very poorly. The reason 
for this is we had few features for these classes.  
The confusion between Jack London and Charles Dickens might 
be because the stylometry features we used were not 
discriminative enough. For the original versions of the texts 
Gaussian classifier did not work. For the translations we had very 
good results for J.K Rowling but for other authors our success 
rates are low.  
5.1.2 KNN 
For k =3: 
The error rates are; 
Feature Set 1:  
Original: 0.3933 
Translation: 0.3949 
Feature Set 2: 
 Original: 0.4121 
 Translation: 0.4319 
Feature Set 3: 
 Original: 0.3638 
 Translation: 0.4047 
For the Gaussian case different feature sets did not change the 
results for the original texts. But for KNN we can see that for the 
same k value we get the lowest error rate with the feature set 1. 
5.1.3 M-perceptron 
 
This algorithm did not converge for our case, so we don’t provide 
any results for this case, although there are many studies where 
neural networks algorithms are used for stylometry. 
5.1.4 K-means 
 
Results for this clustering algorithm are consistent with the results 
of Gaussian classifier.  Three out of five clusters are successful 
and contain a majority of only one author’s works. However, 
remaining two clusters do not correspond to a distinct author’s 
works and they contain a mix of other author’s texts.  Success 
rates for k-means clustering algorithm are low, indicating that 
features are not distinctive enough to discriminate the author’s 
style. 
5.2 Discussion 
 
Stylometric analysis is a method that does not always work. 
Similarities between writing styles easily confuse stylometric 
features and cause bad classification.  
“Which features should be chosen?” is a very important question 
here, like in all classification problems. We chose lexical features 
to form our feature set. But complex texts like novels need much 
more to be represented stylometrically. 
We used k-means clustering to see if our data is consistent in 
itself, but clustering gave bad results for the original and 
translated texts. This suggests that an author’s style can change 
from text to text. If this style change happens to match with 
another author’s in term of our feature set, then we have 
confusion.  
We also wanted to see if the translations really match the original 
texts, for this we formed the mixed feature set. We saw that for 
Harry Potter series we have high success rate for classification. 
Also if we classify the translation of the Harry Potter series we 
have high success rate. So we can say that the translation of Harry 
Potter is stylometrically consistent  
With a larger dataset this study can be expanded, we only had 
limited number of text to proceed with our project. Unfortunately 
the dataset we had did not give as distinctive feature values to 
discriminate the classes from each other. 
'Bag of words' gives good results for classification, although it is 
time and space consuming. Stylometric analysis needs a pre-
processing which will give unique properties of the author. If we 
try to classify simply with function words, n-grams and ratios we 
don't get successful classification results. We chose function 
words as pre-defined words, which are referenced as most 
frequent words in a text; we used no author-specific words. Also 
use of language can be used to extract features and take its place 
as a highly distinctive element. It is very likely that if similar 
literature is classified confusion will be very high.   
 
6. REFERENCES 
[1] Pol, M.S., A Stylometry-Based Method to Measure Intra and 
Inter-Authorial faithfulness for Forensic Applications. 
Stylistic Analysis of Text For Information Access, 2005 
[2] Stamatatos, E., Ensemble-based Author Identification Using 
Character N-grams, In Proc. of the 3rd Int. Workshop on 
Text-based Information Retrieval (TIR'06),  
pp. 41-46, 2006  
[3] Stamatatos, E., Fakotakis, N., and Kokkinakis, G., 
Computer-Based Authorship Attribution without Lexical 
Measures, Computers and the Humanities, 35(2), pp. 193-
214, Kluwer, 2001  
[4] Goeuriot, L., Dubreuil, E., Daille, B., and Morin, E., 
Identifying Criteria to Automatically Distinguish between 
Scientific and Popular Science Registers. In Proceedings, 
28th SIGIR Workshop on Stylistic Analysis of Text for 
Information Access, Salvador, Brazil, 2005 
[5] Uzuner, O., Identifying Expression Fingerprints Using 
Linguistic Information, Ph.D. Thesis, 2005 
 
 
 
 
