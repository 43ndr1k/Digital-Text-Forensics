On the similarity of symbol frequency distributions with heavy tails
Martin Gerlach,1 Francesc Font-Clos,2, 3 and Eduardo G. Altmann1
1Max Planck Institute for the Physics of Complex Systems, D-01187 Dresden, Germany
2Centre de Recerca Matemàtica, Edifici C, Campus Bellaterra, E-08193 Barcelona, Spain.
3Departament de Matemàtiques, Facultat de Ciències,
Universitat Autònoma de Barcelona, E-08193 Barcelona, Spain
October 2, 2015
Quantifying the similarity between symbolic sequences is a traditional problem in Information The-
ory which requires comparing the frequencies of symbols in different sequences. In numerous modern
applications, ranging from DNA over music to texts, the distribution of symbol frequencies is char-
acterized by heavy-tailed distributions (e.g., Zipf’s law). The large number of low-frequency symbols
in these distributions poses major difficulties to the estimation of the similarity between sequences,
e.g., they hinder an accurate finite-size estimation of entropies. Here we show how the accuracy
of estimations depend on the sample size N , not only for the Shannon entropy (α = 1) and its
corresponding similarity measures (e.g., the Jensen-Shanon divergence) but also for measures based
on the generalized entropy of order α. For small α’s, including α = 1, the bias and fluctuations
in the estimations decay slower than the 1/N decay observed in short-tailed distributions. For α
larger than a critical value α∗ ≤ 2, the 1/N -scaling is recovered. We show the practical significance
of our results by quantifying the evolution of the English language over the last two centuries using
a complete α-spectrum of measures. We find that frequent words change more slowly than less
frequent words and that α = 2 provides the most robust measure to quantify language change.
I. INTRODUCTION
Quantifying the similarity of symbolic sequences is a
classical problem in information theory [1] with modern
applications in linguistics [2], genetics [3], and image pro-
cessing [4]. The availability of large databases of texts
sparked a renewed interest in the problem of similarity of
the vocabulary of different collections of texts [5–9]. For
instance, Fig. 1 shows the word-frequency distribution in
three large collections of English texts: from 1850, 1900,
and 1950. We see that the distribution itself remains
essentially the same, a heavy-tailed Zipf distribution [10]
p(r) ∝ r−γ , (1)
where p is the frequency of the r-th most frequent word
and γ ' 1. Changes are seen in the frequency p (or
rank) of specific words, e.g., ship lost and genetic won
popularity. Measures that quantify such changes are es-
sential to answer questions such as: Is the vocabulary
from 1900 more similar to the one from 1850 or to the
one from 1950? How similar are two vocabularies (e.g.,
from different years)? Are the two finite-size observations
compatible with a finite sample of the same underlying
vocabulary? How similar are the vocabulary of different
authors or disciplines? How fast is the lexical change
taking place?
Heavy-tailed and broad distributions of symbol-
frequencies such as Eq. (1) are typical in natural lan-
guages [10, 12–16] and appear also in the DNA (n-grams
of base pairs for large n) [17], in gene expression [18], and
in music [19]. The slow decay observed in a broad range
of frequencies implies that there is no typical frequency
for words and therefore relevant changes can occur in
different ranges of the p-spectrum, from the few large-
100 101 102 103 104 105 106
ranks: r
10−8
10−7
10−6
10−5
10−4
10−3
10−2
10−1
100
fre
qu
en
cy
:
p
(r
)
t = 1950
t = 1900
t = 1850
◦: “and”
: “see”
O: “ship”
M: “genetic”
FIG. 1. The English vocabulary in different years. Rank-
frequency distribution p(r) of individual years t for t = 1850,
1900, and 1950 of the Google-ngram database [11], multi-
plied by a factor of 1, 2, and 4, respectively, for better vi-
sual comparison. The inset shows the original un-transformed
data (same axis), highlighting that the rank-frequency distri-
butions are almost indistinguishable. Individual words (e.g.
“and”,“see”,“ship”,“genetic”) show changes in rank and fre-
quency (symbols), where words with larger ranks (i.e. smaller
frequencies) show larger change.
frequency words all the way to the many low-frequency
words. This imposes a challenge to define similarity mea-
sures that are able to account for this variability and that
also yield accurate estimations based on finite-size obser-
vations.
In this paper we quantify the vocabulary similarity us-
ing a spectrum of measures Dα based on the general-
ized entropy of order α (Dα=1 recovers the usual Jensen-
Shannon divergence). We show how varying α magnifies
ar
X
iv
:1
51
0.
00
27
7v
1 
 [
ph
ys
ic
s.
so
c-
ph
] 
 1
 O
ct
 2
01
5
2
differences in the vocabulary at different scales of the
(heavy-tailed) frequency spectrum, thus providing dif-
ferent information on the vocabulary change. We then
compute the accuracy (bias) and precision (variance) of
estimations of Dα based on sequences of size N and
find that in heavy-tailed distributions the convergence
is much slower than in non-heavy-tailed distributions (it
often scales as 1/Nβ with β < 1). Finally, we come back
to the problem of comparing the English vocabulary in
the last two centuries in order to illustrate the relevance
of our general results.
II. DEFINITION
Consider the probability distribution p =
(p1, p2, . . . , pS) of a random variable over a discrete,
countable set of symbols i = 1, . . . , S (where later we
include the possibility for S →∞). From an information
theory standpoint, a natural measure to quantify the
difference between two such probability distributions p
and q is the Jensen-Shannon divergence (JSD) [20]
D(p, q) = H
(
p + q
2
)
− 1
2
H(p)− 1
2
H(q), (2)
where H is the Shannon entropy [21]
H(p) = −
∑
i
pi log pi. (3)
This definition has several properties which are useful in
the interpretation as a distance: i) D(p, q) ≥ 0 where the
equality holds if and only if p = q; ii) D(p, q) = D(q,p)
(it is a symmetrized Kullback-Leiber-divergence [20]); iii)√
D(p, q) fulfills the triangle inequality and thus is a
metric [22]; and iv) D(p, q) equals the mutual informa-
tion of variables sampled from p and q [3], i.e., D(p, q)
equals the average amount of information in one ran-
domly sampled word-token about which of the two dis-
tribution it was sampled from [23]. The JSD is widely
used in the statistical analysis of language [2], e.g. to au-
tomatically find individual documents that are (seman-
tically) related [5, 6] or to track the rate of evolution in
the lexical inventory of a language over historical time
scales [7, 8].
Here we also consider the generalization of JSD in
which H in Eq. (3) is replaced by the generalized entropy
of order α [24]
Hα(p) =
1
1− α
(∑
i
pαi − 1
)
, (4)
yielding a spectrum of divergence measures Dα param-
eterized by α, first introduced in Ref. [25]. The usual
JSD is retrieved for α = 1. The suitability of Eq. (4) to
describe physical systems is the subject of investigation
of non-extensive statistical mechanics as first proposed in
Ref. [26]. While similar generalizations can be achieved
with other formulations of generalized entropies such as
the Renyi-entropy [4, 27], the corresponding divergences
can become negative. In contrast, Dα is strictly non-
negative and it was recently shown that
√
Dα(p, q) is a
metric for any α ∈ (0, 2] [28]. For heavy-tailed distribu-
tions, Eq. (1), Hα <∞ for α > 1/γ.
We define a normalized version of Dα as
D̃α(p, q) =
Dα(p, q)
Dmaxα (p, q)
(5)
where
Dmaxα (p, q) =
21−α − 1
2
(
Hα (p) +Hα (q) +
2
1− α
)
.
(6)
is the maximum possible Dα between p and q obtained
assuming that the the set of symbols in each distribution
(i.e., the support of p and q) are disjoint. The main moti-
vation for using the measure (5) is that D̃α(p, q) ∈ [0, 1],
while the range of admissible values of Dα depends on
α. This allows for a meaningful comparison of the diver-
gences D̃α(p, q) and D̃α′(p, q) for α 6= α′ and therefore
also for the full spectrum of α’s. In general, the metric
properties of Dα are not preserved by D̃α. An exception
is the case in which the rank-frequency distribution p(r)
underlying all p’s and q’s is invariant (see Fig. 1). Noting
that Eq. (6) is independent of the symbols we obtain that
Dmaxα (p, q) is a constant for all p’s and q’s and therefore
the metric property is preserved for D̃α.
III. INTERPRETATION
In order to clarify the interpretation of Dα, it is use-
ful to consider a toy model. As in Fig. 1, we consider
two distributions p and q that have exactly the same
rank-frequency distribution p(r) but differ in (a sub-
set of) the symbols they use. For simplicity, we con-
sider that symbols that differ in the two cases appear
only in one of the distributions. More precisely, denot-
ing by Ip = {A,B,C,D,E, . . .} the set of symbols in
p with probabilities pi and i ∈ Ip, we replace a sub-
set I∗ ⊂ Ip of symbols in q by a new symbol with the
same probability (this ensures that the rank-frequency
distribution is conserved). Thus the set of symbols in
q is Iq = {i|i ∈ Ip \ I∗} ∪ {i†|i ∈ I∗} with pi = qi for
i ∈ Ip \ I∗ and pi = qi† for i ∈ I∗, see Fig. 2 for one
example.
For a given distribution p and a set of replaced symbols
I∗, we compute Dα(p, I∗) ≡ Dα(p, q) as
Dα(p, I
∗) = cα
∑
i∈I∗
pαi , (7)
where cα = (2
(1−α) − 1)/(1−α). The maximum is given
by
Dmaxα (p, I
∗) = cα
∑
i∈Ip
pαi (8)
3
such that
D̃α(p, I
∗) =
∑
i∈I∗ p
α
i∑
i∈Ip p
α
i
. (9)
This shows that each symbol i ∈ I∗ that is replaced by
a new symbol contributes pαi to Dα. It is thus clear
that varying α, the contribution of different frequencies
become magnified (e.g. for α  1 large frequencies are
enhanced while for α < 0 low frequencies contribute more
to Dα than large frequencies).
In particular, for α = 0, D̃α=0(p, I
∗) = |I
∗|
|Ip| is the frac-
tion of symbols (types) that are different in p and q. Each
symbol i counts the same irrespective of their probabili-
ties pi. For
|I∗|
|Ip|  1, D̃α=0(p, I
∗) = 1− J(Ip, Iq), where
J(Ip, Iq) =
|Ip∩Iq|
|Ip∪Iq| is the Jaccard-coefficient between the
two sets Ip and Iq, an ad-hoc defined similarity mea-
sure widely used in information retrieval [2]. For α = 1,
D̃α=1(p, I
∗) =
∑
i∈I∗ pi showing that each replaced sym-
bol is weighted by its probability pi and thus that D̃α=1
measures the distance in terms of tokens.
The full spectrum D̃α offers information on changes in
all frequencies, a point which is particularly important
for the case of heavy-tailed distributions because word-
frequencies vary over many orders of magnitude. Figure 3
illustrates how different values of α are able to capture
changes at different regions in the frequency-spectrum.
In particular, it shows that D̃α grows (decays) with α
when the modified symbols have high (low) frequency.
Furthermore, the comparison between two given changes
allow us to conclude about which change was more sig-
nificant at different regions of the word-frequency spec-
trum. In the example of the figure, both changes (the
two lines) are equally significant from the point of view
of the modified tokens (D̃1 are the same), the change in
the left affects more types (D̃0 is larger), and the change
in the right affects more frequent words (D̃α is larger for
α 1).
A B C D E F G H
symbol: i
0.0
0.1
0.2
0.3
0.4
0.5
p
i
A B† C D E F G H
symbol: i
0.0
0.1
0.2
0.3
0.4
0.5
q i
FIG. 2. Illustration of our toy model where p (left) and q
(right) have the same rank-frequency distribution, but differ
in the probability for individual symbols. In this example, p
and q are the same (pi = qi) for i ∈ {A,C,D,E, F,G,H},
while the symbol i = B in p is replaced by i = B† in q with
pi=B = qi=B† and pi=B† = qi=B = 0.
−2 −1 0 1 2 3 4
α
0.0
0.2
0.4
0.6
0.8
1.0
D̃
α
(p
,I
∗ )
1 103i
10−5
1
p
i
1 103i
10−5
1
p
i
FIG. 3. The spectrum D̃α(p, I
∗) for two different changes.
The lines correspond to Eq. (9) with pi ∝ i−1 with i =
1, 2, . . . , 1000 and two different sets of replaced symbols I∗1 , I
∗
2 .
Right inset: I∗1 = {1}, i.e., only the symbol with the high-
est probability, pi=1 ≈ 0.13 is changed. Left inset: I∗2 =
{368, . . . , 1000}, i.e, the symbols with small probability are re-
placed. The choice of I∗2 was made such that
∑
i∈I∗2
pi ≈ pi=1
and therefore D̃α=1(p, I
∗
1 ) ≈ D̃α=1(p, I∗2 ).
IV. FINITE-SIZE ESTIMATION
In this section we turn to the estimation of D̃α from
data. Even if D̃α is defined with respect to distributions
p and q, Eq. (5), in practice these distributions are es-
timated from sequences with finite-size N (total number
of symbols or word tokens) yielding finite size estimates
of the distributions p̂ and q̂. The main obstacle in ob-
taining accurate estimates of D̃α is that it requires the
estimation of entropies for which, in general, unbiased
estimators do not exist [29]. Accordingly, even if p = q,
in practice Hα(p̂) 6= Hα(q̂) and D̃α(p̂, q̂) > 0 are mea-
sured not only in single realizations, but also on average
(the bias). Besides the bias, we are also interested in
the expected fluctuation (standard deviation) of the es-
timations of Hα and D̃α and how both they depend on
the sequence size N for large N . In heavy-tailed distri-
butions such as Eq. (1), these estimations are based on
an observed vocabulary V (number of different symbols)
that grows sub-linearly with N as [30–32]
V (N) ∝ N1/γ . (10)
This implies that the entropies in Eq. (4) are estimated
based on a sum of V → ∞ terms (for N → ∞). In
practice, γ and the precise functional form of the heavy-
tailed distribution are unknown and therefore, besides
D̃α, the estimation of Hα is also of interest (see Ref. [33]
for the case in which a power-law form of p is assumed
to be known a priori).
A. Analytical Calculations
Here we extend previous results [34–37] and general-
ize them to arbitrary α. Given a probability distribution
4
p and the measured probabilities p̂ from a finite sam-
ple of N word-tokens, we expand Hα(p̂) around the true
probabilities pi up to second order as
Hα(p̂) ≈ Hα(p)+
∑
i:p̂i>0
(p̂i − pi)
α
1− α
pα−1i
−1
2
∑
i:p̂i>0
(p̂i − pi)2αpα−2i
(11)
where we used that ∂Hα∂pi = α/(1− α)p
α−1
i and
∂2H
∂pi∂pj
=
−αpα−2i δi,j . We then calculate E [Hα(p̂)] by averaging
over the different realization of the random variables p̂i
by assuming that the absolute frequency of each symbol i
is drawn from an independent binomial with probability
pi such that E [p̂i] = pi and V [p̂i] = pi(1−pi)/N ≈ pi/N
yielding
E [Hα(p̂)] ≈ Hα(p)−
α
2N
∑
i∈V
pα−1i = Hα(p)−
αV (α)
2N
,
(12)
which defines the vocabulary size of order α
V (α) ≡
∑
i∈V
pα−1i . (13)
From Eq. (12) we see that the bias in the entropy estima-
tion |Hα(p) − E [Hα(p̂)] | depends only on V (α) and N .
Similar calculations (see Appendix B) show that the large
N behavior of the bias and the fluctuations (variance) of
Hα, Dα, and D̃α can be written as simple functions of
V (α) and N , as summarized in Tab. I.
Hα Dα, D̃α(p 6= q) Dα, D̃α(p = q)
Bias: V (α)/N V (α)/N V (α)/N
Fluctuations: V (2α)/N V (2α)/N V (2α−1)/N2
TABLE I. Scaling of the bias |E[X̂] − X| and the fluctua-
tions V[X] ≡ E[X̂2] − E[X̂]2 of estimations X̂. The results
are valid for large sequence sizes N and depend on the vo-
cabulary of order α, V (α) as in Eqs. (13) and (14). Results
are shown for X = Hα [order α entropy, Eq. (4)], Dα [gen-
eralized divergence], D̃α [normalized divergence, Eq. (5)], see
Appendix B for the derivations. For D̃α, we approximate
D̃α ≈ Dα/E[Dmaxα ].
We now focus on the dependence of V (α) on N . The
sum
∑
i∈V in Eq. (13) indicates that in N samples, on
average, V = V (N) ≡ V (α=1) different symbols are ob-
served. If for N → ∞ the vocabulary V converges to
a finite value, V (α) in Eq. (13) also converges and the
bias scales as 1/N . A more interesting scenario happens
when V grows with N . For the heavy-tailed case of in-
terest here, V grows as N1/γ , Eq. (10), and we obtain
(in Appendix C) that V (α) scales for large N as
V (α) ∝
{
N−α+1+1/γ , α < 1 + 1/γ,
constant , α > 1 + 1/γ,
(14)
where γ > 1 is the Zipf exponent defined in Eq. (1) and
α is the order of the entropy in Eq. (4).
From the combination of Eq. (14) and Tab. I we ob-
tain the scalings with sequence size N of the estimators
of Hα, Dα, and D̃α in a heavy-tailed distribution with
exponent γ. These scalings are summarized in Tab. II.
Three scaling regimes can be identified for the bias and
for the fluctuations. (i) For large α, the decay is 1/N
(except when p = q, where the fluctuations decay even
faster as 1/N2) as in the case of a finite vocabulary and
short-tailed distributions. (ii) For intermediate α, a sub-
linear decay with N is observed. This regime appears
exclusively in heavy-tailed distributions and has impor-
tant consequences in real applications, as shown below.
From the exponents of the sub-linear decay we see that
the bias decays more slowly than the fluctuations. (iii)
For small α, α < 1/γ, Hα(p) is not defined thus the esti-
mator for the mean of Hα and Dα diverge. The growth
of Hα (and therefore D
max
α ) and Dα with N have the
same scaling and therefore cancel each other for D̃α, in
which case a convergence to a well defined value is found
(the fluctuation of D̃α still decays in this regime).
B. Numerical Simulations
Here we perform numerical estimations of the normal-
ized divergence spectrum D̃α that illustrate the regimes
derived above, confirm the validity of the approximations
used in their derivations, and show that the same scalings
are observed for different entropy estimators. We sam-
ple twice N symbols (tokens) from the same distribution
(p = q), and therefore D̃α = 0 and the expected value
E[D̃α(p̂, q̂)] is the bias. (The fact that the bias shows a
slower decay with N than the fluctuations makes these
two effects distinguishable also in this D̃α = 0 case be-
cause E[D̃α(p̂, q̂)] V[D̃α(p̂, q̂)] for large N).
We start with the most important prediction of our
analytical calculations above: the existence in heavy-
tailed distributions of a regime for which the bias and
fluctuations of D̃α decay with N more slowly than 1/N .
This holds already for α = 1, i.e., for the usual Jensen-
Shannon divergence, previously shown for the bias of
Hα=1 in Ref. [37]. One potential limitation of our an-
alytical calculations is that they are based on the plugin-
estimator obtained from replacing the pi’s in the general-
ized entropies, Eq. (4), by the measured frequencies (i.e.
pi 7→ p̂i = Ni/N , with Ni being the number of observed
word tokens of type i). To test the generality of our re-
sults, in the numerical simulations we use four different
estimators of the Shannon entropy (i.e., α = 1): i) the
Plugin-estimator; ii) Miller ’s-estimator [34], which takes
into account the approximation obtained from the expan-
sion in Eq. (12); iii) Grassberger ’s estimator [38] based on
the assumption that frequencies are Poisson-distributed;
and iv) a recently proposed Bayesian estimator described
in [39] which is an extension of the approach by Nemen-
man et al. [40] to the case where the number of possible
5
E[Hα(p̂)] E[Dα(p̂, q̂)] E[D̃α(p̂, q̂)]
αE1 1/γ 1/γ 1/γ
αE2 1 + 1/γ 1 + 1/γ 1 + 1/γ
α < αE1 cN
−α+1/γ cN−α+1/γ c
αE1 < α < α
E
2 Hα(p) + cN
−α+1/γ Dα(p, q) + cN
−α+1/γ D̃α(p, q) + cN
−α+1/γ
α > αE2 Hα(p) + cN
−1 Dα(p, q) + cN
−1 D̃α(p, q) + cN
−1
V[Hα(p̂)] V[Dα(p̂, q̂)] V[D̃α(p̂, q̂)]
p 6= q p = q p 6= q p = q
αV1 1/(2γ) 1/(2γ) 1/(2γ) 1/γ 1/γ
αV2
1
2
(1 + 1/γ) 1
2
(1 + 1/γ) 1 + 1/(2γ) 1
2
(1 + 1/γ) 1 + 1/(2γ)
α < αV1 cN
−2α+1/γ cN−2α+1/γ cN−2α+1/γ cN−1/γ cN−1/γ
αV1 < α < α
V
2 cN
−2α+1/γ cN−2α+1/γ cN−2α+1/γ cN−2α+1/γ cN−2α+1/γ
α > αV2 cN
−1 cN−1 cN−2 cN−1 cN−2
TABLE II. Summary of finite size scaling for distributions with heavy tails. Mean (E) and variance (V) of the plug-in
estimator of Hα, Dα, and D̃α for samples p̂ and q̂ each of size N drawn randomly from p and q with power-law rank-frequency
distributions with exponent γ > 1, Eq. (1). The results are obtained combining Tab. I with Eq. (14) (for details see Appendix B,
C). The constant c depends on α and has a different value in each case but is independent of N . The limit γ →∞ corresponds
to the case in which both p and q have short tails.
10−4
10−3
10−2
10−1
100
B
ia
s
E[
D̃
α
=
1
(p̂
,q̂
)]
∝ N−1
(a)
[Exponential]
100 101 102 103 104
Sample size: N
10−8
10−6
10−4
10−2
100
Fl
uc
tu
at
io
ns
V[
D̃
α
=
1
(p̂
,q̂
)]
∝ N−2
(d)
∝ N−1+1/γ
∝ N−1
(b)
[Power law]
100 101 102 103 104
Sample size: N
∝ N−2
∝ N−2+1/γ
(e)
∝ N−1
(c)
[Empirical]
Plugin
Miller
Grassberger
Bayesian
100 101 102 103 104
Sample size: N
∝ N−2
(f)
FIG. 4. Finite-size estimation of the normalized Jensen-Shannon divergence D̃ = D̃α=1. (a-c) Estimation of E[D̃(p̂, q̂)] between
two sequences of size N drawn from the same distribution (i.e. D(p, q) = 0) using four different estimators of the entropy (see
text) for three representative distributions: (a) Exponential (short-tailed) distribution pi ∝ e−ai for i = 0, 1, . . . with a = 0.1;
(b) Power-law (heavy-tailed) distribution pi ∝ i−γ for i = 1, 2, . . . with γ = 3/2; (c) Empirical Zipf-distribution of word
frequencies, i.e. rank-frequency distribution p(r) from the complete Google-ngram data, pi = f(i = r) for i = 1, . . . , 4623568,
which is well described by a double power-law [10]. (d-f) Show the same as (a-c) for the fluctuations V[D̃(p̂, q̂)]. The dotted
lines show the expected scalings from Tab. II for short-tailed distributions, i.e. N−1 (N−2), and power-law distributions, i.e.
N−1+1/γ (N−2+1/γ), for the bias (fluctuations). In (c) we show the expected scaling for the bias, Vemp(N)/N , where Vemp(N)
is the expected number of different symbols in a random sample of size N from the empirical distribution [32]. Averages are
taken over 1000 realizations.
6
10−6
10−5
10−4
10−3
10−2
10−1
100
B
ia
s
E[
D̃
α
(p̂
,q̂
)]
∝ N−1
∝ const.
(a)
100 101 102 103 104 105 106
Sample size: N
10−12
10−10
10−8
10−6
10−4
10−2
100
Fl
uc
tu
at
io
ns
V[
D̃
α
(p̂
,q̂
)]
∝ N−1/γ
∝ N−2
(c)
0.0
0.5
1.0
1.5
2.0
α
0.5
0.6
0.7
0.8
0.9
1.0
E[
D̃
α
](
2
N
)/
E[
D̃
α
](
N
)
(b)
αE1 α
E
2
N = 25
N = 210
N = 215
N = 219
0.0 0.5 1.0 1.5 2.0
α
0.2
0.3
0.4
0.5
0.6
0.7
V[
D̃
α
](
2
N
)/
V[
D̃
α
](
N
)
(d)
αV1 α
V
2
N = 25
N = 210
N = 215
N = 219
FIG. 5. Bias (a,b) and fluctuations (c,d) in finite-size estimation of D̃α. Estimation of E[D̃α(p̂, q̂)] between two sequences
each of size N drawn numerically from the same power-law distribution pi ∝ i−γ for i = 1, 2, . . . , V → ∞ with γ = 3/2 using
the plugin-estimator (pi 7→ p̂i) for the entropies of order α. (a) Scaling of the bias with N for different α. (b) Decrease of the
bias in D̃α when sample size is doubled (N 7→ 2N) for different values of N as a function of α. (c) and (d) show the same as
(a) and (b) for the fluctuations V[D̃α(p̂, q̂)]. Red lines in all plots indicate the borders between the regimes, αE1 = 1/γ = 2/3,
αE2 = 1 + 1/γ = 5/3 (for the bias in a,b), and α
V
1 = 1/γ = 2/3, α
V
2 = 1 + 1/(2γ) = 4/3 (for the fluctuations in c,d). Dotted
lines indicate the predictions based on Tab. I for α < αE1 , α
V
1 and α > α
E
2 , α
V
2 (in a,c) and all values of α (in b,d). Averages are
taken over 1000 realizations.
symbols is unknown or even countably infinite [41]. The
numerical results in Fig. 4 show that the different estima-
tors are indeed able to reduce the bias of the estimation,
but that the scaling of the bias with N remains the same.
In particular, the transition from short-tailed to heavy-
tailed distribution leads to the predicted transition from
N−1 (N−2) to the slower N−1+1/γ (N−2+1/γ) decay for
the bias (fluctuations) for all estimators. The only ex-
ception is in the bias of the Bayesian estimator for the
exact Zipf’s law (1), but since this estimator shows a
bad performance for the fluctuation and for the real data
we conclude that the slower scaling should be expected
in general also for this elaborated estimator. These re-
sults confirm the generality of our finding that the bias
and fluctuation in D̃α=1 decays more slowly than 1/N in
heavy-tailed distributions. The consequence of this result
to applications will be discussed in the next section.
We now consider the estimation of D̃α for α 6= 1 in
the case of heavy-tailed distributions (1). The numer-
ical results in Fig. 5 confirm the existence of the three
scaling regimes discussed after Eq. (14) and in Tab. II.
The panels (b) and (d) show the relative reduction in the
bias and fluctuations achieved when the sequence size
is doubled. For many different α’s the relative reduc-
tion is larger than 0.5 (0.25) for the bias (fluctuations),
a consequence of the slow decay with N that shows the
difficulty in obtaining a good estimation of D̃α. In prac-
tice, the exponent γ of the distribution is unknown such
that the critical values of α that separates these regimes
(e.g. αE1 = 1/γ and α
E
2 = 1 + 1/γ for the bias) cannot
be determined a priori. Yet, since γ > 1, we know that:
(i) αE1 , α
V
1 < 1 and therefore Dα for α ≥ 1 is such that
Dα(p,p) = 0 for N →∞; and (ii) αE2 , αV2 < 2 and there-
fore the bias and fluctuations of Dα for α ≥ 2 decay as
1/N (or 1/N2 for the fluctuations in the case of p = q).
This suggests Dα=2 as a pragmatic choice for empirical
measurements because any further increase in α will not
lead to a faster convergence.
V. APPLICATION TO REAL DATA
In this section, we show the significance of the general
results of the previous section to specific problems.
A problem that appears in different contexts is to test
whether two finite-size N sequences, described by their
empirical distributions p̂ and q̂, have a common source
(null hypothesis). This involves the computation of a
single divergence D̃α(p, q), which is then compared to
the divergence D̃α(p
′,p′) between two finite-size (ran-
dom) samplings of a single (properly chosen) distribu-
tion p′ (e.g., p′ = 0.5p + 0.5q). The probability of
observing D̃α(p
′,p′) ≥ D̃α(p, q) is then reported as a
7
0.0 0.5 1.0 1.5 2.0
α
10−5
10−4
10−3
10−2
10−1
D̃
α
(p
t
1
,
p
t
2
)
(a)
1850-1900
1900-1950
1850-1950
0 0.5 1 1.5 2
10−4
10−3
10−2
10−1
1
0 50 100 150
∆t
10−4
10−3
10−2
10−1
100
D̃
α
(∆
t)
(b)
α = 0.0
α = 0.5
α = 1.0
α = 2.0
0 50 100 150
0
20
40
60
FIG. 6. Measuring change in the usage of language on historical time-scales. (a) D̃α(pt1 ,pt2) as a function of α for pairs
of word-frequency distributions of the Google-ngram database obtained from the yearly corpora t1 and t2 with (t1, t2) ∈
{(1850, 1900), (1900, 1950), (1850, 1950)} (solid lines). The dotted lines with the same colors show the results of a null model
in which samples of the same size of the ones in t1 and t2 are randomly drawn from the same distribution (obtained from
combining the corpora in t1 and t2) mimicking a minimum distance that can be observed due to finite-size effects. The vertical
lines show the three regimes α < 1/γ, 1/γ < α < 1 + 1/γ, and α > 1 + 1/γ in the convergence of D̃α(pt1 ,pt2) with N
(see Sec. IV), obtained using γ = 1.77 [10]. Inset: ratio D̃α(pt12 ,pt12)/D̃α(pt1 ,pt2). (b) Average divergence as a function of
∆t ≡ |t2 − t1|, calculated as D̃α(∆t) = 1N∆t
∑2000−∆t
t1=1805
D̃α(pt1 ,pt1+∆t) for four different α (solid lines). Shaded areas represent
the standard deviation associated to the average D̃α(∆t). Inset: D̃α(∆t)/D̃α(∆t = 1).
p-value [3]. Besides applications in language, e.g. com-
paring the distribution of word-frequencies, this problem
appears in the identification of coding- and non-coding
regions in DNA [42]. The significance of our results for
finite-size estimations in Sec. IV is that for the case of
heavy-tailed distribution the expected D̃α(p, q) of the
null model may be much larger than the predicted value
based on a 1/N decay (as observed in short-tailed dis-
tributions). If the slower convergence in N is ignored,
e.g., by applying standard tests [3] to heavy-tailed dis-
tributions, one rejects the null hypothesis (low p-value)
even if the data is drawn from the same source because
the measured distance will be much larger. The exam-
ple in Fig. 4(c) shows that, even when the size of both
sequences is on the order of N ≈ 105, the expected D̃1
(JSD) is E[D̃α=1(p̂, q̂)] ≈ 10−1. This is two orders of
magnitude larger than for the exponential distribution
in Fig. 4(a), where E[D̃α=1(p̂, q̂)] ≈ 10−3.
The next problems we consider appear in the anal-
ysis of historical data and in the quantification of lan-
guage change [43]. These problems are representative of
problems that involve the comparison of two or more di-
vergences D̃α(p, q), obtained from different distributions
p 6= q and α′ 6= α. As depicted in Fig. 1, the differ-
ent distributions are obtained based on individual years
(t ∈ {1850, 1900, 1950}) and we calculate the normalized
spectrum D̃α(pt1 ,pt2) between pairs of years (t1, t2). As
argued in Sec. II and Appendix A, D̃α(p, q) is meaning-
ful even if the sequences used to estimate p and q have
different sizes Np 6= Nq. We summarize our results in
Fig. 6, from which different conclusions can be drawn:
a. Temporal change. The change of English from
1850 to 1950 was larger than the change form 1850 to
1900 and from 1900 to 1950, as seen from the fact that
the curve of D̃α(p1850,p1950) in Fig. 6a lies above the
two other curves for all α. This intuitive result (evolu-
tionary dynamics show no recurrences) confirms that the
divergence spectrum D̃α(pt1 ,pt2) is a meaningful quan-
tification of language change. The average dependency of
D̃α(p1850,p1950) on ∆t = |t2 − t1|, shown in Fig. 6b, can
be thus used as a quantification of the speed of language
change.
b. Dependence on α. All observed divergences
D̃α(pt1 ,pt2) decay with α (e.g., the three curves in
Fig. 6a). As discussed in Sec. III, this shows that
for words with a high (low) frequency the distribu-
tions are more (less) similar and thus the change is
slower (faster). This result is consistent with reports
that frequent words tend to be more stable on his-
torical time scales [44, 45]. This dependence on α
is essential when comparing the change 1850 7→ 1900
to the change 1900 7→ 1950 (Fig. 6a). While the
earlier change was smaller if counted on a token ba-
sis, D̃α=1(p1850,p1900) < D̃α=1(p1900,p1950), it be-
comes larger if one focus on the more frequent words
[D̃α=2(p1850,p1900) > D̃α=2(p1900,p1950)].
c. Role of finite-size scalings. Our finding that the
scalings (of the bias and of the fluctuations) in D̃α with
sample size N depend on α allows for a deeper un-
derstanding of the D̃α(pt1 ,pt2) measurements discussed
above. The expected D̃α’s for random sampling of the
same distribution (null model shown as dashed line in
Fig. 6a) are of the same order as the empirical distance
for small α (i.e. D̃α(pt12 ,pt12) ≈ D̃α(pt1 ,pt2)) and it
is only for α > 1 that the null model divergence be-
comes negligible compared to the empirical divergence
8
(i.e. D̃α(pt12 ,pt12)  D̃α(pt1 ,pt2)). This implies that
even though the size of the individual corpora is of the
order of N ≈ 109 word-tokens, the empirically measured
D̃α is still strongly influenced by finite-size effects over a
wide range of values for α, in agreement with our analysis
in Sec. IV. It also emphasizes that D̃α=2 offers a prag-
matic choice in reducing such finite-size effects when the
exponent γ in the power-law distribution is not known.
This conclusion is further corroborated in the analysis of
the dependence of D̃α with ∆t (Fig. 6b). For small α, D̃α
does not converge to zero for ∆t→ 0, but instead it seems
to saturate, i.e. D̃α(∆t → 0) ≈ D̃α(∆t = 1) = α > 0.
The value α is of the same order of magnitude of the
expected bias (e.g., shown as dashed line in Fig. 6a) and,
for small α, still of the same order of magnitude of the
divergence D̃α(∆t = 100) between two corpora separated
by 100 years. For small α and ∆t, it is thus difficult to
distinguish between finite-size effects (α) and actual lan-
guage change. Results for α = 2 show the largest relative
variation with ∆t (see Inset of Fig. 6b) and are therefore
statistically more suited to quantify language change over
time.
VI. CONCLUSIONS
In summary, we investigated the use of generalized en-
tropies Hα to quantify the difference between symbolic
sequences with heavy-tailed frequency distributions. In
particular, we introduced a normalized spectrum of a
generalized divergence, D̃α(p, q) in Eq. (5), that allows
for a comparison between the different distributions p
and q and also for different α’s. Increasing α, D̃α at-
tributes higher weights to high-frequency symbols. The
more complete characterization given by the full spec-
trum D̃α is particularly important in the case of heavy-
tailed distributions because in this case symbols do not
have a characteristic frequency but instead show frequen-
cies on a broad range of values.
Our main analytical finding is how the bias and fluc-
tuations of finite-size N estimations of Hα and D̃α scales
with N , see Tab. II. The existence of regimes in which
these scalings decay slower than 1/N shows that large un-
certainties should be expected in Hα and D̃α estimated
even for very large databases. This should be taken into
account when comparing two or more D̃α’s and when
estimating the probability of two sequences having the
same source. The fact that for large α we recover the
usual scaling (decay with 1/N) suggests D̃α=2 as a prag-
matic choice in applications involving heavy-tailed distri-
butions.
Our investigation of the change in word usage on
historical time scales shows that low-frequency words
change faster than high-frequency words and quantifies
the speed of change in time. Furthermore, we illustrated
that D̃α=2 is more suited to discriminate and quantify
language change over time.
Our results are also of interest beyond the cases treated
here. First, the finite-size scaling we derive appear al-
ready in the entropy and therefore the same scalings are
expected in any entropy-based measure, including those
based on conditional entropies such as the Kullback-
Leibler divergence [1]. Second, the analysis is not nec-
essarily restricted to the word level, it can be straightfor-
ward extended also to n-grams of words which also show
heavy-tailed distributions [46]. Third, the spectrum of
divergences D̃α(p, q) offers a unifying framework which
can be applied to problems involving different partitions
of texts by varying the parameter α. For example, while
in document classification [2] one tries to identify top-
ical words (suggesting the use of low values of α), in
applications of authorship attribution [47] it has been
shown that the comparison of the most-frequent (func-
tion) words yields the best results (suggesting the use of
large values of α). Fourth, heavy-tailed distributions ap-
pear in different problems involving symbolic sequences
(e.g., in the DNA [17], in gene expression [18], and in
music [19]), and the significance of our results is that they
can be applied in all these cases.
ACKNOWLEDGMENTS
We thank Peter Grassberger for insightful discussions.
Appendix A: Documents with different lengths
Here we discuss how to proceed if the JSD is computed
from finite datasets with different finite lengths N , i.e.
when p (q) is estimated from a sequence of length Np
(Nq 6= Np).
1. Different Weights
A possible way to extend Eq. (2) taking into ac-
count the unequal contribution Np 6= Nq is to consider
weights π as [3]
Dπα(p, q) = Hα(πpp+πqq)−πpHα(p)−πqHα(q). (A1)
with πp = Np/N and Nq/N such that πp + πq = 1 with
N = Np +Nq (denoted as natural weights in the follow-
ing). Obviously, if Np = Nq then πp = πq = 1/2 and Dα
is recovered. The normalized distance (5) becomes
D̃πα(p, q) =
Dπα(p, q)
Dπ,maxα (p, q)
, (A2)
where
Dπ,maxα (p, q) =
(
παp − πp
)
Hα (p) +
(
παq − πq
)
Hα (q)
+
1
1− α
(
παp + π
α
q − 1
)
.
(A3)
9
Our main results for the finite-size scaling of Dα summa-
rized in Tab. II remain valid for the weighted divergences.
The approach above follows Ref [3], which introduced
weights to the usual JSD (non-normalized, α = 1) and
showed that the natural weights πp = Np/N and πq =
Nq/N imply certain useful properties for the JSD, e.g.,
that the bias does not depend on the relative size of the
two samples. While their main motivation was to com-
pare the statistical significance of a single measurement of
the JSD in the identification of stationary subsequences
(of possibly different lengths) in a non-stationary sym-
bolic sequence, here, we are mainly interested in com-
paring two (or more) measured distances. In this case,
choosing weights that depend on the size of the individ-
ual samples becomes problematic when the sequences are
of different lengths. The demonstration that
√
Dα(p, q)
is a metric for any α ∈ (0, 2] [28] is valid for fixed weights
πp = πq = 1/2. More generally, the measure D
π
α itself
depends on the weights π such that Dπα and D
π′
α con-
stitute different measures when π 6= π′. It is therefore
not meaningful to compare Dπα(p, q) and D
π′
α (p
′, q′) if
Np/Nq 6= Np′/Nq′ because this would imply that π′ 6= π.
2. Equal Weights
In the previous section we argued that it is essential
to choose fixed weights π when comparing different dis-
tances. The choice of equal weights πp = πq = 1/2 can,
however, still be interpreted in the framework of natural
weights (πp = Np/N , πq = Nq/N) as the distance be-
tween undersampled versions of the sequences. For given
p and q with Np 6= Nq we choose equal weights πp =
πq = 1/2 yielding a distance D
1/2
α (p, q). If we randomly
draw samples p′ and q′ of size N ′p = N
′
q from the distri-
butions p and q, (by construction) the natural weights
coincide with the equal weights, i.e. π′p = π
′
q = N
′
p/N =
N ′q/N = 1/2, and lim
N ′p=N
′
q→∞
Dπ
′
α (p
′, q′) = D1/2α (p, q).
In Fig. 7 we show the difference in D̃πα(p, q) between
two empirical distributions from the Google-ngram with
different sizes (Np 6= Nq) when choosing equal and natu-
ral weights. Using equal weights corresponds to the case
in which we draw samples p̂ and q̂ that are of equal length
(N ′p = N
′
q) such that equal and natural weights coincide
and taking the limit N ′p, N
′
q →∞.
Appendix B: Finite size estimation of Hα, Dα, and
D̃α
In this section we present the calculations on the mean
(i.e. the bias) and the fluctuations in finite-size estimates
of Hα, Dα, and D̃α. The starting point is a finite sample
p̂ = (n1/N, n2/N, . . . , nV /N) of size N (where ni is the
number of times symbol i was observed) which we assume
is obtained from N identical and independent draws from
100 101 102 103 104 105 106 107 108 109
N = Np +Nq
10−3
10−2
10−1
100
D̃
α
(p̂
,q̂
)
α = 0.0
α = 0.5
α = 1.0
α = 2.0
FIG. 7. JSD-α for sequences of different lengths. Measure-
ment of D̃α(p̂, q̂) between sequences p̂, q̂ of sizeN
′
p = N
′
q sam-
pled randomly from the empirical distribution of the Google-
ngram of the years t ∈ {1850, 1950} with different sizes, i.e.
p = pt=1850 and q = pt=1950 with Np 6= Nq, as a function of
the sample size N ′ = N ′p + N
′
q for different values of α. The
dotted (dashed) lines show D̃πα(p, q) between the full distribu-
tions p and q with equal (natural) weights, i.e. πp = πq = 1/2
(πp = Np/(Np + Nq) ≈ 0.22 and πq = Nq/(Np + Nq) ≈ 0.78
corresponding to the relative size of p and q)
the distribution p giving an estimator for Hα:
Hα(p̂) =
1
1− α
 ∑
i:p̂i>0
p̂αi − 1
 . (B1)
In order to take the corresponding expectation values we
expand p̂αi around the true probabilities pi up to second
order
p̂αi ≈ pαi +(p̂i−pi)αpα−1i +
1
2
(p̂i−pi)2α(α−1)pα−2i (B2)
and average over the realizations of the random variables
p̂αi by assuming that each symbol is drawn independently
from binomial with probability pi such that 〈(p̂i−pi)〉 = 0
and 〈(p̂i − pi)2〉 = pi(1− pi)/N ≈ pi/N yielding [37]
〈p̂αi 〉 ≈ pαi +
1
2N
α(α− 1)pα−1i . (B3)
1. Hα
Combining Eqs. (B1) and (B3) we obtain for the mean
E[Hα(p̂)] ≡ 〈Hα(p̂)〉 =
1
1− α
 ∑
i∈〈Vp̂〉
〈p̂αi 〉 − 1

=
1
1− α
 ∑
i∈〈Vp̂〉
pαi − 1
− α
2N
∑
i∈〈Vp̂〉
pα−1i
=
1
1− α
(
V
(α+1)
p̂ − 1
)
− α
2N
V
(α)
p̂
(B4)
10
where we introduce the notation
∑
i∈〈Vp̂〉 indicating that
we average only over the expected number of observed
symbols 〈Vp̂〉 in samples p̂.
For the variance we get
V[Hα(p̂)] ≡E[Hα(p̂)2]− E[Hα(p̂)]2
=
1
(1− α)2
∑
i∈〈Vp̂〉
∑
j∈〈Vp̂〉
(
〈p̂αi p̂αj 〉 − 〈p̂αi 〉〈p̂αj 〉
)
=
α2
(1− α)2N
∑
i∈〈Vp̂〉
p2α−1i −
α2
4N2
∑
i∈〈Vp̂〉
p2α−2i
=
α2
(1− α)2
V
(2α)
p̂
N
− α
2
4
V
(2α−1)
p̂
N2
(B5)
where we used that two different symbols i 6= j are in-
dependently drawn, thus
∑
i,j〈p̂αi p̂αj 〉 =
∑
i 6=j〈p̂αi 〉〈p̂αj 〉 +∑
i〈p̂2αi 〉.
2. Dα
For Dα we have two samples p̂ and q̂ each of size N
randomly sampled from the distributions p and q such
that we can express the mean and the variance from the
expectation values of the corresponding individual en-
tropies.
Introducing the notation P ≡ 12 (p + q) we get for the
mean
E[Dα(p̂, q̂)] =E[Hα(P̂ )]−
1
2
E [Hα (p̂)]−
1
2
E [Hα (q̂)]
=
1
1− α
{
V
(α+1)
P̂
− 1
2
V
(α+1)
p̂ −
1
2
V
(α+1)
q̂
}
+
α
2N
{
1
2
V
(α)
p̂ +
1
2
V
(α)
q̂ −
1
2
V
(α)
P̂
}
.
(B6)
where V
(α)
P̂
denotes the generalized vocabulary, Eq. (13),
for the combined sequence P̂ = 12 (p̂ + q̂), which is of
length 2N .
For the variance we get
V[Dα (p̂, q̂)] ≡E[Dα (p̂, q̂)2]− E[Dα (p̂, q̂)]2
=V[Hα(P̂ )] +
1
4
V [Hα(p̂)] +
1
4
V [Hα(q̂)]
− Cov
[
Hα(P̂ ), Hα(p̂) +Hα(q̂)
]
,
(B7)
where Cov [X,Y ] ≡ E[XY ]−E[X]E[Y ]. We evaluate the
covariance-term in two different ways, i.e.
(1− α)2Cov
[
Hα(P̂ ), Hα(p̂) +Hα(q̂)
]
=
〈 ∑
i:p̂i+q̂i>0
P̂αi
 ∑
j:p̂j>0
p̂αj +
∑
j:q̂j>0
q̂αj
〉
−
〈 ∑
i:p̂i+q̂i>0
P̂αi
〉〈 ∑
j:p̂j>0
p̂αj
〉
+
〈 ∑
j:q̂j>0
q̂αj
〉
=
〈 ∑
i:p̂i+q̂i>0
P̂αi
∑
j:p̂j+q̂j>0
(
p̂αj + q̂
α
j
)〉
−
〈 ∑
i:p̂i+q̂i>0
P̂αi
〉〈 ∑
j:p̂j+q̂j>0
(
p̂αj + q̂
α
j
)〉
=
∑
i∈〈VP̂ 〉
{〈
P̂αi (p̂
α
i + q̂
α
i )
〉
−
〈
P̂αi
〉
(〈p̂αi 〉+ 〈q̂αi 〉)
}
(B8)
and
(1− α)2Cov
[
Hα(P̂ ), Hα(p̂) +Hα(q̂)
]
=
〈 ∑
i:p̂i+q̂i>0
P̂αi
∑
j:p̂j>0
p̂αj
〉
−
〈 ∑
i:p̂i+q̂i>0
P̂αi
〉〈 ∑
j:p̂j>0
p̂αj
〉
+
〈 ∑
i:p̂i+q̂i>0
P̂αi
∑
j:q̂j>0
q̂αj
〉
−
〈 ∑
i:p̂i+q̂i>0
P̂αi
〉〈 ∑
j:q̂j>0
q̂αj
〉
=
∑
i∈〈Vp̂〉
{〈
P̂αi p̂
α
i
〉
−
〈
P̂αi
〉
〈p̂αi 〉
}
+
∑
i∈〈Vq̂〉
{〈
P̂αi q̂
α
i
〉
−
〈
P̂αi
〉
〈q̂αi 〉
}
(B9)
Similarly as in Eq. (B3) we can approximate
〈
P̂αi
〉
≈Pαi +
α(α− 1)
4N
Pα−1i ,〈
P̂αi p̂
α
i
〉
≈Pαi pαi +
α
4N
(3α− 1)Pα−1i p
α
i
+
α
2N
(α− 1)Pαi pα−1i ,〈
P̂αi q̂
α
i
〉
≈Pαi qαi +
α
4N
(3α− 1)Pα−1i q
α
i
+
α
2N
(α− 1)Pαi qα−1i .
(B10)
11
From this we get for the variance of Dα
V[Dα(p̂, q̂)] =
∑
i∈〈VP̂ 〉
{
α2
(1− α)2
1
2N
Pα−1i
[
Pαi −
1
2
(pαi + q
α
i )
]
− α
2
16N2
Pα−1i
[
Pα−1i −
(
pα−1i + q
α−1
i
)]}
+
1
2
∑
i∈〈Vp̂〉
{
α2
(1− α)2
1
2N
pαi
[
pα−1i − P
α−1
i
]
− α
2
8N2
pα−1i
[
pα−1i − P
α−1
i
]}
+
1
2
∑
i∈〈Vq̂〉
{
α2
(1− α)2
1
2N
qαi
[
qα−1i − P
α−1
i
]
− α
2
8N2
qα−1i
[
qα−1i − P
α−1
i
]}
.
(B11)
Now we can see that for p = q = P we get
V[Dα(p̂, q̂)]p=q =
∑
i∈〈VP̂ 〉
1
16N2
α2p2α−2i =
α2
16N2
V
(2α−1)
P̂
.
(B12)
While for arbitrary p and q the variance of the Dα
contains the variances of the individual entropies (e.g.
V
(2α)
P̂
/N) and a covariance term, (only) in the special
case p = q all first-order terms (1/N) vanish yielding a
qualitatively different behaviour V
(2α−1)
P̂
/N2.
3. D̃α
The finite size estimation of D̃α can be obtained ap-
proximately by
D̃α(p̂, q̂) =
Dα(p̂, q̂)
Dα(p̂, q̂)max
≈ Dα(p̂, q̂)
E [Dmaxα (p̂, q̂)]
(B13)
such that
E
[
D̃α(p̂, q̂)
]
≈ E [Dα(p̂, q̂)]
E [Dmaxα (p̂, q̂)]
,
V
[
D̃α(p̂, q̂)
]
≈ V [Dα(p̂, q̂)]
E [Dmaxα (p̂, q̂)]
2 .
(B14)
The mean of Dmaxα is given according to Eq. (6) as a
linear combination of the individual entropies of p̂ and q̂
E [Dmaxα (p̂, q̂)]
=
21−α − 1
2
(
E [Hα (p̂)] + E [Hα (q̂)] +
2
1− α
)
.
(B15)
Appendix C: Derivation of Eq. (14)
In this section we derive the scaling of the general-
ized vocabulary V (α) defined in Eq. (13) assuming that
p is a power-law of the form pi ∝ i−γ , Eq. (1). In-
stead of looking at the probability of individual symbols
i, we consider the distribution of frequencies n, which
in this case yields p(n) ∝ n−1−1/γ [48]. Consider the
sum
∑
i∈V pi =
1
N
∑
i∈V ni =
1
N SV (γ), where SV (γ)
corresponds to the sum of V i.i.d. random variables ni
(i = 1, . . . , V ) drawn from the distribution p(n) It can be
shown that [49]
SV (γ) ∝
{
V γ , γ > 1,
V, γ < 1.
(C1)
The case γ = 1 includes additional logarithmic correc-
tions, but is not of relevance for the discussion, there-
fore, we leave it for sake of simplicity. In the same way,
we can treat
∑
i∈V p
µ
i =
1
Nµ
∑
i∈V n
µ
i =
1
NµSV (γµ) by
noting that SV (γµ) can be interpreted as the sum of V
i.i.d. random variables ni (i = 1, . . . , V ), where ni ∼ p̃(n)
with p̃(n) ∝ n−1−1/(γµ) such that we get
SV (γµ) ∝
{
V γµ, µ < 1/γ,
V, µ > 1/γ.
(C2)
By setting µ = α − 1 in Eq. (13) and noting that for
pi ∝ i−γ , Eq. (1), the number of different symbols scales
as V ∝ N1/γ , Eq. (10), we obtain Eq. (14).
[1] S. Kullback, Information Theory and Statistics (Wiley,
New York, 1959).
[2] C. D. Manning and H. Schütze, Foundations of Statistical
Natural Language Processing (MIT Press, 1999).
12
[3] I. Grosse, P. Bernaola-Galván, P. Carpena, R. Román-
Roldán, J. Oliver, and H. Stanley, Physical Review E
65, 041905 (2002).
[4] Yun He, A. Ben Hamza, and H. Krim, IEEE Transac-
tions on Signal Processing 51, 1211 (2003).
[5] K. W. Boyack, D. Newman, R. J. Duhon, R. Klavans,
M. Patek, J. R. Biberstine, B. Schijvenaars, A. Skupin,
N. Ma, and K. Börner, PloS one 6, e18029 (2011).
[6] A. P. Masucci, A. Kalampokis, V. M. Egúıluz, and
E. Hernández-Garćıa, PloS one 6, e17333 (2011).
[7] V. Bochkarev, V. Solovyev, and S. Wichmann, Journal
of The Royal Society Interface 11, 20140841 (2014).
[8] E. A. Pechenick, C. M. Danforth, and P. S. Dodds,
arXiv:1503.03512 (2015).
[9] G. Cocho, J. Flores, C. Gershenson, C. Pineda, and
S. Sánchez, PLOS ONE 10, e0121898 (2015).
[10] M. Gerlach and E. G. Altmann, Physical Review X 3,
021006 (2013).
[11] J.-B. Michel, Y. K. Shen, A. P. Aiden, A. Veres, M. K.
Gray, J. P. Pickett, D. Hoiberg, D. Clancy, P. Norvig,
J. Orwant, S. Pinker, M. A. Nowak, and E. L. Aiden,
Science 331, 176 (2011).
[12] G. K. Zipf, The Psycho-Biology of Language (Routledge,
1936).
[13] R. H. Baayen, Word Frequency Distributions (Springer
Netherlands, 2001).
[14] R. Ferrer i Cancho, The European Physical Journal B
44, 249 (2005).
[15] M. A. Serrano, A. Flammini, and F. Menczer, PloS one
4, e5372 (2009).
[16] S. K. Baek, S. Bernhardsson, and P. Minnhagen, New
Journal of Physics 13, 043004 (2011).
[17] R. Mantegna, S. Buldyrev, A. Goldberger, S. Havlin,
C. Peng, M. Simons, and H. Stanley, Physical Review
Letters 73, 3169 (1994).
[18] C. Furusawa and K. Kaneko, Physical Review Letters 90
(2003), 10.1103/PhysRevLett.90.088102.
[19] J. Serrà, A. Corral, M. Boguñá, M. Haro, and J. L.
Arcos, Sci. Rep. 2, 521 (2012).
[20] J. Lin, IEEE Transactions on Information Theory 37,
145 (1991).
[21] T. M. Cover and J. A. Thomas, Elements of Information
Theory (Wiley-Interscience, 2006).
[22] D. Endres and J. Schindelin, IEEE Transactions on In-
formation Theory 49, 1858 (2003).
[23] M. A. Montemurro and D. H. Zanette, Advances in Com-
plex Systems 13, 135 (2010).
[24] J. Havrda and F. Charvát, Kybernetika 3, 30 (1967).
[25] J. Burbea and C. Rao, IEEE Transactions on Information
Theory 28, 489 (1982).
[26] C. Tsallis, Journal of Statistical Physics 52, 479 (1988).
[27] A. Rényi, in Berkeley Symposium on Mathematical
Statistics and Probability (University of California Press,
1961).
[28] J. Briët and P. Harremoës, Physical Review A 79, 052311
(2009).
[29] T. Schürmann, Journal of Physics A: Mathematical and
General , 5 (2004).
[30] G. Herdan, Type-token mathematics (Mouton, 1960).
[31] H. S. Heaps, Information Retrieval (Academic Press,
1978).
[32] M. Gerlach and E. G. Altmann, New Journal of Physics
16, 113010 (2014).
[33] T. D. de Wit, The European Physical Journal B - Con-
densed Matter and Complex Systems 11, 513 (1999).
[34] G. A. Miller, in Information theory in psychology: Prob-
lems and methods, edited by H. Quastler (Free Press,
1955).
[35] G. P. Basharin, Theory of Probability & Its Applications
4, 333 (1959).
[36] B. Harris, Colloquia Mathematica Societatis János
Bolyai: Topics in Information Theory 16, 323 (1975).
[37] H. Herzel, A. Schmitt, and W. Ebeling, Chaos, Solitons
& Fractals 4, 97 (1994).
[38] P. Grassberger, arXiv:physics/0307138 (2008).
[39] E. Archer, I. M. Park, and J. W. Pillow, Journal of
Machine Learning Research 15, 2833 (2014).
[40] I. Nemenman, F. Shafee, and W. Bialek, in Advances in
Neural Information Processing Systems 14 (MIT Press,
2002).
[41] For small values of N , the Bayesian estimates of the in-
dividual entropies in some cases yield D̃(p̂, q̂) < 0 such
that we consider averages over |D̃(p̂, q̂)|.
[42] P. Bernaola-Galván, I. Grosse, P. Carpena, J. L. Oliver,
R. Román-Roldán, and H. E. Stanley, Physical Review
Letters 85, 1342 (2000).
[43] We use the Google-ngram corpus containing the fre-
quency of usage of millions of words from 4% of all books
ever published in the English language with a yearly res-
olution [11].
[44] M. Pagel, Q. D. Atkinson, and A. Meade, Nature 449,
717 (2007).
[45] E. Lieberman, J.-B. Michel, J. Jackson, T. Tang, and
M. A. Nowak, Nature 449, 713 (2007).
[46] T. Yasseri, A. Kornai, and J. Kertész, PLoS ONE 7,
e48386 (2012).
[47] E. Stamatatos, Journal of the American Society for In-
formation Science and Technology 60, 538 (2009).
[48] M. Newman, Contemporary Physics 46, 323 (2005).
[49] J.-P. Bouchaud and A. Georges, Reviews of Modern
Physics 195, 127 (1990).
