On the similarity of symbol frequency distributions with heavy tails
Martin Gerlach,1 Francesc Font-Clos,2, 3 and Eduardo G. Altmann1
1Max Planck Institute for the Physics of Complex Systems, D-01187 Dresden, Germany
2Centre de Recerca MatemaÌ€tica, Edifici C, Campus Bellaterra, E-08193 Barcelona, Spain.
3Departament de MatemaÌ€tiques, Facultat de CieÌ€ncies,
Universitat AutoÌ€noma de Barcelona, E-08193 Barcelona, Spain
October 2, 2015
Quantifying the similarity between symbolic sequences is a traditional problem in Information The-
ory which requires comparing the frequencies of symbols in different sequences. In numerous modern
applications, ranging from DNA over music to texts, the distribution of symbol frequencies is char-
acterized by heavy-tailed distributions (e.g., Zipfâ€™s law). The large number of low-frequency symbols
in these distributions poses major difficulties to the estimation of the similarity between sequences,
e.g., they hinder an accurate finite-size estimation of entropies. Here we show how the accuracy
of estimations depend on the sample size N , not only for the Shannon entropy (Î± = 1) and its
corresponding similarity measures (e.g., the Jensen-Shanon divergence) but also for measures based
on the generalized entropy of order Î±. For small Î±â€™s, including Î± = 1, the bias and fluctuations
in the estimations decay slower than the 1/N decay observed in short-tailed distributions. For Î±
larger than a critical value Î±âˆ— â‰¤ 2, the 1/N -scaling is recovered. We show the practical significance
of our results by quantifying the evolution of the English language over the last two centuries using
a complete Î±-spectrum of measures. We find that frequent words change more slowly than less
frequent words and that Î± = 2 provides the most robust measure to quantify language change.
I. INTRODUCTION
Quantifying the similarity of symbolic sequences is a
classical problem in information theory [1] with modern
applications in linguistics [2], genetics [3], and image pro-
cessing [4]. The availability of large databases of texts
sparked a renewed interest in the problem of similarity of
the vocabulary of different collections of texts [5â€“9]. For
instance, Fig. 1 shows the word-frequency distribution in
three large collections of English texts: from 1850, 1900,
and 1950. We see that the distribution itself remains
essentially the same, a heavy-tailed Zipf distribution [10]
p(r) âˆ râˆ’Î³ , (1)
where p is the frequency of the r-th most frequent word
and Î³ ' 1. Changes are seen in the frequency p (or
rank) of specific words, e.g., ship lost and genetic won
popularity. Measures that quantify such changes are es-
sential to answer questions such as: Is the vocabulary
from 1900 more similar to the one from 1850 or to the
one from 1950? How similar are two vocabularies (e.g.,
from different years)? Are the two finite-size observations
compatible with a finite sample of the same underlying
vocabulary? How similar are the vocabulary of different
authors or disciplines? How fast is the lexical change
taking place?
Heavy-tailed and broad distributions of symbol-
frequencies such as Eq. (1) are typical in natural lan-
guages [10, 12â€“16] and appear also in the DNA (n-grams
of base pairs for large n) [17], in gene expression [18], and
in music [19]. The slow decay observed in a broad range
of frequencies implies that there is no typical frequency
for words and therefore relevant changes can occur in
different ranges of the p-spectrum, from the few large-
100 101 102 103 104 105 106
ranks: r
10âˆ’8
10âˆ’7
10âˆ’6
10âˆ’5
10âˆ’4
10âˆ’3
10âˆ’2
10âˆ’1
100
fre
qu
en
cy
:
p
(r
)
t = 1950
t = 1900
t = 1850
â—¦: â€œandâ€
: â€œseeâ€
O: â€œshipâ€
M: â€œgeneticâ€
FIG. 1. The English vocabulary in different years. Rank-
frequency distribution p(r) of individual years t for t = 1850,
1900, and 1950 of the Google-ngram database [11], multi-
plied by a factor of 1, 2, and 4, respectively, for better vi-
sual comparison. The inset shows the original un-transformed
data (same axis), highlighting that the rank-frequency distri-
butions are almost indistinguishable. Individual words (e.g.
â€œandâ€,â€œseeâ€,â€œshipâ€,â€œgeneticâ€) show changes in rank and fre-
quency (symbols), where words with larger ranks (i.e. smaller
frequencies) show larger change.
frequency words all the way to the many low-frequency
words. This imposes a challenge to define similarity mea-
sures that are able to account for this variability and that
also yield accurate estimations based on finite-size obser-
vations.
In this paper we quantify the vocabulary similarity us-
ing a spectrum of measures DÎ± based on the general-
ized entropy of order Î± (DÎ±=1 recovers the usual Jensen-
Shannon divergence). We show how varying Î± magnifies
ar
X
iv
:1
51
0.
00
27
7v
1 
 [
ph
ys
ic
s.
so
c-
ph
] 
 1
 O
ct
 2
01
5
2
differences in the vocabulary at different scales of the
(heavy-tailed) frequency spectrum, thus providing dif-
ferent information on the vocabulary change. We then
compute the accuracy (bias) and precision (variance) of
estimations of DÎ± based on sequences of size N and
find that in heavy-tailed distributions the convergence
is much slower than in non-heavy-tailed distributions (it
often scales as 1/NÎ² with Î² < 1). Finally, we come back
to the problem of comparing the English vocabulary in
the last two centuries in order to illustrate the relevance
of our general results.
II. DEFINITION
Consider the probability distribution p =
(p1, p2, . . . , pS) of a random variable over a discrete,
countable set of symbols i = 1, . . . , S (where later we
include the possibility for S â†’âˆ). From an information
theory standpoint, a natural measure to quantify the
difference between two such probability distributions p
and q is the Jensen-Shannon divergence (JSD) [20]
D(p, q) = H
(
p + q
2
)
âˆ’ 1
2
H(p)âˆ’ 1
2
H(q), (2)
where H is the Shannon entropy [21]
H(p) = âˆ’
âˆ‘
i
pi log pi. (3)
This definition has several properties which are useful in
the interpretation as a distance: i) D(p, q) â‰¥ 0 where the
equality holds if and only if p = q; ii) D(p, q) = D(q,p)
(it is a symmetrized Kullback-Leiber-divergence [20]); iii)âˆš
D(p, q) fulfills the triangle inequality and thus is a
metric [22]; and iv) D(p, q) equals the mutual informa-
tion of variables sampled from p and q [3], i.e., D(p, q)
equals the average amount of information in one ran-
domly sampled word-token about which of the two dis-
tribution it was sampled from [23]. The JSD is widely
used in the statistical analysis of language [2], e.g. to au-
tomatically find individual documents that are (seman-
tically) related [5, 6] or to track the rate of evolution in
the lexical inventory of a language over historical time
scales [7, 8].
Here we also consider the generalization of JSD in
which H in Eq. (3) is replaced by the generalized entropy
of order Î± [24]
HÎ±(p) =
1
1âˆ’ Î±
(âˆ‘
i
pÎ±i âˆ’ 1
)
, (4)
yielding a spectrum of divergence measures DÎ± param-
eterized by Î±, first introduced in Ref. [25]. The usual
JSD is retrieved for Î± = 1. The suitability of Eq. (4) to
describe physical systems is the subject of investigation
of non-extensive statistical mechanics as first proposed in
Ref. [26]. While similar generalizations can be achieved
with other formulations of generalized entropies such as
the Renyi-entropy [4, 27], the corresponding divergences
can become negative. In contrast, DÎ± is strictly non-
negative and it was recently shown that
âˆš
DÎ±(p, q) is a
metric for any Î± âˆˆ (0, 2] [28]. For heavy-tailed distribu-
tions, Eq. (1), HÎ± <âˆ for Î± > 1/Î³.
We define a normalized version of DÎ± as
DÌƒÎ±(p, q) =
DÎ±(p, q)
DmaxÎ± (p, q)
(5)
where
DmaxÎ± (p, q) =
21âˆ’Î± âˆ’ 1
2
(
HÎ± (p) +HÎ± (q) +
2
1âˆ’ Î±
)
.
(6)
is the maximum possible DÎ± between p and q obtained
assuming that the the set of symbols in each distribution
(i.e., the support of p and q) are disjoint. The main moti-
vation for using the measure (5) is that DÌƒÎ±(p, q) âˆˆ [0, 1],
while the range of admissible values of DÎ± depends on
Î±. This allows for a meaningful comparison of the diver-
gences DÌƒÎ±(p, q) and DÌƒÎ±â€²(p, q) for Î± 6= Î±â€² and therefore
also for the full spectrum of Î±â€™s. In general, the metric
properties of DÎ± are not preserved by DÌƒÎ±. An exception
is the case in which the rank-frequency distribution p(r)
underlying all pâ€™s and qâ€™s is invariant (see Fig. 1). Noting
that Eq. (6) is independent of the symbols we obtain that
DmaxÎ± (p, q) is a constant for all pâ€™s and qâ€™s and therefore
the metric property is preserved for DÌƒÎ±.
III. INTERPRETATION
In order to clarify the interpretation of DÎ±, it is use-
ful to consider a toy model. As in Fig. 1, we consider
two distributions p and q that have exactly the same
rank-frequency distribution p(r) but differ in (a sub-
set of) the symbols they use. For simplicity, we con-
sider that symbols that differ in the two cases appear
only in one of the distributions. More precisely, denot-
ing by Ip = {A,B,C,D,E, . . .} the set of symbols in
p with probabilities pi and i âˆˆ Ip, we replace a sub-
set Iâˆ— âŠ‚ Ip of symbols in q by a new symbol with the
same probability (this ensures that the rank-frequency
distribution is conserved). Thus the set of symbols in
q is Iq = {i|i âˆˆ Ip \ Iâˆ—} âˆª {iâ€ |i âˆˆ Iâˆ—} with pi = qi for
i âˆˆ Ip \ Iâˆ— and pi = qiâ€  for i âˆˆ Iâˆ—, see Fig. 2 for one
example.
For a given distribution p and a set of replaced symbols
Iâˆ—, we compute DÎ±(p, Iâˆ—) â‰¡ DÎ±(p, q) as
DÎ±(p, I
âˆ—) = cÎ±
âˆ‘
iâˆˆIâˆ—
pÎ±i , (7)
where cÎ± = (2
(1âˆ’Î±) âˆ’ 1)/(1âˆ’Î±). The maximum is given
by
DmaxÎ± (p, I
âˆ—) = cÎ±
âˆ‘
iâˆˆIp
pÎ±i (8)
3
such that
DÌƒÎ±(p, I
âˆ—) =
âˆ‘
iâˆˆIâˆ— p
Î±
iâˆ‘
iâˆˆIp p
Î±
i
. (9)
This shows that each symbol i âˆˆ Iâˆ— that is replaced by
a new symbol contributes pÎ±i to DÎ±. It is thus clear
that varying Î±, the contribution of different frequencies
become magnified (e.g. for Î±  1 large frequencies are
enhanced while for Î± < 0 low frequencies contribute more
to DÎ± than large frequencies).
In particular, for Î± = 0, DÌƒÎ±=0(p, I
âˆ—) = |I
âˆ—|
|Ip| is the frac-
tion of symbols (types) that are different in p and q. Each
symbol i counts the same irrespective of their probabili-
ties pi. For
|Iâˆ—|
|Ip|  1, DÌƒÎ±=0(p, I
âˆ—) = 1âˆ’ J(Ip, Iq), where
J(Ip, Iq) =
|Ipâˆ©Iq|
|IpâˆªIq| is the Jaccard-coefficient between the
two sets Ip and Iq, an ad-hoc defined similarity mea-
sure widely used in information retrieval [2]. For Î± = 1,
DÌƒÎ±=1(p, I
âˆ—) =
âˆ‘
iâˆˆIâˆ— pi showing that each replaced sym-
bol is weighted by its probability pi and thus that DÌƒÎ±=1
measures the distance in terms of tokens.
The full spectrum DÌƒÎ± offers information on changes in
all frequencies, a point which is particularly important
for the case of heavy-tailed distributions because word-
frequencies vary over many orders of magnitude. Figure 3
illustrates how different values of Î± are able to capture
changes at different regions in the frequency-spectrum.
In particular, it shows that DÌƒÎ± grows (decays) with Î±
when the modified symbols have high (low) frequency.
Furthermore, the comparison between two given changes
allow us to conclude about which change was more sig-
nificant at different regions of the word-frequency spec-
trum. In the example of the figure, both changes (the
two lines) are equally significant from the point of view
of the modified tokens (DÌƒ1 are the same), the change in
the left affects more types (DÌƒ0 is larger), and the change
in the right affects more frequent words (DÌƒÎ± is larger for
Î± 1).
A B C D E F G H
symbol: i
0.0
0.1
0.2
0.3
0.4
0.5
p
i
A Bâ€  C D E F G H
symbol: i
0.0
0.1
0.2
0.3
0.4
0.5
q i
FIG. 2. Illustration of our toy model where p (left) and q
(right) have the same rank-frequency distribution, but differ
in the probability for individual symbols. In this example, p
and q are the same (pi = qi) for i âˆˆ {A,C,D,E, F,G,H},
while the symbol i = B in p is replaced by i = Bâ€  in q with
pi=B = qi=Bâ€  and pi=Bâ€  = qi=B = 0.
âˆ’2 âˆ’1 0 1 2 3 4
Î±
0.0
0.2
0.4
0.6
0.8
1.0
DÌƒ
Î±
(p
,I
âˆ— )
1 103i
10âˆ’5
1
p
i
1 103i
10âˆ’5
1
p
i
FIG. 3. The spectrum DÌƒÎ±(p, I
âˆ—) for two different changes.
The lines correspond to Eq. (9) with pi âˆ iâˆ’1 with i =
1, 2, . . . , 1000 and two different sets of replaced symbols Iâˆ—1 , I
âˆ—
2 .
Right inset: Iâˆ—1 = {1}, i.e., only the symbol with the high-
est probability, pi=1 â‰ˆ 0.13 is changed. Left inset: Iâˆ—2 =
{368, . . . , 1000}, i.e, the symbols with small probability are re-
placed. The choice of Iâˆ—2 was made such that
âˆ‘
iâˆˆIâˆ—2
pi â‰ˆ pi=1
and therefore DÌƒÎ±=1(p, I
âˆ—
1 ) â‰ˆ DÌƒÎ±=1(p, Iâˆ—2 ).
IV. FINITE-SIZE ESTIMATION
In this section we turn to the estimation of DÌƒÎ± from
data. Even if DÌƒÎ± is defined with respect to distributions
p and q, Eq. (5), in practice these distributions are es-
timated from sequences with finite-size N (total number
of symbols or word tokens) yielding finite size estimates
of the distributions pÌ‚ and qÌ‚. The main obstacle in ob-
taining accurate estimates of DÌƒÎ± is that it requires the
estimation of entropies for which, in general, unbiased
estimators do not exist [29]. Accordingly, even if p = q,
in practice HÎ±(pÌ‚) 6= HÎ±(qÌ‚) and DÌƒÎ±(pÌ‚, qÌ‚) > 0 are mea-
sured not only in single realizations, but also on average
(the bias). Besides the bias, we are also interested in
the expected fluctuation (standard deviation) of the es-
timations of HÎ± and DÌƒÎ± and how both they depend on
the sequence size N for large N . In heavy-tailed distri-
butions such as Eq. (1), these estimations are based on
an observed vocabulary V (number of different symbols)
that grows sub-linearly with N as [30â€“32]
V (N) âˆ N1/Î³ . (10)
This implies that the entropies in Eq. (4) are estimated
based on a sum of V â†’ âˆ terms (for N â†’ âˆ). In
practice, Î³ and the precise functional form of the heavy-
tailed distribution are unknown and therefore, besides
DÌƒÎ±, the estimation of HÎ± is also of interest (see Ref. [33]
for the case in which a power-law form of p is assumed
to be known a priori).
A. Analytical Calculations
Here we extend previous results [34â€“37] and general-
ize them to arbitrary Î±. Given a probability distribution
4
p and the measured probabilities pÌ‚ from a finite sam-
ple of N word-tokens, we expand HÎ±(pÌ‚) around the true
probabilities pi up to second order as
HÎ±(pÌ‚) â‰ˆ HÎ±(p)+
âˆ‘
i:pÌ‚i>0
(pÌ‚i âˆ’ pi)
Î±
1âˆ’ Î±
pÎ±âˆ’1i
âˆ’1
2
âˆ‘
i:pÌ‚i>0
(pÌ‚i âˆ’ pi)2Î±pÎ±âˆ’2i
(11)
where we used that âˆ‚HÎ±âˆ‚pi = Î±/(1âˆ’ Î±)p
Î±âˆ’1
i and
âˆ‚2H
âˆ‚piâˆ‚pj
=
âˆ’Î±pÎ±âˆ’2i Î´i,j . We then calculate E [HÎ±(pÌ‚)] by averaging
over the different realization of the random variables pÌ‚i
by assuming that the absolute frequency of each symbol i
is drawn from an independent binomial with probability
pi such that E [pÌ‚i] = pi and V [pÌ‚i] = pi(1âˆ’pi)/N â‰ˆ pi/N
yielding
E [HÎ±(pÌ‚)] â‰ˆ HÎ±(p)âˆ’
Î±
2N
âˆ‘
iâˆˆV
pÎ±âˆ’1i = HÎ±(p)âˆ’
Î±V (Î±)
2N
,
(12)
which defines the vocabulary size of order Î±
V (Î±) â‰¡
âˆ‘
iâˆˆV
pÎ±âˆ’1i . (13)
From Eq. (12) we see that the bias in the entropy estima-
tion |HÎ±(p) âˆ’ E [HÎ±(pÌ‚)] | depends only on V (Î±) and N .
Similar calculations (see Appendix B) show that the large
N behavior of the bias and the fluctuations (variance) of
HÎ±, DÎ±, and DÌƒÎ± can be written as simple functions of
V (Î±) and N , as summarized in Tab. I.
HÎ± DÎ±, DÌƒÎ±(p 6= q) DÎ±, DÌƒÎ±(p = q)
Bias: V (Î±)/N V (Î±)/N V (Î±)/N
Fluctuations: V (2Î±)/N V (2Î±)/N V (2Î±âˆ’1)/N2
TABLE I. Scaling of the bias |E[XÌ‚] âˆ’ X| and the fluctua-
tions V[X] â‰¡ E[XÌ‚2] âˆ’ E[XÌ‚]2 of estimations XÌ‚. The results
are valid for large sequence sizes N and depend on the vo-
cabulary of order Î±, V (Î±) as in Eqs. (13) and (14). Results
are shown for X = HÎ± [order Î± entropy, Eq. (4)], DÎ± [gen-
eralized divergence], DÌƒÎ± [normalized divergence, Eq. (5)], see
Appendix B for the derivations. For DÌƒÎ±, we approximate
DÌƒÎ± â‰ˆ DÎ±/E[DmaxÎ± ].
We now focus on the dependence of V (Î±) on N . The
sum
âˆ‘
iâˆˆV in Eq. (13) indicates that in N samples, on
average, V = V (N) â‰¡ V (Î±=1) different symbols are ob-
served. If for N â†’ âˆ the vocabulary V converges to
a finite value, V (Î±) in Eq. (13) also converges and the
bias scales as 1/N . A more interesting scenario happens
when V grows with N . For the heavy-tailed case of in-
terest here, V grows as N1/Î³ , Eq. (10), and we obtain
(in Appendix C) that V (Î±) scales for large N as
V (Î±) âˆ
{
Nâˆ’Î±+1+1/Î³ , Î± < 1 + 1/Î³,
constant , Î± > 1 + 1/Î³,
(14)
where Î³ > 1 is the Zipf exponent defined in Eq. (1) and
Î± is the order of the entropy in Eq. (4).
From the combination of Eq. (14) and Tab. I we ob-
tain the scalings with sequence size N of the estimators
of HÎ±, DÎ±, and DÌƒÎ± in a heavy-tailed distribution with
exponent Î³. These scalings are summarized in Tab. II.
Three scaling regimes can be identified for the bias and
for the fluctuations. (i) For large Î±, the decay is 1/N
(except when p = q, where the fluctuations decay even
faster as 1/N2) as in the case of a finite vocabulary and
short-tailed distributions. (ii) For intermediate Î±, a sub-
linear decay with N is observed. This regime appears
exclusively in heavy-tailed distributions and has impor-
tant consequences in real applications, as shown below.
From the exponents of the sub-linear decay we see that
the bias decays more slowly than the fluctuations. (iii)
For small Î±, Î± < 1/Î³, HÎ±(p) is not defined thus the esti-
mator for the mean of HÎ± and DÎ± diverge. The growth
of HÎ± (and therefore D
max
Î± ) and DÎ± with N have the
same scaling and therefore cancel each other for DÌƒÎ±, in
which case a convergence to a well defined value is found
(the fluctuation of DÌƒÎ± still decays in this regime).
B. Numerical Simulations
Here we perform numerical estimations of the normal-
ized divergence spectrum DÌƒÎ± that illustrate the regimes
derived above, confirm the validity of the approximations
used in their derivations, and show that the same scalings
are observed for different entropy estimators. We sam-
ple twice N symbols (tokens) from the same distribution
(p = q), and therefore DÌƒÎ± = 0 and the expected value
E[DÌƒÎ±(pÌ‚, qÌ‚)] is the bias. (The fact that the bias shows a
slower decay with N than the fluctuations makes these
two effects distinguishable also in this DÌƒÎ± = 0 case be-
cause E[DÌƒÎ±(pÌ‚, qÌ‚)] V[DÌƒÎ±(pÌ‚, qÌ‚)] for large N).
We start with the most important prediction of our
analytical calculations above: the existence in heavy-
tailed distributions of a regime for which the bias and
fluctuations of DÌƒÎ± decay with N more slowly than 1/N .
This holds already for Î± = 1, i.e., for the usual Jensen-
Shannon divergence, previously shown for the bias of
HÎ±=1 in Ref. [37]. One potential limitation of our an-
alytical calculations is that they are based on the plugin-
estimator obtained from replacing the piâ€™s in the general-
ized entropies, Eq. (4), by the measured frequencies (i.e.
pi 7â†’ pÌ‚i = Ni/N , with Ni being the number of observed
word tokens of type i). To test the generality of our re-
sults, in the numerical simulations we use four different
estimators of the Shannon entropy (i.e., Î± = 1): i) the
Plugin-estimator; ii) Miller â€™s-estimator [34], which takes
into account the approximation obtained from the expan-
sion in Eq. (12); iii) Grassberger â€™s estimator [38] based on
the assumption that frequencies are Poisson-distributed;
and iv) a recently proposed Bayesian estimator described
in [39] which is an extension of the approach by Nemen-
man et al. [40] to the case where the number of possible
5
E[HÎ±(pÌ‚)] E[DÎ±(pÌ‚, qÌ‚)] E[DÌƒÎ±(pÌ‚, qÌ‚)]
Î±E1 1/Î³ 1/Î³ 1/Î³
Î±E2 1 + 1/Î³ 1 + 1/Î³ 1 + 1/Î³
Î± < Î±E1 cN
âˆ’Î±+1/Î³ cNâˆ’Î±+1/Î³ c
Î±E1 < Î± < Î±
E
2 HÎ±(p) + cN
âˆ’Î±+1/Î³ DÎ±(p, q) + cN
âˆ’Î±+1/Î³ DÌƒÎ±(p, q) + cN
âˆ’Î±+1/Î³
Î± > Î±E2 HÎ±(p) + cN
âˆ’1 DÎ±(p, q) + cN
âˆ’1 DÌƒÎ±(p, q) + cN
âˆ’1
V[HÎ±(pÌ‚)] V[DÎ±(pÌ‚, qÌ‚)] V[DÌƒÎ±(pÌ‚, qÌ‚)]
p 6= q p = q p 6= q p = q
Î±V1 1/(2Î³) 1/(2Î³) 1/(2Î³) 1/Î³ 1/Î³
Î±V2
1
2
(1 + 1/Î³) 1
2
(1 + 1/Î³) 1 + 1/(2Î³) 1
2
(1 + 1/Î³) 1 + 1/(2Î³)
Î± < Î±V1 cN
âˆ’2Î±+1/Î³ cNâˆ’2Î±+1/Î³ cNâˆ’2Î±+1/Î³ cNâˆ’1/Î³ cNâˆ’1/Î³
Î±V1 < Î± < Î±
V
2 cN
âˆ’2Î±+1/Î³ cNâˆ’2Î±+1/Î³ cNâˆ’2Î±+1/Î³ cNâˆ’2Î±+1/Î³ cNâˆ’2Î±+1/Î³
Î± > Î±V2 cN
âˆ’1 cNâˆ’1 cNâˆ’2 cNâˆ’1 cNâˆ’2
TABLE II. Summary of finite size scaling for distributions with heavy tails. Mean (E) and variance (V) of the plug-in
estimator of HÎ±, DÎ±, and DÌƒÎ± for samples pÌ‚ and qÌ‚ each of size N drawn randomly from p and q with power-law rank-frequency
distributions with exponent Î³ > 1, Eq. (1). The results are obtained combining Tab. I with Eq. (14) (for details see Appendix B,
C). The constant c depends on Î± and has a different value in each case but is independent of N . The limit Î³ â†’âˆ corresponds
to the case in which both p and q have short tails.
10âˆ’4
10âˆ’3
10âˆ’2
10âˆ’1
100
B
ia
s
E[
DÌƒ
Î±
=
1
(pÌ‚
,qÌ‚
)]
âˆ Nâˆ’1
(a)
[Exponential]
100 101 102 103 104
Sample size: N
10âˆ’8
10âˆ’6
10âˆ’4
10âˆ’2
100
Fl
uc
tu
at
io
ns
V[
DÌƒ
Î±
=
1
(pÌ‚
,qÌ‚
)]
âˆ Nâˆ’2
(d)
âˆ Nâˆ’1+1/Î³
âˆ Nâˆ’1
(b)
[Power law]
100 101 102 103 104
Sample size: N
âˆ Nâˆ’2
âˆ Nâˆ’2+1/Î³
(e)
âˆ Nâˆ’1
(c)
[Empirical]
Plugin
Miller
Grassberger
Bayesian
100 101 102 103 104
Sample size: N
âˆ Nâˆ’2
(f)
FIG. 4. Finite-size estimation of the normalized Jensen-Shannon divergence DÌƒ = DÌƒÎ±=1. (a-c) Estimation of E[DÌƒ(pÌ‚, qÌ‚)] between
two sequences of size N drawn from the same distribution (i.e. D(p, q) = 0) using four different estimators of the entropy (see
text) for three representative distributions: (a) Exponential (short-tailed) distribution pi âˆ eâˆ’ai for i = 0, 1, . . . with a = 0.1;
(b) Power-law (heavy-tailed) distribution pi âˆ iâˆ’Î³ for i = 1, 2, . . . with Î³ = 3/2; (c) Empirical Zipf-distribution of word
frequencies, i.e. rank-frequency distribution p(r) from the complete Google-ngram data, pi = f(i = r) for i = 1, . . . , 4623568,
which is well described by a double power-law [10]. (d-f) Show the same as (a-c) for the fluctuations V[DÌƒ(pÌ‚, qÌ‚)]. The dotted
lines show the expected scalings from Tab. II for short-tailed distributions, i.e. Nâˆ’1 (Nâˆ’2), and power-law distributions, i.e.
Nâˆ’1+1/Î³ (Nâˆ’2+1/Î³), for the bias (fluctuations). In (c) we show the expected scaling for the bias, Vemp(N)/N , where Vemp(N)
is the expected number of different symbols in a random sample of size N from the empirical distribution [32]. Averages are
taken over 1000 realizations.
6
10âˆ’6
10âˆ’5
10âˆ’4
10âˆ’3
10âˆ’2
10âˆ’1
100
B
ia
s
E[
DÌƒ
Î±
(pÌ‚
,qÌ‚
)]
âˆ Nâˆ’1
âˆ const.
(a)
100 101 102 103 104 105 106
Sample size: N
10âˆ’12
10âˆ’10
10âˆ’8
10âˆ’6
10âˆ’4
10âˆ’2
100
Fl
uc
tu
at
io
ns
V[
DÌƒ
Î±
(pÌ‚
,qÌ‚
)]
âˆ Nâˆ’1/Î³
âˆ Nâˆ’2
(c)
0.0
0.5
1.0
1.5
2.0
Î±
0.5
0.6
0.7
0.8
0.9
1.0
E[
DÌƒ
Î±
](
2
N
)/
E[
DÌƒ
Î±
](
N
)
(b)
Î±E1 Î±
E
2
N = 25
N = 210
N = 215
N = 219
0.0 0.5 1.0 1.5 2.0
Î±
0.2
0.3
0.4
0.5
0.6
0.7
V[
DÌƒ
Î±
](
2
N
)/
V[
DÌƒ
Î±
](
N
)
(d)
Î±V1 Î±
V
2
N = 25
N = 210
N = 215
N = 219
FIG. 5. Bias (a,b) and fluctuations (c,d) in finite-size estimation of DÌƒÎ±. Estimation of E[DÌƒÎ±(pÌ‚, qÌ‚)] between two sequences
each of size N drawn numerically from the same power-law distribution pi âˆ iâˆ’Î³ for i = 1, 2, . . . , V â†’ âˆ with Î³ = 3/2 using
the plugin-estimator (pi 7â†’ pÌ‚i) for the entropies of order Î±. (a) Scaling of the bias with N for different Î±. (b) Decrease of the
bias in DÌƒÎ± when sample size is doubled (N 7â†’ 2N) for different values of N as a function of Î±. (c) and (d) show the same as
(a) and (b) for the fluctuations V[DÌƒÎ±(pÌ‚, qÌ‚)]. Red lines in all plots indicate the borders between the regimes, Î±E1 = 1/Î³ = 2/3,
Î±E2 = 1 + 1/Î³ = 5/3 (for the bias in a,b), and Î±
V
1 = 1/Î³ = 2/3, Î±
V
2 = 1 + 1/(2Î³) = 4/3 (for the fluctuations in c,d). Dotted
lines indicate the predictions based on Tab. I for Î± < Î±E1 , Î±
V
1 and Î± > Î±
E
2 , Î±
V
2 (in a,c) and all values of Î± (in b,d). Averages are
taken over 1000 realizations.
symbols is unknown or even countably infinite [41]. The
numerical results in Fig. 4 show that the different estima-
tors are indeed able to reduce the bias of the estimation,
but that the scaling of the bias with N remains the same.
In particular, the transition from short-tailed to heavy-
tailed distribution leads to the predicted transition from
Nâˆ’1 (Nâˆ’2) to the slower Nâˆ’1+1/Î³ (Nâˆ’2+1/Î³) decay for
the bias (fluctuations) for all estimators. The only ex-
ception is in the bias of the Bayesian estimator for the
exact Zipfâ€™s law (1), but since this estimator shows a
bad performance for the fluctuation and for the real data
we conclude that the slower scaling should be expected
in general also for this elaborated estimator. These re-
sults confirm the generality of our finding that the bias
and fluctuation in DÌƒÎ±=1 decays more slowly than 1/N in
heavy-tailed distributions. The consequence of this result
to applications will be discussed in the next section.
We now consider the estimation of DÌƒÎ± for Î± 6= 1 in
the case of heavy-tailed distributions (1). The numer-
ical results in Fig. 5 confirm the existence of the three
scaling regimes discussed after Eq. (14) and in Tab. II.
The panels (b) and (d) show the relative reduction in the
bias and fluctuations achieved when the sequence size
is doubled. For many different Î±â€™s the relative reduc-
tion is larger than 0.5 (0.25) for the bias (fluctuations),
a consequence of the slow decay with N that shows the
difficulty in obtaining a good estimation of DÌƒÎ±. In prac-
tice, the exponent Î³ of the distribution is unknown such
that the critical values of Î± that separates these regimes
(e.g. Î±E1 = 1/Î³ and Î±
E
2 = 1 + 1/Î³ for the bias) cannot
be determined a priori. Yet, since Î³ > 1, we know that:
(i) Î±E1 , Î±
V
1 < 1 and therefore DÎ± for Î± â‰¥ 1 is such that
DÎ±(p,p) = 0 for N â†’âˆ; and (ii) Î±E2 , Î±V2 < 2 and there-
fore the bias and fluctuations of DÎ± for Î± â‰¥ 2 decay as
1/N (or 1/N2 for the fluctuations in the case of p = q).
This suggests DÎ±=2 as a pragmatic choice for empirical
measurements because any further increase in Î± will not
lead to a faster convergence.
V. APPLICATION TO REAL DATA
In this section, we show the significance of the general
results of the previous section to specific problems.
A problem that appears in different contexts is to test
whether two finite-size N sequences, described by their
empirical distributions pÌ‚ and qÌ‚, have a common source
(null hypothesis). This involves the computation of a
single divergence DÌƒÎ±(p, q), which is then compared to
the divergence DÌƒÎ±(p
â€²,pâ€²) between two finite-size (ran-
dom) samplings of a single (properly chosen) distribu-
tion pâ€² (e.g., pâ€² = 0.5p + 0.5q). The probability of
observing DÌƒÎ±(p
â€²,pâ€²) â‰¥ DÌƒÎ±(p, q) is then reported as a
7
0.0 0.5 1.0 1.5 2.0
Î±
10âˆ’5
10âˆ’4
10âˆ’3
10âˆ’2
10âˆ’1
DÌƒ
Î±
(p
t
1
,
p
t
2
)
(a)
1850-1900
1900-1950
1850-1950
0 0.5 1 1.5 2
10âˆ’4
10âˆ’3
10âˆ’2
10âˆ’1
1
0 50 100 150
âˆ†t
10âˆ’4
10âˆ’3
10âˆ’2
10âˆ’1
100
DÌƒ
Î±
(âˆ†
t)
(b)
Î± = 0.0
Î± = 0.5
Î± = 1.0
Î± = 2.0
0 50 100 150
0
20
40
60
FIG. 6. Measuring change in the usage of language on historical time-scales. (a) DÌƒÎ±(pt1 ,pt2) as a function of Î± for pairs
of word-frequency distributions of the Google-ngram database obtained from the yearly corpora t1 and t2 with (t1, t2) âˆˆ
{(1850, 1900), (1900, 1950), (1850, 1950)} (solid lines). The dotted lines with the same colors show the results of a null model
in which samples of the same size of the ones in t1 and t2 are randomly drawn from the same distribution (obtained from
combining the corpora in t1 and t2) mimicking a minimum distance that can be observed due to finite-size effects. The vertical
lines show the three regimes Î± < 1/Î³, 1/Î³ < Î± < 1 + 1/Î³, and Î± > 1 + 1/Î³ in the convergence of DÌƒÎ±(pt1 ,pt2) with N
(see Sec. IV), obtained using Î³ = 1.77 [10]. Inset: ratio DÌƒÎ±(pt12 ,pt12)/DÌƒÎ±(pt1 ,pt2). (b) Average divergence as a function of
âˆ†t â‰¡ |t2 âˆ’ t1|, calculated as DÌƒÎ±(âˆ†t) = 1Nâˆ†t
âˆ‘2000âˆ’âˆ†t
t1=1805
DÌƒÎ±(pt1 ,pt1+âˆ†t) for four different Î± (solid lines). Shaded areas represent
the standard deviation associated to the average DÌƒÎ±(âˆ†t). Inset: DÌƒÎ±(âˆ†t)/DÌƒÎ±(âˆ†t = 1).
p-value [3]. Besides applications in language, e.g. com-
paring the distribution of word-frequencies, this problem
appears in the identification of coding- and non-coding
regions in DNA [42]. The significance of our results for
finite-size estimations in Sec. IV is that for the case of
heavy-tailed distribution the expected DÌƒÎ±(p, q) of the
null model may be much larger than the predicted value
based on a 1/N decay (as observed in short-tailed dis-
tributions). If the slower convergence in N is ignored,
e.g., by applying standard tests [3] to heavy-tailed dis-
tributions, one rejects the null hypothesis (low p-value)
even if the data is drawn from the same source because
the measured distance will be much larger. The exam-
ple in Fig. 4(c) shows that, even when the size of both
sequences is on the order of N â‰ˆ 105, the expected DÌƒ1
(JSD) is E[DÌƒÎ±=1(pÌ‚, qÌ‚)] â‰ˆ 10âˆ’1. This is two orders of
magnitude larger than for the exponential distribution
in Fig. 4(a), where E[DÌƒÎ±=1(pÌ‚, qÌ‚)] â‰ˆ 10âˆ’3.
The next problems we consider appear in the anal-
ysis of historical data and in the quantification of lan-
guage change [43]. These problems are representative of
problems that involve the comparison of two or more di-
vergences DÌƒÎ±(p, q), obtained from different distributions
p 6= q and Î±â€² 6= Î±. As depicted in Fig. 1, the differ-
ent distributions are obtained based on individual years
(t âˆˆ {1850, 1900, 1950}) and we calculate the normalized
spectrum DÌƒÎ±(pt1 ,pt2) between pairs of years (t1, t2). As
argued in Sec. II and Appendix A, DÌƒÎ±(p, q) is meaning-
ful even if the sequences used to estimate p and q have
different sizes Np 6= Nq. We summarize our results in
Fig. 6, from which different conclusions can be drawn:
a. Temporal change. The change of English from
1850 to 1950 was larger than the change form 1850 to
1900 and from 1900 to 1950, as seen from the fact that
the curve of DÌƒÎ±(p1850,p1950) in Fig. 6a lies above the
two other curves for all Î±. This intuitive result (evolu-
tionary dynamics show no recurrences) confirms that the
divergence spectrum DÌƒÎ±(pt1 ,pt2) is a meaningful quan-
tification of language change. The average dependency of
DÌƒÎ±(p1850,p1950) on âˆ†t = |t2 âˆ’ t1|, shown in Fig. 6b, can
be thus used as a quantification of the speed of language
change.
b. Dependence on Î±. All observed divergences
DÌƒÎ±(pt1 ,pt2) decay with Î± (e.g., the three curves in
Fig. 6a). As discussed in Sec. III, this shows that
for words with a high (low) frequency the distribu-
tions are more (less) similar and thus the change is
slower (faster). This result is consistent with reports
that frequent words tend to be more stable on his-
torical time scales [44, 45]. This dependence on Î±
is essential when comparing the change 1850 7â†’ 1900
to the change 1900 7â†’ 1950 (Fig. 6a). While the
earlier change was smaller if counted on a token ba-
sis, DÌƒÎ±=1(p1850,p1900) < DÌƒÎ±=1(p1900,p1950), it be-
comes larger if one focus on the more frequent words
[DÌƒÎ±=2(p1850,p1900) > DÌƒÎ±=2(p1900,p1950)].
c. Role of finite-size scalings. Our finding that the
scalings (of the bias and of the fluctuations) in DÌƒÎ± with
sample size N depend on Î± allows for a deeper un-
derstanding of the DÌƒÎ±(pt1 ,pt2) measurements discussed
above. The expected DÌƒÎ±â€™s for random sampling of the
same distribution (null model shown as dashed line in
Fig. 6a) are of the same order as the empirical distance
for small Î± (i.e. DÌƒÎ±(pt12 ,pt12) â‰ˆ DÌƒÎ±(pt1 ,pt2)) and it
is only for Î± > 1 that the null model divergence be-
comes negligible compared to the empirical divergence
8
(i.e. DÌƒÎ±(pt12 ,pt12)  DÌƒÎ±(pt1 ,pt2)). This implies that
even though the size of the individual corpora is of the
order of N â‰ˆ 109 word-tokens, the empirically measured
DÌƒÎ± is still strongly influenced by finite-size effects over a
wide range of values for Î±, in agreement with our analysis
in Sec. IV. It also emphasizes that DÌƒÎ±=2 offers a prag-
matic choice in reducing such finite-size effects when the
exponent Î³ in the power-law distribution is not known.
This conclusion is further corroborated in the analysis of
the dependence of DÌƒÎ± with âˆ†t (Fig. 6b). For small Î±, DÌƒÎ±
does not converge to zero for âˆ†tâ†’ 0, but instead it seems
to saturate, i.e. DÌƒÎ±(âˆ†t â†’ 0) â‰ˆ DÌƒÎ±(âˆ†t = 1) = Î± > 0.
The value Î± is of the same order of magnitude of the
expected bias (e.g., shown as dashed line in Fig. 6a) and,
for small Î±, still of the same order of magnitude of the
divergence DÌƒÎ±(âˆ†t = 100) between two corpora separated
by 100 years. For small Î± and âˆ†t, it is thus difficult to
distinguish between finite-size effects (Î±) and actual lan-
guage change. Results for Î± = 2 show the largest relative
variation with âˆ†t (see Inset of Fig. 6b) and are therefore
statistically more suited to quantify language change over
time.
VI. CONCLUSIONS
In summary, we investigated the use of generalized en-
tropies HÎ± to quantify the difference between symbolic
sequences with heavy-tailed frequency distributions. In
particular, we introduced a normalized spectrum of a
generalized divergence, DÌƒÎ±(p, q) in Eq. (5), that allows
for a comparison between the different distributions p
and q and also for different Î±â€™s. Increasing Î±, DÌƒÎ± at-
tributes higher weights to high-frequency symbols. The
more complete characterization given by the full spec-
trum DÌƒÎ± is particularly important in the case of heavy-
tailed distributions because in this case symbols do not
have a characteristic frequency but instead show frequen-
cies on a broad range of values.
Our main analytical finding is how the bias and fluc-
tuations of finite-size N estimations of HÎ± and DÌƒÎ± scales
with N , see Tab. II. The existence of regimes in which
these scalings decay slower than 1/N shows that large un-
certainties should be expected in HÎ± and DÌƒÎ± estimated
even for very large databases. This should be taken into
account when comparing two or more DÌƒÎ±â€™s and when
estimating the probability of two sequences having the
same source. The fact that for large Î± we recover the
usual scaling (decay with 1/N) suggests DÌƒÎ±=2 as a prag-
matic choice in applications involving heavy-tailed distri-
butions.
Our investigation of the change in word usage on
historical time scales shows that low-frequency words
change faster than high-frequency words and quantifies
the speed of change in time. Furthermore, we illustrated
that DÌƒÎ±=2 is more suited to discriminate and quantify
language change over time.
Our results are also of interest beyond the cases treated
here. First, the finite-size scaling we derive appear al-
ready in the entropy and therefore the same scalings are
expected in any entropy-based measure, including those
based on conditional entropies such as the Kullback-
Leibler divergence [1]. Second, the analysis is not nec-
essarily restricted to the word level, it can be straightfor-
ward extended also to n-grams of words which also show
heavy-tailed distributions [46]. Third, the spectrum of
divergences DÌƒÎ±(p, q) offers a unifying framework which
can be applied to problems involving different partitions
of texts by varying the parameter Î±. For example, while
in document classification [2] one tries to identify top-
ical words (suggesting the use of low values of Î±), in
applications of authorship attribution [47] it has been
shown that the comparison of the most-frequent (func-
tion) words yields the best results (suggesting the use of
large values of Î±). Fourth, heavy-tailed distributions ap-
pear in different problems involving symbolic sequences
(e.g., in the DNA [17], in gene expression [18], and in
music [19]), and the significance of our results is that they
can be applied in all these cases.
ACKNOWLEDGMENTS
We thank Peter Grassberger for insightful discussions.
Appendix A: Documents with different lengths
Here we discuss how to proceed if the JSD is computed
from finite datasets with different finite lengths N , i.e.
when p (q) is estimated from a sequence of length Np
(Nq 6= Np).
1. Different Weights
A possible way to extend Eq. (2) taking into ac-
count the unequal contribution Np 6= Nq is to consider
weights Ï€ as [3]
DÏ€Î±(p, q) = HÎ±(Ï€pp+Ï€qq)âˆ’Ï€pHÎ±(p)âˆ’Ï€qHÎ±(q). (A1)
with Ï€p = Np/N and Nq/N such that Ï€p + Ï€q = 1 with
N = Np +Nq (denoted as natural weights in the follow-
ing). Obviously, if Np = Nq then Ï€p = Ï€q = 1/2 and DÎ±
is recovered. The normalized distance (5) becomes
DÌƒÏ€Î±(p, q) =
DÏ€Î±(p, q)
DÏ€,maxÎ± (p, q)
, (A2)
where
DÏ€,maxÎ± (p, q) =
(
Ï€Î±p âˆ’ Ï€p
)
HÎ± (p) +
(
Ï€Î±q âˆ’ Ï€q
)
HÎ± (q)
+
1
1âˆ’ Î±
(
Ï€Î±p + Ï€
Î±
q âˆ’ 1
)
.
(A3)
9
Our main results for the finite-size scaling of DÎ± summa-
rized in Tab. II remain valid for the weighted divergences.
The approach above follows Ref [3], which introduced
weights to the usual JSD (non-normalized, Î± = 1) and
showed that the natural weights Ï€p = Np/N and Ï€q =
Nq/N imply certain useful properties for the JSD, e.g.,
that the bias does not depend on the relative size of the
two samples. While their main motivation was to com-
pare the statistical significance of a single measurement of
the JSD in the identification of stationary subsequences
(of possibly different lengths) in a non-stationary sym-
bolic sequence, here, we are mainly interested in com-
paring two (or more) measured distances. In this case,
choosing weights that depend on the size of the individ-
ual samples becomes problematic when the sequences are
of different lengths. The demonstration that
âˆš
DÎ±(p, q)
is a metric for any Î± âˆˆ (0, 2] [28] is valid for fixed weights
Ï€p = Ï€q = 1/2. More generally, the measure D
Ï€
Î± itself
depends on the weights Ï€ such that DÏ€Î± and D
Ï€â€²
Î± con-
stitute different measures when Ï€ 6= Ï€â€². It is therefore
not meaningful to compare DÏ€Î±(p, q) and D
Ï€â€²
Î± (p
â€², qâ€²) if
Np/Nq 6= Npâ€²/Nqâ€² because this would imply that Ï€â€² 6= Ï€.
2. Equal Weights
In the previous section we argued that it is essential
to choose fixed weights Ï€ when comparing different dis-
tances. The choice of equal weights Ï€p = Ï€q = 1/2 can,
however, still be interpreted in the framework of natural
weights (Ï€p = Np/N , Ï€q = Nq/N) as the distance be-
tween undersampled versions of the sequences. For given
p and q with Np 6= Nq we choose equal weights Ï€p =
Ï€q = 1/2 yielding a distance D
1/2
Î± (p, q). If we randomly
draw samples pâ€² and qâ€² of size N â€²p = N
â€²
q from the distri-
butions p and q, (by construction) the natural weights
coincide with the equal weights, i.e. Ï€â€²p = Ï€
â€²
q = N
â€²
p/N =
N â€²q/N = 1/2, and lim
N â€²p=N
â€²
qâ†’âˆ
DÏ€
â€²
Î± (p
â€², qâ€²) = D1/2Î± (p, q).
In Fig. 7 we show the difference in DÌƒÏ€Î±(p, q) between
two empirical distributions from the Google-ngram with
different sizes (Np 6= Nq) when choosing equal and natu-
ral weights. Using equal weights corresponds to the case
in which we draw samples pÌ‚ and qÌ‚ that are of equal length
(N â€²p = N
â€²
q) such that equal and natural weights coincide
and taking the limit N â€²p, N
â€²
q â†’âˆ.
Appendix B: Finite size estimation of HÎ±, DÎ±, and
DÌƒÎ±
In this section we present the calculations on the mean
(i.e. the bias) and the fluctuations in finite-size estimates
of HÎ±, DÎ±, and DÌƒÎ±. The starting point is a finite sample
pÌ‚ = (n1/N, n2/N, . . . , nV /N) of size N (where ni is the
number of times symbol i was observed) which we assume
is obtained from N identical and independent draws from
100 101 102 103 104 105 106 107 108 109
N = Np +Nq
10âˆ’3
10âˆ’2
10âˆ’1
100
DÌƒ
Î±
(pÌ‚
,qÌ‚
)
Î± = 0.0
Î± = 0.5
Î± = 1.0
Î± = 2.0
FIG. 7. JSD-Î± for sequences of different lengths. Measure-
ment of DÌƒÎ±(pÌ‚, qÌ‚) between sequences pÌ‚, qÌ‚ of sizeN
â€²
p = N
â€²
q sam-
pled randomly from the empirical distribution of the Google-
ngram of the years t âˆˆ {1850, 1950} with different sizes, i.e.
p = pt=1850 and q = pt=1950 with Np 6= Nq, as a function of
the sample size N â€² = N â€²p + N
â€²
q for different values of Î±. The
dotted (dashed) lines show DÌƒÏ€Î±(p, q) between the full distribu-
tions p and q with equal (natural) weights, i.e. Ï€p = Ï€q = 1/2
(Ï€p = Np/(Np + Nq) â‰ˆ 0.22 and Ï€q = Nq/(Np + Nq) â‰ˆ 0.78
corresponding to the relative size of p and q)
the distribution p giving an estimator for HÎ±:
HÎ±(pÌ‚) =
1
1âˆ’ Î±
ï£«ï£­ âˆ‘
i:pÌ‚i>0
pÌ‚Î±i âˆ’ 1
ï£¶ï£¸ . (B1)
In order to take the corresponding expectation values we
expand pÌ‚Î±i around the true probabilities pi up to second
order
pÌ‚Î±i â‰ˆ pÎ±i +(pÌ‚iâˆ’pi)Î±pÎ±âˆ’1i +
1
2
(pÌ‚iâˆ’pi)2Î±(Î±âˆ’1)pÎ±âˆ’2i (B2)
and average over the realizations of the random variables
pÌ‚Î±i by assuming that each symbol is drawn independently
from binomial with probability pi such that ã€ˆ(pÌ‚iâˆ’pi)ã€‰ = 0
and ã€ˆ(pÌ‚i âˆ’ pi)2ã€‰ = pi(1âˆ’ pi)/N â‰ˆ pi/N yielding [37]
ã€ˆpÌ‚Î±i ã€‰ â‰ˆ pÎ±i +
1
2N
Î±(Î±âˆ’ 1)pÎ±âˆ’1i . (B3)
1. HÎ±
Combining Eqs. (B1) and (B3) we obtain for the mean
E[HÎ±(pÌ‚)] â‰¡ ã€ˆHÎ±(pÌ‚)ã€‰ =
1
1âˆ’ Î±
ï£«ï£­ âˆ‘
iâˆˆã€ˆVpÌ‚ã€‰
ã€ˆpÌ‚Î±i ã€‰ âˆ’ 1
ï£¶ï£¸
=
1
1âˆ’ Î±
ï£«ï£­ âˆ‘
iâˆˆã€ˆVpÌ‚ã€‰
pÎ±i âˆ’ 1
ï£¶ï£¸âˆ’ Î±
2N
âˆ‘
iâˆˆã€ˆVpÌ‚ã€‰
pÎ±âˆ’1i
=
1
1âˆ’ Î±
(
V
(Î±+1)
pÌ‚ âˆ’ 1
)
âˆ’ Î±
2N
V
(Î±)
pÌ‚
(B4)
10
where we introduce the notation
âˆ‘
iâˆˆã€ˆVpÌ‚ã€‰ indicating that
we average only over the expected number of observed
symbols ã€ˆVpÌ‚ã€‰ in samples pÌ‚.
For the variance we get
V[HÎ±(pÌ‚)] â‰¡E[HÎ±(pÌ‚)2]âˆ’ E[HÎ±(pÌ‚)]2
=
1
(1âˆ’ Î±)2
âˆ‘
iâˆˆã€ˆVpÌ‚ã€‰
âˆ‘
jâˆˆã€ˆVpÌ‚ã€‰
(
ã€ˆpÌ‚Î±i pÌ‚Î±j ã€‰ âˆ’ ã€ˆpÌ‚Î±i ã€‰ã€ˆpÌ‚Î±j ã€‰
)
=
Î±2
(1âˆ’ Î±)2N
âˆ‘
iâˆˆã€ˆVpÌ‚ã€‰
p2Î±âˆ’1i âˆ’
Î±2
4N2
âˆ‘
iâˆˆã€ˆVpÌ‚ã€‰
p2Î±âˆ’2i
=
Î±2
(1âˆ’ Î±)2
V
(2Î±)
pÌ‚
N
âˆ’ Î±
2
4
V
(2Î±âˆ’1)
pÌ‚
N2
(B5)
where we used that two different symbols i 6= j are in-
dependently drawn, thus
âˆ‘
i,jã€ˆpÌ‚Î±i pÌ‚Î±j ã€‰ =
âˆ‘
i 6=jã€ˆpÌ‚Î±i ã€‰ã€ˆpÌ‚Î±j ã€‰ +âˆ‘
iã€ˆpÌ‚2Î±i ã€‰.
2. DÎ±
For DÎ± we have two samples pÌ‚ and qÌ‚ each of size N
randomly sampled from the distributions p and q such
that we can express the mean and the variance from the
expectation values of the corresponding individual en-
tropies.
Introducing the notation P â‰¡ 12 (p + q) we get for the
mean
E[DÎ±(pÌ‚, qÌ‚)] =E[HÎ±(PÌ‚ )]âˆ’
1
2
E [HÎ± (pÌ‚)]âˆ’
1
2
E [HÎ± (qÌ‚)]
=
1
1âˆ’ Î±
{
V
(Î±+1)
PÌ‚
âˆ’ 1
2
V
(Î±+1)
pÌ‚ âˆ’
1
2
V
(Î±+1)
qÌ‚
}
+
Î±
2N
{
1
2
V
(Î±)
pÌ‚ +
1
2
V
(Î±)
qÌ‚ âˆ’
1
2
V
(Î±)
PÌ‚
}
.
(B6)
where V
(Î±)
PÌ‚
denotes the generalized vocabulary, Eq. (13),
for the combined sequence PÌ‚ = 12 (pÌ‚ + qÌ‚), which is of
length 2N .
For the variance we get
V[DÎ± (pÌ‚, qÌ‚)] â‰¡E[DÎ± (pÌ‚, qÌ‚)2]âˆ’ E[DÎ± (pÌ‚, qÌ‚)]2
=V[HÎ±(PÌ‚ )] +
1
4
V [HÎ±(pÌ‚)] +
1
4
V [HÎ±(qÌ‚)]
âˆ’ Cov
[
HÎ±(PÌ‚ ), HÎ±(pÌ‚) +HÎ±(qÌ‚)
]
,
(B7)
where Cov [X,Y ] â‰¡ E[XY ]âˆ’E[X]E[Y ]. We evaluate the
covariance-term in two different ways, i.e.
(1âˆ’ Î±)2Cov
[
HÎ±(PÌ‚ ), HÎ±(pÌ‚) +HÎ±(qÌ‚)
]
=
âŒ© âˆ‘
i:pÌ‚i+qÌ‚i>0
PÌ‚Î±i
ï£«ï£­ âˆ‘
j:pÌ‚j>0
pÌ‚Î±j +
âˆ‘
j:qÌ‚j>0
qÌ‚Î±j
ï£¶ï£¸âŒª
âˆ’
âŒ© âˆ‘
i:pÌ‚i+qÌ‚i>0
PÌ‚Î±i
âŒªï£«ï£­âŒ© âˆ‘
j:pÌ‚j>0
pÌ‚Î±j
âŒª
+
âŒ© âˆ‘
j:qÌ‚j>0
qÌ‚Î±j
âŒªï£¶ï£¸
=
âŒ© âˆ‘
i:pÌ‚i+qÌ‚i>0
PÌ‚Î±i
âˆ‘
j:pÌ‚j+qÌ‚j>0
(
pÌ‚Î±j + qÌ‚
Î±
j
)âŒª
âˆ’
âŒ© âˆ‘
i:pÌ‚i+qÌ‚i>0
PÌ‚Î±i
âŒªâŒ© âˆ‘
j:pÌ‚j+qÌ‚j>0
(
pÌ‚Î±j + qÌ‚
Î±
j
)âŒª
=
âˆ‘
iâˆˆã€ˆVPÌ‚ ã€‰
{âŒ©
PÌ‚Î±i (pÌ‚
Î±
i + qÌ‚
Î±
i )
âŒª
âˆ’
âŒ©
PÌ‚Î±i
âŒª
(ã€ˆpÌ‚Î±i ã€‰+ ã€ˆqÌ‚Î±i ã€‰)
}
(B8)
and
(1âˆ’ Î±)2Cov
[
HÎ±(PÌ‚ ), HÎ±(pÌ‚) +HÎ±(qÌ‚)
]
=
âŒ© âˆ‘
i:pÌ‚i+qÌ‚i>0
PÌ‚Î±i
âˆ‘
j:pÌ‚j>0
pÌ‚Î±j
âŒª
âˆ’
âŒ© âˆ‘
i:pÌ‚i+qÌ‚i>0
PÌ‚Î±i
âŒªâŒ© âˆ‘
j:pÌ‚j>0
pÌ‚Î±j
âŒª
+
âŒ© âˆ‘
i:pÌ‚i+qÌ‚i>0
PÌ‚Î±i
âˆ‘
j:qÌ‚j>0
qÌ‚Î±j
âŒª
âˆ’
âŒ© âˆ‘
i:pÌ‚i+qÌ‚i>0
PÌ‚Î±i
âŒªâŒ© âˆ‘
j:qÌ‚j>0
qÌ‚Î±j
âŒª
=
âˆ‘
iâˆˆã€ˆVpÌ‚ã€‰
{âŒ©
PÌ‚Î±i pÌ‚
Î±
i
âŒª
âˆ’
âŒ©
PÌ‚Î±i
âŒª
ã€ˆpÌ‚Î±i ã€‰
}
+
âˆ‘
iâˆˆã€ˆVqÌ‚ã€‰
{âŒ©
PÌ‚Î±i qÌ‚
Î±
i
âŒª
âˆ’
âŒ©
PÌ‚Î±i
âŒª
ã€ˆqÌ‚Î±i ã€‰
}
(B9)
Similarly as in Eq. (B3) we can approximate
âŒ©
PÌ‚Î±i
âŒª
â‰ˆPÎ±i +
Î±(Î±âˆ’ 1)
4N
PÎ±âˆ’1i ,âŒ©
PÌ‚Î±i pÌ‚
Î±
i
âŒª
â‰ˆPÎ±i pÎ±i +
Î±
4N
(3Î±âˆ’ 1)PÎ±âˆ’1i p
Î±
i
+
Î±
2N
(Î±âˆ’ 1)PÎ±i pÎ±âˆ’1i ,âŒ©
PÌ‚Î±i qÌ‚
Î±
i
âŒª
â‰ˆPÎ±i qÎ±i +
Î±
4N
(3Î±âˆ’ 1)PÎ±âˆ’1i q
Î±
i
+
Î±
2N
(Î±âˆ’ 1)PÎ±i qÎ±âˆ’1i .
(B10)
11
From this we get for the variance of DÎ±
V[DÎ±(pÌ‚, qÌ‚)] =
âˆ‘
iâˆˆã€ˆVPÌ‚ ã€‰
{
Î±2
(1âˆ’ Î±)2
1
2N
PÎ±âˆ’1i
[
PÎ±i âˆ’
1
2
(pÎ±i + q
Î±
i )
]
âˆ’ Î±
2
16N2
PÎ±âˆ’1i
[
PÎ±âˆ’1i âˆ’
(
pÎ±âˆ’1i + q
Î±âˆ’1
i
)]}
+
1
2
âˆ‘
iâˆˆã€ˆVpÌ‚ã€‰
{
Î±2
(1âˆ’ Î±)2
1
2N
pÎ±i
[
pÎ±âˆ’1i âˆ’ P
Î±âˆ’1
i
]
âˆ’ Î±
2
8N2
pÎ±âˆ’1i
[
pÎ±âˆ’1i âˆ’ P
Î±âˆ’1
i
]}
+
1
2
âˆ‘
iâˆˆã€ˆVqÌ‚ã€‰
{
Î±2
(1âˆ’ Î±)2
1
2N
qÎ±i
[
qÎ±âˆ’1i âˆ’ P
Î±âˆ’1
i
]
âˆ’ Î±
2
8N2
qÎ±âˆ’1i
[
qÎ±âˆ’1i âˆ’ P
Î±âˆ’1
i
]}
.
(B11)
Now we can see that for p = q = P we get
V[DÎ±(pÌ‚, qÌ‚)]p=q =
âˆ‘
iâˆˆã€ˆVPÌ‚ ã€‰
1
16N2
Î±2p2Î±âˆ’2i =
Î±2
16N2
V
(2Î±âˆ’1)
PÌ‚
.
(B12)
While for arbitrary p and q the variance of the DÎ±
contains the variances of the individual entropies (e.g.
V
(2Î±)
PÌ‚
/N) and a covariance term, (only) in the special
case p = q all first-order terms (1/N) vanish yielding a
qualitatively different behaviour V
(2Î±âˆ’1)
PÌ‚
/N2.
3. DÌƒÎ±
The finite size estimation of DÌƒÎ± can be obtained ap-
proximately by
DÌƒÎ±(pÌ‚, qÌ‚) =
DÎ±(pÌ‚, qÌ‚)
DÎ±(pÌ‚, qÌ‚)max
â‰ˆ DÎ±(pÌ‚, qÌ‚)
E [DmaxÎ± (pÌ‚, qÌ‚)]
(B13)
such that
E
[
DÌƒÎ±(pÌ‚, qÌ‚)
]
â‰ˆ E [DÎ±(pÌ‚, qÌ‚)]
E [DmaxÎ± (pÌ‚, qÌ‚)]
,
V
[
DÌƒÎ±(pÌ‚, qÌ‚)
]
â‰ˆ V [DÎ±(pÌ‚, qÌ‚)]
E [DmaxÎ± (pÌ‚, qÌ‚)]
2 .
(B14)
The mean of DmaxÎ± is given according to Eq. (6) as a
linear combination of the individual entropies of pÌ‚ and qÌ‚
E [DmaxÎ± (pÌ‚, qÌ‚)]
=
21âˆ’Î± âˆ’ 1
2
(
E [HÎ± (pÌ‚)] + E [HÎ± (qÌ‚)] +
2
1âˆ’ Î±
)
.
(B15)
Appendix C: Derivation of Eq. (14)
In this section we derive the scaling of the general-
ized vocabulary V (Î±) defined in Eq. (13) assuming that
p is a power-law of the form pi âˆ iâˆ’Î³ , Eq. (1). In-
stead of looking at the probability of individual symbols
i, we consider the distribution of frequencies n, which
in this case yields p(n) âˆ nâˆ’1âˆ’1/Î³ [48]. Consider the
sum
âˆ‘
iâˆˆV pi =
1
N
âˆ‘
iâˆˆV ni =
1
N SV (Î³), where SV (Î³)
corresponds to the sum of V i.i.d. random variables ni
(i = 1, . . . , V ) drawn from the distribution p(n) It can be
shown that [49]
SV (Î³) âˆ
{
V Î³ , Î³ > 1,
V, Î³ < 1.
(C1)
The case Î³ = 1 includes additional logarithmic correc-
tions, but is not of relevance for the discussion, there-
fore, we leave it for sake of simplicity. In the same way,
we can treat
âˆ‘
iâˆˆV p
Âµ
i =
1
NÂµ
âˆ‘
iâˆˆV n
Âµ
i =
1
NÂµSV (Î³Âµ) by
noting that SV (Î³Âµ) can be interpreted as the sum of V
i.i.d. random variables ni (i = 1, . . . , V ), where ni âˆ¼ pÌƒ(n)
with pÌƒ(n) âˆ nâˆ’1âˆ’1/(Î³Âµ) such that we get
SV (Î³Âµ) âˆ
{
V Î³Âµ, Âµ < 1/Î³,
V, Âµ > 1/Î³.
(C2)
By setting Âµ = Î± âˆ’ 1 in Eq. (13) and noting that for
pi âˆ iâˆ’Î³ , Eq. (1), the number of different symbols scales
as V âˆ N1/Î³ , Eq. (10), we obtain Eq. (14).
[1] S. Kullback, Information Theory and Statistics (Wiley,
New York, 1959).
[2] C. D. Manning and H. SchuÌˆtze, Foundations of Statistical
Natural Language Processing (MIT Press, 1999).
12
[3] I. Grosse, P. Bernaola-GalvaÌn, P. Carpena, R. RomaÌn-
RoldaÌn, J. Oliver, and H. Stanley, Physical Review E
65, 041905 (2002).
[4] Yun He, A. Ben Hamza, and H. Krim, IEEE Transac-
tions on Signal Processing 51, 1211 (2003).
[5] K. W. Boyack, D. Newman, R. J. Duhon, R. Klavans,
M. Patek, J. R. Biberstine, B. Schijvenaars, A. Skupin,
N. Ma, and K. BoÌˆrner, PloS one 6, e18029 (2011).
[6] A. P. Masucci, A. Kalampokis, V. M. EguÌÄ±luz, and
E. HernaÌndez-GarcÌÄ±a, PloS one 6, e17333 (2011).
[7] V. Bochkarev, V. Solovyev, and S. Wichmann, Journal
of The Royal Society Interface 11, 20140841 (2014).
[8] E. A. Pechenick, C. M. Danforth, and P. S. Dodds,
arXiv:1503.03512 (2015).
[9] G. Cocho, J. Flores, C. Gershenson, C. Pineda, and
S. SaÌnchez, PLOS ONE 10, e0121898 (2015).
[10] M. Gerlach and E. G. Altmann, Physical Review X 3,
021006 (2013).
[11] J.-B. Michel, Y. K. Shen, A. P. Aiden, A. Veres, M. K.
Gray, J. P. Pickett, D. Hoiberg, D. Clancy, P. Norvig,
J. Orwant, S. Pinker, M. A. Nowak, and E. L. Aiden,
Science 331, 176 (2011).
[12] G. K. Zipf, The Psycho-Biology of Language (Routledge,
1936).
[13] R. H. Baayen, Word Frequency Distributions (Springer
Netherlands, 2001).
[14] R. Ferrer i Cancho, The European Physical Journal B
44, 249 (2005).
[15] M. A. Serrano, A. Flammini, and F. Menczer, PloS one
4, e5372 (2009).
[16] S. K. Baek, S. Bernhardsson, and P. Minnhagen, New
Journal of Physics 13, 043004 (2011).
[17] R. Mantegna, S. Buldyrev, A. Goldberger, S. Havlin,
C. Peng, M. Simons, and H. Stanley, Physical Review
Letters 73, 3169 (1994).
[18] C. Furusawa and K. Kaneko, Physical Review Letters 90
(2003), 10.1103/PhysRevLett.90.088102.
[19] J. SerraÌ€, A. Corral, M. BogunÌƒaÌ, M. Haro, and J. L.
Arcos, Sci. Rep. 2, 521 (2012).
[20] J. Lin, IEEE Transactions on Information Theory 37,
145 (1991).
[21] T. M. Cover and J. A. Thomas, Elements of Information
Theory (Wiley-Interscience, 2006).
[22] D. Endres and J. Schindelin, IEEE Transactions on In-
formation Theory 49, 1858 (2003).
[23] M. A. Montemurro and D. H. Zanette, Advances in Com-
plex Systems 13, 135 (2010).
[24] J. Havrda and F. CharvaÌt, Kybernetika 3, 30 (1967).
[25] J. Burbea and C. Rao, IEEE Transactions on Information
Theory 28, 489 (1982).
[26] C. Tsallis, Journal of Statistical Physics 52, 479 (1988).
[27] A. ReÌnyi, in Berkeley Symposium on Mathematical
Statistics and Probability (University of California Press,
1961).
[28] J. BrieÌˆt and P. HarremoeÌˆs, Physical Review A 79, 052311
(2009).
[29] T. SchuÌˆrmann, Journal of Physics A: Mathematical and
General , 5 (2004).
[30] G. Herdan, Type-token mathematics (Mouton, 1960).
[31] H. S. Heaps, Information Retrieval (Academic Press,
1978).
[32] M. Gerlach and E. G. Altmann, New Journal of Physics
16, 113010 (2014).
[33] T. D. de Wit, The European Physical Journal B - Con-
densed Matter and Complex Systems 11, 513 (1999).
[34] G. A. Miller, in Information theory in psychology: Prob-
lems and methods, edited by H. Quastler (Free Press,
1955).
[35] G. P. Basharin, Theory of Probability & Its Applications
4, 333 (1959).
[36] B. Harris, Colloquia Mathematica Societatis JaÌnos
Bolyai: Topics in Information Theory 16, 323 (1975).
[37] H. Herzel, A. Schmitt, and W. Ebeling, Chaos, Solitons
& Fractals 4, 97 (1994).
[38] P. Grassberger, arXiv:physics/0307138 (2008).
[39] E. Archer, I. M. Park, and J. W. Pillow, Journal of
Machine Learning Research 15, 2833 (2014).
[40] I. Nemenman, F. Shafee, and W. Bialek, in Advances in
Neural Information Processing Systems 14 (MIT Press,
2002).
[41] For small values of N , the Bayesian estimates of the in-
dividual entropies in some cases yield DÌƒ(pÌ‚, qÌ‚) < 0 such
that we consider averages over |DÌƒ(pÌ‚, qÌ‚)|.
[42] P. Bernaola-GalvaÌn, I. Grosse, P. Carpena, J. L. Oliver,
R. RomaÌn-RoldaÌn, and H. E. Stanley, Physical Review
Letters 85, 1342 (2000).
[43] We use the Google-ngram corpus containing the fre-
quency of usage of millions of words from 4% of all books
ever published in the English language with a yearly res-
olution [11].
[44] M. Pagel, Q. D. Atkinson, and A. Meade, Nature 449,
717 (2007).
[45] E. Lieberman, J.-B. Michel, J. Jackson, T. Tang, and
M. A. Nowak, Nature 449, 713 (2007).
[46] T. Yasseri, A. Kornai, and J. KerteÌsz, PLoS ONE 7,
e48386 (2012).
[47] E. Stamatatos, Journal of the American Society for In-
formation Science and Technology 60, 538 (2009).
[48] M. Newman, Contemporary Physics 46, 323 (2005).
[49] J.-P. Bouchaud and A. Georges, Reviews of Modern
Physics 195, 127 (1990).
