Proceedings of the International Multiconference on
Computer Science and Information Technology pp. 481–485
ISBN 978-83-60810-22-4
ISSN 1896-7094
An Audiovisual Virtual Interface
Evangelos Kapros
Information Systems Laboratory, School of Computer
Science and Statistics, University of Dublin,
Trinity College, Ireland.
E-mail: ekapros@tcd.ie
Konstantinos Raptis
Department of Information and Communication
Systems Engineering,
University of the Aegean, Greece.
E-mail: icsd02067@icsd.aegean.gr
Abstract—State-of-the-art synthesizers provide numerous con-
trollers through which the user may create a great variety of
sounds. The real-time control of so many parameters though,
is often problematic for the user. The goal of our work has
been to study the physical and remote control of such audial
parameters. For this purpose, a system was developed that
processes video input and recognizes movements of the user’s
body parts (movement of hands, head etc.) and translates them
as change in some audial parameters of an electronic music
instrument (tone, cutoff, etc.). Also, basic techniques of digital
audio synthesis were studied and a prototype synthesizer was
developed that is controlled physically and remotely. The study
didn’t limit itself in technical details, but contained study of the
human psyche and cognition, as much for the audial, as also for
the video part of the implementation of the system.
Index Terms—Synthesizers, Human-Computer Interaction, In-
formation Design, Virtual Environments.
I. INTRODUCTION
DURING the last few years, many audiovisual applica-tions have been developed. Technology makes possible
mappings which were previously not only impossible, but
also inconceivable. Many of the artists who developed these
applications, have also made it possible for the users to express
themselves using these. Usually, this mapping concerns the
visualization of sound; very rarely, to say the least, do we
see the opposite happening. In this paper, we present an
audiovisual application which maps image into sound; a digital
synthesizer which is controlled remotely by the user’s physical
movement.
Music is a form of expression of meaning, which auto-
matically makes the construction of a music application not
trivial. Although there are some general principles which
we may extract from the work of the Gestalt psychologists,
such as the idea that high-frequency information often occurs
crossmodally, the designer of synthetic sound-image mappings
must operate largely from intuition [7].
So, apparently, a simple solution for the designer is to pass
the control of this mapping to the musician. The musician then,
as a special individual can give their own, imaginary meaning
to the result of this control. A way to eliminate the constraints
that could be set by the medium of the computer, is to provide
an interface that requires no physical contact. We propose a
system that recognizes the movements of body parts of the
musician through video input and uses a coordinate system to
accordingly produce sound. To our best of knowledge, such
digital instruments, the Theremin being a non-computerized
instrument, are, to say the least, uncommon.
The most appropriate body part to control audio parameters
could be considered to be the hands, for they are established
in the perception of individuals as controllers. The coordinate
that adjusts the tone of the sound would be reasonable to
be the height of the hand. There are two ways to recognize
movement: brightness and metaballs detection.
Each metaball is defined as a function in n-dimensions (i.e.
for three dimensions, f(x, y, z); three-dimensional metaballs
tend to be the most common). A thresholding value is also
chosen, to define a solid volume. Then, this threshold value
represents whether the volume enclosed by the surface defined
by n metaballs is filled at (x, y, z) or not. The normal approach
in drawing 2d metaballs is to scan every pixel on screen and
evaluate the metaball formula. If the result is true, draw a pixel
there.
In our case, if the surface at (x, y) is filled, the value y
is taken as an input for the sine wave sound function to
adjust its frequency. Apart from the tone adjustment, other
regions are stated to apply a 3 point envelope and a high pass
filter. Additional fade out, normalizing and reverb processing
is done.
II. PREVIOUS WORK
Human-Computer Interaction is a relatively new domain in
the science of Informatics. Its elements have been present since
the beginning of the computer age, but hadn’t been considered
to be important or primary at that time and thus, a distinct
domain wasn’t formed. The fact that the domain emerged
recently, along with the dominant institution of techno-science,
often lead to misunderstandings. An examination of the terms
Interaction, Human, Computer, on their own may elucidate the
situation.
• Interaction: A universal definition is impossible. It can
certainly be told, however, that not all forms of communi-
cation are to be classified under this term (e.g. between two
routers), even if there is a request-and-respond action from the
participants. There has to be a relationship between the n-th
respond and the (n− 1)th request. This relationship has to be
more than a functional or, in general, an ensidic one—there has
to be a relationship of content, a relationship of presentations,
a relationship of meaning.
978-83-60810-22-4/09/$25.00 © 2009 IEEE 481
Authorized licensed use limited to: Aegean University. Downloaded on July 27,2010 at 15:38:24 UTC from IEEE Xplore.  Restrictions apply. 
EVANGELOS KAPROS ET. AL: AN AUDIOVISUAL VIRTUAL INTERFACE 482
• Human: The most usual misunderstanding of the domain
until now is that it tries to explain humans through the external
characteristics of their behaviour (behaviourism). So, we have
a plethora of models of the brain as teukhein, as a deterministic
machine (with some random noise, but without changing the
general concept). Humans are represented in analogy to the
computer, as two different sources of “objective reality”, which
they communicate in the form of bits (reminding us Kant’s for
itself ), but simply having their own special way to express this
“reality”.
This viewpoint cancels human psyche; humans and com-
puters are alien/other to each other, not different; humans
are capable of representing—of making meaning emerge in
a magmatic relationship of the ensidic, imaginary, symbolic,
phantasmatic; it cancels human creation, the social self-
institution which happens through and with the historical
time [1]. Recent research [2], [3], [4], [5] that surpasses a
mechanistic model and discusses about the content of inter-
action, has encountered with astonishment the fact that, apart
from emotions and easy learning, participants in interactive
experiences show enhanced sociality (this was successfully
realized in the Bauhaus School) [6].
• Computer: Interaction is usually considered to be “exter-
nal” action. The metaphor of “navigation” used in Interface
Design shows the contradiction. When we “navigate” in an
application we perceive it in two opposite ways. The Cartesian
view of the digital landscape is external to us, as a map both
independent and spontaneous. On the contrary, our subjective
presence is an undoubted part of the virtual digital landscape.
This contradiction is quite old [2].
The computer, however, bears within itself interactivity—it
has been constructed for this purpose. Its application and
its hardware are thinkable as interactive because they are
organized as such. Anyway, the computer processes data
and not information—meaning-making, due to lack of radical
imaginary, is impossible. This is often omitted during the
development of interactive applications. Inevitably then, we
have to take Humans into consideration even more, if we take
into account this alterity. This distinction is essential to us, and
we view it as Human-Computer Interaction’s principle and not
as a neo-humanistic ideal.
Progress in Human-Computer Interaction is technically un-
doubted, because of research on and understanding of some
basic functions of interactivity. Nevertheless, one has to keep
in mind progress (or rather evolution?) in Human Interaction,
like with Freudian psychoanalysis initially, group therapy
afterwards, etc.
People tend to connect interactivity to computers, especially
after 1990 when Bill Moggridge came up with the term
interaction design, but this is totally insufficient. Interactivity
has existed in different regions and eras of social-historical
institution, in the fires of Homer, in the smoke signals of
Native American, or in the stone cairns of the Celts.
Several centuries later, in the 1830s, Samuel Morse con-
structed and designed not only his code, but also the whole
telegraph system: the electric circuits, the mechanism to pro-
duce the code and user’s training. Of course, it didn’t happen
in one day, but it is the first communication system that is too
complex for the average user. It required an entire system to
be specially designed.
Similarly, for other communication systems such as the ra-
dio, the telephone, or television, interfaces had to be designed
by the engineers. For the very first time the interface was not a
human being (the telegrapher, the typographer, the post-man),
but a mechanical, and often automatic, device. We can define,
for the first time, a virtual interface. These interfaces were
also used for the communication between devices (telephone
routers etc.). This forms an entire virtual environment. Inter-
action design, at that time, was not a priority.
Machinery used for these technological achievements had
human input, but in a simple, linear, non-adoptable way. That
changed with the use of computers.
Other advancements concerning input and data control in
computers came from the medical and the film industries and
have many similarities. In motion capture usually sensors are
used, reflective or gyroscopic, to construct and control virtual
environments. State-of-the-art implementations combine dif-
ferent sensor types (U70 computer, XSens MT9, ARTTrack,
Flock of Birds, ARToolKit, Garmin GPS18 USB). [8], [9]
Usual navigation in virtual environments with touch screens
derives from 2D and 3D computer desktops and its usability
is questioned. The iOrb device was specially designed for
command and data input in virtual environments. The user
can run application commands selecting from 1D and 2D pie
menus. Spatial selection include both selection rays and cones.
Interaction uses relative gyroscopic distances, so measuring is
quite sensitive to errors. [12]
Concerning the development of interactive applications tar-
geting the expression of users’ psychical presentations, haptic
senses are used, a rather constraining factor. State-of-the-
art applications are the reacTable* and "The Manual Input
Sessions" [10], while non-haptic virtual environments had been
restricted to visualization of voice data—as in the installations
Hidden Worlds,Messa di Voce and RE:MARK. [13], [14] There
has been a lack of an electronic music instrument that allows
the musicians to express psychical representations through
their own physical movement, but with a non-haptic interface,
controlled completely remotely.
III. METHODOLOGY
A. General Design Principles
Our goal was to construct such an instrument, that is one
that recognizes physical movements and this video capture
controls remotely some audial parameters.
Our system attempts to offer to its user a complete au-
diovisual experience, which is the reason why the human
body is suggested as an interface device to interact with the
instrument’s virtual environment.
The interface is deliberately designed to be as minimal
as possible so that the musician would not be distracted.
Beside each controller exists a describing word, enhancing
the experience for non-expert users. The only elements using
Authorized licensed use limited to: Aegean University. Downloaded on July 27,2010 at 15:38:24 UTC from IEEE Xplore.  Restrictions apply. 
EVANGELOS KAPROS ET. AL: AN AUDIOVISUAL VIRTUAL INTERFACE 483
a visible language are the drawn waveform and the scrollbar
adjusting the threshold value.
The application was exclusively developed and designed
using the Processing Integrated Development Environment.
Processing is a tool suitable for programming audiovisual
interfaces.
It’s main advantages are flexibility, as it is fully modular
and with easy-to-learn functions and the fact that it is built on
Java, which means that every application can be executed as an
applet, or even the majority of applications can be embedded
in Websites.
Applets that need to access local classes are not allowed
to run from a Web browser, for obvious security reasons. One
can download the applet and execute it locally in almost every
platform.
B. Interface Design
The basic principle taken into consideration in the interface
of the video controlled synthesizer was that the functions of
decay or cutoff are something that most users are incapable
of connecting with certain icons.
The effect of these filters on the sound wave is something
trivial for sound engineers; should the application refer to them
only, the controllers could be accompanied by the appropriate
icons.
On a non-expert user though, the result would be quite
different. Users would tend to validate the icon’s form with the
drawn waveform; that would result to distract their attention
from the production of music.
Too much attention on the interface—too little attention
on the application. In an audiovisual real-time interface this
is more important than, e.g., in a static web site, for the
distraction of the user has an impact on the creation of
something unique. On the contrary, if the “view our portfolio”
icon takes time to disambiguate, the portfolio will still remain
the same.
Consequently, an explanatory word beside each controller
is less distractive. This makes the application more usable.
The text of every word is seen as an initial statement—as
soon as the user gets used to the interface, the control becomes
easier. One can focus on their own movements to produce the
desired sound. This becomes even more important in a virtual
interface, where no haptic senses are used.
C. Audio Processing
1) Tone: To adjust the tone value, enter a dark object, or
preferably, a part of your body in the right region. After the
blob detection occurs, movement of the object or e.g. hand
alters the value of the frequency of the sound. The recognized
magenta edge is the controller of the tone, and movement to
a higher area of the screen means higher tone. The precision
of the edge around the object depends on the value of the
threshold value slide bar, presented in Figure 1.
The frequency is adjusted as follows:
float yoffset = (height - b.yMin*
height)/PApplet.toFloat(height);
Fig. 1. The interface of the application
frequency = pow(1000,yoffset)+150;
where b.yMin is the coordinate of the blob’s edge detection.
2) Cutoff: To adjust the cutoff value enter a dark object, or
preferably, a part of your body in the upper left region. After
the blob detection occurs, movement of the object or e.g. hand
alters the value of a highpass filter applied on the sound. The
recognized magenta edge is the controller of the cutoff, and
for a filter to be applied, some tone must have been initially
played.
The cutoff is adjusted as follows:
float yoffset1 = (height-b.yMin*
height)/PApplet.toFloat(height);
float hi = pow(5000,yoffset1)+500;
myHighPass=new HighPass(hi,0,16);
where b.yMin is the coordinate of the blob’s edge detection.
3) Decay: The decay parameter is adjusted similarly to the
cutoff. All three parameters change when motion is detected
in the suitable areas. These areas are wider than the drawn
controllers, but the drawn controllers tend to focus the users
in a smaller area and so, to have better musical results.
4) Threshold Scrollbar: The threshold scrollbar is the only
interactive element of the interface that is manipulated by a
pointing device, such as a mouse or a tablet pen.
Its use is auxiliary and it is deliberately designed as a device
controlled element, exactly for this very reason. Not too much
attention should be given during the performance; the initial
tuning according to the environment can be done initially, even
change to adapt to the lighting.
It is proved that the Threshold value is a more important
factor to have precise edge detection, rather than high-contrast
lighting conditions. [11]
5) Waveform: Waveform is the only non-interactive ele-
ment of the interface. Its purpose is educational, aiming to
make the changes in sound visually comprehendible. Until a
tone is played, the waveform is not drawn. Once initialised,
it becomes flat when no motion is detected in a suitable area;
otherwise it represents the effected waveform by the filters.
Authorized licensed use limited to: Aegean University. Downloaded on July 27,2010 at 15:38:24 UTC from IEEE Xplore.  Restrictions apply. 
EVANGELOS KAPROS ET. AL: AN AUDIOVISUAL VIRTUAL INTERFACE 484
IV. RESULTS
Evaluation should not be considered a procedure done at
the end of the system’s development, but as frequently its
design allows [15], [16]. Due to the nature of the system,
an experimental evaluation model was applied mainly for the
video and a query technique mainly for the sound. These
models however, had common places, because of the combina-
tional nature of the system. Open type questions were chosen,
that may be harder to analyse, but sometimes suggestions not
initially thought from the designers may be found. So, they
are a more qualitative user feedback, rather than closed type
questions that may be subject to statistical analysis.
A. Evaluation of Video Tracking
From our own use, from watching other users in laboratory
conditions and from watching other users in an installation of
the system at the 1st Mathemartics conference [11], Samos,
Greece, we can classify sets of conditions in order of prefer-
ence:
1) Night conditions, T = 0.2
2) Daylight conditions, T = 0.2
3) Night conditions, T = 0.6
4) Daylight conditions, T = 0.6
Threshold variable T was set with two extreme values for
the system (0.2, 0.6), while daylight conditions refer to time
between 12:00 and 15:00, and night conditions from 21:00 to
03:00. All experiments were conducted from April until June
of 2007 is Samos island.
Fig. 2. Daylight conditions, T = 0.6
B. Evaluation of Sound
The evaluation of the audial result is subjective. For this
reason, a query survey was conducted on users who showed
interest in the system.
The answers were not on the sound alone, but usually
gave a general description of the system. In general, the
answers were from positive to enthusiastic. The results can be
Fig. 3. Night conditions, T = 0.2
categorized—more from their form and their content, rather
than from their content only—as:
• Critical Acceptance: Subjects who described their rela-
tionship with music and technology in full, recognized
many of the requirements and constrains of the system.
Their proposals were constructive. Circa 40% of the
subjects.
• Mild Criticism: Subjects who gave a brief description
proposed in general “a less electronic and more melodic”
sound. Circa 40% of the subjects.
• Incomplete Knowledge: Circa 20% of the subjects stated
that they don’t know enough to answer.
Users who answered questionnaires neither viewed already
answered nor did they interact between them. Questions on
the survey were answered before the procedure. Consequently,
these “groups/categories” emerged “by themselves”. The rela-
tionship to technology and the Dept. of studies, if a student,
didn’t effect the result explicitly. (Implicitly there is an equa-
tion of technology with computers.)
During the development of the application we had the
opportunity to evaluate the audial part ourselves giving input
from the mouse’s coordinates (mouseX, mouseY), until the
desired result was achieved. That couldn’t happen with the
video part, which had to work completely in order to have
any form of evaluation.
The questionnaire also included questions relevant to the
claim of section III-C4 about T , as also a “General Remarks”
field.
V. CONCLUSIONS & FUTURE WORK
We created a synthesizer which is controlled remotely by
the body movements of the musician/user. The movements are
tracked and translated into parameters which are used as input
for the synthesizer. This system was evaluated by end users
with varying musical and technological backgrounds and the
feedback was positive.
We believe that the body as an interface will be interesting
to children, as a part of their education in music. Besides the
Authorized licensed use limited to: Aegean University. Downloaded on July 27,2010 at 15:38:24 UTC from IEEE Xplore.  Restrictions apply. 
EVANGELOS KAPROS ET. AL: AN AUDIOVISUAL VIRTUAL INTERFACE 485
results in [17], this belief has been enhanced from discussions
with members of the Primary School in Samos.
As for technical future work, an application that controls
sound not through brightness/metaballs, but through colour
recognition could be built. Such an application might need
more advanced media than a simple webcam in order to have
higher resolution, or faster CPUs to manage higher resolution
videos and working such media with the Java Virtual Machine.
With the spread of tools like Processing, which begin a
new era in coding (and in the psychology of application
development), there can be a bloom of creative audiovisual
applications.
ACKNOWLEDGEMENTS
Dr. E. Stamatatos, without whom the video controlled syn-
thesizer project, would never come to life. S. Karanikolopou-
los for important clarifications in acquired mathematics. E.
Vouziou and V. Argyropoulou for supporting the questionnaire
research and K. Latsiou for all the help in Mathemartics
conference. A. Madika helped with the use of QuickTime
and I. Argyropoulos friendly recommended improvements on
everything. S. El Koutbia made comments on the text of this
paper. They are all thanked. Important and inspiring has been
the study on the work of C. Castoriadis.
REFERENCES
[1] C. Castoriadis, “The Social Imaginary and the Institution”, The Imagi-
nary Institution of Society, The MIT Press, pp. 165-373, 1978.
[2] J. Wood, “Preface: Curvatures in Space-time-truth”, The Virtual Embod-
ied, Routledge, London, pp. 1-12, 1998.
[3] S. Rafaeli, “Interactivity: From new media to communication”, in R. P.
Hawkins, J. M. Wiemann, & S. Pingree (Eds.), Sage Annual Review of
Communication Research: Advancing Communication Science: Merging
Mass and Interpersonal Processes, 16, pp. 110-134, Sage, 1988.
[4] M. Merleau-Ponty, Phenomenology of Perception, Humanities Press,
New York, 1962.
[5] P. Light, K. Littleton, “Situational Effects in Computer-Based Problem
Solving”, Discourse, Tools, and Reasoning: Essays on Situated Cogni-
tion, Springer-Verlag, 1997.
[6] D. Svanćs, Understanding Interactivity: Steps to a Phenomenology of
Human-Computer Interaction, Ph.D. Thesis, NTNU, Norway, 1999.
[7] G. Levin, Painterly Interfaces for Audiovisual Performance, M.Sc.
Thesis, Massachusetts Institute of Technology, 2000.
[8] D. Schmalstieg, G. Reitmayr, “The World as a User Interface: Aug-
mented Reality for Ubiquitous Computing”, Central European Multi-
media and Virtual Reality Conference, 2005.
[9] N. J. Dellemana, E. den Dekkera, T. K. Tana, I3VR–Intuitive Interactive
Immersive Virtual Reality–Technology, France, 2006.
[10] G. Levin, and Z. Lieberman, “Sounds from Shapes: Audiovisual Perfor-
mance with Hand Silhouette Contours in "The Manual Input Sessions”,
Proceedings of NIME ’05, Vancouver, BC, Canada. May 26-28, 2005.
[11] E. Kapros, K. Raptis, Physical and Remote Control of Audial Parametres
via Video, Installation, 1st Mathemartics Conference, Greece, April
2007.
[12] G. Reitmayr, C. Chiu, A. Kusternig, M. Kusternig, H. Witzmann, “iOrb
- unifying command and 3d input for mobile augmented reality”, Proc.
IEEE VR Workshop on New Directions in 3D User Interfaces, IEEE,
pp. 7–10, 2005.
[13] S. Jordà, “Sonigraphical Instruments: From FMOL to the reacTable*”,
Proceedings of the 3rd Conference on New Interfaces for Musical
Expression (NIME 03), Montreal, Canada, 2003.
[14] G. Levin & Z. Lieberman, “In-Situ Speech Visualization in Real-Time
Interactive Installation and Performance”, Proceedings of The 3rd Inter-
national Symposium on Non-Photorealistic Animation and Rendering,
Annecy, France, 2004.
[15] A. Dix, J. Finlay, G. Abowd, R. Beale, Human-Computer Interaction,
Giourdas, 1997.
[16] N. Avouris, Introduction to Human-Computer Interaction, Diaulos,
Greece, 2000. (Greek text)
[17] A. Lamont, The Effects of Participating in Musical Activities, Depart-
ment of Psychology, Keele University, 2001.
Authorized licensed use limited to: Aegean University. Downloaded on July 27,2010 at 15:38:24 UTC from IEEE Xplore.  Restrictions apply. 
