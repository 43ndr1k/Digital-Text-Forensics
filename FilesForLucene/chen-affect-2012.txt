203H. Chen, Dark Web: Exploring and Data Mining the Dark Side of the Web, 
Integrated Series in Information Systems 30, DOI 10.1007/978-1-4614-1557-2_11, 
© Springer Science+Business Media, LLC 2012
 1  Introduction 
 The need for enhanced information retrieval and knowledge discovery from 
computer-mediated communication (CMC) archives has been articulated by many 
individuals in recent years. One suggested information access refi nement has been 
to mine directional text: text containing emotions and opinions (Hearst  1992 ; Wiebe 
 1994 ) . Affects play an important role in infl uencing people’s perceptions and 
decision making (Picard  1997 ) . Analysis of sentiment and affects is particularly 
important for online discourse, where such information is often more pervasive than 
topical content (Subasic and Huettner  2001 ; Nigam and Hurst  2004 ) . With the 
increased popularity of social computing, the presence and signifi cance of affective 
text is likely to grow (Liu et al.  2003 ) . There has been considerable recent work on 
sentiment analysis of online forums and product reviews (Turney and Littman  2003 ; 
Wiebe et al.  2004 ) . However, research on analysis of affects (including emotions 
and moods) is still relatively sparse (Cho and Lee  2006 ) . While recent studies have 
analyzed the presence of affects in blogs, online stories, chat dialog, transcripts, 
song lyrics, etc., it is unclear which features and techniques are most useful for 
affective computing of online texts. There is therefore a need to compare existing 
features for representing affective content as well as the techniques used for assigning 
emotive intensities. 
 In this chapter, we compare features and techniques for classifi cation of affective 
intensities in online text. The features investigated include a large set of learned 
n-grams as well as automatically and manually generated affect lexicons used in 
prior research. We also propose a support vector regression correlation ensemble 
(SVRCE) method for text-based affect classifi cation. SVRCE combines feature 
subset ensembles with affect correlation information for improved affect classifi cation 
performance. Evaluation of the various feature representations and the proposed 
method in comparison with existing affect analysis techniques found that the use of 
SVRCE with n-grams is highly effective for affect classifi cation of online forums, 
blogs, and stories. 
 Chapter 11 
 Affect Analysis 
204 11 Affect Analysis
 The remainder of this chapter is organized as follows: Section  2 provides a 
review of related work on textual affect analysis. Section  3 outlines our research 
framework based on gaps and questions derived from the literature review. Section  4 
presents an experimental evaluation of the various features and techniques incorpo-
rated in our framework. Section  5 features a brief case study illustrating how the 
proposed affect analysis methods can be applied to large CMC archives. Section  6 
contains concluding remarks and describes future research directions. 
 2  Related Work 
 Affect analysis is concerned with the analysis of text containing emotions (Picard 
 1997 ; Subasic and Huettner  2001 ) . Emotional intelligence, the ability to effectively 
recognize emotions automatically, is crucial for learning-preference-related informa-
tion and determining the importance of particular content (Picard et al.  2001 ) . Affect 
analysis is associated with sentiment analysis, which looks at the directionality of text, 
i.e., whether a text segment is positively or negatively oriented (Hearst  1992 ) . However, 
there are two major differences between affect analysis and sentiment analysis. First, 
affect analysis involves a large number of potential emotions or affect classes (Subasic 
and Huettner  2001 ) . These include happiness, sadness, anger, hate, violence, excite-
ment, fear, etc. In contrast, sentiment analysis primarily deals with positive, negative, 
and neutral sentiment polarities. Second, while the sentiments associated with particu-
lar words or phrases are mutually exclusive, text segments can contain multiple affects 
(Subasic and Huettner  2001 ; Grefenstette et al.  2004b ) . For example, the sentence 
“I can’t stand you!” has only a negative sentiment polarity but simultaneously con-
tains hate and anger affects. Word-level examples include the verb form of “alarm,” 
which can be attributed to fear, warning, and excitement affects (Subasic and Huettner 
 2001 ) , and the adjective “gleeful,” which can be assigned to the happiness and excite-
ment affect classes (Grefenstette et al.  2004b ) . Additionally, certain affect classes may 
be correlated (Subasic and Huettner  2001 ) . For instance, hate and anger often co-
occur in text segments, resulting in a positive correlation. Similarly, happiness and 
sadness are opposing affects that are likely to have a negative correlation. In summary, 
affect analysis involves assigning text with emotive intensities across a set of mutu-
ally inclusive and possibly correlated affect classes. Important affect analysis charac-
teristics include the features used to represent the presence of affects in text, techniques 
for assigning affective intensity scores, and the level of text granularity at which the 
analysis is performed. Table  11.1 presents a summary of the relevant prior studies 
based on these important affect analysis characteristics. 
 Based on the table, we can make several observations regarding the features and 
techniques used in previous affect analysis research.
 1.  Most prior research has used either manually generated lexicons, lexicons 
automatically created using WordNet or semantic orientation, or generic feature 
representations such as word and part-of-speech tag n-grams. It is unclear which 
of these feature representations is most effective for affect analysis. 
2052 Related Work
 Table 11.1  Related prior affect analysis studies 
 Study  Features  Technique(s) 
 Analysis 
level  Test bed and results 
 Donath et al. 
 ( 1999 ) 
 Manual lexicon, 
punctuation 
 Posting scoring  Posting  Greek Usenet forums; 
visualization of 
anger intensities 
over time 
 Subasic and 
Huettner 
 ( 2001 ) 
 Manual lexicon 
(fuzzy semantic 
typing) 
 Word scoring  Word  Movie reviews and 
news stories; 
visualization 
of 83 affects 
 Liu et al.
  ( 2003 ) 
 Language patterns 
derived from 
knowledge base 
 Sentence scoring  Sentence  User study on e-mail 
browser 
 Chuang 
and Wu 
 ( 2004 ) 
 Manual lexicon  Support vector 
machine (SVM) 
 Sentence  Drama broadcast 
transcripts; 76.44% 
accuracy for 7 class 
experiments 
 Grefenstette 
et al. 
 ( 2004a ) 
 Manual lexicon, 
semantic 
orientation 
 Manual tagging, 
pointwise mutual 
information (PMI) 
 Word  Candidate affect words; 
scored intensities 
across 86 affects 
 Grefenstette 
et al. 
 ( 2004b ) 
 Manual lexicon  Word scoring  Word  Political web pages; 
scored text relating 
to certain topic 
 Read  ( 2004 )  Semantic 
orientation 
 Pointwise mutual 
information 
(PMI) 
 Sentence  Short stories; 47.14% 
accuracy for 2 class 
experiments 
 Ma et al. 
 ( 2005 ) 
 Manual lexicon 
(WordNet-Affect 
database) 
 Word scoring  Sentence  Instant messaging chat 
data; no formal 
evaluation 
 Mishne 
 ( 2005 ) 
 BOWs, POS tags, 
document length, 
emphasized 
words, semantic 
orientation, 
WordNet lexicon 
 Support vector 
machine (SVM) 
 Posting  LiveJournal blog 
postings; 60.25% 
accuracy for 2 class 
experiments 
 Cho and Lee 
 ( 2006 ) 
 Manual lexicon, 
BOWs 
 Sentencez scoring, 
support vector 
machine (SVM) 
 Song  Korean song lyrics; 
77.3% accuracy on 
5 class experiments 
 Mishne and 
Rijke 
 ( 2006 ) 
 Word n-grams  Pace regression  Posting  LiveJournal blog 
postings; average 
error of 52.53%, 
correlation 
coeffi cient of 0.827 
for 2 class 
experiments 
 Wu et al. 
 ( 2006 ) 
 Emotion generation 
and association 
rules 
 Separable mixture 
models 
 Posting  Student chat dialog; 
80.98% accuracy 
for 3 class 
experiments 
206 11 Affect Analysis
 2.  Techniques used for assigning affect intensities can be predominantly categorized 
into scoring methods and machine learning techniques. However, we are unaware of 
any prior work attempting to compare various techniques for affect classifi cation. 
 3.  Previous affect classifi cation studies typically utilized between two and seven 
affect classes, applied at the word, sentence, or document levels. Despite the 
presence of multiple interrelated affects (Subasic and Huettner  2001 ; Grefenstette 
et al.  2004b ) , class correlation information was not leveraged for improved affect 
intensity assignment. Additionally, regression-based methods have seen limited 
usage despite their effectiveness in related application domains (Pang and Lee 
 2005 ; Schumaker and Chen  2006 ) . 
 4.  Prior studies mainly focused on a single application domain, such as movie 
reviews, web forums, blogs, chat dialog, song lyrics, stories, etc. Given the differ-
ences in the degree of interaction, language usage, and communication structure 
across these domains, it is unclear if an approach suitable for classifying story 
affects will be applicable on web forums and blogs. The features and techniques 
used in prior affect analysis research are expounded upon in the remainder of 
the section. 
 2.1  Features for Affect Analysis 
 The attributes used to represent affects can be classifi ed into lexicon-based features 
and generic n-gram-based features. Considerable prior research has used manually 
or automatically generated lexicons. As previously stated, in affect lexicons, the 
same word/phrase can be assigned to multiple affect classes. The intensity score for 
an attribute is based on its degree of severity toward that particular affect class. 
Depending upon the semantic relation between affects, certain classes can have a 
positive or negative occurrence correlation (Subasic and Huettner  2001 ) . 
 Many studies have incorporated manually developed affect lexicons. Subasic 
and Huettner  ( 2001 ) used fuzzy semantic typing where each feature was assigned to 
multiple affect categories with varying intensity and centrality scores depending 
upon the word and usage context. For example, the word “rat” was assigned to the 
disloyalty, horror, and repulsion affect categories with intensity scores of 0.9, 0.6, 
and 0.7, respectively (on a 0.0–1.0 scale where 1.0 was highest). In order to compen-
sate for word-sense ambiguity, their approach also assigned each word-affect pair a 
centrality score indicating the likelihood of the word being used for that particular 
affect class. For example, the word “rat” was assigned a centrality score of 0.3 for 
the disloyalty affect and 0.6 for the repulsion affect (also on a 0.0–1.0 scale) since 
the usage of “rat” to convey disloyalty is not as common. Thus, while “rat” was 
more intense for the disloyalty affect, it was also less central to this class. In Subasic 
and Huettner’s  ( 2001 ) approach, the intensity and centrality scores were both uti-
lized for determining the affective composition of a text document. Although the 
accuracy for specifi c term affects may be inaccurate, the fuzzy logic approach is 
2072 Related Work
intended to capture the essence of a document’s various affect intensities. A similar 
method for generating manual lexicons was employed in related work (Grefenstette 
et al.  2004a,  b ) . Many other studies have also utilized manually constructed affect 
lexicons (Chuang and Wu  2004 ; Cho and Lee  2006 ) . Donath et al.  ( 1999 ) used a set 
of keywords relating to anger for analyzing Usenet forums. Ma et al.  ( 2005 ) incor-
porated the WordNet-Affect database created by Valitutti et al.  ( 2004 ) . This database 
is comprised of manually assigned affect intensities for words found in the WordNet 
lexical resource (Fellbaum  1998 ) . Liu et al.  ( 2003 ) manually constructed sentence-
level language patterns for identifi cation of six affect classes, including happiness, 
sadness, anger, fear, etc. 
 Although manually created affect lexicons can provide powerful insight, their 
construction can be time consuming and tedious. As a result, many studies have 
explored the use of automated lexicon generation methods such as semantic orienta-
tion (Grefenstette et al.  2004a ; Read  2004 ; Mishne  2005 ) and WordNet lexicons 
(Mishne  2005 ) . These methods take a small set of manually generated seed/
paradigm words which accurately refl ect the particular affect class and use automated 
methods for lexicon expansion of candidate word scoring. 
 Based on the work of Turney and Littman  ( 2003 ) , the semantic orientation 
approach assesses the intensity of each word based on its frequency of co-occurrence 
with a set of core paradigm words refl ective of that affect class (Grefenstette et al. 
 2004a ) . The occurrence frequencies for the paradigm words and candidate words 
are derived from search engines such as AltaVista (Grefenstette et al.  2004a ; Read 
 2004 ; Mishne  2005 ) or Yahoo! (Mishne  2005 ) . The number of paradigm words used 
for a particular affect class is generally fi ve to seven (Grefenstette et al.  2004a ; Read 
 2004 ) . For example, the paradigm words for the  praise affect may include “acclaim, 
praise, congratulations, homage, approval” (Grefenstette et al.  2004a ) , and addi-
tional lexicon items generated automatically using semantic orientation include the 
words “award, honor, extol.” The semantic orientation approach is typically coupled 
with a pointwise mutual information (PMI) scoring mechanism for assigning candi-
date words intensity scores (Turney and Littman  2003 ) . Traditional PMI assigns 
each word a score based on how often it occurs in proximity with positive and 
negative paradigm words; however, it has been modifi ed to be applicable with affect 
classes (Read  2004 ; Grefenstette et al.  2004a ) . The affect analysis rendition of PMI 
proposed by Grefenstette et al.  ( 2004a ) is as follows:
  
2
2
hits(  Near )
PMI Score( , ) log
log (hits( ))
word cword
cword Classword Class
cword
cword Class
⎛ ⎞Π
∈⎜ ⎟= ⎜ ⎟Π
⎝ ⎠∈
  
where  cword is one of the paradigm words chosen for an affect class  Class and  hits 
is the number of pages found by Alta Vista. 
 Another automated affect lexicon generation method is WordNet lexicons. 
Originally proposed by Kim and Hovy  ( 2004 ) , this method is similar to semantic 
orientation. However, it uses WordNet to expand the seed words associated with a 
208 11 Affect Analysis
particular affect class by comparing each candidate word’s synset with the seed 
word list (Mishne  2005 ) . The intensity for a candidate word is proportional to the 
percentage of its synset also present in the seed word list for that particular affect 
class. Word scores are assigned using the following formula (Kim and Hovy  2004 ) :
  1
1
WordNet Score( , ) ( ) ( , )
( )
n
i
i
word Class P Class count syn Class
count c =
= ∑
  
where  Class is an affect class,  syn 
 i 
 is one of the  n synonyms of  word , and  P ( Class ) 
is the number of words in  Class divided by the total number of words considered. 
 In addition to lexicon-based affect representations, studies have also used generic 
n-gram features. Mishne  ( 2005 ) used bag-of-words (BOWs) and part-of-speech 
(POS) tags in combination with automatically generated lexicons, while Mishne 
and Rijke  ( 2006 ) used word n-grams for affect analysis of blog postings. Cho and 
Lee  ( 2006 ) used BOWs for classifying affects inherent in Korean song lyrics. 
N-grams have also been shown to be highly effective in the related area of sentiment 
classifi cation (Wiebe et al.  2004 ; Abbasi et al.  2008 ) , especially when combined 
with machine learning methods capable of learning n-gram patterns conveying 
opinions and emotions. While prior research has used various n-gram and lexicon 
representations, we are unaware of any work done to evaluate the effectiveness of 
various potential affect analysis features. 
 2.2  Techniques for Assigning Affect Intensities 
 Prior research has utilized scoring and machine learning methods for assigning affect 
intensities. Scoring-based methods, which are generally used in conjunction with 
lexicons, typically use the average intensity across lexicon items occurring in the text 
(i.e., word spotting) (Subasic and Huettner  2001 ; Liu et al.  2003 ; Cho and Lee  2006 ) . 
Sentence-level averaging has also been performed in combination with the word-level 
PMI scores generated using semantic orientation (Turney and Littman  2003 ) as well 
as with WordNet lexicons (Kim and Hovy  2004 ) . Studies that directly developed 
lexicons comprised of sentence patterns obviously do not use averaging (at least at the 
sentence level), but instead simply matching sentences with lexicon entries and assign-
ing intensity scores accordingly (Liu et al.  2003 ; Cho and Lee  2006 ) . 
 Machine learning techniques have also been used for assigning affect intensities. 
Many studies used support vector machine (SVM) for determining whether a text 
segment contained a particular affect class (Chuang and Wu  2004 ; Mishne  2005 ; 
Cho and Lee  2006 ) . One shortcoming of using SVM is that it can only deal with 
discrete class labels, whereas affect intensities can vary along a continuum. Recent 
work has attempted to address this problem by using regression-based classifi ers 
(Pang and Lee  2005 ) . For example, Mishne and Rijke  ( 2006 ) used word n-grams in 
unison with Pace regression (Witten and Frank  2005 ) for assigning affect intensities 
2093 Research Design
in LiveJournal blogs. Nevertheless, regression-based learning methods have seen 
limited usage despite their effectiveness in related application domains such as 
using news story text for stock price prediction (Schumaker and Chen  2006 ) . 
Furthermore, although scoring and machine learning methods have been utilized for 
classifying affect intensities, there has been no research done to investigate the 
effectiveness of these methods. 
 3  Research Design 
 In this section, we highlight affect analysis research gaps based on our review of the 
related work. Research questions are then posed based on the relevant gaps identifi ed. 
Finally, a research framework is presented in order to address these research 
questions, along with some research hypotheses. The framework encompasses 
various feature representations and techniques for assigning affective intensities 
to sentences. 
 3.1  Gaps and Questions 
 Prior research has utilized manually or automatically generated lexicons as well as 
generic n-gram features for representing affective content in text. Since most studies 
used a single feature category and did not compare different alternatives, it is unclear 
which emotive representation is most effective. Furthermore, prior research has 
used scoring-based techniques and machine learning methods such as SVM. 
Regression-based methods capable of assigning continuous intensity scores have 
not been explored in great detail, with the exception of Mishne and Rijke  ( 2006 ) . 
Leveraging the relationship between mutually inclusive affect classes in combina-
tion with powerful regression-based machine learning methods such as support 
vector regression (SVR) could be highly effective for accurate assignment of affect 
intensities. Additionally, most prior affect analysis research was applied to a single 
domain (e.g., blogs, stories, etc.). Application across multiple domains could lend 
greater validity to the effectiveness of affect analysis features and techniques. Based 
on these gaps, we present the following research questions:
 Which feature categories are best at accurately assigning affect intensities?• 
 Can the use of an extended feature set enhance affect analysis performance  –
over individual generic and lexicon-based feature categories? 
 Can a regression ensemble that incorporates affect correlation information • 
outperform existing machine learning and scoring-based methods? 
 What impact will the application domain have on affect intensity assignment? • 
210 11 Affect Analysis
 3.2  Research Framework 
 Our research framework (shown in Fig.  11.1 ) relates to the features and techniques 
used for assigning affect intensity scores. 
 We intend to compare generic n-gram features with automatically and manually 
generated lexicons. We also plan to assess the effectiveness of using an extended 
feature set encompassing all these attributes in comparison with individual feature 
categories. With respect to affect analysis techniques, we propose a support vector 
regression (SVR) ensemble that considers affect correlation information when assign-
ing emotive intensities to sentences. We intend to compare the SVR correlation 
ensemble (SVRCE) with other machine learning and scoring-based methods used in 
prior research. These include Pace regression (Witten and Frank  2005 ; Mishne and 
Rijke  2006 ) , semantic orientation (Grefenstette et al.  2004a ; Read  2004 ) , WordNet 
(Kim and Hovy  2004 ) , and manual lexicon scoring (Subasic and Huettner  2001 ) . 
 We also plan to perform ablation testing to see how the different components of 
the proposed SVRCE method contribute to its overall performance. All testing will 
be performed on several test beds encompassing sentences derived from web forums, 
blogs, and stories. Features and techniques will be evaluated with respect to their 
percentage mean error and correlation coeffi cients in comparison with a human-
annotated gold standard. Further details about the features, techniques, ablation 
testing, and our research hypotheses are presented below, while the test bed and 
evaluation metrics are discussed in greater detail in the ensuing evaluation section. 
 Fig. 11.1  Affect analysis research framework 
 
2113 Research Design
 3.2.1  Affect Analysis Features 
 The n-gram feature set is comprised of word, character, and part-of-speech (POS) 
tag n-grams. For each n-gram category, we used up to trigrams only (i.e., unigrams, 
bigrams, and trigrams), as done in prior related research (Pang et al.  2002 ; Wiebe 
et al.  2004 ) . Word n-grams, including unigrams (e.g., “LIKE”), bigrams (e.g., 
“I LIKE,” “LIKE YOU”), and trigrams (e.g., “I LIKE YOU”), as well as POS tag 
n-grams (e.g., “NP VB,” “JJ NP VB”) have been used in prior affect analysis 
research (Mishne  2005 ) . We also include character n-grams (e.g., “li,” “ik,” “ike”), 
which have been useful in related sentiment classifi cation studies (Abbasi et al. 
 2008 ) . In addition to standard word n-grams, we incorporate hapax legomena and 
dis legomena collocations (Wiebe et al.  2004 ) . Such collocations replace once- 
(hapax legomena) and twice-occurring words (dis legomena) with “HAPAX” and 
“DIS” tags. Hence, the trigram “I hate Jim” would be replaced with “I hate HAPAX” 
provided “Jim” only occurs once in the corpus. The intuition behind such colloca-
tions is to remove sparsely occurring words with tags that will allow the extracted 
n-grams to be more generalizable, and hence, more useful (Wiebe et al.  2004 ) . For 
instance, in the above example, the fact that the writer hates is more important from 
an affect analysis perspective than the specifi c person the hate is directed toward. 
 The lexicons employed are comprised of automated lexicons derived using 
semantic orientation and WordNet models as previously done by Grefenstette et al. 
 ( 2004a ) and Mishne  ( 2005 ) . We selected seven paradigm words for each affect class 
for input into the semantic orientation algorithm, as described in Sect.  2.1 . For the 
WordNet models, sets of up to 50 words were used as the seeds, following the 
guidelines described by Kim and Hovy  ( 2004 ) . 
 Our feature set also consists of a manually crafted word-level lexicon. The lexicon 
is comprised of over 1,000 affect words for several emotive classes (e.g., happiness, 
sadness, anger, hate, violence, etc.). Each word is assigned an intensity and ambiguity 
score between 0 and 1. The intensities are assigned based on the word’s degree of 
severity or valence for its particular affect category (with 1 being highest). This 
approach is consistent with the intensity score assignment methods incorporated in 
previous studies that utilized manually crafted lexicons (Donath et al.  1999 ; Subasic 
and Huettner  2001 ; Grefenstette et al.  2004b ; Chuang and Wu  2004 ) . Each affect 
feature is also assigned an ambiguity score. The ambiguity score is the probability 
of an instance of the feature having semantic congruence with the affect class repre-
sented by that feature. The ambiguity score for each feature is determined by taking 
a sample set of instances of the feature’s occurrence and coding each occurrence as 
to whether the term usage is relevant to its affect. A maximum of 20 samples was 
used per term. Using more instances would be exhaustive, and we observed that the 
size used was suffi cient to accurately capture the probability of an affect being 
relevant. The ambiguity score for each word can be computed as the number of 
correctly appearing instances divided by the total number of instances sampled for 
that word. Hence, an ambiguity value of one suggests that the term always appears 
in the appropriate affective connotation. The intensity and ambiguity assignment 
was done by two independent coders. Each coder initially assigned values without 
212 11 Affect Analysis
consulting the other. The coders then consulted one another in order to resolve 
tagging differences. The inter-coder reliability tests revealed a kappa statistic of 
0.78 prior to coder discussions and 0.89 after discrepancy resolution. For situations 
where the disparity could not be resolved even after discussions, the two coders’ 
values were averaged. Table  11.2 shows examples from the violent affect lexicon. 
The weight for each term is the product of its intensity and ambiguity value. This is 
the value assigned to each occurrence of the term in the text being analyzed. For 
example, “lynch” was considered more severe by the coders than “hang.” Although 
the two terms represent similar actions, the more severe motivation behind “lynch” 
as compared to “hang” resulted in a higher intensity score. Furthermore, the word 
“lynch” was also less ambiguous, conveying only a single violent meaning in the 
samples analyzed by the coders during the disambiguation procedure. 
 3.2.2  Affect Analysis Techniques 
 Ensemble classifi ers use multiple classifi ers, with each built using different techniques, 
training instances, or feature subsets (Dietterich  2000 ) . Particularly, the feature 
subset classifi er approach has been shown to be effective for analysis of style and 
patterns. Stamatatos and Widmer  ( 2002 ) used an SVM ensemble for music performer 
recognition. They used multiple SVMs, each trained using different feature subsets. 
Similarly, Cherkauer  ( 1996 ) used a neural network ensemble for imagery analysis. 
Their ensemble consisted of 32 neural networks trained on eight different feature 
subsets. The intuition behind using a feature ensemble is that it allows each classifi er 
to act as an “expert” on its particular subset of features (Cherkauer  1996 ; Stamatatos 
and Widmer  2002 ) , thereby improving performance over simply using a single 
classifi er. We propose the use of a support vector regression ensemble that incorpo-
rates the relationship between various affect classes in order to enhance affect clas-
sifi cation performance. Our ensemble includes multiple SVR models, each trained 
using a subset of features most effective for differentiating emotive intensities for a 
single affect class. We use the information gain (IG) heuristic to select the features 
for each SVR classifi er. Since affect intensities are continuous, discretization must be 
performed before IG can be applied. We use 5 and 10 class bins (e.g., an intensity 
value of 0.15 would be placed into class 1 of 5 and 2 of 10 using 5 and 10 class bins). 
 Table 11.2  Manual lexicon examples for the violence affect 
 Term  Intensity  Ambiguity  Weight 
 Hit  0.210  0.800  0.168 
 Beat  0.400  0.667  0.267 
 Stab  0.575  1.000  0.575 
 Hang  0.800  0.650  0.520 
 Kill  0.850  0.950  0.808 
 Lynch  1.000  1.000  1.000 
2133 Research Design
All features with an average information gain greater than a threshold  t are selected, 
as done in prior research (Yang and Pederson  1997 ) . 
 The support vector regression correlation ensemble (SVRCE) adjusts the affect 
intensity prediction for a particular sentence based on the predicted intensities of 
other affects. The amount of adjustment is proportional to the level of correlation 
between affect classes (i.e., the affect class being predicted and the ones being used 
to make the adjustment) as derived from the training data. The SVRCE formulation 
is shown in Fig.  11.2 . 
 The rationale behind SVRCE is that in certain situations, a particular sentence 
may get misclassifi ed by a trained model due to a lack of prior exposure to the 
affective cues inherent in its text. In such circumstances, leveraging the relation-
ship between affect classes may help alleviate the magnitude of such erroneous 
classifi cations. 
 We intend to compare the proposed SVRCE method against machine learning 
and scoring-based methods used in prior affect analysis research. These include the 
Pace regression technique proposed by Witten and Frank  ( 2005 ) which was used to 
analyze affect intensities in weblogs (Mishne and Rijke  2006 ) as well as the semantic 
orientation, WordNet model, and manual lexicon scoring approaches. In addition to 
comparing the proposed SVRCE against other affect analysis techniques, we also 
intend to perform ablation testing to better understand the impact different compo-
nents of our proposed method have on classifi cation performance. Since SVRCE 
uses correlation information and feature-subset-based ensembles, we plan to com-
pare it against an SVR ensemble that does not use correlation information as well as 
an SVR trained using a single feature set for all affect classes. The hypotheses 
associated with our research framework are presented below. 
(cx − c) (ax − a)
(cx − c)(ax − a)
 
    For:
22
cx and ax are the actual intensity values for affects c and a assigned to x ∈ M; 
c and a are the average intensity values for affects c and a across the m training instances.
mm
m
x=1
x=1 x=1
∑∑
∑
(Corr(c,a)2(SVRa (i) −  SVRc(i))K )
Corr(c,a) =
Where:
SVRCEc(i) = SVRc (i) +
SVRc(i) is the prediction for instance i for affect class c using an SVR model trainde on M;
the feature subset for SVRc is selected using the IG heuristic;
c and a are part of the set of n affect classes being investigated, and c ≠ a; 
The SVR correlation ensemble intensity score for instance i and affect class c can be computed as follows:
Let M={1,2 ,...m}denote the set of training instances. 
Corr(c,a) is the correlation coefficient for affect classes c and a across the m training instances as follows:
n
a=1
K = 1 if Corr(c,a) > 0, K = −1 otherwise;
∑
 Fig. 11.2  SVR correlation ensemble for assigning affect intensities 
 
214 11 Affect Analysis
 3.3  Research Hypotheses 
 H1: Features 
 The use of learned generic n-gram features will outperform manually and auto-
matically crafted affect lexicons. Additionally, using an extended feature set encom-
passing all features will outperform individual feature sets.
 H1a: N-Grams > manual lexicon, semantic orientation, WordNet models • 
 H1b: All features > n-grams, manual lexicons, semantic orientation, WordNet • 
models 
 H2: Techniques 
 The proposed SVRCE method will outperform comparison techniques used in prior 
studies for affect analysis.
 H2: SVRCE > Pace regression, semantic orientation scores, WordNet model • 
scores, manual lexicon scores 
 H3: Ablation Testing 
 The SVRCE method will outperform an SVR ensemble not using correlation infor-
mation as well as SVR run using a single feature set. Furthermore, the SVR ensem-
ble will also signifi cantly outperform SVR run using a single feature set.
 H3a: SVRCE > SVR ensemble, SVR • 
 H3b: SVR ensemble > SVR • 
 4  Evaluation 
 We conducted experiments to evaluate various affective feature representations 
along with different affect analysis techniques, including the proposed support vector 
regression correlation ensemble (SVRCE). The experiments were conducted on four 
test beds comprised of sentences taken from web forums, blogs, and short stories. 
This section encompasses a description of the test beds, experimental design, exper-
imental results, and outcomes of the hypotheses testing. 
 4.1  Test Bed 
 Analyzing affect intensities across application domains is important in order to get 
a better sense of the effectiveness and generalizability of different features and 
techniques. As a result, our test bed consisted of sentences taken from two corpora 
(shown in Table  11.3 ). The fi rst test bed was a set of supremacist web forums dis-
cussing issues relating to Nazi and socialist ideologies. The second was comprised 
of 1,000 sentences taken from a couple of Arabic language Middle Eastern forums 
discussing issues relating to the war in Iraq. Analysis of such forums is important to 
better understand cyber activism, social movements, and people’s political sentiments. 
2154 Evaluation
 Two independent coders tagged the sentences for intensities across the four affect 
classes used for each test bed (shown in Table  11.3 ). Each sentence was tagged with 
an intensity score between 0 and 1 (with 1 being most intense) for each of the affects. 
The tagging followed the same format as the one used for the manual lexicon cre-
ation. Each coder initially assigned values without consulting the other. The coders 
then consulted one another in order to resolve tagging differences. For situations 
where the disparity could not be resolved even after discussion, the two coders’ 
 values were averaged. The inter-coder reliability kappa values shown in Table  11.3 
are from after discrepancy resolution (prior to averaging). For the Middle Eastern 
forums, the coders were unable to meet to resolve coding differences. For this test 
bed, the kappa value shown is for the two coders’ initial tagging. 
 4.2  Experimental Design 
 Based on our research framework and hypotheses presented in Sect.  3 , three experi-
ments were conducted. The fi rst was intended to compare the performance of 
learned n-grams against manually and automatically crafted lexicons. We also 
investigated the effectiveness of an extended feature set comprised of n-grams and 
lexicons versus individual feature groups. The second experiment compared differ-
ent affect analysis techniques, including the proposed SVRCE, Pace regression, and 
scoring methods. The fi nal experiment pertained to ablation analysis of the major 
components of SVRCE, including the use of correlation information and an ensem-
ble approach to affect classifi cation. In order to allow statistical testing of results, 
we ran 50 bootstrap instances for each condition across all three experiments. In 
each bootstrap run, 95% of the sentences were randomly selected for training while 
the remaining 5% were used for testing (Argamon et al.  2007 ) . The average results 
across the 50 bootstrap runs were reported for each experimental condition. 
Performance was evaluated using standard metrics for affect analysis, which include 
the mean percentage error and the correlation coeffi cient (Mishne and Rijke  2006 ) :
  
( )( )
( ) ( )2 2
100
Mean % Error Corr( , )
n
x x y y
x y X Y
x x y y
− −
= − =
− −
∑∑
∑ ∑  
 
 Table 11.3  Test bed description 
 Test bed name  Source URL(s)  No. of sentences 
 Affect classes 
tagged 
 Inter-coder 
reliability 
 Supremacist Web 
forums (SF) 
 www.stormfront.org 
 www.nazi.org 
 1,000  Violence, anger, 
hate, racism 
 0.89 
 Middle Eastern Web 
forums (MEF) 
 www.montada.com 
 www.alfi rdaws.com 
 1,000  Violence, anger, 
hate, racism 
 0.79* 
 *Kappa value from initial tagging 
216 11 Affect Analysis
where  x and  y are the actual and predicted intensity values for one of the n testing 
instances denoted by the vectors  X and  Y. 
 4.3  Experiment 1: Comparison of Feature Sets 
 In this experiment, we compared generic n-grams with semantic orientation (SO), 
WordNet model (WNet), and the manual lexicon (ML). We also constructed an 
extended feature set comprised of n-grams, SO, WNet, and ML (labeled “All”). All 
feature sets were evaluated using the support vector regression correlation ensemble 
(SVRCE). SVRCE was run using a linear kernel. N-grams were selected using the 
information gain heuristic applied at the affect level, as outlined in Sect.  3.2.2 . The 
information gain was applied to the training data during each of the 50 bootstrap 
instances; these features were then used to train the SVRCE classifi ers used on the 
testing data. This resulted in 16 n-gram feature subsets (one for each affect class 
across the four test beds) and a corresponding SVRCE model for each feature subset. 
SO and WNet were run using the formulas described in Sects.  2.1 and  3.2.2 . For SO, 
WNet, and ML, the word-level scores were computed for each sentence, resulting 
in a vector of scores for each sentence. Since different paradigm/seed words were 
used for each affect across all four test beds, the lexicon methods also generated 16 
sets of sentence vectors each. Consistent with Mishne  ( 2005 ) , these vectors were 
treated as features input into the SVRCE. For the “All” feature set, the lexicon sentence 
vectors were merged with the n-gram frequency vectors. 
 Table  11.4 shows the macrolevel experimental results for the mean percentage 
error and correlation coeffi cients across the fi ve feature sets applied to the two test 
beds. The values shown were averaged across the four affect classes used within 
each test bed. The test bed labels correspond to the abbreviations presented in 
Table  11.3 under the column “Test bed name.” The n-gram features appeared to 
have the best performance, with the lowest mean percentage error and highest cor-
relation coeffi cient for all test beds. The automated (i.e., SO and WNet) and manual 
lexicons all had fairly similar performance, with mean errors typically in the 5–7% 
range and correlation coeffi cients between 0.2 and 0.5. As anticipated, the use of all 
features performed well, outperforming the use of individual lexicons. Surprisingly 
however, using all features (i.e., n-grams in conjunction with lexicons) did not out-
perform the use of n-grams alone. N-grams outperformed the extended feature set 
 Table 11.4  Overall results for various feature sets 
 Features/test bed 
 Mean % error  Correlation coeffi cient 
 SF  MEF  SF  MEF 
 N-grams  4.6360  3.8066  0.6627  0.7455 
 SO  5.0725  4.4742  0.4558  0.5308 
 WNet  4.9646  4.5507  0.4952  0.5122 
 ML  4.9767  4.6147  0.5388  0.4121 
 All  4.8176  4.3522  0.6238  0.6036 
2174 Evaluation
by as much as 0.5% and 0.14 on mean error and correlation coeffi cient, respectively. 
This suggests that the learned n-grams were able to effectively represent affective 
patterns in the text. Adding lexicon features introduced redundancy and, in some 
instances, noise. Further elaboration regarding the performance of n-grams in 
comparison with other feature sets is provided in the hypotheses testing section. 
 4.4  Experiment 2: Comparison of Techniques 
 The SVRCE method was compared against scoring and machine learning methods 
used in prior studies. The comparison techniques included Pace regression (Mishne 
and Rijke  2006 ) , WordNet (WNet) scores (Kim and Hovy  2004 ; Mishne  2005 ) , the 
pointwise mutual information scores from the semantic orientation (SO) approach, 
and the scores from our manual lexicon (ML). For SO, WNet, and ML, the average 
word-level intensities were used as the sentence-level scores as done in prior affect 
analysis research (Subasic and Huettner  2001 ; Grefenstette et al.  2004a ; Read  2004 ; 
Cho and Lee  2006 ) . SVRCE and Pace regression were both run using the n-gram 
features. N-grams were used since they had the best performance in experiment 1. 
Both techniques (i.e., SVRCE and Pace) were run using identical features, with 
each using 16 feature subsets selected using the information gain heuristic as 
described in experiment 1. Any scores outside the 0–1 range were adjusted to fi t the 
possible range of intensities (this was done in order to avoid infl ated errors stem-
ming from values well outside the feasible range). 
 Table  11.5 shows the macrolevel experimental results for the mean percentage 
error and correlation coeffi cients across the fi ve techniques. The SVRCE method 
had the best performance, with the lowest mean percentage error and highest 
correlation coeffi cient for all four test beds. Pace regression, WordNet (WNet) models, 
and the manual lexicon (ML) scoring methods were all in the middle, while the 
semantic orientation scoring method had the worst performance. The results are 
consistent with prior research that has also observed large differences between the 
word-level scores assigned using WNet and SO (Mishne  2005 ) . The machine learn-
ing methods (SVRCE and Pace) both fared well with respect to their correlation 
coeffi cients. Pace also performed well on the supremacist and Middle Eastern 
forums in terms of mean percentage error, but not on the blogs test bed (LJ). 
 Table 11.5  Results for experiment 2 (comparison of techniques) 
 Techniques/test bed 
 Mean % error  Correlation coeffi cient 
 SF  MEF  SF  MEF 
 SO  8.6590  14.8759  0.4673  0.2530 
 WNet  5.9899  8.6639  0.5837  0.5224 
 ML  6.7270  8.3860  0.5500  0.5251 
 SVRCE  4.6360  3.8066  0.6627  0.7455 
 Pace  6.3038  5.8473  0.5692  0.6124 
218 11 Affect Analysis
 4.5  Experiment 3: Ablation Testing 
 Ablation testing was performed to evaluate the effectiveness of the different SVRCE 
components. The SVRCE was compared against a support vector regression ensem-
ble (SVRE) that does not utilize correlation information, as well as a support vector 
regression classifi er using only a single feature set (SVR). The SVR was trained 
using a single feature set (for each test bed) selected by using all n-grams occurring 
at least fi ve times in the corpus (Jiang et al.  2004 ) . The SVRE and SVRCE were 
both run using information gain on the training data to select the 16 feature subsets 
most representative of each affect class. The experiment was intended to evaluate 
the two core components of SVRCE: (1) its use of feature ensembles to better rep-
resent affective content; (2) the use of correlation information for enhanced affect 
classifi cation. Table  11.6 shows the macrolevel results for the mean percentage error 
and correlation coeffi cients for SVRCE, SVRE, and SVR. 
 The SVRCE method had the best performance, with the lowest mean percentage 
error and highest correlation coeffi cient for all test beds. SVRCE marginally outper-
formed SVRE, while both techniques outperformed SVR. The results suggest that 
use of feature ensembles and correlation information are both useful for classifying 
affective intensities. 
 4.6  Hypotheses Results 
 We conducted pairwise t tests on the 50 bootstrap runs for all three experiments. 
Given the large number of comparison conditions, a Bonferroni correction was 
performed to avoid spurious positive results. All P values less than 0.0005 were 
considered signifi cant at alpha = 0.01. 
 4.6.1  H1: Feature Comparison 
 Table  11.7 shows the results for pairwise t tests conducted to compare the effective-
ness of the extended and n-gram feature sets with other feature categories. 
 N-grams and the extended feature set both signifi cantly outperformed the lexicon-
based representations on all test beds with respect to mean error and correlation (all 
P values < 0.0001). Surprisingly, the extended feature set did not outperform n-grams. 
 Table 11.6  Results for experiment 3 (ablation testing) 
 Techniques/test bed 
 Mean % error  Correlation coeffi cient 
 SF  MEF  SF  MEF 
 SVRCE  4.6360  3.8066  0.6627  0.7455 
 SVRE  5.0776  4.0667  0.5990  0.7231 
 SVR  5.7676  5.0460  0.5631  0.5757 
2194 Evaluation
In contrast, the n-gram feature set signifi cantly outperformed the use of all features 
(n-grams plus the three lexicons), with all P values signifi cant at alpha = 0.01. 
 Table  11.8 provides examples of learned n-grams taken from the LiveJournal test 
bed for the hate affect. 
 It also shows some related hateful items from the manual lexicon. The n-grams 
were able to learn many of the concepts conveyed in the lexicon. Furthermore, 
the n-grams were able to provide better context for some features and also learn 
deeper patterns in several instances. For example, the hate in LiveJournal blogs is 
often directed toward specifi c people and frequently involves places and times. This 
pattern is captured by the POS tag n-grams. In contrast, word lexicons cannot accu-
rately represent such complex patterns. The example illustrates how the n-gram 
features learned were more effective than the lexicons employed in this study. 
 Table 11.7  P values for pairwise t tests (n = 50) on feature comparisons 
 Comparison features 
 Mean % error  Correlation coeffi cient 
 SF  MEF  SF  MEF 
 All vs. n-gram  <0.0001*  <0.0001*  <0.0001*  <0.0001* 
 All vs. SO  <0.0001  <0.0001  <0.0001  <0.0001 
 All vs. WNet  <0.0001  <0.0001  <0.0001  <0.0001 
 All vs. ML  <0.0001  <0.0001  <0.0001  <0.0001 
 N-gram vs. SO  <0.0001  <0.0001  <0.0001  <0.0001 
 N-gram vs. WNet  <0.0001  <0.0001  <0.0001  <0.0001 
 N-gram vs. ML  <0.0001  <0.0001  <0.0001  <0.0001 
 *Result contradicts hypothesis 
 Table 11.8  Sample learned n-grams and lexicon items for hate affect 
 Learned n-grams 
 Lexicon items  Category  N-gram 
 Character n-grams  uck, ck, fuc  awful, stupid, terrible, 
sicken, s**t, f**k  Word n-grams  terribly, suck, the stupid, 
the s**t, the f**k 
 Hapax and dis legomena 
collocations 
 HAPAX so awful 
 POS tag n-grams  PERSON_SG, WEEKDAY_
NNP, TIME_SG 
 Table 11.9  P values for pairwise t tests (n = 50) on technique comparisons 
 Comparison features 
 Mean % error  Correlation coeffi cient 
 SF  MEF  SF  MEF 
 SVRCE vs. SO  <0.0001  <0.0001  <0.0001  <0.0001 
 SVRCE vs. WNet  <0.0001  <0.0001  <0.0001  <0.0001 
 SVRCE vs. ML  <0.0001  <0.0001  <0.0001  <0.0001 
 SVRCE vs. Pace  <0.0001  <0.0001  <0.0001  <0.0001 
220 11 Affect Analysis
 4.6.2  H2: Technique Comparison 
 As shown in Table  11.9 , the SVRCE method signifi cantly outperformed all four 
comparison techniques on mean percentage error and correlation coeffi cient across 
all four test beds. All P values were less than 0.0005 and therefore signifi cant at 
alpha = 0.01. The results indicate that the SVRCE method’s use of ensembles of 
learned n-gram features combined with affect correlation information allows the 
classifi er to assign affect intensities with greater effectiveness than comparison 
approaches used in prior research. 
 4.6.3  H3: Ablation Tests 
 Table  11.10 shows the P values for pairwise t tests conducted to assess the contribu-
tion of the major components of the SVRCE method. The results of SVRCE versus 
SVRE revealed that the use of correlation information signifi cantly enhanced per-
formance on most test beds (signifi cant for three out of four test beds on mean error 
and correlation). The results were not signifi cant for mean error on the LiveJournal 
blog test bed (P value = 0.3452) as well as for correlation on the Middle Eastern 
forum dataset (P value = 0.0013). Both SVRCE and SVRE also signifi cantly outper-
formed SVR, indicating that the use of feature ensembles is effective for classifying 
affect intensities (all P values less than 0.0001, signifi cant at alpha = 0.01). 
 5  Case Study: Al-Firdaws vs. Montada 
 Many prior studies have used brief case studies to illustrate the utility of their proposed 
affect analysis methods (Subasic and Huettner  2001 ; Mishne and Rijke  2006 ) . In order 
to demonstrate the usefulness of the SVRCE method coupled with a rich set of learned 
n-grams, we analyzed the affective intensities in two popular Middle Eastern web 
forums:  www.alfi rdaws.org/vb and  www.montada.com . Analysis of affects in such 
forums is important for sociopolitical reasons and to better our understanding of social 
phenomena in online communities. Al-Firdaws is considered a more extreme forum 
by domain experts, with considerable content dedicated to support the Iraqi insurgency 
and al-Qaeda. In contrast, Montada is a general discussion forum with content and 
discussion pertaining to various social matters. We hypothesized that our SVRCE 
 Table 11.10  P values for pairwise t tests (n = 50) on ablation testing 
 Comparison features 
 Mean % error  Correlation coeffi cient 
 SF  MEF  SF  MEF 
 SVRCE vs. SVRE  <0.0001  <0.0001  <0.0001  0.0013 
 SVRCE vs. SVR  <0.0001  <0.0001  <0.0001  <0.0001 
 SVRE vs. SVR  <0.0001  <0.0001  <0.0001  <0.0001 
2215 Case Study: Al-Firdaws vs. Montada
method would be able to effectively depict the likely intensity differences between 
these two web forums for appropriate affect classes. 
 We used spidering programs to collect the content in both web forums. Table  11.11 
shows summary statistics for the content collected from the two forums. The 
Montada forum was considerably larger, with over 31,000 authors and a large num-
ber of threads and postings, partially because it had been around for approximately 
7 years. Al-Firdaws was a relatively newer forum, beginning in 2005. Due to the 
nature of its content and time duration of existence, this forum had fewer authors 
and postings. 
 Figure  11.3 shows the number of posts for each month the forums have been active. 
Montada was very active in 2002 and 2005, with over 20,000 posts in some months, 
 Table 11.11  Summary statistics for the two web forums collected 
 Forum 
 No. of 
authors 
 No. of 
threads 
 No. of 
messages  No. of sentences  Duration 
 Al-Firdaws  2,189  14,526  39,775  244,917  January 2005–July 2007 
 Montada  31,692  114,965  869,264  2,052,511  September 2000–July 
2007 
Al-Firdaws Posts By Month
0
500
1000
1500
2000
2500
3000
3500
Ja
n-
05
M
ay
-0
5
S
ep
-0
5
Ja
n-
06
M
ay
-0
6
S
ep
-0
6
Ja
n-
07
M
ay
-0
7
# 
p
o
st
s
Montada Posts By Month
0
5000
10000
15000
20000
25000
S
ep
-0
0
M
ay
-0
1
Ja
n-
02
S
ep
-0
2
M
ay
-0
3
Ja
n-
04
S
ep
-0
4
M
ay
-0
5
Ja
n-
06
S
ep
-0
6
M
ay
-0
7
# 
p
o
st
s
 Fig. 11.3  Posting frequency for the two web forums 
 
222 11 Affect Analysis
yet appears to be in a down phase in 2007 (similar to 2004). Al-Firdaws consistently 
had between 2,500 and 3,000 posts per month since the second half of 2006. 
 The SVRCE classifi er was employed in conjunction with the n-gram feature set 
to analyze affect intensities in the two web forums. Analysis was performed on 
violence, hate, racism, and anger affects. We computed the average posting level 
intensities (averaged across all sentences in a posting) as well as the total intensity 
per post (the summation of sentence intensities in each posting). The analysis was 
performed on all postings in each forum (approximately 900,000 postings and 2.3 
million sentences). As shown in Table  11.12 , the Al-Firdaws forum had consider-
ably higher affect intensities for all four affect classes, usually 2–3 times greater 
than Montada. 
 Figure  11.4 depicts the average message violence and hate intensities over time 
for all postings in each of the two web forums. The x-axis indicates time, while the 
y-axis shows the intensities (on a scale of 0–1). Each point represents a single mes-
sage; areas with greater message concentrations are darker. The blank periods in the 
diagrams correspond to periods of posting inactivity in forums (see Fig.  11.3 for 
correspondence). Based on the diagram, we can see that Al-Firdaws has consider-
ably higher violence and also greater hate intensity across time. Al-Firdaws also 
appears to have increasing violence intensity in 2007 (based on the concentration of 
 Table 11.12  Affect intensities per posting across two web forums 
 Intensity  Forum  Violence  Anger  Hate  Racism 
 Average per message  Al-Firdaws  0.084  0.018  0.037  0.032 
 Montada  0.027  0.012  0.010  0.014 
 Total per message  Al-Firdaws  0.523  0.127  0.178  0.191 
 Montada  0.246  0.105  0.092  0.134 
Al-Firdaws- 
Violence 
Montada-
Violence 
Al-Firdaws-
Hate 
Montada-Hate 
 Fig. 11.4  Temporal view of intensities in two web forums 
 
223References
postings), possibly attributable to the increased activity in this forum. In contrast, 
violence and hate intensities are consistently low in Montada. The results generated 
using SVRCE and n-gram features are consistent with existing knowledge regard-
ing these two forums. The case study illustrates how the proposed features and 
techniques can be successfully applied toward affect analysis of computer-mediated 
communication text. 
 6  Conclusions 
 In this chapter, we evaluated various features and techniques for affect analysis of 
online texts. In addition, the support vector regression correlation ensemble 
(SVRCE) was proposed. This method leverages an ensemble of SVR classifi ers 
with each constructed for a separate affect class. The ensemble of predictions 
combined with the correlation between affect classes is leveraged for enhanced 
affect classifi cation performance. Experimental results on test beds derived from 
online forums, blogs, and stories revealed that the proposed method outperformed 
existing affect analysis techniques. The results also suggested that learned n-grams 
can improve affect classifi cation performance in comparison with lexicon-based 
representations. However, combining n-gram and lexicon features did not improve 
performance due to increased amounts of noise and redundancy in the extended 
feature set. A case study was also performed to illustrate how the proposed features 
and techniques can be applied to large cyber communities in order to reveal affective 
tendencies inherent in these communities’ discourse. To the best of our knowledge, 
the experiments conducted in this study are the fi rst to evaluate features and tech-
niques for affect analysis. Furthermore, we are also unaware of prior research 
applied to such a vast array of domains and test beds. 
 We believe this chapter provides an important stepping stone for future work 
intended to further enhance the feature representations and techniques used for 
classifying affects. Based on this work, we have identifi ed several future research 
directions. We intend to apply the techniques across a larger set of affect classes 
(e.g., 10–12 affects per test bed). We are also interested in exploring additional 
feature representations, such as the use of richer learned n-grams (e.g., semantic 
collocations, variable n-gram patterns, etc.). We also plan to evaluate the effective-
ness of real-world knowledge bases such as those employed by Liu et al.  ( 2003 ) . 
 References 
 Abbasi, A., Chen, H., and Salem, A. “Sentiment analysis in multiple languages: Feature selection for 
opinion classifi cation in web forums,”  ACM Trans. on Information Systems , vol. 26(3), 2008. 
224 11 Affect Analysis
 Argamon, S., Whitelaw, C., Chase, P., Hota, S.R., Garg, N., and Levitan, S. “Stylistic text classi-
fi cation using functional lexical features,”  Journal of the American Society for Information 
Science and Technology , vol. 58(6), pp. 802–822, 2007. 
 Chuang, Z. and Wu, C. “Multi-modal emotion recognition from speech and text,”  Computational 
Linguistics and Chinese Language Processing , vol. 9(2), pp. 45–62, 2004. 
 Cherkauer, K.J., “Human expert-level performance on a scientifi c image analysis task by a system 
using combined artifi cial neural networks,” In Chan, P. (Ed.),  Working Notes of the AAAI 
Workshop on Integrating Multiple Learned Models , pp. 15–21, 1996. 
 Cho, Y.H. and Lee, K.J. “Automatic affect recognition using natural language processing 
techniques and manually built affect lexicon,”  IEICE Tran. Information Systems , vol. E89(12), 
pp. 2964–2971, 2006. 
 Dietterich, T. G. “Ensemble methods in machine learning,” In  Proc. of the 1  st   Int’l Workshop on 
Multiple Classifi er Systems , pp. 1–15, 2000. 
 Donath, J., Karahalio, K. and Viegas, F. “Visualizing conversation,” In  Proc. of the 32nd Conf. on 
Computer-Human Interaction , Chicago, USA, 1999. 
 Fellbaum, C. (Ed.).  WordNet: An Electronic Lexical Database , Cambridge, MA. MIT Press, 1998. 
 Grefenstette, G., Qu, Y., Evans, D. A., and Shanahan, J. G. “Validating the coverage of lexical 
resources for affect analysis and automatically classifying new words along semantic axes,” In 
Yan Qu, James Shanahan, and Janyce Wiebe, (Eds.),  Exploring Attitude and Affect in Text: 
Theories and Applications , AAAI-2004 Spring Symposium Series, pp. 71–78, 2004a. 
 Grefenstette, G., Qu, Y., Shanahan, J. G., and Evans, D. A. “Coupling niche browsers and affect 
analysis for an opinion mining application,” In  Proc. of the 12  th   Int’l Conf. Recherche 
d’Information Assistee par Ordinateur , pp. 186–194, 2004b. 
 Hearst, M. A. “Direction-based text interpretation as an information access refi nement,” In 
P. Jacobs (Ed.),  Text-Based Intelligent Systems: Current Research and Practice in Information 
Extraction and Retrieval . Mahwah, NJ: Lawrence Erlbaum Associates, 1992. 
 Jiang, M., Jensen, E., Beitzel, S., and Argamon, S. “Choosing the right bigrams for information 
retrieval”  In Proc. of the Meeting of the Int’l Federation of Classifi cation Societies , 2004. 
 Kim, S. and Hovy, E. “Determining the sentiment of opinions,” In  Proc. of the 20  th   Int’l Conf. on 
Computational Linguistics , pp. 1367–1373, 2004. 
 Liu, H., Lieberman, H., Selker, T. “A model of textual affect sensing using real-world knowledge.” 
In  Proc. of the 8th Int’l Conf .  on Intelligent User Interfaces , Miami, Fl., 2003. 
 Ma, C., Prendinger, H., and Ishizuka, M. “Emotion estimation and reasoning based on affective 
textual interaction,” In  Proc. of the First Int’l Conf. on Affective Computing and Intelligent 
Interaction , pp. 622–628, 2005. 
 Mishne, G. “Experiments with mood classifi cation,” In  Proc. of Stylistic Analysis of Text for 
Information Access Workshop , 2005. 
 Mishne, G. and Rijke, M. de. “Capturing global mood levels using blog posts,” In  Proc. of the 
AAAI 2006 Spring Symposium on Computational Approaches to Analysing Weblogs , 2006. 
 Nigam, K. and Hurst, M. “Towards a robust metric of opinion,” In  Proc. of the AAAI Spring 
Symposium on Exploring Attitude and Affect in Text , 2004. 
 Pang, B., Lee, L., and Vaithyanathain, S. “Thumbs up? Sentiment classifi cation using machine 
learning techniques,” In  Proc. of the Empirical Methods in Natural Language Processing , 
pp. 79–86, 2002. 
 Pang, B. and Lee, L. “Seeing stars: Exploiting class relationships for sentiment categorization with 
respect to rating scales,” In  Proc. of the Annual Meeting on Association for Computational 
Linguistics , pp. 115–124, 2005. 
 Picard, R.W.  Affective Computing. Cambridge, MA. MIT Press, 1997. 
 Picard, R.W., Vyzas, E., and Healey, J. “Toward machine emotional intelligence: analysis of affec-
tive physiological state,”  IEEE Tran. Pattern Analysis and Machine Intelligence , vol. 23(10), 
pp. 1179–1191, 2001. 
 Read, J. “Recognizing affect in text using point-wise mutual information,”  Master’s Thesis , 2004. 
225References
 Schumaker, R. and Chen, H. “Textual analysis of stock market prediction using fi nancial news arti-
cles,”  Americas Conference on Information Systems (AMCIS-2006) , Acapulco, Mexico, 2006. 
 Stamatatos, E. and Widmer, G. “Music performer recognition using an ensemble of simple classi-
fi ers,” In  Proc. of the 15  th   European Conf. on Artifi cial Intelligence , Lyon France, 2002. 
 Subasic, P. and Huettner, A. “Affect analysis of text using fuzzy semantic typing,”  IEEE Tran. 
Fuzzy Systems , vol. 9(4), pp. 483–496, 2001. 
 Turney, P.D. and Littman, M.L. “Measuring praise and criticism: inference of semantic orientation 
from association,”  ACM Trans. Information Systems , vol. 21(4), pp. 315–346, 2003. 
 Valitutti, A., Strapparava, C., and Stock, O. “Developing affective lexical resources,”  PsychNology 
Journal , vol. 2(1), pp. 61–83, 2004. 
 Wiebe, J. “Tracking point of view in narrative,”  Computational Linguistics , vol. 20(2), pp. 233–287, 
1994. 
 Wiebe, J., Wilson, T., Bruce, R., Bell, M., and Martin, M. “Learning subjective language,” 
 Computational Linguistics , vol. 30(3), pp. 277–308, 2004. 
 Witten, H. and Frank, E.  Data mining: Practical machine learning tools and techniques , Morgan 
Kauffman, 2 nd Edition, 2005. 
 Wu, C., Chuang, Z., and Lin, Y. “Emotion recognition from text using semantic labels and 
 separable mixture models,”  ACM Trans. Asian Language Information Processing , vol. 5(2), 
pp. 165–182, 2006. 
 Yang, Y. and Pederson, J.O. “A comparative study on feature selection in text categorization,” 
In  Proc. of the 14  th   Int’l Conference on Machine Learning , pp. 412–420, 1997. 
