EDIC RESEARCH PROPOSAL 1
Extraction and Classification of Handwritten
Annotations for Pedagogical Use
Andrea Mazzei
CRAFT, I&C, EPFL
Abstract—This research proposal first describes a method for
extracting and classifying handwritten annotations on printed
documents using a simple camera integrated in a lamp or a
mobile phone. The ambition of such a research is to offer a
seamless integration of notes taken on printed paper in our daily
interactions with digital documents. Existing studies propose a
classification of annotations based on their form and function. We
demonstrate a method for automating such a classification and
report experimental results showing the classification accuracy.
We also present the design of a preliminary user study to
investigate the effects of annotation types on the cognitive
processes involved in rereading documents.
Index Terms—machine-printed and handwritten text separa-
tion, document processing, annotation classification
I. INTRODUCTION
DESPITE the numerous digital reincarnations of the tradi-tional printed book, paper remains the preferred medium
for reading. A consistent body of literature comparing reading
activities on paper and online documents has been summarized
by O’Hara et al. [1]. Paper documents offer better legibility
and better orientation and location. Physical tangibility facil-
itates handwriting and concurrent reading on multiple pages.
Proposal submitted to committee: August 18th, 2010; Can-
didacy exam date: August 26th, 2010; Candidacy exam com-
mittee: Exam president, thesis director, co-examiner.
This research plan has been approved:
Date: ————————————
Doctoral candidate: ————————————
(name and signature)
Thesis director: ————————————
(name and signature)
Thesis co-director: ————————————
(if applicable) (name and signature)
Doct. prog. director:————————————
(R. Urbanke) (signature)
EDIC-ru/05.05.2009
Instant shareability allows for more intuitive cooperative in-
teractions.
In addition annotating on paper has many well known advan-
tages compared to any digital equivalent [2]. Readers often
write comments in the margins of papers, underline important
passages and use other various marking strategies. These
practices help them to understand better what they read and,
at a later stage, find back easier relevant passages. It also
plays an important role for associative thinking and linking the
content with other ideas and documents. Therefore, annotating
means establishing a reciprocity between the raw text and
the reader understanding. This article describes a method for
extracting and classifying handwritten annotations on printed
documents using a simple camera integrated in a lamp or a
mobile phone. The ambition of such a research is to offer
a seamless integration of notes taken on printed paper in our
daily interactions with digital documents, without losing any of
the advantages that paper offers. Handwritten annotations have
different forms and functions (Marshall [3]). We highlight or
underline words as attentional landmarks. We write short notes
within the margins or between lines of texts as interpretation
cues. We use longer notes in blank spaces or near figures to
elaborate with complementary information. The system, de-
scribed in this article, aims at not only extracting handwritten
annotations but also classifying them in one of these three
categories based on their spatial and colourimetric properties.
This automatically generated classification could then be serve
to improve the comprehension while rereading the document
and used to sort, organize and share annotations for instance
in the context of collaborative reading applications.
The first part of the article reviews a number of approaches
that have been studied in the last 20 years to tackle this
issue. The second part presents our own contribution as
original combination of a technique for extracting annotations,
a clustering algorithm and a classification approach. To the
best of our knowledge the method herein described has not
been applied to this problem beforehand. We report the results
of a preliminary study showing that handwritten annotations
can be extracted and classified in a satisfactory manner using
this technique. In the last part of the article we provide a road
map for conducting user-centered studies using eye-tracking
systems. The herein discussed annotation classification will
serve as control element to better investigate around impli-
cations among annotation types, re-reading eye-gaze patterns
and text comprehension.
EDIC RESEARCH PROPOSAL 2
II. MACHINE-PRINTED AND HANDWRITTEN TEXT
CLASSIFICATION: A SHORT REVIEW
Discriminating machine-printed and handwritten text in tex-
tual images is a problem that has been intensely investigated in
the last two decades. In 1990 Umeda and Kasuya [4] described
their discriminator of English characters. Their patented in-
vention is based on the strong assumption of uniformity of
each block. The discrimination is performed by calculating
the ratio between the number of slanted strokes and the sum
of horizontal, vertical and slanted ones and by imposing a
predetermined static threshold. Under these conditions they
achieved a recognition rate of 95%.
Few years later two works focused on the classification at
character level. Kunuke et al. [5] proposed a classification
methodology based on the extraction of scale and rotation
invariant features: the straightness of vertical and horizontal
lines and the symmetry relative to the centre of gravity of the
character. Their results showed a recognition rate of 96.8% on
a training set of 3632 and 78.5% on a test set of 1068 images;
Fan et al. [6] used instead the character block layout variance.
They reported a correctness rate above 85% tested on English
and Japanese textual images: 25 images containing machine
printed text and 25 containing handwritten ones. In 2000 Pal
et al. [7] presented their method for Bangla and Devnagari;
it relies on the analysis of some structural regularities of
the alphabetic characters of these languages. Their method
uses a hierarchy of three different features to perform the
discrimination. The head line is the predominant feature, in
fact it forms a peak in the horizontal projection profile of
machine-printed text. Their recognition rate is attested on
98.6%. Guo et al. [8] suggested a method based on a hidden
Markov model to classify typewritten and handwritten words
based on vertical projection profiles of the word. They tested
the algorithm on a test-set of 187 words, reaching a precision
rate of 92.86% for the typewritten words and 72.19% for the
handwritten ones.
More recently Zheng et al. [9] reported a work on a robust
printed and handwritten text segmentation from extremely
noisy document images. They used different classifiers such
as k-nearest neighbours, support vector machine (SVM) and
Fischer and different features such as pixel density, aspect ratio
and Gabor filter. They achieved a segmentation accuracy of
78%. In the meanwhile Jang et al. [10] described an approach,
specific for Korean text, based on the extraction of geometric
features. They employed a multilayer perceptron classifier
reaching an accuracy rate of 98.9% on a test-set of 3,147
images. On the other hand Kavallieratou [11] showed that a
simple discriminant analysis on the vertical projection profiles
performs comparably to many robust approaches.
One interesting application is the detection and matching of
signatures proposed by Zhu et al. [12], a robust multilingual
approach, in an unconstrained setting of translation, scale,
and rotation invariant nonrigid shape matching. Peng et al.
[13] suggested a novel approach based on three categories of
word level feature and a k-means classifier associated with a
relabelling post-procedure using Markov random field models;
they achieved an overall recall of 96.33%. And finally in a
Fig. 1. Processing pipeline
more general scenario of sparse data and arbitrary rotation
Chanda et al. [14] recently described their approach based on
the SVM classifier and obtaining an accuracy of 96.9% on a
set of 3958 images.
III. METHOD
We here present our assemblage for extracting and classi-
fying handwritten annotations on machine printed documents.
Figure 1 provides an overview of the processing pipeline. It
consists of four steps. The first step takes in input the image
containing the already extracted annotations and proceeds by
clustering the pixels. Parallely the retrieved digital source of
the document is processed in order to acquire an accurate
estimation of the bounding boxes around the main text blocks
present in the image. The set of classified annotations and
the estimated bounding box are given in input to a decision
tree classifier. A final step evaluates the classification accuracy
by comparing the average colour of each annotation with the
predetermined ones. We now describe a novel approach to
separate handwritten annotations from machine-printed text,
introduced by Nakai et al. [15]. This method is strongly based
on a previous report by the same authors focusing on the
camera-based document image retrieval using Locally Likely
Arrangement Hashing (LLAH). [16].
A. Annotation Extraction using alignment based on LLAH
Nakai et al. [15] recently realized a method able to extract
colour annotations from colour documents. Their method is
based on two tasks: fast matching of document images based
on local arrangement of features points and flexible back-
ground subtraction resistant to moderate misalignment. This
method is more general than the above-mentioned ones, since
it deals with any kind of annotation and printed document.
Later improvements by the same authors [17] showed a an
accuracy rate of 85.59%. These considerations encouraged us
to adopt their method. We briefly describe the salient steps of
their method.
EDIC RESEARCH PROPOSAL 3
1) Feature points extraction: In textual images word cen-
troids constitute powerful feature points because of their
spatial predominancy and their presumable colour contrast
with the background, on the other hand the presence of anno-
tations constitutes a disturbing factor for the matching process.
Choosing the word centroid as feature point gives robustness
to uneven illumination, low resolution and distortion. The
preprocessing task to extract features points consists of several
steps:
• colour k-means clustering: image is decomposed in k
grayscale images
• adaptive thresholding of each grayscale image
• extraction of the connected components for each bina-
rized image: the majority of the extracted blobs corre-
spond to typographic characters
• gaussian blur of the binarized images: the parameter is
proportional to the mode of the minimum bounding box
size of the connected components
• adaptive thresholding of the blurred images
• re-extraction of the connected components: the majority
of the re-extracted blobs correspond to word regions
2) Calculation of features: Their stable and discriminative
feature is computed from different feature points present in
a broad local area around the word. The authors use the
affine invariant which for small areas, relative to the viewing
distance, well approximates the cross-ratio, behaving as per-
spective invariant. It is computed on four different coplanar
points p0, p1, p2, p3 as follows:
A(p0, p2, p3)
A(p0, p1, p2)
(1)
where A(pi, pj , pk) is the area of the triangle pi,pi,pk.
We here report the original algorithm for computing
the index to register and retrieve the feature points in the
database.
1. for each p ∈ {All feature points} do
2. Pn is the set of nearest n points of p (sorted clockwise)
3. for each Pm ∈ {Combinations of m points from Pn} do
4. for each Pf ∈ {Combinations of f points from Pm} do
5. r(i) is the invariant calculated with Pf
6. end for
7. Hindex is the hash index computed on the Pf invariants
8. end for
9. end for
Each feature point is first registered in the database using
the above mentioned Hindex which is computed as follows:
Hindex = (
mCf−1∑
i=0
r(i) ∗ ki) mod Hsize (2)
where r(i) is the discrete value of the invariant, k is the level of
quantization of the invariant, and Hsize is the size of the hash
table. In the retrieval process the Hindex is used to look up the
hash table so that each feature point of the query image will
be matched with feature points in the database. The matching
is resolved with a voting system where each matched feature
point vote for a specific document page.
3) Alignment and subtraction: The alignment process is
resolved by estimating the parameters of similarity transforma-
tion employing the RANSAC algorithm, which well deals with
the presence of outliers. Once produced the aligned image.
The annotated image is subtracted by the retrieved original
one. Each pixel value is subtracted with the one that has the
closest colourimetric value and is present in a small predefined
square area around it. This counterbalance some moderate
misalignment introduced in the previous steps of the methods.
B. Annotation Segmentation using DBSCAN
This module is responsible for grouping the colour pixels
constituting the image containing the extracted annotations. To
address this issue we decided to adopt the well known clus-
tering algorithm DBSCAN (Density-Based Spatial Clustering
of Application with Noise) for the following reasons:
• the pixels forming an annotation are subject to the con-
ditions of spatial adjacency and colourimetric proximity
• the number of clusters is not known a priori: the number
of annotations contained in a page is not predictable
• position, orientation, size and colour of an annotation are
variable
• the algorithm should not have a bias toward a particular
cluster shape and it should handle noise: the form of
an annotation can vary from the rectangular highlighted
region to the arbitrary handwritten mark
• the algorithm should distinguish adjacent or even self
containing clusters: for instance the highlighted com-
ments
Wu et al. recently reported significant improvements of the
original DBSCAN algorithm in terms of time complexity
[18]; they removed the original inadequacy in dealing with
large-scale data. This allows us not to be bound up with low
resolution images.
The input image containing the pre-extracted annotations is
reprocessed. Each pixel is specified by 5 components:
pi = (xi, yi, ri, gi, bi) (3)
the local position xi and yi, used as indexing terms, and
the colour information ri, gi and bi, which yields additional
discriminative power. The output is obtained by partitioning
this set of n pixels into a set of k clusters:
A = (A1, A2, ..., Ak) (4)
Each cluster corresponds to a correctly segmented annotation.
The centroid contains the position of the centre of mass and
the mean colour of the annotation. The algorithm is initialized
by setting two radiuses, pos for the spatial domain and rgb
for the colourimetric one and a minimum density MinPts to
discriminate all the pixels in core, density reachable and noise
points.
C. Decision Tree Classification
A classification of different forms of annotation is analyzed
by Marshall [3]. We here report a brief summary of some
of the major findings and conclusions of the study that she
EDIC RESEARCH PROPOSAL 4
Fig. 2. Decision tree classification
conducted. She observed correlations between the form of
an annotation and the relative function. Different types of
annotations are used for different purposes.
• underlining, highlighting and using symbolic cues on
titles and section headings, play the role of future atten-
tional landmark, inviting the student to reread the relative
passage
• underlining, highlighting and circling words and phrases
constitute an aid or an invitation to remember the content
of interest
• notes in margins or near figures or equations bring
elaborated information
• notes in between the text lines seem to function as
interpretive reference
• excessive highlighting and underlining reflect scarce un-
derstanding due to an intrinsic difficulties
• marks not related to the text constitute index of distraction
due to local circumstances
We regroup these marking strategies by functionality: memory
recall for underlined or highlighted elements, interpretation
cues for symbols and short notes in between the lines or over
the text, contents elaboration for notes in margins or other
blank spaces.
We use a decision-tree-based classifier to map the clustered
annotations into these categories. Figure 2 illustrates the
structure of the decision tree and defines the annotation types
in the leaf nodes. In the first level all the annotations are
discriminated according to their local position on the page:
annotations in between the lines or over text and annotations
in the margins or other blank spaces. In the second level all
the annotations are separated according to their rectangularity;
some methods to compute this derived feature are proposed by
Rosin [19]; these methods have desirable properties for our
scenario such as rotation invariance and robustness to noise.
The rectangularity is calculated using the minimum bound-
ing rectangle (MBR). More precisely the MBR can be calcu-
lated on the elliptical approximation of the shape of interest.
Each value of rectangularity is then thresholded to separate
more compact annotations such as highlighted areas from
others with more complex boundaries such as notes and
symbols. Figure 3 shows a satisfactory classification result. In
this figure the red, green and blue ellipses contain the notes
between the lines or over the text, highlighted passages and
notes in the blank spaces respectively.
Fig. 3. Annotation classification output
IV. EXPERIMENTAL RESULTS
We have collected 33 annotated pages of scientific articles
containing a total of 571 annotations produced by a culturally
heterogeneous group of Master and PhD students. They pro-
duced the annotations in their own native languages and using
their personal style. We set only one constraint: we asked them
to use the same colours for each type of annotation within
one page. This constraint is imposed only to automatically
and objectively evaluate the accuracy of our approach. For
each page we supervised the last step of the pipeline (Figure
1) indicating the corresponding function of each colour used
for annotating. The experimental results show a classification
accuracy of 84.47%.
A. Strengths and Weaknesses
We here report the observed strengths and weaknesses.
The adopted method for extracting annotations from printed
documents and the ones discussed in the literature review
introduce noise in the separation. DBSCAN effectively iden-
tifies and handles these noise pixels. We now report some
relevant cases of correct and robust classification and cases
of failures. Figure 4(a) shows a difficult scenario in which
our approach correctly classifies the annotations. An interline
comment is between two highlighted words: in this specific
case the spatial information is not discriminative enough to
distinguish them: the colour information is determinant to
perform the separation. Another strength is that our approach
does not depend on a specific language. Figure 4(b) shows
a case of correct classification of a note written in Iranian.
Figure 4(c) shows a case of correct clusterization but incorrect
classification. The big red ellipse contains a chain of bordering
EDIC RESEARCH PROPOSAL 5
(a) (b) (c) (d) (e)
Fig. 4. Cases of robust, poor and wrong classification
highlighted regions. This region is clustered as a set of
homogeneous annotations but wrongly classified as interline
note because of a wrong value of rectangularity. Figure 4(d)
shows a case of self contained annotations. In this case the
red ink diffuses into the highlighter ink creating a colour
transition between them. This leads to a rough clusterization
result. Lastly our approach is not well-suited to capture the
notion of linking as shown in Figure 4(e).
V. EYE TRACKING IN RE-READING TASKS
Eye tracking systems have been frequently employed to
study eye movements in reading [20]. In this section we
present the design of a preliminary user study to investigate the
cognitive effects of reading annotated documents. We intend to
analyze implications between annotated documents, eye-gaze
patterns and text comprehension in re-reading activities. The
annotation classification proposed in the previous section will
be used to investigate possible interactions.
A. Eye-gaze data collection
The eye-gaze data will be gathered by an eye-tracking
system. The choice between different eye tracking systems
is based on considerations about the precision required for
capturing the eye-gaze data. More precisely the spatial pre-
cision of the system and temporal resolution of the camera
should be high enough to capture reading episodes. Another
consideration goes towards maintaining the natural interaction
between the reader and the book which would encourage the
adoption of wearable eye tracking systems.
B. Participants and instructional material
A corpus of 20 subjects will take part in our experiment.
The instructional material used in this experiment will be
created ad hoc. The candidates whose background knowledge
is presumably close to the used text will be discarded. Possible
effects of the prior knowledge on the comprehension of the
text [21] must be avoided.
C. Tasks
The experiment starts with a first reading session and
proceeds with a second re-reading one.
1) First reading session: Before starting the first reading
of the text, the participants’ prior knowledge of the instruc-
tional material will be assessed with a post-test. Then each
participant will be asked to carefully study the text, making
her personal annotations. She will be alone and will have a
limited amount of time. At the end of the first reading session
a post-test will evaluate her gained knowledge.
2) Re-reading session: The participants will take their
second part of the study one or two weeks later. A pre-test will
assess their recall performance. In this second task they will
reread the same document but in different text conditions. At
the end of the second task the participants will do a last post-
test and they will be paid proportionally to the score achieved.
D. Independent Variables
The independent variable text version determines three
different experimental conditions:
• not annotated: the participants will reread the annotation-
free version of the text
• self annotated: the participants will reread the document
containing their personal annotations
• socially annotated: the participants will reread a version
of the document previously annotated by an expert or the
other participants
E. Dependent Variables
We will measure the Relative Learning Gain (RLG) at the
end of each reading session. It is calculated as normalized
difference between the post-test and pre-test score. It will be
computed for each participant and for each portion of text
involved in a question.
F. Process Variables
There are other variables that we would like to measure
in order to better explain significant implications between
independent and dependent variables.
1) Eye-Gaze Data: Rayner [20] gives a very detailed
overview on the eye-gaze features describing the eye move-
ments in reading. Fixations and saccades are strongly depen-
dent on the text pattern at the word level, in terms of length and
morphology. Interestingly, some of the words are skipped and
some re-fixated. We intend to use these features as explanatory
variable of the RLG.
2) Annotation Type: The types of annotations above dis-
cussed might constitute a meaningful explanatory variable of
the eye-gaze patterns and the RLG.
3) User Distraction: The variance of the 3D position of the
user head might represent an index of distraction that should
be taken into account.
G. Comparative Analysis
We expect that there will be significant correlations between
the experiment conditions varied during the re-reading sessions
and the text comprehension evaluated through the RLG. We
EDIC RESEARCH PROPOSAL 6
also expect to find intermediary variables explaining such im-
plications. We courageously predict some general hypotheses:
(H1): text version has a significant influence on the eye-
gaze re-reading pattern
(H2): the RLG is affected by the text version
We also hope to explain such implications with the above
mentioned process variables:
(H3): the RLG is explained by the re-reading pattern
(H4): the different annotation types influence the eye-gaze
re-reading pattern in different ways
(H5): the different annotation types influence the RLG
pattern in different ways
(H6): the user distraction explains modest RLG
VI. APPLICATIONS
Despite being still far from any preliminary application
design phase we want to briefly describe NiCEBook [22], a
similar prototype that has inspired some of the pedagogical
uses for our system.
A. NiCEBook - Supporting Natural Note Taking
NiCEBook is a system that brings digital capabilities to
paper note-taking. The authors intended to preserve the natural
note taking activity on paper documents basing their prototype
on the well known Anoto technology. Such system robustly
bridges notes, taken on paper, with their digital counterpart.
Their system allows for tagging the notes taken on paper.
The user can explicity assign the specific function to the
relative note by simply ticking the corresponding checkbox,
present at the bottom of each page. In addition, the user can
classify the notes within an arbitrary topic, manually writing it
in a predefined text area. The system also captures and uses the
notion of dog-ear. One advantage of NiCEBook relies on the
mobile usability: it permits users to work everywhere in offline
mode and later transfer the notes to the application. On the
digital side the system offers browsing functionalities based on
the page, category, topic and dog-ear. OCR processing of the
imported notes provides querying and indexing capabilities.
A flexible interface allows to easely transfer notes to other
applications and other users.
B. When annotating becomes a social practice
Teamwork is a skill that is often taught and encouraged
in universities. Students typically work together in small
informal, sometimes formal, groups in order to solve excer-
cises, to discuss the course material and to prepare exams.
Team working involves active participation, interaction and
frequent sharing of ideas and annotated instructional material.
Sometimes team members get the opportunity to physically
meet with each other, sometimes they prefer to work remotely.
In the latter scenario the communication usually takes the form
of a chat, rarely of a voice chat. In such conditions sharing
material is extremely limited by the media-poorness of the
communication channel and consequently by a limited coordi-
nation among the students. We intend to realize a groupware
to support communication and collaboration centred on the
annotated documents. We believe that a granular classification
of the annotation functionalities would better support sharing
and collaboration in these small groups. Such system would
also allow to better investigate salient differences between
social and private annotations.
VII. CONCLUSION
In the first part of this paper we describe a system for
clustering and classifying handwritten annotations, extracted
using already existing techniques, achieving the accuracy rate
of 84.47%. Although there is still room for improvements
using this approach, the results are promising enough to extend
the investigation to a more accurate and granular classification.
In the second part we focus on applications, end-user studies,
and definition of guidelines for future progress.
REFERENCES
[1] K. O’Hara and A. Sellen, “A comparison of reading paper and on-
line documents,” in CHI ’97: Proceedings of the SIGCHI conference on
Human factors in computing systems. New York, NY, USA: ACM,
1997, pp. 335–342.
[2] R. Kawase, E. Herder, and W. Nejdl, “A comparison of paper-based and
online annotations in the workplace,” in EC-TEL ’09: Proceedings of the
4th European Conference on Technology Enhanced Learning. Springer-
Verlag, 2009, pp. 240–253.
[3] C. C. Marshall, “Annotation: from paper books to the digital library,”
in DL ’97: Proceedings of the second ACM international conference on
Digital libraries. ACM, 1997, pp. 131–140.
[4] T. Umeda and S. Kasuya, “Discriminator between handwritten
and machine-printed characters,” 1990. [Online]. Available: http:
//www.freepatentsonline.com/4910787.html
[5] K. Kuhnke, L. Simoncini, and Z. M. Kovacs-V, “A system for machine-
written and hand-written character distinction,” in ICDAR ’95: Pro-
ceedings of the Third International Conference on Document Analysis
and Recognition (Volume 2). Washington, DC, USA: IEEE Computer
Society, 1995, p. 811.
[6] K.-C. Fan, L.-S. Wang, and Y.-T. Tu, “Classification of machine-
printed and handwritten texts using character block layout
variance,” Pattern Recognition, vol. 31, no. 9, pp. 1275 – 1284,
1998. [Online]. Available: http://www.sciencedirect.com/science/article/
B6V14-3W83V7J-6/2/3a2b4ca689c96e5f2cb939b8e0e1a035
[7] U. Pal and B. B. Chaudhuri, “Automatic separation of machine-printed
and hand-written text lines,” Document Analysis and Recognition, Inter-
national Conference on, vol. 0, p. 645, 1999.
[8] J. K. Guo and M. Y. Ma, “Separating handwritten material from
machine printed text using hidden markov models,” in ICDAR ’01:
Proceedings of the Sixth International Conference on Document Analysis
and Recognition. IEEE Computer Society, 2001, p. 439.
[9] Y. Zheng, S. Member, H. Li, and D. Doermann, “Machine printed text
and handwriting identification in noisy document images,” IEEE Trans.
Pattern Analysis Machine Intelligence, vol. 26, p. 2003, 2004.
[10] S. I. Jang, S. H. Jeong, and Y.-S. Nam, “Classification of machine-
printed and handwritten addresses on korean mail piece images using
geometric features,” in ICPR ’04: Proceedings of the Pattern Recog-
nition, 17th International Conference on (ICPR’04) Volume 2. IEEE
Computer Society, 2004, pp. 383–386.
[11] E. Kavallieratou, S. Stamatatos, and H. Antonopoulou, “Machine-printed
from handwritten text discrimination,” Frontiers in Handwriting Recog-
nition, International Workshop on, vol. 0, pp. 312–316, 2004.
[12] G. Zhu, Y. Zheng, D. Doermann, and S. Jaeger, “Signature detection and
matching for document image retrieval,” IEEE Transactions on Pattern
Analysis and Machine Intelligence, vol. 31, pp. 2015–2031, 2009.
[13] X. Peng, S. Setlur, V. Govindaraju, R. Sitaram, and K. Bhuvanagiri,
“Markov random field based text identification from annotated machine
printed documents,” in ICDAR ’09: Proceedings of the 2009 10th
International Conference on Document Analysis and Recognition. IEEE
Computer Society, 2009, pp. 431–435.
[14] S. Chanda, K. Franke, and U. Pal, “Structural handwritten and machine
print classification for sparse content and arbitrary oriented document
fragments,” in SAC ’10: Proceedings of the 2010 ACM Symposium on
Applied Computing. ACM, 2010, pp. 18–22.
EDIC RESEARCH PROPOSAL 7
[15] T. Nakai, K. Kise, and M. Iwamura, “A method of annotation extraction
from paper documents using alignment based on local arrangements of
feature points,” in Document Analysis and Recognition, 2007. ICDAR
2007. Ninth International Conference on, vol. 1, 2007, pp. 23–27.
[16] ——, “Use of affine invariants in locally likely arrangement hashing for
camera-based document image retrieval,” in Document Analysis Systems,
2006, pp. 541–552.
[17] T. Nakai, K. Iwata, and K. Kise, “Accuracy improvement and objective
evaluation of annotation extraction from printed documents,” in Docu-
ment Analysis Systems, 2008. DAS ’08. The Eighth IAPR International
Workshop on, 2008, pp. 329–336.
[18] Y.-P. Wu, J.-J. Guo, and X.-J. Zhang, “A linear dbscan algorithm based
on lsh,” in Machine Learning and Cybernetics, 2007 International
Conference on, vol. 5, 2007, pp. 2608 –2614.
[19] P. L. Rosin, “Measuring shape: ellipticity, rectangularity, and triangular-
ity,” Mach. Vision Appl., vol. 14, no. 3, pp. 172–184, 2003.
[20] K. Rayner, “Eye movements in reading and information processing:
20 years of research.” Psychological bulletin, vol. 124, no. 3, pp.
372–422, November 1998. [Online]. Available: http://view.ncbi.nlm.nih.
gov/pubmed/9849112
[21] B. Anda, K. Hansen, and G. Sand, “An investigation of use case
quality in a large safety-critical software development project,” Inf.
Softw. Technol., vol. 51, no. 12, pp. 1699–1711, 2009.
[22] P. Brandl, C. Richter, and M. Haller, “Nicebook: supporting natural note
taking,” in CHI ’10: Proceedings of the 28th international conference
on Human factors in computing systems. New York, NY, USA: ACM,
2010, pp. 599–608.
