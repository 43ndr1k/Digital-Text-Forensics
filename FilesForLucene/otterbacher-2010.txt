Inferring Gender of Movie Reviewers:  
Exploiting Writing Style, Content and Metadata 
Jahna Otterbacher 
Lewis Department of Humanities 
Illinois Institute of Technology 
3301 S. Dearborn 
Chicago, IL 60616 USA 
jahna.otterbacher@iit.edu 
 
 
 
 
ABSTRACT 
Despite differences in the way that men and women experience 
goods and communicate their perspectives, online review 
communities typically do not provide participants’ gender.  We 
propose to infer author gender, given a set of reviews of a 
particular item, and experiment on reviews posted at the Internet 
Movie Database (IMDb).  Using logistic regression, we explore 
the contribution of three types of information: 1) style, 2) content, 
and 3) metadata (e.g. review age, social feedback).  Our results 
concur with previous research, in that there are salient differences 
in writing style and content between reviews authored by men 
versus women.  However, in comparison to literary or scientific 
texts, to which classification tasks are often applied, reviews are 
brief and occur within the context of an ongoing discourse. 
Therefore, to compensative for the brevity of reviews, content and 
stylistic features can be augmented with metadata.  We find in 
particular that the perceived utility of a review is an important 
correlate of gender.  The model incorporating all features has a 
classification accuracy of 73.7% and is not as sensitive to review 
length as are those based only on stylistic or content features. 
Categories and Subject Descriptors 
H.3.3 [Information Search and Retrieval]: Information filtering – 
author attributes, gender; H.3.1 [Content Analysis and Indexing]: 
Linguistic processing. 
General Terms 
Algorithms, Experimentation, Human Factors 
Keywords 
Text classification, gender, filtering, online community 
1. INTRODUCTION 
Review communities are large, multi-user information spaces in 
which participants exchange information via textual postings 
describing their experiences with and opinions of various items.  
In order to reduce information overload, which often threatens 
systems in which users encounter unstructured text, communities 
must adopt measures to help guide users to content of interest 
[13].  Typically, reviews are filtered by salient properties.  For 
example, filters at popular communities include perceived utility 
(i.e. collaborative filtering [10]; “best” or “most helpful”), 
chronological ordering, and by reviewer’s rating of the item (i.e. 
“most favorable/critical”). 
One potentially useful attribute for filtering reviews is gender.  
However, in many communities, it is difficult to determine the 
gender of reviewers.  In online environments, many participants 
intentionally maintain a gender-neutral identity, as to avoid 
attracting undesirable attention and to be taken more seriously [5].  
More generally, many have privacy concerns [1, 36] and choose 
not to disclose personal information.  However, since 
personalization is a key means of guiding users to information 
likely to be of interest, particularly on the Web, researchers have 
considered how to automatically infer demographic 
characteristics, including gender (e.g. [16, 30]). 
We examine the Internet Movie Database (IMDb.com), where a 
portion of reviews has reviewer gender information.  At many 
sites, including stores (e.g. Amazon.com) and those focused on 
movies (e.g. moviereview.com), gender is not provided.  To 
contrast, IMDb has a filter allowing users to identify reviews 
written by men and women, in order to get a gender-balanced 
perspective on movies.  IMDb’s default display is its 
collaborative filtering mechanism (i.e. reviews are sorted by what 
others found “useful”); by selecting the “male/female” filter, a 
user is presented with an equal number of reviews written by men 
and women, interleaved.  The number of reviews shown for each 
gender corresponds to the size of the smaller gender class. 
Figure 1 shows a review at the forum for Casablanca.  The 
male/female filter has been selected, and it can be seen that a man 
wrote this particular review.  As shown, there are 142 reviews 
(out of 753 in total), for which author gender is available.  
Finally, above each review, the perceived utility is displayed in 
the form “x of y people found the following review useful1.”  
Feedback is solicited following each review, where participants 
are asked if they found it useful or not.  This feedback is used to 
determine the presentation order of reviews under the default 
filter, “Best.” 
                                                                 
1 Following [40], we define utility as the number of users who 
found a review useful divided by the total votes (i.e. x/y). 
 
Permission to make digital or hard copies of all or part of this work for 
personal or classroom use is granted without fee provided that copies are 
not made or distributed for profit or commercial advantage and that 
copies bear this notice and the full citation on the first page. To copy 
otherwise, or republish, to post on servers or to redistribute to lists, 
requires prior specific permission and/or a fee. 
CIKM’10, October 26–30, 2010, Toronto, Ontario, Canada. 
Copyright 2010 ACM  978-1-4503-0099-5/10/10...$10.00. 
 
369
Figure 1. IMDb review forum for Casablanca. 
1.1 Differences in experience and expression 
There are several reasons why it would be beneficial for reviewers’ 
genders to be known.  First, men and women often experience 
goods differently.  This is particularly true of hedonic goods, such 
as books or movies, the consumption of which resembles a personal 
experience as compared to the consumption of utilitarian goods (e.g. 
pencils) [14].  Factors such as a consumer’s age and gender are 
known to correlate to her preferences for hedonic goods [15].  For 
instance, while perhaps stereotypical, certain genres of movies (e.g. 
“tearjerkers”) tend to appeal more to women than to men [29]; thus 
it is natural to expect such preferences to be reflected in reviews.  
Therefore, when reading reviews about an experience-oriented 
good, it is likely that users will want to know what others like 
themselves think about it.  Since demographic features are 
correlated to participants’ attitudes and values, people use them to 
judge whether or not someone is similar to them [38]. 
Men and women also have unique ways of expressing themselves.  
Lakoff’s [22] classic work on gender differences in language 
proposed 10 salient characteristics of “women’s language.”  For 
instance, she claimed that women and men use different lexical 
items (e.g. women describe colors more vividly than men) and that 
women are more likely to use hedges (e.g. “kind of,” “sort of”) in 
order to soften an argument, making it more acceptable to others.  
Similarly, Tannen [37] theorized that women use “rapport-talk,” in 
establishing a level of intimacy with interlocutors while men 
typically adopt a style described as “report-talk,” in which they 
focus on conveying information. 
Herring [11, 12] found that many differences noted in 
conversational speech also apply in computer-mediated settings.  
For example, in online forums, men tend to dominate the 
interactions.  To contrast, female participants typically post fewer 
messages, and are less likely to receive responses from others.  
Gefen and Ridings [9] explained that communication differences in 
communities have to do with how men and women use these 
information spaces.  While male participants value the chance to 
showcase their knowledge of a subject, women often seek or offer 
social support for problem solving. 
1.2 Balanced perspectives in online reviews 
One way people use review communities is to learn what others 
think about an item before investing time and money in consuming 
it [34].  Therefore, a question of interest is whether or not this 
“wisdom of the crowds” is biased, and if so, how (e.g. [20]).  
Likewise, we can pose the question of whether users are likely to 
get a gender-balanced perspective. 
As will be detailed in Section 3, the data we analyze is a collection 
of over 30,000 reviews of 250 movies.  Since IMDb’s default 
display is its social filtering mechanism, we ordered the reviews for 
each movie by perceived utility.  We then considered the top 10 and 
20 most useful reviews for each movie.  Since IMDb displays 10 
reviews per page, this corresponds to what users see on the first and 
second pages of a given review forum. 
As shown in Table 1, female authors contributed only a quarter of 
the most useful reviews.  Years of research on information-seeking 
behavior suggest that when presented with an ordered list of 
documents, users rarely look beyond the first page presented to 
them (e.g. [17]).  Therefore, if it were not for the gender filter, users 
would read primarily reviews written by men. 
Table 1. Proportion of most useful reviews by gender. 
 Femal
e 
Male 
Top 10 reviews 0.24 0.76 
Top 20 reviews 0.26 0.74 
2. RELATED WORK 
Having highlighted key reasons why gender identity is important in 
the context of review communities, we now define the task addressed 
in our current work.  We then explain how this task differs from 
previous research on the prediction of author gender.  Our proposed 
task is: 
Given the set S of user-contributed reviews about movie j, 
predict the author gender of each review, s ⊂ S.  We assume that 
all information seen by users at the main page of the movie’s 
review forum is available for use in our model. 
As in previous work, we will model features of author writing style 
and review content in inferring reviewer gender.  Most approaches to 
text categorization problems rely exclusively on features that 
characterize these two aspects of a text.  Commonly used features 
include token-level measures (e.g. average word or sentence length), 
vocabulary richness (i.e. size of vocabulary / length), and counts of 
occurrences of commonly used words [35].  In addition, researchers 
have applied dimension-reduction techniques (e.g. factor analysis) on 
word frequencies (e.g. [3]). 
In addition to style and content, we will exploit information available 
at the IMDb community.  This information includes movie metadata 
(e.g. the number of reviews contributed, which gives us a measure of 
the movie’s popularity), as well as review metadata (e.g. date posted, 
reviewer’s rating of the movie).  In addition, we have review 
metadata that concerns the social feedback mechanism used at IMDb 
(i.e. utility scores).  The attributes used in the model are fully detailed 
in Section 4. 
Most previous research relevant to ours concerns “clean” text, as 
opposed to user-contributed content that has become so 
characteristic of the Web.  For instance, Koppel and colleagues [19] 
370
classified a set of fiction and non-fiction documents from the British 
National Corpus (BNC).  In addition to lexical features (frequencies 
of commonly used words), they exploited a set of “quasi-syntactic” 
features, since the BNC is tagged for parts-of-speech.  They 
reported 80% accuracy in predicting author gender. 
Argamon et al. [2] also predicted author gender for formal texts 
from the BNC, however, they exploited different features.  In 
particular, they found differences in the use of pronouns.  While 
male and female authors made references to nominals (i.e. wrote 
about “things”) with similar frequencies, females’ use of pronouns 
was much greater, and they used more first person pronouns.  Citing 
Biber [4], they noted that women have a more “involved” writing 
style, whereas men engage in an “informative” style of expression.  
More recently [3], they turned their attention to inferring author 
gender in blog text.  They compared themes discussed by male 
versus female bloggers.  To derive key themes, they used the 
“meaning extraction method” [6], performing factor analysis on 
frequencies of the most common words in the corpus.  They 
discovered 20 themes, and found differences between the texts of 
male and female bloggers.  For instance, “religion” and “politics” 
were common themes discussed by men whereas “conversation” 
and “at home” were popular with women.  They reported 
classification accuracies around 80%. 
Our task differs from those examined previously, and we are not 
aware of any work that considers the prediction of author gender in 
postings in an online community.  First, reviews posted to a 
community are not clean texts.  Most communities are not closely 
moderated and typically allow anyone to post with few barriers to 
entry (i.e. after creating a user name and password).  Reviews often 
contain grammatical errors and may even be off-topic.  Following 
Sahlgren and Karlgren [32], we direct our efforts toward 
indentifying features that are reasonable to compute in real-time, 
and that are generally robust, given the noise in user-contributed 
content.  Therefore, we avoid the use of syntactic or semantic 
parsing. 
Second, movie reviews are significantly shorter than most texts 
analyzed in previous work.  For instance, in [19], the average text 
length was over 34,000 words.  In [3], the minimum blog length was 
500 words.  To contrast, our mean review length is only 256 words.  
Therefore, a movie review provides fewer clues about author 
writing style and themes that she or he might discuss.  On the other 
hand, since movie reviews are part of an online discourse of a film 
as compared to being stand-alone texts, we can exploit metadata that 
provides clues about the social aspects of the community.  The goal 
of our current work then, is to examine how the following three 
types of information contribute to the inference of reviewer gender: 
1) writing style, 2) content, and 3) movie and review metadata. 
3. DATA COLLECTION 
To create a movie review corpus2, we relied on the list of the 250 
top films of all time (as of January 2010), as voted by IMDb 
participants, reasoning that they would have many reviews written 
by both men and women.  At each movie forum, we used the 
“male/female” filter to collect reviews with gender information.  For 
the 250 movies studied, 31,300 reviews had gender identity, out of 
                                                                 
2 While there are several other movie review corpora available 
(e.g. for studying sentiment analysis), we were not able to find 
existing data with author gender. 
157,065 total reviews (19.9%).  Within each movie, the proportion of 
reviews with gender identity ranged from 54% to only 2% (median 
of 18.5%). 
It is not possible to calculate the true proportion of reviewers that 
have disclosed their gender, because IMDb always displays an equal 
number of male- and female-authored reviews.  In other words, for a 
given movie, if 10 authors had disclosed that they are female, while 
35 authors had indicated that they are male, IMDb would only 
display 10 female- and 10 male-authored reviews.  There appears to 
be no way to identify the other 25 male reviewers, as this information 
is not displayed on user profiles. 
3.1 Treatment 
Before performing the analyses that involved word counts, we 
performed stemming.  We also disregarded words that had occurred 
only once in the entire data set.  As in [32], we reasoned that such 
terms were very likely to be junk, such as misspellings or left over 
HTML artifacts. 
3.2 Characteristics 
Most movies have been labeled with more than one genre (e.g. The 
Godfather is labeled as “crime,” “drama,” and “thriller”).  The 
following genres are represented in our data set: action, adventure, 
animation, biography, comedy, crime, drama, family, fantasy, film 
noir, history, horror, musical, mystery, romance, sci-fi, sport, thriller, 
war and western.  “Drama” is by far the most common genre in our 
data (158 films) while “sport” (e.g. Million Dollar Baby) is the least 
common.   
Table 2 presents an overview of corpus attributes.  As can be seen, 
even though there are an equal number of male- and female-authored 
reviews in the data, male reviewers have written over a million words 
more than have female reviewers.  In addition, over all male-
contributed reviews, we find a larger vocabulary as compared to 
female-authored reviews. 
Table 2. Corpus attributes. 
 Male Female All 
Reviews 15,650 15,650 31,300 
Unique 
Reviewers 
10,394 10,618 21,012 
Reviews / Movie 
(mean/median) 
NA NA 125.2 / 69 
Total Words 4,625,377 3,396,819 8,022,196 
Vocabulary Size 27,972 23,403 35,783 
 
Table 3 presents summary statistics for review length in words, as 
well as the total feedback votes received and the perceived utility 
of the review.  As these distributions are often skewed, the table 
shows both means and medians.  It can be seen that there are 
significant differences between male- and female-authored 
reviews; a Wilcoxon-Mann-Whitney rank-sum test [25] for 
evaluating the difference between the medians of males versus 
females has a p-value of approximately zero for all three 
attributes.   
As noted in Section 1, among the most useful reviews, only a 
quarter are written by women.  Likewise, it can be seen that the 
statistics below support Herring’s claims [12] that men dominate 
371
online discussions, with women receiving less attention.  Over all, 
women’s reviews receive significantly fewer feedback votes and 
are viewed by the community as being less useful than men’s 
reviews.  In addition, it is interesting to note that men tend to 
write longer reviews as compared to female reviewers. 
 
Table 3.  Review attributes (mean / median). 
 Male Female 
Length (words)  295.3 / 222 216.8 / 162 
Total votes 30.7 / 3 7.1 / 1 
Utility 0.60 / 0.67 0.26 / 0 
 
4. RESEARCH FRAMEWORK 
We use a statistical regression model [27] to examine the 
contribution of three kinds of information in the task of predicting 
author gender: 1) aspects of author writing style, 2) content of the 
review and 3) metadata concerning the movie reviewed and the 
review.  Specifically, we use logistic regression to model the log 
odds of a review being written by a female versus a male.  More 
formally, the model to be estimated is: 
  
ln Pr( yi = 1)
Pr( yi = 0)
⎡ 
⎣ 
⎢ 
⎤ 
⎦ 
⎥ = b0 + b1x1 + b2x2 + ... + bn xn  
where   x1 to   xn  are the predictors and   yi  is the response (taking 
the value of 0 if the author is a man and 1 if a woman).  Thus, 
male is the default and we are predicting the event of a reviewer 
being female.  Stated otherwise, given a vector of n variables 
describing a review, we use the predicted likelihood to assign the 
vector to one of two groups (i.e. if the likelihood of being female 
is greater than 0.5, the author is labeled as female; otherwise the 
label is left as male).  It should be noted that a variety of 
statistical and machine learning approaches has been applied to 
text classification problems.  Citing Yang [39], Stamatatos [35] 
explains that for relatively large data sets (i.e. at least 300 
observations for each class), there is little difference in 
performance of most classifiers on text classification tasks.  Since 
our focus is to compare the contribution of the three types of 
information, rather than to contribute to the machine learning 
research, we employ a multivariate statistics technique.  Logistic 
regression allows us to model a dichotomous response variable, 
while avoiding assumptions as to the normality of our predictor 
variables. 
4.1 Writing style 
As discussed in Section 2, previous research has found that 
women have a more engaging style of language use as compared 
to men, who focus more on conveying information and less on 
building relationships.  We wish to test whether or not such 
differences in writing styles between women and men are evident 
in the movie review genre.  Table 4 presents the aspects of author 
writing style we have incorporated into our present analysis and 
provides a short justification for each.  These measures include 
the rates of pronoun use and hedging, the complexity of words 
and sentence structure, vocabulary richness, and the frequencies 
of common words (i.e. lexical markers). 
Table 4.  Writing style features. 
 Measure Explanation 
# Pronouns (PN) / 
total words 
# First person PN 
# Second person 
PN 
Inner vs.  
outer-focused 
discourse 
# Third person PN 
[2,3] found that women 
use a higher rate of 
pronouns.  We compute 
the rate of PN use, as 
well as the number of 
first, second and third-
person PNs. 
Hedging # Hedges used / 
total words 
We use a list of 55 
hedges, adapted from 
Lakoff [21].  [22] 
reported that hedging is 
very characteristic of 
women’s language. 
Characters / 
words 
Characters / 
words in title 
Complexity 
Words / sentences 
These provide a basic 
measure of writing 
complexity and have 
been used for automatic 
essay scoring [8]. 
Vocabulary 
richness 
# Unique words / 
total words 
Quantifies the diversity 
of lexical items used.  
[35] warns that the 
measure may not be 
stable in short texts. 
Lexical 
markers 
Frequency of top 
50 words 
We create a list of the 
50 words with the 
highest frequencies 
across the corpus and 
count their occurrences 
in each review. 
4.2 Review content 
We also want to examine if there are content differences between 
reviews written by men versus women.  In other words, do they 
discuss different themes when reviewing a movie?  Do male-
authored or female-authored reviews tend to be more central or 
unique, given the topics discussed over all reviews?  We consider 
five metrics that express different aspects of review content. 
The first four metrics presented in Table 5 tell us something about 
the content of a review, with respect to the other reviews written 
about the same movie.  The first is a measure of review centrality.  
To compute centrality, we create a centroid, a vector consisting of 
all words occurring in the entire set of reviews about movie, j.  
Each element corresponds to the tfidf weight for the given word 
[33].  The centroid score for review s, which is the sum of the tfidf 
scores of all words in review s, quantifies the extent to which it 
contains words that are important across the set of reviews. 
Perplexity, entropy and the out of vocabulary rate (OOV) are 
based on a language modeling framework [26].  We view the set 
of reviews about movie j, but excluding review s, as a bag of 
words, from which a probability distribution is derived.  The 
creation of review s is then viewed as a sequence of randomly 
selected words from this distribution.  Perplexity, entropy and 
OOV each measure an aspect of the uniqueness of review s, given 
all other reviews posted to the forum of movie j.   
372
Table 5.  Review content features. 
Centrality A document-level centroid score [31] 
quantifies the extent to which review s is 
important in the discourse about movie j 
Perplexity [26] measures the extent to 
which review s is surprising.  A perplexity 
of k that means that guessing the words in 
the review is as difficult as guessing 
between k items of equal probability. 
Entropy [26] quantifies the average 
uncertainty of review s, expressed as the 
number of bits required to encode it. 
Uniqueness 
OOV rate is the proportion of words in 
review s that do not appear in any other 
reviews of movie j. 
Themes We use the set of 31,300 reviews to create 
a semantic space on which to perform 
LSA.  We discover the top 20 themes used 
in our IMDb movie reviews.  For each 
review, we use the scores on these 20 
themes to represent the content discussed. 
 
F1  Articles/Prepositions:  the, of, to, a, in, on, that, with 
F2  First Person:  I, my, me, am, think, saw 
F3  Third Person:  he, his, him, himself, man, wife 
F4  Second Person:  you, your, if, go, see, want 
F5 Positive Impression:  good, really, like, pretty, funny 
F6 Success:  perform, best, role, actor, Oscar 
F7  Fantasy:  ring, battle, fantasy, book, fan, version 
F8  Feminine: her, she, woman, girl, beautiful, relationship 
F9  Violence:  scene, plot, end, kill, murder, dead 
F10  Others liked:  movie, watch, great, people, like, good 
F11 Location:  Unit, state, theater 
F12 All time hit:  ever, best, greatest, classic, favorite 
F13 Groups:  they, them, people, other, we, our, together 
F14 Family: kid, girl, children, family, comedy, father 
F15 Aux/modals/past:  was, were, had, did, didn’t, would 
F16 Aux/modals:  have, been, be, if, any, may, should, do 
F17 Comic:  Batman, dark, comic, hero, action, fan 
F18 Relations:  time, relationship, character, together, first 
F19 Time:  hour, minute, two, three, no 
F20 Effects:  effect, special, visual, action, star 
Figure 2.  Factors and lexical items with largest loadings. 
Our last content feature is based on Latent Semantic Analysis 
(LSA) [23].  We first construct a semantic space using our movie 
review data.  Specifically, we begin by computing word counts 
across all reviews.  We then consider the 500 most frequent 
words.  Thus, we have a matrix with dimensions 31,300 by 500, 
which we subject to a factor analysis.  We derive the first 20 factors, 
corresponding to the eigenvectors with the 20 highest eigenvalues.  
Finally, we compute the scores on these 20 factors for each review.  
Our analysis will examine if, based on this representation, there are 
differences in content between the male- and female-authored 
reviews. 
Interestingly, researchers employing LSA and related methods do not 
report their analyses in standard ways.  For example, [3] and [6] do 
not explain how they determined the number of factors to use, 
although they emphasize their interpretation.  To contrast, [23] uses a 
very high dimensional space as to account for a maximum amount of 
variance in the data; however, they do not attempt to interpret what 
these underlying dimensions mean. 
We compared candidate models to a baseline with only one factor 
and computed Bentler and Bonnett’s Normed Fit Index (NFI) [24].  
To achieve a good fit (i.e. NFI of 0.90 or greater) we need 
approximately 100 factors.  However, interpretation becomes 
difficult, as many factors have small eigenvalues.  Therefore, we also 
consider the Kaiser criterion, which states that any factor with an 
eigenvalue less than one should be dropped [18].  We experiment 
with the model having 20 factors, whose smallest eigenvalue is 
slightly greater than 1.   
Figure 2 describes the factors with suggested headings, and shows the 
words with the largest loadings on each factor.  As can be seen, the 
majority of the factors (e.g. F1, F2, F12, F14) have straightforward 
interpretations.  However, others are less obvious.  In particular, it is 
difficult to distinguish between the meanings of F5 and F10, both 
which involve words used to express positive sentiments of movies.  
It can also be seen that several factors (e.g. F1, F2) overlap with 
stylistic features. 
4.3 Metadata 
Table 6 summarizes the metadata used in our task.  As shown, this 
information concerns the movie itself (e.g. popularity), review 
sentiment (i.e. number of stars reviewer gave the movie), the age and 
length of the reviews and how reviews are received by the 
community.  In addition to the reviewer’s rating, we also include the 
deviation of the rating from average (i.e. over all reviewers).  
Previous work found that there is a negative relationship between the 
deviation from average opinion expressed in a review forum, and the 
perceived utility of a review [7].  As noted in Section 3, it is clear that 
women’s reviews receive less attention and are seen as being less 
useful than those written by men.  Since we want to predict reviewer 
gender, we need to examine this phenomenon. 
Table 6.  Metadata. 
Movie popularity # Total reviews about movie j 
Reviewer rating  
(1 to 10 stars) 
Rating assigned by author of  
review s 
Reviewer rating 
deviation 
Reviewer’s rating – average rating 
Age (days) Age of review s 
Length (words) # Words in review s 
Length (sentences) # Sentences in review s 
Title length (words) # Words in title of review s 
Attention from 
community 
Total votes for review s 
Perceived utility Utility of review s 
373
5. ANALYSIS 
To examine the contribution of the three types of information for 
inferring reviewer gender, we build a separate logistic regression 
model with the features of each type as the predictors: 1) stylistic, 2) 
content, and 3) metadata features.  To control for the possibility that 
reviews differ in content or writing style depending on the movie 
reviewed, we initially add dummy variables for the 250 movies.  
However, these are not significant in any of the models. 
To fit the models, we use a forward stepwise procedure [27], which 
begins with an empty model (i.e. assigning the label of the mode) and 
adds a predictor if it significantly reduces the likelihood ratio to the 
current model (i.e. when the Likelihood Ratio test for the effect of the 
predictor has a p-value < 0.05).  Once we have a model, we compare 
the estimated odds ratios of the predictors, to identify features in each 
category that are associated with author gender.  Finally, after having 
examined features of writing style, content and metadata, we follow 
the same procedure to build a model using features of all three types. 
5.1 Model with stylistic features 
Recall that the features of style we examined include the rates of 
pronoun use, vocabulary richness, the use of hedges, word and 
sentence complexity, and lexical features.  For the lexical features, 
we first identify the most frequent 50 words in the corpus.  This 
results in a set of 5 content words (movie, film, story, characters, 
time) and 45 function words (pronouns, articles, prepositions, 
adverbs and verbs).  For each review, we counted the occurrences of 
these words, resulting in a set of 50 lexical features.  Clearly, some of 
the stylistic features are correlated (e.g. occurrences of the word “I” 
and the number of first person pronouns in a given review).  
Therefore, it is expected that several features will end up not having a 
significant effect in the model. 
Table 7 shows the features of writing style that have a significant 
relationship to the log odds of a reviewer being female versus male.  
For ease of interpretation, we show the parameter estimates as odds 
ratios.  For example, the odds ratio for vocabulary richness is 6.6.  
This means that, when holding other factors constant, for a one-unit 
increase in vocabulary richness, the odds of a reviewer being female 
versus male increase by a factor of 6.6.  Obviously, the closer the 
odds ratio is to one, the less likely it is that the variable will be useful 
for discriminating between male and female reviewers.  While the 
model as a whole is highly significant, it has a pseudo R2 
(McFadden’s R2) of only 0.093.  This suggests that stylistic features 
alone most likely do not provide enough information to distinguish 
between male and female reviewers.  While we will report the 
classification accuracy using only stylistic features in Section 5.5, 
here we discuss the significant differences in terms of writing style 
between male and female reviewers. 
Figure 3 summarizes the stylistic trends in movie reviews authored 
by women versus men, which can be observed in Table 7.  As shown, 
female-authored reviews are characterized by a higher rate of 
pronoun use, and in particular, the singular first person.  Despite 
writing shorter reviews than men, women tend to use a richer 
vocabulary.  To contrast, male-authored reviews are 
                                                                
3 Unlike in OLS regression, the pseudo R2 cannot be interpreted 
as the proportion of variance in the independent variable that is 
explained by the model; it is a simple measure of the strength of 
association between the predictors and the independent variable. 
Table 7. Model with style features. 
 Feature Odds Ratio 
PN / 1000 1.0103 Pronoun Use 
# 3rd person PN 1.014 
Vocabulary 
Richness 
Vocab size / words 6.626 
Char / word ratio 0.7919 Complexity 
Word / sent ratio 0.9903 
see 1.0954 
was 1.0818 
from 0.9279 
who 1.0700 
has 0.9446 
it 0.9471 
not 1.0490 
by 0.9514 
so 1.0462 
this 0.9584 
but 0.9576 
on 0.9590 
like 0.9599 
I 1.0394 
and 1.0389 
film 0.9631 
as 0.9681 
all 0.9687 
story 0.9686 
just 0.9719 
a 0.9736 
movie 0.9741 
with 0.9762 
to 1.0163 
of 0.9863 
out 0.9631 
his 0.9788 
time 0.9750 
if 0.9691 
character 1.0257 
be 1.0198 
Lexical 
Features 
more 0.9733 
 
characterized by the use of prepositions.  Also, it is interesting 
that of the five content words, all but the word involving people 
(“character”) are used more often by men than women.  Finally, 
men’s reviews involve more complex words and sentences.  In 
particular, the characters-to-words odds ratio is only 0.79.   
 
374
Overall rate of pronoun use 
Use of first person pronouns (“I”) 
Use of verb “to be” (“be,” “was”) 
Richer vocabulary 
Use of prepositions: as, by, from, of, with, on, out 
Use of third person (“his,” “it”) 
Use of content words (movie, film, story, time) 
Relatively more complex words and sentences 
Figure 3:  Stylistic trends of women (top) and men (bottom). 
In summary, our analysis of stylistic features concurs with the claims 
of previous research, and in particular, that of Argamon and 
colleagues [2,3].  There is evidence that female movie reviewers tend 
to write with a more involved style, frequently writing in first person 
voice.  To contrast, men more often use a reporting style, discussing 
“the movie” or “a film,” and making use of the third person pronouns 
“his” and “it.” 
5.2 Model with content features 
Table 8 shows the content features that have a significant effect in the 
model.  The odds ratios greater than one (i.e. features associated with 
female-authored reviews) are shown at the top of the table and those 
less than one (i.e. related to male reviewers) at the bottom.  We can 
see that female reviewers often discuss people and relationships, and 
use pronouns frequently.  The factors representing the use of 1st, 2nd 
and 3rd person pronouns are all positively related to female-authored 
reviews.  In addition, the factors we described as relating to “groups,” 
“family” and “relations” are associated with women’s reviews. 
Table 8. Model with content features. 
  Odds Ratio 
1st person F2 1.4616 
Groups F13 1.2718 
3rd person F3 1.2017 
Feminine F8 1.2000 
Aux/modals/past F15 1.1845 
Comic F17 1.0759 
Family F14 1.0679 
Relations F18 1.0517 
2nd person F4 1.0481 
 
Articles/Preposition
s 
F1 0.5721 
All time hit F12 0.8158 
Uniqueness Entropy 0.8168 
Violence F9 0.8332 
Positive Impression F5 0.8617 
Effects F20 0.8682 
Success F6 0.8696 
Time F19 0.8795 
Others liked F10 0.9464 
As in the model with stylistic features, we again observe that the 
use of prepositions and articles (F1) is strongly related to male-
authored reviews.  The themes discussed more often by males as 
compared to females include a movie being an “all time hit,” 
violence, and special effects.  Men are also more likely than 
women to discuss the success of the movie (e.g. if it won an 
Oscar).  Finally, entropy is the only feature based on language 
modeling that has a significant effect in the model.  Compared to 
other reviews written about the same movie, male-authored 
reviews tend to be more unique than those of females.  Perhaps 
this might partially explain why men’s reviews are also perceived 
as being more useful than those of women.  We will return to this 
question in the discussion section. 
The likelihood ratio test for the model as a whole is highly 
significant.  However, like the model based on stylistic features, 
its pseudo R2 is only 0.09.  Therefore, it is uncertain whether 
content features alone can adequately distinguish between male 
and female-authored reviews. 
5.3 Predicting gender with metadata features 
Table 9 shows the model that includes the metadata predictors 
having a significant effect.  The model over all is highly 
statistically significant and has a pseudo R2 of 0.1826. 
As can be seen, six of the metadata features have a statistically 
significant effect in the model.  In general, reviews written by 
men tend to be longer, receive more feedback, are seen as more 
useful by others, are posted early on (i.e. are younger) and have 
longer titles, when controlling for the popularity of the movie (i.e. 
number of reviews written).  However, it is perceived utility that 
appears to have the strongest relationship to reviewer gender, as 
its respective odds ratio is only 0.086.   
Table 9. Model with metadata predictors. 
 Odds Ratio 
Utility 0.0863 
Length (words) 0.9988 
# Reviews written 0.9989 
Total votes 0.9940 
Age 0.9998 
Length title (chars) 0.9961 
5.4 Model with all features 
As mentioned, many of the stylistic and content features are 
correlated to one another.  In particular, the counts of first, second 
and third person pronouns used as stylistic features, are highly 
correlated to F2, F4 and F3, respectively.  In addition, F1 accounts 
for the use of articles and prepositions, eliminating the need for the 
majority of the lexical features.  Therefore, to avoid problems with 
collinearity in the model using all three types of predictors, we 
exclude the 50 lexical features. 
The likelihood ratio test for the model as a whole is highly 
significant.  In addition, the model based on stylistic, content and 
metadata features has a pseudo R2 of 0.2379.  As mentioned, the 
pseudo R2 does not have a straightforward interpretation as does R2 in 
the context of ordinary least squares regression.  Therefore, we 
next turn toward evaluating the classification accuracy of each of 
our models. 
375
Table 10. Model with style, content and metadata features. 
 Odds 
Ratio 
Utility 0.0863 
Length (sents) 0.9854 
Total votes 0.9946 
# Reviews 0.9983 
Title length (chars) 0.9982 
Metadata 
Age 0.9998 
Vocab Richness 1.687 
PN / 1000 1.006 
Style 
Words / sent 0.9888 
Uniqueness Entropy 0.8320 
1st person (F2) 1.4596 
Groups (F13) 1.2100 
Comic (F17) 1.2100 
Aux/modals/past (F15) 1.2012 
Feminine (F8) 1.1777 
Location (F11) 1.1710 
Fantasy (F7) 1.1315 
Family (F14) 1.1115 
3rd person (F3) 1.1077 
Violence (F9) 0.8111 
All time hit (F12) 0.8186 
Positive impression 
(F5) 
0.8771 
Articles/prepositions 
(F1) 
0.8800 
Effects (F20) 0.9347 
Content / 
themes 
Success (F6) 0.9447 
5.5 Comparison between models 
In our analyses, we indentified independent variables that are 
correlated to author gender.  To compare models in terms of their 
ability to infer gender, and to be able to compare our results to 
previous work, we now evaluate their classification accuracy on 
this task.  To do this, we use a 10-fold cross-validation procedure 
[28].  Table 11 reports the average classification accuracies for 
each model, as well as the respective standard deviation.  In 
addition, λp [27] is also shown.  This statistic ranges from 0 to 1, 
Table 11. Classification accuracy for all models. 
 Accuracy Std dev. λp 
Stylistic 64.55 1.0717 0.291 
Content 65.03 1.2119 0.301 
Metadata 72.95 3.088 0.459 
Utility 72.46 3.495 0.449 
ALL 73.71 3.662 0.474 
and expresses the proportion reduction in error, relative to the 
baseline of predicting the mode (i.e. labeling all reviewers as male, 
which results in 50% error).  As a comparison, we also show the 
accuracy when review utility is the only predictor. 
As can be seen, while all models do significantly better than the 
baseline, the model incorporating all three types of predictor 
variables has the best performance.  Its accuracy is significantly 
better than that of the content only and style only models.  However, 
there is not a statistically significant difference between its accuracy 
and that of the models using only metadata and only review utility to 
infer gender.  
Others have previously noted that features characterizing the writing 
style and content of a text can be sensitive to the text’s length (e.g. 
[35]).  Therefore, we also examined the average classification 
accuracy of each of the models, as a function of review length.  
Figure 4 indeed illustrates that the accuracy of the models 
incorporating only stylistic and content features is positively 
correlated to length.  To contrast, the models using all features and 
only metadata are relatively more stable. 
 
Figure 4:  Accuracy as a function of review length. 
6. DISCUSSION AND CONCLUSION 
There are salient differences both in terms of writing style and 
content between IMDb movie reviews written by male and female 
authors.  However, the perceived utility of the review by community 
members is the strongest predictor of gender.  We briefly examine 
this point further in order to provide a possible explanation.  We then 
conclude with a discussion of areas for future work. 
6.1 Gender and review utility 
Why is utility such a strong predictor of gender, even when 
controlling for differences in writing style and content and for 
properties such as length and age?  As mentioned in the introduction, 
consumers with similar demographics are more likely to share tastes 
in hedonic goods such as movies.  Therefore, a likely explanation is 
that a large proportion of IMDb participants are men, who give more 
favorable feedback to male-authored reviews, since they are similar 
to their own perspectives and communicate those views in a way that 
is familiar to them. 
We currently do not have the necessary data to examine whether men 
give better feedback to male-authored reviews as compared to those 
written by women.  In addition, we cannot be sure if the majority of 
community participants are male or female, since this information is 
376
not available in user profiles.  However, we can study the 
characteristics of the highly rated female-authored reviews.  In other 
words, could it be the case that reviews authored by women that are 
perceived to be relatively useful are more similar to male-authored 
reviews? 
To examine this question, we divide the female-authored reviews into 
two groups: more useful (utility > 0.5; n = 2,898) and less useful 
(utility ≤ 0.5; n = 12,752).  We use the same forward stepwise 
procedure (with a significance threshold of 0.05) to fit a logistic 
regression model to distinguish the more useful reviews from the less 
useful ones, using stylistic (excluding the lexical features), content 
and metadata predictors.  Several variables, which have been noted to 
be positively correlated to review utility [7], have significant positive 
effects in the model:  total feedback votes, review age, reviewer’s 
rating of the movie, and length. 
Figure 5 displays predictors that have a significant effect in the 
model, with their respective odds ratios.  Of interest is that one of the 
themes that was associated with male-authored reviews in Table 10, 
“all time hit,” is characteristic of the more useful female-authored 
reviews.  Likewise, we can observe that vocabulary richness, which 
was associated with female-authored reviews in Table 10, is more 
correlated to the less useful reviews written by women.  Finally, the 
use of first and second person pronouns is also more characteristic of 
less useful reviews.  In summary, the analysis suggests that reviews 
written by women, which are viewed by the community as being 
more useful, do have similarities to male-authored reviews, as 
compared to less useful female-authored reviews. 
All time hit (F12): 1.253 
Feminine (F8): 1.096 
Vocab richness: 0.226 
2nd person (F4): 0.899 
1st person (F2): 0.934 
Figure 5:  Features of more (top) and less (bottom) useful 
reviews. 
6.2 Future work 
We have found that there are differences in the way that men and 
women write movie reviews in an online community environment.  
Many of these differences concur with results of previous research on 
other genres, including formal [2] and blog [3] text.  The findings 
support the claim that women, in general, have a more involved 
writing style, both in terms of how they express themselves (e.g. 
frequently using pronouns and, in particular, the first person singular) 
and with respect to the themes they discuss in movie reviews (e.g. 
writing about characters and their relationships versus whether or not 
the movie had won an Oscar or had particular special effects). 
However, we also found that the classifiers based only on review 
content or writing style features are not as accurate as the classifier 
incorporating all three types of features.  In particular, since review 
utility is strongly correlated to author gender, its inclusion as a 
predictor significantly improves classification accuracy.  The other 
benefit of exploiting metadata from the IMDb community is that the 
classifier’s performance is not as sensitive to review length, as 
compared to those based only on content and stylistic features.   
In future work, we plan to reexamine the performance of the three 
classifiers, given more information about each author.  Currently, we 
had only one review per author, on which to base a decision as to his 
or her gender, along with movie and review metadata.  Another task 
of interest is to infer a user’s gender, given all of his or her postings at 
the community.  In other words, for many users, we will have 
reviews of multiple movies, along with associated metadata (e.g. the 
user’s average utility over a larger set of contributions).  Given that 
features of writing style and content are known to be sensitive to text 
length [35] and the brief nature of online reviews, we plan to further 
explore reviewer gender classification exploiting different types of 
information. 
Future work should also address how to effectively and responsibly 
use reviewer gender information within the community.  As 
discussed, IMDb does not display participant gender on profiles, but 
does offer a gender filter, that offers users the benefit of getting a 
gender-balanced perspective on a movie of interest.  Gender 
information could also be used to suggest yet unviewed reviews of 
possible interest to a user, which are written by someone of the same 
gender.  In conclusion, mechanisms can be developed that might help 
communities balance out the participation and visibility of male and 
female participants.  However, at the same time, care must be taken 
to ensure that gender identities are not used in a way that might cause 
privacy concerns among community participants. 
7. ACKNOWLEDGMENTS 
The author would like to thank Alexia Panayiotou for discussions 
about gender, language and the IMDb and acknowledges 
Mengyuan (Serena) Li for her assistance with data collection.  
Finally, thanks also go to the four anonymous reviewers for their 
feedback, which helped to improve the paper. 
8. REFERENCES 
[1] Acquisti, A. and Gross, R.  2006.  Imagined communities: 
awareness, information sharing, and privacy on the Facebook.  
Privacy Enhancing Technologies, Lecture Notes in Computer 
Science.  Springer, Berlin, 36-58. 
[2] Argamon, S., Koppel, M., Fine, J. and Shimoni, A. R.  2003.  
Gender, genre, and writing style in formal written texts.  Text, 
23, August 2003. 
[3] Argamon, S., Koppel, M., Pennebaker, J. W., and Schler, J.  
2007.  Mining the Blogosphere: Age, gender and the varieties 
of self-expression.  First Monday, 12, 9. 
[4] Biber, D.  1995.  Dimensions of Register Variation: a Cross-
Linguistic Comparison.  Cambridge University Press. 
[5] Bruckman, A.  1996.  Gender swapping on the Internet.  In 
Ludlow, P. (ed.)  High Noon on the Electronic Frontier: 
Conceptual Issues in Cyberspace.  MIT Press. 
[6] Chung, C. and Pennebaker, J. W.  2008.  Revealing dimensions 
of thinking in open-ended self-descriptions: an automated 
meaning extraction method for natural language.  Journal of 
Research in Personality 42, 96-132. 
[7] Danescu-Niculescu-Mizil, C., Kossinets, G., Kleinberg, J. and 
Lee, L.  2009.  How opinions are received by online 
communities: A case study on Amazon.com helpfulness votes.  
In Proceedings of the International World Wide Web 
Conference, (pp. 141-150).  Madrid, Spain:  ACM Press. 
[8] Foltz, P.W., Laham, D., Landauer, T.K.  1999.  Automated 
essay scoring: Applications to educational technology.  In 
Proceedings of EdMedia. 
377
[9] Gefen, D. and Ridings, C. M.  2005.  If you spoke as she does, 
sir, instead of the way you do: a sociolinguistics perspective of 
gender differences in virtual communities.  In ACM SIGMIS 
Database 36, 2, 78-92. 
[10] Goldberg, D., Nichols, D., Oki, B. M., and Terry, D.  1992.  
Using collaborative filtering to weave an information tapestry.  
Communications of the ACM, 35, 12, 61-70. 
[11] Herring, S. C.  2000.  Gender differences in CMC: Findings 
and implications.  Computer Professionals for Social 
Responsibility Newsletter, 18, 1, Winter. 
[12] Herring, S. C.  2003.  Gender and power in online 
communities.  In Holmes, J. and Meyeroff, M. (eds.)  The 
Handbook of Language and Gender, pp. 202-228.  Oxford: 
Blackwell. 
[13] Hiltz, S. R. and Turoff, M.  1985.  Structuring computer-
mediated communication systems to avoid information 
overload.  Communications of the ACM, 28, 7, 680-689. 
[14] Hirschman, E. C. and Holbrook, M. B.  1982.  Hedonic 
consumption: Emerging concepts, methods and propositions.  
Journal of Marketing, 46 (Summer 1982), 92-101. 
[15] Holbrook, M. B. and Schindler, R. M.  1994.  Age, sex and 
attitude toward the past as predictors of consumers’ aesthetic 
tastes for cultural products.  Journal of Marketing Research, 
31, 412-422. 
[16] Hu, Jian, Zeng, Hua-Jun, Li, Hua, Niu, Cheng and Chen, 
Zheng.  2007.  Demographic prediction based on user’s 
browsing behavior.  In Proceedings of the ACM WWW (Banff, 
Alberta, Canada, May 08-12, 2007).  WWW ’07.  ACM Press, 
New York, NY, 151-160. 
[17] Joachims, T., Granka, L., Pan, B., Humbrooke, H., Radlinski, 
F., and Gay, G.  2007.  Evaluating the accuracy of implicity 
feedback from clicks and query reformations in Web search.  
ACM Transactions on Information Systems 25 (2). 
[18] Kaiser, H. F.  1960.  The application of electronic computers to 
factor analysis.  Educational and Psychological Measurement 
20, 141-151. 
[19] Koppel, M., Argamon, S. and Shimoni, A.R.  2004.  
Automatically categorizing written texts by author gender.  
Literary and Linguistic Computing, 17, 4. 
[20] Kostakos, V.  2009.  Is the crowd’s wisdom biased?  a 
quantitative analysis of three online communities.  In 
Proceedings of IEEE Social Comm, International Symposium 
on Social Intelligence and Networking.  Vancouver, Canada. 
[21] Lakoff, G.  1973.  Hedges: a study in meaning criteria and the 
logic of fuzzy concepts.  Journal of Philosophical Logic 2, 4, 
458-508. 
[22] Lakoff, R.  1973.  Language and woman’s place.  Language in 
Society, 2, 45-79. 
[23] Laudauer, T. K., Foltz, P.W., and Laham, D.  1998.  An 
introduction to latent semantic analysis.  Discourse Processes, 
25, 259-284. 
[24] Loehlin, J. C.  1992.  Latent Variable Models.  Lawrence 
Erlbaum Associates. 
[25] Mann, H.B. and Whitney, D.R.  1947.  On a test of whether 
one of two random variables is stochastically larger than the 
other.  Annals of Mathematical Statistics, 18, 50-60. 
[26] Manning, C. D. and Schutze, H.  2000.  Foundations of 
Statistical Natural Language Processing.  MIT Press. 
[27] Menard, S.  2002.  Applied Logistic Regression Analysis.  
Quantitative Applications in the Social Sciences.  Sage 
University Press. 
[28] Mitchell, T.  1997.  Machine Learning.  New York: New York: 
McGraw Hill. 
[29] Oliver M. B., Weaver III, J. B., and Sargent, S. L.  2000.  An 
examination of factors related to sex differences in enjoyment 
of sad films.  Journal of Broadcasting and Electronic Media 
44, 2, 282-300. 
[30] Popescu, A. and Grefenstette, G.  2010.  Mining user home 
location and gender from Flickr tags.  In Proceedings of the 4th 
International Conference on Weblogs and Social Media 
(Washington D.C., May 23-26, 2010).  AAAI. 
[31] Radev, D., Jing, H., Stys, M. and Tam, D.  2004.  Centroid-
based summarization of multiple documents.  Information 
Processing and Management, 40, 919-938. 
[32] Sahlgren, M. and Karlgren, J.  2009. Terminology mining in 
social media. In Proceedings of the ACM CIKM (Hong Kong, 
China, November 02 - 06, 2009). CIKM '09. ACM Press, New 
York, NY, 405-414.  
[33] Salton, G. and McGill, M.J.  1986.  Introduction to Modern 
Information Retrieval.  New York: McGraw-Hill, Inc. 
[34] Schindler, R.M. and Bickart, B.  2005.  Published word of 
mouth: Referable, consumer-generated information on the 
Internet.  In: Hauvgedt, C., Machleit, K. and Yalch, R. (eds.)  
Online Consumer Psychology: Understanding and Influencing 
Behavior in the Virtual World.  Lawrence Erlbaum Associates, 
35-61. 
[35] Stamatatos, E., Kokkinakis, G., and Fakotakis, N.  2000.  
Automatic text categorization in terms of genre and author.  
Computational Linguistics, 26, 4, 471-495. 
[36] Stutzman, F.  2006.  An evaluation of identity-sharing behavior 
in social network communities.  International Digital and 
Media Arts Journal, 3, 1. 
[37] Tannen, D.  1990.  You Just Don’t Understand.  New York: 
HarperCollins Publishers, Inc. 
[38] Terveen, L. and McDonald, D.W.  2005.  Social matching: a 
framework and research agenda.  ACM Transactions on 
Computer-Human Interaction, 12, 3 (September), 401-434. 
[39] Yang, Y.  1999.  An evaluation of statistical approaches to text 
categorization.  Information Retrieval Journal, 1, 1, 69-90. 
[40] Zhang, Z. and Varadarajan, B.  (2006).  Utility scoring of 
product reviews.  In Proceedings of the Conference on 
Information and Knowledge Management (pp. 51-57).  
Arlington, VA:  ACM Press.
 
378
