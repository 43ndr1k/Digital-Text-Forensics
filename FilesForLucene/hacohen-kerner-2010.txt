 
 
PLEASE SCROLL DOWN FOR ARTICLE
This article was downloaded by: [HEAL-Link Consortium]
On: 7 October 2010
Access details: Access Details: [subscription number 786636650]
Publisher Taylor & Francis
Informa Ltd Registered in England and Wales Registered Number: 1072954 Registered office: Mortimer House, 37-
41 Mortimer Street, London W1T 3JH, UK
Applied Artificial Intelligence
Publication details, including instructions for authors and subscription information:
http://www.informaworld.com/smpp/title~content=t713191765
STYLISTIC FEATURE SETS AS CLASSIFIERS OF DOCUMENTS
ACCORDING TO THEIR HISTORICAL PERIOD AND ETHNIC ORIGIN
Yaakov HaCohen-Kernera; Hananya Becka; Elchai Yehudaia; Dror Mughazb
a Jerusalem College of Technology (Machon Lev), Jerusalem, Israel b Bar-Ilan University, Ramat-Gan,
Israel
Online publication date: 01 October 2010
To cite this Article HaCohen-Kerner, Yaakov , Beck, Hananya , Yehudai, Elchai and Mughaz, Dror(2010) 'STYLISTIC
FEATURE SETS AS CLASSIFIERS OF DOCUMENTS ACCORDING TO THEIR HISTORICAL PERIOD AND ETHNIC
ORIGIN', Applied Artificial Intelligence, 24: 9, 847 — 862
To link to this Article: DOI: 10.1080/08839514.2010.514197
URL: http://dx.doi.org/10.1080/08839514.2010.514197
Full terms and conditions of use: http://www.informaworld.com/terms-and-conditions-of-access.pdf
This article may be used for research, teaching and private study purposes. Any substantial or
systematic reproduction, re-distribution, re-selling, loan or sub-licensing, systematic supply or
distribution in any form to anyone is expressly forbidden.
The publisher does not give any warranty express or implied or make any representation that the contents
will be complete or accurate or up to date. The accuracy of any instructions, formulae and drug doses
should be independently verified with primary sources. The publisher shall not be liable for any loss,
actions, claims, proceedings, demand or costs or damages whatsoever or howsoever caused arising directly
or indirectly in connection with or arising out of the use of this material.
STYLISTIC FEATURE SETS AS CLASSIFIERS OF DOCUMENTS
ACCORDING TO THEIR HISTORICAL PERIOD AND ETHNIC ORIGIN
Yaakov HaCohen-Kerner1, Hananya Beck1, Elchai Yehudai1, and
Dror Mughaz2
1Jerusalem College of Technology (Machon Lev), Jerusalem, Israel
2Bar-Ilan University, Ramat-Gan, Israel
& This research investigates classification of documents according to the ethnic group of their
authors and=or to the historical period when the documents were written. The classification is done
using various combinations of six sets of stylistic features: quantitative, orthographic, topographic,
lexical, function, and vocabulary richness. The application domain is Jewish Law articles written
in Hebrew-Aramaic, languages that are rich in their morphological forms. Four popular machine
learning methods have been applied. The logistic regression method led to the best accuracy results:
about 99.6% while classifying to the ethnic group of their authors or to the historical period when
the articles were written and about 98.3% while classifying to both classifications. The quantitat-
ive feature set was found as very successful and superior to all other sets. The lexical and function
feature sets have also been found to be useful. The quantitative and the function features are
domain independent and language independent. These two feature sets might be generalized to
similar classification tasks for other languages and can therefore be useful for the text classification
community at large.
INTRODUCTION
Text classification (TC) is an important research domain in machine
learning (ML) and information retrieval. It has a substantial potential for
many applications involving text such as filtering and routing. TC is the
supervised learning task of classifying natural language text documents to
one or more predefined categories (Meretakis and Wuthrich 1999). Super-
vised learning is an ML technique for learning a function from training
data. The training data consist of pairs of input objects and preassigned
outputs. The task of this kind of learning is to predict the value of the
Address correspondence to Yaakov HaCohen-Kerner, Department of Computer Science, Jerusalem
College of Technology (Machon Lev), 21 Havaad Haleumi St., P.O. Box 16031, Jerusalem 91160, Israel.
E-mail: kerner@jct.ac.il
Applied Artificial Intelligence, 24:847–862, 2010
Copyright # 2010 Taylor & Francis Group, LLC
ISSN: 0883-9514 print=1087-6545 online
DOI: 10.1080/08839514.2010.514197
D
o
w
n
l
o
a
d
e
d
 
B
y
:
 
[
H
E
A
L
-
L
i
n
k
 
C
o
n
s
o
r
t
i
u
m
]
 
A
t
:
 
0
8
:
3
5
 
7
 
O
c
t
o
b
e
r
 
2
0
1
0
function for any valid input object after learning from a set of training
examples (input and output pairs).
TC is applied in many tasks, such as clustering, filtering, indexing,
information extraction, text mining, and word sense disambiguation
(Knight 1999). Current-day TC presents challenges because of the large
number of features present in the text documents, their dependencies,
and the large number of training documents. The main difficulty with
having a large number of features is determining if part of them are redun-
dant so they can be ignored.
Topic classification is perhaps the most common TC task. Its meaning is
to classify documents according to one or more predefined topics based on
the documents’ content. Another type of classification is stylistic classi-
fication—classifying documents according to their author’s style.
TC according to categories is usually based on content words and
collocations (collocation is the relationship between two words or groups
of words that often go together and form a common expression). For
example, religious texts are different from medical texts by their content
words and collocations. In contrast, stylistic classification is usually based
on linguistic features as in Argamon et al. (1998) on news stories and
Koppel et al. (2002) on gender.
Hebrew and Aramaic present several interesting problems for stylistic
TC. First, Hebrew-Aramaic documents contain words from both languages.
Second, Hebrew and Aramaic are richer than English in their morphologi-
cal forms. Third, such texts include a relatively high rate of abbreviations
(HaCohen-Kerner et al. 2004).
Automatic classification of Hebrew-Aramaic texts has been relatively
little studied. A system for stylistic Classification of Hebrew-Aramaic Texts
(CHAT) was presented by Koppel et al. (2004, 2006) and Mughaz
(2003). CHAT presents applications of several TC tasks to Hebrew-Aramaic
texts. It uses as features only single words, prefixes, and suffixes. It uses sim-
ple ML methods such as Winnow and Perceptron. Its data sets contain only
a few hundred documents. CHAT does not investigate the various tasks
proposed at the end of the introduction section. Classification of Biblical
documents without use of any ML method has been performed by Radai
(1978, 1979, 1982).
In a previous conference paper (HaCohen-Kerner et al. 2006), we
addressed the identification of historical period and ethnic origin of docu-
ments using various stylistic feature sets. The application domain, the classi-
fication tasks, and the stylistic feature sets are the same as in the research
described in this paper. Nevertheless, in the conference paper only the
Support Vector Machine (SVM) ML method was applied using the fivefold
cross-validation.
848 Y. HaCohen-Kerner et al.
D
o
w
n
l
o
a
d
e
d
 
B
y
:
 
[
H
E
A
L
-
L
i
n
k
 
C
o
n
s
o
r
t
i
u
m
]
 
A
t
:
 
0
8
:
3
5
 
7
 
O
c
t
o
b
e
r
 
2
0
1
0
In other research, HaCohen-Kerner et al. (2010a) addressed the identi-
fication of historical period and ethnic origin of documents using various
stylistic feature sets and=or name-based feature sets. The research described
there used a simple hill-climbing process that attempted to select the best
combination of feature sets. In addition, it was tested on a smaller data set
and used only the SVM ML method.
Identification of the ethnic group of the authors based on words’ roots
using an Artificial Neural Network ML method was performed by
HaCohen-Kerner et al. (2007). The best Artificial Neural Network model
led to accuracy classification result of 89.7%.
The corpus analyzed in this research includes responsa (answers given
by Jewish legal scholars in response to questions addressed to them). These
responsa cover a comprehensive variety of domains, such as civil laws,
holiday and Sabbath (Saturday) rules, kosher foods and drinks, financial
issues, and various Jewish customs. Each answer is based on both ancient
Jewish writings and answers given by previous rabbinical authorities over
the years. As well, arguments contradicting the author’s answer are also
included in the responsa. The author gives plausible explanations to
overcome such arguments.
The research’s goal is to examine the following classification tasks: his-
torical period when the responsa were written and=or Jewish ethnic origin
of the authors (Sephardim [Mediterranean] or Ashkenazim [eastern
or central Europe]). These classification tasks are important from the
Orthodox Jewish viewpoint because (1) according to Jewish ruling, a great-
er importance and impact is given to older responsa and (2) the customs of
each ethnic group mainly depend on responsa written by authors who
belong to the same group.
Application of these classification tasks can help: (1) to find which com-
bination of feature sets are most suitable for such classification tasks, (2) to
discover results useful to scholars in the humanities (i.e., identifying the dif-
ferences in writing style, culture, and customs between writers who belong
to different ethnic origin and=or historical period), and (3) to help identify
undated documents or unknown ethnic origin for documents.
To the best of our knowledge, ours is the first such work that attempts
to identify the ethnic origin of documents’ authors by using stylistic feature
sets applied by various ML methods. For each classification task our model
chooses the best combination of sets of stylistic features.
This article is organized as follows. Section 2 provides background
regarding Hebrew and Aramaic. Section 3 discusses feature sets that were
used in related work for classification. Section 4 introduces the proposed
model. Section 5 shows the experimental results and analyzes them.
Section 6 summarizes, concludes, and proposes future research directions.
Stylistic Feature Sets as Classifiers of Documents 849
D
o
w
n
l
o
a
d
e
d
 
B
y
:
 
[
H
E
A
L
-
L
i
n
k
 
C
o
n
s
o
r
t
i
u
m
]
 
A
t
:
 
0
8
:
3
5
 
7
 
O
c
t
o
b
e
r
 
2
0
1
0
THE HEBREW LANGUAGE
English is richer in its vocabulary than Hebrew. The English dictionary
contains about 40,000 roots, whereas the Hebrew dictionary contains about
3,500 roots. There are about 150,000 lexical entries in the English dictionary,
whereas the Hebrew dictionary contains about 35,000 lexical entries (Choueka
et al. 2000). Nevertheless, Hebrew is richer in its morphological forms. Accord-
ing to linguistic estimates, the Hebrew language has about 70,000,000 valid
(inflected) forms, whereas English has only about 1,000,000 (Choueka et al.
2000). In Hebrew there are up to about 7000 inflections for a single root,
whereas in English there are only a few inflections. Wintner (2004) provides
a detailed summary regarding Hebrew computational linguistics.
Hebrew in general is rich in its vocabulary of abbreviations. The num-
ber of Hebrew abbreviations is about 17,000 (Ashkenazi and Jarden 1994),
not including professional abbreviations, which is relatively high compared
with 40,000 lexical entries in the Hebrew language. About 35% of the
abbreviations are ambiguous. That is to say, about 6000 abbreviations have
at least two possible extensions for each abbreviation.
There are large differences between Modern Hebrew and some pre-
vious layers of Hebrew (e.g., Rabbinic Hebrew and unique Hebrew words
used by Rishonim
y
and Acharonim
z
). Modern responsa, which are discussed
in this research, differ from most Modern Hebrew documents, especially
in their words and quantitative terms. For instance, the Hebrew used in
responsa contains much more special religious Hebrew words and Aramaic
words. Also, Jewish Law articles written in Hebrew-Aramaic include a higher
rate of abbreviations in general and ambiguous abbreviations in particular.
About 20% of all words in the documents are abbreviations, whereas about
40% of the abbreviations are ambiguous. Comprehensive research concern-
ing disambiguation of ambiguous abbreviations has been performed by
HaCohen-Kerner et al. (2004, 2008a, 2008b, 2010b).
The abbreviations included in new rabbinic works such as the discussed
documents (19th and 20th centuries) are usually written by the authors them-
selves. This is in contrast to abbreviations that appeared in ancient documents,
which are usually inserted by the original printers, later printers, or editors.
CLASSIFICATION FEATURES USED IN RELATED WORK
As mentioned in the Introduction, classification by topic is typically
based on content-dependent words, whereas classification by author style
Rabbinic Hebrew is the language that was spoken and written by rabbinical authorities who lived in
the Mishna era (70–500 CE) and the Talmud period (200–500 CE).
y
Rishonim: post-Talmud leading rabbis who lived approximately during the 11th to 15th centuries.
z
Acharonim: leading rabbis living approximately from the 16th century to the present.
850 Y. HaCohen-Kerner et al.
D
o
w
n
l
o
a
d
e
d
 
B
y
:
 
[
H
E
A
L
-
L
i
n
k
 
C
o
n
s
o
r
t
i
u
m
]
 
A
t
:
 
0
8
:
3
5
 
7
 
O
c
t
o
b
e
r
 
2
0
1
0
uses content-independent features. Prominent applications of stylistic text
analysis are works on authorship attribution (e.g., Mosteller and Wallace
1964; Baayen et al. 1996; Argamon et al. 1998; Stamatatos et al. 2001). de
Vel et al. (2001) discriminate between authors for the case of both aggregated
e-mail topics as well as across different e-mail topics. Peng et al. (2003) present
a method based on character-leveln-gram language models for language inde-
pendent and task independent TC learning. They present experimental
results on several languages—Greek, English, Chinese, and Japanese—in
several TC problems—language identification, authorship attribution, text
genre classification, and topic detection. Stamatatos (2008) presents a series
of authorship identification experiments for English and Arabic based on
short samples of the training texts that artificially increase the training size
of a class. Schler et al. (2006) use both style-related and content-related
features to find differences in writing style and content between male and
female bloggers as well as among authors of different ages.
Various kinds of content-independent features have been proposed
during the years for stylometric categorization models. Quantitative fea-
tures (e.g., word and sentence length and punctuation signs) were pro-
posed by Yule (1938). Function words (e.g., are, at, is, of, on, then, and
will) and lexical features (e.g., stopwords and summarization words) were
proposed by Mosteller and Wallace (1964). Syntactic features (e.g., fre-
quencies and distribution of parts of speech tags) were proposed by Baayen
et al. (1996) and Stamatatos et al. (2001).
Various kinds of stylistic features have been applied for automatic classi-
fication of documents. Karlgren and Cutting (1994) developed 20 features
that belong to the three following sets: part of speech counts (e.g., noun
and adverb), lexical counts (e.g., that and we), and textual=quantitative
counts (e.g., characters per word and words per sentence). A set of ortho-
graphic features (abbreviations, acronyms, various spellings of the same
words) has been proposed by Friedman (1996). Stamatatos et al. (2001)
applied syntactic features (e.g., frequencies and distribution of parts of
speech tags, such as noun, verb, adjective, adverb; basic syntactic sequences,
such as noun–adjective and subject–verb relations; and active and passive
sentences).
Lim et al. (2005) used five different sets of features for automatic genre
classification of web documents. Two of them were web-oriented: URL tags
(e.g., depth of URL, document type, and domain area) and HTML tags
(frequencies of various types of links). The other three sets were token
information (e.g., average number of characters per word and average
number of words per sentence), lexical information (e.g., frequency of con-
tent words, frequency of function words, and frequency of punctuation
marks), and structural information (e.g., number of declarative sentences
and number of question sentences).
Stylistic Feature Sets as Classifiers of Documents 851
D
o
w
n
l
o
a
d
e
d
 
B
y
:
 
[
H
E
A
L
-
L
i
n
k
 
C
o
n
s
o
r
t
i
u
m
]
 
A
t
:
 
0
8
:
3
5
 
7
 
O
c
t
o
b
e
r
 
2
0
1
0
Hota et al. (2006) use three different feature sets for determining the
gender of Shakespeare’s literary characters. They used syntactic (part of
speech), lexical, and lemma features. Lemma features include lemma uni-
grams and lemma trigrams. Lemma unigrams are meaningful clusters of
words that identify ‘‘typical’’ female and male characters in Shakespeare.
Lemma trigrams are meaningful clusters of patterns that each includes
three words (e.g., I see you). Such patterns help to identify ‘‘typical’’ female
and male concerns in Shakespeare.
Diederich et al. (2003) present a study on authorship attribution with
SVMs. Their feature set consists of full word forms (i.e., Bag-Of-Words)
and tag words, a combination of function words and grammatical infor-
mation. Argamon et al. (2007) apply to various stylistic TC tasks a new type
of lexical feature based on taxonomies of various semantic functions of
different lexical items.
THE MODEL
The current research is an extension to the conference paper
presented in HaCohen-Kerner et al. (2006) as follows:
1. The background and the related works in various subdomains are
explored significantly.
2. We apply four ML methods instead of applying only the SVM ML
method.
3. We use the 10-fold cross-validation instead of the 5-fold cross-validation.
The 10-fold cross-validation tends to give a less-biased estimate of true
generalization error. It contains an average of 10 instead of 5 experi-
ments, and learning in each subexperiment is performed for 90% of
the documents instead of 80%.
4. Measures such as Precision=Recall=F_measure are supplied.
5. More analyses and conclusions are provided.
This research is different from the research presented in HaCohen-
Kerner et al. (2010a) in the following ways: (1) the current research
is tested on a larger data set; (2) we apply four ML methods instead of
applying only the SVM ML method; and (3) instead of using a simple
hill-climbing process that attempts to select the best combination of feature
sets, we test all the possible combinations of feature sets and choose the
best one.
As mentioned in the Introduction, Hebrew-Aramaic documents present
interesting research issues for stylistic classification. Therefore, methods
already used in TC require adaptation to handle these problems. First,
852 Y. HaCohen-Kerner et al.
D
o
w
n
l
o
a
d
e
d
 
B
y
:
 
[
H
E
A
L
-
L
i
n
k
 
C
o
n
s
o
r
t
i
u
m
]
 
A
t
:
 
0
8
:
3
5
 
7
 
O
c
t
o
b
e
r
 
2
0
1
0
the definition of suitable feature sets is required. Second, the proper
combination of feature sets is needed to be investigated for a variety of
classification experiments.
The available Hebrew taggers are appropriate for Modern Hebrew
texts, which are different from the discussed rabbinic Hebrew-Aramaic
texts. We also did not deal with special spelling issues such as plene=
defective spelling and matres lectionis because we did not have the needed
resources for them.
Feature Sets for Classification of Hebrew-Aramaic Texts
Forty-two stylistic features were defined. Many features were normalized
by the number of the word tokens in a document. Features regarding sen-
tences have two versions (relating or not relating to a comma as an end of a
sentence). These features are divided into the following six sets:
1. Lexical features: normalized frequencies of 958 common religious words
(e.g., bible, Jewish, responsa, rabbi), 533 stopwords (e.g., the, a, an, at,
by, in, of, on, by, under), and 307 summarization words (e.g., finding(s),
conclusion(s), to conclude, summary(ies), to sum up, results(s), impor-
tant).
2. Orthographic features: normalized number of acronyms in a sentence and
normalized number of abbreviations in a sentence. In the current
research the investigated documents are considered as modern, and
the acronyms and abbreviations were mostly written by the authors
themselves.x
3. Topographic features: normalized frequency of each word from the follow-
ing groups: the first n (10, 20) word tokens, the last n word tokens, the
first n=2 word tokens, and the last n=2 word tokens; the word tokens in
the first n (2, 4) sentences, the word tokens in the last n sentences, the
word tokens in the first n=2 sentences, and the word tokens in the last n=
2 sentences; the word tokens in the first n (1, 2) paragraphs, the word
tokens in the last n paragraphs, the word tokens in the first n=2
paragraphs, and the word tokens in the last n=2 paragraphs.
4. Quantitative features: average number of characters in a word=sentence=
paragraph=document, average number of word tokens in a sentence=
paragraph=document, average number of sentences in a paragraph=
document, average number of paragraphs in a document, and average
number of punctuation signs (e.g., periods, commas, semicolons,
xIt is important to realize that historically, acronyms and abbreviations depended on the typesetter.
If he agreed to publish a book for a given sum, and it was not high enough to satisfy him fully, he would
abbreviate wherever he could, even mangling readability, in order to reduce the number of pages.
Stylistic Feature Sets as Classifiers of Documents 853
D
o
w
n
l
o
a
d
e
d
 
B
y
:
 
[
H
E
A
L
-
L
i
n
k
 
C
o
n
s
o
r
t
i
u
m
]
 
A
t
:
 
0
8
:
3
5
 
7
 
O
c
t
o
b
e
r
 
2
0
1
0
exclamations points, etc.) in a document. It is important to mention that
punctuation signs make sense only for such texts that have modern
punctuation. Premodern texts had few punctuation signs, and these
were used sparsely. Such texts were possibly edited by the original prin-
ters, later printers, or editors with the addition of punctuation. However,
the texts investigated in this research are considered modern, and the
punctuation signs were written by the authors themselves
5. Function features: normalized frequencies of sentences appeared in brack-
ets and normalized frequency of 11 nonambiguous Hebrew pronouns
(e.g., we, he, she, I, you, they).
6. Vocabulary’s richness: size of author’s vocabulary in word tokens.
The lexical, orthographic, and pronoun features are domain depen-
dent and language dependent. They have been especially fitted to the
Hebrew and Aramaic languages. Dictionaries containing common religious
words, stopwords, summarization words, abbreviations, and acronyms have
been built. Moreover, they have been combined with various kinds of
prepositions, belonging, terminal letters, and so on. The quantitative,
function, and vocabulary’s richness features are domain independent and
language independent.
Finding the Best Combination of Sets
The accuracy that was measured in all experiments is the fraction of the
number of documents that are correctly classified to the total number of docu-
ments to be classified. To find the best combination of sets for any particular
classification task, one should try all the possible combinations of sets. A pro-
perty of the binomial coefficients is that the total number of distinct k-subsets
on a set of n elements (i.e., the number of possible subsets) is 2n, where a
k-subset is a subset of a set on n elements containing exactly k elements.
In this research there are six feature sets. That is, there are 26¼ 64
combinations of complete feature sets (including the empty set). There-
fore, for each ML method we try all 63 combinations of complete feature
sets, and the combination with the highest classification rate is selected.
Investigating whether the accuracy can be improved if the feature selection
process is applied to every single feature and not to the entire feature set is
left for future research.
Machine Learning Methods
A wide range of successful ML algorithms has been applied to TC, for
example, SVM (Joachims 1998, 2002; Dı́az et al. 2004), Artificial Neural
854 Y. HaCohen-Kerner et al.
D
o
w
n
l
o
a
d
e
d
 
B
y
:
 
[
H
E
A
L
-
L
i
n
k
 
C
o
n
s
o
r
t
i
u
m
]
 
A
t
:
 
0
8
:
3
5
 
7
 
O
c
t
o
b
e
r
 
2
0
1
0
Networks (Merkl and Rauber 2000), C4.5 (Gabrilovich and Markovitch
2004), logistic regression (Mathur and Chakrabarti 2006), naı̈ve Bayes
(Meretakis and Wuthrich 1999; Kim et al. 2000), and Winnow (Mughaz
2003; Koppel et al. 2004; 2006). A comprehensive summary on ML applied
to TC can be found in Sebastiani (2002).
The availability of such powerful ML methods enables researchers to
deal with many features. For instance, Madigan et al. (2005) use all the
words that appear at least twice in the corpus, Stamatatos (2006) uses the
1000 most frequent words, and Koppel et al. (2007) use the 250 most
frequent words.
In our model we apply four well-known supervised ML methods: naı̈ve
Bayes, logistic regression (Hosmer and Stanley 2000), J48 (an improved
variant of the C4.5 decision tree induction [Quinlan 1993]), and sequential
minimal optimization (SMO) (Platt 1999) (a variant of the SVM method
[Cortes and Vapnik 1995; Vapnik 1995]). These ML methods have been
applied with default values and no feature normalization using Weka

(Witten and Frank 1999). Model tuning is left for future research.
Main Flow of the Process
The main flow of the process is as follows:
. Cleaning all the documents over the entire corpus (We delete various types
of additions in brackets and redundant spaces that are not part of the
original text. Document IDs such as headlines that include the name of
the book=responsa were also deleted.)
. For each experiment (a certain classification task, e.g., ethnicity, time,
ethnicity & time)
. For each ML method (naı̈ve Bayes, logistic regression, J48, and SMO)
. For each combination of feature sets (out of all 63 possible
combinations)
. Apply a 10-fold cross-validation to learn and test the classification
performance
. Compute the accuracy rate (the fraction of the number of docu-
ments correctly classified to the total number of possible documents
to be classified) of the current combination
. Output the best combination of feature sets and its accuracy rate for
the current experiment and ML method.

Weka is a collection of machine learning algorithms programmed in Java for data mining tasks,
such as classification, regression, clustering, association rules, and visualization.
Stylistic Feature Sets as Classifiers of Documents 855
D
o
w
n
l
o
a
d
e
d
 
B
y
:
 
[
H
E
A
L
-
L
i
n
k
 
C
o
n
s
o
r
t
i
u
m
]
 
A
t
:
 
0
8
:
3
5
 
7
 
O
c
t
o
b
e
r
 
2
0
1
0
The 10-fold cross-validation technique consists of randomly dividing the
data into 10 equally sized subgroups and performing 10 different experi-
ments. In each experiment a single group is considered the test set and
all other 9 groups together with their original classes are considered as
the training set. The average results of these 10 experiments are reported.
EXPERIMENTAL RESULTS
The examined data set is larger than the data set used in HaCohen-
Kerner et al. (2010a). It includes 12,020 responsa downloaded from the
Responsa project (http://www.biu.ac.il/ICJI/Responsa, version 14). These
responsa were composed by 48 Jewish rabbinic scholars who lived in the
19th and 20th centuries. On average, there are 250 documents for each scho-
lar. These scholars belong to the two major Jewish ethnic groups (Sephardim
or Ashkenazim). The total number of word tokens and word types in the data
set are 18,984,065 and 332,843, respectively. A document contains on
average 1579 word types. An author wrote on average 395,501 word types.
The data set can be classified into three classifications: (1) ethnicity
(i.e., ethnic Jewish group [Sephardim or Ashkenazim]), (2) time (i.e.,
the historical period when they were written [in the 19th century or in
the 20th century]), and (3) ethnicity and time. The third experiment deals
with classification to the Cartesian product of the ethnic and the time sets.
That is, there are four possible categories: Old Sephardim, Old Ashkena-
zim, New Sephardim, and New Ashkenazim. Table 1 presents general
statistics concerning the documents included in these four categories.
The responsa presented in Table 1 were collected in such a way that for
each classification the data set can be divided into equally sized suitable
sets. That is, for each experiment the number of documents in the positive
and in the negative sets is the same. A few interesting findings can be
concluded from Table 1. First, new authors wrote longer documents than
old authors. Second, Sephardi authors wrote longer documents than
TABLE 1 General Statistics Concerning the Documents Included in the Four Categories
Sephardim Ashkenazim
Feature
Old
Sephardim
New
Sephardim
Old
Ashkenazim
New
Ashkenazim
No. of documents 3005 3005 3005 3005
No. of authors 16 8 10 14
No. of word tokens 3,589,050 6,406,141 4,035,668 4,953,206
No. of average tokens
per document
1194.3 2131.8 1342.9 1648.3
No. of average tokens
per author
224,315.6 800,767.6 403,566.8 353,800.4
856 Y. HaCohen-Kerner et al.
D
o
w
n
l
o
a
d
e
d
 
B
y
:
 
[
H
E
A
L
-
L
i
n
k
 
C
o
n
s
o
r
t
i
u
m
]
 
A
t
:
 
0
8
:
3
5
 
7
 
O
c
t
o
b
e
r
 
2
0
1
0
Ashkenazi authors. The main reason for the first finding is quite simple.
It is common among rabbinic scholars in their responsa to refer to many
previous rabbinic scholars that address the same question or part of it.
New authors have more rabbinic opinions and scholars to which to refer.
The reason for the second finding is that Sephardi authors relate much
more to books and articles written by other rabbinic scholars.
Table 2 presents the main accuracy results of all three experiments.
The accuracy that was measured in all experiments is the fraction of the
number of documents correctly classified to the total number of possible
documents to be classified.
The most simple baseline classification method is the random function.
Therefore, in the first=second=third experiments (which contain 2=2=4
categories) the baseline methods achieve 50%, 50% and 25.5%, respectively.
Rows 1, 2, and 3 in Table 2 present the results of the classification for
ethnic, time, and both ethnic and time, respectively. Row 1 describes the
classification according to the Jewish ethnic groups: Sephardim (i.e.,
Sephardi Jews) or Ashkenazim (i.e., Ashkenazi Jews). Row 2 describes the
classification according to the two historical periods when the responsa
were written: Old (19th century) or New (20th century). Row 3 describes
the classification for the four possible categories: Old Sephardim, Old
Ashkenazim, New Sephardim, and New Ashkenazim.
TABLE 2 Classification Accuracy Results of the Data Set
ML method
Classification Naı̈ve Bayes
Logistic
regression J48 (C4.5) SMO (SVM)
Ethnic
Best feature set alone {4} {4} {4} {4}
Result, % 80.39 98.9 98.96 98.13
Best comb. of feature sets {1,4,5,6} {1,3,4,5,6} {4,5} {1,3,4,5,6}
Result, % 88.70 99.57 99.20 98.89
Precision=Recall=F_measure 0.85=0.94=0.89 1=1=1 0.99=0.99=0.99 1=0.98=0.99
Time
Best feature set alone {1} {4} {4} {4}
Result, % 79.43 98.93 99.26 96.52
Best comb. of feature sets {1,2,4,5} {1,2,4,5} {4} {1,2,3,4,5,6}
Result, % 85.96 99.68 99.26 99.09
Precision=Recall=F_measure 0.89=0.82=0.85 1=1=1 0.99=0.99=0.99 1=0.98=0.99
Ethnic and time
Best feature set alone {4} {4} {4} {4}
Result, % 67.97 96.08 97.97 93.63
Best comb. of feature sets {1,2,4,5} {1,3,4,5,6} {4} {1,2,3,4,5,6}
Result, % 79.58 98.31 97.97 94.83
Precision=Recall=F_measure 0.78=0.97=0.86 1=1=1 1=0.99=0.99 1=0.99=0.99
The best results for best feature set alone and best combination of feature sets are bolded.
Stylistic Feature Sets as Classifiers of Documents 857
D
o
w
n
l
o
a
d
e
d
 
B
y
:
 
[
H
E
A
L
-
L
i
n
k
 
C
o
n
s
o
r
t
i
u
m
]
 
A
t
:
 
0
8
:
3
5
 
7
 
O
c
t
o
b
e
r
 
2
0
1
0
With regard to macro-averaging, precision and recall are the same
either way. With regard to F_measure, we take the final recall and precision
(the average over the folds) and use them to compute F_measure (rather
than computing F_measure per fold and averaging the values).
Various conclusions can be made from Table 2:
1. The classification accuracy results were highly successful (99.57%,
99.68%, and 98.31%). These results are much better than the results
of the simple baseline classification method (the random function)
and are also better the results achieved by the best set alone—the
quantitative set {4} (98.96%, 99.26%, and 97.97%).
2. The achieved results (99.57%, 99.68%, and 98.31%) are also better
than the matching results (98.67%, 98.95%, and 92.81%) reported by
HaCohen-Kerner et al. (2010a, 2006).
3. Logistic regression was found to be the best ML method in all tasks.
According to the results, the ML methods are ranked in the following
descending order: logistic regression, J48, SMO and naı̈ve Bayes.
4. J48 was found as the best ML method in all tasks while using the best
feature set ({4}, the quantitative set) alone.
5. The quantitative set {4} was superior to all other sets for all four ML
methods, and it was enough for an excellent classification. This find-
ing means that a combination of quantitative features (e.g., average
number of characters in a word=sentence and average number of
words in a sentence=paragraph) is enough for classification for eth-
nic and=or time, at least for the kind of data set with which we
worked.
6. The more complex the classification task (both ethnic and time), the
lower the classification results achieved.
7. Only in the last task, the most complex one (both ethnic and time),
is there a relatively meaningful contribution (2.23% to 11.61%) of
other sets to the quantitative set in the context of all four ML
methods.
8. In addition to the quantitative set, the lexical {1} and function {5}
sets were always part of the best combination of sets for the logistic
regression method. This result means that the combination of lexical
features (e.g., normalized frequencies of stopwords and summarization
words) and function features (normalized frequencies of sentences
appeared in brackets and normalized frequency of 11 pronouns) is also
effective for classification for ethnic and=or time.
9. The orthographic {2}, topographic {3}, and the vocabulary richness {6}
sets were less effective.
10. The Precision=Recall=F_measure results were almost optimal and also
indicate that all three classification tasks were highly successful.
858 Y. HaCohen-Kerner et al.
D
o
w
n
l
o
a
d
e
d
 
B
y
:
 
[
H
E
A
L
-
L
i
n
k
 
C
o
n
s
o
r
t
i
u
m
]
 
A
t
:
 
0
8
:
3
5
 
7
 
O
c
t
o
b
e
r
 
2
0
1
0
The relatively small error rates found in these experiments have been
achieved although the following facts. First, some Sephardi and Ashkenazi
rabbis were active in modern Israel, and their articles were influenced by
the prevalent nonethnic Hebrew writing. Second, some rabbis were active
both in the 19th and 20th centuries.
SUMMARY, CONCLUSIONS, AND FUTURE WORK
We investigated the identification of the historical period of documents
and=or ethnic origin of their authors using six stylistic feature sets applied
by four common ML methods. The application domain is Jewish Law
articles written in Hebrew-Aramaic.
The results of all three classification tasks were highly successful and
better than those achieved in previous research. The quantitative feature
set was superior to all other sets, and in all experiments it was enough
for an excellent classification. Logistic regression was found as the best
ML method in all the tasks. J48 was found as the best ML method in
all the tasks while using the best feature set alone. The lexical and func-
tion sets are always part of the best combination of sets for the logistic
regression method, whereas the orthographic, topographic, and vocabu-
lary richness sets were less effective. These six feature sets (especially the
quantitative and the function features, which are domain independent
and language independent) might be applied to similar classification
tasks for other languages and can therefore be useful to the TC
community at large.
Because the quantitative feature set was found the most predictive for
all three tasks, this warrants an investigation of each of the features in this
set (both individually and collectively, as subsets) to see whether an even
simpler feature set can yield good results for the classification tasks. The
most influential features can be presented using methods (e.g., feature
weight and InfoGain). It will be interesting to compare our results to the
results of the same classification tasks based on only language-dependent
and domain-dependent feature sets (e.g., syntactic and morphological
features).
Another relevant research direction would be to apply to the data set a
cotraining-like algorithm (Blum and Mitchell 1998) based views created
from various types of features. This may both increase accuracy and reduce
the amount of documents needed for training. Other general research
directions are as follows: (1) investigating whether the accuracy can be
improved if the feature selection process is applied to every single feature
and (2) investigating which feature sets are appropriate for which classi-
fication tasks.
Stylistic Feature Sets as Classifiers of Documents 859
D
o
w
n
l
o
a
d
e
d
 
B
y
:
 
[
H
E
A
L
-
L
i
n
k
 
C
o
n
s
o
r
t
i
u
m
]
 
A
t
:
 
0
8
:
3
5
 
7
 
O
c
t
o
b
e
r
 
2
0
1
0
REFERENCES
Argamon-Engelson, S., M. Koppel, and G. Avneri. 1998. Style-based text categorization: What newspaper
am I reading? Proc. of the AAAI Workshop on Learning for Text Categorization, Madison, WI,
July 26–27.
Argamon, S., C. Whitelaw, P. Chase, S. R. Hota, N. Garg, and S. Levitan. 2007. Stylistic text classification
using functional lexical features: Research articles. Journal of the American Society for Information
Science and Technology 58(6):802–822.
Ashkenazi, S., and D. Jarden. 1994. Ozar Rashe Tevot: Thesaurus of Hebrew Abbreviations (in Hebrew). Kiryat
Sefere LTD., Jerusalem.
Baayen, R. H., H. Van Halteren, and F. Tweedie. 1996. Outside the cave of shadows: Using syntactic
annotation to enhance authorship attribution. Literary and Linguistic Computing 11(3):121–131.
Blum, A., and T. Mitchell. 1998. Combining labeled and unlabeled data with co-training. Proceedings of
the Eleventh Annual Conference on Computational Learning Theory, Madison, WI, July 24–26.
Choueka, Y., E. S. Conley, and I. Dagan. 2000. A comprehensive bilingual word alignment system:
Application to disparate languages—Hebrew, English. In: Parallel Text Processing, ed. J. Veronis,
69–96. Kluwer Academic Publishers Dordrecht, Netherlands.
Cortes, C., and V. Vapnik. 1995. Support-vector networks. Machine Learning 20:273–297.
de Vel, O., A. Anderson, M. Corney, and G. M. Mohay. 2001. Mining e-mail content for author
identification forensics. SIGMOD Record 30(4):55–64.
Dı́az, I., J. Ranilla, E. Montañés, J. Fernández, and E. F. Combarro. 2004. Improving performance of text
categorization by combining filtering and supportvector machines. JASIST 55(7):579–592.
Diederich, J., J. Kindermann, E. Leopold, and G. Paass. 2003. Authorship attribution with Support
Vector Machines. Applied Intelligence 19(1–2):109–123.
Friedman, S. 1996. The manuscripts of the Babylonian Talmud: A typology based upon orthographic
and linguistic features. In: Studies in Hebrew and Jewish Languages Presented to Shelomo Morag (in
Hebrew), ed. M. Bar-Asher, 163–190. Jerusalem, The Bialik Institute.
Gabrilovich, E., and S. Markovitch. 2004. Text categorization with many redundant features: Using
aggressive feature selection to make SVMs competitive with C4.5. Proceedings of the Twenty-first
International Conference (ICML 2004), Banff, Alberta, Canada, July 4–8.
HaCohen-Kerner, Y., A. Kass, and A. Peretz. 2004. Baseline methods for automatic disambiguation of
abbreviations in Jewish law documents. Proceedings of Advances in Natural Language Processing,
4th International Conference, EsTAL 2004, Alicante, Spain, October 20–22.
HaCohen-Kerner, Y., H. Beck, E. Yehudai, and D. Mughaz. 2006. Identifying Historical Period and
Ethnic Origin of Documents Using Stylistic Feature Sets. Proceedings of the Ninth International
Conference on Discovery Science, Barcelona, Spain, October 7–10.
HaCohen-Kerner, Y., Z. Boger, H. Beck, and E. Yehudai. 2007. Identification of the Ethnic Group of the
Writers Using Stems. Proceedings of the ISCA 20th International Conference on Computer Appli-
cations in Industry and Engineering, CAINE 2007, San Francisco, CA, November 7–9.
HaCohen-Kerner, Y., A. Kass, and A. Peretz. 2008. Combined One Sense Disambiguation of Abbrevia-
tions. Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics,
Columbus, OH, June 15–20.
HaCohen-Kerner, Y., A. Kass, and A. Peretz. 2008. Abbreviation disambiguation: Experiments with
various variants of the one sense per discourse hypothesis documents. Proceedings of the 13th
International Conference on Applications of Natural Language to Information Systems, NLDB
2008, London, UK, June 24–27.
HaCohen-Kerner, Y., H. Beck, E. Yehudai, M. Rosenstein, and D. Mughaz. 2010. Cuisine: Classification
using stylistic feature sets and=or name-based feature sets. Journal of the American Society for Infor-
mation Science and Technology 61(8):1644–1657.
HaCohen-Kerner, Y., A. Kass, and A. Peretz. 2010. HAADS: A Hebrew Aramaic abbreviation
disambiguation system. Journal of the American Society for Information Science and Technology,
61(9):1923–1932.
Holmes, D. I. 1998. The evolution of stylometry in humanities scholarship. Literary and Linguistic
Computing 13:111–117.
Hosmer, D. W., and L. Stanley. 2000. Applied Logistic Regression, 2nd ed. New York: Wiley.
860 Y. HaCohen-Kerner et al.
D
o
w
n
l
o
a
d
e
d
 
B
y
:
 
[
H
E
A
L
-
L
i
n
k
 
C
o
n
s
o
r
t
i
u
m
]
 
A
t
:
 
0
8
:
3
5
 
7
 
O
c
t
o
b
e
r
 
2
0
1
0
Hota, S., S. Argamon, and R. Chung. 2006. Gender in Shakespeare: Automatic stylistics gender classi-
fication using syntactic, lexical, and lemma features. Digital Humanities and Computer Science.
Joachims, T. 1998. Text categorization with support vector machines: Learning with many relevant
features, Proceedings of the 10th European Conference on Machine Learning, Chemnitz,
Germany, April 21–23.
Joachims, T. 2002. Learning to Classify Text using Support Vector Machines. Kluwer.
Karlgren, J., and D. Cutting. 1994. Recognizing text genres with simple metrics using discriminant
analysis. Proceedings of the 15th International Conference on Computational Linguistics, Kyoto,
Japan, August 5–9.
Kim, Y.-H., S.-Y. Hahn, and B.-T. Zhang. 2000. Text filtering by boosting naive Bayes classifiers. Proceed-
ings of the 23rd Annual International ACM SIGIR Conference on Research and Development in
Information Retrieval, Athens, Greece, July 24–28.
Knight, K. 1999. Mining online text. Commun. ACM 42(11):58–61.
Koppel, M., S. Argamon, and A. R. Shimony. 2002. Automatically categorizing written texts by author
gender. Literary Linguistic Computing 17(4):401–412.
Koppel, M., D. Mughaz, and J. Schler. 2004. Text categorization for authorship verification. Proceedings
of the Eighth International Symposium on Artificial Intelligence and Mathematics, January Fort
Lauderdale, FL, 4–6.
Koppel, M., D. Mughaz, and N. Akiva. 2006. New methods for attribution of rabbinic literature. Hebrew
Linguistics: A Journal for Hebrew Descriptive, Computational, Applied Linguistics 57:v–xviii.
Koppel, M., J. Schler, and E. Bonchek-Dokow. 2007. Measuring differentiability: Unmasking pseudony-
mous authors. Journal of Machine Learning Research 8:1261–1276.
Lim, C. S., K. J. Lee, and G-C. Kim. 2005. Multiple sets of features for automatic genre classification of
web documents. Inf. Process. Manage. 41(5):1263–1276.
Madigan, D., A. Genkin, D. Lewis, S. Argamon, D. Fradkin, and L. Ye. 2005. Author identification on the
large scale. Proceedings of the Joint CSNA ‘05=Interface Meeting, Washington University in
St Louis, June 8–12.
Mathur, A., and S. Chakrabarti. 2006. Accelerating Newton optimization for log-linear models through
feature redundancy. ICDM 2006:404–413.
Matthews, R. A. J., and T. V. N. Merriam. 1997. Distinguishing literary styles using neural networks. In:
Handbook of Neural Computation, eds. E. Fiesler, and R. Beale, IOP publishing and Oxford University
Press.
Meretakis, D., and B. Wuthrich. 1999. Extending naive Bayes classifiers using long itemsets. Proceedings
of the 15th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,
Paris, France, June 28–July 1.
Merkl, D., and A. Rauber. 2000. Document classification with unsupervised artificial neural networks.
In: Soft Computing in Information Retrieval: Techniques and Applications, eds. F. Crestani and G. Pasi,
102–121. Heidelberg: Physica Verlag, 50.
Mosteller, F., and D. L. Wallace. 1964. Inference and Disputed Authorship: The Federalist. Reading, MA:
Addison Wesley.
Mughaz, D. 2003. Classification of Hebrew texts according to style, M.Sc. Thesis (in Hebrew), Bar-Ilan
University, Ramat-Gan, Israel.
Ng, H. T., W. B. Goh, and K. L. Low. 1997. Feature Selection, Perceptron Learning, and a Usability Case
Study for Text Categorization. Proceedings of the 20th Annual International ACM SIGIR Confer-
ence on Research and Development in Information Retrieval, Philadelphia, PA, July 27–31.
Peng, F., D. Schuurmans, and S. Wang. 2003. Language and Task Independent Text Categorization with
Simple Language Models. Proceedings of the HLT-NAACL Human language technology and
NAACL (North American Chapter of the Association for Computational Linguistics), Edmonton,
Canada, May 27–June 1.
Platt, J. C. 1999. Fast training of support vector machines using sequential minimal optimization. In:
Advances in Kernel Methods—Support Vector Learning, eds. B. Scholkopf, C. Burges, and A. J. Smola,
185–208. Cambridge, MA: MIT Press.
Quinlan, J. R. 1993. C4.5: Programs for Machine Learning. Los Altos, CA: Morgan Kaufmann.
Radai, Y. 1978. Hamikra haMemuchshav: Hesegim bikoret umishalot (in Hebrew). Balshanut Ivrit
13:92–99.
Stylistic Feature Sets as Classifiers of Documents 861
D
o
w
n
l
o
a
d
e
d
 
B
y
:
 
[
H
E
A
L
-
L
i
n
k
 
C
o
n
s
o
r
t
i
u
m
]
 
A
t
:
 
0
8
:
3
5
 
7
 
O
c
t
o
b
e
r
 
2
0
1
0
Radai, Y. 1979. Od al hamikra haMemuchshav (in Hebrew). Balshanut Ivrit 15:58–59.
Radai, Y. 1982. Mikra uMachshev: Divrei idkun (in Hebrew). Balshanut Ivrit 47–52.
Schler, J., M. Koppel, S. Argamon, and J. Pennebaker. 2006. Effects of Age and Gender on Blogging. In
Proceedings of 2006 AAAI Spring Symposium on Computational Approaches for Analyzing
Weblogs, Stanford, CA, March 27–29.
Sebastiani, F. 2002. Machine learning in automated text categorization. ACM Computing Surveys
34(1):1–47.
Stamatatos, E., N. Fakotakis, and G. Kokkinakis. 2001. Computer-based authorship attribution without
lexical measures. Computers and the Humanities 35:193–214.
Stamatatos, E. 2006. Authorship attribution based on feature set subspacing ensembles. Int. J. Artificial
Intelligence Tools 15(5):823–838.
Stamatatos, E. 2008. Author identification: Using text sampling to handle the class imbalance problem.
Information Processing and Management 44(2):790–799.
Vapnik, V. N. 1995. The Nature of Statistical Learning Theory. New York: Springer-Verlag.
Wintner, S. 2004. Hebrew computational linguistics: Past and future. Artif. Intell. Rev. 21(2):113–138.
Witten, I. H., and E. Frank. 1999. Weka 3: Machine learning software in Java. Retreived from http://
www.cs.waikato.ac.nz/~ml/weka
Yule, U. 1938. On sentence length as a statistical characteristic of style in prose with application to two
cases of disputed authorship. Biometrika 30:363–390.
862 Y. HaCohen-Kerner et al.
D
o
w
n
l
o
a
d
e
d
 
B
y
:
 
[
H
E
A
L
-
L
i
n
k
 
C
o
n
s
o
r
t
i
u
m
]
 
A
t
:
 
0
8
:
3
5
 
7
 
O
c
t
o
b
e
r
 
2
0
1
0
