Intelligent Data Analysis 15 (2011) 251–276 251
DOI 10.3233/IDA-2010-0466
IOS Press
Exploring discrepancies in findings obtained
with the KDD Cup ’99 data set
Vegard Engen∗, Jonathan Vincent and Keith Phalp
Software Systems Research Centre, Bournemouth University, Fern Barrow, Talbot Campus, Poole, UK
Abstract. The KDD Cup ’99 data set has been widely used to evaluate intrusion detection prototypes, most based on machine
learning techniques, for nearly a decade. The data set served well in the KDD Cup ’99 competition to demonstrate that machine
learning can be useful in intrusion detection systems. However, there are discrepancies in the findings reported in the literature.
Further, some researchers have published criticisms of the data (and the DARPA data from which the KDD Cup ’99 data has
been derived), questioning the validity of results obtained with this data. Despite the criticisms, researchers continue to use the
data due to a lack of better publicly available alternatives. Hence, it is important to identify the value of the data set and the
findings from the extensive body of research based on it, which has largely been ignored by the existing critiques. This paper
reports on an empirical investigation, demonstrating the impact of several methodological differences in the publicly available
subsets, which uncovers several underlying causes of the discrepancy in the results reported in the literature. These findings
allow us to better interpret the current body of research, and inform recommendations for future use of the data set.
Keywords: Machine learning, intrusion detection, KDD Cup ’99 data set, methodology
1. Introduction
Despite the existence of security mechanisms such as cryptography and the use of protocols to control
communication between computers (and users), it is impossible to prevent all intrusions to computer
systems [31, pp. 251–252]. As a complementary security mechanism, intrusion detection refers to
the process of detecting behaviour of users that conflict with the intended use of the system, such as
committing fraud, breaking into the system to steal information, conducting an attack to bring the system
down, etc. Intrusion Detection Systems (IDSs) typically operate on one of two levels: on a host or a
network [20,39]. A host based IDS monitors the local behaviour on a machine, generally by analysing
system logs to determine inter alia access violations and system file modifications. Network based IDSs,
which are considered in this paper, analyse network traffic [48, p. 27].
There are two main approaches to intrusion detection: misuse detection and anomaly detection [26,31,
41,51]. These terms are also known as knowledge-based and behaviour-based intrusion detection [19,
20]. The former attempts to encode knowledge of known intrusions (misuses), typically within rules, and
use this to screen events. The latter attempts to ‘learn’ the features of event patterns that constitute normal
behaviour, and, by observing patterns that deviate from established norms, detect when an intrusion has
occurred [21]. Some intrusion detection systems offer both capabilities, typically via hybridisation
of techniques, see for example [22,26]. However, a system may also be modelled according to both
∗Corresponding author: Vegard Engen, Software Systems Research Centre, Bournemouth University, Fern Barrow, Talbot
Campus, Poole, BH12 5BB, UK. Tel.: +44 1202 965503; E-mail: vengen@bournemouth.ac.uk.
1088-467X/11/$27.50  2011 – IOS Press and the authors. All rights reserved
252 V. Engen et al. / Exploring discrepancies in findings obtained with the KDD Cup ’99 data set
normal and intrusive data, which has become a common approach when adopting machine learning
techniques [7,8,22,69,76,87].
The KDD Cup ’99 data set [81] was created for the Knowledge Discovery and Data Mining Tools com-
petition and associated conference in 1999 [25], and has been used extensively to validate network based
intrusion detection prototypes since. This data set was generated by transforming the raw tcpdump of the
DARPA98/99 data [56,57] to a set of features considered suitable for machine learning techniques [50].
Shortly after the DARPA98 data set was created to evaluate existing IDSs, McHugh [60] published
a critique of the evaluation project, hoping that by criticising the current efforts and discussing issues
openly, this would aid similar initiatives in the future. However, a decade later, no such initiative has been
made. Since the critique of McHugh, additional issues with the DARPA data have been identified [10,
58], which has lead Brugger [9] to claim that the data set is fundamentally flawed and that findings based
solely on this data are invalid.
Most of the existing criticism is directed at the DARPA data [9,10,58,60]. This paper focuses on the
KDD Cup ’99 data set, which, as discussed in the Section 2, does not inherit all of the issues with the
DARPA data, although it has introduced additional methodological factors that affect the results [7,8,
77]. Due to the ready availability of the KDD Cup ’99 data set, it is straightforward to evaluate machine
learning algorithms for intrusion detection, since no preprocessing of raw data is necessary to extract
usable feature vectors. However, many researchers adopt the data set in their studies without sufficient
methodological consideration, and, thus, do not take into account important confounding factors when
interpreting the results. Consequently, this can lead to unsound conclusions. One symptom of this is
that researchers have reported significantly different findings in the literature despite using the same
techniques.
Despite several studies emphasising methodological issues of the DARPA and KDD Cup ’99 data sets,
researchers continue to use them due to a lack of a better publicly available alternative [9,29,30,62,70,
80,87,90]. Hence, it is important to address these methodological issues and identify appropriate use of
the data. This paper seeks to identify methodological causes of discrepancies in published results and to
demonstrate empirically their impact on the findings. These findings allows an improved interpretation
of the current body of research. Additionally, in light of the criticisms in the literature, and in the absence
of a better alternative, this paper discusses appropriate use of the KDD Cup ’99 data set.
The remainder of this paper is organised as follows. Section 2 provides a more thorough description
of the KDD Cup ’99 data set and discusses the criticism from the literature. Section 3 reviews the
different subsets of the data adopted in the literature, and explores the significant differences in the
reported findings. The method adopted for the empirical investigation is presented in Section 4. Results
are reported in Section 5, followed by a summary and conclusions in Section 6. Section 7 concludes the
paper by discussing the implications of the findings in this study and ways in which the KDD Cup ’99
data set can still be used to make valuable contributions to research, in both the intrusion detection and
machine learning/data mining domains.
2. KDD Cup ’99 data set
Details of the data set are presented in Section 2.1 and existing criticism from the literature is discussed
in Section 2.2.
2.1. Data set details
The KDD Cup ’99 data set stems from data gathered at MIT Lincoln Laboratory [64] under sponsorship
of the Defense Advanced Research Projects Agency (DARPA) to evaluate Intrusion Detection Systems
V. Engen et al. / Exploring discrepancies in findings obtained with the KDD Cup ’99 data set 253
Table 1
Intrusions grouped to their respective classes. Normal font represent
intrusions in the training sets; italic for intrusions also present in the
test set, with new attacks in bold
Probing DoS U2R R2L
ipsweep
mscan
nmap
portsweep
saint
satan
apache2
back
land
mailbomb
neptune
pod
processtable
smurf
teardrop
udpstorm
buffer overflow
loadmodule
perl
ps
rootkit
sqlattack
xterm
ftp write
guess passwd
httptunnel
imap
multihop
named
phf
sendmail
snmpgetattack
snmpguess
spy
warezclient
warezmaster
worm
xlock
xsnoop
(IDSs) in 1998 [56] and 1999 [57]. These two data sets are referred to as DARPA98 and DARPA99,
which consist of raw tcpdump data from a simulated medium sized US air force base. The KDD Cup ’99
data set was provided by Stolfo and Lee for the Knowledge Discovery and Data Mining Tools competition
(and associated conference) in 1999 [25]. This is a transformed version of the DARPA tcpdump data,
consisting of a set of features considered suitable for classification with machine learning algorithms.
The data set consists of 41 features, some of which are intrinsic to the network connections, whilst other
are created using domain knowledge. Refer to Lee and Stolfo [50] for further details.
There are three partitions of the KDD Cup ’99 data available online [81]: a full training set (4,898,431
instances), a 10% version of this training set, and a test set (311,029 instances). The test set includes
17 new attacks. The intrusions are commonly grouped into 4 classes, according to the taxonomy of
Kendall [42]: Probing/Surveillance, Denial of Service (DoS), User to Root (U2R) and Remote to Local
(R2L). Some intrusions in the KDD Cup ’99 data set are not described by Kendall [42], but are grouped
here according to that of the KDD contest [24], with two exceptions due to inconsistencies. According
to the KDD classification, three attacks were present in two categories: httptunnel and multihop were
present in U2R and R2L, but are kept only as R2L here; warezmaster was classified as R2L for training,
but as DoS during testing, but is consistently kept as R2L here. An overview of the intrusions, grouped
according to the these classes, is provided in Table 1. The proportions of the classes are given in Table 2.
2.2. Criticism
Both the KDD Cup ’99 and DARPA data sets have been criticised by several researchers [7,8,10,58,
60,77]. Although the KDD Cup ’99 data stems from the DARPA data, not all criticisms apply to both
data sets. However, the transformation of the DARPA data has also introduced further potential issues.
Key points from the literature are discussed below.
2.2.1. Not representative of real network traffic
Shortly after the DARPA99 evaluation [57], McHugh [60] published a paper criticising the data set
(primarily the 1998 evaluation). His main focus was on the evaluation itself, intending to contribute
254 V. Engen et al. / Exploring discrepancies in findings obtained with the KDD Cup ’99 data set
Table 2
Proportion of classes in the KDD Cup ’99 data set
Class\Data set Training (full) Training (10%) Test
Normal 972,780 (19.86%) 97,278 (19.69%) 60,593 (19.48%)
Probing 41,102 (0.84%) 4,107 (0.83%) 4,166 (1.34%)
DoS 3,883,370 (79.30%) 391,458 (79.24%) 229,853 (73.90%)
U2R 52 (0.001%) 52 (0.01%) 70 (0.02%)
R2L 1,126 (0.02%) 1,126 (0.23%) 16,347 (5.26%)
criticism that could help improve future efforts. McHugh argues that the data is not representative of data
in a real network; the structure of the simulated network would not allow for particular intrusions to be
executed properly and the distribution of intrusions are unrealistic. This applies to both the DARPA and
KDD Cup ’99 data sets. As a result, if systems are tested and tuned according to the DARPA (and KDD
Cup ’99) data there is a risk that a system that performs well on this data may be poor in a real intrusion
detection scenario. Related to the usability of the DARPA data, Brugger and Chow [10] acknowledge
that it is feasible to measure true positive rates, but not false positive rates due to unrealistic data that
appear anomalous.
With respect to the proportion of intrusion compared to normal traffic, some researchers have found it
necessary manipulate the data to make it fit their definition of what real intrusion detection data might
look like in order to validate their intrusion detection prototypes. An example of this is some applications
of clustering to anomaly detection [28,52,72], which assume that intrusions are in the minority so that
they can be detected as outliers.
2.2.2. Time to live values
Brugger [9] makes a serious point of the findings of Mahoney and Chan [58], claiming that the DARPA
data is fundamentally flawed and that any findings based solely on this data are invalid. Mahoney and
Chan [58] discovered that only intrusions had ‘Time To Live’ (TTL) values of 126 and 253, whilst
the majority of normal data had TTL values of 127 and 254. Consequently, it would be possible to
perform intrusion detection based on one feature. Based on those findings and feedback from other
researchers, Brugger [9] encourages that papers based solely on the DARPA data set should not be
accepted for publication. However, Brugger extends this statement to the KDD Cup ’99 data set, which
is inappropriate since it does not contain this information its set of features.
2.2.3. Training and test sets
As mentioned above, the test set includes 17 new attacks. Therefore, one can expect the training and
test sets to be significantly different; representing a difficult challenge to misuse detection. Sabhnani
and Serpen [77] empirically investigate the performance of common machine learning techniques on
this data and conclude that they obtain unacceptable detection rates due to the differences between the
training and test sets. They argue that this should not be a reflection of limitations of the techniques, and
suggest merging the training and test sets to produce a new data set, which significantly improved their
detection rates.
2.2.4. R2L intrusions
As pointed out in [8], many snmpgetattack (R2L intrusion) instances are identical to normal instances.
Consequently, it can be observed that this produces false negatives and poor detection rates on R2L
intrusions [7,8].
V. Engen et al. / Exploring discrepancies in findings obtained with the KDD Cup ’99 data set 255
3. Subsets and contradictory results
Researchers have adopted different subsets of the KDD Cup ’99 data in their empirical investigations,
arguably due to the size of the full data set. One problem is memory requirements, the other is the training
(and validation) times of some machine learning techniques, such as artificial neural networks, which
can be very time consuming.
There are two main issues with using different subsets of the data. First, it is difficult to compare
results and conclude how successful particular techniques are. Some studies do make direct comparisons
with other studies that adopt different subsets, which is unsound, as the analysis below demonstrates.
Second, using subsets of the data further compromises the validity of the results (in light of the criticism
discussed in the previous section).
One can expect that using different subsets of the data will lead to different results, particularly since
the original test set includes 17 new attacks. However, not only are the results different, they are in
some cases contradicting. For example, Pan et al. [68] claim that Artificial Neural Networks (ANNs) are
unable to detect U2R and R2L intrusions, whilst Sabhnani and Serpen [77] report that ANNs are able to;
obtaining quite high detection rates (89.28% U2R and 99.44% R2L). Pan et al. only included 1 type of
intrusion for U2R and R2L each, whilst Sabhnani and Serpen merged the training and test sets to form a
new data set.
The use of different subsets is considered here as one of the main contributory factors causing the the
differences in the findings reported in the literature. The subsets used can be classified as one of the
following:
1. selecting only a few types of intrusions
2. compiling a new, small, version of the data set
3. using the original training set only
4. using the original training set and test set
5. merging the training and test sets to create a new data set
6. filtering data to fit assumptions about the distribution of normal and intrusive data
Most of the classification above is self-explanatory, but to avoid ambiguity, it should be noted that studies
using the training set only (#3) do employ validation methods such as hold-out or cross-validation so that
the techniques are tested on unseen data. Approach #6 is not within the scope of this paper, as it has only
been observed in studies using clustering techniques to perform unsupervised anomaly detection [28,52,
72]. However, it can be noted that these studies adopted the training set only (which was then filtered to
meet their assumptions regarding the proportion of intrusion compared to normal data).
Much research has been undertaken in this domain with ANNs [32], Decision Trees (DTs) [73] and
naïve Bayes (NB) [63,65], which are well known classifiers and allow for a comparison of results across
most of the subsets used in the literature. Although the methods in the literature are not identical, clear
trends can be derived from the results, as seen in Table 3. This table presents results on U2R and
R2L intrusions, which best demonstrates the problem. The detection rates on normal, Probing and DoS
intrusions are generally very similar on all subsets.
With reference to Table 3, the following discussion will exclude the first study since no class per-
formance can be compared. The small data subset (#2) has led to higher true positive rates compared
to most of the other subsets; 48–68% U2R and 84.19–95% R2L. Unfortunately, the selection process
used to create this data set is not transparent, which prevents further analysis. Depren et al. [22] do not
offer classification rates for the DT used in their study, but results of their hybrid system are included
to demonstrate the classification rates possible to obtain on the training set alone (#3). The ANN and
256 V. Engen et al. / Exploring discrepancies in findings obtained with the KDD Cup ’99 data set
Table 3
Overview of the detection rates of U2R and R2L intrusions for ANN, DT and
NB techniques reported in the literature. The final column refers to the type
of data subset used, as discussed in Section 3
Study Technique U2R % R2L % Data subset
Pan et al. [68] ANN 0 0 #1
Bosin et al. [5] NB 52.40 94 #2
Chebrolu et al. [14] DT 48 90.58 #2
Mukkamala and Sung [67] ANN 48 95 #2
Peddabachigari et al. [71] DT 68 84.19 #2
Depren et al. [22] Hybrid 80 98.02 #3
Engen et al. [27] DT 9.09 99.56 #3
ANN 0 97.79
Ben Amor [3] DT 7.89 0.52 #4
NB 11.84 7.11
Benferhat and Taiba [4] DT 10.09 0.56 #4
NB 11.40 8.66
Bouzida and Cuppens [7,8] DT 7.02 2.58 #4
ANN 0 ∼27
Sabhnani and Serpen [76] DT 1.80 4.60 #4
ANN 13.20 5.60
Sabhnani and Serpen [77] DT 87.50 99.18 #5
ANN 89.28 99.44
DT in Engen et al. [27] obtain high detection rates of R2L (98.02–99.56%), though not U2R (0–9.09%).
The results obtained on the official test set (#4) are similar for U2R (0–13.20%), but significantly lower
for R2L (0.52–27). Lastly, merging the training and test sets (#5) to produce a new data set leads to the
highest detection rates of all subsets; 87.50–89.28% U2R and 99.18–99.44% R2L. Merging the data sets
changes the intrusion detection challenge significantly since there are no longer new attacks in the test
set, which makes it similar to adopting only the training set.
The results reported in Table 3 do not give a comprehensive insight to the findings. First, the true
negative rates are not reported here and some figures had to be extracted from other data representations
in the source papers (which may lead to inaccuracies). Second, since there are methodological differ-
ences in the studies, such as data preprocessing (scaling, normalisation, removal of duplicates, feature
selection, etc.), algorithm configurations and choice of classification approach (binary, multi-class, etc.),
a comprehensive and objective analysis of the results is not possible. To address the question as to why
the results are as different as they are, i.e., to find the causes of the symptoms, a controlled empirical
investigation is required.
4. Method
This study includes an empirical investigation to provide an objective comparison of findings obtained
with two commonly used machine learning techniques, DTs and NB, as introduced above, on three
subsets of the KDD Cup ’99 data previously used in the literature. The aim of this study is not to
benchmark new techniques that possibly outperform those adopted here; they were chosen because they
are widely used and are well understood, making them ideal for the purpose of this investigation. To
help determine the underlying causes of the discrepancies in the results reported in the literature, several
methodological factors are addressed in this study, which are discussed in Section 4.1. Specifications of
the algorithms adopted here are given in Section 4.2. Details of the data subsets and metrics applied are
provided in Sections 4.3 and 4.4, respectively.
V. Engen et al. / Exploring discrepancies in findings obtained with the KDD Cup ’99 data set 257
4.1. Methodological factors considered in this empirical investigation
This study considers the following methodological factors that may affect the results.
4.1.1. Validation and taxonomy
In addition to the choice of data subset, the choice of validation method may have a significant impact
on the results. Two methods popularly used in the literature are considered here: hold-out and cross-
validation. Cross-validation allows for more extensive use of the data for training and testing, and,
therefore, may yield a more representative measure of performance. The results obtained with hold-out
validation may be significantly different as it will be more sensitive to the selection of training and test
data. Two particular factors in this respect may affect the results: (1) the magnitude of duplicates, which
is discussed further below, and (2) the taxonomy that is adopted, which may affect the minor classes in
particular.
Six attacks in the data set have less than 10 instances each, being as low as 2 in the training set. Even
when grouping the attacks, there are only 52 instances of U2R attacks in the official training set and 70
in the test set. In this context, it is important to bear in mind that the attacks in each of the intrusion
classes (in the taxonomy of Kendall [42]) may not be similar in terms of the feature values, as discussed
by McHugh [60]. Hence, if data is sampled randomly, the majority of instances in the training set may
be of significantly different attacks than those in the test set.
4.1.2. Difference between the training and test sets
The empirical investigation that Sabhnani and Serpen [77] have conducted, which claims that the train-
ing and test sets are unacceptably different, is constrained. First, they only perform binary classification;
U2R versus non-U2R instances, and R2L versus non-R2L instances. It is not clear whether their findings
can be generalised to all classes, and how these would compare to other studies conducting multi-class
classification. Therefore, further analysis as a multi-class classification problem is desirable. Second,
there is no investigation of the data set itself, to indicate why the training and test sets are too different for
successful classification. Their experiments indicate that the difference is not due to the new intrusions
in the data set, though they do not examine the effects of removing them to determine this. Third, their
study does not discuss whether this should be considered an issue of the data set, which is implied, or
whether it is simply a challenge of intrusion detection.
4.1.3. Classification of the R2L class
The classification of R2L intrusions is prone to be poor for two reasons when the original training and
test sets are used:
1. There are normal instances identical to R2L intrusions.
2. Some R2L attacks are not represented in the test set, namely spy and warezclient.
There are only 2 instances of spy attacks, which may not have much impact. However, more significantly,
there are 1020 instances of warezclient, which make up 90.59% of all R2L instances in the training set.
Since the attack representing the majority of data used to train the machine learning techniques is
excluded from the test set, the performance is indeed likely to be poor. Furthermore, there is a large
increase in R2L intrusions in the test set; from 1126 to 16347, doubling the number of attack types.
As pointed out by [8], many snmpgetattack instances are identical to normal instances. Statistics
gathered in this study reveal that there are 8,054 normal instances that are identical to snmpgetattack;
7,273 of which are in the test set, and the remaining in the training set (10% version). There are also
21 normal instances that are identical to DoS attacks; 12 ping of death and 9 teardrop. Due to this,
misclassifications are inevitable, which remains a factor when the training and test sets are merged.
258 V. Engen et al. / Exploring discrepancies in findings obtained with the KDD Cup ’99 data set
Table 4
Proportions of attack classes in the 10% training set and test set before and after removing
duplicates
Training (10%) Test
Before After Before After
Normal 97,278 (19.69%) 87,832 (60.33%) 60,593 (19.48%) 47,913 (62.00%)
Probing 4,107 (0.83%) 2,131 (1.46%) 4,166 (1.34%) 2,682 (3.47%)
DoS 391,458 (79.24%) 54,572 (37.48%) 229,853 (73.90%) 23,568 (30.49%)
U2R 52 (0.01%) 52 (0.03%) 70 (0.02%) 70 (0.09%)
R2L 1,126 (0.23%) 999 (0.69%) 16,347 (5.26%) 3,058 (3.96%)
4.1.4. Duplicates and class imbalance
Due to the lack of temporal information in the KDD Cup ’99 data set, there are many duplicates in
the data set. These duplicates are superfluous for this type of classification problem in which single
connections are being classified independently. Duplicates may have an unfortunate effect on on the
training process of machine learning techniques, because they affect the quality of the data.
Haykin [32, p. 179] and LeCun [49] discuss the importance of high quality training data (in the context
of neural networks trained with backpropagation), and argue that the training samples should be diverse.
Duplicates compromise this diversity, but the effects of this are not indicated by Haykin or LeCun. They
are, however, examined by Kolcz et al. [47], who empirically investigate the effects of duplicates on
NB and a Perceptron with Margins applied to spam detection. Indeed, they observe that the amount of
duplication has a negative effect on the accuracy of the classifiers, and emphasise that they should be
removed.
Duplicates may also lead to somewhat deceptive results during testing, since the ability to detect one
instance will be multiplied according to the number of duplicates. This is a two-fold issue, i.e., if an
instance with many duplicates in the test set is not classified correctly, this can lead to a significant
decrease the classification rate of that particular class.
The effects of duplicates can be compared to the notions of oversampling [36,37,89], which does
affect the class balance significantly in this data set. DoS and Probing are the main classes that suffer
from duplicates, due to the nature of the intrusions. Table 4 gives an overview of the proportions of the
intrusion classes before and after removing duplicates, which illustrates the significant changes to the
class balance.
Until very recently [27], class imbalance had not been acknowledged as a significant challenge
for machine learning applied to intrusion detection. Within the last decade, several researchers have
shown that common machine learning techniques, such as ANNs and DTs, are biased towards the major
class [13,27,38], and, therefore, perform poorly on the minor classes. Consequently, a study that removes
duplicates may obtain significantly different findings to a study that does not. If this is not transparent, it
is not possible to determine whether the findings can be attributed to manipulating the training data, or a
new technique that the study may propose.
4.2. Techniques
Much of the previous research that has motivated this study includes DTs, NB and ANNs. ANNs are
not adopted here, however, since the training process is too time consuming for the extensive experiments
conducted for this investigation, given the constraints on time and computing resources. Nevertheless,
DTs exhibit similar detection behaviour to ANNs [27]. NB is a probabilistic, frequency based technique,
and the literature suggest that NB does not have as strong a bias towards the major classes as DTs (and
V. Engen et al. / Exploring discrepancies in findings obtained with the KDD Cup ’99 data set 259
ANNs) [3,47,69]. Therefore, NB is likely to produce different classification behaviour/performance.
Both the DT and NB algorithms are employed from Weka v. 3.4 [84,85], which allows replication of
these experiments.
4.2.1. Decision tree
A J.48 DT is used; a Java implementation of the C4.5 algorithm [73], which is the most commonly used
DT in this domain. Both pruning and reduced error pruning (REP) have been examined, investigating
confidence factors {0.05, 0.25, 0.50} for the former. Reduced error pruning can prevent over fitting of
the data [65, pp. 69–71], and is employed here with 5 folds, which leaves 80% of the training set to build
the tree from, and 20% for pruning. Other settings include: minimum of 2 instances per leaf node, using
sub tree raising, but not employing binary splits or LaPlace for smoothing counts at leaf nodes. These
settings were determined during preliminary experiments.
4.2.2. Naïve bayes
The NB algorithm in Weka can use one of two configuration options for dealing with numeric
parameters, namely kernel estimation or supervised discretisation. Both options were investigated and it
was observed that no processing of numeric parameters and kernel estimation led to significantly worse
performance than supervised discretisation (in terms of true positive rates). Kernel estimation led to
similar classification rates as the DT, obtaining high classification rates of the major classes, whilst no
processing of numeric parameters simply led to an overall decrease in classification rates. Therefore,
only results with supervised discretisation are reported in this paper.
4.3. Data set and validation
All official subsets of the KDD Cup ’99 data set are adopted here, which are publicly available at the
UCI KDD Archive [81]. However, due to the memory requirements when using the full training set,1
only a version of the data set with no duplicates could be used. Additionally, the 10% training and test
sets were merged to produce a new version of the data set, as in [77]. The small subset used in some
studies [5,14,67,71,79] is not examined since there is not enough information in the literature to recreate
it.
At the time this investigation started, there was one bugged line in the training set (a normal instance
with too many feature values), which was deleted. The detection is performed by the five classes
presented in Section 2.1, rather than individual attacks.
Two validation methods are adopted in this study: hold-out and cross-validation. Hold-out validation
was used in the KDD competition, in which the data was split into a training and test set. In the
experiments conducted on the training set alone, or on the merged data, both hold-out and cross-
validation are examined. According to the findings of Kearns [40], for hold-out validation, the data is
partitioned so that 80% of the instances are used for training and the remaining for testing. The selection
of data for each partition is performed chronologically based on individual attacks, whilst ensuring that
attacks with few instances, even as low as two, will receive at least one instance in the test partition. Ten
folds is chosen for cross-validation, which is supported by an empirical investigation with DTs and NB
by Kohavi [46], and is considered common [22].
1The full training set requires more than 3GB RAM when Weka is used.
260 V. Engen et al. / Exploring discrepancies in findings obtained with the KDD Cup ’99 data set
4.4. Metrics
Several metrics are adopted in this study to provide multiple perspectives on performance. Confusion
matrices are produced to give information about predicted and actual classifications. From the confusion
matrices, overall classification rates (accuracy), true positive and false positive rates are calculated. A true
positive (TP) refers to correctly classifying an intrusion, whilst false positives (FP) refer to classifying
normal instances as intrusion. Similarly, true negatives (TN) refers to correctly classifying normal traffic,
whilst false negatives (FN) refer to intrusions classified as normal. The true and false positive rates are
calculated as follows.2
TPR =
TP
P
=
TP
TP + FN
=
#correct intrusions
#intrusions
(1)
FPR =
FP
N
=
FP
TN + FP
=
#normal as intrusions
#normal
(2)
The confusion matrices allow us to analyse and identify behaviour of the classifiers, whilst the metrics
presented above aim to give a numerical measure of how well the systems perform. The different metrics
can be biased and can give very different impressions of the performance. For example, accuracy does
not give any information about behaviour of the classifiers and they can give the impression of excellent
detection rates provided the major class(es) are detected well. This is an issue here due to the significant
class imbalance in the data set. For example, if U2R intrusions are not detected at all, it will have an
insignificant impact on the accuracy (and the TPR).
Although Receiver Operator Curves (ROCs) are popularly used in the literature, to give a measure of
performance of IDSs in terms of trade-off between TPR and FPR, they are not used here as they do not
add any means of better understanding the results and addressing the aims of this study. For the same
reason, F-measure, precision and the cost matrix used in the KDD Cup ’99 competition are not adopted
here, although they may be useful in other investigations that simply aim to benchmark the performance
of classifiers.
4.5. Outline of empirical investigation
Many experiments are conducted in this study to demonstrate empirically the impact of several
methodological factors that may affect the results, as discussed above. In summary, the following sets
of experiments are conducted:
General observations: notes on the classifiers, the validation methods and specifically the full training
set, which, due to its size, could not be used in the other experiments. As a consequence of the
observations, the results with reduced error pruning and hold-out validation are excluded from the
consecutive experiments as they detract from the focus of the investigation.
Performance on original data sets: serves as a benchmark when analysing the results obtained in the
following experiments. This is conducted on all data sets, except for the full training set.
2The TPR is synonymous to sensitivity and recall, which are other terms often used in the literature.
V. Engen et al. / Exploring discrepancies in findings obtained with the KDD Cup ’99 data set 261
Removing new attacks from the test set: this is done to determine whether these are the sole reason
for the performance difference when adopting the training and test sets, compared to only adopting
the training set or merging the training and test sets. There is a particular focus on the R2L class for
this set of experiments.
Removing normal instances identical to intrusions: conducted on the training and test sets, as well as
the merged data set. As above, there is a focus on the R2L class.
Removing duplicates: this is conducted on all data sets, with an emphasis on how this affects the class
balance.
It should be noted that the purpose of this study is to empirically investigate the factors that may affect
the results, but this does not imply that we consider them issues or flaws of the data. Some of these
factors have indeed been addressed as flaws of the data set in other studies, as previously discussed.
Other factors may be considered practical challenges of such applications. This is discussed further in
Section 7, in the context of the empirical findings here and the existing criticisms.
5. Results
Some general observations are discussed in Section 5.1, followed by a benchmark on the original data
subsets (with no preprocessing) in Section 5.2. Section 5.3 discusses results obtained when removing
new attacks from the test set, to verify the findings of Sabhnani and Serpen [77]. Thereafter, Sections 5.4
and 5.5 discuss results obtained on the different data subsets when removing duplicates and normal
instances identical to intrusions.
5.1. General observations
This section discusses some general observations about algorithm behaviour and the results obtained
with the two validation methods considered in this study. As a consequence of these observations, the
results with reduced error pruning and hold-out validation are excluded from the consecutive sections as
they detract from the focus of the investigation.
5.1.1. Algorithms
Regarding the two techniques adopted in this study, NB obtains higher classification rates on the minor
classes than the DT, as expected. However, the trade-off is a lower classification rate on the major classes,
normal and DoS, which coincides with other observations in the literature [3,69]. Despite this trade-off,
both techniques obtain similar changes in classification rates when different data subsets are employed.
The results obtained with the DT using reduced error pruning (REP) differ from those using confidence
factors (CF). Since REP prunes according to a fold of the training set, the success depends on how accurate
an estimate this fold is of the data in the test set. Considering this in context of the findings of Sabhnani
and Serpen [77], demonstrating how machine learning techniques fail to detect U2R and R2L intrusions
due to significant differences in the training and test sets, it is not unexpected that the performance is
worse with REP.
For all experiments, the accuracy and true positives decrease with REP. Generally, the classification
rates of U2R and R2L are lower with REP, whilst the difference in the detection of the other classes is
negligible. The one exception to this observation is an increased classification rate with REP on Probing
and U2R when the original training and test sets are used. Further analysis of the data would be necessary
to shed light on these observations. Nevertheless, it can be concluded that REP has an overall detrimental
effect on the results.
262 V. Engen et al. / Exploring discrepancies in findings obtained with the KDD Cup ’99 data set
5.1.2. Validation methods
As expected, the results obtained with the two validation methods, hold-out and cross-validation, are
generally significantly different. There are negligible differences in the true positive rates (TPR), though
hold-out validation appears to result in a more pessimistic view of the detection of the minor classes; U2R
in particular. With hold-out validation, there are merely 10 U2R instances in the test partition when the
training set is used, which is a two fold issue. On the one hand, poor detection of this class will have an
insignificant impact on the accuracy and true positive rates. On the other hand, the actual detection rate of
U2R is very sensitive to number of instances correctly classified. Thus, the DT, detecting only one U2R
intrusion leads to only 10% U2R detection, which is merely 1% less TPR than when cross-validation is
used (even though cross-validation leads to 46.15% U2R intrusions detected). Hold-out validation on the
training set leads to unpredictable results in some cases too; for example, when duplicates are removed.
This has no impact on the FPR for the DT (compared to when duplicates are present), but this leads to a
increase of ∼15% in the FPR for NB. In addition, the DoS detection decreases to ∼10%, but is mainly
misclassified as Probing. Similarly, the majority of false positives are detected as Probing.
As will be discussed further in consecutive results sections, the experiments on the merged data set
are more stable and yield more predictable and understandable results. Similarly to the results on the
training set, the TPR remains nearly identical regardless of validation approach. However, the FPR varies
significantly. On the original data, the FPR is higher for both the DT and NB with hold-out validation,
∼5% and ∼11% respectively. However, this difference is reduced when duplicates and normal instances
identical to intrusions are removed. In this case, for NB, the FPR is even ∼2.5% lower than when
cross-validation is used.
Depren et al. [22] state that cross-validation is preferred when there is limited data available. From the
range of experiments conducted, the results obtained with hold-out validation appear deceptive in some
cases compared to the results obtained with cross-validation; seeming to be very sensitive to the class
balance and duplicates. This needs to be considered in context of the discussion in Section 4.1.1, that the
individual attacks within the four classes of intrusion may be significantly different in terms of feature
values. For example, when one split is selected, the training partition may contain many duplicates of
certain attacks, whilst the test partition may have a high proportion of other attacks that are significantly
different to those in the training partition. Then, in experiments where duplicates are removed, the results
will be biased by another training / test split. The effects of this are particularly prominent for the minor
classes; especially U2R.
Generally, then, it is considered here that cross-validation provides a more reliable evaluation, and,
where possible, only cross-validation results are reported in the following sections. Note, however,
that due to memory requirements, the run-time for cross-validation on the full training set is infeasible.
Hence, for the full training set, hold-out validation is used.
5.1.3. Full training set
Due to the magnitude of data in the full training set, it was only possible to adopt a version with no
duplicate instances. Moreover, due to the memory requirements to perform cross-validation, this also
became infeasible. Consequently, results are reported here for hold-out validation.
In most aspects, the results obtained with the full and 10% versions of the training set are similar.
However, whereas the 10% training set led to a significant decrease in DoS classification rates when
duplicates are removed, they remain high when the full training set is used (hold-out validation). The
FPR is also lower. Interestingly, the DT is able to detect 60% more U2R attacks with the full training
set, compared to the 10% version (which is the same detection rate achieved by NB). An overview of the
results is provided in Table 5.
V. Engen et al. / Exploring discrepancies in findings obtained with the KDD Cup ’99 data set 263
Table 5
Overview of results obtained with the full and 10% training sets, using hold-out validation. [o] refers to original
data, and [nd] no duplicate instances
Technique [data set] Accuracy TPR FPR Normal Probing DoS U2R R2L
NB – 10% [o] 77.83% 99.78% 3.91% 96.09% 99.76% 73.00% 80.00% 98.23%
NB – 10% [nd] 52.89% 99.43% 19.13% 80.87% 100% 5.16% 80.00% 98.01%
NB – Full [nd] 95.29% 99.91% 5.60% 94.40% 99.82% 97.93% 70.00% 97.01%
DT (0.50) – 10% [o] 99.61% 99.78% 0.24% 99.76% 99.51% 99.63% 10.00% 80.97%
DT (0.05) – 10% [nd] 65.96% 99.70% 0.31% 99.69% 99.53% 9.85% 10.00% 97.01%
DT (0.50) – Full [nd] 99.94% 99.93% 0.04% 99.96% 98.34% 99.98% 70.00% 96.02%
Table 6
Overview of results obtained with the full and 10% training sets, using original test set. [o] refers to original
data, and [nd] no duplicate instances
Technique [data set] Accuracy TPR FPR Normal Probing DoS U2R R2L
NB [10% o] 91.14% 91.76% 1.10% 98.90% 83.61% 94.98% 64.29% 10.44%
NB [10% nd] 39.48% 78.59% 1.02% 98.98% 84.13% 25.04% 64.29% 10.42%
NB [Full nd] 39.35% 87.95% 1.21% 98.79% 86.68% 24.90% 60.00% 9.93%
DT (0.25) [10% o] 92.58% 91.16% 0.51% 99.49% 75.61% 97.27% 12.86% 5.79%
DT (0.05) [10% nd] 92.48% 91.24% 0.77% 99.23% 79.16% 97.03% 21.43% 7.30%
DT (0.50) [Full nd] 92.18% 91.03% 0.55% 99.45% 80.44% 96.87% 10.00% 2.71%
Table 7
Results on the 10% training set
Technique Accuracy TPR FPR Normal Probing DoS U2R R2L
NB 98.71% 99.83% 1.78% 98.22% 98.47% 98.84% 80.77% 96.71%
DT (CF 0.25) 99.97% 99.98% 0.04% 99.96% 99.37% 99.99% 46.15% 96.36%
An overview of results when the original test set is employed is provided in Table 6. Contrary to the
results obtained with hold-out validation on the training set, the results obtained with the DT deteriorate
when the full training set is used (compared with the 10% version); approximately 10% less U2R and
5% less R2L intrusions are detected. The choice of training set has an insignificant effect on NB except
for the TPR, which is nearly 10% higher with the full training set. This is due to more DoS intrusions
being misclassified as Probing instead of being classified as normal.
5.2. Performance on original data
This section presents the results obtained without any preprocessing, i.e., removing duplicates or
normal instances identical to intrusions.
5.2.1. Training set only
An overview of the results is provided in Table 7. The TPR obtained with the DT and NB are nearly
identical, although the detection of each of the classes differs significantly. The DT detects the major
classes very well and obtains a very low false positive rate (FPR), which is to be expected due to its bias
towards the major class(es) [13,27].
The most significant difference between the two techniques is the detection of U2R intrusions and the
FPR. NB detects nearly twice as many U2R intrusions than the DT. However, the trade-off for detecting
this minor class with higher accuracy is an increased FPR.
264 V. Engen et al. / Exploring discrepancies in findings obtained with the KDD Cup ’99 data set
Table 8
Results on the training and test set
Technique Accuracy TPR FPR Normal Probing DoS U2R R2L
NB 91.14% 91.76% 1.10% 98.90% 83.61% 94.98% 64.29% 10.44%
DT (CF 0.25) 92.58% 91.16% 0.51% 99.49% 75.61% 97.27% 12.86% 5.79%
Table 9
Confusion matrix for the DT with confidence factor 0.25 on the training
and test set
Actual Normal Probing DoS U2R R2L %correct
Normal 60,285 218 75 2 13 99.49
Probing 888 3,112 166 0 0 74.7
DoS 6,253 11 223,588 0 1 97.27
U2R 55 0 0 9 6 12.86
R2L 14,940 344 3 114 946 5.79
Table 10
Results on the merged data set
Technique Accuracy TPR FPR Normal Probing DoS U2R R2L
NB 95.37% 99.03% 14.17% 85.83% 98.79% 97.72% 83.61% 96.11%
DT (CF 0.25) 99.16% 99.53% 2.19% 97.81% 98.96% 99.99% 53.28% 93.05%
5.2.2. Training and test sets
Similarly to the results obtained with the training set, NB detects more U2R and R2L intrusions than
the DT, but the DT obtains a higher accuracy. As expected, the detection rates of both techniques is
significantly lower on the test set than when only the training set is used. An overview of results is
provided in Table 8.
The classes that are affected the most by decreased classification rates are Probing, U2R and R2L
intrusions (the minor classes). For the DT, Probing detection is reduced from ∼99% (when only the
training set is used) to ∼75% when using the test set. Similarly, U2R decreases from ∼46% to ∼13%,
and R2L from ∼96% to ∼6%. NB also suffers from decreased detection rates on the same classes,
although maintaining a higher detection rate. For both techniques, the vast majority of misclassifications
are false negatives, as seen in Table 9. However, this decrease in performance is not solely due to the
introduction of new intrusions in the data set, which will be further discussed in Section 5.3.
5.2.3. Merging the training and test sets
Similarly to the findings of Sabhnani and Serpen [77], the classification rates of U2R and R2L are
significantly higher here when the training and test sets are merged. Further, the third minor class,
Probing, is also detected with a higher accuracy, as seen in Table 10. More than 50% U2R and 80% R2L
intrusions are now detected, which, for the DT, is more than doubling the detection rate for U2R and 20
times more for R2L, compared to using the original training and test sets.
This great improvement is arguably due to validating the techniques on data that is more similar to the
data that was used for training. One factor is that the test set no longer contains completely new attacks.
However, as will be demonstrated in Section 5.3, removing these new attacks from the test set still leads
to poor classification rates on U2R and R2L. Another factor is that R2L validation is handicapped in
the original training and test sets, in which the test set does not contain the intrusion (warezclient) that
makes up 90.59% of the R2L intrusions in the training set, as previously discussed in Section 4.1.3.
V. Engen et al. / Exploring discrepancies in findings obtained with the KDD Cup ’99 data set 265
Table 11
Confusion matrix for NB on the merged data set
Actual Normal Probing DoS U2R R2L %correct
Normal 135,507 2,376 19 824 19,145 85.83
Probing 65 8,173 13 17 5 98.79
DoS 5,889 4,706 607,163 3237 316 97.72
U2R 18 0 0 102 2 83.61
R2L 303 148 0 228 16,794 96.11
%correct 95.57 53.06 99.99 2.31 46.31
Table 12
Overview of results on the training and test set, before and after removing new intrusions from the
test set. [o] refers to the original data set, and [n] refers to having removed new intrusions from the
test set
Technique Accuracy TPR FPR Normal Probing DoS U2R R2L
NB [o] 91.14% 91.76% 1.10% 98.90% 83.61% 94.98% 64.29% 10.44%
NB [n] 96.57% 98.13% 1.10% 98.90% 99.66% 97.74% 64.10% 28.40%
DT [o] 92.58% 91.16% 0.51% 99.49% 75.61% 97.27% 12.86% 5.79%
DT [n] 98.13% 97.98% 0.51% 99.49% 98.65% 99.98% 10.26% 15.77%
Not all classification rates have increased with this merged data set; they are, except for U2R, worse
than those obtained on the training set, and the FPR and normal detection is worse than on the training
and test sets. For both techniques, the detection of normal instances has decreased, and is worst for
NB, only detecting 85.83% normal instances, leading to a 14.17% false positive rate. This is due to a
significant increase in normal instances being detected as R2L intrusions, as seen in Table 11. This is
an interesting observation since it was predominantly R2L intrusions that were misclassified as normal
when the original training and test sets were used. These decreases in classification rates are related to
duplicates and normal instances being identical to intrusions (primarily R2L), which are addressed in
Sections 5.4 and 5.5.
5.3. Removing new attacks from the test set
It is clear that the new attacks indeed pose a greater challenge for the machine learning techniques, but
are not the sole reason for the low detection rates of U2R and R2L when the original training and test sets
are used. As seen in Table 12, removing the new intrusions leads to an overall increase in detection rates
except for U2R. Probing and DoS are now detected with nearly 100% accuracy and there is a significant
increase in R2L detection. However, U2R and R2L detection remains very low compared to the results
obtained with the merged data set (which was more than 50% U2R and 90% R2L detection).
The high detection rates on Probing obtained here imply that the decreased performance on the original
training and test sets is because the new attacks are too different to the existing Probing intrusions used
for training. This is the essence of the findings of Sabhnani and Serpen [77], although they focused only
on U2R and R2L classification. An additional experiment was conducted to verify this assumption, by
performing classification according to the individual attacks so that we can determine which attacks are
being misclassified. Some differences in the results are to be expected, since the classification problem
is changed significantly, but they are quite similar, as seen in Table 13.
Of the two new Probing intrusions, mscan and saint, it is the former that is responsible for the majority
of misclassifications (822 out of 883). The saint intrusions are generally classified as satan and ipsweep
(other Probing intrusions), whilst mscan is predominantly misclassified as normal traffic and neptune
(DoS) intrusions. An analysis of the feature values for the Probing intrusions suggests some underlying
266 V. Engen et al. / Exploring discrepancies in findings obtained with the KDD Cup ’99 data set
Table 13
Overview of predicted Probing classifications for the DT on the training and test sets
Classification approach Normal Probing DoS U2R R2L %correct
Five-class 888 3,112 166 0 0 74.70
Individual attacks 656 3,283 227 0 0 78.80
Table 14
Results obtained on the merged data set with [o] and without [n] normal instances identical to intrusions
Technique Accuracy TPR FPR Normal Probing DoS U2R R2L
NB [o] 95.37% 99.03% 14.17% 85.83% 98.79% 97.72% 83.61% 96.11%
NB [n] 96.33% 99.04% 9.59% 90.41% 98.83% 97.72% 81.97% 95.75%
DT (CF 0.25) [o] 99.16% 99.53% 2.19% 97.81% 98.96% 99.99% 53.28% 93.05%
DT (CF 0.50) [n] 99.93% 99.96% 0.17% 99.83% 99.03% 99.99% 61.48% 98.99%
causes of these misclassifications. Most strikingly, of the services that mscan targets, no other Probing
attacks target imap4, but a large proportion of neptune attacks do. Further, of the other services mscan
targets, very few of the other Probing attacks target the same; only one instance of satan targets pm dump,
five telnet and pop3, and three sunrpc. Compared to the poor classification rates of mscan, all other
Probing attacks contributed with high detection rates, 98.04% ipsweep, 100% nmap, 91.24% portsweep,
99.45% satan, and 97.96% saint.
These observations demonstrate potential dangers of adopting the taxonomy employed in the DARPA
evaluation, as discussed in Section 4.1.1. The saint intrusion is evidently similar to the existing Probing
attacks in the training set (particularly satan), but mscan is not.
5.4. Removing normal instances identical to intrusions
Removing the normal instances that are identical to intrusions does not have a significant impact on
the classification rates when adopting the training and test sets. However, the difference is more clear
on the merged data. The detection rate of normal instances has increased and the false positive rate
has decreased due to not misclassifying as many normal instances as R2L, which coincides with the
expectations. Refer to Table 14 for an overview of these results.
5.5. Removing duplicates
Duplicate instances are removed from all versions of the data subsets examined previously in this
study, and the results are discussed in their respective sections below.
5.5.1. Training set only
There is a negligible difference in the results obtained when duplicates are removed. However, for
both algorithms, the performance has decreased slightly. The most significant difference is the detection
of DoS intrusions for NB, decreasing from 98.84% to 95.50%. As discussed in the previous section,
the results differ significantly depending on the validation method used. It is worth mentioning that
the detection of DoS intrusions decreased from 99.63% to 9.85% for the DT using hold-out validation.
However, the majority of DoS intrusions were detected as Probing, and, thus, the true positive and false
negative rates remained similar.
V. Engen et al. / Exploring discrepancies in findings obtained with the KDD Cup ’99 data set 267
Table 15
Confusion matrix for NB on the training and test sets, when duplicates
are removed from the training set
Actual Normal Probing DoS U2R R2L %correct
Normal 59,972 363 7 67 184 98.98
Probing 396 3,505 125 112 28 84.13
DoS 38,817 132,078 57,561 1,288 109 25.04
U2R 16 0 0 45 9 64.29
R2L 14,391 141 0 112 1,703 10.42
%correct 52.8 2.58 99.77 2.77 83.77
Table 16
Comparison of best classification rates obtained with the original training set [o] and when removing duplicates
[d] from the training set only
Technique [data set] Accuracy TPR FPR Normal Probing DoS U2R R2L
NB [o] 91.14% 91.76% 1.10% 98.90% 83.61% 94.98% 64.29% 10.44%
NB [d] 39.48% 78.59% 1.02% 98.98% 84.13% 25.04% 64.29% 10.42%
DT (CF 0.25) [o] 92.58% 91.16% 0.51% 99.49% 75.61% 97.27% 12.86% 5.79%
DT (CF 0.05) [d] 92.48% 91.24% 0.77% 99.23% 79.16% 97.03% 21.43% 7.30%
Table 17
Overview of classification rates obtained with [o] and without [d] duplicates in the merged data set
Technique Accuracy TPR FPR Normal Probing DoS U2R R2L
NB [o] 95.37% 99.03% 14.17% 85.83% 98.79% 97.72% 83.61% 96.11%
NB [d] 94.45% 97.47% 4.59% 95.41% 98.52% 92.67% 76.23% 89.5%
DT (CF 0.25) [o] 99.16% 99.53% 2.19% 97.81% 98.96% 99.99% 53.28% 93.05%
DT (CF 0.25) [d] 99.68% 99.56% 0.18% 99.82% 97.97% 99.94% 61.48% 93.86%
5.5.2. Training and test set
Similarly to the results obtained on the training set only, the most significant impact of removing
duplicates from the training set (when the test set is used) is on the detection rate of DoS for NB.
However, a much greater decrease is observed here, detecting only 25.04%. The confusion matrix in
Table 15 shows that, again, the majority of misclassifications of DoS intrusions are as Probing. However,
it also now leads to a large number of false negatives.
The results obtained with the DT are more positive, leading to increased detection of the minor classes,
U2R and R2L, as seen in the overview in Table 16. The class imbalance is not as great when removing
duplicates, which is arguably why the DT is now better able to detect the minor classes, whilst NB, being
less biased towards the major classes, maintains an almost identical detection rate of the minor classes.
Hence, these results support the findings in [27].
5.5.3. Merged data set
Overall, the effects of removing duplicates from the merged data set are more stable and positive for
both techniques, compared to using the test set. The largest impact on the results comes from reducing
the misclassifications of normal and R2L, leading to significantly lower false positive rates and false
negative rates. One of the main factors leading to these improvements is that most of the R2L intrusions
identical to normal instances were duplicates, and are now removed; only 86 instances remain. The class
imbalance is also reduced. An overview of the results is presented in Table 17.
Compared with the results obtained on the training and test sets, there are no duplicates in the test
partition(s) here. Further experiments on the training and test sets, removing duplicates from the test
268 V. Engen et al. / Exploring discrepancies in findings obtained with the KDD Cup ’99 data set
Table 18
Overview of results of the Decision Tree (DT) and naïve Bayes (NB) on the different subsets adopted in
this study. Tr = Training set only (10% version); Tr&T = training and test sets; M = merged data set
Data set Accuracy TPR FPR Normal Probing DoS U2R R2L
Tr – NB 98.71% 99.83% 1.78% 98.22% 98.47% 98.84% 80.77% 96.71%
Tr – DT 99.97% 99.98% 0.04% 99.96% 99.37% 99.99% 46.15% 96.36%
T&T – NB 91.14% 91.76% 1.10% 98.90% 83.61% 94.98% 64.29% 10.44%
T&T – DT 92.58% 91.16% 0.51% 99.49% 75.61% 97.27% 12.86% 5.79%
M –NB 95.37% 99.03% 14.17% 85.83% 98.79% 97.72% 83.61% 96.11%
M – DT 99.16% 99.53% 2.19% 97.81% 98.96% 99.99% 53.28% 93.05%
partition also, led to significantly higher classification rates compared to removing duplicates only from
the training set. For example, NB, which achieved merely 25.04% DoS intrusions (see Table 16), now
detects 84.41%. Further, R2L detection increases by approximately 20% for both algorithms.
6. Summary and conclusions
One of the main aims of this study was to identify causes of discrepancy in the findings reported in
the literature with the KDD Cup ’99 data set. An initial analysis uncovered that many different subsets
of the data have been adopted in different studies, which appears to be a likely contributory factor of the
discrepancy. The results from the empirical investigation presented here demonstrate clearly that there is
a significant impact on results depending on the choice of partition, processing, and validation method.
This goes some way toward explaining the often substantial differences and contradictions observed
across the current body of empirical work. The importance of these findings is two-fold: first, the perils
of comparing results and findings across different studies are evident; second, it is clear that there is a
need for deeper consideration of methodological factors in empirical studies.
It was clear from the preliminary analysis that the results would be different with the different subsets.
However, an objective comparison has not been conducted previously, which forms a point of reference
that is important in order to demonstrate the underlying causes of the discrepancy in the findings from
the literature. Table 18 summarises the results on the three different data subsets examined here.
Transparency in choice of data and preprocessing is imperative. Although it may be evident that one
should not make direct comparisons between studies adopting different subsets of the data, there are
factors that affect the results significantly that are not stated in some studies. Therefore, it may not be
evident that inappropriate comparisons are being made. For example, removing duplicates significantly
changes the class balance, which in turn affects the performance of the classifier. Removing duplicates
was also found to remove most of the snmpgetattack instances identical to normal instances; which leads
to significantly improved performance on the R2L class if these instances were misclassified as normal,
or a significantly lower FPR if the identical normal instances were classified as R2L. The results with the
DT on the merged data set demonstrate this clearly, as seen in Table 19; the FPR is lower and smallest
class, U2R, is detected better.
Two new discoveries were made in this study regarding the performance reduction when the original
test set is employed. First, the R2L detection remains poor even when the new attacks are removed.
This was found to be due to the majority of R2L instances during training belonging to an attack that is
not present in the test set; warezclient, which makes up 90.59% of all R2L training instances. Second,
the detection of Probing is hampered by not detecting one new attack that is introduced in the test set;
V. Engen et al. / Exploring discrepancies in findings obtained with the KDD Cup ’99 data set 269
Table 19
Classification rates obtained with the DT on the merged data set when duplicates [d] are removed,
compared to original data [o]
Data set Accuracy TPR FPR Normal Probing DoS U2R R2L
[o] 99.16% 99.53% 2.19% 97.81% 98.96% 99.99% 53.28% 93.05%
[d] 99.68% 99.56% 0.18% 99.82% 97.97% 99.94% 61.48% 93.86%
the mscan attack, which is accountable for the vast majority3 of misclassifications of Probing instances.
The feature values of these attack instances are not similar to the other Probing attacks, with particular
reference to the services that are targeted. This is another indication that the 5-class taxonomy that is
commonly adopted is not appropriate, as previously suggested by McHugh [60]. The other main aim of
this study is to determine whether there is still value in using the KDD Cup ’99 data set. This is discussed
further in the following section. This section also discusses the implications of the data processing
examined here, such as removing duplicates and merging the training and test sets to create a new data
set.
Although the focus of this study is on intrusion detection, specifically exploring discrepancies in results
obtained on the KDD Cup ’99 data set, the findings are of interest to the broader data mining and machine
learning community. First, it is clear that manipulating the data can lead to significant differences in
the results obtained with the same classifier. Therefore, transparency in the method and data processing
is imperative in any study. Second, it is important to consider methodological factors such as those
discussed in this study, which may affect the results significantly. These factors can otherwise lead to
misleading findings. One example of this is the removal of duplicate instances and the impact of class
imbalance. The latter is a critical challenge to learning algorithms, which has received much attention
in recent literature. This is discussed further in the following section.
7. Implications and further work
In this section we consider the implications of the findings of this study, and future use of the KDD
Cup ’99 data set.
7.1. Findings
Although it was not possible to recreate the small subset employed in several studies [5,14,67,71],
due to a lack of available information, the findings in this study provide some indication as to why the
reported U2R and R2L performance on this subset is much higher than on the original training and
test sets (and the training set alone). First, the magnitude of data is reduced significantly, whilst U2R
intrusions remain [71], which implies that the class imbalance is much lower. Hence, according to the
findings here and elsewhere in the literature [13,27,38], detection of the minor classes, Probing, U2R
and R2L are likely to be higher. Second, with particular reference to R2L detection, the high detection
rates may be influenced by avoiding the issues with R2L evaluation in the original training and test sets,
i.e., not testing the warezclient attack, and snmpgetattack instances being identical to normal instances.
Third, duplicates may have been removed, which would increase the quality of the data. However, this
information is not included in the cited papers.
3Out of 883 misclassified Probing instances by the DT, 822 belonged to the mscan attack.
270 V. Engen et al. / Exploring discrepancies in findings obtained with the KDD Cup ’99 data set
Merging the data set may be desirable for evaluating machine learning techniques for misuse detection,
however, it is not a straightforward solution. First, one element of testing is lost: detecting new intrusions
(discussed further below). Second, due to the negative effects of duplicates and normal instances
identical to intrusions, it is beneficial for the performance of machine learning techniques to remove
these. However, to do this, it is necessary to establish some assumptions.
With respect to normal instances identical to intrusions, this could be a result of several factors, such
as flaws in the data collection for the DARPA evaluation, limitations in tcpdump (not enough audit
information to separate certain intrusions from normal behaviour), mislabelling or limitations in the
transformation of DARPA tcpdump to the KDD Cup ’99 data. Bouzida and Cuppens [6,7] assert that
it is due to the latter. Hence, one can justify the removal of these instances, as it would be avoidable
when gathering and preprocessing data for applied misuse detection. However, they do not demonstrate
alternative transformation functions that solve this problem. Therefore, it is not clear whether it is
possible to avoid this issue; that is, such attacks cannot be detected using this audit source.
Duplicates are intrinsic to the intrusion detection problem, due to the nature of some intrusions (mainly
DoS and Probing). Because of the negative impacts of duplicates on the training process, these should
arguably be removed from training data. However, it is not straightforward to determine whether
duplicates should be removed from the test set or not. On the one hand, the existence of duplicates is
more representative of real traffic. On the other hand, duplicates in the test set can skew the results,
and complicate their interpretation. For example, if an attack with many duplicates is detected correctly,
this leads to high detection rates. If an attack with many duplicates is misclassified, this leads to a poor
detection rate. Such classification behaviour can lead to misleading findings.
Duplicate entries are interesting for dealing with alert storms, however, this is not something we can
test properly with the KDD Cup ’99 data due to the absence of temporal information. Instead, they
induce unnecessary complications, such as making it very difficult to determine a suitable split for
hold-out validation. However, this decision needs to be made on the basis of the aims of the particular
investigation.
The decision to merge the data sets should not be motivated by simply obtaining higher classification
rates with techniques that are investigated. As with the issues of removing duplicates, the purpose of
the investigation should ultimately determine which subset of the data to use. For certain studies, using
the original training and test sets may be appropriate and very interesting. The results in this study
(and in the cited literature) indicate that current misuse detection approaches will not be able to detect
a significant proportion of intrusions in the test set. Anomaly detection or hybrid approaches may do,
on the other hand, and when exploring such techniques, novel attacks are essential in the test data. For
example, Xiang et al. [87] propose a hierarchical hybrid system, comprising an AutoClass (AC) classifier
(Bayesian clustering) [15] and a DT. At the first level, the DT will classify an instance as DoS, Probing
or ‘others’. At the next level, ‘others’ are further refined to either ‘normal’ or ‘intrusion’ with the AC.
Finally, the DT will refine ‘intrusion’ further to either U2R or R2L. In addition to this classification
process, each technique is trained on selected data from the original training set; 10000 random normal
and all U2R and R2L instances for the AC, and an under sampled subset for the DT, with only 3000
normal instances, less than 10,000 Probing and DoS intrusions, and all U2R and R2L instances. Thus,
for the DT, the class imbalance is significantly reduced. The classification rates on the original test set
are significantly higher than those reported here, although at the expense of a higher FPR, as seen in
Table 20. Xiang et al. [87] note that all snmpgetattack instances are misclassified as Normal, which is
the main reason the R2L detection is not higher. Therefore, this implies that the FPR is not caused by
this attack.
V. Engen et al. / Exploring discrepancies in findings obtained with the KDD Cup ’99 data set 271
Table 20
Results of the hybrid approach of Xiang et al. [87] compared to the DT in
this study (on the original test set)
FPR Normal Probing DoS U2R R2L
Hybrid 3.2% 96.80% 93.40% 98.66% 71.43% 46.97%
DT 0.77% 99.23% 79.16% 97.03% 21.43% 7.30%
7.2. Is the KDD Cup ’99 data still useful?
With respect to criticism discussed in Section 2.2, it is important to question whether the KDD Cup
’99 data is still useful for research into machine learning and/or intrusion detection. The criticisms of
McHugh [60] and Brugger [9], regarding the data not being representative of real network traffic, are
particularly important. With respect to the DARPA data, the issue with TTL values [58] is indeed a
serious flaw, as it is possible to perform effective intrusion detection based on one feature, which would
not be possible in practice. This is one of the main reasons why the DARPA data set (and by association
the KDD Cup ’99 data set) are now not valued for publication of research findings by some researchers.
However, the issue of TTL values does not apply to the KDD Cup ’99 data, and the additional ‘issues’ of
this data set can be dealt with, as discussed in the previous section. Although the criticism reported in the
literature is justified, what is lacking is the identification of the value of the significant body of research
having adopted the DARPA and KDD Cup ’99 data sets. Ultimately, as Brugger [9] acknowledges,
researchers continue to use this data due to a lack of better publicly available alternatives.
Many papers discuss network based intrusion detection in a generic way, whilst the traffic in different
types of networks are arguably significantly different. Hence, satisfying a requirement for a general
network based data set to be representative of real environments from a machine learning / data mining
point of view is not possible. Research on machine learning applied to intrusion detection implies that
the techniques will be trained on data gathered from the specific environment they are deployed in.
Therefore, if the data set poses some generic, real, challenges for machine learning approaches when
applied to intrusion detection, there is still value in it. It is simply in the interpretation of the findings
of such research that caution must be observed, in evaluating the similarity of these challenges to those
posed by any particular environment in which the techniques may be considered for deployment.
It is important to distinguish machine learning approaches from other signature based approaches, such
as rule based systems [54,55] and state based systems [34,35,82]. Most machine learning approaches
only offer stateless intrusion detection (Hidden Markov Models (HMM) [74] are one exception), i.e.,
classifying single events independently. HMM also have limitations, not supporting complete temporal
event correlation, since the events are generally recorded over a relatively small sliding window, e.g., 10
events [1,83]. Therefore, more sophisticated multi-stage intrusions are impossible to detect. However,
there are benefits of stateless approaches. First, they are more capable of performing real-time intrusion
detection [48]. Second, machine learning approaches offer some desirable flexibility compared with rule
based systems, which are unable to detect variations of known attacks (i.e., they generalise poorly) [53].
Similarly to other authors [9,60], it is not recommended here that the KDD Cup ’99 data set is simply
used as an evaluation benchmark. However, it can be used to address some generic challenges for
machine learning applied to intrusion detection, for example:
– Related to the learning process:
∗ Dealing with high dimensional data (curse of dimensionality and memory requirements)
∗ Learning from a very large data set (learning speed)
∗ Learning from imbalanced data
272 V. Engen et al. / Exploring discrepancies in findings obtained with the KDD Cup ’99 data set
– Feature selection (data reduction)
– Incremental learning
– Detecting new intrusions
– Simulating intrusion detection of encrypted data
Most of the suggested challenges listed above relate directly to the two benefits of using machine learning
for intrusion detection discussed above. Feature selection, for example, is an effective way of reducing
the data, which has benefits for both the training and classification processes. It effectively reduces
the amount of data required to process, reduces the dimensionality of the problem and saves memory.
Several machine learning techniques have been used for this purpose, see for example [14,66,75,79].
Another general challenge is learning from imbalanced data, which is of equal interest to many other
domains, such as medicine [16,59,61], credit scoring [33], customer churn [11,88], natural language
processing [45], lexical acquisition [43] and text recognition [78]. Engen et al. [27] demonstrate that
the intrinsic class imbalance in intrusion detection can be the cause of poor true positive rates for some
machine learning techniques, such as ANNs, which have a bias towards the major class(es). Hence it
is necessary to develop and apply appropriate techniques and/or investigate methods of sampling the
training data.
One particular challenge of interest in this domain is adapting over time. Some techniques require
complete retraining as an offline process once more data is gathered. Therefore, incremental learning is a
desirable trait as the data changes over time due to new applications and new types of intrusion. Several
researchers propose methods that allow incremental learning, for example, Cannady [12] proposes an
IDS with an adaptive ANN, a Cerebellar Model Articulation Controller (CMAC) [2], which is capable
of learning new intrusive behaviour during run-time. For other studies considering incremental learning,
see, for example, Kim and Bentley [44], using Artificial Immune Systems [17,18], and Ramos and
Abraham [75], using Ant Colony Optimisation [23].
If the merged data set is used, two approaches for misuse detection may be taken to be able to test
the detection of new attacks: (1) the approach taken in the KDD Cup ’99 competition, splitting the data
into a training and test partition, in which the test partition contains several new intrusions of each class
of intrusion or (2), train on all but one type of intrusion and test only the detection of the intrusion type
excluded from the training. The latter approach is more laborious, but avoids some complications. If the
taxonomy of Kendall [42] is adopted for classification, the selection of new attacks for the test partition
should not be performed randomly according to these four class of intrusion. This is because the selected
attacks for each partition may not be similar in terms of feature values [60]. Instead, new attacks for the
test set should be selected based on some degree of similarity with the intrusions in the training set to
allow for a fair test.
Lindqvist and Porras [55] promote using both host based and network based intrusion detection systems
since neither alone is likely to be able to detect all forms of misuse. For example, one limitation of
network based intrusion detection systems is detecting intrusion in encrypted traffic (via SSH and SSL).
However, Wright et al. [86] have been able to perform network based intrusion detection of encrypted
traffic based on packet size and arrival time of packets, using HMM. They simulated encryption by
altering the data in the DARPA98 data set and data gathered at George Mason University. In a similar
fashion, encryption can be simulated using the KDD Cup ’99 data set.
Finally, whichever data set is adopted, there is a need for more clear documentation of the data and
experiment method, to support independent validation of findings and comparison across studies. There
is also a need for a more thorough understanding of the nature of the data, in order to be able to interpret
effectively the results, and assess their implications in practical application. For the purpose of evaluating
V. Engen et al. / Exploring discrepancies in findings obtained with the KDD Cup ’99 data set 273
IDS prototypes, we support the existing recommendations of creating, and maintaining, a new data set
that is up to date with state-of-the-art intrusions.
References
[1] M. Al-Subaie and M. Zulkernine, Efficacy of Hidden Markov Models Over Neural Networks in Anomaly Intrusion Detec-
tion. In 30th Annual International Computer Software and Applications Conference (COMPSAC’06), 2006, pages 325–
332.
[2] J. Albus, A new approach to manipulator control: The cerebellar model articulation controller (cmac), Transactions
ASME, Journal Dynamic Systems, Measurement, and Control 97 (1975), 220–227.
[3] N. Ben Amor, S. Benferhat and Z. Elouedi, Naive Bayes vs Decision Trees in Intrusion Detection Systems. In SAC ’04:
Proceedings of the 2004 ACM symposium on Applied computing, New York, NY, USA, 2004, pages 420–424. ACM.
[4] S. Benferhat and K. Tabia, On the combination of naive bayes and decision trees for intrusion detection. In CIMCA ’05:
Proceedings of the International Conference on Computational Intelligence for Modelling, Control and Automation and
International Conference on Intelligent Agents, Web Technologies and Internet Commerce Vol-1 (CIMCA-IAWTIC’06),
Washington, DC, USA, 2005, pages 211–216. IEEE Computer Society.
[5] A. Bosin, N. Dessı̀ and B. Pes, Intelligent Bayesian Classifiers in Network Intrusion Detection. In Proceedings of the
18th international conference on Innovations in Applied Artificial Intelligence, London, UK, 2005, pages 445–447.
Springer-Verlag.
[6] Y. Bouzida, Principal Component Analysis for Intrusion Detection and Supervised Learning for New Attack Detection,
PhD thesis, Graduate Faculty of Ecole Nationale Supérieure des Télécommunications de Bretagne, 2006.
[7] Y. Bouzida and F. Cuppens, Detecting Known and Novel Network Intrusions. In IFIP/SEC 2006, 21st IFIP TC-11
International Information Security Conference, 2006.
[8] Y. Bouzida and F. Cuppens, Neural networks vs. decision trees for intrusion detection. In IEEE / IST Workshop on
Monitoring, Attack Detection and Mitigation (MonAM), 2006.
[9] T. Brugger, Kdd cup ’99 dataset (network intrusion) considered harmful. KDnuggets, n18(item4), September 2007.
Available online: [http://www.kdnuggets.com/news/2007/n18/4i.html] [Accessed November 23 2007].
[10] T. Brugger and J. Chow, An assessment of the DARPA IDS Evaluation Dataset using Snort, Technical Report CSE-2007-1,
University of California, Department of Computer Science, 2007.
[11] J. Burez and D. van den Poel, Handling Class Imbalance in Customer Churn Prediction, Expert Systems with Applications
36(3) (April 2009), 4626–4636.
[12] J. Cannady, Next Generation Intrusion Detection: Autonomous Reinforcement Learning of Network Attacks. In Pro-
ceedings of the 23rd National Information Systems Secuity Conference, 2000.
[13] N.V. Chawla, C4.5 and Imbalanced Data sets: Investigating the effect of sampling method, probabilistic estimate, and
decision tree structure. In Workshop on Learning from Imbalanced Datasets II, ICML, 2003.
[14] S. Chebrolu, A. Abraham and J.P. Thomas, Feature Deduction and Ensemble Design of Intrusion Detection Systems,
Computers and Security 24(4) (June 2005), 295–307.
[15] P. Cheeseman and J. Stutz, Bayesian Classification (AutoClass): Theory and Results. In Advances in Knowledge Discovery
and Data Mining, 1996, pages 153–180.
[16] G. Cohen, M. Hilario, H. Sax, S. Hugonnet and A. Geissbuhler, Learning from imbalanced data in surveillance of
nosocomial infection, Artificial Intelligence in Medicine 37(1) (2006), 7–18.
[17] D. Dasgupta, editor, Artificial Immune Systems and Their Applications, Springer, 1998.
[18] L.N. De Castro and J. Timmis, Artificial Immune Systems: A New Computational Intelligence Approach, Springer, 2002.
[19] H. Debar, An Introduction to Intrusion-Detection Systems. In An Introduction to Intrusion-Detection Systems, 2000.
[20] H. Debar, M. Dacier and A. Wespi, Towards a Taxonomy of Intrusion Detection Systems Computer Networks 31(9)
(1999), 805–822.
[21] D.E. Denning, An intrusion-detection model, IEEE Transactions on Software Engineering 13(2) (1987), 222–232.
[22] O. Depren, M. Topallar, E. Anarim and M.K. Ciliz, An Intelligent Intrusion Detection System for Anomaly and Misuse
Detection in Computer Networks, Expert systems with Applications 29 (2005), 713–722.
[23] M. Dorigo and T. Stutzle, Ant Colony Optimization, The MIT Press, July 2004.
[24] C. Elkan, Results of the KDD’99 Classifier Learning Contest, 1999. Available online: [http://www-cse.ucsd.edu/
˜elkan/clresults.html].
[25] C. Elkan, Results of the KDD’99 classifier learning, SIGKDD Explor Newsl 1(2) (2000), 63–64.
[26] D. Endler, Intrusion Detection: Applying Machine Learning to Solaris Audit Data. In Proceedings of the Annual Computer
Security Applications conference, 1998, pages 267–279.
[27] V. Engen, J. Vincent and K. Phalp, Enhancing Network Based Intrusion Detection for Imbalanced Data. International
Journal of Knowledge-Based Intelligent Engineering Systems (KES) 12(5,6) (December 2008), 357–367.
274 V. Engen et al. / Exploring discrepancies in findings obtained with the KDD Cup ’99 data set
[28] E. Eskin, A. Arnold, M. Prerau, L. Portnoy and S. Stolfo, A Geometric Framework for Unsupervised Anomaly Detection:
Detecting Intrusions in Unlabeled Data. In Data Mining for Security Applications, Kluwer, 2002.
[29] F. Gharibian and A.A. Ghorbani, Comparative Study of Supervised Machine Learning Techniques for Intrusion Detec-
tion. In CNSR ’07: Proceedings of the Fifth Annual Conference on Communication Networks and Services Research,
Washington, DC, USA, 2007, pages 350–358. IEEE Computer Society.
[30] G. Giachinto, R. Perdisci, M. Del Rio and F. Roli, Intrusion detection in computer networks by a modular ensemble of
one-class classifiers, Information Fusion 9 (2008), 69–82.
[31] D. Gollmann, Computer Security, Wiley, 2nd edition, 2006.
[32] S. Haykin, Neural Networks: A Comprehensive Foundation, Prentice Hall, 2 edition, 1998.
[33] Y.-M. Huang, C.-M. Hung and H.C. Jiau, Evaluation of neural networks and data mining methods on a credit assessment
task for class imbalance problem, Nonlinear Analysis: Real World Applications 7(4) (2006),720–747.
[34] K. Ilgun, USTAT: A Real-Time Intrusion Detection System for UNIX. In SP ’93: Proceedings of the 1993 IEEE
Symposium on Security and Privacy, Washington, DC, USA, 1993, page 16. IEEE Computer Society.
[35] K. Ilgun, R.A. Kemmerer and P.A. Porras, State Transition Analysis: A Rule-Based Intrusion Detection Approach,
Software Engineering 21(3) (1995), 181–199.
[36] N. Japkowicz, Learning from Imbalanced Data Sets: A Comparison of Various Strategies. In Proceedings of Learning
from Imbalanced Data Sets, Papers from the AAAI Workshop, Technical Report WS-00-05, 2000, pages 10–15.
[37] N. Japkowicz, The Class Imbalance Problem: Significance and Strategies. In Proceedings of the 2000 International
Conference on Artificial Intelligence (IC-AI’2000), volume 1, 2000, pages 111–117.
[38] T. Jo and N. Japkowicz, Class Imbalances versus Small Disjuncts, SIGKDD Explorations 6(1) (2004), 40–49.
[39] O. Kachirski and R. Guba, Effective Intrusion Detection Using Multiple Sensors in Wireless Ad Hoc Networks. In
Proceedings of the 36th Annual Hawaii International Conference on System Sciences, 2003.
[40] M. Kearns, A Bound on the Error of Cross Validation Using the Approximation and Estimation Rates, with Consequences
for the Training-Test Split. In Advances in Neural Information Processing Systems 8, The MIT Press, 1996, pages 183–
189.
[41] R.A. Kemmerer and G. Vigna, Intrusion detection: A brief history and overview, Security & Privacy 35(4) (2002), 27–30.
[42] K. Kendall, A Database of Computer Attacks for the Evaluation of Intrusion Detection Systems, Master’s thesis, Mas-
sachusetts Institute of Technology, 1999.
[43] K. Kermanidis, M. Maragoudakis, N. Fakotakis and G. Kokkinakis, Learning Greek verb complements: addressing the
class imbalance. In COLING ’04: Proceedings of the 20th international conference on Computational Linguistics, page
1065, Morristown, NJ, USA, 2004. Association for Computational Linguistics.
[44] G.H. Kim and P.J. Bentley, Towards Artificial Immune Systems for Network Intrusion Detection: An Investigation of
Dynamic Clonal Selection. In Proceedings of the Congress on Evolutionary Computation, 2002.
[45] L. Kobyliński and A. Przepiórkowski, Definition Extraction with Balanced Random Forests. In Advances in Natural
Language Processing: Proceedings of the 6th International Conference on Natural Language Processing, Springer-
Verlag, 2008, pages 237–247.
[46] R. Kohavi, A Study of Cross-Validation and Bootstrap for Accuracy Estimation and Model Selection. In International
Joint Conference on Artificial Intelligence (IJCAI), 1995.
[47] A. Kolcz, A. Chowdhury and J. Alspector, Data duplication: an imbalance problem? In ICML’2003 Workshop on
Learning from Imbalanced Data Sets II, 2003.
[48] C. Kruegel, F. Valeur and G. Vigna, Intrusion Detection and Correlation: Challenges and Solutions, Springer-Verlag
Telos, 2004.
[49] Y. LeCun, L. Bottou, G.B. Orr and K.-R. Mueller, Efficient BackProp. In Neural Networks: Tricks of the Trade, this book
is an outgrowth of a 1996 NIPS workshop, London, UK, 1998, pages 9–50. Springer-Verlag.
[50] W. Lee and S.J. Stolfo, A Framework for Constructing Features and Models for Intrusion Detection Systems, ACM
Transactions on Information and System Security 3(4) (2000), 227–261.
[51] W. Lee and D. Xiang, Information Theoretic Measures for Anomaly Detection. In IEEE Symposium on Security and
Privacy, Oakland, CA, May 2001.
[52] K. Leung and C. Leckie, Unsupervised anomaly detection in network intrusion detection using clusters. In ACSC
’05: Proceedings of the 28th Australasian conference on Computer Science, Darlinghurst, Australia, Australia, 2005,
pages 333–342. Australian Computer Society, Inc.
[53] L.M. Lewis, A Case-Based Reasoning Approach to the Resolution of Faults in Communication Networks. In Proceedings
of the IFIP TC6/WG6.6 Third International Symposium on Integrated Network Management with participation of the
IEEE Communications Society CNOM and with support from the Institute for Educational Services, North-Holland,
1993, pages 671–682.
[54] U. Lindqvist and P.A. Porras, Detecting Computer and Network Misuse Through the Production-Based Expert System
Toolset (P-BEST). In IEEE Symposium on Security and Privacy, 1999, pages 146–161.
V. Engen et al. / Exploring discrepancies in findings obtained with the KDD Cup ’99 data set 275
[55] U. Lindqvist and P.A. Porras, eXpert-BSM: A Host Based Intrusion Detection Solution for Sun Solaris. In Proceedings
of the 17th Annual computer Security Applications Conference, New Orleans, USA, Dec 2001, pages 240–251.
[56] R. Lippmann, D. Fried, I. Graf, J. Haines, K. Kendall, D. McClung, D. Weber, S. Webster, D. Wyschogrod, R. Cunningham
and M. Zissman, Evaluating Intrusion Detection Systems: The 1998 DARPA Off-line Intrusion Detection Evaluation. In
Proceedings of the DARPA Information Survivability Conference and Exposition 2, IEEE Computer Society Press, 2000,
pages 12–26.
[57] R. Lippmann, J.W. Haines, D.J. Fried, J. Korba and K. Das, The 1999 darpa off-line intrusion detection evaluation,
Comput Networks 34(4) (2000), 579–595.
[58] M.V. Mahoney and P.K. Chan, An analysis of the 1999 darpa/lincoln laboratory evaluation data for network anomaly
detection. In Proc 6th Intl Symp on Recent Advances in Intrusion Detection (RAID 2003) 2820, 2003, pages 220–237.
[59] M.A. Mazurowski, P.A. Habas, J.M. Zurada, J.Y. Lo, J.A. Baker and G.D. Tourassi. Training neural network classifiers
for medical decision making: The effects of imbalanced datasets on classification performance, Neural Networks 21(2–3)
(2008), 427–436.
[60] J. McHugh, Testing intrusion detection systems: A critique of the 1998 and 1999 darpa intrusion detection system
evaluations as performed by lincoln laboratory, ACM Transactions on Information and System Security 3(4) (2000),
262–294.
[61] L. Mena and J.A. Gonzalez, Machine learning for imbalanced datasets: Application in medical diagnostic. In Proceedings
of the 19th International FLAIRS Conference, 2006.
[62] E. Michailidis, S.K. Katsikas and E. Georgopoulos, Intrusion Detection Using Evolutionary Neural Networks. In PCI
’08: Proceedings of the 2008 Panhellenic Conference on Informatics, Washington, DC, USA, 2008, pages 8–12. IEEE
Computer Society.
[63] D. Michie, D.J. Spiegelhalter and C.C. Taylor, editors. Machine learning, neural and statistical classification. Ellis
Horwood, 1994. Available online: [http://www.amsta.leeds.ac.uk/ charles/statlog/].
[64] MIT Lincoln Labratory, DARPA Intrusion Detection Evaluation Data Sets, 2000. Available online: [http://www.ll.mit.
edu/IST/ideval/data/data index.html].
[65] T.M. Mitchell, Machine Learning, McGraw–Hill, 1997.
[66] S. Mukkamala and A.H. Sung, Identifying Key Features for Intrusion Detection Using Neural Networks. In ICCC
’02: Proceedings of the 15th international conference on Computer communication, Washington, DC, USA, 2002,
pages 1132–1138. International Council for Computer Communication.
[67] S. Mukkamala and A.H. Sung, Artificial Intelligent Techniques for Intrusion Detection. In Proceedings of the IEEE
International Conference on Systems, Man and Cybernetics, 2003.
[68] Z.S. Pan, S.C. Chen, G.B Hu and D.Q. Zhang, Hybrid Neural Network and C4.5 for Misuse Detection. In Machine
Learning and Cybernetics, Xi’an, 2003, pages 2463–2467.
[69] M. Panda and M.R. Patra, Network intrusion detection using naive bayes, IJCSNS International Journal of Computer
Science and Network Security 7(12) (2007), 258–263.
[70] M. Panda and M.R. Patra, Ensemble of classifiers for detecting network intrusion. In ICAC3 ’09: Proceedings of
the International Conference on Advances in Computing, Communication and Control, New York, NY, USA, 2009,
pages 510–515. ACM.
[71] S. Peddabachigari, A. Abraham, C. Grosan and J. Thomas, Modeling Intrusion Detection Systems Using Hybrid Intelligent
Systems, Journal of Network and Computer Applications 30(1) (January 2007), 114–132.
[72] L. Portnoy, E. Eskin and S. Stolfo, Intrusion Detection With Unlabeled Data Using Clustering. In Proceedings of the
ACM Workshop on Data Mining Applied to Security, 2001.
[73] J.R. Quinlan, C4.5: programs for machine learning, Morgan Kaufmann Publishers Inc., San Francisco, CA, USA, 1993.
[74] L.R. Rabiner, A tutorial on hidden markov models and selected applications in speech recognition, Proceedings of the
IEEE 77(2) (1989), 257–286.
[75] V. Ramos and A. Abraham, ANTIDS: Self-Organized Ant-based Clustering Model for Intrusion Detection System, 2004.
[76] M. Sabhnani and G. Serpen, Application of Machine Learning Algorithms to KDD Intrusion Detection Dataset within
Misuse Detection Context. In Proceedings of the International Conference on Machine Learning, Models, Technologies
and Applications (MLMTA 2003) 1 2003, pages 209–215.
[77] M. Sabhnani and G. Serpen, Why machine learning algorithms fail in misuse detection on kdd intrusion detection data
set, Intelligent Data Analysis 8(4) (2004), 403–415.
[78] E. Stamatatos, Author identification: Using text sampling to handle the class imbalance problem, Information Processing
and Management 44(2) (2008), 790–799.
[79] A.H. Sung and S. Mukkamala, Identifying Important Features for Intrusion Detection Using Support Vector Machines
and Neural Networks. In SAINT ’03: Proceedings of the 2003 Symposium on Applications and the Internet, Washington,
DC, USA, 2003, pages 209–216. IEEE Computer Society.
[80] A. Tajbakhsh, M. Rahmati and A. Mirzaei, Intrusion detection using fuzzy association rules, Appl Soft Comput 9(2)
(2009), 462–469.
276 V. Engen et al. / Exploring discrepancies in findings obtained with the KDD Cup ’99 data set
[81] The UCI KDD Archive, Kdd cup ’99 data set, 1999. Available online: [http://kdd.ics.uci.edu/databases/kddcup99/
kddcup99.html].
[82] G. Vigna and R.A. Kemmerer, NetSTAT: A Network-Based Intrusion Detection Approach. In Proceedings of the 14th
Annual Computer Security Application Conference, 1998, pages 25–34.
[83] W. Wang, X. Guan and X. Zhang, Processing of massive audit data streams for real-time anomaly intrusion detection,
Comput Commun 31(1) (2008), 58–72.
[84] Weka 3, Data Mining Software in Java, 2008. http://www.cs.waikato.ac.nz/ml/weka/.
[85] I.H. Witten and E. Frank, Data Mining: Practical machine Learning Tools and Techniques. Morgan Kaufmann, 2nd
edition, 2005.
[86] C. Wright, F. Monrose and G.M. Masson, HMM Profiles for Network Traffic Classification. In VizSEC/DMSEC ’04:
Proceedings of the 2004 ACM workshop on Visualization and data mining for computer security, New York, NY, USA,
2004, pages 9–15. ACM.
[87] C. Xiang, P.C. Yong and L.S. Meng, Design of multiple-level hybrid classifier for intrusion detection system using
Bayesian clustering and decision trees, Pattern Recogn Lett 29(7) (2008), 918–924.
[88] Y. Xie, X. Li, E.W.T. Ngai and W. Ying, Customer Churn Prediction using Improved Balanced Random Forests, Expert
Systems with Applications 36(3) (2009), 5445–5449.
[89] X. Yang, Y. Zheng, M. Siddique and G. Beddoe, Learning from imbalanced data: a comparative study for colon CAD. In
Society of Photo-Optical Instrumentation Engineers (SPIE) Conference Series, volume 6915 of Presented at the Society
of Photo-Optical Instrumentation Engineers (SPIE) Conference, April 2008.
[90] A. Zainal, M.A. Maarof, S.M. Shamsuddin and A. Abraham, Ensemble of One-Class Classifiers for Network Intrusion
Detection System. In Proceedings of the 4th International Conference on Information Assurance and Security, IEEE,
2008, pages 180–185.
