Retrieving Candidate Plagiarised Documents
Using Query Expansion
Rao Muhammad Adeel Nawab1, Mark Stevenson1, and Paul Clough2
1 Department of Computer Science, University of Sheffield, UK
r.nawab|m.stevenson@dcs.shef.ac.uk
2 Information School, University of Sheffield, UK
p.d.clough@sheffield.ac.uk
Abstract. External plagiarism detection systems compare suspicious
texts against a reference collection to identify the original one(s). The
suspicious text may not contain a verbatim copy of the reference col-
lection since plagiarists often try to disguise their behaviour by altering
the text. For large reference collections, such as those accessible via the
internet, it is not practical to compare the suspicious text with every
document in the reference collection. Consequently many approaches to
plagiarism detection begin by identifying a set of candidate documents
from the reference collection. We report an IR-based approach to the can-
didate document selection problem that uses query expansion to identify
candidates which have been altered. The reported system outperforms a
previously reported approach and is also robust to changes in the refer-
ence collection text.
Keywords: information retrieval, external plagiarism detection, query
expansion.
1 Introduction
Plagiarism is generally thought of as the unattributed use of a piece of text
[15]. Large scale electronic document collections, such as those accessible via the
internet, are now readily available making it easy to plagiarise documents and
difficult to identify the source. Plagiarism is acknowledged as a significant prob-
lem in higher education and has been reported to be on the increase [22,17,12].
Consequently plagiarism and its detection has recently received significant atten-
tion [2,18] and automated systems are now routinely used by higher education
institutions to identify plagiarism in students’ work.
Extrinsic plagiarism detection systems are provided with a document that is
suspected as being plagiarised (the suspicious document) and a reference col-
lection.1 The system aims to identify the portions of text from documents in
the reference collection which were used to plagiarise the suspicious document.
Comparing the source document against each document within it is likely to
1 Another type of plagiarism detection, intrinsic plagiarism detection [19], does not
use a reference collection.
R. Baeza-Yates et al. (Eds.): ECIR 2012, LNCS 7224, pp. 207–218, 2012.
c© Springer-Verlag Berlin Heidelberg 2012
208 R.M.A. Nawab, M. Stevenson, and P. Clough
be impractical if the reference collection is large. To avoid this the problem of
external plagiarism detection is commonly split into two stages: candidate doc-
ument selection and detailed analysis [27]. The aim of the candidate document
selection stage is to identify a set of documents from the reference collection
which includes text that has been plagiarised in the suspicious document. These
documents are then analysed more thoroughly in the detailed analysis stage
which aims to match portions of text in the source text with documents in the
reference collection.
Candidate document selection reduces the search space for the detailed anal-
ysis stage and has been shown to improve the speed and efficiency of plagiarism
detection systems [1]. In addition it could also be applied in semi-automatic
approaches to plagiarism detection by providing a human expert with a set of
documents which is small enough to be manually analysed. Despite this there
has been little previous work on the problem [1].
Plagiarists often try to disguise their behaviour by altering the text in some
way (obfuscation), for example by paraphrasing, summarising or deleting por-
tions of text [4,11,13]. However, many previous approaches to plagiarism detec-
tion have been limited to the detection of exact (verbatim) copies of documents
in the reference collection [26,14,8]. Even simple changes to the text in the refer-
ence collection can be enough to stop systems detecting plagiarism. For example,
Maurer et al. [16] tested two well-known commercial plagiarism detection services
with paraphrased text and found that they were unable to detect plagiarism. In
addition, analysis of the systems presented in the 2nd international competition
on plagiarism detection showed that detecting instances of plagiarism when the
text had been copied verbatim was a trivial task but all systems struggled to
detect cases of plagiarism when the text had been altered manually [24].
In this paper we treat candidate document selection as an Information Re-
trieval problem. When the problem is viewed this way the query is formed from
the suspicious document. Documents in the reference collection from which the
suspicious document is plagiarised are treated as relevant while all others are ir-
relevant. An ideal candidate document selection system would return all relevant
documents without returning any irrelevant ones. However, this is unlikely to be
possible in practise and it is important to ensure that all relevant documents
are identified since any which are not cannot be identified later. Consequently,
recall is more important than precision for this problem.
We find that the Information Retrieval approach is suitable for the candidate
document selection problem and show that it outperforms a previously reported
approach on two data sets. Query expansion is applied to deal with cases in which
the suspicious document contains an obfuscated version of the original. Various
query expansion techniques are compared and the most successful is based on a
corpus-derived paraphrase lexicon. Analysis of the approach on different levels
of obfuscation is also carried out and it is found that the improvement over the
alternative approach is greatest for highly obfuscated texts.
Retrieving Candidate Plagiarised Documents Using Query Expansion 209
This paper is organised as follows: Section 2 discusses different existing
techniques for extrinsic plagiarism detection; Section 3 presents our proposed
approach; Section 4 describes the experimental setup (baselines, datasets and
evaluation measures); Section 5 presents the results and analysis of the exper-
iments; Finally Section 6 concludes the paper and discusses avenues for future
work.
2 Background
Various approaches to the candidate document selection problem have been ex-
plored. One method models the documents in the collection as a probability
distribution and compares them by computing Kullback-Leibler divergence [1].
Documents are converted into probability distributions by first removing stop
words and stemming and then computing tf.idf weights for the remaining word
unigrams. If Pd and Qs are the distributions for, respectively, a document in
the reference collection, d, and a suspicious document, s, the Kullback-Leibler
divergence between them (over a feature vector X) is computed as follows:
KLd(P‖Q) =
∑
x∈X
(P (x) −Q(x))log P (x)
Q(x)
(1)
Applying this approach to candidate document selection was shown to re-
duce the execution time of a plagiarism detection system by over 90% and also
improved its accuracy [1].
In the 1st and 2nd international competitions on plagiarism detection [27,24],
the majority of systems used IR-based approaches for candidate document selec-
tion. Using this approach the entire source collection is converted to fixed length
word n-grams (or fingerprints) and indexed. Each word n-gram in a suspicious
document was queried in the index and source documents with word n-grams
above some pre-defined threshold were marked as potential candidates. However,
these approaches only aim to detect candidate documents that have been copied
verbatim with minor changes.
Various approaches have been proposed for identifying plagiarism when the
source document has been altered more substantially. Uzuner et al. [28] demon-
strated an improvement in performance using syntactic features extracted using
context free grammar. Recently, Chong et al. [7] applied various pre-processing
and NLP techniques (e.g. sentence segmentation, tokenization, part of speech
tagging, chunking and dependency parsing) to normalise documents and found
that to improve the performance of existing plagiarism detection approaches.
Mozgovoy et al. [20] also showed that applying parsing to normalize the effect of
word reordering improves performance for plagiarism detection. However these
approaches fail to identify semantic similarity between a pair of documents.
Chen [6] used WordNet synsets and relationships (hypernyms/hyponyms) be-
tween synsets to identify semantic similarity between a plagiarized and source
document. Ceska et al. [5] used WordNet’s first sense, all senses and sense se-
lection after word sense disambiguation to detect synonym replacement in a
210 R.M.A. Nawab, M. Stevenson, and P. Clough
suspicious document. However, results with WordNet did not show any signifi-
cant improvement as compared to a baseline approach.
Our proposed framework for candidate document selection is based on IR (see
Section 3.1) and incorporates query expansion to identify obfuscated documents.
These are based on 1) term co-occurrence statistics, 2) WordNet and 3) a corpus-
derived paraphrase lexicon [3]. To our knowledge paraphrase lexicons have not
been previously used for plagiarism detection.
3 Approach
This section describes our proposed IR-based framework for candidate document
retrieval and different query expansion approaches explored to improve retrieval
performance.
3.1 Retrieval Model
A suspicious document is split into sentences (queries). The source collection
is queried against each sentence from the suspicious document to retrieve a set
of potential source documents. Terms are weighted using the TF.IDF weight-
ing scheme and documents against a query term were matched using the TAAT
(Term-At-A-Time) approach to compute the similarity score. The top Q docu-
ments are selected for each query and the results of multiple sentences merged
using a score-based fusion approach (CombSUM method [10]) to generate a
ranked list of source documents. In the CombSUM method the final similarity
score, Sfinalscore, is obtained by adding the similarity scores obtained for each
sentence q:
Sfinalscore =
Nq∑
q=1
Sq (d) (2)
where Nq is the total number of sentences (queries) to be combined and Sq (d)
is the similarity score of a document d for a query q. The top K documents
in the ranked list generated by CombSUM are marked as potential candidate
documents.
3.2 Query Expansion
Various approaches have been proposed for query expansion. For example, auto-
mated techniques using pseudo relevance feedback or utilising knowledge bases
(e.g. thesauri) and interactive techniques involving users in selecting the expan-
sion terms [9]. In these experiments only content words were expanded (stop
words and numbers are ignored). An original query term is expanded with one
or more expansion terms and different weights were assigned to the expansion
terms. The query expansion methods used are described below.
Retrieving Candidate Plagiarised Documents Using Query Expansion 211
Pseudo Relevance Feedback. Pseudo Relevance Feedback is a widely ex-
plored approach to query expansion that uses co-occurrence statistics to analyse
the top ranked documents and generate additional query terms. Rocchio’s well-
established approach to relevance feedback [25] was used.
Query Expansion using WordNet. WordNet is a lexical database that can
be used to suggest terms for query expansion. WordNet is organised around
synsets, sets of synonymous words. For each query term WordNet is consulted
to identify the synsets in which it occurs and additional search terms selected
from them. Some query terms occur in more than one synset and two approaches
are used to identify search terms in these cases: first sense and all senses. In
the first sense approach additional search terms are selected only from the first
(most frequent) synset containing the query term. In the all senses approach
additional search terms can be selected from any of the synsets containing the
query term.
After the synset(s) have been identified from WordNet the additional search
terms have to be selected from them. All term in the synset(s) are ranked based
on their frequency in the BNC corpus and the highest ranked one used as an
additional search term.2
For example, the query “it was first published in the century magazine” is ex-
panded as ‘it was first one∧w published print∧w in the century magazine mag∧w”
using the first sense approach (where w is the weight assigned to the additional
search terms). The word “century” was not expanded in this case because the
first sense of this word in WordNet only contains the word itself. When the
all senses approach is used the same query is expanded as “it was first low∧w
published issue∧w in the century hundred∧w magazine cartridge∧w”.
Query Expansion using a Paraphrase Lexicon. One weakness of WordNet-
based approaches is that it is unlikely to detect phrase substitutions. For exam-
ple, if the word “rapidly” is replaced by the phrase “very fast” this substitution
will not be detected using WordNet alone. One way to avoid this problem is to
make use a paraphrase lexicon. Table 1 shows paraphrases of the term “first”
generated by an automatic paraphrase generation system [3].
Table 1. An example output of automatic paraphrase generation system for the word
“first”
Word Paraphrase Probability Score
first first and foremost 0.0927
first very first 0.0250
first particular 0.0213
first first of all 0.0192
first most important 0.0125
2 BNC frequency list for all the words http://www.kilgarriff.co.uk/bnc-readme.html
was used.
212 R.M.A. Nawab, M. Stevenson, and P. Clough
The paraphrase lexicon was used for query expansion by ranking the possi-
ble paraphrases of a query term based on their probability score. For example
the query “it was first published in the century magazine” will be expanded as
“it was first first∧w and∧w foremost∧w published advertised∧w in the century
cooperation∧w magazine journal∧w”.
4 Experimental Setup
4.1 Implementation
The approach is implemented using Terrier [21]. Terrier includes an implementa-
tion of Rocchio’s method for relevance feedback. Documents were pre-processed
by splitting them into sentences (using the Natural Language Toolkit),3 con-
verting the text into lower case and removing all non-alphanumeric characters.
Stemming was carried out using the Porter Stemmer and stop words removed.
Our approach requires various parameters to be set, including the number of
documents retrieved for each query (see Section 3.1) and the weights assigned
to the terms added by some of the query expansion approaches (see Section
3.2). A keyword was expanded with 1, 2 and 3 additional search terms. Each
additional term was assigned weight from the set: {1.0, 0.5, 0.1, 0.05, 0.01}.
Number of documents retrieved, number of additional search terms added and
their weights were set automatically using three fold cross validation. Each cor-
pus was split into three folds with two being used to identify the optimal values
for the parameters and the remaining third for evaluation. The results of the
three runs are then averaged.4
4.2 Datasets
Evaluation was carried out using two datasets: 1) the PAN-PC-10 corpus [23]
and 2) the Short Answer Corpus [8]. Both of these corpora consist of simulated
examples of plagiarism created by asking subjects to manually paraphrase texts
with varying degrees of obfuscation.
The PAN-PC-10 corpus contains a variety of different types of plagiarism,
including examples which are created automatically by randomly altering text
and inserting synonyms from WordNet. We did not make use of the automatic
examples in our experiments since the types of obfuscation they contain are
different from those that would be observed in real cases of plagiarism and the
fact that they were created using WordNet would be an unfair advantage to
some of our query expansion approaches. The examples of simulated plagiarism
in this corpus were generated by asking workers on Amazon’s Mechanical Turk
to rewrite texts.5 The PAN-PC-10 corpus contains 411 documents representing
3 http://www.nltk.org/
4 Best results were obtained using 1 additional term with 0.1 weight. Performance
decreases as the number of additional terms increases.
5 https://www.mturk.com/mturk/welcome
Retrieving Candidate Plagiarised Documents Using Query Expansion 213
examples of simulated plagiarism and a source collection of 10,479 documents
which were used for our experiments.
The Short Answer Corpus contains examples of answers to five questions
on a range of topics in Computer Science. 57 of the documents were created
by asking subjects to plagiarise a Wikipedia page which answered one of the
questions. Subjects were asked to use a variety of levels of obfuscation to create
their answers: 1) copy (none obfuscation), 2) light revision (low obfuscation) and
3) heavy revision (high obfuscation). The corpus also contains 38 documents that
are not plagiarised and were created by asking subjects to answer the questions
without reference to Wikipedia. In total the corpus contains 100 documents, a
total of 95 plagiarised and non-plagiarised answers as well as the five Wikipedia
pages which were plagiarised from.
The documents in the Short Answer Corpus contain examples of simulated
plagiarism which are useful for evaluation but the corpus is too small for experi-
ments on candidate document selection. The corpus was extended with examples
from the web to make the problem of candidate document selection more chal-
lenging. The five questions were used as queries for the Google search engine and
the top 99 articles retrieved stored. These were combined with the five original
Wikipedia articles to create a source collection of 500 documents. The suspi-
cious documents in the collection were formed from the set of 57 plagiarised
documents.
4.3 Evaluation Measure
The goal of the candidate document retrieval task is to identify all the source
document(s) for each suspicious document while returning as few non-source
documents as possible. It is important for all source documents to be included
in the top ranked documents returned by the system since otherwise they will
not be identified during later stages of processing. Consequently, recall is more
important than precision for this problem. Macro-averaged recall for the top K
documents was used as the evaluation measure.
5 Results and Analysis
Table 2 shows the results for simulated examples of plagiarism in the PAN-PC-
10 corpus. Macro-averaged recall figures are reported for the top 5, 10, 15 and
20 documents. Results show that the proposed IR-based framework outperforms
the Kullback-Leibler divergence approach and the results can be improved using
query expansion.
As expected, the macro-averaged recall figure increases as the number of re-
trieved documents increases. However, the maximum recall that is achieved when
20 candidate documents are retrieved without query expansion is only 62.74%,
highlighting the difficulty of this problem. The reason for very low performance
in the PAN-PC-10 corpus is that small, obfuscated passages were inserted ran-
domly into suspicious documents, which made the retrieval task difficult. The
214 R.M.A. Nawab, M. Stevenson, and P. Clough
Table 2. Performance with the PAN-PC-10 corpus (highest results in bold)
Avg. Recall for top K documents
Obfuscation Approach 5 10 15 20
Simulated
Kullback-Leibler 0.1707 0.1975 0.2092 0.2178
No Query Expansion 0.5109 0.5758 0.6095 0.6274
Pseudo Relevance 0.4497 0.5182 0.5673 0.5969
First Sense 0.5158 0.5852 0.6277 0.6472
All Senses 0.5215 0.5851 0.6249 0.6456
Paraphrase 0.5211 0.6062 0.6359 0.6602
Table 3. Performance for different types of obfuscations in the short answer corpus
Avg. Recall for top K documents
Obfuscation Approach 1 2 3 4 5
None
Kullback-Leibler 0.9444 1.0000 1.0000 1.0000 1.0000
No Query Expansion 0.9444 1.0000 1.0000 1.0000 1.0000
Pseudo Relevance 0.5263 0.6842 0.7368 0.7368 0.8421
First Sense 0.8889 0.9444 0.9444 1.0000 1.0000
All Senses 0.7778 0.9444 0.9444 1.0000 1.0000
Paraphrase 0.8889 0.9444 0.9444 0.9444 1.0000
Low
Kullback-Leibler 0.5789 0.8421 0.9474 0.9474 0.9474
No Query Expansion 0.6316 0.8421 0.9474 1.0000 1.0000
Pseudo Relevance 0.5263 0.6842 0.7368 0.7368 0.8421
First Sense 0.5789 0.6316 0.8947 0.8947 0.9474
All Senses 0.3684 0.8947 0.8947 0.9474 1.0000
Paraphrase 0.4211 0.9474 1.0000 1.0000 1.0000
High
Kullback-Leibler 0.3684 0.4211 0.4737 0.6316 0.6316
No Query Expansion 0.5789 0.6842 0.7368 0.7895 0.8947
Pseudo Relevance 0.4737 0.5263 0.6316 0.6842 0.6842
First Sense 0.3684 0.6316 0.7368 0.7895 0.9474
All Senses 0.2105 0.6316 0.6842 0.8421 0.9474
Paraphrase 0.4211 0.7368 0.8421 0.8947 0.9474
average length of an inserted passage was just 55 words and on average two
passages were inserted into each document.
All query expansion approaches improve recall, with the exception of pseudo
relevance feedback. The possible reason for low performance is that expansion
terms are selected from a set of documents based on co-occurrence statistics
which could result in selection of noisy expansion terms. Query expansion based
on WordNet improves performance and similar performance is observed for the
first sense and all senses approaches. The best results are obtained using the
paraphrase lexicon, except for average recall for the top 5 documents. Although
small the differences are statistically significant (Wilcoxon signed-rank test, p <
0.05) compared to the next highest result. In the case of average recall at 5
documents, the difference between using all senses and the paraphrase lexicon
Retrieving Candidate Plagiarised Documents Using Query Expansion 215
is not significant. This highlights the fact that phrases have been replaced with
lexical equivalents (bearing the same meaning) to hide plagiarism.
Macro-averaged recall scores for the top 1 to 5 candidate documents for the
short answer corpus are reported in Table 3. The Short Answer Corpus is smaller
than the PAN-PC-10 corpus so results are reported for fewer of the candidate
documents identified. Results are shown for the various levels of obfuscation
included in the corpus: none, low and high.
As expected, retrieval performance decreases as the level of obfuscation in-
creases. Both the reported approach and the Kullback-Leibler divergence method
achieve 100% recall for the two lowest levels of obfuscation (none and low), indi-
cating that detecting plagiarism is straightforward when the text has not been
significantly altered. However, maximum recall achieved for high obfuscation
is lower for all approaches, demonstrating that the problem is more difficult.
The Kullback-Leibler divergence performs well for none and low obfuscations
but performance significantly decreases for high obfuscation indicating that the
approach is not suited to the detection of plagiarism when the text has been
significantly altered. However, performance of the proposed IR-based approach
is more robust to the level of obfuscation.
Pseudo relevance feedback does not improve the performance of the IR-based
approach for any of the three levels of obfuscation in the Short Answer Corpus.
The effect of the WordNet and paraphrase lexicon methods for query expansion
depends on the level of obfuscation. When the text has not been altered (none)
performance actually drops for the first few documents retrieved when these
approaches are applied. However, using query expansion improves performance
when texts have been altered (low and high). The best results achieved for the
most altered documents (high) are achieved using query expansion. In particular,
the best results achieved for the most altered documents are achieved using the
paraphrase lexicon for all but the first retrieved document. The improvement
achieved using this approach compared with when no query expansion is used is
statistically significant (Wilcoxon signed-rank test, p < 0.05).
5.1 Analysis
We carried out further analysis to find out the percentage of queries for which
the ranking is “higher”, “lower” or remains “same” when a query expansion
approach is applied as compared to no query expansion approach (see Table 4).
The rank of a query (suspicious document) was considered in top 5 and 20
documents for the short answer and PAN-PC-10 corpus respectively.
It can be noted that on large dataset (PAN-PC-10 corpus), the percentage of
queries with “lower” rank is at a minimum for the paraphrasing lexicon (13.38%)
and a maximum for pseudo relevance feedback (38.93%). This shows that ex-
pansion with lexical equivalents is more suitable than other query expansion
approaches. In the short answer corpus (57 queries), the percentage of queries
with “lower” rank is higher (26.32% for paraphrase lexicon and first sense),
however overall the paraphrase lexicon performs better than other query expan-
sion approaches. A possible reason for the overall low performance is vocabulary
216 R.M.A. Nawab, M. Stevenson, and P. Clough
Table 4. Query by query performance. Number of queries for which the ranking is
higher, lower or remained same using a query expansion.
No. of Queries (%) effecting Rank
Corpus Approach Higher Lower Same
PAN-PC-10
Pseudo Relevance 56 (13.62) 160 (38.93) 130 (31.63)
First Sense 69 (16.79) 68 (16.54) 211 (51.34)
All Senses 69 (16.79) 72 (17.52) 211 (51.34)
Paraphrase 66 (16.06) 55 (13.38) 231 (56.20)
Short Answer
Pseudo Relevance 6 (10.53) 22 (38.59) 29 (50.87)
First Sense 9 (15.79) 15 (26.32) 33 (57.89)
All Senses 7 (12.28) 20 (35.08) 30 (52.63)
Paraphrase 10 (17.54) 15 (26.32) 32 (56.14)
mismatch: the plagiarized documents in the Short Answer Corpus are from the
Computer Science domain, whereas the paraphrase lexicon is generated using
parallel documents from the European Parliament.
6 Conclusion
This paper describes and evaluates a new approach to the problem of candi-
date document selection for plagiarism detection. In particular we have focused
on cases when the plagiarized version has been highly obfuscated (simulated
plagiarism) as this presents the greatest challenge to automated plagiarism
detection systems. An IR-based approach using score-based fusion and query
expansion shows a significant improvement over an existing state-of-the-art ap-
proach (Kullback-Leibler divergence). Various query expansion techniques have
been experimented with, including pseudo relevance feedback and automatic
query expansion using WordNet and a paraphrase lexicon generated from the
Europarl corpus. As far as we know, a paraphrase lexicon has not previously
been applied to the candidate document selection problem.
Evaluation was carried out using two existing corpora containing examples of
simulated plagiarism. Results show that the proposed approach outperforms ex-
isting techniques and performance can be increased by applying query expansion
methods based on WordNet and a paraphrase lexicon. Analysis shows greatest
improvements with query expansion for cases when the text has been highly
obfuscated. The proposed approach is more robust to changes in the source
documents than existing methods.
Results demonstrate the potential benefits of using query expansion in cases
when texts have been highly obfuscated, however there is still room for improve-
ment. Avenues for future work include experimenting with various methods for
rank fusion and generating domain-specific paraphrase lexicons. The combina-
tion of multiple sources of expansion terms may also provide better domain
coverage and thereby increase performance. Dealing with causes of obfuscation
other than term substitutions is also an area for further research.
Retrieving Candidate Plagiarised Documents Using Query Expansion 217
Acknowledgements. Rao Muhammad Adeel Nawab thanks the COMSATS
Institute of Information Technology, Islamabad, Pakistan for funding this work
under the Faculty Development Program (FDP).
References
1. Barrón-Cedeño, A., Rosso, P., Bened́ı, J.: Reducing the Plagiarism Detection
Search Space on the Basis of the Kullback-Leibler Distance. In: Gelbukh, A. (ed.)
CICLing 2009. LNCS, vol. 5449, pp. 523–534. Springer, Heidelberg (2009)
2. Boisvert, R., Irwin, M.: Plagiarism on the rise. Communications of the ACM 49(6),
23–24 (2006)
3. Callison-Burch, C.: Syntactic constraints on paraphrases extracted from parallel
corpora. In: Proceedings of the Conference on Empirical Methods in Natural Lan-
guage Processing, pp. 196–205. ACM (2008)
4. Campbell, C.: Writing with other’s words: Using background reading text in aca-
demic compositions. In: Kroll, B. (ed.) Second Language Writing: Research Insights
for the Classroom, pp. 211–230. Cambridge University Press, Cambridge (1990)
5. Ceska, Z.: Plagiarism Detection Based on Singular Value Decomposition. In: Nord-
ström, B., Ranta, A. (eds.) GoTAL 2008. LNCS (LNAI), vol. 5221, pp. 108–119.
Springer, Heidelberg (2008)
6. Chen, C., Yeh, J., Ke, H.: Plagiarism Detection using ROUGE and WordNet.
Journal of Computing 2(3), 34–44 (2010)
7. Chong, M., Specia, L., Mitkov, R.: Using Natural Language Processing for Auto-
matic Detection of Plagiarism. In: Proceedings of the 4th International Plagiarism
Conference (IPC 2010), Newcastle, UK (2010)
8. Clough, P., Stevenson, M.: Developing A Corpus of Plagiarised Short Answers. In:
Language Resources and Evaluation: Special Issue on Plagiarism and Authorship
Analysis. Springer, Heidelberg (2010)
9. Efthimiadis, E.: Query expansion. Annual Review of Information Systems and
Technology (ARIST) 31, 121–187 (1996)
10. Fox, E.A., Shaw, J.A.: Combination of Multiple Searches. In: Harman, D.K. (ed.)
Proceedings TREC-2, pp. 243–249 (1994)
11. Johns, A., Myers, P.: An analysis of summary protocols of university ESL students.
Applied Linguistics 11, 253–271 (1990)
12. Judge, G.: Plagiarism: Bringing Economics and Education Together (With a Little
Help from IT). Computers in Higher Education Economics Review 20(1), 21–26
(2008)
13. Keck, C.: The use of paraphrase in summary writing: A comparison of l1 and l2
writers. Journal of Second Language Writing 15, 261–278 (2006)
14. Lane, P., Lyon, C., Malcolm, J.: Demonstration of the Ferret plagiarism detector.
In: Proceedings of the 2nd International Plagiarism Conference (2006)
15. Martin, B.: Plagiarism: a misplaced emphasis. Journal of Information Ethics 3(2),
36–47 (1994)
16. Maurer, H., Kappe, F., Zaka, B.: Plagiarism - A Survey. Journal of Universal
Computer Science 12(8), 1050–1084 (2006)
17. McCabe, D.: Research report of the center for academic integrity (2005),
http://www.academicintegrity.org
18. McCabe, D., Butterfield, K., Trevino, L.: Academic Dishonesty in Graduate Busi-
ness Programs: Prevalence, Causes, and Proposed Action. Academy of Manage-
ment Learning and Education 5(3), 1–294 (2006)
218 R.M.A. Nawab, M. Stevenson, and P. Clough
19. Meyer zu Eissen, S., Stein, B., Kulig, M.: Plagiarism detection without refer-
ence collections. In: Advances in Data Analysis, pp. 359–366. Springer, Heidelberg
(2007)
20. Mozgovoy, M., Kakkonen, T., Sutinen, E.: Using Natural Language Parsers in
Plagiarism Detection. In: Proceedings of SLaTE 2007 Workshop, Pennsylvania,
USA (2007)
21. Ounis, I., Amati, G., Plachouras, V., He, B., Macdonald, C., Johnson, D.: Terrier
Information Retrieval Platform. In: Losada, D.E., Fernández-Luna, J.M. (eds.)
ECIR 2005. LNCS, vol. 3408, pp. 517–519. Springer, Heidelberg (2005)
22. Park, C.: In other (people’s) words: plagiarism by university students – literature
and lessons. Assessment and Evaluation in Higher Education 28(5) (2003)
23. Potthast, M., Stein, B., Barrón-Cedeño, A., Rosso, P.: An Evaluation Framework
for Plagiarism Detection. In: Proceedings of the 23rd International Conference on
Computational Linguistics (COLING 2010), pp. 997–1005 (2010)
24. Potthast, M., Stein, B., Eiselt, A., Cedeño, A., Rosso, P.: Overview of the 2nd
International Competition on Plagiarism Detection. In: Proceedings of the CLEF
2010 Workshop on Uncovering Plagiarism, Authorship and Social Software Misuse,
Padua, Italy (2010)
25. Rocchio, J.: Relevance feedback in information retrieval. In: The SMART Retrieval
System: Experiments in Automatic Document Processing, pp. 313–323 (1971)
26. Shivakumar, N., Garcia-Molina, H.: SCAM: A Copy Detection Mechanism for Dig-
ital Documents. In: Proceedings of the 2nd Annual Conference on the Theory and
Practice of Digital Libraries, Texas, USA (1995)
27. Stein, B., Rosso, P., Stamatatos, E., Koppel, M., Agirre, E.: 3rd PAN Workshop on
Uncovering Plagiarism, Authorship and Social Software Misuse. In: 25th Annual
Conference of the Spanish Society for Natural Language Processing (SEPLN), pp.
1–77 (2009)
28. Uzuner, O., Katz, B., Nahnsen, T.: Using syntactic information to identify plagia-
rism. In: Proceedings of the 2nd Workshop on Building Educational Applications
Using NLP, pp. 37–44. Association for Computational Linguistics (2005)
