We introduce a new measure on linguistic features,
called stability, which captures the extent to which a lan-
guage element such as a word or a syntactic construct is
replaceable by semantically equivalent elements. This
measure may be perceived as quantifying the degree of
available “synonymy” for a language item. We show that
frequent, but unstable, features are especially useful as
discriminators of an author’s writing style.
Introduction
We often wish to find linguistic markers that distinguish
the writing style of a particular author or class of authors. We
seek features that are typically used variably by different au-
thors, but are used consistently by any given author—or, at
least, by the author whose writing we wish to distinguish.
Obviously, these markers will differ from author to author.
In this article, we aim to identify the pool of potentially
useful linguistic features from which markers might fruit-
fully be chosen. To be precise, we do not seek features that
distinguish a particular author but rather author-independent
criteria for ranking features which are worth considering
when seeking distinguishing characteristics of any given
author.
Consider some examples. If a particular author were
found to use the word awful more frequently than other
authors, this would certainly be worth noting. After all, the
word bad is generally a reasonable, and more common,
alternative to awful. The fact that our author chooses to use
the word awful frequently therefore likely reflects deliberate
stylistic choice that we might profitably exploit for identify-
ing the author’s writing. What about the word touchdown?
The frequency of use of touchdown is a rather poor feature
for identifying an author’s style. There actually are two dis-
tinct reasons that this is so. First, there is no alternative word
for expressing the concept touchdown, so its use does not
reflect stylistic choice. Second, touchdown is tightly tied to a
particular topic, so its frequency of use in one corpus is
unlikely to reflect the frequency with which it will be used in
other documents, which might concern other topics. By con-
trast, awful is commonly used across most topics and has
plausible alternatives. Some words satisfy one of these two
criteria, but not both. For example, perspire offers a plausi-
ble alternative (sweat), but is not frequently used across a
broad range of topics. On the other hand, words such as blue
and ten are used across topics, but do not have common
alternatives.
To summarize, we seek linguistic features that might
reflect deliberate stylistic choice by a given author. Such
features will tend to be used across topics and will offer
plausible linguistic alternatives. The first criterion is rela-
tively easy to approximate by checking overall frequency of
use across different topic areas. We will focus on a formal
definition of the second criteria: How can we determine the
extent to which a particular linguistic feature offers alterna-
tives? Although the examples we gave are all words, the
criteria we define will extend beyond words to other types of
linguistic features.
Linguistic Feature Stability
Data-driven approaches to text and language processing
apply various quantitative measures to linguistic elements
such as words and terms. These measures capture important
properties of language items and are often utilized in various
ways for different computational tasks such as information
retrieval, text classification, terminology extraction, and
statistical thesaurus construction.
In this article, we introduce a new type of measure for
linguistic elements that we call meaning-preserving stability,
or stability for short. It captures the degree to which a lin-
guistic element or construct can be substituted for in a piece
of text without affecting the text meaning. This measure 
is applied most intuitively to words and terms, but also is
applicable to other types of linguistic constructs such as
JOURNAL OF THE AMERICAN SOCIETY FOR INFORMATION SCIENCE AND TECHNOLOGY, 57(11):1519–1525, 2006
Feature Instability as a Criterion for Selecting Potential
Style Markers
Moshe Koppel, Navot Akiva, and Ido Dagan
Computer Science Department, Bar Ilan University, Ramat Gan 52900, Israel.
E-mail: {koppel, navot, dagan}@cs.biu.ac.il
Accepted March 24, 2005
© 2006 Wiley Periodicals, Inc. • Published online 14  July 2006 in Wiley
InterScience (www.interscience.wiley.com). DOI: 10.1002/asi.20428
1520 JOURNAL OF THE AMERICAN SOCIETY FOR INFORMATION SCIENCE AND TECHNOLOGY—September 2006
DOI: 10.1002/asi
part-of-speech sequences. Stability captures interesting
properties in language and thus seems interesting from a
purely scientific point of view, but it also has potential use
within applied tasks. Stability can be perceived as quantify-
ing the typical degree of available “synonymy” for a language
element while generalizing the notion of synonymy to any
type of linguistic feature rather than being restricted to its
common use for words only.
To make this a bit more concrete, let us begin with a sim-
ple example. Consider the three sentences
John was lying on the couch next to the window.
John was reclining on the sofa by the window.
John had been lying on the couch near the window.
The three sentences convey (approximately) the same
message. Some of the words remain invariant in all three
versions (e.g., John, window) while some are replaced by
other words (e.g., was: had been; couch: sofa; next to: by:
near; lying: reclining) which do not significantly change the
meaning of the sentence. More generally, consider features
of a text such as word or phrase frequencies, frequencies of
syntactic structures, or any other feature whose value can be
measured in a given text. Roughly speaking, the stability of
such a feature is the extent to which the measured value of
that feature tends to remain invariant across different texts
that convey the same meaning. For example, proper nouns
are very stable while words with many synonyms are
unstable.
Obtaining training material for measuring stability is a
difficult challenge, requiring alternative versions of texts
that carry—in the ideal case, exactly—the same meaning.
“Parallel” (monolingual) texts of this nature have been used
for automatic paraphrase extraction, using either multiple
translations of the same text or news stories from different
sources that describe the same event (Barzilay & McKeown,
2001; Shinyama, Sekine, Sudo, & Grishman, 2002). The
disadvantage of these types of training material is that they
rely on the fact that different people have manually created
different versions of the same text. Such manually created
versions are not easily available for most texts. To lighten
the supervision requirements, we have used a machine trans-
lation (MT) system to translate our training corpus back-and-
forth via multiple languages, thus obtaining several English
versions of the same text. Indeed, the use of automatic trans-
lation to generate training data is problematic for several
reasons, as discussed later. Yet, the combination of MT with
other technologies that can leverage the stability measure,
such as style-based text categorization, provides appealing
advantages that reduce the overall dependency on particular
training materials.
In the remainder of the article, we first present the general
notion of stability and define a concrete stability measure.
We then present empirical measurements of stability and
illustrate the properties that it captures. Finally, we demon-
strate how stability can be utilized effectively for feature
selection in style-based text classification.
A Stability Measure
A stability measure is a quantitative measure that corre-
lates with a feature’s tendency to be preserved across differ-
ent meaning-preserving variants of a text. We formally
define a specific stability measure and consider empirical
results of stability experiments on the Reuters 21578 corpus
(Lewis, 1997) and on the British National Corpus (BNC).
Let {d1, d2, . . . , dn} be a set of documents (or text seg-
ments) and let be m  1 different versions of
di where the meanings of the m versions are all roughly iden-
tical.1 For any measurable feature c, let be the value of c in
document Let us develop the final formula in two steps.
Step 1. Define the stability of a feature in multiple versions
of a single document.
Let Then the stability of a feature c in docu-
ment di is defined to simply be the usual (normalized)
entropy measure:
If is a frequency measure, we can think of as the
probability that a random appearance of c in di is in version
Then H(ci) is just the usual entropy measure, normalized
by log m to keep the range of stability values to [0, 1]. Thus,
for example, if a feature assumed the identical value in every
version of a document, its stability would be 1. If a feature
assumed a positive value in a single version of the docu-
ment, but 0 in all others, its stability would be 0.
Step 2. Extend the definition to multiple documents
{d1, d2, . . . , dn}.
The impact that a given document has on the overall mea-
sure is defined to be proportional to the average value of the
feature in the various versions of that document (e.g., when
c is a frequency measure of some attribute, documents in
which the attribute is more frequent will contribute propor-
tionately more to the overall stability of the feature in the
corpus.)
Let Then 
This formula can be transformed to the equivalent
formula:
(1)S(c) 
a
i
c ki log ki  a
j
c ji  log c
j
i d
K * log m
S(c)  a
i
c a ki
K
b * H(ci) dK  a
i
ki.
dji .
c jikic ji
H(ci) 
a
j
ac ji
ki
 log 
c ji
ki
b
log m
ki  g jc ji .
dji .
c ji
5d1i , d2i , . . . , dmi 6
1Since stability is an empirical measure based on statistical data, its de-
finition does not depend on the exact notion of “meaning preservation” that
holds across the different text versions. Applying the stability measure
would make sense whenever the different text versions represent alterna-
tive, interchangeable linguistic choices.
JOURNAL OF THE AMERICAN SOCIETY FOR INFORMATION SCIENCE AND TECHNOLOGY—September 2006 1521
DOI: 10.1002/asi
Measuring Feature Stability Empirically Using
Machine Translation
To empirically measure stability, one needs several text
variants that convey the same meaning, or at least have sub-
stantial overlap. For example, we might consider translations
of the same text by different translators, and to a much lesser
extent, reports on the same event by different reporters or
journalists. All these are relatively hard to obtain for experi-
mental purposes. One interesting way to generate text vari-
ants artificially is to use an MT system to translate the text to
another language and then translate it back to the original
language. For our experiments, we used SystranPro to trans-
late each document in the Reuters 21578 corpus into each of
five different languages (French, German, Spanish, Italian,
Portuguese) and back into English. To check that the measure
is not overly dependent on the base corpus, we did the same
to a set of several-hundred book-length documents from the
BNC. We applied formula (1) to frequencies of words, 
parts-of-speech n-grams, and other linguistic features.
An obvious weakness of this experiment is that it is sub-
ject to the idiosyncrasies of Systran and, to a lesser extent, of
the particular corpus. We have attempted to mitigate this
affect by using five translation packages. This is only a
partial solution since it is likely that all of them share certain
underlying methods and programming code. Yet, as the
experiments will indicate, this setting does provide an inter-
esting “grasp” of feature stability, probably because the vast
knowledge encoded in the five translation systems does
capture much of the inherent phenomena that determine
linguistic stability.
We will consider the stability distributions of various
classes of features as well as some specific features and
discuss why their respective stabilities are particularly high
or low.
Stability Distributions and Examples
In Figure 1, we show a histogram of stabilities of all sin-
gle words in the Reuters corpus. As is evident, the number 
of words in a given stability range descends as the mean of
the range descends. When we look at specific word classes,
a clearer picture emerges. As might be expected, certain
features, such as proper names, are highly stable. All proper
names have stability close to 1. Similarly, numbers have
very high stability. Words with common synonyms such as
aid (.37) and help (.82) are less stable, with the more com-
mon synonym more stable than the less common one. In
extreme cases such as huge and ratio, stability is reduced to
0; the translation system always replaces them by more com-
mon synonyms such as large and proportion, respectively.
In Figure 2a to c, we show histograms of stability of
nouns, verbs, and function words, respectively. While nouns
follow the general pattern with a plurality in the highest
stability range, verbs and function words distribute more
normally. This is because verbs are on average much more
ambiguous than nouns. Similarly, most function words also
are highly ambiguous and are often replaceable with equiva-
lent syntactic constructions.
In Table 1, we consider a selection of function words and
their respective stabilities using Reuters and BNC, respec-
tively. A number of these examples are very instructive.
Words such as and and the do not offer more natural alterna-
tives and are thus stable; however, words such as has and
been are unstable because, for example, the present perfect
0
0.05
0.1
0.15
0.2
0.25
0.9-
1
0.8-
0.9
0.7-
0.8
0.6-
0.7
0.5-
0.6
0.4-
0.5
0.3-
0.4
0.2-
0.3
0.1-
0.2
0-
0.1
Single Words Stability
FIG. 1. Stability histogram of all words. The x axis denotes stability value
ranges, and the y axis denotes the proportion of features receiving the cor-
responding stability values.
0
0.05
0.1
0.15
0.2
0.25
0.3
0.9-10.7-0.80.5-0.60.3-0.40.1-0.2
Nouns Stability
0
0.05
0.1
0.15
0.9-10.7-0.80.5-0.60.3-0.40.1-0.2
Verbs Stability
0
0.05
0.1
0.15
0.2
0.9-
1
0.8-
0.9
0.7-
0.8
0.6-
0.7
0.5-
0.6
0.4-
0.5
0.3-
0.4
0.2-
0.3
0.1-
0.2
0-
0.1
FW  Stability
FIG. 2. Stability histogram for nouns (a). Stability histogram for verbs (b).
Stability histogram for function words (c).
(a)
(b)
(c)
1522 JOURNAL OF THE AMERICAN SOCIETY FOR INFORMATION SCIENCE AND TECHNOLOGY—September 2006
DOI: 10.1002/asi
TABLE 2. Examples of noun-related part-of-speech (POS).
POS Reuters stability BNC stability
NN_IN_DT 0.94 0.97
IN_DT_NN 0.94 0.98
DT_NN_IN 0.93 0.97
NN_IN_NN 0.92 0.89
IN_NN_IN 0.86 0.86
NN_IN_NNP 0.81 0.89
IN_CD_NN 0.76 0.72
CD_NN_IN 0.72 0.58
JJ_NN_NN 0.67 0.80
NN_NN_NN 0.43 0.59
has been is easily replaced by the past tense was. Similarly,
a word such as over is easily replaced either by synonyms
(e.g., above) or alternative constructions (e.g., go over there,
go there, go to there). Note that with a few exceptions the
stabilities yielded by Reuters and by BNC are quite close to
each other.
Let us now consider features other than words. In Table 2,
we consider a selection of trigrams of parts-of-speech and
their respective stabilities. Note, for example, that the trigram
noun_noun_noun is very unstable. A typical occurrence is
“U.S. construction spending,” a tightly wound phrase
invariably unwound into something looser such as “spending
on construction by the U.S.”
Altogether, it can be seen that the stability measure
captures in a unified way different types of semantic
“uniqueness” that are related to both syntactic and lexical
phenomena, including content words, function words, and
part-of-speech sequences.
Style-Based Text Categorization
Our main hypothesis is that frequent, but unstable, fea-
tures are most useful for identifying an author’s style. To
assess this, we will use frequency and stability as criteria for
feature selection in text categorization experiments. First,
we give the necessary background to the text categorization
and feature-selection literature.
Style-based text categorization tasks, such as authorship
attribution (Holmes, 1995; Mosteller & Wallace, 1964), are in
a sense orthogonal to the more common problem of catego-
rization by topic (Lewis & Ringuette, 1994; Schutze, Hull, &
Pedersen, 1995; Sebastiani, 2002). For style-based catego-
rization, we seek features that are roughly invariant within the
documents of a given author (or, more broadly, style class),
but variant from author to author. Typically, a promising set of
discriminating features is chosen, and then training examples
from each category and some machine-learning algorithm are
employed to produce a model for categorizing.
Since the number of potential discriminating features is
often uncomfortably large, feature-selection methods are
sometimes used to select out a particularly promising set of
features. A number of these methods, conveniently summa-
rized in Sebastiani (2002), have proven to be reasonably
successful for topical categorization. Typically, the features
selected by these methods are those that, individually, dis-
criminate well on the training corpus. While such methods
do tend to eliminate useless features, they sometimes do
harm by preempting the learning algorithms they are meant
to serve: The learning algorithms themselves, by taking into
account dependencies among features, eliminate useless fea-
tures in more subtle ways than these direct feature-selection
methods. Furthermore, in the case of style-based categoriza-
tion, these methods can lead to overfitting by focusing atten-
tion on features that are highly correlated with one of the
authors for reasons that might be specific for the topics dis-
cussed in the training corpus, but unrelated to the author’s
generic style.
A different approach to feature selection for style-based
categorization is that used by Mosteller and Wallace (1964)
in their seminal work on authorship discrimination.
Mosteller and Wallace simply chose a set of features that are
not dependent on the training corpus but rather have certain
appropriate universal properties. In their case, the features
chosen were function words, which were deemed topic-
independent. This is a perfectly sensible approach, but it
does not offer the flexibility of ranking features so that more
or fewer can be chosen and limits consideration in advance
to a specific set of lexical features.
The main hypothesis of this article is that those linguistic
features that are both unstable and frequent are those that are
most useful for style-based text categorization. Indeed,
many function words are prime examples of frequent, but
unstable, features; there are many other such features. The
hypothesis makes intuitive sense: Stable features are ones
which do not offer viable meaning-preserving alternatives,
so differences in usage of stable features between authors
more likely reflect irrelevant differences in content than dif-
ferences in style; differences in usage of frequent unstable
features are likely to reflect different stylistic choices. Since
instability can be ranked, we can choose more or fewer fea-
tures as is needed, possibly using cross-validation to optimize.
Moreover, as we have seen, unlike function words, the stabil-
ity criterion can be applied to any type of linguistic feature,
whether lexical, syntactic, complexity-based, and so on.
TABLE 1. Examples of function words ranked by their respective stabili-
ties using Reuters and British National Corpus (BNC), respectively.
Function word Reuters BNC Function word Reuters BNC
and 0.99 0.99 from 0.77 0.79
the 0.97 0.99 by 0.76 0.75
not 0.95 0.97 been 0.66 0.72
is 0.94 0.95 which 0.56 0.58
of 0.94 0.95 has 0.52 0.27
it’s 0.87 0.88 over 0.47 0.54
JOURNAL OF THE AMERICAN SOCIETY FOR INFORMATION SCIENCE AND TECHNOLOGY—September 2006 1523
DOI: 10.1002/asi
A ranked list of the 50 most highly ranked words accord-
ing to the product of instability and log-frequency (using the
Reuters and BNC corpora) is shown in the Appendix. Note
that due to the specialized nature of the Reuter corpus, many
financial words, which are not particularly relevant to other
contexts, are highly ranked. Overall, of the 400 top words in
each ranked list (BNC and Reuters), 43% are common to
both. We will see later that even when stability and frequency
are measured using an “inappropriate” corpus, they yield
superior results in the selection of potential style markers.
Experiments
In our first experiment, we will attempt to learn to distin-
guish the writing style of Anne Bronte from that of her sister,
Charlotte Bronte. This is a particularly difficult attribution
problem because the authors came from identical social and
linguistic backgrounds and wrote in what appear to be very
similar styles. We consider two books by Anne Bronte
(Agnes Gray, The Tenant of Wildfell Hall) and two by
Charlotte Bronte (The Professor, Jane Eyre). Each book is
divided into between 100 and 150 equal-sized passages. We
train on passages of one book by each author and test on pas-
sages of the remaining two books. We then run the experi-
ment again with the training sets and test sets reversed and
average the results. In the second experiment, we use a cor-
pus of 260 fiction documents from the BNC, evenly split
among male and female authors. We will attempt to learn the
gender of a document’s author (Koppel, Argamon, &
Shimoni, 2003). We test the effectiveness of our learning
methods using fivefold cross-validation.
We begin with a feature set consisting of all features and
then eliminate more and more features according to various
criteria. For each reduced feature set, we will use a learning
algorithm to build a categorization model and then test the
model on the chapters in the test set. We use Balanced
Winnow (Dagan, Karov, & Roth, 1997; Littlestone, 1988) as
our learning method.
In Figure 3a, we show results on the Bronte experiment
using Balanced Winnow on an initial feature set consisting
of all 3,500 words that appear at least four times in the
Bronte corpus. Feature reduction is performed by ranking
features according to various measures (listed later). Since
only 1,300 words appear both in this list and in the Reuters
corpus (which was used for measuring feature stability), the
first data point we show for a reduced feature set is 1300. In
Figure 3b, we repeat the experiment using an initial feature
set consisting of the full word list as well as all parts-of-speech
triples that appear at least five times in the Bronte corpus.
Each of the reduced sets consists of an equal number of
words and parts-of-speech triples.
As our benchmark for the effectiveness of feature selec-
tion, we use the odds ratio (OR) measure (Caropreso, Matwin,
& Sebastiani, 2001; Mladenic, 1998; Ruiz & Srinivasan,
2002), which is a typical and particularly successful repre-
sentative of discrimination-based feature reduction (Sebastiani,
2002). For a given feature c, let cj be the frequency of c in
category j and let cj be the frequency of c in all other cate-
gories.2 Then OR ranks features according to the score 
. Other methods rearrange the same basic ingredi-
ents in different ways.
Altogether, five measures were used for ranking features:
OR  odds ratio in training corpus
Ft  average frequency in training corpus
OR* Ft  odds ratio in training corpus * average frequency
in training corpus
IN  instability [ 1  S(c)]
IN* Fr  instability * average frequency in Reuters
g
j
cj(1  cj )
cj(1  cj )
IN*Fr
1450 1250 1050 850 650 450 250 50
85
80
75
70
65
60
55
IN
Ft
OR
OR*Ft
IN*Fr
IN
Ft
OR
OR*Ft
2600 2100 1600 1100 600 100
90
85
80
75
70
65
60
55
FIG. 3. Categorization accuracy of Balanced Winnow on the Bronte corpus using five feature-reduction methods to select single words. The x axis repre-
sents the number of features used, and the y axis records accuracy (a). Categorization accuracy of Balanced Winnow on the Bronte corpus using five feature-
reduction methods to select single words and POS triples. The x axis represents the number of features used, and the y axis records accuracy (b).
2Technically, the frequency used in OR as defined by Caropreso et al.
(2001) is the proportion of documents in the category in which the feature
appears at least once. Among common features, which appear in every doc-
ument, OR as so defined offers no discrimination. Thus, it is inappropriate
for style-based categorization. Instead, we use the actual relative frequency
of the feature in the category.
(a) (b)
1524 JOURNAL OF THE AMERICAN SOCIETY FOR INFORMATION SCIENCE AND TECHNOLOGY—September 2006
DOI: 10.1002/asi
IN*Fr
IN
Ft
OR
OR*Ft 2600 2100 1600 1100 600 100
80
75
70
65
60
55
IN*Fr
IN
Ft
OR
OR*Ft
5200 4200 3200 2200 1200 200
80
75
70
65
60
FIG. 4. Categorization accuracy of Balanced Winnow on the gender problem in BNC using five feature-reduction methods to select single words. The x
axis represents the number of features used, and the y axis records accuracy (a). Categorization accuracy of Balanced Winnow on the gender problem in BNC
using five feature-reduction methods to select single words and POS triples. The x axis represents the number of features used, and the y axis records 
accuracy (b).
In the case of IN* Fr, we combine instability with fre-
quency in Reuters rather than with frequency in the training
corpus itself to highlight the fact that the measure can be
entirely corpus independent. The curve for IN* Ft (not
shown) is very similar to that of IN* Fr. Note also that to
emphasize the generality of the method, we measure both
instability and frequency using the Reuters corpus rather
than the BNC, which is more similar to the training corpus.
In Figure 4a and b, we show the analogous results for
fivefold cross-validation experiments on the gender experi-
ment. Note that in both experiments, without taking feature
frequency into account, both OR and IN fail miserably: Too
many rare features are ranked highly by each method. More
importantly, these experiments show that although IN* Fr is
completely independent of the training corpus, it is actually
the best measure by which to choose features. It also is evi-
dent that (a) feature selection does lead to improvement over
using the complete feature set (i.e., letting Winnow select
features implicitly), and (b) optimal performance is main-
tained with a quite small feature set. In fact, in both experi-
ments, the best 400 features selected according to this criterion
are better than using a standard list of 400 function words,
which achieves 81% accuracy on Bronte and 72% on the
gender problem.
Note also that when the number of features approaches
the bottom of our range, it is better to use the frequency of a
feature in the training corpus than in Reuters. This is because
many of the highly ranked features based on Reuters fre-
quency do not appear frequently enough in the training/
testing corpora to yield optimal categorization. This sug-
gests that a broader based corpus than Reuters is advisable.
Overall, these results suggest that the set of features iden-
tified as useful by IN* Fr may constitute, to a substantial
degree, a compact universal feature set for style-based cate-
gorization. This set might be thought of as a generalization
of a universal feature set like a list of function words, with
the added advantages that the features can be of any type
(not only lexical) and that they can be ranked.
Conclusions and Future Work
We have shown how the extent to which a linguistic fea-
ture may be replaced with a semantically equivalent feature
can be effectively quantified. The resulting stability measure
is useful for identifying promising candidates for style-
based text categorization. The specific technique that we
used for estimating stability—back and forth translation—is,
admittedly, flawed due to its dependence on a particular soft-
ware package, but is nevertheless novel and effective. The
results of our experiments indicate that we might regard fre-
quent and unstable features as a generalization of function
word lists that can serve as a universal feature set for style-
based text categorization. More experiments—on other cor-
pora, on more feature types, on other learning algorithms,
and using other existing feature reduction methods as bench-
marks—can be performed to strengthen and extend these
conclusions.
References
Barzilay, R., & McKeown, K. (2001). Extracting paraphrases from a
parallel corpus. In Proceedings of the Association for Computational
Linguistics (pp. 50–57).
Caropreso, M.F., Matwin, S., & Sebastiani, F. (2001). A learner-independent
evaluation of the usefulness of statistical phrases for automated text
categorization. In A.G. Chin (Ed.), Text databases and document man-
agement: Theory and practice (pp. 78–102). Hershey, PA: Idea Group.
Dagan, I., Karov, Y., & Roth, D. (1997). Mistake-driven learning in text cat-
egorization. In Proceedings of the 2nd Conference on Empirical Methods
in Natural Language Processing (EMNLP-2) (pp. 55–63).
Holmes, D. (1995). Authorship attribution. Computers and the Humanities,
28, 87–106.
Koppel, M., Argamon, S., & Shimoni, A.R. (2003). Automatically catego-
rizing written texts by author gender. Literary and Linguistic Computing,
17(4), 401–412.
Lewis, D. (1997). Reuters-21578 text categorization test collection 
[Data collection].
Lewis, D., & Ringuette, M. (1994). A comparison of two learning algo-
rithms for text categorization. In Proceedings of the 3rd Symposium on
Document Analysis and Information Retrieval (pp. 81–93). 
(a) (b)
JOURNAL OF THE AMERICAN SOCIETY FOR INFORMATION SCIENCE AND TECHNOLOGY—September 2006 1525
DOI: 10.1002/asi
1. indicated (.89)
2. has (.78)
3. known (.72)
4. which (.66)
5. them (.66)
6. dollar (.62)
7. order (.60)
8. at (.53)
9. series (.50)
10. over (.49)
11. supply (.49)
12. about (.47)
13. up (.46)
14. out (.45)
15. portion (.45)
16. share (.44)
17. have (.42)
18. his (.42)
19. benefit (.42)
20. state (.41)
21. he (.40)
22. says (.40)
23. shares (.40)
24. into (.38)
25. above (.38)
26. approximately (.38)
27. from (.38)
28. ratio (.38)
29. by (.38)
30. had (.37)
31. been (.37)
32. commerce (.36)
33. back (.36)
34. trade (.36)
35. debit (.35)
36. told (.35)
37. part (.34)
38. near (.34)
39. like (.34)
1. has (1.23)
2. which (.70)
3. his (.59)
4. her (.55)
5. into (.54)
6. order (.54)
7. over (.50)
8. him (.48)
9. about (.47)
10. thus (.43)
11. their (.41)
12. she (.40)
13. out (.40)
Appendix
The top 50 words in Reuters ranked by their respective Instability*log(Freq) scores.
The top 50 words in the BNC ranked by their respective Instability*log(Freq) scores.
40. present (.34)
41. as (.34)
42. stock (.34)
43. connection (.33)
44. action (.32)
45. being (.32)
46. network (.31)
47. request (.31)
48. main (.31)
49. they (.31)
50. parts (.31)
14. straight (.40)
15. top (.37)
16. started (.37)
17. away (.37)
18. up (.36)
19. do (.35)
20. off (.35)
21. just (.34)
22. above (.34)
23. by (.34)
24. makes (.34)
25. large (.33)
26. back (.33)
27. towards (.33)
28. everything (.33)
29. at (.32)
30. got (.32)
31. itself (.32)
32. however (.32)
33. indicated (.32)
34. outside (.31)
35. from (.31)
36. approximately (.31)
37. make (.31)
38. return (.31)
39. set (.31)
40. any (.30)
41. again (.29)
42. area (.29)
43. hour (.29)
44. does (.29)
45. went (.29)
46. far (.28)
47. being (.28)
48. low (.28)
49. start (.28)
50. part (.28)
Littlestone, N. (1988). Learning quickly when irrelevant features abound:
A new linear-threshold algorithm. Machine Learning, 2, 285–318.
Mladenic, D. (1998). Feature subset selection in text learning. In Proceed-
ings of the 10th European Conference on Machine Learning (ECML-98)
(pp. 95–100).
Mosteller, F., & Wallace, D.L. (1964). Applied Bayesian and Classical
Inference: The case of the Federalist Papers. New York: Springer.
Ruiz, M., & Srinivasan, P. (2002). Hierarchical text classification using
neural networks. Information Retrieval 5, 87–118.
Schutze, H., Hull, D.A., & Pedersen, J.O. (1995).Acomparison of classifiers
and document representations for the routing problem. In Proceedings of
the 18th ACM International Conference on Research and Development in
Information Retrieval (SIGIR-95) (pp. 229–237).
Sebastiani, F. (2002). Machine learning in automated text categorization.
ACM Computing Surveys, 34(1), 1–47.
Shinyama, Y., Sekine, S., Sudo, K., & Grishman, R. (2002). Automatic
paraphrase acquisition from news articles. In Proceedings of the Human
Language Technology Conference.
