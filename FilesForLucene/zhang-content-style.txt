Learning to Separate Text Content and Style for 
Classification 
Dell Zhang1 and Wee Sun Lee 2 
1 School of Computer Science and Information Systems 
Birkbeck, University of London 
London WC1E 7HX, UK 
dell.z@ieee.org 
2 Department of Computer Science and Singapore-MIT Alliance 
National University of Singapore 
Singapore 117543 
leews@comp.nus.edu.sg 
Abstract. Many text documents naturally have two kinds of labels. For 
example, we may label web pages from universities according to their 
categories, such as “student” or “faculty”, or according the source universities, 
such as “Cornell” or “Texas”. We call one kind of labels the content and the 
other kind the style. Given a set of documents, each with both content and style 
labels, we seek to effectively learn to classify a set of documents in a new style 
with no content labels into its content classes. Assuming that every document is 
generated using words drawn from a mixture of two multinomial component 
models, one content model and one style model, we propose a method named 
Cartesian EM that constructs content models and style models through 
Expectation Maximization and performs classification of the unknown content 
classes transductively. Our experiments on real-world datasets show the 
proposed method to be effective for style independent text content 
classification. 
1   Introduction 
Text classification [1] is a well established area of machine learning. A text classifier 
is first trained using documents with pre-assigned class labels and then offered test 
documents for which it must guess the best class labels. 
We identify and address a special kind of text classification problem where every 
document is associated with a pair of independent labels ( , )i jc s  where ic ∈  
1{ ,..., }mC c c= and js ∈ 1{ ,..., }nS s s= . In other words, the label space is the 
Cartesian product of two independent sets of labels, C S× , as shown in Figure 1. 
This problem setting extends the standard one-dimensional (1D) label space to two-
dimensions (2D). Following the terminology of computational cognitive science and 
pattern recognition [2], we call C  content labels and S  style labels. Given a set of 
labeled training documents in 1n −  styles 1 1{ ,..., }ns s −  and a set of test documents 
in a new style ns , how should computers learn a classifier to predict the content class 
for each test document? 
This machine learning problem is less explored, yet occurs frequently in practice. 
For example, consider a task of classifying academic web pages into several 
categories (such as “faculty” and “student”): one may have labeled pages from several 
universities (such as “Cornell”, “Texas”, and “Washington”) and need to classify 
pages from another university (such as “Wisconsin”), where the categories can be 
considered as content classes and the universities can be regarded as style types. 
Other examples include: learning to classify articles from a new journal; learning to 
classify papers from a new author; learning to classify customer comments for a new 
product; learning to classify news or messages in a new period, and so on. The 
general problem is the same whenever we have a two (or more) dimensional label for 
each instance. 
 
Since we care about the difference among content classes but not the difference 
among style types, it could be beneficial to separate the text content and style so that 
the classifier can focus on the discriminative content information but ignore the 
distractive style information. However, existing approaches to this problem, whether 
inductive or transductive, simply discard the style labels. Assuming that every 
document is generated using words drawn from a mixture of two multinomial 
component models, one content model and one style model, we propose a new 
method named Cartesian EM that constructs content models and style models through 
Expectation-Maximization [3] and performs classification transductively [4]. Our 
experiments on real-world datasets show that the proposed method can not only 
improve classification accuracy but also provide deeper insights about the data. 
2   Learning with 1D Labels 
2.1   Naïve Bayes 
Naïve Bayes (NB) is a popular supervised learning algorithm for probabilistic text 
classification [5]. Though very simple, NB is competitively effective and highly 
efficient [6]. 
 
Fig. 1. The Cartesian product label space. 
Given a set of labeled training documents 1{ ,..., }DD d d= , NB fits a generative 
model which is parameterized by θ  and then applies it to classify unlabeled test 
documents. The generative model of NB assumes that a document d  is generated by 
first choosing its class ic ∈ 1{ ,..., }mC c c=  according to a prior distribution of 
classes, and then producing its words independently according to a multinomial 
distribution of terms conditioned on the chosen class [7].  
The prior class probability Pr[ ]ic  can be estimated by  
            ( )Pr[ ] Pr[ | ]i id Dc c d D∈= ∑ ,       (1) 
where Pr[ | ]ic d  for a labeled document d is 1 if d  is in class ic  or 0 otherwise.  
With the “naïve” assumption that all the words in the document occur 
independently given the class, the conditional document probability Pr[ | ]id c  can be 
estimated by 
            ( ) ( , )Pr[ | ] Pr[ | ] k
k
n w d
i k iw d
d c w c
∈
= ∏ ,       (2) 
where ( , )kn w d  is the number of times word kw  occurs in document d . The 
conditional word probability Pr[ | ]k iw c  can be estimated by  
            ( )
( , )Pr[ | ]
Pr[ | ]
( , )Pr[ | ]
k id D
k i
iw V d D
n w d c d
w c
n w d c d
η
η
∈
∈ ∈
+
=
+
∑
∑ ∑
,     (3) 
where V is the vocabulary, and 0 1η< ≤  is the Lidstone’s smoothing parameter 
[6].  
Given a test document d , NB classifies it into class  
            ˆ( ) arg max Pr[ | ]
i
i
c C
c d c d
∈
= .       (4) 
The posterior class probability Pr[ | ]ic d  for an unlabeled document d  can be 
computed via Bayes’s rule:  
            
Pr[ | ]Pr[ ] Pr[ | ]Pr[ ]
Pr[ | ]
Pr[ ] Pr[ | ]Pr[ ]
i i i i
i
c C
d c c d c c
c d
d d c c
∈
= =
∑
.     (5) 
2.2   1D EM 
The parameter set θ  of the above 1D generative model includes Pr[ ]ic  for all 
ic C∈  and Pr[ | ]k iw c for all kw V∈ , ic C∈ . If the set of labeled documents is not 
large enough, NB may not be able to give a good estimation of θ  therefore the 
classification accuracy may suffer.  However, it has been shown that the model 
estimation could be improved by making use of additional unlabeled documents (such 
as the test documents). This is the idea of semi-supervised learning, i.e., learning from 
both labeled data and unlabeled data. 
NB can be extended to semi-supervised learning by incorporating a set of 
unlabeled documents through Expectation-Maximization [8, 9]. The principle of 
Expectation-Maximization (EM) will be further explained later in the next section. 
Since this EM based method is designed for the standard 1D label space, we call it 1D 
EM. Please refer to [8, 9] for its detailed derivation. 
 
3   Learning with 2D Labels 
3.1   Cartesian Mixture Model 
Let’s consider the Cartesian product label space C S× , where 1{ ,..., }mC c c=  and 
1{ ,..., }nS s s= . In this 2D label space, every document d D∈ is associated with a 
pair of independent labels ( , )i jc s  where ic C∈  and js S∈ .  
We introduce a 2D generative model, Cartesian mixture model, for this problem of 
learning with 2D labels. It naturally assumes that each content class ic C∈  or style 
type js S∈  corresponds to a multinomial model. A document d  is generated by 
first choosing its content class ic  and style type js  according to a prior distribution 
of label-pairs, and then producing its words independently according to a mixture of 
two component models that correspond to ic  and js  respectively. That is to say, 
every specific occurrence of word kw V∈  in a ( , )i jc s  document is generated from 
either the content model Pr[ | ]k iw c  or the style model Pr[ | ]k jw s , though we do not 
know which one is actually responsible. Therefore the probability of a word kw V∈  
to occur in ( , )i jc s  documents can be calculated by  
            Pr[ | , ] Pr[ | ] (1 ) Pr[ | ]k i j k i k jw c s w c w sλ λ= + − ,     (6) 
———————————————————————————————————————————————————————————— 
Initialization: estimate θ  from the labeled documents only, 
using equations (1) and (3). 
while ( θ  has not converged ) { 
E-step: calculate the probabilistic class labels for 
the unlabeled documents based on the current θ , using 
equation (5). 
M-step: re-estimate θ  from both the labeled documents 
and the unlabeled documents that have been assigned 
probabilistic class labels, using equations (1) and 
(3).  
} 
Classify the unlabeled documents using equation (4). 
———————————————————————————————————————————————————————————— 
Fig. 2. The 1D EM algorithm. 
where [0,1]λ ∈  is a parameter used for weighting the component models. In this 
paper, the same weighting parameter λ  is used for all label-pairs, but our method 
can be easily extended to allow every label-pair have its own weighting parameter. 
Since the content label and the style label are independent, we have  
            Pr[ , ] Pr[ ]Pr[ ]i j i jc s c s= .       (7) 
The prior content class probability Pr[ ]ic  can still be estimated using equation (1). 
The prior style type probability Pr[ ]js  can be estimated similarly by  
            ( )Pr[ ] Pr[ | ]j jd Ds s d D∈= ∑ ,       (8) 
where Pr[ | ]js d  for a labeled document d is 1 if d  is in style js  or 0 otherwise.  
Let d  denote the length of document d , and ,p do  denote the word that occurs 
in the p-th position of document d . By imposing the extended “naïve” assumption 
that all the words in the document occur independently given the content class and the 
style type, the conditional document probability Pr[ | , ]i jd c s  can be estimated by 
      ( ) ( , ),1Pr[ | , ] Pr[ | , ] Pr[ | , ]
k
k
d n w d
i j p d i j k i jw dp
d c s o c s w c s
∈=
= = ∏∏ ,    (9) 
where ( , )kn w d  is the number of times word kw  occurs in document d . The 
conditional word probability Pr[ | , ]k i jw c s  given by equation (6) involves λ , 
Pr[ | ]k iw c  and Pr[ | ]k jw s  whose estimation will be discussed later.  
 
In our problem setting (§1), the training documents are fully labeled, but the test 
documents are only half-labeled (we only know that the test document is in style ns  
but we do not know which content class it belongs to). Given a test document d  
whose style is known to be ns , we can predict its content class to be  
            ˆ( ) arg max Pr[ | , ]
i
i n
c C
c d c d s
∈
= .      (10) 
The posterior class probability Pr[ | , ]i nc d s  for an half-labeled document d  in 
style ns  can be calculated using Bayes’s rule and equation (7):  
      
Pr[ | , ]Pr[ , ] Pr[ | , ]Pr[ ]
Pr[ | , ]
Pr[ | ]Pr[ ] Pr[ | , ]Pr[ ]
i n i n i n i
i n
n n nc C
d c s c s d c s c
c d s
d s s d c s c
∈
= =
∑
.   (11) 
 
Fig. 3. The graphical model representation of a document as a Bayesian network. 
3.2   Cartesian EM 
The parameter set θ  of the above Cartesian mixture model includes Pr[ ]ic  for all 
ic C∈ , Pr[ ]js  for all js S∈ , Pr[ | ]k iw c  for all kw V∈ , ic C∈ , Pr[ | ]k jw s  for 
all kw V∈ , js S∈ , and λ . One difficulty to estimate θ  is that we would not be 
able to get the values of λ , Pr[ | ]k iw c  and Pr[ | ]k jw s  by using maximum 
likelihood estimation in a straightforward manner, because for every observed word 
occurrence we do not know exactly whether the content model or the style model 
generated it. Furthermore, we need to take the test documents into consideration while 
estimating θ  (at least Pr[ | ]k jw s ), but for every test document we do not know 
exactly which content class it comes from.  
The Expectation-Maximization (EM) algorithm [3] is a general algorithm for 
maximum likelihood estimation when the data is “incomplete”. In this paper, we 
propose a new EM based method named Cartesian EM that constructs the Cartesian 
mixture model and applies it to predict the content classes of the test documents in a 
new style. Cartesian EM actually belongs to the family of transductive learning [4], a 
special kind of semi-supervised learning that the learning algorithm can see the set of 
test examples and make use of them to improve the classification accuracy on them. 
A common method for estimating the model θ  is maximum likelihood estimation 
in which we choose a θ  that maximizes its likelihood (or equivalently log-
likelihood) for the observed data (in our case the set of documents D  and their 
associated labels):  
            ˆ arg max ( ) arg max log ( ) arg max log Pr[ | ]L L Dθ θ θθ θ θ θ= = = .   (12) 
Given the model θ , we have 
            log Pr[ | ] log Pr[ , ( ), ( )]
d D
D d c d s dθ
∈
=∑ ,(13) 
where ( )c d  and ( )s d  stand for the actual (possibly unknown) content label and 
the actual style label of document d  respectively. 
For a fully-labeled document d , we know that Pr[ | ]ic d  is 1 if d  is in content 
class ic  or 0 otherwise and Pr[ | ]js d  is 1 if d  is in style type js  or 0 otherwise, 
therefore we can calculate the log-likelihood of d  given the model θ  using 
Bayes’s rule and equation (7): 
1 1
1 1 1 1
log Pr[ , ( ), ( )] Pr[ | ]Pr[ | ]log Pr[ | , ]
Pr[ | ]Pr[ | ]log Pr[ ] Pr[ | ]Pr[ | ]log Pr[ ]
m n
i j i ji j
m n m n
i j i i j ji j i j
d c d s d c d s d d c s
c d s d c c d s d s
= =
= = = =
= +
+
∑ ∑
∑ ∑ ∑ ∑
.  (14) 
Furthermore, using equation (6) and equation (9), we get  
            ( ), ,1log Pr[ | , ] log Pr[ | ] (1 ) Pr[ | ]
d
i j p d i p d jp
d c s o c o sλ λ
=
= + −∑ .   (15) 
Now we see that there are two obstacles to estimating θ : one is that the test 
documents are only half-labeled hence equation (14) is not applicable to their log-
likelihood computation, the other is that equation (15) contains logarithms of sums 
hence hard to maximize.  
The basic idea of the EM algorithm is to augment our “incomplete” observed data 
with some latent/hidden variables so that the “complete” data has a much simpler 
likelihood function to maximize [10]. In our case, we introduce a binary latent 
variable for each content class ic C∈ and each (half-labeled) test document d  to 
indicate whether document d  is in the content class ic , i.e.,  
            
1    if document  is in content class 
( , )
0    otherwise
i
i
d c
y c d
⎧
= ⎨
⎩
.    (16) 
Moreover, we introduce a binary latent variable for each word occurrence ,p do  to 
indicate whether the word has been generated from the content model or the style 
mode,  
            
,1    if  is from the content model( , )
0    otherwise
p do
z p d
⎧⎪= ⎨
⎪⎩
.    (17) 
If the two types of latent variables are observed, the data is complete and 
consequently the log-likelihood function becomes much easier to maximize. For a 
half-labeled test document d , we know that the latent variable ( , )iy c d  is 1 if d  
is in content class ic  or 0 otherwise and Pr[ | ]js d  is 1 if d  is in style type js  or 
0 otherwise, therefore as in equation (14) we can calculate its log-likelihood given 
the model θ  and the latent variables ( , )iy c d :  
1 1
1 1 1 1
log Pr[ , ( ), ( )] ( , ) Pr[ | ]log Pr[ | , ]
( , ) Pr[ | ]log Pr[ ] ( , ) Pr[ | ]log Pr[ ]
m n
i j i ji j
m n m n
i j i i j ji j i j
d c d s d y c d s d d c s
y c d s d c y c d s d s
= =
= = = =
= +
+
∑ ∑
∑ ∑ ∑ ∑
.   (18) 
In addition, with the help of the latent variables ( , )z p d , equation (15) for 
computing log Pr[ | , ]i jd c s  can be re-written as  
            
( )
( ) ( )1
( , ) log Pr[ ( ) | ]
log Pr[ | , ]
1 ( , ) log (1 )Pr[ ( ) | ]
d
p i
i j
p p j
z p d o d c
d c s
z p d o d s
λ
λ=
⎛ ⎞+
⎜ ⎟=
⎜ ⎟− −⎝ ⎠
∑ ,   (19) 
because we assume that we know which component model has been used to generate 
each word occurrence. 
The EM algorithm starts with some initial guess of the model (0)θ , then iteratively 
alternates between two steps, called the “E-step” (expectation step) and the “M-step” 
(maximization step) respectively [10]. In the E-step, it computes the expected log-
likelihood for the complete data, or the so-called “Q-function” denoted by ( )( ; )tQ θ θ , 
where the expectation is taken over the computed conditional distribution of the latent 
variables given the current setting of model ( )tθ  and the observed data. In the M-
step, it re-estimates the model to be ( 1)tθ +  by maximizing the Q-function. Once we 
have a new generation of model parameters, we repeat the E-step and the M-step. 
This process continues until the likelihood converges to a local maximum.  
The major computation to be carried out in the E-step is to estimate the 
distributions of latent variables, in our case, ( , )iy c d  and ( , )z p d . For a half-
labeled test document d  in style ns , we have  
            
Pr[ | , ]Pr[ ]
Pr[ ( , ) 1] Pr[ | , ]
Pr[ | , ]Pr[ ]
i n i
i i n
n
c C
d c s c
y c d c d s
d c s c
∈
= = =
∑
    (20) 
via using equation (11), and obviously Pr[ ( , ) 0]iy c d =  1 Pr[ ( , ) 1]iy c d= − = . For a 
word occurrence ,p d ko w=  in a document d  with label-pair ( , )i jc s , we have  
            
Pr[ | ]
Pr[ ( , ) 1]
Pr[ | ] (1 ) Pr[ | ]
k i
k i k j
w c
z p d
w c w s
λ
λ λ
= =
+ −
,    (21) 
and Pr[ ( , ) 0]z p d = 1 Pr[ ( , ) 1]z p d= − = . Since the value of Pr[ ( , ) 1]z p d =  given 
by the above equation is same for every word occurrence ,p d ko w V= ∈  in ( , )i jc s  
documents, we introduce a new notation ijkz  to represent it. 
The M-step involves maximizing the Q-function,   
            ( 1) ( )arg max ( ; )t tQθθ θ θ
+ = .      (22) 
In our case, the Q function can be obtained by combining the equations (13), (14), 
(18), (19), (20) and (21), and taking expectation over latent variables:  
            
( )
1 1
1 1
1 1
( ; ) Pr[ | ]Pr[ | ] ( , , )
Pr[ | ]Pr[ | ]log Pr[ ]
Pr[ | ]Pr[ | ]log Pr[ ]
m nt
i j i jd D i j
m n
i j id D i j
m n
i j jd D i j
Q c d s d E d c s
c d s d c
c d s d s
θ θ
∈ = =
∈ = =
∈ = =
=
+
+
∑ ∑ ∑
∑ ∑ ∑
∑ ∑ ∑
,   (23) 
where 
            ( )( )( , , ) ( , ) ( , ) ( , )
k
i j k k i k jw d
E d c s n w d E w c E w s
∈
= +∑ ,    (24) 
            ( )( , ) log Pr[ | ]k i ijk k iE w c z w cλ= ,      (25) 
            ( )( , ) (1 ) log (1 ) Pr[ | ]k j ijk k jE w s z w sλ= − − .     (26) 
The next model estimation ( 1)tθ + should maximize ( )( ; )tQ θ θ , meanwhile the model 
parameters need to obey some inherent constraints such as  
            Pr[ | ] 1iw V w c∈ =∑ .      (27) 
The M-step turns out to be a constrained optimization problem. Using the Lagrange 
multiplier method, we can get an analytical solution to this problem. The derived 
update rules for the M-step are as follows. The prior content class probabilities 
Pr[ ]ic  are updated using equation (1), while the prior style type probabilities Pr[ ]js  
are kept unchanged. The conditional word probabilities should be adjusted using the 
following equations:  
1 1Pr[ | ] ( , , , ) ( , , , )k i k i i
d D s S w V d D s S
w c Z w d c s Z w d c sη η
∈ ∈ ∈ ∈ ∈
⎛ ⎞⎛ ⎞ ⎛ ⎞
⎜ ⎟= + +⎜ ⎟ ⎜ ⎟
⎜ ⎟ ⎜ ⎟⎜ ⎟
⎝ ⎠ ⎝ ⎠⎝ ⎠
∑∑ ∑ ∑∑    (28) 
where 
            1( , , , ) ( , ) Pr[ | ]Pr[ | ]k i j k i j ijkZ w d c s n w d c d s d z= ,    (29) 
and similarly,   
            
0 0Pr[ | ] ( , , , ) ( , , , )k j k j j
d D c C w V d D c C
w s Z w d c s Z w d c sη η
∈ ∈ ∈ ∈ ∈
⎛ ⎞⎛ ⎞ ⎛ ⎞
⎜ ⎟= + +⎜ ⎟ ⎜ ⎟
⎜ ⎟ ⎜ ⎟⎜ ⎟
⎝ ⎠ ⎝ ⎠⎝ ⎠
∑∑ ∑ ∑∑    (30) 
where 
            0 ( , , , ) ( , ) Pr[ | ]Pr[ | ](1 )k i j k i j ijkZ w d c s n w d c d s d z= − .    (31) 
Besides, the weight parameter should be re-estimated by  
( )1 1 0( , , , ) ( , , , ) ( , , , ) .
w V d D c C s S w V d D c C s S
Z w d c s Z w d c s Z w d c sλ
∈ ∈ ∈ ∈ ∈ ∈ ∈ ∈
⎛ ⎞ ⎛ ⎞= +⎜ ⎟ ⎜ ⎟
⎝ ⎠ ⎝ ⎠
∑∑∑∑ ∑∑∑∑    (32) 
The EM algorithm is essentially a hill-climbing approach, thus it can only be 
guaranteed to reach a local maximum. When there are multiple local maximums, 
whether we will actually reach the global maximum depends on where we start: if we 
start at the “right hill”, we will be able to find a global maximum. In our case, we 
ignore the style labels and use the 1D estimation equations (1) and (3) to initialize 
Pr[ ]ic  and Pr[ | ]k iw c  from the training documents, just as in the standard NB 
algorithm. The initial values of Pr[ ]js  and Pr[ | ]k jw s  can be estimated similarly 
by ignoring the class labels, but from both the training documents and the test 
documents, using equation (8) and  
( )Pr[ | ] ( , )Pr[ | ] ( , ) Pr[ | ]k j k j jd D
d D w V
w s n w d s d n w d s dη η
∈
∈ ∈
⎛ ⎞ ⎛ ⎞
= + +⎜ ⎟ ⎜ ⎟
⎜ ⎟ ⎜ ⎟
⎝ ⎠ ⎝ ⎠
∑ ∑ ∑ .  (33) 
The initial value of the weighting parameter λ  is simply set to 1 2  that puts equal 
weights to the component models. 
 
———————————————————————————————————————————————————————————— 
Initialization: set 1 2λ = , and estimate the probabilities 
Pr[ ]ic , Pr[ | ]k iw c , Pr[ ]js , Pr[ | ]k jw s  just as in the 1D situation, 
using equations (1), (3), (8), (33). 
while ( θ  has not converged ) { 
E-step: calculate Pr[ | ]ic d  for the half-labeled test 
documents using equation (20), and calculate ijkz  using 
equation (21), based on the current θ . 
M-step: re-estimate θ  using equations (28), (30), 
(32), with the help of the latent variables.  
} 
Classify the half-labeled test documents using equation 
(10). 
———————————————————————————————————————————————————————————— 
Fig. 4. The Cartesian EM algorithm. 
4   Experiments 
We have conducted experiments on three real-world datasets to evaluate the 
effectiveness of the proposed Cartesian EM method for text classification in the 2D 
problem setting (stated in §1). Three methods, NB, 1D EM and Cartesian EM (C. 
EM), were compared in terms of classification accuracy. The Lidstone smoothing 
parameter was set to a good value 0.1η =  [6], and document frequency (df) based 
feature selection [11] were performed to let NB achieve optimal average performance 
on each dataset. The document frequency has been shown to have similar effect as 
information gain or chi-square test in feature selection for text classification. 
The WebKB dataset (http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-20/www/ 
data/) contains manually classified Web pages that were collected from the computer 
science departments of four universities (“Cornell”, “Texas”, “Washington” and 
“Wisconsin”) and some other universities (“misc.”). In our experiments, each 
category is considered as a content class, and each source university is considered as a 
style type because each university's pages have their own idiosyncrasies. Only pages 
in the top four largest classes were used, namely “student”, “faculty”, “course” and 
“project”. Furthermore, unlike the popular setting, the pages from the “misc.” 
universities were not used either, because we thought it made little sense to train a 
style model for them. All pages were pre-processed using the Rainbow toolkit [12] 
with the options “--skip-header --skip-html --lex-pipe-command=tag-digits --no-stoplist --
prune-vocab-by-doc-count=3”. The experimental results on this dataset are shown in 
Table 1. The weighting parameter values found by Cartesian EM were consistently 
around 0.75. 
The SRAA dataset (http://www.cs.umass.edu/~mccallum/code-data.html) is a 
collection of 73,218 articles from four newsgroups (simulated-auto, simulated-
aviation, real-auto and real-aviation). In our experiments, “auto” and “aviation” are 
considered as content classes and “simulated” and “real” are considered as style types. 
All articles were pre-processed using the Rainbow toolkit [12] with the option “--skip-
header --skip-html --prune-vocab-by-doc-count=200”. The experimental results on this 
dataset are shown in Table 1. The weighting parameter values found by Cartesian EM 
were consistently around 0.95. It is also possible to take “simulated” and “real” as 
classes while “auto” and “aviation” as styles, but in our experiments, Cartesian EM 
did not work well in that situation. We think the reason is that the actual weight of 
“auto” and “aviation” models are overwhelming (around 0.95) so that Cartesian EM 
would drift away and consequently fail to get good discrimination between 
“simulated” and “real”. This is a known pitfall of the EM algorithm. Therefore 
Cartesian EM may be not suitable when the text content is significantly outweighed 
by text style. It may be possible to detect this situation by observing λ  and back off 
to a simpler method like NB when λ  is small. 
The 20NG dataset (http://people.csail.mit.edu/people/jrennie/20Newsgroups) is a 
collection of approximately 20,000 articles that were collected from 20 different 
newsgroups [13]. The “bydate” version of this dataset along with its train/test split 
was used. In our experiments, each newsgroup is considered as a content class, and 
the time period before and after the split point are considered as two style types. It is 
realistic and common to train a classifier on documents before a time point and then 
test it on documents thereafter. All articles were pre-processed using the Rainbow 
toolkit [12] with the option “--prune-vocab-by-occur-count=2 --prune-vocab-by-doc-
count=0”. The experimental results on this dataset are shown in Table 1. The 
weighting parameter value found by Cartesian EM was about 0.95. 
To sum up, Cartesian EM compared favorably with NB and 1D EM in our 
experiments. In the case where Cartesian EM improved performance, the 
improvements over the standard 1D-EM and Naive Bayes on these three datasets are 
all statistically significant (P-value < 0.05), using the McNemar's test. It is able to 
yield better understanding (such as the weight of content/style) as well as increase 
classification accuracy.  
 
5   Related Works 
The study of separating content and style has a long history in computational 
cognitive science and pattern recognition. One typical work in this area is [2]. Our 
work exploits the characteristic of text data, and the proposed Cartesian EM method is 
relatively more efficient than the existing methods. 
Various mixture models have been used in different text classification problems. 
However, most of them assume that a document is generated by only one component 
model [14], while our Cartesian mixture model assumes that a document is generated 
by two multinomial component models. In [15], a mixture model is proposed for 
multi-label text classification, where each document is assumed to be generated by 
multiple multinomial component models, one per document label. In [16], a mixture 
model is proposed for relevance feedback, where the feedback documents are 
assumed to be generated by two multinomial component models, one known 
background  model and one unknown topic model, combined via a fixed weight. The 
Cartesian mixture model is different with these existing works as it is designed for 
documents with 2D labels and it works in the context of transductive learning. 
One straightforward way to extend 1D EM to 2D scenario is to simply regard each 
label-pair ( , )c s  as a pseudo 1D label. In other words, each cell in the Cartesian 
product label matrix (as shown in Figure 1) is associated with a multinomial model. 
That method named EM2D has been proposed for the problem of learning to integrate 
Table 1. Experimental results. 
Dataset Test Style NB 1D EM C. EM 
Cornell 81.42% 84.96% 84.96% 
Texas 81.75% 82.94% 83.33% 
Washington 77.25% 79.22% 80.00% 
WebKB 
Wisconsin 82.79% 81.17% 84.42% 
simulated 80.11% 91.85% 94.39% SRAA 
real 91.99% 87.00% 92.68% 
20NG new articles 79.55% 80.92% 81.78% 
 
web taxonomies [17]. However, EM2D is not able to generalize content classifiers to 
new styles thus not applicable to our problem. In contrast, Cartesian EM assumes that 
each row or column in the Cartesian product label matrix corresponds to a 
multinomial model and the observations (cells) are generated by the interaction 
between content models (rows) and style models (columns). This is to simulate the 
situation where some words in the document are used for style purposes while other 
words in the same document are used to describe the content. 
6   Conclusion 
The ability of human being to separate content and style is amazing. This paper 
focuses on the problem of style-independent text content classification, and presents 
an EM based approach, Cartesian EM, that has been shown to be effective by 
experiments on real-world datasets. 
References 
1. Sebastiani, F.: Machine Learning in Automated Text Categorization. ACM Computing 
Surveys 34 (2002) 1-47 
2. Tenenbaum, J.B., Freeman, W.T.: Separating Style and Content with Bilinear Models. 
Neural Computation 12 (2000) 1247-1283 
3. Dempster, A.P., Laird, N.M., Rubin, D.B.: Maximum Likelihood from Incomplete Data via 
the EM Algorithm. Journal of the Royal Statistical Society,  Series B 39 (1977) 1-38 
4. Vapnik, V.N.: Statistical Learning Theory. Wiley (1998) 
5. Mitchell, T.: Machine Learning. McGraw Hill (1997) 
6. Agrawal, R., Bayardo, R., Srikant, R.: Athena: Mining-based Interactive Management of 
Text Databases. Proceedings of the 7th International Conference on Extending Database 
Technology (EDBT), Konstanz, Germany (2000) 365-379 
7. McCallum, A., Nigam, K.: A Comparison of Event Models for Naive Bayes Text 
Classification. AAAI-98 Workshop on Learning for Text Categorization, Madison, WI 
(1998) 41-48 
8. Nigam, K., McCallum, A., Thrun, S., Mitchell, T.: Learning to Classify Text from Labeled 
and Unlabeled Documents. Proceedings of the 15th Conference of the American Association 
for Artificial Intelligence (AAAI), Madison, WI (1998) 792-799 
9. Nigam, K., McCallum, A., Thrun, S., Mitchell, T.: Text Classification from Labeled and 
Unlabeled Documents using EM. Machine Learning 39 (2000) 103-134 
10.Zhai, C.: A Note on the Expectation-Maximization (EM) Algorithm.  (2004) 
11.Yang, Y., Pedersen, J.O.: A Comparative Study on Feature Selection in Text Categorization. 
Proceedings of the 14th International Conference on Machine Learning (ICML), Nashville, 
TN (1997) 412-420 
12.McCallum, A.: Bow: A Toolkit for Statistical Language Modeling, Text Retrieval, 
Classification and Clustering.  (1996) 
13.Lang, K.: NewsWeeder: Learning to Filter Netnews. Proceedings of the 12th International 
Conference on Machine Learning (ICML), Tahoe City, CA (1995) 331-339 
14.Pavlov, D., Popescul, A., Pennock, D.M., Ungar, L.H.: Mixtures of Conditional Maximum 
Entropy Models. Proceedings of the 20th International Conference on Machine Learning 
(ICML), Washington DC, USA (2003) 584-591 
15.McCallum, A.: Multi-Label Text Classification with a Mixture Model Trained by EM. 
AAAI'99 Workshop on Text Learning (1999) 
16.Zhai, C., Lafferty, J.D.: Model-based Feedback in the Language Modeling Approach to 
Information Retrieval. Proceedings of the 10th ACM International Conference on 
Information and Knowledge Management (CIKM), Atlanta, GA (2001) 403-410 
17.Sarawagi, S., Chakrabarti, S., Godbole, S.: Cross-Training: Learning Probabilistic Mappings 
between Topics. Proceedings of the 9th ACM SIGKDD International Conference on 
Knowledge Discovery and Data Mining (KDD), Washington DC, USA (2003) 177-186 
