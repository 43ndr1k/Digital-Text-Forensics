A. Gelbukh (Ed.): CICLing 2009, LNCS 5449, pp. 535–546, 2009. 
© Springer-Verlag Berlin Heidelberg 2009 
Empirical Paraphrasing of Modern Greek Text in Two 
Phases: An Application to Steganography 
Katia Lida Kermanidis and Emmanouil Magkos 
Ionian University, Department of Informatics 
7 Pl. Tsirigoti, 49100, Corfu, Greece 
{kerman,emagos}@ionio.gr 
Abstract. This paper describes the application of paraphrasing to steganography, 
using Modern Greek text as the cover medium. Paraphrases are learned in two 
phases: a set of shallow empirical rules are applied to every input sentence, 
leading to an initial pool of paraphrases. The pool is then filtered through 
supervised learning techniques. The syntactic transformations are shallow and 
require minimal linguistic resources, allowing the methodology to be easily 
portable to other inflectional languages. A secret key shared between two 
communicating parties helps them agree on one chosen paraphrase, the presence 
of which (or not) represents a binary bit of hidden information. The ability to 
simultaneously apply more than one rules, and each rule more than one times, to 
an input sentence increases the paraphrase pool size, ensuring thereby 
steganographic security. 
Keywords: paraphrasing, shallow parsing, supervised learning, steganography. 
1   Introduction 
Given an original sentence, that conveys a specific meaning, paraphrasing means 
expressing the same meaning using a different set of words or a different syntactic 
structure. Significant research effort has been put into the identification as well as the 
generation of paraphrases. Paraphrasing has been used extensively for educational 
purposes in language learning, as well as in several NLP tasks like text summarization 
[4], question answering [6] and natural language generation. Recently it has found yet 
another use in steganography.  
Regarding paraphrase identification, previous approaches have utilized supervised 
[8] or unsupervised ([2][3]) machine learning techniques. The authors in [13] use 
named entity recognition in newswire articles from different newspapers to detect 
pairs of sentences that discuss the same topic and find sentence parts that are 
paraphrases. Regarding paraphrase generation, the use of finite state automata [10], 
i.e. paraphrase lattices, has been proposed, as well as the application of empirical 
rules [9] and statistical machine translation techniques [12].  
In the present work, paraphrases of Modern Greek free text are learned in two 
phases. Henceforth, the term ‘paraphrasing’ will stand for shallow syntactic 
536 K.L. Kermanidis and E. Magkos 
transformations, i.e. swaps of consecutive phrasal chunks. Modern Greek is suitable 
for shallow paraphrasing, due to the permissible freedom in the ordering of the 
phrases in a sentence. The rich morphology allows for more freedom in chunk 
ordering compared to other languages, more strict in syntax, like English or German. 
A set of empirical rules is first applied to the input sentences in order to change 
their phrase ordering. The resulting paraphrases are then encoded into feature-value 
vectors and fed into supervised learning schemata so as to further filter out 
paraphrases that are syntactically incorrect and/or stylistically unnatural.  
The paraphrase learning process is based on resource economy, i.e. the utilization 
of as minimal linguistic resources as possible, enabling thereby the methodology to be 
easily applicable to other morphologically rich languages. Regarding pre-processing 
tools, a phrase chunker is used for splitting the sentence into chunks. No use of other 
external thesauri, treebanks, lexica or other sophisticated tools of any kind is made. 
The paraphrased text may then be used for hiding secret information, i.e. 
steganography. Steganographic security will depend on the correctness and the 
naturalness of the paraphrases. The supervised filtering phase ensures higher 
paraphrasing accuracy, which leads to higher security in protecting the hidden 
message from malicious eavesdroppers. 
The rest of this paper is organized as follows. Section 2 describes the corpus that 
was used, as well as the chunking tool. Sections 3 and 4 are dedicated to the two 
phases of the paraphrase learning process. Section 5 introduces steganography 
through text media and describes the use of the produced paraphrases for hiding secret 
information. Section 6 presents the outcome of the paraphrase evaluation process and 
provides some interesting topics for discussion. 
2   Corpus and Pre-processing 
The text corpus used in the experiments is the ILSP/ELEFTHEROTYPIA corpus [7]. 
It consists of 5244 sentences; it is balanced in domain and genre, and manually 
annotated with complete morphological information. Further (phrase structure) 
information is obtained automatically by a multi-pass chunker [14]. 
During chunking, noun (NP), verb (VP), prepositional (PP), adverbial phrases 
(ADP) and conjunctions (CON) are detected via multi-pass parsing. The chunker 
exploits minimal linguistic resources: a keyword lexicon containing 450 keywords 
(i.e. closed-class words such as articles, prepositions etc.) and a suffix lexicon of 
300 of the most common word suffixes in Modern Greek. The chunked phrases are 
non-overlapping. Embedded phrases are flatly split into distinct phrases. Nominal 
modifiers in the genitive case are included in the same phrase with the noun they 
modify; base nouns joined by a coordinating conjunction are grouped into one 
phrase. The chunker identifies basic phrase constructions during the first passes 
(e.g. adjective-nouns, article-nouns), and combines smaller phrases into longer ones 
in later passes (e.g. coordination, inclusion of genitive modifiers, compound 
phrases).  
 Empirical Paraphrasing of Modern Greek Text in Two Phases 537 
3   Phase 1: The Paraphrasing Rules 
A set of nine paraphrasing rules for Modern Greek has been created. The rules have 
been developed empirically, they are bidirectional and they are grouped into two 
categories. The first consists of rules that swap the positions of two consecutive 
chunks; the second consists of rules that swap the positions of the two border chunks 
in a sequence of three consecutive chunks. The complete rule set is shown in Table 1.   
The first rule is for subject-verb swapping. The second rule swaps a purpose or 
result clause with the preceding verb. Rules 3 and 4 swap phrases that participate in a 
coordinating structure, i.e. phrases that surround the conjunction και (and). 
Morphological (case) agreement between NPs and prepositional agreement between 
the PPs is obligatory in order for the swap to be admissible. The fifth rule takes 
advantage of the freedom in adverb positioning and swaps the adverb with the 
preceding phrase. Rules 6 to 8 swap certain types of PPs with their preceding verbs. 
Rule 9 swaps a copular verb with its predicate. 
Unlike the syntactic tools presented in previous approaches [9], that may be 
applied only once to a given sentence, every rule described here may be applied 
multiple times (i.e. in multiple positions) to a sentence. Furthermore, more than one 
rules may be applied to a sentence simultaneously. 
Each sentence is checked for rule applicability. More specifically, the rule set that 
may be applied to it is determined, as well as the number of times each of these rules 
is applicable. All possible combinations of rule applications are produced and the 
resulting sentences are henceforth called the initial pool of paraphrases. In the given 
corpus the size of the initial pool may vary from zero (the sentence does not allow for 
any paraphrasing) to as many as 80 paraphrases. The following example shows the 
initial pool of paraphrases for a corpus sentence. 
 
NP[Στενοί της συνεργάτες] VP[είναι] ADP[επίσης] NP[η Π. 
Καραβίτη] CON[και] NP[ο Π. Αντωνόπουλος]. 
 
([Close colleagues of hers] [are] [also] [P. Karaviti] [and] [P. 
Antonopoulos]) 
 
Paraphrase 1 (Rule 1): 
VP[Είναι] NP[στενοί της συνεργάτες] ADP[επίσης] NP[η Π. 
Καραβίτη] CON[και] NP[ο Π. Αντωνόπουλος]. 
 
Paraphrase 2 (Rule 3):  
NP[Στενοί της συνεργάτες] VP[είναι] ADP[επίσης] NP[ο Π. 
Αντωνόπουλος] CON[και] NP[η Π. Καραβίτη]. 
 
Paraphrase 3 (Rule 5):  
NP[Στενοί της συνεργάτες] ADP[επίσης] VP[είναι] NP[η Π. 
Καραβίτη] CON[και] NP[ο Π. Αντωνόπουλος]. 
 
Paraphrase 4 (Rules 1 & 3): 
VP[Είναι] NP[στενοί της συνεργάτες] ADP[επίσης] NP[ο Π. 
Αντωνόπουλος] CON[και] NP[η Π. Καραβίτη]. 
 
538 K.L. Kermanidis and E. Magkos 
Paraphrase 5 (Rules 3 & 5):  
NP[Στενοί της συνεργάτες] ADP[επίσης] VP[είναι] NP[ο Π. 
Αντωνόπουλος] CON[και] NP[η Π. Καραβίτη]. 
Table 1. The set of paraphrasing rules. 
Rule Example 
1. NP(nom) VP  VP NP(nom) [ο Γιάννης] [ήρθε]  [ήρθε] [ο Γιάννης] 
[John] [came]  [came] [John] 
2. VP1 VP2(να)  VP2(να) VP1 [θέλει] [να παίξει]  [να παίξει] [θέλει] 
[he wants] [to play]  [to play] [he wants] 
3. NP1 και NP2  NP2 και NP1 
(the 2 NPs are in the same case) 
[η γιαγιά] και [ο παππούς]  [ο παππούς] και [η 
γιαγιά]  
[grandma] and [grandpa]  [grandpa] and 
[grandma] 
4. PP1 και PP2 PP2 και PP1 
the 2 PPs start with the same preposition 
[στην Γερμανία] και [στην Αγγλία] [στην 
Αγγλία] και [στην Γερμανία] 
[in Germany] and [in England]  [in England] 
and [in Germany] 
5. XP ADVP  ADVP XP 
XP: NP, PP or VP 
[αποφασίστηκε] [εύκολα]  [εύκολα] 
[αποφασίστηκε] 
[it was decided] [easily]  [easily] [it was 
decided] 
6. VP PP(σε)  PP(σε) VP [κατέβηκε] [στο πάρκο]  [στο πάρκο] 
[κατέβηκε] 
[he went down] [to the park]  [to the park] [he 
went down] 
7. VP PP(με)  PP(με) VP [η νίκη] [επιτυγχάνεται] [με θυσίες]  [η νίκη] 
[με θυσίες] [επιτυγχάνεται]  
[victory] [is achieved] [with sacrifices]  
[victory][with sacrifices][is achieved] 
8. VP PP(για)  PP(για) VP [αποφασίζει] [για τους άλλους]  [για τους 
άλλους] [αποφασίζει] 
[he decides] [for the others]  [for the others] 
[he decides] 
9. VP(cp) NP(nom)  NP(nom) VP(cp) [είναι] [έξυπνοι]  [έξυπνοι] [είναι] 
[they are] [clever]  [clever] [they are] 
 
Rules 1 and 5 cannot be applied to the sentence simultaneously due to overlapping 
of the related phrases (simultaneous application of more than one rules requires that 
the phrases linked to the rules do not overlap), so the initial pool consists of five 
paraphrases. 
The nine paraphrasing rules were applied to the 5244 corpus sentences. Figure 1 
shows the distribution of the number of sentences depending on the sentence length 
(i.e. the number of chunks forming the sentence). Figure 2 shows the distribution of 
the number of sentences depending on the initial paraphrase pool size. Almost 80% of 
the sentences have at least one paraphrase, an impressive number, given that more 
than 24% of the input sentences consist of five or less chunks. 
 
 Empirical Paraphrasing of Modern Greek Text in Two Phases 539 
633 634
985
1121
1263
591
16 1
0
200
400
600
800
1000
1200
1400
1-3 4-5 6-8 9-12 13-20 21-50 51-99 100-150
sentence length
n
u
m
b
er
o
f
se
n
te
n
ce
s
 
1102
1826
1073
681
416
104
36 6
0
200
400
600
800
1000
1200
1400
1600
1800
2000
0 1-2 3-5 6-10 11-20 21-30 31-50 51-80
pool size
n
u
m
b
er
 o
f 
se
n
te
n
ce
s
 
Fig. 1. Distribution of sentence length Fig. 2. Distribution of the pool size 
4   Phase 2: Paraphrase Learning 
Due to the use of the paraphrased sentences in steganography, correctness in syntax as 
well as naturalness are of great significance. Steganographic security depends largely 
on paraphrasing accuracy. Therefore the produced paraphrases are further filtered 
using supervised learning.  
The positions of possible phrase swaps in the input sentences are identified, 
according to the paraphrasing rules. In the example in section 3, the swap positions 
are: 1 (between the first and the second phrase), 2 and 4. 
A learning vector is created for every input sentence and each swap position. The 
features forming the vector encode syntactic information for the phrase right before 
the swap position, as well as two phrases to the left and two phrases to the right. 
Thereby, context information is taken into account. Every phrase is represented 
through a set of six features, shown in Table 2. 
Table 2. The features of the learning vector 
 1 2 3 4 5 6 
NP NP case morph pron gen - 
VP VP - - word cop - 
PP PP - - prep - - 
CON CON - - lemma - num 
ADP ADP - - lemma - num 
 
The first feature for every phrase is the phrase type. The case is one of three 
characters denoting the grammatical case of the headword in an NP (nominative, 
accusative or genitive). The headword is the noun in the nominative or accusative 
case. If there is no noun, it is the adjective in the nominative or the accusative. Else it 
540 K.L. Kermanidis and E. Magkos 
is the numeral or pronoun. If no element is present in either of these two cases, the 
headword is the first element of the phrase. Case is very important, as case agreement 
is decisive for the application of Rule 3, and the case value is significant for Rules 1 
and 9. The morph feature is a three-letter code denoting whether an NP contains a 
definite or indefinite article and its grammatical case. The (in)definitiveness of an NP 
also affects the applicability of rule 3. The fourth feature varies according to the 
phrase type. For NPs it is the type of pronoun appearing in them, in case one does. It 
is the first word introducing a VP, the preposition introducing a PP, and the con-
junction or the adverb in a CON and an ADP respectively. The presence of a genitive 
element is often decisive for the applicability of rules 1, 3 and 9. The fifth feature is 
binary and encodes whether there is an element in the genitive case in an NP, or a 
copular verb in a VP. Finally, the num feature is the number of tokens (words) within 
a CON or an ADP. This feature is very important, as a one-word coordinating CON 
phrase almost always allows a swap between the phrases it links, while a multi-word 
CON phrase very often does not (decisive for rules 3 and 4). In the following example 
the multi-word conjunction changes the meaning of the sentence in case of a swap. 
 
NP[Την απόφαση] VP[θα πάρει] NP[η ΔΕΗ] CΟΝ[και όχι] NP[η 
κυβέρνηση]. 
 
(NP[Τhe decision] VP[will be made by] NP[the National Power 
Company] CΟΝ[and not by] NP[the government]. 
 
The number of tokens constituting an ADP, as well as the adverb lemma, also 
affect phrase swapping. Rule 5 is safely applicable when the adverb expresses manner 
or mode, but cannot be applied if, for example, the adverb is relative like όπως (how), 
όπου (where), etc., or the phrase is formed by certain adverbial expressions.   
To sum up, a total of 5 (phrases) x 6 (features/phrase) features constitute the 
feature vector, plus the binary target class: valid (yes) / not valid (no) paraphrase. 
Native speakers have manually annotated 519 instances (vectors), corresponding to 
193 original sentences, with the correct class label. 26.4% of them were classified as 
incorrect paraphrases. 
Several learning schemata have been experimented with for classification. The 
following table shows the prediction results for various stand-alone classification 
algorithms: decision trees (unpruned C4.5 tree), k-NN instance-based learning (k=5), 
support vector machines (first degree polynomial kernel function, sequential minimal 
optimization algorithm for training the classifier). Accuracy is the number of correctly 
classified instances divided by the total number of instances. Experiments were 
performed using 10-fold cross validation.    
The majority of the incorrectly classified instances are negative (not valid), 
probably due to their rare occurrence in the data, compared to the positive instances.  
 
Table 3. Results: stand-alone classifiers Table 4. Results: ensemble learning 
 C4.5 k-NN SVM 
Accuracy 75.9% 77.9% 79.2%  
 Bagging Boosting 
Accuracy 80.3% 80.1%  
 
 Empirical Paraphrasing of Modern Greek Text in Two Phases 541 
Support vector machines cope better with predicting negative labels, and reach an  
f-score of 64.1% for the rare class.  
To improve classification accuracy, ensemble learning schemata, like bagging and 
boosting, have also been experimented with. The C4.5 unpruned classifier was used 
as a base learner for bagging (the optimal bag size was 50% of the training set and 10 
iterations were performed) and boosting (AdaBoost, again 10 iterations were 
performed). Bagging leads to the best f-score for the negative class: 65.3%. 
5   Application to Steganography 
Steganography is the art of embedding hidden information in unremarkable cover 
media in a way that does not arouse an eavesdropper’s suspicion to the existence of 
hidden content underneath the surface message [11]. Hiding secret messages has 
always attracted the interest of communicating parties. Nowadays, information 
technologies take advantage of redundant information bits in the cover medium, and 
replace them with bits of the secret message, employing easy-to-use methodologies. 
There are three important aspects to steganography: capacity, security and 
robustness. Capacity refers to the amount of information that can be hidden in the 
cover medium. The security level determines the (in)ability of an eavesdropper to 
‘wiretap’ the hidden message, and robustness defines the amount of modification the 
cover medium can withstand without destroying the hidden  information.  
While several steganographic systems and methodologies have been developed for 
multimedia content, i.e. image, audio and video data [5], natural language (text) 
steganography is an emerging field [1]. Natural language steganography aims at 
hiding a message within a cover text by performing a set of linguistic alterations to 
the morphological, syntactic, or semantic structure of the text. Approaches have 
focused on three types of linguistic modifications: synonym substitution, syntactic 
and semantic transformations [15].  
Synonym substitution replaces words in a sentence with their synonyms [15]. More 
specifically, the chosen synonyms are semantically more ambiguous than the initial 
words, i.e. they belong to several synonym sets (they may take several senses), 
making it harder for a third party (eavesdropper) to choose the correct sense. 
Approaches performing syntactic transformations [9] modify the syntactic structure 
of a sentence (while preserving its original meaning). The plural number of syntactic 
structures that a sentence can appear in allows for the embedding of hidden 
information within the syntactic structure itself.   
Semantic transformations identify noun phrases that refer to the same entity (co 
references). Upon co reference detection, a repeated noun phrase may be replaced by 
a referring expression (e.g. anaphora), or a pronoun referring to an aforementioned 
entity in a previous sentence may be replaced by the entity itself.  
Though more widely employed, synonym substitution entails the danger of 
affecting the text semantics, sometimes altering its meaning. Moreover, synonym 
substitution presupposes the use of a word sense disambiguation tool that detects the 
correct sense of a word. Even if such a tool is available for a given language, its 
bounded performance will affect security. 
542 K.L. Kermanidis and E. Magkos 
5.1   The Final Pool of Paraphrases 
A primary concern of the present work is the security of the steganographic process. 
The security level of the proposed system depends on the size of the final pool of 
paraphrases. If this size is non-negligible, then it is not trivial for an eavesdropper to 
detect the actual paraphrase that has taken place and break the system. 
The positively labeled paraphrases from phase 2 constitute one part of the final 
pool of paraphrases. This part consists of paraphrases that have been produced by 
single phrase swaps, and not by combinations of swaps. The other part (due to the fact 
that the learning process does not allow for a paraphrase to be formed by 
combinations of phrase swaps) is formed by those paraphrases derived from phase 1 
that are combinations of two or more correct phrase swaps (the positively labeled 
individual phrase swaps defined by phase 2, i.e. the first part of the final pool). 
5.2   Message Embedding and Extracting 
Once the final pool of paraphrases is formed for every sentence in the input (cover) 
text, the steganographic process starts. A secret message, i.e. a sequence of bits, is to 
be hidden underneath the cover text. The embedding process is completed in 3 stages. 
First, every rule is marked with one bit value, depending on its condition. By 
condition we mean the right or the left-hand side of the rule (right or left-hand side of 
the arrow in Table 1). For example, for Rule 1 a bit value “0” could mean the left 
hand side of the rule, and then a bit value “1” would indicate its right-hand side. In the 
case of symmetrical rules (Rules 3 and 4), the condition may be determined, for 
example, by considering as NP1 the noun phrase which starts with a letter closer to the 
beginning of the alphabet that NP2. This rule marking results from a prior 
understanding between the communicating parties.  
In the next stage, for every sentence, a paraphrase is selected from its final pool. 
The selection may be performed in a round-robin fashion (i.e. to choose the 
paraphrase of each rule one at a time), or based on a secret (e.g. a symmetric 
cryptographic) key shared between the two communicating parties. We leave the 
details of establishing such a key out of the scope of this paper. In case the size of the 
pool is zero, the sentence remains unchanged, and it is not used for information 
embedding. If, however, the pool size is greater than zero, a selection is possible and 
the sentence is useful for information embedding.  
In the third stage, depending on the condition of the selected rule, a secret bit is 
embedded as follows: if the bit to be hidden is the same as the condition of the rule, 
the rule is not applied and the sentence remains unchanged, otherwise it is applied and 
the sentence is paraphrased. For example, a subject-verb sequence in the input 
sentence would mean a condition “0” for Rule 1. If the bit to be hidden is also “0”, 
Rule 1 is not applied and the sentence is transmitted as it is. If the hidden bit were 
“1”, the rule is applied and the sequence in the transmitted sentence now reads verb-
subject, instead of subject-verb. 
On the other end, the watermark extractor receives the final text. Having at his 
disposal the same rule set, (s)he is able to identify the rules that may be applied to 
each sentence. Sharing the same secret key used in the embedding process, (s)he is 
able to select the same rule used in the insertion process. In the previous example that 
 Empirical Paraphrasing of Modern Greek Text in Two Phases 543 
was Rule 1. Reading a subject-verb sequence, and knowing that this sequence 
indicates a bit value “0” for the condition of Rule 1, (s)he decides on “0” to be the 
first secret bit. Reading a verb-subject sequence would have meant a condition value 
“1” and (s)he would have decided on “1” to be the first secret bit. 
5.3   An Example 
Let the following three sentences constitute the initial message. The brackets indicate 
the chunk boundaries. 
 
[Είσαι] [καλά];    ([Are you] [well]?)  (A)  
[Πήγα] [στην εκδρομή] [χτες].  
([I went] [to the excursion] [yesterday].)    (B) 
[Να τα πούμε].   ([Let’s talk].)  (C) 
 
The applicable rules for the given text are: For sentence A, rule 9, for sentence B 
rules 5 and 6, and for sentence C no rule. So the final pool of paraphrases is: 
 
[Καλά] [είσαι];   (A1) 
[Πήγα][χτες][στην εκδρομή].  (B1 – after applying rule 5) 
[Στην εκδρομή][πήγα][χτες].  (B2 - after applying rule 6) 
 
Suppose that the hidden message is the bit sequence 10. For embedding the secret 
message, the rule condition of the first sentence is being checked. According to rule 9, 
a verb-adverb sequence corresponds to a condition of value 0. The fist hidden bit is 1. 
The two bits don’t match, so rule 9 is applied, the paraphrase is taking place and the 
first sentence to be sent is A1. 
Given the secret key, or in a round-robin fashion, the message sender decides on 
one of the two applicable rules for sentence B. Suppose that rule 6 is chosen. 
According to rule 6, a sequence of a verb and a PP introduced by the preposition σε 
corresponds to a condition of value 0. The next message bit to be embedded is 0. The 
two bits match, so rule 6 is not applied, and the second sentence is sent as it is. So the 
sent message is A1 B C. 
The receiver gets this text. He looks at the rules to decide which one can have 
possibly been applied to sentence A1. It is only rule 9. In A1 (s)he detects the 
sequence adverb-verb. According to rule 9, this indicates a condition value 1. So, 
(s)he chooses 1, which is the first hidden bit. The applicable rules for the second 
sentence are 5 and 6. Using his/her secret key, (s)he chooses rule 6, the correct rule. 
According to this rule, a sequence of a verb and a PP introduced by the preposition σε 
corresponds to a rule condition 0. (S)he chooses 0, the second hidden bit.   
5.4   Security Aspects 
In the described process, security relies heavily on two factors. First, it is very 
important for the final text to be natural. Nothing should raise the eavesdropper’s 
suspicion to any sort of hidden information underneath the surface text. Therefore, the 
final text must be grammatically correct, have the same meaning as the original text, 
and not present any anomalies (unnaturalness) in its stylistic properties. 
544 K.L. Kermanidis and E. Magkos 
A second important security aspect is the final pool size of the sentences. Even if 
the eaves-dropper does suspect the existence of hidden information, if the average 
pool size is large enough, it will be non-trivial to decide upon the correct paraphrase. 
Regarding the first security aspect, a series of experiments was conducted to test 
the grammaticality and naturalness of the initial and the final pools of paraphrases. 
The experimental setup is described in detail in the following section. Regarding the 
second security aspect, Figure 2 proves that, unlike [9] (where reportedly a sentence 
can have at most seven paraphrases – one for every rule), the average pool size in this 
work is large enough to ensure inability in detecting the correct paraphrase for 
someone who is not familiar with the secret key. 
In the final pool the average number of paraphrases per sentence is reduced (it still 
remains significantly higher than in [9]), affecting negatively the second security 
aspect and potentially steganographic capacity. The first security aspect, however, is 
strengthened considerably by the learning process of the second phase. 
6   Paraphrasing Evaluation 
Table 5 presents statistical information regarding rule applicability. The frequency 
column represents the applicability value for each rule (the number of times each rule 
is applicable in the corpus sentences) divided by the sum of the applicability values of 
all the rules. As can be seen, the subject-verb displacement (rule 1) and the adverb 
displacement (rule 5) rules constitute together around 70% of rule applications. 
The initial and the final pool of paraphrases of the 193 sentences were checked for 
grammaticality and naturalness by two native language experts. Table 6 shows the 
effect of the paraphrases on the language experts. The first error rate indicates the 
percentage of rule applications that have forced the experts to make modifications in 
order for the paraphrases to become linguistically correct and natural within the initial 
pool. Modifications entail swaps in the ordering of the chunks. The second error rate 
is the same percentage for the final pool. Inter-expert agreement exceeded 94%. 
Due to the automatic chunking process, neither the detection of chunk boundaries 
nor the detection of chunk types is error-free. Chunking errors affect paraphrasing 
performance. One significant type of chunking errors is excessive phrase cut-up. 
Looking up only the word suffix in the suffix lexicon, the chunker may assign an 
erroneous part-of-speech tag to the word, leading, thereby, to mistakes in the 
detection of phrase boundaries. For example, in the following sentence the word 
πλήρες (full) is erroneously tagged as a noun and not as an adjective, leading to false 
phrase splitting. 
 
NP[To πλήρες] NP[κείμενο της ανακοίνωσης]               
(NP[The full] NP[text of the announcement]) 
 
As can be seen in Table 6, the coordination rules present the highest rate error 
(21.8%), mainly due to unnaturalness rather than ungrammaticality. For instance, in 
the following coordination example from the corpus, swapping the NPs would result  
 
 
 Empirical Paraphrasing of Modern Greek Text in Two Phases 545 
Table 5. Frequency of rule applicability Table 6. The rules’ error rate 
Rule Frequency  
1 0.3 
2 0.08 
3,4 0.07 
5 0.38 
6 0.025 
7 0.028 
8 0.02 
9 0.1  
Rule Error Rate 1 Error Rate 2 
1 10.8% 7.2% 
2 14.3% 13.3% 
3,4 21.8% 14.1% 
5 15.9% 11.9% 
6 0% 0% 
7 0% 0% 
8 0% 0% 
9 3.9% 3.1% 
Avg 8.2% 5.5%  
 
in a grammatically correct, but unnatural paraphrase, due to the presence of the 
genitive pronoun του (his). The feature gen helps focus on such cases. 
 
NP[o Μπρετόν] CON[και] NP[οι φίλοι του] 
NP[Breton] CON[and] NP[his friends] 
 
Given the low level of paraphrasing, the results of Table 6 are quite satisfactory, 
when compared to results of approaches that utilize more sophisticated resources, like 
[9], where an average error rate of 12.7% on the applied rules is reported, or the work 
in [12], where a minimum error rate of 10.5% is reported, when applying phrasal 
replacement to create paraphrases. 
An interesting notion is the ability to simultaneously apply more than one rules to a 
sentence, or the same rule more than once. This allows for embedding more than one 
bits within a single sentence, increasing thereby steganographic capacity. 
7   Conclusion 
The application of shallow (chunk level) paraphrasing rules to Modern Greek 
sentences for steganographic purposes has been presented. Among the pool of 
paraphrases of a sentence, one paraphrase is chosen, in a manner that is se-cure and 
known only to the authorized communicating parties, and its presence can encode a 
secret message bit. The low paraphrasing level, as well as the absence of any kind of 
external linguistic resources, enables the easy portability of the methodology to other 
inflectional languages that are poor in resources. Τhe large average size of the 
paraphrase pools, makes it non-trivial for an unauthorized party to detect the correct 
paraphrase. Security also depends on the correctness and naturalness of the 
paraphrase rules, which is quite satisfactory, taking into account the low-level 
linguistic processing. An interesting future direction of the current approach would be 
to take more advantage of the pool size in order to further increase the steganographic 
capacity of the input text. 
546 K.L. Kermanidis and E. Magkos 
References 
1. Atallah, M., McDonough, C., Raskin, V., Nirenburg, S.: Natural Language Processing for 
Information Assurance and Security: An Overview and Implementations. In: Workshop on 
New Security Paradigms, Ballycotton, County Cork, Ireland, pp. 51–65 (2000) 
2. Barzilay, R., McKeown, K.R.: Extracting Paraphrases from a Parallel Corpus. In: 39th 
Annual Meeting and the 10th Conference of the European Chapter of the Association for 
Computational Linguistics, Toulouse, France, pp. 50–57 (2001) 
3. Barzilay, R., Lee, L.: Learning to Paraphrase: An Unsupervised Approach Using Multiple-
Sequence Alignment. In: Human Language Technology-NAACL Conference, Edmonton, 
Canada, pp. 16–23 (2003) 
4. Brockett, C., Dolan, W.B.: Support Vector Machines for Paraphrase Identification and 
Corpus Construction. In: 3rd International Workshop on Paraphrasing (IWP), Korea 
(2005) 
5. Cox, I., Miller, M.L., Bloom, J.A.: Digital Watermarking. Morgan Kaufmann, San Francisco 
(2002) 
6. Duclaye, F., Yvon, F., Collin, O.: Learning Paraphrases to Improve a Question-Answering 
System. In: 10th Conference of EACL Workshop of Natural Language Processing for 
Question-Answering, Budapest, Hungary (2003) 
7. Hatzigeorgiu, N., Gavrilidou, M., Piperidis, S., Carayannis, G., Papakostopoulou, A., 
Spiliotopoulou, A., Vacalopoulou, A., Labropoulou, P., Mantzari, E., Papageorgiou, H., 
Demiros, I.: Design and Implementation of the online ILSP Greek Corpus. In: 2nd 
International Conference on Language Resources and Evaluation, Athens, Greece, pp. 
1737–1742 (2000), http://www.elda.fr/catalogue/en/text/W0022.html 
8. Kozareva, Z., Montoyo, A.: Paraphrase identification on the basis of supervised machine 
learning techniques. In: Salakoski, T., Ginter, F., Pyysalo, S., Pahikkala, T. (eds.) FinTAL 
2006. LNCS (LNAI), vol. 4139, pp. 524–533. Springer, Heidelberg (2006) 
9. Meral, H.M., Sevinc, E., Unkar, E., Sankur, B., Ozsoy, A.S., Gungor, T.: Syntactic Tools 
for Text Watermarking. In: SPIE International Conference on Security, Steganography, 
and Watermarking of Multimedia Contents IX (2007) 
10. Pang, B., Knight, K., Marcu, D.: Syntax-based Alignment of Multiple Translations: 
Extracting Paraphrases and Generating New Sentences. In: Human-Language Technology 
Conference (NAACL-HLT), Edmonton, Canada, pp. 102–109 (2003) 
11. Provos, N., Honeyman, P.: Hide and Seek: An Introduction to Steganography. In: IEEE 
Security and Privacy, Oakland, USA, pp. 32–44 (2003) 
12. Quirk, C., Brockett, C., Dolan, W.B.: Monolingual Machine Translation for Paraphrase 
Generation. In: Conference on Empirical Methods in Natural Language Processing, 
Barcelona, Spain, pp. 142–149 (2004) 
13. Shinyama, Y., Sekine, S., Sudo, K.: Automatic Paraphrase Acquisition from News 
Articles. In: Human-Language Technology Conference (NAACL-HLT), San Diego, 
California, pp. 313–318 (2002) 
14. Stamatatos, E., Fakotakis, N.D., Kokkinakis, G.: A practical chunker for unrestricted text. 
In: Christodoulakis, D.N. (ed.) NLP 2000. LNCS, vol. 1835, pp. 139–150. Springer, 
Heidelberg (2000) 
15. Topkara, M., Taskiran, C.M., Delp, E.: Natural Language Watermarking. In: SPIE 
International Conference on Security, Steganography, and Watermarking of Multimedia 
Contents, San Jose, USA (2005) 
