2007年度
修 士 論 文
Topic/Author Word Model を用いた
トピック推定に関する研究
RESEARCH ON IDENTIFICATION OF THE TOPIC USING
TOPIC/AUTHOR WORD MODEL
指導教官 三浦 孝夫 教授
法政大学大学院工学研究科
電気工学専攻修士課程
06R3124 中山 基
Motoi NAKAYAMA
目 次
第 1章 序論 3
1.1 問題の背景 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
1.2 関連研究 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
1.2.1 語の取り扱い . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
1.2.2 著者/トピックの推定 . . . . . . . . . . . . . . . . . . . . . . . . 4
1.3 扱う問題 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
1.3.1 訓練データに対する分類精度の依存 . . . . . . . . . . . . . . . . 4
1.3.2 Topic/Author Wordモデル . . . . . . . . . . . . . . . . . . . . 5
1.3.3 次元縮小 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5
1.4 論文の構成 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5
1.5 発表論文 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5
第 2章 共起語を考慮に入れたEMアルゴリズムによるテキスト分類 7
2.1 前書き . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
2.2 関連研究 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
2.3 EMアルゴリズムを用いたベイズ分類 . . . . . . . . . . . . . . . . . . . 8
2.3.1 単純ベイズ法による文書分類 . . . . . . . . . . . . . . . . . . . 9
2.3.2 EMアルゴリズム . . . . . . . . . . . . . . . . . . . . . . . . . . 11
2.4 共起語を考慮した EM アルゴリズム . . . . . . . . . . . . . . . . . . . 13
2.4.1 共起語 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
2.4.2 共起語を含む EM アルゴリズム . . . . . . . . . . . . . . . . . . 13
2.5 実験 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
2.5.1 実験データ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
2.5.2 実験手順 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
2.5.3 実験結果 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
2.5.4 考察 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
2.6 結び . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
第 3章 単語分布からのトピック推定 23
3.1 前書き . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
3.2 トピック語モデル . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
3.3 情報検索とデータ次元縮小 . . . . . . . . . . . . . . . . . . . . . . . . . 24
1
3.4 実験 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
3.4.1 準備 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
3.4.2 実験結果 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
3.4.3 考察（実験 1） . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
3.4.4 考察（実験 2） . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
3.4.5 考察（実験 3） . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
3.5 結び . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30
第 4章 Topic/Author 推定方式の改善 31
4.1 前書き . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31
4.2 トピック,著者,単語のモデル化 . . . . . . . . . . . . . . . . . . . . . . 32
4.3 T/AWモデルと次元縮小 . . . . . . . . . . . . . . . . . . . . . . . . . . 33
4.4 実験 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34
4.4.1 T/AW モデルの検証 . . . . . . . . . . . . . . . . . . . . . . . . 34
4.4.2 AW モデルの検証 . . . . . . . . . . . . . . . . . . . . . . . . . . 36
4.4.3 次元縮小 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38
4.5 関連研究 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41
4.6 結び . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41
第 5章 結論 42
謝辞 43
参考文献 45
2
第1章 序論
1.1 問題の背景
近年のインターネットの普及により, 我々は膨大な量の情報が用意に得られるように
なった. しかし,その情報量の増加は驚異的で, 今現在も毎日ページの更新と共にその
情報量は増加の一途を辿っている. そしてまた,その膨大な情報量は既に我々の許容範
囲を大きく上回ってしまっているのが現状である. その為,我々の膨大な情報へのアク
セスを支援するための技術が求められる.
情報へのアクセスを支援する技術としては, 文書をその内容に応じて整理・分類する
ことが考えられる. 例として,インターネット上にはYahooのような組織的なインデッ
クス・サイトが数多く存在する. これらのサイトでは,Webページを内容やトピックに
応じて階層型に分類しており, トピックを検索する場合に有効な手法である. しかし,
このようなサイトでは人手によりページを分類し,トピックを定めており, サーチエン
ジンと比べた場合にどうしても規模が小さくならざるを得ない. また,人手による分類
は,その基準が主観に依存したものになってしまう問題点も存在する. その為,データ収
集が継続的かつ大規模に行われるような場合では, テキストを整理・分類するために,
計算機による自動的な処理,テキストの自動分類の技術が必要となる.
テキストの自動分類は,大きく 2つに分けることができる. 1つは,与えられた文書を
あらかじめ設定されているトピック（著者,著者の作品など）のいずれかに割り当てる
ものであり, もう 1つは,類似したグループにクラスタリングすることで,いくつかのグ
ループに分けるものである. テキストの自動分類は,2つの文書間の類似度を計算する
事で分類する事ができ, 一般的には,すべての文書が単語に関するベクトルとして表さ
れる, ベクトル空間モデル（VSM）によって対象世界を言及する. 本研究では,自動分
類のうちでも,トピックの推定に関して扱う.
また,一般的にテキスト形式のデータは,次元数が数万にも及ぶ高次元データである.
その為,高次元データをそのまま扱うと, 効率,計算機容量の確保及び即応性への対応が
困難となる問題点が存在する.
本研究では, テキストを整理・分類するため,トピックの推定を行う. トピックを推
定する新たな手法を提案し,更に次元縮小手法を適用する. これにより,情報検索の効
率向上に大きなヒントを与えることができる.
3
1.2 関連研究
1.2.1 語の取り扱い
一般的に,テキスト形式のデータでは,テキストは語の並びとして構成されるが, その
語をどのように選択するかに明確な決まりは存在しない. しかし,テキスト形式のデー
タでは語の同義性や類犠牲から, いくつかの語が同時に生じる可能性が高い. 実際,同
義語と多義語を考慮することで,分類の精度を向上させる研究が行われている. 複合語
や共起性の強い語を考慮するかどうかは,分析結果に大きな影響を与える.
1.2.2 著者/トピックの推定
過去一世紀以上にわたる論争の一つに著者の推定問題がある. テキストから何らかの
特徴を捉えて著者の推定を行うもので, 日本のグリコ森永事件の脅迫状の分析やシェー
クスピア実在論が挙げられる. これと並ぶものとして存在するのが,トピックの推定で
あり, 何のトピック（興味ある事柄や出来事）を論じているかを推定する. トピックを
推定することは,効率よく検索するための情報の要約,トピックへの文書の自動分類と
も関連がある. また,これまでの研究によると, テキストから著者よりもトピックとの
関連性を論じる方が分析しやすいことが知られている.
1.3 扱う問題
1.3.1 訓練データに対する分類精度の依存
テキストの分類には経験的にテキストの分類には教師つき学習が有効であり, 様々な
機械学習手法が研究されている. 教師つき学習にはあらかじめ人手により正確に分類
されたデータ（訓練データ）が必要であるが, データ収集が継続的かつ大規模に行われ
るような場合では, 人手を使ってデータを分類することは時間と労力などのコストの大
幅な増大を意味することにもなり, その分類作業自体も主観的なものになってしまう問
題がある. また,分類の精度は訓練データに依存するため,訓練データをどう作成する
かといった事も問題となる.
本研究では,EMアルゴリズムを用い,少量の訓練データで未分類データを分類しなが
ら, その未分類データを訓練データとして利用するステップを繰り返すことで,この問
題を解決する. この際,一旦拡大したエラーが繰り返しにより,修復されにくく誤りを
訂正することが困難であるという問題があるが, 語の共起性を考慮に入れることでこれ
を解決する. 実験により本手法の有効性を示す.
4
1.3.2 Topic/Author Wordモデル
T/AW モデルとは,同一著者の下では,各トピックは対応する語集合の多項式分布確
率で表されるという仮定である. つまり,著者はトピックに依存する多項式分布確率に
より語を確率的に選択していると仮定される. これは語の分布の検証により,文書のト
ピックを推定することが可能であることを意味する. 他方,AWモデルは著者がトピッ
クに独立に著者自身の語集合の多項式分布確率で表されるという仮定である. 一般的
にT/AWモデルは広く信じられているが,AWモデルにおいてはそうではない.
本研究では,T/AW モデルおよびAWモデルが実際に成り立つかどうか検証を行い,
ここでは,様々な重み設定方式を検討し,T/AWモデルを用いて分類の精度向上に寄与
する方式を探る.
1.3.3 次元縮小
一般的にテキスト形式のデータは,次元数が数万にも及ぶ高次元データであるため,
高次元データをそのまま扱うと,効率,計算機容量の確保及び即応性への対応が困難と
なり, 性能及び精度に差が生じるという問題がある. その為,次元縮小技法を用いるこ
とで,高次元文書ベクトルを低次元空間に射影し, 効率よく探索範囲を絞り込む必要が
ある.
本研究では,次元縮小技法を適用し,Topic/Author Wordモデルへの効率的で有効な
処理の方法を提案する.
1.4 論文の構成
本研究では,以上の問題について以下の構成で論じる. 第 2章では,語の共起性をEM
アルゴリズムに融合させることで, 少ない訓練データからテキストデータを自動分類す
る新たな手法を提案し, 実験によりその有効性を示す. 第 3章では,トピック語モデル
の検証を行い,モデルへの効率的で有効な処理の方法を示す. 第 4章では,同一著者の
下での語分布からのトピックを推定できるという仮説を検証する. 著者と語分布の関
係についても検証を行い,これが推定に適さないことを示す. 更に,推定方式の改善を
提案し,実験によりその有効性を示す. 第 5章で結論とする．
1.5 発表論文
1. 中山基, 三浦孝夫: “W共起語を考慮したEMアルゴリズムによるテキスト分類”,
データ工学ワークショップ（DEWS）, 2006.
5
2. 中山基, 三浦孝夫: “単語分布からのトピック推定”, 情報処理学会 第 142 回 デー
タベースシステム研究会および情報処理学会 第 87 回 情報学基礎研究会 , 情報
処理学会 2007-DBS-142(1).
3. Motoi, N. Miura, T.: “Identifying Topics by using Word Distribution”, IEEE
Pacific Rim Conference on Communications, Computers and Signal Processing
(PACRIM),2007,pp.245-248.
4. 中山基, 三浦孝夫, 塩谷勇: “Topic/Author 推定方式の改善”, データ工学ワーク
ショップ（DEWS）, 2008.
6
第2章 共起語を考慮に入れたEMアル
ゴリズムによるテキスト分類
2.1 前書き
近年,インターネットを介して膨大な量のテキストデータが電子的に利用可能であり,
この傾向は増加をたどる一方である. こういったテキストデータの管理・検索をより
高度に行うためには, テキストの分類が欠かせない. しかし,人手によるテキスト分類
は時間と労力を要するのみならず、分類作業が主観的となることから信頼性の確保も
重要な問題となる。
これを解決する技術として,経験的にテキストの分類には教師つき学習が有効である
ことから，決定木やベイズ分類器などの機械学習技法が研究されている [1, 4].　教師
つき学習では，あらかじめ人手により正確に分類されたデータ（訓練データ）を用い
てその性質を分析し，一般的な特性を抽出するという手法がとられる．しかし,人手に
よる訓練データの生成は時間と労力を要するのみならず、分類作業が主観的となるこ
とから信頼性の確保も重要な問題となる。実際問題として,特定の応用分野を限定した
場合でさえ，あらかじめ分類規則（データ)を必要とするための精度の高い訓練データ
を確保する必要がある。
この解決策としてテキスト自動分類技術が必要とされている. しかも,少ない訓練
データから自動分類を行うことのできる EMアルゴリズムを用いることの有用性は高
い。すなわち,少量の訓練データで未分類データを分類しながら，その未分類データを
訓練データとして利用するというステップを繰り返すEMアルゴリズムは,テキスト分
類の手法に有効であろう.
反面,EMアルゴリズムの問題も広く知られている [2]．まず,結果が初期値に大きく
依存しがちとなる．また，繰り返し回数をあらかじめ設定できないため，収束が遅く
なることが多い．同時に，収束しすぎると過学習となることも問題である．一旦拡大
したエラーは修復されにく，誤りを訂正することはきわめて困難であることも重要で
ある. この問題を改善するためには,分類器を多重に用意し, 用いるデータの特性に応
じて変化させる方法がよいとされる [5]．
そこで,本稿では,語の共起性を EM アルゴリズムに融合させ, テキスト分類の精度
向上のための新たな方式を提案する. 通常,テキストデータでは語の同義性や類義性か
ら,いくつかの語が同時に生じる可能性が高い. そのため共起語は,特に高頻度の場合
において強い相関性があると考えることができる. この共起語を分類の際の手がかり
7
として追加することで,テキスト分類の精度向上をはかる. また,本稿では語の共起を
文単位で考える (つまり，共起する語は複数の文に渡ってつながりを保持するとはみな
さない). 一定回数以上の共起頻度を有する語に限定することで計算の効率を高め、効
果的なつながりを重視する. EMアルゴリズムを用いたテキスト分類の精度を向上でき
ることを示すため，実験により有効性と考察を示す.
第 2 章で関連研究を示し, 第 3 章で EM アルゴリズムと共起性概念をまとめる．第
4 章では本稿で提案する方式を導入し，第 5 章で実験によりその有用性を述べる．第
6 章は結論を述べる．
2.2 関連研究
文書を分類する方法としては,上嶋らの研究がある [9]. 上嶋らは,ベイズ法での文書
分類に,同義語と多義語を考慮することによって, その分類精度を向上をさせる手法を
提案している. 通常の文書分類では,単語の持つ意味は考慮せず,単語を単に記号的に
扱う. しかし,通常,文書内には複数意味を持つ単語（多義語）が存在し, 複数の単語が
同じ意味を持つ場合（同義語）もある. これらの語を考慮することで,精度を向上させ
るのがこの手法である.
本稿では,共起語を考慮してEMアルゴリズムでの文書分類の精度を向上させる. EM
アルゴリズムを用いた文書分類方法としてはNigamらの研究がある [5]. 単純ベイズ法
に EMアルゴリズムを組み合わせた文書分類を提案しており，旋律の分類・検索 [10]
やタイムスタンプの推定 [8]にはきわめて有用であることが知られる．しかし，単純に
正規分布に従う確率密度を組み合わせて利用する方法ではきわめて低い分類精度しか
達成できず，さまざまに変形を加えた試みを提案している．この変形はテキストデー
タに生じる特性に基づくものではなく，むしろ確率分布の特性をどう活かすかという
視点に立つものが多い．
EMアルゴリズムを単純に適用すると,精度が逆に悪化することが知られている [11].
つまりある地点まで精度が向上しても, 最終的にはそれよりも低い精度で収束する. 場
合によってはラベル付き文書のみから生成した分類規則の制度よりも低い精度に収束
することもあるため,本稿では繰り返し回数を 20回までとした.
2.3 EMアルゴリズムを用いたベイズ分類
本稿では,トピックをラベルとする訓練データを用いて, 未分類テキストデータを分
類するための EM アルゴリズムを示す. 本稿では,EMアルゴリズムと単純ベイズ法を
組み合わせた手法を用いる.
8
2.3.1 単純ベイズ法による文書分類
文書 dをトピック集合 C = { C1, . . . , Cn } に分類する. ベイズ規則による分類とは，
文書 d がトピック C に属する確率 P (C|d) の確率分布を求めることである. 排他的な
分類の場合,最大事後確率をとるトピック C へ文書 dを分類することで分類のミスを
抑えることができる. 以下では,トピックラベルを次のように定める.
Ck = argmaxC∈CP (C|d)
ベイズ規則を次のように定義する.
P (Ck|d) = P (Ck) ×
P (d|Ck)
P (d)
(2.1)
すなわち,ベイズ規則での分類規則生成（訓練）は訓練データ集合から,確率分布 P (Ck), P (d), P (d|Ck)
を推定することである.
しかし,文書ベクトル d=(w1, . . . , wm)はほぼすべての文書において異なり, P (d|Ck)
や P (d)の推定が問題であるため, 一般に,特徴量 wj の出現は,統計的に他の単語出現
とは独立であるという仮定をおき,各文書を単語の集合と考える単純ベイズ法を使うこ
とが多い. 単純ベイズ法では P (d|Ck) を以下の形式に分解して考える.
P (d|Ck) =
|d|∏
i=1
P (wi|Ck)
これにより,文書主導の排他的分類の場合,ベイズ規則は以下のように表すことができる.
P (Ck|d) = P (Ck) ×
|d|∏
i=1
P (wi|Ck) (2.2)
また,ここでは文書内での単語の出現頻度は考慮せず, 単語の出現有無のみを考えるバ
イナリ独立モデルを用いる.
バイナリ独立モデルとは,単語の出現頻度を 0か 1で表現するもので, 文書内に出現
したときは 1,出現しなかったときは 0とするものである.
例題 1 ベイズ手法によるクラス分類例を示す. トピックCを,”C = {C1, C2}”とし,そ
れぞれが文書 d1,d2として与えられているとする.（D = {d1, d2}）
各トピックの語集合を,
C1 = {a, b, c} C2 = {a, d, e}
として,未分類文書（d3）を分類する.
単純ベイズ法によりトピックへ分類を行う. 定義より,P (C1|d1) = P (C2|d2) = 1,
P (C1|d2) = P (C2|d1) = 0, P (C1|d3) = P (C2|d3) = 0 である. ベイズ規則を用いて,
P (Ck|d3) = P (Ck) ×
P (d3|Ck)
P (d)
9
を最大化するトピック Ck を求める. D = {d1, d2, d3}には 3件の文書が含まれおり, ト
ピックC1,C2にはそれぞれ 1件ずつ含まれている. したがって,
P (Ck) =
1 +
∑|D|
j=1 P (Ck|dj)
|C| + |D|
P (C1) = P (C2) =
1 + 1
2 + 3
=
2
5
文書 語群 単語数
d1 a, b, c 3
d2 a, d, e 3
d3 a, b, c, f 4
トピック P (a|∗) P (b|∗) P (c|∗) P (d|∗) P (e|∗) P (f |∗)
C1 2/4 2/4 2/4 1/4 1/4 1/4
C2 2/4 1/4 1/4 2/4 2/4 1/4
各確率は,スムージングを行っている. この例題では,確率が 0となるのを防ぐため,
単純に分母と分子に 1を足すことにする.
単純ベイズ法の仮定から,
P (Ck|d3) = P (a|Ck) × P (b|Ck) × P (c|Ck) × P (d|Ck) (2.3)
ここで P (C1|d3), P (C2|d3)をそれぞれ求める.
P (C1) × P (d3|C1)
= P (C1) × P (a|C1) × P (b|C1) × P (c|C1) × P (f |C1)
=
2
5
× 2
4
× 2
4
× 2
4
× 1
4
= 0.0125
P (C2) × P (d3|C2)
= P (C2) × P (a|C2) × P (b|C2) × P (c|C2) × P (f |C2)
=
2
5
× 2
4
× 1
4
× 1
4
× 1
4
= 0.0031
これより,P (Ck|d3)を最大化するCkはC1となり,文書 d3はトピック”C1”に割り当て
られた.
また,d3のトピック所属確率 P (Ck|d3)は,
P (C1|d3) = P (C1) × P (d3|C1)/P (d)
=
0.0125
0.0125 + 0.0031
= 0.80
P (C2|d3) = P (C2) × P (d3|C2)/P (d)
=
0.0031
0.0125 + 0.0031
= 0.20
10
2.3.2 EMアルゴリズム
EMアルゴリズムとは,データの欠損部分を最尤推定により求め,欠損部分が分かれば
その形は単純かつ解析的に表現できるという仮定を置く. Expectation（期待値）,Max-
imization（最大化）は,それぞれ欠損値の推定, 期待値を得る過程を与えるパラメータ
の推定に対応している. この EステップとMステップを繰り返すことにより,モデル
の対数尤度を最大化するパラメータを求める手法である.
EMアルゴリズムを文書分類に適用するために次を用いる.
1. 入力：ラベル付文書,ラベルなし文書
2. ラベル付文書のみから単純ベイズ分類規則 θ̂を生成する
3. 以下のステップを一定回数,または分類規則が収束するまで繰り返す
(a: E-step) 現在の分類規則 θ̂を使用し，ラベルなし文書を各トピックへ分類す
る (P (cj|di; θ̂))
(b: M-step) 推定された事後確率（分類結果）による最尤推定を利用して, 分類
規則 θ̂ = P (D|θ)P (θ)を再度生成する.
4. 出力：分類規則 θ̂
本稿では P (wi|Ck)（分類規則）を以下の式で求める.
P (wi|Ck) =
1 +
∑|D|
j=1 N(wi, dj)P (Ck|dj)
|V | + ∑|V |i=1 ∑|D|j=1 N(wi, dj)P (Ck|dj) (2.4)
ここでDは文書データ全体を表し、wiはデータ内の各単語を表す. またN(wi, dj)は文
書 djにおける単語wiの発生回数であるが,本稿では出現の有無により 0か 1の値をと
る. さらに,P (Ck|dj)は前述の文書 djがトピックCkに属する確率であり,ラベル付けさ
れたデータに関しては, そのラベル付けられたトピック Cmにおいては,P (Cm|dj) = 1
であり, それ以外のトピックに対しては 0をとる. ラベルなしデータに関しては,最初
は全カテゴリに対して 0であるが, 最初は通常のベイズ分類により, その後はEMアル
ゴリズムの E-stepにより,徐々に適切な値へと更新される.　式 (2)と (3) により EM
アルゴリズム内で分類規則を生成する. 同様に P (Ck)は以下のように与えられる.
P (Ck) =
1 +
∑|D|
j=1 P (Ck|dj)
|C| + |D|
(2.5)
式 (3),(4)は,それぞれ P (wi|Ck), P (Ck)のスムージングを行っている.
例題 2 例 1でのトピック推定の結果,
P (C1) =
1 + 2
2 + 3
=
3
5
11
トピック P (a|∗) P (b|∗) P (c|∗) P (d|∗) P (e|∗) P (f |∗)
C1 3/8 3/8 3/8 1/8 1/8 2/8
C2 2/4 1/4 1/4 2/4 2/4 1/4
P (C2) =
1 + 1
2 + 3
=
2
5
に変わる. 条件確率にも変化が起こる. ここで，
P (Ck|d3) = P (Ck) ×
P (d3|Ck)
P (d)
を最大化するCkを求めるため P (Ck|d3) を計算する.
P (C1) × P (d3|C1)
= P (C1) × P (a|C1) × P (b|C1) × P (c|C1) × P (f |C1)
=
3
5
×
3
8
×
3
8
×
3
8
×
2
8
= 0.0079
P (C2) × P (d3|C2)
= P (C2) × P (a|C2) × P (b|C2) × P (c|C2) × P (f |C2)
=
2
5
×
2
4
×
1
4
×
1
4
×
1
4
= 0.0031
また,d3のトピック所属確率 P (Ck|d3)は,
P (C1|d3) = P (C1) × P (d3|C1)/P (d)
=
0.0079
0.0079 + 0.0031
= 0.72
P (C2|d3) = P (C2) × P (d3|C2)/P (d)
=
0.0031
0.0079 + 0.0031
= 0.28
この結果、P (Ck|d3)を最大化するトピック Ck は C1であり,”d3”は再度トピックC1
に割り当てられる. EMアルゴリズム部で,変動が起きなくなるまで計算を繰り返す.
12
2.4 共起語を考慮した EM アルゴリズム
2.4.1 共起語
本稿では,語の対 (wi, wj)の dにおける共起度 co(wi, wj)を次のように定義する
1 :
co(wi, wj) =
∑
s∈d |wi|s|wj|s
ここで |x|s は文 sにおける要素 xの出現回数を表し, xが語の場合には |x|sは文 s中
の語 xの出現回数を表す. 式 (5)は,ある文 sに出現した語wiは s中のすべての語wjと
共起しているとみなした共起頻度を表す. すでに述べたように，本稿では語の共起を
文単位で考慮し，複数の文にわたる影響を考えない.
2.4.2 共起語を含む EM アルゴリズム
本章では,EMアルゴリズムを拡張し，単語同士の共起関係を考慮する. 基本的なア
イデアは単純である．すなわち，未分類文書 d が,トピックCkに割り当てられたとき,
トピックCkに属する単語w1, ..., wnのいずれかと, 文書 d 内において共起し,しきい値
以上の共起回数を持つ語 xをトピックCkに追加する. このEMアルゴリズムを共起語
で拡張する手法が，実際に分類精度を向上させるものとなるであろうか？一般にテキ
ストデータでは，語の同義性や類義性からいくつかの語が同時に生じる可能性が高い．
つまり共起語は，特に高頻度の語の場合は強い相関性があると考えることができ，実
際これを手がかりに文書分類の研究がなされている [3, 6]．
更に，本稿では追加する共起語の頻度をある一定以上の割合とする. しきい値 5割
で,文書内の最大共起頻度が 10回ならば,追加語は共起頻度が 5回以上のものとなる.
これは,追加語の共起頻度に制限を設けることによって, 追加語とトピックの語集合と
の相関の強さを操作するためである. しきい値を高く設定すれば,相関的な語が追加さ
れるので, より精度が向上するはずである. 逆に,しきい値を低くすれば,トピックと相
関の低い語が追加される機会が多くなるので,高く設定した場合よりも精度は悪化する
と考える.
例題 3 共起語を考慮に入れた EMアルゴリズムによるクラス分類例を示す.
ここでは,例 1と同じデータを用いて例を示す. トピックCを,”C = {C1, C2}”とし,そ
れぞれが文書 d1,d2として与えられているとする.（D = {d1, d2}）
各トピックの語集合を,
C1 = {a, b, c} C2 = {a, d, e}
として,未分類文書（d3）を分類する.
1本稿では共起を文単位で考えている.そのため,共起を 3単語まで広げる必要がない. つまり,3単語
で共起していれば,必ずそのうちの 2単語も共起していることになる. しかし,これは同時に,熟語を考
慮しないということにもなる. KeyGraph[6] では文章に限っていないために”共起語集合”を扱う.
13
はじめに単純ベイズ法によりトピックへ分類を行う. 計算方法および結果は,例 1と
同様である.
次に推定されたトピックに共起語を追加する. 追加する語は,トピックの語集合の単
語のいずれかと,文書 d3内において共起している語が対象となる. このとき,対象とな
る語がすでにトピックの語集合に含まれている場合は, 追加を行わない.
ここで,文書 d3 = {a, b, c, f}のとき,語対 (wi, wj) の共起度 co(wi, wj)はそれぞれ,
co(a, f) = 3
co(a, b) = co(b, c) = co(b, f) = 2
co(a, c) = co(c, f) = 1
である. しきい値を 80%に設定すると,追加対象となる共起語は, 追加語の共起度が
最大頻度 ×しきい値 = 3 × 0.8 = 2.4 よりも大きいため, 共起度 3の語のみが対象と
なる. これより,追加する語の条件は,トピックC1 = {a, b, c}のいずれかと共起する語,
かつ共起度 3の語となる. d3内において,この条件を満たす語は”f”のみであり,これを
トピック C1の語集合に追加し,C1 = {a, b, c, f}とする. 以降,トピック C1の語集合を
C1 = {a, b, c, f}として分類を行う.
新たに定まった確率値を用いて,再度 d3の所属確率を求める (EMアルゴリズム).
さらに,推定されたトピックに共起語を追加する. 前述と同様に共起語の追加作業を
行うが,すでに対象となる語が含まれている場合は追加作業はない.
最後に，EMアルゴリズム部で再度計算を繰返し,変動が起きなくなるまで計算を繰
り返す.
2.5 実験
2.5.1 実験データ
本稿では，シェークスピアによる戯曲 12 作品をテストコーパスとして使用し, タイ
トルをトピックとして考える. 各タイトルはいずれも 5 章 (chapter)から構成され各章
は場 (Scene)からなる. 実験で用いたタイトルは次の 12作品である.
1. 真夏の夜の夢 (全 5章 9場)
2. 終わりよければすべてよし (全 5章 23場)
3. お気に召すまま (全 5章 22場)
4. シンベリン (全 5章 26場)
5. 恋の骨折り損 (全 5章 10場)
6. から騒ぎ (全 5章 18場)
14
7. ペリクリーズ (全 5章 20場)
8. 間違いの喜劇 (全 5章 11場)
9. ヴェニスの商人 (全 5章 19場)
10. ウィンザーの陽気な女房たち (全 5章 23場)
11. じゃじゃ馬ならし (全 5章 14場)
12. テンペスト (全 5章 9場)
初期訓練データを各トピックの第 1章のすべての場から生成し, 2章以降の全 168 場
をテストデータとして使用する (表 1 参照). それぞれの場のタイトルを隠して分類を
行い,後に適切に分類がされたかを判断する. 各場データはあらかじめステミング [7]
および不要語処理を行う．また,初期訓練データにZipfの法則を適用し,上位 30単語を
トピック単語とする. 各トピックの構成と,ストップワードを除去した後の各場の単語
数を表 1に示す.
共起語の計算は,各場ごとに行う. 本稿では,共起語を各場内において,出現頻度が
2%以上の語のみを計算する. これは,相関性の極めて低い単語を,共起語として算出し
ないためである.
2.5.2 実験手順
提案手法の評価を以下 3点において行う.
• 精度への影響
• しきい値による正答率の変化
• 多数トピックへの共起語の削除の有効性
提案手法の有効性を示すため，本実験では，EMアルゴリズムによる分類と, 提案手
法である共起語を考慮に入れたEMアルゴリズムの正答率の変化を比較することで. 提
案手法を検証する.正答率は次式で定義される.
正しく分類された総文書数
総文書数
× 100（%） (2.6)
推定されたトピックと実際のトピックとの比較を行い,精度への影響を検証する. し
きい値を 10%から 100%と変化させて場合と,しきい値を設けなかった場合についても
実験を行う. 最後に,多数トピックへの共起語の削除の有効性を, 同じしきい値で,削除
した場合と,削除しなかった場合で比較し検証する.
また,本稿では EMアルゴリズムの繰り返し回数を, 0回（単純ベイズ部）,5回,10
回,15回,20回の 5パターンについて示す.
15
表 2.1: データの構成とサイズ
トピック 場 1 章 2 章 3 章 4 章 5 章
1 1 512 146 120 123 219
2 198 85 242 24 -
2 1 462 123 7 39 16
2 224 26 69 43 31
3 482 154 8 167 184
4 - 21 25 14 -
5 - 36 49 52 -
6 - - 44 - -
7 - - 15 - -
3 1 322 30 12 89 36
2 429 35 189 12 49
3 280 8 46 83 22
4 - 13 26 - 127
5 - 111 65 - -
6 - 34 - - -
7 - 55 - - -
4 1 416 27 37 8 11
2 78 118 39 240 14
3 114 68 58 28 52
4 350 7 115 22 97
5 227 - 90 - 296
6 479 - 50 - -
7 - - 9 - -
5 1 576 137 103 97 76
2 311 - 140 96 516
3 - - - 243 -
6 1 518 185 122 156 186
2 69 23 67 47 48
3 158 126 46 - 18
4 - - 86 - 77
5 - - 47 - -
6 - - 25 - -
7 1 110 90 60 53 140
2 399 31 21 68 58
3 302 25 23 54 -
4 92 58 - 3 -
5 353 122 - 109 -
8 1 376 66 76 71 238
2 233 108 99 47 -
3 - - - 46 -
4 - - - 109 -
9 1 624 17 68 249 165
2 346 106 170 13 -
3 - 11 19 - -
4 - 26 39 - -
5 - 30 51 - -
6 - 42 - - -
7 - 34 - - -
8 - 25 - - -
9 - 52 - - -
10 1 444 115 60 40 20
2 40 148 42 118 7
3 258 47 115 9 10
4 304 - 50 47 1
5 - - 60 81 137
6 - - - 30 -
11 1 336 128 53 117 84
2 341 139 131 59 112
3 - 224 - 103 -
4 - - - 54 -
5 - - - 43 -
12 1 187 231 49 146 200
2 970 108 87 - -
3 - - 66 - -
16
2.5.3 実験結果
表 2に,ベイズ手法に EMアルゴリズムを組み合わせた分類の正答率と, 共起語を考
慮に入れた EMアルゴリズムの各しきい値に対する正答率を示す. 表 3に,EMアルゴ
リズムと,共起語を考慮に入れたEMアルゴリズムの各実験結果のうち, 最も精度の高
かったしきい値との分類結果の違いを示す.
また,共起語の追加に対する精度の変化過程を評価するため,代表的な例について考
察する. 表 4に,トピック単位での正答率を示す. 削除を繰り返し,最終的に残った追加
語を表 5に示し,削除が行われた語とそのタイミングを表 6に示す. 分類の変化の例を
表 6に示す. この表 7について,○は EMアルゴリズムでの不正解が,提案手法に置い
て正答に変わったデータ. ×は逆に不正解に変わったデータを示す. また,表 8にはし
きい値による追加語が生じた件数を示し, 表 9に多数トピックへの追加語の削除につい
ての正答率を示す.
表 2.2: 正答率
％ Bayes EM(5) EM(10) EM(15) EM(20)
Bayes + EM 29.17 27.38 28.57 38.10 38.10
％ しきい値 EM(0) EM(5) EM(10) EM(15) EM(20)
B 100 38.10 30.36 29.76 48.81 48.81
a 90 38.69 27.38 29.76 50.00 50.00
y 80 41.07 29.76 30.95 54.76 54.76
e 70 37.50 26.79 30.95 51.19 51.19
s 60 32.74 21.43 28.57 47.62 47.62
+ 50 39.88 27.38 30.36 45.24 45.24
共 40 27.38 25.60 26.79 43.45 43.45
起 30 28.57 24.40 21.43 36.31 36.31
+ 20 32.14 22.02 22.62 38.10 38.10
E 10 32.14 23.81 21.43 36.90 36.90
M 0 31.55 23.21 21.43 35.71 35.71
表 2.3: EMアルゴリズムと共起語を考慮に入れた EMアルゴリズム（しきい値 80%）
の正答の違い
Bayes+EM 繰返し
B 個数 正解 間違い
a 正解 39 30 0
y 間違い 10 89
e 正解 29 21 5
s 間違い 6 112
+ 正解 17 21 10
共 間違い 5 125
起 正解 59 33 15
+ 間違い 5 71
E 正解 59 33 20
M 間違い 5 71
17
表 2.4: トピックごとの正答率
しきい値 80 ％
トピック EM(0) EM(5) EM(10) EM(15) EM(20) 総場数
1 57.14 0.00 14.29 71.43 71.43 7
2 20 20 25 40 40 20
3 42.11 36.84 36.84 47.37 47.37 19
4 35.00 35.00 35.00 55.00 55.00 20
5 75.00 12.50 25.00 62.50 62.50 8
6 40.00 40.00 40.00 80.00 80.00 15
7 33.33 26.67 26.67 46.67 46.67 15
8 33.33 33.33 44.44 66.67 66.67 9
9 47.06 29.41 29.41 47.06 47.06 17
10 63.16 47.37 47.37 63.16 63.16 19
11 25.00 16.67 0.00 25.00 25.00 12
12 42.86 28.57 28.57 85.71 85.71 7
Bayes+EM
トピック EM(0) EM(5) EM(10) EM(15) EM(20) 総場数
1 42.86 42.86 42.86 71.43 71.43 7
2 20.00 10.00 15.00 25.00 25.00 20
3 21.05 26.32 26.32 26.32 26.32 19
4 25.00 25.00 20.00 35.00 35.00 20
5 25.00 12.50 12.50 25.00 25.00 8
6 33.33 20.00 33.33 53.33 53.33 15
7 13.33 20.00 20.00 26.67 26.67 15
8 33.33 22.22 33.33 44.44 44.44 9
9 35.29 23.53 23.53 29.41 29.41 17
10 52.63 57.89 63.16 68.42 68.42 19
11 8.33 16.67 8.33 8.33 8.33 12
12 57.14 71.43 57.14 71.43 71.43 7
表 2.5: 共起語を考慮した EMアルゴリズム しきい値 80% 追加語
文書 分類先 追加語 繰返し 文書 分類先 追加語 繰返し
1 7 fair 2 71 5 beatric 3
1 3 king 4 72 5 hath 0
2 12 fair 8 72 5 fanci 0
3 3 hear 3 72 10 fanci 2
3 3 honour 3 74 9 light 0
3 6 hear 4 74 6 light 2
29 11 our 2 81 5 ill 2
29 10 our 3 81 9 ill 10
29 8 make 4 84 9 twenti 0
39 11 ay 3 87 10 ey 0
39 11 young 3 87 10 clock 0
39 11 man 3 88 10 sir 4
39 11 mine 3 133 4 bianca 3
39 11 sir 3 139 4 presum 0
39 4 find 8 143 7 blame 2
40 8 call 3 143 3 blame 3
41 10 thee 0 153 5 thee 4
44 11 morrow 4 161 10 brook 0
51 3 great 0 163 10 betrai 0
52 5 write 0 163 10 amaz 0
60 8 man 5 165 11 ann 4
71 9 beatric 2
18
表 2.6: 削除語
文書 分類先 追加語 繰返し 文書 分類先 追加語 繰返し
12 4 love 11 127 2 hugh 5
23 9 master 4 127 8 husband 11
31 2 hath 11 149 3 art 4
39 11 find 3 149 3 posthumu 4
39 11 good 3 153 6 thy 0
40 8 give 3 156 6 live 3
40 5 desir 4 157 9 thou 2
43 6 fit 5 158 11 brought 3
44 8 morrow 3 160 7 night 4
103 10 wit 5 165 2 mistress 5
124 10 half 4
2.5.4 考察
表 2より明らかなように, 語の共起性をEMアルゴリズムに融合させてテキスト分類
を行うことは,精度を向上させることができる. また,本稿では,追加語を一定回数以上の
共起頻度を有する語に限定し, より効果的なつながりを重視した. しきい値を 80%にし
た場合に最も良い精度を得たが, 100%,90%,70%の場合においても,EM(15),EM(20)に
おいて,10%以上の精度向上が見られる. 逆に,このしきい値を設けなかった場合,10%と
低く設定した場合において, 精度の低下が見られる. 共起を考えるにあたり,相関の高
い語を追加することが有効であることがいえる.
本稿で提案したように, EMアルゴリズムに共起語を考慮に入れて分類を行うことが,
有効的であったことが確認できる.
共起語の影響
本実験において,誤分類による共起語の追加が正答分類による追加の数を上回ったが,
精度が向上した. 誤分類の追加語の影響より,正答分類での追加語の影響が強いことが
挙げられる. これは,トピックの語集合は,トピックへの依存が比較的強い語で構成され
ていることからくる. 文書の構成は,単語の意味を考慮して書かれている場合が多く,本
来のトピックの語集合との共起と, 誤分類先の語集合の共起は違う意味を持つ. そのた
め,誤分類の追加語において,トピック特有の単語が追加されるケースは少なく, 誤分類
においては汎用性のある単語が追加されるケースが多い. また,このような追加語は多
数トピックの共起となるケースも多く, 本稿で用いた手法における,削除の対象となる.
しかし,誤分類は必ずしも精度の悪化を招く要因とはならない. 共起が起こるという
ことは相関を持つ語であるということである. 本稿では,第 1章の単語に Zipfの法則を
適用し,その頻出上位 30単語をトピックの語集合とした. その際,30単語には入らなかっ
たが,トピックの単語である語も存在する. 表 5より,文書（60）において,単語（man）
が追加語としてトピック 8に追加されている. この文書（60）は本来はトピック 3に
分類される文書である. しかし,この単語（man）が文書（145）の中で存在しており,
19
EMアルゴリズムで不正解だった分類が,本稿の提案手法において正答に移り変わった.
このように,誤分類による追加は必ずしも精度の悪化を招く要因とはならない.
また,共起語の追加で精度が悪化するケースであるが, 文書（97）において著しく悪
化しているケースがある. この文書（97）は本来はトピック 1に分類される文書であ
るが, 提案手法において,トピック 5に分類されることで精度が悪化している. この文
書はトピック 1の 4章:場 2に当たる文書であり,総単語数は 24である. このような少
ない単語数からなる文書は,1単語のトピック推定に与える影響は他のものに比べて大
きくなるのだが, 文書（72）において,この文書（97）にも含まれる単語（hath）がト
ピック 5に分類されている.
表 4より,各トピックごとについても,EM(0),EM(15),EM(20)でほぼ全てのトピック
で正答率の向上が見られる. これは,直接共起語の追加がおきていないトピックに対し
ても向上しており, 共起語の追加は,追加の起きたトピックのみならず,その他のトピッ
クの精度にも影響を与えると見れる. 他トピックへの共起語の追加により,各トピック
に対する所属確率にも変動が起き,精度にも影響が出る. 本提案手法である共起語の考
慮は,特定トピックのみでなく,トピック全体での精度向上に役立つといえる.
また,共起語を考慮に入れたEMアルゴリズムにおいて,EM(15),EM(20)で最も良い
精度となっている. 繰り返し回数による精度向上の幅に,途中での追加語が影響してい
ることは考えられるが, EMアルゴリズムの結果においても同様の変化がおきている.
そのため,EM(15),EM(20)で最も良い精度となる大きな要因としては, EMアルゴリズ
ムの繰り返し計算により,正しいトピックに分類されていくことが大きいと考えられる.
これら,正答分類による追加語と,誤分類における追加語の影響で,本提案手法におい
て精度の向上が見られた.
しきい値の影響
本稿では,実験を行う際に,追加する共起語の共起頻度にしきい値を設けた. 表 2よ
り,しきい値を高くし,相関の強い語を追加することの有効性が確認できる. このとき
の追加語のしきい値としては,本実験において,80%の精度が最も良い. これは,80%に設
定する事で,100%の場合よりも相関の強い語が多く追加される理由による. また,しき
い値を下げると,依存性の弱い単語,つまりどの文書でも出現してくるような単語が多
く追加される. 意味のない単語の追加,各トピックの差が弱まる事で精度が悪化し,ま
た誤分類の追加語が増える事となる. 追加件数にあまり差がないのは,同一文書におい
て,分類先が変わるたびに同じ単語の追加が起きているためである.
多数トピックへの共起語の追加
本稿では,多数トピックへの共起語の追加を制限し,3つ以上のトピックへの追加が起
こるケースにおいて, その追加語を追加した全てのトピックから削除している. 表 9よ
り,本実験において,その手法を用いたことが有効であったことが分かる.
20
表 2.7: EMアルゴリズムと共起語を考慮に入れた EMアルゴリズム の分類の違い
文書 繰り返し回数
0 5 10 15 20
144 ○ - - - -
145 - - - ○ ○
文書 繰り返し回数
0 5 10 15 20
96 - × - - -
97 - × × × ×
表 2.8: 共起語の追加作業が起きた件数
しきい値 追加総数
100% 79
80% 108
表 2.9: 多数トピックへの共起語の削除評価
Bayes+共起+EM 　しきい値＝ 50% 削除なし
EM(0) EM(5) EM(10) EM(15) EM(20)
正答数 54 46 49 74 74
正答率 32.14 27.38 29.17 44.05 44.05
Bayes+共起+EM しきい値＝ 50% 削除あり
EM(0) EM(5) EM(10) EM(15) EM(20)
正答数 67 46 51 76 76
正答率 39.88 27.38 30.36 45.24 45.24
21
2.6 結び
本稿では,共起語を考慮に入れた EMアルゴリズムによるテキスト分類の手法を提
案した。テストコーパスを用いた実験によって, EMアルゴリズムに対して,最も良
い精度を得た結果で, EM(0)で 11.9%, EM(5):2.38%, EM(10):2.38%, EM(15):16.66%,
EM(20)16.66%と精度が向上したことを確認した. また,追加語のしきい値を高くし,よ
り効果的なつながりを重視しすることで, しきい値を設けなかった場合,低く設定した
場合よりも高精度の結果を得た. 共起を考えるにあたり,相関の高い語を追加すること
が有効であり, 本稿によって,相関的な共起語を考慮に入れることにより,テキスト分類
の精度を向上させることができることを確認した.
22
第3章 単語分布からのトピック推定
3.1 前書き
過去一世紀以上にわたる論争の一つに著者推定問題がある。何らかの特徴を捉えて
著者の同定・区別を行おうとするもので, シェークスピア実在論争やグリコ森永事件の
脅迫状分析等がその応用例である [14]．著者推定・分析を行うためには,文体の計量的
特長 (stylometry),例えば語長・文長・語数や機能語 (while, on などの不要語記号)な
どを調べる方法があるが, 同一筆者でも差が大きく特徴が有効とはいいがたい [16]．
これと並んで興味あるものがトピック推定問題である．トピック (topic) とは興味あ
る事柄や出来事を言い, テキスト文書を解析し何のトピックを論じるものかを推定する
ことをトピック推定問題という．この技術は，文書の自動格納・自動分類や自動要約
に主要な手がかりを与え,また背景や領域の推定による文脈情報を付加することで，情
報検索の効率向上に大きなヒントを与えることができる．
これまでの研究結果から，テキストから直接有用な情報を抽出する方法では, 著者固
有の性質よりもトピックとの関連性を論じるほうが分析しやすいことが知られている
[16]．著者トピック語モデル (Author Topic Model) とは著者推定がトピック（テーマ）
の選定に確率分布に従うことをいう．これに対して，同一著者の下では,各トピックは
対応する語集合の多項式分布確率で表わされるとするトピック語モデル (Topic Word
Model)が議論されることが多い [20]．従って，トピック語モデルが正しければ,語の分
布を調べることでトピック推定が可能であり,具体的な推定手順を与える論拠となる．
一般に,文書は複数トピックを含むが, 本稿では 文書とトピックを同一視し,トピック
推定を効率よく実現する手法を検討する．
これまで情報検索では，文書中の語をベクトルで表わし，ベクトル類似の問題に帰
着するベクトル空間モデルが知られる [13]．モデル化が単純であり類似度も簡単に算
出できることから,広く利用されているが,解が重み付け方法に依存し,また数万に至る
高次元処理が必要であることから，性能および精度に差が生じる．このため精度を維
持したままで効率向上を目的とした次元縮小技法が知られている [13]．
本稿では，同一著者の下でトピック語モデルの検証を行い，次に次元縮小技法をト
ピック語モデルに適用し，トピック推定に有用であることを論じる．
第 2 章ではトピック語モデルと評価方法を述べる．第 3 章では次元縮小手法を要約
し,適用手法を論じ，さらに第 4 章で実験によりその有用性を示す．
23
3.2 トピック語モデル
トピック語モデルとは,同一著者の作品（トピック）には特有の語分布が対応し，各
語は多項式分布に従って確率的に選ばれるという特徴を仮定することをいう．これが正
しければ，一般には,トピック上の語分布を比較することでトピック推定が可能になる．
テキスト情報は語の並びとして構成されるが, 語 (word)をどのように設定するかは
自明ではない．英語では (空白などの)特殊文字で区切られた文字列を単語と呼ぶが,複
合語 (New York 等のように複数の単語からなる語)や共起性の強い語 (連語や慣用句)
等を考慮するかどうかは,分析結果に大きな影響を与える．n グラム (n-gram)モデル
では,連続する n 単語をまとめて語とみなすが,単語の区切りを無視して数え上げるた
め,多くのミスを含む可能性がある．反面,複合語や共起性問題を取り扱うことができ
る．日本語では形態素を基本とする．形態素 (Morphology)とは，これ以上に細かくす
ると意味を失う最小の文字列を言う．文章を形態素に分解する処理を形態素解析と呼
ぶ．形態素は単語に対応するが, 英語と同様に複合語・共起語の対応を考える必要が
ある．
本稿では，トピック語モデルを検証するため, 英文テキストに対して n グラムモデ
ル (n = 1, 2)を用いて語分布を調べる．テキスト文書に出現する単語を，予めステミン
グおよび不要語処理を行い自明情報を取り除いたあと,トピックの一部から抽出した教
師データと, 残りから抽出したテストデータの語を調べその出現頻度分布を比較する．
評価を行う際にカイ 2 乗検定のX2値を用い，教師データとテストデータの分布を
比較し独立性を調べる．このため教師データの語 wi の頻度を期待値 ai，テストデー
タの語 wi 頻度を観測値 biとして，以下の式を用いて評価する．
X2 =
∑n
i=1
(bi−ai)2
|ai| (3.1)
上式では，X2値が小さいほど分布が依存することをあらわしている．本稿ではデータ
量の不均等を考慮しX2値の下位 3つを本実験での正答とする．ここで正答率は，検定
文書中の適合する数を p，文書総数 (総場面数)を q とするとき次式で定義される．
p
q
(3.2)
3.3 情報検索とデータ次元縮小
文書に含まれるテキスト情報を検索するには，出現する各語の (出現頻度等)特徴を
値としてベクトル化するベクトル空間モデルが一般的である [13]．一般にテキスト文
書 d は,出現する語 w1, .., wn のベクトルで表現する:
d = (v1, ..., vn)
ここで vi は語 wi に対応する数値であり一般に出現頻度 (Term Frequency) であるこ
とが多い．このとき 2 つの文書 d1, d2 の類似度は出現数の分布を用いて定義され,こ
れは内積 (d1, d2) によって算出できる．
24
この方法は，モデル化が単純であり類似度も簡単に算出できることから, 広く利用
されているが,解が重み付け方法に依存し, 次元数が数万にも及ぶ高次元データをその
まま扱うと，効率,計算機容量の確保および即応性への対応が困難になる．このため，
テキスト情報の次元を縮小し改善を図る次元縮小技法が知られている [13]．次元縮小
技法では高次元文書ベクトルを低次元空間に射影し,この部分だけを検索対象とするた
め，効率よく探索範囲を絞り込むことができる．
次元縮小技法のうち,潜在意味索引つけ (Latent Semantic Indexing) は，源データを
用いて特徴値を算出するためきわめて高精度に縮小可能であるが，特徴値算出手続き
の効率が悪くまた微小な変更でも再計算を要求することから動的な環境に利用できな
い．一方, ランダムプロジェクション (Random Projection, RP)技法は，乱数技法に
より次元縮小するため,次元縮小手続きの効率が良く，低次元空間に縮小するほど少な
くて済む利点がある．またテキスト文書集合が増えても再計算を要求することがない．
反面,精度が悪く適用範囲に限界がある．
本稿では，RP技法を用いて次元縮小を行う．以下では語数d，文書数Nとし，X×N
語・文書行列Xを k × N (k ≪ d)の語・文書行列XRP に射影する．射影を行うため，
k × dのRP行列R = ((rij))を生成する．この際，行列Xの i行 j列の要素Xijは，文
書 jにおける語 iの頻度を意味する．単語・文書行列X のRPによる次元縮小の計算
は以下のように定義される1:
XRPk×N = Rk×dXd×N (3.3)
また，第 i 行ごとに，RP行列の要素 rij, j = 1, ... は，発生確率 p に対して，次の
分布に従うように決定する [12]．
rij =
√
3 ·

+1 (p = 1/6)
0 (p = 2/3)
−1 (p = 1/6)
(3.4)
この分布を取る行列の生成に対する計算量は O(kd)であり，(k ≪ d)でもあることか
ら，実際の処理時間は非常に少ない．
d×N 行列Xから任意の列ベクトルを取り出し，予め用意した訓練用データを用い
て RP 行列を作成しテストデータを比較・評価する．
本研究では，RPを用いた検索では縮小率に伴う正答率の低下を評価する．
3.4 実験
3.4.1 準備
シェークスピアによる戯曲 10作品 (英語テキスト)をテストコーパスとして使用し，
作品をトピックと考える [15]．各タイトルはいずれも 5章（chapter）から構成され，各
章は場（scene）からなる．実験で用いる作品タイトルは次である．
1データの初期生成に対する計算量は O(dkN)となる．
25
タイトル 構成
夏の夜の夢 全 5章 9場
お気に召すまま　 全 5章 22場
シンベリン 全 5章 26場
ハムレット 全 5章 20場
オセロー 全 5章 15場
ジュリアス・シーザー 全 5章 18場
ジョン王 全 5章 16場
リチャード二世 全 5章 19場
ヘンリー八世 全 5章 17場
テンペスト 全 5章 9場
教師データとして各トピックの第 1章の全ての場から単語を抽出し，また 2章以降
の全 138場から単語を抽出してテストデータとして使用する．テストデータとしての
場は，そのタイトルを隠して推定し，上位 3 トピックに正解が含まれているとき正し
く推定されたと判断する．各トピックの構成と，不要語を除去した後の各場の単語数
を表 2 に示す．
本実験では 1 グラム (実験 1), 2 グラム (実験 3) によるトピック語モデルの検証と，
RP 技法による次元縮小の精度 (実験 2)を調べる．特に，次元縮小の精度調査 (実験 2)
では縮小率に伴う正答率の低下を評価する．RP 技法では行列はランダムに生成され
るため，10回繰り返した正答を平均した値を本実験での正答率とする．
3.4.2 実験結果
表 3.1に 実験 1 のおける推定の正答率を示し，表 3.2に (実験 1 の)各教師データの
単語総数を示す．表 3.3に各文書における本来の正答トピックと推定結果を○と×で示
す．場の番号 1から 33までが 2章，34から 71が 3章，72から 100が 4章，101から
138が 5章の場を意味する．次元縮小による精度低下を評価するため，縮小した各次元
における正答率と縮小前の正答率からの低下割合を表 3.4に示す．同様に 2 グラム分
布を用いる実験 3 におけるトピック教師データの総語数を表 3.6に示し，その正答率
を表 3.8に示す．
表 3.1: 1グラムモデルでの正答率
トピック C1 C2 C3 C4 C5
正答率 100 100 40 33.3 75
トピック C6 C7 C8 C9 C10 合計
正答率 86.7 100 46.7 76.9 100 72.46
26
表 3.2: 1 グラムモデルの教師データトピック総語数
トピック C1 C2 C3 C4 C5
単語数 654 783 1171 1327 1132
トピック C6 C7 C8 C9 C10
単語数 849 503 1120 1138 1067
3.4.3 考察（実験1）
表 3.1と表 3.2より，教師データの大きさによる推定結果の違いはあるが，正答率が
72.46% であることから，本実験で用いるデータがトピック語モデルに従うといってよ
い．表 3.1においてトピックC3，C4の推定が低精度となっているが，表 3.7の各トピッ
ク間の出現語の違いより，他トピックに対して偏った語の出現でないことが確認でき，
特殊な分布ではないことが確認できる．この 2つのトピックの推定が低精度となる要
因として，(1) 第 1章からの教師データ作成が困難であったこと，(2) 表 3.2と表 3.7よ
り，他トピックに共通する語の出現が多いこと, (3) 語総数自体が多いことによる．こ
のため，推定が語総数の低いトピックへと割り当てられ精度が低下する要因となる．
3.4.4 考察（実験2）
表 3.4により，140次元への縮小（縮小率 98.59%）で精度低下が 19.30%程度である．
表 3.5では,判定が 10回中過半数で変化が生じるとき「判定変化あり」とする2．表 3.5
より，変化なしの割合は高く，次元の縮小によるトピック語モデルの信頼性は変化が
ない．
3.4.5 考察（実験3）
表 3.9は，本実験で数多く推定された先のトピックとその推定数を示す．表 3.9より，
トピックC1，C6，C7に推定されるテストデータが多い．しかし,表 3.6より，いずれ
も教師データの総語数が低いトピックである．また，表 3.10は 1 グラムモデルと 2 グ
ラムモデルの次元と，1度だけ出現する語の頻度を示している．表 3.10より，1 グラム
モデルでは各トピックに有効で出現が考えられる語は 56.12%なのに対して，2 グラム
モデルでは 8.47%である．他トピックに出現しない語は，そのままデータ量の違いの
影響を受けやすく，偏りが生じやすい．従って 2 グラムモデル分布によって，トピッ
ク語モデルが検証できたとはいい難い．
2変化の度合いが 5回ずつであれば判定不能とする．
27
表 3.3: 1 グラムモデルでの推定
場 正解トピック 推定不可 場 正解トピック 推定不可
1 1 ○ 70 10 ○
2 1 ○ 71 10 ○
3 2 ○ 72 1 ○
4 2 ○ 73 1 ○
5 2 ○ 74 2 ○
6 2 ○ 75 2 ○
7 2 ○ 76 2 ○
8 2 ○ 77 3 ×
9 2 ○ 78 3 ○
10 3 ○ 79 3 ×
11 3 ○ 80 3 ×
12 3 ○ 81 4 ×
13 3 × 82 4 ×
14 4 × 83 4 ×
15 4 ○ 84 4 ×
16 5 ○ 85 4 ×
17 5 × 86 4 ×
18 5 ○ 87 4 ×
19 6 ○ 88 5 ○
20 6 ○ 89 5 ○
21 6 ○ 90 5 ○
22 6 ○ 91 6 ×
23 7 ○ 92 6 ○
24 8 ○ 93 6 ○
25 8 ○ 94 7 ○
26 8 ○ 95 7 ○
27 8 × 96 7 ○
28 9 ○ 97 8 ○
29 9 ○ 98 9 ○
30 9 ○ 99 9 ○
31 9 ○ 100 10 ○
32 10 ○ 101 1 ○
33 10 ○ 102 2 ○
34 1 ○ 103 2 ○
35 1 ○ 104 2 ○
36 2 ○ 105 2 ○
37 2 ○ 106 3 ×
38 2 ○ 107 3 ×
39 2 ○ 108 3 ×
40 2 ○ 109 3 ○
41 3 × 110 3 ○
42 3 × 111 4 ○
43 3 × 112 4 ○
44 3 ○ 113 5 ○
45 3 ○ 114 5 ○
46 3 × 115 6 ○
47 3 × 116 6 ×
48 4 × 117 6 ○
49 4 ○ 118 6 ○
50 4 × 119 6 ○
51 4 ○ 120 7 ○
52 5 × 121 7 ○
53 5 × 122 7 ○
54 5 ○ 123 7 ○
55 5 ○ 124 7 ○
56 6 ○ 125 7 ○
57 6 ○ 126 7 ○
58 6 ○ 127 8 ×
59 7 ○ 128 8 ○
60 7 ○ 129 8 ○
61 7 ○ 130 8 ×
62 7 ○ 131 8 ×
63 8 × 132 8 ×
64 8 × 133 9 ○
65 8 ○ 134 9 ×
66 8 × 135 9 ○
67 9 ○ 136 9 ×
68 9 ○ 137 9 ×
69 10 ○ 138 10 ○
28
表 3.4: 次元縮小における精度の変化
次元数 正答数 正答率 精度低下
9923 100.00 72.46 0.0 %
9000 98.10 71.09 1.89
5000 99.70 72.25 0.29
3000 99.50 72.10 0.50
2000 99.80 72.32 0.19
500 92.60 67.10 7.40
400 91.50 66.30 8.50
300 90.70 65.72 9.30
200 83.70 60.65 16.30
190 82.10 59.49 17.90
180 81.60 59.13 18.40
170 79.80 57.83 20.19
150 81.00 58.70 18.99
140 80.70 58.48 19.29
130 78.90 57.17 21.10
100 77.70 56.30 22.30
表 3.5: 次元縮小前後の判定変化（140次元)
トピック ×→○ ○→× 判定変化無し 判断付かず 総文書数
C1 0 0 100 0 7
C2 0 35.29 64.71 11.76 17
C3 9.09 0 90.9 36.36 11
C4 18.75 0 81.25 25 16
C5 0 0 100 50 8
C6 0 23.08 76.92 15.38 13
C7 0 28.57 71.43 7.14 14
C8 38.46 0 61.54 15.38 13
C9 16.67 33.33 50 8.33 12
C10 0 85.71 14.29 0 7
表 3.6: 2 グラムモデルでの各教師データトピックの総語数
トピック C1 C2 C3 C4 C5
単語数 1029 1481 1991 2075 1672
トピック C6 C7 C8 C9 C10
単語数 1255 710 1915 1592 1765
表 3.7: 各トピック教師データ間の共通語の出現
語数 C2 C3 C4 C5 C6 C7 C8 C9 C10 平均
C1 221 267 266 249 206 142 246 245 228 230
C2 - 352 322 320 263 186 309 321 280 294
C3 - - 439 427 326 231 380 431 392 375
C4 - - - 447 354 213 407 407 381 368
C5 - - - - 289 214 349 386 352 318
C6 - - - - - 178 303 301 312 274
C7 - - - - - - 219 209 193 207
C8 - - - - - - - 355 348 352
C9 - - - - - - - - 358 358
29
表 3.8: 2 グラムモデルでの正答率
C1 C2 C3 C4 C5 C6 C7 C8 C9 C10
正答数 7 3 3 9 1 15 15 8 7 1
文書数 7 19 20 15 12 15 15 15 13 7
正答率 100 15.8 15 60 8.3 100 100 53.3 53.9 14.3
表 3.9: 2 グラムモデルでC1,C6,C7に推定された場数
推定トピック 下位 1 位 下位 2 位 下位 3 位
C1 1 92 39
C6 3 9 96
C7 111 27 0
3.5 結び
本稿では，単語分布からのトピック語モデルの検証,および次元縮小による精度変化
を実験により解析し,利用可能性を論じた．実験によって，1 グラムモデルでは単語分
布によるトピック推定が平均 72% 以上の精度で可能であることを示し，さらに次元縮
小率 98.59%（9923次元から 140次元）であっても信頼性 8 割程度の正答率を維持でき
ることを示した．一方，2 グラムモデルでは，正答率が 50% 程度であり，トピック語
モデルの検証ができなかった．この結果,次元縮小によるトピック語モデルは 1 グラム
モデルで有効であることを確認した．
表 3.10: 次元の違い
次元数 １度のみ出現 有効割合
2 グラム 65166 59648 8.47%
1 グラム 9923 4354 56.12%
30
第4章 Topic/Author 推定方式の改善
4.1 前書き
近年,インターネットが世界中に普及したことにより, 膨大な量の情報を用意に得る
ことができるようになった. これらの情報は一般的にテキスト形式で保持されている.
これは目的に合った場所に文書として格納すべきであるが, クラスや類似性により前
もって分けられていないため, 分類とクラスタリングのような機械学習手法を適用する
ことは難しい.
実際に,“この文書は最新の提案だ”あるいは“この文書はスミス氏の手紙のようだ”
ということがある. このような分類問題は情報検索 (IR)により解決する事ができる.
IRでは,すべての文書が単語に関するベクトルとして表わされる, ベクトル空間モデル
(VSM)によって対象世界を言及する.ここで, 2つの文書の類似性を,コサイン尺度と呼
ばれる 2つのベクトルの内積として定義する [13]. しかし,類似性は,共起語の発生に依
存する.
しかし,IRではテキスト形式がどう見えるか, また誰が文書を作成したかを論じるべ
きで,効率に関する点だけに焦点を特化すべきでない. そのような IRは構文アプロー
チに基づくはずがなく, むしろ自然言語処理 (NLP)の支援が必要である.
代表例として,トピックと著者の識別がある. 著者推定問題は,著者 (あるテキストの
著者)を識別する方法を意味し, 過去一世紀以上にわたる論争の一つでもあり, テキス
トから何らかの特徴を捉えて著者の同定・識別を行おうとするものである. これと並
んで興味あるものがトピック推定問題である. トピック (topic) とは興味ある事柄や出
来事を言い, 文書を解析し何のトピックを論じるものかを推定する. これらの技術は,
文書の自動格納・自動分類や自動要約に主要な手がかりを与え, また背景や領域の推定
による文脈情報を付加することで, 情報検索の効率向上に大きなヒントを与えることが
できる．
これまでの研究結果によると, テキストから直接有用な情報を抽出する方法では, 著
者固有の性質よりもトピックとの関連性を論じるほうが分析しやすいことが知られて
いる [16]. トピック/著者語 (T/AW)モデルとは著者推定がトピックの選定に確率分布
に従うことをいう．一方,同一著者の下では, 各トピックは対応する語集合の多項式分
布確率で表わされるとする著者語 (A/W)モデルが知られる [20]. 仮に,T/AWモデルが
正しければ, 語の分布を調べることでトピック推定が可能であり, 具体的な推定手順を
与える論拠となる.
しかし,テキスト形式の情報検索では高次元の処理を必要とすることから,性能およ
31
び精度に差が生じる. このため精度を維持したままで効率向上を目的とした次元縮小
技法がいくつか提案されている [13]．
本研究では,T/AWモデルおよび AWモデルが実際に成り立つかどうかを検証する.
また,次元縮小技法を用いることによって,効率的に処理可能であることを示す.
第 2章はトピック・著者の語モデルおよびそれらを評価する方法を述べる. 第 3章で
は次元縮小,およびトピック・モデル検証にどのように技術を適用するかを述べる. 第
4章では実験によりその有用性を示し,第 5章では関連研究を述べ, 第 6章で本研究の
結論を述べる.
4.2 トピック,著者,単語のモデル化
T/AWモデルとは, 同一著者の下では, 各トピックは対応する語集合の多項式分布確
率で表わされるという仮定である. つまり,著者は,トピックに依存する多項式分布確
率により語を確率的に選択する仮定されている. これはトピック,語の分布の検証によ
り, 文書がどのトピックを表しているかを推定することが可能であることを意味する.
この状況は機械学習の分類に似ている. 即ち,訓練データを事前に準備し, どのクラス
が最も適しているかを検証することに相当する. 他方,AWモデルは, 著者がトピックに
独立に著者自身の語集合の多項式分布確率で表されるという仮定である. つまり,著者
は多項式分布に従って語を確率的に選択する.
残念ながら,この特性を証明することはできない. 一般的に,T/AWモデルは広く信
じられているが,AWモデルにおいてはそうではない. 本研究では,信じられる・信じら
れないに関わらず,検証を行う.
次にこれらの問題を検証する方法を述べる. 根本的な問題の 1つは語をどう扱うかで
ある. テキスト情報は語の並びとして構成されるが, 語 (word) をどのように設定する
かは自明ではない．英語では (空白などの) 特殊文字で区切られた文字列を単語と呼ぶ
が, 複合語 (”U.S.A.” 等のように複数の単語からなる語) や慣用句 (”get used to” 等),
連語 (”not only… but also” 等)を考慮するかどうかは, 分析結果に大きな影響を与え
る．nグラム (n-gram) モデルでは, 連続する n 単語をまとめて語とみなすが, 単語の
区切りを無視して数え上げるため, 多くのミスを含む可能性がある [21]．本研究では,1
グラムモデル (n = 1)を用いて検証を行う.
同様に考慮すべき点として内容語の問題がある. 効果的に処理を行うには,特有の意
味を持つ語を選ぶ必要があろう. T/AWモデルと AWモデルでは, これらの単語の分
布がテキスト文書の意味的な様相を捉えるので, 本研究では内容語だけを分析する. こ
こでは,前もって不要語 (”a”, ”the” 等)を除去し,ステミング処理も行う. ここでは機
能語 (”when”, ”and”等)に関しては考えない. 重みには語の出現頻度 (TF)や逆出現頻
度 (IDF) 1 を用いて検証を行う.
1IDFは nを文書数,ni を語 wi を含む文書数としたときの, nni の割合をいう.
32
与えられた 2つの分布 (既知のトピック分布と別の 2つの分布)をテストすることで,
実際に 2つの分布が独立であるかどうかを調べたい. 検証する方法として本研究では 2
つの手法を用いる. 1つ目は,カイ二乗値を用いる方法である. 2つの分布 p,qがどのぐ
らい独立しているかを調べるため, 各語wiについて,訓練データとテストデータ 2つの
頻度分布 pi,qiからなる. X
2値を以下に定義する.
　
　　X2 =
∑
i
(qi−pi)2
pi
　　　
(4.1)
2つの分布が類似するほど,定義より算出されるX2値は少なくなる. テスト文書を
与えられたとき,語分布を計算し各訓練データに対するX2値を計算し, X2値が小さく
なったトピックを当該のものと推定する. 2つ目はKLダイバージェンスKL(p||q)を
用いる方法である. 2つの分布 pと qを用いることで,pから qを判別することができる.
定義より,値が小さいときは 2つの分布が類似していることを意味する.
　 　　KL(p||q) = ∑i pi log piqi　　　　 (4.2)
本研究では推定結果の上位に正解が含まれる場合, 正しいトピックに推定ができたと
考える. 実験では上位 1位または 3位までを扱う.
4.3 T/AWモデルと次元縮小
テキスト形式の情報検索では高次元の処理を必要とすることから,多大な計算量を要
する. これまで,精度を維持したままで効率向上を目的とした次元縮小技法がいくつか
提案されている [13].
情報検索アプローチでは, 文書 dに出現する内容語w1, .., wn のベクトルで表現する.
　　　　 d = (v1, ..., vn)
ここで viは語 wiに対応する数値であり一般に出現頻度であることが多い．2つの文
書 d1,d2が類似している場合, d1, d2 の類似度は出現数の分布を用いて定義され, 内積
(d1, d2)によって与える. この方法は,モデル化が単純であり類似度も簡単に算出でき
ることから, 広く利用されているが, 解が重み付け方法に依存し,次元数が数万にも及
ぶ高次元データをそのまま扱うと, 効率,計算機容量の確保および即応性への対応が困
難になる. 次元縮小技術を必要とするような多大な負荷 (CPUとメモリ)がかかるため,
次元縮小技法を用いることで,高次元文書ベクトルを低次元空間に射影し, 効率よく探
索範囲を絞り込むことができる.
次元縮小技法はこれまでも多数提案されてきた [13]. 主な技術として,潜在意味索
引つけ (Latent Semantic Indexing, LSI)技法と, ランダムプロジェクション (Random
Projection, RP) 技法がある. LSIは原データを用いて線形代数の理論を基にしてた特
徴値を算出するため,極めて高精度に縮小可能である. しかし,特異値分解 (SVD)の理
33
論計算に多大な時間がかかり, 微小な変更でも再計算を要求することから動的な環境に
利用できない.
これに対して,RPは乱数技法により次元縮小するため, 次元縮小手続きの効率が良
く,低次元空間に縮小するほど少なくて済む利点がある. さらに,技法は文書に依存せ
ず確率を用いるため, 動的な環境の下でテキスト文書集合が増えても再計算を要求する
ことがない. このため,本研究ではRPを用いて次元縮小を行う. 反面,精度が悪く適用
範囲に限界がある [19].
以下では語数 d,文書数N とし, X ×N 語・文書行列Xを k ×N(k≪ d)の語・文書
行列XRP に射影する. 射影を行うため,k × dのRP行列R = ((rij))を生成する. この
際,行列Xの i行 j列の要素Xijは, 文書 jにおける語 iの頻度を意味する. 単語・文書
行列XのRPによる次元縮小の計算は以下のように定義される.
　　XRPk×N = Rk×dXd×N (4.3)
これを定義するため,次元縮小行列R = ((rij))を, 発生確率 p に対して,次の分布に従
うように決定する [12]．(i = 1...k, j = 1...d)
　　 rij =
√
3 ·

+1 (p = 1/6)
0 (p = 2/3)
−1 (p = 1/6)
(4.4)
単語数 d,文書数N を k次元に縮小する際の行列の生成に対する計算量は O(kd)であ
り, k ≪ dでもあることから,実際の処理は高速である．
d×N行列Xから各列ベクトルを取り出し, 予め用意した訓練用データを用いてRP
行列を作成しテストデータを比較・評価する. 本研究では,RPを用いた検索では縮小率
に伴う正答率の低下を評価する.
4.4 実験
この章では実験により, TA/WモデルおよびAWモデルが実際に成り立つかどうか
を検証する. また,RP技法による次元縮小の効果を調べる.
4.4.1 T/AW モデルの検証
最初の実験ではT/AWモデルが成り立つかどうかを検証する. グーテンベルク・プロ
ジェクト [18]から 3人の著者 (Charles Dickens,George Alfred Henry and Robert Louis
Stevenson,1人当たり 10作品)を選び, 合計 30作品 (トピック)を選ぶ. さらに各トピッ
クをユニットの集合に分割する. 各ユニットはそれぞれ 20の段落から成り,1つの文書
として考える.
34
訓練データのユニット数が 1,2,3,4,5のそれぞれの場合を考え, テストデータは各ト
ピックあたり 10ユニットを扱い実験を行う. 表 4.1はデータを訓練する際の異語数を
示している. 例えば,C.Dickensの作品C1の 4ユニットでは別の 1404語が出現する.
著者 トピック C1,..,C10
Charles Dickens Barnaby Rudge, Bleak House, Little Dorrit,
Master Humphrey’s Clock, Mudfog and Other Sketches,
Reprinted Pieces The Chimes,
The Haunted Man and the Ghost’s Bargain,
The Old Curiosity Shop, The Uncommercial Traveller
George Alfred Henry A Knight of the White Cross, Among Malay Pirates,
At Agincourt, Beric the Briton,
Forest and Frontiers, In Freedom’s Cause,
In the Reign of Terror, One of the 28th,
The Bravest of the Brave, The Lion of the North
Robert Louis Stevenson A n Inland Voyage, David Balfour (Second Part),
Island Nights’ Entertainments, Kidnapped,
Master of Ballantrae, Memoir of Fleeming Jenkin,
Merry Men, New Arabian Nights, Prince Otto (a Romance),
The Black Arrow
訓練データ数 c1 c2 c3 c4 c5 c6 c7 c8 c9 c10
C.Dickens
5 1595 2093 1565 2464 1908 2365 1411 1452 1156 2298
4 1404 1783 1243 1922 1464 1910 1194 1295 1003 1853
3 1198 1542 1076 1430 1241 1806 1028 1045 870 1332
2 1042 963 892 976 870 1099 876 768 606 1102
1 519 696 581 543 516 585 670 491 335 598
G.A.Henry
5 1954 1506 1718 2083 2690 2216 1382 1284 1988 1554
4 1487 1060 1125 1902 2149 1709 1037 1066 1684 1008
3 1038 738 482 1515 1552 1126 817 818 1177 620
2 612 455 203 830 1070 828 521 342 639 378
1 286 269 116 353 537 436 156 154 291 218
R.L.Stevenson
5 2799 1082 1604 1576 1775 4698 1672 1290 1452 1228
4 2263 896 1367 1419 1252 3353 1281 1081 1157 926
3 1719 784 968 1072 813 2398 960 808 993 693
2 1133 634 641 855 497 1946 455 646 771 499
1 585 196 407 462 253 774 69 279 473 292
表 4.1: 異語数
訓練データ数 Dickens Henry Stevenson
5 51 76 71
4 51 73 61
3 58 70 56
2 58 79 63
1 58 75 53
表 4.2: Best3の正解率 (%)
TF,TF∗IDFを重みとして,X2値,KLダイバージェンスを用いて分布を検証する. 表
4.2,表4.3に上位1位が正解となる場合 (Best1)と上位3位までに正解を含む場合 (Best3)
を示す.
4ユニットの訓練データにおいて, Best3の正解率はC.Dickensが 51%,G.A.Henryが
73%,R.L.Stevensonが 61%である. 見て分かるとおり,訓練データが多いほど正解率は
高くなっている. Best1の場合では,5ユニットの正解率が最も高い.
TFの代わりにTF∗IDF(+X2値)を扱うことで,正解率の向上を得た. しかし,KLダ
イバージェンスを用いたケースでは正解率の低下が見られた. これは,各作品 (トピッ
35
訓練データ数 Dickens Henry Stevenson
5 35 64 48
4 39 55 43
3 44 53 44
2 40 61 47
1 28 63 35
表 4.3: Best1の正解率 (%)
手法 Dickens Henry Stevenson
(Best1)
TF*IDF + X2 23 72 49
TF + X2 39 55 43
TF*IDF+ KL 19 42 19
TF + KL 10 24 19
(Best3)
TF*IDF + X2 52 79 65
TF + X2 51 73 61
TF*IDF + KL 33 40 39
TF + KL 32 49 30
表 4.4: 4ユニットにおける正解率
ク)が独自の分布を持っており, KLダイバージェンスと比べると IDFがより効果的で
あることが分かる.
このことより,T/AWモデルが正しく成り立っているということができる. 実際,正
解率はC.Dickensが 58%,G.A.Henryが 79%,R.L.Stevensonが 71%(Best3＋X2)となっ
た. TF∗IDFを用いることにより,正解率を改善し, より多くのデータにおいて精度の
向上が見られた.
4.4.2 AW モデルの検証
第 2の実験ではAWモデルが実際に成り立つかどうかを検証する. 実験データは実験
1と同じコーパスを用いる. テストのため,各トピックを再びユニットに分割する. 各
ユニットは 20の段落から成る. 今回の実験では訓練データは存在しない. コーパスを
通して得ることができる各著者の作品に表れた語分布を用いて実験を行う. ここでは,
著者推定と著者の語分布の 2つの特性を調べる.
表 4.5にコーパスの語分布を示す. 明らかに,分布の中では多くの単語が共通して出
現しており, 著者の間で明確に区別する事ができない (50%～70%).
初めに著者間の分布がどれぐらい類似しているかを検証する. 前述のとおり,著者の
すべての作品の単語頻度を数えることによって (著者ごとに)すべての語分布を取る. さ
らにすべての著者に対して, 著者のすべての作品からそれぞれ 5つのテストユニットの
分布, つまり 150の (=3 × 10 × 6)の分布を取る. その後,各著者の分布と比較してX2
値を計算し,χ2検定を適用する. 5つのテスト・ユニットに関しては著者を識別するた
めに平均したX2値を用いる. この結果を表 4.6に示す.
36
C.Dickens G.A.Henry R.L.Stevenson
C.Dickens 16895 8699 10014
G.A.Henry - 12548 8169
R.L.Stevenson - - 15764
表 4.5: 著者間の共通語数
C.Dickens G.A.Henry R.L.Stevenson
総単語数 661207 407558 316443
異語数 16895 12548 15764
χ2 (99.5%) 13758.0 10215.2 12836.3
χ2 (0.5%) 20378.2 15140.0 19015.3
(著者の分布との比較)
By C.Dickens 548728 770757 590290
By G.A.Henry 583830 735636 576884
By R.L.Stevenson 609703 703688 559996
(5 ユニットの平均)
By C.Dickens 647934 398542 305655
By G.A.Henry 648115 397556 305921
By R.L.Stevenson 6648002 398592 304928
表 4.6: χ2検定のX2値
結果より,語分布が著者自体に依存する訳ではないことが分かる. X2値はすべて χ2
値を越えており,分布が著者を推定するとは断定できないということができる. 信頼性
0.5%の場合でさえも充足するX2値は存在しない. 最小となるX2値はR.L.Stevenson
以外では間違った著者に現れる. さらに悪いことには,すべての場合において 5つのユ
ニットを平均したものが非常に悪くなってしまう.
第 2の問題はテストデータの著者をどのぐらい識別する事ができるのかという問題
である. 実験の目的は異なるが,表 6を用いてX2値を計算する. また著者を識別する
際にはBest1評価方法を適用する. 結果を表 4.7 に示す. 作品全体および 5ユニットの
平均における正解率は 63.3%および 33.3%である. これを見ると結果が良いようにも見
えるが,ほとんどがR.L.Stevensonに推定されている. 実際 5ユニットの場合はすべて
R.L.Stevensonに推定されている.
C.Dickens G.A.Henry R.L.Stevenson Total
(著者の分布との比較)
C.Dickens 4 0 6 40%
G.A.Henry 0 5 5 50%
R.L.Stevenson 0 0 10 100%
(Average) 63.3%
(5 ユニット)
C.Dickens 0 0 10 0%
G.A.Henry 0 0 10 0%
R.L.Stevenson 0 0 10 100%
(Average) 33.3%
表 4.7: 著者推定の正解率
表 4.8 からは,作品全体の場合に関して異語が出現していることが分かる. 同時に
37
異語数 総単語数 推定先の著者
(C.Dickens)
C1 7765 112758 C.Dickens*
C2 9059 141905 C.Dickens*
C3 8997 139133 C.Dickens*
C4 4073 22061 R.L.Stevenson
C5 3588 12905 R.L.Stevenson
C6 6703 42035 R.L.Stevenson
C7 2910 13025 R.L.Stevenson
C8 3009 13096 R.L.Stevenson
C9 7386 97455 C.Dickens*
C10 8555 67232 R.L.Stevenson
(G.A.Henry)
C1 4786 53533 G.A.Henry*
C2 3589 25899 R.L.Stevenson
C3 4389 45303 G.A.Henry*
C4 4728 55643 G.A.Henry*
C5 3586 14904 R.L.Stevenson
C6 4795 48147 R.L.Stevenson
C7 3891 32714 R.L.Stevenson
C8 4388 47130 G.A.Henry*
C9 4282 37170 R.L.Stevenson
C10 4736 47445 G.A.Henry*
(R.L.Stevenson)
C1 4063 16170 R.L.Stevenson*
C2 5298 36317 R.L.Stevenson*
C3 3025 20663 R.L.Stevenson*
C4 4506 30502 R.L.Stevenson*
C5 4970 40326 R.L.Stevenson*
C6 5509 25577 R.L.Stevenson*
C7 6000 40295 R.L.Stevenson*
C8 5903 42263 R.L.Stevenson*
C9 4833 30187 R.L.Stevenson*
C10 4822 34831 R.L.Stevenson*
表 4.8: 著者推定の際の異語数と総単語数
C.DickensとG.A.Henryにおいて, 4つあるいは 5つが正確に推定されていることもわ
かる. これは異語数が推定に影響したことを意味する. 比較的多い異語がR.L.Stevenson
の分布との違いを表し, X2値を小さくしたと考えられる. これは特定の著者に対して
偏った分布を持つことも意味している.
著者推定に関しては正解率 60%を超えたが,これは全体の単語量が推定に影響をした
ことによるもので, 必ずしも分布の違いから得られた結果ではない. 従ってAWモデル
に関して,それを信頼するだけの理由と考えることはできない.
4.4.3 次元縮小
最後の実験はRPの適用精度に関するものである. 実験データとして,W.Shakespeare
の作品より 10作品を選ぶ [15]. 各作品をトピックとし,各トピックでの全ての第 1章を
訓練データとして使用する. データはすべてステミング処理と不要語除去した後,10ト
ピックの第 1章の語分布を調べる. その後,他の章の全 139場面に関してもステミング
処理と不要語除去を行い, 語分布を抽出する. この抽出した分布を訓練データから得ら
38
れた分布と比較することで,場面がどのトピックに属するのかを識別する. 表 4.9 に前
処理を行った後の訓練データの単語数を示す.
トピック 作品名 章/場面
C1 A Midsummer Night’s Dream 5/9
C2 As You Like It 5/22
C3 Cymbeline 5/26
C4 Hamlet 5/20
C5 Othello 5/15
C6 Julius Caesar 5 /18
C7 King John 5/16
C8 Richard II 5/19
C9 Henry VIII 5/17
C10 The Tempest 5/9
C1 C2 C3 C4 C5 C6 C7 C8 C9 C10
654 783 1171 1327 1132 849 503 1120 1138 1067
表 4.9: 訓練データの異語数
C1 C2 C3 C4 C5 C6 C7 C8 C9 C10
100.0 100.0 40.0 33.3 75.0 86.7 100.0 46.7 76.9 100.0
表 4.10: 正解率
最初にT/Wモデルが成り立つかどうかを再度検証する. 表 4.10 に実験結果を示す. 実
験では 72.46%の正解率を得た (Best3)ためT/Wモデルが成立していると言ってよい.
RP手法による結果は任意に生成された RP行列に依存する. 本稿においては 10回の
実験の平均値を評価する.表 4.11 に結果を示す. 表は縮小された次元 (次元)と正解率
(正確さ)の関係を示す. ”精度低下”は次元を縮小したことによる正解率の低下割合を
意味する.
表より,C3と C4で低い正解率となっていることが分かり,C3と C4には多くの単語
が出現している. また表 4.12は 2つのトピックのデータ間の共通単語を示している. 表
4.12 より,C3も C4も他と比べて明確な違いは存在しないことが分かる. しかし,僅か
ではあるが他と比べるとこの 2つのトピック間においては共通の単語が多い. これは
C8にも共通する.
表 4.11 より,500次元で 67%の正解率,140次元 (98.59%の縮小率)では正解率の低下
が 19.30%となっていることが分かる. これらはRP手法を用いた次元縮小が非常によ
く働いていることを意味する. 表 4.13 は 140次元に次元を縮小した場合にトピックの
識別判断がどう変化したかを示している. 表の「NO/YES」は不正解だったものが 140
次元に縮小する事で正解と判断されたことを意味し, 「Yes/NO」は正解だったものが
不正解と判断されたことを意味する. 「NoChange」は識別されたトピックの変更はな
かったことを意味し, 「UnKnown」は判断できない (Yesと Noが 50%ずつ)ことを意
味する. 識別が変更されたトピックはほとんどないことから, 次元を縮小することで
T/Wモデルの充足性は変わらない.
39
次元数 正解率 精度低下
9923 72.46 0.0 %
9000 71.09 1.89
5000 72.25 0.29
3000 72.10 0.50
2000 72.32 0.19
500 67.10 7.40
400 66.30 8.50
300 65.72 9.30
200 60.65 16.30
190 59.49 17.90
180 59.13 18.40
170 57.83 20.19
150 58.70 18.99
140 58.48 19.29
130 57.17 21.10
100 56.30 22.30
表 4.11: 次元縮小と正解率
C2 C3 C4 C5 C6 C7 C8 C9 C10
C1 221 267 266 249 206 142 246 245 228
C2 - 352 322 320 263 186 309 321 280
C3 - - 439 427 326 231 380 431 392
C4 - - - 447 354 213 407 407 381
C5 - - - - 289 214 349 386 352
C6 - - - - - 178 303 301 312
C7 - - - - - - 219 209 193
C8 - - - - - - - 355 348
C9 - - - - - - - - 358
表 4.12: 各トピック訓練データ間の共通語数
トピック NO/YES YES/NO NoChange Unknown
C1 0 0 100 0
C2 0 35.29 64.71 11.76
C3 9.09 0 90.9 36.36
C4 18.75 0 81.25 25
C5 0 0 100 50
C6 0 23.08 76.92 15.38
C7 0 28.57 71.43 7.14
C8 38.46 0 61.54 15.38
C9 16.67 33.33 50 8.33
C10 0 85.71 14.29 0
表 4.13: 次元縮小前後の判定変化 (140次元)
40
4.5 関連研究
原作者の問題とは, テキストや他の特徴をを調べることによってどのように著者を識
別するかを意味する. 応用例として,シェークスピアが実際に生きていたかどうか, 日
本のグリコ森永事件における脅迫状の分析が代表的である. グリコ・森永事件とは,日
本の産業製菓業江崎グリコおよび森永に主として向けられた, 恐喝事件で,現在未解決
のままとなっている [14]. この事件は容疑者,「怪人 21面相」として知られている個人
またはグループとのやり取りが, グリコの社長が誘拐されてから最後に接触するまで,
全体で 17ヶ月かかった事件である. 詳しくは http://ja.wikipedia.org/wiki/グリコ・森
永事件を参照.
著者推定・分析を行うためには,文体の計量的特長 (stylometry),例えば語長・文長・
語数や機能語 (while, on などの不要語記号)などを調べる方法があるが, 同一筆者でも
差が大きく特徴が有効とはいいがたい [16].
トピック推定は, 効率よく検索するための文脈に依存した情報の評価や要約, トピッ
クへの文書の自動分類の仕方とも関係がある.
同一著者の下では, 各トピックは対応する語集合の多項式分布確率で表わされるとす
る T/Wモデルが議論されることが多い [20]. これが正しければ,トピック上の語分布
を検討および確率分布の識別をすることで, トピック推定が可能になる. 一般に,文書
は複数トピックを含むが,本研究では文書とトピックを同一視し, トピック推定を効率
よく実現する手法を考える. 代表例はニュース記事,つまりニュース放送の翻訳である.
4.6 結び
本研究では,T/AWモデルが経験的に適用できることを示した. また,TF∗IDFはX2
分布分析と併用する事で上手く働くことを述べた. さらに,AWモデルを実際に適用す
ることはできず, 著者推定ではなくトピック推定を考えることが得策であることを示し
た. 次にトピック推定に適した次元縮小を提案しランダム・プロジェクションが実際に
有用であることを確認した. 実験結果より 1グラムモデルにおいてトピック語モデル
を仮定することができ, RP技術が次元数の 98.59%を縮小しても良い有効性を維持する
ことができることを示した.
41
第5章 結論
本研究では,テキスト形式で保持されてデータの管理・検索をより高度に行うための
支援の一つとして, テキストデータのトピックを推定する手法とその改善について論
じた.
まず,共起語を考慮に入れたEMアルゴリズムによるテキスト分類の新たな手法を提
案した. 実験の結果から,共起語の概念を取り入れることで分類精度の向上を図ること
に成功した. またこの際,共起語の追加に高いしきい値を設けることで, しきい値を低
く設定した場合よりも, より相関的な共起語を考慮に入れることで,テキスト分類の精
度を向上させることができる.
次に,T/AW モデルの検証および推定方法の改善を提案した. 検証の結果,T/AW モ
デルを用い,著者の作品の語分布を調べることで, トピックの推定を行うことができた.
また推定方法の改善として,いくつかの実験から, 単語の重みとして従来の頻度ではな
く,TF*IDF値を用いることで精度の向上を図ることができた.
また,T/AW モデルにランダムプロジェクションを適用し, モデルに対する効率的で
有効な処理を行った. ランダムプロジェクションを適用する事で, 次元縮小率 98.59%で
あっても信頼性 8割の正答率を維持することが可能であり, 効率,計算機容量の確保及
び即応性への対応が可能となった.
これにより,トピックを推定する事で,テキストデータの自動分類が可能となった. ま
たこれは,作品には固有の特徴,語の分布が存在していると言うことができる. テキス
トデータの特徴を上手く捉えることが,トピックの推定に必要である. この際,モデル
に対してランダムプロジェクション技法を適用することで, モデルを崩さずに次元を縮
小することができ,次元数が数万におよぶ高次元データに対しても, 効率の良さを確保
しつつ,同様の手法で情報へのアクセスを支援することができるようになると言える.
42
謝辞
本研究を遂行するにあたり，日頃より数々のご指導をいただいた，法政大学工学部
情報電気電子工学科 三浦孝夫教授に深く御礼申し上げます．
また，産能大学経営情報学科 塩谷勇教授にも多くのご指導をいただきました．深く
感謝いたします．
データ工学研究室の先輩方，同輩，後輩たちにも，本研究の遂行にあたって数多く
の助言と快適な研究環境の整備をして頂きました．御礼申し上げます．
修士論文として私の研究をまとめることができたのも，多くの皆様方の御支援，御
協力の賜物であります．この場をお借りしまして，厚く御礼申し上げます．
最後に，今までの学生生活を支えてくださった私の両親に感謝したいと思います．
43
参考文献
[1] J.Han, et.al : Data Mining: Concepts and Techniques, Morgan Kaufmann Pub.,
2000
[2] 岩崎 学: 不完全データの統計解析,エコノミクス社,2002
[3] 松尾, 石塚: 語の共起の統計情報に基づく文書からのキーワード抽出アルゴリズム,
人工知能学会論文誌 17-3-D, pp.217-223, 2002
[4] T.Mitchell: Machine Learning, McGraw-Hill Education, 1997
[5] Nigam, K., McCallum, A.K., Thrun, S. and Mitchell, T.M.: Text Classification
from Labeled and Unlabeled Documents using EM , Machine Learning Vol .39,
No.2, pp. 103–134, 2000
[6] 大澤 幸夫,ネルス E.ベンソン,谷内田雅彦,: KeyGraph : 語の共起グラフの分
割・統合によるキーワード抽出, 電子情報通信学会論文誌　D-I Vol.J82-D-I No.2
pp.391-400, 1999
[7] Poter, M.F.:An algorithm for shuffix stripping , Program, Vol. 14, No. 3, pp.130-
137,1980
[8] 上嶋 宏,三浦 孝夫,塩谷 勇,: Estimating Timestamp From Incomplete News Cor-
pus, Journal of Communications in Information and Systems : Special Issue on
Computational Informatics in Data Mining and Information Retrieval, Vo.4, No.4,
International Press, pp.273-288 , 2005
[9] 上嶋 宏,三浦 孝夫,塩谷 勇,: Improving Text Categorization by Synonym and
Polysemy, SYSTEMS AND COMPUTERS IN JAPAN, Vol. 36-4, pp.1-8, 2005
April
[10] 吉原 幸輝, 三浦 孝夫, 塩谷 勇: Classifying Melodies by Using EM Algorithm,
IEEE Computer Software and Application Conference (COMPSAC), pp.204-210,
2005
[11] 新納 浩幸, 佐々木 捻.: EMアルゴリズムの最適ループ回数の予測を用いた語義判
別規則の教師なし学習，情報処理学会論文誌 Vol.44,No.12,2003
44
[12] Achloiptas， D.: Database-friendly random projections，ACM-PODS 2001，
pp.274-281
[13] 北研二，他: 情報検索アルゴリズム，共立出版， 2002
[14] 村上征勝 : シェークスピアは誰ですか?―計量文献学の世界，文藝春秋社， 2004
[15] The Complete Works of William Shakespeare，
http://shakespeare.mit.edu/works.html
[16] E.Stamatos， N.Fakotakis， G.Kokkinakis: Automatic Authorship Attribution，
EACL， 1999
[17] M.Steyvers， P.Smyth， T.Griffiths : Probabilistic Author-Topic Models for In-
formation Discovery，KDD， 2004
[18] Gutenberg Project, http://www.gutenberg.org
[19] Oh’uchi, H., Miura, T. and Shioya, I.: Document Retrieval using Projection by
Frequency Distribution, Intn’l J. on Artificial Intelligence Tools (IJAITS), Special
Issue, Vol.16, 2007
[20] M.Steyvers, P.Smyth, T.Griffiths : Probabilistic Author-Topic Models for Infor-
mation Discovery, KDD, 2004
[21] Nakayama,M., Miura, T.: Identifying Topics by using Word Distribution, IEEE
Pacific Rim Conference on Communications, Computers and Signal Processing
(PACRIM), 2007,pp.245-248
45
