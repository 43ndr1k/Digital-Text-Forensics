Fast Algorithms for Learning with Long N -grams via Suffix Tree Based Matrix
Multiplication
Hristo S. Paskov
Computer Science Dept.
Stanford University
hpaskov@stanford.edu
John C. Mitchell
Computer Science Dept.
Stanford University
john.mitchell@stanford.edu
Trevor J. Hastie
Statistics Dept.
Stanford University
hastie@stanford.edu
Abstract
This paper addresses the computational issues of
learning with long, and possibly all, N -grams in
a document corpus. Our main result uses suf-
fix trees to show that N -gram matrices require
memory and time that is at worst linear (in the
length of the underlying corpus) to store and to
multiply a vector. Our algorithm can speed up
any N -gram based machine learning algorithm
which uses gradient descent or an optimization
procedure that makes progress through multipli-
cation. We also provide a linear running time and
memory framework that screens N -gram fea-
tures according to a multitude of statistical crite-
ria and produces the data structure necessary for
fast multiplication. Experiments on natural lan-
guage and DNA sequence datasets demonstrate
the computational savings of our framework; our
multiplication algorithm is four orders of mag-
nitude more efficient than naı̈ve multiplication
on the DNA data. We also show that prediction
accuracy on large-scale sentiment analysis prob-
lems benefits from long N -grams.
1 Introduction
N -gram models are indespensible in natural language pro-
cessing (NLP) , information retrieval, and, increasingly,
computational biology. They have been applied succuss-
fully in sentiment analysis (Paskov, 2013), text catego-
rization (Cavnar, 1994), author identification (Houvardas,
2006), DNA function prediction (Kähärä, 2013), and nu-
merous other tasks. The allure of N -gram models comes
from their simplicity, interpretability, and efficacy: a doc-
ument corpus is represented by its N -gram matrix, where
each row/column corresponds to a distinct document/N -
gram respectively, and each entry counts the number of oc-
currences of that N -gram in the document. The N -gram
matrix provides a feature representation for statistical mod-
elling and the coefficients of each N -gram can often be in-
terpreted as a score indicating their relevance to the task.
At the simplest extreme, unigrams provide a summary of
the word distribution in each document and serve as an ef-
fective baseline representation for a variety of NLP tasks.
Higher order N -grams provide more nuance by capturing
short-term positional information and can achieve state of
the art results on a variety of tasks (Wang, 2012) (Paskov,
2013). A canonical example of the value of longer N -
grams is given by the phrase ”I don’t like horror movies,
but this was excellent,” which fails to convey its positive
sentiment when its words are scrambled. Unfortunately,
this additional information comes at a cost: a document of
n words may contain up to Θ(Kn) distinct N -grams of
length K1. This growth makes the memory and computa-
tional burden of training N -gram models beyond bigrams
impractical for large natural language corpora. Statistically,
these larger feature representations suffer from the curse of
dimensionality (Hastie, 2001) and may lead the model to
overfit, so careful regularization is necessary.
This paper ameliorates the computational burden of learn-
ing with long N -grams. We demonstrate how the struc-
ture of suffix trees can be used to store and multiply2 any
N -gram matrix in time and space that is at most linear in
the length of the underlying corpus. As most learning al-
gorithms rely on matrix-vector multiplication to learn and
predict, our results equate the computational cost of learn-
ing with N -gram matrices to scanning through the original
corpus. Our method can speed up any learning algorithm
that exhibits such structure by simply replacing its multi-
plication routine with ours. Fast multiplication is possi-
ble by means of a specialized data structure that efficiently
represents the algebraic structure of the N -gram matrix. In
view of the statistical issues associated with longN -grams,
we provide a linear running time and memory framework
that not only computes this data structure, but also filters
N -grams by various criteria and outputs necessary column
1We use N -grams of length K to mean N -grams of length at
most K for brevity.
2Multiplication always refers to matrix-vector multiplication.
normalizations. The emphasis of this framework is mini-
mality: by only storing the topological structure of the suf-
fix tree we achieve memory requirements that are compa-
rable to storing the original document corpus. As such, our
framework can be used to permanently store the corpus in
a format that is optimized for machine learning.
Our paper is organized as follows: section 2 derives the
fast multiplication algorithm by showing that after redun-
dant columns in the N -gram matrix are removed, the al-
gebraic structure of the resulting submatrix is encoded by
the suffix tree of the underlying corpus. We then inves-
tigate how this matrix can be used as a black-box in var-
ious common learning scenarios in section 3. Section 4
presents our preprocessing framework. Timing and mem-
ory benchmarks that demonstrate the efficacy of the multi-
plication algorithm are presented in section 5. We also find
that high-order N -grams can improve prediction accuracy
in large-scale sentiment analysis tasks.
1.1 Related Work
Suffix trees and arrays are used by (Vish., 2004), (Teo,
2006), and (Rieck, 2008) for kernels that efficiently com-
pute pair-wise document similarities based on N -grams.
Computing the similarity of all document pairs limits ker-
nels to moderately sized datasets and the lack of explicit
features prevents the use of sparsity inducing regularizers
such as in the Lasso (Tibs., 1996). Next, (Zhang, 2006)
use suffix trees to identify useful N -grams in a text cor-
pus and to show that the all N -gram matrix may be pruned
since it contains redundant columns. We show in section 2
that the resulting N -gram matrix may still have too many
entries to be practical for large corpora and observe this ex-
perimentally. Suffix trees are also used by (Wood, 2011)
to efficiently represent and perform inference with a hier-
archical process for text. Finally, while (Abou., 2004) and
(Kasai, 2001) provide space efficient frameworks for work-
ing with suffix arrays, our framework is specialized to sta-
tistical processing and achieves greater memory efficiency.
1.2 Multiplication in Machine Learning
We briefly discuss the importance of matrix-vector multi-
plication for learning. Let x1, . . . ,xN ∈ Rd be N data
points with corresponding labels y1, . . . , yN ∈ Y and let
X ∈ RN×d be the feature matrix that stores xi as its ith
row. Matrix-vector multiplication operations abound in all
phases of supervised and unsupervised learning: basic pre-
processing that computes normalizings factors of the form
XT1 (or X1) for every feature (or data point); screening
rules that use |XT y| (when Y ⊂ R) to exclude uninfor-
mative features (Tibs., 2010); or predictions of the form
f(Xw) where w is a learned vector of weights.
Multiplication is also essential for many of the optimiza-
tion techniques that lie at the core of these learning algo-
rithms. A variety of learning problems can be expressed as
optimization problems of the form
minimize
w∈Rd,β∈Rp
Ly(Xw, β) + λR(w) (1)
where w, β are the learning parameters, Ly is a loss func-
tion that encodes the yi labels (if the problem is super-
vised), and R is a regularization penalty. It is important to
remember that this framework captures a number of unsu-
pervised learning problems as well, such as Principle Com-
ponent Analysis, which is useful directly and as a prepro-
cessing step for clustering, deep learning, and other tech-
niques (Hastie, 2001). Any (sub)gradient3 of (1) with re-
spect to w is given by
gw ∈ XT∂XwL(Xw, β) + λ∂wR(w). (2)
where ∂zf(z) is the subdifferential of f with respect to z.
Since every (sub)gradient descent method (Parikh, 2013) or
accelerated variant critically relies on gw as a search direc-
tion, computing Xw and then XT [∂XwL(Xw, β)] is es-
sential and often the most costly part of the optimization.
A number of other popular large-scale optimization meth-
ods also reduce to multiplyingX repeatedly. These include
Krylov subspace algorithms such as the conjugate gradi-
ent method, and various quasi-Newton methods including
BFGS and its limited memory variant (Nocedal, 2006).
1.3 Background and Notation
Let Σ be a finite vocabulary with a strict total ordering
≺ over its elements. A document D = x1x2...xn of
length n is a list of n characters drawn from Σ and an
N -gram is any string of characters drawn from Σ. We
will refer to each of the n suffixes in D via D[i] =
xixi+1...xn. We denote the set of all substrings in D by
D∗ = {xi...xi+k | 1 ≤ i ≤ n, 0 ≤ k ≤ n− i } and the set
of all substrings in a document corpus of N documents
C = {D1, . . . , DN} as C∗ =
⋃N
i=1D
∗
i .
Given a subset S ⊆ C∗ of the set of substrings in (any
of) the documents, entry Xis of the N -gram matrix X ∈
ZN×|S|+ counts how many times substring s ∈ S appears
in document Di. We use Mi to indicate the ith column of
matrix M ; when each column pertains to a specific math-
ematical object, such as an N -gram or tree node, we may
use that object as an index (to avoid imposing a particular
ordering over the objects). We will always take X to be an
N -gram matrix for an implicitly given corpus.
A compact tree T = (V,E) is a tree with nodes V and
edges E where every internal node is required to have at
least 2 children. This ensures that if T has n leaves, then
there are at most n − 1 internal nodes. We use ch(v) ⊂ V
and p(v) ∈ V to denote the children and parent of v ∈ V ,
3To handle non-differentiable objectives, see (Parikh, 2013).
respectively. The root node is given by root(T ), the depth
of any node v ∈ V is d(v) (with d(root(T )) = 1), and
depth(T ) is the maximum depth of any node in V . Finally,
a branch of T is a path starting at the root and ending at
a leaf; we will use the terminal leaf to identify branches.
We will also be concerned with subtrees T̂ = (V̂ , Ê) of
T which contain a subset V̂ ⊂ V of its nodes. We allow
the new edge set Ê to be arbitrary and add a second argu-
ment to ch(v, Ê) and p(v, Ê) to indicate that parent/child
relationships are taken with respect to this new edge set.
1.3.1 Suffix Trees
Given a documentD = x1x2...xn whose characters belong
to an alphabet Σ, the suffix tree TD = (V,E) for D is a
compact tree with n leaves, each of which corresponds to a
distinct suffix of D and is numbered according to the start-
ing position of the suffix 1, . . . , n. The edges along branch
i are labelled with non-empty substrings that partitionD[i]:
suffix D[i] can be recovered by concatenating the edge la-
bels from the root to leaf i. Let l(e) for e ∈ E be the label
of edge e and define the node character c(v) of any non-
root node v ∈ V to be the first character of l((p(v), v)).
The nodes of TD are constrained so that siblings may not
have the same node character and are ordered according to
the≺ relation on these characters. These constraints ensure
that every node has at most |Σ| children and they allow for
well-defined traversals of TD. Moreover, every substring
s ∈ D∗ is represented by a unique path in TD that starts at
the root node and terminates in — possibly the middle of
— an edge. Similarly to suffixes, s equals the concatena-
tion of all characters encountered along edges from the root
to the path’s terminus (only a prefix of the final edge will
be concatenated if the path ends in the middle of an edge).
Remarkably, TD can be constructed in O(n) time (Gus-
field, 1997) and has n leaves and at most n − 1 internal
nodes, yet it represents all O(n2) distinct substrings of D.
This is possible because any substrings whose path repre-
sentation in T ends at the same edge belong to the same
equaivalence class. In particular, for v ∈ V \{root(TD)}
suppose that edge (p(v), v) has a label t = xi . . . xi+k and
let s be the string obtained by concatenating the edge labels
on the path from root(TD) down to p(v). Then the strings
S(v) = {sxi, sxi+1, . . . , sxi+k} belong to the same equiv-
alence class because they occur in the same locations, i.e.
if sxi starts at location l in D, then so do all members of
S(v). For example, in the string ”xaxaba” the substrings
”x” and ”xa” belong to the same equivalence class.
The generalized suffix tree TC for a document corpus C of
n words compactly represents the set of all substrings in C∗
and has n leaves pertaining to every suffix of every docu-
ment in C. Leaves are also annotated with the document
they belong to and TC inherits all of the linear-time stor-
age and computational guarantees of the regular suffix tree
(with respect to the corpus length n).
1.3.2 Tree Traversals and Storage
The majority of algorithms in this paper can be expressed
as a bottom-up or top-down traversal of a tree T = (V,E)
(typically the suffix tree or one of its subtrees) in which in-
formation is only exchanged between a parent and its chil-
dren. Given a fixed ordering of V , the necessary informa-
tion for a traversal is the topology of T , i.e. its parent-
child relationships, as well as any node annotations neces-
sary for the computation. We use two formats which effi-
ciently store this information and make traversals easy: the
breadth-first format (BFF) and preorder depth-first format
(DFF). In both cases we distinguish between the internal
nodes and leaves of T and divide them into their respective
sets I ∪ L = V . In the BFF we order the nodes of I ac-
cording to their breadth-first traversal whereas in the DFF
we order the nodes of I according to their preorder depth
first traversal; both formats assign indices [0, . . . , |I|) to
the nodes in I . Note that for these traversals to be well de-
fined we assume that the children of each node are ordered
in some (arbitrary) but fixed manner. Next, the leaves of T ,
i.e. L, are assigned indices [|I|, . . . , |V |) so that if u, v ∈ L
and p(u) comes before p(v) – note that both parents must
be in I – then u comes before v. This ordering ensures that
leaves are ordered into contiguous blocks with respect to
their parent and that the blocks are in the same order as I .
A pair of arrays (chI , chL), each of size |I|, capture the
topology of T : for all v ∈ I , chIv = |ch(v) ∩ I| stores the
number of internal children of v and chLv = |ch(v) ∩ L|
stores the number of leaf children of v. The number of bits
needed to store this topology is
|I|(dlog2 U(I)e+ dlog2 U(L)e) (3)
whereU(I), U(L) are the largest values in chI , chL respec-
tively, i.e. the largest number of internal/leaf children for
any node. Given node annotations in the same order as
the BFF or DFF, top-down/bottom-up traversals are easy to
perform by a linear sweep of the annotations and chI , chL
arrays. All memory access is sequential and can be per-
formed efficiently by standard (i.e. desktop) memory and
processors.
A speed/memory tradeoff exists for the two formats. The
amount of random access memory necessary for a traversal
is proportional to the depth of T for DFF versus the width
of T for the BFF. As we discuss in section 4, the former
is likely to be smaller than the latter for our purposes. The
space savings of the DFF are achieved by maintaining a
stack of active nodes pertaining to the current branch be-
ing processed. The additional logic required for this book-
keeping makes the DFF slightly slower than the BFF for
the traversal. As such, the DFF is useful for more com-
plicated computations in which the amount of information
stored per node may be large, whereas the BFF is useful for
simple computations that will be performed many times.
2 Fast Multiplication
This section presents our fast multiplication algorithm. Let
TC = (V,E) be the suffix tree for a document corpus C =
{D1, . . . , DN} and let X be an N -gram matrix containing
a column for every s ∈ S ⊆ C∗, i.e. the N -grams we are
interested in. In order to uncover the necessary algebraic
structure for our algorithm we must first remove redundant
columns in X . As observed in (Zhang, 2006), redundant
columns occur whenever strings in S belong to the same
equivalence class. This implies the following lemma:
Lemma 1. For any v ∈ V , any s, s′ ∈ S ∩ S(v) have the
same distribution among the documents in C so Xs = Xs′ .
We remove this redundancy by working with the node ma-
trix X ∈ ZN×M+ , a submatrix of X that contains a single
column for the M equivalence classes present in S. For-
mally, node v ∈ V is present in X if S(v) ∩ S 6= ∅ and
we define V ⊂ V 4 to be the set of all nodes present in X .
Column Xv for v ∈ V is obtained by picking an arbitrary
s ∈ S(v)∩S and settingXv = Xs. We can also reconstruct
X from X by replicating column Xv |S(v)∩S| times; this
underscores the inefficiency in the N -gram matrix.
2.1 Linear Dependencies in the Node Matrix
We are now ready to show how the topology of TC deter-
mines the linear dependencies among the columns of X .
Central to our analysis is the lemma below, which shows
that the document frequency of any node is determined en-
tirely by the leaves of its subtree:
Lemma 2. The number of times node v ∈ V \{root(TC)}
appears in document Di ∈ C equals the number of leaves
that belong to Di in the subtree rooted at v.
The simplest case occurs when V = V \{root(TC)}, i.e.
every node in TC (except for the root) has a corresponding
column in X . In this case lemma 2 directly establishes a
recursive definition for the columns of X :
Xv =
{
eNdoc(v) if v is a leaf∑
u∈ch(v) Xu otherwise.
(4)
Here eNi is the i
th canonical basis vector for RN and doc(v)
indicates the document index leaf v is labelled with. Im-
portantly, (4) shows that the column corresponding to any
internal node can be expressed as a simple linear combina-
tion of the columns of its children. This basic property lies
at the core of our fast multiplication algorithm.
We now show how to apply the reasoning behind (4) to
the more general case when V is an arbitrary subset of
V , i.e. a node’s children may be partly missing. Define
TC(V) = (V̂ , Ê), the restriction of TC to V , to be a tree
4Note that V never includes the root node.
with nodes V̂ = V ∪ {root(TC)}. In addition, for any
v ∈ V \{root(TC)} let la(v, V̂ ) ∈ V̂ be the closest proper
ancestor of v in TC that is also in V̂ ; since root(TC) ∈ V̂ ,
this mapping is always well defined. The edge set Ê pre-
serves the ancestor relationships among the nodes in V̂ : ev-
ery v ∈ V is connected to la(v, V̂ ) as a child. An inductive
argument shows that if u, v ∈ V̂ , then u is an ancestor of v
in TC if and only if u is also an ancestor of v in TC(V).
Associated with TC(V) is a matrix Φ ∈ ZN×|V|+ that sub-
sumes the role of leaf document labels. Φ contains a col-
umn for every node v ∈ V and accounts for all of the leaves
in TC . When v is a leaf in TC and v is included in V we set
Φv = e
N
doc(v). Otherwise, v is accounted for in Φla(v,V̂ ), the
column pertaining to v’s closest ancestor in V . In particu-
lar, if u ∈ V is not a leaf in TC , then
Φu =
∑
v∈leaves(TC)\V
la(v,V̂ )=u
eNdoc(v). (5)
This bookkeeping allows us to relate the columns of X
when V is any subset of V :
Theorem 1. The columns of the node matrix X for V ⊆
V \ {root(TC)} are given recursively by
Xv = Φv +
∑
u∈ch(v;Ê)
Xu
where Φ and TC(V) = (V̂ , Ê) are defined above.
This theorem shows that Xv is a simple linear combination
of the columns of its children in TC(V) plus a correction
term in Φ. We utilize this structure below to give a fast
matrix-vector multiplication algorithm for node matrices.
2.2 Fast Multiplication Algorithm
A simple application of Theorem 1 shows that the matrix-
vector product Xw for w ∈ R|V| can be obtained by re-
cursively collecting entries of w into a vector β ∈ R|V|:
βv = wv + βp(v;Ê) (6a)
Xw = Φβ (6b)
Here we use the convention βroot(TC(V)) = 0. The trans-
posed operation X T y for y ∈ RN can also be written re-
cursively by expressing each entry as
(X T y)v = ΦTv y +
∑
u∈ch(v;Ê)
(X T y)u. (7)
Equations (6-7) lead to the following theorem, for which
we provide a proof sketch:
Theorem 2. Let C be a document corpus of n words and
let X be any node matrix derived from this corpus. Then X
requires O(n) memory to store. Multiplying a vector with
X or X T requires O(n) operations.
Proof. Vector β in equation (6) can be computed in
O(|V|) ∈ O(n) operations by a top-down traversal that
computes each of its entries in constant time. The matrix Φ
is a sparse matrix with at most one entry per leaf of the suf-
fix tree TC , i.e. at most n entries. It follows that the product
Φβ requires O(n) operations which proves the theorem for
multiplication with X . The transposed case follows simi-
larly by noting that we must compute a matrix-vector prod-
uct with ΦT and perform a bottom-up traversal that per-
forms constant time operations for every node in V .
2.2.1 Efficiency Gains
We use naı̈ve multiplication to mean sparse matrix-vector
multiplication in what follows. The supplementary mate-
rial discusses examples which show that naı̈ve multipli-
cation with the N -gram matrix X can be asymptotically
slower than naı̈ve multiplication with X , which in turn can
be asymptotically slower than multiplication with our re-
cursive algorithm. These examples establish the following
complexity separation result:
Theorem 3. There exists documents of n words for which
1. The all N -grams matrix X requires Θ(n2) storage
and operations to multiply naı̈vely.
2. The all N -grams node matrix X requires Θ(n√n)
storage and operations to multiply naı̈vely.
3. In all cases recursive multiplication of the node matrix
requires O(n) storage and operations.
2.3 Matrix Data Structure
The fast multiplication algorithm can be performed directly
on the suffix tree derived from C, but it is faster to use
a dedicated data structure optimized for the algorithm’s
memory access patterns. The breadth-first multiplication
tree (BFMT) stores the topology of TC(V) in the BFF (dis-
cussed in section 1.3.2) and the frequency information in
Φ as a sparse matrix in a modifed compressed sparse col-
umn (csc) format (see the supplementary material) whose
columns are ordered according to the order of the BFF. We
chose this format because executing equations (6) and (7)
requires a simple linear sweep of the BFMT. We expect that
the vectors being multiplied can be stored in memory and
therefore opted for the speed afforded by the BFF instead
of the memory savings of the DFF.
The total number of bits necessary to store the BFMT is
given by equation (3) along with the total number of bits
necessary to store Φ, which is
|V|dlog2 UΦe+ nz(dlog2Me+ dlog2Ne). (8)
Here UΦ = maxv∈V‖Φv‖0 is the largest number of non-
zero elements in a column of Φ, nz is the total number of
non-zero elements in Φ, and M is the largest entry in Φ. It
is easy to verify that |I| ≤ |V| ≤ nz ≤ n and the term in-
volving nz typically dominates the memory requirements.
3 Common Usage Patterns
We now discuss how several common machine learning
scenarios can be adapted to use our representation of the
node matrix or preferably, to treat multiplication with X as
a black-box routine. The most straightforward use case is
to replace the original N -gram matrix with the more suc-
cint node matrix. Moreover, mean centering and column
normalization can be performed implictly, without modify-
ing X , by premultiplying and adding a correction term:((
X − 1µT
)
Σ
)
w = X (Σw)− (µTΣw)1
Here µ is a vector of column means and Σ is a diagonal
matrix of column normalizing factors. Analogous formulas
exist for row centering and normalization.
3.1 Problem Reformulation
A variety of optimization problems can also be reformu-
lated so that they are equivalent to using the original N -
gram matrix. A simple example of such a conversion comes
from using ridge regression to model label yi ∈ R based on
the ith row of the N -gram matrix X . We wish to solve
minimize
w∈Rd
1
2
‖y −Xw‖22 +
λ
2
‖w‖22. (9)
It is easy to show that if λ > 0 and N -grams s, t belong
to the same equivalence class, then ws = wt. We can sim-
ulate the effect of these duplicated variables by collecting
terms. Let S be the set of N -grams present in X , V the
set of suffix tree nodes present in X , and define S(v) =
S(v) ∩ S for brevity. For all v ∈ V let zv = |S(v)|ws
for some s ∈ S(v). Then ∑s∈S(v)Xsws = Xvzv and∑
s∈S(v) w
2
s = |S(v)|−1z2v so problem (9) is equivalent to
a smaller weighted ridge regression using X :
minimize
z∈R|V|
1
2
‖y −X z‖22 +
λ
2
∑
v∈V
z2v
|S(v)| . (10)
Note that this also shows that representations using the N -
gram matrix downweight the ridge penalty of each equiva-
lence class in proportion to its size.
We can characterize the set of optimization problems that
have an equivalent problem where the N -gram matrix can
be replaced with the node matrix. Define a partition J of
the integer set {1, . . . , d} to be a set of m integral inter-
vals ζk = {i, . . . , j} such that
⋃m
k=1 ζk = {1, . . . , d} and
ζk ∩ ζj = ∅ if k 6= j. A function f : Rd → Rp is per-
mutation invariant with respect to J (abbrev. J -PI) if for
all x ∈ Rd, f(x) = f(π[x]) where π : Rd → Rd is any
permutation that only permutes indices within the same in-
terval ζk ∈ J . For our purposes it is important to note that
Lp-norms are J -PI as are affine functionsAx+bwhenever
columns Ai = Aj ∀i, j ∈ ζk,∀ζk ∈ J . It is straightfor-
ward to show that if f, g : Rd → Rp are both J -PI and
c : Rp → Rq then f(x) + g(x) and c(f(x)) are also J -PI.
We prove the following theorem in the supplementary ma-
terial to connect permutation invariance to optimization:
Theorem 4. Let f : Rd → R be any convex function that
is J -PI where m = |J |. Then there exists a convex func-
tion g : Rm → R over m variables such that the problem
minx∈Rd f(x) is equivalent to minz∈Rm g(z). If z∗ is opti-
mal for the second problem, then xi = z∗k ∀i ∈ ζk, ∀ζk ∈
J is optimal for the first problem.
This theorem establishes that any convex loss of the form
L(Xw, b); e.g. SVM, logisitic regression, least squares;
added to any Lp norm, e.g. L2 ridge or L1 lasso penalty,
can be simplified to an equivalent learning problem that
uses the node matrix instead of the N -gram matrix.
3.2 Holding Out Data
Oftentimes the document corpus is organized into T (pos-
sibly overlapping) integral sets Q1, . . . ,QT indexing the
documents. For instance, splitting documents into train-
ing and testing sets yields T = 2 index sets, and further
subdividing the training set for K-fold crossvalidation in-
troduces 2K additional sets (indicating the hold out and
training data for each split). In this case we are not inter-
ested in multiplying all of X , but only the submatrix whose
rows’ indices are in the givenQi. This matrix-vector prod-
uct can be computed by calling the recursive multiplication
algorithm with the topology information in TC(V) (derived
from the full corpus) and with the submatrix of Φ whose
rows’ indices are in Qi. Also note that if only a subset of
the documents will ever be used for training, we can ig-
nore any nodes in TC that do not appear in the training set
since they (should) be ignored by any learning algorithm;
we discuss this further in section 4.2.
4 Preprocessing
We use an intermediary data structure, the depth-first pre-
processing tree (DFPT), to output the BFMT. The DFPT is
computed from a corpus C and stores the minimal informa-
tion in TC = (V,E) necessary to produce any BFMT and to
prune the nodes in V . It can be computed once and used to
store C in a format that is amenable for arbitrary machine
learning tasks. Given a new learning problem the DFPT
proceeds in two stages: 1) it identifies useful N -grams in
V and calculates relevant column normalizations, and 2)
it emits a BFMT tailored to that task. Construction of the
DFPT, as well as its processing stages, requires O(n) time
and memory with respect to the corpus length n.
As suggested by its name, the DFPT stores the topology of
TC in DFF, its leaf-document annotations, and if filtering
by N -gram length, the edge label length for each internal
node of V . Its processing stages are a sequence of of top-
down/bottom-up traversals of TC that are individually more
sophisticated than those required by our multiplication al-
gorithm, so we opted for the memory savings afforded by
the DFF. Indeed, depth(TC) is bounded by the length of
the longest document in C while the tree width is bounded
by the corpus length; the memory savings of the DFF over
the BFF are substantial. Importantly, the traversals stream
through the DFPT so it is reasonable to operate on it via
external memory, e.g. a hard drive, if memory is limited.
In detail, the DFPT requires 2|I|dlog2 |Σ|e + ndlog2Ne
bits to store the topology and leaf-document annotations,
where I is the set of internal nodes of V , N the number
of documents in C, and Σ the alphabet. For reference stor-
ing C requires ndlog2 |Σ|e + Ndlog2 ne bits and |I| < n.
An additional |I|dlog2 nN e bits are used to store edge la-
bels lengths, but this information is only necessary when
pruning by maximum N -gram length.
4.1 Computing the Depth-First Suffix Tree
Computing the DFPT from C represents the least memory
efficient part of our framework as we first compute a suffix
array (SA) (Gusfield, 1997) from the text and then convert
the SA into the DFPT. The process requires 3ndlog2 ne +
ndlog2(|Σ|+N)e bits and O(n) time. We emphasize that
our framework is completely modular so the DFPT only
needs to be computed once. We leave it as an open problem
to determine if a more memory efficient algorithm exists
that directly computes the DFPT.
Recalling that each leaf of TC is numbered according to the
suffix it corresponds to, the SA is a permutation of the in-
tegers [0, . . . , n) that stores the leaves of TC in a pre-order
depth-first traversal. We use an SA rather than a suffix tree
because the former typically requires 4 times less memory
than a suffix tree and can also be constructed in O(n) time
and memory. We use the implementation of (Mori, 2015),
which requires m = 3ndlog2 ne + ndlog2(|Σ|+N)e bits
to construct the SA, where the second term corresponds
to a modified copy of C. This was the most memory ef-
ficient linear-time suffix array construction algorithm we
could find; asymptotically slower but more memory effi-
cient algorithms may be preferable for DNA sequence data.
The details of how we compute the DFPT from a suffix
array are rather involved and will be discussed in an ex-
tended version of this paper. The framework of (Kasai,
2001) is instrumental in our conversion as it allows us
to simulate a post-order depth-first traversal of TC using
the SA. By carefully managing memory and off-loading
unused information to external storage, each step of the
conversion requires at most m − ndlog2 ne bits to be
stored in main memory at any time. The total memory
requirements, including storing the DFPT while it is con-
structed, never exceed the maximum of m−ndlog2 ne and
2ndlog2 ne+ (n+ |I|)dlog2Ne bits; both are less than m.
4.2 Filtering and Normalizations
The first stage of the DFPT’s processing determines which
nodes in TC should be present in the final BFMT. It also
computes any required normalizations, such as the column
mean or norm, of the node matrix X the BFMT represents.
We assume that only the internal nodes I ⊂ V of TC will
ever be used; each leaf appears in only a single document
and is unlikely to carry useful information. We model the
screening process as a sequence of filters that are applied
to I: associated with I is a boolean array b ∈ {0, 1}|I|
where bv = 1 indicates that node v ∈ I is useful and
bv = 1 ∀v ∈ I initially. Each filter takes as input the
DFPT and b, and updates b (in place) with its own crite-
ria. All of our filters are memory efficient and only need to
store |I|+O(depth(TC)) bits in memory as the BFMT can
reasonably be streamed from slower external storage. With
the exception of the unique document filter, all of the filters
listed below run in O(n) time:
N -gram length: removes nodes whose shortest corre-
sponding N -gram is longer than a given threshold.
Training set: removes nodes that do not appear in any doc-
uments designated as the training set.
Unique document frequency: removes nodes that do not
appear in at least some number of distinct documents. We
use an algorithm given in (Paskov, 2015) which runs in
O(nα−1(n)) time, where α−1 is the inverse Ackermann
function (α−1(1080) = 4) and is essentially linear-time. A
O(n) algorithm (Gusfield, 1997) is possible, but it requires
complicated pre-processing and an additional ndlog2 ne
bits of memory.
Strong rules: given mean centered document labels y ∈
RN , removes all nodes v for which |X Tv y| < λ for a
threshold λ. This implements the strong rules of (Tibs.,
2010) and can be applied to a subset of the documents
Itr ⊂ {1, . . . , N} (e.g. training data) by mean centering
only yItr and setting yi = 0 for all i /∈ Itr. Column normal-
izations are achieved by checking η−1v |X Tv y| < λ, where
η−1v is the normalization for column v. This filter essen-
tially multiplies X T y using the DFPT and the normaliza-
tion can be computed on the fly (see discussion below).
Once we know which nodes will be used in the BFMT we
typically require the column mean µ = 1NX T1 and some
kind of normalization η−1v for each column of X . Noting
that all entries of X are non-negative, the L1-norm of each
column is η = X T1. Each of these quantities is a matrix-
vector multiply that we perform using the DFPT. These
quantities can be specialized to training data by setting ap-
propriate entries of the 1 vector to 0. We can also compute
the L2-norm of each column of X or the L1/L2-norm of
each column of X − 1µT , the mean centered node matrix.
These normalizations however require O(N |I|) time and
O(Ndepth(TC)) memory; the space savings of the DFF are
critical for the memory bound. These running times are tol-
erable if performed only once, especially on the short and
wide trees that tend to occur with natural language.
4.3 Producing the Matrix Structure
The final stage in our pipeline produces the BFMT using
the DFPT and filter b. The following lemma follows from
the definitions of breadth-first and depth-first traversals and
is essential for easy conversion between the two formats:
Lemma 3. Given a tree T = (V,E), let β be an (ordered)
list of the nodes in V in breadth-first order and define δ to
be a list of V in depth-first preorder. Define β(d) and δ(d)
to be the (sub) lists of β and δ respectively containing only
nodes at depth d. Then β(d) = δ(d) ∀d = 1, . . . , depth(T ).
This lemma states that the breadth-first and depth-first pre-
order traversals list nodes in the same order if we only con-
sider the nodes of a tree at a specific depth. We thus allocate
memory for the BFMT by counting the number of nodes
with bv = 1 at each depth in the DFPT. The lemma then
allows us to copy the relevant nodes in the DFPT into the
BFMT skeleton by maintaining a stack of size depth(TC)
that keeps track of how many nodes have been written to
the BFMT at each depth. The depth-first traversal also
makes it is easy to determine edges by keeping track of
each node’s nearest ancestor (in TC) that is in the BFMT.
The copying process streams through the DFPT and b in
a single linear sweep and requires storing the BFMT and
O(depth(TC)) bits in memory.
5 Experiments
This section provides benchmarks for our multiplication al-
gorithm and applies it to solve several large-scale sentiment
analysis tasks. We implemented our framework in C 5 and
compiled it used the GCC compiler version 4.4.7 for an
x86-64 architecture. Our reference machine uses an Intel
Xeon E5-2687W processor with 8 cores running at 3.1 GHz
and has 128 Gb of RAM.
5Please contact the first author for source code.
5.1 Memory and Timing Benchmarks
We evaluate our multiplication algorithm on three kinds of
data to investigate its performance in a variety of scenarios:
short natural language articles, long technical papers, and
DNA sequence data. The first is the BeerAdvocate dataset
(McAuley, 2013), a corpus of 1, 586, 088 beer reviews to-
talling 1 Gb of plaintext and each consisting of a median
of 126 words. The second is a collection of 70, 728 jour-
nal articles collected from NCBI (U.S. National Library,
2015) with a median length of 6955 words and totalling
3 Gb of plaintext6. Our third dataset is derived from the
1000 Genomes Project (1000 Genomes Project, 2008) and
it consists of 6, 196, 151 biallelic markers, i.e. binary val-
ues, along chromosome 1 for 250 people.
Our preprocessing framework required at most 3.5 times
the memory of the original datasets for the natural language
data. The third dataset however presents a worst case sce-
nario for our framework and suffix tree/arrays in general.
It requires 185 megabytes to store because of its small al-
phabet size, yet its suffix array requires ndlog2 ne bits, i.e.
31 times more memory, and several times this amount to
compute. While the DFPT ameliorates this memory us-
age, it still requires 10 times more memory than the orig-
inal data and total memory usage went up to 18 gigabytes
when computing it from the suffix array.
Figure 1 compares the memory requirements of the BFMT
to explicitly storing the node and N -gram matrices for all
N -grams up to length K that appear in at least 2 doc-
uments. We show space requirements for our modified
sparse matrix format (MSF) as well as the standard sparse
format (SSF), e.g. used in Matlab. The top two graphs
correspond to the natural language datasets and have sim-
ilar patterns: memory usage for the explicit representa-
tions rises quickly for up to K = 7 and then tapers off
as overly long N -grams are unlikely to appear in multiple
documents. In all cases the BFMT is superior, requiring
approximately 3 times less memory than the MSF and up
to 14 times less memory than its floating-point counterpart.
While there is some separation between the node matrix
and naı̈ve all N -gram matrix, the gap – which is more pro-
nounced in the journal articles – is mitigated by filtering
N -grams that do not appear in multiple documents.
The third graph presents a striking difference between the
representations: the BFMT requires up to 41 times less
memory than the MSF node matrix and over 4, 600 times
less memory than the naive N -gram matrix. The floating
point counterparts for these matrices accentuate the differ-
ences by a factor of 5. Interestingly, the size of the BMFT
decreases as K increases from 103 to 104. This occurs be-
cause whenK ≥ 104, the BFMT behaves as if allN -grams
are included so that all entries in the frequency matrix Φ are
6This data was graciously made available by the Saccha-
romyces Genome Database at Stanford.
0 10 20 30 40 50
0
5
10
15
20 Matrix Memory Utilization
0 10 20 30 40 50
0
10
20
30
G
ig
ab
yt
es
100 101 102 103 104 105
Maximum N-Gram Length
10−5
100
105
BFMT
Node MSF
Node SSF
All MSF
All SSF
Figure 1: Memory utilization for the BFMT, node, and
all N -gram matrices as a function of maximum N -gram
length K on the BeerAdvocate data (top), journal data
(middle) and 1000 genomes data (bottom).
0 or 1. When K ≈ 103, most of the entries are bounded by
1, but a few large entries exist and force additional bits to
be used for all non-zero frequencies in Φ.
Next, figure 2 compares the average multiplication time for
the BFMT to ordinary sparse multiplication with the node
matrix. The top figure presents results for the BeerAdvo-
cate data; we did not include timings for the journal data
since they are essentially the same. We were unable to pro-
vide timing results for the node matrix on the DNA data
because it quickly exceeded our computer’s memory. All
trials were performed using a single core for fairness of
comparison. The difference between the BFMT and the
node matrix closely follows the memory requirement dif-
ferences. This is to be expected as both multiplication rou-
tines make a single pass over the data so running time is
proportional to the amount of data that must be scanned.
We also note that the BFMT running time scales gracefully
0 10 20 30 40 50
0
1
2
3
Se
co
nd
s
Matrix-Vector Multiplication Times
100 101 102 103 104 105
Maximum N-Gram Length
0
6
12
BFMT
Node MSF
Figure 2: Average time to perform one matrix-vector multi-
ply with the BFMT and node matrices as a function of max-
imum N -gram length K on the BeerAdvocate data (top)
and 1000 Genomes Data (bottom). Node matrix times are
missing for the latter because it was impractical to store.
on the DNA data; time increases at a logarithmic rate with
respect to K since the x-axis is logarithmic.
5.2 Sentiment Analysis Tasks
We applied our framework to sentiment analysis tasks on
three large datasets: the BeerAdavocate dataset, a set of
6, 396, 350 music reviews from Amazon (McAuley, 2013)
(4.6 Gb of text), and a set of 7, 850, 072 movie reviews
also from Amazon (McAuley, 2013) (7.4 Gb of text). Each
review’s sentiment is a value between 1 and 5 (indicating
negative or positive) and we tried to predict this sentiment
using a ridge regression model on features provided by the
node matrix. Each dataset was randomly split into train-
ing, validation, and testing sets comprised of 75%, 12.5%,
and 12.5% of the total data; all parameters discussed below
were selected based on their validation set performance.
We solved the regression by implementing a conjugate gra-
dient solver in C that uses our fast multiplication routine.
The ridge parameter λ was tuned on a grid of up to 100
values. We stopped tuning once the validation error in-
creased for 5 consecutive λ values and the procedure typ-
ically terminated after trying 60 values. N -grams were
pruned by maximum N -gram length and were required to
appear in at least 20 distinct documents – we experimented
with several document frequency thresholds. We also used
the strong rules to select a subset of the features for each
λ value and used αλ as the threshold; α = 1 always gave
the best performance. Finally, all columns were mean cen-
tered and normalized by their L2 norm. Our framework
computed this normalization in 2.5 minutes for the larger
movie dataset. The largest and most time intensive feature
set contained 19.4 million features and occured for K = 5
on the movie dataset. It took 26 hours to solve for and eval-
uate 69 lambda values while running on a single core. We
were able to effectively run all N -gram trials in parallel.
Table 1: Mean Squared Error for Sentiment Analysis
K Beer Music Movies
1 0.286 0.766 0.765
2 0.254 0.481 0.237
3 0.245 0.366 0.140
4 0.244 0.333 0.121
5 0.244 0.325 0.115
Table 1 summarizes the mean-squared error of our regres-
sion model on the testing set. All three datasets benefit
from longer N -grams, but we note that the longer datasets
seem to benefit more (size increases from left to right).
Confounding this potential effect are peculiarities specific
to the tasks and specific to BeerAdvocate versus Amazon
reviews (recall that the music and movie reviews both come
from the same data source). Nonetheless, it is also possi-
ble that larger datasets are better equipped to utilize long
N -grams: they provide enough examples to counter the
variance incurred from estimating coefficients for long, and
therefore relatively infrequent,N -grams. It will be interest-
ing to verify this potential effect with more experiments.
6 Conclusion
This paper shows that learning with longN -grams on large
corpora is tractable because of the rich structure in N -
gram matrices. We provide a framework that can be used
to permanently store a document corpus in a format op-
timized for machine learning. Given a particular learning
task, our framework finds helpfulN -grams and emits a data
structure that is tailored to the problem and allows for fast
matrix-vector multiplication – an operation that lies at the
heart of many popular learning algorithms.
Our work suggests a number of extensions. While our mul-
tiplication algorithm is single-threaded, the underlying data
structure is a tree and the necessary traversals are easy to
parallelize. Finding an efficient algorithm to directly com-
pute the DFPT without using a suffix array will also be use-
ful. Finally, our framework provides considerable compu-
tational savings for DNA sequence data over naı̈ve repre-
sentations (up to 4 orders of magnitude). We hope that our
algorithm will inspire additional research into longN -gram
models for this kind of data.
Acknowledgements
In loving memory of Hristo P. Georgiev (Hristo P.
Georgiev). Funding provided by the Air Force Office of
Scientific Research and the National Science Foundation.
References
[1] Mohamed Ibrahim Abouelhoda, Stefan Kurtz, and Enno
Ohlebusch. Replacing suffix trees with enhanced suffix ar-
rays. J. of Discrete Algorithms, 2(1):53–86, March 2004.
[2] William Cavnar and John Trenkle. N-gram-based text cate-
gorization. In In Proceedings of SDAIR-94, 3rd Annual Sym-
posium on Document Analysis and Information Retrieval,
pages 161–175, 1994.
[3] Dan Gusfield. Algorithms on Strings, Trees, and Sequences:
Computer Science and Computational Biology. Cambridge
University Press, New York, NY, USA, 1997.
[4] Trevor Hastie, Robert Tibshirani, and Jerome Friedman. The
Elements of Statistical Learning. Springer Series in Statis-
tics. Springer New York Inc., New York, NY, USA, 2001.
[5] John Houvardas and Efstathios Stamatatos. N-gram feature
selection for authorship identification. In Proceedings of
the 12th International Conference on Artificial Intelligence:
Methodology, Systems, and Applications, AIMSA’06, pages
77–86, Berlin, Heidelberg, 2006. Springer-Verlag.
[6] Juhani Kähärä and Harri Lähdesmäki. Evaluating a lin-
ear k-mer model for protein-dna interactions using high-
throughput selex data. BMC Bioinformatics, 14(S-10):S2,
2013.
[7] Toru Kasai, Hiroki Arimura, and Setsuo Arikawa. Efficient
substring traversal with suffix arrays.
[8] Julian McAuley and Jure Leskovec. From amateurs to con-
noisseurs: Modeling the evolution of user expertise through
online reviews. In Proceedings of the 22Nd International
Conference on World Wide Web, WWW ’13, pages 897–
908, Republic and Canton of Geneva, Switzerland, 2013.
International World Wide Web Conferences Steering Com-
mittee.
[9] Julian McAuley and Jure Leskovec. Hidden factors and hid-
den topics: understanding rating dimensions with review
text. In Proceedings of the 7th ACM conference on Rec-
ommender systems, pages 165–172. ACM, 2013.
[10] Yuta Mori. sais, https://sites.google.com/site/yuta256/sais,
2015.
[11] Jorge Nocedal and Stephen Wright. Numerical Optimiza-
tion. Springer Series in Operations Research and Financial
Engineering. Springer New York, 2006.
[12] U.S. National Library of Medicine. National center for
biotechnology information, 2015.
[13] Neal Parikh and Stephen Boyd. Proximal algorithms. Foun-
dations and Trends in Optimization, 2013.
[14] Hristo Paskov, John Mitchell, and Trevor Hastie. Unique-
ness counts: A nearly linear-time online algorithm for the
multiple common substring problem. In In preparation,
2015.
[15] Hristo Paskov, Robert West, John Mitchell, and Trevor
Hastie. Compressive feature learning. In NIPS, 2013.
[16] 1000 Genomes Project. 1000 genomes project, a deep cata-
log of human genetic variation., 2008.
[17] Konrad Rieck and Pavel Laskov. Linear-time computation
of similarity measures for sequential data. The Journal of
Machine Learning Research, 9:23–48, 2008.
[18] Choon Hui Teo and S. V. N. Vishwanathan. Fast and space
efficient string kernels using suffix arrays. In In Proceed-
ings, 23rd ICMP, pages 929–936. ACM Press, 2006.
[19] Robert Tibshirani. Regression shrinkage and selection via
the lasso. J. R. Stat. Soc. B, 58(1):267–288, 1996.
[20] Robert Tibshirani, Jacob Bien, Jerome Friedman, Trevor
Hastie, Noah Simon, Jonathan Taylor, and Ryan J. Tibshi-
rani. Strong rules for discarding predictors in lasso-type
problems., 2010.
[21] SVN Vishwanathan and Alexander Johannes Smola. Fast
kernels for string and tree matching. Kernel methods in com-
putational biology, pages 113–130, 2004.
[22] Sida Wang and Christopher Manning. Baselines and bi-
grams: Simple, good sentiment and topic classification. In
Proceedings of the ACL, pages 90–94, 2012.
[23] Frank Wood, Jan Gasthaus, Cédric Archambeau, Lancelot
James, and Yee Whye Teh. The sequence memoizer. Com-
munications of the ACM, 54(2):91–98, 2011.
[24] Dell Zhang. Extracting key-substring-group features for text
classification. In In Proceedings of the 12th ACM SIGKDD
international conference on Knowledge discovery and data
mining (KDD 06, pages 474–483. ACM Press, 2006.
